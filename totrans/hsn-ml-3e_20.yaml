- en: Chapter 18\. Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章 强化学习
- en: '*Reinforcement learning* (RL) is one of the most exciting fields of machine
    learning today, and also one of the oldest. It has been around since the 1950s,
    producing many interesting applications over the years,⁠^([1](ch18.html#idm45720166403424))
    particularly in games (e.g., *TD-Gammon*, a Backgammon-playing program) and in
    machine control, but seldom making the headline news. However, a revolution took
    place [in 2013](https://homl.info/dqn), when researchers from a British startup
    called DeepMind demonstrated a system that could learn to play just about any
    Atari game from scratch,⁠^([2](ch18.html#idm45720166401072)) eventually [outperforming
    humans](https://homl.info/dqn2)⁠^([3](ch18.html#idm45720166399568)) in most of
    them, using only raw pixels as inputs and without any prior knowledge of the rules
    of the games.⁠^([4](ch18.html#idm45720166398304)) This was the first of a series
    of amazing feats, culminating with the victory of their system AlphaGo against
    Lee Sedol, a legendary professional player of the game of Go, in March 2016 and
    against Ke Jie, the world champion, in May 2017\. No program had ever come close
    to beating a master of this game, let alone the world champion. Today the whole
    field of RL is boiling with new ideas, with a wide range of applications.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*（RL）是当今最激动人心的机器学习领域之一，也是最古老的之一。自上世纪50年代以来一直存在，多年来产生了许多有趣的应用，特别是在游戏（例如
    *TD-Gammon*，一个下棋程序）和机器控制方面，但很少成为头条新闻。然而，一场革命发生在[2013年](https://homl.info/dqn)，当时来自英国初创公司
    DeepMind 的研究人员展示了一个系统，可以从头开始学习玩几乎任何 Atari 游戏，最终在大多数游戏中[超越人类](https://homl.info/dqn2)，只使用原始像素作为输入，而不需要任何关于游戏规则的先验知识。这是一系列惊人壮举的开始，最终在2016年3月，他们的系统
    AlphaGo 在围棋比赛中击败了传奇职业选手李世石，并在2017年5月击败了世界冠军柯洁。没有任何程序曾经接近击败这个游戏的大师，更不用说世界冠军了。如今，整个强化学习领域充满了新的想法，具有广泛的应用范围。'
- en: 'So how did DeepMind (bought by Google for over $500 million in 2014) achieve
    all this? With hindsight it seems rather simple: they applied the power of deep
    learning to the field of reinforcement learning, and it worked beyond their wildest
    dreams. In this chapter I will first explain what reinforcement learning is and
    what it’s good at, then present two of the most important techniques in deep reinforcement
    learning: policy gradients and deep Q-networks, including a discussion of Markov
    decision processes. Let’s get started!'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，DeepMind（2014年被 Google 以超过5亿美元的价格收购）是如何实现所有这些的呢？回顾起来似乎相当简单：他们将深度学习的力量应用于强化学习领域，而且效果超出了他们最疯狂的梦想。在本章中，我将首先解释什么是强化学习以及它擅长什么，然后介绍深度强化学习中最重要的两种技术：策略梯度和深度
    Q 网络，包括对马尔可夫决策过程的讨论。让我们开始吧！
- en: Learning to Optimize Rewards
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习优化奖励
- en: In reinforcement learning, a software *agent* makes *observations* and takes
    *actions* within an *environment*, and in return it receives *rewards* from the
    environment. Its objective is to learn to act in a way that will maximize its
    expected rewards over time. If you don’t mind a bit of anthropomorphism, you can
    think of positive rewards as pleasure, and negative rewards as pain (the term
    “reward” is a bit misleading in this case). In short, the agent acts in the environment
    and learns by trial and error to maximize its pleasure and minimize its pain.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，软件 *代理* 在一个 *环境* 中进行 *观察* 和 *行动*，并从环境中获得 *奖励*。其目标是学会以一种方式行动，以最大化其随时间的预期奖励。如果您不介意有点拟人化，您可以将积极奖励视为快乐，将负面奖励视为痛苦（在这种情况下，“奖励”这个术语有点误导）。简而言之，代理在环境中行动，并通过试错学习来最大化其快乐并最小化其痛苦。
- en: 'This is quite a broad setting, which can apply to a wide variety of tasks.
    Here are a few examples (see [Figure 18-1](#rl_examples_diagram)):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常广泛的设置，可以应用于各种任务。以下是一些示例（参见 [图18-1](#rl_examples_diagram)）：
- en: The agent can be the program controlling a robot. In this case, the environment
    is the real world, the agent observes the environment through a set of *sensors*
    such as cameras and touch sensors, and its actions consist of sending signals
    to activate motors. It may be programmed to get positive rewards whenever it approaches
    the target destination, and negative rewards whenever it wastes time or goes in
    the wrong direction.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理程序可以是控制机器人的程序。在这种情况下，环境是真实世界，代理通过一组传感器（如摄像头和触摸传感器）观察环境，其行动包括发送信号以激活电机。它可能被编程为在接近目标位置时获得积极奖励，而在浪费时间或走错方向时获得负面奖励。
- en: The agent can be the program controlling *Ms. Pac-Man*. In this case, the environment
    is a simulation of the Atari game, the actions are the nine possible joystick
    positions (upper left, down, center, and so on), the observations are screenshots,
    and the rewards are just the game points.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理可以是控制 *Ms. Pac-Man* 的程序。在这种情况下，环境是 Atari 游戏的模拟，行动是九种可能的摇杆位置（左上、下、中心等），观察是屏幕截图，奖励只是游戏得分。
- en: Similarly, the agent can be the program playing a board game such as Go. It
    only gets a reward if it wins.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，代理可以是玩围棋等棋盘游戏的程序。只有在赢得比赛时才会获得奖励。
- en: The agent does not have to control a physically (or virtually) moving thing.
    For example, it can be a smart thermostat, getting positive rewards whenever it
    is close to the target temperature and saves energy, and negative rewards when
    humans need to tweak the temperature, so the agent must learn to anticipate human
    needs.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理不必控制物理（或虚拟）移动的东西。例如，它可以是一个智能恒温器，每当接近目标温度并节省能源时获得积极奖励，当人类需要调整温度时获得负面奖励，因此代理必须学会预测人类需求。
- en: The agent can observe stock market prices and decide how much to buy or sell
    every second. Rewards are obviously the monetary gains and losses.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理可以观察股市价格并决定每秒买入或卖出多少。奖励显然是货币收益和损失。
- en: Note that there may not be any positive rewards at all; for example, the agent
    may move around in a maze, getting a negative reward at every time step, so it
    had better find the exit as quickly as possible! There are many other examples
    of tasks to which reinforcement learning is well suited, such as self-driving
    cars, recommender systems, placing ads on a web page, or controlling where an
    image classification system should focus its attention.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，可能根本没有任何正面奖励；例如，代理可能在迷宫中四处移动，在每个时间步都会获得负面奖励，因此最好尽快找到出口！还有许多其他适合强化学习的任务示例，例如自动驾驶汽车、推荐系统、在网页上放置广告，或者控制图像分类系统应该关注的位置。
- en: '![mls3 1801](assets/mls3_1801.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1801](assets/mls3_1801.png)'
- en: 'Figure 18-1\. Reinforcement learning examples: (a) robotics, (b) *Ms. Pac-Man*,
    (c) Go player, (d) thermostat, (e) automatic trader⁠^([5](ch18.html#idm45720166374256))'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-1. 强化学习示例：(a) 机器人，(b) *Ms. Pac-Man*，(c) 围棋选手，(d) 恒温器，(e) 自动交易员⁠^([5](ch18.html#idm45720166374256))
- en: Policy Search
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略搜索
- en: The algorithm a software agent uses to determine its actions is called its *policy*.
    The policy could be a neural network taking observations as inputs and outputting
    the action to take (see [Figure 18-2](#rl_with_nn_policy_diagram)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 软件代理用来确定其行动的算法称为其*策略*。策略可以是一个神经网络，将观察作为输入并输出要采取的行动（见[图18-2](#rl_with_nn_policy_diagram)）。
- en: '![mls3 1802](assets/mls3_1802.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1802](assets/mls3_1802.png)'
- en: Figure 18-2\. Reinforcement learning using a neural network policy
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-2。使用神经网络策略的强化学习
- en: The policy can be any algorithm you can think of, and it does not have to be
    deterministic. In fact, in some cases it does not even have to observe the environment!
    For example, consider a robotic vacuum cleaner whose reward is the amount of dust
    it picks up in 30 minutes. Its policy could be to move forward with some probability
    *p* every second, or randomly rotate left or right with probability 1 – *p*. The
    rotation angle would be a random angle between –*r* and +*r*. Since this policy
    involves some randomness, it is called a *stochastic policy*. The robot will have
    an erratic trajectory, which guarantees that it will eventually get to any place
    it can reach and pick up all the dust. The question is, how much dust will it
    pick up in 30 minutes?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 策略可以是你能想到的任何算法，并且不必是确定性的。实际上，在某些情况下，它甚至不必观察环境！例如，考虑一个机器人吸尘器，其奖励是在30分钟内吸尘的量。它的策略可以是每秒以概率*p*向前移动，或者以概率1
    - *p*随机向左或向右旋转。旋转角度将是- *r*和+ *r*之间的随机角度。由于这个策略涉及一些随机性，它被称为*随机策略*。机器人将有一个不规则的轨迹，这保证了它最终会到达它可以到达的任何地方并清理所有的灰尘。问题是，在30分钟内它会吸尘多少？
- en: 'How would you train such a robot? There are just two *policy parameters* you
    can tweak: the probability *p* and the angle range *r*. One possible learning
    algorithm could be to try out many different values for these parameters, and
    pick the combination that performs best (see [Figure 18-3](#policy_search_diagram)).
    This is an example of *policy search*, in this case using a brute-force approach.
    When the *policy space* is too large (which is generally the case), finding a
    good set of parameters this way is like searching for a needle in a gigantic haystack.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何训练这样的机器人？你只能调整两个*策略参数*：概率*p*和角度范围*r*。一个可能的学习算法是尝试许多不同的参数值，并选择表现最好的组合（参见[图18-3](#policy_search_diagram)）。这是一个*策略搜索*的例子，这种情况下使用了一种蛮力方法。当*策略空间*太大时（这通常是情况），通过这种方式找到一组好的参数就像在一个巨大的草堆中寻找一根针。
- en: Another way to explore the policy space is to use *genetic algorithms*. For
    example, you could randomly create a first generation of 100 policies and try
    them out, then “kill” the 80 worst policies⁠^([6](ch18.html#idm45720166354304))
    and make the 20 survivors produce 4 offspring each. An offspring is a copy of
    its parent⁠^([7](ch18.html#idm45720166353536)) plus some random variation. The
    surviving policies plus their offspring together constitute the second generation.
    You can continue to iterate through generations this way until you find a good
    policy.⁠^([8](ch18.html#idm45720166351776))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 探索政策空间的另一种方法是使用*遗传算法*。例如，您可以随机创建第一代100个政策并尝试它们，然后“淘汰”最差的80个政策，并让20个幸存者每人产生4个后代。后代是其父母的副本加上一些随机变化。幸存的政策及其后代一起构成第二代。您可以继续通过这种方式迭代生成，直到找到一个好的政策。
- en: '![mls3 1803](assets/mls3_1803.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1803](assets/mls3_1803.png)'
- en: Figure 18-3\. Four points in the policy space (left) and the agent’s corresponding
    behavior (right)
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-3。政策空间中的四个点（左）和代理的相应行为（右）
- en: Yet another approach is to use optimization techniques, by evaluating the gradients
    of the rewards with regard to the policy parameters, then tweaking these parameters
    by following the gradients toward higher rewards.⁠^([9](ch18.html#idm45720166346048))
    We will discuss this approach, called *policy gradients* (PG), in more detail
    later in this chapter. Going back to the vacuum cleaner robot, you could slightly
    increase *p* and evaluate whether doing so increases the amount of dust picked
    up by the robot in 30 minutes; if it does, then increase *p* some more, or else
    reduce *p*. We will implement a popular PG algorithm using TensorFlow, but before
    we do, we need to create an environment for the agent to live in⁠—so it’s time
    to introduce OpenAI Gym.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用优化技术，通过评估奖励相对于策略参数的梯度，然后通过沿着梯度朝着更高奖励的方向调整这些参数。我们将在本章后面更详细地讨论这种方法，称为*策略梯度*（PG）。回到吸尘器机器人，您可以稍微增加*p*，并评估这样做是否会增加机器人在30分钟内吸尘的量；如果是，那么再增加*p*一些，否则减少*p*。我们将使用TensorFlow实现一个流行的PG算法，但在此之前，我们需要为代理创建一个环境——现在是介绍OpenAI
    Gym的时候了。
- en: Introduction to OpenAI Gym
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym简介
- en: 'One of the challenges of reinforcement learning is that in order to train an
    agent, you first need to have a working environment. If you want to program an
    agent that will learn to play an Atari game, you will need an Atari game simulator.
    If you want to program a walking robot, then the environment is the real world,
    and you can directly train your robot in that environment. However, this has its
    limits: if the robot falls off a cliff, you can’t just click Undo. You can’t speed
    up time either—adding more computing power won’t make the robot move any faster—and
    it’s generally too expensive to train 1,000 robots in parallel. In short, training
    is hard and slow in the real world, so you generally need a *simulated environment*
    at least for bootstrap training. For example, you might use a library like [PyBullet](https://pybullet.org)
    or [MuJoCo](https://mujoco.org) for 3D physics simulation.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个挑战是，为了训练一个代理程序，您首先需要一个可用的环境。如果您想编写一个代理程序来学习玩Atari游戏，您将需要一个Atari游戏模拟器。如果您想编写一个行走机器人，那么环境就是现实世界，您可以直接在该环境中训练您的机器人。然而，这也有其局限性：如果机器人掉下悬崖，您不能简单地点击撤销。您也不能加快时间——增加计算能力不会使机器人移动得更快——而且通常来说，同时训练1000个机器人的成本太高。简而言之，在现实世界中训练是困难且缓慢的，因此您通常至少需要一个*模拟环境*来进行引导训练。例如，您可以使用类似[PyBullet](https://pybullet.org)或[MuJoCo](https://mujoco.org)的库进行3D物理模拟。
- en: '[*OpenAI Gym*](https://gym.openai.com)⁠^([10](ch18.html#idm45720166329952))
    is a toolkit that provides a wide variety of simulated environments (Atari games,
    board games, 2D and 3D physical simulations, and so on), that you can use to train
    agents, compare them, or develop new RL algorithms.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[*OpenAI Gym*](https://gym.openai.com)是一个工具包，提供各种模拟环境（Atari游戏，棋盘游戏，2D和3D物理模拟等），您可以用它来训练代理程序，比较它们，或者开发新的RL算法。'
- en: 'OpenAI Gym is preinstalled on Colab, but it’s an older version, so you’ll need
    to replace it with the latest one. You also need to install a few of its dependencies.
    If you are coding on your own machine instead of Colab, and you followed the installation
    instructions at [*https://homl.info/install*](https://homl.info/install), then
    you can skip this step; otherwise, enter these commands:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym在Colab上预先安装，但是它是一个较旧的版本，因此您需要用最新版本替换它。您还需要安装一些它的依赖项。如果您在自己的机器上编程而不是在Colab上，并且按照[*https://homl.info/install*](https://homl.info/install)上的安装说明进行操作，那么您可以跳过这一步；否则，请输入以下命令：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first `%pip` command upgrades Gym to the latest version. The `-q` option
    stands for *quiet*: it makes the output less verbose. The `-U` option stands for
    *upgrade*. The second `%pip` command installs the libraries required to run various
    kinds of environments. This includes classic environments from *control theory*–the
    science of controlling dynamical systems–such as balancing a pole on a cart. It
    also includes environments based on the Box2D library—a 2D physics engine for
    games. Lastly, it includes environments based on the Arcade Learning Environment
    (ALE), which is an emulator for Atari 2600 games. Several Atari game ROMs are
    downloaded automatically, and by running this code you agree with Atari’s ROM
    licenses.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`%pip`命令将Gym升级到最新版本。`-q`选项代表*quiet*：它使输出更简洁。`-U`选项代表*upgrade*。第二个`%pip`命令安装了运行各种环境所需的库。这包括来自*控制理论*（控制动态系统的科学）的经典环境，例如在小车上平衡杆。它还包括基于Box2D库的环境——一个用于游戏的2D物理引擎。最后，它包括基于Arcade
    Learning Environment（ALE）的环境，这是Atari 2600游戏的模拟器。几个Atari游戏的ROM会被自动下载，通过运行这段代码，您同意Atari的ROM许可证。
- en: 'With that, you’re ready to use OpenAI Gym. Let’s import it and make an environment:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，您就可以使用OpenAI Gym了。让我们导入它并创建一个环境：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we’ve created a CartPole environment. This is a 2D simulation in which
    a cart can be accelerated left or right in order to balance a pole placed on top
    of it (see [Figure 18-4](#cart_pole_diagram)). This is a classic control task.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个CartPole环境。这是一个2D模拟，其中一个小车可以被加速向左或向右，以平衡放在其顶部的杆（参见[图18-4](#cart_pole_diagram)）。这是一个经典的控制任务。
- en: Tip
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The `gym.envs.registry` dictionary contains the names and specifications of
    all the available environments.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`gym.envs.registry`字典包含所有可用环境的名称和规格。'
- en: '![mls3 1804](assets/mls3_1804.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1804](assets/mls3_1804.png)'
- en: Figure 18-4\. The CartPole environment
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-4。CartPole环境
- en: After the environment is created, you must initialize it using the `reset()`
    method, optionally specifying a random seed. This returns the first observation.
    Observations depend on the type of environment. For the CartPole environment,
    each observation is a 1D NumPy array containing four floats representing the cart’s
    horizontal position (`0.0` = center), its velocity (positive means right), the
    angle of the pole (`0.0` = vertical), and its angular velocity (positive means
    clockwise). The `reset()` method also returns a dictionary that may contain extra
    environment-specific information. This can be useful for debugging or for training.
    For example, in many Atari environments, it contains the number of lives left.
    However, in the CartPole environment, this dictionary is empty.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建环境之后，您必须使用`reset()`方法对其进行初始化，可以选择性地指定一个随机种子。这将返回第一个观察结果。观察结果取决于环境的类型。对于CartPole环境，每个观察结果都是一个包含四个浮点数的1D
    NumPy数组，表示小车的水平位置（`0.0` = 中心），其速度（正数表示向右），杆的角度（`0.0` = 垂直），以及其角速度（正数表示顺时针）。`reset()`方法还返回一个可能包含额外环境特定信息的字典。这对于调试或训练可能很有用。例如，在许多Atari环境中，它包含剩余的生命次数。然而，在CartPole环境中，这个字典是空的。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s call the `render()` method to render this environment as an image. Since
    we set `render_mode="rgb_array"` when creating the environment, the image will
    be returned as a NumPy array:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调用`render()`方法将这个环境渲染为图像。由于在创建环境时设置了`render_mode="rgb_array"`，图像将作为一个NumPy数组返回：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can then use Matplotlib’s `imshow()` function to display this image, as
    usual.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用Matplotlib的`imshow()`函数来显示这个图像，就像往常一样。
- en: 'Now let’s ask the environment what actions are possible:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们询问环境有哪些可能的动作：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Discrete(2)` means that the possible actions are integers 0 and 1, which represent
    accelerating left or right. Other environments may have additional discrete actions,
    or other kinds of actions (e.g., continuous). Since the pole is leaning toward
    the right (`obs[2] > 0`), let’s accelerate the cart toward the right:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`Discrete(2)`表示可能的动作是整数0和1，分别代表向左或向右加速。其他环境可能有额外的离散动作，或其他类型的动作（例如连续动作）。由于杆向右倾斜（`obs[2]
    > 0`），让我们加速小车向右：'
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `step()` method executes the desired action and returns five values:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`step()` 方法执行所需的动作并返回五个值：'
- en: '`obs`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`obs`'
- en: This is the new observation. The cart is now moving toward the right (`obs[1]
    > 0`). The pole is still tilted toward the right (`obs[2] > 0`), but its angular
    velocity is now negative (`obs[3] < 0`), so it will likely be tilted toward the
    left after the next step.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是新的观察。小车现在向右移动（`obs[1] > 0`）。杆仍然向右倾斜（`obs[2] > 0`），但它的角速度现在是负的（`obs[3] < 0`），所以在下一步之后它可能会向左倾斜。
- en: '`reward`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`reward`'
- en: In this environment, you get a reward of 1.0 at every step, no matter what you
    do, so the goal is to keep the episode running for as long as possible.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，无论你做什么，每一步都会获得1.0的奖励，所以目标是尽可能让情节运行更长时间。
- en: '`done`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`done`'
- en: This value will be `True` when the episode is over. This will happen when the
    pole tilts too much, or goes off the screen, or after 200 steps (in this last
    case, you have won). After that, the environment must be reset before it can be
    used again.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当情节结束时，这个值将是`True`。当杆倾斜得太多，或者离开屏幕，或者经过200步后（在这种情况下，你赢了），情节就会结束。之后，环境必须被重置才能再次使用。
- en: '`truncated`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`truncated`'
- en: This value will be `True` when an episode is interrupted early, for example
    by an environment wrapper that imposes a maximum number of steps per episode (see
    Gym’s documentation for more details on environment wrappers). Some RL algorithms
    treat truncated episodes differently from episodes finished normally (i.e., when
    `done` is `True`), but in this chapter we will treat them identically.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个情节被提前中断时，这个值将是`True`，例如通过一个强加每个情节最大步数的环境包装器（请参阅Gym的文档以获取有关环境包装器的更多详细信息）。一些强化学习算法会将截断的情节与正常结束的情节（即`done`为`True`时）区别对待，但在本章中，我们将对它们进行相同处理。
- en: '`info`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`info`'
- en: This environment-specific dictionary may provide extra information, just like
    the one returned by the `reset()` method.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定于环境的字典可能提供额外的信息，就像`reset()`方法返回的那样。
- en: Tip
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Once you have finished using an environment, you should call its `close()` method
    to free resources.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用完一个环境后，应该调用它的`close()`方法来释放资源。
- en: 'Let’s hardcode a simple policy that accelerates left when the pole is leaning
    toward the left and accelerates right when the pole is leaning toward the right.
    We will run this policy to see the average rewards it gets over 500 episodes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们硬编码一个简单的策略，当杆向左倾斜时加速向左，当杆向右倾斜时加速向右。我们将运行此策略，以查看它在500个情节中获得的平均奖励：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This code is self-explanatory. Let’s look at the result:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是不言自明的。让我们看看结果：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Even with 500 tries, this policy never managed to keep the pole upright for
    more than 63 consecutive steps. Not great. If you look at the simulation in this
    chapter’s notebook, you will see that the cart oscillates left and right more
    and more strongly until the pole tilts too much. Let’s see if a neural network
    can come up with a better policy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 即使尝试了500次，这个策略也从未成功让杆连续保持直立超过63步。不太好。如果你看一下本章笔记本中的模拟，你会看到小车左右摆动得越来越强烈，直到杆倾斜得太多。让我们看看神经网络是否能提出一个更好的策略。
- en: Neural Network Policies
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络策略
- en: Let’s create a neural network policy. This neural network will take an observation
    as input, and it will output the action to be executed, just like the policy we
    hardcoded earlier. More precisely, it will estimate a probability for each action,
    and then we will select an action randomly, according to the estimated probabilities
    (see [Figure 18-5](#neural_network_policy_diagram)). In the case of the CartPole
    environment, there are just two possible actions (left or right), so we only need
    one output neuron. It will output the probability *p* of action 0 (left), and
    of course the probability of action 1 (right) will be 1 – *p*. For example, if
    it outputs 0.7, then we will pick action 0 with 70% probability, or action 1 with
    30% probability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个神经网络策略。这个神经网络将以观察作为输入，并输出要执行的动作，就像我们之前硬编码的策略一样。更准确地说，它将为每个动作估计一个概率，然后我们将根据估计的概率随机选择一个动作（参见[图18-5](#neural_network_policy_diagram)）。在CartPole环境中，只有两种可能的动作（左或右），所以我们只需要一个输出神经元。它将输出动作0（左）的概率*p*，当然动作1（右）的概率将是1
    - *p*。例如，如果它输出0.7，那么我们将以70%的概率选择动作0，或者以30%的概率选择动作1。
- en: '![mls3 1805](assets/mls3_1805.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1805](assets/mls3_1805.png)'
- en: Figure 18-5\. Neural network policy
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-5\. 神经网络策略
- en: 'You may wonder why we are picking a random action based on the probabilities
    given by the neural network, rather than just picking the action with the highest
    score. This approach lets the agent find the right balance between *exploring*
    new actions and *exploiting* the actions that are known to work well. Here’s an
    analogy: suppose you go to a restaurant for the first time, and all the dishes
    look equally appealing, so you randomly pick one. If it turns out to be good,
    you can increase the probability that you’ll order it next time, but you shouldn’t
    increase that probability up to 100%, or else you will never try out the other
    dishes, some of which may be even better than the one you tried. This *exploration*/*exploitation
    dilemma* is central in reinforcement learning.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想为什么我们根据神经网络给出的概率随机选择一个动作，而不是只选择得分最高的动作。这种方法让代理人在*探索*新动作和*利用*已知效果良好的动作之间找到平衡。这里有一个类比：假设你第一次去一家餐馆，所有菜看起来都一样吸引人，所以你随机挑选了一个。如果它很好吃，你可以增加下次点它的概率，但你不应该将这个概率增加到100%，否则你永远不会尝试其他菜，其中一些可能比你尝试的这个更好。这个*探索*/*利用*的困境在强化学习中是核心的。
- en: Also note that in this particular environment, the past actions and observations
    can safely be ignored, since each observation contains the environment’s full
    state. If there were some hidden state, then you might need to consider past actions
    and observations as well. For example, if the environment only revealed the position
    of the cart but not its velocity, you would have to consider not only the current
    observation but also the previous observation in order to estimate the current
    velocity. Another example is when the observations are noisy; in that case, you
    generally want to use the past few observations to estimate the most likely current
    state. The CartPole problem is thus as simple as can be; the observations are
    noise-free, and they contain the environment’s full state.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在这种特定环境中，过去的动作和观察可以安全地被忽略，因为每个观察包含了环境的完整状态。如果有一些隐藏状态，那么您可能需要考虑过去的动作和观察。例如，如果环境只透露了小车的位置而没有速度，那么您不仅需要考虑当前观察，还需要考虑上一个观察以估计当前速度。另一个例子是当观察是嘈杂的；在这种情况下，通常希望使用过去几个观察来估计最可能的当前状态。因此，CartPole问题非常简单；观察是无噪声的，并且包含了环境的完整状态。
- en: 'Here is the code to build a basic neural network policy using Keras:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用Keras构建基本神经网络策略的代码：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We use a `Sequential` model to define the policy network. The number of inputs
    is the size of the observation space—which in the case of CartPole is 4—and we
    have just five hidden units because it’s a fairly simple task. Finally, we want
    to output a single probability—the probability of going left—so we have a single
    output neuron using the sigmoid activation function. If there were more than two
    possible actions, there would be one output neuron per action, and we would use
    the softmax activation function instead.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Sequential`模型来定义策略网络。输入的数量是观察空间的大小——在CartPole的情况下是4——我们只有五个隐藏单元，因为这是一个相当简单的任务。最后，我们希望输出一个单一的概率——向左移动的概率——因此我们使用具有sigmoid激活函数的单个输出神经元。如果有超过两种可能的动作，每种动作将有一个输出神经元，并且我们将使用softmax激活函数。
- en: OK, we now have a neural network policy that will take observations and output
    action probabilities. But how do we train it?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们有一个神经网络策略，它将接收观察并输出动作概率。但是我们如何训练它呢？
- en: 'Evaluating Actions: The Credit Assignment Problem'
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估动作：信用分配问题
- en: 'If we knew what the best action was at each step, we could train the neural
    network as usual, by minimizing the cross entropy between the estimated probability
    distribution and the target probability distribution. It would just be regular
    supervised learning. However, in reinforcement learning the only guidance the
    agent gets is through rewards, and rewards are typically sparse and delayed. For
    example, if the agent manages to balance the pole for 100 steps, how can it know
    which of the 100 actions it took were good, and which of them were bad? All it
    knows is that the pole fell after the last action, but surely this last action
    is not entirely responsible. This is called the *credit assignment problem*: when
    the agent gets a reward, it is hard for it to know which actions should get credited
    (or blamed) for it. Think of a dog that gets rewarded hours after it behaved well;
    will it understand what it is being rewarded for?'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道每一步的最佳行动是什么，我们可以像平常一样训练神经网络，通过最小化估计概率分布与目标概率分布之间的交叉熵来实现，这将只是常规的监督学习。然而，在强化学习中，智能体得到的唯一指导是通过奖励，而奖励通常是稀疏和延迟的。例如，如果智能体设法在100步内平衡杆，它如何知道这100个动作中哪些是好的，哪些是坏的？它只知道在最后一个动作之后杆倒了，但肯定不是这个最后一个动作完全负责。这被称为*信用分配问题*：当智能体获得奖励时，它很难知道哪些动作应该得到赞扬（或责备）。想象一只狗表现良好几个小时后才得到奖励；它会明白为什么会得到奖励吗？
- en: To tackle this problem, a common strategy is to evaluate an action based on
    the sum of all the rewards that come after it, usually applying a *discount factor*,
    *γ* (gamma), at each step. This sum of discounted rewards is called the action’s
    *return*. Consider the example in [Figure 18-6](#discounted_rewards_diagram).
    If an agent decides to go right three times in a row and gets +10 reward after
    the first step, 0 after the second step, and finally –50 after the third step,
    then assuming we use a discount factor *γ* = 0.8, the first action will have a
    return of 10 + *γ* × 0 + *γ*² × (–50) = –22\. If the discount factor is close
    to 0, then future rewards won’t count for much compared to immediate rewards.
    Conversely, if the discount factor is close to 1, then rewards far into the future
    will count almost as much as immediate rewards. Typical discount factors vary
    from 0.9 to 0.99\. With a discount factor of 0.95, rewards 13 steps into the future
    count roughly for half as much as immediate rewards (since 0.95^(13) ≈ 0.5), while
    with a discount factor of 0.99, rewards 69 steps into the future count for half
    as much as immediate rewards. In the CartPole environment, actions have fairly
    short-term effects, so choosing a discount factor of 0.95 seems reasonable.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，一个常见的策略是基于之后所有奖励的总和来评估一个动作，通常在每一步应用一个*折扣因子*，*γ*（gamma）。这些折扣后的奖励之和被称为动作的*回报*。考虑[图18-6](#discounted_rewards_diagram)中的例子。如果一个智能体连续三次向右移动，并在第一步后获得+10奖励，在第二步后获得0奖励，最后在第三步后获得-50奖励，那么假设我们使用一个折扣因子*γ*=0.8，第一个动作的回报将是10
    + *γ* × 0 + *γ*² × (–50) = –22。如果折扣因子接近0，那么未来的奖励与即时奖励相比不会占据很大比重。相反，如果折扣因子接近1，那么未来的奖励将几乎和即时奖励一样重要。典型的折扣因子从0.9到0.99不等。使用折扣因子0.95，未来13步的奖励大约相当于即时奖励的一半（因为0.95^(13)
    ≈ 0.5），而使用折扣因子0.99，未来69步的奖励相当于即时奖励的一半。在CartPole环境中，动作具有相当短期的影响，因此选择折扣因子0.95似乎是合理的。
- en: '![mls3 1806](assets/mls3_1806.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1806](assets/mls3_1806.png)'
- en: 'Figure 18-6\. Computing an action’s return: the sum of discounted future rewards'
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-6。计算动作的回报：折扣未来奖励之和
- en: Of course, a good action may be followed by several bad actions that cause the
    pole to fall quickly, resulting in the good action getting a low return. Similarly,
    a good actor may sometimes star in a terrible movie. However, if we play the game
    enough times, on average good actions will get a higher return than bad ones.
    We want to estimate how much better or worse an action is, compared to the other
    possible actions, on average. This is called the *action advantage*. For this,
    we must run many episodes and normalize all the action returns, by subtracting
    the mean and dividing by the standard deviation. After that, we can reasonably
    assume that actions with a negative advantage were bad while actions with a positive
    advantage were good. OK, now that we have a way to evaluate each action, we are
    ready to train our first agent using policy gradients. Let’s see how.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一个好的行动可能会被几个导致杆迅速倒下的坏行动跟随，导致好的行动获得较低的回报。同样，一个好的演员有时可能会出演一部糟糕的电影。然而，如果我们玩足够多次游戏，平均而言好的行动将获得比坏行动更高的回报。我们想要估计一个行动相对于其他可能行动的平均优势有多大。这被称为*行动优势*。为此，我们必须运行许多情节，并通过减去均值并除以标准差来标准化所有行动回报。之后，我们可以合理地假设具有负优势的行动是坏的，而具有正优势的行动是好的。现在我们有了一种评估每个行动的方法，我们准备使用策略梯度来训练我们的第一个代理。让我们看看如何。
- en: Policy Gradients
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度
- en: 'As discussed earlier, PG algorithms optimize the parameters of a policy by
    following the gradients toward higher rewards. One popular class of PG algorithms,
    called *REINFORCE algorithms*, was [introduced back in 1992](https://homl.info/132)⁠^([11](ch18.html#idm45720165711216))
    by Ronald Williams. Here is one common variant:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面讨论的，PG算法通过沿着梯度朝着更高奖励的方向优化策略的参数。一种流行的PG算法类别称为*REINFORCE算法*，由Ronald Williams于1992年提出。这里是一个常见的变体：
- en: First, let the neural network policy play the game several times, and at each
    step, compute the gradients that would make the chosen action even more likely—but
    don’t apply these gradients yet.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让神经网络策略玩游戏多次，并在每一步计算使选择的行动更有可能的梯度，但暂时不应用这些梯度。
- en: Once you have run several episodes, compute each action’s advantage, using the
    method described in the previous section.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行了几个情节之后，使用前一节中描述的方法计算每个行动的优势。
- en: If an action’s advantage is positive, it means that the action was probably
    good, and you want to apply the gradients computed earlier to make the action
    even more likely to be chosen in the future. However, if the action’s advantage
    is negative, it means the action was probably bad, and you want to apply the opposite
    gradients to make this action slightly *less* likely in the future. The solution
    is to multiply each gradient vector by the corresponding action’s advantage.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个行动的优势是正的，这意味着这个行动可能是好的，你希望应用之前计算的梯度，使这个行动在未来更有可能被选择。然而，如果一个行动的优势是负的，这意味着这个行动可能是坏的，你希望应用相反的梯度，使这个行动在未来略微减少。解决方案是将每个梯度向量乘以相应行动的优势。
- en: Finally, compute the mean of all the resulting gradient vectors, and use it
    to perform a gradient descent step.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，计算所有结果梯度向量的平均值，并用它执行一步梯度下降。
- en: 'Let’s use Keras to implement this algorithm. We will train the neural network
    policy we built earlier so that it learns to balance the pole on the cart. First,
    we need a function that will play one step. We will pretend for now that whatever
    action it takes is the right one so that we can compute the loss and its gradients.
    These gradients will just be saved for a while, and we will modify them later
    depending on how good or bad the action turned out to be:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Keras来实现这个算法。我们将训练之前构建的神经网络策略，使其学会在小车上平衡杆。首先，我们需要一个函数来执行一步。我们暂时假设无论采取什么行动都是正确的，以便我们可以计算损失及其梯度。这些梯度将暂时保存一段时间，我们稍后会根据行动的好坏来修改它们：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s walk though this function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个函数：
- en: Within the `GradientTape` block (see [Chapter 12](ch12.html#tensorflow_chapter)),
    we start by calling the model, giving it a single observation. We reshape the
    observation so it becomes a batch containing a single instance, as the model expects
    a batch. This outputs the probability of going left.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`GradientTape`块中（参见[第12章](ch12.html#tensorflow_chapter)），我们首先调用模型，给它一个观察值。我们将观察值重塑为包含单个实例的批次，因为模型期望一个批次。这将输出向左移动的概率。
- en: Next, we sample a random float between 0 and 1, and we check whether it is greater
    than `left_proba`. The `action` will be `False` with probability `left_proba`,
    or `True` with probability `1 – left_proba`. Once we cast this Boolean to an integer,
    the action will be 0 (left) or 1 (right) with the appropriate probabilities.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们随机抽取一个介于0和1之间的浮点数，并检查它是否大于`left_proba`。`action`将以`left_proba`的概率为`False`，或以`1
    - left_proba`的概率为`True`。一旦我们将这个布尔值转换为整数，行动将以适当的概率为0（左）或1（右）。
- en: 'We now define the target probability of going left: it is 1 minus the action
    (cast to a float). If the action is 0 (left), then the target probability of going
    left will be 1\. If the action is 1 (right), then the target probability will
    be 0.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们定义向左移动的目标概率：它是1减去行动（转换为浮点数）。如果行动是0（左），那么向左移动的目标概率将是1。如果行动是1（右），那么目标概率将是0。
- en: Then we compute the loss using the given loss function, and we use the tape
    to compute the gradient of the loss with regard to the model’s trainable variables.
    Again, these gradients will be tweaked later, before we apply them, depending
    on how good or bad the action turned out to be.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们使用给定的损失函数计算损失，并使用tape计算损失相对于模型可训练变量的梯度。同样，这些梯度稍后会在应用之前进行调整，取决于行动的好坏。
- en: Finally, we play the selected action, and we return the new observation, the
    reward, whether the episode is ended or not, whether it is truncated or not, and
    of course the gradients that we just computed.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们执行选择的行动，并返回新的观察值、奖励、该情节是否结束、是否截断，当然还有我们刚刚计算的梯度。
- en: 'Now let’s create another function that will rely on the `play_one_step()` function
    to play multiple episodes, returning all the rewards and gradients for each episode
    and each step:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建另一个函数，它将依赖于`play_one_step()`函数来玩多个回合，返回每个回合和每个步骤的所有奖励和梯度：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This code returns a list of reward lists: one reward list per episode, containing
    one reward per step. It also returns a list of gradient lists: one gradient list
    per episode, each containing one tuple of gradients per step and each tuple containing
    one gradient tensor per trainable variable.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码返回了一个奖励列表的列表：每个回合一个奖励列表，每个步骤一个奖励。它还返回了一个梯度列表的列表：每个回合一个梯度列表，每个梯度列表包含每个步骤的一个梯度元组，每个元组包含每个可训练变量的一个梯度张量。
- en: 'The algorithm will use the `play_multiple_episodes()` function to play the
    game several times (e.g., 10 times), then it will go back and look at all the
    rewards, discount them, and normalize them. To do that, we need a couple more
    functions; the first will compute the sum of future discounted rewards at each
    step, and the second will normalize all these discounted rewards (i.e., the returns)
    across many episodes by subtracting the mean and dividing by the standard deviation:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法将使用`play_multiple_episodes()`函数多次玩游戏（例如，10次），然后它将回头查看所有奖励，对其进行折扣，并对其进行归一化。为此，我们需要几个额外的函数；第一个将计算每个步骤的未来折扣奖励总和，第二个将通过减去均值并除以标准差来对所有这些折扣奖励（即回报）在许多回合中进行归一化：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s check that this works:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下这是否有效：
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The call to `discount_rewards()` returns exactly what we expect (see [Figure 18-6](#discounted_rewards_diagram)).
    You can verify that the function `discount_and_normalize_rewards()` does indeed
    return the normalized action advantages for each action in both episodes. Notice
    that the first episode was much worse than the second, so its normalized advantages
    are all negative; all actions from the first episode would be considered bad,
    and conversely all actions from the second episode would be considered good.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`discount_rewards()`返回了我们预期的结果（见[图18-6](#discounted_rewards_diagram)）。您可以验证函数`discount_and_normalize_rewards()`确实返回了两个回合中每个动作的归一化优势。请注意，第一个回合比第二个回合差得多，因此它的归一化优势都是负数；第一个回合的所有动作都被认为是不好的，反之第二个回合的所有动作都被认为是好的。
- en: 'We are almost ready to run the algorithm! Now let’s define the hyperparameters.
    We will run 150 training iterations, playing 10 episodes per iteration, and each
    episode will last at most 200 steps. We will use a discount factor of 0.95:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好运行算法了！现在让我们定义超参数。我们将运行150次训练迭代，每次迭代玩10个回合，每个回合最多持续200步。我们将使用折扣因子0.95：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We also need an optimizer and the loss function. A regular Nadam optimizer
    with learning rate 0.01 will do just fine, and we will use the binary cross-entropy
    loss function because we are training a binary classifier (there are two possible
    actions—left or right):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个优化器和损失函数。一个常规的Nadam优化器，学习率为0.01，将会很好地完成任务，我们将使用二元交叉熵损失函数，因为我们正在训练一个二元分类器（有两种可能的动作——左或右）：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We are now ready to build and run the training loop!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备构建和运行训练循环！
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s walk through this code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步走过这段代码：
- en: At each training iteration, this loop calls the `play_multiple_episodes()` function,
    which plays 10 episodes and returns the rewards and gradients for each step in
    each episode.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次训练迭代中，此循环调用`play_multiple_episodes()`函数，该函数播放10个回合，并返回每个步骤中每个回合的奖励和梯度。
- en: Then we call the `discount_and_normalize_rewards()` function to compute each
    action’s normalized advantage, called the `final_reward` in this code. This provides
    a measure of how good or bad each action actually was, in hindsight.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们调用`discount_and_normalize_rewards()`函数来计算每个动作的归一化优势，这在这段代码中称为`final_reward`。这提供了一个衡量每个动作实际上是好还是坏的指标。
- en: Next, we go through each trainable variable, and for each of them we compute
    the weighted mean of the gradients for that variable over all episodes and all
    steps, weighted by the `final_reward`.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们遍历每个可训练变量，并对每个变量计算所有回合和所有步骤中该变量的梯度的加权平均，权重为`final_reward`。
- en: 'Finally, we apply these mean gradients using the optimizer: the model’s trainable
    variables will be tweaked, and hopefully the policy will be a bit better.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用优化器应用这些均值梯度：模型的可训练变量将被微调，希望策略会有所改善。
- en: And we’re done! This code will train the neural network policy, and it will
    successfully learn to balance the pole on the cart. The mean reward per episode
    will get very close to 200\. By default, that’s the maximum for this environment.
    Success!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了！这段代码将训练神经网络策略，并成功学会在小车上平衡杆。每个回合的平均奖励将非常接近200。默认情况下，这是该环境的最大值。成功！
- en: The simple policy gradients algorithm we just trained solved the CartPole task,
    but it would not scale well to larger and more complex tasks. Indeed, it is highly
    *sample inefficient*, meaning it needs to explore the game for a very long time
    before it can make significant progress. This is due to the fact that it must
    run multiple episodes to estimate the advantage of each action, as we have seen.
    However, it is the foundation of more powerful algorithms, such as *actor-critic*
    algorithms (which we will discuss briefly at the end of this chapter).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚训练的简单策略梯度算法解决了CartPole任务，但是它在扩展到更大更复杂的任务时效果不佳。事实上，它具有很高的*样本效率低*，这意味着它需要很长时间探索游戏才能取得显著进展。这是因为它必须运行多个回合来估计每个动作的优势，正如我们所见。然而，它是更强大算法的基础，比如*演员-评论家*算法（我们将在本章末简要讨论）。
- en: Tip
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Researchers try to find algorithms that work well even when the agent initially
    knows nothing about the environment. However, unless you are writing a paper,
    you should not hesitate to inject prior knowledge into the agent, as it will speed
    up training dramatically. For example, since you know that the pole should be
    as vertical as possible, you could add negative rewards proportional to the pole’s
    angle. This will make the rewards much less sparse and speed up training. Also,
    if you already have a reasonably good policy (e.g., hardcoded), you may want to
    train the neural network to imitate it before using policy gradients to improve
    it.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员试图找到即使代理最初对环境一无所知也能很好运行的算法。然而，除非你在写论文，否则不应该犹豫向代理注入先验知识，因为这将极大加快训练速度。例如，由于你知道杆应该尽可能垂直，你可以添加与杆角度成比例的负奖励。这将使奖励变得不那么稀疏，加快训练速度。此外，如果你已经有一个相当不错的策略（例如硬编码），你可能希望在使用策略梯度来改进之前，训练神经网络来模仿它。
- en: 'We will now look at another popular family of algorithms. Whereas PG algorithms
    directly try to optimize the policy to increase rewards, the algorithms we will
    explore now are less direct: the agent learns to estimate the expected return
    for each state, or for each action in each state, then it uses this knowledge
    to decide how to act. To understand these algorithms, we must first consider *Markov
    decision processes* (MDPs).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看一下另一个流行的算法家族。PG算法直接尝试优化策略以增加奖励，而我们现在要探索的算法则不那么直接：代理学习估计每个状态的预期回报，或者每个状态中每个动作的预期回报，然后利用这些知识来决定如何行动。要理解这些算法，我们首先必须考虑*马尔可夫决策过程*（MDPs）。
- en: Markov Decision Processes
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: In the early 20th century, the mathematician Andrey Markov studied stochastic
    processes with no memory, called *Markov chains*. Such a process has a fixed number
    of states, and it randomly evolves from one state to another at each step. The
    probability for it to evolve from a state *s* to a state *s*′ is fixed, and it
    depends only on the pair (*s*, *s*′), not on past states. This is why we say that
    the system has no memory.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 20世纪初，数学家安德烈·马尔可夫研究了没有记忆的随机过程，称为*马尔可夫链*。这样的过程具有固定数量的状态，并且在每一步中随机从一个状态演变到另一个状态。它从状态*s*演变到状态*s*′的概率是固定的，仅取决于对(*s*,
    *s*′)这一对，而不取决于过去的状态。这就是为什么我们说该系统没有记忆。
- en: '[Figure 18-7](#markov_chain_diagram) shows an example of a Markov chain with
    four states.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-7](#markov_chain_diagram)显示了一个具有四个状态的马尔可夫链的示例。'
- en: '![mls3 1807](assets/mls3_1807.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1807](assets/mls3_1807.png)'
- en: Figure 18-7\. Example of a Markov chain
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-7. 马尔可夫链示例
- en: 'Suppose that the process starts in state *s*[0], and there is a 70% chance
    that it will remain in that state at the next step. Eventually it is bound to
    leave that state and never come back, because no other state points back to *s*[0].
    If it goes to state *s*[1], it will then most likely go to state *s*[2] (90% probability),
    then immediately back to state *s*[1] (with 100% probability). It may alternate
    a number of times between these two states, but eventually it will fall into state
    *s*[3] and remain there forever, since there’s no way out: this is called a *terminal
    state*. Markov chains can have very different dynamics, and they are heavily used
    in thermodynamics, chemistry, statistics, and much more.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 假设过程从状态*s*[0]开始，并且有70%的概率在下一步保持在该状态。最终，它必定会离开该状态并永远不会回来，因为没有其他状态指向*s*[0]。如果它进入状态*s*[1]，那么它很可能会进入状态*s*[2]（90%的概率），然后立即返回到状态*s*[1]（100%的概率）。它可能在这两个状态之间交替多次，但最终会陷入状态*s*[3]并永远留在那里，因为没有出路：这被称为*终止状态*。马尔可夫链的动态可能非常不同，并且在热力学、化学、统计学等领域被广泛使用。
- en: 'Markov decision processes were first described in the 1950s by [Richard Bellman](https://homl.info/133).⁠^([12](ch18.html#idm45720164871120))
    They resemble Markov chains, but with a twist: at each step, an agent can choose
    one of several possible actions, and the transition probabilities depend on the
    chosen action. Moreover, some state transitions return some reward (positive or
    negative), and the agent’s goal is to find a policy that will maximize reward
    over time.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程是在20世纪50年代由[理查德·贝尔曼](https://homl.info/133)首次描述的。⁠^([12](ch18.html#idm45720164871120))
    它们类似于马尔可夫链，但有一个区别：在每一步中，代理可以选择几种可能的动作之一，转移概率取决于所选择的动作。此外，一些状态转移会产生一些奖励（正面或负面），代理的目标是找到一个能够随时间最大化奖励的策略。
- en: For example, the MDP represented in [Figure 18-8](#mdp_diagram) has three states
    (represented by circles) and up to three possible discrete actions at each step
    (represented by diamonds).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，MDP在[图18-8](#mdp_diagram)中表示有三个状态（由圆圈表示），并且在每一步最多有三种可能的离散动作（由菱形表示）。
- en: '![mls3 1808](assets/mls3_1808.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1808](assets/mls3_1808.png)'
- en: Figure 18-8\. Example of a Markov decision process
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-8. 马尔可夫决策过程示例
- en: 'If it starts in state *s*[0], the agent can choose between actions *a*[0],
    *a*[1], or *a*[2]. If it chooses action *a*[1], it just remains in state *s*[0]
    with certainty, and without any reward. It can thus decide to stay there forever
    if it wants to. But if it chooses action *a*[0], it has a 70% probability of gaining
    a reward of +10 and remaining in state *s*[0]. It can then try again and again
    to gain as much reward as possible, but at one point it is going to end up instead
    in state *s*[1]. In state *s*[1] it has only two possible actions: *a*[0] or *a*[2].
    It can choose to stay put by repeatedly choosing action *a*[0], or it can choose
    to move on to state *s*[2] and get a negative reward of –50 (ouch). In state *s*[2]
    it has no choice but to take action *a*[1], which will most likely lead it back
    to state *s*[0], gaining a reward of +40 on the way. You get the picture. By looking
    at this MDP, can you guess which strategy will gain the most reward over time?
    In state *s*[0] it is clear that action *a*[0] is the best option, and in state
    *s*[2] the agent has no choice but to take action *a*[1], but in state *s*[1]
    it is not obvious whether the agent should stay put (*a*[0]) or go through the
    fire (*a*[2]).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代理从状态 *s*[0] 开始，可以在行动 *a*[0]、*a*[1] 或 *a*[2] 之间选择。如果选择行动 *a*[1]，它就会肯定留在状态
    *s*[0]，没有任何奖励。因此，如果愿意，它可以决定永远留在那里。但如果选择行动 *a*[0]，它有70%的概率获得+10的奖励并留在状态 *s*[0]。然后它可以一次又一次地尝试获得尽可能多的奖励，但最终会进入状态
    *s*[1]。在状态 *s*[1] 中，它只有两种可能的行动：*a*[0] 或 *a*[2]。它可以通过反复选择行动 *a*[0] 来保持原地，或者选择移动到状态
    *s*[2] 并获得-50的负奖励（疼）。在状态 *s*[2] 中，它别无选择，只能采取行动 *a*[1]，这很可能会将其带回状态 *s*[0]，在途中获得+40的奖励。你明白了。通过观察这个MDP，你能猜出哪种策略会随着时间获得最多的奖励吗？在状态
    *s*[0] 中，很明显行动 *a*[0] 是最佳选择，在状态 *s*[2] 中，代理别无选择，只能采取行动 *a*[1]，但在状态 *s*[1] 中，不明显代理应该保持原地（*a*[0]）还是冒险前进（*a*[2]）。
- en: Bellman found a way to estimate the *optimal state value* of any state *s*,
    noted *V**(*s*), which is the sum of all discounted future rewards the agent can
    expect on average after it reaches the state, assuming it acts optimally. He showed
    that if the agent acts optimally, then the *Bellman optimality equation* applies
    (see [Equation 18-1](#bellman_optimality_equation)). This recursive equation says
    that if the agent acts optimally, then the optimal value of the current state
    is equal to the reward it will get on average after taking one optimal action,
    plus the expected optimal value of all possible next states that this action can
    lead to.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼找到了一种估计任何状态 *s* 的*最优状态值* *V*(*s*) 的方法，这是代理在到达该状态后可以期望的所有折扣未来奖励的总和，假设它采取最优行动。他表明，如果代理采取最优行动，那么*贝尔曼最优性方程*适用（参见[方程18-1](#bellman_optimality_equation)）。这个递归方程表明，如果代理采取最优行动，那么当前状态的最优值等于在采取一个最优行动后平均获得的奖励，再加上这个行动可能导致的所有可能下一个状态的期望最优值。
- en: Equation 18-1\. Bellman optimality equation
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程18-1\. 贝尔曼最优性方程
- en: <math display="block"><mrow><msup><mi>V</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo movablelimits="true" form="prefix">max</mo>
    <mi>a</mi></munder> <munder><mo>∑</mo> <mrow><mi>s</mi><mo>'</mo></mrow></munder>
    <mrow><mi>T</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>R</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo> <mo>)</mo></mrow>
    <mo>+</mo> <mi>γ</mi> <mo>·</mo> <msup><mi>V</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow> <mtext>for</mtext>
    <mtext>all</mtext> <mi>s</mi></mrow></math>
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msup><mi>V</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo movablelimits="true" form="prefix">max</mo>
    <mi>a</mi></munder> <munder><mo>∑</mo> <mrow><mi>s</mi><mo>'</mo></mrow></munder>
    <mrow><mi>T</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>R</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo> <mo>)</mo></mrow>
    <mo>+</mo> <mi>γ</mi> <mo>·</mo> <msup><mi>V</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow> <mtext>for</mtext>
    <mtext>all</mtext> <mi>s</mi></mrow></math>
- en: 'In this equation:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*T*(*s*, *a*, *s*′) is the transition probability from state *s* to state *s*′,
    given that the agent chose action *a*. For example, in [Figure 18-8](#mdp_diagram),
    *T*(*s*[2], *a*[1], *s*[0]) = 0.8.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T*(*s*, *a*, *s*′) 是从状态 *s* 转移到状态 *s*′ 的转移概率，假设代理选择行动 *a*。例如，在[图18-8](#mdp_diagram)中，*T*(*s*[2],
    *a*[1], *s*[0]) = 0.8。'
- en: '*R*(*s*, *a*, *s*′) is the reward that the agent gets when it goes from state
    *s* to state *s*′, given that the agent chose action *a*. For example, in [Figure 18-8](#mdp_diagram),
    *R*(*s*[2], *a*[1], *s*[0]) = +40.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R*(*s*, *a*, *s*′) 是代理从状态 *s* 转移到状态 *s*′ 时获得的奖励，假设代理选择行动 *a*。例如，在[图18-8](#mdp_diagram)中，*R*(*s*[2],
    *a*[1], *s*[0]) = +40。'
- en: '*γ* is the discount factor.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*γ* 是折扣因子。'
- en: 'This equation leads directly to an algorithm that can precisely estimate the
    optimal state value of every possible state: first initialize all the state value
    estimates to zero, and then iteratively update them using the *value iteration*
    algorithm (see [Equation 18-2](#value_iteration_equation)). A remarkable result
    is that, given enough time, these estimates are guaranteed to converge to the
    optimal state values, corresponding to the optimal policy.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程直接导致了一个算法，可以精确估计每个可能状态的最优状态值：首先将所有状态值估计初始化为零，然后使用*值迭代*算法进行迭代更新（参见[方程18-2](#value_iteration_equation)）。一个显著的结果是，给定足够的时间，这些估计将收敛到最优状态值，对应于最优策略。
- en: Equation 18-2\. Value iteration algorithm
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程18-2\. 值迭代算法
- en: <math display="block"><mrow><msub><mi>V</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>←</mo> <munder><mo movablelimits="true"
    form="prefix">max</mo> <mi>a</mi></munder> <munder><mo>∑</mo> <mrow><mi>s</mi><mo>'</mo></mrow></munder>
    <mrow><mi>T</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>R</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo> <mo>)</mo></mrow>
    <mo>+</mo> <mi>γ</mi> <mo>·</mo> <msub><mi>V</mi> <mi>k</mi></msub> <mrow><mo>(</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow> <mtext>for</mtext>
    <mtext>all</mtext> <mi>s</mi></mrow></math>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>V</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>←</mo> <munder><mo movablelimits="true"
    form="prefix">max</mo> <mi>a</mi></munder> <munder><mo>∑</mo> <mrow><mi>s</mi><mo>'</mo></mrow></munder>
    <mrow><mi>T</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>R</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo> <mo>)</mo></mrow>
    <mo>+</mo> <mi>γ</mi> <mo>·</mo> <msub><mi>V</mi> <mi>k</mi></msub> <mrow><mo>(</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow> <mtext>for</mtext>
    <mtext>all</mtext> <mi>s</mi></mrow></math>
- en: In this equation, *V*[*k*](*s*) is the estimated value of state *s* at the *k*^(th)
    iteration of the algorithm.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*V*[*k*](*s*)是算法的第*k*次迭代中状态*s*的估计值。
- en: Note
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This algorithm is an example of *dynamic programming*, which breaks down a complex
    problem into tractable subproblems that can be tackled iteratively.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法是*动态规划*的一个例子，它将一个复杂的问题分解成可迭代处理的可解子问题。
- en: Knowing the optimal state values can be useful, in particular to evaluate a
    policy, but it does not give us the optimal policy for the agent. Luckily, Bellman
    found a very similar algorithm to estimate the optimal *state-action values*,
    generally called *Q-values* (quality values). The optimal Q-value of the state-action
    pair (*s*, *a*), noted *Q**(*s*, *a*), is the sum of discounted future rewards
    the agent can expect on average after it reaches the state *s* and chooses action
    *a*, but before it sees the outcome of this action, assuming it acts optimally
    after that action.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 知道最优状态值可能很有用，特别是用于评估策略，但它并不能为代理提供最优策略。幸运的是，贝尔曼找到了一个非常相似的算法来估计最优*状态-动作值*，通常称为*Q值*（质量值）。状态-动作对(*s*,
    *a*)的最优Q值，记为*Q**(*s*, *a*)，是代理在到达状态*s*并选择动作*a*后，在看到此动作结果之前，可以期望平均获得的折现未来奖励的总和，假设在此动作之后它表现最佳。
- en: Let’s look at how it works. Once again, you start by initializing all the Q-value
    estimates to zero, then you update them using the *Q-value iteration* algorithm
    (see [Equation 18-3](#q_value_iteration_equation)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何工作的。再次，您首先将所有Q值的估计初始化为零，然后使用*Q值迭代*算法进行更新（参见[方程18-3](#q_value_iteration_equation)）。
- en: Equation 18-3\. Q-value iteration algorithm
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程18-3。Q值迭代算法
- en: <math display="block"><mrow><msub><mi>Q</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>←</mo>
    <munder><mo>∑</mo> <mrow><mi>s</mi><mo>'</mo></mrow></munder> <mrow><mi>T</mi>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo>
    <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>R</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo>
    <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mo>+</mo> <mi>γ</mi>
    <mo>·</mo> <munder><mo movablelimits="true" form="prefix">max</mo> <mrow><mi>a</mi><mo>'</mo></mrow></munder>
    <mrow><msub><mi>Q</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>s</mi><mo>'</mo>
    <mo>,</mo> <mi>a</mi><mo>'</mo> <mo>)</mo></mrow></mrow> <mo>]</mo></mrow></mrow>
    <mtext>for</mtext> <mtext>all</mtext> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>Q</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>←</mo>
    <munder><mo>∑</mo> <mrow><mi>s</mi><mo>'</mo></mrow></munder> <mrow><mi>T</mi>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo>
    <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>R</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo>
    <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mo>+</mo> <mi>γ</mi>
    <mo>·</mo> <munder><mo movablelimits="true" form="prefix">max</mo> <mrow><mi>a</mi><mo>'</mo></mrow></munder>
    <mrow><msub><mi>Q</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>s</mi><mo>'</mo>
    <mo>,</mo> <mi>a</mi><mo>'</mo> <mo>)</mo></mrow></mrow> <mo>]</mo></mrow></mrow>
    <mtext>for</mtext> <mtext>all</mtext> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi>
    <mo>)</mo></mrow></mrow></math>
- en: 'Once you have the optimal Q-values, defining the optimal policy, noted *π**(*s*),
    is trivial; when the agent is in state *s*, it should choose the action with the
    highest Q-value for that state: <math><msup><mi>π</mi><mo>*</mo></msup><mo>(</mo><mi>s</mi><mo>)</mo><mo>=</mo><munder><mo>argmax</mo><mi>a</mi></munder><msup><mi>Q</mi><mo>*</mo></msup><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></math>.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了最优的Q值，定义最优策略*π**(*s*)是微不足道的；当代理处于状态*s*时，它应该选择具有该状态最高Q值的动作：<math><msup><mi>π</mi><mo>*</mo></msup><mo>(</mo><mi>s</mi><mo>)</mo><mo>=</mo><munder><mo>argmax</mo><mi>a</mi></munder><msup><mi>Q</mi><mo>*</mo></msup><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></math>。
- en: 'Let’s apply this algorithm to the MDP represented in [Figure 18-8](#mdp_diagram).
    First, we need to define the MDP:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个算法应用到[图18-8](#mdp_diagram)中表示的MDP中。首先，我们需要定义MDP：
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For example, to know the transition probability of going from *s*[2] to *s*[0]
    after playing action *a*[1], we will look up `transition_probabilities[2][1][0]`
    (which is 0.8). Similarly, to get the corresponding reward, we will look up `rewards[2][1][0]`
    (which is +40). And to get the list of possible actions in *s*[2], we will look
    up `possible_actions[2]` (in this case, only action *a*[1] is possible). Next,
    we must initialize all the Q-values to zero (except for the impossible actions,
    for which we set the Q-values to –∞):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要知道在执行动作*a*[1]后从*s*[2]到*s*[0]的转移概率，我们将查找`transition_probabilities[2][1][0]`（为0.8）。类似地，要获得相应的奖励，我们将查找`rewards[2][1][0]`（为+40）。要获取*s*[2]中可能的动作列表，我们将查找`possible_actions[2]`（在这种情况下，只有动作*a*[1]是可能的）。接下来，我们必须将所有Q值初始化为零（对于不可能的动作，我们将Q值设置为-∞）：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now let’s run the Q-value iteration algorithm. It applies [Equation 18-3](#q_value_iteration_equation)
    repeatedly, to all Q-values, for every state and every possible action:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行Q值迭代算法。它重复应用[方程18-3](#q_value_iteration_equation)，对每个状态和每个可能的动作的所有Q值进行计算：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'That’s it! The resulting Q-values look like this:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！得到的Q值看起来像这样：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For example, when the agent is in state *s*[0] and it chooses action *a*[1],
    the expected sum of discounted future rewards is approximately 17.0.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当代理处于状态*s*[0]并选择动作*a*[1]时，预期的折现未来奖励总和约为17.0。
- en: 'For each state, we can find the action that has the highest Q-value:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个状态，我们可以找到具有最高Q值的动作：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This gives us the optimal policy for this MDP when using a discount factor
    of 0.90: in state *s*[0] choose action *a*[0], in state *s*[1] choose action *a*[0]
    (i.e., stay put), and in state *s*[2] choose action *a*[1] (the only possible
    action). Interestingly, if we increase the discount factor to 0.95, the optimal
    policy changes: in state *s*[1] the best action becomes *a*[2] (go through the
    fire!). This makes sense because the more you value future rewards, the more you
    are willing to put up with some pain now for the promise of future bliss.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了在使用折扣因子为0.90时这个MDP的最优策略：在状态*s*[0]选择动作*a*[0]，在状态*s*[1]选择动作*a*[0]（即保持不动），在状态*s*[2]选择动作*a*[1]（唯一可能的动作）。有趣的是，如果将折扣因子增加到0.95，最优策略会改变：在状态*s*[1]中，最佳动作变为*a*[2]（冲过火！）。这是有道理的，因为你越重视未来的奖励，你就越愿意忍受现在的一些痛苦，以换取未来的幸福。
- en: Temporal Difference Learning
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间差异学习
- en: Reinforcement learning problems with discrete actions can often be modeled as
    Markov decision processes, but the agent initially has no idea what the transition
    probabilities are (it does not know *T*(*s*, *a*, *s*′)), and it does not know
    what the rewards are going to be either (it does not know *R*(*s*, *a*, *s*′)).
    It must experience each state and each transition at least once to know the rewards,
    and it must experience them multiple times if it is to have a reasonable estimate
    of the transition probabilities.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 具有离散动作的强化学习问题通常可以建模为马尔可夫决策过程，但代理最初不知道转移概率是多少（它不知道*T*(*s*, *a*, *s*′)），也不知道奖励将会是什么（它不知道*R*(*s*,
    *a*, *s*′)）。它必须至少体验每个状态和每个转换一次才能知道奖励，如果要对转移概率有合理的估计，它必须多次体验它们。
- en: The *temporal difference (TD) learning* algorithm is very similar to the Q-value
    iteration algorithm, but tweaked to take into account the fact that the agent
    has only partial knowledge of the MDP. In general we assume that the agent initially
    knows only the possible states and actions, and nothing more. The agent uses an
    *exploration policy*—for example, a purely random policy—to explore the MDP, and
    as it progresses, the TD learning algorithm updates the estimates of the state
    values based on the transitions and rewards that are actually observed (see [Equation
    18-4](#td_learning_equation)).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*时间差异（TD）学习*算法与Q值迭代算法非常相似，但经过调整以考虑代理只有对MDP的部分知识这一事实。通常我们假设代理最初只知道可能的状态和动作，什么也不知道。代理使用一个*探索策略*——例如，一个纯随机策略——来探索MDP，随着探索的进行，TD学习算法根据实际观察到的转换和奖励更新状态值的估计（参见[方程18-4](#td_learning_equation)）。'
- en: Equation 18-4\. TD learning algorithm
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程18-4. TD学习算法
- en: <math display="block"><msub><mi>V</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>(</mo><mi>s</mi><mo>)</mo><mo>←</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo>)</mo>
    <msub><mi>V</mi><mi>k</mi></msub> <mo>(</mo><mi>s</mi><mo>)</mo><mo>+</mo><mi>α</mi>
    <mfenced><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo> <msub><mi>V</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>'</mo><mo>)</mo></mrow></mfenced> <mtext>or, equivalently: </mtext>
    <msub><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>(</mo><mi>s</mi><mo>)</mo><mo>←</mo><msub><mi>V</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>)</mo><mo>+</mo><mi>α</mi><mo>·</mo> <msub><mi>δ</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo>,</mo> <mi>s</mi><mo>'</mo><mo>)</mo>
    <mtext>with </mtext><msub><mi>δ</mi><mi>k</mi></msub> <mo>(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>s</mi><mo>′</mo>
    <mo>)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi> <mo>·</mo> <msub><mi>V</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>'</mo><mo>)</mo><mo>-</mo> <msub><mi>V</mi><mi>k</mi></msub><mo>(</mo><mi>s</mi><mo>)</mo></math>
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><msub><mi>V</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>(</mo><mi>s</mi><mo>)</mo><mo>←</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo>)</mo>
    <msub><mi>V</mi><mi>k</mi></msub> <mo>(</mo><mi>s</mi><mo>)</mo><mo>+</mo><mi>α</mi>
    <mfenced><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo> <msub><mi>V</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>'</mo><mo>)</mo></mrow></mfenced> <mtext>或者，等价地：</mtext>
    <msub><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>(</mo><mi>s</mi><mo>)</mo><mo>←</mo><msub><mi>V</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>)</mo><mo>+</mo><mi>α</mi><mo>·</mo> <msub><mi>δ</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo>,</mo> <mi>s</mo>'</mo><mo>)</mo>
    <mtext>其中</mtext><msub><mi>δ</mi><mi>k</mi></msub> <mo>(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>s</mi><mo>′</mo>
    <mo>)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi> <mo>·</mo> <msub><mi>V</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>'</mo><mo>)</mo><mo>-</mo> <msub><mi>V</mi><mi>k</mi></msub><mo>(</mo><mi>s</mi><mo>)</mo></math>
- en: 'In this equation:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*α* is the learning rate (e.g., 0.01).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*α*是学习率（例如，0.01）。'
- en: '*r* + *γ* · *V*[*k*](*s*′) is called the *TD target*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r* + *γ* · *V*[*k*](*s*′)被称为*TD目标*。'
- en: '*δ*[k](*s*, *r*, *s*′) is called the *TD error*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*δ*[k](*s*, *r*, *s*′)被称为*TD误差*。'
- en: 'A more concise way of writing the first form of this equation is to use the
    notation <math><mi>a</mi><munder><mo>←</mo><mi>α</mi></munder><mi>b</mi></math>,
    which means *a*[*k*+1] ← (1 – *α*) · *a*[*k*] + *α* ·*b*[*k*]. So, the first line
    of [Equation 18-4](#td_learning_equation) can be rewritten like this: <math><mi>V</mi><mo>(</mo><mi>s</mi><mo>)</mo><munder><mo>←</mo><mi>α</mi></munder><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><mi>V</mi><mo>(</mo><mi>s</mi><mo>''</mo><mo>)</mo></math>.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 写出这个方程的第一种形式的更简洁方法是使用符号<math><mi>a</mi><munder><mo>←</mo><mi>α</mi></munder><mi>b</mi></math>，意思是*a*[*k*+1]
    ← (1 - *α*) · *a*[*k*] + *α* ·*b*[*k*]。因此，[方程18-4](#td_learning_equation)的第一行可以重写为：<math><mi>V</mi><mo>(</mo><mi>s</mi><mo>)</mo><munder><mo>←</mo><mi>α</mi></munder><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><mi>V</mi><mo>(</mo><mi>s</mi><mo>'</mo><mo>)</mo></math>。
- en: Tip
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: TD learning has many similarities with stochastic gradient descent, including
    the fact that it handles one sample at a time. Moreover, just like SGD, it can
    only truly converge if you gradually reduce the learning rate; otherwise, it will
    keep bouncing around the optimum Q-values.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: TD学习与随机梯度下降有许多相似之处，包括一次处理一个样本。此外，就像SGD一样，只有逐渐降低学习率，它才能真正收敛；否则，它将继续在最优Q值周围反弹。
- en: For each state *s*, this algorithm keeps track of a running average of the immediate
    rewards the agent gets upon leaving that state, plus the rewards it expects to
    get later, assuming it acts optimally.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个状态*s*，该算法跟踪代理离开该状态后获得的即时奖励的平均值，以及它期望获得的奖励，假设它采取最优行动。
- en: Q-Learning
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-Learning
- en: Similarly, the Q-learning algorithm is an adaptation of the Q-value iteration
    algorithm to the situation where the transition probabilities and the rewards
    are initially unknown (see [Equation 18-5](#q_learning_equation)). Q-learning
    works by watching an agent play (e.g., randomly) and gradually improving its estimates
    of the Q-values. Once it has accurate Q-value estimates (or close enough), then
    the optimal policy is just choosing the action that has the highest Q-value (i.e.,
    the greedy policy).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Q-learning算法是Q值迭代算法在转移概率和奖励最初未知的情况下的一种适应。Q-learning通过观察代理玩（例如，随机玩）并逐渐改进其对Q值的估计来工作。一旦它有准确的Q值估计（或足够接近），那么最优策略就是选择具有最高Q值的动作（即，贪婪策略）。
- en: Equation 18-5\. Q-learning algorithm
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程18-5\. Q-learning算法
- en: <math display="block"><mi>Q</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo>
    <munder><mo>←</mo><mi>α</mi></munder> <mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo>
    <munder><mi>max</mi><mrow><mi>a</mi><mo>'</mo></mrow></munder> <mi>Q</mi><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo>
    <mi>a</mi><mo>'</mo><mo>)</mo></math>
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mi>Q</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo>
    <munder><mo>←</mo><mi>α</mi></munder> <mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo>
    <munder><mi>max</mi><mrow><mi>a</mi><mo>'</mo></mrow></munder> <mi>Q</mi><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo>
    <mi>a</mi><mo>'</mo><mo>)</mo></math>
- en: For each state-action pair (*s*, *a*), this algorithm keeps track of a running
    average of the rewards *r* the agent gets upon leaving the state *s* with action
    *a*, plus the sum of discounted future rewards it expects to get. To estimate
    this sum, we take the maximum of the Q-value estimates for the next state *s*′,
    since we assume that the target policy will act optimally from then on.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个状态-动作对（*s*，*a*），该算法跟踪代理离开状态*s*并采取动作*a*后获得的奖励*r*的平均值，以及它期望获得的折现未来奖励的总和。为了估计这个总和，我们取下一个状态*s*′的Q值估计的最大值，因为我们假设目标策略将从那时开始最优地行动。
- en: 'Let’s implement the Q-learning algorithm. First, we will need to make an agent
    explore the environment. For this, we need a step function so that the agent can
    execute one action and get the resulting state and reward:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现Q-learning算法。首先，我们需要让代理探索环境。为此，我们需要一个步骤函数，以便代理可以执行一个动作并获得结果状态和奖励：
- en: '[PRE21]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now let’s implement the agent’s exploration policy. Since the state space is
    pretty small, a simple random policy will be sufficient. If we run the algorithm
    for long enough, the agent will visit every state many times, and it will also
    try every possible action many times:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现代理的探索策略。由于状态空间相当小，一个简单的随机策略就足够了。如果我们运行足够长的时间，代理将多次访问每个状态，并且还将多次尝试每种可能的动作：
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, after we initialize the Q-values just like earlier, we are ready to run
    the Q-learning algorithm with learning rate decay (using power scheduling, introduced
    in [Chapter 11](ch11.html#deep_chapter)):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在我们像之前一样初始化Q值之后，我们准备使用学习率衰减的Q-learning算法运行（使用幂调度，引入于[第11章](ch11.html#deep_chapter)）：
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This algorithm will converge to the optimal Q-values, but it will take many
    iterations, and possibly quite a lot of hyperparameter tuning. As you can see
    in [Figure 18-9](#q_value_plot), the Q-value iteration algorithm (left) converges
    very quickly, in fewer than 20 iterations, while the Q-learning algorithm (right)
    takes about 8,000 iterations to converge. Obviously, not knowing the transition
    probabilities or the rewards makes finding the optimal policy significantly harder!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法将收敛到最优的Q值，但需要很多迭代，可能需要相当多的超参数调整。正如您在[图18-9](#q_value_plot)中看到的那样，Q值迭代算法（左侧）收敛得非常快，在不到20次迭代中，而Q-learning算法（右侧）需要大约8000次迭代才能收敛。显然，不知道转移概率或奖励使得找到最优策略变得更加困难！
- en: '![mls3 1809](assets/mls3_1809.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1809](assets/mls3_1809.png)'
- en: Figure 18-9\. Learning curve of the Q-value iteration algorithm versus the Q-learning
    algorithm
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-9\. Q值迭代算法与Q-learning算法的学习曲线
- en: 'The Q-learning algorithm is called an *off-policy* algorithm because the policy
    being trained is not necessarily the one used during training. For example, in
    the code we just ran, the policy being executed (the exploration policy) was completely
    random, while the policy being trained was never used. After training, the optimal
    policy corresponds to systematically choosing the action with the highest Q-value.
    Conversely, the policy gradients algorithm is an *on-policy* algorithm: it explores
    the world using the policy being trained. It is somewhat surprising that Q-learning
    is capable of learning the optimal policy by just watching an agent act randomly.
    Imagine learning to play golf when your teacher is a blindfolded monkey. Can we
    do better?'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning算法被称为*离策略*算法，因为正在训练的策略不一定是训练过程中使用的策略。例如，在我们刚刚运行的代码中，执行的策略（探索策略）是完全随机的，而正在训练的策略从未被使用过。训练后，最优策略对应于系统地选择具有最高Q值的动作。相反，策略梯度算法是*在策略*算法：它使用正在训练的策略探索世界。令人惊讶的是，Q-learning能够通过观察代理随机行动来学习最优策略。想象一下，在一只被蒙住眼睛的猴子是你的老师时学习打高尔夫球。我们能做得更好吗？
- en: Exploration Policies
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索策略
- en: 'Of course, Q-learning can work only if the exploration policy explores the
    MDP thoroughly enough. Although a purely random policy is guaranteed to eventually
    visit every state and every transition many times, it may take an extremely long
    time to do so. Therefore, a better option is to use the *ε-greedy policy* (ε is
    epsilon): at each step it acts randomly with probability *ε*, or greedily with
    probability 1–*ε* (i.e., choosing the action with the highest Q-value). The advantage
    of the *ε*-greedy policy (compared to a completely random policy) is that it will
    spend more and more time exploring the interesting parts of the environment, as
    the Q-value estimates get better and better, while still spending some time visiting
    unknown regions of the MDP. It is quite common to start with a high value for
    *ε* (e.g., 1.0) and then gradually reduce it (e.g., down to 0.05).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，只有当探索策略足够彻底地探索MDP时，Q学习才能起作用。尽管纯随机策略保证最终会访问每个状态和每个转换多次，但这可能需要非常长的时间。因此，更好的选择是使用*ε-贪心策略*（ε是epsilon）：在每一步中，它以概率*ε*随机行动，或以概率1-*ε*贪婪地行动（即选择具有最高Q值的动作）。*ε-贪心策略*的优势（与完全随机策略相比）在于，随着Q值估计变得越来越好，它将花费越来越多的时间探索环境的有趣部分，同时仍然花费一些时间访问MDP的未知区域。通常会从较高的*ε*值（例如1.0）开始，然后逐渐降低它（例如降至0.05）。
- en: Alternatively, rather than relying only on chance for exploration, another approach
    is to encourage the exploration policy to try actions that it has not tried much
    before. This can be implemented as a bonus added to the Q-value estimates, as
    shown in [Equation 18-6](#exploration_function_equation).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是鼓励探索策略尝试之前尝试过的动作，而不仅仅依赖于机会。这可以作为添加到Q值估计中的奖励来实现，如[方程18-6](#exploration_function_equation)所示。
- en: Equation 18-6\. Q-learning using an exploration function
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程18-6. 使用探索函数的Q学习
- en: <math display="block"><mi>Q</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo>
    <munder><mo>←</mo><mi>α</mi></munder> <mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mi>max</mi>
    <mrow><mi>a</mi><mo>'</mo></mrow></munder> <mi>f</mi> <mfenced><mrow><mi>Q</mi><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo>
    <mo>)</mo><mo>,</mo><mi>N</mi><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo> <mi>a</mi><mo>'</mo><mo>)</mo></mrow></mfenced></math>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mi>Q</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo>
    <munder><mo>←</mo><mi>α</mi></munder> <mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mi>max</mi>
    <mrow><mi>a</mi><mo>'</mo></mrow></munder> <mi>f</mi> <mfenced><mrow><mi>Q</mi><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo>
    <mo>)</mo><mo>,</mo><mi>N</mi><mo>(</mo><mi>s</mo><mo>'</mo><mo>,</mo> <mi>a</mi><mo>'</mo><mo>)</mo></mrow></mfenced></math>
- en: 'In this equation:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*N*(*s*′, *a*′) counts the number of times the action *a*′ was chosen in state
    *s*′.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*(*s*′, *a*′)计算动作*a*′在状态*s*′中被选择的次数。'
- en: '*f*(*Q*, *N*) is an *exploration function*, such as *f*(*Q*, *N*) = *Q* + *κ*/(1
    + *N*), where *κ* is a curiosity hyperparameter that measures how much the agent
    is attracted to the unknown.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f*(*Q*, *N*)是一个探索函数，例如*f*(*Q*, *N*) = *Q* + *κ*/(1 + *N*)，其中*κ*是一个好奇心超参数，衡量了代理对未知的吸引力。'
- en: Approximate Q-Learning and Deep Q-Learning
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 近似Q学习和深度Q学习
- en: The main problem with Q-learning is that it does not scale well to large (or
    even medium) MDPs with many states and actions. For example, suppose you wanted
    to use Q-learning to train an agent to play *Ms. Pac-Man* (see [Figure 18-1](#rl_examples_diagram)).
    There are about 150 pellets that Ms. Pac-Man can eat, each of which can be present
    or absent (i.e., already eaten). So, the number of possible states is greater
    than 2^(150) ≈ 10^(45). And if you add all the possible combinations of positions
    for all the ghosts and Ms. Pac-Man, the number of possible states becomes larger
    than the number of atoms in our planet, so there’s absolutely no way you can keep
    track of an estimate for every single Q-value.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习的主要问题是它在具有许多状态和动作的大型（甚至中等大小）MDP中无法很好地扩展。例如，假设您想使用Q学习来训练一个代理玩《Ms. Pac-Man》（见[图18-1](#rl_examples_diagram)）。Ms.
    Pac-Man可以吃约150个豆子，每个豆子可以存在或不存在（即已经被吃掉）。因此，可能的状态数量大于2^(150) ≈ 10^(45)。如果您考虑所有鬼和Ms.
    Pac-Man的所有可能位置组合，可能的状态数量将大于地球上的原子数量，因此绝对无法跟踪每个单个Q值的估计。
- en: The solution is to find a function *Q*[**θ**](*s*, *a*) that approximates the
    Q-value of any state-action pair (*s*, *a*) using a manageable number of parameters
    (given by the parameter vector **θ**). This is called *approximate Q-learning*.
    For years it was recommended to use linear combinations of handcrafted features
    extracted from the state (e.g., the distances of the closest ghosts, their directions,
    and so on) to estimate Q-values, but in 2013, [DeepMind](https://homl.info/dqn)
    showed that using deep neural networks can work much better, especially for complex
    problems, and it does not require any feature engineering. A DNN used to estimate
    Q-values is called a *deep Q-network* (DQN), and using a DQN for approximate Q-learning
    is called *deep Q-learning*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是找到一个函数*Q*[**θ**](*s*, *a*)，它用可管理数量的参数（由参数向量**θ**给出）来近似任何状态-动作对(*s*, *a*)的Q值。这被称为*近似Q学习*。多年来，人们建议使用从状态中提取的手工制作的特征的线性组合（例如，最近的鬼的距离、它们的方向等）来估计Q值，但在2013年，[DeepMind](https://homl.info/dqn)表明使用深度神经网络可以工作得更好，特别是对于复杂问题，而且不需要任何特征工程。用于估计Q值的DNN称为*深度Q网络*（DQN），并且使用DQN进行近似Q学习称为*深度Q学习*。
- en: Now, how can we train a DQN? Well, consider the approximate Q-value computed
    by the DQN for a given state-action pair (*s*, *a*). Thanks to Bellman, we know
    we want this approximate Q-value to be as close as possible to the reward *r*
    that we actually observe after playing action *a* in state *s*, plus the discounted
    value of playing optimally from then on. To estimate this sum of future discounted
    rewards, we can just execute the DQN on the next state *s*′, for all possible
    actions *a*′. We get an approximate future Q-value for each possible action. We
    then pick the highest (since we assume we will be playing optimally) and discount
    it, and this gives us an estimate of the sum of future discounted rewards. By
    summing the reward *r* and the future discounted value estimate, we get a target
    Q-value *y*(*s*, *a*) for the state-action pair (*s*, *a*), as shown in [Equation
    18-7](#target_q_value_equation).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何训练一个DQN呢？考虑DQN计算给定状态-动作对(*s*, *a*)的近似Q值。由于贝尔曼，我们知道我们希望这个近似Q值尽可能接近我们在状态*s*中执行动作*a*后实际观察到的奖励*r*，加上从那时开始最优地玩的折现值。为了估计未来折现奖励的总和，我们只需在下一个状态*s*′上执行DQN，对所有可能的动作*a*′。我们得到每个可能动作的近似未来Q值。然后我们选择最高的（因为我们假设我们将最优地玩），并对其进行折现，这给我们一个未来折现奖励总和的估计。通过将奖励*r*和未来折现值估计相加，我们得到状态-动作对(*s*,
    *a*)的目标Q值*y*(*s*, *a*)，如[方程18-7](#target_q_value_equation)所示。
- en: Equation 18-7\. Target Q-value
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程18-7\. 目标Q值
- en: <math><mi>y</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mi>max</mi><mrow><mi>a</mi><mo>'</mo></mrow></munder><msub><mi>Q</mi><mi
    mathvariant="bold">θ</mi></msub><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo><mo>)</mo></math>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mi>y</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mi>max</mi><mrow><mi>a</mi><mo>'</mo></mrow></munder><msub><mi>Q</mi><mi
    mathvariant="bold">θ</mi></msub><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo><mo>)</mo></math>
- en: With this target Q-value, we can run a training step using any gradient descent
    algorithm. Specifically, we generally try to minimize the squared error between
    the estimated Q-value *Q*[**θ**](*s*, *a*) and the target Q-value *y*(*s*, *a*),
    or the Huber loss to reduce the algorithm’s sensitivity to large errors. And that’s
    the deep Q-learning algorithm! Let’s see how to implement it to solve the CartPole
    environment.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个目标Q值，我们可以使用任何梯度下降算法运行一个训练步骤。具体来说，我们通常试图最小化估计的Q值*Q*[**θ**](*s*, *a*)和目标Q值*y*(*s*,
    *a*)之间的平方误差，或者使用Huber损失来减少算法对大误差的敏感性。这就是深度Q学习算法！让我们看看如何实现它来解决CartPole环境。
- en: Implementing Deep Q-Learning
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施深度Q学习
- en: 'The first thing we need is a deep Q-network. In theory, we need a neural net
    that takes a state-action pair as input, and outputs an approximate Q-value. However,
    in practice it’s much more efficient to use a neural net that takes only a state
    as input, and outputs one approximate Q-value for each possible action. To solve
    the CartPole environment, we do not need a very complicated neural net; a couple
    of hidden layers will do:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的第一件事是一个深度Q网络。理论上，我们需要一个神经网络，将状态-动作对作为输入，并输出一个近似Q值。然而，在实践中，使用一个只接受状态作为输入，并为每个可能动作输出一个近似Q值的神经网络要高效得多。为了解决CartPole环境，我们不需要一个非常复杂的神经网络；几个隐藏层就足够了：
- en: '[PRE24]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To select an action using this DQN, we pick the action with the largest predicted
    Q-value. To ensure that the agent explores the environment, we will use an *ε*-greedy
    policy (i.e., we will choose a random action with probability *ε*):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个DQN选择动作时，我们选择预测Q值最大的动作。为了确保代理程序探索环境，我们将使用*ε*-贪婪策略（即，我们将以概率*ε*选择一个随机动作）：
- en: '[PRE25]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Instead of training the DQN based only on the latest experiences, we will store
    all experiences in a *replay buffer* (or *replay memory*), and we will sample
    a random training batch from it at each training iteration. This helps reduce
    the correlations between the experiences in a training batch, which tremendously
    helps training. For this, we will just use a double-ended queue (`deque`):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不再仅基于最新经验训练DQN，而是将所有经验存储在一个*重放缓冲区*（或*重放内存*）中，并在每次训练迭代中从中随机抽取一个训练批次。这有助于减少训练批次中经验之间的相关性，从而极大地帮助训练。为此，我们将使用一个双端队列（`deque`）：
- en: '[PRE26]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Tip
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A *deque* is a queue elements can be efficiently added to or removed from on
    both ends. Inserting and deleting items from the ends of the queue is very fast,
    but random access can be slow when the queue gets long. If you need a very large
    replay buffer, you should use a circular buffer instead (see the notebook for
    an implementation), or check out [DeepMind’s Reverb library](https://homl.info/reverb).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*deque*是一个队列，可以高效地在两端添加或删除元素。从队列的两端插入和删除项目非常快，但当队列变长时，随机访问可能会很慢。如果您需要一个非常大的重放缓冲区，您应该使用循环缓冲区（请参阅笔记本中的实现），或查看[DeepMind的Reverb库](https://homl.info/reverb)。'
- en: 'Each experience will be composed of six elements: a state *s*, the action *a*
    that the agent took, the resulting reward *r*, the next state *s′* it reached,
    a Boolean indicating whether the episode ended at that point (`done`), and finally
    another Boolean indicating whether the episode was truncated at that point. We
    will need a small function to sample a random batch of experiences from the replay
    buffer. It will return six NumPy arrays corresponding to the six experience elements:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 每个体验将由六个元素组成：一个状态*s*，代理程序执行的动作*a*，产生的奖励*r*，它达到的下一个状态*s*′，一个指示该点是否结束的布尔值（`done`），最后一个指示该点是否截断的布尔值。我们将需要一个小函数从重放缓冲区中随机抽取一批体验。它将返回六个对应于六个体验元素的NumPy数组：
- en: '[PRE27]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s also create a function that will play a single step using the *ε*-greedy
    policy, then store the resulting experience in the replay buffer:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还创建一个函数，该函数将使用*ε*-贪婪策略执行一个单步操作，然后将结果体验存储在重放缓冲区中：
- en: '[PRE28]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, let’s create one last function that will sample a batch of experiences
    from the replay buffer and train the DQN by performing a single gradient descent
    step on this batch:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们创建一个最后一个函数，该函数将从重放缓冲区中抽取一批体验，并通过在该批次上执行单个梯度下降步骤来训练DQN：
- en: '[PRE29]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here’s what’s happening in this code:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中发生了什么：
- en: First we define some hyperparameters, and we create the optimizer and the loss
    function.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先我们定义一些超参数，然后创建优化器和损失函数。
- en: Then we create the `training_step()` function. It starts by sampling a batch
    of experiences, then it uses the DQN to predict the Q-value for each possible
    action in each experience’s next state. Since we assume that the agent will be
    playing optimally, we only keep the maximum Q-value for each next state. Next,
    we use [Equation 18-7](#target_q_value_equation) to compute the target Q-value
    for each experience’s state-action pair.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们创建`training_step()`函数。它首先对经验进行批量采样，然后使用DQN来预测每个经验的下一个状态中每个可能动作的Q值。由于我们假设代理将会最优地进行游戏，我们只保留每个下一个状态的最大Q值。接下来，我们使用[Equation
    18-7](#target_q_value_equation)来计算每个经验的状态-动作对的目标Q值。
- en: We want to use the DQN to compute the Q-value for each experienced state-action
    pair, but the DQN will also output the Q-values for the other possible actions,
    not just for the action that was actually chosen by the agent. So, we need to
    mask out all the Q-values we do not need. The `tf.one_hot()` function makes it
    possible to convert an array of action indices into such a mask. For example,
    if the first three experiences contain actions 1, 1, 0, respectively, then the
    mask will start with `[[0, 1], [0, 1], [1, 0],...]`. We can then multiply the
    DQN’s output with this mask, and this will zero out all the Q-values we do not
    want. We then sum over axis 1 to get rid of all the zeros, keeping only the Q-values
    of the experienced state-action pairs. This gives us the `Q_values` tensor, containing
    one predicted Q-value for each experience in the batch.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望使用DQN来计算每个经验状态-动作对的Q值，但是DQN还会输出其他可能动作的Q值，而不仅仅是代理实际选择的动作。因此，我们需要屏蔽掉所有我们不需要的Q值。`tf.one_hot()`函数使得将动作索引数组转换为这样的屏蔽变得可能。例如，如果前三个经验包含动作1、1、0，那么屏蔽将以`[[0,
    1], [0, 1], [1, 0], ...]`开始。然后我们可以将DQN的输出与这个屏蔽相乘，这将将我们不想要的所有Q值置零。然后我们沿着轴1求和，去除所有零，只保留经验状态-动作对的Q值。这给我们了`Q_values`张量，包含批量中每个经验的一个预测Q值。
- en: 'Next, we compute the loss: it is the mean squared error between the target
    and predicted Q-values for the experienced state-action pairs.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们计算损失：它是经验状态-动作对的目标和预测Q值之间的均方误差。
- en: Finally, we perform a gradient descent step to minimize the loss with regard
    to the model’s trainable variables.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们执行梯度下降步骤，以最小化损失与模型可训练变量的关系。
- en: 'This was the hardest part. Now training the model is straightforward:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最困难的部分。现在训练模型就很简单了：
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We run 600 episodes, each for a maximum of 200 steps. At each step, we first
    compute the `epsilon` value for the *ε*-greedy policy: it will go from 1 down
    to 0.01, linearly, in a bit under 500 episodes. Then we call the `play_one_step()`
    function, which will use the *ε*-greedy policy to pick an action, then execute
    it and record the experience in the replay buffer. If the episode is done or truncated,
    we exit the loop. Finally, if we are past episode 50, we call the `training_step()`
    function to train the model on one batch sampled from the replay buffer. The reason
    we play many episodes without training is to give the replay buffer some time
    to fill up (if we don’t wait enough, then there will not be enough diversity in
    the replay buffer). And that’s it: we just implemented the Deep Q-learning algorithm!'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行600个episode，每个最多200步。在每一步中，我们首先计算*ε*-贪婪策略的`epsilon`值：它将从1线性下降到0.01，在不到500个episode内。然后我们调用`play_one_step()`函数，该函数将使用*ε*-贪婪策略选择一个动作，然后执行它并记录经验到重放缓冲区。如果episode结束或被截断，我们退出循环。最后，如果我们超过第50个episode，我们调用`training_step()`函数从重放缓冲区中采样一个批次来训练模型。我们之所以在没有训练的情况下运行多个episode，是为了给重放缓冲区一些时间来填充（如果我们不等待足够长的时间，那么重放缓冲区中将没有足够的多样性）。就是这样：我们刚刚实现了深度Q学习算法！
- en: '[Figure 18-10](#dqn_rewards_plot) shows the total rewards the agent got during
    each episode.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 18-10](#dqn_rewards_plot)显示了代理在每个episode中获得的总奖励。'
- en: '![mls3 1810](assets/mls3_1810.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1810](assets/mls3_1810.png)'
- en: Figure 18-10\. Learning curve of the deep Q-learning algorithm
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-10\. 深度Q学习算法的学习曲线
- en: 'As you can see, the algorithm took a while to start learning anything, in part
    because *ε* was very high at the beginning. Then its progress was erratic: it
    first reached the max reward around episode 220, but it immediately dropped, then
    bounced up and down a few times, and soon after it looked like it had finally
    stabilized near the max reward, at around episode 320, its score again dropped
    down dramatically. This is called *catastrophic forgetting*, and it is one of
    the big problems facing virtually all RL algorithms: as the agent explores the
    environment, it updates its policy, but what it learns in one part of the environment
    may break what it learned earlier in other parts of the environment. The experiences
    are quite correlated, and the learning environment keeps changing—this is not
    ideal for gradient descent! If you increase the size of the replay buffer, the
    algorithm will be less subject to this problem. Tuning the learning rate may also
    help. But the truth is, reinforcement learning is hard: training is often unstable,
    and you may need to try many hyperparameter values and random seeds before you
    find a combination that works well. For example, if you try changing the activation
    function from `"elu"` to `"relu"`, the performance will be much lower.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，该算法花了一段时间才开始学习任何东西，部分原因是*ε*在开始时非常高。然后它的进展是不稳定的：它首先在第220集左右达到了最大奖励，但立即下降，然后上下几次反弹，不久后看起来它终于稳定在最大奖励附近，大约在第320集左右，它的得分再次急剧下降。这被称为*灾难性遗忘*，这是几乎所有RL算法面临的一个大问题之一：当代理探索环境时，它更新其策略，但它在环境的一个部分学到的东西可能会破坏它在环境的其他部分早期学到的东西。经验是相当相关的，学习环境不断变化——这对于梯度下降来说并不理想！如果增加回放缓冲区的大小，算法将不太容易受到这个问题的影响。调整学习率也可能有所帮助。但事实是，强化学习很难：训练通常不稳定，您可能需要尝试许多超参数值和随机种子，才能找到一个表现良好的组合。例如，如果您尝试将激活函数从“elu”更改为“relu”，性能将大大降低。
- en: Note
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Reinforcement learning is notoriously difficult, largely because of the training
    instabilities and the huge sensitivity to the choice of hyperparameter values
    and random seeds.⁠^([13](ch18.html#idm45720163009904)) As the researcher Andrej
    Karpathy put it, “[Supervised learning] wants to work. […​] RL must be forced
    to work”. You will need time, patience, perseverance, and perhaps a bit of luck
    too. This is a major reason RL is not as widely adopted as regular deep learning
    (e.g., convolutional nets). But there are a few real-world applications, beyond
    AlphaGo and Atari games: for example, Google uses RL to optimize its datacenter
    costs, and it is used in some robotics applications, for hyperparameter tuning,
    and in recommender systems.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习因训练不稳定性和对超参数值和随机种子选择的极度敏感性而臭名昭著。正如研究人员Andrej Karpathy所说，“[监督学习]想要工作。[...]强化学习必须被迫工作”。您需要时间、耐心、毅力，也许还需要一点运气。这是RL不像常规深度学习（例如，卷积网络）那样被广泛采用的一个主要原因。但除了AlphaGo和Atari游戏之外，还有一些真实世界的应用：例如，谷歌使用RL来优化其数据中心成本，并且它被用于一些机器人应用、超参数调整和推荐系统中。
- en: You might wonder why we didn’t plot the loss. It turns out that loss is a poor
    indicator of the model’s performance. The loss might go down, yet the agent might
    perform worse (e.g., this can happen when the agent gets stuck in one small region
    of the environment, and the DQN starts overfitting this region). Conversely, the
    loss could go up, yet the agent might perform better (e.g., if the DQN was underestimating
    the Q-values and it starts correctly increasing its predictions, the agent will
    likely perform better, getting more rewards, but the loss might increase because
    the DQN also sets the targets, which will be larger too). So, it’s preferable
    to plot the rewards.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想为什么我们没有绘制损失。事实证明，损失是模型性能的一个很差的指标。损失可能会下降，但代理可能表现更差（例如，当代理陷入环境的一个小区域时，DQN开始过度拟合这个区域时可能会发生这种情况）。相反，损失可能会上升，但代理可能表现更好（例如，如果DQN低估了Q值并开始正确增加其预测，代理可能表现更好，获得更多奖励，但损失可能会增加，因为DQN还设置了目标，这也会更大）。因此，最好绘制奖励。
- en: The basic deep Q-learning algorithm we’ve been using so far would be too unstable
    to learn to play Atari games. So how did DeepMind do it? Well, they tweaked the
    algorithm!
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用的基本深度Q学习算法对于学习玩Atari游戏来说太不稳定了。那么DeepMind是如何做到的呢？嗯，他们调整了算法！
- en: Deep Q-Learning Variants
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习变体
- en: Let’s look at a few variants of the deep Q-learning algorithm that can stabilize
    and speed up training.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些可以稳定和加速训练的深度Q学习算法的变体。
- en: Fixed Q-value Targets
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 固定Q值目标
- en: 'In the basic deep Q-learning algorithm, the model is used both to make predictions
    and to set its own targets. This can lead to a situation analogous to a dog chasing
    its own tail. This feedback loop can make the network unstable: it can diverge,
    oscillate, freeze, and so on. To solve this problem, in their 2013 paper the DeepMind
    researchers used two DQNs instead of one: the first is the *online model*, which
    learns at each step and is used to move the agent around, and the other is the
    *target model* used only to define the targets. The target model is just a clone
    of the online model:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本的深度Q学习算法中，模型既用于进行预测，也用于设置自己的目标。这可能导致类似于狗追逐自己尾巴的情况。这种反馈循环可能使网络不稳定：它可能发散、振荡、冻结等。为了解决这个问题，在他们2013年的论文中，DeepMind的研究人员使用了两个DQN而不是一个：第一个是*在线模型*，它在每一步学习并用于移动代理，另一个是*目标模型*，仅用于定义目标。目标模型只是在线模型的一个克隆：
- en: '[PRE31]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, in the `training_step()` function, we just need to change one line to
    use the target model instead of the online model when computing the Q-values of
    the next states:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在`training_step()`函数中，我们只需要更改一行，使用目标模型而不是在线模型来计算下一个状态的Q值：
- en: '[PRE32]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, in the training loop, we must copy the weights of the online model
    to the target model, at regular intervals (e.g., every 50 episodes):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在训练循环中，我们必须定期将在线模型的权重复制到目标模型中（例如，每50个episode）：
- en: '[PRE33]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Since the target model is updated much less often than the online model, the
    Q-value targets are more stable, the feedback loop we discussed earlier is dampened,
    and its effects are less severe. This approach was one of the DeepMind researchers’
    main contributions in their 2013 paper, allowing agents to learn to play Atari
    games from raw pixels. To stabilize training, they used a tiny learning rate of
    0.00025, they updated the target model only every 10,000 steps (instead of 50),
    and they used a very large replay buffer of 1 million experiences. They decreased
    `epsilon` very slowly, from 1 to 0.1 in 1 million steps, and they let the algorithm
    run for 50 million steps. Moreover, their DQN was a deep convolutional net.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标模型更新的频率远低于在线模型，Q值目标更加稳定，我们之前讨论的反馈循环被减弱，其影响也变得不那么严重。这种方法是DeepMind研究人员在2013年的一篇论文中的主要贡献之一，使代理能够从原始像素学习玩Atari游戏。为了稳定训练，他们使用了非常小的学习率0.00025，他们每10000步才更新一次目标模型（而不是50步），并且他们使用了一个非常大的重放缓冲区，包含100万个经验。他们非常缓慢地减小了`epsilon`，在100万步内从1减小到0.1，并让算法运行了5000万步。此外，他们的DQN是一个深度卷积网络。
- en: Now let’s take a look at another DQN variant that managed to beat the state
    of the art once more.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看另一个DQN变体，它再次超越了现有技术水平。
- en: Double DQN
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双重DQN
- en: 'In a [2015 paper](https://homl.info/doubledqn),⁠^([14](ch18.html#idm45720162889472))
    DeepMind researchers tweaked their DQN algorithm, increasing its performance and
    somewhat stabilizing training. They called this variant *double DQN*. The update
    was based on the observation that the target network is prone to overestimating
    Q-values. Indeed, suppose all actions are equally good: the Q-values estimated
    by the target model should be identical, but since they are approximations, some
    may be slightly greater than others, by pure chance. The target model will always
    select the largest Q-value, which will be slightly greater than the mean Q-value,
    most likely overestimating the true Q-value (a bit like counting the height of
    the tallest random wave when measuring the depth of a pool). To fix this, the
    researchers proposed using the online model instead of the target model when selecting
    the best actions for the next states, and using the target model only to estimate
    the Q-values for these best actions. Here is the updated `training_step()` function:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇2015年的论文中，DeepMind研究人员调整了他们的DQN算法，提高了性能并在一定程度上稳定了训练。他们将这个变体称为*双重DQN*。更新基于这样一个观察：目标网络容易高估Q值。实际上，假设所有动作都是同样好的：目标模型估计的Q值应该是相同的，但由于它们是近似值，一些可能略大于其他值，纯粹是偶然的。目标模型将始终选择最大的Q值，这个值将略大于平均Q值，很可能高估真实的Q值（有点像在测量池的深度时计算最高随机波浪的高度）。为了解决这个问题，研究人员建议在选择下一个状态的最佳动作时使用在线模型而不是目标模型，并且仅使用目标模型来估计这些最佳动作的Q值。以下是更新后的`training_step()`函数：
- en: '[PRE34]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Just a few months later, another improvement to the DQN algorithm was propose;
    we’ll look at that next.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅几个月后，DQN算法的另一个改进被提出；我们接下来将看看这个改进。
- en: Prioritized Experience Replay
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优先经验回放
- en: Instead of sampling experiences *uniformly* from the replay buffer, why not
    sample important experiences more frequently? This idea is called *importance
    sampling* (IS) or *prioritized experience replay* (PER), and it was introduced
    in a [2015 paper](https://homl.info/prioreplay)⁠^([15](ch18.html#idm45720162777584))
    by DeepMind researchers (once again!).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 与从重放缓冲区中均匀采样经验不同，为什么不更频繁地采样重要经验呢？这个想法被称为*重要性采样*（IS）或*优先经验回放*（PER），并且是由DeepMind研究人员在2015年的一篇论文中介绍的（再次！）。
- en: 'More specifically, experiences are considered “important” if they are likely
    to lead to fast learning progress. But how can we estimate this? One reasonable
    approach is to measure the magnitude of the TD error *δ* = *r* + *γ*·*V*(*s*′)
    – *V*(*s*). A large TD error indicates that a transition (*s*, *a*, *s*′) is very
    surprising, and thus probably worth learning from.⁠^([16](ch18.html#idm45720162769408))
    When an experience is recorded in the replay buffer, its priority is set to a
    very large value, to ensure that it gets sampled at least once. However, once
    it is sampled (and every time it is sampled), the TD error *δ* is computed, and
    this experience’s priority is set to *p* = |*δ*| (plus a small constant to ensure
    that every experience has a nonzero probability of being sampled). The probability
    *P* of sampling an experience with priority *p* is proportional to *p*^(*ζ*),
    where *ζ* is a hyperparameter that controls how greedy we want importance sampling
    to be: when *ζ* = 0, we just get uniform sampling, and when *ζ* = 1, we get full-blown
    importance sampling. In the paper, the authors used *ζ* = 0.6, but the optimal
    value will depend on the task.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，如果经验很可能导致快速学习进展，那么这些经验被认为是“重要的”。但是我们如何估计这一点呢？一个合理的方法是测量TD误差的大小*δ* = *r*
    + *γ*·*V*(*s*′) – *V*(*s*)。较大的TD误差表明一个转换(*s*, *a*, *s*′)非常令人惊讶，因此可能值得学习。当一个经验被记录在重放缓冲区中时，其优先级被设置为一个非常大的值，以确保至少被采样一次。然而，一旦被采样（并且每次被采样时），TD误差*δ*被计算，并且这个经验的优先级被设置为*p*
    = |*δ*|（再加上一个小常数，以确保每个经验有非零的采样概率）。具有优先级*p*的经验被采样的概率*P*与*p*^(*ζ*)成正比，其中*ζ*是一个控制我们希望重要性采样有多贪婪的超参数：当*ζ*
    = 0时，我们只得到均匀采样，当*ζ* = 1时，我们得到完全的重要性采样。在论文中，作者使用了*ζ* = 0.6，但最佳值将取决于任务。
- en: 'There’s one catch, though: since the samples will be biased toward important
    experiences, we must compensate for this bias during training by downweighting
    the experiences according to their importance, or else the model will just overfit
    the important experiences. To be clear, we want important experiences to be sampled
    more often, but this also means we must give them a lower weight during training.
    To do this, we define each experience’s training weight as *w* = (*n* *P*)^(–*β*),
    where *n* is the number of experiences in the replay buffer, and *β* is a hyperparameter
    that controls how much we want to compensate for the importance sampling bias
    (0 means not at all, while 1 means entirely). In the paper, the authors used *β*
    = 0.4 at the beginning of training and linearly increased it to *β* = 1 by the
    end of training. Again, the optimal value will depend on the task, but if you
    increase one, you will usually want to increase the other as well.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个问题：由于样本将偏向于重要经验，我们必须在训练过程中通过根据其重要性降低经验的权重来补偿这种偏差，否则模型将只是过度拟合重要经验。明确地说，我们希望重要经验被更频繁地抽样，但这也意味着我们必须在训练过程中给它们更低的权重。为了做到这一点，我们将每个经验的训练权重定义为*w*
    = (*n* *P*)^(–*β*)，其中*n*是回放缓冲区中的经验数量，*β*是一个超参数，控制我们想要补偿重要性抽样偏差的程度（0表示根本不补偿，而1表示完全补偿）。在论文中，作者在训练开始时使用*β*
    = 0.4，并在训练结束时线性增加到*β* = 1。再次强调，最佳值将取决于任务，但如果你增加一个值，通常也会想要增加另一个值。
- en: Now let’s look at one last important variant of the DQN algorithm.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下DQN算法的最后一个重要变体。
- en: Dueling DQN
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决斗DQN
- en: 'The *dueling DQN* algorithm (DDQN, not to be confused with double DQN, although
    both techniques can easily be combined) was introduced in yet another [2015 paper](https://homl.info/ddqn)⁠^([17](ch18.html#idm45720162754592))
    by DeepMind researchers. To understand how it works, we must first note that the
    Q-value of a state-action pair (*s*, *a*) can be expressed as *Q*(*s*, *a*) =
    *V*(*s*) + *A*(*s*, *a*), where *V*(*s*) is the value of state *s* and *A*(*s*,
    *a*) is the *advantage* of taking the action *a* in state *s*, compared to all
    other possible actions in that state. Moreover, the value of a state is equal
    to the Q-value of the best action *a*^* for that state (since we assume the optimal
    policy will pick the best action), so *V*(*s*) = *Q*(*s*, *a*^*), which implies
    that *A*(*s*, *a*^*) = 0\. In a dueling DQN, the model estimates both the value
    of the state and the advantage of each possible action. Since the best action
    should have an advantage of 0, the model subtracts the maximum predicted advantage
    from all predicted advantages. Here is a simple DDQN model, implemented using
    the functional API:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*决斗DQN*算法（DDQN，不要与双重DQN混淆，尽管这两种技术可以很容易地结合在一起）是由DeepMind研究人员在另一篇[2015年的论文](https://homl.info/ddqn)中介绍的。要理解它的工作原理，我们首先必须注意到一个状态-动作对(*s*,
    *a*)的Q值可以表示为*Q*(*s*, *a*) = *V*(*s*) + *A*(*s*, *a*)，其中*V*(*s*)是状态*s*的值，*A*(*s*,
    *a*)是在状态*s*中采取动作*a*的*优势*，与该状态下所有其他可能的动作相比。此外，一个状态的值等于该状态的最佳动作*a*^*的Q值（因为我们假设最优策略将选择最佳动作），所以*V*(*s*)
    = *Q*(*s*, *a*^*)，这意味着*A*(*s*, *a*^*) = 0。在决斗DQN中，模型估计了状态的值和每个可能动作的优势。由于最佳动作应该具有优势为0，模型从所有预测的优势中减去了最大预测的优势。这里是一个使用功能API实现的简单DDQN模型：'
- en: '[PRE35]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The rest of the algorithm is just the same as earlier. In fact, you can build
    a double dueling DQN and combine it with prioritized experience replay! More generally,
    many RL techniques can be combined, as DeepMind demonstrated in a [2017 paper](https://homl.info/rainbow):⁠^([18](ch18.html#idm45720162553536))
    the paper’s authors combined six different techniques into an agent called *Rainbow*,
    which largely outperformed the state of the art.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的其余部分与之前完全相同。事实上，你可以构建一个双重决斗DQN并将其与优先经验重放结合起来！更一般地说，许多RL技术可以结合在一起，正如DeepMind在一篇[2017年的论文](https://homl.info/rainbow)中展示的：论文的作者将六种不同的技术结合到一个名为*Rainbow*的代理中，这在很大程度上超越了现有技术水平。
- en: As you can see, deep reinforcement learning is a fast-growing field and there’s
    much more to discover!
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，深度强化学习是一个快速发展的领域，还有很多东西等待探索！
- en: Overview of Some Popular RL Algorithms
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些流行RL算法的概述
- en: 'Before we close this chapter, let’s take a brief look at a few other popular
    algorithms:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本章之前，让我们简要看一下其他几种流行的算法：
- en: '[*AlphaGo*](https://homl.info/alphago)⁠^([19](ch18.html#idm45720162511200))'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[*AlphaGo*](https://homl.info/alphago)'
- en: AlphaGo uses a variant of *Monte Carlo tree search* (MCTS) based on deep neural
    networks to beat human champions at the game of Go. MCTS was invented in 1949
    by Nicholas Metropolis and Stanislaw Ulam. It selects the best move after running
    many simulations, repeatedly exploring the search tree starting from the current
    position, and spending more time on the most promising branches. When it reaches
    a node that it hasn’t visited before, it plays randomly until the game ends, and
    updates its estimates for each visited node (excluding the random moves), increasing
    or decreasing each estimate depending on the final outcome. AlphaGo is based on
    the same principle, but it uses a policy network to select moves, rather than
    playing randomly. This policy net is trained using policy gradients. The original
    algorithm involved three more neural networks, and was more complicated, but it
    was simplified in the [AlphaGo Zero paper](https://homl.info/alphagozero),⁠^([20](ch18.html#idm45720162506192))
    which uses a single neural network to both select moves and evaluate game states.
    The [AlphaZero paper](https://homl.info/alphazero)⁠^([21](ch18.html#idm45720162504560))
    generalized this algorithm, making it capable of tackling not only the game of
    Go, but also chess and shogi (Japanese chess). Lastly, the [MuZero paper](https://homl.info/muzero)⁠^([22](ch18.html#idm45720162502384))
    continued to improve upon this algorithm, outperforming the previous iterations
    even though the agent starts out without even knowing the rules of the game!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo使用基于深度神经网络的*蒙特卡洛树搜索*（MCTS）的变体，在围棋比赛中击败人类冠军。MCTS是由Nicholas Metropolis和Stanislaw
    Ulam于1949年发明的。它在运行许多模拟之后选择最佳移动，重复地探索从当前位置开始的搜索树，并在最有希望的分支上花费更多时间。当它到达一个以前未访问过的节点时，它会随机播放直到游戏结束，并更新每个访问过的节点的估计值（排除随机移动），根据最终结果增加或减少每个估计值。AlphaGo基于相同的原则，但它使用策略网络来选择移动，而不是随机播放。这个策略网络是使用策略梯度进行训练的。原始算法涉及另外三个神经网络，并且更加复杂，但在[AlphaGo
    Zero论文](https://homl.info/alphagozero)中被简化，使用单个神经网络来选择移动和评估游戏状态。[AlphaZero论文](https://homl.info/alphazero)推广了这个算法，使其能够处理不仅是围棋，还有国际象棋和将棋（日本象棋）。最后，[MuZero论文](https://homl.info/muzero)继续改进这个算法，即使代理开始时甚至不知道游戏规则，也能胜过以前的迭代！
- en: Actor-critic algorithms
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic算法
- en: 'Actor-critics are a family of RL algorithms that combine policy gradients with
    deep Q-networks. An actor-critic agent contains two neural networks: a policy
    net and a DQN. The DQN is trained normally, by learning from the agent’s experiences.
    The policy net learns differently (and much faster) than in regular PG: instead
    of estimating the value of each action by going through multiple episodes, then
    summing the future discounted rewards for each action, and finally normalizing
    them, the agent (actor) relies on the action values estimated by the DQN (critic).
    It’s a bit like an athlete (the agent) learning with the help of a coach (the
    DQN).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critics是一类将策略梯度与深度Q网络结合的RL算法。一个actor-critic代理包含两个神经网络：一个策略网络和一个DQN。DQN通过从代理的经验中学习来进行正常训练。策略网络学习方式不同（并且比常规PG快得多）：代理不是通过多个情节估计每个动作的价值，然后为每个动作总结未来折现奖励，最后对其进行归一化，而是依赖于DQN估计的动作值（评论家）。这有点像运动员（代理）在教练（DQN）的帮助下学习。
- en: '[*Asynchronous advantage actor-critic (A3C)*](https://homl.info/a3c)⁠^([23](ch18.html#idm45720162497280))'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[*异步优势actor-critic（A3C）*](https://homl.info/a3c)⁠^([23](ch18.html#idm45720162497280))'
- en: This is an important actor-critic variant introduced by DeepMind researchers
    in 2016 where multiple agents learn in parallel, exploring different copies of
    the environment. At regular intervals, but asynchronously (hence the name), each
    agent pushes some weight updates to a master network, then it pulls the latest
    weights from that network. Each agent thus contributes to improving the master
    network and benefits from what the other agents have learned. Moreover, instead
    of estimating the Q-values, the DQN estimates the advantage of each action (hence
    the second A in the name), which stabilizes training.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这是DeepMind研究人员在2016年引入的一个重要的actor-critic变体，其中多个代理并行学习，探索环境的不同副本。定期但异步地（因此得名），每个代理将一些权重更新推送到主网络，然后从该网络中拉取最新的权重。因此，每个代理都有助于改进主网络，并从其他代理学到的知识中受益。此外，DQN估计每个动作的优势，而不是估计Q值（因此名称中的第二个A），这有助于稳定训练。
- en: '[*Advantage actor-critic (A2C)*](https://homl.info/a2c)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[*优势actor-critic（A2C）*](https://homl.info/a2c)'
- en: A2C is a variant of the A3C algorithm that removes the asynchronicity. All model
    updates are synchronous, so gradient updates are performed over larger batches,
    which allows the model to better utilize the power of the GPU.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: A2C是A3C算法的一个变体，它去除了异步性。所有模型更新都是同步的，因此梯度更新是在更大的批次上执行的，这使模型能够更好地利用GPU的性能。
- en: '[*Soft actor-critic (SAC)*](https://homl.info/sac)⁠^([24](ch18.html#idm45720162490480))'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[*软actor-critic（SAC）*](https://homl.info/sac)⁠^([24](ch18.html#idm45720162490480))'
- en: SAC is an actor-critic variant proposed in 2018 by Tuomas Haarnoja and other
    UC Berkeley researchers. It learns not only rewards, but also to maximize the
    entropy of its actions. In other words, it tries to be as unpredictable as possible
    while still getting as many rewards as possible. This encourages the agent to
    explore the environment, which speeds up training, and makes it less likely to
    repeatedly execute the same action when the DQN produces imperfect estimates.
    This algorithm has demonstrated an amazing sample efficiency (contrary to all
    the previous algorithms, which learn very slowly).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: SAC是由Tuomas Haarnoja和其他加州大学伯克利分校研究人员于2018年提出的actor-critic变体。它不仅学习奖励，还要最大化其动作的熵。换句话说，它试图尽可能不可预测，同时尽可能获得更多奖励。这鼓励代理探索环境，加快训练速度，并使其在DQN产生不完美估计时不太可能重复执行相同的动作。这个算法展示了惊人的样本效率（与所有以前的算法相反，学习速度非常慢）。
- en: '[*Proximal policy optimization (PPO)*](https://homl.info/ppo)⁠^([25](ch18.html#idm45720162487536))'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[*近端策略优化（PPO）*](https://homl.info/ppo)'
- en: This algorithm by John Schulman and other OpenAI researchers is based on A2C,
    but it clips the loss function to avoid excessively large weight updates (which
    often lead to training instabilities). PPO is a simplification of the previous
    [*trust region policy optimization*](https://homl.info/trpo)⁠^([26](ch18.html#idm45720162483456))
    (TRPO) algorithm, also by OpenAI. OpenAI made the news in April 2019 with its
    AI called OpenAI Five, based on the PPO algorithm, which defeated the world champions
    at the multiplayer game *Dota 2*.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这个由John Schulman和其他OpenAI研究人员开发的算法基于A2C，但它剪切损失函数以避免过大的权重更新（这经常导致训练不稳定）。PPO是前一个[*信任区域策略优化*](https://homl.info/trpo)（TRPO）算法的简化版本，也是由OpenAI开发的。OpenAI在2019年4月的新闻中以其基于PPO算法的AI
    OpenAI Five而闻名，该AI在多人游戏*Dota 2*中击败了世界冠军。
- en: '[*Curiosity-based exploration*](https://homl.info/curiosity)⁠^([27](ch18.html#idm45720162479024))'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[*基于好奇心的探索*](https://homl.info/curiosity)'
- en: 'A recurring problem in RL is the sparsity of the rewards, which makes learning
    very slow and inefficient. Deepak Pathak and other UC Berkeley researchers have
    proposed an exciting way to tackle this issue: why not ignore the rewards, and
    just make the agent extremely curious to explore the environment? The rewards
    thus become intrinsic to the agent, rather than coming from the environment. Similarly,
    stimulating curiosity in a child is more likely to give good results than purely
    rewarding the child for getting good grades. How does this work? The agent continuously
    tries to predict the outcome of its actions, and it seeks situations where the
    outcome does not match its predictions. In other words, it wants to be surprised.
    If the outcome is predictable (boring), it goes elsewhere. However, if the outcome
    is unpredictable but the agent notices that it has no control over it, it also
    gets bored after a while. With only curiosity, the authors succeeded in training
    an agent at many video games: even though the agent gets no penalty for losing,
    the game starts over, which is boring so it learns to avoid it.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中经常出现的问题是奖励的稀疏性，这使得学习变得非常缓慢和低效。加州大学伯克利分校的Deepak Pathak和其他研究人员提出了一种激动人心的方法来解决这个问题：为什么不忽略奖励，只是让代理人对探索环境感到极大的好奇心呢？奖励因此变得内在于代理人，而不是来自环境。同样，激发孩子的好奇心更有可能取得好的结果，而不仅仅是因为孩子取得好成绩而奖励他。这是如何运作的呢？代理人不断尝试预测其行动的结果，并寻找结果与其预测不符的情况。换句话说，它希望受到惊喜。如果结果是可预测的（无聊），它会去其他地方。然而，如果结果是不可预测的，但代理人注意到自己无法控制它，那么它也会在一段时间后感到无聊。只有好奇心，作者们成功地训练了一个代理人玩了很多视频游戏：即使代理人输掉了也没有惩罚，游戏重新开始，这很无聊，所以它学会了避免这种情况。
- en: Open-ended learning (OEL)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 开放式学习（OEL）
- en: 'The objective of OEL is to train agents capable of endlessly learning new and
    interesting tasks, typically generated procedurally. We’re not there yet, but
    there has been some amazing progress over the last few years. For example, a [2019
    paper](https://homl.info/poet)⁠^([28](ch18.html#idm45720162472416)) by a team
    of researchers from Uber AI introduced the *POET algorithm*, which generates multiple
    simulated 2D environments with bumps and holes and trains one agent per environment:
    the agent’s goal is to walk as fast as possible while avoiding the obstacles.
    The algorithm starts out with simple environments, but they gradually get harder
    over time: this is called *curriculum learning*. Moreover, although each agent
    is only trained within one environment, it must regularly compete against other
    agents, across all environments. In each environment, the winner is copied over
    and it replaces the agent that was there before. This way, knowledge is regularly
    transferred across environments, and the most adaptable agents are selected. In
    the end, the agents are much better walkers than agents trained on a single task,
    and they can tackle much harder environments. Of course, this principle can be
    applied to other environments and tasks as well. If you’re interested in OEL,
    make sure to check out the [Enhanced POET paper](https://homl.info/epoet),⁠^([29](ch18.html#idm45720162468448))
    as well as DeepMind’s [2021 paper](https://homl.info/oel2021)⁠^([30](ch18.html#idm45720162466960))
    on this topic.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: OEL的目标是训练代理人能够不断学习新颖有趣的任务，通常是通过程序生成的。我们还没有达到这一目标，但在过去几年中取得了一些惊人的进展。例如，Uber AI团队在2019年发表的一篇论文介绍了*POET算法*，该算法生成多个带有凸起和洞的模拟2D环境，并为每个环境训练一个代理人：代理人的目标是尽可能快地行走，同时避开障碍物。该算法从简单的环境开始，但随着时间的推移逐渐变得更加困难：这被称为*课程学习*。此外，尽管每个代理人只在一个环境中接受训练，但它必须定期与其他代理人竞争，跨所有环境。在每个环境中，获胜者被复制并取代之前的代理人。通过这种方式，知识定期在环境之间传递，并选择最具适应性的代理人。最终，这些代理人比单一任务训练的代理人更擅长行走，并且能够应对更加困难的环境。当然，这个原则也可以应用于其他环境和任务。如果您对OEL感兴趣，请务必查看[增强POET论文](https://homl.info/epoet)，以及DeepMind在这个主题上的[2021年论文](https://homl.info/oel2021)。
- en: Tip
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’d like to learn more about reinforcement learning, check out the book
    [*Reinforcement Learning*](https://homl.info/rlbook) by Phil Winder (O’Reilly).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于强化学习的知识，请查看Phil Winder（O'Reilly）的书籍[*强化学习*](https://homl.info/rlbook)。
- en: 'We covered many topics in this chapter: policy gradients, Markov chains, Markov
    decision processes, Q-learning, approximate Q-learning, and deep Q-learning and
    its main variants (fixed Q-value targets, double DQN, dueling DQN, and prioritized
    experience replay), and finally we took a quick look at a few other popular algorithms.
    Reinforcement learning is a huge and exciting field, with new ideas and algorithms
    popping out every day, so I hope this chapter sparked your curiosity: there is
    a whole world to explore!'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了许多主题：策略梯度、马尔可夫链、马尔可夫决策过程、Q 学习、近似 Q 学习、深度 Q 学习及其主要变体（固定 Q 值目标、双重 DQN、对决
    DQN 和优先经验重放），最后我们简要介绍了一些其他流行算法。强化学习是一个庞大且令人兴奋的领域，每天都会涌现出新的想法和算法，因此希望本章引发了您的好奇心：有一个整个世界等待您去探索！
- en: Exercises
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: How would you define reinforcement learning? How is it different from regular
    supervised or unsupervised learning?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您如何定义强化学习？它与常规监督学习或无监督学习有何不同？
- en: Can you think of three possible applications of RL that were not mentioned in
    this chapter? For each of them, what is the environment? What is the agent? What
    are some possible actions? What are the rewards?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能想到本章未提及的三种强化学习的可能应用吗？对于每一种，环境是什么？代理是什么？可能的行动有哪些？奖励是什么？
- en: What is the discount factor? Can the optimal policy change if you modify the
    discount factor?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是折扣因子？如果修改折扣因子，最优策略会改变吗？
- en: How do you measure the performance of a reinforcement learning agent?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何衡量强化学习代理的表现？
- en: What is the credit assignment problem? When does it occur? How can you alleviate
    it?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是信用分配问题？它何时发生？如何缓解它？
- en: What is the point of using a replay buffer?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用重放缓冲区的目的是什么？
- en: What is an off-policy RL algorithm?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是离策略强化学习算法？
- en: Use policy gradients to solve OpenAI Gym’s LunarLander-v2 environment.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略梯度来解决 OpenAI Gym 的 LunarLander-v2 环境。
- en: Use a double dueling DQN to train an agent that can achieve a superhuman level
    at the famous Atari *Breakout* game (`"ALE/Breakout-v5"`). The observations are
    images. To simplify the task, you should convert them to grayscale (i.e., average
    over the channels axis) then crop them and downsample them, so they’re just large
    enough to play, but not more. An individual image does not tell you which way
    the ball and the paddles are going, so you should merge two or three consecutive
    images to form each state. Lastly, the DQN should be composed mostly of convolutional
    layers.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用双重对决 DQN 训练一个代理，使其在著名的 Atari *Breakout* 游戏（`"ALE/Breakout-v5"`）中达到超人水平。观察结果是图像。为了简化任务，您应该将它们转换为灰度图像（即在通道轴上取平均），然后裁剪和降采样，使它们足够大以进行游戏，但不要过大。单个图像无法告诉您球和挡板的移动方向，因此您应该合并两到三个连续图像以形成每个状态。最后，DQN
    应该主要由卷积层组成。
- en: 'If you have about $100 to spare, you can purchase a Raspberry Pi 3 plus some
    cheap robotics components, install TensorFlow on the Pi, and go wild! For an example,
    check out this [fun post](https://homl.info/2) by Lukas Biewald, or take a look
    at GoPiGo or BrickPi. Start with simple goals, like making the robot turn around
    to find the brightest angle (if it has a light sensor) or the closest object (if
    it has a sonar sensor), and move in that direction. Then you can start using deep
    learning: for example, if the robot has a camera, you can try to implement an
    object detection algorithm so it detects people and moves toward them. You can
    also try to use RL to make the agent learn on its own how to use the motors to
    achieve that goal. Have fun!'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您有大约 100 美元可以花费，您可以购买一个树莓派 3 加上一些廉价的机器人组件，在树莓派上安装 TensorFlow，然后尽情玩耍！例如，可以查看
    Lukas Biewald 的这篇 [有趣的帖子](https://homl.info/2)，或者看看 GoPiGo 或 BrickPi。从简单的目标开始，比如让机器人转身找到最亮的角度（如果有光传感器）或最近的物体（如果有声纳传感器），然后朝着那个方向移动。然后您可以开始使用深度学习：例如，如果机器人有摄像头，可以尝试实现一个目标检测算法，使其检测到人并朝他们移动。您还可以尝试使用强化学习，让代理学习如何独立使用电机来实现这个目标。玩得开心！
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解答可在本章笔记本的末尾找到，网址为 [*https://homl.info/colab3*](https://homl.info/colab3)。
- en: '^([1](ch18.html#idm45720166403424-marker)) For more details, be sure to check
    out Richard Sutton and Andrew Barto’s book on RL, *Reinforcement Learning: An
    Introduction* (MIT Press).'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '（1）想了解更多细节，请务必查看 Richard Sutton 和 Andrew Barto 的关于强化学习的书籍 *Reinforcement Learning:
    An Introduction*（麻省理工学院出版社）。'
- en: ^([2](ch18.html#idm45720166401072-marker)) Volodymyr Mnih et al., “Playing Atari
    with Deep Reinforcement Learning”, arXiv preprint arXiv:1312.5602 (2013).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: （2）Volodymyr Mnih 等人，“使用深度强化学习玩 Atari 游戏”，arXiv 预印本 arXiv:1312.5602（2013）。
- en: '^([3](ch18.html#idm45720166399568-marker)) Volodymyr Mnih et al., “Human-Level
    Control Through Deep Reinforcement Learning”, *Nature* 518 (2015): 529–533.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: （3）Volodymyr Mnih 等人，“通过深度强化学习实现人类水平控制”，*自然* 518（2015）：529–533。
- en: ^([4](ch18.html#idm45720166398304-marker)) Check out the videos of DeepMind’s
    system learning to play *Space Invaders*, *Breakout*, and other video games at
    [*https://homl.info/dqn3*](https://homl.info/dqn3).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: （4）查看 DeepMind 系统学习 *Space Invaders*、*Breakout* 和其他视频游戏的视频，网址为 [*https://homl.info/dqn3*](https://homl.info/dqn3)。
- en: ^([5](ch18.html#idm45720166374256-marker)) Images (a), (d), and (e) are in the
    public domain. Image (b) is a screenshot from the *Ms. Pac-Man* game, copyright
    Atari (fair use in this chapter). Image (c) is reproduced from Wikipedia; it was
    created by user Stevertigo and released under [Creative Commons BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: （5）图像（a）、（d）和（e）属于公共领域。图像（b）是来自 *Ms. Pac-Man* 游戏的截图，由 Atari 版权所有（在本章中属于合理使用）。图像（c）是从维基百科复制的；由用户
    Stevertigo 创建，并在 [知识共享署名-相同方式共享 2.0](https://creativecommons.org/licenses/by-sa/2.0)
    下发布。
- en: ^([6](ch18.html#idm45720166354304-marker)) It is often better to give the poor
    performers a slight chance of survival, to preserve some diversity in the “gene
    pool”.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: （6）通常更好地给予表现不佳者一点生存的机会，以保留“基因池”中的一些多样性。
- en: ^([7](ch18.html#idm45720166353536-marker)) If there is a single parent, this
    is called *asexual reproduction*. With two (or more) parents, it is called *sexual
    reproduction*. An offspring’s genome (in this case a set of policy parameters)
    is randomly composed of parts of its parents’ genomes.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch18.html#idm45720166353536-marker)) 如果只有一个父母，这被称为*无性繁殖*。有两个（或更多）父母时，这被称为*有性繁殖*。后代的基因组（在这种情况下是一组策略参数）是随机由其父母的基因组的部分组成的。
- en: ^([8](ch18.html#idm45720166351776-marker)) One interesting example of a genetic
    algorithm used for reinforcement learning is the [*NeuroEvolution of Augmenting
    Topologies*](https://homl.info/neat) (NEAT) algorithm.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch18.html#idm45720166351776-marker)) 用于强化学习的遗传算法的一个有趣例子是[*增强拓扑的神经进化*](https://homl.info/neat)（NEAT）算法。
- en: '^([9](ch18.html#idm45720166346048-marker)) This is called *gradient ascent*.
    It’s just like gradient descent, but in the opposite direction: maximizing instead
    of minimizing.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch18.html#idm45720166346048-marker)) 这被称为*梯度上升*。它就像梯度下降一样，但方向相反：最大化而不是最小化。
- en: ^([10](ch18.html#idm45720166329952-marker)) OpenAI is an artificial intelligence
    research company, funded in part by Elon Musk. Its stated goal is to promote and
    develop friendly AIs that will benefit humanity (rather than exterminate it).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch18.html#idm45720166329952-marker)) OpenAI是一家人工智能研究公司，部分资金来自埃隆·马斯克。其宣称的目标是推广和发展有益于人类的友好人工智能（而不是消灭人类）。
- en: '^([11](ch18.html#idm45720165711216-marker)) Ronald J. Williams, “Simple Statistical
    Gradient-Following Algorithms for Connectionist Reinforcement Leaning”, *Machine
    Learning* 8 (1992) : 229–256.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch18.html#idm45720165711216-marker)) Ronald J. Williams，“用于连接主义强化学习的简单统计梯度跟随算法”，*机器学习*8（1992）：229–256。
- en: '^([12](ch18.html#idm45720164871120-marker)) Richard Bellman, “A Markovian Decision
    Process”, *Journal of Mathematics and Mechanics* 6, no. 5 (1957): 679–684.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch18.html#idm45720164871120-marker)) Richard Bellman，“马尔可夫决策过程”，*数学与力学杂志*6，第5期（1957）：679–684。
- en: ^([13](ch18.html#idm45720163009904-marker)) A great [2018 post](https://homl.info/rlhard)
    by Alex Irpan nicely lays out RL’s biggest difficulties and limitations.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch18.html#idm45720163009904-marker)) Alex Irpan在2018年发表的一篇很棒的[文章](https://homl.info/rlhard)很好地阐述了强化学习的最大困难和局限性。
- en: '^([14](ch18.html#idm45720162889472-marker)) Hado van Hasselt et al., “Deep
    Reinforcement Learning with Double Q-Learning”, *Proceedings of the 30th AAAI
    Conference on Artificial Intelligence* (2015): 2094–2100.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch18.html#idm45720162889472-marker)) Hado van Hasselt等人，“双Q学习的深度强化学习”，*第30届AAAI人工智能大会论文集*（2015）：2094–2100。
- en: ^([15](ch18.html#idm45720162777584-marker)) Tom Schaul et al., “Prioritized
    Experience Replay”, arXiv preprint arXiv:1511.05952 (2015).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch18.html#idm45720162777584-marker)) Tom Schaul等人，“优先经验重放”，arXiv预印本arXiv:1511.05952（2015）。
- en: ^([16](ch18.html#idm45720162769408-marker)) It could also just be that the rewards
    are noisy, in which case there are better methods for estimating an experience’s
    importance (see the paper for some examples).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch18.html#idm45720162769408-marker)) 也可能只是奖励有噪音，此时有更好的方法来估计经验的重要性（请参阅论文中的一些示例）。
- en: ^([17](ch18.html#idm45720162754592-marker)) Ziyu Wang et al., “Dueling Network
    Architectures for Deep Reinforcement Learning”, arXiv preprint arXiv:1511.06581
    (2015).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch18.html#idm45720162754592-marker)) Ziyu Wang等人，“用于深度强化学习的对抗网络架构”，arXiv预印本arXiv:1511.06581（2015）。
- en: '^([18](ch18.html#idm45720162553536-marker)) Matteo Hessel et al., “Rainbow:
    Combining Improvements in Deep Reinforcement Learning”, arXiv preprint arXiv:1710.02298
    (2017): 3215–3222.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch18.html#idm45720162553536-marker)) Matteo Hessel等人，“彩虹：深度强化学习改进的结合”，arXiv预印本arXiv:1710.02298（2017）：3215–3222。
- en: '^([19](ch18.html#idm45720162511200-marker)) David Silver et al., “Mastering
    the Game of Go with Deep Neural Networks and Tree Search”, *Nature* 529 (2016):
    484–489.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch18.html#idm45720162511200-marker)) David Silver等人，“用深度神经网络和树搜索掌握围棋”，*自然*529（2016）：484–489。
- en: '^([20](ch18.html#idm45720162506192-marker)) David Silver et al., “Mastering
    the Game of Go Without Human Knowledge”, *Nature* 550 (2017): 354–359.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch18.html#idm45720162506192-marker)) David Silver等人，“在没有人类知识的情况下掌握围棋”，*自然*550（2017）：354–359。
- en: ^([21](ch18.html#idm45720162504560-marker)) David Silver et al., “Mastering
    Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm”,
    arXiv preprint arXiv:1712.01815.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch18.html#idm45720162504560-marker)) David Silver等人，“通过自我对弈掌握国际象棋和将棋的一般强化学习算法”，arXiv预印本arXiv:1712.01815。
- en: ^([22](ch18.html#idm45720162502384-marker)) Julian Schrittwieser et al., “Mastering
    Atari, Go, Chess and Shogi by Planning with a Learned Model”, arXiv preprint arXiv:1911.08265
    (2019).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch18.html#idm45720162502384-marker)) Julian Schrittwieser等人，“通过学习模型计划掌握Atari、围棋、国际象棋和将棋”，arXiv预印本arXiv:1911.08265（2019）。
- en: '^([23](ch18.html#idm45720162497280-marker)) Volodymyr Mnih et al., “Asynchronous
    Methods for Deep Reinforcement Learning”, *Proceedings of the 33rd International
    Conference on Machine Learning* (2016): 1928–1937.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch18.html#idm45720162497280-marker)) Volodymyr Mnih等人，“深度强化学习的异步方法”，*第33届国际机器学习会议论文集*（2016）：1928–1937。
- en: '^([24](ch18.html#idm45720162490480-marker)) Tuomas Haarnoja et al., “Soft Actor-Critic:
    Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor”,
    *Proceedings of the 35th International Conference on Machine Learning* (2018):
    1856–1865.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch18.html#idm45720162490480-marker)) Tuomas Haarnoja等人，“软演员-评论家：带有随机演员的离策略最大熵深度强化学习”，*第35届国际机器学习会议论文集*（2018）：1856–1865。
- en: ^([25](ch18.html#idm45720162487536-marker)) John Schulman et al., “Proximal
    Policy Optimization Algorithms”, arXiv preprint arXiv:1707.06347 (2017).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch18.html#idm45720162487536-marker)) John Schulman等人，“近端策略优化算法”，arXiv预印本arXiv:1707.06347（2017）。
- en: '^([26](ch18.html#idm45720162483456-marker)) John Schulman et al., “Trust Region
    Policy Optimization”, *Proceedings of the 32nd International Conference on Machine
    Learning* (2015): 1889–1897.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch18.html#idm45720162483456-marker)) John Schulman等人，“信任区域策略优化”，*第32届国际机器学习会议论文集*（2015）：1889–1897。
- en: '^([27](ch18.html#idm45720162479024-marker)) Deepak Pathak et al., “Curiosity-Driven
    Exploration by Self-Supervised Prediction”, *Proceedings of the 34th International
    Conference on Machine Learning* (2017): 2778–2787.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch18.html#idm45720162479024-marker)) Deepak Pathak等，“由自监督预测驱动的好奇心探索”，*第34届国际机器学习会议论文集*（2017）：2778–2787。
- en: '^([28](ch18.html#idm45720162472416-marker)) Rui Wang et al., “Paired Open-Ended
    Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning
    Environments and Their Solutions”, arXiv preprint arXiv:1901.01753 (2019).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch18.html#idm45720162472416-marker)) 王锐等，“配对开放式先驱者（POET）：不断生成越来越复杂和多样化的学习环境及其解决方案”，arXiv预印本arXiv:1901.01753（2019）。
- en: '^([29](ch18.html#idm45720162468448-marker)) Rui Wang et al., “Enhanced POET:
    Open-Ended Reinforcement Learning Through Unbounded Invention of Learning Challenges
    and Their Solutions”, arXiv preprint arXiv:2003.08536 (2020).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch18.html#idm45720162468448-marker)) 王锐等，“增强POET：通过无限创造学习挑战及其解决方案的开放式强化学习”，arXiv预印本arXiv:2003.08536（2020）。
- en: ^([30](ch18.html#idm45720162466960-marker)) Open-Ended Learning Team et al.,
    “Open-Ended Learning Leads to Generally Capable Agents”, arXiv preprint arXiv:2107.12808
    (2021).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch18.html#idm45720162466960-marker)) Open-Ended Learning Team等，“开放式学习导致普遍能力代理”，arXiv预印本arXiv:2107.12808（2021）。
