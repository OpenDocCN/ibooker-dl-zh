- en: Chapter 9\. Scalable Inference Serving on Cloud with TensorFlow Serving and
    KubeFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine this: you just built a top-notch classifier. Your goal, as the Silicon
    Valley motto goes, is to “*make the world a better place*,” which you’re going
    to do... with a spectacular Dog/Cat classifier. You have a solid business plan
    and you cannot wait to pitch your magical classifier to that venture capital firm
    next week. You know that the investors will question you about your cloud strategy,
    and you need to show a solid demo before they even consider giving you the money.
    How would you do this? Creating a model is half the battle, serving it is the
    next challenge, often the bigger one. In fact, for a long time it was common for
    training a model to only take a few weeks, but trying to serve it to a larger
    group of people was a months-long battle, often involving backend engineers and
    DevOps teams.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we answer a few questions that tend to come up in the context
    of hosting and serving custom-built models.
  prefs: []
  type: TYPE_NORMAL
- en: How can I host my model on my personal server so that my coworkers can play
    with it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I am not a backend/infrastructure engineer, but I want to make my model available
    so that it can serve thousands (or even millions) of users. How can I do this
    at a reasonable price without worrying about scalability and reliability issues?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are reasons (such as cost, regulations, privacy, etc.) why I cannot host
    my model on the cloud, but only on-premises (my work network). Can I serve predictions
    at scale and reliably in such a case?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can I do inference on GPUs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much can I expect to pay for each of these options?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Could I scale my training and serving across multiple cloud providers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much time and technical know-how will it take to get these running?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin our journey by looking at the high-level overview of the tools available
    to us.
  prefs: []
  type: TYPE_NORMAL
- en: Landscape of Serving AI Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a multitude of tools, libraries, and cloud services available for getting
    trained AI models to serve prediction requests. [Figure 9-1](part0011.html#a_high-level_overview_and_comparison_of)
    simplifies them into four categories.
  prefs: []
  type: TYPE_NORMAL
- en: '![A high-level overview and comparison of different inference serving options.](../images/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. A high-level overview and comparison of different inference serving
    options
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Depending on our inference scenarios, we can make an appropriate choice. [Table 9-1](part0011.html#tools_to_serve_deep_learning_models_over)
    takes a deeper look.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-1\. Tools to serve deep learning models over the network
  prefs: []
  type: TYPE_NORMAL
- en: '| **Category and examples** | **Expected time to first prediction** | **Pros
    and cons** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| HTTP servers'
  prefs: []
  type: TYPE_NORMAL
- en: Flask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Django
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache OpenWhisk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python `http.server`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| <5 minutes | + Simple to run+ Often runs current Python code– Slow– Not optimized
    for AI |'
  prefs: []
  type: TYPE_TB
- en: '| **Hosted and managed cloud stacks**'
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Sage Maker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithmia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| <15 minutes | + Easier GUI/command-line interfaces+ Highly scalable+ Fully
    managed, reduces the need for DevOps teams– Usually limited to CPU-based inference,
    can be slow for large models– Warm-up query time can be slow |'
  prefs: []
  type: TYPE_TB
- en: '| **Manually managed serving libraries**'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA TensorRT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepDetect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MXNet Model Serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skymind Intelligence Layer with DeepLearning4J
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seldon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepStack AI Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| <15 minutes | + High performance+ Allows manual controls on optimizations,
    batching, etc.+ Can run inference on GPU– More involved setup– Scaling over multiple
    nodes usually requires extra groundwork |'
  prefs: []
  type: TYPE_TB
- en: '| **Cloud AI orchestration frameworks**'
  prefs: []
  type: TYPE_NORMAL
- en: KubeFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ~1 hour | + Makes scaling training and inference easy to manage+ Portability
    between cloud providers+ Consistent environments across development and production+
    For data scientists, integration with familiar tools such as Jupyter Notebooks
    for sending models to production+ Enables composing conditional pipelines to automate
    testing, cascading models+ Uses existing manually managed serving libraries– Still
    evolving– For beginners, hosted and managed cloud stacks offer an easier learning
    curve |'
  prefs: []
  type: TYPE_TB
- en: In this chapter, we explore a range of tools and scenarios. Some of these options
    are easy to use but limited in functionality. Others offer more granular controls
    and higher performance but are more involved to set up. We look at one example
    of each category and take a deeper dive to develop an intuition into when using
    one of those makes sense. We then present a cost analysis of the different solutions
    as well as case studies detailing how some of these solutions work in practice
    today.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flask: Build Your Own Server'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin with the most basic technique of *Build Your Own Server* (BYOS). From
    the choices presented in the first column of [Table 9-1](part0011.html#tools_to_serve_deep_learning_models_over),
    we’ve selected Flask.
  prefs: []
  type: TYPE_NORMAL
- en: Making a REST API with Flask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flask is a Python-based web application framework. Released in 2010 and with
    more than 46,000 stars on GitHub, it is under continuous development. It’s also
    quick and easy to set up and is really useful for prototyping. It is often the
    framework of choice for data science practitioners when they want to serve their
    models to a limited set of users (e.g., sharing with coworkers on a corporate
    network) without a lot of fuss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing Flask with `pip` is fairly straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon installation, we should be able to run the following simple “Hello World”
    program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the command to run the “Hello World” program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: By default, Flask runs on port 5000\. When we open the URL *http://localhost:5000/hello*
    in the browser, we should see the words “Hello World!,” as shown in [Figure 9-2](part0011.html#navigate_to_httpcolonsolidussoliduslocal).
  prefs: []
  type: TYPE_NORMAL
- en: '![Navigate to http://localhost:5000/hello within a web browser to view the
    “Hello World!” web page](../images/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Navigate to http://localhost:5000/hello within a web browser to
    view the “Hello World!” web page
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, it takes barely more than a few lines to get a simple web application
    up and running. One of the most important lines in that script is `@app.route("/hello")`.
    It specifies that the path `/hello` after the hostname would be served by the
    method immediately beneath it. In our case, it merely returns the string “Hello
    World!” In the next step, we look at how to deploy a Keras model to a Flask server
    and create a route that will serve predictions by our model.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Keras Model to Flask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first step is to load our Keras model. The following lines load the model
    from the .*h5* file. You’ll find the scripts for this chapter on the book’s GitHub
    (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)) in
    *code/chapter-9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create the route */infer* that would support inference on our images.
    Naturally, we would support `POST` requests to accept images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To test the inference, let’s use the `curl` command, as follows, on a sample
    image containing a dog:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, we get a prediction of “dog.” This has worked quite well so far.
    At this point, Flask runs only locally; that is, someone else on the network would
    not be able to make a request to this server. To make Flask available to others,
    we can simply change `app.run()` to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can give access to our model to anyone within our network.
    The next question would be—can we do the same to make the model available to the
    general public? The answer to that question is an emphatic no! The Flask website
    has a prominent warning stating *“WARNING: Do not use the development server in
    a production environment.”* Flask indeed does not support production work out
    of the box and would need custom code to enable that. In the upcoming sections,
    we look at how to host our models on systems that are meant for production use.
    With all of this in mind, let’s recap some of the pros and cons of using Flask.'
  prefs: []
  type: TYPE_NORMAL
- en: Pros of Using Flask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Flask provides some advantages, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: Quick to set up and to prototype
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast development cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightweight on resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Broad appeal within the Python community
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons of Using Flask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the same time, Flask might not be your best choice, for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Cannot scale; by default, it is not meant for production loads. Flask can serve
    only one request at one time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not handle model versioning out of the box
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not support batching of requests out of the box
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Desirable Qualities in a Production-Level Serving System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For any cloud service that is serving traffic from the public, there are certain
    attributes that we want to look for when deciding to use a solution. In the context
    of machine learning, there are additional qualities that we would look for while
    building inference services. We look at a few of them if this section.
  prefs: []
  type: TYPE_NORMAL
- en: High Availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our users to trust our service, it must be available almost always. For
    many serious players, they measure their availability metric in terms of “*number
    of nines*.” If a business claims that its service has four 9s availability, they
    mean the system is up and available 99.99% of the time. Even though 99% sounds
    impressive, [Table 9-2](part0011.html#downtime_per_year_for_different_availabi)
    puts that downtime per year in perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-2\. Downtime per year for different availability percentages
  prefs: []
  type: TYPE_NORMAL
- en: '| **Availability %** | **Downtime per year** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 99% (“two nines”) | 3.65 days |'
  prefs: []
  type: TYPE_TB
- en: '| 99.9% (“three nines”) | 8.77 hours |'
  prefs: []
  type: TYPE_TB
- en: '| 99.99% (“four nines”) | 52.6 minutes |'
  prefs: []
  type: TYPE_TB
- en: '| 99.999% (“five nines”) | 5.26 minutes |'
  prefs: []
  type: TYPE_TB
- en: Imagine how ridiculous the situation would be if a major website like Amazon
    were only 99.9% available, losing millions in user revenue during the eight-plus
    hours of downtime. Five 9s is considered the holy grail. Anything less than three
    9s is typically unsuitable for a high-quality production system.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traffic handled by production services is almost never uniform across a larger
    time period. For example, the *New York Times* experiences significantly more
    traffic during morning hours, whereas Netflix typically experiences a surge in
    traffic between the evening and late-night hours, when people chill. There are
    also seasonal factors in traffic. Amazon experiences orders of magnitude more
    traffic on Black Friday and during Christmas season.
  prefs: []
  type: TYPE_NORMAL
- en: 'A higher demand requires a higher amount of resources being available and online
    to serve them. Otherwise, the availability of the system would be in jeopardy.
    A naive way to accomplish this would be to anticipate the highest volume of traffic
    the system would ever serve, determine the number of resources necessary to serve
    that level of traffic, and then allocate that amount all the time, in perpetuity.
    There are two problems with this approach: 1) if your planning was correct, the
    resources would be underutilized most of the time, essentially burning money;
    and 2) if your estimation was insufficient, you might end up affecting the availability
    of your service and end up with a far worse problem of losing the trust of your
    customers and ultimately their wallets.'
  prefs: []
  type: TYPE_NORMAL
- en: A smarter way to manage traffic loads is to monitor them as they are coming
    in and dynamically allocate and deallocate resources that are available for service.
    This ensures that the increased traffic is handled without loss of service while
    keeping operating costs to a minimum during low-traffic times.
  prefs: []
  type: TYPE_NORMAL
- en: When scaling down resources, any resource that is about to be deallocated is
    quite likely to be processing traffic at that moment. It’s essential to ensure
    that all of those requests be completed before shutting down that resource. Also,
    crucially, the resource must not process any new requests. This process is called
    *draining*. Draining is also crucial when machines are taken down for routine
    maintenance and/or upgrades.
  prefs: []
  type: TYPE_NORMAL
- en: Low Latency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider these facts. Amazon published a study in 2008 in which it found that
    every 100 ms increase in latency in its retail website resulted in a 1% loss of
    profit. A one-second delay in loading the website caused a whopping $1.6 billion
    in lost revenue! Google found that a 500 ms latency on mobile websites resulted
    in a traffic drop of 20%. In other words, a 20% decrease in the opportunity-to-serve
    advertisements. And this does not affect only industry giants. If a web page takes
    longer than three seconds to load on a mobile phone, 53% of users abandon it (according
    to a 2017 study by Google). It’s clear that time is money.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting average latency can be misleading because it might paint a cheerier
    picture than a ground reality. It’s like saying if Bill Gates walks into a room,
    everyone is a billionaire on average. Instead, percentile latency is the typically
    reported metric. For example, a service might report 987 ms @ 99th percentile.
    This means that 99% of the requests were served in 987 ms or less. The very same
    system could have a 20 ms latency on average. Of course, as traffic to your service
    increases, the latency might increase if the service is not scaled up to give
    adequate resources. As such, latency, high availability, and scalability are intertwined.
  prefs: []
  type: TYPE_NORMAL
- en: Geographic Availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The distance between New York and Sydney is nearly 10,000 miles (16,000 km).
    The speed of light in a vacuum is roughly 186,282 miles per second (300,000 km
    per second). Silica glass (used in fiber-optic cables) decreases the speed of
    light by a factor of roughly 30% down to 130,487 miles per second (210,000 km
    per second). On a piece of fiber-optic running in a straight line between these
    two cities, the roundtrip travel time alone for a single request is nearly 152
    ms. Keep in mind that this does not account for the amount of time it takes for
    the request to be processed at the server, or the hops that the packets need to
    make across multiple routers along the way. This level of service would be unacceptable
    for many applications.
  prefs: []
  type: TYPE_NORMAL
- en: Services that expect to be used throughout the world must be strategically located
    to minimize latency for the users in those regions. Additionally, resources can
    be dynamically scaled up or down depending on local traffic, thus giving more
    granular control. The major cloud providers have a presence on at least five continents
    (sorry penguins!).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Want to simulate how long the incoming requests would take from your computer
    to a particular datacenter around the world? [Table 9-3](part0011.html#latency_measurement_tools_for_different)
    lists a few handy browser-based tools offered by cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-3\. Latency measurement tools for different cloud providers
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service** | **Cloud provider** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [AzureSpeed.com](http://AzureSpeed.com) | Microsoft Azure |'
  prefs: []
  type: TYPE_TB
- en: '| [CloudPing.info](https://CloudPing.info) | Amazon Web Services |'
  prefs: []
  type: TYPE_TB
- en: '| [GCPing.com](http://GCPing.com) | Google Cloud Platform |'
  prefs: []
  type: TYPE_TB
- en: Additionally, to determine realistic combinations of latency from one location
    to another, *CloudPing.co* measures AWS Inter-Region Latency, between more than
    16 US-based AWS datacenters to one another.
  prefs: []
  type: TYPE_NORMAL
- en: Failure Handling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There’s an old saying that there are only two things that are assured in life—death
    and taxes. In the twenty-first century, this adage applies not just to humans
    but also computer hardware. Machines fail all the time. The question is never
    *if* a machine will fail, it’s *when*. One of the necessary qualities of production-quality
    service is its ability to gracefully handle failures. If a machine goes down,
    quickly bring up another machine to take its place and continue serving traffic.
    If an entire datacenter goes down, seamlessly route traffic to another datacenter
    so that users don’t even realize that anything bad happened in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you can’t measure it, you can’t improve it. Worse, does it even exist? Monitoring
    the number of requests, availability, latency, resource usage, number of nodes,
    distribution of traffic, and location of users is vital to understanding how a
    service performs; finding opportunities to improve it; and, more importantly,
    how much to pay. Most cloud providers already have built-in dashboards providing
    these metrics. Additionally, recording task-specific analytics like time for model
    inference, preprocessing, and so on can add another level of understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Model Versioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have learned in this book (and will continue to learn all the way to the
    last page) that machine learning is always iterative. Particularly in the case
    of applications in the real world, data that the model can learn on is constantly
    being generated. Moreover, the incoming data distribution might shift over time
    compared to what it was trained on, leading to lower prediction power (a phenomenon
    called c*oncept drift*). To provide users with the best possible experience, we
    want to keep improving our models. Every time we train our model with newer data
    to further improve its accuracy and make the best version yet, we want to make
    it available for our users as quickly and seamlessly as possible. Any good production-quality
    inference system should provide the ability to provide different versions of a
    model, including the ability to swap a live version of the model with another
    version at a moment’s notice.
  prefs: []
  type: TYPE_NORMAL
- en: A/B Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to supporting multiple versions of a model, there are reasons we’d
    want to serve different versions of the model at the same time depending on a
    variety of attributes such as the geographic location of the user, demographics,
    or simply by random assignment.
  prefs: []
  type: TYPE_NORMAL
- en: '*A/B testing* is a particularly useful tool when improving a model. After all,
    if our spanking new model was flawed in some way, we’d rather it be deployed to
    only a small subset of our users rather than 100% of them at the same time before
    we catch the flaw. Additionally, if a model meets criteria for success on that
    small subset, it provides validation for the experiment and justifies eventually
    being promoted to all users.'
  prefs: []
  type: TYPE_NORMAL
- en: Support for Multiple Machine Learning Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Last but not least, we don’t want to be locked into a single machine learning
    library. Some data scientists in an organization might train models in PyTorch,
    others in TensorFlow, or maybe scikit-Learn suffices for non–deep learning tasks.
    The flexibility to support multiple libraries would be a welcome bonus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Cloud ML Engine: A Managed Cloud AI Serving Stack'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Considering all the desirable qualities we discussed in a production environment
    in the previous section, it’s generally not a good idea to use Flask for serving
    users. If you do not have a dedicated infrastructure team and would like to spend
    more time making better models than deploying them, using a managed cloud solution
    is the right approach. There are several cloud-based Inference-as-a-Service solutions
    on the market today. We have chosen to explore the Google Cloud ML Engine partly
    because of the convenient TensorFlow integration and partly because it ties in
    nicely with the ML Kit material that we touch upon in [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b).
  prefs: []
  type: TYPE_NORMAL
- en: Pros of Using Cloud ML Engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Easy-to-deploy models in production with web-based GUI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powerful and easily scalable to millions of users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides deep insights into model usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to version models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons of Using Cloud ML Engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: High latency, offers only CPUs for inference (as of August 2019)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsuitable for scenarios involving legal and data privacy issues where the data
    must not leave the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imposes restrictions on architecture design of complex applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Classification API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following step-by-step guide shows how to go about uploading and hosting
    our a Dog/Cat classifier model on Google Cloud ML Engine:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a model on the Google Cloud ML Engine dashboard at [*https://console.cloud.google.com/mlengine/models*](https://console.cloud.google.com/mlengine/models).
    Because this is the first time we’re using the dashboard, we need to click ENABLE
    API, as depicted in [Figure 9-3](part0011.html#listing_page_for_machine_learning_models).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Listing page for machine learning models on the Google Cloud ML Engine dashboard](../images/00005.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9-3\. Listing page for machine learning models on the Google Cloud ML
    Engine dashboard
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Give the model a name and a description ([Figure 9-4](part0011.html#model_creation_page_on_google_cloud_ml_e)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Model creation page on Google Cloud ML Engine](../images/00047.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9-4\. Model creation page on Google Cloud ML Engine
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: After the model is created, we can access the model on the listing page ([Figure 9-5](part0011.html#model_listings_page_on_google_cloud_ml_e)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Model listings page on Google Cloud ML Engine](../images/00295.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9-5\. Model listings page on Google Cloud ML Engine
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Click the model to go to the model details page ([Figure 9-6](part0011.html#details_page_of_the_just-created_dogcat))
    and add a new version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Details page of the just-created DogCat classifier](../images/00298.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9-6\. Details page of the just-created Dog/Cat classifier
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Fill out the necessary information to create the new version. The last field
    at the bottom requires you to upload the model to Google Cloud Storage before
    we can use it. Click the Browse button to create a new bucket for storing the
    model ([Figure 9-7](part0011.html#creating_a_new_version_for_a_machine_lea)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Creating a new version for a machine learning model](../images/00255.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9-7\. Creating a new version for a machine learning model
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Create a new bucket with a unique name, a storage class, and region. After you
    create this bucket, go to [*https://console.cloud.google.com/storage/browser*](https://console.cloud.google.com/storage/browser)
    (in a separate tab while keeping the current one open) to find this newly created
    bucket and upload the model there ([Figure 9-8](part0011.html#creating_a_new_google_cloud_storage_buck)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Creating a new Google Cloud Storage bucket within the ML model version creation
    page](../images/00216.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9-8\. Creating a new Google Cloud Storage bucket within the ML model
    version creation page
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Our Dog/Cat classifier model is an *.h5* file. However, Google Cloud expects
    a SavedModel file. You can find the script to convert the *.h5* file to SavedModel
    on the book’s GitHub repository (see [http://PracticalDeepLearning.ai](http://PracticalDeepLearning.ai))
    at *code/chapter-9/scripts/h5_to_tf.ipynb*. Simply load the model and execute
    the rest of the notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Google Cloud Storage browser, upload the newly converted model ([Figure 9-9](part0011.html#google_cloud_storage_browser_page_showin))
    to the bucket you created in step 6.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Google Cloud Storage Browser page showing the uploaded DogCat classifier
    model in TensorFlow format](../images/00149.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9-9\. Google Cloud Storage Browser page showing the uploaded Dog/Cat
    classifier model in TensorFlow format
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Specify the URI on the model version creation page for the model that you just
    uploaded ([Figure 9-10](part0011.html#add_the_uri_for_the_model_you_uploaded_t)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Add the URI for the model you uploaded to Google Cloud Storage](../images/00139.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9-10\. Add the URI for the model you uploaded to Google Cloud Storage
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Click the Save button and wait for the model version to be created. As soon
    as the model version is created, you can begin making predictions against it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If it’s not already present on your machine, you can download and install the
    Google Cloud SDK from the installation website at [*https://cloud.google.com/sdk/install*](https://cloud.google.com/sdk/install).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can use the Cloud ML Engine REST API to make your requests. However, for
    brevity, use the command-line tools in the Cloud SDK. You first need to convert
    your image into a *request.json* file using the *image-to-json.py* script located
    at *code/chapter-9*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, use the *request.json* file created in the previous step to execute a
    request against our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see from the output, we get similar results as with our Flask server;
    that is, a prediction of “dog” with 85% confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If this is your first time using **`gcloud`**, you need to run the following
    command to tie the command-line tool to your Google account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, select the project using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Piece of cake, wasn’t it? In our example, we used the Google Cloud SDK to request
    a prediction for the sake of brevity. In a production scenario, you would want
    to execute the same prediction request using Google’s API endpoints, instead;
    either by generating HTTP requests or by using their client libraries. We can
    follow the documentation on Google Cloud Docs for production scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the model is ready to be served to any user anywhere in the world
    using applications on the browser, mobile and edge devices, desktop, as well as
    cloud environments. Using a hosted stack is a pretty viable option for individuals
    and organizations who want the flexibility and reliability that the cloud provides
    while having to do minimal setup and maintenance work for the infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, there are situations for which a hosted solution might not be the
    best approach. Reasons could include pricing models, data privacy issues, legal
    questions, technical issues, trust concerns, or contractual obligations. In such
    cases, a solution that is hosted and managed locally (or, “on-premises”) would
    be preferable.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For processing a large number of images at one time, you can modify *image-to-json.py*
    to create a *request.json* file that contains an array of multiple inputs.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow Serving is an open source library in the TensorFlow ecosystem for
    serving machine learning models fast. Unlike Flask, it’s built for performance,
    with low overhead, and designed for use in production. TensorFlow Serving is widely
    used by large companies to serve their models for prediction services. It is one
    of the integral components of TensorFlow Extended (TFX)—an end-to-end deep learning
    pipeline in the TensorFlow ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw when we looked at the desired qualities of a production system, TensorFlow
    serving offers low latency, failure handling, high throughput, and model versioning.
    Another benefit includes the ability to serve multiple models at the same time
    on the same service. It implements several techniques to speed up serving:'
  prefs: []
  type: TYPE_NORMAL
- en: During server startup, it starts a burst of threads for fast model loading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses separate thread pools for loading models and for serving inferences
    while giving higher priority to the threads in the inference pool. This is crucial
    to lowering request latency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It builds minibatches of incoming asynchronous requests for short periods of
    time. As we have seen, with the power of batching data on GPUs during training,
    it aims to bring similar efficiencies on asynchronous requests. As an example,
    waiting 500 ms to group together several requests for inference. While at worst
    case, this adds a 500 ms penalty for the first request in the batch, it reduces
    the average latency across requests and maximizes hardware utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TensorFlow serving gives you full control over the model rollout procedure.
    You can serve different models or different versions of the same kind of model
    in the same process. You just need to make sure that you know the name and location
    of the version that you want to remove or put into production.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few different ways of setting up TensorFlow Serving:'
  prefs: []
  type: TYPE_NORMAL
- en: Building from source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and installing using APT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Docker images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re feeling adventurous, building from source might be the danger you
    seek. But if you just want to get up and running quickly, we recommend using Docker
    because it requires minimal steps to get the system up and running. What is Docker,
    you might ask? Docker provides virtualization of a Linux environment for applications
    that run within it. It provides isolation of resources that essentially operate
    as a clean slate for setting up an environment in which an application can run.
    Typically, an application and all of its dependencies are packaged into a single
    Docker container that can then be deployed repeatedly as necessary. Because the
    application is set up in a clean environment, it reduces the likelihood of configuration
    and deployment errors. This makes Docker very well suited for running applications
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest benefit that Docker provides for us is alleviating “dependency hell”
    because all the necessary dependencies are packaged within the container. One
    additional advantage of using Docker is that the process of setting up your application
    remains more or less the same across different platforms, whether you use Windows,
    Linux, or Mac.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Docker installation instructions, depending on the target platform, are
    available on the Docker home page. This should not take more than a few minutes
    because the setup is fairly straightforward. After you’ve installed Docker, you
    can run the following command to set up TensorFlow Serving for CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For GPU-enabled machines, run the following command, instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In either case, if everything went smoothly, you should have a REST API running
    on your local port 8501 serving our Dog/Cat classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In any inference request, the end-to-end latency is a summation of time taken
    by multiple steps along the process. This includes round-trip network time, request
    time to serialize/deserialize the request and response objects, and, of course,
    time to perform the actual inference. One more component that adds overhead is
    the serving framework; that is, TensorFlow Serving. Google claims that the overhead
    contributed by TensorFlow Serving is minimal. In its experiments, it observed
    that TensorFlow Serving alone was able to handle approximately 100,000 QPS per
    core on a 16 vCPU Intel Xeon E5 2.6 GHz machine. Because it is measuring the overhead,
    this excludes the remote procedure call (RPC) time and the TensorFlow inference
    processing time.
  prefs: []
  type: TYPE_NORMAL
- en: Even though TensorFlow Serving is a great choice for serving inferences from
    a single machine, it does not have built-in functionality for horizontal scaling.
    Instead, it is built to be used in conjunction with other systems that can supercharge
    TensorFlow Serving with dynamic scaling. We explore one such solution in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: KubeFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we have explored the various steps of an end-to-end deep
    learning pipeline, from data ingestion, analysis, distributed training (including
    hyperparameter tuning) at scale, tracking experiments, deployment, and eventually
    to serving prediction requests at scale. Each of these steps is complex in its
    own right, with its set of tools, ecosystems, and areas of expertise. People dedicate
    their lifetimes developing expertise in just one of these fields. It’s not exactly
    a walk in the park. The combinatorial explosion of the know-how required when
    factoring for the necessary backend engineering, hardware engineering, infrastructure
    engineering, dependency management, DevOps, fault tolerance, and other engineering
    challenges can result in a very expensive hiring process for most organizations.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous section, Docker saves us the hassle of dependency
    management by making portable containers available. It helps us make TensorFlow
    Serving available across platforms easily without having to build it from source
    code or install dependencies manually. Great! But it still doesn’t have an answer
    to many of the other challenges. How are we going to scale up containers to match
    rises in demand? How would we efficiently distribute traffic across containers?
    How do we ensure that the containers are visible to one another and can communicate?
  prefs: []
  type: TYPE_NORMAL
- en: These are questions answered by *Kubernetes*. Kubernetes is an orchestration
    framework for automatically deploying, scaling, and managing containers (like
    Docker). Because it takes advantage of the portability offered by Docker, we can
    use Kubernetes to deploy to developer laptops as well as thousand-machine clusters
    in an almost identical manner. This helps maintain consistency across different
    environments, with the added benefit of scalability in an accessible manner. It
    is worth noting that Kubernetes is not a dedicated solution for machine learning
    (neither is Docker); rather, it is a general-purpose solution to many of the problems
    faced in software development, which we use in the context of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: But let’s not get ahead of ourselves just yet. After all, if Kubernetes were
    the be-all and end-all solution, it would have appeared in the chapter title!
    A machine learning practitioner using Kubernetes still needs to assemble all of
    the appropriate sets of containers (for training, deployment, monitoring, API
    management, etc.) that then need to be orchestrated together to make a fully functioning
    end-to-end pipeline. Unfortunately, many data scientists are trying to do exactly
    this in their own silos, reinventing the wheel building ad hoc machine learning-specific
    pipelines. Couldn’t we save everyone the trouble and make one Kubernetes-based
    solution for machine learning scenarios?
  prefs: []
  type: TYPE_NORMAL
- en: Enter *KubeFlow*, which promises to automate a large chunk of these engineering
    challenges and hide the complexity of running a distributed, scalable, end-to-end
    deep learning system behind a web GUI-based tool and a powerful command-line tool.
    This is more than just an inference service. Think of it as a large ecosystem
    of tools that can interoperate seamlessly and, more importantly, scale up with
    demand. KubeFlow is built for the cloud. Though not just one cloud—it’s built
    to be compatible with all major cloud providers. This has significant implications
    on cost. Because we are not tied to a specific cloud provider, we have the freedom
    to move all of our operations at a moment’s notice if a competing cloud provider
    drops its prices. After all, competition benefits consumers.
  prefs: []
  type: TYPE_NORMAL
- en: KubeFlow supports a variety of hardware infrastructure, from developer laptops
    and on-premises datacenters, all the way to public cloud services. And because
    it’s built on top of Docker and Kubernetes, we can rest assured that the environments
    will be identical whether deployed on a developer laptop or a large cluster in
    a datacenter. Every single way in which the developer setup is different from
    the production environment could result in an outage, so it’s really valuable
    to have this consistency across environments.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 9-4](part0011.html#tools_available_on_kubeflow) shows a brief list of
    readily available tools within the KubeFlow ecosystem.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-4\. Tools available on KubeFlow
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tool** | **Functionality** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Jupyter Hub | Notebook environment |'
  prefs: []
  type: TYPE_TB
- en: '| TFJob | Training TensorFlow models |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow Serving | Serving TensorFlow models |'
  prefs: []
  type: TYPE_TB
- en: '| Seldon | Serving models |'
  prefs: []
  type: TYPE_TB
- en: '| NVIDIA TensorRT | Serving models |'
  prefs: []
  type: TYPE_TB
- en: '| Intel OpenVINO | Serving models |'
  prefs: []
  type: TYPE_TB
- en: '| KFServing | Abstraction for serving Tensorflow, XGBoost, scikit-learn, PyTorch,
    and ONNX models |'
  prefs: []
  type: TYPE_TB
- en: '| Katib | Hyperparameter tuning and NAS |'
  prefs: []
  type: TYPE_TB
- en: '| Kubebench | Running benchmarking jobs |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | Training PyTorch models |'
  prefs: []
  type: TYPE_TB
- en: '| Istio | API services, authentication, A/B testing, rollouts, metrics |'
  prefs: []
  type: TYPE_TB
- en: '| Locust | Load testing |'
  prefs: []
  type: TYPE_TB
- en: '| Pipelines | Managing experiments, jobs, and runs, scheduling machine learning
    workflows |'
  prefs: []
  type: TYPE_TB
- en: As the joke goes in the community, with so many technologies prepackaged, KubeFlow
    finally makes our résumés buzzword- (and recruiter-) compliant.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many people assume that KubeFlow is a combination of Kubernetes and TensorFlow,
    which, as you have seen, is not the case. It is that and much more.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two important parts to KubeFlow that make it unique: pipelines and
    fairing.'
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pipelines give us the ability to compose steps across the machine learning to
    schedule complex workflows. [Figure 9-11](part0011.html#an_end-to-end_pipeline_illustrated_in_ku)
    shows us an example of a pipeline. Having visibility into the pipeline through
    a GUI tool helps stakeholders understand it (beyond just the engineers who built
    it).
  prefs: []
  type: TYPE_NORMAL
- en: '![An end-to-end pipeline illustrated in KubeFlow](../images/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-11\. An end-to-end pipeline illustrated in KubeFlow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fairing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fairing allows us to manage the entire build, train, and deploy lifecycle directly
    through Jupyter Notebooks. [Figure 9-12](part0011.html#creating_a_new_jupyter_notebook_server_o)
    shows how to start a new notebook server, where we can host all of our Jupyter
    Notebooks, run training on them, and deploy our models to Google Cloud using the
    following few lines of code, all the while being in the comfort of a very familiar
    Jupyter environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Creating a new Jupyter Notebook server on KubeFlow](../images/00197.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-12\. Creating a new Jupyter Notebook server on KubeFlow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating a new KubeFlow deployment is a fairly straightforward process that
    is well documented on the KubeFlow website. You can set up KubeFlow using the
    browser for GCP. Alternatively, you can use the KubeFlow command-line tool to
    set up a deployment on GCP, AWS, and Microsoft Azure. [Figure 9-13](part0011.html#creating_a_kubeflow_deployment_on_gcp_us)
    shows a GCP deployment using the web browser.
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a KubeFlow deployment on GCP using the browser](../images/00014.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-13\. Creating a KubeFlow deployment on GCP using the browser
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As of this writing, KubeFlow is in active development and shows no signs of
    stopping. Companies such as Red Hat, Cisco, Dell, Uber, and Alibaba are some of
    the active contributors on top of cloud giants like Microsoft, Google, and IBM.
    Ease and accessibility for solving tough challenges attract more people to any
    platform, and KubeFlow is doing exactly that.
  prefs: []
  type: TYPE_NORMAL
- en: Price Versus Performance Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 6](part0008.html#7K4G3-13fa565533764549a6f0ab7f11eed62b), we looked
    at how to improve our model performance for inference (whether on smartphones
    or on a server). Now let’s look from another side: the hardware performance and
    the price involved.'
  prefs: []
  type: TYPE_NORMAL
- en: Often while building a production system, we want the flexibility of choosing
    suitable hardware to strike the proper balance between performance, scale, and
    price for our scenario. Consider building an app that requires cloud-based inference.
    We can go set up our own stack manually (using Flask or TensorFlow Serving or
    KubeFlow), or we could use a managed Inference-as-a-Service stack (like the Google
    Cloud ML Engine). Assuming that our service went viral, let’s see how much it
    would cost.
  prefs: []
  type: TYPE_NORMAL
- en: Cost Analysis of Inference-as-a-Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For Google Cloud ML Engine, as of August 2019 in North America, it costs a rather
    inexpensive $0.0401 per hour of combined inference time on a single-core CPU machine.
    There’s also an option for a quad-core CPU machine, but really, a single core
    should suffice for most applications. Running several queries to the server with
    a small image of 12 KB took roughly 3.5 seconds on average, as illustrated in
    [Figure 9-14](part0011.html#google_cloud_ml_engine_showing_incoming). This does
    sound slow, and is partly because of doing inference on a moderate-speed machine,
    and, more important on a CPU server. It’s worth mentioning that this benchmark
    is on a warmed-up machine that has recently received an API request and hence
    has the model preloaded. For comparison, the first query takes between 30 and
    60 seconds. This shows the importance of keeping the service running constantly
    or sending frequent warm-up queries. This happens because the Google Cloud ML
    engine takes down a model if it notices a prolonged period of nonuse.
  prefs: []
  type: TYPE_NORMAL
- en: '![Google Cloud ML Engine showing incoming queries and latency of serving the
    calls, with the end-to-end latency at user’s end of about 3.5 seconds](../images/00304.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-14\. Google Cloud ML Engine showing incoming queries and latency of
    serving the calls, with end-to-end latency at user’s end of about 3.5 seconds
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If a request came in at every second for an entire month, there would be a total
    of 60 x 60 x 24 x 30 = 2,592,000 calls per month. Assuming that each inference
    takes 3.5 seconds, a single node would be insufficient. The cloud service would
    quickly realize that and, in response to the increased traffic, bring up three
    additional machines to handle the traffic. In total, with four machines running
    for a month at $0.0401 per hour per node, it would cost a grand total of $115.48\.
    To put this into perspective, for two million calls, that’s about the cost of
    a cup of Starbucks coffee a day for an entire month. And let’s not forget this
    is without involving much of the DevOps team members, whose time is expensive.
    If we took the hypothetical scenario of a Yelp-like service for which users, on
    average, upload photos of food at 64 QPS, running inferences on them using a classification
    model would cost only $7,390.
  prefs: []
  type: TYPE_NORMAL
- en: Cost Analysis of Building Your Own Stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Less spending and high scalability, now that’s a winning combination. But the
    one downside is the total roundtrip latency of each request. Taking matters into
    our own hands, getting a VM with a modest GPU on the cloud, and setting up our
    scaling pipeline (using KubeFlow or the native cloud load-balancing features with
    TensorFlow Serving), we would be able to respond either in milliseconds or batch
    a few incoming queries together (say every 500 ms) to serve them. As an example,
    looking at the inventory of VMs on Azure, for $2.07 per hour, we can rent out
    an ND6 machine that features an NVIDIA P40 GPU and 112 GiB RAM. By batching incoming
    requests every 500 ms to 1 second, this machine can serve 64 requests per second
    at a total cost of $1,490 per month, and faster than the Google Cloud ML Engine.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the cost savings and performance benefits of orchestrating our own
    cloud machine environment kicks in big time when working on large QPS scenarios,
    as demonstrated in [Figure 9-15](part0011.html#cost_comparison_of_infrastructure_as_a_s).
  prefs: []
  type: TYPE_NORMAL
- en: '![Cost comparison of Infrastructure as a service (Google Cloud ML Engine) versus
    Building Your Own Stack over Virtual Machines (Azure VM) (Costs as of August 2019)](../images/00244.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-15\. Cost comparison of infrastructure as a service (Google Cloud ML
    Engine) versus building your own stack over virtual machines (Azure VM) (costs
    as of August 2019)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A common question that arises while benchmarking is what is my system’s limit?
    [JMeter](https://jmeter.apache.org) can help answer this. JMeter is a load-testing
    tool that lets you perform stress testing of your system with an easy-to-use graphical
    interface. It lets you create reusable configurations to simulate a variety of
    usage scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we answered the question most engineers and developers ask:
    how do we serve model prediction requests at scale for applications in the real
    world? We explored four different methods of serving an image recognition model:
    using Flask, Google Cloud ML, TensorFlow Serving, and KubeFlow. Depending on the
    scale, latency requirements, and our skill level, some solutions might be more
    attractive than others. Finally, we developed an intuition into the cost effectiveness
    of different stacks. Now that we can show our fabulous classifier model off to
    the world, all that’s left is to make our work go viral!'
  prefs: []
  type: TYPE_NORMAL
