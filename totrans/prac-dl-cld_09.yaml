- en: Chapter 9\. Scalable Inference Serving on Cloud with TensorFlow Serving and
    KubeFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。使用TensorFlow Serving和KubeFlow在云上进行可扩展推断服务
- en: 'Imagine this: you just built a top-notch classifier. Your goal, as the Silicon
    Valley motto goes, is to “*make the world a better place*,” which you’re going
    to do... with a spectacular Dog/Cat classifier. You have a solid business plan
    and you cannot wait to pitch your magical classifier to that venture capital firm
    next week. You know that the investors will question you about your cloud strategy,
    and you need to show a solid demo before they even consider giving you the money.
    How would you do this? Creating a model is half the battle, serving it is the
    next challenge, often the bigger one. In fact, for a long time it was common for
    training a model to only take a few weeks, but trying to serve it to a larger
    group of people was a months-long battle, often involving backend engineers and
    DevOps teams.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下：你刚刚建立了一个一流的分类器。你的目标，正如硅谷的座右铭所说，“*让世界变得更美好*”，你将通过一个出色的狗/猫分类器来实现这一目标。你有一个扎实的商业计划，迫不及待地想要在下周向风险投资公司推销你的神奇分类器。你知道投资者会问及你的云策略，你需要在他们考虑给你钱之前展示一个扎实的演示。你会怎么做？创建模型只是战斗的一半，提供服务是下一个挑战，通常是更大的挑战。事实上，很长一段时间以来，训练模型只需要几周的时间，但是试图将其提供给更多人使用是一个长达数月的战斗，通常涉及后端工程师和DevOps团队。
- en: In this chapter, we answer a few questions that tend to come up in the context
    of hosting and serving custom-built models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回答了一些关于托管和提供定制模型的常见问题。
- en: How can I host my model on my personal server so that my coworkers can play
    with it?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何在我的个人服务器上托管我的模型，以便我的同事可以使用它？
- en: I am not a backend/infrastructure engineer, but I want to make my model available
    so that it can serve thousands (or even millions) of users. How can I do this
    at a reasonable price without worrying about scalability and reliability issues?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我不是后端/基础设施工程师，但我想让我的模型可用，以便为成千上万（甚至数百万）的用户提供服务。如何在合理的价格下做到这一点，而不必担心可扩展性和可靠性问题？
- en: There are reasons (such as cost, regulations, privacy, etc.) why I cannot host
    my model on the cloud, but only on-premises (my work network). Can I serve predictions
    at scale and reliably in such a case?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些原因（如成本、法规、隐私等）使我无法将我的模型托管在云上，而只能在内部网络（我的工作网络）上托管。在这种情况下，我能够以规模和可靠性提供预测吗？
- en: Can I do inference on GPUs?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以在GPU上进行推断吗？
- en: How much can I expect to pay for each of these options?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以期待每个选项的费用是多少？
- en: Could I scale my training and serving across multiple cloud providers?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能否跨多个云提供商扩展我的训练和服务？
- en: How much time and technical know-how will it take to get these running?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使这些运行需要多少时间和技术知识？
- en: Let’s begin our journey by looking at the high-level overview of the tools available
    to us.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从高层次概述可用工具开始我们的旅程。
- en: Landscape of Serving AI Predictions
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供AI预测的景观
- en: There is a multitude of tools, libraries, and cloud services available for getting
    trained AI models to serve prediction requests. [Figure 9-1](part0011.html#a_high-level_overview_and_comparison_of)
    simplifies them into four categories.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多工具、库和云服务可用于训练AI模型以提供预测请求。[图9-1](part0011.html#a_high-level_overview_and_comparison_of)将它们简化为四类。
- en: '![A high-level overview and comparison of different inference serving options.](../images/00100.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![不同推断服务选项的高级概述和比较。](../images/00100.jpeg)'
- en: Figure 9-1\. A high-level overview and comparison of different inference serving
    options
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 不同推断服务选项的高级概述和比较
- en: Depending on our inference scenarios, we can make an appropriate choice. [Table 9-1](part0011.html#tools_to_serve_deep_learning_models_over)
    takes a deeper look.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的推断场景，我们可以做出适当的选择。[表9-1](part0011.html#tools_to_serve_deep_learning_models_over)深入探讨。
- en: Table 9-1\. Tools to serve deep learning models over the network
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-1。通过网络提供深度学习模型的工具
- en: '| **Category and examples** | **Expected time to first prediction** | **Pros
    and cons** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **类别和示例** | **预期的首次预测时间** | **优缺点** |'
- en: '| --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| HTTP servers'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '| HTTP服务器'
- en: Flask
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flask
- en: Django
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Django
- en: Apache OpenWhisk
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache OpenWhisk
- en: Python `http.server`
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python `http.server`
- en: '| <5 minutes | + Simple to run+ Often runs current Python code– Slow– Not optimized
    for AI |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| <5分钟 | + 运行简单+ 通常运行当前Python代码– 较慢– 未经AI优化 |'
- en: '| **Hosted and managed cloud stacks**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '| **托管和管理的云堆栈**'
- en: Google Cloud ML
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud ML
- en: Azure ML
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure ML
- en: Amazon Sage Maker
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊Sage Maker
- en: Algorithmia
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Algorithmia
- en: '| <15 minutes | + Easier GUI/command-line interfaces+ Highly scalable+ Fully
    managed, reduces the need for DevOps teams– Usually limited to CPU-based inference,
    can be slow for large models– Warm-up query time can be slow |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| <15分钟 | + 更容易的GUI/命令行界面+ 高度可扩展+ 完全托管，减少了对DevOps团队的需求– 通常仅限于基于CPU的推断，对于大型模型可能较慢–
    预热查询时间可能较慢 |'
- en: '| **Manually managed serving libraries**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '| **手动管理的服务库**'
- en: TensorFlow Serving
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Serving
- en: NVIDIA TensorRT
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英伟达TensorRT
- en: DeepDetect
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepDetect
- en: MXNet Model Serving
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MXNet模型服务
- en: Skymind Intelligence Layer with DeepLearning4J
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skymind Intelligence Layer with DeepLearning4J
- en: Seldon
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seldon
- en: DeepStack AI Server
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepStack AI服务器
- en: '| <15 minutes | + High performance+ Allows manual controls on optimizations,
    batching, etc.+ Can run inference on GPU– More involved setup– Scaling over multiple
    nodes usually requires extra groundwork |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| <15分钟 | + 高性能+ 允许手动控制优化、批处理等+ 可以在GPU上运行– 设置更复杂– 要在多个节点上扩展通常需要额外的工作 |'
- en: '| **Cloud AI orchestration frameworks**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '| **云AI编排框架**'
- en: KubeFlow
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KubeFlow
- en: '| ~1 hour | + Makes scaling training and inference easy to manage+ Portability
    between cloud providers+ Consistent environments across development and production+
    For data scientists, integration with familiar tools such as Jupyter Notebooks
    for sending models to production+ Enables composing conditional pipelines to automate
    testing, cascading models+ Uses existing manually managed serving libraries– Still
    evolving– For beginners, hosted and managed cloud stacks offer an easier learning
    curve |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| ~1小时 | + 使扩展训练和推理易于管理+ 在云提供商之间可移植+ 开发和生产环境一致+ 对于数据科学家，与熟悉的工具（如Jupyter Notebooks）集成，用于将模型发送到生产环境+
    可以组合条件管道以自动化测试、级联模型+ 使用现有的手动管理的服务库– 仍在发展中– 对于初学者，托管和管理的云堆栈提供了更简单的学习曲线 |'
- en: In this chapter, we explore a range of tools and scenarios. Some of these options
    are easy to use but limited in functionality. Others offer more granular controls
    and higher performance but are more involved to set up. We look at one example
    of each category and take a deeper dive to develop an intuition into when using
    one of those makes sense. We then present a cost analysis of the different solutions
    as well as case studies detailing how some of these solutions work in practice
    today.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了一系列工具和场景。其中一些选项易于使用，但功能有限。其他选项提供更精细的控制和更高的性能，但设置更复杂。我们将看一个每个类别的例子，并深入研究，以便了解何时使用其中之一是有意义的。然后，我们将对不同解决方案进行成本分析，并详细介绍一些解决方案在实践中的工作方式。
- en: 'Flask: Build Your Own Server'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flask：构建自己的服务器
- en: We begin with the most basic technique of *Build Your Own Server* (BYOS). From
    the choices presented in the first column of [Table 9-1](part0011.html#tools_to_serve_deep_learning_models_over),
    we’ve selected Flask.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从*构建自己的服务器*（BYOS）的最基本技术开始。从[表9-1](part0011.html#tools_to_serve_deep_learning_models_over)的第一列中选择，我们选择了Flask。
- en: Making a REST API with Flask
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Flask创建REST API
- en: Flask is a Python-based web application framework. Released in 2010 and with
    more than 46,000 stars on GitHub, it is under continuous development. It’s also
    quick and easy to set up and is really useful for prototyping. It is often the
    framework of choice for data science practitioners when they want to serve their
    models to a limited set of users (e.g., sharing with coworkers on a corporate
    network) without a lot of fuss.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Flask是一个基于Python的Web应用程序框架。它于2010年发布，在GitHub上拥有超过46,000颗星，正在持续开发。它快速且易于设置，对于原型设计非常有用。当数据科学从业者想要向一组有限的用户提供他们的模型（例如，在企业网络上与同事共享）时，通常会选择这个框架，而不会有太多麻烦。
- en: 'Installing Flask with `pip` is fairly straightforward:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pip`安装Flask相当简单：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Upon installation, we should be able to run the following simple “Hello World”
    program:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，我们应该能够运行以下简单的“Hello World”程序：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following is the command to run the “Hello World” program:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是运行“Hello World”程序的命令：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By default, Flask runs on port 5000\. When we open the URL *http://localhost:5000/hello*
    in the browser, we should see the words “Hello World!,” as shown in [Figure 9-2](part0011.html#navigate_to_httpcolonsolidussoliduslocal).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Flask在端口5000上运行。当我们在浏览器中打开URL *http://localhost:5000/hello*时，我们应该看到“Hello
    World!”这几个字，如[图9-2](part0011.html#navigate_to_httpcolonsolidussoliduslocal)所示。
- en: '![Navigate to http://localhost:5000/hello within a web browser to view the
    “Hello World!” web page](../images/00132.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![在Web浏览器中导航到http://localhost:5000/hello，查看“Hello World！”网页](../images/00132.jpeg)'
- en: Figure 9-2\. Navigate to http://localhost:5000/hello within a web browser to
    view the “Hello World!” web page
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。在Web浏览器中导航到http://localhost:5000/hello，查看“Hello World！”网页
- en: As you can see, it takes barely more than a few lines to get a simple web application
    up and running. One of the most important lines in that script is `@app.route("/hello")`.
    It specifies that the path `/hello` after the hostname would be served by the
    method immediately beneath it. In our case, it merely returns the string “Hello
    World!” In the next step, we look at how to deploy a Keras model to a Flask server
    and create a route that will serve predictions by our model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，只需几行代码就可以让一个简单的Web应用程序运行起来。在该脚本中最重要的一行是`@app.route("/hello")`。它指定了主机名后的路径`/hello`将由其下方的方法提供服务。在我们的情况下，它只是返回字符串“Hello
    World！”在下一步中，我们将看看如何将Keras模型部署到Flask服务器，并创建一个路由来提供我们模型的预测。
- en: Deploying a Keras Model to Flask
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将Keras模型部署到Flask
- en: 'Our first step is to load our Keras model. The following lines load the model
    from the .*h5* file. You’ll find the scripts for this chapter on the book’s GitHub
    (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)) in
    *code/chapter-9*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是加载我们的Keras模型。以下行从.*h5*文件中加载模型。您可以在本书的GitHub（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）的*code/chapter-9*目录中找到本章的脚本：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we create the route */infer* that would support inference on our images.
    Naturally, we would support `POST` requests to accept images:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建路由*/infer*，以支持对我们的图像进行推理。自然地，我们将支持`POST`请求以接受图像：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To test the inference, let’s use the `curl` command, as follows, on a sample
    image containing a dog:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试推理，让我们使用`curl`命令，在包含一只狗的示例图像上进行如下操作：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As expected, we get a prediction of “dog.” This has worked quite well so far.
    At this point, Flask runs only locally; that is, someone else on the network would
    not be able to make a request to this server. To make Flask available to others,
    we can simply change `app.run()` to the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们得到了“dog”的预测。到目前为止，这个方法运行得相当顺利。此时，Flask只在本地运行；也就是说，网络上的其他人无法向该服务器发出请求。要使Flask对其他人可用，我们只需将`app.run()`更改为以下内容：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'At this point, we can give access to our model to anyone within our network.
    The next question would be—can we do the same to make the model available to the
    general public? The answer to that question is an emphatic no! The Flask website
    has a prominent warning stating *“WARNING: Do not use the development server in
    a production environment.”* Flask indeed does not support production work out
    of the box and would need custom code to enable that. In the upcoming sections,
    we look at how to host our models on systems that are meant for production use.
    With all of this in mind, let’s recap some of the pros and cons of using Flask.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以让我们的模型对我们网络内的任何人开放。下一个问题将是——我们是否可以做同样的事情让模型对普通公众可用？对于这个问题的答案是一个坚定的否定！Flask网站上有一个显著的警告：“警告：不要在生产环境中使用开发服务器。”
    Flask确实不支持开箱即用的生产工作，需要自定义代码来启用。在接下来的部分中，我们将看看如何在适用于生产使用的系统上托管我们的模型。考虑到所有这些，让我们回顾一下使用Flask的一些优缺点。
- en: Pros of Using Flask
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Flask的优点
- en: 'Flask provides some advantages, namely:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Flask提供了一些优势，包括：
- en: Quick to set up and to prototype
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速设置和原型设计
- en: Fast development cycle
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速开发周期
- en: Lightweight on resources
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源占用轻
- en: Broad appeal within the Python community
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python社区中具有广泛吸引力
- en: Cons of Using Flask
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Flask的缺点
- en: 'At the same time, Flask might not be your best choice, for the following reasons:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，出于以下原因，Flask可能不是您的最佳选择：
- en: Cannot scale; by default, it is not meant for production loads. Flask can serve
    only one request at one time
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法扩展；默认情况下，它不适用于生产负载。Flask一次只能处理一个请求
- en: Does not handle model versioning out of the box
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开箱即用不支持模型版本控制
- en: Does not support batching of requests out of the box
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持批量请求处理
- en: Desirable Qualities in a Production-Level Serving System
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产级服务系统中的理想特质
- en: For any cloud service that is serving traffic from the public, there are certain
    attributes that we want to look for when deciding to use a solution. In the context
    of machine learning, there are additional qualities that we would look for while
    building inference services. We look at a few of them if this section.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何从公共网络提供流量的云服务，我们在决定使用解决方案时希望看到某些属性。在机器学习的背景下，在构建推理服务时，我们会寻找额外的特质。如果本节中有一些特质，我们将看一下其中的一些。
- en: High Availability
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高可用性
- en: For our users to trust our service, it must be available almost always. For
    many serious players, they measure their availability metric in terms of “*number
    of nines*.” If a business claims that its service has four 9s availability, they
    mean the system is up and available 99.99% of the time. Even though 99% sounds
    impressive, [Table 9-2](part0011.html#downtime_per_year_for_different_availabi)
    puts that downtime per year in perspective.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的用户信任我们的服务，它必须几乎始终可用。对于许多严肃的参与者，他们用“*九的数量*”来衡量他们的可用性指标。如果一个企业声称其服务有四个九的可用性，他们的意思是系统99.99%的时间都是正常运行和可用的。尽管99%听起来令人印象深刻，[表9-2](part0011.html#downtime_per_year_for_different_availabi)将每年的停机时间放在了透视中。
- en: Table 9-2\. Downtime per year for different availability percentages
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-2。不同可用性百分比的每年停机时间
- en: '| **Availability %** | **Downtime per year** |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **可用性 %** | **每年停机时间** |'
- en: '| --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 99% (“two nines”) | 3.65 days |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 99%（“两个九”）| 3.65天 |'
- en: '| 99.9% (“three nines”) | 8.77 hours |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 99.9%（“三个九”）| 8.77小时 |'
- en: '| 99.99% (“four nines”) | 52.6 minutes |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 99.99%（“四个九”）| 52.6分钟 |'
- en: '| 99.999% (“five nines”) | 5.26 minutes |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 99.999%（“五个九”）| 5.26分钟 |'
- en: Imagine how ridiculous the situation would be if a major website like Amazon
    were only 99.9% available, losing millions in user revenue during the eight-plus
    hours of downtime. Five 9s is considered the holy grail. Anything less than three
    9s is typically unsuitable for a high-quality production system.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果像亚马逊这样的主要网站只有99.9%的可用性，每年超过八个小时的停机时间将导致数百万用户收入损失。五个九被认为是圣杯。少于三个九的任何可用性通常不适合高质量的生产系统。
- en: Scalability
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Traffic handled by production services is almost never uniform across a larger
    time period. For example, the *New York Times* experiences significantly more
    traffic during morning hours, whereas Netflix typically experiences a surge in
    traffic between the evening and late-night hours, when people chill. There are
    also seasonal factors in traffic. Amazon experiences orders of magnitude more
    traffic on Black Friday and during Christmas season.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 生产服务处理的流量在较长时间段内几乎从不是均匀的。例如，《纽约时报》在早晨经历的流量明显更多，而Netflix通常在晚上和深夜经历流量激增，人们在那时放松。流量还受季节因素影响。亚马逊在黑色星期五和圣诞季节经历了数量级更多的流量。
- en: 'A higher demand requires a higher amount of resources being available and online
    to serve them. Otherwise, the availability of the system would be in jeopardy.
    A naive way to accomplish this would be to anticipate the highest volume of traffic
    the system would ever serve, determine the number of resources necessary to serve
    that level of traffic, and then allocate that amount all the time, in perpetuity.
    There are two problems with this approach: 1) if your planning was correct, the
    resources would be underutilized most of the time, essentially burning money;
    and 2) if your estimation was insufficient, you might end up affecting the availability
    of your service and end up with a far worse problem of losing the trust of your
    customers and ultimately their wallets.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的需求需要更多的资源可用和在线以为他们提供服务。否则，系统的可用性将受到威胁。实现这一目标的一种天真的方法是预测系统将提供的最高流量量，确定为提供该流量水平所需的资源数量，然后永久分配该数量。这种方法有两个问题：1）如果您的规划是正确的，那么大部分时间资源将被低效利用，实际上是在浪费金钱；2）如果您的估计不足，您可能会影响服务的可用性，并最终失去客户的信任和钱包。
- en: A smarter way to manage traffic loads is to monitor them as they are coming
    in and dynamically allocate and deallocate resources that are available for service.
    This ensures that the increased traffic is handled without loss of service while
    keeping operating costs to a minimum during low-traffic times.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 管理流量负载的更智能的方法是在流入时监视它们，并动态分配和释放可用于服务的资源。这确保了增加的流量在不丢失服务的情况下处理，同时在低流量时期将运营成本降至最低。
- en: When scaling down resources, any resource that is about to be deallocated is
    quite likely to be processing traffic at that moment. It’s essential to ensure
    that all of those requests be completed before shutting down that resource. Also,
    crucially, the resource must not process any new requests. This process is called
    *draining*. Draining is also crucial when machines are taken down for routine
    maintenance and/or upgrades.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在缩减资源时，即将被释放的任何资源很可能正在处理流量。在关闭该资源之前，必须确保所有这些请求都已完成。此外，关键是资源不能处理任何新请求。这个过程被称为*排空*。当机器因例行维护和/或升级而关闭时，排空也至关重要。
- en: Low Latency
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低延迟
- en: Consider these facts. Amazon published a study in 2008 in which it found that
    every 100 ms increase in latency in its retail website resulted in a 1% loss of
    profit. A one-second delay in loading the website caused a whopping $1.6 billion
    in lost revenue! Google found that a 500 ms latency on mobile websites resulted
    in a traffic drop of 20%. In other words, a 20% decrease in the opportunity-to-serve
    advertisements. And this does not affect only industry giants. If a web page takes
    longer than three seconds to load on a mobile phone, 53% of users abandon it (according
    to a 2017 study by Google). It’s clear that time is money.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这些事实。亚马逊在2008年发表了一项研究，发现其零售网站的每增加100毫秒的延迟会导致1％的利润损失。网站加载延迟一秒会导致高达16亿美元的收入损失！谷歌发现移动网站500毫秒的延迟导致流量下降20％。换句话说，广告服务的机会减少了20％。而这不仅影响行业巨头。如果一个网页在手机上加载时间超过三秒，53％的用户会放弃它（根据谷歌2017年的一项研究）。显然，时间就是金钱。
- en: Reporting average latency can be misleading because it might paint a cheerier
    picture than a ground reality. It’s like saying if Bill Gates walks into a room,
    everyone is a billionaire on average. Instead, percentile latency is the typically
    reported metric. For example, a service might report 987 ms @ 99th percentile.
    This means that 99% of the requests were served in 987 ms or less. The very same
    system could have a 20 ms latency on average. Of course, as traffic to your service
    increases, the latency might increase if the service is not scaled up to give
    adequate resources. As such, latency, high availability, and scalability are intertwined.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 报告平均延迟可能会产生误导，因为它可能比实际情况更乐观。这就好比说如果比尔·盖茨走进一个房间，那么每个人平均都是亿万富翁。相反，百分位延迟通常是报告的指标。例如，一个服务可能报告99th百分位的987毫秒。这意味着99％的请求在987毫秒或更短的时间内得到响应。同一个系统的平均延迟可能是20毫秒。当然，随着对您的服务的流量增加，如果服务没有扩展以提供足够的资源，延迟可能会增加。因此，延迟、高可用性和可伸缩性是相互交织在一起的。
- en: Geographic Availability
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 地理可用性
- en: The distance between New York and Sydney is nearly 10,000 miles (16,000 km).
    The speed of light in a vacuum is roughly 186,282 miles per second (300,000 km
    per second). Silica glass (used in fiber-optic cables) decreases the speed of
    light by a factor of roughly 30% down to 130,487 miles per second (210,000 km
    per second). On a piece of fiber-optic running in a straight line between these
    two cities, the roundtrip travel time alone for a single request is nearly 152
    ms. Keep in mind that this does not account for the amount of time it takes for
    the request to be processed at the server, or the hops that the packets need to
    make across multiple routers along the way. This level of service would be unacceptable
    for many applications.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 纽约和悉尼之间的距离接近10,000英里（16,000公里）。真空中的光速大约为每秒186,282英里（每秒300,000公里）。二氧化硅玻璃（用于光纤电缆）将光速降低约30％，降至每秒130,487英里（每秒210,000公里）。在连接这两个城市之间的一条直线上运行的光纤上，单个请求的往返传输时间仅为约152毫秒。请记住，这并不考虑请求在服务器上处理所需的时间，或者数据包在途中通过多个路由器进行跳转的时间。对于许多应用程序来说，这种服务水平是不可接受的。
- en: Services that expect to be used throughout the world must be strategically located
    to minimize latency for the users in those regions. Additionally, resources can
    be dynamically scaled up or down depending on local traffic, thus giving more
    granular control. The major cloud providers have a presence on at least five continents
    (sorry penguins!).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 希望在全球范围内使用的服务必须被战略性地放置，以最小化用户在这些地区的延迟。此外，资源可以根据当地流量动态扩展或缩减，从而提供更精细的控制。主要的云服务提供商至少在五个大洲设有基地（抱歉企鹅！）。
- en: Tip
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Want to simulate how long the incoming requests would take from your computer
    to a particular datacenter around the world? [Table 9-3](part0011.html#latency_measurement_tools_for_different)
    lists a few handy browser-based tools offered by cloud providers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 想要模拟从您的计算机到世界各地特定数据中心的传入请求需要多长时间吗？[表9-3](part0011.html#latency_measurement_tools_for_different)列出了一些由云服务提供商提供的方便的基于浏览器的工具。
- en: Table 9-3\. Latency measurement tools for different cloud providers
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-3. 不同云服务提供商的延迟测量工具
- en: '| **Service** | **Cloud provider** |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **服务** | **云服务提供商** |'
- en: '| --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| [AzureSpeed.com](http://AzureSpeed.com) | Microsoft Azure |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| [AzureSpeed.com](http://AzureSpeed.com) | 微软Azure |'
- en: '| [CloudPing.info](https://CloudPing.info) | Amazon Web Services |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| [CloudPing.info](https://CloudPing.info) | 亚马逊网络服务 |'
- en: '| [GCPing.com](http://GCPing.com) | Google Cloud Platform |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| [GCPing.com](http://GCPing.com) | 谷歌云平台 |'
- en: Additionally, to determine realistic combinations of latency from one location
    to another, *CloudPing.co* measures AWS Inter-Region Latency, between more than
    16 US-based AWS datacenters to one another.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了确定从一个位置到另一个位置的延迟的现实组合，*CloudPing.co*测量了AWS区域间的延迟，包括16个以上美国AWS数据中心之间的延迟。
- en: Failure Handling
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障处理
- en: There’s an old saying that there are only two things that are assured in life—death
    and taxes. In the twenty-first century, this adage applies not just to humans
    but also computer hardware. Machines fail all the time. The question is never
    *if* a machine will fail, it’s *when*. One of the necessary qualities of production-quality
    service is its ability to gracefully handle failures. If a machine goes down,
    quickly bring up another machine to take its place and continue serving traffic.
    If an entire datacenter goes down, seamlessly route traffic to another datacenter
    so that users don’t even realize that anything bad happened in the first place.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有一句古老的谚语说生活中只有两件事是确定的——死亡和税收。在21世纪，这句格言不仅适用于人类，也适用于计算机硬件。机器经常出现故障。问题从来不是机器会出现故障，而是何时会出现故障。生产质量服务的一个必要特性是它能够优雅地处理故障。如果一台机器出现故障，迅速启动另一台机器来替代它并继续提供流量服务。如果整个数据中心出现故障，无缝地将流量路由到另一个数据中心，以便用户甚至不会意识到一开始发生了任何不好的事情。
- en: Monitoring
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控
- en: If you can’t measure it, you can’t improve it. Worse, does it even exist? Monitoring
    the number of requests, availability, latency, resource usage, number of nodes,
    distribution of traffic, and location of users is vital to understanding how a
    service performs; finding opportunities to improve it; and, more importantly,
    how much to pay. Most cloud providers already have built-in dashboards providing
    these metrics. Additionally, recording task-specific analytics like time for model
    inference, preprocessing, and so on can add another level of understanding.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不能测量它，你就无法改进它。更糟糕的是，它甚至存在吗？监控请求数量、可用性、延迟、资源使用情况、节点数量、流量分布和用户位置对于了解服务的表现至关重要；找到改进的机会；更重要的是，要支付多少。大多数云提供商已经内置了提供这些指标的仪表板。此外，记录任务特定的分析，如模型推断、预处理等的时间，可以增加另一层理解。
- en: Model Versioning
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型版本控制
- en: We have learned in this book (and will continue to learn all the way to the
    last page) that machine learning is always iterative. Particularly in the case
    of applications in the real world, data that the model can learn on is constantly
    being generated. Moreover, the incoming data distribution might shift over time
    compared to what it was trained on, leading to lower prediction power (a phenomenon
    called c*oncept drift*). To provide users with the best possible experience, we
    want to keep improving our models. Every time we train our model with newer data
    to further improve its accuracy and make the best version yet, we want to make
    it available for our users as quickly and seamlessly as possible. Any good production-quality
    inference system should provide the ability to provide different versions of a
    model, including the ability to swap a live version of the model with another
    version at a moment’s notice.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中学到（并将继续学习直到最后一页）机器学习始终是迭代的。特别是在现实世界的应用中，模型可以学习的数据不断生成。此外，传入数据的分布可能随时间而变化，与训练时相比，导致预测能力降低（称为概念漂移现象）。为了为用户提供最佳体验，我们希望不断改进我们的模型。每次我们使用更新的数据训练模型以进一步提高其准确性并制作最佳版本时，我们希望尽快而无缝地为用户提供。任何良好的生产质量推断系统都应该提供提供模型的不同版本的能力，包括在一瞬间交换模型的实时版本的能力。
- en: A/B Testing
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A/B测试
- en: In addition to supporting multiple versions of a model, there are reasons we’d
    want to serve different versions of the model at the same time depending on a
    variety of attributes such as the geographic location of the user, demographics,
    or simply by random assignment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 除了支持模型的多个版本之外，我们希望根据用户的地理位置、人口统计数据或简单的随机分配等各种属性，在同一时间为模型提供不同版本的原因。
- en: '*A/B testing* is a particularly useful tool when improving a model. After all,
    if our spanking new model was flawed in some way, we’d rather it be deployed to
    only a small subset of our users rather than 100% of them at the same time before
    we catch the flaw. Additionally, if a model meets criteria for success on that
    small subset, it provides validation for the experiment and justifies eventually
    being promoted to all users.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*A/B测试*是改进模型时特别有用的工具。毕竟，如果我们全新的模型在某种程度上存在缺陷，我们宁愿它只部署给我们用户的一个小子集，而不是同时部署给所有用户，以便在捕捉到缺陷之前。此外，如果模型在这个小子集上符合成功标准，它为实验提供了验证，并证明最终可以提升到所有用户。'
- en: Support for Multiple Machine Learning Libraries
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持多个机器学习库
- en: Last but not least, we don’t want to be locked into a single machine learning
    library. Some data scientists in an organization might train models in PyTorch,
    others in TensorFlow, or maybe scikit-Learn suffices for non–deep learning tasks.
    The flexibility to support multiple libraries would be a welcome bonus.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但并非最不重要的是，我们不想被锁定在单一的机器学习库中。组织中的一些数据科学家可能会在PyTorch中训练模型，其他人可能会在TensorFlow中训练，或者对于非深度学习任务，scikit-Learn可能就足够了。支持多个库的灵活性将是一个受欢迎的奖励。
- en: 'Google Cloud ML Engine: A Managed Cloud AI Serving Stack'
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud ML Engine：托管的云AI服务堆栈
- en: Considering all the desirable qualities we discussed in a production environment
    in the previous section, it’s generally not a good idea to use Flask for serving
    users. If you do not have a dedicated infrastructure team and would like to spend
    more time making better models than deploying them, using a managed cloud solution
    is the right approach. There are several cloud-based Inference-as-a-Service solutions
    on the market today. We have chosen to explore the Google Cloud ML Engine partly
    because of the convenient TensorFlow integration and partly because it ties in
    nicely with the ML Kit material that we touch upon in [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们在前一节中讨论的生产环境中所有理想的特性，使用Flask为用户提供服务通常不是一个好主意。如果您没有专门的基础设施团队，并且希望花更多时间制作更好的模型而不是部署它们，那么使用托管的云解决方案是正确的方法。今天市场上有几种基于云的推断即服务解决方案。我们选择探索Google
    Cloud ML Engine部分原因是因为方便的TensorFlow集成，部分原因是因为它与我们在第13章中提到的ML Kit材料很好地结合在一起。
- en: Pros of Using Cloud ML Engine
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Cloud ML Engine的优点
- en: Easy-to-deploy models in production with web-based GUI
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于部署到生产环境中，具有基于Web的GUI
- en: Powerful and easily scalable to millions of users
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能强大，易于扩展到数百万用户
- en: Provides deep insights into model usage
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供深入的模型使用洞察
- en: Ability to version models
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够对模型进行版本控制
- en: Cons of Using Cloud ML Engine
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Cloud ML Engine的缺点
- en: High latency, offers only CPUs for inference (as of August 2019)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高延迟，仅提供CPU进行推断（截至2019年8月）
- en: Unsuitable for scenarios involving legal and data privacy issues where the data
    must not leave the network
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适用于涉及法律和数据隐私问题的场景，其中数据不能离开网络
- en: Imposes restrictions on architecture design of complex applications
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对复杂应用程序的架构设计施加限制
- en: Building a Classification API
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建分类API
- en: 'The following step-by-step guide shows how to go about uploading and hosting
    our a Dog/Cat classifier model on Google Cloud ML Engine:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下逐步指南展示了如何在Google Cloud ML Engine上上传和托管我们的Dog/Cat分类器模型：
- en: Create a model on the Google Cloud ML Engine dashboard at [*https://console.cloud.google.com/mlengine/models*](https://console.cloud.google.com/mlengine/models).
    Because this is the first time we’re using the dashboard, we need to click ENABLE
    API, as depicted in [Figure 9-3](part0011.html#listing_page_for_machine_learning_models).
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[*https://console.cloud.google.com/mlengine/models*](https://console.cloud.google.com/mlengine/models)上在Google
    Cloud ML Engine仪表板上创建一个模型。因为这是我们第一次使用仪表板，我们需要单击“启用API”，如[图9-3](part0011.html#listing_page_for_machine_learning_models)所示。
- en: '![Listing page for machine learning models on the Google Cloud ML Engine dashboard](../images/00005.jpeg)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Google Cloud ML Engine仪表板上机器学习模型的列表页面](../images/00005.jpeg)'
- en: Figure 9-3\. Listing page for machine learning models on the Google Cloud ML
    Engine dashboard
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. Google Cloud ML Engine仪表板上机器学习模型的列表页面
- en: Give the model a name and a description ([Figure 9-4](part0011.html#model_creation_page_on_google_cloud_ml_e)).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为模型命名并添加描述（[图9-4](part0011.html#model_creation_page_on_google_cloud_ml_e)）。
- en: '![Model creation page on Google Cloud ML Engine](../images/00047.jpeg)'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Google Cloud ML Engine上的模型创建页面](../images/00047.jpeg)'
- en: Figure 9-4\. Model creation page on Google Cloud ML Engine
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4\. Google Cloud ML Engine上的模型创建页面
- en: After the model is created, we can access the model on the listing page ([Figure 9-5](part0011.html#model_listings_page_on_google_cloud_ml_e)).
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型创建完成后，我们可以在列表页面上访问模型（[图9-5](part0011.html#model_listings_page_on_google_cloud_ml_e)）。
- en: '![Model listings page on Google Cloud ML Engine](../images/00295.jpeg)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Google Cloud ML Engine上的模型列表页面](../images/00295.jpeg)'
- en: Figure 9-5\. Model listings page on Google Cloud ML Engine
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5\. Google Cloud ML Engine上的模型列表页面
- en: Click the model to go to the model details page ([Figure 9-6](part0011.html#details_page_of_the_just-created_dogcat))
    and add a new version.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击模型以转到模型详细信息页面（[图9-6](part0011.html#details_page_of_the_just-created_dogcat)）并添加新版本。
- en: '![Details page of the just-created DogCat classifier](../images/00298.jpeg)'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![刚创建的DogCat分类器的详细页面](../images/00298.jpeg)'
- en: Figure 9-6\. Details page of the just-created Dog/Cat classifier
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6\. 刚创建的Dog/Cat分类器的详细页面
- en: Fill out the necessary information to create the new version. The last field
    at the bottom requires you to upload the model to Google Cloud Storage before
    we can use it. Click the Browse button to create a new bucket for storing the
    model ([Figure 9-7](part0011.html#creating_a_new_version_for_a_machine_lea)).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写必要信息以创建新版本。底部的最后一个字段要求您在使用之前将模型上传到Google Cloud Storage。单击“浏览”按钮以创建用于存储模型的新存储桶（[图9-7](part0011.html#creating_a_new_version_for_a_machine_lea)）。
- en: '![Creating a new version for a machine learning model](../images/00255.jpeg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![为机器学习模型创建新版本](../images/00255.jpeg)'
- en: Figure 9-7\. Creating a new version for a machine learning model
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7\. 为机器学习模型创建一个新版本
- en: Create a new bucket with a unique name, a storage class, and region. After you
    create this bucket, go to [*https://console.cloud.google.com/storage/browser*](https://console.cloud.google.com/storage/browser)
    (in a separate tab while keeping the current one open) to find this newly created
    bucket and upload the model there ([Figure 9-8](part0011.html#creating_a_new_google_cloud_storage_buck)).
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有唯一名称、存储类和区域的新存储桶。创建此存储桶后，转到[*https://console.cloud.google.com/storage/browser*](https://console.cloud.google.com/storage/browser)（在保持当前标签页打开的同时在新标签页中打开）找到这个新创建的存储桶并将模型上传到那里（[图9-8](part0011.html#creating_a_new_google_cloud_storage_buck)）。
- en: '![Creating a new Google Cloud Storage bucket within the ML model version creation
    page](../images/00216.jpeg)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![在ML模型版本创建页面内创建一个新的Google Cloud Storage存储桶](../images/00216.jpeg)'
- en: Figure 9-8\. Creating a new Google Cloud Storage bucket within the ML model
    version creation page
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8\. 在ML模型版本创建页面内创建一个新的Google Cloud Storage存储桶
- en: Our Dog/Cat classifier model is an *.h5* file. However, Google Cloud expects
    a SavedModel file. You can find the script to convert the *.h5* file to SavedModel
    on the book’s GitHub repository (see [http://PracticalDeepLearning.ai](http://PracticalDeepLearning.ai))
    at *code/chapter-9/scripts/h5_to_tf.ipynb*. Simply load the model and execute
    the rest of the notebook.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的Dog/Cat分类器模型是一个*.h5*文件。但是，Google Cloud期望一个SavedModel文件。您可以在本书的GitHub存储库（请参阅[http://PracticalDeepLearning.ai](http://PracticalDeepLearning.ai)）的*code/chapter-9/scripts/h5_to_tf.ipynb*中找到将*.h5*文件转换为SavedModel的脚本。只需加载模型并执行笔记本的其余部分。
- en: In the Google Cloud Storage browser, upload the newly converted model ([Figure 9-9](part0011.html#google_cloud_storage_browser_page_showin))
    to the bucket you created in step 6.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud Storage浏览器中，将新转换的模型（[图9-9](part0011.html#google_cloud_storage_browser_page_showin)）上传到您在第6步中创建的存储桶中。
- en: '![Google Cloud Storage Browser page showing the uploaded DogCat classifier
    model in TensorFlow format](../images/00149.jpeg)'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Google Cloud Storage浏览器页面显示以TensorFlow格式上传的DogCat分类器模型](../images/00149.jpeg)'
- en: Figure 9-9\. Google Cloud Storage Browser page showing the uploaded Dog/Cat
    classifier model in TensorFlow format
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-9\. Google Cloud Storage浏览器页面显示以TensorFlow格式上传的Dog/Cat分类器模型
- en: Specify the URI on the model version creation page for the model that you just
    uploaded ([Figure 9-10](part0011.html#add_the_uri_for_the_model_you_uploaded_t)).
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型版本创建页面上指定您刚上传的模型的URI（[图9-10](part0011.html#add_the_uri_for_the_model_you_uploaded_t)）。
- en: '![Add the URI for the model you uploaded to Google Cloud Storage](../images/00139.jpeg)'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![将上传到Google Cloud Storage的模型的URI添加到其中](../images/00139.jpeg)'
- en: Figure 9-10\. Add the URI for the model you uploaded to Google Cloud Storage
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-10\. 将上传到Google Cloud Storage的模型的URI添加到其中
- en: Click the Save button and wait for the model version to be created. As soon
    as the model version is created, you can begin making predictions against it.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击保存按钮，等待模型版本创建。一旦模型版本创建完成，您就可以开始针对它进行预测。
- en: If it’s not already present on your machine, you can download and install the
    Google Cloud SDK from the installation website at [*https://cloud.google.com/sdk/install*](https://cloud.google.com/sdk/install).
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的机器上还没有安装，您可以从安装网站[*https://cloud.google.com/sdk/install*](https://cloud.google.com/sdk/install)下载并安装Google
    Cloud SDK。
- en: 'You can use the Cloud ML Engine REST API to make your requests. However, for
    brevity, use the command-line tools in the Cloud SDK. You first need to convert
    your image into a *request.json* file using the *image-to-json.py* script located
    at *code/chapter-9*:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用Cloud ML Engine REST API进行请求。但是，为了简洁起见，请使用Cloud SDK中的命令行工具。您首先需要使用位于*code/chapter-9*的*image-to-json.py*脚本将图像转换为*request.json*文件：
- en: '[PRE7]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, use the *request.json* file created in the previous step to execute a
    request against our model:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用在上一步中创建的*request.json*文件来执行针对我们模型的请求：
- en: '[PRE8]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see from the output, we get similar results as with our Flask server;
    that is, a prediction of “dog” with 85% confidence.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看到，我们得到了与我们的Flask服务器相似的结果；即“狗”的预测，置信度为85%。
- en: Note
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'If this is your first time using **`gcloud`**, you need to run the following
    command to tie the command-line tool to your Google account:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是您第一次使用**`gcloud`**，您需要运行以下命令将命令行工具与您的Google账户绑定：
- en: '[PRE9]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, select the project using the following command:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令选择项目：
- en: '[PRE10]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Piece of cake, wasn’t it? In our example, we used the Google Cloud SDK to request
    a prediction for the sake of brevity. In a production scenario, you would want
    to execute the same prediction request using Google’s API endpoints, instead;
    either by generating HTTP requests or by using their client libraries. We can
    follow the documentation on Google Cloud Docs for production scenarios.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 小菜一碟，不是吗？在我们的示例中，为了简洁起见，我们使用Google Cloud SDK请求预测。在生产场景中，您可能希望使用Google的API端点执行相同的预测请求；可以通过生成HTTP请求或使用他们的客户端库来执行。我们可以在Google
    Cloud文档中查看生产场景的文档。
- en: At this point, the model is ready to be served to any user anywhere in the world
    using applications on the browser, mobile and edge devices, desktop, as well as
    cloud environments. Using a hosted stack is a pretty viable option for individuals
    and organizations who want the flexibility and reliability that the cloud provides
    while having to do minimal setup and maintenance work for the infrastructure.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，该模型已准备好通过浏览器、移动设备和边缘设备、桌面以及云环境上的应用程序提供给全球任何用户。对于希望在云提供的灵活性和可靠性的个人和组织来说，使用托管堆栈是一个相当可行的选择，同时对基础设施的设置和维护工作要求最小。
- en: In contrast, there are situations for which a hosted solution might not be the
    best approach. Reasons could include pricing models, data privacy issues, legal
    questions, technical issues, trust concerns, or contractual obligations. In such
    cases, a solution that is hosted and managed locally (or, “on-premises”) would
    be preferable.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，有些情况下托管解决方案可能不是最佳选择。原因可能包括定价模型、数据隐私问题、法律问题、技术问题、信任问题或合同义务。在这种情况下，一个在本地托管和管理的解决方案将更可取。
- en: Tip
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For processing a large number of images at one time, you can modify *image-to-json.py*
    to create a *request.json* file that contains an array of multiple inputs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了一次处理大量图像，您可以修改*image-to-json.py*以创建包含多个输入数组的*request.json*文件。
- en: TensorFlow Serving
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Serving
- en: TensorFlow Serving is an open source library in the TensorFlow ecosystem for
    serving machine learning models fast. Unlike Flask, it’s built for performance,
    with low overhead, and designed for use in production. TensorFlow Serving is widely
    used by large companies to serve their models for prediction services. It is one
    of the integral components of TensorFlow Extended (TFX)—an end-to-end deep learning
    pipeline in the TensorFlow ecosystem.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving是TensorFlow生态系统中用于快速提供机器学习模型的开源库。与Flask不同，它专为性能而构建，具有低开销，并设计用于生产环境中使用。大公司广泛使用TensorFlow
    Serving来为他们的预测服务提供模型。它是TensorFlow Extended (TFX)的一个重要组件，TFX是TensorFlow生态系统中的端到端深度学习管道之一。
- en: 'As we saw when we looked at the desired qualities of a production system, TensorFlow
    serving offers low latency, failure handling, high throughput, and model versioning.
    Another benefit includes the ability to serve multiple models at the same time
    on the same service. It implements several techniques to speed up serving:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在查看生产系统所需的特性时看到的，TensorFlow Serving提供低延迟、故障处理、高吞吐量和模型版本控制。另一个好处是能够在同一服务上同时提供多个模型。它实现了几种技术来加速服务：
- en: During server startup, it starts a burst of threads for fast model loading.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在服务器启动期间，它启动了一批线程以快速加载模型。
- en: It uses separate thread pools for loading models and for serving inferences
    while giving higher priority to the threads in the inference pool. This is crucial
    to lowering request latency.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为加载模型和提供推理使用单独的线程池，同时给予推理池中的线程更高的优先级。这对降低请求延迟至关重要。
- en: It builds minibatches of incoming asynchronous requests for short periods of
    time. As we have seen, with the power of batching data on GPUs during training,
    it aims to bring similar efficiencies on asynchronous requests. As an example,
    waiting 500 ms to group together several requests for inference. While at worst
    case, this adds a 500 ms penalty for the first request in the batch, it reduces
    the average latency across requests and maximizes hardware utilization.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在短时间内构建传入异步请求的小批量。正如我们所见，在训练期间在GPU上批处理数据的强大功能，它旨在在异步请求中带来类似的效率。例如，等待500毫秒来将几个推理请求分组。在最坏的情况下，这会为批处理中的第一个请求增加500毫秒的惩罚，但它会减少请求的平均延迟，并最大化硬件利用率。
- en: Note
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: TensorFlow serving gives you full control over the model rollout procedure.
    You can serve different models or different versions of the same kind of model
    in the same process. You just need to make sure that you know the name and location
    of the version that you want to remove or put into production.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving让你完全控制模型发布过程。你可以在同一个进程中为不同的模型或同一种模型的不同版本提供服务。你只需要确保你知道要删除或投入生产的版本的名称和位置。
- en: Installation
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: 'There are a few different ways of setting up TensorFlow Serving:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的设置TensorFlow Serving的方法：
- en: Building from source
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从源代码构建
- en: Downloading and installing using APT
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用APT下载和安装
- en: Deploying Docker images
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署Docker镜像
- en: If you’re feeling adventurous, building from source might be the danger you
    seek. But if you just want to get up and running quickly, we recommend using Docker
    because it requires minimal steps to get the system up and running. What is Docker,
    you might ask? Docker provides virtualization of a Linux environment for applications
    that run within it. It provides isolation of resources that essentially operate
    as a clean slate for setting up an environment in which an application can run.
    Typically, an application and all of its dependencies are packaged into a single
    Docker container that can then be deployed repeatedly as necessary. Because the
    application is set up in a clean environment, it reduces the likelihood of configuration
    and deployment errors. This makes Docker very well suited for running applications
    in production.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感到冒险，从源代码构建可能是你寻求的危险。但如果你只是想快速启动并运行，我们建议使用Docker，因为它需要最少的步骤来启动系统。你可能会问，什么是Docker？Docker为在其中运行的应用程序提供了Linux环境的虚拟化。它提供了资源隔离，基本上作为一个干净的板凳来设置一个应用程序可以运行的环境。通常，一个应用程序及其所有依赖项被打包到一个单独的Docker容器中，然后根据需要重复部署。因为应用程序是在一个干净的环境中设置的，它减少了配置和部署错误的可能性。这使得Docker非常适合在生产中运行应用程序。
- en: The biggest benefit that Docker provides for us is alleviating “dependency hell”
    because all the necessary dependencies are packaged within the container. One
    additional advantage of using Docker is that the process of setting up your application
    remains more or less the same across different platforms, whether you use Windows,
    Linux, or Mac.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Docker为我们提供的最大好处是减轻“依赖地狱”，因为所有必要的依赖项都打包在容器中。使用Docker的另一个优势是，在不同平台上设置应用程序的过程基本上保持一致，无论你使用Windows、Linux还是Mac。
- en: 'The Docker installation instructions, depending on the target platform, are
    available on the Docker home page. This should not take more than a few minutes
    because the setup is fairly straightforward. After you’ve installed Docker, you
    can run the following command to set up TensorFlow Serving for CPU:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Docker安装说明，取决于目标平台，可在Docker主页上找到。这应该不会花费太多时间，因为设置非常简单。安装完Docker后，你可以运行以下命令来为CPU设置TensorFlow
    Serving：
- en: '[PRE11]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For GPU-enabled machines, run the following command, instead:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于启用GPU的机器，运行以下命令：
- en: '[PRE12]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In either case, if everything went smoothly, you should have a REST API running
    on your local port 8501 serving our Dog/Cat classifier.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种情况，如果一切顺利，你应该在本地端口8501上运行一个REST API，为我们的狗/猫分类器提供服务。
- en: Note
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In any inference request, the end-to-end latency is a summation of time taken
    by multiple steps along the process. This includes round-trip network time, request
    time to serialize/deserialize the request and response objects, and, of course,
    time to perform the actual inference. One more component that adds overhead is
    the serving framework; that is, TensorFlow Serving. Google claims that the overhead
    contributed by TensorFlow Serving is minimal. In its experiments, it observed
    that TensorFlow Serving alone was able to handle approximately 100,000 QPS per
    core on a 16 vCPU Intel Xeon E5 2.6 GHz machine. Because it is measuring the overhead,
    this excludes the remote procedure call (RPC) time and the TensorFlow inference
    processing time.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何推理请求中，端到端延迟是沿着过程中多个步骤所花费的时间的总和。这包括往返网络时间，序列化/反序列化请求和响应对象的时间，当然还有执行实际推理的时间。增加开销的另一个组件是服务框架；也就是TensorFlow
    Serving。谷歌声称TensorFlow Serving贡献的开销很小。在实验中，它观察到TensorFlow Serving单独能够在16核Intel
    Xeon E5 2.6 GHz机器上每个核心处理大约100,000个QPS。因为它正在测量开销，这不包括远程过程调用（RPC）时间和TensorFlow推理处理时间。
- en: Even though TensorFlow Serving is a great choice for serving inferences from
    a single machine, it does not have built-in functionality for horizontal scaling.
    Instead, it is built to be used in conjunction with other systems that can supercharge
    TensorFlow Serving with dynamic scaling. We explore one such solution in the following
    section.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管TensorFlow Serving是从单台机器提供推理的绝佳选择，但它没有用于水平扩展的内置功能。相反，它被设计用于与其他系统一起使用，这些系统可以通过动态扩展来增强TensorFlow
    Serving。我们将在下一节中探讨一个这样的解决方案。
- en: KubeFlow
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KubeFlow
- en: Throughout this book, we have explored the various steps of an end-to-end deep
    learning pipeline, from data ingestion, analysis, distributed training (including
    hyperparameter tuning) at scale, tracking experiments, deployment, and eventually
    to serving prediction requests at scale. Each of these steps is complex in its
    own right, with its set of tools, ecosystems, and areas of expertise. People dedicate
    their lifetimes developing expertise in just one of these fields. It’s not exactly
    a walk in the park. The combinatorial explosion of the know-how required when
    factoring for the necessary backend engineering, hardware engineering, infrastructure
    engineering, dependency management, DevOps, fault tolerance, and other engineering
    challenges can result in a very expensive hiring process for most organizations.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们探讨了端到端深度学习流水线的各个步骤，从数据摄入、分析、规模化的分布式训练（包括超参数调整）、跟踪实验、部署，最终到规模化提供预测请求。每个步骤都有其自身的复杂性，具有一套工具、生态系统和专业领域。人们将毕生精力投入到这些领域中的一个。这并不是一件轻而易举的事情。当考虑到必要的后端工程、硬件工程、基础设施工程、依赖管理、DevOps、容错和其他工程挑战时，所需的专业知识的组合爆炸可能导致大多数组织的招聘过程非常昂贵。
- en: As we saw in the previous section, Docker saves us the hassle of dependency
    management by making portable containers available. It helps us make TensorFlow
    Serving available across platforms easily without having to build it from source
    code or install dependencies manually. Great! But it still doesn’t have an answer
    to many of the other challenges. How are we going to scale up containers to match
    rises in demand? How would we efficiently distribute traffic across containers?
    How do we ensure that the containers are visible to one another and can communicate?
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中看到的，Docker通过提供可移植的容器，省去了我们管理依赖关系的麻烦。它帮助我们轻松地在各个平台上提供TensorFlow Serving，而无需从源代码构建或手动安装依赖项。太好了！但它仍然没有回答许多其他挑战。我们如何扩展容器以匹配需求的增长？我们如何有效地在容器之间分发流量？我们如何确保容器彼此可见并能够通信？
- en: These are questions answered by *Kubernetes*. Kubernetes is an orchestration
    framework for automatically deploying, scaling, and managing containers (like
    Docker). Because it takes advantage of the portability offered by Docker, we can
    use Kubernetes to deploy to developer laptops as well as thousand-machine clusters
    in an almost identical manner. This helps maintain consistency across different
    environments, with the added benefit of scalability in an accessible manner. It
    is worth noting that Kubernetes is not a dedicated solution for machine learning
    (neither is Docker); rather, it is a general-purpose solution to many of the problems
    faced in software development, which we use in the context of deep learning.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题是由*Kubernetes*回答的。Kubernetes是一个自动部署、扩展和管理容器（如Docker）的编排框架。由于它利用了Docker提供的可移植性，我们可以使用Kubernetes在开发人员笔记本电脑和几千台机器集群上几乎相同的方式部署。这有助于在不同环境中保持一致性，并具有可访问的可扩展性。值得注意的是，Kubernetes不是机器学习的专用解决方案（Docker也不是）；相反，它是解决软件开发中许多问题的通用解决方案，我们在深度学习的背景下使用它。
- en: But let’s not get ahead of ourselves just yet. After all, if Kubernetes were
    the be-all and end-all solution, it would have appeared in the chapter title!
    A machine learning practitioner using Kubernetes still needs to assemble all of
    the appropriate sets of containers (for training, deployment, monitoring, API
    management, etc.) that then need to be orchestrated together to make a fully functioning
    end-to-end pipeline. Unfortunately, many data scientists are trying to do exactly
    this in their own silos, reinventing the wheel building ad hoc machine learning-specific
    pipelines. Couldn’t we save everyone the trouble and make one Kubernetes-based
    solution for machine learning scenarios?
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们不要过于急躁。毕竟，如果Kubernetes是一切的解决方案，它就会出现在章节标题中！使用Kubernetes的机器学习从业者仍然需要组装所有适当的容器集合（用于训练、部署、监控、API管理等），然后需要将它们协调在一起，以构建一个完全运作的端到端流水线。不幸的是，许多数据科学家正在尝试在他们自己的孤立环境中做这件事，重新发明轮子，构建临时的机器学习特定流水线。我们能不能省去所有人的麻烦，为机器学习场景制定一个基于Kubernetes的解决方案呢？
- en: Enter *KubeFlow*, which promises to automate a large chunk of these engineering
    challenges and hide the complexity of running a distributed, scalable, end-to-end
    deep learning system behind a web GUI-based tool and a powerful command-line tool.
    This is more than just an inference service. Think of it as a large ecosystem
    of tools that can interoperate seamlessly and, more importantly, scale up with
    demand. KubeFlow is built for the cloud. Though not just one cloud—it’s built
    to be compatible with all major cloud providers. This has significant implications
    on cost. Because we are not tied to a specific cloud provider, we have the freedom
    to move all of our operations at a moment’s notice if a competing cloud provider
    drops its prices. After all, competition benefits consumers.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*KubeFlow*登场了，它承诺自动化大部分这些工程挑战，并隐藏了运行分布式、可扩展、端到端深度学习系统的复杂性背后的一个基于Web GUI的工具和一个强大的命令行工具。这不仅仅是一个推理服务。把它看作一个大型工具生态系统，可以无缝地互操作，更重要的是，可以随需求扩展。KubeFlow是为云构建的。虽然不只是一个云——它建立在与所有主要云提供商兼容的基础上。这对成本有重大影响。因为我们不受限于特定的云提供商，如果竞争云提供商降低价格，我们可以随时自由地转移所有操作。毕竟，竞争有利于消费者。'
- en: KubeFlow supports a variety of hardware infrastructure, from developer laptops
    and on-premises datacenters, all the way to public cloud services. And because
    it’s built on top of Docker and Kubernetes, we can rest assured that the environments
    will be identical whether deployed on a developer laptop or a large cluster in
    a datacenter. Every single way in which the developer setup is different from
    the production environment could result in an outage, so it’s really valuable
    to have this consistency across environments.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: KubeFlow支持各种硬件基础设施，从开发人员的笔记本电脑和本地数据中心，一直到公共云服务。由于它是建立在Docker和Kubernetes之上的，我们可以放心，无论是在开发人员的笔记本电脑上部署还是在数据中心的大型集群上部署，环境都是相同的。开发人员设置与生产环境不同的任何方式都可能导致故障，因此在各种环境中保持一致性非常有价值。
- en: '[Table 9-4](part0011.html#tools_available_on_kubeflow) shows a brief list of
    readily available tools within the KubeFlow ecosystem.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[表9-4](part0011.html#tools_available_on_kubeflow)展示了KubeFlow生态系统中现成可用的工具的简要列表。'
- en: Table 9-4\. Tools available on KubeFlow
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-4\. KubeFlow上可用的工具
- en: '| **Tool** | **Functionality** |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **工具** | **功能** |'
- en: '| --- | --- |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Jupyter Hub | Notebook environment |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Jupyter Hub | 笔记本环境 |'
- en: '| TFJob | Training TensorFlow models |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| TFJob | 训练TensorFlow模型 |'
- en: '| TensorFlow Serving | Serving TensorFlow models |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow Serving | 服务TensorFlow模型 |'
- en: '| Seldon | Serving models |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Seldon | 服务模型 |'
- en: '| NVIDIA TensorRT | Serving models |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| NVIDIA TensorRT | 服务模型 |'
- en: '| Intel OpenVINO | Serving models |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Intel OpenVINO | 服务模型 |'
- en: '| KFServing | Abstraction for serving Tensorflow, XGBoost, scikit-learn, PyTorch,
    and ONNX models |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| KFServing | 用于服务Tensorflow、XGBoost、scikit-learn、PyTorch和ONNX模型的抽象 |'
- en: '| Katib | Hyperparameter tuning and NAS |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Katib | 超参数调整和NAS |'
- en: '| Kubebench | Running benchmarking jobs |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Kubebench | 运行基准测试作业 |'
- en: '| PyTorch | Training PyTorch models |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch | 训练PyTorch模型 |'
- en: '| Istio | API services, authentication, A/B testing, rollouts, metrics |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Istio | API服务、身份验证、A/B测试、发布、指标 |'
- en: '| Locust | Load testing |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Locust | 负载测试 |'
- en: '| Pipelines | Managing experiments, jobs, and runs, scheduling machine learning
    workflows |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 管道 | 管理实验、作业和运行，调度机器学习工作流程 |'
- en: As the joke goes in the community, with so many technologies prepackaged, KubeFlow
    finally makes our résumés buzzword- (and recruiter-) compliant.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 社区中有一个笑话，有这么多预打包技术，KubeFlow最终使我们的简历符合流行词（和招聘人员）的要求。
- en: Note
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Many people assume that KubeFlow is a combination of Kubernetes and TensorFlow,
    which, as you have seen, is not the case. It is that and much more.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人认为KubeFlow是Kubernetes和TensorFlow的组合，但正如您所见，事实并非如此。它是那样，而且更多。
- en: 'There are two important parts to KubeFlow that make it unique: pipelines and
    fairing.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: KubeFlow的两个重要部分使其独特：管道和公平。
- en: Pipelines
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道
- en: Pipelines give us the ability to compose steps across the machine learning to
    schedule complex workflows. [Figure 9-11](part0011.html#an_end-to-end_pipeline_illustrated_in_ku)
    shows us an example of a pipeline. Having visibility into the pipeline through
    a GUI tool helps stakeholders understand it (beyond just the engineers who built
    it).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 管道使我们能够在机器学习中组合步骤，安排复杂的工作流程。[图9-11](part0011.html#an_end-to-end_pipeline_illustrated_in_ku)展示了一个管道的示例。通过GUI工具查看管道可以帮助利益相关者理解它（不仅仅是构建它的工程师）。
- en: '![An end-to-end pipeline illustrated in KubeFlow](../images/00094.jpeg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![在KubeFlow中展示的端到端管道](../images/00094.jpeg)'
- en: Figure 9-11\. An end-to-end pipeline illustrated in KubeFlow
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-11\. 在KubeFlow中展示的端到端管道
- en: Fairing
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平
- en: 'Fairing allows us to manage the entire build, train, and deploy lifecycle directly
    through Jupyter Notebooks. [Figure 9-12](part0011.html#creating_a_new_jupyter_notebook_server_o)
    shows how to start a new notebook server, where we can host all of our Jupyter
    Notebooks, run training on them, and deploy our models to Google Cloud using the
    following few lines of code, all the while being in the comfort of a very familiar
    Jupyter environment:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 公平允许我们通过Jupyter Notebooks直接管理整个构建、训练和部署生命周期。[图9-12](part0011.html#creating_a_new_jupyter_notebook_server_o)展示了如何启动一个新的笔记本服务器，在那里我们可以托管所有的Jupyter笔记本，在上面运行训练，并使用几行代码将我们的模型部署到谷歌云，同时还可以在非常熟悉的Jupyter环境中进行操作：
- en: '[PRE13]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Creating a new Jupyter Notebook server on KubeFlow](../images/00197.jpeg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![在KubeFlow上创建一个新的Jupyter Notebook服务器](../images/00197.jpeg)'
- en: Figure 9-12\. Creating a new Jupyter Notebook server on KubeFlow
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-12\. 在KubeFlow上创建一个新的Jupyter Notebook服务器
- en: Installation
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: Creating a new KubeFlow deployment is a fairly straightforward process that
    is well documented on the KubeFlow website. You can set up KubeFlow using the
    browser for GCP. Alternatively, you can use the KubeFlow command-line tool to
    set up a deployment on GCP, AWS, and Microsoft Azure. [Figure 9-13](part0011.html#creating_a_kubeflow_deployment_on_gcp_us)
    shows a GCP deployment using the web browser.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的KubeFlow部署是一个非常简单的过程，在KubeFlow网站上有详细的文档。您可以使用浏览器为GCP设置KubeFlow。或者，您可以使用KubeFlow命令行工具在GCP、AWS和Microsoft
    Azure上设置部署。[图9-13](part0011.html#creating_a_kubeflow_deployment_on_gcp_us)展示了使用Web浏览器进行GCP部署。
- en: '![Creating a KubeFlow deployment on GCP using the browser](../images/00014.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![在浏览器上创建一个在GCP上使用KubeFlow部署](../images/00014.jpeg)'
- en: Figure 9-13\. Creating a KubeFlow deployment on GCP using the browser
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-13\. 在浏览器上创建一个在GCP上使用KubeFlow部署
- en: As of this writing, KubeFlow is in active development and shows no signs of
    stopping. Companies such as Red Hat, Cisco, Dell, Uber, and Alibaba are some of
    the active contributors on top of cloud giants like Microsoft, Google, and IBM.
    Ease and accessibility for solving tough challenges attract more people to any
    platform, and KubeFlow is doing exactly that.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，KubeFlow正在积极开发，并没有停止的迹象。像红帽、思科、戴尔、优步和阿里巴巴这样的公司是一些积极的贡献者，云巨头如微软、谷歌和IBM也是如此。解决困难挑战的简易性和可访问性吸引更多人使用任何平台，而KubeFlow正是在做这件事。
- en: Price Versus Performance Considerations
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 价格与性能考虑
- en: 'In [Chapter 6](part0008.html#7K4G3-13fa565533764549a6f0ab7f11eed62b), we looked
    at how to improve our model performance for inference (whether on smartphones
    or on a server). Now let’s look from another side: the hardware performance and
    the price involved.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](part0008.html#7K4G3-13fa565533764549a6f0ab7f11eed62b)中，我们看了如何提高我们的模型性能，无论是在智能手机上还是在服务器上进行推理。现在让我们从另一个角度来看：硬件性能和涉及的价格。
- en: Often while building a production system, we want the flexibility of choosing
    suitable hardware to strike the proper balance between performance, scale, and
    price for our scenario. Consider building an app that requires cloud-based inference.
    We can go set up our own stack manually (using Flask or TensorFlow Serving or
    KubeFlow), or we could use a managed Inference-as-a-Service stack (like the Google
    Cloud ML Engine). Assuming that our service went viral, let’s see how much it
    would cost.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建生产系统时，我们通常希望灵活选择合适的硬件，以在性能、规模和价格之间取得适当的平衡。考虑构建一个需要基于云的推理的应用程序。我们可以手动设置我们自己的堆栈（使用Flask或TensorFlow
    Serving或KubeFlow），或者我们可以使用托管的推理即服务堆栈（如Google Cloud ML Engine）。假设我们的服务爆红了，让我们看看会花费多少。
- en: Cost Analysis of Inference-as-a-Service
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理即服务的成本分析
- en: For Google Cloud ML Engine, as of August 2019 in North America, it costs a rather
    inexpensive $0.0401 per hour of combined inference time on a single-core CPU machine.
    There’s also an option for a quad-core CPU machine, but really, a single core
    should suffice for most applications. Running several queries to the server with
    a small image of 12 KB took roughly 3.5 seconds on average, as illustrated in
    [Figure 9-14](part0011.html#google_cloud_ml_engine_showing_incoming). This does
    sound slow, and is partly because of doing inference on a moderate-speed machine,
    and, more important on a CPU server. It’s worth mentioning that this benchmark
    is on a warmed-up machine that has recently received an API request and hence
    has the model preloaded. For comparison, the first query takes between 30 and
    60 seconds. This shows the importance of keeping the service running constantly
    or sending frequent warm-up queries. This happens because the Google Cloud ML
    engine takes down a model if it notices a prolonged period of nonuse.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Google Cloud ML Engine，在2019年8月的北美地区，单核CPU机器的综合推理时间每小时的成本相当便宜，为0.0401美元。还有一个四核CPU机器的选项，但实际上，单核应该足够满足大多数应用程序的需求。向服务器运行几个查询，使用一个12
    KB的小图像，平均需要大约3.5秒，如[图9-14](part0011.html#google_cloud_ml_engine_showing_incoming)所示。这听起来很慢，部分原因是在一个中等速度的机器上进行推理，更重要的是在一个CPU服务器上。值得一提的是，这个基准测试是在一个已经接收到API请求并且已经预加载了模型的机器上进行的。相比之下，第一个查询需要30到60秒。这显示了保持服务持续运行或发送频繁的预热查询的重要性。这是因为如果Google
    Cloud ML引擎注意到长时间不使用，它会关闭模型。
- en: '![Google Cloud ML Engine showing incoming queries and latency of serving the
    calls, with the end-to-end latency at user’s end of about 3.5 seconds](../images/00304.jpeg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![Google Cloud ML Engine显示传入查询和提供调用的延迟，用户端的端到端延迟约为3.5秒](../images/00304.jpeg)'
- en: Figure 9-14\. Google Cloud ML Engine showing incoming queries and latency of
    serving the calls, with end-to-end latency at user’s end of about 3.5 seconds
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-14。Google Cloud ML Engine显示传入查询和提供调用的延迟，用户端的端到端延迟约为3.5秒
- en: If a request came in at every second for an entire month, there would be a total
    of 60 x 60 x 24 x 30 = 2,592,000 calls per month. Assuming that each inference
    takes 3.5 seconds, a single node would be insufficient. The cloud service would
    quickly realize that and, in response to the increased traffic, bring up three
    additional machines to handle the traffic. In total, with four machines running
    for a month at $0.0401 per hour per node, it would cost a grand total of $115.48\.
    To put this into perspective, for two million calls, that’s about the cost of
    a cup of Starbucks coffee a day for an entire month. And let’s not forget this
    is without involving much of the DevOps team members, whose time is expensive.
    If we took the hypothetical scenario of a Yelp-like service for which users, on
    average, upload photos of food at 64 QPS, running inferences on them using a classification
    model would cost only $7,390.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每秒有一个请求持续一个月，总共将有60 x 60 x 24 x 30 = 2,592,000次调用。假设每个推理需要3.5秒，单个节点将不足以满足需求。云服务会迅速意识到这一点，并针对增加的流量，启动三台额外的机器来处理流量。总共，四台机器在每个节点每小时0.0401美元的情况下运行一个月，总成本将为115.48美元。换句话说，对于两百万次调用，这大约相当于一个月每天一杯星巴克咖啡的成本。而且我们不要忘记这还没有涉及太多的DevOps团队成员，他们的时间是昂贵的。如果我们考虑一个类似Yelp的服务的假设情景，用户平均每秒上传64个QPS的食物照片，使用分类模型对其进行推理只需要花费7390美元。
- en: Cost Analysis of Building Your Own Stack
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建自己的堆栈的成本分析
- en: Less spending and high scalability, now that’s a winning combination. But the
    one downside is the total roundtrip latency of each request. Taking matters into
    our own hands, getting a VM with a modest GPU on the cloud, and setting up our
    scaling pipeline (using KubeFlow or the native cloud load-balancing features with
    TensorFlow Serving), we would be able to respond either in milliseconds or batch
    a few incoming queries together (say every 500 ms) to serve them. As an example,
    looking at the inventory of VMs on Azure, for $2.07 per hour, we can rent out
    an ND6 machine that features an NVIDIA P40 GPU and 112 GiB RAM. By batching incoming
    requests every 500 ms to 1 second, this machine can serve 64 requests per second
    at a total cost of $1,490 per month, and faster than the Google Cloud ML Engine.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 少花钱，高可扩展性，这是一个成功的组合。但唯一的缺点是每个请求的总往返延迟。自己动手，获取一个在云上带有适度GPU的虚拟机，并设置我们的扩展管道（使用KubeFlow或使用TensorFlow
    Serving的本地云负载平衡功能），我们可以在毫秒内做出响应，或者批量处理几个传入的查询（比如每500毫秒）来提供服务。例如，在Azure的VM清单中，我们可以以每小时2.07美元的价格租用一台ND6机器，该机器配备了NVIDIA
    P40 GPU和112 GiB RAM。通过每500毫秒到1秒批量处理传入请求，这台机器可以以每月1490美元的总成本为64个请求每秒提供服务，比Google
    Cloud ML Engine更快。
- en: In summary, the cost savings and performance benefits of orchestrating our own
    cloud machine environment kicks in big time when working on large QPS scenarios,
    as demonstrated in [Figure 9-15](part0011.html#cost_comparison_of_infrastructure_as_a_s).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在处理大量QPS场景时，自己编排云机器环境的成本节约和性能优势在[图9-15](part0011.html#cost_comparison_of_infrastructure_as_a_s)中得到了充分展示。
- en: '![Cost comparison of Infrastructure as a service (Google Cloud ML Engine) versus
    Building Your Own Stack over Virtual Machines (Azure VM) (Costs as of August 2019)](../images/00244.jpeg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![基础设施即服务（Google Cloud ML Engine）与在虚拟机上构建自己的堆栈（Azure VM）的成本比较（截至2019年8月的成本）](../images/00244.jpeg)'
- en: Figure 9-15\. Cost comparison of infrastructure as a service (Google Cloud ML
    Engine) versus building your own stack over virtual machines (Azure VM) (costs
    as of August 2019)
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-15。基础设施即服务（Google Cloud ML Engine）与在虚拟机上构建自己的堆栈（Azure VM）的成本比较（截至2019年8月的成本）
- en: Tip
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A common question that arises while benchmarking is what is my system’s limit?
    [JMeter](https://jmeter.apache.org) can help answer this. JMeter is a load-testing
    tool that lets you perform stress testing of your system with an easy-to-use graphical
    interface. It lets you create reusable configurations to simulate a variety of
    usage scenarios.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在基准测试中经常出现的一个问题是我的系统的极限是什么？[JMeter](https://jmeter.apache.org)可以帮助回答这个问题。JMeter是一个负载测试工具，可以让您通过易于使用的图形界面对系统进行压力测试。它允许您创建可重用的配置来模拟各种使用场景。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we answered the question most engineers and developers ask:
    how do we serve model prediction requests at scale for applications in the real
    world? We explored four different methods of serving an image recognition model:
    using Flask, Google Cloud ML, TensorFlow Serving, and KubeFlow. Depending on the
    scale, latency requirements, and our skill level, some solutions might be more
    attractive than others. Finally, we developed an intuition into the cost effectiveness
    of different stacks. Now that we can show our fabulous classifier model off to
    the world, all that’s left is to make our work go viral!'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们回答了大多数工程师和开发人员提出的问题：在现实世界的应用程序中如何扩展为规模化的模型预测请求？我们探讨了四种不同的方法来提供图像识别模型：使用Flask、Google
    Cloud ML、TensorFlow Serving和KubeFlow。根据规模、延迟要求和我们的技能水平，一些解决方案可能比其他更具吸引力。最后，我们对不同堆栈的成本效益有了直观的了解。现在我们可以向世界展示我们出色的分类器模型，剩下的就是让我们的工作走向病毒式传播！
