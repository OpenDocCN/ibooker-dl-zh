- en: Chapter 9\. Scalable Inference Serving on Cloud with TensorFlow Serving and
    KubeFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。使用TensorFlow Serving和KubeFlow在云上进行可扩展推断服务
- en: 'Imagine this: you just built a top-notch classifier. Your goal, as the Silicon
    Valley motto goes, is to “*make the world a better place*,” which you’re going
    to do... with a spectacular Dog/Cat classifier. You have a solid business plan
    and you cannot wait to pitch your magical classifier to that venture capital firm
    next week. You know that the investors will question you about your cloud strategy,
    and you need to show a solid demo before they even consider giving you the money.
    How would you do this? Creating a model is half the battle, serving it is the
    next challenge, often the bigger one. In fact, for a long time it was common for
    training a model to only take a few weeks, but trying to serve it to a larger
    group of people was a months-long battle, often involving backend engineers and
    DevOps teams.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下：你刚刚建立了一个一流的分类器。你的目标，正如硅谷的座右铭所说，“*让世界变得更美好*”，你将通过一个出色的狗/猫分类器来实现这一目标。你有一个扎实的商业计划，迫不及待地想要在下周向风险投资公司推销你的神奇分类器。你知道投资者会问及你的云策略，你需要在他们考虑给你钱之前展示一个扎实的演示。你会怎么做？创建模型只是战斗的一半，提供服务是下一个挑战，通常是更大的挑战。事实上，很长一段时间以来，训练模型只需要几周的时间，但是试图将其提供给更多人使用是一个长达数月的战斗，通常涉及后端工程师和DevOps团队。
- en: In this chapter, we answer a few questions that tend to come up in the context
    of hosting and serving custom-built models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回答了一些关于托管和提供定制模型的常见问题。
- en: How can I host my model on my personal server so that my coworkers can play
    with it?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何在我的个人服务器上托管我的模型，以便我的同事可以使用它？
- en: I am not a backend/infrastructure engineer, but I want to make my model available
    so that it can serve thousands (or even millions) of users. How can I do this
    at a reasonable price without worrying about scalability and reliability issues?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我不是后端/基础设施工程师，但我想让我的模型可用，以便为成千上万（甚至数百万）的用户提供服务。如何在合理的价格下做到这一点，而不必担心可扩展性和可靠性问题？
- en: There are reasons (such as cost, regulations, privacy, etc.) why I cannot host
    my model on the cloud, but only on-premises (my work network). Can I serve predictions
    at scale and reliably in such a case?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些原因（如成本、法规、隐私等）使我无法将我的模型托管在云上，而只能在内部网络（我的工作网络）上托管。在这种情况下，我能够以规模和可靠性提供预测吗？
- en: Can I do inference on GPUs?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以在GPU上进行推断吗？
- en: How much can I expect to pay for each of these options?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以期待每个选项的费用是多少？
- en: Could I scale my training and serving across multiple cloud providers?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能否跨多个云提供商扩展我的训练和服务？
- en: How much time and technical know-how will it take to get these running?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使这些运行需要多少时间和技术知识？
- en: Let’s begin our journey by looking at the high-level overview of the tools available
    to us.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从高层次概述可用工具开始我们的旅程。
- en: Landscape of Serving AI Predictions
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供AI预测的景观
- en: There is a multitude of tools, libraries, and cloud services available for getting
    trained AI models to serve prediction requests. [Figure 9-1](part0011.html#a_high-level_overview_and_comparison_of)
    simplifies them into four categories.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多工具、库和云服务可用于训练AI模型以提供预测请求。[图9-1](part0011.html#a_high-level_overview_and_comparison_of)将它们简化为四类。
- en: '![A high-level overview and comparison of different inference serving options.](../images/00100.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![不同推断服务选项的高级概述和比较。](../images/00100.jpeg)'
- en: Figure 9-1\. A high-level overview and comparison of different inference serving
    options
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 不同推断服务选项的高级概述和比较
- en: Depending on our inference scenarios, we can make an appropriate choice. [Table 9-1](part0011.html#tools_to_serve_deep_learning_models_over)
    takes a deeper look.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的推断场景，我们可以做出适当的选择。[表9-1](part0011.html#tools_to_serve_deep_learning_models_over)深入探讨。
- en: Table 9-1\. Tools to serve deep learning models over the network
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-1。通过网络提供深度学习模型的工具
- en: '| **Category and examples** | **Expected time to first prediction** | **Pros
    and cons** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **类别和示例** | **预期的首次预测时间** | **优缺点** |'
- en: '| --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| HTTP servers'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '| HTTP服务器'
- en: Flask
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flask
- en: Django
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Django
- en: Apache OpenWhisk
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache OpenWhisk
- en: Python `http.server`
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python `http.server`
- en: '| <5 minutes | + Simple to run+ Often runs current Python code– Slow– Not optimized
    for AI |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| <5分钟 | + 运行简单+ 通常运行当前Python代码– 较慢– 未经AI优化 |'
- en: '| **Hosted and managed cloud stacks**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '| **托管和管理的云堆栈**'
- en: Google Cloud ML
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud ML
- en: Azure ML
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure ML
- en: Amazon Sage Maker
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊Sage Maker
- en: Algorithmia
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Algorithmia
- en: '| <15 minutes | + Easier GUI/command-line interfaces+ Highly scalable+ Fully
    managed, reduces the need for DevOps teams– Usually limited to CPU-based inference,
    can be slow for large models– Warm-up query time can be slow |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| <15分钟 | + 更容易的GUI/命令行界面+ 高度可扩展+ 完全托管，减少了对DevOps团队的需求– 通常仅限于基于CPU的推断，对于大型模型可能较慢–
    预热查询时间可能较慢 |'
- en: '| **Manually managed serving libraries**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '| **手动管理的服务库**'
- en: TensorFlow Serving
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Serving
- en: NVIDIA TensorRT
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英伟达TensorRT
- en: DeepDetect
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepDetect
- en: MXNet Model Serving
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MXNet模型服务
- en: Skymind Intelligence Layer with DeepLearning4J
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skymind Intelligence Layer with DeepLearning4J
- en: Seldon
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seldon
- en: DeepStack AI Server
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepStack AI服务器
- en: '| <15 minutes | + High performance+ Allows manual controls on optimizations,
    batching, etc.+ Can run inference on GPU– More involved setup– Scaling over multiple
    nodes usually requires extra groundwork |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| <15分钟 | + 高性能+ 允许手动控制优化、批处理等+ 可以在GPU上运行– 设置更复杂– 要在多个节点上扩展通常需要额外的工作 |'
- en: '| **Cloud AI orchestration frameworks**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '| **云AI编排框架**'
- en: KubeFlow
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KubeFlow
- en: '| ~1 hour | + Makes scaling training and inference easy to manage+ Portability
    between cloud providers+ Consistent environments across development and production+
    For data scientists, integration with familiar tools such as Jupyter Notebooks
    for sending models to production+ Enables composing conditional pipelines to automate
    testing, cascading models+ Uses existing manually managed serving libraries– Still
    evolving– For beginners, hosted and managed cloud stacks offer an easier learning
    curve |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: In this chapter, we explore a range of tools and scenarios. Some of these options
    are easy to use but limited in functionality. Others offer more granular controls
    and higher performance but are more involved to set up. We look at one example
    of each category and take a deeper dive to develop an intuition into when using
    one of those makes sense. We then present a cost analysis of the different solutions
    as well as case studies detailing how some of these solutions work in practice
    today.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Flask: Build Your Own Server'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin with the most basic technique of *Build Your Own Server* (BYOS). From
    the choices presented in the first column of [Table 9-1](part0011.html#tools_to_serve_deep_learning_models_over),
    we’ve selected Flask.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Making a REST API with Flask
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flask is a Python-based web application framework. Released in 2010 and with
    more than 46,000 stars on GitHub, it is under continuous development. It’s also
    quick and easy to set up and is really useful for prototyping. It is often the
    framework of choice for data science practitioners when they want to serve their
    models to a limited set of users (e.g., sharing with coworkers on a corporate
    network) without a lot of fuss.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing Flask with `pip` is fairly straightforward:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Upon installation, we should be able to run the following simple “Hello World”
    program:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following is the command to run the “Hello World” program:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By default, Flask runs on port 5000\. When we open the URL *http://localhost:5000/hello*
    in the browser, we should see the words “Hello World!,” as shown in [Figure 9-2](part0011.html#navigate_to_httpcolonsolidussoliduslocal).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Navigate to http://localhost:5000/hello within a web browser to view the
    “Hello World!” web page](../images/00132.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Navigate to http://localhost:5000/hello within a web browser to
    view the “Hello World!” web page
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, it takes barely more than a few lines to get a simple web application
    up and running. One of the most important lines in that script is `@app.route("/hello")`.
    It specifies that the path `/hello` after the hostname would be served by the
    method immediately beneath it. In our case, it merely returns the string “Hello
    World!” In the next step, we look at how to deploy a Keras model to a Flask server
    and create a route that will serve predictions by our model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Keras Model to Flask
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first step is to load our Keras model. The following lines load the model
    from the .*h5* file. You’ll find the scripts for this chapter on the book’s GitHub
    (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)) in
    *code/chapter-9*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we create the route */infer* that would support inference on our images.
    Naturally, we would support `POST` requests to accept images:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To test the inference, let’s use the `curl` command, as follows, on a sample
    image containing a dog:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As expected, we get a prediction of “dog.” This has worked quite well so far.
    At this point, Flask runs only locally; that is, someone else on the network would
    not be able to make a request to this server. To make Flask available to others,
    we can simply change `app.run()` to the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'At this point, we can give access to our model to anyone within our network.
    The next question would be—can we do the same to make the model available to the
    general public? The answer to that question is an emphatic no! The Flask website
    has a prominent warning stating *“WARNING: Do not use the development server in
    a production environment.”* Flask indeed does not support production work out
    of the box and would need custom code to enable that. In the upcoming sections,
    we look at how to host our models on systems that are meant for production use.
    With all of this in mind, let’s recap some of the pros and cons of using Flask.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Pros of Using Flask
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Flask provides some advantages, namely:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Quick to set up and to prototype
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast development cycle
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightweight on resources
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Broad appeal within the Python community
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons of Using Flask
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the same time, Flask might not be your best choice, for the following reasons:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Cannot scale; by default, it is not meant for production loads. Flask can serve
    only one request at one time
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not handle model versioning out of the box
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not support batching of requests out of the box
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Desirable Qualities in a Production-Level Serving System
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For any cloud service that is serving traffic from the public, there are certain
    attributes that we want to look for when deciding to use a solution. In the context
    of machine learning, there are additional qualities that we would look for while
    building inference services. We look at a few of them if this section.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: High Availability
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our users to trust our service, it must be available almost always. For
    many serious players, they measure their availability metric in terms of “*number
    of nines*.” If a business claims that its service has four 9s availability, they
    mean the system is up and available 99.99% of the time. Even though 99% sounds
    impressive, [Table 9-2](part0011.html#downtime_per_year_for_different_availabi)
    puts that downtime per year in perspective.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-2\. Downtime per year for different availability percentages
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '| **Availability %** | **Downtime per year** |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| 99% (“two nines”) | 3.65 days |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '| 99.9% (“three nines”) | 8.77 hours |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '| 99.99% (“four nines”) | 52.6 minutes |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '| 99.999% (“five nines”) | 5.26 minutes |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: Imagine how ridiculous the situation would be if a major website like Amazon
    were only 99.9% available, losing millions in user revenue during the eight-plus
    hours of downtime. Five 9s is considered the holy grail. Anything less than three
    9s is typically unsuitable for a high-quality production system.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traffic handled by production services is almost never uniform across a larger
    time period. For example, the *New York Times* experiences significantly more
    traffic during morning hours, whereas Netflix typically experiences a surge in
    traffic between the evening and late-night hours, when people chill. There are
    also seasonal factors in traffic. Amazon experiences orders of magnitude more
    traffic on Black Friday and during Christmas season.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'A higher demand requires a higher amount of resources being available and online
    to serve them. Otherwise, the availability of the system would be in jeopardy.
    A naive way to accomplish this would be to anticipate the highest volume of traffic
    the system would ever serve, determine the number of resources necessary to serve
    that level of traffic, and then allocate that amount all the time, in perpetuity.
    There are two problems with this approach: 1) if your planning was correct, the
    resources would be underutilized most of the time, essentially burning money;
    and 2) if your estimation was insufficient, you might end up affecting the availability
    of your service and end up with a far worse problem of losing the trust of your
    customers and ultimately their wallets.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: A smarter way to manage traffic loads is to monitor them as they are coming
    in and dynamically allocate and deallocate resources that are available for service.
    This ensures that the increased traffic is handled without loss of service while
    keeping operating costs to a minimum during low-traffic times.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: When scaling down resources, any resource that is about to be deallocated is
    quite likely to be processing traffic at that moment. It’s essential to ensure
    that all of those requests be completed before shutting down that resource. Also,
    crucially, the resource must not process any new requests. This process is called
    *draining*. Draining is also crucial when machines are taken down for routine
    maintenance and/or upgrades.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Low Latency
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider these facts. Amazon published a study in 2008 in which it found that
    every 100 ms increase in latency in its retail website resulted in a 1% loss of
    profit. A one-second delay in loading the website caused a whopping $1.6 billion
    in lost revenue! Google found that a 500 ms latency on mobile websites resulted
    in a traffic drop of 20%. In other words, a 20% decrease in the opportunity-to-serve
    advertisements. And this does not affect only industry giants. If a web page takes
    longer than three seconds to load on a mobile phone, 53% of users abandon it (according
    to a 2017 study by Google). It’s clear that time is money.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Reporting average latency can be misleading because it might paint a cheerier
    picture than a ground reality. It’s like saying if Bill Gates walks into a room,
    everyone is a billionaire on average. Instead, percentile latency is the typically
    reported metric. For example, a service might report 987 ms @ 99th percentile.
    This means that 99% of the requests were served in 987 ms or less. The very same
    system could have a 20 ms latency on average. Of course, as traffic to your service
    increases, the latency might increase if the service is not scaled up to give
    adequate resources. As such, latency, high availability, and scalability are intertwined.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Geographic Availability
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The distance between New York and Sydney is nearly 10,000 miles (16,000 km).
    The speed of light in a vacuum is roughly 186,282 miles per second (300,000 km
    per second). Silica glass (used in fiber-optic cables) decreases the speed of
    light by a factor of roughly 30% down to 130,487 miles per second (210,000 km
    per second). On a piece of fiber-optic running in a straight line between these
    two cities, the roundtrip travel time alone for a single request is nearly 152
    ms. Keep in mind that this does not account for the amount of time it takes for
    the request to be processed at the server, or the hops that the packets need to
    make across multiple routers along the way. This level of service would be unacceptable
    for many applications.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Services that expect to be used throughout the world must be strategically located
    to minimize latency for the users in those regions. Additionally, resources can
    be dynamically scaled up or down depending on local traffic, thus giving more
    granular control. The major cloud providers have a presence on at least five continents
    (sorry penguins!).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Want to simulate how long the incoming requests would take from your computer
    to a particular datacenter around the world? [Table 9-3](part0011.html#latency_measurement_tools_for_different)
    lists a few handy browser-based tools offered by cloud providers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-3\. Latency measurement tools for different cloud providers
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service** | **Cloud provider** |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: '| [AzureSpeed.com](http://AzureSpeed.com) | Microsoft Azure |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
- en: '| [CloudPing.info](https://CloudPing.info) | Amazon Web Services |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: '| [GCPing.com](http://GCPing.com) | Google Cloud Platform |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: Additionally, to determine realistic combinations of latency from one location
    to another, *CloudPing.co* measures AWS Inter-Region Latency, between more than
    16 US-based AWS datacenters to one another.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Failure Handling
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There’s an old saying that there are only two things that are assured in life—death
    and taxes. In the twenty-first century, this adage applies not just to humans
    but also computer hardware. Machines fail all the time. The question is never
    *if* a machine will fail, it’s *when*. One of the necessary qualities of production-quality
    service is its ability to gracefully handle failures. If a machine goes down,
    quickly bring up another machine to take its place and continue serving traffic.
    If an entire datacenter goes down, seamlessly route traffic to another datacenter
    so that users don’t even realize that anything bad happened in the first place.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有一句古老的谚语说生活中只有两件事是确定的——死亡和税收。在21世纪，这句格言不仅适用于人类，也适用于计算机硬件。机器经常出现故障。问题从来不是机器会出现故障，而是何时会出现故障。生产质量服务的一个必要特性是它能够优雅地处理故障。如果一台机器出现故障，迅速启动另一台机器来替代它并继续提供流量服务。如果整个数据中心出现故障，无缝地将流量路由到另一个数据中心，以便用户甚至不会意识到一开始发生了任何不好的事情。
- en: Monitoring
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控
- en: If you can’t measure it, you can’t improve it. Worse, does it even exist? Monitoring
    the number of requests, availability, latency, resource usage, number of nodes,
    distribution of traffic, and location of users is vital to understanding how a
    service performs; finding opportunities to improve it; and, more importantly,
    how much to pay. Most cloud providers already have built-in dashboards providing
    these metrics. Additionally, recording task-specific analytics like time for model
    inference, preprocessing, and so on can add another level of understanding.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不能测量它，你就无法改进它。更糟糕的是，它甚至存在吗？监控请求数量、可用性、延迟、资源使用情况、节点数量、流量分布和用户位置对于了解服务的表现至关重要；找到改进的机会；更重要的是，要支付多少。大多数云提供商已经内置了提供这些指标的仪表板。此外，记录任务特定的分析，如模型推断、预处理等的时间，可以增加另一层理解。
- en: Model Versioning
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型版本控制
- en: We have learned in this book (and will continue to learn all the way to the
    last page) that machine learning is always iterative. Particularly in the case
    of applications in the real world, data that the model can learn on is constantly
    being generated. Moreover, the incoming data distribution might shift over time
    compared to what it was trained on, leading to lower prediction power (a phenomenon
    called c*oncept drift*). To provide users with the best possible experience, we
    want to keep improving our models. Every time we train our model with newer data
    to further improve its accuracy and make the best version yet, we want to make
    it available for our users as quickly and seamlessly as possible. Any good production-quality
    inference system should provide the ability to provide different versions of a
    model, including the ability to swap a live version of the model with another
    version at a moment’s notice.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中学到（并将继续学习直到最后一页）机器学习始终是迭代的。特别是在现实世界的应用中，模型可以学习的数据不断生成。此外，传入数据的分布可能随时间而变化，与训练时相比，导致预测能力降低（称为概念漂移现象）。为了为用户提供最佳体验，我们希望不断改进我们的模型。每次我们使用更新的数据训练模型以进一步提高其准确性并制作最佳版本时，我们希望尽快而无缝地为用户提供。任何良好的生产质量推断系统都应该提供提供模型的不同版本的能力，包括在一瞬间交换模型的实时版本的能力。
- en: A/B Testing
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A/B测试
- en: In addition to supporting multiple versions of a model, there are reasons we’d
    want to serve different versions of the model at the same time depending on a
    variety of attributes such as the geographic location of the user, demographics,
    or simply by random assignment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 除了支持模型的多个版本之外，我们希望根据用户的地理位置、人口统计数据或简单的随机分配等各种属性，在同一时间为模型提供不同版本的原因。
- en: '*A/B testing* is a particularly useful tool when improving a model. After all,
    if our spanking new model was flawed in some way, we’d rather it be deployed to
    only a small subset of our users rather than 100% of them at the same time before
    we catch the flaw. Additionally, if a model meets criteria for success on that
    small subset, it provides validation for the experiment and justifies eventually
    being promoted to all users.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*A/B测试*是改进模型时特别有用的工具。毕竟，如果我们全新的模型在某种程度上存在缺陷，我们宁愿它只部署给我们用户的一个小子集，而不是同时部署给所有用户，以便在捕捉到缺陷之前。此外，如果模型在这个小子集上符合成功标准，它为实验提供了验证，并证明最终可以提升到所有用户。'
- en: Support for Multiple Machine Learning Libraries
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持多个机器学习库
- en: Last but not least, we don’t want to be locked into a single machine learning
    library. Some data scientists in an organization might train models in PyTorch,
    others in TensorFlow, or maybe scikit-Learn suffices for non–deep learning tasks.
    The flexibility to support multiple libraries would be a welcome bonus.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但并非最不重要的是，我们不想被锁定在单一的机器学习库中。组织中的一些数据科学家可能会在PyTorch中训练模型，其他人可能会在TensorFlow中训练，或者对于非深度学习任务，scikit-Learn可能就足够了。支持多个库的灵活性将是一个受欢迎的奖励。
- en: 'Google Cloud ML Engine: A Managed Cloud AI Serving Stack'
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud ML Engine：托管的云AI服务堆栈
- en: Considering all the desirable qualities we discussed in a production environment
    in the previous section, it’s generally not a good idea to use Flask for serving
    users. If you do not have a dedicated infrastructure team and would like to spend
    more time making better models than deploying them, using a managed cloud solution
    is the right approach. There are several cloud-based Inference-as-a-Service solutions
    on the market today. We have chosen to explore the Google Cloud ML Engine partly
    because of the convenient TensorFlow integration and partly because it ties in
    nicely with the ML Kit material that we touch upon in [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们在前一节中讨论的生产环境中所有理想的特性，使用Flask为用户提供服务通常不是一个好主意。如果您没有专门的基础设施团队，并且希望花更多时间制作更好的模型而不是部署它们，那么使用托管的云解决方案是正确的方法。今天市场上有几种基于云的推断即服务解决方案。我们选择探索Google
    Cloud ML Engine部分原因是因为方便的TensorFlow集成，部分原因是因为它与我们在第13章中提到的ML Kit材料很好地结合在一起。
- en: Pros of Using Cloud ML Engine
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Cloud ML Engine的优点
- en: Easy-to-deploy models in production with web-based GUI
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于部署到生产环境中，具有基于Web的GUI
- en: Powerful and easily scalable to millions of users
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能强大，易于扩展到数百万用户
- en: Provides deep insights into model usage
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供深入的模型使用洞察
- en: Ability to version models
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够对模型进行版本控制
- en: Cons of Using Cloud ML Engine
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Cloud ML Engine的缺点
- en: High latency, offers only CPUs for inference (as of August 2019)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高延迟，仅提供CPU进行推断（截至2019年8月）
- en: Unsuitable for scenarios involving legal and data privacy issues where the data
    must not leave the network
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适用于涉及法律和数据隐私问题的场景，其中数据不能离开网络
- en: Imposes restrictions on architecture design of complex applications
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对复杂应用程序的架构设计施加限制
- en: Building a Classification API
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建分类API
- en: 'The following step-by-step guide shows how to go about uploading and hosting
    our a Dog/Cat classifier model on Google Cloud ML Engine:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下逐步指南展示了如何在Google Cloud ML Engine上上传和托管我们的Dog/Cat分类器模型：
- en: Create a model on the Google Cloud ML Engine dashboard at [*https://console.cloud.google.com/mlengine/models*](https://console.cloud.google.com/mlengine/models).
    Because this is the first time we’re using the dashboard, we need to click ENABLE
    API, as depicted in [Figure 9-3](part0011.html#listing_page_for_machine_learning_models).
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[*https://console.cloud.google.com/mlengine/models*](https://console.cloud.google.com/mlengine/models)上在Google
    Cloud ML Engine仪表板上创建一个模型。因为这是我们第一次使用仪表板，我们需要单击“启用API”，如[图9-3](part0011.html#listing_page_for_machine_learning_models)所示。
- en: '![Listing page for machine learning models on the Google Cloud ML Engine dashboard](../images/00005.jpeg)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Google Cloud ML Engine仪表板上机器学习模型的列表页面](../images/00005.jpeg)'
- en: Figure 9-3\. Listing page for machine learning models on the Google Cloud ML
    Engine dashboard
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. Google Cloud ML Engine仪表板上机器学习模型的列表页面
- en: Give the model a name and a description ([Figure 9-4](part0011.html#model_creation_page_on_google_cloud_ml_e)).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为模型命名并添加描述（[图9-4](part0011.html#model_creation_page_on_google_cloud_ml_e)）。
- en: '![Model creation page on Google Cloud ML Engine](../images/00047.jpeg)'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Google Cloud ML Engine上的模型创建页面](../images/00047.jpeg)'
- en: Figure 9-4\. Model creation page on Google Cloud ML Engine
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4\. Google Cloud ML Engine上的模型创建页面
- en: After the model is created, we can access the model on the listing page ([Figure 9-5](part0011.html#model_listings_page_on_google_cloud_ml_e)).
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型创建完成后，我们可以在列表页面上访问模型（[图9-5](part0011.html#model_listings_page_on_google_cloud_ml_e)）。
- en: '![Model listings page on Google Cloud ML Engine](../images/00295.jpeg)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Google Cloud ML Engine上的模型列表页面](../images/00295.jpeg)'
- en: Figure 9-5\. Model listings page on Google Cloud ML Engine
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5\. Google Cloud ML Engine上的模型列表页面
- en: Click the model to go to the model details page ([Figure 9-6](part0011.html#details_page_of_the_just-created_dogcat))
    and add a new version.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击模型以转到模型详细信息页面（[图9-6](part0011.html#details_page_of_the_just-created_dogcat)）并添加新版本。
- en: '![Details page of the just-created DogCat classifier](../images/00298.jpeg)'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![刚创建的DogCat分类器的详细页面](../images/00298.jpeg)'
- en: Figure 9-6\. Details page of the just-created Dog/Cat classifier
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6\. 刚创建的Dog/Cat分类器的详细页面
- en: Fill out the necessary information to create the new version. The last field
    at the bottom requires you to upload the model to Google Cloud Storage before
    we can use it. Click the Browse button to create a new bucket for storing the
    model ([Figure 9-7](part0011.html#creating_a_new_version_for_a_machine_lea)).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写必要信息以创建新版本。底部的最后一个字段要求您在使用之前将模型上传到Google Cloud Storage。单击“浏览”按钮以创建用于存储模型的新存储桶（[图9-7](part0011.html#creating_a_new_version_for_a_machine_lea)）。
- en: '![Creating a new version for a machine learning model](../images/00255.jpeg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![为机器学习模型创建新版本](../images/00255.jpeg)'
- en: Figure 9-7\. Creating a new version for a machine learning model
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7\. 为机器学习模型创建一个新版本
- en: Create a new bucket with a unique name, a storage class, and region. After you
    create this bucket, go to [*https://console.cloud.google.com/storage/browser*](https://console.cloud.google.com/storage/browser)
    (in a separate tab while keeping the current one open) to find this newly created
    bucket and upload the model there ([Figure 9-8](part0011.html#creating_a_new_google_cloud_storage_buck)).
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有唯一名称、存储类和区域的新存储桶。创建此存储桶后，转到[*https://console.cloud.google.com/storage/browser*](https://console.cloud.google.com/storage/browser)（在保持当前标签页打开的同时在新标签页中打开）找到这个新创建的存储桶并将模型上传到那里（[图9-8](part0011.html#creating_a_new_google_cloud_storage_buck)）。
- en: '![Creating a new Google Cloud Storage bucket within the ML model version creation
    page](../images/00216.jpeg)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![在ML模型版本创建页面内创建一个新的Google Cloud Storage存储桶](../images/00216.jpeg)'
- en: Figure 9-8\. Creating a new Google Cloud Storage bucket within the ML model
    version creation page
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8\. 在ML模型版本创建页面内创建一个新的Google Cloud Storage存储桶
- en: Our Dog/Cat classifier model is an *.h5* file. However, Google Cloud expects
    a SavedModel file. You can find the script to convert the *.h5* file to SavedModel
    on the book’s GitHub repository (see [http://PracticalDeepLearning.ai](http://PracticalDeepLearning.ai))
    at *code/chapter-9/scripts/h5_to_tf.ipynb*. Simply load the model and execute
    the rest of the notebook.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的Dog/Cat分类器模型是一个*.h5*文件。但是，Google Cloud期望一个SavedModel文件。您可以在本书的GitHub存储库（请参阅[http://PracticalDeepLearning.ai](http://PracticalDeepLearning.ai)）的*code/chapter-9/scripts/h5_to_tf.ipynb*中找到将*.h5*文件转换为SavedModel的脚本。只需加载模型并执行笔记本的其余部分。
- en: In the Google Cloud Storage browser, upload the newly converted model ([Figure 9-9](part0011.html#google_cloud_storage_browser_page_showin))
    to the bucket you created in step 6.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud Storage浏览器中，将新转换的模型（[图9-9](part0011.html#google_cloud_storage_browser_page_showin)）上传到您在第6步中创建的存储桶中。
- en: '![Google Cloud Storage Browser page showing the uploaded DogCat classifier
    model in TensorFlow format](../images/00149.jpeg)'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Google Cloud Storage浏览器页面显示以TensorFlow格式上传的DogCat分类器模型](../images/00149.jpeg)'
- en: Figure 9-9\. Google Cloud Storage Browser page showing the uploaded Dog/Cat
    classifier model in TensorFlow format
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-9\. Google Cloud Storage浏览器页面显示以TensorFlow格式上传的Dog/Cat分类器模型
- en: Specify the URI on the model version creation page for the model that you just
    uploaded ([Figure 9-10](part0011.html#add_the_uri_for_the_model_you_uploaded_t)).
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型版本创建页面上指定您刚上传的模型的URI（[图9-10](part0011.html#add_the_uri_for_the_model_you_uploaded_t)）。
- en: '![Add the URI for the model you uploaded to Google Cloud Storage](../images/00139.jpeg)'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![将上传到Google Cloud Storage的模型的URI添加到其中](../images/00139.jpeg)'
- en: Figure 9-10\. Add the URI for the model you uploaded to Google Cloud Storage
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-10\. 将上传到Google Cloud Storage的模型的URI添加到其中
- en: Click the Save button and wait for the model version to be created. As soon
    as the model version is created, you can begin making predictions against it.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击保存按钮，等待模型版本创建。一旦模型版本创建完成，您就可以开始针对它进行预测。
- en: If it’s not already present on your machine, you can download and install the
    Google Cloud SDK from the installation website at [*https://cloud.google.com/sdk/install*](https://cloud.google.com/sdk/install).
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的机器上还没有安装，您可以从安装网站[*https://cloud.google.com/sdk/install*](https://cloud.google.com/sdk/install)下载并安装Google
    Cloud SDK。
- en: 'You can use the Cloud ML Engine REST API to make your requests. However, for
    brevity, use the command-line tools in the Cloud SDK. You first need to convert
    your image into a *request.json* file using the *image-to-json.py* script located
    at *code/chapter-9*:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用Cloud ML Engine REST API进行请求。但是，为了简洁起见，请使用Cloud SDK中的命令行工具。您首先需要使用位于*code/chapter-9*的*image-to-json.py*脚本将图像转换为*request.json*文件：
- en: '[PRE7]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, use the *request.json* file created in the previous step to execute a
    request against our model:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用在上一步中创建的*request.json*文件来执行针对我们模型的请求：
- en: '[PRE8]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see from the output, we get similar results as with our Flask server;
    that is, a prediction of “dog” with 85% confidence.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看到，我们得到了与我们的Flask服务器相似的结果；即“狗”的预测，置信度为85%。
- en: Note
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'If this is your first time using **`gcloud`**, you need to run the following
    command to tie the command-line tool to your Google account:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是您第一次使用**`gcloud`**，您需要运行以下命令将命令行工具与您的Google账户绑定：
- en: '[PRE9]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, select the project using the following command:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令选择项目：
- en: '[PRE10]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Piece of cake, wasn’t it? In our example, we used the Google Cloud SDK to request
    a prediction for the sake of brevity. In a production scenario, you would want
    to execute the same prediction request using Google’s API endpoints, instead;
    either by generating HTTP requests or by using their client libraries. We can
    follow the documentation on Google Cloud Docs for production scenarios.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 小菜一碟，不是吗？在我们的示例中，为了简洁起见，我们使用Google Cloud SDK请求预测。在生产场景中，您可能希望使用Google的API端点执行相同的预测请求；可以通过生成HTTP请求或使用他们的客户端库来执行。我们可以在Google
    Cloud文档中查看生产场景的文档。
- en: At this point, the model is ready to be served to any user anywhere in the world
    using applications on the browser, mobile and edge devices, desktop, as well as
    cloud environments. Using a hosted stack is a pretty viable option for individuals
    and organizations who want the flexibility and reliability that the cloud provides
    while having to do minimal setup and maintenance work for the infrastructure.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，该模型已准备好通过浏览器、移动设备和边缘设备、桌面以及云环境上的应用程序提供给全球任何用户。对于希望在云提供的灵活性和可靠性的个人和组织来说，使用托管堆栈是一个相当可行的选择，同时对基础设施的设置和维护工作要求最小。
- en: In contrast, there are situations for which a hosted solution might not be the
    best approach. Reasons could include pricing models, data privacy issues, legal
    questions, technical issues, trust concerns, or contractual obligations. In such
    cases, a solution that is hosted and managed locally (or, “on-premises”) would
    be preferable.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，有些情况下托管解决方案可能不是最佳选择。原因可能包括定价模型、数据隐私问题、法律问题、技术问题、信任问题或合同义务。在这种情况下，一个在本地托管和管理的解决方案将更可取。
- en: Tip
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For processing a large number of images at one time, you can modify *image-to-json.py*
    to create a *request.json* file that contains an array of multiple inputs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了一次处理大量图像，您可以修改*image-to-json.py*以创建包含多个输入数组的*request.json*文件。
- en: TensorFlow Serving
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Serving
- en: TensorFlow Serving is an open source library in the TensorFlow ecosystem for
    serving machine learning models fast. Unlike Flask, it’s built for performance,
    with low overhead, and designed for use in production. TensorFlow Serving is widely
    used by large companies to serve their models for prediction services. It is one
    of the integral components of TensorFlow Extended (TFX)—an end-to-end deep learning
    pipeline in the TensorFlow ecosystem.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving是TensorFlow生态系统中用于快速提供机器学习模型的开源库。与Flask不同，它专为性能而构建，具有低开销，并设计用于生产环境中使用。大公司广泛使用TensorFlow
    Serving来为他们的预测服务提供模型。它是TensorFlow Extended (TFX)的一个重要组件，TFX是TensorFlow生态系统中的端到端深度学习管道之一。
- en: 'As we saw when we looked at the desired qualities of a production system, TensorFlow
    serving offers low latency, failure handling, high throughput, and model versioning.
    Another benefit includes the ability to serve multiple models at the same time
    on the same service. It implements several techniques to speed up serving:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在查看生产系统所需的特性时看到的，TensorFlow Serving提供低延迟、故障处理、高吞吐量和模型版本控制。另一个好处是能够在同一服务上同时提供多个模型。它实现了几种技术来加速服务：
- en: During server startup, it starts a burst of threads for fast model loading.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在服务器启动期间，它启动了一批线程以快速加载模型。
- en: It uses separate thread pools for loading models and for serving inferences
    while giving higher priority to the threads in the inference pool. This is crucial
    to lowering request latency.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为加载模型和提供推理使用单独的线程池，同时给予推理池中的线程更高的优先级。这对降低请求延迟至关重要。
- en: It builds minibatches of incoming asynchronous requests for short periods of
    time. As we have seen, with the power of batching data on GPUs during training,
    it aims to bring similar efficiencies on asynchronous requests. As an example,
    waiting 500 ms to group together several requests for inference. While at worst
    case, this adds a 500 ms penalty for the first request in the batch, it reduces
    the average latency across requests and maximizes hardware utilization.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在短时间内构建传入异步请求的小批量。正如我们所见，在训练期间在GPU上批处理数据的强大功能，它旨在在异步请求中带来类似的效率。例如，等待500毫秒来将几个推理请求分组。在最坏的情况下，这会为批处理中的第一个请求增加500毫秒的惩罚，但它会减少请求的平均延迟，并最大化硬件利用率。
- en: Note
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: TensorFlow serving gives you full control over the model rollout procedure.
    You can serve different models or different versions of the same kind of model
    in the same process. You just need to make sure that you know the name and location
    of the version that you want to remove or put into production.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving让你完全控制模型发布过程。你可以在同一个进程中为不同的模型或同一种模型的不同版本提供服务。你只需要确保你知道要删除或投入生产的版本的名称和位置。
- en: Installation
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: 'There are a few different ways of setting up TensorFlow Serving:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的设置TensorFlow Serving的方法：
- en: Building from source
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从源代码构建
- en: Downloading and installing using APT
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用APT下载和安装
- en: Deploying Docker images
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署Docker镜像
- en: If you’re feeling adventurous, building from source might be the danger you
    seek. But if you just want to get up and running quickly, we recommend using Docker
    because it requires minimal steps to get the system up and running. What is Docker,
    you might ask? Docker provides virtualization of a Linux environment for applications
    that run within it. It provides isolation of resources that essentially operate
    as a clean slate for setting up an environment in which an application can run.
    Typically, an application and all of its dependencies are packaged into a single
    Docker container that can then be deployed repeatedly as necessary. Because the
    application is set up in a clean environment, it reduces the likelihood of configuration
    and deployment errors. This makes Docker very well suited for running applications
    in production.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感到冒险，从源代码构建可能是你寻求的危险。但如果你只是想快速启动并运行，我们建议使用Docker，因为它需要最少的步骤来启动系统。你可能会问，什么是Docker？Docker为在其中运行的应用程序提供了Linux环境的虚拟化。它提供了资源隔离，基本上作为一个干净的板凳来设置一个应用程序可以运行的环境。通常，一个应用程序及其所有依赖项被打包到一个单独的Docker容器中，然后根据需要重复部署。因为应用程序是在一个干净的环境中设置的，它减少了配置和部署错误的可能性。这使得Docker非常适合在生产中运行应用程序。
- en: The biggest benefit that Docker provides for us is alleviating “dependency hell”
    because all the necessary dependencies are packaged within the container. One
    additional advantage of using Docker is that the process of setting up your application
    remains more or less the same across different platforms, whether you use Windows,
    Linux, or Mac.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Docker为我们提供的最大好处是减轻“依赖地狱”，因为所有必要的依赖项都打包在容器中。使用Docker的另一个优势是，在不同平台上设置应用程序的过程基本上保持一致，无论你使用Windows、Linux还是Mac。
- en: 'The Docker installation instructions, depending on the target platform, are
    available on the Docker home page. This should not take more than a few minutes
    because the setup is fairly straightforward. After you’ve installed Docker, you
    can run the following command to set up TensorFlow Serving for CPU:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Docker安装说明，取决于目标平台，可在Docker主页上找到。这应该不会花费太多时间，因为设置非常简单。安装完Docker后，你可以运行以下命令来为CPU设置TensorFlow
    Serving：
- en: '[PRE11]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For GPU-enabled machines, run the following command, instead:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于启用GPU的机器，运行以下命令：
- en: '[PRE12]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In either case, if everything went smoothly, you should have a REST API running
    on your local port 8501 serving our Dog/Cat classifier.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种情况，如果一切顺利，你应该在本地端口8501上运行一个REST API，为我们的狗/猫分类器提供服务。
- en: Note
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In any inference request, the end-to-end latency is a summation of time taken
    by multiple steps along the process. This includes round-trip network time, request
    time to serialize/deserialize the request and response objects, and, of course,
    time to perform the actual inference. One more component that adds overhead is
    the serving framework; that is, TensorFlow Serving. Google claims that the overhead
    contributed by TensorFlow Serving is minimal. In its experiments, it observed
    that TensorFlow Serving alone was able to handle approximately 100,000 QPS per
    core on a 16 vCPU Intel Xeon E5 2.6 GHz machine. Because it is measuring the overhead,
    this excludes the remote procedure call (RPC) time and the TensorFlow inference
    processing time.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何推理请求中，端到端延迟是沿着过程中多个步骤所花费的时间的总和。这包括往返网络时间，序列化/反序列化请求和响应对象的时间，当然还有执行实际推理的时间。增加开销的另一个组件是服务框架；也就是TensorFlow
    Serving。谷歌声称TensorFlow Serving贡献的开销很小。在实验中，它观察到TensorFlow Serving单独能够在16核Intel
    Xeon E5 2.6 GHz机器上每个核心处理大约100,000个QPS。因为它正在测量开销，这不包括远程过程调用（RPC）时间和TensorFlow推理处理时间。
- en: Even though TensorFlow Serving is a great choice for serving inferences from
    a single machine, it does not have built-in functionality for horizontal scaling.
    Instead, it is built to be used in conjunction with other systems that can supercharge
    TensorFlow Serving with dynamic scaling. We explore one such solution in the following
    section.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管TensorFlow Serving是从单台机器提供推理的绝佳选择，但它没有用于水平扩展的内置功能。相反，它被设计用于与其他系统一起使用，这些系统可以通过动态扩展来增强TensorFlow
    Serving。我们将在下一节中探讨一个这样的解决方案。
- en: KubeFlow
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KubeFlow
- en: Throughout this book, we have explored the various steps of an end-to-end deep
    learning pipeline, from data ingestion, analysis, distributed training (including
    hyperparameter tuning) at scale, tracking experiments, deployment, and eventually
    to serving prediction requests at scale. Each of these steps is complex in its
    own right, with its set of tools, ecosystems, and areas of expertise. People dedicate
    their lifetimes developing expertise in just one of these fields. It’s not exactly
    a walk in the park. The combinatorial explosion of the know-how required when
    factoring for the necessary backend engineering, hardware engineering, infrastructure
    engineering, dependency management, DevOps, fault tolerance, and other engineering
    challenges can result in a very expensive hiring process for most organizations.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous section, Docker saves us the hassle of dependency
    management by making portable containers available. It helps us make TensorFlow
    Serving available across platforms easily without having to build it from source
    code or install dependencies manually. Great! But it still doesn’t have an answer
    to many of the other challenges. How are we going to scale up containers to match
    rises in demand? How would we efficiently distribute traffic across containers?
    How do we ensure that the containers are visible to one another and can communicate?
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: These are questions answered by *Kubernetes*. Kubernetes is an orchestration
    framework for automatically deploying, scaling, and managing containers (like
    Docker). Because it takes advantage of the portability offered by Docker, we can
    use Kubernetes to deploy to developer laptops as well as thousand-machine clusters
    in an almost identical manner. This helps maintain consistency across different
    environments, with the added benefit of scalability in an accessible manner. It
    is worth noting that Kubernetes is not a dedicated solution for machine learning
    (neither is Docker); rather, it is a general-purpose solution to many of the problems
    faced in software development, which we use in the context of deep learning.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: But let’s not get ahead of ourselves just yet. After all, if Kubernetes were
    the be-all and end-all solution, it would have appeared in the chapter title!
    A machine learning practitioner using Kubernetes still needs to assemble all of
    the appropriate sets of containers (for training, deployment, monitoring, API
    management, etc.) that then need to be orchestrated together to make a fully functioning
    end-to-end pipeline. Unfortunately, many data scientists are trying to do exactly
    this in their own silos, reinventing the wheel building ad hoc machine learning-specific
    pipelines. Couldn’t we save everyone the trouble and make one Kubernetes-based
    solution for machine learning scenarios?
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Enter *KubeFlow*, which promises to automate a large chunk of these engineering
    challenges and hide the complexity of running a distributed, scalable, end-to-end
    deep learning system behind a web GUI-based tool and a powerful command-line tool.
    This is more than just an inference service. Think of it as a large ecosystem
    of tools that can interoperate seamlessly and, more importantly, scale up with
    demand. KubeFlow is built for the cloud. Though not just one cloud—it’s built
    to be compatible with all major cloud providers. This has significant implications
    on cost. Because we are not tied to a specific cloud provider, we have the freedom
    to move all of our operations at a moment’s notice if a competing cloud provider
    drops its prices. After all, competition benefits consumers.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: KubeFlow supports a variety of hardware infrastructure, from developer laptops
    and on-premises datacenters, all the way to public cloud services. And because
    it’s built on top of Docker and Kubernetes, we can rest assured that the environments
    will be identical whether deployed on a developer laptop or a large cluster in
    a datacenter. Every single way in which the developer setup is different from
    the production environment could result in an outage, so it’s really valuable
    to have this consistency across environments.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 9-4](part0011.html#tools_available_on_kubeflow) shows a brief list of
    readily available tools within the KubeFlow ecosystem.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-4\. Tools available on KubeFlow
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tool** | **Functionality** |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| Jupyter Hub | Notebook environment |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| TFJob | Training TensorFlow models |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow Serving | Serving TensorFlow models |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| Seldon | Serving models |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| NVIDIA TensorRT | Serving models |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| Intel OpenVINO | Serving models |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| KFServing | Abstraction for serving Tensorflow, XGBoost, scikit-learn, PyTorch,
    and ONNX models |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| Katib | Hyperparameter tuning and NAS |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| Kubebench | Running benchmarking jobs |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | Training PyTorch models |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| Istio | API services, authentication, A/B testing, rollouts, metrics |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| Locust | Load testing |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| Pipelines | Managing experiments, jobs, and runs, scheduling machine learning
    workflows |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: As the joke goes in the community, with so many technologies prepackaged, KubeFlow
    finally makes our résumés buzzword- (and recruiter-) compliant.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many people assume that KubeFlow is a combination of Kubernetes and TensorFlow,
    which, as you have seen, is not the case. It is that and much more.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two important parts to KubeFlow that make it unique: pipelines and
    fairing.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pipelines give us the ability to compose steps across the machine learning to
    schedule complex workflows. [Figure 9-11](part0011.html#an_end-to-end_pipeline_illustrated_in_ku)
    shows us an example of a pipeline. Having visibility into the pipeline through
    a GUI tool helps stakeholders understand it (beyond just the engineers who built
    it).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![An end-to-end pipeline illustrated in KubeFlow](../images/00094.jpeg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: Figure 9-11\. An end-to-end pipeline illustrated in KubeFlow
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fairing
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fairing allows us to manage the entire build, train, and deploy lifecycle directly
    through Jupyter Notebooks. [Figure 9-12](part0011.html#creating_a_new_jupyter_notebook_server_o)
    shows how to start a new notebook server, where we can host all of our Jupyter
    Notebooks, run training on them, and deploy our models to Google Cloud using the
    following few lines of code, all the while being in the comfort of a very familiar
    Jupyter environment:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Creating a new Jupyter Notebook server on KubeFlow](../images/00197.jpeg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: Figure 9-12\. Creating a new Jupyter Notebook server on KubeFlow
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Installation
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating a new KubeFlow deployment is a fairly straightforward process that
    is well documented on the KubeFlow website. You can set up KubeFlow using the
    browser for GCP. Alternatively, you can use the KubeFlow command-line tool to
    set up a deployment on GCP, AWS, and Microsoft Azure. [Figure 9-13](part0011.html#creating_a_kubeflow_deployment_on_gcp_us)
    shows a GCP deployment using the web browser.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a KubeFlow deployment on GCP using the browser](../images/00014.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: Figure 9-13\. Creating a KubeFlow deployment on GCP using the browser
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As of this writing, KubeFlow is in active development and shows no signs of
    stopping. Companies such as Red Hat, Cisco, Dell, Uber, and Alibaba are some of
    the active contributors on top of cloud giants like Microsoft, Google, and IBM.
    Ease and accessibility for solving tough challenges attract more people to any
    platform, and KubeFlow is doing exactly that.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Price Versus Performance Considerations
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 6](part0008.html#7K4G3-13fa565533764549a6f0ab7f11eed62b), we looked
    at how to improve our model performance for inference (whether on smartphones
    or on a server). Now let’s look from another side: the hardware performance and
    the price involved.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Often while building a production system, we want the flexibility of choosing
    suitable hardware to strike the proper balance between performance, scale, and
    price for our scenario. Consider building an app that requires cloud-based inference.
    We can go set up our own stack manually (using Flask or TensorFlow Serving or
    KubeFlow), or we could use a managed Inference-as-a-Service stack (like the Google
    Cloud ML Engine). Assuming that our service went viral, let’s see how much it
    would cost.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Cost Analysis of Inference-as-a-Service
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For Google Cloud ML Engine, as of August 2019 in North America, it costs a rather
    inexpensive $0.0401 per hour of combined inference time on a single-core CPU machine.
    There’s also an option for a quad-core CPU machine, but really, a single core
    should suffice for most applications. Running several queries to the server with
    a small image of 12 KB took roughly 3.5 seconds on average, as illustrated in
    [Figure 9-14](part0011.html#google_cloud_ml_engine_showing_incoming). This does
    sound slow, and is partly because of doing inference on a moderate-speed machine,
    and, more important on a CPU server. It’s worth mentioning that this benchmark
    is on a warmed-up machine that has recently received an API request and hence
    has the model preloaded. For comparison, the first query takes between 30 and
    60 seconds. This shows the importance of keeping the service running constantly
    or sending frequent warm-up queries. This happens because the Google Cloud ML
    engine takes down a model if it notices a prolonged period of nonuse.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![Google Cloud ML Engine showing incoming queries and latency of serving the
    calls, with the end-to-end latency at user’s end of about 3.5 seconds](../images/00304.jpeg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: Figure 9-14\. Google Cloud ML Engine showing incoming queries and latency of
    serving the calls, with end-to-end latency at user’s end of about 3.5 seconds
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If a request came in at every second for an entire month, there would be a total
    of 60 x 60 x 24 x 30 = 2,592,000 calls per month. Assuming that each inference
    takes 3.5 seconds, a single node would be insufficient. The cloud service would
    quickly realize that and, in response to the increased traffic, bring up three
    additional machines to handle the traffic. In total, with four machines running
    for a month at $0.0401 per hour per node, it would cost a grand total of $115.48\.
    To put this into perspective, for two million calls, that’s about the cost of
    a cup of Starbucks coffee a day for an entire month. And let’s not forget this
    is without involving much of the DevOps team members, whose time is expensive.
    If we took the hypothetical scenario of a Yelp-like service for which users, on
    average, upload photos of food at 64 QPS, running inferences on them using a classification
    model would cost only $7,390.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Cost Analysis of Building Your Own Stack
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Less spending and high scalability, now that’s a winning combination. But the
    one downside is the total roundtrip latency of each request. Taking matters into
    our own hands, getting a VM with a modest GPU on the cloud, and setting up our
    scaling pipeline (using KubeFlow or the native cloud load-balancing features with
    TensorFlow Serving), we would be able to respond either in milliseconds or batch
    a few incoming queries together (say every 500 ms) to serve them. As an example,
    looking at the inventory of VMs on Azure, for $2.07 per hour, we can rent out
    an ND6 machine that features an NVIDIA P40 GPU and 112 GiB RAM. By batching incoming
    requests every 500 ms to 1 second, this machine can serve 64 requests per second
    at a total cost of $1,490 per month, and faster than the Google Cloud ML Engine.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the cost savings and performance benefits of orchestrating our own
    cloud machine environment kicks in big time when working on large QPS scenarios,
    as demonstrated in [Figure 9-15](part0011.html#cost_comparison_of_infrastructure_as_a_s).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![Cost comparison of Infrastructure as a service (Google Cloud ML Engine) versus
    Building Your Own Stack over Virtual Machines (Azure VM) (Costs as of August 2019)](../images/00244.jpeg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: Figure 9-15\. Cost comparison of infrastructure as a service (Google Cloud ML
    Engine) versus building your own stack over virtual machines (Azure VM) (costs
    as of August 2019)
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A common question that arises while benchmarking is what is my system’s limit?
    [JMeter](https://jmeter.apache.org) can help answer this. JMeter is a load-testing
    tool that lets you perform stress testing of your system with an easy-to-use graphical
    interface. It lets you create reusable configurations to simulate a variety of
    usage scenarios.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we answered the question most engineers and developers ask:
    how do we serve model prediction requests at scale for applications in the real
    world? We explored four different methods of serving an image recognition model:
    using Flask, Google Cloud ML, TensorFlow Serving, and KubeFlow. Depending on the
    scale, latency requirements, and our skill level, some solutions might be more
    attractive than others. Finally, we developed an intuition into the cost effectiveness
    of different stacks. Now that we can show our fabulous classifier model off to
    the world, all that’s left is to make our work go viral!'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
