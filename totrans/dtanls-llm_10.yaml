- en: 8 GPT alternatives
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 个 GPT 替代品
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Claude, from Anthropic
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Claude，来自 Anthropic
- en: Command R+, from Cohere
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令 R+，来自 Cohere
- en: Bard, from Google
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bard，来自 Google
- en: Hugging Face’s models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face 的模型
- en: 'Time to meet some of GPT’s “friends”! So far, we have been focusing on GPT
    and other OpenAI models. But OpenAI is not the only game in town. Quite the contrary:
    we are currently witnessing a “Cambrian explosion” of language models, with new
    models popping up every week. Before using language models in production, you
    want to make sure you’re using the best model for your task. In this chapter,
    we’ll look at many of the OpenAI alternatives out there and discuss the pros and
    cons of different models as well as how to use them.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候见一见 GPT 的“朋友们”了！到目前为止，我们一直专注于 GPT 和其他 OpenAI 模型。但 OpenAI 并非市场上唯一的游戏。恰恰相反：我们目前正在见证语言模型的“寒武纪大爆发”，每周都有新的模型涌现。在使用语言模型进行生产之前，您想要确保使用最适合您任务的模型。在本章中，我们将探讨许多
    OpenAI 的替代品，并讨论不同模型的优缺点以及如何使用它们。
- en: 'Almost all language models nowadays are based on a similar architecture (the
    Transformer architecture). However, models from different providers may differ
    in the way they are trained, the way they represent text, or the way in which
    they are offered and priced. All those factors can make a difference in terms
    of processing fees and output quality for your specific task. Models like GPT-4o
    are powerful and solve almost any task. But this generality comes at a cost: if
    a small, specialized model is available, trained for just the task you’re interested
    in, using such a model may very well be the optimal choice.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有现代语言模型都基于类似的架构（Transformer 架构）。然而，不同提供商的模型可能在训练方式、文本表示方式或提供和定价方式上有所不同。所有这些因素都可能影响您特定任务的处理费用和输出质量。像
    GPT-4o 这样的模型功能强大，几乎可以解决任何任务。但这种通用性是有代价的：如果有一个小型、专门针对您感兴趣的任务训练的模型，使用这样的模型可能非常合适。
- en: 'You will notice throughout the following sections that many of the models we
    discuss can be accessed via interfaces that are similar to OpenAI’s interface.
    That’s good news for you: no need to get into a novel and complex framework each
    time you want to try a different model! And with that, let’s start our exploration
    of GPT alternatives.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在接下来的章节中注意到，我们讨论的许多模型可以通过与 OpenAI 接口类似的界面访问。这对您来说是个好消息：每次想要尝试不同的模型时，无需进入一个全新的复杂框架！有了这个，我们就开始探索
    GPT 的替代品。
- en: Why isn’t my favorite model listed?
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么我的首选模型没有列出？
- en: If your favorite model or model provider is not listed in this chapter, don’t
    panic! With the growing number of providers and models, it has become impossible
    to give a full overview of available models. If a model is not included here,
    it does not mean it can’t be the best alternative for your task. The interfaces
    of different providers tend to be similar, so you should still be able to use
    what you have learned so far to employ other models without much trouble. Also,
    note that we list model providers in alphabetical order in this chapter. Don’t
    infer any priority from that (we’re not discussing the best providers first).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的首选模型或模型提供商在本章中没有列出，请不要慌张！随着提供商和模型数量的增加，全面概述所有可用模型已经变得不可能。如果一个模型没有包括在这里，并不意味着它不能成为您任务的最佳替代品。不同提供商的界面通常相似，因此您仍然可以使用到目前为止所学到的知识来轻松使用其他模型。此外，请注意，我们在本章中按字母顺序列出模型提供商。不要从那中推断出任何优先级（我们不是先讨论最好的提供商）。
- en: 8.1 Anthropic
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 Anthropic
- en: 'Many of the stories by Isaac Asimov, one of the most prolific science-fiction
    authors of all time, center on the “three laws of robotics” and their interpretation:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 许多艾萨克·阿西莫夫（Isaac Asimov）的作品，他是史上最富产的科幻作家之一，其故事围绕“机器人三大定律”及其解释展开：
- en: A robot may not injure a human being or, through inaction, allow a human being
    to come to harm.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器人不得伤害人类，或者通过不作为，让人类受到伤害。
- en: A robot must obey orders given to it by human beings except where such orders
    would conflict with the First Law.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器人必须服从人类给予的命令，除非这些命令与第一定律相冲突。
- en: A robot must protect its own existence as long as such protection does not conflict
    with the First or Second Law.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器人必须保护自己的存在，只要这种保护不与第一或第二定律相冲突。
- en: The goal here is to have a concise set of guidelines that makes sure robots
    are helpful and harmless. Whether the previously mentioned laws provided any inspiration,
    this idea connects to the language models produced by Anthropic, yet another provider
    of large-scale language models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的目标是制定一套简洁的指导方针，确保机器人既有用又无害。无论之前提到的法律是否提供了灵感，这一想法都与Anthropic生产的语言模型相联系，Anthropic是另一家大规模语言模型的提供商。
- en: Anthropic, founded in 2021 (by several former OpenAI members), has repeatedly
    touted the idea of “Constitutional AI” [1] as one of the distinguishing factors,
    compared to other models. In a nutshell, this means that when training models
    to provide users with accurate and inoffensive answers, we rely on a small set
    of rules—a “constitution,” so to speak—to judge the quality of answers. Instead
    of relying on human testers to label answers generated by the model during training,
    we employ a second AI, charged with evaluating the answers of the former according
    to the constitution.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic，成立于2021年（由几位前OpenAI成员创立），反复强调“宪法AI” [1] 这一独特理念，与其他模型相比。简而言之，这意味着在训练模型以向用户提供准确且无冒犯性的答案时，我们依赖一组小规则——可以说是一份“宪法”——来评判答案的质量。我们不是依赖人类测试员在训练过程中对模型生成的答案进行标记，而是采用第二个AI，负责根据宪法评估前者的答案。
- en: At the time of writing, Claude 3.5 (a reference to the amazing Claude Shannon)
    is the latest model released by Anthropic. In this section, we will try Claude
    via (you guessed it) a web interface and a Python library.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Claude 3.5（指代令人惊叹的Claude Shannon）是Anthropic发布的最新模型。在本节中，我们将通过（你猜对了）一个网页界面和Python库来尝试使用Claude。
- en: 8.1.1 Chatting with Claude
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 与Claude聊天
- en: We will have a quick chat with Claude (currently in version 3.5) to get a sense
    of its capabilities. First, go to the Anthropic website at [www.anthropic.com](http://www.anthropic.com),
    and click the Talk to Claude button.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将与Claude（目前版本为3.5）进行简短聊天，以了解其功能。首先，访问Anthropic网站[www.anthropic.com](http://www.anthropic.com)，然后点击“与Claude聊天”按钮。
- en: Unless you have created an Anthropic account before, you will be asked to provide
    an email address and a phone number. After verifying your data, you should see
    Claude’s chat interface, shown in figure [8.1](#fig__claude).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你之前已经创建了Anthropic账户，否则你将被要求提供电子邮件地址和电话号码。在验证你的数据后，你应该能看到Claude的聊天界面，如图[8.1](#fig__claude)所示。
- en: '![figure](../Images/CH08_F01_Trummer.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F01_Trummer.png)'
- en: Figure 8.1 Web interface for Anthropic’s chatbot Claude
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1 Anthropic聊天机器人Claude的网页界面
- en: 'The interface is pretty intuitive: simply enter text in the corresponding field,
    and click the button on the right to start chatting! Begin with a friendly greeting,
    chat about the weather, or try using Claude to solve some of the tasks from chapter
    2 (e.g., classifying reviews by sentiment or translating questions to SQL queries).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 界面非常直观：只需在相应的字段中输入文本，然后点击右侧的按钮开始聊天！可以从友好的问候语开始，聊聊天气，或者尝试使用Claude解决第2章的一些任务（例如，根据情感对评论进行分类或把问题翻译成SQL查询）。
- en: 8.1.2 Python library
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 Python库
- en: 'Assuming you have created an account with Anthropic, you can create keys at
    the following URL: [https://console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys).
    Be sure to copy your access key after creating it (as you will not be able to
    access it again afterward)!'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经创建了Anthropic的账户，你可以在以下URL创建密钥：[https://console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys)。务必在创建后复制你的访问密钥（因为之后你将无法再次访问它）！
- en: 'After obtaining your access key, go to the terminal and run the following command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 获得访问密钥后，转到终端并运行以下命令：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will install the Anthropic Python library. If you are familiar with OpenAI’s
    Python library (and chances are that, after reading the previous chapters, you
    are), you should get used to the Anthropic library very quickly.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装Anthropic Python库。如果你熟悉OpenAI的Python库（并且在你阅读了前面的章节后，你很可能熟悉），你应该会很快习惯Anthropic库。
- en: For instance, the following listing shows Python code for answering questions
    using Claude. Of course, this code does not do anything that you cannot do via
    the web interface. The purpose is just to show how easy it is to use Claude via
    the Python interface.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下列表展示了使用Claude回答问题的Python代码。当然，这段代码并不能做通过网页界面做不到的事情。其目的只是展示通过Python接口使用Claude是多么简单。
- en: Listing 8.1 Answering questions with Anthropic’s Claude model
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.1 使用Anthropic的Claude模型回答问题
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Defines parameters'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义参数'
- en: '#2 Configures Anthropic'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 配置Anthropic'
- en: '#3 Uses Claude for completion'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用Claude进行完成'
- en: '#4 Prints the completion result'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 打印完成结果'
- en: As input parameters (**1**), we use the Anthropic access key and a question
    we would like answered. Similar to OpenAI’s libraries, we configure access using
    the access key (**2**). After that, we can construct prompts for completion by
    Claude (**3**).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输入参数（**1**），我们使用Anthropic访问密钥和我们想要回答的问题。类似于OpenAI的库，我们使用访问密钥（**2**）配置访问。之后，我们可以构建Claude的完成提示（**3**）。
- en: Using Anthropic’s `anthropic.messages` endpoint, we specify the ID of the model
    to use (`claude-3-5-sonnet-20241022` is the newest model by Anthropic at the time
    of writing) and the maximum number of tokens for completion (using the `max_tokens`
    parameter). Similar to OpenAI’s chat models, Claude is designed for chats between
    users and the model. Therefore, the input to Claude is a list of messages (containing
    only a single element in this specific scenario). We obtain the result of Claude’s
    prompt completion in the `content` field (which we print (**4**)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Anthropic的`anthropic.messages`端点，我们指定要使用的模型ID（`claude-3-5-sonnet-20241022`是撰写本文时的Anthropic最新模型）和完成的最大令牌数（使用`max_tokens`参数）。类似于OpenAI的聊天模型，Claude是为用户与模型之间的聊天而设计的。因此，Claude的输入是一个包含消息的列表（在这个特定场景中只包含一个元素）。我们在`content`字段中获取Claude提示完成的成果（我们打印**4**）。
- en: 'You can find this listing as the Anthropic item on the companion website. To
    execute it, open your terminal, and switch to the containing folder. Then, execute
    the following command:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在配套网站上找到这个列表作为Anthropic条目。要执行它，请打开您的终端，切换到包含文件夹。然后，执行以下命令：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Replace the three dots with your Anthropic access key. When executing the program,
    you should see an answer to your question, generated by Anthropic’s model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将三个点替换为您的Anthropic访问密钥。在执行程序时，您应该会看到由Anthropic模型生成的对您问题的回答。
- en: 8.2 Cohere
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 Cohere
- en: We briefly discussed hallucinations in chapter 2\. Essentially, a hallucination
    occurs when language models make stuff up because they don’t have access to data
    that is relevant to the task at hand. The Canadian startup Cohere puts a particular
    emphasis on avoiding such hallucinations using a method called *grounding*. Grounding
    the answer of a language model means linking it to real data, thereby reducing
    the chances of “creative output” not based in reality.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第2章中简要讨论了幻觉。本质上，当语言模型因为无法访问与当前任务相关的数据而编造内容时，就会发生幻觉。加拿大初创公司Cohere特别强调使用一种称为*grounding*的方法来避免这种幻觉。将语言模型的答案grounding意味着将其与真实数据联系起来，从而降低“基于现实之外的创造性输出”的可能性。
- en: Cohere supports a wide range of connectors, enabling its models to access external
    data. For instance, web search is supported, as well access to various databases.
    Internally, Cohere accesses those data sources and provides the language model
    with information tailored to the request. But even better, Cohere shows you all
    the data sources used to generate your answer. If you are the suspicious type
    (and when it comes to answers from language models, you generally should be),
    you can follow up on references and validate that they support the generated answer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Cohere支持广泛的连接器，使其模型能够访问外部数据。例如，支持网络搜索，以及访问各种数据库。内部，Cohere访问这些数据源，并为语言模型提供针对请求定制的信息。但更好的是，Cohere会向您展示用于生成您答案的所有数据源。如果您是那种怀疑型的人（在涉及语言模型的答案时，您通常应该是），您可以跟进参考文献并验证它们是否支持生成的答案。
- en: Let’s see how all that works in practice. Time to chat with Cohere’s Command
    R+ model!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这一切在实际中是如何工作的。现在是时候与Cohere的Command R+模型聊天了！
- en: 8.2.1 Chatting with Command R+
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 与Command R+聊天
- en: At the time of writing, Command R+ is one of Cohere’s latest models. As usual,
    you can try it via web interface and use it via Python when processing large amounts
    of data. First, we’ll try the web interface. For that, go to [https://cohere.com/](https://cohere.com/),
    and click Try Now. After signing up for an account, click Playground. You should
    see the web interface in figure [8.2](#fig__cohere_3).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Command R+是Cohere的最新模型之一。像往常一样，您可以通过Web界面尝试它，并在处理大量数据时通过Python使用它。首先，我们将尝试Web界面。为此，请访问[https://cohere.com/](https://cohere.com/)，并点击“立即尝试”。注册账户后，点击“游乐场”。您应该能在图8.2中看到Web界面。
- en: '![figure](../Images/CH08_F02_Trummer.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH08_F02_Trummer.png)'
- en: Figure 8.2 Web interface for chatting with Cohere’s language model Command R+
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2 与Cohere的语言模型Command R+聊天的Web界面
- en: Figure [8.2](#fig__cohere_3) shows where to enter your prompt and the button
    to generate an answer. What about the window on the right (containing the Web
    Connector button)? This is where you specify connectors to use when generating
    your replies. A connector enables Cohere to access external data sources. We can
    activate (or deactivate) the web connector by toggling the corresponding button.
    This connector enables Cohere to query the web, similar to what all of us would
    do when trying to answer a hard question that involves factual knowledge. Give
    it a try, and see how the replies to factual questions change if the web connector
    is activated or deactivated!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8.2](#fig__cohere_3) 展示了如何输入你的提示以及生成答案的按钮。那么右侧的窗口（包含 Web 连接器按钮）是做什么的呢？这就是你指定在生成回复时使用的连接器的地方。连接器使
    Cohere 能够访问外部数据源。我们可以通过切换相应的按钮来激活（或停用）网络连接器。这个连接器使 Cohere 能够查询网络，类似于我们尝试回答涉及事实知识难题时所做的那样。试一试，看看如果激活或停用网络连接器，事实问题的回复会有何变化！
- en: What is RAG?
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 什么是 RAG？
- en: You may have noticed that Cohere’s website prominently advertises *RAG*, but
    what is that? RAG stands for Retrieval Augmented Generation. It means that when
    generating an answer, we augment the input used by the language model with data
    we retrieve from an external source.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到 Cohere 的网站显著宣传了 *RAG*，但那是什么意思呢？RAG 代表检索增强生成。这意味着在生成答案时，我们通过从外部源检索的数据来增强语言模型使用的输入。
- en: 8.2.2 Python library
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 Python 库
- en: 'Cohere offers a Python library similar to the ones we have seen in previous
    sections. Enter the following command in the terminal to install the required
    library:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Cohere 提供了一个类似于我们在前几节中看到的 Python 库。在终端中输入以下命令来安装所需的库：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Listing [8.2](#code__cohere) (available as the Cohere item on the website) contains
    code for a simple question-answering interface. Users enter their access key and
    a question on the command line. Visit [https://dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys)
    to get your access key. In listing [8.2](#code__cohere), after configuring the
    Cohere library with the access key (**1**), we generate an answer using the Cohere
    library (**2**). Note the reference to `connectors` in the call to the `chat`
    function! Here, we specify a list of connectors, enabling Cohere’s model to access
    data from external sources. Connectors are specified as a list (i.e., we can enable
    access not to just one but to a multitude of connectors). Here we use the connector
    with ID `web-search` (that’s the same web search connector we used over the web
    interface in the previous section).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [8.2](#code__cohere)（作为网站上的 Cohere 项目提供）包含一个简单问答界面的代码。用户在命令行中输入他们的访问密钥和问题。访问
    [https://dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys)
    获取你的访问密钥。在列表 [8.2](#code__cohere) 中，在用访问密钥配置 Cohere 库（**1**）之后，我们使用 Cohere 库生成答案（**2**）。注意在调用
    `chat` 函数时对 `connectors` 的引用！在这里，我们指定了一个连接器列表，使 Cohere 的模型能够访问外部数据源。连接器被指定为一个列表（即，我们可以启用对多个连接器的访问，而不仅仅是单个连接器）。在这里，我们使用
    ID 为 `web-search` 的连接器（这就是我们在前一个章节中通过网页界面使用的同一个网络搜索连接器）。
- en: Finally (**3**), we print the answer generated by the model and a list of the
    web sources consulted to generate the answer (along with the queries issued to
    retrieve those documents). That enables us to verify that the generated answer
    is indeed implied by the source material.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后（**3**），我们打印出模型生成的答案以及生成答案时咨询的网页源列表（包括检索这些文档所发出的查询）。这使我们能够验证生成的答案确实是由源材料暗示的。
- en: Listing 8.2 Answering questions using Cohere’s Python library
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.2 使用 Cohere 的 Python 库回答问题
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Configures access'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 配置访问'
- en: '#2 Generates an answer using Cohere'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用 Cohere 生成答案'
- en: '#3 Prints answer and citations'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 打印答案和引用'
- en: 'Let’s try it! Switch to the folder containing the code, and run the following
    command in the terminal (replace the three dots with your Cohere access key):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试！切换到包含代码的文件夹，并在终端中运行以下命令（用你的 Cohere 访问密钥替换三个点）：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You will get an answer similar to the following (which is slightly abbreviated):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到一个类似于以下（略有缩写）的答案：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Generated answer'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 生成的答案'
- en: '#2 Web search queries'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 网络搜索查询'
- en: '#3 Web documents used'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用的网页文档'
- en: Let’s have a closer look. The initial part of the output (**1**) is the answer
    generated by the model. The answer seems reasonable, but can we trust it? This
    is where the remaining parts of the output come into play. The middle part (**2**)
    provides us with information on the web used by Cohere to inform the generated
    answer. Those web queries are chosen automatically based on the input question.
    In this case, web queries correspond precisely to the input question (this is
    not necessarily the case for longer input text). At the end of the output (**3**),
    we find text snippets and the URLs of the documents retrieved via the prior queries.
    In this case, text snippets taken from the web documents (e.g., “Steven Paul Jobs
    was born on February 24, 1955 in San Francisco, California”) make a good case
    supporting the answer from the model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看。输出的初始部分（**1**）是模型生成的答案。答案看起来合理，但我们能信任它吗？这就是输出剩余部分发挥作用的地方。中间部分（**2**）提供了
    Cohere 用于生成答案所使用的网络信息。这些网络查询是根据输入问题自动选择的。在这种情况下，网络查询与输入问题完全对应（对于较长的输入文本，这并不一定是这种情况）。在输出的末尾（**3**），我们找到了通过先前查询检索到的文本片段和文档的网址。在这种情况下，从网络文档中摘取的文本片段（例如，“史蒂夫·乔布斯于1955年2月24日出生于加利福尼亚州旧金山”）很好地支持了模型的答案。
- en: 8.3 Google
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 Google
- en: Google, a company that needs no introduction, has been deeply involved with
    language models since the very beginning. In fact, the Transformer architecture
    [2] used by virtually all language models was invented (primarily) by Google researchers.
    No wonder Google is developing its own models. At the time of writing, Gemini
    is one of Google’s most recent models, and we’ll try it in this section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 无需介绍，Google 公司自语言模型开始以来就深度参与其中。事实上，几乎所有语言模型使用的 Transformer 架构 [2] 是由 Google
    研究人员发明的（主要是）。难怪 Google 正在开发自己的模型。在撰写本文时，Gemini 是 Google 最新的模型之一，我们将在本节中尝试它。
- en: 8.3.1 Chatting with Gemini
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 与 Gemini 聊天
- en: To try Gemini, go to [https://gemini.google.com/](https://gemini.google.com/).
    After signing up for an account, you should see the interface depicted in figure
    [8.3](#fig__google_1).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试 Gemini，请访问 [https://gemini.google.com/](https://gemini.google.com/)。注册账户后，你应该能看到图
    [8.3](#fig__google_1) 中展示的界面。
- en: '![figure](../Images/CH08_F03_Trummer.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH08_F03_Trummer.png)'
- en: Figure 8.3 Web interface for chatting with Google’s Gemini model. Click Settings
    to activate additional functionality.
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.3 与 Google 的 Gemini 模型进行聊天的网络界面。点击设置以激活附加功能。
- en: Simply enter your text in the corresponding input field (labeled Enter prompt
    here in figure [8.3](#fig__google_1)), and press Enter to generate an answer.
    Gemini is not limited to text input. Click the button on the right to upload pictures.
    In your chats with Gemini, you can reference those pictures and ask questions
    about them.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地将你的文本输入相应的输入字段（如图 [8.3](#fig__google_1) 中的“在此处输入提示”所示），然后按 Enter 键生成答案。Gemini
    不仅限于文本输入。点击右侧的按钮上传图片。在与 Gemini 的对话中，你可以引用这些图片并对其提问。
- en: One particularity of Gemini, distinguishing it from all the other models we
    have discussed so far, is its integration with other Google tools. Click the Settings
    button (marked in figure [8.3](#fig__google_1)) and then the Extensions option.
    For instance, you can give Gemini access to your emails by clicking the associated
    button. Ever had the problem of finding information hidden in year-old emails
    in your inbox? Google’s Gemini has the potential to help with that.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini 的一个独特之处在于，它与其他我们之前讨论过的模型不同，那就是它与其他 Google 工具的集成。点击图 [8.3](#fig__google_1)
    中的设置按钮（标记），然后选择扩展选项。例如，你可以通过点击相关按钮让 Gemini 访问你的电子邮件。你是否遇到过在收件箱中寻找隐藏在一年前电子邮件中的信息的问题？Google
    的 Gemini 有潜力帮助解决这个问题。
- en: 8.3.2 The Python library
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 Python 库
- en: 'Like other providers of language models, Google offers a Python library for
    model access. You can install the library using the following code in the terminal:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他语言模型提供商一样，Google 提供了一个用于模型访问的 Python 库。你可以在终端中使用以下代码安装库：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Go to [https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)
    to get your access key for the Google API. Follow the instructions, and copy the
    key after creating it. Listing [8.3](#code__gemini) shows how to use Gemini in
    Python to answer questions. The steps are similar to the previous libraries. The
    input parameters (**1**) include the access key (alternatively, we can store the
    key in an environment variable) as well as the question to answer. Next, we configure
    the Google library with the access key (**2**). Now we can generate a model and
    use it to answer questions via the `generate_content` method (**3**). Finally,
    we print out the generated question (**4**).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)以获取Google
    API的访问密钥。按照指示操作，创建后复制密钥。列表[8.3](#code__gemini)展示了如何使用Python中的Gemini来回答问题。步骤与之前的库类似。输入参数（**1**）包括访问密钥（或者，我们也可以将密钥存储在环境变量中）以及要回答的问题。接下来，我们使用访问密钥配置Google库（**2**）。现在我们可以生成一个模型并使用`generate_content`方法来回答问题（**3**）。最后，我们打印出生成的问答（**4**）。
- en: Listing 8.3 Answering questions using Google’s Gemini model
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.3 使用Google的Gemini模型回答问题
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Defines the input parameters'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义输入参数'
- en: '#2 Configures the API with the access key'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 配置API访问密钥'
- en: '#3 Generates an answer with Gemini'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用Gemini生成答案'
- en: '#4 Prints the answer'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 打印答案'
- en: 'You can find the code on the book’s website using the Google link. In the terminal,
    change to the directory containing the code. For instance, run the following command
    to test Gemini (replace the three dots with your Google access key):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Google链接在本书的网站上找到代码。在终端中，切换到包含代码的目录。例如，运行以下命令以测试Gemini（将三个点替换为您的Google访问密钥）：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A full overview of Google’s library is beyond the scope of this book. However,
    knowing the libraries of other providers of large language models, you should
    be able to familiarize yourself quickly with this API as well.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Google库的全面概述超出了本书的范围。然而，了解大型语言模型其他提供商的库，您应该能够快速熟悉这个API。
- en: 8.4 Hugging Face
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 Hugging Face
- en: Providers like OpenAI invest millions of dollars to train models like GPT-4o.
    The result of all that expensive training is values for model parameters that
    make the model perform best. After investing all that money, you would not necessarily
    want to share the results of training freely, right? Hence, models like GPT-4o
    are typically closed, meaning OpenAI does not share the parameter values that
    result from training (note that OpenAI has shared other models, such as Whisper).
    Instead, OpenAI processes prompts for you on its own infrastructure while charging
    you a processing fee (that’s how we ultimately pay for all that expensive model
    training).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于OpenAI这样的提供商投资数百万美元来训练GPT-4o等模型。所有这些昂贵训练的结果是使模型表现最佳的模型参数值。在投入所有这些资金之后，您可能并不想免费分享训练结果，对吧？因此，像GPT-4o这样的模型通常是封闭的，这意味着OpenAI不会分享训练产生的参数值（注意，OpenAI已经分享了其他模型，如Whisper）。相反，OpenAI在自己的基础设施上为您处理提示，并收取处理费用（这就是我们最终为所有昂贵的模型训练付费的方式）。
- en: More and more, however, language model providers face competition from an extremely
    lively open source sector. Universities, startup companies, and enthusiasts all
    train their own models and often release the models (and their parameter values)
    freely to the public. That enables you to run those models yourself, locally,
    on your own dedicated infrastructure. For smaller models, a laptop with a GPU
    will often suffice. For larger models, you may need to use a GPU cluster (or resort
    to cloud providers that run those open source models for you). Besides potential
    financial advantages (running models on your own infrastructure may be cheaper),
    other considerations can make running models locally the only viable choice. For
    instance, you may not want to trust external providers with particularly sensitive
    data. If you don’t want to send your data, running locally is the only option.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，越来越多的语言模型提供商面临着来自一个极其活跃的开源领域的竞争。大学、初创公司和爱好者都在训练自己的模型，并且经常免费向公众发布模型（及其参数值）。这使得您可以在自己的专用基础设施上本地运行这些模型。对于较小的模型，带有GPU的笔记本电脑通常就足够了。对于较大的模型，您可能需要使用GPU集群（或者求助于为您运行这些开源模型的云服务提供商）。除了潜在的财务优势（在自己的基础设施上运行模型可能更便宜）之外，其他考虑因素也可能使本地运行模型成为唯一可行的选择。例如，您可能不想将特别敏感的数据交给外部提供商。如果您不想发送数据，本地运行是唯一的选择。
- en: Typically, open source models are significantly smaller than the models offered
    by cloud providers. That makes sense because after all, who has a few million
    dollars lying around to train a model? However, due to the sheer number of models
    available, it is often possible to find an open source model that is specialized
    in solving just the task you’re interested in. For instance, the Hugging Face
    Transformers platform features over 1,000,000 Transformer models at the time of
    writing! Whatever task you are facing, chances are that you may find just the
    right model. In this section, we will look at the Hugging Face platform and see
    how to use its models locally.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，开源模型比云提供商提供的模型要小得多。这是有道理的，因为毕竟，谁会手头有几百万美元来训练一个模型呢？然而，由于可用的模型数量众多，通常可以找到一个专门解决你感兴趣的任务的开源模型。例如，Hugging
    Face Transformers 平台在撰写本文时拥有超过 1,000,000 个 Transformer 模型！无论你面临什么任务，你可能会找到恰好适合的模型。在本节中，我们将查看
    Hugging Face 平台，并了解如何在其本地使用模型。
- en: 8.4.1 Web platform
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 网络平台
- en: Go to [https://huggingface.co/](https://huggingface.co/). Hugging Face Transformers
    offers various resources around Transformer models. That includes not only the
    models but also data sets you can use to train your own models as well as cloud
    offerings that let you run open source models on Hugging Face’s cloud infrastructure.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[https://huggingface.co/](https://huggingface.co/)。Hugging Face Transformers
    提供了围绕 Transformer 模型的各种资源。这包括不仅限于模型，还有你可以用来训练自己模型的数据库集，以及允许你在 Hugging Face 的云基础设施上运行开源模型的云服务。
- en: For the moment, we are interested in models. Click the Models button to see
    the list of models shown in figure [8.4](#fig__hf_2).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们关注的是模型。点击“模型”按钮，查看如图 [8.4](#fig__hf_2) 所示的模型列表。
- en: '![figure](../Images/CH08_F04_Trummer.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F04_Trummer.png)'
- en: Figure 8.4 Overview of Hugging Face Transformer models. Click the Tasks filters
    to narrow the selec-tion. Click models in the Models list to see details.
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.4 Hugging Face Transformer 模型概述。点击“任务”筛选器以缩小选择范围。点击“模型”列表中的模型以查看详细信息。
- en: We’re seeing a list of over 1 million Transformer models (and, as the number
    of models is growing daily, you will probably see even more)! Whew. That’s a few
    too many. Let’s narrow them down. On the left side of the screen are various filter
    options to get the list down to the models you really care about. For instance,
    we can filter by the type of task we need the model to do. This includes text
    classification (e.g., classifying reviews by sentiment), visual question answering
    (e.g., does the picture show an apple?), and speech-to-text transcription (e.g.,
    transcribing voice queries to text). For almost all the tasks we have discussed
    in this book, you may be able to find a specialized model. Click any of the standard
    tasks to see only the models that solve that task.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了超过 100 万个 Transformer 模型的列表（并且随着模型数量的每日增长，你可能还会看到更多！）哇。这太多了点。让我们缩小范围。屏幕左侧有各种筛选选项，可以帮助你筛选出真正关心的模型。例如，我们可以根据模型需要完成的任务类型进行筛选。这包括文本分类（例如，根据情感对评论进行分类）、视觉问答（例如，图片是否显示苹果？）和语音转文本转录（例如，将语音查询转录为文本）。对于本书中讨论的几乎所有任务，你可能都能找到一个专门的模型。点击任何标准任务，只显示解决该任务的模型。
- en: When you click one of the remaining models, you will see a detailed model description
    such as that shown in figure [8.5](#fig__hf_3) for the BLIP model from Salesforce,
    a model that processes images to generate suitable captions. On the left side
    is a thorough description of the model along with links to relevant papers and
    code samples showing you how to use the model on different hardware platforms
    (i.e., locally). On the right side is an interface that allows you to try the
    model on a few sample pictures.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当你点击剩余的任何模型时，你将看到如图 [8.5](#fig__hf_3) 所示的详细模型描述，例如 Salesforce 的 BLIP 模型，这是一个处理图像以生成合适标题的模型。左侧是对模型的详细描述，以及链接到相关论文和代码示例，展示如何在不同的硬件平台上（即本地）使用该模型。右侧是一个界面，允许你尝试在几个样本图片上使用该模型。
- en: '![figure](../Images/CH08_F05_Trummer.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F05_Trummer.png)'
- en: Figure 8.5 Detailed description of Salesforce’s BLIP model ([https://huggingface.co/Salesforce/blip](https://huggingface.co/Salesforce/blip-image-captioning-large)
    [-image-captioning-large](https://huggingface.co/Salesforce/blip-image-captioning-large)).
    Read the description on the left, or try the model via the interface on the right.
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5 Salesforce的BLIP模型的详细描述（[https://huggingface.co/Salesforce/blip](https://huggingface.co/Salesforce/blip-image-captioning-large)
    [-image-captioning-large](https://huggingface.co/Salesforce/blip-image-captioning-large)）。阅读左侧的描述，或者通过右侧的界面尝试该模型。
- en: 8.4.2 Python library
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 Python库
- en: 'We are now about to run Transformer models on our own local infrastructure!
    The performance you get will, of course, depend on the properties of the hardware
    you’re using. However, even with moderate computation power, you should be able
    to work with the models we’re about to try. But first, we must install the Hugging
    Face Transformers library. Enter the following commands in your terminal:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在即将在我们的本地基础设施上运行Transformer模型！你获得的性能当然将取决于你所使用的硬件特性。然而，即使拥有适度的计算能力，你也应该能够使用我们即将尝试的模型。但首先，我们必须安装Hugging
    Face Transformers库。在你的终端中输入以下命令：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The Transformers library is based on PyTorch, a popular machine-learning framework.
    If you haven’t installed PyTorch yet, run the following command in your terminal
    (otherwise, you will receive an error message when trying to run the following
    code):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers库基于PyTorch，这是一个流行的机器学习框架。如果你还没有安装PyTorch，请在你的终端中运行以下命令（否则，在尝试运行以下代码时你会收到错误信息）：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: That’s it for the setup! We’re ready to use the Hugging Face Transformers library,
    which we import via `import transformers`. The Transformers library offers a plethora
    of features and various ways to use the model in its repository (or to train your
    own models, for that matter). In this section, we’ll only cover a small subset
    of them, but it’s enough to get a first impression.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 设置到此结束！我们准备好使用Hugging Face Transformers库了，我们通过`import transformers`导入它。Transformers库提供了丰富的功能和多种使用模型的方法，包括在它的存储库中（或者训练你自己的模型）。在本节中，我们只介绍其中的一小部分，但足以获得初步印象。
- en: Let’s assume that you have found a model in the Hugging Face model repository
    that you would like to try. To make things more concrete, let’s say we’re talking
    about the Roberta model for sentiment classification, offered by Cardiff University
    (you can find that model at [https://mng.bz/rKoX](https://mng.bz/rKoX)). Compared
    to GPT-4o and most other models discussed in this section, this is a fairly small
    model. However, it is specialized for analyzing text to determine the underlying
    sentiment. Although it is much less generic than GPT-4o and similar models, it
    does one task and does it fairly well. If you are looking to classify reviews,
    for instance, you may find this model very suitable.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在Hugging Face模型存储库中找到了一个你想要尝试的模型。为了使事情更具体，让我们假设我们正在讨论卡迪夫大学提供的用于情感分类的Roberta模型（你可以在[https://mng.bz/rKoX](https://mng.bz/rKoX)找到该模型）。与GPT-4o和本节中讨论的大多数其他模型相比，这是一个相当小的模型。然而，它专门用于分析文本以确定潜在的情感。尽管它比GPT-4o和类似模型要通用得多，但它做一项任务并且做得相当不错。如果你想要对评论进行分类，例如，你可能发现这个模型非常合适。
- en: 'The easiest way to use a model via the Transformers library is via a `pipeline`.
    The following command creates a pipeline for sentiment classification based on
    the Roberta model:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Transformers库使用模型的最简单方法是使用`pipeline`。以下命令创建了一个基于Roberta模型的情感分类管道：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As you see, we specify the model using the last part of its URL: the name of
    the account providing the model (`cardiffnlp`) followed by the ID of the model
    itself. When using this code for the first time, the Transformers library will
    automatically download the model from its public model repository. Note that this
    code works in part because the model we are referring to is associated with a
    specific task class. For other models, you may have to specify the types of tasks
    you want them to solve as a separate input parameter.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们使用URL的最后部分指定模型：提供模型的账户名称（`cardiffnlp`）以及模型本身的ID。当你第一次使用此代码时，Transformers库将自动从其公共模型存储库下载模型。请注意，此代码部分之所以有效，是因为我们引用的模型与特定的任务类别相关联。对于其他模型，你可能需要指定你希望它们解决的任务类型作为单独的输入参数。
- en: 'We created a pipeline! Now we can use it to classify text—for example, like
    so (we assume that the variable `text_to_classify` contains, you guessed it, the
    text to classify):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个管道！现在我们可以用它来分类文本——例如，如下所示（我们假设变量`text_to_classify`包含的是要分类的文本）：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We have all we need to build a simple application that classifies reviews (based
    on whether the underlying sentiment is positive, meaning a good review, or negative,
    meaning the review is bad). The next listing shows the corresponding code (you
    can find it as the Hugging Face item on the book’s website).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经拥有了构建一个简单的应用所需的一切，该应用可以分类评论（基于是否为积极情绪，即好评，或消极情绪，即差评）。下面的列表显示了相应的代码（您可以在书籍网站上找到它作为Hugging
    Face项目）。
- en: Listing 8.4 Sentiment classification using Hugging Face Transformers
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.4 使用Hugging Face Transformers进行情感分类
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Creates a pipeline'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建管道'
- en: '#2 Applies the pipeline to input'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 应用管道到输入'
- en: '#3 Prints out the classification result'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 打印分类结果'
- en: 'You may notice a difference from all the code we have seen previously: we don’t
    need to specify an access key! Because the Hugging Face models are publicly available
    and we’re running them on our own infrastructure, there is no need to provide
    any kind of credentials. Instead, the only input is a review text that we want
    to classify.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到与我们之前看到的代码有所不同：我们不需要指定访问密钥！因为Hugging Face模型是公开可用的，并且我们正在自己的基础设施上运行它们，所以不需要提供任何类型的凭证。相反，唯一的输入是我们想要分类的评论文本。
- en: 'The code composes the snippets discussed earlier. It creates a pipeline (**1**),
    uses it to classify the input text (**2**), and finally prints out the result
    (**3**). You can try it by switching to the containing folder in your terminal
    and entering, for instance, the following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 代码组合了之前讨论的片段。它创建了一个管道（**1**），使用它来分类输入文本（**2**），并最终打印出结果（**3**）。您可以通过在终端中切换到包含文件夹并输入，例如以下内容来尝试它：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When running the code for the first time, you may have to wait for a few minutes
    while the Transformers library downloads the model you are referencing. But no
    worries: the library caches the downloaded model so you won’t have to wait when
    you run the code a second time. After processing finishes, you should see output
    like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当您第一次运行代码时，您可能需要等待几分钟，因为Transformers库正在下载您引用的模型。但请放心：库会缓存下载的模型，所以您在第二次运行代码时就不需要等待了。处理完成后，您应该会看到如下输出：
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'That’s certainly correct for the sample input: the review is concise and 100%
    negative. Try it with a few different reviews, and compare the output to what
    you get with models like GPT-4o. In the majority of cases, the classification
    result should be fairly similar. Of course, GPT-4o is a much more generic model
    and can be used to solve a variety of other tasks as well. But as long as you’re
    interested in classifying reviews, this model offers an interesting tradeoff between
    quality and cost.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于样本输入来说这当然是正确的：评论简洁且100%负面。尝试用几篇不同的评论，并将输出与GPT-4o等模型的结果进行比较。在大多数情况下，分类结果应该相当相似。当然，GPT-4o是一个更通用的模型，也可以用于解决各种其他任务。但只要您对分类评论感兴趣，这个模型在质量和成本之间提供了一个有趣的权衡。
- en: Summary
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In addition to OpenAI, several other providers offer large language models.
    Most providers offer closed-source models via a cloud API. The models differ based
    on their generality, output quality, and pricing.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了OpenAI，还有其他几家提供商提供大型语言模型。大多数提供商通过云API提供封闭源代码模型。这些模型在通用性、输出质量和定价方面有所不同。
- en: Most providers offer a Python library to access language models.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数提供商提供Python库来访问语言模型。
- en: Hugging Face Transformers offers various models for free download.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face Transformers提供各种模型供免费下载。
- en: 8.6 References
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 参考文献
- en: 'Bai, Y., Kadavath, S., Kundu, S., et al. (2022). Constitutional AI: Harmlessness
    from AI Feedback. *CoRR abs/2212.0*, 1–32.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bai, Y., Kadavath, S., Kundu, S., 等人. (2022). 从AI反馈中消除危害的宪法AI. *CoRR abs/2212.0*，第1–32页。
- en: Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is All You Need.
    In *Advances in Neural Information Processing Systems*, pp. 5999–6009.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vaswani, A., Shazeer, N., Parmar, N., 等人. (2017). 注意力就是一切. 在 *神经信息处理系统进展*，第5999–6009页。
