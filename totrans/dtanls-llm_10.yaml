- en: 8 GPT alternatives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Claude, from Anthropic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Command R+, from Cohere
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bard, from Google
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face’s models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Time to meet some of GPT’s “friends”! So far, we have been focusing on GPT
    and other OpenAI models. But OpenAI is not the only game in town. Quite the contrary:
    we are currently witnessing a “Cambrian explosion” of language models, with new
    models popping up every week. Before using language models in production, you
    want to make sure you’re using the best model for your task. In this chapter,
    we’ll look at many of the OpenAI alternatives out there and discuss the pros and
    cons of different models as well as how to use them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Almost all language models nowadays are based on a similar architecture (the
    Transformer architecture). However, models from different providers may differ
    in the way they are trained, the way they represent text, or the way in which
    they are offered and priced. All those factors can make a difference in terms
    of processing fees and output quality for your specific task. Models like GPT-4o
    are powerful and solve almost any task. But this generality comes at a cost: if
    a small, specialized model is available, trained for just the task you’re interested
    in, using such a model may very well be the optimal choice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will notice throughout the following sections that many of the models we
    discuss can be accessed via interfaces that are similar to OpenAI’s interface.
    That’s good news for you: no need to get into a novel and complex framework each
    time you want to try a different model! And with that, let’s start our exploration
    of GPT alternatives.'
  prefs: []
  type: TYPE_NORMAL
- en: Why isn’t my favorite model listed?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If your favorite model or model provider is not listed in this chapter, don’t
    panic! With the growing number of providers and models, it has become impossible
    to give a full overview of available models. If a model is not included here,
    it does not mean it can’t be the best alternative for your task. The interfaces
    of different providers tend to be similar, so you should still be able to use
    what you have learned so far to employ other models without much trouble. Also,
    note that we list model providers in alphabetical order in this chapter. Don’t
    infer any priority from that (we’re not discussing the best providers first).
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Anthropic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many of the stories by Isaac Asimov, one of the most prolific science-fiction
    authors of all time, center on the “three laws of robotics” and their interpretation:'
  prefs: []
  type: TYPE_NORMAL
- en: A robot may not injure a human being or, through inaction, allow a human being
    to come to harm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A robot must obey orders given to it by human beings except where such orders
    would conflict with the First Law.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A robot must protect its own existence as long as such protection does not conflict
    with the First or Second Law.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The goal here is to have a concise set of guidelines that makes sure robots
    are helpful and harmless. Whether the previously mentioned laws provided any inspiration,
    this idea connects to the language models produced by Anthropic, yet another provider
    of large-scale language models.
  prefs: []
  type: TYPE_NORMAL
- en: Anthropic, founded in 2021 (by several former OpenAI members), has repeatedly
    touted the idea of “Constitutional AI” [1] as one of the distinguishing factors,
    compared to other models. In a nutshell, this means that when training models
    to provide users with accurate and inoffensive answers, we rely on a small set
    of rules—a “constitution,” so to speak—to judge the quality of answers. Instead
    of relying on human testers to label answers generated by the model during training,
    we employ a second AI, charged with evaluating the answers of the former according
    to the constitution.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, Claude 3.5 (a reference to the amazing Claude Shannon)
    is the latest model released by Anthropic. In this section, we will try Claude
    via (you guessed it) a web interface and a Python library.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Chatting with Claude
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will have a quick chat with Claude (currently in version 3.5) to get a sense
    of its capabilities. First, go to the Anthropic website at [www.anthropic.com](http://www.anthropic.com),
    and click the Talk to Claude button.
  prefs: []
  type: TYPE_NORMAL
- en: Unless you have created an Anthropic account before, you will be asked to provide
    an email address and a phone number. After verifying your data, you should see
    Claude’s chat interface, shown in figure [8.1](#fig__claude).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F01_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 Web interface for Anthropic’s chatbot Claude
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The interface is pretty intuitive: simply enter text in the corresponding field,
    and click the button on the right to start chatting! Begin with a friendly greeting,
    chat about the weather, or try using Claude to solve some of the tasks from chapter
    2 (e.g., classifying reviews by sentiment or translating questions to SQL queries).'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 Python library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Assuming you have created an account with Anthropic, you can create keys at
    the following URL: [https://console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys).
    Be sure to copy your access key after creating it (as you will not be able to
    access it again afterward)!'
  prefs: []
  type: TYPE_NORMAL
- en: 'After obtaining your access key, go to the terminal and run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will install the Anthropic Python library. If you are familiar with OpenAI’s
    Python library (and chances are that, after reading the previous chapters, you
    are), you should get used to the Anthropic library very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the following listing shows Python code for answering questions
    using Claude. Of course, this code does not do anything that you cannot do via
    the web interface. The purpose is just to show how easy it is to use Claude via
    the Python interface.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1 Answering questions with Anthropic’s Claude model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Configures Anthropic'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Uses Claude for completion'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Prints the completion result'
  prefs: []
  type: TYPE_NORMAL
- en: As input parameters (**1**), we use the Anthropic access key and a question
    we would like answered. Similar to OpenAI’s libraries, we configure access using
    the access key (**2**). After that, we can construct prompts for completion by
    Claude (**3**).
  prefs: []
  type: TYPE_NORMAL
- en: Using Anthropic’s `anthropic.messages` endpoint, we specify the ID of the model
    to use (`claude-3-5-sonnet-20241022` is the newest model by Anthropic at the time
    of writing) and the maximum number of tokens for completion (using the `max_tokens`
    parameter). Similar to OpenAI’s chat models, Claude is designed for chats between
    users and the model. Therefore, the input to Claude is a list of messages (containing
    only a single element in this specific scenario). We obtain the result of Claude’s
    prompt completion in the `content` field (which we print (**4**)).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find this listing as the Anthropic item on the companion website. To
    execute it, open your terminal, and switch to the containing folder. Then, execute
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Replace the three dots with your Anthropic access key. When executing the program,
    you should see an answer to your question, generated by Anthropic’s model.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Cohere
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We briefly discussed hallucinations in chapter 2\. Essentially, a hallucination
    occurs when language models make stuff up because they don’t have access to data
    that is relevant to the task at hand. The Canadian startup Cohere puts a particular
    emphasis on avoiding such hallucinations using a method called *grounding*. Grounding
    the answer of a language model means linking it to real data, thereby reducing
    the chances of “creative output” not based in reality.
  prefs: []
  type: TYPE_NORMAL
- en: Cohere supports a wide range of connectors, enabling its models to access external
    data. For instance, web search is supported, as well access to various databases.
    Internally, Cohere accesses those data sources and provides the language model
    with information tailored to the request. But even better, Cohere shows you all
    the data sources used to generate your answer. If you are the suspicious type
    (and when it comes to answers from language models, you generally should be),
    you can follow up on references and validate that they support the generated answer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how all that works in practice. Time to chat with Cohere’s Command
    R+ model!
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Chatting with Command R+
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the time of writing, Command R+ is one of Cohere’s latest models. As usual,
    you can try it via web interface and use it via Python when processing large amounts
    of data. First, we’ll try the web interface. For that, go to [https://cohere.com/](https://cohere.com/),
    and click Try Now. After signing up for an account, click Playground. You should
    see the web interface in figure [8.2](#fig__cohere_3).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F02_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Web interface for chatting with Cohere’s language model Command R+
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure [8.2](#fig__cohere_3) shows where to enter your prompt and the button
    to generate an answer. What about the window on the right (containing the Web
    Connector button)? This is where you specify connectors to use when generating
    your replies. A connector enables Cohere to access external data sources. We can
    activate (or deactivate) the web connector by toggling the corresponding button.
    This connector enables Cohere to query the web, similar to what all of us would
    do when trying to answer a hard question that involves factual knowledge. Give
    it a try, and see how the replies to factual questions change if the web connector
    is activated or deactivated!
  prefs: []
  type: TYPE_NORMAL
- en: What is RAG?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You may have noticed that Cohere’s website prominently advertises *RAG*, but
    what is that? RAG stands for Retrieval Augmented Generation. It means that when
    generating an answer, we augment the input used by the language model with data
    we retrieve from an external source.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Python library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cohere offers a Python library similar to the ones we have seen in previous
    sections. Enter the following command in the terminal to install the required
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Listing [8.2](#code__cohere) (available as the Cohere item on the website) contains
    code for a simple question-answering interface. Users enter their access key and
    a question on the command line. Visit [https://dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys)
    to get your access key. In listing [8.2](#code__cohere), after configuring the
    Cohere library with the access key (**1**), we generate an answer using the Cohere
    library (**2**). Note the reference to `connectors` in the call to the `chat`
    function! Here, we specify a list of connectors, enabling Cohere’s model to access
    data from external sources. Connectors are specified as a list (i.e., we can enable
    access not to just one but to a multitude of connectors). Here we use the connector
    with ID `web-search` (that’s the same web search connector we used over the web
    interface in the previous section).
  prefs: []
  type: TYPE_NORMAL
- en: Finally (**3**), we print the answer generated by the model and a list of the
    web sources consulted to generate the answer (along with the queries issued to
    retrieve those documents). That enables us to verify that the generated answer
    is indeed implied by the source material.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.2 Answering questions using Cohere’s Python library
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Configures access'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Generates an answer using Cohere'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Prints answer and citations'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try it! Switch to the folder containing the code, and run the following
    command in the terminal (replace the three dots with your Cohere access key):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get an answer similar to the following (which is slightly abbreviated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Generated answer'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Web search queries'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Web documents used'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a closer look. The initial part of the output (**1**) is the answer
    generated by the model. The answer seems reasonable, but can we trust it? This
    is where the remaining parts of the output come into play. The middle part (**2**)
    provides us with information on the web used by Cohere to inform the generated
    answer. Those web queries are chosen automatically based on the input question.
    In this case, web queries correspond precisely to the input question (this is
    not necessarily the case for longer input text). At the end of the output (**3**),
    we find text snippets and the URLs of the documents retrieved via the prior queries.
    In this case, text snippets taken from the web documents (e.g., “Steven Paul Jobs
    was born on February 24, 1955 in San Francisco, California”) make a good case
    supporting the answer from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Google
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google, a company that needs no introduction, has been deeply involved with
    language models since the very beginning. In fact, the Transformer architecture
    [2] used by virtually all language models was invented (primarily) by Google researchers.
    No wonder Google is developing its own models. At the time of writing, Gemini
    is one of Google’s most recent models, and we’ll try it in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Chatting with Gemini
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To try Gemini, go to [https://gemini.google.com/](https://gemini.google.com/).
    After signing up for an account, you should see the interface depicted in figure
    [8.3](#fig__google_1).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F03_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Web interface for chatting with Google’s Gemini model. Click Settings
    to activate additional functionality.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Simply enter your text in the corresponding input field (labeled Enter prompt
    here in figure [8.3](#fig__google_1)), and press Enter to generate an answer.
    Gemini is not limited to text input. Click the button on the right to upload pictures.
    In your chats with Gemini, you can reference those pictures and ask questions
    about them.
  prefs: []
  type: TYPE_NORMAL
- en: One particularity of Gemini, distinguishing it from all the other models we
    have discussed so far, is its integration with other Google tools. Click the Settings
    button (marked in figure [8.3](#fig__google_1)) and then the Extensions option.
    For instance, you can give Gemini access to your emails by clicking the associated
    button. Ever had the problem of finding information hidden in year-old emails
    in your inbox? Google’s Gemini has the potential to help with that.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 The Python library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Like other providers of language models, Google offers a Python library for
    model access. You can install the library using the following code in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Go to [https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)
    to get your access key for the Google API. Follow the instructions, and copy the
    key after creating it. Listing [8.3](#code__gemini) shows how to use Gemini in
    Python to answer questions. The steps are similar to the previous libraries. The
    input parameters (**1**) include the access key (alternatively, we can store the
    key in an environment variable) as well as the question to answer. Next, we configure
    the Google library with the access key (**2**). Now we can generate a model and
    use it to answer questions via the `generate_content` method (**3**). Finally,
    we print out the generated question (**4**).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.3 Answering questions using Google’s Gemini model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines the input parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Configures the API with the access key'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Generates an answer with Gemini'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Prints the answer'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code on the book’s website using the Google link. In the terminal,
    change to the directory containing the code. For instance, run the following command
    to test Gemini (replace the three dots with your Google access key):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: A full overview of Google’s library is beyond the scope of this book. However,
    knowing the libraries of other providers of large language models, you should
    be able to familiarize yourself quickly with this API as well.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Hugging Face
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Providers like OpenAI invest millions of dollars to train models like GPT-4o.
    The result of all that expensive training is values for model parameters that
    make the model perform best. After investing all that money, you would not necessarily
    want to share the results of training freely, right? Hence, models like GPT-4o
    are typically closed, meaning OpenAI does not share the parameter values that
    result from training (note that OpenAI has shared other models, such as Whisper).
    Instead, OpenAI processes prompts for you on its own infrastructure while charging
    you a processing fee (that’s how we ultimately pay for all that expensive model
    training).
  prefs: []
  type: TYPE_NORMAL
- en: More and more, however, language model providers face competition from an extremely
    lively open source sector. Universities, startup companies, and enthusiasts all
    train their own models and often release the models (and their parameter values)
    freely to the public. That enables you to run those models yourself, locally,
    on your own dedicated infrastructure. For smaller models, a laptop with a GPU
    will often suffice. For larger models, you may need to use a GPU cluster (or resort
    to cloud providers that run those open source models for you). Besides potential
    financial advantages (running models on your own infrastructure may be cheaper),
    other considerations can make running models locally the only viable choice. For
    instance, you may not want to trust external providers with particularly sensitive
    data. If you don’t want to send your data, running locally is the only option.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, open source models are significantly smaller than the models offered
    by cloud providers. That makes sense because after all, who has a few million
    dollars lying around to train a model? However, due to the sheer number of models
    available, it is often possible to find an open source model that is specialized
    in solving just the task you’re interested in. For instance, the Hugging Face
    Transformers platform features over 1,000,000 Transformer models at the time of
    writing! Whatever task you are facing, chances are that you may find just the
    right model. In this section, we will look at the Hugging Face platform and see
    how to use its models locally.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 Web platform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Go to [https://huggingface.co/](https://huggingface.co/). Hugging Face Transformers
    offers various resources around Transformer models. That includes not only the
    models but also data sets you can use to train your own models as well as cloud
    offerings that let you run open source models on Hugging Face’s cloud infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: For the moment, we are interested in models. Click the Models button to see
    the list of models shown in figure [8.4](#fig__hf_2).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F04_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 Overview of Hugging Face Transformer models. Click the Tasks filters
    to narrow the selec-tion. Click models in the Models list to see details.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’re seeing a list of over 1 million Transformer models (and, as the number
    of models is growing daily, you will probably see even more)! Whew. That’s a few
    too many. Let’s narrow them down. On the left side of the screen are various filter
    options to get the list down to the models you really care about. For instance,
    we can filter by the type of task we need the model to do. This includes text
    classification (e.g., classifying reviews by sentiment), visual question answering
    (e.g., does the picture show an apple?), and speech-to-text transcription (e.g.,
    transcribing voice queries to text). For almost all the tasks we have discussed
    in this book, you may be able to find a specialized model. Click any of the standard
    tasks to see only the models that solve that task.
  prefs: []
  type: TYPE_NORMAL
- en: When you click one of the remaining models, you will see a detailed model description
    such as that shown in figure [8.5](#fig__hf_3) for the BLIP model from Salesforce,
    a model that processes images to generate suitable captions. On the left side
    is a thorough description of the model along with links to relevant papers and
    code samples showing you how to use the model on different hardware platforms
    (i.e., locally). On the right side is an interface that allows you to try the
    model on a few sample pictures.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F05_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 Detailed description of Salesforce’s BLIP model ([https://huggingface.co/Salesforce/blip](https://huggingface.co/Salesforce/blip-image-captioning-large)
    [-image-captioning-large](https://huggingface.co/Salesforce/blip-image-captioning-large)).
    Read the description on the left, or try the model via the interface on the right.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 8.4.2 Python library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are now about to run Transformer models on our own local infrastructure!
    The performance you get will, of course, depend on the properties of the hardware
    you’re using. However, even with moderate computation power, you should be able
    to work with the models we’re about to try. But first, we must install the Hugging
    Face Transformers library. Enter the following commands in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The Transformers library is based on PyTorch, a popular machine-learning framework.
    If you haven’t installed PyTorch yet, run the following command in your terminal
    (otherwise, you will receive an error message when trying to run the following
    code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: That’s it for the setup! We’re ready to use the Hugging Face Transformers library,
    which we import via `import transformers`. The Transformers library offers a plethora
    of features and various ways to use the model in its repository (or to train your
    own models, for that matter). In this section, we’ll only cover a small subset
    of them, but it’s enough to get a first impression.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that you have found a model in the Hugging Face model repository
    that you would like to try. To make things more concrete, let’s say we’re talking
    about the Roberta model for sentiment classification, offered by Cardiff University
    (you can find that model at [https://mng.bz/rKoX](https://mng.bz/rKoX)). Compared
    to GPT-4o and most other models discussed in this section, this is a fairly small
    model. However, it is specialized for analyzing text to determine the underlying
    sentiment. Although it is much less generic than GPT-4o and similar models, it
    does one task and does it fairly well. If you are looking to classify reviews,
    for instance, you may find this model very suitable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to use a model via the Transformers library is via a `pipeline`.
    The following command creates a pipeline for sentiment classification based on
    the Roberta model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As you see, we specify the model using the last part of its URL: the name of
    the account providing the model (`cardiffnlp`) followed by the ID of the model
    itself. When using this code for the first time, the Transformers library will
    automatically download the model from its public model repository. Note that this
    code works in part because the model we are referring to is associated with a
    specific task class. For other models, you may have to specify the types of tasks
    you want them to solve as a separate input parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We created a pipeline! Now we can use it to classify text—for example, like
    so (we assume that the variable `text_to_classify` contains, you guessed it, the
    text to classify):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We have all we need to build a simple application that classifies reviews (based
    on whether the underlying sentiment is positive, meaning a good review, or negative,
    meaning the review is bad). The next listing shows the corresponding code (you
    can find it as the Hugging Face item on the book’s website).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.4 Sentiment classification using Hugging Face Transformers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates a pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Applies the pipeline to input'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Prints out the classification result'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may notice a difference from all the code we have seen previously: we don’t
    need to specify an access key! Because the Hugging Face models are publicly available
    and we’re running them on our own infrastructure, there is no need to provide
    any kind of credentials. Instead, the only input is a review text that we want
    to classify.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code composes the snippets discussed earlier. It creates a pipeline (**1**),
    uses it to classify the input text (**2**), and finally prints out the result
    (**3**). You can try it by switching to the containing folder in your terminal
    and entering, for instance, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When running the code for the first time, you may have to wait for a few minutes
    while the Transformers library downloads the model you are referencing. But no
    worries: the library caches the downloaded model so you won’t have to wait when
    you run the code a second time. After processing finishes, you should see output
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s certainly correct for the sample input: the review is concise and 100%
    negative. Try it with a few different reviews, and compare the output to what
    you get with models like GPT-4o. In the majority of cases, the classification
    result should be fairly similar. Of course, GPT-4o is a much more generic model
    and can be used to solve a variety of other tasks as well. But as long as you’re
    interested in classifying reviews, this model offers an interesting tradeoff between
    quality and cost.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to OpenAI, several other providers offer large language models.
    Most providers offer closed-source models via a cloud API. The models differ based
    on their generality, output quality, and pricing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most providers offer a Python library to access language models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face Transformers offers various models for free download.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.6 References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bai, Y., Kadavath, S., Kundu, S., et al. (2022). Constitutional AI: Harmlessness
    from AI Feedback. *CoRR abs/2212.0*, 1–32.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is All You Need.
    In *Advances in Neural Information Processing Systems*, pp. 5999–6009.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
