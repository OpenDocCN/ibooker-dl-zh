- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: A Brief History of Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习简史
- en: The roots of the current deep learning boom go surprisingly far back, to the
    1950s. While vague ideas of “intelligent machines” can be found further back in
    fiction and speculation, the 1950s and ’60s saw the introduction of the first
    “artificial neural networks,” based on a dramatically simplified model of biological
    neurons. Amongst these models, the Perceptron system articulated by Frank Rosenblatt
    garnered particular interest (and hype). Connected to a simple “camera” circuit,
    it could learn to distinguish different types of objects. Although the first version
    ran as software on an IBM computer, subsequent versions were done in pure hardware.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当前深度学习热潮的根源出奇地早，可以追溯到上世纪50年代。虽然“智能机器”的模糊想法可以在小说和推测中找到更早的踪迹，但上世纪50年代和60年代见证了第一批基于生物神经元极为简化模型的“人工神经网络”的引入。在这些模型中，由弗兰克·罗森布拉特提出的感知器系统引起了特别的兴趣（和炒作）。连接到一个简单的“摄像头”电路，它可以学会区分不同类型的物体。尽管第一个版本是在IBM计算机上作为软件运行的，但随后的版本都是在纯硬件上完成的。
- en: Interest in the multilayer perceptron (MLP) model continued through the ’60s.
    This changed when, in 1969, Marvin Minksy and Seymour Papert published their book
    *Perceptrons* (MIT Press). The book contained a proof showing that linear perceptrons
    could not classify the behavior of a nonlinear function (XOR). Despite the limitations
    of the proof (nonlinear perceptron models existed at the time of the book’s publication,
    and are even noted by the authors), its publication heralded the plummeting of
    funding for neural network models. Research would not recover until the 1980s,
    with the rise of a new generation of researchers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对多层感知器（MLP）模型的兴趣在60年代持续。这种情况在1969年马文·明斯基和西摩·帕普特出版了他们的书《感知器》（麻省理工学院出版社）后发生了变化。这本书中包含了一个证明，证明了线性感知器无法对非线性函数（XOR）的行为进行分类。尽管证明存在局限性（在书出版时存在非线性感知器模型，作者甚至有所提及），但其出版标志着对神经网络模型的资金投入急剧下降。直到1980年代，研究才得以恢复，新一代研究人员崛起。
- en: The increase in computing power together with the development of the back-propagation
    technique (known in various forms since the ’60s, but not applied in general until
    the ’80s) prompted a resurgence of interest in neural networks. Not only did computers
    have the power to train larger networks, but we also had the techniques to train
    deeper networks efficiently. The first convolutional neural networks combined
    these insights with a model of visual recognition from mammalian brains, yielding
    for the first time networks that could efficiently recognize complex images such
    as handwritten digits and faces. Convolutional networks do this by applying the
    same “subnetwork” to different locations of the image and aggregating the results
    of these into higher-level features. In [Chapter 12](ch12.html#image_style) we
    look at how this works in more detail.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算能力的增加以及反向传播技术的发展（自上世纪60年代以来以各种形式存在，但直到80年代才得到普遍应用），引发了对神经网络的兴趣再次高涨。计算机不仅有能力训练更大的网络，而且我们还有技术可以高效地训练更深层次的网络。第一个卷积神经网络将这些见解与哺乳动物大脑的视觉识别模型相结合，首次实现了能够高效识别复杂图像（如手写数字和人脸）的网络。卷积网络通过将相同的“子网络”应用于图像的不同位置，并将这些结果聚合到更高级的特征中来实现这一点。在[第12章](ch12.html#image_style)中，我们将更详细地探讨这一点。
- en: In the ’90s and early 2000s interest in neural networks declined again as more
    “understandable” models like support vector machines (SVMs) and decision trees
    became popular. SVMs proved to be excellent classifiers for many data sources
    of the time, especially when coupled with human-engineered features. In computer
    vision, “feature engineering” became popular. This involves building feature detectors
    for small elements in a picture and combining them by hand into something that
    recognizes more complex forms. It later turned out that deep learning nets learn
    to recognize very similar features and learn to combine them in a very similar
    way. In [Chapter 12](ch12.html#image_style) we explore some of the inner workings
    of these networks and visualize what they learn.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在90年代和21世纪初，对神经网络的兴趣再次下降，因为更“可理解”的模型如支持向量机（SVM）和决策树变得流行。SVM在当时许多数据源中被证明是优秀的分类器，特别是当与人工设计的特征结合时。在计算机视觉中，“特征工程”变得流行。这涉及构建图片中小元素的特征检测器，并手动将它们组合成识别更复杂形式的东西。后来发现，深度学习网络学会识别非常相似的特征，并学会以非常相似的方式将它们组合起来。在[第12章](ch12.html#image_style)中，我们探索了这些网络的一些内部工作，并可视化它们学到的内容。
- en: With the advent of general-purpose programming on graphics processing units
    (GPUs) in the late 2000s, neural network architectures were able to make great
    strides over the competition. GPUs contain thousands of small processors that
    can do trillions of operations per second in parallel. Originally developed for
    computer gaming, where this is needed to render complex 3D scenes in real time,
    it turned out that the same hardware can be used to train neural networks in parallel,
    achieving speed improvements of a factor of 10 or higher.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着2000年代末期通用图形处理单元（GPU）上的通用编程的出现，神经网络架构能够在竞争中取得巨大进步。GPU包含数千个小处理器，可以并行进行数万亿次操作。最初是为计算机游戏开发的，用于实时渲染复杂的3D场景，结果表明相同的硬件可以并行训练神经网络，实现速度提高10倍或更高的因素。
- en: The other thing that happened was that the internet made very large training
    sets available. Where researchers had been training classifiers with thousands
    of images before, now they had access to tens if not hundreds of millions of images.
    Combined with larger networks, neural networks had their chance to shine. This
    dominance has only continued in the succeeding years, with improved techniques
    and applications of neural networks to areas outside of image recognition, including
    translation, speech recognition, and image synthesis.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件事是互联网提供了非常大的训练集。在此之前，研究人员通常使用数千张图像来训练分类器，现在他们可以访问数千万甚至数亿张图像。结合更大的网络，神经网络有了展示自己的机会。这种主导地位在接下来的几年中持续存在，通过改进的技术和将神经网络应用于图像识别之外的领域，包括翻译、语音识别和图像合成。
- en: Why Now?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么现在？
- en: While the boom in computational power and better techniques led to an increase
    in interest in neural networks, we have also seen huge strides in *usability*.
    In particular, deep learning frameworks like TensorFlow, Theano, and Torch allow
    nonexperts to construct complex neural networks to solve their own machine learning
    problems. This has turned a task that used to require months or years of handcoding
    and head-on-table-banging effort (writing efficient GPU kernels is hard!) into
    something that anyone can do in an afternoon (or really a few days in practice).
    Increased usability has greatly increased the number of researchers who can work
    on deep learning problems. Frameworks like Keras with an even higher level of
    abstraction make it possible for anyone with a working knowledge of Python and
    some tools to run some interesting experiments, as this book will show.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管计算能力的激增和更好的技术导致了对神经网络的兴趣增加，但我们也看到了在*可用性*方面取得了巨大进步。特别是，像TensorFlow、Theano和Torch这样的深度学习框架允许非专家构建复杂的神经网络来解决他们自己的机器学习问题。这使得以前需要数月甚至数年的手工编码和头撞桌子的努力（编写高效的GPU内核很难！）的任务变成了任何人都可以在一个下午（或者实际上几天）完成的事情。增加的可用性极大地增加了可以处理深度学习问题的研究人员数量。像Keras这样的框架具有更高级别的抽象，使得任何具有Python工作知识和一些工具的人都可以运行一些有趣的实验，正如本书所示。
- en: A second important factor for “why now” is that large datasets have become available
    for everybody. Yes, Facebook and Google might still have the upper hand with access
    to billions of pictures, user comments, and what have you, but datasets with millions
    of items can be had from a variety of sources. In [Chapter 1](ch01.html#tools_techniques)
    we’ll look at a variety of options, and throughout the book the example code for
    each chapter will usually show in the first recipe how to get the needed training
    data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: “为什么现在”第二个重要因素是大型数据集已经对每个人都可用。是的，Facebook和Google可能仍然拥有访问数十亿张图片、用户评论等的优势，但数百万项数据集可以从各种来源获得。在[第1章](ch01.html#tools_techniques)中，我们将看看各种选择，在整本书中，每个章节的示例代码通常会在第一个配方中显示如何获取所需的训练数据。
- en: At the same time, private companies have started to produce and collect orders
    of magnitude more data, which has made the whole area of deep learning suddenly
    commercially very interesting. A model that can tell the difference between a
    cat and a dog is all very well, but a model that increases sales by 15% by taking
    all historic sales data into account can be the difference between life and death
    for a company.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，私营公司已经开始生产和收集数量级更多的数据，这使得整个深度学习领域突然变得商业上非常有趣。一个可以区分猫和狗的模型是很好的，但一个可以通过考虑所有历史销售数据使销售额增加15%的模型对于一家公司来说可能是生死攸关的区别。
- en: What Do You Need to Know?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你需要了解什么？
- en: 'These days there is a wide choice of platforms, technologies, and programming
    languages for deep learning. In this book all the examples are in Python and most
    of the code relies on the excellent Keras framework. The example code is available
    on GitHub as a set of Python notebooks, one per chapter. So, having a working
    knowledge of the following will help:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，深度学习有各种平台、技术和编程语言可供选择。在本书中，所有示例都是用Python编写的，大部分代码依赖于出色的Keras框架。示例代码作为一套Python笔记本在GitHub上可用，每章一个。因此，具有以下工作知识将有所帮助：
- en: Python
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: Python 3 is preferred, but Python 2.7 should also work. We use a variety of
    helper libraries that all can easily be installed using pip. The code is generally
    straightforward so even a relative novice should be able to follow the action.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首选Python 3，但Python 2.7也可以工作。我们使用各种辅助库，所有这些库都可以使用pip轻松安装。代码通常很简单，因此即使是相对新手也应该能够跟上操作。
- en: Keras
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Keras
- en: The heavy lifting for machine learning is done almost completely by Keras. Keras
    is an abstraction over either TensorFlow or Theano, both deep learning frameworks.
    Keras makes it easy to define neural networks in a very readable way. All code
    is tested against TensorFlow but should also work with Theano.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的大部分繁重工作几乎完全由Keras完成。Keras是TensorFlow或Theano的抽象，两者都是深度学习框架。Keras使得以一种非常可读的方式定义神经网络变得容易。所有代码都经过了TensorFlow的测试，但也应该可以与Theano一起工作。
- en: NumPy, SciPy, scikit-learn
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy、SciPy、scikit-learn
- en: These useful and extensive libraries are casually used in many recipes. Most
    of the time it should be clear what is happening from the context, but a quick
    read-up on them won’t hurt.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些有用且广泛的库在许多配方中随意使用。大多数情况下，从上下文中应该清楚发生了什么，但对它们进行快速了解也不会有害。
- en: Jupyter Notebook
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook
- en: Notebooks are a very nice way to share code; they allow for a mixture of code,
    output of code, and comments, all viewable in the browser.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本是分享代码的一种非常好的方式；它们允许在浏览器中查看代码、代码输出和注释的混合。
- en: 'Each chapter has a corresponding notebook that contains working code. The code
    in the book often leaves out details like imports, so it is a good idea to get
    the code from Git and launch a local notebook. First check out the code and enter
    the new directory:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每个章节都有一个包含工作代码的对应笔记本。书中的代码经常省略了导入等细节，因此最好从Git获取代码并启动本地笔记本。首先检出代码并进入新目录：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then set up a virtual environment for the project:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后为项目设置一个虚拟环境：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And install the dependencies:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 并安装依赖项：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you have a GPU and want to use that, you’ll need to uninstall `tensorflow`
    and install `tensorflow-gpu` instead, which you can easily do using pip:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有GPU并希望使用它，您需要卸载`tensorflow`并安装`tensorflow-gpu`，您可以使用pip轻松完成：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You’ll also need to have a compatible GPU library setup, which can be a bit
    of a hassle.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要设置一个兼容的GPU库，这可能有点麻烦。
- en: 'Finally, bring up the IPython notebook server:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，启动IPython笔记本服务器：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If everything worked, this should automatically open a web browser with an
    overview of the notebooks, one for each chapter. Feel free to play with the code;
    you can use Git to easily undo any changes you’ve made if you want to go back
    to the baseline:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，这应该会自动打开一个包含每章笔记本概述的网页浏览器。随意尝试代码；您可以使用Git轻松撤消您所做的任何更改，如果您想返回到基线：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first section of every chapter lists the notebooks relevant for that chapter
    and the notebooks are numbered according to the chapters, so it should in general
    be easy to find your way around. In the notebook folder, you’ll also find three
    other directories:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每章的第一部分列出了该章相关的笔记本，笔记本按章节编号，因此通常很容易找到您要找的内容。在笔记本文件夹中，您还会找到另外三个目录：
- en: Data
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据
- en: Contains data needed by the various notebooks—mostly samples of open datasets
    or things that would be too cumbersome to generate yourself.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 包含各种笔记本所需的数据，主要是开放数据集的样本或您自己生成会太繁琐的内容。
- en: Generated
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的
- en: Used to store intermediate data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用于存储中间数据。
- en: Zoo
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 动物园
- en: Contains a subdirectory for each chapter that holds saved models for that chapter.
    If you don’t have the time to actually train the models, you can still run the
    models by loading them from here.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 每章包含一个子目录，其中保存了该章节的已保存模型。如果您没有时间实际训练模型，仍然可以通过从这里加载模型来运行模型。
- en: How This Book Is Structured
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书结构
- en: '[Chapter 1](ch01.html#tools_techniques) provides in-depth information about
    how neural networks function, where to get data from, and how to preprocess that
    data to make it easier to consume. [Chapter 2](ch02.html#getting_unstuck) is about
    getting stuck and what to do about it. Neural nets are notoriously hard to debug
    and the tips and tricks in this chapter on how to make them behave will come in
    handy when going through the more project-oriented recipes in the rest of the
    book. If you are impatient, you can skip this chapter and go back to it later
    when you do get stuck.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](ch01.html#tools_techniques)提供了有关神经网络如何运作、从哪里获取数据以及如何预处理数据以便更容易消化的深入信息。[第2章](ch02.html#getting_unstuck)是关于遇到困难以及如何应对的内容。神经网络极其难以调试，本章中关于如何使其行为良好的技巧和窍门在阅读本书其余以项目为导向的配方时将会派上用场。如果您心急，可以跳过本章，等到遇到困难时再回来阅读。'
- en: Chapters [3](ch03.html#word_embeddings) through [15](ch15.html#deep_music) are
    grouped around media, starting with text processing, followed by image processing,
    and finally music processing in [Chapter 15](ch15.html#deep_music). Each chapter
    describes one project split into various recipes. Typically a chapter will start
    with a data acquisition recipe, followed by a few recipes that build toward the
    goal of the chapter and a recipe on data visualization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第[3](ch03.html#word_embeddings)章到第[15](ch15.html#deep_music)章围绕媒体进行分组，从文本处理开始，然后是图像处理，最后是[第15章](ch15.html#deep_music)中的音乐处理。每一章描述一个项目，分为各种配方。通常，一章将以数据获取配方开始，然后是几个配方，以实现章节目标，并包含一个数据可视化配方。
- en: '[Chapter 16](ch16.html#productionizing) is about using models in production.
    Running experiments in notebooks is great, but ultimately we want to share our
    results with actual users and get our models run on real servers or mobile devices.
    This chapter goes through the options.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[第16章](ch16.html#productionizing)是关于在生产中使用模型。在笔记本中运行实验很好，但最终我们希望与实际用户分享我们的结果，并在真实服务器或移动设备上运行我们的模型。本章介绍了各种选择。'
- en: Conventions Used in This Book
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书中使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用以下排版约定：
- en: '*Italic*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*斜体*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 指示新术语、URL、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`等宽字体`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序清单，以及在段落中引用程序元素，如变量或函数名称、数据库、数据类型、环境变量、语句和关键字。
- en: '*`Constant width italic`*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*`等宽斜体`*'
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 显示应替换为用户提供的值或由上下文确定的值的文本。
- en: Tip
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: This element signifies a tip or suggestion.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示提示或建议。
- en: Note
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This element signifies a general note.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示一般说明。
- en: Accompanying Code
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附带代码
- en: Each chapter in this book comes with one or more Python notebooks that contain
    the example code referred to in the chapters themselves. You can read the chapters
    without running the code, but it is more fun to work with the notebooks as you
    read. The code can be found at [*https://github.com/DOsinga/deep_learning_cookbook*](https://github.com/DOsinga/deep_learning_cookbook).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的每一章都附带一个或多个Python笔记本，其中包含章节中提到的示例代码。您可以阅读章节而不运行代码，但在阅读时使用笔记本会更有趣。代码可以在[*https://github.com/DOsinga/deep_learning_cookbook*](https://github.com/DOsinga/deep_learning_cookbook)找到。
- en: 'To get the example code for the recipes up and running, execute the following
    commands in a shell:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要使配方中的示例代码运行起来，请在shell中执行以下命令：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This book is here to help you get your job done. All code in the accompanying
    notebooks is licensed under the permissive Apache License 2.0.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在帮助您完成工作。所有附带笔记本中的代码都在宽松的Apache许可证2.0下许可。
- en: 'We appreciate, but do not require, attribution. An attribution usually includes
    the title, author, publisher, and ISBN. For example: “*Deep Learning Cookbook*
    by Douwe Osinga (O’Reilly). Copyright 2018 Douwe Osinga, 978-1-491-99584-6.”'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感激，但不要求归属。归属通常包括标题、作者、出版商和ISBN。例如：“*Deep Learning Cookbook* by Douwe Osinga
    (O’Reilly)。版权所有2018年Douwe Osinga，978-1-491-99584-6。”
- en: O’Reilly Safari
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O’Reilly Safari
- en: Note
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: '[*Safari*](http://oreilly.com/safari) (formerly Safari Books Online) is a membership-based
    training and reference platform for enterprise, government, educators, and individuals.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Safari*](http://oreilly.com/safari)（前身为Safari Books Online）是一个面向企业、政府、教育工作者和个人的基于会员制的培训和参考平台。'
- en: Members have access to thousands of books, training videos, Learning Paths,
    interactive tutorials, and curated playlists from over 250 publishers, including
    O’Reilly Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley
    Professional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press,
    Cisco Press, John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt,
    Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett,
    and Course Technology, among others.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 会员可以访问来自250多家出版商的数千本书籍、培训视频、学习路径、交互式教程和策划播放列表，包括O’Reilly Media、哈佛商业评论、Prentice
    Hall专业、Addison-Wesley专业、微软出版社、Sams、Que、Peachpit出版社、Adobe、Focal Press、思科出版社、约翰威利和儿子、Syngress、摩根考夫曼、IBM红皮书、Packt、Adobe出版社、FT出版社、Apress、Manning、新骑手、麦格劳希尔、琼斯和巴特利特以及课程技术等。
- en: For more information, please visit [*http://oreilly.com/safari*](http://oreilly.com/safari).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请访问[*http://oreilly.com/safari*](http://oreilly.com/safari)。
- en: How to Contact Us
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关本书的评论和问题发送给出版商：
- en: O’Reilly Media, Inc.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly Media, Inc.
- en: 1005 Gravenstein Highway North
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加利福尼亚州塞巴斯托波尔95472
- en: 800-998-9938 (in the United States or Canada)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-998-9938（美国或加拿大）
- en: 707-829-0515 (international or local)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0515（国际或本地）
- en: 707-829-0104 (fax)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104（传真）
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*http://bit.ly/deep-learning-cookbook*](http://bit.ly/deep-learning-cookbook).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这本书创建了一个网页，列出了勘误、示例和任何额外信息。您可以在[*http://bit.ly/deep-learning-cookbook*](http://bit.ly/deep-learning-cookbook)上访问此页面。
- en: To comment or ask technical questions about this book, send email to [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要就本书发表评论或提出技术问题，请发送电子邮件至[*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com)。
- en: For more information about our books, courses, conferences, and news, see our
    website at [*http://www.oreilly.com*](http://www.oreilly.com).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有关我们的书籍、课程、会议和新闻的更多信息，请访问我们的网站[*http://www.oreilly.com*](http://www.oreilly.com)。
- en: 'Find us on Facebook: [*http://facebook.com/oreilly*](http://facebook.com/oreilly)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在Facebook上找到我们：[*http://facebook.com/oreilly*](http://facebook.com/oreilly)
- en: 'Follow us on Twitter: [*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在Twitter上关注我们：[*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)
- en: 'Watch us on YouTube: [*http://www.youtube.com/oreillymedia*](http://www.youtube.com/oreillymedia)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上观看我们：[*http://www.youtube.com/oreillymedia*](http://www.youtube.com/oreillymedia)
- en: Acknowledgments
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: From academics sharing new ideas by (pre)publishing papers on [*https://arxiv.org*](https://arxiv.org),
    to hackers coding up those ideas on GitHub to public and private institutions
    publishing datasets for anybody to use, the world of machine learning is full
    of people and organizations that welcome newcomers and make it as easy to get
    started as it is. Open data, open source, and open access publishing—this book
    wouldn’t be here without machine learning’s culture of sharing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从学者在[*https://arxiv.org*](https://arxiv.org)上（预）发布论文分享新想法，到黑客在GitHub上编写这些想法的代码，再到公共和私人机构发布数据集供任何人使用，机器学习领域充满了欢迎新人的人和组织，使入门变得如此容易。开放数据、开源和开放获取出版——如果没有机器学习分享文化，这本书就不会存在。
- en: What is true for the ideas presented in this book is even more true for the
    code in this book. Writing a machine learning model from scratch is hard, so almost
    all the models in the notebooks are based on code from somewhere else. This is
    the best way to get things done—find a model that does something similar to what
    you want and change it step by step, verifying at each step that things still
    work.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书中所呈现的想法是真实的，这本书中的代码更是如此。从头开始编写一个机器学习模型很难，因此笔记本中的几乎所有模型都基于其他地方的代码。这是完成任务的最佳方式——找到一个与您想要的类似的模型，并逐步更改它，确保每一步都仍然有效。
- en: A special thanks goes out to my friend and coauthor for this book, Russell Power.
    Apart from helping to write this Preface, [Chapter 6](ch06.html#question_matching),
    and [Chapter 7](ch07.html#suggest_emojis), he has been instrumental in checking
    the technical soundness of the book and the accompanying code. Moreover, he’s
    been an invaluable asset as a sounding board for many ideas, some of which made
    it into the book.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢我的朋友和这本书的合著者Russell Power。除了帮助撰写这篇序言，[第6章](ch06.html#question_matching)和[第7章](ch07.html#suggest_emojis)，他在检查书籍和相关代码的技术可靠性方面发挥了重要作用。此外，他作为许多想法的
    sounding board，对于一些想法，其中一些已经被纳入了书中，他是一个宝贵的资产。
- en: Then there is my lovely wife, who was the first line of defense when it came
    to proofreading chapters as they came into being. She has an uncanny ability to
    spot mistakes in a text that is neither in her native language nor about a subject
    she’s previously been an expert on.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是我的可爱妻子，她是第一道防线，当章节逐渐形成时，她负责校对。她有一种神奇的能力，能够发现文本中的错误，即使这些错误不是她的母语，也不是她之前是专家的主题。
- en: The *requirements.in* file lists the open source packages that are used in this
    book. A heartfelt thank you goes out to all the contributors to all of these projects.
    This goes doubly for Keras, since almost all the code is based on that framework
    and often borrows from its examples.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*requirements.in*文件列出了本书中使用的开源软件包。衷心感谢所有这些项目的所有贡献者。对于Keras，感谢更加深刻，因为几乎所有的代码都是基于该框架的，并且经常借鉴其示例。'
- en: 'Example code and ideas from these packages and many blog posts contributed
    to this book. In particular:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的例子代码和想法来自这些软件包和许多博客文章。特别是：
- en: '[Chapter 2, *Getting Unstuck*](ch02.html#getting_unstuck)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[第2章，*摆脱困境*]'
- en: This chapter takes ideas from Slav Ivanov’s blog post [“37 Reasons Why Your
    Neural Network Is Not Working”](http://bit.ly/2IDxljz).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章节借鉴了Slav Ivanov的博客文章[“37 Reasons Why Your Neural Network Is Not Working”]。
- en: '[Chapter 3, *Calculating Text Similarity Using Word Embeddings*](ch03.html#word_embeddings)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章，*使用词嵌入计算文本相似性*]'
- en: Thanks to Google for publishing its Word2vec model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢Google发布其Word2vec模型。
- en: Radim Řehůřek’s Gensim powers this chapter, and some of the code is based on
    examples from this great project.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Radim Řehůřek的Gensim支持本章，部分代码基于这个伟大项目的示例。
- en: '[Chapter 5, *Generating Text in the Style of an Example Text*](ch05.html#text_generation)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章，*生成文本的风格*]'
- en: This chapter draws heavily on the great blog post [“The Unreasonable Effectiveness
    of Recurrent Neural Networks”](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    by Andrej Karpathy. That blog post rekindled my interest in neural networks.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章节在很大程度上借鉴了Andrej Karpathy的博客文章[“递归神经网络的不合理有效性”]。那篇博客文章重新激发了我对神经网络的兴趣。
- en: The visualization was inspired by Motoki Wu’s [“Visualizations of Recurrent
    Neural Networks”](http://bit.ly/2s8uAvg).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化灵感来自Motoki Wu的[“递归神经网络的可视化”]。
- en: '[Chapter 6, *Question Matching*](ch06.html#question_matching)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章，*问题匹配*]'
- en: This chapter was somewhat inspired by [the Quora Question Pairs challenge on
    Kaggle](https://www.kaggle.com/c/quora-question-pairs).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章节在一定程度上受到了Kaggle上Quora Question Pairs挑战的启发。
- en: '[Chapter 8, *Sequence-to-Sequence Mapping*](ch08.html#seq2seq_mapping)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章，*序列到序列映射*]'
- en: The example code is copied from one of the Keras examples, but applied on a
    slightly different dataset.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码是从Keras的一个示例中复制的，但应用在一个稍微不同的数据集上。
- en: '[Chapter 11, *Detecting Multiple Images*](ch11.html#multiple_images)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[第11章，*检测多个图像*]'
- en: This chapter is based on Yann Henon’s [*keras_frcnn*](https://github.com/yhenon/keras-frcnn).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章基于Yann Henon的[*keras_frcnn*]。
- en: '[Chapter 12, *Image Style*](ch12.html#image_style)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[第12章，*图像风格*]'
- en: This borrows from [“How Convolutional Neural Networks See the World”](http://bit.ly/2s4ORCf)
    and of course Google’s [DeepDream](https://github.com/google/deepdream/blob/master/dream.ipynb).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这借鉴了[“How Convolutional Neural Networks See the World”]和当然也包括Google的[DeepDream]。
- en: '[Chapter 13, *Generating Images with Autoencoders*](ch13.html#autoencoders)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[第13章，*使用自动编码器生成图像*]'
- en: Code and ideas are based on Nicholas Normandin’s [Conditional Variational Autoencoder](http://nnormandin.com/science/2017/07/01/cvae.html).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 代码和想法基于Nicholas Normandin的[条件变分自动编码器]。
- en: '[Chapter 14, *Generating Icons Using Deep Nets*](ch14.html#generating_icons)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[第14章，*使用深度网络生成图标*]'
- en: Autoencoder training code for Keras is based on Qin Yongliang’s [DCGAN-Keras](https://github.com/ctmakro/DCGAN-Keras/blob/master/lets_gan.py).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的自动编码器训练代码基于Qin Yongliang的[DCGAN-Keras]。
- en: '[Chapter 15, *Music and Deep Learning*](ch15.html#deep_music)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[第15章，*音乐和深度学习*]'
- en: This was inspired by Heitor Guimarães’s [*gtzan.keras*](https://github.com/Hguimaraes/gtzan.keras).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这受到了Heitor Guimarães的[*gtzan.keras*]的启发。
