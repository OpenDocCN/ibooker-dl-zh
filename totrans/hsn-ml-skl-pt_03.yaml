- en: Chapter 2\. End-to-End Machine Learning Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter you will work through an example project end to end, pretending
    to be a recently hired data scientist at a real estate company. This example is
    fictitious; the goal is to illustrate the main steps of a machine learning project,
    not to learn anything about the real estate business. Here are the main steps
    we will walk through:'
  prefs: []
  type: TYPE_NORMAL
- en: Look at the big picture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore and visualize the data to gain insights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the data for machine learning algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a model and train it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tune your model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Present your solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch, monitor, and maintain your system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Working with Real Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you are learning about machine learning, it is best to experiment with
    real-world data, not artificial datasets. Fortunately, there are thousands of
    open datasets to choose from, ranging across all sorts of domains. Here are a
    few popular open data repositories you can use to get data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Google Datasets Search](https://datasetsearch.research.google.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hugging Face Datasets](https://huggingface.co/docs/datasets)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenML.org](https://openml.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kaggle.com](https://kaggle.com/datasets)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stanford Large Network Dataset Collection](https://snap.stanford.edu/data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Amazon’s AWS datasets](https://registry.opendata.aws)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[U.S. Government’s Open Data](https://data.gov)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DataPortals.org](https://dataportals.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Wikipedia’s list of machine learning datasets](https://homl.info/9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter we’ll use the California Housing Prices dataset from the StatLib
    repository⁠^([1](ch02.html#id990)) (see [Figure 2-1](#california_housing_prices_plot)).
    This dataset is based on data from the 1990 California census. It is not exactly
    recent (a nice house in the Bay Area was still affordable at the time), but it
    has many qualities for learning, so we will pretend it is recent data. For teaching
    purposes I’ve added a categorical attribute and removed a few features.
  prefs: []
  type: TYPE_NORMAL
- en: '![Map of California displaying housing price data with colored dots representing
    median house values and dot size indicating population density.](assets/hmls_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. California housing prices
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Look at the Big Picture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the Machine Learning Housing Corporation! Your first task is to use
    California census data to build a model of housing prices in the state. This data
    includes metrics such as the population, median income, and median housing price
    for each block group in California. Block groups are the smallest geographical
    unit for which the US Census Bureau publishes sample data (a block group typically
    has a population of 600 to 3,000 people). I will call them “districts” for short.
  prefs: []
  type: TYPE_NORMAL
- en: Your model should learn from this data and be able to predict the median housing
    price in any district, given all the other metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since you are a well-organized data scientist, the first thing you should do
    is pull out your machine learning project checklist. You can start with the one
    at [*https://homl.info/checklist*](https://homl.info/checklist); it should work
    reasonably well for most machine learning projects, but make sure to adapt it
    to your needs. In this chapter we will go through many checklist items, but we
    will also skip a few, either because they are self-explanatory or because they
    will be discussed in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Frame the Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first question to ask your boss is what exactly the business objective is.
    Building a model is probably not the end goal. How does the company expect to
    use and benefit from this model? Knowing the objective is important because it
    will determine how you frame the problem, which algorithms you will select, which
    performance measure you will use to evaluate your model, and how much effort you
    will spend tweaking it.
  prefs: []
  type: TYPE_NORMAL
- en: Your boss answers that your model’s output (a prediction of a district’s median
    housing price) will be essential to determine whether it is worth investing in
    a given area. More specifically, your model’s output will be fed to another machine
    learning system (see [Figure 2-2](#house_pricing_pipeline_diagram)), along with
    some other signals.⁠^([2](ch02.html#id994)) So it’s important to make our housing
    price model as accurate as we can.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question to ask your boss is what the current solution looks like
    (if any). The current situation will often give you a reference for performance,
    as well as insights on how to solve the problem. Your boss answers that the district
    housing prices are currently estimated manually by experts: a team gathers up-to-date
    information about a district, and when they cannot get the median housing price,
    they estimate it using complex rules.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing a machine learning pipeline for real estate, highlighting
    data flow from district data to district pricing, investment analysis, and investments.](assets/hmls_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. A machine learning pipeline for real estate investments
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is costly and time-consuming, and their estimates are not great; in cases
    where they manage to find out the actual median housing price, they often realize
    that their estimates were off by more than 30%. This is why the company thinks
    that it would be useful to train a model to predict a district’s median housing
    price, given other data about that district. The census data looks like a great
    dataset to exploit for this purpose, since it includes the median housing prices
    of thousands of districts, as well as other data.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all this information, you are now ready to start designing your system.
    First, determine what kind of training supervision the model will need: is it
    a supervised, unsupervised, semi-supervised, self-supervised, or reinforcement
    learning task? And is it a classification task, a regression task, or something
    else? Should you use batch learning or online learning techniques? Before you
    read on, pause and try to answer these questions for yourself.'
  prefs: []
  type: TYPE_NORMAL
- en: Have you found the answers? Let’s see. This is clearly a typical supervised
    learning task, since the model can be trained with *labeled* examples (each instance
    comes with the expected output, i.e., the district’s median housing price). It
    is a typical regression task, since the model will be asked to predict a value.
    More specifically, this is a *multiple regression* problem, since the system will
    use multiple features to make a prediction (the district’s population, the median
    income, etc.). It is also a *univariate regression* problem, since we are only
    trying to predict a single value for each district. If we were trying to predict
    multiple values per district, it would be a *multivariate regression* problem.
    Finally, there is no continuous flow of data coming into the system, there is
    no particular need to adjust to changing data rapidly, and the data is small enough
    to fit in memory, so plain batch learning should do just fine.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the data were huge, you could either split your batch learning work across
    multiple servers (using the MapReduce technique) or use an online learning technique.
  prefs: []
  type: TYPE_NORMAL
- en: Select a Performance Measure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your next step is to select a performance measure. A typical performance measure
    for regression problems is the *root mean squared error* (RMSE). It gives an idea
    of how much error the system typically makes in its predictions, with a higher
    weight given to large errors. [Equation 2-1](#rmse_equation) shows the mathematical
    formula to compute the RMSE.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2-1\. Root mean squared error (RMSE)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $RMSE left-parenthesis bold upper X comma bold y comma h right-parenthesis equals
    StartRoot StartFraction 1 Over m EndFraction sigma-summation Underscript i equals
    1 Overscript m Endscripts left-parenthesis h left-parenthesis bold x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis minus y Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis squared EndRoot$
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the RMSE is generally the preferred performance measure for regression
    tasks, in some contexts you may prefer to use another function, especially when
    there are many outliers in the data, as the RMSE is quite sensitive to them. In
    that case, you may consider using the *mean absolute error* (MAE, also called
    the *average absolute deviation*), shown in [Equation 2-2](#mae_equation):'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2-2\. Mean absolute error (MAE)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $MAE left-parenthesis bold upper X comma bold y comma h right-parenthesis equals
    StartFraction 1 Over m EndFraction sigma-summation Underscript i equals 1 Overscript
    m Endscripts StartAbsoluteValue h left-parenthesis bold x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis minus y Superscript left-parenthesis
    i right-parenthesis Baseline EndAbsoluteValue$
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the RMSE and the MAE are ways to measure the distance between two vectors:
    the vector of predictions and the vector of target values. Various distance measures,
    or *norms*, are possible:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing the root of a sum of squares (RMSE) corresponds to the *Euclidean
    norm*: this is the notion of distance we are all familiar with. It is also called
    the ℓ[2] *norm*, denoted ∥ · ∥[2] (or just ∥ · ∥).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the sum of absolutes (MAE) corresponds to the ℓ[1] *norm*, denoted
    ∥ · ∥[1]. This is sometimes called the *Manhattan norm* because it measures the
    distance between two points in a city if you can only travel along orthogonal
    city blocks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More generally, the ℓ[*k*] *norm* of a vector **v** containing *n* elements
    is defined as ∥**v**∥[*k*] = (|*v*[1]|^(*k*) + |*v*[2]|^(*k*) + ... + |*v*[*n*]|^(*k*))^(1/*k*).
    ℓ[0] gives the number of nonzero elements in the vector, and ℓ[∞] gives the maximum
    absolute value in the vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The higher the norm index, the more it focuses on large values and neglects
    small ones. This is why the RMSE is more sensitive to outliers than the MAE. But
    when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs
    very well and is generally preferred.
  prefs: []
  type: TYPE_NORMAL
- en: Check the Assumptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lastly, it is good practice to list and verify the assumptions that have been
    made so far (by you or others); this can help you catch serious issues early on.
    For example, the district prices that your system outputs are going to be fed
    into a downstream machine learning system, and you assume that these prices are
    going to be used as such. But what if the downstream system converts the prices
    into categories (e.g., “cheap”, “medium”, or “expensive”) and then uses those
    categories instead of the prices themselves? In this case, getting the price perfectly
    right is not important at all; your system just needs to get the category right.
    If that’s so, then the problem should have been framed as a classification task,
    not a regression task. You don’t want to find this out after working on a regression
    system for months.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, after talking with the team in charge of the downstream system,
    you are confident that they do indeed need the actual prices, not just categories.
    Great! You’re all set, the lights are green, and you can start coding now!
  prefs: []
  type: TYPE_NORMAL
- en: Get the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s time to get your hands dirty. Don’t hesitate to pick up your laptop and
    walk through the code examples. As I mentioned in the preface, all the code examples
    in this book are open source and available [online](https://github.com/ageron/handson-mlp)
    as Jupyter notebooks, which are interactive documents containing text, images,
    and executable code snippets (Python in our case). In this book I will assume
    you are running these notebooks on Google Colab, a free service that lets you
    run any Jupyter notebook directly online, without having to install anything on
    your machine. If you want to use another online platform (e.g., Kaggle) or if
    you want to install everything locally on your own machine, please see the instructions
    on the book’s GitHub page.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code Examples Using Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, open a web browser and visit [*https://homl.info/colab-p*](https://homl.info/colab-p):
    this will lead you to Google Colab, and it will display the list of Jupyter notebooks
    for this book (see [Figure 2-3](#google_colab_notebook_list)). You will find one
    notebook per chapter, plus a few extra notebooks and tutorials for NumPy, Matplotlib,
    Pandas, linear algebra, and differential calculus. For example, if you click *02_end_to_end_machine_learning_project.ipynb*,
    the notebook from [Chapter 2](#project_chapter) will open up in Google Colab (see
    [Figure 2-4](#notebook_in_colab)).'
  prefs: []
  type: TYPE_NORMAL
- en: A Jupyter notebook is composed of a list of cells. Each cell contains either
    executable code or text. Try double-clicking the first text cell (which contains
    the sentence “Welcome to Machine Learning Housing Corp.!”). This will open the
    cell for editing. Notice that Jupyter notebooks use Markdown syntax for formatting
    (e.g., `**bold**`, `*italics*`, `# Title`, `[url](link text)`, and so on). Try
    modifying this text, then press Shift-Enter to see the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![Google Colab interface showing a list of Jupyter notebooks in the "ageron/handson-mlp"
    repository on GitHub, with "02_end_to_end_machine_learning_project.ipynb" highlighted.](assets/hmls_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. List of notebooks in Google Colab
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Screenshot of a Google Colab notebook showing a section titled "Chapter 2
    – End-to-end Machine Learning project," with instructions for editing and running
    text and code cells.](assets/hmls_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Your notebook in Google Colab
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next, create a new code cell by selecting Insert → “Code cell” from the menu.
    Alternatively, you can click the + Code button in the toolbar, or hover your mouse
    over the bottom of a cell until you see + Code and + Text appear, then click +
    Code. In the new code cell, type some Python code, such as `print("Hello World")`,
    then press Shift-Enter to run this code (or click the ▷ button on the left side
    of the cell).
  prefs: []
  type: TYPE_NORMAL
- en: If you’re not logged in to your Google account, you’ll be asked to log in now
    (if you don’t already have a Google account, you’ll need to create one). Once
    you are logged in, when you try to run the code you’ll see a security warning
    telling you that this notebook was not authored by Google. A malicious person
    could create a notebook that tries to trick you into entering your Google credentials
    so they can access your personal data, so before you run a notebook, always make
    sure you trust its author (or double-check what each code cell will do before
    running it). Assuming you trust me (or you plan to check every code cell), you
    can now click “Run anyway”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Colab will then allocate a new *runtime* for you: this is a free virtual machine
    located on Google’s servers that contains a bunch of tools and Python libraries,
    including everything you’ll need for most chapters (in some chapters, you’ll need
    to run a command to install additional libraries). This will take a few seconds.
    Next, Colab will automatically connect to this runtime and use it to execute your
    new code cell. Importantly, the code runs on the runtime, *not* on your machine.
    The code’s output will be displayed under the cell. Congrats, you’ve run some
    Python code on Colab!'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To insert a new code cell, you can also type Ctrl-M (or Cmd-M on macOS) followed
    by A (to insert above the active cell) or B (to insert below). There are many
    other keyboard shortcuts available: you can view and edit them by typing Ctrl-M
    (or Cmd-M) then H. If you choose to run the notebooks on Kaggle or on your own
    machine using JupyterLab or an IDE such as Visual Studio Code with the Jupyter
    extension, you will see some minor differences—runtimes are called *kernels*,
    the user interface and keyboard shortcuts are slightly different, etc.—but switching
    from one Jupyter environment to another is not too hard.'
  prefs: []
  type: TYPE_NORMAL
- en: Saving Your Code Changes and Your Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can make changes to a Colab notebook, and they will persist for as long
    as you keep your browser tab open. But once you close it, the changes will be
    lost. To avoid this, make sure you save a copy of the notebook to your Google
    Drive by selecting File → “Save a copy in Drive”. Alternatively, you can download
    the notebook to your computer by selecting File → Download → “Download .ipynb”.
    Then you can later visit [*https://colab.research.google.com*](https://colab.research.google.com)
    and open the notebook again (either from Google Drive or by uploading it from
    your computer).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Google Colab is meant only for interactive use: you can play around in the
    notebooks and tweak the code as you like, but you cannot let the notebooks run
    unattended for a long period of time, or else the runtime will be shut down and
    all of its data will be lost.'
  prefs: []
  type: TYPE_NORMAL
- en: If the notebook generates data that you care about, make sure you download this
    data before the runtime shuts down. To do this, click the Files icon (see step
    1 in [Figure 2-5](#save_data_google_colab)), find the file you want to download,
    click the vertical dots next to it (step 2), and click Download (step 3). Alternatively,
    you can mount your Google Drive on the runtime, allowing the notebook to read
    and write files directly to Google Drive as if it were a local directory. For
    this, click the Files icon (step 1), then click the Google Drive icon (circled
    in [Figure 2-5](#save_data_google_colab)) and follow the on-screen instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of Google Colab interface showing steps to download a file or
    mount Google Drive, with icons and menu options highlighted.](assets/hmls_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. Downloading a file from a Google Colab runtime (steps 1 to 3),
    or mounting your Google Drive (circled icon)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By default, your Google Drive will be mounted at */content/drive/MyDrive*.
    If you want to back up a data file, simply copy it to this directory by running
    `!cp [.keep-together]#/content/my_great_model /content/drive/MyDrive`.# Any command
    starting with a bang (`!`) is treated as a shell command, not as Python code:
    `cp` is the Linux shell command to copy a file from one path to another. Note
    that Colab runtimes run on Linux (specifically, Ubuntu).'
  prefs: []
  type: TYPE_NORMAL
- en: The Power and Danger of Interactivity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Jupyter notebooks are interactive, and that’s a great thing: you can run each
    cell one by one, stop at any point, insert a cell, play with the code, go back
    and run the same cell again, etc., and I highly encourage you to do so. If you
    just run the cells one by one without ever playing around with them, you won’t
    learn as fast. However, this flexibility comes at a price: it’s very easy to run
    cells in the wrong order, or to forget to run a cell. If this happens, the subsequent
    code cells are likely to fail. For example, the very first code cell in each notebook
    contains setup code (such as imports), so make sure you run it first, or else
    nothing will work.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you ever run into a weird error, try restarting the runtime (by selecting
    Runtime → “Restart runtime” from the menu) and then run all the cells again from
    the beginning of the notebook. This often solves the problem. If not, it’s likely
    that one of the changes you made broke the notebook: just revert to the original
    notebook and try again. If it still fails, please file an issue on GitHub.'
  prefs: []
  type: TYPE_NORMAL
- en: Book Code Versus Notebook Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may sometimes notice some little differences between the code in this book
    and the code in the notebooks. This may happen for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A library may have changed slightly by the time you read these lines, or perhaps
    despite my best efforts I made an error in the book. Sadly, I cannot magically
    fix the code in your copy of this book (unless you are reading an electronic copy
    and you can download the latest version), but I *can* fix the notebooks. So, if
    you run into an error after copying code from this book, please look for the fixed
    code in the notebooks: I will strive to keep them error-free and up-to-date with
    the latest library versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The notebooks contain some extra code to beautify the figures (adding labels,
    setting font sizes, etc.) and to save them in high resolution for this book. You
    can safely ignore this extra code if you want.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I optimized the code for readability and simplicity: I made it as linear and
    flat as possible, defining very few functions or classes. The goal is to ensure
    that the code you are running is generally right in front of you, and not nested
    within several layers of abstractions that you have to search through. This also
    makes it easier for you to play with the code. For simplicity, there’s limited
    error handling, and I placed some of the least common imports right where they
    are needed (instead of placing them at the top of the file, as is recommended
    by the PEP 8 Python style guide). That said, your production code will not be
    very different: just a bit more modular, and with additional tests and error handling.'
  prefs: []
  type: TYPE_NORMAL
- en: OK! Once you’re comfortable with Colab, you’re ready to download the data.
  prefs: []
  type: TYPE_NORMAL
- en: Download the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In typical environments your data would be available in a relational database
    or some other common data store, and spread across multiple tables/documents/files.
    To access it, you would first need to get your credentials and access authorizations
    and familiarize yourself with the data schema.⁠^([4](ch02.html#id1032)) In this
    project, however, things are much simpler: you will just download a single compressed
    file, *housing.tgz*, which contains a comma-separated values (CSV) file called
    *housing.csv* with all the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than manually downloading and decompressing the data, it’s usually preferable
    to write a function that does it for you. This is useful in particular if the
    data changes regularly: you can write a small script that uses the function to
    fetch the latest data (or you can set up a scheduled job to do that automatically
    at regular intervals). Automating the process of fetching the data is also useful
    if you need to install the dataset on multiple machines.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the function to fetch and load the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you get an SSL `CERTIFICATE_VERIFY_FAILED` error on macOS, then you most
    likely need to install the `certifi` package, as explained at [*https://homl.info/sslerror*](https://homl.info/sslerror).
  prefs: []
  type: TYPE_NORMAL
- en: When `load_housing_data()` is called, it looks for the *datasets/housing.tgz*
    file. If it does not find it, it creates the *datasets* directory inside the current
    directory (which is */content* by default, in Colab), downloads the *housing.tgz*
    file from the *ageron/data* GitHub repository, and extracts its content into the
    *datasets* directory; this creates the *datasets*/*housing* directory with the
    *housing.csv* file inside it. Lastly, the function loads this CSV file into a
    Pandas DataFrame object containing all the data, and returns it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you are using Python 3.12 or 3.13, you should add `filter=''data''` to the
    `extractall()` method’s arguments: this limits what the extraction algorithm can
    do and improves security (see the documentation for more details).'
  prefs: []
  type: TYPE_NORMAL
- en: Take a Quick Look at the Data Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You start by looking at the top five rows of data using the DataFrame’s `head()`
    method (see [Figure 2-6](#housing_head_screenshot)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot showing the top five rows of a housing dataset, including attributes
    such as longitude, latitude, housing median age, median income, ocean proximity,
    and median house value.](assets/hmls_0206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. Top five rows in the dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Each row represents one district. There are 10 attributes (they are not all
    shown in the screenshot): `longitude`, `latitude`, `housing_median_age`, `total_rooms`,
    `total_bedrooms`, `population`, `households`, `median_income`, `median_house_value`,
    and `ocean_proximity`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `info()` method is useful to get a quick description of the data, in particular
    the total number of rows, each attribute’s type, and the number of non-null values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this book, when a code example contains a mix of code and outputs, as is
    the case here, it is formatted like in the Python interpreter for better readability:
    the code lines are prefixed with `>>>` (or `...` for indented blocks), and the
    outputs have no prefix.'
  prefs: []
  type: TYPE_NORMAL
- en: There are 20,640 instances in the dataset, which means that it is fairly small
    by machine learning standards, but it’s perfect to get started. You notice that
    the `total_bedrooms` attribute has only 20,433 non-null values, meaning that 207
    districts are missing this feature. You will need to take care of this later.
  prefs: []
  type: TYPE_NORMAL
- en: 'All attributes are numerical, except for `ocean_proximity`. Its type is `object`,
    so it could hold any kind of Python object. But since you loaded this data from
    a CSV file, you know that it must be a text attribute. When you looked at the
    top five rows, you probably noticed that the values in the `ocean_proximity` column
    were repetitive, which means that it is probably a categorical attribute. You
    can find out what categories exist and how many districts belong to each category
    by using the `value_counts()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the other fields. The `describe()` method shows a summary of the
    numerical attributes ([Figure 2-7](#housing_describe_screenshot)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A table generated by the `describe()` method, displaying statistical summaries
    for numerical attributes such as count, mean, standard deviation, minimum, maximum,
    and percentiles.](assets/hmls_0207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. Summary of each numerical attribute
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `count`, `mean`, `min`, and `max` rows are self-explanatory. Note that
    the null values are ignored (so, for example, the `count` of `total_bedrooms`
    is 20,433, not 20,640). The `std` row shows the *standard deviation*, which measures
    how dispersed the values are.⁠^([5](ch02.html#id1038)) The `25%`, `50%`, and `75%`
    rows show the corresponding *percentiles*: a percentile indicates the value below
    which a given percentage of observations in a group of observations fall. For
    example, 25% of the districts have a `housing_median_age` lower than 18, while
    50% are lower than 29, and 75% are lower than 37\. These are often called the
    25th percentile (or first *quartile*), the median, and the 75th percentile (or
    third quartile).'
  prefs: []
  type: TYPE_NORMAL
- en: Another quick way to get a feel of the type of data you are dealing with is
    to plot a histogram for each numerical attribute. A histogram shows the number
    of instances (on the vertical axis) that have a given value range (on the horizontal
    axis). You can either plot this one attribute at a time, or you can call the `hist()`
    method on the whole dataset (as shown in the following code example), and it will
    plot a histogram for each numerical attribute (see [Figure 2-8](#attribute_histogram_plots)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Histograms displaying the distribution of various numerical attributes such
    as longitude, latitude, housing median age, total rooms, total bedrooms, population,
    households, median income, and median house value.](assets/hmls_0208.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-8\. A histogram for each numerical attribute
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The number of value ranges can be adjusted using the `bins` argument (try playing
    with it to see how it affects the histograms):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at these histograms, you notice a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the median income attribute does not look like it is expressed in US
    dollars (USD). After checking with the team that collected the data, you are told
    that the data has been scaled and capped at 15 (actually, 15.0001) for higher
    median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers
    represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000).
    Working with preprocessed attributes is common in machine learning, and it is
    not necessarily a problem, but you should try to understand how the data was computed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The housing median age and the median house value were also capped. The latter
    may be a serious problem since it is your target attribute (your labels). Your
    machine learning algorithms may learn that prices never go beyond that limit.
    You need to check with your client team (the team that will use your system’s
    output) to see if this is a problem or not. If they tell you that they need precise
    predictions even beyond $500,000, then you have two options:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collect proper labels for the districts whose labels were capped.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove those districts from the training set (and also from the test set, since
    your system should not be evaluated poorly if it predicts values beyond $500,000).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These attributes have very different scales. We will discuss this later in this
    chapter when we explore feature scaling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, many histograms are *skewed right*: they extend much farther to the
    right of the median than to the left. This may make it a bit harder for some machine
    learning algorithms to detect patterns. Later, you’ll try transforming these attributes
    to have more symmetrical and bell-shaped distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should now have a better understanding of the kind of data you’re dealing
    with.
  prefs: []
  type: TYPE_NORMAL
- en: Create a Test Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you look at the data any further, you need to create a test set, put
    it aside, and never look at it. It may seem strange to voluntarily set aside part
    of the data at this stage. After all, you have only taken a quick glance at the
    data, and surely you should learn a whole lot more about it before you decide
    what algorithms to use, right? This is true, but your brain is an amazing pattern
    detection system, which also means that it is highly prone to overfitting: if
    you look at the test set, you may stumble upon some seemingly interesting pattern
    in the test data that leads you to select a particular kind of machine learning
    model. When you estimate the generalization error using the test set, your estimate
    will be too optimistic, and you will launch a system that will not perform as
    well as expected. This is called *data snooping* bias.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a test set is theoretically simple; pick some instances randomly,
    typically 20% of the dataset (or less if your dataset is very large), and set
    them aside:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then use this function like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, this works, but it is not perfect: if you run the program again, it will
    generate a different test set! Over time, you (or your machine learning algorithms)
    will get to see the whole dataset, which is what you want to avoid.'
  prefs: []
  type: TYPE_NORMAL
- en: One solution is to save the test set on the first run and then load it in subsequent
    runs. Another option is to set the random number generator’s seed (e.g., by passing
    `seed=42` to the `default_rng()` function)⁠^([6](ch02.html#id1054)) to ensure
    it always generates the same sequence of random numbers every time you run the
    program.
  prefs: []
  type: TYPE_NORMAL
- en: However, both these solutions will break the next time you fetch an updated
    dataset. To have a stable train/test split even after updating the dataset, a
    common solution is to use each instance’s identifier to decide whether it should
    go in the test set (assuming instances have unique and immutable identifiers).
    For example, you could compute a hash of each instance’s identifier and put that
    instance in the test set if the hash is lower than or equal to 20% of the maximum
    hash value. This ensures that the test set will remain consistent across multiple
    runs, even if you refresh the dataset. The new test set will contain 20% of the
    new instances, but it will not contain any instance that was previously in the
    training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a possible implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, the housing dataset does not have an identifier column. The
    simplest solution is to use the row index as the ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you use the row index as a unique identifier, you need to make sure that
    new data gets appended to the end of the dataset and that no row ever gets deleted.
    If this is not possible, then you can try to use the most stable features to build
    a unique identifier. For example, a district’s latitude and longitude are guaranteed
    to be stable for a few million years, so you could combine them into an ID like
    so:⁠^([7](ch02.html#id1055))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Scikit-Learn provides a few functions to split datasets into multiple subsets
    in various ways. The simplest function is `train_test_split()`, which does pretty
    much the same thing as the `shuffle_and_split_data()` function we defined earlier,
    with a couple of additional features. First, there is a `random_state` parameter
    that allows you to set the random generator seed. Second, you can pass it multiple
    datasets with an identical number of rows, and it will split them on the same
    indices (this is very useful, for example, if you have a separate DataFrame for
    labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'So far we have considered purely random sampling methods. This is generally
    fine if your dataset is large enough (especially relative to the number of attributes),
    but if it is not, you run the risk of introducing a significant sampling bias.
    When employees at a survey company decide to call 1,000 people to ask them a few
    questions, they don’t just pick 1,000 people randomly in a phone book. They try
    to ensure that these 1,000 people are representative of the whole population,
    with regard to the questions they want to ask. For example, according to the US
    Census Bureau, 51.6% of citizens of voting age are female, while 48.4% are male,
    so a well-conducted survey in the US would try to maintain this ratio in the sample:
    516 females and 484 males (at least if it seems likely that the answers may vary
    across genders). This is called *stratified sampling*: the population is divided
    into homogeneous subgroups called *strata*, and the right number of instances
    are sampled from each stratum to guarantee that the test set is representative
    of the overall population. If the people running the survey used purely random
    sampling, there would be over 10% chance of sampling a skewed test set with less
    than 49% female or more than 54% female participants. Either way, the survey results
    would likely be quite biased.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you’ve chatted with some experts who told you that the median income
    is a very important attribute to predict median housing prices. You may want to
    ensure that the test set is representative of the various categories of incomes
    in the whole dataset. Since the median income is a continuous numerical attribute,
    you first need to create an income category attribute. Let’s look at the median
    income histogram more closely (back in [Figure 2-8](#attribute_histogram_plots)):
    most median income values are clustered around 1.5 to 6 (i.e., $15,000–$60,000),
    but some median incomes go far beyond 6\. It is important to have a sufficient
    number of instances in your dataset for each stratum, or else the estimate of
    a stratum’s importance may be biased. This means that you should not have too
    many strata, and each stratum should be large enough. The following code uses
    the `pd.cut()` function to create an income category attribute with five categories
    (labeled from 1 to 5); category 1 ranges from 0 to 1.5 (i.e., less than $15,000),
    category 2 from 1.5 to 3, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'These income categories are represented in [Figure 2-9](#housing_income_cat_bar_plot):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now you are ready to do stratified sampling based on the income category. Scikit-Learn
    provides a number of splitter classes in the `sklearn.model_selection` package
    that implement various strategies to split your dataset into a training set and
    a test set. Each splitter has a `split()` method that returns an iterator over
    different training/test splits of the same data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Bar chart illustrating the distribution of districts across five income categories,
    with categories two and three having the highest counts.](assets/hmls_0209.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-9\. Histogram of income categories
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To be precise, the `split()` method yields the training and test *indices*,
    not the data itself. Having multiple splits can be useful if you want to better
    estimate the performance of your model, as you will see when we discuss cross-validation
    later in this chapter. For example, the following code generates 10 different
    stratified splits of the same dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For now, you can just use the first split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, since stratified sampling is fairly common, there’s a shorter way to get
    a single split using the `train_test_split()` function with the `stratify` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see if this worked as expected. You can start by looking at the income
    category proportions in the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: With similar code you can measure the income category proportions in the full
    dataset. [Figure 2-10](#compare_sampling_errors_screenshot) compares the income
    category proportions in the overall dataset, in the test set generated with stratified
    sampling, and in a test set generated using purely random sampling. As you can
    see, the test set generated using stratified sampling has income category proportions
    almost identical to those in the full dataset, whereas the test set generated
    using purely random sampling is skewed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Table comparing income category proportions in the overall dataset, stratified
    sampling, and random sampling, highlighting lower errors in stratified sampling.](assets/hmls_0210.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-10\. Sampling bias comparison of stratified versus purely random sampling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You won’t use the `income_cat` column again, so you might as well drop it,
    reverting the data back to its original state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We spent quite a bit of time on test set generation for a good reason: this
    is an often neglected but critical part of a machine learning project. Moreover,
    many of these ideas will be useful later when we discuss cross-validation. Now
    it’s time to move on to the next stage: exploring the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Explore and Visualize the Data to Gain Insights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far you have only taken a quick glance at the data to get a general understanding
    of the kind of data you are manipulating. Now the goal is to go into a little
    more depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, make sure you have put the test set aside and you are only exploring
    the training set. Also, if the training set is very large, you may want to sample
    an exploration set, to make manipulations easy and fast during the exploration
    phase. In this case, the training set is quite small, so you can just work directly
    on the full set. Since you’re going to experiment with various transformations
    of the full training set, you should make a copy of the original so you can revert
    to it afterwards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing Geographical Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because the dataset includes geographical information (latitude and longitude),
    it is a good idea to create a scatterplot of all the districts to visualize the
    data ([Figure 2-11](#bad_visualization_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Scatter plot displaying geographical data points with longitude on the x-axis
    and latitude on the y-axis, showing a distribution resembling California.](assets/hmls_0211.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-11\. A geographical scatterplot of the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This looks like California all right, but other than that it is hard to see
    any particular pattern. Setting the `alpha` option to `0.2` makes it much easier
    to visualize the places where there is a high density of data points ([Figure 2-12](#better_visualization_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that’s much better: you can clearly see the high-density areas, namely
    the Bay Area and around Los Angeles and San Diego, plus a long line of fairly
    high-density areas in the Central Valley (in particular, around Sacramento and
    Fresno).'
  prefs: []
  type: TYPE_NORMAL
- en: Our brains are very good at spotting patterns in pictures, but you may need
    to play around with visualization parameters to make the patterns stand out.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatter plot showing the distribution of housing locations by latitude and
    longitude, illustrating high-density areas with darker blue clusters.](assets/hmls_0212.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-12\. A better visualization that highlights high-density areas
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next, you look at the housing prices ([Figure 2-13](#housing_prices_scatterplot)).
    The radius of each circle represents the district’s population (option `s`), and
    the color represents the price (option `c`). Here you use a predefined color map
    (option `cmap`) called `jet`, which ranges from blue (low values) to red (high
    prices):⁠^([8](ch02.html#id1068))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This image tells you that the housing prices are very much related to the location
    (e.g., close to the ocean) and to the population density, as you probably knew
    already. A clustering algorithm should be useful for detecting the main cluster
    and for adding new features that measure the proximity to the cluster centers.
    The ocean proximity attribute may be useful as well, although in Northern California
    the housing prices in coastal districts are not too high, so it is not a simple
    rule.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatter plot showing California housing prices by location, with red indicating
    expensive areas and blue indicating cheaper ones; larger circles represent areas
    with larger population density.](assets/hmls_0213.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-13\. California housing prices: red is expensive, blue is cheap, larger
    circles indicate areas with a larger population'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Look for Correlations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the dataset is not too large, you can easily compute the *standard correlation
    coefficient* (also called *Pearson’s r*) between every pair of numerical attributes
    using the `corr()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can look at how much each attribute correlates with the median house
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The correlation coefficient ranges from –1 to 1\. When it is close to 1, it
    means that there is a strong positive correlation; for example, the median house
    value tends to go up when the median income goes up. When the coefficient is close
    to –1, it means that there is a strong negative correlation; you can see a small
    negative correlation between the latitude and the median house value (i.e., prices
    have a slight tendency to go down when you go north). Finally, coefficients close
    to 0 mean that there is no linear correlation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to check for correlation between attributes is to use the Pandas
    `scatter_matrix()` function, which plots every numerical attribute against every
    other numerical attribute. Since there are now 9 numerical attributes, you would
    get 9² = 81 plots, which would not fit on a page—so you decide to focus on a few
    promising attributes that seem most correlated with the median housing value ([Figure 2-14](#scatter_matrix_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![Scatter matrix displaying pairwise comparisons of median house value, median
    income, total rooms, and housing median age, with histograms on the diagonals.](assets/hmls_0214.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-14\. This scatter matrix plots every numerical attribute against every
    other numerical attribute, plus a histogram of each numerical attribute’s values
    on the main diagonal (top left to bottom right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The main diagonal would be full of straight lines if Pandas plotted each variable
    against itself, which would not be very useful. So instead, the Pandas displays
    a histogram of each attribute (other options are available; see the Pandas documentation
    for more details).
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the correlation scatterplots, it seems like the most promising attribute
    to predict the median house value is the median income, so you zoom in on that
    scatterplot ([Figure 2-15](#income_vs_house_value_scatterplot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![Scatter plot showing the correlation between median income and median house
    value, highlighting an upward trend and visible price caps at specific values.](assets/hmls_0215.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-15\. Median income versus median house value
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This plot reveals a few things. First, the correlation is indeed quite strong;
    you can clearly see the upward trend, although the data is noisy. Second, the
    price cap you noticed earlier is clearly visible as a horizontal line at $500,000\.
    But the plot also reveals other less obvious straight lines: a horizontal line
    around $450,000, another around $350,000, perhaps one around $280,000, and a few
    more below that. You may want to try removing the corresponding districts to prevent
    your algorithms from learning to reproduce these data quirks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the correlation coefficient only measures linear correlations (“as
    *x* goes up, *y* generally goes up/down”). It may completely miss out on nonlinear
    relationships (e.g., “as *x* approaches 0, *y* generally goes up”). [Figure 2-16](#correlation_coefficient_plots)
    shows a variety of datasets along with their correlation coefficient. Note how
    all the plots of the bottom row have a correlation coefficient equal to 0, despite
    the fact that their axes are clearly *not* independent: these are examples of
    nonlinear relationships. Also, the second row shows examples where the correlation
    coefficient is equal to 1 or –1; notice that this has nothing to do with the slope.
    For example, your height in inches has a correlation coefficient of 1 with your
    height in feet or in nanometers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatter plots of various datasets showing how correlation coefficients can
    indicate strong linear relationships in some cases but fail to capture nonlinear
    relationships, with coefficients ranging from -1 to 1.](assets/hmls_0216.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-16\. Standard correlation coefficient of various datasets (source:
    Wikipedia; public domain image)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Experiment with Attribute Combinations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hopefully the previous sections gave you an idea of a few ways you can explore
    the data and gain insights. You identified a few data quirks that you may want
    to clean up before feeding the data to a machine learning algorithm, and you found
    interesting correlations between attributes, in particular with the target attribute.
    You also noticed that some attributes have a skewed-right distribution, so you
    may want to transform them (e.g., by computing their logarithm or square root).
    Of course, your mileage will vary considerably with each project, but the general
    ideas are similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'One last thing you may want to do before preparing the data for machine learning
    algorithms is to try out various attribute combinations. For example, the total
    number of rooms in a district is not very useful if you don’t know how many households
    there are. What you really want is the number of rooms per household. Similarly,
    the total number of bedrooms by itself is not very useful: you probably want to
    compare it to the total number of rooms. And the population per household also
    seems like an interesting attribute combination to look at. You create these new
    attributes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'And then you look at the correlation matrix again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Hey, not bad! The new `bedrooms_ratio` attribute is much more correlated with
    the median house value than the total number of rooms or bedrooms. It’s a strong
    negative correlation, so it looks like houses with a lower bedroom/room ratio
    tend to be more expensive. The number of rooms per household is also more informative
    than the total number of rooms in a district—obviously the larger the houses,
    the more expensive they are.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When creating new combined features, make sure they are not too linearly correlated
    with existing features: *collinearity* can cause issues with some models, such
    as linear regression. In particular, avoid simple weighted sums of existing features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This round of exploration does not have to be absolutely thorough; the point
    is to start off on the right foot and quickly gain insights that will help you
    get a first reasonably good prototype. But this is an iterative process: once
    you get a prototype up and running, you can analyze its output to gain more insights
    and come back to this exploration step.'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the Data for Machine Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s time to prepare the data for your machine learning algorithms. Instead
    of doing this manually, you should write functions for this purpose, for several
    good reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: This will allow you to reproduce these transformations easily on any dataset
    (e.g., the next time you get a fresh dataset).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will gradually build a library of transformation functions that you can
    reuse in future projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use these functions in your live system to transform the new data before
    feeding it to your algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will make it possible for you to easily try various transformations and
    see which combination of transformations works best.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But first, revert to a clean training set (by copying `strat_train_set` once
    again). You should also separate the predictors and the labels, since you don’t
    necessarily want to apply the same transformations to the predictors and the target
    values (note that `drop()` creates a copy of the data and does not affect `strat_train_set`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Clean the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most machine learning algorithms cannot work with missing features, so you’ll
    need to take care of these. For example, you noticed earlier that the `total_bedrooms`
    attribute has some missing values. You have three options to fix this:'
  prefs: []
  type: TYPE_NORMAL
- en: Get rid of the corresponding districts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get rid of the whole attribute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the missing values to some value (zero, the mean, the median, etc.). This
    is called *imputation*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can accomplish these easily using the Pandas DataFrame’s `dropna()`, `drop()`,
    and `fillna()` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You decide to go for option 3 since it is the least destructive, but instead
    of the preceding code, you will use a handy Scikit-Learn class: `SimpleImputer`.
    The benefit is that it will store the median value of each feature: this will
    make it possible to impute missing values not only on the training set, but also
    on the validation set, the test set, and any new data fed to the model. To use
    it, first you need to create a `SimpleImputer` instance, specifying that you want
    to replace each attribute’s missing values with the median of that attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the median can only be computed on numerical attributes, you then need
    to create a copy of the data with only the numerical attributes (this will exclude
    the text attribute `ocean_proximity`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can fit the `imputer` instance to the training data using the `fit()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `imputer` has simply computed the median of each attribute and stored the
    result in its `statistics_` instance variable. Only the `total_bedrooms` attribute
    had missing values, but you cannot be sure that there won’t be any missing values
    in new data after the system goes live, so it is safer to apply the `imputer`
    to all the numerical attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can use this “trained” `imputer` to transform the training set by replacing
    missing values with the learned medians:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Missing values can also be replaced with the mean value (`strategy="mean"`),
    or with the most frequent value (`strategy="most_frequent"`), or with a constant
    value (`strategy="constant", fill_value=`…​). The last two strategies support
    non-numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are also more powerful imputers available in the `sklearn.​impute` package
    (both for numerical features only):'
  prefs: []
  type: TYPE_NORMAL
- en: '`KNNImputer` replaces each missing value with the mean of the *k*-nearest neighbors’
    values for that feature. The distance is based on all the available features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IterativeImputer` trains a regression model per feature to predict the missing
    values based on all the other available features. It then trains the model again
    on the updated data, and repeats the process several times, improving the models
    and the replacement values at each iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scikit-Learn transformers output NumPy arrays (or sometimes SciPy sparse matrices)
    even when they are fed Pandas DataFrames as input.⁠^([11](ch02.html#id1107)) So,
    the output of `imputer.transform(housing_num)` is a NumPy array: `X` has neither
    column names nor index. Luckily, it’s not too hard to wrap `X` in a DataFrame
    and recover the column names and index from `housing_num`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Handling Text and Categorical Attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far we have only dealt with numerical attributes, but your data may also
    contain text attributes. In this dataset, there is just one: the `ocean_proximity`
    attribute. Let’s look at its value for the first few instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s not arbitrary text: there are a limited number of possible values, each
    of which represents a category. So this attribute is a categorical attribute.
    Most machine learning algorithms prefer to work with numbers, so let’s convert
    these categories from text to numbers. For this, we can use Scikit-Learn’s `OrdinalEncoder`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what the first few encoded values in `housing_cat_encoded` look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'You can get the list of categories using the `categories_` instance variable.
    It is a list containing a 1D array of categories for each categorical attribute
    (in this case, a list containing a single array since there is just one categorical
    attribute):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'One issue with this representation is that ML algorithms will assume that two
    nearby values are more similar than two distant values. This may be fine in some
    cases (e.g., for ordered categories such as “bad”, “average”, “good”, and “excellent”),
    but it is obviously not the case for the `ocean_proximity` column (for example,
    categories 0 and 4 are clearly more similar than categories 0 and 1). To fix this
    issue, a common solution is to create one binary attribute per category: one attribute
    equal to 1 when the category is `"<1H OCEAN"` (and 0 otherwise), another attribute
    equal to 1 when the category is `"INLAND"` (and 0 otherwise), and so on. This
    is called *one-hot encoding*, because only one attribute will be equal to 1 (hot),
    while the others will be 0 (cold). The new attributes are sometimes called *dummy*
    attributes. Scikit-Learn provides a `OneHotEncoder` class to convert categorical
    values into one-hot vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the output of a `OneHotEncoder` is a SciPy *sparse matrix*, instead
    of a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'A sparse matrix is a very efficient representation for matrices that contain
    mostly zeros. Indeed, internally it only stores the nonzero values and their positions.
    When a categorical attribute has hundreds or thousands of categories, one-hot
    encoding it results in a very large matrix full of 0s except for a single 1 per
    row. In this case, a sparse matrix is exactly what you need: it will save plenty
    of memory and speed up computations. You can use a sparse matrix mostly like a
    normal 2D array,⁠^([12](ch02.html#id1118)) but if you want to convert it to a
    (dense) NumPy array, just call the `toarray()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can set `sparse_output=False` when creating the `OneHotEncoder`,
    in which case the `transform()` method will return a regular (dense) NumPy array
    directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'As with the `OrdinalEncoder`, you can get the list of categories using the
    encoder’s `categories_` instance variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Pandas has a function called `get_dummies()`, which also converts each categorical
    feature into a one-hot representation, with one binary feature per category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks nice and simple, so why not use it instead of `OneHotEncoder`? Well,
    the advantage of `OneHotEncoder` is that it remembers which categories it was
    trained on. This is very important because once your model is in production, it
    should be fed exactly the same features as during training: no more, no less.
    Look what our trained `cat_encoder` outputs when we make it transform the same
    `df_test` (using `transform()`, not `fit_transform()`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'See the difference? `get_dummies()` saw only two categories, so it output two
    columns, whereas `OneHotEncoder` output one column per learned category, in the
    right order. Moreover, if you feed `get_dummies()` a DataFrame containing an unknown
    category (e.g., `"<2H OCEAN"`), it will happily generate a column for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'But `OneHotEncoder` is smarter: it will detect the unknown category and raise
    an exception. If you prefer, you can set the `handle_unknown` hyperparameter to
    `"ignore"`, in which case it will just represent the unknown category with zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If a categorical attribute has a large number of possible categories (e.g.,
    country code, profession, species), then one-hot encoding will result in a large
    number of input features. This may slow down training and degrade performance.
    If this happens, you may want to replace the categorical input with useful numerical
    features related to the categories: for example, you could replace the `ocean_proximity`
    feature with the distance to the ocean (similarly, a country code could be replaced
    with the country’s population and GDP per capita). Alternatively, you can use
    one of the encoders provided by the `category_encoders` package on [GitHub](https://github.com/scikit-learn-contrib/category_encoders).
    Or, when dealing with neural networks, you can replace each category with a learnable,
    low-dimensional vector called an *embedding* (see [Chapter 14](ch14.html#nlp_chapter)).
    This is an example of *representation learning* (we will see more examples in
    [Chapter 18](ch18.html#autoencoders_chapter)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you fit any Scikit-Learn estimator using a DataFrame, the estimator stores
    the column names in the `feature_names_in_` attribute. Scikit-Learn then ensures
    that any DataFrame fed to this estimator after that (e.g., to `transform()` or
    `predict()`) has the same column names. Transformers also provide a `get_feature_names_out()`
    method that you can use to build a DataFrame around the transformer’s output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This feature helps avoid column mismatches, and it’s also quite useful when
    debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Scaling and Transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most important transformations you need to apply to your data is
    *feature scaling*. With few exceptions, machine learning algorithms don’t perform
    well when the input numerical attributes have very different scales. This is the
    case for the housing data: the total number of rooms ranges from about 6 to 39,320,
    while the median incomes only range from 0 to 15\. Without any scaling, most models
    will be biased toward ignoring the median income and focusing more on the number
    of rooms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common ways to get all attributes to have the same scale: *min-max
    scaling* and *standardization*.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As with all estimators, it is important to fit the scalers to the training
    data only: never use `fit()` or `fit_transform()` for anything else than the training
    set. Once you have a trained scaler, you can then use it to `transform()` any
    other set, including the validation set, the test set, and new data. Note that
    while the training set values will always be scaled to the specified range, if
    new data contains outliers, these may end up scaled outside the range. If you
    want to avoid this, just set the `clip` hyperparameter to `True`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Min-max scaling (many people call this *normalization*) is the simplest: for
    each attribute, the values are shifted and rescaled so that they end up ranging
    from 0 to 1\. This is performed by subtracting the min value from all values,
    and dividing the results by the difference between the min and the max. Scikit-Learn
    provides a transformer called `MinMaxScaler` for this. It has a `feature_range`
    hyperparameter that lets you change the range if, for some reason, you don’t want
    0–1 (e.g., neural networks work best with zero-mean inputs, so a range of –1 to
    1 is preferable). It’s quite easy to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Standardization is different: first it subtracts the mean value (so standardized
    values have a zero mean), then it divides the result by the standard deviation
    (so standardized values have a standard deviation equal to 1). Unlike min-max
    scaling, standardization does not restrict values to a specific range. However,
    standardization is much less affected by outliers. For example, suppose a district
    has a median income equal to 100 (by mistake), instead of the usual 0–15\. Min-max
    scaling to the 0–1 range would map this outlier down to 1 and it would crush all
    the other values down to 0–0.15, whereas standardization would not be much affected.
    Scikit-Learn provides a transformer called `StandardScaler` for standardization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you want to scale a sparse matrix without converting it to a dense matrix
    first, you can use a `StandardScaler` with its `with_mean` hyperparameter set
    to `False`: it will only divide the data by the standard deviation, without subtracting
    the mean (as this would break sparsity).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When a feature’s distribution has a *heavy tail* (i.e., when values far from
    the mean are not exponentially rare), both min-max scaling and standardization
    will squash most values into a small range. Machine learning models generally
    don’t like this at all, as you will see in [Chapter 4](ch04.html#linear_models_chapter).
    So *before* you scale the feature, you should first transform it to shrink the
    heavy tail, and if possible to make the distribution roughly symmetrical. For
    example, a common way to do this for positive features with a heavy tail to the
    right is to replace the feature with its square root (or raise the feature to
    a power between 0 and 1). If the feature has a really long and heavy tail, such
    as a *power law distribution*, then replacing the feature with its logarithm may
    help. For example, the `population` feature roughly follows a power law: districts
    with 10,000 inhabitants are only 10 times less frequent than districts with 1,000
    inhabitants, not exponentially less frequent. [Figure 2-17](#long_tail_plot) shows
    how much better this feature looks when you compute its log: it’s very close to
    a Gaussian distribution (i.e., bell-shaped).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Two histograms compare the distribution of a population feature: the left
    graph shows a heavily right-skewed distribution, while the right graph shows a
    more symmetrical, Gaussian-like distribution after applying a logarithmic transformation.](assets/hmls_0217.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-17\. Transforming a feature to make it closer to a Gaussian distribution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another approach to handle heavy-tailed features consists in *bucketizing* the
    feature. This means chopping its distribution into roughly equal-sized buckets,
    and replacing each feature value with the index of the bucket it belongs to, much
    like we did to create the `income_cat` feature (although we only used it for stratified
    sampling). For example, you could replace each value with its percentile. Bucketizing
    with equal-sized buckets results in a feature with an almost uniform distribution,
    so there’s no need for further scaling, or you can just divide by the number of
    buckets to force the values to the 0–1 range.
  prefs: []
  type: TYPE_NORMAL
- en: When a feature has a multimodal distribution (i.e., with two or more clear peaks,
    called *modes*), such as the `housing_median_age` feature, it can also be helpful
    to bucketize it, but this time treating the bucket IDs as categories, rather than
    as numerical values. This means that the bucket indices must be encoded, for example
    using a `OneHotEncoder` (so you usually don’t want to use too many buckets). This
    approach will allow the regression model to more easily learn different rules
    for different ranges of this feature value. For example, perhaps houses built
    around 35 years ago have a peculiar style that fell out of fashion, and therefore
    they’re cheaper than their age alone would suggest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach to transforming multimodal distributions is to add a feature
    for each of the modes (at least the main ones), representing the similarity between
    the housing median age and that particular mode. The similarity measure is typically
    computed using a *radial basis function* (RBF)—any function that depends only
    on the distance between the input value and a fixed point. The most commonly used
    RBF is the Gaussian RBF, whose output value decays exponentially as the input
    value moves away from the fixed point. For example, the Gaussian RBF similarity
    between the housing age *x* and 35 is given by the equation exp(–*γ*(*x* – 35)²).
    The hyperparameter *γ* (gamma) determines how quickly the similarity measure decays
    as *x* moves away from 35\. Using Scikit-Learn’s `rbf_kernel()` function, you
    can create a new Gaussian RBF feature measuring the similarity between the housing
    median age and 35:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 2-18](#age_similarity_plot) shows this new feature as a function of
    the housing median age (solid line). It also shows what the feature would look
    like if you used a smaller `gamma` value. As the chart shows, the new age similarity
    feature peaks at 35, right around the spike in the housing median age distribution:
    if this particular age group is well correlated with lower prices, there’s a good
    chance that this new feature will help.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram and line plot showing the Gaussian RBF feature for age similarity
    peaking at a housing median age of 35, with gamma values of 0.10 and 0.03.](assets/hmls_0218.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-18\. Gaussian RBF feature measuring the similarity between the housing
    median age and 35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So far we’ve only looked at the input features, but the target values may also
    need to be transformed. For example, if the target distribution has a heavy tail,
    you may choose to replace the target with its logarithm. But if you do, the regression
    model will now predict the *log* of the median house value, not the median house
    value itself. You will need to compute the exponential of the model’s prediction
    if you want the predicted median house value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, most of Scikit-Learn’s transformers have an `inverse_transform()`
    method, making it easy to compute the inverse of their transformations. For example,
    the following code example shows how to scale the labels using a `StandardScaler`
    (just like we did for inputs), then train a simple linear regression model on
    the resulting scaled labels and use it to make predictions on some new data, which
    we transform back to the original scale using the trained scaler’s `inverse_transform()`
    method. Note that we convert the labels from a Pandas Series to a DataFrame, since
    the `StandardScaler` expects 2D inputs. Also, in this example we just train the
    model on a single raw input feature (median income), for simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This works fine, but it’s simpler and less error-prone to use a `TransformedTarget​Regressor`,
    avoiding potential scaling mismatches. We just need to construct it, giving it
    the regression model and the label transformer, then fit it on the training set,
    using the original unscaled labels. It will automatically use the transformer
    to scale the labels and train the regression model on the resulting scaled labels,
    just like we did previously. Then, when we want to make a prediction, it will
    call the regression model’s `predict()` method and use the scaler’s `inverse_transform()`
    method to produce the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Custom Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although Scikit-Learn provides many useful transformers, you will occasionally
    need to write your own for tasks such as custom transformations, cleanup operations,
    or combining specific attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For transformations that don’t require any training, you can just write a function
    that takes a NumPy array as input and outputs the transformed array. For example,
    as discussed in the previous section, it’s often a good idea to transform features
    with heavy-tailed distributions by replacing them with their logarithm (assuming
    the feature is positive and the tail is on the right). Let’s create a log-transformer
    and apply it to the `population` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The `inverse_func` argument is optional. It lets you specify an inverse transform
    function, e.g., if you plan to use your transformer in a `TransformedTargetRegressor`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your transformation function can take hyperparameters as additional arguments.
    For example, here’s how to create a transformer that computes the same Gaussian
    RBF similarity measure as earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that there’s no inverse function for the RBF kernel, since there are always
    two values at a given distance from a fixed point (except at distance 0). Also
    note that `rbf_kernel()` does not treat the features separately. If you pass it
    an array with two features, it will measure the 2D distance (Euclidean) to measure
    similarity. For example, here’s how to add a feature that will measure the geographic
    similarity between each district and San Francisco:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Custom transformers are also useful to combine features. For example, here’s
    a `FunctionTransformer` that computes the ratio between the input features 0 and
    1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '`FunctionTransformer` is very handy, but what if you would like your transformer
    to be trainable, learning some parameters in the `fit()` method and using them
    later in the `transform()` method? For this, you need to write a custom class.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The rest of this section shows how to define custom transformer classes. In
    particular, it defines a custom transformer that groups districts into 10 geographical
    clusters, then measures the distance between each district and the center of each
    cluster, adding 10 corresponding RBF similarity features to the data. Since defining
    custom transformer classes is somewhat advanced, please feel free to skip to the
    next section and come back whenever needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn relies on duck typing,⁠^([13](ch02.html#id1177)) so custom transformer
    classes do not have to inherit from any particular base class. All they need is
    three methods: `fit()` (which must return `self`), `transform()`, and `fit_transform()`.
    You can get `fit_transform()` for free by simply adding `TransformerMixin` as
    a base class: the default implementation will just call `fit()` and then `transform()`.
    If you add `BaseEstimator` as a base class (and avoid using `*args` and `**kwargs`
    in your constructor), you will also get two extra methods: `get_params()` and
    `set_params()`. These will be useful for automatic hyperparameter tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here’s a custom transformer that acts much like the `StandardScaler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are a few things to note:'
  prefs: []
  type: TYPE_NORMAL
- en: The `sklearn.utils.validation` package contains several functions we can use
    to validate the inputs. For simplicity, we will skip such tests in the rest of
    this book, but production code should have them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-Learn pipelines require the `fit()` method to have two arguments `X`
    and `y`, which is why we need the `y=None` argument even though we don’t use `y`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All Scikit-Learn estimators set `n_features_in_` in the `fit()` method, and
    they ensure that the data passed to `transform()` or `predict()` has this number
    of features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `fit()` method must return `self`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This implementation is not 100% complete: all estimators should set `feature_​names_in_`
    in the `fit()` method when they are passed a DataFrame. Moreover, all transformers
    should provide a `get_feature_names_out()` method, as well as an `inverse_transform()`
    method when their transformation can be reversed. See the last exercise at the
    end of this chapter for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A custom transformer can (and often does) use other estimators in its implementation.
    For example, the following code demonstrates a custom transformer that uses a
    `KMeans` clusterer in the `fit()` method to identify the main clusters in the
    training data, and then uses `rbf_kernel()` in the `transform()` method to measure
    how similar each sample is to each cluster center:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can check whether your custom estimator respects Scikit-Learn’s API by passing
    an instance to `check_estimator()` from the `sklearn.utils.estimator_checks` package.
    For the full API, check out [*https://scikit-learn.org/stable/developers*](https://scikit-learn.org/stable/developers).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you will see in [Chapter 8](ch08.html#unsupervised_learning_chapter), *k*-means
    is a clustering algorithm that locates clusters in the data. For example, we can
    use it to find the most populated regions in California. How many clusters *k*-means
    searches for is controlled by the `n_clusters` hyperparameter. The `fit()` method
    of `KMeans` supports an optional argument `sample_weight`, which lets the user
    specify the relative weights of the samples. For example, we could pass it the
    median income if we wanted the clusters to be biased toward wealthy districts.
    After training, the cluster centers are available via the `cluster_centers_` attribute.
    *k*-means is a stochastic algorithm, meaning that it relies on randomness to locate
    the clusters, so if you want reproducible results, you must set the `random_state`
    parameter. As you can see, despite the complexity of the task, the code is fairly
    straightforward. Now let’s use this custom transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This code creates a `ClusterSimilarity` transformer, setting the number of
    clusters to 10\. Then it calls `fit_transform()` with the latitude and longitude
    of every district in the training set (you can try weighting each district by
    its median income to see how that affects the clusters). The transformer uses
    *k*-means to locate the clusters, then measures the Gaussian RBF similarity between
    each district and all 10 cluster centers. The result is a matrix with one row
    per district, and one column per cluster. Let’s look at the first three rows,
    rounding to two decimal places:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 2-19](#district_cluster_plot) shows the 10 cluster centers found by
    *k*-means. The districts are colored according to their geographic similarity
    to their closest cluster center. Notice that most clusters are located in highly
    populated areas.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatter plot showing geographic clusters based on Gaussian RBF similarity,
    with highly populated areas highlighted.](assets/hmls_0219.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-19\. Gaussian RBF similarity to the nearest cluster center
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Transformation Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you can see, there are many data transformation steps that need to be executed
    in the right order. Fortunately, Scikit-Learn provides the `Pipeline` class to
    help with such sequences of transformations. Here is a small pipeline for numerical
    attributes, which will first impute then scale the input features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Pipeline` constructor takes a list of name/estimator pairs (2-tuples)
    defining a sequence of steps. The names can be anything you like, as long as they
    are unique and don’t contain double underscores (`__`). They will be useful later,
    when we discuss hyperparameter tuning. The estimators must all be transformers
    (i.e., they must have a `fit_transform()` method), except for the last one, which
    can be anything: a transformer, a predictor, or any other type of estimator.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a Jupyter notebook, if you `import` `sklearn` and run `sklearn.​set_config(display="diagram")`,
    all Scikit-Learn estimators will be rendered as interactive diagrams. This is
    particularly useful for visualizing pipelines. To visualize `num_pipeline`, run
    a cell with `num_pipeline` as the last line. Clicking an estimator will show more
    details.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t want to have to name the transformers, you can use the convenient
    `make_pipeline()` function instead; it takes transformers as positional arguments
    and creates a `Pipeline` using the names of the transformers’ classes, in lowercase
    and without underscores (e.g., `"simpleimputer"`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: If multiple transformers have the same name, an index is appended to their names
    (e.g., `"foo-1"`, `"foo-2"`, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: When you call the pipeline’s `fit()` method, it calls `fit_transform()` sequentially
    on all the transformers, passing the output of each call as the parameter to the
    next call until it reaches the final estimator, for which it just calls the `fit()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline exposes the same methods as the final estimator. In this example
    the last estimator is a `StandardScaler`, which is a transformer, so the pipeline
    also acts like a transformer. If you call the pipeline’s `transform()` method,
    it will sequentially apply all the transformations to the data. If the last estimator
    were a predictor instead of a transformer, then the pipeline would have a `predict()`
    method rather than a `transform()` method. Calling it would sequentially apply
    all the transformations to the data and pass the result to the predictor’s `predict()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s call the pipeline’s `fit_transform()` method and look at the output’s
    first two rows, rounded to two decimal places:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'As you saw earlier, if you want to recover a nice DataFrame, you can use the
    pipeline’s `get_feature_names_out()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Pipelines support indexing; for example, `pipeline[1]` returns the second estimator
    in the pipeline, and `pipeline[:-1]` returns a `Pipeline` object containing all
    but the last estimator. You can also access the estimators via the `steps` attribute,
    which is a list of name/estimator pairs, or via the `named_steps` dictionary attribute,
    which maps the names to the estimators. For example, `num_pipeline["simpleimputer"]`
    returns the estimator named `"simpleimputer"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have handled the categorical columns and the numerical columns separately.
    It would be more convenient to have a single transformer capable of handling all
    columns, applying the appropriate transformations to each column. For this, you
    can use a `ColumnTransformer`. For example, the following `ColumnTransformer`
    will apply `num_pipeline` (the one we just defined) to the numerical attributes,
    and `cat_pipeline` to the categorical attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: First we import the `ColumnTransformer` class, then we define the list of numerical
    and categorical column names and construct a simple pipeline for categorical attributes.
    Lastly, we construct a `ColumnTransformer`. Its constructor requires a list of
    triplets (3-tuples), each containing a name (which must be unique and not contain
    double underscores), a transformer, and a list of names (or indices) of columns
    that the transformer should be applied to.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instead of using a transformer, you can specify the string `"drop"` if you want
    the columns to be dropped, or you can specify `"passthrough"` if you want the
    columns to be left untouched. By default, the remaining columns (i.e., the ones
    that were not listed) will be dropped, but you can set the `remainder` hyperparameter
    to any transformer (or to `"passthrough"`) if you want these columns to be handled
    differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since listing all the column names is not very convenient, Scikit-Learn provides
    a `make_column_selector` class that you can use to automatically select all the
    features of a given type, such as numerical or categorical. You can pass a selector
    to the `ColumnTransformer` instead of column names or indices. Moreover, if you
    don’t care about naming the transformers, you can use `make_column_transformer()`,
    which chooses the names for you, just like `make_pipeline()` does. For example,
    the following code creates the same `ColumnTransformer` as earlier, except the
    transformers are automatically named `"pipeline-1"` and `"pipeline-2"` instead
    of `"num"` and `"cat"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’re ready to apply this `ColumnTransformer` to the housing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Great! We have a preprocessing pipeline that takes the entire training dataset
    and applies each transformer to the appropriate columns, then concatenates the
    transformed columns horizontally (transformers must never change the number of
    rows). Once again this returns a NumPy array, but you can get the column names
    using `preprocessing.get_feature_names_out()` and wrap the data in a nice DataFrame
    as we did before.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `OneHotEncoder` returns a sparse matrix and the `num_pipeline` returns a
    dense matrix. When there is such a mix of sparse and dense matrices, the `ColumnTransformer`
    estimates the density of the final matrix (i.e., the ratio of nonzero cells),
    and it returns a sparse matrix if the density is lower than a given threshold
    (by default, `sparse_threshold=0.3`). In this example, it returns a dense matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your project is going really well and you’re almost ready to train some models!
    You now want to create a single pipeline that will perform all the transformations
    you’ve experimented with up to now. Let’s recap what the pipeline will do and
    why:'
  prefs: []
  type: TYPE_NORMAL
- en: Missing values in numerical features will be imputed by replacing them with
    the median, as most ML algorithms don’t expect missing values. In categorical
    features, missing values will be replaced by the most frequent category.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The categorical feature will be one-hot encoded, as most ML algorithms only
    accept numerical inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A few ratio features will be computed and added: `bedrooms_ratio`, `rooms_​per_house`,
    and `people_per_house`. Hopefully these will better correlate with the median
    house value, and thereby help the ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few cluster similarity features will also be added. These will likely be more
    useful to the model than latitude and longitude.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features with a long tail will be replaced by their logarithm, as most models
    prefer features with roughly uniform or Gaussian distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All numerical features will be standardized, as most ML algorithms prefer when
    all features have roughly the same scale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code that builds the pipeline to do all of this should look familiar to
    you by now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this `ColumnTransformer`, it performs all the transformations and
    outputs a NumPy array with 24 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Select and Train a Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At last! You framed the problem, you got the data and explored it, you sampled
    a training set and a test set, and you wrote a preprocessing pipeline to automatically
    clean up and prepare your data for machine learning algorithms. You are now ready
    to select and train a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Train and Evaluate on the Training Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The good news is that thanks to all these previous steps, things are now going
    to be easy! You decide to train a very basic linear regression model to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Done! You now have a working linear regression model. You try it out on the
    training set, looking at the first five predictions and comparing them to the
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, it works, but not always: the first prediction is way off (by over $200,000!),
    while the other predictions are better: two are off by about 25%, and two are
    off by less than 10%. Remember that you chose to use the RMSE as your performance
    measure, so you want to measure this regression model’s RMSE on the whole training
    set using Scikit-Learn’s `root_mean_squared_error()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We’re not using the `score()` method here because it returns the *R² coefficient
    of determination* instead of the RMSE. This coefficient represents the ratio of
    the variance in the data that the model can explain: the closer to 1 (which is
    the max value), the better. If the model simply predicts the mean all the time,
    it does not explain any part of the variance, so the model’s R² score is 0\. And
    if the model does even worse than that, then its R² score can be negative, and
    indeed arbitrarily low.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is better than nothing, but clearly not a great score: the `median_housing_values`
    of most districts range between $120,000 and $265,000, so a typical prediction
    error of $68,973 is really not very satisfying. This is an example of a model
    underfitting the training data. When this happens it can mean that the features
    do not provide enough information to make good predictions, or that the model
    is not powerful enough. As we saw in the previous chapter, the main ways to fix
    underfitting are to select a more powerful model, to feed the training algorithm
    with better features, or to reduce the constraints on the model. This model is
    not regularized, which rules out the last option. You could try to add more features,
    but first you want to try a more complex model to see how it does.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You decide to try a `DecisionTreeRegressor`, as this is a fairly powerful model
    capable of finding complex nonlinear relationships in the data (decision trees
    are presented in more detail in [Chapter 5](ch05.html#trees_chapter)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the model is trained, you evaluate it on the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Wait, what!? No error at all? Could this model really be absolutely perfect?
    Of course, it is much more likely that the model has badly overfit the data. How
    can you be sure? As you saw earlier, you don’t want to touch the test set until
    you are ready to launch a model you are confident about, so you need to use part
    of the training set for training and part of it for model validation.
  prefs: []
  type: TYPE_NORMAL
- en: Better Evaluation Using Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to evaluate the decision tree model would be to use the `train_​test_split()`
    function to split the training set into a smaller training set and a validation
    set, then train your models against the smaller training set and evaluate them
    against the validation set. It’s a bit of effort, but nothing too difficult, and
    it would work fairly well.
  prefs: []
  type: TYPE_NORMAL
- en: A great alternative is to use Scikit-Learn’s *k-fold cross-validation* feature.
    You split the training set into *k* nonoverlapping subsets called *folds*, then
    you train and evaluate your model *k* times, picking a different fold for evaluation
    every time (i.e., the validation fold) and using the other *k* – 1 folds for training.
    This process produces *k* evaluation scores (see [Figure 2-20](#k_fold_cross_validation_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating _k_-fold cross-validation with _k_ = 10, displaying
    different validation folds and corresponding evaluation scores for each split.](assets/hmls_0220.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-20\. *k*-fold cross-validation, with *k* = 10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides a convenient `cross_val_score()` function that does just
    that, and it returns an array containing the *k* evaluation scores. For example,
    let’s use it to evaluate our tree regressor, using *k* = 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scikit-Learn’s cross-validation features expect a utility function (greater
    is better) rather than a cost function (lower is better), so the scoring function
    is actually the opposite of the RMSE. It’s a negative value, so you need to switch
    the sign of the output to get the RMSE scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Now the decision tree doesn’t look as good as it did earlier. In fact, it seems
    to perform almost as poorly as the linear regression model! Notice that cross-validation
    allows you to get not only an estimate of the performance of your model, but also
    a measure of how precise this estimate is (i.e., its standard deviation). The
    decision tree has an RMSE of about 66,574, with a standard deviation of about
    1,103\. You would not have this information if you just used one validation set.
    But cross-validation comes at the cost of training the model several times, so
    it is not always feasible.
  prefs: []
  type: TYPE_NORMAL
- en: If you compute the same metric for the linear regression model, you will find
    that the mean RMSE is 70,003 and the standard deviation is 4,182\. So the decision
    tree model seems to perform very slightly better than the linear model, but the
    difference is minimal due to severe overfitting. We know there’s an overfitting
    problem because the training error is low (actually zero) while the validation
    error is high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try one last model now: the `RandomForestRegressor`. As you will see
    in [Chapter 6](ch06.html#ensembles_chapter), random forests work by training many
    decision trees on random subsets of the features, then averaging out their predictions.
    Such models composed of many other models are called *ensembles*: if the underlying
    models are very diverse, then their errors will not be very correlated, and therefore
    averaging out the predictions will smooth out the errors, reduce overfitting,
    and improve the overall performance. The code is much the same as earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Wow, this is much better: random forests really look very promising for this
    task! However, if you train a `RandomForestRegressor` and measure the RMSE on
    the training set, you will find roughly 17,551: that’s much lower, meaning that
    there’s still quite a lot of overfitting going on. Possible solutions are to simplify
    the model, constrain it (i.e., regularize it), or get a lot more training data.
    Before you dive much deeper into random forests, however, you should try out many
    other models from various categories of machine learning algorithms (e.g., several
    support vector machines with different kernels, and possibly a neural network),
    without spending too much time tweaking the hyperparameters. The goal is to shortlist
    a few (two to five) promising models.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tune Your Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s assume that you now have a shortlist of promising models. You now need
    to fine-tune them. Let’s look at a few ways you can do that.
  prefs: []
  type: TYPE_NORMAL
- en: Grid Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One option would be to fiddle with the hyperparameters manually, until you find
    a great combination of hyperparameter values. This would be very tedious work,
    and you may not have time to explore many combinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, you can use Scikit-Learn’s `GridSearchCV` class to search for you.
    All you need to do is tell it which hyperparameters you want it to experiment
    with and what values to try out, and it will use cross-validation to evaluate
    all the possible combinations of hyperparameter values. For example, the following
    code searches for the best combination of hyperparameter values for the `RandomForestRegressor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Notice that you can refer to any hyperparameter of any estimator in a pipeline,
    even if this estimator is nested deep inside several pipelines and column transformers.
    For example, when Scikit-Learn sees `"preprocessing__geo__n_clusters"`, it splits
    this string at the double underscores, then it looks for an estimator named `"preprocessing"`
    in the pipeline and finds the preprocessing `ColumnTransformer`. Next, it looks
    for a transformer named `"geo"` inside this `ColumnTransformer` and finds the
    `ClusterSimilarity` transformer we used on the latitude and longitude attributes.
    Then it finds this transformer’s `n_clusters` hyperparameter. Similarly, `random_forest__max_features`
    refers to the `max_features` hyperparameter of the estimator named `"random_forest"`,
    which is of course the `RandomForestRegressor` model (the `max_features` hyperparameter
    will be explained in [Chapter 6](ch06.html#ensembles_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Wrapping preprocessing steps in a Scikit-Learn pipeline allows you to tune
    the preprocessing hyperparameters along with the model hyperparameters. This is
    a good thing since they often interact. For example, perhaps increasing `n_clusters`
    requires increasing `max_features` as well. If fitting the pipeline transformers
    is computationally expensive, you can set the pipeline’s `memory` parameter to
    the path of a caching directory: when you first fit the pipeline, Scikit-Learn
    will save the fitted transformers to this directory. If you then fit the pipeline
    again with the same hyperparameters, Scikit-Learn will just load the cached transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two dictionaries in this `param_grid`, so `GridSearchCV` will first
    evaluate all 3 × 3 = 9 combinations of `n_clusters` and `max_features` hyperparameter
    values specified in the first `dict`, then it will try all 2 × 3 = 6 combinations
    of hyperparameter values in the second `dict`. So in total the grid search will
    explore 9 + 6 = 15 combinations of hyperparameter values, and it will train the
    pipeline 3 times per combination, since we are using 3-fold cross validation.
    This means there will be a grand total of 15 × 3 = 45 rounds of training! It may
    take a while, but when it is done you can get the best combination of parameters
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the best model is obtained by setting `n_clusters` to 15 and
    setting `max_features` to 6.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since 15 is the maximum value that was evaluated for `n_clusters`, you should
    probably try searching again with higher values; the score may continue to improve.
  prefs: []
  type: TYPE_NORMAL
- en: You can access the best estimator using `grid_search.best_estimator_`. If `Grid​SearchCV`
    is initialized with `refit=True` (which is the default), then once it finds the
    best estimator using cross-validation, it retrains it on the whole training set.
    This is usually a good idea, since feeding it more data will likely improve its
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation scores are available using `grid_search.cv_results_`. This is
    a dictionary, but if you wrap it in a DataFrame you get a nice list of all the
    test scores for each combination of hyperparameters and for each cross-validation
    split, as well as the mean test score across all splits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: The mean test RMSE score for the best model is 43,590, which is better than
    the score you got earlier using the default hyperparameter values (which was 47,038).
    Congratulations, you have successfully fine-tuned your best model!
  prefs: []
  type: TYPE_NORMAL
- en: Randomized Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The grid search approach is fine when you are exploring relatively few combinations,
    like in the previous example, but `RandomizedSearchCV` is often preferable, especially
    when the hyperparameter search space is large. This class can be used in much
    the same way as the `GridSearchCV` class, but instead of trying out all possible
    combinations it evaluates a fixed number of combinations, selecting a random value
    for each hyperparameter at every iteration. This may sound surprising, but this
    approach has several benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: If some of your hyperparameters are continuous (or discrete but with many possible
    values), and you let randomized search run for, say, 1,000 iterations, then it
    will explore 1,000 different values for each of these hyperparameters, whereas
    grid search would only explore the few values you listed for each one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose a hyperparameter does not actually make much difference, but you don’t
    know it yet. If it has 10 possible values and you add it to your grid search,
    then training will take 10 times longer. But if you add it to a random search,
    it will not make any difference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are 6 hyperparameters to explore, each with 10 possible values, then
    grid search offers no other choice than training the model a million times, whereas
    random search can always run for any number of iterations you choose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each hyperparameter, you must provide either a list of possible values,
    or a probability distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Scikit-Learn also has `HalvingRandomSearchCV` and `HalvingGridSearchCV` hyperparameter
    search classes. Their goal is to use the computational resources more efficiently,
    either to train faster or to explore a larger hyperparameter space. Here’s how
    they work: in the first round, many hyperparameter combinations (called “candidates”)
    are generated using either the grid approach or the random approach. These candidates
    are then used to train models that are evaluated using cross-validation, as usual.
    However, training uses limited resources, which speeds up this first round considerably.
    By default, “limited resources” means that the models are trained on a small part
    of the training set. However, other limitations are possible, such as reducing
    the number of training iterations if the model has a hyperparameter to set it.
    Once every candidate has been evaluated, only the best ones go on to the second
    round, where they are allowed more resources to compete. After several rounds,
    the final candidates are evaluated using full resources. This may save you some
    time tuning hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way to fine-tune your system is to try to combine the models that perform
    best. The group (or “ensemble”) will often perform better than the best individual
    model—just like random forests perform better than the individual decision trees
    they rely on—especially if the individual models make very different types of
    errors. For example, you could train and fine-tune a *k*-nearest neighbors model,
    then create an ensemble model that just predicts the mean of the random forest
    prediction and that model’s prediction. We will cover this topic in more detail
    in [Chapter 6](ch06.html#ensembles_chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the Best Models and Their Errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will often gain good insights on the problem by inspecting the best models.
    For example, the `RandomForestRegressor` can indicate the relative importance
    of each attribute for making accurate predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s sort these importance scores in descending order and display them next
    to their corresponding attribute names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: With this information, you may want to try dropping some of the less useful
    features (e.g., apparently only one `ocean_proximity` category is really useful,
    so you could try dropping the others).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `sklearn.feature_selection.SelectFromModel` transformer can automatically
    drop the least useful features for you: when you fit it, it trains a model (typically
    a random forest), looks at its `feature_importances_` attribute, and selects the
    most useful features. Then when you call `transform()`, it drops the other features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should also look at the specific errors that your system makes, then try
    to understand why it makes them and what could fix the problem: adding extra features
    or getting rid of uninformative ones, cleaning up outliers, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now is also a good time to check *model fairness*: it should not only work
    well on average, but also on various categories of districts, whether they’re
    rural or urban, rich or poor, northern or southern, minority or not, etc. This
    requires a detailed *bias analysis*: creating subsets of your validation set for
    each category, and analyzing your model’s performance on them. That’s a lot of
    work, but it’s important: if your model performs poorly on a whole category of
    districts, then it should probably not be deployed until the issue is resolved,
    or at least it should not be used to make predictions for that category, as it
    may do more harm than good.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate Your System on the Test Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After tweaking your models for a while, you eventually have a system that performs
    sufficiently well. You are ready to evaluate the final model on the test set.
    There is nothing special about this process; just get the predictors and the labels
    from your test set and run your `final_model` to transform the data and make predictions,
    then evaluate these predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'In some cases, such a point estimate of the generalization error will not be
    quite enough to convince you to launch: what if it is just 0.1% better than the
    model currently in production? You might want to have an idea of how precise this
    estimate is. For this, you can compute a 95% *confidence interval* for the generalization
    error using `scipy.stats.bootstrap()`. You get a fairly large interval from 39,521
    to 43,702, and your previous point estimate of 41,445 is roughly in the middle
    of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: If you do a lot of hyperparameter tuning, the performance will usually be slightly
    worse than what you measured using cross-validation. That’s because your system
    ends up fine-tuned to perform well on the validation data and will likely not
    perform as well on unknown datasets. That’s not the case in this example since
    the test RMSE is lower than the validation RMSE, but when it happens you must
    resist the temptation to tweak the hyperparameters to make the numbers look good
    on the test set; the improvements would be unlikely to generalize to new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now comes the project prelaunch phase. Presenting your solution effectively
    is what sets great data scientists apart from good ones. You should create concise
    reports (Markdown, PDFs, slides), visualize key insights (e.g., using Matplotlib
    or other tools such as SeaBorn or Tableau), and tailor your message to the audience:
    technical for peers, high-level for stakeholders. Provide impactful and easy-to-remember
    statements (e.g., “the median income is the number one predictor of housing prices”).
    Highlight what you have learned, what worked and what did not, what assumptions
    were made, and what your system’s limitations are.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Your results should be reproducible (as much as possible): make the code accessible
    to your team (e.g., via GitHub), add a structured *README* file to guide a technical
    person through the installation steps. Provide clear notebooks (e.g., Jupyter)
    with code, explanations, and results, writing clean, well-commented code. Define
    a *requirements.txt* or *environment.yml* file containing all the required libraries
    along with their precise versions (or create a Docker image). Set seeds for random
    generators, and remove any other source of variability.'
  prefs: []
  type: TYPE_NORMAL
- en: In this California housing example, the final performance of the system is not
    much better than the experts’ price estimates, which were often off by 30%, but
    it may still be a good idea to launch it, especially if this frees up some time
    for the experts so they can work on more interesting and productive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Launch, Monitor, and Maintain Your System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perfect, you got approval to launch! You now need to get your solution ready
    for production (e.g., polish the code, write documentation and tests, and so on).
    Then you can deploy your model to your production environment. The most basic
    way to do this is just to save the best model you trained, transfer the file to
    your production environment, and load it. To save the model, you can use the `joblib`
    library like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s often a good idea to save every model you experiment with so that you can
    come back easily to any model you want. You may also save the cross-validation
    scores and perhaps the actual predictions on the validation set. This will allow
    you to easily compare scores across model types, and compare the types of errors
    they make.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your model is transferred to production, you can load it and use it. For
    this you must first import any custom classes and functions the model relies on
    (which means transferring the code to production), then load the model using `joblib`
    and use it to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, perhaps the model will be used within a website: the user will
    type in some data about a new district and click the Estimate Price button. This
    will send a query containing the data to the web server, which will forward it
    to your web application, and finally your code will simply call the model’s `predict()`
    method (you want to load the model upon server startup, rather than every time
    the model is used). Alternatively, you can wrap the model within a dedicated web
    service that your web application can query through a REST API⁠^([14](ch02.html#id1268))
    (see [Figure 2-21](#webservice_model_diagram)). This makes it easier to upgrade
    your model to new versions without interrupting the main application. It also
    simplifies scaling, since you can start as many web services as needed and load-balance
    the requests coming from your web application across these web services. Moreover,
    it allows your web application to use any programming language, not just Python.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing a user interacting with a web app, which sends inputs to
    a web service hosting a model and receives predictions in return.](assets/hmls_0221.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-21\. A model deployed as a web service and used by a web application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Another popular strategy is to deploy your model to the cloud, for example
    on Google’s Vertex AI (formerly Google Cloud AI Platform and Google Cloud ML Engine):
    just save your model using `joblib` and upload it to Google Cloud Storage (GCS),
    then go to Vertex AI and create a new model version, pointing it to the GCS file.
    That’s it! This gives you a simple web service that takes care of load balancing
    and scaling for you. It takes JSON requests containing the input data (e.g., of
    a district) and returns JSON responses containing the predictions. You can then
    use this web service in your website (or whatever production environment you are
    using).'
  prefs: []
  type: TYPE_NORMAL
- en: 'But deployment is not the end of the story. You also need to write monitoring
    code to check your system’s live performance at regular intervals and trigger
    alerts when it drops. It may drop very quickly, for example if a component breaks
    in your infrastructure, but be aware that it could also decay very slowly, which
    can easily go unnoticed for a long time. This is quite common because of data
    drift: if the model was trained with last year’s data, it may not be adapted to
    today’s data.'
  prefs: []
  type: TYPE_NORMAL
- en: So, you need to monitor your model’s live performance. But how do you do that?
    Well, it depends. In some cases, the model’s performance can be inferred from
    downstream metrics. For example, if your model is part of a recommender system
    and it suggests products that the users may be interested in, then it’s easy to
    monitor the number of recommended products sold each day. If this number drops
    (compared to nonrecommended products), then the prime suspect is the model. This
    may be because the data pipeline is broken, or perhaps the model needs to be retrained
    on fresh data (as we will discuss shortly).
  prefs: []
  type: TYPE_NORMAL
- en: However, you may also need human analysis to assess the model’s performance.
    For example, suppose you trained an image classification model (we’ll look at
    these in [Chapter 3](ch03.html#classification_chapter)) to detect various product
    defects on a production line. How can you get an alert if the model’s performance
    drops, before thousands of defective products get shipped to your clients? One
    solution is to send to human raters a sample of all the pictures that the model
    classified (especially pictures that the model wasn’t so sure about). Depending
    on the task, the raters may need to be experts, or they could be nonspecialists,
    such as workers on a crowdsourcing platform (e.g., Amazon Mechanical Turk). In
    some applications they could even be the users themselves, responding, for example,
    via surveys or repurposed captchas.⁠^([15](ch02.html#id1274))
  prefs: []
  type: TYPE_NORMAL
- en: Either way, you need to put in place a monitoring system (with or without human
    raters to evaluate the live model), as well as all the relevant processes to define
    what to do in case of failures and how to prepare for them. Unfortunately, this
    can be a lot of work. In fact, it is often much more work than building and training
    a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the data keeps evolving, you will need to update your datasets and retrain
    your model regularly. You should probably automate the whole process as much as
    possible. Here are a few things you can automate:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect fresh data regularly and label it (e.g., using human raters).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a script to train the model and fine-tune the hyperparameters automatically.
    This script could run automatically, for example every day or every week, depending
    on your needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write another script that will evaluate both the new model and the previous
    model on the updated test set, and deploy the model to production if the performance
    has not decreased (if it did, make sure you investigate why). The script should
    probably test the performance of your model on various subsets of the test set,
    such as poor or rich districts, rural or urban districts, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should also make sure you evaluate the model’s input data quality. Sometimes
    performance will degrade slightly because of a poor-quality signal (e.g., a malfunctioning
    sensor sending random values, or another team’s output becoming stale), but it
    may take a while before your system’s performance degrades enough to trigger an
    alert. If you monitor your model’s inputs, you may catch this earlier. For example,
    you could trigger an alert if more and more inputs are missing a feature, or the
    mean or standard deviation drifts too far from the training set, or a categorical
    feature starts containing new categories.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, make sure you keep backups of every model you create and have the process
    and tools in place to roll back to a previous model quickly, in case the new model
    starts failing badly for some reason. Having backups also makes it possible to
    easily compare new models with previous ones. Similarly, you should keep backups
    of every version of your datasets so that you can roll back to a previous dataset
    if the new one ever gets corrupted (e.g., if the fresh data that gets added to
    it turns out to be full of outliers). Having backups of your datasets also allows
    you to evaluate any model against any previous dataset.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, machine learning involves quite a lot of infrastructure. This
    is a very broad topic called *ML Operations* (MLOps), which deserves its own book.
    So don’t be surprised if your first ML project takes a lot of effort and time
    to build and deploy to production. Fortunately, once all the infrastructure is
    in place, going from idea to production will be much faster.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Out!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hopefully this chapter gave you a good idea of what a machine learning project
    looks like as well as showing you some of the tools you can use to train a great
    system. As you can see, much of the work is in the data preparation step: building
    monitoring tools, setting up human evaluation pipelines, and automating regular
    model training. The machine learning algorithms are important, of course, but
    it is probably preferable to be comfortable with the overall process and know
    three or four algorithms well rather than to spend all your time exploring advanced
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if you have not already done so, now is a good time to pick up a laptop,
    select a dataset that you are interested in, and try to go through the whole process
    from A to Z. A good place to start is on a competition website such as [Kaggle](https://kaggle.com):
    you will have a dataset to play with, a clear goal, and people to share the experience
    with. Have fun!'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following exercises are based on this chapter’s housing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Try a support vector machine regressor (`sklearn.svm.SVR`) with various hyperparameters,
    such as `kernel="linear"` (with various values for the `C` hyperparameter) or
    `kernel="rbf"` (with various values for the `C` and `gamma` hyperparameters).
    Note that support vector machines don’t scale well to large datasets, so you should
    probably train your model on just the first 5,000 instances of the training set
    and use only 3-fold cross-validation, or else it will take hours. Don’t worry
    about what the hyperparameters mean for now; these are explained in the online
    chapter on SVMs at [*https://homl.info/*](https://homl.info/). How does the best
    `SVR` predictor perform?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try replacing the `GridSearchCV` with a `RandomizedSearchCV`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try adding a `SelectFromModel` transformer in the preparation pipeline to select
    only the most important attributes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try creating a custom transformer that trains a *k*-nearest neighbors regressor
    (`sklearn.neighbors.KNeighborsRegressor`) in its `fit()` method, and outputs the
    model’s predictions in its `transform()` method. The KNN regressor should use
    only the latitude and longitude as input and predict the median income. Next,
    add this new transformer to the preprocessing pipeline. This will add a feature
    representing the smoothed median income over the nearby districts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Automatically explore some preparation options using `RandomizedSearchCV`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Try to implement the `StandardScalerClone` class again from scratch, then add
    support for the `inverse_transform()` method: executing `scaler.​inverse_transform(scaler.fit_transform(X))`
    should return an array very close to `X`. Then add support for feature names:
    set `feature_names_in_` in the `fit()` method if the input is a DataFrame. This
    attribute should be a NumPy array of column names. Lastly, implement the `get_feature_names_out()`
    method: it should have one optional `input_features=None` argument. If passed,
    the method should check that its length matches `n_features_in_`, and it should
    match `feature_names_in_` if it is defined; then `input_features` should be returned.
    If `input_features` is `None`, then the method should either return `feature_names_in_`
    if it is defined or `np.array(["x0", "x1", ...])` with length `n_features_in_`
    otherwise.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tackle a regression task of your choice by following the process you learned
    in this chapter. For example, you can try tackling the [Vehicle dataset](https://homl.info/usedcars),
    where the goal is to predict the selling price of a used car, based on its age,
    the number of kilometers it has driven, its make and model, and more. Another
    good dataset to try is the [Bike Sharing dataset](https://homl.info/bikes): the
    objective is to predict the number of bikes rented within a period of time (column
    `cnt`), based on the day of the week, the time, and the weather conditions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch02.html#id990-marker)) The original dataset appeared in R. Kelley Pace
    and Ronald Barry, “Sparse Spatial Autoregressions”, *Statistics & Probability
    Letters* 33, no. 3 (1997): 291–297.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch02.html#id994-marker)) A piece of information fed to a machine learning
    system is often called a *signal*, in reference to Claude Shannon’s information
    theory, which he developed at Bell Labs to improve telecommunications. His theory:
    you want a high signal-to-noise ratio.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch02.html#id1005-marker)) Recall that the transpose operator flips a column
    vector into a row vector (and vice versa).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch02.html#id1032-marker)) You might also need to check legal constraints,
    such as private fields that should never be copied to unsafe data stores.
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch02.html#id1038-marker)) The standard deviation is generally denoted
    *σ* (the Greek letter sigma), and it is the square root of the *variance*, which
    is the average of the squared deviation from the mean. When a feature has a bell-shaped
    *normal distribution* (also called a *Gaussian distribution*), which is very common,
    the “68-95-99.7” rule applies: about 68% of the values fall within 1*σ* of the
    mean, 95% within 2*σ*, and 99.7% within 3*σ*.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch02.html#id1054-marker)) You will often see people set the random seed
    to 42\. This number has no special property, other than being the Answer to the
    Ultimate Question of Life, the Universe, and Everything.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch02.html#id1055-marker)) The location information is actually quite coarse,
    and as a result many districts will have the exact same ID, so they will end up
    in the same set (test or train). This introduces some unfortunate sampling bias.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch02.html#id1068-marker)) If you are reading this in grayscale, grab a
    red pen and scribble over most of the coastline from the Bay Area down to San
    Diego (as you might expect). You can add a patch of yellow around Sacramento as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch02.html#id1102-marker)) For more details on the design principles,
    see Lars Buitinck et al., “API Design for Machine Learning Software: Experiences
    from the Scikit-Learn Project”, arXiv preprint arXiv:1309.0238 (2013).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch02.html#id1103-marker)) Some predictors also provide methods to measure
    the confidence of their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch02.html#id1107-marker)) If you run `sklearn.set_config(transform_output="pandas")`,
    all transformers will output Pandas DataFrames when they receive a DataFrame as
    input: Pandas in, Pandas out.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch02.html#id1118-marker)) See SciPy’s documentation for more details.
  prefs: []
  type: TYPE_NORMAL
- en: '^([13](ch02.html#id1177-marker)) With duck typing, an object’s methods and
    behavior are what matters, not its type: “if it looks like a duck and quacks like
    a duck, it must be a duck”.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch02.html#id1268-marker)) In a nutshell, a REST (or RESTful) API is an
    HTTP-based API that follows some conventions, such as using standard HTTP verbs
    to read, update, create, or delete resources (GET, POST, PUT, and DELETE) and
    using JSON for the inputs and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch02.html#id1274-marker)) A captcha is a test to ensure a user is not
    a robot. These tests have often been used as a cheap way to label training data.
  prefs: []
  type: TYPE_NORMAL
