- en: Chapter 2\. End-to-End Machine Learning Project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章\. 端到端机器学习项目
- en: 'In this chapter you will work through an example project end to end, pretending
    to be a recently hired data scientist at a real estate company. This example is
    fictitious; the goal is to illustrate the main steps of a machine learning project,
    not to learn anything about the real estate business. Here are the main steps
    we will walk through:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将从头到尾完成一个示例项目，假装你是一家房地产公司新聘请的数据科学家。这个例子是虚构的；目的是说明机器学习项目的关键步骤，而不是学习有关房地产业务的知识。以下是我们将要经历的步骤：
- en: Look at the big picture.
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看清大局。
- en: Get the data.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据。
- en: Explore and visualize the data to gain insights.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索和可视化数据以获得洞察。
- en: Prepare the data for machine learning algorithms.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为机器学习算法准备数据。
- en: Select a model and train it.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个模型并对其进行训练。
- en: Fine-tune your model.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调你的模型。
- en: Present your solution.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展示你的解决方案。
- en: Launch, monitor, and maintain your system.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动、监控和维护你的系统。
- en: Working with Real Data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理真实数据
- en: 'When you are learning about machine learning, it is best to experiment with
    real-world data, not artificial datasets. Fortunately, there are thousands of
    open datasets to choose from, ranging across all sorts of domains. Here are a
    few popular open data repositories you can use to get data:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当你学习机器学习时，最好是使用真实世界的数据进行实验，而不是人工数据集。幸运的是，有数千个开放数据集可供选择，涵盖各种领域。以下是一些你可以用来获取数据的流行开放数据存储库：
- en: '[Google Datasets Search](https://datasetsearch.research.google.com)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Google数据集搜索](https://datasetsearch.research.google.com)'
- en: '[Hugging Face Datasets](https://huggingface.co/docs/datasets)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hugging Face数据集](https://huggingface.co/docs/datasets)'
- en: '[OpenML.org](https://openml.org)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenML.org](https://openml.org)'
- en: '[Kaggle.com](https://kaggle.com/datasets)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle.com](https://kaggle.com/datasets)'
- en: '[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[UC Irvine机器学习存储库](https://archive.ics.uci.edu)'
- en: '[Stanford Large Network Dataset Collection](https://snap.stanford.edu/data)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[斯坦福大学大型网络数据集收藏](https://snap.stanford.edu/data)'
- en: '[Amazon’s AWS datasets](https://registry.opendata.aws)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Amazon的AWS数据集](https://registry.opendata.aws)'
- en: '[U.S. Government’s Open Data](https://data.gov)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[美国政府的开放数据](https://data.gov)'
- en: '[DataPortals.org](https://dataportals.org)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DataPortals.org](https://dataportals.org)'
- en: '[Wikipedia’s list of machine learning datasets](https://homl.info/9)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[维基百科的机器学习数据集列表](https://homl.info/9)'
- en: In this chapter we’ll use the California Housing Prices dataset from the StatLib
    repository⁠^([1](ch02.html#id990)) (see [Figure 2-1](#california_housing_prices_plot)).
    This dataset is based on data from the 1990 California census. It is not exactly
    recent (a nice house in the Bay Area was still affordable at the time), but it
    has many qualities for learning, so we will pretend it is recent data. For teaching
    purposes I’ve added a categorical attribute and removed a few features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用来自StatLib存储库的加利福尼亚房价数据集（见[图2-1](#california_housing_prices_plot)）。这个数据集基于1990年加利福尼亚的人口普查数据。它并不完全是最新的（当时旧金山湾区的房子仍然负担得起），但它具有许多学习特性，所以我们将假装它是最新数据。为了教学目的，我添加了一个分类属性并删除了一些特征。
- en: '![Map of California displaying housing price data with colored dots representing
    median house values and dot size indicating population density.](assets/hmls_0201.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![显示加利福尼亚房价数据的地图，用彩色点表示平均房价，点的大小表示人口密度。](assets/hmls_0201.png)'
- en: Figure 2-1\. California housing prices
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 加利福尼亚房价
- en: Look at the Big Picture
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 看清大局
- en: Welcome to the Machine Learning Housing Corporation! Your first task is to use
    California census data to build a model of housing prices in the state. This data
    includes metrics such as the population, median income, and median housing price
    for each block group in California. Block groups are the smallest geographical
    unit for which the US Census Bureau publishes sample data (a block group typically
    has a population of 600 to 3,000 people). I will call them “districts” for short.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到机器学习住房公司！你的第一个任务是使用加利福尼亚人口普查数据来构建该州房价的模型。这些数据包括加利福尼亚每个街区组的指标，如人口、平均收入和平均房价。街区组是美国人口普查局发布样本数据的最小地理单位（街区组通常有600到3000人）。我将简称为“地区”。
- en: Your model should learn from this data and be able to predict the median housing
    price in any district, given all the other metrics.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型应该从这些数据中学习，并能够预测任何地区的平均房价，给定所有其他指标。
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Since you are a well-organized data scientist, the first thing you should do
    is pull out your machine learning project checklist. You can start with the one
    at [*https://homl.info/checklist*](https://homl.info/checklist); it should work
    reasonably well for most machine learning projects, but make sure to adapt it
    to your needs. In this chapter we will go through many checklist items, but we
    will also skip a few, either because they are self-explanatory or because they
    will be discussed in later chapters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您是一位组织有序的数据科学家，您应该做的第一件事是拿出您的机器学习项目清单。您可以从[*https://homl.info/checklist*](https://homl.info/checklist)上的清单开始；它应该对大多数机器学习项目都适用，但请确保根据您的需求进行调整。在本章中，我们将讨论许多清单项，但也会跳过一些，要么是因为它们是自我解释的，要么是因为它们将在后面的章节中讨论。
- en: Frame the Problem
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 明确问题
- en: The first question to ask your boss is what exactly the business objective is.
    Building a model is probably not the end goal. How does the company expect to
    use and benefit from this model? Knowing the objective is important because it
    will determine how you frame the problem, which algorithms you will select, which
    performance measure you will use to evaluate your model, and how much effort you
    will spend tweaking it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您首先需要问您的老板业务目标是什么。构建模型可能不是最终目标。公司期望如何使用和从该模型中获益？了解目标是重要的，因为它将决定您如何明确问题，您将选择哪些算法，您将使用哪些性能指标来评估您的模型，以及您将投入多少精力来调整它。
- en: Your boss answers that your model’s output (a prediction of a district’s median
    housing price) will be essential to determine whether it is worth investing in
    a given area. More specifically, your model’s output will be fed to another machine
    learning system (see [Figure 2-2](#house_pricing_pipeline_diagram)), along with
    some other signals.⁠^([2](ch02.html#id994)) So it’s important to make our housing
    price model as accurate as we can.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 您的老板回答说，您的模型输出（一个对某个地区中位数房价的预测）对于确定是否值得在该地区投资至关重要。更具体地说，您的模型输出将被输入到另一个机器学习系统中（见[图2-2](#house_pricing_pipeline_diagram)），以及其他一些信号。⁠^([2](ch02.html#id994))
    因此，使我们的房价模型尽可能准确是很重要的。
- en: 'The next question to ask your boss is what the current solution looks like
    (if any). The current situation will often give you a reference for performance,
    as well as insights on how to solve the problem. Your boss answers that the district
    housing prices are currently estimated manually by experts: a team gathers up-to-date
    information about a district, and when they cannot get the median housing price,
    they estimate it using complex rules.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来您需要问您的老板当前解决方案（如果有的话）是什么样子。当前情况通常会为您提供性能的参考，以及如何解决问题的见解。您的老板回答说，目前地区房价是由专家手动估算的：一个团队收集关于某个地区的最新信息，当他们无法获得中位数房价时，他们会使用复杂的规则进行估算。
- en: '![Diagram showing a machine learning pipeline for real estate, highlighting
    data flow from district data to district pricing, investment analysis, and investments.](assets/hmls_0202.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![展示房地产机器学习流程的图表，突出从地区数据到地区定价、投资分析和投资的流程。](assets/hmls_0202.png)'
- en: Figure 2-2\. A machine learning pipeline for real estate investments
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. 房地产投资的机器学习流程
- en: This is costly and time-consuming, and their estimates are not great; in cases
    where they manage to find out the actual median housing price, they often realize
    that their estimates were off by more than 30%. This is why the company thinks
    that it would be useful to train a model to predict a district’s median housing
    price, given other data about that district. The census data looks like a great
    dataset to exploit for this purpose, since it includes the median housing prices
    of thousands of districts, as well as other data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这既昂贵又耗时，他们的估算并不理想；在那些他们设法找到实际中位数房价的情况下，他们常常发现他们的估算误差超过30%。这就是为什么公司认为，训练一个模型来预测某个地区的中位数房价，给定该地区的其他数据，将是有用的。人口普查数据看起来是一个很好的数据集，可以用于此目的，因为它包括了数千个地区的中位数房价以及其他数据。
- en: 'With all this information, you are now ready to start designing your system.
    First, determine what kind of training supervision the model will need: is it
    a supervised, unsupervised, semi-supervised, self-supervised, or reinforcement
    learning task? And is it a classification task, a regression task, or something
    else? Should you use batch learning or online learning techniques? Before you
    read on, pause and try to answer these questions for yourself.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得所有这些信息后，你现在可以开始设计你的系统了。首先，确定模型需要的训练监督类型：是监督学习、无监督学习、半监督学习、自监督学习还是强化学习任务？它是分类任务、回归任务还是其他任务？你应该使用批处理学习还是在线学习技术？在你继续阅读之前，暂停一下，试着为自己回答这些问题。
- en: Have you found the answers? Let’s see. This is clearly a typical supervised
    learning task, since the model can be trained with *labeled* examples (each instance
    comes with the expected output, i.e., the district’s median housing price). It
    is a typical regression task, since the model will be asked to predict a value.
    More specifically, this is a *multiple regression* problem, since the system will
    use multiple features to make a prediction (the district’s population, the median
    income, etc.). It is also a *univariate regression* problem, since we are only
    trying to predict a single value for each district. If we were trying to predict
    multiple values per district, it would be a *multivariate regression* problem.
    Finally, there is no continuous flow of data coming into the system, there is
    no particular need to adjust to changing data rapidly, and the data is small enough
    to fit in memory, so plain batch learning should do just fine.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你找到答案了吗？让我们看看。这显然是一个典型的监督学习任务，因为模型可以用*标记*的例子进行训练（每个实例都带有预期的输出，即该地区的平均房价）。它是一个典型的回归任务，因为模型将被要求预测一个值。更具体地说，这是一个*多元回归*问题，因为系统将使用多个特征进行预测（地区的总人口，平均收入等）。它也是一个*单变量回归*问题，因为我们只尝试预测每个地区的单个值。如果我们试图为每个地区预测多个值，那么它将是一个*多元回归*问题。最后，系统中没有连续的数据流进入，没有特别需要快速调整数据的需求，而且数据量足够小，可以放入内存中，所以普通的批处理学习应该就足够了。
- en: Tip
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If the data were huge, you could either split your batch learning work across
    multiple servers (using the MapReduce technique) or use an online learning technique.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据量巨大，你可以选择将批处理学习工作分散到多个服务器上（使用MapReduce技术）或者使用在线学习技术。
- en: Select a Performance Measure
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择性能指标
- en: Your next step is to select a performance measure. A typical performance measure
    for regression problems is the *root mean squared error* (RMSE). It gives an idea
    of how much error the system typically makes in its predictions, with a higher
    weight given to large errors. [Equation 2-1](#rmse_equation) shows the mathematical
    formula to compute the RMSE.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你的下一步是选择一个性能指标。回归问题的典型性能指标是*均方根误差*（RMSE）。它给出了系统在预测中通常犯多少错误的估计，对大误差给予更高的权重。[方程式
    2-1](#rmse_equation)显示了计算RMSE的数学公式。
- en: Equation 2-1\. Root mean squared error (RMSE)
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 2-1\. 均方根误差 (RMSE)
- en: $RMSE left-parenthesis bold upper X comma bold y comma h right-parenthesis equals
    StartRoot StartFraction 1 Over m EndFraction sigma-summation Underscript i equals
    1 Overscript m Endscripts left-parenthesis h left-parenthesis bold x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis minus y Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis squared EndRoot$
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: $RMSE\left(\boldsymbol{X}, \boldsymbol{y}, h\right) = \sqrt{\frac{1}{m} \sum_{i=1}^{m}
    \left(h\left(\boldsymbol{x}^{(i)}\right) - \boldsymbol{y}^{(i)}\right)^2}$
- en: 'Although the RMSE is generally the preferred performance measure for regression
    tasks, in some contexts you may prefer to use another function, especially when
    there are many outliers in the data, as the RMSE is quite sensitive to them. In
    that case, you may consider using the *mean absolute error* (MAE, also called
    the *average absolute deviation*), shown in [Equation 2-2](#mae_equation):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RMSE通常是回归任务的首选性能指标，但在某些情况下，你可能更愿意使用另一个函数，尤其是在数据中存在许多异常值时，因为RMSE对它们相当敏感。在这种情况下，你可能考虑使用*均值绝对误差*（MAE，也称为*平均绝对偏差*），如[方程式
    2-2](#mae_equation)所示：
- en: Equation 2-2\. Mean absolute error (MAE)
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 2-2\. 均值绝对误差 (MAE)
- en: $MAE left-parenthesis bold upper X comma bold y comma h right-parenthesis equals
    StartFraction 1 Over m EndFraction sigma-summation Underscript i equals 1 Overscript
    m Endscripts StartAbsoluteValue h left-parenthesis bold x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis minus y Superscript left-parenthesis
    i right-parenthesis Baseline EndAbsoluteValue$
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $MAE left-parenthesis bold upper X comma bold y comma h right-parenthesis equals
    StartFraction 1 Over m EndFraction sigma-summation Underscript i equals 1 Overscript
    m Endscripts StartAbsoluteValue h left-parenthesis bold x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis minus y Superscript left-parenthesis
    i right-parenthesis Baseline EndAbsoluteValue$
- en: 'Both the RMSE and the MAE are ways to measure the distance between two vectors:
    the vector of predictions and the vector of target values. Various distance measures,
    or *norms*, are possible:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE 和 MAE 都是衡量两个向量之间距离的方法：预测向量和目标值向量。可能的距离度量或 *范数* 有很多：
- en: 'Computing the root of a sum of squares (RMSE) corresponds to the *Euclidean
    norm*: this is the notion of distance we are all familiar with. It is also called
    the ℓ[2] *norm*, denoted ∥ · ∥[2] (or just ∥ · ∥).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算平方和的平方根（RMSE）对应于 *欧几里得范数*：这是我们所有人都熟悉的距离概念。它也被称为 ℓ[2] *范数*，表示为 ∥ · ∥[2]（或简称
    ∥ · ∥）。
- en: Computing the sum of absolutes (MAE) corresponds to the ℓ[1] *norm*, denoted
    ∥ · ∥[1]. This is sometimes called the *Manhattan norm* because it measures the
    distance between two points in a city if you can only travel along orthogonal
    city blocks.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算绝对值之和（MAE）对应于 ℓ[1] *范数*，表示为 ∥ · ∥[1]。这有时被称为 *曼哈顿范数*，因为它衡量了在只能沿正交城市街区行走的条件下，两个点之间的距离。
- en: More generally, the ℓ[*k*] *norm* of a vector **v** containing *n* elements
    is defined as ∥**v**∥[*k*] = (|*v*[1]|^(*k*) + |*v*[2]|^(*k*) + ... + |*v*[*n*]|^(*k*))^(1/*k*).
    ℓ[0] gives the number of nonzero elements in the vector, and ℓ[∞] gives the maximum
    absolute value in the vector.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更一般地，包含 *n* 个元素的向量 **v** 的 ℓ[*k*] *范数* 定义为 ∥**v**∥[*k*] = (|*v*[1]|^(*k*) +
    |*v*[2]|^(*k*) + ... + |*v*[*n*]|^(*k*))^(1/*k*). ℓ[0] 表示向量中非零元素的数量，而 ℓ[∞] 表示向量中的最大绝对值。
- en: The higher the norm index, the more it focuses on large values and neglects
    small ones. This is why the RMSE is more sensitive to outliers than the MAE. But
    when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs
    very well and is generally preferred.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 范数指数越高，它就越关注大值而忽略小值。这就是为什么 RMSE 比 MAE 更敏感于异常值。但当异常值指数级罕见（如钟形曲线中）时，RMSE 表现得非常好，并且通常更受欢迎。
- en: Check the Assumptions
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查假设
- en: Lastly, it is good practice to list and verify the assumptions that have been
    made so far (by you or others); this can help you catch serious issues early on.
    For example, the district prices that your system outputs are going to be fed
    into a downstream machine learning system, and you assume that these prices are
    going to be used as such. But what if the downstream system converts the prices
    into categories (e.g., “cheap”, “medium”, or “expensive”) and then uses those
    categories instead of the prices themselves? In this case, getting the price perfectly
    right is not important at all; your system just needs to get the category right.
    If that’s so, then the problem should have been framed as a classification task,
    not a regression task. You don’t want to find this out after working on a regression
    system for months.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，列出并验证到目前为止（由你或他人）所做出的假设是一个好的实践；这可以帮助你及早发现严重问题。例如，你的系统输出的区域价格将被输入到下游机器学习系统中，你假设这些价格将被用作此类。但如果是下游系统将价格转换为类别（例如，“便宜”、“中等”或“昂贵”）然后使用这些类别而不是价格本身呢？在这种情况下，价格完全正确并不重要；你的系统只需要正确分类。如果真是这样，那么问题应该被界定为分类任务，而不是回归任务。你不想在为回归系统工作了数月之后才发现这一点。
- en: Fortunately, after talking with the team in charge of the downstream system,
    you are confident that they do indeed need the actual prices, not just categories.
    Great! You’re all set, the lights are green, and you can start coding now!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在与负责下游系统的团队交谈后，你确信他们确实需要实际的价格，而不仅仅是类别。太好了！一切准备就绪，绿灯亮起，你现在可以开始编码了！
- en: Get the Data
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: It’s time to get your hands dirty. Don’t hesitate to pick up your laptop and
    walk through the code examples. As I mentioned in the preface, all the code examples
    in this book are open source and available [online](https://github.com/ageron/handson-mlp)
    as Jupyter notebooks, which are interactive documents containing text, images,
    and executable code snippets (Python in our case). In this book I will assume
    you are running these notebooks on Google Colab, a free service that lets you
    run any Jupyter notebook directly online, without having to install anything on
    your machine. If you want to use another online platform (e.g., Kaggle) or if
    you want to install everything locally on your own machine, please see the instructions
    on the book’s GitHub page.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候动手实践了。不要犹豫，拿起您的笔记本电脑，浏览代码示例。正如我在前言中提到的，本书中的所有代码示例都是开源的，并以Jupyter笔记本的形式[在线](https://github.com/ageron/handson-mlp)提供，这些是包含文本、图像和可执行代码片段（在我们的案例中是Python）的交互式文档。在这本书中，我将假设您正在Google
    Colab上运行这些笔记本，这是一个免费服务，允许您直接在线运行任何Jupyter笔记本，而无需在您的机器上安装任何东西。如果您想使用其他在线平台（例如Kaggle）或如果您想在您的机器上本地安装所有内容，请参阅本书GitHub页面上的说明。
- en: Running the Code Examples Using Google Colab
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Google Colab运行代码示例
- en: 'First, open a web browser and visit [*https://homl.info/colab-p*](https://homl.info/colab-p):
    this will lead you to Google Colab, and it will display the list of Jupyter notebooks
    for this book (see [Figure 2-3](#google_colab_notebook_list)). You will find one
    notebook per chapter, plus a few extra notebooks and tutorials for NumPy, Matplotlib,
    Pandas, linear algebra, and differential calculus. For example, if you click *02_end_to_end_machine_learning_project.ipynb*,
    the notebook from [Chapter 2](#project_chapter) will open up in Google Colab (see
    [Figure 2-4](#notebook_in_colab)).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，打开一个网页浏览器并访问[*https://homl.info/colab-p*](https://homl.info/colab-p)：这将带您进入Google
    Colab，并显示本书的Jupyter笔记本列表（见[图2-3](#google_colab_notebook_list)）。您将找到每个章节的一个笔记本，以及一些额外的NumPy、Matplotlib、Pandas、线性代数和微分计算的笔记本和教程。例如，如果您点击*02_end_to_end_machine_learning_project.ipynb*，来自[第2章](#project_chapter)的笔记本将在Google
    Colab中打开（见[图2-4](#notebook_in_colab)）。
- en: A Jupyter notebook is composed of a list of cells. Each cell contains either
    executable code or text. Try double-clicking the first text cell (which contains
    the sentence “Welcome to Machine Learning Housing Corp.!”). This will open the
    cell for editing. Notice that Jupyter notebooks use Markdown syntax for formatting
    (e.g., `**bold**`, `*italics*`, `# Title`, `[url](link text)`, and so on). Try
    modifying this text, then press Shift-Enter to see the result.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本由一系列单元格组成。每个单元格包含可执行代码或文本。尝试双击第一个文本单元格（包含句子“欢迎来到机器学习住房公司！”）。这将打开单元格以进行编辑。请注意，Jupyter笔记本使用Markdown语法进行格式化（例如，`**粗体**`，`*斜体*`，`#
    标题`，`[url](链接文本)`等）。尝试修改此文本，然后按Shift-Enter查看结果。
- en: '![Google Colab interface showing a list of Jupyter notebooks in the "ageron/handson-mlp"
    repository on GitHub, with "02_end_to_end_machine_learning_project.ipynb" highlighted.](assets/hmls_0203.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![Google Colab界面显示GitHub上"ageron/handson-mlp"存储库中的Jupyter笔记本列表，其中"02_end_to_end_machine_learning_project.ipynb"突出显示。](assets/hmls_0203.png)'
- en: Figure 2-3\. List of notebooks in Google Colab
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. Google Colab中的笔记本列表
- en: '![Screenshot of a Google Colab notebook showing a section titled "Chapter 2
    – End-to-end Machine Learning project," with instructions for editing and running
    text and code cells.](assets/hmls_0204.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![Google Colab笔记本的截图，显示标题为“第2章 – 端到端机器学习项目”的部分，包含编辑和运行文本和代码单元格的说明。](assets/hmls_0204.png)'
- en: Figure 2-4\. Your notebook in Google Colab
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. 您在Google Colab中的笔记本
- en: Next, create a new code cell by selecting Insert → “Code cell” from the menu.
    Alternatively, you can click the + Code button in the toolbar, or hover your mouse
    over the bottom of a cell until you see + Code and + Text appear, then click +
    Code. In the new code cell, type some Python code, such as `print("Hello World")`,
    then press Shift-Enter to run this code (or click the ▷ button on the left side
    of the cell).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过选择菜单中的“插入”→“代码单元格”来创建一个新的代码单元格。或者，您也可以点击工具栏上的+代码按钮，或者将鼠标悬停在单元格底部直到出现+代码和+文本，然后点击+代码。在新的代码单元格中，输入一些Python代码，例如`print("Hello
    World")`，然后按Shift-Enter运行此代码（或点击单元格左侧的▷按钮）。
- en: If you’re not logged in to your Google account, you’ll be asked to log in now
    (if you don’t already have a Google account, you’ll need to create one). Once
    you are logged in, when you try to run the code you’ll see a security warning
    telling you that this notebook was not authored by Google. A malicious person
    could create a notebook that tries to trick you into entering your Google credentials
    so they can access your personal data, so before you run a notebook, always make
    sure you trust its author (or double-check what each code cell will do before
    running it). Assuming you trust me (or you plan to check every code cell), you
    can now click “Run anyway”.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未登录到您的 Google 账户，您现在将被要求登录（如果您还没有 Google 账户，您需要创建一个）。一旦您登录，当您尝试运行代码时，您将看到一个安全警告，告诉您这个笔记本不是由
    Google 编写的。恶意的人可能会创建一个试图诱骗您输入您的 Google 凭据的笔记本，以便他们可以访问您的个人数据，因此在运行笔记本之前，请始终确保您信任其作者（或者运行之前双重检查每个代码单元格将执行的操作）。假设您信任我（或者您计划检查每个代码单元格），现在您可以点击“继续运行”。
- en: 'Colab will then allocate a new *runtime* for you: this is a free virtual machine
    located on Google’s servers that contains a bunch of tools and Python libraries,
    including everything you’ll need for most chapters (in some chapters, you’ll need
    to run a command to install additional libraries). This will take a few seconds.
    Next, Colab will automatically connect to this runtime and use it to execute your
    new code cell. Importantly, the code runs on the runtime, *not* on your machine.
    The code’s output will be displayed under the cell. Congrats, you’ve run some
    Python code on Colab!'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 Colab 将为您分配一个新的 *运行时*：这是一个位于 Google 服务器上的免费虚拟机，其中包含许多工具和 Python 库，包括您在大多数章节中所需的所有内容（在某些章节中，您需要运行命令来安装额外的库）。这需要几秒钟。接下来，Colab
    将自动连接到这个运行时并使用它来执行您的新代码单元格。重要的是，代码是在运行时上运行的，*而不是* 在您的机器上。代码的输出将显示在单元格下方。恭喜，您已经在
    Colab 上运行了一些 Python 代码！
- en: Tip
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'To insert a new code cell, you can also type Ctrl-M (or Cmd-M on macOS) followed
    by A (to insert above the active cell) or B (to insert below). There are many
    other keyboard shortcuts available: you can view and edit them by typing Ctrl-M
    (or Cmd-M) then H. If you choose to run the notebooks on Kaggle or on your own
    machine using JupyterLab or an IDE such as Visual Studio Code with the Jupyter
    extension, you will see some minor differences—runtimes are called *kernels*,
    the user interface and keyboard shortcuts are slightly different, etc.—but switching
    from one Jupyter environment to another is not too hard.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要插入一个新的代码单元格，您也可以输入 Ctrl-M（或在 macOS 上为 Cmd-M）后跟 A（在活动单元格上方插入）或 B（在活动单元格下方插入）。还有许多其他的快捷键可用：您可以通过输入
    Ctrl-M（或在 macOS 上为 Cmd-M）然后 H 来查看和编辑它们。如果您选择在 Kaggle 或在自己的机器上使用 JupyterLab 或带有
    Jupyter 扩展的 IDE（如 Visual Studio Code）运行笔记本，您将看到一些细微的差异——运行时被称为 *kernels*，用户界面和快捷键略有不同等——但从一种
    Jupyter 环境切换到另一种并不太难。
- en: Saving Your Code Changes and Your Data
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存代码更改和您的数据
- en: You can make changes to a Colab notebook, and they will persist for as long
    as you keep your browser tab open. But once you close it, the changes will be
    lost. To avoid this, make sure you save a copy of the notebook to your Google
    Drive by selecting File → “Save a copy in Drive”. Alternatively, you can download
    the notebook to your computer by selecting File → Download → “Download .ipynb”.
    Then you can later visit [*https://colab.research.google.com*](https://colab.research.google.com)
    and open the notebook again (either from Google Drive or by uploading it from
    your computer).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以对 Colab 笔记本进行修改，并且只要您保持浏览器标签页打开，这些修改就会持续存在。但一旦您关闭它，修改就会丢失。为了避免这种情况，请确保通过选择“文件”→“在
    Drive 中保存副本”来将笔记本的副本保存到您的 Google Drive。或者，您也可以通过选择“文件”→“下载”→“下载 .ipynb”将笔记本下载到您的电脑上。然后您可以在稍后访问[*https://colab.research.google.com*](https://colab.research.google.com)并再次打开笔记本（无论是从
    Google Drive 还是上传到您的电脑）。
- en: Warning
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Google Colab is meant only for interactive use: you can play around in the
    notebooks and tweak the code as you like, but you cannot let the notebooks run
    unattended for a long period of time, or else the runtime will be shut down and
    all of its data will be lost.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab 仅适用于交互式使用：您可以在笔记本中随意操作并按需调整代码，但您不能让笔记本长时间无人看管，否则运行时将被关闭，所有数据都将丢失。
- en: If the notebook generates data that you care about, make sure you download this
    data before the runtime shuts down. To do this, click the Files icon (see step
    1 in [Figure 2-5](#save_data_google_colab)), find the file you want to download,
    click the vertical dots next to it (step 2), and click Download (step 3). Alternatively,
    you can mount your Google Drive on the runtime, allowing the notebook to read
    and write files directly to Google Drive as if it were a local directory. For
    this, click the Files icon (step 1), then click the Google Drive icon (circled
    in [Figure 2-5](#save_data_google_colab)) and follow the on-screen instructions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果笔记本生成了你关心的数据，确保在运行时关闭之前下载这些数据。为此，点击文件图标（见[图2-5](#save_data_google_colab)中的步骤1），找到你想要下载的文件，点击它旁边的垂直点（步骤2），然后点击下载（步骤3）。或者，你可以在运行时挂载你的Google
    Drive，允许笔记本直接将文件读写到Google Drive，就像它是本地目录一样。为此，点击文件图标（步骤1），然后点击[图2-5](#save_data_google_colab)中圈出的Google
    Drive图标，并按照屏幕上的说明操作。
- en: '![Screenshot of Google Colab interface showing steps to download a file or
    mount Google Drive, with icons and menu options highlighted.](assets/hmls_0205.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Google Colab界面截图，显示下载文件或挂载Google Drive的步骤，图标和菜单选项被突出显示。](assets/hmls_0205.png)'
- en: Figure 2-5\. Downloading a file from a Google Colab runtime (steps 1 to 3),
    or mounting your Google Drive (circled icon)
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. 从Google Colab运行时下载文件（步骤1到3），或挂载你的Google Drive（圈出的图标）
- en: 'By default, your Google Drive will be mounted at */content/drive/MyDrive*.
    If you want to back up a data file, simply copy it to this directory by running
    `!cp [.keep-together]#/content/my_great_model /content/drive/MyDrive`.# Any command
    starting with a bang (`!`) is treated as a shell command, not as Python code:
    `cp` is the Linux shell command to copy a file from one path to another. Note
    that Colab runtimes run on Linux (specifically, Ubuntu).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，你的Google Drive将挂载在*/content/drive/MyDrive*。如果你想备份一个数据文件，只需通过运行`!cp [.keep-together]#/content/my_great_model
    /content/drive/MyDrive`将其复制到这个目录。# 任何以感叹号（`!`）开头的命令都被视为shell命令，而不是Python代码：“cp”是Linux
    shell命令，用于将文件从一个路径复制到另一个路径。请注意，Colab运行时在Linux上运行（具体来说，是Ubuntu）。
- en: The Power and Danger of Interactivity
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互式功能的强大与危险
- en: 'Jupyter notebooks are interactive, and that’s a great thing: you can run each
    cell one by one, stop at any point, insert a cell, play with the code, go back
    and run the same cell again, etc., and I highly encourage you to do so. If you
    just run the cells one by one without ever playing around with them, you won’t
    learn as fast. However, this flexibility comes at a price: it’s very easy to run
    cells in the wrong order, or to forget to run a cell. If this happens, the subsequent
    code cells are likely to fail. For example, the very first code cell in each notebook
    contains setup code (such as imports), so make sure you run it first, or else
    nothing will work.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本是交互式的，这是一个非常好的特性：你可以逐个运行每个单元格，在任何地方停止，插入一个单元格，玩弄代码，返回并再次运行相同的单元格，等等，我强烈鼓励你这样做。如果你只是逐个运行单元格而不进行任何操作，你学得不会那么快。然而，这种灵活性是有代价的：很容易以错误的顺序运行单元格，或者忘记运行某个单元格。如果发生这种情况，后续的代码单元格很可能会失败。例如，每个笔记本中的第一个代码单元格包含设置代码（例如导入），所以请确保你首先运行它，否则什么都不会工作。
- en: Tip
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you ever run into a weird error, try restarting the runtime (by selecting
    Runtime → “Restart runtime” from the menu) and then run all the cells again from
    the beginning of the notebook. This often solves the problem. If not, it’s likely
    that one of the changes you made broke the notebook: just revert to the original
    notebook and try again. If it still fails, please file an issue on GitHub.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到任何奇怪的错误，尝试重新启动运行时（通过从菜单中选择“运行时”→“重新启动运行时”），然后从笔记本的开头重新运行所有单元格。这通常可以解决问题。如果不行，很可能是你做的某个更改破坏了笔记本：只需恢复到原始笔记本并再次尝试。如果仍然失败，请在GitHub上提交一个问题。
- en: Book Code Versus Notebook Code
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 书本代码与笔记本代码
- en: 'You may sometimes notice some little differences between the code in this book
    and the code in the notebooks. This may happen for several reasons:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你有时可能会注意到这本书中的代码和笔记本中的代码之间有一些小小的差异。这可能由几个原因造成：
- en: 'A library may have changed slightly by the time you read these lines, or perhaps
    despite my best efforts I made an error in the book. Sadly, I cannot magically
    fix the code in your copy of this book (unless you are reading an electronic copy
    and you can download the latest version), but I *can* fix the notebooks. So, if
    you run into an error after copying code from this book, please look for the fixed
    code in the notebooks: I will strive to keep them error-free and up-to-date with
    the latest library versions.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你阅读这些行时，库可能已经略有变化，或者也许尽管我尽了最大努力，但在书中我可能犯了一个错误。遗憾的是，我无法神奇地修复你这本书中的代码（除非你正在阅读电子版，并且可以下载最新版本），但我*可以*修复笔记本。所以，如果你从这本书中复制代码后遇到错误，请查找笔记本中的修复代码：我将努力确保它们没有错误，并且与最新库版本保持更新。
- en: The notebooks contain some extra code to beautify the figures (adding labels,
    setting font sizes, etc.) and to save them in high resolution for this book. You
    can safely ignore this extra code if you want.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本中包含一些额外的代码，用于美化图形（添加标签、设置字体大小等），并将它们以高分辨率保存到这本书中。如果你想忽略这些额外的代码，可以安全地忽略它们。
- en: 'I optimized the code for readability and simplicity: I made it as linear and
    flat as possible, defining very few functions or classes. The goal is to ensure
    that the code you are running is generally right in front of you, and not nested
    within several layers of abstractions that you have to search through. This also
    makes it easier for you to play with the code. For simplicity, there’s limited
    error handling, and I placed some of the least common imports right where they
    are needed (instead of placing them at the top of the file, as is recommended
    by the PEP 8 Python style guide). That said, your production code will not be
    very different: just a bit more modular, and with additional tests and error handling.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我优化了代码的可读性和简洁性：我使其尽可能线性和平坦，定义了非常少的函数或类。目标是确保你运行的代码通常就在你面前，而不是嵌套在几层抽象中，你需要搜索。这也使得你可以更容易地玩转代码。为了简单起见，错误处理有限，我将一些不太常见的导入放在了它们需要的位置（而不是像PEP
    8 Python风格指南推荐的那样放在文件顶部）。话虽如此，你的生产代码不会有很大不同：只是稍微模块化一些，并增加了额外的测试和错误处理。
- en: OK! Once you’re comfortable with Colab, you’re ready to download the data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！一旦你对Colab感到舒适，你就可以下载数据了。
- en: Download the Data
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载数据
- en: 'In typical environments your data would be available in a relational database
    or some other common data store, and spread across multiple tables/documents/files.
    To access it, you would first need to get your credentials and access authorizations
    and familiarize yourself with the data schema.⁠^([4](ch02.html#id1032)) In this
    project, however, things are much simpler: you will just download a single compressed
    file, *housing.tgz*, which contains a comma-separated values (CSV) file called
    *housing.csv* with all the data.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的环境中，你的数据将存储在关系数据库或其他常见的数据存储中，并分布在多个表/文档/文件中。要访问它，你首先需要获取你的凭证和访问授权，并熟悉数据模式。⁠^([4](ch02.html#id1032))
    然而，在这个项目中，事情要简单得多：你只需下载一个单个的压缩文件，*housing.tgz*，它包含一个名为*housing.csv*的逗号分隔值（CSV）文件，其中包含所有数据。
- en: 'Rather than manually downloading and decompressing the data, it’s usually preferable
    to write a function that does it for you. This is useful in particular if the
    data changes regularly: you can write a small script that uses the function to
    fetch the latest data (or you can set up a scheduled job to do that automatically
    at regular intervals). Automating the process of fetching the data is also useful
    if you need to install the dataset on multiple machines.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动下载和解压缩数据相比，通常更可取的是编写一个为你完成这些操作的函数。特别是如果数据经常变化，这很有用：你可以编写一个小脚本，使用该函数获取最新数据（或者你可以设置一个计划任务，定期自动执行该操作）。如果需要将数据集安装到多台机器上，自动化获取数据的过程也很有用。
- en: 'Here is the function to fetch and load the data:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是获取和加载数据的函数：
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you get an SSL `CERTIFICATE_VERIFY_FAILED` error on macOS, then you most
    likely need to install the `certifi` package, as explained at [*https://homl.info/sslerror*](https://homl.info/sslerror).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到SSL `CERTIFICATE_VERIFY_FAILED`错误，那么你很可能是需要安装`certifi`包，如[*https://homl.info/sslerror*](https://homl.info/sslerror)中所述。
- en: When `load_housing_data()` is called, it looks for the *datasets/housing.tgz*
    file. If it does not find it, it creates the *datasets* directory inside the current
    directory (which is */content* by default, in Colab), downloads the *housing.tgz*
    file from the *ageron/data* GitHub repository, and extracts its content into the
    *datasets* directory; this creates the *datasets*/*housing* directory with the
    *housing.csv* file inside it. Lastly, the function loads this CSV file into a
    Pandas DataFrame object containing all the data, and returns it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用 `load_housing_data()` 时，它会寻找 *datasets/housing.tgz* 文件。如果找不到，它将在当前目录（在 Colab
    中默认为 */content*）内创建 *datasets* 目录，从 *ageron/data* GitHub 仓库下载 *housing.tgz* 文件，并将其内容提取到
    *datasets* 目录中；这将在 *datasets*/*housing* 目录中创建一个包含 *housing.csv* 文件的目录。最后，该函数将这个
    CSV 文件加载到一个包含所有数据的 Pandas DataFrame 对象中，并返回它。
- en: Note
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you are using Python 3.12 or 3.13, you should add `filter=''data''` to the
    `extractall()` method’s arguments: this limits what the extraction algorithm can
    do and improves security (see the documentation for more details).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 Python 3.12 或 3.13，你应该在 `extractall()` 方法的参数中添加 `filter='data'`：这限制了提取算法可以执行的操作并提高了安全性（更多详情请参阅文档）。
- en: Take a Quick Look at the Data Structure
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速查看数据结构
- en: You start by looking at the top five rows of data using the DataFrame’s `head()`
    method (see [Figure 2-6](#housing_head_screenshot)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先使用 DataFrame 的 `head()` 方法查看数据的前五行（见 [图 2-6](#housing_head_screenshot)）。
- en: '![A screenshot showing the top five rows of a housing dataset, including attributes
    such as longitude, latitude, housing median age, median income, ocean proximity,
    and median house value.](assets/hmls_0206.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![显示住房数据集前五行的一个截图，包括经度、纬度、住房中位数年龄、中位数收入、海洋接近度和中位数房价等属性。](assets/hmls_0206.png)'
- en: Figure 2-6\. Top five rows in the dataset
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 数据集的前五行
- en: 'Each row represents one district. There are 10 attributes (they are not all
    shown in the screenshot): `longitude`, `latitude`, `housing_median_age`, `total_rooms`,
    `total_bedrooms`, `population`, `households`, `median_income`, `median_house_value`,
    and `ocean_proximity`.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 每行代表一个区域。有 10 个属性（它们并非都在截图显示）：`longitude`、`latitude`、`housing_median_age`、`total_rooms`、`total_bedrooms`、`population`、`households`、`median_income`、`median_house_value`
    和 `ocean_proximity`。
- en: 'The `info()` method is useful to get a quick description of the data, in particular
    the total number of rows, each attribute’s type, and the number of non-null values:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`info()` 方法很有用，可以快速获取数据的描述，特别是总行数、每个属性的类型以及非空值的数量：'
- en: '[PRE1]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'In this book, when a code example contains a mix of code and outputs, as is
    the case here, it is formatted like in the Python interpreter for better readability:
    the code lines are prefixed with `>>>` (or `...` for indented blocks), and the
    outputs have no prefix.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，当代码示例包含代码和输出的混合时，就像这里一样，它将被格式化为 Python 解释器中的格式，以便更好地阅读：代码行以 `>>>`（或缩进块的
    `...`）为前缀，而输出没有前缀。
- en: There are 20,640 instances in the dataset, which means that it is fairly small
    by machine learning standards, but it’s perfect to get started. You notice that
    the `total_bedrooms` attribute has only 20,433 non-null values, meaning that 207
    districts are missing this feature. You will need to take care of this later.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有 20,640 个实例，这意味着按照机器学习标准来看，它相当小，但非常适合入门。你可能会注意到 `total_bedrooms` 属性只有 20,433
    个非空值，这意味着有 207 个区域缺少这个特征。你需要在稍后处理这个问题。
- en: 'All attributes are numerical, except for `ocean_proximity`. Its type is `object`,
    so it could hold any kind of Python object. But since you loaded this data from
    a CSV file, you know that it must be a text attribute. When you looked at the
    top five rows, you probably noticed that the values in the `ocean_proximity` column
    were repetitive, which means that it is probably a categorical attribute. You
    can find out what categories exist and how many districts belong to each category
    by using the `value_counts()` method:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所有属性都是数值型，除了 `ocean_proximity`。它的类型是 `object`，所以它可以包含任何类型的 Python 对象。但既然你从 CSV
    文件中加载了这些数据，你就知道它必须是一个文本属性。当你查看前五行时，你可能会注意到 `ocean_proximity` 列中的值是重复的，这意味着它可能是一个分类属性。你可以使用
    `value_counts()` 方法找出存在的分类以及每个分类包含多少个区域：
- en: '[PRE2]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s look at the other fields. The `describe()` method shows a summary of the
    numerical attributes ([Figure 2-7](#housing_describe_screenshot)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看其他字段。`describe()` 方法显示了数值属性的摘要（[图 2-7](#housing_describe_screenshot)）。
- en: '![A table generated by the `describe()` method, displaying statistical summaries
    for numerical attributes such as count, mean, standard deviation, minimum, maximum,
    and percentiles.](assets/hmls_0207.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![由`describe()`方法生成的表格，显示数值属性（如计数、平均值、标准差、最小值、最大值和百分位数）的统计摘要。](assets/hmls_0207.png)'
- en: Figure 2-7\. Summary of each numerical attribute
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7。每个数值属性的摘要
- en: 'The `count`, `mean`, `min`, and `max` rows are self-explanatory. Note that
    the null values are ignored (so, for example, the `count` of `total_bedrooms`
    is 20,433, not 20,640). The `std` row shows the *standard deviation*, which measures
    how dispersed the values are.⁠^([5](ch02.html#id1038)) The `25%`, `50%`, and `75%`
    rows show the corresponding *percentiles*: a percentile indicates the value below
    which a given percentage of observations in a group of observations fall. For
    example, 25% of the districts have a `housing_median_age` lower than 18, while
    50% are lower than 29, and 75% are lower than 37\. These are often called the
    25th percentile (or first *quartile*), the median, and the 75th percentile (or
    third quartile).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`count`、`mean`、`min`和`max`行具有自解释性。请注意，空值被忽略（例如，`total_bedrooms`的`count`为20,433，而不是20,640）。`std`行显示的是*标准差*，它衡量值如何分散。⁠^([5](ch02.html#id1038))
    `25%`、`50%`和`75%`行显示相应的*百分位数*：百分位数表示在观察组中，给定百分比的数据点低于该值。例如，25%的区域`housing_median_age`低于18岁，而50%低于29岁，75%低于37岁。这些通常被称为第25百分位数（或第一*四分位数*）、中位数和第75百分位数（或第三四分位数）。'
- en: Another quick way to get a feel of the type of data you are dealing with is
    to plot a histogram for each numerical attribute. A histogram shows the number
    of instances (on the vertical axis) that have a given value range (on the horizontal
    axis). You can either plot this one attribute at a time, or you can call the `hist()`
    method on the whole dataset (as shown in the following code example), and it will
    plot a histogram for each numerical attribute (see [Figure 2-8](#attribute_histogram_plots)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种快速了解你正在处理的数据类型的方法是为每个数值属性绘制直方图。直方图显示了具有给定值范围（在水平轴上）的实例数（在垂直轴上）。你可以一次绘制一个属性，或者可以在整个数据集上调用`hist()`方法（如下面的代码示例所示），它将为每个数值属性绘制直方图（参见[图2-8](#attribute_histogram_plots)）。
- en: '![Histograms displaying the distribution of various numerical attributes such
    as longitude, latitude, housing median age, total rooms, total bedrooms, population,
    households, median income, and median house value.](assets/hmls_0208.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![显示各种数值属性（如经度、纬度、住房中位数年龄、总房间数、总卧室数、人口、家庭、中位数收入和中位数房价）分布的直方图。](assets/hmls_0208.png)'
- en: Figure 2-8\. A histogram for each numerical attribute
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8。每个数值属性的直方图
- en: 'The number of value ranges can be adjusted using the `bins` argument (try playing
    with it to see how it affects the histograms):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`bins`参数调整值范围的数目（尝试调整它以查看它如何影响直方图）：
- en: '[PRE3]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Looking at these histograms, you notice a few things:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这些直方图，你会注意到一些事情：
- en: First, the median income attribute does not look like it is expressed in US
    dollars (USD). After checking with the team that collected the data, you are told
    that the data has been scaled and capped at 15 (actually, 15.0001) for higher
    median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers
    represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000).
    Working with preprocessed attributes is common in machine learning, and it is
    not necessarily a problem, but you should try to understand how the data was computed.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，中位收入属性看起来不像是以美元（USD）表示的。在与收集数据的团队确认后，你被告知数据已被缩放并限制在15（实际上是15.0001）以上，对于较低的中位收入限制在0.5（实际上是0.4999）。这些数字代表大约数万美元（例如，3实际上意味着大约30,000美元）。在机器学习中使用预处理属性很常见，这并不一定是问题，但你应该尝试理解数据是如何计算的。
- en: 'The housing median age and the median house value were also capped. The latter
    may be a serious problem since it is your target attribute (your labels). Your
    machine learning algorithms may learn that prices never go beyond that limit.
    You need to check with your client team (the team that will use your system’s
    output) to see if this is a problem or not. If they tell you that they need precise
    predictions even beyond $500,000, then you have two options:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 住房中位数年龄和中位数房价也被限制。后者可能是一个严重的问题，因为它是你目标属性（你的标签）。你的机器学习算法可能会学习到价格永远不会超过那个限制。你需要与你的客户团队（将使用你的系统输出的团队）确认，看看这是否是一个问题。如果他们告诉你，他们需要精确预测甚至超过500,000美元，那么你有两个选择：
- en: Collect proper labels for the districts whose labels were capped.
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集那些标签被截断的区域的正确标签。
- en: Remove those districts from the training set (and also from the test set, since
    your system should not be evaluated poorly if it predicts values beyond $500,000).
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从训练集（以及测试集，因为如果你的系统预测的值超过 $500,000$，那么它应该不会表现得太差）中移除那些区域。
- en: These attributes have very different scales. We will discuss this later in this
    chapter when we explore feature scaling.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些属性具有非常不同的尺度。我们将在本章后面讨论特征缩放时再讨论这个问题。
- en: 'Finally, many histograms are *skewed right*: they extend much farther to the
    right of the median than to the left. This may make it a bit harder for some machine
    learning algorithms to detect patterns. Later, you’ll try transforming these attributes
    to have more symmetrical and bell-shaped distributions.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，许多直方图是*右偏的*：它们在均值右侧延伸得比左侧远得多。这可能会让某些机器学习算法检测模式变得有点困难。稍后，你将尝试将这些属性转换成更对称和钟形分布。
- en: You should now have a better understanding of the kind of data you’re dealing
    with.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该对你要处理的数据类型有了更好的理解。
- en: Create a Test Set
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建测试集
- en: 'Before you look at the data any further, you need to create a test set, put
    it aside, and never look at it. It may seem strange to voluntarily set aside part
    of the data at this stage. After all, you have only taken a quick glance at the
    data, and surely you should learn a whole lot more about it before you decide
    what algorithms to use, right? This is true, but your brain is an amazing pattern
    detection system, which also means that it is highly prone to overfitting: if
    you look at the test set, you may stumble upon some seemingly interesting pattern
    in the test data that leads you to select a particular kind of machine learning
    model. When you estimate the generalization error using the test set, your estimate
    will be too optimistic, and you will launch a system that will not perform as
    well as expected. This is called *data snooping* bias.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在你进一步查看数据之前，你需要创建一个测试集，将其放在一边，并且永远不要再看它。在这个阶段自愿将数据的一部分放在一边似乎很奇怪。毕竟，你只是快速浏览了一下数据，在你决定使用哪些算法之前，你当然应该更多地了解它，对吧？这是真的，但你的大脑是一个惊人的模式检测系统，这也意味着它非常容易过拟合：如果你查看测试集，你可能会在测试数据中发现一些看似有趣的模式，这会让你选择特定的机器学习模型。当你使用测试集估计泛化误差时，你的估计将过于乐观，你将启动一个表现不如预期的系统。这被称为*数据窥探*偏差。
- en: 'Creating a test set is theoretically simple; pick some instances randomly,
    typically 20% of the dataset (or less if your dataset is very large), and set
    them aside:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 创建测试集在理论上很简单；随机选择一些实例，通常是数据集的20%（如果数据集非常大，则更少），并将它们放在一边：
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can then use this function like this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这个函数如下：
- en: '[PRE5]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Well, this works, but it is not perfect: if you run the program again, it will
    generate a different test set! Over time, you (or your machine learning algorithms)
    will get to see the whole dataset, which is what you want to avoid.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这行得通，但并不完美：如果你再次运行程序，它将生成不同的测试集！随着时间的推移，你（或你的机器学习算法）将看到整个数据集，这正是你想要避免的。
- en: One solution is to save the test set on the first run and then load it in subsequent
    runs. Another option is to set the random number generator’s seed (e.g., by passing
    `seed=42` to the `default_rng()` function)⁠^([6](ch02.html#id1054)) to ensure
    it always generates the same sequence of random numbers every time you run the
    program.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是在第一次运行时保存测试集，然后在后续运行中加载它。另一个选项是设置随机数生成器的种子（例如，通过将 `seed=42` 传递给 `default_rng()`
    函数）⁠^([6](ch02.html#id1054))，以确保每次运行程序时都生成相同的随机数序列。
- en: However, both these solutions will break the next time you fetch an updated
    dataset. To have a stable train/test split even after updating the dataset, a
    common solution is to use each instance’s identifier to decide whether it should
    go in the test set (assuming instances have unique and immutable identifiers).
    For example, you could compute a hash of each instance’s identifier and put that
    instance in the test set if the hash is lower than or equal to 20% of the maximum
    hash value. This ensures that the test set will remain consistent across multiple
    runs, even if you refresh the dataset. The new test set will contain 20% of the
    new instances, but it will not contain any instance that was previously in the
    training set.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这两种解决方案在下次获取更新后的数据集时都会失效。为了在更新数据集后仍然有一个稳定的训练/测试分割，一个常见的解决方案是使用每个实例的标识符来决定它是否应该进入测试集（假设实例具有唯一且不可变的标识符）。例如，你可以计算每个实例标识符的哈希值，并将该实例放入测试集，如果哈希值小于或等于最大哈希值的20%。这确保了测试集在多次运行中保持一致性，即使你刷新了数据集。新的测试集将包含20%的新实例，但不会包含任何之前在训练集中的实例。
- en: 'Here is a possible implementation:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个可能的实现：
- en: '[PRE6]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Unfortunately, the housing dataset does not have an identifier column. The
    simplest solution is to use the row index as the ID:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，住房数据集没有标识符列。最简单的解决方案是使用行索引作为ID：
- en: '[PRE7]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you use the row index as a unique identifier, you need to make sure that
    new data gets appended to the end of the dataset and that no row ever gets deleted.
    If this is not possible, then you can try to use the most stable features to build
    a unique identifier. For example, a district’s latitude and longitude are guaranteed
    to be stable for a few million years, so you could combine them into an ID like
    so:⁠^([7](ch02.html#id1055))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用行索引作为唯一标识符，你需要确保新数据被追加到数据集的末尾，并且永远不会删除任何行。如果这不可能实现，那么你可以尝试使用最稳定的特征来构建一个唯一标识符。例如，一个地区的纬度和经度在几百万年内是保证稳定的，因此你可以将它们组合成一个ID，如下所示：⁠^([7](ch02.html#id1055))
- en: '[PRE8]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Scikit-Learn provides a few functions to split datasets into multiple subsets
    in various ways. The simplest function is `train_test_split()`, which does pretty
    much the same thing as the `shuffle_and_split_data()` function we defined earlier,
    with a couple of additional features. First, there is a `random_state` parameter
    that allows you to set the random generator seed. Second, you can pass it multiple
    datasets with an identical number of rows, and it will split them on the same
    indices (this is very useful, for example, if you have a separate DataFrame for
    labels):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了一些函数，可以将数据集以多种方式分割成多个子集。最简单的函数是`train_test_split()`，它几乎与我们在前面定义的`shuffle_and_split_data()`函数做的是同样的事情，但增加了一些额外的功能。首先，有一个`random_state`参数，允许你设置随机生成器的种子。其次，你可以传递多个具有相同行数的数据集，并且它将在相同的索引上分割它们（这在某些情况下非常有用，例如，如果你有一个单独的DataFrame用于标签）：
- en: '[PRE9]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'So far we have considered purely random sampling methods. This is generally
    fine if your dataset is large enough (especially relative to the number of attributes),
    but if it is not, you run the risk of introducing a significant sampling bias.
    When employees at a survey company decide to call 1,000 people to ask them a few
    questions, they don’t just pick 1,000 people randomly in a phone book. They try
    to ensure that these 1,000 people are representative of the whole population,
    with regard to the questions they want to ask. For example, according to the US
    Census Bureau, 51.6% of citizens of voting age are female, while 48.4% are male,
    so a well-conducted survey in the US would try to maintain this ratio in the sample:
    516 females and 484 males (at least if it seems likely that the answers may vary
    across genders). This is called *stratified sampling*: the population is divided
    into homogeneous subgroups called *strata*, and the right number of instances
    are sampled from each stratum to guarantee that the test set is representative
    of the overall population. If the people running the survey used purely random
    sampling, there would be over 10% chance of sampling a skewed test set with less
    than 49% female or more than 54% female participants. Either way, the survey results
    would likely be quite biased.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们考虑了纯随机抽样方法。如果你的数据集足够大（尤其是相对于属性数量），这通常是可行的，但如果不这样，你可能会引入显著的抽样偏差。当调查公司的员工决定给1000人打电话，问他们一些问题时，他们不会在电话簿中随机挑选1000人。他们试图确保这1000人能代表整个群体，针对他们想要询问的问题。例如，根据美国人口普查局的数据，51.6%的投票年龄公民是女性，而48.4%是男性，因此在美国进行的一项良好调查会试图在样本中保持这一比例：516名女性和484名男性（至少如果答案可能因性别而异的话）。这被称为*分层抽样*：人口被划分为同质子群体，称为*层*，并从每个层中抽取适当数量的实例，以确保测试集能代表整体人口。如果进行调查的人使用纯随机抽样，那么抽取一个女性比例低于49%或高于54%的测试集的概率会超过10%。无论如何，调查结果可能会非常偏颇。
- en: 'Suppose you’ve chatted with some experts who told you that the median income
    is a very important attribute to predict median housing prices. You may want to
    ensure that the test set is representative of the various categories of incomes
    in the whole dataset. Since the median income is a continuous numerical attribute,
    you first need to create an income category attribute. Let’s look at the median
    income histogram more closely (back in [Figure 2-8](#attribute_histogram_plots)):
    most median income values are clustered around 1.5 to 6 (i.e., $15,000–$60,000),
    but some median incomes go far beyond 6\. It is important to have a sufficient
    number of instances in your dataset for each stratum, or else the estimate of
    a stratum’s importance may be biased. This means that you should not have too
    many strata, and each stratum should be large enough. The following code uses
    the `pd.cut()` function to create an income category attribute with five categories
    (labeled from 1 to 5); category 1 ranges from 0 to 1.5 (i.e., less than $15,000),
    category 2 from 1.5 to 3, and so on:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经和一些专家交谈过，他们告诉你中位数收入是预测中位数房价的一个重要属性。你可能想确保测试集能代表整个数据集中各种收入类别。由于中位数收入是一个连续的数值属性，你首先需要创建一个收入类别属性。让我们更仔细地看看中位数收入直方图（回到[图2-8](#attribute_histogram_plots)）：大多数中位数收入值集中在1.5到6之间（即，$15,000–$60,000），但有些中位数收入远远超过6。在你的数据集中为每个层有足够数量的实例是很重要的，否则对层重要性的估计可能会偏颇。这意味着你不应该有太多的层，并且每个层应该足够大。以下代码使用`pd.cut()`函数创建了一个有五个类别（从1到5标记）的收入类别属性；类别1的范围是0到1.5（即，低于$15,000），类别2从1.5到3，依此类推：
- en: '[PRE10]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'These income categories are represented in [Figure 2-9](#housing_income_cat_bar_plot):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这些收入类别在[图2-9](#housing_income_cat_bar_plot)中表示：
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now you are ready to do stratified sampling based on the income category. Scikit-Learn
    provides a number of splitter classes in the `sklearn.model_selection` package
    that implement various strategies to split your dataset into a training set and
    a test set. Each splitter has a `split()` method that returns an iterator over
    different training/test splits of the same data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以根据收入类别进行分层抽样了。Scikit-Learn在`sklearn.model_selection`包中提供了一些分割器类，它们实现了将数据集分割为训练集和测试集的各种策略。每个分割器都有一个`split()`方法，它返回对相同数据的不同训练/测试分割的迭代器。
- en: '![Bar chart illustrating the distribution of districts across five income categories,
    with categories two and three having the highest counts.](assets/hmls_0209.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![展示五个收入类别中地区分布的条形图，其中第二和第三个类别的计数最高](assets/hmls_0209.png)'
- en: Figure 2-9\. Histogram of income categories
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-9. 收入类别直方图
- en: 'To be precise, the `split()` method yields the training and test *indices*,
    not the data itself. Having multiple splits can be useful if you want to better
    estimate the performance of your model, as you will see when we discuss cross-validation
    later in this chapter. For example, the following code generates 10 different
    stratified splits of the same dataset:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更精确，`split()`方法返回的是训练和测试*索引*，而不是数据本身。如果你想要更好地估计模型的性能，多个分割可能会有用，正如我们在本章后面讨论交叉验证时将看到的。例如，以下代码生成了10个不同的分层分割的相同数据集：
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For now, you can just use the first split:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你可以只使用第一个分割：
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Or, since stratified sampling is fairly common, there’s a shorter way to get
    a single split using the `train_test_split()` function with the `stratify` argument:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，由于分层抽样相当常见，你可以使用带有`stratify`参数的`train_test_split()`函数来通过更短的方式获得单个分割：
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s see if this worked as expected. You can start by looking at the income
    category proportions in the test set:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是否如预期那样工作。你可以从查看测试集中的收入类别比例开始：
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With similar code you can measure the income category proportions in the full
    dataset. [Figure 2-10](#compare_sampling_errors_screenshot) compares the income
    category proportions in the overall dataset, in the test set generated with stratified
    sampling, and in a test set generated using purely random sampling. As you can
    see, the test set generated using stratified sampling has income category proportions
    almost identical to those in the full dataset, whereas the test set generated
    using purely random sampling is skewed.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类似的代码，你可以测量整个数据集中收入类别的比例。[图2-10](#compare_sampling_errors_screenshot)比较了整体数据集、使用分层抽样生成的测试集以及使用纯随机抽样生成的测试集中的收入类别比例。如图所示，使用分层抽样生成的测试集的收入类别比例几乎与整个数据集中的比例相同，而使用纯随机抽样生成的测试集则存在偏差。
- en: '![Table comparing income category proportions in the overall dataset, stratified
    sampling, and random sampling, highlighting lower errors in stratified sampling.](assets/hmls_0210.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![比较整体数据集、分层抽样和随机抽样中收入类别比例的表格，突出分层抽样中的低误差](assets/hmls_0210.png)'
- en: Figure 2-10\. Sampling bias comparison of stratified versus purely random sampling
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-10. 分层抽样与纯随机抽样的采样偏差比较
- en: 'You won’t use the `income_cat` column again, so you might as well drop it,
    reverting the data back to its original state:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你将不再使用`income_cat`列，所以你可以将其删除，将数据恢复到其原始状态：
- en: '[PRE16]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We spent quite a bit of time on test set generation for a good reason: this
    is an often neglected but critical part of a machine learning project. Moreover,
    many of these ideas will be useful later when we discuss cross-validation. Now
    it’s time to move on to the next stage: exploring the data.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试集生成上花费了大量时间，这是有充分理由的：这是机器学习项目中经常被忽视但至关重要的一个部分。此外，许多这些想法将在我们后面讨论交叉验证时变得有用。现在，是时候进入下一阶段：探索数据。
- en: Explore and Visualize the Data to Gain Insights
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索和可视化数据以获得洞察
- en: So far you have only taken a quick glance at the data to get a general understanding
    of the kind of data you are manipulating. Now the goal is to go into a little
    more depth.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你只是快速浏览了数据，以获得对所处理数据类型的一般了解。现在目标是深入一些。
- en: 'First, make sure you have put the test set aside and you are only exploring
    the training set. Also, if the training set is very large, you may want to sample
    an exploration set, to make manipulations easy and fast during the exploration
    phase. In this case, the training set is quite small, so you can just work directly
    on the full set. Since you’re going to experiment with various transformations
    of the full training set, you should make a copy of the original so you can revert
    to it afterwards:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确保你已经将测试集放在一边，并且你只探索训练集。此外，如果训练集非常大，你可能想要采样一个探索集，以便在探索阶段轻松快速地进行操作。在这种情况下，训练集相当小，所以你可以直接在完整集上工作。由于你将要对整个训练集的各种转换进行实验，你应该制作一个原始副本，以便之后可以恢复：
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Visualizing Geographical Data
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化地理数据
- en: 'Because the dataset includes geographical information (latitude and longitude),
    it is a good idea to create a scatterplot of all the districts to visualize the
    data ([Figure 2-11](#bad_visualization_plot)):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集包括地理信息（纬度和经度），创建所有地区的散点图以可视化数据是个好主意（[图2-11](#bad_visualization_plot)）：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Scatter plot displaying geographical data points with longitude on the x-axis
    and latitude on the y-axis, showing a distribution resembling California.](assets/hmls_0211.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示地理数据点，经度在x轴上，纬度在y轴上，分布类似于加利福尼亚](assets/hmls_0211.png)'
- en: Figure 2-11\. A geographical scatterplot of the data
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-11. 数据的地理散点图
- en: 'This looks like California all right, but other than that it is hard to see
    any particular pattern. Setting the `alpha` option to `0.2` makes it much easier
    to visualize the places where there is a high density of data points ([Figure 2-12](#better_visualization_plot)):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来确实像加利福尼亚，但除此之外，很难看出任何特定的模式。将`alpha`选项设置为`0.2`使得可视化数据点密集的地方变得容易多了（[图2-12](#better_visualization_plot)）：
- en: '[PRE19]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that’s much better: you can clearly see the high-density areas, namely
    the Bay Area and around Los Angeles and San Diego, plus a long line of fairly
    high-density areas in the Central Valley (in particular, around Sacramento and
    Fresno).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在好多了：你可以清楚地看到高密度区域，即旧金山湾区以及洛杉矶和圣地亚哥周围，以及中央谷地中一条相当高密度区域的漫长线条（特别是在萨克拉门托和弗雷斯诺周围）。
- en: Our brains are very good at spotting patterns in pictures, but you may need
    to play around with visualization parameters to make the patterns stand out.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑在图片中寻找模式方面非常擅长，但你可能需要调整可视化参数以使模式更加突出。
- en: '![Scatter plot showing the distribution of housing locations by latitude and
    longitude, illustrating high-density areas with darker blue clusters.](assets/hmls_0212.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示按纬度和经度分布的住房位置，用较深的蓝色簇表示高密度区域](assets/hmls_0212.png)'
- en: Figure 2-12\. A better visualization that highlights high-density areas
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-12. 更好的可视化，突出显示高密度区域
- en: Next, you look at the housing prices ([Figure 2-13](#housing_prices_scatterplot)).
    The radius of each circle represents the district’s population (option `s`), and
    the color represents the price (option `c`). Here you use a predefined color map
    (option `cmap`) called `jet`, which ranges from blue (low values) to red (high
    prices):⁠^([8](ch02.html#id1068))
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你查看房价（[图2-13](#housing_prices_scatterplot)）。每个圆的半径代表该地区的总人口（选项`s`），颜色代表价格（选项`c`）。在这里，你使用一个预定义的颜色映射（选项`cmap`）称为`jet`，范围从蓝色（低值）到红色（高价）：⁠^([8](ch02.html#id1068))
- en: '[PRE20]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This image tells you that the housing prices are very much related to the location
    (e.g., close to the ocean) and to the population density, as you probably knew
    already. A clustering algorithm should be useful for detecting the main cluster
    and for adding new features that measure the proximity to the cluster centers.
    The ocean proximity attribute may be useful as well, although in Northern California
    the housing prices in coastal districts are not too high, so it is not a simple
    rule.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片告诉你，房价与位置（例如，靠近海洋）和人口密度有很大关系，你可能已经知道了。聚类算法应该有助于检测主要簇并添加测量簇中心接近度的特征。海洋接近度属性也可能很有用，尽管在加利福尼亚北部，沿海地区的房价并不太高，因此这不是一个简单的规则。
- en: '![Scatter plot showing California housing prices by location, with red indicating
    expensive areas and blue indicating cheaper ones; larger circles represent areas
    with larger population density.](assets/hmls_0213.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示加利福尼亚房价按位置分布，红色表示昂贵地区，蓝色表示较便宜的地区；较大的圆圈代表人口密度较大的地区](assets/hmls_0213.png)'
- en: 'Figure 2-13\. California housing prices: red is expensive, blue is cheap, larger
    circles indicate areas with a larger population'
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-13. 加利福尼亚房价：红色表示昂贵，蓝色表示便宜，较大的圆圈表示人口较多的地区
- en: Look for Correlations
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找相关性
- en: 'Since the dataset is not too large, you can easily compute the *standard correlation
    coefficient* (also called *Pearson’s r*) between every pair of numerical attributes
    using the `corr()` method:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集不是很大，你可以轻松地使用`corr()`方法计算每对数值属性之间的*标准相关系数*（也称为*皮尔逊相关系数*）：
- en: '[PRE21]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now you can look at how much each attribute correlates with the median house
    value:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以查看每个属性与中值房价的相关程度：
- en: '[PRE22]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The correlation coefficient ranges from –1 to 1\. When it is close to 1, it
    means that there is a strong positive correlation; for example, the median house
    value tends to go up when the median income goes up. When the coefficient is close
    to –1, it means that there is a strong negative correlation; you can see a small
    negative correlation between the latitude and the median house value (i.e., prices
    have a slight tendency to go down when you go north). Finally, coefficients close
    to 0 mean that there is no linear correlation.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 相关系数范围从-1到1。当它接近1时，表示存在强烈的正相关；例如，当中等收入上升时，中等房价往往会上升。当系数接近-1时，表示存在强烈的负相关；你可以看到纬度与中等房价之间存在轻微的负相关（即，价格有轻微的下降趋势，当你向北走时）。最后，接近0的系数表示没有线性相关性。
- en: 'Another way to check for correlation between attributes is to use the Pandas
    `scatter_matrix()` function, which plots every numerical attribute against every
    other numerical attribute. Since there are now 9 numerical attributes, you would
    get 9² = 81 plots, which would not fit on a page—so you decide to focus on a few
    promising attributes that seem most correlated with the median housing value ([Figure 2-14](#scatter_matrix_plot)):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 检查属性之间的相关性还有另一种方法，即使用Pandas的`scatter_matrix()`函数，该函数将每个数值属性与每个其他数值属性进行绘图。由于现在有9个数值属性，你会得到81个绘图，这不会适合在一页纸上显示——因此你决定专注于几个似乎与中等住房价值最相关的有希望的属性（[图2-14](#scatter_matrix_plot)）：
- en: '[PRE23]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Scatter matrix displaying pairwise comparisons of median house value, median
    income, total rooms, and housing median age, with histograms on the diagonals.](assets/hmls_0214.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![散点矩阵显示中等房价、中等收入、总房间数和住房中位数年龄的双变量比较，对角线上有直方图](assets/hmls_0214.png)'
- en: Figure 2-14\. This scatter matrix plots every numerical attribute against every
    other numerical attribute, plus a histogram of each numerical attribute’s values
    on the main diagonal (top left to bottom right)
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-14\. 这个散点矩阵将每个数值属性与每个其他数值属性进行对比，同时在主对角线上（从左上角到底右角）显示每个数值属性的值直方图
- en: The main diagonal would be full of straight lines if Pandas plotted each variable
    against itself, which would not be very useful. So instead, the Pandas displays
    a histogram of each attribute (other options are available; see the Pandas documentation
    for more details).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Pandas将每个变量与其自身进行绘图，主对角线将充满直线，这不会很有用。因此，Pandas显示每个属性的直方图（其他选项也可用；有关更多详细信息，请参阅Pandas文档）。
- en: 'Looking at the correlation scatterplots, it seems like the most promising attribute
    to predict the median house value is the median income, so you zoom in on that
    scatterplot ([Figure 2-15](#income_vs_house_value_scatterplot)):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 观察相关性散点图，似乎预测中等房价最有希望的属性是中等收入，因此你放大了该散点图（[图2-15](#income_vs_house_value_scatterplot)）：
- en: '[PRE24]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Scatter plot showing the correlation between median income and median house
    value, highlighting an upward trend and visible price caps at specific values.](assets/hmls_0215.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示中等收入与中等房价之间的相关性，突出显示上升趋势和特定值处的可见价格上限](assets/hmls_0215.png)'
- en: Figure 2-15\. Median income versus median house value
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-15\. 中等收入与中等房价对比
- en: 'This plot reveals a few things. First, the correlation is indeed quite strong;
    you can clearly see the upward trend, although the data is noisy. Second, the
    price cap you noticed earlier is clearly visible as a horizontal line at $500,000\.
    But the plot also reveals other less obvious straight lines: a horizontal line
    around $450,000, another around $350,000, perhaps one around $280,000, and a few
    more below that. You may want to try removing the corresponding districts to prevent
    your algorithms from learning to reproduce these data quirks.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 此图揭示了一些事情。首先，相关性确实相当强；你可以清楚地看到上升趋势，尽管数据有噪声。其次，你之前注意到的价格上限在$500,000处清楚地显示为一条水平线。但该图还揭示了其他不太明显的直线：大约在$450,000处的一条水平线，另一条大约在$350,000处，可能还有一条大约在$280,000处，以及一些更低的。你可能想要尝试删除相应的区域，以防止你的算法学习到这些数据异常。
- en: 'Note that the correlation coefficient only measures linear correlations (“as
    *x* goes up, *y* generally goes up/down”). It may completely miss out on nonlinear
    relationships (e.g., “as *x* approaches 0, *y* generally goes up”). [Figure 2-16](#correlation_coefficient_plots)
    shows a variety of datasets along with their correlation coefficient. Note how
    all the plots of the bottom row have a correlation coefficient equal to 0, despite
    the fact that their axes are clearly *not* independent: these are examples of
    nonlinear relationships. Also, the second row shows examples where the correlation
    coefficient is equal to 1 or –1; notice that this has nothing to do with the slope.
    For example, your height in inches has a correlation coefficient of 1 with your
    height in feet or in nanometers.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，相关系数仅衡量线性相关性（“当*x*增加时，*y*通常增加/减少”）。它可能完全忽略非线性关系（例如，“当*x*接近0时，*y*通常增加”）。[图2-16](#correlation_coefficient_plots)显示了各种数据集及其相关系数。注意，底部所有图表的相关系数都等于0，尽管它们的轴显然*不是*独立的：这些都是非线性关系的例子。此外，第二行显示了相关系数等于1或-1的例子；请注意，这与斜率无关。例如，你的英寸身高与你的英尺身高或纳米身高有1的相关系数。
- en: '![Scatter plots of various datasets showing how correlation coefficients can
    indicate strong linear relationships in some cases but fail to capture nonlinear
    relationships, with coefficients ranging from -1 to 1.](assets/hmls_0216.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![各种数据集的散点图，显示相关系数在某些情况下可以指示强烈的线性关系，但无法捕捉非线性关系，系数范围从-1到1。](assets/hmls_0216.png)'
- en: 'Figure 2-16\. Standard correlation coefficient of various datasets (source:
    Wikipedia; public domain image)'
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-16\. 各种数据集的标准相关系数（来源：维基百科；公有领域图片）
- en: Experiment with Attribute Combinations
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试属性组合
- en: Hopefully the previous sections gave you an idea of a few ways you can explore
    the data and gain insights. You identified a few data quirks that you may want
    to clean up before feeding the data to a machine learning algorithm, and you found
    interesting correlations between attributes, in particular with the target attribute.
    You also noticed that some attributes have a skewed-right distribution, so you
    may want to transform them (e.g., by computing their logarithm or square root).
    Of course, your mileage will vary considerably with each project, but the general
    ideas are similar.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 希望前几节能给你一些启发，让你了解一些探索数据并获取洞察的方法。你识别出了一些可能需要清理的数据异常，在将数据输入机器学习算法之前，你发现了属性之间的有趣相关性，特别是与目标属性的相关性。你还注意到一些属性具有偏右分布，因此你可能想要对它们进行转换（例如，通过计算它们的对数或平方根）。当然，每个项目的里程数可能会有很大差异，但总体思路是相似的。
- en: 'One last thing you may want to do before preparing the data for machine learning
    algorithms is to try out various attribute combinations. For example, the total
    number of rooms in a district is not very useful if you don’t know how many households
    there are. What you really want is the number of rooms per household. Similarly,
    the total number of bedrooms by itself is not very useful: you probably want to
    compare it to the total number of rooms. And the population per household also
    seems like an interesting attribute combination to look at. You create these new
    attributes as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备数据用于机器学习算法之前，你可能还想尝试各种属性组合。例如，如果你不知道该地区有多少家庭，那么该地区总房间数并不是很有用。你真正想要的是每户家庭的房间数。同样，仅凭卧室总数本身也不是很有用：你可能想将其与总房间数进行比较。而且，每户家庭的人口数也是一个有趣的属性组合。你可以按照以下方式创建这些新属性：
- en: '[PRE25]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And then you look at the correlation matrix again:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你再次查看相关矩阵：
- en: '[PRE26]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Hey, not bad! The new `bedrooms_ratio` attribute is much more correlated with
    the median house value than the total number of rooms or bedrooms. It’s a strong
    negative correlation, so it looks like houses with a lower bedroom/room ratio
    tend to be more expensive. The number of rooms per household is also more informative
    than the total number of rooms in a district—obviously the larger the houses,
    the more expensive they are.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿，不错！新的`bedrooms_ratio`属性与总房间数或卧室总数相比，与中位数房价的相关性要强得多。这是一个强烈的负相关，所以看起来卧室/房间比例较低的房屋往往更贵。每户家庭的房间数也比该地区总房间数更有信息量——显然，房屋越大，价格越高。
- en: Warning
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'When creating new combined features, make sure they are not too linearly correlated
    with existing features: *collinearity* can cause issues with some models, such
    as linear regression. In particular, avoid simple weighted sums of existing features.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建新的组合特征时，确保它们与现有特征不是过于线性相关的：*共线性*可能会导致某些模型（如线性回归）出现问题。特别是，避免简单地将现有特征的加权总和。
- en: 'This round of exploration does not have to be absolutely thorough; the point
    is to start off on the right foot and quickly gain insights that will help you
    get a first reasonably good prototype. But this is an iterative process: once
    you get a prototype up and running, you can analyze its output to gain more insights
    and come back to this exploration step.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这轮探索不必绝对彻底；目的是从正确的起点开始，快速获得有助于您获得第一个合理良好的原型的洞察力。但这是一个迭代过程：一旦原型运行起来，您可以通过分析其输出获得更多洞察力，并返回到这一探索步骤。
- en: Prepare the Data for Machine Learning Algorithms
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据以供机器学习算法使用
- en: 'It’s time to prepare the data for your machine learning algorithms. Instead
    of doing this manually, you should write functions for this purpose, for several
    good reasons:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候为您的机器学习算法准备数据了。而不是手动进行，您应该为此编写函数，有以下几个很好的理由：
- en: This will allow you to reproduce these transformations easily on any dataset
    (e.g., the next time you get a fresh dataset).
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将使您能够轻松地在任何数据集（例如，下次您获得新的数据集时）上重现这些转换。
- en: You will gradually build a library of transformation functions that you can
    reuse in future projects.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将逐步构建一个可以重复用于未来项目的转换函数库。
- en: You can use these functions in your live system to transform the new data before
    feeding it to your algorithms.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在您的实时系统中使用这些函数，在将新数据输入到算法之前对其进行转换。
- en: This will make it possible for you to easily try various transformations and
    see which combination of transformations works best.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将使您能够轻松尝试各种转换，并查看哪种转换组合效果最佳。
- en: 'But first, revert to a clean training set (by copying `strat_train_set` once
    again). You should also separate the predictors and the labels, since you don’t
    necessarily want to apply the same transformations to the predictors and the target
    values (note that `drop()` creates a copy of the data and does not affect `strat_train_set`):'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，恢复到一个干净的训练集（通过再次复制 `strat_train_set`）。您还应该分离预测值和标签，因为您不一定想对预测值和目标值应用相同的转换（注意
    `drop()` 会创建数据的副本，不会影响 `strat_train_set`）：
- en: '[PRE27]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Clean the Data
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理数据
- en: 'Most machine learning algorithms cannot work with missing features, so you’ll
    need to take care of these. For example, you noticed earlier that the `total_bedrooms`
    attribute has some missing values. You have three options to fix this:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法无法处理缺失特征，因此您需要处理这些问题。例如，您之前注意到 `total_bedrooms` 属性有一些缺失值。您有三个选项来修复这个问题：
- en: Get rid of the corresponding districts.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除相应的区域。
- en: Get rid of the whole attribute.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除整个属性。
- en: Set the missing values to some value (zero, the mean, the median, etc.). This
    is called *imputation*.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将缺失值设置为某个值（零、平均值、中间值等）。这被称为 *填充*。
- en: 'You can accomplish these easily using the Pandas DataFrame’s `dropna()`, `drop()`,
    and `fillna()` methods:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Pandas DataFrame 的 `dropna()`、`drop()` 和 `fillna()` 方法轻松完成这些操作：
- en: '[PRE28]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You decide to go for option 3 since it is the least destructive, but instead
    of the preceding code, you will use a handy Scikit-Learn class: `SimpleImputer`.
    The benefit is that it will store the median value of each feature: this will
    make it possible to impute missing values not only on the training set, but also
    on the validation set, the test set, and any new data fed to the model. To use
    it, first you need to create a `SimpleImputer` instance, specifying that you want
    to replace each attribute’s missing values with the median of that attribute:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 您决定选择选项 3，因为它破坏性最小，但您将使用一个方便的 Scikit-Learn 类：`SimpleImputer`。其好处是它会存储每个特征的中间值：这将使得在训练集、验证集、测试集以及任何新数据输入到模型中时，都可以进行缺失值的填充。要使用它，首先您需要创建一个
    `SimpleImputer` 实例，指定您想要用该属性的中间值替换每个属性的缺失值：
- en: '[PRE29]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Since the median can only be computed on numerical attributes, you then need
    to create a copy of the data with only the numerical attributes (this will exclude
    the text attribute `ocean_proximity`):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 由于中间值只能在数值属性上计算，因此您需要创建一个只包含数值属性的数据副本（这将排除文本属性 `ocean_proximity`）：
- en: '[PRE30]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now you can fit the `imputer` instance to the training data using the `fit()`
    method:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用 `fit()` 方法将 `imputer` 实例拟合到训练数据：
- en: '[PRE31]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `imputer` has simply computed the median of each attribute and stored the
    result in its `statistics_` instance variable. Only the `total_bedrooms` attribute
    had missing values, but you cannot be sure that there won’t be any missing values
    in new data after the system goes live, so it is safer to apply the `imputer`
    to all the numerical attributes:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`imputer` 简单地计算了每个属性的均值并将结果存储在其 `statistics_` 实例变量中。只有 `total_bedrooms` 属性有缺失值，但您不能确定系统上线后新数据中不会有任何缺失值，因此将
    `imputer` 应用于所有数值属性更安全：'
- en: '[PRE32]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now you can use this “trained” `imputer` to transform the training set by replacing
    missing values with the learned medians:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用这个“训练好的”`imputer`通过用学习到的中位数替换缺失值来转换训练集：
- en: '[PRE33]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Missing values can also be replaced with the mean value (`strategy="mean"`),
    or with the most frequent value (`strategy="most_frequent"`), or with a constant
    value (`strategy="constant", fill_value=`…​). The last two strategies support
    non-numerical data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值也可以用平均值（`strategy="mean"`）替换，或者用最频繁出现的值（`strategy="most_frequent"`）替换，或者用常数值（`strategy="constant",
    fill_value=`…​）替换。后两种策略支持非数值数据。
- en: Tip
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'There are also more powerful imputers available in the `sklearn.​impute` package
    (both for numerical features only):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `sklearn.impute` 包中还有更多强大的填充器可用（仅适用于数值特征）：
- en: '`KNNImputer` replaces each missing value with the mean of the *k*-nearest neighbors’
    values for that feature. The distance is based on all the available features.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KNNImputer` 将每个缺失值替换为该特征的 *k*-近邻值的平均值。距离基于所有可用特征。'
- en: '`IterativeImputer` trains a regression model per feature to predict the missing
    values based on all the other available features. It then trains the model again
    on the updated data, and repeats the process several times, improving the models
    and the replacement values at each iteration.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IterativeImputer` 为每个特征训练一个回归模型来预测基于所有其他可用特征的缺失值。然后它在更新后的数据上再次训练模型，并重复此过程多次，每次迭代都改进模型和替换值。'
- en: 'Scikit-Learn transformers output NumPy arrays (or sometimes SciPy sparse matrices)
    even when they are fed Pandas DataFrames as input.⁠^([11](ch02.html#id1107)) So,
    the output of `imputer.transform(housing_num)` is a NumPy array: `X` has neither
    column names nor index. Luckily, it’s not too hard to wrap `X` in a DataFrame
    and recover the column names and index from `housing_num`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 转换器输出 NumPy 数组（有时是 SciPy 稀疏矩阵），即使它们以 Pandas DataFrame 作为输入。⁠^([11](ch02.html#id1107))
    因此，`imputer.transform(housing_num)` 的输出是一个 NumPy 数组：`X` 没有列名也没有索引。幸运的是，将 `X` 包装在
    DataFrame 中并从 `housing_num` 中恢复列名和索引并不太难：
- en: '[PRE34]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Handling Text and Categorical Attributes
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理文本和分类属性
- en: 'So far we have only dealt with numerical attributes, but your data may also
    contain text attributes. In this dataset, there is just one: the `ocean_proximity`
    attribute. Let’s look at its value for the first few instances:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只处理了数值属性，但您的数据可能还包含文本属性。在这个数据集中，只有一个：`ocean_proximity` 属性。让我们看看前几个实例的值：
- en: '[PRE35]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It’s not arbitrary text: there are a limited number of possible values, each
    of which represents a category. So this attribute is a categorical attribute.
    Most machine learning algorithms prefer to work with numbers, so let’s convert
    these categories from text to numbers. For this, we can use Scikit-Learn’s `OrdinalEncoder`
    class:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是任意文本：可能的值数量有限，每个值代表一个类别。因此，这个属性是分类属性。大多数机器学习算法更喜欢使用数字，所以让我们将这些类别从文本转换为数字。为此，我们可以使用
    Scikit-Learn 的 `OrdinalEncoder` 类：
- en: '[PRE36]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here’s what the first few encoded values in `housing_cat_encoded` look like:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 `housing_cat_encoded` 中编码的前几个值的样子：
- en: '[PRE37]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You can get the list of categories using the `categories_` instance variable.
    It is a list containing a 1D array of categories for each categorical attribute
    (in this case, a list containing a single array since there is just one categorical
    attribute):'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `categories_` 实例变量获取类别列表。它是一个包含每个分类属性（在这种情况下，由于只有一个分类属性，所以是一个包含单个数组的列表）的类别的一维数组：
- en: '[PRE38]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'One issue with this representation is that ML algorithms will assume that two
    nearby values are more similar than two distant values. This may be fine in some
    cases (e.g., for ordered categories such as “bad”, “average”, “good”, and “excellent”),
    but it is obviously not the case for the `ocean_proximity` column (for example,
    categories 0 and 4 are clearly more similar than categories 0 and 1). To fix this
    issue, a common solution is to create one binary attribute per category: one attribute
    equal to 1 when the category is `"<1H OCEAN"` (and 0 otherwise), another attribute
    equal to 1 when the category is `"INLAND"` (and 0 otherwise), and so on. This
    is called *one-hot encoding*, because only one attribute will be equal to 1 (hot),
    while the others will be 0 (cold). The new attributes are sometimes called *dummy*
    attributes. Scikit-Learn provides a `OneHotEncoder` class to convert categorical
    values into one-hot vectors:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示方法的一个问题是，机器学习算法会假设两个相邻的值比两个遥远的值更相似。在某些情况下（例如，对于有序类别，如“差”，“平均”，“好”和“优秀”），这可能没问题，但对于
    `ocean_proximity` 列（例如，类别 0 和 4 明显比类别 0 和 1 更相似），这显然不是情况。为了解决这个问题，一个常见的解决方案是为每个类别创建一个二进制属性：一个属性等于
    1 当类别是 `"<1H OCEAN"`（否则为 0），另一个属性等于 1 当类别是 `"INLAND"`（否则为 0），依此类推。这被称为 *独热编码*，因为只有一个属性将等于
    1（热），而其他属性将为 0（冷）。这些新属性有时被称为 *虚拟* 属性。Scikit-Learn 提供了一个 `OneHotEncoder` 类来将分类值转换为独热向量：
- en: '[PRE39]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'By default, the output of a `OneHotEncoder` is a SciPy *sparse matrix*, instead
    of a NumPy array:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`OneHotEncoder` 的输出是一个 SciPy 稀疏矩阵，而不是 NumPy 数组：
- en: '[PRE40]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'A sparse matrix is a very efficient representation for matrices that contain
    mostly zeros. Indeed, internally it only stores the nonzero values and their positions.
    When a categorical attribute has hundreds or thousands of categories, one-hot
    encoding it results in a very large matrix full of 0s except for a single 1 per
    row. In this case, a sparse matrix is exactly what you need: it will save plenty
    of memory and speed up computations. You can use a sparse matrix mostly like a
    normal 2D array,⁠^([12](ch02.html#id1118)) but if you want to convert it to a
    (dense) NumPy array, just call the `toarray()` method:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵是表示包含大量零的矩阵的一种非常高效的方式。实际上，它只存储非零值及其位置。当一个分类属性有数百或数千个类别时，将其独热编码会导致一个非常大的矩阵，除了每行只有一个
    1 以外，其余都是 0。在这种情况下，稀疏矩阵正是你所需要的：它将节省大量内存并加快计算速度。你可以像使用正常的二维数组一样使用稀疏矩阵，⁠^([12](ch02.html#id1118))
    但如果你想将其转换为（密集）NumPy 数组，只需调用 `toarray()` 方法：
- en: '[PRE41]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Alternatively, you can set `sparse_output=False` when creating the `OneHotEncoder`,
    in which case the `transform()` method will return a regular (dense) NumPy array
    directly:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以在创建 `OneHotEncoder` 时设置 `sparse_output=False`，在这种情况下，`transform()` 方法将直接返回一个常规（密集）的
    NumPy 数组：
- en: '[PRE42]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As with the `OrdinalEncoder`, you can get the list of categories using the
    encoder’s `categories_` instance variable:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `OrdinalEncoder` 一样，你可以使用编码器的 `categories_` 实例变量来获取类别列表：
- en: '[PRE43]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Pandas has a function called `get_dummies()`, which also converts each categorical
    feature into a one-hot representation, with one binary feature per category:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 有一个名为 `get_dummies()` 的函数，它也会将每个分类特征转换为独热表示，每个类别一个二进制特征：
- en: '[PRE44]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'It looks nice and simple, so why not use it instead of `OneHotEncoder`? Well,
    the advantage of `OneHotEncoder` is that it remembers which categories it was
    trained on. This is very important because once your model is in production, it
    should be fed exactly the same features as during training: no more, no less.
    Look what our trained `cat_encoder` outputs when we make it transform the same
    `df_test` (using `transform()`, not `fit_transform()`):'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来既美观又简单，那么为什么不使用它来代替 `OneHotEncoder` 呢？嗯，`OneHotEncoder` 的优势在于它能记住它训练过的哪些类别。这一点非常重要，因为一旦你的模型投入生产，它应该接收与训练期间完全相同的特征：不多也不少。看看我们的训练好的
    `cat_encoder` 在我们让它转换相同的 `df_test` 时（使用 `transform()` 而不是 `fit_transform()`）输出的结果：
- en: '[PRE45]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'See the difference? `get_dummies()` saw only two categories, so it output two
    columns, whereas `OneHotEncoder` output one column per learned category, in the
    right order. Moreover, if you feed `get_dummies()` a DataFrame containing an unknown
    category (e.g., `"<2H OCEAN"`), it will happily generate a column for it:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 看到了区别吗？`get_dummies()` 只看到了两个类别，因此它输出了两列，而 `OneHotEncoder` 按照学习到的类别顺序输出每一列。此外，如果你给
    `get_dummies()` 提供一个包含未知类别（例如，`"<2H OCEAN"`）的 DataFrame，它将愉快地为其生成一列：
- en: '[PRE46]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'But `OneHotEncoder` is smarter: it will detect the unknown category and raise
    an exception. If you prefer, you can set the `handle_unknown` hyperparameter to
    `"ignore"`, in which case it will just represent the unknown category with zeros:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 但`OneHotEncoder`更智能：它会检测未知类别并引发异常。如果你愿意，可以将`handle_unknown`超参数设置为`"ignore"`，在这种情况下，它将仅用零表示未知类别：
- en: '[PRE47]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Tip
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If a categorical attribute has a large number of possible categories (e.g.,
    country code, profession, species), then one-hot encoding will result in a large
    number of input features. This may slow down training and degrade performance.
    If this happens, you may want to replace the categorical input with useful numerical
    features related to the categories: for example, you could replace the `ocean_proximity`
    feature with the distance to the ocean (similarly, a country code could be replaced
    with the country’s population and GDP per capita). Alternatively, you can use
    one of the encoders provided by the `category_encoders` package on [GitHub](https://github.com/scikit-learn-contrib/category_encoders).
    Or, when dealing with neural networks, you can replace each category with a learnable,
    low-dimensional vector called an *embedding* (see [Chapter 14](ch14.html#nlp_chapter)).
    This is an example of *representation learning* (we will see more examples in
    [Chapter 18](ch18.html#autoencoders_chapter)).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个分类属性有大量的可能类别（例如，国家代码、职业、物种），那么独热编码将导致大量的输入特征。这可能会减慢训练速度并降低性能。如果出现这种情况，你可能想用与类别相关的有用数值特征替换分类输入：例如，你可以用距离海洋的距离替换`ocean_proximity`特征（同样，国家代码可以用国家的人口和人均GDP来替换）。或者，当处理神经网络时，你可以用可学习的低维向量（称为*嵌入*）替换每个类别（参见[第14章](ch14.html#nlp_chapter)）。这是一个*表示学习*的例子（我们将在[第18章](ch18.html#autoencoders_chapter)中看到更多例子）。
- en: 'When you fit any Scikit-Learn estimator using a DataFrame, the estimator stores
    the column names in the `feature_names_in_` attribute. Scikit-Learn then ensures
    that any DataFrame fed to this estimator after that (e.g., to `transform()` or
    `predict()`) has the same column names. Transformers also provide a `get_feature_names_out()`
    method that you can use to build a DataFrame around the transformer’s output:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用DataFrame拟合任何Scikit-Learn估计器时，估计器会将列名存储在`feature_names_in_`属性中。Scikit-Learn然后确保任何在此之后馈送到此估计器的DataFrame（例如，用于`transform()`或`predict()`）具有相同的列名。转换器还提供了一个`get_feature_names_out()`方法，你可以使用它来构建围绕转换器输出的DataFrame：
- en: '[PRE48]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This feature helps avoid column mismatches, and it’s also quite useful when
    debugging.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能有助于避免列不匹配，而且在调试时也非常有用。
- en: Feature Scaling and Transformation
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征缩放和转换
- en: 'One of the most important transformations you need to apply to your data is
    *feature scaling*. With few exceptions, machine learning algorithms don’t perform
    well when the input numerical attributes have very different scales. This is the
    case for the housing data: the total number of rooms ranges from about 6 to 39,320,
    while the median incomes only range from 0 to 15\. Without any scaling, most models
    will be biased toward ignoring the median income and focusing more on the number
    of rooms.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要应用到你数据上的最重要的转换之一是*特征缩放*。在少数例外的情况下，机器学习算法在输入数值属性尺度差异很大时表现不佳。这种情况适用于住房数据：房间总数从大约6到39,320不等，而中位数收入仅从0到15不等。如果没有缩放，大多数模型都会偏向于忽略中位数收入，而更多地关注房间数量。
- en: 'There are two common ways to get all attributes to have the same scale: *min-max
    scaling* and *standardization*.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种常见的方法可以使所有属性具有相同的尺度：*最小-最大缩放*和*标准化*。
- en: Warning
  id: totrans-282
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'As with all estimators, it is important to fit the scalers to the training
    data only: never use `fit()` or `fit_transform()` for anything else than the training
    set. Once you have a trained scaler, you can then use it to `transform()` any
    other set, including the validation set, the test set, and new data. Note that
    while the training set values will always be scaled to the specified range, if
    new data contains outliers, these may end up scaled outside the range. If you
    want to avoid this, just set the `clip` hyperparameter to `True`.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有估计器一样，重要的是只将缩放器拟合到训练数据：永远不要为训练集以外的任何内容使用`fit()`或`fit_transform()`。一旦你有了训练好的缩放器，你就可以用它来`transform()`任何其他集合，包括验证集、测试集和新数据。请注意，虽然训练集的值将始终缩放到指定的范围，但如果新数据包含异常值，这些值可能最终会缩放到范围之外。如果你想避免这种情况，只需将`clip`超参数设置为`True`。
- en: 'Min-max scaling (many people call this *normalization*) is the simplest: for
    each attribute, the values are shifted and rescaled so that they end up ranging
    from 0 to 1\. This is performed by subtracting the min value from all values,
    and dividing the results by the difference between the min and the max. Scikit-Learn
    provides a transformer called `MinMaxScaler` for this. It has a `feature_range`
    hyperparameter that lets you change the range if, for some reason, you don’t want
    0–1 (e.g., neural networks work best with zero-mean inputs, so a range of –1 to
    1 is preferable). It’s quite easy to use:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大缩放（许多人称之为*归一化*）是最简单的：对于每个属性，值被移动并重新缩放，以便它们最终的范围从0到1。这是通过从所有值中减去最小值，然后除以最小值和最大值之间的差来完成的。Scikit-Learn提供了一个名为`MinMaxScaler`的转换器用于此。它有一个`feature_range`超参数，允许你更改范围，如果你出于某种原因不想用0–1（例如，神经网络在零均值输入上表现最佳，因此-1到1的范围更可取）。它非常容易使用：
- en: '[PRE49]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Standardization is different: first it subtracts the mean value (so standardized
    values have a zero mean), then it divides the result by the standard deviation
    (so standardized values have a standard deviation equal to 1). Unlike min-max
    scaling, standardization does not restrict values to a specific range. However,
    standardization is much less affected by outliers. For example, suppose a district
    has a median income equal to 100 (by mistake), instead of the usual 0–15\. Min-max
    scaling to the 0–1 range would map this outlier down to 1 and it would crush all
    the other values down to 0–0.15, whereas standardization would not be much affected.
    Scikit-Learn provides a transformer called `StandardScaler` for standardization:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化则不同：首先它减去平均值（因此标准化值具有零均值），然后它将结果除以标准差（因此标准化值具有标准差等于1）。与最小-最大缩放不同，标准化不限制值到特定范围。然而，标准化受异常值的影响要小得多。例如，假设一个区的中位数收入等于100（错误地），而不是通常的0–15。将最小-最大缩放到0–1范围会将这个异常值映射到1，并且会将所有其他值压缩到0–0.15，而标准化则不会受到太大影响。Scikit-Learn提供了一个名为`StandardScaler`的转换器用于标准化：
- en: '[PRE50]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Tip
  id: totrans-288
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you want to scale a sparse matrix without converting it to a dense matrix
    first, you can use a `StandardScaler` with its `with_mean` hyperparameter set
    to `False`: it will only divide the data by the standard deviation, without subtracting
    the mean (as this would break sparsity).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要缩放一个稀疏矩阵而不先将其转换为密集矩阵，你可以使用一个`StandardScaler`，并将其`with_mean`超参数设置为`False`：它将只除以标准差，而不减去均值（因为这会破坏稀疏性）。
- en: 'When a feature’s distribution has a *heavy tail* (i.e., when values far from
    the mean are not exponentially rare), both min-max scaling and standardization
    will squash most values into a small range. Machine learning models generally
    don’t like this at all, as you will see in [Chapter 4](ch04.html#linear_models_chapter).
    So *before* you scale the feature, you should first transform it to shrink the
    heavy tail, and if possible to make the distribution roughly symmetrical. For
    example, a common way to do this for positive features with a heavy tail to the
    right is to replace the feature with its square root (or raise the feature to
    a power between 0 and 1). If the feature has a really long and heavy tail, such
    as a *power law distribution*, then replacing the feature with its logarithm may
    help. For example, the `population` feature roughly follows a power law: districts
    with 10,000 inhabitants are only 10 times less frequent than districts with 1,000
    inhabitants, not exponentially less frequent. [Figure 2-17](#long_tail_plot) shows
    how much better this feature looks when you compute its log: it’s very close to
    a Gaussian distribution (i.e., bell-shaped).'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个特征的分布具有*重尾*（即，当远离均值的值不是指数级稀有的情况），最小-最大缩放和标准化都会将大多数值压缩到一个小范围内。机器学习模型通常非常不喜欢这种情况，正如你在[第4章](ch04.html#linear_models_chapter)中将会看到的。因此，在*缩放特征之前*，你应该首先将其转换以缩小重尾，如果可能的话，使其分布大致对称。例如，对于具有右侧重尾的正特征，一种常见的做法是用其平方根（或将其提升到0到1之间的幂）来替换特征。如果特征具有非常长且重的尾，例如*幂律分布*，那么用其对数来替换特征可能有所帮助。例如，`population`特征大致遵循幂律：有10,000居民的区只有比有1,000居民的区少10倍，而不是指数级少。[图2-17](#long_tail_plot)显示了当你计算其对数时，这个特征看起来有多好：它非常接近高斯分布（即，钟形）。
- en: '![Two histograms compare the distribution of a population feature: the left
    graph shows a heavily right-skewed distribution, while the right graph shows a
    more symmetrical, Gaussian-like distribution after applying a logarithmic transformation.](assets/hmls_0217.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![两个直方图比较人口特征的分布：左图显示一个严重右偏的分布，而右图显示在应用对数变换后更对称、类似高斯分布的分布。](assets/hmls_0217.png)'
- en: Figure 2-17\. Transforming a feature to make it closer to a Gaussian distribution
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-17\. 将特征转换为更接近高斯分布
- en: Another approach to handle heavy-tailed features consists in *bucketizing* the
    feature. This means chopping its distribution into roughly equal-sized buckets,
    and replacing each feature value with the index of the bucket it belongs to, much
    like we did to create the `income_cat` feature (although we only used it for stratified
    sampling). For example, you could replace each value with its percentile. Bucketizing
    with equal-sized buckets results in a feature with an almost uniform distribution,
    so there’s no need for further scaling, or you can just divide by the number of
    buckets to force the values to the 0–1 range.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 处理重尾特征的另一种方法是对特征进行**桶化**。这意味着将其分布切割成大致相等大小的桶，并将每个特征值替换为它所属桶的索引，就像我们创建`income_cat`特征时做的那样（尽管我们只用它进行分层抽样）。例如，你可以用每个值的百分位数来替换。使用等大小桶进行桶化会导致一个几乎均匀分布的特征，因此不需要进一步缩放，或者你可以简单地除以桶的数量来强制将值限制在0-1范围内。
- en: When a feature has a multimodal distribution (i.e., with two or more clear peaks,
    called *modes*), such as the `housing_median_age` feature, it can also be helpful
    to bucketize it, but this time treating the bucket IDs as categories, rather than
    as numerical values. This means that the bucket indices must be encoded, for example
    using a `OneHotEncoder` (so you usually don’t want to use too many buckets). This
    approach will allow the regression model to more easily learn different rules
    for different ranges of this feature value. For example, perhaps houses built
    around 35 years ago have a peculiar style that fell out of fashion, and therefore
    they’re cheaper than their age alone would suggest.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个特征具有多模态分布（即有两个或更多清晰的峰值，称为**模式**）时，例如`housing_median_age`特征，对其进行桶化也可能很有帮助，但这次将桶ID视为类别，而不是数值。这意味着桶索引必须进行编码，例如使用`OneHotEncoder`（因此通常不想使用太多的桶）。这种方法将允许回归模型更容易地学习不同范围的特征值的不同规则。例如，也许大约35年前建造的房屋有一种独特的风格，这种风格已经过时，因此它们的价格比仅仅根据其年龄要便宜。
- en: 'Another approach to transforming multimodal distributions is to add a feature
    for each of the modes (at least the main ones), representing the similarity between
    the housing median age and that particular mode. The similarity measure is typically
    computed using a *radial basis function* (RBF)—any function that depends only
    on the distance between the input value and a fixed point. The most commonly used
    RBF is the Gaussian RBF, whose output value decays exponentially as the input
    value moves away from the fixed point. For example, the Gaussian RBF similarity
    between the housing age *x* and 35 is given by the equation exp(–*γ*(*x* – 35)²).
    The hyperparameter *γ* (gamma) determines how quickly the similarity measure decays
    as *x* moves away from 35\. Using Scikit-Learn’s `rbf_kernel()` function, you
    can create a new Gaussian RBF feature measuring the similarity between the housing
    median age and 35:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种转换多模态分布的方法是为每个模式（至少是主要模式）添加一个特征，表示住房中位数年龄与该特定模式之间的相似性。相似性度量通常使用**径向基函数**（RBF）来计算——任何只依赖于输入值与固定点之间距离的函数。最常用的RBF是高斯RBF，其输出值随着输入值远离固定点而指数衰减。例如，住房年龄*x*与35之间的高斯RBF相似度由方程exp(–*γ*(*x*
    – 35)²)给出。超参数*γ*（伽马）决定了相似性度量随*x*远离35时的衰减速度。使用Scikit-Learn的`rbf_kernel()`函数，你可以创建一个新的高斯RBF特征，测量住房中位数年龄与35之间的相似性：
- en: '[PRE51]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[Figure 2-18](#age_similarity_plot) shows this new feature as a function of
    the housing median age (solid line). It also shows what the feature would look
    like if you used a smaller `gamma` value. As the chart shows, the new age similarity
    feature peaks at 35, right around the spike in the housing median age distribution:
    if this particular age group is well correlated with lower prices, there’s a good
    chance that this new feature will help.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-18](#age_similarity_plot)显示了这一新特征作为住房中位年龄（实线）的函数。它还显示了如果你使用较小的`gamma`值时该特征会是什么样子。如图表所示，新的年龄相似性特征在35岁时达到峰值，正好在住房中位年龄分布的峰值附近：如果这个特定的年龄组与较低的价格高度相关，那么这个新特征很可能会有所帮助。'
- en: '![Histogram and line plot showing the Gaussian RBF feature for age similarity
    peaking at a housing median age of 35, with gamma values of 0.10 and 0.03.](assets/hmls_0218.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![直方图和线图显示年龄相似性的高斯径向基函数特征，在住房中位年龄为35岁时达到峰值，gamma值为0.10和0.03。](assets/hmls_0218.png)'
- en: Figure 2-18\. Gaussian RBF feature measuring the similarity between the housing
    median age and 35
  id: totrans-299
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-18\. 测量住房中位年龄与35之间相似性的高斯径向基函数特征
- en: So far we’ve only looked at the input features, but the target values may also
    need to be transformed. For example, if the target distribution has a heavy tail,
    you may choose to replace the target with its logarithm. But if you do, the regression
    model will now predict the *log* of the median house value, not the median house
    value itself. You will need to compute the exponential of the model’s prediction
    if you want the predicted median house value.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看了输入特征，但目标值可能也需要进行转换。例如，如果目标分布有一个重尾，你可能会选择用其对数来替换目标。但如果你这样做，回归模型现在将预测中位房屋价值的*对数*，而不是中位房屋价值本身。如果你想得到预测的中位房屋价值，你需要计算模型预测的指数。
- en: 'Luckily, most of Scikit-Learn’s transformers have an `inverse_transform()`
    method, making it easy to compute the inverse of their transformations. For example,
    the following code example shows how to scale the labels using a `StandardScaler`
    (just like we did for inputs), then train a simple linear regression model on
    the resulting scaled labels and use it to make predictions on some new data, which
    we transform back to the original scale using the trained scaler’s `inverse_transform()`
    method. Note that we convert the labels from a Pandas Series to a DataFrame, since
    the `StandardScaler` expects 2D inputs. Also, in this example we just train the
    model on a single raw input feature (median income), for simplicity:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Scikit-Learn的大多数转换器都有一个`inverse_transform()`方法，这使得计算其变换的逆变得容易。例如，以下代码示例展示了如何使用`StandardScaler`（就像我们对输入所做的那样）缩放标签，然后在缩放后的标签上训练一个简单的线性回归模型，并使用它对新数据进行预测，我们使用训练好的缩放器的`inverse_transform()`方法将数据转换回原始尺度。请注意，我们将标签从Pandas
    Series转换为DataFrame，因为`StandardScaler`期望2D输入。此外，在这个例子中，我们为了简单起见，只在一个原始输入特征（中位收入）上训练模型：
- en: '[PRE52]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This works fine, but it’s simpler and less error-prone to use a `TransformedTarget​Regressor`,
    avoiding potential scaling mismatches. We just need to construct it, giving it
    the regression model and the label transformer, then fit it on the training set,
    using the original unscaled labels. It will automatically use the transformer
    to scale the labels and train the regression model on the resulting scaled labels,
    just like we did previously. Then, when we want to make a prediction, it will
    call the regression model’s `predict()` method and use the scaler’s `inverse_transform()`
    method to produce the prediction:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是可行的，但使用`TransformedTargetRegressor`会更简单且错误更少，可以避免潜在的缩放不匹配问题。我们只需要构建它，给它回归模型和标签转换器，然后在训练集上拟合，使用原始未缩放的标签。它将自动使用转换器缩放标签，并在缩放后的标签上训练回归模型，就像我们之前做的那样。然后，当我们想要进行预测时，它将调用回归模型的`predict()`方法，并使用缩放器的`inverse_transform()`方法来生成预测：
- en: '[PRE53]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Custom Transformers
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义转换器
- en: Although Scikit-Learn provides many useful transformers, you will occasionally
    need to write your own for tasks such as custom transformations, cleanup operations,
    or combining specific attributes.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Scikit-Learn提供了许多有用的转换器，但你偶尔需要编写自己的转换器来完成诸如自定义转换、清理操作或组合特定属性等任务。
- en: 'For transformations that don’t require any training, you can just write a function
    that takes a NumPy array as input and outputs the transformed array. For example,
    as discussed in the previous section, it’s often a good idea to transform features
    with heavy-tailed distributions by replacing them with their logarithm (assuming
    the feature is positive and the tail is on the right). Let’s create a log-transformer
    and apply it to the `population` feature:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不需要任何训练的转换，您可以简单地编写一个函数，该函数接受NumPy数组作为输入并输出转换后的数组。例如，如前所述，通常将具有重尾分布的特征通过替换为它们的对数（假设特征是正的，尾部在右边）进行转换是个好主意。让我们创建一个对数转换器并将其应用于`population`特征：
- en: '[PRE54]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The `inverse_func` argument is optional. It lets you specify an inverse transform
    function, e.g., if you plan to use your transformer in a `TransformedTargetRegressor`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`inverse_func`参数是可选的。它允许您指定一个逆转换函数，例如，如果您计划在`TransformedTargetRegressor`中使用您的转换器。'
- en: 'Your transformation function can take hyperparameters as additional arguments.
    For example, here’s how to create a transformer that computes the same Gaussian
    RBF similarity measure as earlier:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 您的转换函数可以接受超参数作为额外的参数。例如，以下是如何创建一个转换器，它计算与之前相同的Gaussian RBF相似度度量：
- en: '[PRE55]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Note that there’s no inverse function for the RBF kernel, since there are always
    two values at a given distance from a fixed point (except at distance 0). Also
    note that `rbf_kernel()` does not treat the features separately. If you pass it
    an array with two features, it will measure the 2D distance (Euclidean) to measure
    similarity. For example, here’s how to add a feature that will measure the geographic
    similarity between each district and San Francisco:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，RBF核没有逆函数，因为在给定距离的固定点处始终有两个值（除了距离0）。另外注意，`rbf_kernel()`不单独处理特征。如果您传递一个包含两个特征的数组，它将测量2D距离（欧几里得）来测量相似性。例如，以下是如何添加一个将测量每个地区与旧金山之间地理相似度的特征：
- en: '[PRE56]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Custom transformers are also useful to combine features. For example, here’s
    a `FunctionTransformer` that computes the ratio between the input features 0 and
    1:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义转换器也有助于合并特征。例如，以下是一个计算输入特征0和1之间比率的`FunctionTransformer`：
- en: '[PRE57]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '`FunctionTransformer` is very handy, but what if you would like your transformer
    to be trainable, learning some parameters in the `fit()` method and using them
    later in the `transform()` method? For this, you need to write a custom class.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`FunctionTransformer`非常方便，但如果您希望您的转换器是可训练的，在`fit()`方法中学习一些参数并在`transform()`方法中使用它们，该怎么办？为此，您需要编写一个自定义类。'
- en: Note
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The rest of this section shows how to define custom transformer classes. In
    particular, it defines a custom transformer that groups districts into 10 geographical
    clusters, then measures the distance between each district and the center of each
    cluster, adding 10 corresponding RBF similarity features to the data. Since defining
    custom transformer classes is somewhat advanced, please feel free to skip to the
    next section and come back whenever needed.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的其余部分展示了如何定义自定义转换器类。特别是，它定义了一个自定义转换器，该转换器将地区分组到10个地理集群中，然后测量每个地区与每个集群中心的距离，并将10个相应的RBF相似度特征添加到数据中。由于定义自定义转换器类相对较复杂，请随时跳到下一节，并在需要时返回。
- en: 'Scikit-Learn relies on duck typing,⁠^([13](ch02.html#id1177)) so custom transformer
    classes do not have to inherit from any particular base class. All they need is
    three methods: `fit()` (which must return `self`), `transform()`, and `fit_transform()`.
    You can get `fit_transform()` for free by simply adding `TransformerMixin` as
    a base class: the default implementation will just call `fit()` and then `transform()`.
    If you add `BaseEstimator` as a base class (and avoid using `*args` and `**kwargs`
    in your constructor), you will also get two extra methods: `get_params()` and
    `set_params()`. These will be useful for automatic hyperparameter tuning.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn依赖于鸭子类型，⁠^([13](ch02.html#id1177)) 因此自定义转换器类不需要继承任何特定的基类。它们只需要三个方法：`fit()`（必须返回`self`），`transform()`和`fit_transform()`。只需将`TransformerMixin`作为基类即可免费获得`fit_transform()`：默认实现将只是调用`fit()`然后`transform()`。如果您将`BaseEstimator`作为基类（并在构造函数中避免使用`*args`和`**kwargs`），您还将获得两个额外的方法：`get_params()`和`set_params()`。这些将有助于自动超参数调整。
- en: 'For example, here’s a custom transformer that acts much like the `StandardScaler`:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里有一个自定义的转换器，其行为与`StandardScaler`非常相似：
- en: '[PRE58]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Here are a few things to note:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些需要注意的事项：
- en: The `sklearn.utils.validation` package contains several functions we can use
    to validate the inputs. For simplicity, we will skip such tests in the rest of
    this book, but production code should have them.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn.utils.validation`包包含我们可以使用的几个函数来验证输入。为了简单起见，我们将跳过本书其余部分中的此类测试，但生产代码应该包含它们。'
- en: Scikit-Learn pipelines require the `fit()` method to have two arguments `X`
    and `y`, which is why we need the `y=None` argument even though we don’t use `y`.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-Learn的管道需要`fit()`方法有两个参数`X`和`y`，这就是为什么即使我们不使用`y`，我们也需要`y=None`参数的原因。
- en: All Scikit-Learn estimators set `n_features_in_` in the `fit()` method, and
    they ensure that the data passed to `transform()` or `predict()` has this number
    of features.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有Scikit-Learn估计器在`fit()`方法中设置`n_features_in_`，并确保传递给`transform()`或`predict()`的数据具有此数量的特征。
- en: The `fit()` method must return `self`.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit()`方法必须返回`self`。'
- en: 'This implementation is not 100% complete: all estimators should set `feature_​names_in_`
    in the `fit()` method when they are passed a DataFrame. Moreover, all transformers
    should provide a `get_feature_names_out()` method, as well as an `inverse_transform()`
    method when their transformation can be reversed. See the last exercise at the
    end of this chapter for more details.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此实现并非100%完整：所有估计器在接收到DataFrame时，应在`fit()`方法中设置`feature_​names_in_`。此外，所有转换器应提供`get_feature_names_out()`方法，以及当它们的转换可以反转时，还应提供`inverse_transform()`方法。有关更多详细信息，请参阅本章末尾的最后练习。
- en: 'A custom transformer can (and often does) use other estimators in its implementation.
    For example, the following code demonstrates a custom transformer that uses a
    `KMeans` clusterer in the `fit()` method to identify the main clusters in the
    training data, and then uses `rbf_kernel()` in the `transform()` method to measure
    how similar each sample is to each cluster center:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义转换器可以在其实现中使用其他估计器（并且通常确实如此）。例如，以下代码演示了一个自定义转换器，它使用`KMeans`聚类器在`fit()`方法中识别训练数据中的主要聚类，然后在`transform()`方法中使用`rbf_kernel()`来衡量每个样本与每个聚类中心的相似程度：
- en: '[PRE59]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Tip
  id: totrans-330
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can check whether your custom estimator respects Scikit-Learn’s API by passing
    an instance to `check_estimator()` from the `sklearn.utils.estimator_checks` package.
    For the full API, check out [*https://scikit-learn.org/stable/developers*](https://scikit-learn.org/stable/developers).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将实例传递给`sklearn.utils.estimator_checks`包中的`check_estimator()`来检查您的自定义估计器是否遵守Scikit-Learn的API。有关完整API，请访问[*https://scikit-learn.org/stable/developers*](https://scikit-learn.org/stable/developers)。
- en: 'As you will see in [Chapter 8](ch08.html#unsupervised_learning_chapter), *k*-means
    is a clustering algorithm that locates clusters in the data. For example, we can
    use it to find the most populated regions in California. How many clusters *k*-means
    searches for is controlled by the `n_clusters` hyperparameter. The `fit()` method
    of `KMeans` supports an optional argument `sample_weight`, which lets the user
    specify the relative weights of the samples. For example, we could pass it the
    median income if we wanted the clusters to be biased toward wealthy districts.
    After training, the cluster centers are available via the `cluster_centers_` attribute.
    *k*-means is a stochastic algorithm, meaning that it relies on randomness to locate
    the clusters, so if you want reproducible results, you must set the `random_state`
    parameter. As you can see, despite the complexity of the task, the code is fairly
    straightforward. Now let’s use this custom transformer:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将在[第8章](ch08.html#unsupervised_learning_chapter)中看到的那样，*k*-means是一种聚类算法，它可以在数据中定位聚类。例如，我们可以用它来找到加利福尼亚州人口最多的地区。*k*-means搜索的聚类数量由`n_clusters`超参数控制。`KMeans`的`fit()`方法支持一个可选的`sample_weight`参数，允许用户指定样本的相对权重。例如，如果我们想使聚类偏向富裕地区，我们可以传递中位数收入。训练后，聚类中心可通过`cluster_centers_`属性访问。*k*-means是一个随机算法，这意味着它依赖于随机性来定位聚类，因此如果您想得到可重复的结果，您必须设置`random_state`参数。如您所见，尽管任务复杂，但代码相当简单。现在让我们使用这个自定义转换器：
- en: '[PRE60]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This code creates a `ClusterSimilarity` transformer, setting the number of
    clusters to 10\. Then it calls `fit_transform()` with the latitude and longitude
    of every district in the training set (you can try weighting each district by
    its median income to see how that affects the clusters). The transformer uses
    *k*-means to locate the clusters, then measures the Gaussian RBF similarity between
    each district and all 10 cluster centers. The result is a matrix with one row
    per district, and one column per cluster. Let’s look at the first three rows,
    rounding to two decimal places:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码创建了一个 `ClusterSimilarity` 转换器，将簇的数量设置为 10。然后它使用训练集中每个地区的纬度和经度调用 `fit_transform()`（您可以尝试通过每个地区的中位数收入来加权每个地区，看看这如何影响簇）。转换器使用
    *k*-means 定位簇，然后测量每个地区与所有 10 个簇中心的 Gaussian RBF 相似性。结果是一个矩阵，每行代表一个地区，每列代表一个簇。让我们看看前三个行，四舍五入到两位小数：
- en: '[PRE61]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[Figure 2-19](#district_cluster_plot) shows the 10 cluster centers found by
    *k*-means. The districts are colored according to their geographic similarity
    to their closest cluster center. Notice that most clusters are located in highly
    populated areas.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-19](#district_cluster_plot) 显示了由 *k*-means 找到的 10 个簇中心。地区根据其与最近簇中心的地理相似性着色。请注意，大多数簇都位于高人口密度区域。'
- en: '![Scatter plot showing geographic clusters based on Gaussian RBF similarity,
    with highly populated areas highlighted.](assets/hmls_0219.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![基于高斯 RBF 相似性的地理聚类散点图，高人口密度区域突出显示](assets/hmls_0219.png)'
- en: Figure 2-19\. Gaussian RBF similarity to the nearest cluster center
  id: totrans-338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-19\. 到最近簇中心的 Gaussian RBF 相似性
- en: Transformation Pipelines
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换器管道
- en: 'As you can see, there are many data transformation steps that need to be executed
    in the right order. Fortunately, Scikit-Learn provides the `Pipeline` class to
    help with such sequences of transformations. Here is a small pipeline for numerical
    attributes, which will first impute then scale the input features:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，有许多需要按正确顺序执行的数据转换步骤。幸运的是，Scikit-Learn 提供了 `Pipeline` 类来帮助处理这样的转换序列。以下是一个用于数值属性的简单管道示例，它将首先填充缺失值然后缩放输入特征：
- en: '[PRE62]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The `Pipeline` constructor takes a list of name/estimator pairs (2-tuples)
    defining a sequence of steps. The names can be anything you like, as long as they
    are unique and don’t contain double underscores (`__`). They will be useful later,
    when we discuss hyperparameter tuning. The estimators must all be transformers
    (i.e., they must have a `fit_transform()` method), except for the last one, which
    can be anything: a transformer, a predictor, or any other type of estimator.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipeline` 构造函数接受一个定义步骤序列的名称/估计器对（2-元组）列表。名称可以是任何您喜欢的，只要它们是唯一的并且不包含双下划线（`__`）。它们将在我们讨论超参数调整时很有用。估计器必须全部是转换器（即它们必须有
    `fit_transform()` 方法），除了最后一个，可以是任何东西：一个转换器、一个预测器或任何其他类型的估计器。'
- en: Tip
  id: totrans-343
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: In a Jupyter notebook, if you `import` `sklearn` and run `sklearn.​set_config(display="diagram")`,
    all Scikit-Learn estimators will be rendered as interactive diagrams. This is
    particularly useful for visualizing pipelines. To visualize `num_pipeline`, run
    a cell with `num_pipeline` as the last line. Clicking an estimator will show more
    details.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter 笔记本中，如果你 `import` `sklearn` 并运行 `sklearn.​set_config(display="diagram")`，所有
    Scikit-Learn 估计器都将被渲染为交互式图表。这对于可视化管道特别有用。要可视化 `num_pipeline`，请运行一个以 `num_pipeline`
    为最后一行的单元格。点击估计器将显示更多详细信息。
- en: 'If you don’t want to have to name the transformers, you can use the convenient
    `make_pipeline()` function instead; it takes transformers as positional arguments
    and creates a `Pipeline` using the names of the transformers’ classes, in lowercase
    and without underscores (e.g., `"simpleimputer"`):'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想命名转换器，可以使用方便的 `make_pipeline()` 函数代替；它接受转换器作为位置参数，并使用转换器类的名称创建一个 `Pipeline`，名称为小写且不带下划线（例如，`"simpleimputer"`）：
- en: '[PRE63]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: If multiple transformers have the same name, an index is appended to their names
    (e.g., `"foo-1"`, `"foo-2"`, etc.).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果多个转换器具有相同的名称，则会在其名称后附加一个索引（例如，`"foo-1"`，`"foo-2"` 等）。
- en: When you call the pipeline’s `fit()` method, it calls `fit_transform()` sequentially
    on all the transformers, passing the output of each call as the parameter to the
    next call until it reaches the final estimator, for which it just calls the `fit()`
    method.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用管道的 `fit()` 方法时，它将对所有转换器依次调用 `fit_transform()`，将每个调用的输出作为下一个调用的参数，直到它达到最终的估计器，对于这个估计器，它只调用
    `fit()` 方法。
- en: The pipeline exposes the same methods as the final estimator. In this example
    the last estimator is a `StandardScaler`, which is a transformer, so the pipeline
    also acts like a transformer. If you call the pipeline’s `transform()` method,
    it will sequentially apply all the transformations to the data. If the last estimator
    were a predictor instead of a transformer, then the pipeline would have a `predict()`
    method rather than a `transform()` method. Calling it would sequentially apply
    all the transformations to the data and pass the result to the predictor’s `predict()`
    method.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 管道公开了与最终估计器相同的方法。在这个例子中，最后一个估计器是一个`StandardScaler`，它是一个转换器，因此管道也像转换器一样工作。如果你调用管道的`transform()`方法，它将按顺序应用所有转换到数据上。如果最后一个估计器是一个预测器而不是转换器，那么管道将有一个`predict()`方法而不是`transform()`方法。调用它将按顺序应用所有转换到数据上，并将结果传递给预测器的`predict()`方法。
- en: 'Let’s call the pipeline’s `fit_transform()` method and look at the output’s
    first two rows, rounded to two decimal places:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调用管道的`fit_transform()`方法，并查看输出前两行，结果四舍五入到小数点后两位：
- en: '[PRE64]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'As you saw earlier, if you want to recover a nice DataFrame, you can use the
    pipeline’s `get_feature_names_out()` method:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如你之前所见，如果你想恢复一个漂亮的DataFrame，你可以使用管道的`get_feature_names_out()`方法：
- en: '[PRE65]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Pipelines support indexing; for example, `pipeline[1]` returns the second estimator
    in the pipeline, and `pipeline[:-1]` returns a `Pipeline` object containing all
    but the last estimator. You can also access the estimators via the `steps` attribute,
    which is a list of name/estimator pairs, or via the `named_steps` dictionary attribute,
    which maps the names to the estimators. For example, `num_pipeline["simpleimputer"]`
    returns the estimator named `"simpleimputer"`.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 管道支持索引；例如，`pipeline[1]`返回管道中的第二个估计器，而`pipeline[:-1]`返回一个不包含最后一个估计器的`Pipeline`对象。你也可以通过`steps`属性访问估计器，它是一个包含名称/估计器对的列表，或者通过`named_steps`字典属性访问，它将名称映射到估计器。例如，`num_pipeline["simpleimputer"]`返回名为`"simpleimputer"`的估计器。
- en: 'So far, we have handled the categorical columns and the numerical columns separately.
    It would be more convenient to have a single transformer capable of handling all
    columns, applying the appropriate transformations to each column. For this, you
    can use a `ColumnTransformer`. For example, the following `ColumnTransformer`
    will apply `num_pipeline` (the one we just defined) to the numerical attributes,
    and `cat_pipeline` to the categorical attribute:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经分别处理了分类列和数值列。如果有一个能够处理所有列的单个转换器，并且对每个列应用适当的转换，将会更方便。为此，你可以使用`ColumnTransformer`。例如，以下`ColumnTransformer`将应用我们刚刚定义的`num_pipeline`到数值属性，并将`cat_pipeline`应用到分类属性：
- en: '[PRE66]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: First we import the `ColumnTransformer` class, then we define the list of numerical
    and categorical column names and construct a simple pipeline for categorical attributes.
    Lastly, we construct a `ColumnTransformer`. Its constructor requires a list of
    triplets (3-tuples), each containing a name (which must be unique and not contain
    double underscores), a transformer, and a list of names (or indices) of columns
    that the transformer should be applied to.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入`ColumnTransformer`类，然后定义数值和分类列名的列表，并为分类属性构建一个简单的管道。最后，我们构建一个`ColumnTransformer`。其构造函数需要一个包含三元组（3-tuples）的列表，每个三元组包含一个名称（必须是唯一的，且不能包含双下划线）、一个转换器和应用于转换器的列名（或索引）列表。
- en: Tip
  id: totrans-358
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Instead of using a transformer, you can specify the string `"drop"` if you want
    the columns to be dropped, or you can specify `"passthrough"` if you want the
    columns to be left untouched. By default, the remaining columns (i.e., the ones
    that were not listed) will be dropped, but you can set the `remainder` hyperparameter
    to any transformer (or to `"passthrough"`) if you want these columns to be handled
    differently.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想使用转换器，你可以指定字符串`"drop"`来删除列，或者如果你想保持列不变，可以指定`"passthrough"`。默认情况下，剩余的列（即未列出的列）将被删除，但你可以将`remainder`超参数设置为任何转换器（或设置为`"passthrough"`)，如果你希望这些列以不同的方式处理。
- en: 'Since listing all the column names is not very convenient, Scikit-Learn provides
    a `make_column_selector` class that you can use to automatically select all the
    features of a given type, such as numerical or categorical. You can pass a selector
    to the `ColumnTransformer` instead of column names or indices. Moreover, if you
    don’t care about naming the transformers, you can use `make_column_transformer()`,
    which chooses the names for you, just like `make_pipeline()` does. For example,
    the following code creates the same `ColumnTransformer` as earlier, except the
    transformers are automatically named `"pipeline-1"` and `"pipeline-2"` instead
    of `"num"` and `"cat"`:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 由于列出所有列名不太方便，Scikit-Learn提供了一个`make_column_selector`类，你可以使用它来自动选择给定类型的所有特征，例如数值或分类。你可以将选择器传递给`ColumnTransformer`而不是列名或索引。此外，如果你不关心命名转换器，你可以使用`make_column_transformer()`，它将为你选择名称，就像`make_pipeline()`做的那样。例如，以下代码创建了一个与之前相同的`ColumnTransformer`，但转换器的名称自动命名为`"pipeline-1"`和`"pipeline-2"`而不是`"num"`和`"cat"`：
- en: '[PRE67]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now we’re ready to apply this `ColumnTransformer` to the housing data:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好将这个`ColumnTransformer`应用到住房数据上了：
- en: '[PRE68]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Great! We have a preprocessing pipeline that takes the entire training dataset
    and applies each transformer to the appropriate columns, then concatenates the
    transformed columns horizontally (transformers must never change the number of
    rows). Once again this returns a NumPy array, but you can get the column names
    using `preprocessing.get_feature_names_out()` and wrap the data in a nice DataFrame
    as we did before.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们有一个预处理管道，它接受整个训练数据集，并将每个转换器应用于适当的列，然后水平地连接转换后的列（转换器绝不能改变行数）。再次强调，这返回一个NumPy数组，但你可以使用`preprocessing.get_feature_names_out()`来获取列名，并将数据包裹在一个漂亮的DataFrame中，就像我们之前做的那样。
- en: Note
  id: totrans-365
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `OneHotEncoder` returns a sparse matrix and the `num_pipeline` returns a
    dense matrix. When there is such a mix of sparse and dense matrices, the `ColumnTransformer`
    estimates the density of the final matrix (i.e., the ratio of nonzero cells),
    and it returns a sparse matrix if the density is lower than a given threshold
    (by default, `sparse_threshold=0.3`). In this example, it returns a dense matrix.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneHotEncoder`返回一个稀疏矩阵，而`num_pipeline`返回一个密集矩阵。当存在这种稀疏和密集矩阵的混合时，`ColumnTransformer`会估计最终矩阵的密度（即非零单元格的比例），如果密度低于给定的阈值（默认为`sparse_threshold=0.3`），则返回一个稀疏矩阵。在这个例子中，它返回一个密集矩阵。'
- en: 'Your project is going really well and you’re almost ready to train some models!
    You now want to create a single pipeline that will perform all the transformations
    you’ve experimented with up to now. Let’s recap what the pipeline will do and
    why:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 你的项目进展得非常顺利，你几乎准备好训练一些模型了！你现在想要创建一个单一的管道，该管道将执行你迄今为止实验过的所有转换。让我们回顾一下管道将做什么以及为什么：
- en: Missing values in numerical features will be imputed by replacing them with
    the median, as most ML algorithms don’t expect missing values. In categorical
    features, missing values will be replaced by the most frequent category.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值特征中的缺失值将通过用中位数替换它们来填充，因为大多数机器学习算法不期望存在缺失值。在分类特征中，缺失值将被最频繁出现的类别所替换。
- en: The categorical feature will be one-hot encoded, as most ML algorithms only
    accept numerical inputs.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类特征将被进行独热编码，因为大多数机器学习算法只接受数值输入。
- en: 'A few ratio features will be computed and added: `bedrooms_ratio`, `rooms_​per_house`,
    and `people_per_house`. Hopefully these will better correlate with the median
    house value, and thereby help the ML models.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将计算并添加一些比率特征：`bedrooms_ratio`、`rooms_per_house`和`people_per_house`。希望这些特征能更好地与中位数房价相关联，从而帮助机器学习模型。
- en: A few cluster similarity features will also be added. These will likely be more
    useful to the model than latitude and longitude.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将添加一些聚类相似度特征。这些特征可能比经纬度对模型更有用。
- en: Features with a long tail will be replaced by their logarithm, as most models
    prefer features with roughly uniform or Gaussian distributions.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尾部较长的特征将被它们的对数所替代，因为大多数模型更喜欢具有大致均匀或高斯分布的特征。
- en: All numerical features will be standardized, as most ML algorithms prefer when
    all features have roughly the same scale.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有数值特征都将进行标准化，因为大多数机器学习算法更喜欢所有特征具有大致相同的尺度。
- en: 'The code that builds the pipeline to do all of this should look familiar to
    you by now:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 构建执行所有这些操作的管道的代码现在应该对你来说很熟悉了：
- en: '[PRE69]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'If you run this `ColumnTransformer`, it performs all the transformations and
    outputs a NumPy array with 24 features:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个`ColumnTransformer`，它将执行所有转换，并输出一个包含24个特征的NumPy数组：
- en: '[PRE70]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Select and Train a Model
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择和训练模型
- en: At last! You framed the problem, you got the data and explored it, you sampled
    a training set and a test set, and you wrote a preprocessing pipeline to automatically
    clean up and prepare your data for machine learning algorithms. You are now ready
    to select and train a machine learning model.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 终于！你已经定义了问题，获取了数据并对其进行了探索，你采样了一个训练集和一个测试集，并编写了一个预处理管道来自动清理和准备你的数据以供机器学习算法使用。你现在准备好选择和训练一个机器学习模型了。
- en: Train and Evaluate on the Training Set
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在训练集上训练和评估
- en: 'The good news is that thanks to all these previous steps, things are now going
    to be easy! You decide to train a very basic linear regression model to get started:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，由于所有这些之前的步骤，现在事情将会变得简单！你决定从一个非常基础的线性回归模型开始训练：
- en: '[PRE71]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Done! You now have a working linear regression model. You try it out on the
    training set, looking at the first five predictions and comparing them to the
    labels:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！你现在有一个正在工作的线性回归模型。你尝试在训练集上使用它，查看前五个预测并将其与标签进行比较：
- en: '[PRE72]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Well, it works, but not always: the first prediction is way off (by over $200,000!),
    while the other predictions are better: two are off by about 25%, and two are
    off by less than 10%. Remember that you chose to use the RMSE as your performance
    measure, so you want to measure this regression model’s RMSE on the whole training
    set using Scikit-Learn’s `root_mean_squared_error()` function:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，它确实有效，但并不总是如此：第一次预测偏差很大（超过$200,000！），而其他预测则更好：两个偏差大约为25%，另外两个偏差小于10%。记住，你选择使用RMSE作为性能指标，所以你想使用Scikit-Learn的`root_mean_squared_error()`函数在整个训练集上测量这个回归模型的RMSE：
- en: '[PRE73]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Tip
  id: totrans-387
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'We’re not using the `score()` method here because it returns the *R² coefficient
    of determination* instead of the RMSE. This coefficient represents the ratio of
    the variance in the data that the model can explain: the closer to 1 (which is
    the max value), the better. If the model simply predicts the mean all the time,
    it does not explain any part of the variance, so the model’s R² score is 0\. And
    if the model does even worse than that, then its R² score can be negative, and
    indeed arbitrarily low.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不使用`score()`方法，因为它返回的是*R²决定系数*而不是RMSE。这个系数表示模型可以解释的数据方差的比例：越接近1（这是最大值），越好。如果模型始终简单地预测平均值，它不能解释任何方差的部分，因此模型的R²分数是0。如果模型的表现甚至更差，那么它的R²分数可以是负数，实际上可以是任意低。
- en: 'This is better than nothing, but clearly not a great score: the `median_housing_values`
    of most districts range between $120,000 and $265,000, so a typical prediction
    error of $68,973 is really not very satisfying. This is an example of a model
    underfitting the training data. When this happens it can mean that the features
    do not provide enough information to make good predictions, or that the model
    is not powerful enough. As we saw in the previous chapter, the main ways to fix
    underfitting are to select a more powerful model, to feed the training algorithm
    with better features, or to reduce the constraints on the model. This model is
    not regularized, which rules out the last option. You could try to add more features,
    but first you want to try a more complex model to see how it does.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 这比没有好，但显然不是一个很好的分数：大多数地区的`median_housing_values`介于$120,000和$265,000之间，所以典型的预测误差$68,973实际上并不令人满意。这是一个模型欠拟合训练数据的例子。当这种情况发生时，这可能意味着特征没有提供足够的信息来做出好的预测，或者模型本身不够强大。正如我们在上一章中看到的，解决欠拟合的主要方法是有选择地选择一个更强大的模型，向训练算法提供更好的特征，或者减少对模型的约束。这个模型没有正则化，这排除了最后一个选项。你可以尝试添加更多特征，但首先你想要尝试一个更复杂的模型来看看它的表现如何。
- en: 'You decide to try a `DecisionTreeRegressor`, as this is a fairly powerful model
    capable of finding complex nonlinear relationships in the data (decision trees
    are presented in more detail in [Chapter 5](ch05.html#trees_chapter)):'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 你决定尝试一个`DecisionTreeRegressor`，因为这个模型相当强大，能够找到数据中的复杂非线性关系（决策树在[第5章](ch05.html#trees_chapter)中有更详细的介绍）：
- en: '[PRE74]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Now that the model is trained, you evaluate it on the training set:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经训练好了，你在训练集上对其进行评估：
- en: '[PRE75]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Wait, what!? No error at all? Could this model really be absolutely perfect?
    Of course, it is much more likely that the model has badly overfit the data. How
    can you be sure? As you saw earlier, you don’t want to touch the test set until
    you are ready to launch a model you are confident about, so you need to use part
    of the training set for training and part of it for model validation.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，什么？完全没有错误？这个模型真的绝对完美吗？当然，模型严重过拟合数据的可能性要大得多。你怎么能确定呢？正如你之前看到的，你不想在准备好一个你确信的模型之前触摸测试集，所以你需要使用训练集的一部分进行训练，另一部分用于模型验证。
- en: Better Evaluation Using Cross-Validation
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用交叉验证进行更好的评估
- en: One way to evaluate the decision tree model would be to use the `train_​test_split()`
    function to split the training set into a smaller training set and a validation
    set, then train your models against the smaller training set and evaluate them
    against the validation set. It’s a bit of effort, but nothing too difficult, and
    it would work fairly well.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 评估决策树模型的一种方法是将训练集使用`train_​test_split()`函数分成较小的训练集和验证集，然后针对较小的训练集训练模型，并对其使用验证集进行评估。这需要一点努力，但并不太难，而且效果相当不错。
- en: A great alternative is to use Scikit-Learn’s *k-fold cross-validation* feature.
    You split the training set into *k* nonoverlapping subsets called *folds*, then
    you train and evaluate your model *k* times, picking a different fold for evaluation
    every time (i.e., the validation fold) and using the other *k* – 1 folds for training.
    This process produces *k* evaluation scores (see [Figure 2-20](#k_fold_cross_validation_diagram)).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的替代方案是使用Scikit-Learn的**k折交叉验证**功能。你将训练集分成**k**个非重叠的子集，称为**折**，然后你训练和评估你的模型**k**次，每次评估时选择不同的折进行评估（即验证折），而使用其他**k**
    - 1个折进行训练。这个过程会产生**k**个评估分数（见[图2-20](#k_fold_cross_validation_diagram)）。
- en: '![Diagram illustrating _k_-fold cross-validation with _k_ = 10, displaying
    different validation folds and corresponding evaluation scores for each split.](assets/hmls_0220.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![展示**k**-折交叉验证（**k** = 10）的示意图，显示不同的验证折和每个分割对应的评估分数。](assets/hmls_0220.png)'
- en: Figure 2-20\. *k*-fold cross-validation, with *k* = 10
  id: totrans-399
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-20\. **k**-折交叉验证，其中**k** = 10
- en: 'Scikit-Learn provides a convenient `cross_val_score()` function that does just
    that, and it returns an array containing the *k* evaluation scores. For example,
    let’s use it to evaluate our tree regressor, using *k* = 10:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了一个方便的`cross_val_score()`函数，它正是这样做的，并返回一个包含**k**个评估分数的数组。例如，让我们使用它来评估我们的树回归器，使用**k**
    = 10：
- en: '[PRE76]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Warning
  id: totrans-402
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Scikit-Learn’s cross-validation features expect a utility function (greater
    is better) rather than a cost function (lower is better), so the scoring function
    is actually the opposite of the RMSE. It’s a negative value, so you need to switch
    the sign of the output to get the RMSE scores.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn的交叉验证功能期望一个效用函数（越大越好）而不是成本函数（越小越好），所以评分函数实际上是RMSE的相反。它是一个负值，所以你需要切换输出的符号以获得RMSE分数。
- en: 'Let’s look at the results:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果：
- en: '[PRE77]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Now the decision tree doesn’t look as good as it did earlier. In fact, it seems
    to perform almost as poorly as the linear regression model! Notice that cross-validation
    allows you to get not only an estimate of the performance of your model, but also
    a measure of how precise this estimate is (i.e., its standard deviation). The
    decision tree has an RMSE of about 66,574, with a standard deviation of about
    1,103\. You would not have this information if you just used one validation set.
    But cross-validation comes at the cost of training the model several times, so
    it is not always feasible.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 现在决策树看起来没有之前那么好了。实际上，它的表现似乎和线性回归模型几乎一样差！请注意，交叉验证不仅允许你得到你模型性能的估计，还可以衡量这个估计的精确度（即其标准差）。决策树有大约66,574的RMSE，标准差大约为1,103。如果你只使用一个验证集，你将不会得到这些信息。但是，交叉验证的代价是多次训练模型，所以它并不总是可行的。
- en: If you compute the same metric for the linear regression model, you will find
    that the mean RMSE is 70,003 and the standard deviation is 4,182\. So the decision
    tree model seems to perform very slightly better than the linear model, but the
    difference is minimal due to severe overfitting. We know there’s an overfitting
    problem because the training error is low (actually zero) while the validation
    error is high.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你为线性回归模型计算相同的指标，你会发现平均RMSE为70,003，标准差为4,182。所以决策树模型似乎比线性模型表现略好，但由于严重的过拟合，差异很小。我们知道存在过拟合问题，因为训练错误率低（实际上为零），而验证错误率高。
- en: 'Let’s try one last model now: the `RandomForestRegressor`. As you will see
    in [Chapter 6](ch06.html#ensembles_chapter), random forests work by training many
    decision trees on random subsets of the features, then averaging out their predictions.
    Such models composed of many other models are called *ensembles*: if the underlying
    models are very diverse, then their errors will not be very correlated, and therefore
    averaging out the predictions will smooth out the errors, reduce overfitting,
    and improve the overall performance. The code is much the same as earlier:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试最后一个模型：`RandomForestRegressor`。正如你将在[第6章](ch06.html#ensembles_chapter)中看到的那样，随机森林通过在特征随机子集上训练许多决策树，然后平均它们的预测来工作。由许多其他模型组成的这种模型被称为*集成*：如果底层模型非常多样化，那么它们的误差将不会非常相关，因此平均预测将平滑误差，减少过拟合，并提高整体性能。代码与之前大致相同：
- en: '[PRE78]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Let’s look at the scores:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看分数：
- en: '[PRE79]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Wow, this is much better: random forests really look very promising for this
    task! However, if you train a `RandomForestRegressor` and measure the RMSE on
    the training set, you will find roughly 17,551: that’s much lower, meaning that
    there’s still quite a lot of overfitting going on. Possible solutions are to simplify
    the model, constrain it (i.e., regularize it), or get a lot more training data.
    Before you dive much deeper into random forests, however, you should try out many
    other models from various categories of machine learning algorithms (e.g., several
    support vector machines with different kernels, and possibly a neural network),
    without spending too much time tweaking the hyperparameters. The goal is to shortlist
    a few (two to five) promising models.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这好多了：随机森林在这个任务上看起来非常有希望！然而，如果你训练一个`RandomForestRegressor`并在训练集上测量RMSE，你会发现大约17,551：这要低得多，这意味着仍然有相当多的过拟合。可能的解决方案是简化模型，约束它（即正则化它），或者获取更多的训练数据。然而，在你深入研究随机森林之前，你应该尝试许多来自各种机器学习算法类别的其他模型（例如，几个具有不同核的支持向量机，以及可能的一个神经网络），而不要花太多时间调整超参数。目标是筛选出几个（两到五个）有希望的模型。
- en: Fine-Tune Your Model
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调你的模型
- en: Let’s assume that you now have a shortlist of promising models. You now need
    to fine-tune them. Let’s look at a few ways you can do that.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你现在有一份有希望的模型短名单。你现在需要微调它们。让我们看看你可以这样做的一些方法。
- en: Grid Search
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索
- en: One option would be to fiddle with the hyperparameters manually, until you find
    a great combination of hyperparameter values. This would be very tedious work,
    and you may not have time to explore many combinations.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 一个选择是手动调整超参数，直到你找到一个很好的超参数值组合。这将是一项非常繁琐的工作，你可能没有时间探索许多组合。
- en: 'Instead, you can use Scikit-Learn’s `GridSearchCV` class to search for you.
    All you need to do is tell it which hyperparameters you want it to experiment
    with and what values to try out, and it will use cross-validation to evaluate
    all the possible combinations of hyperparameter values. For example, the following
    code searches for the best combination of hyperparameter values for the `RandomForestRegressor`:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你可以使用Scikit-Learn的`GridSearchCV`类来帮你搜索。你需要做的就是告诉它你想让它实验哪些超参数以及尝试哪些值，然后它会使用交叉验证来评估所有可能超参数值的组合。例如，以下代码搜索`RandomForestRegressor`的最佳超参数值组合：
- en: '[PRE80]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Notice that you can refer to any hyperparameter of any estimator in a pipeline,
    even if this estimator is nested deep inside several pipelines and column transformers.
    For example, when Scikit-Learn sees `"preprocessing__geo__n_clusters"`, it splits
    this string at the double underscores, then it looks for an estimator named `"preprocessing"`
    in the pipeline and finds the preprocessing `ColumnTransformer`. Next, it looks
    for a transformer named `"geo"` inside this `ColumnTransformer` and finds the
    `ClusterSimilarity` transformer we used on the latitude and longitude attributes.
    Then it finds this transformer’s `n_clusters` hyperparameter. Similarly, `random_forest__max_features`
    refers to the `max_features` hyperparameter of the estimator named `"random_forest"`,
    which is of course the `RandomForestRegressor` model (the `max_features` hyperparameter
    will be explained in [Chapter 6](ch06.html#ensembles_chapter)).
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您可以在管道中的任何估计器的任何超参数中引用，即使这个估计器嵌套在几个管道和列转换器中。例如，当 Scikit-Learn 看到字符串 `"preprocessing__geo__n_clusters"`
    时，它会将这个字符串在双下划线处分割，然后它在管道中查找名为 `"preprocessing"` 的估计器并找到预处理 `ColumnTransformer`。接下来，它在
    `ColumnTransformer` 中查找名为 `"geo"` 的转换器并找到我们在经纬度属性上使用的 `ClusterSimilarity` 转换器。然后它找到这个转换器的
    `n_clusters` 超参数。同样，`random_forest__max_features` 指的是名为 `"random_forest"` 的估计器的
    `max_features` 超参数，当然这是 `RandomForestRegressor` 模型（`max_features` 超参数将在 [第 6 章](ch06.html#ensembles_chapter)
    中解释）。
- en: Tip
  id: totrans-420
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Wrapping preprocessing steps in a Scikit-Learn pipeline allows you to tune
    the preprocessing hyperparameters along with the model hyperparameters. This is
    a good thing since they often interact. For example, perhaps increasing `n_clusters`
    requires increasing `max_features` as well. If fitting the pipeline transformers
    is computationally expensive, you can set the pipeline’s `memory` parameter to
    the path of a caching directory: when you first fit the pipeline, Scikit-Learn
    will save the fitted transformers to this directory. If you then fit the pipeline
    again with the same hyperparameters, Scikit-Learn will just load the cached transformers.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 将预处理步骤包装在 Scikit-Learn 管道中允许您同时调整预处理超参数和模型超参数。这是一件好事，因为它们通常相互作用。例如，可能需要增加 `n_clusters`
    以提高 `max_features`。如果拟合管道转换器的计算成本很高，可以将管道的 `memory` 参数设置为缓存目录的路径：当您第一次拟合管道时，Scikit-Learn
    将将拟合的转换器保存到该目录。如果您随后使用相同的超参数再次拟合管道，Scikit-Learn 将仅加载缓存的转换器。
- en: 'There are two dictionaries in this `param_grid`, so `GridSearchCV` will first
    evaluate all 3 × 3 = 9 combinations of `n_clusters` and `max_features` hyperparameter
    values specified in the first `dict`, then it will try all 2 × 3 = 6 combinations
    of hyperparameter values in the second `dict`. So in total the grid search will
    explore 9 + 6 = 15 combinations of hyperparameter values, and it will train the
    pipeline 3 times per combination, since we are using 3-fold cross validation.
    This means there will be a grand total of 15 × 3 = 45 rounds of training! It may
    take a while, but when it is done you can get the best combination of parameters
    like this:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 `param_grid` 中有两个字典，所以 `GridSearchCV` 将首先评估第一个 `dict` 中指定的 `n_clusters`
    和 `max_features` 超参数值的所有 3 × 3 = 9 种组合，然后它将尝试第二个 `dict` 中所有 2 × 3 = 6 种超参数值的组合。因此，网格搜索将探索
    9 + 6 = 15 种超参数值的组合，并且我们将为每种组合训练管道 3 次，因为我们使用的是 3 折交叉验证。这意味着总共将有 15 × 3 = 45 轮训练！这可能需要一些时间，但完成之后，您可以像这样获得最佳参数组合：
- en: '[PRE81]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: In this example, the best model is obtained by setting `n_clusters` to 15 and
    setting `max_features` to 6.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，通过将 `n_clusters` 设置为 15 并将 `max_features` 设置为 6 来获得最佳模型。
- en: Tip
  id: totrans-425
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Since 15 is the maximum value that was evaluated for `n_clusters`, you should
    probably try searching again with higher values; the score may continue to improve.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `n_clusters` 评估的最大值是 15，您可能需要尝试使用更高的值再次搜索；分数可能会继续提高。
- en: You can access the best estimator using `grid_search.best_estimator_`. If `Grid​SearchCV`
    is initialized with `refit=True` (which is the default), then once it finds the
    best estimator using cross-validation, it retrains it on the whole training set.
    This is usually a good idea, since feeding it more data will likely improve its
    performance.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `grid_search.best_estimator_` 访问最佳估计器。如果 `GridSearchCV` 使用 `refit=True`（这是默认值）初始化，那么一旦它使用交叉验证找到最佳估计器，它就会在整个训练集上重新训练它。这通常是一个好主意，因为提供更多数据可能会提高其性能。
- en: 'The evaluation scores are available using `grid_search.cv_results_`. This is
    a dictionary, but if you wrap it in a DataFrame you get a nice list of all the
    test scores for each combination of hyperparameters and for each cross-validation
    split, as well as the mean test score across all splits:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`grid_search.cv_results_`可以获取评估分数。这是一个字典，但如果你将其包装在一个DataFrame中，你会得到一个所有测试分数的列表，包括每个超参数组合和每个交叉验证分割的测试分数，以及所有分割的平均测试分数：
- en: '[PRE82]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The mean test RMSE score for the best model is 43,590, which is better than
    the score you got earlier using the default hyperparameter values (which was 47,038).
    Congratulations, you have successfully fine-tuned your best model!
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳模型的平均测试RMSE分数为43,590，这比之前使用默认超参数值得到的分数（47,038）要好。恭喜你，你已经成功微调了你的最佳模型！
- en: Randomized Search
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机搜索
- en: 'The grid search approach is fine when you are exploring relatively few combinations,
    like in the previous example, but `RandomizedSearchCV` is often preferable, especially
    when the hyperparameter search space is large. This class can be used in much
    the same way as the `GridSearchCV` class, but instead of trying out all possible
    combinations it evaluates a fixed number of combinations, selecting a random value
    for each hyperparameter at every iteration. This may sound surprising, but this
    approach has several benefits:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 当你探索相对较少的组合时，例如在之前的例子中，网格搜索方法是可以接受的，但通常`RandomizedSearchCV`更可取，尤其是在超参数搜索空间较大时。这个类可以像`GridSearchCV`类一样使用，但它不是尝试所有可能的组合，而是在每次迭代中评估固定数量的组合，为每个超参数选择一个随机值。这听起来可能有些令人惊讶，但这种方法有几个优点：
- en: If some of your hyperparameters are continuous (or discrete but with many possible
    values), and you let randomized search run for, say, 1,000 iterations, then it
    will explore 1,000 different values for each of these hyperparameters, whereas
    grid search would only explore the few values you listed for each one.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的某些超参数是连续的（或者离散但有许多可能的值），并且你让随机搜索运行，比如说，1,000次迭代，那么它将为这些超参数中的每一个探索1,000个不同的值，而网格搜索只会探索你为每个超参数列出的少数几个值。
- en: Suppose a hyperparameter does not actually make much difference, but you don’t
    know it yet. If it has 10 possible values and you add it to your grid search,
    then training will take 10 times longer. But if you add it to a random search,
    it will not make any difference.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设一个超参数实际上并没有太大的影响，但你还没有意识到这一点。如果它有10个可能的值，并且你将其添加到你的网格搜索中，那么训练将需要更长的时间。但如果将其添加到随机搜索中，它将不会产生任何影响。
- en: If there are 6 hyperparameters to explore, each with 10 possible values, then
    grid search offers no other choice than training the model a million times, whereas
    random search can always run for any number of iterations you choose.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有6个超参数需要探索，每个有10个可能的值，那么网格搜索除了训练模型一百万次外，没有其他选择，而随机搜索可以运行任何你选择的迭代次数。
- en: 'For each hyperparameter, you must provide either a list of possible values,
    or a probability distribution:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个超参数，你必须提供一个可能的值列表，或者一个概率分布：
- en: '[PRE83]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Scikit-Learn also has `HalvingRandomSearchCV` and `HalvingGridSearchCV` hyperparameter
    search classes. Their goal is to use the computational resources more efficiently,
    either to train faster or to explore a larger hyperparameter space. Here’s how
    they work: in the first round, many hyperparameter combinations (called “candidates”)
    are generated using either the grid approach or the random approach. These candidates
    are then used to train models that are evaluated using cross-validation, as usual.
    However, training uses limited resources, which speeds up this first round considerably.
    By default, “limited resources” means that the models are trained on a small part
    of the training set. However, other limitations are possible, such as reducing
    the number of training iterations if the model has a hyperparameter to set it.
    Once every candidate has been evaluated, only the best ones go on to the second
    round, where they are allowed more resources to compete. After several rounds,
    the final candidates are evaluated using full resources. This may save you some
    time tuning hyperparameters.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 还提供了 `HalvingRandomSearchCV` 和 `HalvingGridSearchCV` 超参数搜索类。它们的目标是更有效地使用计算资源，要么是为了更快地训练，要么是为了探索更大的超参数空间。以下是它们的工作原理：在第一轮中，使用网格方法或随机方法生成许多超参数组合（称为“候选人”）。然后，使用这些候选人来训练模型，并像往常一样使用交叉验证来评估这些模型。然而，训练使用有限的资源，这大大加快了第一轮的速度。默认情况下，“有限资源”意味着模型是在训练集的小部分上训练的。然而，其他限制也是可能的，例如，如果模型有一个需要设置的超参数，可以减少训练迭代次数。一旦评估了所有候选人，只有最好的候选人才会进入第二轮，在那里它们可以获得更多的资源来竞争。经过几轮后，最终候选人将使用全部资源进行评估。这可能会节省您一些调整超参数的时间。
- en: Ensemble Methods
  id: totrans-439
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成方法
- en: Another way to fine-tune your system is to try to combine the models that perform
    best. The group (or “ensemble”) will often perform better than the best individual
    model—just like random forests perform better than the individual decision trees
    they rely on—especially if the individual models make very different types of
    errors. For example, you could train and fine-tune a *k*-nearest neighbors model,
    then create an ensemble model that just predicts the mean of the random forest
    prediction and that model’s prediction. We will cover this topic in more detail
    in [Chapter 6](ch06.html#ensembles_chapter).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种微调您系统的方法是尝试组合表现最好的模型。通常，组（或“集成”）的表现会比单个最佳模型更好——就像随机森林的表现比它们依赖的个别决策树更好一样——特别是如果个别模型犯下非常不同类型的错误。例如，您可以训练和微调一个
    *k*-最近邻模型，然后创建一个集成模型，该模型仅预测随机森林预测和该模型预测的均值。我们将在[第6章](ch06.html#ensembles_chapter)中更详细地介绍这个主题。
- en: Analyzing the Best Models and Their Errors
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析最佳模型及其错误
- en: 'You will often gain good insights on the problem by inspecting the best models.
    For example, the `RandomForestRegressor` can indicate the relative importance
    of each attribute for making accurate predictions:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查最佳模型，您通常可以从问题中获得很好的见解。例如，`RandomForestRegressor` 可以指示每个属性对准确预测的相对重要性：
- en: '[PRE84]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Let’s sort these importance scores in descending order and display them next
    to their corresponding attribute names:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些重要性分数按降序排列，并在它们对应的属性名称旁边显示：
- en: '[PRE85]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: With this information, you may want to try dropping some of the less useful
    features (e.g., apparently only one `ocean_proximity` category is really useful,
    so you could try dropping the others).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些信息，您可能想要尝试删除一些不太有用的特征（例如，显然只有一个 `ocean_proximity` 类别真正有用，因此您可以尝试删除其他类别）。
- en: Tip
  id: totrans-447
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'The `sklearn.feature_selection.SelectFromModel` transformer can automatically
    drop the least useful features for you: when you fit it, it trains a model (typically
    a random forest), looks at its `feature_importances_` attribute, and selects the
    most useful features. Then when you call `transform()`, it drops the other features.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.feature_selection.SelectFromModel` 转换器可以自动为您删除最不实用的特征：当您将其拟合时，它会训练一个模型（通常是随机森林），查看其
    `feature_importances_` 属性，并选择最实用的特征。然后当您调用 `transform()` 时，它会删除其他特征。'
- en: 'You should also look at the specific errors that your system makes, then try
    to understand why it makes them and what could fix the problem: adding extra features
    or getting rid of uninformative ones, cleaning up outliers, etc.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应该查看您的系统犯的具体错误，然后尝试理解为什么它会犯这些错误以及什么可以修复问题：添加额外特征或去除无信息特征，清理异常值等。
- en: 'Now is also a good time to check *model fairness*: it should not only work
    well on average, but also on various categories of districts, whether they’re
    rural or urban, rich or poor, northern or southern, minority or not, etc. This
    requires a detailed *bias analysis*: creating subsets of your validation set for
    each category, and analyzing your model’s performance on them. That’s a lot of
    work, but it’s important: if your model performs poorly on a whole category of
    districts, then it should probably not be deployed until the issue is resolved,
    or at least it should not be used to make predictions for that category, as it
    may do more harm than good.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 现在也是检查*模型公平性*的好时机：它不仅应该在平均意义上表现良好，还应该在各种类型的地区上表现良好，无论是农村还是城市，富裕还是贫穷，北方还是南方，少数民族还是不是，等等。这需要详细的*偏差分析*：为每个类别创建你的验证集的子集，并分析你的模型在这些子集上的性能。这是一项大量工作，但非常重要：如果你的模型在某个地区的整个类别上表现不佳，那么在问题解决之前，它可能不应该部署，或者至少不应该用于该类别的预测，因为它可能会造成比好处更大的伤害。
- en: Evaluate Your System on the Test Set
  id: totrans-451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试集上评估你的系统
- en: 'After tweaking your models for a while, you eventually have a system that performs
    sufficiently well. You are ready to evaluate the final model on the test set.
    There is nothing special about this process; just get the predictors and the labels
    from your test set and run your `final_model` to transform the data and make predictions,
    then evaluate these predictions:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整你的模型一段时间后，你最终拥有了一个表现足够好的系统。你现在可以准备在测试集上评估最终模型。这个过程没有什么特别之处；只需从你的测试集中获取预测器和标签，运行你的`final_model`来转换数据并做出预测，然后评估这些预测：
- en: '[PRE86]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'In some cases, such a point estimate of the generalization error will not be
    quite enough to convince you to launch: what if it is just 0.1% better than the
    model currently in production? You might want to have an idea of how precise this
    estimate is. For this, you can compute a 95% *confidence interval* for the generalization
    error using `scipy.stats.bootstrap()`. You get a fairly large interval from 39,521
    to 43,702, and your previous point estimate of 41,445 is roughly in the middle
    of it:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，这样的泛化误差的点估计可能不足以让你决定发布：如果它只是比当前生产中的模型好0.1%，你会怎么做？你可能想了解这个估计的精确度。为此，你可以使用`scipy.stats.bootstrap()`计算泛化误差的95%*置信区间*。你得到一个相当大的区间，从39,521到43,702，而你之前的点估计41,445大致位于这个区间的中间：
- en: '[PRE87]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: If you do a lot of hyperparameter tuning, the performance will usually be slightly
    worse than what you measured using cross-validation. That’s because your system
    ends up fine-tuned to perform well on the validation data and will likely not
    perform as well on unknown datasets. That’s not the case in this example since
    the test RMSE is lower than the validation RMSE, but when it happens you must
    resist the temptation to tweak the hyperparameters to make the numbers look good
    on the test set; the improvements would be unlikely to generalize to new data.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你做了很多超参数调整，性能通常会略低于你使用交叉验证测量的性能。这是因为你的系统最终会微调以在验证数据上表现良好，并且可能不会在未知数据集上表现得一样好。在这个例子中，测试RMSE低于验证RMSE，但这并不是这种情况；当这种情况发生时，你必须抵制调整超参数以使测试集上的数字看起来更好的诱惑；这种改进不太可能推广到新数据。
- en: 'Now comes the project prelaunch phase. Presenting your solution effectively
    is what sets great data scientists apart from good ones. You should create concise
    reports (Markdown, PDFs, slides), visualize key insights (e.g., using Matplotlib
    or other tools such as SeaBorn or Tableau), and tailor your message to the audience:
    technical for peers, high-level for stakeholders. Provide impactful and easy-to-remember
    statements (e.g., “the median income is the number one predictor of housing prices”).
    Highlight what you have learned, what worked and what did not, what assumptions
    were made, and what your system’s limitations are.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是项目预发布阶段。有效地展示你的解决方案是区分优秀数据科学家和良好数据科学家的关键。你应该创建简洁的报告（Markdown，PDFs，幻灯片），可视化关键见解（例如，使用Matplotlib或其他工具如SeaBorn或Tableau），并根据受众调整你的信息：对同行使用技术性语言，对利益相关者使用高级语言。提供有影响力和易于记忆的陈述（例如，“中位数收入是房价的第一大预测因素”）。强调你学到了什么，什么有效，什么无效，做了哪些假设，以及你系统的局限性是什么。
- en: 'Your results should be reproducible (as much as possible): make the code accessible
    to your team (e.g., via GitHub), add a structured *README* file to guide a technical
    person through the installation steps. Provide clear notebooks (e.g., Jupyter)
    with code, explanations, and results, writing clean, well-commented code. Define
    a *requirements.txt* or *environment.yml* file containing all the required libraries
    along with their precise versions (or create a Docker image). Set seeds for random
    generators, and remove any other source of variability.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 您的结果应该是可复制的（尽可能多）：使代码对您的团队可访问（例如，通过GitHub），添加一个结构化的*README*文件来指导技术人员完成安装步骤。提供清晰的笔记本（例如，Jupyter）带有代码、解释和结果，编写干净、注释良好的代码。定义一个*requirements.txt*或*environment.yml*文件，包含所有必需的库及其精确版本（或创建一个Docker镜像）。为随机生成器设置种子，并移除任何其他变异来源。
- en: In this California housing example, the final performance of the system is not
    much better than the experts’ price estimates, which were often off by 30%, but
    it may still be a good idea to launch it, especially if this frees up some time
    for the experts so they can work on more interesting and productive tasks.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个加利福尼亚住房示例中，系统的最终性能并不比专家的价格估计好多少，专家的估计通常误差高达30%，但仍然可能是一个好主意来推出它，特别是如果这可以为专家节省一些时间，让他们可以从事更有趣和更有生产力的任务。
- en: Launch, Monitor, and Maintain Your System
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动、监控和维护您的系统
- en: 'Perfect, you got approval to launch! You now need to get your solution ready
    for production (e.g., polish the code, write documentation and tests, and so on).
    Then you can deploy your model to your production environment. The most basic
    way to do this is just to save the best model you trained, transfer the file to
    your production environment, and load it. To save the model, you can use the `joblib`
    library like this:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 完美，您获得了批准来启动！现在您需要为您的解决方案准备生产环境（例如，完善代码、编写文档和测试等）。然后您可以将模型部署到您的生产环境。最基本的方法是只是保存您训练的最佳模型，将文件转移到您的生产环境，并加载它。要保存模型，您可以使用`joblib`库，如下所示：
- en: '[PRE88]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Tip
  id: totrans-463
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: It’s often a good idea to save every model you experiment with so that you can
    come back easily to any model you want. You may also save the cross-validation
    scores and perhaps the actual predictions on the validation set. This will allow
    you to easily compare scores across model types, and compare the types of errors
    they make.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 保存您实验过的每个模型通常是一个好主意，这样您可以轻松地回到任何您想要的模型。您还可以保存交叉验证分数，也许还有验证集上的实际预测。这将允许您轻松地比较不同模型类型的分数，并比较它们犯的错误类型。
- en: 'Once your model is transferred to production, you can load it and use it. For
    this you must first import any custom classes and functions the model relies on
    (which means transferring the code to production), then load the model using `joblib`
    and use it to make predictions:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的模型转移到生产环境，您就可以加载它并使用它。为此，您必须首先导入模型所依赖的任何自定义类和函数（这意味着将代码转移到生产环境），然后使用`joblib`加载模型并使用它进行预测：
- en: '[PRE89]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'For example, perhaps the model will be used within a website: the user will
    type in some data about a new district and click the Estimate Price button. This
    will send a query containing the data to the web server, which will forward it
    to your web application, and finally your code will simply call the model’s `predict()`
    method (you want to load the model upon server startup, rather than every time
    the model is used). Alternatively, you can wrap the model within a dedicated web
    service that your web application can query through a REST API⁠^([14](ch02.html#id1268))
    (see [Figure 2-21](#webservice_model_diagram)). This makes it easier to upgrade
    your model to new versions without interrupting the main application. It also
    simplifies scaling, since you can start as many web services as needed and load-balance
    the requests coming from your web application across these web services. Moreover,
    it allows your web application to use any programming language, not just Python.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，模型可能被用于网站内部：用户将输入关于新区域的一些数据，然后点击“估算价格”按钮。这将发送包含数据的查询到网络服务器，服务器再将它转发到您的网络应用，最后您的代码将简单地调用模型的`predict()`方法（您希望在服务器启动时加载模型，而不是每次使用模型时都加载）。或者，您可以将模型封装在一个专用的网络服务中，您的网络应用可以通过REST
    API查询这个服务⁠^([14](ch02.html#id1268))（参见[图2-21](#webservice_model_diagram)）。这使得在不中断主应用的情况下升级模型到新版本变得更加容易。它还简化了扩展，因为您可以根据需要启动尽可能多的网络服务，并将来自您的网络应用的网络服务请求进行负载均衡。此外，它还允许您的网络应用使用任何编程语言，而不仅仅是Python。
- en: '![Diagram showing a user interacting with a web app, which sends inputs to
    a web service hosting a model and receives predictions in return.](assets/hmls_0221.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![展示用户与网页应用程序交互的图表，该应用程序将输入发送到托管模型的网络服务，并接收预测结果。](assets/hmls_0221.png)'
- en: Figure 2-21\. A model deployed as a web service and used by a web application
  id: totrans-469
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-21. 作为网络服务部署的模型及其被网页应用程序使用的场景
- en: 'Another popular strategy is to deploy your model to the cloud, for example
    on Google’s Vertex AI (formerly Google Cloud AI Platform and Google Cloud ML Engine):
    just save your model using `joblib` and upload it to Google Cloud Storage (GCS),
    then go to Vertex AI and create a new model version, pointing it to the GCS file.
    That’s it! This gives you a simple web service that takes care of load balancing
    and scaling for you. It takes JSON requests containing the input data (e.g., of
    a district) and returns JSON responses containing the predictions. You can then
    use this web service in your website (or whatever production environment you are
    using).'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的策略是将您的模型部署到云端，例如在Google的Vertex AI（以前称为Google Cloud AI Platform和Google
    Cloud ML Engine）上：只需使用`joblib`保存您的模型并将其上传到Google Cloud Storage（GCS），然后转到Vertex
    AI并创建一个新的模型版本，将其指向GCS文件。就这样！这为您提供了一个简单的网络服务，它会为您处理负载均衡和扩展。它接受包含输入数据的JSON请求（例如，某个地区的输入数据），并返回包含预测结果的JSON响应。然后您可以在您的网站（或您正在使用的任何生产环境中）使用此网络服务。
- en: 'But deployment is not the end of the story. You also need to write monitoring
    code to check your system’s live performance at regular intervals and trigger
    alerts when it drops. It may drop very quickly, for example if a component breaks
    in your infrastructure, but be aware that it could also decay very slowly, which
    can easily go unnoticed for a long time. This is quite common because of data
    drift: if the model was trained with last year’s data, it may not be adapted to
    today’s data.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 但部署并不是故事的结束。您还需要编写监控代码，定期检查系统的实时性能，并在性能下降时触发警报。它可能下降得非常快，例如，如果您的基础设施中的某个组件损坏，但请注意，它也可能非常缓慢地衰减，这可能会长时间不被注意。这种情况很常见，因为数据漂移：如果模型是用去年的数据训练的，那么它可能无法适应今天的数据。
- en: So, you need to monitor your model’s live performance. But how do you do that?
    Well, it depends. In some cases, the model’s performance can be inferred from
    downstream metrics. For example, if your model is part of a recommender system
    and it suggests products that the users may be interested in, then it’s easy to
    monitor the number of recommended products sold each day. If this number drops
    (compared to nonrecommended products), then the prime suspect is the model. This
    may be because the data pipeline is broken, or perhaps the model needs to be retrained
    on fresh data (as we will discuss shortly).
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您需要监控模型的实时性能。但您如何做到这一点呢？嗯，这取决于情况。在某些情况下，模型的表现可以从下游指标中推断出来。例如，如果您的模型是推荐系统的一部分，并且它建议用户可能感兴趣的产品，那么监控每天销售的建议产品数量很容易。如果这个数字下降了（与非推荐产品相比），那么首要嫌疑人就是模型。这可能是因为数据管道损坏，或者可能需要用新鲜数据重新训练模型（我们将在稍后讨论）。
- en: However, you may also need human analysis to assess the model’s performance.
    For example, suppose you trained an image classification model (we’ll look at
    these in [Chapter 3](ch03.html#classification_chapter)) to detect various product
    defects on a production line. How can you get an alert if the model’s performance
    drops, before thousands of defective products get shipped to your clients? One
    solution is to send to human raters a sample of all the pictures that the model
    classified (especially pictures that the model wasn’t so sure about). Depending
    on the task, the raters may need to be experts, or they could be nonspecialists,
    such as workers on a crowdsourcing platform (e.g., Amazon Mechanical Turk). In
    some applications they could even be the users themselves, responding, for example,
    via surveys or repurposed captchas.⁠^([15](ch02.html#id1274))
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您可能还需要人工分析来评估模型的表现。例如，假设您训练了一个图像分类模型（我们将在[第3章](ch03.html#classification_chapter)中探讨这些内容）来检测生产线上的各种产品缺陷。在数千个有缺陷的产品被发送到您的客户之前，您如何获得模型性能下降的警报？一个解决方案是将模型分类的所有图片样本（特别是模型不太确定的图片）发送给人工评分员。根据任务的不同，评分员可能是专家，也可能是非专业人士，例如众包平台（例如，Amazon
    Mechanical Turk）上的工人。在某些应用中，他们甚至可以是用户本人，例如通过调查或重新设计的验证码进行响应。⁠^([15](ch02.html#id1274))
- en: Either way, you need to put in place a monitoring system (with or without human
    raters to evaluate the live model), as well as all the relevant processes to define
    what to do in case of failures and how to prepare for them. Unfortunately, this
    can be a lot of work. In fact, it is often much more work than building and training
    a model.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，你都需要建立一个监控系统（无论是否有人类评分员来评估实时模型），以及所有相关流程来定义在出现故障时应该做什么以及如何准备。不幸的是，这可能是一项大量工作。事实上，这通常比构建和训练模型的工作量要大得多。
- en: 'If the data keeps evolving, you will need to update your datasets and retrain
    your model regularly. You should probably automate the whole process as much as
    possible. Here are a few things you can automate:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据持续演变，你需要定期更新你的数据集并重新训练你的模型。你很可能需要尽可能自动化整个过程。以下是一些你可以自动化的内容：
- en: Collect fresh data regularly and label it (e.g., using human raters).
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期收集新鲜数据并对其进行标记（例如，使用人类评分员）。
- en: Write a script to train the model and fine-tune the hyperparameters automatically.
    This script could run automatically, for example every day or every week, depending
    on your needs.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写一个脚本来训练模型并自动微调超参数。这个脚本可以自动运行，例如每天或每周，具体取决于你的需求。
- en: Write another script that will evaluate both the new model and the previous
    model on the updated test set, and deploy the model to production if the performance
    has not decreased (if it did, make sure you investigate why). The script should
    probably test the performance of your model on various subsets of the test set,
    such as poor or rich districts, rural or urban districts, etc.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写另一个脚本来评估新模型和旧模型在更新的测试集上的表现，如果性能没有下降（如果下降了，确保你调查原因），则将模型部署到生产环境中。这个脚本可能需要测试你的模型在测试集的各个子集上的性能，例如贫困或富裕地区，农村或城市地区等。
- en: You should also make sure you evaluate the model’s input data quality. Sometimes
    performance will degrade slightly because of a poor-quality signal (e.g., a malfunctioning
    sensor sending random values, or another team’s output becoming stale), but it
    may take a while before your system’s performance degrades enough to trigger an
    alert. If you monitor your model’s inputs, you may catch this earlier. For example,
    you could trigger an alert if more and more inputs are missing a feature, or the
    mean or standard deviation drifts too far from the training set, or a categorical
    feature starts containing new categories.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该确保评估模型输入数据的质量。有时性能会因为信号质量差（例如，一个故障的传感器发送随机值，或另一个团队的输出变得过时）而略有下降，但可能需要一段时间，你的系统性能才会下降到足以触发警报的程度。如果你监控模型的输入，你可能可以更早地捕捉到这一点。例如，你可以触发一个警报，如果越来越多的输入缺失一个特征，或者均值或标准差偏离训练集太远，或者一个分类特征开始包含新的类别。
- en: Finally, make sure you keep backups of every model you create and have the process
    and tools in place to roll back to a previous model quickly, in case the new model
    starts failing badly for some reason. Having backups also makes it possible to
    easily compare new models with previous ones. Similarly, you should keep backups
    of every version of your datasets so that you can roll back to a previous dataset
    if the new one ever gets corrupted (e.g., if the fresh data that gets added to
    it turns out to be full of outliers). Having backups of your datasets also allows
    you to evaluate any model against any previous dataset.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，确保你为每个创建的模型都保留备份，并确保有流程和工具可以快速回滚到旧模型，以防新模型由于某种原因开始严重失败。有备份也可以让你轻松比较新旧模型。同样，你应该为数据集的每个版本都保留备份，以便在新的数据集被损坏时（例如，如果添加到其中的新鲜数据结果是一堆异常值）可以回滚到旧数据集。保留数据集的备份还允许你评估任何模型与任何旧数据集。
- en: As you can see, machine learning involves quite a lot of infrastructure. This
    is a very broad topic called *ML Operations* (MLOps), which deserves its own book.
    So don’t be surprised if your first ML project takes a lot of effort and time
    to build and deploy to production. Fortunately, once all the infrastructure is
    in place, going from idea to production will be much faster.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，机器学习涉及相当多的基础设施。这是一个非常广泛的话题，被称为*机器学习操作*（MLOps），它值得有自己的书籍。所以，如果你的第一个ML项目需要大量的努力和时间来构建和部署到生产环境，请不要感到惊讶。幸运的是，一旦所有基础设施都到位，从想法到生产的过程将会快得多。
- en: Try It Out!
  id: totrans-482
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试一下！
- en: 'Hopefully this chapter gave you a good idea of what a machine learning project
    looks like as well as showing you some of the tools you can use to train a great
    system. As you can see, much of the work is in the data preparation step: building
    monitoring tools, setting up human evaluation pipelines, and automating regular
    model training. The machine learning algorithms are important, of course, but
    it is probably preferable to be comfortable with the overall process and know
    three or four algorithms well rather than to spend all your time exploring advanced
    algorithms.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这一章能给你一个关于机器学习项目外观的好想法，同时向你展示一些你可以用来训练优秀系统的工具。正如你所看到的，大部分工作都在数据准备步骤：构建监控工具，设置人工评估管道，以及自动化常规模型训练。机器学习算法当然很重要，但可能更可取的是对整个过程感到舒适，并且对三四个算法有很好的了解，而不是把所有时间都花在探索高级算法上。
- en: 'So, if you have not already done so, now is a good time to pick up a laptop,
    select a dataset that you are interested in, and try to go through the whole process
    from A to Z. A good place to start is on a competition website such as [Kaggle](https://kaggle.com):
    you will have a dataset to play with, a clear goal, and people to share the experience
    with. Have fun!'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你还没有这样做，现在是一个好时机去挑选一台笔记本电脑，选择一个你感兴趣的数据库集，并尝试从头到尾完成整个过程。一个好的开始地方是在一个竞赛网站上，比如[Kaggle](https://kaggle.com)：你将有一个可以玩耍的数据集，一个明确的目标，以及可以分享经验的人。祝你好玩！
- en: Exercises
  id: totrans-485
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'The following exercises are based on this chapter’s housing dataset:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 以下练习基于本章的住房数据集：
- en: Try a support vector machine regressor (`sklearn.svm.SVR`) with various hyperparameters,
    such as `kernel="linear"` (with various values for the `C` hyperparameter) or
    `kernel="rbf"` (with various values for the `C` and `gamma` hyperparameters).
    Note that support vector machines don’t scale well to large datasets, so you should
    probably train your model on just the first 5,000 instances of the training set
    and use only 3-fold cross-validation, or else it will take hours. Don’t worry
    about what the hyperparameters mean for now; these are explained in the online
    chapter on SVMs at [*https://homl.info/*](https://homl.info/). How does the best
    `SVR` predictor perform?
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用具有各种超参数的支持向量机回归器（`sklearn.svm.SVR`），例如`kernel="linear"`（`C`超参数的多个值）或`kernel="rbf"`（`C`和`gamma`超参数的多个值）。请注意，支持向量机在大数据集上扩展性不好，因此你可能在训练集的前5,000个实例上训练模型，并仅使用3折交叉验证，否则可能需要数小时。现在不用担心超参数的含义；这些在SVMs的在线章节中有解释[*https://homl.info/*](https://homl.info/)。最佳的`SVR`预测器表现如何？
- en: Try replacing the `GridSearchCV` with a `RandomizedSearchCV`.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试将`GridSearchCV`替换为`RandomizedSearchCV`。
- en: Try adding a `SelectFromModel` transformer in the preparation pipeline to select
    only the most important attributes.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在准备管道中添加一个`SelectFromModel`转换器，以仅选择最重要的属性。
- en: Try creating a custom transformer that trains a *k*-nearest neighbors regressor
    (`sklearn.neighbors.KNeighborsRegressor`) in its `fit()` method, and outputs the
    model’s predictions in its `transform()` method. The KNN regressor should use
    only the latitude and longitude as input and predict the median income. Next,
    add this new transformer to the preprocessing pipeline. This will add a feature
    representing the smoothed median income over the nearby districts.
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试创建一个自定义转换器，该转换器在其`fit()`方法中训练一个**k**-最近邻回归器（`sklearn.neighbors.KNeighborsRegressor`），并在其`transform()`方法中输出模型的预测结果。KNN回归器应仅使用纬度和经度作为输入，并预测中位数收入。接下来，将这个新的转换器添加到预处理管道中。这将添加一个表示附近地区平滑中位数收入的特征。
- en: Automatically explore some preparation options using `RandomizedSearchCV`.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`RandomizedSearchCV`自动探索一些准备选项。
- en: 'Try to implement the `StandardScalerClone` class again from scratch, then add
    support for the `inverse_transform()` method: executing `scaler.​inverse_transform(scaler.fit_transform(X))`
    should return an array very close to `X`. Then add support for feature names:
    set `feature_names_in_` in the `fit()` method if the input is a DataFrame. This
    attribute should be a NumPy array of column names. Lastly, implement the `get_feature_names_out()`
    method: it should have one optional `input_features=None` argument. If passed,
    the method should check that its length matches `n_features_in_`, and it should
    match `feature_names_in_` if it is defined; then `input_features` should be returned.
    If `input_features` is `None`, then the method should either return `feature_names_in_`
    if it is defined or `np.array(["x0", "x1", ...])` with length `n_features_in_`
    otherwise.'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次从头开始实现 `StandardScalerClone` 类，然后添加对 `inverse_transform()` 方法的支持：执行 `scaler.inverse_transform(scaler.fit_transform(X))`
    应该返回一个与 `X` 非常接近的数组。然后添加对特征名称的支持：如果输入是 DataFrame，则在 `fit()` 方法中设置 `feature_names_in_`。这个属性应该是一个包含列名称的
    NumPy 数组。最后，实现 `get_feature_names_out()` 方法：它应该有一个可选的 `input_features=None` 参数。如果传递了参数，方法应该检查其长度是否与
    `n_features_in_` 匹配，如果已定义，则应与 `feature_names_in_` 匹配；然后返回 `input_features`。如果
    `input_features` 是 `None`，则方法应该返回 `feature_names_in_`（如果已定义）或 `np.array(["x0",
    "x1", ...])`，其长度为 `n_features_in_`。
- en: 'Tackle a regression task of your choice by following the process you learned
    in this chapter. For example, you can try tackling the [Vehicle dataset](https://homl.info/usedcars),
    where the goal is to predict the selling price of a used car, based on its age,
    the number of kilometers it has driven, its make and model, and more. Another
    good dataset to try is the [Bike Sharing dataset](https://homl.info/bikes): the
    objective is to predict the number of bikes rented within a period of time (column
    `cnt`), based on the day of the week, the time, and the weather conditions.'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过遵循本章学到的过程，解决你选择的回归任务。例如，你可以尝试解决[车辆数据集](https://homl.info/usedcars)，目标是根据车辆年龄、行驶公里数、制造商和型号等预测二手车售价。另一个值得尝试的数据集是[共享单车数据集](https://homl.info/bikes)：目标是预测一段时间内租借自行车的数量（列
    `cnt`），基于星期几、时间和天气状况。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解答可在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: '^([1](ch02.html#id990-marker)) The original dataset appeared in R. Kelley Pace
    and Ronald Barry, “Sparse Spatial Autoregressions”, *Statistics & Probability
    Letters* 33, no. 3 (1997): 291–297.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.html#id990-marker)) 原始数据集出现在 R. Kelley Pace 和 Ronald Barry 的文章中，“稀疏空间自回归”，*统计学与概率通讯*
    33，第3期（1997年）：291–297。
- en: '^([2](ch02.html#id994-marker)) A piece of information fed to a machine learning
    system is often called a *signal*, in reference to Claude Shannon’s information
    theory, which he developed at Bell Labs to improve telecommunications. His theory:
    you want a high signal-to-noise ratio.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.html#id994-marker)) 机器学习系统中输入的信息通常被称为 *信号*，这是参考克劳德·香农的信息理论，他在贝尔实验室开发这一理论以改善电信技术。他的理论是：你希望信号与噪声的比值高。
- en: ^([3](ch02.html#id1005-marker)) Recall that the transpose operator flips a column
    vector into a row vector (and vice versa).
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.html#id1005-marker)) 回想一下，转置操作将列向量翻转成行向量（反之亦然）。
- en: ^([4](ch02.html#id1032-marker)) You might also need to check legal constraints,
    such as private fields that should never be copied to unsafe data stores.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.html#id1032-marker)) 你可能还需要检查法律约束，例如那些不应复制到不安全数据存储的私有字段。
- en: '^([5](ch02.html#id1038-marker)) The standard deviation is generally denoted
    *σ* (the Greek letter sigma), and it is the square root of the *variance*, which
    is the average of the squared deviation from the mean. When a feature has a bell-shaped
    *normal distribution* (also called a *Gaussian distribution*), which is very common,
    the “68-95-99.7” rule applies: about 68% of the values fall within 1*σ* of the
    mean, 95% within 2*σ*, and 99.7% within 3*σ*.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch02.html#id1038-marker)) 标准差通常用 *σ*（希腊字母西格玛）表示，它是 *方差* 的平方根，而方差是平均的平方偏差。当一个特征具有钟形
    *正态分布*（也称为 *高斯分布*），这在现实中非常常见时，适用“68-95-99.7”规则：大约68%的值在平均值加减1*σ*的范围内，95%在加减2*σ*的范围内，99.7%在加减3*σ*的范围内。
- en: ^([6](ch02.html#id1054-marker)) You will often see people set the random seed
    to 42\. This number has no special property, other than being the Answer to the
    Ultimate Question of Life, the Universe, and Everything.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch02.html#id1054-marker)) 你经常会看到人们将随机种子设置为42。这个数字没有特殊属性，除了它是生命、宇宙和万物的终极问题的答案。
- en: ^([7](ch02.html#id1055-marker)) The location information is actually quite coarse,
    and as a result many districts will have the exact same ID, so they will end up
    in the same set (test or train). This introduces some unfortunate sampling bias.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch02.html#id1055-marker)) 位置信息实际上相当粗糙，因此许多区域将具有完全相同的ID，因此它们将最终位于同一个集合（测试或训练）中。这引入了一些不幸的采样偏差。
- en: ^([8](ch02.html#id1068-marker)) If you are reading this in grayscale, grab a
    red pen and scribble over most of the coastline from the Bay Area down to San
    Diego (as you might expect). You can add a patch of yellow around Sacramento as
    well.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch02.html#id1068-marker)) 如果你正在以灰度模式阅读，拿一支红笔在从湾区到圣地亚哥的大部分海岸线处涂鸦（正如你所预期的那样）。你还可以在萨克拉门托周围添加一块黄色。
- en: '^([9](ch02.html#id1102-marker)) For more details on the design principles,
    see Lars Buitinck et al., “API Design for Machine Learning Software: Experiences
    from the Scikit-Learn Project”, arXiv preprint arXiv:1309.0238 (2013).'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch02.html#id1102-marker)) 关于设计原则的更多细节，请参阅Lars Buitinck等人撰写的“机器学习软件的API设计：Scikit-Learn项目的经验”，arXiv预印本arXiv:1309.0238（2013）。
- en: ^([10](ch02.html#id1103-marker)) Some predictors also provide methods to measure
    the confidence of their predictions.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch02.html#id1103-marker)) 一些预测器还提供了测量其预测置信度的方法。
- en: '^([11](ch02.html#id1107-marker)) If you run `sklearn.set_config(transform_output="pandas")`,
    all transformers will output Pandas DataFrames when they receive a DataFrame as
    input: Pandas in, Pandas out.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch02.html#id1107-marker)) 如果你运行`sklearn.set_config(transform_output="pandas")`，所有转换器在接收到DataFrame作为输入时都会输出Pandas
    DataFrame：Pandas输入，Pandas输出。
- en: ^([12](ch02.html#id1118-marker)) See SciPy’s documentation for more details.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch02.html#id1118-marker)) 更多细节请参阅SciPy的文档。
- en: '^([13](ch02.html#id1177-marker)) With duck typing, an object’s methods and
    behavior are what matters, not its type: “if it looks like a duck and quacks like
    a duck, it must be a duck”.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch02.html#id1177-marker)) 使用鸭子类型，一个对象的方法和行为才是重要的，而不是它的类型：“如果它看起来像鸭子，叫起来也像鸭子，那么它一定是一只鸭子”。
- en: ^([14](ch02.html#id1268-marker)) In a nutshell, a REST (or RESTful) API is an
    HTTP-based API that follows some conventions, such as using standard HTTP verbs
    to read, update, create, or delete resources (GET, POST, PUT, and DELETE) and
    using JSON for the inputs and outputs.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch02.html#id1268-marker)) 简而言之，REST（或RESTful）API是一个基于HTTP的API，它遵循一些约定，例如使用标准的HTTP动词来读取、更新、创建或删除资源（GET、POST、PUT和DELETE），并使用JSON作为输入和输出。
- en: ^([15](ch02.html#id1274-marker)) A captcha is a test to ensure a user is not
    a robot. These tests have often been used as a cheap way to label training data.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch02.html#id1274-marker)) 验证码是一种测试，以确保用户不是机器人。这些测试通常被用作标记训练数据的廉价方式。
