["```py\nimport plotly.graph_objects as go\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndocuments = [      #1\n    \"The sky is blue and beautiful.\",\n    \"Love this blue and beautiful sky!\",\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"A king's breakfast has sausages, ham, bacon, eggs, toast, and beans\",\n    \"I love green eggs, ham, sausages and bacon!\",\n    \"The brown fox is quick and the blue dog is lazy!\",\n    \"The sky is very blue and the sky is very beautiful today\",\n    \"The dog is lazy but the brown fox is quick!\"\n]\n\nvectorizer = TfidfVectorizer()     #2\nX = vectorizer.fit_transform(documents)      #3\n```", "```py\ncosine_similarities = cosine_similarity(X)      #1\n\nwhile True:      #2\n    selected_document_index = input(f\"Enter a document number\n↪ (0-{len(documents)-1}) or 'exit' to quit: \").strip()\n\n    if selected_document_index.lower() == 'exit':\n        break\n\n    if not selected_document_index.isdigit() or \n↪ not 0 <= int(selected_document_index) < len(documents):\n        print(\"Invalid input. Please enter a valid document number.\")\n        continue\n\n    selected_document_index = int(selected_document_index)    #3\n\n    selected_document_similarities = cosine_similarities[selected_document_index]     #4\n\n# code to plot document similarities omitted\n```", "```py\n# code above omitted\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(documents)\nvector_database = X.toarray()     #1\n\ndef cosine_similarity_search(query,\n                             database, \n                             vectorizer, \n                             top_n=5):     #2\n    query_vec = vectorizer.transform([query]).toarray()\n    similarities = cosine_similarity(query_vec, database)[0]\n    top_indices = np.argsort(-similarities)[:top_n]  # Top n indices\n    return [(idx, similarities[idx]) for idx in top_indices]\n\nwhile True:      #3\n    query = input(\"Enter a search query (or 'exit' to stop): \")\n    if query.lower() == 'exit':\n        break\n    top_n = int(input(\"How many top matches do you want to see? \"))\n    search_results = cosine_similarity_search(query,\n                                              vector_database, \n                                              vectorizer, \n                                              top_n)\n\n    print(\"Top Matched Documents:\")\n    for idx, score in search_results:\n        print(f\"- {documents[idx]} (Score: {score:.4f})\")   #4\n\n    print(\"\\n\")\n###Output\nEnter a search query (or 'exit' to stop): blue\nHow many top matches do you want to see? 3\nTop Matched Documents:\n- The sky is blue and beautiful. (Score: 0.4080)\n- Love this blue and beautiful sky! (Score: 0.3439)\n- The brown fox is quick and the blue dog is lazy! (Score: 0.2560)\n```", "```py\nload_dotenv()      #1\napi_key = os.getenv('OPENAI_API_KEY')\nif not api_key:\n    raise ValueError(\"No API key found. Please check your .env file.\")\nclient = OpenAI(api_key=api_key)     #1            \n\ndef get_embedding(text, model=\"text-embedding-ada-002\"):     #2\n    text = text.replace(\"\\n\", \" \")\n    return client.embeddings.create(input=[text],\n              model=model).data[0].embedding                #2\n\n# Sample documents (omitted)\n\nembeddings = [get_embedding(doc) for doc in documents]    #3\nprint(embeddings_array.shape)\n\nembeddings_array = np.array(embeddings)    #4\n\npca = PCA(n_components=3)   #5\nreduced_embeddings = pca.fit_transform(embeddings_array)\n```", "```py\nembeddings = [get_embedding(doc) for doc in documents]     #1\nids = [f\"id{i}\" for i in range(len(documents))]           #1\n\nchroma_client = chromadb.Client()               #2\ncollection = chroma_client.create_collection(\n                       name=\"documents\")       #2\ncollection.add(     #3\n    embeddings=embeddings,\n    documents=documents,\n    ids=ids\n)\n\ndef query_chromadb(query, top_n=2):      #4\n    query_embedding = get_embedding(query)\n    results = collection.query(\n        query_embeddings=[query_embedding],\n        n_results=top_n\n    )\n    return [(id, score, text) for id, score, text in\n            zip(results['ids'][0],\n                results['distances'][0], \n                results['documents'][0])]\n\nwhile True:     #5\n    query = input(\"Enter a search query (or 'exit' to stop): \")\n    if query.lower() == 'exit':\n        break\n    top_n = int(input(\"How many top matches do you want to see? \"))\n    search_results = query_chromadb(query, top_n)\n\n    print(\"Top Matched Documents:\")\n    for id, score, text in search_results:\n        print(f\"\"\"\nID:{id} TEXT: {text} SCORE: {round(score, 2)}\n\"\"\")    #5\n\n    print(\"\\n\")\n###Output\nEnter a search query (or 'exit' to stop): dogs are lazy\nHow many top matches do you want to see? 3\nTop Matched Documents:\nID:id7 TEXT: The dog is lazy but the brown fox is quick! SCORE: 0.24\nID:id5 TEXT: The brown fox is quick and the blue dog is lazy! SCORE: 0.28\nID:id2 TEXT: The quick brown fox jumps over the lazy dog. SCORE: 0.29\n```", "```py\nFrom langchain_community.document_loaders \n                     ↪ import UnstructuredHTMLLoader    #1\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n#previous code\n\nloader = UnstructuredHTMLLoader(\n                   \"sample_documents/mother_goose.html\")   #2\ndata = loader.load    #3\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=100,\n    chunk_overlap=25,     #4\n    length_function=len,\n    add_start_index=True,\n)\ndocuments = text_splitter.split_documents(data)\n\ndocuments = [doc.page_content \n                ↪ for doc in documents] [100:350]   #5\n\nembeddings = [get_embedding(doc) for doc in documents]     #6\nids = [f\"id{i}\" for i in range(len(documents))]\n###Output\nEnter a search query (or 'exit' to stop): **who kissed the girls and made** \n**them cry?**\nHow many top matches do you want to see? 3\nTop Matched Documents:\nID:id233 TEXT: And chid her daughter,\n        And kissed my sister instead of me. SCORE: 0.4…\n```", "```py\nloader = UnstructuredHTMLLoader(\"sample_documents/mother_goose.html\")\ndata = loader.load()\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=50, chunk_overlap=10      #1\n)\n\ndocuments = text_splitter.split_documents(data)\ndocuments = [doc for doc in documents][8:94]      #2\n\ndb = Chroma.from_documents(documents, OpenAIEmbeddings())\n\ndef query_documents(query, top_n=2):\n    docs = db.similarity_search(query, top_n)      #3\n    return docs\n###Output\nCreated a chunk of size 68, \nwhich is longer than the specified 50\nCreated a chunk of size 67, \nwhich is longer than the specified 50     #4\nEnter a search query (or 'exit' to stop): \n                     who kissed the girls and made them cry?\nHow many top matches do you want to see? 3\nTop Matched Documents:\nDocument 1: GEORGY PORGY\n\n        Georgy Porgy, pudding and pie,\n        Kissed the girls and made them cry.\n```", "```py\nEnter a search query (or 'exit' to stop): Who made the girls cry?\nHow many top matches do you want to see? 3\nTop Matched Documents:\nDocument 1: WILLY, WILLY\n\n        Willy, Willy Wilkin…\n```", "```py\n# to install and run\npip install git+https://github.com/cxbxmxcx/Nexus.git\n\nnexus run\n# install in development mode\ngit clone https://github.com/cxbxmxcx/Nexus.git\n\n# Install the cloned repository in editable mode\npip install -e Nexus\n```", "```py\nSummarize the conversation and create a set of statements that summarize \nthe conversation. Return a JSON object with the following keys: 'summary'. \nEach key should have a list of statements that are relevant to that \ncategory. Return only the JSON object and nothing else.\n```"]