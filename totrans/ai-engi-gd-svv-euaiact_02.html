<html><head></head><body><section data-pdf-bookmark="Chapter 2. AI Engineering: A Proactive Compliance Catalyst" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_2_ai_engineering_a_proactive_compliance_catalyst_1748539917637495">&#13;
<h1><span class="label">Chapter 2. </span>AI Engineering: A Proactive <span class="keep-together">Compliance Catalyst</span></h1>&#13;
&#13;
<p>AI engineering<a contenteditable="false" data-primary="AI engineering" data-type="indexterm" id="id387"/> can support companies in complying with the EU AI Act by promoting appropriate tools and practices. A key part of this is machine learning operations (MLOps), which focuses on the operationalization of ML models. Together, these disciplines provide the technical components and repeatable processes necessary for designing, developing, deploying, and maintaining AI systems in a reliable and compliant way.</p>&#13;
&#13;
<p>Implementing engineering practices such as automation, versioning, testing, reproducibility, deployment, and monitoring helps ensure AI systems meet the EU AI Act’s requirements for high-quality, safe, and trustworthy AI. This chapter introduces some practical frameworks that you can use to design and architect compliant AI systems (although these are by no means the only options). Tools like the Machine Learning Canvas and the MLOps Stack Canvas are designed to address key requirements such as risk management, technical documentation, transparency, robustness, and post-market monitoring. We’ll also discuss CRISP-ML(Q), a machine learning process model with built-in quality assurance that emphasizes risk assessment, quality assurance, comprehensive documentation, and continuous monitoring throughout the AI lifecycle. The synergy between CRISP-ML(Q) and AI engineering enables organizations to proactively engineer compliance into the AI lifecycle, ensuring that AI systems are developed and deployed in an EU AI Act–compliant manner.</p>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Structuring the AI System Development Process with CRISP-ML(Q)" data-type="sect1"><div class="sect1" id="chapter_2_structuring_the_ai_system_development_process_with_1748539917637703">&#13;
<h1 class="less_space">Structuring the AI System Development Process with CRISP-ML(Q)</h1>&#13;
&#13;
<p>In this section, we’ll explore developing <em>AI-powered applications</em>, which are software applications that embed machine learning models as a technological solution for one or several features. <a contenteditable="false" data-primary="quality assurance" data-type="indexterm" id="id388"/>We’ll focus on CRISP-ML(Q), the Cross-Industry Standard Process for the development of Machine Learning applications with Quality assurance methodology<a contenteditable="false" data-primary="CRISP-ML(Q)" data-type="indexterm" id="crisp-2"/>. CRISP-ML(Q) offers a systematic and quality-focused approach for developing AI systems, facilitating compliance with the EU AI Act across various risk levels.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><a data-type="xref" href="app01.html#appendix_a_designing_ai_powered_applications_1748539915159206">Appendix A</a> introduces the principles of designing AI-powered applications with an emphasis on aligning AI initiatives with business goals and user needs. As explained in the appendix, the “backward thinking” approach and the <a contenteditable="false" data-primary="Machine Learning Canvas" data-type="indexterm" id="id389"/>Machine Learning Canvas offer practical frameworks for planning and executing AI projects <span class="keep-together">effectively</span>.</p>&#13;
&#13;
<p>The Machine Learning Canvas underlines the importance of a holistic perspective, which incorporates the value proposition, monitoring, and the integration of both prediction and training components within a larger software system. Furthermore, it highlights critical considerations around data management, ethical implications in feature engineering, and the collaborative nature of building successful AI products. See <a data-type="xref" href="app01.html#appendix_a_designing_ai_powered_applications_1748539915159206">Appendix A</a> for further details.</p>&#13;
</div>&#13;
&#13;
<p>As described in the paper <a href="https://oreil.ly/srSRa">“Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology”</a>, CRISP-ML(Q) builds upon the established CRISP-DM<a contenteditable="false" data-primary="CRISP-DM" data-type="indexterm" id="id390"/> framework (see <a data-type="xref" href="#chapter_2_figure_1_1748539917622735">Figure 2-1</a>). The motivation for extending CRISP-DM was twofold: it focused on data mining rather than ML models deployed over extended periods, and it lacked guidance on quality assurance. CRISP-ML(Q) better meets the needs and challenges of designing and operationalizing machine learning products, with a strong emphasis on quality assurance.</p>&#13;
&#13;
<figure><div class="figure" id="chapter_2_figure_1_1748539917622735"><img src="assets/taie_0201.png"/>&#13;
<h6><span class="label">Figure 2-1. </span>The phases of the CRISP-ML(Q) framework</h6>&#13;
</div></figure>&#13;
&#13;
<section data-pdf-bookmark="Risk Mitigation, Quality Assurance, and Alignment with the EU AI Act" data-type="sect2"><div class="sect2" id="chapter_2_risk_mitigation_quality_assurance_and_alignment_1748539917637775">&#13;
<h2>Risk Mitigation, Quality Assurance, and Alignment with the EU AI Act</h2>&#13;
&#13;
<p>Working with CRISP-ML(Q)<a contenteditable="false" data-primary="quality assurance" data-type="indexterm" id="id391"/><a contenteditable="false" data-primary="risk mitigation" data-type="indexterm" id="id392"/> will help you incorporate the requirements of the EU AI Act for high-risk and limited-risk AI systems into your development processes. The framework emphasizes continuous risk assessment throughout the ML development process, from understanding the business needs to deployment and monitoring. This approach aligns with the EU AI Act’s requirements for ongoing quality assessment and risk management.</p>&#13;
&#13;
<p>As you learned in <a data-type="xref" href="ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819">Chapter 1</a>, the EU AI Act defines four levels of risk for AI systems: unacceptable, high, limited, and minimal or no risk. CRISP-ML(Q)<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="alignment with the EU AI Act" data-type="indexterm" id="id393"/> can accommodate these different classifications by tailoring its quality assurance methodology accordingly. For example, high-risk AI systems will require more extensive testing, documentation, and human oversight than limited- or minimal-risk systems.</p>&#13;
&#13;
<p>For each ML development phase, CRISP-ML(Q) defines specific requirements and constraints to address risks (such as bias, overfitting, or lack of reproducibility) that could impact the success of the ML system. Appropriate quality assurance methods, such as cross-validation and thorough documentation processes, are implemented to mitigate these risks. This supports compliance with the EU AI Act’s requirement that providers establish robust risk management systems to identify, assess, and mitigate risks.</p>&#13;
&#13;
<p>CRISP-ML(Q) also mandates that AI systems be thoroughly tested prior to deployment to ensure they perform consistently for their intended purpose and comply with requirements. Testing is carried out against defined metrics and thresholds. This meets the EU AI Act’s requirement for pre-market conformity assessments for <span class="keep-together">high-risk</span> AI systems. Additionally, CRISP-ML(Q) incorporates continuous post-deployment monitoring to detect performance degradation or deviations. Serious incidents must be reported. The EU AI Act has similar requirements for market surveillance, human oversight, and incident reporting by providers and users of high-risk AI.</p>&#13;
&#13;
<p>Finally, CRISP-ML(Q) requires that you document the entire ML development process, including the risk management measures. This is important because the EU AI Act introduces technical documentation and transparency obligations to ensure humans are appropriately informed when interacting with AI systems.</p>&#13;
&#13;
<p>In summary, the framework’s quality assurance methodology and emphasis on continuous risk management throughout the ML lifecycle are well aligned with the risk-based approach mandated by the EU AI Act. Adopting CRISP-ML(Q) can help organizations proactively identify and mitigate the risks of AI systems to ensure safer and more trustworthy AI<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="six phases of" data-type="indexterm" id="crisp-phases-1"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="The Six Phases of CRISP-ML(Q)" data-type="sect2"><div class="sect2" id="chapter_2_the_six_phases_of_crisp_ml_q_1748539917637847">&#13;
<h2>The Six Phases of CRISP-ML(Q)</h2>&#13;
&#13;
<p>As described in the previous section, a key principle of CRISP-ML(Q) is integrating quality assurance practices into each phase of the AI development lifecycle. This includes defining requirements, identifying risks, and applying risk mitigation methods based on established best practices. As a result, CRISP-ML(Q) provides a comprehensive methodology for developing a high-quality, sustainable AI system that meets the defined business objectives and is ready for real-world deployment.</p>&#13;
&#13;
<p>CRISP-ML(Q) defines six key phases in the ML development process:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Business and data understanding</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Data preparation</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Modeling</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Evaluation</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Deployment</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Monitoring and maintenance</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>Each phase involves specific tasks and results in specific outputs. Importantly, while CRISP-ML(Q)<a contenteditable="false" data-primary="CRISP-ML(Q)" data-startref="crisp-2" data-type="indexterm" id="id394"/> is often depicted as a sequential process, it’s actually iterative and admits several backward trajectories—insights gained in later phases often require revisiting earlier steps.</p>&#13;
&#13;
<p>Let’s briefly examine each phase in more detail.</p>&#13;
&#13;
<section data-pdf-bookmark="Business and data understanding" data-type="sect3"><div class="sect3" id="chapter_2_business_and_data_understanding_1748539917637909">&#13;
<h3>Business and data understanding</h3>&#13;
&#13;
<p>The initial phase<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="business and data understanding phase" data-type="indexterm" id="id395"/> focuses on defining the ML application’s business goals and success criteria. Key tasks include:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Determine business objectives</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Identify data sources</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Collect initial data</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Describe data</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Explore data</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Verify data quality</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Close collaboration between business stakeholders and the data science team is critical to ensure business alignment. The main outputs include the project objectives, success criteria, and an initial assessment of data quality and project feasibility.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Data preparation" data-type="sect3"><div class="sect3" id="chapter_2_data_preparation_1748539917637964">&#13;
<h3>Data preparation</h3>&#13;
&#13;
<p>After business goals are defined and communicated across the organization, you are ready to prepare the data for modeling. Data preparation<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="data preparation phase" data-type="indexterm" id="id396"/> tasks include:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Select data</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Clean data</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Construct data</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Integrate data</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Format data</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>This phase involves feature engineering, handling missing values, and normalizing data. Outputs include the final dataset(s) for modeling and documentation of how they were constructed.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Model engineering" data-type="sect3"><div class="sect3" id="chapter_2_model_engineering_1748539917638017">&#13;
<h3>Model engineering</h3>&#13;
&#13;
<p>The core machine learning work happens in this third phase, which is also called the <em>modeling</em> phase<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="model engineering phase" data-type="indexterm" id="id397"/>. Key tasks here include:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Select modeling techniques</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Generate test design</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Build model</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Assess model</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>You should build and compare multiple models using appropriate evaluation metrics and cross-validation techniques. Domain knowledge can be incorporated to improve results. This phase concludes by selecting the best-performing model(s).</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Model evaluation" data-type="sect3"><div class="sect3" id="chapter_2_model_evaluation_1748539917638069">&#13;
<h3>Model evaluation</h3>&#13;
&#13;
<p>Before deploying a model, you must thoroughly evaluate it from both a data science and a business perspective. Model evaluation<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="model evaluation phase" data-type="indexterm" id="id398"/> tasks include:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Evaluate prediction results</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Review process</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Decide on next steps</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>The model is tested on independent data to validate its real-world performance. You should also examine the explainability and robustness of the model, and business stakeholders should verify that the model meets the defined success criteria. Outputs are the final model approval and deployment plan.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Model deployment" data-type="sect3"><div class="sect3" id="chapter_2_model_deployment_1748539917638121">&#13;
<h3>Model deployment</h3>&#13;
&#13;
<p>Deploying the model<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="model deployment phase" data-type="indexterm" id="id399"/> into the production environment involves tasks such as the <span class="keep-together">following</span>:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Plan deployment</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Plan monitoring and maintenance</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Produce final report</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Review project</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Deployment can range from generating a simple report to persisting the model in the model repository or embedding it into a complex production system. Create monitoring and maintenance plans to ensure the model performs well over time.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Model monitoring and maintenance" data-type="sect3"><div class="sect3" id="chapter_2_model_monitoring_and_maintenance_1748539917638174">&#13;
<h3>Model monitoring and maintenance</h3>&#13;
&#13;
<p>ML models can degrade in performance if not proactively monitored<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="model monitoring and maintenance phase" data-type="indexterm" id="id400"/> and updated. This final phase involves these tasks:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Monitor the model</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Evaluate the model</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Retrain/update the model as needed</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Data inputs, model predictions, and business outcomes are continuously tracked to identify issues. Models are retrained on new data to prevent drift. The ML application is maintained and enhanced based on changing business needs.</p>&#13;
&#13;
<p>CRISP-ML(Q)<a contenteditable="false" data-primary="CRISP-ML(Q)" data-type="indexterm" id="id401"/> provides a robust framework for developing machine learning applications. By implementing a systematic process and integrating risk identification with quality assurance<a contenteditable="false" data-primary="quality assurance" data-type="indexterm" id="id402"/>, organizations can enhance the success rate and business impact of their ML initiatives while guaranteeing compliance with the EU AI Act<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="six phases of" data-startref="crisp-phases-1" data-type="indexterm" id="id403"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Understanding MLOps" data-type="sect1"><div class="sect1" id="chapter_2_understanding_mlops_1748539917638242">&#13;
<h1>Understanding MLOps</h1>&#13;
&#13;
<p>In this section we’ll turn our attention to <em>MLOps</em>, <a contenteditable="false" data-primary="machine learning operations (MLOps)" data-seealso="CRISP-ML(Q); MLOps Stack Canvas" data-type="indexterm" id="mlops-1"/>a set of<a contenteditable="false" data-primary="MLOps" data-see="machine learning operations (MLOps)" data-type="indexterm" id="id404"/> processes and technical components designed to ensure AI systems are developed, deployed, and maintained in a continuous and reliable manner. By understanding and implementing MLOps, you establish the technical foundation necessary for complying with the EU AI Act, as many of its requirements depend on MLOps practices.</p>&#13;
&#13;
<p>MLOps practices are not tied to a specific language, framework, platform, or infrastructure. However, deploying ML in production involves various interconnected components. <a data-type="xref" href="#chapter_2_figure_2_1748539917622765">Figure 2-2</a> shows a canonical architecture<a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="canonical architecture" data-type="indexterm" id="id405"/> for ML systems. In the next section, we’ll explore an application- and industry-neutral MLOps Stack Canvas framework that can be used to specify appropriate components.</p>&#13;
&#13;
<figure><div class="figure" id="chapter_2_figure_2_1748539917622765"><img src="assets/taie_0202.png"/>&#13;
<h6><span class="label">Figure 2-2. </span>A canonical architecture for ML systems</h6>&#13;
</div></figure>&#13;
&#13;
<p class="fix_tracking">ML/AI technologies are currently widely utilized across various applications. Consequently, software systems often incorporate ML/AI components as standard practice. However, as organizations seek to enhance their products or processes with machine learning, many ML models remain in the experimental phase or fail to move beyond the proof of concept (PoC) stage. To increase your chances of success, when designing ML systems it’s essential to clearly define the interface, algorithms, data, infrastructure, and hardware to ensure they meet the specific requirements of the software system.</p>&#13;
&#13;
<p>MLOps has emerged as a discipline aimed at automating the development and deployment of machine learning models. Its primary value lies in optimizing business outcomes by improving the efficiency, reliability, and scalability of ML workflows through a supporting technology stack and infrastructure. MLOps also focuses on automating and streamlining the development lifecycle, facilitating deployment into production for newly developed ML models. To ensure model quality and adaptability, any changes in the code, training data, or model should automatically trigger the build process in the ML development pipeline.</p>&#13;
&#13;
<p><a data-type="xref" href="#chapter_2_table_1_1748539917627746">Table 2-1</a> summarizes the core principles of MLOps: automation, version control, rigorous testing, reproducibility, streamlined deployment, and continuous monitoring. Later in the book, we will explore relevant requirements for the EU AI Act and how the MLOps engineering principles <a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="automation" data-type="indexterm" id="id406"/><a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="version control" data-type="indexterm" id="id407"/><a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="testing" data-type="indexterm" id="id408"/>address<a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="reproducibility" data-type="indexterm" id="id409"/><a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="deployment" data-type="indexterm" id="id410"/><a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="monitoring" data-type="indexterm" id="id411"/> these requirements<a contenteditable="false" data-primary="version control (MLOps principle)" data-type="indexterm" id="id412"/><a contenteditable="false" data-primary="testing (MLOps principle)" data-type="indexterm" id="id413"/><a contenteditable="false" data-primary="reproducibility (MLOps principle)" data-type="indexterm" id="id414"/><a contenteditable="false" data-primary="deployment (MLOps principle)" data-type="indexterm" id="id415"/><a contenteditable="false" data-primary="monitoring (MLOps principle)" data-type="indexterm" id="id416"/>.</p>&#13;
&#13;
<table class="striped" id="chapter_2_table_1_1748539917627746">&#13;
	<caption><span class="label">Table 2-1. </span>MLOps core principles</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th>MLOps principle</th>&#13;
			<th>Key aspects</th>&#13;
			<th>Why is it important?</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>&#13;
			<p><em>Automation</em> focuses on transitioning experimental ML models into fully operational production systems. The goal is to automate repetitive tasks in the areas of data preparation, model training, evaluation, deployment, and monitoring.</p>&#13;
			</td>&#13;
			<td>&#13;
			<p>Consider automating:</p>&#13;
&#13;
			<ul>&#13;
				<li>Data pipelines for data ingestion, cleaning, and feature engineering</li>&#13;
				<li>Model training and hyperparameter tuning</li>&#13;
				<li>Testing and validation of models</li>&#13;
				<li>Deployment of models to production environments</li>&#13;
				<li>Monitoring and retraining of models</li>&#13;
				<li>Model auditing</li>&#13;
			</ul>&#13;
			</td>&#13;
			<td>&#13;
			<p>Automation permits increased developer efficiency and productivity and reduces manual errors. It also enables faster iteration and experimentation, making it easier to scale machine learning workflows. Tools like DVC, Airflow, Kubeflow, and MLflow<a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="tools for automation" data-type="indexterm" id="id417"/> can be used to build automated ML pipelines.</p>&#13;
			</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>&#13;
			<p><em>Version control</em> is crucial for tracking and managing changes across the ML lifecycle.</p>&#13;
			</td>&#13;
			<td>&#13;
			<p>What should be versioned:</p>&#13;
&#13;
			<ul>&#13;
				<li>Data: Tracking different versions of datasets used for training and evaluation</li>&#13;
				<li>Code: Version control of ML code, scripts, and notebooks</li>&#13;
				<li>Models: Versioning trained model artifacts and associated metadata</li>&#13;
				<li>Configurations: Tracking hyperparameters, environment configurations, etc.</li>&#13;
			</ul>&#13;
			</td>&#13;
			<td>&#13;
			<p>Proper versioning is essential as it allows for reproducibility of experiments and results, enabling you to roll back to previous versions if any issues occur. It also facilitates collaboration between team members and provides the traceability needed for auditing and compliance with industry standards. Tools like Git, DVC, and MLflow can<a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="tools for version control" data-type="indexterm" id="id418"/> be used to version different ML assets.</p>&#13;
			</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>&#13;
			<p>Thorough <em>testing</em> is essential for building robust and reliable ML systems.</p>&#13;
			</td>&#13;
			<td>&#13;
			<p>Key types of tests include:</p>&#13;
&#13;
			<ul>&#13;
				<li>Unit tests to test individual components and functions</li>&#13;
				<li>Integration tests to test interactions between components</li>&#13;
				<li>Data tests to validate data quality and integrity</li>&#13;
				<li>Model tests to evaluate performance and behavior (fairness, non-discrimination, bias, robustness)</li>&#13;
			</ul>&#13;
			</td>&#13;
			<td>&#13;
			<p>Automating testing and integrating it into continuous integration/continuous delivery (CI/CD) pipelines<a contenteditable="false" data-primary="continuous integration/continuous delivery (CI/CD) pipeline" data-type="indexterm" id="id419"/> helps catch issues early in the development process. The benefits of comprehensive testing include improved model quality and reliability, faster debugging and issue resolution, and increased confidence in model behavior.</p>&#13;
&#13;
			<p>Tools like pytest, Great Expectations, and Deepchecks can be used for ML testing<a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="tools for testing" data-type="indexterm" id="id420"/>.</p>&#13;
			</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>&#13;
			<p><em>Reproducibility</em> ensures that ML experiments and results can be reliably re-created. This is crucial for debugging, auditing, and scientific rigor.</p>&#13;
			</td>&#13;
			<td>&#13;
			<p>Engineering processes that guarantee reproducibility include:</p>&#13;
&#13;
			<ul>&#13;
				<li>Versioning of code, data, and models</li>&#13;
				<li>Tracking of random seeds and configurations</li>&#13;
				<li>Documenting the model development, dependencies, and environments</li>&#13;
				<li>Containerization of ML workflows</li>&#13;
			</ul>&#13;
			</td>&#13;
			<td>&#13;
			<p>The benefits of reproducibility include simplifying debugging and troubleshooting, enabling validation of results by others, and supporting regulatory compliance. When it comes to improving reproducibility in ML experiments, containerization tools like Docker and workflow management<a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="tools for reproducibility" data-type="indexterm" id="id421"/> and metadata management tools play a significant role.</p>&#13;
			</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>&#13;
			<p>Automated <em>deployment</em> focuses on reliably and efficiently moving ML models from development to production.</p>&#13;
			</td>&#13;
			<td>&#13;
			<p>Considerations include:</p>&#13;
&#13;
			<ul>&#13;
				<li>Model packaging and containerization</li>&#13;
				<li>Deployment strategies<a contenteditable="false" data-primary="deployment strategies" data-type="indexterm" id="id422"/> (e.g., canary, blue–green)</li>&#13;
				<li>Scaling and load balancing</li>&#13;
				<li>Monitoring and logging</li>&#13;
				<li>Security and access control</li>&#13;
			</ul>&#13;
			</td>&#13;
			<td>&#13;
			<p>Automated deployment practices allow for ML models to be brought to production more quickly, without human intervention, and for model serving to be reliable and scalable. If issues arise, the deployment process should allow for easy rollback. Tools such as Kubernetes, TensorFlow Serving, and cloud ML platforms <a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="tools for deployment" data-type="indexterm" id="id423"/>can help streamline the model deployment process.</p>&#13;
			</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>&#13;
			<p><em>Monitoring</em> ensures that ML models perform as expected in production.</p>&#13;
			</td>&#13;
			<td>&#13;
			<p>Key elements to monitor include:</p>&#13;
&#13;
			<ul>&#13;
				<li>Model performance metrics</li>&#13;
				<li>Data drift and concept drift<a contenteditable="false" data-primary="concept drift" data-type="indexterm" id="id424"/></li>&#13;
				<li>System health and resource utilization</li>&#13;
				<li>Prediction latency and throughput</li>&#13;
				<li>Business validation metrics</li>&#13;
				<li>AI product health metrics</li>&#13;
			</ul>&#13;
			</td>&#13;
			<td>&#13;
			<p>Effective monitoring enables teams to quickly detect and respond to issues, gain an understanding of model behavior in production, and make data-driven decisions on when to retrain the model. Tools like Prometheus, Grafana, and specialized ML monitoring platforms can be used to implement comprehensive monitoring<a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="tools for monitoring" data-type="indexterm" id="id425"/>.</p>&#13;
			</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>In general, MLOps addresses the challenge of maintaining software products that incorporate ML/AI components in continuous, reliable operation.<em> </em>The specific activities and technical components of MLOps<a contenteditable="false" data-primary="machine learning operations (MLOps)" data-secondary="core principles of" data-startref="mlops-core-prin-1" data-type="indexterm" id="id426"/> depend on the expected operational lifespan and performance requirements of the system.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Defining Technical Components with the MLOps Stack Canvas" data-type="sect1"><div class="sect1" id="chapter_2_defining_technical_components_with_the_mlops_stack_1748539917638337">&#13;
<h1>Defining Technical Components with the <span class="keep-together">MLOps Stack Canvas</span></h1>&#13;
&#13;
<p>This section outlines a structured approach for defining the infrastructure for an AI project. It uses the<strong> </strong>MLOps Stack Canvas<a contenteditable="false" data-primary="MLOps Stack Canvas" data-type="indexterm" id="mlops-stack-1"/> framework to identify and organize the workflows, architecture, and infrastructure components needed for the MLOps stack.</p>&#13;
&#13;
<p>As you plan for implementation, it’s important to ensure that the ML models deliver measurable business value. This involves accounting for the costs associated with the infrastructure components within the MLOps stack and the costs of orchestrating and maintaining the ML system throughout its lifecycle. Key considerations include continuous integration, training, and delivery of ML assets; monitoring to ensure the system meets business objectives; and alerting mechanisms to detect and address model failures.</p>&#13;
&#13;
<p class="fix_tracking">The ML system should also be designed to ensure reproducibility (through versioning, feature stores, and pipelines), reliability (by minimizing outages and enabling safe failovers), and efficiency (by ensuring model predictions are fast and cost-effective).</p>&#13;
&#13;
<figure><div class="figure" id="chapter_2_figure_3_1748539917622790"><img src="assets/taie_0203.png"/>&#13;
<h6><span class="label">Figure 2-3. </span>The application- and industry-neutral MLOps Stack Canvas framework to specify an architecture and infrastructure stack for ML operations</h6>&#13;
</div></figure>&#13;
&#13;
<p>As shown in <a data-type="xref" href="#chapter_2_figure_3_1748539917622790">Figure 2-3</a>, the MLOps Stack Canvas is structured into three main areas: Data and Code Management, Model Management, and Metadata Management, each with its building blocks. In the following sections, we’ll go through the items in each area. Note that the general discussion about building the ML project’s infrastructure should include consideration of various organizational aspects of MLOps, such as tooling, platforms, and required skills. Issues that are identified should be noted in the MLOps Dilemmas section of the canvas.</p>&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Value Proposition" data-type="sect2"><div class="sect2" id="chapter_2_value_proposition_1748539917638411">&#13;
<h2>Value Proposition</h2>&#13;
&#13;
<p>We’ll start with<a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Value Proposition" data-type="indexterm" id="mlops-vp-1"/> the Value Proposition<a contenteditable="false" data-primary="Value Proposition (MLOps Stack Canvas)" data-type="indexterm" id="vp-1"/> (item 1 on the MLOps Stack Canvas). The framework emphasizes the value proposition as a critical element behind the motivation for building the MLOps architecture because it aims to create awareness about the pain points being addressed. Here’s an example value proposition for a hypothetical “MLOps Platform”:</p>&#13;
&#13;
<ul class="simplelist">&#13;
	<li>&#13;
	<p><strong>For</strong> data science and machine learning teams</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><strong>w</strong><strong>ho</strong> need to efficiently build, train, and deploy ML models at scale.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><strong>The </strong><strong>MLOps</strong><strong> Platform</strong> is an end-to-end machine learning platform</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><strong>t</strong><strong>hat</strong> enables teams to rapidly develop and operationalize ML solutions to drive business value.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><strong>Unlike</strong> other ML platforms that require complex setup and lack key features such as observability and monitoring</p>&#13;
	</li>&#13;
	<li>&#13;
	<p class="fix_tracking"><strong>o</strong><strong>ur </strong><strong>MLOps</strong><strong> platform</strong> provides a fully managed, intuitive interface with automated workflows, built-in experiment tracking, powerful AutoML capabilities, and seamless deployment to accelerate the ML lifecycle from prototype to production.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>This allows you to identify key elements such as:</p>&#13;
&#13;
<dl>&#13;
	<dt>Target customers</dt><dd><p>Data science and ML teams</p>&#13;
	</dd>&#13;
	<dt>Need</dt><dd><p>Efficiently building and deploying ML models at scale</p>&#13;
	</dd>&#13;
	<dt>Product name and category</dt><dd><p>MLOps Platform, an end-to-end machine learning platform</p>&#13;
	</dd>&#13;
	<dt>Key benefit</dt><dd><p>Rapidly develop and operationalize ML to drive business value</p>&#13;
	</dd>&#13;
	<dt>Competition</dt><dd><p>Other ML platforms that are complex and lack features such as observability and monitoring</p>&#13;
	</dd>&#13;
	<dt>Differentiation</dt><dd><p>Fully managed, intuitive, automated, with experiment tracking, AutoML, and seamless deployment to accelerate the full ML lifecycle</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>Identifying the right target customers and their needs is essential for building a right-sized MLOps platform without unnecessary technical complexity.</p>&#13;
&#13;
<p>Answering the following questions will help you confidently articulate your value proposition:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>What are we trying to do for the end user(s)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What is the problem?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Why is this an important problem?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Who is our persona? (ML engineer, data scientist, operation/business user)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Who owns the model(s) in production?</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>You can learn more about <a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Value Proposition" data-startref="mlops-vp-1" data-type="indexterm" id="id427"/>crafting value<a contenteditable="false" data-primary="Value Proposition (MLOps Stack Canvas)" data-startref="vp-1" data-type="indexterm" id="id428"/> propositions in <a data-type="xref" href="app01.html#appendix_a_designing_ai_powered_applications_1748539915159206">Appendix A</a>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Data and Code Management" data-type="sect2"><div class="sect2" id="chapter_2_data_and_code_management_1748539917638472">&#13;
<h2>Data and Code Management</h2>&#13;
&#13;
<p>The Data and Code Management<a contenteditable="false" data-primary="Data and Code Management (MLOps Stack Canvas)" data-type="indexterm" id="dcm-1"/> block of the MLOps Stack Canvas<a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Data and Code Management" data-type="indexterm" id="mlops-dcm-1"/> consists of technical components required for managing data and code—two essential elements of every ML system. Let’s examine the next building blocks of the canvas, namely, Data Sources and Data Versioning, Data Analysis and Experiment Management, Feature Store and Workflows, and Foundations.</p>&#13;
&#13;
<section data-pdf-bookmark="Data Sources and Data Versioning" data-type="sect3"><div class="sect3" id="chapter_2_data_sources_and_data_versioning_1748539917638530">&#13;
<h3>Data Sources and Data Versioning</h3>&#13;
&#13;
<p>Data is the backbone of any machine learning system. After articulating the business problem in the Value Proposition, the next step is<a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Data Sources and Data Versioning" data-type="indexterm" id="id429"/> Data Sources<a contenteditable="false" data-primary="Data Sources and Data Versioning (MLOps Stack Canvas)" data-type="indexterm" id="id430"/> and Data Versioning. The goal here is to estimate the cost of data acquisition, storage, and processing during the business and data understanding and data preparation phases of the CRISP-ML(Q) framework. Dataset development and further processing for ML algorithms may be costly. Data versioning is essential for analyzing model performance with new data and may be a regulatory requirement.</p>&#13;
&#13;
<p>Here are some considerations for the Data Sources and Data Versioning building block:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Is data versioning optional or mandatory? For example, is it a regulatory <span class="keep-together">requirement?</span></p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What data sources are available (e.g., owned, public, purchased)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Where is that data stored (e.g., in a data lake or data warehouse)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Is manual labeling required? Do we have human resources for it?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How do we version data for each trained model?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What tooling is available for data pipelines/workflows?</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>You might also use the <a href="https://oreil.ly/hsXsZ">Data Landscape Canvas</a> to<a contenteditable="false" data-primary="Data Landscape Canvas" data-type="indexterm" id="id431"/> inventory your organization’s available, accessible, and required data sources for the ML project.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Data Analysis and Experiment Management" data-type="sect3"><div class="sect3" id="chapter_2_data_analysis_and_experiment_management_1748539917638585">&#13;
<h3>Data Analysis and Experiment Management</h3>&#13;
&#13;
<p>According to the CRISP-ML(Q) framework<a contenteditable="false" data-primary="CRISP-ML(Q)" data-type="indexterm" id="id432"/>, the project’s initial phase includes running experiments and implementing a proof of concept. The<a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Data Analysis and Experiment Management" data-type="indexterm" id="id433"/> Data Analysis and Experiment Management block<a contenteditable="false" data-primary="Data Analysis and Experiment Management (MLOps Stack Canvas)" data-type="indexterm" id="id434"/> focuses on the applicability of ML technology for specific business goals and data preparation. Here, you need to answer the following questions regarding tooling:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>What programming language will you use for analysis (e.g., R, Python, Scala, Julia, SQL)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What ML-specific and business evaluation metrics need to be computed?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What metadata about ML experiments is collected, for reproducibility (datasets, hyperparameters, etc.)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What ML framework expertise is available on the team?</p>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Feature Store and Workflows" data-type="sect3"><div class="sect3" id="chapter_2_feature_store_and_workflows_1748539917638638">&#13;
<h3>Feature Store and Workflows</h3>&#13;
&#13;
<p><em>Feature engineering</em> is the process<a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Feature Store and Workflows" data-type="indexterm" id="id435"/> of transforming<a contenteditable="false" data-primary="feature engineering" data-type="indexterm" id="id436"/> raw<a contenteditable="false" data-primary="Feature Store and Workflows (MLOps Stack Canvas)" data-type="indexterm" id="id437"/> input data into features, a numerical representation suitable for machine learning algorithms. The feature store is a technical component in the MLOps stack used for managing, reproducing, discovering, and reusing features across ML projects and data science teams. It separates feature engineering from ML model development and speeds up the process. However, as this is an advanced component, using a feature store may add complexity, so you need to consider whether it’s appropriate for the project at hand. Here are some questions to ask:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Is a feature store optional or mandatory? Do we have a data governance process that requires feature engineering to be reproducible?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How are features computed (workflows) during the training and prediction <span class="keep-together">phases?</span></p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What are the infrastructure requirements for feature engineering?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Should we buy or make the feature store?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What databases are involved in feature storage?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Do we design APIs for feature engineering?</p>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Foundations (Reflecting DevOps)" data-type="sect3"><div class="sect3" id="chapter_2_foundations_reflecting_devops_1748539917638692">&#13;
<h3>Foundations (Reflecting DevOps)</h3>&#13;
&#13;
<p>DevOps is a set of practices and tools that integrates<a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Foundations (Reflecting DevOps)" data-type="indexterm" id="id438"/> software development and IT operations to automate, streamline, and accelerate the building, testing, and deployment of software applications with a focus on reliability and scalability.</p>&#13;
&#13;
<p>In the next step in the MLOps Stack Canvas, you’ll take stock of the available DevOps infrastructure and promote awareness of current DevOps principles within the ML project. This helps in extrapolating DevOps best practices to MLOps activities. If there are gaps in traditional DevOps practices, it’s essential to address them before moving on to more complex activities such as model and data versioning, continuous model training, or implementing a feature store.</p>&#13;
&#13;
<p>Follow the guidelines<a contenteditable="false" data-primary="DORA (DevOps Research and Assessment)" data-type="indexterm" id="id439"/> in the DORA <a href="https://oreil.ly/YzySM">“Accelerate State of DevOps Report”</a> and execute a self-assessment of your software delivery performance by answering the following questions:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>How do we maintain the code? What source version control system is used?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How do we monitor the system performance?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Do we need versioning for notebooks?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Is a trunk-based development approach used?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What is the CI/CD pipeline for the codebase? What tools are used for it?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Do we track metrics such as deployment frequency, lead time for changes, mean time to restore, and change failure rate?</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Adhering to DevOps principles directly influences the performance of software delivery. MLOps is built on top of DevOps, so establishing a stable DevOps culture for software projects is essential for the success of <a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Data and Code Management" data-startref="mlops-dcm-1" data-type="indexterm" id="id440"/>ML projects<a contenteditable="false" data-primary="Data and Code Management (MLOps Stack Canvas)" data-startref="dcm-1" data-type="indexterm" id="id441"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Model Management" data-type="sect2"><div class="sect2" id="chapter_2_model_management_1748539917638751">&#13;
<h2>Model Management</h2>&#13;
&#13;
<p>Model management <a contenteditable="false" data-primary="Model Management (MLOps Stack Canvas)" data-type="indexterm" id="mom-mlops-1"/>encompasses<a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Model Management" data-type="indexterm" id="mlops-mom-1"/> a set of practices, processes, and tools used to efficiently track, version, deploy, and monitor machine learning models throughout their lifecycle, guaranteeing consistency, reproducibility, and scalability while meeting business requirements. The building blocks in this area of the MLOps Stack Canvas include CI/CD/CT: ML Pipeline Orchestration, Model Registry and Model Versioning, Model Deployment, Prediction Serving, and Model, Data, and Application <span class="keep-together">Monitoring</span>.</p>&#13;
&#13;
<section data-pdf-bookmark="CI/CT/CD: ML Pipeline Orchestration" data-type="sect3"><div class="sect3" id="chapter_2_ci_ct_cd_ml_pipeline_orchestration_1748539917638820">&#13;
<h3>CI/CT/CD: ML Pipeline Orchestration</h3>&#13;
&#13;
<p>You reviewed the existing CI/CD pipelines<a class="fse fs" contenteditable="false" data-primary="Model Management (MLOps Stack Canvas)" data-secondary="CI/CT/CD: ML Pipeline Orchestration" data-type="indexterm" id="id442"/> for software delivery in the previous block. In this block, you’ll examine the CI/CD routine for ML model release. You’ll also introduce continuous training (CT)<a contenteditable="false" data-primary="continuous training (CT)" data-type="indexterm" id="id443"/>. While continuous integration involves the building, testing, and packaging of data and model pipelines, continuous training focuses on automatically retraining ML models. You’ll use the pipeline pattern, which involves steps such as data verification, feature and data selection, data cleaning, feature engineering, and model training.</p>&#13;
&#13;
<p>A pipeline is structured as a directed acyclic graph (DAG) representing the overall workflow. This is a common way to represent dependencies and execution order. Depending on the maturity level, you can automate the data and model training pipeline workflows to operationalize the model. You should trigger data preparation and model training pipelines whenever new data is available or when the source code for a pipeline has changed.</p>&#13;
&#13;
<p>In this block of the MLOps Stack Canvas, you should clarify the processes and the toolchain for CI/CT in the CRISP-ML(Q) model engineering phase by answering the following questions:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>How often are models expected to be retrained? What is the trigger for this (scheduled, event-based, or ad hoc)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Where does this happen (locally or in the cloud)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What is the formalized workflow for an ML pipeline? What tech stack is used?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Is distributed model training required? Do we have the necessary infrastructure for distributed training?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What is the workflow for the CI pipeline? What tools are used?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What are the nonfunctional requirements for the ML model (efficiency, fairness, robustness, interpretability, etc.)? How are they tested? Are these tests integrated into the CI/CT workflow?</p>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Model Registry and Model Versioning" data-type="sect3"><div class="sect3" id="chapter_2_model_registry_and_model_versioning_1748539917638879">&#13;
<h3>Model Registry and Model Versioning</h3>&#13;
&#13;
<p>The next section in the MLOps Stack Canvas<a contenteditable="false" data-primary="Model Management (MLOps Stack Canvas)" data-secondary="Model Registry and Model Versioning" data-type="indexterm" id="id444"/> is concerned with the ML model registry and versioning component, which is a crucial element of the model evaluation phase in the CRISP-ML(Q) process. The machine learning model, along with the data and the software code, is an essential asset. As with code versioning in traditional software engineering, establishing a model and data versioning practice is the foundation for reproducibility in machine learning.</p>&#13;
&#13;
<p>Depending on your use case, a change in code or data might require retraining the model. Models may also need to be updated due to “model decay,”<a contenteditable="false" data-primary="model decay" data-type="indexterm" id="id445"/> where the model’s performance declines over time with new data. In regulated industries like healthcare, finance, and the military, all ML models should be versioned and thoroughly documented. It’s also important to ensure backward compatibility by being able to roll back to previously built models. By tracking multiple versions of the ML model, it is possible to implement different deployment strategies, such as “canary” or “shadow” deployment (methods used to gradually roll out changes in a controlled manner to mitigate risk while evaluating the performance of the newly trained model).</p>&#13;
&#13;
<p>In this section of the MLOps Stack Canvas, you should answer the following <span class="keep-together">questions</span>:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Is using a model registry and model versioning optional or mandatory? A model registry might be required if you have multiple models in production and need to track them all. Model versioning might be necessary if you have a requirement for reproducibility.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Where should new ML models be stored and tracked?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What versioning standards are used (e.g., semantic versioning)?</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Note that implementing versioning and an ML model registry may be more appropriate in the later stages of ML projects.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Model Deployment" data-type="sect3"><div class="sect3" id="chapter_2_model_deployment_1748539917638933">&#13;
<h3>Model Deployment</h3>&#13;
&#13;
<p>After training and evaluating the model<a contenteditable="false" data-primary="Model Management (MLOps Stack Canvas)" data-secondary="Model Deployment" data-type="indexterm" id="id446"/>, you’ll move on to the next phase of the CRISP-ML(Q) process and deploy the machine learning model. Deploying an ML model means making it available in the target environment to receive prediction requests. In this section of the MLOps Stack Canvas, you’ll define your model <span class="keep-together">exposure</span> strategies and the infrastructure aspects of continuous deployment (the automatic deployment of ML models into the target environment based on predetermined evaluation metrics) by addressing the following questions:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>What is the delivery format for the model?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What is the expected time for changes (time from commit to production)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What is the target environment to serve predictions?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What is our model release policy? Is A/B testing or multi-armed bandits testing required (e.g., for measuring the effectiveness of the new model on business <span class="keep-together">metrics</span> and deciding what model should be promoted in the production <span class="keep-together">environment</span>)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What is our deployment strategy? For example, is shadow or canary deployment required?</p>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Prediction Serving" data-type="sect3"><div class="sect3" id="chapter_2_prediction_serving_1748539917638985">&#13;
<h3>Prediction Serving</h3>&#13;
&#13;
<p>This section of the MLOps Stack Canvas<a contenteditable="false" data-primary="Model Management (MLOps Stack Canvas)" data-secondary="Prediction Serving" data-type="indexterm" id="id447"/> covers the process of applying a machine learning model to new input data, commonly referred to as model serving. There are two primary serving modes<a contenteditable="false" data-primary="serving modes" data-type="indexterm" id="id448"/>: online and batch. ML models can be deployed using five different patterns: Model-as-a-Service, Model-as-a-Dependency, Precompute, Model-on-Demand, and Hybrid Serving. Each pattern requires different infrastructure <span class="keep-together">settings</span>. For example, Model-as-a-Service involves distributing the model as a service for input requests via a REST API and uses an on-demand mode for prediction responses. On the other hand, the Precompute pattern uses the batch prediction mode, with model predictions precomputed and stored in a relational database.</p>&#13;
&#13;
<p>To identify the environment for model serving, you should answer the following questions:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>What is the serving mode (batch or online)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Is distributed model serving required?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Is multi-model prediction serving required?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Is pre-assertion for input data implemented?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What fallback method for an inadequate model output (post-assertion) is implemented? (Do we have a heuristic benchmark?)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Do we need ML inference accelerators (like special hardware such as Tensor Processing Units)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What is the expected target volume of predictions per month (or hour/day)?</p>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Model, Data, and Application Monitoring" data-type="sect3"><div class="sect3" id="chapter_2_model_data_and_application_monitoring_1748539917639041">&#13;
<h3>Model, Data, and Application Monitoring</h3>&#13;
&#13;
<p>Once the ML model<a contenteditable="false" data-primary="Model Management (MLOps Stack Canvas)" data-secondary="Monitoring" data-type="indexterm" id="id449"/> is put into operation, it must be monitored continuously to ensure that its quality is maintained and it continues to deliver accurate results. This section of the MLOps Stack Canvas addresses the monitoring aspect of operating the ML system in production. It corresponds to the sixth stage of the CRISP-ML(Q) process, model deployment and monitoring. Here are some questions to answer:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Is continuous monitoring optional or mandatory? For instance, do you need to assess the effectiveness of your model during prediction serving? Do you need to monitor your model for performance degradation and trigger an alert if it starts performing badly? Is model retraining automatically triggered by specific events, such as data drift or concept drift?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What ML metrics are collected?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What domain-specific metrics are collected?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How is the model performance decay detected?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How is the data skew detected?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What operational aspects need to be monitored (e.g., model prediction latency, CPU/RAM usage)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What is the alerting strategy (e.g., thresholds)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What triggers model retraining? Is it ad hoc, event-based<a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Model Management" data-startref="mlops-mom-1" data-type="indexterm" id="id450"/>, or scheduled<a contenteditable="false" data-primary="Model Management (MLOps Stack Canvas)" data-startref="mom-mlops-1" data-type="indexterm" id="id451"/>?</p>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Metadata Management" data-type="sect2"><div class="sect2" id="chapter_2_metadata_management_1748539917639095">&#13;
<h2>Metadata Management</h2>&#13;
&#13;
<p>Metadata management<a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Metadata Management" data-type="indexterm" id="mlops-mm-1"/> for machine learning systems<a contenteditable="false" data-primary="Metadata Management (MLOps Stack Canvas)" data-type="indexterm" id="mm-mlops-1"/> is a holistic process of collecting, organizing, storing, and utilizing data about the various components and stages of the ML lifecycle. This includes tracking information related to datasets, models, experiments, deployments, and performance metrics to ensure reproducibility, comparability, and traceability throughout the ML pipeline (see <a data-type="xref" href="#chapter_2_figure_4_1748539917622886">Figure 2-4</a>).</p>&#13;
&#13;
<figure><div class="figure" id="chapter_2_figure_4_1748539917622886"><img src="assets/taie_0204.png"/>&#13;
<h6><span class="label">Figure 2-4. </span>Metadata management</h6>&#13;
</div></figure>&#13;
&#13;
<p class="fix_tracking">The <em>m</em><em>etadata </em><em>s</em><em>tore</em> is a <a contenteditable="false" data-primary="metadata store" data-type="indexterm" id="id452"/>cross-cutting component that supports all elements of the MLOps infrastructure stack. Implementing an ML model governance process—which may be necessary, depending on your organization and regulatory <span class="keep-together">requirements—relies</span> heavily on metadata, making a metadata store an essential <span class="keep-together">component</span>.</p>&#13;
&#13;
<p>In the last block of the MLOps Stack Canvas, you’ll answer the following questions:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>What kind of metadata related to the code, data, and model management needs to be collected (e.g., the pipeline run ID, trigger, performed steps, start/end timestamps, train/test dataset split, hyperparameters, model object, various statistics, etc.)?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Are any ML governance processes included in the MLOps lifecycle? What metadata will be required?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What is the documentation strategy? Do we treat documentation as a code?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What operational metrics<a contenteditable="false" data-primary="MLOps Stack Canvas" data-secondary="Metadata Management" data-startref="mlops-mm-1" data-type="indexterm" id="id453"/> need to be<a contenteditable="false" data-primary="Metadata Management (MLOps)" data-startref="mm-mlops-1" data-type="indexterm" id="id454"/> collected (e.g., time to restore, change fail percentage)?</p>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Additional Considerations of MLOps" data-type="sect2"><div class="sect2" id="chapter_2_additional_considerations_of_mlops_1748539917639150">&#13;
<h2>Additional Considerations of MLOps</h2>&#13;
&#13;
<p>An MLOps platform comprises self-service APIs, tools, services, knowledge, and support organized as a product (for more on this concept, see the Team Topologies video <a href="https://oreil.ly/2IZ7x">“WTF Is Platform as Product?”</a>). Platforms are designed to enable multiple autonomous delivery teams to release product features more quickly, with decreased dependencies and coordination between teams. There are also organizational aspects of AI engineering<a contenteditable="false" data-primary="AI engineering" data-seealso="machine learning operations (MLOps)" data-type="indexterm" id="id455"/> to consider as part of the broader discussion about constructing the infrastructure for AI projects. For example:</p>&#13;
&#13;
<dl>&#13;
	<dt>Tooling</dt>&#13;
	<dd>&#13;
	<p>Should we purchase, use existing open source, or develop in-house tools for each of the MLOps components? What are the risks, trade-offs, and impacts of each decision?</p>&#13;
	</dd>&#13;
	<dt>Platforms</dt>&#13;
	<dd>&#13;
	<p>Should we standardize on a single MLOps platform or create a hybrid solution? What are the risks, trade-offs, and impacts of this decision?<sup><a data-type="noteref" href="ch02.html#id456" id="id456-marker">1</a></sup></p>&#13;
	</dd>&#13;
	<dt>Skills</dt>&#13;
	<dd>&#13;
	<p>What is the cost associated with either hiring or training our own machine learning engineering talent?</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>Using the MLOps Stack Canvas<a contenteditable="false" data-primary="MLOps Stack Canvas" data-startref="mlops-stack-1" data-type="indexterm" id="id457"/> helps you identify the workflows, architecture, and infrastructure components for your AI project and develop a good cost estimation for your AI project in every phase.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="CRISP-ML(Q) and MLOps" data-type="sect1"><div class="sect1" id="chapter_2_crisp_ml_q_and_mlops_1748539917639205">&#13;
<h1>CRISP-ML(Q) and MLOps</h1>&#13;
&#13;
<p>MLOps<a contenteditable="false" data-primary="CRISP-ML(Q) and MLOps" data-type="indexterm" id="cr-mlo-1"/> and CRISP-ML(Q)<a contenteditable="false" data-primary="CRISP-ML(Q)" data-type="indexterm" id="crisp-3"/> are complementary approaches that can work together synergistically to support and improve the development and deployment of machine learning models. While CRISP-ML(Q) provides a structured process framework, MLOps offers technical and organizational best practices for implementing and operationalizing ML projects. <a data-type="xref" href="#chapter_2_figure_5_1748539917622906">Figure 2-5</a> illustrates the synergy between the two.</p>&#13;
&#13;
<figure><div class="figure" id="chapter_2_figure_5_1748539917622906"><img src="assets/taie_0205.png"/>&#13;
<h6><span class="label">Figure 2-5. </span>MLOps and CRISP-ML(Q) synergy</h6>&#13;
</div></figure>&#13;
&#13;
<p>These two approaches can be integrated to bind the ML development process with technical implementation. In particular, CRISP-ML(Q) suggests a high-level, structured process for machine learning projects, dividing the development workflow into six main phases: business and data understanding, data preparation, modeling, evaluation, deployment, and monitoring and maintenance. MLOps provides the technical infrastructure and best practices to implement these phases effectively. For example, during the data engineering phase of CRISP-ML(Q), MLOps practices can be applied to automate data pipelines, ensure data versioning, and maintain data quality.</p>&#13;
&#13;
<p class="fix_tracking">The main strength of CRISP-ML(Q), and what makes it a suitable ML process model for EU AI Act implementation, is its integration of quality assurance and risk awareness throughout the machine learning lifecycle. MLOps complements this by providing tools and practices for continuous integration, continuous delivery, and automated testing, which are essential for maintaining quality in production environments.</p>&#13;
&#13;
<p>ML development is, by nature, iterative, and both CRISP-ML(Q) and MLOps support this. CRISP-ML(Q) provides a framework for iterating through the phases of ML development, while MLOps offers technical solutions for rapid experimentation, version control, and automated retraining of ML models. Furthermore, MLOps facilitates model retraining by incorporating practices such as continuous monitoring, logging, and automated model<a contenteditable="false" data-primary="DORA (DevOps Research and Assessment)" data-type="indexterm" id="id458"/> updates. <a href="https://oreil.ly/YzySM">Research by DORA</a> shows that employing continuous delivery (and hence MLOps) practices can significantly improve organizations’ technical and business performance. By enabling teams to consistently deliver value and adapt to changes in market demands more effectively, MLOps practices not only lower the risks of deployment and production failures but also reduce time to feedback, decrease cognitive load, and lessen the general stress associated with operating models in production.</p>&#13;
&#13;
<p>Being quality-driven, CRISP-ML(Q) emphasizes the importance of documenting business decisions, data, the ML model development process, and the models and experiments themselves. MLOps complements this approach by providing version control for code, data, and models, thereby ensuring the reproducibility of results. It also introduces technical components, such as metadata, model, and feature stores, to support these functions.</p>&#13;
&#13;
<p>CRISP-ML(Q)’s iterative nature aligns well with the emphasis of MLOps on continuous improvement. MLOps practices can be used to implement the feedback loops suggested by CRISP-ML(Q), allowing for ongoing refinement of models and processes. MLOps also offers technical solutions for scaling ML operations, including automated model training, evaluation, and deployment pipelines.</p>&#13;
&#13;
<p class="fix_tracking">In practice, organizations can use CRISP-ML(Q) as a high-level framework for structuring their machine learning projects, while applying MLOps best practices and tools within each phase. This combination ensures that projects follow a well-defined process while benefiting from modern, efficient, and scalable technical<a contenteditable="false" data-primary="CRISP-ML(Q) and MLOps" data-startref="cr-mlo-1" data-type="indexterm" id="id459"/> implementations<a contenteditable="false" data-primary="machine learning operations (MLOps)" data-startref="mlops-1" data-type="indexterm" id="id460"/><a contenteditable="false" data-primary="CRISP-ML(Q)" data-startref="crisp-3" data-type="indexterm" id="id461"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="chapter_2_conclusion_1748539917639254">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>This chapter introduced CRISP-ML(Q) and MLOps and showed how this pairing provides a robust framework for addressing EU AI Act compliance requirements: CRISP-ML(Q) provides the process blueprint, while MLOps supplies the tools and practices needed to operationalize that process in a compliant manner. Together, they enable organizations to proactively embed EU AI Act compliance into the ML lifecycle, from data collection to monitoring, rather than treating it as an afterthought.</p>&#13;
&#13;
<p class="pagebreak-before">In the following chapter, to complete the compliance engineering picture, you will learn about data governance, a framework of policies, processes, and responsibilities that ensures the quality, security, compliance, and ethical use of data throughout the AI development lifecycle. You will also see how AI governance ensures that AI systems are developed, deployed, and monitored in a responsible, ethical, secure, and compliant manner.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="id456"><sup><a href="ch02.html#id456-marker">1</a></sup> Thoughtworks provides an <a href="https://oreil.ly/nE93Z">excellent guide</a> for understanding the MLOps platform landscape.</p></div></div></section></body></html>