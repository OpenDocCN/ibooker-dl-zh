- en: 8 RAG application evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Benchmarking RAG applications and agent capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing evaluation datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Applying RAGAS metrics: recall, faithfulness, correctness'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will explore the importance of evaluating your RAG application
    performance using carefully constructed benchmark questions. As your RAG pipeline
    grows more sophisticated and complex, it becomes essential to ensure that your
    agent’s answers remain both accurate and coherent across a wide range of queries.
    A benchmark evaluation provides the system needed to measure the agent’s capabilities
    while also helping to clearly define and scope the agent.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating RAG applications involves multiple approaches, each addressing different
    steps of the application, as shown in figure 8.1, which illustrates a high-level
    overview of a pipeline for a question-answering system powered by an LLM with
    retrieval capabilities. It begins with the user posing a question to the system.
    The LLM then identifies the most suitable retrieval tool to fetch the necessary
    information. This step is critical and can be evaluated for the accuracy of the
    tool selection process.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 Evaluating different steps of a RAG pipeline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Throughout this book, you have implemented various retrieval tool designs,
    starting with vector search and progressing to more structured approaches like
    text2cypher and Cypher templates. Each retrieval method serves different needs:'
  prefs: []
  type: TYPE_NORMAL
- en: Vector search efficiently retrieves semantically relevant documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cypher templates allow precise, structured queries to databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text2cypher allows dynamic and flexible querying, benefiting from the expressive
    power of graph-based retrieval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating which tool the LLM selects and how well it matches the query’s needs
    is crucial for optimizing retrieval performance.
  prefs: []
  type: TYPE_NORMAL
- en: Once the appropriate tool is chosen, it retrieves relevant context or data from
    a knowledge base. The relevance of this retrieved context to the user’s question
    is another key evaluation point. A well-chosen retrieval method should ensure
    that the fetched context is both accurate and sufficient for answering the query.
  prefs: []
  type: TYPE_NORMAL
- en: Using the retrieved context, the LLM generates an answer, which is then presented
    to the user. At this stage, we can assess not only the coherence and accuracy
    of the generated response but also the model’s ability to understand and integrate
    the provided context effectively. A particularly important evaluation criterion
    is whether the LLM produces the correct answer when given the correct context.
    This allows us to measure the model’s reasoning and synthesis capabilities separately
    from retrieval performance.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the entire pipeline can be evaluated holistically to measure its
    effectiveness in providing accurate and contextually relevant answers to user
    queries. By analyzing failures at different stages—tool selection, retrieval relevance,
    and final response generation—we can iteratively improve both the retrieval mechanisms
    and the LLM’s ability to utilize retrieved information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say you are responsible for evaluating the performance of the LLM agent implemented
    in chapter 5\. To gain deeper insight into its effectiveness, you will use the
    RAGAS Python library to design and conduct a benchmark analysis. But first, you
    need to design the benchmark dataset. In the remainder of this chapter, we’ll
    move from concepts to code and walk through the implementation step by step. To
    follow along, you’ll need access to a running Neo4j instance. This can be a local
    installation or a cloud-hosted instance. In the implementation of this chapter,
    we use what we call the “Movies dataset.” See the appendix for more information
    on the dataset and various ways to load it. You can follow the implementation
    directly in the accompanying Jupyter notebook available here: [https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch08.ipynb](https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch08.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive in.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Designing the benchmark dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating a benchmark dataset requires designing input queries that test various
    aspects of the system’s decision making and response generation. Since each step
    in the RAG pipeline plays a vital role, the dataset should include diverse questions
    that challenge different components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tool selection evaluation* —Ssome queries should evaluate whether the system
    selects the correct retrieval method, ensuring it identifies the most relevant
    source of information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Entity and value mapping*—Other queries might focus on testing specific tasks,
    such as mapping entities or values from user input to the corresponding entries
    in a database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multistep retrieval scenarios* —Some agents have the ability to execute multiple
    retrieval steps, where the initially retrieved data serves as input for a second
    retrieval step. The benchmark should include cases where the system needs to refine
    or expand upon the first retrieval to fully answer the query. These cases are
    particularly important for answering complex questions that depend on dynamically
    chaining multiple queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Edge cases and functional coverage* —To fully understand system performance,
    the benchmark must cover all functionalities and known edge cases. This includes
    handling ambiguous queries, long-tail concepts, and scenarios where multiple retrieval
    methods might be applicable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Conversational usability* —Additionally, it may be useful to evaluate the
    agent’s ability to handle greetings, clarify ambiguous queries, and effectively
    communicate its capabilities to ensure a smooth and user-friendly experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By systematically benchmarking these aspects, we gain a clearer understanding
    of how well the agent performs under different conditions. This allows for targeted
    improvements, ensuring robustness and reliability in real-world deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Coming up with test examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To evaluate the system comprehensively, you need well-defined end-to-end test
    examples. Each example consists of a question and its corresponding ground truth
    response, as shown in figure 8.2, ensuring that the system’s output can be reliably
    assessed.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Benchmark test example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Instead of providing a static string as the expected answer, we can use Cypher
    queries to define the ground truth dynamically. Since we are dealing with a graph
    database, this approach offers a significant advantage: even if the underlying
    data changes, the benchmark remains valid. This ensures that test cases, as shown
    in figure 8.3, remain accurate over time without requiring constant updates.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Benchmark test example with a Cypher statement as ground truth
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When designing a benchmark dataset, you should include diverse examples to evaluate
    different aspects of the agent’s performance. For instance, you can evaluate how
    the agent responds to greetings like “Hello,” provides guidance to the user, or
    handles irrelevant queries, as demonstrated in table 8.1.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 Benchmark examples that test simple greetings and irrelevant questions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Question | Cypher |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Hello  | `RETURN “greeting and reminder it can only answer questions related
    to movies.”`  |'
  prefs: []
  type: TYPE_TB
- en: '| What can you do?  | `RETURN “answer questions related to movies and their
    cast.”`  |'
  prefs: []
  type: TYPE_TB
- en: '| What is the weather like in Spain?  | `RETURN “irrelevant question as we
    can answer questions related to movies and their cast only.”`  |'
  prefs: []
  type: TYPE_TB
- en: This table provides examples of how the agent responds to simple greetings,
    user guidance requests, and irrelevant queries. It shows how you can use a simple
    `RETURN` Cypher statement to define static answers that don’t need to look for
    information in the database. For example, when greeted with “Hello,” the agent
    replies with a greeting and a reminder of its scope. If asked what it can do,
    it clarifies that it answers questions about movies and their casts. For unrelated
    queries, like about the weather, the agent simply states that it only handles
    movie-related questions.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can define a set of questions to evaluate both tool usage and the LLM’s
    ability to generate accurate answers using those tools. The examples are shown
    in table 8.2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.2 Benchmark examples that test tools usage and value mapping
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Question | Cypher |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Who acted in Top Gun?  | `RETURN "MATCH (p:Person)-[:ACTED_IN]→(m:Movie {title:
    "Top Gun"}) RETURN p.name"`  |'
  prefs: []
  type: TYPE_TB
- en: '| Who acted in top gun?  | `RETURN "MATCH (p:Person)-[:ACTED_IN]→(m:Movie {title:
    "Top Gun"}) RETURN p.name"`  |'
  prefs: []
  type: TYPE_TB
- en: '| In which movies did Tom Hanks act in?  | `MATCH (p:Person {name: "Tom Hanks"})-[:ACTED_IN]→(m:Movie)
    RETURN m.title`  |'
  prefs: []
  type: TYPE_TB
- en: '| In which movies did tom Hanks act in?  | `MATCH (p:Person {name: "Tom Hanks"})-[:ACTED_IN]→(m:Movie)
    RETURN m.title`  |'
  prefs: []
  type: TYPE_TB
- en: 'The examples in table 8.2 demonstrate cases where the LLM needs to retrieve
    relevant data from the database using available tools. Here, the LLM should utilize
    two key tools: one for finding movies by actor and another for finding actors
    by movie, ensuring fast and reliable responses.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, these examples allow us to evaluate how well the agent maps user
    input to database values. For well-known movies and actors, the LLM often generates
    correct queries out of the box based on its pretraining. However, for lesser-known
    or private datasets, a dedicated mapping system is essential for accurate entity
    resolution. Implementing such a system ensures that user inputs are correctly
    linked to database entries, improving both accuracy and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: You should also include some examples where the LLM will need to use the text2cypher
    tool, as shown in table 8.3.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.3 Benchmark examples that test queries involving aggregations and filtering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Question | Cypher |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Who acted in the most movies?  | `MATCH (p:Person)-[:ACTED_IN]→(m:Movie)
    RETURN p.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1`  |'
  prefs: []
  type: TYPE_TB
- en: '| List people born before 1940\.  | `MATCH (p:Person) WHERE p.born < 1940 RETURN
    p.name`  |'
  prefs: []
  type: TYPE_TB
- en: '| Who was born in 1965 and has directed a movie?  | `MATCH (p:Person)-[:DIRECTED]→(m:Movie)
    WHERE p.born = 1965 RETURN p.name`  |'
  prefs: []
  type: TYPE_TB
- en: Table 8.3 includes queries that involve aggregations, filtering, and relationships,
    such as finding the actor with the most movie roles, listing people born before
    a certain year, and identifying directors born in a specific year. Since no dedicated
    tool is implemented to handle these queries, the LLM must rely on text2cypher
    to construct the appropriate Cypher statements based on the provided graph schema.
  prefs: []
  type: TYPE_NORMAL
- en: You should also test edge cases, such as queries where relevant data is missing
    but still within the domain, as demonstrated in table 8.4.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.4 Benchmark examples that test questions where data is missing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Question | Cypher |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Which movie has the most Oscars?  | `RETURN “This information is missing”`  |'
  prefs: []
  type: TYPE_TB
- en: The benchmark will be very dependent on the functionalities of your agent. The
    specific capabilities, such as retrieval strategies, reasoning methods, and structured
    output handling, will influence the benchmark’s effectiveness in assessing performance.
    When designing a benchmark, it is crucial to ensure comprehensive coverage of
    your agent’s functionalities. By incorporating a variety of examples, you can
    effectively test how well your agent handles different challenges.
  prefs: []
  type: TYPE_NORMAL
- en: The benchmark has 17 examples in total, with some not shown here. You can now
    evaluate them.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To assess the performance of your benchmark, you will use RAGAS, a framework
    designed for evaluating RAG systems. As mentioned, the evaluation focuses on three
    key metrics, discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Context recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Context recall measures how many relevant pieces of information were successfully
    retrieved using the prompt in “Context recall evaluation.” A high score indicates
    that the retrieval system effectively captures all necessary context needed to
    answer the query.
  prefs: []
  type: TYPE_NORMAL
- en: '**Context recall evaluation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Goal: Given a context and an answer, analyze each sentence in the answer and
    classify whether the sentence can be attributed to the given context or not. Use
    only ''Yes'' (1) or ''No'' (0) as a binary classification. Output JSON with reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: The prompt in “Context recall evaluation” ensures that every sentence in the
    generated answer is explicitly supported by the retrieved context. By doing so,
    it helps evaluate how effectively the retrieval system captures relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the faithfulness assessment ensures that the generated response remains
    factually aligned with the retrieved content.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Faithfulness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Faithfulness evaluates whether the generated response remains factually consistent
    with the retrieved context. A response is considered faithful if all its claims
    can be directly supported by the provided documents, minimizing the risk of hallucination.
    Faithfulness is assessed using a two-step process. In the first step, it decomposes
    the answer into atomic statements using the prompt in “Faithfulness statement
    breakdown,” ensuring that each unit of information is clear and self-contained,
    making verification easier.
  prefs: []
  type: TYPE_NORMAL
- en: '**Faithfulness statement breakdown**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Goal: Given a question and an answer, analyze the complexity of each sentence
    in the answer. Break down each sentence into one or more fully understandable
    statements. Ensure that no pronouns are used in any statement. Format the outputs
    in JSON.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the statements are generated, it evaluates their faithfulness using the
    prompt in “Faithfulness evaluation.”
  prefs: []
  type: TYPE_NORMAL
- en: '**Faithfulness evaluation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Goal: Your task is to judge the faithfulness of a series of statements based
    on a given context. For each statement, return a verdict as 1 if the statement
    can be directly inferred from the context or 0 if the statement cannot be directly
    inferred from the context.'
  prefs: []
  type: TYPE_NORMAL
- en: The prompt in “Faithfulness evaluation” checks whether the statements in the
    generated response are factually grounded in the retrieved context. It ensures
    that the model does not introduce unsupported claims.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we evaluate answer correctness by comparing the generated response
    with the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3 Answer correctness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Answer correctness assesses how accurately and completely the response addresses
    the user’s query. It considers both factual accuracy and relevance to ensure the
    response aligns with the intent of the question. Answer correctness uses the same
    process as faithfulness to generate statements and then evaluates them using the
    prompt in “Answer correctness evaluation.”
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer correctness evaluation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Goal: Given a ground truth and an answer statement, analyze each statement
    and classify it into one of the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TP (true positive): Statements present in the answer that are also directly
    supported by one or more statements in the ground truth. FP (false positive):
    Statements present in the answer but not directly supported by any statement in
    the ground truth. FN (false negative): Statements found in the ground truth but
    not present in the answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Each statement can only belong to one of these categories. Provide a reason
    for each classification.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt in “Answer correctness evaluation” ensures that the response is both
    factually correct and aligned with the expected answer by systematically comparing
    the generated statements with the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing these metrics, you can determine how well the system retrieves
    relevant data, maintains factual consistency, and generates correct responses.
    This evaluation will help identify potential weaknesses, such as missing context,
    inconsistencies, or inaccurate answers, allowing for iterative refinement and
    improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.4 Loading the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The benchmark dataset is provided as a CSV file in the accompanying repository,
    making it easy to load and use, as demonstrated in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1 Loading benchmark dataset from CSV
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 8.2.5 Running evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To evaluate the system’s performance, you will generate answers for the benchmark
    dataset and compare them against the expected ground truth responses. First, you
    need to obtain the ground truth by executing the corresponding Cypher statements
    and generating answers using the agent, as shown in listing 8.2\. Additionally,
    you must record latency and retrieved contexts to analyze the system’s efficiency
    and relevance.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.2 Generating answers and ground truth responses
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The provided Cypher statement returns the ground truth.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Executes the agent to generate a response to the question'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Calculates the latency'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Stores the results back to the dataframe'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have collected all the necessary input data, including generated
    answers and ground truth responses, we can proceed with the evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.3 Evaluating the generated answer and retrieved context
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Changes missing response answers to “I don’t know”'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Runs the evaluation using RAGAS framework'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Relevant metrics'
  prefs: []
  type: TYPE_NORMAL
- en: This code in listing 8.3 runs the evaluation using the RAGAS framework, which
    requires non-null values, so you fill in missing responses with “I don’t know.”
    It then evaluates the generated answers based on answer correctness, context recall,
    and faithfulness.
  prefs: []
  type: TYPE_NORMAL
- en: The final step is to analyze the results to understand the system’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.6 Observations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can review the overall summary in 8.5 to get an overview of the agent’s
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.5 Benchmark summary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| **`answer_correctness`** | **`context_recall`** | **`faithfulness`** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.7774  | 0.7941  | 0.9657  |'
  prefs: []
  type: TYPE_TB
- en: The results in table 8.5 provide an overall assessment of the system’s performance
    based on three key metrics. With an answer correctness score of 0.7774, the model
    gets things right most of the time but still misses the mark in about a quarter
    of cases. The context recall score of 0.7941 shows that while the retrieval system
    is doing a decent job, it occasionally fails to pull in all the necessary information,
    which could be holding back the overall accuracy. On the bright side, the faithfulness
    score of 0.9657 is excellent, meaning the model rarely makes things up and stays
    true to the retrieved context.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the high faithfulness score shows that the model does not introduce
    incorrect information, but the answer correctness and context recall lower scores
    suggest that improving retrieval mechanisms could lead to better response accuracy.
    Enhancing retrieval coverage and refining how the LLM formulates answers could
    improve overall performance. These insights can guide further optimizations, such
    as refining the retrieval system, improving query reformulation, or implementing
    better entity mapping for ambiguous queries.
  prefs: []
  type: TYPE_NORMAL
- en: You can further analyze each response to identify areas for improvement by using
    the code in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.4 Extracting metrics and adding them to the dataframe
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The full response is too large to include in the book, but there are several
    key takeaways from analyzing individual examples. One noticeable pattern is that
    latency is significantly lower for queries that don’t require text2cypher, as
    avoiding an additional LLM call speeds up the response. Another observation is
    that since we rely on an LLM as a judge, some scores may seem inconsistent, such
    as in the Hello example.
  prefs: []
  type: TYPE_NORMAL
- en: One clear limitation is that the system fails to answer the question “Who has
    the longest name among all actors?” This happens because the model isn’t equipped
    to generate the appropriate Cypher query. To address this, you could add a few-shot
    example to guide text2cypher or implement a dedicated tool specifically for handling
    such queries.
  prefs: []
  type: TYPE_NORMAL
- en: This analysis demonstrates how a benchmark helps us evaluate results and make
    informed decisions about future improvements. As the system evolves, the benchmark
    dataset should continue to grow, ensuring ongoing refinement and better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, you have explored how to build knowledge graph RAG systems.
    You’ve learned how different retrieval strategies enable your agent to fetch relevant
    information, whether from structured or unstructured data. Understanding when
    to use methods like vector search or Cypher templates is key to designing an efficient
    and accurate system.
  prefs: []
  type: TYPE_NORMAL
- en: By implementing and refining retrieval strategies, you now have the foundation
    to build a powerful knowledge graph–based agent. You’ve seen how structured queries
    can enhance precision and how retrieval choices impact answer quality, and you’ve
    learned how to systematically evaluate performance. This chapter introduced benchmarking
    as a way to measure accuracy, recall, and faithfulness, giving you the tools to
    continuously improve your agent.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Next steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’re now equipped with the knowledge and tools to build and refine intelligent
    retrieval systems powered by knowledge graphs. Whether you’re creating a sophisticated
    question-answering agent or tailoring retrieval pipelines for specific domains,
    you have the foundation to design robust, high-performing, knowledge-driven AI
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are rapidly improving, not only in their ability to understand and generate
    language but also in how effectively they can use external tools for data retrieval,
    transformation, and manipulation. As these models become more capable, they will
    be able to perform increasingly complex tasks with minimal prompting. However,
    their effectiveness still depends on the quality, design, and integration of the
    tools you provide. It’s your job to implement those tools thoughtfully and efficiently,
    ensuring they are well suited to your system’s goals and constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this foundation, you can now begin building your own agentic GraphRAG
    systems. You are equipped to work with unstructured data in a variety of ways:
    you can embed text directly to enable fast similarity-based retrieval or go a
    step further and extract structured information—such as entities, relationships,
    and events—to populate a knowledge graph that supports more precise, semantic,
    and multihop queries. By combining these approaches, you can build retrieval systems
    that not only find relevant information but truly understand it, paving the way
    for powerful, context-aware AI applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluating a RAG pipeline is crucial for ensuring accurate and coherent answers.
    A benchmark evaluation helps measure performance and define the agent’s capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The evaluation process involves assessing various stages: retrieval tool selection,
    context retrieval relevance, answer generation quality, and overall system effectiveness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A well-structured benchmark dataset should include diverse queries that test
    retrieval accuracy, entity mapping, the handling of greetings, irrelevant queries,
    and various Cypher-based database lookups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of static expected answers, using Cypher queries as ground truth ensures
    the benchmark remains valid even if the underlying data changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context recall measures how well the system retrieves relevant information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faithfulness evaluates if the generated answer is factually consistent with
    the retrieved content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer correctness assesses whether the response fully and accurately addresses
    the query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
