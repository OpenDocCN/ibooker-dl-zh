["```py\nimport pygraphviz as pgv     #1\nfrom IPython.display import Image    #2\n\ncausal_graph = \"\"\"\ndigraph {\n    \"Prior Experience\" -> \"Player Skill Level\";\n    \"Prior Experience\" -> \"Time Spent Playing\";\n    \"Time Spent Playing\" -> \"Player Skill Level\";\n    \"Guild Membership\" -> \"Side-quest Engagement\";\n    \"Guild Membership\" -> \"In-game Purchases\";\n    \"Player Skill Level\" -> \"Side-quest Engagement\";\n    \"Player Skill Level\" -> \"In-game Purchases\";\n    \"Time Spent Playing\" -> \"Side-quest Engagement\";\n    \"Time Spent Playing\" -> \"In-game Purchases\";\n    \"Side-quest Group Assignment\" -> \"Side-quest Engagement\";\n    \"Customization Level\" -> \"Side-quest Engagement\";\n    \"Side-quest Engagement\" -> \"Won Items\";\n    \"Won Items\" -> \"In-game Purchases\";\n    \"Won Items\" -> \"Total Inventory\";\n    \"In-game Purchases\" -> \"Total Inventory\";\n}\n\"\"\"     #3\nG = pgv.AGraph(string=causal_graph)   #3\nG.draw('/tmp/causal_graph.png', prog='dot')    #4\nImage('/tmp/causal_graph.png')    #5\n```", "```py\nimport pandas as pd\ndata = pd.read_csv(\n    \"https://raw.githubusercontent.com/altdeep/causalML/master/datasets\n  ↪/online_game_example_do_why.csv\"     #1\n)\nprint(data.columns)     #2\n```", "```py\nIndex(['Guild Membership', 'Player Skill Level', 'Time Spent Playing',\n       'Side-quest Group Assignment', 'Customization Level',\n       'Side-quest Engagement', 'Won Items', 'In-game Purchases',\n       'Total Inventory'],\n      dtype='object')\n```", "```py\nIdentification.from_expression(\n    graph=dag,\n    query=query,\n    estimand=observational_distribution\n)\n```", "```py\nfrom dowhy import CausalModel     #1\n\nmodel = CausalModel(\n    data=data,     #2\n    treatment='Side-quest Engagement',     #3\n    outcome='In-game Purchases',   #3\n    graph=causal_graph    #4\n)\n```", "```py\nidentified_estimand = model.identify_effect()    #1\nprint(identified_estimand)\n```", "```py\nEstimand type: EstimandType.NONPARAMETRIC_ATE\n### Estimand : 1\nEstimand name: backdoor\nEstimand expression:\n           d                                                                  \n────────────────────────(E[In-game Purchases|Time Spent Playing,Guild \nd[Side-quest Engagement]                                                      \n\nMembership, Player Skill Level])\n\nEstimand assumption 1, Unconfoundedness: If U→{Side-quest Engagement} and U→In-game Purchases then P(In-game Purchases|Side-quest Engagement,Time Spent Playing,Guild Membership,Player Skill Level,U) = P(In-game Purchases|Side-quest Engagement,Time Spent Playing,Guild Membership,Player Skill Level)\n```", "```py\n### Estimand : 2\nEstimand name: iv\nEstimand expression:\n\n ⎡                                         -1⎤\n ⎢ d ⎛ d ⎞ `⎥`\n`E``⎢``──────────(IGP)``⋅` `───────────([SQE]) ` ```", "```py  ```", "```py` ```", "```py ```", "```py`` ```", "```py\n```", "```py`` ```", "```py ### Estimand : 3 Estimand name: frontdoor Estimand expression:   ⎡     d                d        ⎤ E⎢────────────(IGP)⋅───────([WI])⎥  ⎣d[WI]       d[SQE]             ⎦   Estimand assumption 1, Full-mediation:      WI intercepts (blocks) all directed paths from SQE to IGP. Estimand assumption 2, First-stage-unconfoundedness:     If U→{SQE} and U→{WI}     then P(WI|SQE,U) = P(WI|SQE) Estimand assumption 3, Second-stage-unconfoundedness:     If U→{WI} and U→IGP     then P(IGP|WI, SQE, U) = P(IGP|WI, SQE) ```", "```py causal_estimate_reg = model.estimate_effect(     identified_estimand,    #1     method_name=\"backdoor.linear_regression\",  #2     confidence_intervals=True    #3 ) ```", "```py print(causal_estimate_reg) ```", "```py ## Realized estimand b: In-game Purchases~Side-quest Engagement+Guild Membership+Time Spent Playing+Player Skill Level Target units: ate  ## Estimate Mean value: 178.08617115757784 95.0% confidence interval: [[168.68114922 187.4911931 ]] ```", "```py causal_estimate_strat = model.estimate_effect(     identified_estimand,     method_name=\"backdoor.propensity_score_stratification\",     #1     target_units=\"ate\",     confidence_intervals=True )  print(causal_estimate_strat) ```", "```py ## Estimate Mean value: 187.2931023294184 95.0% confidence interval: (180.3291962554186, 196.4556029137768) ```", "```py causal_estimate_match = model.estimate_effect(     identified_estimand,     method_name=\"backdoor.propensity_score_matching\",     #1     target_units=\"ate\",     confidence_intervals=True ) print(causal_estimate_match) ```", "```py ## Estimate Mean value: 199.8110290000004 95.0% confidence interval: (183.23361900000054, 210.5281390000008) ```", "```py causal_estimate_ipw = model.estimate_effect(     identified_estimand,     method_name=\"backdoor.propensity_score_weighting\",    #1     target_units = \"ate\",     method_params={\"weighting_scheme\":\"ips_weight\"},    #2     confidence_intervals=True ) print(causal_estimate_ipw) ```", "```py ## Estimate Mean value: 437.79246624944926 95.0% confidence interval: (358.10472302821745, 515.2480572854872) ```", "```py from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LassoCV from sklearn.ensemble import GradientBoostingRegressor  featurizer = PolynomialFeatures(degree=1, include_bias=False) gb_estimate = model.estimate_effect(     identified_estimand,     method_name = \"backdoor.econml.dml.DML\",     #1     control_value = 0,     treatment_value = 1,     method_params={         \"init_params\":{             'model_y': GradientBoostingRegressor(),    #2             'model_t': GradientBoostingRegressor(),    #3             'model_final': LassoCV(fit_intercept=False),     #4             'featurizer': featurizer          },         \"fit_params\":{}     } ) print(gb_estimate) ```", "```py ## Estimate Mean value: 175.7229947190752 ```", "```py from sklearn.ensemble import RandomForestRegressor     #1 metalearner_estimate = model.estimate_effect(   #1     identified_estimand,   #1     method_name=\"backdoor.econml.metalearners.TLearner\",   #1     method_params={   #1         \"init_params\": {'models': RandomForestRegressor()},    #1         \"fit_params\": {}    #1     }    #1 )    #1  print(metalearner_estimate) ```", "```py ## Estimate Mean value: 197.20665049459512 Effect estimates: [[ 192.6234]  [  -5.3165]  [ 133.2457]  ...  [  17.2561]  [-152.1482]  [ 264.887 ]] ```", "```py causal_estimate_fd = model.estimate_effect(     identified_estimand,     method_name=\"frontdoor.two_stage_regression\",     #1     target_units = \"ate\",     method_params={\"weighting_scheme\": \"ips_weight\"},     #2     confidence_intervals=True ) print(causal_estimate_fd) ```", "```py ## Estimate Mean value: 170.20560581290403 95.0% confidence interval: (141.53468188231938, 202.97221450388332) ```", "```py causal_estimate_iv = model.estimate_effect(     identified_estimand,     method_name=\"iv.instrumental_variable\",    #1     method_params = {         \"iv_instrument_name\": \"Side-quest Group Assignment\"   #2     },     confidence_intervals=True ) print(causal_estimate_iv) ```", "```py ## Estimate Mean value: 205.82297621514252 95.0% confidence interval: (-369.04011492007703, 923.6814756173349) ```", "```py causal_estimate_regdist = model.estimate_effect(     identified_estimand,     method_name=\"iv.regression_discontinuity\",    #1     method_params={         'rd_variable_name':'Customization Level',    #2         'rd_threshold_value':0.5,     #3         'rd_bandwidth': 0.15     #4     },     confidence_intervals=True, ) ```", "```py Mean value: 156.85691281931338 95.0% confidence interval: (-463.32687612531663, 940.698188663685) ```", "```py identified_estimand.set_identifier_method(\"frontdoor\")     #1 res_subset = model.refute_estimate(     identified_estimand,     #2     causal_estimate_fd,  #2     method_name=\"data_subset_refuter\",     #3     subset_fraction=0.8,     #4     num_simulations=100 ) print(res_subset) ```", "```py Refute: Use a subset of data Estimated effect:170.20560581290403 New effect:169.14858189323638 p value:0.82 ```", "```py identified_estimand.set_identifier_method(\"backdoor\") res_random = model.refute_estimate(    #1     identified_estimand,     #1     gb_estimate,    #1     method_name=\"random_common_cause\",    #1     num_simulations=100,    #1 )    #1 print(res_random) ```", "```py Refute: Add a random common cause Estimated effect:175.2192519976428 New effect:176.59119763647792 p value:0.30000000000000004 ```", "```py identified_estimand.set_identifier_method(\"backdoor\") res_placebo = model.refute_estimate( identified_estimand,     #1     causal_estimate_ipw,     #1     method_name=\"placebo_treatment_refuter\",     #1     placebo_type=\"permute\",     #1     num_simulations=100     #1 )  print(res_placebo) ```", "```py Refute: Use a Placebo Treatment Estimated effect:437.79246624944926 New effect:-531.2490111208127 p value:0.0 ```", "```py import numpy as np  coefficients = np.array([100.0, 50.0]) bias = 50.0 def linear_gen(df):      #1     subset = df[['guild_membership','player_skill_level']]    #1     y_new = np.dot(subset.values, coefficients) + bias    #1     return y_new     #1  ref = model.refute_estimate(    #2     identified_estimand,     #2     causal_estimate_fd,    #2     method_name=\"dummy_outcome_refuter\",    #2     outcome_function=linear_gen     #2 )     #2  res_dummy_outcome = ref[0] print(res_dummy_outcome)  Refute: Use a Dummy Outcome Estimated effect:0 New effect:-0.024480394297227835 p value:0.86 ```", "```py identified_estimand.set_identifier_method(\"backdoor\") res_unobserved = model.refute_estimate(     #1     identified_estimand,    #1     causal_estimate_fd,     #1     method_name=\"add_unobserved_common_cause\"    #1 )  print(res_unobserved) ```", "```py Refute: Add an Unobserved Common Cause Estimated effect:187.2931023294184 New effect:(-181.5795321684548, 398.98672237350416) ```", "```py import pandas as pd import torch  url = (\"https://raw.githubusercontent.com/altdeep/\"    #1        \"causalML/master/datasets/online_game_ate.csv\")    #1 df = pd.read_csv(url)     #1 df = df[[\"Guild Membership\", \"Side-quest Engagement\",   #2          \"Won Items\", \"In-game Purchases\"]]  #2   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     #3 data = {   #3     col: torch.tensor(df[col].values, dtype=torch.float32).to(device)    #3     for col in df.columns  #3 }   #3 ```", "```py import torch.nn as nn  class Confounders2Engagement(nn.Module):     def __init__(         self,         input_dim=1+1,    #1         hidden_dim=5    #2     ):         super().__init__()         self.fc1 = nn.Linear(input_dim, hidden_dim)    #3         self.f_engagement_*ρ* = nn.Linear(hidden_dim, 1)     #4         self.softplus = nn.Softplus()    #5         self.sigmoid = nn.Sigmoid()     #6      def forward(self, input):         input = input.t()         hidden = self.softplus(self.fc1(input))     #7         *ρ*_engagement = self.sigmoid(self.f_engagement_*ρ*(hidden))    #8         *ρ*_engagement = *ρ*_engagement.t().squeeze(0)         return *ρ*_engagement ```", "```py class PurchasesNetwork(nn.Module):     def __init__(         self,         input_dim=1+1+1,   #1         hidden_dim=5    #2     ):         super().__init__()         self.f_hidden = nn.Linear(input_dim, hidden_dim)    #3         self.f_purchase_μ = nn.Linear(hidden_dim, 1)     #4         self.f_purchase_σ = nn.Linear(hidden_dim, 1)     #5         self.softplus = nn.Softplus()     #6      def forward(self, input):         input = input.t()         hidden = self.softplus(self.f_hidden(input))     #7         μ_purchases = self.f_purchase_μ(hidden)   H #8         σ_purchases = 1e-6 + self.softplus(self.f_purchase_σ(hidden))    #9         μ_purchases = μ_purchases.t().squeeze(0)         σ_purchases = σ_purchases.t().squeeze(0)         return μ_purchases, σ_purchases ```", "```py from pyro import sample from pyro.distributions import Bernoulli, Normal from torch import tensor, stack  def model(params, device=device):     #1     z_dist = Normal(    #2         tensor(0.0, device=device),    #2         tensor(1.0, device=device))     #2     z = sample(\"Z\", z_dist)   #2     member_dist = Bernoulli(params['*ρ*_member'])    #3     is_guild_member = sample(\"Guild Membership\", member_dist)    #3     engagement_input = stack((is_guild_member, z)).to(device)    #4     *ρ*_engagement = confounders_2_engagement(engagement_input)    #4     engage_dist = Bernoulli(*ρ*_engagement)     is_highly_engaged = sample(\"Side-quest Engagement\", engage_dist)     #5     p_won = (     #6         params['*ρ*_won_engaged'] * is_highly_engaged +     #6         params['*ρ*_won_not_engaged'] * (1 - is_highly_engaged)     #6     )    #6     won_items = sample(\"Won Items\", Bernoulli(p_won))     #6     purchase_input = stack((won_items, is_guild_member, z)).to(device)     #7     μ_purchases, σ_purchases = purchases_network(purchase_input)   #7     purchase_dist = Normal(μ_purchases, σ_purchases)    #8     in_game_purchases = sample(\"In-game Purchases\", purchase_dist)   #8 ```", "```py import pyro from pyro import render_model, plate from pyro.distributions import Beta from pyro import render_model  confounders_2_engagement = Confounders2Engagement().to(device)     #1 purchases_network = PurchasesNetwork().to(device)   #1 def data_model(data, device=device):     pyro.module(\"confounder_2_engagement\", confounders_2_engagement)     #2     pyro.module(\"confounder_2_purchases\", purchases_network)   #2     two = tensor(2., device=device)     five = tensor(5., device=device)     params = {         '*ρ*_member': sample('*ρ*_member', Beta(five, five)),     #3         '*ρ*_won_engaged': sample('*ρ*_won_engaged', Beta(five, two)),    #4         '*ρ*_won_not_engaged': sample('*ρ*_won_not_engaged', Beta(two, five)), #5     }     N = len(data[\"In-game Purchases\"])     with plate(\"N\", N):    #6         model(params)    #6  render_model(data_model, (data, )) ```", "```py class Encoder(nn.Module):     def __init__(self, input_dim=3,  #1                  z_dim=1,     #2                  hidden_dim=5):     #3         super().__init__()         self.f_hidden = nn.Linear(input_dim, hidden_dim)         self.f_loc = nn.Linear(hidden_dim, z_dim)         self.f_scale = nn.Linear(hidden_dim, z_dim)         self.softplus = nn.Softplus()      def forward(self, input):         input = input.t()         hidden = self.softplus(self.f_hidden(input))   #4         z_loc = self.f_loc(hidden)   #5         z_scale = 1e-6 + self.softplus(self.f_scale(hidden))     #6         return z_loc.t().squeeze(0), z_scale.t().squeeze(0) ```", "```py from pyro import param from torch.distributions.constraints import positive  encoder = Encoder().to(device)  def guide(data, device=device):     pyro.module(\"encoder\", encoder)     α_member = param(\"α_member\", tensor(1.0, device=device),     #1                      constraint=positive)   #1     β_member = param(\"β_member\", tensor(1.0, device=device),    #1                         constraint=positive)  #1     sample('*ρ*_member', Beta(α_member, β_member))   #1     α_won_engaged = param(\"α_won_engaged\", tensor(5.0, device=device),     #2                          constraint=positive)   #2     β_won_engaged = param(\"β_won_engaged\", tensor(2.0, device=device),   #2                         constraint=positive)    #2     sample('*ρ*_won_engaged', Beta(α_won_engaged, β_won_engaged))   #2     α_won_not_engaged = param(\"α_won_not_engaged\",   #2                         tensor(2.0, device=device),    #2                         constraint=positive)    #2     β_won_not_engaged = param(\"β_won_not_engaged\",   #2                         tensor(5.0, device=device), #2                         constraint=positive)   #2     beta_dist = Beta(α_won_not_engaged, β_won_not_engaged)  #2     sample('*ρ*_won_not_engaged', beta_dist)   #2     N = len(data[\"In-game Purchases\"])     with pyro.plate(\"N\", N):         z_input = torch.stack(    #3             (data[\"Guild Membership\"],     #3              data[\"Side-quest Engagement\"],     #3              data[\"In-game Purchases\"])    #3         ).to(device)     #3         z_loc, z_scale = encoder(z_input)   #3         pyro.sample(\"Z\", Normal(z_loc, z_scale))    #3 ```", "```py from pyro.infer import SVI, Trace_ELBO from pyro.optim import Adam from pyro import condition  pyro.clear_param_store()    #1 adam_params = {\"lr\": 0.0001, \"betas\": (0.90, 0.999)}    #2 optimizer = Adam(adam_params)    #2 training_model = condition(data_model, data)     #3 svi = SVI(training_model, guide, optimizer, loss=Trace_ELBO())     #4 elbo_values = []    #5 N = len(data['In-game Purchases'])    #5 for step in range(500_000):    #5     loss = svi.step(data) / N    #5     elbo_values.append(loss)   #5     if step % 500 == 0:     #5         print(loss)     #5 ```", "```py import math import matplotlib.pyplot as plt  plt.plot([math.log(item) for item in elbo_values])     #1 plt.xlabel('Step')     #1 plt.ylabel('Log-Loss')     #1 plt.title('Log Training Loss')     #1 plt.show()     #1 ```", "```py print((          pyro.param(\"α_member\"),          pyro.param(\"β_member\"),          pyro.param(\"α_won_engaged\"),          pyro.param(\"β_won_engaged\"),          pyro.param(\"α_won_not_engaged\"),          pyro.param(\"β_won_not_engaged\")     )) ```", "```py (tensor(1.3953, grad_fn=<AddBackward0>), tensor(1.3558, grad_fn=<AddBackward0>), tensor(4.3976, grad_fn=<AddBackward0>), tensor(3.1667, grad_fn=<AddBackward0>), tensor(0.8065, grad_fn=<AddBackward0>), tensor(10.8452, grad_fn=<AddBackward0>)) ```", "```py import matplotlib.pyplot as plt import seaborn as sns from pyro.infer import Predictive  predictive = Predictive(data_model, guide=guide, num_samples=1000)     #1 predictive_samples_all = predictive(data)    #1 predictive_samples = predictive_samples_all[\"In-game Purchases\"]    #1 for i, sample_data in enumerate(predictive_samples):     #2     if i == 0:    #2         sns.kdeplot(sample_data,    #2             color=\"lightgrey\", label=\"Predictive density\")    #2     else:     #2         sns.kdeplot(sample_data,     #2             color=\"lightgrey\", linewidth=0.2, alpha=0.5)     #2  sns.kdeplot(     #3     data['In-game Purchases'],    #3     color=\"black\",   #3     linewidth=1,    #3     label=\"Empirical density\"     #3 )   #3  plt.legend() plt.title(\"Posterior Predictive Check of In-game Purchases\") plt.xlabel(\"Value\") plt.ylabel(\"Density\") plt.show() ```", "```py from pyro.infer import Predictive from pyro import do  data_model_low_engagement = do(    #1     data_model, {\"Side-quest Engagement\": 0.})   #1 predictive_low_engagement = Predictive(     #2     data_model_low_engagement, guide=guide, num_samples=1000)   #2 predictive_low_engagement_samples = predictive_low_engagement(data)   #2  data_model_high_engagement = do(    #3     data_model, {\"Side-quest Engagement\": 1.})    #3 predictive_high_engagement = Predictive(    #4     data_model_high_engagement, guide=guide, num_samples=1000)   #4 predictive_high_engagement_samples = predictive_high_engagement(data) ```", "```py low_samples = predictive_low_engagement_samples[\"In-game Purchases\"]     #1 for i, sample_data in enumerate(low_samples):   #1     if i == 0:  #1         sns.kdeplot(sample_data,   #1             clip=(0, 35000), color=\"darkgrey\", label=\"$P(I_{E=0})$\")    #1     else:   #1         sns.kdeplot(sample_data,  #1             clip=(0, 35000), color=\"darkgrey\",    #1             linewidth=0.2, alpha=0.5)   #1             #1 high_samples = predictive_high_engagement_samples[\"In-game Purchases\"]    #1 for i, sample_data in enumerate(high_samples):   #1     if i == 0:    #1         sns.kdeplot(sample_data,   #1             clip=(0, 35000), color=\"lightgrey\", label=\"$P(I_{E=1})$\")    #1     else:   #1         sns.kdeplot(sample_data,    #1             clip=(0, 35000), color=\"lightgrey\",   #1             linewidth=0.2, alpha=0.5)    #1 title = (\"Posterior predictive sample density \"     #2          \"curves of $P(I_{E=1})$ & $P(I_{E=0})$\")     #2 plt.title(title)     #2 plt.legend()     #2 plt.xlabel(\"Value\")   #2 plt.ylabel(\"Density\")     #2 plt.ylim((0, .0010))    #2 plt.xlim((0, 4000))     #2 plt.show()     #2 ```", "```py samp_high = predictive_high_engagement_samples['In-game Purchases']    #1 exp_high = samp_high.mean(1)   #1 samp_low = predictive_low_engagement_samples['In-game Purchases']    #2 exp_low = samp_low.mean(1)    #2 ate_distribution = exp_high - exp_low    #3  sns.kdeplot(ate_distribution)     #4 plt.title(\"Posterior distribution of the ATE\")    #4 plt.xlabel(\"Value\")     #4 plt.ylabel(\"Density\")    #4 plt.show()    #4 ```", "```py` ```"]