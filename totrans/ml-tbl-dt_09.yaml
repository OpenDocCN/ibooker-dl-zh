- en: Part 3\. Deep learning for tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Part 3 is your guide to the know-how and practical insights needed to apply
    deep learning to tabular data problems. As a stand-alone solution or integrated
    with gradient boosting, deep learning can get good results with tabular data when
    you know how to use its unique way of finding solutions to predictive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8 explores various deep learning stacks and frameworks for working with
    tabular data, including low-level frameworks like TensorFlow and PyTorch and high-level
    APIs like fastai and Lightning Flash. It introduces several libraries specifically
    designed for tabular deep learning tasks, such as TabNet, PyTorch Tabular, SAINT,
    and DeepTables. We compare the different stacks and discuss each one’s strengths
    and weaknesses. Chapter 9 extends the discussion to best practices. We use the
    Kuala Lumpur real estate dataset to illustrate these best practices for deep learning
    with tabular data, including data preparation, model architecture design, and
    model training. A Keras-based project, the example emphasizes easy, understandable,
    and effective data pipelines, along with a modular approach that promotes code
    reuse.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 10 explores how to make a trained deep learning model available for
    use in a real-world environment using Flask, a Python framework that excels in
    web interfaces and API serving. Chapter 11 extends the discussion by guiding you
    through the steps to define a pipeline for training and deploying a model using
    the Vertex AI environment in Google Cloud, including creating Docker containers
    to encapsulate the model code and dependencies, defining pipeline steps, and running
    the pipeline on Vertex AI. We then proceed to discuss Gemini’s capabilities for
    Google Cloud, such as answering questions about Google Cloud, generating code
    from text, interpreting code, and summarizing log entries. All these capabilities
    can be applied to your workflow to create your own machine learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The book concludes with chapter 12, where you will learn how to combine deep
    learning with machine learning to achieve state-of-the-art results in predictive
    tasks. After you read this book, model design, training, deployment, and interpretability
    will no longer hold any secrets for you!
  prefs: []
  type: TYPE_NORMAL
