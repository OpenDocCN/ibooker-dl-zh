- en: Chapter 1\. Foundations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to *Learning GitHub Copilot*! I’m excited you’re reading this book and
    hope you find it useful. AI tools like ChatGPT and AI agents have changed, and
    will continue to change, how we interact with software applications. GitHub Copilot
    and similar tools have changed, and will continue to change, how programmers create
    software applications. Through its ability to take context from existing code
    or natural language prompts, GitHub Copilot provides a richer and more powerful
    code-generation capability than any we’ve seen before.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, I’ll help you understand how to use GitHub Copilot’s capabilities—from
    performing code generation and completion, forming tests, and translating and
    explaining code, to working with repositories, pull requests, and issues directly
    in GitHub. You’ll see examples of using Copilot across multiple domains and programming
    languages. You’ll learn to leverage it to your advantage and craft prompts to
    get the best results. You’ll even learn how to add your own custom functionality
    to it. And you’ll understand how it does what it does, including why it sometimes
    doesn’t provide the results you expect—and how to mitigate those situations.
  prefs: []
  type: TYPE_NORMAL
- en: Some foundational knowledge is required to begin with, though. That’s the purpose
    of this chapter. I’ll start by explaining GitHub Copilot at a high level. We’ll
    then explore the key underlying technology, its overall flow, some usage considerations,
    how it differs from tools like ChatGPT, and what you need to know about getting
    and installing it. So, let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot Catchall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we begin this discussion, be aware that *copilot* is a popular term for AI
    applications that collect information, formulate prompts, and return answers and
    suggestions. For example, Microsoft has an [Office 365 Copilot](https://oreil.ly/DaSLT)
    that does this for Microsoft Office applications. It analyzes context from Word,
    Teams, Outlook, etc., and provides summarizations, suggested responses, and other
    valuable interactions.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, I use *Copilot* to mean *GitHub Copilot*. The only exceptions
    will occur when I am referring to another system that uses the same word. In those
    cases, I’ll explicitly identify them with their formal names, such as *Office
    365 Copilot*.
  prefs: []
  type: TYPE_NORMAL
- en: What Is GitHub Copilot?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GitHub Copilot is a cloud-based, generative artificial intelligence (AI) tool.
    Let’s break down these buzzwords. We can generically define AI as *computers doing
    tasks that previously only humans were thought to be able to accomplish because
    of required reasoning and skills*. More recently, this has also taken on the aspect
    of interacting with humans in a natural and human-like way through natural language
    processing (NLP), chat interfaces, and automated processing and decision making
    with AI agents.
  prefs: []
  type: TYPE_NORMAL
- en: '*Cloud-based* refers to the pathway through which Copilot returns suggestions
    and generates answers. It refers to a cloud environment managed by GitHub that
    facilitates interaction with the AI models. *Generative* expresses the AI’s ability
    to *generate* new results from the context it takes in. Copilot can offer responses
    and suggestions for software development based on the context and prompts from
    the user’s environment. How well it does this depends on several factors that
    we’ll talk about in the next section. But, as a quick example, [Figure 1-1](#using-copilot-on-a-pr)
    shows using Copilot to suggest optimizations based on a project in Visual Studio
    (VS) Code.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/lghc_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. Using Copilot on a project in VS Code
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Default IDE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because it’s not possible to represent all integrated development environments
    (IDEs) in this book, we’ll be using VS Code or GitHub Codespace in our examples
    where an IDE is involved. If you use a different IDE, please consult the Copilot
    documentation for any differences in use, controls, etc.
  prefs: []
  type: TYPE_NORMAL
- en: How Does Copilot Work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how Copilot works, we need to understand some of the pieces that
    underpin its functionality and that of similar AI tools. These details include
    where it gets information to base responses on and its overall flow.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot and most AI applications get their data from large language models (LLMs)
    trained on extensive data collections. If you’re unfamiliar with term LLM, the
    following section provides a brief explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s challenging to discuss AI tooling today without mentioning LLMs. LLMs are
    AI models trained on vast amounts of existing data to predict the next words or
    other types of content (e.g., *tokens*) that *fit* given some input (a *prompt*).
  prefs: []
  type: TYPE_NORMAL
- en: LLMs differ from traditional computer models that can process formatted data
    or respond to a math problem because LLMs are taught to understand context, syntax,
    and structure. This is done by algorithms that consider vast numbers of parameters
    to figure out what words or tokens make the most sense to come next. In this way,
    the models statistically craft responses based on how the input is presented,
    not just the input itself.
  prefs: []
  type: TYPE_NORMAL
- en: The model’s prediction capabilities are learned and tuned from extensive collections
    of existing content. In this training process, the models map how the various
    pieces of information relate in any given domain. Technically, given a sequence
    of tokens as a query, LLMs can assess the input’s syntax and structure, infer
    context based on the model’s training data, and predict the sequence of tokens
    that would likely come next. Simply put, LLMs can figure out what would sound
    right based on all the data they’ve digested and continue the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: To appreciate the difference from traditional processing, think of learning
    a language and then conversing with someone who speaks it as a first language.
    You can learn the vocabulary and individual words and phrases. But in a conversation
    with someone who speaks this language well, you also need to gather and understand
    the context of what the other person is saying. This is so you can choose the
    right words or phrases to respond with. You also need to be able to frame your
    response in a context that will make sense to the other person. Language has syntax
    and structure, but the context of a conversation is how we ensure that syntax
    and structure make sense and convey meaning. LLMs can assess context from the
    inputs and supply relevant context in their outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Turning back to Copilot, it uses, by default, LLMs developed and managed by
    [OpenAI](https://openai.com), the same company behind [ChatGPT](https://chat.openai.com).
    In conjunction with OpenAI, GitHub developed Copilot over several years. Recently,
    they have added options to use several other families of models, including ones
    from Claude by Anthropic and Google Gemini. Across models, the common characteristic
    of Copilot is the focus on creating software, and its conversational language
    is code.
  prefs: []
  type: TYPE_NORMAL
- en: Code and Generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Programming languages, by definition, have specific, required syntax and semantics
    that differ for each language. When coding in Python versus Go, you use different
    tokens and structures to create the program. However, the tokens and structure
    you use have rules. They are well-defined and form a closed set. Copilot’s abilities
    are targeted to provide coding suggestions and related information that match
    syntax and structure. But the real value-add is providing responses that are relevant
    to what the coder is creating or prompting about. The context that gets fed into
    Copilot can come from several sources, including these:'
  prefs: []
  type: TYPE_NORMAL
- en: A set of code being written in an editor in a development environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactions with the model via direct natural language prompts or queries,
    aka *chat models*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typical development activities in GitHub itself, such as pull requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’ll discuss these interactions more throughout the book. But regardless of
    the interface, the context (code, directives/input, or GitHub elements you work
    with) gets turned into a *prompt*—your side of the conversation that you expect
    the AI to respond to. Processing those and deciding on a response based on context
    and the model’s training and capabilities is the *generative* part of Copilot’s
    *generative AI* functionality. The response can be suggested code, an answer to
    a question, or step-by-step directions. It is what the AI thinks completes the
    code, satisfies the prompt, or answers the query.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of this process like describing a set of symptoms to a doctor
    over a Zoom call so they can try to reach a diagnosis based on their years of
    training. Or describing to the car mechanic over the phone an issue that you are
    seeing with your vehicle so that they can suggest a fix. In these cases, the communication
    and context you provide, with the professional’s training, make all the difference.
    The interaction can result in suggestions to address the issue (some of which
    may not apply) or indicate that the professional didn’t have enough context or
    understanding to help. Generative AI behaves the same way.
  prefs: []
  type: TYPE_NORMAL
- en: When Copilot offers suggestions as you’re coding, items in your coding environment
    provide the context to create a prompt for the AI model. Copilot can produce suggestions
    that follow the coding style used in the files that are part of the project you’re
    working on. This can be both good and bad.
  prefs: []
  type: TYPE_NORMAL
- en: It can be good that Copilot’s suggestions are often similar to the users’ existing
    coding styles if those coding styles reflect best practices. It can be bad if
    Copilot sometimes skews too much towards bad practices in existing code. The latter
    situation can lead to reinforcing limited and inefficient coding practices—or
    you may have to wade through a more extensive set of suggestions to find the one
    that fits. The quality and quantity of the examples that Copilot has to draw on
    from your environment, and its own training, can affect its responses.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the generative AI employed with Copilot can be very useful. Still,
    be aware that the coding suggestions will be biased by the context and training
    Copilot has to work with. You can use this to your advantage to provide Copilot
    with more examples of the style of suggestions it should return. However, this
    bias can also be a disadvantage if the context that Copilot has to work with is
    limited. We’ll discuss how to help Copilot return the best results throughout
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s briefly look at Copilot’s high-level flow to understand more about how
    it interacts with context to respond to you.
  prefs: []
  type: TYPE_NORMAL
- en: High-Level Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand how Copilot works at a high level, we can trace the basic workflow
    from the perspective of working in one of the supported IDEs. Currently, the supported
    IDEs for Copilot include Visual Studio, VS Code, NeoVim, any of the JetBrains
    family of IDEs, and others. Copilot also works in selected other environments,
    such as [GitHub Codespaces](https://oreil.ly/QyEks).
  prefs: []
  type: TYPE_NORMAL
- en: GitHub has also released the [Copilot Language Server SDK](https://oreil.ly/6rDHq),
    which can be used to integrate GitHub Copilot into any editor or IDE. So, expect
    to see more applications integrated with Copilot in the future.
  prefs: []
  type: TYPE_NORMAL
- en: About Codespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GitHub Codespaces utilize virtual machines (VMs) running in the cloud that provide
    a full-featured and customizable development environment for GitHub users. Their
    interface is similar to that of VS Code; they can be tailored with the same extensions
    used in VS Code in a browser or connected to several IDEs. GitHub provides them
    as an optional service.
  prefs: []
  type: TYPE_NORMAL
- en: 'When writing code in an editor and using Copilot to suggest coding completions,
    several pieces of information are automatically scanned to gather context about
    what you’re working on:'
  prefs: []
  type: TYPE_NORMAL
- en: Current file
  prefs: []
  type: TYPE_NORMAL
- en: The current file that a user is editing is one key source of context for Copilot.
  prefs: []
  type: TYPE_NORMAL
- en: Name of the currently active file
  prefs: []
  type: TYPE_NORMAL
- en: When named descriptively, this can provide Copilot with clues about what the
    code is intended to do, such as *TestConfig.go*.
  prefs: []
  type: TYPE_NORMAL
- en: Content before and after the current cursor position
  prefs: []
  type: TYPE_NORMAL
- en: Copilot can draw context from the code and comments immediately before and after
    the cursor position in the file. This can help it decide what to fill in and/or
    understand gaps in the code.
  prefs: []
  type: TYPE_NORMAL
- en: Comments
  prefs: []
  type: TYPE_NORMAL
- en: Like human pair programmers or reviewers, Copilot can use comments to understand
    what code is doing and the intent of code that has yet to be written. This is
    one of the primary ways to provide context for Copilot—the more precise and detailed
    the comments, the more likely the code that Copilot suggests will be relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Other open files in the editor
  prefs: []
  type: TYPE_NORMAL
- en: Copilot uses the code being developed in any open files as context. This is
    key to gathering information about the current task and augmenting information
    in the model. For example, one strategy for dealing with deprecated features stored
    in the LLM is to open a file in the editor with the replacement approach for the
    deprecated feature. From this example, Copilot can interpret preferred alternatives
    for coding instead of relying on the deprecated approach used to train the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Local index
  prefs: []
  type: TYPE_NORMAL
- en: Copilot automatically parses most files in a project opened in IDEs like VS
    Code and builds an advanced local *index* for the project. (See the following
    sidebar for more info.)
  prefs: []
  type: TYPE_NORMAL
- en: If you are using the chat interface, it will usually be pre-populated with a
    file, selection, or terminal command as context, depending on what you were most
    recently working on. However, you can explicitly change that before submitting
    your prompt. (More on this in [Chapter 3](ch03.html#ch03).)
  prefs: []
  type: TYPE_NORMAL
- en: So, when you’re using one of the interfaces with Copilot installed and activated,
    Copilot gathers context from these sources as you enter code ([Figure 1-2](#gathering-context-fro)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/lghc_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Gathering context from the IDE environment
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: That context is processed and ultimately sent to GitHub, synthesized into a
    prompt ([Figure 1-3](#prompt-synthesized)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/lghc_0103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. Synthesizing the prompt
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The prompt is then passed through GitHub to the LLM, and possible completions
    or answers are returned. Once results are generated from the prompt to the LLM,
    GitHub’s Copilot systems perform additional processing on the result (more on
    that later). After that, the response is returned to the IDE, where you can evaluate
    it and choose how to proceed. See [Figure 1-4](#flow-from-prompt-to-s) for the
    flow for code completion.
  prefs: []
  type: TYPE_NORMAL
- en: This process continues and repeats with the user/Copilot interactions. In this
    way, Copilot acts as an assistant to help you with whatever use case you are working
    on, whether crafting boilerplate code, searching for a sophisticated algorithm,
    generating data or queries, writing unit tests, or learning a new programming
    language.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/lghc_0104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. Flow from the prompt to suggestions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You’ll see more of how the flow happens as we discuss how to use Copilot throughout
    the remaining chapters. But at this point, it’s worth understanding some of the
    usage considerations when using Copilot to help you produce software.
  prefs: []
  type: TYPE_NORMAL
- en: Usage Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you know what Copilot is and how it works, here’s a key point to understand:
    Copilot, like any AI, can give incorrect or incomplete answers. It can be wrong,
    or it can give you unexpected results. This is not common, but you should keep
    some important things in mind when using it so you can be aware. In this section,
    we’ll look at the following considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: Timeliness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completeness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And by the way, these aren’t unique to Copilot. They can apply to any current
    AI tool assisting you with a task.
  prefs: []
  type: TYPE_NORMAL
- en: Timeliness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Timeliness* here refers to the currency of Copilot’s suggestions. This may
    seem like an odd choice to start with, but it can intersect with all of the others.'
  prefs: []
  type: TYPE_NORMAL
- en: Copilot relies on models trained initially at a point in time, so its data is
    based on what was current in the training data at that point. For example, if
    the model Copilot uses was trained two years ago, it does not necessarily know
    about changes since then.
  prefs: []
  type: TYPE_NORMAL
- en: Your everyday use of Copilot can produce outdated suggestions and answers. Results
    could contain deprecated code that no longer works with your compiler or interpreter.
    You could get a suggestion that references a version of a dependency with a known
    vulnerability or a response that might include an outdated approach.
  prefs: []
  type: TYPE_NORMAL
- en: You might ask the chat interface in Copilot, “Is *X* deprecated?” and get an
    answer that *X* is *not* deprecated when, in fact, it is. Copilot is answering
    as of the point in time when the model it is using was trained. Or you might ask,
    “What is the current version of *X*?” and get a result from two years ago. I’m
    sure you can see how this could cause issues. Results can also vary significantly
    depending on the model you’ve chosen (from the ones that Copilot supports) for
    the current interaction.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 7](ch07.html#ch07) discusses techniques for providing Copilot with
    more up-to-date context for deprecated items within your environment, increasing
    the likelihood of returning more up-to-date responses.'
  prefs: []
  type: TYPE_NORMAL
- en: Relevance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Copilot’s suggestions and chat responses are based on LLMs trained on large
    sets of code. If you’re using the default OpenAI models, that codebase is the
    public code hosted on GitHub. GitHub arguably has the most comprehensive collection
    of repositories for open source software. This includes code written in today’s
    most popular programming languages, such as Python, Go, and JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: Given the training, it stands to reason that Copilot will be most effective
    when you’re working in one of the more common programming languages or frameworks
    represented in the codebase used for training. The more a language or framework
    is represented in that collection (GitHub repositories for the OpenAI models),
    the more references Copilot can learn from. Think of it like the subjects you
    spent more time studying in school—those are the ones you know the most about.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working in a less represented language or framework, the suggestions
    may not be as useful. That doesn’t imply you can’t get relevant suggestions and
    answers, but you may find them less helpful in answering your exact query.
  prefs: []
  type: TYPE_NORMAL
- en: A key factor beyond Copilot’s control is the amount of context it has to work
    with. For example, if your code consists of a function named `ParseData` or you
    supply a generic or ambiguous prompt, such as “Create a function to parse data,”
    without additional context, the results returned from Copilot are likely to also
    be generic.
  prefs: []
  type: TYPE_NORMAL
- en: Completeness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI can sometimes return incomplete or unusable suggestions. This
    rarely happens in the chat interface, but it’s not uncommon for Copilot’s code
    completion to return a set of suggestions that are only partial solutions.
  prefs: []
  type: TYPE_NORMAL
- en: The same caveat discussed at the end of the previous section on relevancy applies
    here. If your code or query is generic or ambiguous, Copilot may not have enough
    context to draw on to return a complete result.
  prefs: []
  type: TYPE_NORMAL
- en: In other instances, you might provide specific context to Copilot, but it suggests
    only a comment or the first line of a function. This is generally seen when working
    through the suggestion process in the IDE rather than in the chat interface. You
    may need to provide additional prompting or supply more context for Copilot. Sometimes
    you can *nudge* Copilot by giving it a hint (typing a keyword, for example), and
    that will be enough to make it return a more complete suggestion. At other times,
    Copilot may simply respond with a blank line, and accepting that response may
    cause it to continue generating code.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot does offer options to get more suggestions if the immediate one isn’t
    a good fit, but those can be of limited utility. I’ll delve into details on those
    approaches in [Chapter 2](ch02.html#ch02).
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Except in the simplest of cases, you cannot assume that any response from Copilot
    is entirely correct or the best answer. You should always carefully review Copilot
    responses. It’s not hard to find stories of AI results in other domains that made
    their way into official records with references that don’t exist. For example,
    there have been reports of court briefs that were filed referencing previous cases
    that didn’t happen. I know of people who have tried to use AI to plan vacation
    itineraries and received a promising agenda—only with hotels that didn’t exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, Copilot may return a response that is valid syntax but references
    constructs or variables that do not exist in the code. This is a coding form of
    AI *hallucination*: the AI presents information that is incorrect, made up, or
    otherwise not grounded in reality as a valid solution or suggestion. This is rare
    but can still occur. Sometimes this can be because Copilot needs to create suggestions
    around elements that don’t exist yet. For example, if you ask Copilot how to open
    and write to a file without specifying the name, it may use a filename that has
    no meaning in your code within its example.'
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the circumstances, the user is always responsible for reviewing
    Copilot’s responses for accuracy. This should be no different from reviewing a
    human’s contributions to your code; you want to ensure they are correct and do
    no harm.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts and Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The results from Copilot can often be improved by developing a better prompt
    for the AI. We’ll cover more about prompting throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another aspect of working with Copilot is data privacy. As previously noted,
    several information sources (including open files and current files) are read,
    and information is collected and transferred through Copilot to factor into the
    prompt for the LLM. That means some data is going across the web and being processed
    outside your control. This may seem like a potential security risk. However, Copilot
    includes options when signing up to specify whether or not you want to allow GitHub
    to include your context information as part of its data to help Copilot get better.
    (See the bottom part of [Figure 1-5](#options-at-signup-for).) If you do not,
    while the information will be gathered, it will not be retained. It will be collected
    to construct the prompt and then discarded.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot Trust Center
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GitHub has a [Trust Center](https://oreil.ly/0GM3-) to help with any privacy-related
    content issues or concerns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/lghc_0105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-5\. Options at sign-up for data privacy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Copilot intersects with user data in three areas: user engagement, prompts,
    and suggestions. Here’s a summary:'
  prefs: []
  type: TYPE_NORMAL
- en: User engagement
  prefs: []
  type: TYPE_NORMAL
- en: This is usage data about how you interact with Copilot (telemetry). It can include
    whether you accept or dismiss suggestions from Copilot, how you interact with
    the chat UI, and metrics such as latency and error messages.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts
  prefs: []
  type: TYPE_NORMAL
- en: As previously discussed, the context information taken from your environment,
    or a chat query passed back to GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Suggestions
  prefs: []
  type: TYPE_NORMAL
- en: The suggested code completions returned by Copilot and/or the responses to chat
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: User engagement data is tracked by default and could include anonymized data,
    although there are nuances depending on the particular Copilot plan you’re using.
    (See the [Trust Center](https://oreil.ly/rC8Rm) documentation for more information.)
    Mechanisms exist to encrypt data in transit and at rest. GitHub has controls to
    strictly limit who can access data on their side.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Repositories and Legal Concerns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The legality of using the *public* repositories on GitHub to train some LLMs
    has been questioned in public opinion and in courts. We will leave those considerations
    for others to sort out, and we won’t focus on, or comment on, those aspects of
    the initial training process in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'One other concern is often raised about Copilot when using a model trained
    on licensed repositories: Copilot could generate suggestions that closely match
    content from that codebase. The implication is that users could end up unwittingly
    violating licensing terms and intellectual property rights by having the duplicated
    code included in their work. Copilot includes an option for individuals and administrators
    to block public code matches if those are generated as part of the AI’s process.
    (See the top part of [Figure 1-5](#options-at-signup-for).) If that option is
    selected, Copilot will alert you to the situation and filter out matches from
    the public code base.'
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Last but certainly not least is the area of security in the results that Copilot
    returns. Security is an ever-present concern in any product or application used
    today. Addressing that concern starts with secure coding practices.
  prefs: []
  type: TYPE_NORMAL
- en: I stated earlier that once Copilot receives potential results from the LLM,
    GitHub performs some additional processing on them. This processing includes running
    algorithms to quickly look for possible vulnerabilities. GitHub does not run a
    security scanning application against the result, which would take too long. Instead,
    it quickly scans for patterns indicating vulnerabilities and/or insecure coding
    practices. If those are identified, the proposed suggestion is flagged.
  prefs: []
  type: TYPE_NORMAL
- en: Even with these measures, there is no guarantee that something hasn’t slipped
    through. Since the processing isn’t the same as running full scans with an application
    focused on finding vulnerabilities, the results from Copilot should still be subject
    to whatever other security checks you would use for any code.
  prefs: []
  type: TYPE_NORMAL
- en: '*The clear and ever-present requirement when using generative AI is that you
    must always review and assess any suggestions it returns. You should not assume
    that it completely and correctly interpreted the context. And, in all but the
    simplest of cases, you cannot assume that the result is perfect.*'
  prefs: []
  type: TYPE_NORMAL
- en: To state this another way, while Copilot is often referred to as an *AI pair
    programmer*, it does not have the same understanding and level of familiarity
    with your code as an actual human pair programmer would. Instead, it is best to
    think of Copilot as a skilled programmer new to the project. A programmer in that
    position can create useful code based on what they can observe and what has been
    shared with them, but they can’t possess all of the larger context, project history,
    or backstory. As a result, you must be diligent in ensuring that the code they
    produce is accurate, secure, and suitable to merge. Copilot’s answers and suggestions
    should be treated the same way.
  prefs: []
  type: TYPE_NORMAL
- en: And, as when working with a new programmer on the team, the more details you
    provide (whether code to use as context or specific directions), the better the
    result will usually be. Giving Copilot more coding examples to draw on and more
    specifics in your prompt in the chat can go a long way towards getting a better
    result from the AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another consideration may come up as you start to work with or consider using
    an assistant like Copilot: why not just use ChatGPT or a similar chatbot to produce
    code? The answer is that you certainly can. However, the two approaches and interfaces
    have key differences. The following section provides a quick comparison, using
    ChatGPT to represent other generalized AI applications that can create code.'
  prefs: []
  type: TYPE_NORMAL
- en: Copilot Versus ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might wonder how Copilot or similar coding assistants differ from ChatGPT
    or similar chatbots, given both may use the same underlying models to produce
    responses. The general distinction is that Copilot is focused only on the coding
    domain and provides functionality to specifically help with that. ChatGPT, on
    the other hand, is targeted more broadly across any domain and doesn’t provide
    the same level of integration. [Table 1-1](#comparison-of-copilot10) highlights
    differences in several more specific categories.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-1\. Comparison of Copilot to ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | GitHub Copilot | ChatGPT |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Primary function | Understanding and generating code or code-related prompts
    | Understanding and generating any natural language |'
  prefs: []
  type: TYPE_TB
- en: '| Primary user interface | Code editors, chat | Chat |'
  prefs: []
  type: TYPE_TB
- en: '| Developer | GitHub with OpenAI | OpenAI |'
  prefs: []
  type: TYPE_TB
- en: '| Primary use cases | Writing and augmenting code and code documentation |
    Conversational responses with text generation |'
  prefs: []
  type: TYPE_TB
- en: '| Pricing model | Subscription based with usage tracking for premium models
    | Usage based and subscription based |'
  prefs: []
  type: TYPE_TB
- en: '| Training data | Code repos, documentation | Diverse, broad text content |'
  prefs: []
  type: TYPE_TB
- en: '| Public APIs | Limited to telemetry, monitoring, and license management |
    Broad API surface for interaction |'
  prefs: []
  type: TYPE_TB
- en: In general, you can think of Copilot as a very domain-specific implementation
    of generative AI. This is as opposed to the broader (domain-less) implementation
    and function of ChatGPT and other general chat tools. ChatGPT can certainly be
    used for generating code, but it lacks the integration with development environments,
    coding focus, and GitHub support and features that Copilot has.
  prefs: []
  type: TYPE_NORMAL
- en: Although we’ve been discussing Copilot as a single application, it actually
    has five configurations. Let’s finish up this introductory chapter by helping
    explain the plans available for you to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot Plans
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Copilot comes in five plans at the time of this writing: *Free*, *Pro*, *Pro+*,
    *Business*, and *Enterprise*. To understand the differences between them, you
    need to understand some common terms:'
  prefs: []
  type: TYPE_NORMAL
- en: Code completions
  prefs: []
  type: TYPE_NORMAL
- en: AI-suggested code for completing code being worked on in your IDE.
  prefs: []
  type: TYPE_NORMAL
- en: Chat request
  prefs: []
  type: TYPE_NORMAL
- en: A prompt, which can be a direction or a question, that you pass to Copilot through
    a chat interface.
  prefs: []
  type: TYPE_NORMAL
- en: Agent mode
  prefs: []
  type: TYPE_NORMAL
- en: An option in Copilot to have it act on a prompt by independently planning, executing,
    and iterating across files to suggest changes needed to accomplish a task.
  prefs: []
  type: TYPE_NORMAL
- en: Agent mode request
  prefs: []
  type: TYPE_NORMAL
- en: A request made to Copilot to accomplish a task while it is in Agent mode.
  prefs: []
  type: TYPE_NORMAL
- en: Model access
  prefs: []
  type: TYPE_NORMAL
- en: The LLMs that Copilot is allowed to access from the set of all LLMs it works
    with; a list of models currently available is included in the [Copilot documentation](https://oreil.ly/icFQK).
  prefs: []
  type: TYPE_NORMAL
- en: Premium requests
  prefs: []
  type: TYPE_NORMAL
- en: Interactions that use advanced AI models with operations such as Copilot Chat,
    Agent mode, code review, or extensions. These consume more compute resources and
    are counted separately from standard code completions.
  prefs: []
  type: TYPE_NORMAL
- en: With those terms in mind, [Table 1-2](#comparison-of-github) explains the available
    plans and the key differences between them.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-2\. Comparison of GitHub Copilot plans
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Free | Pro | Pro+ | Business | Enterprise |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Target users | Individual developers looking to explore Copilot | Individual
    developers who want unlimited access to Copilot but don’t need to use all models
    | Individual developers who want maximum flexibility and access to all models
    | Organizations or enterprises that need unlimited access to core Copilot functionality
    and business management features but not all models | Organizations or enterprises
    that need unlimited access to core Copilot functionality and business management
    features with maximum flexibility and access to all models |'
  prefs: []
  type: TYPE_TB
- en: '| Coding completions, chat requests, and Agent mode requests | 50 chat requests
    or Agent mode requests/month (counted as premium)2,000 code completion requests/month
    | Unlimited | Unlimited | Unlimited | Unlimited |'
  prefs: []
  type: TYPE_TB
- en: '| Model access | Access to a small subset of models | Access to more models
    | Access to all models | Access to more models | Access to all models |'
  prefs: []
  type: TYPE_TB
- en: '| Premium requests | 50 premium requests/month | 300 premium requests/month
    | 1,500 premium requests/month | 300 premium requests/month | 1,000 premium requests/month
    |'
  prefs: []
  type: TYPE_TB
- en: '| Additional features | Limited code review for selections in VS Code |'
  prefs: []
  type: TYPE_TB
- en: Code review
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pull request summaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Code review
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pull request summaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Code review
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pull request summaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User management and metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP indemnity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Code review
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pull request summaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User management and metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP indemnity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Administration | Individual | Individual | Individual | Enterprise | Enterprise
    |'
  prefs: []
  type: TYPE_TB
- en: '| Cost | Free | $10/month or $100/year (free trial available for 30 days) |
    $39/month or $390/year | $19/month/user | $39/month/user |'
  prefs: []
  type: TYPE_TB
- en: Latest Info
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The information in [Table 1-2](#comparison-of-github) is accurate as of the
    time of this writing. Consult [the Copilot features page](https://oreil.ly/ptoVy)
    for the latest information.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, cost is a significant factor. If you are an individual user who only
    uses Copilot on a limited basis in your IDE and doesn’t need the extra features,
    the Free plan may make sense. If you’re an individual user who wants to use Copilot
    on a regular basis and can benefit from the additional features, the Pro or Pro+
    plans can provide a good match.
  prefs: []
  type: TYPE_NORMAL
- en: At a corporate or community level, if you need or want the extra ability to
    easily assign and manage licenses across multiple users, as well as additional
    administrative oversight, the Business subscription can provide that. And if,
    as a business or community, you you want to take full advantage of advanced features
    and have maximum access to models, then the Enterprise plan is your best option.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing Plans
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is possible to mix plan types within an enterprise at the organization level.
    See the [documentation](https://oreil.ly/BZ1p6).
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of the plan you choose, the steps are similar:'
  prefs: []
  type: TYPE_NORMAL
- en: Sign up and register for license(s).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish a payment process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install Copilot via an extension or whatever process you use to add functionality
    in your IDE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At an organization level, once the plan is set up, organization admins can add
    users and manage licenses as shown in [Figure 1-6](#managing-a-copilot-or).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/lghc_0106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-6\. Managing a Copilot Business plan
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once you gain access to a Copilot subscription, you can install it into whichever
    IDE you use. [The GitHub Copilot documentation](https://oreil.ly/kF4rn) provides
    links for installing the GitHub Copilot extension in your chosen IDE. As of this
    writing, the currently supported IDEs include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Azure Data Studio](https://oreil.ly/uUNGn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Eclipse](https://oreil.ly/Mqdq2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[JetBrains IDEs](https://oreil.ly/dlYdh)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vim/Neovim](https://oreil.ly/rA6oI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Visual Studio](https://oreil.ly/qYbh_)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Visual Studio Code](https://oreil.ly/GYc9X)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Xcode](https://oreil.ly/MGeCb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub Copilot Language Server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GitHub also provides the [Copilot Language Server SDK](https://oreil.ly/008MF).
    This SDK allows integrating Copilot with any editor or IDE that can use the Language
    Server Protocol (LSP) standard. So expect to see more integrations being developed.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope you’re starting to have a better sense of what GitHub Copilot is all
    about and how it potentially can be used. I also hope you’re finding yourself
    intrigued to learn and understand more about it.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I’ve provided an overview of what Copilot is, how it works,
    some key considerations to keep in mind when using it, and how to understand the
    plans it provides. This paints a picture of how Copilot fits in with the current
    use and potential of AI to help you in a given domain. In this case, that domain
    is creating software, and more generally, coding.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few key points to take away from this text. One is that AI can greatly
    simplify and support your efforts as a coder. A second is that context is king
    when it comes to helping Copilot provide you with the best results. And a third,
    and arguably the most important one, is that you still have the ultimate responsibility
    to review and assess any suggestions and answers from the AI. Copilot is great
    at what it does, but it is only as good as the context we give it and the capabilities
    and training data of the model it is using. And as with human coders, those variables
    factor in to how complete, relevant, and accurate (or not) any result is.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of standard ways of leveraging Copilot to complete code.
    In the next chapter, you’ll look at how to use and work with Copilot code completions
    in the IDE interfaces where you are coding. Understanding those will help get
    you to the next level of using the tool, completing the foundation you need for
    the rest of the book, and getting the most out of Copilot.
  prefs: []
  type: TYPE_NORMAL
