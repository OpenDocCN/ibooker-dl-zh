- en: Chapter 2\. How AI Coding Technology Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we‚Äôll crack open the hood of AI-assisted programming tools
    and take a peek at what makes them tick. We‚Äôll briefly wade through the history,
    take a whirl with transformer models and LLMs, and demo the OpenAI Playground.
    Then we‚Äôll get some advice on how to evaluate LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Grasping what this powerful technology can and can‚Äôt do will pave the way for
    smarter use of AI-assisted programming tools for real-world software projects.
  prefs: []
  type: TYPE_NORMAL
- en: Key Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The market has been buzzing about AI-assisted programming tools such as GitHub
    Copilot, Tabnine, CodiumAI, and Amazon CodeWhisperer. The makers of each product
    attempt to flaunt their own set of bells and whistles. But there‚Äôs a good chunk
    of capabilities these tools share. [Table¬†2-1](#common_functions_of_aihyphenassisted_pro)
    summarizes some of the main features.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Common functions of AI-assisted programming tools
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Code suggestions | Provides code suggestions based on comments and file context;
    recommends individual lines or whole functions. |'
  prefs: []
  type: TYPE_TB
- en: '| Context-aware completions | Offers context-aware code completions based on
    all or a part of the code base, as well as suggestions to aid in coding. |'
  prefs: []
  type: TYPE_TB
- en: '| Test generation | Analyzes code to generate meaningful tests, map code behaviors,
    and surface edge cases to ensure software reliability before shipping. |'
  prefs: []
  type: TYPE_TB
- en: '| User‚ÄìIDE interaction | Automatically activates and provides guidance as users
    type code in the IDE; users can interact with the code through chat. |'
  prefs: []
  type: TYPE_TB
- en: '| Code analysis | Analyzes code snippets, docstrings, and comments to provide
    reliable code predictions and tag suspicious code. |'
  prefs: []
  type: TYPE_TB
- en: '| Bug detection and fixing | Identifies potential bugs in code and suggests
    ways to fix them. |'
  prefs: []
  type: TYPE_TB
- en: '| Code autodocumentation | Automatically adds docstrings and enhances code
    documentation. |'
  prefs: []
  type: TYPE_TB
- en: '| Routine task automation | Helps with code creation for routine or time-consuming
    tasks, unfamiliar APIs or SDKs, and other common coding scenarios like file operations
    and image processing. |'
  prefs: []
  type: TYPE_TB
- en: '| API and SDK usage optimization | Aids in making correct and effective use
    of APIs and SDKs. |'
  prefs: []
  type: TYPE_TB
- en: '| Open source discovery and attribution | Facilitates discovery and attribution
    of open source code and libraries. |'
  prefs: []
  type: TYPE_TB
- en: The list in [Table¬†2-1](#common_functions_of_aihyphenassisted_pro) isn‚Äôt the
    be-all and end-all; innovation has been moving at a rapid clip. Clearly, these
    systems can give developers a big leg up, in large part by providing code suggestions
    and context-aware completions. We‚Äôll take a closer look at these in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Code Suggestions and Context-Aware Completions Versus Smart Code Completion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The magic of *smart code completion*, also known as autocompletion or Microsoft‚Äôs
    term IntelliSense, is something many IDEs bring to the table. They lend developers
    a hand by suggesting, filling in, and spotlighting bits of code as the humans
    hammer away at the keyboard. This technology has actually been around since the
    late 1950s with the inception of spellcheckers.
  prefs: []
  type: TYPE_NORMAL
- en: The breakthrough came in the mid-1990s. Microsoft‚Äôs Microsoft Visual Basic 5.0
    provided real-time suggestions and completions, with an emphasis on basic syntax
    and function signatures. This greatly improved productivity and reduced errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'So you might be wondering: How does something like IntelliSense stack up against
    AI-assisted programming tools? After all, IntelliSense has a smattering of AI
    and machine learning under its belt.'
  prefs: []
  type: TYPE_NORMAL
- en: Yet, there‚Äôs an important distinction to be made. AI-assisted tools are powered
    by generative AI. They serve up not just code but a buffet of documentation, planning
    documents, and helpful guides among other things. Thanks to generative AI, these
    tools get the knack of churning out, tweaking, and understanding human-like text
    based on the given context, making them champs at translation, summarization,
    text analytics, topic modeling, and answering queries. Engaging with these tools
    can sometimes be like having a casual chat with your code. With an LLM at their
    core, they can catch the drift of the context and intent from your input.
  prefs: []
  type: TYPE_NORMAL
- en: Compilers Versus AI-Assisted Programming Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get a better understanding of AI-assisted programming tools, it helps to
    understand what compilers do. Here are the main steps that a compiler performs:'
  prefs: []
  type: TYPE_NORMAL
- en: Lexical analysis (tokenization)
  prefs: []
  type: TYPE_NORMAL
- en: The compiler acts like a language teacher, breaking your code into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax analysis
  prefs: []
  type: TYPE_NORMAL
- en: Here, the compiler checks how your tokens are grouped. It makes sure your coding
    has the right structure, not just the right commands.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic analysis (error checks)
  prefs: []
  type: TYPE_NORMAL
- en: The compiler ensures that your code makes sense in the context of the programming
    language. It‚Äôs not just about correct syntax. It‚Äôs about correct meaning, too.
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate code generation
  prefs: []
  type: TYPE_NORMAL
- en: This is where your code starts its transformation journey. The compiler translates
    your high-level code into an intermediate form. It‚Äôs not quite machine language
    yet, but it‚Äôs getting there.
  prefs: []
  type: TYPE_NORMAL
- en: Code optimization
  prefs: []
  type: TYPE_NORMAL
- en: In this step, the compiler is like a personal trainer for your code, making
    it leaner and more efficient. It tweaks the intermediate code to run faster and
    take up less space.
  prefs: []
  type: TYPE_NORMAL
- en: Code generation
  prefs: []
  type: TYPE_NORMAL
- en: This is the final transformation. The compiler converts the optimized intermediate
    code into machine code or assembly language that your CPU can understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linking and loading:'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes considered a part of the compilation process, *linking* involves combining
    various pieces of code and libraries into a single executable program. *Loading*
    is the process of placing the program into memory for execution.
  prefs: []
  type: TYPE_NORMAL
- en: As for AI-assisted programming tools like Copilot, they‚Äôre a different beast.
    They don‚Äôt really ‚Äúget‚Äù programming languages like compilers do. This is fine.
    The compiler does this. Instead, they use AI to guess and suggest bits of code
    based on tons of code that‚Äôs already out there. Since the tools are playing the
    odds, the suggestions can vary a lot. The compiler will then take this code and
    make it so the machine can run the program.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, AI tools might miss something simple like a bracket, which a human
    coder or a compiler would spot in a heartbeat. That‚Äôs because the LLMs are based
    on predicting patterns, not a compiler engine. If something‚Äôs not common in the
    training, they might not catch it. Also, these tools might get fancy and suggest
    complex code based on the situation. Yes, AI-assisted programming tools can get
    carried away.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to spotting errors, AI-assisted programming tools are generally
    effective but still do not quite match up to a compiler‚Äôs ninja-like error-checking
    skills. Yet the tools are still powerful. For example, they can help catch pesky
    syntax errors‚Äîmissing semicolons, typos in function names, mismatched brackets‚Äîand
    swiftly suggest the right fix. They also shine in steering you clear of common
    coding pitfalls. Whether it‚Äôs reminding you to properly close a file after opening
    it or suggesting more efficient ways to loop through an array, this tool has your
    back. And when it comes to logical errors, AI-assisted programming tools can be
    surprisingly insightful. They may not solve every complex problem, but they can
    often propose alternative approaches or solutions you might not have considered,
    nudging your problem-solving journey in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: This all means that while AI tools are helpful for making coding smoother, they‚Äôre
    not a replacement for the thorough checks a compiler does or the keen eye of a
    human coder.
  prefs: []
  type: TYPE_NORMAL
- en: These drawbacks really underline how crucial it is to blend the smarts of AI-assisted
    tools with the thoroughness of compiler checks and a human touch. After all, you
    want to make sure your code is not just good but spot-on accurate and correct.
  prefs: []
  type: TYPE_NORMAL
- en: Levels of Capability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In October 2023, Quinn Slack, the CEO and cofounder of Sourcegraph, shared an
    insightful [blog post](https://oreil.ly/SoDqc). He dived into the world of AI-assisted
    programming tools like Copilot and came up with an interesting way to think about
    them, which he called ‚Äúlevels of code AI.‚Äù His step-by-step framework makes it
    easier for everyone to get what these AI tools can do and check if the boastful
    claims by the companies selling them actually hold water. [Figure¬†2-1](#programming_systems_have_different_level)
    shows the levels of code.
  prefs: []
  type: TYPE_NORMAL
- en: The first three levels focus on human-led coding, where the developer is the
    main player. Starting off, Level 0 is where there is no AI assistance, which is
    old-school coding. Developers do everything by hand with no AI in sight. It‚Äôs
    the baseline that sets the stage for AI to step in later on.
  prefs: []
  type: TYPE_NORMAL
- en: Then there‚Äôs Level 1, code completion. This is where AI starts to chip in and
    helps to whip up single lines or chunks of code based on what‚Äôs going on around
    it. At this stage, the developer is still in the driver‚Äôs seat, directing the
    overall program and using AI as a shortcut for typical coding tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiap_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Programming systems have different levels of AI capability
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Level 2, code creation, ramps up the AI. Here, it gets more hands-on and crafts
    longer code sections. The AI can, for example, design APIs and even fix existing
    code. Of course, it‚Äôs all happening with a human keeping an eye on things. This
    level needs the AI to get the codebase and the context around it so it can come
    up with code that‚Äôs not just correct but also fits in nicely.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with Level 3, supervised automation, we see a shift toward AI taking
    the lead in coding. In this stage, the AI tackles several tasks to meet broader
    goals set by humans, and it doesn‚Äôt need a check-in at every turn. Working at
    this level is like delegating work to a junior developer. The AI at this level
    is savvy enough to sort out bugs, toss in new features, and mesh systems together,
    reaching out to its human counterpart for any clarifications along the way.
  prefs: []
  type: TYPE_NORMAL
- en: At Level 4, full automation, the AI really steps up its game. Here, it handles
    complex tasks all on its own, without needing humans to give the final thumbs-up
    on the code. Imagine the trust you‚Äôd have in a top-notch engineer if you were
    a CEO or product manager. This is the kind of relationship this level aims for.
    The AI isn‚Äôt just reacting. It‚Äôs proactively keeping an eye on the code, spotting
    and sorting out issues as they come up.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there‚Äôs Level 5, AI-led full autonomy. This level is a whole different
    ball game, where AI isn‚Äôt just following human instructions but setting its own
    objectives. It‚Äôs about AI working off a core reward function. Think of it as playing
    its own game in a world where it faces off against other agents. Sure, this level
    sounds a bit like sci-fi, but given how fast things are moving, it‚Äôs not too wild
    to think we might see this level become a reality in our lifetimes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right now, tools like Copilot are hovering around Level 3, give or take. Pinning
    down the exact level can be tricky, but Quinn Slack‚Äôs framework does a pretty
    solid job of making sense of the technology and its key interactions. And one
    thing‚Äôs for sure: the technology isn‚Äôt slowing down‚Äîit‚Äôs moving forward really
    fast.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI and Large Language Models (LLMs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using AI-assisted programming tools doesn‚Äôt require you to be a whiz at the
    nitty-gritty of generative AI technology. However, having a bird‚Äôs-eye view of
    the technology can be quite handy. You‚Äôll be able to evaluate the responses, capabilities,
    and limitations of these tools in a sharper way.
  prefs: []
  type: TYPE_NORMAL
- en: '*Transparency* isn‚Äôt just a buzzword here. For a new technology to really catch
    on, having a clear picture of what‚Äôs under the hood is crucial. Adoption is all
    about trust. In the coding world, reliability and accountability aren‚Äôt just fancy
    extras‚Äîthey‚Äôre the bread and butter.'
  prefs: []
  type: TYPE_NORMAL
- en: As we venture into the upcoming sections, we‚Äôll skim the surface of generative
    AI and LLMs to give you a clearer picture.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The story of generative AI has its roots stretching back several decades, with
    one of its earliest examples being ELIZA, the pioneering chatbot brought to life
    by Massachusetts Institute of Technology professor Joseph Weizenbaum in the mid-60s.
    ELIZA was crafted to mimic chats with a psychotherapist ([you can still find it
    online](https://oreil.ly/MbLP8)). Sure, it was basic, running on a rule-based
    algorithm and mostly parroting back user input.
  prefs: []
  type: TYPE_NORMAL
- en: Yet many folks found ELIZA more pleasant to chat with than a real therapist,
    and some were even fooled into thinking they were communicating with a human.
    This curious occurrence, dubbed the ‚ÄúELIZA effect,‚Äù showcased how easily people
    can imagine human-like understanding on the part of a computer program.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the journey of generative AI wasn‚Äôt exactly a sprint. The tech gears
    at its core were quite basic, and progress was more of a slow crawl. But come
    the 2010s, the scene hit a turning point. The technology world was now boasting
    hefty compute power, flashy hardware systems like GPUs (graphics processing units),
    a treasure trove of data, and the fine-tuning of sophisticated models like deep
    learning. And just like that, generative AI was back in the fast lane. As it developed,
    different methods emerged:'
  prefs: []
  type: TYPE_NORMAL
- en: Variational autoencoders (VAEs)
  prefs: []
  type: TYPE_NORMAL
- en: This technology made its debut in 2013, thanks to Diederik P. Kingma and Max
    Welling and their paper [‚ÄúAuto-Encoding Variational Bayes‚Äù](https://arxiv.org/abs/1312.6114).
    Their VAE model consists of lower-dimensional latent space from more complex,
    higher-dimensional data, all without supervision. It also includes an encoder‚Äìdecoder
    structure. When we say *higher-dimensional data*, we‚Äôre talking about data with
    many features, each being a dimension‚Äîthink of a 28 √ó 28 pixel image in a 784-dimension
    space. The lower-dimensional latent space is like a compact version of this data,
    holding onto the crucial information while shedding the extra dimensions. This
    is important because it lightens the computational load, fights off the curse
    of dimensionality, and makes the data easier to visualize and interpret. This
    leap from a higher- to a lower-dimensional space is called *dimensionality reduction*,
    and it simplifies the data to its bare essentials. Unlike their cousins, the traditional
    autoencoders, that spit out a single value for each latent attribute, the encoder
    in a VAE gives you a probability distribution. The decoder then picks samples
    from this distribution to rebuild the data. This neat trick of offering a range
    of data in the latent space rather than a single value opens the door to create
    new data or images.
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks (GANs)
  prefs: []
  type: TYPE_NORMAL
- en: Introduced by [Ian Goodfellow and his colleagues in 2014](https://arxiv.org/abs/1406.2661),
    generative adversarial networks are a class of AI algorithms used in unsupervised
    machine learning. At the heart of GANs are two neural networks, dubbed the *generator*
    and the *discriminator*, that go head-to-head in a game-like showdown. The generator
    churns out new data nuggets, while the discriminator plays the judge, distinguishing
    the real from the fake data. With each round, the generator ups its game, crafting
    data that‚Äôs eerily similar to real instances. This clever setup has swung open
    doors to new possibilities, leading to AI that creates realistic images, voice
    recordings, and a whole lot more.
  prefs: []
  type: TYPE_NORMAL
- en: These types of generative AI would be important building blocks for the transformer
    model, a real breakthrough that has made the power of LLMs a reality.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before transformers made a splash, the go-to method for natural language processing
    (NLP) was the recurrent neural network (RNN). RNNs were crafted to tackle sequential
    or time-series data. They would keep tabs on a hidden state to remember bits from
    previous steps in a sequence‚Äîa handy feature for things like language modeling,
    speech recognition, and sentiment analysis. The RNNs take it step-by-step, processing
    one piece of the sequence at a time, updating their hidden state based on the
    current input and what‚Äôs been processed before‚Äîhence the term *recurrent*. But
    they hit a snag when faced with long sequences, getting tripped up by the vanishing
    or exploding gradient problem. This made it hard for them to keep track of long-term
    relationships in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Enter the transformer, flipping the script entirely. Instead of taking the step-by-step
    approach of RNNs, transformers breeze through data in parallel and tap into attention
    mechanisms to keep tabs on relationships between different bits in the input sequence,
    no matter where they‚Äôre placed. This switch in the architectural blueprint lets
    transformers handle both short and long sequences with ease. It also sidesteps
    the gradient woes. Plus, their parallel processing capabilities mesh nicely with
    sophisticated chip architectures like graphics processing units (GPUs) or tensor
    processing units (TPUs).
  prefs: []
  type: TYPE_NORMAL
- en: Ashish Vaswani and his fellow researchers at Google created the transformer
    and published the core architecture in the pathbreaking paper [‚ÄúAttention Is All
    You Need‚Äù](https://arxiv.org/abs/1706.03762) in 2017\. [Figure¬†2-2](#the_architecture_of_the_transformer_mode)
    illustrates the main parts of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The transformer model is like a brilliant linguist, adept at unraveling the
    intricacies of language. Its magic unfolds in two primary stages: encoding and
    decoding. Each is composed of its own set of layers. During the *encoding* stage,
    the model reads and comprehends the input text similar to how a linguist would
    understand a sentence in a foreign language. Then in the *decoding* stage, the
    model generates a new piece of text or translation based on the understanding
    acquired in the encoding stage, much like a linguist translating that sentence
    into your native language.'
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of the transformer is a mechanism called *attention*, which allows
    it to assess the relevance of each word in a sentence to the other words. It assigns
    an attention score to each. For example, take the sentence ‚ÄúThe cat sat on the
    mat.‚Äù When the model focuses on the word *sat*, the words *cat* and *mat* might
    receive higher attention scores due to their direct relationship to the action
    of sitting.
  prefs: []
  type: TYPE_NORMAL
- en: One notable feature of this model is the *self-attention mechanism*. This allows
    it to look at an entire sentence, understand the relationships between words,
    and retain these relationships over long stretches of text. This grants the transformer
    a form of long-term memory by enabling it to focus on all the words or *tokens*
    (whole words or parts of a word) that have appeared so far, thereby understanding
    the broader context.
  prefs: []
  type: TYPE_NORMAL
- en: However, despite these capabilities, the transformer initially lacks the ability
    to recognize the order of words in a sentence, which is crucial for understanding
    the meaning. Here, *positional encoding* steps in. It acts like a GPS to provide
    the model with the information about the position of each word within the sentence
    and aids in making sense of clauses like ‚ÄúThe cat chases the mouse‚Äù versus ‚ÄúThe
    mouse chases the cat.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiap_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. The architecture of the transformer model is at the heart of LLMs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Adding to the sophistication, the transformer employs a *multi-head attention
    mechanism*. Envision the model having multiple pairs of eyes, each pair examining
    the sentence from a unique angle and focusing on different aspects or relationships
    between the words. For instance, one pair might focus on understanding actions,
    another on identifying characters, and yet another on recognizing locations. This
    multi-view approach enables the transformer to grasp a richer understanding of
    the text.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, each stage of the transformer encompasses layers of a *feedforward
    neural network*, a straightforward network that aids in processing relationships
    between words. This further enhances the understanding and generation of text.
  prefs: []
  type: TYPE_NORMAL
- en: A transformer is in the form of a pretrained model. It has already been trained
    on an enormous amount of data and is ready for use or further fine-tuning. Once
    pretrained, the model can be accessed as an API, allowing for its immediate use
    in various language-processing tasks. Companies or individuals can rapidly integrate
    this model into their systems, such as AI-assisted programming applications. Moreover,
    the pretrained LLM can be further honed to excel in specialized domains, such
    as medical or legal text analysis, by fine-tuning it on domain-specific data.
    This eliminates the need for developing a complex language model from the ground
    up, saving a substantial amount of time, effort, and resources. The pretrained
    model, with its foundational language understanding, acts as a springboard for
    the development of generative AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Building and operating an LLM is costly. During early 2023, GitHub Copilot was
    [losing an average of more than $20 a month per user](https://oreil.ly/D2NiB),
    according to the *Wall Street Journal*. In some cases, some users were losing
    the company $80 per month. However, as the infrastructure is scaled for generative
    AI in the coming years, per-user costs should decrease.
  prefs: []
  type: TYPE_NORMAL
- en: The two main types of transformer systems are *generative pretrained transformer*
    (GPT) and *bidirectional encoder representations from transformers* (BERT). GPT
    is a tool from OpenAI that is ideal for creating text, summarizing information,
    and translating languages. It is based on an autoregressive LLM architecture.
    This means that it crafts text by carefully considering each word based on what
    it‚Äôs already output, much like a storyteller building a narrative one word at
    a time. Its skills come from being trained on a colossal amount of text data.
    GPT uses the decoder for generating content.
  prefs: []
  type: TYPE_NORMAL
- en: BERT, on the other hand, uses an autoencoding approach. This design enables
    it to deeply understand the context of words in a sentence, making it adept at
    deciphering the nuances and meanings of language. Google developed BERT in 2018
    as an open source project. Since then, many variations and enhancements to the
    core model have emerged.
  prefs: []
  type: TYPE_NORMAL
- en: As for AI-assisted programming applications, the main type of transformer model
    is GPT. It has been shown to predict and autocomplete code efficiently, based
    on the context provided by the programmer.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Playground
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [OpenAI Playground](https://platform.openai.com) is a generative AI sandbox
    that provides access to various models developed by OpenAI. It allows for model
    customization via an intuitive graphical interface.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI Playground makes it easier to understand the strengths and weaknesses
    of the various LLMs. Moreover, it enables real-time testing and adjustments of
    models in response to different inputs, like temperature.
  prefs: []
  type: TYPE_NORMAL
- en: However, OpenAI charges for use of the platform. Fees are based on the number
    of tokens used, as seen in [Table¬†2-2](#the_costs_of_openai_llms). Keep in mind
    that prices change periodically. The good news is that all changes as of this
    writing have been reductions in price.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2\. The costs of OpenAI LLMs
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Input | Output |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4/8K context | $0.03/1K tokens | $0.06/1K tokens |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4/32K context | $0.06/1K tokens | $0.12/1K tokens |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo/4K context | $0.0015/1K tokens | $0.002/1K tokens |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo/16K context | $0.003/1K tokens | $0.004/1K tokens |'
  prefs: []
  type: TYPE_TB
- en: For example, suppose you are using the GPT-4/8K context LLM. You have a prompt
    with 1,000 tokens, and the response to this from the model is 2,000 tokens. Then
    the cost will be 3 cents for the input and 12 cents for the output.
  prefs: []
  type: TYPE_NORMAL
- en: When you first sign up for an OpenAI account, you will get a $5 credit that
    can be used for the OpenAI Playground. This can be used for calls to the API.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs take a more detailed look at tokens. OpenAI has a tool called the [Tokenizer](https://platform.openai.com/tokenizer),
    shown in [Figure¬†2-3](#the_openai_tokenizer_displays_the_tokens) where I have
    entered the following for analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input:* ChatGPT is unbelievable! üéâ I love it.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](assets/aiap_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. The OpenAI Tokenizer displays the tokens for an excerpt of text
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the tokenization‚Äîwhich is highlighted with colors‚Äîthe word *ChatGPT* is composed
    of three tokens. The breakdown is Chat, G, and PT. The word *unbelievable* and
    its following exclamation point have two tokens, one for the word and one for
    the punctuation. As for the emoji, it consists of three tokens. Each punctuation
    mark is a token. Spaces are included with an adjacent word.
  prefs: []
  type: TYPE_NORMAL
- en: The Tokenizer is for GPT-3, GPT-3.5, and GPT-4\. Keep in mind that tokenization
    is often different among the LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a rule of thumb, 1,000 tokens is roughly equivalent to 750 words.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Platform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you go to the OpenAI Playground, you get access to a dashboard, shown in
    [Figure¬†2-4](#the_openai_playground_has_a_dashboard_wi).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiap_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. The OpenAI Playground has a dashboard with tips, resources, and
    interaction areas
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The middle of the screen has the main workflow for the interactions with an
    LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: System
  prefs: []
  type: TYPE_NORMAL
- en: This is where you provide some context for the LLM, for example, ‚ÄúYou are an
    expert in Python programming.‚Äù The system prompt is the first message in a session
    and sets the stage for the interaction. Customizing the system prompt allows for
    greater control over how the model behaves in the conversation, which can be particularly
    useful to ensure that it stays within desired parameters or contexts.
  prefs: []
  type: TYPE_NORMAL
- en: User
  prefs: []
  type: TYPE_NORMAL
- en: This is the main instruction of the prompt. For example, this is where you can
    ask the LLM to carry out a coding task.
  prefs: []
  type: TYPE_NORMAL
- en: Add message
  prefs: []
  type: TYPE_NORMAL
- en: This allows you to have an ongoing chat with the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs try an example. Suppose you‚Äôre working on a Python project and you‚Äôre
    having trouble understanding how to implement the Tkinter library to get user
    input. You can enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*System message:* You are a Python expert specialized in Tkinter.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*User message:* I want to create a simple GUI using Tkinter to get a user‚Äôs
    name and age. How can I do that?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The LLM will generate the code listing. But suppose you want to add validation
    for the input. You can press the Add button and enter ‚ÄúHow can I ensure the age
    entered is a number and not text?‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: The LLM will respond with the code for this, using a `try-except` block to convert
    the age input to an integer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Granted, this is like using ChatGPT‚Äîbut with more structure. Also, the real
    power is the ability for customization. You‚Äôll find these features on the right
    side of the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs: []
  type: TYPE_NORMAL
- en: You can select from a variety of models and can even use your own fine-tuned
    LLMs to ensure the model is focused on the unique needs of your coding. You can
    find more information about fine-tuning a model in the [OpenAI API documentation](https://oreil.ly/L3y09).
  prefs: []
  type: TYPE_NORMAL
- en: Temperature
  prefs: []
  type: TYPE_NORMAL
- en: This adjusts the randomness or creativity of the generated content. The range
    of values is from 0 to 2\. The lower the value, the more deterministic and focused
    are the responses. [Table¬†2-3](#suggested_temperature_levels_for_certain) shows
    suggested temperature levels for different types of development tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-3\. Suggested temperature levels for certain types of programming tasks
  prefs: []
  type: TYPE_NORMAL
- en: '| Task category | Temperature value | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Code generation | 0.2‚Äì0.3 | Ensures more deterministic, accurate code adhering
    to common conventions for reliable and understandable outcomes. |'
  prefs: []
  type: TYPE_TB
- en: '| Code review | 0.2 or less | Focuses on well-established best practices and
    standards for precise feedback. |'
  prefs: []
  type: TYPE_TB
- en: '| Bug fixing | 0.2 or less | Produces more accurate and straightforward solutions
    to identified issues. |'
  prefs: []
  type: TYPE_TB
- en: '| Creative problem solving | 0.7‚Äì1.0 | Explores a broader range of possible
    solutions, useful in brainstorming or innovative problem solving. |'
  prefs: []
  type: TYPE_TB
- en: '| Learning and experimentation | 0.7‚Äì1.0 | Provides a wider variety of examples
    and solutions for understanding different approaches to problem solving. |'
  prefs: []
  type: TYPE_TB
- en: '| Data analysis and visualization | 0.2 or less | Generates accurate and meaningful
    visualizations or analyses. |'
  prefs: []
  type: TYPE_TB
- en: '| Optimization tasks | Varied | Permits striking a balance between exploration
    (higher temperature) and exploitation (lower temperature) for efficient solutions.
    |'
  prefs: []
  type: TYPE_TB
- en: 'However, if you use a fairly high value for the temperature, the results can
    be nonsensical. Here‚Äôs a sample prompt when using a value of 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* In Python, what are the steps to migrate data from a CSV file to
    a MySQL database?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Figure¬†2-5](#when_using_a_temperature_of_twocomma_the) shows the output. As
    you can see, this makes little sense!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiap_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. When using a temperature of 2, the LLM‚Äôs results are mostly nonsensical
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs look at the other features you can adjust:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum length
  prefs: []
  type: TYPE_NORMAL
- en: This is the maximum number of tokens to use to generate content. The number
    includes usage for both the prompt and response. The ratio of tokens to content
    depends on the model you use.
  prefs: []
  type: TYPE_NORMAL
- en: Stop sequence
  prefs: []
  type: TYPE_NORMAL
- en: This indicates a point at which the LLM should stop creating further text. You
    can specify a particular string or sequence of characters that, when detected
    in the generated text, will signal the model to halt the process.
  prefs: []
  type: TYPE_NORMAL
- en: Top p
  prefs: []
  type: TYPE_NORMAL
- en: Also known as nucleus sampling, this technique selects words based on a cumulative
    probability threshold, denoted by *p*, which can range from 0 to 1\. In simpler
    terms, instead of always choosing from the top few most likely next words, the
    model considers a broader or narrower range of possible next words based on the
    specified *p*-value. A lower *p*-value results in a smaller, more focused set
    of words to choose from, leading to more predictable and coherent text. A higher
    *p*-value, on the other hand, allows for a wider set of possible next words, leading
    to more diverse and creative text generation.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency penalty
  prefs: []
  type: TYPE_NORMAL
- en: This helps to tackle a common problem with LLMs, which is repetitive phrases
    or sentences. The value ranges from 0 to 2\. The higher the value, the less repetition.
    However, at values greater than 1, text generation can get unpredictable and even
    nonsensical.
  prefs: []
  type: TYPE_NORMAL
- en: Presence penalty
  prefs: []
  type: TYPE_NORMAL
- en: This also has a value of 0 to 2\. A higher value will allow the LLM to include
    a wider variety of tokens, which means using a more diverse vocabulary or broader
    universe of concepts.
  prefs: []
  type: TYPE_NORMAL
- en: With the frequency penalty, presence penalty, and top *p*, OpenAI recommends
    selecting one approach to adjust for your task. But don‚Äôt shy away from experimentation.
    The path to optimizing LLMs isn‚Äôt paved with strict rules, thanks to the intricate
    dance of the complexities involved.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assessing LLMs is a hefty task. These behemoths are often so opaque that they
    can seem impossible to understand. The competition among AI firms only worsens
    this. It‚Äôs become par for the course to see scant details on the datasets these
    models are trained on, the number of parameters used to fine-tune their behavior,
    and the hardware that powers them.
  prefs: []
  type: TYPE_NORMAL
- en: But there is some good news, thanks to some researchers at Stanford. They‚Äôve
    created a [scoring system](https://oreil.ly/FoVAr) dubbed the Foundation Model
    Transparency Index to size up the openness of LLMs. This yardstick, shaped by
    a hundred criteria, is a bid to usher some clarity into the murky waters of LLM
    transparency.
  prefs: []
  type: TYPE_NORMAL
- en: The ranking is based on a percentage scale. [Table¬†2-4](#rankings_of_top_llms_in_terms_of_transpa)
    shows the rankings. Unfortunately, the results are far from encouraging. No major
    LLM is close to achieving ‚Äúadequate transparency,‚Äù according to the researchers,
    and the mean score is only 37%.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-4\. Rankings of top LLMs in terms of transparency of their models^([a](ch02.html#id399))
  prefs: []
  type: TYPE_NORMAL
- en: '| Company | Model | Rank |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Meta | LLaMA 2 | 54% |'
  prefs: []
  type: TYPE_TB
- en: '| BigScience | BLOOMZ | 53% |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI | GPT-4 | 48% |'
  prefs: []
  type: TYPE_TB
- en: '| Stability.ai | Stable Diffusion 2 | 47% |'
  prefs: []
  type: TYPE_TB
- en: '| Google | PaLM 2 | 40% |'
  prefs: []
  type: TYPE_TB
- en: '| Anthropic | Claude 2 | 36% |'
  prefs: []
  type: TYPE_TB
- en: '| Cohere | Command | 34% |'
  prefs: []
  type: TYPE_TB
- en: '| AI21Labs | Jurassic-2 | 25% |'
  prefs: []
  type: TYPE_TB
- en: '| Inflection | Inflection-1 | 21% |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon | Titan Text | 12% |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch02.html#id399-marker)) Center for Research on Foundation Models,
    Foundation Model Transparency Index Total Scores 2023, [*https://crfm.stanford.edu/fmti*](https://crfm.stanford.edu/fmti)
    |'
  prefs: []
  type: TYPE_TB
- en: The flexibility of LLMs to handle various domains and tasks, such as software
    development, is a notable advantage. However, it also complicates the evaluation
    process, as it requires domain-specific evaluation metrics and benchmarks to ensure
    the model‚Äôs effectiveness and safety in each particular application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite all this, there are some metrics to consider when evaluating LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: BERTScore
  prefs: []
  type: TYPE_NORMAL
- en: This metric is designed to evaluate text generation models by comparing generated
    text to reference text using BERT embeddings. Although primarily used for natural
    language text, it can be extended or adapted for code generation tasks, especially
    when the code is annotated or commented in natural language.
  prefs: []
  type: TYPE_NORMAL
- en: Perplexity
  prefs: []
  type: TYPE_NORMAL
- en: This is a common metric for evaluating probabilistic models like LLMs. It quantifies
    how well the probability distribution predicted by the model aligns with the actual
    distribution of the data. In the context of code generation, lower perplexity
    values indicate that the model is better at predicting the next token in a sequence
    of code.
  prefs: []
  type: TYPE_NORMAL
- en: BLEU (bilingual evaluation understudy)
  prefs: []
  type: TYPE_NORMAL
- en: Originally developed for machine translation, BLEU is also used in code generation
    to compare the generated code with reference code. It computes *n*-gram precision
    scores to quantify the similarity between the generated and reference texts, which
    can help in evaluating the syntactic correctness of the generated code. A higher
    *n*-gram precision score indicates better agreement between the generated and
    reference text for that specific sequence of *n* words.
  prefs: []
  type: TYPE_NORMAL
- en: ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
  prefs: []
  type: TYPE_NORMAL
- en: This is another metric borrowed from NLP that can be used to evaluate code generation
    models. It calculates the overlap of *n*-grams between the generated and reference
    texts, providing insights into how well the generated code aligns with the expected
    output.
  prefs: []
  type: TYPE_NORMAL
- en: MBXP (most basic X programming problems)
  prefs: []
  type: TYPE_NORMAL
- en: This benchmark is designed specifically for evaluating code generation models
    across multiple programming languages. It uses a scalable conversion framework
    to transpile prompts and test cases from original datasets into target languages,
    thereby facilitating a comprehensive multilingual evaluation of code generation
    models.
  prefs: []
  type: TYPE_NORMAL
- en: HumanEval
  prefs: []
  type: TYPE_NORMAL
- en: This is a benchmark to evaluate the code generation capabilities of LLMs by
    measuring their functional correctness in synthesizing programs from docstrings.
    This benchmark is crucial for the continuous development and enhancement of AI
    models in code generation. While different models display varying levels of proficiency
    on HumanEval, an extended version called HUMANEVAL+ has been key in identifying
    previously undetected incorrect code generated by popular LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual HumanEval (HumanEval-X)
  prefs: []
  type: TYPE_NORMAL
- en: This is an extension of the original HumanEval benchmark. Multilingual HumanEval
    evaluates LLMs‚Äô code generation and translation capabilities across more than
    10 programming languages. It employs a conversion framework to transpile prompts
    and test cases from Python into corresponding data in target languages, creating
    a more comprehensive benchmark for multilingual code generation and translation.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to evaluate an LLM is to look at the number of parameters‚Äîwhich
    can be in the hundreds of billions. So the more, the better, right? Not necessarily.
    Evaluation should take a more nuanced approach. First of all, the costs of scaling
    the parameters can be enormous, in terms of compute power and energy usage. This
    could make an LLM uneconomical for monetizing applications. Next, as the parameter
    counts balloon, so does the complexity of the model, which could potentially lead
    to overfitting. *Overfitting* occurs when the model learns to perform exceedingly
    well on the training data but fumbles when exposed to unseen data. This dilutes
    its generalization capability.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue is the need for vast and diverse training datasets to feed the
    insatiable appetite of these models for data. However, obtaining and curating
    such extensive datasets not only is resource intensive but also poses challenges
    pertaining to data privacy and bias. What‚Äôs more, the evaluation of these behemoths
    becomes increasingly intricate with the surge in parameters. The evaluation metrics
    need to be more comprehensive and diverse to accurately gauge the model‚Äôs performance
    across a myriad of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, fine-tuning can be a better way to get more out of models without the
    need for large increases in the parameter size of the underlying LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Types of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are various types of LLMs, and one prominent category is open source LLMs.
    Anyone can use, tweak, or share them. Their transparency means you can see how
    these models tick. Plus, open source LLMs allow developers to collaborate on innovation
    as well as develop add-ons and, of course, fix pesky bugs.
  prefs: []
  type: TYPE_NORMAL
- en: And the best part? They don‚Äôt come with a price tag.
  prefs: []
  type: TYPE_NORMAL
- en: But open source LLMs are not all rainbows and unicorns. There‚Äôs usually no dedicated
    team to swoop in and fix issues or roll out regular updates. So, if you hit a
    snag, you might have to roll up your sleeves and dive into the forums for some
    help.
  prefs: []
  type: TYPE_NORMAL
- en: The quality and performance of open source models can sometimes feel like a
    rollercoaster. Then there are the nagging security issues. Since everything is
    available, hackers are more likely to find ways to insert nefarious code. Caution
    is advised.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, when it comes to user guides and documentation, open source LLMs might
    have you wishing for more. The guides can sometimes feel like they were written
    in hieroglyphics.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table¬†2-5](#top_open_source_llms) shows some of the top open source LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-5\. Top open source LLMs
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Developer | Parameters (B = billion) | Noteworthy features |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-NeoX-20B | EleutherAI | 20B | Trained on ‚ÄúThe Pile‚Äù dataset; capable
    of various NLP tasks such as story generation, chatbots, and summarization |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA 2 | Meta | 7B to 70B | Trained on 2 trillion tokens; double the context
    length of LLaMA 1 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-175B | Meta | 175B | Part of a suite of models; trained with a lower
    carbon footprint than GPT-3 |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM | BigScience | 176B | Trained on ROOTS^([a](ch02.html#id417)) corpus;
    designed for transparency with disclosed training data details and evaluation
    methods |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon-40B | Technology Innovation Institute (TII) | 40B | Trained on 1,000B
    tokens |'
  prefs: []
  type: TYPE_TB
- en: '| Dolly 2.0 | Databricks | 12B | Based on EleutherAI‚Äôs Pythia model family;
    delivers ChatGPT-like instruction-following interactivity |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral 7B | Mistral AI | 7.3B | Uses grouped-query and sliding window attention;
    trained on a vast dataset and excels in longer sequence handling |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral 8X7B | Mistral AI | 46.7B | Sparse mixture of experts model; performs
    inference like a 12.9B model, supports multiple languages, and excels in various
    tasks including code generation and reasoning |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch02.html#id417-marker)) Responsible Open-science Open-collaboration
    Text Sources |'
  prefs: []
  type: TYPE_TB
- en: Closed-source or proprietary LLMs, on the other hand, are much more secretive.
    They mostly keep their code, training data, and model structures under tight wraps.
    However, the companies that develop these complex systems usually have enormous
    amounts of capital. [Table¬†2-6](#venture_capital_raised_by_top_llm_develo) shows
    the capital raised by these firms in 2023.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-6\. Venture capital raised by top LLM developers
  prefs: []
  type: TYPE_NORMAL
- en: '| Company | Funding |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Anthropic | $1.25 billion |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI | $10 billion |'
  prefs: []
  type: TYPE_TB
- en: '| Cohere | $270 million |'
  prefs: []
  type: TYPE_TB
- en: '| Inflection AI | $1.3 billion |'
  prefs: []
  type: TYPE_TB
- en: With such resources, these companies can hire the world‚Äôs best data scientists
    and build sophisticated infrastructure. The result is that the LLMs are often
    state-of-the-art in terms of performance. They are also built for scale and the
    rigorous needs of enterprises, such as for security and privacy.
  prefs: []
  type: TYPE_NORMAL
- en: As for the downsides, there is the problem with trust. How do these models come
    up with their responses? What about hallucinations and bias? Answers to these
    questions can be lacking in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Then there is the risk that these mega AI operators will become a monopoly.
    This could mean that a customer would be locked into an ecosystem. Lastly, closed-source
    LLMs might be more prone to stagnation than open source projects, as they might
    not benefit from the diverse input and scrutiny that open source projects usually
    enjoy.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of AI-Assisted Programming Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Figuring out which AI-assisted programming tool to go for can be a head-scratcher.
    You‚Äôve got to weigh many factors like its precision, chat features, security,
    speed, and user-friendliness. Sometimes, it boils down to what feels right to
    work with. But then again, your hands might be tied if your employer insists on
    a specific system.
  prefs: []
  type: TYPE_NORMAL
- en: To get a sense of what‚Äôs hot right now, [Stack Overflow‚Äôs 2023 Developer Survey](https://oreil.ly/nvqKY)
    is a handy resource. Stack Overflow gathered insights from nearly 90,000 coders
    on the most popular tools, which you can see in [Table¬†2-7](#the_ranking_of_popular_aihyphenassisted).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-7\. The ranking of popular AI-assisted programming tools^([a](ch02.html#id424))
  prefs: []
  type: TYPE_NORMAL
- en: '| AI-assisted developer tool | Percentage |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GitHub Copilot | 54.77% |'
  prefs: []
  type: TYPE_TB
- en: '| Tabnine | 12.88% |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon CodeWhisperer | 5.14% |'
  prefs: []
  type: TYPE_TB
- en: '| Snyk Code | 1.33% |'
  prefs: []
  type: TYPE_TB
- en: '| Codeium | 1.25% |'
  prefs: []
  type: TYPE_TB
- en: '| Wispr AI | 1.13% |'
  prefs: []
  type: TYPE_TB
- en: '| Replit Ghostwriter | 0.83% |'
  prefs: []
  type: TYPE_TB
- en: '| Mintlify | 0.52% |'
  prefs: []
  type: TYPE_TB
- en: '| Adrenaline | 0.43% |'
  prefs: []
  type: TYPE_TB
- en: '| Rubberduck AI | 0.37% |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch02.html#id424-marker)) [Stack Overflow, 2023 Developer Survey](https://oreil.ly/0u7WZ)
    |'
  prefs: []
  type: TYPE_TB
- en: This chart gives you a glimpse of the numerous tools available. When you‚Äôre
    looking to pick one, a smart move is to get recommendations from other developers.
    Plus, it‚Äôs a good idea to test drive a few yourself. Luckily, most of these tools
    offer free trials, so you can give them a whirl without committing right off the
    bat.
  prefs: []
  type: TYPE_NORMAL
- en: Another key aspect to consider is the company‚Äôs financial backing. Does it have
    venture capital funding? Without this, a company might struggle not just to grow
    but also to keep its platform innovative. Already, several AI-assisted programming
    firms have had to pull the plug on their services, and that can really throw a
    wrench in the works for developers. Take Kite, for instance. It was one of the
    early players in this field, starting up in 2014\. However, by 2022, the company
    decided to [call it quits on the project](https://oreil.ly/Bnz9U). The silver
    lining? It open sourced most of the tool‚Äôs codebase.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we pulled back the curtain on generative AI and LLMs. We got
    a glimpse of some of the fascinating history, such as with ELIZA, and then focused
    on one of the biggest breakthroughs in AI: the transformer model. We also tried
    out the OpenAI Playground and showed how to customize the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Some of the key nuggets in this chapter include tokens, the advantages of piggybacking
    on pretrained models, the dos and don‚Äôts of sizing up LLMs, metrics like perplexity
    and BLEU scores, and open source versus proprietary models.
  prefs: []
  type: TYPE_NORMAL
