- en: Chapter 7\. Deploying PyTorch to Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of this book so far has focused on model design and training. Earlier chapters
    showed you how to use the built-in capabilities of PyTorch to design your models
    and create custom NN modules, loss functions, optimizers, and other algorithms.
    In the previous chapter, we looked at how to use distributed training and model
    optimizations to accelerate your model training times and minimize the resources
    needed for running your models.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you have everything you need to create some well-trained, cutting-edge
    NN models—but don’t let your innovations sit in isolation. Now it’s time to deploy
    your models into the world through applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, going from research to production was a challenging task that required
    a team of software engineers to move PyTorch models to a framework and integrate
    them nto a (often non-Python) production environment. Today, PyTorch includes
    built-in tools and external libraries to support rapid deployment to a variety
    of production environments.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we focus on deploying your model for inference, not training,
    and we’ll explore how to deploy your trained PyTorch models into a variety of
    applications. First, I’ll describe the various built-in capabilities and tools
    within PyTorch that you can use for deployment. Tools like TorchServe and TorchScript
    allow you to easily deploy your PyTorch models to the cloud and to mobile or edge
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the application and environment, you may have several options for
    deployment, each with its own trade-offs. I’ll show you examples of how you can
    deploy your PyTorch models in multiple cloud and edge environments. You’ll learn
    how to deploy to web servers for development and production at scale, to iOS and
    Android mobile devices, and to Internet of Things (IoT) devices based on ARM processors,
    GPUs, and field-programmable gate array (FPGA) hardware.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter will also provide reference code, including references to the key
    APIs and libraries we use, to make getting started easy. When it comes time to
    deploy your models, you can refer back to this chapter for a quick reference so
    you can demonstrate your applications in cloud or mobile environments.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by reviewing the resources that PyTorch provides to assist you in
    deploying your models.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Deployment Tools and Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch includes built-in tools and capabilities to facilitate deploying your
    model to production environments and edge devices. In this section, we’ll explore
    those tools, and in the rest of the chapter we’ll apply them to various environments.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch’s deployment capabilities include its natural Python API, as well as
    the TorchServe, TorchScript, ONNX, and mobile libraries. Since PyTorch’s natural
    API is Python-based, PyTorch models can be deployed as is in any environment that
    supports Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 7-1](#table_deployment_resources) summarizes the various resources available
    for deployment and indicates how to appropriately use each one.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-1\. PyTorch resources for deployment
  prefs: []
  type: TYPE_NORMAL
- en: '| Resource | Use |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Python API | Perform fast prototyping, training, and experimentation; program
    Python runtimes. |'
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | Improve performance and portability (e.g., load and run a model
    in C++); program non-Python runtimes or strict latency and performance requirements.
    |'
  prefs: []
  type: TYPE_TB
- en: '| TorchServe | A fast production environment tool with model store, A/B testing,
    monitoring, and RESTful API. |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX | Deploy to systems with ONNX runtimes or FPGA devices. |'
  prefs: []
  type: TYPE_TB
- en: '| Mobile libraries | Deploy to iOS and Android devices. |'
  prefs: []
  type: TYPE_TB
- en: The following sections provide a reference and some sample code for each deployment
    resource. In each case, we’ll use the same example model, described next.
  prefs: []
  type: TYPE_NORMAL
- en: Common Example Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For each of the deployment resource examples and applications, as well as the
    reference code provided in this chapter, we will use the same model. For our examples,
    we’ll deploy an image classifier using a VGG16 model pretrained with ImageNet
    data. That way, each section can focus on the deployment approach used and not
    the model itself. For each approach, you can replace the VGG16 model with one
    of your own and follow the same workflow to achieve results with your own designs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code instantiates the model for use throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve used the VGG16 model before. To give you an idea of the model’s complexity,
    let’s print out the number of trainable parameters using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The VGG16 model has 138,357,544 trainable parameters. As we go through each
    approach, keep in mind the performance at this level of complexity. You can use
    this as a rough benchmark when comparing the complexity of your models.
  prefs: []
  type: TYPE_NORMAL
- en: After we instantiate the VGG16 model, it requires minimal effort to deploy it
    in a Python application. In fact, we’ve already done this when we tested our models
    in previous chapters. Let’s review the process one more time before we jump into
    other approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Python API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Python API is not a new resource. It’s the same one we’ve been using throughout
    the book. I mention it here to point out that you can deploy your PyTorch models
    without any changes to your code. In this case, you simply call your model in
    evaluation mode with your desired inputs from any Python application, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The code loads the model, passes in the input, and prints out the output. This
    is a simple standalone Python application. You’ll see how to deploy a model to
    a Python web server using a RESTful API and Flask later in this chapter. Using
    a Flask web server, you can build a quick browser application that demonstrates
    your model’s capability.
  prefs: []
  type: TYPE_NORMAL
- en: Python is not always used in production environments due to its slower performance
    and lack of true multithreading. If your production environment uses another language
    (e.g., C++, Java, Rust, or Go), you can convert your models to TorchScript code.
  prefs: []
  type: TYPE_NORMAL
- en: TorchScript
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TorchScript is a way to serialize and optimize your PyTorch model code so that
    your PyTorch models can be saved and executed in non-Python runtime environments
    with no dependency on Python. TorchScript is commonly used to run PyTorch models
    in C++ and with any language that supports C++ bindings.
  prefs: []
  type: TYPE_NORMAL
- en: TorchScript represents a PyTorch model in a format that can be understood, compiled,
    and serialized by the TorchScript compiler. The TorchScript compiler creates a
    serialized, optimized version of your model that can be used in C++ applications.
    To load your TorchScript model in C++, you would use the PyTorch C++ API library
    called *LibTorch*.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to convert your PyTorch models to TorchScript. The first
    one is called *tracing*, which is a process in which you pass in an example input
    and perform the conversion with one line of code. It’s used in most cases. The
    second is called *scripting*, and it’s used when your model has more complex control
    code. For example, if your model has conditional `if` statements that depend on
    the input itself, you’ll want to use scripting. Let’s take a look at some reference
    code for each case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our VGG16 example model does not have any control flow, we can use tracing
    to convert our model to TorchScript, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The code creates a Python callable model, `torchscript_model`, that can be evaluated
    using a normal PyTorch approach such as `output = torchscript_model(inputs)`.
    Once we save the model, we can use it in a C++ application.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The “normal” method of evaluating a model in PyTorch is often called `eager
    mode` since it’s the quickest way to evaluate your models for development.
  prefs: []
  type: TYPE_NORMAL
- en: 'If our model used control flow, we would need to use the annotation method
    to convert it to TorchScript. Let’s consider the following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `ControlFlowModel` outputs and weights depend on the input
    values. In this case, we need to use `torch.jit.script()`, and then we can save
    the model to TorchScript just like we did with tracing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can use our model in a C++ application, as shown in the following C++
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We pass in the filename of the TorchScript module to the program and load the
    model using `torch::jit::load()`. Then we create a sample input vector, run it
    through our TorchScript model, and convert the outputs to tensors, printing them
    to `stdout`.
  prefs: []
  type: TYPE_NORMAL
- en: The TorchScript API provides additional functions to support converting your
    models to TorchScript. [Table 7-2](#tableTorchScriptapi) lists the supported functions.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-2\. TorchScript API functions
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `script(`*`obj[, optimize, _frames_up, _rcb]`*`)` | Inspects the source code,
    compiles it as *TorchScript* code using the *TorchScript* compiler, and returns
    a `ScriptModule` or `ScriptFunction` |'
  prefs: []
  type: TYPE_TB
- en: '| `trace(`*`func, example_inputs[, optimize, ...]`*`)` | Traces a function
    and returns an executable or `ScriptFunction` that will be optimized using just-in-time
    compilation |'
  prefs: []
  type: TYPE_TB
- en: '| `script_if_tracing(`*`fn`*`)` | Compiles `fn` when it is first called during
    tracing |'
  prefs: []
  type: TYPE_TB
- en: '| `trace_module(`*`mod, inputs[, optimize, ...]`*`)` | Traces a module and
    returns an executable `ScriptModule` that will be optimized using just-in-time
    compilation |'
  prefs: []
  type: TYPE_TB
- en: '| `fork(`*`func, *args, **kwargs`*`)` | Creates an asynchronous task executing
    `func` and a reference to the value of the result of this execution |'
  prefs: []
  type: TYPE_TB
- en: '| `wait(`*`future`*`)` | Forces the completion of a `torch.jit.Future[T]` asynchronous
    task, returning the result of the task |'
  prefs: []
  type: TYPE_TB
- en: '| `ScriptModule()` | Wraps a script into a C++ `torch::jit::Module` |'
  prefs: []
  type: TYPE_TB
- en: '| `ScriptFunction()` | Works the same as `ScriptModule()` but represents a
    single function and does not have any attributes or parameters |'
  prefs: []
  type: TYPE_TB
- en: '| `freeze(`*`mod[, preserved_attrs]`*`)` | Clones a `ScriptModule` and attempts
    to inline the cloned module’s submodules, parameters, and attributes as constants
    in the *TorchScript* IR Graph |'
  prefs: []
  type: TYPE_TB
- en: '| `save(`*`m, f[, _extra_files]`*`)` | Saves an offline version of the module
    for use in a separate process |'
  prefs: []
  type: TYPE_TB
- en: '| `load(`*`f[, map_location, _extra_files]`*`)` | Loads a `ScriptModule` or
    `ScriptFunction` previously saved with `torch.jit.save()` |'
  prefs: []
  type: TYPE_TB
- en: '| `ignore(`*`[drop]`*`)` | Indicates to the compiler that a function or method
    should be ignored and left as a Python function |'
  prefs: []
  type: TYPE_TB
- en: '| `unused(`*`fn`*`)` | Indicates to the compiler that a function or method
    should be ignored and replaced with the raising of an exception |'
  prefs: []
  type: TYPE_TB
- en: '| `isinstance(`*`obj, target_type`*`)` | Provides for container-type refinement
    in TorchScript |'
  prefs: []
  type: TYPE_TB
- en: In this section, we used TorchScript to increase the performance of our model
    when it’s used in a C++ application or in a language that binds to C++. However,
    deploying PyTorch models at scale requires additional capabilities, like packaging
    models, configuring runtime environments, exposing API endpoints, logging and
    monitoring, and managing multiple model versions. Fortunately, PyTorch provides
    a tool called TorchServe to facilitate these tasks and rapidly deploy your models
    for inference at scale.
  prefs: []
  type: TYPE_NORMAL
- en: TorchServe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TorchServe is an open-source model-serving framework that makes it easy to deploy
    trained PyTorch models. It was developed by AWS engineers and jointly released
    with Facebook in April 2020, and it is actively maintained by AWS. TorchServe
    supports all the features needed to deploy models to production at scale, including
    multimodel serving, model versioning for A/B testing, logging and metrics for
    monitoring, and a RESTful API for integration with other systems. [Figure 7-1](#fig_torchserve)
    illustrates how TorchServe works.
  prefs: []
  type: TYPE_NORMAL
- en: '![“TorchServe Architecture”](Images/ptpr_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. TorchServe architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The client application interfaces with TorchServe through multiple APIs. The
    Inference API provides the main inference requests and predictions. The client
    application sends input data through the RESTful API request and receives the
    prediction results. The Management API allows you to register and manage your
    deployed models. You can register, unregister, set default models, configure A/B
    testing, check status, and specify the number of workers for a model. The Metrics
    API allows you to monitor each model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: TorchServe runs all model instances and captures server logs. It processes the
    frontend APIs and manages the model storage to disk. TorchServe also provides
    a number of default handlers for common applications like object detection and
    text classification. The handlers take care of converting data from the API into
    a format that your model will process. This helps speed up deployment since you
    don’t have to write custom code for these common applications.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TorchServe is experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: To deploy your models via TorchServe, you will need to follow a few steps. First
    you need to install TorchServe’s tools. Then you’ll package your model using the
    model archiver tool. Once your models are archived, you’ll then run the TorchServe
    web server. Once the web server is running, you can use its APIs to request predictions,
    manage your models, perform monitoring, or access server logs. Let’s take a look
    at how to perform each step.
  prefs: []
  type: TYPE_NORMAL
- en: Install TorchServe and torch-model-archiver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS provides preinstalled machines with TorchServe in Amazon SageMaker or Amazon
    EC2 instances. If you’re using a different cloud provider, check with them to
    see if preinstalled instances exist before getting started. If you’re using a
    local server or need to install TorchServe, see the [TorchServe installation instructions](https://pytorch.tips/torchserve-install).
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple approach to try is to install with `conda` or `pip`, as shown in the
    following command lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you run into issues, refer to the TorchServe installation instructions at
    the preceding link.
  prefs: []
  type: TYPE_NORMAL
- en: Package a model archive
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TorchServe has the ability to package all model artifacts into a single-model
    archive file. To do so, we will use the `torch-model-archiver` command-line tool
    that we installed in the previous step. It packages model checkpoints as well
    as the `state_dict` into a *.mar* file that the TorchServe server uses to serve
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the `torch-model-archiver` to archive your TorchScript models as
    well as the standard “eager-mode” implementations, as shown in the following code.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a TorchScript moel, the command line is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We set the model as our example VGG16 model and use the saved serialized file,
    *model.pt*. In this case, we can use the default `image_classifier` handler as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the standard eager-mode model we would use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is similar to the previous command, but we also need to specify the model
    file, *model.py*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete set of options for the `torch-model-archiver` tool is shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Table 7-3\. Model archiver tool options
  prefs: []
  type: TYPE_NORMAL
- en: '| Options | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `-h`, `--help` | Help message. After the help message is displayed, the program
    will exit. |'
  prefs: []
  type: TYPE_TB
- en: '| `--model-name *MODEL_NAME*` | Exported model name. Exported file will be
    named as *<model-name>.mar* and saved in the current working directory if no `--export-path`
    is specified, else it will be saved under the export path. |'
  prefs: []
  type: TYPE_TB
- en: '| `--serialized-file` `*SERIALIZED_FILE*` | Path to a _.pt_ or _.pth_ file
    containing `state_dict` in case of eager mode or an executable `ScriptModule`
    in case of TorchScript. |'
  prefs: []
  type: TYPE_TB
- en: '| `--model-file *MODEL_FILE*` | Path to Python file containing the model architecture.
    This parameter is mandatory for eager-mode models. The model architecture file
    must contain only one class definition extended from `torch.nn.modules`. |'
  prefs: []
  type: TYPE_TB
- en: '| `--handler *HANDLER*` | *TorchServe*’s default handler name or Python file
    path to handle custom *TorchServe* inference logic. |'
  prefs: []
  type: TYPE_TB
- en: '| `--extra-files *EXTRA_FILES*` | Comma-separated path to extra dependency
    files. |'
  prefs: []
  type: TYPE_TB
- en: '| `--runtime *{python, python2, python3}*` | The runtime specifies which language
    to run your inference code on. The default runtime is `RuntimeType.PYTHON`, but
    at present the following runtimes are supported: `python`, `python2`, and `python3`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `--export-path *EXPORT_PATH*` | Path where the exported _.mar_ file will
    be saved. This is an optional parameter. If `--export-path` is not specified,
    the file will be saved in the current working directory. |'
  prefs: []
  type: TYPE_TB
- en: '| `--archive-format *{tgz, no-archive, default}*` | The format in which the
    model artifacts are archived. `tgz` creates the model archive in *<model-name>.tar.gz*
    format. If platform hosting requires model artifacts to be in *.tar.gz*, use this
    option. `no-archive` creates a nonarchived version of model artifacts at *<export-path>*/*<model-name>*.
    As a result of this choice, a MANIFEST file will be created at that location without
    archiving these model files. `default` creates the model archive in *<model-name>.mar*
    format. This is the default archiving format. Models archived in this format will
    be readily hostable on *TorchServe*. |'
  prefs: []
  type: TYPE_TB
- en: '| `-f`,`--force` | When the `-f` or `--force` flag is specified, an existing
    *.mar* file with the same name as that provided in `--model-name` in the path
    specified by `--export-path` will be overwritten. |'
  prefs: []
  type: TYPE_TB
- en: '| `-v`, `--version` | Model’s version. |'
  prefs: []
  type: TYPE_TB
- en: '| `-r`, `--requirements-file` | Path to a *requirements.txt* file containing
    a list of model-specific Python packages to be installed by *TorchServe* for seamless
    model serving. |'
  prefs: []
  type: TYPE_TB
- en: We can save our model archive *.mar* file in the */models* folder. We’ll use
    this as our model store. Next, let’s run the TorchServe web server.
  prefs: []
  type: TYPE_NORMAL
- en: Run TorchServe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TorchServe includes a built-in web server that is run from the command line.
    It wraps one or more PyTorch models in a set of REST APIs and provides controls
    for configuring the port, host, and logging. The following command starts the
    web server with all models in the model store located in the */models* folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: A complete set of options is shown in [Table 7-4](#table_torchserve_options).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-4\. TorchServe options
  prefs: []
  type: TYPE_NORMAL
- en: '| Options | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `--model-store` *+MODEL_STORE+* *+(mandatory)+* | Specifies the model store
    location where models can be loaded |'
  prefs: []
  type: TYPE_TB
- en: '| `-h`, `--help` | Shows the help message and exits |'
  prefs: []
  type: TYPE_TB
- en: '| `-v`, `--version` | Returns the TorchServe version |'
  prefs: []
  type: TYPE_TB
- en: '| `--start` | Starts the model server |'
  prefs: []
  type: TYPE_TB
- en: '| `--stop` | Stops the model server |'
  prefs: []
  type: TYPE_TB
- en: '| `--ts-config` *+TS_CONFIG+* | Indicates the configuration file for TorchServe
    |'
  prefs: []
  type: TYPE_TB
- en: '| `--models` *+MODEL_PATH1 MODEL_NAME=MODEL_PATH2… [MODEL_PATH1 MODEL_NAME=MODEL_PATH2…
    …]+* | Indicates the models to be loaded using *`[model_name=]model_location`*
    format; locations can be an HTTP URL, a model archive file, or a directory that
    contains model archive files in `MODEL_STORE` |'
  prefs: []
  type: TYPE_TB
- en: '| `--log-config` *+LOG_CONFIG+* | Indicates the log4j configuration file for
    TorchServe |'
  prefs: []
  type: TYPE_TB
- en: '| `--ncs`, `--no-config-snapshots` | Disables the snapshot feature |'
  prefs: []
  type: TYPE_TB
- en: Now that the TorchServe web server is running, you can use the Inference API
    to send data and request predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Request predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You use the Inference API to pass data and request predictions. The Inference
    API listens on port 8080 and is only accessible from localhost by default. To
    change the default setting, refer to the [TorchServe documentation](https://pytorch.org/serve/configuration.html).
    To get predictions from the server, we use the Inference API’s `Service.Predictions`
    gRPC API and make a REST call to */predictions/<model_name>*, as shown using `curl`
    in the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The code assumes we have an image file, *hot_dog.jpg.* The JSON-formatted response
    would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the Inference API to do a health check using the following
    request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The response will look like the following if the server is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For a full list of inference APIs use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Logging and monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can configure metrics using the Metrics API and monitor and log your models’
    performance when deployed. The Metrics API listens on port 8082 and is only accessible
    from localhost by default, but you can change the default when configuring your
    TorchServe server. The following command illustrates how to access metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The default metrics endpoint returns Prometheus-formatted metrics. Prometheus
    is a free software application used for event monitoring and alerting that records
    real-time metrics in a time series database built using an HTTP pull model. You
    can query metrics using `curl` requests or point a Prometheus Server to the endpoint
    and use Grafana for dashboards. See the [Metrics API documentation](https://pytorch.tips/serve-metrics)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics are logged to a file. TorchServe also supports other types of server
    logging, including access logs and TorchServe logs. Access logs record the inference
    requests and the time it takes to complete the requests. As defined in the *properties*
    file, the access logs are collected in the *<log_location>/access_log.log* file.
    TorchServe logs collect all the logs from TorchServe and its backend workers.
  prefs: []
  type: TYPE_NORMAL
- en: TorchServe supports capabilities beyond the default settings for metrics and
    logging. Metrics and logging can be configured in many different ways. In addition,
    you can create custom logs. For more information on metric and logging customization
    and other advanced features of TorchServe, refer to the [TorchServe documentation](https://pytorch.tips/torchserve).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The *NVIDIA Triton Inference Server* is becoming more popular and is also used
    to deploy AI models at scale in production. Although not part of the PyTorch project,
    you may want to consider the Triton Inference Server as an alternative to TorchServe,
    especially when deploying to NVIDIA GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Triton Inference Server is open source software and can load models from
    local storage, GCP, or AWS S3\. Triton supports running multiple models on single
    or multiple GPUs, low latency and shared memory, and model ensembles. Some possible
    advantages of Triton over TorchServe include:'
  prefs: []
  type: TYPE_NORMAL
- en: Triton is out of beta.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is the fastest way to infer on NVIDIA hardware (common).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can use `int4` quantization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can port directly from PyTorch without ONNX.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available as a Docker container, Triton Inference Server also integrates with
    Kubernetes for orchestration, metrics, and auto-scaling. For more information,
    visit the [NVIDIA Triton Inference Server documentation](https://pytorch.tips/triton).
  prefs: []
  type: TYPE_NORMAL
- en: ONNX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your platform doesn’t support PyTorch and you cannot use TorchScript/C++
    or TorchServe for your deployment, it may be possible that your deployment platform
    supports the Open Neural Network Exchange (ONNX) format. The ONNX format defines
    a common set of operators and a common file format so that deep learning engineers
    can use models across a variety of frameworks, tools, runtimes, and compilers.
  prefs: []
  type: TYPE_NORMAL
- en: ONNX was developed by Facebook and Microsoft to allow model interoperability
    between PyTorch and other frameworks, such as Caffe2 and Microsoft Cognitive Toolkit
    (CTK). ONNX is currently supported by inference runtimes from a number of providers,
    including Cadence Systems, Habana, Intel AI, NVIDIA, Qualcomm, Tencent, Windows,
    and Xilinx.
  prefs: []
  type: TYPE_NORMAL
- en: An example use case is edge deployment on a Xilinx FPGA device. FPGA devices
    are custom chips that can be programmed with specific logic. They are used by
    edge devices for low-latency or high-performance applications, like video. If
    you want to deploy your new innovative model to an FPGA device, you would first
    convert it to ONNX format and then use the Xilinx FPGA development tools to generate
    an FPGA image with your model’s implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at an example of how to export a model to ONNX, again using
    our VGG16 model. The ONNX exporter can use tracing or scripting. We learned about
    tracing and scripting, described in the earlier section on TorchScript. We can
    use tracing by simply providing the model and an example input. The following
    code shows how we’d export our VGG16 model to ONNX using tracing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We define an example input and call `torch.onnx.export()`. The resulting file,
    *vgg16.onnx*, is a binary protobuf file that contains both the network structure
    and the parameters of the VGG16 model we exported.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to verify that our model was converted to ONNX properly, we can
    use the ONNX checker, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This code uses the Python ONNX library to load the model, run the checker, and
    print out a human-readable version of the model. You may need to install the ONNX
    library before running the code, using `conda` or `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about converting to ONNX or running in an ONNX runtime, check
    out the [ONNX tutorial](https://pytorch.tips/onnx-tutorial) on the PyTorch website.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to TorchScript, TorchServe, and ONNX, more tools are being developed
    to support PyTorch model deployment. Let’s consider some tools used to deploy
    models to mobile platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Mobile Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Android and iPhone devices are continuously evolving and adding native support
    for deep learning acceleration in their custom chipsets. In addition, there is
    a growing need to reduce latency, preserve privacy, and interact seamlessly with
    deep learning models in applications such as augmented reality (AR). Deployment
    to mobile devices is further complicated due to mobile runtimes that can significantly
    differ from the training environments used by developers, leading to errors and
    challenges during mobile deployment.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Mobile addresses these challenges and provides an end-to-end workflow
    to go from training to mobile deployment. PyTorch Mobile is available for iOS,
    Android, and Linux and provides APIs for the preprocessing and integration tasks
    needed for mobile applications. The basic workflow is shown in [Figure 7-2](#fig_pytorch_mobile).
  prefs: []
  type: TYPE_NORMAL
- en: '![“PyTorch Mobile Workflow”](Images/ptpr_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. PyTorch mobile workflow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You start by designing your model in PyTorch as you normally would. Then you
    may quantize your model to reduce its complexity with minimal degradation in performance.
    Subsequently, you would use tracing or scripting to convert to TorchScript and
    optimize your model for mobile devices using `torch.utils`. Next, save your model
    and use the appropriate mobile library for deployment. Android uses the Maven
    PyTorch library and iOS uses CocoPods with the LibTorch pod.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyTorch Mobile is still being developed and is subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: For the latest details on PyTorch Mobile, refer to the [PyTorch Mobile documentation](https://pytorch.tips/mobile).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve explored some PyTorch tools available for deploying our models,
    let’s take a look at some reference applications and code for deployment to the
    cloud and to edge devices. First I’ll show you how to build a web server for development
    using Flask.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to a Flask App
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before deploying to full-scale production, you may want to deploy your models
    to a development web server. This enables you to integrate your deep learning
    algorithms with other systems and quickly build prototypes to demonstrate your
    new models. One of the easiest ways to build a development server is with Python
    using Flask.
  prefs: []
  type: TYPE_NORMAL
- en: Flask is a simple micro web framework written in Python. It is called a “micro”
    framework because it does not include a database abstraction layer, form validation,
    upload handling, various authentication technologies, or anything else that might
    be provided with other libraries. We won’t cover Flask in depth in this book,
    but I’ll show you how to use Flask to deploy your models in Python.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also expose a REST API so that other applications can pass in data and
    receive predictions. In the following examples, we’ll deploy our pretrained VGG16
    model and classify images. First we’ll define our API endpoints, request types,
    and response types. Our API endpoint will be at */predict*, which takes in POST
    requests (including the image file). The response will be in JSON format and contain
    a `class_id` and `class_name` from the ImageNet dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create our main Flask file, called *app.py*. First we’ll import the required
    packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We’ll be using `io` to convert bytes to an image and `json` to handle JSON-formatted
    data. We’ll be using `torchvision` to create our VGG16 model and transform the
    image data into the appropriate format for our model. Finally, we import `Flask`,
    `jsonnify`, and `request` to handle the API requests and responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we create our web server, let’s define a `get_prediction()` function
    that reads in image data, preprocesses it, passes it into our model, and returns
    the image class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Since our model will return a number indicating the class, we’ll need a lookup
    table to convert this number to a class name. We create a dictionary called `imagenet_class_index`
    by reading in the JSON conversion file. We then instantiate our VGG16 model and
    define our image transforms to preprocess a PIL image by resizing it, center-cropping
    it, converting it to a tensor, and normalizing it. These steps are required prior
    to sending the image into our model.
  prefs: []
  type: TYPE_NORMAL
- en: Our `get_prediction()` function creates a PIL image object based on the received
    bytes and applies the required image transforms to create an input tensor. Next,
    we perform the forward pass (or model inference) and find the class with highest
    probability, `y`. Last, we look up the class name using the output class value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have code that preprocesses an image, passes it through our model,
    and returns the predicted class, we can create our Flask web server and endpoints
    and deploy our model, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Our web server object is called `app`. We’ve created it, but it’s not running
    yet. We set our endpoint to */predict* and configured it to handle POST requests.
    When the web server receives the POST, it will execute the `predict()` function
    that reads the image, gets the prediction, and returns the image class in JSON
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s it! Now we just need to add the following code so that the web server
    runs when we execute *app.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To test our web server, we can run it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can send an image using a simple Python file and the `requests` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we’re running a web server on our local machine on port 5000
    (`localhost:5000`). You may want to run your development web server in Google
    Colab to take advantage of cloud GPUs. I’ll show you how to do so next.
  prefs: []
  type: TYPE_NORMAL
- en: Colab Flask App
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps you’ve been developing your PyTorch models in Colab to take advantage
    of its rapid development or its GPUs. Colab provides a virtual machine (VM) which
    routes its `localhost` to our machine’s local host. To expose it to a public URL,
    we can use a library called `ngrok`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First install `ngrok` in Colab:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To run our Flask app with `ngrok`, all we need to do is add two lines of code,
    as shown in the following annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deploying_pytorch_to_production_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `ngrok` library.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deploying_pytorch_to_production_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Starts `ngrok` when the app is run.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_deploying_pytorch_to_production_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Since we’re running in Colab, we don’t need to check for `main`.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve omitted the other imports and the `get_prediction()` function as they
    do not change. Now you can run your development web server in Colab for even faster
    prototyping. The `ngrok` library provides a secure URL for the server running
    in Colab; you’ll find the URL in the Colab notebook output when running the Flask
    app. For example, the following output shows that the URL is *[*http://c0c97117ba27.ngrok.io*](http://c0c97117ba27.ngrok.io)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, you can send a POST request with an image to test the web server.
    You can run the following code locally or in another Colab notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Notice the URL has changed. Deploying your model in a Flask app is a good way
    to quickly test it and the `get_prediction()` function with a REST API. However,
    our Flask app here is used as a development web server, not for production deployment.
    When deploying your models at scale, you will need to address things like model
    management, A/B testing, monitoring, logging, and other tasks to ensure your model
    server is working properly. To deploy to production at scale, we’ll use TorchServe.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to the Cloud with TorchServe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we’ll deploy our VGG16 image classifier to a production environment.
    Let’s pretend our company makes a software tool that will sort collections of
    retail product images into categories depending on which objects appear in the
    images. The company is growing rapidly and now supports millions of small businesses
    that use the tool daily.
  prefs: []
  type: TYPE_NORMAL
- en: As part of the machine learning engineering team, you’ll need to deploy your
    model to production and provide a simple REST API that the software tool will
    use to classify its images. Because we want to deploy something as quickly as
    possible, we’ll use a Docker container in an AWS EC2 instance.
  prefs: []
  type: TYPE_NORMAL
- en: Quick Start with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TorchServe provides scripts to create Docker images based on a variety of platforms
    and options. Running a Docker container eliminates the need to reinstall all the
    dependencies required to run TorchServe. In addition, we can scale our model inference
    by spinning multiple Docker containers using Kubernetes. First we must create
    the Docker image according to the resources we have on our EC2 instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to clone the TorchServe repository and navigate to the *Docker*
    folder, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we’ll need to add our model archive for VGG16 into the Docker image. We
    do this by adding the following line to the Dockerfile that downloads the archived
    model file and saves it within the */home/model-server/* directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run the *build_image.sh* script to create a Docker image with the
    public binaries installed. Since we’re running on an EC2 instance with a GPU,
    we’ll use the `-g` flag, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: You can run **`./build_image.sh -h`** to see additional options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once our Docker image is created, we can run the container with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This command will start the container with the 8080/81/82 and 7070/71 ports
    exposed to the outer world’s localhost. It uses one GPU with the latest CUDA version.
  prefs: []
  type: TYPE_NORMAL
- en: Now our TorchServe Docker container is running. Our company’s software tool
    can send inference requests by sending the image file to *ourip.com/predict* and
    can receive image classifications via JSON.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on running TorchServe in Docker, refer to the [TorchServe Docker
    documentation](https://pytorch.tips/torchserve-docker). To learn more about TorchServe,
    visit the [TorchServe repository](https://pytorch.tips/torchserve-github).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can deploy your models to your local machine and cloud servers using
    Flask for development or TorchServe for production. This is useful for prototyping
    and integrating with other applications through a REST API. Next, you’ll expand
    your deployment capabilities outside of the cloud: in the following section we’ll
    explore how you would deploy models to mobile devices and other edge devices.'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Mobile and Edge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Edge devices are (usually small) hardware systems that interface directly with
    the user or environment and run machine learning computations directly on the
    device instead of on a centralized server in the cloud. Some examples of edge
    devices include mobile phones and tablets, wearables like smart watches and heart
    rate monitors, and other IoT devices such as industrial sensors and home thermostats.
    There’s a growing need to run deep learning algorithms on edge devices in order
    to maintain privacy, reduce data transfer, minimize latency, and support new interactive
    use cases in real time.
  prefs: []
  type: TYPE_NORMAL
- en: First we’ll explore how to deploy your PyTorch models on mobile devices with
    iOS and Android, then we’ll cover other edge devices. PyTorch’s support for edge
    deployment is limited but growing. These sections will provide some reference
    code to help you get started using PyTorch Mobile.
  prefs: []
  type: TYPE_NORMAL
- en: iOS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to Apple, as of January 2021 there were over 1.65 billion active iOS
    devices in the world. The support for machine learning hardware acceleration continues
    to grow with each new model and custom processing unit. Learning how to deploy
    your PyTorch models to iOS opens the doors for many opportunities to create an
    iOS app based on deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: To deploy your model to an iOS device, you’ll need to learn how to create an
    iOS application using development tools like Xcode. We won’t cover iOS development
    in this book, but you can find a “Hello, World” program and sample code to help
    you build your app at the [PyTorch iOS Example Apps GitHub repository](https://pytorch.tips/ios-demo).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s describe the workflow for deploying our VGG16 network to an iOS application.
    iOS will use the PyTorch C++ API to interface with our model, so we’ll need to
    convert and save our model to TorchScript first. Then we’ll wrap the C++ function
    in Objective-C so iOS Swift code can access the API. We’ll use Swift to load and
    preprocess an image, and then we’ll pass the image data into our model to predict
    its class.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will convert our VGG16 model to TorchScript using tracing and save
    it as *model.pt*, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deploying_pytorch_to_production_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Define `example` using random data.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deploying_pytorch_to_production_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Convert `model` to TorchScript.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_deploying_pytorch_to_production_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: New step to optimize the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_deploying_pytorch_to_production_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Save the model.
  prefs: []
  type: TYPE_NORMAL
- en: As described earlier, using tracing requires defining an example input, and
    we do so using random data. Then we convert the model to TorchScript using `torch.jit.trace()`.
    We then add a new step to optimize the TorchScript code for mobile platforms using
    the `torch.utils.mobile_optimizer` package. Finally, we save the model to a file
    named *model.pt*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’ll need to write our Swift iOS application. Our iOS app will use the
    PyTorch C++ library, which we can install via CocoaPods as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need to write some Swift code to load a sample image. You can improve
    this in the future by accessing the camera or photos on the device, but for now
    we’ll keep it simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Here we resize the image to 224 × 224 pixels and run a function to normalize
    the image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we load and instantiate our model into our iOS app, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: iOS is written in Swift, and Swift cannot interface to C++, so we need to use
    an Objective-C class, `TorchModule`, as a wrapper for `torch::jit::script::Module`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our model is loaded, we can predict an image’s class by passing the
    preprocessed image data into our model and running a prediction, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Under the hood, the `predict()` Objective-C wrapper calls the C++ `forward()`
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: When you run the sample app, you should see output similar to [Figure 7-3](#fig_ios_example)
    for the sample image file.
  prefs: []
  type: TYPE_NORMAL
- en: 'This image classification example is only a small representation of the capabilities
    of coding for iOS devices. For more advanced use cases, you can still follow the
    same process: convert and save to TorchScript, create an Objective-C wrapper,
    preprocess the input, and call your `predict()` function. Next, we’ll follow a
    similar process for deploying PyTorch to Android mobile devices.'
  prefs: []
  type: TYPE_NORMAL
- en: '![“iOS Example”](Images/ptpr_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. iOS example
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Android
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Android mobile devices are also abundantly used throughout the world, with the
    OS estimated to have a market share of over 70% in mobile devices at the start
    of 2021\. This means there is also a huge opportunity to deploy PyTorch models
    to Android devices.
  prefs: []
  type: TYPE_NORMAL
- en: Android uses the PyTorch Android API, and you will need to install the Android
    development tools to build a sample app. Using Android Studio, you will be able
    to install the Android native development kit (NDK) and software development kit
    (SDK). We won’t cover Android development in the book, but you can find a “Hello,
    World” program and sample code to help you build your app at the [PyTorch Android
    Example GitHub repository](https://pytorch.tips/android-demo).
  prefs: []
  type: TYPE_NORMAL
- en: The workflow for deploying a PyTorch model on an Android device is very similar
    to the process we used for iOS. We’ll still need to convert our model to TorchScript
    to use it with the PyTorch Android API. However, since the API natively supports
    loading and running our TorchScript model, we do not need to wrap it in C++ code
    as we did with iOS. Instead, we’ll use Java to write an Android app that loads
    and preprocesses an image file, passes it to our model for inference, and returns
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s deploy our VGG16 model to Android. First we convert the model to TorchScript
    just like we did for iOS, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We convert the model to TorchScript using tracing with `torch.jit.trace()`.
    We then add a new step to optimize the TorchScript code for mobile platforms using
    the `torch.utils.mobile_optimizer` package. Finally, we save the model to a file
    named *model.pt*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create our Android app using Java. We add the PyTorch Android API
    to our app as a Gradle dependency by adding the following code to *build.gradle*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we write our Android app. We start by loading an image and preprocessing
    it with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our image, we can predict its class, but first we must load
    our model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can run inference to predict the image’s class and process the results
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This workflow can be used for more advanced use cases. You can use the camera
    or photos on the device or other Android sensors to create more complex apps.
    For more PyTorch Android demo applications, visit the [PyTorch Android Demo App
    GitHub repository](https://pytorch.tips/android-demo-repo).
  prefs: []
  type: TYPE_NORMAL
- en: Other Edge Devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mobile devices running iOS or Android represent one type of edge device, but
    there are many more that can execute deep learning algorithms. Edge devices are
    often built using custom hardware for a specific application. Examples of other
    edge devices include sensors, video equipment, medical monitors, software-defined
    radios, thermostats, farming machines, and manufacturing sensors to detect defects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most edge devices include computer processors, GPUs, FPGAs, or other custom
    ASIC computer chips that are capable of running deep learning models. So how do
    you deploy your PyTorch models to these edge devices? Well, it depends on what
    processing components are used on the device. Let’s explore some ideas for commonly
    used chips:'
  prefs: []
  type: TYPE_NORMAL
- en: CPUs
  prefs: []
  type: TYPE_NORMAL
- en: If your edge device uses a CPU, such as an Intel or AMD processor, PyTorch can
    be deployed in Python and C++ using both TorchScript and the C++ frontend API.
    Mobile and edge CPU chipsets are usually optimized to minimize power, and memory
    may be more limited on an edge device. It may be worthwhile to optimize your models
    using pruning or quantization prior to deployment to minimize the power and memory
    required to run inference.
  prefs: []
  type: TYPE_NORMAL
- en: ARMs
  prefs: []
  type: TYPE_NORMAL
- en: ARM processors are a family of computer processors with a reduced set of instructions.
    They typically run at lower power and clock speeds than Intel or AMD CPUs and
    can be included within Systems on a Chip (SoCs). In addition to the processor,
    SoCs chips usually include other electronics such as programmable FPGA logic or
    GPUs. Running PyTorch in Linux on ARM devices is currently under development.
  prefs: []
  type: TYPE_NORMAL
- en: Microcontrollers
  prefs: []
  type: TYPE_NORMAL
- en: Microcontrollers are very limited processors that are usually aimed at very
    simple control tasks. Some popular microcontrollers include Arduino and Beaglebone
    processors. Support for microcontrollers is limited due to the few resources available.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs
  prefs: []
  type: TYPE_NORMAL
- en: Edge devices may include GPU chips. NVIDIA GPUs, are the most widely supported
    GPUs, but other companies (such as AMD and Intel) manufacture GPU chips as well.
    NVIDIA supports PyTorch in its GPU development kits, including its Jetson Nano,
    Xavier, and NX boards.
  prefs: []
  type: TYPE_NORMAL
- en: FPGAs
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch models can be deployed to many FPGA devices, including Xilinx (recently
    acquired by AMD) and Intel FPGA device families. Neither platform supports direct
    PyTorch deployment; however, they do support the ONNX format. The typical approach
    is to convert PyTorch models to ONNX and use the FPGA development tools to create
    FPGA logic from the ONNX model.
  prefs: []
  type: TYPE_NORMAL
- en: TPUs
  prefs: []
  type: TYPE_NORMAL
- en: Google’s TPU chips are being deployed across edge devices as well. PyTorch is
    supported via the XLA library, as described in [“PyTorch on a TPU”](ch06.xhtml#onatpu).
    Deploying your models to edge devices that utilize TPUs can enable you to run
    inference using the XLA library.
  prefs: []
  type: TYPE_NORMAL
- en: ASICs
  prefs: []
  type: TYPE_NORMAL
- en: Many companies are developing their own custom chips or ASICs that implement
    model designs in a highly optimized and efficient manner. The ability to deploy
    your PyTorch models will depend heavily on the capabilities supported by the custom
    ASIC chip designs and development tools. In some cases, you may be able to use
    the PyTorch/XLA library if the ASIC supports it.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes time to deploy your PyTorch models to an edge device, consider
    the processing components available on the system. Depending on the chips available,
    investigate your options to utilize the C++ frontend API, leverage TorchScript,
    convert your models to ONNX format, or access the PyTorch XLA library to deploy
    your models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use the standard Python API, TorchScript/C++,
    TorchServe, ONNX, and the PyTorch mobile libraries to deploy your models for inference.
    The chapter also provided reference code to deploy your PyTorch models to local
    development servers or production environments in the cloud using Flask and TorchServe,
    as well as to iOS and Android devices.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch supports a large, active ecosystem of useful tools for model development
    and deployment. We’ll explore this ecosystem in the next chapter, which also provides
    reference code for some of the most popular PyTorch tools.
  prefs: []
  type: TYPE_NORMAL
