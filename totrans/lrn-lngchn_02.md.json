["```py\nfrom langchain_community.document_loaders import TextLoader\n\nloader = TextLoader(\"./test.txt\")\nloader.load()\n```", "```py\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\nconst loader = new TextLoader(\"./test.txt\");\n\nconst docs = await loader.load();\n```", "```py\n[Document(page_content='text content \\n', metadata={'line_number': 0, 'source': \n    './test.txt'})]\n```", "```py\npip install beautifulsoup4\n```", "```py\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.langchain.com/\")\nloader.load()\n```", "```py\n// install cheerio: npm install cheerio\nimport { \n  CheerioWebBaseLoader \n} from \"@langchain/community/document_loaders/web/cheerio\";\n\nconst loader = new CheerioWebBaseLoader(\"https://www.langchain.com/\");\n\nconst docs = await loader.load();\n```", "```py\n# install the pdf parsing library\n# pip install pypdf\n\nfrom langchain_community.document_loaders import PyPDFLoader\n\nloader = PyPDFLoader(\"./test.pdf\")\npages = loader.load()\n```", "```py\n// install the pdf parsing library: npm install pdf-parse\n\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"./test.pdf\");\n\nconst docs = await loader.load();\n```", "```py\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nloader = TextLoader(\"./test.txt\") # or any other loader\ndocs = loader.load()\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n)\nsplitted_docs = splitter.split_documents(docs)\n```", "```py\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst loader = new TextLoader(\"./test.txt\"); // or any other loader \nconst docs = await loader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n});\n\nconst splittedDocs = await splitter.splitDocuments(docs)\n```", "```py\nfrom langchain_text_splitters import (\n    Language,\n    RecursiveCharacterTextSplitter,\n)\n\nPYTHON_CODE = \"\"\"\ndef hello_world():\n print(\"Hello, World!\")\n\n# Call the function\nhello_world()\n\"\"\"\npython_splitter = RecursiveCharacterTextSplitter.from_language(\n    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n)\npython_docs = python_splitter.create_documents([PYTHON_CODE])\n```", "```py\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst PYTHON_CODE = `\ndef hello_world():\n print(\"Hello, World!\")\n\n# Call the function\nhello_world()\n`;\n\nconst pythonSplitter = RecursiveCharacterTextSplitter.fromLanguage(\"python\", {\n  chunkSize: 50,\n  chunkOverlap: 0,\n});\nconst pythonDocs = await pythonSplitter.createDocuments([PYTHON_CODE]);\n```", "```py\n[Document(page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n    Document(page_content='# Call the function\\nhello_world()')]\n```", "```py\nmarkdown_text = \"\"\"\n# LangChain\n\n⚡ Building applications with LLMs through composability ⚡\n\n## Quick Install\n\n```", "```py\n\nAs an open source project in a rapidly developing field, we are extremely open \n to contributions.\n\"\"\"\n\nmd_splitter = RecursiveCharacterTextSplitter.from_language(\n    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\n)\nmd_docs = md_splitter.create_documents([markdown_text], \n    [{\"source\": \"https://www.langchain.com\"}])\n```", "```py\nconst markdownText = `\n# LangChain\n\n⚡ Building applications with LLMs through composability ⚡\n\n## Quick Install\n\n\\`\\`\\`bash\npip install langchain\n\\`\\`\\`\n\nAs an open source project in a rapidly developing field, we are extremely \n open to contributions.\n`;\n\nconst mdSplitter = RecursiveCharacterTextSplitter.fromLanguage(\"markdown\", {\n  chunkSize: 60,\n  chunkOverlap: 0,\n});\nconst mdDocs = await mdSplitter.createDocuments([markdownText], \n  [{\"source\": \"https://www.langchain.com\"}]);\n```", "```py\n[Document(page_content='# LangChain', \n    metadata={\"source\": \"https://www.langchain.com\"}),\n Document(page_content='⚡ Building applications with LLMs through composability \n    ⚡', metadata={\"source\": \"https://www.langchain.com\"}),\n Document(page_content='## Quick Install\\n\\n```", "```py', metadata={\"source\": \"https://www.langchain.com\"}),\n Document(page_content='As an open source project in a rapidly developing field, \n    we', metadata={\"source\": \"https://www.langchain.com\"}),\n Document(page_content='are extremely open to contributions.', \n    metadata={\"source\": \"https://www.langchain.com\"})]\n```", "```py\nfrom langchain_openai import OpenAIEmbeddings\n\nmodel = OpenAIEmbeddings()\n\nembeddings = model.embed_documents([\n    \"Hi there!\",\n    \"Oh, hello!\",\n    \"What's your name?\",\n    \"My friends call me World\",\n    \"Hello World!\"\n])\n```", "```py\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst model = new OpenAIEmbeddings();\n\nconst embeddings = await embeddings.embedDocuments([\n  \"Hi there!\",\n  \"Oh, hello!\",\n  \"What' s your name?\",\n  \"My friends call me World\",\n  \"Hello World!\"\n]);\n```", "```py\n[\n  [\n    -0.004845875, 0.004899438, -0.016358767, -0.024475135, -0.017341806,\n      0.012571548, -0.019156644, 0.009036391, -0.010227379, -0.026945334,\n      0.022861943, 0.010321903, -0.023479493, -0.0066544134, 0.007977734,\n    0.0026371893, 0.025206111, -0.012048521, 0.012943339, 0.013094575,\n    -0.010580265, -0.003509951, 0.004070787, 0.008639394, -0.020631202,\n    ... 1511 more items\n  ]\n  [\n      -0.009446913, -0.013253193, 0.013174579, 0.0057552797, -0.038993083,\n      0.0077763423, -0.0260478, -0.0114384955, -0.0022683728, -0.016509168,\n      0.041797023, 0.01787183, 0.00552271, -0.0049789557, 0.018146982,\n      -0.01542166, 0.033752076, 0.006112323, 0.023872782, -0.016535373,\n      -0.006623321, 0.016116094, -0.0061090477, -0.0044155475, -0.016627092,\n    ... 1511 more items\n  ]\n  ... 3 more items\n]\n```", "```py\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\n\n## Load the document \n\nloader = TextLoader(\"./test.txt\")\ndoc = loader.load()\n\n\"\"\"\n[\n Document(page_content='Document loaders\\n\\nUse document loaders to load data \n from a source as `Document`\\'s. A `Document` is a piece of text\\nand \n associated metadata. For example, there are document loaders for \n loading a simple `.txt` file, for loading the text\\ncontents of any web \n page, or even for loading a transcript of a YouTube video.\\n\\nEvery \n document loader exposes two methods:\\n1\\. \"Load\": load documents from \n the configured source\\n2\\. \"Load and split\": load documents from the \n configured source and split them using the passed in text \n splitter\\n\\nThey optionally implement:\\n\\n3\\. \"Lazy load\": load \n documents into memory lazily\\n', metadata={'source': 'test.txt'})\n]\n\"\"\"\n\n## Split the document\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=20,\n)\nchunks = text_splitter.split_documents(doc)\n\n## Generate embeddings\n\nembeddings_model = OpenAIEmbeddings()\nembeddings = embeddings_model.embed_documents(\n    [chunk.page_content for chunk in chunks]\n)\n\"\"\"\n[[0.0053587136790156364,\n -0.0004999046213924885,\n 0.038883671164512634,\n -0.003001077566295862,\n -0.00900818221271038, ...], ...]\n\"\"\"\n```", "```py\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\n// Load the document \n\nconst loader = new TextLoader(\"./test.txt\");\nconst docs = await loader.load();\n\n// Split the document\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n});\nconst chunks = await splitter.splitDocuments(docs)\n\n// Generate embeddings\n\nconst model = new OpenAIEmbeddings();\nawait embeddings.embedDocuments(chunks.map(c => c.pageContent));\n```", "```py\n    docker run \\\n        --name pgvector-container \\\n        -e POSTGRES_USER=langchain \\\n        -e POSTGRES_PASSWORD=langchain \\\n        -e POSTGRES_DB=langchain \\\n        -p 6024:5432 \\\n        -d pgvector/pgvector:pg16\n    ```", "```py\n    postgresql+psycopg://langchain:langchain@localhost:6024/langchain\n    ```", "```py\n# first, pip install langchain-postgres\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_postgres.vectorstores import PGVector\nfrom langchain_core.documents import Document\nimport uuid\n\n# Load the document, split it into chunks\nraw_documents = TextLoader('./test.txt').load()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n    chunk_overlap=200)\ndocuments = text_splitter.split_documents(raw_documents)\n\n# embed each chunk and insert it into the vector store\nembeddings_model = OpenAIEmbeddings()\nconnection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\ndb = PGVector.from_documents(documents, embeddings_model, connection=connection)\n```", "```py\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { PGVectorStore } from \"@langchain/community/vectorstores/pgvector\";\nimport { v4 as uuidv4 } from 'uuid';\n\n// Load the document, split it into chunks\nconst loader = new TextLoader(\"./test.txt\");\nconst raw_docs = await loader.load();\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n});\nconst docs = await splitter.splitDocuments(docs)\n\n// embed each chunk and insert it into the vector store\nconst embeddings_model = new OpenAIEmbeddings();\nconst db = await PGVectorStore.fromDocuments(docs, embeddings_model, {\n  postgresConnectionOptions: {\n    connectionString: 'postgresql://langchain:langchain@localhost:6024/langchain'\n  }\n})\n```", "```py\ndb.similarity_search(\"query\", k=4)\n```", "```py\nawait pgvectorStore.similaritySearch(\"query\", 4);\n```", "```py\nids = [str(uuid.uuid4()), str(uuid.uuid4())]\ndb.add_documents(\n    [\n        Document(\n            page_content=\"there are cats in the pond\",\n            metadata={\"location\": \"pond\", \"topic\": \"animals\"},\n        ),\n        Document(\n            page_content=\"ducks are also found in the pond\",\n            metadata={\"location\": \"pond\", \"topic\": \"animals\"},\n        ),\n    ],\n    ids=ids,\n)\n```", "```py\nconst ids = [uuidv4(), uuidv4()];\n\nawait db.addDocuments(\n  [\n    {\n      pageContent: \"there are cats in the pond\",\n      metadata: {location: \"pond\", topic: \"animals\"}\n    }, \n    {\n      pageContent: \"ducks are also found in the pond\",\n      metadata: {location: \"pond\", topic: \"animals\"}\n    },\n  ], \n  {ids}\n);\n```", "```py\ndb.delete(ids=[1])\n```", "```py\nawait db.delete({ ids: [ids[1]] })\n```", "```py\nfrom langchain.indexes import SQLRecordManager, index\nfrom langchain_postgres.vectorstores import PGVector\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.docstore.document import Document\n\nconnection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\ncollection_name = \"my_docs\"\nembeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\nnamespace = \"my_docs_namespace\"\n\nvectorstore = PGVector(\n    embeddings=embeddings_model,\n    collection_name=collection_name,\n    connection=connection,\n    use_jsonb=True,\n)\n\nrecord_manager = SQLRecordManager(\n    namespace,\n    db_url=\"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\",\n)\n\n# Create the schema if it doesn't exist\nrecord_manager.create_schema()\n\n# Create documents\ndocs = [\n    Document(page_content='there are cats in the pond', metadata={\n        \"id\": 1, \"source\": \"cats.txt\"}),\n    Document(page_content='ducks are also found in the pond', metadata={\n        \"id\": 2, \"source\": \"ducks.txt\"}),\n]\n\n# Index the documents\nindex_1 = index(\n    docs,\n    record_manager,\n    vectorstore,\n    cleanup=\"incremental\",  # prevent duplicate documents\n    source_id_key=\"source\",  # use the source field as the source_id\n)\n\nprint(\"Index attempt 1:\", index_1)\n\n# second time you attempt to index, it will not add the documents again\nindex_2 = index(\n    docs,\n    record_manager,\n    vectorstore,\n    cleanup=\"incremental\",\n    source_id_key=\"source\",\n)\n\nprint(\"Index attempt 2:\", index_2)\n\n# If we mutate a document, the new version will be written and all old \n# versions sharing the same source will be deleted.\n\ndocs[0].page_content = \"I just modified this document!\"\n\nindex_3 = index(\n    docs,\n    record_manager,\n    vectorstore,\n    cleanup=\"incremental\",\n    source_id_key=\"source\",\n)\n\nprint(\"Index attempt 3:\", index_3)\n```", "```py\n/** \n1\\. Ensure docker is installed and running (https://docs.docker.com/get-docker/)\n2\\. Run the following command to start the postgres container:\n\ndocker run \\\n --name pgvector-container \\\n -e POSTGRES_USER=langchain \\\n -e POSTGRES_PASSWORD=langchain \\\n -e POSTGRES_DB=langchain \\\n -p 6024:5432 \\\n -d pgvector/pgvector:pg16\n3\\. Use the connection string below for the postgres container\n*/\n\nimport { PostgresRecordManager } from '@langchain/community/indexes/postgres';\nimport { index } from 'langchain/indexes';\nimport { OpenAIEmbeddings } from '@langchain/openai';\nimport { PGVectorStore } from '@langchain/community/vectorstores/pgvector';\nimport { v4 as uuidv4 } from 'uuid';\n\nconst tableName = 'test_langchain';\nconst connectionString =\n  'postgresql://langchain:langchain@localhost:6024/langchain';\n// Load the document, split it into chunks\n\nconst config = {\n  postgresConnectionOptions: {\n    connectionString,\n  },\n  tableName: tableName,\n  columns: {\n    idColumnName: 'id',\n    vectorColumnName: 'vector',\n    contentColumnName: 'content',\n    metadataColumnName: 'metadata',\n  },\n};\n\nconst vectorStore = await PGVectorStore.initialize(\n  new OpenAIEmbeddings(),\n  config\n);\n\n// Create a new record manager\nconst recordManagerConfig = {\n  postgresConnectionOptions: {\n    connectionString,\n  },\n  tableName: 'upsertion_records',\n};\nconst recordManager = new PostgresRecordManager(\n  'test_namespace',\n  recordManagerConfig\n);\n\n// Create the schema if it doesn't exist\nawait recordManager.createSchema();\n\nconst docs = [\n  {\n    pageContent: 'there are cats in the pond',\n    metadata: { id: uuidv4(), source: 'cats.txt' },\n  },\n  {\n    pageContent: 'ducks are also found in the pond',\n    metadata: { id: uuidv4(), source: 'ducks.txt' },\n  },\n];\n\n// the first attempt will index both documents\nconst index_attempt_1 = await index({\n  docsSource: docs,\n  recordManager,\n  vectorStore,\n  options: {\n    // prevent duplicate documents by id from being indexed\n    cleanup: 'incremental',\n    // the key in the metadata that will be used to identify the document \n    sourceIdKey: 'source', \n  },\n});\n\nconsole.log(index_attempt_1);\n\n// the second attempt will skip indexing because the identical documents \n// already exist\nconst index_attempt_2 = await index({\n  docsSource: docs,\n  recordManager,\n  vectorStore,\n  options: {\n    cleanup: 'incremental',\n    sourceIdKey: 'source',\n  },\n});\n\nconsole.log(index_attempt_2);\n\n// If we mutate a document, the new version will be written and all old \n// versions sharing the same source will be deleted.\ndocs[0].pageContent = 'I modified the first document content';\nconst index_attempt_3 = await index({\n  docsSource: docs,\n  recordManager,\n  vectorStore,\n  options: {\n    cleanup: 'incremental',\n    sourceIdKey: 'source',\n  },\n});\n\nconsole.log(index_attempt_3);\n```", "```py\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_postgres.vectorstores import PGVector\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom pydantic import BaseModel\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.documents import Document\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain.storage import InMemoryStore\nimport uuid\n\nconnection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\ncollection_name = \"summaries\"\nembeddings_model = OpenAIEmbeddings()\n# Load the document\nloader = TextLoader(\"./test.txt\", encoding=\"utf-8\")\ndocs = loader.load()\n\nprint(\"length of loaded docs: \", len(docs[0].page_content))\n# Split the document\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nchunks = splitter.split_documents(docs)\n\n# The rest of your code remains the same, starting from:\nprompt_text = \"Summarize the following document:\\n\\n{doc}\"\n\nprompt = ChatPromptTemplate.from_template(prompt_text)\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\nsummarize_chain = {\n    \"doc\": lambda x: x.page_content} | prompt | llm | StrOutputParser()\n\n# batch the chain across the chunks\nsummaries = summarize_chain.batch(chunks, {\"max_concurrency\": 5})\n```", "```py\n# The vectorstore to use to index the child chunks\nvectorstore = PGVector(\n    embeddings=embeddings_model,\n    collection_name=collection_name,\n    connection=connection,\n    use_jsonb=True,\n)\n# The storage layer for the parent documents\nstore = InMemoryStore()\nid_key = \"doc_id\"\n\n# indexing the summaries in our vector store, whilst retaining the original \n# documents in our document store:\nretriever = MultiVectorRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    id_key=id_key,\n)\n\n# Changed from summaries to chunks since we need same length as docs\ndoc_ids = [str(uuid.uuid4()) for _ in chunks]\n\n# Each summary is linked to the original document by the doc_id\nsummary_docs = [\n    Document(page_content=s, metadata={id_key: doc_ids[i]})\n    for i, s in enumerate(summaries)\n]\n\n# Add the document summaries to the vector store for similarity search\nretriever.vectorstore.add_documents(summary_docs)\n\n# Store the original documents in the document store, linked to their summaries \n# via doc_ids\n# This allows us to first search summaries efficiently, then fetch the full \n# docs when needed\nretriever.docstore.mset(list(zip(doc_ids, chunks)))\n\n# vector store retrieves the summaries\nsub_docs = retriever.vectorstore.similarity_search(\n    \"chapter on philosophy\", k=2)\n```", "```py\n# Whereas the retriever will return the larger source document chunks:\nretrieved_docs = retriever.invoke(\"chapter on philosophy\")\n\n```", "```py\nimport * as uuid from 'uuid';\nimport { MultiVectorRetriever } from 'langchain/retrievers/multi_vector';\nimport { OpenAIEmbeddings } from '@langchain/openai';\nimport { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';\nimport { InMemoryStore } from '@langchain/core/stores';\nimport { TextLoader } from 'langchain/document_loaders/fs/text';\nimport { Document } from '@langchain/core/documents';\nimport { PGVectorStore } from '@langchain/community/vectorstores/pgvector';\nimport { ChatOpenAI } from '@langchain/openai';\nimport { PromptTemplate } from '@langchain/core/prompts';\nimport { RunnableSequence } from '@langchain/core/runnables';\nimport { StringOutputParser } from '@langchain/core/output_parsers';\n\nconst connectionString =\n  'postgresql://langchain:langchain@localhost:6024/langchain';\nconst collectionName = 'summaries';\n\nconst textLoader = new TextLoader('./test.txt');\nconst parentDocuments = await textLoader.load();\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10000,\n  chunkOverlap: 20,\n});\nconst docs = await splitter.splitDocuments(parentDocuments);\n\nconst prompt = PromptTemplate.fromTemplate(\n  `Summarize the following document:\\n\\n{doc}`\n);\n\nconst llm = new ChatOpenAI({ modelName: 'gpt-3.5-turbo' });\n\nconst chain = RunnableSequence.from([\n  { doc: (doc) => doc.pageContent },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n\n// batch summarization chain across the chunks\nconst summaries = await chain.batch(docs, {\n  maxConcurrency: 5,\n});\n\nconst idKey = 'doc_id';\nconst docIds = docs.map((_) => uuid.v4());\n// create summary docs with metadata linking to the original docs\nconst summaryDocs = summaries.map((summary, i) => {\n  const summaryDoc = new Document({\n    pageContent: summary,\n    metadata: {\n      [idKey]: docIds[i],\n    },\n  });\n  return summaryDoc;\n});\n\n// The byteStore to use to store the original chunks\nconst byteStore = new InMemoryStore();\n\n// vector store for the summaries\nconst vectorStore = await PGVectorStore.fromDocuments(\n  docs,\n  new OpenAIEmbeddings(),\n  {\n    postgresConnectionOptions: {\n      connectionString,\n    },\n  }\n);\n\nconst retriever = new MultiVectorRetriever({\n  vectorstore: vectorStore,\n  byteStore,\n  idKey,\n});\n\nconst keyValuePairs = docs.map((originalDoc, i) => [docIds[i], originalDoc]);\n\n// Use the retriever to add the original chunks to the document store\nawait retriever.docstore.mset(keyValuePairs);\n\n// Vectorstore alone retrieves the small chunks\nconst vectorstoreResult = await retriever.vectorstore.similaritySearch(\n  'chapter on philosophy',\n  2\n);\nconsole.log(`summary: ${vectorstoreResult[0].pageContent}`);\nconsole.log(\n  `summary retrieved length: ${vectorstoreResult[0].pageContent.length}`\n);\n\n// Retriever returns larger chunk result\nconst retrieverResult = await retriever.invoke('chapter on philosophy');\nconsole.log(\n  `multi-vector retrieved chunk length: ${retrieverResult[0].pageContent.length}`\n);\n```", "```py\n# RAGatouille is a library that makes it simple to use ColBERT\n#! pip install -U ragatouille\n\nfrom ragatouille import RAGPretrainedModel\nRAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n\nimport requests\n\ndef get_wikipedia_page(title: str):\n    \"\"\"\n Retrieve the full text content of a Wikipedia page.\n\n :param title: str - Title of the Wikipedia page.\n :return: str - Full text content of the page as raw string.\n \"\"\"\n    # Wikipedia API endpoint\n    URL = \"https://en.wikipedia.org/w/api.php\"\n\n    # Parameters for the API request\n    params = {\n        \"action\": \"query\",\n        \"format\": \"json\",\n        \"titles\": title,\n        \"prop\": \"extracts\",\n        \"explaintext\": True,\n    }\n\n    # Custom User-Agent header to comply with Wikipedia's best practices\n    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1\"}\n\n    response = requests.get(URL, params=params, headers=headers)\n    data = response.json()\n\n    # Extracting page content\n    page = next(iter(data[\"query\"][\"pages\"].values()))\n    return page[\"extract\"] if \"extract\" in page else None\n\nfull_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n\n## Create an index\nRAG.index(\n    collection=[full_document],\n    index_name=\"Miyazaki-123\",\n    max_document_length=180,\n    split_documents=True,\n)\n\n#query\nresults = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\nresults\n\n#utilize langchain retriever\nretriever = RAG.as_langchain_retriever(k=3)\nretriever.invoke(\"What animation studio did Miyazaki found?\")\n```"]