["```py\nfrom keras.datasets import mnist\nimport numpy as np\n\n(train_images, train_labels), _ = mnist.load_data()\ntrain_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype(\"float32\") / 255\n\ntrain_images_with_noise_channels = np.concatenate(\n    [train_images, np.random.random((len(train_images), 784))], axis=1\n)\n\ntrain_images_with_zeros_channels = np.concatenate(\n    [train_images, np.zeros((len(train_images), 784))], axis=1\n) \n```", "```py\nimport keras\nfrom keras import layers\n\ndef get_model():\n    model = keras.Sequential(\n        [\n            layers.Dense(512, activation=\"relu\"),\n            layers.Dense(10, activation=\"softmax\"),\n        ]\n    )\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\nmodel = get_model()\nhistory_noise = model.fit(\n    train_images_with_noise_channels,\n    train_labels,\n    epochs=10,\n    batch_size=128,\n    validation_split=0.2,\n)\n\nmodel = get_model()\nhistory_zeros = model.fit(\n    train_images_with_zeros_channels,\n    train_labels,\n    epochs=10,\n    batch_size=128,\n    validation_split=0.2,\n) \n```", "```py\n(train_images, train_labels), _ = mnist.load_data()\ntrain_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype(\"float32\") / 255\n\n# Copies train_labels\nrandom_train_labels = train_labels[:]\nnp.random.shuffle(random_train_labels)\n\nmodel = keras.Sequential(\n    [\n        layers.Dense(512, activation=\"relu\"),\n        layers.Dense(10, activation=\"softmax\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"rmsprop\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(\n    train_images,\n    random_train_labels,\n    epochs=100,\n    batch_size=128,\n    validation_split=0.2,\n) \n```", "```py\nnum_validation_samples = 10000\n# Shuffling the data is usually appropriate.\nnp.random.shuffle(data)\n# Defines the validation set\nvalidation_data = data[:num_validation_samples]\n# Defines the training set\ntraining_data = data[num_validation_samples:]\n# Trains a model on the training data and evaluates it on the\n# validation data\nmodel = get_model()\nmodel.fit(training_data, ...)\nvalidation_score = model.evaluate(validation_data, ...)\n\n# At this point, you can tune your model, retrain it, evaluate it, tune\n# it again, and so on.\n...\n\n# Once you've tuned your hyperparameters, it's common to train your\n# final model from scratch on all non-test data available.\nmodel = get_model()\nmodel.fit(\n    np.concatenate([training_data, validation_data]),\n    ...,\n)\ntest_score = model.evaluate(test_data, ...) \n```", "```py\nk = 3\nnum_validation_samples = len(data) // k\nnp.random.shuffle(data)\nvalidation_scores = []\nfor fold in range(k):\n    # Selects the validation-data partition\n    validation_data = data[\n        num_validation_samples * fold : num_validation_samples * (fold + 1)\n    ]\n    # Uses the remainder of the data as training data.\n    training_data = np.concatenate(\n        data[: num_validation_samples * fold],\n        data[num_validation_samples * (fold + 1) :],\n    )\n    # Creates a brand-new instance of the model (untrained)\n    model = get_model()\n    model.fit(training_data, ...)\n    validation_score = model.evaluate(validation_data, ...)\n    validation_scores.append(validation_score)\n# Validation score: average of the validation scores of the k folds\nvalidation_score = np.average(validation_scores)\n# Trains the final model on all non-test data available\nmodel = get_model()\nmodel.fit(data, ...)\ntest_score = model.evaluate(test_data, ...) \n```", "```py\n(train_images, train_labels), _ = mnist.load_data()\ntrain_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype(\"float32\") / 255\n\nmodel = keras.Sequential(\n    [\n        layers.Dense(512, activation=\"relu\"),\n        layers.Dense(10, activation=\"softmax\"),\n    ]\n)\nmodel.compile(\n    optimizer=keras.optimizers.RMSprop(learning_rate=1.0),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(\n    train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2\n) \n```", "```py\nmodel = keras.Sequential(\n    [\n        layers.Dense(512, activation=\"relu\"),\n        layers.Dense(10, activation=\"softmax\"),\n    ]\n)\nmodel.compile(\n    optimizer=keras.optimizers.RMSprop(learning_rate=1e-2),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(\n    train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2\n) \n```", "```py\nmodel = keras.Sequential([layers.Dense(10, activation=\"softmax\")])\nmodel.compile(\n    optimizer=\"rmsprop\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nhistory_small_model = model.fit(\n    train_images, train_labels, epochs=20, batch_size=128, validation_split=0.2\n) \n```", "```py\nimport matplotlib.pyplot as plt\n\nval_loss = history_small_model.history[\"val_loss\"]\nepochs = range(1, 21)\nplt.plot(epochs, val_loss, \"b-\", label=\"Validation loss\")\nplt.title(\"Validation loss for a model with insufficient capacity\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show() \n```", "```py\nmodel = keras.Sequential(\n    [\n        layers.Dense(128, activation=\"relu\"),\n        layers.Dense(128, activation=\"relu\"),\n        layers.Dense(10, activation=\"softmax\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"rmsprop\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nhistory_large_model = model.fit(\n    train_images,\n    train_labels,\n    epochs=20,\n    batch_size=128,\n    validation_split=0.2,\n) \n```", "```py\nmodel = keras.Sequential(\n    [\n        layers.Dense(2048, activation=\"relu\"),\n        layers.Dense(2048, activation=\"relu\"),\n        layers.Dense(2048, activation=\"relu\"),\n        layers.Dense(10, activation=\"softmax\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"rmsprop\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nhistory_very_large_model = model.fit(\n    train_images,\n    train_labels,\n    epochs=20,\n    # When training larger models, you can reduce the batch size to\n    # limit memory consumption.\n    batch_size=32,\n    validation_split=0.2,\n) \n```", "```py\nfrom keras.datasets import imdb\n\n(train_data, train_labels), _ = imdb.load_data(num_words=10000)\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.0\n    return results\n\ntrain_data = vectorize_sequences(train_data)\n\nmodel = keras.Sequential(\n    [\n        layers.Dense(16, activation=\"relu\"),\n        layers.Dense(16, activation=\"relu\"),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"rmsprop\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nhistory_original = model.fit(\n    train_data,\n    train_labels,\n    epochs=20,\n    batch_size=512,\n    validation_split=0.4,\n) \n```", "```py\nmodel = keras.Sequential(\n    [\n        layers.Dense(4, activation=\"relu\"),\n        layers.Dense(4, activation=\"relu\"),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"rmsprop\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nhistory_smaller_model = model.fit(\n    train_data,\n    train_labels,\n    epochs=20,\n    batch_size=512,\n    validation_split=0.4,\n) \n```", "```py\nmodel = keras.Sequential(\n    [\n        layers.Dense(512, activation=\"relu\"),\n        layers.Dense(512, activation=\"relu\"),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"rmsprop\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nhistory_larger_model = model.fit(\n    train_data,\n    train_labels,\n    epochs=20,\n    batch_size=512,\n    validation_split=0.4,\n) \n```", "```py\nfrom keras.regularizers import l2\n\nmodel = keras.Sequential(\n    [\n        layers.Dense(16, kernel_regularizer=l2(0.002), activation=\"relu\"),\n        layers.Dense(16, kernel_regularizer=l2(0.002), activation=\"relu\"),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"rmsprop\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nhistory_l2_reg = model.fit(\n    train_data,\n    train_labels,\n    epochs=20,\n    batch_size=512,\n    validation_split=0.4,\n) \n```", "```py\nfrom keras import regularizers\n\n# L1 regularization\nregularizers.l1(0.001)\n# Simultaneous L1 and L2 regularization\nregularizers.l1_l2(l1=0.001, l2=0.001) \n```", "```py\n# At training time, drops out 50% of the units in the output\nlayer_output *= np.random.randint(low=0, high=2, size=layer_output.shape) \n```", "```py\n# At test time\nlayer_output *= 0.5 \n```", "```py\n# At training time\nlayer_output *= np.random.randint(low=0, high=2, size=layer_output.shape)\n# Note that we're scaling up rather scaling down in this case.\nlayer_output /= 0.5 \n```", "```py\nmodel = keras.Sequential(\n    [\n        layers.Dense(16, activation=\"relu\"),\n        layers.Dropout(0.5),\n        layers.Dense(16, activation=\"relu\"),\n        layers.Dropout(0.5),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\nmodel.compile(\n    optimizer=\"rmsprop\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nhistory_dropout = model.fit(\n    train_data,\n    train_labels,\n    epochs=20,\n    batch_size=512,\n    validation_split=0.4,\n) \n```"]