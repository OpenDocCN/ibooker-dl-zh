<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">5</span></span> <span class="chapter-title-text">Pretraining on unlabeled data</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Computing the training and validation set losses to assess the quality of LLM-generated text during training</li> 
    <li class="readable-text" id="p3">Implementing a training function and pretraining the LLM</li> 
    <li class="readable-text" id="p4">Saving and loading model weights to continue training an LLM</li> 
    <li class="readable-text" id="p5">Loading pretrained weights from OpenAI</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>Thus far, we have implemented the data sampling and attention mechanism and coded the LLM architecture. It is now time to implement a training function and pretrain the LLM. We will learn about basic model evaluation techniques to measure the quality of the generated text, which is a requirement for optimizing the LLM during the training process. Moreover, we will discuss how to load pretrained weights, giving our LLM a solid starting point for fine-tuning. Figure 5.1 lays out our overall plan, highlighting what we will discuss in this chapter.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p7">  
   <img alt="figure" src="../Images/5-1.png" width="1012" height="474"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.1</span> The three main stages of coding an LLM. This chapter focuses on stage 2: pretraining the LLM (step 4), which includes implementing the training code (step 5), evaluating the performance (step 6), and saving and loading model weights (step 7).</h5>
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p8"> 
    <h5 class=" callout-container-h5 readable-text-h5">Weight parameters </h5> 
   </div> 
   <div class="readable-text" id="p9"> 
    <p>In the context of LLMs and other deep learning models, <em>weights</em> refer to the trainable parameters that the learning process adjusts. These weights are also known as <em>weight parameters</em> or simply <em>parameters</em>. In frameworks like PyTorch, these weights are stored in linear layers; we used these to implement the multi-head attention module in chapter 3 and the <code>GPTModel</code> in chapter 4. After initializing a layer (<code>new_layer</code> <code>=</code> <code>torch.nn.Linear(...)</code>), we can access its weights through the <code>.weight</code> attribute, <code>new_layer.weight</code>. Additionally, for convenience, PyTorch allows direct access to all a model’s trainable parameters, including weights and biases, through the method <code>model.parameters()</code>, which we will use later when implementing the model training.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p10"> 
   <h2 class=" readable-text-h2"><span class="num-string">5.1</span> Evaluating generative text models</h2> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>After briefly recapping the text generation from chapter 4, we will set up our LLM for text generation and then discuss basic ways to evaluate the quality of the generated text. We will then calculate the training and validation losses. Figure 5.2 shows the topics covered in this chapter, with these first three steps highlighted.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p12">  
   <img alt="figure" src="../Images/5-2.png" width="1100" height="486"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.2</span> An overview of the topics covered in this chapter. We begin by recapping text generation (step 1) before moving on to discuss basic model evaluation techniques (step 2) and training and validation losses (step 3).</h5>
  </div> 
  <div class="readable-text" id="p13"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.1.1</span> Using GPT to generate text</h3> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>Let’s set up the LLM and briefly recap the text generation process we implemented in chapter 4. We begin by initializing the GPT model that we will later evaluate and train using the <code>GPTModel</code> class and <code>GPT_CONFIG_124M</code> dictionary (see chapter 4):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p15"> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch
from chapter04 import GPTModel

GPT_CONFIG_124M = {
    "vocab_size": 50257,
    "context_length": 256,   <span class="aframe-location"/> #1
    "emb_dim": 768,
    "n_heads": 12,
    "n_layers": 12, 
    "drop_rate": 0.1,      <span class="aframe-location"/> #2
    "qkv_bias": False
}
torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()</pre> 
    <div class="code-annotations-overlay-container">
     #1 We shorten the context length from 1,024 to 256 tokens.
     <br/>#2 It’s possible and common to set dropout to 0.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>Considering the <code>GPT_CONFIG_124M</code> dictionary, the only adjustment we have made compared to the previous chapter is that we have reduced the context length (<code>context_ length</code>) to 256 tokens. This modification reduces the computational demands of training the model, making it possible to carry out the training on a standard laptop computer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>Originally, the GPT-2 model with 124 million parameters was configured to handle up to 1,024 tokens. After the training process, we will update the context size setting and load pretrained weights to work with a model configured for a 1,024-token context length.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>Using the <code>GPTModel</code> instance, we adopt the <code>generate_text_simple</code> function from chapter 4 and introduce two handy functions: <code>text_to_token_</code> <code>ids</code> and <code>token_ids_ to_text</code>. These functions facilitate the conversion between text and token representations, a technique we will utilize throughout this chapter. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p19">  
   <img alt="figure" src="../Images/5-3.png" width="1012" height="479"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.3</span> Generating text involves encoding text into token IDs that the LLM processes into logit vectors. The logit vectors are then converted back into token IDs, detokenized into a text representation.</h5>
  </div> 
  <div class="readable-text" id="p20"> 
   <p>Figure 5.3 illustrates a three-step text generation process using a GPT model. First, the tokenizer converts input text into a series of token IDs (see chapter 2). Second, the model receives these token IDs and generates corresponding logits, which are vectors representing the probability distribution for each token in the vocabulary (see chapter 4). Third, these logits are converted back into token IDs, which the tokenizer decodes into human-readable text, completing the cycle from textual input to textual output.</p> 
  </div> 
  <div class="readable-text intended-text" id="p21"> 
   <p>We can implement the text generation process, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p22"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.1</span> Utility functions for text to token ID conversion</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import tiktoken
from chapter04 import generate_text_simple

def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={'&lt;|endoftext|&gt;'})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)   <span class="aframe-location"/> #1
    return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)               <span class="aframe-location"/> #2
    return tokenizer.decode(flat.tolist())

start_context = "Every effort moves you"
tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(start_context, tokenizer),
    max_new_tokens=10,
    context_size=GPT_CONFIG_124M["context_length"]
)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer))</pre> 
    <div class="code-annotations-overlay-container">
     #1 .unsqueeze(0) adds the batch dimension
     <br/>#2 Removes batch dimension
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>Using this code, the <code>model</code> generates the following text:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p24"> 
   <div class="code-area-container"> 
    <pre class="code-area">Output text:
 Every effort moves you rentingetic wasnم refres RexMeCHicular stren</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>Clearly, the model isn’t yet producing coherent text because it hasn’t undergone training. To define what makes text “coherent” or “high quality,” we have to implement a numerical method to evaluate the generated content. This approach will enable us to monitor and enhance the model’s performance throughout its training process.</p> 
  </div> 
  <div class="readable-text intended-text" id="p26"> 
   <p>Next, we will calculate a <em>loss metric</em> for the generated outputs. This loss serves as a progress and success indicator of the training progress. Furthermore, in later chapters, when we fine-tune our LLM, we will review additional methodologies for assessing model quality.</p> 
  </div> 
  <div class="readable-text" id="p27"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.1.2</span> Calculating the text generation loss</h3> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>Next, let’s explore techniques for numerically assessing text quality generated during training by calculating a <em>text generation loss</em>. We will go over this topic step by step with a practical example to make the concepts clear and applicable, beginning with a short recap of how the data is loaded and how the text is generated via the <code>generate_text_simple</code> function. </p> 
  </div> 
  <div class="readable-text intended-text" id="p29"> 
   <p>Figure 5.4 illustrates the overall flow from input text to LLM-generated text using a five-step procedure. This text-generation process shows what the <code>generate_text_simple</code> function does internally. We need to perform these same initial steps before we can compute a loss that measures the generated text quality later in this section.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p30">  
   <img alt="figure" src="../Images/5-4.png" width="1002" height="525"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.4</span> For each of the three input tokens, shown on the left, we compute a vector containing probability scores corresponding to each token in the vocabulary. The index position of the highest probability score in each vector represents the most likely next token ID. These token IDs associated with the highest probability scores are selected and mapped back into a text that represents the text generated by the model.</h5>
  </div> 
  <div class="readable-text intended-text" id="p31"> 
   <p>Figure 5.4 outlines the text generation process with a small seven-token vocabulary to fit this image on a single page. However, our <code>GPTModel</code> works with a much larger vocabulary consisting of 50,257 words; hence, the token IDs in the following code will range from 0 to 50,256 rather than 0 to 6. </p> 
  </div> 
  <div class="readable-text intended-text" id="p32"> 
   <p>Also, figure 5.4 only shows a single text example (<code>"every</code> <code>effort</code> <code>moves"</code>) for simplicity. In the following hands-on code example that implements the steps in the figure, we will work with two input examples for the GPT model (<code>"every</code> <code>effort</code> <code>moves"</code> and <code>"I</code> <code>really</code> <code>like"</code>).</p> 
  </div> 
  <div class="readable-text intended-text" id="p33"> 
   <p>Consider these two input examples, which have already been mapped to token IDs (figure 5.4, step 1):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p34"> 
   <div class="code-area-container"> 
    <pre class="code-area">inputs = torch.tensor([[16833, 3626, 6100],   # ["every effort moves",
                       [40,    1107, 588]])   #  "I really like"]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>Matching these inputs, the <code>targets</code> contain the token IDs we want the model to produce:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p36"> 
   <div class="code-area-container"> 
    <pre class="code-area">targets = torch.tensor([[3626, 6100, 345  ],  # [" effort moves you",
                        [1107, 588, 11311]])  #  " really like chocolate"]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>Note that the targets are the inputs but shifted one position forward, a concept we covered in chapter 2 during the implementation of the data loader. This shifting strategy is crucial for teaching the model to predict the next token in a sequence.</p> 
  </div> 
  <div class="readable-text intended-text" id="p38"> 
   <p>Now we feed the inputs into the model to calculate logits vectors for the two input examples, each comprising three tokens. Then we apply the <code>softmax</code> function to transform these logits into probability scores (<code>probas</code>; figure 5.4, step 2):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p39"> 
   <div class="code-area-container"> 
    <pre class="code-area">with torch.no_grad():    <span class="aframe-location"/> #1
    logits = model(inputs)
probas = torch.softmax(logits, dim=-1)    <span class="aframe-location"/> #2
print(probas.shape)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Disables gradient tracking since we are not training yet
     <br/>#2 Probability of each token in vocabulary
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>The resulting tensor dimension of the probability score (<code>probas</code>) tensor is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p41"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.Size([2, 3, 50257])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>The first number, 2, corresponds to the two examples (rows) in the inputs, also known as batch size. The second number, 3, corresponds to the number of tokens in each input (row). Finally, the last number corresponds to the embedding dimensionality, which is determined by the vocabulary size. Following the conversion from logits to probabilities via the <code>softmax</code> function, the <code>generate_text_simple</code> function then converts the resulting probability scores back into text (figure 5.4, steps 3–5).</p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>We can complete steps 3 and 4 by applying the <code>argmax</code> function to the probability scores to obtain the corresponding token IDs:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p44"> 
   <div class="code-area-container"> 
    <pre class="code-area">token_ids = torch.argmax(probas, dim=-1, keepdim=True)
print("Token IDs:\n", token_ids)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>Given that we have two input batches, each containing three tokens, applying the <code>argmax</code> function to the probability scores (figure 5.4, step 3) yields two sets of outputs, each with three predicted token IDs:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p46"> 
   <div class="code-area-container"> 
    <pre class="code-area">Token IDs:
 tensor([[[16657],      <span class="aframe-location"/> #1
         [  339],
         [42826]],
        [[49906],       <span class="aframe-location"/> #2
         [29669],
         [41751]]])</pre> 
    <div class="code-annotations-overlay-container">
     #1 First batch
     <br/>#2 Second batch
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>Finally, step 5 converts the token IDs back into text:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p48"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(f"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}")
print(f"Outputs batch 1:"
      f" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>When we decode these tokens, we find that these output tokens are quite different from the target tokens we want the model to generate:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p50"> 
   <div class="code-area-container"> 
    <pre class="code-area">Targets batch 1:  effort moves you
Outputs batch 1:  Armed heNetflix</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>The model produces random text that is different from the target text because it has not been trained yet. We now want to evaluate the performance of the model’s generated text numerically via a loss (figure 5.5). Not only is this useful for measuring the quality of the generated text, but it’s also a building block for implementing the training function, which we will use to update the model’s weight to improve the generated text.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p52">  
   <img alt="figure" src="../Images/5-5.png" width="1012" height="422"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.5</span> An overview of the topics covered in this chapter. We have completed step 1. We are now ready to implement the text evaluation function (step 2).</h5>
  </div> 
  <div class="readable-text" id="p53"> 
   <p>Part of the text evaluation process that we implement, as shown in figure 5.5, is to measure “how far” the generated tokens are from the correct predictions (targets). The training function we implement later will use this information to adjust the model weights to generate text that is more similar to (or, ideally, matches) the target text.</p> 
  </div> 
  <div class="readable-text intended-text" id="p54"> 
   <p>The model training aims to increase the softmax probability in the index positions corresponding to the correct target token IDs, as illustrated in figure 5.6. This softmax probability is also used in the evaluation metric we will implement next to numerically assess the model’s generated outputs: the higher the probability in the correct positions, the better.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p55">  
   <img alt="figure" src="../Images/5-6.png" width="1005" height="544"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.6</span> Before training, the model produces random next-token probability vectors. The goal of model training is to ensure that the probability values corresponding to the highlighted target token IDs are maximized.</h5>
  </div> 
  <div class="readable-text intended-text" id="p56"> 
   <p>Remember that figure 5.6 displays the softmax probabilities for a compact seven-token vocabulary to fit everything into a single figure. This implies that the starting random values will hover around 1/7, which equals approximately 0.14. However, the vocabulary we are using for our GPT-2 model has 50,257 tokens, so most of the initial probabilities will hover around 0.00002 (1/50,257). </p> 
  </div> 
  <div class="readable-text" id="p57"> 
   <p>For each of the two input texts, we can print the initial softmax probability scores corresponding to the target tokens using the following code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p58"> 
   <div class="code-area-container"> 
    <pre class="code-area">text_idx = 0
target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 1:", target_probas_1)

text_idx = 1
target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 2:", target_probas_2)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>The three target token ID probabilities for each batch are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p60"> 
   <div class="code-area-container"> 
    <pre class="code-area">Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])
Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>The goal of training an LLM is to maximize the likelihood of the correct token, which involves increasing its probability relative to other tokens. This way, we ensure the LLM consistently picks the target token—essentially the next word in the sentence—as the next token it generates.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p62"> 
    <h5 class=" callout-container-h5 readable-text-h5">Backpropagation</h5> 
   </div> 
   <div class="readable-text" id="p63"> 
    <p>How do we maximize the softmax probability values corresponding to the target tokens? The big picture is that we update the model weights so that the model outputs higher values for the respective token IDs we want to generate. The weight update is done via a process called <em>backpropagation</em>, a standard technique for training deep neural networks (see sections A.3 to A.7 in appendix A for more details about backpropagation and model training).</p> 
   </div> 
   <div class="readable-text" id="p64"> 
    <p>Backpropagation requires a loss function, which calculates the difference between the model’s predicted output (here, the probabilities corresponding to the target token IDs) and the actual desired output. This loss function measures how far off the model’s predictions are from the target values.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p65"> 
   <p>Next, we will calculate the loss for the probability scores of the two example batches, <code>target_probas_1</code> and <code>target_probas_2</code>. The main steps are illustrated in figure 5.7. Since we already applied steps 1 to 3 to obtain <code>target_probas_1</code> and <code>target_ probas_2</code>, we proceed with step 4, applying the <em>logarithm</em> to the probability scores:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p66"> 
   <div class="code-area-container"> 
    <pre class="code-area">log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))
print(log_probas)<span class="aframe-location"/></pre>  
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p67">  
   <img alt="figure" src="../Images/5-7.png" width="817" height="352"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.7</span> Calculating the loss involves several steps. Steps 1 to 3, which we have already completed, calculate the token probabilities corresponding to the target tensors. These probabilities are then transformed via a logarithm and averaged in steps 4 to 6.</h5>
  </div> 
  <div class="readable-text" id="p68"> 
   <p>This results in the following values:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p69"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>Working with logarithms of probability scores is more manageable in mathematical optimization than handling the scores directly. This topic is outside the scope of this book, but I’ve detailed it further in a lecture, which can be found in appendix B.</p> 
  </div> 
  <div class="readable-text intended-text" id="p71"> 
   <p>Next, we combine these log probabilities into a single score by computing the average (step 5 in figure 5.7):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p72"> 
   <div class="code-area-container"> 
    <pre class="code-area">avg_log_probas = torch.mean(log_probas)
print(avg_log_probas)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>The resulting average log probability score is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p74"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor(-10.7940)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>The goal is to get the average log probability as close to 0 as possible by updating the model’s weights as part of the training process. However, in deep learning, the common practice isn’t to push the average log probability up to 0 but rather to bring the negative average log probability down to 0. The negative average log probability is simply the average log probability multiplied by –1, which corresponds to step 6 in figure 5.7:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p76"> 
   <div class="code-area-container"> 
    <pre class="code-area">neg_avg_log_probas = avg_log_probas * -1
print(neg_avg_log_probas)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>This prints <code>tensor(10.7940)</code>. In deep learning, the term for turning this negative value, –10.7940, into 10.7940, is known as the <em>cross entropy </em>loss. PyTorch comes in handy here, as it already has a built-in <code>cross_entropy</code> function that takes care of all these six steps in figure 5.7 for us.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p78"> 
    <h5 class=" callout-container-h5 readable-text-h5">Cross entropy loss</h5> 
   </div> 
   <div class="readable-text" id="p79"> 
    <p>At its core, the cross entropy loss is a popular measure in machine learning and deep learning that measures the difference between two probability distributions—typically, the true distribution of labels (here, tokens in a dataset) and the predicted distribution from a model (for instance, the token probabilities generated by an LLM). </p> 
   </div> 
   <div class="readable-text" id="p80"> 
    <p>In the context of machine learning and specifically in frameworks like PyTorch, the <code>cross_entropy</code> function computes this measure for discrete outcomes, which is similar to the negative average log probability of the target tokens given the model’s generated token probabilities, making the terms “cross entropy” and “negative average log probability” related and often used interchangeably in practice.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>Before we apply the <code>cross_entropy</code> function, let’s briefly recall the shape of the logits and target tensors:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p82"> 
   <div class="code-area-container"> 
    <pre class="code-area">print("Logits shape:", logits.shape)
print("Targets shape:", targets.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>The resulting shapes are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p84"> 
   <div class="code-area-container"> 
    <pre class="code-area">Logits shape: torch.Size([2, 3, 50257])
Targets shape: torch.Size([2, 3])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p85"> 
   <p>As we can see, the <code>logits</code> tensor has three dimensions: batch size, number of tokens, and vocabulary size. The <code>targets</code> tensor has two dimensions: batch size and number of tokens.</p> 
  </div> 
  <div class="readable-text intended-text" id="p86"> 
   <p>For the <code>cross_entropy</code> loss function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p87"> 
   <div class="code-area-container"> 
    <pre class="code-area">logits_flat = logits.flatten(0, 1)
targets_flat = targets.flatten()
print("Flattened logits:", logits_flat.shape)
print("Flattened targets:", targets_flat.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>The resulting tensor dimensions are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p89"> 
   <div class="code-area-container"> 
    <pre class="code-area">Flattened logits: torch.Size([6, 50257])
Flattened targets: torch.Size([6])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>Remember that the <code>targets</code> are the token IDs we want the LLM to generate, and the <code>logits</code> contain the unscaled model outputs before they enter the <code>softmax</code> function to obtain the probability scores.</p> 
  </div> 
  <div class="readable-text intended-text" id="p91"> 
   <p>Previously, we applied the <code>softmax</code> function, selected the probability scores corresponding to the target IDs, and computed the negative average log probabilities. PyTorch’s <code>cross_entropy</code> function will take care of all these steps for us:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p92"> 
   <div class="code-area-container"> 
    <pre class="code-area">loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)
print(loss)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>The resulting loss is the same that we obtained previously when applying the individual steps in figure 5.7 manually:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p94"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor(10.7940)</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p95"> 
    <h5 class=" callout-container-h5 readable-text-h5">Perplexity</h5> 
   </div> 
   <div class="readable-text" id="p96"> 
    <p><em>Perplexity</em> is a measure often used alongside cross entropy loss to evaluate the performance of models in tasks like language modeling. It can provide a more interpretable way to understand the uncertainty of a model in predicting the next token in a sequence. </p> 
   </div> 
   <div class="readable-text" id="p97"> 
    <p>Perplexity measures how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset. Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution.</p> 
   </div> 
   <div class="readable-text" id="p98"> 
    <p>Perplexity can be calculated as <code>perplexity</code> <code>=</code> <code>torch.exp(loss)</code>, which returns <code>tensor(48725.8203)</code> when applied to the previously calculated loss. </p> 
   </div> 
   <div class="readable-text" id="p99"> 
    <p>Perplexity is often considered more interpretable than the raw loss value because it signifies the effective vocabulary size about which the model is uncertain at each step. In the given example, this would translate to the model being unsure about which among 48,725 tokens in the vocabulary to generate as the next token.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>We have now calculated the loss for two small text inputs for illustration purposes. Next, we will apply the loss computation to the entire training and validation sets.</p> 
  </div> 
  <div class="readable-text" id="p101"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.1.3</span> Calculating the training and validation set losses</h3> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>We must first prepare the training and validation datasets that we will use to train the LLM. Then, as highlighted in figure 5.8, we will calculate the cross entropy for the training and validation sets, which is an important component of the model training process.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p103">  
   <img alt="figure" src="../Images/5-8.png" width="1012" height="422"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.8</span> Having completed steps 1 and 2, including computing the cross entropy loss, we can now apply this loss computation to the entire text dataset that we will use for model training.</h5>
  </div> 
  <div class="readable-text" id="p104"> 
   <p>To compute the loss on the training and validation datasets, we use a very small text dataset, the “The Verdict” short story by Edith Wharton, which we have already worked with in chapter 2. By selecting a text from the public domain, we circumvent any concerns related to usage rights. Additionally, using such a small dataset allows for the execution of code examples on a standard laptop computer in a matter of minutes, even without a high-end GPU, which is particularly advantageous for educational purposes. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p105"> 
   <p><span class="print-book-callout-head">Note</span>  Interested readers can also use the supplementary code for this book to prepare a larger-scale dataset consisting of more than 60,000 public domain books from Project Gutenberg and train an LLM on these (see appendix D for details).</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p106"> 
    <h5 class=" callout-container-h5 readable-text-h5">The cost of pretraining LLMs </h5> 
   </div> 
   <div class="readable-text" id="p107"> 
    <p>To put the scale of our project into perspective, consider the training of the 7 billion parameter Llama 2 model, a relatively popular openly available LLM. This model required 184,320 GPU hours on expensive A100 GPUs, processing 2 trillion tokens. At the time of writing, running an 8 × A100 cloud server on AWS costs around $30 per hour. A rough estimate puts the total training cost of such an LLM at around $690,000 (calculated as 184,320 hours divided by 8, then multiplied by $30).</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>The following code loads the “The Verdict” short story:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p109"> 
   <div class="code-area-container"> 
    <pre class="code-area">file_path = "the-verdict.txt"
with open(file_path, "r", encoding="utf-8") as file:
    text_data = file.read()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>After loading the dataset, we can check the number of characters and tokens in the dataset:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p111"> 
   <div class="code-area-container"> 
    <pre class="code-area">total_characters = len(text_data)
total_tokens = len(tokenizer.encode(text_data))
print("Characters:", total_characters)
print("Tokens:", total_tokens)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p113"> 
   <div class="code-area-container"> 
    <pre class="code-area">Characters: 20479
Tokens: 5145</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p114"> 
   <p>With just 5,145 tokens, the text might seem too small to train an LLM, but as mentioned earlier, it’s for educational purposes so that we can run the code in minutes instead of weeks. Plus, later we will load pretrained weights from OpenAI into our <code>GPTModel</code> code.</p> 
  </div> 
  <div class="readable-text intended-text" id="p115"> 
   <p>Next, we divide the dataset into a training and a validation set and use the data loaders from chapter 2 to prepare the batches for LLM training. This process is visualized in figure 5.9.<span class="aframe-location"/> Due to spatial constraints, we use a <code>max_length=6</code>. However, for the actual data loaders, we set the <code>max_length</code> equal to the 256-token context length that the LLM supports so that the LLM sees longer texts during training.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p116">  
   <img alt="figure" src="../Images/5-9.png" width="1012" height="759"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.9</span> When preparing the data loaders, we split the input text into training and validation set portions. Then we tokenize the text (only shown for the training set portion for simplicity) and divide the tokenized text into chunks of a user-specified length (here, 6). Finally, we shuffle the rows and organize the chunked text into batches (here, batch size 2), which we can use for model training.</h5>
  </div> 
  <div class="readable-text print-book-callout" id="p117"> 
   <p><span class="print-book-callout-head">Note</span>  We are training the model with training data presented in similarly sized chunks for simplicity and efficiency. However, in practice, it can also be beneficial to train an LLM with variable-length inputs to help the LLM to better generalize across different types of inputs when it is being used.</p> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>To implement the data splitting and loading, we first define a <code>train_ratio</code> to use 90% of the data for training and the remaining 10% as validation data for model evaluation during training:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p119"> 
   <div class="code-area-container"> 
    <pre class="code-area">train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>Using the <code>train_data</code> and <code>val_data</code> subsets, we can now create the respective data loader reusing the <code>create_dataloader_v1</code> code from chapter 2:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p121"> 
   <div class="code-area-container"> 
    <pre class="code-area">from chapter02 import create_dataloader_v1
torch.manual_seed(123)

train_loader = create_dataloader_v1(
    train_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=True,
    shuffle=True,
    num_workers=0
)
val_loader = create_dataloader_v1(
    val_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=False,
    shuffle=False,
    num_workers=0
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>We used a relatively small batch size to reduce the computational resource demand because we were working with a very small dataset. In practice, training LLMs with batch sizes of 1,024 or larger is not uncommon.</p> 
  </div> 
  <div class="readable-text intended-text" id="p123"> 
   <p>As an optional check, we can iterate through the data loaders to ensure that they were created correctly:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p124"> 
   <div class="code-area-container"> 
    <pre class="code-area">print("Train loader:")
for x, y in train_loader:
    print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
    print(x.shape, y.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p>We should see the following outputs:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p126"> 
   <div class="code-area-container"> 
    <pre class="code-area">Train loader:
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])

Validation loader:
torch.Size([2, 256]) torch.Size([2, 256])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p127"> 
   <p>Based on the preceding code output, we have nine training set batches with two samples and 256 tokens each. Since we allocated only 10% of the data for validation, there is only one validation batch consisting of two input examples. As expected, the input data (<code>x</code>) and target data (<code>y</code>) have the same shape (the batch size times the number of tokens in each batch) since the targets are the inputs shifted by one position, as discussed in chapter 2.</p> 
  </div> 
  <div class="readable-text intended-text" id="p128"> 
   <p>Next, we implement a utility function to calculate the cross entropy loss of a given batch returned via the training and validation loader:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p129"> 
   <div class="code-area-container"> 
    <pre class="code-area">def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch = input_batch.to(device)        <span class="aframe-location"/> #1
    target_batch = target_batch.to(device)      
    logits = model(input_batch)
    loss = torch.nn.functional.cross_entropy(
        logits.flatten(0, 1), target_batch.flatten()
    )
    return loss</pre> 
    <div class="code-annotations-overlay-container">
     #1 The transfer to a given device allows us to transfer the data to a GPU.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p130"> 
   <p>We can now use this <code>calc_loss_batch</code> utility function, which computes the loss for a single batch, to implement the following <code>calc_loss_loader</code> function that computes the loss over all the batches sampled by a given data loader.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p131"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.2</span> Function to compute the training and validation loss</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)    <span class="aframe-location"/> #1
    else:
        num_batches = min(num_batches, len(data_loader))  <span class="aframe-location"/> #2
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i &lt; num_batches:
            loss = calc_loss_batch(
                input_batch, target_batch, model, device
            )
            total_loss += loss.item()   <span class="aframe-location"/> #3
        else:
            break
    return total_loss / num_batches   <span class="aframe-location"/> #4</pre> 
    <div class="code-annotations-overlay-container">
     #1 Iteratives over all batches if no fixed num_batches is specified
     <br/>#2 Reduces the number of batches to match the total number of batches in the data loader if num_batches exceeds the number of batches in the data loader
     <br/>#3 Sums loss for each batch
     <br/>#4 Averages the loss over all batches
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>By default, the <code>calc_loss_loader</code> function iterates over all batches in a given data loader, accumulates the loss in the <code>total_loss</code> variable, and then computes and averages the loss over the total number of batches. Alternatively, we can specify a smaller number of batches via <code>num_batches</code> to speed up the evaluation during model training.</p> 
  </div> 
  <div class="readable-text intended-text" id="p133"> 
   <p>Let’s now see this <code>calc_loss_loader</code> function in action, applying it to the training and validation set loaders:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p134"> 
   <div class="code-area-container"> 
    <pre class="code-area">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)  <span class="aframe-location"/> #1
with torch.no_grad():                                       <span class="aframe-location"/> #2
    train_loss = calc_loss_loader(train_loader, model, device)   <span class="aframe-location"/> #3
    val_loss = calc_loss_loader(val_loader, model, device)
print("Training loss:", train_loss)
print("Validation loss:", val_loss)</pre> 
    <div class="code-annotations-overlay-container">
     #1 If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code.
     <br/>#2 Disables gradient tracking for efficiency because we are not training yet
     <br/>#3 Via the “device” setting, we ensure the data is loaded onto the same device as the LLM model.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>The resulting loss values are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p136"> 
   <div class="code-area-container"> 
    <pre class="code-area">Training loss: 10.98758347829183
Validation loss: 10.98110580444336</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>The loss values are relatively high because the model has not yet been trained. For comparison, the loss approaches 0 if the model learns to generate the next tokens as they appear in the training and validation sets.</p> 
  </div> 
  <div class="readable-text intended-text" id="p138"> 
   <p>Now that we have a way to measure the quality of the generated text, we will train the LLM to reduce this loss so that it becomes better at generating text, as illustrated in figure 5.10.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p139">  
   <img alt="figure" src="../Images/5-10.png" width="887" height="309"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.10</span> We have recapped the text generation process (step 1) and implemented basic model evaluation techniques (step 2) to compute the training and validation set losses (step 3). Next, we will go to the training functions and pretrain the LLM (step 4).</h5>
  </div> 
  <div class="readable-text" id="p140"> 
   <p>Next, we will focus on pretraining the LLM. After model training, we will implement alternative text generation strategies and save and load pretrained model weights.</p> 
  </div> 
  <div class="readable-text" id="p141"> 
   <h2 class=" readable-text-h2"><span class="num-string">5.2</span> Training an LLM</h2> 
  </div> 
  <div class="readable-text" id="p142"> 
   <p>It is finally time to implement the code for pretraining the LLM, our <code>GPTModel</code>. For this, we focus on a straightforward training loop to keep the code concise and readable. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p143"> 
   <p><span class="print-book-callout-head">Note</span>  Interested readers can learn about more advanced techniques, including <em>learning rate warmup</em>, <em>cosine annealing</em>, and <em>gradient clipping</em>, in appendix D.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p144">  
   <img alt="figure" src="../Images/5-11.png" width="672" height="690"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.11</span> A typical training loop for training deep neural networks in PyTorch consists of numerous steps, iterating over the batches in the training set for several epochs. In each loop, we calculate the loss for each training set batch to determine loss gradients, which we use to update the model weights so that the training set loss is minimized.</h5>
  </div> 
  <div class="readable-text" id="p145"> 
   <p>The flowchart in figure 5.11 depicts a typical PyTorch neural network training workflow, which we use for training an LLM. It outlines eight steps, starting with iterating over each epoch, processing batches, resetting gradients, calculating the loss and new gradients, and updating weights and concluding with monitoring steps like printing losses and generating text samples. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p146"> 
   <p><span class="print-book-callout-head">Note</span>  If you are relatively new to training deep neural networks with PyTorch and any of these steps are unfamiliar, consider reading sections A.5 to A.8 in appendix A.</p> 
  </div> 
  <div class="readable-text" id="p147"> 
   <p>We can implement this training flow via the <code>train_model_simple</code> function in code.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p148"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.3</span> The main function for pretraining LLMs</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def train_model_simple(model, train_loader, val_loader,
                       optimizer, device, num_epochs,
                       eval_freq, eval_iter, start_context, tokenizer):
    train_losses, val_losses, track_tokens_seen = [], [], []   <span class="aframe-location"/> #1
    tokens_seen, global_step = 0, -1

    for epoch in range(num_epochs):   <span class="aframe-location"/> #2
        model.train()
        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()  <span class="aframe-location"/> #3
            loss = calc_loss_batch(
                input_batch, target_batch, model, device
            )
            loss.backward()                    <span class="aframe-location"/> #4
            optimizer.step()                   <span class="aframe-location"/> #5
            tokens_seen += input_batch.numel()
            global_step += 1

            if global_step % eval_freq == 0:   <span class="aframe-location"/> #6
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                track_tokens_seen.append(tokens_seen)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, "
                      f"Val loss {val_loss:.3f}"
                )

        generate_and_print_sample(                     <span class="aframe-location"/> #7
            model, tokenizer, device, start_context
        )
    return train_losses, val_losses, track_tokens_seen</pre> 
    <div class="code-annotations-overlay-container">
     #1 Initializes lists to track losses and tokens seen
     <br/>#2 Starts the main training loop
     <br/>#3 Resets loss gradients from the previous batch iteration
     <br/>#4 Calculates loss gradients
     <br/>#5 Updates model weights using loss gradients
     <br/>#6 Optional evaluation step
     <br/>#7 Prints a sample text after each epoch
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p149"> 
   <p>Note that the <code>train_model_simple</code> function we just created uses two functions we have not defined yet: <code>evaluate_model</code> and <code>generate_and_print_sample</code>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p150"> 
   <p>The <code>evaluate_model</code> function corresponds to step 7 in figure 5.11. It prints the training and validation set losses after each model update so we can evaluate whether the training improves the model. More specifically, the <code>evaluate_model</code> function calculates the loss over the training and validation set while ensuring the model is in evaluation mode with gradient tracking and dropout disabled when calculating the loss over the training and validation sets:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p151"> 
   <div class="code-area-container"> 
    <pre class="code-area">def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval() <span class="aframe-location"/> #1
    with torch.no_grad():                             <span class="aframe-location"/> #2
        train_loss = calc_loss_loader(
            train_loader, model, device, num_batches=eval_iter
        )
        val_loss = calc_loss_loader(
            val_loader, model, device, num_batches=eval_iter
        )
    model.train()
    return train_loss, val_loss</pre> 
    <div class="code-annotations-overlay-container">
     #1 Dropout is disabled during evaluation for stable, reproducible results.
     <br/>#2 Disables gradient tracking, which is not required during evaluation, to reduce the computational overhead
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p152"> 
   <p>Similar to <code>evaluate_model</code>, the <code>generate_and_print_sample</code> function is a convenience function that we use to track whether the model improves during the training. In particular, the <code>generate_and_print_sample</code> function takes a text snippet (<code>start_context</code>) as input, converts it into token IDs, and feeds it to the LLM to generate a text sample using the <code>generate_text_simple</code> function we used earlier:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p153"> 
   <div class="code-area-container"> 
    <pre class="code-area">def generate_and_print_sample(model, tokenizer, device, start_context):
    model.eval()
    context_size = model.pos_emb.weight.shape[0]
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    with torch.no_grad():
        token_ids = generate_text_simple(
            model=model, idx=encoded,
            max_new_tokens=50, context_size=context_size
        )
    decoded_text = token_ids_to_text(token_ids, tokenizer)
    print(decoded_text.replace("\n", " "))     <span class="aframe-location"/> #1
    model.train()</pre> 
    <div class="code-annotations-overlay-container">
     #1 Compact print format
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p154"> 
   <p>While the <code>evaluate_model</code> function gives us a numeric estimate of the model’s training progress, this <code>generate_and_print_sample</code> text function provides a concrete text example generated by the model to judge its capabilities during training.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p155"> 
    <h5 class=" callout-container-h5 readable-text-h5">AdamW </h5> 
   </div> 
   <div class="readable-text" id="p156"> 
    <p><em>Adam</em> optimizers are a popular choice for training deep neural networks. However, in our training loop, we opt for the <em>AdamW</em> optimizer. AdamW is a variant of Adam that improves the weight decay approach, which aims to minimize model complexity and prevent overfitting by penalizing larger weights. This adjustment allows AdamW to achieve more effective regularization and better generalization; thus, AdamW is frequently used in the training of LLMs.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p157"> 
   <p>Let’s see this all in action by training a <code>GPTModel</code> instance for 10 epochs using an <code>AdamW</code> optimizer and the <code>train_model_simple</code> function we defined earlier: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p158"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(
     model.parameters(),          <span class="aframe-location"/> #1
    lr=0.0004, weight_decay=0.1
)
num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=5, eval_iter=5,
    start_context="Every effort moves you", tokenizer=tokenizer
)</pre> 
    <div class="code-annotations-overlay-container">
     #1 The .parameters() method returns all trainable weight parameters of the model.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p159"> 
   <p>Executing the <code>train_model_simple</code> function starts the training process, which takes about 5 minutes to complete on a MacBook Air or a similar laptop. The output printed during this execution is as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p160"> 
   <div class="code-area-container"> 
    <pre class="code-area">Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933
Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339
Every effort moves you,,,,,,,,,,,,.                                     
Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048
Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616
Every effort moves you, and, and, and, and, and, and, and, and, and, and,
 and, and, and, and, and, and, and, and, and, and, and, and,, and, and,
[...]                                                  <span class="aframe-location"/> #1
Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393
Every effort moves you?"  "Yes--quite insensible to the irony. She wanted
him vindicated--and by me!"  He laughed again, and threw back the 
window-curtains, I had the donkey. "There were days when I
Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452
Every effort moves you know," was one of the axioms he laid down across the
Sevres and silver of an exquisitely appointed luncheon-table, when, on a
later day, I had again run over from Monte Carlo; and Mrs. Gis</pre> 
    <div class="code-annotations-overlay-container">
     #1 Intermediate results removed to save space
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p161"> 
   <p>As we can see, the training loss improves drastically, starting with a value of 9.781 and converging to 0.391. The language skills of the model have improved quite a lot. In the beginning, the model is only able to append commas to the start context (<code>Every</code> <code>effort</code> <code>moves</code> <code>you,,,,,,,,,,,,</code>) or repeat the word <code>and</code>. At the end of the training, it can generate grammatically correct text. </p> 
  </div> 
  <div class="readable-text intended-text" id="p162"> 
   <p>Similar to the training set loss, we can see that the validation loss starts high (9.933) and decreases during the training. However, it never becomes as small as the training set loss and remains at 6.452 after the 10th epoch.</p> 
  </div> 
  <div class="readable-text intended-text" id="p163"> 
   <p>Before discussing the validation loss in more detail, let’s create a simple plot that shows the training and validation set losses side by side:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p164"> 
   <div class="code-area-container"> 
    <pre class="code-area">import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
    fig, ax1 = plt.subplots(figsize=(5, 3))
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(
        epochs_seen, val_losses, linestyle="-.", label="Validation loss"
    )
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")
    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
    ax2 = ax1.twiny()                  <span class="aframe-location"/> #1
    ax2.plot(tokens_seen, train_losses, alpha=0)    <span class="aframe-location"/> #2
    ax2.set_xlabel("Tokens seen")
    fig.tight_layout()
    plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates a second x-axis that shares the same y-axis
     <br/>#2 Invisible plot for aligning ticks
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p165"> 
   <p>The resulting training and validation loss plot is shown in figure 5.12. As we can see, both the training and validation losses start to improve for the first epoch. However, the losses start to diverge past the second epoch. This divergence and the fact that the validation loss is much larger than the training loss indicate that the model is overfitting to the training data. We can confirm that the model memorizes the training data verbatim by searching for the generated text snippets, such as <code>quite</code> <code>insensible</code> <code>to</code> <code>the</code> <code>irony</code> in the “The Verdict” text file. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p166">  
   <img alt="figure" src="../Images/5-12.png" width="597" height="339"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.12</span> At the beginning of the training, both the training and validation set losses sharply decrease, which is a sign that the model is learning. However, the training set loss continues to decrease past the second epoch, whereas the validation loss stagnates. This is a sign that the model is still learning, but it’s overfitting to the training set past epoch 2.</h5>
  </div> 
  <div class="readable-text" id="p167"> 
   <p>This memorization is expected since we are working with a very, very small training dataset and training the model for multiple epochs. Usually, it’s common to train a model on a much larger dataset for only one epoch. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p168"> 
   <p><span class="print-book-callout-head">Note</span>  As mentioned earlier, interested readers can try to train the model on 60,000 public domain books from Project Gutenberg, where this overfitting does not occur; see appendix B for details. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p169">  
   <img alt="figure" src="../Images/5-13.png" width="887" height="314"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.13</span> Our model can generate coherent text after implementing the training function. However, it often memorizes passages from the training set verbatim. Next, we will discuss strategies to generate more diverse output texts.</h5>
  </div> 
  <div class="readable-text" id="p170"> 
   <p>As illustrated in figure 5.13, we have completed four of our objectives for this chapter. Next, we will cover text generation strategies for LLMs to reduce training data memorization and increase the originality of the LLM-generated text before we cover weight loading and saving and loading pretrained weights from OpenAI’s GPT model.</p> 
  </div> 
  <div class="readable-text" id="p171"> 
   <h2 class=" readable-text-h2"><span class="num-string">5.3</span> Decoding strategies to control randomness</h2> 
  </div> 
  <div class="readable-text" id="p172"> 
   <p>Let’s look at text generation strategies (also called decoding strategies) to generate more original text. First, we will briefly revisit the <code>generate_text_simple</code> function that we used inside <code>generate_and_print_sample</code> earlier. Then we will cover two techniques,<em> temperature scaling</em> and <em>top-k sampling</em>, to improve this function.</p> 
  </div> 
  <div class="readable-text intended-text" id="p173"> 
   <p>We begin by transferring the model back from the GPU to the CPU since inference with a relatively small model does not require a GPU. Also, after training, we put the model into evaluation mode to turn off random components such as dropout:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p174"> 
   <div class="code-area-container"> 
    <pre class="code-area">model.to("cpu")
model.eval()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p175"> 
   <p>Next, we plug the <code>GPTModel</code> instance (<code>model</code>) into the <code>generate_text_simple</code> function, which uses the LLM to generate one token at a time:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p176"> 
   <div class="code-area-container"> 
    <pre class="code-area">tokenizer = tiktoken.get_encoding("gpt2")
token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids("Every effort moves you", tokenizer),
    max_new_tokens=25,
    context_size=GPT_CONFIG_124M["context_length"]
)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p177"> 
   <p>The generated text is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p178"> 
   <div class="code-area-container"> 
    <pre class="code-area">Output text:
Every effort moves you know," was one of the axioms he laid down across the
Sevres and silver of an exquisitely appointed lun</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p179"> 
   <p>As explained earlier, the generated token is selected at each generation step corresponding to the largest probability score among all tokens in the vocabulary. This means that the LLM will always generate the same outputs even if we run the preceding <code>generate_text_simple</code> function multiple times on the same start context (<code>Every</code> <code>effort</code> <code>moves</code> <code>you</code>).</p> 
  </div> 
  <div class="readable-text" id="p180"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.3.1</span> Temperature scaling</h3> 
  </div> 
  <div class="readable-text" id="p181"> 
   <p>Let’s now look at temperature scaling, a technique that adds a probabilistic selection process to the next-token generation task. Previously, inside the <code>generate_text_simple</code> function, we always sampled the token with the highest probability as the next token using <code>torch.argmax</code>, also known as <em>greedy decoding</em>. To generate text with more variety, we can replace <code>argmax</code> with a function that samples from a probability distribution (here, the probability scores the LLM generates for each vocabulary entry at each token generation step).</p> 
  </div> 
  <div class="readable-text intended-text" id="p182"> 
   <p>To illustrate the probabilistic sampling with a concrete example, let’s briefly discuss the next-token generation process using a very small vocabulary for illustration purposes:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p183"> 
   <div class="code-area-container"> 
    <pre class="code-area">vocab = { 
    "closer": 0,
    "every": 1, 
    "effort": 2, 
    "forward": 3,
    "inches": 4,
    "moves": 5, 
    "pizza": 6,
    "toward": 7,
    "you": 8,
} 
inverse_vocab = {v: k for k, v in vocab.items()}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p184"> 
   <p>Next, assume the LLM is given the start context <code>"every</code> <code>effort</code> <code>moves</code> <code>you"</code> and generates the following next-token logits:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p185"> 
   <div class="code-area-container"> 
    <pre class="code-area">next_token_logits = torch.tensor(
    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p186"> 
   <p>As discussed in chapter 4, inside <code>generate_text_simple</code>, we convert the logits into probabilities via the <code>softmax</code> function and obtain the token ID corresponding to the generated token via the <code>argmax</code> function, which we can then map back into text via the inverse vocabulary:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p187"> 
   <div class="code-area-container"> 
    <pre class="code-area">probas = torch.softmax(next_token_logits, dim=0)
next_token_id = torch.argmax(probas).item()
print(inverse_vocab[next_token_id])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p188"> 
   <p>Since the largest logit value and, correspondingly, the largest softmax probability score are in the fourth position (index position 3 since Python uses 0 indexing), the generated word is <code>"forward"</code>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p189"> 
   <p>To implement a probabilistic sampling process, we can now replace <code>argmax</code> with the <code>multinomial</code> function in PyTorch:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p190"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123) 
next_token_id = torch.multinomial(probas, num_samples=1).item()
print(inverse_vocab[next_token_id])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p191"> 
   <p>The printed output is <code>"forward"</code> just like before. What happened? The <code>multinomial</code> function samples the next token proportional to its probability score. In other words, <code>"forward"</code> is still the most likely token and will be selected by <code>multinomial</code> most of the time but not all the time. To illustrate this, let’s implement a function that repeats this sampling 1,000 times:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p192"> 
   <div class="code-area-container"> 
    <pre class="code-area">def print_sampled_tokens(probas):
    torch.manual_seed(123)
    sample = [torch.multinomial(probas, num_samples=1).item()
             for i in range(1_000)]
    sampled_ids = torch.bincount(torch.tensor(sample))
    for i, freq in enumerate(sampled_ids):
        print(f"{freq} x {inverse_vocab[i]}")

print_sampled_tokens(probas)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p193"> 
   <p>The sampling output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p194"> 
   <div class="code-area-container"> 
    <pre class="code-area">73 x closer
0 x every
0 x effort
582 x forward
2 x inches
0 x moves
0 x pizza
343 x toward</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p195"> 
   <p>As we can see, the word <code>forward</code> is sampled most of the time (582 out of 1,000 times), but other tokens such as <code>closer</code>, <code>inches</code>, and <code>toward</code> will also be sampled some of the time. This means that if we replaced the <code>argmax</code> function with the <code>multinomial</code> function inside the <code>generate_and_print_sample</code> function, the LLM would sometimes generate texts such as <code>every</code> <code>effort</code> <code>moves</code> <code>you</code> <code>toward</code>, <code>every</code> <code>effort</code> <code>moves</code> <code>you</code> <code>inches</code>, and <code>every</code> <code>effort</code> <code>moves</code> <code>you</code> <code>closer</code> instead of <code>every</code> <code>effort</code> <code>moves</code> <code>you</code> <code>forward</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p196"> 
   <p>We can further control the distribution and selection process via a concept called <em>temperature scaling.</em> Temperature scaling is just a fancy description for dividing the logits by a number greater than 0:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p197"> 
   <div class="code-area-container"> 
    <pre class="code-area">def softmax_with_temperature(logits, temperature):
    scaled_logits = logits / temperature
    return torch.softmax(scaled_logits, dim=0)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p198"> 
   <p>Temperatures greater than 1 result in more uniformly distributed token probabilities, and temperatures smaller than 1 will result in more confident (sharper or more peaky) distributions. Let’s illustrate this by plotting the original probabilities alongside probabilities scaled with different temperature values:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p199"> 
   <div class="code-area-container"> 
    <pre class="code-area">temperatures = [1, 0.1, 5]                                    <span class="aframe-location"/> #1
scaled_probas = [softmax_with_temperature(next_token_logits, T)
                for T in temperatures]
x = torch.arange(len(vocab))
bar_width = 0.15
fig, ax = plt.subplots(figsize=(5, 3))
for i, T in enumerate(temperatures):
    rects = ax.bar(x + i * bar_width, scaled_probas[i], 
                   bar_width, label=f'Temperature = {T}')
ax.set_ylabel('Probability')
ax.set_xticks(x)
ax.set_xticklabels(vocab.keys(), rotation=90)
ax.legend()
plt.tight_layout()
plt.show()</pre> 
    <div class="code-annotations-overlay-container">
     #1 Original, lower, and higher confidence
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p200"> 
   <p>The resulting plot is shown in figure 5.14.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p201">  
   <img alt="figure" src="../Images/5-14.png" width="996" height="571"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.14</span> A temperature of 1 represents the unscaled probability scores for each token in the vocabulary. Decreasing the temperature to 0.1 sharpens the distribution, so the most likely token (here, “forward”) will have an even higher probability score. Likewise, increasing the temperature to 5 makes the distribution more uniform.</h5>
  </div> 
  <div class="readable-text intended-text" id="p202"> 
   <p>A temperature of 1 divides the logits by 1 before passing them to the <code>softmax</code> function to compute the probability scores. In other words, using a temperature of 1 is the same as not using any temperature scaling. In this case, the tokens are selected with a probability equal to the original softmax probability scores via the <code>multinomial</code> sampling function in PyTorch. For example, for the temperature setting 1, the token corresponding to “forward” would be selected about 60% of the time, as we can see in figure 5.14.</p> 
  </div> 
  <div class="readable-text" id="p203"> 
   <p>Also, as we can see in figure 5.14, applying very small temperatures, such as 0.1, will result in sharper distributions such that the behavior of the <code>multinomial</code> function selects the most likely token (here, <code>"forward"</code>) almost 100% of the time, approaching the behavior of the <code>argmax</code> function. Likewise, a temperature of 5 results in a more uniform distribution where other tokens are selected more often. This can add more variety to the generated texts but also more often results in nonsensical text. For example, using the temperature of 5 results in texts such as <code>every</code> <code>effort</code> <code>moves</code> <code>you</code> <code>pizza</code> about 4% of the time.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p204"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 5.1</h5> 
   </div> 
   <div class="readable-text" id="p205"> 
    <p>Use the <code>print_sampled_tokens</code> function to print the sampling frequencies of the softmax probabilities scaled with the temperatures shown in figure 5.14. How often is the word <code>pizza</code> sampled in each case? Can you think of a faster and more accurate way to determine how often the word <code>pizza</code> is sampled?</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p206"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.3.2</span> Top-k sampling</h3> 
  </div> 
  <div class="readable-text" id="p207"> 
   <p>We’ve now implemented a probabilistic sampling approach coupled with temperature scaling to increase the diversity of the outputs. We saw that higher temperature values result in more uniformly distributed next-token probabilities, which result in more diverse outputs as it reduces the likelihood of the model repeatedly selecting the most probable token. This method allows for the exploring of less likely but potentially more interesting and creative paths in the generation process. However, one downside of this approach is that it sometimes leads to grammatically incorrect or completely nonsensical outputs such as <code>every</code> <code>effort</code> <code>moves</code> <code>you</code> <code>pizza</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p208"> 
   <p><em>Top-k sampling</em>, when combined with probabilistic sampling and temperature scaling, can improve the text generation results. In top-k sampling, we can restrict the sampled tokens to the top-k most likely tokens and exclude all other tokens from the selection process by masking their probability scores, as illustrated in figure 5.15.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p209">  
   <img alt="figure" src="../Images/5-15.png" width="1005" height="624"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.15</span> Using top-k sampling with k = 3, we focus on the three tokens associated with the highest logits and mask out all other tokens with negative infinity (<code>–inf</code>) before applying the <code>softmax</code> function. This results in a probability distribution with a probability value 0 assigned to all non-top-k tokens. (The numbers in this figure are truncated to two digits after the decimal point to reduce visual clutter. The values in the “Softmax” row should add up to 1.0.)</h5>
  </div> 
  <div class="readable-text" id="p210"> 
   <p>The top-k approach replaces all nonselected logits with negative infinity value (<code>-inf</code>), such that when computing the softmax values, the probability scores of the non-top-k tokens are 0, and the remaining probabilities sum up to 1. (Careful readers may remember this masking trick from the causal attention module we implemented in chapter 3, section 3.5.1.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p211"> 
   <p>In code, we can implement the top-k procedure in figure 5.15 as follows, starting with the selection of the tokens with the largest logit values:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p212"> 
   <div class="code-area-container"> 
    <pre class="code-area">top_k = 3
top_logits, top_pos = torch.topk(next_token_logits, top_k)
print("Top logits:", top_logits)
print("Top positions:", top_pos)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p213"> 
   <p>The logits values and token IDs of the top three tokens, in descending order, are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p214"> 
   <div class="code-area-container"> 
    <pre class="code-area">Top logits: tensor([6.7500, 6.2800, 4.5100])
Top positions: tensor([3, 7, 0])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p215"> 
   <p>Subsequently, we apply PyTorch’s <code>where</code> function to set the logit values of tokens that are below the lowest logit value within our top-three selection to negative infinity (<code>-inf</code>): </p> 
  </div> 
  <div class="browsable-container listing-container" id="p216"> 
   <div class="code-area-container"> 
    <pre class="code-area">new_logits = torch.where(
    condition=next_token_logits &lt; top_logits[-1],   <span class="aframe-location"/> #1
    input=torch.tensor(float('-inf')),    <span class="aframe-location"/> #2
    other=next_token_logits    <span class="aframe-location"/> #3
)
print(new_logits)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Identifies logits less than the minimum in the top 3
     <br/>#2 Assigns –inf to these lower logits
     <br/>#3 Retains the original logits for all other tokens
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p217"> 
   <p>The resulting logits for the next token in the nine-token vocabulary are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p218"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,
     -inf])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p219"> 
   <p>Lastly, let’s apply the <code>softmax</code> function to turn these into next-token probabilities:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p220"> 
   <div class="code-area-container"> 
    <pre class="code-area">topk_probas = torch.softmax(new_logits, dim=0)
print(topk_probas)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p221"> 
   <p>As we can see, the result of this top-three approach are three non-zero probability scores:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p222"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610,
        0.0000])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p223"> 
   <p>We can now apply the temperature scaling and multinomial function for probabilistic sampling to select the next token among these three non-zero probability scores to generate the next token. We do this next by modifying the text generation function.</p> 
  </div> 
  <div class="readable-text" id="p224"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.3.3</span> Modifying the text generation function</h3> 
  </div> 
  <div class="readable-text" id="p225"> 
   <p>Now, let’s combine temperature sampling and top-k sampling to modify the <code>generate_ text_simple</code> function we used to generate text via the LLM earlier, creating a new <code>generate</code> function.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p226"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.4</span> A modified text generation function with more diversity</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def generate(model, idx, max_new_tokens, context_size,
             temperature=0.0, top_k=None, eos_id=None):
    for _ in range(max_new_tokens):           <span class="aframe-location"/> #1
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]
        if top_k is not None:               <span class="aframe-location"/> #2
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(
                logits &lt; min_val,
                torch.tensor(float('-inf')).to(logits.device),
                logits
            )
        if temperature &gt; 0.0:                 <span class="aframe-location"/> #3
            logits = logits / temperature
            probs = torch.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
        else:   <span class="aframe-location"/> #4
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)
        if idx_next == eos_id:             <span class="aframe-location"/> #5
            break
        idx = torch.cat((idx, idx_next), dim=1)
    return idx</pre> 
    <div class="code-annotations-overlay-container">
     #1 The for loop is the same as before: gets logits and only focuses on the last time step.
     <br/>#2 Filters logits with top_k sampling
     <br/>#3 Applies temperature scaling
     <br/>#4 Carries out greedy next-token selection as before when temperature scaling is disabled
     <br/>#5 Stops generating early if end-of-sequence token is encountered
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p227"> 
   <p>Let’s now see this new <code>generate</code> function in action:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p228"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
token_ids = generate(
    model=model,
    idx=text_to_token_ids("Every effort moves you", tokenizer),
    max_new_tokens=15,
    context_size=GPT_CONFIG_124M["context_length"],
    top_k=25,
    temperature=1.4
)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p229"> 
   <p>The generated text is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p230"> 
   <div class="code-area-container"> 
    <pre class="code-area">Output text:
 Every effort moves you stand to work on surprise, a one of us had gone
 with random-</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p231"> 
   <p>As we can see, the generated text is very different from the one we previously generated via the <code>generate_simple</code> function in section 5.3 (<code>"Every</code> <code>effort</code> <code>moves</code> <code>you</code> <code>know,"</code> <code>was</code> <code>one</code> <code>of</code> <code>the</code> <code>axioms</code> <code>he</code> <code>laid...!</code> ), which was a memorized passage from the training set.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p232"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 5.2</h5> 
   </div> 
   <div class="readable-text" id="p233"> 
    <p>Play around with different temperatures and top-k settings. Based on your observations, can you think of applications where lower temperature and top-k settings are desired? Likewise, can you think of applications where higher temperature and top-k settings are preferred? (It’s recommended to also revisit this exercise at the end of the chapter after loading the pretrained weights from OpenAI.)</p> 
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p234"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 5.3</h5> 
   </div> 
   <div class="readable-text" id="p235"> 
    <p>What are the different combinations of settings for the <code>generate</code> function to force deterministic behavior, that is, disabling the random sampling such that it always produces the same outputs similar to the <code>generate_simple</code> function?</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p236"> 
   <h2 class=" readable-text-h2"><span class="num-string">5.4</span> Loading and saving model weights in PyTorch</h2> 
  </div> 
  <div class="readable-text" id="p237"> 
   <p>Thus far, we have discussed how to numerically evaluate the training progress and pretrain an LLM from scratch. Even though both the LLM and dataset were relatively small, this exercise showed that pretraining LLMs is computationally expensive. Thus, it is important to be able to save the LLM so that we don’t have to rerun the training every time we want to use it in a new session. </p> 
  </div> 
  <div class="readable-text intended-text" id="p238"> 
   <p>So, let’s discuss how to save and load a pretrained model, as highlighted in figure 5.16. Later, we will load a more capable pretrained GPT model from OpenAI into our <code>GPTModel</code> instance.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p239">  
   <img alt="figure" src="../Images/5-16.png" width="887" height="314"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.16</span> After training and inspecting the model, it is often helpful to save the model so that we can use or continue training it later (step 6).</h5>
  </div> 
  <div class="readable-text" id="p240"> 
   <p>Fortunately, saving a PyTorch model is relatively straightforward. The recommended way is to save a model’s <code>state_dict</code>, a dictionary mapping each layer to its parameters, using the <code>torch.save</code> function:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p241"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.save(model.state_dict(), "model.pth")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p242"> 
   <p><code>"model.pth"</code> is the filename where the <code>state_dict</code> is saved. The <code>.pth</code> extension is a convention for PyTorch files, though we could technically use any file extension. </p> 
  </div> 
  <div class="readable-text intended-text" id="p243"> 
   <p>Then, after saving the model weights via the <code>state_dict</code>, we can load the model weights into a new <code>GPTModel</code> model instance:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p244"> 
   <div class="code-area-container"> 
    <pre class="code-area">model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(torch.load("model.pth", map_location=device))
model.eval()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p245"> 
   <p>As discussed in chapter 4, dropout helps prevent the model from overfitting to the training data by randomly “dropping out” of a layer’s neurons during training. However, during inference, we don’t want to randomly drop out any of the information the network has learned. Using <code>model.eval()</code> switches the model to evaluation mode for inference, disabling the dropout layers of the <code>model</code>. If we plan to continue pretraining a model later—for example, using the <code>train_model_simple</code> function we defined earlier in this chapter—saving the optimizer state is also recommended.</p> 
  </div> 
  <div class="readable-text intended-text" id="p246"> 
   <p>Adaptive optimizers such as AdamW store additional parameters for each model weight. AdamW uses historical data to adjust learning rates for each model parameter dynamically. Without it, the optimizer resets, and the model may learn suboptimally or even fail to converge properly, which means it will lose the ability to generate coherent text. Using <code>torch.save</code>, we can save both the model and optimizer <code>state_dict</code> contents:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p247"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.save({
    "model_state_dict": model.state_dict(),
    "optimizer_state_dict": optimizer.state_dict(),
    }, 
    "model_and_optimizer.pth"
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p248"> 
   <p>Then we can restore the model and optimizer states by first loading the saved data via <code>torch.load</code> and then using the <code>load_state_dict</code> method:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p249"> 
   <div class="code-area-container"> 
    <pre class="code-area">checkpoint = torch.load("model_and_optimizer.pth", map_location=device)
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint["model_state_dict"])
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
model.train();</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p250"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 5.4</h5> 
   </div> 
   <div class="readable-text" id="p251"> 
    <p>After saving the weights, load the model and optimizer in a new Python session or Jupyter notebook file and continue pretraining it for one more epoch using the <code>train_model_simple</code> function. </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p252"> 
   <h2 class=" readable-text-h2"><span class="num-string">5.5</span> Loading pretrained weights from OpenAI</h2> 
  </div> 
  <div class="readable-text" id="p253"> 
   <p>Previously, we trained a small GPT-2 model using a limited dataset comprising a short-story book. This approach allowed us to focus on the fundamentals without the need for extensive time and computational resources. </p> 
  </div> 
  <div class="readable-text intended-text" id="p254"> 
   <p>Fortunately, OpenAI openly shared the weights of their GPT-2 models, thus eliminating the need to invest tens to hundreds of thousands of dollars in retraining the model on a large corpus ourselves. So, let’s load these weights into our <code>GPTModel</code> class and use the model for text generation. Here, <em>weights</em> refer to the weight parameters stored in the <code>.weight</code> attributes of PyTorch’s <code>Linear</code> and <code>Embedding</code> layers, for example. We accessed them earlier via <code>model.parameters()</code> when training the model. In chapter 6, will reuse these pretrained weights to fine-tune the model for a text classification task and follow instructions similar to ChatGPT.</p> 
  </div> 
  <div class="readable-text intended-text" id="p255"> 
   <p>Note that OpenAI originally saved the GPT-2 weights via TensorFlow, which we have to install to load the weights in Python. The following code will use a progress bar tool called <code>tqdm</code> to track the download process, which we also have to install.</p> 
  </div> 
  <div class="readable-text intended-text" id="p256"> 
   <p>You can install these libraries by executing the following command in your terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p257"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install tensorflow&gt;=2.15.0  tqdm&gt;=4.66</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p258"> 
   <p>The download code is relatively long, mostly boilerplate, and not very interesting. Hence, instead of devoting precious space to discussing Python code for fetching files from the internet, we download the <code>gpt_download.py</code> Python module directly from this chapter’s online repository:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p259"> 
   <div class="code-area-container"> 
    <pre class="code-area">import urllib.request
url = (
    "https://raw.githubusercontent.com/rasbt/"
    "LLMs-from-scratch/main/ch05/"
    "01_main-chapter-code/gpt_download.py"
)
filename = url.split('/')[-1]
urllib.request.urlretrieve(url, filename)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p260"> 
   <p>Next, after downloading this file to the local directory of your Python session, you should briefly inspect the contents of this file to ensure that it was saved correctly and contains valid Python code.</p> 
  </div> 
  <div class="readable-text intended-text" id="p261"> 
   <p>We can now import the <code>download_and_load_gpt2</code> function from the <code>gpt_download .py</code> file as follows, which will load the GPT-2 architecture settings (<code>settings</code>) and weight parameters (<code>params</code>) into our Python session:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p262"> 
   <div class="code-area-container"> 
    <pre class="code-area">from gpt_download import download_and_load_gpt2
settings, params = download_and_load_gpt2(
    model_size="124M", models_dir="gpt2"
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p263"> 
   <p>Executing this code downloads the following seven files associated with the <code>124M</code> parameter GPT-2 model:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p264"> 
   <div class="code-area-container"> 
    <pre class="code-area">checkpoint: 100%|███████████████████████████| 77.0/77.0 [00:00&lt;00:00, 
                                                         63.9kiB/s]
encoder.json: 100%|█████████████████████████| 1.04M/1.04M [00:00&lt;00:00,
                                                           2.20MiB/s]
hprams.json: 100%|██████████████████████████| 90.0/90.0 [00:00&lt;00:00,
                                                         78.3kiB/s]
model.ckpt.data-00000-of-00001: 100%|███████| 498M/498M [01:09&lt;00:00,
                                                         7.16MiB/s]
model.ckpt.index: 100%|█████████████████████| 5.21k/5.21k [00:00&lt;00:00,
                                                           3.24MiB/s]
model.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00&lt;00:00, 
                                                         2.46MiB/s]
vocab.bpe: 100%|████████████████████████████| 456k/456k [00:00&lt;00:00,
                                                         1.70MiB/s]</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p265"> 
   <p><span class="print-book-callout-head">Note</span>  If the download code does not work for you, it could be due to intermittent internet connection, server problems, or changes in how OpenAI shares the weights of the open-source GPT-2 model. In this case, please visit this chapter’s online code repository at <a href="https://github.com/rasbt/LLMs-from-scratch">https://github.com/rasbt/LLMs-from-scratch</a> for alternative and updated instructions, and reach out via the Manning Forum for further questions. </p> 
  </div> 
  <div class="readable-text" id="p266"> 
   <p>Assuming the execution of the previous code has completed, let’s inspect the contents of <code>settings</code> and <code>params</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p267"> 
   <div class="code-area-container"> 
    <pre class="code-area">print("Settings:", settings)
print("Parameter dictionary keys:", params.keys())</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p268"> 
   <p>The contents are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p269"> 
   <div class="code-area-container"> 
    <pre class="code-area">Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12,
           'n_layer': 12}
Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p270"> 
   <p>Both <code>settings</code> and <code>params</code> are Python dictionaries. The <code>settings</code> dictionary stores the LLM architecture settings similarly to our manually defined <code>GPT_CONFIG_124M</code> settings. The <code>params</code> dictionary contains the actual weight tensors. Note that we only printed the dictionary keys because printing the weight contents would take up too much screen space; however, we can inspect these weight tensors by printing the whole dictionary via <code>print(params)</code> or by selecting individual tensors via the respective dictionary keys, for example, the embedding layer weights:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p271"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(params["wte"])
print("Token embedding weight tensor dimensions:", params["wte"].shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p272"> 
   <p>The weights of the token embedding layer are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p273"> 
   <div class="code-area-container"> 
    <pre class="code-area">[[-0.11010301 ... -0.1363697   0.01506208   0.04531523]
 [ 0.04034033 ...  0.08605453  0.00253983   0.04318958]
 [-0.12746179  ...  0.08991534 -0.12972379 -0.08785918]
 ...
 [-0.04453601 ...   0.10435229  0.09783269 -0.06952604]
 [ 0.1860082  ...  -0.09625227  0.07847701 -0.02245961]
 [ 0.05135201 ...   0.00704835  0.15519823  0.12067825]]
Token embedding weight tensor dimensions: (50257, 768)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p274"> 
   <p>We downloaded and loaded the weights of the smallest GPT-2 model via the <code>download_ and_load_gpt2(model_size="124M",</code> <code>...)</code> setting. OpenAI also shares the weights of larger models: <code>355M</code>, <code>774M</code>, and <code>1558M</code>. The overall architecture of these differently sized GPT models is the same, as illustrated in figure 5.17,<span class="aframe-location"/> except that different architectural elements are repeated different numbers of times and the embedding size differs. The remaining code in this chapter is also compatible with these larger models.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p275">  
   <img alt="figure" src="../Images/5-17.png" width="1017" height="1019"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.17</span> GPT-2 LLMs come in several different model sizes, ranging from 124 million to 1,558 million parameters. The core architecture is the same, with the only difference being the embedding sizes and the number of times individual components like the attention heads and transformer blocks are repeated.</h5>
  </div> 
  <div class="readable-text intended-text" id="p276"> 
   <p>After loading the GPT-2 model weights into Python, we still need to transfer them from the <code>settings</code> and <code>params</code> dictionaries into our <code>GPTModel</code> instance. First, we create a dictionary that lists the differences between the different GPT model sizes in figure 5.17:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p277"> 
   <div class="code-area-container"> 
    <pre class="code-area">model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p278"> 
   <p>Suppose we are interested in loading the smallest model, <code>"gpt2-small</code> <code>(124M)"</code>. We can use the corresponding settings from the <code>model_configs</code> table to update our full-length <code>GPT_CONFIG_124M</code> we defined and used earlier:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p279"> 
   <div class="code-area-container"> 
    <pre class="code-area">model_name = "gpt2-small (124M)"
NEW_CONFIG = GPT_CONFIG_124M.copy()
NEW_CONFIG.update(model_configs[model_name])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p280"> 
   <p>Careful readers may remember that we used a 256-token length earlier, but the original GPT-2 models from OpenAI were trained with a 1,024-token length, so we have to update the <code>NEW_CONFIG</code> accordingly:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p281"> 
   <div class="code-area-container"> 
    <pre class="code-area">NEW_CONFIG.update({"context_length": 1024})</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p282"> 
   <p>Also, OpenAI used bias vectors in the multi-head attention module’s linear layers to implement the query, key, and value matrix computations. Bias vectors are not commonly used in LLMs anymore as they don’t improve the modeling performance and are thus unnecessary. However, since we are working with pretrained weights, we need to match the settings for consistency and enable these bias vectors:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p283"> 
   <div class="code-area-container"> 
    <pre class="code-area">NEW_CONFIG.update({"qkv_bias": True})</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p284"> 
   <p>We can now use the updated <code>NEW_CONFIG</code> dictionary to initialize a new <code>GPTModel</code> instance:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p285"> 
   <div class="code-area-container"> 
    <pre class="code-area">gpt = GPTModel(NEW_CONFIG)
gpt.eval()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p286"> 
   <p>By default, the <code>GPTModel</code> instance is initialized with random weights for pretraining. The last step to using OpenAI’s model weights is to override these random weights with the weights we loaded into the <code>params</code> dictionary. For this, we will first define a small <code>assign</code> utility function that checks whether two tensors or arrays (<code>left</code> and <code>right</code>) have the same dimensions or shape and returns the right tensor as trainable PyTorch parameters:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p287"> 
   <div class="code-area-container"> 
    <pre class="code-area">def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, "
                          "Right: {right.shape}"
        )
    return torch.nn.Parameter(torch.tensor(right))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p288"> 
   <p>Next, we define a <code>load_weights_into_gpt</code> function that loads the weights from the <code>params</code> dictionary into a <code>GPTModel</code> instance <code>gpt.</code></p> 
  </div> 
  <div class="browsable-container listing-container" id="p289"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.5</span> Loading OpenAI weights into our GPT model code</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import numpy as np

def load_weights_into_gpt(gpt, params):          <span class="aframe-location"/> #1
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):    <span class="aframe-location"/> #2
        q_w, k_w, v_w = np.split(                           <span class="aframe-location"/> #3
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight, 
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias, 
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight, 
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias, 
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight, 
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias, 
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale, 
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift, 
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale, 
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift, 
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])   <span class="aframe-location"/> #4</pre> 
    <div class="code-annotations-overlay-container">
     #1 Sets the model’s positional and token embedding weights to those specified in params.
     <br/>#2 Iterates over each transformer block in the model
     <br/>#3 The np.split function is used to divide the attention and bias weights into three equal parts for the query, key, and value components.
     <br/>#4 The original GPT-2 model by OpenAI reused the token embedding weights in the output layer to reduce the total number of parameters, which is a concept known as weight tying.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p290"> 
   <p>In the <code>load_weights_into_gpt</code> function, we carefully match the weights from OpenAI’s implementation with our <code>GPTModel</code> implementation. To pick a specific example, OpenAI stored the weight tensor for the output projection layer for the first transformer block as <code>params["blocks"][0]["attn"]["c_proj"]["w"]</code>. In our implementation, this weight tensor corresponds to <code>gpt.trf_blocks[b].att.out_proj .weight</code>, where <code>gpt</code> is a <code>GPTModel</code> instance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p291"> 
   <p>Developing the <code>load_weights_into_gpt</code> function took a lot of guesswork since OpenAI used a slightly different naming convention from ours. However, the <code>assign</code> function would alert us if we try to match two tensors with different dimensions. Also, if we made a mistake in this function, we would notice this, as the resulting GPT model would be unable to produce coherent text.</p> 
  </div> 
  <div class="readable-text intended-text" id="p292"> 
   <p>Let’s now try the <code>load_weights_into_gpt</code> out in practice and load the OpenAI model weights into our <code>GPTModel</code> instance <code>gpt</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p293"> 
   <div class="code-area-container"> 
    <pre class="code-area">load_weights_into_gpt(gpt, params)
gpt.to(device)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p294"> 
   <p>If the model is loaded correctly, we can now use it to generate new text using our previous <code>generate</code> function:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p295"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
token_ids = generate(
    model=gpt,
    idx=text_to_token_ids("Every effort moves you", tokenizer).to(device),
    max_new_tokens=25,
    context_size=NEW_CONFIG["context_length"],
    top_k=50,
    temperature=1.5
)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p296"> 
   <p>The resulting text is as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p297"> 
   <div class="code-area-container"> 
    <pre class="code-area">Output text:
 Every effort moves you toward finding an ideal new way to practice something!
What makes us want to be on top of that?</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p298"> 
   <p>We can be confident that we loaded the model weights correctly because the model can produce coherent text. A tiny mistake in this process would cause the model to fail. In the following chapters, we will work further with this pretrained model and fine-tune it to classify text and follow instructions.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p299"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 5.5</h5> 
   </div> 
   <div class="readable-text" id="p300"> 
    <p>Calculate the training and validation set losses of the <code>GPTModel</code> with the pretrained weights from OpenAI on the “The Verdict” dataset.</p> 
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p301"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 5.6</h5> 
   </div> 
   <div class="readable-text" id="p302"> 
    <p>Experiment with GPT-2 models of different sizes—for example, the largest 1,558 million parameter model—and compare the generated text to the 124 million model. </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p303"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p304"> When LLMs generate text, they output one token at a time. </li> 
   <li class="readable-text" id="p305"> By default, the next token is generated by converting the model outputs into probability scores and selecting the token from the vocabulary that corresponds to the highest probability score, which is known as “greedy decoding.” </li> 
   <li class="readable-text" id="p306"> Using probabilistic sampling and temperature scaling, we can influence the diversity and coherence of the generated text. </li> 
   <li class="readable-text" id="p307"> Training and validation set losses can be used to gauge the quality of text generated by LLM during training. </li> 
   <li class="readable-text" id="p308"> Pretraining an LLM involves changing its weights to minimize the training loss. </li> 
   <li class="readable-text" id="p309"> The training loop for LLMs itself is a standard procedure in deep learning, using a conventional cross entropy loss and AdamW optimizer. </li> 
   <li class="readable-text" id="p310"> Pretraining an LLM on a large text corpus is time- and resource-intensive, so we can load openly available weights as an alternative to pretraining the model on a large dataset ourselves. </li> 
  </ul>
 </div></div></body></html>