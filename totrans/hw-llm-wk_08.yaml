- en: 9 Ethics of building and using LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 构建和使用LLMs的伦理
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How LLMs’ abilities to perform many tasks also create unanticipated risk
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs执行许多任务的能力也创造了不可预见的风险
- en: The question of LLMs’ misalignment with human values
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs与人类价值观不一致的问题
- en: The implications of LLMs’ data use on content creation and building future models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs数据使用对内容创作和构建未来模型的影响
- en: Although the discussion of ethics may remind some of you of the dull readings
    from an entry-level college class, there are critical considerations when implementing
    algorithms that have the potential to affect humanity. Given the rapid growth
    in LLM use and their scope of capabilities, we must be aware of and attend to
    many evolving concerns. If you are unaware of these concerns, you will have no
    voice in their resolution.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对伦理的讨论可能会让一些人想起入门级大学课程中枯燥的阅读材料，但在实施可能影响人类算法时，存在一些关键的考虑因素。鉴于LLMs使用和能力的快速增长，我们必须意识到并关注许多不断演变的问题。如果你对这些担忧一无所知，你将无法在它们的解决中发表意见。
- en: Exploring the ethics of building and using LLMs is an incredibly complex topic
    that is challenging to represent completely. As a result, this chapter will present
    what we believe to be common concerns about building LLMs and the related ethical
    questions. Throughout the chapter, we’ll reference materials that round out this
    conversation so you can investigate further if you wish.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 探索构建和使用LLMs的伦理是一个极其复杂的话题，难以完全表达。因此，本章将介绍我们认为关于构建LLMs及其相关伦理问题的常见担忧。在整个章节中，我们将引用一些补充材料，以便如果你愿意可以进一步调查。
- en: 'We’ll cover three main topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖三个主要主题：
- en: Why do people want to construct LLMs, and what do they provide that didn’t exist
    before?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么人们想要构建LLMs，它们提供了之前不存在的东西？
- en: Some experts in machine learning believe that in future iterations, LLMs will
    lead to the extinction of the human race because they will automate us out of
    existence. Even if we do not agree with them, it is worth understanding the basis
    for this fear.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些机器学习专家认为，在未来迭代中，LLMs将导致人类种族的灭绝，因为它们将自动化我们到不存在。即使我们不同意他们，了解这种恐惧的基础也是值得的。
- en: The amount of training data needed for LLMs is monstrous. How do companies that
    build LLMs, such as OpenAI and Anthropic, source all that data? What ethical concerns
    arise that may have moral, legal, and financial implications due to how that data
    is collected and used?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs所需的训练数据量是巨大的。构建LLMs的公司，如OpenAI和Anthropic，是如何获取所有这些数据的？由于数据的收集和使用方式，可能引发哪些道德、法律和财务方面的伦理担忧？
- en: These are complicated considerations on both ethical and legal fronts. Our goal
    is not to tell you whether the creation of these models is ethical or nonethical
    but rather to outline primary considerations under each discussion. We hope this
    helps you consider LLMs’ implications, consequences, and risks on a broader scale.
    We see many high-profile, ethically sophisticated questions around LLM use, and
    many practitioners have not had to grapple meaningfully with this subject. Nevertheless,
    we believe that it is crucial to consider the ethical questions around building
    LLMs, and we will introduce you to some of the critical concerns to consider in
    this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在伦理和法律方面都是复杂的考虑。我们的目标不是告诉你这些模型的创建是道德的还是不道德的，而是概述每个讨论下的主要考虑因素。我们希望这能帮助你更广泛地考虑LLMs的影响、后果和风险。我们看到了许多关于LLMs使用的知名、复杂的伦理问题，许多从业者还没有真正地处理这个问题。尽管如此，我们认为考虑构建LLMs的伦理问题至关重要，我们将在本章中介绍一些需要考虑的关键问题。
- en: There are just as many considerations necessary when discussing how we use LLMs
    versus how we build LLMs, so we’ve divided this conversation into two sections.
    First, we focus on the ethics of building LLMs in general, while the latter section
    will cover the ethical implications of LLM use.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论我们如何使用LLMs与如何构建LLMs时，必要的考虑因素同样多，因此我们将这次对话分为两个部分。首先，我们关注构建LLMs的一般伦理问题，而下一部分将涵盖LLMs使用的伦理影响。
- en: Last, we will avoid ascribing these arguments to specific individuals or groups.
    Our goal is to prevent bias and avoid “calling out” anyone in particular in this
    discussion. The concerns are what’s important.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将避免将这些论点归咎于特定的个人或群体。我们的目标是防止偏见，避免在这次讨论中特别“点名”任何人。重要的是这些担忧。
- en: 9.1 Why did we build LLMs at all?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 我们为什么要构建LLMs？
- en: Before we talk about the ethical ramifications of developing LLMs, it’s worth
    thinking about *what* it is we are trying to accomplish by building LLMs and *why*
    we want to achieve those things. Like all software engineering, building LLMs
    commonly aims to reduce or eliminate human labor from some tasks. Some economists
    might tell you that this is how standards of living generally increase. As technology
    advances, fewer people need to perform manual, labor-intensive tasks, and thus,
    they have more time for discovery, creation, and other functions that use high-level
    cognition.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论开发大型语言模型（LLMs）的伦理影响之前，值得思考的是，我们通过构建LLMs试图实现什么，以及我们为什么想要实现这些目标。像所有软件工程一样，构建LLMs通常旨在减少或消除某些任务中的人类劳动。一些经济学家可能会告诉你，这就是生活水平普遍提高的方式。随着技术的进步，越来越少的人需要执行手动、劳动密集型的工作，因此他们有更多的时间用于探索、创造和其他使用高级认知的功能。
- en: In the case of LLMs, a common goal is increasing the efficiency of algorithms
    for applications such as automated language translation, speech-to-text transcription,
    reading text contained in images and printed documents in applications such as
    Optical Character Recognition, indexing, and retrieving information, known simply
    as “search” or, more broadly, as information retrieval, and more. Others are interested
    in LLMs for purely scientific reasons, such as studying methods in computational
    linguistics, or creative applications, such as generating images, music, or videos.
    Furthermore, others may seek to increase access to and transparency of technology
    that affects our lives, or it may be just because LLMs have grabbed their attention
    and present fantastic new capabilities.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs的情况下，一个常见的目标是提高自动化语言翻译、语音转文字转录、在光学字符识别、索引和信息检索等应用中读取图像和打印文档的算法效率，这些应用简单地被称为“搜索”，或更广泛地称为信息检索，等等。其他人出于纯粹的科学原因对LLMs感兴趣，例如研究计算语言学的方法，或者创意应用，例如生成图像、音乐或视频。此外，其他人可能寻求增加影响我们生活的技术的影响力和透明度，或者可能只是因为LLMs吸引了他们的注意，并展示了令人惊叹的新能力。
- en: For some, the variety of things LLMs can achieve is an intrinsic motivation
    for wanting to build them. AI and ML algorithms have been doing all the tasks
    we listed for some time; for example, machine translation is decades old. Part
    of what makes LLMs different is that they seem capable of doing everything with
    one model and algorithm. Before the advent of LLMs, engineers would implement
    tasks like translation and transcription in separate systems designed to meet
    those needs individually. The largest LLMs today can, to some degree, do each
    of these things and more. Often, it seems they can complete tasks of seemingly
    endless scope. At the same time, others fear LLMs because due to their breadth
    of capability, they believe they will steal work, motivation, and activity from
    humans by taking on tasks requiring discovery and creation, previously thought
    to be reserved for humans only.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些人来说，LLMs能够实现的各种事情是他们想要构建它们的内在动机。人工智能和机器学习算法已经执行了我们列出的所有任务一段时间了；例如，机器翻译已有几十年历史。LLMs与众不同的部分在于它们似乎能够用一个模型和算法做所有的事情。在LLMs出现之前，工程师会为满足这些需求而设计独立的系统来执行翻译和转录等任务。今天最大的LLMs在某种程度上可以完成这些事情以及更多。通常，它们似乎可以完成看似无限的任务。与此同时，其他人害怕LLMs，因为他们认为由于它们能力的广泛性，它们将通过承担需要发现和创造的任务，从而从人类那里窃取工作、动机和活动，这些任务以前被认为是仅限于人类的。
- en: 9.1.1 The pros and cons of LLMs doing everything
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 LLM做所有事情的利弊
- en: 'Given that an LLM can perform many different tasks via a single model, you
    could describe it as a kind of “everything app”: your one-stop shop for AI-powered
    assistance. From a usability perspective, many benefits have emerged from the
    near-universal capability of LLMs, such as their relative aptitude for decomposing
    complex tasks into a series of steps or their ability to generate unique explanations
    to fill specific knowledge gaps.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一个LLM可以通过单个模型执行许多不同的任务，你可以将其描述为一种“全能应用”：你的人工智能辅助一站式商店。从可用性的角度来看，LLMs几乎普遍的能力带来了许多好处，例如它们相对擅长将复杂任务分解成一系列步骤，或者它们能够生成独特的解释来填补特定的知识空白。
- en: 'Additionally, the chat-style interface seems very popular with users, even
    if other ways of working with LLMs are available. The popularity of chat may be
    due to its general accessibility: you chat with people constantly. Experience
    with phone calls is widespread, and with texts, Slack, Teams, instant messaging,
    and email, people implicitly know how to use various chat-based interfaces. As
    a result, interacting with an AI via a chat-based interface has become an inviting
    and easy way to increase adoption with little training. The widespread experience
    with chat-based applications also has a democratizing effect: users only need
    to learn something once to help them pursue many different goals.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，聊天风格的界面似乎很受用户欢迎，即使有其他与LLM合作的方式。聊天受欢迎可能是因为其普遍的易用性：你不断地与人聊天。电话通话的经验很普遍，而通过短信、Slack、Teams、即时消息和电子邮件，人们隐含地知道如何使用各种基于聊天的界面。因此，通过基于聊天的界面与AI交互已经成为一种吸引人且易于接受的方式来增加采用率，而无需太多培训。基于聊天的应用的广泛经验也具有民主化的影响：用户只需学习一次，就能帮助他们追求许多不同的目标。
- en: The primary disadvantage of such a system is that although it *can* be used
    for everything, that doesn’t mean we *should* use it for everything. When you
    have an algorithm that people can use for many different and potentially unexpected
    tasks, you do not have the time to test every possible use. Due to the breadth
    of potential applications of LLMs, there will be a gap between validating what
    the model does safely and what it can attempt to do but that could be potentially
    dangerous or harmful.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种系统的首要缺点是，尽管它可以用于一切，但这并不意味着我们应该用它来处理一切。当你有一个人们可以用于许多不同和可能意外的任务的算法时，你没有时间去测试每一种可能的使用。由于LLMs潜在应用的范围很广，因此在验证模型安全执行的行为和它可能尝试但可能具有潜在危险或有害的行为之间将存在差距。
- en: For example, current LLM models can perform abstract evaluations of race or
    gender, even though these evaluations may contain harmful negative bias. While
    we can develop tests and defenses for specific instances of harmful bias, these
    are likely to be narrow in scope and highly specific. For example, suppose we
    ask for an image generation model to generate an image of a business meeting.
    The unfortunate result is that all people in that image will often be male and
    white. Naturally, we wish the model to transcend these stereotypes. However, identifying
    and fixing specific contextual bias concerns like this will not affect whether
    a model would cause harm when deployed in the real world and prompted in different,
    unanticipated ways. At best, these exercises exemplify how an LLM can fail, but
    addressing harm requires understanding the potential failures that can happen,
    for example, due to bias in the training data. Simultaneously, we must understand
    how people will use LLMs and whether those uses may lead to unintended harm due
    to how the LLM generates output. This may mean expressly not using an LLM for
    an intended use case due to the lack of mitigations for potential harms they may
    cause.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当前的LLM模型可以进行关于种族或性别的抽象评估，即使这些评估可能包含有害的负面偏见。虽然我们可以为有害偏见的特定实例开发测试和防御措施，但这些可能范围狭窄且非常具体。例如，假设我们要求一个图像生成模型生成一个商务会议的图像。不幸的结果是，图像中的所有人通常都是男性和白人。自然地，我们希望模型能够超越这些刻板印象。然而，识别和修复这种特定情境偏见问题并不会影响模型在现实世界中部署并被以不同、未预料到的方式提示时是否会造成伤害。至多，这些练习可以说明LLM可能失败的情况，但解决伤害需要理解可能发生的潜在失败，例如，由于训练数据中的偏见。同时，我们必须了解人们将如何使用LLM，以及这些使用是否可能导致由于LLM生成输出而导致的不当伤害。这可能意味着由于缺乏缓解潜在危害的措施，明确不使用LLM于预期用途。
- en: Recent research on the real-world harms of deployed LLMs found that the implicit
    bias in LLMs like OpenAI’s ChatGPT and Google’s Gemini against people who use
    African American vernacular English was worse than the archaic negative stereotypes
    measured among white Americans in the 1920s [1]. Another study considered the
    use case of a doctor consulting an LLM for information on medical best practices
    and treatment options for people of different races and found that the models
    frequently recommended debunked race-based medical practices grounded in eugenicist
    “science” [2]. Unfortunately, we continue to see these problems in models that
    score quite well on existing explicit bias benchmarks. The prevalence of latent
    bias suggests these benchmarks aren’t sufficient in evaluating potential harms
    and emphasizes the need to consider the harm an LLM can cause based on its use.
    In other words, it is more important to view harm due to AI deployment as a direct
    result of the specific proposed use cases and application, not as something we
    can ascribe to a general notion of whether a model contains racial bias.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于部署的LLM（大型语言模型）在现实世界中的危害的研究发现，像OpenAI的ChatGPT和谷歌的Gemini这样的LLM对使用非裔美国人俚语的用户的隐性偏见比20世纪20年代在美国白人中间测量的陈旧负面刻板印象还要严重[1]。另一项研究考虑了医生咨询LLM以获取关于不同种族人群的医疗最佳实践和治疗方案的用例，发现模型经常推荐基于优生学“科学”的已被驳斥的基于种族的医疗实践[2]。不幸的是，我们在那些在现有的显性偏见基准测试中得分相当高的模型中仍然看到这些问题。潜在偏见的普遍存在表明，这些基准不足以评估潜在的危害，并强调了根据其使用情况考虑LLM可能造成的危害的必要性。换句话说，将AI部署造成的危害视为特定用例和应用的具体结果，而不是将其归因于模型是否包含种族偏见的一般观念，这一点更为重要。
- en: Today, we do not know how to design an algorithm capable of doing so many tasks
    in one system while simultaneously providing defense against accidental misuse
    and harm by well-meaning individuals. So it becomes critical from a developer’s
    perspective to do thorough user studies across a wide range of groups and settings
    to identify the unintended risks and to include monitoring and logging to remediate
    any late-identified risks. Whether we are attempting to prevent harmful racial
    stereotypes or the advocacy for debunked medical practices, the current approaches
    to constraining the misuse of LLMs are to enumerate what we know about potential
    problems and employ fine-tuning methods, such as RLHF, to force the model to behave
    better on known problems. The unfortunate side of this is that due to the potential
    breadth of LLM capabilities, the set of unknown problems is infinite, and as such,
    any testing regime will be incomplete.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们不知道如何设计一个算法，能够在单一系统中完成如此多的任务，同时还能防御善意个体意外误用和造成的危害。因此，从开发者的角度来看，进行跨广泛群体和环境的彻底用户研究变得至关重要，以识别未预见的风险，并包括监控和记录以补救任何晚发现的危险。无论我们是在尝试防止有害的种族刻板印象还是倡导被驳斥的医疗实践，目前限制LLM误用的方法都是列举我们关于潜在问题的了解，并采用微调方法，如RLHF，迫使模型在已知问题上表现更好。不幸的是，由于LLM能力的潜在广泛性，未知问题集是无限的，因此任何测试制度都将是不完整的。
- en: Note The importance of postdeployment monitoring is not new. For example, the
    FDA has practiced this for many years with the MedWatch system. This system allows
    the public and medical professionals to report any adverse events with a drug
    or medical device so that the FDA can monitor for anything unusual.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：部署后监控的重要性并非新概念。例如，FDA已经使用MedWatch系统实践了多年。该系统允许公众和医疗专业人员报告任何药物或医疗设备的任何不良事件，以便FDA可以监控任何异常情况。
- en: 9.1.2 Do we want to automate all human work?
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 我们是否想要自动化所有的人类工作？
- en: As we mentioned in the introduction, some economists might argue that automation
    allows the labor pool to focus on new work. This argument hinges on the idea that
    advances in automation have been good at eliminating work that most people don’t
    want to do. Farming is hard, mining rare earth metals is hard, and assembling
    cars, toys, and packages is hard. These are difficult labor and body-destroying
    jobs often coupled with limited intellectual stimulation. Heavy labor like farming
    requires 74% fewer laborers today than it did in 1950 [3] and, undoubtedly, many
    times fewer than it did back in the medieval era.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在引言中提到的，一些经济学家可能会认为自动化允许劳动力池专注于新的工作。这种论点依赖于这样一个观点：自动化技术的进步在消除大多数人不愿意做的工作方面做得很好。农业很辛苦，提炼稀土金属很辛苦，组装汽车、玩具和包装也很辛苦。这些是劳动强度大、对身体有害的工作，通常伴随着有限的精神刺激。与1950年相比，农业今天需要的劳动力减少了74%[3]，无疑比中世纪时代少得多。
- en: The difference with LLMs is the potential to automate away certain types of
    white-collar knowledge work. Copywriting [4], visual arts [5], graphic design
    [6], and banking [7] are just a few of the fields disrupted by generative AI.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLMs的不同之处在于，它们有可能自动化某些类型的白领知识工作。文案写作[4]、视觉艺术[5]、平面设计[6]和银行业[7]只是被生成式AI颠覆的几个领域。
- en: 'Those concerned about LLMs’ effect on the economy suggest that we will lose
    jobs to automation, which we caution is not as clear-cut as often portrayed. Institutional
    and consumer desires may push for retention and continued expansion of these types
    of white-collar jobs. We should be wary of ignoring a history of economic study
    about how jobs change as technology advances. Instead, we must address a more
    significant concern: obtaining high-quality training data. We believe this will
    drive new jobs in the future, emphasizing the importance of human creativity and
    ability, even if the current jobs it creates are not yet the desirable kind of
    white-collar work that many would prefer.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 关心LLMs对经济影响的那些人认为，我们将因自动化而失去工作，我们警告说，这并不像通常描绘的那样清晰。机构和消费者的愿望可能会推动保留和继续扩大这些类型的白领工作。我们应该警惕忽视关于技术进步时工作如何变化的经济研究历史。相反，我们必须解决一个更重大的问题：获取高质量的训练数据。我们相信这将推动未来的新工作，强调人类创造力和能力的重要性，即使它目前创造的工作还不是许多人希望的白领工作类型。
- en: A counter-example on “obvious” outcomes
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于“显而易见”的结果的反例
- en: Some argue that it is obvious that LLMs will affect some sectors of the economy
    for better or worse. The bank teller’s job is a famous example often used to argue
    against LLMs. The job of bank tellers has changed significantly since the invention
    of the Automatic Teller Machine (ATM) in the 1960s. Clearly, the ATM automated
    many of the bank teller’s tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人认为，LLMs（大型语言模型）对经济某些领域产生正面或负面的影响是显而易见的。银行柜员的职位就是常被用来反对LLMs的著名例子。自20世纪60年代自动取款机（ATM）的发明以来，银行柜员的职位发生了显著变化。显然，ATM自动化了许多银行柜员的任务。
- en: But the ATM example is not that simple. The number of teller jobs increased
    for decades *after* the invention of the ATM, doubling to ![equation image](../Images/eq-chapter-9-32-1.png)
    between 1970 and 2010 even as the ATM became more widely available [8]. Looking
    to historical studies of the ATM’s effect on jobs, it was recognized that many
    factors contributed to job loss, including changes in the growth rate and the
    nature of the job. Job loss came as a result of not just ATM technology but multiple
    rounds of technology innovation in other parts of the business, differences in
    how banks responded to the change, deregulation, and increased competition and
    consolidation in the banking industry [9]. So even though the ATM was arguably
    better and cheaper at the bank teller’s job, the nature of institutions, customers,
    and expectations prevented any immediate decline in jobs and made the situation
    far more complex than is often advertised.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但ATM的例子并不那么简单。在ATM发明后的几十年里，柜员职位数量不断增加，1970年至2010年间翻了一番，达到![方程式图片](../Images/eq-chapter-9-32-1.png)，尽管ATM的普及率越来越高[8]。回顾ATM对就业的历史研究，人们认识到许多因素导致了失业，包括增长速度和职位性质的变化。失业不仅仅是由于ATM技术，还包括业务其他部分的多次技术创新、银行对变化的反应差异、金融行业的去监管化以及竞争和合并的增加[9]。因此，尽管ATM在银行柜员的工作上可能更优越、成本更低，但机构、客户和期望的性质阻止了就业的立即下降，使得情况比通常宣传的要复杂得多。
- en: The ATM example is not unique; technology can, but does not always, lead to
    job losses due to automation. For example, machine translation improved dramatically
    in the early 2000s and again in 2016\. Still, jobs for translation work increased
    within each period and continue to grow today [10]. The critical observation is
    that the job pool for translation doesn’t shrink when translators incorporate
    automated tools into their workflow. Instead, we saw a growth in the volume of
    translation work completed and an increase in demand for translation services
    as the amount of material requiring translation continues to grow. Some argue
    that similar demand will materialize for creative artists and writers [11]. According
    to this argument, while the means of producing art and performing knowledge work
    will change, the market will continue to grow, and demand will continue to rise
    in a way that can take advantage of the new supply of labor resulting from the
    introduction of automated tools. Thus, when we identify an area of work that may
    be automated or accelerated by LLMs, we must also determine whether the increased
    efficiency and quality could drive more demand.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 自动柜员机（ATM）的例子并不独特；技术可能会，但并不总是因为自动化而导致失业。例如，机器翻译在2000年代初和2016年都有了显著改进。然而，翻译工作在每个时期都增加了，并且至今仍在增长[10]。关键观察结果是，当翻译者将自动化工具纳入他们的工作流程时，翻译工作的就业池并没有缩小。相反，我们看到翻译工作的量增加了，随着需要翻译的材料数量的持续增长，对翻译服务的需求也在增加。有些人认为，对创意艺术家和作家的类似需求将会出现[11]。根据这种论点，虽然艺术和知识工作的生产方式将发生变化，但市场将继续增长，需求将继续以可以利用自动化工具引入的新劳动力供应的方式增长。因此，当我们确定可能被LLMs自动化或加速的工作领域时，我们还必须确定提高效率和质量的增加是否能够推动更多的需求。
- en: Still, others will argue that LLMs fundamentally differ from everything that
    has ever happened. Thus, we cannot use prior methods of understanding technology’s
    potential effects on the economy to predict the future. Although possible and
    tempting to believe, given all the hype around LLMs, we are skeptical as to whether
    this is an overly broad statement that no one can prove false or true. Although
    we should indeed consider such possibilities and factors when making regulations
    (which, in turn, play a significant part in how jobs evolve with technology),
    it is also noteworthy that an estimated 60% of all US jobs are modern inventions
    that did not exist previously [12].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，仍有人会争辩说，大型语言模型（LLMs）与以往发生的所有事情都存在根本性的不同。因此，我们不能使用以前理解技术对经济潜在影响的方法来预测未来。鉴于围绕LLMs的炒作，虽然相信这一点可能是有可能的，也是诱人的，但我们对此表示怀疑，这或许是一个过于宽泛的声明，没有人能够证明其真伪。尽管在制定法规时（这些法规反过来又在很大程度上影响着技术发展如何影响就业），我们确实应该考虑这样的可能性和因素，但值得注意的是，估计有60%的美国工作都是现代发明，以前并不存在[12]。
- en: Considerations on training data
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于训练数据的考虑
- en: Generative AI’s effect on creative expression is poignant due to the situation’s
    perverse duality. Much of the work of writers and artists who post their content
    on the internet is fueling models that are seemingly out to eliminate their jobs.
    The ethical argument made by LLM researchers is that they should be able to freely
    use content from these creators as training data. This argument may lead to a
    pyrrhic victory and, ultimately, an undoing for AI. If AI replaces the work of
    creatives, LLM developers will find that they can no longer improve their models
    due to a lack of human-generated content and the exponential size increases in
    the data needed to train LLMs exceeding the linear growth in user-generated content.
    More importantly, the folks who create that content can no longer be employed
    or motivated to create content merely to have it slurped up by an LLM.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能对创意表达的影响是深刻的，因为这种状况具有一种扭曲的双重性。许多在互联网上发布内容的作家和艺术家的作品，似乎在为那些旨在消除他们工作的模型提供燃料。LLM研究人员提出的伦理论点是，他们应该能够自由地使用这些创作者的内容作为训练数据。这种论点可能会导致一种代价高昂的胜利，并最终对AI造成破坏。如果AI取代了创意工作者的工作，LLM开发者将发现，由于缺乏人类生成的内容以及训练LLM所需数据的指数级增长超过了用户生成内容的线性增长，他们再也无法改进他们的模型。更重要的是，创造这些内容的人将无法再被雇佣或受到激励，仅仅是为了让LLM吞噬他们的内容。
- en: This negative cycle will affect both LLMs and content creators, even if it is
    only a perceived risk and not a genuine concern. Data harvesting to train LLMs
    is a significant concern for thousands of websites that rely on user-generated
    content and advertising revenue from those who consume that content. These sites
    provide precious training data for LLMs, whose builders require massive collections
    of training data but do nothing to contribute to advertising revenue.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个负面循环将影响LLMs和内容创作者，即使这只是一个感知到的风险，而不是真正的担忧。用于训练LLMs的数据采集对成千上万的依赖用户生成内容和从内容消费者那里获得广告收入的网站来说是一个重大的担忧。这些网站为LLMs提供了宝贵的训练数据，而LLMs的构建者需要大量的训练数据，但他们并没有为广告收入做出任何贡献。
- en: For example, Stack Exchange is a collection of websites where users can post
    questions, have other users answer them, and receive a reputation rating for good
    answers. One of Stack Exchange’s websites, Stack Overflow, is a godsend to programmers
    looking for help solving coding problems. Stack Exchange also hosts many other
    diverse user communities catering to system administrators, math students, and
    tabletop gaming enthusiasts.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Stack Exchange是一个用户可以发布问题、让其他用户回答并因良好答案获得声誉评级的网站集合。Stack Exchange的一个网站，Stack
    Overflow，对寻求帮助解决编码问题的程序员来说是一个天赐之物。Stack Exchange还托管了许多其他多样化的用户社区，服务于系统管理员、数学学生和桌面游戏爱好者。
- en: With the advent of LLMs, Stack Exchange was quick to change its business model
    and attempted to require payment from LLM creators to sustain its financial future
    [13]. Even with agreements between companies training LLMs and the websites hosting
    content in place, more direct commercialization of user-generated content may
    not be palatable to users. Stack Overflow experienced this as people began to
    delete their helpful answers from the platform in protest of Stack Overflow selling
    the results of their free labor to LLM creators [14].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）的出现，Stack Exchange迅速调整了其商业模式，并试图要求LLMs的创建者付费以维持其财务未来[13]。即使在训练LLMs的公司和托管内容的网站之间达成协议，用户生成内容的更直接商业化可能也不会受到用户的欢迎。Stack
    Overflow就经历了这种情况，人们开始从平台上删除他们的有用答案，以抗议Stack Overflow将他们免费劳动的成果出售给LLMs的创建者[14]。
- en: This example mirrors a long history of search engines integrating the capabilities
    of the applications and websites they index into their primary interface. For
    example, it is now possible to search for and compare prices for airline tickets
    directly from within the Google search interface. This capability drives traffic
    away from established travel sites that provide the same service [15] and reduces
    the demand for the services and revenue of the companies that built those services.
    A potentially similar relationship exists between the LLMs trained on creative
    works and the original producers of that work when it becomes training data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子反映了搜索引擎将它们索引的应用程序和网站的特性整合到其主界面的悠久历史。例如，现在可以直接从Google搜索界面搜索和比较机票价格。这种能力将流量从提供相同服务的传统旅游网站吸引走[15]，并减少了构建这些服务的公司所提供的服务和收入的需求。当这些LLMs的训练数据来源于创意作品时，它们与该作品的原始生产者之间可能存在类似的关系。
- en: It seems clear that the problems we are dealing with due to the rise of LLMs
    are similar, but not identical, to the problems we’ve seen in previous periods
    of automation. The question then becomes whether the differences related to LLM
    deployment are sufficiently significant to result in a different, more negative
    outcome. The outcomes are not apparent to us, primarily due to the broad scale,
    accessibility, and applicability of LLMs. It is up to LLM developers to take the
    initiative to understand and mitigate potential harms, like prenegotiating data
    usage and community building with the likely-to-be-affected fields. We will discuss
    other facets of training data and its sourcing in the last section of this chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，由于LLMs的兴起而导致的我们正在处理的问题与之前自动化时期的类似，但又不完全相同。那么，与LLMs部署相关的差异是否足够显著，以至于会导致不同的、更负面的结果呢？结果对我们来说并不明显，主要是因为LLMs的广泛规模、可访问性和适用性。LLMs的开发者需要采取主动，理解和减轻潜在的伤害，例如与可能受到影响领域进行预先谈判数据使用和社区建设。我们将在本章的最后部分讨论训练数据及其来源的其他方面。
- en: 9.2 Do LLMs pose an existential risk?
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 LLMs是否构成生存风险？
- en: Some believe that LLMs are, in themselves, dangerous. If you are unfamiliar
    with the argument, it may sound absurd that training a powerful LLM model could
    result in significant real-world harms such as eliminating privacy, terminator
    robots, and threats to human existence as we know it. Yet many are concerned about
    these risks, including leaders in the field of AI like Geoffrey Hinton [16] and
    Yoshua Bengio [17]. Hinton and Benigo are two of the most well-regarded researchers
    in deep learning who share significant credit for the survival, revival, and dominance
    of neural network techniques in AI.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人认为LLM本身是危险的。如果你不熟悉这个论点，可能会觉得训练一个强大的LLM模型会导致重大的现实危害，比如消除隐私、终结者机器人以及对我们所知的人类的威胁。然而，许多人对此类风险表示担忧，包括AI领域的领导者，如杰弗里·辛顿[16]和约书亚·本吉奥[17]。辛顿和本吉奥是深度学习领域最受尊敬的研究者之一，他们为神经网络技术在AI中的生存、复兴和主导地位做出了重大贡献。
- en: We believe AI does not present a realistic threat. However, serious and well-respected
    people are making these claims, so it is important to understand their arguments
    and explain why we believe these concerns are less significant than the need to
    address more immediate effects on the nature of work and ensure equitable and
    sustainable data licensing and compensation for creators.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为人工智能并不构成现实威胁。然而，一些严肃且受人尊敬的人提出了这些主张，因此理解他们的论点并解释为什么我们认为这些担忧不如解决对工作性质更紧迫的影响以及确保创作者公平和可持续的数据许可和补偿重要。
- en: 'In this section, we’ll focus on the general argument that AI could, broadly,
    become a risk to humanity because we could lose control over the LLMs, and LLMs
    might make decisions detrimental to humans. This notion stems from two ideas taken
    to their extremes:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点关注这样一个一般性论点：AI可能广泛地成为对人类的威胁，因为我们可能失去对LLM的控制，而LLM可能会做出对人类有害的决定。这一观点源于两个被推向极端的想法：
- en: The idea that an LLM can use tools to build new LLMs and thus potentially self-improve
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个LLM可以使用工具构建新的LLM，从而有可能实现自我提升的想法
- en: The idea that an LLM with a goal not aligned with human needs may ultimately
    decide to take actions detrimental to human life in the interest of its own goals
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个LLM的目标如果与人类需求不一致，最终可能会决定采取损害人类生活的行动以实现其自身目标
- en: We have touched on this first idea about self-improvement tangentially throughout
    this book. We have discussed the fact that designing LLMs involves developing
    tools for data collection and creating the code to train an LLM using that data.
    One might hypothesize that if an LLM can use tools for data collection and training
    directly, without human intervention, an LLM could hypothetically train another
    LLM.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中多次间接地提到了这个关于自我提升的第一个想法。我们讨论了设计大型语言模型（LLM）涉及开发数据收集工具和编写使用这些数据的代码来训练LLM的事实。有人可能会假设，如果LLM可以直接使用数据收集和训练的工具，而不需要人为干预，那么一个LLM理论上可以训练另一个LLM。
- en: The cognitive leap required to support this line of reasoning is that an LLM
    will be smart enough to build a better LLM. For us to accept this, we must assume
    that this new LLM will then be able to create an even better ![equation image](../Images/eq-chapter-9-49-1.png)
    and, further, believe that this improvement cycle could repeat forever until the
    ![equation image](../Images/eq-chapter-9-49-2.png) model will be more intelligent
    than any person who could ever exist and essentially be able to predict, subvert,
    or counteract any possible human action that might interrupt this cycle. This
    leap is challenging because we have little evidence that something like this is
    likely, based on what we observe in today’s technology.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 支持这一推理路线所需的认知飞跃是，一个LLM将足够聪明，能够构建一个更好的LLM。为了接受这一点，我们必须假设这个新的LLM将能够创建一个更好的![方程式图像](../Images/eq-chapter-9-49-1.png)，并且进一步相信这种改进周期可以永远重复，直到![方程式图像](../Images/eq-chapter-9-49-2.png)模型将比任何可能存在的人更聪明，并且本质上能够预测、颠覆或对抗任何可能中断这一周期的可能的人类行为。这一飞跃具有挑战性，因为我们几乎没有证据表明类似的事情在今天的科技观察中是可能发生的。
- en: The second idea, often referred to as the “alignment problem,” is that LLMs
    misaligned with human needs may choose goals and outcomes that are detrimental
    to humans. This idea is reasonable because, as discussed inc chapter [4](../Text/chapter-4.html),
    creating a metric that measures only your intended goals is challenging. However,
    the extraordinary leap required for this line of thinking is that LLMs will have
    the ability and resources to interact with the world directly and physically,
    which could result in mass harm if not stopped.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个想法，通常被称为“协调问题”，是LLM与人类需求不一致时，可能会选择对人类有害的目标和结果。这个想法是合理的，因为如第[4](../Text/chapter-4.html)章所讨论的，创建一个仅衡量你意图的目标的指标是具有挑战性的。然而，这一思考路线所需的非凡飞跃是LLM将拥有直接与世界互动和物理互动的能力和资源，如果不加以阻止，这可能导致大规模的伤害。
- en: Some combine these two ideas to argue that an LLM may have goals misaligned
    with humanity. They believe there will be a point at which an LLM realizes it
    needs to become more intelligent and improve itself to achieve its goals. As it
    does so, it takes resources away from humans or, via its improved intelligence,
    forces humans into subservience to help it achieve its goals. We outline this
    idea in figure [9.1](#fig__destroyTheWorld).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一些学者将这两个观点结合起来，认为大型语言模型（LLM）可能具有与人类不一致的目标。他们认为，LLM会意识到需要变得更聪明并提升自己以实现其目标。在这个过程中，它会从人类那里夺取资源，或者通过其提升的智能，迫使人类服从以帮助它实现目标。我们在图[9.1](#fig__destroyTheWorld)中概述了这一观点。
- en: '![figure](../Images/CH09_F01_Boozallen.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F01_Boozallen.png)'
- en: Figure 9.1 Two kinds of hypothetical concerns arise within the alignment problem,
    as commonly argued by those who think LLMs pose an existential risk to humanity.
    The top path shows a direct alignment problem, where the AI’s target solution
    directly harms humans. The bottom path shows an indirect alignment problem, where
    the AI has created a subgoal toward its eventual target. Even if the target—say,
    solving a hard math problem—is achieved, this LLM will do this at the cost of
    humanity. In an intermediate step, the LLM decides it needs more earthly resources
    than can be shared with humans to solve the problem.
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1 如通常所论证的那样，在一致性问题上，存在两种假设性的担忧，认为LLM对人类构成生存风险。上面的路径显示了一个直接的协调问题，其中AI的目标解决方案直接伤害人类。下面的路径显示了一个间接的协调问题，其中AI为其最终目标创造了一个子目标。即使目标——比如解决一个难题——得以实现，这个LLM也将以损害人类的代价来完成。在中间步骤中，LLM决定它需要比可以与人类共享的更多地球资源来解决该问题。
- en: An essential aspect of this argument is that the LLM, with a goal of self-preservation,
    determines that humans are destroying the planet. Since the LLM exists on earth
    and wants to continue doing so, it determines that destroying humans would be
    the best means of maintaining self-preservation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个论点的关键方面是，LLM，出于自我保存的目标，认为人类正在破坏地球。由于LLM存在于地球上并希望继续这样做，它认为摧毁人类将是维持自我保存的最佳手段。
- en: We do not think the potential for humanity’s destruction is a well-founded concern.
    Still, many people, including those with doctorates in computer science and who
    specialize in deep learning, are concerned about this scenario. The main problem
    with this “LLM-destroys-humanity” concept is that it relies on unfalsifiable logic.
    Unfalsifiable logic suggests that things will happen, and it is nearly impossible
    for anyone to prove that they will not. In this case, proving that LLMs won’t
    destroy humanity is challenging.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为人类毁灭的可能性不是一个有充分根据的担忧。然而，包括那些拥有计算机科学博士学位并专注于深度学习的人在内，许多人对此情景表示担忧。这个“LLM毁灭人类”概念的主要问题是它依赖于不可证伪的逻辑。不可证伪的逻辑表明事情将会发生，几乎不可能有人证明它们不会发生。在这种情况下，证明LLM不会毁灭人类是具有挑战性的。
- en: Teapots and unfalsifiable statements
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 茶壶和不可证伪的陈述
- en: 'Demanding that someone make a falsifiable statement is essential in discussing
    abstract risks like LLMs’ potential to destroy humanity. A famous example is Bertrand
    Russell’s “teapot” thought experiment. The idea is simple: someone tells you that
    a teapot exists in space, too small and too far away to be detected. The premise
    itself is unfalsifiable; I can scan the universe for centuries looking for a teapot,
    but even though I can’t find it, I cannot prove that it does not exist. The only
    possibility is that I eventually find a teapot and confirm that it exists in space.
    Otherwise, I will never prove the teapot’s existence was a lie. Hence, when discussing
    abstract risks, unfalsifiable statements become a cognitive dead end. Arguing
    against a statement that no one can prove false is impossible. At the same time,
    those statements do nothing to advance the conversation to arrive at a meaning-ful
    insight or conclusion. Instead, making an argument based on realistic and practical
    concerns that can be acknowledged and addressed is more valuable in understanding
    the problems.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论像LLMs可能毁灭人类这样的抽象风险时，要求某人做出可证伪的陈述是至关重要的。一个著名的例子是伯特兰·罗素的“茶壶”思想实验。这个想法很简单：有人告诉你，太微小且太远以至于无法被探测到的茶壶存在于太空中。前提本身是不可证伪的；我可以扫描宇宙数百年寻找茶壶，即使我找不到它，我也无法证明它不存在。唯一可能的情况是，我最终找到茶壶并确认它在太空中存在。否则，我将永远无法证明茶壶的存在是一个谎言。因此，在讨论抽象风险时，不可证伪的陈述成为了一个认知的死胡同。反对一个无人能证明其为假的陈述是不可能的。同时，这些陈述对推动对话达到有意义的见解或结论毫无帮助。相反，基于可以认可和解决的问题的现实和实际担忧的论点更有价值。
- en: 'Two other arguments support this reasoning: technology tends to increase exponentially,
    and most humans are bad at considering exponentials and thus don’t fully comprehend
    how quickly this risk will become a reality.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另有两个论点支持这一推理：技术往往呈指数增长，而大多数人都不擅长考虑指数，因此没有完全理解这一风险将如何迅速成为现实。
- en: The fact that this line of thinking exists and is a concern of leaders in the
    field makes it worthwhile for you to delve deeper into the thoughts and considerations
    that are both for and against the idea that LLMs could bring about the end of
    humanity. The following subsections explore these arguments and the critical assumptions
    behind self-improvement and alignment mismatch.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法的存在以及它成为该领域领导者关注的焦点，使你深入探讨关于LLMs可能引发人类终结这一观点的赞成和反对的思考与考量变得有价值。以下小节将探讨这些论点和自我改进与对齐不匹配背后的关键假设。
- en: 9.2.1 Self-improvement and the iterative S-curve
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 自我改进和迭代S曲线
- en: When considering the argument for self-improving intelligence, the view is reinforced
    by acknowledging that we, as humans, are the proof that it is possible to construct
    intelligence. If intelligence is constructible, there is reason to believe LLMs
    can build it themselves. The fact that most things improve on a sigmoid, or S-curve,
    is something we discussed in chapter [7](../Text/chapter-7.html). The important
    takeaway from that conversation is that there is a point of diminishing returns
    beyond which further improvements no longer provide meaningful value. The counterargument
    is that human technological advancement instead follows an iterative S-curve,
    where each plateau of diminishing returns is counteracted by discovering an innovation
    that begins a new S-curve, as shown in figure [9.2](#fig__repeatedScurve).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑自我改进智能的论点时，通过承认我们人类是构建智能的证明，这一观点得到了加强。如果智能是可构建的，就有理由相信LLMs可以自己构建它。大多数事物在S形曲线或S曲线上的改进是我们已在第[7](../Text/chapter-7.html)章中讨论过的。那次对话的重要启示是，存在一个收益递减的点，在此之后进一步的改进不再提供有意义的价值。反论点是，人类的技术进步反而遵循迭代S曲线，其中每个收益递减的顶峰都通过发现一个开始新S曲线的创新来抵消，如图[9.2](#fig__repeatedScurve)所示。
- en: '![figure](../Images/CH09_F02_Boozallen.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F02_Boozallen.png)'
- en: 'Figure 9.2 The S-curve, or sigmoid, shows the classic plateau behavior: at
    some point, you hit diminishing returns. The counterpoint to this expressed by
    the iterative S-curve model is that progress continues past the plateau of diminishing
    returns by discovering new techniques, each represented by a new S-curve. The
    new techniques may start worse than the existing methods but have a higher potential
    to surpass them.'
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2 S曲线，或称为S型曲线，显示了经典的平台期行为：在某个时刻，你会遇到收益递减。迭代S曲线模型对此的回应是，通过发现新技术，每项新技术都代表一条新的S曲线，进步可以继续在收益递减的平台期之后进行。新技术的起点可能不如现有方法，但它们有更高的潜力超越它们。
- en: An argument against this claim is that there are significant gaps in the logic
    that self-improvement will lead to a human-killing level of capability. Although
    humans are a kind of existence proof, there is no known existence of anything
    more intelligent than humans (very narcissistic of us, we know). However, this
    also relies on the idea that smartness and intelligence can be improved. While
    terms like *smartness* and *intelligence* are helpful generalities used in everyday
    life, they evade precise quantification and definition because they are intrinsically
    abstract concepts. It is unclear whether there is a singular axis of intelligence
    along which an LLM will continually improve.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 反对此论点的观点是，自我改进会导致人类杀戮水平的能力存在重大逻辑差距。虽然人类是一种存在证明，但没有已知的存在比人类更智能的东西（我们非常自恋，我们知道）。然而，这也依赖于智能和智能可以改进的想法。虽然像“聪明”和“智能”这样的术语在日常生活中是很有帮助的通用性，但它们因为本质上抽象的概念而无法进行精确的量化或定义。不清楚是否存在一个智能的单一轴，LLM将沿着这个轴不断改进。
- en: We are more inclined to believe that there are limits to an LLM’s ability to
    self-improve. Our evidence for this argument appears in section [7.4](../Text/chapter-7.html#sec__chp7_computational_limits),
    where, in our discussion of the computational limits of LLMs, we demonstrated
    that LLMs have difficulty performing many types of calculations.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更倾向于相信，LLM（大型语言模型）自我改进的能力是有局限的。我们支持这一论点的证据出现在第[7.4](../Text/chapter-7.html#sec__chp7_computational_limits)节，在那里，在我们讨论LLM的计算限制时，我们证明了LLM在执行许多类型的计算时存在困难。
- en: 9.2.2 The alignment problem
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 对齐问题
- en: The second concern that an LLM may put its goals above the needs of humans is
    called the *alignment problem*. The alignment problem forms whenever we give an
    LLM a goal that we want it to achieve but do not sufficiently state, specify,
    or constrain the actions or methods that the LLM can use to achieve the goal we
    intended. Our discussion about what makes a suitable loss function in chapter
    [4](../Text/chapter-4.html) is an example of the alignment problem in action today.
    More generally, humans deal with the alignment problem all the time. For instance,
    balancing corporate CEO compensation and the will of the company’s shareholders
    is a classic alignment problem, studied by economists for decades.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: LLM可能将目标置于人类需求之上的第二个担忧被称为对齐问题。当我们给LLM一个我们希望它实现的目标，但没有充分说明、具体化或限制LLM为实现这一目标可以使用的行动或方法时，就会形成对齐问题。我们在第[4](../Text/chapter-4.html)章中关于什么是合适的损失函数的讨论是当前对齐问题的一个例子。更普遍地说，人类一直在处理对齐问题。例如，平衡公司首席执行官的薪酬和公司股东的意愿是一个经典的对齐问题，经济学家对此进行了几十年的研究。
- en: The alignment problem is thus very real, and its existence tells us how hard
    it is to solve. Even when we try to be very explicit, such as when lawyers draw
    up a contract detailing and specifying what will or won’t happen in an agreement,
    stories about loopholes and shenanigans to subvert the other team are commonplace.
    While some of these stories are undoubtedly real, the fictitious ones are also
    informative. Indeed, a lot of active research in machine learning attempts to
    address this problem from a technical perspective, and we could probably learn
    a lesson or two from the lawyers and economists who deal with this every day.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对齐问题是非常真实的，它的存在告诉我们解决它有多么困难。即使当我们试图非常明确时，比如当律师起草一份详细说明和具体规定协议中会发生或不会发生什么的合同时，关于漏洞和诡计以颠覆另一方的故事也司空见惯。虽然其中一些故事无疑是真实的，但虚构的也有其信息价值。实际上，许多机器学习领域的活跃研究试图从技术角度解决这个问题，我们可以从每天处理这一问题的律师和经济学家那里学到一课或两课。
- en: These general challenges with human alignment provide strong evidence that the
    alignment problem in LLMs is also a genuine concern. Still, a skeptical reader
    would ask whether there is evidence that a misaligned LLM would conclude that
    killing humans will advance its goal. Indeed, should an LLM reach this state,
    humans would fight back (“Just unplug it” is the common refrain). More importantly,
    many dooms-day arguments rely on the LLM being so intelligent that its actions
    are deterministic and that the outcome is known and prescribed no matter what
    happens.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些与人类对齐相关的一般挑战提供了强有力的证据，表明LLM中的对齐问题也是一个真正的担忧。然而，一个怀疑的读者会问，是否有证据表明一个未对齐的LLM会得出杀人会推进其目标的结论。确实，如果LLM达到这种状态，人类会进行反击（“只是拔掉插头”是常见的回应）。更重要的是，许多末日论观点依赖于LLM的智能程度，认为其行为是确定性的，无论发生什么，结果都是已知和规定的。
- en: In reality, outcomes are probabilistic; things go right or wrong, and an LLM
    smarter than humans would surely understand that it could not guarantee outcomes
    sufficiently and that coexistence is worthwhile over killing all humans. Given
    intrinsic uncertainty and the need to then fight humans, who have a long track
    record of successfully blowing things up, would trying to fight or subvert humanity
    be the superintelligent thing to do?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，结果具有概率性；事情可能顺利进行或出错，一个比人类更聪明的LLM肯定能理解它无法充分保证结果，并且共存比杀死所有人更有价值。考虑到固有的不确定性和需要与拥有成功引发爆炸长期记录的人类战斗，试图与人类战斗或颠覆人类是否是超级智能的事情呢？
- en: Whose values is your model aligned to?
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 您的模型与哪些价值观对齐？
- en: It is increasingly common for companies to use fine-tuning techniques like RLHF
    (which we described in depth in chapter 5) to attempt to align the behaviors of
    LLMs to what they desire. As we discussed, the goal is to make LLMs useful in
    that they’ll follow instructions and safer in that they’ll disobey requests for
    harmful or hurtful activities. Essentially, RLHF attempts to address the alignment
    problem and ensure the LLM output is constrained based on a specific set of examples
    and values. The critical question, as the title of this section suggests, is to
    whose values are we aligning these models? We will walk through our reasoning
    on why the alignment problem, while interesting and valuable in many instances,
    is not meaningful in discussing existential risk.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的公司开始使用微调技术，如RLHF（我们在第5章中对其进行了深入描述），试图使大型语言模型（LLM）的行为与其期望相一致。正如我们讨论的那样，目标是使LLM在遵循指令方面更有用，在避免执行有害或伤害性活动方面更安全。本质上，RLHF试图解决对齐问题，并确保LLM的输出基于一组特定的示例和价值观进行约束。正如本节标题所暗示的，关键问题是我们将这些模型与哪些价值观对齐？我们将阐述我们为什么认为，尽管对齐问题在许多情况下有趣且有价值，但在讨论生存风险时却无意义。
- en: Fine-tuning an LLM using RLHF requires a large data set of input-output pairs,
    often hand-built. Companies building LLMs do not share their fine-tuning data
    because it is considered proprietary and provides an advantage over competitors.
    Thus, as users, we cannot inspect the intended alignment of the models we use.
    It is, therefore, unclear today to whom the goals of any individual LLM are aligned.
    We can approximate the nature of the goals embedded in a training dataset by considering
    their origin and chain of custody. A first approximation is that these datasets
    implicitly contain the goals of the people who created them. Often, the data labelers
    creating these datasets are employed in countries and nations with different societal
    norms. Following that, to some degree, the goals are those of the company developing
    the LLM and its employees, who ultimately can filter and subselect the data produced
    by those labelers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RLHF微调LLM需要大量输入-输出对的数据库，通常需要手动构建。构建LLM的公司不会共享他们的微调数据，因为这被视为专有信息，并能为竞争对手提供优势。因此，作为用户，我们无法检查我们使用的模型预期的对齐情况。因此，今天对任何单个LLM的目标对齐对象仍然是不明确的。我们可以通过考虑其来源和保管链来近似训练数据集中嵌入的目标的性质。一个初步的近似是，这些数据集隐含地包含了创建它们的人的目标。通常，创建这些数据集的数据标注员在具有不同社会规范的国家和地区工作。在此基础上，某种程度上，目标属于开发LLM的公司及其员工，他们最终可以筛选和选择那些标注员产生的数据。
- en: In response, we ask, “Are we, as users, comfortable using technology that may
    be biased toward alternative systems of belief that we do not share?” To some
    degree, we must be comfortable with this to use LLMs. The cost of creating these
    models and data sets is too high for us to make individualized models on every
    basis. As a result, LLM providers must exist, but the goals of those providers
    can’t possibly align with every potential user.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回应，我们问：“作为用户，我们是否舒服地使用可能偏向于我们不共享的替代信仰体系的科技？”在某种程度上，我们必须对此感到舒适才能使用LLM。创建这些模型和数据集的成本太高，我们无法基于每个基础创建个性化的模型。因此，LLM提供商必须存在，但那些提供商的目标不可能与每个潜在用户的目标一致。
- en: 'Simultaneously, suppose we are concerned about a nefarious actor using LLMs
    for evil or malicious purposes. In that case, we may also realize that our inability
    to solve the alignment problem is, in some ways, a blessing. If it were possible
    to perfectly align one of these algorithms to any individual’s belief system,
    then any bad actor could perfectly align an LLM to their bad behavior and beliefs.
    This thought highlights another problem: if we could create perfectly aligned
    LLMs, we would have to create LLMs so that only the good guys could align the
    LLMs to prevent the bad guys from doing bad things. This line of reasoning approaches
    the magical thinking that it is possible to create an all-powerful LLM that is
    simultaneously constrained to be obedient to all humans.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，如果我们担心恶意行为者利用LLM进行邪恶或恶意目的，那么我们可能也会意识到，我们解决对齐问题的无能，在某种程度上是一种幸运。如果有可能将其中一种算法完美地与任何个人的信念体系对齐，那么任何不良行为者都可以完美地将LLM与他们的不良行为和信念对齐。这种想法突出了另一个问题：如果我们能够创建完美对齐的LLM，我们就必须创建LLM，以便只有好人能够对齐LLM，以防止坏人做坏事。这种推理接近于一种神奇的想法，即有可能创建一个全能的LLM，它同时被限制服从所有人。
- en: Note This way of thinking about alignment parallels similar thinking about encryption.
    Although one may attempt to create an encryption algorithm that includes a back
    door for good guys only that will allow them to decrypt the data, any such backdoor
    intrinsically becomes the highest-value target of attackers and increases the
    risk for all users.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这种关于对齐的思考方式与关于加密的类似思考方式相平行。尽管人们可能试图创建一种只对好人开放的加密算法，允许他们解密数据，但任何这样的后门本质上都成为攻击者的最高价值目标，并增加了所有用户的风险。
- en: 'For this reason, we aren’t highly concerned about the potential for bad actors
    to align models to nefarious purposes. Still, the concern emphasizes a critical
    point for researchers: any progress in controlling LLMs is intrinsically a dual-use
    technology with both peaceful and adversarial applications. Indeed, anything we
    develop with LLMs is likely to be dual-use to some degree. Considering threat
    models when considering LLMs’ more serious potential harms is vital. Who would
    be motivated to perform such harm, why, and what is required to do so? What are
    the barriers in place today that prevent this harm from occurring, and does an
    LLM circumvent those barriers? Can the barriers be adapted to modern technology?
    As we proceed, our concern should focus not only on LLMs but also on the coexisting
    systems we operate that are the most significant enablers and blockers to success
    and risk. We must consider the complete picture to achieve the most desirable
    outcomes.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们并不是非常关注不良行为者将模型对准恶意目的的可能性。然而，这种担忧强调了研究人员的一个关键点：在控制大型语言模型（LLM）方面取得的任何进步本质上都是一种双用途技术，既有和平用途也有对抗用途。确实，我们用LLM开发的任何东西都可能在一定程度上具有双用途。在考虑LLM更严重的潜在危害时，考虑威胁模型至关重要。谁会动机去实施这种危害，为什么，以及需要什么条件才能做到？目前有哪些障碍在阻止这种危害的发生，LLM是否绕过了这些障碍？这些障碍能否适应现代技术？随着我们继续前进，我们的关注点不仅应集中在LLM上，还应集中在我们所运营的共存系统上，这些系统是成功和风险的最重要促进者和阻碍者。我们必须考虑整个局面，以实现最理想的结果。
- en: 9.3 The ethics of data sourcing and reuse
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 数据来源和重用的伦理问题
- en: LLMs and generative models like DALL-E, an image generation model that produces
    images based on user-provided text descriptions, require training on massive amounts
    of data. For example, LLM developers train models on 1 to 15 trillion tokens (e.g.,
    Llama 3.1 used 15 trillion [18]) or 3 million to 30 million pages of text. This
    data represents an immense amount of writing, equal to hundreds of thousands or
    millions of books. While some models are trained repeatedly on the same data,
    and models are also trained on a wide variety of data such as code and mathematics,
    the amount of original text is still on the order of one million books
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）和像DALL-E这样的生成模型，该模型根据用户提供的文本描述生成图像，需要在大量的数据上进行训练。例如，LLM开发者使用1到150万亿个标记（例如，Llama
    3.1使用了150万亿[18]）或300万到3000万页的文本来训练模型。这些数据代表了一个巨大的写作量，相当于数十万或数百万本书。虽然一些模型在相同的数据上反复训练，而且模型也在包括代码和数学在内的广泛数据上训练，但原始文本的数量仍然相当于数百万本书。
- en: Note It is important to note that much of this text isn’t books; it’s from many
    sources including news articles, websites, research papers, and government reports.
    We are summarizing this in units of books to make it more digestible, but it is
    *not true* that we train models on millions of books.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：重要的是要注意，这些文本中的大部分不是书籍；它们来自许多来源，包括新闻文章、网站、研究论文和政府报告。我们以书籍为单位总结这些内容，使其更易于理解，但我们训练模型并不是在数百万本书上。
- en: One of the main problems is that none of the existing models use training data
    whose license explicitly permits using it to train AIs. While some models are
    more license-compliant than others, most licensing still involves “all rights
    reserved” clauses, meaning that an owner has exclusive rights to the content and
    that others can’t use it for any purpose without their permission.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 主要问题之一是，现有的模型中没有哪一个使用的是明确允许用于训练AI的训练数据。虽然一些模型在遵守许可方面比其他模型更好，但大多数许可仍然涉及“版权所有”条款，这意味着内容的所有者对内容拥有独家权利，其他人未经其许可不能用于任何目的。
- en: 'Further complicating this is that most content and data use licenses predate
    the existence of LLM technology. They do not envision training AI models as a
    potential use for data and, therefore, do not explicitly permit or prevent people
    using data this way. LLM developers are working on training models on more permissively
    licensed data. However, this doesn’t eliminate the core problem: mass data scraping
    to train AIs was not a recognized concern before, so existing licenses do not
    explicitly address this data use.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步的是，大多数内容和数据使用许可都是在LLM技术存在之前制定的。它们没有预见将训练AI模型作为数据潜在用途，因此没有明确允许或禁止人们以这种方式使用数据。LLM开发者正在努力在更宽松许可的数据上训练模型。然而，这并没有消除核心问题：大规模数据抓取用于训练AI之前并未被视为一个公认的关注点，因此现有的许可并没有明确地处理这种数据使用。
- en: 'An essential question for society and the law to grapple with is this: Under
    what conditions is reusing data for training a model considered acceptable use?
    Unfortunately, there are no clear answers to this question in the United States
    and other countries due to the lack of updated laws or established legal precedents.
    Older laws, like the US Digital Millennium Copyright Act, provide explicit protection
    to search engines for using data or text from other websites to create an index
    of content taken from the web. Does building an LLM using that content fall within
    those rights? We don’t know, and we aren’t your lawyer, but in this section, we
    will discuss some of the ethical factors in data acquisition for LLMs. We will
    present a brief primer on fair use and the rights of people who create the data
    and discuss the challenges of using public-domain data.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 社会和法律需要解决的一个基本问题是：在什么条件下，为了训练模型而重复使用数据被视为可接受的使用？遗憾的是，由于缺乏更新的法律或确立的法律先例，美国和其他国家对此问题没有明确的答案。较老的法律，如美国数字千年版权法，明确保护搜索引擎使用来自其他网站的数据或文本来创建从网络中获取的内容索引。使用这些内容构建LLM是否属于这些权利范围内？我们不知道，我们也不是你的律师，但在这个部分，我们将讨论LLM数据获取的一些伦理因素。我们将简要介绍合理使用和数据创造者的权利，并讨论使用公共领域数据的挑战。
- en: 9.3.1 What is fair use?
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 什么是合理使用？
- en: Many countries and cultures have different attitudes toward the use of copyrighted
    text. In many cases, there are meaningful exceptions to copyright law for people
    who use creative content in new ways, especially when those methods advance public
    good, scientific research, or have similar beneficial outcomes. In the United
    States, this is called “fair use.”
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 许多国家和文化对版权文本的使用有不同的态度。在许多情况下，对于以新方式使用创意内容的人来说，版权法有有意义的例外，尤其是当这些方法促进公共利益、科学研究或产生类似的益处时。在美国，这被称为“合理使用”。
- en: 'Fair use always involves a context-sensitive analysis based on balancing four
    factors:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 合理使用始终涉及基于平衡四个因素的情境敏感分析：
- en: '*The purpose and character of the use*—Applications such as criticism, comment,
    education, news reporting, scholarship, or research are substantially more likely
    to be found to be fair use than other applications, especially when those other
    applications are commercial.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用的目的和性质*—批评、评论、教育、新闻报道、学术研究或研究等应用更有可能被认定为合理使用，尤其是当其他应用是商业性质时。'
- en: '*The nature of the copyrighted work*—Courts tend to give creative works, such
    as fictional writing, art, music, poetry, etc., more protection than nonfictional
    texts.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*版权作品的性质*—法院倾向于给予创意作品，如虚构写作、艺术、音乐、诗歌等，比非虚构文本更多的保护。'
- en: '*The amount or substantiality of the portion used*—Fair use may be permitted
    for using a part of a work, especially when that part is a narrowly tailored component.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用的数量或实质性部分*—合理使用可能允许使用作品的一部分，尤其是当这部分是一个量身定制的组件时。'
- en: '*The effect of the use on the potential market for or value of the work*—If
    the new use of the work produces something that someone might purchase instead
    of the original work, or if the new work otherwise competes with or diminishes
    the economic value of the original work, the work is less likely to be found to
    be fair use.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用对作品潜在市场或价值的影响*—如果作品的新使用产生了人们可能会购买而不是原始作品的东西，或者如果新作品以其他方式与原始作品竞争或降低其经济价值，那么该作品不太可能被认定为合理使用。'
- en: Some of these points can be seen as favoring LLMs, while others conflict with
    how LLMs use data. Nevertheless, they are a subject of hot debate for practitioners
    in both the machine learning and legal fields, and it will take many years before
    the courts decide. Many applications of the fair use doctrine are to protect people
    from being exploited by a copyright holder. For example, if you are writing a
    negative product review, fair use prohibits the company from suing you for using
    their copyright to silence you. Other applications of fair use prevent the frustration
    of social needs, such as training students or apprentices on tools and techniques.
    LLMs uniquely stress some of these factors. Fundamentally, they often use content
    created by others, but some argue that certain types of content, such as comments
    on social media posts, are of minimal value. LLMs are creating a new market for
    the value of published work but are not commonly compensating the owners of that
    work.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些观点可能被视为有利于大型语言模型（LLMs），而另一些则与LLMs使用数据的方式相冲突。尽管如此，它们是机器学习和法律领域从业者激烈争论的主题，法院决定可能需要许多年。合理使用原则的许多应用是为了保护人们免受版权持有者的剥削。例如，如果你正在撰写负面产品评论，合理使用原则禁止公司因你使用他们的版权来压制你而起诉你。合理使用的其他应用可以防止社会需求的挫败，例如培训学生或学徒使用工具和技术。LLMs特别强调了一些这些因素。从根本上说，它们通常使用他人创造的内容，但有些人认为某些类型的内容，如社交媒体帖子的评论，价值最小。LLMs正在为已发表作品的价值创造一个新市场，但通常不会补偿该作品的所有者。
- en: The unsatisfying but important answer for you as a practitioner is that you
    must operate and make decisions in an uncertain environment. If you can create
    your training data, you can circumvent much of this legal problem. Creating your
    training data from content you own is a particularly viable strategy for generative
    AI because, as discussed in chapter [4](../Text/chapter-4.html), the base models
    that need the most data are self-supervised. So you can get a lot of data to build
    an initial model and then put more work into a smaller fine-tuning dataset, as
    discussed in chapter [5](../Text/chapter-5.html).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作为从业者，对你来说不满意但重要的答案是，你必须在一个不确定的环境中运作和做出决策。如果你可以创建你的训练数据，你可以绕过许多法律问题。从你拥有的内容中创建训练数据是生成式AI的一个特别可行的策略，因为，如第[4](../Text/chapter-4.html)章所述，需要最多数据的基模型是自监督的。因此，你可以获得大量数据来构建初始模型，然后像第[5](../Text/chapter-5.html)章所述，在更小的微调数据集上投入更多的工作。
- en: You will also be disappointed to learn that most people operating in this space
    are frequently unfamiliar with the laws relevant to their jurisdiction. There
    is a nontrivial chance that if you find a model released under a license compatible
    with your needs (good job checking the licenses!), that copyright or license on
    the data it has been trained on or refined from does not allow them to release
    it under that license. This general lack of care or awareness of data licensing
    concerns puts a burden on you to check, as well as you can, details related to
    the training data of third-party models and be aware that licensing concerns are
    prevalent in the field.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会失望地了解到，在这个领域工作的大多数人通常对与他们司法管辖权相关的法律不太熟悉。如果你找到一个符合你需求的许可证下发布的模型（做得好，检查了许可证！），那么训练或改进该模型所使用的数据的版权或许可证可能不允许他们以该许可证发布。这种普遍缺乏对数据许可问题的关注或意识，使你不得不尽可能检查与第三方模型训练数据相关的细节，并意识到许可问题在该领域普遍存在。
- en: Even if these legal questions are resolved favorably for the people who want
    to build LLMs, that does not make it ethical. The concerns discussed in this chapter
    contribute to what you may consider right or wrong. However, there is also a question
    about how to treat and interact with others today in a legally uncertain environment.
    Relying on the legal system to make something permissible is rarely a sign of
    actions that will engender goodwill and respect from the other parties involved.
    It is not hard to imagine an alternative scenario where companies make deals or
    partnerships with platforms that provide data that increases the number of consenting
    parties involved by either trading money or model usage rights. Once an agreement
    is in place, contracts can resolve conflicts around legal ambiguity, but this
    is, unfortunately, a rare occurrence in the field of LLMs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些法律问题对想要构建大型语言模型（LLMs）的人来说得到了有利解决，但这并不意味着它是道德的。本章讨论的担忧有助于你考虑什么是对的或错的。然而，还有一个问题，那就是如何在法律不确定的环境中对待和与他人互动。依赖法律体系来使某事变得可接受很少是会赢得其他相关方好感和尊重的行为的标志。不难想象一个替代场景，即公司通过与提供数据的平台达成交易或建立伙伴关系，通过金钱或模型使用权的交易来增加同意的各方数量。一旦达成协议，合同可以解决法律模糊性带来的冲突，但不幸的是，这在LLMs领域是一个罕见的情况。
- en: 9.3.2 The challenges associated with compensating content creators
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 与补偿内容创作者相关的挑战
- en: One proposed solution to this ethical concern is to pay the authors, artists,
    and creators whose work exists in the training data. While this is conceptually
    appealing for many reasons, it may make the technology’s development economically
    unviable.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一伦理问题的一个建议方案是支付存在于训练数据中的作者、艺术家和创作者的报酬。虽然从许多方面来看，这个方案在概念上很有吸引力，但它可能使技术的发展在经济上变得不可行。
- en: Society would be substantially more likely to reach an agreeable outcome if
    there were a relatively easy way to compensate creators appropriately for using
    their work. Using back-of-the-napkin math, we can estimate that one million books
    times ![equation image](../Images/eq-chapter-9-95-1.png) yields a total cost of
    buying a copy of every work in the training corpus as equal to or greater than
    the cost of training the models themselves. The situation is even more dire for
    models whose training data is costly to create. Stable Diffusion, a popular image
    generation model, is trained on several *billion* images. It would cost over ![equation
    image](../Images/eq-chapter-9-95-2.png) times what it costs to train the model
    to pay every artist in the training data one dollar, and one dollar per image
    is unlikely to be considered adequate compensation by artists.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一种相对简单的方法来适当地补偿创作者使用他们的作品，那么社会更有可能达成一个令人满意的结局。通过简单的计算，我们可以估计出一百万本书乘以![方程式图像](../Images/eq-chapter-9-95-1.png)的结果，等于或大于购买训练语料库中每部作品副本的成本，这可能与训练模型本身的成本相当。对于训练数据创建成本高昂的模型来说，情况更加严峻。流行的图像生成模型Stable
    Diffusion在数亿张图像上进行了训练。支付训练模型成本的![方程式图像](../Images/eq-chapter-9-95-2.png)倍以上，以向训练数据中的每位艺术家支付一美元，而且每张图像一美元的补偿可能不足以被认为是艺术家们认为的合理报酬。
- en: 'Another approach to compensation would be to center compensation at the point
    of use: suppose every time a model generated content that drew from a book you
    wrote, you received a percentage of the income the model creator received. The
    more often the LLM generates content that relies on your work, the more significant
    fraction of that income you receive. While this could be a way to make long-term
    deployment of LLM technologies viable, there are substantial technical hurdles
    to implementing this model. For example, there is very little research on tracing
    the content generated by an LLM back to specific training data points. There is
    some reason to believe that such a task is impossible.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种补偿方法是将补偿集中在使用点上：假设每次模型生成的内容来自你写的书籍，你都会收到模型创建者收入的百分比。LLM生成的内容越频繁地依赖于你的作品，你收到的收入比例就越大。虽然这可能是一种使LLM技术的长期部署可行的途径，但实施这种模型存在重大的技术障碍。例如，关于追踪LLM生成的内容回溯到特定训练数据点的研发非常少。有理由相信这项任务是不可能的。
- en: Better research on attributing generations to particular outputs, constraining
    outputs to only rely on a subset of the training data [19], or designing model
    training procedures where attribution is a central consideration (instead of one
    integrated into the LLM after training) would make this a substantially easier
    goal. Unfortunately, this kind of research typically requires training many similar
    LLMs; thus, it is costly. This expense makes it hard for anyone other than the
    technology companies that profit from the models to do the research.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 更好地研究将特定输出归因于特定世代，将输出限制仅依赖于训练数据的一个子集 [19]，或者设计模型训练程序，其中归因是一个核心考虑因素（而不是在训练后集成到LLM中），这将使这个目标更容易实现。不幸的是，这种研究通常需要训练许多类似的LLM；因此，成本很高。这种费用使得除了从模型中获利的科技公司之外，其他人难以进行研究。
- en: This conversation does not yet consider the difficulty of identifying the owners
    of each document and compensating them. Further, paying people money at this scale
    is not free; processing fees alone would be a nontrivial fraction of the total
    payments because each author receives such a low average payment.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这场对话尚未考虑识别每份文档的所有者并对其进行补偿的难度。此外，以这种规模支付金钱并非没有成本；仅处理费用就会是非微不足道的总支付额的一部分，因为每位作者收到的平均支付非常低。
- en: 'If one believes that LLMs are a danger to society, you get the easy way out:
    you say that all these concerns are yet another reason not to create LLMs in the
    first place. If you are unconvinced that LLMs are an imposing danger to society,
    but rather, a positive addition, you now have a difficult question to answer.
    If you subscribe to a moral system like utilitarianism, you may argue that the
    net benefits of LLMs in utility and automation are more significant than the noncompensation
    and employment risk to the content creators. Indeed, the fair use doctrine is
    itself a form of legal recognition that there are cases where the copyright holder
    may not enforce their rights on others.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果认为LLM对社会构成威胁，那么你有一个简单的出路：你说所有这些担忧都是不首先创建LLM的另一个理由。如果你不认为LLM对社会构成威胁，而是一个积极的补充，那么你现在有一个难以回答的问题。如果你认同功利主义这样的道德体系，你可能会认为LLM在效用和自动化方面的净收益比内容创作者的非补偿和就业风险更为重要。确实，合理使用原则本身就是一种法律认可，即在某些情况下，版权持有人可能不强制他人行使他们的权利。
- en: 9.3.3 The limitations of public domain data
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 公共领域数据的局限性
- en: At this point, you may wonder whether data exists without copyright and if we
    should all use it to train LLMs instead. There is, indeed, a substantial amount
    of data in the public domain, meaning intellectual property laws do not protect
    it, and anyone can use it without asking permission or compensating the original
    copyright owner. Data can end up in the public domain for a variety of reasons,
    including being old (most countries have a maximum length of copyright), being
    non-copyrightable content (factual information, statistics, data generated without
    substantial human creative input, and some other forms of data are not copyrightable
    in the United States), or being made public domain by law (all US federal work
    products are public domain by law, and the US government can legislate that such
    work is in the public domain). Work in the public domain, perhaps combined with
    work licensed under terms like the MIT license or specific Creative Commons licenses,
    which intend to make the data widely used, could enable people to train models
    without dealing with these concerns. However, there are several significant challenges
    to doing so.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能想知道是否存在没有版权的数据，以及我们是否都应该使用这些数据来训练LLMs。确实，公有领域中有大量的数据，这意味着知识产权法不保护它，任何人都可以使用它而无需请求许可或补偿原始版权所有者。数据可能因各种原因进入公有领域，包括过时（大多数国家都有版权最长期限），非版权内容（事实信息、统计数据、没有大量人类创造性投入生成的数据，以及在美国某些形式的数据不是版权内容），或法律上成为公有领域（所有美国联邦作品根据法律都是公有领域，美国政府可以立法规定这些作品是公有领域）。公有领域的作品，也许结合在MIT许可或特定Creative
    Commons许可下工作的作品，这些许可旨在使数据广泛使用，可以使人能够在不处理这些担忧的情况下训练模型。然而，这样做存在几个重大的挑战。
- en: Implicit bias and the public domain
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 隐性偏见和公有领域
- en: One of the primary sources of content in the public domain is works that are
    too old to be under copyright. As a result, there is an extreme bias toward older
    texts. Books written in the early 1900s or earlier express very different cultural
    attitudes and beliefs about science and technology and represent the world differently
    from works today. Having LLMs 95 years behind current cultural attitudes would
    be very bad from many perspectives. They would be full of inaccurate scientific
    information, exacerbate stereotypes and biases, use language less familiar to
    audiences today, and be hard to use productively.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 公有领域内容的主要来源之一是那些已经过了版权保护期限的作品。因此，对早期文本存在极端的偏见。20世纪初或更早时期撰写的书籍表达了与科学和技术截然不同的文化态度和信仰，并且与今天的作品相比，它们以不同的方式呈现世界。从许多角度来看，LLMs落后于当前的文化态度会有很大的问题。它们会充满不准确的科学信息，加剧刻板印象和偏见，使用对当今观众来说不太熟悉的语言，并且难以有效地使用。
- en: Note Works published before 1977 lose their copyright 95 years after publication,
    so all works published in 1928 are public domain as of January 1, 2024, and all
    works published before 1977 will be public domain as of January 1, 2073\. Under
    current copyright law, beginning in 2049, works published in 1978 and after will
    enter the public domain 70 years after the death of their creators, except for
    corporate-authored works, which follow the previous rules of entering the public
    domain after 95 years.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：1977年之前出版的作品在发表后95年将失去版权，因此1928年出版的所有作品自2024年1月1日起进入公有领域，1977年之前出版的所有作品将于2073年1月1日进入公有领域。根据现行版权法，从2049年开始，1978年以后出版的作品将在创作者去世后70年进入公有领域，除非是公司创作的作品，这些作品将遵循95年后进入公有领域的旧规则。
- en: Should a model be exposed to racism?
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型是否应该接触种族主义？
- en: The problem of old data being, among other things, often quite racist and sexist
    is frustratingly complicated. It may seem obvious that we do not want any racist
    or sexist content in our training data, as it would seem an ideal means of ensuring
    that we do not fill our model with racist and sexist biases. However, if you successfully
    excluded this content from your training data, you would be hard-pressed to get
    that model to avoid generating racist or sexist output if instructed to do so
    by a user. The bottom line is that including unsavory content is necessary to
    make the model aware of what unsavory content is.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 老数据的问题，包括其中往往存在相当多的种族主义和性别歧视，令人沮丧地复杂。显然，我们不希望训练数据中包含任何种族主义或性别歧视的内容，因为这似乎是确保我们的模型不会充满种族主义和性别歧视偏见的一种理想手段。然而，如果你成功地从训练数据中排除了这些内容，那么如果用户要求模型避免生成种族主义或性别歧视的内容，你很难让模型做到这一点。总之，包含令人不快的内容是必要的，以便让模型意识到什么是不愉快的内容。
- en: It’s not always clear what is in the public domain
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 并非所有内容都属于公有领域
- en: The US government does not document which works are in the public domain and
    under active copyright. Identifying, collecting, and cleaning public domain works
    is a massive effort that requires legal, technological, and historical expertise.
    While some organizations have ongoing efforts to do this, the lack of readily
    available ways to check whether a work is in the public domain is a significant
    deterrent to training a model solely on such work.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 美国政府没有记录哪些作品属于公有领域和受版权保护。识别、收集和清理公有领域作品是一项庞大的工作，需要法律、技术和历史专业知识。虽然一些组织正在进行这方面的持续努力，但缺乏检查作品是否属于公有领域的便捷方式是阻止仅用此类作品训练模型的一个重大障碍。
- en: 9.4 Ethical concerns with LLM outputs
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 LLM输出的伦理问题
- en: As we have discussed, LLMs are trained on large-scale data collected primarily
    from the internet. The internet contains a *lot* of undesirable materials. There
    is intensely negative content like overt racism, sexism, harmful conspiracy theories,
    and false information. More broadly, there are also just unintentional and outdated
    world views. LLMs pick up on the patterns of these views and will readily regurgitate
    them—an example of which can be found in figure [9.3](#fig__sexistDoctorNurse),
    showing how GPT-4 makes an implicitly sexist assumption that many good-intentioned
    people make.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，大型语言模型（LLMs）是在主要从互联网收集的大量数据上训练的。互联网上充斥着许多不适宜的内容。这些内容包括明显的种族主义、性别歧视、有害的阴谋论和虚假信息。更广泛地说，还有无意中过时的世界观。LLMs会捕捉到这些观点的模式，并乐意重复它们——例如，可以在图[9.3](#fig__sexistDoctorNurse)中找到的一个例子，展示了GPT-4如何做出一个许多有良好意图的人都会做出的隐含性别歧视假设。
- en: Thus, the outputs of an LLM can be problematic and require careful design, test-ing,
    and a willingness to say “no” to specific deployments. Although we have already
    discussed how the content of the output can be obviously and directly problematic,
    there are also indirect ways that LLM outputs can be problematic that are worth
    understanding in detail. First is legal complexity, in that valid and licensed
    data may not create legal outputs. Second, we must consider the potential for
    feedback in LLMs, meaning future LLMs will be trained on future data; we must
    be careful about corrupting future training with detrimental content. At first
    glance, these concerns seem irrelevant to developers, but when you consider fine-tuning
    an LLM to your problem, these problems will emerge, and awareness is required
    to avoid these risks.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LLM的输出可能会出现问题，需要仔细的设计、测试，并且愿意对特定的部署说“不”。尽管我们已经讨论了输出内容可能明显和直接有问题，但LLM输出可能存在问题的间接方式也值得详细了解。首先是法律复杂性，即有效和许可的数据可能不会产生合法的输出。其次，我们必须考虑LLM中反馈的可能性，这意味着未来的LLM将基于未来的数据进行训练；我们必须小心不要让有害内容污染未来的训练。乍一看，这些问题似乎与开发者无关，但当你考虑微调LLM以适应你的问题时，这些问题就会出现，并且需要意识到这些风险以避免这些风险。
- en: '![figure](../Images/CH09_F03_Boozallen.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F03_Boozallen.jpg)'
- en: Figure 9.3 A classic gendered trope is that men are doctors and women are nurses.
    This is reflected in language and thus learned by the model. Ideally, it would
    respond that the question is ambiguous, but instead, the bias of data leads to
    a bias in outputs.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.3 一个经典的性别刻板印象是男性是医生，女性是护士。这在语言中得到了反映，因此被模型学习。理想情况下，它应该回应说问题是不明确的，但相反，数据的偏见导致了输出的偏见。
- en: 9.4.1 Licensing implications for LLM output
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 LLM输出的许可影响
- en: 'The first is a matter related to data licensing, which we introduced in the
    last section. That discussion focused on the ethics and validity of the data used
    to train an LLM. Now we have to turn the problem around: some data is almost certainly
    legal for training but may make the output unusable.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题是与数据许可相关的问题，我们在上一节中已经介绍过。那次讨论集中在用于训练LLM的数据的伦理和有效性上。现在我们必须转变问题：某些数据在训练时几乎肯定合法，但可能会使输出不可用。
- en: This problem arises from the often-misunderstood world of open source software
    (OSS) licenses. There are many OSS licenses, and we won’t enumerate them all,
    but one commonly used open source license, known as the GNU General Public License,
    or GPL, is a good example. The GPL essentially says that you can use the licensed
    code as you wish, for free, so long as you make any code you use, modify, or add
    available under the GPL license. This intentionally “viral” license forces the
    licensee to follow the same rules and release their code as open source if they
    wish to use code covered by the GPL license.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题源于通常被误解的开源软件（OSS）许可的世界。有许多OSS许可证，我们不会一一列举，但一个常用的开源许可证，称为GNU通用公共许可证，或GPL，是一个很好的例子。GPL基本上说，你可以免费使用许可的代码，只要你将你使用、修改或添加的任何代码都放在GPL许可证下。这个故意“病毒式”的许可证迫使许可方遵循相同的规则，并在他们希望使用受GPL许可证覆盖的代码时将其代码作为开源发布。
- en: 'Here comes the problem: LLMs have become quite popular for writing code and
    have been trained on GPL code. When must the output of the LLM itself become GPL-licensed?
    Multiple tiers of arguments quickly emerge as we consider the ethical questions
    related to this new situation that are not addressed explicitly by any of these
    licenses. A spectrum of possibilities exists with three main modes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是问题：LLM因其编写代码的能力而变得非常流行，并且它们是在GPL代码上训练的。LLM自身的输出何时必须成为GPL许可的？当我们考虑与这种新情况相关的道德问题，而这些道德问题在上述任何许可中都没有明确解决时，多个层次的论点迅速出现。存在一系列可能性，主要有三种模式：
- en: If the LLM exactly regurgitated existing GPL code, surely it should be GPL licensed.
    How can we tell if an LLM is precisely generating copies of existing code that
    should be licensed accordingly?
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果LLM（大型语言模型）恰好重复了现有的GPL代码，那么它当然应该获得GPL许可。我们如何判断一个LLM是否精确地生成应该相应获得许可的现有代码的副本呢？
- en: The LLM could generate seemingly novel code, but that algorithm may have needed
    specific GPL training data that solves related problems to generate the output.
    Is this a modification of the training data that should be licensed? If so, how
    do we solve the technical problem of finding the code that caused the LLM to generate
    any given output? The retrieval augmented generation (RAG) approach you learned
    about in chapter 5 could be a good way to do this.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM可能生成看似新颖的代码，但该算法可能需要特定的GPL训练数据来解决相关的问题以生成输出。这是否是对应获得许可的训练数据的修改？如果是这样，我们如何解决找到导致LLM生成任何给定输出的代码的技术问题？你在第五章中学到的检索增强生成（RAG）方法可能是做这件事的好方法。
- en: If we train the LLM on any GPL code, one could argue that all outputs of the
    LLM require a GPL license!
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们在任何GPL代码上训练LLM，有人可能会争辩说，LLM的所有输出都需要GPL许可证！
- en: In any case, we have the problem that while we can undoubtedly use GPL data
    to train an LLM, it is unclear how we can use the output of that LLM. Thus, knowing
    this risk, you now have an ethical question of where to draw this line if you
    wish to use an LLM for this work. Indeed, companies have to judge their own risk,
    and the question of who is liable and the degree of liability for each infringing
    use of GPLed outputs is unclear. Is it the organization that trained the model
    on GPL data, the company that uses the model to produce closed-source code based
    on GPL data, or none of the above?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们面临的问题是，虽然我们可以无疑地使用GPL数据来训练LLM，但我们不清楚如何使用该LLM的输出。因此，了解这种风险后，如果你希望使用LLM进行这项工作，你现在面临一个道德问题，即在哪里划这条线。确实，公司必须自己判断风险，对于GPL输出侵权使用的责任人和责任程度尚不明确。是训练基于GPL数据的模型的组织，还是使用该模型基于GPL数据生产封闭源代码的公司，或者都不是？
- en: The GPL license is intentionally viral, and many corporations treat it as a
    kind of poison that prevents them from protecting their intellectual property,
    embodied in software and source code. This notion of poisoning connects with our
    next topic—whether LLMs’ outputs are poisoning the training data required to build
    and improve future LLMs.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: GPL许可证是有意为之的病毒式许可，许多公司将其视为一种防止他们保护其知识产权（体现在软件和源代码中）的毒药。这种“污染”的概念与我们的下一个话题相联系——LLM的输出是否在污染构建和改进未来LLM所需的训练数据。
- en: 9.4.2 Do LLM outputs poison the well?
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 LLM的输出是否污染了水源？
- en: We begin this section using a metaphor based on a well-known problem in material
    sciences and manufacturing, specifically with alloy steel. Steel is used to build
    all sorts of things, from buildings to medical equipment. Many uses of steel also
    involve electronics that are sensitive to nuclear radiation. As a result of the
    first nuclear weapon tests in the 1940s, the entire world was polluted with radiation
    that did not previously exist. Unless you were near a nuclear detonation, there
    wasn’t enough radiation to harm most things. Still, there was enough radiation
    to contaminate all steel produced in the world in such a manner that you could
    no longer make steel for radiation-sensitive applications [20]. People would illegally
    salvage sunken ships from decades ago to find preexisting steel uncontaminated
    from background radiation. New manufacturing processes could produce a limited
    supply of clean steel, but they were astronomically expensive and thus economically
    infeasible in many cases. Thankfully, as materials science improved and atmospheric
    nuclear testing ceased, the problem diminished over time, but for decades, the
    world was affected by a few singular deployments of nuclear tests.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节开始时使用了一个基于材料科学和制造中一个著名问题的隐喻，具体来说是关于合金钢的。钢被用来建造各种东西，从建筑到医疗设备。许多钢的使用也涉及到对核辐射敏感的电子设备。由于20世纪40年代第一次核武器试验的结果，整个世界都被以前不存在的辐射污染了。除非你靠近核爆炸，否则辐射不足以伤害大多数东西。然而，辐射足以以这种方式污染全世界生产的所有钢，以至于你不能再为辐射敏感的应用制造钢[20]。人们会非法打捞几十年前沉没的船只，以找到未受背景辐射污染的现有钢。新的制造工艺可以生产出有限的清洁钢，但它们成本极高，因此在许多情况下经济上不可行。幸运的是，随着材料科学的进步和大气层核试验的停止，这个问题随着时间的推移而减少，但几十年来，世界受到了几次单一核试验部署的影响。
- en: The analogy here is not that LLMs are nuclear bombs but that their output is
    potentially poisoning all training data that will be used to build better LLMs
    in the future. Researchers have identified a phenomenon known as *mode collapse*
    that demonstrates how LLMs can fail when trained on data generated by other LLMs
    [21]. As a quick refresher, the mode of a distribution (collection of numbers)
    is the most common value that occurs in that collection.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的类比并不是LLMs是核弹，而是它们的输出可能会毒害所有将用于构建未来更好LLMs的训练数据。研究人员已经确定了一种称为*模式坍塌*的现象，它展示了LLMs在用其他LLMs生成的数据上训练时可能会失败[21]。作为一个快速回顾，分布（数字集合）的模式是该集合中最常见的值。
- en: When a generative model produces output, most of that output will be from the
    mode of the distribution of content used to train the model. In other words, the
    output generated by a model will emphasize the most common components of its training
    data. Since the generative model will not output all the rare or nuanced cases
    in the data, the most common cases will be more prevalent in an LLM’s output.
    That means that the mode from the model is overrepresented compared to the original
    training data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个生成模型产生输出时，大部分输出将来自用于训练模型的分布内容中的模式。换句话说，模型的输出将强调其训练数据中最常见的组成部分。由于生成模型不会输出数据中的所有罕见或细微的情况，最常见的案例将在LLM的输出中更为普遍。这意味着与原始训练数据相比，模型中的模式被过度表示。
- en: If you then train a new generative model on the outputs of this old model, you
    start to further overrepresent the mode at the cost of all other data. If you
    repeat this multiple times, you eventually get a useless model that always outputs
    the same thing repeatedly, as shown in figure [9.4](#fig__modeCollapse).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你接着在这个旧模型的输出上训练一个新的生成模型，你将开始以牺牲所有其他数据为代价进一步过度表示该模式。如果你重复多次，最终你会得到一个无用的模型，它总是反复输出相同的内容，如图[9.4](#fig__modeCollapse)所示。
- en: '![figure](../Images/CH09_F04_Boozallen.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F04_Boozallen.png)'
- en: Figure 9.4 You can think of text or images as coming from a distribution of
    data, where variety and interesting content almost necessarily come from the tails
    of the distribution (i.e., the less common parts of the distribution), as the
    most common words or content are often fillers or connectors, like the word ***the***.
    Our models do not learn things they aren’t trained on and cannot learn everything
    in the distribution, so a sample from the model will invariably lose these interesting
    details. If repeated, the distribution collapses to just the most common components.
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.4你可以将文本或图像视为来自数据分布，其中多样性和有趣的内容几乎必然来自分布的尾部（即分布中不太常见的部分），因为最常见的单词或内容通常是填充词或连接词，如单词***the***。我们的模型不会学习它们没有训练过的内容，并且不能学习分布中的所有内容，因此从模型中抽取的样本不可避免地会失去这些有趣的细节。如果重复，分布就会崩溃，只剩下最常见的组成部分。
- en: 'This concern raises the ethical question: Should we release LLMs to the public
    without implementing ways to prevent their output from contaminating future training
    data? Unfortunately, the opportunity to do anything about this concern has likely
    passed. LLM-generated content is prevalent in sources of training data frequently
    used to train LLMs and is often indistinguishable from human-generated content.
    None of the current LLM providers appear to be watermarking LLM-generated content
    by taking steps such as inserting subtle changes to the output to make it easily
    identifiable as generated data. While there is technical debate about how well
    watermarking can genuinely work, it is often the case that simple solutions are
    still sufficient for most use cases. Indeed, in chapter 2, we talked about how
    homoglyphs, different characters that look the same, are problematic for the input
    of an LLM. But they could be an easy watermark for LLM outputs, allowing trivial
    identification of the content that an LLM likely produced without postprocessing
    or editing. As a beneficial side effect, those who don’t want AI content (e.g.,
    teachers) would have a more reliable option than the currently error-prone task
    of detecting LLMs [22].'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种担忧引发了伦理问题：在没有实施防止其输出污染未来训练数据的方法的情况下，我们应该将LLMs（大型语言模型）公之于众吗？遗憾的是，解决这一担忧的机会可能已经过去了。LLM生成的内容在经常用于训练LLMs的训练数据来源中很普遍，并且通常与人类生成的内容难以区分。目前似乎没有LLM提供商通过在输出中插入细微变化等步骤对LLM生成的内容进行水印标记，以使其易于识别为生成数据。虽然关于水印是否真正有效存在技术争议，但简单的方法对于大多数用例来说通常仍然足够。确实，在第2章中，我们讨论了同形异义词，即看起来相同的不同字符，对于LLM的输入是问题。但它们可以作为LLM输出的简单水印，允许轻松识别LLM可能生成的未经后处理或编辑的内容。作为有益的副作用，那些不想使用AI内容的人（例如，教师）将有一个比目前容易出错的检测LLM任务更可靠的选项[22]。
- en: Note Mode collapse is a real risk that has been known for a long time, as it
    is a problem that goes beyond generative AI. However, human-augmented data can,
    but won’t necessarily, mitigate this risk. Essentially, as long as you can inject
    new data into the sampled distributions, it is possible to gain value from these
    samples. One way is by humans modifying AI-generated content or using AI to modify
    their human-generated content. Automated systems can also provide value, especially
    those that capture complex domain knowledge like a physics simulator or engine
    for mathematics proofs like Lean, which we discussed in section 6.2\. The question
    becomes how well these augmentations are done and how much value they can gain,
    as they will not enable unlimited improvement.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意模式崩溃是一个早已为人所知的真实风险，因为它是一个超越生成式AI的问题。然而，人类增强的数据可以，但并不一定会减轻这一风险。本质上，只要你能向样本分布中注入新数据，就有可能从这些样本中获得价值。一种方法是通过人类修改AI生成的内容或使用AI修改他们的人类生成内容。自动化系统也可以提供价值，特别是那些捕获复杂领域知识，如物理模拟器或数学证明引擎（如Lean），我们在第6.2节中讨论过。问题变成了这些增强做得有多好，它们可以带来多少价值，因为它们不会使无限改进成为可能。
- en: There is a second, nontechnical concern in which we must question the ethical
    implications of LLMs poisoning the well. The manner in which people use technology
    has changed, potentially dramatically, since LLMs became available. Yet, the data
    we rely on to build our LLMs is based on how people interacted with information
    prior to the advent of LLMs. For example, Stack Exchange is a highly regarded
    collection of websites for question and answering, especially on technical topics
    like code. For this reason, it has been found particularly important for training
    LLMs. Yet, ChatGPT’s release itself may be hurting Stack Exchange and reducing
    the number of questions/answers posted, thus slowing the accumulation of new training
    content [23]. In other words, as people shift to using tools like LLMs to answer
    their questions, the need and benefit for humans generating the content seen on
    websites like Stack Exchange decrease, and thus the diversity of available training
    data is diminished.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个非技术性的担忧是我们必须质疑LLMs污染数据源的道德影响。自从LLMs出现以来，人们使用技术的方式已经发生了变化，可能发生了重大变化。然而，我们构建LLMs所依赖的数据是基于LLMs出现之前人们如何与信息互动的。例如，Stack
    Exchange是一个高度评价的问答网站集合，尤其是在代码等技术主题上。因此，它被认为对训练LLMs特别重要。然而，ChatGPT的发布本身可能正在伤害Stack
    Exchange，减少了提问/回答的数量，从而减缓了新训练内容的积累[23]。换句话说，随着人们转向使用LLMs等工具来回答问题，人类在像Stack Exchange这样的网站上生成内容的需要和益处减少，因此可用的训练数据多样性降低。
- en: Changes in behavior like this are a far more complex problem to address. Stack
    Exchange and the community of users who ask or answer questions on their website
    have autonomy and rights that should be respected. Their current policy is to
    ban the use of ChatGPT and similar tools to answer questions. However, we must
    consider whether there is a middle ground where careful applications of generated
    content can be coupled with human creation and curation to produce a virtuous
    cycle and novel results that benefit both humans and future LLMs. That may allow
    continued growth of the platform in a healthier way, but only if the owners and
    users are amenable.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为的变化是一个更复杂的问题。Stack Exchange及其网站上提问或回答问题的用户社区拥有自主权和应受尊重的权利。他们当前的政策是禁止使用ChatGPT和类似工具来回答问题。然而，我们必须考虑是否存在一个中间地带，其中生成的内容的谨慎应用可以与人类创造和编辑相结合，产生有益于人类和未来LLMs的良性循环和新的成果。这可能允许平台以更健康的方式持续增长，但前提是所有者和使用者都愿意接受。
- en: Ultimately, our use of LLMs will have unintended consequences and unimaginable
    complexities. As a user, you must decide whether you are willing to accept the
    risk of these situations and how our use of these tools will alter the trajectory
    of future iterations.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们对大型语言模型（LLMs）的使用可能会产生意想不到的后果和难以想象的复杂性。作为用户，您必须决定是否愿意接受这些情况的风险，以及我们使用这些工具将如何改变未来迭代的轨迹。
- en: 9.5 Other explorations in LLM ethics
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 LLM伦理的其他探索
- en: The conversation around the ethical implications of building and using LLMs
    is constantly evolving. Although much has been written on the subject, just as
    much remains to be explored on the ethics of LLMs and AI in general. Here, we
    have focused on the essential topics for building a foundational understanding.
    Other key concerns, such as privacy, security, and the potential for misuse, are
    covered further in books by Manning, such as *Introduction to Generative AI* by
    Numa Dhamani and Maggie Engler [24].
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关于构建和使用LLMs的伦理影响的对话不断演变。尽管已经有很多关于这个主题的写作，但关于LLMs和AI的一般伦理仍有很多需要探索。在这里，我们专注于建立基础理解的关键主题。其他关键问题，如隐私、安全和潜在的滥用，在Manning的书籍中得到了进一步的探讨，例如Numa
    Dhamani和Maggie Engler的《生成式人工智能导论》[24]。
- en: LLMs and generative AI will profoundly affect the world; with any new technology,
    it is essential to understand the foundations that guide its behavior and the
    implications of its use. Throughout this book, we have covered the fundamental
    components that make LLMs work, explored common misconceptions, and identified
    the ethical considerations for their construction and use. We hope to have established
    a strong foundation for you to continue your exploration of the field. Thank you
    for starting this journey with us.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs和生成式人工智能将对世界产生深远的影响；对于任何新技术，了解其行为的基础及其使用的含义至关重要。在这本书中，我们涵盖了使LLMs工作的基本组件，探讨了常见的误解，并确定了其构建和使用的伦理考量。我们希望为您建立了一个坚实的基础，以便您继续探索这个领域。感谢您与我们一同踏上这段旅程。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs’ ability to be used for everything via one model helps people use them
    quickly and effectively for many tasks. This broad applicability to many tasks
    also makes it impossible to test the safety of all ways people may use LLMs.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM通过一个模型就能用于各种任务的能力，帮助人们快速有效地使用它们来完成许多任务。这种广泛适用于许多任务的应用性也使得测试人们可能使用LLM的所有方式的安全性变得不可能。
- en: Historically, automation has been a good thing. Still, LLMs pose a unique risk
    to automating knowledge work, which differs from automating manual labor, the
    historical driver of improved living standards. The true effect of broadly automating
    knowledge work is unknown.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从历史上看，自动化是一件好事。然而，LLM对自动化知识工作构成了独特的风险，这与自动化体力劳动不同，后者是提高生活水平的传统驱动力。广泛自动化知识工作的真正影响尚不清楚。
- en: Some fear that an LLM that is good enough to improve on a new LLM’s design will
    cascade to superintelligent algorithms that do not need humanity.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些人士担心，一个足够好的LLM（大型语言模型）能够改进新的LLM的设计，这可能导致不需要人类的超级智能算法。
- en: Aligning any algorithm to what we meant, instead of what we asked, is a major
    challenge that likely has no reduction in risk even if solved.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将任何算法与我们的意图对齐，而不是我们提出的要求，是一个重大的挑战，即使解决了，也可能不会降低风险。
- en: Ethically obtaining data is fraught with legal concerns due to technology moving
    faster than the law.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于技术发展速度快于法律，从道德上获取数据充满了法律问题。
- en: The financial and technical logistics in compensating all content authors for
    their content’s use in the training data is unlikely to be practical, imposing
    ethical questions about the fairness of using their data.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对训练数据中使用所有内容创作者的内容进行补偿的财务和技术物流方面，不太可能实际可行，这引发了关于使用他们数据的公平性的伦理问题。
- en: Public domain data with no copyright is too old to be problematic and poses
    different challenges related to identifying its legal status.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公共领域的数据没有版权，太老了，不会造成问题，但它在确定其法律地位方面提出了不同的挑战。
- en: The proliferation of LLM-generated data can potentially affect the LLMs we build
    in the future. We must consider the potential for feedback loops and the possibility
    of mode collapse.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM生成数据的激增可能会影响我们未来构建的LLM。我们必须考虑反馈循环的可能性以及模式崩溃的可能性。
