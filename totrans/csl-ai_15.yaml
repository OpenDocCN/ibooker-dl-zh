- en: 12 Causal decisions and reinforcement learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 因果决策与强化学习
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using causal models to automate decisions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用因果模型来自动化决策
- en: Setting up causal bandit algorithms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置因果伯努利算法
- en: How to incorporate causality into reinforcement learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将因果关系融入强化学习
- en: 'When we apply methods from statistics and machine learning, it is typically
    in service of making a decision or automating decision-making. Algorithms for
    automated decision-making, such as *bandit* and *reinforcement learning* (RL)
    algorithms, involve agents that *learn* how to make good decisions. In both cases,
    decision-making is fundamentally a causal problem: a decision to take some course
    of action leads to consequences, and the objective is to choose the action that
    leads to consequences favorable to the decision-maker. That motivates a causal
    framing.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们应用来自统计学和机器学习的方法时，通常是为了做出决策或自动化决策。自动化决策算法，如 *伯努利* 和 *强化学习* (RL) 算法，涉及学习如何做出良好决策的代理。在这两种情况下，决策本质上是一个因果问题：采取某些行动的决策会导致后果，目标是选择对决策者有利的后果。这促使我们采用因果框架。
- en: Often, the path from action to consequences has a degree of randomness. For
    example, your choice of how to play a hand of poker may be optimal, but you still
    might lose due to chance. That motivates a probabilistic modeling approach.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，从行动到后果的路径具有一定的随机性。例如，你选择如何玩一副扑克牌可能是最优的，但你仍然可能因为运气而输掉。这促使我们采用概率建模方法。
- en: The causal probabilistic modeling approach we’ve used so far in this book is
    a stone that hits both these birds. This chapter will provide a *causality-first*
    introduction to basic ideas in statistical decision theory, sequential decision-making,
    bandits, and RL. By “causality-first,” I mean I’ll use the foundation we’ve built
    in previous chapters to introduce these ideas in a causal light. I’ll also present
    the ideas in a way that is compatible with our probabilistic ML framing. Even
    if you are already familiar with these decision-making and RL concepts, I encourage
    you to read on and see them again through a causal lens. Once we do that, we’ll
    see cases where the causal approach to RL gets a better result than the noncausal
    approach.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中迄今为止使用的因果概率建模方法是一块同时击中这两只鸟的石头。本章将提供一个 *因果优先* 的介绍，介绍统计决策理论、顺序决策、伯努利和RL的基本思想。这里的“因果优先”意味着我将使用我们在前几章中建立的基础，以因果的角度介绍这些思想。我还会以与我们的概率机器学习框架兼容的方式呈现这些思想。即使你已经熟悉这些决策和RL概念，我也鼓励你继续阅读，并通过因果的视角再次审视它们。一旦我们这样做，我们就会看到因果方法在RL中比非因果方法得到更好的结果的情况。
- en: 12.1 A causal primer on decision theory
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 决策理论的因果入门
- en: Decision theory is concerned with the reasoning underlying an agent’s choice
    of some course of action. An “agent” here is an entity that chooses an action.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 决策理论关注的是代理选择某些行动路线背后的推理。这里的“代理”是一个选择行动的实体。
- en: For example, suppose you were deciding whether to invest in a company by purchasing
    equity or purchasing debt (i.e., loaning money to the company and receiving interest
    payments). We’ll call this variable *X*. Whether the company is successful (*Y*)
    depends on the type of investment it receives.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你正在决定是否通过购买股权或购买债务（即向公司贷款并收取利息）来投资一家公司。我们将这个变量称为 *X*。公司是否成功 (*Y*) 取决于它所获得的投资类型。
- en: '![figure](../Images/CH12_F01_Ness.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F01_Ness.png)'
- en: Figure 12.1 A simple causal DAG where action *X* causes some outcome *Y*. Decision
    theory is a causal problem because if deciding on an action didn’t have causal
    consequences, what would be the point of making decisions?
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.1 一个简单的因果DAG，其中行动 *X* 导致某些结果 *Y*。决策理论是一个因果问题，因为如果决定采取的行动没有因果后果，那么做出决策的意义何在？
- en: Since *X* causally drives *Y*, we can immediately introduce a causal DAG, as
    in figure 12.1.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *X* 因果地驱动 *Y*，我们可以立即引入一个因果DAG，如图12.1所示。
- en: We’ll use this example to illustrate basic concepts in decision theory from
    a causal point of view.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个例子来从因果的角度说明决策理论的基本概念。
- en: 12.1.1 Utility, reward, loss, and cost
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 效用、奖励、损失和成本
- en: 'The agent generally chooses actions that will cause them to gain some utility
    (or minimize some loss). In decision modeling, you can define a utility function
    (aka a reward function) that quantifies the desirability of various outcomes of
    a decision. Suppose you invest at $1,000:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 代理通常选择那些能让他们获得一些效用（或最小化一些损失）的行动。在决策建模中，你可以定义一个效用函数（也称为奖励函数），它量化了决策各种结果的吸引力。假设你投资了$1,000：
- en: If the company becomes successful, you get $100,000\. Your utility is 100,000
    – 1,000 = $99,000\.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果公司变得成功，你将得到$100,000。你的效用是 100,000 – 1,000 = $99,000。
- en: If the company fails, you get $0 and lose your investment. Your utility is –1,000\.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果公司失败，你将得到$0并损失你的投资。你的效用是 –1,000。
- en: We can add this utility as a node on the graph, as in figure 12.2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在图上添加这个效用节点，如图12.2所示。
- en: '![figure](../Images/CH12_F02_Ness.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F02_Ness.png)'
- en: Figure 12.2 A utility node can represent utility/reward, loss/cost.
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.2 效用节点可以代表效用/奖励，损失/成本。
- en: Note that utility is a deterministic function of *Y* in this model, which we’ll
    denote *U*(*Y*).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个模型中，效用是 *Y* 的确定性函数，我们将用 *U*(*Y*) 表示。
- en: '![figure](../Images/ness-ch12-eqs-0x.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-0x.png)'
- en: Instead of a utility/reward function, we could define a loss function (aka,
    a cost function), which is simply –1 times the utility/reward function. For example,
    in the second scenario, where you purchase stock and the company fails, your utility
    is –$1,000 and your loss is $1,000.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是效用/奖励函数，我们可以定义一个损失函数（也称为成本函数），它只是效用/奖励函数的-1倍。例如，在第二个情景中，你购买股票而公司失败的情况下，你的效用是
    –$1,000，你的损失是$1,000。
- en: While the agent’s goal is to decide on a course of action that will maximize
    utility, doing so is challenging because there is typically some uncertainty in
    whether an action will lead to the desired result. In our example, it may seem
    obvious to invest in equity because equity will lead to business success, and
    business success will definitely lead to more utility. But there is some uncertainty
    in whether an equity investment will lead to business success. In other words
    we don’t assume *P*(*Y*=success|*X*=equity) = 1\. Both success and failure have
    nonzero probability in *P*(*Y*|*X*=equity).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然代理的目标是决定一条将最大化效用的行动路线，但这样做是有挑战性的，因为通常在某个行动是否会导致期望的结果方面存在一些不确定性。在我们的例子中，投资股票可能看起来很明显，因为股票将导致商业成功，而商业成功无疑将导致更多的效用。但是，股票投资是否会导致商业成功存在一些不确定性。换句话说，我们不假设
    *P*(*Y*=success|*X*=equity) = 1。在 *P*(*Y*|*X*=equity) 中，成功和失败都有非零的概率。
- en: 12.1.2 Uncertainty comes from other causes
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 不确定性来自其他原因
- en: In causal terms, given action *X*, there is still some uncertainty in the outcome
    *Y* because there are other causal factors driving that outcome. For example,
    suppose the success of the business depends on economic conditions, as in figure
    12.3.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在因果术语中，给定行动 *X*，结果 *Y* 仍然存在一些不确定性，因为还有其他因果因素在推动这个结果。例如，假设商业的成功取决于经济条件，如图12.3所示。
- en: '![figure](../Images/CH12_F03_Ness.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F03_Ness.png)'
- en: Figure 12.3 We typically have uncertainty in our decision-making. From a causal
    perspective, uncertainty is because of other causal factors out of our control
    that affect variables downstream of our actions.
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.3 我们在决策中通常存在不确定性。从因果的角度来看，不确定性是由于我们无法控制的其他因果因素影响我们行动下游的变量。
- en: Alternatively, those other causal factors could affect utility directly. For
    example, rather than the two discrete scenarios of profit or loss I outlined for
    our business investment, the amount of utility (or loss) could depend on how well
    or how poorly the economy fares, as in figure 12.4\. We can leverage statistical
    and probability modeling to address this uncertainty.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，那些其他因果因素可能直接影响效用。例如，而不是我为我们商业投资概述的两个离散情景——盈利或亏损，效用（或损失）可能取决于经济表现的好坏，如图12.4所示。我们可以利用统计和概率建模来应对这种不确定性。
- en: '![figure](../Images/CH12_F04_Ness.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F04_Ness.png)'
- en: Figure 12.4 Causal factors outside of our control can impact utility (or loss)
    directly.
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.4中，我们无法控制的因素可能直接影响效用（或损失）。
- en: Suppose you are thinking about whether to invest in this business. You want
    your decision to be data-driven, so you research what other investors in this
    market have done before. You consider the causal DAG in figure 12.5.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在考虑是否要投资这个业务。你希望你的决策是基于数据的，因此你研究了这个市场中其他投资者之前都做了什么。你考虑了图12.5中的因果DAG。
- en: '![figure](../Images/CH12_F05_Ness.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F05_Ness.png)'
- en: Figure 12.5 In this DAG, economic conditions drive how investors choose to invest.
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.5 在这个DAG中，经济条件驱动投资者选择如何投资。
- en: Based on your research, you conclude that past investors’ equity vs. debt choice
    also depends on the economic conditions. *P*(*X*|*C*) represents an action distribution—the
    distribution of actions that the population of investors you are studying take.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的研究，你得出结论，过去投资者的股票与债务选择也取决于经济条件。*P*(*X*|*C*)代表一个行动分布——你正在研究的投资者群体采取的行动分布。
- en: However, the goal of your analysis centers on yourself, not other investors.
    You want to answer questions like “what if *I* bought equity?” That question puts
    us in causal territory. We are not reasoning about observational investment trends;
    we are reasoning about conditional hypotheticals. That is an indicator that we
    need to introduce intervention-based reasoning and counterfactual notation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你分析的目标是围绕你自己，而不是其他投资者。你想要回答像“如果我买股票会怎样？”这样的问题。这个问题让我们进入了因果领域。我们不是在推理观察到的投资趋势；我们是在推理条件假设。这是一个我们需要引入基于干预的推理和反事实记号的指标。
- en: 12.2 Causal decision theory
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 因果决策理论
- en: In this section, we’ll highlight decision-making as a causal query and examine
    what that means for modeling decision-making.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将强调决策作为因果查询，并探讨这对建模决策意味着什么。
- en: 12.2.1 Decisions as a level 2 query
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 决策作为第二层查询
- en: A major source of confusion for causal decision modeling is the difference between
    actions and interventions. In many decision contexts, especially in RL, the action
    is a thing that the agent *does* that changes their environment. Yet, the action
    is also a variable *driven by* the environment. We see this when we look at the
    investment example, shown again in figure 12.6.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因果决策建模的混淆来源之一是行动和干预之间的区别。在许多决策情境中，特别是在强化学习（RL）中，行动是代理人*做*的事情，它改变了他们的环境。然而，行动也是由环境*驱动*的变量。当我们查看投资例子时，我们会看到这一点，如图12.6所示。
- en: '![figure](../Images/CH12_F06_Ness.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F06_Ness.png)'
- en: Figure 12.6 In this version of the investment DAG, the choice of action is caused
    by external factors.
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.6 在这个投资DAG版本中，行动的选择是由外部因素引起的。
- en: The action of selecting equity or debt is a variable causally driven by the
    economy. What does that mean? Is an action a variable with causes, or is it an
    intervention?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 选择股票或债券的行为是由经济因素因果驱动的变量。这意味着什么？这个行为是一个有原因的变量，还是一个干预措施？
- en: The answer is *both*, depending on context. When it is which depends on the
    question we are asking and where that question sits in the causal hierarchy (discussed
    in chapter 10). When we are talking about what actions usually happen, such as
    when we are observing the actions of other agents (or even when reflecting on
    our own past actions) and what results those actions led to, we are reflecting
    on trends in population, and we are on level 1 of the causal hierarchy. In the
    case of our investment example, we’re reasoning about *P*(*C*, *X*, *Y*, *U*).
    But if we’re asking questions like “what would happen if I made an equity investment?”
    then we’re asking a level 2 question, and we need the proposed action as an intervention.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是**两者都是**，这取决于上下文。当我们谈论通常会发生什么行为，例如当我们观察其他代理人的行为（甚至当我们反思我们自己的过去行为）以及这些行为导致了什么结果时，我们正在反思人口趋势，我们处于因果层次的第一层。在我们的投资例子中，我们正在推理*P*(*C*,
    *X*, *Y*, *U*)。但如果我们问的问题是“如果我进行股票投资会发生什么？”那么我们就是在问一个第二层的问题，我们需要将提议的行为作为干预措施。
- en: Next, we’ll characterize common decision rules using our causal notation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用我们的因果符号来描述常见的决策规则。
- en: 12.2.2 Causal characterization of decision rules and policies
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 决策规则和策略的因果特征
- en: A decision rule is a rule for choosing an action based on the utility distribution
    *P*(*U*(*Y*[*X*][=][*x*])). The agent chooses an optimal action according to a
    decision rule. For example, a common decision rule is choosing the action that
    minimizes loss or cost or maximizes utility or reward.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 决策规则是基于效用分布*P*(*U*(*Y*[*X*][=][*x*]))选择行动的规则。代理人根据决策规则选择最优行动。例如，一个常见的决策规则是选择最小化损失或成本或最大化效用或奖励的行动。
- en: In automated decision-making, the decision rule is often called a “policy.”
    In public health settings, decision rules are sometimes called “treatment regimes.”
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动决策中，决策规则通常被称为“策略”。在公共卫生环境中，决策规则有时被称为“治疗方案”。
- en: Maximizing expected utility
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最大化预期效用
- en: The most intuitive and commonly seen decision rule is to choose the action that
    maximizes expected utility. First, we can look at the expectation of the utility
    distribution. Since utility is a deterministic function of *Y*[*X*][=][*x*], this
    is just the expectation of *U*(*Y*[*X*][=][*x*]) over the intervention distribution
    of *Y*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最直观且最常见的决策规则是选择最大化预期效用的行动。首先，我们可以查看效用分布的期望值。由于效用是 *Y*[*X*][=][*x*] 的确定性函数，这仅仅是
    *U*(*Y*[*X*][=][*x*]) 在 *Y* 的干预分布上的期望。
- en: '![figure](../Images/ness-ch12-eqs-1x.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-1x.png)'
- en: 'We then choose the action (value of *x*) that maximizes expected utility:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后选择最大化预期效用的行动（*x* 的值）：
- en: '![figure](../Images/ness-ch12-eqs-2x.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-2x.png)'
- en: In our investment example, this means choosing the investment approach that
    is expected to make you the most money.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的投资示例中，这意味着选择预期能让你赚最多钱的投资方法。
- en: Minimax decision rules
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最小-最大决策规则
- en: To understand the minimax decision rule, recall that the terms “utility” and
    “loss” are two sides of the same coin; utility == negative loss. Let *L*(*y*)
    = –*U*(*y*). Then a minimax decision rule is
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解最小-最大决策规则，请记住，“效用”和“损失”是同一枚硬币的两面；效用 == 负损失。设 *L*(*y*) = –*U*(*y*)。那么最小-最大决策规则是
- en: '![figure](../Images/ness-ch12-eqs-3x.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-3x.png)'
- en: In plain English, this means “choose the action that minimizes the maximum amount
    of possible loss.” In our investment example, this means choosing the investment
    approach that will minimize the amount of money you’d lose in the worst case scenario.
    There are many variants of minimax rules, but they have the same flavor—minimizing
    loss or maximizing utility during bad times.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的话说，这意味着“选择一个行动，以最小化可能的最大损失。”在我们的投资示例中，这意味着选择一个投资方法，以最小化在最坏情况下的损失。存在许多最小-最大规则变体，但它们有相同的味道——在困难时期最小化损失或最大化效用。
- en: Softmax rules
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Softmax 规则
- en: A softmax decision rule randomly selects an action with a probability proportional
    to the resulting utility.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 决策规则随机选择一个行动，其概率与结果的效用成比例。
- en: Let’s define *C*(*x*) as the probability of choosing the action *x*. Then *C*(*x*)
    is defined as a probability value proportional to
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义 *C*(*x*) 为选择行动 *x* 的概率。那么 *C*(*x*) 被定义为与以下成比例的概率值
- en: '![figure](../Images/ness-ch12-eqs-4x.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-4x.png)'
- en: The noise parameter *α* modulates between the two extremes. When *α*=0, we have
    a uniform distribution on all the choices. As *α* gets larger, we approach maximizing
    expected utility.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声参数 *α* 在两种极端之间调节。当 *α*=0 时，我们在所有选择上都有均匀分布。随着 *α* 的增大，我们接近最大化预期效用。
- en: Sometimes our goal is to model the decision-making of other agents, such as
    in inverse RL. The softmax decision rule is useful when agents don’t always make
    the utility-optimizing choice. The softmax decision rule provides a simple, analytically
    tractable, and empirically validated model of suboptimal choice.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们的目标是模拟其他智能体的决策，例如在逆强化学习（inverse RL）中。当智能体不总是做出效用优化的选择时，softmax 决策规则很有用。softmax
    决策规则提供了一个简单、可分析的、经验验证的次优选择模型。
- en: Another reason we might want to use the softmax rule is when there is a trade-off
    between *exploring* and *exploiting,* such as with bandit problems. Suppose the
    agent is uncertain about the shape of the distibution *P*(*Y*[*X*][=][*x*]). The
    optimal action according to an incorrect model of *P*(*Y*[*X*][=][*x*]) might
    be different from the optimal choice according to the correct model of *P*(*Y*[*X*][=][*x*]).
    The softmax decision rule allows us to choose various actions, get some data on
    the results, and use that data to update our model of *P*(*Y*[*X*][=][*x*]). When
    this is done in sequence, it’s often called *Thompson sampling*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们可能想要使用 softmax 规则的原因是在 *探索* 和 *利用* 之间存在权衡，例如在老虎机问题中。假设智能体对分布 *P*(*Y*[*X*][=][*x*])
    的形状不确定。根据 *P*(*Y*[*X*][=][*x*]) 的错误模型所采取的最优行动可能与根据正确模型 *P*(*Y*[*X*][=][*x*]) 所做的最优选择不同。softmax
    决策规则允许我们选择各种行动，获取一些关于结果的数据，并使用这些数据来更新我们对 *P*(*Y*[*X*][=][*x*]) 的模型。当这些操作按顺序进行时，通常被称为
    *汤普森抽样*。
- en: In our investment analogy, suppose we were to invest in several businesses.
    Perhaps, according to our current model, equity investment maximizes expected
    utility, but we’re not fully confident in our current model, so we opt to select
    debt investment even though the current model says its less optimal. The goal
    is to add diversity to our dataset, so that we can learn a better model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的投资类比中，假设我们要投资几个企业。也许，根据我们的当前模型，股权投资最大化预期效用，但我们对我们当前模型并不完全自信，所以我们选择选择债务投资，尽管当前模型表示它不太优。目标是增加我们的数据集的多样性，这样我们就可以学习更好的模型。
- en: Other types of decision rules
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他类型的决策规则
- en: There are other types of decision rules, and they can become complicated, especially
    when they involve statistical estimation. For example, using *p*-values in statistical
    hypothesis testing involves a nuanced utility function that balances the chances
    of a false positive (incorrectly choosing the alternative hypothesis) and a false
    negative (incorrectly choosing the null hypothesis).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他类型的决策规则，并且它们可能会变得复杂，尤其是在涉及统计估计时。例如，在统计假设检验中使用*p*值涉及一个微妙的效用函数，它平衡了假阳性（错误地选择备择假设）和假阴性（错误地选择零假设）的机会。
- en: Fortunately, when we work with probabilistic causal models, the math tends to
    be easier, and we get a nice guarantee called *admissibility*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，当我们与概率因果模型一起工作时，数学通常更容易，我们得到一个称为*可接受性*的保证。
- en: 12.2.3 Causal probabilistic decision-modeling and admissibility
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.3 因果概率决策建模和可接受性
- en: In this section, I’ll provide a short justification for choosing a causal probabilistic
    modeling approach to decision-making. When you implement an automated decision-making
    algorithm in a production setting, you might have to explain why your implementation
    is better than another. In that setting, it is useful if you know if your algorithm
    is *admissible*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将简要说明选择因果概率建模方法进行决策的理由。当你在一个生产环境中实施自动化决策算法时，你可能需要解释为什么你的实现比另一个更好。在这种情况下，如果你知道你的算法是*可接受的*，那将是有用的。
- en: A decision rule is *admissible* if there are no other rules that dominate it.
    A decision rule dominates another rule if the performance of the former is sometimes
    better, and never worse, than that of the other rule with respect to the utility
    function. For example, the softmax decision rule is dominated by maximizing expected
    utility (assuming you know the true shape of *P*(*Y**[X]*[=]*[x]*)) because sometimes
    it will select suboptimal actions, and it is thus inadmissible. Determining *admissibility*
    is a key task in decision theory.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有其他规则支配它，则决策规则是*可接受的*。一个决策规则支配另一个规则，如果前者的性能在某些情况下比后者好，并且永远不会比后者差，相对于效用函数而言。例如，softmax决策规则被最大化预期效用所支配（假设你知道*P*(*Y**[X]*[=]*[x]*)的真实形状），因为它有时会选择次优行动，因此是不可接受的。确定*可接受性*是决策理论中的一个关键任务。
- en: The challenge for us occurs when we use data and statistics to deal with unknowns,
    such as parameters or latent variables. If we want to use data to estimate a parameter
    or work with latent variables, there are usually a variety of statistical approaches
    to choose from. If our decision-making algorithm depends on a statistical procedure,
    the choice of procedure can influence which action is considered optimal. How
    do we know if our statistical decision-making procedure is admissible?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，挑战出现在我们使用数据和统计来处理未知情况时，例如参数或潜在变量。如果我们想使用数据来估计一个参数或处理潜在变量，通常有多种统计方法可供选择。如果我们的决策算法依赖于一个统计过程，过程的选择可能会影响被认为最优的行动。我们如何知道我们的统计决策过程是可接受的？
- en: Probabilistic modeling libraries like Pyro leverage Bayesian inference to estimate
    parameters or impute latent variables. Bayesian decision theory tells us that
    *Bayes rules*, (not to be confused with Bayes’s rule) decision rules that optimize
    posterior expected utility, have an admissibility guarantee under mild regularity
    conditions. This means that if we use Bayesian inference in Pyro or similar libraries
    to calculate and optimize posterior expected loss, we have an admissibility guarantee
    (if those mild conditions hold, and they usually do). That means you needn’t worry
    that someone else’s decision-making model (that makes the same modeling assumptions,
    has the same utility function, and uses the same data) will beat yours.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.4 The deceptive alignment of argmax values of causal and non-causal expectations
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most conventional approaches to decision-making, including in RL, focus on maximizing
    *E*(*U*(*Y*)|*X*=*x*) rather than *E*(*U*(*Y*[*X*][=][*x*])). Let’s implement
    the model in figure 12.6 with pgmpy and compare the two approaches.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll build the DAG in the model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This code was written with pgmpy version 0.1.24\. See the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for a link to the notebook that runs this code.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1 DAG for investment decision model
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Set up the DAG'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we’ll build the causal Markov kernels for *Economy* (*C*), *Debt vs. Equity*
    (*X*), and *Business Success* (*Y*). The causal Markov kernel for *Economy* (*C*)
    will take two values: “bear” for bad economic conditions and “bull” for good.
    The causal Markov kernel for *Debt vs. Equity* (*X*) will depend on *C*, reflecting
    the fact that investors tend to prefer equity in a bull economy and debt in a
    bear economy. *Success* (*Y*) depends on the economy and the choice of debt or
    equity investment.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 Create causal Markov kernels for *C*, *X*, and *Y*
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Set up causal Markov kernel for C (economy). It takes two values: “bull”
    and “bear”.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Set up causal Markov kernel for action X, either making a debt investment
    or equity investment depending on the economy.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Set up causal Markov kernel for business outcome Y, either success or failure,
    depending on the type of investment provided (X) and the economy (C).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll add the *Utility* node (*U*). We use probabilities of 1 and 0
    to represent a deterministic function of *Y*. We end by adding all the kernels
    to the model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.3 Implement the utility node and initialize the model
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Set up the utility node.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Set up the utility node.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'This code prints out the following conditional probability tables for our causal
    Markov kernels. This one is for the *Utility* variable:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This reflects the investor trends of favoring equity investments in a bull market
    and debt investments in a bear market.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'The following probability table is for the *Business Success* variable *Y*:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This reflects debt being a less preferred source of financing in a bear market
    when interest rate payments are higher, and equity being preferred in a bull market
    because equity is cheaper.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这反映了在熊市中，由于利率支付较高，债务融资不如债券融资受欢迎，而在牛市中，由于债券更便宜，债券融资更受欢迎。
- en: 'Finally, the *Utility* node is a simple deterministic function that maps *Y*
    to utility values:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*效用*节点是一个简单的确定性函数，将 *Y* 映射到效用值：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Next, we’ll calculate* *E*(*U*(*Y*[*X*][=][*x*])) and *E*(*U*(*Y*)|*X*=*x*).
    Before proceeding, download and load a helper function that implements an ideal
    intervention. To allay any security concerns of directly executing downloaded
    code, the code prints the downloaded script and prompts you to confirm before
    executing the script.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*接下来，我们将计算* *E*(*U*(*Y*[*X*][=][*x*])) 和 *E*(*U*(*Y*)|*X*=*x*). 在继续之前，下载并加载一个实现理想干预的辅助函数。为了缓解直接执行下载代码可能带来的安全担忧，代码会打印下载的脚本并提示您在执行脚本之前确认。*'
- en: Listing 12.4 Download helper function for implementing an ideal intervention
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.4 下载实现理想干预的辅助函数
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Load an implementation of an ideal intervention.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载理想干预的实现。'
- en: '#2 To allay security concerns, you can inspect the downloaded script and confirm
    it before running.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为了缓解安全担忧，您可以检查下载的脚本并在运行之前确认。'
- en: By now, in this book, you should not be surprised that *E*(*U*(*Y*[*X*][=][*x*]))
    is different from *E*(*U*(*Y*)|*X*=*x*). Let’s look at these values.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，在这本书中，您不应该对 *E*(*U*(*Y*[*X*][=][*x*])) 与 *E*(*U*(*Y*)|*X*=*x*) 不同感到惊讶。让我们看看这些值。
- en: Listing 12.5 Calculate *E*(*U*(*Y*)|*X*=*x*) and *E*(*U*(*Y**[X]*[=]*[x]*))
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.5 计算 *E*(*U*(*Y*)|*X*=*x*) 和 *E*(*U*(*Y**[X]*[=]*[x]*))
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 A helper function for calculating the expected utility'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用于计算预期效用的辅助函数'
- en: '#2 Set X by intervention to debt and equity and calculate the expectation of
    U under each intervention.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 通过干预将 X 设置为债务和债券，并计算每个干预下的 U 的期望值。'
- en: '#3 Condition on X = debt and X = equity, and calculate the expectation of U.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 在 X = debt 和 X = equity 的条件下，计算 U 的期望值。'
- en: 'This gives us the following conditional expected utilities (I’ve marked the
    highest with *):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下条件预期效用（我已经用 * 标记了最高的）：
- en: '*E*(*U*(*Y*)|*X*=debt) = 57000 *'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*)|*X*=debt) = 57000 *'
- en: '*E*(*U*(*Y*)|*X*=equity) = 37000'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*)|*X*=equity) = 37000'
- en: 'It also gives us the following interventional expected utilities:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 它还给我们以下干预预期效用：
- en: '*E*(*U*(*Y*[*X*][=debt])) = 39000 *'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*X*][=debt])) = 39000 *'
- en: '*E*(*U*(*Y*[*X*][=equity])) = 34000'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*X*][=equity])) = 34000'
- en: So *E*(*U*(*Y*)|*X*=debt) is different from *E*(*U*(*Y*[*X*][=debt])), and *E*(*U*(*Y*)|*X*=equity)
    is different from *E*(*U*(*Y*[*X*][=][equity])). However, our goal is to optimize
    expected utility, and in this case, debt maximizes both *E*(*U*(*Y*)|*X*=x) and
    *E*(*U*(*Y*[*X*][=][*x*])).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*E*(*U*(*Y*)|*X*=debt) 与 *E*(*U*(*Y*[*X*][=debt])) 不同，*E*(*U*(*Y*)|*X*=equity)
    与 *E*(*U*(*Y*[*X*][=][equity])) 不同。然而，我们的目标是优化预期效用，在这种情况下，债务最大化了 *E*(*U*(*Y*)|*X*=x)
    和 *E*(*U*(*Y*[*X*][=][*x*]))。
- en: '![figure](../Images/ness-ch12-eqs-5x.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-5x.png)'
- en: If “debt” maximizes both queries, what is the point of causal decision theory?
    What does it matter if *E*(*U*(*Y*)|*X*=*x*) and *E*(*U*(*Y*[*X*][=][*x*])) are
    different if the optimal action for both is the same?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“债务”最大化了两个查询，因果决策理论有什么用？如果最优行动对于两者都是相同的，*E*(*U*(*Y*)|*X*=*x*) 和 *E*(*U*(*Y*[*X*][=][*x*]))
    是否不同又有什么关系？
- en: In decision problems, it is quite common that a causal formulation of the problem
    provides the same answer as more traditional noncausal formulations. This is especially
    true in higher dimensional problems common in RL. You might observe this and wonder
    why the causal formulation is needed at all.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策问题中，通常情况下，问题的因果表述与更传统的非因果表述提供相同的答案。这在 RL 中常见的更高维问题中尤其如此。您可能会观察到这一点，并想知道为什么因果表述是必需的。
- en: To answer, watch what happens when we make a slight change to the parameters
    of *Y* in the model. Specifically, we’ll change the parameter for *P*(*Y*=success|*X*=equity,
    *C*=bull) from .4 to .6\. First, we’ll rebuild the model with the parameter change.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，观察当我们对模型中 *Y* 的参数进行轻微修改时会发生什么。具体来说，我们将把 *P*(*Y*=success|*X*=equity,
    *C*=bull) 的参数从 .4 改为 .6。首先，我们将使用参数更改重建模型。
- en: Listing 12.6 Change a parameter in the causal Markov kernel for *Y*
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.6 修改 *Y* 的因果马尔可夫核中的参数
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Initialize a new model.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化一个新的模型。'
- en: '#2 Create a new conditional probability distribution for Y.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为 Y 创建一个新的条件概率分布。'
- en: '#3 Change the parameter P(Y=success|X=equity, C=bull) = 0.4 (the last parameter
    in the first list) to 0.6.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将第一个列表中的最后一个参数 P(Y=success|X=equity, C=bull) = 0.4 改为 0.6。'
- en: '#4 Add the causal Markov kernels to the model.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将因果马尔可夫核添加到模型中。'
- en: Next, we rerun inference.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们重新运行推理。
- en: Listing 12.7 Compare outcomes with changed parameters
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.7 比较改变参数后的结果
- en: '[PRE9]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Set X by intervention to debt and equity, and calculate the expectation
    of U under each intervention.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 通过干预将X设置为debt和equity，并计算每种干预下的U的期望。'
- en: '#2 Condition on X = debt and X = equity, and calculate the expectation of U.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在X = debt和X = equity的条件下进行条件化，并计算U的期望。'
- en: 'This gives us the following conditional expectations (* indicates the optimal
    choice):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下条件期望（*表示最优选择）：
- en: '*E*(*U*(*Y*)|*X*=debt) = 57000 *'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*)|*X*=debt) = 57000 *'
- en: '*E*(*U*(*Y*)|*X*=equity) = 53000'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*)|*X*=equity) = 53000'
- en: 'It also gives us the following interventional expectations:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 它还给我们以下干预期望：
- en: '*E*(*U*(*Y*[*X*][=debt])) = 39000'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*X*][=debt])) = 39000'
- en: '*E*(*U*(*Y*[*X*][=equity])) = 44000 *'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*X*][=equity])) = 44000 *'
- en: With that slight change in a single parameter, “debt” is still the optimal value
    of *x* in *E*(*U*(*Y*)|*X*=*x*), but now “equity” is the optimal value of *x*
    in *E*(*U*(*Y*[*X*][=][*x*])). This is a case where the causal answer and the
    answer from conditioning on evidence are different. Since we are trying to answer
    a level 2 query, the causal approach is the right approach.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个参数的微小变化下，“债务”仍然是 *E*(*U*(*Y*)|*X*=*x*) 中 *x* 的最优值，但现在“股权”是 *E*(*U*(*Y*[*X*][=][*x*]))
    中 *x* 的最优值。这是一个因果答案与基于证据的条件答案不同的情况。由于我们正在尝试回答一个第二级查询，因果方法是正确的方法。
- en: This means that while simply optimizing a conditional expectation often gets
    you the right answer, you are vulnerable to getting the wrong answer in certain
    circumstances. Compare this to our discussion of semi-supervised learning in chapter
    4—often the unlabeled data can help with learning, but, in specific circumstances,
    the unlabeled data adds no value. Causal analysis helped us characterize those
    circumstances in precise terms. Similarly, in this case, there are specific scenarios
    where the causal formulation of the problem will lead to a different and more
    correct result relative to the traditional noncausal formulation. Even the most
    popular decision-optimization algorithms, including the deep learning-based approaches
    used in deep RL, can improve performance by leveraging the causal structure of
    a decision problem.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着虽然简单地优化条件期望通常可以得到正确答案，但在某些情况下，你可能会得到错误答案。这与我们在第4章中关于半监督学习的讨论类似——通常未标记数据可以帮助学习，但在特定情况下，未标记数据不会增加任何价值。因果分析帮助我们以精确的术语描述了这些情况。同样，在这种情况下，有特定的场景，因果问题的因果表述相对于传统的非因果表述将导致不同的、更正确的结果。即使是包括在深度强化学习中使用的基于深度学习的最流行的决策优化算法，也可以通过利用决策问题的因果结构来提高性能。
- en: Next, we’ll see another example with Newcomb’s paradox.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到另一个Newcomb悖论的例子。
- en: 12.2.5 Newcomb’s paradox
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.5 Newcomb悖论
- en: A famous thought experiment called Newcomb’s paradox contrasts the causal approach
    to decision theory, maximizing utility under intervention, with the conventional
    approach of maximizing utility conditional on some action. We’ll look at an AI-inspired
    version of this thought experiment in this section, and the next section will
    show how to approach it with a formal causal model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的思想实验，称为Newcomb悖论，对比了决策理论中的因果方法，即在干预下最大化效用，与传统的在某种行动条件下最大化效用的方法。在本节中，我们将探讨这个思想实验的AI版本，下一节将展示如何使用形式化的因果模型来处理它。
- en: There are two boxes designated A and B as shown in figure 12.7\. Box A always
    contains $1,000\. Box B contains either $1,000,000 or $0\. The decision-making
    agent must choose between taking only box B or *both* boxes. The agent does not
    know what is in box B until they decide. Given this information, it is obvious
    the agent should take both boxes—choosing both yields either $1,000 or $1,001,000,
    while choosing only B yields either $0 or $1,000,000.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如图12.7所示，有两个标记为A和B的盒子。盒子A总是包含$1,000。盒子B包含$1,000,000或$0。决策代理必须选择只拿盒子B或*两个*盒子。代理在做出决定之前不知道盒子B里有什么。根据这个信息，很明显代理应该拿两个盒子——选择两个盒子可以得到$1,000或$1,001,000，而只选择B可以得到$0或$1,000,000。
- en: '![figure](../Images/CH12_F07_Ness.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F07_Ness.png)'
- en: Figure 12.7 An illustration of the boxes in Newcomb's paradox
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.7 Newcomb悖论中盒子的示意图
- en: Now, suppose there is an AI that can predict with high accuracy what choice
    the agent intends to make. If the AI predicts that the agent intends to take both
    boxes, it will put no money in box B. If the AI is correct and the agent takes
    both boxes, the agent only gets $1,000\. However, if the AI predicts that the
    agent intends to take only box B, it will put $1,000,000 in box B. If the AI predicts
    correctly, the agent gets the $1,000,000 in box B but not the $1,000 in box A.
    The agent does not know for sure what the AI predicted or what box B contains
    until they make their choice.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: The traditional paradox arises as follows. A causality-minded agent reasons
    that the actions of the AI are out of their control. They only focus on what they
    can control—the causal consequences of their choice. They can’t *cause* the content
    of box B, so they pick both boxes on the off-chance box B has the million, just
    as one would if the AI didn’t exist. But if the agent knows how the AI works,
    doesn’t it make more sense to choose only box B and get the million with certainty?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dig in further by enumerating the possible outcomes and their probabilities.
    Let’s assume the AI’s predictions are 95% accurate. If the agent chooses both
    boxes, there is a 95% chance the AI will have guessed the agent’s choice and put
    no money in B, in which case the agent only gets the $1,000\. There is a 5% chance
    the algorithm will guess wrong, in which case it puts 1,000,000 in box B, and
    the agent wins $1,001,000\. If the agent chooses only box B, there is a 95% chance
    the AI will have predicted the choice and placed $1,000,000 in box B, giving the
    agent $1,000,000 in winnings. There is a 5% chance it will not, and the agent
    will take home nothing. We see these outcomes in table 12.1\. The expected utility
    calculations are shown in table 12.2.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.1 Newcomb’s problem outcomes and their probabilities
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Strategy | AI action | Winnings | Probability |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| Choose both  | Put $0 in box B  | $1,000  | .95  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| Choose both  | Put $1,000,000 in box B  | $1,001,000  | .05  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| Choose only box B  | Put $1,000,000 in box B  | $1,000,000  | .95  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| Choose only box B  | Put $0 in box B  | $0  | .05  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: Table 12.2 Expected utility of each choice in Newcomb’s problem
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Strategy ( *x*) | *E*( *U*&#124; *X*= *x*) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| Choose both  | 1,000 × .95 + 1,001,000 × .05 = $51,000  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| Choose only box B  | 1,000,000 × .05 + 0 × .05 = $950,000  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: The conventional approach suggests choosing box only box B.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: When the paradox was created, taking a causal approach to the problem meant
    only attending to the causal consequences of one’s actions. Remember that the
    AI makes the prediction *before* the agent acts. Since effects cannot precede
    causes in time, the AI’s behavior is not a consequence of the agent’s actions,
    so the agent with the causal view ignores the AI and goes with the original strategy
    of choosing both boxes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: It would seem that the agent with the causal view is making an error in failing
    to account for the actions of the AI. But we can resolve this error by having
    the agent use a formal causal model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.6 Newcomb’s paradox with a causal model
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the traditional formulation of Newcomb’s paradox, the assumption is that
    the agent using causal decision theory only attends to the consequences of their
    actions—they are reasoning on the causal DAG in figure 12.8\. But the true data
    generating process (DGP) is better captured by figure 12.9.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F08_Ness.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 Newcomb’s paradox assumes a version of causal decision theory where
    a naive agent uses this incorrect causal DAG.
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F09_Ness.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Figure 12.9 A better causal DAG representing the framing of Newcomb’s paradox
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The choice of the agent can’t *cause* the AI’s prediction, because the prediction
    happens first. Thus, we assume the AI agent is inferring the agent’s *intent*,
    and thus the intent of the agent is the cause of the AI’s prediction.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The causal decision-making agent would prefer the graph in figure 12.9 because
    it is a better representation of the DGP. The clever agent wouldn’t focus on maximizing
    *E*(*U*[*choice*][=][*x*]). The clever agent is aware of its own intention, and
    knowing that this intention is a cause of the content of box B, it focuses on
    optimizing *E*(*U*[*choice*][=][*x*]|*intent*=*i*), where *i* is their original
    intention of which box to pick.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-6x.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: We’ll assume the agent’s initial intention is an impulse it cannot control.
    But while they can’t control their initial intent, they can do some introspection
    and become aware of this intent. Further, we’ll assume that upon doing so, they
    have the ability to change their choice to something different from what it initially
    intended, after the AI has made their prediction and set the contents of box B.
    Let’s model this system in pgmpy and evaluate maximizing *E*(*U*[*choice*][=][*x*]|*intent*=*i*).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s build the DAG.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.8 Create the DAG
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next, we’ll create causal Markov kernels for intent and choice.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.9 Create causal Markov kernels for intent and choice
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 We assume a 50-50 chance the agent will prefer both boxes vs. box B.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We assume the agent’s choice is deterministically driven by their intent.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we’ll create the causal Markov kernels for the AI’s decision and
    the content of box B.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.10 Create causal Markov kernels for AI prediction and box B content
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 The AI’s prediction is 95% accurate.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Box B contents are set deterministically by the AI’s prediction.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll create a causal Markov kernel for utility and add all the kernels
    to the model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.11 Create utility kernel and build the model
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Set up the utility node.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Build the model.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll evaluate maximizing *E*(*U*[*choice*][=][*x*]|*intent*=*i*).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.12 Infer optimal choice using intervention and conditioning on intent
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Infer E(U(Y [choice=both]|intent=both)).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Infer E(U(Y [choice=box B]|intent=both)).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Infer E(U(Y [choice=both]|intent=B)).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Infer E(U(Y [choice=box B]|intent=B)).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'This code produces the following results (* indicates the optimal choice for
    a given intent):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*choice*][=both]|*intent*=both)) = 51000 *'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*choice*][=box B]|*intent*=both)) = 50000'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*choice*][=both]|*intent*=B)) = 951000 *'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*choice*][=box B]|*intent*=B)) = 950000'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the agent’s initial intention is to select both, the best choice is to
    select both. When the agent intends to choose only box B, the best choice is to
    ignore those intentions and choose both. Either way, the agent should choose both.
    Note that when the agent initially intends to choose only box B, switching to
    both boxes gives them an expected utility of $951,000 which is greater than the
    optimal choice utility of $950,000 in the noncausal approach.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The agent, unfortunately, cannot control their initial intent; if they could,
    they would deliberately ‘intend’ to pick box B and then switch at the last minute
    to choosing both boxes after the AI placed the million in box B. However, they
    can engage in a form of introspection, factoring their initial intent into their
    decision and, in so doing, accounting for the AI’s behavior rather than ignoring
    it.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.7 Introspection in causal decision theory
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Newcomb’s problem illustrates a key capability of causal decision theory—the
    ability for us to include introspection as part of the DGP.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F10_Ness.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: Figure 12.10 Often our actions are simply reactions to our environment, rather
    than the result of deliberate decision-making.
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To illustrate, consider that often our actions are simply *reactions* to our
    environment, as in figure 12.10.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: For example, you might have purchased a chocolate bar *because* you were hungry
    and it was positioned to tempt you as you waited in the checkout aisle of the
    grocery store. Rather than go through some deliberative decision-making process,
    you had a simple, perhaps even unconscious, *reaction* to your craving and an
    easy way to satisfy it.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: However, humans are capable of introspection—observing and thinking about their
    internal states. A human might consider their normal reactive behavior as part
    of the DGP. This introspection is illustrated in figure 12.11.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F11_Ness.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: Figure 12.11 Humans and some other agents can think about a DGP that includes
    them as a component of that process.
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Through this introspection, the agent can perform level 2 hierarchical reasoning
    about what would happen if they did not react as usual but acted deliberately
    (e.g., sticking to their diet and not buying the chocolate bar), as in figure
    12.12.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F12_Ness.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: Figure 12.12 The agent reasons about a DGP that includes them as a component.
    They then use that reasoning in asking level 2 “what would happen if...” questions
    about that process.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In many cases, the agent may not know the full state of their environment. However,
    if the agent can disentangle their urge to react a certain way from their action,
    they can use that “urge” as evidence in deliberative decision-making, as in figure
    12.13.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F13_Ness.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: Figure 12.13 The agent may not know the states of other variables in the environment,
    but through introspection, they may have an intuition about those variables. That
    intuition can be used as evidence in conditional causal inferences.
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We saw this pattern in the Newcomb example; the agent does not know what the
    AI has predicted, but, through introspection, they can use their initial intention
    to choose both boxes as *evidence* of what the AI has chosen.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Was there ever a time where you noticed you had started to make clumsy errors
    in your work and used that as evidence that you were fatigued, even though you
    didn’t feel so, and you thought, “what if I take a break?” Have you had a gut
    feeling that something was off, despite not knowing what, and based on this feeling
    started to make different decisions? Causal modeling, particularly with causal
    generative models, make it easy to write algorithms that capture this type of
    self-introspection in decision-making.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at causal modeling of sequential decision-making.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Causal DAGs and sequential decisions
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sequential decision processes are processes of back-to-back decision-making.
    These processes can involve sequential decisions made by humans or by algorithms
    and engineered agents.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: When I model decision processes in sequence, I use a subscript to indicate a
    discrete step in the series, such as *Y*[1], *Y*[2], *Y*[3]. When I want to indicate
    an intervention subscript, I’ll place it to the right of the time-step subscript,
    as in *Y*[1,][*X*][=][*x*], *Y*[2,][*X*][=][*x*], *Y*[3,][*X*][=][*x*].
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I’ll show causal DAGs for several canonical sequential decision-making
    processes, but you should view these as templates, not as fixed structures. You
    can add or remove edges in whatever way you deem appropriate for a given problem.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the simplest case, bandit feedback.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.1 Bandit feedback
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Bandit feedback* refers to cases where, at each step in the sequence, there
    is an act *X* that leads to an outcome *Y*, with some utility *U*(*Y*). A bandit
    sequence has two key features. The first is that, at every step, there is instant
    feedback after an act occurs. The second is independent trials, meaning that the
    variables at the *t*^(th) timestep are independent of variables at other timesteps.
    The term “bandit” comes from an analogy to “one-armed bandits,” which is a slang
    term for casino slot machines that traditionally have an arm that the player pulls
    to initiate gameplay. Slot machine gameplay provides bandit feedback—you deposit
    a token, pull the arm, and instantly find out if you win or lose. That outcome
    is independent of previous plays.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: We can capture bandit feedback with the causal DAG in figure 12.14.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F14_Ness.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: Figure 12.14 A causal DAG illustrating simple bandit feedback
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The causal DAG in figure 12.14 captures instant feedback with a utility node
    at each timestep, and with a lack of edges, reflecting an independence of variables
    across timesteps.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2 Contextual bandit feedback
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In contextual bandit feedback, one or more variables are common causes for both
    the act and the outcome. In figure 12.15, the context variable *C* is common to
    each {*X*, *Y*} tuple in the sequence. In this case, the context variable *C*
    could represent the profile of a particular individual, and the act variable *X*
    is that user’s behavior.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F15_Ness.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: Figure 12.15 A causal DAG illustrating contextual bandit feedback
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Alternatively, the context variable could change at each step, as in figure
    12.16.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F16_Ness.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Figure 12.16 A causal DAG illustrating contextual bandit feedback where the
    context changes at each timestep
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can vary this template in different ways. For example, we could have the
    actions drive the context variables in the next timestep, as in figure 12.17\.
    The choice depends on your specific problem.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F17_Ness.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: Figure 12.17 A causal DAG where the action at one timestep influences the context
    at the next timestep
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 12.3.3 Delayed feedback
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a delayed-feedback setting, the outcome variable and corresponding utility
    are no longer instant feedback. Instead, they come at the end of a sequence. Let’s
    consider an example where a context variable drives the acts. The acts affect
    the next instance of the context variable.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F18_Ness.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: Figure 12.18 Example of a causal DAG for sequential decision making with delayed
    feedback
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Again, figure 12.18 shows an example of this approach based on the previous
    model. Here the act at time *k* influences the context variable (*C*) at time
    *k* + 1, which in turn affects the act at time *k* + 1.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Consider a case of chronic pain. Here the context variable represents whether
    a subject is experiencing pain (*C*). The presence of pain drives the act of taking
    a painkiller (*X*). Taking the painkiller (or not) affects whether there is pain
    in the next step. Figure 12.19 illustrates this DAG.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F19_Ness.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: Figure 12.19 A causal DAG representing the treatment of chronic pain
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Y* here is the ultimate health outcome of the subject, and it is driven both
    by the overall amount of pain over time, and the amount of drugs the subject took
    (because perhaps overuse of painkillers has a detrimental health effect).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.4 Causal queries on a sequential model
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We may want to calculate some causal query for our sequential decision problem.
    For example, given the DAG in figure 12.19, we might want to calculate the causal
    effect of *X*[0] on *U*(*Y*):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-7x.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: 'Or perhaps we might be interested in the causal effect of the full sequence
    of acts on *U*(*Y*):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-8x.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Either way, now that we have framed the sequential problem as a causal model,
    we are in familiar territory; we can simply use the causal inference tools we’ve
    learned in previous chapters to answer causal queries with this model.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we must be attentive to the possibility of latent causes that can
    confound our causal inference. In the case of causal effects, our concern is latent
    common confounding causes between acts (*X*) and outcomes (*Y*), or alternatively
    between acts (*X*) and utilities (*U*). Figure 12.20 is the same as figure 12.15,
    except it introduces a latent *Z* confounder.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F20_Ness.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: Figure 12.20 Contextual bandit with a latent confounder
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similarly, we could have a unique confounder at every timestep, as in figure
    12.21.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F21_Ness.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: Figure 12.21 Bandit feedback with a different context and latent confounders
    at each timestep
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similarly, figure 12.22 shows a second version of the chronic pain graph where
    the confounders affect each other and the context variables. This confounder could
    be some external factor in the subject’s environment that triggers the pain and
    affects well-being.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: These confounders become an issue when we want to infer the causal effect of
    a sequence of actions on *U*(*Y*).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F22_Ness.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
- en: Figure 12.22 A version of the chronic pain DAG where the confounders affect
    each other and the context variables
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next, we’ll look at how we can view policies for automatic decision making in
    sequential decision-making processes as stochastic interventions.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 Policies as stochastic interventions
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In automated sequential decision-making, the term “policy” is preferred to
    “decision rule.” I’ll introduce a special notation for a policy: *π*(.). It will
    be a function that takes in observed outcomes of other variables and returns an
    action.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: To consider how a policy affects the model, we’ll contrast the DAG before and
    after a policy is implemented. Figure 12.23 illustrates a simple example with
    a context variable *C* and a latent variable *Z*. The policy uses context *C*
    to select a value of *X*.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F23_Ness.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: Figure 12.23 The dashed lines show edges modulated by the policy. The policy
    breaks the influence of the confounder *Z* like an ideal intervention, but dependence
    on *C* remains through the policy.
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The policy is a type of stochastic intervention; it selects a intervention value
    for *X* from some process that depends on *C*. Like an ideal intervention, it
    changes the graph. The left of figure 12.23 shows the DAG prior to deployment
    of the policy. On the right is the DAG after the policy is deployed. I add a special
    policy node to the graph to illustrate how the policy modulates the graph. The
    dashed edges highlight edges modulated by the policy. Just like an ideal intervention,
    the policy-generated intervention removes *X*’s original incoming edges *C*→*X*
    and *Z*→*X*. However, because the policy depends on *C*, the dashed edges illustrate
    the new flow of influence from *C* to *X*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we are interested in what value *Y* would have for a policy-selected
    action *X*=Π. In counterfactual notation, we write
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-9x.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: 'In sequence settings, the policy applies a stochastic intervention at multiple
    steps in the sequence. From a possible worlds perspective, each intervention induces
    a new hypothetical world. This can stretch the counterfactual notation a bit,
    so going forward, I’ll simplify the counterfactual notation to look like this:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-10x.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: This means *Y*[3] (*Y* at timestep 3) is under influence of the policy’s outcomes
    at times 0, 1, and 2.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.1 Examples in sequential decision-making
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of bandit feedback, the actions are produced by a *bandit algorithm*,
    which is a type of policy that incorporates the entire history of actions and
    utility outcomes in deciding the optimal current action. Though actions and outcomes
    in the bandit feedback process are independent at each time step, the policy introduces
    dependence on past actions and outcomes, as shown in figure 12.24\.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F24_Ness.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: Figure 12.24 Bandit feedback where a bandit policy algorithm selects the next
    action based on past actions and reward outcomes
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recall our previous example of an agent taking pain medication in response to
    the onset of pain. Figure 12.25 shows how a policy would take in the history of
    degree of pain and how much medication was provided.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F25_Ness.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: Figure 12.25 In the pain example, the policy considers the history of recorded
    levels of pain and corresponding dosages of medication.
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The policy is like a doctor making the rounds on a hospital floor. They come
    to a patient’s bed, and the patient reports some level of pain. The doctor looks
    at that patient’s history of pain reports and the subsequent dosages of medication
    and uses that information to decide what dosage to provide this time. The doctor’s
    utility function is in terms of pain, risk of overdose, and risk of addiction.
    They need to consider historic data, not just the current level of pain, to optimize
    this utility function.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.2 How policies can introduce confounding
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As stochastic interventions, policies introduce interventions conditional on
    other nodes in the graph. Because of this, there is a possibility that the policy
    will introduce new backdoor paths that can confound causal inferences. For example,
    consider again the DAG in figure 12.26.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F26_Ness.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: Figure 12.26 The policy eliminates the backdoor path through *Z* but not the
    backdoor path through *C*.
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The policy breaks the backdoor path from *X* to *Y* through *Z*, but there is
    still a path from *X* to *Y* through *C*. Thus, typical causal queries involving
    *X* and *Y* would have to condition on or adjust for *C*.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll characterize causal RL in causal terms.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Causal reinforcement learning
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning (RL) is a branch of machine learning that generally involves
    an agent learning policies that maximize cumulative reward (utility). The agent
    learns from the consequences of its actions, rather than from being explicitly
    taught, and adjusts its behavior based on the rewards or losses (reinforcements)
    it receives. Many sequential decision-making problems can be cast as RL problems.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.1 Connecting causality and Markov decision processes
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RL typically casts a decision process as a Markov decision process (MDP). A
    canonical toy example of an MDP is a grid world, illustrated in figure 12.27\.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.27 presents a 3 × 4 grid world. An agent can act within this grid
    world with a fixed set of actions, moving up, down, left, and right. The agent
    wants to execute a set of actions that deliver it to the upper-right corner {0,
    3}, where it gains a reward of 100\. The agent wants to avoid the middle-right
    square {1, 3}, where it has a reward of –100 (a *loss* of 100). Position {1, 1}
    contains an obstacle the agent cannot traverse.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F27_Ness.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: Figure 12.27 A simple grid world
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can think of it as a game. When the game starts, the agent “spawns” randomly
    in one of the squares, except for {0, 3}, {1, 3}, and {1, 1}. When the agent moves
    into a goal square, the game ends. To win, the agent must navigate around the
    obstacle in {1, 1}, avoid {1, 3}, and reach {0, 3}.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: A Markov decision process models this and much more complicated “worlds” (aka
    domains, problems, etc.) with abstractions for states, actions, transition functions,
    and rewards.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: States
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: States are a set that represents the current situation or context that the agent
    is in, within its environment. In the grid-world example, a state represents the
    agent being at a specific cell. In this grid, there are 12 different states (the
    cell at {1, 1} is an unreachable state). We assume the agent has some way of knowing
    which state they are in.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: We’ll denote state as a variable *S*. In a grid world, *S* is a discrete variable,
    but in other problems, *S* could be continuous.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Actions are the things the agent can do, and they lead to a change of state.
    Some actions might not be available when in a particular state. For example, in
    the grid world, the borders of the grid are constraints on the movements of the
    agent. If the agent is in the bottom-left square {2, 0}, and they try to move
    left or down, they will stay in place. Similarly, the cell at {1, 1} is an obstacle
    the agent must navigate around. We denote actions with the variable *A*, which
    has four possible outcomes {up, down, right, left}.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Transition function
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The transition function is a probability distribution function. It tells us
    the probability of moving to a specific next state, given the current state and
    the action taken.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'If states are discrete, the transition function looks like this:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-11x.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: Here, *S**[t]*=*s* means the agent is currently in state *s*. *A**[t]*=*a* means
    the agent performs action *a*. *P*(*S**[t]**[+1]*=*s'|S**[t]*=*s, A**[t]*=*a*)
    is the probability that the agent transitions to a new state s' given it is in
    state s and performs action *a*. When the action leads to a new state with complete
    certainty, this probability distribution function becomes degenerate (all probability
    is concentrated on one value).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Rewards
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The term “reward” is preferred to “utility” in RL. In the context of MDPs, the
    reward function will always take a state *s* as an argument. We will write it
    as *U*(*s*).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: In the grid-world example, *U*({0, 3}) = 100, *U*({1, 3)) = –100\. The reward
    of all other states is 0\. Note that sometimes in the MDP/RL literature, *U*()
    is a function of state and an action, as in *U*(*s*, *a*). We don’t lose anything
    by just having actions be a function of state because you can always fold actions
    into the definition of a state.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.2 The MDP as a causal DAG
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Figure 12.28 shows the MDP as a causal DAG.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F28_Ness.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: Figure 12.28 The Markov decision process represented as a DAG
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As a causal DAG, the MDP looks like the other sequential decision processes
    we’ve outlined, except that we limit ourselves to states, actions, and rewards.
    In figure 12.28, the process continues until we reach a terminal state (*S*[*k*]),
    such as getting to the terminal cells in the grid-world example.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: The causal Markov property and the MDP
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The “Markov” in “Markov decision process” comes from the fact that the current
    state is independent of the full history of states given the last state. Contrast
    this with the causal Markov property of causal DAGs: a node in the DAG is independent
    of indirect “ancestor” causes given its direct causal parents. We can see that
    when we view the MDP as a causal DAG, this Markovian assumption is equivalent
    to the causal Markov property. That means we can use our d-separation-based causal
    reasoning, including the do-calculus, in the MDP graphical setting.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: The transition function and the causal Markov kernel
  id: totrans-329
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Note that based on this DAG, the parents of a state *S*[(]*[t]*[+1)] are the
    previous state *S**[t]* and the action *A**[t]* taken when in that previous state.
    Therefore, the causal Markov kernel is *P*(*S**[t]**[+1]*=*s'|S**[t]*=*s, A**[t]*=*a*),
    i.e., the transition function. Thus, the transition function is the causal Markov
    kernel for a given state.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.3 Partially observable MDPs
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An extension of MDPs is *partially observed MDPs* (POMDPs). In a POMDP, the
    agent doesn’t know with certainty what state they are in, and they must make inferences
    about that state given incomplete evidence from their environment. This applies
    to many practical problems where the agent cannot observe the full state of the
    environment.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: A POMDP can entail different causal structures depending on our assumptions
    about the causal relationships between the unobserved and observed states. For
    example, suppose a latent state *S* is a cause of the observed state *X*. The
    observed state *X* now drives the act *A* instead of *S*. Figure 12.29 illustrates
    this formulation of a POMDP as a causal DAG.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F29_Ness.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: Figure 12.29 A POMDP where a latent state *S* causes an observed state *X*.
    *X* drives the actions *A*.
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In contrast, figure 12.30 illustrates an example where the latent state is a
    latent common cause (denoted Z) of the observed state (mediated through the agent’s
    action) and the utility (note a slight change of notation from *U*(*S*[*i*]) to
    *U*[*i*]). Here, unobserved factors influence both the agent’s behavior and the
    resulting utility of that behavior.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Again, the basic MDP and POMDP DAGs should be seen as templates for starting
    our analysis. Once we understand what causal queries we are interested in answering,
    we can explicitly represent various components of observed and unobserved states
    as specific nodes in the graph, and then use identification and adjustment techniques
    to answer our causal queries.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F30_Ness.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
- en: Figure 12.30 A POMPD formulation where the unobserved states are latent common
    causes that could act as confounders in causal inferences
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 12.5.4 Policy in an MDP
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As before, policies in an MDP act as stochastic interventions. Figure 12.31
    illustrates a policy that selects an optimal action based on the current state
    in a way that disrupts any influence on the action from a confounder.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F31_Ness.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
- en: Figure 12.31 Modification of an MDP DAG by a policy
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 12.31 is simple in that it only selects an action based on the current
    state. The challenge is in the implementation, because in most RL settings, states
    can be high-dimensional objects.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.5 Causal Bellman equation
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RL is about searching for the optimal policy, which is characterized with the
    Bellman equation, often written as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-12x.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: In plain words, we’re looking for a policy Π^* maximizes the cumulative reward
    over time. Here *γ* is a discount rate, a value between 0 and 1, that makes sure
    the agent values rewards in the near future more than rewards in the far future.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we’re reasoning about what would happen if we deployed the policy, the
    causal formulation would be as follows:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-13x.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
- en: Note that we could do the same causal rewrite for other variants of the Bellman
    equation, such as the Q-function used in Q-learning.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: The difference between the noncausal and causal formulations of the Bellman
    equation is the same as the difference between optimizing *E*(*U*(*Y*)|*X*=*x*)
    and *E*(*U*(*Y*[*X*][=][*x*])) in section 12.2.4\. The process of solving the
    causally naive version of the Bellman equation may introduce biases from latent
    confounders or from conditioning on colliders and mediators. Our causally attuned
    approach can help avoid these biases. In many cases, the solution of the naive
    approach will coincide with the causal approach because those biases might not
    affect the ranking of the top policy relative to others. However, as in the *E*(*U*(*Y*)|*X*=*x*)
    versus *E*(*U*(*Y*[*X*][=][*x*])) example, there will be cases where the solutions
    to the noncausal and causal formulations differ, and your RL problem might be
    one of those cases.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 12.6 Counterfactual reasoning for decision-theory
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve discussed the problem of choosing optimal actions with respect
    to a utility function as a level 2 query on the causal hierarchy. Is there a use
    for level 3 counterfactual reasoning in decision theory? In this section, we’ll
    briefly review some applications for level 3 reasoning.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 12.6.1 Counterfactual policy evaluation
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Counterfactual policy evaluation involves taking logged data from a policy in
    production and asking, “given we used this policy and got this cumulative reward,
    how much cumulative reward would we have gotten had we used a different policy?”
    See the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references to techniques such as *counterfactually guided policy search* and
    *counterfactual risk minimization*.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 12.6.2 Counterfactual regret minimization
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapters 8 and 9, I introduced *regret* as a counterfactual concept. We can
    further clarify the idea now that we have introduced the language of decision-making;
    regret is the difference between the utility/reward that was realized given a
    specific action or set of actions, and the utility/reward that would have been
    realized had another action or set of actions been taken.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '*Counterfactual regret minimization* is an approach to optimizing policies
    that seeks to minimize regret. To illustrate, suppose we have a policy variable
    *Π*, which can return one of several available policies. The policies take in
    the context and return an action. The action leads to some reward *U*.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Suppose, for a single instance in our logged data, the policy was *Π*=*π* and
    the context was *C*=*c*. We get a certain action *A*=*π*(*c*) and reward *U*=*u*.
    For some policy *π**'*, regret is the answer to the counterfactual question, “How
    much more reward would we have gotten if the policy had been *π*=*π**'*?” In terms
    of expectation,
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-14x.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
- en: Again, this is regret for a single instance in logged data where the context
    was *C*=*c* and the utility was *u*. There are many variations, but the general
    idea is to find the policy that would have minimized cumulative regret over all
    the cases of *C*=*c* in the logged data, with the goal of favoring that policy
    in cases of *C*=*c* in the future.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 12.6.3 Making level 3 assumptions in decision problems
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The question, of course, is how to make the level 3 assumptions that enable
    counterfactual inferences. One approach would be to specify an SCM and use the
    general algorithm for counterfactual reasoning (discussed in chapter 9). For example,
    in RL, the transition function *P*(*S**[t]**[+1]*=*s'|S**[t]*=*s, A**[t]*=*a*)
    captures the rules of state changes in the environment. As I mentioned, *P*(*S**[t]**[+]**[1]**|S**[t]*=*s,
    A**[t]*=*a*) is the causal Markov kernel for a given state *S**[t]**[+1]*. We
    could specify an SCM with an assignment function that entails that causal Markov
    kernel, and write that assignment function as
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-15x.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: Here, *n*[*s*]*[']* is the value of an exogenous variable for *S**[t]*.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: The challenge is specifying assignment functions that encode the correct counterfactual
    distributions. This is easier in domains where we know more about the underlying
    causal mechanisms. A key example is in rule-based games; game rules can provide
    the level 3 constraints that enable simulation of counterfactuals. Recall how,
    in chapter 9, the simple rules of the Monte Hall problem enabled us to simulate
    counterfactual outcomes for stay versus switch strategies. Or consider multiplayer
    games like poker, where in a round of play each player is dealt a hand of cards
    and can take certain actions (check, bet, call, raise, or fold) that lead to outcomes
    (win, lose, tie) based on simple rules, which in turn determine the amount of
    chips won or lost in that round. A player’s counterfactual regret is the difference
    between the chips they netted and the most they could have netted had they decided
    on different actions. This is done while accounting for the information available
    at the time of the decision, not using hindsight about the opponents’ cards.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Counterfactual regret minimization algorithms in this domain attempt to find
    game playing policies that minimize counterfactual regret across multiple players.
    The concrete rules of the game enable simulation of counterfactual game trajectories.
    The challenge lies in searching for optimal policies within a space of possible
    counterfactual trajectories that is quite large because of multiple player interactions
    over several rounds of play. See the chapter notes on counterfactual regret minimization
    in multiagent games at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision-making is naturally a causal problem because decisions cause consequences,
    and our goal is to make the decision that leads to favorable consequences.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the optimal decision is a level 2 query as we are asking “what would
    happen if I made this decision?”
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*|*X*=*x*)) and *E*(*U*(*Y*[*X*][=][*x*])) are different quantities.
    Usually, people want to know the value of *X* that optimizes *E*(*U*(*Y*[*X*][=][*x*])),
    but optimizing *E*(*U*(*Y*|*X*=*x*)) will often yield the same answer without
    the bother of specifying a causal model.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially true in reinforcement learning (RL), where the analogs to
    *E*(*U*(*Y*|*X*=*x*)) and *E*(*U*(*Y*[*X*][=][*x*])) are, respectively, the conventional
    and causal formulations of the Bellman equation. Confounder, mediator, and collider
    biases may be present in conventional approaches to solving the Bellman equation.
    But those bias often don’t influence the ranking of the top policy relative to
    other policies.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonetheless, sometimes the value of *X* that optimizes *E*(*U*(*Y*|*X*=*x*))
    is different from that which optimizes *E*(*U*(*Y*[*X*][=][*x*])). Similarly,
    addressing causal nuances when solving the Bellman equation may result in a different
    policy than ignoring them. If your decision problem falls into this category,
    causal approaches are the better choice.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newcomb’s paradox is a thought experiment meant to contrast causal and noncausal
    approaches to decision theory. The “paradox” is less mysterious once we use a
    formal causal model.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal decision theory, combined with probabilistic modeling tools like Pyro
    and pgmpy, is well suited to modeling introspection, where an agent reflects on
    their internal state (feelings, intuition, urges, intent) and uses that information
    to predict the “what-if” outcomes of their decisions.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we represent a sequential decision process with a causal DAG, we can employ
    all the tools of graphical causal inference in that decision problem.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policies operate like stochastic interventions. They change the graph but still
    have dependence on observed nodes in the past, and that dependence can introduce
    backdoor confounding.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In causal RL, we can represent MDPs and POMDPs as causal DAGs and, again, make
    use of graphical causal inference theory.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use template DAGs to represent sequential decision processes, but you
    should tailor these templates for your problem.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common use cases for counterfactual reasoning in decision theory are counterfactual
    policy evaluation and counterfactual regret minimization.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have access to the rules underlying state transitions in your MDP, such
    as in physical systems or games, you could build an SCM that is counter- factually
    faithful to those rules, and use it to handle counterfactual use cases in decision-making.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
