- en: 12 Causal decisions and reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using causal models to automate decisions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up causal bandit algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to incorporate causality into reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we apply methods from statistics and machine learning, it is typically
    in service of making a decision or automating decision-making. Algorithms for
    automated decision-making, such as *bandit* and *reinforcement learning* (RL)
    algorithms, involve agents that *learn* how to make good decisions. In both cases,
    decision-making is fundamentally a causal problem: a decision to take some course
    of action leads to consequences, and the objective is to choose the action that
    leads to consequences favorable to the decision-maker. That motivates a causal
    framing.'
  prefs: []
  type: TYPE_NORMAL
- en: Often, the path from action to consequences has a degree of randomness. For
    example, your choice of how to play a hand of poker may be optimal, but you still
    might lose due to chance. That motivates a probabilistic modeling approach.
  prefs: []
  type: TYPE_NORMAL
- en: The causal probabilistic modeling approach we’ve used so far in this book is
    a stone that hits both these birds. This chapter will provide a *causality-first*
    introduction to basic ideas in statistical decision theory, sequential decision-making,
    bandits, and RL. By “causality-first,” I mean I’ll use the foundation we’ve built
    in previous chapters to introduce these ideas in a causal light. I’ll also present
    the ideas in a way that is compatible with our probabilistic ML framing. Even
    if you are already familiar with these decision-making and RL concepts, I encourage
    you to read on and see them again through a causal lens. Once we do that, we’ll
    see cases where the causal approach to RL gets a better result than the noncausal
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 A causal primer on decision theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision theory is concerned with the reasoning underlying an agent’s choice
    of some course of action. An “agent” here is an entity that chooses an action.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose you were deciding whether to invest in a company by purchasing
    equity or purchasing debt (i.e., loaning money to the company and receiving interest
    payments). We’ll call this variable *X*. Whether the company is successful (*Y*)
    depends on the type of investment it receives.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F01_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 A simple causal DAG where action *X* causes some outcome *Y*. Decision
    theory is a causal problem because if deciding on an action didn’t have causal
    consequences, what would be the point of making decisions?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since *X* causally drives *Y*, we can immediately introduce a causal DAG, as
    in figure 12.1.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use this example to illustrate basic concepts in decision theory from
    a causal point of view.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.1 Utility, reward, loss, and cost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The agent generally chooses actions that will cause them to gain some utility
    (or minimize some loss). In decision modeling, you can define a utility function
    (aka a reward function) that quantifies the desirability of various outcomes of
    a decision. Suppose you invest at $1,000:'
  prefs: []
  type: TYPE_NORMAL
- en: If the company becomes successful, you get $100,000\. Your utility is 100,000
    – 1,000 = $99,000\.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the company fails, you get $0 and lose your investment. Your utility is –1,000\.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can add this utility as a node on the graph, as in figure 12.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F02_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 A utility node can represent utility/reward, loss/cost.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note that utility is a deterministic function of *Y* in this model, which we’ll
    denote *U*(*Y*).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: Instead of a utility/reward function, we could define a loss function (aka,
    a cost function), which is simply –1 times the utility/reward function. For example,
    in the second scenario, where you purchase stock and the company fails, your utility
    is –$1,000 and your loss is $1,000.
  prefs: []
  type: TYPE_NORMAL
- en: While the agent’s goal is to decide on a course of action that will maximize
    utility, doing so is challenging because there is typically some uncertainty in
    whether an action will lead to the desired result. In our example, it may seem
    obvious to invest in equity because equity will lead to business success, and
    business success will definitely lead to more utility. But there is some uncertainty
    in whether an equity investment will lead to business success. In other words
    we don’t assume *P*(*Y*=success|*X*=equity) = 1\. Both success and failure have
    nonzero probability in *P*(*Y*|*X*=equity).
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.2 Uncertainty comes from other causes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In causal terms, given action *X*, there is still some uncertainty in the outcome
    *Y* because there are other causal factors driving that outcome. For example,
    suppose the success of the business depends on economic conditions, as in figure
    12.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F03_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 We typically have uncertainty in our decision-making. From a causal
    perspective, uncertainty is because of other causal factors out of our control
    that affect variables downstream of our actions.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Alternatively, those other causal factors could affect utility directly. For
    example, rather than the two discrete scenarios of profit or loss I outlined for
    our business investment, the amount of utility (or loss) could depend on how well
    or how poorly the economy fares, as in figure 12.4\. We can leverage statistical
    and probability modeling to address this uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F04_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 Causal factors outside of our control can impact utility (or loss)
    directly.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Suppose you are thinking about whether to invest in this business. You want
    your decision to be data-driven, so you research what other investors in this
    market have done before. You consider the causal DAG in figure 12.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F05_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 In this DAG, economic conditions drive how investors choose to invest.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Based on your research, you conclude that past investors’ equity vs. debt choice
    also depends on the economic conditions. *P*(*X*|*C*) represents an action distribution—the
    distribution of actions that the population of investors you are studying take.
  prefs: []
  type: TYPE_NORMAL
- en: However, the goal of your analysis centers on yourself, not other investors.
    You want to answer questions like “what if *I* bought equity?” That question puts
    us in causal territory. We are not reasoning about observational investment trends;
    we are reasoning about conditional hypotheticals. That is an indicator that we
    need to introduce intervention-based reasoning and counterfactual notation.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Causal decision theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll highlight decision-making as a causal query and examine
    what that means for modeling decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Decisions as a level 2 query
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A major source of confusion for causal decision modeling is the difference between
    actions and interventions. In many decision contexts, especially in RL, the action
    is a thing that the agent *does* that changes their environment. Yet, the action
    is also a variable *driven by* the environment. We see this when we look at the
    investment example, shown again in figure 12.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F06_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 In this version of the investment DAG, the choice of action is caused
    by external factors.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The action of selecting equity or debt is a variable causally driven by the
    economy. What does that mean? Is an action a variable with causes, or is it an
    intervention?
  prefs: []
  type: TYPE_NORMAL
- en: The answer is *both*, depending on context. When it is which depends on the
    question we are asking and where that question sits in the causal hierarchy (discussed
    in chapter 10). When we are talking about what actions usually happen, such as
    when we are observing the actions of other agents (or even when reflecting on
    our own past actions) and what results those actions led to, we are reflecting
    on trends in population, and we are on level 1 of the causal hierarchy. In the
    case of our investment example, we’re reasoning about *P*(*C*, *X*, *Y*, *U*).
    But if we’re asking questions like “what would happen if I made an equity investment?”
    then we’re asking a level 2 question, and we need the proposed action as an intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll characterize common decision rules using our causal notation.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 Causal characterization of decision rules and policies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A decision rule is a rule for choosing an action based on the utility distribution
    *P*(*U*(*Y*[*X*][=][*x*])). The agent chooses an optimal action according to a
    decision rule. For example, a common decision rule is choosing the action that
    minimizes loss or cost or maximizes utility or reward.
  prefs: []
  type: TYPE_NORMAL
- en: In automated decision-making, the decision rule is often called a “policy.”
    In public health settings, decision rules are sometimes called “treatment regimes.”
  prefs: []
  type: TYPE_NORMAL
- en: Maximizing expected utility
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The most intuitive and commonly seen decision rule is to choose the action that
    maximizes expected utility. First, we can look at the expectation of the utility
    distribution. Since utility is a deterministic function of *Y*[*X*][=][*x*], this
    is just the expectation of *U*(*Y*[*X*][=][*x*]) over the intervention distribution
    of *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-1x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We then choose the action (value of *x*) that maximizes expected utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-2x.png)'
  prefs: []
  type: TYPE_IMG
- en: In our investment example, this means choosing the investment approach that
    is expected to make you the most money.
  prefs: []
  type: TYPE_NORMAL
- en: Minimax decision rules
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To understand the minimax decision rule, recall that the terms “utility” and
    “loss” are two sides of the same coin; utility == negative loss. Let *L*(*y*)
    = –*U*(*y*). Then a minimax decision rule is
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-3x.png)'
  prefs: []
  type: TYPE_IMG
- en: In plain English, this means “choose the action that minimizes the maximum amount
    of possible loss.” In our investment example, this means choosing the investment
    approach that will minimize the amount of money you’d lose in the worst case scenario.
    There are many variants of minimax rules, but they have the same flavor—minimizing
    loss or maximizing utility during bad times.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax rules
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A softmax decision rule randomly selects an action with a probability proportional
    to the resulting utility.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s define *C*(*x*) as the probability of choosing the action *x*. Then *C*(*x*)
    is defined as a probability value proportional to
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-4x.png)'
  prefs: []
  type: TYPE_IMG
- en: The noise parameter *α* modulates between the two extremes. When *α*=0, we have
    a uniform distribution on all the choices. As *α* gets larger, we approach maximizing
    expected utility.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes our goal is to model the decision-making of other agents, such as
    in inverse RL. The softmax decision rule is useful when agents don’t always make
    the utility-optimizing choice. The softmax decision rule provides a simple, analytically
    tractable, and empirically validated model of suboptimal choice.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason we might want to use the softmax rule is when there is a trade-off
    between *exploring* and *exploiting,* such as with bandit problems. Suppose the
    agent is uncertain about the shape of the distibution *P*(*Y*[*X*][=][*x*]). The
    optimal action according to an incorrect model of *P*(*Y*[*X*][=][*x*]) might
    be different from the optimal choice according to the correct model of *P*(*Y*[*X*][=][*x*]).
    The softmax decision rule allows us to choose various actions, get some data on
    the results, and use that data to update our model of *P*(*Y*[*X*][=][*x*]). When
    this is done in sequence, it’s often called *Thompson sampling*.
  prefs: []
  type: TYPE_NORMAL
- en: In our investment analogy, suppose we were to invest in several businesses.
    Perhaps, according to our current model, equity investment maximizes expected
    utility, but we’re not fully confident in our current model, so we opt to select
    debt investment even though the current model says its less optimal. The goal
    is to add diversity to our dataset, so that we can learn a better model.
  prefs: []
  type: TYPE_NORMAL
- en: Other types of decision rules
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are other types of decision rules, and they can become complicated, especially
    when they involve statistical estimation. For example, using *p*-values in statistical
    hypothesis testing involves a nuanced utility function that balances the chances
    of a false positive (incorrectly choosing the alternative hypothesis) and a false
    negative (incorrectly choosing the null hypothesis).
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, when we work with probabilistic causal models, the math tends to
    be easier, and we get a nice guarantee called *admissibility*.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.3 Causal probabilistic decision-modeling and admissibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, I’ll provide a short justification for choosing a causal probabilistic
    modeling approach to decision-making. When you implement an automated decision-making
    algorithm in a production setting, you might have to explain why your implementation
    is better than another. In that setting, it is useful if you know if your algorithm
    is *admissible*.
  prefs: []
  type: TYPE_NORMAL
- en: A decision rule is *admissible* if there are no other rules that dominate it.
    A decision rule dominates another rule if the performance of the former is sometimes
    better, and never worse, than that of the other rule with respect to the utility
    function. For example, the softmax decision rule is dominated by maximizing expected
    utility (assuming you know the true shape of *P*(*Y**[X]*[=]*[x]*)) because sometimes
    it will select suboptimal actions, and it is thus inadmissible. Determining *admissibility*
    is a key task in decision theory.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge for us occurs when we use data and statistics to deal with unknowns,
    such as parameters or latent variables. If we want to use data to estimate a parameter
    or work with latent variables, there are usually a variety of statistical approaches
    to choose from. If our decision-making algorithm depends on a statistical procedure,
    the choice of procedure can influence which action is considered optimal. How
    do we know if our statistical decision-making procedure is admissible?
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic modeling libraries like Pyro leverage Bayesian inference to estimate
    parameters or impute latent variables. Bayesian decision theory tells us that
    *Bayes rules*, (not to be confused with Bayes’s rule) decision rules that optimize
    posterior expected utility, have an admissibility guarantee under mild regularity
    conditions. This means that if we use Bayesian inference in Pyro or similar libraries
    to calculate and optimize posterior expected loss, we have an admissibility guarantee
    (if those mild conditions hold, and they usually do). That means you needn’t worry
    that someone else’s decision-making model (that makes the same modeling assumptions,
    has the same utility function, and uses the same data) will beat yours.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.4 The deceptive alignment of argmax values of causal and non-causal expectations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most conventional approaches to decision-making, including in RL, focus on maximizing
    *E*(*U*(*Y*)|*X*=*x*) rather than *E*(*U*(*Y*[*X*][=][*x*])). Let’s implement
    the model in figure 12.6 with pgmpy and compare the two approaches.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll build the DAG in the model.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This code was written with pgmpy version 0.1.24\. See the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for a link to the notebook that runs this code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1 DAG for investment decision model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Set up the DAG'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we’ll build the causal Markov kernels for *Economy* (*C*), *Debt vs. Equity*
    (*X*), and *Business Success* (*Y*). The causal Markov kernel for *Economy* (*C*)
    will take two values: “bear” for bad economic conditions and “bull” for good.
    The causal Markov kernel for *Debt vs. Equity* (*X*) will depend on *C*, reflecting
    the fact that investors tend to prefer equity in a bull economy and debt in a
    bear economy. *Success* (*Y*) depends on the economy and the choice of debt or
    equity investment.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 Create causal Markov kernels for *C*, *X*, and *Y*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Set up causal Markov kernel for C (economy). It takes two values: “bull”
    and “bear”.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Set up causal Markov kernel for action X, either making a debt investment
    or equity investment depending on the economy.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Set up causal Markov kernel for business outcome Y, either success or failure,
    depending on the type of investment provided (X) and the economy (C).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll add the *Utility* node (*U*). We use probabilities of 1 and 0
    to represent a deterministic function of *Y*. We end by adding all the kernels
    to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.3 Implement the utility node and initialize the model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Set up the utility node.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Set up the utility node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This code prints out the following conditional probability tables for our causal
    Markov kernels. This one is for the *Utility* variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This reflects the investor trends of favoring equity investments in a bull market
    and debt investments in a bear market.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following probability table is for the *Business Success* variable *Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This reflects debt being a less preferred source of financing in a bear market
    when interest rate payments are higher, and equity being preferred in a bull market
    because equity is cheaper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the *Utility* node is a simple deterministic function that maps *Y*
    to utility values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Next, we’ll calculate* *E*(*U*(*Y*[*X*][=][*x*])) and *E*(*U*(*Y*)|*X*=*x*).
    Before proceeding, download and load a helper function that implements an ideal
    intervention. To allay any security concerns of directly executing downloaded
    code, the code prints the downloaded script and prompts you to confirm before
    executing the script.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.4 Download helper function for implementing an ideal intervention
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Load an implementation of an ideal intervention.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 To allay security concerns, you can inspect the downloaded script and confirm
    it before running.'
  prefs: []
  type: TYPE_NORMAL
- en: By now, in this book, you should not be surprised that *E*(*U*(*Y*[*X*][=][*x*]))
    is different from *E*(*U*(*Y*)|*X*=*x*). Let’s look at these values.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.5 Calculate *E*(*U*(*Y*)|*X*=*x*) and *E*(*U*(*Y**[X]*[=]*[x]*))
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A helper function for calculating the expected utility'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Set X by intervention to debt and equity and calculate the expectation of
    U under each intervention.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Condition on X = debt and X = equity, and calculate the expectation of U.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us the following conditional expected utilities (I’ve marked the
    highest with *):'
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*)|*X*=debt) = 57000 *'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*)|*X*=equity) = 37000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It also gives us the following interventional expected utilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*X*][=debt])) = 39000 *'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*X*][=equity])) = 34000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So *E*(*U*(*Y*)|*X*=debt) is different from *E*(*U*(*Y*[*X*][=debt])), and *E*(*U*(*Y*)|*X*=equity)
    is different from *E*(*U*(*Y*[*X*][=][equity])). However, our goal is to optimize
    expected utility, and in this case, debt maximizes both *E*(*U*(*Y*)|*X*=x) and
    *E*(*U*(*Y*[*X*][=][*x*])).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-5x.png)'
  prefs: []
  type: TYPE_IMG
- en: If “debt” maximizes both queries, what is the point of causal decision theory?
    What does it matter if *E*(*U*(*Y*)|*X*=*x*) and *E*(*U*(*Y*[*X*][=][*x*])) are
    different if the optimal action for both is the same?
  prefs: []
  type: TYPE_NORMAL
- en: In decision problems, it is quite common that a causal formulation of the problem
    provides the same answer as more traditional noncausal formulations. This is especially
    true in higher dimensional problems common in RL. You might observe this and wonder
    why the causal formulation is needed at all.
  prefs: []
  type: TYPE_NORMAL
- en: To answer, watch what happens when we make a slight change to the parameters
    of *Y* in the model. Specifically, we’ll change the parameter for *P*(*Y*=success|*X*=equity,
    *C*=bull) from .4 to .6\. First, we’ll rebuild the model with the parameter change.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.6 Change a parameter in the causal Markov kernel for *Y*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Initialize a new model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Create a new conditional probability distribution for Y.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Change the parameter P(Y=success|X=equity, C=bull) = 0.4 (the last parameter
    in the first list) to 0.6.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Add the causal Markov kernels to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we rerun inference.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.7 Compare outcomes with changed parameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Set X by intervention to debt and equity, and calculate the expectation
    of U under each intervention.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Condition on X = debt and X = equity, and calculate the expectation of U.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us the following conditional expectations (* indicates the optimal
    choice):'
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*)|*X*=debt) = 57000 *'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*)|*X*=equity) = 53000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It also gives us the following interventional expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*X*][=debt])) = 39000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*X*][=equity])) = 44000 *'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that slight change in a single parameter, “debt” is still the optimal value
    of *x* in *E*(*U*(*Y*)|*X*=*x*), but now “equity” is the optimal value of *x*
    in *E*(*U*(*Y*[*X*][=][*x*])). This is a case where the causal answer and the
    answer from conditioning on evidence are different. Since we are trying to answer
    a level 2 query, the causal approach is the right approach.
  prefs: []
  type: TYPE_NORMAL
- en: This means that while simply optimizing a conditional expectation often gets
    you the right answer, you are vulnerable to getting the wrong answer in certain
    circumstances. Compare this to our discussion of semi-supervised learning in chapter
    4—often the unlabeled data can help with learning, but, in specific circumstances,
    the unlabeled data adds no value. Causal analysis helped us characterize those
    circumstances in precise terms. Similarly, in this case, there are specific scenarios
    where the causal formulation of the problem will lead to a different and more
    correct result relative to the traditional noncausal formulation. Even the most
    popular decision-optimization algorithms, including the deep learning-based approaches
    used in deep RL, can improve performance by leveraging the causal structure of
    a decision problem.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll see another example with Newcomb’s paradox.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.5 Newcomb’s paradox
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A famous thought experiment called Newcomb’s paradox contrasts the causal approach
    to decision theory, maximizing utility under intervention, with the conventional
    approach of maximizing utility conditional on some action. We’ll look at an AI-inspired
    version of this thought experiment in this section, and the next section will
    show how to approach it with a formal causal model.
  prefs: []
  type: TYPE_NORMAL
- en: There are two boxes designated A and B as shown in figure 12.7\. Box A always
    contains $1,000\. Box B contains either $1,000,000 or $0\. The decision-making
    agent must choose between taking only box B or *both* boxes. The agent does not
    know what is in box B until they decide. Given this information, it is obvious
    the agent should take both boxes—choosing both yields either $1,000 or $1,001,000,
    while choosing only B yields either $0 or $1,000,000.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F07_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 An illustration of the boxes in Newcomb's paradox
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now, suppose there is an AI that can predict with high accuracy what choice
    the agent intends to make. If the AI predicts that the agent intends to take both
    boxes, it will put no money in box B. If the AI is correct and the agent takes
    both boxes, the agent only gets $1,000\. However, if the AI predicts that the
    agent intends to take only box B, it will put $1,000,000 in box B. If the AI predicts
    correctly, the agent gets the $1,000,000 in box B but not the $1,000 in box A.
    The agent does not know for sure what the AI predicted or what box B contains
    until they make their choice.
  prefs: []
  type: TYPE_NORMAL
- en: The traditional paradox arises as follows. A causality-minded agent reasons
    that the actions of the AI are out of their control. They only focus on what they
    can control—the causal consequences of their choice. They can’t *cause* the content
    of box B, so they pick both boxes on the off-chance box B has the million, just
    as one would if the AI didn’t exist. But if the agent knows how the AI works,
    doesn’t it make more sense to choose only box B and get the million with certainty?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dig in further by enumerating the possible outcomes and their probabilities.
    Let’s assume the AI’s predictions are 95% accurate. If the agent chooses both
    boxes, there is a 95% chance the AI will have guessed the agent’s choice and put
    no money in B, in which case the agent only gets the $1,000\. There is a 5% chance
    the algorithm will guess wrong, in which case it puts 1,000,000 in box B, and
    the agent wins $1,001,000\. If the agent chooses only box B, there is a 95% chance
    the AI will have predicted the choice and placed $1,000,000 in box B, giving the
    agent $1,000,000 in winnings. There is a 5% chance it will not, and the agent
    will take home nothing. We see these outcomes in table 12.1\. The expected utility
    calculations are shown in table 12.2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.1 Newcomb’s problem outcomes and their probabilities
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Strategy | AI action | Winnings | Probability |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Choose both  | Put $0 in box B  | $1,000  | .95  |'
  prefs: []
  type: TYPE_TB
- en: '| Choose both  | Put $1,000,000 in box B  | $1,001,000  | .05  |'
  prefs: []
  type: TYPE_TB
- en: '| Choose only box B  | Put $1,000,000 in box B  | $1,000,000  | .95  |'
  prefs: []
  type: TYPE_TB
- en: '| Choose only box B  | Put $0 in box B  | $0  | .05  |'
  prefs: []
  type: TYPE_TB
- en: Table 12.2 Expected utility of each choice in Newcomb’s problem
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Strategy ( *x*) | *E*( *U*&#124; *X*= *x*) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Choose both  | 1,000 × .95 + 1,001,000 × .05 = $51,000  |'
  prefs: []
  type: TYPE_TB
- en: '| Choose only box B  | 1,000,000 × .05 + 0 × .05 = $950,000  |'
  prefs: []
  type: TYPE_TB
- en: The conventional approach suggests choosing box only box B.
  prefs: []
  type: TYPE_NORMAL
- en: When the paradox was created, taking a causal approach to the problem meant
    only attending to the causal consequences of one’s actions. Remember that the
    AI makes the prediction *before* the agent acts. Since effects cannot precede
    causes in time, the AI’s behavior is not a consequence of the agent’s actions,
    so the agent with the causal view ignores the AI and goes with the original strategy
    of choosing both boxes.
  prefs: []
  type: TYPE_NORMAL
- en: It would seem that the agent with the causal view is making an error in failing
    to account for the actions of the AI. But we can resolve this error by having
    the agent use a formal causal model.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.6 Newcomb’s paradox with a causal model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the traditional formulation of Newcomb’s paradox, the assumption is that
    the agent using causal decision theory only attends to the consequences of their
    actions—they are reasoning on the causal DAG in figure 12.8\. But the true data
    generating process (DGP) is better captured by figure 12.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F08_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 Newcomb’s paradox assumes a version of causal decision theory where
    a naive agent uses this incorrect causal DAG.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F09_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.9 A better causal DAG representing the framing of Newcomb’s paradox
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The choice of the agent can’t *cause* the AI’s prediction, because the prediction
    happens first. Thus, we assume the AI agent is inferring the agent’s *intent*,
    and thus the intent of the agent is the cause of the AI’s prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The causal decision-making agent would prefer the graph in figure 12.9 because
    it is a better representation of the DGP. The clever agent wouldn’t focus on maximizing
    *E*(*U*[*choice*][=][*x*]). The clever agent is aware of its own intention, and
    knowing that this intention is a cause of the content of box B, it focuses on
    optimizing *E*(*U*[*choice*][=][*x*]|*intent*=*i*), where *i* is their original
    intention of which box to pick.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-6x.png)'
  prefs: []
  type: TYPE_IMG
- en: We’ll assume the agent’s initial intention is an impulse it cannot control.
    But while they can’t control their initial intent, they can do some introspection
    and become aware of this intent. Further, we’ll assume that upon doing so, they
    have the ability to change their choice to something different from what it initially
    intended, after the AI has made their prediction and set the contents of box B.
    Let’s model this system in pgmpy and evaluate maximizing *E*(*U*[*choice*][=][*x*]|*intent*=*i*).
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s build the DAG.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.8 Create the DAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll create causal Markov kernels for intent and choice.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.9 Create causal Markov kernels for intent and choice
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We assume a 50-50 chance the agent will prefer both boxes vs. box B.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We assume the agent’s choice is deterministically driven by their intent.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we’ll create the causal Markov kernels for the AI’s decision and
    the content of box B.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.10 Create causal Markov kernels for AI prediction and box B content
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The AI’s prediction is 95% accurate.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Box B contents are set deterministically by the AI’s prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll create a causal Markov kernel for utility and add all the kernels
    to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.11 Create utility kernel and build the model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Set up the utility node.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Build the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll evaluate maximizing *E*(*U*[*choice*][=][*x*]|*intent*=*i*).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.12 Infer optimal choice using intervention and conditioning on intent
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Infer E(U(Y [choice=both]|intent=both)).'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Infer E(U(Y [choice=box B]|intent=both)).'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Infer E(U(Y [choice=both]|intent=B)).'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Infer E(U(Y [choice=box B]|intent=B)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This code produces the following results (* indicates the optimal choice for
    a given intent):'
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*choice*][=both]|*intent*=both)) = 51000 *'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*choice*][=box B]|*intent*=both)) = 50000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*choice*][=both]|*intent*=B)) = 951000 *'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*[*choice*][=box B]|*intent*=B)) = 950000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the agent’s initial intention is to select both, the best choice is to
    select both. When the agent intends to choose only box B, the best choice is to
    ignore those intentions and choose both. Either way, the agent should choose both.
    Note that when the agent initially intends to choose only box B, switching to
    both boxes gives them an expected utility of $951,000 which is greater than the
    optimal choice utility of $950,000 in the noncausal approach.
  prefs: []
  type: TYPE_NORMAL
- en: The agent, unfortunately, cannot control their initial intent; if they could,
    they would deliberately ‘intend’ to pick box B and then switch at the last minute
    to choosing both boxes after the AI placed the million in box B. However, they
    can engage in a form of introspection, factoring their initial intent into their
    decision and, in so doing, accounting for the AI’s behavior rather than ignoring
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.7 Introspection in causal decision theory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Newcomb’s problem illustrates a key capability of causal decision theory—the
    ability for us to include introspection as part of the DGP.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F10_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.10 Often our actions are simply reactions to our environment, rather
    than the result of deliberate decision-making.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To illustrate, consider that often our actions are simply *reactions* to our
    environment, as in figure 12.10.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you might have purchased a chocolate bar *because* you were hungry
    and it was positioned to tempt you as you waited in the checkout aisle of the
    grocery store. Rather than go through some deliberative decision-making process,
    you had a simple, perhaps even unconscious, *reaction* to your craving and an
    easy way to satisfy it.
  prefs: []
  type: TYPE_NORMAL
- en: However, humans are capable of introspection—observing and thinking about their
    internal states. A human might consider their normal reactive behavior as part
    of the DGP. This introspection is illustrated in figure 12.11.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F11_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.11 Humans and some other agents can think about a DGP that includes
    them as a component of that process.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Through this introspection, the agent can perform level 2 hierarchical reasoning
    about what would happen if they did not react as usual but acted deliberately
    (e.g., sticking to their diet and not buying the chocolate bar), as in figure
    12.12.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F12_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.12 The agent reasons about a DGP that includes them as a component.
    They then use that reasoning in asking level 2 “what would happen if...” questions
    about that process.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In many cases, the agent may not know the full state of their environment. However,
    if the agent can disentangle their urge to react a certain way from their action,
    they can use that “urge” as evidence in deliberative decision-making, as in figure
    12.13.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F13_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.13 The agent may not know the states of other variables in the environment,
    but through introspection, they may have an intuition about those variables. That
    intuition can be used as evidence in conditional causal inferences.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We saw this pattern in the Newcomb example; the agent does not know what the
    AI has predicted, but, through introspection, they can use their initial intention
    to choose both boxes as *evidence* of what the AI has chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Was there ever a time where you noticed you had started to make clumsy errors
    in your work and used that as evidence that you were fatigued, even though you
    didn’t feel so, and you thought, “what if I take a break?” Have you had a gut
    feeling that something was off, despite not knowing what, and based on this feeling
    started to make different decisions? Causal modeling, particularly with causal
    generative models, make it easy to write algorithms that capture this type of
    self-introspection in decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at causal modeling of sequential decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Causal DAGs and sequential decisions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sequential decision processes are processes of back-to-back decision-making.
    These processes can involve sequential decisions made by humans or by algorithms
    and engineered agents.
  prefs: []
  type: TYPE_NORMAL
- en: When I model decision processes in sequence, I use a subscript to indicate a
    discrete step in the series, such as *Y*[1], *Y*[2], *Y*[3]. When I want to indicate
    an intervention subscript, I’ll place it to the right of the time-step subscript,
    as in *Y*[1,][*X*][=][*x*], *Y*[2,][*X*][=][*x*], *Y*[3,][*X*][=][*x*].
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I’ll show causal DAGs for several canonical sequential decision-making
    processes, but you should view these as templates, not as fixed structures. You
    can add or remove edges in whatever way you deem appropriate for a given problem.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the simplest case, bandit feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.1 Bandit feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Bandit feedback* refers to cases where, at each step in the sequence, there
    is an act *X* that leads to an outcome *Y*, with some utility *U*(*Y*). A bandit
    sequence has two key features. The first is that, at every step, there is instant
    feedback after an act occurs. The second is independent trials, meaning that the
    variables at the *t*^(th) timestep are independent of variables at other timesteps.
    The term “bandit” comes from an analogy to “one-armed bandits,” which is a slang
    term for casino slot machines that traditionally have an arm that the player pulls
    to initiate gameplay. Slot machine gameplay provides bandit feedback—you deposit
    a token, pull the arm, and instantly find out if you win or lose. That outcome
    is independent of previous plays.'
  prefs: []
  type: TYPE_NORMAL
- en: We can capture bandit feedback with the causal DAG in figure 12.14.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F14_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.14 A causal DAG illustrating simple bandit feedback
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The causal DAG in figure 12.14 captures instant feedback with a utility node
    at each timestep, and with a lack of edges, reflecting an independence of variables
    across timesteps.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2 Contextual bandit feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In contextual bandit feedback, one or more variables are common causes for both
    the act and the outcome. In figure 12.15, the context variable *C* is common to
    each {*X*, *Y*} tuple in the sequence. In this case, the context variable *C*
    could represent the profile of a particular individual, and the act variable *X*
    is that user’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F15_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.15 A causal DAG illustrating contextual bandit feedback
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Alternatively, the context variable could change at each step, as in figure
    12.16.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F16_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.16 A causal DAG illustrating contextual bandit feedback where the
    context changes at each timestep
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can vary this template in different ways. For example, we could have the
    actions drive the context variables in the next timestep, as in figure 12.17\.
    The choice depends on your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F17_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.17 A causal DAG where the action at one timestep influences the context
    at the next timestep
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 12.3.3 Delayed feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a delayed-feedback setting, the outcome variable and corresponding utility
    are no longer instant feedback. Instead, they come at the end of a sequence. Let’s
    consider an example where a context variable drives the acts. The acts affect
    the next instance of the context variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F18_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.18 Example of a causal DAG for sequential decision making with delayed
    feedback
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Again, figure 12.18 shows an example of this approach based on the previous
    model. Here the act at time *k* influences the context variable (*C*) at time
    *k* + 1, which in turn affects the act at time *k* + 1.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a case of chronic pain. Here the context variable represents whether
    a subject is experiencing pain (*C*). The presence of pain drives the act of taking
    a painkiller (*X*). Taking the painkiller (or not) affects whether there is pain
    in the next step. Figure 12.19 illustrates this DAG.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F19_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.19 A causal DAG representing the treatment of chronic pain
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Y* here is the ultimate health outcome of the subject, and it is driven both
    by the overall amount of pain over time, and the amount of drugs the subject took
    (because perhaps overuse of painkillers has a detrimental health effect).'
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.4 Causal queries on a sequential model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We may want to calculate some causal query for our sequential decision problem.
    For example, given the DAG in figure 12.19, we might want to calculate the causal
    effect of *X*[0] on *U*(*Y*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-7x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or perhaps we might be interested in the causal effect of the full sequence
    of acts on *U*(*Y*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-8x.png)'
  prefs: []
  type: TYPE_IMG
- en: Either way, now that we have framed the sequential problem as a causal model,
    we are in familiar territory; we can simply use the causal inference tools we’ve
    learned in previous chapters to answer causal queries with this model.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we must be attentive to the possibility of latent causes that can
    confound our causal inference. In the case of causal effects, our concern is latent
    common confounding causes between acts (*X*) and outcomes (*Y*), or alternatively
    between acts (*X*) and utilities (*U*). Figure 12.20 is the same as figure 12.15,
    except it introduces a latent *Z* confounder.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F20_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.20 Contextual bandit with a latent confounder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similarly, we could have a unique confounder at every timestep, as in figure
    12.21.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F21_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.21 Bandit feedback with a different context and latent confounders
    at each timestep
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similarly, figure 12.22 shows a second version of the chronic pain graph where
    the confounders affect each other and the context variables. This confounder could
    be some external factor in the subject’s environment that triggers the pain and
    affects well-being.
  prefs: []
  type: TYPE_NORMAL
- en: These confounders become an issue when we want to infer the causal effect of
    a sequence of actions on *U*(*Y*).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F22_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.22 A version of the chronic pain DAG where the confounders affect
    each other and the context variables
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next, we’ll look at how we can view policies for automatic decision making in
    sequential decision-making processes as stochastic interventions.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 Policies as stochastic interventions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In automated sequential decision-making, the term “policy” is preferred to
    “decision rule.” I’ll introduce a special notation for a policy: *π*(.). It will
    be a function that takes in observed outcomes of other variables and returns an
    action.'
  prefs: []
  type: TYPE_NORMAL
- en: To consider how a policy affects the model, we’ll contrast the DAG before and
    after a policy is implemented. Figure 12.23 illustrates a simple example with
    a context variable *C* and a latent variable *Z*. The policy uses context *C*
    to select a value of *X*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F23_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.23 The dashed lines show edges modulated by the policy. The policy
    breaks the influence of the confounder *Z* like an ideal intervention, but dependence
    on *C* remains through the policy.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The policy is a type of stochastic intervention; it selects a intervention value
    for *X* from some process that depends on *C*. Like an ideal intervention, it
    changes the graph. The left of figure 12.23 shows the DAG prior to deployment
    of the policy. On the right is the DAG after the policy is deployed. I add a special
    policy node to the graph to illustrate how the policy modulates the graph. The
    dashed edges highlight edges modulated by the policy. Just like an ideal intervention,
    the policy-generated intervention removes *X*’s original incoming edges *C*→*X*
    and *Z*→*X*. However, because the policy depends on *C*, the dashed edges illustrate
    the new flow of influence from *C* to *X*.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we are interested in what value *Y* would have for a policy-selected
    action *X*=Π. In counterfactual notation, we write
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-9x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In sequence settings, the policy applies a stochastic intervention at multiple
    steps in the sequence. From a possible worlds perspective, each intervention induces
    a new hypothetical world. This can stretch the counterfactual notation a bit,
    so going forward, I’ll simplify the counterfactual notation to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-10x.png)'
  prefs: []
  type: TYPE_IMG
- en: This means *Y*[3] (*Y* at timestep 3) is under influence of the policy’s outcomes
    at times 0, 1, and 2.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.1 Examples in sequential decision-making
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of bandit feedback, the actions are produced by a *bandit algorithm*,
    which is a type of policy that incorporates the entire history of actions and
    utility outcomes in deciding the optimal current action. Though actions and outcomes
    in the bandit feedback process are independent at each time step, the policy introduces
    dependence on past actions and outcomes, as shown in figure 12.24\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F24_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.24 Bandit feedback where a bandit policy algorithm selects the next
    action based on past actions and reward outcomes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recall our previous example of an agent taking pain medication in response to
    the onset of pain. Figure 12.25 shows how a policy would take in the history of
    degree of pain and how much medication was provided.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F25_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.25 In the pain example, the policy considers the history of recorded
    levels of pain and corresponding dosages of medication.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The policy is like a doctor making the rounds on a hospital floor. They come
    to a patient’s bed, and the patient reports some level of pain. The doctor looks
    at that patient’s history of pain reports and the subsequent dosages of medication
    and uses that information to decide what dosage to provide this time. The doctor’s
    utility function is in terms of pain, risk of overdose, and risk of addiction.
    They need to consider historic data, not just the current level of pain, to optimize
    this utility function.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.2 How policies can introduce confounding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As stochastic interventions, policies introduce interventions conditional on
    other nodes in the graph. Because of this, there is a possibility that the policy
    will introduce new backdoor paths that can confound causal inferences. For example,
    consider again the DAG in figure 12.26.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F26_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.26 The policy eliminates the backdoor path through *Z* but not the
    backdoor path through *C*.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The policy breaks the backdoor path from *X* to *Y* through *Z*, but there is
    still a path from *X* to *Y* through *C*. Thus, typical causal queries involving
    *X* and *Y* would have to condition on or adjust for *C*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll characterize causal RL in causal terms.
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Causal reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning (RL) is a branch of machine learning that generally involves
    an agent learning policies that maximize cumulative reward (utility). The agent
    learns from the consequences of its actions, rather than from being explicitly
    taught, and adjusts its behavior based on the rewards or losses (reinforcements)
    it receives. Many sequential decision-making problems can be cast as RL problems.
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.1 Connecting causality and Markov decision processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RL typically casts a decision process as a Markov decision process (MDP). A
    canonical toy example of an MDP is a grid world, illustrated in figure 12.27\.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.27 presents a 3 × 4 grid world. An agent can act within this grid
    world with a fixed set of actions, moving up, down, left, and right. The agent
    wants to execute a set of actions that deliver it to the upper-right corner {0,
    3}, where it gains a reward of 100\. The agent wants to avoid the middle-right
    square {1, 3}, where it has a reward of –100 (a *loss* of 100). Position {1, 1}
    contains an obstacle the agent cannot traverse.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F27_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.27 A simple grid world
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can think of it as a game. When the game starts, the agent “spawns” randomly
    in one of the squares, except for {0, 3}, {1, 3}, and {1, 1}. When the agent moves
    into a goal square, the game ends. To win, the agent must navigate around the
    obstacle in {1, 1}, avoid {1, 3}, and reach {0, 3}.
  prefs: []
  type: TYPE_NORMAL
- en: A Markov decision process models this and much more complicated “worlds” (aka
    domains, problems, etc.) with abstractions for states, actions, transition functions,
    and rewards.
  prefs: []
  type: TYPE_NORMAL
- en: States
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: States are a set that represents the current situation or context that the agent
    is in, within its environment. In the grid-world example, a state represents the
    agent being at a specific cell. In this grid, there are 12 different states (the
    cell at {1, 1} is an unreachable state). We assume the agent has some way of knowing
    which state they are in.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll denote state as a variable *S*. In a grid world, *S* is a discrete variable,
    but in other problems, *S* could be continuous.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Actions are the things the agent can do, and they lead to a change of state.
    Some actions might not be available when in a particular state. For example, in
    the grid world, the borders of the grid are constraints on the movements of the
    agent. If the agent is in the bottom-left square {2, 0}, and they try to move
    left or down, they will stay in place. Similarly, the cell at {1, 1} is an obstacle
    the agent must navigate around. We denote actions with the variable *A*, which
    has four possible outcomes {up, down, right, left}.
  prefs: []
  type: TYPE_NORMAL
- en: Transition function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The transition function is a probability distribution function. It tells us
    the probability of moving to a specific next state, given the current state and
    the action taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'If states are discrete, the transition function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-11x.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *S**[t]*=*s* means the agent is currently in state *s*. *A**[t]*=*a* means
    the agent performs action *a*. *P*(*S**[t]**[+1]*=*s'|S**[t]*=*s, A**[t]*=*a*)
    is the probability that the agent transitions to a new state s' given it is in
    state s and performs action *a*. When the action leads to a new state with complete
    certainty, this probability distribution function becomes degenerate (all probability
    is concentrated on one value).
  prefs: []
  type: TYPE_NORMAL
- en: Rewards
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The term “reward” is preferred to “utility” in RL. In the context of MDPs, the
    reward function will always take a state *s* as an argument. We will write it
    as *U*(*s*).
  prefs: []
  type: TYPE_NORMAL
- en: In the grid-world example, *U*({0, 3}) = 100, *U*({1, 3)) = –100\. The reward
    of all other states is 0\. Note that sometimes in the MDP/RL literature, *U*()
    is a function of state and an action, as in *U*(*s*, *a*). We don’t lose anything
    by just having actions be a function of state because you can always fold actions
    into the definition of a state.
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.2 The MDP as a causal DAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Figure 12.28 shows the MDP as a causal DAG.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F28_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.28 The Markov decision process represented as a DAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As a causal DAG, the MDP looks like the other sequential decision processes
    we’ve outlined, except that we limit ourselves to states, actions, and rewards.
    In figure 12.28, the process continues until we reach a terminal state (*S*[*k*]),
    such as getting to the terminal cells in the grid-world example.
  prefs: []
  type: TYPE_NORMAL
- en: The causal Markov property and the MDP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The “Markov” in “Markov decision process” comes from the fact that the current
    state is independent of the full history of states given the last state. Contrast
    this with the causal Markov property of causal DAGs: a node in the DAG is independent
    of indirect “ancestor” causes given its direct causal parents. We can see that
    when we view the MDP as a causal DAG, this Markovian assumption is equivalent
    to the causal Markov property. That means we can use our d-separation-based causal
    reasoning, including the do-calculus, in the MDP graphical setting.'
  prefs: []
  type: TYPE_NORMAL
- en: The transition function and the causal Markov kernel
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Note that based on this DAG, the parents of a state *S*[(]*[t]*[+1)] are the
    previous state *S**[t]* and the action *A**[t]* taken when in that previous state.
    Therefore, the causal Markov kernel is *P*(*S**[t]**[+1]*=*s'|S**[t]*=*s, A**[t]*=*a*),
    i.e., the transition function. Thus, the transition function is the causal Markov
    kernel for a given state.
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.3 Partially observable MDPs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An extension of MDPs is *partially observed MDPs* (POMDPs). In a POMDP, the
    agent doesn’t know with certainty what state they are in, and they must make inferences
    about that state given incomplete evidence from their environment. This applies
    to many practical problems where the agent cannot observe the full state of the
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: A POMDP can entail different causal structures depending on our assumptions
    about the causal relationships between the unobserved and observed states. For
    example, suppose a latent state *S* is a cause of the observed state *X*. The
    observed state *X* now drives the act *A* instead of *S*. Figure 12.29 illustrates
    this formulation of a POMDP as a causal DAG.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F29_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.29 A POMDP where a latent state *S* causes an observed state *X*.
    *X* drives the actions *A*.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In contrast, figure 12.30 illustrates an example where the latent state is a
    latent common cause (denoted Z) of the observed state (mediated through the agent’s
    action) and the utility (note a slight change of notation from *U*(*S*[*i*]) to
    *U*[*i*]). Here, unobserved factors influence both the agent’s behavior and the
    resulting utility of that behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the basic MDP and POMDP DAGs should be seen as templates for starting
    our analysis. Once we understand what causal queries we are interested in answering,
    we can explicitly represent various components of observed and unobserved states
    as specific nodes in the graph, and then use identification and adjustment techniques
    to answer our causal queries.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F30_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.30 A POMPD formulation where the unobserved states are latent common
    causes that could act as confounders in causal inferences
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 12.5.4 Policy in an MDP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As before, policies in an MDP act as stochastic interventions. Figure 12.31
    illustrates a policy that selects an optimal action based on the current state
    in a way that disrupts any influence on the action from a confounder.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F31_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.31 Modification of an MDP DAG by a policy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 12.31 is simple in that it only selects an action based on the current
    state. The challenge is in the implementation, because in most RL settings, states
    can be high-dimensional objects.
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.5 Causal Bellman equation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RL is about searching for the optimal policy, which is characterized with the
    Bellman equation, often written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-12x.png)'
  prefs: []
  type: TYPE_IMG
- en: In plain words, we’re looking for a policy Π^* maximizes the cumulative reward
    over time. Here *γ* is a discount rate, a value between 0 and 1, that makes sure
    the agent values rewards in the near future more than rewards in the far future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we’re reasoning about what would happen if we deployed the policy, the
    causal formulation would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-13x.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we could do the same causal rewrite for other variants of the Bellman
    equation, such as the Q-function used in Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between the noncausal and causal formulations of the Bellman
    equation is the same as the difference between optimizing *E*(*U*(*Y*)|*X*=*x*)
    and *E*(*U*(*Y*[*X*][=][*x*])) in section 12.2.4\. The process of solving the
    causally naive version of the Bellman equation may introduce biases from latent
    confounders or from conditioning on colliders and mediators. Our causally attuned
    approach can help avoid these biases. In many cases, the solution of the naive
    approach will coincide with the causal approach because those biases might not
    affect the ranking of the top policy relative to others. However, as in the *E*(*U*(*Y*)|*X*=*x*)
    versus *E*(*U*(*Y*[*X*][=][*x*])) example, there will be cases where the solutions
    to the noncausal and causal formulations differ, and your RL problem might be
    one of those cases.
  prefs: []
  type: TYPE_NORMAL
- en: 12.6 Counterfactual reasoning for decision-theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve discussed the problem of choosing optimal actions with respect
    to a utility function as a level 2 query on the causal hierarchy. Is there a use
    for level 3 counterfactual reasoning in decision theory? In this section, we’ll
    briefly review some applications for level 3 reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 12.6.1 Counterfactual policy evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Counterfactual policy evaluation involves taking logged data from a policy in
    production and asking, “given we used this policy and got this cumulative reward,
    how much cumulative reward would we have gotten had we used a different policy?”
    See the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references to techniques such as *counterfactually guided policy search* and
    *counterfactual risk minimization*.
  prefs: []
  type: TYPE_NORMAL
- en: 12.6.2 Counterfactual regret minimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapters 8 and 9, I introduced *regret* as a counterfactual concept. We can
    further clarify the idea now that we have introduced the language of decision-making;
    regret is the difference between the utility/reward that was realized given a
    specific action or set of actions, and the utility/reward that would have been
    realized had another action or set of actions been taken.
  prefs: []
  type: TYPE_NORMAL
- en: '*Counterfactual regret minimization* is an approach to optimizing policies
    that seeks to minimize regret. To illustrate, suppose we have a policy variable
    *Π*, which can return one of several available policies. The policies take in
    the context and return an action. The action leads to some reward *U*.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose, for a single instance in our logged data, the policy was *Π*=*π* and
    the context was *C*=*c*. We get a certain action *A*=*π*(*c*) and reward *U*=*u*.
    For some policy *π**'*, regret is the answer to the counterfactual question, “How
    much more reward would we have gotten if the policy had been *π*=*π**'*?” In terms
    of expectation,
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-14x.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, this is regret for a single instance in logged data where the context
    was *C*=*c* and the utility was *u*. There are many variations, but the general
    idea is to find the policy that would have minimized cumulative regret over all
    the cases of *C*=*c* in the logged data, with the goal of favoring that policy
    in cases of *C*=*c* in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 12.6.3 Making level 3 assumptions in decision problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The question, of course, is how to make the level 3 assumptions that enable
    counterfactual inferences. One approach would be to specify an SCM and use the
    general algorithm for counterfactual reasoning (discussed in chapter 9). For example,
    in RL, the transition function *P*(*S**[t]**[+1]*=*s'|S**[t]*=*s, A**[t]*=*a*)
    captures the rules of state changes in the environment. As I mentioned, *P*(*S**[t]**[+]**[1]**|S**[t]*=*s,
    A**[t]*=*a*) is the causal Markov kernel for a given state *S**[t]**[+1]*. We
    could specify an SCM with an assignment function that entails that causal Markov
    kernel, and write that assignment function as
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch12-eqs-15x.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n*[*s*]*[']* is the value of an exogenous variable for *S**[t]*.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge is specifying assignment functions that encode the correct counterfactual
    distributions. This is easier in domains where we know more about the underlying
    causal mechanisms. A key example is in rule-based games; game rules can provide
    the level 3 constraints that enable simulation of counterfactuals. Recall how,
    in chapter 9, the simple rules of the Monte Hall problem enabled us to simulate
    counterfactual outcomes for stay versus switch strategies. Or consider multiplayer
    games like poker, where in a round of play each player is dealt a hand of cards
    and can take certain actions (check, bet, call, raise, or fold) that lead to outcomes
    (win, lose, tie) based on simple rules, which in turn determine the amount of
    chips won or lost in that round. A player’s counterfactual regret is the difference
    between the chips they netted and the most they could have netted had they decided
    on different actions. This is done while accounting for the information available
    at the time of the decision, not using hindsight about the opponents’ cards.
  prefs: []
  type: TYPE_NORMAL
- en: Counterfactual regret minimization algorithms in this domain attempt to find
    game playing policies that minimize counterfactual regret across multiple players.
    The concrete rules of the game enable simulation of counterfactual game trajectories.
    The challenge lies in searching for optimal policies within a space of possible
    counterfactual trajectories that is quite large because of multiple player interactions
    over several rounds of play. See the chapter notes on counterfactual regret minimization
    in multiagent games at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision-making is naturally a causal problem because decisions cause consequences,
    and our goal is to make the decision that leads to favorable consequences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the optimal decision is a level 2 query as we are asking “what would
    happen if I made this decision?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E*(*U*(*Y*|*X*=*x*)) and *E*(*U*(*Y*[*X*][=][*x*])) are different quantities.
    Usually, people want to know the value of *X* that optimizes *E*(*U*(*Y*[*X*][=][*x*])),
    but optimizing *E*(*U*(*Y*|*X*=*x*)) will often yield the same answer without
    the bother of specifying a causal model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially true in reinforcement learning (RL), where the analogs to
    *E*(*U*(*Y*|*X*=*x*)) and *E*(*U*(*Y*[*X*][=][*x*])) are, respectively, the conventional
    and causal formulations of the Bellman equation. Confounder, mediator, and collider
    biases may be present in conventional approaches to solving the Bellman equation.
    But those bias often don’t influence the ranking of the top policy relative to
    other policies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonetheless, sometimes the value of *X* that optimizes *E*(*U*(*Y*|*X*=*x*))
    is different from that which optimizes *E*(*U*(*Y*[*X*][=][*x*])). Similarly,
    addressing causal nuances when solving the Bellman equation may result in a different
    policy than ignoring them. If your decision problem falls into this category,
    causal approaches are the better choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newcomb’s paradox is a thought experiment meant to contrast causal and noncausal
    approaches to decision theory. The “paradox” is less mysterious once we use a
    formal causal model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal decision theory, combined with probabilistic modeling tools like Pyro
    and pgmpy, is well suited to modeling introspection, where an agent reflects on
    their internal state (feelings, intuition, urges, intent) and uses that information
    to predict the “what-if” outcomes of their decisions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we represent a sequential decision process with a causal DAG, we can employ
    all the tools of graphical causal inference in that decision problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policies operate like stochastic interventions. They change the graph but still
    have dependence on observed nodes in the past, and that dependence can introduce
    backdoor confounding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In causal RL, we can represent MDPs and POMDPs as causal DAGs and, again, make
    use of graphical causal inference theory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use template DAGs to represent sequential decision processes, but you
    should tailor these templates for your problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common use cases for counterfactual reasoning in decision theory are counterfactual
    policy evaluation and counterfactual regret minimization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have access to the rules underlying state transitions in your MDP, such
    as in physical systems or games, you could build an SCM that is counter- factually
    faithful to those rules, and use it to handle counterfactual use cases in decision-making.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
