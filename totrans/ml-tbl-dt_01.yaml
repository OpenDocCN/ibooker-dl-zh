- en: 1 Understanding tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: What tabular data is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why tabular data matters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distinction between deep learning andnon-deep learning approaches to tabular
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What people think about using deep learning with tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characteristics of tabular data that distinguish it from other kinds of data,
    like image, sound, or text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tabular data is central to our modern lives and, for most of us, to our work
    lives. Tabular data exists in spreadsheets as CSV files and in the tables of relational
    databases, it populates analysis and reports, and it can be the fuel for training
    machine learning models. Machine learning models trained on your tabular business
    can successfully solve many useful problems, such as predicting inventory requirements
    in retail outlets or predicting the price of market commodities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we introduce the process of selecting the appropriate modeling
    approach for tabular data problems. We present two main approaches: deep learning
    and classical machine learning. Then, from the data perspective, we look at some
    of the unique considerations you face when using tabular data with machine learning
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 What is tabular data?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the purposes of this book, *tabular data* is simply data that is organized
    in rows and columns. A collection of tabular data can be called a *tabular dataset*
    or a *table*. All the entries in a row are related to a common data point or an
    observation. Each row is autonomous from the other rows and completely describes
    a specific condition. The columns represent attributes for that data point, and
    they are often mentioned as variables (a more statistical term) or features (a
    term more typical of machine learning). All the entries in a column have a common
    data type, such as integer, string, or floating point number. The columns in a
    table usually have a common type.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a table that contains information about currencies used in a set of
    nations, as shown in figure 1.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_F01_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 An example of tabular data
  prefs: []
  type: TYPE_NORMAL
- en: 'Columns in this table contain values of different types:'
  prefs: []
  type: TYPE_NORMAL
- en: Country, Currency name, Currency symbol, and ISO 4217 code are all *categorical
    columns* because valid values for these columns come from a finite, relatively
    small set of values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Currency nicknames are free-form text columns because they can contain a range
    of values or no value at all, depending on the country.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Units per US Dollar is a continuous column because it contains real number values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will explore more details about the characteristics of tabular data in chapter
    2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tabular data can reside in a variety of physical formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Standalone files,* including CSV files and spreadsheet files such as Excel
    and Google Sheets files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tables in relational databases,* such as'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open-source databases, such as Postgres ([https://www.postgresql.org/](https://www.postgresql.org/))
    and MySQL ([https://www.mysql.com/](https://www.mysql.com/))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: On-premise vendor databases, such as SQL Server ([https://mng.bz/MD2W](https://mng.bz/MD2W))
    and Oracle ([https://www.oracle.com/ca-en/database/](https://www.oracle.com/ca-en/database/))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-native databases, such as Google Cloud Spanner (https://cloud.google.com/spanner),
    AWS Aurora ([https://aws.amazon.com/rds/aurora/](https://aws.amazon.com/rds/aurora/)),
    and Snowflake([https://www.snowflake.com/](https://www.snowflake.com/))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE You may have heard the term *structured data* used interchangeably with
    *tabular data*. However, these two terms are not synonymous. For example, people
    sometimes refer to data that has a degree of structure but is not tabular, such
    as nested JSON, as structured data. Structured data, also encompasses relational
    data, time series data, graph data, and spatial data, any of which might also
    be represented in tabular form. To avoid any confusion, we will use the term *tabular
    data* exclusively in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have established what tabular data is, what isn’t tabular data?
    This is an important question because the differences between tabular data and
    nontabular data help explain one of the key questions answered in this book: Are
    there situations where you should use deep learning with tabular data? The following
    are some examples of data that is not tabular:'
  prefs: []
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensor data in JSON format, such as data generated by Internet of Things devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social media streaming data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can you think of one thing all these nontabular data types have in common? If
    you answered, “They have all been very successfully used to train deep learning
    models,” you would be absolutely right. Indeed, in the last 10 years, one groundbreaking
    model after another has been created using various nontabular datasets. In this
    book, we’ll explore why deep learning hasn’t set the world of tabular data on
    fire in the same way and under what circumstances it makes sense to apply deep
    learning to tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 The world runs on tabular data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to the article “Structured vs Unstructured Data” ([https://mng.bz/5g7Z](https://mng.bz/5g7Z)),
    up to 90% of all the digital data in the world is nontabular, and the proportion
    that is nontabular is increasing every year. If this is true, then why read a
    book about applying machine learning methods to tabular data? While it’s probably
    true that only a small portion of the world’s data is tabular, this portion is
    absolutely essential. Every bank, every insurance company, every government agency,
    every retailer, every manufacturer—all of them run their core activities on tabular
    data. Such predominance is dictated, first, because its format as a table arranged
    in rows and columns makes tabular data easy to input, retrieve, manage, and analyze.
    Second, tabular data is supported by many business software and applications,
    such as spreadsheets, databases, and business intelligence tools.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to their core activities, these organizations depend on tabular
    data to monitor their progress and detect problems. As you live a modern life
    as a consumer, an employee, and a citizen, your daily activities generate updates
    in hundreds, even thousands, of tables.
  prefs: []
  type: TYPE_NORMAL
- en: For three years, one of the authors of this book had the privilege of running
    the worldwide support function for one of the largest relational database products
    in the world. Around the clock, seven days a week, this job exposed the width
    and breadth of organizations that ran on tabular data. It also exposed what happens
    when the tabular data systems fail. Shoppers across an entire continent couldn’t
    use their credit cards, trucks were backed up for miles at the border, freight
    trains stopped running, retail websites crashed on Black Friday, and factories
    creating artificial hearts ground to a halt. It is no exaggeration—the world runs
    on tabular data and on structured data in general.
  prefs: []
  type: TYPE_NORMAL
- en: Tabular data is everywhere, and it is critically important. For many of us,
    our jobs revolve around tabular data. Because of this, understanding how to efficiently
    apply machine learning (and, where appropriate, deep learning) to tabular data
    is a very useful skill. In this book, you will learn techniques to unlock the
    potential of tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Machine learning vs. deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both deep learning and classical machine learning methods aim to map input data
    to a prediction. However, they take different approaches, as deep learning methods
    have been designed to mimic the behavior of a biological brain, whereas other
    machine learning techniques are often based on statistical optimizations or similarity
    comparisons. However, apart from the different approaches taken, they also imply
    a profoundly different way to make good use of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In classical machine learning approaches, feature transformation and engineering
    are king because, no matter the model you adopt, you will always need to apply
    appropriate transformations to your data based on data characteristics and the
    knowledge domain the data comes from (Is it business data? Does it represent any
    social, economic, or physical phenomena?). The following are some reasons why
    feature engineering is so essential in classical machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Relevant information extraction*—Not all raw data is equally relevant for
    a specific task. Feature engineering helps identify and extract the most informative
    aspects of the data, discarding irrelevant or noisy parts. By focusing on the
    relevant features, the model can concentrate on learning the essential patterns,
    leading to better generalization and improved performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data representation*—Different models have different requirements in terms
    of data representation. Feature engineering allows you to convert the data into
    a suitable format that fits the model’s assumptions and limitations. This step
    guarantees that the model can learn effectively from the data and make accurate
    predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Addressing nonlinearity*—In many real-world problems, the relationships between
    the features and the target variable may not be linear. Feature engineering can
    help transform the data to address nonlinearities, making it easier for linear
    models to approximate complex relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Domain-specific knowledge*—In some cases, domain experts may have valuable
    insights about the data that can be used to engineer relevant features. Incorporating
    domain knowledge can significantly improve the model’s performance in specific
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, deep learning approaches rely on *representation learning*,
    which is their ability to internally and automatically process the data into a
    meaningful form for solving the problem at hand. Representation learning capabilities
    of deep learning models allow them to transform the data into a more compact and
    meaningful format that captures relevant features and patterns for a specific
    task. In fact, during the learning process, thanks to the fact that all input
    features interact in a nonlinear way with the others, deep learning models discover
    by themselves intricate patterns and dependencies in the data that may not be
    apparent through manual feature engineering, and they manage to develop hierarchical
    representations of the input data, starting from basic features and gradually
    building up to more complex and abstract ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, classical machine learning primarily centers on thorough and effective
    feature engineering, whereas deep learning models for tabular data are much more
    focused on the architecture of the arrangement of the layers of neurons and on
    the characteristics of individual neurons. This dichotomy constitutes a fundamental
    aspect of our book: upcoming chapters not only emphasize the distinction between
    classical machine learning and deep learning models but also introduce different
    ways to frame data problems and find solutions based on this distinction.'
  prefs: []
  type: TYPE_NORMAL
- en: While it may not be completely orthodox to simplify the terminology in such
    a way, throughout the book, we use the generic term *machine learning* or *classical
    machine learning* for all the machine learning approaches but neural networks
    and use *deep learning* for neural network-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover basic and more advanced machine learning models based on popular
    packages such as
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic machine learning models available in Scikit-learn ([https://scikit-learn.org/](https://scikit-learn.org/))
    and in GPU-specialized libraries [such as NVIDIA Rapids ([https://developer.nvidia.com/rapids](https://developer.nvidia.com/rapids))]:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized linear models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Some tree-based methods available in Scikit-learn are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging ensembles of weak predictors
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extremely randomized trees
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Histogram-based gradient-boosted approaches, including
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost eXtreme Gradient Boosting ([https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft’s LightGBM ([https://github.com/Microsoft/LightGBM](https://github.com/Microsoft/LightGBM))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: HistGradientBoosting from Scikit-learn
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For deep learning, we will cover a range of architectures that are effective
    with tabular data implemented in TensorFlow or PyTorch deep learning frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: Shallow networks with categorical embeddings (directly implemented using one
    of the available deep learning frameworks)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fastai tabular ([https://docs.fast.ai/tabular.model.html](https://docs.fast.ai/tabular.model.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch Tabular ([https://github.com/manujosephv/pytorch_tabular](https://github.com/manujosephv/pytorch_tabular))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TabNet ([https://arxiv.org/abs/1908.07442](https://arxiv.org/abs/1908.07442))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAINT ([https://arxiv.org/abs/2106.01342](https://arxiv.org/abs/2106.01342))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepTables ([https://github.com/DataCanvasIO/deeptables](https://github.com/DataCanvasIO/deeptables))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will see this distinction between machine learning and deep learning throughout
    this book as we explore both approaches and advise when to use each approach to
    solve tabular data problems.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 What makes tabular data different?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know that deep learning approaches dominate solving problems involving many
    types of data that we could define as “nontabular” or “unstructured” because of
    their great variety of characteristics, sizes, and modalities that you cannot
    constrain in a rows/columns data model. Typical examples of unstructured data
    that have been successfully tackled by deep learning are
  prefs: []
  type: TYPE_NORMAL
- en: Audio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, contrary to structured tabular data problems, you don’t have anything
    near the typical matrix-shaped format but different files or instances containing
    a multitude of information in an unordered way. Before deep learning revolutionized
    the way unstructured data is modeled, unstructured data that we wanted to use
    for a predictive model had to be brought back into a structured data format by
    carefully creating well-defined and specific features (a procedure called feature
    engineering). For each type of unstructured data problem, researchers and practitioners
    took years to find the best features to be extracted from the data to feed a machine
    learning model and then obtain satisfactory predictive results.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to their representational power, deep learning models can handle all
    the necessary transformations to turn unstructured data into a viable prediction,
    in an end-to-end fashion, directly from input to solution. Given this background,
    you might expect deep learning models to be even more effective on tabular data,
    but this has not been the case up to now.
  prefs: []
  type: TYPE_NORMAL
- en: There are, in truth, various reasons that can explain the challenge that deep
    learning faces with tabular data problems. The first reason involves the actual
    directions of academic research and private investment in new technologies and
    methodologies. As we mentioned, in the past, researchers spent time and effort
    finding the best way to turn unstructured data into structured data to fit the
    machine learning paradigm of the time. Nowadays, the same efforts are spent on
    advancing deep learning, concentrating particularly on unstructured data because
    it is more easily available in public repositories and more “uniform” than tabular
    data, thus bringing more research success.
  prefs: []
  type: TYPE_NORMAL
- en: Image repositories such as ImageNet ([https://image-net.org/index.php](https://image-net.org/index.php))
    and open text corpora such as Wikipedia or the Common Crawl’s web archive ([https://commoncrawl.org/](https://commoncrawl.org/))
    are easily available to both academic researchers and practitioners to train or
    refine their deep learning models. As for tabular data, there is no equivalent
    in terms of a common open-source data repository. On the contrary, tabular data
    is dispersed into a multitude of private databases, each one showing an even higher
    degree of variability than unstructured data because each database has its own
    data collection rules and structure of features.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the fact that open-source tabular datasets representing real-world
    business problems are generally harder to find, you must also consider a second
    reason. Open-source tabular datasets are usually smaller in size and often quite
    different from the data that is owned privately by businesses and governments.
    Consequently, the lack of data usually causes neural networks to underperform.
    In addition, there is no golden rule to benchmark one’s progress because using
    a particular kind of data is limited to a specific problem in the vast domain
    of tabular data problems. For any researcher, it is much more challenging to generalize
    best practices starting from a tabular dataset, or even a limited choice of them,
    than to do the same using images, audio, or texts that are universally available
    and accepted as a reference benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Being difficult to access and extremely varied in the type of information they
    contain, tabular datasets present a further limitation for deep learning solutions:
    you cannot think of any pretrained solutions because you cannot get a hold of
    all the kinds of tabular problems. Once you develop a deep learning model for
    images and text problems, you can make it available to the public and expect other
    academics or practitioners to find it useful for their problem after tweaking
    it a bit. This is technically called *transfer learning* because you can successfully
    apply, with a limited additional modeling effort, the deep learning network trained
    on a problem to another similar task. Such an opportunity has strongly driven
    the diffusion of deep learning models in their pretrained form in recent years.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, the lack of generalizable tabular examples, a great variety
    of kinds of tabular data, and more attention from academics on unstructured data
    have led to feature engineering playing a different role between machine learning
    and deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, feature engineering can yield much more predictive power
    for tabular data than algorithms themselves, and it is commonly regarded more
    as an art than a science.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In deep learning, on the contrary, academics and practitioners tend to rely
    too much on representation learning and let the network deal with everything instead
    of using feature engineering themselves and demonstrating how deep learning, given
    the same data framework as a machine learning algorithm, can learn a solution
    in a different and useful way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Truly, as demonstrated by recent studies, tabular data characteristics such
    as redundant features, skewed distributions, and irregular patterns of the prediction
    target pose a challenge to neural networks. We will discuss this in more detail
    in chapter 5, when dealing specifically with the gradient boosting models. All
    the same, we assert that both machine learning and deep learning models are viable
    ways to solve tabular data problems, and deep learning will grow in importance
    as practitioners and researchers put more effort into testing architectures and
    solutions on more realistic tabular data than those that are easily available
    today.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Generative AI and tabular data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI—in particular, large language models (LLMs)—are capable assistants
    in various tasks related to text production and processing. Generally speaking,
    LLMs have proven to be a breakthrough solution for a certain range of tasks, such
    as
  prefs: []
  type: TYPE_NORMAL
- en: '*Generation*—Generate text such as the next token, words to complete a phrase,
    up to generate a text from an instructional prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Extraction*—Named entity recognition, sentence segmentation, keyword extraction,
    topic modeling, named entity recognition, semantic relationship extraction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Classification*—Language, intention, sentiment, semantics, and even tricky
    problems such as sarcasm, irony, negation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transformation* of the text—Translations, corrections, style modifications,
    paraphrasing, summarization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Comprehension*—Leading to question and answering, reasoning, knowledge completion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many of these tasks can extend to the work of a data scientist or a data engineer.
    LLMs can support the user in activities such as feature engineering, coding functions,
    and visualization instructions (for instance, using commands from the matplotlib
    package), providing analytical advice, helping interpret results, and presenting
    them synthetically for charts and reports. One notable practical application of
    LLMs in tabular data is automating textual data-related tasks. When dealing with
    textual variable fields, they can engineer new features by summarizing, categorizing,
    and identifying key themes. They can also develop the code to process the same
    text in Python, for instance, by creating functions or figuring out the correct
    regular expression for text processing.
  prefs: []
  type: TYPE_NORMAL
- en: Besides supporting the user and being a useful assistant, LLMs can also play
    a more direct and active role in analytics. Recent applications for ChatGPT (the
    Advanced Data Analytics API) also provide direct data analysis in CSV format,
    followed by other data-related tasks, including summarization, preprocessing,
    analysis, visualization, and report generation. At each step, the tool can provide
    the Python code to execute and obtain the same results, it runs the code for you
    and provides some visualizations in charts and tables. This aligns with the expected
    capabilities for TableGPT or other tools such as MediTab. TableGPT ([https://arxiv.org/pdf/2307.08674.pdf](https://arxiv.org/pdf/2307.08674.pdf))
    is a new framework that utilizes LLMs to enhance human interaction with tabular
    data. It enables users to interact with tables using commands expressed in natural
    language and perform various tasks such as question answering, data manipulation,
    data visualization, report generation, and automated prediction. MediTab ([https://arxiv.org/pdf/2305.12081.pdf](https://arxiv.org/pdf/2305.12081.pdf)),
    instead, works on medical tabular data by consolidating tabular samples, aligning
    out-domain data with the target task, and expanding the training data. Faced with
    prediction tasks based on textual data, it even demonstrated performances superior
    to classical machine learning algorithms such as XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, LLMs do not offer comparable performance in prediction tasks
    on tabular data, as demonstrated by the TABLET benchmark ([https://arxiv.org/pdf/2304.13188.pdf](https://arxiv.org/pdf/2304.13188.pdf)).
    In assessing LLMs’ performance relative to fully supervised models, the paper
    compared Flan-T5 11b and ChatGPT using 4-shot examples compared with XGBoost trained
    on the entire dataset. The XGBoost model, when applied to all the data, achieved
    an average F1 score of 0.94 on the prediction tasks. In contrast, ChatGPT averaged
    a score of 0.68, and Flan-T5 11b achieved a score of 0.66 using the F1 score.
    This analysis highlights that there is still a significant margin for improvement
    in predictive tasks for LLMs involving tabular data with multimodal types (text
    and numbers) and that such tools continue to excel in executing instructions,
    particularly when working with textual inputs and producing textual outputs. A
    tool such as llm-classifier ([https://github.com/lamini-ai/llm-classifier](https://github.com/lamini-ai/llm-classifier))
    works and surprises because it can use the information already contained in the
    used LLM but cannot acquire much additional information typical of tabular problems.
  prefs: []
  type: TYPE_NORMAL
- en: In sum, generative AI is not yet the solution when dealing with tabular data,
    not only due to performance reasons but also for other crucial aspects, such as
  prefs: []
  type: TYPE_NORMAL
- en: '*Cost*—Generative AI models often require intensive GPU resources, leading
    to higher operational costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalability*—The resource-intensive nature of generative AI models, particularly
    their reliance on GPUs, can hinder scalability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Latency and throughput*—Larger models tend to increase processing time per
    request, affecting latency and throughput.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bias*—Generative AI models may inherit biases from the data they were trained
    on, potentially perpetuating or amplifying existing biases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Flexibility*—Adapting generative AI models to custom tasks often necessitates
    extensive retraining, limiting their flexibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Determinism*—The inherent complexity of generative AI models can make it challenging
    to control and predict their output, affecting determinism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Explainability*—The complexity of generative AI models can hinder explainability,
    making it difficult to understand how they operate and produce results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acknowledging such present limitations for generative AI to handle tabular data
    problems, we will focus on the core classical machine learning and deep learning
    techniques for learning from tabular data and on how to prepare this data for
    analysis correctly and properly. However, we will also reserve some space to deal
    with generative AI tools such as ChatGPT, Google Gemini, and Gemini for Google
    Cloud because we recognize generative AI’s transformative force in the tabular
    data field. Based on our experience in the field, we do not foresee any replacement
    by LLMs or other generative tools of the classical machine learning algorithms
    or the deep learning architecture specialized in tabular data because of the advantages
    that such consolidated tools offer, both in terms of performance and control.
    Instead, we recognize how LLMs and other generative AI models can support and
    enhance tabular data processing, analysis, and modeling, helping practitioners
    become more proficient and performative in their tabular data projects.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tabular data is data organized in rows and columns, such as data in CSV files
    or relational database tables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured data is sometimes used as an alternative term for tabular data, but
    it is a broader concept, including JSON formatted data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tabular data makes up a small portion of all the digital data in the world,
    but it has an enormous effect on our lives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to other types of data (e.g., images, video, text, audio), the type
    of data that most jobs revolve around is tabular data, so learning how to efficiently
    apply machine learning/deep learning to tabular data is a useful skill that many
    people can apply to their jobs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book, we simply refer to machine learning approaches excluding neural
    networks (going from linear regressions to gradient boosting methods) as *classical
    machine learning* or just *machine learning* to distinguish them from deep learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to deep learning with other types of data (e.g., images, video, text,
    audio), deep learning with tabular data gets little attention from academic researchers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conventional wisdom is to use a gradient boosting approach like XGBoost with
    tabular data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s a lively debate in social media about whether or not there is a place
    for deep learning in solving problems involving tabular data. In this book, we
    don’t pick a side in this debate. Instead, we try to objectively describe why
    you would use machine learning or deep learning for a given tabular data problem
    and the best practices to use for each approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tabular data has some unique characteristics not shared by other types of data,
    such as images, video, or text. These characteristics include a lack of large
    open-source datasets that represent the kinds of tabular datasets that you would
    see in real-world business problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI, especially LLMs, significantly affects how AI is perceived, diffused
    across individuals and organizations, and utilized. LLMs can help automate various
    tasks related to tabular data analysis and modeling, especially when related to
    textual inputs and outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
