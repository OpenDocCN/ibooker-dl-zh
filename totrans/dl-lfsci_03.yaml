- en: Chapter 3\. Machine Learning with DeepChem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章。使用DeepChem进行机器学习
- en: This chapter provides a brief introduction to machine learning with DeepChem,
    a library built on top of the TensorFlow platform to facilitate the use of deep
    learning in the life sciences. DeepChem provides a large collection of models,
    algorithms, and datasets that are suited to applications in the life sciences.
    In the remainder of this book, we will use DeepChem to perform our case studies.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了使用DeepChem进行机器学习的内容，DeepChem是建立在TensorFlow平台之上的库，旨在促进在生命科学领域中使用深度学习。DeepChem提供了大量适用于生命科学应用的模型、算法和数据集。在本书的其余部分，我们将使用DeepChem来进行案例研究。
- en: Why Not Just Use Keras, TensorFlow, or PyTorch?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么不直接使用Keras、TensorFlow或PyTorch？
- en: This is a common question. The short answer is that the developers of these
    packages focus their attention on supporting certain types of use cases that prove
    useful to their core users. For example, there’s extensive support for image processing,
    text handling, and speech analysis. But there’s often not a similar level of support
    in these libraries for molecule handling, genetic datasets, or microscopy datasets.
    The goal of DeepChem is to give these applications first-class support in the
    library. This means adding custom deep learning primitives, support for needed
    file types, and extensive tutorials and documentation for these use cases.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个常见问题。简短的答案是，这些软件包的开发人员专注于支持对他们的核心用户有用的某些类型的用例。例如，对于图像处理、文本处理和语音分析有广泛的支持。但是在这些库中通常没有类似的支持来处理分子、基因数据集或显微镜数据集。DeepChem的目标是为这些应用程序提供一流的支持。这意味着添加自定义的深度学习原语、支持所需的文件类型，以及为这些用例提供广泛的教程和文档。
- en: DeepChem is also designed to be well integrated with the TensorFlow ecosystem,
    so you should be able to mix and match DeepChem code with your other TensorFlow
    application code.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: DeepChem还设计为与TensorFlow生态系统很好地集成，因此您应该能够将DeepChem代码与其他TensorFlow应用程序代码混合使用。
- en: In the rest of this chapter, we will assume that you have DeepChem installed
    on your machine and that you are ready to run the examples. If you don’t have
    DeepChem installed, never fear. Just head over to the [DeepChem website](https://deepchem.io/)
    and follow the installation directions for your system.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将假设您已经在您的计算机上安装了DeepChem，并且准备运行示例。如果您尚未安装DeepChem，不要担心。只需访问[DeepChem网站](https://deepchem.io/)，并按照您系统的安装说明进行操作。
- en: Windows Support for DeepChem
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepChem对Windows的支持
- en: At present, DeepChem doesn’t support installation on Windows. If possible, we
    recommend that you work through the examples in this book using a Mac or Linux
    workstation. We have heard from our users that DeepChem works on the Windows Subsystem
    for Linux (WSL) in more modern Windows distributions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，DeepChem不支持在Windows上安装。如果可能的话，我们建议您使用Mac或Linux工作站来完成本书中的示例。我们从用户那里得知，DeepChem可以在更现代的Windows发行版中的Windows子系统Linux（WSL）上运行。
- en: If it’s not feasible for you to get access to a Mac or Linux machine or work
    with WSL, we’d love to have your help getting Windows support for DeepChem. Please
    contact the authors with the specific issues you’re seeing, and we will try to
    address them. Our hope is to remove this restriction in a future edition of the
    book and support Windows for future readers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您无法获得Mac或Linux机器的访问权限，或者无法使用WSL，我们很乐意帮助您获得DeepChem在Windows上的支持。请联系作者，告诉我们您遇到的具体问题，我们将尽力解决。我们希望在未来版本的书中取消这一限制，并为未来的读者提供对Windows的支持。
- en: DeepChem Datasets
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepChem数据集
- en: 'DeepChem uses the basic abstraction of the`Dataset` object to wrap the data
    it uses for machine learning. A `Dataset` contains the information about a set
    of samples: the input vectors `x`, the target output vectors `y`, and possibly
    other information such as a description of what each sample represents. There
    are subclasses of `Dataset` corresponding to different ways of storing the data.
    The `NumpyDataset` object in particular serves as a convenient wrapper for NumPy
    arrays and will be used extensively. In this section, we will walk through a simple
    code case study of how to use `NumpyDataset`. All of this code can be entered
    in the interactive Python interpreter; where appropriate, the output is shown.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: DeepChem使用`Dataset`对象的基本抽象来封装用于机器学习的数据。`Dataset`包含有关一组样本的信息：输入向量`x`、目标输出向量`y`，以及可能包括每个样本表示的描述等其他信息。有不同方式存储数据的`Dataset`的子类。特别是，`NumpyDataset`对象作为NumPy数组的便捷包装器，并将被广泛使用。在本节中，我们将演示如何使用`NumpyDataset`进行一个简单的代码案例研究。所有这些代码都可以在交互式Python解释器中输入；在适当的情况下，输出将显示出来。
- en: 'We start with some simple imports:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一些简单的导入开始：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s now construct some simple NumPy arrays:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在构建一些简单的NumPy数组：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This dataset will have four samples. The array `x` has five elements (“features”)
    for each sample, and `y` has one element for each sample. Let’s take a quick look
    at the actual arrays we’ve sampled (note that when you run this code locally,
    you should expect to see different numbers since your random seed will be different):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集将有四个样本。数组`x`对于每个样本有五个元素（“特征”），而`y`对于每个样本有一个元素。让我们快速查看我们抽样的实际数组（请注意，当您在本地运行此代码时，您应该期望看到不同的数字，因为您的随机种子将是不同的）：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s now wrap these arrays in a `NumpyDataset` object:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在将这些数组封装在一个`NumpyDataset`对象中：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can unwrap the `dataset` object to get at the original arrays that we stored
    inside:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以解开`dataset`对象，以获取我们存储在其中的原始数组：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Note that these arrays are the same as the original arrays `x` and `y`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些数组与原始数组`x`和`y`相同：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Other Types of Datasets
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他类型的数据集
- en: DeepChem has support for other types of `Dataset` objects, as mentioned previously.
    These types primarily become useful when dealing with larger datasets that can’t
    be entirely stored in computer memory. There is also integration for DeepChem
    to use TensorFlow’s `tf.data` dataset loading utilities. We will touch on these
    more advanced library features as we need them.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: DeepChem支持其他类型的`Dataset`对象，如前所述。当处理无法完全存储在计算机内存中的大型数据集时，这些类型主要变得有用。DeepChem还集成了使用TensorFlow的`tf.data`数据集加载工具的功能。我们将在需要时涉及这些更高级的库功能。
- en: Training a Model to Predict Toxicity of Molecules
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个模型来预测分子的毒性
- en: 'In this section, we will demonstrate how to use DeepChem to train a model to
    predict the toxicity of molecules. In a later chapter, we will explain how toxicity
    prediction for molecules works in much greater depth, but in this section, we
    will treat it as a black-box example of how DeepChem models can be used to solve
    machine learning challenges. Let’s start with a pair of needed imports:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将演示如何使用DeepChem来训练一个模型来预测分子的毒性。在后面的章节中，我们将更深入地解释分子毒性预测的工作原理，但在这一部分，我们将把它作为一个黑盒示例，展示DeepChem模型如何用于解决机器学习挑战。让我们从一对必要的导入开始：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The next step is loading the associated toxicity datasets for training a machine
    learning model. DeepChem maintains a module called `dc.molnet` (short for MoleculeNet)
    that contains a number of preprocessed datasets for use in machine learning experimentation.
    In particular, we will make use of the `dc.molnet.load_tox21()` function, which
    will load and process the Tox21 toxicity dataset for us. When you run these commands
    for the first time, DeepChem will process the dataset locally on your machine.
    You should expect to see processing notes like the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是加载用于训练机器学习模型的相关毒性数据集。DeepChem维护一个名为`dc.molnet`（MoleculeNet的缩写）的模块，其中包含一些用于机器学习实验的预处理数据集。特别是，我们将使用`dc.molnet.load_tox21()`函数，它将为我们加载和处理Tox21毒性数据集。当您第一次运行这些命令时，DeepChem将在您的计算机上本地处理数据集。您应该期望看到如下的处理说明：
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The process of *featurization* is how a dataset containing information about
    molecules is transformed into matrices and vectors for use in machine learning
    analyses. We will explore this process in greater depth in subsequent chapters.
    Let’s start here, though, by taking a quick peek at the data we’ve processed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征化*的过程是将包含有关分子信息的数据集转换为矩阵和向量，以便在机器学习分析中使用。我们将在后续章节中更深入地探讨这个过程。不过，让我们从这里开始，快速查看我们处理过的数据。'
- en: 'The `dc.molnet.load_tox21()` function returns multiple outputs: `tox21_tasks`,
    `tox21_datasets`, and `transformers`. Let’s briefly take a look at each:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`dc.molnet.load_tox21()`函数返回多个输出：`tox21_tasks`、`tox21_datasets`和`transformers`。让我们简要地看一下每一个：'
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Each of the 12 tasks here corresponds with a particular biological experiment.
    In this case, each of these tasks is for an *enzymatic assay* which measures whether
    the molecules in the Tox21 dataset bind with the *biological target* in question.
    The terms `NR-AR` and so on correspond with these targets. In this case, each
    of these targets is a particular enzyme believed to be linked to toxic responses
    to potential therapeutic molecules.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的12个任务中的每一个对应于一个特定的生物实验。在这种情况下，这些任务中的每一个都是针对*酶活性测定*的，该测定衡量了Tox21数据集中的分子是否与所讨论的*生物靶标*结合。诸如`NR-AR`等术语对应于这些靶标。在这种情况下，这些靶标中的每一个都是一种被认为与潜在治疗分子的毒性反应相关联的特定酶。
- en: How Much Biology Do I Need to Know?
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我需要了解多少生物学知识？
- en: For computer scientists and engineers entering the life sciences, the array
    of biological terms can be dizzying. However, it’s not necessary to have a deep
    understanding of biology in order to begin making an impact in the life sciences.
    If your primary background is in computer science, it can be useful to try understanding
    biological systems in terms of computer scientific analogues. Imagine that cells
    or animals are complex legacy codebases that you have no control over. As an engineer,
    you have a few experimental measurements of these systems (assays) which you can
    use to gain some understanding of the underlying mechanics. Machine learning is
    an extraordinarily powerful tool for understanding biological systems since learning
    algorithms are capable of extracting useful correlations in a mostly automatic
    fashion. This allows even biological beginners to sometimes find deep biological
    insights.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于进入生命科学领域的计算机科学家和工程师来说，生物学术语的范围可能令人眼花缭乱。然而，并不需要深入了解生物学就能开始在生命科学领域产生影响。如果您的主要背景是计算机科学，尝试以计算机科学的类比方式理解生物系统可能会有所帮助。想象细胞或动物是您无法控制的复杂遗留代码库。作为工程师，您有一些关于这些系统（测定）的实验性测量数据，可以用来对底层机制有一些了解。机器学习是理解生物系统的一种非常强大的工具，因为学习算法能够以大多数自动的方式提取有用的相关性。这使得即使是生物学初学者有时也能发现深刻的生物洞察。
- en: In the remainder of this book, we discuss basic biology in brief asides. These
    notes can serve as entry points into the vast biological literature. Public references
    such as Wikipedia often contain a wealth of useful information, and can help bootstrap
    your biological education.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我们会简要讨论基础生物学。这些说明可以作为进入广阔生物学文献的入口点。公共参考资料，如维基百科，通常包含大量有用的信息，可以帮助启动您的生物学教育。
- en: 'Next, let’s consider `tox21_datasets`. The use of the plural is a clue that
    this field is actually a tuple containing multiple`dc.data.Dataset` objects:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们考虑`tox21_datasets`。使用复数形式的提示表明，这个字段实际上是一个包含多个`dc.data.Dataset`对象的元组：
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this case, these datasets correspond to the training, validation, and test
    sets you learned about in the previous chapter. You might note that these are
    `DiskDataset` objects; the `dc.molnet` module caches these datasets on your disk
    so that you don’t need to repeatedly refeaturize the Tox21 dataset. Let’s split
    up these datasets correctly:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这些数据集对应于您在上一章中了解的训练、验证和测试集。您可能注意到这些是`DiskDataset`对象；`dc.molnet`模块会将这些数据集缓存在您的磁盘上，这样您就不需要反复对Tox21数据集进行特征化。让我们正确地拆分这些数据集：
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When dealing with new datasets, it’s very useful to start by taking a look
    at their shapes. To do so, inspect the `shape` attribute:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 处理新数据集时，首先查看它们的形状非常有用。要这样做，请检查`shape`属性：
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `train_dataset` contains a total of 6,264 samples, each of which has an
    associated feature vector of length 1,024\. Similarly, `valid_dataset` and `test_dataset`contain
    respectively 783 and 784 samples. Let’s now take a quick look at the `y` vectors
    for these datasets:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_dataset`包含总共6,264个样本，每个样本都有一个长度为1,024的相关特征向量。同样，`valid_dataset`和`test_dataset`分别包含783和784个样本。现在让我们快速查看这些数据集的`y`向量：'
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There are 12 data points, also known as *labels*, for each sample. These correspond
    to the 12 tasks we discussed earlier. In this particular dataset, the samples
    correspond to molecules, the tasks correspond to biochemical assays, and each
    label is the result of a particular assay on a particular molecule. Those are
    what we want to train our model to predict.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本有12个数据点，也称为*标签*。这些对应于我们之前讨论的12个任务。在这个特定的数据集中，样本对应于分子，任务对应于生化测定，每个标签是特定分子上特定测定的结果。这些是我们想要训练模型来预测的内容。
- en: 'There’s a complication, however: the actual experimental dataset for Tox21
    did not test every molecule in every biological experiment. That means that some
    of these labels are meaningless placeholders. We simply don’t have any data for
    some properties of some molecules, so we need to ignore those elements of the
    arrays when training and testing the model.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个复杂之处：Tox21的实际实验数据集并没有测试每个生物实验中的每种分子。这意味着一些标签是没有意义的占位符。对于一些分子的一些属性，我们根本没有数据，因此在训练和测试模型时需要忽略这些数组的元素。
- en: 'How can we find which labels were actually measured? We can check the dataset’s
    `w` field, which records its *weights*. Whenever we compute the loss function
    for a model, we multiply by `w` before summing over tasks and samples. This can
    be used for a few purposes, one being to flag missing data. If a label has a weight
    of 0, that label does not affect the loss and is ignored during training. Let’s
    do some digging to find how many labels have actually been measured in our datasets:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何找出哪些标签实际上被测量了？我们可以检查数据集的`w`字段，记录其*权重*。每当我们为模型计算损失函数时，我们在对任务和样本求和之前乘以`w`。这可以用于几个目的，其中一个是标记缺失数据。如果一个标签的权重为0，则该标签不会影响损失，并且在训练过程中会被忽略。让我们深入挖掘一下，找出我们的数据集中实际测量了多少个标签：
- en: '[PRE13]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Of the 6,264 × 12 = 75,168 elements in the array of labels, only 62,166 were
    actually measured. The other 13,002 correspond to missing measurements and should
    be ignored. You might ask, then, why we still keep such entries around. The answer
    is mainly for convenience; irregularly shaped arrays are much harder to reason
    about and deal with in code than regular matrices with an associated set of weights.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在标签数组中的6,264×12 = 75,168个元素中，只有62,166个实际测量过。其他13,002个对应于缺失的测量值，应该被忽略。您可能会问，为什么我们仍然保留这样的条目。答案主要是为了方便；不规则形状的数组比带有一组权重的常规矩阵更难在代码中进行推理和处理。
- en: Processing Datasets Is Challenging
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据集具有挑战性
- en: It’s important to note here that cleaning and processing a dataset for use in
    the life sciences can be extremely challenging. Many raw datasets will contain
    systematic classes of errors. If the dataset in question has been constructed
    from an experiment conducted by an external organization (a contract research
    organization, or CRO), it’s quite possible that the dataset will be systematically
    wrong. For this reason, many life science organizations maintain scientists in-house
    whose job it is to verify and clean such datasets.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要注意的是，为了在生命科学中使用，清理和处理数据集可能非常具有挑战性。许多原始数据集将包含系统性的错误类别。如果所讨论的数据集是由外部组织（合同研究机构或CRO）进行的实验构建的，那么该数据集很可能是系统性错误的。因此，许多生命科学组织都会保留内部的科学家，他们的工作是验证和清理这些数据集。
- en: In general, if your machine learning algorithm isn’t working for a life science
    task, there’s a significant chance that the root cause stems not from the algorithm
    but from systematic errors in the source of data that you’re using.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果您的机器学习算法在生命科学任务中无法正常工作，很可能根本原因不是算法本身，而是您使用的数据源中存在的系统性错误。
- en: 'Now let’s examine `transformers`, the final output that was returned by `load_tox21()`.
    A *transformer* is an object that modifies a dataset in some way. DeepChem provides
    many transformers that manipulate data in useful ways. The data-loading routines
    found in MoleculeNet always return a list of transformers that have been applied
    to the data, since you may need them later to “untransform” the data. Let’s see
    what we have in this case:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检查`transformers`，这是`load_tox21()`返回的最终输出。*转换器*是一种以某种方式修改数据集的对象。DeepChem提供许多可以以有用方式操作数据的转换器。在MoleculeNet中找到的数据加载例程总是返回已应用于数据的转换器列表，因为您可能以后需要它们来“取消转换”数据。让我们看看这种情况下有什么：
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, the data has been transformed with a `BalancingTransformer`. This class
    is used to correct for unbalanced data. In the case of Tox21, most molecules do
    not bind to most of the targets. In fact, over 90% of the labels are 0\. That
    means a model could trivially achieve over 90% accuracy simply by always predicting
    0, no matter what input it was given. Unfortunately, that model would be completely
    useless! Unbalanced data, where there are many more training samples for some
    classes than others, is a common problem in classification tasks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的数据已经通过 `BalancingTransformer` 进行了转换。这个类用于纠正不平衡的数据。在 Tox21 的情况下，大多数分子不与大多数目标结合。事实上，超过
    90% 的标签是 0。这意味着一个模型可以轻松地通过始终预测 0 来获得超过 90% 的准确率，无论输入是什么。不幸的是，那个模型将是完全无用的！在分类任务中，不平衡的数据，即某些类别的训练样本比其他类别多得多，是一个常见问题。
- en: 'Fortunately, there is an easy solution: adjust the dataset’s matrix of weights
    to compensate. `BalancingTransformer` adjusts the weights for individual data
    points so that the total weight assigned to every class is the same. That way,
    the loss function has no systematic preference for any one class. The loss can
    only be decreased by learning to correctly distinguish between classes.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个简单的解决方案：调整数据集的权重矩阵以进行补偿。`BalancingTransformer` 调整单个数据点的权重，使得分配给每个类别的总权重相同。这样，损失函数对任何一个类别都没有系统偏好。损失只能通过学会正确区分类别来减少。
- en: Now that we’ve explored the Tox21 datasets, let’s start exploring how we can
    train models on these datasets. DeepChem’s `dc.models` submodule contains a variety
    of different life science–specific models. All of these various models inherit
    from the parent class `dc.models.Model`. This parent class is designed to provide
    a common API that follows common Python conventions. If you’ve used other Python
    machine learning packages, you should find that many of the `dc.models.Model`
    methods look quite familiar.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了 Tox21 数据集，让我们开始探索如何在这些数据集上训练模型。DeepChem 的 `dc.models` 子模块包含各种不同的生命科学特定模型。所有这些不同的模型都继承自父类
    `dc.models.Model`。这个父类旨在提供一个遵循常见 Python 约定的通用 API。如果您使用过其他 Python 机器学习包，您应该会发现许多
    `dc.models.Model` 方法看起来非常熟悉。
- en: 'In this chapter, we won’t really dig into the details of how these models are
    constructed. Rather, we will just provide an example of how to instantiate a standard
    DeepChem model, `dc.models.MultitaskClassifier`. This model builds a fully connected
    network (an MLP) that maps input features to multiple output predictions. This
    makes it useful for *multitask* problems, where there are multiple labels for
    every sample. It’s well suited for our Tox21 datasets, since we have a total of
    12 different assays we wish to predict simultaneously. Let’s see how we can construct
    a `MultitaskClassifier` in DeepChem:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不会深入探讨这些模型是如何构建的细节。相反，我们将提供一个实例，展示如何实例化一个标准的 DeepChem 模型，`dc.models.MultitaskClassifier`。这个模型构建了一个全连接网络（MLP），将输入特征映射到多个输出预测。这对于
    *多任务* 问题非常有用，其中每个样本有多个标签。它非常适合我们的 Tox21 数据集，因为我们有 12 个不同的检测任务需要同时预测。让我们看看如何在 DeepChem
    中构建一个 `MultitaskClassifier`：
- en: '[PRE15]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There are a variety of different options here. Let’s briefly review them. `n_tasks`
    is the number of tasks, and `n_features` is the number of input features for each
    sample. As we saw earlier, the Tox21 dataset has 12 tasks and 1,024 features for
    each sample. `layer_sizes` is a list that sets the number of fully connected hidden
    layers in the network, and the width of each one. In this case, we specify that
    there is a single hidden layer of width 1,000.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有各种不同的选项。让我们简要回顾一下。`n_tasks` 是任务的数量，`n_features` 是每个样本的输入特征数量。正如我们之前看到的，Tox21
    数据集有 12 个任务和每个样本 1,024 个特征。`layer_sizes` 是一个设置网络中完全连接隐藏层数量和每个隐藏层宽度的列表。在这种情况下，我们指定有一个宽度为
    1,000 的单隐藏层。
- en: 'Now that we’ve constructed the model, how can we train it on the Tox21 datasets?
    Each `Model` object has a `fit()` method that fits the model to the data contained
    in a `Dataset` object. Fitting our `MultitaskClassifier` object is then a simple
    call:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了模型，我们如何在 Tox21 数据集上训练它呢？每个 `Model` 对象都有一个 `fit()` 方法，用于将模型拟合到包含在 `Dataset`
    对象中的数据中。然后，对我们的 `MultitaskClassifier` 对象进行拟合是一个简单的调用：
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that we added on a flag here. `nb_epoch=10` says that 10 epochs of gradient
    descent training will be conducted. An *epoch* refers to one complete pass through
    all the samples in a dataset. To train a model, you divide the training set into
    batches and take one step of gradient descent for each batch. In an ideal world,
    you would reach a well-optimized model before running out of data. In practice,
    there usually isn’t enough training data for that, so you run out of data before
    the model is fully trained. You then need to start reusing data, making additional
    passes through the dataset. This lets you train models with smaller amounts of
    data, but the more epochs you use, the more likely you are to end up with an overfit
    model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里添加了一个标志。`nb_epoch=10` 表示将进行 10 个梯度下降训练周期。一个 *epoch* 指的是对数据集中所有样本进行一次完整遍历。为了训练模型，您将训练集分成批次，并对每个批次进行一步梯度下降。在理想情况下，您会在数据用尽之前达到一个良好优化的模型。实际上，通常没有足够的训练数据，所以在模型完全训练之前就用尽了数据。然后您需要开始重复使用数据，对数据集进行额外的遍历。这样可以使用更少的数据训练模型，但使用的
    epoch 越多，最终得到过拟合模型的可能性就越大。
- en: 'Let’s now evaluate the performance of the trained model. In order to evaluate
    how well a model works, it is necessary to specify a metric. The DeepChem class
    `dc.metrics.Metric` provides a general way to specify metrics for models. For
    the Tox21 datasets, the ROC AUC score is a useful metric, so let’s do our analysis
    using it. However, note a subtlety here: there are multiple Tox21 tasks. Which
    one do we compute the ROC AUC on? A good tactic is to compute the mean ROC AUC
    score across all tasks. Luckily, it’s easy to do this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们评估训练模型的性能。为了评估模型的工作效果，有必要指定一个度量标准。DeepChem类`dc.metrics.Metric`提供了一种为模型指定度量标准的通用方法。对于Tox21数据集，ROC
    AUC分数是一个有用的度量标准，所以让我们使用它进行分析。然而，请注意这里的一个细微之处：有多个Tox21任务。我们应该在哪一个上计算ROC AUC？一个好的策略是计算所有任务的平均ROC
    AUC分数。幸运的是，这很容易做到：
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Since we’ve specified `np.mean`, the mean of the ROC AUC scores across all
    tasks will be reported. DeepChem models support the evaluation function `model.evaluate()`,
    which evaluates the performance of the model on a given dataset and metric:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们指定了`np.mean`，所有任务的ROC AUC分数的平均值将被报告。DeepChem模型支持评估函数`model.evaluate()`，该函数评估模型在给定数据集和度量标准上的性能。
- en: ROC AUC
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC AUC
- en: We want to classify molecules as toxic or nontoxic, but the model outputs continuous
    numbers, not discrete predictions. In practice, you pick a threshold value and
    predict that a molecule is toxic whenever the output is greater than the threshold.
    A low threshold will produce many false positives (predicting a safe molecule
    is actually toxic). A higher threshold will give fewer false positives but more
    false negatives (incorrectly predicting that a toxic molecule is safe).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想将分子分类为有毒或无毒，但模型输出连续数字，而不是离散预测。在实践中，您选择一个阈值值，并预测当输出大于阈值时分子是有毒的。较低的阈值将产生许多假阳性（预测安全分子实际上是有毒的）。较高的阈值将产生较少的假阳性，但会产生更多的假阴性（错误地预测有毒分子是安全的）。
- en: The *receiver operating characteristic* (ROC) curve is a convenient way to visualize
    this trade-off. You try many different threshold values, then plot a curve of
    the true positive rate versus the false positive rate as the threshold is varied.
    An example is shown in [Figure 3-1](#the_roc_curve_for_one_of_the_twelve).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接收器操作特性（ROC）曲线是一种方便的可视化权衡方式。您可以尝试许多不同的阈值值，然后绘制真正阳性率与假阳性率随着阈值变化而变化的曲线。一个示例显示在[图3-1](#the_roc_curve_for_one_of_the_twelve)中。
- en: The ROC AUC is the total area under the ROC curve. The *area under the curve*
    (AUC) provides an indication of the model’s ability to distinguish different classes.
    If there exists any threshold value for which every sample is classified correctly,
    the ROC AUC score is 1\. At the other extreme, if the model outputs completely
    random values unrelated to the true classes, the ROC AUC score is 0.5\. This makes
    it a useful number for summarizing how well a classifier works. It’s just a heuristic,
    but it’s a popular one.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ROC AUC是ROC曲线下的总面积。曲线下的面积（AUC）提供了模型区分不同类别的能力的指示。如果存在任何阈值值，每个样本都被正确分类，ROC AUC分数为1。在另一个极端，如果模型输出与真实类别无关的完全随机值，ROC
    AUC分数为0.5。这使得它成为一个用于总结分类器工作效果的有用数字。这只是一个启发式方法，但是它是一个流行的方法。
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we’ve calculated the scores, let’s take a look!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算了分数，让我们来看看！
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice that our score on the training set (0.96) is much better than our score
    on the test set (0.79). This shows the model has been overfit. The test set score
    is the one we really care about. These numbers aren’t the best possible on this
    dataset—at the time of writing, the state of the art ROC AUC scores for the Tox21
    dataset are a little under 0.9—but they aren’t bad at all for an out-of-the-box
    system. The complete ROC curve for one of the 12 tasks is shown in [Figure 3-1](#the_roc_curve_for_one_of_the_twelve).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在训练集上的得分（0.96）比测试集上的得分（0.79）要好得多。这表明模型已经过拟合。我们真正关心的是测试集得分。这些数字在这个数据集上并不是最好的可能值
    - 在撰写本文时，Tox21数据集的最先进ROC AUC分数略低于0.9 - 但对于一个开箱即用的系统来说，它们并不算差。其中一个12个任务的完整ROC曲线显示在[图3-1](#the_roc_curve_for_one_of_the_twelve)中。
- en: '![](Images/dlls_0301.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlls_0301.png)'
- en: Figure 3-1\. The ROC curve for one of the 12 tasks. The dotted diagonal line
    shows what the curve would be for a model that just guessed at random. The actual
    curve is consistently well above the diagonal, showing that we are doing much
    better than random guessing.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。12个任务中的一个的ROC曲线。虚线对角线显示了一个只是随机猜测的模型的曲线。实际曲线始终远高于对角线，表明我们比随机猜测要好得多。
- en: 'Case Study: Training an MNIST Model'
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：训练一个MNIST模型
- en: In the previous section, we covered the basics of training a machine learning
    model with DeepChem. However, we used a premade model class, `dc.models.MultitaskClassifier`.
     Sometimes you may want to create a new deep learning architecture instead of
    using a preconfigured one. In this section, we discuss how to train a convolutional
    neural network on the MNIST digit recognition dataset. Instead of using a premade
    architecture like in the previous example, this time we will specify the full
    deep learning architecture ourselves. To do so, we will introduce the `dc.models.TensorGraph`
    class, which provides a framework for building deep architectures in DeepChem.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们介绍了使用DeepChem训练机器学习模型的基础知识。然而，我们使用了一个预先制作的模型类`dc.models.MultitaskClassifier`。有时候，您可能希望创建一个新的深度学习架构，而不是使用一个预配置的架构。在本节中，我们将讨论如何在MNIST数字识别数据集上训练卷积神经网络。与之前的示例不同，这次我们将自己指定完整的深度学习架构。为此，我们将介绍`dc.models.TensorGraph`类，它提供了在DeepChem中构建深度架构的框架。
- en: When Do Canned Models Make Sense?
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时使用预制模型有意义？
- en: In this section, we’re going to use a custom architecture on MNIST. In the previous
    example, we used a “canned” (that is, predefined) architecture instead. When does
    each alternative make sense? If you have a well-debugged canned architecture for
    a problem, it will likely make sense to use it. But if you’re working on a new
    dataset where no such architecture has been put together, you’ll often have to
    create a custom architecture. It’s important to be familiar with using both canned
    and custom architectures, so we’ve included an example of each in this chapter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在MNIST上使用自定义架构。在之前的示例中，我们使用了“罐头”（即预定义）架构。每种选择何时合理？如果对于某个问题有一个经过充分调试的罐头架构，那么使用它可能是合理的。但如果你正在处理一个没有组合好这样的架构的新数据集，通常需要创建一个自定义架构。熟悉使用罐头和自定义架构是很重要的，因此我们在本章中包含了每种类型的示例。
- en: The MNIST Digit Recognition Dataset
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST手写数字识别数据集
- en: The MNIST digit recognition dataset (see [Figure 3-2](#samples_drawn_from_the_mnist_handwritten_digit_recognition_dataset))
    requires the construction of a machine learning model that can learn to classify
    handwritten digits correctly. The challenge is to classify digits from 0 to 9
    given 28 × 28-pixel black and white images. The dataset contains 60,000 training
    examples and a test set of 10,000 examples.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST手写数字识别数据集（参见[图3-2](#samples_drawn_from_the_mnist_handwritten_digit_recognition_dataset)）需要构建一个机器学习模型，可以正确分类手写数字。挑战在于对0到9的数字进行分类，给定28×28像素的黑白图像。数据集包含60,000个训练示例和10,000个测试示例。
- en: '![](Images/dlls_0302.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlls_0302.png)'
- en: 'Figure 3-2\. Samples drawn from the MNIST handwritten digit recognition dataset.
    (Source: [GitHub](https://github.com/mnielsen/rmnist/blob/master/data/rmnist_10.png))'
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2。从MNIST手写数字识别数据集中抽取的样本。 (来源：[GitHub](https://github.com/mnielsen/rmnist/blob/master/data/rmnist_10.png))
- en: The MNIST dataset is not particularly challenging as far as machine learning
    problems go. Decades of research have produced state-of-the-art algorithms that
    achieve close to 100% test set accuracy on this dataset. As a result, the MNIST
    dataset is no longer suitable for research work, but it is a good tool for pedagogical
    purposes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 就机器学习问题而言，MNIST数据集并不特别具有挑战性。数十年的研究已经产生了最先进的算法，在这个数据集上实现了接近100%的测试集准确率。因此，MNIST数据集不再适用于研究工作，但对于教学目的来说是一个很好的工具。
- en: Isn’t DeepChem Just for the Life Sciences?
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepChem只适用于生命科学吗？
- en: 'As we mentioned earlier in the chapter, it’s entirely feasible to use other
    deep learning packages for life science applications. Similarly, it’s possible
    to build general machine learning systems using DeepChem. Although building a
    movie recommendation system in DeepChem might be trickier than it would be with
    more specialized tools, it would be quite feasible to do so. And for good reason:
    there have been multiple studies looking into the use of recommendation system
    algorithms for use in molecular binding prediction. Machine learning architectures
    used in one field tend to carry over to other fields, so it’s important to retain
    the flexibility needed for innovative work.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章前面提到的，完全可以使用其他深度学习包来进行生命科学应用。同样，也可以使用DeepChem构建通用的机器学习系统。虽然在DeepChem中构建电影推荐系统可能比使用更专门的工具更困难，但这是完全可行的。而且有充分的理由：已经有多项研究探讨了将推荐系统算法用于分子结合预测的应用。在一个领域使用的机器学习架构通常会延伸到其他领域，因此保持创新工作所需的灵活性是很重要的。
- en: A Convolutional Architecture for MNIST
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST的卷积架构
- en: DeepChem uses the `TensorGraph` class to construct nonstandard deep learning
    architectures. In this section, we will walk through the code required to construct
    the convolutional architecture shown in [Figure 3-3](#this_diagram_illustrates_the_artchitecture_that_we_will_construct_in_this).
    It begins with two convolutional layers to identify local features within the
    image. They are followed by two fully connected layers to predict the digit from
    those local features.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: DeepChem使用`TensorGraph`类来构建非标准的深度学习架构。在本节中，我们将逐步介绍构建卷积架构所需的代码，如[图3-3](#this_diagram_illustrates_the_artchitecture_that_we_will_construct_in_this)所示。它从两个卷积层开始，用于识别图像中的局部特征。然后是两个全连接层，用于从这些局部特征预测数字。
- en: '![](Images/dlls_0303.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlls_0303.png)'
- en: Figure 3-3\. An illustration of the architecture that we will construct in this
    section for processing the MNIST dataset.
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3。本节中将构建的用于处理MNIST数据集的架构示意图。
- en: 'To begin, execute the following commands to download the raw MNIST data files
    and store them locally:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，执行以下命令下载原始的MNIST数据文件并将其存储在本地：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s now load these datasets:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们加载这些数据集：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We’re going to process this raw data into a format suitable for analysis by
    DeepChem. Let’s start with the necessary imports:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将处理这些原始数据，使其适合DeepChem进行分析。让我们从必要的导入开始：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The submodule `deepchem.models.tensorgraph.layers` contains a collection of
    “layers.” These layers serve as building blocks of deep architectures and can
    be composed to build new deep learning architectures. We will demonstrate how
    layer objects are used shortly. Next, we construct `NumpyDataset` objects that
    wrap the MNIST training and test datasets:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 子模块`deepchem.models.tensorgraph.layers`包含一系列“层”。这些层作为深度架构的构建模块，可以组合起来构建新的深度学习架构。我们将很快展示层对象是如何使用的。接下来，我们构建`NumpyDataset`对象，用来包装MNIST的训练和测试数据集：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that although there wasn’t originally a test dataset defined, the `input_data()`
    function from TensorFlow takes care of separating out a proper test dataset for
    our use. With the training and test datasets in hand, we can now turn our attention
    towards defining the architecture for the MNIST convolutional network.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管最初没有定义测试数据集，但TensorFlow的`input_data()`函数会负责分离出一个适当的测试数据集供我们使用。有了训练和测试数据集，我们现在可以转向定义MNIST卷积网络的架构。
- en: 'The key concept this is based on is that layer objects can be composed to build
    new models. As we discussed in the previous chapter, each layer takes input from
    previous layers and computes an output that can be passed to subsequent layers.
    At the very start, there are input layers that take in features and labels. At
    the other end are output layers that return the results of the performed computation.
    In this example, we will compose a sequence of layers in order to construct an
    image-processing convolutional network. We start by defining a new`TensorGraph`object:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于的关键概念是可以组合层对象来构建新模型。正如我们在上一章中讨论的，每个层从前面的层接收输入并计算一个输出，该输出可以传递给后续层。在最开始，有接收特征和标签的输入层。在另一端是返回执行计算结果的输出层。在这个例子中，我们将组合一系列层以构建一个图像处理卷积网络。我们首先定义一个新的`TensorGraph`对象：
- en: '[PRE24]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `model_dir` option specifies a directory where the model’s parameters should
    be saved. You can omit this, as we did in the previous example, but then the model
    will not be saved. As soon as the Python interpreter exits, all your hard work
    training the model will be thrown out! Specifying a directory allows you to reload
    the model later and make new predictions with it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`model_dir`选项指定应保存模型参数的目录。您可以省略这一点，就像我们在之前的例子中所做的那样，但是那样模型就不会被保存。一旦Python解释器退出，您辛苦训练模型的所有工作都将被抛弃！指定一个目录允许您稍后重新加载模型并进行新的预测。'
- en: 'Note that since `TensorGraph` inherits from `Model`, this object is an instance
    of `dc.models.Model` and supports the same `fit()` and `evaluate()` functions
    we saw previously:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于`TensorGraph`继承自`Model`，因此该对象是`dc.models.Model`的一个实例，并支持我们之前看到的相同的`fit()`和`evaluate()`函数：
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We haven’t added anything to `model` yet, so our model isn’t likely to be very
    interesting. Let’s start by adding some inputs for features and labels by using
    the `Feature` and `Label` classes:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有向`model`中添加任何内容，因此我们的模型可能不太有趣。让我们通过使用`Feature`和`Label`类为特征和标签添加一些输入：
- en: '[PRE26]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: MNIST contains images of size 28 × 28\. When flattened, these form feature vectors
    of length 784\. The labels have a second dimension of 10 since there are 10 possible
    digit values, and the vector is one-hot encoded. Note that `None` is used as an
    input dimension. In systems that build on TensorFlow, the value `None` often encodes
    the ability for a given layer to accept inputs that have any size in that dimension.
    Put another way, our object `feature` is capable of accepting inputs of shape
    `(20, 784)` and `(97, 784)` with equal facility. In this case, the first dimension
    corresponds to the batch size, so our model will be able to accept batches with
    any number of samples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST包含大小为28×28的图像。当展平时，这些形成长度为784的特征向量。标签具有第二维度为10，因为有10个可能的数字值，并且该向量是独热编码的。请注意，`None`被用作输入维度。在构建在TensorFlow上的系统中，值`None`通常表示给定层能够接受该维度上任何大小的输入。换句话说，我们的对象`feature`能够接受形状为`(20,
    784)`和`(97, 784)`的输入。在这种情况下，第一个维度对应于批量大小，因此我们的模型将能够接受任意数量样本的批次。
- en: One-Hot Encoding
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独热编码
- en: The MNIST dataset is categorical. That is, objects belong to one of a finite
    list of potential categories. In this case, these categories are the digits 0
    through 9\. How can we feed these categories into a machine learning system? One
    obvious answer would be to simply feed in a single number that takes values from
    0 through 9\. However, for a variety of technical reasons, this encoding often
    doesn’t seem to work well. The alternative that people commonly use is to *one-hot
    encode*. Each label for MNIST is a vector of length 10 in which a single element
    is set to 1, and all others are set to 0\. If the nonzero value is at the 0th
    index, then the label corresponds to the digit 0\. If the nonzero value is at
    the 9th index, then the label corresponds to the digit 9.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集是分类的。也就是说，对象属于有限列表中的一个潜在类别。在这种情况下，这些类别是数字0到9。我们如何将这些类别馈送到机器学习系统中？一个明显的答案是简单地输入一个从0到9取值的单个数字。然而，出于各种技术原因，这种编码通常似乎效果不佳。人们通常使用的替代方法是*独热编码*。MNIST的每个标签是一个长度为10的向量，其中一个元素设置为1，其他所有元素设置为0。如果非零值在第0个索引处，则标签对应于数字0。如果非零值在第9个索引处，则标签对应于数字9。
- en: 'In order to apply convolutional layers to our input, we need to convert our
    flat feature vectors into matrices of shape `(28, 28)`. To do this, we will use
    a `Reshape` layer:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将卷积层应用于我们的输入，我们需要将我们的平面特征向量转换为形状为`(28, 28)`的矩阵。为此，我们将使用一个`Reshape`层：
- en: '[PRE27]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here again the value `None` indicates that arbitrary batch sizes can be handled.
    Note that we have a keyword argument `in_layers=feature`. This indicates that
    the `Reshape` layer takes our previous `Feature` layer, `feature`, as input. Now
    that we have successfully reshaped the input, we can pass it through to the convolutional
    layers:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里再次值`None`表示可以处理任意批量大小。请注意我们有一个关键字参数`in_layers=feature`。这表示`Reshape`层以我们先前的`Feature`层`feature`作为输入。现在我们已成功地重塑了输入，我们可以将其传递给卷积层：
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, the `Conv2D` class applies a 2D convolution to each sample of its input,
    then passes it through a rectified linear unit (ReLU) activation function. Note
    how `in_layers` is used to pass along previous layers as inputs to succeeding 
    layers. We want to end by applying `Dense` (fully connected) layers to the outputs
    of the convolutional layer. However, the output of `Conv2D` layers is 2D, so we
    will first need to apply a `Flatten` layer to flatten our input to one dimension
    (more precisely, the `Conv2D` layer produces a 2D output *for each sample*, so
    its output has three dimensions; the `Flatten` layer collapses this to a single
    dimension per sample, or two dimensions in total):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`Conv2D`类对其输入的每个样本应用2D卷积，然后通过修正线性单元（ReLU）激活函数传递。请注意如何使用`in_layers`将先前的层传递给后续层作为输入。我们希望最后应用`Dense`（全连接）层到卷积层的输出。但是，`Conv2D`层的输出是2D的，因此我们首先需要应用一个`Flatten`层将我们的输入展平为一维（更准确地说，`Conv2D`层为每个样本产生一个2D输出，因此其输出具有三个维度；`Flatten`层将其折叠为每个样本的单个维度，或者总共两个维度）：
- en: '[PRE29]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `out_channels` argument in a `Dense` layer specifies the width of the layer.
    The first layer outputs 1,024 values per sample, but the second layer outputs
    10 values, corresponding to our 10 possible digit values. We now want to hook
    this output up to a loss function, so we can train the output to accurately predict
    classes. We will use the `SoftMaxCrossEntropy` loss to perform this form of training:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dense`层中的`out_channels`参数指定了层的宽度。第一层每个样本输出1,024个值，但第二层输出10个值，对应于我们的10个可能的数字值。现在我们希望将此输出连接到损失函数，以便我们可以训练输出以准确预测类别。我们将使用`SoftMaxCrossEntropy`损失来执行这种形式的训练：'
- en: '[PRE30]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that the `SoftMaxCrossEntropy` layer accepts both the labels and the output
    of the last `Dense` layer as inputs. It computes the value of the loss function
    for every sample, so we then need to average over all samples to obtain the final
    loss. This is done with the `ReduceMean` layer, which we set as our model’s loss
    function by calling `model.set_loss()`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`SoftMaxCrossEntropy`层接受最后一个`Dense`层的标签和输出作为输入。它计算每个样本的损失函数的值，因此我们需要对所有样本进行平均以获得最终损失。这是通过调用`model.set_loss()`将`ReduceMean`层设置为我们模型的损失函数来完成的。
- en: SoftMax and SoftMaxCrossEntropy
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SoftMax和SoftMaxCrossEntropy
- en: 'You often want a model to output a probability distribution. For MNIST, we
    want to output the probability that a given sample represents each of the 10 digits.
    Every output must be positive, and they must sum to 1\. An easy way to achieve
    this is to let the model compute arbitrary numbers, then pass them through the
    confusingly named *softmax* function:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通常希望模型输出概率分布。对于MNIST，我们希望输出给定样本代表每个数字的概率。每个输出必须为正，并且它们必须总和为1。实现这一点的一种简单方法是让模型计算任意数字，然后通过令人困惑地命名为*softmax*函数传递它们：
- en: <math><mrow><msub><mi>σ</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><msup><mi>e</mi> <msub><mi>x</mi> <mi>i</mi></msub></msup> <mrow><msub><mo>∑</mo>
    <mi>j</mi></msub> <msup><mi>e</mi> <msub><mi>x</mi> <mi>j</mi></msub></msup></mrow></mfrac></mrow></math>The
    exponential in the numerator ensures that all values are positive, and the sum
    in the denominator ensures they add up to 1\. If one element of <math><mi>x</mi></math>
    is much larger than the others, the corresponding output element is very close
    to 1 and all the other outputs are very close to 0.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 指数在分子中确保所有值为正，并且分母中的总和确保它们加起来为1。如果<math><mi>x</mi></math>的一个元素远远大于其他元素，则相应的输出元素非常接近1，而所有其他输出则非常接近0。
- en: '`SoftMaxCrossEntropy` first uses a softmax function to convert the outputs
    to probabilities, then computes the cross entropy of those probabilities with
    the labels. Remember that the labels are one-hot encoded: 1 for the correct class,
    0 for all others. You can think of that as a probability distribution! The loss
    is minimized when the predicted probability of the correct class is as close to
    1 as possible. These two operations (softmax followed by cross entropy) often
    appear together, and computing them as a single step turns out to be more numerically
    stable than performing them separately.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`SoftMaxCrossEntropy`首先使用softmax函数将输出转换为概率，然后计算这些概率与标签的交叉熵。请记住，标签是独热编码的：正确类别为1，其他所有类别为0。您可以将其视为概率分布！当正确类别的预测概率尽可能接近1时，损失最小。这两个操作（softmax后跟交叉熵）经常一起出现，将它们作为单个步骤进行计算比分开执行更稳定。'
- en: 'For numerical stability, layers like `SoftMaxCrossEntropy` compute in log probabilities.
    We’ll need to transform the output with a `SoftMax` layer to obtain per-class
    output probabilities. We’ll add this output to `model` with `model.add_output()`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了数值稳定性，像`SoftMaxCrossEntropy`这样的层会计算对数概率。我们需要使用`SoftMax`层来转换输出以获得每个类别的输出概率。我们将使用`model.add_output()`将此输出添加到`model`中：
- en: '[PRE31]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can now train the model using the same `fit()` function we called in the
    previous section:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用与上一节中调用的相同的`fit()`函数来训练模型：
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that this method call might take some time to execute on a standard laptop!
    If the function is not executing quickly enough, try using `nb_epoch=1`. The results
    will be worse, but you will be able to complete the rest of this chapter more
    quickly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个方法调用可能需要一些时间在标准笔记本电脑上执行！如果函数执行速度不够快，请尝试使用`nb_epoch=1`。结果会更糟，但您将能够更快地完成本章的其余部分。
- en: 'Let’s define our metric this time to be accuracy, the fraction of labels that
    are correctly predicted:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们将定义我们的度量为准确率，即正确预测的标签比例：
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can then compute the accuracy using the same computation as before:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用与之前相同的计算来计算准确率：
- en: '[PRE34]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This produces excellent performance: the accuracy is 0.999 on the training
    set, and 0.991 on the test set. Our model identifies more than 99% of the test
    set samples correctly.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生出色的性能：训练集的准确率为0.999，测试集的准确率为0.991。我们的模型正确识别了超过99%的测试集样本。
- en: Try to Get Access to a GPU
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试获取GPU访问权限
- en: As you saw in this chapter, deep learning code can run pretty slowly! Training
    a convolutional neural network on a good laptop can take more than an hour to
    complete. This is because this code depends on a large number of linear algebraic
    operations on image data. Most CPUs are not well equipped to perform these types
    of computations.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在本章中看到的，深度学习代码可能运行得相当慢！在一台好的笔记本电脑上训练卷积神经网络可能需要超过一个小时才能完成。这是因为这段代码依赖于对图像数据的大量线性代数运算。大多数CPU并不适合执行这些类型的计算。
- en: If possible, try to get access to a modern graphics processing unit. These cards
    were originally developed for gaming, but are now used for many types of numeric
    computations. Most modern deep learning workloads will run much faster on GPUs.
    The examples you’ll see in this book will be easier to complete with GPUs as well.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，尽量获取现代图形处理单元的访问权限。这些卡最初是为游戏开发的，但现在用于许多类型的数值计算。大多数现代深度学习工作负载在GPU上运行速度要快得多。您将在本书中看到的示例也将更容易使用GPU完成。
- en: If it’s not feasible to get access to a GPU, don’t worry. You’ll still be able
    to complete the exercises in this book—they might just take a little longer (you
    might have to grab a coffee or read a book while you wait for the code to finish
    running).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法获得GPU的访问权限，也不用担心。您仍然可以完成本书中的练习，只是可能会花费更长的时间（您可能需要在等待代码运行完成时喝杯咖啡或读本书）。
- en: Conclusion
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you’ve learned how to use the DeepChem library to implement
    some simple machine learning systems. In the remainder of this book, we will continue
    to use DeepChem as our library of choice, so don’t worry if you don’t have a strong
    grasp of the fundamentals of the library yet. There will be plenty more examples
    coming.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，您已经学会了如何使用DeepChem库来实现一些简单的机器学习系统。在本书的其余部分中，我们将继续使用DeepChem作为我们的首选库，所以如果您还没有对该库的基本知识有很好的掌握，不要担心。将会有更多的例子出现。
- en: In subsequent chapters, we will begin to introduce the basic concepts needed
    to do effective machine learning on life science datasets. In the next chapter,
    we will introduce you to machine learning on molecules.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将开始介绍在生命科学数据集上进行有效机器学习所需的基本概念。在下一章中，我们将向您介绍分子上的机器学习。
