["```py\n!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n!unzip jena_climate_2009_2016.csv.zip \n```", "```py\nimport os\n\nfname = os.path.join(\"jena_climate_2009_2016.csv\")\n\nwith open(fname) as f:\n    data = f.read()\n\nlines = data.split(\"\\n\")\nheader = lines[0].split(\",\")\nlines = lines[1:]\nprint(header)\nprint(len(lines)) \n```", "```py\n[\"Date Time\",\n \"p (mbar)\",\n \"T (degC)\",\n \"Tpot (K)\",\n \"Tdew (degC)\",\n \"rh (%)\",\n \"VPmax (mbar)\",\n \"VPact (mbar)\",\n \"VPdef (mbar)\",\n \"sh (g/kg)\",\n \"H2OC (mmol/mol)\",\n \"rho (g/m**3)\",\n \"wv (m/s)\",\n \"max. wv (m/s)\",\n \"wd (deg)\"] \n```", "```py\nimport numpy as np\n\ntemperature = np.zeros((len(lines),))\nraw_data = np.zeros((len(lines), len(header) - 1))\n\nfor i, line in enumerate(lines):\n    values = [float(x) for x in line.split(\",\")[1:]]\n    # We store column 1 in the temperature array.\n    temperature[i] = values[1]\n    # We store all columns (including the temperature) in the raw_data\n    # array.\n    raw_data[i, :] = values[:] \n```", "```py\nfrom matplotlib import pyplot as plt\n\nplt.plot(range(len(temperature)), temperature) \n```", "```py\nplt.plot(range(1440), temperature[:1440]) \n```", "```py\n>>> num_train_samples = int(0.5 * len(raw_data))\n>>> num_val_samples = int(0.25 * len(raw_data))\n>>> num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n>>> print(\"num_train_samples:\", num_train_samples)\n>>> print(\"num_val_samples:\", num_val_samples)\n>>> print(\"num_test_samples:\", num_test_samples)\nnum_train_samples: 210225\nnum_val_samples: 105112\nnum_test_samples: 105114\n```", "```py\nmean = raw_data[:num_train_samples].mean(axis=0)\nraw_data -= mean\nstd = raw_data[:num_train_samples].std(axis=0)\nraw_data /= std \n```", "```py\nsampling_rate = 6\nsequence_length = 120\ndelay = sampling_rate * (sequence_length + 24 - 1)\nbatch_size = 256\n\ntrain_dataset = keras.utils.timeseries_dataset_from_array(\n    raw_data[:-delay],\n    targets=temperature[delay:],\n    sampling_rate=sampling_rate,\n    sequence_length=sequence_length,\n    shuffle=True,\n    batch_size=batch_size,\n    start_index=0,\n    end_index=num_train_samples,\n)\n\nval_dataset = keras.utils.timeseries_dataset_from_array(\n    raw_data[:-delay],\n    targets=temperature[delay:],\n    sampling_rate=sampling_rate,\n    sequence_length=sequence_length,\n    shuffle=True,\n    batch_size=batch_size,\n    start_index=num_train_samples,\n    end_index=num_train_samples + num_val_samples,\n)\n\ntest_dataset = keras.utils.timeseries_dataset_from_array(\n    raw_data[:-delay],\n    targets=temperature[delay:],\n    sampling_rate=sampling_rate,\n    sequence_length=sequence_length,\n    shuffle=True,\n    batch_size=batch_size,\n    start_index=num_train_samples + num_val_samples,\n) \n```", "```py\n>>> for samples, targets in train_dataset:\n>>>     print(\"samples shape:\", samples.shape)\n>>>     print(\"targets shape:\", targets.shape)\n>>>     break\nsamples shape: (256, 120, 14)\ntargets shape: (256,)\n```", "```py\nnp.mean(np.abs(preds - targets)) \n```", "```py\ndef evaluate_naive_method(dataset):\n    total_abs_err = 0.0\n    samples_seen = 0\n    for samples, targets in dataset:\n        # The temperature feature is at column 1, so `samples[:, -1,\n        # 1]` is the last temperature measurement in the input\n        # sequence. Recall that we normalized our features to retrieve\n        # a temperature in Celsius degrees, we need to un-normalize it,\n        # by multiplying it by the standard deviation and adding back\n        # the mean.\n        preds = samples[:, -1, 1] * std[1] + mean[1]\n        total_abs_err += np.sum(np.abs(preds - targets))\n        samples_seen += samples.shape[0]\n    return total_abs_err / samples_seen\n\nprint(f\"Validation MAE: {evaluate_naive_method(val_dataset):.2f}\")\nprint(f\"Test MAE: {evaluate_naive_method(test_dataset):.2f}\") \n```", "```py\nimport keras\nfrom keras import layers\n\ninputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\nx = layers.Flatten()(inputs)\nx = layers.Dense(16, activation=\"relu\")(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)\n\ncallbacks = [\n    # We use a callback to save the best-performing model.\n    keras.callbacks.ModelCheckpoint(\"jena_dense.keras\", save_best_only=True)\n]\nmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\nhistory = model.fit(\n    train_dataset,\n    epochs=10,\n    validation_data=val_dataset,\n    callbacks=callbacks,\n)\n\n# Reloads the best model and evaluates it on the test data\nmodel = keras.models.load_model(\"jena_dense.keras\")\nprint(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\") \n```", "```py\nimport matplotlib.pyplot as plt\n\nloss = history.history[\"mae\"]\nval_loss = history.history[\"val_mae\"]\nepochs = range(1, len(loss) + 1)\nplt.figure()\nplt.plot(epochs, loss, \"r--\", label=\"Training MAE\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\nplt.title(\"Training and validation MAE\")\nplt.legend()\nplt.show() \n```", "```py\ninputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\nx = layers.Conv1D(8, 24, activation=\"relu\")(inputs)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Conv1D(8, 12, activation=\"relu\")(x)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Conv1D(8, 6, activation=\"relu\")(x)\nx = layers.GlobalAveragePooling1D()(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"jena_conv.keras\", save_best_only=True)\n]\nmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\nhistory = model.fit(\n    train_dataset,\n    epochs=10,\n    validation_data=val_dataset,\n    callbacks=callbacks,\n)\n\nmodel = keras.models.load_model(\"jena_conv.keras\")\nprint(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\") \n```", "```py\ninputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\nx = layers.LSTM(16)(inputs)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"jena_lstm.keras\", save_best_only=True)\n]\nmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\nhistory = model.fit(\n    train_dataset,\n    epochs=10,\n    validation_data=val_dataset,\n    callbacks=callbacks,\n)\n\nmodel = keras.models.load_model(\"jena_lstm.keras\")\nprint(\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\") \n```", "```py\n# The state at t\nstate_t = 0\n# Iterates over sequence elements\nfor input_t in input_sequence:\n    output_t = f(input_t, state_t)\n    # The previous output becomes the state for the next iteration.\n    state_t = output_t \n```", "```py\nstate_t = 0\nfor input_t in input_sequence:\n    output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n    state_t = output_t \n```", "```py\nimport numpy as np\n\n# Number of timesteps in the input sequence\ntimesteps = 100\n# Dimensionality of the input feature space\ninput_features = 32\n# Dimensionality of the output feature space\noutput_features = 64\n# Input data: random noise for the sake of the example\ninputs = np.random.random((timesteps, input_features))\n# Initial state: an all-zero vector\nstate_t = np.zeros((output_features,))\n# Creates random weight matrices\nW = np.random.random((output_features, input_features))\nU = np.random.random((output_features, output_features))\nb = np.random.random((output_features,))\nsuccessive_outputs = []\n# input_t is a vector of shape (input_features,).\nfor input_t in inputs:\n    # Combines the input with the current state (the previous output)\n    # to obtain the current output. We use tanh to add nonlinearity (we\n    # could use any other activation function).\n    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n    # Stores this output in a list\n    successive_outputs.append(output_t)\n    # Updates the state of the network for the next timestep\n    state_t = output_t\n# The final output is a rank-2 tensor of shape (timesteps,\n# output_features).\nfinal_output_sequence = np.concatenate(successive_outputs, axis=0) \n```", "```py\noutput_t = tanh(matmul(input_t, W) + matmul(state_t, U) + b) \n```", "```py\nnum_features = 14\ninputs = keras.Input(shape=(None, num_features))\noutputs = layers.SimpleRNN(16)(inputs) \n```", "```py\n>>> num_features = 14\n>>> steps = 120\n>>> inputs = keras.Input(shape=(steps, num_features))\n>>> # Note that return_sequences=False is the default.\n>>> outputs = layers.SimpleRNN(16, return_sequences=False)(inputs)\n>>> print(outputs.shape)\n(None, 16)\n```", "```py\n>>> num_features = 14\n>>> steps = 120\n>>> inputs = keras.Input(shape=(steps, num_features))\n>>> # Sets return_sequences to True\n>>> outputs = layers.SimpleRNN(16, return_sequences=True)(inputs)\n>>> print(outputs.shape)\n(None, 120, 16)\n```", "```py\ninputs = keras.Input(shape=(steps, num_features))\nx = layers.SimpleRNN(16, return_sequences=True)(inputs)\nx = layers.SimpleRNN(16, return_sequences=True)(x)\noutputs = layers.SimpleRNN(16)(x) \n```", "```py\ny = activation(dot(state_t, U) + dot(input_t, W) + b) \n```", "```py\noutput_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)\ni_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)\nf_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)\nk_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk) \n```", "```py\nc_t+1 = i_t * k_t + c_t * f_t \n```", "```py\ninputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\nx = layers.LSTM(32, recurrent_dropout=0.25)(inputs)\n# To regularize the Dense layer, we also add a Dropout layer after the\n# LSTM.\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        \"jena_lstm_dropout.keras\", save_best_only=True\n    )\n]\nmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\nhistory = model.fit(\n    train_dataset,\n    epochs=50,\n    validation_data=val_dataset,\n    callbacks=callbacks,\n) \n```", "```py\ninputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\nx = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)\nx = layers.GRU(32, recurrent_dropout=0.5)(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        \"jena_stacked_gru_dropout.keras\", save_best_only=True\n    )\n]\nmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\nhistory = model.fit(\n    train_dataset,\n    epochs=50,\n    validation_data=val_dataset,\n    callbacks=callbacks,\n)\nmodel = keras.models.load_model(\"jena_stacked_gru_dropout.keras\")\nprint(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\") \n```", "```py\ninputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\nx = layers.Bidirectional(layers.LSTM(16))(inputs)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)\n\nmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\nhistory = model.fit(\n    train_dataset,\n    epochs=10,\n    validation_data=val_dataset,\n) \n```"]