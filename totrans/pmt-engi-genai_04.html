<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Advanced Techniques for Text Generation with LangChain"><div class="chapter" id="advanced_text_04">
<h1><span class="label">Chapter 4. </span>Advanced Techniques for <span class="keep-together">Text Generation with LangChain</span></h1>


<p>Using simple prompt engineering techniques will often work for most tasks, but occasionally you’ll need to use a more powerful toolkit to solve complex generative AI problems. Such problems and tasks include:</p>
<dl>
<dt>Context length</dt>
<dd>
<p>Summarizing an entire book into a digestible synopsis.</p>
</dd>
<dt>Combining sequential LLM inputs/outputs</dt>
<dd>
<p>Creating a story for a book including the characters, plot, and world building.</p>
</dd>
<dt>Performing complex reasoning tasks</dt>
<dd>
<p>LLMs acting as an agent. For example, you could create an LLM agent to help you achieve your personal fitness goals.</p>
</dd>
</dl>

<p>To skillfully tackle such complex generative AI challenges, becoming acquainted with LangChain, an open source framework, is highly beneficial. This tool simplifies and enhances your LLM’s workflows substantially.</p>






<section data-type="sect1" data-pdf-bookmark="Introduction to LangChain"><div class="sect1" id="id54">
<h1>Introduction to LangChain</h1>

<p>LangChain is a versatile framework <a data-type="indexterm" data-primary="LangChain" id="id761"/>that enables the creation of applications utilizing LLMs and is available as both a <a href="https://oreil.ly/YPid-">Python</a> and a <a href="https://oreil.ly/5Vl0W">TypeScript</a> package. Its central tenet is that the most impactful and distinct applications won’t merely interface with a language model via an API, but will also:</p>
<dl class="pagebreak-before">
<dt>Enhance data awareness</dt>
<dd>
<p>The framework aims to establish a seamless connection between a language model and external data sources.</p>
</dd>
<dt>Enhance agency</dt>
<dd>
<p>It strives to equip language models with the ability to engage with and influence their environment.</p>
</dd>
</dl>

<p>The LangChain framework illustrated in <a data-type="xref" href="#figure-4-1">Figure 4-1</a> provides <a data-type="indexterm" data-primary="LangChain" data-secondary="framework" id="id762"/>a range of modular abstractions that are essential for working with LLMs, along with a broad selection of implementations for these abstractions.</p>

<figure><div id="figure-4-1" class="figure">
<img src="assets/pega_0401.png" alt="pega 0401" width="600" height="596"/>
<h6><span class="label">Figure 4-1. </span>The major modules of the LangChain LLM framework</h6>
</div></figure>

<p>Each module is designed to be user-friendly and can be <a data-type="indexterm" data-primary="LangChain" data-secondary="modules" id="id763"/>efficiently utilized independently or together. There are currently six common modules within LangChain:</p>
<dl>
<dt>Model I/O</dt>
<dd>
<p>Handles input/output operations related to the model</p>
</dd>
<dt>Retrieval</dt>
<dd>
<p>Focuses on retrieving relevant text for the LLM</p>
</dd>
<dt>Chains</dt>
<dd>
<p>Also known as <em>LangChain runnables</em>, chains enable the construction of sequences of LLM operations or function calls</p>
</dd>
<dt>Agents</dt>
<dd>
<p>Allows chains to make decisions on which tools to use based on high-level directives or instructions</p>
</dd>
<dt>Memory</dt>
<dd>
<p>Persists the state of an application between different runs of a chain</p>
</dd>
<dt>Callbacks</dt>
<dd>
<p>For running additional code on specific events, such as when every new token is generated</p>
</dd>
</dl>








<section data-type="sect2" data-pdf-bookmark="Environment Setup"><div class="sect2" id="id163">
<h2>Environment Setup</h2>

<p>You can install LangChain on your terminal <a data-type="indexterm" data-primary="LangChain" data-secondary="installing" id="id764"/>with either of these commands:</p>

<ul>
<li>
<p><code>pip install langchain langchain-openai</code></p>
</li>
<li>
<p><code>conda install -c conda-forge langchain langchain-openai</code></p>
</li>
</ul>

<p>If you would prefer to install the package requirements for the entire book, you can use the <a href="https://oreil.ly/WKOma"><em>requirements.txt</em></a> file from the GitHub repository.</p>

<p>It’s recommended to install the packages within a <a data-type="indexterm" data-primary="LangChain" data-secondary="virtual environment" id="id765"/>virtual environment:</p>
<dl>
<dt>Create a virtual environment</dt>
<dd>
<p><code>python -m venv venv</code></p>
</dd>
<dt>Activate the virtual environment</dt>
<dd>
<p><code>source venv/bin/activate</code></p>
</dd>
<dt>Install the dependencies</dt>
<dd>
<p><code>pip install -r requirements.txt</code></p>
</dd>
</dl>

<p>LangChain requires integrations with one or more model providers. For example, to use OpenAI’s model APIs, you’ll need to install their Python package with <code>pip install openai</code>.</p>

<p>As discussed in <a data-type="xref" href="ch01.html#five_principles_01">Chapter 1</a>, it’s best practice to set an environment variable called <code>OPENAI_API_KEY</code> in your terminal or load it from an <em>.env</em> file using <a href="https://oreil.ly/wvuO7"><code>python-dotenv</code></a>. However, for prototyping you can choose to skip this step by passing in your API key directly when loading a chat model in LangChain:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="n">chat</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">api_key</code><code class="o">=</code><code class="s2">"api_key"</code><code class="p">)</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Hardcoding API keys in scripts is not recommended <a data-type="indexterm" data-primary="API keys" data-secondary="hardcoding" id="id766"/>due to security reasons. Instead, utilize environment variables or configuration files to manage your keys.</p>
</div>

<p>In the constantly evolving landscape of LLMs, you can encounter the challenge of disparities across different model APIs. The lack of standardization in interfaces can induce extra layers of complexity in prompt engineering and obstruct the seamless integration of diverse models into your projects.</p>

<p>This is where LangChain comes into play. As a comprehensive framework, LangChain allows you to easily consume the varying interfaces of different models.</p>

<p>LangChain’s functionality ensures that you aren’t required to reinvent your prompts or code every time you switch between models. Its platform-agnostic approach promotes rapid experimentation with a broad range of models, such as <a href="https://www.anthropic.com">Anthropic</a>, <a href="https://cloud.google.com/vertex-ai">Vertex AI</a>, <a href="https://openai.com">OpenAI</a>, and <a href="https://oreil.ly/bedrock">BedrockChat</a>. This not only expedites the model evaluation process but also saves critical time and resources by simplifying complex model integrations.</p>

<p>In the sections that follow, you’ll be using the OpenAI package and their API in LangChain.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Chat Models"><div class="sect1" id="id55">
<h1>Chat Models</h1>

<p>Chat models such as GPT-4 have become the <a data-type="indexterm" data-primary="LangChain" data-secondary="OpenAI and" id="lgcpa"/><a data-type="indexterm" data-primary="OpenAI" data-secondary="LangChain and" data-see="LangChain" id="id767"/>primary way to interface with OpenAI’s API. Instead of offering a straightforward “input text, output text” response, they propose an interaction method where <em>chat messages</em> are the input and output elements.</p>

<p>Generating LLM responses using chat models involves <a data-type="indexterm" data-primary="chat models" data-secondary="LangChain, message types" id="id768"/>inputting one or more messages into the chat model. In the context of LangChain, the currently accepted message types are <code>AIMessage</code>, <code>HumanMessage</code>, and <code>SystemMessage</code>. The output from a chat model will always be an <code>AIMessage</code>.</p>
<dl>
<dt>SystemMessage</dt>
<dd>
<p>Represents information that should <a data-type="indexterm" data-primary="LangChain" data-secondary="chat models" data-tertiary="SystemMessage" id="id769"/><a data-type="indexterm" data-primary="SystemMessage message type" id="id770"/>be instructions to the AI system. These are used to guide the AI’s behavior or actions in some way.</p>
</dd>
<dt>HumanMessage</dt>
<dd>
<p>Represents information coming from <a data-type="indexterm" data-primary="LangChain" data-secondary="chat models" data-tertiary="HumanMessage" id="id771"/><a data-type="indexterm" data-primary="HumanMessage message type" id="id772"/>a human interacting with the AI system. This could be a question, a command, or any other input from a human user that the AI needs to process and respond to.</p>
</dd>
<dt>AIMessage</dt>
<dd>
<p>Represents information coming from <a data-type="indexterm" data-primary="LangChain" data-secondary="chat models" data-tertiary="AIMessage" id="id773"/><a data-type="indexterm" data-primary="AIMessage message type" id="id774"/>the AI system itself. This is typically the AI’s response to a <code>HumanMessage</code> or the result of a <code>SystemMessage</code> instruction.</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Make sure to leverage the <code>SystemMessage</code> for delivering explicit directions. OpenAI has refined GPT-4 and upcoming LLM models to pay particular attention to the guidelines given within this type of message.</p>
</div>

<p>Let’s create a joke generator in <a data-type="indexterm" data-primary="LangChain" data-secondary="joke generator" id="id775"/>LangChain.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langchain.schema</code> <code class="kn">import</code> <code class="n">AIMessage</code><code class="p">,</code> <code class="n">HumanMessage</code><code class="p">,</code> <code class="n">SystemMessage</code>

<code class="n">chat</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">temperature</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>
<code class="n">messages</code> <code class="o">=</code> <code class="p">[</code><code class="n">SystemMessage</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="s1">'''Act as a senior software engineer</code>
<code class="s1">at a startup company.'''</code><code class="p">),</code>
<code class="n">HumanMessage</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="s1">'''Please can you provide a funny joke</code>
<code class="s1">about software engineers?'''</code><code class="p">)]</code>
<code class="n">response</code> <code class="o">=</code> <code class="n">chat</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="nb">input</code><code class="o">=</code><code class="n">messages</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Sure</code><code class="p">,</code> <code class="n">here</code><code class="s1">'s a lighthearted joke for you:</code><code class="w"/>
<code class="n">Why</code> <code class="n">did</code> <code class="n">the</code> <code class="n">software</code> <code class="n">engineer</code> <code class="n">go</code> <code class="n">broke</code><code class="err">?</code>
<code class="n">Because</code> <code class="n">he</code> <code class="n">lost</code> <code class="n">his</code> <code class="n">domain</code> <code class="ow">in</code> <code class="n">a</code> <code class="n">bet</code> <code class="ow">and</code> <code class="n">couldn</code><code class="s1">'t afford to renew it.</code><code class="w"/></pre>

<p>First, you’ll import <code>ChatOpenAI</code>, <code>AIMessage</code>, <code>HumanMessage</code>, and <code>SystemMessage</code>. Then create an instance of the <code>ChatOpenAI</code> class with a temperature parameter of 0.5 (randomness).</p>

<p>After creating a model, a list named <code>messages</code> is populated with a <code>SystemMessage</code> object, defining the role for the LLM, and a <code>HumanMessage</code> object, which asks for a software engineer—related joke.</p>

<p>Calling the chat model with <code>.invoke(input=messages)</code> feeds the LLM with a list of messages, and then you retrieve the LLM’s response with <code>response.content</code>.</p>

<p>There is a legacy method that allows you to <a data-type="indexterm" data-primary="LangChain" data-secondary="OpenAI and" data-startref="lgcpa" id="id776"/>directly call the <code>chat</code> object with <code>chat(messages=messages)</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">response</code> <code class="o">=</code> <code class="n">chat</code><code class="p">(</code><code class="n">messages</code><code class="o">=</code><code class="n">messages</code><code class="p">)</code></pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Streaming Chat Models"><div class="sect1" id="id56">
<h1>Streaming Chat Models</h1>

<p>You might have observed while <a data-type="indexterm" data-primary="chat models" data-secondary="streaming" id="id777"/><a data-type="indexterm" data-primary="LangChain" data-secondary="chat models" data-tertiary="streaming" id="id778"/><a data-type="indexterm" data-primary="streaming chat models, LangChain" id="id779"/>using ChatGPT how words are sequentially returned to you, one character at a time. This distinct pattern of response generation is referred to as <em>streaming</em>, and it plays a crucial role in enhancing the performance of chat-based applications:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">chunk</code> <code class="ow">in</code> <code class="n">chat</code><code class="o">.</code><code class="n">stream</code><code class="p">(</code><code class="n">messages</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">chunk</code><code class="o">.</code><code class="n">content</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="s2">""</code><code class="p">,</code> <code class="n">flush</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>When you call <code>chat.stream(messages)</code>, it yields chunks of the message one at a time. This means each segment of the chat message is individually returned. As each chunk arrives, it is then instantaneously printed to the terminal and flushed. This way, <em>streaming</em> allows for minimal latency from your LLM responses.</p>

<p>Streaming holds several benefits from an end-user perspective. First, it dramatically reduces the waiting time for users. As soon as the text starts generating character by character, users can start interpreting the message. There’s no need for a full message to be constructed before it is seen. This, in turn, significantly enhances user interactivity and minimizes latency.</p>

<p>Nevertheless, this technique comes with its own set of challenges. One significant challenge is parsing the outputs while they are being streamed. Understanding and appropriately responding to the message as it is being formed can prove to be intricate, especially when the content is complex and detailed.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Creating Multiple LLM Generations"><div class="sect1" id="id57">
<h1>Creating Multiple LLM Generations</h1>

<p>There may be scenarios where you find it useful <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="multiple generations" id="llmlggg"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="list of message lists" id="lgrgmlss"/>to generate multiple responses from LLMs. This is particularly true while creating dynamic content like social media posts. Rather than providing a list of messages, you provide a <em>list of message lists</em>.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># 2x lists of messages, which is the same as [messages, messages]</code>
<code class="n">synchronous_llm_result</code> <code class="o">=</code> <code class="n">chat</code><code class="o">.</code><code class="n">batch</code><code class="p">([</code><code class="n">messages</code><code class="p">]</code><code class="o">*</code><code class="mi">2</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">synchronous_llm_result</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="n">AIMessage</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="s1">'''Sure, here's a lighthearted joke for you:</code><code class="se">\n\n</code><code class="s1">Why did</code>
<code class="s1">the software engineer go broke?</code><code class="se">\n\n</code><code class="s1">Because he kept forgetting to Ctrl+ Z</code>
<code class="s1">his expenses!'''</code><code class="p">),</code>
<code class="n">AIMessage</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="s1">'''Sure, here</code><code class="se">\'</code><code class="s1">s a lighthearted joke for you:</code><code class="se">\n\n</code><code class="s1">Why do</code>
<code class="s1">software engineers prefer dark mode?</code><code class="se">\n\n</code><code class="s1">Because it</code><code class="se">\'</code><code class="s1">s easier on their</code>
<code class="s1">"byte" vision!'''</code><code class="p">)]</code></pre>

<p>The benefit of using <code>.batch()</code> over <code>.invoke()</code> is that you can parallelize the number of API requests made to OpenAI.</p>

<p>For any runnable in LangChain, you can add a <code>RunnableConfig</code> argument to the <code>batch</code> function that contains many configurable parameters, including <code>max_</code><span class="keep-together"><code>concurrency</code></span>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.runnables.config</code> <code class="kn">import</code> <code class="n">RunnableConfig</code>

<code class="c1"># Create a RunnableConfig with the desired concurrency limit:</code>
<code class="n">config</code> <code class="o">=</code> <code class="n">RunnableConfig</code><code class="p">(</code><code class="n">max_concurrency</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>

<code class="c1"># Call the .batch() method with the inputs and config:</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">chat</code><code class="o">.</code><code class="n">batch</code><code class="p">([</code><code class="n">messages</code><code class="p">,</code> <code class="n">messages</code><code class="p">],</code> <code class="n">config</code><code class="o">=</code><code class="n">config</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In computer science, <em>asynchronous (async) functions</em> are those <a data-type="indexterm" data-primary="asynchronous (async) functions" id="id780"/><a data-type="indexterm" data-primary="LangChain" data-secondary="asynchronous functions" id="id781"/>that operate independently of other processes, thereby enabling several API requests to be run concurrently without waiting for each other. In LangChain, these async functions let you make many API requests all at once, not one after the other. This is especially helpful in more complex workflows and decreases the overall latency to your users.</p>

<p>Most of the asynchronous functions within LangChain are simply prefixed with the letter <code>a</code>, such as <code>.ainvoke()</code> and <code>.abatch()</code>. If you would like to use the async API for more efficient task performance, then <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="multiple generations" data-startref="llmlggg" id="id782"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="list of message lists" data-startref="lgrgmlss" id="id783"/>utilize these functions.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="LangChain Prompt Templates"><div class="sect1" id="id58">
<h1>LangChain Prompt Templates</h1>

<p>Up until this point, you’ve been hardcoding the <a data-type="indexterm" data-primary="LangChain" data-secondary="prompt templates" id="lgcpplt"/><a data-type="indexterm" data-primary="templates" id="tplppp"/>strings in the <code>ChatOpenAI</code> objects. As your LLM applications grow in size, it becomes increasingly important to utilize <em>prompt templates</em>.</p>

<p><em>Prompt templates</em> are good for generating reproducible prompts for AI language models. They consist of a <em>template</em>, a text string that can take in parameters, and construct a text prompt for a language model.</p>

<p>Without prompt templates, you would likely use <a data-type="indexterm" data-primary="Python" data-secondary="f-string formatting" id="id784"/>Python <code>f-string</code> formatting:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">language</code> <code class="o">=</code> <code class="s2">"Python"</code>
<code class="n">prompt</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"What is the best way to learn coding in </code><code class="si">{</code><code class="n">language</code><code class="si">}</code><code class="s2">?"</code>
<code class="nb">print</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code> <code class="c1"># What is the best way to learn coding in Python?</code></pre>

<p>But why not simply use an <code>f-string</code> for prompt templating? Using LangChain’s prompt templates instead allows you to easily:</p>

<ul>
<li>
<p>Validate your prompt inputs</p>
</li>
<li>
<p>Combine multiple prompts together with composition</p>
</li>
<li>
<p>Define custom selectors that will inject k-shot examples into your prompt</p>
</li>
<li>
<p>Save and load prompts from <em>.yml</em> and <em>.json</em> files</p>
</li>
<li>
<p>Create custom prompt templates that <a data-type="indexterm" data-primary="LangChain" data-secondary="prompt templates" data-startref="lgcpplt" id="id785"/><a data-type="indexterm" data-primary="templates" data-startref="tplppp" id="id786"/>execute additional code or instructions when created</p>
</li>
</ul>
</div></section>






<section data-type="sect1" data-pdf-bookmark="LangChain Expression Language (LCEL)"><div class="sect1" id="id59">
<h1>LangChain Expression Language (LCEL)</h1>

<p>The <code>|</code> pipe operator is a key component of <a data-type="indexterm" data-primary="LCEL (LangChain Expression Language)" id="lcgcxgg"/><a data-type="indexterm" data-primary="LangChain Expression Language (LCEL)" id="lcxlgau"/>LangChain Expression Language (LCEL) that allows you to chain together different components or <em>runnables</em> in a data processing pipeline.</p>

<p>In LCEL, the <code>|</code> operator is similar to the Unix pipe operator. It takes the output of one component and feeds it as input to the next component in the chain. This allows you to easily connect and combine different components to create a complex chain of operations:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">chain</code> <code class="o">=</code> <code class="n">prompt</code> <code class="o">|</code> <code class="n">model</code></pre>

<p>The <code>|</code> operator is used to chain together the prompt and model components. The output of the prompt component is passed as input to the model component. This chaining mechanism allows you to build complex chains from basic components and enables the seamless flow of data between different stages of the processing pipeline.</p>

<p>Additionally, <em>the order matters</em>, so you could technically create this chain:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">bad_order_chain</code> <code class="o">=</code> <code class="n">model</code> <code class="o">|</code> <code class="n">prompt</code></pre>

<p>But it would produce an error after using the <code>invoke</code> function, because the values returned from <code>model</code> are not compatible with the expected inputs for the prompt.</p>

<p>Let’s create a business name generator using prompt templates that will return five to seven relevant business names:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="p">(</code><code class="n">SystemMessagePromptTemplate</code><code class="p">,</code>
<code class="n">ChatPromptTemplate</code><code class="p">)</code>

<code class="n">template</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">You are a creative consultant brainstorming names for businesses.</code>

<code class="s2">You must follow the following principles:</code>
<code class="si">{principles}</code><code class="s2"/>

<code class="s2">Please generate a numerical list of five catchy names for a start-up in the</code>
<code class="si">{industry}</code><code class="s2"> industry that deals with </code><code class="si">{context}</code><code class="s2">?</code>

<code class="s2">Here is an example of the format:</code>
<code class="s2">1. Name1</code>
<code class="s2">2. Name2</code>
<code class="s2">3. Name3</code>
<code class="s2">4. Name4</code>
<code class="s2">5. Name5</code>
<code class="s2">"""</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>
<code class="n">system_prompt</code> <code class="o">=</code> <code class="n">SystemMessagePromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="n">template</code><code class="p">)</code>
<code class="n">chat_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">([</code><code class="n">system_prompt</code><code class="p">])</code>

<code class="n">chain</code> <code class="o">=</code> <code class="n">chat_prompt</code> <code class="o">|</code> <code class="n">model</code>

<code class="n">result</code> <code class="o">=</code> <code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code>
    <code class="s2">"industry"</code><code class="p">:</code> <code class="s2">"medical"</code><code class="p">,</code>
    <code class="s2">"context"</code><code class="p">:</code><code class="s1">'''creating AI solutions by automatically summarizing patient</code>
<code class="s1">    records'''</code><code class="p">,</code>
    <code class="s2">"principles"</code><code class="p">:</code><code class="s1">'''1. Each name should be short and easy to</code>
<code class="s1">    remember. 2. Each name should be easy to pronounce.</code>
<code class="s1">    3. Each name should be unique and not already taken by another company.'''</code>
<code class="p">})</code>

<code class="nb">print</code><code class="p">(</code><code class="n">result</code><code class="o">.</code><code class="n">content</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">1. SummarAI
2. MediSummar
3. AutoDocs
4. RecordAI
5. SmartSummarize</pre>

<p>First, you’ll import <code>ChatOpenAI</code>, <code>SystemMessagePromptTemplate</code>, and <span class="keep-together"><code>ChatPromptTemplate</code></span>. Then, you’ll define a prompt template with specific guidelines under <code>template</code>, instructing the LLM to generate business names. <code>ChatOpenAI()</code> initializes the chat, while <code>SystemMessagePromptTemplate.from_template(template)</code> and <code>ChatPromptTemplate.from_messages([system_prompt])</code> create your prompt template.</p>

<p>You create an LCEL <code>chain</code> by piping together <code>chat_prompt</code> and the <code>model</code>, which is then <em>invoked</em>. This replaces the <code>{industries}</code>, <code>{context}</code>, and <code>{principles}</code> placeholders in the prompt with the dictionary values within the <code>invoke</code> function.</p>

<p>Finally, you extract the LLM’s response as a string accessing the <code>.content</code> property on the <code>result</code> variable.</p>
<div data-type="tip"><h1>Give Direction and Specify Format</h1>
<p>Carefully crafted instructions might include things like “You are a creative consultant brainstorming names for businesses” and “Please generate a numerical list of five to seven catchy <a data-type="indexterm" data-primary="LCEL (LangChain Expression Language)" data-startref="lcgcxgg" id="id787"/><a data-type="indexterm" data-primary="LangChain Expression Language (LCEL)" data-startref="lcxlgau" id="id788"/><a data-type="indexterm" data-primary="Give Direction principle" data-secondary="LCEL (LangChain Expression Language) and" id="id789"/><a data-type="indexterm" data-primary="Specify Format principle" data-secondary="LCEL (LangChain Expression Language) and" id="id790"/>names for a start-up.” Cues like these guide your LLM to perform the exact task you require from it.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Using PromptTemplate with Chat Models"><div class="sect1" id="id60">
<h1>Using PromptTemplate with Chat Models</h1>

<p>LangChain provides a more traditional <a data-type="indexterm" data-primary="LangChain" data-secondary="PromptTemplate" id="id791"/><a data-type="indexterm" data-primary="PromptTemplate" id="id792"/><a data-type="indexterm" data-primary="chat models" data-secondary="PromptTemplate" id="id793"/>template called <code>PromptTemplate</code>, which requires <code>input_variables</code> and <code>template</code> arguments.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">PromptTemplate</code>
<code class="kn">from</code> <code class="nn">langchain.prompts.chat</code> <code class="kn">import</code> <code class="n">SystemMessagePromptTemplate</code>
<code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="n">prompt</code><code class="o">=</code><code class="n">PromptTemplate</code><code class="p">(</code>
 <code class="n">template</code><code class="o">=</code><code class="s1">'''You are a helpful assistant that translates </code><code class="si">{input_language}</code><code class="s1"> to</code>
 <code class="si">{output_language}</code><code class="s1">.'''</code><code class="p">,</code>
 <code class="n">input_variables</code><code class="o">=</code><code class="p">[</code><code class="s2">"input_language"</code><code class="p">,</code> <code class="s2">"output_language"</code><code class="p">],</code>
<code class="p">)</code>
<code class="n">system_message_prompt</code> <code class="o">=</code> <code class="n">SystemMessagePromptTemplate</code><code class="p">(</code><code class="n">prompt</code><code class="o">=</code><code class="n">prompt</code><code class="p">)</code>
<code class="n">chat</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>
<code class="n">chat</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">system_message_prompt</code><code class="o">.</code><code class="n">format_messages</code><code class="p">(</code>
<code class="n">input_language</code><code class="o">=</code><code class="s2">"English"</code><code class="p">,</code><code class="n">output_language</code><code class="o">=</code><code class="s2">"French"</code><code class="p">))</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">AIMessage</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="s2">"Vous êtes un assistant utile qui traduit l'anglais en</code><code class="w"/>
<code class="n">français</code><code class="o">.</code><code class="s2">", additional_kwargs=</code><code class="si">{}</code><code class="s2">, example=False)</code><code class="w"/></pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Output Parsers"><div class="sect1" id="id61">
<h1>Output Parsers</h1>

<p>In <a data-type="xref" href="ch03.html#standard_practices_03">Chapter 3</a>, you used regular expressions (regex) to extract structured data from text that contained numerical lists, but it’s possible to do this automatically in LangChain with <em>output parsers</em>.</p>

<p><em>Output parsers</em> are a higher-level abstraction provided <a data-type="indexterm" data-primary="LangChain" data-secondary="output parsers" id="lngttp"/><a data-type="indexterm" data-primary="output parsers (LangChain)" id="otptrgc"/>by LangChain for parsing structured data from LLM string responses. Currently the available output parsers are:</p>
<dl>
<dt>List parser</dt>
<dd>
<p>Returns a list of comma-separated <a data-type="indexterm" data-primary="LangChain" data-secondary="output parsers" data-tertiary="list parser" id="id794"/><a data-type="indexterm" data-primary="output parsers (LangChain)" data-secondary="list parser" id="id795"/><a data-type="indexterm" data-primary="list parser" id="id796"/>items.</p>
</dd>
<dt>Datetime parser</dt>
<dd>
<p>Parses an LLM output into datetime <a data-type="indexterm" data-primary="LangChain" data-secondary="output parsers" data-tertiary="datetime parser" id="id797"/><a data-type="indexterm" data-primary="output parsers (LangChain)" data-secondary="datetime parser" id="id798"/><a data-type="indexterm" data-primary="datetime parser" id="id799"/>format.</p>
</dd>
<dt>Enum parser</dt>
<dd>
<p>Parses strings into enum <a data-type="indexterm" data-primary="LangChain" data-secondary="output parsers" data-tertiary="enum parser" id="id800"/><a data-type="indexterm" data-primary="output parsers (LangChain)" data-secondary="enum parser" id="id801"/><a data-type="indexterm" data-primary="enum parser" id="id802"/>values.</p>
</dd>
<dt>Auto-fixing parser</dt>
<dd>
<p>Wraps another output parser, and if that <a data-type="indexterm" data-primary="LangChain" data-secondary="output parsers" data-tertiary="auto-fixing parser" id="id803"/><a data-type="indexterm" data-primary="output parsers (LangChain)" data-secondary="auto-fixing parser" id="id804"/><a data-type="indexterm" data-primary="auto-fixing parser" id="id805"/>output parser fails, it will call another LLM to fix any errors.</p>
</dd>
<dt>Pydantic (JSON) parser</dt>
<dd>
<p>Parses LLM responses into JSON output that <a data-type="indexterm" data-primary="LangChain" data-secondary="output parsers" data-tertiary="Pydantic (JSON) parser" id="id806"/><a data-type="indexterm" data-primary="output parsers (LangChain)" data-secondary="Pydantic (JSON) parser" id="id807"/><a data-type="indexterm" data-primary="Pydantic (JSON) parser" id="id808"/>conforms to a Pydantic schema.</p>
</dd>
<dt>Retry parser</dt>
<dd>
<p>Provides retrying a failed parse <a data-type="indexterm" data-primary="LangChain" data-secondary="output parsers" data-tertiary="retry parser" id="id809"/><a data-type="indexterm" data-primary="output parsers (LangChain)" data-secondary="retry parser" id="id810"/><a data-type="indexterm" data-primary="retry parser" id="id811"/>from a previous output parser.</p>
</dd>
<dt>Structured output parser</dt>
<dd>
<p>Can be used when you want to <a data-type="indexterm" data-primary="LangChain" data-secondary="output parsers" data-tertiary="structured output parser" id="id812"/><a data-type="indexterm" data-primary="output parsers (LangChain)" data-secondary="structured output parser" id="id813"/><a data-type="indexterm" data-primary="structured output parser" id="id814"/>return multiple fields.</p>
</dd>
<dt>XML parser</dt>
<dd>
<p>Parses LLM responses into an XML-based <a data-type="indexterm" data-primary="LangChain" data-secondary="output parsers" data-tertiary="XML parser" id="id815"/><a data-type="indexterm" data-primary="output parsers (LangChain)" data-secondary="XML parser" id="id816"/><a data-type="indexterm" data-primary="XML parser" id="id817"/>format.</p>
</dd>
</dl>

<p>As you’ll discover, there are two important functions <a data-type="indexterm" data-primary="LangChain" data-secondary="output parsers" data-tertiary="functions" id="id818"/><a data-type="indexterm" data-primary="output parsers (LangChain)" data-secondary="functions" id="id819"/>for LangChain output parsers:</p>
<dl>
<dt><code>.get_format_instructions()</code></dt>
<dd>
<p>This function provides the necessary instructions into your prompt to output a structured format that can be parsed.</p>
</dd>
<dt><code>.parse(llm_output: str)</code></dt>
<dd>
<p>This function is responsible for parsing your LLM responses into a predefined format.</p>
</dd>
</dl>

<p>Generally, you’ll find that the Pydantic (JSON) parser with <code>ChatOpenAI()</code> provides the most flexibility.</p>

<p>The Pydantic (JSON) parser takes advantage of the <a href="https://oreil.ly/QIMih">Pydantic</a> library in Python. Pydantic is a data validation library that provides a way to validate incoming data using Python type annotations. This means that Pydantic allows you to create schemas for your data and automatically validates and parses input data according to those schemas.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.prompts.chat</code> <code class="kn">import</code> <code class="p">(</code>
    <code class="n">ChatPromptTemplate</code><code class="p">,</code>
    <code class="n">SystemMessagePromptTemplate</code><code class="p">,</code>
<code class="p">)</code>
<code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langchain.output_parsers</code> <code class="kn">import</code> <code class="n">PydanticOutputParser</code>
<code class="kn">from</code> <code class="nn">pydantic.v1</code> <code class="kn">import</code> <code class="n">BaseModel</code><code class="p">,</code> <code class="n">Field</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code>

<code class="n">temperature</code> <code class="o">=</code> <code class="mf">0.0</code>

<code class="k">class</code> <code class="nc">BusinessName</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">):</code>
    <code class="n">name</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code><code class="n">description</code><code class="o">=</code><code class="s2">"The name of the business"</code><code class="p">)</code>
    <code class="n">rating_score</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code><code class="n">description</code><code class="o">=</code><code class="s1">'''The rating score of the</code>
<code class="s1">    business. 0 is the worst, 10 is the best.'''</code><code class="p">)</code>

<code class="k">class</code> <code class="nc">BusinessNames</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">):</code>
    <code class="n">names</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">BusinessName</code><code class="p">]</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code><code class="n">description</code><code class="o">=</code><code class="s1">'''A list</code>
<code class="s1">    of busines names'''</code><code class="p">)</code>

<code class="c1"># Set up a parser + inject instructions into the prompt template:</code>
<code class="n">parser</code> <code class="o">=</code> <code class="n">PydanticOutputParser</code><code class="p">(</code><code class="n">pydantic_object</code><code class="o">=</code><code class="n">BusinessNames</code><code class="p">)</code>

<code class="n">principles</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">- The name must be easy to remember.</code>
<code class="s2">- Use the </code><code class="si">{industry}</code><code class="s2"> industry and Company context to create an effective name.</code>
<code class="s2">- The name must be easy to pronounce.</code>
<code class="s2">- You must only return the name without any other text or characters.</code>
<code class="s2">- Avoid returning full stops, </code><code class="se">\n</code><code class="s2">, or any other characters.</code>
<code class="s2">- The maximum length of the name must be 10 characters.</code>
<code class="s2">"""</code>

<code class="c1"># Chat Model Output Parser:</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>
<code class="n">template</code> <code class="o">=</code> <code class="s2">"""Generate five business names for a new start-up company in the</code>
<code class="si">{industry}</code><code class="s2"> industry.</code>
<code class="s2">You must follow the following principles: </code><code class="si">{principles}</code><code class="s2"/>
<code class="si">{format_instructions}</code><code class="s2"/>
<code class="s2">"""</code>
<code class="n">system_message_prompt</code> <code class="o">=</code> <code class="n">SystemMessagePromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="n">template</code><code class="p">)</code>
<code class="n">chat_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">([</code><code class="n">system_message_prompt</code><code class="p">])</code>

<code class="c1"># Creating the LCEL chain:</code>
<code class="n">prompt_and_model</code> <code class="o">=</code> <code class="n">chat_prompt</code> <code class="o">|</code> <code class="n">model</code>

<code class="n">result</code> <code class="o">=</code> <code class="n">prompt_and_model</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code>
    <code class="p">{</code>
        <code class="s2">"principles"</code><code class="p">:</code> <code class="n">principles</code><code class="p">,</code>
        <code class="s2">"industry"</code><code class="p">:</code> <code class="s2">"Data Science"</code><code class="p">,</code>
        <code class="s2">"format_instructions"</code><code class="p">:</code> <code class="n">parser</code><code class="o">.</code><code class="n">get_format_instructions</code><code class="p">(),</code>
    <code class="p">}</code>
<code class="p">)</code>
<code class="c1"># The output parser, parses the LLM response into a Pydantic object:</code>
<code class="nb">print</code><code class="p">(</code><code class="n">parser</code><code class="o">.</code><code class="n">parse</code><code class="p">(</code><code class="n">result</code><code class="o">.</code><code class="n">content</code><code class="p">))</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">names</code><code class="o">=</code><code class="p">[</code><code class="n">BusinessName</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'DataWiz'</code><code class="p">,</code> <code class="n">rating_score</code><code class="o">=</code><code class="mf">8.5</code><code class="p">),</code>
<code class="n">BusinessName</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'InsightIQ'</code><code class="p">,</code>
<code class="n">rating_score</code><code class="o">=</code><code class="mf">9.2</code><code class="p">),</code> <code class="n">BusinessName</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'AnalytiQ'</code><code class="p">,</code> <code class="n">rating_score</code><code class="o">=</code><code class="mf">7.8</code><code class="p">),</code>
<code class="n">BusinessName</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'SciData'</code><code class="p">,</code> <code class="n">rating_score</code><code class="o">=</code><code class="mf">8.1</code><code class="p">),</code>
<code class="n">BusinessName</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'InfoMax'</code><code class="p">,</code> <code class="n">rating_score</code><code class="o">=</code><code class="mf">9.5</code><code class="p">)]</code></pre>

<p>After you’ve loaded the necessary libraries, you’ll set up a ChatOpenAI model. Then create <code>SystemMessagePromptTemplate</code> from your template and form a <code>ChatPromptTemplate</code> with it. You’ll use the Pydantic models <code>BusinessName</code> and <code>BusinessNames</code> to structure your desired output, a list of unique business names. You’ll create a <code>Pydantic</code> parser for parsing these models and format the prompt using user-inputted variables by calling the <code>invoke</code> function. Feeding this customized prompt to your model, you’re enabling it to produce creative, unique business names by using the <code>parser</code>.</p>

<p>It’s possible to use output parsers inside of LCEL by using this syntax:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">chain</code> <code class="o">=</code> <code class="n">prompt</code> <code class="o">|</code> <code class="n">model</code> <code class="o">|</code> <code class="n">output_parser</code></pre>

<p>Let’s add the output parser directly to the chain.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">parser</code> <code class="o">=</code> <code class="n">PydanticOutputParser</code><code class="p">(</code><code class="n">pydantic_object</code><code class="o">=</code><code class="n">BusinessNames</code><code class="p">)</code>
<code class="n">chain</code> <code class="o">=</code> <code class="n">chat_prompt</code> <code class="o">|</code> <code class="n">model</code> <code class="o">|</code> <code class="n">parser</code>

<code class="n">result</code> <code class="o">=</code> <code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code>
    <code class="p">{</code>
        <code class="s2">"principles"</code><code class="p">:</code> <code class="n">principles</code><code class="p">,</code>
        <code class="s2">"industry"</code><code class="p">:</code> <code class="s2">"Data Science"</code><code class="p">,</code>
        <code class="s2">"format_instructions"</code><code class="p">:</code> <code class="n">parser</code><code class="o">.</code><code class="n">get_format_instructions</code><code class="p">(),</code>
    <code class="p">}</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">result</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">names=[BusinessName(name='DataTech', rating_score=9.5),...]</pre>

<p>The chain is now responsible for prompt formatting, LLM calling, and parsing the LLM’s response into a <code>Pydantic</code> object.</p>
<div data-type="tip"><h1>Specify Format</h1>
<p>The preceding prompts <a data-type="indexterm" data-primary="Specify Format principle" data-secondary="output parsers" id="id820"/>use Pydantic models and output parsers, allowing you explicitly tell an LLM your desired response format.</p>
</div>

<p>It’s worth knowing that by asking an LLM to provide structured JSON output, you can create a flexible and generalizable API from the LLM’s response. There are limitations to this, such as the size of the JSON created and the reliability of your prompts, but it still is a promising area for LLM applications.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>You should take care of edge cases as well as adding error handling statements, since LLM outputs might not always be in your desired format.</p>
</div>

<p>Output parsers save you from the complexity and intricacy of regular expressions, providing easy-to-use functionalities for a variety of use cases. Now that you’ve seen them in action, you can utilize output parsers to effortlessly structure and retrieve the data you need from an LLM’s output, harnessing the full potential of AI for your tasks.</p>

<p>Furthermore, using parsers to structure the data extracted from LLMs allows you to easily choose how to organize outputs for <a data-type="indexterm" data-primary="LangChain" data-secondary="output parsers" data-startref="lngttp" id="id821"/><a data-type="indexterm" data-primary="output parsers (LangChain)" data-startref="otptrgc" id="id822"/>more efficient use. This can be useful if you’re dealing with extensive lists and need to sort them by certain criteria, like business names.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="LangChain Evals"><div class="sect1" id="id62">
<h1>LangChain Evals</h1>

<p>As well as output parsers to check for <a data-type="indexterm" data-primary="eval metrics" data-secondary="LangChain" id="evlchn"/><a data-type="indexterm" data-primary="LangChain" data-secondary="eval metrics" id="lgcnvls"/>formatting errors, most AI systems also make use of <em>evals</em>, or evaluation metrics, to measure the performance of each prompt response. LangChain has a number of off-the-shelf evaluators, which can be directly be logged in their <a href="https://oreil.ly/0Fn94">LangSmith</a> platform for further debugging, monitoring, and testing. <a href="https://wandb.ai/site">Weights and Biases</a> is alternative machine learning platform that offers similar functionality and tracing capabilities for LLMs.</p>

<p>Evaluation metrics are useful for <a data-type="indexterm" data-primary="evaluation metrics" data-see="eval metrics" id="id823"/>more than just prompt testing, as they can be used to identify positive and negative examples for retrieval as well as to build datasets for fine-tuning custom models.</p>

<p>Most eval metrics rely on a set of test cases, which are input and output pairings where you know the correct answer. Often these reference answers are created or curated manually by a human, but it’s also common practice to use a smarter model like GPT-4 to generate the ground truth answers, which has been done for the following example. Given a list of descriptions of financial transactions, we used GPT-4 to classify each transaction with a <code>transaction_category</code> and <code>transaction_type</code>. The process can be found in the <code>langchain-evals.ipynb</code> Jupyter Notebook in the <a href="https://oreil.ly/a4Hut">GitHub repository</a> for the book.</p>

<p>With the GPT-4 answer being taken as the correct answer, it’s now possible to rate the accuracy of smaller models like GPT-3.5-turbo and Mixtral 8x7b (called <code>mistral-small</code> in the API). If you can achieve good enough accuracy with a smaller model, you can save money or decrease latency. In addition, if that model is available open source like <a href="https://oreil.ly/Ec578">Mistral’s model</a>, you can migrate that task to run on your own servers, avoiding sending potentially sensitive data outside of your organization. We recommend testing with an external API first, before going to the trouble of self-hosting an OS model.</p>

<p><a href="https://mistral.ai">Remember to sign up</a> and subscribe to obtain an API key; then expose that as an environment variable by typing in your terminal:</p>
<ul class="simplelist">
<li><code><strong>export MISTRAL_API_KEY=api-key</strong></code></li>
</ul>

<p>The following script is part of a <a href="https://oreil.ly/DqDOf">notebook</a> that has previously defined a dataframe <code>df</code>. For <a data-type="indexterm" data-primary="Mistral, evaluation metrics" id="mstvltrc"/><a data-type="indexterm" data-primary="eval metrics" data-secondary="Mistral" id="evmmst"/>brevity let’s investigate only the evaluation section of the script, assuming a dataframe is already defined.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">os</code>
<code class="kn">from</code> <code class="nn">langchain_mistralai.chat_models</code> <code class="kn">import</code> <code class="n">ChatMistralAI</code>
<code class="kn">from</code> <code class="nn">langchain.output_parsers</code> <code class="kn">import</code> <code class="n">PydanticOutputParser</code>
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>
<code class="kn">from</code> <code class="nn">pydantic.v1</code> <code class="kn">import</code> <code class="n">BaseModel</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Literal</code><code class="p">,</code> <code class="n">Union</code>
<code class="kn">from</code> <code class="nn">langchain_core.output_parsers</code> <code class="kn">import</code> <code class="n">StrOutputParser</code>

<code class="c1"># 1. Define the model:</code>
<code class="n">mistral_api_key</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="p">[</code><code class="s2">"MISTRAL_API_KEY"</code><code class="p">]</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">ChatMistralAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"mistral-small"</code><code class="p">,</code> <code class="n">mistral_api_key</code><code class="o">=</code><code class="n">mistral_api_key</code><code class="p">)</code>

<code class="c1"># 2. Define the prompt:</code>
<code class="n">system_prompt</code> <code class="o">=</code> <code class="s2">"""You are are an expert at analyzing</code>
<code class="s2">bank transactions, you will be categorizing a single</code>
<code class="s2">transaction.</code>
<code class="s2">Always return a transaction type and category:</code>
<code class="s2">do not return None.</code>
<code class="s2">Format Instructions:</code>
<code class="si">{format_instructions}</code><code class="s2">"""</code>

<code class="n">user_prompt</code> <code class="o">=</code> <code class="s2">"""Transaction Text:</code>
<code class="si">{transaction}</code><code class="s2">"""</code>

<code class="n">prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">(</code>
    <code class="p">[</code>
        <code class="p">(</code>
            <code class="s2">"system"</code><code class="p">,</code>
            <code class="n">system_prompt</code><code class="p">,</code>
        <code class="p">),</code>
        <code class="p">(</code>
            <code class="s2">"user"</code><code class="p">,</code>
            <code class="n">user_prompt</code><code class="p">,</code>
        <code class="p">),</code>
    <code class="p">]</code>
<code class="p">)</code>

<code class="c1"># 3. Define the pydantic model:</code>
<code class="k">class</code> <code class="nc">EnrichedTransactionInformation</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">):</code>
    <code class="n">transaction_type</code><code class="p">:</code> <code class="n">Union</code><code class="p">[</code>
        <code class="n">Literal</code><code class="p">[</code><code class="s2">"Purchase"</code><code class="p">,</code> <code class="s2">"Withdrawal"</code><code class="p">,</code> <code class="s2">"Deposit"</code><code class="p">,</code>
        <code class="s2">"Bill Payment"</code><code class="p">,</code> <code class="s2">"Refund"</code><code class="p">],</code> <code class="kc">None</code>
    <code class="p">]</code>
    <code class="n">transaction_category</code><code class="p">:</code> <code class="n">Union</code><code class="p">[</code>
        <code class="n">Literal</code><code class="p">[</code><code class="s2">"Food"</code><code class="p">,</code> <code class="s2">"Entertainment"</code><code class="p">,</code> <code class="s2">"Transport"</code><code class="p">,</code>
        <code class="s2">"Utilities"</code><code class="p">,</code> <code class="s2">"Rent"</code><code class="p">,</code> <code class="s2">"Other"</code><code class="p">],</code>
        <code class="kc">None</code><code class="p">,</code>
    <code class="p">]</code>


<code class="c1"># 4. Define the output parser:</code>
<code class="n">output_parser</code> <code class="o">=</code> <code class="n">PydanticOutputParser</code><code class="p">(</code>
    <code class="n">pydantic_object</code><code class="o">=</code><code class="n">EnrichedTransactionInformation</code><code class="p">)</code>

<code class="c1"># 5. Define a function to try to fix and remove the backslashes:</code>
<code class="k">def</code> <code class="nf">remove_back_slashes</code><code class="p">(</code><code class="n">string</code><code class="p">):</code>
    <code class="c1"># double slash to escape the slash</code>
    <code class="n">cleaned_string</code> <code class="o">=</code> <code class="n">string</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"</code><code class="se">\\</code><code class="s2">"</code><code class="p">,</code> <code class="s2">""</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">cleaned_string</code>

<code class="c1"># 6. Create an LCEL chain that fixes the formatting:</code>
<code class="n">chain</code> <code class="o">=</code> <code class="n">prompt</code> <code class="o">|</code> <code class="n">model</code> <code class="o">|</code> <code class="n">StrOutputParser</code><code class="p">()</code> \
<code class="o">|</code> <code class="n">remove_back_slashes</code> <code class="o">|</code> <code class="n">output_parser</code>

<code class="n">transaction</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="s2">"Transaction Description"</code><code class="p">]</code>
<code class="n">result</code> <code class="o">=</code> <code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code>
        <code class="p">{</code>
            <code class="s2">"transaction"</code><code class="p">:</code> <code class="n">transaction</code><code class="p">,</code>
            <code class="s2">"format_instructions"</code><code class="p">:</code> \
            <code class="n">output_parser</code><code class="o">.</code><code class="n">get_format_instructions</code><code class="p">(),</code>
        <code class="p">}</code>
    <code class="p">)</code>

<code class="c1"># 7. Invoke the chain for the whole dataset:</code>
<code class="n">results</code> <code class="o">=</code> <code class="p">[]</code>

<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">iterrows</code><code class="p">(),</code> <code class="n">total</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">df</code><code class="p">)):</code>
    <code class="n">transaction</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="s2">"Transaction Description"</code><code class="p">]</code>
    <code class="k">try</code><code class="p">:</code>
        <code class="n">result</code> <code class="o">=</code> <code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code>
            <code class="p">{</code>
                <code class="s2">"transaction"</code><code class="p">:</code> <code class="n">transaction</code><code class="p">,</code>
                <code class="s2">"format_instructions"</code><code class="p">:</code> \
                <code class="n">output_parser</code><code class="o">.</code><code class="n">get_format_instructions</code><code class="p">(),</code>
            <code class="p">}</code>
        <code class="p">)</code>
    <code class="k">except</code><code class="p">:</code>
        <code class="n">result</code> <code class="o">=</code> <code class="n">EnrichedTransactionInformation</code><code class="p">(</code>
            <code class="n">transaction_type</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
            <code class="n">transaction_category</code><code class="o">=</code><code class="kc">None</code>
        <code class="p">)</code>

    <code class="n">results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">result</code><code class="p">)</code>

<code class="c1"># 8. Add the results to the dataframe, as columns transaction type and</code>
<code class="c1"># transaction category:</code>
<code class="n">transaction_types</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">transaction_categories</code> <code class="o">=</code> <code class="p">[]</code>

<code class="k">for</code> <code class="n">result</code> <code class="ow">in</code> <code class="n">results</code><code class="p">:</code>
    <code class="n">transaction_types</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">result</code><code class="o">.</code><code class="n">transaction_type</code><code class="p">)</code>
    <code class="n">transaction_categories</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
        <code class="n">result</code><code class="o">.</code><code class="n">transaction_category</code><code class="p">)</code>

<code class="n">df</code><code class="p">[</code><code class="s2">"mistral_transaction_type"</code><code class="p">]</code> <code class="o">=</code> <code class="n">transaction_types</code>
<code class="n">df</code><code class="p">[</code><code class="s2">"mistral_transaction_category"</code><code class="p">]</code> <code class="o">=</code> <code class="n">transaction_categories</code>
<code class="n">df</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre>

<p>Output:</p>

<pre data-type="programlisting">Transaction Description	transaction_type
transaction_category	mistral_transaction_type
mistral_transaction_category
0	cash deposit at local branch	Deposit	Other	Deposit
Other
1	cash deposit at local branch	Deposit	Other	Deposit
Other
2	withdrew money for rent payment	Withdrawal	Rent
Withdrawal	Rent
3	withdrew cash for weekend expenses	Withdrawal	Other
Withdrawal	Other
4	purchased books from the bookstore	Purchase	Other
Purchase	Entertainment</pre>

<p>The code does the following:</p>
<ol>
<li>
<p><code>from langchain_mistralai.chat_models import ChatMistralAI</code>: We import LangChain’s Mistral implementation.</p>
</li>
<li>
<p><code>from langchain.output_parsers import PydanticOutputParser</code>: Imports the <code>PydanticOutputParser</code> class, which is used for parsing output using Pydantic models. We also import a string output parser to handle an interim step where we remove backslashes from the JSON key (a common problem with responses from Mistral).</p>
</li>
<li>
<p><code>mistral_api_key = os.environ["MISTRAL_API_KEY"]</code>: Retrieves the Mistral API key from the environment variables. This needs to be set prior to running the notebook.</p>
</li>
<li>
<p><code>model = ChatMistralAI(model="mistral-small", mistral_api_key=mistral_api_key)</code>: Initializes an instance of <code>ChatMistralAI</code> with the specified model and API key. Mistral Small is what they call the Mixtral 8x7b model (also available open source) in their API.</p>
</li>
<li>
<p><code>system_prompt</code> and <code>user_prompt</code>: These lines define templates for the system and user prompts used in the chat to classify the transactions.</p>
</li>
<li>
<p><code>class EnrichedTransactionInformation(BaseModel)</code>: Defines a Pydantic model <code>EnrichedTransactionInformation</code> with two fields: <code>transaction_type</code> and <code>transaction_category</code>, each with specific allowed values and the possibility of being <code>None</code>. This is what tells us if the output is in the correct format.</p>
</li>
<li>
<p><code>def remove_back_slashes(string)</code>: Defines a function to remove backslashes from a string.</p>
</li>
<li>
<p><code>chain = prompt | model | StrOutputParser() | remove_back_slashes | output_parser</code>: Updates the chain to include a string output parser and the <code>remove_back_slashes</code> function before the original output parser.</p>
</li>
<li>
<p><code>transaction = df.iloc[0]["Transaction Description"]</code>: Extracts the first transaction description from a dataframe <code>df</code>. This dataframe is loaded earlier in the <a href="https://oreil.ly/-koAO">Jupyter Notebook</a> (omitted for brevity).</p>
</li>
<li>
<p><code>for i, row in tqdm(df.iterrows(), total=len(df))</code>: Iterates over each row in the dataframe <code>df</code>, with a progress bar.</p>
</li>
<li>
<p><code>result = chain.invoke(...)</code>: Inside the loop, the chain is invoked for each transaction.</p>
</li>
<li>
<p><code>except</code>: In case of an exception, a default <code>EnrichedTransactionInformation</code> object with <code>None</code> values is created. These will be treated as errors in evaluation but will not break the processing loop.</p>
</li>
<li>
<p><code>df["mistral_transaction_type"] = transaction_types</code>, <code>df["mistral_transaction_category"] = transaction_categories</code>: Adds the transaction types and categories as new columns in the dataframe, which we then display with <code>df.head()</code>.</p>
</li>

</ol>

<p>With the responses from Mistral saved in <a data-type="indexterm" data-primary="Mistral, evaluation metrics" data-startref="mstvltrc" id="id824"/><a data-type="indexterm" data-primary="eval metrics" data-secondary="Mistral" data-startref="evmmst" id="id825"/>the dataframe, it’s possible to compare them to the transaction categories and types defined earlier to check the accuracy of Mistral. The most basic LangChain eval metric is to do an exact string match of a prediction against a reference answer, which returns a score of 1 if correct, and a 0 if incorrect. The notebook gives an example of how to <a href="https://oreil.ly/vPUfI">implement this</a>, which shows that Mistral’s accuracy is 77.5%. However, if all you are doing is comparing strings, you probably don’t need to implement it in LangChain.</p>

<p>Where LangChain is valuable is in its standardized and tested approaches to implementing more advanced evaluators using LLMs. The evaluator <code>labeled_pairwise_string</code> compares two outputs and gives a reason for choosing between them, using GPT-4. One common use case for this type of evaluator is to compare the outputs from two different prompts or models, particularly if the models being tested are less sophisticated than GPT-4. This evaluator using GPT-4 does still work for evaluating GPT-4 responses, but you should manually review the reasoning and scores to ensure it is doing a good job: if GPT-4 is bad at a task, it may also be bad at evaluating that task. In <a href="https://oreil.ly/9O7Mb">the notebook</a>, the same transaction classification was run again with the model changed to <code>model = ChatOpenAI(model="gpt-3.5-turbo-1106", model_kwargs={"response_format": {"type": "json_object"}},)</code>. Now it’s possible to do pairwise comparison between the Mistral and GPT-3.5 responses, as shown in the following example. You can see in the output the reasoning that is given to justify the score.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Evaluate answers using LangChain evaluators:</code>
<code class="kn">from</code> <code class="nn">langchain.evaluation</code> <code class="kn">import</code> <code class="n">load_evaluator</code>
<code class="n">evaluator</code> <code class="o">=</code> <code class="n">load_evaluator</code><code class="p">(</code><code class="s2">"labeled_pairwise_string"</code><code class="p">)</code>

<code class="n">row</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="n">transaction</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="s2">"Transaction Description"</code><code class="p">]</code>
<code class="n">gpt3pt5_category</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="s2">"gpt3.5_transaction_category"</code><code class="p">]</code>
<code class="n">gpt3pt5_type</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="s2">"gpt3.5_transaction_type"</code><code class="p">]</code>
<code class="n">mistral_category</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="s2">"mistral_transaction_category"</code><code class="p">]</code>
<code class="n">mistral_type</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="s2">"mistral_transaction_type"</code><code class="p">]</code>
<code class="n">reference_category</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="s2">"transaction_category"</code><code class="p">]</code>
<code class="n">reference_type</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="s2">"transaction_type"</code><code class="p">]</code>

<code class="c1"># Put the data into JSON format for the evaluator:</code>
<code class="n">gpt3pt5_data</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"""</code><code class="se">{{</code><code class="s2"/>
<code class="s2">    "transaction_category": "</code><code class="si">{</code><code class="n">gpt3pt5_category</code><code class="si">}</code><code class="s2">",</code>
<code class="s2">    "transaction_type": "</code><code class="si">{</code><code class="n">gpt3pt5_type</code><code class="si">}</code><code class="s2">"</code>
<code class="se">}}</code><code class="s2">"""</code>

<code class="n">mistral_data</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"""</code><code class="se">{{</code><code class="s2"/>
<code class="s2">    "transaction_category": "</code><code class="si">{</code><code class="n">mistral_category</code><code class="si">}</code><code class="s2">",</code>
<code class="s2">    "transaction_type": "</code><code class="si">{</code><code class="n">mistral_type</code><code class="si">}</code><code class="s2">"</code>
<code class="se">}}</code><code class="s2">"""</code>

<code class="n">reference_data</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"""</code><code class="se">{{</code><code class="s2"/>
<code class="s2">    "transaction_category": "</code><code class="si">{</code><code class="n">reference_category</code><code class="si">}</code><code class="s2">",</code>
<code class="s2">    "transaction_type": "</code><code class="si">{</code><code class="n">reference_type</code><code class="si">}</code><code class="s2">"</code>
<code class="se">}}</code><code class="s2">"""</code>

<code class="c1"># Set up the prompt input for context for the evaluator:</code>
<code class="n">input_prompt</code> <code class="o">=</code> <code class="s2">"""You are an expert at analyzing bank</code>
<code class="s2">transactions,</code>
<code class="s2">you will be categorizing a single transaction.</code>
<code class="s2">Always return a transaction type and category: do not</code>
<code class="s2">return None.</code>
<code class="s2">Format Instructions:</code>
<code class="si">{format_instructions}</code><code class="s2"/>
<code class="s2">Transaction Text:</code>
<code class="si">{transaction}</code><code class="s2"/>
<code class="s2">"""</code>

<code class="n">transaction_types</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">transaction_type_score</code><code class="p">)</code>
<code class="n">transaction_categories</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
    <code class="n">transaction_category_score</code><code class="p">)</code>

<code class="n">accuracy_score</code> <code class="o">=</code> <code class="mi">0</code>

<code class="k">for</code> <code class="n">transaction_type_score</code><code class="p">,</code> <code class="n">transaction_category_score</code> \
    <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code>
        <code class="n">transaction_types</code><code class="p">,</code> <code class="n">transaction_categories</code>
    <code class="p">):</code>
    <code class="n">accuracy_score</code> <code class="o">+=</code> <code class="n">transaction_type_score</code><code class="p">[</code><code class="s1">'score'</code><code class="p">]</code> <code class="o">+</code> \
    <code class="n">transaction_category_score</code><code class="p">[</code><code class="s1">'score'</code><code class="p">]</code>

<code class="n">accuracy_score</code> <code class="o">=</code> <code class="n">accuracy_score</code> <code class="o">/</code> <code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">transaction_types</code><code class="p">)</code> \
    <code class="o">*</code> <code class="mi">2</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Accuracy score: </code><code class="si">{</code><code class="n">accuracy_score</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

<code class="n">evaluator</code><code class="o">.</code><code class="n">evaluate_string_pairs</code><code class="p">(</code>
    <code class="n">prediction</code><code class="o">=</code><code class="n">gpt3pt5_data</code><code class="p">,</code>
    <code class="n">prediction_b</code><code class="o">=</code><code class="n">mistral_data</code><code class="p">,</code>
    <code class="nb">input</code><code class="o">=</code><code class="n">input_prompt</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
        <code class="n">format_instructions</code><code class="o">=</code><code class="n">output_parser</code><code class="o">.</code><code class="n">get_format_instructions</code><code class="p">(),</code>
        <code class="n">transaction</code><code class="o">=</code><code class="n">transaction</code><code class="p">),</code>
    <code class="n">reference</code><code class="o">=</code><code class="n">reference_data</code><code class="p">,</code>
<code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">{'reasoning': '''Both Assistant A and Assistant B provided the exact same
response to the user\'s question. Their responses are both helpful, relevant,
correct, and demonstrate depth of thought. They both correctly identified the
transaction type as "Deposit" and the transaction category as "Other" based on
the transaction text provided by the user. Both responses are also
well-formatted according to the JSON schema provided by the user. Therefore,
it\'s a tie between the two assistants. \n\nFinal Verdict: [[C]]''',
 'value': None,
 'score': 0.5}</pre>

<p>This code demonstrates the simple exact string matching evaluator from LangChain:</p>
<ol>
<li>
<p><code>evaluator = load_evaluator("labeled_pairwise_string")</code>: This is a helper function that can be used to load any LangChain evaluator by name. In this case, it is the <code>labeled_pairwise_string</code> evaluator being used.</p>
</li>
<li>
<p><code>row = df.iloc[0]</code>: This line and the seven lines that follow get the first row and extract the values for the different columns needed. It includes the transaction description, as well as the Mistral and GPT-3.5 transaction category and types. This is showcasing a single transaction, but this code can easily run in a loop through each transaction, replacing this line with an <code>iterrows</code> function <code>for i, row in tqdm(df.iterrows(), total=len(df)):</code>, as is done later in <a href="https://oreil.ly/dcCOO">the notebook</a>.</p>
</li>
<li>
<p><code>gpt3pt5_data = f"""{{</code>: To use the pairwise comparison evaluator, we need to pass the results in a way that is formatted correctly for the prompt. This is done for Mistral and GPT-3.5, as well as the reference data.</p>
</li>
<li>
<p><code>input_prompt = """You are an expert...</code>: The other formatting we have to get right is in the prompt. To get accurate evaluation scores, the evaluator needs to see the instructions that were given for the task.</p>
</li>
<li>
<p><code>evaluator.evaluate_string_pairs(...</code>: All that remains is to run the evaluator by passing in the <code>prediction</code> and <code>prediction_b</code> (GPT-3.5 and Mistral, respectively), as well as the <code>input</code> prompt, and <code>reference</code> data, which serves as the ground truth.</p>
</li>
<li>
<p>Following this code <a href="https://oreil.ly/hW8Wr">in the notebook</a>, there is an example of looping through and running the evaluator on every row in the dataframe and then saving the results and reasoning back to the dataframe.</p>
</li>

</ol>

<p>This example demonstrates how to <a data-type="indexterm" data-primary="eval metrics" data-secondary="string distance" id="id826"/><a data-type="indexterm" data-primary="eval metrics" data-secondary="embedding distance" id="id827"/><a data-type="indexterm" data-primary="eval metrics" data-secondary="Levenshtein" id="id828"/>use a LangChain evaluator, but there are many different kinds of evaluator available. String distance (<a href="https://oreil.ly/Al5G3">Levenshtein</a>) or <a href="https://oreil.ly/0p_nE">embedding distance</a> evaluators are often used in scenarios where answers are not an exact match for the reference answer, but only need to be close enough semantically. Levenshtein distance allows for fuzzy matches based on how many single-character edits would be needed to transform the predicted text into the reference text, and embedding distance makes use of vectors (covered in <a data-type="xref" href="ch05.html#vector_databases_05">Chapter 5</a>) to calculate similarity between the answer and reference.</p>

<p>The other kind of evaluator we often use in <a data-type="indexterm" data-primary="eval metrics" data-secondary="pairwise comparisons" id="id829"/><a data-type="indexterm" data-primary="pairwise comparisons" id="id830"/>our work is pairwise comparisons, which are useful for comparing two different prompts or models, using a smarter model like GPT-4. This type of comparison is helpful because reasoning is provided for each comparison, which can be useful in debugging why one approach was favored over another. The <a href="https://oreil.ly/iahTJ">notebook for this section</a> shows an example of using a pairwise comparison evaluator to check GPT-3.5-turbo’s accuracy versus Mixtral 8x7b.</p>
<div data-type="tip"><h1>Evaluate Quality</h1>
<p>Without defining an appropriate set of eval metrics to define success, it can be difficult to tell if changes to the prompt or wider system are improving or harming the quality of responses. If you can automate eval metrics using smart models like GPT-4, you can iterate faster to <a data-type="indexterm" data-primary="eval metrics" data-secondary="LangChain" data-startref="evlchn" id="id831"/><a data-type="indexterm" data-primary="LangChain" data-secondary="eval metrics" data-startref="lgcnvls" id="id832"/><a data-type="indexterm" data-primary="Evaluate Quality principle" data-secondary="LangChain evals" id="id833"/>improve results without costly or time-consuming manual human review.</p>
</div>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="OpenAI Function Calling"><div class="sect1" id="id63">
<h1>OpenAI Function Calling</h1>

<p><em>Function calling</em> provides an alternative <a data-type="indexterm" data-primary="OpenAI" data-secondary="function calling" id="poafccl"/><a data-type="indexterm" data-primary="function calling" id="fccopai"/>method to output parsers, leveraging fine-tuned OpenAI models. These models identify when a function should be executed and generate a JSON response with the <em>name and arguments</em> for a predefined function. Several use cases include:</p>
<dl>
<dt>Designing sophisticated chat bots</dt>
<dd>
<p>Capable of organizing and managing schedules. For example, you can define a function to schedule a meeting: <code>schedule_meeting(date: str, time: str, attendees: List[str])</code>.</p>
</dd>
<dt>Convert natural language into actionable API calls</dt>
<dd>
<p>A command like “Turn on the hallway lights” can be converted to <code>control_device(device: str, action: 'on' | 'off')</code> for interacting with your home automation API.</p>
</dd>
<dt>Extracting structured data</dt>
<dd>
<p>This could be done by defining a function such as <code>extract_contextual_data(context: str, data_points: List[str])</code> or <code>search_database(query: str)</code>.</p>
</dd>
</dl>

<p>Each function that you use within function <a data-type="indexterm" data-primary="JSON Schema, function calling and" id="id834"/>calling will require an appropriate <em>JSON schema</em>. Let’s explore an example with the <code>OpenAI</code> package:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">OpenAI</code>
<code class="kn">import</code> <code class="nn">json</code>
<code class="kn">from</code> <code class="nn">os</code> <code class="kn">import</code> <code class="n">getenv</code>

<code class="k">def</code> <code class="nf">schedule_meeting</code><code class="p">(</code><code class="n">date</code><code class="p">,</code> <code class="n">time</code><code class="p">,</code> <code class="n">attendees</code><code class="p">):</code>
    <code class="c1"># Connect to calendar service:</code>
    <code class="k">return</code> <code class="p">{</code> <code class="s2">"event_id"</code><code class="p">:</code> <code class="s2">"1234"</code><code class="p">,</code> <code class="s2">"status"</code><code class="p">:</code> <code class="s2">"Meeting scheduled successfully!"</code><code class="p">,</code>
            <code class="s2">"date"</code><code class="p">:</code> <code class="n">date</code><code class="p">,</code> <code class="s2">"time"</code><code class="p">:</code> <code class="n">time</code><code class="p">,</code> <code class="s2">"attendees"</code><code class="p">:</code> <code class="n">attendees</code> <code class="p">}</code>

<code class="n">OPENAI_FUNCTIONS</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"schedule_meeting"</code><code class="p">:</code> <code class="n">schedule_meeting</code>
<code class="p">}</code></pre>

<p>After importing <code>OpenAI</code> and <code>json</code>, you’ll create a function named <code>schedule_meeting</code>. This function is a mock-up, simulating the process of scheduling a meeting, and returns details such as <code>event_id</code>, <code>date</code>, <code>time</code>, and <code>attendees</code>. Following that, make an <code>OPENAI_FUNCTIONS</code> dictionary to map the function name to the actual function for ease of reference.</p>

<p class="less_space pagebreak-before">Next, define a <code>functions</code> list that provides the function’s JSON schema. This schema includes its name, a brief description, and the parameters it requires, guiding the LLM on how to interact with it:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Our predefined function JSON schema:</code>
<code class="n">functions</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">{</code>
        <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"function"</code><code class="p">,</code>
        <code class="s2">"function"</code><code class="p">:</code> <code class="p">{</code>
            <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"object"</code><code class="p">,</code>
            <code class="s2">"name"</code><code class="p">:</code> <code class="s2">"schedule_meeting"</code><code class="p">,</code>
            <code class="s2">"description"</code><code class="p">:</code> <code class="s1">'''Set a meeting at a specified date and time for</code>
<code class="s1">            designated attendees'''</code><code class="p">,</code>
            <code class="s2">"parameters"</code><code class="p">:</code> <code class="p">{</code>
                <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"object"</code><code class="p">,</code>
                <code class="s2">"properties"</code><code class="p">:</code> <code class="p">{</code>
                    <code class="s2">"date"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"type"</code><code class="p">:</code> <code class="s2">"string"</code><code class="p">,</code> <code class="s2">"format"</code><code class="p">:</code> <code class="s2">"date"</code><code class="p">},</code>
                    <code class="s2">"time"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"type"</code><code class="p">:</code> <code class="s2">"string"</code><code class="p">,</code> <code class="s2">"format"</code><code class="p">:</code> <code class="s2">"time"</code><code class="p">},</code>
                    <code class="s2">"attendees"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"type"</code><code class="p">:</code> <code class="s2">"array"</code><code class="p">,</code> <code class="s2">"items"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"type"</code><code class="p">:</code> <code class="s2">"string"</code><code class="p">}},</code>
                <code class="p">},</code>
                <code class="s2">"required"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"date"</code><code class="p">,</code> <code class="s2">"time"</code><code class="p">,</code> <code class="s2">"attendees"</code><code class="p">],</code>
            <code class="p">},</code>
        <code class="p">},</code>
    <code class="p">}</code>
<code class="p">]</code></pre>
<div data-type="tip"><h1>Specify Format</h1>
<p>When using function calling with <a data-type="indexterm" data-primary="Specify Format principle" data-secondary="OpenAI function calling" id="id835"/>your OpenAI models, always ensure to define a detailed JSON schema (including the name and description). This acts as a blueprint for the function, guiding the model to understand when and how to properly invoke it.</p>
</div>

<p>After defining the functions, let’s make an OpenAI API request. Set up a <code>messages</code> list with the user query. Then, using an OpenAI <code>client</code> object, you’ll send this message and the function schema to the model. The LLM analyzes the conversation, discerns a need to trigger a function, and provides the function name and arguments. The <code>function</code> and <code>function_args</code> are parsed from the LLM response. Then the function is executed, and its results are added back into the conversation. Then you call the model again for a user-friendly summary of the entire process.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">client</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">(</code><code class="n">api_key</code><code class="o">=</code><code class="n">getenv</code><code class="p">(</code><code class="s2">"OPENAI_API_KEY"</code><code class="p">))</code>

<code class="c1"># Start the conversation:</code>
<code class="n">messages</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">{</code>
        <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code>
        <code class="s2">"content"</code><code class="p">:</code> <code class="s1">'''Schedule a meeting on 2023-11-01 at 14:00</code>
<code class="s1">        with Alice and Bob.'''</code><code class="p">,</code>
    <code class="p">}</code>
<code class="p">]</code>

<code class="c1"># Send the conversation and function schema to the model:</code>
<code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo-1106"</code><code class="p">,</code>
    <code class="n">messages</code><code class="o">=</code><code class="n">messages</code><code class="p">,</code>
    <code class="n">tools</code><code class="o">=</code><code class="n">functions</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">response</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code>

<code class="c1"># Check if the model wants to call our function:</code>
<code class="k">if</code> <code class="n">response</code><code class="o">.</code><code class="n">tool_calls</code><code class="p">:</code>
    <code class="c1"># Get the first function call:</code>
    <code class="n">first_tool_call</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">tool_calls</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>

    <code class="c1"># Find the function name and function args to call:</code>
    <code class="n">function_name</code> <code class="o">=</code> <code class="n">first_tool_call</code><code class="o">.</code><code class="n">function</code><code class="o">.</code><code class="n">name</code>
    <code class="n">function_args</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">loads</code><code class="p">(</code><code class="n">first_tool_call</code><code class="o">.</code><code class="n">function</code><code class="o">.</code><code class="n">arguments</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"This is the function name: "</code><code class="p">,</code> <code class="n">function_name</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"These are the function arguments: "</code><code class="p">,</code> <code class="n">function_args</code><code class="p">)</code>

    <code class="n">function</code> <code class="o">=</code> <code class="n">OPENAI_FUNCTIONS</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">function_name</code><code class="p">)</code>

    <code class="k">if</code> <code class="ow">not</code> <code class="n">function</code><code class="p">:</code>
        <code class="k">raise</code> <code class="ne">Exception</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Function </code><code class="si">{</code><code class="n">function_name</code><code class="si">}</code><code class="s2"> not found."</code><code class="p">)</code>

    <code class="c1"># Call the function:</code>
    <code class="n">function_response</code> <code class="o">=</code> <code class="n">function</code><code class="p">(</code><code class="o">**</code><code class="n">function_args</code><code class="p">)</code>

    <code class="c1"># Share the function's response with the model:</code>
    <code class="n">messages</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
        <code class="p">{</code>
            <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"function"</code><code class="p">,</code>
            <code class="s2">"name"</code><code class="p">:</code> <code class="s2">"schedule_meeting"</code><code class="p">,</code>
            <code class="s2">"content"</code><code class="p">:</code> <code class="n">json</code><code class="o">.</code><code class="n">dumps</code><code class="p">(</code><code class="n">function_response</code><code class="p">),</code>
        <code class="p">}</code>
    <code class="p">)</code>

    <code class="c1"># Let the model generate a user-friendly response:</code>
    <code class="n">second_response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo-0613"</code><code class="p">,</code> <code class="n">messages</code><code class="o">=</code><code class="n">messages</code>
    <code class="p">)</code>

    <code class="nb">print</code><code class="p">(</code><code class="n">second_response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code><code class="p">)</code></pre>

<p class="less_space pagebreak-before">Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">These</code> <code class="n">are</code> <code class="n">the</code> <code class="n">function</code> <code class="n">arguments</code><code class="p">:</code>  <code class="p">{</code><code class="s1">'date'</code><code class="p">:</code> <code class="s1">'2023-11-01'</code><code class="p">,</code> <code class="s1">'time'</code><code class="p">:</code> <code class="s1">'14:00'</code><code class="p">,</code>
<code class="s1">'attendees'</code><code class="p">:</code> <code class="p">[</code><code class="s1">'Alice'</code><code class="p">,</code> <code class="s1">'Bob'</code><code class="p">]}</code>
<code class="n">This</code> <code class="ow">is</code> <code class="n">the</code> <code class="n">function</code> <code class="n">name</code><code class="p">:</code>  <code class="n">schedule_meeting</code>
<code class="n">I</code> <code class="n">have</code> <code class="n">scheduled</code> <code class="n">a</code> <code class="n">meeting</code> <code class="n">on</code> <code class="mi">2023</code><code class="o">-</code><code class="mi">11</code><code class="o">-</code><code class="mi">01</code> <code class="n">at</code> <code class="mi">14</code><code class="p">:</code><code class="mi">00</code> <code class="k">with</code> <code class="n">Alice</code> <code class="ow">and</code> <code class="n">Bob</code><code class="o">.</code>
<code class="n">The</code> <code class="n">event</code> <code class="n">ID</code> <code class="ow">is</code> <code class="mf">1234.</code></pre>

<p>Several important points to note while function calling:</p>

<ul>
<li>
<p>It’s possible to have many functions that the LLM can call.</p>
</li>
<li>
<p>OpenAI can hallucinate function parameters, so be more explicit within the <code>system</code> message to overcome this.</p>
</li>
<li>
<p>The <code>function_call</code> parameter can be set in various ways:</p>

<ul>
<li>
<p>To mandate a specific function call: <code>tool_choice: {"type: "function", "function": {"name": "my_function"}}}</code>.</p>
</li>
<li>
<p>For a user message without function invocation: <code>tool_choice: "none"</code>.</p>
</li>
<li>
<p>By default (<code>tool_choice: "auto"</code>), the model <a data-type="indexterm" data-primary="OpenAI" data-secondary="function calling" data-startref="poafccl" id="id836"/><a data-type="indexterm" data-primary="function calling" data-startref="fccopai" id="id837"/>autonomously decides if and which function to call.</p>
</li>
</ul>
</li>
</ul>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Parallel Function Calling"><div class="sect1" id="id64">
<h1>Parallel Function Calling</h1>

<p>You can set your chat messages to include <a data-type="indexterm" data-primary="function calling" data-secondary="parallel function calling" id="fcpfcll"/><a data-type="indexterm" data-primary="parallel function calling" id="prllfcc"/><a data-type="indexterm" data-primary="OpenAI" data-secondary="function calling" data-tertiary="parallel function calling" id="paifcllpf"/>intents that request simultaneous calls to multiple tools. This strategy is known as <em>parallel function calling</em>.</p>

<p>Modifying the previously used code, the <code>messages</code> list is updated to mandate the scheduling of two meetings:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Start the conversation:</code>
<code class="n">messages</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">{</code>
        <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code>
        <code class="s2">"content"</code><code class="p">:</code> <code class="s1">'''Schedule a meeting on 2023-11-01 at 14:00 with Alice</code>
<code class="s1">        and Bob. Then I want to schedule another meeting on 2023-11-02 at</code>
<code class="s1">        15:00 with Charlie and Dave.'''</code>
    <code class="p">}</code>
<code class="p">]</code></pre>

<p>Then, adjust the previous code section by incorporating a <code>for</code> loop.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Send the conversation and function schema to the model:</code>
<code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo-1106"</code><code class="p">,</code>
    <code class="n">messages</code><code class="o">=</code><code class="n">messages</code><code class="p">,</code>
    <code class="n">tools</code><code class="o">=</code><code class="n">functions</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">response</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code>

<code class="c1"># Check if the model wants to call our function:</code>
<code class="k">if</code> <code class="n">response</code><code class="o">.</code><code class="n">tool_calls</code><code class="p">:</code>
    <code class="k">for</code> <code class="n">tool_call</code> <code class="ow">in</code> <code class="n">response</code><code class="o">.</code><code class="n">tool_calls</code><code class="p">:</code>
        <code class="c1"># Get the function name and arguments to call:</code>
        <code class="n">function_name</code> <code class="o">=</code> <code class="n">tool_call</code><code class="o">.</code><code class="n">function</code><code class="o">.</code><code class="n">name</code>
        <code class="n">function_args</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">loads</code><code class="p">(</code><code class="n">tool_call</code><code class="o">.</code><code class="n">function</code><code class="o">.</code><code class="n">arguments</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"This is the function name: "</code><code class="p">,</code> <code class="n">function_name</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"These are the function arguments: "</code><code class="p">,</code> <code class="n">function_args</code><code class="p">)</code>

        <code class="n">function</code> <code class="o">=</code> <code class="n">OPENAI_FUNCTIONS</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">function_name</code><code class="p">)</code>

        <code class="k">if</code> <code class="ow">not</code> <code class="n">function</code><code class="p">:</code>
            <code class="k">raise</code> <code class="ne">Exception</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Function </code><code class="si">{</code><code class="n">function_name</code><code class="si">}</code><code class="s2"> not found."</code><code class="p">)</code>

        <code class="c1"># Call the function:</code>
        <code class="n">function_response</code> <code class="o">=</code> <code class="n">function</code><code class="p">(</code><code class="o">**</code><code class="n">function_args</code><code class="p">)</code>

        <code class="c1"># Share the function's response with the model:</code>
        <code class="n">messages</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
            <code class="p">{</code>
                <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"function"</code><code class="p">,</code>
                <code class="s2">"name"</code><code class="p">:</code> <code class="n">function_name</code><code class="p">,</code>
                <code class="s2">"content"</code><code class="p">:</code> <code class="n">json</code><code class="o">.</code><code class="n">dumps</code><code class="p">(</code><code class="n">function_response</code><code class="p">),</code>
            <code class="p">}</code>
        <code class="p">)</code>

    <code class="c1"># Let the model generate a user-friendly response:</code>
    <code class="n">second_response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo-0613"</code><code class="p">,</code> <code class="n">messages</code><code class="o">=</code><code class="n">messages</code>
    <code class="p">)</code>

    <code class="nb">print</code><code class="p">(</code><code class="n">second_response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">This</code> <code class="ow">is</code> <code class="n">the</code> <code class="n">function</code> <code class="n">name</code><code class="p">:</code>  <code class="n">schedule_meeting</code>
<code class="n">These</code> <code class="n">are</code> <code class="n">the</code> <code class="n">function</code> <code class="n">arguments</code><code class="p">:</code>  <code class="p">{</code><code class="s1">'date'</code><code class="p">:</code> <code class="s1">'2023-11-01'</code><code class="p">,</code> <code class="s1">'time'</code><code class="p">:</code> <code class="s1">'14:00'</code><code class="p">,</code>
<code class="s1">'attendees'</code><code class="p">:</code> <code class="p">[</code><code class="s1">'Alice'</code><code class="p">,</code> <code class="s1">'Bob'</code><code class="p">]}</code>
<code class="n">This</code> <code class="ow">is</code> <code class="n">the</code> <code class="n">function</code> <code class="n">name</code><code class="p">:</code>  <code class="n">schedule_meeting</code>
<code class="n">These</code> <code class="n">are</code> <code class="n">the</code> <code class="n">function</code> <code class="n">arguments</code><code class="p">:</code>  <code class="p">{</code><code class="s1">'date'</code><code class="p">:</code> <code class="s1">'2023-11-02'</code><code class="p">,</code> <code class="s1">'time'</code><code class="p">:</code> <code class="s1">'15:00'</code><code class="p">,</code>
<code class="s1">'attendees'</code><code class="p">:</code> <code class="p">[</code><code class="s1">'Charlie'</code><code class="p">,</code> <code class="s1">'Dave'</code><code class="p">]}</code>
<code class="n">Two</code> <code class="n">meetings</code> <code class="n">have</code> <code class="n">been</code> <code class="n">scheduled</code><code class="p">:</code>
<code class="mf">1.</code> <code class="n">Meeting</code> <code class="k">with</code> <code class="n">Alice</code> <code class="ow">and</code> <code class="n">Bob</code> <code class="n">on</code> <code class="mi">2023</code><code class="o">-</code><code class="mi">11</code><code class="o">-</code><code class="mi">01</code> <code class="n">at</code> <code class="mi">14</code><code class="p">:</code><code class="mf">00.</code>
<code class="mf">2.</code> <code class="n">Meeting</code> <code class="k">with</code> <code class="n">Charlie</code> <code class="ow">and</code> <code class="n">Dave</code> <code class="n">on</code> <code class="mi">2023</code><code class="o">-</code><code class="mi">11</code><code class="o">-</code><code class="mi">02</code> <code class="n">at</code> <code class="mi">15</code><code class="p">:</code><code class="mf">00.</code></pre>

<p>From this example, it’s clear how you can effectively manage multiple function calls. You’ve seen how the <code>schedule_meeting</code> function was called twice in a row to arrange different meetings. This demonstrates how flexibly and effortlessly you can handle varied and complex <a data-type="indexterm" data-primary="function calling" data-secondary="parallel function calling" data-startref="fcpfcll" id="id838"/><a data-type="indexterm" data-primary="parallel function calling" data-startref="prllfcc" id="id839"/><a data-type="indexterm" data-primary="OpenAI" data-secondary="function calling" data-tertiary="parallel function calling" data-startref="paifcllpf" id="id840"/>requests using AI-powered tools.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Function Calling in LangChain"><div class="sect1" id="id164">
<h1>Function Calling in LangChain</h1>

<p>If you’d prefer to avoid writing JSON schema <a data-type="indexterm" data-primary="function calling" data-secondary="LangChain" id="fccllch"/><a data-type="indexterm" data-primary="LangChain" data-secondary="function calling" id="lgcnfncc"/><a data-type="indexterm" data-primary="Pydantic (JSON) parser" data-secondary="function calling" id="id841"/>and simply want to extract structured data from an LLM response, then LangChain allows you to use function calling with Pydantic.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain.output_parsers.openai_tools</code> <code class="kn">import</code> <code class="n">PydanticToolsParser</code>
<code class="kn">from</code> <code class="nn">langchain_core.utils.function_calling</code> <code class="kn">import</code> <code class="n">convert_to_openai_tool</code>
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>
<code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langchain_core.pydantic_v1</code> <code class="kn">import</code> <code class="n">BaseModel</code><code class="p">,</code> <code class="n">Field</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Optional</code>

<code class="k">class</code> <code class="nc">Article</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">):</code>
    <code class="sd">"""Identifying key points and contrarian views in an article."""</code>

    <code class="n">points</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code><code class="o">...</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"Key points from the article"</code><code class="p">)</code>
    <code class="n">contrarian_points</code><code class="p">:</code> <code class="n">Optional</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code>
        <code class="kc">None</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"Any contrarian points acknowledged in the article"</code>
    <code class="p">)</code>
    <code class="n">author</code><code class="p">:</code> <code class="n">Optional</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code><code class="kc">None</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"Author of the article"</code><code class="p">)</code>

<code class="n">_EXTRACTION_TEMPLATE</code> <code class="o">=</code> <code class="s2">"""Extract and save the relevant entities mentioned </code><code class="se">\</code>
<code class="s2">in the following passage together with their properties.</code>

<code class="s2">If a property is not present and is not required in the function parameters,</code>
<code class="s2">do not include it in the output."""</code>

<code class="c1"># Create a prompt telling the LLM to extract information:</code>
<code class="n">prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">(</code>
    <code class="p">{(</code><code class="s2">"system"</code><code class="p">,</code> <code class="n">_EXTRACTION_TEMPLATE</code><code class="p">),</code> <code class="p">(</code><code class="s2">"user"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{input}</code><code class="s2">"</code><code class="p">)}</code>
<code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>

<code class="n">pydantic_schemas</code> <code class="o">=</code> <code class="p">[</code><code class="n">Article</code><code class="p">]</code>

<code class="c1"># Convert Pydantic objects to the appropriate schema:</code>
<code class="n">tools</code> <code class="o">=</code> <code class="p">[</code><code class="n">convert_to_openai_tool</code><code class="p">(</code><code class="n">p</code><code class="p">)</code> <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="n">pydantic_schemas</code><code class="p">]</code>

<code class="c1"># Give the model access to these tools:</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">bind_tools</code><code class="p">(</code><code class="n">tools</code><code class="o">=</code><code class="n">tools</code><code class="p">)</code>

<code class="c1"># Create an end to end chain:</code>
<code class="n">chain</code> <code class="o">=</code> <code class="n">prompt</code> <code class="o">|</code> <code class="n">model</code> <code class="o">|</code> <code class="n">PydanticToolsParser</code><code class="p">(</code><code class="n">tools</code><code class="o">=</code><code class="n">pydantic_schemas</code><code class="p">)</code>

<code class="n">result</code> <code class="o">=</code> <code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code>
    <code class="p">{</code>
        <code class="s2">"input"</code><code class="p">:</code> <code class="s2">"""In the recent article titled 'AI adoption in industry,'</code>
<code class="s2">        key points addressed include the growing interest ... However, the</code>
<code class="s2">        author, Dr. Jane Smith, ..."""</code>
    <code class="p">}</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">result</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="n">Article</code><code class="p">(</code><code class="n">points</code><code class="o">=</code><code class="s1">'The growing interest in AI in various sectors, ...'</code><code class="p">,</code>
<code class="n">contrarian_points</code><code class="o">=</code><code class="s1">'Without stringent regulations, ...'</code><code class="p">,</code>
<code class="n">author</code><code class="o">=</code><code class="s1">'Dr. Jane Smith'</code><code class="p">)]</code></pre>

<p>You’ll start by importing various modules, including <code>PydanticToolsParser</code> and <code>ChatPromptTemplate</code>, essential for parsing and templating your prompts. Then, you’ll define a Pydantic model, <code>Article</code>, to specify the structure of the information you want to extract from a given text. With the use of a custom prompt template and the ChatOpenAI model, you’ll instruct the AI to extract key points and contrarian views from an article. Finally, the extracted data is neatly converted into your predefined Pydantic model and printed out, allowing you to see the structured information pulled <a data-type="indexterm" data-primary="function calling" data-secondary="LangChain" data-startref="fccllch" id="id842"/><a data-type="indexterm" data-primary="LangChain" data-secondary="function calling" data-startref="lgcnfncc" id="id843"/>from the text.</p>

<p>There are several key points, including:</p>
<dl>
<dt>Converting Pydantic schema to OpenAI tools</dt>
<dd>
<p><code>tools = [convert_to_openai_tool(p) for p in pydantic_schemas]</code></p>
</dd>
<dt>Binding the tools directly to the LLM</dt>
<dd>
<p><code>model = model.bind_tools(tools=tools)</code></p>
</dd>
<dt>Creating an LCEL chain that contains a tools parser</dt>
<dd>
<p><code>chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)</code></p>
</dd>
</dl>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Extracting Data with LangChain"><div class="sect1" id="id65">
<h1>Extracting Data with LangChain</h1>

<p>The <code>create_extraction_chain_pydantic</code> function provides a <a data-type="indexterm" data-primary="LangChain" data-secondary="data extraction" id="lgncdxt"/><a data-type="indexterm" data-primary="data extraction, LangChain" id="dxtlgcn"/>more concise version of the previous implementation. By simply inserting a Pydantic model and an LLM that supports function calling, you can easily achieve parallel function calling.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain.chains.openai_tools</code> <code class="kn">import</code> <code class="n">create_extraction_chain_pydantic</code>
<code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langchain_core.pydantic_v1</code> <code class="kn">import</code> <code class="n">BaseModel</code><code class="p">,</code> <code class="n">Field</code>

<code class="c1"># Make sure to use a recent model that supports tools:</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo-1106"</code><code class="p">)</code>

<code class="k">class</code> <code class="nc">Person</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">):</code>
    <code class="sd">"""A person's name and age."""</code>

    <code class="n">name</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code><code class="o">...</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"The person's name"</code><code class="p">)</code>
    <code class="n">age</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code><code class="o">...</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"The person's age"</code><code class="p">)</code>

<code class="n">chain</code> <code class="o">=</code> <code class="n">create_extraction_chain_pydantic</code><code class="p">(</code><code class="n">Person</code><code class="p">,</code> <code class="n">model</code><code class="p">)</code>
<code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s1">'input'</code><code class="p">:</code><code class="s1">'''Bob is 25 years old. He lives in New York.</code>
<code class="s1">He likes to play basketball. Sarah is 30 years old. She lives in San</code>
<code class="s1">Francisco. She likes to play tennis.'''</code><code class="p">})</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="n">Person</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'Bob'</code><code class="p">,</code> <code class="n">age</code><code class="o">=</code><code class="mi">25</code><code class="p">),</code> <code class="n">Person</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'Sarah'</code><code class="p">,</code> <code class="n">age</code><code class="o">=</code><code class="mi">30</code><code class="p">)]</code></pre>

<p>The <code>Person</code> Pydantic model has two properties, <code>name</code> and <code>age</code>; by calling the <code>create_extraction_chain_pydantic</code> function with the input text, the LLM invokes the same function twice and <a data-type="indexterm" data-primary="LangChain" data-secondary="data extraction" data-startref="lgncdxt" id="id844"/><a data-type="indexterm" data-primary="data extraction, LangChain" data-startref="dxtlgcn" id="id845"/>creates two <code>People</code> objects.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Query Planning"><div class="sect1" id="id66">
<h1>Query Planning</h1>

<p>You may experience problems when user queries have <a data-type="indexterm" data-primary="query planning" id="qyplg"/><a data-type="indexterm" data-primary="OpenAI" data-secondary="query planning" id="opaiqyp"/>multiple intents with intricate dependencies. <em>Query planning</em> is an effective way to parse a user’s query into a series of steps that can be executed as a query graph with relevant dependencies:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langchain.output_parsers.pydantic</code> <code class="kn">import</code> <code class="n">PydanticOutputParser</code>
<code class="kn">from</code> <code class="nn">langchain_core.prompts.chat</code> <code class="kn">import</code> <code class="p">(</code>
    <code class="n">ChatPromptTemplate</code><code class="p">,</code>
    <code class="n">SystemMessagePromptTemplate</code><code class="p">,</code>
<code class="p">)</code>
<code class="kn">from</code> <code class="nn">pydantic.v1</code> <code class="kn">import</code> <code class="n">BaseModel</code><code class="p">,</code> <code class="n">Field</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code>

<code class="k">class</code> <code class="nc">Query</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">):</code>
    <code class="nb">id</code><code class="p">:</code> <code class="nb">int</code>
    <code class="n">question</code><code class="p">:</code> <code class="nb">str</code>
    <code class="n">dependencies</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code>
        <code class="n">default_factory</code><code class="o">=</code><code class="nb">list</code><code class="p">,</code>
        <code class="n">description</code><code class="o">=</code><code class="s2">"""A list of sub-queries that must be completed before</code>
<code class="s2">        this task can be completed.</code>
<code class="s2">        Use a sub query when anything is unknown and we might need to ask</code>
<code class="s2">        many queries to get an answer.</code>
<code class="s2">        Dependencies must only be other queries."""</code>
    <code class="p">)</code>

<code class="k">class</code> <code class="nc">QueryPlan</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">):</code>
    <code class="n">query_graph</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Query</code><code class="p">]</code></pre>

<p>Defining <code>QueryPlan</code> and <code>Query</code> allows you to first ask an LLM to parse a user’s query into multiple steps. Let’s investigate how to create the query plan.</p>

<p class="less_space pagebreak-before">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Set up a chat model:</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>

<code class="c1"># Set up a parser:</code>
<code class="n">parser</code> <code class="o">=</code> <code class="n">PydanticOutputParser</code><code class="p">(</code><code class="n">pydantic_object</code><code class="o">=</code><code class="n">QueryPlan</code><code class="p">)</code>

<code class="n">template</code> <code class="o">=</code> <code class="s2">"""Generate a query plan. This will be used for task execution.</code>

<code class="s2">Answer the following query: </code><code class="si">{query}</code><code class="s2"/>

<code class="s2">Return the following query graph format:</code>
<code class="si">{format_instructions}</code><code class="s2"/>
<code class="s2">"""</code>
<code class="n">system_message_prompt</code> <code class="o">=</code> <code class="n">SystemMessagePromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="n">template</code><code class="p">)</code>
<code class="n">chat_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">([</code><code class="n">system_message_prompt</code><code class="p">])</code>

<code class="c1"># Create the LCEL chain with the prompt, model, and parser:</code>
<code class="n">chain</code> <code class="o">=</code> <code class="n">chat_prompt</code> <code class="o">|</code> <code class="n">model</code> <code class="o">|</code> <code class="n">parser</code>

<code class="n">result</code> <code class="o">=</code> <code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code>
<code class="s2">"query"</code><code class="p">:</code><code class="s1">'''I want to get the results from my database. Then I want to find</code>
<code class="s1">out what the average age of my top 10 customers is. Once I have the average</code>
<code class="s1">age, I want to send an email to John. Also I just generally want to send a</code>
<code class="s1">welcome introduction email to Sarah, regardless of the other tasks.'''</code><code class="p">,</code>
<code class="s2">"format_instructions"</code><code class="p">:</code><code class="n">parser</code><code class="o">.</code><code class="n">get_format_instructions</code><code class="p">()})</code>

<code class="nb">print</code><code class="p">(</code><code class="n">result</code><code class="o">.</code><code class="n">query_graph</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="n">Query</code><code class="p">(</code><code class="nb">id</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">question</code><code class="o">=</code><code class="s1">'Get top 10 customers'</code><code class="p">,</code> <code class="n">dependencies</code><code class="o">=</code><code class="p">[]),</code>
<code class="n">Query</code><code class="p">(</code><code class="nb">id</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">question</code><code class="o">=</code><code class="s1">'Calculate average age of customers'</code><code class="p">,</code> <code class="n">dependencies</code><code class="o">=</code><code class="p">[</code><code class="mi">1</code><code class="p">]),</code>
<code class="n">Query</code><code class="p">(</code><code class="nb">id</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">question</code><code class="o">=</code><code class="s1">'Send email to John'</code><code class="p">,</code> <code class="n">dependencies</code><code class="o">=</code><code class="p">[</code><code class="mi">2</code><code class="p">]),</code>
<code class="n">Query</code><code class="p">(</code><code class="nb">id</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">question</code><code class="o">=</code><code class="s1">'Send welcome email to Sarah'</code><code class="p">,</code> <code class="n">dependencies</code><code class="o">=</code><code class="p">[])]</code></pre>

<p>Initiate a <code>ChatOpenAI</code> instance and create a <code>PydanticOutputParser</code> for the <code>QueryPlan</code> structure. Then the LLM response is called and parsed, producing a structured <code>query_graph</code> for your tasks with their <a data-type="indexterm" data-primary="query planning" data-startref="qyplg" id="id846"/><a data-type="indexterm" data-primary="OpenAI" data-secondary="query planning" data-startref="opaiqyp" id="id847"/>unique dependencies.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Creating Few-Shot Prompt Templates"><div class="sect1" id="id67">
<h1>Creating Few-Shot Prompt Templates</h1>

<p>Working with the generative capabilities of LLMs often involves making a choice between <em>zero-shot</em> and <em>few-shot learning (k-shot)</em>. While zero-shot learning requires no <a data-type="indexterm" data-primary="few-shot learning" id="id848"/><a data-type="indexterm" data-primary="k-shot" data-seealso="few-shot learning" id="id849"/>explicit examples and adapts to tasks based solely on the prompt, its dependence on the pretraining phase means it may not always yield precise results.</p>

<p class="less_space pagebreak-before">On the other hand, with few-shot learning, which involves providing a few examples of the desired task performance in the prompt, you have the opportunity to optimize the model’s behavior, leading to more desirable outputs.</p>

<p>Due to the token LLM context length, you will often finding yourself competing between adding lots of high-quality k-shot examples into your prompts while still aiming to generate an effective and deterministic LLM output.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Even as the token context window limit within LLMs continues to increase, providing a specific number of k-shot examples helps you minimize API costs.</p>
</div>

<p>Let’s explore two methods for adding k-shot examples into your prompts with <em>few-shot prompt templates</em>: using <em>fixed examples</em> and using an <em>example selector</em>.</p>








<section data-type="sect2" data-pdf-bookmark="Fixed-Length Few-Shot Examples"><div class="sect2" id="id165">
<h2>Fixed-Length Few-Shot Examples</h2>

<p>First, let’s look at how to create a <a data-type="indexterm" data-primary="few-shot prompt templates" data-secondary="fixed-length examples" id="id850"/>few-shot prompt template using a fixed number of examples. The foundation of this method lies in creating a robust set of few-shot examples:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="p">(</code>
    <code class="n">FewShotChatMessagePromptTemplate</code><code class="p">,</code>
    <code class="n">ChatPromptTemplate</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">examples</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">{</code>
        <code class="s2">"question"</code><code class="p">:</code> <code class="s2">"What is the capital of France?"</code><code class="p">,</code>
        <code class="s2">"answer"</code><code class="p">:</code> <code class="s2">"Paris"</code><code class="p">,</code>
    <code class="p">},</code>
    <code class="p">{</code>
        <code class="s2">"question"</code><code class="p">:</code> <code class="s2">"What is the capital of Spain?"</code><code class="p">,</code>
        <code class="s2">"answer"</code><code class="p">:</code> <code class="s2">"Madrid"</code><code class="p">,</code>
    <code class="p">}</code> <code class="c1"># ...more examples...</code>
<code class="p">]</code></pre>

<p>Each example is a dictionary containing a <code>question</code> and <code>answer</code> key that will be used to create pairs of <code>HumanMessage</code> and <code>AIMessage</code> messages.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Formatting the Examples"><div class="sect2" id="id68">
<h2>Formatting the Examples</h2>

<p>Next, you’ll configure a <code>ChatPromptTemplate</code> for formatting <a data-type="indexterm" data-primary="few-shot prompt templates" data-secondary="fixed-length examples" data-tertiary="formatting" id="wpppxgx"/><a data-type="indexterm" data-primary="few-shot prompt templates" data-secondary="ChatPromptTemplate" id="wpmpmpt"/><a data-type="indexterm" data-primary="ChatPromptTemplate" id="cppplxw"/>the individual examples, which will then be inserted into a <code>FewShotChatMessagePromptTemplate</code>.</p>

<p class="less_space pagebreak-before">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">example_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">(</code>
    <code class="p">[</code>
        <code class="p">(</code><code class="s2">"human"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{question}</code><code class="s2">"</code><code class="p">),</code>
        <code class="p">(</code><code class="s2">"ai"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{answer}</code><code class="s2">"</code><code class="p">),</code>
    <code class="p">]</code>
<code class="p">)</code>

<code class="n">few_shot_prompt</code> <code class="o">=</code> <code class="n">FewShotChatMessagePromptTemplate</code><code class="p">(</code>
    <code class="n">example_prompt</code><code class="o">=</code><code class="n">example_prompt</code><code class="p">,</code>
    <code class="n">examples</code><code class="o">=</code><code class="n">examples</code><code class="p">,</code>
<code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="n">few_shot_prompt</code><code class="o">.</code><code class="n">format</code><code class="p">())</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Human</code><code class="p">:</code> <code class="n">What</code> <code class="ow">is</code> <code class="n">the</code> <code class="n">capital</code> <code class="n">of</code> <code class="n">France</code><code class="err">?</code>
<code class="n">AI</code><code class="p">:</code> <code class="n">Paris</code>
<code class="n">Human</code><code class="p">:</code> <code class="n">What</code> <code class="ow">is</code> <code class="n">the</code> <code class="n">capital</code> <code class="n">of</code> <code class="n">Spain</code><code class="err">?</code>
<code class="n">AI</code><code class="p">:</code> <code class="n">Madrid</code>
<code class="o">...</code><code class="n">more</code> <code class="n">examples</code><code class="o">...</code></pre>

<p>Notice how <code>example_prompt</code> will create <code>HumanMessage</code> and <code>AIMessage</code> pairs with the prompt inputs of <code>{question}</code> and <code>{answer}</code>.</p>

<p>After running <code>few_shot_prompt.format()</code>, the few-shot examples are printed as a string. As you’d like to use these within a <code>ChatOpenAI()</code> LLM request, let’s create a new <code>ChatPromptTemplate</code>.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.output_parsers</code> <code class="kn">import</code> <code class="n">StrOutputParser</code>

<code class="n">final_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">(</code>
    <code class="p">[(</code><code class="s2">"system"</code><code class="p">,</code><code class="s1">'''You are responsible for answering</code>
<code class="s1">    questions about countries. Only return the country</code>
<code class="s1">    name.'''</code><code class="p">),</code>
    <code class="n">few_shot_prompt</code><code class="p">,(</code><code class="s2">"human"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{question}</code><code class="s2">"</code><code class="p">),]</code>
<code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>

<code class="c1"># Creating the LCEL chain with the prompt, model, and a StrOutputParser():</code>
<code class="n">chain</code> <code class="o">=</code> <code class="n">final_prompt</code> <code class="o">|</code> <code class="n">model</code> <code class="o">|</code> <code class="n">StrOutputParser</code><code class="p">()</code>

<code class="n">result</code> <code class="o">=</code> <code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"question"</code><code class="p">:</code> <code class="s2">"What is the capital of America?"</code><code class="p">})</code>

<code class="nb">print</code><code class="p">(</code><code class="n">result</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Washington</code><code class="p">,</code> <code class="n">D</code><code class="o">.</code><code class="n">C</code><code class="o">.</code></pre>

<p>After invoking the LCEL chain on <code>final_prompt</code>, your few-shot examples are added after the <code>SystemMessage</code>.</p>

<p>Notice that the LLM only returns <code>'Washington, D.C.'</code> This is because after the LLMs response is returned, <em>it is parsed</em> by <code>StrOutputParser()</code>, an output parser. Adding <code>StrOutputParser()</code> is a common way to ensure that LLM responses in chains <em>return string values</em>. You’ll explore <a data-type="indexterm" data-primary="few-shot prompt templates" data-secondary="fixed-length examples" data-tertiary="formatting" data-startref="wpppxgx" id="id851"/><a data-type="indexterm" data-primary="few-shot prompt templates" data-secondary="ChatPromptTemplate" data-startref="wpmpmpt" id="id852"/><a data-type="indexterm" data-primary="ChatPromptTemplate" data-startref="cppplxw" id="id853"/>this more in depth while learning sequential chains in LCEL.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Selecting Few-Shot Examples by Length"><div class="sect2" id="id69">
<h2>Selecting Few-Shot Examples by Length</h2>

<p>Before diving into the code, let’s outline <a data-type="indexterm" data-primary="few-shot prompt templates" data-secondary="fixed-length examples" data-tertiary="selecting by length" id="wmpmpxx"/>your task. Imagine you’re building a storytelling application powered by GPT-4. A user enters a list of character names with previously generated stories. However, each user’s list of characters might have a different length. Including too many characters might generate a story that surpasses the LLM’s context window limit. That’s where <a data-type="indexterm" data-primary="LengthBasedExampleSelector" id="id854"/>you can use <code>LengthBasedExampleSelector</code> to adapt the prompt according to the length of user input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">FewShotPromptTemplate</code><code class="p">,</code> <code class="n">PromptTemplate</code>
<code class="kn">from</code> <code class="nn">langchain.prompts.example_selector</code> <code class="kn">import</code> <code class="n">LengthBasedExampleSelector</code>
<code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langchain_core.messages</code> <code class="kn">import</code> <code class="n">SystemMessage</code>
<code class="kn">import</code> <code class="nn">tiktoken</code>

<code class="n">examples</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">{</code><code class="s2">"input"</code><code class="p">:</code> <code class="s2">"Gollum"</code><code class="p">,</code> <code class="s2">"output"</code><code class="p">:</code> <code class="s2">"&lt;Story involving Gollum&gt;"</code><code class="p">},</code>
    <code class="p">{</code><code class="s2">"input"</code><code class="p">:</code> <code class="s2">"Gandalf"</code><code class="p">,</code> <code class="s2">"output"</code><code class="p">:</code> <code class="s2">"&lt;Story involving Gandalf&gt;"</code><code class="p">},</code>
    <code class="p">{</code><code class="s2">"input"</code><code class="p">:</code> <code class="s2">"Bilbo"</code><code class="p">,</code> <code class="s2">"output"</code><code class="p">:</code> <code class="s2">"&lt;Story involving Bilbo&gt;"</code><code class="p">},</code>
<code class="p">]</code>

<code class="n">story_prompt</code> <code class="o">=</code> <code class="n">PromptTemplate</code><code class="p">(</code>
    <code class="n">input_variables</code><code class="o">=</code><code class="p">[</code><code class="s2">"input"</code><code class="p">,</code> <code class="s2">"output"</code><code class="p">],</code>
    <code class="n">template</code><code class="o">=</code><code class="s2">"Character: </code><code class="si">{input}</code><code class="se">\n</code><code class="s2">Story: </code><code class="si">{output}</code><code class="s2">"</code><code class="p">,</code>
<code class="p">)</code>

<code class="k">def</code> <code class="nf">num_tokens_from_string</code><code class="p">(</code><code class="n">string</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>
    <code class="sd">"""Returns the number of tokens in a text string."""</code>
    <code class="n">encoding</code> <code class="o">=</code> <code class="n">tiktoken</code><code class="o">.</code><code class="n">get_encoding</code><code class="p">(</code><code class="s2">"cl100k_base"</code><code class="p">)</code>
    <code class="n">num_tokens</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">encoding</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">string</code><code class="p">))</code>
    <code class="k">return</code> <code class="n">num_tokens</code>

<code class="n">example_selector</code> <code class="o">=</code> <code class="n">LengthBasedExampleSelector</code><code class="p">(</code>
    <code class="n">examples</code><code class="o">=</code><code class="n">examples</code><code class="p">,</code>
    <code class="n">example_prompt</code><code class="o">=</code><code class="n">story_prompt</code><code class="p">,</code>
    <code class="n">max_length</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="c1"># 1000 tokens are to be included from examples</code>
    <code class="c1"># get_text_length: Callable[[str], int] = lambda x: len(re.split("\n| ", x))</code>
    <code class="c1"># You have modified the get_text_length function to work with the</code>
    <code class="c1"># TikToken library based on token usage:</code>
    <code class="n">get_text_length</code><code class="o">=</code><code class="n">num_tokens_from_string</code><code class="p">,</code>
<code class="p">)</code></pre>

<p class="less_space pagebreak-before">First, you set up a <code>PromptTemplate</code> that takes two input variables for each example. Then <code>LengthBasedExampleSelector</code> adjusts the number of examples according to the <em>length of the examples input</em>, ensuring your LLM doesn’t generate a story beyond its context window.</p>

<p>Also, you’ve customized the <code>get_text_length</code> function to use the <code>num_tokens_from_string</code> function that counts the total <a data-type="indexterm" data-primary="tokens" data-secondary="total number of" id="id855"/>number of tokens using <code>tiktoken</code>. This means that <code>max_length=1000</code> represents the <em>number of tokens</em> rather than using the following default function:</p>

<p><code>get_text_length: Callable[[str], int] = lambda x: len(re.split("\n| ", x))</code></p>

<p>Now, to tie all these elements together:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dynamic_prompt</code> <code class="o">=</code> <code class="n">FewShotPromptTemplate</code><code class="p">(</code>
    <code class="n">example_selector</code><code class="o">=</code><code class="n">example_selector</code><code class="p">,</code>
    <code class="n">example_prompt</code><code class="o">=</code><code class="n">story_prompt</code><code class="p">,</code>
    <code class="n">prefix</code><code class="o">=</code><code class="s1">'''Generate a story for </code><code class="si">{character}</code><code class="s1"> using the</code>
<code class="s1">    current Character/Story pairs from all of the characters</code>
<code class="s1">    as context.'''</code><code class="p">,</code>
    <code class="n">suffix</code><code class="o">=</code><code class="s2">"Character: </code><code class="si">{character}</code><code class="se">\n</code><code class="s2">Story:"</code><code class="p">,</code>
    <code class="n">input_variables</code><code class="o">=</code><code class="p">[</code><code class="s2">"character"</code><code class="p">],</code>
<code class="p">)</code>

<code class="c1"># Provide a new character from Lord of the Rings:</code>
<code class="n">formatted_prompt</code> <code class="o">=</code> <code class="n">dynamic_prompt</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">character</code><code class="o">=</code><code class="s2">"Frodo"</code><code class="p">)</code>

<code class="c1"># Creating the chat model:</code>
<code class="n">chat</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>

<code class="n">response</code> <code class="o">=</code> <code class="n">chat</code><code class="o">.</code><code class="n">invoke</code><code class="p">([</code><code class="n">SystemMessage</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="n">formatted_prompt</code><code class="p">)])</code>
<code class="nb">print</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">Frodo was a young hobbit living a peaceful life in the Shire. However,
his life...</pre>
<div data-type="tip"><h1>Provide Examples and Specify Format</h1>
<p>When working with few-shot examples, the <a data-type="indexterm" data-primary="Provide Examples principle" data-secondary="few-shot prompt templates" id="id856"/><a data-type="indexterm" data-primary="Specify Format principle" data-secondary="few-shot prompt templates" id="id857"/>length of the content matters in determining how many examples the AI model can take into account. Tune the length of your input content and provide apt examples for efficient results to prevent the LLM from generating content that might surpass its context window limit.</p>
</div>

<p>After formatting the prompt, you create a chat model <a data-type="indexterm" data-primary="few-shot prompt templates" data-secondary="fixed-length examples" data-tertiary="selecting by length" data-startref="wmpmpxx" id="id858"/>with <code>ChatOpenAI()</code> and load the formatted prompt into a <code>SystemMessage</code> that creates a small story about Frodo from <em>Lord of the Rings</em>.</p>

<p>Rather than creating and formatting a <code>ChatPromptTemplate</code>, it’s often much easier to simply invoke a <code>SystemMesage</code> with a formatted prompt:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">result</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">invoke</code><code class="p">([</code><code class="n">SystemMessage</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="n">formatted_prompt</code><code class="p">)])</code></pre>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Limitations with Few-Shot Examples"><div class="sect1" id="id192">
<h1>Limitations with Few-Shot Examples</h1>

<p>Few-shot learning has limitations. Although it can <a data-type="indexterm" data-primary="few-shot prompt templates" data-secondary="fixed-length examples" data-tertiary="limitations" id="id859"/>prove beneficial in certain scenarios, it might not always yield the expected high-quality results. This is primarily due to two reasons:</p>

<ul>
<li>
<p>Pretrained models like GPT-4 can sometimes overfit to the few-shot examples, making them prioritize the examples over the actual prompt.</p>
</li>
<li>
<p>LLMs have a token limit. As a result, there will always be a trade-off between the number of examples and the length of the response. Providing more examples might limit the response length and vice versa.</p>
</li>
</ul>

<p>These limitations can be addressed in several ways. First, if few-shot prompting is not yielding the desired results, consider using differently framed phrases or experimenting with the language of the prompts themselves. Variations in how the prompt is phrased can result in different responses, highlighting the trial-and-error nature of prompt engineering.</p>

<p>Second, think about including explicit instructions to the model to ignore the examples after it understands the task or to use the examples just for formatting guidance. This might influence the model to not overfit to the examples.</p>

<p>If the tasks are complex and the performance of the model with few-shot learning is not satisfactory, you might need to consider <a href="https://oreil.ly/S40bZ">fine-tuning</a> your model. Fine-tuning provides a more nuanced understanding of a specific task to the model, thus improving the performance significantly.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Saving and Loading LLM Prompts"><div class="sect1" id="id70">
<h1>Saving and Loading LLM Prompts</h1>

<p>To effectively leverage generative AI models such as GPT-4, it is beneficial to store prompts as files <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="prompts" data-tertiary="saving" id="llmppv"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="prompts" data-tertiary="loading" id="llggmpp"/>instead of Python code. This approach enhances the shareability, storage, and versioning of your prompts.</p>

<p class="less_space pagebreak-before">LangChain supports both saving and loading prompts from JSON and YAML. Another key feature of LangChain is its support for detailed specification in one file or distributed across multiple files. This means you have the flexibility to store different components such as templates, examples, and others in distinct files and reference them as required.</p>

<p>Let’s learn how to save and load prompts:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">PromptTemplate</code><code class="p">,</code> <code class="n">load_prompt</code>

<code class="n">prompt</code> <code class="o">=</code> <code class="n">PromptTemplate</code><code class="p">(</code>
    <code class="n">template</code><code class="o">=</code><code class="s1">'''Translate this sentence from English to Spanish.</code>
    <code class="se">\n</code><code class="s1">Sentence: </code><code class="si">{sentence}</code><code class="se">\n</code><code class="s1">Translation:'''</code><code class="p">,</code>
    <code class="n">input_variables</code><code class="o">=</code><code class="p">[</code><code class="s2">"sentence"</code><code class="p">],</code>
<code class="p">)</code>

<code class="n">prompt</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"translation_prompt.json"</code><code class="p">)</code>

<code class="c1"># Loading the prompt template:</code>
<code class="n">load_prompt</code><code class="p">(</code><code class="s2">"translation_prompt.json"</code><code class="p">)</code>
<code class="c1"># Returns PromptTemplate()</code></pre>

<p>After importing <code>PromptTemplate</code> and <code>load_prompt</code> from the <code>langchain.prompts</code> module, you define a <code>PromptTemplate</code> for English-to-Spanish translation tasks and save it as <em>translation_prompt.json</em>. Finally, you load the saved prompt template using the <code>load_prompt</code> function, which returns an instance of <code>PromptTemplate</code>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Please be aware that LangChain’s prompt saving may not work with all types of prompt templates. To mitigate this, you <a data-type="indexterm" data-primary="pickle library" id="id860"/>can utilize the <em>pickle</em> library or <em>.txt</em> files to read and write any prompts that LangChain does not support.</p>
</div>

<p>You’ve learned how to create few-shot prompt templates using LangChain with two techniques: a fixed number of examples and using an example selector.</p>

<p>The former creates a set of few-shot examples and uses a <code>ChatPromptTemplate</code> object to format these into chat messages. This forms the basis for creating a <code>FewShotChatMessagePromptTemplate</code> object.</p>

<p>The latter approach, using an example selector, is handy when user input varies significantly in length. In such scenarios, a <code>LengthBasedExampleSelector</code> can be utilized to adjust the number of examples based on user input length. This ensures your LLM does not exceed its context window limit.</p>

<p>Moreover, you’ve seen how easy it is to store/load prompts as files, enabling <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="prompts" data-tertiary="saving" data-startref="llmppv" id="id861"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="prompts" data-tertiary="loading" data-startref="llggmpp" id="id862"/>enhanced shareability, storage, and versioning.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Data Connection"><div class="sect1" id="id71">
<h1>Data Connection</h1>

<p>Harnessing an LLM application, coupled with <a data-type="indexterm" data-primary="data connection" id="dtcnnct"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="data connection" id="lgggmdcnn"/>your data, uncovers a plethora of opportunities to boost efficiency while refining your decision-making processes.</p>

<p>Your organization’s data may manifest in various forms:</p>
<dl>
<dt>Unstructured data</dt>
<dd>
<p>This could include Google Docs, threads <a data-type="indexterm" data-primary="data" data-secondary="unstructured" id="id863"/><a data-type="indexterm" data-primary="unstructured data" id="id864"/>from communication platforms such as Slack or Microsoft Teams, web pages, internal documentation, or code repositories on GitHub.</p>
</dd>
<dt>Structured data</dt>
<dd>
<p>Data neatly housed within SQL, NoSQL, or Graph <a data-type="indexterm" data-primary="data" data-secondary="structured" id="id865"/><a data-type="indexterm" data-primary="structured data" id="id866"/>databases.</p>
</dd>
</dl>

<p>To query your unstructured data, a process of loading, transforming, embedding, and subsequently storing <a data-type="indexterm" data-primary="vector databases" id="id867"/><a data-type="indexterm" data-primary="databases" data-see="vector databases" id="id868"/><a data-type="indexterm" data-primary="unstructured data" data-secondary="vector databases" id="id869"/>it within a vector database is necessary. A <em>vector database</em> is a specialized type of database designed to efficiently store and query data in the form of vectors, which represent complex data like text or images in a format suitable for machine learning and similarity search.</p>

<p>As for structured data, given its already indexed and stored state, you can utilize a LangChain agent to conduct an intermediate query on your database. This allows for the extraction of specific features, which can then be used within your LLM prompts.</p>

<p>There are multiple Python packages that can help with your data ingestion, including <a href="https://oreil.ly/n0hDD">Unstructured</a>, <a href="https://www.llamaindex.ai">LlamaIndex</a>, and <a href="https://oreil.ly/PjV9o">LangChain</a>.</p>

<p><a data-type="xref" href="#figure-4-2">Figure 4-2</a> illustrates a standardized approach to data ingestion. It begins with the data sources, which are then loaded into documents. These documents are then chunked and stored within a vector database for later retrieval.</p>

<figure><div id="figure-4-2" class="figure">
<img src="assets/pega_0402.png" alt="Data Connection" width="600" height="182"/>
<h6><span class="label">Figure 4-2. </span>A data connection to retrieval pipeline</h6>
</div></figure>

<p class="less_space pagebreak-before">In particular LangChain equips you with essential components to load, modify, store, and retrieve your data:</p>
<dl>
<dt>Document loaders</dt>
<dd>
<p>These facilitate uploading informational <a data-type="indexterm" data-primary="LangChain" data-secondary="document loaders" id="id870"/>resources, or <em>documents</em>, from a diverse range of sources such as Word documents, PDF files, text files, or even web pages.</p>
</dd>
<dt>Document transformers</dt>
<dd>
<p>These tools allow the segmentation <a data-type="indexterm" data-primary="LangChain" data-secondary="document transformers" id="id871"/>of documents, conversion into a Q&amp;A layout, elimination of superfluous documents, and much more.</p>
</dd>
<dt>Text embedding models</dt>
<dd>
<p>These can transform unstructured <a data-type="indexterm" data-primary="LangChain" data-secondary="text embedding" id="id872"/>text into a sequence of floating-point numbers used for similarity search by vector stores.</p>
</dd>
<dt>Vector databases (vector stores)</dt>
<dd>
<p>These databases can save and execute searches <a data-type="indexterm" data-primary="LangChain" data-secondary="vector databases" id="id873"/><a data-type="indexterm" data-primary="vector databases" id="id874"/>over embedded data.</p>
</dd>
<dt>Retrievers</dt>
<dd>
<p>These tools offer the capability to query and retrieve data.</p>
</dd>
</dl>

<p>Also, it’s worth mentioning that other LLM frameworks <a data-type="indexterm" data-primary="data connection" data-startref="dtcnnct" id="id875"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="data connection" data-startref="lgggmdcnn" id="id876"/>such as <a href="https://oreil.ly/9NcTB">LlamaIndex</a> work seamlessly with LangChain. <a href="https://llamahub.ai">LlamaHub</a> is another open source library dedicated to document loaders and can create LangChain-specific <code>Document</code> objects.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Document Loaders"><div class="sect1" id="id72">
<h1>Document Loaders</h1>

<p>Let’s imagine you’ve been tasked with building <a data-type="indexterm" data-primary="document loaders" id="dcmtldr"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="document loaders" id="llgggld"/>an LLM data collection pipeline for NutriFusion Foods. The information that you need to gather for the LLM is contained within:</p>

<ul>
<li>
<p>A PDF of a book called <em>Principles of Marketing</em></p>
</li>
<li>
<p>Two <em>.docx</em> marketing reports in a public Google Cloud Storage bucket</p>
</li>
<li>
<p>Three <em>.csv</em> files showcasing the marketing performance data for 2021, 2022, and 2023</p>
</li>
</ul>

<p>Create a new Jupyter Notebook <a data-type="indexterm" data-primary="Jupyter Notebook" id="id877"/>or Python file in <em>content/chapter_4</em> of the <a href="https://oreil.ly/cVTyI">shared repository</a>, and then run <code>pip install pdf2image docx2txt pypdf</code>, which will install three packages.</p>

<p class="less_space pagebreak-before">All of the data apart from <em>.docx</em> files can be found in <a href="https://oreil.ly/u9gMx"><em>content/chapter_4/data</em></a>. You can start by importing all of your various data loaders and creating an empty <code>all_documents</code> list to store all of the <code>Document</code> objects across your data sources.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">Docx2txtLoader</code>
<code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">PyPDFLoader</code>
<code class="kn">from</code> <code class="nn">langchain_community.document_loaders.csv_loader</code> <code class="kn">import</code> <code class="n">CSVLoader</code>
<code class="kn">import</code> <code class="nn">glob</code>
<code class="kn">from</code> <code class="nn">langchain.text_splitter</code> <code class="kn">import</code> <code class="n">CharacterTextSplitter</code>

<code class="c1"># To store the documents across all data sources:</code>
<code class="n">all_documents</code> <code class="o">=</code> <code class="p">[]</code>

<code class="c1"># Load the PDF:</code>
<code class="n">loader</code> <code class="o">=</code> <code class="n">PyPDFLoader</code><code class="p">(</code><code class="s2">"data/principles_of_marketing_book.pdf"</code><code class="p">)</code>
<code class="n">pages</code> <code class="o">=</code> <code class="n">loader</code><code class="o">.</code><code class="n">load_and_split</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code><code class="n">pages</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>

<code class="c1"># Add extra metadata to each page:</code>
<code class="k">for</code> <code class="n">page</code> <code class="ow">in</code> <code class="n">pages</code><code class="p">:</code>
    <code class="n">page</code><code class="o">.</code><code class="n">metadata</code><code class="p">[</code><code class="s2">"description"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">"Principles of Marketing Book"</code>

<code class="c1"># Checking that the metadata has been added:</code>
<code class="k">for</code> <code class="n">page</code> <code class="ow">in</code> <code class="n">pages</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">2</code><code class="p">]:</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">page</code><code class="o">.</code><code class="n">metadata</code><code class="p">)</code>

<code class="c1"># Saving the marketing book pages:</code>
<code class="n">all_documents</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">pages</code><code class="p">)</code>

<code class="n">csv_files</code> <code class="o">=</code> <code class="n">glob</code><code class="o">.</code><code class="n">glob</code><code class="p">(</code><code class="s2">"data/*.csv"</code><code class="p">)</code>

<code class="c1"># Filter to only include the word Marketing in the file name:</code>
<code class="n">csv_files</code> <code class="o">=</code> <code class="p">[</code><code class="n">f</code> <code class="k">for</code> <code class="n">f</code> <code class="ow">in</code> <code class="n">csv_files</code> <code class="k">if</code> <code class="s2">"Marketing"</code> <code class="ow">in</code> <code class="n">f</code><code class="p">]</code>

<code class="c1"># For each .csv file:</code>
<code class="k">for</code> <code class="n">csv_file</code> <code class="ow">in</code> <code class="n">csv_files</code><code class="p">:</code>
    <code class="n">loader</code> <code class="o">=</code> <code class="n">CSVLoader</code><code class="p">(</code><code class="n">file_path</code><code class="o">=</code><code class="n">csv_file</code><code class="p">)</code>
    <code class="n">data</code> <code class="o">=</code> <code class="n">loader</code><code class="o">.</code><code class="n">load</code><code class="p">()</code>
    <code class="c1"># Saving the data to the all_documents list:</code>
    <code class="n">all_documents</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>

<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">CharacterTextSplitter</code><code class="o">.</code><code class="n">from_tiktoken_encoder</code><code class="p">(</code>
    <code class="n">chunk_size</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code> <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">0</code>
<code class="p">)</code>

<code class="n">urls</code> <code class="o">=</code> <code class="p">[</code>

    <code class="sd">'''https://storage.googleapis.com/oreilly-content/NutriFusion%20Foods%2</code>
<code class="sd">    0Marketing%20Plan%202022.docx'''</code><code class="p">,</code>
    <code class="sd">'''https://storage.googleapis.com/oreilly-content/NutriFusion%20Foods%2</code>
<code class="sd">    0Marketing%20Plan%202023.docx'''</code><code class="p">,</code>
<code class="p">]</code>

<code class="n">docs</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">url</code> <code class="ow">in</code> <code class="n">urls</code><code class="p">:</code>
    <code class="n">loader</code> <code class="o">=</code> <code class="n">Docx2txtLoader</code><code class="p">(</code><code class="n">url</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s1">'</code><code class="se">\n</code><code class="s1">'</code><code class="p">,</code> <code class="s1">''</code><code class="p">))</code>
    <code class="n">pages</code> <code class="o">=</code> <code class="n">loader</code><code class="o">.</code><code class="n">load</code><code class="p">()</code>
    <code class="n">chunks</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">split_documents</code><code class="p">(</code><code class="n">pages</code><code class="p">)</code>

    <code class="c1"># Adding the metadata to each chunk:</code>
    <code class="k">for</code> <code class="n">chunk</code> <code class="ow">in</code> <code class="n">chunks</code><code class="p">:</code>
        <code class="n">chunk</code><code class="o">.</code><code class="n">metadata</code><code class="p">[</code><code class="s2">"source"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">"NutriFusion Foods Marketing Plan - 2022/2023"</code>
    <code class="n">docs</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">chunks</code><code class="p">)</code>

<code class="c1"># Saving the marketing book pages:</code>
<code class="n">all_documents</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">docs</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">page_content='Principles of Mark eting'
metadata={'source': 'data/principles_of_marketing_book.pdf', 'page': 0}
{'source': 'data/principles_of_marketing_book.pdf', 'page': 0,
'description': 'Principles of Marketing Book'}
{'source': 'data/principles_of_marketing_book.pdf', 'page': 1,
'description': 'Principles of Marketing Book'}</pre>

<p>Then using <code>PyPDFLoader</code>, you can import a <em>.pdf</em> file and split it into multiple pages using the <code>.load_and_split()</code> function.</p>

<p>Additionally, it’s possible to add extra metadata to each page because the metadata is a Python dictionary on each <code>Document</code> object. Also, notice in the preceding output for <code>Document</code> objects the metadata <code>source</code> is attached to.</p>

<p>Using the package <code>glob</code>, you can easily find all of the <em>.csv</em> files and individually load these into LangChain <code>Document</code> objects with a <code>CSVLoader</code>.</p>

<p>Finally, the two marketing reports are loaded from a public Google Cloud Storage bucket and are then split into 200 token-chunk sizes using a <code>text_splitter</code>.</p>

<p>This section equipped you with the necessary knowledge to create a comprehensive document-loading pipeline for NutriFusion Foods’ LLM. Starting with data extraction from a PDF, several CSV files and two .<em>docx</em> files, each document was enriched with relevant metadata for better context.</p>

<p>You now have the ability to seamlessly <a data-type="indexterm" data-primary="LangChain" data-secondary="document loaders" data-startref="lgcdcl" id="id878"/><a data-type="indexterm" data-primary="document loaders" data-startref="dcmtldr" id="id879"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="document loaders" data-startref="llgggld" id="id880"/>integrate data from a variety of document sources into a cohesive data pipeline.</p>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Text Splitters"><div class="sect1" id="id73">
<h1>Text Splitters</h1>

<p>Balancing the length of each <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="text splitters" id="id881"/><a data-type="indexterm" data-primary="text splitters" id="id882"/><a data-type="indexterm" data-primary="LangChain" data-secondary="text splitters" id="id883"/>document is also a crucial factor. If a document is too lengthy, it may surpass the <em>context length</em> of the LLM (the maximum number of tokens that an LLM can process within a single request). But if the documents are excessively fragmented into smaller chunks, there’s a risk of losing significant contextual information, which is equally undesirable.</p>

<p>You might encounter specific challenges while text splitting, such as:</p>

<ul>
<li>
<p>Special characters such as hashtags, @ symbols, or links might not split as anticipated, affecting the overall structure of the split documents.</p>
</li>
<li>
<p>If your document contains intricate formatting like tables, lists, or multilevel headings, the text splitter might find it difficult to retain the original formatting.</p>
</li>
</ul>

<p>There are ways to overcome these challenges that we’ll explore later.</p>

<p>This section introduces you to text splitters in LangChain, tools utilized to break down large chunks of text to better adapt to your model’s context window.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There isn’t a perfect document size. Start by using good heuristics and then build a training/test set that you can use for LLM evaluation.</p>
</div>

<p>LangChain provides a range of text splitters so <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="text splitters" data-startref="llggxp" id="id884"/><a data-type="indexterm" data-primary="text splitters" data-startref="xttprtt" id="id885"/><a data-type="indexterm" data-primary="LangChain" data-secondary="text splitters" data-startref="gchnxpl" id="id886"/>that you can easily split by any of the following:</p>

<ul>
<li>
<p>Token count</p>
</li>
<li>
<p>Recursively by multiple characters</p>
</li>
<li>
<p>Character count</p>
</li>
<li>
<p>Code</p>
</li>
<li>
<p>Markdown headers</p>
</li>
</ul>

<p>Let’s explore three popular splitters: <code>CharacterTextSplitter</code>,
<code>TokenTextSplitter</code>, and <code>RecursiveCharacterTextSplitter</code>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Text Splitting by Length and Token Size"><div class="sect1" id="id166">
<h1>Text Splitting by Length and Token Size</h1>

<p>In <a data-type="xref" href="ch03.html#standard_practices_03">Chapter 3</a>, you learned how <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="text splitters" data-tertiary="by length and token size" id="llgxptk"/><a data-type="indexterm" data-primary="text splitters" data-secondary="by length and token size" id="xpgkzs"/><a data-type="indexterm" data-primary="LangChain" data-secondary="text splitters" data-tertiary="by length and token size" id="gcxpgkdz"/>to count the number of tokens within a GPT-4 call with <a href="https://oreil.ly/uz05O">tiktoken</a>. You can also use tiktoken to split strings into appropriately sized chunks and documents.</p>

<p>Remember to install tiktoken and langchain-text-splitters with <code>pip install tiktoken langchain-text-splitters</code>.</p>

<p>To split by token count in LangChain, you can use a <code>CharacterTextSplitter</code> with a <code>.from_tiktoken_encoder()</code> function.</p>

<p>You’ll initially create a <code>CharacterTextSplitter</code> with a chunk size of 50 characters and no overlap. Using the <code>split_text</code> method, you’re chopping the text into pieces and then printing out the total number of chunks created.</p>

<p>Then you’ll do the same thing, but this <a data-type="indexterm" data-primary="chunking text" data-secondary="chunk overlap" id="id887"/>time with a <em>chunk overlap</em> of 48 characters. This shows how the number of chunks changes based <em>on whether you allow overlap</em>, illustrating the impact of these settings on how your text gets divided:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_text_splitters</code> <code class="kn">import</code> <code class="n">CharacterTextSplitter</code>

<code class="n">text</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">Biology is a fascinating and diverse field of science that explores the</code>
<code class="s2">living world and its intricacies </code><code class="se">\n\n</code><code class="s2">. It encompasses the study of life, its</code>
<code class="s2">origins, diversity, structure, function, and interactions at various levels</code>
<code class="s2">from molecules and cells to organisms and ecosystems </code><code class="se">\n\n</code><code class="s2">. In this 1000-word</code>
<code class="s2">essay, we will delve into the core concepts of biology, its history, key</code>
<code class="s2">areas of study, and its significance in shaping our understanding of the</code>
<code class="s2">natural world. </code><code class="se">\n\n</code><code class="s2"> ...(truncated to save space)...</code>
<code class="s2">"""</code>
<code class="c1"># No chunk overlap:</code>
<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">CharacterTextSplitter</code><code class="o">.</code><code class="n">from_tiktoken_encoder</code><code class="p">(</code>
<code class="n">chunk_size</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">separator</code><code class="o">=</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">texts</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">split_text</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Number of texts with no chunk overlap: </code><code class="si">{</code><code class="nb">len</code><code class="p">(</code><code class="n">texts</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

<code class="c1"># Including a chunk overlap:</code>
<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">CharacterTextSplitter</code><code class="o">.</code><code class="n">from_tiktoken_encoder</code><code class="p">(</code>
<code class="n">chunk_size</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">48</code><code class="p">,</code> <code class="n">separator</code><code class="o">=</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">texts</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">split_text</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Number of texts with chunk overlap: </code><code class="si">{</code><code class="nb">len</code><code class="p">(</code><code class="n">texts</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">Number of texts with no chunk overlap: 3
Number of texts with chunk overlap: 6</pre>

<p>In the previous section, you used the following to load and split the <em>.pdf</em> into LangChain documents:</p>
<ul class="simplelist">
<li><code>pages = loader.load_and_split()</code></li>
</ul>

<p>It’s possible for you to have more granular control on the size of each document by creating a <code>TextSplitter</code> and attaching it to your <code>Document</code> loading pipelines:</p>
<ul class="simplelist">
<li><code>def load_and_split(text_splitter: TextSplitter | None = None) -&gt; List[Document]</code></li>
</ul>

<p>Simply create a <code>TokenTextSplitter</code> with a <code>chunk_size=500</code> and a <code>chunk_overlap</code> <span class="keep-together">of 50:</span></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain.text_splitter</code> <code class="kn">import</code> <code class="n">TokenTextSplitter</code>
<code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">PyPDFLoader</code>

<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">TokenTextSplitter</code><code class="p">(</code><code class="n">chunk_size</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>
<code class="n">loader</code> <code class="o">=</code> <code class="n">PyPDFLoader</code><code class="p">(</code><code class="s2">"data/principles_of_marketing_book.pdf"</code><code class="p">)</code>
<code class="n">pages</code> <code class="o">=</code> <code class="n">loader</code><code class="o">.</code><code class="n">load_and_split</code><code class="p">(</code><code class="n">text_splitter</code><code class="o">=</code><code class="n">text_splitter</code><code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">pages</code><code class="p">))</code> <code class="c1">#737</code></pre>

<p>The <em>Principles of Marketing</em> book contains 497 pages, <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="text splitters" data-tertiary="by length and token size" data-startref="llgxptk" id="id888"/><a data-type="indexterm" data-primary="text splitters" data-secondary="by length and token size" data-startref="xpgkzs" id="id889"/><a data-type="indexterm" data-primary="LangChain" data-secondary="text splitters" data-tertiary="by length and token size" data-startref="gcxpgkdz" id="id890"/>but after using a <code>TokenTextSplitter</code> with a <code>chunk_size</code> of 500 tokens, you’ve created 776 smaller LangChain <code>Document</code> objects.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Text Splitting with Recursive Character Splitting"><div class="sect1" id="id74">
<h1>Text Splitting with Recursive Character Splitting</h1>

<p>Dealing with sizable blocks of text can present <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="text splitters" data-tertiary="recursive character splitting" id="lmmxpccp"/><a data-type="indexterm" data-primary="text splitters" data-secondary="recursive character splitting" id="xpcvccpl"/><a data-type="indexterm" data-primary="LangChain" data-secondary="text splitters" data-tertiary="recursive character splitting" id="lcxprcrv"/><a data-type="indexterm" data-primary="character splitting, recursive" id="ccpttcv"/><a data-type="indexterm" data-primary="recursive character splitting" id="rccpltt"/>unique challenges in text analysis. A helpful strategy for such situations involves the use of <em>recursive character splitting</em>. This method facilitates the division of a large body of text into manageable segments, making further analysis more accessible.</p>

<p>This approach becomes incredibly effective when handling generic text. It leverages a list of characters as parameters and sequentially splits the text based on these characters. The resulting sections continue to be divided until they reach an acceptable size. By default, the character list comprises <code>"\n\n"</code>, <code>"\n"</code>, <code>" "</code>, and <code>""</code>. This arrangement aims to retain the integrity of paragraphs, sentences, and words, preserving the semantic context.</p>

<p>The process hinges on the character list provided and sizes the resulting sections based on the character count.</p>

<p>Before diving into the code, it’s essential to understand what the <code>RecursiveCharacterTextSplitter</code> does. It takes a text and a list of delimiters (characters that define the boundaries for splitting the text). Starting from the first delimiter in the list, the splitter attempts to divide the text. If the resulting chunks are still too large, it proceeds to the next delimiter, and so on. This process continues <em>recursively</em> until the chunks are small enough or all delimiters are exhausted.</p>

<p>Using the preceding <code>text</code> variable, start by importing <span><code>RecursiveCharacterText​Splitter</code></span>. This instance will be responsible for splitting the text. When initializing the splitter, parameters <code>chunk_size</code>, <code>chunk_overlap</code>, and <code>length_function</code> are set. Here, <code>chunk_size</code> is set to 100, and <code>chunk_overlap</code> to 20.</p>

<p>The <code>length_function</code> is defined as <code>len</code> to determine the size of the chunks. It’s also possible to modify the <code>length_function</code> argument to use a tokenizer count instead of using the default <code>len</code> function, which will count characters:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_text_splitters</code> <code class="kn">import</code> <code class="n">RecursiveCharacterTextSplitter</code>

<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="p">(</code>
    <code class="n">chunk_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
    <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code>
    <code class="n">length_function</code><code class="o">=</code><code class="nb">len</code><code class="p">,</code>
<code class="p">)</code></pre>

<p>Once the <code>text_splitter</code> instance is ready, you can use <code>.split_text</code> to split the <code>text</code> variable into smaller chunks. These chunks are stored in the <code>texts</code> Python list:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Split the text into chunks:</code>
<code class="n">texts</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">split_text</code><code class="p">(</code><code class="n">text</code><code class="p">)</code></pre>

<p>As well as simply splitting the text with overlap into a list of strings, you can easily create LangChain <code>Document</code> objects with the <code>.create_documents</code> function. Creating <code>Document</code> objects is useful because it allows you to:</p>

<ul>
<li>
<p>Store documents within a vector database for semantic search</p>
</li>
<li>
<p>Add metadata to specific pieces of text</p>
</li>
<li>
<p>Iterate over multiple documents to create a higher-level summary</p>
</li>
</ul>

<p>To add metadata, provide a list of dictionaries to the <code>metadatas</code> argument:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Create documents from the chunks:</code>
<code class="n">metadatas</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"title"</code><code class="p">:</code> <code class="s2">"Biology"</code><code class="p">,</code> <code class="s2">"author"</code><code class="p">:</code> <code class="s2">"John Doe"</code><code class="p">}</code>
<code class="n">docs</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">create_documents</code><code class="p">(</code><code class="n">texts</code><code class="p">,</code> <code class="n">metadatas</code><code class="o">=</code><code class="p">[</code><code class="n">metadatas</code><code class="p">]</code> <code class="o">*</code> <code class="nb">len</code><code class="p">(</code><code class="n">texts</code><code class="p">))</code></pre>

<p>But what if your existing <code>Document</code> objects are too long?</p>

<p>You can easily handle that by using the <code>.split_documents</code> function with a <span class="keep-together"><code>TextSplitter</code></span>. This will take in a list of <code>Document</code> objects and will return a new list of <code>Document</code> objects based on your <code>TextSplitter</code> class argument settings:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">text_splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="p">(</code><code class="n">chunk_size</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code>
<code class="n">splitted_docs</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">split_documents</code><code class="p">(</code><code class="n">docs</code><code class="p">)</code></pre>

<p>You’ve now gained the ability to craft an efficient data loading pipeline, leveraging sources such as PDFs, CSVs, and Google Cloud Storage links. Furthermore, you’ve learned how to enrich the collected documents with relevant metadata, providing meaningful context for analysis and prompt engineering.</p>

<p>With the introduction of text splitters, you can now strategically manage document sizes, optimizing for both the LLM’s context window and the preservation of context-rich information. You’ve navigated handling larger texts by employing recursion and character splitting. This newfound knowledge empowers you to work seamlessly with <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="text splitters" data-tertiary="recursive character splitting" data-startref="lmmxpccp" id="id891"/><a data-type="indexterm" data-primary="text splitters" data-secondary="recursive character splitting" data-startref="xpcvccpl" id="id892"/><a data-type="indexterm" data-primary="LangChain" data-secondary="text splitters" data-tertiary="recursive character splitting" data-startref="lcxprcrv" id="id893"/><a data-type="indexterm" data-primary="character splitting, recursive" data-startref="ccpttcv" id="id894"/><a data-type="indexterm" data-primary="recursive character splitting" data-startref="rccpltt" id="id895"/>various document sources and integrate them into a robust data pipeline.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Task Decomposition"><div class="sect1" id="id75">
<h1>Task Decomposition</h1>

<p><em>Task decomposition</em> is the strategic process <a data-type="indexterm" data-primary="task decomposition" id="id896"/>of dissecting complex problems into a suite of manageable subproblems. This approach aligns seamlessly with the natural tendencies of software engineers, who often conceptualize tasks as interrelated subcomponents.</p>

<p>In software engineering, by utilizing task decomposition you can reduce cognitive burden and harness the advantages of problem isolation and adherence to the single responsibility principle.</p>

<p>Interestingly, LLMs stand to gain considerably <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="task decomposition" id="lmgumdps"/>from the application of task decomposition across a range of use cases. This approach aids in maximizing the utility and effectiveness of LLMs in problem-solving scenarios by enabling them to handle intricate tasks that would be challenging to resolve as a single entity, as illustrated in <a data-type="xref" href="#figure-4-3">Figure 4-3</a>.</p>

<p>Here are several examples of LLMs using decomposition:</p>
<dl>
<dt>Complex problem solving</dt>
<dd>
<p>In instances where a problem is multifaceted and cannot be solved through a single prompt, task decomposition is extremely useful. For example, solving a complex legal case could be broken down into understanding the case’s context, identifying relevant laws, determining legal precedents, and crafting arguments. Each subtask can be solved independently by an LLM, providing a comprehensive solution when combined.</p>
</dd>
<dt>Content generation</dt>
<dd>
<p>For generating long-form content such as articles or blogs, the task can be decomposed into generating an outline, writing individual sections, and then compiling and refining the final draft. Each step can be individually managed by GPT-4 for better results.</p>
</dd>
<dt>Large document summary</dt>
<dd>
<p>Summarizing lengthy documents such as <a data-type="indexterm" data-primary="summarization" data-secondary="task decomposition" id="id897"/>research papers or reports can be done more effectively by decomposing the task into several smaller tasks, like understanding individual sections, summarizing them independently, and then compiling a final summary.</p>
</dd>
<dt>Interactive conversational agents</dt>
<dd>
<p>For creating advanced chatbots, task decomposition can help manage different aspects of conversation such as understanding user input, maintaining context, generating relevant responses, and managing dialogue flow.</p>
</dd>
<dt>Learning and tutoring systems</dt>
<dd>
<p>In digital tutoring systems, decomposing the <a data-type="indexterm" data-primary="tutoring systems, task decomposition and" id="id898"/>task of teaching a concept into understanding the learner’s current knowledge, identifying gaps, suggesting learning materials, and evaluating progress can make the system more effective. Each subtask can leverage GPT-4’s generative <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="task decomposition" data-startref="lmgumdps" id="id899"/>abilities.</p>
</dd>
</dl>

<figure><div id="figure-4-3" class="figure">
<img src="assets/pega_0403.png" alt=".Task decomposition with GPT-4." width="600" height="712"/>
<h6><span class="label">Figure 4-3. </span>Task decomposition with LLMs</h6>
</div></figure>
<div data-type="tip"><h1>Divide Labor</h1>
<p>Task decomposition is a crucial strategy <a data-type="indexterm" data-primary="Divide Labor principle" data-secondary="task decomposition" id="id900"/>for you to tap into the full potential of LLMs. By dissecting complex problems into simpler, manageable tasks, you can leverage the problem-solving abilities of these models more effectively and efficiently.</p>
</div>

<p>In the sections ahead, you’ll learn how to create and integrate multiple LLM chains to orchestrate more complicated workflows.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Prompt Chaining"><div class="sect1" id="id76">
<h1>Prompt Chaining</h1>

<p>Often you’ll find that attempting <a data-type="indexterm" data-primary="prompt chaining" id="id901"/>to do a single task within one prompt is impossible. You can utilize a mixture of <em>prompt chaining</em>, which involves combining multiple prompt inputs/outputs with specifically tailored LLM prompts to build up an idea.</p>

<p>Let’s imagine an example with a film company that would like to partially automate their film creation. This could be broken down into several key components, such as:</p>

<ul>
<li>
<p>Character creation</p>
</li>
<li>
<p>Plot generation</p>
</li>
<li>
<p>Scenes/world building</p>
</li>
</ul>

<p><a data-type="xref" href="#figure-4-4">Figure 4-4</a> shows what the prompt workflow might look like.</p>

<figure><div id="figure-4-4" class="figure">
<img src="assets/pega_0404.png" alt="Sequential Story Creation Process" width="569" height="800"/>
<h6><span class="label">Figure 4-4. </span>A sequential story creation process</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Sequential Chain"><div class="sect2" id="id167">
<h2>Sequential Chain</h2>

<p>Let’s decompose the task into <em>multiple chains</em> and <a data-type="indexterm" data-primary="prompt chaining" data-secondary="sequential" id="pmpqu"/><a data-type="indexterm" data-primary="prompt chaining" data-secondary="multiple chains" id="ppcpc"/>recompose them into a single chain:</p>
<dl>
<dt><code>character_generation_chain</code></dt>
<dd>
<p>A chain responsible for creating multiple characters given a <code>'genre'</code>.</p>
</dd>
<dt><code>plot_generation_chain</code></dt>
<dd>
<p>A chain that will create the plot given the <code>'characters'</code> and <code>'genre'</code> keys.</p>
</dd>
<dt><code>scene_generation_chain</code></dt>
<dd>
<p>This chain will generate any missing scenes that were not initially generated from the <code>plot_generation_chain</code>.</p>
</dd>
</dl>

<p>Let’s start by creating three separate <code>ChatPromptTemplate</code> variables, one for each chain:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.prompts.chat</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>

<code class="n">character_generation_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code>
    <code class="sd">"""I want you to brainstorm three to five characters for my short story. The</code>
<code class="sd">    genre is {genre}. Each character must have a Name and a Biography.</code>
<code class="sd">    You must provide a name and biography for each character, this is very</code>
<code class="sd">    important!</code>
<code class="sd">    ---</code>
<code class="sd">    Example response:</code>
<code class="sd">    Name: CharWiz, Biography: A wizard who is a master of magic.</code>
<code class="sd">    Name: CharWar, Biography: A warrior who is a master of the sword.</code>
<code class="sd">    ---</code>
<code class="sd">    Characters: """</code>
<code class="p">)</code>

<code class="n">plot_generation_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code>
    <code class="sd">"""Given the following characters and the genre, create an effective</code>
<code class="sd">    plot for a short story:</code>
<code class="sd">    Characters:</code>
<code class="sd">    {characters}</code>
<code class="sd">    ---</code>
<code class="sd">    Genre: {genre}</code>
<code class="sd">    ---</code>
<code class="sd">    Plot: """</code>
    <code class="p">)</code>

<code class="n">scene_generation_plot_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code>
    <code class="sd">"""Act as an effective content creator.</code>
<code class="sd">    Given multiple characters and a plot, you are responsible for</code>
<code class="sd">    generating the various scenes for each act.</code>

<code class="sd">    You must decompose the plot into multiple effective scenes:</code>
<code class="sd">    ---</code>
<code class="sd">    Characters:</code>
<code class="sd">    {characters}</code>
<code class="sd">    ---</code>
<code class="sd">    Genre: {genre}</code>
<code class="sd">    ---</code>
<code class="sd">    Plot: {plot}</code>
<code class="sd">    ---</code>
<code class="sd">    Example response:</code>
<code class="sd">    Scenes:</code>
<code class="sd">    Scene 1: Some text here.</code>
<code class="sd">    Scene 2: Some text here.</code>
<code class="sd">    Scene 3: Some text here.</code>
<code class="sd">    ----</code>
<code class="sd">    Scenes:</code>
<code class="sd">    """</code>
<code class="p">)</code></pre>

<p>Notice that as the prompt templates flow from character to plot and scene generation, you add more placeholder variables from the previous steps.</p>

<p>The question remains, how can you guarantee that these <a data-type="indexterm" data-primary="prompt chaining" data-secondary="sequential" data-startref="pmpqu" id="id902"/><a data-type="indexterm" data-primary="prompt chaining" data-secondary="multiple chains" data-startref="ppcpc" id="id903"/>extra strings are available for your downstream <code>ChatPromptTemplate</code> variables?</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="itemgetter and Dictionary Key Extraction"><div class="sect2" id="id77">
<h2>itemgetter and Dictionary Key Extraction</h2>

<p>Within LCEL you can use the <code>itemgetter</code> function <a data-type="indexterm" data-primary="prompt chaining" data-secondary="itemgetter" id="poctmg"/><a data-type="indexterm" data-primary="prompt chaining" data-secondary="dictionary key extraction" id="ppcdkyx"/><a data-type="indexterm" data-primary="itemgetter" id="itgtter"/><a data-type="indexterm" data-primary="dictionary key extraction" id="dcyyxt"/>from the <code>operator</code> package to extract keys from the previous step, as long as a dictionary was present within the previous step:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">operator</code> <code class="kn">import</code> <code class="n">itemgetter</code>
<code class="kn">from</code> <code class="nn">langchain_core.runnables</code> <code class="kn">import</code> <code class="n">RunnablePassthrough</code>

<code class="n">chain</code> <code class="o">=</code> <code class="n">RunnablePassthrough</code><code class="p">()</code> <code class="o">|</code> <code class="p">{</code>
    <code class="s2">"genre"</code><code class="p">:</code> <code class="n">itemgetter</code><code class="p">(</code><code class="s2">"genre"</code><code class="p">),</code>
  <code class="p">}</code>
<code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"genre"</code><code class="p">:</code> <code class="s2">"fantasy"</code><code class="p">})</code>
<code class="c1"># {'genre': 'fantasy'}</code></pre>

<p>The <code>RunnablePassThrough</code> function simply <a data-type="indexterm" data-primary="RunnablePassThrough function" id="id904"/>passes any inputs directly to the next step. Then a new dictionary is created by using the same key within the <code>invoke</code> function; this key is extracted by using <code>itemgetter("genre")</code>.</p>

<p>It’s essential to use the <code>itemgetter</code> function throughout parts of your LCEL chains so that any subsequent <code>ChatPromptTemplate</code> placeholder variables will always have valid values.</p>

<p>Additionally, you can use <code>lambda</code> or <code>RunnableLambda</code> functions within an LCEL chain to manipulate previous dictionary values. A lambda <a data-type="indexterm" data-primary="lambda functions" id="id905"/>is an anonynous function within Python:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.runnables</code> <code class="kn">import</code> <code class="n">RunnableLambda</code>

<code class="n">chain</code> <code class="o">=</code> <code class="n">RunnablePassthrough</code><code class="p">()</code> <code class="o">|</code> <code class="p">{</code>
    <code class="s2">"genre"</code><code class="p">:</code> <code class="n">itemgetter</code><code class="p">(</code><code class="s2">"genre"</code><code class="p">),</code>
    <code class="s2">"upper_case_genre"</code><code class="p">:</code> <code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="s2">"genre"</code><code class="p">]</code><code class="o">.</code><code class="n">upper</code><code class="p">(),</code>
    <code class="s2">"lower_case_genre"</code><code class="p">:</code> <code class="n">RunnableLambda</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="s2">"genre"</code><code class="p">]</code><code class="o">.</code><code class="n">lower</code><code class="p">()),</code>
<code class="p">}</code>
<code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"genre"</code><code class="p">:</code> <code class="s2">"fantasy"</code><code class="p">})</code>
<code class="c1"># {'genre': 'fantasy', 'upper_case_genre': 'FANTASY',</code>
<code class="c1"># 'lower_case_genre': 'fantasy'}</code></pre>

<p>Now that you’re aware of how to use <code>RunnablePassThrough</code>, <code>itemgetter</code>, and <code>lambda</code> functions, let’s introduce one <a data-type="indexterm" data-primary="RunnableParallel function" id="id906"/>final piece of syntax: <code>RunnableParallel</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.runnables</code> <code class="kn">import</code> <code class="n">RunnableParallel</code>

<code class="n">master_chain</code> <code class="o">=</code> <code class="n">RunnablePassthrough</code><code class="p">()</code> <code class="o">|</code> <code class="p">{</code>
    <code class="s2">"genre"</code><code class="p">:</code> <code class="n">itemgetter</code><code class="p">(</code><code class="s2">"genre"</code><code class="p">),</code>
    <code class="s2">"upper_case_genre"</code><code class="p">:</code> <code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="s2">"genre"</code><code class="p">]</code><code class="o">.</code><code class="n">upper</code><code class="p">(),</code>
    <code class="s2">"lower_case_genre"</code><code class="p">:</code> <code class="n">RunnableLambda</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="s2">"genre"</code><code class="p">]</code><code class="o">.</code><code class="n">lower</code><code class="p">()),</code>
<code class="p">}</code>

<code class="n">master_chain_two</code> <code class="o">=</code> <code class="n">RunnablePassthrough</code><code class="p">()</code> <code class="o">|</code> <code class="n">RunnableParallel</code><code class="p">(</code>
        <code class="n">genre</code><code class="o">=</code><code class="n">itemgetter</code><code class="p">(</code><code class="s2">"genre"</code><code class="p">),</code>
        <code class="n">upper_case_genre</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="s2">"genre"</code><code class="p">]</code><code class="o">.</code><code class="n">upper</code><code class="p">(),</code>
        <code class="n">lower_case_genre</code><code class="o">=</code><code class="n">RunnableLambda</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="s2">"genre"</code><code class="p">]</code><code class="o">.</code><code class="n">lower</code><code class="p">()),</code>
<code class="p">)</code>

<code class="n">story_result</code> <code class="o">=</code> <code class="n">master_chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"genre"</code><code class="p">:</code> <code class="s2">"Fantasy"</code><code class="p">})</code>
<code class="nb">print</code><code class="p">(</code><code class="n">story_result</code><code class="p">)</code>

<code class="n">story_result</code> <code class="o">=</code> <code class="n">master_chain_two</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"genre"</code><code class="p">:</code> <code class="s2">"Fantasy"</code><code class="p">})</code>
<code class="nb">print</code><code class="p">(</code><code class="n">story_result</code><code class="p">)</code>

<code class="c1"># master chain: {'genre': 'Fantasy', 'upper_case_genre': 'FANTASY',</code>
<code class="c1"># 'lower_case_genre': 'fantasy'}</code>
<code class="c1"># master chain two: {'genre': 'Fantasy', 'upper_case_genre': 'FANTASY',</code>
<code class="c1"># 'lower_case_genre': 'fantasy'}</code></pre>

<p>First, you import <code>RunnableParallel</code> and create two LCEL chains called <span class="keep-together"><code>master_chain</code></span> and <code>master_chain_two</code>. These are then invoked with exactly the same arguments; the <code>RunnablePassthrough</code> then passes the dictionary into the second part of the chain.</p>

<p>The second part of <code>master_chain</code> and <code>master_chain_two</code> will return exactly the <em>same result.</em></p>

<p>So rather than directly using a dictionary, you can choose to use a <code>RunnableParallel</code> function instead. These two chain outputs <em>are interchangeable</em>, so choose whichever syntax you find more comfortable.</p>

<p class="less_space pagebreak-before">Let’s create three LCEL chains using the prompt templates:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langchain_core.output_parsers</code> <code class="kn">import</code> <code class="n">StrOutputParser</code>

<code class="c1"># Create the chat model:</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>

<code class="c1"># Create the subchains:</code>
<code class="n">character_generation_chain</code> <code class="o">=</code> <code class="p">(</code> <code class="n">character_generation_prompt</code>
<code class="o">|</code> <code class="n">model</code>
<code class="o">|</code> <code class="n">StrOutputParser</code><code class="p">()</code> <code class="p">)</code>

<code class="n">plot_generation_chain</code> <code class="o">=</code> <code class="p">(</code> <code class="n">plot_generation_prompt</code>
<code class="o">|</code> <code class="n">model</code>
<code class="o">|</code> <code class="n">StrOutputParser</code><code class="p">()</code> <code class="p">)</code>

<code class="n">scene_generation_plot_chain</code> <code class="o">=</code> <code class="p">(</code> <code class="n">scene_generation_plot_prompt</code>
<code class="o">|</code> <code class="n">model</code>
<code class="o">|</code> <code class="n">StrOutputParser</code><code class="p">()</code>  <code class="p">)</code></pre>

<p>After creating all the chains, you can then attach them to a master LCEL chain.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.runnables</code> <code class="kn">import</code> <code class="n">RunnableParallel</code>
<code class="kn">from</code> <code class="nn">operator</code> <code class="kn">import</code> <code class="n">itemgetter</code>
<code class="kn">from</code> <code class="nn">langchain_core.runnables</code> <code class="kn">import</code> <code class="n">RunnablePassthrough</code>

<code class="n">master_chain</code> <code class="o">=</code> <code class="p">(</code>
    <code class="p">{</code><code class="s2">"characters"</code><code class="p">:</code> <code class="n">character_generation_chain</code><code class="p">,</code> <code class="s2">"genre"</code><code class="p">:</code>
    <code class="n">RunnablePassthrough</code><code class="p">()}</code>
    <code class="o">|</code> <code class="n">RunnableParallel</code><code class="p">(</code>
        <code class="n">characters</code><code class="o">=</code><code class="n">itemgetter</code><code class="p">(</code><code class="s2">"characters"</code><code class="p">),</code>
        <code class="n">genre</code><code class="o">=</code><code class="n">itemgetter</code><code class="p">(</code><code class="s2">"genre"</code><code class="p">),</code>
        <code class="n">plot</code><code class="o">=</code><code class="n">plot_generation_chain</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="o">|</code> <code class="n">RunnableParallel</code><code class="p">(</code>
        <code class="n">characters</code><code class="o">=</code><code class="n">itemgetter</code><code class="p">(</code><code class="s2">"characters"</code><code class="p">),</code>
        <code class="n">genre</code><code class="o">=</code><code class="n">itemgetter</code><code class="p">(</code><code class="s2">"genre"</code><code class="p">),</code>
        <code class="n">plot</code><code class="o">=</code><code class="n">itemgetter</code><code class="p">(</code><code class="s2">"plot"</code><code class="p">),</code>
        <code class="n">scenes</code><code class="o">=</code><code class="n">scene_generation_plot_chain</code><code class="p">,</code>
    <code class="p">)</code>
<code class="p">)</code>

<code class="n">story_result</code> <code class="o">=</code> <code class="n">master_chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"genre"</code><code class="p">:</code> <code class="s2">"Fantasy"</code><code class="p">})</code></pre>

<p>The output is truncated when you see <code>...</code> to save space. However, in total there were five characters and nine scenes generated.</p>

<p class="less_space pagebreak-before">Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">{</code><code class="s1">'characters'</code><code class="p">:</code> <code class="s1">'''Name: Lyra, Biography: Lyra is a young elf who possesses</code>
<code class="s1">..</code><code class="se">\n\n</code><code class="s1">Name: Orion, Biography: Orion is a ..'''</code><code class="p">,</code> <code class="s1">'genre'</code><code class="p">:</code> <code class="p">{</code><code class="s1">'genre'</code><code class="p">:</code>
<code class="s1">'Fantasy'</code><code class="p">}</code> <code class="s1">'plot'</code><code class="p">:</code> <code class="s1">'''In the enchanted forests of a mystical realm, a great</code>
<code class="s1">darkness looms, threatening to engulf the land and its inhabitants. Lyra,</code>
<code class="s1">the young elf with a deep connection to nature, ...'''</code><code class="p">,</code> <code class="s1">'scenes'</code><code class="p">:</code> <code class="s1">'''Scene</code>
<code class="s1">1: Lyra senses the impending danger in the forest ...</code><code class="se">\n\n</code><code class="s1">Scene 2: Orion, on</code>
<code class="s1">his mission to investigate the disturbances in the forest...</code><code class="se">\n\n</code><code class="s1">Scene 9:</code>
<code class="s1">After the battle, Lyra, Orion, Seraphina, Finnegan...'''</code><code class="p">}</code></pre>

<p>The scenes are split into separate items within a Python list. Then two new prompts are created to generate both a <a data-type="indexterm" data-primary="summarization" id="id907"/><a data-type="indexterm" data-primary="character script" id="id908"/>character script and a summarization prompt:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Extracting the scenes using .split('\n') and removing empty strings:</code>
<code class="n">scenes</code> <code class="o">=</code> <code class="p">[</code><code class="n">scene</code> <code class="k">for</code> <code class="n">scene</code> <code class="ow">in</code> <code class="n">story_result</code><code class="p">[</code><code class="s2">"scenes"</code><code class="p">]</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code> <code class="k">if</code> <code class="n">scene</code><code class="p">]</code>
<code class="n">generated_scenes</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">previous_scene_summary</code> <code class="o">=</code> <code class="s2">""</code>

<code class="n">character_script_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code>
    <code class="n">template</code><code class="o">=</code><code class="s2">"""Given the following characters: </code><code class="si">{characters}</code><code class="s2"> and the genre:</code>
    <code class="si">{genre}</code><code class="s2">, create an effective character script for a scene.</code>

<code class="s2">    You must follow the following principles:</code>
<code class="s2">    - Use the Previous Scene Summary: </code><code class="si">{previous_scene_summary}</code><code class="s2"> to avoid</code>
<code class="s2">    repeating yourself.</code>
<code class="s2">    - Use the Plot: </code><code class="si">{plot}</code><code class="s2"> to create an effective scene character script.</code>
<code class="s2">    - Currently you are generating the character dialogue script for the</code>
<code class="s2">    following scene: </code><code class="si">{scene}</code><code class="s2"/>

<code class="s2">    ---</code>
<code class="s2">    Here is an example response:</code>
<code class="s2">    SCENE 1: ANNA'S APARTMENT</code>

<code class="s2">    (ANNA is sorting through old books when there is a knock at the door.</code>
<code class="s2">    She opens it to reveal JOHN.)</code>
<code class="s2">    ANNA: Can I help you, sir?</code>
<code class="s2">    JOHN: Perhaps, I think it's me who can help you. I heard you're</code>
<code class="s2">    researching time travel.</code>
<code class="s2">    (Anna looks intrigued but also cautious.)</code>
<code class="s2">    ANNA: That's right, but how do you know?</code>
<code class="s2">    JOHN: You could say... I'm a primary source.</code>

<code class="s2">    ---</code>
<code class="s2">    SCENE NUMBER: </code><code class="si">{index}</code><code class="s2"/>

<code class="s2">    """</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">summarize_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code>
    <code class="n">template</code><code class="o">=</code><code class="s2">"""Given a character script, create a summary of the scene.</code>
<code class="s2">    Character script: </code><code class="si">{character_script}</code><code class="s2">"""</code><code class="p">,</code>
<code class="p">)</code></pre>

<p>Technically, you could generate all of the scenes asynchronously. However, it’s beneficial to know what each character has done in the <em>previous scene to avoid repeating points</em>.</p>

<p>Therefore, you can create two LCEL chains, one for generating the character scripts per scene and the other for summarizations of previous scenes:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Loading a chat model:</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s1">'gpt-3.5-turbo-16k'</code><code class="p">)</code>

<code class="c1"># Create the LCEL chains:</code>
<code class="n">character_script_generation_chain</code> <code class="o">=</code> <code class="p">(</code>
    <code class="p">{</code>
        <code class="s2">"characters"</code><code class="p">:</code> <code class="n">RunnablePassthrough</code><code class="p">(),</code>
        <code class="s2">"genre"</code><code class="p">:</code> <code class="n">RunnablePassthrough</code><code class="p">(),</code>
        <code class="s2">"previous_scene_summary"</code><code class="p">:</code> <code class="n">RunnablePassthrough</code><code class="p">(),</code>
        <code class="s2">"plot"</code><code class="p">:</code> <code class="n">RunnablePassthrough</code><code class="p">(),</code>
        <code class="s2">"scene"</code><code class="p">:</code> <code class="n">RunnablePassthrough</code><code class="p">(),</code>
        <code class="s2">"index"</code><code class="p">:</code> <code class="n">RunnablePassthrough</code><code class="p">(),</code>
    <code class="p">}</code>
    <code class="o">|</code> <code class="n">character_script_prompt</code>
    <code class="o">|</code> <code class="n">model</code>
    <code class="o">|</code> <code class="n">StrOutputParser</code><code class="p">()</code>
<code class="p">)</code>

<code class="n">summarize_chain</code> <code class="o">=</code> <code class="n">summarize_prompt</code> <code class="o">|</code> <code class="n">model</code> <code class="o">|</code> <code class="n">StrOutputParser</code><code class="p">()</code>

<code class="c1"># You might want to use tqdm here to track the progress,</code>
<code class="c1"># or use all of the scenes:</code>
<code class="k">for</code> <code class="n">index</code><code class="p">,</code> <code class="n">scene</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">scenes</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">3</code><code class="p">]):</code>

    <code class="c1"># # Create a scene generation:</code>
    <code class="n">scene_result</code> <code class="o">=</code> <code class="n">character_script_generation_chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code>
        <code class="p">{</code>
            <code class="s2">"characters"</code><code class="p">:</code> <code class="n">story_result</code><code class="p">[</code><code class="s2">"characters"</code><code class="p">],</code>
            <code class="s2">"genre"</code><code class="p">:</code> <code class="s2">"fantasy"</code><code class="p">,</code>
            <code class="s2">"previous_scene_summary"</code><code class="p">:</code> <code class="n">previous_scene_summary</code><code class="p">,</code>
            <code class="s2">"index"</code><code class="p">:</code> <code class="n">index</code><code class="p">,</code>
        <code class="p">}</code>
    <code class="p">)</code>

    <code class="c1"># Store the generated scenes:</code>
    <code class="n">generated_scenes</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
        <code class="p">{</code><code class="s2">"character_script"</code><code class="p">:</code> <code class="n">scene_result</code><code class="p">,</code> <code class="s2">"scene"</code><code class="p">:</code> <code class="n">scenes</code><code class="p">[</code><code class="n">index</code><code class="p">]}</code>
    <code class="p">)</code>

    <code class="c1"># If this is the first scene then we don't have a</code>
    <code class="c1"># previous scene summary:</code>
    <code class="k">if</code> <code class="n">index</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
        <code class="n">previous_scene_summary</code> <code class="o">=</code> <code class="n">scene_result</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="c1"># If this is the second scene or greater then</code>
        <code class="c1"># we can use and generate a summary:</code>
        <code class="n">summary_result</code> <code class="o">=</code> <code class="n">summarize_chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code>
            <code class="p">{</code><code class="s2">"character_script"</code><code class="p">:</code> <code class="n">scene_result</code><code class="p">}</code>
        <code class="p">)</code>
        <code class="n">previous_scene_summary</code> <code class="o">=</code> <code class="n">summary_result</code></pre>

<p>First, you’ll establish a <code>character_script_generation_chain</code> in your script, utilizing various runnables like <code>RunnablePassthrough</code> for smooth data flow. Crucially, this chain integrates model = <code>ChatOpenAI(model='gpt-3.5-turbo-16k')</code>, a powerful model with a generous 16k context window, ideal for extensive content generation tasks. When invoked, this chain adeptly generates character scripts, drawing on inputs such as character profiles, genre, and scene specifics.</p>

<p>You dynamically enrich each scene by adding the summary of the previous scene, creating a simple yet effective buffer memory. This technique ensures continuity and context in the narrative, enhancing the LLM’s ability to generate coherent character scripts.</p>

<p>Additionally, you’ll see how the <code>StrOutputParser</code> elegantly converts model outputs into structured strings, making the generated content easily usable.</p>
<div data-type="tip"><h1>Divide Labor</h1>
<p>Remember, designing your tasks in a <a data-type="indexterm" data-primary="Divide Labor principle" data-secondary="prompt chaining" id="id909"/>sequential chain greatly benefits from the Divide Labor principle. Breaking tasks down into smaller, manageable chains can increase the overall quality of your output. Each chain in the sequential chain contributes its individual effort toward achieving the overarching task goal.</p>

<p>Using chains gives you the ability to use different models. For example, using a smart model for the ideation and a cheap model for the generation usually gives optimal results. This also means you can have <a data-type="indexterm" data-primary="prompt chaining" data-secondary="itemgetter" data-startref="poctmg" id="id910"/><a data-type="indexterm" data-primary="prompt chaining" data-secondary="dictionary key extraction" data-startref="ppcdkyx" id="id911"/><a data-type="indexterm" data-primary="itemgetter" data-startref="itgtter" id="id912"/><a data-type="indexterm" data-primary="dictionary key extraction" data-startref="dcyyxt" id="id913"/>fine-tuned models on each step.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Structuring LCEL Chains"><div class="sect2" id="id78">
<h2>Structuring LCEL Chains</h2>

<p>In LCEL you must ensure that the first part of <a data-type="indexterm" data-primary="prompt chaining" data-secondary="LCEL chains" id="pprcllc"/><a data-type="indexterm" data-primary="LCEL chains" id="lclchais"/>your LCEL chain is a <em>runnable</em> type. The following code will throw an error:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.prompts.chat</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>
<code class="kn">from</code> <code class="nn">operator</code> <code class="kn">import</code> <code class="n">itemgetter</code>
<code class="kn">from</code> <code class="nn">langchain_core.runnables</code> <code class="kn">import</code> <code class="n">RunnablePassthrough</code><code class="p">,</code> <code class="n">RunnableLambda</code>

<code class="n">bad_first_input</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"film_required_age"</code><code class="p">:</code> <code class="mi">18</code><code class="p">,</code>
<code class="p">}</code>

<code class="n">prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code>
    <code class="s2">"Generate a film title, the age is </code><code class="si">{film_required_age}</code><code class="s2">"</code>
<code class="p">)</code>

<code class="c1"># This will error:</code>
<code class="n">bad_chain</code> <code class="o">=</code> <code class="n">bad_first_input</code> <code class="o">|</code> <code class="n">prompt</code></pre>

<p>A Python dictionary with a value of 18 will not create a runnable LCEL chain. However, all of the following implementations will work:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># All of these chains enforce the runnable interface:</code>
<code class="n">first_good_input</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"film_required_age"</code><code class="p">:</code> <code class="n">itemgetter</code><code class="p">(</code><code class="s2">"film_required_age"</code><code class="p">)}</code>

<code class="c1"># Creating a dictionary within a RunnableLambda:</code>
<code class="n">second_good_input</code> <code class="o">=</code> <code class="n">RunnableLambda</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="p">{</code> <code class="s2">"film_required_age"</code><code class="p">:</code>
<code class="n">x</code><code class="p">[</code><code class="s2">"film_required_age"</code><code class="p">]</code> <code class="p">}</code> <code class="p">)</code>

<code class="n">third_good_input</code> <code class="o">=</code> <code class="n">RunnablePassthrough</code><code class="p">()</code>
<code class="n">fourth_good_input</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"film_required_age"</code><code class="p">:</code> <code class="n">RunnablePassthrough</code><code class="p">()}</code>
<code class="c1"># You can also create a chain starting with RunnableParallel(...)</code>

<code class="n">first_good_chain</code> <code class="o">=</code> <code class="n">first_good_input</code> <code class="o">|</code> <code class="n">prompt</code>
<code class="n">second_good_chain</code> <code class="o">=</code> <code class="n">second_good_input</code> <code class="o">|</code> <code class="n">prompt</code>
<code class="n">third_good_chain</code> <code class="o">=</code> <code class="n">third_good_input</code> <code class="o">|</code> <code class="n">prompt</code>
<code class="n">fourth_good_chain</code> <code class="o">=</code> <code class="n">fourth_good_input</code> <code class="o">|</code> <code class="n">prompt</code>

<code class="n">first_good_chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code>
    <code class="s2">"film_required_age"</code><code class="p">:</code> <code class="mi">18</code>
<code class="p">})</code> <code class="c1"># ...</code></pre>

<p>Sequential chains are great at incrementally building generated knowledge that is used by future chains, but they often yield slower response times due to their sequential nature. As such, <code>SequentialChain</code> data pipelines are best suited for server-side tasks, where immediate responses <a data-type="indexterm" data-primary="prompt chaining" data-secondary="LCEL chains" data-startref="pprcllc" id="id914"/><a data-type="indexterm" data-primary="LCEL chains" data-startref="lclchais" id="id915"/>are not a priority and users aren’t awaiting real-time feedback.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Document Chains"><div class="sect2" id="id79">
<h2>Document Chains</h2>

<p>Let’s imagine that before accepting your <a data-type="indexterm" data-primary="prompt chaining" data-secondary="document chains" id="rmtcdcns"/><a data-type="indexterm" data-primary="document chains" id="dccsha"/>generated story, the local publisher has requested that you provide a summary based on all of the character scripts. This is a good use case for <em>document chains</em> because you need to provide an LLM with a large amount of text that wouldn’t fit within a single LLM request due to the context length restrictions.</p>

<p>Before delving into the code, let’s first get a sense of the broader picture. The script you are going to see performs a text <a data-type="indexterm" data-primary="summarization" data-secondary="document chains and" id="id916"/>summarization task on a collection of scenes.</p>

<p>Remember to install Pandas with <code>pip install pandas</code>.</p>

<p class="less_space pagebreak-before">Now, let’s start with the first set of code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_text_splitters</code> <code class="kn">import</code> <code class="n">CharacterTextSplitter</code>
<code class="kn">from</code> <code class="nn">langchain.chains.summarize</code> <code class="kn">import</code> <code class="n">load_summarize_chain</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code></pre>

<p>These lines are importing all the necessary tools you need. <code>CharacterTextSplitter</code> and <code>load_summarize_chain</code> are from the LangChain package and will help with text processing, while Pandas (imported as <code>pd</code>) will help manipulate your data.</p>

<p>Next, you’ll be dealing with your data:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">generated_scenes</code><code class="p">)</code></pre>

<p>Here, you create a Pandas DataFrame from the <code>generated_scenes</code> variable, effectively converting your raw scenes into a tabular data format that Pandas can easily manipulate.</p>

<p>Then you need to consolidate your text:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">all_character_script_text</code> <code class="o">=</code> <code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">character_script</code><code class="o">.</code><code class="n">tolist</code><code class="p">())</code></pre>

<p>In this line, you’re transforming the <code>character_script</code> column from your DataFrame into a single text string. Each entry in the column is converted into a list item, and all items are joined together with new lines in between, resulting in a single string that contains all character scripts.</p>

<p>Once you have your text ready, you prepare it for the summarization process:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">text_splitter</code> <code class="o">=</code> <code class="n">CharacterTextSplitter</code><code class="o">.</code><code class="n">from_tiktoken_encoder</code><code class="p">(</code>
    <code class="n">chunk_size</code><code class="o">=</code><code class="mi">1500</code><code class="p">,</code> <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">200</code>
<code class="p">)</code>
<code class="n">docs</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">create_documents</code><code class="p">([</code><code class="n">all_character_script_text</code><code class="p">])</code></pre>

<p>Here, you create a <code>CharacterTextSplitter</code> instance using its class method <code>from_tiktoken_encoder</code>, with specific parameters for chunk size and overlap. You then use this text splitter to split your consolidated script text into chunks suitable for processing by your summarization tool.</p>

<p>Next, you set up your summarization tool:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">chain</code> <code class="o">=</code> <code class="n">load_summarize_chain</code><code class="p">(</code><code class="n">llm</code><code class="o">=</code><code class="n">model</code><code class="p">,</code> <code class="n">chain_type</code><code class="o">=</code><code class="s2">"map_reduce"</code><code class="p">)</code></pre>

<p>This line is about setting up your summarization process. You’re calling a function that loads a summarization chain with a chat model in a <code>map-reduce</code> style approach.</p>

<p>Then you run the summarization:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">summary</code> <code class="o">=</code> <code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">docs</code><code class="p">)</code></pre>

<p>This is where you actually perform the text summarization. The <code>invoke</code> method executes the summarization on the chunks of text you prepared earlier and stores the summary into a variable.</p>

<p>Finally, you print the result:</p>

<pre data-type="programlisting" data-code-language="python"><code class="nb">print</code><code class="p">(</code><code class="n">summary</code><code class="p">[</code><code class="s1">'output_text'</code><code class="p">])</code></pre>

<p>This is the culmination of all your hard work. The resulting summary text is printed to the console for you to see.</p>

<p>This script takes a collection of scenes, consolidates the text, chunks it up, summarizes it, and then prints the summary:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain.text_splitter</code> <code class="kn">import</code> <code class="n">CharacterTextSplitter</code>
<code class="kn">from</code> <code class="nn">langchain.chains.summarize</code> <code class="kn">import</code> <code class="n">load_summarize_chain</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">generated_scenes</code><code class="p">)</code>

<code class="n">all_character_script_text</code> <code class="o">=</code> <code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">character_script</code><code class="o">.</code><code class="n">tolist</code><code class="p">())</code>

<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">CharacterTextSplitter</code><code class="o">.</code><code class="n">from_tiktoken_encoder</code><code class="p">(</code>
    <code class="n">chunk_size</code><code class="o">=</code><code class="mi">1500</code><code class="p">,</code> <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">200</code>
<code class="p">)</code>

<code class="n">docs</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">create_documents</code><code class="p">([</code><code class="n">all_character_script_text</code><code class="p">])</code>

<code class="n">chain</code> <code class="o">=</code> <code class="n">load_summarize_chain</code><code class="p">(</code><code class="n">llm</code><code class="o">=</code><code class="n">model</code><code class="p">,</code> <code class="n">chain_type</code><code class="o">=</code><code class="s2">"map_reduce"</code><code class="p">)</code>
<code class="n">summary</code> <code class="o">=</code> <code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">docs</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">summary</code><code class="p">[</code><code class="s1">'output_text'</code><code class="p">])</code></pre>

<p>Output:</p>

<pre data-type="programlisting">Aurora and Magnus agree to retrieve a hidden artifact, and they enter an
ancient library to find a book that will guide them to the relic...'</pre>

<p>It’s worth noting that even <a data-type="indexterm" data-primary="prompt chaining" data-secondary="document chains" data-startref="rmtcdcns" id="id917"/><a data-type="indexterm" data-primary="document chains" data-startref="dccsha" id="id918"/>though you’ve used a <code>map_reduce</code> chain, there are four core chains for working with <code>Document</code> objects within LangChain.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Stuff"><div class="sect2" id="id80">
<h2>Stuff</h2>

<p>The document <a data-type="indexterm" data-primary="prompt chaining" data-secondary="stuff chain" id="id919"/><a data-type="indexterm" data-primary="stuff chains" id="id920"/>insertion chain, also referred to as the <em>stuff</em> chain (drawing from the concept of <em>stuffing</em> or <em>filling</em>), is the simplest approach among various document chaining strategies. <a data-type="xref" href="#figure-4-5">Figure 4-5</a> illustrates the process of integrating multiple documents into a single LLM request.</p>

<figure class="less_space pagebreak-before"><div id="figure-4-5" class="figure">
<img src="assets/pega_0405.png" alt="Stuff Documents Chain" width="600" height="148"/>
<h6><span class="label">Figure 4-5. </span>Stuff documents chain</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Refine"><div class="sect2" id="id81">
<h2>Refine</h2>

<p>The refine documents chain (<a data-type="xref" href="#figure-4-6">Figure 4-6</a>) creates an LLM <a data-type="indexterm" data-primary="prompt chaining" data-secondary="document chains" data-tertiary="refine" id="id921"/><a data-type="indexterm" data-primary="document chains" data-secondary="refine documents chain" id="id922"/><a data-type="indexterm" data-primary="refine documents chain" id="id923"/>response through a cyclical process that <em>iteratively updates its output</em>. During each loop, it combines the current output (derived from the LLM) with the current document. Another LLM request is made to <em>update the current output</em>. This process continues until all documents have been processed.</p>

<figure><div id="figure-4-6" class="figure">
<img src="assets/pega_0406.png" alt="Refine Documents Chain" width="600" height="212"/>
<h6><span class="label">Figure 4-6. </span>Refine documents chain</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Map Reduce"><div class="sect2" id="id82">
<h2>Map Reduce</h2>

<p>The map reduce documents <a data-type="indexterm" data-primary="prompt chaining" data-secondary="document chains" data-tertiary="map reduce" id="id924"/><a data-type="indexterm" data-primary="document chains" data-secondary="map reduce" id="id925"/><a data-type="indexterm" data-primary="map reduce document chain" id="id926"/>chain in <a data-type="xref" href="#figure-4-7">Figure 4-7</a> starts with an LLM chain to each separate document (a process known as the Map step), interpreting the resulting output as a newly generated document.</p>

<p>Subsequently, all these newly created documents are introduced to a distinct combine documents chain to formulate a singular output (a process referred to as the Reduce step). If necessary, to ensure the new documents seamlessly fit into the context length, an optional compression process is used on the mapped documents. If required, this compression happens recursively.</p>

<figure><div id="figure-4-7" class="figure">
<img src="assets/pega_0407.png" alt="Map Reduce Chain" width="600" height="318"/>
<h6><span class="label">Figure 4-7. </span>Map reduce documents chain</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Map Re-rank"><div class="sect2" id="id83">
<h2>Map Re-rank</h2>

<p>There is also map re-rank, which operates <a data-type="indexterm" data-primary="prompt chaining" data-secondary="document chains" data-tertiary="map re-rank" id="id927"/><a data-type="indexterm" data-primary="document chains" data-secondary="map re-rank" id="id928"/><a data-type="indexterm" data-primary="map re-rank document chain" id="id929"/>by executing an initial prompt on each document. This not only strives to fulfill a given task but also assigns a confidence score reflecting the certainty of its answer. The response with the highest confidence score is then selected and returned.</p>

<p><a data-type="xref" href="#table-4-1">Table 4-1</a> demonstrates the advantages and disadvantages for choosing a specific document chain strategy.</p>
<table id="table-4-1">
<caption><span class="label">Table 4-1. </span>Overview of document chain strategies</caption>
<thead>
<tr>
<th>Approach</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Stuff Documents Chain</p></td>
<td><p>Simple to implement. Ideal for scenarios with small documents and few inputs.</p></td>
<td><p>May not be suitable for handling large documents or multiple inputs due to prompt size limitation.</p></td>
</tr>
<tr>
<td><p>Refine Documents Chain</p></td>
<td><p>Allows iterative refining of the response. More control over each step of response generation. Good for progressive extraction tasks.</p></td>
<td><p>Might not be optimal for real-time applications due to the loop process.</p></td>
</tr>
<tr>
<td><p>Map Reduce Documents Chain</p></td>
<td><p>Enables independent processing of each document. Can handle large datasets by reducing them into manageable chunks.</p></td>
<td><p>Requires careful management of the process. Optional compression step can add complexity and loses document order.</p></td>
</tr>
<tr>
<td><p>Map Re-rank Documents Chain</p></td>
<td><p>Provides a confidence score for each answer, allowing for better selection of responses.</p></td>
<td><p>The ranking algorithm can be complex to implement and manage. May not provide the best answer if the scoring mechanism is not reliable or well-tuned.</p></td>
</tr>
</tbody>
</table>

<p>You can read more about how to implement different document chains in <a href="https://oreil.ly/FQUK_">LangChain’s comprehensive API</a> and <a href="https://oreil.ly/9xr_6">here</a>.</p>

<p>Also, it’s possible to simply change the chain type within the <code>load_summarize_chain</code> function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">chain</code> <code class="o">=</code> <code class="n">load_summarize_chain</code><code class="p">(</code><code class="n">llm</code><code class="o">=</code><code class="n">model</code><code class="p">,</code> <code class="n">chain_type</code><code class="o">=</code><code class="s1">'refine'</code><code class="p">)</code></pre>

<p>There are newer, more customizable approaches to creating summarization chains using LCEL, but for most of your needs <code>load_summarize_chain</code> provides sufficient results.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id351">
<h1>Summary</h1>

<p>In this chapter, you comprehensively reviewed the LangChain framework and its essential components. You learned about the importance of document loaders for gathering data and the role of text splitters in handling large text blocks.</p>

<p>Moreover, you were introduced to the concepts of task decomposition and prompt chaining. By breaking down complex problems into smaller tasks, you saw the power of problem isolation. Furthermore, you now grasp how prompt chaining can combine multiple inputs/outputs for richer idea generation.</p>

<p>In the next chapter, you’ll learn about vector databases, including how to integrate these with documents from LangChain, and this ability will serve a pivotal role in enhancing the accuracy of knowledge extraction from your data.</p>
</div></section>
</div></section></div></div></body></html>