- en: Chapter 11\. Detecting Multiple Images
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。检测多个图像
- en: In the previous chapters we saw how we can work with pretrained classifiers
    to detect images and learn new categories. In all those experiments, though, we
    always assumed there was only one thing to see in our images. In the real world
    this is not always the case—we might have an image with both a cat and a dog,
    for example.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们看到了如何使用预训练分类器来检测图像并学习新的类别。然而，在所有这些实验中，我们总是假设我们的图像中只有一件事情要看。在现实世界中，情况并非总是如此——例如，我们可能有一张既有猫又有狗的图像。
- en: This chapter explores some techniques to overcome this limitation. We start
    out with building on a pretrained classifier and modifying the setup in such a
    way that we get multiple answers. We then look at a state-of-the art solution
    to solving this problem.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章探讨了一些技术来克服这个限制。我们首先建立在一个预训练的分类器上，并修改设置，以便我们得到多个答案。然后我们看一下解决这个问题的最先进的解决方案。
- en: This is an area of active research, and the most advanced algorithms are tricky
    to reproduce inside a Python notebook on top of Keras. Instead, we use an open
    source library in the second and third recipes of this chapter to demonstrate
    what is possible.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个活跃研究领域，最先进的算法很难在Python笔记本上重现，而且还要在Keras之上。相反，我们在本章的第二和第三个配方中使用一个开源库来演示可能性。
- en: 'The code for this chapter can be found in the following notebook:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下笔记本中找到：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 11.1 Detecting Multiple Images Using a Pretrained Classifier
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.1 使用预训练分类器检测多个图像
- en: Problem
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you find multiple image classes in a single image?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在单个图像中找到多个图像类别？
- en: Solution
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the outputs of the middle layers as a feature map and run a sliding window
    over them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用中间层的输出作为特征图，并在其上运行一个滑动窗口。
- en: 'Using a pretrained neural network to do image classifying is not very difficult
    once we have everything set up. If there are multiple objects to detect in the
    image, we don’t do so well though: the pretrained network will return the likelihood
    that the image represents any of the classes. If it sees two different objects,
    it might split the score returned. It will also split the score if it sees one
    object but isn’t sure whether it is one of two classes.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的神经网络进行图像分类并不难，一旦我们设置好一切。如果图像中有多个对象要检测，我们就做得不太好：预训练网络将返回图像代表任何类别的可能性。如果它看到两个不同的对象，它可能会分割返回的分数。如果它看到一个对象但不确定它是两个类别中的一个，它也会分割分数。
- en: 'One idea is to run a sliding window over the image. Rather than downsampling
    the image to 224×224, we downsample it to 448×448, double the original. We then
    feed all the different crops that we can get out of the larger image:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个想法是在图像上运行一个滑动窗口。我们不是将图像下采样到224×224，而是将其下采样到448×448，原始尺寸的两倍。然后我们将所有不同的裁剪都输入到较大图像中：
- en: '![cat and dog with two crops](assets/cat_dog_cropped.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![带有两个裁剪的猫和狗](assets/cat_dog_cropped.png)'
- en: 'Let’s create the crops from the larger image:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从较大的图像中创建裁剪：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Classifiers run over batches, so we can feed the `crops` object into the classifier
    that we’ve loaded before in the same fashion:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器在批处理上运行，因此我们可以以相同的方式将“crops”对象馈送到之前加载的分类器中：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The classifier mostly seems to think that the various tiles are either cats
    or dogs, but isn’t really sure what type. Let’s take a look at the crops that
    have the highest value for a given tag:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器似乎大多认为各种瓷砖要么是猫，要么是狗，但并不确定是哪种类型。让我们看看对于给定标签具有最高值的裁剪：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![best crop for egyptian cat](assets/dlcb_11in01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![埃及猫的最佳裁剪](assets/dlcb_11in01.png)'
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![best crop for labrador](assets/dlcb_11in02.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![拉布拉多的最佳裁剪](assets/dlcb_11in02.png)'
- en: 'This approach works, but is rather expensive. And we duplicate a lot of work.
    Remember that the way a CNN works is by way of running a convolution over the
    image, which is very similar to doing all those crops. Moreover, if we load a
    pretrained network without its top layers, it can run on an image of any size:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有效，但相当昂贵。而且我们重复了很多工作。记住CNN的工作方式是通过在图像上运行卷积来进行的，这与做所有这些裁剪非常相似。此外，如果我们加载一个没有顶层的预训练网络，它可以在任何大小的图像上运行：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The top layer of the network expects an input of 7×7×512\. We can recreate
    the top layer of the network based on the network we already loaded and copy the
    weights:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的顶层期望输入为7×7×512。我们可以根据已加载的网络重新创建网络的顶层，并复制权重：
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we can do the cropping based on the output of the bottom model and feed
    that into the top model, which means we only run the bottom model on 4 times the
    pixels of the original image, rather than 64 times as we did before. First, let’s
    run the image through the bottom model:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以根据底部模型的输出进行裁剪，并将其输入到顶部模型中，这意味着我们只对原始图像的4倍像素运行底部模型，而不是像之前那样运行64倍。首先，让我们通过底部模型运行图像：
- en: '[PRE9]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we’ll create the crops of the output:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建输出的裁剪：
- en: '[PRE10]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And run the top classifier:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行顶部分类器：
- en: '[PRE11]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This should give us the same results as before, but much faster!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该给我们带来与以前相同的结果，但速度更快！
- en: Discussion
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In this recipe we’ve taken advantage of the fact that the lower layers of a
    neural network have spatial information about what the network sees, even though
    this information is discarded at prediction time. This trick is based on some
    of the work done around Faster RCNN (see the next recipe), but doesn’t require
    the expensive training step.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们利用了神经网络的较低层具有关于网络看到的空间信息的事实，尽管这些信息在预测时被丢弃。这个技巧基于围绕Faster RCNN（见下一个配方）进行的一些工作，但不需要昂贵的训练步骤。
- en: The fact that our pretrained classifier works on images with a fixed size (224×224
    pixels, in this case) somewhat limits the approach here. The output regions always
    have the same size, and we have to decide into how many cells we split the original
    image. However, it does work well to find interesting subimages and is easy to
    deploy.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预训练分类器在固定大小的图像（在本例中为224×224像素）上运行，这在这里有些限制。输出区域始终具有相同的大小，我们必须决定将原始图像分成多少个单元格。然而，它确实很好地找到有趣的子图像，并且易于部署。
- en: Faster RNN itself doesn’t have the same drawbacks, but is much more costly to
    train. We’ll take a look at this in the next recipe.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Faster RNN本身并没有相同的缺点，但训练成本更高。我们将在下一个示例中看一下这一点。
- en: 11.2 Using Faster RCNN for Object Detection
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.2 使用Faster RCNN进行目标检测
- en: Problem
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you find multiple objects in an image with tight bounding boxes?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在图像中找到多个紧密边界框的对象？
- en: Solution
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a (pretrained) Faster RCNN network.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用（预训练的）Faster RCNN网络。
- en: 'Faster RCNN is a neural network solution for finding bounding boxes of objects
    in an image. Unfortunately, the algorithm is too complex to easily reproduce in
    a Python notebook; instead, we’ll rely on an open source implementation and treat
    that code more or less as a black box. Let’s clone it from GitHub:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Faster RCNN是一种神经网络解决方案，用于在图像中找到对象的边界框。不幸的是，该算法过于复杂，无法在Python笔记本中轻松复制；相反，我们将依赖于一个开源实现，并将该代码更多地视为黑盒。让我们从GitHub克隆它：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: After we’ve installed the dependencies from *requirements.txt*, we can train
    the network. We can either train it using our own data or on the standard dataset
    from the [Visual Object Challenge](http://host.robots.ox.ac.uk/pascal/VOC/). The
    latter contains many images with bounding boxes and 20 classes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们从*requirements.txt*安装了依赖项之后，我们可以训练网络。我们可以使用我们自己的数据进行训练，也可以使用[Visual Object
    Challenge](http://host.robots.ox.ac.uk/pascal/VOC/)的标准数据集进行训练。后者包含许多带有边界框和20个类别的图像。
- en: 'After we’ve downloaded the VOC 2007/2012 dataset and unpacked it, we can start
    training with:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们下载了VOC 2007/2012数据集并解压缩后，我们可以开始训练：
- en: '[PRE13]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This takes quite a long time—about a day on a serious GPU, and much longer on
    just CPUs. If you’d prefer to skip this step, there’s a pretrained network available
    at [*https://storage.googleapis.com/deep-learning-cookbook/model_frcnn.hdf5*](https://storage.googleapis.com/deep-learning-cookbook/model_frcnn.hdf5).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要相当长的时间——在一台强大的GPU上大约需要一天，而在仅使用CPU上则需要更长时间。如果您希望跳过此步骤，可以在[*https://storage.googleapis.com/deep-learning-cookbook/model_frcnn.hdf5*](https://storage.googleapis.com/deep-learning-cookbook/model_frcnn.hdf5)上找到一个预训练的网络。
- en: 'The training script saves the weights of the model every time it sees an improvement.
    Instantiation of the model for testing purposes is somewhat complex:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本会在每次看到改进时保存模型的权重。为了测试目的实例化模型有些复杂：
- en: '[PRE14]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We now have two models, one that is able to suggest regions that might have
    something interesting going on and the other able to tell us what it is. Let’s
    load the weights of the models and compile:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两个模型，一个能够建议可能有一些有趣内容的区域，另一个能够告诉我们那是什么。让我们加载模型的权重并编译：
- en: '[PRE15]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now let’s feed our image into the region suggester model. We’ll reshape the
    output in a way that will make it easier to run the next step. After this, `r2`
    is a three-dimensional structure with the last dimension holding the predictions:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将图像输入到区域建议模型中。我们将重塑输出，以便更容易运行下一步。之后，`r2`是一个三维结构，最后一个维度保存了预测：
- en: '[PRE16]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The image classifier runs over one-dimensional batches, so we have to feed
    in the two dimensions of `r2` one by one. `p_cls` will contain the detected classes
    and `p_regr` fine-tuning information for the boxes:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类器在一维批次上运行，因此我们必须逐个输入`r2`的两个维度。`p_cls`将包含检测到的类别，`p_regr`将包含框的微调信息：
- en: '[PRE17]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Putting the three arrays together to get the actual boxes, labels, and certainty
    is a matter of looping through the two dimensions:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将三个数组组合在一起以获得实际的框、标签和确定性是通过循环遍历两个维度来实现的：
- en: '[PRE18]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The list `boxes` now contains the detected cats and dogs. There are a lot of
    overlapping rectangles that can be resolved into each other.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在列表`boxes`中包含了检测到的猫和狗。有许多重叠的矩形，可以解析成彼此。
- en: Discussion
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The Faster RCNN algorithm is an evolution of the Fast RCNN algorithm, which
    in turn was an improvement on the original RCNN. All these algorithms work similarly;
    a region proposer comes up with possible rectangles that might contain interesting
    images and the image classifier then detects what—if anything—can be seen there.
    The approach is not so different from what we did in the previous recipe, where
    our region proposer just produced 64 subcrops of an image.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Faster RCNN算法是Fast RCNN算法的演变，而Fast RCNN算法又是原始RCNN的改进。所有这些算法工作方式类似；一个区域提议者提出可能包含有趣图像的矩形，然后图像分类器检测那里是否有什么。这种方法与我们在上一个示例中所做的并没有太大不同，那里我们的区域提议者只是生成了图像的64个子裁剪。
- en: Jian Sun, who came up with Faster RCNN, quite cleverly observed that the CNN
    that produces the feature map we used in the previous recipe could also be a good
    source for region proposals. So instead of treating the problem of region proposing
    separately, Faster RCNN trains the region proposal in parallel on the same feature
    map on which the image classification is done.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Jian Sun提出了Faster RCNN，他聪明地观察到在前一个示例中使用的产生特征图的CNN也可以成为区域提议的良好来源。因此，Faster RCNN不是将区域提议问题单独处理，而是在相同的特征图上并行训练区域提议，该特征图也用于图像分类。
- en: 'You can read more about the evolution of RCNN to Faster RCNN and how these
    algorithms work in the Athelas blog post [“A Brief History of CNNs in Image Segmentation:
    From R-CNN to Mask-CNN.”](https://bit.ly/2oUCh88)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Athelas博客文章["CNN在图像分割中的简要历史：从R-CNN到Mask-CNN"](https://bit.ly/2oUCh88)中了解RCNN演变为Faster
    RCNN以及这些算法的工作原理。
- en: 11.3 Running Faster RCNN over Our Own Images
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.3 在我们自己的图像上运行Faster RCNN
- en: Problem
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’d like to train a Faster RCNN model, but don’t want to have to start from
    scratch.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要训练一个Faster RCNN模型，但不想从头开始。
- en: Solution
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Start training from a pretrained model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练模型开始训练。
- en: Training from scratch requires a lot of labeled data. The VOC dataset contains
    more than 20,000 labeled images for 20 classes. So what do we do if we don’t have
    that much labeled data? We can use the transfer learning trick we came across
    first in [Chapter 9](ch09.html#transfer_learning).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始训练需要大量标记数据。VOC数据集包含20个类别的20000多个标记图像。那么如果我们没有那么多标记数据怎么办？我们可以使用我们在[第9章](ch09.html#transfer_learning)中首次遇到的迁移学习技巧。
- en: The training script already loads weights if it is restarted; what we need to
    do is convert the weights from the network trained on the VOC dataset to our own.
    In the previous recipe we constructed a dual network and loaded weights. As long
    as our new task is similar to the VOC classification task, all we need to do is
    change the number of classes, write back the weights, and start training.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果重新启动训练脚本，它已经加载了权重；我们需要做的是将来自在VOC数据集上训练的网络的权重转换为我们自己的权重。在之前的示例中，我们构建了一个双网络并加载了权重。只要我们的新任务类似于VOC分类任务，我们只需要改变类别数量，写回权重，然后开始训练。
- en: 'The easiest way to do this is to let the training script run just long enough
    for it to write its configuration file and then use that configuration file and
    the previously loaded model to get to these weights. For training our own data,
    it is best to use the comma-separated format described on GitHub with the format:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是让训练脚本运行足够长的时间，以便它写入其配置文件，然后使用该配置文件和先前加载的模型来获取这些权重。对于训练我们自己的数据，最好使用GitHub上描述的逗号分隔格式：
- en: '[PRE19]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here, `filepath` should be the full path to the image and `x1`, `y1`, `x2`,
    and `y2` form the bounding box in pixels on that image. We can now train the model
    with:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`filepath`应该是图像的完整路径，`x1`、`y1`、`x2`和`y2`形成了该图像上的像素边界框。我们现在可以用以下方式训练模型：
- en: '[PRE20]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, after we’ve loaded the pretrained model as before, we can load the new
    configuration file with:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们像以前一样加载了预训练模型之后，我们可以加载新的配置文件：
- en: '[PRE21]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can see that the training model only depends on the number of classes for
    the classifier object. So, we want to reconstruct the classifier object and any
    object that depend on it, then save the weights. That way we’ve constructed our
    new model based on the old weights. If we peek into the code that constructs the
    classifier, we see that it all depends on the third-to-last layer. So let’s copy
    that code, but run it using the `new_config`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到训练模型只取决于分类器对象的类别数量。因此，我们想要重建分类器对象和任何依赖它的对象，然后保存权重。这样我们就基于旧权重构建了新模型。如果我们查看构建分类器的代码，我们会发现它完全依赖于倒数第三层。因此，让我们复制该代码，但使用`new_config`运行：
- en: '[PRE22]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With the new classifier in hand, we can construct the model as before and save
    the weights. These weights will retain what the model learned before, but have
    zeros for the classifier bit that is specific to the new training task:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有了新的分类器，我们可以像以前一样构建模型并保存权重。这些权重将保留模型之前学到的内容，但对于特定于新训练任务的分类器部分将为零：
- en: '[PRE23]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can now continue training with:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以继续训练：
- en: '[PRE24]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Discussion
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Most examples of transfer learning are based on image recognition networks.
    This is partly because of the easy availability of pretrained networks and the
    fact that getting a training set of labeled images is straightforward. In this
    recipe we saw that we can apply this technique in other situations too. All we
    need is a pretrained network and an insight into how the network is constructed.
    By loading up the network weights, modifying the network for the new dataset,
    and saving the weights again, we can increase the speed of learning dramatically.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数迁移学习的例子都基于图像识别网络。这部分是因为预训练网络易于获取，而且获取带标签图像的训练集也很简单。在这个示例中，我们看到我们也可以在其他情况下应用这种技术。我们只需要一个预训练网络和对网络构建方式的了解。通过加载网络权重，修改网络以适应新数据集，并再次保存权重，我们可以显著提高学习速度。
- en: Even in situations where we don’t have a pretrained network available, but where
    there is a large set of public training data available and our own dataset is
    small, it might make sense to first train on the public dataset and then transfer
    that learning to our own set. For the bounding box case discussed in this recipe,
    this could easily be the case.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在没有预训练网络可用的情况下，但有大量公共训练数据可用且我们自己的数据集很小的情况下，首先在公共数据集上进行训练，然后将学习迁移到我们自己的数据集可能是有意义的。对于本示例中讨论的边界框情况，这很容易成为可能。
- en: Tip
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If your own dataset is small, it might make sense to experiment with setting
    part of the network to untrainable as we did in [Recipe 9.7](ch09.html#retraining-image-recognition-networks).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您自己的数据集很小，可能有必要像我们在[第9.7节](ch09.html#retraining-image-recognition-networks)中所做的那样，将网络的一部分设置为不可训练。
