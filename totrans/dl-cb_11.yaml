- en: Chapter 11\. Detecting Multiple Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters we saw how we can work with pretrained classifiers
    to detect images and learn new categories. In all those experiments, though, we
    always assumed there was only one thing to see in our images. In the real world
    this is not always the case—we might have an image with both a cat and a dog,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explores some techniques to overcome this limitation. We start
    out with building on a pretrained classifier and modifying the setup in such a
    way that we get multiple answers. We then look at a state-of-the art solution
    to solving this problem.
  prefs: []
  type: TYPE_NORMAL
- en: This is an area of active research, and the most advanced algorithms are tricky
    to reproduce inside a Python notebook on top of Keras. Instead, we use an open
    source library in the second and third recipes of this chapter to demonstrate
    what is possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found in the following notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 11.1 Detecting Multiple Images Using a Pretrained Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you find multiple image classes in a single image?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the outputs of the middle layers as a feature map and run a sliding window
    over them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a pretrained neural network to do image classifying is not very difficult
    once we have everything set up. If there are multiple objects to detect in the
    image, we don’t do so well though: the pretrained network will return the likelihood
    that the image represents any of the classes. If it sees two different objects,
    it might split the score returned. It will also split the score if it sees one
    object but isn’t sure whether it is one of two classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One idea is to run a sliding window over the image. Rather than downsampling
    the image to 224×224, we downsample it to 448×448, double the original. We then
    feed all the different crops that we can get out of the larger image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![cat and dog with two crops](assets/cat_dog_cropped.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s create the crops from the larger image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Classifiers run over batches, so we can feed the `crops` object into the classifier
    that we’ve loaded before in the same fashion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The classifier mostly seems to think that the various tiles are either cats
    or dogs, but isn’t really sure what type. Let’s take a look at the crops that
    have the highest value for a given tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![best crop for egyptian cat](assets/dlcb_11in01.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![best crop for labrador](assets/dlcb_11in02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This approach works, but is rather expensive. And we duplicate a lot of work.
    Remember that the way a CNN works is by way of running a convolution over the
    image, which is very similar to doing all those crops. Moreover, if we load a
    pretrained network without its top layers, it can run on an image of any size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The top layer of the network expects an input of 7×7×512\. We can recreate
    the top layer of the network based on the network we already loaded and copy the
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can do the cropping based on the output of the bottom model and feed
    that into the top model, which means we only run the bottom model on 4 times the
    pixels of the original image, rather than 64 times as we did before. First, let’s
    run the image through the bottom model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we’ll create the crops of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And run the top classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This should give us the same results as before, but much faster!
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe we’ve taken advantage of the fact that the lower layers of a
    neural network have spatial information about what the network sees, even though
    this information is discarded at prediction time. This trick is based on some
    of the work done around Faster RCNN (see the next recipe), but doesn’t require
    the expensive training step.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that our pretrained classifier works on images with a fixed size (224×224
    pixels, in this case) somewhat limits the approach here. The output regions always
    have the same size, and we have to decide into how many cells we split the original
    image. However, it does work well to find interesting subimages and is easy to
    deploy.
  prefs: []
  type: TYPE_NORMAL
- en: Faster RNN itself doesn’t have the same drawbacks, but is much more costly to
    train. We’ll take a look at this in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Using Faster RCNN for Object Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you find multiple objects in an image with tight bounding boxes?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a (pretrained) Faster RCNN network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Faster RCNN is a neural network solution for finding bounding boxes of objects
    in an image. Unfortunately, the algorithm is too complex to easily reproduce in
    a Python notebook; instead, we’ll rely on an open source implementation and treat
    that code more or less as a black box. Let’s clone it from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: After we’ve installed the dependencies from *requirements.txt*, we can train
    the network. We can either train it using our own data or on the standard dataset
    from the [Visual Object Challenge](http://host.robots.ox.ac.uk/pascal/VOC/). The
    latter contains many images with bounding boxes and 20 classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we’ve downloaded the VOC 2007/2012 dataset and unpacked it, we can start
    training with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This takes quite a long time—about a day on a serious GPU, and much longer on
    just CPUs. If you’d prefer to skip this step, there’s a pretrained network available
    at [*https://storage.googleapis.com/deep-learning-cookbook/model_frcnn.hdf5*](https://storage.googleapis.com/deep-learning-cookbook/model_frcnn.hdf5).
  prefs: []
  type: TYPE_NORMAL
- en: 'The training script saves the weights of the model every time it sees an improvement.
    Instantiation of the model for testing purposes is somewhat complex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have two models, one that is able to suggest regions that might have
    something interesting going on and the other able to tell us what it is. Let’s
    load the weights of the models and compile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s feed our image into the region suggester model. We’ll reshape the
    output in a way that will make it easier to run the next step. After this, `r2`
    is a three-dimensional structure with the last dimension holding the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The image classifier runs over one-dimensional batches, so we have to feed
    in the two dimensions of `r2` one by one. `p_cls` will contain the detected classes
    and `p_regr` fine-tuning information for the boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting the three arrays together to get the actual boxes, labels, and certainty
    is a matter of looping through the two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The list `boxes` now contains the detected cats and dogs. There are a lot of
    overlapping rectangles that can be resolved into each other.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Faster RCNN algorithm is an evolution of the Fast RCNN algorithm, which
    in turn was an improvement on the original RCNN. All these algorithms work similarly;
    a region proposer comes up with possible rectangles that might contain interesting
    images and the image classifier then detects what—if anything—can be seen there.
    The approach is not so different from what we did in the previous recipe, where
    our region proposer just produced 64 subcrops of an image.
  prefs: []
  type: TYPE_NORMAL
- en: Jian Sun, who came up with Faster RCNN, quite cleverly observed that the CNN
    that produces the feature map we used in the previous recipe could also be a good
    source for region proposals. So instead of treating the problem of region proposing
    separately, Faster RCNN trains the region proposal in parallel on the same feature
    map on which the image classification is done.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about the evolution of RCNN to Faster RCNN and how these
    algorithms work in the Athelas blog post [“A Brief History of CNNs in Image Segmentation:
    From R-CNN to Mask-CNN.”](https://bit.ly/2oUCh88)'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Running Faster RCNN over Our Own Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to train a Faster RCNN model, but don’t want to have to start from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start training from a pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: Training from scratch requires a lot of labeled data. The VOC dataset contains
    more than 20,000 labeled images for 20 classes. So what do we do if we don’t have
    that much labeled data? We can use the transfer learning trick we came across
    first in [Chapter 9](ch09.html#transfer_learning).
  prefs: []
  type: TYPE_NORMAL
- en: The training script already loads weights if it is restarted; what we need to
    do is convert the weights from the network trained on the VOC dataset to our own.
    In the previous recipe we constructed a dual network and loaded weights. As long
    as our new task is similar to the VOC classification task, all we need to do is
    change the number of classes, write back the weights, and start training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to do this is to let the training script run just long enough
    for it to write its configuration file and then use that configuration file and
    the previously loaded model to get to these weights. For training our own data,
    it is best to use the comma-separated format described on GitHub with the format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `filepath` should be the full path to the image and `x1`, `y1`, `x2`,
    and `y2` form the bounding box in pixels on that image. We can now train the model
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, after we’ve loaded the pretrained model as before, we can load the new
    configuration file with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the training model only depends on the number of classes for
    the classifier object. So, we want to reconstruct the classifier object and any
    object that depend on it, then save the weights. That way we’ve constructed our
    new model based on the old weights. If we peek into the code that constructs the
    classifier, we see that it all depends on the third-to-last layer. So let’s copy
    that code, but run it using the `new_config`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'With the new classifier in hand, we can construct the model as before and save
    the weights. These weights will retain what the model learned before, but have
    zeros for the classifier bit that is specific to the new training task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now continue training with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most examples of transfer learning are based on image recognition networks.
    This is partly because of the easy availability of pretrained networks and the
    fact that getting a training set of labeled images is straightforward. In this
    recipe we saw that we can apply this technique in other situations too. All we
    need is a pretrained network and an insight into how the network is constructed.
    By loading up the network weights, modifying the network for the new dataset,
    and saving the weights again, we can increase the speed of learning dramatically.
  prefs: []
  type: TYPE_NORMAL
- en: Even in situations where we don’t have a pretrained network available, but where
    there is a large set of public training data available and our own dataset is
    small, it might make sense to first train on the public dataset and then transfer
    that learning to our own set. For the bounding box case discussed in this recipe,
    this could easily be the case.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your own dataset is small, it might make sense to experiment with setting
    part of the network to untrainable as we did in [Recipe 9.7](ch09.html#retraining-image-recognition-networks).
  prefs: []
  type: TYPE_NORMAL
