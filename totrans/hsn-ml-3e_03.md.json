["```py\nfrom pathlib import Path\nimport pandas as pd\nimport tarfile\nimport urllib.request\n\ndef load_housing_data():\n    tarball_path = Path(\"datasets/housing.tgz\")\n    if not tarball_path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n        urllib.request.urlretrieve(url, tarball_path)\n        with tarfile.open(tarball_path) as housing_tarball:\n            housing_tarball.extractall(path=\"datasets\")\n    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n\nhousing = load_housing_data()\n```", "```py\n>>> housing.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype\n---  ------              --------------  -----\n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object\ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n```", "```py\n>>> housing[\"ocean_proximity\"].value_counts()\n<1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64\n```", "```py\nimport matplotlib.pyplot as plt\n\nhousing.hist(bins=50, figsize=(12, 8))\nplt.show()\n```", "```py\nimport numpy as np\n\ndef shuffle_and_split_data(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n```", "```py\n>>> train_set, test_set = shuffle_and_split_data(housing, 0.2)\n>>> len(train_set)\n16512\n>>> len(test_set)\n4128\n```", "```py\nfrom zlib import crc32\n\ndef is_id_in_test_set(identifier, test_ratio):\n    return crc32(np.int64(identifier)) < test_ratio * 2**32\n\ndef split_data_with_id_hash(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set]\n```", "```py\nhousing_with_id = housing.reset_index()  # adds an `index` column\ntrain_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"index\")\n```", "```py\nhousing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"id\")\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n```", "```py\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\n```", "```py\nhousing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\nplt.xlabel(\"Income category\")\nplt.ylabel(\"Number of districts\")\nplt.show()\n```", "```py\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\nstrat_splits = []\nfor train_index, test_index in splitter.split(housing, housing[\"income_cat\"]):\n    strat_train_set_n = housing.iloc[train_index]\n    strat_test_set_n = housing.iloc[test_index]\n    strat_splits.append([strat_train_set_n, strat_test_set_n])\n```", "```py\nstrat_train_set, strat_test_set = strat_splits[0]\n```", "```py\nstrat_train_set, strat_test_set = train_test_split(\n    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)\n```", "```py\n>>> strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n3    0.350533\n2    0.318798\n4    0.176357\n5    0.114341\n1    0.039971\nName: income_cat, dtype: float64\n```", "```py\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)\n```", "```py\nhousing = strat_train_set.copy()\n```", "```py\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\nplt.show()\n```", "```py\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\nplt.show()\n```", "```py\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n             s=housing[\"population\"] / 100, label=\"population\",\n             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n             legend=True, sharex=False, figsize=(10, 7))\nplt.show()\n```", "```py\ncorr_matrix = housing.corr()\n```", "```py\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\nmedian_house_value    1.000000\nmedian_income         0.688380\ntotal_rooms           0.137455\nhousing_median_age    0.102175\nhouseholds            0.071426\ntotal_bedrooms        0.054635\npopulation           -0.020153\nlongitude            -0.050859\nlatitude             -0.139584\nName: median_house_value, dtype: float64\n```", "```py\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\nplt.show()\n```", "```py\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1, grid=True)\nplt.show()\n```", "```py\nhousing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\nhousing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\nhousing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]\n```", "```py\n>>> corr_matrix = housing.corr()\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\nmedian_house_value    1.000000\nmedian_income         0.688380\nrooms_per_house       0.143663\ntotal_rooms           0.137455\nhousing_median_age    0.102175\nhouseholds            0.071426\ntotal_bedrooms        0.054635\npopulation           -0.020153\npeople_per_house     -0.038224\nlongitude            -0.050859\nlatitude             -0.139584\nbedrooms_ratio       -0.256397\nName: median_house_value, dtype: float64\n```", "```py\nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\n```", "```py\nhousing.dropna(subset=[\"total_bedrooms\"], inplace=True)  # option 1\n\nhousing.drop(\"total_bedrooms\", axis=1)  # option 2\n\nmedian = housing[\"total_bedrooms\"].median()  # option 3\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\n```", "```py\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"median\")\n```", "```py\nhousing_num = housing.select_dtypes(include=[np.number])\n```", "```py\nimputer.fit(housing_num)\n```", "```py\n>>> imputer.statistics_\narray([-118.51 , 34.26 , 29\\. , 2125\\. , 434\\. , 1167\\. , 408\\. , 3.5385])\n>>> housing_num.median().values\narray([-118.51 , 34.26 , 29\\. , 2125\\. , 434\\. , 1167\\. , 408\\. , 3.5385])\n```", "```py\nX = imputer.transform(housing_num)\n```", "```py\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing_num.index)\n```", "```py\n>>> housing_cat = housing[[\"ocean_proximity\"]]\n>>> housing_cat.head(8)\n ocean_proximity\n13096        NEAR BAY\n14973       <1H OCEAN\n3785           INLAND\n14689          INLAND\n20507      NEAR OCEAN\n1286           INLAND\n18078       <1H OCEAN\n4396         NEAR BAY\n```", "```py\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n```", "```py\n>>> housing_cat_encoded[:8]\narray([[3.],\n [0.],\n [1.],\n [1.],\n [4.],\n [1.],\n [0.],\n [3.]])\n```", "```py\n>>> ordinal_encoder.categories_\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n dtype=object)]\n```", "```py\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n```", "```py\n>>> housing_cat_1hot\n<16512x5 sparse matrix of type '<class 'numpy.float64'>'\n with 16512 stored elements in Compressed Sparse Row format>\n```", "```py\n>>> housing_cat_1hot.toarray()\narray([[0., 0., 0., 1., 0.],\n [1., 0., 0., 0., 0.],\n [0., 1., 0., 0., 0.],\n ...,\n [0., 0., 0., 0., 1.],\n [1., 0., 0., 0., 0.],\n [0., 0., 0., 0., 1.]])\n```", "```py\n>>> cat_encoder.categories_\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n dtype=object)]\n```", "```py\n>>> df_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\"]})\n>>> pd.get_dummies(df_test)\n ocean_proximity_INLAND  ocean_proximity_NEAR BAY\n0                       1                         0\n1                       0                         1\n```", "```py\n>>> cat_encoder.transform(df_test)\narray([[0., 1., 0., 0., 0.],\n [0., 0., 0., 1., 0.]])\n```", "```py\n>>> df_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"<2H OCEAN\", \"ISLAND\"]})\n>>> pd.get_dummies(df_test_unknown)\n ocean_proximity_<2H OCEAN  ocean_proximity_ISLAND\n0                          1                       0\n1                          0                       1\n```", "```py\n>>> cat_encoder.handle_unknown = \"ignore\"\n>>> cat_encoder.transform(df_test_unknown)\narray([[0., 0., 0., 0., 0.],\n [0., 0., 1., 0., 0.]])\n```", "```py\n>>> cat_encoder.feature_names_in_\narray(['ocean_proximity'], dtype=object)\n>>> cat_encoder.get_feature_names_out()\narray(['ocean_proximity_<1H OCEAN', 'ocean_proximity_INLAND',\n 'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',\n 'ocean_proximity_NEAR OCEAN'], dtype=object)\n>>> df_output = pd.DataFrame(cat_encoder.transform(df_test_unknown),\n...                          columns=cat_encoder.get_feature_names_out(),\n...                          index=df_test_unknown.index)\n...\n```", "```py\nfrom sklearn.preprocessing import MinMaxScaler\n\nmin_max_scaler = MinMaxScaler(feature_range=(-1, 1))\nhousing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\nhousing_num_std_scaled = std_scaler.fit_transform(housing_num)\n```", "```py\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nage_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1)\n```", "```py\nfrom sklearn.linear_model import LinearRegression\n\ntarget_scaler = StandardScaler()\nscaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n\nmodel = LinearRegression()\nmodel.fit(housing[[\"median_income\"]], scaled_labels)\nsome_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n\nscaled_predictions = model.predict(some_new_data)\npredictions = target_scaler.inverse_transform(scaled_predictions)\n```", "```py\nfrom sklearn.compose import TransformedTargetRegressor\n\nmodel = TransformedTargetRegressor(LinearRegression(),\n                                   transformer=StandardScaler())\nmodel.fit(housing[[\"median_income\"]], housing_labels)\npredictions = model.predict(some_new_data)\n```", "```py\nfrom sklearn.preprocessing import FunctionTransformer\n\nlog_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\nlog_pop = log_transformer.transform(housing[[\"population\"]])\n```", "```py\nrbf_transformer = FunctionTransformer(rbf_kernel,\n                                      kw_args=dict(Y=[[35.]], gamma=0.1))\nage_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])\n```", "```py\nsf_coords = 37.7749, -122.41\nsf_transformer = FunctionTransformer(rbf_kernel,\n                                     kw_args=dict(Y=[sf_coords], gamma=0.1))\nsf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])\n```", "```py\n>>> ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n>>> ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))\narray([[0.5 ],\n [0.75]])\n```", "```py\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_array, check_is_fitted\n\nclass StandardScalerClone(BaseEstimator, TransformerMixin):\n    def __init__(self, with_mean=True):  # no *args or **kwargs!\n        self.with_mean = with_mean\n\n    def fit(self, X, y=None):  # y is required even though we don't use it\n        X = check_array(X)  # checks that X is an array with finite float values\n        self.mean_ = X.mean(axis=0)\n        self.scale_ = X.std(axis=0)\n        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n        return self  # always return self!\n\n    def transform(self, X):\n        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n        X = check_array(X)\n        assert self.n_features_in_ == X.shape[1]\n        if self.with_mean:\n            X = X - self.mean_\n        return X / self.scale_\n```", "```py\nfrom sklearn.cluster import KMeans\n\nclass ClusterSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n        self.n_clusters = n_clusters\n        self.gamma = gamma\n        self.random_state = random_state\n\n    def fit(self, X, y=None, sample_weight=None):\n        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n        self.kmeans_.fit(X, sample_weight=sample_weight)\n        return self  # always return self!\n\n    def transform(self, X):\n        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n\n    def get_feature_names_out(self, names=None):\n        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n```", "```py\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\nsimilarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\n                                           sample_weight=housing_labels)\n```", "```py\n>>> similarities[:3].round(2)\narray([[0\\.  , 0.14, 0\\.  , 0\\.  , 0\\.  , 0.08, 0\\.  , 0.99, 0\\.  , 0.6 ],\n [0.63, 0\\.  , 0.99, 0\\.  , 0\\.  , 0\\.  , 0.04, 0\\.  , 0.11, 0\\.  ],\n [0\\.  , 0.29, 0\\.  , 0\\.  , 0.01, 0.44, 0\\.  , 0.7 , 0\\.  , 0.3 ]])\n```", "```py\nfrom sklearn.pipeline import Pipeline\n\nnum_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"standardize\", StandardScaler()),\n])\n```", "```py\nfrom sklearn.pipeline import make_pipeline\n\nnum_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n```", "```py\n>>> housing_num_prepared = num_pipeline.fit_transform(housing_num)\n>>> housing_num_prepared[:2].round(2)\narray([[-1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],\n [ 0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17]])\n```", "```py\ndf_housing_num_prepared = pd.DataFrame(\n    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),\n    index=housing_num.index)\n```", "```py\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\ncat_attribs = [\"ocean_proximity\"]\n\ncat_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"most_frequent\"),\n    OneHotEncoder(handle_unknown=\"ignore\"))\n\npreprocessing = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", cat_pipeline, cat_attribs),\n])\n```", "```py\nfrom sklearn.compose import make_column_selector, make_column_transformer\n\npreprocessing = make_column_transformer(\n    (num_pipeline, make_column_selector(dtype_include=np.number)),\n    (cat_pipeline, make_column_selector(dtype_include=object)),\n)\n```", "```py\nhousing_prepared = preprocessing.fit_transform(housing)\n```", "```py\ndef column_ratio(X):\n    return X[:, [0]] / X[:, [1]]\n\ndef ratio_name(function_transformer, feature_names_in):\n    return [\"ratio\"]  # feature names out\n\ndef ratio_pipeline():\n    return make_pipeline(\n        SimpleImputer(strategy=\"median\"),\n        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n        StandardScaler())\n\nlog_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"median\"),\n    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n    StandardScaler())\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\ndefault_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n                                     StandardScaler())\npreprocessing = ColumnTransformer([\n        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n                               \"households\", \"median_income\"]),\n        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n    ],\n    remainder=default_num_pipeline)  # one column remaining: housing_median_age\n```", "```py\n>>> housing_prepared = preprocessing.fit_transform(housing)\n>>> housing_prepared.shape\n(16512, 24)\n>>> preprocessing.get_feature_names_out()\narray(['bedrooms__ratio', 'rooms_per_house__ratio',\n 'people_per_house__ratio', 'log__total_bedrooms',\n 'log__total_rooms', 'log__population', 'log__households',\n 'log__median_income', 'geo__Cluster 0 similarity', [...],\n 'geo__Cluster 9 similarity', 'cat__ocean_proximity_<1H OCEAN',\n 'cat__ocean_proximity_INLAND', 'cat__ocean_proximity_ISLAND',\n 'cat__ocean_proximity_NEAR BAY', 'cat__ocean_proximity_NEAR OCEAN',\n 'remainder__housing_median_age'], dtype=object)\n```", "```py\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = make_pipeline(preprocessing, LinearRegression())\nlin_reg.fit(housing, housing_labels)\n```", "```py\n>>> housing_predictions = lin_reg.predict(housing)\n>>> housing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred\narray([243700., 372400., 128800.,  94400., 328300.])\n>>> housing_labels.iloc[:5].values\narray([458300., 483800., 101700.,  96100., 361800.])\n```", "```py\n>>> from sklearn.metrics import mean_squared_error\n>>> lin_rmse = mean_squared_error(housing_labels, housing_predictions,\n...                               squared=False)\n...\n>>> lin_rmse\n68687.89176589991\n```", "```py\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\ntree_reg.fit(housing, housing_labels)\n```", "```py\n>>> housing_predictions = tree_reg.predict(housing)\n>>> tree_rmse = mean_squared_error(housing_labels, housing_predictions,\n...                                squared=False)\n...\n>>> tree_rmse\n0.0\n```", "```py\nfrom sklearn.model_selection import cross_val_score\n\ntree_rmses = -cross_val_score(tree_reg, housing, housing_labels,\n                              scoring=\"neg_root_mean_squared_error\", cv=10)\n```", "```py\n>>> pd.Series(tree_rmses).describe()\ncount       10.000000\nmean     66868.027288\nstd       2060.966425\nmin      63649.536493\n25%      65338.078316\n50%      66801.953094\n75%      68229.934454\nmax      70094.778246\ndtype: float64\n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = make_pipeline(preprocessing,\n                           RandomForestRegressor(random_state=42))\nforest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\n                                scoring=\"neg_root_mean_squared_error\", cv=10)\n```", "```py\n>>> pd.Series(forest_rmses).describe()\ncount       10.000000\nmean     47019.561281\nstd       1033.957120\nmin      45458.112527\n25%      46464.031184\n50%      46967.596354\n75%      47325.694987\nmax      49243.765795\ndtype: float64\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\n\nfull_pipeline = Pipeline([\n    (\"preprocessing\", preprocessing),\n    (\"random_forest\", RandomForestRegressor(random_state=42)),\n])\nparam_grid = [\n    {'preprocessing__geo__n_clusters': [5, 8, 10],\n     'random_forest__max_features': [4, 6, 8]},\n    {'preprocessing__geo__n_clusters': [10, 15],\n     'random_forest__max_features': [6, 8, 10]},\n]\ngrid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n                           scoring='neg_root_mean_squared_error')\ngrid_search.fit(housing, housing_labels)\n```", "```py\n>>> grid_search.best_params_\n{'preprocessing__geo__n_clusters': 15, 'random_forest__max_features': 6}\n```", "```py\n>>> cv_res = pd.DataFrame(grid_search.cv_results_)\n>>> cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n>>> [...]  # change column names to fit on this page, and show rmse = -score\n>>> cv_res.head()  # note: the 1st column is the row ID\n n_clusters max_features  split0  split1  split2  mean_test_rmse\n12         15            6   43460   43919   44748           44042\n13         15            8   44132   44075   45010           44406\n14         15           10   44374   44286   45316           44659\n7          10            6   44683   44655   45657           44999\n9          10            6   44683   44655   45657           44999\n```", "```py\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),\n                  'random_forest__max_features': randint(low=2, high=20)}\n\nrnd_search = RandomizedSearchCV(\n    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\n    scoring='neg_root_mean_squared_error', random_state=42)\n\nrnd_search.fit(housing, housing_labels)\n```", "```py\n>>> final_model = rnd_search.best_estimator_  # includes preprocessing\n>>> feature_importances = final_model[\"random_forest\"].feature_importances_\n>>> feature_importances.round(2)\narray([0.07, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.19, [...], 0.01])\n```", "```py\n>>> sorted(zip(feature_importances,\n...            final_model[\"preprocessing\"].get_feature_names_out()),\n...            reverse=True)\n...\n[(0.18694559869103852, 'log__median_income'),\n (0.0748194905715524, 'cat__ocean_proximity_INLAND'),\n (0.06926417748515576, 'bedrooms__ratio'),\n (0.05446998753775219, 'rooms_per_house__ratio'),\n (0.05262301809680712, 'people_per_house__ratio'),\n (0.03819415873915732, 'geo__Cluster 0 similarity'),\n [...]\n (0.00015061247730531558, 'cat__ocean_proximity_NEAR BAY'),\n (7.301686597099842e-05, 'cat__ocean_proximity_ISLAND')]\n```", "```py\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nfinal_predictions = final_model.predict(X_test)\n\nfinal_rmse = mean_squared_error(y_test, final_predictions, squared=False)\nprint(final_rmse)  # prints 41424.40026462184\n```", "```py\n>>> from scipy import stats\n>>> confidence = 0.95\n>>> squared_errors = (final_predictions - y_test) ** 2\n>>> np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n...                          loc=squared_errors.mean(),\n...                          scale=stats.sem(squared_errors)))\n...\narray([39275.40861216, 43467.27680583])\n```", "```py\nimport joblib\n\njoblib.dump(final_model, \"my_california_housing_model.pkl\")\n```", "```py\nimport joblib\n[...]  # import KMeans, BaseEstimator, TransformerMixin, rbf_kernel, etc.\n\ndef column_ratio(X): [...]\ndef ratio_name(function_transformer, feature_names_in): [...]\nclass ClusterSimilarity(BaseEstimator, TransformerMixin): [...]\n\nfinal_model_reloaded = joblib.load(\"my_california_housing_model.pkl\")\n\nnew_data = [...]  # some new districts to make predictions for\npredictions = final_model_reloaded.predict(new_data)\n```"]