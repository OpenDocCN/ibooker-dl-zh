- en: Chapter 4\. Neural Network Development Reference Designs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter we covered NN development process at a high level, and
    you learned how to implement each stage in PyTorch. The examples in that chapter
    focused on solving an image classification problem with the CIFAR-10 dataset and
    a simple fully connected network. CIFAR-10 image classification is a good academic
    example to illustrate the NN development process, but there’s a lot more to developing
    deep learning models with PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter presents some additional reference designs for NN development with
    PyTorch. Reference designs are code examples that you can use as a reference to
    solve similar types of problems.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the set of reference designs in this chapter merely scratches the surface
    when it comes to the possibilities of deep learning; however, I’ll attempt to
    provide you with enough variety to assist you in the development of your own solutions.
    We will use three examples to process a variety of data, design different model
    architectures, and explore other approaches to the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: The first example uses PyTorch to perform transfer learning to classify images
    of bees and ants with a small dataset and a pretrained network. The second example
    uses PyTorch to perform sentiment analysis using text data to train an NLP model
    that predicts the positive or negative sentiment of movie reviews. And the third
    example uses PyTorch to demonstrate generative learning by training a generative
    adversarial network (GAN) to generate images of articles of clothing.
  prefs: []
  type: TYPE_NORMAL
- en: In each example, I’ll provide PyTorch code so that you can use this chapter
    as a quick reference when writing code for your own designs. Let’s begin by seeing
    how PyTorch can solve a computer vision problem using transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Image Classification with Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The subject of image classification has been studied in depth, and many famous
    models, like the AlexNet and VGG models we saw earlier, are readily available
    through PyTorch. However, these models have been trained with the ImageNet dataset.
    Although ImageNet contains 1,000 different image classes, it may not contain the
    classes that you need to solve your image classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, you can apply *transfer learning*, a process in which we fine-tune
    pretrained models with a much smaller dataset of new images. For our next example,
    we will train a model to classify images of bees and ants—classes not contained
    in ImageNet. Bees and ants look very similar and can be difficult to distinguish.
  prefs: []
  type: TYPE_NORMAL
- en: To train our new classifier, we will fine-tune another famous model, called
    ResNet18, by loading the pretrained model and training it with 120 new training
    images of bees and ants—a much smaller set compared to the millions of images
    in ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: Data Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s begin by loading our data, defining our transforms, and configuring our
    dataloaders for batch sampling. As we did earlier, we’ll leverage functions from
    the Torchvision library for creating the datasets, loading the data, and applying
    the data transforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'First let’s import the required libraries for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we’ll download the data that we’ll use for training and validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use the `io`, `urlib`, and `zipfile` libraries to download and unzip
    a file to our local filesystem. After running the previous code, you should have
    your training and validation images in your local *data/* folder. They are located
    in *data/hymenoptera_data/train* and *data/hymenoptera_data/val*, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Next let’s define our transforms, load the data, and configure our batch samplers.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we’ll define our transforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we randomly resize, crop, and flip images for training but not for
    validation. The “magic” numbers used in the `Normalize` transforms are precomputed
    values for the means and standard deviations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s define the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code we used the ImageFolder dataset to pull images from our
    data folders and set the transforms to the ones we defined earlier. Next, we define
    our dataloaders for batch iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We’re using a batch size of 4, and we set `num_workers` to `4` to configure
    four CPU processes to handle the parallel processing.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have prepared our training and validation data, we can design our
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Model Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this example we’ll use a ResNet18 model that has been pretrained with ImageNet
    data. However, ResNet18 is designed to detect 1,000 classes, and in our case,
    we only need 2 classes—bees and ants. We can modify the final layer to detect
    2 classes instead of 1,000 as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We first load a pretrained ResNet18 model using the function `torchvision.models.resnet18()`.
    Next, we read the number of features before the final layer with `model.fc.in_features`.
    Then we change the final layer by directly setting `model.fc` to a fully connected
    layer with two outputs.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use the pretrained model as a starting point and fine-tune its
    parameters with new data. Since we replaced the final linear layer, its parameters
    are now randomly initialized.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a ResNet18 model with all weights pretrained with ImageNet images
    except for the last layer. Next, we need to train our model with images of bees
    and ants.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Torchvision provides many famous pretrained models for computer vision and
    image processing, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VGG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SqueezeNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception v3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GoogLeNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ShuffleNet v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MobileNet v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNeXt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wide ResNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MNASNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, explore the `torchvision.models` class or visit the [Torchvision
    models documentation](https://pytorch.tips/torchvision-models).
  prefs: []
  type: TYPE_NORMAL
- en: Training and Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we fine-tune our model, let’s configure our training with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Move the model to a GPU if available.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Define our loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Define our optimizer algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_neural_network_development_reference_designs_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Use a learning rate scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: The code should look familiar, with the exception of the learning rate scheduler.
    Here we will use a scheduler from PyTorch to adjust the learning rate of our SGD
    optimizer after several epochs. Using a learning rate scheduler will help our
    NN adjust its weights more precisely as training goes on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates the entire training loop, including validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Training loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Schedule the learning rate for next the epoch of training.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Validation loop.
  prefs: []
  type: TYPE_NORMAL
- en: We should see the training and validation loss decrease while the accuracies
    improve. The results may bounce around a little.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s test our model and deploy it by saving the model to a file. To test our
    model, we’ll display a batch of images and show how our model classified them,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Define a new function to plot images from our tensor images.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Switch from C × H × W to H × W × C image formats for plotting.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Undo the normalization we do during transforms so we can properly view images.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_neural_network_development_reference_designs_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Grab a batch of images from our validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_neural_network_development_reference_designs_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Perform classification using our fine-tuned ResNet18.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_neural_network_development_reference_designs_CO3-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Take the “winning” class.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_neural_network_development_reference_designs_CO3-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Display the input images and their predicted classes.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have such a small dataset, we simply test the model by visualizing
    the output to make sure the images match the labels. [Figure 4-1](#fig_bees_ants)
    shows an example test. Your results will vary since the `val_loader` will return
    a randomly sampled batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: '![“Results of Image Classification”](Images/ptpr_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. Results of image classification
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When we are done, we save the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can use this reference design for other cases of transfer learning, not
    only with image classification but with other types of data as well. As long as
    you can find a suitable pretrained model, you will be able to modify the model
    and retrain only a portion of it with a small amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: This example was based on the ["*Transfer Learning for Computer Vision Tutorial*"](https://pytorch.tips/transfer-learning-tutorial)
    by Sasank Chilamkurthy. You can find more details in the tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll venture into the field of NLP and explore a reference design that
    processes text data.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Analysis with Torchtext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another popular deep learning application is *sentiment analysis*, in which
    people classify a block of text data. In this example, we will train an NN to
    predict whether a movie review is either positive or negative using the well-known
    Internet Movie Database (IMDb) dataset. Sentiment analysis of IMDb data is a common
    beginner example for learning NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Data Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The IMDb dataset consists of 25,000 movie reviews from IMDb that are labeled
    by sentiment (e.g., positive or negative). The PyTorch project includes a library
    called *Torchtext* that provides convenient capabilities for performing deep learning
    on text data. To begin our example reference design, we will use Torchtext to
    load and preprocess the IMDb dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Before we load the dataset, we will define a function called `generate_bigrams()`
    that we’ll use to preprocess our text review data. The model that we’ll use for
    this example computes *n*-grams of an input sentence and appends them to the end.
    We’ll use bi-grams, which are pairs of words or tokens that appear in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows our preprocessing function, `generate_bigrams()`,
    and provides an example of how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have defined our preprocessing function, we can build our IMDb
    datasets as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Load data from IMDb dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Redefine iterators as lists.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Split training data into two sets, 70% for training and 30% for validation.
  prefs: []
  type: TYPE_NORMAL
- en: In the code, we load the training and test datasets using the `IMDB` class.
    We then use the `random_split()` function to break the training data into two
    smaller sets for training and validation.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Torchtext API changed significantly in PyTorch 1.8\. Be sure you are using
    at least Torchtext 0.9 when running the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a quick look at the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, our datasets have 17,500 reviews for training, 7,500 for validation,
    and 25,000 for testing. We also printed out the 21st review and its sentiment,
    as shown in the output. The splits are randomly sampled, so your results may be
    different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we need to convert our text data into numerical data so that an NN can
    process it. We do this by creating preprocessing functions and a data pipeline.
    The data pipeline will use our `generate_bigrams()` function, a tokenizer, and
    a vocabulary, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Define our tokenizer (how to break up text).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Make a list of all the tokens used in our training data and count how many times
    each occurs.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a vocabulary (list of possible tokens) and define how tokens are converted
    to numbers.
  prefs: []
  type: TYPE_NORMAL
- en: In the code, we define the instructions for converting text to tensors. For
    the review text, we specify *spaCy* as the tokenizer. spaCy is a popular Python
    package for NLP and includes its own tokenizer. A tokenizer breaks text into components
    like words and punctuation marks.
  prefs: []
  type: TYPE_NORMAL
- en: We also create a vocabulary and an embedding. A vocabulary is just a set of
    words that we can use. If we find a word in the movie review that is not contained
    in the vocabulary, we set the word to a special word called “unknown.” We limit
    our dictionary to 25,000 words, much smaller than the full set of words in the
    English language.
  prefs: []
  type: TYPE_NORMAL
- en: We also specify our vocabulary vectors, which causes us to download a pretrained
    embedding called GloVe (Global Vectors for Word Representation) with 100 dimensions.
    It may take several minutes to download the GloVe data and create a vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: An embedding is a method to map a word or series of words to a numeric vector.
    Defining a vocabulary and an embedding is a complex topic and is beyond the scope
    of this book. For this example, we’ll just build a vocabulary from our training
    data and download the popular pretrained GloVe embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined our tokenizer and vocabulary, we can build our data
    pipelines for the review and label text data, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We use `lambda` functions to pass text data through the pipeline so that PyTorch
    dataloaders can convert each text review to a 100-element vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined our datasets and preprocessing, we can create our
    dataloaders. Our dataloaders load batches of data from a sampling of the dataset
    and preprocess the data, as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the code, we set the batch size to 64 and use a GPU if available. We also
    define a collation function called `collate_batch()` and pass it into our dataloaders
    to execute our data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have configured our pipelines and dataloaders, let’s define our
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Model Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this example we will use a model called FastText from the paper [“Bag of
    Tricks for Efficient Text Classification”](https://pytorch.tips/bag-of-tricks)
    by Armand Joulin et al. While many sentiment analysis models use RNNs, this model
    uses a simpler approach instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code implements the FastText model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model calculates the word embedding for each word using
    the `nn.Embedded` layer, and then it calculates the average of all the word embeddings
    with the `avg_pool2d()` function. Finally, it feeds the average through a linear
    layer. Refer to the paper for more details on this model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build our model with its appropriate parameters using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Rather than train our embedding layer from scratch, we’ll initialize the layer’s
    weights with pretrained embeddings. This process is similar to how we used pretrained
    weights in the transfer learning example in [“Image Classification with Transfer
    Learning”](#ictf):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Load the pretrained embedding from our vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the embedding layer’s weights.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the embedding weights of an unknown token to zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_neural_network_development_reference_designs_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the embedding weights of a pad token to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Now that it’s initialized properly, we can train our model.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training and validation process should look familiar. It’s similar to the
    one we’ve used in previous examples. First we configure our loss function and
    our optimizer algorithm, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are using the Adam optimizer and the `BCEWithLogitsLoss()`
    loss function. The Adam optimizer is a replacement for SGD and performs better
    for sparse or noisy gradients. The `BCEWithLogitsLoss()` function is commonly
    used for binary classification. We also move our model to a GPU if available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we run our training and validation loops, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We should see validation accuracies around 85–90% with only five epochs of training.
    Let’s see how our model performs against the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier, we constructed our `test_iterator` based on the IMDb test dataset.
    Recall that none of the data in the test dataset has been used for training or
    validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our test loop is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Not necessary for this model, but good practice.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we process one batch at a time and cumulate the accuracy
    over the entire test dataset. You should get 85–90% accuracy on the test set as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we’ll predict the sentiment of our own reviews, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: A result close to 0 corresponds to a negative review, while an output close
    to 1 indicates a positive review. As you can see, the model correctly predicted
    the sentiment of the sample review. Try it with some of your own movie reviews!
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we’ll save our model for deployment as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In this example, you learned how to preprocess text data and designed a FastText
    model for sentiment analysis. You also trained the model, evaluated its performance,
    and saved the model for deployment. You can use this design pattern and reference
    code to solve other sentiment analysis problems in your own work.
  prefs: []
  type: TYPE_NORMAL
- en: This example was based on the “Faster Sentiment Analysis” tutorial by Ben Trevett.
    You can find more details and other great Torchtext tutorials in his [PyTorch
    Sentiment Analysis GitHub repository](https://pytorch.tips/sentiment-tutorials).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to our final reference design, in which we will use deep learning
    and PyTorch to generate image data.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Learning—Generating Fashion-MNIST Images with DCGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most interesting areas of deep learning is *generative learning*,
    in which NNs are used to create data. Sometimes these NNs can create images, music,
    text, and time series data so well that it is difficult to tell the difference
    between real data and the generated data. Generative learning is used to create
    images of people and places that don’t exist, increase image resolution, predict
    frames in video, augment datasets, generate news articles, and convert styles
    of art and music.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I’ll show you how to use PyTorch for generative learning. The
    development process is similar to the previous examples; however, here we’ll use
    an unsupervised approach in which the data is not labeled.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we’ll design and train a GAN, which is quite different from the
    models and training loops of previous examples. Testing and evaluating the GAN
    involves a slightly different process as well. The overall development sequence
    is consistent with the process in [Chapter 2](ch02.xhtml#Chapter_2), but each
    part will be unique to generative learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will train a GAN to generate images similar to the training
    images used in the Fashion-MNIST dataset. Fashion-MNIST is a popular academic
    dataset used for image classification that includes images of articles of clothing.
    Let’s access the Fashion-MNIST data to get an idea of what these images look like,
    and then we’ll create some synthetic images based on what we’ve seen.
  prefs: []
  type: TYPE_NORMAL
- en: Data Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike models used for supervised learning, where the model learns the relationships
    between data and labels, generative models look to learn the distribution of the
    training data so as to generate data similar to the training data at hand. Therefore,
    in this example we only need training data, because if we build a good model and
    train it long enough, the model should begin to produce good synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First let’s import the required libraries, define some constants, and set our
    device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code loads the training data, defines the transforms, and creates
    a dataloader for batch iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This code should look familiar to you. We are once again using Torchvision functions
    to define the transforms, create a dataset, and set up a dataloader that will
    sample the dataset, apply transforms, and return a batch of images for our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can display a batch of images with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Torchvision provides a nice utility called `make_grid` to display a grid of
    images. [Figure 4-2](#fig_fashion_mnist) shows an example batch of Fashion-MNIST
    images.
  prefs: []
  type: TYPE_NORMAL
- en: '![“FashionMNIST Images”](Images/ptpr_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. Fashion-MNIST images
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s see what model we’ll use for our data generation task.
  prefs: []
  type: TYPE_NORMAL
- en: Model Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To generate new image data, we’ll use a GAN. The goal of the GAN model is to
    generate “fake” data based on the training data’s distribution. The GAN accomplishes
    this goal with two distinct modules: the generator and the discriminator.'
  prefs: []
  type: TYPE_NORMAL
- en: The job of the generator is to generate fake images that look real. The job
    of the discriminator is to correctly identify whether an image is fake. Although
    the design of GANs is beyond the scope of this book, I’ll provide a sample reference
    design using a deep convolutional GAN, or DCGAN.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: GANs were first described in the famous paper by Ian Goodfellow et al. in 2014
    titled [“Generative Adversarial Nets”](https://pytorch.tips/gan-paper). Guidelines
    for building more stable convolutional GANs were proposed by Alec Radford et al.
    in the 2015 paper [“Unsupervised Representation Learning with Deep Convolutional
    Generative Adversarial Networks”](https://pytorch.tips/dcgan-paper). This paper
    describes the DCGAN used in this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator is designed to create an image from an input vector of 100 random
    values. Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This example generator uses 2D convolutional transpose layers with batch normalization
    and ReLU activations. The layers are defined in the `__init__()` function. It
    works like our image classification models, except in reverse order.
  prefs: []
  type: TYPE_NORMAL
- en: That is, instead of reducing an image to a smaller representation, it takes
    a random vector and creates a full image from it. We also instantiate the `Generator`
    module as `netG`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the `Discriminator` module, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The discriminator is a binary classification network that determines the probability
    that the input image is real. This example discriminator NN uses 2D convolutional
    layers with batch normalization and leaky ReLU activation functions. We instantiate
    the `Discriminator` as `netD`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of the DCGAN paper found that it helps to initialize the weights
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have designed our two modules, we can set up and train the GAN.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training a GAN is somewhat more complicated than the previous training examples.
    In each epoch, we will first train the discriminator with a real batch of data,
    then use the generator to create a fake batch, and then train the discriminator
    with the generated fake batch of data. Lastly, we will train the generator NN
    to produce better fakes.
  prefs: []
  type: TYPE_NORMAL
- en: This is a good example of how powerful PyTorch is when creating custom training
    loops. It provides the flexibility to develop and implement new ideas with ease.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start training, we need to define the loss function and optimizers
    that will be used to train the generator and the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we define a label for real versus fake images. Then we
    use the binary cross entropy (BCE) loss function, which is commonly used for binary
    classification. Remember the discriminator is performing binary classification
    by classifying an image as real or fake. We use the commonly used Adam optimizer
    for updating the model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define values for the real and fake labels and create tensors for computing
    the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we start training, we will create lists for storing the errors and define
    a test vector to show the results later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can execute the training loop. If the GAN is stable, it should improve
    as more epochs are trained. The training loop is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Pass real images to the `Discriminator`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Pass fake images to the `Discriminator`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO8-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Run backpropagation and update the `Discriminator`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_neural_network_development_reference_designs_CO8-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Pass fake images to the updated `Discriminator`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_neural_network_development_reference_designs_CO8-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The `Generator` loss is based on cases in which the[.keep-together] `Discriminator`
    is wrong.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_neural_network_development_reference_designs_CO8-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Run backpropagation and update the `Generator`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_neural_network_development_reference_designs_CO8-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a batch of images and save them after each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve done in the previous examples, we loop through all the data, one batch
    at a time, using the dataloader during each epoch. First we train the discriminator
    with a batch of real images so it can compute the output, calculate the loss,
    and compute the gradients. Then we train the discriminator with a batch of fake
    images.
  prefs: []
  type: TYPE_NORMAL
- en: The fake images are created by the generator from a vector of random values.
    Again, we compute the discriminator output, calculate the loss, and compute the
    gradients. Next, we add the gradients from all the real and all the fake batches
    and apply backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: We compute the outputs from the freshly trained discriminator using the same
    fake data, and compute the loss or error of the generator. Using this loss, we
    compute the gradients and apply backpropagation on the generator itself.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we’ll keep track of the loss after each epoch to see if the GAN’s training
    is consistently improving and stable. [Figure 4-3](#fig_gan_curves) shows the
    loss curve for both the generator and the discriminator during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![“GAN Training Curves”](Images/ptpr_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. GAN training curves
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The loss curves plot the generator and the discriminator loss for each batch
    over all epochs, so the loss bounces around depending on the computed loss of
    the batch. We can see though that the loss in both cases has been reduced from
    the beginning of training. If we trained over more epochs, we’d look for these
    loss values to approach zero.
  prefs: []
  type: TYPE_NORMAL
- en: In general, GANs are tricky to train, and the learning rate, betas, and other
    optimizer hyperparameters can have a major impact.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine the average results of the discriminator for each batch over all
    the epochs, as shown in [Figure 4-4](#fig_d_results).
  prefs: []
  type: TYPE_NORMAL
- en: '![“Discriminator Results”](Images/ptpr_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. Discriminator results
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the GAN was perfect, the discriminator would not be able to correctly identify
    fake images as fake or real images as real, and we would expect the average error
    to be 0.5 in both cases. The results show that some batches are close to 0.5,
    but we can certainly do better.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have trained our network, let’s see how well it does at creating
    fake images of clothing.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During supervised learning, we usually set aside a test dataset that has not
    been used to train or validate the model. In generative learning, there are no
    labels produced by the generator. We could pass our generated images into a Fashion-MNIST
    classifier, but there’s no way for us to know if the errors are caused by the
    classifier or the GAN unless we hand-label the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s test and evaluate our GAN by comparing the results from the first
    epoch with the generated images from the last epoch. We create a test vector,
    `z`, for testing and use the computed generator results at the end of each epoch
    in our training loop code.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-5](#fig_fmnist_0) shows the generated images from the first epoch,
    while [Figure 4-6](#fig_fmnist_4) shows the results after training only five epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![“Generator Results (First Epoch)”](Images/ptpr_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. Generator results (first epoch)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![“Generator Results (Last Epoch)”](Images/ptpr_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. Generator results (last epoch)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can see that the generator has improved some. Look at the boot at the end
    of the second row or the shirt at the end of the third row. Our GAN is not perfect,
    but it seems to be improving after just five epochs. Training over more epochs
    or improving our design might produce even better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can save our trained model for deployment and use it to generate
    more synthetic Fashion-MNIST images using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We expanded our PyTorch deep learning capabilities by designing and training
    a GAN in this generative learning reference design. You can use this reference
    design to create and train other GAN models and test their performance at generating
    new data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered additional examples to show a variety of data processing,
    model design, and training approaches with PyTorch—but what if you have an amazing
    idea for some new, innovative NN? Or what if you come up with a new optimization
    algorithm or loss function that nobody’s seen before? In the next chapter, I’ll
    show you how to create your own custom modules and functions so you can expand
    your deep learning research and experiment with new ideas.
  prefs: []
  type: TYPE_NORMAL
