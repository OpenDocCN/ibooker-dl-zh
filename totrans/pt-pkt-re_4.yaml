- en: Chapter 4\. Neural Network Development Reference Designs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章. 神经网络开发参考设计
- en: In the previous chapter we covered NN development process at a high level, and
    you learned how to implement each stage in PyTorch. The examples in that chapter
    focused on solving an image classification problem with the CIFAR-10 dataset and
    a simple fully connected network. CIFAR-10 image classification is a good academic
    example to illustrate the NN development process, but there’s a lot more to developing
    deep learning models with PyTorch.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们以高层次介绍了NN开发过程，并学习了如何在PyTorch中实现每个阶段。该章节中的示例侧重于使用CIFAR-10数据集和简单的全连接网络解决图像分类问题。CIFAR-10图像分类是一个很好的学术示例，用来说明NN开发过程，但在使用PyTorch开发深度学习模型时还有很多内容。
- en: This chapter presents some additional reference designs for NN development with
    PyTorch. Reference designs are code examples that you can use as a reference to
    solve similar types of problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一些用于PyTorch中NN开发的参考设计。参考设计是代码示例，您可以将其用作解决类似类型问题的参考。
- en: Indeed, the set of reference designs in this chapter merely scratches the surface
    when it comes to the possibilities of deep learning; however, I’ll attempt to
    provide you with enough variety to assist you in the development of your own solutions.
    We will use three examples to process a variety of data, design different model
    architectures, and explore other approaches to the learning process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，本章中的参考设计仅仅触及了深度学习的可能性表面；然而，我将尝试为您提供足够多样性的内容，以帮助您开发自己的解决方案。我们将使用三个示例来处理各种数据，设计不同的模型架构，并探索学习过程的其他方法。
- en: The first example uses PyTorch to perform transfer learning to classify images
    of bees and ants with a small dataset and a pretrained network. The second example
    uses PyTorch to perform sentiment analysis using text data to train an NLP model
    that predicts the positive or negative sentiment of movie reviews. And the third
    example uses PyTorch to demonstrate generative learning by training a generative
    adversarial network (GAN) to generate images of articles of clothing.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个示例使用PyTorch执行迁移学习，使用小数据集和预训练网络对蜜蜂和蚂蚁的图像进行分类。第二个示例使用PyTorch执行情感分析，使用文本数据训练一个NLP模型，预测电影评论的积极或消极情感。第三个示例使用PyTorch展示生成学习，通过训练生成对抗网络（GAN）生成服装的图像。
- en: In each example, I’ll provide PyTorch code so that you can use this chapter
    as a quick reference when writing code for your own designs. Let’s begin by seeing
    how PyTorch can solve a computer vision problem using transfer learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个示例中，我将提供PyTorch代码，以便您可以将本章作为快速参考，用于编写自己设计的代码。让我们开始看看PyTorch如何使用迁移学习解决计算机视觉问题。
- en: Image Classification with Transfer Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习进行图像分类
- en: The subject of image classification has been studied in depth, and many famous
    models, like the AlexNet and VGG models we saw earlier, are readily available
    through PyTorch. However, these models have been trained with the ImageNet dataset.
    Although ImageNet contains 1,000 different image classes, it may not contain the
    classes that you need to solve your image classification problem.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类的主题已经深入研究，许多著名的模型，如之前看到的AlexNet和VGG模型，都可以通过PyTorch轻松获得。然而，这些模型是使用ImageNet数据集进行训练的。虽然ImageNet包含1,000个不同的图像类别，但可能不包含您需要解决的图像分类问题的类别。
- en: In this case, you can apply *transfer learning*, a process in which we fine-tune
    pretrained models with a much smaller dataset of new images. For our next example,
    we will train a model to classify images of bees and ants—classes not contained
    in ImageNet. Bees and ants look very similar and can be difficult to distinguish.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您可以应用*迁移学习*，这是一个过程，通过在一个更小的新图像数据集上微调预训练模型。在下一个示例中，我们将训练一个模型来对蜜蜂和蚂蚁的图像进行分类——这些类别不包含在ImageNet中。蜜蜂和蚂蚁看起来非常相似，很难区分。
- en: To train our new classifier, we will fine-tune another famous model, called
    ResNet18, by loading the pretrained model and training it with 120 new training
    images of bees and ants—a much smaller set compared to the millions of images
    in ImageNet.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的新分类器，我们将微调另一个著名的模型ResNet18，通过加载预训练模型并使用120张新的蜜蜂和蚂蚁的训练图像进行训练——与ImageNet中数百万张图像相比，这是一个更小的数据集。
- en: Data Processing
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据处理
- en: Let’s begin by loading our data, defining our transforms, and configuring our
    dataloaders for batch sampling. As we did earlier, we’ll leverage functions from
    the Torchvision library for creating the datasets, loading the data, and applying
    the data transforms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载数据，定义转换和配置数据加载器以进行批量采样开始。与之前一样，我们将利用Torchvision库中的函数来创建数据集，加载数据并应用数据转换。
- en: 'First let’s import the required libraries for this example:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们导入本示例所需的库：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we’ll download the data that we’ll use for training and validation:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将下载用于训练和验证的数据：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we use the `io`, `urlib`, and `zipfile` libraries to download and unzip
    a file to our local filesystem. After running the previous code, you should have
    your training and validation images in your local *data/* folder. They are located
    in *data/hymenoptera_data/train* and *data/hymenoptera_data/val*, respectively.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`io`、`urlib`和`zipfile`库来下载并解压文件到本地文件系统。运行前面的代码后，您应该在本地*data/*文件夹中拥有您的训练和验证图像。它们分别位于*data/hymenoptera_data/train*和*data/hymenoptera_data/val*中。
- en: Next let’s define our transforms, load the data, and configure our batch samplers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们定义我们的转换，加载数据，并配置我们的批采样器。
- en: 'First we’ll define our transforms:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们定义我们的转换：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that we randomly resize, crop, and flip images for training but not for
    validation. The “magic” numbers used in the `Normalize` transforms are precomputed
    values for the means and standard deviations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在训练时随机调整、裁剪和翻转图像，但在验证时不这样做。`Normalize`转换中使用的“魔术”数字是预先计算的均值和标准差。
- en: 'Now let’s define the datasets:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义数据集：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the previous code we used the ImageFolder dataset to pull images from our
    data folders and set the transforms to the ones we defined earlier. Next, we define
    our dataloaders for batch iteration:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We’re using a batch size of 4, and we set `num_workers` to `4` to configure
    four CPU processes to handle the parallel processing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have prepared our training and validation data, we can design our
    model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Model Design
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this example we’ll use a ResNet18 model that has been pretrained with ImageNet
    data. However, ResNet18 is designed to detect 1,000 classes, and in our case,
    we only need 2 classes—bees and ants. We can modify the final layer to detect
    2 classes instead of 1,000 as shown in the following code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We first load a pretrained ResNet18 model using the function `torchvision.models.resnet18()`.
    Next, we read the number of features before the final layer with `model.fc.in_features`.
    Then we change the final layer by directly setting `model.fc` to a fully connected
    layer with two outputs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use the pretrained model as a starting point and fine-tune its
    parameters with new data. Since we replaced the final linear layer, its parameters
    are now randomly initialized.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a ResNet18 model with all weights pretrained with ImageNet images
    except for the last layer. Next, we need to train our model with images of bees
    and ants.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Torchvision provides many famous pretrained models for computer vision and
    image processing, including the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VGG
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SqueezeNet
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseNet
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception v3
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GoogLeNet
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ShuffleNet v2
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MobileNet v2
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNeXt
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wide ResNet
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MNASNet
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, explore the `torchvision.models` class or visit the [Torchvision
    models documentation](https://pytorch.tips/torchvision-models).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Training and Validation
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we fine-tune our model, let’s configure our training with the following
    code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO1-1)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Move the model to a GPU if available.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO1-2)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Define our loss function.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO1-3)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Define our optimizer algorithm.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_neural_network_development_reference_designs_CO1-4)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Use a learning rate scheduler.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The code should look familiar, with the exception of the learning rate scheduler.
    Here we will use a scheduler from PyTorch to adjust the learning rate of our SGD
    optimizer after several epochs. Using a learning rate scheduler will help our
    NN adjust its weights more precisely as training goes on.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates the entire training loop, including validation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO2-1)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Training loop.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO2-2)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Schedule the learning rate for next the epoch of training.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO2-3)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Validation loop.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: We should see the training and validation loss decrease while the accuracies
    improve. The results may bounce around a little.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Testing and Deployment
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s test our model and deploy it by saving the model to a file. To test our
    model, we’ll display a batch of images and show how our model classified them,
    as shown in the following code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO3-1)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Define a new function to plot images from our tensor images.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO3-2)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Switch from C × H × W to H × W × C image formats for plotting.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO3-3)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Undo the normalization we do during transforms so we can properly view images.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_neural_network_development_reference_designs_CO3-4)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Grab a batch of images from our validation dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_neural_network_development_reference_designs_CO3-5)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Perform classification using our fine-tuned ResNet18.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_neural_network_development_reference_designs_CO3-6)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Take the “winning” class.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_neural_network_development_reference_designs_CO3-7)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Display the input images and their predicted classes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Since we have such a small dataset, we simply test the model by visualizing
    the output to make sure the images match the labels. [Figure 4-1](#fig_bees_ants)
    shows an example test. Your results will vary since the `val_loader` will return
    a randomly sampled batch of images.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![“Results of Image Classification”](Images/ptpr_0401.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. Results of image classification
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When we are done, we save the model:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can use this reference design for other cases of transfer learning, not
    only with image classification but with other types of data as well. As long as
    you can find a suitable pretrained model, you will be able to modify the model
    and retrain only a portion of it with a small amount of data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: This example was based on the ["*Transfer Learning for Computer Vision Tutorial*"](https://pytorch.tips/transfer-learning-tutorial)
    by Sasank Chilamkurthy. You can find more details in the tutorial.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll venture into the field of NLP and explore a reference design that
    processes text data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Analysis with Torchtext
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another popular deep learning application is *sentiment analysis*, in which
    people classify a block of text data. In this example, we will train an NN to
    predict whether a movie review is either positive or negative using the well-known
    Internet Movie Database (IMDb) dataset. Sentiment analysis of IMDb data is a common
    beginner example for learning NLP.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Data Processing
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The IMDb dataset consists of 25,000 movie reviews from IMDb that are labeled
    by sentiment (e.g., positive or negative). The PyTorch project includes a library
    called *Torchtext* that provides convenient capabilities for performing deep learning
    on text data. To begin our example reference design, we will use Torchtext to
    load and preprocess the IMDb dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Before we load the dataset, we will define a function called `generate_bigrams()`
    that we’ll use to preprocess our text review data. The model that we’ll use for
    this example computes *n*-grams of an input sentence and appends them to the end.
    We’ll use bi-grams, which are pairs of words or tokens that appear in a sentence.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows our preprocessing function, `generate_bigrams()`,
    and provides an example of how it works:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we have defined our preprocessing function, we can build our IMDb
    datasets as shown in the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO4-1)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Load data from IMDb dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO4-2)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Redefine iterators as lists.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO4-3)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Split training data into two sets, 70% for training and 30% for validation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: In the code, we load the training and test datasets using the `IMDB` class.
    We then use the `random_split()` function to break the training data into two
    smaller sets for training and validation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Torchtext API changed significantly in PyTorch 1.8\. Be sure you are using
    at least Torchtext 0.9 when running the code.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a quick look at the data:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, our datasets have 17,500 reviews for training, 7,500 for validation,
    and 25,000 for testing. We also printed out the 21st review and its sentiment,
    as shown in the output. The splits are randomly sampled, so your results may be
    different.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we need to convert our text data into numerical data so that an NN can
    process it. We do this by creating preprocessing functions and a data pipeline.
    The data pipeline will use our `generate_bigrams()` function, a tokenizer, and
    a vocabulary, as shown in the following code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO5-1)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Define our tokenizer (how to break up text).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们的分词器（如何分割文本）。
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO5-2)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO5-2)'
- en: Make a list of all the tokens used in our training data and count how many times
    each occurs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列出我们训练数据中使用的所有标记，并计算每个标记出现的次数。
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO5-3)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO5-3)'
- en: Create a vocabulary (list of possible tokens) and define how tokens are converted
    to numbers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个词汇表（可能标记的列表）并定义如何将标记转换为数字。
- en: In the code, we define the instructions for converting text to tensors. For
    the review text, we specify *spaCy* as the tokenizer. spaCy is a popular Python
    package for NLP and includes its own tokenizer. A tokenizer breaks text into components
    like words and punctuation marks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们定义了将文本转换为张量的指令。对于评论文本，我们指定*spaCy*作为分词器。spaCy是一个流行的Python包，用于自然语言处理，并包含自己的分词器。分词器将文本分解为词和标点等组件。
- en: We also create a vocabulary and an embedding. A vocabulary is just a set of
    words that we can use. If we find a word in the movie review that is not contained
    in the vocabulary, we set the word to a special word called “unknown.” We limit
    our dictionary to 25,000 words, much smaller than the full set of words in the
    English language.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了一个词汇表和一个嵌入。词汇表只是我们可以使用的一组单词。如果我们在电影评论中发现一个词不在词汇表中，我们将该词设置为一个特殊的单词“未知”。我们将我们的字典限制在25,000个单词，远远小于英语语言中的所有单词。
- en: We also specify our vocabulary vectors, which causes us to download a pretrained
    embedding called GloVe (Global Vectors for Word Representation) with 100 dimensions.
    It may take several minutes to download the GloVe data and create a vocabulary.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还指定了我们的词汇向量，这导致我们下载了一个名为GloVe（全局词向量表示）的预训练嵌入，具有100个维度。下载GloVe数据并创建词汇表可能需要几分钟。
- en: An embedding is a method to map a word or series of words to a numeric vector.
    Defining a vocabulary and an embedding is a complex topic and is beyond the scope
    of this book. For this example, we’ll just build a vocabulary from our training
    data and download the popular pretrained GloVe embedding.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是将单词或一系列单词映射到数值向量的方法。定义词汇表和嵌入是一个复杂的话题，超出了本书的范围。在这个例子中，我们将从训练数据中构建一个词汇表，并下载流行的预训练的GloVe嵌入。
- en: 'Now that we have defined our tokenizer and vocabulary, we can build our data
    pipelines for the review and label text data, as shown in the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的分词器和词汇表，我们可以为评论和标签文本数据构建我们的数据管道，如以下代码所示：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We use `lambda` functions to pass text data through the pipeline so that PyTorch
    dataloaders can convert each text review to a 100-element vector.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`lambda`函数通过管道传递文本数据，以便PyTorch数据加载器可以将每个文本评论转换为一个100元素的向量。
- en: 'Now that we have defined our datasets and preprocessing, we can create our
    dataloaders. Our dataloaders load batches of data from a sampling of the dataset
    and preprocess the data, as in the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的数据集和预处理，我们可以创建我们的数据加载器。我们的数据加载器从数据集的采样中加载数据批次，并预处理数据，如以下代码所示：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the code, we set the batch size to 64 and use a GPU if available. We also
    define a collation function called `collate_batch()` and pass it into our dataloaders
    to execute our data pipelines.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们将批处理大小设置为64，并在有GPU时使用。我们还定义了一个名为`collate_batch()`的整理函数，并将其传递给我们的数据加载器以执行我们的数据管道。
- en: Now that we have configured our pipelines and dataloaders, let’s define our
    model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配置了我们的管道和数据加载器，让我们定义我们的模型。
- en: Model Design
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型设计
- en: For this example we will use a model called FastText from the paper [“Bag of
    Tricks for Efficient Text Classification”](https://pytorch.tips/bag-of-tricks)
    by Armand Joulin et al. While many sentiment analysis models use RNNs, this model
    uses a simpler approach instead.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用一种称为FastText的模型，该模型来自Armand Joulin等人的论文“高效文本分类的技巧袋”。虽然许多情感分析模型使用RNN，但这个模型使用了一种更简单的方法。
- en: 'The following code implements the FastText model:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实现了FastText模型：
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, the model calculates the word embedding for each word using
    the `nn.Embedded` layer, and then it calculates the average of all the word embeddings
    with the `avg_pool2d()` function. Finally, it feeds the average through a linear
    layer. Refer to the paper for more details on this model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，该模型使用`nn.Embedded`层为每个单词计算单词嵌入，然后使用`avg_pool2d()`函数计算所有单词嵌入的平均值。最后，它通过一个线性层传递平均值。有关此模型的更多详细信息，请参考论文。
- en: 'Let’s build our model with its appropriate parameters using the following code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码构建我们的模型及其适当的参数：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Rather than train our embedding layer from scratch, we’ll initialize the layer’s
    weights with pretrained embeddings. This process is similar to how we used pretrained
    weights in the transfer learning example in [“Image Classification with Transfer
    Learning”](#ictf):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会从头开始训练我们的嵌入层，而是使用预训练的嵌入来初始化层的权重。这个过程类似于我们在“使用迁移学习进行图像分类”示例中使用预训练权重的方式：
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO6-1)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO6-1)'
- en: Load the pretrained embedding from our vocabulary.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的词汇表中加载预训练的嵌入。
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO6-2)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO6-2)'
- en: Initialize the embedding layer’s weights.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化嵌入层的权重。
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO6-3)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO6-3)'
- en: Initialize the embedding weights of an unknown token to zero.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 将未知标记的嵌入权重初始化为零。
- en: '[![4](Images/4.png)](#co_neural_network_development_reference_designs_CO6-4)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_neural_network_development_reference_designs_CO6-4)'
- en: Initialize the embedding weights of a pad token to zero.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 将填充标记的嵌入权重初始化为零。
- en: Now that it’s initialized properly, we can train our model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它已经正确初始化，我们可以训练我们的模型。
- en: Training and Validation
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和验证
- en: 'The training and validation process should look familiar. It’s similar to the
    one we’ve used in previous examples. First we configure our loss function and
    our optimizer algorithm, as shown in the following code:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this example, we are using the Adam optimizer and the `BCEWithLogitsLoss()`
    loss function. The Adam optimizer is a replacement for SGD and performs better
    for sparse or noisy gradients. The `BCEWithLogitsLoss()` function is commonly
    used for binary classification. We also move our model to a GPU if available.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we run our training and validation loops, as shown in the following code:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We should see validation accuracies around 85–90% with only five epochs of training.
    Let’s see how our model performs against the test dataset.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Testing and Deployment
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier, we constructed our `test_iterator` based on the IMDb test dataset.
    Recall that none of the data in the test dataset has been used for training or
    validation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Our test loop is shown in the following code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO7-1)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Not necessary for this model, but good practice.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we process one batch at a time and cumulate the accuracy
    over the entire test dataset. You should get 85–90% accuracy on the test set as
    well.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we’ll predict the sentiment of our own reviews, using the following code:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: A result close to 0 corresponds to a negative review, while an output close
    to 1 indicates a positive review. As you can see, the model correctly predicted
    the sentiment of the sample review. Try it with some of your own movie reviews!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we’ll save our model for deployment as shown in the following code:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this example, you learned how to preprocess text data and designed a FastText
    model for sentiment analysis. You also trained the model, evaluated its performance,
    and saved the model for deployment. You can use this design pattern and reference
    code to solve other sentiment analysis problems in your own work.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: This example was based on the “Faster Sentiment Analysis” tutorial by Ben Trevett.
    You can find more details and other great Torchtext tutorials in his [PyTorch
    Sentiment Analysis GitHub repository](https://pytorch.tips/sentiment-tutorials).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to our final reference design, in which we will use deep learning
    and PyTorch to generate image data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Generative Learning—Generating Fashion-MNIST Images with DCGAN
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most interesting areas of deep learning is *generative learning*,
    in which NNs are used to create data. Sometimes these NNs can create images, music,
    text, and time series data so well that it is difficult to tell the difference
    between real data and the generated data. Generative learning is used to create
    images of people and places that don’t exist, increase image resolution, predict
    frames in video, augment datasets, generate news articles, and convert styles
    of art and music.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I’ll show you how to use PyTorch for generative learning. The
    development process is similar to the previous examples; however, here we’ll use
    an unsupervised approach in which the data is not labeled.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we’ll design and train a GAN, which is quite different from the
    models and training loops of previous examples. Testing and evaluating the GAN
    involves a slightly different process as well. The overall development sequence
    is consistent with the process in [Chapter 2](ch02.xhtml#Chapter_2), but each
    part will be unique to generative learning.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will train a GAN to generate images similar to the training
    images used in the Fashion-MNIST dataset. Fashion-MNIST is a popular academic
    dataset used for image classification that includes images of articles of clothing.
    Let’s access the Fashion-MNIST data to get an idea of what these images look like,
    and then we’ll create some synthetic images based on what we’ve seen.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Data Processing
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike models used for supervised learning, where the model learns the relationships
    between data and labels, generative models look to learn the distribution of the
    training data so as to generate data similar to the training data at hand. Therefore,
    in this example we only need training data, because if we build a good model and
    train it long enough, the model should begin to produce good synthetic data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与用于监督学习的模型不同，那里模型学习数据和标签之间的关系，生成模型旨在学习训练数据的分布，以便生成类似于手头训练数据的数据。因此，在这个例子中，我们只需要训练数据，因为如果我们构建一个好的模型并训练足够长的时间，模型应该开始产生良好的合成数据。
- en: 'First let’s import the required libraries, define some constants, and set our
    device:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们导入所需的库，定义一些常量，并设置我们的设备：
- en: '[PRE24]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following code loads the training data, defines the transforms, and creates
    a dataloader for batch iteration:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载训练数据，定义了转换操作，并创建了一个用于批量迭代的数据加载器：
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This code should look familiar to you. We are once again using Torchvision functions
    to define the transforms, create a dataset, and set up a dataloader that will
    sample the dataset, apply transforms, and return a batch of images for our model.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码应该对你来说很熟悉。我们再次使用Torchvision函数来定义转换、创建数据集，并设置一个数据加载器，该加载器将对数据集进行采样，应用转换，并为我们的模型返回一批图像。
- en: 'We can display a batch of images with the following code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码显示一批图像：
- en: '[PRE26]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Torchvision provides a nice utility called `make_grid` to display a grid of
    images. [Figure 4-2](#fig_fashion_mnist) shows an example batch of Fashion-MNIST
    images.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Torchvision提供了一个很好的实用工具叫做`make_grid`来显示一组图像。[图4-2](#fig_fashion_mnist)展示了一个Fashion-MNIST图像的示例批次。
- en: '![“FashionMNIST Images”](Images/ptpr_0402.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![“FashionMNIST Images”](Images/ptpr_0402.png)'
- en: Figure 4-2\. Fashion-MNIST images
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2. Fashion-MNIST图像
- en: Let’s see what model we’ll use for our data generation task.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们将用于数据生成任务的模型。
- en: Model Design
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型设计
- en: 'To generate new image data, we’ll use a GAN. The goal of the GAN model is to
    generate “fake” data based on the training data’s distribution. The GAN accomplishes
    this goal with two distinct modules: the generator and the discriminator.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成新的图像数据，我们将使用GAN。GAN模型的目标是基于训练数据的分布生成“假”数据。GAN通过两个不同的模块实现这一目标：生成器和鉴别器。
- en: The job of the generator is to generate fake images that look real. The job
    of the discriminator is to correctly identify whether an image is fake. Although
    the design of GANs is beyond the scope of this book, I’ll provide a sample reference
    design using a deep convolutional GAN, or DCGAN.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的工作是生成看起来真实的假图像。鉴别器的工作是正确识别图像是否为假的。尽管GAN的设计超出了本书的范围，但我将提供一个使用深度卷积GAN（DCGAN）的示例参考设计。
- en: Note
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: GANs were first described in the famous paper by Ian Goodfellow et al. in 2014
    titled [“Generative Adversarial Nets”](https://pytorch.tips/gan-paper). Guidelines
    for building more stable convolutional GANs were proposed by Alec Radford et al.
    in the 2015 paper [“Unsupervised Representation Learning with Deep Convolutional
    Generative Adversarial Networks”](https://pytorch.tips/dcgan-paper). This paper
    describes the DCGAN used in this example.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: GAN首次在Ian Goodfellow等人于2014年发表的著名论文中描述，标题为[“生成对抗网络”](https://pytorch.tips/gan-paper)。Alec
    Radford等人在2015年的论文中提出了构建更稳定的卷积GAN的指导方针，标题为[“使用深度卷积生成对抗网络进行无监督表示学习”](https://pytorch.tips/dcgan-paper)。本例中使用的DCGAN在这篇论文中有描述。
- en: 'The generator is designed to create an image from an input vector of 100 random
    values. Here’s the code:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器被设计为从一个包含100个随机值的输入向量创建图像。以下是代码：
- en: '[PRE27]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This example generator uses 2D convolutional transpose layers with batch normalization
    and ReLU activations. The layers are defined in the `__init__()` function. It
    works like our image classification models, except in reverse order.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例生成器使用2D卷积转置层与批量归一化和ReLU激活。这些层在`__init__()`函数中定义。它的工作方式类似于我们的图像分类模型，只是顺序相反。
- en: That is, instead of reducing an image to a smaller representation, it takes
    a random vector and creates a full image from it. We also instantiate the `Generator`
    module as `netG`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，它不是将图像缩小为较小的表示，而是从一个随机向量创建完整的图像。我们还将`Generator`模块实例化为`netG`。
- en: 'Next, we create the `Discriminator` module, as shown in the following code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建`Discriminator`模块，如下所示的代码：
- en: '[PRE28]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The discriminator is a binary classification network that determines the probability
    that the input image is real. This example discriminator NN uses 2D convolutional
    layers with batch normalization and leaky ReLU activation functions. We instantiate
    the `Discriminator` as `netD`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器是一个二元分类网络，确定输入图像是真实的概率。这个示例鉴别器NN使用2D卷积层与批量归一化和泄漏ReLU激活函数。我们将`Discriminator`实例化为`netD`。
- en: 'The authors of the DCGAN paper found that it helps to initialize the weights
    as shown in the following code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN论文的作者发现，初始化权重有助于提高性能，如下所示的代码：
- en: '[PRE29]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now that we have designed our two modules, we can set up and train the GAN.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设计好了两个模块，我们可以设置并训练GAN。
- en: Training
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: Training a GAN is somewhat more complicated than the previous training examples.
    In each epoch, we will first train the discriminator with a real batch of data,
    then use the generator to create a fake batch, and then train the discriminator
    with the generated fake batch of data. Lastly, we will train the generator NN
    to produce better fakes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GAN比之前的训练示例要复杂一些。在每个时代中，我们将首先用真实数据批次训练鉴别器，然后使用生成器创建一个假批次，然后用生成的假数据批次训练鉴别器。最后，我们将训练生成器NN以生成更好的假数据。
- en: This is a good example of how powerful PyTorch is when creating custom training
    loops. It provides the flexibility to develop and implement new ideas with ease.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的例子，展示了PyTorch在创建自定义训练循环时的强大功能。它提供了灵活性，可以轻松开发和实现新的想法。
- en: 'Before we start training, we need to define the loss function and optimizers
    that will be used to train the generator and the discriminator:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，我们需要定义用于训练生成器和鉴别器的损失函数和优化器：
- en: '[PRE30]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding code, we define a label for real versus fake images. Then we
    use the binary cross entropy (BCE) loss function, which is commonly used for binary
    classification. Remember the discriminator is performing binary classification
    by classifying an image as real or fake. We use the commonly used Adam optimizer
    for updating the model parameters.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们为真实与假图像定义了一个标签。然后我们使用二元交叉熵（BCE）损失函数，这是用于二元分类的常用函数。请记住，鉴别器通过将图像分类为真实或假来执行二元分类。我们使用常用的
    Adam 优化器来更新模型参数。
- en: 'Let’s define values for the real and fake labels and create tensors for computing
    the loss:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为真实和假标签定义值，并创建用于计算损失的张量：
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Before we start training, we will create lists for storing the errors and define
    a test vector to show the results later:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，我们将创建用于存储错误的列表，并定义一个测试向量以后显示结果：
- en: '[PRE32]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we can execute the training loop. If the GAN is stable, it should improve
    as more epochs are trained. The training loop is shown in the following code:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以执行训练循环。如果 GAN 是稳定的，随着更多时代的训练，它应该会改进。以下是训练循环的代码：
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[![1](Images/1.png)](#co_neural_network_development_reference_designs_CO8-1)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](Images/1.png)'
- en: Pass real images to the `Discriminator`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 将真实图像传递给“鉴别器”。
- en: '[![2](Images/2.png)](#co_neural_network_development_reference_designs_CO8-2)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](Images/2.png)'
- en: Pass fake images to the `Discriminator`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 将假图像传递给“鉴别器”。
- en: '[![3](Images/3.png)](#co_neural_network_development_reference_designs_CO8-3)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](Images/3.png)'
- en: Run backpropagation and update the `Discriminator`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 运行反向传播并更新“鉴别器”。
- en: '[![4](Images/4.png)](#co_neural_network_development_reference_designs_CO8-4)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](Images/4.png)'
- en: Pass fake images to the updated `Discriminator`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 将假图像传递给更新后的“鉴别器”。
- en: '[![5](Images/5.png)](#co_neural_network_development_reference_designs_CO8-5)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](Images/5.png)'
- en: The `Generator` loss is based on cases in which the[.keep-together] `Discriminator`
    is wrong.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: “生成器”的损失基于“鉴别器”错误的情况。
- en: '[![6](Images/6.png)](#co_neural_network_development_reference_designs_CO8-6)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](Images/6.png)'
- en: Run backpropagation and update the `Generator`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 运行反向传播并更新“生成器”。
- en: '[![7](Images/7.png)](#co_neural_network_development_reference_designs_CO8-7)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![7](Images/7.png)'
- en: Create a batch of images and save them after each epoch.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一批图像并在每个时代后保存它们。
- en: As we’ve done in the previous examples, we loop through all the data, one batch
    at a time, using the dataloader during each epoch. First we train the discriminator
    with a batch of real images so it can compute the output, calculate the loss,
    and compute the gradients. Then we train the discriminator with a batch of fake
    images.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的示例一样，我们循环遍历所有数据，每次一个批次，在每个时代使用数据加载器。首先，我们用一批真实图像训练鉴别器，以便它可以计算输出，计算损失并计算梯度。然后我们用一批假图像训练鉴别器。
- en: The fake images are created by the generator from a vector of random values.
    Again, we compute the discriminator output, calculate the loss, and compute the
    gradients. Next, we add the gradients from all the real and all the fake batches
    and apply backpropagation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 假图像是由生成器从随机值向量创建的。再次，我们计算鉴别器输出，计算损失并计算梯度。接下来，我们添加所有真实和所有假批次的梯度，并应用反向传播。
- en: We compute the outputs from the freshly trained discriminator using the same
    fake data, and compute the loss or error of the generator. Using this loss, we
    compute the gradients and apply backpropagation on the generator itself.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用刚训练的鉴别器从相同的假数据计算输出，并计算生成器的损失或错误。利用这个损失，我们计算梯度并在生成器本身上应用反向传播。
- en: Lastly, we’ll keep track of the loss after each epoch to see if the GAN’s training
    is consistently improving and stable. [Figure 4-3](#fig_gan_curves) shows the
    loss curve for both the generator and the discriminator during training.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将跟踪每个时代后的损失，以查看 GAN 的训练是否持续改进和稳定。[图 4-3](#fig_gan_curves) 显示了生成器和鉴别器在训练过程中的损失曲线。
- en: '![“GAN Training Curves”](Images/ptpr_0403.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![“GAN 训练曲线”](Images/ptpr_0403.png)'
- en: Figure 4-3\. GAN training curves
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3. GAN 训练曲线
- en: The loss curves plot the generator and the discriminator loss for each batch
    over all epochs, so the loss bounces around depending on the computed loss of
    the batch. We can see though that the loss in both cases has been reduced from
    the beginning of training. If we trained over more epochs, we’d look for these
    loss values to approach zero.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 损失曲线绘制了每个批次在所有时代中的生成器和鉴别器损失，因此损失会根据批次的计算损失而波动。不过我们可以看到，两种情况下的损失都从训练开始时减少了。如果我们训练更多时代，我们会期待这些损失值接近零。
- en: In general, GANs are tricky to train, and the learning rate, betas, and other
    optimizer hyperparameters can have a major impact.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，GAN 很难训练，学习率、beta 和其他优化器超参数可能会产生重大影响。
- en: Let’s examine the average results of the discriminator for each batch over all
    the epochs, as shown in [Figure 4-4](#fig_d_results).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查鉴别器在每个批次上所有时代的平均结果，如[图 4-4](#fig_d_results)所示。
- en: '![“Discriminator Results”](Images/ptpr_0404.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![“鉴别器结果”](Images/ptpr_0404.png)'
- en: Figure 4-4\. Discriminator results
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4. 鉴别器结果
- en: If the GAN was perfect, the discriminator would not be able to correctly identify
    fake images as fake or real images as real, and we would expect the average error
    to be 0.5 in both cases. The results show that some batches are close to 0.5,
    but we can certainly do better.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 GAN 完美的话，鉴别器将无法正确识别假图像为假或真实图像为真，我们期望在这两种情况下平均误差为0.5。结果显示有些批次接近0.5，但我们肯定可以做得更好。
- en: Now that we have trained our network, let’s see how well it does at creating
    fake images of clothing.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了我们的网络，让我们看看它在创建服装的假图像方面表现如何。
- en: Testing and Deployment
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试和部署
- en: During supervised learning, we usually set aside a test dataset that has not
    been used to train or validate the model. In generative learning, there are no
    labels produced by the generator. We could pass our generated images into a Fashion-MNIST
    classifier, but there’s no way for us to know if the errors are caused by the
    classifier or the GAN unless we hand-label the outputs.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们通常留出一个未用于训练或验证模型的测试数据集。在生成式学习中，生成器没有生成标签。我们可以将生成的图像传递给Fashion-MNIST分类器，但除非我们手动标记输出，否则我们无法知道错误是由分类器还是GAN引起的。
- en: For now, let’s test and evaluate our GAN by comparing the results from the first
    epoch with the generated images from the last epoch. We create a test vector,
    `z`, for testing and use the computed generator results at the end of each epoch
    in our training loop code.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过比较第一个时代的结果和最后一个时代生成的图像来测试和评估我们的GAN。我们为测试创建一个名为`z`的测试向量，并在我们的训练循环代码中使用每个时代末尾计算的生成器结果。
- en: '[Figure 4-5](#fig_fmnist_0) shows the generated images from the first epoch,
    while [Figure 4-6](#fig_fmnist_4) shows the results after training only five epochs.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-5](#fig_fmnist_0)显示了第一个时代生成的图像，而[图4-6](#fig_fmnist_4)显示了仅训练五个时代后的结果。'
- en: '![“Generator Results (First Epoch)”](Images/ptpr_0405.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![“生成器结果（第一个时代）”](Images/ptpr_0405.png)'
- en: Figure 4-5\. Generator results (first epoch)
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5. 生成器结果（第一个时代）
- en: '![“Generator Results (Last Epoch)”](Images/ptpr_0406.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![“生成器结果（最后一个时代）”](Images/ptpr_0406.png)'
- en: Figure 4-6\. Generator results (last epoch)
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6. 生成器结果（最后一个时代）
- en: You can see that the generator has improved some. Look at the boot at the end
    of the second row or the shirt at the end of the third row. Our GAN is not perfect,
    but it seems to be improving after just five epochs. Training over more epochs
    or improving our design might produce even better results.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到生成器有所改进。看看第二行末尾的靴子或第三行末尾的衬衫。我们的GAN并不完美，但在只经过五个时代后似乎有所改善。训练更多时代或改进我们的设计可能会产生更好的结果。
- en: 'Finally, we can save our trained model for deployment and use it to generate
    more synthetic Fashion-MNIST images using the following code:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以保存我们训练好的模型以供部署，并使用以下代码生成更多合成的Fashion-MNIST图像：
- en: '[PRE34]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We expanded our PyTorch deep learning capabilities by designing and training
    a GAN in this generative learning reference design. You can use this reference
    design to create and train other GAN models and test their performance at generating
    new data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过设计和训练一个GAN来扩展了我们的PyTorch深度学习能力，在这个生成式学习参考设计中。您可以使用这个参考设计来创建和训练其他GAN模型，并测试它们生成新数据的性能。
- en: In this chapter, we covered additional examples to show a variety of data processing,
    model design, and training approaches with PyTorch—but what if you have an amazing
    idea for some new, innovative NN? Or what if you come up with a new optimization
    algorithm or loss function that nobody’s seen before? In the next chapter, I’ll
    show you how to create your own custom modules and functions so you can expand
    your deep learning research and experiment with new ideas.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了更多示例，展示了使用PyTorch的各种数据处理、模型设计和训练方法，但是如果您有一个新颖的、创新的NN的惊人想法呢？或者如果您想出了一个新的优化算法或损失函数，以前没有人见过的呢？在下一章中，我将向您展示如何创建自己的自定义模块和函数，以便扩展您的深度学习研究并尝试新的想法。
