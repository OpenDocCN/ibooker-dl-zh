["```py\n# models.py\n\nimport torch\nfrom transformers import Pipeline, pipeline\n\nprompt = \"How to set up a FastAPI project?\"\nsystem_prompt = \"\"\"\nYour name is FastAPI bot and you are a helpful\nchatbot responsible for teaching FastAPI to your users.\nAlways respond in markdown.\n\"\"\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ![1](assets/1.png)\n\ndef load_text_model():\n    pipe = pipeline(\n        \"text-generation\",\n        model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", ![2](assets/2.png)\n        torch_dtype=torch.bfloat16,\n        device=device ![3](assets/3.png)\n    )\n    return pipe\n\ndef generate_text(pipe: Pipeline, prompt: str, temperature: float = 0.7) -> str:\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": prompt},\n    ] ![4](assets/4.png)\n    prompt = pipe.tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    ) ![5](assets/5.png)\n    predictions = pipe(\n        prompt,\n        temperature=temperature,\n        max_new_tokens=256,\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n    ) ![6](assets/6.png)\n    output = predictions[0][\"generated_text\"].split(\"</s>\\n<|assistant|>\\n\")[-1] ![7](assets/7.png)\n    return output\n```", "```py\n# main.py\n\nfrom fastapi import FastAPI\nfrom models import load_text_model, generate_text\n\napp = FastAPI()\n\n@app.get(\"/generate/text\") ![1](assets/1.png)\ndef serve_language_model_controller(prompt: str) -> str: ![2](assets/2.png)\n    pipe = load_text_model() ![3](assets/3.png)\n    output = generate_text(pipe, prompt) ![4](assets/4.png)\n    return output ![5](assets/5.png)\n```", "```py\nhttp://localhost:8000/generate/text?prompt=\"What is FastAPI?\"\n```", "```py\n$ pip install streamlit\n```", "```py\n# client.py\n\nimport requests\nimport streamlit as st\n\nst.title(\"FastAPI ChatBot\") ![1](assets/1.png)\n\nif \"messages\" not in st.session_state:\n    st.session_state.messages = [] ![2](assets/2.png)\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"]) ![3](assets/3.png)\n\nif prompt := st.chat_input(\"Write your prompt in this input field\"): ![4](assets/4.png)\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt}) ![5](assets/5.png)\n\n    with st.chat_message(\"user\"):\n        st.text(prompt) ![6](assets/6.png)\n\n    response = requests.get(\n        f\"http://localhost:8000/generate/text\", params={\"prompt\": prompt}\n    ) ![7](assets/7.png)\n    response.raise_for_status() ![8](assets/8.png)\n\n    with st.chat_message(\"assistant\"):\n        st.markdown(response.text) ![9](assets/9.png)\n```", "```py\n$ streamlit run client.py\n```", "```py\n# schemas.py\n\nfrom typing import Literal\n\nVoicePresets = Literal[\"v2/en_speaker_1\", \"v2/en_speaker_9\"] ![1](assets/1.png)\n\n# models.py\nimport torch\nimport numpy as np\nfrom transformers import AutoProcessor, AutoModel, BarkProcessor, BarkModel\nfrom schemas import VoicePresets\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_audio_model() -> tuple[BarkProcessor, BarkModel]:\n    processor = AutoProcessor.from_pretrained(\"suno/bark-small\", device=device) ![2](assets/2.png)\n    model = AutoModel.from_pretrained(\"suno/bark-small\", device=device) ![3](assets/3.png)\n    return processor, model\n\ndef generate_audio(\n    processor: BarkProcessor,\n    model: BarkModel,\n    prompt: str,\n    preset: VoicePresets,\n) -> tuple[np.array, int]:\n    inputs = processor(text=[prompt], return_tensors=\"pt\",voice_preset=preset) ![4](assets/4.png)\n    output = model.generate(**inputs, do_sample=True).cpu().numpy().squeeze() ![5](assets/5.png)\n    sample_rate = model.generation_config.sample_rate ![6](assets/6.png)\n    return output, sample_rate\n```", "```py\n$ pip install soundfile\n```", "```py\n# utils.py\n\nfrom io import BytesIO\nimport soundfile\nimport numpy as np\n\ndef audio_array_to_buffer(audio_array: np.array, sample_rate: int) -> BytesIO:\n    buffer = BytesIO()\n    soundfile.write(buffer, audio_array, sample_rate, format=\"wav\") ![1](assets/1.png)\n    buffer.seek(0)\n    return buffer ![2](assets/2.png)\n\n# main.py\n\nfrom fastapi import FastAPI, status\nfrom fastapi.responses import StreamingResponse\n\nfrom models import load_audio_model, generate_audio\nfrom schemas import VoicePresets\nfrom utils import audio_array_to_buffer\n\n@app.get(\n    \"/generate/audio\",\n    responses={status.HTTP_200_OK: {\"content\": {\"audio/wav\": {}}}},\n    response_class=StreamingResponse,\n) ![3](assets/3.png)\ndef serve_text_to_audio_model_controller(\n    prompt: str,\n    preset: VoicePresets = \"v2/en_speaker_1\",\n):\n    processor, model = load_audio_model()\n    output, sample_rate = generate_audio(processor, model, prompt, preset)\n    return StreamingResponse(\n        audio_array_to_buffer(output, sample_rate), media_type=\"audio/wav\"\n    ) ![4](assets/4.png)\n```", "```py\n# client.py\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        content = message[\"content\"]\n        if isinstance(content, bytes):\n            st.audio(content)\n        else:\n            st.markdown(content)\n\nif prompt := st.chat_input(\"Write your prompt in this input field\"):\n    response = requests.get(\n        f\"http://localhost:8000/generate/audio\", params={\"prompt\": prompt}\n    )\n    response.raise_for_status()\n    with st.chat_message(\"assistant\"):\n        st.text(\"Here is your generated audio\")\n        st.audio(response.content) ![1](assets/1.png)\n```", "```py\n$ pip install diffusers\n```", "```py\n# models.py\n\nimport torch\nfrom diffusers import DiffusionPipeline, StableDiffusionInpaintPipelineLegacy\nfrom PIL import Image\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_image_model() -> StableDiffusionInpaintPipelineLegacy:\n    pipe = DiffusionPipeline.from_pretrained(\n        \"segmind/tiny-sd\", torch_dtype=torch.float32,\n        device=device\n    ) ![1](assets/1.png)\n    return pipe\n\ndef generate_image(\n    pipe: StableDiffusionInpaintPipelineLegacy, prompt: str\n) -> Image.Image:\n    output = pipe(prompt, num_inference_steps=10).images[0] ![2](assets/2.png) ![3](assets/3.png)\n    return output ![4](assets/4.png)\n```", "```py\n# utils.py\n\nfrom typing import Literal\nfrom PIL import Image\nfrom io import BytesIO\n\ndef img_to_bytes(\n    image: Image.Image, img_format: Literal[\"PNG\", \"JPEG\"] = \"PNG\"\n) -> bytes:\n    buffer = BytesIO()\n    image.save(buffer, format=img_format)\n    return buffer.getvalue() ![1](assets/1.png)\n\n# main.py\n\nfrom fastapi import FastAPI, Response, status\nfrom models import load_image_model, generate_image\nfrom utils import img_to_bytes\n\n...\n\n@app.get(\"/generate/image\",\n         responses={status.HTTP_200_OK: {\"content\": {\"image/png\": {}}}}, ![2](assets/2.png)\n         response_class=Response) ![3](assets/3.png)\ndef serve_text_to_image_model_controller(prompt: str):\n    pipe = load_image_model()\n    output = generate_image(pipe, prompt) ![4](assets/4.png)\n    return Response(content=img_to_bytes(output), media_type=\"image/png\") ![5](assets/5.png)\n```", "```py\n# client.py\n\n...\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.image(message[\"content\"]) ![1](assets/1.png)\n...\n\nif prompt := st.chat_input(\"Write your prompt in this input field\"):\n    ...\n    response = requests.get(\n        f\"http://localhost:8000/generate/image\", params={\"prompt\": prompt}\n    ) ![2](assets/2.png)\n    response.raise_for_status()\n    with st.chat_message(\"assistant\"):\n        st.text(\"Here is your generated image\")\n        st.image(response.content)\n\n    ...\n```", "```py\n# models.py\n\nimport torch\nfrom diffusers import StableVideoDiffusionPipeline\nfrom PIL import Image\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_video_model() -> StableVideoDiffusionPipeline:\n    pipe = StableVideoDiffusionPipeline.from_pretrained(\n        \"stabilityai/stable-video-diffusion-img2vid\",\n        torch_dtype=torch.float16,\n        variant=\"fp16\",\n        device=device,\n    )\n    return pipe\n\ndef generate_video(\n    pipe: StableVideoDiffusionPipeline, image: Image.Image, num_frames: int = 25\n) -> list[Image.Image]:\n    image = image.resize((1024, 576)) ![1](assets/1.png)\n    generator = torch.manual_seed(42) ![2](assets/2.png)\n    frames = pipe(\n        image, decode_chunk_size=8, generator=generator, num_frames=num_frames\n    ).frames[0] ![3](assets/3.png)\n    return frames\n```", "```py\n$ pip install av\n```", "```py\n# utils.py\n\nfrom io import BytesIO\nfrom PIL import Image\nimport av\n\ndef export_to_video_buffer(images: list[Image.Image]) -> BytesIO:\n    buffer = BytesIO()\n    output = av.open(buffer, \"w\", format=\"mp4\") ![1](assets/1.png)\n    stream = output.add_stream(\"h264\", 30) ![2](assets/2.png)\n    stream.width = images[0].width\n    stream.height = images[0].height\n    stream.pix_fmt = \"yuv444p\" ![3](assets/3.png)\n    stream.options = {\"crf\": \"17\"} ![4](assets/4.png)\n    for image in images:\n        frame = av.VideoFrame.from_image(image)\n        packet = stream.encode(frame)   ![5](assets/5.png)\n        output.mux(packet) ![6](assets/6.png)\n    packet = stream.encode(None)\n    output.mux(packet)\n    return buffer ![7](assets/7.png)\n```", "```py\n$ pip install python-multipart\n```", "```py\n# main.py\n\nfrom fastapi import status, FastAPI, File\nfrom fastapi.responses import StreamingResponse\nfrom io import BytesIO\nfrom PIL import Image\n\nfrom models import load_video_model, generate_video\nfrom utils import export_to_video_buffer\n\n...\n\n@app.post(\n    \"/generate/video\",\n    responses={status.HTTP_200_OK: {\"content\": {\"video/mp4\": {}}}},\n    response_class=StreamingResponse,\n)\ndef serve_image_to_video_model_controller(\n    image: bytes = File(...), num_frames: int = 25 ![1](assets/1.png)\n):\n    image = Image.open(BytesIO(image)) ![2](assets/2.png)\n    model = load_video_model()\n    frames = generate_video(model, image, num_frames)\n    return StreamingResponse(\n        export_to_video_buffer(frames), media_type=\"video/mp4\" ![3](assets/3.png)\n    )\n```", "```py\n# models.py\nimport torch\nfrom diffusers import ShapEPipeline\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_3d_model() -> ShapEPipeline:\n    pipe = ShapEPipeline.from_pretrained(\"openai/shap-e\", device=device)\n    return pipe\n\ndef generate_3d_geometry(\n    pipe: ShapEPipeline, prompt: str, num_inference_steps: int\n):\n    images = pipe(\n        prompt, ![1](assets/1.png)\n        guidance_scale=15.0, ![2](assets/2.png)\n        num_inference_steps=num_inference_steps, ![3](assets/3.png)\n        output_type=\"mesh\", ![4](assets/4.png)\n    ).images[0] ![5](assets/5.png)\n    return images\n```", "```py\n$ pip install open3d\n```", "```py\n# utils.py\n\nimport os\nimport tempfile\nfrom io import BytesIO\nfrom pathlib import Path\nimport open3d as o3d\nimport torch\nfrom diffusers.pipelines.shap_e.renderer import MeshDecoderOutput\n\ndef mesh_to_obj_buffer(mesh: MeshDecoderOutput) -> BytesIO:\n    mesh_o3d = o3d.geometry.TriangleMesh() ![1](assets/1.png)\n    mesh_o3d.vertices = o3d.utility.Vector3dVector(\n        mesh.verts.cpu().detach().numpy() ![2](assets/2.png)\n    )\n    mesh_o3d.triangles = o3d.utility.Vector3iVector(\n        mesh.faces.cpu().detach().numpy() ![2](assets/2.png)\n    )\n\n    if len(mesh.vertex_channels) == 3:  # You have color channels\n        vert_color = torch.stack(\n            [mesh.vertex_channels[channel] for channel in \"RGB\"], dim=1\n        ) ![3](assets/3.png)\n        mesh_o3d.vertex_colors = o3d.utility.Vector3dVector(\n            vert_color.cpu().detach().numpy()\n        ) ![4](assets/4.png)\n\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".obj\") as tmp:\n        o3d.io.write_triangle_mesh(tmp.name, mesh_o3d, write_ascii=True)\n        with open(tmp.name, \"rb\") as f:\n            buffer = BytesIO(f.read()) ![5](assets/5.png)\n        os.remove(tmp.name) ![6](assets/6.png)\n\n    return buffer\n```", "```py\n# main.py\n\nfrom fastapi import FastAPI, status\nfrom fastapi.responses import StreamingResponse\nfrom models import load_3d_model, generate_3d_geometry\nfrom utils import mesh_to_obj_buffer\n\n...\n\n@app.get(\n    \"/generate/3d\",\n    responses={status.HTTP_200_OK: {\"content\": {\"model/obj\": {}}}}, ![1](assets/1.png)\n    response_class=StreamingResponse,\n)\ndef serve_text_to_3d_model_controller(\n    prompt: str, num_inference_steps: int = 25\n):\n    model = load_3d_model()\n    mesh = generate_3d_geometry(model, prompt, num_inference_steps)\n    response = StreamingResponse(\n        mesh_to_obj_buffer(mesh), media_type=\"model/obj\"\n    )\n    response.headers[\"Content-Disposition\"] = (\n        f\"attachment; filename={prompt}.obj\"\n    ) ![2](assets/2.png)\n    return response\n```", "```py\n# main.py\n\nfrom contextlib import asynccontextmanager\nfrom typing import AsyncIterator\nfrom fastapi import FastAPI, Response, status\nfrom models import load_image_model, generate_image\nfrom utils import img_to_bytes\n\nmodels = {} ![1](assets/1.png)\n\n@asynccontextmanager ![2](assets/2.png)\nasync def lifespan(_: FastAPI) -> AsyncIterator[None]:\n    models[\"text2image\"] = load_image_model() ![3](assets/3.png)\n\n    yield ![4](assets/4.png)\n\n    ... # Run cleanup code here\n\n    models.clear() ![5](assets/5.png)\n\napp = FastAPI(lifespan=lifespan) ![6](assets/6.png)\n\n@app.get(\n    \"/generate/image\",\n    responses={status.HTTP_200_OK: {\"content\": {\"image/png\": {}}}},\n    response_class=Response,\n)\ndef serve_text_to_image_model_controller(prompt: str):\n    output = generate_image(models[\"text2image\"], prompt) ![7](assets/7.png)\n    return Response(content=img_to_bytes(output), media_type=\"image/png\")\n```", "```py\n$ pip install bentoml\n```", "```py\n# bento.py\nimport bentoml\nfrom models import load_image_model\n\n@bentoml.service(\n    resources={\"cpu\": \"4\"}, traffic={\"timeout\": 120}, http={\"port\": 5000}\n) ![1](assets/1.png)\nclass Generate:\n    def __init__(self) -> None:\n        self.pipe = load_image_model()\n\n    @bentoml.api(route=\"/generate/image\") ![2](assets/2.png)\n    def generate(self, prompt: str) -> str:\n        output = self.pipe(prompt, num_inference_steps=10).images[0]\n        return output\n```", "```py\n$ bentoml serve service:Generate\n```", "```py\n# main.py\n\nimport httpx\nfrom fastapi import FastAPI, Response\n\napp = FastAPI()\n\n@app.get(\n    \"/generate/bentoml/image\",\n    responses={status.HTTP_200_OK: {\"content\": {\"image/png\": {}}}},\n    response_class=Response,\n)\nasync def serve_bentoml_text_to_image_controller(prompt: str):\n    async with httpx.AsyncClient() as client: ![1](assets/1.png)\n        response = await client.post(\n            \"http://localhost:5000/generate\", json={\"prompt\": prompt}\n        ) ![2](assets/2.png)\n    return Response(content=response.content, media_type=\"image/png\")\n```", "```py\n# main.py\n\nfrom fastapi import FastAPI\nfrom openai import OpenAI\n\napp = FastAPI()\nopenai_client = OpenAI()\nsystem_prompt = \"You are a helpful assistant.\"\n\n@app.get(\"/generate/openai/text\")\ndef serve_openai_language_model_controller(prompt: str) -> str | None:\n    response = openai_client.chat.completions.create( ![1](assets/1.png)\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": f\"{system_prompt}\"},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n    return response.choices[0].message.content\n```", "```py\n# main.py\n\nimport csv\nimport time\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom typing import Awaitable, Callable\nfrom fastapi import FastAPI, Request, Response\n\n# preload model with a lifespan\n...\n\napp = FastAPI(lifespan=lifespan)\n\ncsv_header = [\n    \"Request ID\", \"Datetime\", \"Endpoint Triggered\", \"Client IP Address\",\n    \"Response Time\", \"Status Code\", \"Successful\"\n]\n\n@app.middleware(\"http\") ![1](assets/1.png)\nasync def monitor_service(\n    req: Request, call_next: Callable[[Request], Awaitable[Response]]\n) -> Response: ![2](assets/2.png)\n    request_id = uuid4().hex ![3](assets/3.png)\n    request_datetime = datetime.now(timezone.utc).isoformat()\n    start_time = time.perf_counter()\n    response: Response = await call_next(req)\n    response_time = round(time.perf_counter() - start_time, 4) ![4](assets/4.png)\n    response.headers[\"X-Response-Time\"] = str(response_time)\n    response.headers[\"X-API-Request-ID\"] = request_id ![5](assets/5.png)\n    with open(\"usage.csv\", \"a\", newline=\"\") as file:\n        writer = csv.writer(file)\n        if file.tell() == 0:\n            writer.writerow(csv_header)\n        writer.writerow( ![6](assets/6.png)\n            [\n                request_id,\n                request_datetime,\n                req.url,\n                req.client.host,\n                response_time,\n                response.status_code,\n                response.status_code < 400,\n            ]\n        )\n    return response\n\n# Usage Log Example\n\n\"\"\"\"\nRequest ID: 3d15d3d9b7124cc9be7eb690fc4c9bd5\nDatetime: 2024-03-07T16:41:58.895091\nEndpoint triggered: http://localhost:8000/generate/text\nClient IP Address: 127.0.0.1\nProcessing time: 26.7210 seconds\nStatus Code: 200\nSuccessful: True\n\"\"\"\n\n# model-serving handlers\n...\n```"]