["```py`##### Esempio 3-1\\. Scaricare e caricare un modello linguistico dal repository di Hugging Face    ```", "```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO1-1)      Controlla se è disponibile una GPU NVIDIA e, in caso affermativo, imposta `device` sulla GPU corrente abilitata a CUDA. Altrimenti, continua a usare la CPU.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO1-2)      Scarica e carica in memoria il modello TinyLlama con un tipo di dati di precisione tensoriale `float16`.^([9](ch03.html#id667))      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO1-3)      Sposta l'intera pipeline sulla GPU al primo caricamento.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO1-4)      Prepara l'elenco dei messaggi, che consiste in dizionari con coppie chiave-valore di ruolo e contenuto. L'ordine dei dizionari detta l'ordine dei messaggi da quelli più vecchi a quelli più recenti in una conversazione. Il primo messaggio è spesso un prompt del sistema per guidare l'output del modello in una conversazione.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO1-5)      Convertire l'elenco dei messaggi di chat in un elenco di token interi per il modello. Al modello viene quindi chiesto di generare un output in formato testuale, non in token interi `tokenize=False`. Viene anche aggiunto un prompt di generazione alla fine dei messaggi di chat (`add_generation_prompt=True`) in modo che il modello sia incoraggiato a generare una risposta basata sulla cronologia della chat.      [![6](assets/6.png)](#co_ai_integration_and_model_serving_CO1-6)      Il prompt preparato viene passato al modello con diversi parametri di inferenza per ottimizzare le prestazioni di generazione del testo. Alcuni di questi parametri di inferenza chiave includono:    *   `max_new_tokens`: Specifica il numero massimo di nuovi token da generare nell'output.           *   `do_sample`: Determina, quando produce l'output, se scegliere un token in modo casuale da un elenco di token adatti (`True`) o se scegliere semplicemente il token più probabile a ogni passo (`False`).           *   `temperature`: I valori più bassi rendono i risultati del modello più precisi, mentre quelli più alti consentono risposte più creative.           *   `top_k`: Limita le previsioni dei token del modello alle prime K opzioni.`top_k=50` significa creare un elenco dei 50 token più adatti da scegliere nella fase di previsione dei token corrente.           *   `top_p`: Implementa il *campionamento dei nuclei* quando si crea un elenco dei token più adatti.`top_p=0.95` significa creare un elenco dei token migliori fino a quando non si è soddisfatti che l'elenco contenga il 95% dei token più adatti da cui scegliere, per la fase di previsione dei token corrente.                [![7](assets/7.png)](#co_ai_integration_and_model_serving_CO1-7)      L'output finale è ottenuto dall'oggetto `predictions`. Il testo generato da TinyLlama include l'intera cronologia della conversazione, con la risposta generata aggiunta alla fine. Il token di stop `</s>` seguito dai token `\\n<|assistant|>\\n` sono utilizzati per selezionare il contenuto dell'ultimo messaggio della conversazione, che è la risposta del modello.      L['esempio 3-1](#language_model_usage_example) è un buon punto di partenza; puoi caricare questo modello sulla tua CPU e ottenere risposte in tempi ragionevoli. Tuttavia, TinyLlama potrebbe non avere le stesse prestazioni delle sue controparti più grandi. Per i carichi di lavoro di produzione, vorrai utilizzare modelli più grandi per ottenere una migliore qualità e prestazioni.    A questo punto puoi utilizzare le funzioni `load_model` e `predict` all'interno di una funzione del controller^([10](ch03.html#id668)) e poi aggiungere un decoratore di gestione delle rotte per servire il modello tramite un endpoint, come mostrato nell'[Esempio 3-2](#text_endpoint).    ##### Esempio 3-2\\. Servire un modello linguistico tramite un endpoint FastAPI    ```", "```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO2-1)      Crea un server FastAPI e aggiungi un gestore di rotte `/generate` per servire il modello.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO2-2)      Il sito `serve_language_model_controller` è responsabile di prendere il prompt dai parametri della query di richiesta.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO2-3)      Il modello viene caricato in memoria.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO2-4)      Il controllore passa la query al modello per eseguire la previsione.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO2-5)      Il server FastAPI invia l'output come risposta HTTP al client.      Una volta che il servizio FastAPI è attivo e funzionante, puoi visitare la pagina di documentazione Swagger all'indirizzo `http://localhost:8000/docs`per testare il tuo nuovo endpoint:    ```", "```py    Se stai eseguendo gli esempi di codice su una CPU, ci vorrà circa un minuto per ricevere una risposta dal modello, come mostrato nella [Figura 3-11](#text_gen_response).  ![bgai 0311](assets/bgai_0311.png)  ###### Figura 3-11\\. Risposta di TinyLlama    Non è una cattiva risposta per un piccolo modello di linguaggio (SLM) che gira su una CPU del tuo computer, se non fosse che TinyLlama ha avuto *l'allucinazione* di credere che FastAPI utilizzi Flask. Si tratta di un'affermazione errata: FastAPI utilizza Starlette come framework web sottostante, non Flask.    Le*allucinazioni* si riferiscono a risultati che non sono basati sui dati di addestramento o sulla realtà. Anche se SLM open source come TinyLlama sono stati addestrati su un numero impressionante di token (3 trilioni), un numero ridotto di parametri del modello può aver limitato la loro capacità di apprendere la verità di base nei dati.Inoltre, potrebbero essere stati utilizzati anche dati di addestramento non filtrati, che possono contribuire ad aumentare i casi di allucinazioni.    ###### Avvertenze    Quando utilizzi i modelli linguistici, informa sempre i tuoi utenti di controllare i risultati con fonti esterne, perché i modelli linguistici potrebbero avere delle *allucinazioni* e produrre affermazioni errate.    Ora puoi utilizzare un client per browser web in Python per testare visivamente il tuo servizio con maggiore interattività rispetto all'utilizzo di un client a riga di comando.    Un ottimo pacchetto Python per sviluppare rapidamente un'interfaccia utente è [Streamlit](https://oreil.ly/9BXmn), che ti permette di creare UI belle e personalizzabili per i tuoi servizi di AI con poco sforzo.```", "```py`` ### Connettere FastAPI con il generatore di UI Streamlit    Streamlit ti permette di creare facilmente un'interfaccia utente di chat per il test e la prototipazione di modelli. Puoi installare il pacchetto `streamlit` utilizzando `pip`:    ```", "```py   ```", "```py # client.py  import requests import streamlit as st  st.title(\"FastAPI ChatBot\") ![1](assets/1.png)  if \"messages\" not in st.session_state:     st.session_state.messages = [] ![2](assets/2.png)  for message in st.session_state.messages:     with st.chat_message(message[\"role\"]):         st.markdown(message[\"content\"]) ![3](assets/3.png)  if prompt := st.chat_input(\"Write your prompt in this input field\"): ![4](assets/4.png)     st.session_state.messages.append({\"role\": \"user\", \"content\": prompt}) ![5](assets/5.png)      with st.chat_message(\"user\"):         st.text(prompt) ![6](assets/6.png)      response = requests.get(         f\"http://localhost:8000/generate/text\", params={\"prompt\": prompt}     ) ![7](assets/7.png)     response.raise_for_status() ![8](assets/8.png)      with st.chat_message(\"assistant\"):         st.markdown(response.text) ![9](assets/9.png) ```", "```py $ streamlit run client.py ```", "```py` ```", "```py```", "````py` ````", "```` ```py````", "```py # schemas.py  from typing import Literal  VoicePresets = Literal[\"v2/en_speaker_1\", \"v2/en_speaker_9\"] ![1](assets/1.png)  # models.py import torch import numpy as np from transformers import AutoProcessor, AutoModel, BarkProcessor, BarkModel from schemas import VoicePresets  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  def load_audio_model() -> tuple[BarkProcessor, BarkModel]:     processor = AutoProcessor.from_pretrained(\"suno/bark-small\", device=device) ![2](assets/2.png)     model = AutoModel.from_pretrained(\"suno/bark-small\", device=device) ![3](assets/3.png)     return processor, model   def generate_audio(     processor: BarkProcessor,     model: BarkModel,     prompt: str,     preset: VoicePresets, ) -> tuple[np.array, int]:     inputs = processor(text=[prompt], return_tensors=\"pt\",voice_preset=preset) ![4](assets/4.png)     output = model.generate(**inputs, do_sample=True).cpu().numpy().squeeze() ![5](assets/5.png)     sample_rate = model.generation_config.sample_rate ![6](assets/6.png)     return output, sample_rate ```", "```py $ pip install soundfile ```", "```py`L['esempio 3-5](#audio_endpoint) mostra come puoi trasmettere il contenuto audio al client.    ##### Esempio 3-5\\. Endpoint FastAPI per la restituzione dell'audio generato    ```", "```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO5-1)      Installa la libreria `soundfile` per scrivere l'array audio nella memoria buffer utilizzando la sua frequenza di campionamento.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO5-2)      Riporta il cursore del buffer all'inizio del buffer e restituisce il buffer iterabile.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO5-3)      Crea un nuovo endpoint audio che restituisca il tipo di contenuto `audio/wav` come `StreamingResponse`.`StreamingResponse` è tipicamente utilizzato quando si desidera trasmettere i dati di risposta, ad esempio quando si restituiscono file di grandi dimensioni o quando si generano i dati di risposta. Ti permette di restituire una funzione di generazione che produce pezzi di dati da inviare al client.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO5-4)      Converte l'array audio generato in un buffer iterabile che può essere passato alla risposta dello streaming.      Nell'[Esempio 3-5](#audio_endpoint), hai generato un array audio utilizzando il modello small Bark e hai trasmesso in streaming il buffer di memoria del contenuto audio. Lo streaming è più efficiente per i file più grandi, in quanto il client può consumare il contenuto mentre viene servito. Negli esempi precedenti non abbiamo utilizzato le risposte in streaming, in quanto le immagini o il testo generati possono essere piuttosto piccoli rispetto ai contenuti audio o video.    ###### Suggerimento    Lo streaming di contenuti audio direttamente da un buffer di memoria è più veloce ed efficiente della scrittura dell'array audio in un file e dello streaming dei contenuti dal disco rigido.    Se hai bisogno di memoria per altre attività, puoi scrivere prima l'array audio su un file e poi fare lo streaming da esso utilizzando un generatore di lettura di file.    Ora che hai un endpoint per la generazione dell'audio, puoi aggiornare il codice del tuo client Streamlit UI per eseguire il rendering dei messaggi audio. Aggiorna il codice del tuo client Streamlit come mostrato nell'[Esempio 3-6](#barksmall_streamlit_ui).    ##### Esempio 3-6\\. UI audio Streamlit che utilizza l'endpoint di generazione FastAPI `/audio`    ```", "```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO6-1)      Aggiorna il codice del client Streamlit per renderizzare i contenuti audio.      Con Streamlit puoi scambiare i componenti per renderizzare qualsiasi tipo di contenuto, comprese immagini, audio e video.    Ora dovresti essere in grado di generare un audio vocale molto realistico nell'interfaccia utente Streamlit aggiornata, come mostrato nella [Figura 3-16](#streamlit_bark_ui).  ![bgai 0316](assets/bgai_0316.png)  ###### Figura 3-16\\. Rendering delle risposte audio nell'interfaccia utente Streamlit    Tieni presente che stai utilizzando la versione compressa del modello Bark, ma con la versione light puoi generare audio vocale e musicale in modo abbastanza veloce anche con una sola CPU, in cambio di una certa qualità nella generazione dell'audio.    Ora dovresti sentirti più a tuo agio nel servire contenuti più grandi ai tuoi utenti tramite le risposte in streaming e nel lavorare con i modelli audio.    Finora hai costruito servizi di conversazione e text-to-speech. Ora vediamo come interagire con un modello di visione per costruire un servizio di generazione di immagini.```", "```py```", "```py $ pip install diffusers ```", "```py`L['esempio 3-7](#sd_model_usage_example) mostra come caricare un modello SD in memoria.    ##### Esempio 3-7\\. Scaricare e caricare un modello SD dal repository di Hugging Face    ```", "```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO7-1)      Scarica e carica il modello TinySD in memoria con il tipo di tensore `float32`, meno efficiente dal punto di vista della memoria. L'uso di `float16`, che ha una precisione limitata per modelli grandi e complessi, porta all'instabilità numerica e alla perdita di precisione. Inoltre, il supporto hardware per `float16` è limitato, quindi cercare di eseguire un modello SD sulla tua CPU con il tipo di tensore `float16` potrebbe non essere possibile. Fonte: [Hugging Face](https://oreil.ly/rzw8P).      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO7-2)      Passa il prompt al modello per generare un elenco di immagini e scegliere la prima. Alcuni modelli ti permettono di generare più immagini in un unico passaggio di inferenza.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO7-3)      `num_inference_steps=10` specifica il numero di passi di diffusione da eseguire durante l'inferenza. In ogni passo di diffusione, viene prodotta un'immagine rumorosa più forte dai passi di diffusione precedenti. Il modello genera più immagini rumorose eseguendo più passi di diffusione. Con queste immagini, il modello può comprendere meglio i modelli di rumore presenti nei dati di input e imparare a rimuoverli in modo più efficace. Maggiore è il numero di passi di inferenza, migliori saranno i risultati ottenuti, ma al costo della potenza di calcolo necessaria e di tempi di elaborazione più lunghi.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO7-4)      L'immagine generata sarà un tipo di immagine di Python Pillow, quindi avrai accesso a una serie di metodi di immagine di Pillow per la post-elaborazione e l'archiviazione. Ad esempio, puoi chiamare il metodo `image.save()` per archiviare l'immagine nel tuo filesystem.      ###### Nota    I modelli di visione sono estremamente affamati di risorse. Per caricare e utilizzare un piccolo modello di visione come TinySD sulla CPU, avrai bisogno di circa5 GB di spazio su disco e di RAM. Tuttavia, puoi installare `accelerate` utilizzando `pip install accelerate` per ottimizzare le risorse richieste in modo che la pipeline del modello utilizzi meno memoria della CPU.    Per servire i modelli video, dovrai utilizzare una GPU. Più avanti in questo capitolo ti mostrerò come sfruttare le GPU per imodelli video.    Ora puoi impacchettare questo modello in un altro endpoint come nell'[Esempio 3-2](#text_endpoint), con la differenza che la risposta restituita sarà un'immagine binaria (non un testo). Fai riferimento all'[Esempio 3-8](#image_endpoint).    ##### Esempio 3-8\\. Endpoint FastAPI per la restituzione di un'immagine generata    ```", "```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO8-1)      Crea un buffer in memoria, salva l'immagine in questo buffer in un determinato formato e poi restituisce i dati grezzi in byte del buffer.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO8-2)      Specifica il tipo di contenuto multimediale e i codici di stato per la pagina di documentazione Swagger UI generata automaticamente.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO8-3)      Specifica la classe di risposta per evitare che FastAPI aggiunga `application/json`come ulteriore tipo di supporto di risposta accettabile.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO8-4)      La risposta restituita dal modello sarà in formato immagine Pillow.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO8-5)      Dovremo utilizzare la classe FastAPI `Response` per inviare una risposta speciale che trasporta byte di immagini con un tipo di supporto PNG.      [La Figura 3-18](#tinysd_swagger_docs) mostra i risultati del test del nuovo endpoint `/generate/image` tramite i documenti FastAPI Swagger con il prompt `A cosy living room with trees in it`.  ![bgai 0318](assets/bgai_0318.png)  ###### Figura 3-18\\. Servizio TinySD FastAPI    A questo punto, collega il tuo endpoint a un'interfaccia utente Streamlit per la prototipazione, come mostrato nell'[Esempio 3-9](#tinysd_streamlit_code).    ##### Esempio 3-9\\. Streamlit Vision UI che consuma l'endpoint di generazione FastAPI `*/image*` per la generazione di endpoint    ```", "```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO9-1)      Le immagini trasferite tramite il protocollo HTTP saranno in formato binario. Pertanto, aggiorniamo la funzione di visualizzazione per rendere il contenuto delle immagini binario. Puoi utilizzare il metodo `st.image` per visualizzare le immagini nell'interfaccia utente.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO9-2)      Aggiorna la richiesta `GET` in modo che raggiunga l'endpoint `/generate/image`. Quindi, visualizza un messaggio testuale e un'immagine per l'utente.      La[Figura 3-19](#tinysd_streamlitui) mostra i risultati finali dell'esperienza dell'utente con il modello.  ![bgai 0319](assets/bgai_0319.png)  ###### Figura 3-19\\. Rendering dei messaggi immagine nell'interfaccia utente di Streamlit    Abbiamo visto come anche con un modello SD di piccole dimensioni sia possibile generare immagini dall'aspetto ragionevole. Le versioni XL possono produrre immagini ancora più realistiche, ma hanno comunque i lorolimiti.    Al momento in cui scriviamo, gli attuali modelli di SD open source presentano alcunelimitazioni:    Coerenza      I modelli non possono produrre tutti i dettagli descritti nei prompt e le composizioni complesse.      Dimensioni dell'uscita      Le immagini in uscita possono avere solo dimensioni predefinite come 512 × 512 o 1024 × 1024 pixel.      Compostezza      Non puoi controllare completamente l'immagine generata e definire la composizione dell'immagine.      Fotorealismo      Gli output generati mostrano dettagli che fanno capire che sono stati generati dall'intelligenza artificiale.      Testo leggibile      Alcuni modelli non sono in grado di generare testi leggibili.      Il modello `tinysd` con cui hai lavorato è un modello in fase iniziale che ha subito il processo di *distillazione* (cioè di compressione) dal modello SD V1.5 più grande.Di conseguenza, i risultati generati potrebbero non soddisfare gli standard di produzione o non essere del tutto coesi e potrebbero non incorporare tutti i concetti menzionati nei prompt del testo. Tuttavia, i modelli distillati potrebbero funzionare bene se [li*metti a punto* usando il *Low-Rank Adaptation* (LoRA)](https://oreil.ly/Nqtkm) su concetti/stili specifici.    Ora puoi costruire servizi GenAI sia basati sul testo che sulle immagini. Tuttavia, ti starai chiedendo come costruire servizi da testo a video basati su modelli video. Scopriamo di più sui modelli video, come funzionano e come costruire un servizio di animazione di immagini con essi.```", "```py```", "```py # models.py  import torch from diffusers import StableVideoDiffusionPipeline from PIL import Image  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  def load_video_model() -> StableVideoDiffusionPipeline:     pipe = StableVideoDiffusionPipeline.from_pretrained(         \"stabilityai/stable-video-diffusion-img2vid\",         torch_dtype=torch.float16,         variant=\"fp16\",         device=device,     )     return pipe  def generate_video(     pipe: StableVideoDiffusionPipeline, image: Image.Image, num_frames: int = 25 ) -> list[Image.Image]:     image = image.resize((1024, 576)) ![1](assets/1.png)     generator = torch.manual_seed(42) ![2](assets/2.png)     frames = pipe(         image, decode_chunk_size=8, generator=generator, num_frames=num_frames     ).frames[0] ![3](assets/3.png)     return frames ```", "```py $ pip install av ```", "```py`` Ora puoi utilizzare l'[Esempio 3-11](#frames_to_videos) per creare buffer video in streaming.    ##### Esempio 3-11\\. Esportazione dell'output del modello video dai fotogrammi a un buffer video in streaming utilizzando la libreria `av`    ```", "```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO11-1)      Apri un buffer per scrivere un file MP4 e poi configura un flusso video con il multiplexer video di AV.^([13](ch03.html#id697))      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO11-2)      Imposta la codifica video su `h264` a 30 fotogrammi al secondo e assicurati che le dimensioni dei fotogrammi corrispondano a quelle fornite alla funzione.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO11-3)      Imposta il formato dei pixel del flusso video su `yuv444p` in modo che ogni pixel abbia la risoluzione completa per i componenti `y` (luminanza o luminosità) e `u` e `v` (crominanza o colore).      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO11-4)      Configura il fattore di velocità costante (CRF) dello stream per controllare la qualità e la compressione del video. Imposta il CRF a 17 per produrre un video di alta qualità senza perdite e con una compressione minima.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO11-5)      Codifica i fotogrammi in ingresso in pacchetti codificati con il multiplexer video configurato.      [![6](assets/6.png)](#co_ai_integration_and_model_serving_CO11-6)      Aggiunge i fotogrammi codificati nel buffer del contenitore video aperto.      [![7](assets/7.png)](#co_ai_integration_and_model_serving_CO11-7)      Spegne tutti i fotogrammi rimasti nel codificatore e combina il pacchetto risultante nel file di uscita prima di restituire il buffer contenente il video codificato.      Per utilizzare i prompt di immagini con il servizio come upload di file, devi installare la libreria `python-multipart`:^([14](ch03.html#id698))    ```", "```py   ```", "```py # main.py  from fastapi import status, FastAPI, File from fastapi.responses import StreamingResponse from io import BytesIO from PIL import Image  from models import load_video_model, generate_video from utils import export_to_video_buffer  ...  @app.post(     \"/generate/video\",     responses={status.HTTP_200_OK: {\"content\": {\"video/mp4\": {}}}},     response_class=StreamingResponse, ) def serve_image_to_video_model_controller(     image: bytes = File(...), num_frames: int = 25 ![1](assets/1.png) ):     image = Image.open(BytesIO(image)) ![2](assets/2.png)     model = load_video_model()     frames = generate_video(model, image, num_frames)     return StreamingResponse(         export_to_video_buffer(frames), media_type=\"video/mp4\" ![3](assets/3.png)     ) ```", "```py` ```", "```py```", "````` ## Modelli 3D    Ora hai capito come i modelli precedentemente citati utilizzano trasformatori e diffusori per generare qualsiasi forma di dati testuali, audio o visivi. La produzione di geometrie 3D richiede un approccio diverso rispetto alla generazione di immagini, audio e testi, perché devi tenere conto delle relazioni spaziali, delle informazioni sulla profondità e della coerenza geometrica, che aggiungono livelli di complessità non presenti in altri tipi di dati.    Per le geometrie 3D, le *mesh* vengono utilizzate per definire la forma di un oggetto. Pacchetti software come Autodesk 3ds Max, Maya e SolidWorks possono essere utilizzati per produrre, modificare e renderizzare queste mesh.    Le mesh sono effettivamente una collezione di *vertici*, *bordi* e *facce* che risiedono in uno spazio virtuale 3D. I vertici sono punti nello spazio che si collegano per formare i bordi. I bordi formano le facce (poligoni) quando si racchiudono su una superficie piana, spesso a forma di triangolo o quadrilatero. La[Figura 3-24](#vertices_edges_faces) mostra le differenze tra vertici, bordi e facce.  ![bgai 0324](assets/bgai_0324.png)  ###### Figura 3-24\\. Vertici, bordi e facce    Puoi definire i vertici in base alle loro coordinate in uno spazio 3D, solitamente determinate da un sistema di coordinate cartesiane (x, y, z). In sostanza, la disposizione e la connessione dei vertici formano le superfici di una maglia 3D che definiscono una geometria.    La[Figura 3-25](#mesh) mostra come queste caratteristiche si combinano per definire una mesh di una geometria 3D come la testa di una scimmia.  ![bgai 0325](assets/bgai_0325.png)  ###### Figura 3-25\\. Mesh per la geometria 3D di una testa di scimmia che utilizza poligoni triangolari equadrilateri (mostrato in Blender, software di modellazione 3D open source)    Puoi addestrare e utilizzare un modello trasformatore per prevedere il prossimo token di una sequenza in cui la sequenza è costituita da coordinate di vertici su una superficie mesh 3D. Un modello generativo di questo tipo può produrre geometrie 3D prevedendo il prossimo set di vertici e facce all'interno di uno spazio 3D che formano la geometria desiderata. Tuttavia, la geometria richiederebbe migliaia di vertici e facce per ottenere una superficie liscia.    Ciò significa che per ogni oggetto 3D è necessario attendere a lungo il completamento della generazione e che i risultati possono rimanere a bassa fedeltà. Per questo motivo, i modelli più efficaci (ad esempio Shap-E di OpenAI) nella produzione di geometrie 3D utilizzano funzioni (con molti parametri) per definire implicitamente superfici e volumi in uno spazio 3D.    Le funzioni implicite sono utili per creare superfici lisce o per gestire dettagli intricati che sono difficili da gestire per le rappresentazioni discrete come le mesh. Un modello addestrato può consistere in un codificatore che mappa i modelli in una funzione implicita.Invece di generare esplicitamente sequenze di vertici e facce per una mesh, i modelli 3D*condizionali* possono valutare le funzioni implicite addestrate in uno spazio 3D continuo. Di conseguenza, il processo di generazione ha un alto grado di libertà, controllo e flessibilità nella produzione di output ad alta fedeltà, diventando adatto per le applicazioni che richiedono geometrie 3D dettagliate e intricate.    Una volta che l'encoder del modello è stato addestrato a produrre funzioni implicite, sfrutta la tecnica di rendering dei *campi di radianza neurale* (NeRF), come parte del decodificatore, per costruire scene 3D. NeRF mappa una coppia di input - una coordinata spaziale 3D e una direzione di visione 3D - in un output costituito da una densità di oggetti e da un colore RGB tramite le funzioni implicite. Per sintetizzare nuove viste in una scena 3D, il metodo NeRF considera la finestra di visualizzazione come una matrice di raggi. Ogni pixel corrispondente a un raggio ha origine dalla posizione della telecamera e si estende nella direzione di visualizzazione. Il colore di ogni raggio e del pixel associato viene calcolato valutando la funzione implicita lungo il raggio e integrando i risultati per calcolare il colore RGB.    Una volta calcolata la scena 3D, le *funzioni di distanza firmata* (SDF) vengono utilizzate per generare mesh o wireframe di oggetti 3D calcolando la distanza e il colore di ogni punto rispetto alla superficie più vicina dell'oggetto 3D. Pensa alle SDF come a un modo per descrivere un oggetto 3D indicando la distanza di ogni punto nello spazio dalla superficie dell'oggetto. Questa funzione fornisce un numero per ogni punto: se il punto si trova all'interno dell'oggetto, il numero è negativo; se si trova sulla superficie, il numero è zero; se si trova all'esterno, il numero è positivo. La superficie dell'oggetto è quella in cui tutti i punti hanno il numero zero. Le SDF aiutano a trasformare queste informazioni in una mesh 3D.    Nonostante l'uso di funzioni implicite, la qualità dei risultati è ancora inferiore a quella degli asset 3D creati dall'uomo e può sembrare un cartone animato. Tuttavia, con i modelli 3D GenAI puoi generare le geometrie 3D iniziali per iterare i concetti e perfezionare gli asset 3D in modo rapido.    ### OpenAI Shap-E    *Shap-E* (sviluppato da OpenAI) è un modello open source che \"condiziona\" i dati 3D in ingresso (descrizioni, parametri, geometrie parziali, colori, ecc.) per generare forme 3D specifiche. Puoi usare Shap-E per creare un'immagine o servizi text-to-3D.    Come al solito, si inizia scaricando e caricando il modello da Hugging Face, come mostrato nell'[Esempio 3-13](#loading_shap-e).    ##### Esempio 3-13\\. Scaricare e caricare il modello Shap-E di OpenAI    ```py # models.py import torch from diffusers import ShapEPipeline  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  def load_3d_model() -> ShapEPipeline:     pipe = ShapEPipeline.from_pretrained(\"openai/shap-e\", device=device)     return pipe  def generate_3d_geometry(     pipe: ShapEPipeline, prompt: str, num_inference_steps: int ):     images = pipe(         prompt, ![1](assets/1.png)         guidance_scale=15.0, ![2](assets/2.png)         num_inference_steps=num_inference_steps, ![3](assets/3.png)         output_type=\"mesh\", ![4](assets/4.png)     ).images[0] ![5](assets/5.png)     return images ```    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO13-1)      Questa specifica pipeline di Shap-E accetta prompt testuali, ma se vuoi passare prompt di immagini, devi caricare una pipeline diversa.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO13-2)      Utilizza il parametro `guidance_scale` per regolare il processo di generazione in modo da adattarlo meglio al prompt.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO13-3)      Utilizza il parametro `num_inference_steps` per controllare la risoluzione dell'output in cambio di calcoli aggiuntivi. Richiedendo un numero maggiore di passi di inferenza o aumentando la scala di guida puoi allungare i tempi di rendering in cambio di output di qualità superiore che seguono meglio le richieste dell'utente.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO13-4)      Imposta il parametro `output_type` per produrre i tensori `mesh` come output.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO13-5)      Per impostazione predefinita, la pipeline Shap-E produce una sequenza di immagini che possono essere combinate per generare un'animazione GIF rotante dell'oggetto. Puoi esportare questo output in GIF, video o file OBJ che possono essere caricati in strumenti di modellazione 3D come Blender.      Ora che disponi delle funzioni di caricamento del modello e di generazione della mesh 3D, esporta la mesh in un buffer utilizzando l'[Esempio 3-14](#mesh_to_buffer).    ###### Suggerimento    `open3d` è una libreria open source per l'elaborazione di dati 3D come nuvole di punti, mesh e immagini a colori con informazioni sulla profondità (ad esempio, immagini RGB-D). Per eseguire l'[Esempio 3-14](#mesh_to_buffer) è necessario installare `open3d`:    ```py $ pip install open3d ```   ```py`##### Esempio 3-14\\. Esportazione di una mesh tensoriale 3D in un buffer OBJ Wavefront    ``` # utils.py  import os import tempfile from io import BytesIO from pathlib import Path import open3d as o3d import torch from diffusers.pipelines.shap_e.renderer import MeshDecoderOutput  def mesh_to_obj_buffer(mesh: MeshDecoderOutput) -> BytesIO:     mesh_o3d = o3d.geometry.TriangleMesh() ![1](assets/1.png)     mesh_o3d.vertices = o3d.utility.Vector3dVector(         mesh.verts.cpu().detach().numpy() ![2](assets/2.png)     )     mesh_o3d.triangles = o3d.utility.Vector3iVector(         mesh.faces.cpu().detach().numpy() ![2](assets/2.png)     )      if len(mesh.vertex_channels) == 3:  # You have color channels         vert_color = torch.stack(             [mesh.vertex_channels[channel] for channel in \"RGB\"], dim=1         ) ![3](assets/3.png)         mesh_o3d.vertex_colors = o3d.utility.Vector3dVector(             vert_color.cpu().detach().numpy()         ) ![4](assets/4.png)      with tempfile.NamedTemporaryFile(delete=False, suffix=\".obj\") as tmp:         o3d.io.write_triangle_mesh(tmp.name, mesh_o3d, write_ascii=True)         with open(tmp.name, \"rb\") as f:             buffer = BytesIO(f.read()) ![5](assets/5.png)         os.remove(tmp.name) ![6](assets/6.png)      return buffer ```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO14-1)      Crea un oggetto mesh triangolare Open3D.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO14-2)      Convertire la mesh generata dal modello in un oggetto mesh triangolare Open3D. Per fare ciò, prendi i vertici e i triangoli dalla mesh 3D generata spostando i tensori dei vertici e delle facce della mesh sulla CPU e convertendoli in array `numpy`.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO14-4)      Controlla se il mesh ha tre canali di colore dei vertici (che indicano i dati dei colori RGB) e impila questi canali in un tensore.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO14-5)      Converte il tensore del colore della mesh in un formato compatibile con Open3D per impostare i colori dei vertici della mesh.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO14-6)      Utilizza un file temporaneo per creare e restituire un buffer di dati.      [![6](assets/6.png)](#co_ai_integration_and_model_serving_CO14-7)      Windows non supporta l'opzione `NameTemporaryFile`'`delete=True`. Invece, rimuovi manualmente il file temporaneo creato appena prima di restituire il buffer in memoria.      Infine, puoi creare gli endpoint, come mostrato nell'[esempio 3-15](#shap-e_endpoint).    ##### Esempio 3-15\\. Creazione dell'endpoint di servizio del modello 3D    ``` # main.py  from fastapi import FastAPI, status from fastapi.responses import StreamingResponse from models import load_3d_model, generate_3d_geometry from utils import mesh_to_obj_buffer  ...  @app.get(     \"/generate/3d\",     responses={status.HTTP_200_OK: {\"content\": {\"model/obj\": {}}}}, ![1](assets/1.png)     response_class=StreamingResponse, ) def serve_text_to_3d_model_controller(     prompt: str, num_inference_steps: int = 25 ):     model = load_3d_model()     mesh = generate_3d_geometry(model, prompt, num_inference_steps)     response = StreamingResponse(         mesh_to_obj_buffer(mesh), media_type=\"model/obj\"     )     response.headers[\"Content-Disposition\"] = (         f\"attachment; filename={prompt}.obj\"     ) ![2](assets/2.png)     return response ```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO15-1)      Specifica la specifica OpenAPI per una risposta di successo che includa `model/obj` come tipo di contenuto multimediale.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO15-2)      Indica ai client che il contenuto della risposta in streaming deve essere trattato come un allegato.      Se invii una richiesta all'endpoint `/generate/3d`, il download dell'oggetto 3D come file OBJ Wavefront dovrebbe iniziare non appena la generazione è completata.    Puoi importare il file OBJ in qualsiasi software di modellazione 3D, come Blender, per visualizzare la geometria 3D. Utilizzando prompt come `apple`, `car`, `phone` e `donut`puoi generare le geometrie 3D mostrate nella [Figura 3-26](#shape_e_blender).  ![bgai 0326](assets/bgai_0326.png)  ###### Figura 3-26\\. Geometrie 3D di un'automobile, una mela, un telefono e una ciambella importate in Blender    Se isoli un oggetto come la mela e attivi la vista wireframe, puoi vedere tutti i vertici e gli spigoli che compongono la maglia della mela, rappresentati come poligoni triangolari, come mostrato all'indirizzo nella [Figura 3-27](#shape_e_apple_wireframe).  ![bgai 0327](assets/bgai_0327.png)  ###### Figura 3-27\\. Ingrandimento della mesh 3D generata per visualizzare i poligoni triangolari; in basso: visualizzazione della mesh della geometria della mela generata (inclusi vertici e bordi).    Shap-E sostituisce un altro modello più vecchio, chiamato *Point-E*, che genera *nuvole di punti*di oggetti 3D.Questo perché Shap-E, rispetto a Point-E, converge più velocemente eraggiunge una qualità di generazione della forma paragonabile o migliore, nonostante la modellazione di uno spazio di output multidimensionale e multirappresentazione.    Le nuvole di punti (spesso utilizzate nel settore edile) sono un'ampia raccolta di coordinate di punti che rappresentano fedelmente un oggetto 3D (come la struttura di un edificio) in uno spazio reale. I dispositivi di scansione ambientale, come gli scanner laser LiDAR, producono nuvole di punti per rappresentare gli oggetti all'interno di uno spazio 3D con misure approssimative vicine all'ambiente reale.    Con il miglioramento dei modelli 3D, potrebbe essere possibile generare oggetti che rappresentano fedelmente le loro controparti reali.```` ```py`` `````", "``````py` ``````", "``````py``` ``````", "```` ```py````", "```py`  ```", "```py```", "``````py`` ``````", "``` # main.py  from contextlib import asynccontextmanager from typing import AsyncIterator from fastapi import FastAPI, Response, status from models import load_image_model, generate_image from utils import img_to_bytes  models = {} ![1](assets/1.png)  @asynccontextmanager ![2](assets/2.png) async def lifespan(_: FastAPI) -> AsyncIterator[None]:     models[\"text2image\"] = load_image_model() ![3](assets/3.png)      yield ![4](assets/4.png)      ... # Run cleanup code here      models.clear() ![5](assets/5.png)  app = FastAPI(lifespan=lifespan) ![6](assets/6.png)  @app.get(     \"/generate/image\",     responses={status.HTTP_200_OK: {\"content\": {\"image/png\": {}}}},     response_class=Response, ) def serve_text_to_image_model_controller(prompt: str):     output = generate_image(models[\"text2image\"], prompt) ![7](assets/7.png)     return Response(content=img_to_bytes(output), media_type=\"image/png\") ```", "``` $ pip install bentoml ```", "````` Puoi vedere come avviare un server BentoML nell'[Esempio 3-18](#bentoml_usage).    ##### Esempio 3-18\\. Servire un modello di immagine con BentoML    ```py # bento.py import bentoml from models import load_image_model  @bentoml.service(     resources={\"cpu\": \"4\"}, traffic={\"timeout\": 120}, http={\"port\": 5000} ) ![1](assets/1.png) class Generate:     def __init__(self) -> None:         self.pipe = load_image_model()      @bentoml.api(route=\"/generate/image\") ![2](assets/2.png)     def generate(self, prompt: str) -> str:         output = self.pipe(prompt, num_inference_steps=10).images[0]         return output ```    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO17-1)      Dichiara un servizio BentoML con quattro CPU allocate. Il servizio deve andare in time out in 120 secondi se il modello non viene generato in tempo e deve essere eseguito dalla porta `5000`.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO17-2)      Dichiara un controllore API per eseguire il processo di generazione del modello principale. Questo controllore si aggancia al gestore delle rotte API di BentoML.      Puoi quindi eseguire il servizio BentoML localmente:    ```py $ bentoml serve service:Generate ```   ```py`Il tuo server FastAPI può ora diventare un client con il modello che viene servito esternamente. Ora puoi fare richieste HTTP `POST` dall'interno di FastAPI per ottenere una risposta, come mostrato nell'[esempio 3-19](#fastapi_bentoml_usage).    ##### Esempio 3-19\\. Endpoint BentoML tramite FastAPI    ``` # main.py  import httpx from fastapi import FastAPI, Response  app = FastAPI()  @app.get(     \"/generate/bentoml/image\",     responses={status.HTTP_200_OK: {\"content\": {\"image/png\": {}}}},     response_class=Response, ) async def serve_bentoml_text_to_image_controller(prompt: str):     async with httpx.AsyncClient() as client: ![1](assets/1.png)         response = await client.post(             \"http://localhost:5000/generate\", json={\"prompt\": prompt}         ) ![2](assets/2.png)     return Response(content=response.content, media_type=\"image/png\") ```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO18-1)      Crea un client HTTP asincrono utilizzando la libreria `httpx`.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO18-2)      Invia una richiesta `POST` all'endpoint del modello di generazione di immagini BentoML.```` ```py``  `````", "```py` ### Fornitori di modelli    Oltre a BentoML e ai cloud provider, puoi anche utilizzare fornitori esterni di servizi di modello come OpenAI. In questo caso, la tua applicazione FastAPI diventa un wrapper di servizio sulle API di OpenAI.    Fortunatamente, l'integrazione con le API dei fornitori di modelli come OpenAI è piuttosto semplice, come mostrato nell'[Esempio 3-20](#openai_usage).    ###### Suggerimento    Per eseguire l'[Esempio 3-20](#openai_usage), devi ottenere una chiave API e impostare la variabile d'ambiente `OPENAI_API_KEY` su questa chiave, come consigliato da OpenAI.    ##### Esempio 3-20\\. Integrazione con il servizio OpenAI    ```", "```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO19-1)      Usa il modello `gpt-4o` per chattare con il modello tramite l'API OpenAI.      Ora dovresti essere in grado di ottenere gli output tramite chiamate esterne al servizio OpenAI.   `Quando utilizzi servizi esterni, tieni presente che i dati saranno condivisi con fornitori di servizi terzi. In questo caso, se tieni alla privacy e alla sicurezza dei dati, potresti preferire le soluzioni self-hosted. Con il self-hosting, il compromesso sarà una maggiore complessità nell'implementazione e nella gestione dei tuoi server modello.    Se vuoi davvero evitare di servire da solo modelli di grandi dimensioni, i provider Cloud possono fornire soluzioni gestite in cui i tuoi dati non vengono mai condivisi con terze parti. Un esempio è Azure OpenAI, che al momento in cui scriviamo fornisce istantanee dei migliori LLMs e del generatore di immagini di OpenAI.    Ora hai alcune opzioni per il servizio del modello. Un ultimo sistema da implementare prima di concludere questo capitolo è la registrazione e il monitoraggio del servizio.` ```", "```py``  ```", "```py ```", "```py`# Il ruolo del middleware nel monitoraggio dei servizi    Puoi implementare un semplice strumento di monitoraggio in cui i prompt e le risposte possono essere registrati insieme all'utilizzo dei token di richiesta e di risposta. Per implementare il sistema di logging, puoi scrivere alcune funzioni di logging all'interno del controller che serve il modello. Tuttavia, se hai più modelli ed endpoint, potresti trarre vantaggio dallo sfruttamento del meccanismo middleware FastAPI.    Il middleware è un blocco di codice essenziale che viene eseguito prima e dopo l'elaborazione di una richiesta da parte di uno qualsiasi dei tuoi controller. Puoi definire un middleware personalizzato da allegare ai gestori delle rotte API. Una volta che le richieste raggiungono i gestori delle rotte, il middleware funge da intermediario, elaborando le richieste e le risposte tra il client e il controller del server.    Tra gli utilizzi eccellenti del middleware ci sono il logging e il monitoraggio, la limitazione della velocità, il filtraggio dei contenuti e l'implementazione di CORS (cross-origin resource sharing).    L['esempio 3-22](#middleware_monitoring_example) mostra come puoi monitorare i gestori che servono il modello.    # Registrazione dell'uso tramite middleware personalizzato in produzione    Non usare l'[Esempio 3-22](#middleware_monitoring_example) in produzione perché i registri di monitoraggio possono scomparire se esegui l'applicazione da un container Docker o da una macchina host che può essere cancellata o riavviata senza un volume persistente montato o un registro su un database.    Nel [Capitolo 7](ch07.html#ch07), integrerai il sistema di monitoraggio con un database per conservare i log al di fuori dell'ambiente applicativo.    ##### Esempio 3-22\\. Utilizzo dei meccanismi middleware per acquisire i log di utilizzo dei servizi    ```", "```py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO21-1)      Dichiara una funzione decorata dal meccanismo middleware HTTP di FastAPI. La funzione deve ricevere l'oggetto `Request` e la funzione di callback `call_next` per essere considerata un middleware `http` valido.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO21-2)      Passa la richiesta al gestore del percorso per elaborare la risposta.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO21-3)      Genera un ID di richiesta per tracciare tutte le richieste in arrivo anche se viene sollevato un errore in `call_next` durante l'elaborazione della richiesta.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO21-4)      Calcola la durata della risposta con quattro cifre decimali.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO21-5)      Imposta intestazioni di risposta personalizzate per il tempo di elaborazione e l'ID della richiesta.      [![6](assets/6.png)](#co_ai_integration_and_model_serving_CO21-6)      Registra l'URL dell'endpoint attivato, la data e l'ID della richiesta, l'indirizzo IP del cliente, il tempo di elaborazione della risposta e il codice di stato in un file CSV su disco in modalità `append`.      In questa sezione vengono catturate le informazioni sull'utilizzo dell'endpoint, tra cui il tempo di elaborazione, il codice di stato, il percorso dell'endpoint e l'IP del client.    Il middleware è un potente sistema per eseguire blocchi di codice prima che le richieste vengano passate ai gestori delle rotte e prima che le risposte vengano inviate all'utente. Hai visto un esempio di come il middleware può essere utilizzato per registrare l'utilizzo del modello per qualsiasi endpoint che serva il modello.    # Accesso ai corpi delle richieste e delle risposte nel middleware    Se hai bisogno di tracciare le interazioni con i tuoi modelli, compresi i prompt e il contenuto che generano, l'utilizzo del middleware per il logging è più efficiente rispetto all'aggiunta di logger individuali a ogni gestore. Tuttavia, devi tenere conto della privacy dei dati e delle prestazioni quando registri i corpi delle richieste e delle risposte, poiché l'utente potrebbe inviare al tuo servizio dati sensibili o di grandi dimensioni, che richiederanno una gestione attenta.    # Sommario    In questo capitolo abbiamo trattato molti concetti, quindi rivediamo rapidamente tutto ciò che abbiamo discusso.    Hai visto come scaricare, integrare e servire una serie di modelli GenAI open source dal repository Hugging Face in una semplice interfaccia utente utilizzando il pacchetto Streamlit, con poche righe di codice. Hai anche esaminato diversi tipi di modelli e come servirli tramite endpoint FastAPI. I modelli che hai sperimentato erano basati su testo, immagini, audio, video e 3D e hai visto come elaborano i dati. Hai anche imparato le architetture dei modelli e i meccanismi sottostanti che li alimentano.    In seguito, hai esaminato diverse strategie di model-serving, tra cui lo scambio di modelli su richiesta, il preloading dei modelli e infine il model-serving al di fuori dell'applicazione FastAPI utilizzando altri framework come BentoML o API di terze parti.    Poi hai notato che i modelli più grandi potevano impiegare un po' di tempo per generare le risposte. Infine, hai implementato un meccanismo di monitoraggio dei servizi per i tuoi modelli che sfrutta il sistema middleware FastAPI per ogni endpoint che serve i modelli. Hai poi scritto i log su disco per analisi future.    Ora dovresti sentirti più sicuro nel costruire i tuoi servizi GenAI basati su una serie di modelli open source.    Nel prossimo capitolo imparerai a conoscere meglio la sicurezza dei tipi e il suo ruolo nell'eliminare i bug delle applicazioni e nel ridurre l'incertezza quando si lavora con API e servizi esterni. Vedrai anche come convalidare le richieste e gli schemi di risposta per rendere i tuoi servizi ancora più affidabili.    # Riferimenti aggiuntivi    *   [\"Bark\",](https://oreil.ly/HKT8O) nella documentazione di \"Transformers\", *Hugging Face*, consultato il 26 marzo 2024.           *   Borsos, Z., et al. (2022).[\"AudioLM: A Language Modeling Approach to Audio Generation\".](https://oreil.ly/8YZBr) arXiv preprint arXiv:2209.03143.           *   Brooks, T., et al. (2024).[\"Video Generation Models as World Simulators\".](https://oreil.ly/52duF) OpenAI.           *   Défossez, A., et al. (2022).[\"High-Fidelity Neural Audio Compression\".](https://oreil.ly/p4_-5) arXiv preprint arXiv:2210.13438.           *   Jun, H. & Nichol, A. (2023).[\"Shap-E: Generating Conditional 3D Implicit Functions\".](https://oreil.ly/LzLy0) arXiv preprint arXiv:2305.02463.           *   Kim, B.-K., et al. (2023).[\"BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion\".](https://oreil.ly/uErOQ) arXiv preprint arXiv:2305.15798.           *   Liu, Y., et al. (2024).[\"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models\".](https://oreil.ly/Zr6bJ) arXiv preprint arXiv:2402.17177.           *   Mildenhall, B., et al. (2020).[\"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\".](https://oreil.ly/hBiBV) arXiv preprint arXiv:2003.08934.           *   Nichol, A., et al. (2022).[\"Point-E: A System for Generating 3D Point Clouds from Complex Prompts\".](https://oreil.ly/FW-wT) arXiv preprint arXiv:2212.08751.           *   Vaswani, A., et al. (2017).[\"Attention Is All You Need\".](https://oreil.ly/N4MkH) arXiv preprint arXiv:1706.03762.           *   Wang, C., et al. (2023).[\"Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers\".](https://oreil.ly/h1D0e) arXiv preprint arXiv:2301.02111.           *   Zhang, P., et al. (2024). [\"TinyLlama: An Open-Source Small Language Model\".](https://oreil.ly/Idi1B) arXiv preprint arXiv:2401.02385.              ^([1](ch03.html#id630-marker)) Hugging Face offre l'accesso a un'ampia gamma di modelli di apprendimento automatico pre-addestrati, set di dati e applicazioni.    ^([2](ch03.html#id631-marker)) A. Vaswani et al. (2017), [\"Attention Is All You Need\",](https://oreil.ly/sO33r) arXiv preprint arXiv:1706.03762.    ^([3](ch03.html#id635-marker)) Un ottimo strumento per visualizzare le mappe di attenzione è [BertViz](https://oreil.ly/e2Q7X).    ^([4](ch03.html#id637-marker)) Puoi trovare l'elenco aggiornato degli LLMs open source sul [repository Open LLM GitHub](https://oreil.ly/GZaEr).    ^([5](ch03.html#id647-marker)) Un modello di embedding o uno strato di embedding come in un trasformatore    ^([6](ch03.html#id658-marker)) Questo processo di generazione sequenziale dei token può anche limitare la scalabilità di sequenze lunghe, poiché ogni token si basa su quello precedente.    ^([7](ch03.html#id665-marker)) Il [repository dei modelli di Hugging Face](https://huggingface.co) è una risorsa per gli sviluppatori di IA per pubblicare e condividere i loro modelli pre-addestrati.    ^([8](ch03.html#id666-marker)) Consulta la [documentazione](https://pytorch.org) di [PyTorch](https://pytorch.org) per le istruzioni di installazione.    ^([9](ch03.html#id667-marker)) La precisione del tensore `float16` è più efficiente in termini di memoria in ambienti con vincoli di memoria. I calcoli possono essere più veloci ma la precisione è inferiore rispetto ai tensori di `float32`. Per maggiori informazioni, consulta la [scheda del modello TinyLlama](https://oreil.ly/rsmoB).    ^([10](ch03.html#id668-marker)) Come abbiamo visto nel [Capitolo 2](ch02.html#ch02), i controller sono funzioni che gestiscono le richieste in entrata di una rotta API e restituiscono le risposte al cliente attraverso un'esecuzione logica di servizi o provider.    ^([11](ch03.html#id674-marker)) Streamlit raccoglie le statistiche di utilizzo per impostazione predefinita, ma puoi disattivare questa funzione utilizzando un [file di configurazione](https://oreil.ly/m_Jix).    ^([12](ch03.html#id686-marker)) Lo spazio latente di un modello addestrato, quando viene visualizzato, può sembrare un rumore bianco ma contiene rappresentazioni strutturate che il modello ha imparato a codificare e decodificare.    ^([13](ch03.html#id697-marker)) *Il multiplexing* è il processo di combinazione di più flussi (come audio, video e sottotitoli) in un unico file o flusso in modo sincronizzato.    ^([14](ch03.html#id698-marker)) La libreria `python-multipart` è utilizzata per il parsing di `multipart/form-data`, una codifica comunemente utilizzata per l'invio di moduli di caricamento di file.```", "```py`` ```", "```py ```", "```py` ```", "```py`` ```"]