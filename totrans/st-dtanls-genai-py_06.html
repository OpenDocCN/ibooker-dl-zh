<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">7</span> </span> <span class="chapter-title-text">Scaling and performance optimization</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Optimizing data analysis code</li> 
    <li class="readable-text" id="p3">Scaling code across multiple CPUs and GPUs</li> 
    <li class="readable-text" id="p4">Code conversion between programming languages</li> 
    <li class="readable-text" id="p5">Cloud-based deployment</li> 
    <li class="readable-text" id="p6">Using the Dask framework for all of this</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>In today’s data-driven world, it’s more likely than not that the insights required by your business will be gained from vast amounts of data. As data volumes continue to grow and analytical complexity increases, and as algorithms and data processing pipelines reach the point where the sheer volume of data or the complexity of operations limits their ability to deliver timely and accurate results, you’ll encounter the need for code performance optimization. It will likely become essential for ensuring your analytics remain effective and efficient, even when the underlying data and computational demands grow.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>Throughout this chapter, we’ll assume that the analysis of the Brazilian e-commerce data we performed in previous chapters was so successful that, instead of encompassing a set of a hundred thousand orders, it now needs to be performed periodically on datasets of millions of orders. To make things even more interesting, let’s assume that the business grew and became multinational. Such growth could easily increase the amount of data by several orders of magnitude. At first glance, you might think throwing more CPUs at the problem or migrating to the latest GPU for its sheer processing power would solve the problem, but this would invariably involve either capital expenditure or an extension of the budget for cloud solutions. Just as an old IT adage states that any data will quickly fill all space allocated to it, you will quickly find that, when attempting any non-trivial data analysis, the calculations invariably consume all computing power available.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>Fortunately, you have numerous strategies for scaling and performance optimization at your disposal, including parallel and distributed computing, GPU acceleration, and specialized hardware or software solutions. By exploring various methods and tools, you should be able to find the most appropriate techniques for your specific data analytics needs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>There is a catch, though. There is no one-size-fits-all solution. Your choices will almost always be limited by the technology stack available to you and either by the budget or by what your business or institution has authorized for use. That is why we’ll provide an overview of different methods here rather than taking a deep dive into a single option.</p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>As you’ll see, generative AI, and particularly GPT models, can greatly support your optimization and scaling efforts. These advanced natural language processing models can help you identify relevant strategies, suggest improvements, review code, and even generate optimized code snippets.</p> 
  </div> 
  <div class="readable-text intended-text" id="p12"> 
   <p>In this chapter, we will delve deeper into the importance of scaling and optimization in the context of data analytics. We will cover various techniques and approaches to achieve optimal performance and discuss the role of GPT models in supporting this process. Additionally, we will provide practical examples and case studies to demonstrate the real-world impact of effective scaling and optimization in data analytics.</p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>Although there are many tools that purport to perform optimization and scaling automatically, we believe that knowing these topics is vital in understanding proper code performance. Sadly, a lot of newbie data scientists, analysts, and engineers take code performance for granted or delegate it to said automated tools, potentially seriously underachieving in their efforts.</p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>By the end of this chapter, you will have a solid understanding of the critical role that scaling and performance optimization play in data analytics, and you’ll have the knowledge necessary to identify, evaluate, and implement effective strategies to ensure your analytics pipeline remains efficient and effective in the face of ever-growing data volumes and computational demands.</p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_109"><span class="num-string">7.1</span> Performance measurement</h2> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>When setting out to optimize your analytics, the first crucial step is defining what good performance means for your goals. You need a clear yardstick to measure the success of your efforts. This means defining two basic concepts:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p17"> What characteristics of the system’s performance are most important? Should it prioritize quick response or accuracy? Should it allow multiple user access, or is it OK for users to have to queue? Can it block access while it is processing, or does it still need to allow users to view the data while they are waiting? </li> 
   <li class="readable-text" id="p18"> What are the baselines for the preceding characteristics? Should we aim for the current expected load, or already plan for future expansion? Can we reliably define what a normal load will look like? </li> 
  </ul> 
  <div class="readable-text" id="p19"> 
   <p>Here, we could consider such metrics as the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p20"> <em>Latency</em><em> </em><em>—</em>The time it takes for a data analysis system to begin displaying initial results after the data input is complete. </li> 
   <li class="readable-text" id="p21"> <em>Throughput</em><em> </em><em>—</em>The amount of data the system can process within a given time frame. It is typically measured in records per second or gigabytes per hour, reflecting the system’s efficiency in handling large datasets. </li> 
   <li class="readable-text" id="p22"> <em>Parallelism or concurrency</em><em> </em><em>—</em>The number of data analysis queries or processes the system can handle simultaneously without performance degradation. It demonstrates the system’s ability to manage multiple user requests or tasks at the same time. </li> 
   <li class="readable-text" id="p23"> <em>Bandwidth</em><em> </em>—In the context of data analysis, this refers to the volume of data that can be imported into or exported out of the system per unit time. It is crucial for systems that need to handle large data transfers efficiently. </li> 
   <li class="readable-text" id="p24"> <em>Response time</em><em> </em>—The total time it takes for a system to complete a data analysis query or task from the moment it is initiated. This metric includes both the computation time and the time taken to retrieve and display the results. </li> 
   <li class="readable-text" id="p25"> <em>Availability</em><em> </em>—The likelihood that the data analysis system is operational and available for use when needed. It’s particularly important in systems requiring high uptime for continuous data analysis and decision-making processes. </li> 
   <li class="readable-text" id="p26"> <em>Scalability</em><em> </em>—The ability of the data analysis system to maintain or improve performance as the size of the data grows. This includes adding more data sources, handling more complex queries, or serving more users without a drop in performance. </li> 
   <li class="readable-text" id="p27"> <em>Reliability</em><em> </em>—The consistency of the data analysis system in providing accurate and timely results under different conditions. This metric is often assessed by the frequency and severity of system failures or inaccuracies in the output data. </li> 
  </ul> 
  <div class="readable-text" id="p28"> 
   <p>Let’s look at a real-world scenario: a company was moving its infrastructure to the cloud and, suddenly, solutions that worked well on virtual machines became unusable due to the sheer amount of data to be analyzed. Processing times shot up dramatically, sometimes exceeding a 5,400% increase. Management had to reassess their approach, targets, and priorities swiftly, and, due to limited capacities, it became critical to have a good understanding of what “good” means in this specific context.</p> 
  </div> 
  <div class="readable-text intended-text" id="p29"> 
   <p>Comparing vastly different domains can illustrate the different priorities. If you consider a data analysis system for an astronomical observatory, it is likely that latency or response time will not be as critical as in the case of, for example, a patient-monitoring system in a hospital. Furthermore, it would probably make more sense to build a system for monitoring a single patient, and create separate instances for each patient, than invest in a single multi-patient system, given that each patient is likely to require separate care. In contrast, an automation system for a chemical plant would likely require many concurrent and linked processes to run and link together to provide a full picture to the operators.</p> 
  </div> 
  <div class="readable-text intended-text" id="p30"> 
   <p>Once you understand your requirements, you’ll be able to define whether your current system, whether automated or manual, is sufficient, and where it requires improvements. Then, once changes are made, you should be able to understand how your new system performs compared to the previous version, and whether the changes provided the desired impact.</p> 
  </div> 
  <div class="readable-text intended-text" id="p31"> 
   <p>In this section, we’ll explore straightforward approaches and metrics that can be used to analyze the efficiency of your data analytics systems.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p32"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">How to measure?</h5> 
   </div> 
   <div class="readable-text" id="p33"> 
    <p>There are various approaches to measuring the performance of data analytics systems, including the following:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p34"> <em>Profiling</em>—Collecting detailed information about the execution of a program or algorithm, such as the time spent in each function or the amount of memory allocated. This data can help identify hot spots or areas of the code that consume the most resources. </li> 
    <li class="readable-text" id="p35"> <em>Stress testing</em>—Related to profiling, but under loads approaching or exceeding maximum expected amounts. This allows you to check if the system can still perform adequately if pushed to the limit. </li> 
    <li class="readable-text" id="p36"> <em>Benchmarking</em>—Comparing the performance of a system against a reference implementation or industry-standard metrics. </li> 
    <li class="readable-text" id="p37"> <em>Monitoring</em>—Collecting and analyzing performance metrics over time, allowing businesses to track the impact of optimization efforts and identify any emerging issues or trends in response to changing data volumes and computational demands. </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>Let’s take an analysis example performed in chapter 4, where we used bootstrap methods to calculate confidence intervals for the average sales price and average review score per state. Bootstrapping is a computationally intensive technique, as it involves repeatedly drawing random samples with replacements from the original data, and recalculating the statistic of interest. </p> 
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>Several performance metrics can be used to evaluate the efficiency of a data analytics system. We’ll look at three of the most important areas in the rest of this section: execution time, throughput, and resource utilization. We will, however, focus mostly on the execution time. Depending on the properties and limitations of your computing environment, some of these areas might be of more importance than others.</p> 
  </div> 
  <div class="readable-text" id="p40"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_110"><span class="num-string">7.1.1</span> Execution time</h3> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>Execution time is a direct indicator of your system’s overall speed and responsiveness. It’s defined as the time it takes to complete a specific task or process a given volume of data.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p42"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">In CPU time, I’m young; in real time, my coffee’s cold again</h5> 
   </div> 
   <div class="readable-text" id="p43"> 
    <p>In today’s multitasking and multithreading systems, measuring execution time cannot be reliably performed with just a stopwatch. If your CPU is busy crunching numbers, and you decide to play the latest AAA game on the same machine to kill time, you could be disappointed with the performance of your program, even though it’s trying its absolute best.</p> 
   </div> 
   <div class="readable-text" id="p44"> 
    <p>In this context, <em>CPU time</em> refers to the actual time your processor spent on the calculation, as opposed to handling all the other tasks, like updating your screen, running your antivirus in the background, and refreshing your browser while you’re reading the news. Invariably, this will be significantly less than <em>real time</em>, or elapsed time.</p> 
   </div> 
   <div class="readable-text" id="p45"> 
    <p>Some of the methods used in this chapter will rely on measuring elapsed time as a proxy, since it’s much easier to do and usually suffices unless you are running advanced analysis of execution times on very short pieces of code. However, bear these points in mind:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p46"> It’s best to not run other intensive processes while measuring the performance of your calculations. </li> 
    <li class="readable-text" id="p47"> When measuring performance, use relatively large dataset sizes, as this will make comparisons between different pieces of code more reliable by reducing the impact of overhead processing. </li> 
   </ul> 
   <div class="readable-text" id="p48"> 
    <p>Don’t be surprised if running the same code multiple times produces different execution times. It’s best to average the runtime of several runs to get a better approximation. Alternatively, take the minimum runtime from several attempts, as this will reflect the minimum impact of overhead on the processor.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>Let’s see if we can get our GPT advisor to provide us with the code required to measure the execution time of our bootstrap function.</p> 
  </div> 
  <div class="readable-text prompt" id="p50"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>I have a function <code>bootstrap_CI()</code>. <code>bootstrap_CI()</code> is being run from within another function, <code>calculate_ci(group)</code>, which, in turn, is run with the command: <code>ci_df = grouped_data.apply(calculate_ci)</code>. The <code>grouped_data</code> object is created as <code>grouped_data = filtered_df.groupby('seller_state')</code></p> 
  </div> 
  <div class="readable-text prompt" id="p51"> 
   <p>Please provide code to measure the performance of the <code>bootstrap_CI()</code> function depending on the size of <code>filtered_df</code> DataFrame (randomly selecting appropriate subset), from 100 to 9944 elements, and graph it.</p> 
  </div> 
  <div class="readable-text response" id="p52"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To measure the performance of the <code>bootstrap_CI()</code> function depending on the number of randomly selected entries from <code>filtered_df</code> DataFrame, you can create the function which uses the <code>sample</code> function from pandas, applies the <code>groupby</code> operation, and measures the time it takes to run the <code>apply</code> function with <code>calculate_ci</code> on the grouped data. Here’s the sample code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p53"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import numpy as np
import pandas as pd
import timeit
import matplotlib.pyplot as plt

<em># Your bootstrap_CI function here</em>
<em># Your calculate_ci function here</em>

<em># Use your actual filtered_df DataFrame</em>

def measure_performance(data_sizes):
<em>    """</em>
<em>    Measure the performance of the calculate_ci function for different </em>
<em>    <span class="">↪</span>input data sizes.</em>
<em>    </em>
<em><em> </em>Parameters:</em>
<em>    data_sizes (list): A list of data sizes to test.</em>
<em>    </em>
<em><em> </em>Returns:</em>
<em>    list: A list of execution times for each data size.</em>
<em>    """</em>
    times = []
    for data_size in data_sizes:
        subset_df = filtered_df.sample(n=data_size, replace=False, 
        <span class="">↪</span>random_state=42)
        grouped_data = subset_df.groupby('seller_state')

        start_time = timeit.default_timer()
        ci_df = grouped_data.apply(calculate_ci)
        end_time = timeit.default_timer()
        times.append(end_time - start_time)

    return times

data_sizes = [100, 500, 1000, 5000, 9944]
times = measure_performance(data_sizes)

plt.plot(data_sizes, times, marker='o')
plt.xlabel('Size of filtered_df')
plt.ylabel('Execution Time (s)')
plt.title('Performance of calculate_ci Function')
plt.grid(True)
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p54"> 
   <p>This code measures the performance of the <code>calculate_ci()</code> function for different sizes of randomly-selected subsets of <code>filtered_df</code>. The execution times are plotted against the sizes of the subsets.</p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <p>The result is presented in figure 7.1.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p56">  
   <img alt="figure" src="../Images/CH07_F01_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.1</span> Chart of time of executing bootstrapping function depending on the size of the processed DataFrame</h5>
  </div> 
  <div class="readable-text" id="p57"> 
   <p>This metric is pretty straightforward; however, you need to remember that this is the narrowest metric, as its measurement is only valid for the specific input, system load, configurations, and so on.</p> 
  </div> 
  <div class="readable-text intended-text" id="p58"> 
   <p>Furthermore, this simple example only takes into account a single measurement. In practice, this will not be very accurate, as modern computers are designed to perform many tasks concurrently, and that will have an impact on the one business-critical process you are trying to measure. Therefore, to isolate these variable effects, but still get a reliable reading, you should do the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p59"> Perform the profiling under expected loads. There is no point profiling a system on an isolated test server, and then expecting it to perform identically on a production server with 50 other processes running, competing for resources. </li> 
   <li class="readable-text" id="p60"> Profile many runs and perform appropriate statistical analysis, calculating at the very least the minimum, maximum, and average times. This will give you a much better picture of expected performance than a single run. </li> 
  </ul> 
  <div class="readable-text" id="p61"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_111"><span class="num-string">7.1.2</span> Throughput</h3> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>Throughput is a measure of the rate at which a system can process data, typically measured in transactions per second or records processed per second. High throughput is essential for handling large data volumes and meeting real-time processing requirements.</p> 
  </div> 
  <div class="readable-text intended-text" id="p63"> 
   <p>As throughput is a simple division of the size of data by elapsed time, we slightly modified the code we used for the execution time measurement. You can find the full code in the Jupyter notebook in the book’s GitHub repository. The result, presented in figure 7.2, may be somewhat surprising.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p64">  
   <img alt="figure" src="../Images/CH07_F02_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.2</span> Dependence of the execution time (solid line) and throughput (dashed line) of the bootstrapping function on the size of the processed DataFrame</h5>
  </div> 
  <div class="readable-text" id="p65"> 
   <p>Intuitively, if throughput is the number of processed records divided by the time of processing all records, each should be processed in the same way. It’s true that, if each record were processed independently and in a constant time, the throughput would remain constant. However, in practice, this is not always the case. Various factors can affect the processing time of each record and the overall throughput, among them record size and complexity (not our case), data dependencies (not our case), resource contention (maybe our case), algorithmic behavior (likely our case), or external factors like the network latency, disk speed, or other system processes. </p> 
  </div> 
  <div class="readable-text intended-text" id="p66"> 
   <p>Throughput can be a helpful tool when estimating the projected increase in resource requirements in relation to an increase in the amount of data to be processed. </p> 
  </div> 
  <div class="readable-text" id="p67"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_112"><span class="num-string">7.1.3</span> Resource utilization</h3> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>To get a fuller view of how your code is performing, you should not just take into account the execution time of your code, but also observe system resources, like memory and storage, used during code execution. High resource utilization can tell you if the system is efficiently using the available resources, and it may also signal potential bottlenecks or resource contention.</p> 
  </div> 
  <div class="readable-text intended-text" id="p69"> 
   <p>Monitoring system resources is material for a separate book (and likely more than one), but some simple measurements and observations can tell you if you are hitting the limits, and measurements should be available for you irrespective of whether you use your Windows laptop (the Resource Monitor app), Linux (the top utility), or cloud solutions (provider-dependent).</p> 
  </div> 
  <div class="readable-text intended-text" id="p70"> 
   <p>Measurement of resource utilization is critical in distinguishing two primary limiting factors, or bottlenecks, of your program or task: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p71"> <em>CPU-bound tasks</em><em> </em><strong>—</strong>These tasks are mainly limited by the processing power of the CPU on which your program runs. The performance of CPU-bound tasks can be improved by using a faster CPU or by parallelizing the task across multiple CPU cores. Examples of CPU-bound tasks include numerical simulations, complex mathematical calculations, and data processing tasks like sorting or searching. </li> 
   <li class="readable-text" id="p72"> <em>I/O-bound tasks</em><em> </em>—These tasks are primarily limited by input/output (I/O) operations, such as reading from or writing to a file (swapping data to a hard drive if you run out of memory), network communication, or user input. I/O-bound tasks spend most of their time waiting for I/O operations to complete rather than executing instructions on the CPU. The performance of I/O-bound tasks can be improved by using faster storage devices, optimizing I/O operations, or using asynchronous I/O to allow the program to perform other tasks while waiting for I/O operations to complete. Examples of I/O-bound tasks include reading large files, downloading data from the internet, or waiting for user input. </li> 
  </ul> 
  <div class="readable-text" id="p73"> 
   <p>Let’s see if we can get some quick solutions for resource measurement from our GPT advisor.</p> 
  </div> 
  <div class="readable-text prompt" id="p74"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>How can I quickly check if my function is CPU-bound or IO-bound, and what is its memory usage?</p> 
  </div> 
  <div class="readable-text response" id="p75"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To quickly determine if your function is CPU-bound or I/O-bound, and what is its memory usage, you can monitor the CPU usage, I/O activity and used memory while the function is running. Here’s a high-level approach to achieve this:</p> 
  </div> 
  <div class="readable-text response" id="p76"> 
   <p>Monitor CPU usage: Track the CPU usage during the execution of your function. If the CPU usage remains consistently high, it is a strong indication that your function is CPU-bound.</p> 
  </div> 
  <div class="readable-text response" id="p77"> 
   <p>Monitor I/O activity: Track I/O operations, such as disk reads/writes and network activity, during the execution of your function. If you observe high I/O activity but relatively low CPU usage, it is a strong indication that your function is I/O-bound.</p> 
  </div> 
  <div class="readable-text response" id="p78"> 
   <p>Monitor memory use: Track memory use during the execution of your function. To account only for the memory usage associated with the execution of your function, you can capture the memory usage before and after running the function and subtract the background memory usage accordingly.</p> 
  </div> 
  <div class="readable-text response" id="p79"> 
   <p>You can use built-in operating system tools or Python libraries to monitor CPU, I/O activity, and memory usage. Here’s an example using the psutil library to monitor them:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p80"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">def monitor_cpu_io_memory(duration, interval):
    cpu_percentages = []
    io_counters = []
    memory_usages = []

    end_time = time.time() + duration
    while time.time() &lt; end_time:
        cpu_percentages.append(psutil.cpu_percent(interval=interval))
        io_counters.append(psutil.disk_io_counters())
        memory_usages.append(psutil.virtual_memory().used)
        time.sleep(interval)

    return cpu_percentages, io_counters, memory_usages

<em># Monitor CPU, I/O activity, and memory usage for 10 seconds with </em>
<em><span class="">↪</span>1-second intervals</em>
cpu_percentages, io_counters, memory_usages = 
<span class="">↪</span>monitor_cpu_io_memory(duration=10, interval=1)

<em># Capture memory usage before running your_function()</em>
memory_before = psutil.virtual_memory().used

def your_function():
<em>    # Your code here</em>
    pass

<em># Run your function</em>
your_function()

<em># Capture memory usage after running your_function()</em>
memory_after = psutil.virtual_memory().used

<em># Calculate function's memory usage</em>
function_memory_usage = memory_after - memory_before

<em># Analyze CPU usage</em>
avg_cpu_usage = sum(cpu_percentages) / len(cpu_percentages)
print(f"Average CPU usage: {avg_cpu_usage}%")

<em># Analyze I/O activity</em>
read_bytes = [c.read_bytes for c in io_counters]
write_bytes = [c.write_bytes for c in io_counters]
print(f"Total read bytes: {sum(read_bytes)}")
print(f"Total write bytes: {sum(write_bytes)}")

<em># Analyze memory usage</em>
avg_memory_usage = sum(memory_usages) / len(memory_usages) - memory_before
print(f"Average memory usage during function execution: 
<span class="">↪</span>{avg_memory_usage / (1024 * 1024):.2f} MB")
print(f"Function memory usage: 
<span class="">↪</span>{function_memory_usage / (1024 * 1024):.2f} MB")</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p81"> 
   <p>Replace <code>your_function()</code> with the function you want to analyze. This script will monitor the CPU usage, I/O activity and memory usage for 10 seconds with 1-second intervals. If the average CPU usage is high, your function is likely CPU-bound. If the I/O activity (read and write bytes) is high but the CPU usage is relatively low, your function is likely I/O-bound.</p> 
  </div> 
  <div class="readable-text response" id="p82"> 
   <p>Keep in mind that this is a simple example, and you might need to adjust the monitoring duration and interval to match the runtime of your function for more accurate results. Additionally, this approach does not account for other factors that could be affecting your function’s performance, such as memory consumption or network latency.</p> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>In this chapter’s Jupyter notebook in the book’s GitHub repository, we provide the code modified for our example. These are the results we got:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p84"> 
   <div class="code-area-container"> 
    <pre class="code-area">Average CPU usage: 1.54%
Total read bytes: 2028772421120
Total write bytes: 2388311421440
Average memory usage during function execution: 2.59 MB
Function memory usage: 1.32 MB</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p85"> 
   <p>With this scale of data and simplicity of performed operations, the limiting factor is related to disk operations. In this case, we shouldn’t expect much improvement if we parallelize a calculation across multiple processors, although we will do so in the following section for illustration’s sake. Increasing CPU performance will likely have minimum impact if all the parallel tasks have to fight to read data from a single file.</p> 
  </div> 
  <div class="readable-text intended-text" id="p86"> 
   <p>Measuring where your data analysis spends most of its time and energy is critical to understanding how to make it go faster. There is no point optimizing a calculation if the limiting factor is the data transfer bandwidth—swapping a hard drive for a faster one might give you a much more significant boost, costing only a few bucks.</p> 
  </div> 
  <div class="readable-text" id="p87"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_113"><span class="num-string">7.2</span> Improving code performance</h2> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>In the context of data analytics, you may encounter two terms used interchangeably: “code optimization” and “code scaling.” These refer to two distinct concepts. They are both crucial to improving the performance and efficiency of data processing pipelines, but it’s important to distinguish between them, as they have different roles in overall performance improvement: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p89"> <em>Code optimization</em> is the process of fine-tuning the algorithm, code, or data processing pipeline to improve its performance, speed, or resource utilization. </li> 
   <li class="readable-text" id="p90"> <em>Code scaling</em> refers to adapting the code to use larger or better performing hardware more efficiently, or to adapting it to different architectures, such as GPU or multithreading. </li> 
  </ul> 
  <div class="readable-text" id="p91"> 
   <p>Let’s consider some examples. In the case of the patient-monitoring system we mentioned in section 7.1, it is likely that each patient who requires monitoring will be provided with their own unit. (Many units may connect to a single monitoring dashboard at the nurses’ station, but the data analysis will be done independently for each patient, which is what counts here.) Each unit should perform the analysis quickly and accurately, with very high availability, or even redundancy, as a human life may be at stake. Code optimization may be the best approach here, as each unit will be self-contained, will need to be mobile so it can move with the patient if they are taken for tests, and will likely already be very expensive.</p> 
  </div> 
  <div class="readable-text intended-text" id="p92"> 
   <p>By contrast, a monitoring system for financial markets will likely not require such a focus on optimizing every line of code. To preserve the maintainability and usability of such a complex system, it will be much better to ensure that the code is clearly written and documented; adequate performance will be achieved by running the system on sufficiently powerful servers capable of handling the load. Additional capacity may then be obtained by scaling—adding more CPUs or network bandwidth, or introducing processing segmentation and parallelism.</p> 
  </div> 
  <div class="readable-text intended-text" id="p93"> 
   <p>In this section, we will first try to optimize and then scale our <code>bootstrap_CI</code> function.</p> 
  </div> 
  <div class="readable-text" id="p94"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_114"><span class="num-string">7.2.1</span> Optimizing code</h3> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>You can optimize code with various techniques, such as code refactoring, algorithmic improvements, or by using more efficient data structures and libraries. If it seems daunting, there’s a good reason. As much as code and algorithm optimization is a valuable activity, it’s often difficult and time-consuming if performed manually. Reviewing each line of code by hand and trying to optimize it while avoiding introducing bugs can conceivably be done for concise and simple snippets of code. For longer ones, more automated approaches are necessary. Let’s see if GPT models can be of help here.</p> 
  </div> 
  <div class="readable-text prompt" id="p96"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Optimize this code, preferably without parallelization.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p97"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">def bootstrap_CI(data, func, n_bootstraps=1000, ci=95, axis=0):
    bootstrapped_statistics = []
    for _ in range(n_bootstraps):
        bootstrap_sample = np.random.choice(data, size=len(data), 
        <span class="">↪</span>replace=True)
        bootstrapped_statistic = func(bootstrap_sample, axis=axis)
        bootstrapped_statistics.append(bootstrapped_statistic)

    lower_bound = np.percentile(bootstrapped_statistics, (100 - ci) / 2)
    upper_bound = 
    <span class="">↪</span>np.percentile(bootstrapped_statistics, 100 - (100 - ci) / 2)

    return lower_bound, upper_bound</pre>  
   </div> 
  </div> 
  <div class="readable-text prompt" id="p98"> 
   <p>Name the resulting function <code>bootstrap_CI_optimized</code>. </p> 
  </div> 
  <div class="readable-text response" id="p99"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>You can optimize the code without parallelization by using NumPy’s vectorized operations to generate all the bootstrap samples at once. Here’s an optimized version:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p100"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import numpy as np

def bootstrap_CI_optimized(data, func, n_bootstraps=1000, ci=95, axis=0):
    data = np.asarray(data)
    idx = np.random.randint(0, len(data), size=(n_bootstraps, len(data)))
    bootstrap_samples = data[idx]

    bootstrapped_statistics = func(bootstrap_samples, axis=axis)

    lower_bound = np.percentile(bootstrapped_statistics, (100 - ci) / 2)
    upper_bound = 
    <span class="">↪</span>np.percentile(bootstrapped_statistics, 100 - (100 - ci) / 2)

    return lower_bound, upper_bound</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p101"> 
   <p>In this version, we create an index array <code>idx</code> with the shape <code>(n_bootstraps, len(data))</code> using <code>np.random.randint</code>. Then, we use advanced indexing to create the <code>bootstrap_samples</code> array, which contains all the bootstrap samples at once. Finally, we apply the <code>func</code> directly to the <code>bootstrap_samples</code> array along the specified axis. This version takes advantage of NumPy’s vectorized operations to efficiently compute the bootstrapped statistics without parallelization.</p> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>It seems that despite the simplicity of the provided code (and the fact that it was generated by ChatGPT 4 in the first place), it looks like there was still room for improvement. Or was there?</p> 
  </div> 
  <div class="readable-text intended-text" id="p103"> 
   <p>Before running this code on the biggest cluster we can find, we need to check if the proposed change indeed brought improvements. A comparison of the execution time of the basic and optimized functions is presented in figure 7.3.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p104">  
   <img alt="figure" src="../Images/CH07_F03_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.3</span> Dependence of the execution time of base (solid line) and optimized (dashed line) bootstrapping functions on the size of the processed DataFrame</h5>
  </div> 
  <div class="readable-text" id="p105"> 
   <p>Seems like code optimization did well. If you compare the scales of the two graphs, the time required for function execution dropped by a factor of almost seven. Looks like there is now much less time to drink coffee between calculation runs.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p106"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Don’t trust it further than you can throw it</h5> 
   </div> 
   <div class="readable-text" id="p107"> 
    <p>While the optimized code may look great, and we are assured by the model that it performs the same function, it is imperative to subject the code to the same thorough testing as you would for human-developed code. See chapter 8 for a more complete discussion of the risks associated with using GPT models and the phenomenon of AI delusion.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>Note that even though LLMs with very large or even theoretically infinite context sizes have been developed, it is likely that you may have to split long code into several prompts to fit into your particular model’s maximum context size. While a human expert would certainly ask for background information and a wider context, especially if questioned repeatedly about related concepts, ChatGPT will not. It’s therefore very important that you properly test the code to verify any assumptions you make about the output of the conversation with ChatGPT or the function of the final code. Also, using the iterative prompting techniques described in section 7.3.2 may be helpful in getting ChatGPT to perform correct optimizations and prevent it from hallucinating too much.</p> 
  </div> 
  <div class="readable-text" id="p109"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_115"><span class="num-string">7.2.2</span> Scaling code</h3> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>Let’s assume that we are content with how our function runs on a single processor or thread. It may now be worthwhile to look at how it can be implemented on better-performing architectures, allowing you to achieve better performance still. An obvious example would be to invest in a faster processor or more RAM. Yet, potentially surprisingly, most implementations don’t take full advantage of the architectures of commonly available hardware. Almost all processors currently available on the market have multiple cores, and GPUs are installed in virtually every PC. Both of these allow several calculations to run in parallel, albeit with some technical differences. We will discuss these a bit later, but first let’s focus on how ChatGPT can help us utilize both.</p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Scaling across multiple CPU cores</h4> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>You can easily scale your code to utilize the multicore architectures available in virtually all modern processors with available libraries and the power of ChatGPT. Let’s try to parallelize our optimized bootstrap function.</p> 
  </div> 
  <div class="readable-text prompt" id="p113"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Can you parallelize the following code?</p> 
  </div> 
  <div class="browsable-container listing-container" id="p114"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">def bootstrap_CI_optimized(data, func, n_bootstraps=1000, ci=95, axis=0):
    data = np.asarray(data)
    idx = np.random.randint(0, len(data), size=(n_bootstraps, len(data)))
    bootstrap_samples = data[idx]

    bootstrapped_statistics = func(bootstrap_samples, axis=axis)

    lower_bound = np.percentile(bootstrapped_statistics, (100 - ci) / 2)
    upper_bound = 
    <span class="">↪</span>np.percentile(bootstrapped_statistics, 100 - (100 - ci) / 2)

    return lower_bound, upper_bound

def calculate_ci_optimized(group):
    return pd.Series({
        'price_ci_lower': bootstrap_CI_optimized(group['price'], np.mean, 
<span class="">↪</span>n_bootstraps=1000, ci=95)[0],
        'price_ci_upper': bootstrap_CI_optimized(group['price'], np.mean, 
<span class="">↪</span>n_bootstraps=1000, ci=95)[1],
        'review_score_ci_lower': 
         <span class="">↪</span>bootstrap_CI_optimized(group['review_score'], np.mean, 
         <span class="">↪</span>n_bootstraps=1000, ci=95)[0],
        'review_score_ci_upper': 
         <span class="">↪</span>bootstrap_CI_optimized(group['review_score'], 
         <span class="">↪</span>np.mean, n_bootstraps=1000, ci=95)[1]
    })</pre>  
   </div> 
  </div> 
  <div class="readable-text prompt" id="p115"> 
   <p>Name the resulting function <code>bootstrap_CI_parallelized</code>.</p> 
  </div> 
  <div class="readable-text response" id="p116"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>You can parallelize <code>bootstrap_CI_optimized</code> function using the <code>concurrent .futures</code> module. I’ll demonstrate using a <code>ThreadPoolExecutor</code>. Here’s the modified version of the <code>bootstrap_CI_optimized</code> function:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p117"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import numpy as np
import pandas as pd
from concurrent.futures import ThreadPoolExecutor

def bootstrap_CI_parallelized(data, func, n_bootstraps=1000, ci=95, 
<span class="">↪</span>axis=0):
    def single_bootstrap(idx):
        bootstrap_sample = data[idx]
        return func(bootstrap_sample, axis=axis)

    data = np.asarray(data)
    idx = np.random.randint(0, len(data), size=(n_bootstraps, len(data)))

<em>    # Set the number of threads you want to use for parallelization</em>
    num_threads = 4

    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        bootstrapped_statistics = list(executor.map(single_bootstrap,
        <span class="">↪</span>idx))

    lower_bound = np.percentile(bootstrapped_statistics, (100 - ci) / 2)
    upper_bound = 
    <span class="">↪</span>np.percentile(bootstrapped_statistics, 100 - (100 - ci) / 2)

    return lower_bound, upper_bound

<em># ... (calculate_ci_optimized function definition remains the same) ...</em></pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p118"> 
   <p>This will parallelize the <code>bootstrap_CI_optimized</code> function across multiple threads. However, please note that if your data is stored in a Pandas DataFrame and you are using a custom function for <code>func</code>, you may need to handle the GIL (Global Interpreter Lock) in Python, which could limit the parallelization benefits in some cases.</p> 
  </div> 
  <div class="readable-text" id="p119"> 
   <p>Now that we have parallelized our function, it will run like a bat out of hell! Yeah! </p> 
  </div> 
  <div class="readable-text intended-text" id="p120"> 
   <p>The results of the run on 16 parallel threads are presented in figure 7.4. The code is now considerably . . . slower! Looks like ChatGPT’s final remark was true.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p121">  
   <img alt="figure" src="../Images/CH07_F04_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.4</span> Dependence of the execution time of optimized (solid line) and parallelized (dashed line) bootstrapping functions on the size of the processed DataFrame</h5>
  </div> 
  <div class="readable-text" id="p122"> 
   <p>Fortunately, there is a lesson to be learned here. In section 7.1, we distinguished between CPU-bound and I/O-bound tasks. Our <code>bootstrap_CI</code> function utilized 1.5% of the available processing power, and we mentioned that this function doesn’t seem like the best candidate for parallelization. We did it anyway, and now we’re suffering the consequences. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p123"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">There ain’t no such thing as a free lunch</h5> 
   </div> 
   <div class="readable-text" id="p124"> 
    <p>Parallelization is not free. There is an associated overhead cost related to dividing data into smaller chunks, starting and queuing the processes, and gluing the results together afterward. In our case, the main problem is the relatively minuscule size of the data, but this issue might also be important for large-scale tasks, depending on the structure of your data.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p>Our ChatGPT advisor warned us that something called the Global Interpreter Lock (GIL) might limit our parallelization benefits. GIL is a mechanism that simplifies Python’s memory management but at the cost of serious limitations on the performance of multithreaded applications and their ability to utilize multiple CPU cores efficiently.</p> 
  </div> 
  <div class="readable-text intended-text" id="p126"> 
   <p>Let’s see if moving from CPU to GPU will improve things considerably.</p> 
  </div> 
  <div class="readable-text" id="p127"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Scaling by using GPU</h4> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>With certain exceptions, scaling across CPUs may provide significant benefits, but the GPU (graphics processing unit) is another piece of hardware available on most PCs nowadays. This piece of silicone is specifically optimized for performing a vast number of identical operations on vectors or matrices of data on hundreds of cores in parallel, a characteristic required for most computer-generated graphics calculations. </p> 
  </div> 
  <div class="readable-text intended-text" id="p129"> 
   <p>There is nothing preventing GPUs being used for calculations that have similar requirements but are not related to graphics. It is important, however, to understand that a GPU and CPU are very different, as we’ll discuss in the next section.</p> 
  </div> 
  <div class="readable-text intended-text" id="p130"> 
   <p>For now, let’s see how a GPU might be used in a simple example. The following code and techniques below use an Nvidia graphics card with a CUDA interface, allowing us to direct the code to utilize an Nvidia GPU for scaling.</p> 
  </div> 
  <div class="readable-text prompt" id="p131"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Can you adjust the <code>bootstrap_CI_optimized</code> function to run with CUDA? Name the resulting function <code>bootstrap_CI_cuda</code>.</p> 
  </div> 
  <div class="readable-text response" id="p132"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To run the <code>bootstrap_CI_optimized</code> function on CUDA, you can use the cupy library, which provides GPU-accelerated versions of many NumPy functions. First, install the cupy library if you haven’t already:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p133"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install cupy</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p134"> 
   <p>Then, modify the <code>bootstrap_CI_optimized</code> function to use <code>cupy</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p135"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import cupy as cp

def bootstrap_CI_cuda(data, func, n_bootstraps=1000, ci=95, axis=0):
    data = cp.asarray(data)
    idx = cp.random.randint(0, len(data), size=(n_bootstraps, len(data)))
    bootstrap_samples = data[idx]

    bootstrapped_statistics = func(bootstrap_samples, axis=axis)

    lower_bound = cp.percentile(bootstrapped_statistics, (100 - ci) / 2)
    upper_bound = 
    <span class="">↪</span>cp.percentile(bootstrapped_statistics, 100 - (100 - ci) / 2)

    return cp.asnumpy(lower_bound), cp.asnumpy(upper_bound)</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p136"> 
   <p>Please note that the <code>func</code> argument must also be compatible with cupy arrays. If the <code>func</code> relies on NumPy functions, you may need to replace them with their equivalent cupy functions.</p> 
  </div> 
  <div class="readable-text response" id="p137"> 
   <p>Keep in mind that, depending on your problem size and GPU, you might not see significant performance improvements. GPU acceleration tends to be more beneficial for larger data and more computationally intensive operations.</p> 
  </div> 
  <div class="readable-text" id="p138"> 
   <p>ChatGPT’s final remark certainly doesn’t bring a lot of optimism, especially after our parallelization effort. Nevertheless, we need to stop delaying the inevitable and move to the results presented in figure 7.5. Fortunately, our effort this time was not in vain. We can see a significant improvement in the code execution time.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p139">  
   <img alt="figure" src="../Images/CH07_F05_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.5</span> Dependence of the execution time of optimized (solid line) and GPU-utilizing (dashed line) bootstrapping functions on the size of the processed DataFrame</h5>
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p140"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">To infinity and beyond</h5> 
   </div> 
   <div class="readable-text" id="p141"> 
    <p>GPUs are not the end of the line, at least not for all applications. Google designed hardware specialized for high-performance machine learning tasks, particularly deep learning, called Tensor Processing Units (TPUs).</p> 
   </div> 
   <div class="readable-text" id="p142"> 
    <p>TPUs are particularly well-suited for large-scale training and inference of neural networks, offering faster processing times and better energy efficiency compared to CPUs and GPUs. They can be especially beneficial for natural language processing, computer vision, and reinforcement learning applications. Incorporating TPUs into your data analytics pipeline can further optimize performance and reduce the time required to train and deploy complex machine learning models.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p143"> 
   <p>Following the piece of advice from our last conversation with ChatGPT, you should be aware that many popular Python libraries already have counterparts optimized for GPUs and TPUs (Tensor Processing Units) or provide built-in support to accelerate computations. Keep that in the back of your mind if you’re planning to utilize GPUs/TPUs in your work. </p> 
  </div> 
  <div class="readable-text intended-text" id="p144"> 
   <p>There is no need to memorize all these optimized libraries, as you can ask ChatGPT to provide you with a list, as in the following transcript. Similarly, you can ask ChatGPT to reference other implementations with specific niche uses or optimizations. It could save you hours of looking through the web.</p> 
  </div> 
  <div class="readable-text prompt" id="p145"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Which Python libraries have GPU-optimized counterparts?</p> 
  </div> 
  <div class="readable-text response" id="p146"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Many popular Python libraries have GPU-optimized counterparts or provide built-in GPU support to accelerate computations. Some of these libraries include:</p> 
  </div> 
  <ul class="response"> 
   <li class="readable-text" id="p147"> CuPy: A GPU-accelerated version of NumPy that runs on NVIDIA CUDA. It has a very similar interface to NumPy, making it easy to port existing code to CuPy. </li> 
   <li class="readable-text" id="p148"> Numba: A Just-In-Time (JIT) compiler for Python functions that can generate GPU-accelerated code for NVIDIA CUDA-enabled GPUs. </li> 
  </ul> 
  <div class="readable-text response" id="p149"> 
   <p>SciPy:</p> 
  </div> 
  <ul class="response"> 
   <li class="readable-text" id="p150"> CuPy: CuPy also provides GPU-accelerated versions of some SciPy functions, such as sparse matrix operations and linear algebra routines. </li> 
   <li class="readable-text" id="p151"> ArrayFire: A high-performance library that provides GPU-optimized versions of some SciPy functions for NVIDIA, AMD, and Intel GPUs. </li> 
  </ul> 
  <div class="readable-text response" id="p152"> 
   <p>Pandas:</p> 
  </div> 
  <ul class="response"> 
   <li class="readable-text" id="p153"> cuDF: A part of RAPIDS, cuDF is a GPU-accelerated library for data manipulation and analysis, providing a pandas-like API for handling data on NVIDIA CUDA-enabled GPUs. </li> 
   <li class="readable-text" id="p154"> Dask: While not strictly a GPU-optimized counterpart, Dask can work with cuDF to parallelize and distribute data processing tasks on GPUs. </li> 
  </ul> 
  <div class="readable-text response" id="p155"> 
   <p>Scikit-learn:</p> 
  </div> 
  <ul class="response"> 
   <li class="readable-text" id="p156"> cuML: Also part of RAPIDS, cuML is a GPU-accelerated library for machine learning that provides an API compatible with scikit-learn. It supports various algorithms like linear regression, k-means clustering, and principal component analysis, optimized for NVIDIA CUDA-enabled GPUs. </li> 
   <li class="readable-text" id="p157"> Dask-ML: Dask-ML extends Dask to parallelize machine learning algorithms, including support for GPU-accelerated libraries like CuPy and cuDF. </li> 
  </ul> 
  <div class="readable-text response" id="p158"> 
   <p>TensorFlow and PyTorch:</p> 
  </div> 
  <ul class="response"> 
   <li class="readable-text" id="p159"> Both TensorFlow and PyTorch are popular deep learning libraries that provide built-in support for GPU acceleration using NVIDIA CUDA and cuDNN libraries. </li> 
  </ul> 
  <div class="readable-text response" id="p160"> 
   <p>JAX:</p> 
  </div> 
  <ul class="response"> 
   <li class="readable-text" id="p161"> JAX is a library for composable transformations of Python+NumPy programs, including automatic differentiation, vectorization, and parallelization. It has built-in support for GPU acceleration using NVIDIA CUDA and also supports TPUs. </li> 
  </ul> 
  <div class="readable-text" id="p162"> 
   <p>These are just a few examples of Python libraries with optimized counterparts or built-in GPU/TPU support. Depending on your specific use case and hardware, you might find other libraries that can leverage specialized chips for accelerated computation.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p163"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">ChatGPT learning cutoff</h5> 
   </div> 
   <div class="readable-text" id="p164"> 
    <p>As stated several times before, ChatGPT has limited knowledge beyond a certain point, depending on the version, so its knowledge of the latest developments and most recent versions of libraries may be limited. Nevertheless, it should provide a good starting point for further exploration.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p165"> 
   <p>Keep in mind that GPU/TPU optimizations can have varying levels of performance improvements, so it’s essential to profile and test your code on the target hardware to ensure that you’re getting the desired performance gains.</p> 
  </div> 
  <div class="readable-text" id="p166"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Scaling across CPUs vs. GPUs</h4> 
  </div> 
  <div class="readable-text" id="p167"> 
   <p>The decision to scale operations across CPUs or GPUs depends on the task’s nature and the available hardware resources. Here’s a general guideline for when to use CPUs and GPUs for different types of operations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p168"> 
   <p>Use CPUs for</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p169"> Sequential tasks or tasks that involve branching or complex control structures, or that require a limited amount of parallelism </li> 
   <li class="readable-text" id="p170"> Tasks that have high I/O requirements or involve extensive communication between processes or threads </li> 
   <li class="readable-text" id="p171"> Tasks with a small or moderate dataset size, as, even though the available memory on CPUs is typically larger than that on GPUs, processing of large datasets on CPUs may be less effective than on GPUs. </li> 
   <li class="readable-text" id="p172"> General-purpose tasks, as CPUs are designed to handle a wide variety of workloads </li> 
  </ul> 
  <div class="readable-text" id="p173"> 
   <p>Use GPUs for</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p174"> Highly parallel tasks that can be divided into thousands of smaller tasks to be executed simultaneously </li> 
   <li class="readable-text" id="p175"> Tasks involving many arithmetic operations, such as matrix multiplications, convolutions, or element-wise operations on large datasets </li> 
   <li class="readable-text" id="p176"> Tasks with large dataset sizes, as GPUs are designed to handle massive amounts of data quickly due to their high memory bandwidth </li> 
   <li class="readable-text" id="p177"> Deep learning, computer vision, or scientific simulations, which can benefit from the specialized architecture of GPUs </li> 
  </ul> 
  <div class="readable-text" id="p178"> 
   <p>In general, if the task can be broken down into many simpler, preferably similar, subtasks, and requires a large amount of computation, it’s likely better suited for a GPU. However, if the task involves complex control structures, extensive communication, or is I/O-bound, it’s likely better suited for a CPU. Additionally, you need to consider the available hardware resources, the efficiency of the libraries being used, and your task’s specific requirements before deciding whether to use CPUs or GPUs for your operations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p179"> 
   <p>There is a very useful heuristic when deciding which approach to use. Unless you are prepared to rewrite large portions of the code to include parallelism, consider which libraries you use in your original code and whether they already have parallelized versions. If you are a heavy user of libraries with GPU-optimized counterparts and have the relevant GPU hardware available, it’s usually best to go with those. Otherwise, consider sticking with CPU parallelization.</p> 
  </div> 
  <div class="readable-text intended-text" id="p180"> 
   <p>You can also combine both approaches, switching to using GPU-optimized libraries in the sections of the code most involved with calling these and then parallelizing the rest of the code using multiple CPUs. Be careful, though. As we showed in section 7.2.2, any parallelization comes with overhead, especially when communication or synchronization between different CPU cores is involved. You may end up with overheads far exceeding any benefits.</p> 
  </div> 
  <div class="readable-text intended-text" id="p181"> 
   <p>When you run out of PC cores, you may need to turn to cloud solutions. Those benefits come at the price of some extra work on your part to deploy your solution to your friendly cloud provider.</p> 
  </div> 
  <div class="readable-text" id="p182"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_116"><span class="num-string">7.3</span> Cloud-based deployment</h2> 
  </div> 
  <div class="readable-text" id="p183"> 
   <p>As data continues to grow in volume, variety, and complexity, it may exceed the computing power of your PC machine. Instead of waiting unreasonably long to see the output (or a memory overflow error) or splashing out on the latest overpriced chips when they come into stock after being sold out to scalpers, you can reach into the vast power of cloud computing.</p> 
  </div> 
  <div class="readable-text" id="p184"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_117"><span class="num-string">7.3.1</span> What is cloud computing?</h3> 
  </div> 
  <div class="readable-text" id="p185"> 
   <p>Cloud computing is a technology model that enables users to access and utilize computing resources, such as storage, processing power, and software applications, over the internet. Instead of investing in and maintaining physical hardware and software on-premises, users can leverage cloud service providers. The providers manage and maintain the infrastructure in large, remote data centers, providing on-demand access to a wide range of computing resources, storage, and managed services. You can scale your workload up or down as needed and analyze massive amounts of data without investing in expensive hardware.</p> 
  </div> 
  <div class="readable-text intended-text" id="p186"> 
   <p>The main advantages of cloud computing include scalability, flexibility, cost-efficiency, and reliability. You can also choose to use exotic architectures like TPUs (or GPUs) from the safe confines of your laptop.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p187"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">1+1 ≠ 2</h5> 
   </div> 
   <div class="readable-text" id="p188"> 
    <p>Multiple interconnected computers, known as nodes, working together as a single unified system to perform complex computational tasks are referred to as <em>clusters</em>.</p> 
   </div> 
   <div class="readable-text" id="p189"> 
    <p> In cluster computing, tasks are often divided into smaller subtasks and distributed across the nodes in the cluster. The nodes then work in parallel, processing their assigned subtasks simultaneously, with specific processes providing synchronization and fault-tolerance to the whole system. Since this is a complex topic, we won’t be delving deeper into these issues, assuming instead that users will be more likely to utilize one of the established frameworks that provide solutions to these issues than write their own.</p> 
   </div> 
   <div class="readable-text" id="p190"> 
    <p>Common cluster computing software frameworks include Apache Hadoop, Apache Spark, and the Message Passing Interface (MPI), which provide tools and libraries to simplify the development, deployment, and management of distributed applications across cluster nodes.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p191"> 
   <p>The code we used as an example to compare performance didn’t turn out to be the best candidate for parallelization. In this section, we will work with a function calculating straight-line (or Euclidean) distances between sets of points. It may not be very useful for most business cases, but it parallelizes beautifully—each pair’s distance can be calculated independently, so no communication between processors or nodes will be required, apart from gathering the final results. </p> 
  </div> 
  <div class="readable-text" id="p192"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_118"><span class="num-string">7.3.2</span> Moving your code to the cloud</h3> 
  </div> 
  <div class="readable-text" id="p193"> 
   <p>The simplest cloud computing use case is to deploy your code to a virtual machine (VM) in the cloud. The VM is like a computer within a computer, with its own operating system and all. It acts as an isolated space, enabling you to test software, run different applications, or work in separate environments without affecting the main system. You run your analytics on a VM just as you would on your local PC. ChatGPT can provide you with complete instructions:</p> 
  </div> 
  <div class="readable-text prompt" id="p194"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>How can I deploy my calculation to the cloud? Provide me with a walkthrough.</p> 
  </div> 
  <div class="readable-text" id="p195"> 
   <p>Since the exact steps change frequently, we’ve decided to omit the lengthy transcript here. It should suffice to say that ChatGPT can provide detailed walkthroughs, but only up to the point where the provider changes the interface and the steps differ. Caveat lector (reader beware). </p> 
  </div> 
  <div class="readable-text response" id="p196"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Depending on your specific use case and requirements, you might need to customize the environment, configure additional resources, or use other cloud services, such as managed data processing services or serverless computing options like Google Cloud Functions or AWS Lambda.</p> 
  </div> 
  <div class="readable-text" id="p197"> 
   <p>You can ask ChatGPT about the details of each of these steps.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p198"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">ChatGPT’s narrow context </h5> 
   </div> 
   <div class="readable-text" id="p199"> 
    <p>You’ll notice that, in the discussion with ChatGPT on the deployment of code to the cloud, the response only provided a way to run the code on a single virtual machine, since the prompt didn’t specify anything otherwise. This kind of deployment is usually the first step in moving your application to the cloud, so it’s still worth looking at, although this will usually provide only a moderate increase in computational power, limited by the power of a single VM, while having the significant benefit of requiring no changes to the code.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p200"> 
   <p>Finally, let’s look at how ChatGPT can help us take our code into the cloud, this time properly distributing it across multiple cloud resources so that we can harness the vast power of cluster computing. Let’s look at the solution proposed by ChatGPT:</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p201"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">ChatGPT is not loyal to one platform</h5> 
   </div> 
   <div class="readable-text" id="p202"> 
    <p>In the responses in this section, you’ll see ChatGPT starting to answer using Amazon AWS. The prompt did not specify the choice of the cloud provider, although we have discussed using Google Cloud before. Don’t be afraid to stop ChatGPT if you don’t like where it is going and redirect it on a path you prefer by refining the prompt.</p> 
   </div> 
  </div> 
  <div class="readable-text prompt" id="p203"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>I now want to distribute the calculation across the cloud. How can I do that? (Forget saving to disk.)</p> 
  </div> 
  <div class="readable-text response" id="p204"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To distribute the calculation across the cloud, you can use Dask along with a cloud provider like Amazon Web Services (AWS), Google Cloud Platform (GCP), or Microsoft Azure. In this example, we’ll use Dask with AWS, but the process is similar for other cloud providers.</p> 
  </div> 
  <div class="readable-text response" id="p205"> 
   <p>First, you’ll need to set up a Dask cluster on AWS. You can follow the official Dask guide for setting up a Dask cluster on AWS using the dask-cloudprovider package.</p> 
  </div> 
  <div class="readable-text response" id="p206"> 
   <p>Once you’ve set up your Dask cluster</p> 
  </div> 
  <div class="readable-text" id="p207"> 
   <p>We stopped the response generation at this point.</p> 
  </div> 
  <div class="readable-text prompt" id="p208"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Please change the last answer to use GCP</p> 
  </div> 
  <div class="readable-text response" id="p209"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To distribute the calculation across the cloud using Google Cloud Platform (GCP), you can use Dask along with the dask-cloudprovider package to create and manage Dask clusters on GCP.</p> 
  </div> 
  <div class="readable-text response" id="p210"> 
   <p>&lt;Here, we again received an answer applicable at the time of ChatGPTs knowledge cutoff.&gt;</p> 
  </div> 
  <div class="readable-text" id="p211"> 
   <p>The rest of the code is similar to the previous examples but now uses GCP cluster as the underlying infrastructure. The key point here is that there are ready-made solutions, as mentioned in the previous sections, which can abstract away the technicalities of cloud or cluster computing and allow you to focus on coding the core problem rather than worrying whether the network communication is going to work correctly. Frameworks like Dask can provide these solutions and even allow you to switch almost seamlessly between different low-level providers like AWS or GCP.</p> 
  </div> 
  <div class="readable-text intended-text" id="p212"> 
   <p>It’s important to remember that by submitting code and prompts requesting different iterations of code, we may end up with an implementation containing a mixture of the different solutions, or we may end up with totally different functionality. Whatever modifications we request ChatGPT to make to the code, it’s vital that it is retested with the same, or even higher, scrutiny as if it were written from scratch. Furthermore, since parallelized or distributed code is usually much harder to test fully than a localized single-threaded version, care should be taken to apply best practices, which are beyond the scope of this book.</p> 
  </div> 
  <div class="readable-text" id="p213"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_119"><span class="num-string">7.4</span> Code conversion</h2> 
  </div> 
  <div class="readable-text" id="p214"> 
   <p>Finally, let’s look at using ChatGPT to convert code between programming languages. This can serve multiple purposes, such as generating optimized code or adapting code to work within specific frameworks like Hadoop. When translating code from one language to another, developers have the opportunity to refactor, streamline, and improve the code in the target language. It can lead to optimization in several ways. Let’s look at some of these:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p215"> <em>Improved performance</em><em> </em><em>—</em>Converting code to a more efficient language or one that offers better support for specific hardware or platforms can lead to performance gains. For example, translating code from a high-level interpreted language like Python to a lower-level compiled language like C++ can result in faster execution times. </li> 
   <li class="readable-text" id="p216"> <em>Reduced memory usage</em><em> </em><em>—</em>Code conversion can lead to more memory-efficient implementations. For instance, certain data structures and algorithms in the target language might consume less memory than their counterparts in the source language. </li> 
   <li class="readable-text" id="p217"> <em>Better maintainability</em><em> </em><em>—</em>Converting code to a more modern or widely used language can improve maintainability by leveraging newer language features, libraries, and community support. This can make the code easier to read, understand, and modify in the long run. </li> 
   <li class="readable-text" id="p218"> <em>Framework compatibility</em><em> </em><em>—</em>Ensuring code compatibility with different frameworks often requires converting it to a supported language. For instance, in the next section, we’ll be discussing distributing the processing to the cloud, and one of the options would be to use Hadoop. However, when working with Hadoop, you might need to convert a Python script to Java or Scala to utilize the framework’s ecosystem efficiently. This ensures seamless integration with Hadoop’s Java-based libraries and tools. </li> 
  </ul> 
  <div class="readable-text" id="p219"> 
   <p>As the frameworks and compatibility restrictions change fairly often, it is best to check the current documentation online to make informed decisions regarding which framework is the best option for your specific implementation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p220"> 
   <p>It’s essential to carefully consider the cost and complexity of code conversion, as it can be time-consuming and may introduce new bugs or issues, and manual code conversion is rarely worth the effort. Although automated tools for code conversion have existed before, the use of models like ChatGPT has made code conversion much easier and may actually bring sizable benefits even with significant chunks of code. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p221"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Limited trust still applies</h5> 
   </div> 
   <div class="readable-text" id="p222"> 
    <p>Please remember that even though it may look like the code conversion is easy, thorough testing and performance monitoring are still necessary to ensure the translated code works as expected and meets the desired optimization goals. Techniques linked to unit testing, system testing, and integration testing, which are beyond the scope of this book, should be applied both before and after changing the code to reduce the risk of bugs creeping in.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p223"> 
   <p>Let’s look at some examples of code conversion using ChatGPT. Imagine you wrote the code in Python for one of your analyses but are now looking to reuse the same concept in a different part of the organization that has everything else written in Java. Although there are solutions for interoperability between the two languages, converting the code directly to Java might be the simplest solution.</p> 
  </div> 
  <div class="readable-text prompt" id="p224"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Take this code and convert it into Java: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p225"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">def bootstrap_CI_optimized(data, func, n_bootstraps=1000, ci=95, axis=0):
    data = np.asarray(data)
    idx = np.random.randint(0, len(data), size=(n_bootstraps, len(data)))
    bootstrap_samples = data[idx]

    bootstrapped_statistics = func(bootstrap_samples, axis=axis)

    lower_bound = np.percentile(bootstrapped_statistics, (100 - ci) / 2)
    upper_bound = 
    <span class="">↪</span>np.percentile(bootstrapped_statistics, 100 - (100 - ci) / 2)

    return lower_bound, upper_bound</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p226"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Here’s the equivalent Java code for the provided Python function:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p227"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import org.apache.commons.math3.random.RandomDataGenerator;
import org.apache.commons.math3.stat.descriptive.rank.Percentile;
import org.apache.commons.math3.stat.descriptive.rank.Percentile
<span class="">↪</span>.EstimationType;

public class BootstrapUtils {
    public static double[] bootstrapCI(double[] data, int nBootstraps, 
    <span class="">↪</span>int ci) {
        RandomDataGenerator randomDataGenerator = new 
        <span class="">↪</span>RandomDataGenerator();
        Percentile percentile = new 
        <span class="">↪</span>Percentile().withEstimationType(EstimationType.R_7);

        double[] bootstrapStatistics = new double[nBootstraps];
        int dataSize = data.length;

        for (int i = 0; i &lt; nBootstraps; i++) {
            double[] bootstrapSample = new double[dataSize];
            for (int j = 0; j &lt; dataSize; j++) {
                int randomIndex = 
                <span class="">↪</span>randomDataGenerator.nextInt(0, dataSize - 1);
                bootstrapSample[j] = data[randomIndex];
            }
            bootstrapStatistics[i] = calculateStatistics(bootstrapSample);
        }

        double lowerBound = 
        <span class="">↪</span>percentile.evaluate(bootstrapStatistics, (100 - ci) / 2);
        double upperBound = 
        <span class="">↪</span>percentile.evaluate(bootstrapStatistics, 100 - (100 - ci) / 2);

        return new double[]{lowerBound, upperBound};
    }

    private static double calculateStatistics(double[] data) {
        // Replace this with your custom statistics calculation logic
        // For example, you can use Apache Commons Math library for mean 
        <span class="">↪</span>calculation
        return org.apache.commons.math3.stat.StatUtils.mean(data);
    }

    public static void main(String[] args) {
        // Example usage
        double[] data = { /* Your data here */ };
        double[] ci = bootstrapCI(data, 1000, 95);

        // Use ci as needed
    }
}</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p228"> 
   <p>This code uses the Apache Commons Math library for the mean calculation, replacing the custom mean calculation from the previous example. Adjust the <code>calculateStatistics</code> method if you have a different custom statistics calculation logic.</p> 
  </div> 
  <div class="readable-text" id="p229"> 
   <p>It may be interesting at this point to investigate how much ChatGPT “understands” the code it is converting. It’s interesting to see how ChatGPT might be able to conceptualize the function of the code, which may lead to more efficient conversion with the right prompting.</p> 
  </div> 
  <div class="readable-text prompt" id="p230"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>What does this code do?</p> 
  </div> 
  <div class="readable-text response" id="p231"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>This code defines a Java utility class, <code>BootstrapUtils</code>, that provides a method <code>bootstrapCI</code> for calculating bootstrap confidence intervals (CI) for a given dataset. Let’s break down the code:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text buletless-item" id="p232"> Imports: 
    <ol style="list-style: lower-alpha"> 
     <li> The code imports classes from the Apache Commons Math library for statistical calculations, including random data generation and percentile calculation. </li> 
    </ol></li> 
   <li class="readable-text" id="p233"> <code>BootstrapUtils</code> Class: 
    <ol style="list-style: lower-alpha"> 
     <li> Contains two methods: <code>bootstrapCI</code> and <code>calculateStatistics</code>. </li> 
    </ol></li> 
   <li class="readable-text" id="p234"> <code>bootstrapCI</code> Method: 
    <ol style="list-style: lower-alpha"> 
     <li> Takes three parameters: an array of data, the number of bootstrap samples (<code>nBootstraps</code>), and the confidence interval percentage (<code>ci</code>). </li> 
     <li> Creates a random data generator and a <code>Percentile</code> object with a specific estimation type. </li> 
     <li> Initializes an array to store bootstrap statistics and gets the size of the input data. </li> 
    </ol></li> 
   <li class="readable-text buletless-item" id="p235"> Bootstrap Sampling: 
    <ol style="list-style: lower-alpha"> 
     <li> Iterates <code>nBootstraps</code> times. </li> 
     <li> For each iteration, it creates a bootstrap sample by randomly selecting data points from the original dataset. </li> 
    </ol></li> 
   <li class="readable-text buletless-item" id="p236"> Statistics Calculation: 
    <ol style="list-style: lower-alpha"> 
     <li> Calls the <code>calculateStatistics</code> method to calculate the statistics of each bootstrap sample. The default implementation uses the mean calculation from the Apache Commons Math library. </li> 
    </ol></li> 
   <li class="readable-text buletless-item" id="p237"> Confidence Interval Calculation: 
    <ol style="list-style: lower-alpha"> 
     <li> Uses the <code>Percentile</code> object to calculate the lower and upper bounds of the confidence interval based on the percentiles of the bootstrapped statistics. </li> 
    </ol></li> 
   <li class="readable-text buletless-item" id="p238"> Return: 
    <ol style="list-style: lower-alpha"> 
     <li> Returns an array containing the lower and upper bounds of the bootstrap confidence interval. </li> 
    </ol></li> 
   <li class="readable-text" id="p239"> <code>calculateStatistics</code> Method: 
    <ol style="list-style: lower-alpha"> 
     <li> A private method for calculating statistics on a given array. The default implementation uses the mean calculation from the Apache Commons Math library. </li> 
    </ol></li> 
   <li class="readable-text" id="p240"> <code>main</code> Method (Example Usage): 
    <ol style="list-style: lower-alpha"> 
     <li> Demonstrates how to use the <code>bootstrapCI</code> method with an example dataset (<code>data</code>). It calculates the bootstrap confidence interval with 1000 bootstraps and a 95% confidence interval. </li> 
    </ol></li> 
  </ol> 
  <div class="readable-text response" id="p241"> 
   <p>In summary, this Java code provides a flexible and reusable utility for calculating bootstrap confidence intervals. You can replace the <code>calculateStatistics</code> method with your custom statistics calculation logic if needed. The code is well-organized and modular, making it easy to understand and extend for different statistical calculations.</p> 
  </div> 
  <div class="readable-text" id="p242"> 
   <p>Now that we’ve extracted the function of the code from ChatGPT, we can ask it for a more optimal solution in our target language. Please note that the prompt asking ChatGPT for the description of the function of the code is important in the flow of the conversation, as it steers ChatGPT toward the correct understanding of our intention to convert the function of the code rather than translate it instruction-by-instruction. Furthermore, it allows us to check if ChatGPT’s understanding of the function is correct. If it were not correct, we might adapt subsequent prompts to steer it toward the right output. In our case, it deduced the function correctly, so we can just ask it for an optimized solution in the next prompt.</p> 
  </div> 
  <div class="readable-text prompt" id="p243"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Can this be done using some optimized implementation?</p> 
  </div> 
  <div class="readable-text response" id="p244"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Certainly! If you’re looking for optimized implementations, Java has libraries that provide efficient statistical calculations. One such library is the Apache Commons Math library. Here’s the modified code using Apache Commons Math for both bootstrap sampling and statistics calculation:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p245"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import org.apache.commons.math3.random.RandomDataGenerator;
import org.apache.commons.math3.stat.descriptive.rank.Percentile;
import org.apache.commons.math3.stat.descriptive.rank.Percentile
<span class="">↪</span>.EstimationType;

public class BootstrapUtils {
    public static double[] bootstrapCI(double[] data, int nBootstraps, 
    <span class="">↪</span>int ci) {
        RandomDataGenerator randomDataGenerator = 
        <span class="">↪</span>new RandomDataGenerator();
        Percentile percentile = 
        <span class="">↪</span>new Percentile().withEstimationType(EstimationType.R_7);

        double[] bootstrapStatistics = new double[nBootstraps];
        int dataSize = data.length;

        for (int i = 0; i &lt; nBootstraps; i++) {
            double[] bootstrapSample = new double[dataSize];
            for (int j = 0; j &lt; dataSize; j++) {
                int randomIndex = 
                <span class="">↪</span>randomDataGenerator.nextInt(0, dataSize - 1);
                bootstrapSample[j] = data[randomIndex];
            }
            bootstrapStatistics[i] = 
          <span class="">↪</span>org.apache.commons.math3.stat.StatUtils.mean(bootstrapSample);
        }

        double lowerBound = 
        <span class="">↪</span>percentile.evaluate(bootstrapStatistics, (100 - ci) / 2);
        double upperBound = 
        <span class="">↪</span>percentile.evaluate(bootstrapStatistics, 100 - (100 - ci) / 2);

        return new double[]{lowerBound, upperBound};
    }

    public static void main(String[] args) {
        // Example usage
        double[] data = { /* Your data here */ };
        double[] ci = bootstrapCI(data, 1000, 95);

        // Use ci as needed
    }
}</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p246"> 
   <p>This version uses <code>org.apache.commons.math3.stat.StatUtils.mean</code> for the mean calculation, which is an optimized implementation from Apache Commons Math. Make sure to include the Apache Commons Math library in your project dependencies.</p> 
  </div> 
  <div class="readable-text" id="p247"> 
   <p>In the preceding exchange, ChatGPT has translated the code directly. Further interrogation provided even better alternatives, such as by asking ChatGPT to replace parts of the code with optimized implementations from other libraries available for Java, although it’s up to you to explore this. Don’t leave it to the machine to do <em>all</em> the work.</p> 
  </div> 
  <div class="readable-text intended-text" id="p248"> 
   <p>By this point, we trust you’ve gained an understanding of how you can use generative AI to choose, implement, and assess various approaches to enhance the efficiency of the code employed in your analysis. However, it’s crucial to exercise discretion in choosing your optimization battles. If your analysis is a single, ad hoc occurrence, the time required to achieve optimal performance might surpass the potential time savings.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p249"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Things to ask ChatGPT</h5> 
   </div> 
   <ul> 
    <li class="readable-text" id="p250"> Modify my code to run on multiple GPUs/CPUs. </li> 
    <li class="readable-text" id="p251"> Convert this snippet I downloaded from StackOverflow to my favorite programming language. </li> 
    <li class="readable-text" id="p252"> Explain what this code snippet does. </li> 
    <li class="readable-text" id="p253"> Modify my code to run on the Amazon/Google/Microsoft Azure cloud. </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p254"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_120">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p255"> Don’t try to re-implement common algorithms, as there are usually optimized implementations available. </li> 
   <li class="readable-text" id="p256"> If you want to use a different programming language, ChatGPT can significantly simplify migration. Just remember to retest the code thoroughly. </li> 
   <li class="readable-text" id="p257"> Code should be optimized to achieve results as quickly as possible on a given platform. If this is not fast enough, you can scale up by using multiple CPUs or GPUs or deploying to the cloud. </li> 
   <li class="readable-text" id="p258"> Frameworks like Dask can help you achieve scaling with minimum modifications to your code. </li> 
  </ul>
 </body></html>