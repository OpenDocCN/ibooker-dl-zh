<html><head></head><body><section data-pdf-bookmark="Chapter 11. Trust the Process" data-type="chapter" epub:type="chapter"><div class="chapter" id="trust_the_process">
      <h1><span class="label">Chapter 11. </span>Trust the Process</h1>
      <blockquote data-type="epigraph" epub:type="epigraph">
        <p>If you can’t describe what you are doing as a process, you don’t know what you’re doing.</p>
        <p data-type="attribution">W. Edwards Deming</p>
      </blockquote>
      <p>We’ve spent most of this book exploring the dangers of applying LLM technology in production. While there is great power in technology, there are many risks. Security, privacy, financial, legal, and reputational risks seem to be around every corner. With that understanding, how can you move forward with confidence? It’s time to talk about actionable, durable, repeatable solutions. While we’ve discussed practical mitigation strategies for each risk, tackling them individually as a patchwork isn’t likely to cut it. You must build security into your development process to ensure your success.</p>
      <p>This chapter will discuss two process elements that have emerged as key ingredients in successful projects. First, we’ll discuss the evolution of the DevSecOps movement and how it’s become central to application security for any large software project. We will examine how it has evolved to encompass specific challenges with AI/ML and LLMs. As part of this discussion, we’ll look at development-time tools to scan for security vulnerabilities and runtime tools (known as guardrails) that can help protect your LLM in production.</p>
      <p>We’ll also look at how security testing has evolved and the emerging field of AI red teaming. Red teams have been around for a long time in cybersecurity circles, but AI red teaming has recently gained more prominence as specific techniques have evolved that apply to LLM projects.</p>
      <section data-pdf-bookmark="The Evolution of DevSecOps" data-type="sect1"><div class="sect1" id="the_evolution_of_devsecops">
        <h1>The Evolution of DevSecOps</h1>
        <p><a contenteditable="false" data-primary="DevSecOps" data-type="indexterm" id="ch11.html0"/>The <a contenteditable="false" data-primary="DevOps, origins of" data-type="indexterm" id="id587"/>origin of <em>DevOps</em> can be traced back to the early 2000s when it emerged in response to the growing need for better collaboration and integration between software development (Dev) and IT operations (Ops) teams. This need arose from the limitations observed in traditional software development methodologies, which often led to siloed teams, delayed releases, and a need for more alignment between development objectives and operational stability. The DevOps movement aimed to bridge this gap by promoting a culture of collaboration, automation, continuous integration, and continuous delivery (CI/CD), thereby enhancing the speed and quality of software deployment.</p>
        <p>As DevOps practices matured and became more widely adopted, the critical need to integrate security principles into the development lifecycle became increasingly apparent. This realization led to integrating security (Sec) into the DevOps process, giving us <em>DevSecOps</em>. DevSecOps enriches DevOps practices by embedding security at every phase of the software development process, from design to deployment. The goal is to ensure that security considerations are not an afterthought but are integrated into the workflow, enabling the early discovery and mitigation of vulnerabilities, thus building more secure software.</p>
        <p>We want to enable this same proactive security stance in the development and deployment of applications using LLMs. To do so, the principles of DevOps and DevSecOps have further inspired the emergence of <em>MLOps</em> and <em>LLMOps</em> to address the unique challenges and requirements of deploying and managing AI/ML systems. </p>
        <p>MLOps (machine learning operations) focuses on automating and optimizing the machine learning lifecycle (including data preparation, model training, deployment, and observability) to ensure consistent and efficient ML model development and maintenance. <a contenteditable="false" data-primary="LLMOps (large language model operations)" data-type="indexterm" id="id588"/>LLMOps (large language model operations) explicitly addresses the operational needs of large language models, focusing on aspects such as prompt engineering, model fine-tuning, and RAG. These specialized practices demonstrate the ongoing expansion of the DevOps philosophy, which has adapted to encompass emerging technologies’ operational and security needs, thus ensuring their effective integration into the broader software development and deployment ecosystem. Using concepts from both MLOps and LLMOps will help you extend your organization’s DevSecOps process to account for the specific needs of adding advanced AI technology to your stack.</p>
        <section data-pdf-bookmark="MLOps" data-type="sect2"><div class="sect2" id="mlops_id8S3Yrz">
          <h2>MLOps</h2>
          <p><a contenteditable="false" data-primary="DevSecOps" data-secondary="MLOps" data-type="indexterm" id="id589"/><a contenteditable="false" data-primary="MLOps" data-type="indexterm" id="id590"/>MLOps is a set of best practices that aims to streamline and automate the machine learning lifecycle, from data preparation and model development to deployment and monitoring. Key elements of MLOps include version control for both models and data, ensuring reproducibility and traceability, model training, and validation for selecting the best model candidates. </p>
          <p>CI/CD pipelines are tailored for ML workflows to automate the testing and deployment of models and for monitoring model performance in production to catch and address model degradation due to model or data drift over time. Additionally, MLOps emphasizes collaboration between data scientists, ML engineers, and operations teams to facilitate a more efficient and seamless development process, ensuring accurate, scalable, and maintainable ML models.</p>
          <p>MLOps infrastructure plays a critical role in the security landscape of machine learning systems. By integrating security practices throughout the ML lifecycle, MLOps can help identify and mitigate risks early in development. This includes ensuring data privacy and compliance with regulations such as GDPR, managing access to sensitive datasets, and securing model endpoints against adversarial attacks. Automated vulnerability scanning and incorporating security checks into CI/CD pipelines help catch security issues before deployment. Moreover, monitoring deployed models for anomalous behavior can detect potential security breaches, contributing to a more robust security posture for ML applications.</p>
        </div></section>
        <section data-pdf-bookmark="LLMOps" data-type="sect2"><div class="sect2" id="llmops_id8l4ZbH">
          <h2>LLMOps</h2>
          <p><a contenteditable="false" data-primary="DevSecOps" data-secondary="LLMOps" data-type="indexterm" id="id591"/><a contenteditable="false" data-primary="LLMOps (large language model operations)" data-secondary="basics" data-type="indexterm" id="id592"/>MLOps, while crucial in establishing practices for any application leveraging machine learning, doesn’t address all the unique challenges LLMs pose. LLMs introduce specific challenges, such as prompt engineering, robust monitoring to capture the nuanced performance, and the potential misuse of generated outputs. This means we must take advantage of the best that DevSecOps and MLOps can teach us and then add more techniques specific to LLMs.</p>
          <p>LLMOps evolved as a specialized discipline to address these challenges. It encompasses practices tailored for deploying, monitoring, and maintaining LLMs in production environments. LLMOps deals with aspects such as model versioning and management at a much larger scale, advanced deployment strategies to handle the high computational load, and specific monitoring techniques for evaluating the qualitative aspects of model outputs. Furthermore, LLMOps emphasizes the importance of prompt engineering and feedback loops to refine model performance and mitigate risks associated with model-generated content. This specialized focus ensures that LLM deployments are efficient, ethical, and aligned with user expectations and regulatory requirements.</p>
          <p>Now, let’s examine how best to integrate security practices into LLMOps to ensure a repeatable process for delivering more secure applications.<a contenteditable="false" data-primary="" data-startref="ch11.html0" data-type="indexterm" id="id593"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Building Security into LLMOps " data-type="sect1"><div class="sect1" id="building_security_into_llmops">
        <h1>Building Security into LLMOps </h1>
        <p><a contenteditable="false" data-primary="LLMOps (large language model operations)" data-secondary="building security into" data-type="indexterm" id="id594"/>All this discussion about DevSecOps, MLOps, and LLMOps may sound daunting. However, the critical tasks required to secure our process for building secure LLM apps can be broken down into five simple steps: foundation model selection, data preparation, validation, deployment, and monitoring, as shown in <a data-type="xref" href="#table-11-1">Table 11-1</a>.</p>
        <table id="table-11-1">
          <caption><span class="label">Table 11-1. </span>LLMOps steps</caption>
          <thead>
            <tr>
              <th>Task</th>
              <th>LLMOps security measures</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Foundation model selection</td>
              <td>Opt for foundation models with robust security features. Assess the security history and vulnerability reports of the model’s source. Review the model card provided with the foundation model and the security-specific information provided. Review what you can about the datasets used to train the foundation model. Implement processes to watch for new versions of the foundation model, which may add security or alignment improvements.</td>
            </tr>
            <tr>
              <td>Data preparation</td>
              <td>If you plan to use fine-tuning or RAG to enhance the domain-specific knowledge available to your application, you must prepare your data. Carefully evaluate the sources of your datasets. Ensure data is scrubbed, anonymized, and free from illegal or inappropriate content. Evaluate your data for possible bias. Implement secure data handling and access controls during fine-tuning or embedding generation. </td>
            </tr>
            <tr>
              <td>Validation</td>
              <td>Extend your security testing to include LLM-specific vulnerability scanners and AI red teaming exercises. (We’ll talk more about AI red teams later in the chapter.) Extend your validation steps to check for nontraditional security threats such as toxicity and bias.</td>
            </tr>
            <tr>
              <td>Deployment</td>
              <td>Ensure you have appropriate runtime guardrails to screen prompts entering your model and output. Automate your build process to ensure that your ML-BOM is regenerated and stored with every set of changes.</td>
            </tr>
            <tr>
              <td>Monitoring</td>
              <td>Log all activity and monitor for anomalies that could indicate jailbreaks, attempts to deny service, or other compromises of your infrastructure.</td>
            </tr>
          </tbody>
        </table>
      </div></section>
      <section data-pdf-bookmark="Security in the LLM Development Process" data-type="sect1"><div class="sect1" id="security_in_the_llm_development_process">
        <h1>Security in the LLM Development Process</h1>
        <p><a contenteditable="false" data-primary="development process security" data-type="indexterm" id="ch11.html1"/>Now it’s time to move past process abstractions and get into the practical steps you must adopt to make your secure development process repeatable. We’ll look at topics that range across the entire development lifecycle. We’ll start by looking at how to make sure your development environment and pipeline are secure. Then we’ll look into LLM-specific security testing tools you can use to check your security procedures before deployment. We’ll also review the steps you must take to ensure the security of your software supply chain. </p>
        <section data-pdf-bookmark="Securing Your CI/CD" data-type="sect2"><div class="sect2" id="securing_your_ci_cd">
          <h2>Securing Your CI/CD</h2>
          <p><a contenteditable="false" data-primary="CI/CD security" data-type="indexterm" id="ch11.html2"/><a contenteditable="false" data-primary="development process security" data-secondary="CI/CD security" data-type="indexterm" id="ch11.html3"/>The security of the development pipeline is paramount in preventing your project from becoming a weak link in the supply chain. In <a data-type="xref" href="ch09.html#find_the_weakest_link">Chapter 9</a>, we reviewed the SolarWinds case study, which shows how disastrous it can be for you and your downstream customers if your pipeline is compromised. This section explores strategies to fortify the pipeline against threats, ensuring that your LLM application does not get compromised or inadvertently contribute to the security vulnerabilities of downstream users.</p>
          <section class="pagebreak-before less_space" data-pdf-bookmark="Implementing robust security practices" data-type="sect3"><div class="sect3" id="implementing_robust_security_practices">
            <h3>Implementing robust security practices</h3>
            <p><a contenteditable="false" data-primary="CI/CD security" data-secondary="implementing robust security practices" data-type="indexterm" id="id595"/><a contenteditable="false" data-primary="development process security" data-secondary="CI/CD security" data-tertiary="implementing robust security practices" data-type="indexterm" id="id596"/>Let’s look at some critical practices you’ll need to implement your security program:</p>
            <dl>
              <dt>CI/CD security</dt>
              <dd>
                <p>Integrate security checks into the CI/CD pipeline to automatically detect vulnerabilities or misconfigurations early in the development process.</p>
              </dd>
              <dt>Dependency management</dt>
              <dd>
                <p>Regularly audit and update the dependencies used in your project to mitigate vulnerabilities associated with outdated or compromised libraries. ML-specific, open source build pipeline components, such as PyTorch, have had severe, zero-day security issues reported recently, demonstrating the importance of this step.</p>
              </dd>
              <dt>Access control and monitoring</dt>
              <dd>
                <p>Limit access to the CI/CD environment and monitor activity to promptly detect and respond to suspicious behavior. Secure your training data repositories, just as you would your source code, to help protect against possible data poisoning attacks.</p>
              </dd>
            </dl>
          </div></section>
          <section data-pdf-bookmark="&#10;              Fostering a culture of security awareness&#10;            " data-type="sect3"><div class="sect3" id="fostering_a_culture_of_security_awareness">
            <h3>
              Fostering a culture of security awareness
            </h3>
            <p><a contenteditable="false" data-primary="CI/CD security" data-secondary="fostering a culture of security awareness" data-type="indexterm" id="id597"/><a contenteditable="false" data-primary="development process security" data-secondary="CI/CD security" data-tertiary="fostering a culture of security awareness" data-type="indexterm" id="id598"/>Training your humans can be just as important as training your LLM in building a secure app. Here are some things to think about in how your train and prepare your people:</p>
            <dl>
              <dt>Training and awareness</dt>
              <dd>
                <p>Educate members of the development team on the importance of supply chain security and their role in maintaining it. Ensure your team understands the new components, such as foundation models and training datasets, that must be managed as part of your application’s supply chain.</p>
              </dd>
              <dt>Incident response planning</dt>
              <dd>
                <p>Develop and regularly update an incident response plan that includes procedures for addressing supply chain threats, including zero-day vulnerability disclosures.<a contenteditable="false" data-primary="" data-startref="ch11.html3" data-type="indexterm" id="id599"/><a contenteditable="false" data-primary="" data-startref="ch11.html2" data-type="indexterm" id="id600"/></p>
              </dd>
            </dl>
          </div></section>
        </div></section>
        <section data-pdf-bookmark="LLM-Specific Security Testing Tools" data-type="sect2"><div class="sect2" id="llm_specific_security_testing_tools">
          <h2>LLM-Specific Security Testing Tools</h2>
          <p><a contenteditable="false" data-primary="development process security" data-secondary="LLM-specific security testing tools" data-type="indexterm" id="ch11.html4"/><a contenteditable="false" data-primary="security testing tools" data-type="indexterm" id="ch11.html5"/>Application security testing tools can come in multiple flavors, such as Static Application Security Testing (SAST), Dynamic Application Security Testing (DAST), and Interactive Application Security Testing (IAST). All have established themselves as indispensable instruments in developing traditional web applications. While each has its strengths and weaknesses, they all help automate the identification of vulnerabilities and security flaws, facilitating early detection and remediation. Their integration into the software development lifecycle enables organizations to adopt a proactive stance on security, ensuring that applications are functional and secure by design.</p>
          <p>LLMs present unique security challenges that are not fully addressed by traditional security testing methodologies. Their complexity, novelty, and susceptibility to issues like data bias, hallucination, and adversarial attacks necessitate specialized tools tailored to their distinct context. Although the field is relatively nascent, new tools aimed at fortifying LLM applications against a spectrum of vulnerabilities are beginning to emerge. Let’s look at several examples.</p>
          <section data-pdf-bookmark="TextAttack" data-type="sect3"><div class="sect3" id="textattack">
            <h3>TextAttack</h3>
            <p><a contenteditable="false" data-primary="development process security" data-secondary="LLM-specific security testing tools" data-tertiary="TextAttack" data-type="indexterm" id="id601"/><a contenteditable="false" data-primary="security testing tools" data-secondary="TextAttack" data-type="indexterm" id="id602"/><a contenteditable="false" data-primary="TextAttack" data-type="indexterm" id="id603"/>TextAttack has been around in some form since at least 2020. It is a sophisticated Python framework designed for adversarial testing of NLP models, including LLMs. Free and open source, distributed under the MIT license, it facilitates the exploration of vulnerabilities in language models and the development of robust defenses against adversarial attacks. </p>
            <p>TextAttack stands out by offering a modular architecture that allows for the customization and testing of attack strategies across various models and datasets. It simulates adversarial examples to reveal potential weaknesses in NLP applications, thereby guiding improvements in model resilience. The tool provides detailed reports on attack methodologies, success rates, and model responses, making it invaluable for security assessments. Its adaptability and comprehensive coverage of attack techniques make TextAttack a powerful tool for developers and researchers aiming to enhance the security and reliability of LLM applications.</p>
          </div></section>
          <section data-pdf-bookmark="Garak" data-type="sect3"><div class="sect3" id="garak_idTQoRya">
            <h3>Garak</h3>
            <p><a contenteditable="false" data-primary="development process security" data-secondary="LLM-specific security testing tools" data-tertiary="Garak" data-type="indexterm" id="id604"/><a contenteditable="false" data-primary="Garak" data-type="indexterm" id="id605"/><a contenteditable="false" data-primary="security testing tools" data-secondary="Garak" data-type="indexterm" id="id606"/>Garak, named after an obscure <em>Star Trek</em> character, is an LLM vulnerability scanner. Garak was developed by Leon Derczynski, who was a significant contributor to developing the first versions of the OWASP Top 10 for LLM Applications. Garak is free to use and distributed under a liberal Apache open source license.</p>
            <p>Garak adopts a model similar to that of DAST tools, where it probes the application at runtime and examines its behavior, looking for vulnerabilities. The tool sends various prompts to models, analyzing multiple outputs using detectors to identify unwanted content. The results aren’t scientifically validated, but a higher passing percentage indicates better performance. It can be customized with plug-ins for additional prompts or vulnerabilities. It generates detailed reports that include all test parameters, prompts, responses, and scores. There’s potential for expansion to different models and vulnerabilities based on user contributions and requests.</p>
          </div></section>
          <section data-pdf-bookmark="Responsible AI Toolbox" data-type="sect3"><div class="sect3" id="responsible_ai_toolbox">
            <h3>Responsible AI Toolbox</h3>
            <p><a contenteditable="false" data-primary="development process security" data-secondary="LLM-specific security testing tools" data-tertiary="Responsible AI Toolbox" data-type="indexterm" id="id607"/><a contenteditable="false" data-primary="Responsible AI Toolbox" data-type="indexterm" id="id608"/><a contenteditable="false" data-primary="security testing tools" data-secondary="Responsible AI Toolbox" data-type="indexterm" id="id609"/>The <a href="https://oreil.ly/6hpZE">Responsible AI Toolbox</a>, developed by Microsoft, is an open source tool suite that enables developers and data scientists to infuse ethical principles, fairness, and transparency into their AI systems. This toolbox is <span class="keep-together">distributed</span> under the MIT license and offers an integrated environment to assess, improve, and monitor models on various dimensions of responsible AI, including fairness, interpretability, and privacy.</p>
          </div></section>
          <section data-pdf-bookmark="Giskard LLM Scan" data-type="sect3"><div class="sect3" id="giskard_llm_scan">
            <h3>Giskard LLM Scan</h3>
            <p><a contenteditable="false" data-primary="development process security" data-secondary="LLM-specific security testing tools" data-tertiary="Giskard LLM Scan" data-type="indexterm" id="id610"/><a contenteditable="false" data-primary="Giskard LLM Scan" data-type="indexterm" id="id611"/><a contenteditable="false" data-primary="security testing tools" data-secondary="Giskard LLM Scan" data-type="indexterm" id="id612"/>Giskard LLM Scan is an open source tool used to assess an LLM’s ethical considerations and safety. Available under the Apache 2.0 license, this component of the Giskard AI suite aims to identify biases, detect instances of toxic content, and promote the responsible deployment of LLMs. It employs a variety of metrics and tests designed to evaluate LLM behavior in terms of fairness, toxicity, and inclusiveness. Through its interface, Giskard LLM Scan offers detailed reports highlighting areas of concern, assisting developers and researchers in understanding and potentially mitigating ethical risks in their AI models.</p>
          </div></section>
          <section data-pdf-bookmark="Integrating security tools into DevOps" data-type="sect3"><div class="sect3" id="integrating_security_tools_into_devops">
            <h3>Integrating security tools into DevOps</h3>
            <p><a contenteditable="false" data-primary="development process security" data-secondary="LLM-specific security testing tools" data-tertiary="integrating security tools into DevOps" data-type="indexterm" id="id613"/><a contenteditable="false" data-primary="LLMOps (large language model operations)" data-secondary="integrating security tools into" data-type="indexterm" id="id614"/><a contenteditable="false" data-primary="security testing tools" data-secondary="integrating security tools into DevOps" data-type="indexterm" id="id615"/>Integrating automated, LLM-specific security testing tools and traditional AST (application security testing) tools into LLMOps processes is not merely beneficial but imperative. Embedding these tools within CI/CD pipelines ensures that security is not an afterthought but a foundational aspect of application development. This approach enables automated, repeatable security checks performed with every build, significantly reducing the risk of vulnerabilities in production. Moreover, it <span class="keep-together">fosters a culture </span> of security mindfulness among development teams, ensuring that security considerations are paramount from the inception of a project through to its <span class="keep-together">deployment.</span><a contenteditable="false" data-primary="" data-startref="ch11.html5" data-type="indexterm" id="id616"/><a contenteditable="false" data-primary="" data-startref="ch11.html4" data-type="indexterm" id="id617"/></p>
          </div></section>
        </div></section>
        <section data-pdf-bookmark="Managing Your Supply Chain" data-type="sect2"><div class="sect2" id="managing_your_supply_chain">
          <h2>Managing Your Supply Chain</h2>
          <p><a contenteditable="false" data-primary="development process security" data-secondary="supply chain management" data-type="indexterm" id="id618"/><a contenteditable="false" data-primary="supply chains (for LLM application development)" data-secondary="management" data-type="indexterm" id="id619"/>As discussed in <a data-type="xref" href="ch09.html#find_the_weakest_link">Chapter 9</a>, the supply chain represents more than sourcing components and tools. It involves the meticulous generation, storage, and accessibility of development artifacts such as model cards and ML-BOMs. </p>
          <p>Model cards are essential documentation for LLMs, providing an overview of a model’s purpose, performance, and potential biases. Similarly, ML-BOMs detail the components, datasets, and dependencies involved in developing an application using machine learning technologies like an LLM. Together, these artifacts form a cornerstone of transparency and accountability in LLM development.</p>
          <p>To manage them effectively, developers must implement systems for generating, storing, and making these artifacts easily searchable. This facilitates regulatory compliance and enhances stakeholder collaboration and trust. By integrating these practices into a broader SBOM strategy, teams can ensure a holistic view of both AI and non-AI components of their applications, reinforcing the security and integrity of the <span class="keep-together">supply chain.</span></p>
          <p>You’ll need to focus on three pillars to ensure your artifacts are properly tracked, thus helping to ensure you’re in control of your supply chain:</p>
          <dl>
            <dt>Automated generation</dt>
            <dd>
              <p>Implement tools and workflows that automatically generate model cards and ML-BOMs at key development milestones.</p>
            </dd>
            <dt>Secure storage</dt>
            <dd>
              <p>Store these artifacts in secure, version-controlled repositories to ensure they are tamper-proof and retrievable.</p>
            </dd>
            <dt>Accessibility</dt>
            <dd>
              <p>Make these artifacts accessible to relevant stakeholders, incorporating search functionalities to facilitate quick retrieval and review.</p>
            </dd>
          </dl>
          <p>The supply chain in LLM application development is a complex ecosystem that requires diligent management to ensure the security and integrity of both development artifacts and the development pipeline. By prioritizing the generation and storage of key artifacts like model cards and ML-BOMs and by securing the development pipeline, organizations can safeguard against supply chain vulnerabilities, fostering trust and reliability in their LLM applications.<a contenteditable="false" data-primary="" data-startref="ch11.html1" data-type="indexterm" id="id620"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Protect Your App with Guardrails" data-type="sect1"><div class="sect1" id="protect_your_app_with_guardrails">
        <h1>Protect Your App with Guardrails</h1>
        <p><a contenteditable="false" data-primary="guardrails, for app protection" data-type="indexterm" id="ch11.html6"/>Tools such as web application firewalls (WAFs) and runtime application self-protection (RASP) have become fundamental in defending web applications against attacks during runtime. Unlike AST tools that analyze code for vulnerabilities at build and test time, WAFs and RASP provide continuous protection while an application operates in production. They act as vigilant guardians, identifying and mitigating threats in real time, thus adding a critical layer of security.</p>
        <p>In the context of LLMs, a parallel can be drawn with the concept of <em>guardrails</em>. Guardrails help ensure that LLMs operate within defined ethical, legal, and safety parameters, preventing misuse and guiding the models toward generating appropriate and safe outputs. Initially, guardrail implementations were relatively simplistic, often built in house and tailored to specific use cases. In <a data-type="xref" href="ch07.html#trust_no_one">Chapter 7</a>, we walked through the construction of some simple guardrails to help screen output from the LLM for toxicity and PII. This exercise was a great way to understand the basics of how some guardrails work.</p>
        <p>However, the demand for more sophisticated security and safety frameworks has increased as LLM-based applications have grown more complex. Today, there is a burgeoning ecosystem of tools, both open source and commercial, offering more comprehensive guardrail frameworks for LLMs. These tools serve as runtime security measures, continuously monitoring and guiding the behavior of LLMs to prevent the generation of harmful, biased, or otherwise undesirable content. They are akin to WAFs and RASP in the web application space, providing a dynamic shield that adapts to emerging threats and challenges.</p>
        <section data-pdf-bookmark="The Role of Guardrails in an LLM Security Strategy" data-type="sect2"><div class="sect2" id="the_role_of_guardrails_in_an_llm_security_strategy">
          <h2>The Role of Guardrails in an LLM Security Strategy</h2>
          <p><a contenteditable="false" data-primary="guardrails, for app protection" data-secondary="role of guardrails" data-type="indexterm" id="ch11.html7"/>Incorporating advanced guardrail solutions into LLM deployments is not just a recommendation; it’s becoming a necessity. As these models become more deeply integrated into critical and consumer-facing applications, the potential impact of their misuse or malfunction grows exponentially. Guardrails offer a way to mitigate these risks. Guardrails frameworks offer a range of functionality, but here are some typical functions you’ll want to look for as you evaluate your options.</p>
          <section data-pdf-bookmark="Input validation" data-type="sect3"><div class="sect3" id="input_validation">
            <h3>Input validation</h3>
            <p><a contenteditable="false" data-primary="guardrails, for app protection" data-secondary="role of guardrails" data-tertiary="input validation" data-type="indexterm" id="id621"/><a contenteditable="false" data-primary="input validation" data-secondary="for app protection guardrails" data-type="indexterm" id="id622"/>There are several benefits of implementing guardrails that scan the input into your LLM:</p>
            <dl>
              <dt>Prompt injection prevention</dt>
              <dd>
                <p>Monitor for signs of prompt injection, such as unusual phrases, hidden characters, and odd encodings, to prevent malicious manipulation of the LLM.</p>
              </dd>
              <dt>Domain limitation</dt>
              <dd>
                <p>Keep the LLM focused on relevant topics by restricting or ignoring irrelevant prompts. This enhances security by reducing the risk of generating inappropriate or irrelevant content and diminishing the likelihood of hallucinations.</p>
              </dd>
              <dt>Anonymization and secret detection</dt>
              <dd>
                <p>While interacting with the LLM, users may input confidential data, like email addresses, telephone numbers, or API keys. This poses a problem if the data is logged, stored, or transferred to a third-party LLM provider or if the data could potentially be used for training purposes. It’s crucial to anonymize PII and redact sensitive data before the LLM processes it.</p>
              </dd>
            </dl>
          </div></section>
          <section data-pdf-bookmark="Output validation" data-type="sect3"><div class="sect3" id="output_validation">
            <h3>Output validation</h3>
            <p><a contenteditable="false" data-primary="guardrails, for app protection" data-secondary="role of guardrails" data-tertiary="output validation" data-type="indexterm" id="id623"/><a contenteditable="false" data-primary="output validation, for app protection guardrails" data-type="indexterm" id="id624"/>Screening all output from your LLM is a critical part of your zero trust strategy. Here are some of the benefits:</p>
            <dl>
              <dt>Ethical screening</dt>
              <dd>
                <p>Filter outputs for content that could be considered toxic, inappropriate, or hateful to ensure the LLM’s interactions align with ethical guidelines. This could have saved poor Tay from <a data-type="xref" href="ch01.html#chatbots_breaking_bad">Chapter 1</a> and countless other projects from falling victim to vulnerabilities such as unchecked toxicity.</p>
              </dd>
              <dt>Sensitive information protection</dt>
              <dd>
                <p>Implement measures to prevent the disclosure of PII or other sensitive data through the LLM’s outputs.</p>
              </dd>
              <dt>Code output</dt>
              <dd>
                <p>Look for unintended code generation that could lead to downstream attacks such as SQL injection, server-side request forgery (SSRF), and XSS.</p>
              </dd>
              <dt>Compliance assurance</dt>
              <dd>
                <p>In sectors with strict regulatory standards, like health care or legal, tailor outputs to meet specific compliance requirements and keep the LLM’s responses within the scope of its intended use.</p>
              </dd>
              <dt>Fact-checking and hallucination detection</dt>
              <dd>
                <p>Verify the accuracy of LLM outputs against trusted sources to ensure the information provided is factual and reliable. Identify and mitigate instances where the LLM generates fictitious or irrelevant content to ensure outputs remain relevant and grounded in reality.<a contenteditable="false" data-primary="" data-startref="ch11.html7" data-type="indexterm" id="id625"/></p>
              </dd>
            </dl>
          </div></section>
        </div></section>
        <section data-pdf-bookmark="Open Source Versus Commercial Guardrail Solutions" data-type="sect2"><div class="sect2" id="open_source_versus_commercial_guardrail_solutions">
          <h2>Open Source Versus Commercial Guardrail Solutions</h2>
          <p><a contenteditable="false" data-primary="guardrails, for app protection" data-secondary="open source versus commercial guardrail solutions" data-type="indexterm" id="id626"/><a contenteditable="false" data-primary="open source guardrail solutions" data-type="indexterm" id="id627"/>The choice between open source and commercial guardrail solutions depends on several factors, including the organization’s specific needs, the level of customization required, and budget considerations. </p>
          <p>Open source tools offer the benefits of flexibility and community support, allowing organizations to tailor solutions to their unique requirements. However, they may require significant internal expertise and resources to deploy and maintain effectively. Some examples of open source guardrails tools you may wish to evaluate include NVIDIA NeMo-Guardrails, Meta Llama Guard, Guardrails AI, and Protect AI.</p>
          <p>On the other hand, commercial solutions may provide more out-of-the-box functionality with the added benefits of professional support, regular updates, and advanced features. Some examples of commercial guardrail options include Prompt Security, Lakera Guard, WhyLabs LangKit,  Lasso Security, PromptArmor, and Cloudflare Firewall for AI.</p>
        </div></section>
        <section data-pdf-bookmark="Mixing Custom and Packaged Guardrails" data-type="sect2"><div class="sect2" id="mixing_custom_and_packaged_guardrails">
          <h2>Mixing Custom and Packaged Guardrails</h2>
          <p><a contenteditable="false" data-primary="guardrails, for app protection" data-secondary="mixing custom/packaged guardrails" data-type="indexterm" id="id628"/>In <a data-type="xref" href="ch07.html#trust_no_one">Chapter 7</a>, we implemented some basic guardrails by hand. While the emergence of prebuilt guardrail frameworks can offer a significant boost in security, these <span class="keep-together">handcrafted</span> guardrails still have a role. Supplementing a guardrail framework with your own custom, domain-, or application-specific guardrails can make a lot of sense. These types of defense-in-depth strategies are often the most successful in <span class="keep-together">cybersecurity.</span><a contenteditable="false" data-primary="" data-startref="ch11.html6" data-type="indexterm" id="id629"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Monitoring Your App" data-type="sect1"><div class="sect1" id="monitoring_your_app">
        <h1>Monitoring Your App</h1>
        <p><a contenteditable="false" data-primary="app monitoring" data-type="indexterm" id="id630"/>In the lifecycle of LLM applications, effective monitoring encompasses not only the conventional components—such as web servers, middleware, application code, and databases—but also the unique elements intrinsic to LLMs, including the model itself and associated vector databases used for RAG. This comprehensive approach is pivotal for maintaining operational integrity and security throughout the application’s lifecycle.</p>
        <section data-pdf-bookmark="Logging Every Prompt and Response" data-type="sect2"><div class="sect2" id="logging_every_prompt_and_response">
          <h2>Logging Every Prompt and Response</h2>
          <p><a contenteditable="false" data-primary="app monitoring" data-secondary="logging every prompt and response" data-type="indexterm" id="id631"/>One of the foundational practices in monitoring LLM applications is to log every prompt and response. This detailed logging serves multiple purposes: it provides insights into how users interact with the application, enables the identification of potential misuse or problematic outputs, and forms a baseline for understanding the model’s performance over time. Such granular data collection is critical for diagnosing issues, optimizing model behavior, and ensuring compliance with data governance standards.</p>
        </div></section>
        <section data-pdf-bookmark="Centralized Log and Event Management" data-type="sect2"><div class="sect2" id="centralized_log_and_event_management">
          <h2>Centralized Log and Event Management</h2>
          <p><a contenteditable="false" data-primary="app monitoring" data-secondary="centralized log/event management" data-type="indexterm" id="id632"/><a contenteditable="false" data-primary="security information and event management (SIEM) systems" data-type="indexterm" id="id633"/><a contenteditable="false" data-primary="SIEM (security information and event management) systems" data-type="indexterm" id="id634"/>Aggregating logs and application events into a <em>security information and event <span class="keep-together">management</span></em> (SIEM) system is essential. A SIEM system enables data consolidation across the entire application stack, offering a unified view of all activities. This allows your organization to easily store a historical record of how your application has responded to every user input. These centralized logs can then be stored for compliance purposes. Also, SIEM systems offer advanced search tools that enable your team to quickly search for patterns across a huge range of prompts and responses. This can enable your security operations team to hunt for threats while your application is in production.</p>
        </div></section>
        <section data-pdf-bookmark="User and Entity Behavior Analytics" data-type="sect2"><div class="sect2" id="user_and_entity_behavior_analytics">
          <h2>User and Entity Behavior Analytics</h2>
          <p><a contenteditable="false" data-primary="app monitoring" data-secondary="user/entity behavior analytics" data-type="indexterm" id="id635"/><a contenteditable="false" data-primary="user/entity behavior analytics (UEBA)" data-type="indexterm" id="id636"/>To enhance monitoring capabilities further, incorporating <em>user and entity behavior analytics </em>(UEBA) technology can be layered on top of SIEM. UEBA extends traditional monitoring by leveraging machine learning and analytics to understand how users and entities typically interact with the application, thereby enabling the detection of activities that deviate from the norm. For LLM applications, extending UEBA frameworks to encompass model-specific behaviors—such as unusual prompt-response patterns or atypical access to the vector database—can provide early warning signs of security breaches, data leaks, or the need for model retraining. In addition, dramatic changes in usage patterns could help you identify denial-of-service, denial-of-wallet, and model cloning attacks,<a contenteditable="false" data-primary="" data-startref="ch11.html12" data-type="indexterm" id="id637"/> as discussed in <a data-type="xref" href="ch08.html#don_t_lose_your_wallet">Chapter 8</a>.</p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Build Your AI Red Team" data-type="sect1"><div class="sect1" id="build_your_ai_red_team">
        <h1>Build Your AI Red Team</h1>
        <p><a contenteditable="false" data-primary="AI red teams" data-type="indexterm" id="ch11.html10"/>So far in this chapter, we’ve looked at how to secure your development pipeline, use security testing tools in a repeatable way, and guard and then monitor your application in production. These are all critical steps, but they’ve repeatedly been shown to be necessary but insufficient in understanding your application’s actions in the real world. The emerging field of <em>AI red teaming</em> is designed to do just this. Let’s look at how an AI red team can become an important part of validating the security of your application.</p>
        <p>An AI red team is a group of security professionals who adopt an adversarial approach to rigorously challenge the safety and security of applications using AI technology, such as an LLM. Their objective is to identify and exploit weaknesses in AI systems, much like an external attacker might, but to improve security rather than cause harm.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>AI red teams catapulted to the forefront of the AI and LLM security discussion when US President Biden issued his October 2023 <a href="https://oreil.ly/yGHW8">“Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence,”</a> which contains the following language: </p>
          <ul class="simplelist"><li><em>The term “AI red-teaming” means a structured testing effort to find flaws and vulnerabilities in an AI system, often in a controlled environment and in collaboration with developers of AI. Artificial Intelligence red-teaming is most often performed by dedicated “red teams” that adopt adversarial methods to identify flaws and vulnerabilities, such as harmful or discriminatory outputs from an AI system, unforeseen or undesirable system behaviors, limitations, or potential risks associated with the misuse of the system.</em></li></ul>
          <p>As a result of this order, the US Artificial Intelligence Safety Institute, part of the National Institute of Standards and Technology (NIST), has created a dedicated working group on red teaming best practices.</p>
        </div>
        <p>An AI red team operates under the premise that AI systems have unique vulnerabilities that traditional software may not possess, such as adversarial input attacks, data poisoning, and model stealing attacks. The AI red team helps organizations anticipate and mitigate security breaches by simulating real-world AI-specific threats.</p>
        <p class="pagebreak-before">The critical functions of an AI red team include:</p>
        <dl>
          <dt>Adversarial attack simulation</dt>
          <dd>
            <p>Crafting and executing attacks that exploit weaknesses in AI systems, such as feeding deceptive input to manipulate outcomes or extract sensitive data.</p>
          </dd>
          <dt>Vulnerability assessment</dt>
          <dd>
            <p>Systematically reviewing AI systems to identify vulnerabilities that could be exploited by attackers, including those in the underlying infrastructure, training data, and model outputs.</p>
          </dd>
          <dt>Risk analysis</dt>
          <dd>
            <p>Evaluating the potential impact of identified vulnerabilities and providing a risk-based assessment to prioritize remediation efforts.</p>
          </dd>
          <dt>Mitigation strategy development</dt>
          <dd>
            <p>Recommending defenses and countermeasures to protect AI systems against identified threats and vulnerabilities.</p>
          </dd>
          <dt>Awareness and training</dt>
          <dd>
            <p>Educating developers, security teams, and stakeholders about AI security threats and best practices to foster a culture of security-minded AI development.</p>
          </dd>
        </dl>
        <p>An AI red team is essential to a robust AI security framework. It ensures that AI systems are designed and developed securely, continuously tested, and fortified against evolving threats in the wild.</p>
        <section data-pdf-bookmark="Advantages of AI Red Teaming" data-type="sect2"><div class="sect2" id="advantages_of_ai_red_teaming">
          <h2>Advantages of AI Red Teaming</h2>
          <p><a contenteditable="false" data-primary="AI red teams" data-secondary="advantages" data-type="indexterm" id="id638"/>Traditional security measures, while necessary, are often insufficient to address complex LLM-specific vulnerabilities. A red team, with its holistic and adversarial approach, becomes crucial in identifying and mitigating these threats, not just through technical means but by examining the broader implications of human and organizational behaviors.</p>
          <p>Hallucinations, for example, represent a significant risk. A red team, by simulating advanced testing scenarios, can identify potential triggers for such behavior, enabling developers to understand and mitigate these risks in ways automated testing cannot.</p>
          <p>Data bias poses a more subtle yet profound threat, as it can lead to unfair or unethical outcomes. Red teams can assess the technical aspects of bias and systemic issues within data collection and processing practices. The team’s external perspective can uncover blind spots in data handling and algorithm training that might be overlooked by internal teams focused on functionality.</p>
          <p class="pagebreak-before">Excessive agency in LLMs, where the model can act beyond its intended scope, requires continuous and creative testing to identify. Red teams can probe the limits of LLM behavior to ensure that safeguards against unintended autonomous actions are robust and effective.</p>
          <p>Prompt injection attacks exploit how LLMs process input to produce unintended outcomes, highlighting the need for a red team’s innovative thinking. The team can simulate sophisticated attack vectors that challenge the LLM’s ability to handle adversarial inputs safely.</p>
          <p>Moreover, risks like overreliance on LLMs involve technical, human, and organizational factors. Red teams can evaluate the broader impact of LLM integration into decision-making processes, highlighting areas where reliance on automation might undermine critical thinking or operational security.</p>
          <p>The necessity of a red team in LLM application security is not merely a matter of adding another layer of defense; it’s about adopting a comprehensive and proactive approach to security that addresses the full spectrum of risks—from the technical to the human. This approach ensures that LLM applications are resilient against current threats and prepared to evolve in the face of emerging vulnerabilities.</p>
        </div></section>
        <section data-pdf-bookmark="Red Teams Versus Pen Tests" data-type="sect2"><div class="sect2" id="red_teams_versus_pen_tests">
          <h2>Red Teams Versus Pen Tests</h2>
          <p><a contenteditable="false" data-primary="AI red teams" data-secondary="pen tests versus" data-type="indexterm" id="id639"/>Red teams and traditional penetration tests are often discussed in the same breath, yet they occupy distinct roles in an organization’s security posture. As we tease apart the differences between these two approaches, we must recognize that they are not mutually exclusive but complementary in fortifying defenses against cyber threats. <em>Penetration testing</em> is a point-in-time assessment identifying exploitable vulnerabilities. In contrast, red teaming is an ongoing, dynamic process that simulates real-world attacks across the entire digital and physical spectrum of an organization’s defenses.</p>
          <p>Red teaming is particularly crucial when safeguarding the integrity of LLM applications, where the attack surface is vast and qualitatively different from traditional applications. A red team, operating with a mindset aligned with that of a potential adversary, engages in a broader and more fluid form of security testing. This encompasses technical vulnerabilities and the organizational, behavioral, and psychological aspects of security. In this way, red teaming can also include checking for responsible and ethical outcomes, which is extremely difficult for fully automated testing.</p>
          <p><a data-type="xref" href="#table-11-2">Table 11-2</a> summarizes the differences between a pen test and the red team.</p>
          <table class="pagebreak-before less_space" id="table-11-2">
            <caption><span class="label">Table 11-2. </span>Pen test versus red team</caption>
            <thead>
              <tr>
                <th>Aspect</th>
                <th>Pen test</th>
                <th>Red team</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Objective</td>
                <td>Identify and exploit specific vulnerabilities</td>
                <td>Emulate realistic cyberattacks to test response capabilities</td>
              </tr>
              <tr>
                <td>Scope</td>
                <td>Focused on specific systems, networks, or applications</td>
                <td>Broad, includes a variety of attack vectors like social engineering, physical security, and network security</td>
              </tr>
              <tr>
                <td>Duration</td>
                <td>Short-term, typically a few days to a few weeks</td>
                <td>Long-term, can span several weeks to months to simulate persistent threats</td>
              </tr>
              <tr>
                <td>Frequency</td>
                <td>Regular intervals, or as part of compliance assessments</td>
                <td>Frequent or continuous</td>
              </tr>
              <tr>
                <td>Approach</td>
                <td>Tactical, seeking to uncover specific technical vulnerabilities</td>
                <td>Strategic, aiming to reveal systemic weaknesses and organizational response</td>
              </tr>
              <tr>
                <td>Reporting</td>
                <td>Detailed list of vulnerabilities with remediation steps</td>
                <td>Comprehensive assessment of security posture and recommendations for holistic improvement</td>
              </tr>
            </tbody>
          </table>
        </div></section>
        <section data-pdf-bookmark="Tools and Approaches" data-type="sect2"><div class="sect2" id="tools_and_approaches">
          <h2>Tools and Approaches</h2>
          <p><a contenteditable="false" data-primary="AI red teams" data-secondary="tools and approaches" data-type="indexterm" id="id640"/>While you can build a red team entirely on your own, there are emerging tools and services that can help. This space will evolve quickly, but we’ll review a couple of emerging options so that you’ll know what to look for.</p>
          <section data-pdf-bookmark="Red team automation tooling" data-type="sect3"><div class="sect3" id="red_team_automation_tooling">
            <h3>Red team automation tooling</h3>
            <p><a contenteditable="false" data-primary="AI red teams" data-secondary="tools and approaches" data-tertiary="red team automation tooling" data-type="indexterm" id="id641"/>Introduced in February 2024, PyRIT (Python Risk Identification Toolkit for generative AI) is Microsoft’s open source initiative to augment the capabilities of AI red teams. PyRIT, which evolved from earlier internal tools developed by Microsoft, is designed to support identifying and analyzing vulnerabilities within generative AI systems. The toolkit serves as an augmentation tool for human red teamers, not as a replacement, emphasizing the toolkit’s role in enhancing human-led security efforts.</p>
            <p>PyRIT automates aspects of the red teaming process, allowing security professionals to efficiently uncover potential weaknesses that could be exploited in generative AI systems. PyRIT enables human red teamers to allocate more time to strategic, complex attack simulations and creative vulnerability exploration by streamlining the detection of issues such as adversarial attacks and data poisoning. This combination of automation and human expertise aims to deepen the security testing of AI systems, ensuring they are resilient against a broad spectrum of cyber threats. </p>
          </div></section>
          <section data-pdf-bookmark="Red team as a service" data-type="sect3"><div class="sect3" id="red_team_as_a_service">
            <h3>Red team as a service</h3>
            <p><a contenteditable="false" data-primary="AI red teams" data-secondary="tools and approaches" data-tertiary="red team as a service" data-type="indexterm" id="id642"/>HackerOne’s AI safety red teaming service offers a possible solution for organizations that lack the time, resources, or expertise to develop and sustain an in-house red team dedicated to the security of their AI systems. This service provides a flexible, “as-a-service” approach, allowing organizations to access the specialized skills and insights necessary for comprehensive AI security assessments without significant internal investment.</p>
            <p>By leveraging HackerOne’s network of crowdsourced security professionals, companies can benefit from thorough and creative adversarial testing tailored to AI technologies’ unique vulnerabilities. This external expertise supports identifying and mitigating potential threats to enhance the security posture of AI systems with flexibility and scalability that aligns with organizational needs and capacities.<a contenteditable="false" data-primary="" data-startref="ch11.html10" data-type="indexterm" id="id643"/> </p>
          </div></section>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Continuous Improvement" data-type="sect1"><div class="sect1" id="continuous_improvement">
        <h1>Continuous Improvement</h1>
        <p><a contenteditable="false" data-primary="continuous improvement" data-type="indexterm" id="ch11.html11"/>The secure deployment of LLM applications is not a onetime effort but a continuous journey of improvement and adaptation. Insights gleaned from logged prompts and responses, UEBA, and AI red team exercises are invaluable assets in this process. They provide a rich dataset from which to learn and a roadmap for enhancing your LLM applications’ security and functionality. Based on the results you see from these sources, there are many activities you can execute continuously to improve your overall security and safety posture.</p>
        <section data-pdf-bookmark="Establishing and Tuning Guardrails" data-type="sect2"><div class="sect2" id="establishing_and_tuning_guardrails">
          <h2>Establishing and Tuning Guardrails</h2>
          <p><a contenteditable="false" data-primary="continuous improvement" data-secondary="establishing/tuning guardrails" data-type="indexterm" id="id644"/><a contenteditable="false" data-primary="guardrails, for app protection" data-secondary="establishing and tuning" data-type="indexterm" id="id645"/>Earlier in this chapter, we discussed the importance of guardrails and how they can be flexibly implemented. You should make maintaining and updating your guardrails part of your DevOps process. Whether you build your own guardrails by hand or use one of the frameworks discussed earlier, you’ll still need to update and tune them continuously:</p>
          <dl>
            <dt>
              Adaptive 
              guardrails
            </dt>
            <dd>
              <p>Use the insights from your monitoring and testing activities to fine-tune existing guardrails around your LLM’s operations. This might involve adjusting thresholds for acceptable behavior, refining content filters, or enhancing data privacy measures.</p>
            </dd>
            <dt>
              New 
              guardrails
            </dt>
            <dd>
              <p>Beyond tuning, the intelligence gathered can reveal the need for entirely new guardrails. These might address emerging threats, new patterns of misuse, or unintended model behaviors that were previously unnoticed.</p>
            </dd>
          </dl>
        </div></section>
        <section data-pdf-bookmark="Managing Data Access and Quality" data-type="sect2"><div class="sect2" id="managing_data_access_and_quality">
          <h2>Managing Data Access and Quality</h2>
          <p><a contenteditable="false" data-primary="continuous improvement" data-secondary="data access and quality management" data-type="indexterm" id="id646"/><a contenteditable="false" data-primary="data access" data-type="indexterm" id="id647"/><a contenteditable="false" data-primary="quality management" data-type="indexterm" id="id648"/>In two previous chapters, we’ve discussed the delicate balance of giving your LLM too much or too little data. In <a data-type="xref" href="ch05.html#can_your_llm_know_too_much">Chapter 5</a>, we discussed the risks of sensitive information disclosure. In <a data-type="xref" href="ch06.html#do_language_models_dream_of_electric_sheep">Chapter 6</a>, we discussed the risks of hallucination. We can help keep those risks in check by incorporating these lessons into our process. This is the time to add new expertise to your overall DevSecOps approach. As you include MLOps and LLMOps approaches, you’ll want to include data scientists and behavioral analysts in your workflows:</p>
          <dl>
            <dt>
              Data 
              access
            </dt>
            <dd>
              <p>Regularly review and manage the data your LLM can access. This involves removing access to sensitive or irrelevant data and incorporating new datasets to help the model avoid hallucinations or biases, thereby improving its reliability and the quality of its outputs.</p>
            </dd>
            <dt>
              Quality 
              control
            </dt>
            <dd>
              <p>Ensure that the data fed into your LLM is of high quality and representative. This reduces the risk of training the model on misleading or harmful information, which can directly impact its security and effectiveness.</p>
            </dd>
          </dl>
        </div></section>
        <section data-pdf-bookmark="Leveraging RLHF for Alignment and Security" data-type="sect2"><div class="sect2" id="leveraging_rlhf_for_alignment_and_security">
          <h2>Leveraging RLHF for Alignment and Security</h2>
          <p><a contenteditable="false" data-primary="continuous improvement" data-secondary="RLHF leveraging" data-type="indexterm" id="id649"/><a contenteditable="false" data-primary="RLHF (reinforcement learning from human feedback)" data-type="indexterm" id="id650"/><em>Reinforcement learning from human feedback</em> (RLHF) is a sophisticated machine learning technique that significantly enhances the performance and alignment of LLMs with human values and expectations. At its core, RLHF involves training LLMs using feedback generated by human evaluators rather than relying solely on predefined reward functions or datasets. This process starts with humans reviewing the outputs produced by a model in response to certain inputs or prompts. Evaluators then provide feedback, ranging from rankings and ratings to direct corrections or preferences. This human-generated feedback is used to create or refine a reward model, guiding the LLM in generating responses that are more closely aligned with human judgment and ethical standards. The iterative nature of RLHF allows for continuous improvement of the model’s accuracy, relevance, and safety, which makes it a critical tool in developing user-centric AI applications.</p>
          <p>RLHF bridges the gap between raw computational output and the nuanced understanding of language and context that characterizes human communication by integrating human insights into the training process. This method improves the model’s ability to generate coherent and contextually appropriate responses and ensures that these outputs adhere to ethical guidelines and societal norms. As AI applications become increasingly integrated into everyday life, the role of RLHF in ensuring these technologies act in beneficial and nonharmful ways to humans becomes ever more crucial.</p>
          <p>Admittedly, incorporating RLHF into the process is more complex, involved, and expensive than straightforward interventions, such as tweaking guardrails, fine-tuning, or augmenting RAG data. However, for applications where accuracy, alignment with human values, and ethical considerations are paramount, RLHF stands out as one of the most powerful tools available. Its capability to iteratively refine and align the model’s outputs through direct human feedback makes it an invaluable asset for developing LLM applications that are not only technologically advanced but also deeply attuned to the nuances of human interaction and expectations.</p>
          <div data-type="warning" epub:type="warning"><h6>Warning</h6>
            <p>While RLHF offers significant advantages in aligning LLMs with human values and improving their performance, it is crucial to be aware of its limitations and potential pitfalls. Firstly, introducing human feedback into the training process can inadvertently introduce or amplify biases, reflecting the evaluators’ subjective perspectives or unconscious prejudices. Additionally, RLHF does not inherently protect against adversarial attacks; sophisticated adversaries might still find ways to exploit vulnerabilities in the model’s responses. Another concern is the potential for <em>policy overfitting</em>, where the model becomes overly specialized in generating responses that satisfy the feedback, but loses generalizability and performance across broader contexts. Developers need to weigh these factors carefully and consider implementing complementary strategies to mitigate these limitations and ensure the responsible development of AI technologies.<a contenteditable="false" data-primary="" data-startref="ch11.html11" data-type="indexterm" id="id651"/></p>
          </div>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="conclusion_13">
        <h1>Conclusion</h1>
        <p>Integrating LLMs into production is complex and demands a sophisticated approach to security and operations. The shift toward DevSecOps, MLOps, and LLMOps represents a critical evolution in developing, deploying, and securing software, which highlights the importance of embedding security deeply within the development lifecycle. This foundation is crucial for navigating the risks associated with LLM technologies, from privacy and security to ethical and regulatory concerns.</p>
        <p>The role of AI red teaming offers a proactive means to identify and mitigate potential vulnerabilities through simulated adversarial attacks. Red teaming, alongside continuous monitoring and improvement principles, sets the stage for a dynamic and resilient approach to LLM application security. It underscores the necessity of a vigilant, adaptive stance toward technology integration, where ongoing evaluation and refinement are key to safeguarding against evolving threats.</p>
        <p>Securing LLM applications is a journey that emphasizes the importance of a continuous, iterative process. By rigorously applying the cycle of development, deployment, monitoring, and refining, organizations can create systems of unparalleled robustness and security. This commitment to perpetual enhancement, guided by the latest security practices and insights from each cycle, ensures that with every iteration, the applications become safer, more secure, and more aligned with ethical standards. This relentless pursuit of improvement will lead to the most resilient LLM applications, ready to meet the challenges of tomorrow with confidence.</p>
      </div></section>
    </div></section></body></html>