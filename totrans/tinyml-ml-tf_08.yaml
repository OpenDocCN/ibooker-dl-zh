- en: 'Chapter 8\. Wake-Word Detection: Training a Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](ch07.xhtml#chapter_speech_wake_word_example), we built an application
    around a model trained to recognize “yes” and “no.” In this chapter, we will train
    a *new* model that can recognize different words.
  prefs: []
  type: TYPE_NORMAL
- en: Our application code is fairly general. All it does is capture and process audio,
    feed it into a TensorFlow Lite model, and do something based on the output. It
    mostly doesn’t care which words the model is looking for. This means that if we
    train a new model, we can just drop it into our application and it should work
    right away.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the things we need to consider when training a new model:'
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs: []
  type: TYPE_NORMAL
- en: The new model must be trained on input data that is the same shape and format,
    with the same preprocessing as our application code.
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the new model must be in the same format: a tensor of probabilities,
    one for each class.'
  prefs: []
  type: TYPE_NORMAL
- en: Training data
  prefs: []
  type: TYPE_NORMAL
- en: Whichever new words we pick, we’ll need many recordings of people saying them
    so that we can train our new model.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  prefs: []
  type: TYPE_NORMAL
- en: The model must be optimized to run efficiently on a microcontroller with limited
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately for us, our existing model was trained using a publicly available
    script that was published by the TensorFlow team, and we can use this script to
    train a new model. We also have access to a free dataset of spoken audio that
    we can use as training data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll walk through the process of training a model with
    this script. Then, in [“Using the Model in Our Project”](#micro_speech_using_model_in_project),
    we’ll incorporate the new model into our existing application code. After that,
    in [“How the Model Works”](#micro_speech_how_the_model_works), you’ll learn how
    the model actually works. Finally, in [“Training with Your Own Data”](#micro_speech_training_with_your_own_data),
    you’ll see how to train a model using your own dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Training Our New Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model we are using was trained with the TensorFlow [Simple Audio Recognition](https://oreil.ly/E292V)
    script, an example script designed to demonstrate how to build and train a model
    for audio recognition using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script makes it very easy to train an audio recognition model. Among other
    things, it allows us to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Download a dataset with audio featuring 20 spoken words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose which subset of words to train the model on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify what type of preprocessing to use on the audio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose from several different types of model architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize the model for microcontrollers using quantization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we run the script, it downloads the dataset, trains a model, and outputs
    a file representing the trained model. We then use some other tools to convert
    this file into the correct form for TensorFlow Lite.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s common for model authors to create these types of training scripts. It
    allows them to easily experiment with different variants of model architectures
    and hyperparameters, and to share their work with others.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to run the training script is within a Colaboratory (Colab)
    notebook, which we do in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Training in Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google Colab is a great place to train models. It provides access to powerful
    computing resources in the cloud, and it comes set up with tools that we can use
    to monitor the training process. It’s also completely free.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this section, we will use a Colab notebook to train our new
    model. The notebook we use is available in the TensorFlow repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[Open the notebook](https://oreil.ly/0Z2ra) and click the “Run in Google Colab”
    button, as shown in [Figure 8-1](#run_in_google_colab_2).'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ''Run in Google Colab'' button](Images/timl_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. The “Run in Google Colab” button
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As of this writing, there’s a bug in GitHub that results in intermittent error
    messages when displaying Jupyter notebooks. If you see the message “Sorry, something
    went wrong. Reload?” when trying to access the notebook, follow the instructions
    in [“Building Our Model”](ch04.xhtml#ch4_building_our_model).
  prefs: []
  type: TYPE_NORMAL
- en: 'This notebook will guide us through the process of training a model. It runs
    through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the correct dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring training using something called TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the training script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting the training output into a model we can use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable GPU training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.xhtml#chapter_hello_world_training), we trained a very simple
    model on a small amount of data. The model we are training now is a lot more sophisticated,
    has a much larger dataset, and will take a lot longer to train. On an average
    modern computer CPU, training it would take three or four hours.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the time it takes to train the model, we can use something called
    *GPU acceleration*. A *GPU*, or graphics processing unit. It’s a piece of hardware
    designed to help computers process image data quickly, allowing them to smoothly
    render things like user interfaces and video games. Most computers have one.
  prefs: []
  type: TYPE_NORMAL
- en: Image processing involves running a lot of tasks in parallel, and so does training
    a deep learning network. This means that it’s possible to use GPU hardware to
    accelerate deep learning training. It’s common for training to be 5 to 10 times
    faster when run on a GPU as opposed to a CPU.
  prefs: []
  type: TYPE_NORMAL
- en: The audio preprocessing required in our training process means that we won’t
    see quite such a massive speed-up, but our model will still train a lot faster
    on a GPU—it will take around one to two hours, total.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily for us, Colab supports training via GPU. It’s not enabled by default,
    but it’s easy to switch on. To do so, go to Colab’s Runtime menu, then click “Change
    runtime type,” as demonstrated in [Figure 8-2](#change_runtime_type).
  prefs: []
  type: TYPE_NORMAL
- en: '![The ''Change runtime type'' option in Colab](Images/timl_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. The “Change runtime type” option in Colab
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you select this option, the “Notebook settings” box shown in [Figure 8-3](#notebook_settings)
    opens.
  prefs: []
  type: TYPE_NORMAL
- en: '![The ''Notebook settings'' box](Images/timl_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. The “Notebook settings” box
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Select GPU from the “Hardware accelerator” drop-down list, as in [Figure 8-4](#hardware_accelerator),
    and then click SAVE.
  prefs: []
  type: TYPE_NORMAL
- en: '![The ''Hardware accelerator'' dropdown](Images/timl_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. The “Hardware accelerator” drop-down list
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Colab will now run its Python on a backend computer (referred to as a *runtime*)
    that has a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to configure the notebook with the words we’d like to train.
  prefs: []
  type: TYPE_NORMAL
- en: Configure training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training scripts are configured via a bunch of command-line flags that control
    everything from the model’s architecture to the words it will be trained to classify.
  prefs: []
  type: TYPE_NORMAL
- en: To make it easier to run the scripts, the notebook’s first cell stores some
    important values in environment variables. These will be substituted into the
    scripts’ command-line flags when they are run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one, `WANTED_WORDS`, allows us to select the words on which to train
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'By default the selected words are “yes” and “no,” but we can provide any combination
    of the following words, all of which appear in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Common commands: *yes*, *no*, *up*, *down*, *left*, *right*, *on*, *off*, *stop*,
    *go*, *backward*, *forward*, *follow*, *learn*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Digits zero through nine: *zero*, *one*, *two*, *three*, *four*, *five*, *six*,
    *seven*, *eight*, *nine*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Random words: *bed*, *bird*, *cat*, *dog*, *happy*, *house*, *Marvin*, *Sheila*,
    *tree*, *wow*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To select words, we can just include them in a comma-separated list. Let’s
    choose the words “on” and “off” to train our new model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Any words not included in the list will be grouped under the “unknown” category
    when the model is trained.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s fine to choose more than two words here; we would just need to tweak the
    application code slightly. We provide instructions on doing this in [“Using the
    Model in Our Project”](#micro_speech_using_model_in_project).
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice also the `TRAINING_STEPS` and `LEARNING_RATE` variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 3](ch03.xhtml#chapter_get_up_to_speed), we learned that a model’s
    weights and biases are incrementally adjusted so that over time, the output of
    the model gets closer to matching the desired value. `TRAINING_STEPS` refers to
    the number of times a batch of training data will be run through the network and
    its weights and biases updated. `LEARNING_RATE` sets the rate of adjustment.
  prefs: []
  type: TYPE_NORMAL
- en: With a high learning rate, the weights and biases are adjusted more with each
    iteration, meaning convergence happens fast. However, these big jumps mean that
    it’s more difficult to get to the ideal values because we might keep jumping past
    them. With a lower learning rate, the jumps are smaller. It takes more steps to
    converge, but the final result might be better. The best learning rate for a given
    model is determined through trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: In the aforementioned variables, the training steps and learning rate are defined
    as comma-separated lists that define the learning rate for each stage of training.
    With the values we just looked at, the model will train for 15,000 steps with
    a learning rate of 0.001, and then 3,000 steps with a learning rate of 0.0001\.
    The total number of steps will be 18,000.
  prefs: []
  type: TYPE_NORMAL
- en: This means we’ll do a bunch of iterations with a high learning rate, allowing
    the network to quickly converge. We’ll then do a smaller number of iterations
    with a low learning rate, fine-tuning the weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we’ll leave these values as they are, but it’s good to know what they
    are for. Run the cell. You’ll see the following output printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This gives a summary of how our model will be trained.
  prefs: []
  type: TYPE_NORMAL
- en: Install dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next up, we grab some dependencies that are necessary for running the scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the next two cells to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Install a specific version of the TensorFlow `pip` package that includes the
    ops required for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clone a corresponding version of the TensorFlow GitHub repository so that we
    can access the training scripts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load TensorBoard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To monitor the training process, we use [TensorBoard](https://oreil.ly/wginD).
    It’s a user interface that can show us graphs, statistics, and other insight into
    how training is going.
  prefs: []
  type: TYPE_NORMAL
- en: When training has completed, it will look something like the screenshot in [Figure 8-5](#tensorboard_complete).
    You’ll learn what all of these graphs mean later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of TensorBoard](Images/timl_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. A screenshot of TensorBoard after training is complete
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Run the next cell to load TensorBoard. It will appear in Colab, but it won’t
    show anything interesting until we begin training.
  prefs: []
  type: TYPE_NORMAL
- en: Begin training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following cell runs the script that begins training. You can see that it
    has a lot of command-line arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Some of these, like `--wanted_words=${WANTED_WORDS}`, use the environment variables
    we defined earlier to configure the model we’re creating. Others set up the output
    of the script, such as `--train_dir=/content/speech_commands_train`, which defines
    where the trained model will be saved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Leave the arguments as they are, and run the cell. You’ll begin to see some
    output stream past. It will pause for a few moments while the Speech Commands
    dataset is downloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When this is done, some more output will appear. There might be some warnings,
    which you can ignore as long as the cell continues running. At this point, you
    should scroll back up to TensorBoard, which should hopefully look something like
    [Figure 8-6](#tensorboard_start). If you don’t see any graphs, click the SCALARS
    tab.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of TensorBoard](Images/timl_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. A screenshot of TensorBoard at the beginning of training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Hooray! This means that training has begun. The cell you’ve just run will continue
    to execute for the duration of training, which will take up to two hours to complete.
    The cell won’t output any more logs, but data about the training run will appear
    in TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: You can see that TensorBoard shows two graphs, “accuracy” and “cross_entropy,”
    as shown in [Figure 8-7](#tensorboard_graphs). Both graphs show the current steps
    on the x-axis. The “accuracy” graph shows the model’s accuracy on its y-axis,
    which signals how much of the time it is able to detect a word correctly. The
    “cross_entropy” graph shows the model’s loss, which quantifies how far from the
    correct values the model’s predictions are.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of graphs in TensorBoard](Images/timl_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. The “accuracy” and “cross_entropy” graphs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Cross entropy is a common way of measuring loss in machine learning models that
    perform classification, for which the goal is to predict which category an input
    belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: The jagged lines on the graph correspond to performance on the training dataset,
    whereas the straight lines reflect performance on the validation dataset. Validation
    occurs periodically, so there are fewer validation datapoints on the graph.
  prefs: []
  type: TYPE_NORMAL
- en: New data will arrive in the graphs over time, but to show it, you need to adjust
    their scales to fit. You can do this by clicking the rightmost button under each
    graph, as shown in [Figure 8-8](#tensorboard_fit_button).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of an icon in TensorBoard](Images/timl_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. Click this button to adjust the graph’s scale to fit all available
    data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also click the button shown in [Figure 8-9](#tensorboard_expand_button)
    to make each graph larger.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of an icon in TensorBoard](Images/timl_0809.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. Click this button to enlarge the graph
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition to graphs, TensorBoard can show the inputs being fed into the model.
    Click the IMAGES tab, which displays a view similar to [Figure 8-10](#tensorboard_spectrogram).
    This is an example of a spectrogram that is being input to the model during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of the IMAGES tab in TensorBoard](Images/timl_0810.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. The IMAGES tab of TensorBoard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Wait for training to complete
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training the model will take between one and two hours, so our job now is to
    be patient. Fortunately for us, we have TensorBoard’s pretty graphs to keep us
    entertained.
  prefs: []
  type: TYPE_NORMAL
- en: As training progresses, you’ll notice that the metrics tend to jump around within
    a range. This is normal, but it makes the graphs appear fuzzy and difficult to
    read. To make it easier to see how training is going, we can use TensorFlow’s
    Smoothing feature.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-11](#tensorboard_smoothing_before) shows graphs with the default
    amount of smoothing applied; notice how fuzzy they are.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of graphs in TensorBoard](Images/timl_0811.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. Training graphs with the default amount of smoothing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By adjusting the Smoothing slider, shown in [Figure 8-12](#tensorboard_smoothing_slider),
    we can increase the amount of smoothing, making the trends more obvious.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot TensorBoard''s *Smoothing* slider](Images/timl_0812.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12\. TensorBoard’s Smoothing slider
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 8-13](#tensorboard_smoothing_after) shows the same graphs with a higher
    level of smoothing. The original data is visible in lighter colors, underneath.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of graphs in TensorBoard](Images/timl_0813.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13\. Training graphs with increased smoothing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keeping Colab running
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To prevent abandoned projects from consuming resources, Colab will shut down
    your runtime if it isn’t actively being used. Because our training will take a
    while, we need to prevent this from happening. There are a couple of things we
    need to think about.
  prefs: []
  type: TYPE_NORMAL
- en: First, if we’re not actively interacting with the Colab browser tab, the web
    user interface will disconnect from the backend runtime where the training scripts
    are being executed. This will happen after a few minutes, and will cause your
    TensorBoard graphs to stop updating with the latest training metrics. There’s
    no need to panic if this happens—your training is still running in the background.
  prefs: []
  type: TYPE_NORMAL
- en: If your runtime has disconnected, you’ll see a Reconnect button appear in Colab’s
    user interface, as shown in [Figure 8-14](#colab_reconnect_button). Click this
    button to reconnect your runtime.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of Colab''s Reconnect button](Images/timl_0814.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-14\. Colab’s Reconnect button
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A disconnected runtime is no big deal, but Colab’s next timeout deserves some
    attention. *If you don’t interact with Colab for 90 consecutive minutes, your
    runtime instance will be recycled*. This is a problem: you will lose all of your
    training progress, along with any data stored in the instance!'
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this happening, you just need to interact with Colab at least once
    every 90 minutes. Open the tab, make sure the runtime is connected, and take a
    look at your beautiful graphs. As long as you do this before 90 minutes have elapsed,
    the connection will stay open.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Even if your Colab tab is closed, the runtime will continue running in the background
    for up to 90 minutes. As long as you open the original URL in your browser, you
    can reconnect to the runtime and continue as before.
  prefs: []
  type: TYPE_NORMAL
- en: However, TensorBoard will disappear when the tab is closed. If training is still
    running when the tab is reopened, you will not be able to view TensorBoard again
    until training is complete.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, *a Colab runtime has a maximum lifespan of 12 hours*. If your training
    takes longer than 12 hours, you’re out of luck—Colab will shut down and reset
    your instance before training has a chance to complete. If your training is likely
    to run this long, you should avoid Colab and use one of the alternative solutions
    described in [“Other Ways to Run the Scripts”](#micro_speech_other_ways_to_run_training).
    Luckily, training our wake-word model won’t take anywhere near that long.
  prefs: []
  type: TYPE_NORMAL
- en: When your graphs show data for 18,000 steps, training is complete! We now must
    run a few more commands to prepare our model for deployment. Don’t worry—this
    part is *much* quicker.
  prefs: []
  type: TYPE_NORMAL
- en: Freeze the graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you learned earlier in this book, training is the process of iteratively
    tweaking a model’s weights and biases until it produces useful predictions. The
    training script writes these weights and biases to *checkpoint* files. A checkpoint
    is written once every hundred steps. This means that if training fails partway
    through, it can be restarted from the most recent checkpoint without losing progress.
  prefs: []
  type: TYPE_NORMAL
- en: The *train.py* script is called with an argument, `--train_dir`, which specifies
    where these checkpoint files will be written. In our Colab, it’s set to */content/speech_commands_train*.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the checkpoint files by opening Colab’s lefthand panel, which has
    a file browser. To do so, click the button shown in [Figure 8-15](#colab_sidebar_button).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of the button that opens Colab''s sidebar](Images/timl_0815.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-15\. The button that opens Colab’s sidebar
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this panel, click the Files tab to see the runtime’s filesystem. If you open
    the *speech_commands_train/* directory you’ll see the checkpoint files, as in
    [Figure 8-16](#colab_files_checkpoints). The number in each filename indicates
    the step at which the checkpoint was saved.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of Colab''s file browser showing a list of checkpoint files](Images/timl_0816.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-16\. Colab’s file browser showing a list of checkpoint files
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A TensorFlow model consists of two main things:'
  prefs: []
  type: TYPE_NORMAL
- en: The weights and biases resulting from training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A graph of operations that combine the model’s input with these weights and
    biases to produce the model’s output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this juncture, our model’s operations are defined in the Python scripts,
    and its trained weights and biases are in the most recent checkpoint file. We
    need to unite the two into a single model file with a specific format, which we
    can use to run inference. The process of creating this model file is called *freezing*—we’re
    creating a static representation of the graph with the weights *frozen* into it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To freeze our model, we run a script. You’ll find it in the next cell, in the
    “Freeze the graph” section. The script is called as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To point the script toward the correct graph of operations to freeze, we pass
    some of the same arguments we used in training. We also pass a path to the final
    checkpoint file, which is the one whose filename ends with the total number of
    training steps.
  prefs: []
  type: TYPE_NORMAL
- en: Run this cell to freeze the graph. The frozen graph will be output to a file
    named *tiny_conv.pb*.
  prefs: []
  type: TYPE_NORMAL
- en: This file is the fully trained TensorFlow model. It can be loaded by TensorFlow
    and used to run inference. That’s great, but it’s still in the format used by
    regular TensorFlow, not TensorFlow Lite. Our next step is to convert the model
    into the TensorFlow Lite format.
  prefs: []
  type: TYPE_NORMAL
- en: Convert to TensorFlow Lite
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Conversion is another easy step: we just need to run a single command. Now
    that we have a frozen graph file to work with, we’ll be using `toco`, the command-line
    interface for the TensorFlow Lite converter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the “Convert the model” section, run the first cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the arguments, we specify the model that we want to convert, the output location
    for the TensorFlow Lite model file, and some other values that depend on the model
    architecture. Because the model was quantized during training, we also provide
    some arguments (`inference_type`, `mean_values`, and `std_dev_values`) that instruct
    the converter how to map its low-precision values into real numbers.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why the `input_shape` argument has a leading `1` before
    the width, height, and channels parameters. This is the batch size; for efficiency
    during training, we send a lot of inputs in together, but when we’re running in
    a real-time application we’ll be working on only one sample at a time, which is
    why the batch size is fixed as `1`.
  prefs: []
  type: TYPE_NORMAL
- en: The converted model will be written to *tiny_conv.tflite*. Congratulations;
    this a fully formed TensorFlow Lite model!
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how tiny this model is, in the next cell, run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that the model is super small: `Model is 18208 bytes`.'
  prefs: []
  type: TYPE_NORMAL
- en: Our next step is to get this model into a form that we can deploy to microcontrollers.
  prefs: []
  type: TYPE_NORMAL
- en: Create a C array
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Back in [“Converting to a C File”](ch04.xhtml#hello_world_training_convert_to_c_file),
    we used the `xxd` command to convert a TensorFlow Lite model into a C array. We’ll
    do the same thing in the next cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The final part of the output will be the file’s contents, which are a C array
    and an integer holding its length, as follows (the exact values you see might
    be slightly different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This code is also written to a file, *tiny_conv.cc*, which you can download
    using Colab’s file browser. Because your Colab runtime will expire after 12 hours,
    it’s a good idea to download this file to your computer now.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll integrate this newly trained model with the `micro_speech` project
    so that we can deploy it to some hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Model in Our Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use our new model, we need to do three things:'
  prefs: []
  type: TYPE_NORMAL
- en: In [*micro_features/tiny_conv_micro_features_model_data.cc*](https://oreil.ly/EAR0U),
    replace the original model data with our new model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the label names in [*micro_features/micro_model_settings.cc*](https://oreil.ly/bqw67)
    with our new “on” and “off” labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the device-specific *command_responder.cc* to take the actions we want
    for the new labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replacing the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To replace the model, open *micro_features/tiny_conv_micro_features_model_data.cc*
    in your text editor.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re working with the Arduino example, the file will appear as a tab in
    the Arduino IDE. Its name will be *micro_features_tiny_conv_micro_features_model_data.cpp*.
    If you’re working with the SparkFun Edge, you can edit the files directly in your
    local copy of the TensorFlow repository. If you’re working with the STM32F746G,
    you should edit the files in your Mbed project directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *tiny_conv_micro_features_model_data.cc* file contains an array declaration
    that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You’ll need to replace the contents of the array as well as the value of the
    constant `g_tiny_conv_micro_features_model_data_len`, if it has changed.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, open the *tiny_conv.cc* file that you downloaded at the end of the
    previous section. Copy and paste the contents of the array, but not its definition,
    into the array defined in *tiny_conv_micro_features_model_data.cc*. Make sure
    you are overwriting the array’s contents, but not its declaration.
  prefs: []
  type: TYPE_NORMAL
- en: At the bottom of *tiny_conv.cc* you’ll find `_content_tiny_conv_tflite_len`,
    a variable whose value represents the length of the array. Back in *tiny_conv_micro_features_model_data.cc*,
    replace the value of `g_tiny_conv_micro_features_model_data_len` with the value
    of this variable. Then save the file; you’re done updating it.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the Labels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, open *micro_features/micro_model_settings.cc*. This file contains an
    array of class labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To adjust this for our new model, we can just swap the “yes” and “no” for “on”
    and “off.” We match labels with the model’s output tensor elements by order, so
    it’s important to list these in the same order in which they were provided to
    the training script.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the expected code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If you trained a model with more than two labels, just add them all to the list.
  prefs: []
  type: TYPE_NORMAL
- en: We’re now done switching over the model. The only remaining step is to update
    any output code that uses the labels.
  prefs: []
  type: TYPE_NORMAL
- en: Updating command_responder.cc
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The project contains a different device-specific implementation of *command_responder.cc*
    for the Arduino, SparkFun Edge, and STM32F746G. We show how to update each of
    these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Arduino
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Arduino command responder, located in *arduino/command_responder.cc*, lights
    an LED for 3 seconds when it hears the word “yes.” Let’s update it to light the
    LED when it hears either “on” or “off.” In the file, locate the following `if`
    statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `if` statement tests whether the first letter of the command is “y,” for
    “yes.” If we change this “y” to an “o,” the LED will be lit for either “on” or
    “off,” because they both begin with “o”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: After you’ve made these code changes, deploy to your device and give it a try.
  prefs: []
  type: TYPE_NORMAL
- en: SparkFun Edge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The SparkFun Edge command responder, located in *sparkfun_edge/command_responder.cc*,
    lights up a different LED depending on whether it heard “yes” or “no.” In the
    file, locate the following `if` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s simple to update these so that “on” and “off” each turn on different LEDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Because both commands begin with the same letter, we need to look at their second
    letters to disambiguate them. Now, the yellow LED will light when “on” is spoken,
    and the red will light for “off.”
  prefs: []
  type: TYPE_NORMAL
- en: When you’re finished making the changes, deploy and run the code using the same
    process you followed in [“Running the example”](ch07.xhtml#micro_speech_sparkfun_edge_running_the_example).
  prefs: []
  type: TYPE_NORMAL
- en: STM32F746G
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The STM32F746G command responder, located in *disco_f746ng/command_responder.cc*,
    displays a different word depending on which command it heard. In the file, locate
    the following `if` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s easy to update this so that it responds to “on” and “off,” instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Again, because both commands begin with the same letter, we look at their second
    letters to disambiguate them. Now we display the appropriate text for each command.
  prefs: []
  type: TYPE_NORMAL
- en: Other Ways to Run the Scripts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you’re not able to use Colab, there are two other recommended ways to train
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: In a cloud virtual machine (VM) with a GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On your local workstation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The drivers necessary for GPU-based training are available only on Linux. Without
    Linux, training will take around four hours. For this reason, it’s recommended
    to use either a cloud VM with a GPU, or a similarly equipped Linux workstation.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your VM or workstation is beyond the scope of this book. However,
    we do have some recommendations. If you’re using a VM, you can launch a [Google
    Cloud Deep Learning VM Image](https://oreil.ly/PVRtP), which is preconfigured
    with all of the dependencies you’ll need for GPU training. If you’re using a Linux
    workstation, the [TensorFlow GPU Docker image](https://oreil.ly/PFYVr) has everything
    you’ll need.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the model, you need to install a nightly build of TensorFlow. To uninstall
    any existing version and replace it with one that is confirmed to work, use the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, open a command line and change to a directory you use to store code.
    Use the following commands to clone TensorFlow and open a specific commit that
    is confirmed to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can run the *train.py* script to train the model. This will train a
    model to recognize “yes” and “no,” and output the checkpoint files to */tmp*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After training, run the following script to freeze the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, convert the model to the TensorFlow Lite format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, convert the file into a C source file that you can compile into an
    embedded system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How the Model Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know how to train your own model, let’s explore how it works. So
    far, we’ve treated the machine learning model as a black box—something that we
    feed training data into, and eventually it figures out how to predict results.
    It’s not essential to understand what’s happening under the hood to use the model,
    but it can be helpful for debugging problems, and it’s interesting in its own
    right. This section gives you some insights into how the model comes up with its
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Inputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 8-17](#tensorboard_spectrogram_2) illustrates what is actually being
    fed into the neural network. This is a 2D array with a single channel, so we can
    visualize it as a monochrome image. We’re working with 16 KHz audio sample data,
    so how do we get to this representation from that source? The process is an example
    of what’s known as “feature generation” in machine learning, and the goal is to
    turn an input format that’s more difficult to work with (in this case 16,000 numerical
    values representing a second of audio) into something that’s easier for a machine
    learning model to make sense of. You might not have encountered this if you’ve
    previously studied machine vision use cases for deep learning, because it happens
    that images are usually comparatively easy for a network to take as inputs without
    much preprocessing; but in a lot of other domains, like audio and natural language
    processing, it’s still common to transform the input before feeding it into a
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of the IMAGES tab in TensorBoard](Images/timl_0810.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-17\. The IMAGES tab of TensorBoard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To develop an intuition for why it’s easier for our model to deal with preprocessed
    input, let’s look at the original raw representations of some audio recordings,
    as presented in Figures [8-18](#audio_waveform_yes) through [8-21](#audio_waveform_no_1).
  prefs: []
  type: TYPE_NORMAL
- en: '![A visualization of 16,000 audio samples of someone saying ''yes''.](Images/timl_0818.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-18\. Waveform of an audio recording of someone saying “yes”
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![A visualization of 16,000 audio samples of someone saying ''no''.](Images/timl_0819.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-19\. Waveform of an audio recording of someone saying “no”
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![A visualization of 16,000 audio samples of someone saying ''yes''.](Images/timl_0820.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-20\. Another waveform of an audio recording of someone saying “yes”
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![A visualization of 16,000 audio samples of someone saying ''no''.](Images/timl_0821.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-21\. Another waveform of an audio recording of someone saying “no”
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Without the labels, you’d have trouble distinguishing which pairs of waveforms
    represented the same words. Now look at Figures [8-22](#features_yes_0) through
    [8-25](#features_no_1), which shows the result of running those same one-second
    recordings through feature generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![A visualization of 16,000 audio samples of someone saying ''yes''.](Images/timl_0822.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-22\. Spectrogram of an audio recording of someone saying “yes”
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![A visualization of 16,000 audio samples of someone saying ''no''.](Images/timl_0823.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-23\. Spectrogram of an audio recording of someone saying “no”
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![A visualization of 16,000 audio samples of someone saying ''yes''.](Images/timl_0824.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-24\. Another spectrogram of an audio recording of someone saying “yes”
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![A visualization of 16,000 audio samples of someone saying ''no''.](Images/timl_0825.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-25\. Another spectrogram of an audio recording of someone saying “no”
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These still aren’t simple to interpret, but hopefully you can see that both
    of the “yes” spectrograms have a shape a bit like an inverted L, and the “no”
    features show a different shape. We can discern the difference between spectrograms
    more easily than raw waveforms, and hopefully it’s intuitive that it is easier
    for models to do the same.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect to this is that the generated spectrograms are a lot smaller
    than the sample data. Each spectrogram consist of 1,960 numeric values, whereas
    the waveform has 16,000\. They are a summary of the audio data, which reduces
    the amount of work that the neural network must do. It is in fact possible for
    a specifically designed model, like [DeepMind’s WaveNet](https://oreil.ly/IH9J3),
    to take raw sample data as its input instead, but the resulting models tend to
    involve more computation than the combination of a neural network fed with hand-engineered
    features that we’re using, so for resource-constrained environments like embedded
    systems, we prefer the approach used here.
  prefs: []
  type: TYPE_NORMAL
- en: How Does Feature Generation Work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’ve had experience working with audio processing, you might be familiar
    with approaches like [mel-frequency cepstral coefficients (MFCCs)](https://oreil.ly/HTAev).
    This is a common approach to generating the kind of spectrograms we’re working
    with, but our example actually uses a related but different approach. It’s the
    same method used in production across Google, which means that it has had a lot
    of practical validation, but it hasn’t been published in the research literature.
    Here, we describe roughly how it works, but for the details the best reference
    is [the code itself](https://oreil.ly/NeOnW).
  prefs: []
  type: TYPE_NORMAL
- en: The process begins by generating a Fourier transform, (also known as a fast
    Fourier transform or FFT) for a given time slice—in our case 30 ms of audio data.
    This FFT is generated on data that’s been filtered with a [Hann window](https://oreil.ly/jhn8c),
    a bell-shaped function that reduces the influence of samples at either end of
    the 30-ms window. A Fourier transform produces complex numbers with real and imaginary
    components for every frequency, but all we care about is the overall energy, so
    we sum the squares of the two components and then apply a square root to get a
    magnitude for each frequency bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Given *N* samples, a Fourier transform produces information on *N*/2 frequencies.
    30 ms at a rate of 16,000 samples per second requires 480 samples, and because
    our FFT algorithm needs a power of two input, we pad that with zeros to 512 samples,
    giving us 256 frequency buckets. This is larger than we need, so to shrink it
    down we average adjacent frequencies into 40 downsampled buckets. This downsampling
    isn’t linear, though; instead, it uses the human perception–based mel frequency
    scale to give more weight to lower frequencies so that there are more buckets
    available for them, and higher frequencies are merged into broader buckets. [Figure 8-26](#feature_generation)
    presents a diagram of that process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of the feature generation process.](Images/timl_0826.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-26\. Diagram of the feature-generation process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One unusual aspect of this feature generator is that it then includes a noise
    reduction step. This works by keeping a running average of the value in each frequency
    bucket and then subtracting this average from the current value. The idea is that
    background noise will be fairly constant over time and show up in particular frequencies.
    By subtracting the running average, we have a good chance of removing some of
    the effect of that noise and leaving the more rapidly changing speech that we’re
    interested in intact. The tricky part is that the feature generator does retain
    state to track the running averages for each bucket, so if you’re trying to reproduce
    the same spectrogram output for a given input—like we try to [for testing](https://oreil.ly/HtPve)—you
    will need to reset that state to the correct values.
  prefs: []
  type: TYPE_NORMAL
- en: Another part of the noise reduction that initially surprised us was its use
    of different coefficients for the odd and even frequency buckets. This results
    in the distinctive comb-tooth patterns that you can see in the final generated
    feature images (Figures [8-22](#features_yes_0) through [8-25](#features_no_1)).
    Initially we thought this was a bug, but on talking to the original implementors,
    we learned that it was actually added deliberately to help performance. There’s
    an extended discussion of this approach in section 4.3 of the [“Trainable Frontend
    for Robust and Far-Field Keyword Spotting”](https://oreil.ly/QZ4Yb), by Yuxuan
    Wang et al. which also includes the background to some of the other design decisions
    that went into this feature generation pipeline. We also tested it empirically
    with our model, and removing the difference in the treatment of odd and even buckets
    did noticeably reduce accuracy in evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: We then use per-channel amplitude normalization (PCAN) auto-gain to boost the
    signal based on the running average noise. Finally, we apply a log scale to all
    the bucket values, so that relatively loud frequencies don’t drown out quieter
    portions of the spectrum—a normalization that helps the subsequent model work
    with the features.
  prefs: []
  type: TYPE_NORMAL
- en: This process is repeated 49 times in total, with a 30-ms window that’s moved
    forward 20 ms each time between iterations, to cover the full one second of audio
    input data. This produces a 2D array of values that’s 40 elements wide (one for
    each frequency bucket) and 49 rows high (one row for each time slice).
  prefs: []
  type: TYPE_NORMAL
- en: If this all sounds very complicated to implement, don’t worry. Because the code
    that implements it is all open source, you’re welcome to reuse it in your own
    audio projects.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Model Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The neural network model we’re using is defined as a small graph of operations.
    You can find the code that defines it at training time in the [`create_tiny_conv_model()`
    function](https://oreil.ly/fMARv), and [Figure 8-27](#model_visualization) presents
    a visualization of the result.
  prefs: []
  type: TYPE_NORMAL
- en: This model consists of a convolutional layer, followed by a fully connected
    layer, and then a softmax layer at the end. In the figure the convolutional layer
    is labeled as “DepthwiseConv2D,” but this is just a quirk of the TensorFlow Lite
    converter (it turns out that a convolutional layer with a single-channel input
    image can also be expressed as a depthwise convolution). You’ll also see a layer
    labeled “Reshape_1,” but this is just an input placeholder rather than a real
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![A visualization the speech recognition model as a graph.](Images/timl_0827.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-27\. Graph visualization of the speech recognition model, courtesy
    of [the Netron tool](https://oreil.ly/UiuXU)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Convolutional layers are used for spotting 2D patterns in input images. Each
    filter is a rectangular array of values that is moved as a sliding window across
    the input, and the output image is a representation of how closely the input and
    filter match at every point. You can think of the convolution operation as moving
    a series of rectangular filters across the image, with the result at each pixel
    for each filter corresponding to how similar the filter is to that patch in the
    image. In our case, each filter is 8 pixels wide and 10 high, and there are 8
    of them in total. Figures [8-28](#filter0) through [8-35](#filter7) show what
    they look like.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization of convolutional filter.](Images/timl_0828.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-28\. First filter image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Visualization of convolutional filter.](Images/timl_0829.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-29\. Second filter image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Visualization of convolutional filter.](Images/timl_0830.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-30\. Third filter image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Visualization of convolutional filter.](Images/timl_0831.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-31\. Fourth filter image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Visualization of convolutional filter.](Images/timl_0832.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-32\. Fifth filter image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Visualization of convolutional filter.](Images/timl_0833.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-33\. Sixth filter image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Visualization of convolutional filter.](Images/timl_0834.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-34\. Seventh filter image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Visualization of convolutional filter.](Images/timl_0835.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-35\. Eighth filter image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can think of each of these filters as a small patch of the input image.
    The operation is trying to match this small patch to parts of the input image
    that look similar. Where the image is similar to the patch, a high value will
    be written into the corresponding part of the output image. Intuitively, each
    filter is a pattern that the model has learned to look for in the training inputs
    to help it distinguish between the different classes that it has to deal with.
  prefs: []
  type: TYPE_NORMAL
- en: Because we have eight filters, there will be eight different output images,
    each corresponding to the respective filter’s match value as it’s slid across
    the input. These filter outputs are actually combined into a single output image
    with eight channels. We have set the stride to be two in both directions, which
    means we slide each filter by two pixels each time, rather than just by one. Because
    we’re skipping every other position, this means our output image is half the size
    of the input.
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the visualization that the input image is 49 pixels high and
    40 wide, with a single channel, which is what we’d expect given the feature spectrograms
    we discussed in the previous section. Because we’re skipping every other pixel
    in the horizontal and vertical directions when we slide the convolutional filters
    across the input, the output of the convolution is half the size, or 25 pixels
    high and 20 wide. There are eight filters though, so the image becomes eight channels
    deep.
  prefs: []
  type: TYPE_NORMAL
- en: The next operation is a fully connected layer. This is a different kind of pattern
    matching process. Instead of sliding a small window across the input, there’s
    a weight for every value in the input tensor. The result is an indication of how
    closely the input matches the weights, after comparing every value. You can think
    of this as a global pattern match, where you have an ideal result that you’d expect
    to get as an input, and the output is how close that ideal (held in the weights)
    is to the actual input. Each class in our model has its own weights, so there’s
    an ideal pattern for “silence,” “unknown,” “yes,” and “no,” and four output values
    are generated. There are 4,000 values in the input `(25 * 20 * 8)`, so each class
    is represented by 4,000 weights.
  prefs: []
  type: TYPE_NORMAL
- en: The last layer is a softmax. This effectively helps increase the difference
    between the highest output and its nearest competitors, which doesn’t change their
    relative order (whichever class produced the largest value from the fully connected
    layer will remain the highest) but does help produce a more useful score. This
    score is often informally referred to as a *probability*, but strictly speaking
    you can’t reliably use it like that without more calibration on what the mix of
    input data actually is. For example, if you had more words in the detector, it’s
    likely that an uncommon one like “antidisestablishmentarianism” would be less
    likely to show up than something like “okay,” but depending on the distribution
    of the training data that might not be reflected in the raw scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'As well as these major layers, there are biases that are added on to the results
    of the fully connected and convolutional layers to help tweak their outputs, and
    a rectified linear unit (ReLU) activation function after each. The ReLU just makes
    sure that no output is less than zero, setting any negative results to a minimum
    of zero. This type of activation function was one of the breakthroughs that enabled
    deep learning to become much more effective: it helps the training process converge
    much more quickly than the network would otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Model Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The end result of the model is the output of the softmax layer. This is four
    numbers, one for each of “silence,” “unknown,” “yes,” and “no.” These values are
    the scores for each category, and the one with the highest score is the model’s
    prediction, with the score representing the confidence the model has in its prediction.
    As an example, if the model output is `[10, 4, 231, 80]`, it’s predicting that
    the third category, “yes,” is the most likely result with a score of 231\. (We’re
    giving these values in their quantized forms, between 0 and 255, but because these
    are just relative scores it’s not usually necessary to convert them back to their
    real-valued equivalents.)
  prefs: []
  type: TYPE_NORMAL
- en: One thing that’s tricky is that this result is based on analyzing the entire
    last second of audio. If we run it only once per second, we might end up with
    an utterance that is half in the previous second, and half in the current. It’s
    not possible for any model to do a good job recognizing a word when it hears only
    a part of it, so in that case the word spotting would fail. To overcome this,
    we need to run the model more often than once per second to give us as high a
    chance as possible of catching an entire word in our one-second window. In practice,
    we’ve found we have to run it 10 or 15 times per second to achieve good results.
  prefs: []
  type: TYPE_NORMAL
- en: If we’re getting all of these results coming in so fast, how do we decide when
    a score is high enough? We implement a postprocessing class that averages the
    scores over time and triggers a recognition only when we’ve had several high scores
    for the same word in a short amount of time. You can see the implementation of
    this in the [`RecognizeCommands class`](https://oreil.ly/FuYfL). This is fed the
    raw results from the model, and then it uses an accumulation and averaging algorithm
    to determine whether any of the categories have crossed the threshold. These postprocessed
    results are then fed to the [`CommandResponder`](https://oreil.ly/b8ArK) to take
    an action, depending on the platform’s output capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The model parameters are all learned from the training data, but the algorithm
    used by the command recognizer was manually created, so all of the [thresholds](https://oreil.ly/tfNfr)—like
    the score value required to trigger a recognition, or the time window of positive
    results needed—have been hand-picked. This means that there’s no guarantee they
    are optimal, so if you’re seeing poor results in your own application, you might
    want to try tweaking them yourself.
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated speech recognition models typically use a model that’s able
    to take in streaming data (like a recursive neural network) rather than the single-layer
    convolutional network we show in this chapter. Having the streaming baked into
    the model design means that you don’t need to do the postprocessing to get accurate
    results, though it does make the training significantly more complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Training with Your Own Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s not very likely that the product you want to build only needs to respond
    to “yes” and “no,” so you’ll want to train a model that is sensitive to the audio
    you care about. The training script we used earlier has been designed to let you
    create custom models using your own data. The toughest part of the process is
    usually gathering a large enough dataset, and ensuring that it’s appropriate for
    your problem. We discuss general approaches to data gathering and cleaning in
    [Chapter 16](ch16.xhtml#chapter_optimizing_energy_usage), but this section covers
    some of the ways in which you can train your own audio model.
  prefs: []
  type: TYPE_NORMAL
- en: The Speech Commands Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *train.py* script downloads the Speech Commands dataset by default. This
    is an open source collection of more than 100,000 one-second WAV files, covering
    a variety of short words from a lot of different speakers. It’s distributed by
    Google, but the utterances have been collected from volunteers around the world.
    [“Visual Wake Words Dataset”](https://oreil.ly/EC6nd) by Aakanksha Chowdhery et
    al. provides more details.
  prefs: []
  type: TYPE_NORMAL
- en: As well as *yes* and *no*, the dataset includes eight other command words (*on*,
    *off*, *up*, *down*, *left*, *right*, *stop* and *go*), and the 10 digits from
    *zero* through *nine*. There are several thousand examples of each of these words.
    There are also other words, like *Marvin*, that have a lot fewer examples each.
    The command words are intended to have enough utterances that you can train a
    reasonable model to recognize them. The other words are intended to be used to
    populate an *unknown* category, so a model can spot when a word it’s not been
    trained on is uttered, instead of mistaking it for a command.
  prefs: []
  type: TYPE_NORMAL
- en: Because the training script uses this dataset, you can easily train a model
    on a combination of some of the command words that have lots of examples. If you
    update the `--wanted_words` argument with a comma-separated list of words present
    in the training set and run training from scratch, you should find you can create
    a useful model. The main things to watch out for are that you are restricting
    yourself to the 10 command words and/or digits, or you won’t have enough examples
    to train accurately, and that you adjust the `--silence_percentage` and `--unknown_percentage`
    values down if you have more than two wanted words. These last two arguments control
    how many silent and unknown samples are mixed in during training. The *silent*
    examples aren’t actually complete silence; instead, they’re randomly selected
    one-second snippets of recorded background noise, pulled from the WAVs in the
    *background* folder of the dataset. The *unknown* samples are utterances picked
    from any of the words that are in the training set, but aren’t in the `wanted_words`
    list. This is why we have a selection of miscellaneous words in the dataset with
    comparatively few utterances each; it gives us the chance to recognize that a
    lot of different words aren’t actually the ones we’re looking for. This is a particular
    problem with speech and audio recognition, because our products often need to
    operate in environments in which there are a lot of words and noises we might
    never have encountered in training. There are many thousands of different words
    that could show up just in common English, and to be useful, a model must be able
    to ignore those on which it hasn’t been trained. That’s why the *unknown* category
    is so important in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of training on different words using the existing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Training on Your Own Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The default for the training script is to use Speech Commands, but if you have
    your own dataset, you can use the `--data_dir` argument to use it, instead. The
    directory you’re pointing to should be organized like Speech Commands, with one
    subfolder per class that you want to recognize, each containing a set of WAV files.
    You should also have a special *background* subfolder that contains longer WAV
    recordings of the kind of background noise you expect your application to encounter.
    You’ll also need to pick a recognition duration if the default of one second doesn’t
    work for your use case, and specify it through the `--sample_duration_ms` argument.
    Then you can set the classes that you want to recognize using the `--wanted_words`
    argument. Despite the name, these classes can be any kind of audio event, from
    breaking glass to laughter; as long as you have enough WAVs of each class the
    training process should work just as it does for speech.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you had folders of WAVs named *glass* and *laughter* inside a root */tmp/my_wavs*
    directory, here’s how you could train your own model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The most difficult part often is finding enough data. As an example, it turns
    out that the real sound of breaking glass is very different from the sound effects
    we’re used to hearing in movies. This means that you need to either find existing
    recordings, or arrange to record some yourself. Because the training process can
    require many thousand examples of each class, and they need to cover all of the
    variations that are likely to occur in a real application, this data-gathering
    process can be frustrating, expensive, and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: A common solution for this with image models is to use *transfer learning*,
    where you take a model that’s been trained on a large public dataset and fine-tune
    its weights on different classes using other data. This approach doesn’t require
    nearly as many examples in the secondary dataset as you would need if you were
    training from scratch with it, and it often produces high-accuracy results. Unfortunately
    transfer learning for speech models is still being researched, but watch this
    space.
  prefs: []
  type: TYPE_NORMAL
- en: How to Record Your Own Audio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you need to capture audio of words you care about, it’s a lot easier if you
    have a tool that prompts speakers and splits the result into labeled files. The
    Speech Commands dataset was recorded using the [Open Speech Recording app](https://oreil.ly/UWsG3),
    a hosted app that lets users record utterances through most common web browsers.
    As a user, you’ll see a web page that first asks you to agree to being recorded,
    with a default Google agreement, that’s [easily changeable](https://oreil.ly/z5vka).
    After you have agreed, you’re sent to a new page that has recording controls.
    When you press the record button, words will appear as prompts, and the audio
    you say for each word is recorded. When all of the requested words have been recorded,
    you’ll be asked to submit the results to the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are instructions in the README for running it on Google Cloud, but it’s
    a Flask app written in Python, so you should be able to port it to other environments.
    If you are using Google Cloud, you’ll need to update the [*app.yaml*](https://oreil.ly/dV2kv)
    file to point to your own storage bucket and supply your own random session key
    (this is used just for hashing, so it can be any value). To customize which words
    are recorded, you’ll need to edit a couple of arrays in [the client-side JavaScript](https://oreil.ly/XcJIe):
    one for the frequently repeated main words, and one for the secondary fillers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The recorded files are stored as OGG compressed audio in the Google Cloud bucket,
    but training requires WAVs, so you need to convert them. It’s also likely that
    some of your recordings contain errors, like people forgetting to say the word
    or saying it too quietly, so it’s helpful to automatically filter out those mistakes
    where possible. If you have set up your bucket name in a `BUCKET_NAME` variable,
    you can begin by copying your files to a local machine by using these bash commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'One nice property of the compressed OGG format is that quiet or silent audio
    results in very small files, so a good first step is removing any that are particularly
    tiny, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The easiest way we’ve found to convert OGGs to WAVs is using the [FFmpeg project](https://ffmpeg.org/),
    which offers a command-line tool. Here are a set of commands that can convert
    an entire directory of OGG files into the format we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The Open Speech Recording application records more than one second for each
    word. This ensures that the user’s utterance is captured even if their timing
    is a bit earlier or later than we expect. The training requires one-second recordings,
    and it works best if the word is centered in the middle of each recording. We’ve
    created a small open source utility to look at the volume of each recording over
    time to try to get the centering right and trim the audio so that it is just one
    second. Enter the following commands in your terminal to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This will give you a folder full of files in the correct format and of the required
    length, but the training process needs the WAVs organized into subfolders by labels.
    The label is encoded in the name of each file, so we have [an example Python script](https://oreil.ly/BpQBJ)
    that uses those filenames to sort them into the appropriate folders.
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data augmentation is another method to effectively enlarge your training data
    and improve accuracy. In practice, this means taking recorded utterances and applying
    audio transformations to them before they’re used for training. These transforms
    can include altering the volume, mixing in background noise, or trimming the start
    or end of the clips slightly. The training script applies all of these transformations
    by default, but you can adjust how often they’re used and how strongly they’re
    applied using command-line arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This kind of augmentation does help make a small dataset go further, but it
    can’t work miracles. If you apply transformations too strongly, you can end up
    distorting the training inputs so much that they’d no longer be recognizable by
    a person, which can cause the model to mistakenly start triggering on sounds that
    bear no resemblance to the intended categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how you can use some of those command-line arguments to control the
    augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Model Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The “yes"/"no” model we trained earlier was designed to be small and fast.
    It’s only 18 KB, and requires 400,000 arithmetic operations to execute once. To
    fit within those constraints, it trades off accuracy. If you’re designing your
    own application, you might want to make different trade-offs, especially if you’re
    trying to recognize more than two categories. You can specify your own model architectures
    by modifying the *models.py* file and then using the `--model_architecture` argument.
    You’ll need to write your own model creation function, like `create_tiny_conv_model0`
    but with the layers you want in your model specified instead. Then, you can update
    the `if` statement in `create_model0` to give your architecture a name, and call
    your new creation function when it’s passed in as the architecture argument on
    the command line. You can look at some of the existing creation functions for
    inspiration, including how to handle dropout. If you have added your own model
    code, here’s how you can call it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recognizing spoken words with a small memory footprint is a tricky real-world
    problem, and tackling it requires us to work with many more components than we
    need to for a simpler example. Most production machine learning applications require
    thinking about issues like feature generation, model architecture choices, data
    augmentation, finding the best-suited training data, and how to turn the results
    of a model into actionable information.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of trade-offs to consider depending on the actual requirements
    of your product, and hopefully you now understand some of the options you have
    as you try to move from training into deployment.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we explore how to run inference with a different type of
    data that, although *seemingly* more complex than audio, is surprisingly easy
    to work with.
  prefs: []
  type: TYPE_NORMAL
