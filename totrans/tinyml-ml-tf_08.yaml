- en: 'Chapter 8\. Wake-Word Detection: Training a Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。唤醒词检测：训练模型
- en: In [Chapter 7](ch07.xhtml#chapter_speech_wake_word_example), we built an application
    around a model trained to recognize “yes” and “no.” In this chapter, we will train
    a *new* model that can recognize different words.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.xhtml#chapter_speech_wake_word_example)中，我们围绕一个训练有能力识别“是”和“否”的模型构建了一个应用程序。在本章中，我们将训练一个*新*模型，可以识别不同的单词。
- en: Our application code is fairly general. All it does is capture and process audio,
    feed it into a TensorFlow Lite model, and do something based on the output. It
    mostly doesn’t care which words the model is looking for. This means that if we
    train a new model, we can just drop it into our application and it should work
    right away.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用代码相当通用。它只是捕获和处理音频，将其输入到TensorFlow Lite模型中，并根据输出执行某些操作。它大多数情况下不关心模型正在寻找哪些单词。这意味着如果我们训练一个新模型，我们可以直接将其放入我们的应用程序中，它应该立即运行。
- en: 'Here are the things we need to consider when training a new model:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练新模型时，我们需要考虑以下事项：
- en: Input
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: The new model must be trained on input data that is the same shape and format,
    with the same preprocessing as our application code.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 新模型必须在与我们的应用代码相同形状和格式的输入数据上进行训练，具有相同的预处理。
- en: Output
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: 'The output of the new model must be in the same format: a tensor of probabilities,
    one for each class.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 新模型的输出必须采用相同的格式：每个类别一个概率张量。
- en: Training data
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据
- en: Whichever new words we pick, we’ll need many recordings of people saying them
    so that we can train our new model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们选择哪些新单词，我们都需要很多人说这些单词的录音，这样我们才能训练我们的新模型。
- en: Optimization
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 优化
- en: The model must be optimized to run efficiently on a microcontroller with limited
    memory.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模型必须经过优化，以在内存有限的微控制器上高效运行。
- en: Fortunately for us, our existing model was trained using a publicly available
    script that was published by the TensorFlow team, and we can use this script to
    train a new model. We also have access to a free dataset of spoken audio that
    we can use as training data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们现有的模型是使用由TensorFlow团队发布的公开可用脚本进行训练的，我们可以使用这个脚本来训练一个新模型。我们还可以访问一个免费的口语音频数据集，可以用作训练数据。
- en: In the next section, we’ll walk through the process of training a model with
    this script. Then, in [“Using the Model in Our Project”](#micro_speech_using_model_in_project),
    we’ll incorporate the new model into our existing application code. After that,
    in [“How the Model Works”](#micro_speech_how_the_model_works), you’ll learn how
    the model actually works. Finally, in [“Training with Your Own Data”](#micro_speech_training_with_your_own_data),
    you’ll see how to train a model using your own dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将详细介绍使用此脚本训练模型的过程。然后，在[“在我们的项目中使用模型”](#micro_speech_using_model_in_project)中，我们将把新模型整合到我们现有的应用程序代码中。之后，在[“模型的工作原理”](#micro_speech_how_the_model_works)中，您将了解模型的实际工作原理。最后，在[“使用您自己的数据进行训练”](#micro_speech_training_with_your_own_data)中，您将看到如何使用您自己的数据集训练模型。
- en: Training Our New Model
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们的新模型
- en: The model we are using was trained with the TensorFlow [Simple Audio Recognition](https://oreil.ly/E292V)
    script, an example script designed to demonstrate how to build and train a model
    for audio recognition using TensorFlow.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用的模型是使用TensorFlow [Simple Audio Recognition](https://oreil.ly/E292V)脚本进行训练的，这是一个示例脚本，旨在演示如何使用TensorFlow构建和训练用于音频识别的模型。
- en: 'The script makes it very easy to train an audio recognition model. Among other
    things, it allows us to do the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本使训练音频识别模型变得非常容易。除其他事项外，它还允许我们执行以下操作：
- en: Download a dataset with audio featuring 20 spoken words.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载一个包含20个口语单词的音频数据集。
- en: Choose which subset of words to train the model on.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择要训练模型的单词子集。
- en: Specify what type of preprocessing to use on the audio.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定在音频上使用哪种类型的预处理。
- en: Choose from several different types of model architecture.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择几种不同类型的模型架构。
- en: Optimize the model for microcontrollers using quantization.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用量化将模型优化为微控制器。
- en: When we run the script, it downloads the dataset, trains a model, and outputs
    a file representing the trained model. We then use some other tools to convert
    this file into the correct form for TensorFlow Lite.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行脚本时，它会下载数据集，训练模型，并输出代表训练模型的文件。然后我们使用其他工具将此文件转换为适合TensorFlow Lite的正确形式。
- en: Note
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It’s common for model authors to create these types of training scripts. It
    allows them to easily experiment with different variants of model architectures
    and hyperparameters, and to share their work with others.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 模型作者通常会创建这些类型的训练脚本。这使他们能够轻松地尝试不同变体的模型架构和超参数，并与他人分享他们的工作。
- en: The easiest way to run the training script is within a Colaboratory (Colab)
    notebook, which we do in the following section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 运行训练脚本的最简单方法是在Colaboratory（Colab）笔记本中进行，我们将在下一节中进行。
- en: Training in Colab
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Colab中训练
- en: Google Colab is a great place to train models. It provides access to powerful
    computing resources in the cloud, and it comes set up with tools that we can use
    to monitor the training process. It’s also completely free.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab是一个很好的训练模型的地方。它提供了云中强大的计算资源，并且设置了我们可以用来监视训练过程的工具。而且它完全免费。
- en: Over the course of this section, we will use a Colab notebook to train our new
    model. The notebook we use is available in the TensorFlow repository.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Colab笔记本来训练我们的新模型。我们使用的笔记本可以在TensorFlow存储库中找到。
- en: '[Open the notebook](https://oreil.ly/0Z2ra) and click the “Run in Google Colab”
    button, as shown in [Figure 8-1](#run_in_google_colab_2).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[打开笔记本](https://oreil.ly/0Z2ra)并单击“在Google Colab中运行”按钮，如[图8-1](#run_in_google_colab_2)所示。'
- en: '![The ''Run in Google Colab'' button](Images/timl_0403.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![在Google Colab中运行按钮](Images/timl_0403.png)'
- en: Figure 8-1\. The “Run in Google Colab” button
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1。在Google Colab中运行按钮
- en: Tip
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: As of this writing, there’s a bug in GitHub that results in intermittent error
    messages when displaying Jupyter notebooks. If you see the message “Sorry, something
    went wrong. Reload?” when trying to access the notebook, follow the instructions
    in [“Building Our Model”](ch04.xhtml#ch4_building_our_model).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，GitHub存在一个错误，导致在显示Jupyter笔记本时出现间歇性错误消息。如果尝试访问笔记本时看到消息“抱歉，出了点问题。重新加载？”，请按照[“构建我们的模型”](ch04.xhtml#ch4_building_our_model)中的说明操作。
- en: 'This notebook will guide us through the process of training a model. It runs
    through the following steps:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本笔记本将指导我们完成训练模型的过程。它将按照以下步骤进行：
- en: Configuring parameters
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置参数
- en: Installing the correct dependencies
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装正确的依赖项
- en: Monitoring training using something called TensorBoard
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用称为TensorBoard的工具监视训练
- en: Running the training script
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行训练脚本
- en: Converting the training output into a model we can use
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练输出转换为我们可以使用的模型
- en: Enable GPU training
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用GPU训练
- en: In [Chapter 4](ch04.xhtml#chapter_hello_world_training), we trained a very simple
    model on a small amount of data. The model we are training now is a lot more sophisticated,
    has a much larger dataset, and will take a lot longer to train. On an average
    modern computer CPU, training it would take three or four hours.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.xhtml#chapter_hello_world_training)中，我们在少量数据上训练了一个非常简单的模型。我们现在正在训练的模型要复杂得多，具有更大的数据集，并且需要更长时间来训练。在一台普通的现代计算机CPU上，训练它需要三到四个小时。
- en: To reduce the time it takes to train the model, we can use something called
    *GPU acceleration*. A *GPU*, or graphics processing unit. It’s a piece of hardware
    designed to help computers process image data quickly, allowing them to smoothly
    render things like user interfaces and video games. Most computers have one.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缩短训练模型所需的时间，我们可以使用一种称为*GPU加速*的东西。*GPU*，即图形处理单元。它是一种旨在帮助计算机快速处理图像数据的硬件部件，使其能够流畅地渲染用户界面和视频游戏等内容。大多数计算机都有一个。
- en: Image processing involves running a lot of tasks in parallel, and so does training
    a deep learning network. This means that it’s possible to use GPU hardware to
    accelerate deep learning training. It’s common for training to be 5 to 10 times
    faster when run on a GPU as opposed to a CPU.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图像处理涉及并行运行许多任务，训练深度学习网络也是如此。这意味着可以使用GPU硬件加速深度学习训练。通常情况下，使用GPU运行训练比使用CPU快5到10倍是很常见的。
- en: The audio preprocessing required in our training process means that we won’t
    see quite such a massive speed-up, but our model will still train a lot faster
    on a GPU—it will take around one to two hours, total.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练过程中需要的音频预处理意味着我们不会看到如此巨大的加速，但我们的模型在GPU上仍然会训练得更快 - 大约需要一到两个小时。
- en: Luckily for us, Colab supports training via GPU. It’s not enabled by default,
    but it’s easy to switch on. To do so, go to Colab’s Runtime menu, then click “Change
    runtime type,” as demonstrated in [Figure 8-2](#change_runtime_type).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Colab支持通过GPU进行训练。默认情况下未启用，但很容易打开。要这样做，请转到Colab的运行时菜单，然后单击“更改运行时类型”，如[图8-2](#change_runtime_type)所示。
- en: '![The ''Change runtime type'' option in Colab](Images/timl_0802.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![在Colab中的“更改运行时类型”选项](Images/timl_0802.png)'
- en: Figure 8-2\. The “Change runtime type” option in Colab
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2. 在Colab中的“更改运行时类型”选项
- en: When you select this option, the “Notebook settings” box shown in [Figure 8-3](#notebook_settings)
    opens.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 选择此选项后，将打开[图8-3](#notebook_settings)中显示的“笔记本设置”框。
- en: '![The ''Notebook settings'' box](Images/timl_0803.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![“笔记本设置”框](Images/timl_0803.png)'
- en: Figure 8-3\. The “Notebook settings” box
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3. “笔记本设置”框
- en: Select GPU from the “Hardware accelerator” drop-down list, as in [Figure 8-4](#hardware_accelerator),
    and then click SAVE.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从“硬件加速器”下拉列表中选择GPU，如[图8-4](#hardware_accelerator)所示，然后单击保存。
- en: '![The ''Hardware accelerator'' dropdown](Images/timl_0804.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![在Colab中的“硬件加速器”下拉菜单](Images/timl_0804.png)'
- en: Figure 8-4\. The “Hardware accelerator” drop-down list
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4. “硬件加速器”下拉列表
- en: Colab will now run its Python on a backend computer (referred to as a *runtime*)
    that has a GPU.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Colab现在将在具有GPU的后端计算机（称为*运行时*）上运行其Python。
- en: The next step is to configure the notebook with the words we’d like to train.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是配置笔记本，以包含我们想要训练的单词。
- en: Configure training
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置训练
- en: The training scripts are configured via a bunch of command-line flags that control
    everything from the model’s architecture to the words it will be trained to classify.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本通过一系列命令行标志进行配置，这些标志控制从模型架构到将被训练分类的单词等所有内容。
- en: To make it easier to run the scripts, the notebook’s first cell stores some
    important values in environment variables. These will be substituted into the
    scripts’ command-line flags when they are run.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易运行脚本，笔记本的第一个单元格将一些重要值存储在环境变量中。当运行这些脚本时，这些值将被替换为脚本的命令行标志。
- en: 'The first one, `WANTED_WORDS`, allows us to select the words on which to train
    the model:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是`WANTED_WORDS`，允许我们选择要训练模型的单词：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'By default the selected words are “yes” and “no,” but we can provide any combination
    of the following words, all of which appear in our dataset:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，选定的单词是“yes”和“no”，但我们可以提供以下单词的任何组合，这些单词都出现在我们的数据集中：
- en: 'Common commands: *yes*, *no*, *up*, *down*, *left*, *right*, *on*, *off*, *stop*,
    *go*, *backward*, *forward*, *follow*, *learn*'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见命令：*yes*、*no*、*up*、*down*、*left*、*right*、*on*、*off*、*stop*、*go*、*backward*、*forward*、*follow*、*learn*
- en: 'Digits zero through nine: *zero*, *one*, *two*, *three*, *four*, *five*, *six*,
    *seven*, *eight*, *nine*'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字零到九：*zero*、*one*、*two*、*three*、*four*、*five*、*six*、*seven*、*eight*、*nine*
- en: 'Random words: *bed*, *bird*, *cat*, *dog*, *happy*, *house*, *Marvin*, *Sheila*,
    *tree*, *wow*'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机单词：*bed*、*bird*、*cat*、*dog*、*happy*、*house*、*Marvin*、*Sheila*、*tree*、*wow*
- en: 'To select words, we can just include them in a comma-separated list. Let’s
    choose the words “on” and “off” to train our new model:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择单词，我们只需将它们包含在逗号分隔的列表中。让我们选择单词“on”和“off”来训练我们的新模型：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Any words not included in the list will be grouped under the “unknown” category
    when the model is trained.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，未包含在列表中的任何单词将在模型训练时归为“未知”类别。
- en: Note
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It’s fine to choose more than two words here; we would just need to tweak the
    application code slightly. We provide instructions on doing this in [“Using the
    Model in Our Project”](#micro_speech_using_model_in_project).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里选择超过两个单词是可以的；我们只需要稍微调整应用代码。我们提供了在[“在我们的项目中使用模型”](#micro_speech_using_model_in_project)中执行此操作的说明。
- en: 'Notice also the `TRAINING_STEPS` and `LEARNING_RATE` variables:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意`TRAINING_STEPS`和`LEARNING_RATE`变量：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In [Chapter 3](ch03.xhtml#chapter_get_up_to_speed), we learned that a model’s
    weights and biases are incrementally adjusted so that over time, the output of
    the model gets closer to matching the desired value. `TRAINING_STEPS` refers to
    the number of times a batch of training data will be run through the network and
    its weights and biases updated. `LEARNING_RATE` sets the rate of adjustment.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.xhtml#chapter_get_up_to_speed)中，我们了解到模型的权重和偏差会逐渐调整，以便随着时间的推移，模型的输出越来越接近所期望的值。`TRAINING_STEPS`指的是训练数据批次通过网络运行的次数，以及其权重和偏差的更新次数。`LEARNING_RATE`设置调整速率。
- en: With a high learning rate, the weights and biases are adjusted more with each
    iteration, meaning convergence happens fast. However, these big jumps mean that
    it’s more difficult to get to the ideal values because we might keep jumping past
    them. With a lower learning rate, the jumps are smaller. It takes more steps to
    converge, but the final result might be better. The best learning rate for a given
    model is determined through trial and error.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高学习率，权重和偏差在每次迭代中调整更多，意味着收敛速度快。然而，这些大幅跳跃意味着更难以达到理想值，因为我们可能会一直跳过它们。使用较低的学习率，跳跃较小。需要更多步骤才能收敛，但最终结果可能更好。对于给定模型的最佳学习率是通过试错确定的。
- en: In the aforementioned variables, the training steps and learning rate are defined
    as comma-separated lists that define the learning rate for each stage of training.
    With the values we just looked at, the model will train for 15,000 steps with
    a learning rate of 0.001, and then 3,000 steps with a learning rate of 0.0001\.
    The total number of steps will be 18,000.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述变量中，训练步骤和学习率被定义为逗号分隔的列表，定义了每个训练阶段的学习率。根据我们刚刚查看的值，模型将进行15,000步的训练，学习率为0.001，然后进行3,000步的训练，学习率为0.0001。总步数将为18,000。
- en: This means we’ll do a bunch of iterations with a high learning rate, allowing
    the network to quickly converge. We’ll then do a smaller number of iterations
    with a low learning rate, fine-tuning the weights and biases.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们将使用高学习率进行一系列迭代，使网络快速收敛。然后我们将使用低学习率进行较少的迭代，微调权重和偏差。
- en: 'For now, we’ll leave these values as they are, but it’s good to know what they
    are for. Run the cell. You’ll see the following output printed:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将保持这些值不变，但知道它们是什么是很好的。运行单元格。您将看到以下输出打印：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This gives a summary of how our model will be trained.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了我们的模型将如何训练的摘要。
- en: Install dependencies
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装依赖项
- en: Next up, we grab some dependencies that are necessary for running the scripts.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们获取一些运行脚本所必需的依赖项。
- en: 'Run the next two cells to do the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 运行下面的两个单元格来执行以下操作：
- en: Install a specific version of the TensorFlow `pip` package that includes the
    ops required for training.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装包含训练所需操作的特定版本的TensorFlow `pip`软件包。
- en: Clone a corresponding version of the TensorFlow GitHub repository so that we
    can access the training scripts.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克隆TensorFlow GitHub存储库的相应版本，以便我们可以访问训练脚本。
- en: Load TensorBoard
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载TensorBoard
- en: To monitor the training process, we use [TensorBoard](https://oreil.ly/wginD).
    It’s a user interface that can show us graphs, statistics, and other insight into
    how training is going.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监视训练过程，我们使用[TensorBoard](https://oreil.ly/wginD)。这是一个用户界面，可以向我们显示图表、统计数据和其他关于训练进展的见解。
- en: When training has completed, it will look something like the screenshot in [Figure 8-5](#tensorboard_complete).
    You’ll learn what all of these graphs mean later in this chapter.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练完成时，它将看起来像[图8-5](#tensorboard_complete)中的截图。您将在本章后面了解所有这些图表的含义。
- en: '![A screenshot of TensorBoard](Images/timl_0805.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![TensorBoard的截图](Images/timl_0805.png)'
- en: Figure 8-5\. A screenshot of TensorBoard after training is complete
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5。训练完成后的TensorBoard截图
- en: Run the next cell to load TensorBoard. It will appear in Colab, but it won’t
    show anything interesting until we begin training.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 运行下一个单元格以加载TensorBoard。它将出现在Colab中，但在我们开始训练之前不会显示任何有趣的内容。
- en: Begin training
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开始训练
- en: 'The following cell runs the script that begins training. You can see that it
    has a lot of command-line arguments:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下单元格运行开始训练的脚本。您可以看到它有很多命令行参数：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Some of these, like `--wanted_words=${WANTED_WORDS}`, use the environment variables
    we defined earlier to configure the model we’re creating. Others set up the output
    of the script, such as `--train_dir=/content/speech_commands_train`, which defines
    where the trained model will be saved.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些，如`--wanted_words=${WANTED_WORDS}`，使用我们之前定义的环境变量来配置我们正在创建的模型。其他设置脚本的输出，例如`--train_dir=/content/speech_commands_train`，定义了训练模型将保存的位置。
- en: 'Leave the arguments as they are, and run the cell. You’ll begin to see some
    output stream past. It will pause for a few moments while the Speech Commands
    dataset is downloaded:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 保持参数不变，运行单元格。您将开始看到一些输出流过。在下载语音命令数据集时，它将暂停一段时间：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When this is done, some more output will appear. There might be some warnings,
    which you can ignore as long as the cell continues running. At this point, you
    should scroll back up to TensorBoard, which should hopefully look something like
    [Figure 8-6](#tensorboard_start). If you don’t see any graphs, click the SCALARS
    tab.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，会出现更多输出。可能会有一些警告，只要单元格继续运行，您可以忽略它们。此时，您应该向上滚动到TensorBoard，希望它看起来像[图8-6](#tensorboard_start)。如果您看不到任何图表，请单击SCALARS选项卡。
- en: '![A screenshot of TensorBoard](Images/timl_0806.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![TensorBoard的截图](Images/timl_0806.png)'
- en: Figure 8-6\. A screenshot of TensorBoard at the beginning of training
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-6。训练开始时的TensorBoard截图
- en: Hooray! This means that training has begun. The cell you’ve just run will continue
    to execute for the duration of training, which will take up to two hours to complete.
    The cell won’t output any more logs, but data about the training run will appear
    in TensorBoard.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 万岁！这意味着训练已经开始。您刚刚运行的单元将继续执行，训练将需要最多两个小时才能完成。该单元将不会输出更多日志，但有关训练运行的数据将出现在TensorBoard中。
- en: You can see that TensorBoard shows two graphs, “accuracy” and “cross_entropy,”
    as shown in [Figure 8-7](#tensorboard_graphs). Both graphs show the current steps
    on the x-axis. The “accuracy” graph shows the model’s accuracy on its y-axis,
    which signals how much of the time it is able to detect a word correctly. The
    “cross_entropy” graph shows the model’s loss, which quantifies how far from the
    correct values the model’s predictions are.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到TensorBoard显示了两个图形，“准确度”和“交叉熵”，如[图8-7](#tensorboard_graphs)所示。两个图形都显示了x轴上的当前步骤。“准确度”图显示了模型在y轴上的准确度，这表明它能够正确检测单词的时间有多少。“交叉熵”图显示了模型的损失，量化了模型预测与正确值之间的差距。
- en: '![A screenshot of graphs in TensorBoard](Images/timl_0807.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: ！[TensorBoard中图形的屏幕截图]（Images/timl_0807.png）
- en: Figure 8-7\. The “accuracy” and “cross_entropy” graphs
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-7。"准确度"和"交叉熵"图
- en: Note
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Cross entropy is a common way of measuring loss in machine learning models that
    perform classification, for which the goal is to predict which category an input
    belongs to.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵是衡量机器学习模型损失的常见方法，用于执行分类，目标是预测输入属于哪个类别。
- en: The jagged lines on the graph correspond to performance on the training dataset,
    whereas the straight lines reflect performance on the validation dataset. Validation
    occurs periodically, so there are fewer validation datapoints on the graph.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图形上的锯齿线对应于训练数据集上的性能，而直线反映了验证数据集上的性能。验证定期进行，因此图上的验证数据点较少。
- en: New data will arrive in the graphs over time, but to show it, you need to adjust
    their scales to fit. You can do this by clicking the rightmost button under each
    graph, as shown in [Figure 8-8](#tensorboard_fit_button).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 新数据将随着时间的推移出现在图形中，但要显示它，您需要调整它们的比例以适应。您可以通过单击每个图形下面的最右边的按钮来实现这一点，如[图8-8](#tensorboard_fit_button)所示。
- en: '![A screenshot of an icon in TensorBoard](Images/timl_0808.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: ！[TensorBoard中图标的屏幕截图]（Images/timl_0808.png）
- en: Figure 8-8\. Click this button to adjust the graph’s scale to fit all available
    data
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8。单击此按钮以调整图形的比例，以适应所有可用数据
- en: You can also click the button shown in [Figure 8-9](#tensorboard_expand_button)
    to make each graph larger.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以单击[图8-9](#tensorboard_expand_button)中显示的按钮，使每个图形变大。
- en: '![A screenshot of an icon in TensorBoard](Images/timl_0809.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: ！[TensorBoard中图标的屏幕截图]（Images/timl_0809.png）
- en: Figure 8-9\. Click this button to enlarge the graph
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-9。单击此按钮以放大图形
- en: In addition to graphs, TensorBoard can show the inputs being fed into the model.
    Click the IMAGES tab, which displays a view similar to [Figure 8-10](#tensorboard_spectrogram).
    This is an example of a spectrogram that is being input to the model during training.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图形外，TensorBoard还可以显示输入传入模型。单击IMAGES选项卡，显示类似于[图8-10](#tensorboard_spectrogram)的视图。这是在训练期间输入到模型中的频谱图的示例。
- en: '![A screenshot of the IMAGES tab in TensorBoard](Images/timl_0810.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: ！[TensorBoard中IMAGES选项卡的屏幕截图]（Images/timl_0810.png）
- en: Figure 8-10\. The IMAGES tab of TensorBoard
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10。TensorBoard的IMAGES选项卡
- en: Wait for training to complete
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 等待训练完成
- en: Training the model will take between one and two hours, so our job now is to
    be patient. Fortunately for us, we have TensorBoard’s pretty graphs to keep us
    entertained.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型将需要一到两个小时，所以我们现在的工作是耐心等待。幸运的是，我们有TensorBoard漂亮的图形来娱乐我们。
- en: As training progresses, you’ll notice that the metrics tend to jump around within
    a range. This is normal, but it makes the graphs appear fuzzy and difficult to
    read. To make it easier to see how training is going, we can use TensorFlow’s
    Smoothing feature.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的进行，您会注意到指标在一定范围内跳动。这是正常的，但它使图形看起来模糊且难以阅读。为了更容易看到训练的进展，我们可以使用TensorFlow的平滑功能。
- en: '[Figure 8-11](#tensorboard_smoothing_before) shows graphs with the default
    amount of smoothing applied; notice how fuzzy they are.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-11](#tensorboard_smoothing_before)显示了应用默认平滑度的图形；请注意它们有多模糊。'
- en: '![A screenshot of graphs in TensorBoard](Images/timl_0811.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: ！[TensorBoard中图形的屏幕截图]（Images/timl_0811.png）
- en: Figure 8-11\. Training graphs with the default amount of smoothing
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-11。默认平滑度的训练图
- en: By adjusting the Smoothing slider, shown in [Figure 8-12](#tensorboard_smoothing_slider),
    we can increase the amount of smoothing, making the trends more obvious.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整[图8-12](#tensorboard_smoothing_slider)中显示的平滑滑块，我们可以增加平滑度，使趋势更加明显。
- en: '![A screenshot TensorBoard''s *Smoothing* slider](Images/timl_0812.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: ！[TensorBoard的*平滑*滑块的屏幕截图]（Images/timl_0812.png）
- en: Figure 8-12\. TensorBoard’s Smoothing slider
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-12。TensorBoard的平滑滑块
- en: '[Figure 8-13](#tensorboard_smoothing_after) shows the same graphs with a higher
    level of smoothing. The original data is visible in lighter colors, underneath.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-13](#tensorboard_smoothing_after)显示了具有更高平滑度级别的相同图形。原始数据以较浅的颜色可见，在下面。'
- en: '![A screenshot of graphs in TensorBoard](Images/timl_0813.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: ！[TensorBoard中图形的屏幕截图]（Images/timl_0813.png）
- en: Figure 8-13\. Training graphs with increased smoothing
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-13。增加平滑度的训练图
- en: Keeping Colab running
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 保持Colab运行
- en: To prevent abandoned projects from consuming resources, Colab will shut down
    your runtime if it isn’t actively being used. Because our training will take a
    while, we need to prevent this from happening. There are a couple of things we
    need to think about.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止废弃的项目占用资源，如果Colab没有被积极使用，它将关闭您的运行时。因为我们的训练需要一段时间，所以我们需要防止这种情况发生。我们需要考虑一些事情。
- en: First, if we’re not actively interacting with the Colab browser tab, the web
    user interface will disconnect from the backend runtime where the training scripts
    are being executed. This will happen after a few minutes, and will cause your
    TensorBoard graphs to stop updating with the latest training metrics. There’s
    no need to panic if this happens—your training is still running in the background.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果我们没有在与Colab浏览器标签进行活动交互，Web用户界面将与后端运行时断开连接，训练脚本正在执行的地方。几分钟后会发生这种情况，并且会导致您的TensorBoard图表停止更新最新的训练指标。如果发生这种情况，无需恐慌—您的训练仍在后台运行。
- en: If your runtime has disconnected, you’ll see a Reconnect button appear in Colab’s
    user interface, as shown in [Figure 8-14](#colab_reconnect_button). Click this
    button to reconnect your runtime.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的运行时已断开连接，您将在Colab的用户界面中看到一个重新连接按钮，如[图8-14](#colab_reconnect_button)所示。点击此按钮以重新连接您的运行时。
- en: '![A screenshot of Colab''s Reconnect button](Images/timl_0814.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![Colab的重新连接按钮截图](Images/timl_0814.png)'
- en: Figure 8-14\. Colab’s Reconnect button
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-14\. Colab的重新连接按钮
- en: 'A disconnected runtime is no big deal, but Colab’s next timeout deserves some
    attention. *If you don’t interact with Colab for 90 consecutive minutes, your
    runtime instance will be recycled*. This is a problem: you will lose all of your
    training progress, along with any data stored in the instance!'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 断开连接的运行时并不是什么大问题，但Colab的下一个超时需要一些注意。*如果您在90分钟内不与Colab进行交互，您的运行时实例将被回收*。这是一个问题：您将丢失所有的训练进度，以及实例中存储的任何数据！
- en: To avoid this happening, you just need to interact with Colab at least once
    every 90 minutes. Open the tab, make sure the runtime is connected, and take a
    look at your beautiful graphs. As long as you do this before 90 minutes have elapsed,
    the connection will stay open.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况发生，您只需要每90分钟至少与Colab进行一次交互。打开标签页，确保运行时已连接，并查看您美丽的图表。只要在90分钟过去之前这样做，连接就会保持打开状态。
- en: Warning
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Even if your Colab tab is closed, the runtime will continue running in the background
    for up to 90 minutes. As long as you open the original URL in your browser, you
    can reconnect to the runtime and continue as before.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您关闭了Colab标签页，运行时也会在后台继续运行长达90分钟。只要在浏览器中打开原始URL，您就可以重新连接到运行时，并继续之前的操作。
- en: However, TensorBoard will disappear when the tab is closed. If training is still
    running when the tab is reopened, you will not be able to view TensorBoard again
    until training is complete.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当标签页关闭时，TensorBoard将消失。如果在重新打开标签页时训练仍在进行，您将无法查看TensorBoard，直到训练完成。
- en: Finally, *a Colab runtime has a maximum lifespan of 12 hours*. If your training
    takes longer than 12 hours, you’re out of luck—Colab will shut down and reset
    your instance before training has a chance to complete. If your training is likely
    to run this long, you should avoid Colab and use one of the alternative solutions
    described in [“Other Ways to Run the Scripts”](#micro_speech_other_ways_to_run_training).
    Luckily, training our wake-word model won’t take anywhere near that long.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*Colab运行时的最长寿命为12小时*。如果您的训练时间超过12小时，那就倒霉了—Colab将在训练完成之前关闭并重置您的实例。如果您的训练可能持续这么长时间，您应该避免使用Colab，并使用[“其他运行脚本的方法”](#micro_speech_other_ways_to_run_training)中描述的替代方案之一。幸运的是，训练我们的唤醒词模型不会花费那么长时间。
- en: When your graphs show data for 18,000 steps, training is complete! We now must
    run a few more commands to prepare our model for deployment. Don’t worry—this
    part is *much* quicker.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的图表显示了18000步的数据时，训练就完成了！现在我们必须运行几个命令来准备我们的模型进行部署。不用担心—这部分要快得多。
- en: Freeze the graph
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 冻结图表
- en: As you learned earlier in this book, training is the process of iteratively
    tweaking a model’s weights and biases until it produces useful predictions. The
    training script writes these weights and biases to *checkpoint* files. A checkpoint
    is written once every hundred steps. This means that if training fails partway
    through, it can be restarted from the most recent checkpoint without losing progress.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在本书中早些时候学到的，训练是一个迭代调整模型权重和偏差的过程，直到它产生有用的预测。训练脚本将这些权重和偏差写入*检查点*文件。每一百步写入一个检查点。这意味着如果训练在中途失败，可以从最近的检查点重新启动而不会丢失进度。
- en: The *train.py* script is called with an argument, `--train_dir`, which specifies
    where these checkpoint files will be written. In our Colab, it’s set to */content/speech_commands_train*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*train.py*脚本被调用时带有一个参数，`--train_dir`，用于指定这些检查点文件将被写入的位置。在我们的Colab中，它被设置为*/content/speech_commands_train*。'
- en: You can see the checkpoint files by opening Colab’s lefthand panel, which has
    a file browser. To do so, click the button shown in [Figure 8-15](#colab_sidebar_button).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过打开Colab的左侧面板来查看检查点文件，该面板具有一个文件浏览器。要这样做，请点击[图8-15](#colab_sidebar_button)中显示的按钮。
- en: '![A screenshot of the button that opens Colab''s sidebar](Images/timl_0815.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![打开Colab侧边栏的按钮截图](Images/timl_0815.png)'
- en: Figure 8-15\. The button that opens Colab’s sidebar
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-15\. 打开Colab侧边栏的按钮
- en: In this panel, click the Files tab to see the runtime’s filesystem. If you open
    the *speech_commands_train/* directory you’ll see the checkpoint files, as in
    [Figure 8-16](#colab_files_checkpoints). The number in each filename indicates
    the step at which the checkpoint was saved.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在此面板中，点击“文件”选项卡以查看运行时的文件系统。如果您打开*speech_commands_train/*目录，您将看到检查点文件，如[图8-16](#colab_files_checkpoints)所示。每个文件名中的数字表示保存检查点的步骤。
- en: '![A screenshot of Colab''s file browser showing a list of checkpoint files](Images/timl_0816.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![Colab的文件浏览器显示检查点文件列表的截图](Images/timl_0816.png)'
- en: Figure 8-16\. Colab’s file browser showing a list of checkpoint files
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-16\. Colab的文件浏览器显示检查点文件列表
- en: 'A TensorFlow model consists of two main things:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个TensorFlow模型由两个主要部分组成：
- en: The weights and biases resulting from training
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练产生的权重和偏差
- en: A graph of operations that combine the model’s input with these weights and
    biases to produce the model’s output
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型的输入与这些权重和偏差结合起来产生模型的输出的操作图
- en: At this juncture, our model’s operations are defined in the Python scripts,
    and its trained weights and biases are in the most recent checkpoint file. We
    need to unite the two into a single model file with a specific format, which we
    can use to run inference. The process of creating this model file is called *freezing*—we’re
    creating a static representation of the graph with the weights *frozen* into it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们的模型操作在Python脚本中定义，并且其训练的权重和偏差在最新的检查点文件中。我们需要将这两者合并为一个具有特定格式的单个模型文件，以便我们可以用来运行推断。创建此模型文件的过程称为*冻结*——我们正在创建一个具有*冻结*权重的图的静态表示。
- en: 'To freeze our model, we run a script. You’ll find it in the next cell, in the
    “Freeze the graph” section. The script is called as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了冻结我们的模型，我们运行一个脚本。您将在下一个单元格中找到它，在“冻结图”部分。脚本的调用如下：
- en: '[PRE6]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To point the script toward the correct graph of operations to freeze, we pass
    some of the same arguments we used in training. We also pass a path to the final
    checkpoint file, which is the one whose filename ends with the total number of
    training steps.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指向正确的操作图以冻结的脚本，我们传递了一些与训练中使用的相同参数。我们还传递了最终检查点文件的路径，该文件的文件名以训练步骤的总数结尾。
- en: Run this cell to freeze the graph. The frozen graph will be output to a file
    named *tiny_conv.pb*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此单元格以冻结图。冻结的图将输出到名为*tiny_conv.pb*的文件中。
- en: This file is the fully trained TensorFlow model. It can be loaded by TensorFlow
    and used to run inference. That’s great, but it’s still in the format used by
    regular TensorFlow, not TensorFlow Lite. Our next step is to convert the model
    into the TensorFlow Lite format.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件是完全训练过的TensorFlow模型。它可以被TensorFlow加载并用于运行推断。这很棒，但它仍然是常规TensorFlow使用的格式，而不是TensorFlow
    Lite。我们的下一步是将模型转换为TensorFlow Lite格式。
- en: Convert to TensorFlow Lite
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换为TensorFlow Lite
- en: 'Conversion is another easy step: we just need to run a single command. Now
    that we have a frozen graph file to work with, we’ll be using `toco`, the command-line
    interface for the TensorFlow Lite converter.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 转换是另一个简单的步骤：我们只需要运行一个命令。现在我们有一个冻结的图文件可以使用，我们将使用`toco`，TensorFlow Lite转换器的命令行界面。
- en: 'In the “Convert the model” section, run the first cell:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在“转换模型”部分，运行第一个单元格：
- en: '[PRE7]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the arguments, we specify the model that we want to convert, the output location
    for the TensorFlow Lite model file, and some other values that depend on the model
    architecture. Because the model was quantized during training, we also provide
    some arguments (`inference_type`, `mean_values`, and `std_dev_values`) that instruct
    the converter how to map its low-precision values into real numbers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数中，我们指定要转换的模型，TensorFlow Lite模型文件的输出位置，以及一些取决于模型架构的其他值。因为模型在训练期间被量化，我们还提供了一些参数（`inference_type`，`mean_values`和`std_dev_values`），指导转换器如何将其低精度值映射到实数。
- en: You might be wondering why the `input_shape` argument has a leading `1` before
    the width, height, and channels parameters. This is the batch size; for efficiency
    during training, we send a lot of inputs in together, but when we’re running in
    a real-time application we’ll be working on only one sample at a time, which is
    why the batch size is fixed as `1`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么`input_shape`参数在宽度、高度和通道参数之前有一个前导`1`。这是批处理大小；为了在训练期间提高效率，我们一次发送很多输入，但当我们在实时应用中运行时，我们每次只处理一个样本，这就是为什么批处理大小固定为`1`。
- en: The converted model will be written to *tiny_conv.tflite*. Congratulations;
    this a fully formed TensorFlow Lite model!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的模型将被写入*tiny_conv.tflite*。恭喜！这是一个完全成型的TensorFlow Lite模型！
- en: 'To see how tiny this model is, in the next cell, run the code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这个模型有多小，在下一个单元格中运行以下代码：
- en: '[PRE8]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output shows that the model is super small: `Model is 18208 bytes`.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示模型非常小：`模型大小为18208字节`。
- en: Our next step is to get this model into a form that we can deploy to microcontrollers.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是将这个模型转换为可以部署到微控制器的形式。
- en: Create a C array
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个C数组
- en: 'Back in [“Converting to a C File”](ch04.xhtml#hello_world_training_convert_to_c_file),
    we used the `xxd` command to convert a TensorFlow Lite model into a C array. We’ll
    do the same thing in the next cell:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[“转换为C文件”](ch04.xhtml#hello_world_training_convert_to_c_file)中，我们使用`xxd`命令将TensorFlow
    Lite模型转换为C数组。我们将在下一个单元格中做同样的事情：
- en: '[PRE9]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The final part of the output will be the file’s contents, which are a C array
    and an integer holding its length, as follows (the exact values you see might
    be slightly different):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后部分将是文件的内容，其中包括一个C数组和一个保存其长度的整数，如下所示（您看到的确切值可能略有不同）：
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This code is also written to a file, *tiny_conv.cc*, which you can download
    using Colab’s file browser. Because your Colab runtime will expire after 12 hours,
    it’s a good idea to download this file to your computer now.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码也被写入一个文件*tiny_conv.cc*，您可以使用Colab的文件浏览器下载。因为您的Colab运行时将在12小时后到期，现在将此文件下载到您的计算机是一个好主意。
- en: Next, we’ll integrate this newly trained model with the `micro_speech` project
    so that we can deploy it to some hardware.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把这个新训练过的模型与`micro_speech`项目集成起来，以便我们可以将其部署到一些硬件上。
- en: Using the Model in Our Project
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在我们的项目中使用模型
- en: 'To use our new model, we need to do three things:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用我们的新模型，我们需要做三件事：
- en: In [*micro_features/tiny_conv_micro_features_model_data.cc*](https://oreil.ly/EAR0U),
    replace the original model data with our new model.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[*micro_features/tiny_conv_micro_features_model_data.cc*](https://oreil.ly/EAR0U)中，用我们的新模型替换原始模型数据。
- en: Update the label names in [*micro_features/micro_model_settings.cc*](https://oreil.ly/bqw67)
    with our new “on” and “off” labels.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[*micro_features/micro_model_settings.cc*](https://oreil.ly/bqw67)中用我们的新“on”和“off”标签更新标签名称。
- en: Update the device-specific *command_responder.cc* to take the actions we want
    for the new labels.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新特定设备的*command_responder.cc*以执行我们对新标签的操作。
- en: Replacing the Model
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替换模型
- en: To replace the model, open *micro_features/tiny_conv_micro_features_model_data.cc*
    in your text editor.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要替换模型，请在文本编辑器中打开*micro_features/tiny_conv_micro_features_model_data.cc*。
- en: Note
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re working with the Arduino example, the file will appear as a tab in
    the Arduino IDE. Its name will be *micro_features_tiny_conv_micro_features_model_data.cpp*.
    If you’re working with the SparkFun Edge, you can edit the files directly in your
    local copy of the TensorFlow repository. If you’re working with the STM32F746G,
    you should edit the files in your Mbed project directory.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用Arduino示例，该文件将显示为Arduino IDE中的一个选项卡。它的名称将是*micro_features_tiny_conv_micro_features_model_data.cpp*。如果你正在使用SparkFun
    Edge，你可以直接在本地的TensorFlow存储库副本中编辑文件。如果你正在使用STM32F746G，你应该在Mbed项目目录中编辑文件。
- en: 'The *tiny_conv_micro_features_model_data.cc* file contains an array declaration
    that looks like this:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*tiny_conv_micro_features_model_data.cc*文件包含一个看起来像这样的数组声明：'
- en: '[PRE11]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You’ll need to replace the contents of the array as well as the value of the
    constant `g_tiny_conv_micro_features_model_data_len`, if it has changed.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 需要替换数组的内容以及常量`g_tiny_conv_micro_features_model_data_len`的值，如果已经更改。
- en: To do so, open the *tiny_conv.cc* file that you downloaded at the end of the
    previous section. Copy and paste the contents of the array, but not its definition,
    into the array defined in *tiny_conv_micro_features_model_data.cc*. Make sure
    you are overwriting the array’s contents, but not its declaration.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，打开你在上一节末尾下载的*tiny_conv.cc*文件。复制并粘贴数组的内容，但不包括定义，到*tiny_conv_micro_features_model_data.cc*中定义的数组中。确保你正在覆盖数组的内容，但不是它的声明。
- en: At the bottom of *tiny_conv.cc* you’ll find `_content_tiny_conv_tflite_len`,
    a variable whose value represents the length of the array. Back in *tiny_conv_micro_features_model_data.cc*,
    replace the value of `g_tiny_conv_micro_features_model_data_len` with the value
    of this variable. Then save the file; you’re done updating it.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在*tiny_conv.cc*的底部，你会找到`_content_tiny_conv_tflite_len`，一个变量，其值表示数组的长度。回到*tiny_conv_micro_features_model_data.cc*，用这个变量的值替换`g_tiny_conv_micro_features_model_data_len`的值。然后保存文件；你已经完成了更新。
- en: Updating the Labels
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新标签
- en: 'Next, open *micro_features/micro_model_settings.cc*. This file contains an
    array of class labels:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开*micro_features/micro_model_settings.cc*。这个文件包含一个类标签的数组：
- en: '[PRE12]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To adjust this for our new model, we can just swap the “yes” and “no” for “on”
    and “off.” We match labels with the model’s output tensor elements by order, so
    it’s important to list these in the same order in which they were provided to
    the training script.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调整我们的新模型，我们可以简单地将“yes”和“no”交换为“on”和“off”。我们按顺序将标签与模型的输出张量元素匹配，因此重要的是按照它们提供给训练脚本的顺序列出这些标签。
- en: 'Here’s the expected code:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是预期的代码：
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If you trained a model with more than two labels, just add them all to the list.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你训练了一个具有两个以上标签的模型，只需将它们全部添加到列表中。
- en: We’re now done switching over the model. The only remaining step is to update
    any output code that uses the labels.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了切换模型的工作。唯一剩下的步骤是更新使用标签的任何输出代码。
- en: Updating command_responder.cc
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新command_responder.cc
- en: The project contains a different device-specific implementation of *command_responder.cc*
    for the Arduino, SparkFun Edge, and STM32F746G. We show how to update each of
    these in the following sections.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目包含针对Arduino、SparkFun Edge和STM32F746G的不同设备特定实现的*command_responder.cc*。我们将在以下部分展示如何更新每个设备。
- en: Arduino
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Arduino
- en: 'The Arduino command responder, located in *arduino/command_responder.cc*, lights
    an LED for 3 seconds when it hears the word “yes.” Let’s update it to light the
    LED when it hears either “on” or “off.” In the file, locate the following `if`
    statement:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 位于*arduino/command_responder.cc*中的Arduino命令响应器在听到“yes”时会点亮LED 3秒钟。让我们将其更新为在听到“on”或“off”时点亮LED。在文件中，找到以下`if`语句：
- en: '[PRE14]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `if` statement tests whether the first letter of the command is “y,” for
    “yes.” If we change this “y” to an “o,” the LED will be lit for either “on” or
    “off,” because they both begin with “o”:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`if`语句测试命令的第一个字母是否为“y”，表示“yes”。如果我们将这个“y”改为“o”，LED将点亮“on”或“off”，因为它们都以“o”开头：'
- en: '[PRE15]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: After you’ve made these code changes, deploy to your device and give it a try.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些代码更改后，部署到你的设备并尝试一下。
- en: SparkFun Edge
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SparkFun Edge
- en: 'The SparkFun Edge command responder, located in *sparkfun_edge/command_responder.cc*,
    lights up a different LED depending on whether it heard “yes” or “no.” In the
    file, locate the following `if` statements:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 位于*sparkfun_edge/command_responder.cc*中的SparkFun Edge命令响应器会根据听到的“yes”或“no”点亮不同的LED。在文件中，找到以下`if`语句：
- en: '[PRE16]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It’s simple to update these so that “on” and “off” each turn on different LEDs:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易更新这些，使得“on”和“off”分别点亮不同的LED：
- en: '[PRE17]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Because both commands begin with the same letter, we need to look at their second
    letters to disambiguate them. Now, the yellow LED will light when “on” is spoken,
    and the red will light for “off.”
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这两个命令都以相同的字母开头，我们需要查看它们的第二个字母来消除歧义。现在，当说“on”时，黄色LED将点亮，当说“off”时，红色LED将点亮。
- en: When you’re finished making the changes, deploy and run the code using the same
    process you followed in [“Running the example”](ch07.xhtml#micro_speech_sparkfun_edge_running_the_example).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 完成更改后，部署并运行代码，使用与[“运行示例”](ch07.xhtml#micro_speech_sparkfun_edge_running_the_example)中遵循的相同过程。
- en: STM32F746G
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: STM32F746G
- en: 'The STM32F746G command responder, located in *disco_f746ng/command_responder.cc*,
    displays a different word depending on which command it heard. In the file, locate
    the following `if` statement:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 位于*disco_f746ng/command_responder.cc*中的STM32F746G命令响应器会根据听到的命令显示不同的单词。在文件中，找到以下`if`语句：
- en: '[PRE18]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'It’s easy to update this so that it responds to “on” and “off,” instead:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易更新以便响应“on”和“off”：
- en: '[PRE19]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Again, because both commands begin with the same letter, we look at their second
    letters to disambiguate them. Now we display the appropriate text for each command.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，因为这两个命令都以相同的字母开头，我们需要查看它们的第二个字母来消除歧义。现在我们为每个命令显示适当的文本。
- en: Other Ways to Run the Scripts
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行脚本的其他方法
- en: 'If you’re not able to use Colab, there are two other recommended ways to train
    the model:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法使用Colab，有两种其他推荐的训练模型的方法：
- en: In a cloud virtual machine (VM) with a GPU
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个带有GPU的云虚拟机（VM）中
- en: On your local workstation
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的本地工作站上
- en: The drivers necessary for GPU-based training are available only on Linux. Without
    Linux, training will take around four hours. For this reason, it’s recommended
    to use either a cloud VM with a GPU, or a similarly equipped Linux workstation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 进行基于GPU的训练所需的驱动程序仅在Linux上可用。没有Linux，训练将需要大约四个小时。因此，建议使用带有GPU的云虚拟机或类似配置的Linux工作站。
- en: Setting up your VM or workstation is beyond the scope of this book. However,
    we do have some recommendations. If you’re using a VM, you can launch a [Google
    Cloud Deep Learning VM Image](https://oreil.ly/PVRtP), which is preconfigured
    with all of the dependencies you’ll need for GPU training. If you’re using a Linux
    workstation, the [TensorFlow GPU Docker image](https://oreil.ly/PFYVr) has everything
    you’ll need.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 设置您的虚拟机或工作站超出了本书的范围。但是，我们有一些建议。如果您使用虚拟机，可以启动一个[Google Cloud深度学习虚拟机镜像](https://oreil.ly/PVRtP)，该镜像预先配置了所有您进行GPU训练所需的依赖项。如果您使用Linux工作站，[TensorFlow
    GPU Docker镜像](https://oreil.ly/PFYVr)包含了您所需的一切。
- en: 'To train the model, you need to install a nightly build of TensorFlow. To uninstall
    any existing version and replace it with one that is confirmed to work, use the
    following commands:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，您需要安装TensorFlow的夜间版本。要卸载任何现有版本并替换为已确认可用的版本，请使用以下命令：
- en: '[PRE20]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, open a command line and change to a directory you use to store code.
    Use the following commands to clone TensorFlow and open a specific commit that
    is confirmed to work:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开命令行并切换到用于存储代码的目录。使用以下命令克隆TensorFlow并打开一个已确认可用的特定提交：
- en: '[PRE21]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now you can run the *train.py* script to train the model. This will train a
    model to recognize “yes” and “no,” and output the checkpoint files to */tmp*:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以运行*train.py*脚本来训练模型。这将训练一个能识别“是”和“不”的模型，并将检查点文件输出到*/tmp*：
- en: '[PRE22]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After training, run the following script to freeze the model:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，运行以下脚本来冻结模型：
- en: '[PRE23]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, convert the model to the TensorFlow Lite format:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将模型转换为TensorFlow Lite格式：
- en: '[PRE24]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, convert the file into a C source file that you can compile into an
    embedded system:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将文件转换为C源文件，以便编译到嵌入式系统中：
- en: '[PRE25]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How the Model Works
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型的工作原理
- en: Now that you know how to train your own model, let’s explore how it works. So
    far, we’ve treated the machine learning model as a black box—something that we
    feed training data into, and eventually it figures out how to predict results.
    It’s not essential to understand what’s happening under the hood to use the model,
    but it can be helpful for debugging problems, and it’s interesting in its own
    right. This section gives you some insights into how the model comes up with its
    predictions.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您知道如何训练自己的模型了，让我们探讨一下它是如何工作的。到目前为止，我们将机器学习模型视为黑匣子——我们将训练数据输入其中，最终它会找出如何预测结果。要使用模型并不一定要理解底层发生了什么，但这对于调试问题可能有帮助，而且本身也很有趣。本节将为您提供一些关于模型如何进行预测的见解。
- en: Visualizing the Inputs
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化输入
- en: '[Figure 8-17](#tensorboard_spectrogram_2) illustrates what is actually being
    fed into the neural network. This is a 2D array with a single channel, so we can
    visualize it as a monochrome image. We’re working with 16 KHz audio sample data,
    so how do we get to this representation from that source? The process is an example
    of what’s known as “feature generation” in machine learning, and the goal is to
    turn an input format that’s more difficult to work with (in this case 16,000 numerical
    values representing a second of audio) into something that’s easier for a machine
    learning model to make sense of. You might not have encountered this if you’ve
    previously studied machine vision use cases for deep learning, because it happens
    that images are usually comparatively easy for a network to take as inputs without
    much preprocessing; but in a lot of other domains, like audio and natural language
    processing, it’s still common to transform the input before feeding it into a
    model.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-17](#tensorboard_spectrogram_2)说明了实际输入神经网络的内容。这是一个具有单个通道的2D数组，因此我们可以将其可视化为单色图像。我们使用16
    KHz音频样本数据，那么我们如何从源数据得到这种表示？这个过程是机器学习中所谓的“特征生成”的一个示例，目标是将更难处理的输入格式（在本例中是代表一秒音频的16,000个数值）转换为机器学习模型更容易理解的内容。如果您之前研究过深度学习的机器视觉用例，您可能没有遇到这种情况，因为图像通常相对容易让网络接受而无需太多预处理；但在许多其他领域，如音频和自然语言处理，仍然常见在将输入馈入模型之前对其进行转换。'
- en: '![A screenshot of the IMAGES tab in TensorBoard](Images/timl_0810.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![TensorBoard中IMAGES选项卡的屏幕截图](Images/timl_0810.png)'
- en: Figure 8-17\. The IMAGES tab of TensorBoard
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-17。TensorBoard的IMAGES选项卡
- en: To develop an intuition for why it’s easier for our model to deal with preprocessed
    input, let’s look at the original raw representations of some audio recordings,
    as presented in Figures [8-18](#audio_waveform_yes) through [8-21](#audio_waveform_no_1).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对我们的模型为什么更容易处理预处理输入有直觉，让我们看一下一些音频录音的原始表示，如图[8-18](#audio_waveform_yes)到[8-21](#audio_waveform_no_1)所示。
- en: '![A visualization of 16,000 audio samples of someone saying ''yes''.](Images/timl_0818.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![一个人说“是”的16,000个音频样本的可视化。](Images/timl_0818.png)'
- en: Figure 8-18\. Waveform of an audio recording of someone saying “yes”
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-18。一个人说“是”的音频录音的波形
- en: '![A visualization of 16,000 audio samples of someone saying ''no''.](Images/timl_0819.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![一个人说“不”的16,000个音频样本的可视化。](Images/timl_0819.png)'
- en: Figure 8-19\. Waveform of an audio recording of someone saying “no”
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-19。一个人说“不”的音频录音的波形
- en: '![A visualization of 16,000 audio samples of someone saying ''yes''.](Images/timl_0820.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![一个人说“是”的16,000个音频样本的可视化。](Images/timl_0820.png)'
- en: Figure 8-20\. Another waveform of an audio recording of someone saying “yes”
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-20。一个人说“是”的音频录音的另一个波形
- en: '![A visualization of 16,000 audio samples of someone saying ''no''.](Images/timl_0821.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![一个人说“不”的16,000个音频样本的可视化。](Images/timl_0821.png)'
- en: Figure 8-21\. Another waveform of an audio recording of someone saying “no”
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-21。一个人说“不”的音频录音的另一个波形
- en: Without the labels, you’d have trouble distinguishing which pairs of waveforms
    represented the same words. Now look at Figures [8-22](#features_yes_0) through
    [8-25](#features_no_1), which shows the result of running those same one-second
    recordings through feature generation.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有标签，你会很难区分哪些波形对应相同的单词。现在看看图[8-22](#features_yes_0)到[8-25](#features_no_1)，展示了将相同的一秒录音通过特征生成处理后的结果。
- en: '![A visualization of 16,000 audio samples of someone saying ''yes''.](Images/timl_0822.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![一个人说“是”时的16,000个音频样本的可视化。](Images/timl_0822.png)'
- en: Figure 8-22\. Spectrogram of an audio recording of someone saying “yes”
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-22。一个人说“是”时的谱图
- en: '![A visualization of 16,000 audio samples of someone saying ''no''.](Images/timl_0823.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![一个人说“否”时的16,000个音频样本的可视化。](Images/timl_0823.png)'
- en: Figure 8-23\. Spectrogram of an audio recording of someone saying “no”
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-23。一个人说“否”时的谱图
- en: '![A visualization of 16,000 audio samples of someone saying ''yes''.](Images/timl_0824.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![一个人说“是”时的16,000个音频样本的可视化。](Images/timl_0824.png)'
- en: Figure 8-24\. Another spectrogram of an audio recording of someone saying “yes”
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-24。一个人说“是”时的另一个谱图
- en: '![A visualization of 16,000 audio samples of someone saying ''no''.](Images/timl_0825.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![一个人说“否”时的16,000个音频样本的可视化。](Images/timl_0825.png)'
- en: Figure 8-25\. Another spectrogram of an audio recording of someone saying “no”
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-25。一个人说“否”时的另一个谱图
- en: These still aren’t simple to interpret, but hopefully you can see that both
    of the “yes” spectrograms have a shape a bit like an inverted L, and the “no”
    features show a different shape. We can discern the difference between spectrograms
    more easily than raw waveforms, and hopefully it’s intuitive that it is easier
    for models to do the same.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这些仍然不容易解释，但希望你能看出“是”谱图的形状有点像倒置的L，而“否”特征显示出不同的形状。我们可以更容易地辨别谱图之间的差异，希望直觉告诉你，对于模型来说做同样的事情更容易。
- en: Another aspect to this is that the generated spectrograms are a lot smaller
    than the sample data. Each spectrogram consist of 1,960 numeric values, whereas
    the waveform has 16,000\. They are a summary of the audio data, which reduces
    the amount of work that the neural network must do. It is in fact possible for
    a specifically designed model, like [DeepMind’s WaveNet](https://oreil.ly/IH9J3),
    to take raw sample data as its input instead, but the resulting models tend to
    involve more computation than the combination of a neural network fed with hand-engineered
    features that we’re using, so for resource-constrained environments like embedded
    systems, we prefer the approach used here.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个方面是生成的谱图比样本数据要小得多。每个谱图由1,960个数值组成，而波形有16,000个。它们是音频数据的摘要，减少了神经网络必须进行的工作量。事实上，一个专门设计的模型，比如[DeepMind的WaveNet](https://oreil.ly/IH9J3)，可以将原始样本数据作为输入，但结果模型往往涉及比我们使用的神经网络加手工设计特征组合更多的计算，因此对于资源受限的环境，如嵌入式系统，我们更喜欢这里使用的方法。
- en: How Does Feature Generation Work?
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征生成是如何工作的？
- en: If you’ve had experience working with audio processing, you might be familiar
    with approaches like [mel-frequency cepstral coefficients (MFCCs)](https://oreil.ly/HTAev).
    This is a common approach to generating the kind of spectrograms we’re working
    with, but our example actually uses a related but different approach. It’s the
    same method used in production across Google, which means that it has had a lot
    of practical validation, but it hasn’t been published in the research literature.
    Here, we describe roughly how it works, but for the details the best reference
    is [the code itself](https://oreil.ly/NeOnW).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有处理音频的经验，你可能熟悉像[梅尔频率倒谱系数（MFCCs）](https://oreil.ly/HTAev)这样的方法。这是一种常见的生成我们正在使用的谱图的方法，但我们的示例实际上使用了一种相关但不同的方法。这是谷歌在生产中使用的相同方法，这意味着它已经得到了很多实际验证，但它还没有在研究文献中发表。在这里，我们大致描述了它的工作原理，但对于详细信息，最好的参考是[代码本身](https://oreil.ly/NeOnW)。
- en: The process begins by generating a Fourier transform, (also known as a fast
    Fourier transform or FFT) for a given time slice—in our case 30 ms of audio data.
    This FFT is generated on data that’s been filtered with a [Hann window](https://oreil.ly/jhn8c),
    a bell-shaped function that reduces the influence of samples at either end of
    the 30-ms window. A Fourier transform produces complex numbers with real and imaginary
    components for every frequency, but all we care about is the overall energy, so
    we sum the squares of the two components and then apply a square root to get a
    magnitude for each frequency bucket.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程开始通过为给定时间片段生成傅立叶变换（也称为快速傅立叶变换或FFT）-在我们的情况下是30毫秒的音频数据。这个FFT是在使用[汉宁窗口](https://oreil.ly/jhn8c)过滤的数据上生成的，汉宁窗口是一个钟形函数，减少了30毫秒窗口两端样本的影响。傅立叶变换为每个频率产生具有实部和虚部的复数，但我们只关心总能量，因此我们对两个分量的平方求和，然后应用平方根以获得每个频率桶的幅度。
- en: Given *N* samples, a Fourier transform produces information on *N*/2 frequencies.
    30 ms at a rate of 16,000 samples per second requires 480 samples, and because
    our FFT algorithm needs a power of two input, we pad that with zeros to 512 samples,
    giving us 256 frequency buckets. This is larger than we need, so to shrink it
    down we average adjacent frequencies into 40 downsampled buckets. This downsampling
    isn’t linear, though; instead, it uses the human perception–based mel frequency
    scale to give more weight to lower frequencies so that there are more buckets
    available for them, and higher frequencies are merged into broader buckets. [Figure 8-26](#feature_generation)
    presents a diagram of that process.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 给定*N*个样本，傅立叶变换提供*N*/2个频率的信息。以每秒16,000个样本的速率的30毫秒需要480个样本，因为我们的FFT算法需要二的幂输入，所以我们用零填充到512个样本，给我们256个频率桶。这比我们需要的要大，因此为了缩小它，我们将相邻频率平均到40个降采样桶中。然而，这种降采样不是线性的；相反，它使用基于人类感知的梅尔频率刻度，以便更多地为低频率分配权重，从而为它们提供更多的桶，而高频率则合并到更广泛的桶中。[图8-26](#feature_generation)展示了该过程的图表。
- en: '![Diagram of the feature generation process.](Images/timl_0826.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![特征生成过程的图表。](Images/timl_0826.png)'
- en: Figure 8-26\. Diagram of the feature-generation process
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-26。特征生成过程的图表
- en: One unusual aspect of this feature generator is that it then includes a noise
    reduction step. This works by keeping a running average of the value in each frequency
    bucket and then subtracting this average from the current value. The idea is that
    background noise will be fairly constant over time and show up in particular frequencies.
    By subtracting the running average, we have a good chance of removing some of
    the effect of that noise and leaving the more rapidly changing speech that we’re
    interested in intact. The tricky part is that the feature generator does retain
    state to track the running averages for each bucket, so if you’re trying to reproduce
    the same spectrogram output for a given input—like we try to [for testing](https://oreil.ly/HtPve)—you
    will need to reset that state to the correct values.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特征生成器的一个不寻常之处是它包含了一个降噪步骤。这通过保持每个频率桶中的值的运行平均值，然后从当前值中减去这个平均值来实现。其思想是背景噪音随时间保持相对恒定，并显示在特定频率上。通过减去运行平均值，我们有很大机会去除一些噪音的影响，保留我们感兴趣的更快变化的语音。棘手的部分是特征生成器确实保留状态以跟踪每个桶的运行平均值，因此如果您尝试为给定输入重现相同的频谱图输出——就像我们尝试的那样[进行测试](https://oreil.ly/HtPve)——您将需要将该状态重置为正确的值。
- en: Another part of the noise reduction that initially surprised us was its use
    of different coefficients for the odd and even frequency buckets. This results
    in the distinctive comb-tooth patterns that you can see in the final generated
    feature images (Figures [8-22](#features_yes_0) through [8-25](#features_no_1)).
    Initially we thought this was a bug, but on talking to the original implementors,
    we learned that it was actually added deliberately to help performance. There’s
    an extended discussion of this approach in section 4.3 of the [“Trainable Frontend
    for Robust and Far-Field Keyword Spotting”](https://oreil.ly/QZ4Yb), by Yuxuan
    Wang et al. which also includes the background to some of the other design decisions
    that went into this feature generation pipeline. We also tested it empirically
    with our model, and removing the difference in the treatment of odd and even buckets
    did noticeably reduce accuracy in evaluations.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 噪音降低的另一个部分最初让我们感到惊讶的是它对奇数和偶数频率桶使用不同系数。这导致了您可以在最终生成的特征图像中看到的独特的梳齿图案（图[8-22](#features_yes_0)至[8-25](#features_no_1)）。最初我们以为这是一个错误，但在与原始实施者交谈后，我们了解到这实际上是有意为之，以帮助性能。在[Yuxuan
    Wang等人的“用于强健和远场关键词检测的可训练前端”](https://oreil.ly/QZ4Yb)的第4.3节中对这种方法进行了详细讨论，该论文还包括了进入此特征生成流程的其他设计决策的背景。我们还通过我们的模型进行了实证测试，去除奇数和偶数桶处理差异确实会显着降低评估的准确性。
- en: We then use per-channel amplitude normalization (PCAN) auto-gain to boost the
    signal based on the running average noise. Finally, we apply a log scale to all
    the bucket values, so that relatively loud frequencies don’t drown out quieter
    portions of the spectrum—a normalization that helps the subsequent model work
    with the features.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用每通道幅度归一化（PCAN）自动增益，根据运行平均噪音来增强信号。最后，我们对所有桶值应用对数尺度，以便相对较大的频率不会淹没频谱中较安静的部分——这种归一化有助于后续模型处理这些特征。
- en: This process is repeated 49 times in total, with a 30-ms window that’s moved
    forward 20 ms each time between iterations, to cover the full one second of audio
    input data. This produces a 2D array of values that’s 40 elements wide (one for
    each frequency bucket) and 49 rows high (one row for each time slice).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程总共重复了49次，每次之间以30毫秒的窗口向前移动20毫秒，以覆盖完整的一秒音频输入数据。这产生了一个40个元素宽（每个频率桶一个）和49行高（每个时间片一个）的值的2D数组。
- en: If this all sounds very complicated to implement, don’t worry. Because the code
    that implements it is all open source, you’re welcome to reuse it in your own
    audio projects.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一切听起来很复杂，不用担心。因为实现它的代码都是开源的，您可以在自己的音频项目中重用它。
- en: Understanding the Model Architecture
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解模型架构
- en: The neural network model we’re using is defined as a small graph of operations.
    You can find the code that defines it at training time in the [`create_tiny_conv_model()`
    function](https://oreil.ly/fMARv), and [Figure 8-27](#model_visualization) presents
    a visualization of the result.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用的神经网络模型被定义为一组操作的小图。您可以在[`create_tiny_conv_model()`函数](https://oreil.ly/fMARv)中找到定义它的代码，并且[图8-27](#model_visualization)展示了结果的可视化。
- en: This model consists of a convolutional layer, followed by a fully connected
    layer, and then a softmax layer at the end. In the figure the convolutional layer
    is labeled as “DepthwiseConv2D,” but this is just a quirk of the TensorFlow Lite
    converter (it turns out that a convolutional layer with a single-channel input
    image can also be expressed as a depthwise convolution). You’ll also see a layer
    labeled “Reshape_1,” but this is just an input placeholder rather than a real
    operation.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由一个卷积层、一个全连接层和最后的softmax层组成。在图中，卷积层标记为“DepthwiseConv2D”，但这只是TensorFlow Lite转换器的一个怪癖（事实证明，具有单通道输入图像的卷积层也可以表示为深度卷积）。您还会看到一个标记为“Reshape_1”的层，但这只是一个输入占位符，而不是一个真正的操作。
- en: '![A visualization the speech recognition model as a graph.](Images/timl_0827.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![将语音识别模型可视化为图形。](Images/timl_0827.png)'
- en: Figure 8-27\. Graph visualization of the speech recognition model, courtesy
    of [the Netron tool](https://oreil.ly/UiuXU)
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-27。语音识别模型的图形可视化，由[Netron工具](https://oreil.ly/UiuXU)提供
- en: Convolutional layers are used for spotting 2D patterns in input images. Each
    filter is a rectangular array of values that is moved as a sliding window across
    the input, and the output image is a representation of how closely the input and
    filter match at every point. You can think of the convolution operation as moving
    a series of rectangular filters across the image, with the result at each pixel
    for each filter corresponding to how similar the filter is to that patch in the
    image. In our case, each filter is 8 pixels wide and 10 high, and there are 8
    of them in total. Figures [8-28](#filter0) through [8-35](#filter7) show what
    they look like.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层用于在输入图像中发现2D模式。每个滤波器是一个值的矩形数组，它作为一个滑动窗口在输入上移动，输出图像表示输入和滤波器在每个点匹配程度。您可以将卷积操作视为在图像上移动一系列矩形滤波器，每个滤波器在每个像素处的结果对应于滤波器与图像中该补丁的相似程度。在我们的情况下，每个滤波器宽8像素，高10像素，总共有8个。图[8-28](#filter0)到[8-35](#filter7)显示它们的外观。
- en: '![Visualization of convolutional filter.](Images/timl_0828.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![卷积滤波器的可视化。](Images/timl_0828.png)'
- en: Figure 8-28\. First filter image
  id: totrans-281
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-28。第一个滤波器图像
- en: '![Visualization of convolutional filter.](Images/timl_0829.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![卷积滤波器的可视化。](Images/timl_0829.png)'
- en: Figure 8-29\. Second filter image
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-29。第二个滤波器图像
- en: '![Visualization of convolutional filter.](Images/timl_0830.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![卷积滤波器的可视化。](Images/timl_0830.png)'
- en: Figure 8-30\. Third filter image
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-30。第三个滤波器图像
- en: '![Visualization of convolutional filter.](Images/timl_0831.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![卷积滤波器的可视化。](Images/timl_0831.png)'
- en: Figure 8-31\. Fourth filter image
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-31。第四个滤波器图像
- en: '![Visualization of convolutional filter.](Images/timl_0832.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![卷积滤波器的可视化。](Images/timl_0832.png)'
- en: Figure 8-32\. Fifth filter image
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-32。第五个滤波器图像
- en: '![Visualization of convolutional filter.](Images/timl_0833.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![卷积滤波器的可视化。](Images/timl_0833.png)'
- en: Figure 8-33\. Sixth filter image
  id: totrans-291
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-33。第六个滤波器图像
- en: '![Visualization of convolutional filter.](Images/timl_0834.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![卷积滤波器的可视化。](Images/timl_0834.png)'
- en: Figure 8-34\. Seventh filter image
  id: totrans-293
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-34。第七个滤波器图像
- en: '![Visualization of convolutional filter.](Images/timl_0835.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![卷积滤波器的可视化。](Images/timl_0835.png)'
- en: Figure 8-35\. Eighth filter image
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-35。第八个滤波器图像
- en: You can think of each of these filters as a small patch of the input image.
    The operation is trying to match this small patch to parts of the input image
    that look similar. Where the image is similar to the patch, a high value will
    be written into the corresponding part of the output image. Intuitively, each
    filter is a pattern that the model has learned to look for in the training inputs
    to help it distinguish between the different classes that it has to deal with.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将这些滤波器中的每一个视为输入图像的一个小补丁。该操作试图将此小补丁与看起来相似的输入图像部分进行匹配。当图像与补丁相似时，高值将被写入输出图像的相应部分。直观地说，每个滤波器都是模型已经学会在训练输入中寻找的模式，以帮助它区分不同类别。
- en: Because we have eight filters, there will be eight different output images,
    each corresponding to the respective filter’s match value as it’s slid across
    the input. These filter outputs are actually combined into a single output image
    with eight channels. We have set the stride to be two in both directions, which
    means we slide each filter by two pixels each time, rather than just by one. Because
    we’re skipping every other position, this means our output image is half the size
    of the input.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有八个滤波器，所以将有八个不同的输出图像，每个对应于相应滤波器的匹配值，当它在输入上滑动时。这些滤波器输出实际上被合并为一个具有八个通道的单个输出图像。我们已将步幅设置为两个方向，这意味着每次我们将每个滤波器向前滑动两个像素，而不仅仅是一个像素。因为我们跳过每个其他位置，这意味着我们的输出图像是输入大小的一半。
- en: You can see in the visualization that the input image is 49 pixels high and
    40 wide, with a single channel, which is what we’d expect given the feature spectrograms
    we discussed in the previous section. Because we’re skipping every other pixel
    in the horizontal and vertical directions when we slide the convolutional filters
    across the input, the output of the convolution is half the size, or 25 pixels
    high and 20 wide. There are eight filters though, so the image becomes eight channels
    deep.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到在可视化中，输入图像高49像素，宽40像素，具有单个通道，这是我们在前一节中讨论的特征频谱图所期望的。因为我们在水平和垂直方向上滑动卷积滤波器时跳过每个其他像素，所以卷积的输出是一半大小，即高25像素，宽20像素。然而有八个滤波器，所以图像变为八个通道深。
- en: The next operation is a fully connected layer. This is a different kind of pattern
    matching process. Instead of sliding a small window across the input, there’s
    a weight for every value in the input tensor. The result is an indication of how
    closely the input matches the weights, after comparing every value. You can think
    of this as a global pattern match, where you have an ideal result that you’d expect
    to get as an input, and the output is how close that ideal (held in the weights)
    is to the actual input. Each class in our model has its own weights, so there’s
    an ideal pattern for “silence,” “unknown,” “yes,” and “no,” and four output values
    are generated. There are 4,000 values in the input `(25 * 20 * 8)`, so each class
    is represented by 4,000 weights.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个操作是全连接层。这是一种不同的模式匹配过程。与在输入上滑动一个小窗口不同，这里为输入张量中的每个值都有一个权重。结果是指示输入与权重匹配程度的指标，在比较每个值之后。您可以将其视为全局模式匹配，其中您有一个理想的结果，您期望作为输入获得，输出是理想值（保存在权重中）与实际输入之间的接近程度。我们模型中的每个类都有自己的权重，因此“静音”，“未知”，“是”和“否”都有一个理想模式，并生成四个输出值。输入中有4,000个值`(25
    * 20 * 8)`，因此每个类由4,000个权重表示。
- en: The last layer is a softmax. This effectively helps increase the difference
    between the highest output and its nearest competitors, which doesn’t change their
    relative order (whichever class produced the largest value from the fully connected
    layer will remain the highest) but does help produce a more useful score. This
    score is often informally referred to as a *probability*, but strictly speaking
    you can’t reliably use it like that without more calibration on what the mix of
    input data actually is. For example, if you had more words in the detector, it’s
    likely that an uncommon one like “antidisestablishmentarianism” would be less
    likely to show up than something like “okay,” but depending on the distribution
    of the training data that might not be reflected in the raw scores.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层是一个softmax层。这有效地增加了最高输出和其最近竞争对手之间的差异，这不会改变它们的相对顺序（从全连接层产生最大值的类仍将保持最高），但有助于产生一个更有用的分数。这个分数通常非正式地被称为“概率”，但严格来说，如果没有更多关于输入数据实际混合的校准，你不能可靠地像那样使用它。例如，如果检测器中有更多的单词，那么像“反对建立教会主义”这样的不常见单词可能不太可能出现，而像“好的”这样的单词可能更有可能出现，但根据训练数据的分布，这可能不会反映在原始分数中。
- en: 'As well as these major layers, there are biases that are added on to the results
    of the fully connected and convolutional layers to help tweak their outputs, and
    a rectified linear unit (ReLU) activation function after each. The ReLU just makes
    sure that no output is less than zero, setting any negative results to a minimum
    of zero. This type of activation function was one of the breakthroughs that enabled
    deep learning to become much more effective: it helps the training process converge
    much more quickly than the network would otherwise.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些主要层外，还有偏差被添加到全连接和卷积层的结果中，以帮助调整它们的输出，并在每个之后使用修正线性单元（ReLU）激活函数。ReLU只是确保没有输出小于零，将任何负结果设置为零的最小值。这种类型的激活函数是使深度学习变得更加有效的突破之一：它帮助训练过程比网络本来会更快地收敛。
- en: Understanding the Model Output
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解模型输出
- en: The end result of the model is the output of the softmax layer. This is four
    numbers, one for each of “silence,” “unknown,” “yes,” and “no.” These values are
    the scores for each category, and the one with the highest score is the model’s
    prediction, with the score representing the confidence the model has in its prediction.
    As an example, if the model output is `[10, 4, 231, 80]`, it’s predicting that
    the third category, “yes,” is the most likely result with a score of 231\. (We’re
    giving these values in their quantized forms, between 0 and 255, but because these
    are just relative scores it’s not usually necessary to convert them back to their
    real-valued equivalents.)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的最终结果是softmax层的输出。这是四个数字，分别对应“沉默”，“未知”，“是”和“否”。这些值是每个类别的分数，具有最高分数的类别是模型的预测，分数代表模型对其预测的信心。例如，如果模型输出是`[10,
    4, 231, 80]`，它预测第三个类别“是”是最可能的结果，得分为231。 （我们以它们的量化形式给出这些值，介于0和255之间，但因为这些只是相对分数，通常不需要将它们转换回它们的实值等价物。）
- en: One thing that’s tricky is that this result is based on analyzing the entire
    last second of audio. If we run it only once per second, we might end up with
    an utterance that is half in the previous second, and half in the current. It’s
    not possible for any model to do a good job recognizing a word when it hears only
    a part of it, so in that case the word spotting would fail. To overcome this,
    we need to run the model more often than once per second to give us as high a
    chance as possible of catching an entire word in our one-second window. In practice,
    we’ve found we have to run it 10 or 15 times per second to achieve good results.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 有一件棘手的事情是，这个结果是基于分析音频的最后一秒。如果我们每秒只运行一次，可能会得到一个话语，一半在上一秒，一半在当前秒。当模型只听到部分单词时，任何模型都不可能很好地识别单词，因此在这种情况下，单词识别会失败。为了克服这个问题，我们需要比每秒运行模型更频繁，以尽可能高的概率在我们的一秒窗口内捕捉到整个单词。实际上，我们发现我们必须每秒运行10到15次才能取得良好的结果。
- en: If we’re getting all of these results coming in so fast, how do we decide when
    a score is high enough? We implement a postprocessing class that averages the
    scores over time and triggers a recognition only when we’ve had several high scores
    for the same word in a short amount of time. You can see the implementation of
    this in the [`RecognizeCommands class`](https://oreil.ly/FuYfL). This is fed the
    raw results from the model, and then it uses an accumulation and averaging algorithm
    to determine whether any of the categories have crossed the threshold. These postprocessed
    results are then fed to the [`CommandResponder`](https://oreil.ly/b8ArK) to take
    an action, depending on the platform’s output capabilities.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们得到这些结果如此迅速，我们如何决定何时得分足够高？我们实现了一个后处理类，它会随着时间平均分数，并仅在短时间内同一个单词的得分高时触发识别。您可以在[`RecognizeCommands类`](https://oreil.ly/FuYfL)中看到这个实现。这个类接收模型的原始结果，然后使用累积和平均算法来确定是否有任何类别已经超过了阈值。然后将这些后处理结果传递给[`CommandResponder`](https://oreil.ly/b8ArK)以根据平台的输出能力采取行动。
- en: The model parameters are all learned from the training data, but the algorithm
    used by the command recognizer was manually created, so all of the [thresholds](https://oreil.ly/tfNfr)—like
    the score value required to trigger a recognition, or the time window of positive
    results needed—have been hand-picked. This means that there’s no guarantee they
    are optimal, so if you’re seeing poor results in your own application, you might
    want to try tweaking them yourself.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数都是从训练数据中学习的，但命令识别器使用的算法是手动创建的，所以所有的[阈值](https://oreil.ly/tfNfr)——比如触发识别所需的得分值，或者需要的正结果时间窗口——都是手动选择的。这意味着不能保证它们是最佳的，所以如果在您自己的应用中看到不佳的结果，您可能希望尝试自己调整它们。
- en: More sophisticated speech recognition models typically use a model that’s able
    to take in streaming data (like a recursive neural network) rather than the single-layer
    convolutional network we show in this chapter. Having the streaming baked into
    the model design means that you don’t need to do the postprocessing to get accurate
    results, though it does make the training significantly more complicated.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的语音识别模型通常使用能够接收流数据的模型（如递归神经网络），而不是我们在本章中展示的单层卷积网络。将流式处理嵌入到模型设计中意味着您无需进行后处理即可获得准确的结果，尽管这确实使训练变得更加复杂。
- en: Training with Your Own Data
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用您自己的数据进行训练
- en: It’s not very likely that the product you want to build only needs to respond
    to “yes” and “no,” so you’ll want to train a model that is sensitive to the audio
    you care about. The training script we used earlier has been designed to let you
    create custom models using your own data. The toughest part of the process is
    usually gathering a large enough dataset, and ensuring that it’s appropriate for
    your problem. We discuss general approaches to data gathering and cleaning in
    [Chapter 16](ch16.xhtml#chapter_optimizing_energy_usage), but this section covers
    some of the ways in which you can train your own audio model.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 您要构建的产品很可能不仅需要回答“是”和“否”，因此您需要训练一个对您关心的音频敏感的模型。我们之前使用的训练脚本旨在让您使用自己的数据创建自定义模型。这个过程中最困难的部分通常是收集足够大的数据集，并确保它适用于您的问题。我们在[第16章](ch16.xhtml#chapter_optimizing_energy_usage)中讨论了数据收集和清理的一般方法，但本节涵盖了一些您可以训练自己的音频模型的方法。
- en: The Speech Commands Dataset
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音命令数据集
- en: The *train.py* script downloads the Speech Commands dataset by default. This
    is an open source collection of more than 100,000 one-second WAV files, covering
    a variety of short words from a lot of different speakers. It’s distributed by
    Google, but the utterances have been collected from volunteers around the world.
    [“Visual Wake Words Dataset”](https://oreil.ly/EC6nd) by Aakanksha Chowdhery et
    al. provides more details.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*train.py*脚本默认下载了Speech Commands数据集。这是一个开源集合，包含超过10万个一秒钟的WAV文件，涵盖了许多不同说话者的各种短单词。它由Google分发，但话语是从世界各地的志愿者那里收集的。Aakanksha
    Chowdhery等人的[“Visual Wake Words Dataset”](https://oreil.ly/EC6nd)提供了更多细节。'
- en: As well as *yes* and *no*, the dataset includes eight other command words (*on*,
    *off*, *up*, *down*, *left*, *right*, *stop* and *go*), and the 10 digits from
    *zero* through *nine*. There are several thousand examples of each of these words.
    There are also other words, like *Marvin*, that have a lot fewer examples each.
    The command words are intended to have enough utterances that you can train a
    reasonable model to recognize them. The other words are intended to be used to
    populate an *unknown* category, so a model can spot when a word it’s not been
    trained on is uttered, instead of mistaking it for a command.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 除了“是”和“否”之外，数据集还包括另外八个命令词（“打开”，“关闭”，“上”，“下”，“左”，“右”，“停止”和“前进”），以及从“零”到“九”的十个数字。每个单词都有几千个示例。还有其他单词，比如“Marvin”，每个单词的示例要少得多。命令词旨在有足够的话语，以便您可以训练一个合理的模型来识别它们。其他单词旨在用于填充“未知”类别，因此模型可以发现当发出未经训练的单词时，而不是将其误认为是一个命令。
- en: Because the training script uses this dataset, you can easily train a model
    on a combination of some of the command words that have lots of examples. If you
    update the `--wanted_words` argument with a comma-separated list of words present
    in the training set and run training from scratch, you should find you can create
    a useful model. The main things to watch out for are that you are restricting
    yourself to the 10 command words and/or digits, or you won’t have enough examples
    to train accurately, and that you adjust the `--silence_percentage` and `--unknown_percentage`
    values down if you have more than two wanted words. These last two arguments control
    how many silent and unknown samples are mixed in during training. The *silent*
    examples aren’t actually complete silence; instead, they’re randomly selected
    one-second snippets of recorded background noise, pulled from the WAVs in the
    *background* folder of the dataset. The *unknown* samples are utterances picked
    from any of the words that are in the training set, but aren’t in the `wanted_words`
    list. This is why we have a selection of miscellaneous words in the dataset with
    comparatively few utterances each; it gives us the chance to recognize that a
    lot of different words aren’t actually the ones we’re looking for. This is a particular
    problem with speech and audio recognition, because our products often need to
    operate in environments in which there are a lot of words and noises we might
    never have encountered in training. There are many thousands of different words
    that could show up just in common English, and to be useful, a model must be able
    to ignore those on which it hasn’t been trained. That’s why the *unknown* category
    is so important in practice.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练脚本使用了这个数据集，您可以轻松地训练一个模型，结合一些有很多示例的命令词。如果您使用训练集中存在的单词的逗号分隔列表更新“--wanted_words”参数，并从头开始运行训练，您应该会发现您可以创建一个有用的模型。需要注意的主要事项是，您要限制自己只使用这10个命令词和/或数字，否则您将没有足够的示例进行准确训练，并且如果您有超过两个想要的单词，则需要将“--silence_percentage”和“--unknown_percentage”值调低。这两个参数控制训练过程中混合了多少无声和未知样本。*无声*示例实际上并不是完全的沉默；相反，它们是从数据集的*background*文件夹中的WAV文件中随机选择的一秒钟的录制背景噪音片段。*未知*样本是从训练集中的任何单词中挑选出来的话语，但不在“wanted_words”列表中。这就是为什么数据集中有一些杂项单词，每个单词的话语相对较少；这让我们有机会认识到很多不同的单词实际上并不是我们正在寻找的单词。这在语音和音频识别中是一个特别的问题，因为我们的产品通常需要在可能从未在训练中遇到的环境中运行。仅在常见英语中就可能出现成千上万个不同的单词，为了有用，模型必须能够忽略那些它没有经过训练的单词。这就是为什么*未知*类别在实践中如此重要。
- en: 'Here is an example of training on different words using the existing dataset:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用现有数据集训练不同单词的示例：
- en: '[PRE26]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Training on Your Own Dataset
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在您自己的数据集上训练
- en: The default for the training script is to use Speech Commands, but if you have
    your own dataset, you can use the `--data_dir` argument to use it, instead. The
    directory you’re pointing to should be organized like Speech Commands, with one
    subfolder per class that you want to recognize, each containing a set of WAV files.
    You should also have a special *background* subfolder that contains longer WAV
    recordings of the kind of background noise you expect your application to encounter.
    You’ll also need to pick a recognition duration if the default of one second doesn’t
    work for your use case, and specify it through the `--sample_duration_ms` argument.
    Then you can set the classes that you want to recognize using the `--wanted_words`
    argument. Despite the name, these classes can be any kind of audio event, from
    breaking glass to laughter; as long as you have enough WAVs of each class the
    training process should work just as it does for speech.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本的默认设置是使用Speech Commands，但如果您有自己的数据集，可以使用`--data_dir`参数来使用它。您指向的目录应该像Speech
    Commands一样组织，每个包含一组WAV文件的类别都有一个子文件夹。您还需要一个特殊的*background*子文件夹，其中包含您的应用程序预计会遇到的背景噪音类型的较长的WAV录音。如果默认的一秒持续时间对您的用例不起作用，您还需要选择一个识别持续时间，并通过`--sample_duration_ms`参数指定。然后，您可以使用`--wanted_words`参数设置要识别的类别。尽管名称如此，这些类别可以是任何类型的音频事件，从玻璃破碎到笑声；只要您有足够的每个类别的WAV文件，训练过程应该与语音一样有效。
- en: 'If you had folders of WAVs named *glass* and *laughter* inside a root */tmp/my_wavs*
    directory, here’s how you could train your own model:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在根目录*/tmp/my_wavs*中有名为*glass*和*laughter*的WAV文件夹，这是如何训练您自己的模型的：
- en: '[PRE27]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The most difficult part often is finding enough data. As an example, it turns
    out that the real sound of breaking glass is very different from the sound effects
    we’re used to hearing in movies. This means that you need to either find existing
    recordings, or arrange to record some yourself. Because the training process can
    require many thousand examples of each class, and they need to cover all of the
    variations that are likely to occur in a real application, this data-gathering
    process can be frustrating, expensive, and time-consuming.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 通常最困难的部分是找到足够的数据。例如，事实证明，真实的玻璃破碎声与我们在电影中听到的声音效果非常不同。这意味着你需要找到现有的录音，或者安排自己录制一些。由于训练过程可能需要每个类别的成千上万个示例，并且它们需要涵盖在真实应用中可能发生的所有变化，这个数据收集过程可能令人沮丧、昂贵且耗时。
- en: A common solution for this with image models is to use *transfer learning*,
    where you take a model that’s been trained on a large public dataset and fine-tune
    its weights on different classes using other data. This approach doesn’t require
    nearly as many examples in the secondary dataset as you would need if you were
    training from scratch with it, and it often produces high-accuracy results. Unfortunately
    transfer learning for speech models is still being researched, but watch this
    space.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像模型，一个常见的解决方案是使用*迁移学习*，即使用已经在大型公共数据集上训练过的模型，并使用其他数据对不同类别进行微调。这种方法在次要数据集中不需要像从头开始训练那样多的示例，而且通常会产生高准确度的结果。不幸的是，语音模型的迁移学习仍在研究中，但请继续关注。
- en: How to Record Your Own Audio
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何录制您自己的音频
- en: If you need to capture audio of words you care about, it’s a lot easier if you
    have a tool that prompts speakers and splits the result into labeled files. The
    Speech Commands dataset was recorded using the [Open Speech Recording app](https://oreil.ly/UWsG3),
    a hosted app that lets users record utterances through most common web browsers.
    As a user, you’ll see a web page that first asks you to agree to being recorded,
    with a default Google agreement, that’s [easily changeable](https://oreil.ly/z5vka).
    After you have agreed, you’re sent to a new page that has recording controls.
    When you press the record button, words will appear as prompts, and the audio
    you say for each word is recorded. When all of the requested words have been recorded,
    you’ll be asked to submit the results to the server.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要捕捉您关心的单词的音频，如果您有一个提示说话者并将结果拆分为标记文件的工具，那将会更容易。Speech Commands数据集是使用[Open
    Speech Recording app](https://oreil.ly/UWsG3)录制的，这是一个托管应用程序，允许用户通过大多数常见的网络浏览器录制话语。作为用户，您将看到一个网页，首先要求您同意被录制，带有默认的谷歌协议，这是[可以轻松更改的](https://oreil.ly/z5vka)。同意后，您将被发送到一个具有录音控件的新页面。当您按下录制按钮时，单词将作为提示出现，您说的每个单词的音频将被记录。当所有请求的单词都被记录时，您将被要求将结果提交到服务器。
- en: 'There are instructions in the README for running it on Google Cloud, but it’s
    a Flask app written in Python, so you should be able to port it to other environments.
    If you are using Google Cloud, you’ll need to update the [*app.yaml*](https://oreil.ly/dV2kv)
    file to point to your own storage bucket and supply your own random session key
    (this is used just for hashing, so it can be any value). To customize which words
    are recorded, you’ll need to edit a couple of arrays in [the client-side JavaScript](https://oreil.ly/XcJIe):
    one for the frequently repeated main words, and one for the secondary fillers.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: README中有在Google Cloud上运行的说明，但这是一个用Python编写的Flask应用程序，因此您应该能够将其移植到其他环境中。如果您使用Google
    Cloud，您需要更新[*app.yaml*](https://oreil.ly/dV2kv)文件，指向您自己的存储桶，并提供您自己的随机会话密钥（这仅用于哈希，因此可以是任何值）。要自定义记录的单词，您需要编辑[客户端JavaScript](https://oreil.ly/XcJIe)中的一些数组：一个用于频繁重复的主要单词，一个用于次要填充词。
- en: 'The recorded files are stored as OGG compressed audio in the Google Cloud bucket,
    but training requires WAVs, so you need to convert them. It’s also likely that
    some of your recordings contain errors, like people forgetting to say the word
    or saying it too quietly, so it’s helpful to automatically filter out those mistakes
    where possible. If you have set up your bucket name in a `BUCKET_NAME` variable,
    you can begin by copying your files to a local machine by using these bash commands:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 记录的文件以OGG压缩音频的形式存储在Google Cloud存储桶中，但训练需要WAV文件，因此您需要将它们转换。而且很可能您的一些录音包含错误，比如人们忘记说单词或说得太轻，因此在可能的情况下自动过滤出这些错误是有帮助的。如果您已经在`BUCKET_NAME`变量中设置了您的存储桶名称，您可以通过使用以下bash命令将文件复制到本地机器开始：
- en: '[PRE28]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'One nice property of the compressed OGG format is that quiet or silent audio
    results in very small files, so a good first step is removing any that are particularly
    tiny, like so:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩的OGG格式的一个好处是安静或无声的音频会生成非常小的文件，因此一个很好的第一步是删除那些特别小的文件，比如：
- en: '[PRE29]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The easiest way we’ve found to convert OGGs to WAVs is using the [FFmpeg project](https://ffmpeg.org/),
    which offers a command-line tool. Here are a set of commands that can convert
    an entire directory of OGG files into the format we need:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现将OGG转换为WAV的最简单方法是使用[FFmpeg项目](https://ffmpeg.org/)，它提供了一个命令行工具。以下是一组命令，可以将一个目录中的所有OGG文件转换为我们需要的格式：
- en: '[PRE30]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The Open Speech Recording application records more than one second for each
    word. This ensures that the user’s utterance is captured even if their timing
    is a bit earlier or later than we expect. The training requires one-second recordings,
    and it works best if the word is centered in the middle of each recording. We’ve
    created a small open source utility to look at the volume of each recording over
    time to try to get the centering right and trim the audio so that it is just one
    second. Enter the following commands in your terminal to use it:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 开放语音录制应用程序为每个单词记录超过一秒的音频。这确保了用户的话语被捕捉到，即使他们的时间比我们预期的早或晚一点。训练需要一秒钟的录音，并且最好是单词位于每个录音的中间。我们创建了一个小型开源实用程序，用于查看每个录音随时间的音量，以便正确居中并修剪音频，使其仅为一秒钟。在终端中输入以下命令来使用它：
- en: '[PRE31]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This will give you a folder full of files in the correct format and of the required
    length, but the training process needs the WAVs organized into subfolders by labels.
    The label is encoded in the name of each file, so we have [an example Python script](https://oreil.ly/BpQBJ)
    that uses those filenames to sort them into the appropriate folders.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为您提供一个格式正确且所需长度的文件夹，但训练过程需要将WAV文件按标签组织到子文件夹中。标签编码在每个文件的名称中，因此我们有一个[示例Python脚本](https://oreil.ly/BpQBJ)，它使用这些文件名将它们分类到适当的文件夹中。
- en: Data Augmentation
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: Data augmentation is another method to effectively enlarge your training data
    and improve accuracy. In practice, this means taking recorded utterances and applying
    audio transformations to them before they’re used for training. These transforms
    can include altering the volume, mixing in background noise, or trimming the start
    or end of the clips slightly. The training script applies all of these transformations
    by default, but you can adjust how often they’re used and how strongly they’re
    applied using command-line arguments.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是另一种有效扩大训练数据并提高准确性的方法。在实践中，这意味着对记录的话语应用音频变换，然后再用于训练。这些变换可以包括改变音量、混入背景噪音，或者轻微修剪片段的开头或结尾。训练脚本默认应用所有这些变换，但您可以使用命令行参数调整它们的使用频率和强度。
- en: Warning
  id: totrans-336
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This kind of augmentation does help make a small dataset go further, but it
    can’t work miracles. If you apply transformations too strongly, you can end up
    distorting the training inputs so much that they’d no longer be recognizable by
    a person, which can cause the model to mistakenly start triggering on sounds that
    bear no resemblance to the intended categories.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这种增强确实有助于使小数据集发挥更大作用，但它不能创造奇迹。如果你应用变换太强烈，可能会使训练输入变形得无法被人识别，这可能导致模型错误地开始触发与预期类别毫不相似的声音。
- en: 'Here’s how you can use some of those command-line arguments to control the
    augmentation:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用其中一些命令行参数来控制增强：
- en: '[PRE32]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Model Architectures
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型架构
- en: 'The “yes"/"no” model we trained earlier was designed to be small and fast.
    It’s only 18 KB, and requires 400,000 arithmetic operations to execute once. To
    fit within those constraints, it trades off accuracy. If you’re designing your
    own application, you might want to make different trade-offs, especially if you’re
    trying to recognize more than two categories. You can specify your own model architectures
    by modifying the *models.py* file and then using the `--model_architecture` argument.
    You’ll need to write your own model creation function, like `create_tiny_conv_model0`
    but with the layers you want in your model specified instead. Then, you can update
    the `if` statement in `create_model0` to give your architecture a name, and call
    your new creation function when it’s passed in as the architecture argument on
    the command line. You can look at some of the existing creation functions for
    inspiration, including how to handle dropout. If you have added your own model
    code, here’s how you can call it:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前训练的“是”/“否”模型旨在小而快速。它只有18 KB，并且执行一次需要400,000次算术运算。为了符合这些约束条件，它牺牲了准确性。如果您正在设计自己的应用程序，您可能希望做出不同的权衡，特别是如果您试图识别超过两个类别。您可以通过修改*models.py*文件指定自己的模型架构，然后使用`--model_architecture`参数。您需要编写自己的模型创建函数，例如`create_tiny_conv_model0`，但要指定您想要的模型中的层。然后，您可以更新`create_model0`中的`if`语句，为您的架构命名，并在通过命令行传递架构参数时调用您的新创建函数。您可以查看一些现有的创建函数以获取灵感，包括如何处理辍学。如果您已添加了自己的模型代码，以下是如何调用它的方法：
- en: '[PRE33]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Wrapping Up
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Recognizing spoken words with a small memory footprint is a tricky real-world
    problem, and tackling it requires us to work with many more components than we
    need to for a simpler example. Most production machine learning applications require
    thinking about issues like feature generation, model architecture choices, data
    augmentation, finding the best-suited training data, and how to turn the results
    of a model into actionable information.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 识别具有小内存占用的口语是一个棘手的现实世界问题，解决它需要我们与比简单示例更多的组件一起工作。大多数生产机器学习应用程序需要考虑问题，如特征生成、模型架构选择、数据增强、找到最适合的训练数据，以及如何将模型的结果转化为可操作信息。
- en: There are a lot of trade-offs to consider depending on the actual requirements
    of your product, and hopefully you now understand some of the options you have
    as you try to move from training into deployment.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 根据产品的实际需求，需要考虑很多权衡，希望您现在了解一些选项，以便在从训练转向部署时使用。
- en: In the next chapter, we explore how to run inference with a different type of
    data that, although *seemingly* more complex than audio, is surprisingly easy
    to work with.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用不同类型的数据进行推断，尽管这种数据*看起来*比音频更复杂，但实际上却很容易处理。
