- en: 7 Evolving RAGOps stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The design of RAG systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available tools and technologies that enable a RAG system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Production best practices for RAG systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have discussed the indexing pipeline, generation pipeline, and evaluation
    of a retrieval-augmented generation (RAG) system. Chapter 6 also covered some
    advanced strategies and techniques that are useful when building production-grade
    RAG systems. These strategies help improve the accuracy of retrieval and generation
    and, in some cases, reduce the system latency. With all this information, you
    should be able to stitch together a RAG system for your use cases. Chapter 2 briefly
    laid out the design of a RAG system. This chapter elaborates on that design.
  prefs: []
  type: TYPE_NORMAL
- en: A RAG system is composed of standard application layers, as well as layers specific
    to generative AI applications. Stacked together, these layers create a robust
    RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: These layers are supported by a technology infrastructure. We delve into these
    layers and the available technologies and tools offered by popular service providers
    that can be used in crafting a RAG system. Some providers have started offering
    managed end-to-end RAG solutions, which we touch upon in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We wrap up the chapter with some learnings and best practices for putting RAG
    systems in production. Chapter 7 also marks the end of part 3 of the book.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should
  prefs: []
  type: TYPE_NORMAL
- en: Understand the details of the layers in a RAG (RAGOps) stack.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be familiar with a host of service providers and the tools and technologies
    they offer for RAG systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Know some of the pitfalls and best practices of putting RAG systems in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A RAG system includes a lot of additional components compared to traditional
    software applications. Vector stores and embeddings models are essential components
    of the indexing pipeline. Knowledge graphs are becoming increasingly popular indexing
    structures. The generation component can have different kinds of language models.
    In addition, prompt management is becoming increasingly complex. The production
    ecosystem for RAG and LLM (large language models) applications is still evolving,
    but early tooling and design patterns have emerged. RAGOps refers to the operational
    practices, tools, and processes involved in deploying, maintaining, and optimizing
    RAG systems in production environments.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 The evolving RAGOps stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section describes different components required to build a RAG system in
    layers. These layers come together to form the operations stack for RAG. We will
    also take this opportunity to revise the workflow of the RAG system discussed
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted that RAG, like generative AI in general, is an evolving
    technology, and therefore, the operations stack continues to evolve. You may find
    varying definitions and structures. This chapter provides a holistic view and
    discusses the components from the perspective of their criticality to the RAG
    system. We look at the layers divided into the following three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Critical layers that are fundamental to the operation of a RAG system. A RAG
    system is likely to fail if any of these layers are missing or are incomplete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essential layers that are important for performance, reliability, and safety
    of the system. These essential components bring the system to a standard that
    provides value to the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancement layers that improve the efficiency, scalability, and usability of
    the system. These components are used to make the RAG system better and are selected
    based on the end requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.1.1 Critical layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The indexing pipeline and the generation pipeline (discussed in detail in chapters
    3 and 4) form the core of a RAG system. Figure 7.1 illustrates the indexing pipeline
    that facilitates the creation of the knowledge base for RAG systems and the generation
    pipeline that uses the knowledge base to generate context-aware responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F01_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1  Indexing and generation pipelines forming the core of a RAG system
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Layers enabling these two pipelines form the critical layers of the RAGOps stack.
  prefs: []
  type: TYPE_NORMAL
- en: Data layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The data layer serves the critical role of creating and storing the knowledge
    base for RAG. It is responsible for collecting data from source systems, transforming
    it into a usable format, and storing it for efficient retrieval. Here are some
    components of the data layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data ingestion componen**t*—It collects data from source systems such as databases,
    content management systems, file systems, APIs, devices, and even the internet.
    The data can be ingested in batches or as a stream, depending on the use case.
    For ingesting data, your choice of tool can depend on factors such as data volume,
    types of data source, ingestion frequency, cost, and ease of setup. Data ingestion
    is not specific to RAG but is a mainstream component in modern software applications.
    AWS Glue, Azure Data Factory, Google Cloud Dataflow, Fivetran, Apache NiFi, Apache
    Kafka, and Airbyte are among tools available for use. For rapid prototyping and
    proof of concepts (PoCs), frameworks such as Lang­Chain and LlamaIndex have inbuilt
    functions that can assist in connecting to some sources and extracting information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data transformation componen**t*—It converts the ingested data from a raw
    to a usable form. A core process in the indexing pipeline is the *chunking* of
    data. We know that *embeddings* is the preferred format of choice for RAG applications
    because it makes it easier to apply semantic search. *Graph structures* are becoming
    increasingly popular in advanced systems. Certain pre-processing steps such as
    cleaning, de-duplication, metadata enrichment, and masking of sensitive information
    are also a part of this phase. While the volume of data and the nature of transformation
    play an important role in any data-transformation step, they are especially critical
    in RAG systems. All the extract–transform–load (ETL) tools mentioned in the data
    ingestion step in conjunction with tools such as Apache Spark and dbt also allow
    transformations. However, if we focus just on RAG, Unstructured.io specializes
    in processing and transforming unstructured data for use in LLM applications.
    It offers open source libraries as well as managed services. Constructing knowledge
    graphs from unstructured data has evolved today from early semantic networks and
    ontologies into robust frameworks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft’s GraphRAG is a framework that has pioneered the use of LLMs to extract
    entities and relationships from text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Data storage componen**t*—It stores the transformed data in a way that allows
    for fast and efficient retrieval. We have discussed that to store embeddings,
    *vector databases* are widely used because they are efficient in similarity search.
    For graph structures, *graph databases* are used. Most traditional database providers
    are incorporating vector search capabilities into their systems. Cost, scale,
    and speed are the primary drivers in the choice of data storage. We have used
    a vector index such as FAISS in this book. Pinecone is a fully managed cloud-native
    service. Milvus, Qdrant, and Chroma are among the open source vector databases.
    Weviate is another database that also has a GraphQL-based interface for knowledge
    graphs. Neo4j is a leading graph database for storing and querying graph data.
    A comparison of popular vector databases is available at [https://www.superlinked.com/vector-db-comparison](https://www.superlinked.com/vector-db-comparison).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The flow from source systems to data storage via the ingestion and transformation
    components that lead to the creation of the knowledge base is shown in figure
    7.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F02_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2  Data layer: Creating the knowledge base by extracting, transforming,
    and loading (ETL) data from source systems'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A strong data layer is the foundation of an efficient RAG system. The data layer
    also comes in handy when there is a need for fine-tuning of models. We discuss
    this feature briefly later in the chapter. Next, we look at the model layer, which
    includes the embeddings models used to transform text into vectors and the LLMs
    used in generation.
  prefs: []
  type: TYPE_NORMAL
- en: Model layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Predictive models enable generative AI applications. Some models are provided
    by third parties, and some need to be custom trained or fine-tuned. Generating
    quick and cost-effective model responses is also an important aspect of using
    predictive models. The model layer includes the following three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model librar**y*—It contains the list of models that have been chosen for
    the application. The most popular models are the LLMs that generate text and other
    generative models that can generate images, video, and audio. We saw that in the
    data layer, raw text is transformed into vector embeddings, and this is done using
    embeddings models. Apart from this, there are other models used in RAG systems:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings models are used to transform data into vector format. We have discussed
    embeddings models in detail in chapter 3\. Recall that the choice of embeddings
    model depends on the domain, use case, and cost considerations. Providers such
    as OpenAI, Gemini by Google, Voyage AI, and Cohere provide a variety of embeddings
    model choices, and a host of open source embeddings models can also be used via
    Hugging Face transformers. Multimodal embeddings map data of different modalities
    into a shared embeddings space.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Foundation models or the pre-trained LLMs are used for the generation of outputs,
    as well as for evaluation and adaptive tasks where LLMs are used to judge. We
    have discussed LLMs as part of the generation pipeline in chapter 4\. Recall that
    the GPT series by OpenAI, Gemini Series by Google, Claude Series by Anthropic,
    and Command R series by Cohere are popular proprietary LLMs. The llama series
    by Meta and Mistral are open source models that have gained popularity. Most LLMs
    now include multimodal capabilities and are continuously evolving.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Task-specific models are machine learning models that are not core to RAG but
    come in handy for various tasks. These models are used in advanced RAG pipelines.
    Query classification models for efficient routing and intent detection, NER models
    to detect entities for metadata, query-expansion models, hallucination-detection
    models, and bias- and toxicity-moderation models are some examples of task-specific
    models useful in RAG systems. While task-specific models are generally custom
    trained, providers such as OpenAI, Hugging Face, and Google also offer these services.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model training and fine-tuning componen**t*—This component is responsible
    for building custom models and fine-tuning foundation models on custom data. In
    chapter 4, we discussed that fine-tuning of LLMs is sometimes required for domain
    adaptation. Fine-tuning can also be done for embeddings models. Additionally,
    the task-specific models can be trained on custom data. This component supports
    the algorithms used for training and fine-tuning the models. For training data,
    this component interacts with the data layer where the training data can be created
    and managed. A regular MLOps layer is also recommended for the development and
    maintenance of the models. This is enabled via ML platforms such as Hugging Face,
    AWS SageMaker, Azure ML, and similar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference optimization component—This component is responsible for generating
    responses quickly and cost-effectively, which can be done by employing a variety
    of methods such as quantization, batching, KV(Key Value)-caching, and similar.
    ONNX and NVIDIA TensorRT-LLM are popular frameworks that optimize inferencing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 7.3 illustrates different components of the model layer. It shows how
    the model layer helps in deciding which models to use in the RAG system, facilitates
    training and fine-tuning of the model, and optimizes the models for efficient
    serving.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F03_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3  The model layer: The model library is the store for all models
    selected for the application, model training and fine-tuning interact with the
    data layer to source training data and train custom models, while the inference
    optimization component is responsible for efficient serving of the model.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This layer is responsible for making the RAG system available to the application
    layer. It handles the infrastructure of the models. It also ensures that the models
    can be accessed reliably. There are four main methods by which the models can
    be deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Fully managed deploymen**t*—Itcan be provided by proprietary model providers
    such as OpenAI, Google, Anthropic, and Cohere, where all infrastructure for model
    deployment, serving, and scaling is managed and optimized by these providers.
    Services such as AWS SageMaker, Google Vertex AI, Azure Machine Learning, and
    Hugging Face offer platforms to deploy, serve, and monitor both open source and
    custom-developed models. Amazon Bedrock is another fully managed service that
    provides access to a variety of foundation models, both proprietary and open source,
    simplifying model access and deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Self-hosted deploymen**t*—This type of deployment is enabled by cloud VM providers
    such as AWS, GCP, Azure, and hardware providers such as Nvidia. In this scenario,
    models are deployed in private clouds or on-premises, and the infrastructure is
    managed by the application developer. Tools such as Kubernetes and Docker are
    widely used for containerization and orchestration of models, while Nvidia Triton
    Inference Server can optimize inference on Nvidia hardware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Local/edge deploymen**t*—It involves running optimized versions of models
    on local hardware or edge devices, ensuring data privacy, reduced latency, and
    offline functionality. Local/edge deployment typically requires model compression
    techniques such as quantization and pruning, and smaller models tailored for resource-constrained
    environments. Tools such as ONNX, TensorFlow Lite, and PyTorch Mobile enable efficient
    deployment on mobile and embedded platforms, while GGML and NVIDIA TensorRT support
    CPU and GPU optimizations. GPT4All is a popular open source solution for running
    quantized LLMs locally on devices such as laptops, IoT devices, and edge servers
    without relying on cloud infrastructure. These frameworks facilitate low-latency,
    power-efficient execution, making AI accessible in decentralized environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model deployment is a relatively complex task that requires engineering skills
    when self-hosted and local/edge deployment is done. Figure 7.4 illustrates the
    three ways in which models are deployed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F04_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4  The model deployment layer manages the infrastructure for hosting
    and deployment for efficient serving of all the models in the RAG system.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: With the data and the model layers, the most essential components of the RAG
    system are in place. Now we need a layer that manages the co-ordination between
    the data and the models. This is the responsibility of the application orchestration
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Application orchestration layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When we hear the term *orchestration*, a musical conductor leading a group of
    musicians in an orchestra comes to mind. An application orchestration layer is
    somewhat similar. It is responsible for managing the interactions among the other
    layers in the system. It is a central coordinator that enables communication between
    data, retrieval systems, generation models, and other services. The major components
    of the orchestration layer are
  prefs: []
  type: TYPE_NORMAL
- en: '*Query orchestration componen**t*—Responsible for receiving and orchestrating
    user queries. All pre-retrieval query optimization steps such as query classification,
    expansion, and rewriting are orchestrated by this component. The query orchestration
    layer may coordinate with the end application layer to receive the input, and
    the model layer to access the models required for the query optimization. This
    component will generally pass on the processed query to the retrieval coordination
    and the generation coordination components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retrieval coordination componen**t*—Hosts the various retrieval logics. Depending
    on the input from the query orchestration module, it selects the appropriate retrieval
    method (dense retrieval or hybrid retrieval) and interacts with the data layer.
    Depending on the retrieval strategy, it may also interact with the model layer
    if any recursive or adaptive retrieval method is invoked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generation coordination componen**t*—Receives the query and the context from
    the previous components and coordinates all the post-retrieval steps. Its primary
    function is to interact with the model layer and prompt the LLM to generate the
    output. Apart from generation, all the post-retrieval steps such as re-ranking
    and contextual compression are coordinated by this component. Post-generation
    tasks such as reflection, fact-checking, and moderation can be coordinated by
    the generation component. This component can also be made responsible for passing
    the output to the application layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are the three primary components of the orchestration layer. There are
    two additional components to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Multi-agent orchestration componen**t*—Used for agentic RAG where multiple
    agents handle specific tasks. We will take a deeper look at agentic RAG in chapter
    8\. The orchestration layer is responsible for managing agent interactions and
    coordination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Workflow automation componen**t*—Sometimes employed for managing the flow
    and the movement of data between different components. This component is not specific
    to RAG systems but is commonly employed in data products. Apache Airflow and Dagster
    are popular tools used for workflow automation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 7.5 illustrates the orchestration layer components interacting with the
    application layer, which is supported by the model deployment and data layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F05_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5  The app orchestration layer accepts the user query from the application
    layer and sends the response back to the application layer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LangChain and LlamaIndex are the most common orchestration frameworks used to
    develop RAG systems. They provide abstractions for different components. Microsoft’s
    AutoGen and CrewAI are upcoming frameworks for multi-agent orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: With these four layers (i.e., data, model, model deployment, and application
    orchestration), the critical RAG system is complete. This core system can interact
    with the end-software application layer, which acts as the interface between the
    RAG system and the user. While the application layer is generally custom built,
    platforms such as Streamlit, Vercel, and Heroku are popular for hosting the application.
    Figure 7.6 summarizes the critical layers of the RAGOps stack.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are familiar with the core layers of the stack, let’s look next
    at the essential layers that improve the performance and reliability of the system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F06_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6  Core RAGOps stack where data, model, model deployment, and app orchestration
    layers interact with source systems and managed service providers, and co-ordinate
    with the application layer to interface with the user
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 7.1.2 Essential layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the critical layers form the core of the stack, they do not evaluate or
    monitor the system. They do not test the prompting strategies or offer any protection
    against the vulnerabilities of LLMs. These layers are essential to the system.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While the generation coordination component of the orchestration layer can simply
    put together the user query and the retrieved context, poor prompting can lead
    to hallucinations and subpar results. Proper engineering and evaluation of the
    prompts are vital to guiding the model toward generating relevant, grounded, and
    accurate responses. This process often involves experimentation. Developers create
    prompts, observe the results, and then iterate on the prompts to improve the effectiveness
    of the app. This also requires tracking and collaboration. Azure Prompt Flow,
    Lang­Chain Expression Language (LCEL), Weights & Biases prompts, and PromptLayer
    are among the several applications that can be used to create and manage prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Chapter 5 discussed RAG evaluations at length. Regular evaluation of retrieval
    accuracy, context relevance, faithfulness, and answer relevance of the system
    is necessary to ensure the quality of responses. TruLens by TruEra, Ragas, and
    Weights & Biases are commonly used platforms and frameworks for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Continuous monitoring ensures the long-term health of the RAG system. Observing
    the execution of the processing chain is essential for understanding system behavior
    and identifying points of failure. Assessing the relevance and adequacy of information
    provided to the language model is also critical. Apart from this, regular system
    metrics tracking such as resource utilization, latency, and error rates form the
    part of the monitoring layer. ARISE, RAGAS, and ARES are evaluation frameworks
    that are also used in monitoring. TraceLoop, TruLens, and Galileo are examples
    of providers that offer monitoring services.
  prefs: []
  type: TYPE_NORMAL
- en: LLM security and privacy layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While security and privacy are features of any software system, in the context
    of RAG, there are additional aspects to this. RAG systems rely on large knowledge
    bases stored in vector databases, which can contain sensitive information. They
    need to follow all data privacy regulations. AI models are susceptible to manipulation
    and poisoning. Prompt injection is a malicious attack via prompts to retrieve
    sensitive information. Data protection strategies such as anonymization, encryption,
    and differential privacy should be employed. Query validation, sanitization, and
    output filtering assist in protection against attacks. Implementing guardrails,
    access controls, monitoring, and auditing are also components of the security
    and privacy layer.
  prefs: []
  type: TYPE_NORMAL
- en: Caching layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Caching has become a very important component of any LLM-based application.
    This is because of the high costs and inherent latency of generative AI models.
    With the addition of a retrieval layer, the costs and latency increase further
    in RAG systems. One way to control this increase is to cache responses to frequently
    asked queries. In principle, caching LLM responses is like caching in any other
    software application, but for generative AI apps, it becomes more important.
  prefs: []
  type: TYPE_NORMAL
- en: These essential layers stacked together with the critical layers create a robust,
    accurate, and high-performing RAG system. Figure 7.7 adds the essential layers
    and their components to the critical RAGOps stack.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F07_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7  Adding essential layers to the critical RAGOps stack lays the path
    to a robust RAG system for user applications.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Table 7.1 is a recap of the critical and essential layers of the RAGOps stack.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.1 Critical and essential layers of the RAGOps stack
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Layer | Category | Description | Example tools |'
  prefs: []
  type: TYPE_TB
- en: '| Data layer | Critical | Responsible for creating and storing the knowledge
    base via ingestion from various sources, transformation into embeddings or graph
    structures, and storing for retrieval | AWS Glue, Apache Kafka, FAISS, Pinecone,
    Neo4j, Weaviate, Milvus |'
  prefs: []
  type: TYPE_TB
- en: '| Model layer | Critical | Contains the models required for generation and
    retrieval in RAG; includes embeddings models for vector generation, LLMs for text
    generation, and models for query classification, hallucination detection, or re-ranking
    | OpenAI, Hugging Face Transformers, Google Gemini, Llama, Anthropic |'
  prefs: []
  type: TYPE_TB
- en: '| Model deployment | Critical | Ensures the models are accessible, performant,
    and scalable; responsible for serving models and optimizing inference for fast
    response times | SageMaker, Vertex AI, NVIDIA Triton, Hugging Face |'
  prefs: []
  type: TYPE_TB
- en: '| Application orchestration layer | Critical | Manages the interaction between
    layers and services, ensures that queries flow through retrieval and generation
    stages, and coordinates retrieval methods and generation tasks | LangChain, Haystack,
    Dagster, Apache Airflow, AutoGen, CrewAI |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt layer | Essential | Designs and maintains the input queries to ensure
    the LLM generates relevant, high-quality outputs; ensures continuous prompt refinement
    to avoid hallucinations and improve accuracy | Weights & Biases Prompts, Azure
    Prompt Flow |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation layer | Essential | Evaluates the performance of the retrieval
    and generation stages, ensuring that the outputs are relevant, factual, and accurate.
    | TruLens by TruEra, Ragas, Weights & Biases |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring layer | Essential | Continuously monitors the performance, health,
    and resource usage of the RAG system; tracks key metrics such as latency, resource
    consumption, and error rates to ensure system stability. | Prometheus, Grafana,
    TruLens, Galileo |'
  prefs: []
  type: TYPE_TB
- en: '| LLM security & privacy layer | Essential | Ensures that the RAG system adheres
    to data privacy regulations and protects against prompt injection or other forms
    of AI manipulation; implements security strategies such as encryption, access
    control, and guardrails | AWS KMS, Azure Key Vault, Prompt Injection Guards |'
  prefs: []
  type: TYPE_TB
- en: '| Model training/Fine-tuning layer | Essential | Handles the training and fine-tuning
    of models for specific domains or tasks; fine-tuning models such as embeddings
    or LLMs using domain-specific datasets ensure better performance for specialized
    use cases. | Hugging Face, AWS SageMaker, Google Vertex AI, Azure ML |'
  prefs: []
  type: TYPE_TB
- en: '| Caching layer | Essential | Caching frequently used queries and responses
    to reduce the latency and cost associated with repeated retrieval and generation
    tasks; ensures faster response times for common queries and minimizes resource
    usage for repeated tasks. | Redis, Varnish, ElasticCache |'
  prefs: []
  type: TYPE_TB
- en: We will now briefly look at a few enhancement layers, which are not mandatory
    but may be employed to further improve the RAG systems. Note that there can be
    several enhancement layers and that they should be tailored to the use case requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.3 Enhancement layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enhancement layers are the parts of the RAGOps stack that are optional but can
    lead to significant gains, depending on the use case environment. They focus on
    the efficiency, usability, and scalability of the system. Some possible layers
    are described in the following.
  prefs: []
  type: TYPE_NORMAL
- en: Human-in-the-loop layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This layer provides critical oversight where human judgment is necessary, especially
    for use cases requiring higher accuracy or ethical considerations. It helps reduce
    model hallucinations and bias.
  prefs: []
  type: TYPE_NORMAL
- en: Cost optimization layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RAG systems can become very costly, especially with multiple calls to the LLMs
    for advanced techniques, evaluations, guardrails, and monitoring. This layer helps
    manage resources efficiently, which is particularly important for large-scale
    systems. Optimizing infrastructure can save significant costs but is not critical
    to the system functioning.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability and interpretability layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This layer helps provide transparency for system decisions, especially important
    for domains requiring accountability (e.g., legal and healthcare). However, many
    applications can still function without this in nonregulated environments.
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration and experimentation layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This layer is useful for teams working on development and experimentation but
    noncritical for system operation. This layer enhances productivity and iterative
    improvements. Weights & Biases is a popular platform that helps track experiments.
  prefs: []
  type: TYPE_NORMAL
- en: These enhancement layers should be chosen depending on the application requirements.
    There may be other layers that you may deem fit for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Managed RAG solutions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Building a RAG system can be complex if you don’t have prior knowledge, budget,
    or time. To address these challenges, service providers offer managed RAG solutions.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI offers the File Search tool that automatically parses and chunks your
    documents, creates and stores the embeddings, and uses both vector and keyword
    search to retrieve relevant content to answer user queries. AWS offers Amazon
    Bedrock Knowledge Bases, which is fully managed support for end-to-end RAG workflow.
    Azure AI, such as OpenAI file search, provides indexing and querying. Anthropic
    offers Claude projects where users can upload documents and provide context to
    have focused chats.
  prefs: []
  type: TYPE_NORMAL
- en: Several other providers offer RAG as a service and can handle video and audio
    transcription, image content extraction, and document parsing. For quick and easy
    deployment of a RAG solution, managed service providers can be considered.
  prefs: []
  type: TYPE_NORMAL
- en: We have also discussed several service providers, tools, and technologies that
    you can use in the development of RAG systems. The choice of these tools and technologies
    may depend on factors such as
  prefs: []
  type: TYPE_NORMAL
- en: '*Scalability and performance require**d*—RAG systems need to handle large volumes
    of data efficiently, while maintaining low latency. As data scales or traffic
    spikes, the system must remain performant to ensure fast response times. Choose
    cloud platforms that allow for auto-scaling and variable loads. For high-performance
    and scalable retrieval, choose the vector databases that can handle millions of
    embeddings with low-latency search capabilities. Use inference optimization tools
    to help reduce latency during the generation phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Integration with existing stac**k*—Seamless integration with your current
    technology stack minimizes disruption and reduces complexity. If your system already
    operates on AWS, GCP, or Azure, using services that integrate well with these
    platforms can streamline development and maintenance. Choosing tools that natively
    integrate with your cloud provider, offer strong API support, and ensure that
    the chosen frameworks support these tools can be highly beneficial.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost efficienc**y*—LLMs require much more resources than traditional ML models.
    Costs, even with pay-as-you-go models, can escalate quickly with scale. Caching
    and inference optimization can help manage the costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Domain adaptatio**n*—RAG systems often need to be adapted to specific industries
    or domains (e.g., healthcare and legal). Pre-trained models might not be fully
    effective for specific use cases unless fine-tuned with domain-specific data.
    For domain adaptation, models that can be easily fine-tuned should be chosen.
    Existing domain-specific models can also be considered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vendor lock-in constraint**s*—Since generative AI is an evolving field, using
    proprietary tools or services from a single vendor may lead to vendor lock-in,
    making it difficult to migrate to other platforms or adjust your stack as requirements
    change. Using open source or interoperable technologies where possible helps in
    maintaining flexibility. Choosing tools that are cloud-agnostic or support multi-cloud
    deployments to reduce dependency on a single vendor. A modular architecture is
    advised to swap components without a system redesign.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Community suppor**t*—Strong community support means access to resources, tutorials,
    troubleshooting, and regular updates, which can accelerate development and reduce
    debugging time. This is especially true for rapidly evolving fields such as LLMs
    and RAG. Tools with active communities such as Hugging Face, LangChain, and similar
    are more likely to offer frequent updates, plugins, and third-party integrations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the knowledge of the critical, essential, and enhancement layers, you should
    be ready to put together a technology stack to build your RAG system. Let’s now
    look at some common pitfalls and best practices to consider when building and
    deploying production-grade RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Production best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite earnest efforts in designing and planning the RAG system, some problems
    will inevitably creep up during development and deployment. Although RAG is still
    in its nascent form, some early trends of common mishaps and best practices have
    emerged. There have been many experiments and learnings derived from them to make
    RAG systems work. This section discusses five such practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Latency of the system*—RAG systems can introduce latency due to the need for
    multiple steps: retrieval, reranking, and generation. High latency can significantly
    degrade user experience, especially in real-time applications like chatbots or
    interactive search engines, which happens because each component adds processing
    time. Effective classification and routing of the queries can help in optimizing
    latency. A filtering approach is useful in hybrid retrieval, which first filters
    the embeddings based on keywords or sparse retrieval techniques and then uses
    similarity search on the filtered results. This reduces the time taken to calculate
    similarity, especially in large knowledge bases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Continued hallucinatio**n*—Despite best efforts, LLMs may continue to generate
    responses that are factually incorrect or irrelevant to the retrieved content.
    This may happen if the retrieved data is ambiguous or incomplete. Post-processing
    validation steps may be required to address these. A common approach is to make
    RAG systems recommendation oriented rather than action oriented. This means that
    a human is looped into the system for verification and final action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Insufficient scalability planning*—Early prototypes of RAG systems often work
    well on small datasets but can struggle as the volume of data or the number of
    concurrent users grows. Managed vector database services with autoscaling features
    can be an easier way to plan for growth in demand and computation requirements.
    Similarly, autoscaling can also be used for the overall application using cloud-native
    solutions such as AWS Lambda.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Domain-adaptation challenge**s*—The embeddings and language models may not
    work well in niche or specialized domains. Also, the retrieval model and the language
    model may not always complement each other well, leading to disjointed or incoherent
    results. Retrieval models and LLMs are often developed and fine-tuned independently,
    which can cause a mismatch between the content retrieved and the way the LLM generates
    responses. It becomes important to fine-tune both the retrieval and generation
    models together for highly specialized domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inadequate handling of data privacy and PII*—Pre-trained models may generate
    content that includes sensitive information (e.g., personal data and confidential
    details) due to biases in training data. RAG systems may inadvertently leak sensitive
    information or personally identifiable information (PII) in their responses, leading
    to privacy breaches. Data exfiltration, also known as data theft, extrusion, or
    exportation, is a major threat in the digital world. The solution is to use PII
    masking and data redaction during both the pre- and post-processing stages. Ensure
    compliance with privacy regulations such as GDPR or HIPAA and deploy models with
    privacy filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list of best practices continues to evolve. Latency and scalability are
    critical for managing user experience and access. The promise of hallucination-free
    generation and data safety needs to be maintained for the reliability of the system.
    Table 7.2 summarizes the challenges of and potential solutions to putting RAG
    systems into production.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.2 Production challenges and potential solutions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Challenge | Description | Solution |'
  prefs: []
  type: TYPE_TB
- en: '| Latency of the system | RAG systems add latency due to retrieval, re-ranking,
    and generation steps, affecting real-time performance. | Use query classification,
    hybrid retrieval filtering, and limit similarity searches |'
  prefs: []
  type: TYPE_TB
- en: '| Continued hallucination | LLMs may generate incorrect or irrelevant responses
    due to ambiguous or incomplete data. | Add post-processing validation and make
    systems recommendation-based with human verification. |'
  prefs: []
  type: TYPE_TB
- en: '| Insufficient scalability planning | Early RAG systems struggle with scalability
    as data and user load grow. | Use autoscaling vector databases and cloud solutions
    such as AWS Lambda. |'
  prefs: []
  type: TYPE_TB
- en: '| Domain-adaptation challenges | Embeddings and LLMs may perform poorly in
    specialized domains, leading to incoherent results. | Fine-tune both retrieval
    and generation models for niche use cases. |'
  prefs: []
  type: TYPE_TB
- en: '| Inadequate handling of data privacy and PII | Models may expose sensitive
    data or PII, leading to privacy issues. | Apply PII masking, data redaction, and
    privacy filters, ensuring compliance with regulations. |'
  prefs: []
  type: TYPE_TB
- en: In this chapter, we have looked at a holistic RAGOps stack that enables the
    building of production-grade RAG systems. You also learned about some commonly
    available tools and technologies, along with a few best practices. This brings
    us to a close in our discussion of the RAGOps stack. We have now completed part
    3 of the book, which means you should be ready to build RAG systems and put them
    into production. In the last part of this book, we discuss some emerging patterns
    in RAG-like multimodal capabilities, agentic RAG, and graphRAG, along with closing
    comments on future directions and continued learning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAGOps stack is a layered approach to designing a RAG system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These layers are categorized into critical, essential, and enhancement layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Critical layers are fundamental for operation; essential layers ensure performance
    and reliability; and enhancement layers improve efficiency, scalability, and usability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Critical layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Data laye**r*—Responsible for collecting, transforming, and storing the knowledge
    base. Ingestion tools such as AWS Glue, Azure Data Factory, and Apache Kafka enable
    data collection. Data transformation includes chunking, metadata enrichment, and
    converting data into vector formats. Tools such as FAISS, Pinecone, and Neo4j
    are used for storing embeddings and graph data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model laye**r*—Includes embeddings models and LLMs for generation. Embeddings
    models transform the text into vectors, with options from OpenAI, Google, Cohere,
    and Hugging Face. Foundation models (LLMs) such as GPT, Claude, and Llama generate
    outputs and evaluate tasks. Task-specific models handle specialized tasks such
    as query classification and bias detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model deploymen**t*—Manages hosting and serving of LLMs and embeddings models.
    Popular platforms include AWS SageMaker, Google Vertex, and Hugging Face. Inference
    optimization reduces response time and costs with methods such as quantization
    and batching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Application orchestration laye**r*—Coordinates data flow between different
    components:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query orchestration handles query classification and optimization.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval coordination manages retrieval methods like dense or hybrid search.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generation coordination handles prompt generation and post-retrieval tasks such
    as re-ranking.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Essential layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Prompt layer*—Ensures prompts are well-engineered to guide LLMs for relevant,
    accurate responses. Tools such as LangChain and Azure Prompt Flow assist in prompt
    management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Evaluation laye**r*—Monitors system performance by evaluating retrieval accuracy,
    faithfulness, and context relevance. Tools such as TruLens and Ragas provide evaluation
    frameworks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monitoring layer*—Tracks system health, resource usage, and latency. Platforms
    such as TraceLoop and Galileo provide monitoring services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM security and privacy laye**r*—Protects against data breaches and prompt
    injection attacks. Tools such as encryption, anonymization, and differential privacy
    should be used to safeguard sensitive data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Caching layer*—Caches frequently generated responses to reduce costs and latency
    in RAG systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancement layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Human-in-the-loop laye**r*—Adds human oversight to ensure higher accuracy
    and ethical decision-making.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost optimization laye**r*—Reduces infrastructure costs, especially in large-scale
    RAG systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Explainability and interpretability laye**r*—Provides transparency into system
    decisions, critical for domains such as healthcare and legal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Collaboration and experimentation laye**r*—Useful for team-based development
    and continuous improvement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Production best practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Latenc**y*—RAG systems often introduce latency due to multiple steps. Using
    techniques such as filtering in hybrid retrieval can help reduce response times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hallucinatio**n*—LLMs may still generate incorrect responses. Post-processing
    validation and human-in-the-loop systems help mitigate this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalabilit**y*—Early prototypes may struggle to scale. Managed vector database
    services with autoscaling can help plan for growth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Domain adaptatio**n*—Embeddings and language models may not perform well in
    niche domains. Fine-tuning both retrieval and generation models is necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data privac**y*—Models may leak sensitive information. PII masking, encryption,
    and compliance with data regulations are essential for protecting user data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
