["```py\nimport torch\nimport torch.nn as nn\n\nlayer = nn.Linear(40, 10)\nlayer.weight.data *= 6 ** 0.5  # Kaiming init (or 3 ** 0.5 for LeCun init)\ntorch.zero_(layer.bias.data)\n```", "```py\nnn.init.kaiming_uniform_(layer.weight)\nnn.init.zeros_(layer.bias)\n```", "```py\ndef use_he_init(module):\n    if isinstance(module, nn.Linear):\n        nn.init.kaiming_uniform_(module.weight)\n        nn.init.zeros_(module.bias)\n\nmodel = nn.Sequential(nn.Linear(50, 40), nn.ReLU(), nn.Linear(40, 1), nn.ReLU())\nmodel.apply(use_he_init)\n```", "```py\nalpha = 0.2\nmodel = nn.Sequential(nn.Linear(50, 40), nn.LeakyReLU(negative_slope=alpha))\nnn.init.kaiming_uniform_(model[0].weight, alpha, nonlinearity=\"leaky_relu\")\n```", "```py\nmodel = nn.Sequential(\n    nn.Flatten(),\n    nn.BatchNorm1d(1 * 28 * 28),\n    nn.Linear(1 * 28 * 28, 300),\n    nn.ReLU(),\n    nn.BatchNorm1d(300),\n    nn.Linear(300, 100),\n    nn.ReLU(),\n    nn.BatchNorm1d(100),\n    nn.Linear(100, 10)\n)\n```", "```py\n>>> dict(model[1].named_parameters()).keys() `dict_keys(['weight', 'bias'])`\n```", "```py`` And if you look at the buffers of this same BN layer, you will find three: `run⁠ning_​mean`, `running_var`, and `num_batches_tracked`. The first two correspond to the running means **μ** and **σ**² discussed earlier, and `num_batches_tracked` simply counts the number of batches seen during training:    ```", "```py   ```", "```py model = nn.Sequential(     nn.Flatten(),     nn.Linear(1 * 28 * 28, 300, bias=False),     nn.BatchNorm1d(300),     nn.ReLU(),     nn.Linear(300, 100, bias=False),     nn.BatchNorm1d(100),     nn.ReLU(),     nn.Linear(100, 10) ) ```", "```py` ```", "```py ``### Batch norm 1D, 2D, and 3D    In the previous examples, we flattened the input images before sending them through the first `nn.BatchNorm1d` layer. This is because an `nn.BatchNorm1d` layer works on batches of shape `[batch_size, num_features]` (just like the `nn.Linear` layer does), so you would get an error if you moved it before the `nn.Flatten` layer.    However, you could use an `nn.BatchNorm2d` layer before the `nn.Flatten` layer: indeed, it expects its inputs to be image batches of shape `[batch_size, channels, height, width]`, and it computes the batch mean and variance across both the batch dimension (dimension 0) and the spatial dimensions (dimensions 2 and 3). This means that all pixels in the same batch and channel get normalized using the same mean and variance: the `nn.BatchNorm2d` layer only has one weight per channel and one bias per channel (e.g., three weights and three bias terms for color images with three channels for red, green, and blue). This generally works better when dealing with image datasets.    There’s also an `nn.BatchNorm3d` layer which expects batches of shape `[batch_size, channels, depth, height, width]`: this is useful for datasets of 3D images, such as CT scans.    The `nn.BatchNorm1d` layer can also work on batches of sequences. The convention in PyTorch is to represent batches of sequences as 3D tensors of shape `[batch_size, sequence_length, num_features]`. For example, suppose you work on particle physics and you have a dataset of particle trajectories, where each trajectory is composed of a sequence of 100 points in 3D space, then a batch of 32 trajectories will have a shape of `[32, 100, 3]`. However, the `nn.BatchNorm1d` layer expects the shape to be `[batch_size, num_features, sequence_length]`, and it computes the batch mean and variance across the first and last dimensions to get one mean and variance per feature. So you must permute the last two dimensions of the data using `X.permute(0, 2, 1)` before letting it go through the `nn.BatchNorm1d` layer. We will discuss sequences further in [Chapter 13](ch13.html#rnn_chapter).    Batch normalization has become one of the most-used layers in deep neural networks, especially deep convolutional neural networks discussed in [Chapter 12](ch12.html#cnn_chapter), to the point that it is often omitted in the architecture diagrams: it is assumed that BN is added after every layer. That said, it is not perfect. In particular, the computed statistics for an instance are biased by the other samples in a batch, which may reduce performance (especially for small batch sizes). Moreover, BN struggles with some architectures, such as recurrent nets, as we will see in [Chapter 13](ch13.html#rnn_chapter). For these reasons, batch-norm is more and more often replaced by layer-norm.`` ```", "```py`` ```", "```py inputs = torch.randn(32, 3, 100, 200)  # a batch of random RGB images layer_norm = nn.LayerNorm([100, 200]) result = layer_norm(inputs)  # normalizes over the last two dimensions ```", "```py means = inputs.mean(dim=[2, 3], keepdim=True)  # shape: [32, 3, 1, 1] vars_ = inputs.var(dim=[2, 3], keepdim=True, unbiased=False)  # shape: same stds = torch.sqrt(vars_ + layer_norm.eps)  # eps is a smoothing term (1e-5) result = layer_norm.weight * (inputs - means) / stds + layer_norm.bias # result shape: [32, 3, 100, 200] ```", "```py layer_norm = nn.LayerNorm([3, 100, 200]) result = layer_norm(inputs)  # normalizes over the last three dimensions ```", "```py for epoch in range(n_epochs):     for X_batch, y_batch in train_loader:         X_batch, y_batch = X_batch.to(device), y_batch.to(device)         y_pred = model(X_batch)         loss = loss_fn(y_pred, y_batch)         loss.backward()         nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)         optimizer.step()         optimizer.zero_grad() ```", "```py` ```", "```py```", "```py```", "```py torch.manual_seed(42)  model_A = nn.Sequential(     nn.Flatten(),     nn.Linear(1 * 28 * 28, 100),     nn.ReLU(),     nn.Linear(100, 100),     nn.ReLU(),     nn.Linear(100, 100),     nn.ReLU(),     nn.Linear(100, 8) ) [...]  # train this model or load pretrained weights ```", "```py import copy  torch.manual_seed(42) reused_layers = copy.deepcopy(model_A[:-1]) model_B_on_A = nn.Sequential(     *reused_layers,     nn.Linear(100, 1)  # new output layer for task B ).to(device) ```", "```py for layer in model_B_on_A[:-1]:     for param in layer.parameters():         param.requires_grad = False ```", "```py xentropy = nn.BCEWithLogitsLoss() accuracy = torchmetrics.Accuracy(task=\"binary\").to(device) [...]  # train model_B_on_A ```", "```py optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.05) ```", "```py optimizer = torch.optim.SGD(model.parameters(),                             momentum=0.9, nesterov=True, lr=0.05) ```", "```py optimizer = torch.optim.RMSprop(model.parameters(), alpha=0.9, lr=0.05) ```", "```py optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), lr=0.05) ```", "```py model = [...]  # build the model optimizer = torch.optim.SGD(model.parameters(), lr=0.05)  # or any other optim. scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) ```", "```py for epoch in range(n_epochs):     for X_batch, y_batch in train_loader:         [...]  # the rest of the training loop remains unchanged      scheduler.step() ```", "```py cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(     optimizer, T_max=20, eta_min=0.001) ```", "```py [...]  # build the model and optimizer scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(   optimizer, mode=\"max\", patience=2, factor=0.1) ```", "```py metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device) for epoch in range(n_epochs):     for X_batch, y_batch in train_loader:         [...]  # the rest of the training loop remains unchanged     val_metric = evaluate_tm(model, valid_loader, metric).item()     scheduler.step(val_metric) ```", "```py warmup_scheduler = torch.optim.lr_scheduler.LinearLR(     optimizer, start_factor=0.1, end_factor=1.0, total_iters=3) ```", "```py warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(     optimizer,     lambda epoch: (min(epoch, 3) / 3) * (1.0 - 0.1) + 0.1) ```", "```py for epoch in range(n_epochs):     warmup_scheduler.step()     for X_batch, y_batch in train_loader:         [...]  # the rest of the training loop is unchanged     if epoch >= 3:  # deactivate other scheduler(s) during warmup         scheduler.step(val_metric) ```", "```py cosine_repeat_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(     optimizer, T_0=2, T_mult=2, eta_min=0.001) ```", "```py optimizer = torch.optim.SGD(model.parameters(), lr=0.05, weight_decay=1e-4) [...]  # use the optimizer normally during training ```", "```py optimizer = torch.optim.SGD(model.parameters(), lr=0.05) params_to_regularize = [     param for name, param in model.named_parameters()     if not \"bias\" in name and not \"bn\" in name] for epoch in range(n_epochs):     for X_batch, y_batch in train_loader:         [...]  # the rest of the training loop is unchanged         main_loss = loss_fn(y_pred, y_batch)         l2_loss = sum(param.pow(2.0).sum() for param in params_to_regularize)         loss = main_loss + 1e-4 * l2_loss         [...] ```", "```py params_bias_and_bn = [     param for name, param in model.named_parameters()     if \"bias\" in name or \"bn\" in name] optimizer = torch.optim.SGD([     {\"params\": params_to_regularize, \"weight_decay\": 1e-4},     {\"params\": params_bias_and_bn},     ], lr=0.05) [...]  # use the optimizer normally during training ```", "```py l1_loss = sum(param.abs().sum() for param in params_to_regularize) loss = main_loss + 1e-4 * l1_loss ```", "```py model = nn.Sequential(     nn.Flatten(),     nn.Dropout(p=0.2), nn.Linear(1 * 28 * 28, 100), nn.ReLU(),     nn.Dropout(p=0.2), nn.Linear(100, 100), nn.ReLU(),     nn.Dropout(p=0.2), nn.Linear(100, 100), nn.ReLU(),     nn.Dropout(p=0.2), nn.Linear(100, 10) ).to(device) ```", "```py model.eval() for module in model.modules():     if isinstance(module, nn.Dropout):         module.train()  X_new = [...]  # some new images, e.g., the first 3 images of the test set X_new = X_new.to(device)  torch.manual_seed(42) with torch.no_grad():     X_new_repeated = X_new.repeat_interleave(100, dim=0)     y_logits_all = model(X_new_repeated).reshape(3, 100, 10)     y_probas_all = torch.nn.functional.softmax(y_logits_all, dim=-1)     y_probas = y_probas_all.mean(dim=1) ```", "```py >>> y_probas.round(decimals=2) `tensor([[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.010, 0.000, 0.990],`  `[0.990, 0.000, 0.000, 0.000, 0.000, 0.000, 0.010, 0.000, 0.000, 0.000],`  `[0.410, 0.040, 0.040, 0.230, 0.040, 0.000, 0.230, 0.000, 0.010, 0.000]],`  `device='cuda:0')` ```", "```py```", "``` >>> y_std = y_probas_all.std(dim=1) `>>>` `y_std``.``round``(``decimals``=``2``)` `` `tensor([[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.020, 0.000, 0.020],`  `[0.020, 0.000, 0.000, 0.000, 0.000, 0.000, 0.020, 0.000, 0.000, 0.000],`  `[0.170, 0.030, 0.030, 0.130, 0.050, 0.000, 0.090, 0.000, 0.010, 0.000]],`  `device='cuda:0')` `` ```", "````` ```py`There’s a standard deviation of 0.02 for the probability estimate of class 9 (ankle boot) for the first image. This adds a grain of salt to the estimated probability of 99% for this class: in fact, the model is really saying “mmh, I’m guessing over 95%”. If you were building a risk-sensitive system (e.g., a medical or financial system), you may want to consider only the predictions with both a high estimated probability *and* a low standard deviation.    ###### Note    The number of Monte Carlo samples you use (100 in this example) is a hyperparameter you can tweak. The higher it is, the more accurate the predictions and their uncertainty estimates are, but also the slower the predictions are. Moreover, above a certain number of samples, you will notice little improvement. Your job is to find the right trade-off among latency, throughput, and accuracy, depending on your application.    If you want to train an MC dropout model from scratch rather than reuse an existing dropout model, you should probably use a custom `McDropout` module rather than using `nn.Dropout` and hacking around with `train()` and `eval()`, as this is a bit brittle (e.g., it won’t play nicely with the evaluation function). Here is a three-line implementation:    ``` class McDropout(nn.Dropout):     def forward(self, input):         return F.dropout(input, self.p, training=True) ```py    In short, MC dropout is a great technique that boosts dropout models and provides better uncertainty estimates. And of course, since it is just regular dropout during training, it also acts like a regularizer.```` ```py`` `````", "``````py ````` ```py`## Max-Norm Regularization    Another fairly popular regularization technique for neural networks is called *max-norm regularization*: for each neuron, it constrains the weights **w** of the incoming connections such that ∥ **w** ∥[2] ≤ *r*, where *r* is the max-norm hyperparameter and ∥ · ∥[2] is the ℓ[2] norm.    Reducing *r* increases the amount of regularization and helps reduce overfitting. Max-norm regularization can also help alleviate the unstable gradients problems (if you are not using batch-norm or layer-norm).    Rather than adding a regularization loss term to the overall loss function, max-norm regularization is typically implemented by computing ∥ **w** ∥[2] after each training step and rescaling **w** if needed (**w** ← **w** *r* / ∥ **w** ∥[2]). Here’s a common way to implement this in PyTorch:    ``` def apply_max_norm(model, max_norm=2, epsilon=1e-8, dim=1):     with torch.no_grad():         for name, param in model.named_parameters():             if 'bias' not in name:                 actual_norm = param.norm(p=2, dim=dim, keepdim=True)                 target_norm = torch.clamp(actual_norm, 0, max_norm)                 param *= target_norm / (epsilon + actual_norm) ```py    This function iterates through all of the model’s weight matrices (i.e., all parameters except for the bias terms), and for each one of them it uses the `norm()` method to compute the ℓ[2] norm of each row (`dim=1`). A `nn.Linear` layer has weights of shape [*number of neurons*, *number of inputs*], so using `dim=1` means that we will get one norm per neuron, as desired. Then the function uses `torch.clamp()` to compute the target norm for each neuron’s weights: this creates a copy of the `actual_norm` tensor, except that all values greater than `max_norm` are replaced by `max_norm` (this corresponds to *r* in the previous equation). Lastly, we rescale the weight matrix so that each column ends up with the target norm. Note that the smoothing term `epsilon` is used to avoid division by zero in case some columns have a norm equal to zero.    Next, all you need to do is call `apply_max_norm(model)` in the training loop, right after calling the optimizer’s `step()` method. And of course you probably want to fine-tune the `max_norm` hyperparameter.    ###### Tip    When using max-norm with layers other than `nn.Linear`, you may need to tweak the `dim` argument. For example, when using convolutional layers (see [Chapter 12](ch12.html#cnn_chapter)), you generally want to set `dim=[1, 2, 3]` to limit the norm of each convolutional kernel.```` ```py`` ``````", "```` ```py ``# Practical Guidelines    In this chapter we have covered a wide range of techniques, and you may be wondering which ones you should use. This depends on the task, and there is no clear consensus yet, but I have found the configuration in [Table 11-3](#default_deep_neural_network_config) to work fine in most cases, without requiring much hyperparameter tuning. That said, please do not consider these defaults as hard rules!      Table 11-3\\. Default DNN configuration   | Hyperparameter | Default value | | --- | --- | | Kernel initializer | He initialization | | Activation function | ReLU if shallow; Swish if deep | | Normalization | None if shallow; batch-norm or layer-norm if deep | | Regularization | Early stopping; weight decay if needed | | Optimizer | Nesterov accelerated gradients or AdamW | | Learning rate schedule | Performance scheduling or 1cycle |    You should also try to reuse parts of a pretrained neural network if you can find one that solves a similar problem, or use unsupervised pretraining if you have a lot of unlabeled data, or use pretraining on an auxiliary task if you have a lot of labeled data for a similar task.    While the previous guidelines should cover most cases, there are some exceptions:    *   If you need a sparse model, use ℓ[1] regularization. You can also try zeroing out the smallest weights after training (for example, using the `torch.​nn.utils.prune.l1_unstructured()` function). This will break self-normalization, so you should use the default configuration in this case.           *   If you need a low-latency model (one that performs lightning-fast predictions), you may need to use fewer layers; use a fast activation function such as `nn.ReLU`, `nn.LeakyReLU`, or `nn.Hardswish`; and fold the batch-norm and layer-norm layers into the previous layers after training. Having a sparse model will also help. Finally, you may want to reduce the float precision from 32 bits to 16 or even 8 bits. [Appendix B](app02.html#precision_appendix) covers several techniques to make models smaller and faster, including reduced precision models, mixed precision models, and quantization.           *   If you are building a risk-sensitive application, or inference latency is not very important in your application, you can use MC dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates.              Over the last three chapters, we have learned what artificial neural nets are, how to build and train them using Scikit-Learn and PyTorch, and a variety of techniques that make it possible to train deep and complex nets. In the next chapter, all of this will come together as we dive into one of the most important applications of deep learning: computer vision.    # Exercises    1.  What is the problem that Glorot initialization and He initialization aim to fix?           2.  Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?           3.  Is it OK to initialize the bias terms to 0?           4.  In which cases would you want to use each of the activation functions we discussed in this chapter?           5.  What may happen if you set the `momentum` hyperparameter too close to 1 (e.g., 0.99999) when using an `SGD` optimizer?           6.  Name three ways you can produce a sparse model.           7.  Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC dropout?           8.  Practice training a deep neural network on the CIFAR10 image dataset:               1.  Load CIFAR10 just like you loaded the FashionMNIST dataset in [Chapter 10](ch10.html#pytorch_chapter), but using `torchvision.datasets.CIFAR10` instead of `FashionMNIST`. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes.                       2.  Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function (using `nn.SiLU`). Since this is a classification task, you will need an output layer with one neuron per class.                       3.  Using NAdam optimization and early stopping, train the network on the CIFAR10 dataset. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.                       4.  Now try adding batch-norm and compare the learning curves: is it converging faster than before? Does it produce a better model? How does it affect training speed?                       5.  Try replacing batch-norm with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).                       6.  Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC dropout.                       7.  Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.                      Solutions to these exercises are available at the end of this chapter’s notebook, at [*https://homl.info/colab-p*](https://homl.info/colab-p).    ^([1](ch11.html#id2449-marker)) Xavier Glorot and Yoshua Bengio, “Understanding the Difficulty of Training Deep Feedforward Neural Networks”, *Proceedings of the 13th International Conference on Artificial Intelligence and Statistics* (2010): 249–256.    ^([2](ch11.html#id2451-marker)) Here’s an analogy: if you set a microphone amplifier’s volume knob too close to zero, people won’t hear your voice, but if you set it too close to the max, your voice will be saturated and people won’t understand what you are saying. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come out loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude as it came in.    ^([3](ch11.html#id2456-marker)) Kaiming He et al., “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification”, *Proceedings of the 2015 IEEE International Conference on Computer Vision* (2015): 1026–1034.    ^([4](ch11.html#id2457-marker)) A PyTorch issue (#18182) has been open since 2019 to update the weight initialization to use the current best practices.    ^([5](ch11.html#id2461-marker)) Andrew Saxe et al., “Exact solutions to the nonlinear dynamics of learning in deep linear neural networks”, ICLR (2014).    ^([6](ch11.html#id2473-marker)) A dead neuron may come back to life if its inputs evolve over time and eventually return within a range where the ReLU activation function gets a positive input again. For example, this may happen if gradient descent tweaks the neurons in the layers below the dead neuron.    ^([7](ch11.html#id2477-marker)) Bing Xu et al., “Empirical Evaluation of Rectified Activations in Convolutional Network”, arXiv preprint arXiv:1505.00853 (2015).    ^([8](ch11.html#id2492-marker)) Djork-Arné Clevert et al., “Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)”, *Proceedings of the International Conference on Learning Representations*, arXiv preprint (2015).    ^([9](ch11.html#id2500-marker)) Günter Klambauer et al., “Self-Normalizing Neural Networks”, *Proceedings of the 31st International Conference on Neural Information Processing Systems* (2017): 972–981.    ^([10](ch11.html#id2505-marker)) Dan Hendrycks and Kevin Gimpel, “Gaussian Error Linear Units (GELUs)”, arXiv preprint arXiv:1606.08415 (2016).    ^([11](ch11.html#id2506-marker)) A function is convex if the line segment between any two points on the curve never lies below the curve. A monotonic function only increases, or only decreases.    ^([12](ch11.html#id2509-marker)) Prajit Ramachandran et al., “Searching for Activation Functions”, arXiv preprint arXiv:1710.05941 (2017).    ^([13](ch11.html#id2519-marker)) Noam Shazeer, “GLU Variants Improve Transformer”, arXiv preprint arXiv:2002.05202 (2020).    ^([14](ch11.html#id2520-marker)) Yann Dauphin et al., “Language Modeling with Gated Convolutional Networks”, arXiv preprint arXiv:1612.08083 (2016).    ^([15](ch11.html#id2524-marker)) Diganta Misra, “Mish: A Self Regularized Non-Monotonic Activation Function”, arXiv preprint arXiv:1908.08681 (2019).    ^([16](ch11.html#id2532-marker)) So et al., “Primer: Searching for Efficient Transformers for Language Modeling”, arXiv preprint arXiv:2109.08668 (2021).    ^([17](ch11.html#id2548-marker)) Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”, *Proceedings of the 32nd International Conference on Machine Learning* (2015): 448–456.    ^([18](ch11.html#id2562-marker)) Jimmy Lei Ba et al., “Layer Normalization”, arXiv preprint arXiv:1607.06450 (2016).    ^([19](ch11.html#id2572-marker)) Razvan Pascanu et al., “On the Difficulty of Training Recurrent Neural Networks”, *Proceedings of the 30th International Conference on Machine Learning* (2013): 1310–1318.    ^([20](ch11.html#id2599-marker)) Boris T. Polyak, “Some Methods of Speeding Up the Convergence of Iteration Methods”, *USSR Computational Mathematics and Mathematical Physics* 4, no. 5 (1964): 1–17.    ^([21](ch11.html#id2609-marker)) Yurii Nesterov, “A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence *O*(1/*k*²)”, *Doklady AN USSR* 269 (1983): 543–547.    ^([22](ch11.html#id2615-marker)) John Duchi et al., “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization”, *Journal of Machine Learning Research* 12 (2011): 2121–2159.    ^([23](ch11.html#id2619-marker)) This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012 and presented by Geoffrey Hinton in his Coursera class on neural networks (slides: [*https://homl.info/57*](https://homl.info/57), video: [*https://homl.info/58*](https://homl.info/58)). Amusingly, since the authors did not write a paper to describe the algorithm, researchers often cite “slide 29 in lecture 6e” in their papers.    ^([24](ch11.html#id2626-marker)) Diederik P. Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization”, arXiv preprint arXiv:1412.6980 (2014).    ^([25](ch11.html#id2644-marker)) Timothy Dozat, [“Incorporating Nesterov Momentum into Adam”](https://homl.info/nadam), (2016).    ^([26](ch11.html#id2645-marker)) Ilya Loshchilov, and Frank Hutter, “Decoupled Weight Decay Regularization”, arXiv preprint arXiv:1711.05101 (2017).    ^([27](ch11.html#id2650-marker)) Ashia C. Wilson et al., “The Marginal Value of Adaptive Gradient Methods in Machine Learning”, *Advances in Neural Information Processing Systems* 30 (2017): 4148–4158.    ^([28](ch11.html#id2656-marker)) The *Jacobian matrix* contains all the first-order partial derivatives of a function with multiple parameters and multiple outputs: one column per parameter, and one row per output. When training a neural net with gradient descent, there’s a single output—the loss—so the matrix contains a single row, and there’s one column per model parameter, so it’s a 1×*n* matrix. The *Hessian matrix* contains all the second-order derivatives of a single-output function with multiple parameters: for each model parameter it contains one row and one column, so it’s an *n*×*n* matrix. The informal names *Jacobians* and *Hessians* refer to the elements of these matrices.    ^([29](ch11.html#id2658-marker)) V. Gupta et al., [“Shampoo: Preconditioned Stochastic Tensor Optimization”](https://homl.info/shampoo), arXiv preprint arXiv:1802.09568 (2018).    ^([30](ch11.html#id2674-marker)) Ilya Loshchilov and Frank Hutter, “SGDR: Stochastic Gradient Descent With Warm Restarts”, arXiv preprint arXiv:1608.03983 (2016).    ^([31](ch11.html#id2677-marker)) Leslie N. Smith, “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch Size, Momentum, and Weight Decay”, arXiv preprint arXiv:1803.09820 (2018).    ^([32](ch11.html#id2696-marker)) Geoffrey E. Hinton et al., “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors”, arXiv preprint arXiv:1207.0580 (2012).    ^([33](ch11.html#id2697-marker)) Nitish Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from Overfitting”, *Journal of Machine Learning Research* 15 (2014): 1929–1958.    ^([34](ch11.html#id2709-marker)) Yarin Gal and Zoubin Ghahramani, “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning”, *Proceedings of the 33rd International Conference on Machine Learning* (2016): 1050–1059.    ^([35](ch11.html#id2710-marker)) Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian inference in a specific type of probabilistic model called a *deep Gaussian process*.`` ``` ```py` ````", "```````"]