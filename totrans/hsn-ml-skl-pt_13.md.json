["```py\nimport torch\nimport torch.nn as nn\n\nlayer = nn.Linear(40, 10)\nlayer.weight.data *= 6 ** 0.5  # Kaiming init (or 3 ** 0.5 for LeCun init)\ntorch.zero_(layer.bias.data)\n```", "```py\nnn.init.kaiming_uniform_(layer.weight)\nnn.init.zeros_(layer.bias)\n```", "```py\ndef use_he_init(module):\n    if isinstance(module, nn.Linear):\n        nn.init.kaiming_uniform_(module.weight)\n        nn.init.zeros_(module.bias)\n\nmodel = nn.Sequential(nn.Linear(50, 40), nn.ReLU(), nn.Linear(40, 1), nn.ReLU())\nmodel.apply(use_he_init)\n```", "```py\nalpha = 0.2\nmodel = nn.Sequential(nn.Linear(50, 40), nn.LeakyReLU(negative_slope=alpha))\nnn.init.kaiming_uniform_(model[0].weight, alpha, nonlinearity=\"leaky_relu\")\n```", "```py\nmodel = nn.Sequential(\n    nn.Flatten(),\n    nn.BatchNorm1d(1 * 28 * 28),\n    nn.Linear(1 * 28 * 28, 300),\n    nn.ReLU(),\n    nn.BatchNorm1d(300),\n    nn.Linear(300, 100),\n    nn.ReLU(),\n    nn.BatchNorm1d(100),\n    nn.Linear(100, 10)\n)\n```", "```py\n>>> dict(model[1].named_parameters()).keys()\ndict_keys(['weight', 'bias'])\n```", "```py\n>>> dict(model[1].named_buffers()).keys()\ndict_keys(['running_mean', 'running_var', 'num_batches_tracked'])\n```", "```py\nmodel = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(1 * 28 * 28, 300, bias=False),\n    nn.BatchNorm1d(300),\n    nn.ReLU(),\n    nn.Linear(300, 100, bias=False),\n    nn.BatchNorm1d(100),\n    nn.ReLU(),\n    nn.Linear(100, 10)\n)\n```", "```py\ninputs = torch.randn(32, 3, 100, 200)  # a batch of random RGB images\nlayer_norm = nn.LayerNorm([100, 200])\nresult = layer_norm(inputs)  # normalizes over the last two dimensions\n```", "```py\nmeans = inputs.mean(dim=[2, 3], keepdim=True)  # shape: [32, 3, 1, 1]\nvars_ = inputs.var(dim=[2, 3], keepdim=True, unbiased=False)  # shape: same\nstds = torch.sqrt(vars_ + layer_norm.eps)  # eps is a smoothing term (1e-5)\nresult = layer_norm.weight * (inputs - means) / stds + layer_norm.bias\n# result shape: [32, 3, 100, 200]\n```", "```py\nlayer_norm = nn.LayerNorm([3, 100, 200])\nresult = layer_norm(inputs)  # normalizes over the last three dimensions\n```", "```py\nfor epoch in range(n_epochs):\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n```", "```py\ntorch.manual_seed(42)\n\nmodel_A = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(1 * 28 * 28, 100),\n    nn.ReLU(),\n    nn.Linear(100, 100),\n    nn.ReLU(),\n    nn.Linear(100, 100),\n    nn.ReLU(),\n    nn.Linear(100, 8)\n)\n[...]  # train this model or load pretrained weights\n```", "```py\nimport copy\n\ntorch.manual_seed(42)\nreused_layers = copy.deepcopy(model_A[:-1])\nmodel_B_on_A = nn.Sequential(\n    *reused_layers,\n    nn.Linear(100, 1)  # new output layer for task B\n).to(device)\n```", "```py\nfor layer in model_B_on_A[:-1]:\n    for param in layer.parameters():\n        param.requires_grad = False\n```", "```py\nxentropy = nn.BCEWithLogitsLoss()\naccuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n[...]  # train model_B_on_A\n```", "```py\noptimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.05)\n```", "```py\noptimizer = torch.optim.SGD(model.parameters(),\n                            momentum=0.9, nesterov=True, lr=0.05)\n```", "```py\noptimizer = torch.optim.RMSprop(model.parameters(), alpha=0.9, lr=0.05)\n```", "```py\noptimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), lr=0.05)\n```", "```py\nmodel = [...]  # build the model\noptimizer = torch.optim.SGD(model.parameters(), lr=0.05)  # or any other optim.\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n```", "```py\nfor epoch in range(n_epochs):\n    for X_batch, y_batch in train_loader:\n        [...]  # the rest of the training loop remains unchanged\n\n    scheduler.step()\n```", "```py\ncosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=20, eta_min=0.001)\n```", "```py\n[...]  # build the model and optimizer\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n  optimizer, mode=\"max\", patience=2, factor=0.1)\n```", "```py\nmetric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\nfor epoch in range(n_epochs):\n    for X_batch, y_batch in train_loader:\n        [...]  # the rest of the training loop remains unchanged\n    val_metric = evaluate_tm(model, valid_loader, metric).item()\n    scheduler.step(val_metric)\n```", "```py\nwarmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n    optimizer, start_factor=0.1, end_factor=1.0, total_iters=3)\n```", "```py\nwarmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lambda epoch: (min(epoch, 3) / 3) * (1.0 - 0.1) + 0.1)\n```", "```py\nfor epoch in range(n_epochs):\n    warmup_scheduler.step()\n    for X_batch, y_batch in train_loader:\n        [...]  # the rest of the training loop is unchanged\n    if epoch >= 3:  # deactivate other scheduler(s) during warmup\n        scheduler.step(val_metric)\n```", "```py\ncosine_repeat_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=2, T_mult=2, eta_min=0.001)\n```", "```py\noptimizer = torch.optim.SGD(model.parameters(), lr=0.05, weight_decay=1e-4)\n[...]  # use the optimizer normally during training\n```", "```py\noptimizer = torch.optim.SGD(model.parameters(), lr=0.05)\nparams_to_regularize = [\n    param for name, param in model.named_parameters()\n    if not \"bias\" in name and not \"bn\" in name]\nfor epoch in range(n_epochs):\n    for X_batch, y_batch in train_loader:\n        [...]  # the rest of the training loop is unchanged\n        main_loss = loss_fn(y_pred, y_batch)\n        l2_loss = sum(param.pow(2.0).sum() for param in params_to_regularize)\n        loss = main_loss + 1e-4 * l2_loss\n        [...]\n```", "```py\nparams_bias_and_bn = [\n    param for name, param in model.named_parameters()\n    if \"bias\" in name or \"bn\" in name]\noptimizer = torch.optim.SGD([\n    {\"params\": params_to_regularize, \"weight_decay\": 1e-4},\n    {\"params\": params_bias_and_bn},\n    ], lr=0.05)\n[...]  # use the optimizer normally during training\n```", "```py\nl1_loss = sum(param.abs().sum() for param in params_to_regularize)\nloss = main_loss + 1e-4 * l1_loss\n```", "```py\nmodel = nn.Sequential(\n    nn.Flatten(),\n    nn.Dropout(p=0.2), nn.Linear(1 * 28 * 28, 100), nn.ReLU(),\n    nn.Dropout(p=0.2), nn.Linear(100, 100), nn.ReLU(),\n    nn.Dropout(p=0.2), nn.Linear(100, 100), nn.ReLU(),\n    nn.Dropout(p=0.2), nn.Linear(100, 10)\n).to(device)\n```", "```py\nmodel.eval()\nfor module in model.modules():\n    if isinstance(module, nn.Dropout):\n        module.train()\n\nX_new = [...]  # some new images, e.g., the first 3 images of the test set\nX_new = X_new.to(device)\n\ntorch.manual_seed(42)\nwith torch.no_grad():\n    X_new_repeated = X_new.repeat_interleave(100, dim=0)\n    y_logits_all = model(X_new_repeated).reshape(3, 100, 10)\n    y_probas_all = torch.nn.functional.softmax(y_logits_all, dim=-1)\n    y_probas = y_probas_all.mean(dim=1)\n```", "```py\n>>> y_probas.round(decimals=2)\ntensor([[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.010, 0.000, 0.990],\n [0.990, 0.000, 0.000, 0.000, 0.000, 0.000, 0.010, 0.000, 0.000, 0.000],\n [0.410, 0.040, 0.040, 0.230, 0.040, 0.000, 0.230, 0.000, 0.010, 0.000]],\n device='cuda:0')\n```", "```py\n>>> y_std = y_probas_all.std(dim=1)\n>>> y_std.round(decimals=2)\ntensor([[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.020, 0.000, 0.020],\n [0.020, 0.000, 0.000, 0.000, 0.000, 0.000, 0.020, 0.000, 0.000, 0.000],\n [0.170, 0.030, 0.030, 0.130, 0.050, 0.000, 0.090, 0.000, 0.010, 0.000]],\n device='cuda:0')\n```", "```py\nclass McDropout(nn.Dropout):\n    def forward(self, input):\n        return F.dropout(input, self.p, training=True)\n```", "```py\ndef apply_max_norm(model, max_norm=2, epsilon=1e-8, dim=1):\n    with torch.no_grad():\n        for name, param in model.named_parameters():\n            if 'bias' not in name:\n                actual_norm = param.norm(p=2, dim=dim, keepdim=True)\n                target_norm = torch.clamp(actual_norm, 0, max_norm)\n                param *= target_norm / (epsilon + actual_norm)\n```"]