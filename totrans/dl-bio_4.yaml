- en: Chapter 4\. Understanding Drug–Drug Interactions Using Graphs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章\. 使用图理解药物-药物相互作用
- en: '*Graphs* are a fundamental structure found everywhere in the world around us.
    A familiar example is social networks, where *nodes* represent individuals and
    *edges* capture the relationship between them. In train systems, nodes could represent
    stations and edges the routes linking them. Less obvious examples include research
    collaborations linked by coauthorship, web pages interconnected by hyperlinks,
    and supermarket baskets, where frequently copurchased items are connected.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*图* 是我们周围世界中无处不在的基本结构。一个熟悉的例子是社交网络，其中 *节点* 代表个人，而 *边* 捕获他们之间的关系。在铁路系统中，节点可以代表车站，而边则是连接它们的路线。不那么明显的例子包括通过共同作者关系相连的研究合作，通过超链接相互连接的网页，以及超市购物篮，其中经常一起购买的商品是相互连接的。'
- en: Biology, too, is filled with data that naturally lends itself to a network framework—genes
    interact to control cell functions, proteins physically bind to each other, and
    cells send signals to each other, all forming graph-like systems. Even molecules
    can be represented as graphs, with atoms as nodes and chemical bonds as edges,
    as shown in [Figure 4-1](#example-graphs). At larger biological scales, ecological
    food webs capture predator–prey and other species interactions, while disease
    transmission networks map the spread of pathogens through populations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 生物学也是如此，其中充满了自然适合网络框架的数据——基因相互作用以控制细胞功能，蛋白质在物理上相互结合，细胞相互发送信号，所有这些都形成了类似图的结构。甚至分子也可以表示为图，其中原子作为节点，化学键作为边，如图[图4-1](#example-graphs)所示。在更大的生物尺度上，生态食物网捕捉捕食者-猎物和其他物种之间的相互作用，而疾病传播网络则映射病原体在人群中的传播。
- en: '![](assets/dlfb_0401.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0401.png)'
- en: Figure 4-1\. Examples of graphs from different contexts. The social network
    shows people as nodes connected by edges representing relationships. The rail
    network illustrates stations as nodes and train routes as edges. The molecule
    network depicts the molecular structure of caffeine, where nodes represent atoms,
    and edges represent chemical bonds (hydrogen atoms are not shown).
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1\. 不同上下文中的图示例。社交网络显示人们作为节点通过表示关系的边连接。铁路网络说明车站作为节点，火车路线作为边。分子网络描绘了咖啡因的分子结构，其中节点代表原子，边代表化学键（未显示氢原子）。
- en: These types of network relationships can be modeled using *graph neural networks*
    (GNNs). Recently, deep learning on graphs has become increasingly popular and
    effective. In this chapter, we will explore a graph of *drug–drug interactions*
    (DDIs) to gain insights into its connectivity. Specifically, we aim to predict
    whether two nodes should connect, which is a task known as *link prediction*.
    Link prediction is valuable here because, while we have an existing DDI graph,
    it may be incomplete—some true connections between drugs might be missing due
    to limited research or untested combinations. By accurately predicting these links,
    one could improve drug safety by identifying potential negative interactions and
    even discover new combination therapies by predicting which drugs might interact
    positively.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型的网络关系可以使用 *图神经网络* (GNNs) 进行建模。最近，图上的深度学习变得越来越流行和有效。在本章中，我们将探索一个 *药物-药物相互作用*
    (DDIs) 的图，以了解其连通性。具体来说，我们旨在预测两个节点是否应该连接，这是一个被称为 *链接预测* 的任务。链接预测在这里很有价值，因为我们虽然有一个现有的DDI图，但它可能是不完整的——由于研究有限或未测试的组合，一些药物之间的真实连接可能缺失。通过准确预测这些链接，人们可以通过识别潜在的负面相互作用来提高药物的安全性，甚至可以通过预测哪些药物可能产生积极的相互作用来发现新的联合疗法。
- en: Tip
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: As in earlier chapters, we recommend keeping this chapter’s companion Colab
    notebook open as you read. Running the code yourself helps reinforce the material
    and gives you a place to immediately experiment with new ideas.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的章节一样，我们建议你在阅读本章时保持其配套的Colab笔记本打开。亲自运行代码有助于巩固材料，并为你提供一个立即实验新想法的地方。
- en: Biology Primer
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物学入门
- en: DDIs occur when the effects of one drug are altered by the presence of another.
    DDIs can amplify each drug’s effects, counteract them, or change the way a drug
    is processed in the body, which may result in either therapeutic benefits or adverse
    outcomes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当一种药物的效果因另一种药物的存在而改变时，就会发生DDIs。DDIs可以增强每种药物的效果，抵消它们，或改变药物在体内的处理方式，这可能导致治疗益处或不良后果。
- en: Beneficial Drug–Drug Interactions
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有益的药物-药物相互作用
- en: In some cases, DDIs can be harnessed for therapeutic advantage. In cancer treatment,
    for example, combination therapies pair drugs that target different pathways in
    cancer cells. One drug may inhibit tumor growth, while another restricts the tumor’s
    blood supply, weakening it further. This multitargeted approach not only improves
    patient outcomes but also reduces the likelihood of drug resistance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，药物相互作用（DDI）可以被利用来获得治疗优势。例如，在癌症治疗中，联合疗法将针对癌细胞不同途径的药物配对。一种药物可能抑制肿瘤生长，而另一种则限制肿瘤的血液供应，进一步削弱它。这种多靶点方法不仅改善了患者的预后，还降低了药物耐药性的可能性。
- en: Similarly, certain antibiotics work better in combination. For instance, penicillin
    and gentamicin are often combined to treat infections like endocarditis. Penicillin
    weakens the bacterial cell wall, allowing gentamicin to penetrate the cell and
    disrupt protein synthesis, leading to a more effective antibiotic treatment.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，某些抗生素在联合使用时效果更好。例如，青霉素和庆大霉素常联合使用来治疗心内膜炎等感染。青霉素削弱了细菌细胞壁，使庆大霉素能够穿透细胞并破坏蛋白质合成，从而实现更有效的抗生素治疗。
- en: Harmful Drug–Drug Interactions
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有害的药物-药物相互作用
- en: 'Harmful DDIs are generally much more common than beneficial ones—most drugs
    are not designed with other drugs in mind, which often leads to unintended side
    effects in patients taking multiple medications. Additionally, many drugs influence
    similar biological pathways, increasing the likelihood that one drug will amplify
    or counteract the effects of another. For example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有害的药物相互作用通常比有益的更常见——大多数药物在设计时并没有考虑与其他药物相互作用，这往往会导致患者服用多种药物时出现意外的副作用。此外，许多药物影响相似的生物途径，增加了其中一种药物会增强或抵消另一种药物效果的可能性。例如：
- en: Amplification example
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 增强示例
- en: Aspirin, commonly used as a pain reliever or blood thinner, can amplify the
    effects of other anticoagulants, such as warfarin. When taken together, both drugs
    thin the blood more than intended, raising the risk of excessive bleeding or bruising.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 阿司匹林，常作为止痛药或抗凝血剂使用，可以增强其他抗凝剂（如华法林）的效果。当两者同时服用时，两种药物都会使血液比预期更薄，从而增加过度出血或瘀伤的风险。
- en: Counteraction example
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 抵消示例
- en: Ibuprofen can reduce the effectiveness of antihypertensive drugs, such as ACE
    inhibitors and beta-blockers. Ibuprofen causes the body to retain sodium and fluid,
    which raises blood pressure and counteracts these medications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 布洛芬会降低抗高血压药物（如ACE抑制剂和β-阻滞剂）的有效性。布洛芬会导致身体保留钠和液体，从而升高血压并抵消这些药物。
- en: Most negative DDIs are actually more *indirect*. For instance, many drugs are
    metabolized in the liver by the cytochrome P450 enzyme system, so drugs that inhibit
    this system can impact a wide range of other medications. Grapefruit, though not
    a “drug” in the traditional sense, contains compounds that inhibit the cytochrome
    P450 system. One of the most serious grapefruit interactions occurs with certain
    statins used to control cholesterol. Grapefruit compounds inhibit an enzyme that
    would normally break down these statins, causing higher-than-expected drug levels
    to accumulate in the bloodstream. This buildup can lead to very severe side effects,
    including liver damage and muscle tissue breakdown.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数负面的药物相互作用实际上更为 *间接*。例如，许多药物在肝脏通过细胞色素 P450 酶系统代谢，因此抑制该系统的药物可能会影响广泛的其它药物。尽管葡萄柚在传统意义上不是“药物”，但它含有抑制细胞色素
    P450 系统的化合物。最严重的葡萄柚相互作用之一是某些用于控制胆固醇的降脂药。葡萄柚化合物抑制了本应分解这些降脂药的酶，导致高于预期的药物水平在血液中积累。这种积累可能导致非常严重的副作用，包括肝损伤和肌肉组织分解。
- en: DrugBank
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DrugBank
- en: DrugBank is one of the largest databases of drug interactions, providing detailed
    information on drugs and their known interactions. It has been widely used in
    various DDI studies. For example, in [Figure 4-2](#drugbank-clusters), an early
    study from 2016 clustered DrugBank DDIs (at the time, the database contained around
    1,000 nodes; in this chapter, we work with a more recent version containing over
    4,000 nodes) to reveal major drug clusters, including those related to cytochrome
    P450 interactions discussed earlier.^([1](ch04.html#id726))
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DrugBank 是最大的药物相互作用数据库之一，提供有关药物及其已知相互作用的详细信息。它已被广泛应用于各种药物相互作用（DDI）研究中。例如，在 [图
    4-2](#drugbank-clusters) 中，2016 年的一项早期研究将 DrugBank 的 DDI（当时数据库包含约 1,000 个节点；在本章中，我们使用包含超过
    4,000 个节点的较新版本）进行聚类，以揭示主要的药物聚类，包括之前讨论过的与细胞色素 P450 相互作用相关的聚类。[1](ch04.html#id726)
- en: '![](assets/dlfb_0402.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0402.png)'
- en: Figure 4-2\. Community-based drug–drug interaction network using data from DrugBank
    4.1, containing 1,141 nodes (drugs) and 11,688 edges (drug–drug interactions).
    Clustering was performed using the Force Atlas 2 layout algorithm, which simulates
    a physical system to position nodes closer together based on their interactions,
    with colors assigned to highlight distinct communities of interacting drugs.
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. 使用DrugBank 4.1数据构建的基于社区的药物-药物相互作用网络，包含1,141个节点（药物）和11,688条边（药物-药物相互作用）。聚类是通过使用Force
    Atlas 2布局算法进行的，该算法通过模拟物理系统来定位节点，根据它们的相互作用将节点放置得更近，并使用颜色来突出显示相互作用的药物的不同社区。
- en: In this chapter, we will use a processed version of DrugBank’s DDI data, available
    through a publicly accessible benchmark dataset from the [Open Graph Benchmark](https://oreil.ly/S_wR-)
    resource by researchers from Stanford University.^([2](ch04.html#id727)) Before
    diving into the dataset and its applications, let’s begin with a brief primer
    on machine learning on graphs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用DrugBank的DDI数据的处理版本，该版本可通过斯坦福大学研究人员提供的公开可访问的基准数据集[Open Graph Benchmark](https://oreil.ly/S_wR-)获得.^([2](ch04.html#id727))
    在深入数据集及其应用之前，让我们先简要介绍图上的机器学习。
- en: Machine Learning Primer
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习入门
- en: 'You probably already have an intuitive sense of what a graph is, but to be
    more precise, a graph is a structure that represents relationships between pairs
    of objects. It consists of two main components:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经对图有一个直观的感觉，但要更精确，图是表示成对对象之间关系的结构。它由两个主要组件组成：
- en: Nodes (or vertices)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 节点（或顶点）
- en: These represent individual entities, like people in a social network or proteins
    in an interaction network.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代表单个实体，如社交网络中的人或相互作用网络中的蛋白质。
- en: Edges
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 边
- en: These are the connections between nodes, indicating relationships or interactions.
    In a social network, for example, an edge might represent a friendship, while
    in a protein interaction network, an edge represents an observed physical interaction
    between two proteins.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是节点之间的连接，表示关系或相互作用。例如，在一个社交网络中，边可能代表友谊，而在蛋白质相互作用网络中，边表示两个蛋白质之间观察到的物理相互作用。
- en: Graphs can be *directed* (where edges have a direction, showing a one-way relationship)
    or *undirected* (indicating a two-way relationship). An example of a directed
    biological graph is predator–prey relationships between species in an ecosystem—an
    owl preys on mice; it’s usually not the other way around. An example of an undirected
    biological graph is gene coexpression networks, where the nodes are genes and
    the edges are correlations between the expression levels of each gene pair.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图可以是*有向的*（边有方向，表示单向关系）或*无向的*（表示双向关系）。一个有向生物图的例子是生态系统中山种之间的捕食者-猎物关系——猫头鹰捕食老鼠；通常不是相反的情况。一个无向生物图的例子是基因共表达网络，其中节点是基因，边是每对基因表达水平之间的相关性。
- en: Edges can have *attributes* such as *weights*, which reflect the strength of
    a connection. Nodes can also have attributes that capture additional information.
    For example, in the predator–prey example, edge weights might represent the number
    of times one species predates another, and each node might contain additional
    information about that species such as its estimated population size. Graphs vary
    in connection density (sparse versus dense), may include self-loops (nodes connected
    to themselves), and can be dynamic (changing over time, like social networks)
    or static.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 边可以有*属性*，例如*权重*，它反映了连接的强度。节点也可以有属性来捕获额外的信息。例如，在捕食者-猎物示例中，边权重可能表示一个物种捕食另一个物种的次数，每个节点可能包含有关该物种的额外信息，例如其估计的种群大小。图在连接密度（稀疏与密集）上有所不同，可能包括自环（节点连接到自身），并且可以是动态的（随时间变化，如社交网络）或静态的。
- en: Certain graph properties have significant computational implications. For instance,
    *graph size* can pose a challenge, as large graphs may need to be distributed
    across multiple processing units to avoid memory overload. *Graph sparsity*—which
    is the proportion of existing edges relative to the total possible edges in the
    graph—affects storage and computation efficiency, with specialized techniques
    designed to handle sparsely connected networks. Additionally, sparse graphs allow
    for more efficient convolution operations, as fewer neighbors need to be considered
    (explained further later in this chapter). Finally, the level of *connectivity*
    plays a crucial role. While graphs with many small, disconnected subgraphs can
    often be processed in parallel, densely connected graphs are more challenging
    to parallelize.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 某些图属性具有重大的计算影响。例如，*图大小*可能是一个挑战，因为大型图可能需要分布在多个处理单元上以避免内存过载。*图稀疏性*——即现有边与图中可能存在的总边数的比例——影响存储和计算效率，专门的技术被设计来处理稀疏连接的网络。此外，稀疏图允许更高效的卷积操作，因为需要考虑的邻居更少（将在本章稍后进一步解释）。最后，*连通性*的水平起着至关重要的作用。虽然具有许多小型、不连接的子图的图通常可以并行处理，但密集连接的图更难以并行化。
- en: Representing Graph Structures
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示图结构
- en: 'In [Figure 4-3](#graph-representations), we see an undirected graph containing
    five nodes (N0, N1, N2, N3, N4) and five edges:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 4-3](#graph-representations)中，我们看到一个包含五个节点（N0、N1、N2、N3、N4）和五个边的无向图：
- en: '![](assets/dlfb_0403.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0403.png)'
- en: Figure 4-3\. Visual representation of an undirected graph. The same graph is
    represented as an adjacency matrix and as a bidirectional edge list, where each
    undirected edge is shown in both directions. In practice, many GNN libraries require
    such bidirectional edge lists. Self-edges (not shown here) are also often included
    by default to help preserve node identity. Each node is often associated with
    a feature vector (not shown), but not always (as in this chapter).
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 无向图的视觉表示。相同的图被表示为邻接矩阵和双向边列表，其中每个无向边在两个方向上显示。在实际应用中，许多 GNN 库需要这样的双向边列表。默认情况下，通常还包括自环（此处未显示）以帮助保留节点身份。每个节点通常与一个特征向量相关联（此处未显示），但并非总是如此（如本章所示）。
- en: 'We can numerically represent the graph structure in two main ways:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两种主要方式数值地表示图结构：
- en: Adjacency matrix
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 邻接矩阵
- en: Each node is listed along the rows and columns of a matrix, with edges indicated
    by values in the corresponding cells.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点都列在矩阵的行和列中，边由对应单元格中的值表示。
- en: Edge list
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 边列表
- en: Each row in this list represents an edge by specifying its start and end nodes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的每一行通过指定其起始节点和结束节点来表示一条边。
- en: The choice of representation impacts memory usage, especially depending on graph
    sparsity. An adjacency matrix has fixed high memory usage, as it accounts for
    all possible edges that could exist, while an edge list is more compact because
    it only stores the edges that exist. For sparse graphs, where the number of edges
    is much smaller than the total possible, an edge list is typically much more memory
    efficient.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表示的选择影响内存使用，尤其是在图稀疏性方面。邻接矩阵具有固定的较高内存使用，因为它考虑了所有可能存在的边，而边列表则更紧凑，因为它只存储存在的边。对于边数远小于总可能边的稀疏图，边列表通常更节省内存。
- en: Graph Neural Networks
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图神经网络
- en: With this foundational understanding of graphs, we can explore how graph neural
    networks learn from them. GNNs are a class of models designed to operate directly
    on graph structures, capturing information from both nodes and their connections.
    At a high level, GNNs work by iteratively aggregating information from a node’s
    neighbors, producing rich representations (embeddings) that reflect both the node’s
    features and its position within the broader graph structure. We’ll break down
    this process in more detail shortly—but first, why do we need GNNs in the first
    place? What kinds of graph-related problems can they solve?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对图的基础理解，我们可以探索图神经网络如何从图中学习。GNNs 是一类直接在图结构上操作、从节点及其连接中捕获信息的模型。从高层次来看，GNNs 通过迭代地聚合来自节点邻居的信息，产生丰富的表示（嵌入），这些表示反映了节点的特征及其在更广泛的图结构中的位置。我们将在稍后更详细地分解这个过程——但首先，为什么我们最初需要
    GNNs 呢？它们可以解决哪些与图相关的问题？
- en: 'GNNs are commonly used for these main tasks:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs 通常用于以下主要任务：
- en: Node classification
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 节点分类
- en: Predicting the type or property of a node within a graph; for example, determining
    the category of a drug within a DDI network (e.g., antidepressant, antihistamine,
    or antibiotic).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 预测图中节点类型或属性；例如，在DDI网络中确定药物的类别（例如，抗抑郁药、抗组胺药或抗生素）。
- en: Edge classification
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 边分类
- en: Predicting the type or existence of a connection between two nodes; for example,
    determining whether two drugs are likely to interact.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 预测两个节点之间连接的类型或存在性；例如，确定两种药物是否可能相互作用。
- en: Edge regression
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 边回归
- en: Estimating a continuous value for a connection between nodes. In the context
    of DDI networks, this could involve predicting the severity or strength of an
    interaction rather than just its presence or type.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 估计节点之间连接的连续值。在DDI网络的情况下，这可能涉及预测交互的严重性或强度，而不仅仅是其存在或类型。
- en: Graph classification
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图分类
- en: Predicting a property of an entire graph; for example, identifying whether a
    drug molecule, represented as a graph of atoms and bonds, has a specific property,
    such as being water soluble or binding to a specific disease-associated protein.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 预测整个图的属性；例如，确定表示为原子和键的药物分子是否具有特定的属性，例如水溶性或与特定疾病相关蛋白结合。
- en: 'These tasks all rely on the GNN’s ability to extract meaningful representations
    from the graph structure. Whether the goal is to classify nodes, predict edges,
    or assess whole-graph properties, the core mechanism remains the same: learning
    expressive embeddings through iterative information exchange. This leads us to
    the central idea behind most GNN architectures: *message passing*.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务都依赖于GNN从图结构中提取有意义表示的能力。无论是为了对节点进行分类、预测边还是评估整个图的属性，核心机制都是相同的：通过迭代信息交换学习表达性嵌入。这使我们来到了大多数GNN架构背后的核心思想：**消息传递**。
- en: Graph Embeddings and Message Passing
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图嵌入和消息传递
- en: A primary goal in GNNs is to learn the structure around each node by generating
    a per-node embedding vector that captures information from its neighborhood. Unlike
    in images, where pixels have a fixed spatial arrangement, graph connections lack
    inherent order, making traditional convolutional approaches less applicable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在GNN中，一个主要目标是通过对每个节点生成一个捕获其邻域信息的节点嵌入向量来学习每个节点周围的结构。与图像不同，图像中的像素具有固定的空间排列，图连接缺乏固有的顺序，这使得传统的卷积方法不太适用。
- en: To address this, modern GNNs use a framework known as *message passing*, where
    each node iteratively exchanges messages with its neighbors and aggregates their
    information to update its own representation. This idea was formalized in the
    Message Passing Neural Network (MPNN) framework by Gilmer *et al*., which has
    become the foundation for many contemporary GNN architectures.^([3](ch04.html#id740))
    Earlier forms of GNNs were introduced by Scarselli *et al*., who proposed recursive
    neural models for learning on graphs, though without the modular message-passing
    abstraction seen today.^([4](ch04.html#id741))
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，现代GNN使用一种称为**消息传递**的框架，其中每个节点与其邻居迭代交换消息并聚合它们的信息以更新其自身的表示。这一想法在Gilmer等人提出的消息传递神经网络（MPNN）框架中得到形式化，这已成为许多当代GNN架构的基础。[3](ch04.html#id740)
    早期形式的GNN由Scarselli等人引入，他们提出了用于图学习的递归神经网络模型，尽管没有今天看到的模块化消息传递抽象。[4](ch04.html#id741)
- en: Message passing is a flexible framework that underpins many GNN models. It often
    refers to the interaction between *sender* and *receiver* nodes, where the sender
    transmits information and the receiver aggregates it to update its own representation.
    *Graph convolution* is one specific implementation of message passing, where nodes
    aggregate information from their neighbors using functions such as summation,
    mean, or max. In contrast, nonconvolutional approaches such as graph attention
    networks (GATs) use attention mechanisms to assign different weights to neighbors
    based on their relative importance. The choice of aggregation function—whether
    sum, mean, max, or attention—affects the types of patterns the GNN can learn.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递是一个灵活的框架，是许多GNN模型的基础。它通常指的是**发送者**和**接收者**节点之间的交互，其中发送者传输信息，接收者将其聚合以更新其自身的表示。**图卷积**是消息传递的一种具体实现，其中节点使用求和、平均值或最大值等函数从其邻居聚合信息。相比之下，非卷积方法，如图注意力网络（GATs），使用注意力机制根据邻居的相对重要性分配不同的权重。聚合函数的选择——是求和、平均值、最大值还是注意力——会影响GNN可以学习的模式类型。
- en: 'Increasing the number of message-passing layers (i.e., the number of hops a
    node can “see”) expands each node’s receptive field, enabling it to incorporate
    information from more distant parts of the graph. However, deeper GNNs can run
    into two key challenges:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 增加消息传递层的数量（即节点可以“看到”的跳数）扩展了每个节点的感受野，使其能够整合来自图更远部分的信息。然而，更深的GNN可能会遇到两个关键挑战：
- en: Over-smoothing
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 过度平滑
- en: As the number of message-passing layers increases, each node incorporates information
    from a broader neighborhood. While this can be beneficial up to a point, stacking
    too many layers causes node embeddings to become increasingly similar—eventually
    collapsing to near-identical representations regardless of a node’s local structure
    or features. This degrades the model’s ability to distinguish between nodes, especially
    in classification tasks where fine-grained differences can be very important.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 随着消息传递层数量的增加，每个节点从更广泛的邻域中整合信息。虽然这在一定程度上是有益的，但堆叠过多的层会导致节点嵌入越来越相似——最终无论节点的局部结构或特征如何，都会接近于相同的表示。这降低了模型区分节点的能力，尤其是在分类任务中，细微的差异可能非常重要。
- en: Over-squashing
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 过度压缩
- en: When long-range information must pass through a limited number of intermediate
    nodes or edges, it becomes overly compressed. This bottleneck prevents distant
    signals from being accurately preserved—especially in graphs with long, narrow
    paths, such as trees or hierarchies often seen in biology (e.g., gene regulatory
    networks or phylogenetic trees). As a result, important context from far-apart
    nodes gets “squashed” before it can meaningfully influence predictions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当长距离信息必须通过有限数量的中间节点或边传递时，它变得过于压缩。这个瓶颈阻止了远距离信号被准确保留——特别是在具有长而窄路径的图中，如生物中常见的树或层次结构（例如，基因调控网络或系统发育树）。因此，来自遥远节点的关键上下文在能够有意义地影响预测之前就被“压缩”了。
- en: To mitigate these issues, common strategies include adding skip connections,
    incorporating attention mechanisms, or rewiring the graph to shorten path lengths.
    In practice, using just two or three message-passing layers—capturing information
    from a few hops away—often provides a good balance between expressivity and stability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这些问题，常见的策略包括添加跳过连接、整合注意力机制或重新布线图以缩短路径长度。在实践中，仅使用两到三个消息传递层——捕获几跳之外的信息——通常在表达性和稳定性之间提供了良好的平衡。
- en: Tip
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: In GNNs, “layers” are better understood as message-passing iterations rather
    than traditional neural network layers. Unlike MLPs, where each layer applies
    a distinct learned transformation, each GNN layer aggregates information from
    a node’s immediate neighbors. Thus, a model with three layers enables each node
    to incorporate information from up to three hops away in the graph.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在GNN中，“层”更好地理解为消息传递迭代，而不是传统的神经网络层。与MLP不同，MLP的每一层应用一个不同的学习转换，每个GNN层则从节点的直接邻居中聚合信息。因此，具有三层模型的模型使每个节点能够整合来自图上最多三跳的信息。
- en: Cold-Start Problem
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 冷启动问题
- en: A significant challenge in GNNs is predicting on *unseen nodes*, often referred
    to as the *cold-start problem*. Many traditional graph models operate in a *transductive*
    setting, where training occurs on a fixed graph, limiting predictions to relationships
    among nodes seen during training.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GNN中的一个重大挑战是预测*未见过*的节点，通常被称为*冷启动问题*。许多传统的图模型在*归纳*设置中运行，其中训练发生在固定的图上，限制预测仅限于训练期间看到的节点之间的关系。
- en: 'However, real-world applications often involve dynamic graphs where new nodes
    are introduced. For example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现实世界的应用通常涉及动态图，其中会引入新的节点。例如：
- en: In a social network, a new user joins, and the platform needs to predict their
    potential connections.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在社交网络中，新用户加入，平台需要预测他们可能建立的联系。
- en: In a recommendation system, a newly released product must be matched to relevant
    users based on their preferences.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推荐系统中，新发布的产品必须根据用户的偏好与相关用户匹配。
- en: In drug discovery, a newly synthesized compound must be evaluated for interactions
    with existing molecules.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在药物发现中，新合成的化合物必须评估其与现有分子的相互作用。
- en: 'To address the cold-start problem, GNNs can adopt an *inductive learning* approach,
    enabling generalization to new, unseen nodes. This capability is essential for
    dynamic graphs where new nodes are frequently added, as it eliminates the need
    to retrain the model whenever the graph changes. It is achieved by learning patterns
    and relationships that are transferable across the graph. For example:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决冷启动问题，GNN可以采用*归纳学习*方法，使其能够泛化到新的、未见过的节点。这种能力对于动态图至关重要，因为新节点经常被添加，它消除了每次图发生变化时都需要重新训练模型的需求。这是通过学习在整个图中可转移的模式和关系来实现的。例如：
- en: Instead of memorizing specific connections, the model identifies structural
    similarities (e.g., the role of a node in its local neighborhood) or shared features
    (e.g., common attributes across nodes).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型不是记住特定的连接，而是识别结构相似性（例如，节点在其局部邻域中的作用）或共享特征（例如，节点之间的共同属性）。
- en: When a new node is added, its feature vector and immediate connections to existing
    nodes provide enough context for the model to embed it within the graph and make
    predictions.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当添加新节点时，其特征向量和与现有节点的直接连接为模型提供了足够的上下文，以便在图中嵌入它并做出预测。
- en: Notable frameworks like GraphSAGE focus on inductive learning by sampling neighborhoods
    and aggregating local features to generate embeddings for unseen nodes. Techniques
    such as feature propagation and attention mechanisms further enhance this capability,
    making GNNs highly adaptable to evolving, real-world graphs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GraphSAGE这样的知名框架通过采样邻域和聚合局部特征来生成未见节点的嵌入，专注于归纳学习。技术如特征传播和注意力机制进一步增强了这一能力，使GNN能够高度适应不断发展的现实世界图。
- en: Note
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For a more detailed (but highly accessible) introduction to GNNs, we recommend
    the excellent lecture [“Theoretical Foundations of Graph Neural Networks”](https://oreil.ly/GXy3v)
    by Petar Veličković on YouTube. It offers clear GNN explanations from one of the
    leading experts in the field.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个更详细（但易于理解）的GNN介绍，我们推荐YouTube上Petar Veličković的出色讲座[“Graph Neural Networks的理论基础”](https://oreil.ly/GXy3v)。它提供了来自该领域领先专家的清晰GNN解释。
- en: GraphSAGE
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphSAGE
- en: In this chapter, we implement a GraphSAGE model,^([5](ch04.html#id754)) an inductive
    approach that can predict the properties of nodes it has never seen before by
    aggregating information from their neighbors. In the original paper, GraphSAGE
    was evaluated on tasks like classifying academic papers into six biology-related
    categories using citation graphs, assigning Reddit posts to 50 communities based
    on user interactions, and predicting protein functions across multiple protein–protein
    interaction graphs. These benchmarks demonstrated GraphSAGE’s ability to generalize
    to unseen nodes and outperform traditional methods, highlighting its versatility
    in dynamic, real-world graphs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们实现了一个GraphSAGE模型，^([5](ch04.html#id754))，这是一种归纳方法，可以通过聚合其邻居的信息来预测它之前从未见过的节点的属性。在原始论文中，GraphSAGE在诸如将学术论文分类到六个与生物学相关的类别、根据用户互动将Reddit帖子分配到50个社区以及预测多个蛋白质-蛋白质相互作用图中的蛋白质功能等任务上进行了评估。这些基准测试展示了GraphSAGE泛化到未见节点并优于传统方法的能力，突显了其在动态、现实世界图中的多功能性。
- en: A key advantage of GraphSAGE is its scalability to massive graphs. Training
    on large graphs can be resource intensive because embedding updates for each node
    requires iterating over its neighbors. GraphSAGE addresses this challenge by using
    *subsampling*, where only a small, fixed number of neighbors is sampled for each
    node. These subgraphs are processed in mini-batches, significantly reducing memory
    and computation costs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: GraphSAGE的一个关键优势是其可扩展性，能够处理大规模图。在大型图上进行训练可能需要大量资源，因为每个节点的嵌入更新都需要遍历其邻居。GraphSAGE通过使用*子采样*来应对这一挑战，即对每个节点只采样一小部分固定数量的邻居。这些子图以小批量进行处理，显著降低了内存和计算成本。
- en: 'As illustrated in [Figure 4-4](#graphsage-illustration) (from the [original
    paper](https://oreil.ly/wz_mG)), GraphSAGE has two main components: sampling a
    subgraph and aggregating neighborhood information for each node. The resulting
    embeddings can be used for downstream tasks such as node classification or link
    prediction. While GraphSAGE can incorporate edge or node annotations, it does
    not depend on them, and for most of this chapter, we will focus solely on the
    graph structure.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图4-4](#graphsage-illustration)（来自原始论文[https://oreil.ly/wz_mG](https://oreil.ly/wz_mG)）所示，GraphSAGE有两个主要组件：采样子图和为每个节点聚合邻域信息。生成的嵌入可以用于下游任务，如节点分类或链接预测。虽然GraphSAGE可以包含边或节点注释，但它不依赖于它们，在本章的大部分内容中，我们将专注于图结构。
- en: '![](assets/dlfb_0404.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![dlfb_0404](assets/dlfb_0404.png)'
- en: 'Figure 4-4\. GraphSAGE stands for Graph SAmple and AggreGatE, representing
    its two main steps: (1) sampling a node’s neighbors and (2) aggregating their
    features to generate an embedding. These embeddings can be used for downstream
    tasks, such as (3) predicting node properties or relationships within the graph.'
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4。GraphSAGE代表Graph SAmple and AggreGatE，表示其两个主要步骤：（1）采样一个节点的邻居，（2）聚合它们的特征以生成嵌入。这些嵌入可以用于下游任务，例如（3）预测图中的节点属性或关系。
- en: Selecting a Dataset
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择数据集
- en: 'In this chapter, we’ll work with a unique data source: the Open Graph Benchmark
    (OGB) dataset of processed DrugBank DDIs called [`ogbl-ddi`](https://oreil.ly/WWr52).
    This dataset is particularly convenient for two reasons:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个独特的数据源：处理过的DrugBank DDIs的Open Graph Benchmark (OGB)数据集，称为[`ogbl-ddi`](https://oreil.ly/WWr52)。这个数据集有两个特别方便的原因：
- en: It is well studied, providing a wealth of existing research to draw inspiration
    from.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它已经得到了很好的研究，提供了丰富的现有研究以供借鉴。
- en: It enables us to compare our model’s performance with other approaches using
    the [leaderboard](https://oreil.ly/VPv1R).
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这使我们能够使用[排行榜](https://oreil.ly/VPv1R)将我们的模型性能与其他方法进行比较。
- en: Additionally, OGB simplifies the workflow by offering built-in data loaders
    compatible with various deep learning frameworks and an `Evaluator` class for
    computing problem-specific metrics. This allows us to focus on building and refining
    our model rather than spending a long time on data preparation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，OGB通过提供与各种深度学习框架兼容的内置数据加载器和用于计算特定问题度量的`Evaluator`类，简化了工作流程。这使得我们能够专注于构建和改进我们的模型，而不是花费大量时间在数据准备上。
- en: Describing the Dataset
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述数据集
- en: We have already discussed DDI networks in general. The OGBL DDI dataset in particular
    is an unweighted, undirected graph of DDIs, where each node is an FDA-approved
    or experimental drug and edges represent either beneficial or harmful interactions
    between drugs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了DDI网络的一般情况。特别是，OGBL DDI数据集是一个无权重的、无向的DDI图，其中每个节点是FDA批准或实验性药物，边表示药物之间的有益或有害相互作用。
- en: To make the problem more challenging, the dataset is split in an interesting
    way—by the proteins that each drug targets. This “protein–target split” ensures
    that the test set contains drugs that primarily bind to different proteins than
    those in the training and validation sets, meaning they are more likely to operate
    through distinct biological mechanisms. This forces the model to learn more generalizable
    biology. If we created our own split—such as a random split of drugs—there would
    likely be greater overlap in biological mechanisms between the training and test
    sets, which would make the problem easier but would ultimately reduce the model’s
    ability to generalize to unseen drugs in real-world scenarios.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使问题更具挑战性，数据集以有趣的方式分割——通过每个药物靶向的蛋白质。这种“蛋白质-靶点分割”确保测试集包含主要与训练和验证集中不同蛋白质结合的药物，这意味着它们更有可能通过不同的生物机制起作用。这迫使模型学习更具普遍性的生物学。如果我们创建了自己的分割——例如药物的随机分割——训练和测试集之间在生物机制上可能会有更大的重叠，这会使问题更容易，但最终会降低模型将未见过的药物推广到现实世界场景中的能力。
- en: Exploring the Dataset
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索数据集
- en: 'As always, let’s start by doing some exploratory analysis of the dataset to
    get a feel for what we’re dealing with. We start by loading the data:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，让我们先对数据集进行一些探索性分析，以了解我们正在处理的内容。我们首先加载数据：
- en: '[PRE0]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This downloads the `ogbl-ddi` dataset and neatly packs it into an object ready
    for inspection. The full graph is accessible with `.graph`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载`ogbl-ddi`数据集，并将其整齐地打包成一个可供检查的对象。完整的图可以通过`.graph`访问：
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE2]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The graph is stored in *edge-list* format under the key `edge_index`. Both
    `edge_feat` and `node_feat` are `None`, meaning the graph includes only the structure—without
    additional edge features such as interaction strengths or node features such as
    drug properties. Next, let’s examine the number of nodes and edges in the graph:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图被存储在`edge_index`键下的*边列表*格式中。`edge_feat`和`node_feat`都是`None`，这意味着图只包含结构——没有额外的边特征，如相互作用强度或节点特征，如药物属性。接下来，让我们检查图中的节点和边的数量：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can plot the *degree distribution*, or the distribution of the number of
    connections per node, to get a sense of the high-level graph structure (depicted
    in [Figure 4-5](#fig4-5)):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制*度分布*，即每个节点的连接数分布，以了解图的高级结构（如图4-5所示）：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](assets/dlfb_0405.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![dlfb_0405](assets/dlfb_0405.png)'
- en: Figure 4-5\. The degree distribution of nodes in the DDI network follows a power-law
    distribution where a few drugs interact with many others, but the majority is
    more isolated.
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5。DDI网络中节点的度分布遵循幂律分布，其中一些药物与许多其他药物相互作用，但大多数药物更加孤立。
- en: We observe that a few drugs act as *hubs*, exhibiting a high degree of interaction
    with many other drugs, while most drugs have a low degree, interacting with only
    a few other drugs. This pattern is consistent with a [*power-law distribution*](https://oreil.ly/pcVKt),
    commonly seen in biological and social networks, where a small number of elements
    have very high connectivity (hubs) while the majority have low connectivity. However,
    it is important to note that this characteristic might be specific to this dataset
    and may not generalize to all DDI networks.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到一些药物充当*中心节点*，与许多其他药物有高度的相互作用，而大多数药物的度数较低，只与其他少数药物相互作用。这种模式与在生物和社会网络中常见的[*幂律分布*](https://oreil.ly/pcVKt)一致，其中少数元素具有非常高的连接性（中心节点），而大多数元素具有较低的连接性。然而，需要注意的是，这个特征可能只针对这个数据集，可能不适用于所有DDI网络。
- en: 'We can compute the *density* of the graph, or the ratio of edges to the number
    of possible edges, to quantify how densely interconnected our graph is:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算图的*密度*，即边数与可能边数的比率，以量化我们的图连接的密集程度：
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This shows that while the dataset contains a seemingly large number of edges,
    it is not extremely dense, as 77% of possible connections are absent. With a density
    of 23%, the graph might be considered moderately interconnected, though this label
    is a bit subjective and depends on the specific context.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，尽管数据集包含看似大量的边，但它并不极端密集，因为77%的可能连接不存在。以23%的密度计算，该图可能被认为是适度互联的，尽管这个标签有点主观，并且取决于具体上下文。
- en: 'The dataset comes with its own methods to extract useful information. For example,
    `.get_edge_split` will list the graph’s edges across the different data splits:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集附带其自己的方法来提取有用信息。例如，`.get_edge_split`将列出图在不同数据拆分中的边：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can see that the `valid` and `test` splits actually contain two types of
    edges:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`valid`和`test`拆分实际上包含两种类型的边：
- en: The `edge` key holds the positive data, representing known drug interactions.
    Here, *positive* refers to the fact that these interactions are known, not whether
    they are beneficial or harmful.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`edge`键包含正数据，表示已知的药物相互作用。在这里，*正*指的是这些相互作用是已知的，而不是它们是有益的还是有害的。'
- en: The `edge_neg` key contains negative edges, representing drug pairs with no
    known interactions. However, because some interactions may simply be undiscovered,
    this data is considered *weakly labeled* and may include inaccuracies (false negatives).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`edge_neg`键包含负边，表示没有已知相互作用的药物对。然而，由于一些相互作用可能只是未被发现，因此这些数据被认为是*弱标签*，可能包含不准确（假阴性）。'
- en: Importantly, the training dataset does not include explicit negative edges (i.e.,
    there’s no predefined `edge_neg` list in `train`). However, since most node pairs
    in a sparse graph are unconnected, negative samples can be drawn from this large
    set of nonedges during training. The method used to sample these negatives is
    an important hyperparameter, as it can significantly affect performance. Some
    negative edges are trivially easy to distinguish, which can lead to inflated metrics.
    In contrast, the validation and test datasets include a predefined `edge_neg`
    key that specifies which unconnected node pairs to use for evaluation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，训练数据集不包含显式的负边（即在`train`中没有预定义的`edge_neg`列表）。然而，由于稀疏图中大多数节点对都是未连接的，因此可以在训练期间从这个大量非边集中抽取负样本。用于抽取这些负样本的方法是一个重要的超参数，因为它可以显著影响性能。一些负边很容易区分，这可能导致指标膨胀。相比之下，验证和测试数据集包括一个预定义的`edge_neg`键，它指定了用于评估的未连接节点对。
- en: 'Let’s now examine the relative sizes of the `train`, `valid`, and `test` splits:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在检查`train`、`valid`和`test`拆分的相对大小：
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this dataset, the training set contains roughly 10 times more positive edges
    than the validation and test sets.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，训练集包含的正边数大约是验证和测试集的10倍。
- en: 'Another important consideration is whether all nodes in the validation and
    test sets also appear in the training set. This determines whether the model will
    encounter completely unseen nodes during evaluation—a key distinction between
    transductive and inductive learning. You can check this with the following code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的考虑因素是验证集和测试集中的所有节点是否也出现在训练集中。这决定了模型在评估过程中是否会遇到完全未见的节点——这是归纳学习和推理学习之间的一个关键区别。您可以使用以下代码进行检查：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In our case, all nodes in the validation and test sets are indeed present in
    the training graph. This defines a transductive setting, where the model sees
    all nodes during training and only needs to predict whether specific edges exist
    between them. This setup is simpler than the inductive case, where the model must
    make predictions involving entirely unseen nodes.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，验证集和测试集中的所有节点确实都存在于训练图中。这定义了一个推理设置，其中模型在训练期间看到所有节点，并且只需要预测它们之间是否存在特定的边。这种设置比归纳情况简单，在归纳情况下，模型必须对涉及完全未见节点的预测进行预测。
- en: Starting with transductive evaluation allows us to assess model performance
    in a controlled setting before tackling the more complex inductive scenario. Models
    like GraphSAGE are well suited to inductive tasks because they generate node embeddings
    based on local neighborhoods. This means that even unseen nodes can be embedded
    meaningfully, provided they connect to known parts of the graph.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 从推理评估开始，我们可以在一个受控的环境中评估模型性能，然后再处理更复杂的归纳场景。像GraphSAGE这样的模型非常适合归纳任务，因为它们根据局部邻域生成节点嵌入。这意味着即使未见过的节点也可以有意义地嵌入，前提是它们连接到图中的已知部分。
- en: For now, we’ll focus on the transductive case and ensure that the model performs
    well when all nodes are known.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将专注于推理情况，并确保当所有节点都已知时，模型表现良好。
- en: Examining Drug Names
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查药物名称
- en: 'Although not immediately available in the graph object, there is additional
    annotation data that comes with the `ogbl-ddi` dataset. Let’s examine this information:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些附加注释数据在图对象中不是立即可用的，但`ogbl-ddi`数据集附带了一些额外的注释数据。让我们检查这些信息：
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Output:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can see that each row is a DDI, with each drug having an ID (an accession
    in the [DrugBank database](https://oreil.ly/CfISy)) and a description of the nature
    of the interaction.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每一行都是一个药物-药物相互作用（DDI），每个药物都有一个ID（[DrugBank数据库](https://oreil.ly/CfISy)中的访问号）以及描述其相互作用性质的描述。
- en: Note
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This chapter’s dataset is ultimately derived from [DrugBank](https://oreil.ly/MYb6A),
    which provides extensive information about drugs and their interactions. While
    some of this information is included in the benchmark dataset, much more could
    be added, such as chemical properties, target genes, and other drug-specific details.
    However, access to the full DrugBank resource is not free for nonacademic users.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的数据集最终来源于[DrugBank](https://oreil.ly/MYb6A)，它提供了关于药物及其相互作用的广泛信息。虽然其中一些信息包含在基准数据集中，但还可以添加更多，例如化学性质、靶基因和其他药物特定细节。然而，非学术用户访问完整的DrugBank资源并非免费。
- en: 'When working with our graph, we will mostly be dealing with node indices, but
    we can always look up the mapping between node ID and DrugBank drug IDs:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理我们的图时，我们主要会处理节点索引，但我们始终可以查找节点ID和DrugBank药物ID之间的映射：
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This lookup allows us to look a bit deeper at the degree distribution observation
    from earlier. What are the drugs that bind many other drugs? Let’s examine the
    drugs with the highest number of edges. Since all but 14 drug interactions are
    represented twice in this dataframe (once as *A-B* and once as *B-A*), we can
    count on the `first drug name` column to get the most frequently binding drugs:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这种查找允许我们更深入地观察之前观察到的度分布。哪些药物与许多其他药物结合？让我们检查具有最高边数的药物。由于除了14个药物相互作用之外，这个数据框中的所有药物相互作用都表示了两次（一次作为“A-B”，一次作为“B-A”），我们可以依靠“第一个药物名称”列来获取最频繁结合的药物：
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Output:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Figure 4-6](#top-interacting-drugs) visualizes the structure of these top
    interacting drugs. Interestingly, many of these drugs, such as desipramine, amitriptyline,
    and clomipramine, share a common three-ring (tricyclic) core structure, which
    may contribute to their similar interaction profiles.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-6](#top-interacting-drugs)展示了这些顶级相互作用药物的结构。有趣的是，许多这些药物，如去甲替林、阿米替林和氯米帕明，共享一个常见的三环（三环）核心结构，这可能会对它们的相似相互作用特征做出贡献。'
- en: '![](assets/dlfb_0406.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0406.png)'
- en: Figure 4-6\. Chemical structures of the top 10 drugs with the highest number
    of drug–drug interactions in the dataset. Interestingly, many of these drugs,
    such as desipramine, amitriptyline, and clomipramine, share a common three-ring
    (tricyclic) core structure, which may contribute to their similar interaction
    profiles by promoting broad target binding and extensive metabolism via cytochrome
    P450 enzymes. Structures were acquired from DrugBank.
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6。数据集中药物-药物相互作用次数最多的前10种药物的化学结构。有趣的是，许多这些药物，如去甲替林、阿米替林和氯米帕明，具有一个共同的三个环（三环）核心结构，这可能会通过促进广泛的靶点结合和通过细胞色素P450酶的广泛代谢来促进它们相似的相互作用模式。结构来自DrugBank。
- en: 'This list of drugs may seem a bit obscure if you’re not accustomed to memorizing
    drug names, but there are a few emergent patterns here:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不习惯于记忆药物名称，这个药物列表可能看起来有点晦涩，但这里有一些明显的模式：
- en: Affecting transporter proteins
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 影响转运蛋白
- en: The drug with the highest number of interactions (2,477) is quinidine, used
    to treat certain heart arrhythmias. Like other drugs on this list, such as clozapine
    and carbamazepine, quinidine interacts strongly with transporter proteins (with
    the most famous one being a protein called P-glycoprotein), which regulate the
    absorption and transport of many drugs across cells. This broad influence on drug
    levels largely explains its high interaction count in this dataset.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 交互次数最多的药物（2,477次）是奎尼丁，用于治疗某些心律失常。像列表上的其他药物，如氯氮平和卡马西平一样，奎尼丁与转运蛋白（最著名的是一种称为P-糖蛋白的蛋白质）强烈相互作用，这些蛋白调节许多药物在细胞间的吸收和运输。这种对药物水平广泛的影响在很大程度上解释了它在数据集中高交互次数的原因。
- en: Affecting drug metabolism
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 影响药物代谢
- en: Many of these drugs, like the antidepressants (desipramine, amitriptyline, clomipramine,
    imipramine), the antipsychotics (chlorpromazine, clozapine, haloperidol), and
    the mood stabilizer carbamazepine, are metabolized by the cytochrome P450 enzyme
    family in the liver. This system, introduced earlier, plays a major role in drug
    metabolism and is central to many drug interactions, because drugs that inhibit
    or activate cytochrome P450 enzymes can alter the metabolism of other drugs taken
    simultaneously.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些药物，如抗抑郁药（去甲替林、阿米替林、氯米帕明、丙咪嗪）、抗精神病药（氯丙嗪、氯氮平、氟哌啶醇）和情绪稳定剂卡马西平，在肝脏中被细胞色素P450酶家族代谢。这个系统在药物代谢中起着重要作用，并且是许多药物相互作用的核心，因为抑制或激活细胞色素P450酶的药物可以改变同时使用的其他药物的代谢。
- en: Dosage sensitivity
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 剂量敏感性
- en: Finally, these top interacting drugs also tend to have narrow *therapeutic ranges*,
    meaning even small changes in blood concentrations can lead to adverse effects.
    This makes interactions more likely to occur and be noticed.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这些顶级相互作用药物也往往具有狭窄的*治疗范围*，这意味着血液浓度的小幅变化可能导致不良反应。这使得相互作用更有可能发生并被注意到。
- en: 'From this additional table of drug information, we can construct a lookup table
    of `node_id` to DrugBank `dbid` to drug names, allowing us to bring more biological
    context to our project as we start modeling:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个额外的药物信息表中，我们可以构建一个`node_id`到DrugBank `dbid`再到药物名称的查找表，这样我们就可以在我们开始建模时为我们的项目带来更多的生物背景：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Output:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: For example, using this lookup table, we can see that node ID 935 corresponds
    to the drug memantine, which has the DrugBank ID DB01043.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用这个查找表，我们可以看到节点ID 935对应的是药物美金刚，其DrugBank ID为DB01043。
- en: Visualizing Graphs
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图形可视化
- en: 'Now let’s take a look at what a portion of this graph data actually looks like.
    The entire graph is too large to meaningfully visualize all at once, but we can
    sample a subgraph and visualize that. The strategy here is to select a subset
    of nodes from the original training graph and then subset the split dataset by
    these nodes:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这些图数据的一部分实际看起来是什么样子。整个图太大，无法一次性有意义地可视化，但我们可以采样一个子图并可视化它。这里的策略是从原始训练图中选择一组节点，然后通过这些节点对分割数据集进行子集划分：
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Output:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This extracts a subgraph containing 50 nodes from the training set. We can
    visualize it in [Figure 4-7](#ddi-subgraph-plot) using the `plot_ddi_graph` function,
    which leverages the popular `networkx` library—a widely used Python tool for creating
    and visualizing graph structures:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作从训练集中提取了一个包含50个节点的子图。我们可以使用`plot_ddi_graph`函数将其可视化，该函数利用了流行的`networkx`库——一个广泛使用的Python工具，用于创建和可视化图结构：[图4-7](#ddi-subgraph-plot)
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](assets/dlfb_0407.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0407.png)'
- en: Figure 4-7\. A sampled subgraph of the DDI network, with nodes labeled by drug
    names. While this graph was sampled to just 50 nodes for clarity, the visualization
    already highlights the diversity of interactions, including densely connected
    drugs and more isolated drugs.
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7\. DDI网络的采样子图，节点以药物名称标注。虽然为了清晰起见，这个图只采样了50个节点，但可视化已经突出了交互的多样性，包括密集连接的药物和更孤立的药物。
- en: '[Figure 4-7](#ddi-subgraph-plot) highlights the diversity of interactions,
    including densely connected clusters (e.g., around Clomipramine, an antidepressant)
    and isolated or sparsely connected drugs. While this graph was sampled for clarity,
    it illustrates how certain drugs act as hubs, reflecting their broad interaction
    profiles, and others interact more selectively, potentially due to specific biological
    mechanisms.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-7](#ddi-subgraph-plot)突出了交互的多样性，包括密集连接的簇（例如，围绕抗抑郁药氯米帕明）和孤立或稀疏连接的药物。虽然这个图是为了清晰起见而采样的，但它说明了某些药物如何作为中心节点，反映了它们广泛的交互特征，而其他药物则更具有选择性，这可能是由于特定的生物机制。'
- en: With this initial data exploration complete, we’re ready to move on to building
    the dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成初步的数据探索后，我们准备开始构建数据集。
- en: Building a Dataset
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建数据集
- en: Having explored the dataset from `LinkPropPredDataset`, we now turn our attention
    to the process of preparing it for use in the JAX/Flax framework. Although the
    dataset isn’t out-of-the-box compatible, this offers a valuable opportunity to
    better understand the intricacies of graph processing. In this section, we’ll
    walk through the necessary adjustments to ensure that the dataset is properly
    formatted for our model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了`LinkPropPredDataset`中的数据集后，我们现在将注意力转向准备它用于JAX/Flax框架的过程。尽管数据集不是即插即用的，但这为我们更好地理解图处理细节提供了一个宝贵的机会。在本节中，我们将介绍必要的调整，以确保数据集以适当格式为我们的模型准备。
- en: Fortunately, we don’t have to start from scratch. The JAX ecosystem has `jraph`,
    a graph library that offers foundational, graph-aware classes and data structures,
    allowing us to build flexible graph processing models while benefiting from JAX’s
    speed and efficiency. If you’d like to explore `jraph` in more detail before diving
    into our implementation, we recommend this excellent tutorial on graph nets with
    `jraph` from DeepMind.^([6](ch04.html#id764))
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不必从头开始。JAX生态系统中的`jraph`是一个图库，它提供了基础、图感知的类和数据结构，使我们能够在利用JAX的速度和效率的同时构建灵活的图处理模型。如果你想在深入我们的实现之前更详细地了解`jraph`，我们推荐DeepMind提供的这篇关于使用`jraph`的图网的优秀教程.^([6](ch04.html#id764))
- en: Note
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: PyTorch, particularly its extension library `pytorch-geometric`, is arguably
    the most comprehensive deep learning framework for working with graphs. It offers
    a robust toolkit that simplifies selecting graph models from a model zoo, handling
    efficient data loading, and working with convenient data classes. Datasets like
    OGBL have dedicated data loaders tailored for this framework. However, in this
    chapter, we are using `jraph`, as it integrates seamlessly with JAX, aligning
    better with our overall approach for the book.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch，尤其是其扩展库`pytorch-geometric`，可以说是用于处理图的最为全面的深度学习框架。它提供了一个强大的工具包，简化了从模型库中选择图模型、处理高效数据加载以及使用方便的数据类的工作。像OGBL这样的数据集有针对该框架定制的数据加载器。然而，在本章中，我们使用`jraph`，因为它与JAX无缝集成，更好地符合我们为本书制定的整体方法。
- en: Let’s get started with building a dataset we can train models on. As mentioned,
    there are several ways to represent a graph, such as using an adjacency matrix
    or an edge list. Since we’re using `jraph`, we go for the edge-list format, the
    default, which is much more memory efficient for sparser datasets like a DDI network.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建一个可以训练模型的数据集。如前所述，有几种方式来表示图，例如使用邻接矩阵或边列表。由于我们使用`jraph`，我们选择了边列表格式，这是默认格式，对于像DDI网络这样的稀疏数据集来说，它更加内存高效。
- en: Creating a Dataset Builder
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据集构建器
- en: 'We have packaged the dataset building into a class called `DatasetBuilder`.
    As we go along, you’ll recognize many parts from the previous section where we
    explored the raw dataset. Let’s go through it step-by-step, starting with the
    main method, `build`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集构建封装到一个名为`DatasetBuilder`的类中。随着我们的进行，你会认识到许多来自上一节的内容，我们在那里探讨了原始数据集。让我们一步一步地来，从主方法`build`开始：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: During instantiation, the builder receives a `path` to ensure that the dataset
    is stored in the specified location, eliminating the need to redownload it every
    time. The `build` method then generates a dictionary where the keys indicate data
    splits, each associated with a `Dataset` value. We’ll examine the `Dataset` class
    in a little more detail shortly, but for now, think of it as a dataset bundle
    with convenience methods for easier handling during training. The parameters passed
    to `build` help with subsetting the graph, which we will also get into a little
    bit later.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化过程中，构建器接收一个`path`以确保数据集存储在指定的位置，从而消除了每次都需要重新下载的需求。然后`build`方法生成一个字典，其中键表示数据分割，每个键都与一个`Dataset`值相关联。我们将在稍后更详细地检查`Dataset`类，但到目前为止，可以将其视为一个数据集包，它具有在训练期间便于处理的便利方法。传递给`build`的参数有助于子集化图，我们稍后会详细介绍这一点。
- en: Download the Raw Dataset
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载原始数据集
- en: 'Looking at the `build` code, we can see that the raw dataset is first downloaded,
    leveraging the `LinkPropPredDataset` we saw before. Since the training split does
    not have negative pairs, we add a `neg_edges` key to simplify later handling:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`build`代码，我们可以看到首先下载了原始数据集，利用了之前看到的`LinkPropPredDataset`。由于训练分割没有负样本对，我们添加了一个`neg_edges`键以简化后续处理：
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Prepare the Annotation
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备标注
- en: 'The dataset annotation is not directly useful when training our model in this
    project, but it is handy to have readily accessible to perform all sorts of sanity
    checks. You will recognize the implementation from before:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中训练我们的模型时，数据集标注不是直接有用的，但它便于随时可用以执行各种合理性检查。您将从前面的实现中认出它：
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The annotation is the same for all the dataset splits; hence, we only need to
    prepare it once and assign it to the `Dataset`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 标注对所有数据集分割都是相同的；因此，我们只需要准备一次并将其分配给`Dataset`。
- en: Prepare the Graph
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备图
- en: 'Next we look at `prepare_graph`, one of the main functions of the dataset builder:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看看`prepare_graph`，这是数据集构建器的主要函数之一：
- en: '[PRE28]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `make_undirected` method ensures that the DDI graph is undirected, meaning
    the relationship between drugs *A-B* is equivalent to *B-A*. Since `jraph` does
    not offer a toggle between directed and undirected graphs, we need to represent
    all edges in both directions. This process, known as *symmetrizing* the graph,
    makes the adjacency relationships symmetric, effectively converting a directed
    graph into an undirected one. This transformation is applied across all dataset
    splits.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_undirected`方法确保DDI图是无向的，这意味着药物*A-B*之间的关系等同于*B-A*。由于`jraph`不提供在有向图和无向图之间切换的功能，我们需要以两种方向表示所有边。这个过程称为*对称化*图，使得邻接关系对称，有效地将有向图转换为无向图。这种转换应用于所有数据集分割。'
- en: 'Practically speaking, implementing this transformation is straightforward.
    We start with the `pos_pairs` and add a corresponding set of edges where the sender
    and receiver nodes are swapped:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，实现这种转换很简单。我们从`pos_pairs`开始，添加一组相应的边，其中发送者和接收者节点被交换：
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we prepare the graph using the main parameters that `GraphsTuple` expects:
    `senders` and `receivers`, which define the edges by specifying the source and
    destination nodes. Each node or edge can be annotated (although they are not here),
    with node annotations stored in `nodes` and edge annotations in `edges`. In addition,
    `GraphsTuple` incorporates metadata such as `n_nodes` and `n_edges`, which indicate
    the number of nodes and edges, respectively, and `globals`, which can store graph-level
    information such as a unique graph identifier or aggregated features. While we
    won’t use `globals` here, it remains available for scenarios requiring data applicable
    to the entire graph.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`GraphsTuple`期望的主要参数来准备图：`senders`和`receivers`，通过指定源节点和目标节点来定义边。每个节点或边都可以进行标注（尽管这里没有），节点标注存储在`nodes`中，边标注存储在`edges`中。此外，`GraphsTuple`还包含元数据，如`n_nodes`和`n_edges`，分别表示节点和边的数量，以及`globals`，可以存储图级别的信息，如唯一的图标识符或聚合特征。虽然我们在这里不会使用`globals`，但它仍然可用于需要适用于整个图的数据的场景。
- en: Warning
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: You may wonder why we sometimes need to pass the number of nodes independently
    from the graph. Why can’t this be inferred from the edges? This is because inferring
    node count from edges could miss isolated nodes, which have no connections to
    other nodes (i.e., no interactions with other drugs).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道为什么有时我们需要独立于图传递节点数。为什么不能从边中推断出来？这是因为从边中推断节点数可能会错过孤立节点，这些节点没有与其他节点（即没有与其他药物）的连接（即没有交互）。
- en: In general, `GraphsTuple` is a versatile data structure that can host data in
    various ways. Instead of having one `GraphsTuple` per data split, we could construct
    a single graph containing both training and evaluation datasets, using the `nodes`
    attribute to specify which set each node belongs to.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，`GraphsTuple` 是一种灵活的数据结构，可以以多种方式存储数据。我们不必为每个数据分割创建一个 `GraphsTuple`，而是可以构建一个包含训练集和评估数据集的单个图，使用
    `nodes` 属性来指定每个节点属于哪个集合。
- en: Prepare the Pairs
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备样本对
- en: 'With the graph in place, we use the `prepare_pairs` method to obtain the drug–drug
    pairs—both positive and negative—that the model will classify as either connected
    or not:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图构建完成后，我们使用 `prepare_pairs` 方法获取模型将分类为连接或不连接的药物-药物对——包括正负样本：
- en: '[PRE30]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: For evaluation datasets, preparing pairs is straightforward, as we can directly
    use the positive and negative pairs provided by the OGBL dataset.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于评估数据集，准备样本对很简单，因为我们可以直接使用 OGBL 数据集提供的正负样本对。
- en: Note
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You might wonder why we don’t use the edges from the graph we just created to
    generate the positive pairs. The reason is that since we made the graph undirected,
    each positive pair is represented twice, which could lead to redundancy and errors
    during evaluation.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么我们不使用我们刚刚创建的图中的边来生成正样本对。原因是，由于我们使图无向，每个正样本对被表示了两次，这可能导致评估过程中的冗余和错误。
- en: 'For the training dataset, preparing pairs is slightly more complex because
    negative pairs are not provided and must be inferred using the `infer_negative_pairs`
    method:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据集，准备样本对稍微复杂一些，因为负样本对没有提供，必须使用 `infer_negative_pairs` 方法推断：
- en: '[PRE31]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The method begins by constructing an adjacency matrix and initializing it with
    zeros using NumPy. It then marks all existing edges with ones. To identify negative
    edges, the method flips the matrix values so that connections become zeros and
    nonconnections become ones. Finally, it retains only the upper triangle (`triu`)
    of the matrix (excluding the diagonal) to avoid self-loops and duplicate pairs.
    The remaining nonzero entries are converted into an edge list of negative node
    pairs.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法首先使用 NumPy 构建一个邻接矩阵，并用零初始化。然后，它将所有现有边标记为 1。为了识别负边，该方法翻转矩阵值，使得连接变为 0，非连接变为
    1。最后，它仅保留矩阵的上三角部分（不包括对角线），以避免自环和重复样本对。剩余的非零条目被转换为负节点对的边列表。
- en: The resulting negative pairs far outnumber the positive pairs due to the graph’s
    sparsity. This imbalance can be advantageous, as it provides more examples of
    negative edges to sample. However, as mentioned previously, the way negative pairs
    are sampled significantly affects performance, as some pairs are trivial to predict
    as being unconnected. We will need to carefully select a fair subset of negative
    pairs during training.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图稀疏性，生成的负样本对远多于正样本对。这种不平衡可能是有利的，因为它提供了更多负边的样本。然而，如前所述，负样本对的采样方式会显著影响性能，因为有些对很容易预测为未连接。在训练过程中，我们需要仔细选择一个公平的负样本子集。
- en: Note
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Using an adjacency matrix approach assumes that it can fit into memory. If this
    is not feasible, alternative methods for generating negative node pairs include
    sampling a noncomprehensive subset or using efficient implementations that rely
    on sparse adjacency matrices.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用邻接矩阵方法假设它可以适应内存。如果这不可行，生成负节点对的替代方法包括采样一个非全面子集或使用依赖于稀疏邻接矩阵的高效实现。
- en: The positive and negative pairs are then encapsulated in a `Pairs` data class,
    which we’ll examine further during training. It is a simple data class that stores
    arrays of positive and negative pairs and includes utilities for subsampling pairs
    during learning and accessing pairs in batches.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 正负样本对随后被封装在一个名为 `Pairs` 的数据类中，我们将在训练过程中进一步探讨它。这是一个简单的数据类，用于存储正负样本对数组，并包括在学习过程中对样本进行子采样和在批量中访问样本的实用工具。
- en: Subsetting the Graph
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图的子集
- en: 'To efficiently explore how a model learns from graphs, it is useful to be able
    to create a smaller subset of the dataset. This allows us to work with a more
    manageable graph size for experimentation. The subset method does exactly that:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地探索模型如何从图中学习，能够创建数据集的一个较小子集非常有用。这允许我们在实验中使用更易于管理的图大小。子集方法正是这样做的：
- en: '[PRE32]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: It selects a subset of nodes based on a specified `node_limit` and applies this
    subset consistently across all dataset splits (i.e., training, validation, test).
    By default, the subsetted graph renumbers the node IDs to create a smaller, compact
    graph. However, you can retain the original node IDs from the full dataset by
    setting the `keep_original_ids` parameter to `True`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 它根据指定的`node_limit`选择节点子集，并在所有数据集分割（即训练、验证、测试）中一致地应用此子集。默认情况下，子集图重新编号节点ID以创建一个更小、更紧凑的图。然而，您可以通过将`keep_original_ids`参数设置为`True`来保留来自完整数据集的原始节点ID。
- en: The Dataset Class
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集类
- en: 'Finally, we have all the pieces in place to create a `Dataset` class that will
    enable flexible exploration of the graph along with its annotations:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们已经准备好所有组件来创建一个`Dataset`类，它将允许灵活地探索图及其注释：
- en: '[PRE33]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here, we show only the `Dataset` fields, but the class also provides several
    useful methods. Notably, it handles subsetting the graph and managing annotations
    consistently behind the scenes. While we encourage you to explore the code online
    to get a better sense of its functionality, understanding every detail isn’t necessary
    to begin training.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只展示了`Dataset`字段，但该类还提供了几个有用的方法。值得注意的是，它在幕后处理图的子集和注释管理。虽然我们鼓励您在线探索代码以更好地了解其功能，但理解每个细节并不是开始训练所必需的。
- en: Building a Prototype
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建原型
- en: Let’s start simple. We need to build a model that predicts links between nodes.
    We’ll do this using only the graph’s connectivity—no node features or annotations.
    Surprisingly, this connectivity information alone can be quite powerful for learning
    which node pairs are likely to be connected.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单开始。我们需要构建一个预测节点之间链接的模型。我们将仅使用图的连通性——没有节点特征或注释来完成这项工作。令人惊讶的是，仅凭这种连通性信息本身就可以非常强大，用于学习哪些节点对可能连接在一起。
- en: While link prediction can be framed as a binary classification task (i.e., connection
    versus no-connection), it differs from typical classification problems in key
    ways. The input to the model is not a single node or feature vector, but a pair
    of nodes, and the prediction depends on their structural relationship in the graph.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然链接预测可以被视为一个二元分类任务（即连接与不连接），但它与典型的分类问题在关键方面有所不同。模型的输入不是一个单独的节点或特征向量，而是一对节点，并且预测取决于它们在图中的结构关系。
- en: 'Our prototype model will be made up of several key components, some defined
    directly within the model and others handled as part of the training process:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的原型模型将由几个关键组件组成，其中一些直接在模型中定义，而其他则作为训练过程的一部分进行处理：
- en: 'Model components:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型组件：
- en: 'Neighborhood encoding: Generates node embeddings that reflect the local graph
    structure'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邻域编码：生成反映局部图结构的节点嵌入
- en: 'Link prediction: Uses these embeddings to score the likelihood of a connection
    between node pairs'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 链接预测：使用这些嵌入来评估节点对之间连接的可能性
- en: 'Training components:'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练组件：
- en: 'Negative sampling: Selects unconnected node pairs to contrast with true edges'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负采样：选择未连接的节点对以与真实边进行对比
- en: 'Loss function: Computes the training signal to optimize model performance'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数：计算训练信号以优化模型性能
- en: 'We begin by focusing on the most crucial part: how we encode each node’s local
    neighborhood.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先关注最关键的部分：我们如何编码每个节点的局部邻域。
- en: Node Encoder
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点编码器
- en: 'Arguably the most impactful choice for our model is how we encode the nodes’
    neighborhood. For this, we use a GraphSAGE-inspired implementation:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型来说，最具影响力的选择可能是我们如何编码节点的邻域。为此，我们使用一个受GraphSAGE启发的实现：
- en: '[PRE34]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The main input parameters to the module are:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 模块的主要输入参数包括：
- en: '`n_nodes`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_nodes`'
- en: Defines the total number of nodes in the original graph.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 定义原始图中的节点总数。
- en: '`embedding_dim`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`embedding_dim`'
- en: Specifies the dimensionality of the node embeddings. This controls how richly
    the model can represent neighborhood information. Lower values (e.g., 16 or 32)
    may limit expressiveness, while higher values (e.g., 128 or 256) offer more capacity
    at the cost of increased computation. In practice, smaller graphs can support
    higher embedding dimensions, while larger graphs benefit from lower values here.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 指定节点嵌入的维度。这控制了模型可以多么丰富地表示邻域信息。较低的值（例如，16或32）可能会限制表达能力，而较高的值（例如，128或256）提供了更多的容量，但代价是增加了计算量。在实践中，较小的图可以支持较高的嵌入维度，而较大的图则受益于较低的值。
- en: '`dropout_rate`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`dropout_rate`'
- en: Sets the fraction of neurons randomly deactivated during training to reduce
    overfitting.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 设置在训练期间随机失活的神经元比例，以减少过拟合。
- en: '`last_layer_self` and `degree_norm`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`last_layer_self`和`degree_norm`'
- en: Configure aspects of the graph convolution behavior, which are described in
    more detail in the next section.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 配置图卷积行为的一些方面，这些方面将在下一节中更详细地描述。
- en: At the heart of GraphSAGE is the `node_embeddings` matrix. In the `setup` method,
    this is initialized as a learnable parameter using `nn.Embed`, with shape `[n_nodes,
    embedding_dim]`. The embeddings are initialized using the `glorot_uniform` method
    to promote stable training dynamics. During training, these embeddings are updated
    by aggregating information from each node’s neighbors, gradually encoding higher-order
    structural patterns. The goal is for these embeddings to converge to representations
    that reflect the likelihood of connections between node pairs.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: GraphSAGE的核心是`node_embeddings`矩阵。在`setup`方法中，这个矩阵被初始化为一个可学习的参数，使用`nn.Embed`，其形状为`[n_nodes,
    embedding_dim]`。这些嵌入使用`glorot_uniform`方法初始化，以促进稳定的训练动态。在训练过程中，这些嵌入通过聚合每个节点的邻居信息来更新，逐渐编码更高阶的结构模式。目标是让这些嵌入收敛到反映节点对之间连接可能性的表示。
- en: 'The main logic of the encoder is implemented in the `__call__` method. One
    important (though hard-coded) design choice here is the number of `SAGEConv` layers,
    which defines how many rounds of message passing are applied:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的主要逻辑在`__call__`方法中实现。这里的一个重要（尽管是硬编码的）设计选择是`SAGEConv`层的数量，它定义了应用多少轮消息传递：
- en: With one layer, each node aggregates information from its immediate neighbors.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一层中，每个节点从其直接邻居那里聚合信息。
- en: With two layers, each node can access information from neighbors up to two hops
    away.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在两层中，每个节点可以访问到两跳远的邻居信息。
- en: Thus, the number of layers controls the receptive field of each node. Between
    layers, ReLU activation introduces nonlinearity, and dropout is applied for regularization
    to prevent overfitting.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，层的数量控制了每个节点的感受野。层之间，ReLU激活引入了非线性，并且应用dropout进行正则化以防止过拟合。
- en: Graph Convolution
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图卷积
- en: 'We’ve now reached the core architectural component of our model: the `SAGEConv`
    layer. Let’s dive into it:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经到达了我们模型的核心架构组件：`SAGEConv`层。让我们深入探讨它：
- en: '[PRE35]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: When setting up a `SAGEConv` layer, we specify the embedding dimension (using
    `embedding_dim`), whether to add self-loops (`with_self`), and whether to apply
    degree normalization (`degree_norm`). The latter two options are optional, as
    their impact on model performance depends on the dataset’s characteristics, such
    as size and connectivity patterns. Enabling or disabling these features can significantly
    influence model behavior.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置`SAGEConv`层时，我们指定嵌入维度（使用`embedding_dim`），是否添加自环（`with_self`），以及是否应用度数归一化（`degree_norm`）。后两个选项是可选的，因为它们对模型性能的影响取决于数据集的特征，如大小和连接模式。启用或禁用这些功能可以显著影响模型行为。
- en: 'Each `SAGEConv` layer performs the following key steps:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`SAGEConv`层执行以下关键步骤：
- en: Optionally adds self-edges
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地添加自环
- en: Allows each node to consider its own embedding during aggregation
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 允许每个节点在聚合时考虑其自身的嵌入
- en: Aggregates neighborhood embeddings
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合邻域嵌入
- en: Collects and averages the embeddings of each node’s neighbors
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 收集并平均每个节点邻居的嵌入
- en: Optionally normalizes by degree
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地按度数进行归一化
- en: Scales the contribution of neighbors based on their degree, reducing bias from
    highly connected nodes
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 根据邻居的度数缩放邻居的贡献，减少高度连接节点的偏差
- en: Combines embeddings with neighbors
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 将嵌入与邻居结合
- en: Merges the original node embeddings with the aggregated neighbor embeddings
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始节点嵌入与聚合的邻居嵌入合并
- en: Note
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'When reading the implementation of the `SAGEConv` convolutional layer, you
    might be wondering: *where’s the convolution?*'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当阅读`SAGEConv`卷积层的实现时，你可能想知道：*卷积在哪里？*
- en: Unlike convolutional neural networks (CNNs) for images or sequences, where `nn.Conv`
    applies spatial filters over regular grids, graph neural networks define “convolution”
    more abstractly. In GNNs, convolution means aggregating information from a node’s
    local neighborhood and combining it with its own embedding. This is implemented
    using operations like `segment_mean`, followed by a learnable transformation—typically
    via `nn.Dense`. So, while you won’t see an explicit `nn.Conv` in `SAGEConv`, the
    dense layer at the end serves as the core trainable part of the “graph convolution.”
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 与用于图像或序列的卷积神经网络（CNNs）不同，其中`nn.Conv`在规则网格上应用空间滤波器，图神经网络（GNNs）对“卷积”的定义更为抽象。在GNNs中，卷积意味着从节点的局部邻域聚合信息，并将其与自身的嵌入相结合。这通过使用如`segment_mean`之类的操作来实现，随后是一个可学习的转换——通常通过`nn.Dense`。因此，虽然你不会在`SAGEConv`中看到显式的`nn.Conv`，但最后的密集层充当了“图卷积”的核心可训练部分。
- en: It’s worth noting that `SAGEConv` focuses solely on aggregation and linear transformation.
    It does not include nonlinear activations, batch normalization, or dropout—those
    are typically applied at the model level to allow for greater flexibility and
    reuse.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，`SAGEConv`仅关注聚合和线性转换。它不包括非线性激活、批量归一化或dropout——这些通常在模型级别应用，以允许更大的灵活性和重用。
- en: Adding self edges
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加自环
- en: In the `NodeEncoder`, we use two `SAGEConv` layers, and the first layer has
    the `with_self` parameter set to `True`. Adding self-loops ensures that a node’s
    own embedding is included in the aggregation process during neighborhood updates
    (by including the node in its own list of senders). Without self-loops, a node’s
    updated embedding would reflect only information from its neighbors, potentially
    biasing the representation away from its original identity. Including self-loops
    allows each node to contribute to its own update, balancing its existing features
    with neighborhood context.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在`NodeEncoder`中，我们使用两个`SAGEConv`层，第一层的`with_self`参数设置为`True`。添加自环确保在邻域更新过程中（通过将节点包含在其自己的发送者列表中）包含节点自身的嵌入。没有自环，节点的更新嵌入将仅反映其邻居的信息，可能会使表示偏离其原始身份。包括自环允许每个节点对其自身的更新做出贡献，平衡其现有特征与邻域上下文。
- en: This can be accomplished by adding ones to the diagonal of the adjacency matrix,
    effectively connecting each node to itself. In our implementation, the `with_self`
    and `last_layer_self` parameters control whether self-loops are included in the
    first and second `SAGEConv` layers, respectively.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过向邻接矩阵的对角线添加1来实现，有效地将每个节点连接到自身。在我们的实现中，`with_self`和`last_layer_self`参数分别控制是否在第一和第二`SAGEConv`层中包含自环。
- en: Note
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Even without explicit self-loops, models can preserve a node’s identity by using
    skip connections. For example, in our implementation, we concatenate the original
    and updated embeddings using `combined_embeddings = jnp.concatenate([x, x_updated],
    axis=-1)`. This helps retain the node’s original features. However, it differs
    from self-loops in that the original embedding is added after the neighborhood
    aggregation, not as part of it. Including self-loops ensures that the node’s identity
    contributes directly to the aggregation step itself.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有显式的自环，模型也可以通过使用跳过连接来保留节点的身份。例如，在我们的实现中，我们使用`combined_embeddings = jnp.concatenate([x,
    x_updated], axis=-1)`将原始和更新后的嵌入连接起来。这有助于保留节点的原始特征。然而，这与自环不同，原始嵌入是在邻域聚合之后添加的，而不是作为其一部分。包括自环确保节点的身份直接贡献于聚合步骤本身。
- en: Aggregating the neighborhood
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚合邻域
- en: 'We now turn to the core operation of message passing: aggregating information
    from each node’s neighborhood.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在转向消息传递的核心操作：从每个节点的邻域聚合信息。
- en: This is the first point where the graph structure is explicitly used. For each
    edge, the sender node’s embedding (or features) is gathered and aggregated on
    a per-receiver basis. In other words, each receiver node updates its embedding
    by combining information from all its connected senders. The result is a new representation
    that reflects the structure and features of its local neighborhood.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一次明确使用图结构。对于每条边，发送节点的嵌入（或特征）被收集并按接收者进行聚合。换句话说，每个接收节点通过结合所有连接的发送者的信息来更新其嵌入。结果是新的表示，反映了其局部邻域的结构和特征。
- en: In our implementation, we use `jraph.segment_mean` as the aggregation function,
    which computes the mean of sender embeddings for each receiver. Other common choices
    include sum, max, or even attention-weighted aggregation, as used in GAT-style
    models. The optimal aggregation method often depends on the graph’s topology and
    the downstream task, so experimenting with different strategies can be valuable.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们使用`jraph.segment_mean`作为聚合函数，它为每个接收者计算发送者嵌入的平均值。其他常见的选择包括求和、最大值，甚至像GAT风格模型中使用的注意力加权聚合。最佳的聚合方法通常取决于图的拓扑结构和下游任务，因此尝试不同的策略可能很有价值。
- en: Normalizing by degree
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过度数归一化
- en: Degree normalization ensures that all nodes, regardless of their connectivity,
    contribute more evenly during training. This can help stabilize optimization,
    avoiding exploding or vanishing gradients. However, excessive normalization may
    *over-smooth* node embeddings, potentially erasing finer details of a node’s local
    structure.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 度数归一化确保所有节点，无论其连接性如何，在训练期间都更均匀地贡献。这可以帮助稳定优化，避免梯度爆炸或消失。然而，过度归一化可能会*过度平滑*节点嵌入，可能抹去节点局部结构的细微细节。
- en: 'In our implementation, we apply *symmetric degree normalization*, which works
    as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们应用*对称度数归一化*，其工作原理如下：
- en: Messages are divided by the square root of the *sender’s degree* before aggregation.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在聚合之前，消息被除以*发送者的度数*的平方根。
- en: The aggregated result is then divided by the square root of the *receiver’s
    degree*.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后将聚合结果除以*接收者的度数*。
- en: This square-root scaling—common in various flavors of GNNs—balances message
    influence across nodes while avoiding unintended scaling effects.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这种平方根缩放——在各种GNN变体中都很常见——平衡了节点间的消息影响，同时避免了意外的缩放效应。
- en: Tip
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Whether degree normalization improves performance depends on the dataset and
    its structural properties. It’s especially helpful in graphs with a high degree
    of variability, where a few high-degree nodes might otherwise dominate message
    passing. It also improves stability in deeper GNNs by keeping signal magnitudes
    under control.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 度数归一化是否提高性能取决于数据集及其结构属性。它在具有高度可变性的图中特别有帮助，在这些图中，一些高度节点可能会在其他情况下主导消息传递。它还通过保持信号幅度在控制范围内，提高了深层GNN的稳定性。
- en: However, in graphs where raw connection strength or node centrality carries
    important meaning—such as physical interaction networks, citation graphs, or transportation
    systems—normalization may suppress informative signals. If time allows, it’s often
    worth comparing normalized and unnormalized variants during model development.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在原始连接强度或节点中心性具有重要意义的图中——例如物理交互网络、引用图或交通系统——归一化可能会抑制信息信号。如果时间允许，在模型开发过程中通常值得比较归一化和未归一化的变体。
- en: Combining embeddings with neighborhoods
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合嵌入与邻域
- en: After aggregation (and optional normalization), we combine the updated node
    embeddings with the original embeddings—typically by concatenation. This produces
    a unified representation that captures both the node’s initial features and the
    information gathered from its local neighborhood. This enrichment step ensures
    that embeddings reflect both individual identity and structural context.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚合（和可选的归一化）之后，我们将更新的节点嵌入与原始嵌入结合——通常是通过连接。这产生了一个统一表示，它捕捉了节点的初始特征以及从其局部邻域收集的信息。这一丰富步骤确保嵌入反映了个体身份和结构背景。
- en: 'Concatenating the original and aggregated embeddings doubles the feature dimensionality.
    To bring this back to the intended embedding size, the combined vector is passed
    through a fully connected `Dense` layer. This layer serves two key purposes:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始嵌入和聚合嵌入连接起来，将特征维度翻倍。为了将这个结果还原到预期的嵌入大小，将组合向量通过一个全连接的`Dense`层。这一层有两个关键作用：
- en: Maintains consistent dimensionality
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 保持一致的维度
- en: Projects the concatenated vector back to the original embedding size, ensuring
    compatibility with the next model layer
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 将连接的向量投影回原始嵌入大小，确保与下一个模型层的兼容性
- en: Learns better representations
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 学习更好的表示
- en: Learns how to optimally fuse original and neighborhood features, enabling the
    model to refine what information to retain or emphasize
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何最优地融合原始和邻域特征，使模型能够细化要保留或强调的信息
- en: Because the `Dense` transformation is learnable, the model adapts over training
    to make the most effective use of both types of input.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`Dense`转换是可学习的，模型在训练过程中会适应，以最有效地使用这两种类型的输入。
- en: Note
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: During model training, the graph structure remains fixed. Only the `node_embeddings`
    are updated, evolving over time as the model learns from the neighborhood aggregation
    and feature transformation process.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练过程中，图结构保持不变。只有 `node_embeddings` 被更新，随着时间的推移，随着模型从邻域聚合和特征转换过程中学习而演变。
- en: Link Prediction
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链接预测
- en: We now use the learned node embeddings to predict whether a given pair of nodes
    is connected. If the embeddings have effectively captured the graph’s structure,
    the model should be able to assign high scores to true edges and low scores to
    unrelated node pairs.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用学习到的节点嵌入来预测给定的节点对是否连接。如果嵌入有效地捕捉了图的结构，该模型应该能够为真实边分配高分数，为无关节点对分配低分数。
- en: 'Let’s look at how the embeddings are used in the `LinkPredictor` module:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看嵌入如何在 `LinkPredictor` 模块中使用：
- en: '[PRE36]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `LinkPredictor` takes a pair of node embeddings—one from the sender and
    one from the receiver—and estimates the likelihood of an edge between them. Here’s
    how it works:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '`LinkPredictor` 接收一对节点嵌入——一个来自发送者，一个来自接收者——并估计它们之间边的可能性。以下是其工作原理：'
- en: Combining embeddings
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入组合
- en: The sender and receiver embeddings are combined using element-wise multiplication.
    This operation captures the interaction between corresponding dimensions of each
    embedding, producing a fixed-size vector that reflects their pairwise compatibility.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 发送者和接收者的嵌入通过逐元素乘法结合。这个操作捕捉了每个嵌入对应维度的交互，产生一个固定大小的向量，反映了它们的成对兼容性。
- en: Transforming representations
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 表示转换
- en: The combined vector is passed through a multilayer perceptron (MLP), consisting
    of several `Dense` layers with ReLU activation and dropout. These layers are defined
    by the `n_layers` and `embedding_dim` parameters and serve to learn increasingly
    abstract representations of the node pair interaction.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 合并向量通过一个多层感知器（MLP），由几个带有 ReLU 激活和 dropout 的 `Dense` 层组成。这些层由 `n_layers` 和 `embedding_dim`
    参数定义，旨在学习节点对交互的越来越抽象的表示。
- en: Output layer
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层
- en: The final layer is a single-neuron `Dense` layer that outputs a logit—an unnormalized
    score representing the likelihood of an edge. During training, this logit is passed
    through a sigmoid to produce a probability between 0 and 1.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 最终层是一个单神经元 `Dense` 层，输出一个 logit——一个未归一化的分数，表示边的可能性。在训练过程中，这个 logit 通过 sigmoid
    函数转换，产生一个介于 0 和 1 之间的概率。
- en: The overall goal is for the model to learn to output high scores for true (positive)
    edges and low scores for negative edges, forming the basis for binary link prediction.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 总体目标是让模型学会输出高分数给真实（正）边，低分数给负边，这构成了二元链接预测的基础。
- en: Drug–Drug Interaction Model
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 药物-药物相互作用模型
- en: 'Let’s now put everything together into a `DdiModel` that we can train:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将所有内容组合成一个 `DdiModel`，我们可以对其进行训练：
- en: '[PRE37]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let’s walk through the main components of this code:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们浏览一下这段代码的主要组件：
- en: '`setup` method: Initializes two core submodules:'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setup` 方法：初始化两个核心子模块：'
- en: '`node_encoder`: Generates node embeddings from the input graph'
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node_encoder`：从输入图中生成节点嵌入'
- en: '`link_predictor`: Scores node pairs based on their embeddings to predict the
    presence or absence of an edge'
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`link_predictor`：根据节点嵌入对节点对进行评分，以预测边的存在或不存在'
- en: '`__call__` method: Defines the forward pass of the model and supports both
    training and inference:'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__call__` 方法：定义模型的正向传递并支持训练和推理：'
- en: The `node_encoder` computes embeddings `h` for all nodes in the graph. The use
    of `h` as a variable name follows a common convention, where `h` represents a
    “hidden state” or embedding.
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node_encoder` 计算图中所有节点的嵌入 `h`。将 `h` 作为变量名遵循一个常见的约定，其中 `h` 代表“隐藏状态”或嵌入。'
- en: 'If `is_pred=False` (training mode):'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `is_pred=False`（训练模式）：
- en: 'Positive pairs: The model passes sender and receiver embeddings through the
    `link_predictor` to estimate connection likelihood.'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正对：模型通过 `link_predictor` 传递发送者和接收者嵌入来估计连接可能性。
- en: 'Negative pairs: Similarly processed to estimate nonconnection scores.'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负对：类似地处理以估计非连接分数。
- en: Returns a dictionary of predicted scores for both “pos” and “neg” pairs.
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回一个包含预测分数的字典，针对“正”和“负”对。
- en: 'And if `is_pred=True` (inference mode): Takes an arbitrary array of node pairs
    and returns predicted scores. This enables applying the model to new or unseen
    node pairs after training.'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `is_pred=True`（推理模式）：接受一个任意的节点对数组，并返回预测分数。这允许在训练后应用模型到新的或未见的节点对。
- en: '`create_train_state` method: Sets up the training process:'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_train_state` 方法：设置训练过程：'
- en: Initializes model parameters using dummy inputs and a random seed
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用虚拟输入和随机种子初始化模型参数
- en: Constructs a `TrainState` object with the model’s `apply_fn`, parameters, optimizer
    (`tx`), and dropout key for training
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型的`apply_fn`、参数、优化器（`tx`）和训练的dropout键构建`TrainState`对象
- en: '`add_mean_embedding` static method: Appends a global mean embedding to the
    existing embedding matrix. This can be useful in downstream tasks where a graph-level
    summary representation is desired.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_mean_embedding`静态方法：将全局平均嵌入追加到现有的嵌入矩阵中。这在需要图级别摘要表示的下游任务中可能很有用。'
- en: Together, these components define a full link prediction pipeline for drug–drug
    interaction graphs. Next, we’ll look at how to train this model end to end.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件共同定义了一个药物-药物相互作用图的完整链接预测管道。接下来，我们将探讨如何从头到尾训练此模型。
- en: Training the Model
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: From the previous sections, we have established how to prepare datasets and
    define a model. Now we’ll proceed by creating instances of both before moving
    forward with training.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的章节中，我们已经确定了如何准备数据集和定义模型。现在，在继续训练之前，我们将创建这两个实例。
- en: Create a Manageable Dataset
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建可管理的数据集
- en: 'We will create a subset containing approximately 10% of the total graph data:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个包含大约10%总图数据的子集：
- en: '[PRE38]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: By reducing the dataset size, we create a graph that is easier to handle during
    initial experimentation. This smaller graph allows us to test the model architecture
    and training setup more efficiently.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少数据集大小，我们创建了一个在初始实验中更容易处理的图。这个较小的图使我们能够更有效地测试模型架构和训练设置。
- en: We now have a graph with a set of positive and negative node pairs that we can
    learn from. The visualization in [Figure 4-8](#ddi-training-subgraph) provides
    a high-level view of the training dataset’s structure, where nodes represent drugs
    and edges represent interactions between them. The circular layout arranges all
    nodes around a circle, with edges connecting related nodes.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含一组正负节点对的图，我们可以从中学习。图[图4-8](#ddi-training-subgraph)中的可视化提供了训练数据集结构的概述，其中节点代表药物，边代表它们之间的相互作用。圆形布局将所有节点围绕一个圆排列，相关节点通过边连接。
- en: '![](assets/dlfb_0408.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0408.png)'
- en: Figure 4-8\. Circular layout of training dataset of 500 nodes. Each node represents
    a drug, and edges represent interactions between them.
  id: totrans-337
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-8\. 500个节点的训练数据集的圆形布局。每个节点代表一种药物，边代表它们之间的相互作用。
- en: While the individual node labels and details may not be legible in this plot,
    it offers a broad overview of key graph properties. These include the density
    of connections, overall sparsity, and presence of clusters or isolated nodes.
    This visualization illustrates the graph’s complexity, despite being a small subsampled
    set.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在此图中单个节点标签和细节可能不够清晰，但它提供了一个关键图属性的广泛概述。这包括连接密度、整体稀疏性和集群或孤立节点的存在。这种可视化说明了尽管是小的子采样集，但图的复杂性。
- en: '[PRE39]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Create the Training Loop
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练循环
- en: 'Next, let’s examine the training loop:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检查训练循环：
- en: '[PRE40]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `train` function coordinates the entire training process, broken down into
    several key stages:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '`train`函数协调整个训练过程，分为几个关键阶段：'
- en: '*Initialization*'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*初始化*'
- en: Initializes a `MetricsLogger` to track training and evaluation metrics
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化一个`MetricsLogger`以跟踪训练和评估指标
- en: Estimates an optimal batch size using the `optimal_batch_size` function
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`optimal_batch_size`函数估计最佳批次大小
- en: Displays training progress using a `tqdm` progress bar
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`tqdm`进度条显示训练进度
- en: '*Training over epochs*'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在多个epoch中进行训练*'
- en: 'For each epoch:'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个epoch：
- en: Sets up random number generators (`rng`) for shuffling and sampling training
    pairs.
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置随机数生成器（`rng`）以进行洗牌和采样训练对
- en: The training set is iterated over in batches using `get_train_batches`, which
    samples positive and negative node pairs.
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`get_train_batches`以批次迭代训练集，该函数采样正负节点对。
- en: Each batch is passed to `train_step`, which updates model parameters based on
    the current loss.
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个批次都传递给`train_step`，该步骤根据当前损失更新模型参数。
- en: Training metrics (e.g., loss and hits@20) are logged after each batch via `metrics.log_step`.
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个批次后通过`metrics.log_step`记录训练指标（例如，损失和hits@20）。
- en: '*Evaluation*'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*评估*'
- en: At intervals defined by `eval_every`, the model is evaluated on the validation
    set using `get_eval_batches` and `eval_step`.
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`eval_every`定义的间隔内，使用`get_eval_batches`和`eval_step`在验证集上评估模型。
- en: Evaluation metrics are logged via `metrics.log_step`, and summary statistics
    are printed using `metrics.latest()`.
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`metrics.log_step`记录评估指标，并使用`metrics.latest()`打印摘要统计信息。
- en: 'Additional features of this training loop include:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 此训练循环的附加功能包括：
- en: Support for custom loss functions
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 支持自定义损失函数
- en: A user-defined `loss_fn` can be passed in to control the optimization objective.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 可以传递用户定义的`loss_fn`来控制优化目标。
- en: Optional loss normalization
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的损失归一化
- en: Controlled by the `norm_loss` flag. When `True`, the loss is averaged over examples
    to ensure scale invariance across batch sizes.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 由`norm_loss`标志控制。当`True`时，损失在示例之间平均，以确保不同批量大小的尺度不变性。
- en: Restorable training state
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 可恢复的训练状态
- en: The function is decorated with `@restorable`, enabling checkpointing and resumption
    of training mid-run.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 函数用`@restorable`装饰，启用检查点和训练过程中的恢复。
- en: The dataset is processed in batches using the `Pairs` class, which handles consistent
    sampling of node pairs for both training and evaluation. We’ll take a closer look
    at how this class works next.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Pairs`类对数据集进行批处理，该类处理训练和评估中节点对的持续采样。我们将在下一节更详细地了解这个类的工作原理。
- en: Create the Pairs Class
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建`Pairs`类
- en: 'The `Pairs` class is a utility that simplifies the handling of positive and
    negative node pairs during training and evaluation. You’ve already seen it in
    action within the training loop. Here, we’ll break down its functionality more
    explicitly:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pairs`类是一个实用工具，简化了在训练和评估过程中处理正负节点对的操作。您已经在训练循环中看到了它的实际应用。在这里，我们将更明确地分解其功能：'
- en: '[PRE41]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The class provides several key methods for batching and sampling:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 该类提供了几个关键的批处理和采样方法：
- en: '`get_eval_batches`'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_eval_batches`'
- en: Returns evaluation batches of positive and negative pairs, ensuring shape alignment
    and balanced sizes. It slices the `pos` and `neg` arrays using the same indices,
    up to the size of the smaller set.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 返回正负对的评估批次，确保形状对齐和大小平衡。它使用相同的索引切片`pos`和`neg`数组，直到较小集合的大小。
- en: '`get_train_batches`'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_train_batches`'
- en: Returns shuffled training batches. Positive pairs are shuffled using `rng_shuffle`,
    and fresh negative pairs are sampled using `_global_negative_sampling`. This introduces
    variation between epochs and improves generalization.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 返回打乱顺序的训练批次。正对使用`rng_shuffle`打乱，新的负对使用`_global_negative_sampling`进行采样。这引入了epoch之间的变化，并提高了泛化能力。
- en: '`get_dummy_input`'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_dummy_input`'
- en: Returns a very small batch (two positive and two negative pairs). This is useful
    to get the correct shape of data that we can use to initialize the model parameters.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个非常小的批次（两个正对和两个负对）。这对于获取我们可以用来初始化模型参数的正确数据形状非常有用。
- en: Together, these methods enable consistent and efficient batch generation for
    both training and evaluation, while introducing enough variability to improve
    learning.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法共同实现了训练和评估的持续和高效批生成，同时引入了足够的变异性以改善学习。
- en: Batching by pairs
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 按对进行批处理
- en: 'During each epoch, we must process a large number of positive and negative
    node pairs. As we’ve seen in the introduction to graph convolution layers, calculating
    node embeddings for large networks can become computationally expensive. To address
    this, we use batching—processing subsets of the data one at a time. This strategy
    is applied to both training and evaluation data, with some key differences:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个epoch中，我们必须处理大量的正负节点对。正如我们在图卷积层介绍中看到的，为大型网络计算节点嵌入可能会变得计算成本高昂。为了解决这个问题，我们使用批处理——一次处理数据的一个子集。这种策略应用于训练和评估数据，有一些关键的区别：
- en: Training batches
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练批次
- en: The `get_train_batches` method provides batches of positive and negative pairs,
    shuffling the data at the start of every epoch to introduce diversity in the order
    of pairs.
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_train_batches`方法提供正负对的批次，在每个epoch开始时对数据进行打乱，以引入对顺序的多样性。'
- en: Negative pairs are resampled once per epoch using the `_global_negative_sampling`
    method. This variation helps the model learn from a more diverse set of examples.
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`_global_negative_sampling`方法在每个epoch中重新采样负对。这种变化有助于模型从更多样化的示例集中学习。
- en: Evaluation batches
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估批次
- en: The `get_eval_batches` method returns batches of positive and negative pairs
    according to the specified `batch_size`.
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_eval_batches`方法根据指定的`batch_size`返回正负对的批次。'
- en: To ensure compatibility, the `_n_pairs` method limits the batch size to the
    smaller of the positive or negative sets, so the two arrays match in shape.
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了确保兼容性，`_n_pairs`方法将批大小限制为正负集合中较小的一个，以便两个数组在形状上匹配。
- en: Evaluation batches are processed in a fixed order for reproducibility and consistent
    metrics across runs. This deterministic behavior simplifies debugging and ensures
    that order-sensitive metrics like Hits@K and MRR are stable.
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估批次以固定顺序处理，以确保可重复性和运行之间的指标一致性。这种确定性行为简化了调试，并确保像Hits@K和MRR这样的顺序敏感指标是稳定的。
- en: To maintain uniform batch sizes, the `Pairs` class drops the final incomplete
    batch during both training and evaluation. This avoids irregularities in computation.
    Over multiple epochs, shuffling ensures that all data points are eventually seen,
    even if some are skipped in a single run.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持均匀的批量大小，`Pairs`类在训练和评估期间都会丢弃最后的未完成批次。这避免了计算中的不规则性。在多个epoch中，洗牌确保所有数据点最终都会被看到，即使在某些运行中有些被跳过。
- en: 'To maximize efficiency, we use the `optimal_batch_size` utility function:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化效率，我们使用`optimal_batch_size`实用函数：
- en: '[PRE42]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This function computes the largest batch size that minimizes dropped data and
    ensures consistency across training and evaluation. It balances computational
    efficiency and data utilization by selecting sizes that are both large and compatible
    with the dataset structure.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数计算最小化丢失数据并确保训练和评估之间一致性的最大批量大小。它通过选择既大又与数据集结构兼容的大小来平衡计算效率和数据利用率。
- en: Consistent batch sizes are critical for optimizing jitted functions, which rely
    on static input shapes. They prevent unnecessary recompilation, improve memory
    and compute efficiency on accelerators like GPUs and TPUs, and reduce the complexity
    of handling irregular input.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 一致的批量大小对于优化静态输入形状依赖的即时函数至关重要。它们防止不必要的重新编译，提高在GPU和TPU等加速器上的内存和计算效率，并减少处理不规则输入的复杂性。
- en: Sampling negative pairs
  id: totrans-390
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 采样负对
- en: An important aspect of training is how we sample negative pairs. Since there
    are many more pairs without connections than with connections, we cannot use all
    negative examples; doing so would create a highly imbalanced training dataset.
    Instead, we select a subset of negative pairs to balance the dataset. This is
    where `_global_negative_sampling` comes in.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的一个重要方面是我们如何采样负对。由于没有连接的对比有连接的对多得多，我们不能使用所有负例；这样做会创建一个高度不平衡的训练数据集。相反，我们选择负对的一个子集来平衡数据集。这就是`_global_negative_sampling`发挥作用的地方。
- en: 'The subset of negative samples can significantly impact training. In this implementation,
    we use the simplest approach: *global sampling*, where we uniformly sample from
    all possible negative pairs. This strategy is suitable when we are broadly interested
    in potential node connections across the entire graph:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 负样本子集可以显著影响训练。在此实现中，我们使用最简单的方法：*全局采样*，其中我们从所有可能的负对中均匀采样。这种策略在我们在整个图中广泛关注潜在节点连接时是合适的：
- en: '[PRE43]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'While global sampling is straightforward and effective, many alternative strategies
    exist that drive the model to learn different patterns. For example:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然全局采样简单且有效，但存在许多替代策略，可以驱动模型学习不同的模式。例如：
- en: Local sampling
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 本地采样
- en: Ensures that negative pairs share at least one sender node, focusing on pairs
    that are structurally similar to positive pairs. This can help the model learn
    more fine-grained distinctions.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 确保负对至少共享一个发送节点，专注于与正对在结构上相似的配对。这可以帮助模型学习更精细的区分。
- en: Hard negative sampling
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 硬负采样
- en: Selects negative pairs that the model struggles to classify as negatives (i.e.,
    pairs with a high predicted likelihood of being connected, even though they are
    not). This approach forces the model to improve on challenging cases and can accelerate
    learning.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 选择模型难以将其分类为负例的负对（即，即使它们不相连，预测概率也很高的对）。这种方法迫使模型在具有挑战性的案例上改进，并可以加速学习。
- en: Adversarial negative sampling
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性负采样
- en: Generates challenging negative pairs using an adversarial approach, where a
    secondary model selects negatives that maximize the main model’s loss. While computationally
    expensive, it can lead to robust embeddings and improved performance.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对抗性方法生成具有挑战性的负对，其中次要模型选择最大化主模型损失的负例。虽然计算成本高昂，但它可以导致鲁棒的嵌入和改进的性能。
- en: Ratio of positive to negative pairs
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 正负对比例
- en: Balances the number of positive and negative pairs in the dataset. While a 1:1
    ratio is common, some tasks may benefit from a higher ratio of negatives (e.g.,
    1:5). In our DDI problem, we explored varying the ratio, but it did not significantly
    impact performance (not shown) and introduced unnecessary complexity.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡数据集中正样本和负样本对的数量。虽然1:1的比例很常见，但某些任务可能从更高的负样本比例中受益（例如，1:5）。在我们的DDI问题中，我们探索了改变比例，但它并没有显著影响性能（未显示）并引入了不必要的复杂性。
- en: Create the Train Step Function
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练步骤函数
- en: The `train_step` function is where learning actually happens during training.
    It performs a forward pass, computes the loss, and applies gradients to update
    the model’s parameters. This function is applied to batches of node pairs throughout
    each epoch.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_step` 函数是训练过程中实际发生学习的地方。它执行前向传递，计算损失，并将梯度应用于更新模型的参数。此函数在整个每个epoch中对节点对批次进行应用。'
- en: '[PRE44]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: You’ll notice that the function is decorated using `@partial(jax.jit, static_argnames=["loss_fn",
    "norm_loss"])`. This pattern allows these arguments—the loss function (`loss_fn`)
    and whether to normalize the loss (`norm_loss`)—to be treated as static during
    JAX’s just-in-time (JIT) compilation. By marking these arguments as static, JAX
    avoids recompiling the function every time they change.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到该函数使用 `@partial(jax.jit, static_argnames=["loss_fn", "norm_loss"])` 装饰。这种模式允许这些参数——损失函数（`loss_fn`）以及是否标准化损失（`norm_loss`）——在JAX的即时（JIT）编译期间被视为静态。通过将这些参数标记为静态，JAX避免了每次它们更改时重新编译函数。
- en: 'Inside the function, a nested `calculate_loss` method defines the core of the
    computation:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数内部，嵌套的 `calculate_loss` 方法定义了计算的核心：
- en: The model is applied to the current batch using `state.apply_fn(...)`, producing
    predictions (`scores`) for positive and negative node pairs.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `state.apply_fn(...)` 将模型应用于当前批次，为正节点对和负节点对生成预测（分数）。
- en: The specified `loss_fn` is used to compute the training loss.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定的 `loss_fn` 用于计算训练损失。
- en: The `evaluate_hits_at_20` function computes the `hits@20` metric to track how
    well the model ranks correct node pairs near the top.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluate_hits_at_20` 函数计算 `hits@20` 指标，以跟踪模型在顶部正确节点对排名方面的表现。'
- en: JAX’s `value_and_grad` is then used to calculate both the loss and its gradient
    with respect to the model parameters. These gradients are applied to the training
    state using `state.apply_gradients`.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用 JAX 的 `value_and_grad` 计算损失及其相对于模型参数的梯度。这些梯度通过 `state.apply_gradients`
    应用到训练状态。
- en: If `norm_loss=True`, the total loss is divided by the number of training pairs
    in the batch (positive + negative), ensuring that loss magnitudes remain comparable
    across different batch sizes or sampling ratios.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `norm_loss=True`，则将总损失除以批次中训练对的数量（正样本+负样本），以确保在不同批次大小或采样比率下损失幅度保持可比性。
- en: 'The default loss function used in training is the binary log loss function:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 训练中使用的默认损失函数是二元对数损失函数：
- en: '[PRE45]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This loss function performs the following steps:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 此损失函数执行以下步骤：
- en: Sigmoid transformation
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 变换
- en: Converts the raw logits (unbounded scores) into probabilities in the range (0,
    1). These probabilities represent the model’s confidence that a given node pair
    is a positive class (i.e., a true drug–drug interaction).
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始的logits（无界分数）转换为范围在（0，1）之间的概率。这些概率代表模型对给定节点对是正类（即真正的药物-药物相互作用）的置信度。
- en: Clipping
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 剪裁
- en: Ensures numerical stability by constraining probabilities to lie slightly within
    (0, 1). This avoids issues such as `log(0)`, which can cause the loss to diverge
    or return `NaN` during training.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将概率约束在（0，1）略微范围内，确保数值稳定性。这避免了诸如 `log(0)` 的问题，这可能导致损失发散或在训练期间返回 `NaN`。
- en: Loss calculation
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 损失计算
- en: Ensures that positive pairs are penalized when their predicted probabilities
    are far from 1, and negative pairs are penalized when their predicted probabilities
    are far from 0\. The total loss is computed as the average of the positive and
    negative log losses.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 确保当正样本对的预测概率远离1时，它们会受到惩罚，而当负样本对的预测概率远离0时，它们也会受到惩罚。总损失是正样本和负样本对数损失的平均值。
- en: This loss provides a simple yet effective objective for training the model to
    distinguish between interacting and noninteracting drug pairs.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 这种损失为训练模型区分相互作用和非相互作用药物对提供了一个简单而有效的目标。
- en: Create the Evaluation Metric
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建评估指标
- en: 'Finally, we want to evaluate the model’s performance. This follows a similar
    approach to `train_step`:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想要评估模型的表现。这遵循与 `train_step` 类似的方法：
- en: '[PRE46]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The notable differences in `eval_step` are:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '`eval_step` 中的显著差异是：'
- en: No training mode
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 没有训练模式
- en: The `is_training` flag is set to `False`, which disables behaviors like dropout
    and ensures that the model is evaluated deterministically.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 将`is_training`标志设置为`False`，这将禁用诸如dropout之类的行为，并确保模型评估是确定的。
- en: Evaluation metric
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标
- en: Rather than just returning the loss, the function also computes `hits@20`, a
    ranking-based metric commonly used in link prediction tasks.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数不仅返回损失，还计算`hits@20`，这是一种基于排名的指标，常用于链接预测任务中。
- en: Hits@20 evaluates how well the model ranks positive node pairs compared to negative
    ones, giving an intuitive signal of ranking quality. Specifically, it identifies
    the 20th highest score among the negative pairs as a threshold and calculates
    the proportion of positive scores that exceed this threshold; then it calculates
    the proportion of positive scores that exceeds this threshold. A higher Hits@20
    indicates that the model correctly ranks many true interactions above even one
    of the most confident false ones.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: Hits@20评估模型在正节点对和负节点对之间的排名效果，为排名质量提供了一个直观的信号。具体来说，它将负对中得分最高的20个作为阈值，并计算超过此阈值的正得分比例；然后计算超过此阈值的正得分比例。更高的Hits@20表明，模型正确地将许多真实交互排在甚至最自信的假交互之上。
- en: 'Here’s the implementation:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是它的实现方式：
- en: '[PRE47]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Note
  id: totrans-434
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We could have imported the `ogb.linkproppred.Evaluator` from the Open Graph
    Benchmark (OGB) library, which computes Hits@20\. However, by directly implementing
    the metric, we make the evaluation process more transparent and tailored to our
    specific use case. This approach provides greater flexibility for modifications
    and extensions while clearly showing how the model is evaluated.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以从Open Graph Benchmark (OGB)库中导入`ogb.linkproppred.Evaluator`，该库计算Hits@20。然而，通过直接实现该指标，我们使评估过程更加透明，并针对我们的特定用例进行了定制。这种方法提供了更大的灵活性，以便进行修改和扩展，同时清楚地显示了模型是如何被评估的。
- en: Train the Simplest Model
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练最简单的模型
- en: 'We are finally ready to train the model. We’ll start with a relatively simple
    architecture and monitor its performance:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好训练模型了。我们将从一个相对简单的架构开始，并监控其性能：
- en: '[PRE48]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The learning curves in [Figure 4-9](#ddi-simplest-model) show the training process
    over 500 epochs. The training loss decreases steadily and the Hits@20 metric improves
    on both training and validation sets. However, the growing divergence between
    the training and validation curves—particularly visible in the later epochs—suggests
    moderate overfitting. This indicates that while the model is capturing meaningful
    patterns, its generalization to unseen data could be improved.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-9](#ddi-simplest-model)中的学习曲线显示了500个epoch的训练过程。训练损失稳步下降，且在训练集和验证集上，Hits@20指标都有所提升。然而，训练曲线和验证曲线之间的逐渐增大差异——尤其是在后期epoch中尤为明显——表明存在适度的过拟合。这表明，虽然模型捕捉到了有意义的模式，但其对未见数据的泛化能力可能需要提高。'
- en: '[PRE49]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](assets/dlfb_0409.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0409.png)'
- en: Figure 4-9\. Learning curves showing the training loss (left) and Hits@20 metric
    (right) over 500 epochs. While both metrics improve on the training set, a growing
    gap between training and validation suggests moderate overfitting.
  id: totrans-442
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9。显示500个epoch内训练损失（左侧）和Hits@20指标（右侧）的学习曲线。虽然这两个指标在训练集上都有所提升，但训练集和验证集之间不断扩大的差距表明存在适度的过拟合。
- en: 'A word of warning: two sources of variance can lead to surprisingly different
    metrics curves in this plot—even with the same model and training setup.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一句：在这个图表中，两种方差来源可能导致指标曲线出现惊人的不同——即使使用相同的模型和训练设置。
- en: Stochastic node sampling
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 随机节点采样
- en: Graph neural networks that use neighborhood sampling can be highly sensitive
    to the random seed. In drug–drug interaction prediction, some nodes are connected
    to many easily predictable links, while others are not. Which nodes get included
    in your training data can have a major impact on the learned representations,
    loss, and evaluation metrics.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 使用邻域采样的图神经网络对随机种子非常敏感。在药物-药物相互作用预测中，一些节点连接到许多容易预测的链接，而另一些则没有。你训练数据中包含哪些节点可能会对学习到的表示、损失和评估指标产生重大影响。
- en: Discontinuous metrics
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 不连续的指标
- en: 'Hits@20 is a ranking-based metric that compares each positive score to the
    20th highest negative score. This thresholding introduces discontinuity: a small
    change in any score near the threshold—especially among the top-ranked negatives—can
    flip the outcome for many positives from success to failure or vice versa. This
    makes Hits@20 unusually sensitive to minor score shifts, even when the model or
    loss appears unchanged. This doesn’t mean it’s a bad metric—just something to
    be aware of.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: Hits@20是一个基于排名的指标，它将每个正评分与第20个最高的负评分进行比较。这种阈值引入了不连续性：任何接近阈值的评分的微小变化——尤其是在排名靠前的负评分中——都可能将许多正评分的结果从成功变为失败或反之亦然。这使得Hits@20对评分的微小变化特别敏感，即使模型或损失似乎没有变化。这并不意味着它是一个不好的指标——只是需要注意的一点。
- en: To reduce this variability, we can increase the number of nodes we include in
    the dataset (as shown in [“Train on a Larger Dataset”](#training-on-a-larger-dataset)).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少这种可变性，我们可以增加数据集中包含的节点数量（如[“在更大的数据集上训练”](#training-on-a-larger-dataset)所示）。
- en: Now that we’ve trained a first working model, we’ll explore strategies to reduce
    overfitting and improve overall performance.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了一个初步工作的模型，我们将探讨减少过拟合并提高整体性能的策略。
- en: Improving the Model
  id: totrans-450
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进模型
- en: Everything is working end-to-end—we have prepared a dataset, built a model,
    and trained it. However, the model’s performance is suboptimal. In this section,
    we’ll explore some tweaks to see if we can achieve better results.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 整个流程都是端到端工作的——我们已经准备了一个数据集，构建了一个模型，并对其进行了训练。然而，该模型的表现并不理想。在本节中，我们将探讨一些调整方法，看看我们是否能取得更好的结果。
- en: Change to AUC Loss
  id: totrans-452
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换为AUC损失
- en: So far, we’ve used binary log loss to train our model. However, for our task,
    the primary goal is to ensure that positive pairs are ranked higher than negative
    pairs. While probabilities can also be used to prioritize, they often saturate
    near 1 or 0 for confident predictions, making it harder to differentiate between
    highly ranked pairs. In contrast, ranking-based metrics focus on the relative
    ordering of scores, which better aligns with the task of identifying and prioritizing
    the most promising drug interactions. This is particularly valuable in DDI prediction,
    where the goal is often to focus on the top-scoring pairs for further investigation.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用二元对数损失来训练我们的模型。然而，对于我们的任务，主要目标是确保正对被排名高于负对。虽然概率也可以用来优先排序，但它们在自信预测时通常接近1或0，这使得区分高度排名的对更加困难。相比之下，基于排名的指标关注评分的相对顺序，这更好地符合识别和优先考虑最有希望的药物相互作用的任务。这在DDI预测中尤其有价值，其目标通常是关注得分最高的对以进行进一步研究。
- en: Inspired by a paper on pairwise learning for neural link prediction (PLNLP),^([7](ch04.html#id808))
    which outlines key stages of a link prediction pipeline, we will swap the loss
    function to better align with our objective. Instead of focusing on binary classifications,
    we adopt a ranking-based approach that encourages the model to score connected
    pairs higher than unconnected ones, aligning conceptually with the area under
    the curve (AUC) metric.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 受一篇关于神经网络链接预测成对学习的论文（PLNLP）的启发，该论文概述了链接预测管道的关键阶段，我们将交换损失函数以更好地符合我们的目标。我们不再关注二元分类，而是采用基于排名的方法，鼓励模型将连接对评分高于未连接对，从概念上与曲线下面积（AUC）指标相一致。
- en: AUC measures the probability that a randomly chosen positive instance (connected
    node pair) has a higher score than a randomly chosen negative instance (nonconnected
    pair). While directly optimizing AUC would be ideal, it is computationally challenging
    because its gradients are often undefined or zero. To address this, we use a *surrogate
    loss function* that mimics AUC’s properties while remaining easy to optimize.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: AUC衡量随机选择的正实例（连接节点对）的评分高于随机选择的负实例（非连接对）的概率。虽然直接优化AUC是理想的，但由于其梯度通常未定义或为零，因此计算上具有挑战性。为了解决这个问题，我们使用一个*代理损失函数*，该函数模仿AUC的特性，同时易于优化。
- en: 'A simple and effective surrogate is the squared loss, which penalizes deviations
    from the target score difference of 1 between positive and negative pairs. This
    means the model is penalized both when the difference is less than 1 (underestimation)
    and when it is greater than 1 (overestimation). By minimizing this penalty, the
    model learns to consistently assign higher scores to connected pairs while maintaining
    an appropriate margin over unconnected ones. Here’s the implementation:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单而有效的代理损失函数是平方损失，它惩罚正负对之间目标分数差异为1的偏差。这意味着当差异小于1（低估）或大于1（高估）时，模型都会受到惩罚。通过最小化这种惩罚，模型学会始终对连接对分配更高的分数，同时保持对未连接对适当的间隔。以下是实现方式：
- en: '[PRE50]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This loss function encourages the model to score linked pairs higher than nonlinked
    pairs, improving its ranking performance. The motivation for discarding the cross-entropy
    loss is that the Hits@N metric, commonly used by Open Graph Benchmark (OGB) for
    evaluating link prediction benchmarks, does not measure the quality of predicted
    probabilities. Instead, it focuses solely on ensuring that true edges are ranked
    higher than false edges. Although the difference between these approaches is subtle,
    it has significant practical implications. Let’s explore the effect of the loss
    function switch on the model:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数鼓励模型对连接对评分高于非连接对，从而提高其排名性能。放弃交叉熵损失的原因是，Open Graph Benchmark (OGB)在评估链接预测基准时常用的Hits@N指标并不衡量预测概率的质量。相反，它仅关注确保真实边比虚假边排名更高。尽管这两种方法之间的差异很微妙，但它具有重大的实际意义。让我们探索损失函数切换对模型的影响：
- en: '[PRE51]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'From the learning curves in [Figure 4-10](#ddi-simplest-auc-model), we can
    see that changing the loss function to an AUC-based objective has led to better
    results. First, training is noticeably smoother: the training and validation losses
    fall in near-lockstep, with virtually no gap. Second, Hits@20 climbs more quickly
    and saturates at a higher level on the validation set than before, reflecting
    stronger generalization. In short, aligning the objective with our ranking metric
    directly translates into better and more reliable performance.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图4-10](#ddi-simplest-auc-model)的学习曲线中，我们可以看到将损失函数更改为基于AUC的目标函数已经导致了更好的结果。首先，训练过程明显更加平滑：训练和验证损失几乎同步下降，几乎没有差距。其次，在验证集上，Hits@20的上升速度更快，并且达到更高的水平，这反映了更强的泛化能力。简而言之，将目标与我们的排名指标直接对齐可以直接转化为更好和更可靠的性能。
- en: '[PRE52]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](assets/dlfb_0410.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0410.png)'
- en: Figure 4-10\. Learning curves for loss (left) and Hits@20 (right) after replacing
    binary-cross-entropy with an AUC-optimizing loss. The new objective keeps train
    and validation losses tightly coupled and pushes the validation Hits@20 to higher,
    stabler values.
  id: totrans-463
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-10。用AUC优化损失替换二元交叉熵后的损失（左）和Hits@20（右）的学习曲线。新的目标使训练和验证损失紧密耦合，并将验证集的Hits@20推向更高、更稳定的值。
- en: It appears that this model is already fairly strong as measured by our choice
    of metric, but let’s see if we can push performance even higher and reduce overfitting
    further by exploring hyperparameter sweeps.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们选择的指标来看，这个模型似乎已经相当强大，但让我们看看是否可以通过探索超参数扫描来进一步提高性能并进一步减少过拟合。
- en: Set Model Sweeping and Training Parameters
  id: totrans-465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置模型扫描和训练参数
- en: Our model and training loop include several hyperparameters, and it’s not immediately
    clear which combinations will yield the best performance. One natural starting
    point is the embedding dimension, which directly controls the capacity of the
    model to richly represent graph structure.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式和训练循环包括几个超参数，并且不清楚哪些组合会产生最佳性能。一个自然的起点是嵌入维度，它直接控制模型丰富地表示图结构的能力。
- en: Varying embedding dimensions
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 改变嵌入维度
- en: 'We will vary the `embedding_dim` parameter and train new models for each value
    to evaluate its impact on performance. Since previous experiments showed improved
    results with longer training, we will also extend the number of epochs in this
    sweep:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将改变`embedding_dim`参数，并为每个值训练新的模型以评估其对性能的影响。由于之前的实验表明更长的训练时间会产生更好的结果，我们还将在这个扫描中增加epoch的数量：
- en: '[PRE53]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The following loop automates training across a range of `embedding_dim` values,
    storing the evaluation metrics for later analysis:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的循环自动在一系列`embedding_dim`值上执行训练，并将评估指标存储起来以供后续分析：
- en: '[PRE54]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'As shown in [Figure 4-11](#ddi-sweep-embedding-dim), there’s a clear relationship
    between embedding size and model performance: larger embedding vectors tend to
    yield better validation metrics. This is likely because higher-dimensional embeddings
    provide greater capacity to capture complex relationships and patterns in the
    graph:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 4-11](#ddi-sweep-embedding-dim) 所示，嵌入大小与模型性能之间存在明显的关系：较大的嵌入向量往往会产生更好的验证指标。这可能是由于高维嵌入提供了更大的能力来捕捉图中复杂的关系和模式：
- en: '[PRE55]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](assets/dlfb_0411.png)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0411.png)'
- en: Figure 4-11\. Maximum Hits@20 achieved by models with varying embedding dimensions,
    highlighting the impact of embedding size on performance.
  id: totrans-475
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-11\. 具有不同嵌入维度的模型实现的最大 Hits@20，突出了嵌入大小对性能的影响。
- en: The model with embedding dimensions set to 512 seem to achieve a perfect Hits@20
    of ~1 on the training set, suggesting that it has sufficient capacity to fully
    (and even overly) fit the training data.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 将嵌入维度设置为 512 的模型在训练集上似乎实现了完美的 Hits@20 大约 ~1，这表明它具有足够的容量来完全（甚至过度）拟合训练数据。
- en: Note
  id: totrans-477
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Increasing the embedding size and training the model for longer, as we have
    done in this section, comes with higher computational costs. Whether this trade-off
    is worthwhile depends on the resources available and the relative importance of
    improving model performance in your specific use case.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 增加嵌入大小并延长训练时间，如本节所述，会带来更高的计算成本。这种权衡是否值得取决于可用的资源以及提高模型性能在您特定用例中的相对重要性。
- en: Varying multiple hyperparameters
  id: totrans-479
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整多个超参数
- en: For this experiment, we broaden our search by sweeping over multiple model hyperparameters
    simultaneously. This builds on the previous experiment but explores a wider region
    of the hyperparameter space in the hopes of discovering better-performing configurations.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我们通过同时扫描多个模型超参数来扩大搜索范围。这基于之前的实验，但探索了超参数空间更广泛的区域，希望发现性能更好的配置。
- en: 'We will vary the following parameters:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将调整以下参数：
- en: 'Dropout rate: 0, 0.3, or 0.5'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout 率：0、0.3 或 0.5
- en: 'Self-edges in the last convolutional layer: Whether to include self-edges (`last_layer_self`:
    True or False)'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '最后卷积层中的自环：是否包含自环 (`last_layer_self`: True 或 False)'
- en: 'Degree normalization: Whether to normalize node embeddings by their degree
    (`degree_norm`: True or False)'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '度规范化：是否通过节点嵌入的度数进行规范化 (`degree_norm`: True 或 False)'
- en: 'Number of MLP layers in the link predictor: 1, 2, or 3 (`n_mlp_layers`)'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 链接预测器中的 MLP 层数数：1、2 或 3 (`n_mlp_layers`)
- en: For this experiment, we fix the embedding dimension at 512, as it was among
    the best-performing settings in the earlier sweep. While an embedding size of
    256 also performed reasonably well and would reduce computational cost, we opt
    for 512 in anticipation of scaling to larger graphs. A larger embedding size offers
    greater capacity to model complex relational patterns.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我们将嵌入维度固定在 512，因为它在早期的扫描中是表现最好的设置之一。虽然嵌入大小为 256 也表现相当好，并且可以降低计算成本，但我们选择
    512 以期待扩展到更大的图。更大的嵌入大小提供了更大的能力来模拟复杂的关系模式。
- en: 'Let’s set up the sweep:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置扫描：
- en: '[PRE56]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Output:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE57]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'To train models with each parameter combination, we use the following approach:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练具有每个参数组合的模型，我们采用以下方法：
- en: '[PRE58]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Similar to the earlier experiment, this loop automates the process of training
    models across all parameter combinations. Each model’s performance is evaluated,
    and metrics such as Hits@20 are recorded for later analysis.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期的实验类似，这个循环自动化了在所有参数组合上训练模型的过程。评估每个模型的性能，并记录如 Hits@20 等指标以供后续分析。
- en: We then calculate the maximum Hits@20 metric for each parameter combination
    and split. Additionally, we generate a more readable representation for the convolutional
    layer configurations used in the encoder. This systematic exploration helps us
    identify the most effective combinations of hyperparameters for optimizing the
    model.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算每个参数组合和分割的最大 Hits@20 指标，并生成用于编码器中卷积层配置的更易读的表示。这种系统性的探索帮助我们确定优化模型的最有效的超参数组合。
- en: 'We then extract the maximum Hits@20 metric for each parameter combination and
    split:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们提取每个参数组合和分割的最大 Hits@20 指标：
- en: '[PRE59]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Next, in [Figure 4-12](#ddi-sweep-all), we visualize an overview of model performance
    across different hyperparameter combinations. This plot contains a lot of information,
    so let’s first clarify how to interpret it. Each pair of points represents the
    training and validation Hits@20 scores for a single model configuration. Our goal
    is to identify configurations where validation performance is high and closely
    matches training performance—suggesting good generalization without overfitting.
    This is particularly important because the dataset is split by protein targets,
    meaning that the validation set contains drugs targeting different proteins than
    those seen during training.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在[图4-12](#ddi-sweep-all)中，我们可视化了模型在不同超参数组合下的性能概述。这个图表包含了很多信息，所以让我们首先明确如何解读它。每一对点代表单个模型配置的训练和验证Hits@20分数。我们的目标是识别那些验证性能高且与训练性能接近的配置——这表明模型具有良好的泛化能力且没有过拟合。这一点尤为重要，因为数据集是根据蛋白质靶点划分的，这意味着验证集包含的药物靶标与训练过程中看到的靶标不同。
- en: Tip
  id: totrans-498
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Models that show similar performance on both training and validation sets are
    more likely to generalize well. Prioritizing these configurations can help guide
    robust model selection.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和验证集上表现相似的模式更有可能具有良好的泛化能力。优先考虑这些配置可以帮助指导稳健的模型选择。
- en: '[PRE60]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![](assets/dlfb_0412.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0412.png)'
- en: Figure 4-12\. Maximum Hits@20 achieved by each configuration in our hyper-parameter
    grid. Rows correspond to dropout_rate (0.0, 0.3, 0.5); columns vary the number
    of MLP layers (1–3) in the link predictor. Each x-axis category toggles self-loops
    and degree normalization. The best generalization comes from the minimal model
    (far left, top row).
  id: totrans-502
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12\. 在我们的超参数网格中，每个配置达到的最大Hits@20分数。行对应于dropout_rate（0.0，0.3，0.5）；列变化了链接预测器中MLP层的数量（1-3）。每个x轴类别切换自环和度归一化。最佳泛化来自最小模型（最左侧，第一行）。
- en: 'There are many models in this plot, but one stands out in the upper-left corner:
    the best-performing model without dropout. It uses a simple convolutional setup
    (no self-edges, no normalization) and a single-layer link predictor. It’s notable
    that such a minimal configuration performs so well, suggesting that simpler models
    can still be highly effective.'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图表中有很多模型，但左上角的一个模型特别突出：没有dropout的最佳性能模型。它使用了一个简单的卷积设置（没有自环，没有归一化）和一个单层链接预测器。值得注意的是，这样一个最小配置表现如此之好，表明简单的模型仍然可以非常有效。
- en: 'Other observations from the plot:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中观察到的其他现象：
- en: Degree normalization hurts almost everywhere. Both “with norm” columns show
    a noticeable drop in validation performance relative to their “no norm” counterparts.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 度归一化几乎在所有地方都会造成损害。两个“带归一化”的列相对于它们的“无归一化”对应列，验证性能都有明显下降。
- en: Deeper MLP heads overfit more. Moving from one to three MLP layers widens the
    train–valid gap and very rarely yields a higher peak validation score.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度MLP头部更容易过拟合。从一层到三层的MLP层，训练-验证差距扩大，并且很少能产生更高的峰值验证分数。
- en: Dropout does not consistently help, though some models with high dropout perform
    well, suggesting it’s not strictly harmful either.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout并不总是有帮助，尽管一些具有高dropout的模型表现良好，这表明它也不是严格有害的。
- en: Self-edges make little difference. The two “with self-edges” categories track
    their “no self-edges” twins closely, suggesting that this graph already conveys
    enough reciprocal information. Overall, the task appears quite forgiving—many
    configurations exceed 0.8 on validation—but the simplest, lowest-capacity model
    remains the most reliable choice.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自环几乎没有影响。两个“带自环”的类别与它们的“无自环”双胞胎紧密跟踪，表明这个图已经传达了足够的互反信息。总体而言，任务似乎相当宽容——许多配置在验证中超过了0.8，但最简单、容量最低的模型仍然是最可靠的选择。
- en: Let’s now inspect the learning curves for the best-performing configuration,
    shown in [Figure 4-13](#ddi-best-model).
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查最佳性能配置的学习曲线，如图[图4-13](#ddi-best-model)所示。
- en: '[PRE61]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Recall that the previous metrics represented maximum values—meaning they may
    not have occurred at the same point in training and do not rule out eventual overfitting
    if training were to proceed. Plotting the full learning curves provides additional
    insight. In [Figure 4-13](#ddi-best-model), we see strong learning on the training
    set, but the configuration appears prone to overfitting without early stopping.
    This is evident from the widening gap between training and validation metrics
    over time.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，之前的指标代表的是最大值——这意味着它们可能不在训练的同一时间点发生，并且如果继续训练，不能排除最终过拟合的可能性。绘制完整的训练曲线提供了额外的见解。在[图4-13](#ddi-best-model)中，我们看到训练集上有强大的学习效果，但配置似乎容易过拟合，如果没有提前停止。这一点从训练和验证指标随时间拉大的差距中可以看出。
- en: '![](assets/dlfb_0413.png)'
  id: totrans-512
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0413.png)'
- en: Figure 4-13\. Learning curves for loss (left) and Hits@20 (right) of the best-performing
    configuration. The model achieves near-perfect training performance, but the validation
    metric plateaus and then declines, indicating overfitting.
  id: totrans-513
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-13. 最佳配置的损失（左）和Hits@20（右）的学习曲线。模型实现了接近完美的训练性能，但验证指标达到平台期后下降，表明过拟合。
- en: 'Let’s also examine the learning curves of an alternative high-performing model
    in [Figure 4-14](#ddi-alt-best-model). This configuration comes from a different
    region of the hyperparameter space: it includes more layers in the link predictor
    and applies a high dropout rate to encourage generalization. It also uses a different
    convolutional setup. Notably, the training and validation curves remain closer
    together, suggesting that this model learns a more generalizable representation
    and is less susceptible to overfitting:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看看[图4-14](#ddi-alt-best-model)中另一个高性能模型的训练曲线。此配置来自超参数空间的不同区域：它在链接预测器中包含更多层，并应用高dropout率以鼓励泛化。它还使用了不同的卷积设置。值得注意的是，训练和验证曲线更接近，这表明该模型学习到了更具泛化性的表示，并且不太容易过拟合：
- en: '[PRE62]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![](assets/dlfb_0414.png)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0414.png)'
- en: Figure 4-14\. Learning curves for loss (left) and Hits@20 (right) of an alternative
    high-performing model. This configuration demonstrates improved generalization,
    with train and validation metrics remaining stable and closely aligned.
  id: totrans-517
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-14. 另一个高性能模型的损失（左）和Hits@20（右）的学习曲线。此配置展示了改进的泛化能力，训练和验证指标保持稳定且紧密一致。
- en: While it’s tempting to focus only on the top headline metrics to identify the
    best-performing configuration, examining training dynamics can offer deeper insight
    into model performance. Based on both maximum metrics and learning curve behavior,
    we would recommend this latest model for its strong performance and stability.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然只关注顶级指标以识别最佳配置很有吸引力，但检查训练动态可以提供对模型性能的更深入见解。基于最大指标和学习曲线行为，我们推荐这个最新的模型，因为它性能强大且稳定。
- en: Note
  id: totrans-519
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are many additional hyperparameters that could be explored—including those
    already present in our model and training loop, as well as others not yet considered.
    For example, we could experiment with alternative negative sampling strategies,
    vary the ratio of negative to positive pairs, or increase the depth of the graph
    convolutional layers. However, for this smaller dataset, the current performance
    is strong enough that we consider further tuning unnecessary.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多其他超参数可以探索——包括我们模型和训练循环中已经存在的，以及尚未考虑的。例如，我们可以尝试不同的负采样策略，改变负对正对的比例，或者增加图卷积层的深度。然而，对于这个较小的数据集，当前的性能已经足够强大，我们认为进一步调整是不必要的。
- en: Train on a Larger Dataset
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在更大的数据集上训练
- en: Finally, we train a model on a much larger DDI dataset—scaling up from using
    roughly one-tenth to approximately one-half of the available data.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在一个更大的DDI数据集上训练了一个模型——从使用大约十分之一的数据增加到大约一半的数据。
- en: As noted earlier, the optimal model configuration often depends on dataset size
    and graph connectivity. With this larger dataset, additional hyperparameter exploration
    revealed that *degree normalization* becomes critical for strong performance.
    Moreover, a moderate dropout rate of 0.3 struck the best balance between regularization
    and learning capacity for this setting.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，最佳模型配置通常取决于数据集大小和图连接性。在这个更大的数据集中，额外的超参数探索表明*度归一化*对于强大性能至关重要。此外，0.3的适度dropout率在此设置中在正则化和学习容量之间取得了最佳平衡。
- en: 'Here is the full `DdiModel` setup:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的`DdiModel`设置：
- en: '[PRE63]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: The resulting learning curves in [Figure 4-15](#ddi-large-best-model) look quite
    different from the smaller-set runs. Validation Hits@20 climbs to ∼0.9—on par
    with our best earlier models—but training Hits@20 remains unexpectedly low, hovering
    near 0.1\. This wide gap likely reflects the increased difficulty of distinguishing
    positives from a much larger and more diverse set of negatives in the denser graph.
    It may also indicate that even with an AUC-like loss, nailing top-k ranking metrics
    like Hits@20 remains challenging at scale. Either way, further work—such as improved
    negative sampling or top-k–specific objectives—may be needed to fully leverage
    the larger dataset, and we encourage you to explore further.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-15](#ddi-large-best-model)中的学习曲线与较小集的运行结果截然不同。验证Hits@20上升到∼0.9——与我们的最佳早期模型相当——但训练Hits@20仍然出人意料地低，徘徊在0.1左右。这个巨大的差距可能反映了在更密集的图中区分正例和大量更多样化的负例的难度增加。这也可能表明，即使使用类似于AUC的损失，在规模上实现top-k排名指标如Hits@20仍然具有挑战性。无论如何，进一步的工作——例如改进负采样或top-k特定的目标——可能需要充分利用更大的数据集，我们鼓励您进一步探索。'
- en: '[PRE64]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![](assets/dlfb_0415.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0415.png)'
- en: Figure 4-15\. Learning curves for loss (left) and Hits@20 (right) of the best-performing
    model trained on the larger dataset. Maximum validation performance is high but
    does not show significant improvement over the performance of models trained on
    smaller datasets, suggesting further hyperparameter tuning or model adjustments
    may be needed.
  id: totrans-529
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-15。在较大数据集上训练的最佳性能模型的学习曲线（左）和Hits@20（右）。最大验证性能很高，但与在较小数据集上训练的模型相比没有显著改进，这表明可能需要进一步调整超参数或模型调整。
- en: Note
  id: totrans-530
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The strong validation performance suggests this model could scale to even larger
    datasets. However, scaling beyond 50% of the dataset becomes difficult due to
    memory constraints. With an embedding size of 512 for all nodes, the current implementation
    can run into out-of-memory (OOM) issues during XLA compilation.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 强验证性能表明，该模型可以扩展到更大的数据集。然而，由于内存限制，扩展到超过数据集50%的部分变得困难。对于所有节点使用512的嵌入大小，当前实现可能在XLA编译期间遇到内存不足（OOM）问题。
- en: A better solution would be to adopt a sampling strategy, as used in GraphSAGE,
    where the model processes subgraphs in batches. This makes it feasible to scale
    to larger datasets without reducing embedding size or compromising performance.
    However, implementing this is beyond the scope of the current chapter.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的解决方案是采用类似于GraphSAGE的采样策略，在该策略中，模型以批处理的方式处理子图。这使得在不减少嵌入大小或影响性能的情况下，扩展到更大的数据集成为可能。然而，实现这一点超出了当前章节的范围。
- en: Extensions
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展
- en: 'There are many ways to extend the model and training setup explored in this
    chapter:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了多种扩展模型和训练设置的方法：
- en: Graph sampling for scalability
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性的图采样
- en: Replace full-batch training with neighborhood sampling to scale to larger graphs
    without exceeding memory constraints.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 用邻域采样替换全批训练，以扩展到更大的图而不会超过内存限制。
- en: Incorporate node features
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 集成节点特征
- en: Integrate drug-specific information, such as chemical structure or pharmacological
    annotations, to enrich the learned embeddings and improve prediction accuracy.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 集成药物特定的信息，例如化学结构或药理学注释，以丰富学习到的嵌入并提高预测精度。
- en: Improve negative sampling
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 改进负采样
- en: Use more informative strategies like hard negative mining, where the model is
    shown negative pairs it finds confusing (e.g., those with high predicted interaction
    scores). This encourages the model to focus on challenging distinctions and improves
    generalization.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更有信息量的策略，如硬负样本挖掘，其中模型被展示出它认为难以区分的负对（例如，具有高预测交互分数的）。这鼓励模型关注具有挑战性的区分，并提高泛化能力。
- en: Try alternative GNN architectures
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试替代的GNN架构
- en: Explore other models like Graph Attention Networks, which weigh the importance
    of neighboring nodes, which are theoretically more expressive.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 探索其他模型，如Graph Attention Networks，这些模型权衡了相邻节点的重要性，这在理论上更具表现力。
- en: Transfer to new biological problems
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 转移到新的生物学问题
- en: Apply the same modeling framework to other interaction networks, such as gene
    regulatory, protein-protein, or drug-target interaction graphs.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 将相同的建模框架应用于其他交互网络，例如基因调控、蛋白质-蛋白质或药物-靶点交互图。
- en: 'Additionally, here are some analysis ideas you could explore:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这里还有一些您可以探索的分析思路：
- en: Error analysis by drug class
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 按药物类别进行错误分析
- en: Break down model performance by therapeutic class or chemical category to identify
    where the model struggles most.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 通过治疗类别或化学类别分解模型性能，以确定模型最挣扎的地方。
- en: Prediction certainty
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 预测确定性
- en: Examine the distribution of predicted probabilities. Which drug pairs is the
    model highly confident about (close to 0 or 1)? Which fall near 0.5? Investigate
    whether uncertain predictions share common characteristics (e.g., unusual structures,
    sparse connectivity).
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 检查预测概率的分布。模型对哪些药物对高度自信（接近0或1）？哪些接近0.5？调查不确定的预测是否具有共同特征（例如，不寻常的结构，稀疏的连接性）。
- en: Embedding visualization
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入可视化
- en: Use dimensionality reduction methods (e.g., t-SNE, UMAP) to project node embeddings
    into 2D and inspect whether drugs with similar functions cluster together.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 使用降维方法（例如，t-SNE，UMAP）将节点嵌入投影到二维空间，并检查具有相似功能的药物是否聚集在一起。
- en: Temporal validation
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 时间验证
- en: If drug interaction timestamps are available, evaluate the model on future data
    after training on past interactions. This mimics a real-world deployment scenario.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有药物相互作用的时间戳，在训练过去相互作用后，在未来的数据上评估模型。这模仿了现实世界的部署场景。
- en: Counterfactual analysis
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实分析
- en: Perturb the graph structure—for example, remove a known interaction or introduce
    a plausible but incorrect one—and observe how predictions change. This helps probe
    model sensitivity and identify influential edges.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 扰动图结构——例如，移除已知的相互作用或引入一个合理但错误的相互作用——并观察预测如何变化。这有助于探测模型敏感性并识别有影响力的边。
- en: Summary
  id: totrans-556
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we developed graph neural network models to predict links between
    nodes, applying them to the biologically meaningful task of drug–drug interaction
    (DDI) prediction. Starting with a simple architecture, we systematically explored
    model components and training strategies, eventually achieving strong validation
    performance through careful tuning of the loss function and key hyperparameters.
    Along the way, we introduced practical tools for evaluating performance, diagnosing
    overfitting, and scaling to larger datasets. These techniques are broadly applicable
    beyond DDIs, extending to a wide range of biological graph problems.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们开发了图神经网络模型来预测节点之间的链接，并将它们应用于生物上有意义的任务——药物-药物相互作用（DDI）预测。从简单的架构开始，我们系统地探索了模型组件和训练策略，最终通过仔细调整损失函数和关键超参数，实现了强大的验证性能。在这个过程中，我们介绍了评估性能、诊断过拟合以及扩展到更大数据集的实用工具。这些技术广泛应用于DDI之外，扩展到广泛的生物图问题。
- en: Our results show that even relatively simple graph models, when thoughtfully
    designed and optimized, can learn meaningful biological structure. As graph-based
    data becomes increasingly central in biology, the approaches introduced here provide
    a solid foundation for tackling more complex tasks—in drug discovery, genomics,
    systems biology, and beyond.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果表明，即使相对简单的图模型，当经过深思熟虑的设计和优化时，也能学习到有意义的生物结构。随着基于图的数据在生物学中变得越来越重要，这里介绍的方法为解决更复杂的任务提供了坚实的基础——在药物发现、基因组学、系统生物学以及更多领域。
- en: '^([1](ch04.html#id726-marker)) Udrescu, L. et al. [“Clustering Drug–Drug Interaction
    Networks with Energy Model Layouts: Community Analysis and Drug Repurposing”](https://oreil.ly/rdShs).
    *Scientific Reports*, 6 (2016): 32745.'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#id726-marker)) Udrescu，L.等人。[“使用能量模型布局对药物-药物相互作用网络进行聚类：社区分析和药物再利用”](https://oreil.ly/rdShs)。*Scientific
    Reports*，6（2016年）：32745。
- en: '^([2](ch04.html#id727-marker)) Hu, Weihua, Matthias Fey, Marinka Zitnik, Yuxiao
    Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020\. [“Open
    Graph Benchmark: Datasets for Machine Learning on Graphs”](https://oreil.ly/eK20s).
    arXiv.Org. May 2, 2020.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#id727-marker)) 胡伟华，马蒂亚斯·费伊，玛林卡·齐特尼克，董宇晓，任红玉，刘波，米歇尔·卡塔斯塔，以及尤雷·莱斯科夫。2020年。[“开放图基准：图上机器学习的数据集”](https://oreil.ly/eK20s)。arXiv.Org。2020年5月2日。
- en: '^([3](ch04.html#id740-marker)) Gilmer et al., “Neural Message Passing for Quantum
    Chemistry,” *Proceedings of the 34th International Conference on Machine Learning*
    (Sydney, NSW, Australia), vol. 70 (2017): 1263–1272, JMLR.org.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.html#id740-marker)) 吉尔默等人，“神经消息传递在量子化学中的应用”，*第34届国际机器学习会议论文集*（悉尼，新南威尔士州，澳大利亚），第70卷（2017年）：1263–1272，JMLR.org。
- en: '^([4](ch04.html#id741-marker)) F. Scarselli et al., “The Graph Neural Network
    Model,” *IEEE Transactions on Neural Networks* 20, no. 1 (2009): 61–80.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.html#id741-marker)) F. Scarselli等人，“图神经网络模型”，*IEEE Transactions on
    Neural Networks* 20，第1期（2009年）：61–80。
- en: ^([5](ch04.html#id754-marker)) Hamilton, W. L., Ying, Z., & Leskovec, J. (2017).
    [Inductive representation learning on large graphs](https://oreil.ly/8mNRd). *Neural
    Information Processing Systems*, 30, 1024–1034.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.html#id754-marker)) 汉密尔顿，W. L.，英，Z.，& 莱斯科夫，J. (2017). [大型图上的归纳表示学习](https://oreil.ly/8mNRd).
    *神经信息处理系统*, 30, 1024–1034.
- en: ^([6](ch04.html#id764-marker)) Google DeepMind. [“Intro to Graph Neural Nets
    with JAX/Jraph”](https://oreil.ly/r6MCp).
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.html#id764-marker)) 谷歌DeepMind. [“使用JAX/Jraph的图神经网络入门”](https://oreil.ly/r6MCp).
- en: ^([7](ch04.html#id808-marker)) Wang, Z., Zhou, Y., Hong, L., Zou, Y., Su, H.,
    & Chen, S. (2021). [Pairwise learning for neural link prediction](https://doi.org/10.48550/arxiv.2112.02936).
    arXiv (Cornell University).
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.html#id808-marker)) 王志，周毅，洪磊，邹宇，苏辉，& 陈思 (2021). [神经网络链接预测的成对学习](https://doi.org/10.48550/arxiv.2112.02936).
    arXiv (康奈尔大学).
