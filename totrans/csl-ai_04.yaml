- en: 3 Building a causal graphical model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building a causal DAG to model a DGP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using your causal graph as a communication, computation, and reasoning tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a causal DAG in pgmpy and Pyro
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a probabilistic machine learning model using the causal DAG as a scaffold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we’ll build our first models of the data generating process
    (DGP) using the *causal directed acyclic graph* (causal DAG)—a directed graph
    without cycles, where the edges represent causal relationships. We’ll also look
    at how to train a statistical model using the causal DAG as a scaffold.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Introducing the causal DAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s assume we can partition the DGP into a set of variables where a given
    combination of variable values represents a possible state of the DGP. Those variables
    may be discrete or continuous. They can be univariate, or they can be multivariate
    vectors or matrices.
  prefs: []
  type: TYPE_NORMAL
- en: A causal DAG is a directed graph where the nodes are this set of variables and
    the directed edges represent the causal relationships between them. When we use
    a causal DAG to represent the DGP, we assume the edges reflect true causality
    in the DGP.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, recall the rock-throwing DGP from chapter 2\. We started with
    Jenny and Brian having a certain amount of inclination to throw rocks at a window,
    which has a certain amount of strength. If either person’s inclination to throw
    surpasses a threshold, they throw. The window breaks depending on whether either
    or both of them throw and the strength of the window.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The code in this chapter was written with pgmpy version 0.1.24, pyro-ppl version
    1.8.6, and DoWhy version 0.11.1\. Version 0.20.1 of Python’s Graphviz library
    was used to draw an image of a DAG, and this depends on having the core Graphviz
    software installed. Comment out the Graphviz code if you would prefer not to set
    up Graphviz for now.
  prefs: []
  type: TYPE_NORMAL
- en: See the book’s notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for links to the Jupyter notebooks with the code.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll now create a causal DAG that will visualize this process. As a Python
    function, the DGP is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 DAG rock-throwing example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Input variables are numbers between 0 and 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Jenny and Brian throw the rock if so inclined.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 If both throw the rock, the strength of impact is .8.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 If one of them throws, the strength of impact is .6.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 If neither throws, the strength of impact is 0.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 The window breaks if the strength of impact is greater than the window strength.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 illustrates the rock-throwing DGP as a causal DAG.
  prefs: []
  type: TYPE_NORMAL
- en: In figure 3.1, each node corresponds to a random variable in the DGP. The directed
    edges correspond to cause-effect relationships (the source node is the cause and
    the target node is the effect).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F01_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 A causal DAG representing the rock-throwing DGP. In this example,
    each node corresponds to a random variable in the DGP.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '3.1.1 Case study: A causal model for transportation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we’ll look at a model of people’s choice of transportation
    on their daily commutes. This example will make overly strong assumptions (to
    the point of being borderline offensive) that will help illustrate the core ideas
    of model building. You’ll find links to the accompanying code and tutorials at
    [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you were an urban planning consultant trying to model the relationships
    between people’s demographic background, the size of the city where they live,
    their job status, and their decision on how to commute to work each day.
  prefs: []
  type: TYPE_NORMAL
- en: 'You could break down the key variables in the system as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Age (A)*—The age of an individual'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gender (S)*—An individual’s reported gender (using “S” instead of “G,” since
    “G” is usually reserved for DAGs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Education (E)*—The highest level of education or training completed by an
    individual'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Occupation (O)*—An individual’s occupation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Residence (R)*—The size of the city the individual resides in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Travel (T)*—The means of transport favored by the individual'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You could then think about the causal relationships between these variables,
    using knowledge about the domain. Here is a possible narrative:'
  prefs: []
  type: TYPE_NORMAL
- en: Educational standards are different across generations. For older people, a
    high school degree was sufficient to achieve a middle-class lifestyle, but younger
    people need at least a college degree to achieve the same lifestyle. Thus, age
    (*A*) is a cause of education (*E*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, a person’s gender is often a factor in their decision to pursue higher
    levels of education. So, gender (*S*) is a cause of education (*E*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many white-collar jobs require higher education. Many credentialed professions
    (e.g., doctor, lawyer, or accountant) certainly require higher education. So education
    (*E*) is a cause of occupation (*O*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White-collar jobs that depend on higher levels of education tend to cluster
    in urban areas. Thus, education (*E*) is a cause of where people reside (*R*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: People who are self-employed might work from home and therefore don’t need to
    commute, while people with employers do. Thus, occupation (*O*) is a cause of
    transportation (*T*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: People in big cities might find it more convenient to commute by walking or
    using public transportation, while people in small cities and towns rely on cars
    to get around. Thus, residence (*R*) is a cause of transportation (*T*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could have created this narrative based on your knowledge about the domain,
    or based on your research into the domain. Alternatively, you could have consulted
    with a domain expert, such as a social scientist who specializes in this area.
    Finally, you could reduce this narrative to the causal DAG shown in figure 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F02_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 A causal DAG representing a model of the causal factors behind how
    people commute to work
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You could build this causal DAG using the following code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Building the transportation DAG in pgmpy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 pgmpy provides a BayesianNetwork class where we add the edges to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Input the ΔAG as a list of edges (tuples).'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Input the ΔAG as a list of edges (tuples).'
  prefs: []
  type: TYPE_NORMAL
- en: The `BayesianNetwork` object in pgmpy is built on the `DiGraph` class from NetworkX,
    the preeminent graph modeling library in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Causal abstraction and causal representation learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In modeling, the *level of abstraction* refers to the level of detail and granularity
    of the variables in the model. In figure 3.2, there is a mapping between the variables
    in the data and the variables in the causal DAG, because the level of abstraction
    in the data generated by the DGP and the level of abstraction of the causal DAG
    are the same. But it is possible for variables in the data to be at a different
    levels of abstraction. This is particularly common in machine learning, where
    we often deal with low-level features, such as pixels.
  prefs: []
  type: TYPE_NORMAL
- en: When the level of abstraction in the data is lower than the level the modeler
    wants to work with, the modeler must use domain knowledge to derive the high-level
    abstractions that will appear as nodes in the DAG. For example, a doctor may be
    interested in a high-level binary variable node like “Tumor (present/absent),”
    while the data itself contains low-level variables such as a matrix of pixels
    from medical imaging technology.
  prefs: []
  type: TYPE_NORMAL
- en: That doctor must look at each image in the dataset and manually label the high-level
    tumor variable. Alternatively, a modeler can use analytical means (e.g., math
    or logic) to map low-level abstractions to high-level ones. Further, they must
    do so in a way that preserves causal assumptions about the DGP.
  prefs: []
  type: TYPE_NORMAL
- en: This task of creating high-level variables from lower-level ones in a causally
    rigorous way is called *causal abstraction*. In machine learning, the term “feature
    engineering” applies to the task of computing *useful* high-level features from
    lower-level features. Causal abstraction differs in that requirement for causal
    rigor. You’ll find some sources for causal abstraction information in the book’s
    notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook).
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to learning high-level causal abstractions from lower ones
    in data is to use deep learning—this is called *causal representation learning*.
    We’ll touch briefly on this topic in chapter 5.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Why use a causal DAG?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The causal DAG is the best-known representation of causality, but to understand
    its value, it’s useful to think about other ways of modeling causality. One alternative
    is using a mathematical model, such as a set of ordinary differential equations
    or partial differential equations, as is common in physics and engineering. Another
    option is to use computational simulators, such as are used in meteorology and
    climate science.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to those alternatives, a causal DAG requires a much less mathematically
    detailed understanding of the DGP. A causal DAG only requires you to specify what
    causes what, in the form of a graph. Graphs are easy for humans to think about;
    they are the go-to method for making sense of complicated domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, there are several benefits of using a causal DAG as a representation
    of the DGP:'
  prefs: []
  type: TYPE_NORMAL
- en: DAGs are useful in communicating and visualizing causal assumptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have many tools for computing over DAGs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal DAGs represent time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DAGs link causality to conditional independence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DAGs can provide scaffolding for probabilistic ML models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameters in those probabilistic ML models are modular parameters, and
    they encode causal invariance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review these benefits one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 DAGs are useful in communicating and visualizing causal assumptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A causal DAG is a powerful communication device. Visual communication of information
    involves highlighting important information at the expense of other information.
    As an analogy, consider the two maps of the London Underground in figure 3.3\.
    The map on the left is geographically accurate. The simpler map on the right ignores
    the geographic detail and focuses on the position of each station relative to
    other stations, which is, arguably, all one needs to find their way around London.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F03_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 Visual communication is a powerful use case for a graphical representation.
    For instance, the map of the London Underground on the left is geographically
    accurate, while the one on the right trades that accuracy for a clear representation
    of each station’s position relative to the others. The latter is more useful for
    train riders than the one with geographic accuracy. Similarly, a causal DAG abstracts
    away much detail of the causal mechanism to create a simple representation that
    is easy to reason about visually.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similarly, a causal DAG highlights causal relationships while ignoring other
    things. For example, the rock-throwing DAG ignores the if-then conditional logic
    of how Jenny and Brian’s throws combined to break the window. The transportation
    DAG says nothing about the types of variables we are dealing with. Should we consider
    age (*A*) in terms of continuous time, integer years, categories like young/middle-aged/elderly,
    or intervals like 18–29, 30–44, 45–64, and >65? What are the categories of the
    transportation variable (*T*)? Could the occupation variable (*O*) be a multidimensional
    tuple like {employed, engineer, works-from-home}? The DAG also fails to capture
    which of these variables are observed in the data, and the number of data points
    in that data.
  prefs: []
  type: TYPE_NORMAL
- en: Causal DAGs don’t illustrate mechanism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A causal DAG also doesn’t visualize interactions between causes. For example,
    in older generations, women were less likely to go to college than men. In younger
    generations, the reverse is true. While both age (*A*) and gender (*S*) are causes
    of education (*E*), you can’t look at the DAG and see anything about how age and
    gender interact to affect education.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, DAGs can’t convey any information about the causal mechanism
    or *how* the causes impact the effect. They only establish the *what* of causality,
    as in *what* causes *what*. Consider, for example, the various logic gates in
    figure 3.4\. The input binary values for *A* and *B* determine the output differently
    depending on the type of logic gate. But if we represent a logic gate as a causal
    DAG, then all the logic gates have the same causal DAG. We can use the causal
    DAG as a scaffold for causal graphical models that capture this logic, but we
    can’t *see* the logic in the DAG.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F04_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 The various kinds of logic gates all have the same causal DAG.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is a strength and a weakness. A causal DAG simplifies matters by communicating
    what causes what, but not *how*. However, in some cases (such as logic gates),
    visualizing the *how* would be desirable.
  prefs: []
  type: TYPE_NORMAL
- en: Causal DAGs represent causal assumptions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A causal DAG represents the modeler’s assumptions and beliefs about the DGP,
    because we don’t have access to that process most of the time. Thus, a causal
    DAG allows us to visualize our assumptions and communicate them to others.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond this visualization and communication, the benefits of a causal DAG are
    mathematical and computational (I’ll explain these in the following subsections).
    Causal inference researchers vary in their opinions on the degree to which these
    mathematical and computational properties of causal DAGs are practically beneficial.
    However, most agree on the fundamental benefit of visualization and communication
    of causal assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: The assumptions encoded in a causal DAG are strong. Let’s look again at the
    transportation DAG from figure 3.2, shown again in figure 3.5\. Consider the alternatives
    to that DAG; how many possible DAGs could we draw on this simple six-node system?
    The answer is 3,781,503, so when we use a causal DAG to communicate our assumptions
    about this system, we’re communicating our top choice over 3,781,502 alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F05_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 A causal DAG model of transportation choices. This DAG encodes strong
    assumptions about how these variables do and do not relate to one another.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: And how about some of those competing DAGs? Some of them seem plausible. Perhaps
    baby boomers prefer small-town life while millennials prefer city life, implying
    that there should be an *A* → *R* edge. Perhaps gender norms determine preferences
    and opportunities in certain professions and industries, implying an *S* → *O*
    edge. The assumption that age and gender cause occupation and residence only indirectly
    through education is a powerful assumption that would provide useful inferences
    *if* *it is right*.
  prefs: []
  type: TYPE_NORMAL
- en: But what if our causal DAG is wrong? It seems it is likely to be wrong, given
    its 3,781,502 competitors. In chapter 4, we’ll use data to show us when the causal
    assumptions in our chosen DAG fail to hold.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 We have many tools for computing over DAGs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Directed graphs are well-studied objects in math and in computer science, where
    they are a fundamental data structure. Computer scientists have used graph algorithms
    to solve many practical problems with theoretical guarantees on how long they
    will take to arrive at solutions. The programming languages commonly used in data
    science and machine learning have libraries that implement these algorithms, such
    as NetworkX in Python. These popular libraries make it easier to write code that
    works with causal DAGs.
  prefs: []
  type: TYPE_NORMAL
- en: We can bring all that theory and tooling to bear on a causal modeling problem
    when we represent a causal model in the form of a causal DAG. For example, in
    pgmpy we can train a causal DAG on data to get a directed causal graphical model.
    Given that model, we can apply algorithms for graph-based probabilistic inference,
    such as *belief propagation*, to estimate conditional probabilities defined on
    variables in the graph. The directed graph structure enables these algorithms
    to work in typical settings without our needing to configure them to a specific
    problem or task.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, I’ll introduce the concept of *d-separation*, which is
    a graphical abstraction for conditional independence and the fundamental idea
    behind the do-calculus theory for causal inference. D-separation is all about
    finding paths between nodes in the directed graph, which is something any worthwhile
    graph library makes easy by default. Indeed, conditional independence is the key
    idea behind the third benefit of the causal DAG.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 Causal DAGs can represent time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The causal DAG has an implicit representation of time. In more technical terms,
    the causal DAG provides a *partial temporal ordering* because causes precede effects
    in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the graph in figure 3.6\. This graph describes a DGP
    where a change in cloud cover (Cloudy) causes both a change in the state of a
    weather-activated sprinkler (Sprinkler) and the state of rain (Rain), and these
    both cause a change in the state of the wetness of the grass (Wet Grass). We know
    that a change in the state of the weather causes rain and sprinkler activation,
    and that these both cause a change in the state of the wetness of the grass. However,
    it is only a *partial* temporal ordering, because the graph doesn’t tell us which
    happens first: the sprinkler activation or the rain.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F06_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 A causal DAG representing the state of some grass (wet or dry). The
    DAG gives us a partial temporal ordering over its nodes because causes precede
    effects in time.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The partial ordering in figure 3.6 may seem trivial, but consider the DAG in
    figure 3.7\. Visualization libraries can use the partial ordering in the hairball-like
    DAG on the left of figure 3.7 to create the much more readable form on the right.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F07_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 A visualization library can use the DAG’s partial ordering to unravel
    the hairball-like DAG on the left into the more readable form on the right.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sometimes we need a causal DAG to be more explicit about time. For example,
    we may be modeling causality in a dynamic setting, such as in the models used
    in reinforcement learning. In this case, we can make time explicit by defining
    and labeling the variables of the model, as in figure 3.8\. We can represent continuous
    time with interval variables like “Δ.” Chapter 12 will provide some concrete examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F08_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 If we need a causal DAG to be explicit about time, we can make time
    explicit in the definition of the variables and labeling of their nodes. We can
    represent continuous time with interval variables like “Δ.”
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The causal DAG doesn’t allow for any cycles. In some causal systems, relaxing
    the acyclicity constraint makes sense, such as with systems that have feedback
    loops, and some advanced causal models allow for cycles. But sticking to the simpler
    acyclic assumption allows us to leverage the benefits of the causal DAG.
  prefs: []
  type: TYPE_NORMAL
- en: If you have cycles, sometimes you can *unroll* the cycle over time and make
    the time explicit to get acyclicity. A graph *X* ⇄ *Y* can unroll as *X*[0]→*Y*[0]
    →*X*[1]→*Y*[1] . . . . For example, you may have a cycle between supply, price,
    and demand, but perhaps you could rewrite this as price at time 0 affects supply
    and demand at time 1, which then affects price at time 2, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.6 DAGs link causality to conditional independence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another benefit of a causal DAG is that it allows us to use causality to reason
    about conditional independence. Humans have an innate ability to reason in terms
    of causality—that’s how we get the first and second benefits of causal DAGs. But
    reasoning probabilistically doesn’t come nearly as easily. As a result, the ability
    to use causality to reason about conditional independence (a concept from probability)
    is a considerable feature of DAGs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F09_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 The causal relationships in the transportation DAG encode key assumptions
    about conditional independence.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Consider the transportation DAG, displayed again in figure 3.9\.
  prefs: []
  type: TYPE_NORMAL
- en: The six variables in the DAG have a joint distribution *P*(*A*,*S*,*E*,*O*,*R*,*T*).
    Recall the chain rule from chapter 2, which says that we can factorize any joint
    probability into a chain of conditional probability factors. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch3-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The chaining works for any ordering of the variables. But instead of choosing
    any ordering, we’ll choose the (partial) ordering of the causal DAG, since that
    ordering aligns with our assumptions of the causal flow of the variables in the
    DGP. Looking at figure 3.9, the ordering of variables is {(*A*, *S*), *E*, (*O*,
    *R*), *T*}. The pairs (*A*, *S*) and (*O*, *R*) are unordered. If we arbitrarily
    pick an ordering, letting *A* come before *S* and *O* come before *R*, we get
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch3-eqs-1x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we’ll use the causal DAG to further simplify this factorization. Each
    factor is a conditional probability, so we’ll simplify those factors by *conditioning
    each node on only its parents* in the DAG. In other words, for each variable,
    we’ll look at that variable’s direct parents in the graph, then we’ll drop everything
    on the right side of the conditioning bar (|) that isn’t one of those direct parents.
    If we condition only on parents, we get the following simplification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch3-eqs-2x.png)'
  prefs: []
  type: TYPE_IMG
- en: What is going on here? Why should the causal DAG magically mean we can say *P*(*s|a*)
    is equal to *P*(*s*) and *P*(*r|o,e,s,a*) simplifies to *P*(*r**|e*)?As discussed
    in chapter 2, stating that *P*(*s*|*a*)*=**P*(*s*) and *P*(*t*|*o,r,e,s,a*)*=**P*(*t*|*o,r*)
    is equivalent to saying that *S* and *A* are independent, and *T* is conditionally
    independent of *E*, *S*, and *A*, given *O* and *R*. In other words, the causal
    DAG gives us a way to impose conditional independence constraints over the joint
    probability distribution of the variables in the DGP.
  prefs: []
  type: TYPE_NORMAL
- en: Why should we care about things being conditionally independent? Conditional
    independence makes life as a modeler easier. For example, suppose you were to
    model the transportation variable *T* with a predictive model. The predictive
    model implied by *P*(*t*|*o,r,e,s,a*) requires having features *O*, *R*, *E*,
    *S*, and *A*, while the predictive model implied by *P*(*t*|*o,r*) just requires
    features *O* and *R* to predict *T*. The latter model will have fewer parameters
    to learn, have more degrees of freedom, take less space in memory, train faster,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: But why does a causal DAG give us the right to impose conditional independence?
    Let’s build some intuition about the connection between causality and conditional
    independence. Consider the example of using genetic data from family members to
    draw conclusions about an individual. For example, the Golden State Killer was
    a California-based serial killer captured using genetic genealogy. Investigators
    used DNA left by the killer at crime scenes to identify genetic relatives in public
    databases. They then triangulated from those relatives to find the killer.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you had a close relative and a distant relative on the same line of
    ancestry. Could the distant relative provide any additional genetic information
    about you once we had already accounted for that close relative? Let’s simplify
    a bit by focusing just on blood type. Suppose the close relative was your father,
    and the distant relative was your paternal grandfather, as in figure 3.10\. Indeed,
    your grandfather’s blood type is a cause of yours. If we saw a large dataset of
    grandfather/grandchild blood type pairs, we’d see a correlation. However, your
    father’s blood type is a more direct cause, and the connection between your grandfather’s
    blood type and yours passes through your father. So, if our goal were to predict
    your blood type, and we already had your father’s blood type as a predictor, your
    paternal grandfather’s blood type could provide no additional predictive information.
    Thus, your blood type and your paternal grandfather’s blood type are conditionally
    independent, given your father’s blood type.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F10_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 Causality implies conditional independence. Your paternal grandfather’s
    blood type is a cause of your father’s, which is a cause of yours. You and your
    paternal grandfather’s blood types are conditionally independent given your father’s
    blood type because your father’s blood type already contains all the information
    your grandfather’s type could provide about yours.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The way causality makes correlated variables conditionally independent is called
    the *causal Markov property*. In graphical terms, the causal Markov property means
    that variables are conditionally independent of their non-descendants (e.g., ancestors,
    uncles/aunts, cousins, etc.) given their parents in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'This “non-descendants” definition of the causal Markov property is sometimes
    called the *local Markov property*. An equivalent articulation is called the *Markov
    factorization property*, which is the property that if your causal DAG is true,
    you can factorize a joint probability into conditional probabilities of variables,
    given their parents in the causal DAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch3-eqs-3x.png)'
  prefs: []
  type: TYPE_IMG
- en: If our transportation DAG is a true representation of the DGP, then the local
    Markov property should hold. In the next chapter, we’ll see how to test this assumption
    with data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.7 DAGs can provide scaffolding for probabilistic ML models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many modeling approaches in probabilistic machine learning use a DAG as the
    model structure. Examples include directed graphical models (aka Bayesian networks)
    and latent variable models (e.g., topic models). Deep generative models, such
    as variational autoencoders, often have an underlying directed graph.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of building a probabilistic machine learning model on top of a
    causal graph is, rather obviously, that you have a probabilistic *causal* machine
    learning model. You can train it on data, and you can use it for prediction and
    other inferences, like any probabilistic machine learning model. Moreover, because
    it is built on top of a causal DAG, it is a causal model, so you can use it to
    make causal inferences.
  prefs: []
  type: TYPE_NORMAL
- en: A benefit that follows from providing scaffolding is that *the parameters in
    those models are modular and encode causal invariance*. Before exploring this
    benefit, let’s first build a graphical model on the transportation DAG.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Recall our factorization of the joint probability distribution of the transportation
    variables over the ordering of the variables in the transportation DAG.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch3-eqs-4x.png)'
  prefs: []
  type: TYPE_IMG
- en: We have a set of factors, {*P*(*a*)*, P*(*s*)*, P*(*e*|*s,a*)*, P*(*o*|*e*)*,
    P*(*r*|*e*)*, P*(*t*|*o,r*)}*.* From here on, we’ll build on the term “Markov
    kernel” from chapter 2 and call these factors *causal Markov kernels*.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll build our probabilistic machine learning model by implementing these causal
    Markov kernels in code and then composing them into one model. Our implementations
    for each kernel will be able to return a probability value, given input arguments.
    For example, *P*(*a*) will take an outcome value for *A* and return a probability
    value for that outcome. Similarly, *P*(*t*|*o,r*) will take in values for *T*,
    *O*, and *R* and return a probability value for *T*=*t*, where *t* is the queried
    value. Our implementations will also be able to generate from the causal Markov
    kernels. To do this, these implementations will require parameters that map the
    inputs to the outputs. We’ll use standard statistical learning approaches to fit
    those parameters from the data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.8 Training a model on the causal DAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider the DGP for the transportation DAG. What sort of data would this process
    generate?
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we administered a survey covering 500 individuals, getting values for
    each of the variables in this DAG. The data encodes the variables in our DAG as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Age (A)*—Recorded as young (“young”) for individuals up to and including 29
    years, adult (“adult”) for individuals between 30 and 60 years old (inclusive),
    and old (“old”) for people 61 and over'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gender (S)*—The self-reported gender of an individual, recorded as male (“M”),
    female (“F”), or other (“O”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Education (E)*—The highest level of education or training completed by the
    individual, recorded either high school (“high”) or university degree (“uni”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Occupation (O)*—Employee (“emp”) or a self-employed worker (“self”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Residence (R)*—The population size of the city the individual lives in, recorded
    as small (“small”) or big (“big”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Travel (T)*—The means of transport favored by the individual, recorded as
    car (“car”), train (“train”), or other (“other”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labeling causal abstractions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: How we conceptualize the variables of a model matters greatly in machine learning.
    For example, ImageNet, a database of 14 million images, contains anachronistic
    and offensive labels for racial categories. Even if renamed to be less offensive,
    race categories themselves are fluid across time and culture. What are the “correct”
    labels to use in a predictive algorithm?
  prefs: []
  type: TYPE_NORMAL
- en: How we define our variables isn’t just a question of politics and census forms.
    A simple thought experiment by philosopher Nelson Goodman shows how a simple change
    in label can change a prediction to a contradictory prediction. Suppose you regularly
    search for gems and record the color of every gem you find. It turns out 100%
    of the gems in your dataset are green. Now let’s define a new label “grue” to
    mean “green if observed before now, blue otherwise.” So 100% of your data is “green”
    or “grue,” depending on your choice of label. Now suppose you predict the future
    based on extrapolating from the past. Then you can predict the next emerald will
    be green based on data where all past emeralds were green, or you can predict
    the next emerald will be “grue” (i.e., *blue*) based on the data that all past
    emeralds were “grue.” Obviously, you would never invent such an absurd label,
    but this thought experiment is enough to show that the inference depends on the
    abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: In data science and machine learning, we’re often encouraged to blindly model
    data and not to think about the DGP. We’re encouraged to take the variable names
    for granted as columns in a spreadsheet or attributes in a database table. When
    possible, it is better to choose abstractions that are appropriate to the inference
    problem and collect or encode data according to that abstraction. When it is not
    possible, keep in mind that the results of your analysis will depend on how other
    people have defined the variables.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 7, I’ll introduce the idea of “no causation without manipulation”—an
    idea that provides a useful heuristic for how to define causal variables.
  prefs: []
  type: TYPE_NORMAL
- en: The variables in the transportation data are all *categorical* variables. In
    this simple categorical case, we can rely on a graphical modeling library like
    pgmpy*.*
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Loading transportation data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We’ll load the data into a pandas ΔataFrame with the read_csv method.'
  prefs: []
  type: TYPE_NORMAL
- en: This produces the DataFrame in figure 3.11.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F11_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 An example of data from the DGP underlying the transportation model.
    In this case, the data is 500 survey responses.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `BayesianNetwork` class we initialized in listing 3.2 has a `fit` method
    that will learn the parameters of our causal Markov kernels. Since our variables
    are categorical, our causal Markov kernels will be in the form of conditional
    probability tables represented by pgmpy’s `TabularCPD` class. The `fit` method
    will fit (“learn”) estimates of the parameters of those conditional probability
    tables using the data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.4 Learning parameters for the causal Markov kernels
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The fit method on the BayesianNetwork object will estimate parameters from
    data (a pandas ΔataFrame).'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Retrieve and view the causal Markov kernels learned by fit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the structure of the causal Markov kernel for the transportation
    variable *T*. We can see from printing out the `causal_markov_kernels` list that
    *T* is the last item in the list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that in this printout, I truncated the numbers so the table fits on the
    page.
  prefs: []
  type: TYPE_NORMAL
- en: '`cmk_T` is the implementation of the causal Markov kernel *P*(*T**|O,R*) as
    a conditional probability table, a type of lookup table where, given the values
    of *T*, *O*, and *R*, we get the corresponding probability mass value. For example,
    *P*(*T*=car|*O*=emp, *R*=big) = 0.7034\. Note that these are conditional probabilities.
    For each combination of values for *O* and *R*, there are conditional probabilities
    for the three outcomes of *T* that sum to 1\. For example, when *O*=emp and *R*=big,
    *P*(*T*=car| *O*=emp, *R*=big) + (*P*(*T*=other| *O*=emp, *R*=big) + *P*(*T*=train|
    *O*=emp, *R*=big) = 1.'
  prefs: []
  type: TYPE_NORMAL
- en: The causal Markov kernel in the case of nodes with no parents is just a simple
    probability table. For example, `print(causal_markov_kernels[2])` prints the causal
    Markov kernel for gender (*S*), the third item in the `causal_markov_kernels`
    list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `fit` method learns parameters by calculating the proportions of each class
    in the data. Alternatively, we could use other techniques for parameter learning.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.9 Different techniques for parameter learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several ways we could go about training these parameters. Let’s look
    at a few common ways of training parameters in conditional probability tables.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The learning algorithm I used in the `fit` method on the `BayesianNetwork`
    model object was *maximum likelihood estimation* (discussed in chapter 2). It
    is the default parameter learning method, so I didn’t specify “maximum likelihood”
    in the call to `fit`. Generally, maximum likelihood estimation seeks the parameter
    that maximizes the likelihood of seeing the data we use to train the model. In
    the context of categorical data, maximum likelihood estimation is equivalent to
    taking proportions of counts in the data. For example, the parameter for *P*(*O*=emp|*E*=high)
    is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch3-eqs-5x.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayesian estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In chapter 2, I also introduced Bayesian estimation. It is generally mathematically
    intractable and relies on computationally expensive algorithms (e.g., sampling
    algorithms and variational inference). A key exception is the case of *conjugate
    priors*, where the prior distribution and the target (posterior) distribution
    have the same canonical form. That means the code implementation can just calculate
    the parameter values of the target distribution with simple math, without the
    need for complicated Bayesian inference algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: For example, pgmpy implements a *Dirichlet conjugate prior* for categorical
    outcomes. For each value of *O* in *P*(*O*|*E*=high), we have a probability value,
    and we want to infer these probability values from the data. A Bayesian approach
    assigns a prior distribution to these values. A good choice for a prior on a set
    of probability values is the *Dirichlet distribution*, because it is defined for
    a *simplex*, a set of numbers between zero and one that sum to one. Further it
    is *conjugate* to categorical distributions like *P*(*O*|*E*=high), meaning the
    posterior distribution on the parameter values is also a Dirichlet distribution.
    That means we can calculate point estimates of the probability values using simple
    math, combining counts in the data and parameters in the prior. pgmpy does this
    math for us.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.5 Bayesian point estimation with a Dirichlet conjugate prior
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Import BayesianEstimator and initialize it on the model and data.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Pass the estimator object to the fit method.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 pseudo_counts refers to the parameters of the Δirichlet prior.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Extract the causal Markov kernels and view P(T|O,R).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In contrast to maximum likelihood estimation, Bayesian estimation of a categorical
    parameter with a Dirichlet prior acts like a smoothing mechanism. For example,
    the maximum likelihood parameter estimate says 100% of self-employed people in
    small towns take a car to work. This is probably extreme. Certainly, some self-employed
    people bike to work—we just didn’t manage to survey any of them. Some small cities,
    such as Crystal City in the US state of Virginia (population 22,000), have subway
    stations. I’d wager at least a few of the entrepreneurs in those cities use the
    train.
  prefs: []
  type: TYPE_NORMAL
- en: Causal modelers and Bayesians
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Bayesian philosophy goes beyond mere parameter estimation. Indeed, Bayesian
    philosophy has much in common with DAG-based causal modeling. Bayesians try to
    encode subjective beliefs, uncertainty, and prior knowledge into “prior” probability
    distributions on variables in the model. Causal modelers try to encode subjective
    beliefs and prior knowledge about the DGP into the form of a causal DAG. The two
    approaches are compatible. Given a causal DAG, you can be Bayesian about inferring
    the parameters of the probabilistic model you build on top of the causal DAG.
    You can even be Bayesian about the DAG itself and compute probability distributions
    over possible DAGs!
  prefs: []
  type: TYPE_NORMAL
- en: I focus on causality in this book and keep Bayesian discussions to a minimum.
    But we’ll use the libraries Pyro (and its NumPy-JAX alternative NumPyro) to implement
    causal models; these libraries provide complete support for Bayesian inference
    on models as well as parameters. In chapter 11, we’ll look at an example of Bayesian
    inference of a causal effect using a causal graphical model we build from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Other techniques for parameter estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We need not use a conditional probability table to represent the causal Markov
    kernels. There are models within the generalized linear modeling framework for
    modeling categorical outcomes. For some of the variables in the transportation
    model, we might have used non-categorical outcomes. Age, for example, might have
    been recorded as an integer outcome in the survey. For variables with numeric
    outcomes, we might use other modeling approaches. You can also use neural network
    architectures to model individual causal Markov kernels.
  prefs: []
  type: TYPE_NORMAL
- en: '*Parametric assumptions* refer to how we specify the outcomes of a node in
    the DAG (e.g., category or real number) and how we map parents to the outcome
    (e.g., table or neural network). Note that the causal assumptions encoded by the
    causal DAG are decoupled from the parametric assumptions for a causal Markov kernel.
    For example, when we assumed that age was a direct cause of education level and
    encoded that into our DAG as an edge, we didn’t have to decide if we were going
    to treat age as an ordered set of classes, as an integer, or as seconds elapsed
    since birth, etc. Furthermore, we didn’t have to know whether to use a conditional
    categorical distribution or a regression model. That step comes after we specify
    the causal DAG and want to implement *P*(*E*|*A*, *S*).'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, when we make predictions and probabilistic inferences on a trained
    causal model, the considerations of what inference or prediction algorithms to
    use, while important, are separate from our causal questions. This separation
    simplifies our work. Often we can build our knowledge and skill set in causal
    modeling and reasoning independently of our knowledge of statistics, computational
    Bayes, and applied machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.10 Learning parameters when there are latent variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since we are modeling the DGP and not the data, it is likely that some nodes
    in the causal DAG will not be observed in the data. Fortunately, probabilistic
    machine learning provides us with tools for learning the parameters of causal
    Markov kernels of latent variables.
  prefs: []
  type: TYPE_NORMAL
- en: Learning latent variables with pgmpy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To illustrate, suppose the education variable in the transportation survey data
    was not recorded. pgmpy gives us a utility for learning the parameters of the
    causal Markov kernel for latent *E* using an algorithm called *structural expectation
    maximization*, which is a variant of parameter learning with maximum likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.6 Training a causal graphical model with a latent variable
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Δownload the data and convert to a pandas ΔataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Keep all the columns except education (E).'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Indicate which variables are latent when training the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Run the structural expectation maximization algorithm to learn the causal
    Markov kernel for E. You have to indicate the cardinality of the latent variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Print out the learned causal Markov kernel for E. Print it as a factor object
    for legibility.'
  prefs: []
  type: TYPE_NORMAL
- en: The `print` line prints a factor object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The outcomes for *E* are 0 and 1 because the algorithm doesn’t know the outcome
    names. Perhaps 0 is “high” (high school) and 1 is “uni” (university), but correctly
    mapping the default outcomes from a latent variable estimation method to the names
    of those outcomes would require further assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: There are other algorithms for learning parameters when there are latent variables,
    including some that use special parametric assumptions (i.e., functional assumptions
    about how the latent variables relate to the observed variables).
  prefs: []
  type: TYPE_NORMAL
- en: Latent variables and identification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In statistical inference, we say a parameter is “identified” when it is theoretically
    possible to learn its true value given an infinite number of examples in the data.
    It is “unidentified” if more data doesn’t get you closer to learning its true
    value. Unfortunately, your data may not be sufficient to learn the causal Markov
    kernels of the latent variables in your causal DAG. If we did not care about representing
    causality, we could restrict ourselves to a latent variable graphical model with
    latent variables that are identifiable from data. But we must build a causal DAG
    that represents the DGP, even if we can’t identify the latent variables and parameters
    given the data.
  prefs: []
  type: TYPE_NORMAL
- en: That said, even if you have non-identifiable parameters in your causal model,
    you still may be able to identify the quantity that answers your causal question.
    Indeed, much of causal inference methodology is focused on robust estimation of
    causal effects (how much a cause affects an effect) despite having latent “confounders.”
    We’ll cover this in detail in chapter 11\. On the other hand, even if your parameters
    are identified, the quantity that answers your causal question may not be identified.
    We’ll cover causal identification in detail in chapter 10.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.11 Inference with a trained causal probabilistic machine learning model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A probabilistic machine learning model of a set of variables can use computational
    inference algorithms to infer the conditional probability of an outcome for any
    subset of the variables, given outcomes for the other variables. We use the variable
    elimination algorithm for a directed graphical model with categorical outcomes
    (introduced in chapter 2).
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we want to compare education levels amongst car drivers
    to that of train riders. We can calculate and compare *P*(*E*|*T*) when *T*=car
    to when *T*=train by using variable elimination, an inference algorithm for tabular
    graphical models.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.7 Inference on the trained causal graphical model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 VariableElimination is an inference algorithm specific to graphical models.'
  prefs: []
  type: TYPE_NORMAL
- en: This prints the probability tables for “train” and “car.”
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems car drivers are more likely to have a university education than train
    riders: (*P*(*E*=''uni''|*T*=''car'') > *P*(*E*=''uni''|*T*=''train''). That inference
    is based on our DAG-based causal assumption that university education indirectly
    determines how people get to work.'
  prefs: []
  type: TYPE_NORMAL
- en: In a tool like Pyro, you have to be a bit more hands-on with the inference algorithm.
    The following listing illustrates the inference of *P*(*E*|*T*=“train”) using
    a probabilistic inference algorithm called importance sampling. First, we’ll specify
    the model. Rather than fit the parameters, we’ll explicitly specify the parameter
    values we fit with pgmpy.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.8 Implementing the trained causal model in Pyro
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The categorical distribution only returns integers, so it’s useful to write
    the integers’ mapping to categorical outcome names.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 For simplicity, we’ll use rounded versions of parameters learned with the
    fit method in pgmpy (listing 3.4), though we could have learned the parameters
    in a training procedure.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 When we implement the model in Pyro, we specify the causal ΔAG implicitly
    using code logic.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 We can then generate a figure of the implied ΔAG using pyro.render_model().
    Note that we need to have Graphviz installed.'
  prefs: []
  type: TYPE_NORMAL
- en: The `pyro.render_model` function draws the implied causal DAG from the Pyro
    model in figure 3.12\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F12_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 You can visualize the causal DAG in Pyro by using the `pyro.render_model()`
    function. This assumes you have Graphviz installed.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Pyro provides probabilistic inference algorithms, such as importance sampling,
    that we can apply to our causal model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.9 Inference on the causal model in Pyro
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We’ll use two inference-related classes, Importance and EmpiricalMarginal.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 pyro.condition is a conditioning operation on the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 It takes in the model and evidence for conditioning on. The evidence is
    a dictionary that maps variable names to values. The need to specify variable
    names during inference is why we have the name argument in the calls to pyro.sample.
    Here we condition on T=“train”.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 We’ll run an inference algorithm that will generate m samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 I use an inference algorithm called importance sampling. The Importance
    class constructs this inference algorithm. It takes the conditioned model and
    the number of samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Run the random process algorithm with the run method. The inference algorithm
    will generate from the joint probability of the variables we didn’t condition
    on (everything but T) given the variables we conditioned on (T).'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 We are interested in the conditional probability distribution of education,
    so we extract education values from the posterior.'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Based on these samples, we produce a Monte Carlo estimation of the probabilities
    in P(E|T=“train”).'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Plot a visualization of the learned probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: This produces the plot in figure 3.13\. The probabilities shown are close to
    the results from the pgmpy model, though they’re slightly different due to different
    algorithms and the rounding of the parameter estimates to two decimal places.
  prefs: []
  type: TYPE_NORMAL
- en: This probabilistic inference is not yet causal inference—we’ll look at examples
    combining causal inference with probabilistic inference starting in chapter 7\.
    In chapter 8, you’ll see how to use probabilistic inference to implement causal
    inference. For now, we’ll look at the benefit of parameter modularity, and at
    how parameters encode causal invariance.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F13_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 Visualization of the P(E|T=“train”) distribution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 3.2 Causal invariance and parameter modularity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we were interested in modeling the relationship between altitude and
    temperature. The two are clearly correlated; the higher up you go, the colder
    it gets. However, you know temperature doesn’t cause altitude, or heating the
    air within a city would cause the city to fly. Altitude is the cause, and temperature
    is the effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can come up with a simple causal DAG that we think captures the relationship
    between temperature and altitude, along with other causes, as shown in figure
    3.14\. Let’s have *A* be altitude, *C* be cloud cover, *L* be latitude, *S* be
    season, and *T* be temperature. The DAG in figure 3.14 has five causal Markov
    kernels: {*P*(*A*), *P*(*C*), *P*(*L*), *P*(*S*), *P*(*T*|*A*, *C*, *L*, *S*)}.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F14_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 A simple model of outdoor temperature
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To train a causal graphical model on top of this DAG, we need to learn parameters
    for each of these causal Markov kernels.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Independence of mechanism and parameter modularity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are some underlying thermodynamic mechanisms in the DGP underlying the
    causal Markov kernels in our temperature DAG. For example, the causal Markov kernel
    *P*(*T*|*A*, *C*, *L*, *S*) is the conditional probability induced by the physics-based
    mechanism, wherein altitude, cloud cover, latitude, and season drive the temperature.
    That mechanism is distinct from the mechanism that determines cloud cover (according
    to our DAG). *Independence of mechanism* refers to this distinction between mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: The independence of the mechanism leads to a property called *parameter modularity*.
    In our model, for each causal Markov kernel, we choose a parameterized representation
    of the causal Markov kernels. If *P*(*T*|*A*, *C*, *L*, *S*) and *P*(*C*) are
    distinct mechanisms, then our representations of *P*(*T*|*A*, *C*, *L*, *S*) and
    *P*(*C*) are representations of distinct mechanisms. That means we can change
    one representation without worrying about how that change affects the other representations.
    Such modularity is atypical in statistical models; you can’t usually change one
    part of a model and expect the other part to be unaffected.
  prefs: []
  type: TYPE_NORMAL
- en: One way this comes in handy is during training. Typically, when you train a
    model, you optimize all the parameters at the same time. Parameter modularity
    means you could train the parameters for each causal Markov kernel separately,
    or train them simultaneously as decoupled sets, allowing you to enjoy some dimensionality
    reduction during training. In Bayesian terms, the parameter sets are a priori
    independent (though they are generally dependent in the posterior). This provides
    a nice causal justification for using an independent prior distribution for each
    causal Markov kernel’s parameter set.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Causal transfer learning, data fusion, and invariant prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may not be a climatologist or a meteorologist. Still, you know the relationship
    between temperature and altitude has something to do with air pressure, climate,
    sunlight, and such. You also know that whatever the physics of that relationship
    is, the physics is the same in Katmandu as it is in El Paso. So, when we train
    a causal Markov kernel on data solely collected from Katmandu, we learn a causal
    representation of a mechanism that is invariant between Katmandu and El Paso.
    This invariance helps with transfer learning; we should be able to use that trained
    causal Markov kernel to make inferences about the temperature in El Paso.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are caveats to leveraging this notion of causal invariance.
    For example, this assumes your causal model is correct and that there is enough
    information about the underlying mechanism in the Katmandu data to effectively
    apply what you’ve learned about that mechanism in El Paso.
  prefs: []
  type: TYPE_NORMAL
- en: Several advanced methods lean heavily on causal invariance and independence
    of mechanism. For example, *causal data fusion* uses this idea to learn a causal
    model by combining multiple datasets. *Causal transfer learning* uses causal invariance
    to make causal inferences using data outside the domain of the training data.
    *Causal invariant prediction* leverages causal invariance in prediction tasks.
    See the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Fitting parameters with common sense
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the temperature model, we have an intuition about the physics of the mechanism
    that induces *P*(*T*|*A*, *C*, *L*, *S*). In non-natural science domains, such
    as econometrics and other social sciences, the “physics” of the system is more
    abstract and harder to write down. Fortunately, we can rely on similar invariance-based
    intuition in these non-natural science domains. In these domains, we can still
    assume the causal Markov kernels correspond to distinct causal mechanisms in the
    real world, assuming the model is true. For example, recall *P*(*T*|*O*, *R*)
    in our transportation model. We still assume the underlying mechanism is distinct
    from the others; if there were changes to the mechanism underlying *P*(*T*|*O*,
    *R*), only *P*(*T*|*O*, *R*) should change—other kernels in the model should not.
    If something changes the mechanism underlying *P*(*R*|*E*), the causal Markov
    kernel for *R*, this change should affect *P*(*R*|*E*) but have no effect on the
    parameters of *P*(*T*|*O*, *R*).
  prefs: []
  type: TYPE_NORMAL
- en: This invariance can help us estimate parameters *without* statistical learning
    by reasoning about the underlying causal mechanism. For example, let’s look again
    at the causal Markov kernel *P*(*R*|*E*) (recall *R* is residence, *E* is education).
    Let’s try to reason our way to estimates of the parameters of this distribution
    without using statistical learning.
  prefs: []
  type: TYPE_NORMAL
- en: People who don’t get more than a high school degree are more likely to stay
    in their hometowns. However, people from small towns who attain college degrees
    are likely to move to a big city where they can apply their credentials to get
    higher-paying jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s think about US demographics. Suppose a web search tells you that 80%
    of the US lives in an urban area (*P*(*R*=big) = .8), while 95% of college degree
    holders live in an urban area (*P*(*R*=big|*E*=uni) = .95). Further, 25% of the
    overall adult population in the US has a university degree (*P*(*E*=uni) = .25).
    Then, with some back-of-the-envelope math, you calculate your probability values
    as *P*(*R*=small|*E*=high)=.25, *P*(*R*=big|*E*=high) = .75, *P*(*R*=small|*E*=uni)
    = .05, and *P*(*R*=big|*E*=uni) = .95\. The ability to calculate parameters in
    this manner is particularly useful if data is unavailable for parameter learning.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Your causal question scopes the DAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a modeler meets a problem for the first time, there is often already a
    set of available data, and a common mistake is to define your DAG using only the
    variables in that data. Letting the data scope your DAG is attractive, because
    you don’t have to decide what variables to include in your DAG. But causal modelers
    model the DGP, not the data. The true causal structure in the world doesn’t care
    about what happens to be measured in your dataset. In your causal DAG, you should
    include causally relevant variables whether they are in your dataset or not.
  prefs: []
  type: TYPE_NORMAL
- en: But if the data doesn’t define the DAG’s scope, what does? While your data has
    a fixed set of variables, the variables that could comprise your DGP are only
    bounded by your imagination. Given a variable, you could include its causes, those
    causes’ causes, those causes’ causes’ causes, continuing all the way back to Aristotle’s
    “prime mover,” the single cause of everything. Fortunately, there is no need to
    go back that far. Let’s look at a procedure you can use to select variables for
    inclusion in your causal DAG.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Selecting variables for inclusion in the DAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall that there are several kinds of causal inference questions. As I mentioned
    in chapter 1, causal effect inference is the most common type of causal question.
    I use causal effect inference as an example, but this workflow is meant for all
    types of causal questions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Include variables central to your causal question(s)*—The first step is to
    include all the variables central to your causal question. If you intend to ask
    multiple questions, include all the variables relevant to those questions. As
    an example, consider figure 3.15\. Suppose that we intend to ask about the causal
    effect of *V* on *U* and *Y*. These become the first variables we include in the
    DAG.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Include any common causes for the variables in step 1*—Add any common causes
    for the variables you included in the first step. In our example, you would start
    with variables *U*, *V* and *Y* in figure 3.15, and trace back their causal lineages
    and identify shared ancestors. These shared ancestors are common causes. In figure
    3.16, *W*[0], *W*[1], and *W*[2] are common causes of *V*, *U*, and *Y*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F15_Ness.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.15 First include variables central to your causal question(s). Here,
    suppose you are interested in asking questions about *V*, *U*, and *Y*.
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F16_Ness.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.16 Satisfy causal sufficiency; include common causes to the variables
    from step 1.
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
- en: In formal terms, a variable is a common cause *Z* of a pair of variables *X*
    and *Y* if there is a directed path from *Z* to *X* that does not include *Y*
    and a directed path from *Z* to *Y* that does not include *X*. The formal principle
    of including common causes is called *causal sufficiency*. A set of variables
    is causally sufficient if it doesn’t exclude any common causes between any pair
    of variables in the set. Furthermore, once you include a common cause, you don’t
    have to include earlier common causes on the same paths. For example, figure 3.17
    illustrates how we might exclude variables’ earlier common causes. ![figure](../Images/CH03_F17_Ness.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.17 Once you include a common cause, you don’t have to include any earlier
    common causes on the same paths to the step 1 variables.
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
- en: In figure 3.17, *W*[2] is on *W*[0]’s path to *Y* and *U*, but we include *W*[0]
    because it has its own path to *V*. In contrast, while *C* is a common cause of
    *V*, *Y*, and *U*, *W*[0] is on all of *C*’s paths to *V*, *Y* and *U*, so we
    can exclude it after including *W*[0]. Similarly, *W*[2] lets us exclude *E*,
    and *W*[0] and *W*[2] together let us exclude *D*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Include variables that may be useful in causal inference statistical analysis*—Now
    we include variables that may be useful in statistical methods for the causal
    inferences you want to make. For example, in figure 3.18, suppose you were interested
    in estimating the cause effect of *V* on *Y*. You might want to include possible
    “instrumental variables.” We’ll define these formally in part 4 of this book,
    but for now in a causal effect question, an *instrument* is a parent of a variable
    of interest, and it can help in statistical estimation of the causal effect. In
    figure 3.18, *Z* can function as an instrumental variable. You do not need to
    include *Z* for causal sufficiency, but you might choose to include it to help
    with quantifying the causal effect.![figure](../Images/CH03_F18_Ness.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.18 Include variables that may be useful in the causal inference statistical
    analysis. *W*’s are confounders, *Z*’s are instruments, *X*’s are effect modifiers,
    *Y* is the outcome, *V* is a treatment, and U is a front door mediator.
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
- en: Similarly, *X*[0] and *X*[1] could also be of use in the analysis by accounting
    for other sources of variation in *Y*. We could potentially use them to reduce
    variance in the statistical estimation of a causal effect. Alternatively, we may
    be interested in the *heterogeneity* of the causal effect (how the causal effect
    varies) across subsets of the population defined by *X*[0] and *X*[1]. We’ll look
    at causal effect heterogeneity more closely in chapter 11\.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Include variables that help the DAG communicate a complete story*—Finally,
    include any variables that help the DAG better function as a communicative tool.
    Consider the common cause *D* in figure 3.19.![figure](../Images/CH03_F19_Ness.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.19 Include variables that help the DAG tell a complete story. In this
    example, despite having excluded *D* in step 2 (figure 3.17) we still might want
    to include *D* if it has communicative value.
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
- en: In figure 3.17, we concluded that the common cause *D* could be excluded after
    including common causes *W*[0] and *W*[2]. But perhaps *D* is an important variable
    in how domain experts conceptualize the domain. While it is not useful in quantifying
    the causal effect of *V* on *U* and *Y*, leaving it out might feel awkward. If
    so, including it may help the DAG tell a better story by showing how a key variable
    relates to the variables you included. When your causal DAG tells a convincing
    story, your causal analysis is more convincing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.3.2 Including variables in causal DAGs by their role in inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many experts in causal inference de-emphasize writing their assumptions in the
    form of a causal DAG in favor of specifying a set of relevant variables, according
    to their *role* in causal inference calculations. Focusing on variable-role-in-inference
    over a causal DAG is common in econometrics pedagogy. Examples of such roles include
    terms I’ve already introduced, such as “common cause,” “instrumental variable,”
    and “effect modifier.” Again, we’ll define these formally in chapter 11.
  prefs: []
  type: TYPE_NORMAL
- en: For now, I want to make clear that this is not a competing paradigm. An economist
    might say they are interested in the causal effect of *V* on *U*, conditional
    on some “*effect modifiers*,” and that they plan to “*adjust for* the influence
    of *common causes”* using an “*instrumental variable*.” These roles all correspond
    to structure in a causal DAG; common causes of *U* and *V* in figure 3.19 are
    *W*[0], *W*[1], and *W*[2]. *Z* is an instrumental variable, and *X*[0] and *X*[1]
    are effect modifiers. Assuming variables with these roles are important to your
    causal effect estimation analysis is implicitly assuming that your DGP follows
    the causal DAG with this structure.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, given a set of variables and their roles, we can construct the implied
    causal DAG on that set. The DoWhy causal inference library shows us how.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.10 Creating a DAG based on roles in causal effect inference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 datasets.linear_ dataset generates a ΔAG from the specified variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Add one treatment variable, like V in figure 3.19.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Z in figure 3.19 is an example of an instrumental variable; a variable that
    is a cause of the treatment, but its only causal path to the outcome is through
    the treatment. Here we create two instruments.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 X [0] and X [1] in figure 3.19 are examples of “effect modifiers” that help
    model heterogeneity in the causal effect. ΔoWhy defines these as other causes
    of the outcome (though they needn’t be). Here we create two effect modifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 We add 5 common causes, like the three W [0], W [1], and W [2] in figure
    3.19\. Unlike the nuanced structure between these variables in figure 3.19, the
    structure here will be simple.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Front door variables are on the path between the treatment and the effect,
    like U in figure 3.19\. Here we add one.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 This code extracts the graph, creates a plotting layout, and plots the graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 This code extracts the graph, creates a plotting layout, and plots the graph.'
  prefs: []
  type: TYPE_NORMAL
- en: This code produces the DAG pictured in figure 3.20.
  prefs: []
  type: TYPE_NORMAL
- en: This role-based approach produces a simple template causal DAG. It won’t give
    you the nuance that we have in figure 3.19, and it will exclude the good storytelling
    variables that we added in step 4, like *D* in figure 3.19\. But it will be enough
    for tackling the predefined causal effect query. It’s a great tool to use when
    working with collaborators who are skeptical of DAGs but are comfortable talking
    about variable roles. But don’t believe claims that this approach is DAG-free.
    The DAG is just implicit in the assumptions underlying the specification of the
    roles.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F20_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 A causal DAG built by specifying variables by their role in causal
    effect inference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Such a template method could be used for other causal queries as well. You can
    also use this approach to get a basic causal DAG in a first step, which you could
    then build upon to produce a more nuanced graph.
  prefs: []
  type: TYPE_NORMAL
- en: '3.4 Looking ahead: Model testing and combining causal graphs with deep learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The big question when building a causal DAG is “what if my causal DAG is wrong?”
    How can we be confident in our selected DAG? In the next chapter, we’ll look at
    how to use data to stress test our causal DAG. A key insight will be that while
    data can never prove that a causal DAG is right, it can help show when it is wrong.
    You’ll also learn about causal discovery, a set of algorithms for learning causal
    DAGs from data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we explored building a simple causal graphical model on the
    DAG structure using pgmpy. Throughout the book, you’ll see how to build more sophisticated
    causal graphical models that leverage neural networks and automatic differentiation.
    Even in those more sophisticated models, the causal Markov property and the benefits
    of the DAG including causal invariance, and parameter modularity will still hold.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The causal directed acyclic graph (DAG) can represent our causal assumptions
    about the data generating process (DGP).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The causal DAG is a useful tool for visualizing and communicating your causal
    assumptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DAGs are fundamental data structures in computer science, and they admit many
    fast algorithms we can bring to bear on causal inference tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DAGs link causality to conditional independence via the causal Markov property.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DAGs can provide scaffolding for probabilistic ML models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use various methods for statistical parameter learning to train a probabilistic
    model on top of a DAG. These include maximum likelihood estimation and Bayesian
    estimation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a causal DAG, the modeler can choose from a variety of parameterizations
    of the causal Markov kernels in the DAG, ranging from conditional probability
    tables to regression models to neural networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A causally sufficient set of variables contains all common causes between pairs
    in that set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can build a causal DAG by starting with a set of variables of interest,
    expanding that to a causally sufficient set, adding variables useful to causal
    inference analysis, and finally adding any variables that help the DAG communicate
    a complete story.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each causal Markov kernel represents a distinct causal mechanism that determines
    how the child node is determined by its parents (assuming the DAG is correct).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Independence of mechanism” refers to how mechanisms are distinct from the others—a
    change to one mechanism does not affect the others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you build a generative model on the causal DAG, the parameters of each
    causal Markov kernel represents an encoding of the underlying causal mechanism.
    This leads to “parameter modularity,” which enables you to learn each parameter
    set separately and even use common sense reasoning to estimate parameters instead
    of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fact that each causal Markov kernel represents a distinct causal mechanism
    provides a source of invariance that can be leveraged in advanced tasks, like
    transfer learning, data fusion, and invariant prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can specify a DAG by the roles variables play in a specific causal inference
    task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
