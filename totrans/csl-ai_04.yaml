- en: 3 Building a causal graphical model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 构建因果图形模型
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Building a causal DAG to model a DGP
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建因果DAG来模拟DGP
- en: Using your causal graph as a communication, computation, and reasoning tool
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用你的因果图作为沟通、计算和推理工具
- en: Building a causal DAG in pgmpy and Pyro
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在pgmpy和Pyro中构建因果DAG
- en: Training a probabilistic machine learning model using the causal DAG as a scaffold
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用因果DAG作为支架训练概率机器学习模型
- en: In this chapter, we’ll build our first models of the data generating process
    (DGP) using the *causal directed acyclic graph* (causal DAG)—a directed graph
    without cycles, where the edges represent causal relationships. We’ll also look
    at how to train a statistical model using the causal DAG as a scaffold.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用*因果有向无环图*（因果DAG）构建数据生成过程（DGP）的第一个模型——一个没有环的有向图，其中边代表因果关系。我们还将探讨如何使用因果DAG作为支架来训练统计模型。
- en: 3.1 Introducing the causal DAG
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 引入因果DAG
- en: Let’s assume we can partition the DGP into a set of variables where a given
    combination of variable values represents a possible state of the DGP. Those variables
    may be discrete or continuous. They can be univariate, or they can be multivariate
    vectors or matrices.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们可以将DGP划分为一组变量，其中给定的一组变量值代表DGP的可能状态。这些变量可以是离散的或连续的。它们可以是单变量，也可以是多变量向量或矩阵。
- en: A causal DAG is a directed graph where the nodes are this set of variables and
    the directed edges represent the causal relationships between them. When we use
    a causal DAG to represent the DGP, we assume the edges reflect true causality
    in the DGP.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因果DAG是一个有向图，其中节点是这组变量，有向边代表它们之间的因果关系。当我们使用因果DAG来表示DGP时，我们假设边反映了DGP中的真实因果关系。
- en: To illustrate, recall the rock-throwing DGP from chapter 2\. We started with
    Jenny and Brian having a certain amount of inclination to throw rocks at a window,
    which has a certain amount of strength. If either person’s inclination to throw
    surpasses a threshold, they throw. The window breaks depending on whether either
    or both of them throw and the strength of the window.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，回想一下第二章中的投掷石头DGP。我们开始时，珍妮和布莱恩有一定倾向去砸窗户，窗户有一定的强度。如果其中任何一个人的砸窗倾向超过了一个阈值，他们就会砸。窗户是否破碎取决于他们是否砸或者两者都砸，以及窗户的强度。
- en: Setting up your environment
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设置你的环境
- en: The code in this chapter was written with pgmpy version 0.1.24, pyro-ppl version
    1.8.6, and DoWhy version 0.11.1\. Version 0.20.1 of Python’s Graphviz library
    was used to draw an image of a DAG, and this depends on having the core Graphviz
    software installed. Comment out the Graphviz code if you would prefer not to set
    up Graphviz for now.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码是用pgmpy版本0.1.24，pyro-ppl版本1.8.6，和DoWhy版本0.11.1编写的。Python的Graphviz库的版本0.20.1用于绘制DAG的图像，这取决于是否安装了核心Graphviz软件。如果你现在不想设置Graphviz，请注释掉Graphviz代码。
- en: See the book’s notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for links to the Jupyter notebooks with the code.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅本书的笔记[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)，以获取包含代码的Jupyter笔记本的链接。
- en: We’ll now create a causal DAG that will visualize this process. As a Python
    function, the DGP is shown in the following listing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个因果DAG来可视化这个过程。作为一个Python函数，DGP在下面的列表中展示。
- en: Listing 3.1 DAG rock-throwing example
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1 DAG投掷石头示例
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Input variables are numbers between 0 and 1.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 输入变量是介于0和1之间的数字。'
- en: '#2 Jenny and Brian throw the rock if so inclined.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 如果珍妮和布莱恩有砸石头的倾向，他们会砸石头。'
- en: '#3 If both throw the rock, the strength of impact is .8.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 如果两人都砸石头，冲击力为0.8。'
- en: '#4 If one of them throws, the strength of impact is .6.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 如果其中一个人砸，冲击力为0.6。'
- en: '#5 If neither throws, the strength of impact is 0.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 如果两人都不砸，冲击力为0。'
- en: '#6 The window breaks if the strength of impact is greater than the window strength.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 如果冲击力大于窗户强度，窗户会破碎。'
- en: Figure 3.1 illustrates the rock-throwing DGP as a causal DAG.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1展示了作为因果DAG的投掷石头DGP。
- en: In figure 3.1, each node corresponds to a random variable in the DGP. The directed
    edges correspond to cause-effect relationships (the source node is the cause and
    the target node is the effect).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.1中，每个节点对应DGP中的一个随机变量。有向边对应因果关系（源节点是原因，目标节点是结果）。
- en: '![figure](../Images/CH03_F01_Ness.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F01_Ness.png)'
- en: Figure 3.1 A causal DAG representing the rock-throwing DGP. In this example,
    each node corresponds to a random variable in the DGP.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1 表示投掷石头DGP的因果DAG。在这个例子中，每个节点对应DGP中的一个随机变量。
- en: '3.1.1 Case study: A causal model for transportation'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 案例研究：交通的因果模型
- en: In this chapter, we’ll look at a model of people’s choice of transportation
    on their daily commutes. This example will make overly strong assumptions (to
    the point of being borderline offensive) that will help illustrate the core ideas
    of model building. You’ll find links to the accompanying code and tutorials at
    [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨人们日常通勤选择交通方式的模型。这个例子将做出过于强烈的假设（以至于接近冒犯），这将有助于说明模型构建的核心思想。你可以在[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)找到相关的代码和教程链接。
- en: Suppose you were an urban planning consultant trying to model the relationships
    between people’s demographic background, the size of the city where they live,
    their job status, and their decision on how to commute to work each day.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一名城市规划顾问，试图模拟人们的人口背景、他们居住的城市规模、他们的工作状态以及他们每天如何通勤到工作的决策之间的关系。
- en: 'You could break down the key variables in the system as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将系统中的关键变量分解如下：
- en: '*Age (A)*—The age of an individual'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*年龄 (A)*—个人的年龄'
- en: '*Gender (S)*—An individual’s reported gender (using “S” instead of “G,” since
    “G” is usually reserved for DAGs)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性别 (S)*—个人报告的性别（使用“S”而不是“G”，因为“G”通常保留用于 DAG）'
- en: '*Education (E)*—The highest level of education or training completed by an
    individual'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*教育 (E)*—个人完成的教育或培训的最高水平'
- en: '*Occupation (O)*—An individual’s occupation'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*职业 (O)*—个人的职业'
- en: '*Residence (R)*—The size of the city the individual resides in'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*居住地 (R)*—个人居住的城市规模'
- en: '*Travel (T)*—The means of transport favored by the individual'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*出行 (T)*—个人偏好的交通方式'
- en: 'You could then think about the causal relationships between these variables,
    using knowledge about the domain. Here is a possible narrative:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以考虑这些变量之间的因果关系，使用对该领域的了解。以下是一个可能的叙述：
- en: Educational standards are different across generations. For older people, a
    high school degree was sufficient to achieve a middle-class lifestyle, but younger
    people need at least a college degree to achieve the same lifestyle. Thus, age
    (*A*) is a cause of education (*E*).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教育标准在不同代人之间是不同的。对于老年人来说，高中学位就足以实现中产阶级的生活方式，但年轻人至少需要大学学位才能达到同样的生活方式。因此，年龄 (*A*)
    是教育 (*E*) 的原因。
- en: Similarly, a person’s gender is often a factor in their decision to pursue higher
    levels of education. So, gender (*S*) is a cause of education (*E*).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，一个人的性别通常是他们决定追求更高教育水平的一个因素。因此，性别 (*S*) 是教育 (*E*) 的原因。
- en: Many white-collar jobs require higher education. Many credentialed professions
    (e.g., doctor, lawyer, or accountant) certainly require higher education. So education
    (*E*) is a cause of occupation (*O*).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多白领工作需要较高的教育水平。许多认证职业（例如，医生、律师或会计师）肯定需要较高的教育水平。因此，教育 (*E*) 是职业 (*O*) 的原因。
- en: White-collar jobs that depend on higher levels of education tend to cluster
    in urban areas. Thus, education (*E*) is a cause of where people reside (*R*).
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于较高教育水平的白领工作往往集中在城市地区。因此，教育 (*E*) 是人们居住地点 (*R*) 的原因。
- en: People who are self-employed might work from home and therefore don’t need to
    commute, while people with employers do. Thus, occupation (*O*) is a cause of
    transportation (*T*).
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自雇人士可能在家工作，因此不需要通勤，而雇佣他人的人则需要。因此，职业 (*O*) 是交通 (*T*) 的原因。
- en: People in big cities might find it more convenient to commute by walking or
    using public transportation, while people in small cities and towns rely on cars
    to get around. Thus, residence (*R*) is a cause of transportation (*T*).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大城市的人可能发现步行或使用公共交通通勤更方便，而小城市和镇上的人则依赖汽车出行。因此，居住地 (*R*) 是交通 (*T*) 的原因。
- en: You could have created this narrative based on your knowledge about the domain,
    or based on your research into the domain. Alternatively, you could have consulted
    with a domain expert, such as a social scientist who specializes in this area.
    Finally, you could reduce this narrative to the causal DAG shown in figure 3.2.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据你对该领域的了解，或者基于对该领域的研究来构建这个叙述。或者，你也可以咨询该领域的专家，例如专门研究这个领域的社会科学家。最后，你可以将这个叙述简化为图
    3.2 所示的因果 DAG。
- en: '![figure](../Images/CH03_F02_Ness.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F02_Ness.png)'
- en: Figure 3.2 A causal DAG representing a model of the causal factors behind how
    people commute to work
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.2 代表人们通勤到工作背后的因果因素的因果 DAG
- en: You could build this causal DAG using the following code.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下代码构建这个因果 DAG。
- en: Listing 3.2 Building the transportation DAG in pgmpy
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2 在pgmpy中构建运输DAG
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 pgmpy provides a BayesianNetwork class where we add the edges to the model.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 pgmpy提供了一个BayesianNetwork类，我们可以将边添加到模型中。'
- en: '#2 Input the ΔAG as a list of edges (tuples).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将ΔAG作为边（元组）列表输入。'
- en: '#3 Input the ΔAG as a list of edges (tuples).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将ΔAG作为边（元组）列表输入。'
- en: The `BayesianNetwork` object in pgmpy is built on the `DiGraph` class from NetworkX,
    the preeminent graph modeling library in Python.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: pgmpy中的`BayesianNetwork`对象建立在NetworkX的`DiGraph`类之上，NetworkX是Python中首屈一指的图建模库。
- en: Causal abstraction and causal representation learning
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 因果抽象和因果表示学习
- en: In modeling, the *level of abstraction* refers to the level of detail and granularity
    of the variables in the model. In figure 3.2, there is a mapping between the variables
    in the data and the variables in the causal DAG, because the level of abstraction
    in the data generated by the DGP and the level of abstraction of the causal DAG
    are the same. But it is possible for variables in the data to be at a different
    levels of abstraction. This is particularly common in machine learning, where
    we often deal with low-level features, such as pixels.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模中，*抽象级别*指的是模型中变量的详细程度和粒度。在图3.2中，数据中的变量和因果DAG中的变量之间存在映射，因为DGP生成数据的抽象级别和因果DAG的抽象级别是相同的。但是，数据中的变量可能处于不同的抽象级别。这在机器学习中尤为常见，我们经常处理低级特征，如像素。
- en: When the level of abstraction in the data is lower than the level the modeler
    wants to work with, the modeler must use domain knowledge to derive the high-level
    abstractions that will appear as nodes in the DAG. For example, a doctor may be
    interested in a high-level binary variable node like “Tumor (present/absent),”
    while the data itself contains low-level variables such as a matrix of pixels
    from medical imaging technology.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据的抽象级别低于建模者想要工作的级别时，建模者必须使用领域知识推导出将作为DAG节点出现的高级抽象。例如，医生可能对像“肿瘤（存在/不存在）”这样的高级二元变量节点感兴趣，而数据本身包含来自医学成像技术的像素矩阵等低级变量。
- en: That doctor must look at each image in the dataset and manually label the high-level
    tumor variable. Alternatively, a modeler can use analytical means (e.g., math
    or logic) to map low-level abstractions to high-level ones. Further, they must
    do so in a way that preserves causal assumptions about the DGP.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 那位医生必须查看数据集中的每一张图像，并手动标记高级肿瘤变量。或者，建模者可以使用分析手段（例如数学或逻辑）将低级抽象映射到高级抽象。此外，他们必须以保留DGP因果假设的方式来做。
- en: This task of creating high-level variables from lower-level ones in a causally
    rigorous way is called *causal abstraction*. In machine learning, the term “feature
    engineering” applies to the task of computing *useful* high-level features from
    lower-level features. Causal abstraction differs in that requirement for causal
    rigor. You’ll find some sources for causal abstraction information in the book’s
    notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从低级变量以因果严谨的方式创建高级变量的任务被称为*因果抽象*。在机器学习中，术语“特征工程”适用于从低级特征计算*有用*的高级特征的任务。因果抽象与因果严谨性的要求不同。你可以在本书的注释中找到一些因果抽象的信息，网址为[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)。
- en: Another approach to learning high-level causal abstractions from lower ones
    in data is to use deep learning—this is called *causal representation learning*.
    We’ll touch briefly on this topic in chapter 5.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中的低级抽象学习高级因果抽象的另一种方法是使用深度学习——这被称为*因果表示学习*。我们将在第5章简要介绍这个主题。
- en: 3.1.2 Why use a causal DAG?
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 为什么使用因果DAG？
- en: The causal DAG is the best-known representation of causality, but to understand
    its value, it’s useful to think about other ways of modeling causality. One alternative
    is using a mathematical model, such as a set of ordinary differential equations
    or partial differential equations, as is common in physics and engineering. Another
    option is to use computational simulators, such as are used in meteorology and
    climate science.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因果DAG是因果性最著名的表示，但要理解其价值，思考其他建模因果性的方法是有用的。一种替代方法是使用数学模型，例如一组常微分方程或偏微分方程，这在物理学和工程学中很常见。另一种选择是使用计算模拟器，如气象学和气候科学中使用的。
- en: In contrast to those alternatives, a causal DAG requires a much less mathematically
    detailed understanding of the DGP. A causal DAG only requires you to specify what
    causes what, in the form of a graph. Graphs are easy for humans to think about;
    they are the go-to method for making sense of complicated domains.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与那些替代方案相比，因果DAG对DGP的数学理解要求要少得多。因果DAG只要求你以图形的形式指定什么导致什么。图形对人类来说很容易思考；它们是理解复杂领域的方法。
- en: 'Indeed, there are several benefits of using a causal DAG as a representation
    of the DGP:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，使用因果DAG作为DGP表示的几个好处包括：
- en: DAGs are useful in communicating and visualizing causal assumptions.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAGs在沟通和可视化因果假设方面很有用。
- en: We have many tools for computing over DAGs.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有许多工具可以用于在DAG上进行计算。
- en: Causal DAGs represent time.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果DAG表示时间。
- en: DAGs link causality to conditional independence.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAGs将因果关系与条件独立性联系起来。
- en: DAGs can provide scaffolding for probabilistic ML models.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAGs可以为概率机器学习模型提供支架。
- en: The parameters in those probabilistic ML models are modular parameters, and
    they encode causal invariance.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那些概率机器学习模型中的参数是模块化参数，并且它们编码了因果不变性。
- en: Let’s review these benefits one at a time.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一回顾这些好处。
- en: 3.1.3 DAGs are useful in communicating and visualizing causal assumptions
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 DAGs在沟通和可视化因果假设方面很有用
- en: A causal DAG is a powerful communication device. Visual communication of information
    involves highlighting important information at the expense of other information.
    As an analogy, consider the two maps of the London Underground in figure 3.3\.
    The map on the left is geographically accurate. The simpler map on the right ignores
    the geographic detail and focuses on the position of each station relative to
    other stations, which is, arguably, all one needs to find their way around London.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因果DAG是一种强大的沟通工具。信息视觉沟通涉及以牺牲其他信息为代价突出重要信息。作为一个类比，考虑图3.3中的两个伦敦地铁图。左边的地图在地理上是准确的。右边的简单地图忽略了地理细节，专注于每个车站相对于其他车站的位置，这可能是找到在伦敦周围路线所需的所有信息。
- en: '![figure](../Images/CH03_F03_Ness.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F03_Ness.png)'
- en: Figure 3.3 Visual communication is a powerful use case for a graphical representation.
    For instance, the map of the London Underground on the left is geographically
    accurate, while the one on the right trades that accuracy for a clear representation
    of each station’s position relative to the others. The latter is more useful for
    train riders than the one with geographic accuracy. Similarly, a causal DAG abstracts
    away much detail of the causal mechanism to create a simple representation that
    is easy to reason about visually.
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3 图形表示是视觉沟通的一个强大用例。例如，左边的伦敦地铁图在地理上是准确的，而右边的则为了更清晰地表示每个车站相对于其他车站的位置而牺牲了这种准确性。对于乘客来说，后者比地理准确性的地图更有用。同样，因果DAG抽象掉了因果机制的大部分细节，以创建一个简单且易于视觉推理的表示。
- en: Similarly, a causal DAG highlights causal relationships while ignoring other
    things. For example, the rock-throwing DAG ignores the if-then conditional logic
    of how Jenny and Brian’s throws combined to break the window. The transportation
    DAG says nothing about the types of variables we are dealing with. Should we consider
    age (*A*) in terms of continuous time, integer years, categories like young/middle-aged/elderly,
    or intervals like 18–29, 30–44, 45–64, and >65? What are the categories of the
    transportation variable (*T*)? Could the occupation variable (*O*) be a multidimensional
    tuple like {employed, engineer, works-from-home}? The DAG also fails to capture
    which of these variables are observed in the data, and the number of data points
    in that data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，因果有向图（DAG）突出了因果关系，而忽略了其他事物。例如，投掷石头的DAG忽略了珍妮和布莱恩的投掷如何结合在一起打破窗户的if-then条件逻辑。运输DAG没有提及我们正在处理的变量类型。我们应该将年龄（*A*）视为连续时间、整数年、如年轻/中年/老年这样的类别，还是18-29、30-44、45-64和>65这样的区间？运输变量（*T*）的分类是什么？职业变量（*O*）是否可以是如{就业、工程师、在家工作}这样的多维元组？DAG还未能捕捉到这些变量中哪些在数据中被观察到，以及该数据中的数据点数量。
- en: Causal DAGs don’t illustrate mechanism
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 因果DAG没有说明机制
- en: A causal DAG also doesn’t visualize interactions between causes. For example,
    in older generations, women were less likely to go to college than men. In younger
    generations, the reverse is true. While both age (*A*) and gender (*S*) are causes
    of education (*E*), you can’t look at the DAG and see anything about how age and
    gender interact to affect education.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因果有向图也没有可视化原因之间的交互。例如，在上一代人中，女性上大学的机会比男性少。在年轻一代中，情况相反。虽然年龄(*A*)和性别(*S*)都是教育(*E*)的原因，但你无法从DAG中看到关于年龄和性别如何相互作用影响教育的内容。
- en: More generally, DAGs can’t convey any information about the causal mechanism
    or *how* the causes impact the effect. They only establish the *what* of causality,
    as in *what* causes *what*. Consider, for example, the various logic gates in
    figure 3.4\. The input binary values for *A* and *B* determine the output differently
    depending on the type of logic gate. But if we represent a logic gate as a causal
    DAG, then all the logic gates have the same causal DAG. We can use the causal
    DAG as a scaffold for causal graphical models that capture this logic, but we
    can’t *see* the logic in the DAG.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地说，DAGs无法传达关于因果机制或*如何*原因影响效果的任何信息。它们只建立了因果的*什么*，即*什么*导致*什么*。以图3.4中的各种逻辑门为例。*A*和*B*的输入二进制值根据逻辑门类型的不同而决定输出不同。但如果我们用一个因果有向图来表示逻辑门，那么所有逻辑门都有相同的因果有向图。我们可以使用因果有向图作为捕捉这种逻辑的因果图形模型的支架，但我们无法*看到*在图中逻辑。
- en: '![figure](../Images/CH03_F04_Ness.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F04_Ness.png)'
- en: Figure 3.4 The various kinds of logic gates all have the same causal DAG.
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4 各种逻辑门都有相同的因果有向图。
- en: This is a strength and a weakness. A causal DAG simplifies matters by communicating
    what causes what, but not *how*. However, in some cases (such as logic gates),
    visualizing the *how* would be desirable.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这既是优点也是缺点。因果有向图（DAG）通过说明什么导致什么来简化问题，但并不说明*如何*。然而，在某些情况下（例如逻辑门），可视化*如何*将是可取的。
- en: Causal DAGs represent causal assumptions
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 因果有向图代表了因果假设
- en: A causal DAG represents the modeler’s assumptions and beliefs about the DGP,
    because we don’t have access to that process most of the time. Thus, a causal
    DAG allows us to visualize our assumptions and communicate them to others.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因果有向图代表了模型者对DGP的假设和信念，因为我们大多数时候无法访问那个过程。因此，因果有向图使我们能够可视化我们的假设并将它们传达给他人。
- en: Beyond this visualization and communication, the benefits of a causal DAG are
    mathematical and computational (I’ll explain these in the following subsections).
    Causal inference researchers vary in their opinions on the degree to which these
    mathematical and computational properties of causal DAGs are practically beneficial.
    However, most agree on the fundamental benefit of visualization and communication
    of causal assumptions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这种可视化和沟通之外，因果有向图的益处是数学的和计算的（我将在下一个小节中解释这些）。因果推断研究人员在因果有向图的数学和计算属性在实践中的益处程度上意见不一。然而，大多数人同意可视化和沟通因果假设的基本益处。
- en: The assumptions encoded in a causal DAG are strong. Let’s look again at the
    transportation DAG from figure 3.2, shown again in figure 3.5\. Consider the alternatives
    to that DAG; how many possible DAGs could we draw on this simple six-node system?
    The answer is 3,781,503, so when we use a causal DAG to communicate our assumptions
    about this system, we’re communicating our top choice over 3,781,502 alternatives.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因果有向图中编码的假设是强烈的。让我们再次看看图3.2中的运输DAG，再次在图3.5中展示。考虑那个DAG的替代方案；在这个简单的六个节点系统中，我们可能画出多少个可能的DAG？答案是3,781,503，所以当我们使用因果有向图来沟通我们对这个系统的假设时，我们是在沟通我们对3,781,502个替代方案中的首选。
- en: '![figure](../Images/CH03_F05_Ness.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F05_Ness.png)'
- en: Figure 3.5 A causal DAG model of transportation choices. This DAG encodes strong
    assumptions about how these variables do and do not relate to one another.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5 运输选择的因果有向图模型。这个DAG编码了关于这些变量如何以及如何不相互关联的强烈假设。
- en: And how about some of those competing DAGs? Some of them seem plausible. Perhaps
    baby boomers prefer small-town life while millennials prefer city life, implying
    that there should be an *A* → *R* edge. Perhaps gender norms determine preferences
    and opportunities in certain professions and industries, implying an *S* → *O*
    edge. The assumption that age and gender cause occupation and residence only indirectly
    through education is a powerful assumption that would provide useful inferences
    *if* *it is right*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 那么关于那些竞争的DAG呢？其中一些看起来是合理的。也许婴儿潮一代更喜欢小镇生活，而千禧一代更喜欢城市生活，这意味着应该有一个*A* → *R*的边。也许性别规范决定了某些职业和行业中的偏好和机会，这意味着有一个*S*
    → *O*的边。年龄和性别通过教育间接导致职业和居住地的假设是一个强大的假设，如果它是正确的，将提供有用的推理。
- en: But what if our causal DAG is wrong? It seems it is likely to be wrong, given
    its 3,781,502 competitors. In chapter 4, we’ll use data to show us when the causal
    assumptions in our chosen DAG fail to hold.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们的因果DAG是错误的怎么办？考虑到它有3,781,502个竞争对手，它很可能是错误的。在第4章中，我们将使用数据来展示我们选择的DAG中的因果假设何时不成立。
- en: 3.1.4 We have many tools for computing over DAGs
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 我们有许多工具可以用于在DAG上进行计算
- en: Directed graphs are well-studied objects in math and in computer science, where
    they are a fundamental data structure. Computer scientists have used graph algorithms
    to solve many practical problems with theoretical guarantees on how long they
    will take to arrive at solutions. The programming languages commonly used in data
    science and machine learning have libraries that implement these algorithms, such
    as NetworkX in Python. These popular libraries make it easier to write code that
    works with causal DAGs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有向图在数学和计算机科学中是研究得很好的对象，它们是一个基本的数据结构。计算机科学家已经使用图算法解决了许多具有理论保证的实用问题，这些保证了他们找到解决方案所需的时间。在数据科学和机器学习中常用的编程语言都有实现这些算法的库，例如Python中的NetworkX。这些流行的库使得编写与因果DAG一起工作的代码变得更加容易。
- en: We can bring all that theory and tooling to bear on a causal modeling problem
    when we represent a causal model in the form of a causal DAG. For example, in
    pgmpy we can train a causal DAG on data to get a directed causal graphical model.
    Given that model, we can apply algorithms for graph-based probabilistic inference,
    such as *belief propagation*, to estimate conditional probabilities defined on
    variables in the graph. The directed graph structure enables these algorithms
    to work in typical settings without our needing to configure them to a specific
    problem or task.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将所有这些理论和工具应用于因果建模问题，当我们以因果有向无环图（DAG）的形式表示因果模型时。例如，在pgmpy中，我们可以使用数据来训练一个因果DAG，从而得到一个有向因果图模型。有了这个模型，我们可以应用基于图的概率推理算法，例如*信念传播*，来估计图上变量定义的条件概率。有向图结构使得这些算法能够在典型设置下工作，而无需我们针对特定问题或任务进行配置。
- en: In the next chapter, I’ll introduce the concept of *d-separation*, which is
    a graphical abstraction for conditional independence and the fundamental idea
    behind the do-calculus theory for causal inference. D-separation is all about
    finding paths between nodes in the directed graph, which is something any worthwhile
    graph library makes easy by default. Indeed, conditional independence is the key
    idea behind the third benefit of the causal DAG.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我将介绍*d-separation*的概念，它是条件独立性的图形抽象，也是因果推理的do-calculus理论的基本思想。D-separation完全是关于在有向图中寻找节点之间的路径，这是任何有价值的图形库默认情况下都使其变得容易的事情。实际上，条件独立性是因果DAG的第三个主要益处的关键思想。
- en: 3.1.5 Causal DAGs can represent time
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.5 因果DAG可以表示时间
- en: The causal DAG has an implicit representation of time. In more technical terms,
    the causal DAG provides a *partial temporal ordering* because causes precede effects
    in time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因果DAG隐含地表示了时间。更技术地说，因果DAG提供了一个*部分时间顺序*，因为原因在时间上先于效果。
- en: 'For example, consider the graph in figure 3.6\. This graph describes a DGP
    where a change in cloud cover (Cloudy) causes both a change in the state of a
    weather-activated sprinkler (Sprinkler) and the state of rain (Rain), and these
    both cause a change in the state of the wetness of the grass (Wet Grass). We know
    that a change in the state of the weather causes rain and sprinkler activation,
    and that these both cause a change in the state of the wetness of the grass. However,
    it is only a *partial* temporal ordering, because the graph doesn’t tell us which
    happens first: the sprinkler activation or the rain.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑图3.6中的图。这个图描述了一个DGP，其中云量的变化（多云）导致天气激活的喷水装置（喷水装置）和雨（雨）的状态发生变化，这两者又导致草地湿润状态（湿草）的变化。我们知道天气状态的变化会导致下雨和喷水装置激活，这两者又导致草地湿润状态的变化。然而，这只是一个*部分*时间排序，因为图没有告诉我们哪个先发生：喷水装置激活还是下雨。
- en: '![figure](../Images/CH03_F06_Ness.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F06_Ness.png)'
- en: Figure 3.6 A causal DAG representing the state of some grass (wet or dry). The
    DAG gives us a partial temporal ordering over its nodes because causes precede
    effects in time.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6 表示某些草（湿或干）状态的因果DAG。DAG给出了其节点的一个部分时间排序，因为原因在时间上先于效果。
- en: The partial ordering in figure 3.6 may seem trivial, but consider the DAG in
    figure 3.7\. Visualization libraries can use the partial ordering in the hairball-like
    DAG on the left of figure 3.7 to create the much more readable form on the right.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6中的部分排序可能看起来很平凡，但考虑图3.7中的DAG。可视化库可以使用图3.7左侧毛线球状DAG中的部分排序来创建右侧的更易读形式。
- en: '![figure](../Images/CH03_F07_Ness.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F07_Ness.png)'
- en: Figure 3.7 A visualization library can use the DAG’s partial ordering to unravel
    the hairball-like DAG on the left into the more readable form on the right.
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7 一个可视化库可以使用DAG的部分排序将左侧的毛线球状DAG展开成右侧更易读的形式。
- en: Sometimes we need a causal DAG to be more explicit about time. For example,
    we may be modeling causality in a dynamic setting, such as in the models used
    in reinforcement learning. In this case, we can make time explicit by defining
    and labeling the variables of the model, as in figure 3.8\. We can represent continuous
    time with interval variables like “Δ.” Chapter 12 will provide some concrete examples.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要一个因果DAG来更明确地表示时间。例如，我们可能在动态环境中建模因果关系，如强化学习中使用的模型。在这种情况下，我们可以通过定义和标记模型的变量来使时间明确，如图3.8所示。我们可以用像“Δ”这样的区间变量表示连续时间。第12章将提供一些具体例子。
- en: '![figure](../Images/CH03_F08_Ness.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F08_Ness.png)'
- en: Figure 3.8 If we need a causal DAG to be explicit about time, we can make time
    explicit in the definition of the variables and labeling of their nodes. We can
    represent continuous time with interval variables like “Δ.”
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8 如果我们需要一个因果DAG来明确表示时间，我们可以在变量的定义和节点标记中使时间明确。我们可以用像“Δ”这样的区间变量表示连续时间。
- en: The causal DAG doesn’t allow for any cycles. In some causal systems, relaxing
    the acyclicity constraint makes sense, such as with systems that have feedback
    loops, and some advanced causal models allow for cycles. But sticking to the simpler
    acyclic assumption allows us to leverage the benefits of the causal DAG.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因果DAG不允许有任何循环。在一些因果系统中，放松无环性约束是有意义的，例如具有反馈循环的系统，一些高级因果模型允许循环。但坚持简单的无环假设使我们能够利用因果DAG的好处。
- en: If you have cycles, sometimes you can *unroll* the cycle over time and make
    the time explicit to get acyclicity. A graph *X* ⇄ *Y* can unroll as *X*[0]→*Y*[0]
    →*X*[1]→*Y*[1] . . . . For example, you may have a cycle between supply, price,
    and demand, but perhaps you could rewrite this as price at time 0 affects supply
    and demand at time 1, which then affects price at time 2, etc.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有循环，有时你可以通过时间展开循环并使时间明确来得到无环性。一个图*X* ⇄ *Y*可以展开为*X*[0]→*Y*[0] →*X*[1]→*Y*[1]
    . . . 。例如，你可能存在供应、价格和需求之间的循环，但也许你可以将其重写为时间0的价格影响时间1的供应和需求，然后这些又影响时间2的价格，等等。
- en: 3.1.6 DAGs link causality to conditional independence
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.6 DAGs将因果关系与条件独立性联系起来
- en: Another benefit of a causal DAG is that it allows us to use causality to reason
    about conditional independence. Humans have an innate ability to reason in terms
    of causality—that’s how we get the first and second benefits of causal DAGs. But
    reasoning probabilistically doesn’t come nearly as easily. As a result, the ability
    to use causality to reason about conditional independence (a concept from probability)
    is a considerable feature of DAGs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 因果DAG的另一个好处是它允许我们利用因果关系来推理条件独立性。人类天生具有基于因果关系的推理能力——这就是因果DAG的第一个和第二个好处。但是，概率推理并不那么容易。因此，利用因果关系来推理条件独立性（一个来自概率的概念）是DAG的一个显著特征。
- en: '![figure](../Images/CH03_F09_Ness.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F09_Ness.png)'
- en: Figure 3.9 The causal relationships in the transportation DAG encode key assumptions
    about conditional independence.
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9 运输DAG中的因果关系编码了关于条件独立性的关键假设。
- en: Consider the transportation DAG, displayed again in figure 3.9\.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑运输DAG，如图3.9所示。
- en: The six variables in the DAG have a joint distribution *P*(*A*,*S*,*E*,*O*,*R*,*T*).
    Recall the chain rule from chapter 2, which says that we can factorize any joint
    probability into a chain of conditional probability factors. For example,
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: DAG中的六个变量具有联合分布 *P*(*A*,*S*,*E*,*O*,*R*,*T*)。回忆第2章中的链式法则，它说我们可以将任何联合概率分解为一系列条件概率因子。例如，
- en: '![figure](../Images/ness-ch3-eqs-0x.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch3-eqs-0x.png)'
- en: 'The chaining works for any ordering of the variables. But instead of choosing
    any ordering, we’ll choose the (partial) ordering of the causal DAG, since that
    ordering aligns with our assumptions of the causal flow of the variables in the
    DGP. Looking at figure 3.9, the ordering of variables is {(*A*, *S*), *E*, (*O*,
    *R*), *T*}. The pairs (*A*, *S*) and (*O*, *R*) are unordered. If we arbitrarily
    pick an ordering, letting *A* come before *S* and *O* come before *R*, we get
    this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则适用于任何变量的顺序。但我们将选择因果DAG的（部分）顺序，因为这种顺序与我们对DGP中变量因果流动的假设相一致。查看图3.9，变量的顺序是 {(*A*,
    *S*), *E*, (*O*, *R*), *T*}。(*A*, *S*) 和 (*O*, *R*) 这对是无序的。如果我们任意选择一个顺序，让 *A*
    在 *S* 之前，*O* 在 *R* 之前，我们得到以下结果：
- en: '![figure](../Images/ness-ch3-eqs-1x.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch3-eqs-1x.png)'
- en: 'Next, we’ll use the causal DAG to further simplify this factorization. Each
    factor is a conditional probability, so we’ll simplify those factors by *conditioning
    each node on only its parents* in the DAG. In other words, for each variable,
    we’ll look at that variable’s direct parents in the graph, then we’ll drop everything
    on the right side of the conditioning bar (|) that isn’t one of those direct parents.
    If we condition only on parents, we get the following simplification:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用因果DAG进一步简化这个分解。每个因子都是一个条件概率，因此我们将通过在DAG中仅对每个节点进行其父节点的条件化来简化这些因子。换句话说，对于每个变量，我们将查看该变量在图中的直接父节点，然后我们将丢弃条件化符号（|）右侧的所有不是直接父节点的元素。如果我们只对父节点进行条件化，我们得到以下简化：
- en: '![figure](../Images/ness-ch3-eqs-2x.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch3-eqs-2x.png)'
- en: What is going on here? Why should the causal DAG magically mean we can say *P*(*s|a*)
    is equal to *P*(*s*) and *P*(*r|o,e,s,a*) simplifies to *P*(*r**|e*)?As discussed
    in chapter 2, stating that *P*(*s*|*a*)*=**P*(*s*) and *P*(*t*|*o,r,e,s,a*)*=**P*(*t*|*o,r*)
    is equivalent to saying that *S* and *A* are independent, and *T* is conditionally
    independent of *E*, *S*, and *A*, given *O* and *R*. In other words, the causal
    DAG gives us a way to impose conditional independence constraints over the joint
    probability distribution of the variables in the DGP.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么？为什么因果DAG神奇地意味着我们可以说 *P*(*s|a*) 等于 *P*(*s*)，并且 *P*(*r|o,e,s,a*) 简化为 *P*(*r**|e*)？如第2章所述，断言
    *P*(*s*|*a*)*=**P*(*s*) 和 *P*(*t*|*o,r,e,s,a*)*=**P*(*t*|*o,r*) 等价于说 *S* 和 *A*
    是独立的，且 *T* 在给定 *O* 和 *R* 的条件下独立于 *E*、*S* 和 *A*。换句话说，因果DAG为我们提供了一种在DGP中变量的联合概率分布上施加条件独立性约束的方法。
- en: Why should we care about things being conditionally independent? Conditional
    independence makes life as a modeler easier. For example, suppose you were to
    model the transportation variable *T* with a predictive model. The predictive
    model implied by *P*(*t*|*o,r,e,s,a*) requires having features *O*, *R*, *E*,
    *S*, and *A*, while the predictive model implied by *P*(*t*|*o,r*) just requires
    features *O* and *R* to predict *T*. The latter model will have fewer parameters
    to learn, have more degrees of freedom, take less space in memory, train faster,
    etc.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要关心事物之间的条件独立性呢？条件独立性使得作为模型构建者的人生更加容易。例如，假设你想要用预测模型来建模运输变量*T*。由*P*(t|o,r,e,s,a)隐含的预测模型需要具有特征*O*、*R*、*E*、*S*和*A*，而由*P*(t|o,r*)隐含的预测模型只需要特征*O*和*R*来预测*T*。后者模型将具有更少的参数需要学习，具有更多的自由度，在内存中占用的空间更少，训练速度更快等。
- en: But why does a causal DAG give us the right to impose conditional independence?
    Let’s build some intuition about the connection between causality and conditional
    independence. Consider the example of using genetic data from family members to
    draw conclusions about an individual. For example, the Golden State Killer was
    a California-based serial killer captured using genetic genealogy. Investigators
    used DNA left by the killer at crime scenes to identify genetic relatives in public
    databases. They then triangulated from those relatives to find the killer.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么因果有向图（DAG）让我们有权施加条件独立性呢？让我们对因果关系和条件独立性之间的联系建立一些直觉。考虑使用家庭成员的遗传数据来得出关于个人的结论的例子。例如，金州杀手是一个在加利福尼亚州被捕的连环杀手，他使用遗传家谱学被捕获。调查人员使用杀手在犯罪现场留下的DNA来识别公共数据库中的遗传亲属。然后，他们从那些亲属那里进行三角测量以找到杀手。
- en: Suppose you had a close relative and a distant relative on the same line of
    ancestry. Could the distant relative provide any additional genetic information
    about you once we had already accounted for that close relative? Let’s simplify
    a bit by focusing just on blood type. Suppose the close relative was your father,
    and the distant relative was your paternal grandfather, as in figure 3.10\. Indeed,
    your grandfather’s blood type is a cause of yours. If we saw a large dataset of
    grandfather/grandchild blood type pairs, we’d see a correlation. However, your
    father’s blood type is a more direct cause, and the connection between your grandfather’s
    blood type and yours passes through your father. So, if our goal were to predict
    your blood type, and we already had your father’s blood type as a predictor, your
    paternal grandfather’s blood type could provide no additional predictive information.
    Thus, your blood type and your paternal grandfather’s blood type are conditionally
    independent, given your father’s blood type.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个近亲和一个远亲在同一条血统线上。在我们已经考虑了那个近亲之后，远亲还能提供关于你的任何额外的遗传信息吗？让我们通过只关注血型来简化一下。假设近亲是你的父亲，而远亲是你的父亲的父亲，如图3.10所示。确实，你祖父的血型是你血型的原因。如果我们看到一组祖父/孙子血型对的大数据集，我们会看到相关性。然而，你父亲的血型是一个更直接的原因，你祖父的血型和你之间的联系是通过你的父亲传递的。所以，如果我们目标是预测你的血型，而我们已经有你父亲的血型作为预测因子，你父亲的父亲的血型就不能提供任何额外的预测信息。因此，你的血型和你的父亲的父亲的血型在给定你父亲的血型的情况下是条件独立的。
- en: '![figure](../Images/CH03_F10_Ness.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F10_Ness.png)'
- en: Figure 3.10 Causality implies conditional independence. Your paternal grandfather’s
    blood type is a cause of your father’s, which is a cause of yours. You and your
    paternal grandfather’s blood types are conditionally independent given your father’s
    blood type because your father’s blood type already contains all the information
    your grandfather’s type could provide about yours.
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10 因果关系意味着条件独立性。你父亲的血型是祖父的血型的原因，而祖父的血型又是你血型的原因。给定你父亲的血型，你和你的父亲的父亲的血型在条件上是独立的，因为你的父亲的血型已经包含了祖父的血型关于你血型可能提供的一切信息。
- en: The way causality makes correlated variables conditionally independent is called
    the *causal Markov property*. In graphical terms, the causal Markov property means
    that variables are conditionally independent of their non-descendants (e.g., ancestors,
    uncles/aunts, cousins, etc.) given their parents in the graph.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因果关系使相关变量条件独立的方式被称为*因果马尔可夫性质*。在图形术语中，因果马尔可夫性质意味着在图中，变量在给定其父母的情况下，与其非后代（例如，祖先、叔叔/阿姨、堂兄弟姐妹等）条件独立。
- en: 'This “non-descendants” definition of the causal Markov property is sometimes
    called the *local Markov property*. An equivalent articulation is called the *Markov
    factorization property*, which is the property that if your causal DAG is true,
    you can factorize a joint probability into conditional probabilities of variables,
    given their parents in the causal DAG:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“非后裔”定义的因果马尔可夫属性有时被称为*局部马尔可夫属性*。一个等效的表述被称为*马尔可夫分解属性*，这是如果你的因果DAG是真实的，你可以将联合概率分解为变量在因果DAG中给定其父变量的条件概率的性质：
- en: '![figure](../Images/ness-ch3-eqs-3x.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch3-eqs-3x.png)'
- en: If our transportation DAG is a true representation of the DGP, then the local
    Markov property should hold. In the next chapter, we’ll see how to test this assumption
    with data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的运输DAG是DGP的真实表示，那么局部马尔可夫属性应该成立。在下一章中，我们将看到如何使用数据来测试这个假设。
- en: 3.1.7 DAGs can provide scaffolding for probabilistic ML models
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.7 DAGs可以为概率ML模型提供脚手架
- en: Many modeling approaches in probabilistic machine learning use a DAG as the
    model structure. Examples include directed graphical models (aka Bayesian networks)
    and latent variable models (e.g., topic models). Deep generative models, such
    as variational autoencoders, often have an underlying directed graph.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率机器学习中，许多建模方法使用有向无环图（DAG）作为模型结构。例如，包括有向图模型（也称为贝叶斯网络）和潜在变量模型（例如，主题模型）。深度生成模型，如变分自编码器，通常有一个底层的有向图。
- en: The advantage of building a probabilistic machine learning model on top of a
    causal graph is, rather obviously, that you have a probabilistic *causal* machine
    learning model. You can train it on data, and you can use it for prediction and
    other inferences, like any probabilistic machine learning model. Moreover, because
    it is built on top of a causal DAG, it is a causal model, so you can use it to
    make causal inferences.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在因果图上构建概率机器学习模型的优势，显然是，你有一个概率*因果*机器学习模型。你可以在数据上训练它，你可以用它进行预测和其他推理，就像任何概率机器学习模型一样。此外，因为它建立在因果DAG之上，所以它是一个因果模型，你可以用它来进行因果推理。
- en: A benefit that follows from providing scaffolding is that *the parameters in
    those models are modular and encode causal invariance*. Before exploring this
    benefit, let’s first build a graphical model on the transportation DAG.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从提供脚手架中获得的益处是，*这些模型中的参数是模块化的，并编码了因果不变性*。在探讨这一益处之前，让我们首先在运输DAG上构建一个图形模型。
- en: '[PRE2]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Recall our factorization of the joint probability distribution of the transportation
    variables over the ordering of the variables in the transportation DAG.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们在运输DAG中关于变量排序的联合概率分布的分解。
- en: '![figure](../Images/ness-ch3-eqs-4x.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch3-eqs-4x.png)'
- en: We have a set of factors, {*P*(*a*)*, P*(*s*)*, P*(*e*|*s,a*)*, P*(*o*|*e*)*,
    P*(*r*|*e*)*, P*(*t*|*o,r*)}*.* From here on, we’ll build on the term “Markov
    kernel” from chapter 2 and call these factors *causal Markov kernels*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一组因子，{*P*(*a*)*, P*(*s*)*, P*(*e*|*s,a*)*, P*(*o*|*e*)*, P*(*r*|*e*)*, P*(*t*|*o,r*)}*。从现在起，我们将从第二章中的“马尔可夫核”术语出发，并称这些因子为*因果马尔可夫核*。
- en: We’ll build our probabilistic machine learning model by implementing these causal
    Markov kernels in code and then composing them into one model. Our implementations
    for each kernel will be able to return a probability value, given input arguments.
    For example, *P*(*a*) will take an outcome value for *A* and return a probability
    value for that outcome. Similarly, *P*(*t*|*o,r*) will take in values for *T*,
    *O*, and *R* and return a probability value for *T*=*t*, where *t* is the queried
    value. Our implementations will also be able to generate from the causal Markov
    kernels. To do this, these implementations will require parameters that map the
    inputs to the outputs. We’ll use standard statistical learning approaches to fit
    those parameters from the data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在代码中实现这些因果马尔可夫核并将它们组合成一个模型来构建我们的概率机器学习模型。我们为每个核的实现将能够返回一个概率值，给定输入参数。例如，*P*(*a*)将接受一个关于*A*的结果值并返回该结果的概率值。同样，*P*(*t*|*o,r*)将接受*T*，*O*和*R*的值并返回*T*=*t*的概率值，其中*t*是查询值。我们的实现还将能够从因果马尔可夫核中生成。为此，这些实现将需要将输入映射到输出的参数。我们将使用标准的统计学习方法来从数据中拟合这些参数。
- en: 3.1.8 Training a model on the causal DAG
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.8 在因果DAG上训练模型
- en: Consider the DGP for the transportation DAG. What sort of data would this process
    generate?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑运输DAG的DGP。这个过程会生成什么样的数据？
- en: 'Suppose we administered a survey covering 500 individuals, getting values for
    each of the variables in this DAG. The data encodes the variables in our DAG as
    follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们进行了一项涵盖500个人的调查，得到了这个DAG中每个变量的值。数据按照以下方式编码我们DAG中的变量：
- en: '*Age (A)*—Recorded as young (“young”) for individuals up to and including 29
    years, adult (“adult”) for individuals between 30 and 60 years old (inclusive),
    and old (“old”) for people 61 and over'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*年龄 (A)*—记录为年轻（“年轻”）对于29岁及以下的人，成年（“成年”）对于30至60岁的人（包括60岁），以及61岁及以上的人为老年（“老年”）'
- en: '*Gender (S)*—The self-reported gender of an individual, recorded as male (“M”),
    female (“F”), or other (“O”)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性别 (S)*—个人的自我报告性别，记录为男性（“M”）、女性（“F”）或其他（“O”）'
- en: '*Education (E)*—The highest level of education or training completed by the
    individual, recorded either high school (“high”) or university degree (“uni”)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*教育 (E)*—个人完成的教育或培训的最高水平，记录为高中（“高中”）或大学学位（“大学”）'
- en: '*Occupation (O)*—Employee (“emp”) or a self-employed worker (“self”)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*职业 (O)*—雇员（“雇员”）或自雇工人（“自雇”）'
- en: '*Residence (R)*—The population size of the city the individual lives in, recorded
    as small (“small”) or big (“big”)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*居住地 (R)*—个人居住的城市的人口规模，记录为小（“小”）或大（“大”）'
- en: '*Travel (T)*—The means of transport favored by the individual, recorded as
    car (“car”), train (“train”), or other (“other”)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*旅行 (T)*—个人偏好的交通方式，记录为汽车（“汽车”）、火车（“火车”）或其他（“其他”）'
- en: Labeling causal abstractions
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标记因果抽象
- en: How we conceptualize the variables of a model matters greatly in machine learning.
    For example, ImageNet, a database of 14 million images, contains anachronistic
    and offensive labels for racial categories. Even if renamed to be less offensive,
    race categories themselves are fluid across time and culture. What are the “correct”
    labels to use in a predictive algorithm?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们如何概念化模型中的变量非常重要。例如，ImageNet，一个包含1400万张图片的数据库，包含种族类别的时代错误和冒犯性标签。即使更名为不那么冒犯，种族类别本身在时间和文化中是流动的。在预测算法中应该使用哪些“正确”的标签？
- en: How we define our variables isn’t just a question of politics and census forms.
    A simple thought experiment by philosopher Nelson Goodman shows how a simple change
    in label can change a prediction to a contradictory prediction. Suppose you regularly
    search for gems and record the color of every gem you find. It turns out 100%
    of the gems in your dataset are green. Now let’s define a new label “grue” to
    mean “green if observed before now, blue otherwise.” So 100% of your data is “green”
    or “grue,” depending on your choice of label. Now suppose you predict the future
    based on extrapolating from the past. Then you can predict the next emerald will
    be green based on data where all past emeralds were green, or you can predict
    the next emerald will be “grue” (i.e., *blue*) based on the data that all past
    emeralds were “grue.” Obviously, you would never invent such an absurd label,
    but this thought experiment is enough to show that the inference depends on the
    abstraction.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何定义我们的变量不仅仅是政治和人口普查表的问题。哲学家纳尔逊·古德曼的一个简单的思想实验展示了简单的标签变化如何将预测变为矛盾的预测。假设你经常寻找宝石并记录你找到的每一颗宝石的颜色。结果发现你数据集中的100%的宝石都是绿色的。现在让我们定义一个新的标签“grue”，表示“如果在此之前观察到，则为绿色，否则为蓝色。”所以你的100%的数据是“绿色”或“grue”，这取决于你的标签选择。现在假设你根据过去的数据进行预测。那么你可以根据所有过去翡翠都是绿色的数据预测下一颗翡翠将是绿色的，或者你可以根据所有过去翡翠都是“grue”（即蓝色）的数据预测下一颗翡翠将是“grue”。显然，你永远不会发明这样的荒谬标签，但这个思想实验足以表明推理取决于抽象。
- en: In data science and machine learning, we’re often encouraged to blindly model
    data and not to think about the DGP. We’re encouraged to take the variable names
    for granted as columns in a spreadsheet or attributes in a database table. When
    possible, it is better to choose abstractions that are appropriate to the inference
    problem and collect or encode data according to that abstraction. When it is not
    possible, keep in mind that the results of your analysis will depend on how other
    people have defined the variables.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学和机器学习中，我们经常被鼓励盲目地建模数据，不要考虑DGP。我们被鼓励将变量名称视为电子表格中的列或数据库表中的属性。当可能时，最好选择适合推理问题的抽象，并根据该抽象收集或编码数据。当不可能时，请记住，你的分析结果将取决于其他人如何定义变量。
- en: In chapter 7, I’ll introduce the idea of “no causation without manipulation”—an
    idea that provides a useful heuristic for how to define causal variables.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我将介绍“无操纵则无因果关系”这一概念——这一概念为如何定义因果变量提供了一个有用的启发式方法。
- en: The variables in the transportation data are all *categorical* variables. In
    this simple categorical case, we can rely on a graphical modeling library like
    pgmpy*.*
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 运输数据中的变量都是 *分类* 变量。在这个简单的分类情况下，我们可以依赖像 pgmpy*.* 这样的图形建模库。
- en: Listing 3.3 Loading transportation data
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.3 加载运输数据
- en: '[PRE3]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 We’ll load the data into a pandas ΔataFrame with the read_csv method.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们将使用 read_csv 方法将数据加载到 pandas ΔataFrame 中。'
- en: This produces the DataFrame in figure 3.11.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了图 3.11 中的 DataFrame。
- en: '![figure](../Images/CH03_F11_Ness.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F11_Ness.png)'
- en: Figure 3.11 An example of data from the DGP underlying the transportation model.
    In this case, the data is 500 survey responses.
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.11 运输模型下 DGP 数据的一个示例。在这种情况下，数据是 500 份调查回复。
- en: The `BayesianNetwork` class we initialized in listing 3.2 has a `fit` method
    that will learn the parameters of our causal Markov kernels. Since our variables
    are categorical, our causal Markov kernels will be in the form of conditional
    probability tables represented by pgmpy’s `TabularCPD` class. The `fit` method
    will fit (“learn”) estimates of the parameters of those conditional probability
    tables using the data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在列表 3.2 中初始化的 `BayesianNetwork` 类有一个 `fit` 方法，它将学习我们的因果马尔可夫核的参数。由于我们的变量是分类的，我们的因果马尔可夫核将以
    pgmpy 的 `TabularCPD` 类表示的条件概率表的形式存在。`fit` 方法将使用数据来拟合（学习）那些条件概率表的参数估计。
- en: Listing 3.4 Learning parameters for the causal Markov kernels
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.4 为因果马尔可夫核学习参数
- en: '[PRE4]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 The fit method on the BayesianNetwork object will estimate parameters from
    data (a pandas ΔataFrame).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 BayesianNetwork 对象上的 fit 方法将从数据（pandas ΔataFrame）中估计参数。'
- en: '#2 Retrieve and view the causal Markov kernels learned by fit.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 检索并查看通过 fit 学习到的因果马尔可夫核。'
- en: 'This returns the following output:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回以下输出：
- en: '[PRE5]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s look at the structure of the causal Markov kernel for the transportation
    variable *T*. We can see from printing out the `causal_markov_kernels` list that
    *T* is the last item in the list.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看运输变量 *T* 的因果马尔可夫核的结构。我们可以从打印 `causal_markov_kernels` 列表看到，*T* 是列表中的最后一个项目。
- en: '[PRE6]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We get the following output:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE7]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that in this printout, I truncated the numbers so the table fits on the
    page.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个打印输出中，我截断了数字，以便表格能适应页面。
- en: '`cmk_T` is the implementation of the causal Markov kernel *P*(*T**|O,R*) as
    a conditional probability table, a type of lookup table where, given the values
    of *T*, *O*, and *R*, we get the corresponding probability mass value. For example,
    *P*(*T*=car|*O*=emp, *R*=big) = 0.7034\. Note that these are conditional probabilities.
    For each combination of values for *O* and *R*, there are conditional probabilities
    for the three outcomes of *T* that sum to 1\. For example, when *O*=emp and *R*=big,
    *P*(*T*=car| *O*=emp, *R*=big) + (*P*(*T*=other| *O*=emp, *R*=big) + *P*(*T*=train|
    *O*=emp, *R*=big) = 1.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`cmk_T` 是因果马尔可夫核 *P*(*T**|O,R*) 作为条件概率表（一种查找表）的实现，给定 *T*，*O* 和 *R* 的值，我们得到相应的概率质量值。例如，*P*(*T*=car|*O*=emp,
    *R*=big) = 0.7034。注意，这些都是条件概率。对于 *O* 和 *R* 的每一组值，都有三个 *T* 的结果的条件概率，它们的和为 1。例如，当
    *O*=emp 和 *R*=big 时，*P*(*T*=car| *O*=emp, *R*=big) + (*P*(*T*=other| *O*=emp,
    *R*=big) + *P*(*T*=train| *O*=emp, *R*=big) = 1。'
- en: The causal Markov kernel in the case of nodes with no parents is just a simple
    probability table. For example, `print(causal_markov_kernels[2])` prints the causal
    Markov kernel for gender (*S*), the third item in the `causal_markov_kernels`
    list.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有父节点的节点，因果马尔可夫核只是一个简单的概率表。例如，`print(causal_markov_kernels[2])` 打印了 `causal_markov_kernels`
    列表中的第三个项目，即性别 (*S*) 的因果马尔可夫核。
- en: '[PRE8]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `fit` method learns parameters by calculating the proportions of each class
    in the data. Alternatively, we could use other techniques for parameter learning.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit` 方法通过计算数据中每个类的比例来学习参数。或者，我们也可以使用其他参数学习技术。'
- en: 3.1.9 Different techniques for parameter learning
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.9 参数学习的不同技术
- en: There are several ways we could go about training these parameters. Let’s look
    at a few common ways of training parameters in conditional probability tables.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几种方法可以用来训练这些参数。让我们看看在条件概率表中训练参数的几种常见方法。
- en: Maximum likelihood estimation
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最大似然估计
- en: 'The learning algorithm I used in the `fit` method on the `BayesianNetwork`
    model object was *maximum likelihood estimation* (discussed in chapter 2). It
    is the default parameter learning method, so I didn’t specify “maximum likelihood”
    in the call to `fit`. Generally, maximum likelihood estimation seeks the parameter
    that maximizes the likelihood of seeing the data we use to train the model. In
    the context of categorical data, maximum likelihood estimation is equivalent to
    taking proportions of counts in the data. For example, the parameter for *P*(*O*=emp|*E*=high)
    is calculated as:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我在`BayesianNetwork`模型对象上的`fit`方法中使用的学习算法是**最大似然估计**（在第2章中讨论）。它是默认的参数学习方法，因此我在调用`fit`时没有指定“最大似然”。一般来说，最大似然估计寻求最大化我们用于训练模型的观测数据的似然度的参数。在分类数据的上下文中，最大似然估计等同于取数据中计数的比例。例如，*P*(*O*=emp|*E*=high)的参数计算如下：
- en: '![figure](../Images/ness-ch3-eqs-5x.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch3-eqs-5x.png)'
- en: Bayesian estimation
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贝叶斯估计
- en: In chapter 2, I also introduced Bayesian estimation. It is generally mathematically
    intractable and relies on computationally expensive algorithms (e.g., sampling
    algorithms and variational inference). A key exception is the case of *conjugate
    priors*, where the prior distribution and the target (posterior) distribution
    have the same canonical form. That means the code implementation can just calculate
    the parameter values of the target distribution with simple math, without the
    need for complicated Bayesian inference algorithms.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我也介绍了贝叶斯估计。它通常在数学上难以处理，并且依赖于计算成本高昂的算法（例如，采样算法和变分推断）。一个关键例外是**共轭先验**的情况，其中先验分布和目标（后验）分布具有相同的规范形式。这意味着代码实现只需用简单的数学计算目标分布的参数值，无需复杂的贝叶斯推断算法。
- en: For example, pgmpy implements a *Dirichlet conjugate prior* for categorical
    outcomes. For each value of *O* in *P*(*O*|*E*=high), we have a probability value,
    and we want to infer these probability values from the data. A Bayesian approach
    assigns a prior distribution to these values. A good choice for a prior on a set
    of probability values is the *Dirichlet distribution*, because it is defined for
    a *simplex*, a set of numbers between zero and one that sum to one. Further it
    is *conjugate* to categorical distributions like *P*(*O*|*E*=high), meaning the
    posterior distribution on the parameter values is also a Dirichlet distribution.
    That means we can calculate point estimates of the probability values using simple
    math, combining counts in the data and parameters in the prior. pgmpy does this
    math for us.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，pgmpy实现了用于分类结果的*Dirichlet共轭先验*。对于*P*(*O*|*E*=high)中的每个*O*值，我们都有一个概率值，我们希望从数据中推断这些概率值。贝叶斯方法为这些值分配一个先验分布。对于概率值集合的先验，一个好的选择是*Dirichlet分布*，因为它定义在*单纯形*上，即介于零和一之间的数字之和为1。此外，它对分类分布（如*P*(*O*|*E*=high)）是**共轭的**，这意味着参数值的后验分布也是一个Dirichlet分布。这意味着我们可以使用简单的数学计算概率值的点估计，结合数据中的计数和先验中的参数。pgmpy为我们做这个数学计算。
- en: Listing 3.5 Bayesian point estimation with a Dirichlet conjugate prior
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5 使用Dirichlet共轭先验的贝叶斯点估计
- en: '[PRE9]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Import BayesianEstimator and initialize it on the model and data.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 导入BayesianEstimator并在模型和数据上初始化它。'
- en: '#2 Pass the estimator object to the fit method.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将估计器对象传递给fit方法。'
- en: '#3 pseudo_counts refers to the parameters of the Δirichlet prior.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 pseudo_counts指的是Δirichlet先验的参数。'
- en: '#4 Extract the causal Markov kernels and view P(T|O,R).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 提取因果马尔可夫核并查看P(T|O,R)。'
- en: 'The preceding code prints the following output:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码打印以下输出：
- en: '[PRE10]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In contrast to maximum likelihood estimation, Bayesian estimation of a categorical
    parameter with a Dirichlet prior acts like a smoothing mechanism. For example,
    the maximum likelihood parameter estimate says 100% of self-employed people in
    small towns take a car to work. This is probably extreme. Certainly, some self-employed
    people bike to work—we just didn’t manage to survey any of them. Some small cities,
    such as Crystal City in the US state of Virginia (population 22,000), have subway
    stations. I’d wager at least a few of the entrepreneurs in those cities use the
    train.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与最大似然估计相比，具有Dirichlet先验的分类参数的贝叶斯估计作用像一个平滑机制。例如，最大似然参数估计表明，在小镇上的自雇人士中有100%的人开车上班。这可能是极端的。当然，一些自雇人士骑自行车上班——我们只是没有设法调查他们中的任何一个人。一些小城市，如美国弗吉尼亚州的Crystal
    City（人口22,000），有地铁站。我敢打赌，那些城市中的至少几位企业家使用火车。
- en: Causal modelers and Bayesians
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 因果模型构建者和贝叶斯主义者
- en: The Bayesian philosophy goes beyond mere parameter estimation. Indeed, Bayesian
    philosophy has much in common with DAG-based causal modeling. Bayesians try to
    encode subjective beliefs, uncertainty, and prior knowledge into “prior” probability
    distributions on variables in the model. Causal modelers try to encode subjective
    beliefs and prior knowledge about the DGP into the form of a causal DAG. The two
    approaches are compatible. Given a causal DAG, you can be Bayesian about inferring
    the parameters of the probabilistic model you build on top of the causal DAG.
    You can even be Bayesian about the DAG itself and compute probability distributions
    over possible DAGs!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯哲学超越了仅仅参数估计的范围。事实上，贝叶斯哲学与基于DAG的因果建模有很多共同之处。贝叶斯主义者试图将主观信念、不确定性和先验知识编码到模型中变量的“先验”概率分布上。因果模型者试图将关于DGP（数据生成过程）的主观信念和先验知识编码成因果DAG的形式。这两种方法是可以兼容的。给定一个因果DAG，你可以对在因果DAG之上构建的概率模型进行贝叶斯推断。你甚至可以对DAG本身进行贝叶斯推断，并计算可能DAG的概率分布！
- en: I focus on causality in this book and keep Bayesian discussions to a minimum.
    But we’ll use the libraries Pyro (and its NumPy-JAX alternative NumPyro) to implement
    causal models; these libraries provide complete support for Bayesian inference
    on models as well as parameters. In chapter 11, we’ll look at an example of Bayesian
    inference of a causal effect using a causal graphical model we build from scratch.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这本书中专注于因果性，并将贝叶斯讨论保持在最低限度。但我们将使用Pyro库（及其NumPy-JAX替代库NumPyro）来实现因果模型；这些库为模型以及参数提供了完整的贝叶斯推断支持。在第11章中，我们将查看一个使用我们从零开始构建的因果图模型进行因果效应贝叶斯推断的例子。
- en: Other techniques for parameter estimation
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他参数估计技术
- en: We need not use a conditional probability table to represent the causal Markov
    kernels. There are models within the generalized linear modeling framework for
    modeling categorical outcomes. For some of the variables in the transportation
    model, we might have used non-categorical outcomes. Age, for example, might have
    been recorded as an integer outcome in the survey. For variables with numeric
    outcomes, we might use other modeling approaches. You can also use neural network
    architectures to model individual causal Markov kernels.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要使用条件概率表来表示因果马尔可夫核。在广义线性建模框架内存在用于建模分类结果的模型。对于运输模型中的某些变量，我们可能使用了非分类结果。例如，年龄可能在调查中被记录为整数结果。对于具有数值结果的变量，我们可能使用其他建模方法。您还可以使用神经网络架构来建模个体因果马尔可夫核。
- en: '*Parametric assumptions* refer to how we specify the outcomes of a node in
    the DAG (e.g., category or real number) and how we map parents to the outcome
    (e.g., table or neural network). Note that the causal assumptions encoded by the
    causal DAG are decoupled from the parametric assumptions for a causal Markov kernel.
    For example, when we assumed that age was a direct cause of education level and
    encoded that into our DAG as an edge, we didn’t have to decide if we were going
    to treat age as an ordered set of classes, as an integer, or as seconds elapsed
    since birth, etc. Furthermore, we didn’t have to know whether to use a conditional
    categorical distribution or a regression model. That step comes after we specify
    the causal DAG and want to implement *P*(*E*|*A*, *S*).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*参数假设*指的是我们如何指定DAG（有向无环图）中一个节点的结果（例如，类别或实数）以及我们如何将父节点映射到结果（例如，表格或神经网络）。请注意，因果DAG中编码的因果假设与因果马尔可夫核的参数假设是解耦的。例如，当我们假设年龄是教育水平的直接原因并将其编码到我们的DAG中作为一条边时，我们不必决定是否将年龄视为有序的类别集合、整数或自出生以来经过的秒数等。此外，我们也不必知道是否要使用条件分类分布或回归模型。这一步骤是在我们指定因果DAG并想要实现
    *P*(*E*|*A*, *S*) 之后进行的。'
- en: Similarly, when we make predictions and probabilistic inferences on a trained
    causal model, the considerations of what inference or prediction algorithms to
    use, while important, are separate from our causal questions. This separation
    simplifies our work. Often we can build our knowledge and skill set in causal
    modeling and reasoning independently of our knowledge of statistics, computational
    Bayes, and applied machine learning.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当我们对一个训练好的因果模型进行预测和概率推理时，关于使用哪种推理或预测算法的考虑，虽然很重要，但与我们的因果问题相分离。这种分离简化了我们的工作。通常，我们可以在因果建模和推理方面独立于我们对统计学、计算贝叶斯和应用的机器学习的知识来构建我们的知识和技能集。
- en: 3.1.10 Learning parameters when there are latent variables
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.10 存在潜在变量时的参数学习
- en: Since we are modeling the DGP and not the data, it is likely that some nodes
    in the causal DAG will not be observed in the data. Fortunately, probabilistic
    machine learning provides us with tools for learning the parameters of causal
    Markov kernels of latent variables.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在建模 DGP 而不是数据，因此因果 DAG 中的某些节点可能不会在数据中观察到。幸运的是，概率机器学习为我们提供了学习潜在变量因果马尔可夫核参数的工具。
- en: Learning latent variables with pgmpy
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 pgmpy 学习潜在变量
- en: To illustrate, suppose the education variable in the transportation survey data
    was not recorded. pgmpy gives us a utility for learning the parameters of the
    causal Markov kernel for latent *E* using an algorithm called *structural expectation
    maximization*, which is a variant of parameter learning with maximum likelihood.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，假设交通调查数据中的教育变量没有被记录。pgmpy 通过使用一种称为结构化期望最大化的算法，为我们提供了一个学习潜在 *E* 的因果马尔可夫核参数的实用工具，这是一种最大似然参数学习的一个变体。
- en: Listing 3.6 Training a causal graphical model with a latent variable
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.6 使用潜在变量训练因果图模型
- en: '[PRE11]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Δownload the data and convert to a pandas ΔataFrame.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 下载数据并将其转换为 pandas DataFrame。'
- en: '#2 Keep all the columns except education (E).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 保留除教育（E）之外的所有列。'
- en: '#3 Indicate which variables are latent when training the model.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 训练模型时指出哪些变量是潜在的。'
- en: '#4 Run the structural expectation maximization algorithm to learn the causal
    Markov kernel for E. You have to indicate the cardinality of the latent variable.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 运行结构化期望最大化算法来学习 E 的因果马尔可夫核。你必须指出潜在变量的基数。'
- en: '#5 Print out the learned causal Markov kernel for E. Print it as a factor object
    for legibility.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 打印出学习到的 E 的因果马尔可夫核。为了可读性，将其打印为因子对象。'
- en: The `print` line prints a factor object.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`print` 行打印一个因子对象。'
- en: '[PRE12]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The outcomes for *E* are 0 and 1 because the algorithm doesn’t know the outcome
    names. Perhaps 0 is “high” (high school) and 1 is “uni” (university), but correctly
    mapping the default outcomes from a latent variable estimation method to the names
    of those outcomes would require further assumptions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*E* 的结果为 0 和 1，因为算法不知道结果名称。也许 0 是“高”（高中）而 1 是“uni”（大学），但将默认结果从潜在变量估计方法映射到这些结果名称需要进一步的假设。'
- en: There are other algorithms for learning parameters when there are latent variables,
    including some that use special parametric assumptions (i.e., functional assumptions
    about how the latent variables relate to the observed variables).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在潜在变量时，还有其他算法用于学习参数，包括一些使用特殊参数假设（即关于潜在变量如何与观测变量相关的函数假设）的算法。
- en: Latent variables and identification
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 潜在变量和识别
- en: In statistical inference, we say a parameter is “identified” when it is theoretically
    possible to learn its true value given an infinite number of examples in the data.
    It is “unidentified” if more data doesn’t get you closer to learning its true
    value. Unfortunately, your data may not be sufficient to learn the causal Markov
    kernels of the latent variables in your causal DAG. If we did not care about representing
    causality, we could restrict ourselves to a latent variable graphical model with
    latent variables that are identifiable from data. But we must build a causal DAG
    that represents the DGP, even if we can’t identify the latent variables and parameters
    given the data.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计推断中，我们说一个参数是“可识别的”，当理论上在给定数据中的无限多个示例的情况下，可以学习到它的真实值。如果更多的数据不能让你更接近学习它的真实值，那么它是“不可识别的”。不幸的是，你的数据可能不足以学习你的因果
    DAG 中潜在变量的因果马尔可夫核。如果我们不关心表示因果关系，我们可以限制自己使用可从数据中识别的潜在变量的潜在变量图模型。但我们必须构建一个表示 DGP
    的因果 DAG，即使我们无法根据数据识别潜在变量和参数。
- en: That said, even if you have non-identifiable parameters in your causal model,
    you still may be able to identify the quantity that answers your causal question.
    Indeed, much of causal inference methodology is focused on robust estimation of
    causal effects (how much a cause affects an effect) despite having latent “confounders.”
    We’ll cover this in detail in chapter 11\. On the other hand, even if your parameters
    are identified, the quantity that answers your causal question may not be identified.
    We’ll cover causal identification in detail in chapter 10.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，即使你在你的因果模型中有不可识别的参数，你仍然可能能够识别出回答你因果问题的数量。实际上，因果推断方法的大部分内容都集中在稳健地估计因果效应（一个原因对效应的影响程度）上，即使存在潜在的“混杂因素”。我们将在第11章中详细讨论这一点。另一方面，即使你的参数被识别，回答你因果问题的数量可能没有被识别。我们将在第10章中详细讨论因果识别。
- en: 3.1.11 Inference with a trained causal probabilistic machine learning model
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.11 使用训练好的因果概率机器学习模型进行推理
- en: A probabilistic machine learning model of a set of variables can use computational
    inference algorithms to infer the conditional probability of an outcome for any
    subset of the variables, given outcomes for the other variables. We use the variable
    elimination algorithm for a directed graphical model with categorical outcomes
    (introduced in chapter 2).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一组变量的概率机器学习模型可以使用计算推理算法来推断任何变量子集的条件的概率，给定其他变量的结果。我们使用变量消除算法来处理具有分类结果的定向图形模型（在第
    2 章中介绍）。
- en: For example, suppose we want to compare education levels amongst car drivers
    to that of train riders. We can calculate and compare *P*(*E*|*T*) when *T*=car
    to when *T*=train by using variable elimination, an inference algorithm for tabular
    graphical models.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要比较汽车驾驶员和火车乘客的教育水平。我们可以通过变量消除，一个表格图形模型的推理算法，来计算并比较当 *T*=car 和当 *T*=train
    时的 *P*(*E*|*T*)。
- en: Listing 3.7 Inference on the trained causal graphical model
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.7 在训练好的因果图形模型上进行推理
- en: '[PRE13]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 VariableElimination is an inference algorithm specific to graphical models.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 VariableElimination 是一种针对图形模型的特定推理算法。'
- en: This prints the probability tables for “train” and “car.”
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出“train”和“car”的概率表。
- en: '[PRE14]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It seems car drivers are more likely to have a university education than train
    riders: (*P*(*E*=''uni''|*T*=''car'') > *P*(*E*=''uni''|*T*=''train''). That inference
    is based on our DAG-based causal assumption that university education indirectly
    determines how people get to work.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来汽车驾驶员比火车乘客更有可能拥有大学教育：(*P*(*E*='uni'|*T*='car') > *P*(*E*='uni'|*T*='train')。这个推断基于我们的基于
    DAG 的因果假设，即大学教育间接决定了人们如何去工作。
- en: In a tool like Pyro, you have to be a bit more hands-on with the inference algorithm.
    The following listing illustrates the inference of *P*(*E*|*T*=“train”) using
    a probabilistic inference algorithm called importance sampling. First, we’ll specify
    the model. Rather than fit the parameters, we’ll explicitly specify the parameter
    values we fit with pgmpy.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pyro 这样的工具中，您需要对推理算法进行更多手动操作。以下列表说明了使用名为重要性采样的概率推理算法推断 *P*(*E*|*T*="train")
    的过程。首先，我们将指定模型。我们不会拟合参数，而是明确指定我们使用 pgmpy 拟合的参数值。
- en: Listing 3.8 Implementing the trained causal model in Pyro
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.8 在 Pyro 中实现训练好的因果模型
- en: '[PRE15]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 The categorical distribution only returns integers, so it’s useful to write
    the integers’ mapping to categorical outcome names.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 分类分布只返回整数，因此将整数映射到分类结果名称是有用的。'
- en: '#2 For simplicity, we’ll use rounded versions of parameters learned with the
    fit method in pgmpy (listing 3.4), though we could have learned the parameters
    in a training procedure.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为了简单起见，我们将使用 pgmpy 中 fit 方法学习到的参数的舍入版本（列表 3.4），尽管我们可以在训练过程中学习参数。'
- en: '#3 When we implement the model in Pyro, we specify the causal ΔAG implicitly
    using code logic.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 当我们在 Pyro 中实现模型时，我们通过代码逻辑隐式地指定了因果 ΔAG。'
- en: '#4 We can then generate a figure of the implied ΔAG using pyro.render_model().
    Note that we need to have Graphviz installed.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 然后，我们可以使用 pyro.render_model() 生成隐含 ΔAG 的图。请注意，我们需要安装 Graphviz。'
- en: The `pyro.render_model` function draws the implied causal DAG from the Pyro
    model in figure 3.12\.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`pyro.render_model` 函数从 Pyro 模型中绘制了图 3.12 所示的隐含因果 DAG。'
- en: '![figure](../Images/CH03_F12_Ness.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F12_Ness.png)'
- en: Figure 3.12 You can visualize the causal DAG in Pyro by using the `pyro.render_model()`
    function. This assumes you have Graphviz installed.
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.12 您可以使用 `pyro.render_model()` 函数在 Pyro 中可视化因果 DAG。这假设您已安装 Graphviz。
- en: Pyro provides probabilistic inference algorithms, such as importance sampling,
    that we can apply to our causal model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Pyro 提供了概率推理算法，例如重要性采样，我们可以将其应用于我们的因果模型。
- en: Listing 3.9 Inference on the causal model in Pyro
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.9 在 Pyro 中的因果模型上进行推理
- en: '[PRE16]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 We’ll use two inference-related classes, Importance and EmpiricalMarginal.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们将使用两个与推理相关的类，即 Importance 和 EmpiricalMarginal。'
- en: '#2 pyro.condition is a conditioning operation on the model.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 pyro.condition 是对模型的一种条件操作。'
- en: '#3 It takes in the model and evidence for conditioning on. The evidence is
    a dictionary that maps variable names to values. The need to specify variable
    names during inference is why we have the name argument in the calls to pyro.sample.
    Here we condition on T=“train”.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 它接受模型和用于条件化的证据。证据是一个将变量名称映射到值的字典。在推理过程中需要指定变量名称，这就是为什么我们在 pyro.sample 的调用中有了名称参数。这里我们条件化
    T="train"。'
- en: '#4 We’ll run an inference algorithm that will generate m samples.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 我们将运行一个推理算法，该算法将生成m个样本。'
- en: '#5 I use an inference algorithm called importance sampling. The Importance
    class constructs this inference algorithm. It takes the conditioned model and
    the number of samples.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 我使用一个名为重要性采样的推理算法。Importance类构建了这个推理算法。它接受条件化模型和样本数量。'
- en: '#6 Run the random process algorithm with the run method. The inference algorithm
    will generate from the joint probability of the variables we didn’t condition
    on (everything but T) given the variables we conditioned on (T).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 使用run方法运行随机过程算法。推理算法将从我们未条件化的变量的联合概率（除了T以外的所有变量）生成，给定我们条件化的变量（T）。'
- en: '#7 We are interested in the conditional probability distribution of education,
    so we extract education values from the posterior.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 我们对教育条件的概率分布感兴趣，因此我们从后验中提取教育值。'
- en: '#8 Based on these samples, we produce a Monte Carlo estimation of the probabilities
    in P(E|T=“train”).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 基于这些样本，我们对P(E|T=“train”）中的概率进行蒙特卡洛估计。'
- en: '#9 Plot a visualization of the learned probabilities.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 绘制学习概率的可视化。'
- en: This produces the plot in figure 3.13\. The probabilities shown are close to
    the results from the pgmpy model, though they’re slightly different due to different
    algorithms and the rounding of the parameter estimates to two decimal places.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了图3.13中的图表。显示的概率接近于pgmpy模型的结果，尽管由于不同的算法和参数估计四舍五入到两位小数，它们略有不同。
- en: This probabilistic inference is not yet causal inference—we’ll look at examples
    combining causal inference with probabilistic inference starting in chapter 7\.
    In chapter 8, you’ll see how to use probabilistic inference to implement causal
    inference. For now, we’ll look at the benefit of parameter modularity, and at
    how parameters encode causal invariance.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这种概率推理还不是因果推理——我们将在第7章开始查看结合因果推理和概率推理的示例。在第8章中，你将看到如何使用概率推理来实现因果推理。现在，我们将探讨参数模块化的好处，以及参数如何编码因果不变性。
- en: '![figure](../Images/CH03_F13_Ness.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F13_Ness.png)'
- en: Figure 3.13 Visualization of the P(E|T=“train”) distribution
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.13 P(E|T=“train”)分布的可视化
- en: 3.2 Causal invariance and parameter modularity
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 因果不变性和参数模块化
- en: Suppose we were interested in modeling the relationship between altitude and
    temperature. The two are clearly correlated; the higher up you go, the colder
    it gets. However, you know temperature doesn’t cause altitude, or heating the
    air within a city would cause the city to fly. Altitude is the cause, and temperature
    is the effect.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们感兴趣于建模海拔和温度之间的关系。这两者显然是相关的；你爬得越高，天气就越冷。然而，你知道温度不会导致海拔，或者加热城市内的空气不会使城市飞起。海拔是原因，温度是结果。
- en: 'We can come up with a simple causal DAG that we think captures the relationship
    between temperature and altitude, along with other causes, as shown in figure
    3.14\. Let’s have *A* be altitude, *C* be cloud cover, *L* be latitude, *S* be
    season, and *T* be temperature. The DAG in figure 3.14 has five causal Markov
    kernels: {*P*(*A*), *P*(*C*), *P*(*L*), *P*(*S*), *P*(*T*|*A*, *C*, *L*, *S*)}.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提出一个简单的因果DAG，我们认为它捕捉了温度和海拔之间的关系，以及其他原因，如图3.14所示。让我们让*A*代表海拔，*C*代表云量，*L*代表纬度，*S*代表季节，*T*代表温度。图3.14中的DAG有五个因果马尔可夫核：{*P*(*A*),
    *P*(*C*), *P*(*L*), *P*(*S*), *P*(*T*|*A*, *C*, *L*, *S*)}。
- en: '![figure](../Images/CH03_F14_Ness.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F14_Ness.png)'
- en: Figure 3.14 A simple model of outdoor temperature
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.14 室外温度的简单模型
- en: To train a causal graphical model on top of this DAG, we need to learn parameters
    for each of these causal Markov kernels.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 要在DAG之上训练因果图模型，我们需要学习每个因果马尔可夫核的参数。
- en: 3.2.1 Independence of mechanism and parameter modularity
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 机制独立性和参数模块化
- en: There are some underlying thermodynamic mechanisms in the DGP underlying the
    causal Markov kernels in our temperature DAG. For example, the causal Markov kernel
    *P*(*T*|*A*, *C*, *L*, *S*) is the conditional probability induced by the physics-based
    mechanism, wherein altitude, cloud cover, latitude, and season drive the temperature.
    That mechanism is distinct from the mechanism that determines cloud cover (according
    to our DAG). *Independence of mechanism* refers to this distinction between mechanisms.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们温度有向无环图（DAG）中因果马尔可夫核的DGP中存在一些基本的动力学机制。例如，因果马尔可夫核*P*(*T*|*A*, *C*, *L*, *S*)是由基于物理机制的物理条件概率诱导的，其中海拔、云层覆盖、纬度和季节驱动温度。这种机制与确定云层覆盖的机制（根据我们的DAG）是不同的。"机制独立性"指的是这种机制之间的区别。
- en: The independence of the mechanism leads to a property called *parameter modularity*.
    In our model, for each causal Markov kernel, we choose a parameterized representation
    of the causal Markov kernels. If *P*(*T*|*A*, *C*, *L*, *S*) and *P*(*C*) are
    distinct mechanisms, then our representations of *P*(*T*|*A*, *C*, *L*, *S*) and
    *P*(*C*) are representations of distinct mechanisms. That means we can change
    one representation without worrying about how that change affects the other representations.
    Such modularity is atypical in statistical models; you can’t usually change one
    part of a model and expect the other part to be unaffected.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 机制的独立性导致了一种称为*参数模块化*的特性。在我们的模型中，对于每个因果马尔可夫核，我们选择因果马尔可夫核的参数化表示。如果*P*(*T*|*A*,
    *C*, *L*, *S*)和*P*(*C*)是不同的机制，那么我们对*P*(*T*|*A*, *C*, *L*, *S*)和*P*(*C*)的表示是不同机制的表示。这意味着我们可以改变一个表示，而不用担心这种改变如何影响其他表示。这种模块化在统计模型中是不常见的；你通常不能改变模型的一部分并期望另一部分不受影响。
- en: One way this comes in handy is during training. Typically, when you train a
    model, you optimize all the parameters at the same time. Parameter modularity
    means you could train the parameters for each causal Markov kernel separately,
    or train them simultaneously as decoupled sets, allowing you to enjoy some dimensionality
    reduction during training. In Bayesian terms, the parameter sets are a priori
    independent (though they are generally dependent in the posterior). This provides
    a nice causal justification for using an independent prior distribution for each
    causal Markov kernel’s parameter set.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这种独立性在训练过程中很有用。通常，当你训练一个模型时，你同时优化所有参数。参数模块化意味着你可以分别训练每个因果马尔可夫核的参数，或者将它们作为解耦集同时训练，这允许你在训练过程中享受一些维度降低。在贝叶斯术语中，参数集是先验独立的（尽管它们在后验中通常是相关的）。这为使用每个因果马尔可夫核参数集的独立先验分布提供了一个很好的因果理由。
- en: 3.2.2 Causal transfer learning, data fusion, and invariant prediction
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 因果迁移学习、数据融合和不变预测
- en: You may not be a climatologist or a meteorologist. Still, you know the relationship
    between temperature and altitude has something to do with air pressure, climate,
    sunlight, and such. You also know that whatever the physics of that relationship
    is, the physics is the same in Katmandu as it is in El Paso. So, when we train
    a causal Markov kernel on data solely collected from Katmandu, we learn a causal
    representation of a mechanism that is invariant between Katmandu and El Paso.
    This invariance helps with transfer learning; we should be able to use that trained
    causal Markov kernel to make inferences about the temperature in El Paso.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不是气候学家或气象学家。然而，你知道温度和海拔之间的关系与气压、气候、阳光等因素有关。你也知道，无论这种关系的物理性质是什么，在卡塔曼杜和埃尔帕索的物理性质是相同的。因此，当我们仅在卡塔曼杜收集的数据上训练因果马尔可夫核时，我们学习到的机制表示在卡塔曼杜和埃尔帕索之间是不变的。这种不变性有助于迁移学习；我们应该能够使用训练好的因果马尔可夫核来对埃尔帕索的温度进行推理。
- en: Of course, there are caveats to leveraging this notion of causal invariance.
    For example, this assumes your causal model is correct and that there is enough
    information about the underlying mechanism in the Katmandu data to effectively
    apply what you’ve learned about that mechanism in El Paso.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，利用这种因果不变性的概念也有一些注意事项。例如，这假设你的因果模型是正确的，并且关于卡塔曼杜数据中潜在机制的足够信息可以有效地应用你在埃尔帕索学到的关于该机制的知识。
- en: Several advanced methods lean heavily on causal invariance and independence
    of mechanism. For example, *causal data fusion* uses this idea to learn a causal
    model by combining multiple datasets. *Causal transfer learning* uses causal invariance
    to make causal inferences using data outside the domain of the training data.
    *Causal invariant prediction* leverages causal invariance in prediction tasks.
    See the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 几种高级方法在很大程度上依赖于因果不变性和机制独立性。例如，*因果数据融合* 通过结合多个数据集来学习因果模型。*因果迁移学习* 使用因果不变性来使用训练数据域之外的数据进行因果推断。*因果不变性预测*
    在预测任务中利用因果不变性。有关参考文献，请参阅[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)章节注释。
- en: 3.2.3 Fitting parameters with common sense
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 使用常识拟合参数
- en: In the temperature model, we have an intuition about the physics of the mechanism
    that induces *P*(*T*|*A*, *C*, *L*, *S*). In non-natural science domains, such
    as econometrics and other social sciences, the “physics” of the system is more
    abstract and harder to write down. Fortunately, we can rely on similar invariance-based
    intuition in these non-natural science domains. In these domains, we can still
    assume the causal Markov kernels correspond to distinct causal mechanisms in the
    real world, assuming the model is true. For example, recall *P*(*T*|*O*, *R*)
    in our transportation model. We still assume the underlying mechanism is distinct
    from the others; if there were changes to the mechanism underlying *P*(*T*|*O*,
    *R*), only *P*(*T*|*O*, *R*) should change—other kernels in the model should not.
    If something changes the mechanism underlying *P*(*R*|*E*), the causal Markov
    kernel for *R*, this change should affect *P*(*R*|*E*) but have no effect on the
    parameters of *P*(*T*|*O*, *R*).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在温度模型中，我们对引起 *P*(*T*|*A*, *C*, *L*, *S*) 的机制有物理直觉。在非自然科学领域，如计量经济学和其他社会科学中，系统的“物理”更抽象，更难描述。幸运的是，我们可以在这些非自然科学领域依赖类似的基于不变性的直觉。在这些领域，我们仍然可以假设因果马尔可夫核对应于现实世界中的不同因果机制，假设模型是真实的。例如，回忆一下我们交通模型中的
    *P*(*T*|*O*, *R*)。我们仍然假设潜在机制与其他机制不同；如果 *P*(*T*|*O*, *R*) 的潜在机制发生变化，只有 *P*(*T*|*O*,
    *R*) 应该改变——模型中的其他核不应该改变。如果某种东西改变了 *P*(*R*|*E*)（*R* 的因果马尔可夫核）的潜在机制，这种变化应该影响 *P*(*R*|*E*)
    但不应影响 *P*(*T*|*O*, *R*) 的参数。
- en: This invariance can help us estimate parameters *without* statistical learning
    by reasoning about the underlying causal mechanism. For example, let’s look again
    at the causal Markov kernel *P*(*R*|*E*) (recall *R* is residence, *E* is education).
    Let’s try to reason our way to estimates of the parameters of this distribution
    without using statistical learning.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不变性可以帮助我们通过推理潜在的因果机制来估计参数，而无需进行统计学习。例如，让我们再次看看因果马尔可夫核 *P*(*R*|*E*)（回忆 *R*
    是居住地，*E* 是教育）。让我们尝试通过推理来估计这个分布的参数，而不使用统计学习。
- en: People who don’t get more than a high school degree are more likely to stay
    in their hometowns. However, people from small towns who attain college degrees
    are likely to move to a big city where they can apply their credentials to get
    higher-paying jobs.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 没有获得高中以上学位的人更有可能留在他们的家乡。然而，那些来自小镇并取得大学学位的人可能会搬到大城市，在那里他们可以应用他们的资历来获得更高薪的工作。
- en: Now let’s think about US demographics. Suppose a web search tells you that 80%
    of the US lives in an urban area (*P*(*R*=big) = .8), while 95% of college degree
    holders live in an urban area (*P*(*R*=big|*E*=uni) = .95). Further, 25% of the
    overall adult population in the US has a university degree (*P*(*E*=uni) = .25).
    Then, with some back-of-the-envelope math, you calculate your probability values
    as *P*(*R*=small|*E*=high)=.25, *P*(*R*=big|*E*=high) = .75, *P*(*R*=small|*E*=uni)
    = .05, and *P*(*R*=big|*E*=uni) = .95\. The ability to calculate parameters in
    this manner is particularly useful if data is unavailable for parameter learning.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来思考一下美国的 demographics。假设一个网络搜索告诉你，80% 的美国人口居住在城市地区（*P*(*R*=big) = .8），而95%
    的拥有大学学位的人居住在城市地区（*P*(*R*=big|*E*=uni) = .95）。此外，美国总体成年人口中有 25% 拥有大学学位（*P*(*E*=uni)
    = .25）。然后，通过一些简单的估算数学，你计算出你的概率值为 *P*(*R*=small|*E*=high)=.25，*P*(*R*=big|*E*=high)
    = .75，*P*(*R*=small|*E*=uni) = .05，和 *P*(*R*=big|*E*=uni) = .95。以这种方式计算参数的能力在数据不可用于参数学习时尤其有用。
- en: 3.3 Your causal question scopes the DAG
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 你的因果问题定义了 DAG
- en: When a modeler meets a problem for the first time, there is often already a
    set of available data, and a common mistake is to define your DAG using only the
    variables in that data. Letting the data scope your DAG is attractive, because
    you don’t have to decide what variables to include in your DAG. But causal modelers
    model the DGP, not the data. The true causal structure in the world doesn’t care
    about what happens to be measured in your dataset. In your causal DAG, you should
    include causally relevant variables whether they are in your dataset or not.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型师第一次遇到问题时，通常已经有一组可用的数据，一个常见的错误是仅使用该数据中的变量来定义你的DAG。让数据范围决定你的DAG是有吸引力的，因为你不必决定在DAG中包含哪些变量。但因果模型师建模的是DGP，而不是数据。世界中的真实因果结构并不关心你的数据集中发生了什么。在你的因果DAG中，你应该包括与因果相关的变量，无论它们是否在你的数据集中。
- en: But if the data doesn’t define the DAG’s scope, what does? While your data has
    a fixed set of variables, the variables that could comprise your DGP are only
    bounded by your imagination. Given a variable, you could include its causes, those
    causes’ causes, those causes’ causes’ causes, continuing all the way back to Aristotle’s
    “prime mover,” the single cause of everything. Fortunately, there is no need to
    go back that far. Let’s look at a procedure you can use to select variables for
    inclusion in your causal DAG.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果数据没有定义DAG的范围，那是什么定义的呢？虽然你的数据有一组固定的变量，但可能构成你的DGP的变量只受你的想象力所限。给定一个变量，你可以包括它的原因，这些原因的原因，这些原因的原因的原因，一直追溯到亚里士多德的“第一推动者”，即一切事物的单一原因。幸运的是，我们不需要追溯到那么远。让我们看看你可以用来选择在因果DAG中包含的变量的一个程序。
- en: 3.3.1 Selecting variables for inclusion in the DAG
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 选择要包含在DAG中的变量
- en: Recall that there are several kinds of causal inference questions. As I mentioned
    in chapter 1, causal effect inference is the most common type of causal question.
    I use causal effect inference as an example, but this workflow is meant for all
    types of causal questions.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，存在几种类型的因果推断问题。正如我在第一章中提到的，因果效应推断是最常见的因果问题类型。我以因果效应推断为例，但这个工作流程适用于所有类型的因果问题。
- en: '*Include variables central to your causal question(s)*—The first step is to
    include all the variables central to your causal question. If you intend to ask
    multiple questions, include all the variables relevant to those questions. As
    an example, consider figure 3.15\. Suppose that we intend to ask about the causal
    effect of *V* on *U* and *Y*. These become the first variables we include in the
    DAG.'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*包含与你的因果问题（s）相关的变量*——第一步是包含所有与你的因果问题相关的变量。如果你打算问多个问题，包括与这些问题相关的所有变量。作为一个例子，考虑图3.15。假设我们打算询问*V*对*U*和*Y*的因果效应。这些就成为我们在DAG中首先包含的变量。'
- en: '*Include any common causes for the variables in step 1*—Add any common causes
    for the variables you included in the first step. In our example, you would start
    with variables *U*, *V* and *Y* in figure 3.15, and trace back their causal lineages
    and identify shared ancestors. These shared ancestors are common causes. In figure
    3.16, *W*[0], *W*[1], and *W*[2] are common causes of *V*, *U*, and *Y*.'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在第1步中包含任何变量的共同原因*——添加你在第一步中包含的变量的任何共同原因。在我们的例子中，你将从图3.15中的变量*U*、*V*和*Y*开始，追溯它们的因果谱系并识别共同的祖先。这些共同的祖先就是共同原因。在图3.16中，*W*[0]、*W*[1]和*W*[2]是*V*、*U*和*Y*的共同原因。'
- en: '![figure](../Images/CH03_F15_Ness.png)'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F15_Ness.png)'
- en: Figure 3.15 First include variables central to your causal question(s). Here,
    suppose you are interested in asking questions about *V*, *U*, and *Y*.
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.15 首先包括与你的因果问题（s）相关的变量。在这里，假设你感兴趣的是询问关于*V*、*U*和*Y*的问题。
- en: '![figure](../Images/CH03_F16_Ness.png)'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F16_Ness.png)'
- en: Figure 3.16 Satisfy causal sufficiency; include common causes to the variables
    from step 1.
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.16 满足因果充分性；包含来自第1步的变量的共同原因。
- en: In formal terms, a variable is a common cause *Z* of a pair of variables *X*
    and *Y* if there is a directed path from *Z* to *X* that does not include *Y*
    and a directed path from *Z* to *Y* that does not include *X*. The formal principle
    of including common causes is called *causal sufficiency*. A set of variables
    is causally sufficient if it doesn’t exclude any common causes between any pair
    of variables in the set. Furthermore, once you include a common cause, you don’t
    have to include earlier common causes on the same paths. For example, figure 3.17
    illustrates how we might exclude variables’ earlier common causes. ![figure](../Images/CH03_F17_Ness.png)
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在正式术语中，一个变量是变量对 *X* 和 *Y* 的共同原因 *Z*，如果存在从 *Z* 到 *X* 的有向路径，该路径不包括 *Y*，以及从 *Z*
    到 *Y* 的有向路径，该路径不包括 *X*。包括共同原因的正式原则称为 *因果充分性*。一组变量是因果充分的，如果它不排除该组中任何一对变量之间的任何共同原因。此外，一旦包含了一个共同原因，就不必在相同路径上包含更早的共同原因。例如，图
    3.17 展示了我们可以如何排除变量的早期共同原因。![图](../Images/CH03_F17_Ness.png)
- en: Figure 3.17 Once you include a common cause, you don’t have to include any earlier
    common causes on the same paths to the step 1 variables.
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.17 一旦包含了一个共同原因，就不必在通往步骤 1 变量的相同路径上包含任何更早的共同原因。
- en: In figure 3.17, *W*[2] is on *W*[0]’s path to *Y* and *U*, but we include *W*[0]
    because it has its own path to *V*. In contrast, while *C* is a common cause of
    *V*, *Y*, and *U*, *W*[0] is on all of *C*’s paths to *V*, *Y* and *U*, so we
    can exclude it after including *W*[0]. Similarly, *W*[2] lets us exclude *E*,
    and *W*[0] and *W*[2] together let us exclude *D*.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在图 3.17 中，*W*[2] 在 *W*[0] 通向 *Y* 和 *U* 的路径上，但我们包含 *W*[0]，因为它有自己的路径通向 *V*。相比之下，虽然
    *C* 是 *V*、*Y* 和 *U* 的共同原因，但 *W*[0] 在 *C* 到 *V*、*Y* 和 *U* 的所有路径上，因此我们在包含 *W*[0]
    后可以排除它。同样，*W*[2] 让我们可以排除 *E*，而 *W*[0] 和 *W*[2] 一起让我们可以排除 *D*。
- en: '*Include variables that may be useful in causal inference statistical analysis*—Now
    we include variables that may be useful in statistical methods for the causal
    inferences you want to make. For example, in figure 3.18, suppose you were interested
    in estimating the cause effect of *V* on *Y*. You might want to include possible
    “instrumental variables.” We’ll define these formally in part 4 of this book,
    but for now in a causal effect question, an *instrument* is a parent of a variable
    of interest, and it can help in statistical estimation of the causal effect. In
    figure 3.18, *Z* can function as an instrumental variable. You do not need to
    include *Z* for causal sufficiency, but you might choose to include it to help
    with quantifying the causal effect.![figure](../Images/CH03_F18_Ness.png)'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*包含在因果推断统计分析中可能有用的变量*——现在我们包含在您想要进行的因果推断统计方法中可能有用的变量。例如，在图 3.18 中，假设您对估计 *V*
    对 *Y* 的因果效应感兴趣。您可能希望包括可能的“工具变量”。我们将在本书的第 4 部分正式定义这些变量，但就目前而言，在因果效应问题中，*工具* 是一个感兴趣变量的父变量，并且它可以帮助统计估计因果效应。在图
    3.18 中，*Z* 可以作为工具变量。您不需要包含 *Z* 以满足因果充分性，但您可能选择包含它以帮助量化因果效应。![图](../Images/CH03_F18_Ness.png)'
- en: Figure 3.18 Include variables that may be useful in the causal inference statistical
    analysis. *W*’s are confounders, *Z*’s are instruments, *X*’s are effect modifiers,
    *Y* is the outcome, *V* is a treatment, and U is a front door mediator.
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.18 包含在因果推断统计分析中可能有用的变量。*W* 是混杂因素，*Z* 是工具变量，*X* 是效应修饰因子，*Y* 是结果，*V* 是治疗，而
    U 是前门中介。
- en: Similarly, *X*[0] and *X*[1] could also be of use in the analysis by accounting
    for other sources of variation in *Y*. We could potentially use them to reduce
    variance in the statistical estimation of a causal effect. Alternatively, we may
    be interested in the *heterogeneity* of the causal effect (how the causal effect
    varies) across subsets of the population defined by *X*[0] and *X*[1]. We’ll look
    at causal effect heterogeneity more closely in chapter 11\.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类似地，*X*[0] 和 *X*[1] 在分析 *Y* 的其他变异来源时也可能有用。我们可能可以利用它们来减少因果效应统计估计中的方差。或者，我们可能对因果效应的
    *异质性*（因果效应在由 *X*[0] 和 *X*[1] 定义的群体子集中如何变化）感兴趣。我们将在第 11 章中更详细地研究因果效应异质性。
- en: '*Include variables that help the DAG communicate a complete story*—Finally,
    include any variables that help the DAG better function as a communicative tool.
    Consider the common cause *D* in figure 3.19.![figure](../Images/CH03_F19_Ness.png)'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*包含帮助 DAG 传达完整故事的变量*——最后，包含任何有助于 DAG 更好地作为沟通工具的变量。考虑图 3.19 中的共同原因 *D*。![图](../Images/CH03_F19_Ness.png)'
- en: Figure 3.19 Include variables that help the DAG tell a complete story. In this
    example, despite having excluded *D* in step 2 (figure 3.17) we still might want
    to include *D* if it has communicative value.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.19 包含帮助DAG讲述完整故事的变量。在这个例子中，尽管在第二步（图3.17）中排除了*D*，但我们仍然可能希望包含*D*，如果它具有沟通价值的话。
- en: In figure 3.17, we concluded that the common cause *D* could be excluded after
    including common causes *W*[0] and *W*[2]. But perhaps *D* is an important variable
    in how domain experts conceptualize the domain. While it is not useful in quantifying
    the causal effect of *V* on *U* and *Y*, leaving it out might feel awkward. If
    so, including it may help the DAG tell a better story by showing how a key variable
    relates to the variables you included. When your causal DAG tells a convincing
    story, your causal analysis is more convincing.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在图3.17中，我们得出结论，在包含共同原因*W*[0]和*W*[2]之后，可以排除共同原因*D*。但也许*D*是领域专家概念化领域的重要变量。虽然它对量化*V*对*U*和*Y*的因果效应没有帮助，但省略它可能会感觉有些不自然。如果是这样，包含它可能有助于DAG讲述更好的故事，通过展示关键变量如何与您包含的变量相关联。当您的因果DAG讲述一个令人信服的故事时，您的因果分析就更有说服力。
- en: 3.3.2 Including variables in causal DAGs by their role in inference
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 通过变量在推理中的作用包括因果DAG中的变量
- en: Many experts in causal inference de-emphasize writing their assumptions in the
    form of a causal DAG in favor of specifying a set of relevant variables, according
    to their *role* in causal inference calculations. Focusing on variable-role-in-inference
    over a causal DAG is common in econometrics pedagogy. Examples of such roles include
    terms I’ve already introduced, such as “common cause,” “instrumental variable,”
    and “effect modifier.” Again, we’ll define these formally in chapter 11.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 许多因果推理专家倾向于不将他们的假设以因果DAG的形式写出，而是根据它们在因果推理计算中的*角色*指定一组相关变量。在计量经济学教学中，关注变量-角色-推理而不是因果DAG是常见的。这样的角色包括我之前已经介绍过的，如“共同原因”、“工具变量”和“效应调节因子”。同样，我们将在第11章中正式定义这些。
- en: For now, I want to make clear that this is not a competing paradigm. An economist
    might say they are interested in the causal effect of *V* on *U*, conditional
    on some “*effect modifiers*,” and that they plan to “*adjust for* the influence
    of *common causes”* using an “*instrumental variable*.” These roles all correspond
    to structure in a causal DAG; common causes of *U* and *V* in figure 3.19 are
    *W*[0], *W*[1], and *W*[2]. *Z* is an instrumental variable, and *X*[0] and *X*[1]
    are effect modifiers. Assuming variables with these roles are important to your
    causal effect estimation analysis is implicitly assuming that your DGP follows
    the causal DAG with this structure.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我想明确指出这并不是一个竞争性的范式。经济学家可能会说他们对*V*对*U*的因果效应感兴趣，条件是在某些“*效应调节因子*”下，并且他们计划使用“*工具变量*”来“*调整*”共同原因的影响。这些角色都对应于因果DAG中的结构；图3.19中*U*和*V*的共同原因包括*W*[0]、*W*[1]和*W*[2]。*Z*是一个工具变量，而*X*[0]和*X*[1]是效应调节因子。假设具有这些角色的变量对您的因果效应估计分析很重要，这隐含地假设您的DGP遵循具有这种结构的因果DAG。
- en: In fact, given a set of variables and their roles, we can construct the implied
    causal DAG on that set. The DoWhy causal inference library shows us how.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，给定一组变量及其角色，我们可以在该集合上构建隐含的因果DAG。DoWhy因果推理库展示了如何做到这一点。
- en: Listing 3.10 Creating a DAG based on roles in causal effect inference
  id: totrans-294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.10 基于因果效应推理中的角色创建DAG
- en: '[PRE17]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 datasets.linear_ dataset generates a ΔAG from the specified variables.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 datasets.linear_ dataset从指定的变量生成ΔAG。'
- en: '#2 Add one treatment variable, like V in figure 3.19.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 添加一个处理变量，例如图3.19中的V。'
- en: '#3 Z in figure 3.19 is an example of an instrumental variable; a variable that
    is a cause of the treatment, but its only causal path to the outcome is through
    the treatment. Here we create two instruments.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 图3.19中的Z是一个工具变量的例子；一个导致处理的原因，但其到达结果的单一路径是通过处理。在这里，我们创建了两个工具变量。'
- en: '#4 X [0] and X [1] in figure 3.19 are examples of “effect modifiers” that help
    model heterogeneity in the causal effect. ΔoWhy defines these as other causes
    of the outcome (though they needn’t be). Here we create two effect modifiers.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 图3.19中的X [0]和X [1]是帮助模型因果效应异质性的“效应调节因子”的例子。ΔoWhy将这些定义为结果的其他原因（尽管它们不必是）。在这里，我们创建了两个效应调节因子。'
- en: '#5 We add 5 common causes, like the three W [0], W [1], and W [2] in figure
    3.19\. Unlike the nuanced structure between these variables in figure 3.19, the
    structure here will be simple.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 我们添加了5个共同原因，例如图3.19中的三个W [0]、W [1]和W [2]。与图3.19中这些变量之间微妙的结构不同，这里的结构将会简单。'
- en: '#6 Front door variables are on the path between the treatment and the effect,
    like U in figure 3.19\. Here we add one.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 前门变量位于处理和效应之间的路径上，如图3.19中的U。这里我们添加一个。'
- en: '#7 This code extracts the graph, creates a plotting layout, and plots the graph.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 此代码提取图，创建绘图布局，并绘制图形。'
- en: '#8 This code extracts the graph, creates a plotting layout, and plots the graph.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 此代码提取图，创建绘图布局，并绘制图形。'
- en: This code produces the DAG pictured in figure 3.20.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码生成图3.20中所示的DAG。
- en: This role-based approach produces a simple template causal DAG. It won’t give
    you the nuance that we have in figure 3.19, and it will exclude the good storytelling
    variables that we added in step 4, like *D* in figure 3.19\. But it will be enough
    for tackling the predefined causal effect query. It’s a great tool to use when
    working with collaborators who are skeptical of DAGs but are comfortable talking
    about variable roles. But don’t believe claims that this approach is DAG-free.
    The DAG is just implicit in the assumptions underlying the specification of the
    roles.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于角色的方法产生了一个简单的模板因果DAG。它不会给你像图3.19中那样的细微差别，并且会排除我们在步骤4中添加的好的叙事变量，如图3.19中的*D*。但它足以应对预定义的因果效应查询。当与对DAG持怀疑态度但乐于讨论变量角色的人合作时，这是一个非常好的工具。但不要相信这种方法是无DAG的。DAG只是隐含在角色指定的假设背后的。
- en: '![figure](../Images/CH03_F20_Ness.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F20_Ness.png)'
- en: Figure 3.20 A causal DAG built by specifying variables by their role in causal
    effect inference
  id: totrans-307
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.20 通过指定变量在因果效应推理中的作用构建的因果DAG
- en: Such a template method could be used for other causal queries as well. You can
    also use this approach to get a basic causal DAG in a first step, which you could
    then build upon to produce a more nuanced graph.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模板方法也可以用于其他因果查询。你还可以使用这种方法在第一步中获得一个基本的因果DAG，然后你可以在此基础上构建一个更细微的图。
- en: '3.4 Looking ahead: Model testing and combining causal graphs with deep learning'
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 展望：模型测试和将因果图与深度学习相结合
- en: The big question when building a causal DAG is “what if my causal DAG is wrong?”
    How can we be confident in our selected DAG? In the next chapter, we’ll look at
    how to use data to stress test our causal DAG. A key insight will be that while
    data can never prove that a causal DAG is right, it can help show when it is wrong.
    You’ll also learn about causal discovery, a set of algorithms for learning causal
    DAGs from data.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建因果有向无环图（DAG）时，最大的问题是“如果我的因果DAG是错误的怎么办？”我们如何对我们的选定的DAG有信心？在下一章中，我们将探讨如何使用数据来对因果DAG进行压力测试。一个关键的见解是，虽然数据永远不能证明因果DAG是正确的，但它可以帮助显示它何时是错误的。你还将了解因果发现，这是一组从数据中学习因果DAG的算法。
- en: In this chapter, we explored building a simple causal graphical model on the
    DAG structure using pgmpy. Throughout the book, you’ll see how to build more sophisticated
    causal graphical models that leverage neural networks and automatic differentiation.
    Even in those more sophisticated models, the causal Markov property and the benefits
    of the DAG including causal invariance, and parameter modularity will still hold.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用pgmpy在DAG结构上构建简单的因果图形模型。在整个书中，你将看到如何构建更复杂的因果图形模型，这些模型利用神经网络和自动微分。即使在那些更复杂的模型中，因果马尔可夫性质和DAG的好处，包括因果不变性和参数模块化，仍然会存在。
- en: Summary
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The causal directed acyclic graph (DAG) can represent our causal assumptions
    about the data generating process (DGP).
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果有向无环图（DAG）可以表示我们对数据生成过程（DGP）的因果假设。
- en: The causal DAG is a useful tool for visualizing and communicating your causal
    assumptions.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果DAG是可视化并传达你的因果假设的有用工具。
- en: DAGs are fundamental data structures in computer science, and they admit many
    fast algorithms we can bring to bear on causal inference tasks.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAGs是计算机科学中的基本数据结构，并且允许我们使用许多快速算法来处理因果推断任务。
- en: DAGs link causality to conditional independence via the causal Markov property.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAGs通过因果马尔可夫性质将因果关系与条件独立性联系起来。
- en: DAGs can provide scaffolding for probabilistic ML models.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAGs可以为概率机器学习模型提供支架。
- en: We can use various methods for statistical parameter learning to train a probabilistic
    model on top of a DAG. These include maximum likelihood estimation and Bayesian
    estimation.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用各种统计参数学习方法在DAG之上训练概率模型。这包括最大似然估计和贝叶斯估计。
- en: Given a causal DAG, the modeler can choose from a variety of parameterizations
    of the causal Markov kernels in the DAG, ranging from conditional probability
    tables to regression models to neural networks.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个因果有向无环图（DAG），模型构建者可以从DAG中因果马尔可夫核的各种参数化中选择，范围从条件概率表到回归模型再到神经网络。
- en: A causally sufficient set of variables contains all common causes between pairs
    in that set.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个因果充分集包含该集合中所有对之间的共同原因。
- en: You can build a causal DAG by starting with a set of variables of interest,
    expanding that to a causally sufficient set, adding variables useful to causal
    inference analysis, and finally adding any variables that help the DAG communicate
    a complete story.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过从一组感兴趣的变量开始，将其扩展到因果充分集，添加对因果推断分析有用的变量，最后添加任何有助于有向无环图（DAG）传达完整故事的变量来构建一个因果有向无环图（DAG）。
- en: Each causal Markov kernel represents a distinct causal mechanism that determines
    how the child node is determined by its parents (assuming the DAG is correct).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个因果马尔可夫核代表一种独特的因果机制，它决定了子节点如何由其父节点决定（假设有向无环图（DAG）是正确的）。
- en: “Independence of mechanism” refers to how mechanisms are distinct from the others—a
    change to one mechanism does not affect the others.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “机制独立性”指的是机制如何与其他机制区分开来——对一个机制的更改不会影响其他机制。
- en: When you build a generative model on the causal DAG, the parameters of each
    causal Markov kernel represents an encoding of the underlying causal mechanism.
    This leads to “parameter modularity,” which enables you to learn each parameter
    set separately and even use common sense reasoning to estimate parameters instead
    of data.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你在因果有向无环图（DAG）上构建生成模型时，每个因果马尔可夫核的参数代表了对潜在因果机制的编码。这导致了“参数模块化”，这使得你可以单独学习每个参数集，甚至可以使用常识推理来估计参数而不是数据。
- en: The fact that each causal Markov kernel represents a distinct causal mechanism
    provides a source of invariance that can be leveraged in advanced tasks, like
    transfer learning, data fusion, and invariant prediction.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个因果马尔可夫核代表一个独特的因果机制，这为高级任务（如迁移学习、数据融合和不变预测）提供了可利用的不变性来源。
- en: You can specify a DAG by the roles variables play in a specific causal inference
    task.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过变量在特定因果推断任务中扮演的角色来指定一个DAG。
