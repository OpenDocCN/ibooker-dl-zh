<html><head></head><body><section data-pdf-bookmark="Chapter 20. Tuning Generative Image Models with LoRA and Diffusers" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch20_tuning_generative_image_models_with_lora_and_diffu_1748550104901965">
      <h1><span class="label">Chapter 20. </span>Tuning Generative Image Models <span class="keep-together">with LoRA and Diffusers</span></h1>
      <p>In <a data-type="xref" href="ch19.html#ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373">Chapter 19</a>, you explored the idea of diffusers and how models trained with diffusion techniques can generate images based on prompts. Like text-based models (as we explored in <a data-type="xref" href="ch16.html#ch16_using_llms_with_custom_data_1748550037719939">Chapter 16</a>), text-to-image models<a contenteditable="false" data-primary="fine-tuning generative image models" data-type="indexterm" id="20all"/><a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="tuning generative image models" data-tertiary="about" data-type="indexterm" id="id1986"/><a contenteditable="false" data-primary="generative AI" data-secondary="tuning image models" data-tertiary="about" data-type="indexterm" id="id1987"/><a contenteditable="false" data-primary="fine-tuning generative image models" data-secondary="about" data-type="indexterm" id="id1988"/><a contenteditable="false" data-primary="image generator" data-secondary="tuning generative image models" data-tertiary="about" data-type="indexterm" id="id1989"/> can be fine-tuned for specific tasks. The architecture of diffusion models and how to fine-tune them is enough for a full book in its own right, so in this chapter, you’ll just explore these concepts at a high level. <a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="LoRA trained with diffusers" data-tertiary="about LoRA" data-type="indexterm" id="id1990"/><a contenteditable="false" data-primary="generative AI" data-secondary="LoRA trained with diffusers" data-tertiary="about LoRA" data-type="indexterm" id="id1991"/><a contenteditable="false" data-primary="image generator" data-secondary="LoRA trained with diffusers" data-tertiary="about LoRA" data-type="indexterm" id="id1992"/><a contenteditable="false" data-primary="fine-tuning generative image models" data-secondary="LoRA trained with diffusers" data-tertiary="about LoRA" data-type="indexterm" id="id1993"/><a contenteditable="false" data-primary="LoRA (low-rank adaptation)" data-secondary="about" data-type="indexterm" id="id1994"/>There are several techniques for doing this, including <em>DreamBooth, textual inversion,</em> and the more recent <em>low-ranking adaptation </em>(LoRA), which you’ll go through step-by step in this chapter. This last technique allows you to customize models for a specific subject or style with very little data.</p>
      <p>As with transformers, the diffusers Hugging Face library<a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-seealso="fine-tuning generative image models" data-tertiary="tuning generative image models" data-type="indexterm" id="id1995"/> is designed to make using diffusers, as well as fine-tuning them, as easy as possible. To that end, it includes pre-built scripts that you can use. </p>
      <p>We’ll go through a full sample of creating a dataset of a fictitious digital influencer called Misato, using LoRA and diffusers to fine-tune a text-to-image model called Stable Diffusion 2 for her. Then, we’ll perform text-to-image inference to demonstrate how to create new images of Misato (see <a data-type="xref" href="#ch20_figure_1_1748550104889464">Figure 20-1</a>).</p>
      <figure><div class="figure" id="ch20_figure_1_1748550104889464">
        <img src="assets/aiml_2001.png"/>
        <h6><span class="label">Figure 20-1. </span>LoRA-tuned Stable Diffusion 2 images</h6>
      </div></figure>
      <section data-pdf-bookmark="Training a LoRA with Diffusers" data-type="sect1"><div class="sect1" id="ch20_training_a_lora_with_diffusers_1748550104902305">
        <h1>Training a LoRA with Diffusers</h1>
        <p>To train a LoRA with diffusers,<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="LoRA trained with diffusers" data-type="indexterm" id="ch20all"/><a contenteditable="false" data-primary="LoRA (low-rank adaptation)" data-secondary="training with diffusers" data-type="indexterm" id="ch20all2"/><a contenteditable="false" data-primary="generative AI" data-secondary="LoRA trained with diffusers" data-type="indexterm" id="ch20all3"/><a contenteditable="false" data-primary="image generator" data-secondary="LoRA trained with diffusers" data-type="indexterm" id="ch20all4"/><a contenteditable="false" data-primary="fine-tuning generative image models" data-secondary="LoRA trained with diffusers" data-type="indexterm" id="ch20all5"/> you’ll need to perform the following steps. First, you’ll need to get the source code for diffusers so you can have access to its premade training scripts. Then, you’ll get or create a dataset that you can use to fine-tune Stable Diffusion. After that, you’ll run the training scripts to get a fine-tune for the model, publish the fine-tune to Hugging Face, and run inference against the base model with the LoRA layers applied. Once you’re done, you should be able to create images like those shown in <a data-type="xref" href="#ch20_figure_1_1748550104889464">Figure 20-1</a>. Let’s walk through each of these steps.</p>
        <section data-pdf-bookmark="Getting Diffusers" data-type="sect2"><div class="sect2" id="ch20_getting_diffusers_1748550104902437">
          <h2>Getting Diffusers</h2>
          <p>To get started with LoRA,<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="tuning generative image models" data-tertiary="getting diffusers" data-type="indexterm" id="id1996"/><a contenteditable="false" data-primary="generative AI" data-secondary="tuning image models" data-tertiary="getting diffusers" data-type="indexterm" id="id1997"/><a contenteditable="false" data-primary="image generator" data-secondary="tuning generative image models" data-tertiary="getting diffusers" data-type="indexterm" id="id1998"/><a contenteditable="false" data-primary="LoRA (low-rank adaptation)" data-secondary="training with diffusers" data-tertiary="getting diffusers" data-type="indexterm" id="id1999"/><a contenteditable="false" data-primary="fine-tuning generative image models" data-secondary="getting diffusers" data-type="indexterm" id="id2000"/> I have found the best thing to do is to first clone the source code for diffusers to get the training scripts. </p>
          <p>You can do this quite simply by git-cloning it, changing into the directory, and running <code>pip install</code> at the current location:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">git</code> <code class="n">clone</code> <code class="n">https</code><code class="p">:</code><code class="o">//</code><code class="n">github</code><code class="o">.</code><code class="n">com</code><code class="o">/</code><code class="n">huggingface</code><code class="o">/</code><code class="n">diffusers</code>
<code class="n">cd</code> <code class="n">diffusers</code>
<code class="n">pip</code> <code class="n">install</code> <code class="o">.</code></pre>
          <p>If you’re using Colab or another hosted notebook, you’ll use syntax like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="err">!</code><code class="n">git</code> <code class="n">clone</code> <code class="n">https</code><code class="p">:</code><code class="o">//</code><code class="n">github</code><code class="o">.</code><code class="n">com</code><code class="o">/</code><code class="n">huggingface</code><code class="o">/</code><code class="n">diffusers</code>
<code class="o">%</code><code class="n">cd</code> <code class="n">diffusers</code>
<code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="o">.</code></pre>
          <p>This will give you a local version of diffusers that you can use. The text-to-image LoRA fine-tuning scripts are in the <em>/diffusers/examples/text_to_image</em> directory, and you’ll need to install their dependencies like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="o">%</code><code class="n">cd</code> <code class="o">/</code><code class="n">content</code><code class="o">/</code><code class="n">diffusers</code><code class="o">/</code><code class="n">examples</code><code class="o">/</code><code class="n">text_to_image</code> <code class="c1"># or whatever your dir is</code>
<code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="o">-</code><code class="n">r</code> <code class="n">requirements</code><code class="o">.</code><code class="n">txt</code></pre>
          <p>These dependencies include the specific versions of tools like accelerate, transformers, and torchvision. It’s good to git-clone from source so that you get the latest versions of the <em>requirements.txt</em> to make your life easier!</p>
          <p>Finally, you’ll also need the xformers library, which is designed to make transformers more efficient and thus speed up the process for you. You can get it like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">xformers</code></pre>
          <p>Now, you have a diffusers environment that you can use for fine-tuning. In the next step, you’ll get the data.</p>
        </div></section>
        <section data-pdf-bookmark="Getting Data for Fine-Tuning a LoRA" data-type="sect2"><div class="sect2" id="ch20_getting_data_for_fine_tuning_a_lora_1748550104902534">
          <h2>Getting Data for Fine-Tuning a LoRA</h2>
          <p>The two main ways in which<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="tuning generative image models" data-tertiary="getting data" data-type="indexterm" id="ch20dat"/><a contenteditable="false" data-primary="generative AI" data-secondary="tuning image models" data-tertiary="getting data" data-type="indexterm" id="ch20dat2"/><a contenteditable="false" data-primary="image generator" data-secondary="tuning generative image models" data-tertiary="getting data" data-type="indexterm" id="ch20dat3"/><a contenteditable="false" data-primary="LoRA (low-rank adaptation)" data-secondary="training with diffusers" data-tertiary="getting data" data-type="indexterm" id="ch20dat4"/><a contenteditable="false" data-primary="fine-tuning generative image models" data-secondary="getting data" data-type="indexterm" id="ch20dat5"/> you’ll fine-tune a LoRA are for <em>style</em> and for <em>subject. </em>In the former case, you can get a number of images of the specific style that you want and train the model so that it will output in that style. I would urge caution when doing this because many artists earn their livelihood from their style of creation, and you should respect that. Similarly, you should consider the impact of training models based on commercial styles. Unfortunately, many of the tutorials I have seen online ignore this, and such practices bring down the overall impact of AI and drive the narrative of generative AI away from being <em>creative</em> and toward <em>stealing IP</em>. So, please be careful with that.</p>
          <p>Similarly, when it comes to the subject, I see many tutorials that use examples of doing a Google Image search for a celebrity so you can create a LoRA of them. Again, I would urge you <em>not</em> to do this. Please only create a LoRA for someone whose likeness you have permission to use. </p>
          <p>So that you can have something you <em>can</em> use,<a contenteditable="false" data-primary="datasets" data-secondary="digital influencer Misato" data-type="indexterm" id="id2001"/><a contenteditable="false" data-primary="online resources" data-secondary="digital influencer Misato" data-type="indexterm" id="id2002"/><a contenteditable="false" data-primary="digital influencer Misato dataset" data-type="indexterm" id="id2003"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="digital influencer Misato dataset" data-type="indexterm" id="id2004"/> I created a dataset for a digital influencer. I call her Misato, after my favorite character in a popular anime. All of the images were rendered by me using the popular Daz 3D rendering software.</p>
          <p>You can find this dataset on the <a href="https://oreil.ly/Y1qeY">Hugging Facewebsite</a>.</p>
          <p>If you want to create a dataset like this, I would recommend that you use images of the same figure from multiple angles that also focus on specific segments. For example, you can use these:</p>
          <ul>
            <li>
              <p>3–4 portrait headshots (passport-style photos)</p>
            </li>
            <li>
              <p>3–4 three-quarters headshots from each side</p>
            </li>
            <li>
              <p>3–4 profile pictures, showing the side of the face</p>
            </li>
            <li>
              <p>3–4 full-length body shots</p>
            </li>
          </ul>
          <p class="pagebreak-before">For each of these images, you also need a prompt that describes the image. You’ll use this in training to give context to the image and how it should be represented.</p>
          <p>So, for example, consider <a data-type="xref" href="#ch20_figure_2_1748550104889522">Figure 20-2</a>, which is a portrait shot that I generated for Misato.</p>
          <figure><div class="figure" id="ch20_figure_2_1748550104889522">
            <img src="assets/aiml_2002.png"/>
            <h6><span class="label">Figure 20-2. </span>Portrait shot of Misato from the dataset</h6>
          </div></figure>
          <p>This image is paired with the following prompt: “Photo of (lora-misato-token), high-quality portrait, clear facial features, neutral expression, front view, natural lighting.”</p>
          <p>Note the use of (lora-misato-token), where we indicate the subject of the image. Later, when we create prompts to generate new images, we can use the same token—for example, “(lora-misato-token) in food ad, billboard sign, 90s, anime, Japanese pop, Japanese words, front view, plain background.” This prompt will give us what you can see in <a data-type="xref" href="#ch20_figure_3_1748550104889557">Figure 20-3</a>. We have an entirely new composition, with Misato as the model in a fast-food campaign! </p>
          <p>Once you have a set of images, you’ll need to create a <em>metadata.jsonl</em> file that contains the images associated with their prompts in a standard format that you can use when fine-tuning. It’s JSON with a link to the filename and the prompt for that image. The one for Misato is on the <a href="https://oreil.ly/MfmGh">Hugging Face website</a>.</p>
                    <figure><div class="figure" id="ch20_figure_3_1748550104889557">
            <img src="assets/aiml_2003.png"/>
            <h6><span class="label">Figure 20-3. </span>Inference from a LoRA token</h6>
          </div></figure>
          <p>A snippet of the <em>metadata.jsonl</em> file is here:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="p">{</code> <code class="s2">"file_name"</code><code class="p">:</code> <code class="s2">"rightprofile-smile.png"</code><code class="p">,</code> 
               <code class="s2">"prompt"</code><code class="p">:</code> <code class="s2">"photo of (lora-misato-token), </code><code class="w"/>
               <code class="n">right</code> <code class="n">side</code> <code class="n">profile</code><code class="p">,</code> <code class="n">high</code> <code class="n">quality</code><code class="p">,</code> <code class="n">detailed</code> <code class="n">features</code><code class="p">,</code> 
               <code class="n">smiling</code><code class="p">,</code> <code class="n">professional</code> <code class="n">photo</code> <code class="s2">"}</code><code class="w"/>
 
<code class="p">{</code> <code class="s2">"file_name"</code><code class="p">:</code> <code class="s2">"rightprofile-neutral.png"</code><code class="p">,</code> 
               <code class="s2">"prompt"</code><code class="p">:</code> <code class="s2">"photo of (lora-misato-token), </code><code class="w"/>
               <code class="n">right</code> <code class="n">side</code> <code class="n">profile</code><code class="p">,</code> <code class="n">high</code> <code class="n">quality</code><code class="p">,</code> <code class="n">detailed</code> <code class="n">features</code><code class="p">,</code> 
               <code class="n">professional</code> <code class="n">photo</code> <code class="s2">"}</code><code class="w"/></pre>
          <p>That’s pretty much all you need. For training with diffusers, I’ve found it much easier if you publish your dataset on Hugging Face. To do this, when logged in, visit the <a href="https://oreil.ly/Ez3Gp">Hugging Face website</a>. There, you’ll be able to specify the name of the new dataset and whether or not it’s public. Once you’ve done this, you’ll be able to upload the files through the web interface (see <a data-type="xref" href="#ch20_figure_4_1748550104889591">Figure 20-4</a>).</p>
          <p>Once you’ve done this, your dataset will be available at <a href="https://huggingface.co/datasets/"><em>https://huggingface.co/datasets/</em></a><em>&lt;yourname&gt;/&lt;datasetname&gt;</em>. So, for example, my username (see <a data-type="xref" href="#ch20_figure_4_1748550104889591">Figure 20-4</a>) is “lmoroney,” and the dataset name is “misato,” so you can see this dataset at <a href="https://huggingface.co/datasets/lmoroney/misato"><em>https://huggingface.co/datasets/lmoroney/misato</em></a>.<a contenteditable="false" data-primary="" data-startref="ch20dat" data-type="indexterm" id="id2005"/><a contenteditable="false" data-primary="" data-startref="ch20dat2" data-type="indexterm" id="id2006"/><a contenteditable="false" data-primary="" data-startref="ch20dat3" data-type="indexterm" id="id2007"/><a contenteditable="false" data-primary="" data-startref="ch20dat4" data-type="indexterm" id="id2008"/><a contenteditable="false" data-primary="" data-startref="ch20dat5" data-type="indexterm" id="id2009"/></p>
                    <figure><div class="figure" id="ch20_figure_4_1748550104889591">
            <img src="assets/aiml_2004.png"/>
            <h6><span class="label">Figure 20-4. </span>Creating a new dataset on Hugging Face</h6>
          </div></figure>
        </div></section>
        <section data-pdf-bookmark="Fine-Tuning a Model with Diffusers" data-type="sect2"><div class="sect2" id="ch20_fine_tuning_a_model_with_diffusers_1748550104902639">
          <h2>Fine-Tuning a Model with Diffusers</h2>
          <p>As mentioned earlier, when you<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="tuning generative image models" data-tertiary="fine-tuning the model" data-type="indexterm" id="ch20ft"/><a contenteditable="false" data-primary="generative AI" data-secondary="tuning image models" data-tertiary="fine-tuning the model" data-type="indexterm" id="ch20ft2"/><a contenteditable="false" data-primary="image generator" data-secondary="tuning generative image models" data-tertiary="fine-tuning the model" data-type="indexterm" id="ch20ft3"/><a contenteditable="false" data-primary="LoRA (low-rank adaptation)" data-secondary="training with diffusers" data-tertiary="fine-tuning the model" data-type="indexterm" id="ch20ft4"/><a contenteditable="false" data-primary="fine-tuning generative image models" data-secondary="fine-tuning the model" data-type="indexterm" id="ch20ft5"/> clone the diffusers repo, you get access to a number of example pre-written scripts that give you a head start in various tasks. One of these is training text-to-image LoRAs. But before running the script, it’s a good idea to use <code>accelerate</code>, which abstracts underlying accelerator hardware, including distribution across multiple chips. With <code>accelerate</code>, you can define a configuration. Find the details on the <a href="https://oreil.ly/TnaII">Hugging Face website</a>. </p>
          <p>For the purposes of simplicity, when you’re using Colab, here’s how you can set up a basic <code>accelerate</code> profile:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">accelerate.utils</code> <code class="kn">import</code> <code class="n">write_basic_config</code>
 
<code class="n">write_basic_config</code><code class="p">()</code></pre>
          <p>Then, once you have that, you can use <code>accelerate launch</code> to run the training script. Here’s an example:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="err">!</code><code class="n">accelerate</code> <code class="n">launch</code> <code class="n">train_text_to_image_lora</code><code class="o">.</code><code class="n">py</code> \
  <code class="o">--</code><code class="n">pretrained_model_name_or_path</code><code class="o">=</code><code class="s2">"stabilityai/stable-diffusion-2"</code> \
  <code class="o">--</code><code class="n">dataset_name</code><code class="o">=</code><code class="s2">"lmoroney/misato"</code> \
  <code class="o">--</code><code class="n">caption_column</code><code class="o">=</code><code class="s2">"prompt"</code> \
  <code class="o">--</code><code class="n">resolution</code><code class="o">=</code><code class="mi">512</code> \
  <code class="o">--</code><code class="n">random_flip</code> \
  <code class="o">--</code><code class="n">train_batch_size</code><code class="o">=</code><code class="mi">1</code> \
  <code class="o">--</code><code class="n">num_train_epochs</code><code class="o">=</code><code class="mi">1000</code> \
  <code class="o">--</code><code class="n">checkpointing_steps</code><code class="o">=</code><code class="mi">5000</code> \
  <code class="o">--</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">1e-04</code> \
  <code class="o">--</code><code class="n">lr_scheduler</code><code class="o">=</code><code class="s2">"constant"</code> \
  <code class="o">--</code><code class="n">lr_warmup_steps</code><code class="o">=</code><code class="mi">0</code> \
  <code class="o">--</code><code class="n">seed</code><code class="o">=</code><code class="mi">42</code> \
  <code class="o">--</code><code class="n">output_dir</code><code class="o">=</code><code class="s2">"/content/lm-misato-lora"</code></pre>
          <p>Note that running this is very computationally intensive. With the preceding set of hyperparameters (I’ll explain each one in a moment), using an A100 in Google Colab took me about 2 hours (or 17 compute units) to train. Compute units cost money (at the time of publication, about 10 cents each), so be sure to understand how this all works and that it does cost money!</p>
          <p>The script takes the following hyperparameters:</p>
          <dl>
            <dt>Pretrained_model_name_or_path</dt>
            <dd>
              <p>This can be a local folder (for example, <em>/content/model/</em>) or the location on <em>huggingface.co</em>—so for example, <a href="http://huggingface.co/stabilityai/stable-diffusion-2"><em>http://huggingface.co/stabilityai/stable-diffusion-2</em></a> is the location of the model called Stable Diffusion 2. You can also specify this without the <em>huggingface.co</em> part of the URL.</p>
            </dd>
            <dt>Dataset-name</dt>
            <dd>
              <p>Similarly, this can be a local directory containing the dataset or the address of it on <em>huggingface.co</em>. As you can see, I’m using the Misato dataset here.</p>
            </dd>
            <dt>Caption_column</dt>
            <dd>
              <p>This is the column in the <em>jsonl</em> file that contains the caption for the images. You can specify the caption here.</p>
            </dd>
            <dt>Resolution</dt>
            <dd>
              <p>This is the resolution that we’ll train the images for. In this case, it’s 512 × 512.</p>
            </dd>
            <dt>Random_Flip</dt>
            <dd>
              <p>This is image augmentation (as in <a data-type="xref" href="ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912">Chapter 3</a>). As the Misato dataset already has multiple angles covered, this probably isn’t needed.</p>
            </dd>
            <dt>Train_batch_size</dt>
            <dd>
              <p>This is the number of images per batch. It’s good to start with 1 and then tweak it as you see fit. When I was using the A100 GPU in Colab, I noticed that training was only using about 7 GB of the 40 GB, so this could be safely turned up to speed up training.</p>
            </dd>
            <dt>Num_training_epochs</dt>
            <dd>
              <p>This is how many epochs to train for.</p>
            </dd>
            <dt>Checkpointing_steps</dt>
            <dd>
              <p>This is how often you should save a checkpoint.</p>
            </dd>
            <dt>Learning_rate</dt>
            <dd>
              <p>This is the LR hyperparameter.</p>
            </dd>
            <dt>LR_scheduler</dt>
            <dd>
              <p>If you want to use an adjustable learning rate, you can specify the scheduler here. The nice thing with an adjustable LR is that the best LR later in the training cycle isn’t always the same as the best one from earlier in the cycle, so you can adjust it on the fly.</p>
            </dd>
            <dt>LR_Warmup_steps</dt>
            <dd>
              <p>This is the number of steps you’ll take to set the initial LR.</p>
            </dd>
            <dt>Seed</dt>
            <dd>
              <p>This is a random seed.</p>
            </dd>
            <dt>Output_dir</dt>
            <dd>
              <p>This is where you save the checkpoints as training happens.</p>
            </dd>
          </dl>
          <p>Then, when training, you’ll see a status that looks something like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">Resolving</code> <code class="n">data</code> <code class="n">files</code><code class="p">:</code> <code class="mi">100</code><code class="o">%</code> <code class="mi">22</code><code class="o">/</code><code class="mi">22</code> <code class="p">[</code><code class="mi">00</code><code class="p">:</code><code class="mi">00</code><code class="o">&lt;</code><code class="mi">00</code><code class="p">:</code><code class="mi">00</code><code class="p">,</code> <code class="mf">74.14</code><code class="n">it</code><code class="o">/</code><code class="n">s</code><code class="p">]</code>
<code class="mi">12</code><code class="o">/</code><code class="mi">30</code><code class="o">/</code><code class="mi">2024</code> <code class="mi">19</code><code class="p">:</code><code class="mi">23</code><code class="p">:</code><code class="mi">48</code> <code class="o">-</code> <code class="n">INFO</code> <code class="o">-</code> <code class="n">__main__</code> <code class="o">-</code> <code class="o">*****</code> <code class="n">Running</code> <code class="n">training</code> <code class="o">*****</code>
<code class="mi">12</code><code class="o">/</code><code class="mi">30</code><code class="o">/</code><code class="mi">2024</code> <code class="mi">19</code><code class="p">:</code><code class="mi">23</code><code class="p">:</code><code class="mi">48</code> <code class="o">-</code> <code class="n">INFO</code> <code class="o">-</code> <code class="n">__main__</code> <code class="o">-</code>   <code class="n">Num</code> <code class="n">examples</code> <code class="o">=</code> <code class="mi">21</code>
<code class="mi">12</code><code class="o">/</code><code class="mi">30</code><code class="o">/</code><code class="mi">2024</code> <code class="mi">19</code><code class="p">:</code><code class="mi">23</code><code class="p">:</code><code class="mi">48</code> <code class="o">-</code> <code class="n">INFO</code> <code class="o">-</code> <code class="n">__main__</code> <code class="o">-</code>   <code class="n">Num</code> <code class="n">Epochs</code> <code class="o">=</code> <code class="mi">1000</code>
<code class="mi">12</code><code class="o">/</code><code class="mi">30</code><code class="o">/</code><code class="mi">2024</code> <code class="mi">19</code><code class="p">:</code><code class="mi">23</code><code class="p">:</code><code class="mi">48</code> <code class="o">-</code> <code class="n">INFO</code> <code class="o">-</code> <code class="n">__main__</code> <code class="o">-</code>   <code class="n">Instantaneous</code> <code class="n">batch</code> <code class="n">size</code> <code class="n">per</code> <code class="n">device</code><code class="o">...</code>
<code class="mi">12</code><code class="o">/</code><code class="mi">30</code><code class="o">/</code><code class="mi">2024</code> <code class="mi">19</code><code class="p">:</code><code class="mi">23</code><code class="p">:</code><code class="mi">48</code> <code class="o">-</code> <code class="n">INFO</code> <code class="o">-</code> <code class="n">__main__</code> <code class="o">-</code>   <code class="n">Total</code> <code class="n">train</code> <code class="n">batch</code> <code class="n">size</code> <code class="p">(</code><code class="n">w</code><code class="o">.</code> <code class="n">parallel</code><code class="o">...</code>
<code class="mi">12</code><code class="o">/</code><code class="mi">30</code><code class="o">/</code><code class="mi">2024</code> <code class="mi">19</code><code class="p">:</code><code class="mi">23</code><code class="p">:</code><code class="mi">48</code> <code class="o">-</code> <code class="n">INFO</code> <code class="o">-</code> <code class="n">__main__</code> <code class="o">-</code>   <code class="n">Gradient</code> <code class="n">Accumulation</code> <code class="n">steps</code> <code class="o">=</code> <code class="mi">1</code>
<code class="mi">12</code><code class="o">/</code><code class="mi">30</code><code class="o">/</code><code class="mi">2024</code> <code class="mi">19</code><code class="p">:</code><code class="mi">23</code><code class="p">:</code><code class="mi">48</code> <code class="o">-</code> <code class="n">INFO</code> <code class="o">-</code> <code class="n">__main__</code> <code class="o">-</code>   <code class="n">Total</code> <code class="n">optimization</code> <code class="n">steps</code> <code class="o">=</code> <code class="mi">1000</code>
<code class="n">Steps</code><code class="p">:</code>  <code class="mi">10</code><code class="o">%</code> <code class="mi">103</code><code class="o">/</code><code class="mi">1000</code> <code class="p">[</code><code class="mi">05</code><code class="p">:</code><code class="mi">03</code><code class="o">&lt;</code><code class="mi">44</code><code class="p">:</code><code class="mi">00</code><code class="p">,</code>  <code class="mf">2.94</code><code class="n">s</code><code class="o">/</code><code class="n">it</code><code class="p">,</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.0001</code><code class="p">,</code> <code class="n">step_loss</code><code class="o">=</code><code class="mf">0.227</code><code class="p">]</code></pre>
          <p class="pagebreak-before">Once the model is trained, in its directory folder, you’ll see a structure like the one depicted in <a data-type="xref" href="#ch20_figure_5_1748550104889622">Figure 20-5</a>.</p>
          <figure><div class="figure" id="ch20_figure_5_1748550104889622">
            <img src="assets/aiml_2005.png"/>
            <h6><span class="label">Figure 20-5. </span>A trained directory</h6>
          </div></figure>
          <p>The original <code>model.safetensors</code> model is highlighted, and you can see that it is 3.47 GB in size! The fine-tuned LoRA, on the other hand, is much smaller at just 3.4 MB.</p>
          <p>You can use this in the next step, where you upload the model to the Hugging Face repository to make it very easy for inference to use it.<a contenteditable="false" data-primary="" data-startref="ch20ft" data-type="indexterm" id="id2010"/><a contenteditable="false" data-primary="" data-startref="ch20ft2" data-type="indexterm" id="id2011"/><a contenteditable="false" data-primary="" data-startref="ch20ft3" data-type="indexterm" id="id2012"/><a contenteditable="false" data-primary="" data-startref="ch20ft4" data-type="indexterm" id="id2013"/><a contenteditable="false" data-primary="" data-startref="ch20ft5" data-type="indexterm" id="id2014"/></p>
        </div></section>
        <section data-pdf-bookmark="Publishing Your Model" data-type="sect2"><div class="sect2" id="ch20_publishing_your_model_1748550104902734">
          <h2>Publishing Your Model</h2>
          <p>The fine-tuned directory that<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="tuning generative image models" data-tertiary="publishing the model" data-type="indexterm" id="ch20pub"/><a contenteditable="false" data-primary="generative AI" data-secondary="tuning image models" data-tertiary="publishing the model" data-type="indexterm" id="ch20pub2"/><a contenteditable="false" data-primary="image generator" data-secondary="tuning generative image models" data-tertiary="publishing the model" data-type="indexterm" id="ch20pub3"/><a contenteditable="false" data-primary="LoRA (low-rank adaptation)" data-secondary="training with diffusers" data-tertiary="publishing the model" data-type="indexterm" id="ch20pub4"/><a contenteditable="false" data-primary="fine-tuning generative image models" data-secondary="publishing the model" data-type="indexterm" id="ch20pub5"/><a contenteditable="false" data-primary="publishing your fine-tuned generative image model" data-type="indexterm" id="ch20pub6"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="publishing your generative image model" data-type="indexterm" id="ch20pub7"/> you’ve saved while training contains a lot more information than you need, including clones of the base model. As a result, if you try to publish and upload the model, you’ll end up taking a lot longer because you’ll have to upload lots of unneeded gigabytes!</p>
          <p>Therefore, you should edit your directory structure to remove the <em>model.safetensors</em> files from the checkpoint directories and keep the rest.</p>
          <p>Then, when you’re signed into Hugging Face, you can visit <a href="http://huggingface.co/new"><em>huggingface.co/new</em></a> to see the “Create New Model Repository” page (see <a data-type="xref" href="#ch20_figure_6_1748550104889653">Figure 20-6</a>).</p>
          <figure><div class="figure" id="ch20_figure_6_1748550104889653">
            <img src="assets/aiml_2006.png"/>
            <h6><span class="label">Figure 20-6. </span>Creating a new repository</h6>
          </div></figure>
          <p>Follow the steps, and be sure to select a license. Then, when you’re done, you can upload the files via the web interface in the next step. When you’re done with that, you should see something like the screen depicted in <a data-type="xref" href="#ch20_figure_7_1748550104889686">Figure 20-7</a>, where I named the model “finetuned-misato-sd2,” given that the data was “misato” and the model I tuned was Stable Diffusion 2.</p>
          <p>You can see this for yourself on the <a href="https://oreil.ly/zmlal">Hugging Face website</a>.</p>
          <figure><div class="figure" id="ch20_figure_7_1748550104889686">
            <img src="assets/aiml_2007.png"/>
            <h6><span class="label">Figure 20-7. </span>The fine-tuned Misato LoRA for Stable Diffusion 2</h6>
          </div></figure>
          <p>Now that the dataset and the model are both published on Hugging Face, using diffusers to do an inference with it is super simple. We’ll see that in the next step.<a contenteditable="false" data-primary="" data-startref="ch20pub" data-type="indexterm" id="id2015"/><a contenteditable="false" data-primary="" data-startref="ch20pub2" data-type="indexterm" id="id2016"/><a contenteditable="false" data-primary="" data-startref="ch20pub3" data-type="indexterm" id="id2017"/><a contenteditable="false" data-primary="" data-startref="ch20pub4" data-type="indexterm" id="id2018"/><a contenteditable="false" data-primary="" data-startref="ch20pub5" data-type="indexterm" id="id2019"/><a contenteditable="false" data-primary="" data-startref="ch20pub6" data-type="indexterm" id="id2020"/><a contenteditable="false" data-primary="" data-startref="ch20pub7" data-type="indexterm" id="id2021"/></p>
        </div></section>
        <section data-pdf-bookmark="Generating an Image with the Custom LoRA" data-type="sect2"><div class="sect2" id="ch20_generating_an_image_with_the_custom_lora_1748550104902829">
          <h2>Generating an Image with the Custom LoRA</h2>
          <p>To create an image using<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="LoRA trained with diffusers" data-tertiary="generating an image" data-type="indexterm" id="ch20gen"/><a contenteditable="false" data-primary="generative AI" data-secondary="LoRA trained with diffusers" data-tertiary="generating an image" data-type="indexterm" id="ch20gen2"/><a contenteditable="false" data-primary="image generator" data-secondary="LoRA trained with diffusers" data-tertiary="generating an image" data-type="indexterm" id="ch20gen3"/><a contenteditable="false" data-primary="LoRA (low-rank adaptation)" data-secondary="training with diffusers" data-tertiary="generating an image" data-type="indexterm" id="ch20gen4"/><a contenteditable="false" data-primary="fine-tuning generative image models" data-secondary="LoRA trained with diffusers" data-tertiary="generating an image" data-type="indexterm" id="ch20gen5"/><a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="tuning generative image models" data-tertiary="generating an image" data-type="indexterm" id="ch20gen6"/><a contenteditable="false" data-primary="generative AI" data-secondary="tuning image models" data-tertiary="generating an image" data-type="indexterm" id="ch20gen7"/><a contenteditable="false" data-primary="image generator" data-secondary="tuning generative image models" data-tertiary="generating an image" data-type="indexterm" id="ch20gen8"/><a contenteditable="false" data-primary="LoRA (low-rank adaptation)" data-secondary="training with diffusers" data-tertiary="generating an image" data-type="indexterm" id="ch20gen9"/> the custom LoRA, we’ll go through a process that’s similar to the one in <a data-type="xref" href="ch19.html#ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373">Chapter 19</a>. You’ll use diffusers to create a pipeline, <a contenteditable="false" data-primary="schedulers" data-type="indexterm" id="id2022"/><a contenteditable="false" data-primary="online resources" data-secondary="schedulers" data-type="indexterm" id="id2023"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="schedulers" data-type="indexterm" id="id2024"/><a contenteditable="false" data-primary="EulerAncestralDiscreteScheduler" data-type="indexterm" id="id2025"/>but you’ll also add a scheduler. In stable diffusion, the role of the scheduler determines how the image evolves from random noise to the final image. Not all schedulers work with LoRA, and you’ll have to ensure that the scheduler you use works with the base model you’re working with. </p>
          <p>There are lots of schedulers you can use, and you can find them on the <a href="https://oreil.ly/SUlZl">Hugging Face website</a>.</p>
          <p>In this case, you can experiment with using the <code>EulerAncestralDiscreteScheduler</code>:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">diffusers</code> <code class="kn">import</code> <code class="p">(</code>
    <code class="n">StableDiffusionPipeline</code><code class="p">,</code>
    <code class="n">EulerAncestralDiscreteScheduler</code><code class="p">,</code>  
<code class="p">)</code></pre>
          <p class="pagebreak-before">Then, specify our <code>model_id</code> and pick the appropriate version of the scheduler for it:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">model_id</code> <code class="o">=</code> <code class="s2">"stabilityai/stable-diffusion-2"</code>
 
<code class="c1"># Choose your device</code>
<code class="n">device</code> <code class="o">=</code> <code class="s2">"cuda"</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">()</code> <code class="k">else</code> <code class="s2">"cpu"</code>
 
<code class="c1"># 1. Pick your scheduler</code>
<code class="n">scheduler</code> <code class="o">=</code> <code class="n">EulerAncestralDiscreteScheduler</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="n">model_id</code><code class="p">,</code>
    <code class="n">subfolder</code><code class="o">=</code><code class="s2">"scheduler"</code>
<code class="p">)</code></pre>
          <p>Once you’ve done that, you can create the pipeline from the <code>StableDiffusionPipeline</code> class and load it to the accelerator device:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 2. Load the pipeline with the chosen scheduler</code>
<code class="n">pipe</code> <code class="o">=</code> <code class="n">StableDiffusionPipeline</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="n">model_id</code><code class="p">,</code>
    <code class="n">scheduler</code><code class="o">=</code><code class="n">scheduler</code><code class="p">,</code>
    <code class="n">torch_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code>
<code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>
          <p>The next step is to assign the new LoRA weights, which are the retrained layers that determine the new behavior of the model:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 3. (Optional) Load LoRA weights</code>
<code class="n">pipe</code><code class="o">.</code><code class="n">load_lora_weights</code><code class="p">(</code><code class="s2">"lmoroney/finetuned-misato-sd2"</code><code class="p">)</code></pre>
          <p>Stable diffusion supports both a prompt <em>and</em> a negative prompt, where the first prompt defines what you want in the image and the second prompt defines what you<em> do not</em> want. Here’s an example:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 4. Define prompts and parameters</code>
<code class="n">prompt</code> <code class="o">=</code> <code class="s2">"(lora-misato-token) in food ad, billboard sign, 90s, anime, </code><code class="w"/>
         <code class="n">japanese</code> <code class="n">pop</code><code class="p">,</code> <code class="n">japanese</code> <code class="n">words</code><code class="p">,</code> <code class="n">front</code> <code class="n">view</code><code class="p">,</code> <code class="n">plain</code> <code class="n">background</code><code class="s2">"</code><code class="w"/>
 
<code class="n">negative_prompt</code> <code class="o">=</code> <code class="p">(</code>
    <code class="s2">"(deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, </code><code class="w"/>
     <code class="n">wrong</code> <code class="n">anatomy</code><code class="p">,</code> <code class="s2">"</code><code class="w"/>
    <code class="s2">"extra limb, missing limb, floating limbs, (mutated hands and </code><code class="w"/>
     <code class="n">fingers</code><code class="p">:</code><code class="mf">1.4</code><code class="p">),</code> <code class="s2">"</code><code class="w"/>
    <code class="s2">"disconnected limbs, mutation, mutated, ugly, disgusting, blurry, </code><code class="w"/>
     <code class="n">amputation</code><code class="s2">"</code><code class="w"/>
<code class="p">)</code></pre>
          <p>The negative prompt is very useful in helping you avoid some of the issues with AI-generated visuals, such as deformed hands and faces.</p>
          <p class="pagebreak-before less_space">Next up is to define the hyperparameters, such as the number of inference steps, the size of the image, and the seed. There’s also a parameter called <em>guidance scale</em>, which controls how imaginative your model is. A guidance scale value of less than 5 gives the model more creative freedom, but the model may not follow your prompt closely. A guidance scale value that’s higher than 7 will make the model adhere more strongly to your prompt, but it can also lead to strange artifacts. The guidance scale value in the middle—6—is a nice balance between freedom and adherence. There’s no hard and fast rule, so feel free to experiment:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">num_inference_steps</code> <code class="o">=</code> <code class="mi">50</code>
<code class="n">guidance_scale</code> <code class="o">=</code> <code class="mf">6.0</code>
<code class="n">width</code> <code class="o">=</code> <code class="mi">512</code>
<code class="n">height</code> <code class="o">=</code> <code class="mi">512</code>
<code class="n">seed</code> <code class="o">=</code> <code class="mi">1234567</code></pre>
          <p>Next, you just generate the image as usual:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 5. Create a generator for reproducible results</code>
<code class="n">generator</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">Generator</code><code class="p">(</code><code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code>
 
<code class="c1"># 6. Run the pipeline</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code>
    <code class="n">prompt</code><code class="p">,</code>
    <code class="n">negative_prompt</code><code class="o">=</code><code class="n">negative_prompt</code><code class="p">,</code>
    <code class="n">width</code><code class="o">=</code><code class="n">width</code><code class="p">,</code>
    <code class="n">height</code><code class="o">=</code><code class="n">height</code><code class="p">,</code>
    <code class="n">num_inference_steps</code><code class="o">=</code><code class="n">num_inference_steps</code><code class="p">,</code>
    <code class="n">guidance_scale</code><code class="o">=</code><code class="n">guidance_scale</code><code class="p">,</code>
    <code class="n">generator</code><code class="o">=</code><code class="n">generator</code><code class="p">,</code>
<code class="p">)</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
 
<code class="c1"># 7. Save the result</code>
<code class="n">image</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"lora-with-negative.png"</code><code class="p">)</code></pre>
          <p>As an experiment, you can try using a different scheduler with the same hyperparameters to yield similar results (see <a data-type="xref" href="#ch20_figure_8_1748550104889721">Figure 20-8</a>):</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># For DPMSolver, use:</code>
<code class="kn">from</code> <code class="nn">diffusers</code> <code class="kn">import</code> <code class="n">DPMSolverMultistepScheduler</code>
 
<code class="n">scheduler</code> <code class="o">=</code> <code class="n">DPMSolverMultistepScheduler</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_id</code><code class="p">,</code> 
            <code class="n">subfolder</code><code class="o">=</code><code class="s2">"scheduler"</code><code class="p">,</code> <code class="n">algorithm_type</code><code class="o">=</code><code class="s2">"dpmsolver++"</code><code class="p">)</code></pre>
          <figure><div class="figure" id="ch20_figure_8_1748550104889721">
            <img src="assets/aiml_2008.png"/>
            <h6><span class="label">Figure 20-8. </span>The same prompt and hyperparameters with different schedulers</h6>
          </div></figure>
          <p>Note that the text in the image is entirely made up, but given that the prompt is about advertisements, the tone is similar. In the picture on the left, the characters represent “loneliness” and “no,” while in the image on the right, they suggest “husband split?”</p>
          <p>What’s most interesting is the consistency in the character! For example, consider <a data-type="xref" href="#ch20_figure_9_1748550104889755">Figure 20-9</a>, in which Misato was painted in the styles of Monet and Picasso. We can see that the features learned by LoRA were consistent enough to (mostly) survive the restyling process.</p>
          <figure><div class="figure" id="ch20_figure_9_1748550104889755">
            <img src="assets/aiml_2009.png"/>
            <h6><span class="label">Figure 20-9. </span>Character consistency across styles</h6>
          </div></figure>
          <p>This example used Stable Diffusion 2, which is an older model but one that’s easy to tune with LoRA. As you use more advanced models and tune them, you can get much better results, but the time and costs of tuning will be much higher. I’d recommend starting with a simpler model like this one and working on your craft. From there, you can build up to the more advanced models. </p>
          <p>Additionally, Misato’s synthetic nature has triggered different features in the LoRA retraining, leading to the new images that have been created from her having a low-res, highly synthetic look. While the images have been close to photoreal to the human eye, they clearly haven’t been to the model, which learned a LoRA that was very CGI in nature and lower resolution than the ones in the training set!<a contenteditable="false" data-primary="" data-startref="ch20all" data-type="indexterm" id="id2026"/><a contenteditable="false" data-primary="" data-startref="ch20all2" data-type="indexterm" id="id2027"/><a contenteditable="false" data-primary="" data-startref="ch20all3" data-type="indexterm" id="id2028"/><a contenteditable="false" data-primary="" data-startref="ch20all4" data-type="indexterm" id="id2029"/><a contenteditable="false" data-primary="" data-startref="ch20all5" data-type="indexterm" id="id2030"/><a contenteditable="false" data-primary="" data-startref="20all" data-type="indexterm" id="id2031"/><a contenteditable="false" data-primary="" data-startref="ch20gen" data-type="indexterm" id="id2032"/><a contenteditable="false" data-primary="" data-startref="ch20gen2" data-type="indexterm" id="id2033"/><a contenteditable="false" data-primary="" data-startref="ch20gen3" data-type="indexterm" id="id2034"/><a contenteditable="false" data-primary="" data-startref="ch20gen4" data-type="indexterm" id="id2035"/><a contenteditable="false" data-primary="" data-startref="ch20gen5" data-type="indexterm" id="id2036"/><a contenteditable="false" data-primary="" data-startref="ch20gen6" data-type="indexterm" id="id2037"/><a contenteditable="false" data-primary="" data-startref="ch20gen7" data-type="indexterm" id="id2038"/><a contenteditable="false" data-primary="" data-startref="ch20gen8" data-type="indexterm" id="id2039"/><a contenteditable="false" data-primary="" data-startref="ch20gen9" data-type="indexterm" id="id2040"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch20_summary_1748550104902916">
        <h1>Summary</h1>
        <p>In this chapter, you had a walk-through of how to fine tune a text-to-image model like stable diffusion by using LoRA and the diffusers library. This technique allows you to customize models for a specific subject or style with a small custom file. In this case, you saw how to tune Stable Diffusion 2 for a synthetic character. In this chapter, you also went through all the steps—from cloning diffusers to creating a training environment for them that included a fully custom dataset. You learned how to use the training scripts to create a new LoRA based on the synthetic character and how to publish that to Hugging Face. Finally, you saw how to apply the LoRA to the model at inference time to create novel images using the LoRA for the Misato character!</p>
      </div></section>
    </div></section></body></html>