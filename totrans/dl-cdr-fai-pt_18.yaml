- en: Chapter 14\. ResNets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章。ResNets
- en: In this chapter, we will build on top of the CNNs introduced in the previous
    chapter and explain to you the ResNet (residual network) architecture. It was
    introduced in 2015 by Kaiming He et al. in the article [“Deep Residual Learning
    for Image Recognition”](https://oreil.ly/b68K8) and is by far the most used model
    architecture nowadays. More recent developments in image models almost always
    use the same trick of residual connections, and most of the time, they are just
    a tweak of the original ResNet.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在上一章介绍的CNN基础上构建，并向您解释ResNet（残差网络）架构。它是由Kaiming He等人于2015年在文章[“Deep Residual
    Learning for Image Recognition”](https://oreil.ly/b68K8)中引入的，到目前为止是最常用的模型架构。最近在图像模型中的发展几乎总是使用残差连接的相同技巧，大多数时候，它们只是原始ResNet的调整。
- en: We will first show you the basic ResNet as it was first designed and then explain
    the modern tweaks that make it more performant. But first, we will need a problem
    a little bit more difficult than the MNIST dataset, since we are already close
    to 100% accuracy with a regular CNN on it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先展示最初设计的基本ResNet，然后解释使其性能更好的现代调整。但首先，我们需要一个比MNIST数据集更难一点的问题，因为我们已经在常规CNN上接近100%的准确率了。
- en: Going Back to Imagenette
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回到Imagenette
- en: It’s going to be tough to judge any improvements we make to our models when
    we are already at an accuracy that is as high as we saw on MNIST in the previous
    chapter, so we will tackle a tougher image classification problem by going back
    to Imagenette. We’ll stick with small images to keep things reasonably fast.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们已经在上一章的MNIST中看到的准确率已经很高时，要评估我们对模型的任何改进将会很困难，因此我们将通过回到Imagenette来解决一个更困难的图像分类问题。我们将继续使用小图像以保持事情相对快速。
- en: 'Let’s grab the data—we’ll use the already-resized 160 px version to make things
    faster still, and will random crop to 128 px:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取数据——我们将使用已经调整大小为160像素的版本以使事情更快，然后将随机裁剪到128像素：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](Images/dlcf_14in01.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_14in01.png)'
- en: When we looked at MNIST, we were dealing with 28×28-pixel images. For Imagenette,
    we are going to be training with 128×128-pixel images. Later, we would like to
    be able to use larger images as well—at least as big as 224×224-pixels, the ImageNet
    standard. Do you recall how we managed to get a single vector of activations for
    each image out of the MNIST convolutional neural network?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看MNIST时，我们处理的是28×28像素的图像。对于Imagenette，我们将使用128×128像素的图像进行训练。稍后，我们希望能够使用更大的图像，至少与224×224像素的ImageNet标准一样大。您还记得我们如何从MNIST卷积神经网络中获得每个图像的单个激活向量吗？
- en: 'The approach we used was to ensure that there were enough stride-2 convolutions
    such that the final layer would have a grid size of 1. Then we just flattened
    out the unit axes that we ended up with, to get a vector for each image (so, a
    matrix of activations for a mini-batch). We could do the same thing for Imagenette,
    but that would cause two problems:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用的方法是确保有足够的步幅为2的卷积，以使最终层具有1的网格大小。然后我们展平我们最终得到的单位轴，为每个图像获得一个向量（因此，对于一个小批量的激活矩阵）。我们可以对Imagenette做同样的事情，但这会导致两个问题：
- en: We’d need lots of stride-2 layers to make our grid 1×1 at the end—perhaps more
    than we would otherwise choose.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要很多步幅为2的层，才能使我们的网格在最后变成1×1的大小——可能比我们本来会选择的要多。
- en: The model would not work on images of any size other than the size we originally
    trained on.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型将无法处理除最初训练的大小之外的任何大小的图像。
- en: 'One approach to dealing with the first issue would be to flatten the final
    convolutional layer in a way that handles a grid size other than 1×1\. We could
    simply flatten a matrix into a vector as we have done before, by laying out each
    row after the previous row. In fact, this is the approach that convolutional neural
    networks up until 2013 nearly always took. The most famous example is the 2013
    ImageNet winner VGG, still sometimes used today. But there was another problem
    with this architecture: it not only did not work with images other than those
    of the same size used in the training set, but also required a lot of memory,
    because flattening out the convolutional layer resulted in many activations being
    fed into the final layers. Therefore, the weight matrices of the final layers
    were enormous.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 处理第一个问题的一种方法是以一种处理1×1以外的网格大小的方式展平最终的卷积层。我们可以简单地将矩阵展平为向量，就像我们以前做过的那样，通过将每一行放在前一行之后。事实上，这是卷积神经网络直到2013年几乎总是采用的方法。最著名的例子是2013年ImageNet的获奖者VGG，有时今天仍在使用。但这种架构还有另一个问题：它不仅不能处理与训练集中使用的相同大小的图像之外的图像，而且需要大量内存，因为展平卷积层导致许多激活被馈送到最终层。因此，最终层的权重矩阵是巨大的。
- en: 'This problem was solved through the creation of *fully convolutional networks*.
    The trick in fully convolutional networks is to take the average of activations
    across a convolutional grid. In other words, we can simply use this function:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题通过创建*完全卷积网络*来解决。完全卷积网络的技巧是对卷积网格中的激活进行平均。换句话说，我们可以简单地使用这个函数：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you see, it is taking the mean over the x- and y-axes. This function will
    always convert a grid of activations into a single activation per image. PyTorch
    provides a slightly more versatile module called `nn.AdaptiveAvgPool2d`, which
    averages a grid of activations into whatever sized destination you require (although
    we nearly always use a size of 1).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，它正在计算x轴和y轴上的平均值。这个函数将始终将一组激活转换为每个图像的单个激活。PyTorch提供了一个稍微更灵活的模块，称为`nn.AdaptiveAvgPool2d`，它将一组激活平均到您需要的任何大小的目标（尽管我们几乎总是使用大小为1）。
- en: 'A fully convolutional network, therefore, has a number of convolutional layers,
    some of which will be stride 2, at the end of which is an adaptive average pooling
    layer, a flatten layer to remove the unit axes, and finally a linear layer. Here
    is our first fully convolutional network:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个完全卷积网络具有多个卷积层，其中一些将是步幅为2的，在最后是一个自适应平均池化层，一个展平层来移除单位轴，最后是一个线性层。这是我们的第一个完全卷积网络：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We’re going to be replacing the implementation of `block` in the network with
    other variants in a moment, which is why we’re not calling it `conv` anymore.
    We’re also saving some time by taking advantage of fastai’s `ConvLayer`, which
    already provides the functionality of `conv` from the preceding chapter (plus
    a lot more!).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在网络中用其他变体替换`block`的实现，这就是为什么我们不再称其为`conv`。我们还通过利用fastai的`ConvLayer`节省了一些时间，它已经提供了前一章中`conv`的功能（还有更多！）。
- en: Stop and Think
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停下来思考
- en: 'Consider this question: would this approach make sense for an optical character
    recognition (OCR) problem such as MNIST? The vast majority of practitioners tackling
    OCR and similar problems tend to use fully convolutional networks, because that’s
    what nearly everybody learns nowadays. But it really doesn’t make any sense! You
    can’t decide, for instance, whether a number is a 3 or an 8 by slicing it into
    small pieces, jumbling them up, and deciding whether on average each piece looks
    like a 3 or an 8\. But that’s what adaptive average pooling effectively does!
    Fully convolutional networks are really a good choice only for objects that don’t
    have a single correct orientation or size (e.g., like most natural photos).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个问题：这种方法对于像MNIST这样的光学字符识别（OCR）问题是否有意义？绝大多数从事OCR和类似问题的从业者倾向于使用全卷积网络，因为这是现在几乎每个人都学习的。但这真的毫无意义！例如，你不能通过将数字切成小块、混在一起，然后决定每个块平均看起来像3还是8来判断一个数字是3还是8。但这正是自适应平均池化有效地做的事情！全卷积网络只对没有单一正确方向或大小的对象（例如大多数自然照片）是一个很好的选择。
- en: Once we are done with our convolutional layers, we will get activations of size
    `bs x ch x h x w` (batch size, a certain number of channels, height, and width).
    We want to convert this to a tensor of size `bs x ch`, so we take the average
    over the last two dimensions and flatten the trailing 1×1 dimension as we did
    in our previous model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成卷积层，我们将得到大小为`bs x ch x h x w`的激活（批量大小、一定数量的通道、高度和宽度）。我们想将其转换为大小为`bs x
    ch`的张量，因此我们取最后两个维度的平均值，并像在我们之前的模型中那样展平尾随的1×1维度。
- en: This is different from regular pooling in the sense that those layers will generally
    take the average (for average pooling) or the maximum (for max pooling) of a window
    of a given size. For instance, max pooling layers of size 2, which were very popular
    in older CNNs, reduce the size of our image by half on each dimension by taking
    the maximum of each 2×2 window (with a stride of 2).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这与常规池化不同，因为这些层通常会取给定大小窗口的平均值（对于平均池化）或最大值（对于最大池化）。例如，大小为2的最大池化层在旧的CNN中非常流行，通过在每个维度上取每个2×2窗口的最大值（步幅为2），将图像的尺寸减半。
- en: 'As before, we can define a `Learner` with our custom model and then train it
    on the data we grabbed earlier:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，我们可以使用我们自定义的模型定义一个`Learner`，然后在之前获取的数据上对其进行训练：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](Images/dlcf_14in02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_14in02.png)'
- en: '3e-3 is often a good learning rate for CNNs, and that appears to be the case
    here too, so let’s try that:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CNN来说，3e-3通常是一个很好的学习率，这在这里也是如此，所以让我们试一试：
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1.901582 | 2.155090 | 0.325350 | 00:07 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.901582 | 2.155090 | 0.325350 | 00:07 |'
- en: '| 1 | 1.559855 | 1.586795 | 0.507771 | 00:07 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.559855 | 1.586795 | 0.507771 | 00:07 |'
- en: '| 2 | 1.296350 | 1.295499 | 0.571720 | 00:07 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.296350 | 1.295499 | 0.571720 | 00:07 |'
- en: '| 3 | 1.144139 | 1.139257 | 0.639236 | 00:07 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.144139 | 1.139257 | 0.639236 | 00:07 |'
- en: '| 4 | 1.049770 | 1.092619 | 0.659108 | 00:07 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.049770 | 1.092619 | 0.659108 | 00:07 |'
- en: That’s a pretty good start, considering we have to pick the correct one of 10
    categories, and we’re training from scratch for just 5 epochs! We can do way better
    than this using a deeper model, but just stacking new layers won’t really improve
    our results (you can try and see for yourself!). To work around this problem,
    ResNets introduce the idea of *skip connections*. We’ll explore those and other
    aspects of ResNets in the next section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们必须从头开始选择10个类别中的正确一个，而且我们只训练了5个时期，这是一个相当不错的开始！我们可以通过使用更深的模型做得更好，但只是堆叠新层并不会真正改善我们的结果（你可以尝试自己看看！）。为了解决这个问题，ResNets引入了*跳跃连接*的概念。我们将在下一节中探讨ResNets的这些方面。
- en: 'Building a Modern CNN: ResNet'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建现代CNN：ResNet
- en: 'We now have all the pieces we need to build the models we have been using in
    our computer vision tasks since the beginning of this book: ResNets. We’ll introduce
    the main idea behind them and show how it improves accuracy on Imagenette compared
    to our previous model, before building a version with all the recent tweaks.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经拥有构建我们自从本书开始就一直在计算机视觉任务中使用的模型所需的所有要素：ResNets。我们将介绍它们背后的主要思想，并展示它如何在Imagenette上提高了准确性，然后构建一个带有所有最新调整的版本。
- en: Skip Connections
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跳跃连接
- en: 'In 2015, the authors of the ResNet paper noticed something that they found
    curious. Even after using batchnorm, they saw that a network using more layers
    was doing less well than a network using fewer layers—and there were no other
    differences between the models. Most interestingly, the difference was observed
    not only in the validation set, but also in the training set; so it wasn’t just
    a generalization issue, but a training issue. As the paper explains:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，ResNet论文的作者们注意到了一件他们觉得奇怪的事情。即使使用了批量归一化，他们发现使用更多层的网络表现不如使用更少层的网络，并且模型之间没有其他差异。最有趣的是，这种差异不仅在验证集中观察到，而且在训练集中也观察到；因此这不仅仅是一个泛化问题，而是一个训练问题。正如论文所解释的：
- en: Unexpectedly, such degradation is not caused by overfitting, and adding more
    layers to a suitably deep model leads to higher training error, as [previously
    reported] and thoroughly verified by our experiments.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 出乎意料的是，这种退化并不是由过拟合引起的，向适当深度的模型添加更多层会导致更高的训练错误，正如我们的实验[先前报告]和彻底验证的那样。
- en: This phenomenon was illustrated by the graph in [Figure 14-1](#resnet_depth),
    with training error on the left and test error on the right.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象在[图14-1](#resnet_depth)中的图表中有所说明，左侧是训练错误，右侧是测试错误。
- en: '![Training of networks of different depth](Images/dlcf_1401.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![不同深度网络的训练](Images/dlcf_1401.png)'
- en: Figure 14-1\. Training of networks of different depth (courtesy of Kaiming He
    et al.)
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-1。不同深度网络的训练（由Kaiming He等人提供）。
- en: 'As the authors mention here, they are not the first people to have noticed
    this curious fact. But they were the first to make a very important leap:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如作者在这里提到的，他们并不是第一个注意到这个奇怪事实的人。但他们是第一个迈出非常重要的一步：
- en: 'Let us consider a shallower architecture and its deeper counterpart that adds
    more layers onto it. There exists a solution by construction to the deeper model:
    the added layers are identity mapping, and the other layers are copied from the
    learned shallower model.'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们考虑一个更浅的架构及其更深的对应物，后者在其上添加更多层。存在一种通过构建解决更深模型的方法：添加的层是恒等映射，其他层是从学习的更浅模型中复制的。
- en: 'As this is an academic paper, this process is described in a rather inaccessible
    way, but the concept is actually very simple: start with a 20-layer neural network
    that is trained well, and add another 36 layers that do nothing at all (for instance,
    they could be linear layers with a single weight equal to 1, and bias equal to
    0). The result will be a 56-layer network that does exactly the same thing as
    the 20-layer network, proving that there are always deep networks that should
    be *at least as good* as any shallow network. But for some reason, SGD does not
    seem able to find them.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一篇学术论文，这个过程以一种不太易懂的方式描述，但概念实际上非常简单：从一个训练良好的20层神经网络开始，然后添加另外36层什么都不做的层（例如，它们可以是具有单个权重等于1和偏置等于0的线性层）。结果将是一个56层的网络，它与20层网络完全相同，证明总是存在深度网络应该*至少和*任何浅层网络一样好。但由于某种原因，随机梯度下降似乎无法找到它们。
- en: 'Jargon: Identity Mapping'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行话：恒等映射
- en: Returning the input without changing it at all. This process is performed by
    an *identity function*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入返回而不做任何改变。这个过程由一个*恒等函数*执行。
- en: Actually, there is another way to create those extra 36 layers, which is much
    more interesting. What if we replaced every occurrence of `conv(x)` with `x +
    conv(x)`, where `conv` is the function from the previous chapter that adds a second
    convolution, then a ReLU, then a batchnorm layer. Furthermore, recall that batchnorm
    does `gamma*y + beta`. What if we initialized `gamma` to zero for every one of
    those final batchnorm layers? Then our `conv(x)` for those extra 36 layers will
    always be equal to zero, which means `x+conv(x)` will always be equal to `x`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，还有另一种更有趣的方法来创建这些额外的36层。如果我们用`x + conv(x)`替换每次出现的`conv(x)`，其中`conv`是上一章中添加第二个卷积，然后是ReLU，然后是批量归一化层的函数。此外，回想一下批量归一化是`gamma*y
    + beta`。如果我们为这些最终批量归一化层中的每一个初始化`gamma`为零会怎样？那么我们这些额外的36层的`conv(x)`将始终等于零，这意味着`x+conv(x)`将始终等于`x`。
- en: What has that gained us? The key thing is that those 36 extra layers, as they
    stand, are an *identity mapping*, but they have *parameters*, which means they
    are *trainable*. So, we can start with our best 20-layer model, add these 36 extra
    layers that initially do nothing at all, and then *fine-tune the whole 56-layer
    model*. Those extra 36 layers can then learn the parameters that make them most
    useful!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了什么好处？关键是，这36个额外的层，就目前而言，是一个*恒等映射*，但它们有*参数*，这意味着它们是*可训练的*。因此，我们可以从最好的20层模型开始，添加这36个最初什么都不做的额外层，然后*微调整个56层模型*。这些额外的36层可以学习使它们最有用的参数！
- en: The ResNet paper proposed a variant of this, which is to instead “skip over”
    every second convolution, so effectively we get `x+conv2(conv1(x))`. This is shown
    by the diagram in [Figure 14-2](#resnet_block) (from the paper).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet论文提出了这样的一个变体，即“跳过”每第二个卷积，因此我们实际上得到了`x+conv2(conv1(x))`。这在[图14-2](#resnet_block)（来自论文）中的图表中显示。
- en: '![A simple ResNet block](Images/dlcf_1402.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![一个简单的ResNet块](Images/dlcf_1402.png)'
- en: Figure 14-2\. A simple ResNet block (courtesy of Kaiming He et al.)
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-2。一个简单的ResNet块（由Kaiming He等人提供）。
- en: That arrow on the right is just the `x` part of `x+conv2(conv1(x))` and is known
    as the *identity branch*, or *skip connection*. The path on the left is the `conv2(conv1(x))`
    part. You can think of the identity path as providing a direct route from the
    input to the output.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的箭头只是`x+conv2(conv1(x))`中的`x`部分，被称为*恒等分支*或*跳跃连接*。左侧路径是`conv2(conv1(x))`部分。您可以将恒等路径视为提供从输入到输出的直接路径。
- en: In a ResNet, we don’t proceed by first training a smaller number of layers,
    and then adding new layers on the end and fine-tuning. Instead, we use ResNet
    blocks like the one in [Figure 14-2](#resnet_block) throughout the CNN, initialized
    from scratch in the usual way and trained with SGD in the usual way. We rely on
    the skip connections to make the network easier to train with SGD.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在ResNet中，我们不是先训练少量层，然后在末尾添加新层并进行微调。相反，我们在整个CNN中使用像[图14-2](#resnet_block)中的ResNet块这样的块，以通常的方式从头开始初始化并以通常的方式使用SGD进行训练。我们依靠跳跃连接使网络更容易使用SGD进行训练。
- en: 'There’s another (largely equivalent) way to think of these ResNet blocks. This
    is how the paper describes it:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种（在很大程度上等效的）思考这些ResNet块的方式。这就是论文描述的方式：
- en: Instead of hoping each few stacked layers directly fit a desired underlying
    mapping, we explicitly let these layers fit a residual mapping. Formally, denoting
    the desired underlying mapping as *H*(*x*), we let the stacked nonlinear layers
    fit another mapping of *F*(*x*) := H(*x*)*−x*. The original mapping is recast
    into *F*(*x*)+*x*. We hypothesize that it is easier to optimize the residual mapping
    than to optimize the original, unreferenced mapping. To the extreme, if an identity
    mapping were optimal, it would be easier to push the residual to zero than to
    fit an identity mapping by a stack of nonlinear layers.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们不是希望每几个堆叠的层直接适应所需的底层映射，而是明确让这些层适应一个残差映射。形式上，将所需的底层映射表示为*H*(*x*)，我们让堆叠的非线性层适应另一个映射*F*(*x*)
    := H(*x*)*−x*。原始映射被重新构造为*F*(*x*)+*x*。我们假设优化残差映射比优化原始未引用的映射更容易。在极端情况下，如果恒等映射是最佳的，将残差推向零将比通过一堆非线性层适应恒等映射更容易。
- en: 'Again, this is rather inaccessible prose—so let’s try to restate it in plain
    English! If the outcome of a given layer is `x` and we’re using a ResNet block
    that returns `y = x + block(x)`, we’re not asking the block to predict `y`; we
    are asking it to predict the difference between `y` and `x`. So the job of those
    blocks isn’t to predict certain features, but to minimize the error between `x`
    and the desired `y`. A ResNet is, therefore, good at learning about slight differences
    between doing nothing and passing through a block of two convolutional layers
    (with trainable weights). This is how these models got their name: they’re predicting
    residuals (reminder: “residual” is prediction minus target).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这是相当晦涩的文字，让我们尝试用简单的英语重新表述一下！如果给定层的结果是`x`，我们使用一个返回`y = x + block(x)`的ResNet块，我们不是要求该块预测`y`；我们要求它预测`y`和`x`之间的差异。因此，这些块的任务不是预测特定的特征，而是最小化`x`和期望的`y`之间的误差。因此，ResNet擅长学习不做任何事情和通过两个卷积层块（具有可训练权重）之间的区别。这就是这些模型得名的原因：它们在预测残差（提醒：“残差”是预测减去目标）。
- en: 'One key concept that both of these two ways of thinking about ResNets share
    is the idea of ease of learning. This is an important theme. Recall the universal
    approximation theorem, which states that a sufficiently large network can learn
    anything. This is still true, but there turns out to be a very important difference
    between what a network *can learn* in principle, and what it is *easy for it to
    learn* with realistic data and training regimes. Many of the advances in neural
    networks over the last decade have been like the ResNet block: the result of realizing
    how to make something that was always possible actually feasible.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种关于ResNet的思考方式共享的一个关键概念是学习的便利性。这是一个重要的主题。回想一下普遍逼近定理，它指出一个足够大的网络可以学习任何东西。这仍然是真的，但事实证明，在原始数据和训练方案下，网络在原则上可以学习的东西与它实际上容易学习的东西之间存在非常重要的区别。过去十年中神经网络的许多进步都像ResNet块一样：意识到如何使一些一直可能的东西变得可行。
- en: True Identity Path
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真实身份路径
- en: The original paper didn’t actually do the trick of using zero for the initial
    value of `gamma` in the last batchnorm layer of each block; that came a couple
    of years later. So, the original version of ResNet didn’t quite begin training
    with a true identity path through the ResNet blocks, but nonetheless having the
    ability to “navigate through” the skip connections did make it train better. Adding
    the batchnorm `gamma` init trick made the models train at even higher learning
    rates.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文实际上并没有在每个块的最后一个batchnorm层中使用零作为`gamma`的初始值的技巧；这是几年后才出现的。因此，ResNet的原始版本并没有真正以真实的身份路径开始训练ResNet块，但是尽管如此，具有“穿越”跳过连接的能力确实使其训练效果更好。添加batchnorm
    `gamma`初始化技巧使模型能够以更高的学习速率训练。
- en: 'Here’s the definition of a simple ResNet block (fastai initializes the `gamma`
    weights of the last batchnorm layer to zero because of `norm_type=NormType.BatchZero`):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单ResNet块的定义（fastai将最后一个batchnorm层的`gamma`权重初始化为零，因为`norm_type=NormType.BatchZero`）：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This has two problems, however: it can’t handle a stride other than 1, and
    it requires that `ni==nf`. Stop for a moment to think carefully about why this
    is.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这有两个问题：它无法处理除1以外的步幅，并且要求`ni==nf`。停下来仔细思考为什么会这样。
- en: 'The issue is that with a stride of, say, 2 on one of the convolutions, the
    grid size of the output activations will be half the size on each axis of the
    input. So then we can’t add that back to `x` in `forward` because `x` and the
    output activations have different dimensions. The same basic issue occurs if `ni!=nf`:
    the shapes of the input and output connections won’t allow us to add them together.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，如果在其中一个卷积层上使用步幅为2，输出激活的网格大小将是输入的每个轴的一半。因此，我们无法将其添加回`forward`中的`x`，因为`x`和输出激活具有不同的维度。如果`ni!=nf`，则会出现相同的基本问题：输入和输出连接的形状不允许我们将它们相加。
- en: 'To fix this, we need a way to change the shape of `x` to match the result of
    `self.convs`. Halving the grid size can be done using an average pooling layer
    with a stride of 2: that is, a layer that takes 2×2 patches from the input and
    replaces them with their average.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要一种方法来改变`x`的形状，使其与`self.convs`的结果匹配。可以通过使用步幅为2的平均池化层来减半网格大小：也就是说，该层从输入中获取2×2的块，并用它们的平均值替换它们。
- en: Changing the number of channels can be done by using a convolution. We want
    this skip connection to be as close to an identity map as possible, however, which
    means making this convolution as simple as possible. The simplest possible convolution
    is one with a kernel size of 1\. That means that the kernel is size `ni` × `nf`
    × `1` × `1`, so it’s only doing a dot product over the channels of each input
    pixel—it’s not combining across pixels at all. This kind of *1x1 convolution*
    is widely used in modern CNNs, so take a moment to think about how it works.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用卷积来改变通道数。然而，我们希望这个跳过连接尽可能接近一个恒等映射，这意味着使这个卷积尽可能简单。最简单的卷积是一个卷积核大小为1的卷积。这意味着卷积核大小为`ni`
    × `nf` × `1` × `1`，因此它只是对每个输入像素的通道进行点积运算，根本不跨像素进行组合。这种*1x1卷积*在现代CNN中被广泛使用，因此花一点时间思考它是如何工作的。
- en: 'Jargon: 1x1 Convolution'
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：1x1卷积
- en: A convolution with a kernel size of 1.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核大小为1的卷积。
- en: 'Here’s a ResBlock using these tricks to handle changing shape in the skip connection:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用这些技巧处理跳过连接中形状变化的ResBlock：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that we’re using the `noop` function here, which simply returns its input
    unchanged (*noop* is a computer science term that stands for “no operation”).
    In this case, `idconv` does nothing at all if `nf==nf`, and `pool` does nothing
    if `stride==1`, which is what we wanted in our skip connection.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里使用`noop`函数，它只是返回其未更改的输入（*noop*是一个计算机科学术语，代表“无操作”）。在这种情况下，如果`nf==nf`，`idconv`什么也不做，如果`stride==1`，`pool`也不做任何操作，这正是我们在跳过连接中想要的。
- en: Also, you’ll see that we’ve removed the ReLU (`act_cls=None`) from the final
    convolution in `convs` and from `idconv`, and moved it to *after* we add the skip
    connection. The thinking behind this is that the whole ResNet block is like a
    layer, and you want your activation to be after your layer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您会看到我们已经从`convs`的最后一个卷积层和`idconv`中删除了ReLU（`act_cls=None`），并将其移到*在*我们添加跳跃连接之后。这样做的想法是整个ResNet块就像一个层，您希望激活在层之后。
- en: 'Let’s replace our `block` with `ResBlock` and try it out:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用`ResBlock`替换我们的`block`并尝试一下：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1.973174 | 1.845491 | 0.373248 | 00:08 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.973174 | 1.845491 | 0.373248 | 00:08 |'
- en: '| 1 | 1.678627 | 1.778713 | 0.439236 | 00:08 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.678627 | 1.778713 | 0.439236 | 00:08 |'
- en: '| 2 | 1.386163 | 1.596503 | 0.507261 | 00:08 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.386163 | 1.596503 | 0.507261 | 00:08 |'
- en: '| 3 | 1.177839 | 1.102993 | 0.644841 | 00:09 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.177839 | 1.102993 | 0.644841 | 00:09 |'
- en: '| 4 | 1.052435 | 1.038013 | 0.667771 | 00:09 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.052435 | 1.038013 | 0.667771 | 00:09 |'
- en: 'It’s not much better. But the whole point of this was to allow us to train
    *deeper* models, and we’re not really taking advantage of that yet. To create
    a model that’s, say, twice as deep, all we need to do is replace our `block` with
    two `ResBlock`s in a row:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这并没有好多少。但这一切的目的是让我们能够训练*更深*的模型，而我们实际上还没有充分利用这一点。要创建一个比如说深两倍的模型，我们只需要用两个`ResBlock`替换我们的`block`：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1.964076 | 1.864578 | 0.355159 | 00:12 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.964076 | 1.864578 | 0.355159 | 00:12 |'
- en: '| 1 | 1.636880 | 1.596789 | 0.502675 | 00:12 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.636880 | 1.596789 | 0.502675 | 00:12 |'
- en: '| 2 | 1.335378 | 1.304472 | 0.588535 | 00:12 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.335378 | 1.304472 | 0.588535 | 00:12 |'
- en: '| 3 | 1.089160 | 1.065063 | 0.663185 | 00:12 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.089160 | 1.065063 | 0.663185 | 00:12 |'
- en: '| 4 | 0.942904 | 0.963589 | 0.692739 | 00:12 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.942904 | 0.963589 | 0.692739 | 00:12 |'
- en: Now we’re making good progress!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们取得了良好的进展！
- en: 'The authors of the ResNet paper went on to win the 2015 ImageNet challenge.
    At the time, this was by far the most important annual event in computer vision.
    We have already seen another ImageNet winner: the 2013 winners, Zeiler and Fergus.
    It is interesting to note that in both cases, the starting points for the breakthroughs
    were experimental observations: observations about what layers actually learn,
    in the case of Zeiler and Fergus, and observations about which kinds of networks
    can be trained, in the case of the ResNet authors. This ability to design and
    analyze thoughtful experiments, or even just to see an unexpected result, say,
    “Hmmm, that’s interesting,” and then, most importantly, set about figuring out
    what on earth is going on, with great tenacity, is at the heart of many scientific
    discoveries. Deep learning is not like pure mathematics. It is a heavily experimental
    field, so it’s important to be a strong practitioner, not just a theoretician.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet论文的作者后来赢得了2015年ImageNet挑战赛。当时，这是计算机视觉领域迄今为止最重要的年度事件。我们已经看到另一个ImageNet的获奖者：2013年的获奖者Zeiler和Fergus。值得注意的是，在这两种情况下，突破的起点都是实验观察：Zeiler和Fergus案例中关于层实际学习内容的观察，以及ResNet作者案例中关于可以训练哪种网络的观察。设计和分析周到的实验，甚至只是看到一个意想不到的结果，然后，最重要的是，开始弄清楚到底发生了什么，具有极大的坚韧性，这是许多科学发现的核心。深度学习不像纯数学。这是一个非常实验性的领域，因此成为一个强大的实践者，而不仅仅是一个理论家，是非常重要的。
- en: Since the ResNet was introduced, it’s been widely studied and applied to many
    domains. One of the most interesting papers, published in 2018, is [“Visualizing
    the Loss Landscape of Neural Nets”](https://oreil.ly/C9cFi) by Hao Li et al. It
    shows that using skip connections helps smooth the loss function, which makes
    training easier as it avoids falling into a very sharp area. [Figure 14-3](#resnet_surface)
    shows a stunning picture from the paper, illustrating the difference between the
    bumpy terrain that SGD has to navigate to optimize a regular CNN (left) versus
    the smooth surface of a ResNet (right).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 自ResNet推出以来，它已经被广泛研究和应用于许多领域。其中最有趣的论文之一，发表于2018年，是由Hao Li等人撰写的[“可视化神经网络损失景观”](https://oreil.ly/C9cFi)。它表明使用跳跃连接有助于平滑损失函数，这使得训练更容易，因为它避免了陷入非常陡峭的区域。[图14-3](#resnet_surface)展示了该论文中的一幅惊人图片，说明了SGD需要导航以优化普通CNN（左侧）与ResNet（右侧）之间的不同之处。
- en: '![Impact of ResNet on loss landscape](Images/dlcf_1403.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![ResNet对损失景观的影响](Images/dlcf_1403.png)'
- en: Figure 14-3\. Impact of ResNet on loss landscape (courtesy of Hao Li et al.)
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-3\. ResNet对损失景观的影响（由Hao Li等人提供）
- en: Our first model is already good, but further research has discovered more tricks
    we can apply to make it better. We’ll look at those next.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个模型已经很好了，但进一步的研究发现了更多可以应用的技巧，使其变得更好。我们接下来将看看这些技巧。
- en: A State-of-the-Art ResNet
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个最先进的ResNet
- en: In [“Bag of Tricks for Image Classification with Convolutional Neural Networks”](https://oreil.ly/n-qhd),
    Tong He et al. study variations of the ResNet architecture that come at almost
    no additional cost in terms of number of parameters or computation. By using a
    tweaked ResNet-50 architecture and Mixup, they achieved 94.6% top-5 accuracy on
    ImageNet, in comparison to 92.2% with a regular ResNet-50 without Mixup. This
    result is better than that achieved by regular ResNet models that are twice as
    deep (and twice as slow, and much more likely to overfit).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“用卷积神经网络进行图像分类的技巧”](https://oreil.ly/n-qhd)中，Tong He等人研究了ResNet架构的变体，这几乎没有额外的参数或计算成本。通过使用调整后的ResNet-50架构和Mixup，他们在ImageNet上实现了94.6%的Top-5准确率，而普通的ResNet-50没有Mixup只有92.2%。这个结果比普通ResNet模型取得的结果更好，后者深度是它的两倍（速度也是两倍，更容易过拟合）。
- en: 'Jargon: Top-5 Accuracy'
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：Top-5准确率
- en: A metric testing how often the label we want is in the top 5 predictions of
    our model. It was used in the ImageNet competition because many of the images
    contained multiple objects, or contained objects that could be easily confused
    or may even have been mislabeled with a similar label. In these situations, looking
    at top-1 accuracy may be inappropriate. However, recently CNNs have been getting
    so good that top-5 accuracy is nearly 100%, so some researchers are using top-1
    accuracy for ImageNet too now.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个度量，测试我们模型的前5个预测中我们想要的标签有多少次。在ImageNet竞赛中使用它，因为许多图像包含多个对象，或者包含可以轻松混淆甚至可能被错误标记为相似标签的对象。在这些情况下，查看前1的准确率可能不合适。然而，最近CNN的表现越来越好，以至于前5的准确率几乎达到100%，因此一些研究人员现在也在ImageNet中使用前1的准确率。
- en: 'We’ll use this tweaked version as we scale up to the full ResNet, because it’s
    substantially better. It differs a little bit from our previous implementation,
    in that instead of just starting with ResNet blocks, it begins with a few convolutional
    layers followed by a max pooling layer. This is what the first layers, called
    the *stem* of the network, look like:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们扩展到完整的ResNet时，我们将使用这个调整过的版本，因为它要好得多。它与我们之前的实现略有不同，它不是直接从ResNet块开始，而是从几个卷积层开始，然后是一个最大池化层。这就是网络的第一层，称为*干*的样子：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Jargon: Stem'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：干
- en: The first few layers of a CNN. Generally, the stem has a different structure
    than the main body of the CNN.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的前几层。通常，干的结构与CNN的主体不同。
- en: 'The reason that we have a stem of plain convolutional layers, instead of ResNet
    blocks, is based on an important insight about all deep convolutional neural networks:
    the vast majority of the computation occurs in the early layers. Therefore, we
    should keep the early layers as fast and simple as possible.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以有一系列普通卷积层的起始，而不是ResNet块，是基于对所有深度卷积神经网络的一个重要洞察：绝大部分的计算发生在早期层。因此，我们应该尽可能保持早期层的速度和简单。
- en: To see why so much computation occurs in the early layers, consider the very
    first convolution on a 128-pixel input image. If it is a stride-1 convolution,
    it will apply the kernel to every one of the 128×128 pixels. That’s a lot of work!
    In the later layers, however, the grid size could be as small as 4×4 or even 2×2,
    so there are far fewer kernel applications to do.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解为什么绝大部分的计算发生在早期层，考虑一下在128像素输入图像上的第一个卷积。如果是步幅为1的卷积，它将应用核到128×128个像素中的每一个。这是很多工作！然而，在后续层中，网格大小可能只有4×4甚至2×2，因此要做的核应用要少得多。
- en: On the other hand, the first-layer convolution has only 3 input features and
    32 output features. Since it is a 3×3 kernel, this is 3×32×3×3 = 864 parameters
    in the weights. But the last convolution will have 256 input features and 512
    output features, resulting in 1,179,648 weights! So the first layers contain the
    vast majority of the computation, but the last layers contain the vast majority
    of the parameters.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，第一层卷积只有3个输入特征和32个输出特征。由于它是一个3×3的核，这是权重中的864个参数。但最后一个卷积将有256个输入特征和512个输出特征，导致1,179,648个权重！因此，第一层包含了绝大部分的计算量，而最后几层包含了绝大部分的参数。
- en: A ResNet block takes more computation than a plain convolutional block, since
    (in the stride-2 case) a ResNet block has three convolutions and a pooling layer.
    That’s why we want to have plain convolutions to start off our ResNet.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一个ResNet块比一个普通卷积块需要更多的计算，因为（在步幅为2的情况下）一个ResNet块有三个卷积和一个池化层。这就是为什么我们希望从普通卷积开始我们的ResNet。
- en: 'We’re now ready to show the implementation of a modern ResNet, with the “bag
    of tricks.” It uses the four groups of ResNet blocks, with 64, 128, 256, then
    512 filters. Each group starts with a stride-2 block, except for the first one,
    since it’s just after a `MaxPooling` layer:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备展示一个现代ResNet的实现，带有“技巧袋”。它使用了四组ResNet块，分别为64、128、256和512个滤波器。每组都以步幅为2的块开始，除了第一组，因为它紧接着一个`MaxPooling`层：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `_make_layer` function is just there to create a series of `n_layers` blocks.
    The first one is going from `ch_in` to `ch_out` with the indicated `stride`, and
    all the others are blocks of stride 1 with `ch_out` to `ch_out` tensors. Once
    the blocks are defined, our model is purely sequential, which is why we define
    it as a subclass of `nn.Sequential`. (Ignore the `expansion` parameter for now;
    we’ll discuss it in the next section. For now, it’ll be `1`, so it doesn’t do
    anything.)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`_make_layer`函数只是用来创建一系列`n_layers`块。第一个是从`ch_in`到`ch_out`，步幅为指定的`stride`，其余所有块都是步幅为1的块，从`ch_out`到`ch_out`张量。一旦块被定义，我们的模型就是纯顺序的，这就是为什么我们将其定义为`nn.Sequential`的子类。（暂时忽略`expansion`参数；我们将在下一节讨论它。暂时设为`1`，所以它不起作用。）'
- en: 'The various versions of the models (ResNet-18, -34, -50, etc.) just change
    the number of blocks in each of those groups. This is the definition of a ResNet-18:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的各个版本（ResNet-18、-34、-50等）只是改变了每个组中块的数量。这是ResNet-18的定义：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s train it for a little bit and see how it fares compared to the previous
    model:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练一下，看看它与之前的模型相比如何：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1.673882 | 1.828394 | 0.413758 | 00:13 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.673882 | 1.828394 | 0.413758 | 00:13 |'
- en: '| 1 | 1.331675 | 1.572685 | 0.518217 | 00:13 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.331675 | 1.572685 | 0.518217 | 00:13 |'
- en: '| 2 | 1.087224 | 1.086102 | 0.650701 | 00:13 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.087224 | 1.086102 | 0.650701 | 00:13 |'
- en: '| 3 | 0.900428 | 0.968219 | 0.684331 | 00:12 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.900428 | 0.968219 | 0.684331 | 00:12 |'
- en: '| 4 | 0.760280 | 0.782558 | 0.757197 | 00:12 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.760280 | 0.782558 | 0.757197 | 00:12 |'
- en: Even though we have more channels (and our model is therefore even more accurate),
    our training is just as fast as before thanks to our optimized stem.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们有更多的通道（因此我们的模型更准确），但由于我们优化了干，我们的训练速度与以前一样快。
- en: 'To make our model deeper without taking too much compute or memory, we can
    use another kind of layer introduced by the ResNet paper for ResNets with a depth
    of 50 or more: the bottleneck layer.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的模型更深，而不占用太多计算或内存，我们可以使用ResNet论文引入的另一种层：瓶颈层。
- en: Bottleneck Layers
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 瓶颈层
- en: 'Instead of stacking two convolutions with a kernel size of 3, bottleneck layers
    use three convolutions: two 1×1 (at the beginning and the end) and one 3×3, as
    shown on the right in [Figure 14-4](#resnet_compare).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 瓶颈层不是使用3个内核大小为3的卷积堆叠，而是使用三个卷积：两个1×1（在开头和结尾）和一个3×3，如右侧在[图14-4](#resnet_compare)中所示。
- en: '![Comparison of regular and bottleneck ResNet blocks](Images/dlcf_1404.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![常规和瓶颈ResNet块的比较](Images/dlcf_1404.png)'
- en: Figure 14-4\. Comparison of regular and bottleneck ResNet blocks (courtesy of
    Kaiming He et al.)
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-4\. 常规和瓶颈ResNet块的比较（由Kaiming He等人提供）
- en: 'Why is that useful? 1×1 convolutions are much faster, so even if this seems
    to be a more complex design, this block executes faster than the first ResNet
    block we saw. This then lets us use more filters: as we see in the illustration,
    the number of filters in and out is four times higher (256 instead of 64). The
    1×1 convs diminish then restore the number of channels (hence the name *bottleneck*).
    The overall impact is that we can use more filters in the same amount of time.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这很有用？1×1卷积速度更快，因此即使这似乎是一个更复杂的设计，这个块的执行速度比我们看到的第一个ResNet块更快。这样一来，我们可以使用更多的滤波器：正如我们在插图中看到的，输入和输出的滤波器数量是四倍更高的（256而不是64）。1×1卷积减少然后恢复通道数（因此称为*瓶颈*）。总体影响是我们可以在相同的时间内使用更多的滤波器。
- en: 'Let’s try replacing our `ResBlock` with this bottleneck design:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试用这种瓶颈设计替换我们的`ResBlock`：
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We’ll use this to create a ResNet-50 with group sizes of `(3,4,6,3)`. We now
    need to pass `4` into the `expansion` parameter of `ResNet`, since we need to
    start with four times fewer channels and we’ll end with four times more channels.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个来创建一个具有组大小`(3,4,6,3)`的ResNet-50。现在我们需要将`4`传递给`ResNet`的`expansion`参数，因为我们需要从四倍少的通道开始，最终将以四倍多的通道结束。
- en: 'Deeper networks like this don’t generally show improvements when training for
    only 5 epochs, so we’ll bump it up to 20 epochs this time to make the most of
    our bigger model. And to really get great results, let’s use bigger images too:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样更深的网络通常在仅训练5个时期时不会显示出改进，所以这次我们将将其增加到20个时期，以充分利用我们更大的模型。为了获得更好的结果，让我们也使用更大的图像：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We don’t have to do anything to account for the larger 224-pixel images; thanks
    to our fully convolutional network, it just works. This is also why we were able
    to do *progressive resizing* earlier in the book—the models we used were fully
    convolutional, so we were even able to fine-tune models trained with different
    sizes. We can now train our model and see the effects:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必为更大的224像素图像做任何调整；由于我们的全卷积网络，它可以正常工作。这也是为什么我们能够在本书的早期进行*渐进调整*的原因——我们使用的模型是全卷积的，所以我们甚至能够微调使用不同尺寸训练的模型。现在我们可以训练我们的模型并查看效果：
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1.613448 | 1.473355 | 0.514140 | 00:31 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.613448 | 1.473355 | 0.514140 | 00:31 |'
- en: '| 1 | 1.359604 | 2.050794 | 0.397452 | 00:31 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.359604 | 2.050794 | 0.397452 | 00:31 |'
- en: '| 2 | 1.253112 | 4.511735 | 0.387006 | 00:31 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.253112 | 4.511735 | 0.387006 | 00:31 |'
- en: '| 3 | 1.133450 | 2.575221 | 0.396178 | 00:31 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.133450 | 2.575221 | 0.396178 | 00:31 |'
- en: '| 4 | 1.054752 | 1.264525 | 0.613758 | 00:32 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.054752 | 1.264525 | 0.613758 | 00:32 |'
- en: '| 5 | 0.927930 | 2.670484 | 0.422675 | 00:32 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.927930 | 2.670484 | 0.422675 | 00:32 |'
- en: '| 6 | 0.838268 | 1.724588 | 0.528662 | 00:32 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.838268 | 1.724588 | 0.528662 | 00:32 |'
- en: '| 7 | 0.748289 | 1.180668 | 0.666497 | 00:31 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.748289 | 1.180668 | 0.666497 | 00:31 |'
- en: '| 8 | 0.688637 | 1.245039 | 0.650446 | 00:32 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.688637 | 1.245039 | 0.650446 | 00:32 |'
- en: '| 9 | 0.645530 | 1.053691 | 0.674904 | 00:31 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.645530 | 1.053691 | 0.674904 | 00:31 |'
- en: '| 10 | 0.593401 | 1.180786 | 0.676433 | 00:32 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.593401 | 1.180786 | 0.676433 | 00:32 |'
- en: '| 11 | 0.536634 | 0.879937 | 0.713885 | 00:32 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.536634 | 0.879937 | 0.713885 | 00:32 |'
- en: '| 12 | 0.479208 | 0.798356 | 0.741656 | 00:32 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 0.479208 | 0.798356 | 0.741656 | 00:32 |'
- en: '| 13 | 0.440071 | 0.600644 | 0.806879 | 00:32 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 0.440071 | 0.600644 | 0.806879 | 00:32 |'
- en: '| 14 | 0.402952 | 0.450296 | 0.858599 | 00:32 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 0.402952 | 0.450296 | 0.858599 | 00:32 |'
- en: '| 15 | 0.359117 | 0.486126 | 0.846369 | 00:32 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 0.359117 | 0.486126 | 0.846369 | 00:32 |'
- en: '| 16 | 0.313642 | 0.442215 | 0.861911 | 00:32 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 0.313642 | 0.442215 | 0.861911 | 00:32 |'
- en: '| 17 | 0.294050 | 0.485967 | 0.853503 | 00:32 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 17 | 0.294050 | 0.485967 | 0.853503 | 00:32 |'
- en: '| 18 | 0.270583 | 0.408566 | 0.875924 | 00:32 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 18 | 0.270583 | 0.408566 | 0.875924 | 00:32 |'
- en: '| 19 | 0.266003 | 0.411752 | 0.872611 | 00:33 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 19 | 0.266003 | 0.411752 | 0.872611 | 00:33 |'
- en: We’re getting a great result now! Try adding Mixup, and then training this for
    a hundred epochs while you go get lunch. You’ll have yourself a very accurate
    image classifier, trained from scratch.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了一个很好的结果！尝试添加Mixup，然后在吃午餐时将其训练一百个时期。你将拥有一个从头开始训练的非常准确的图像分类器。
- en: The bottleneck design we’ve shown here is typically used in only ResNet-50,
    -101, and -152 models. ResNet-18 and -34 models usually use the non-bottleneck
    design seen in the previous section. However, we’ve noticed that the bottleneck
    layer generally works better even for the shallower networks. This just goes to
    show that the little details in papers tend to stick around for years, even if
    they’re not quite the best design! Questioning assumptions and “stuff everyone
    knows” is always a good idea, because this is still a new field, and lots of details
    aren’t always done well.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的瓶颈设计通常仅用于ResNet-50、-101和-152模型。ResNet-18和-34模型通常使用前一节中看到的非瓶颈设计。然而，我们注意到瓶颈层通常即使对于较浅的网络也效果更好。这只是表明，论文中的细节往往会持续多年，即使它们并不是最佳设计！质疑假设和“每个人都知道的东西”总是一个好主意，因为这仍然是一个新领域，很多细节并不总是做得很好。
- en: Conclusion
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: You have now seen how the models we have been using for computer vision since
    the first chapter are built, using skip connections to allow deeper models to
    be trained. Even though there has been a lot of research into better architectures,
    they all use one version or another of this trick to make a direct path from the
    input to the end of the network. When using transfer learning, the ResNet is the
    pretrained model. In the next chapter, we will look at the final details of how
    the models we used were built from it.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 自第一章以来，我们一直在使用的计算机视觉模型是如何构建的，使用跳跃连接来训练更深的模型。尽管已经进行了大量研究以寻找更好的架构，但它们都使用这个技巧的某个版本来建立从输入到网络末端的直接路径。在使用迁移学习时，ResNet是预训练模型。在下一章中，我们将看一下我们使用的模型是如何从中构建的最终细节。
- en: Questionnaire
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问卷调查
- en: How did we get to a single vector of activations in the CNNs used for MNIST
    in previous chapters? Why isn’t that suitable for Imagenette?
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以前的章节中，我们如何将用于MNIST的CNN转换为单个激活向量？为什么这对Imagenette不适用？
- en: What do we do for Imagenette instead?
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在Imagenette上做了什么？
- en: What is adaptive pooling?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是自适应池化？
- en: What is average pooling?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是平均池化？
- en: Why do we need `Flatten` after an adaptive average pooling layer?
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在自适应平均池化层之后需要`Flatten`？
- en: What is a skip connection?
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是跳跃连接？
- en: Why do skip connections allow us to train deeper models?
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么跳跃连接使我们能够训练更深的模型？
- en: What does [Figure 14-1](#resnet_depth) show? How did that lead to the idea of
    skip connections?
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[图14-1](#resnet_depth)展示了什么？这是如何导致跳跃连接的想法的？'
- en: What is identity mapping?
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是恒等映射？
- en: What is the basic equation for a ResNet block (ignoring batchnorm and ReLU layers)?
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ResNet块的基本方程是什么（忽略批量归一化和ReLU层）？
- en: What do ResNets have to do with residuals?
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ResNet与残差有什么关系？
- en: How do we deal with the skip connection when there is a stride-2 convolution?
    How about when the number of filters changes?
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当存在步幅为2的卷积时，我们如何处理跳跃连接？当滤波器数量发生变化时呢？
- en: How can we express a 1×1 convolution in terms of a vector dot product?
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何用向量点积表示1×1卷积？
- en: Create a 1×1 convolution with `F.conv2d` or `nn.Conv2d` and apply it to an image.
    What happens to the shape of the image?
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`F.conv2d`或`nn.Conv2d`创建一个1×1卷积并将其应用于图像。图像的形状会发生什么变化？
- en: What does the `noop` function return?
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`noop`函数返回什么？'
- en: Explain what is shown in [Figure 14-3](#resnet_surface).
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释[图14-3](#resnet_surface)中显示的内容。
- en: When is top-5 accuracy a better metric than top-1 accuracy?
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 何时使用前5准确度比前1准确度更好？
- en: What is the “stem” of a CNN?
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNN的“起始”是什么？
- en: Why do we use plain convolutions in the CNN stem instead of ResNet blocks?
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在CNN的起始部分使用普通卷积而不是ResNet块？
- en: How does a bottleneck block differ from a plain ResNet block?
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 瓶颈块与普通ResNet块有何不同？
- en: Why is a bottleneck block faster?
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么瓶颈块更快？
- en: How do fully convolutional nets (and nets with adaptive pooling in general)
    allow for progressive resizing?
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完全卷积网络（以及具有自适应池化的网络）如何实现渐进式调整大小？
- en: Further Research
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究
- en: Try creating a fully convolutional net with adaptive average pooling for MNIST
    (note that you’ll need fewer stride-2 layers). How does it compare to a network
    without such a pooling layer?
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试为MNIST创建一个带有自适应平均池化的完全卷积网络（请注意，您将需要更少的步幅为2的层）。与没有这种池化层的网络相比如何？
- en: In [Chapter 17](ch17.xhtml#chapter_foundations), we introduce *Einstein summation
    notation*. Skip ahead to see how this works, and then write an implementation
    of the 1×1 convolution operation using `torch.einsum`. Compare it to the same
    operation using `torch.conv2d`.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[第17章](ch17.xhtml#chapter_foundations)中，我们介绍了*爱因斯坦求和符号*。快进去看看它是如何工作的，然后使用`torch.einsum`编写一个1×1卷积操作的实现。将其与使用`torch.conv2d`进行相同操作进行比较。
- en: Write a top-5 accuracy function using plain PyTorch or plain Python.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用纯PyTorch或纯Python编写一个前5准确度函数。
- en: Train a model on Imagenette for more epochs, with and without label smoothing.
    Take a look at the Imagenette leaderboards and see how close you can get to the
    best results shown. Read the linked pages describing the leading approaches.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Imagenette上训练一个模型更多的epochs，使用和不使用标签平滑。查看Imagenette排行榜，看看你能达到最佳结果有多接近。阅读描述领先方法的链接页面。
