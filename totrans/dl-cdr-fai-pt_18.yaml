- en: Chapter 14\. ResNets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will build on top of the CNNs introduced in the previous
    chapter and explain to you the ResNet (residual network) architecture. It was
    introduced in 2015 by Kaiming He et al. in the article [“Deep Residual Learning
    for Image Recognition”](https://oreil.ly/b68K8) and is by far the most used model
    architecture nowadays. More recent developments in image models almost always
    use the same trick of residual connections, and most of the time, they are just
    a tweak of the original ResNet.
  prefs: []
  type: TYPE_NORMAL
- en: We will first show you the basic ResNet as it was first designed and then explain
    the modern tweaks that make it more performant. But first, we will need a problem
    a little bit more difficult than the MNIST dataset, since we are already close
    to 100% accuracy with a regular CNN on it.
  prefs: []
  type: TYPE_NORMAL
- en: Going Back to Imagenette
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s going to be tough to judge any improvements we make to our models when
    we are already at an accuracy that is as high as we saw on MNIST in the previous
    chapter, so we will tackle a tougher image classification problem by going back
    to Imagenette. We’ll stick with small images to keep things reasonably fast.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s grab the data—we’ll use the already-resized 160 px version to make things
    faster still, and will random crop to 128 px:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_14in01.png)'
  prefs: []
  type: TYPE_IMG
- en: When we looked at MNIST, we were dealing with 28×28-pixel images. For Imagenette,
    we are going to be training with 128×128-pixel images. Later, we would like to
    be able to use larger images as well—at least as big as 224×224-pixels, the ImageNet
    standard. Do you recall how we managed to get a single vector of activations for
    each image out of the MNIST convolutional neural network?
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach we used was to ensure that there were enough stride-2 convolutions
    such that the final layer would have a grid size of 1. Then we just flattened
    out the unit axes that we ended up with, to get a vector for each image (so, a
    matrix of activations for a mini-batch). We could do the same thing for Imagenette,
    but that would cause two problems:'
  prefs: []
  type: TYPE_NORMAL
- en: We’d need lots of stride-2 layers to make our grid 1×1 at the end—perhaps more
    than we would otherwise choose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model would not work on images of any size other than the size we originally
    trained on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One approach to dealing with the first issue would be to flatten the final
    convolutional layer in a way that handles a grid size other than 1×1\. We could
    simply flatten a matrix into a vector as we have done before, by laying out each
    row after the previous row. In fact, this is the approach that convolutional neural
    networks up until 2013 nearly always took. The most famous example is the 2013
    ImageNet winner VGG, still sometimes used today. But there was another problem
    with this architecture: it not only did not work with images other than those
    of the same size used in the training set, but also required a lot of memory,
    because flattening out the convolutional layer resulted in many activations being
    fed into the final layers. Therefore, the weight matrices of the final layers
    were enormous.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This problem was solved through the creation of *fully convolutional networks*.
    The trick in fully convolutional networks is to take the average of activations
    across a convolutional grid. In other words, we can simply use this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you see, it is taking the mean over the x- and y-axes. This function will
    always convert a grid of activations into a single activation per image. PyTorch
    provides a slightly more versatile module called `nn.AdaptiveAvgPool2d`, which
    averages a grid of activations into whatever sized destination you require (although
    we nearly always use a size of 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'A fully convolutional network, therefore, has a number of convolutional layers,
    some of which will be stride 2, at the end of which is an adaptive average pooling
    layer, a flatten layer to remove the unit axes, and finally a linear layer. Here
    is our first fully convolutional network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We’re going to be replacing the implementation of `block` in the network with
    other variants in a moment, which is why we’re not calling it `conv` anymore.
    We’re also saving some time by taking advantage of fastai’s `ConvLayer`, which
    already provides the functionality of `conv` from the preceding chapter (plus
    a lot more!).
  prefs: []
  type: TYPE_NORMAL
- en: Stop and Think
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider this question: would this approach make sense for an optical character
    recognition (OCR) problem such as MNIST? The vast majority of practitioners tackling
    OCR and similar problems tend to use fully convolutional networks, because that’s
    what nearly everybody learns nowadays. But it really doesn’t make any sense! You
    can’t decide, for instance, whether a number is a 3 or an 8 by slicing it into
    small pieces, jumbling them up, and deciding whether on average each piece looks
    like a 3 or an 8\. But that’s what adaptive average pooling effectively does!
    Fully convolutional networks are really a good choice only for objects that don’t
    have a single correct orientation or size (e.g., like most natural photos).'
  prefs: []
  type: TYPE_NORMAL
- en: Once we are done with our convolutional layers, we will get activations of size
    `bs x ch x h x w` (batch size, a certain number of channels, height, and width).
    We want to convert this to a tensor of size `bs x ch`, so we take the average
    over the last two dimensions and flatten the trailing 1×1 dimension as we did
    in our previous model.
  prefs: []
  type: TYPE_NORMAL
- en: This is different from regular pooling in the sense that those layers will generally
    take the average (for average pooling) or the maximum (for max pooling) of a window
    of a given size. For instance, max pooling layers of size 2, which were very popular
    in older CNNs, reduce the size of our image by half on each dimension by taking
    the maximum of each 2×2 window (with a stride of 2).
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we can define a `Learner` with our custom model and then train it
    on the data we grabbed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_14in02.png)'
  prefs: []
  type: TYPE_IMG
- en: '3e-3 is often a good learning rate for CNNs, and that appears to be the case
    here too, so let’s try that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.901582 | 2.155090 | 0.325350 | 00:07 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.559855 | 1.586795 | 0.507771 | 00:07 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.296350 | 1.295499 | 0.571720 | 00:07 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.144139 | 1.139257 | 0.639236 | 00:07 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.049770 | 1.092619 | 0.659108 | 00:07 |'
  prefs: []
  type: TYPE_TB
- en: That’s a pretty good start, considering we have to pick the correct one of 10
    categories, and we’re training from scratch for just 5 epochs! We can do way better
    than this using a deeper model, but just stacking new layers won’t really improve
    our results (you can try and see for yourself!). To work around this problem,
    ResNets introduce the idea of *skip connections*. We’ll explore those and other
    aspects of ResNets in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building a Modern CNN: ResNet'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now have all the pieces we need to build the models we have been using in
    our computer vision tasks since the beginning of this book: ResNets. We’ll introduce
    the main idea behind them and show how it improves accuracy on Imagenette compared
    to our previous model, before building a version with all the recent tweaks.'
  prefs: []
  type: TYPE_NORMAL
- en: Skip Connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In 2015, the authors of the ResNet paper noticed something that they found
    curious. Even after using batchnorm, they saw that a network using more layers
    was doing less well than a network using fewer layers—and there were no other
    differences between the models. Most interestingly, the difference was observed
    not only in the validation set, but also in the training set; so it wasn’t just
    a generalization issue, but a training issue. As the paper explains:'
  prefs: []
  type: TYPE_NORMAL
- en: Unexpectedly, such degradation is not caused by overfitting, and adding more
    layers to a suitably deep model leads to higher training error, as [previously
    reported] and thoroughly verified by our experiments.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This phenomenon was illustrated by the graph in [Figure 14-1](#resnet_depth),
    with training error on the left and test error on the right.
  prefs: []
  type: TYPE_NORMAL
- en: '![Training of networks of different depth](Images/dlcf_1401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-1\. Training of networks of different depth (courtesy of Kaiming He
    et al.)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As the authors mention here, they are not the first people to have noticed
    this curious fact. But they were the first to make a very important leap:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider a shallower architecture and its deeper counterpart that adds
    more layers onto it. There exists a solution by construction to the deeper model:
    the added layers are identity mapping, and the other layers are copied from the
    learned shallower model.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'As this is an academic paper, this process is described in a rather inaccessible
    way, but the concept is actually very simple: start with a 20-layer neural network
    that is trained well, and add another 36 layers that do nothing at all (for instance,
    they could be linear layers with a single weight equal to 1, and bias equal to
    0). The result will be a 56-layer network that does exactly the same thing as
    the 20-layer network, proving that there are always deep networks that should
    be *at least as good* as any shallow network. But for some reason, SGD does not
    seem able to find them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Jargon: Identity Mapping'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Returning the input without changing it at all. This process is performed by
    an *identity function*.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, there is another way to create those extra 36 layers, which is much
    more interesting. What if we replaced every occurrence of `conv(x)` with `x +
    conv(x)`, where `conv` is the function from the previous chapter that adds a second
    convolution, then a ReLU, then a batchnorm layer. Furthermore, recall that batchnorm
    does `gamma*y + beta`. What if we initialized `gamma` to zero for every one of
    those final batchnorm layers? Then our `conv(x)` for those extra 36 layers will
    always be equal to zero, which means `x+conv(x)` will always be equal to `x`.
  prefs: []
  type: TYPE_NORMAL
- en: What has that gained us? The key thing is that those 36 extra layers, as they
    stand, are an *identity mapping*, but they have *parameters*, which means they
    are *trainable*. So, we can start with our best 20-layer model, add these 36 extra
    layers that initially do nothing at all, and then *fine-tune the whole 56-layer
    model*. Those extra 36 layers can then learn the parameters that make them most
    useful!
  prefs: []
  type: TYPE_NORMAL
- en: The ResNet paper proposed a variant of this, which is to instead “skip over”
    every second convolution, so effectively we get `x+conv2(conv1(x))`. This is shown
    by the diagram in [Figure 14-2](#resnet_block) (from the paper).
  prefs: []
  type: TYPE_NORMAL
- en: '![A simple ResNet block](Images/dlcf_1402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-2\. A simple ResNet block (courtesy of Kaiming He et al.)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: That arrow on the right is just the `x` part of `x+conv2(conv1(x))` and is known
    as the *identity branch*, or *skip connection*. The path on the left is the `conv2(conv1(x))`
    part. You can think of the identity path as providing a direct route from the
    input to the output.
  prefs: []
  type: TYPE_NORMAL
- en: In a ResNet, we don’t proceed by first training a smaller number of layers,
    and then adding new layers on the end and fine-tuning. Instead, we use ResNet
    blocks like the one in [Figure 14-2](#resnet_block) throughout the CNN, initialized
    from scratch in the usual way and trained with SGD in the usual way. We rely on
    the skip connections to make the network easier to train with SGD.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s another (largely equivalent) way to think of these ResNet blocks. This
    is how the paper describes it:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of hoping each few stacked layers directly fit a desired underlying
    mapping, we explicitly let these layers fit a residual mapping. Formally, denoting
    the desired underlying mapping as *H*(*x*), we let the stacked nonlinear layers
    fit another mapping of *F*(*x*) := H(*x*)*−x*. The original mapping is recast
    into *F*(*x*)+*x*. We hypothesize that it is easier to optimize the residual mapping
    than to optimize the original, unreferenced mapping. To the extreme, if an identity
    mapping were optimal, it would be easier to push the residual to zero than to
    fit an identity mapping by a stack of nonlinear layers.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Again, this is rather inaccessible prose—so let’s try to restate it in plain
    English! If the outcome of a given layer is `x` and we’re using a ResNet block
    that returns `y = x + block(x)`, we’re not asking the block to predict `y`; we
    are asking it to predict the difference between `y` and `x`. So the job of those
    blocks isn’t to predict certain features, but to minimize the error between `x`
    and the desired `y`. A ResNet is, therefore, good at learning about slight differences
    between doing nothing and passing through a block of two convolutional layers
    (with trainable weights). This is how these models got their name: they’re predicting
    residuals (reminder: “residual” is prediction minus target).'
  prefs: []
  type: TYPE_NORMAL
- en: 'One key concept that both of these two ways of thinking about ResNets share
    is the idea of ease of learning. This is an important theme. Recall the universal
    approximation theorem, which states that a sufficiently large network can learn
    anything. This is still true, but there turns out to be a very important difference
    between what a network *can learn* in principle, and what it is *easy for it to
    learn* with realistic data and training regimes. Many of the advances in neural
    networks over the last decade have been like the ResNet block: the result of realizing
    how to make something that was always possible actually feasible.'
  prefs: []
  type: TYPE_NORMAL
- en: True Identity Path
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The original paper didn’t actually do the trick of using zero for the initial
    value of `gamma` in the last batchnorm layer of each block; that came a couple
    of years later. So, the original version of ResNet didn’t quite begin training
    with a true identity path through the ResNet blocks, but nonetheless having the
    ability to “navigate through” the skip connections did make it train better. Adding
    the batchnorm `gamma` init trick made the models train at even higher learning
    rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the definition of a simple ResNet block (fastai initializes the `gamma`
    weights of the last batchnorm layer to zero because of `norm_type=NormType.BatchZero`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This has two problems, however: it can’t handle a stride other than 1, and
    it requires that `ni==nf`. Stop for a moment to think carefully about why this
    is.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The issue is that with a stride of, say, 2 on one of the convolutions, the
    grid size of the output activations will be half the size on each axis of the
    input. So then we can’t add that back to `x` in `forward` because `x` and the
    output activations have different dimensions. The same basic issue occurs if `ni!=nf`:
    the shapes of the input and output connections won’t allow us to add them together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To fix this, we need a way to change the shape of `x` to match the result of
    `self.convs`. Halving the grid size can be done using an average pooling layer
    with a stride of 2: that is, a layer that takes 2×2 patches from the input and
    replaces them with their average.'
  prefs: []
  type: TYPE_NORMAL
- en: Changing the number of channels can be done by using a convolution. We want
    this skip connection to be as close to an identity map as possible, however, which
    means making this convolution as simple as possible. The simplest possible convolution
    is one with a kernel size of 1\. That means that the kernel is size `ni` × `nf`
    × `1` × `1`, so it’s only doing a dot product over the channels of each input
    pixel—it’s not combining across pixels at all. This kind of *1x1 convolution*
    is widely used in modern CNNs, so take a moment to think about how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jargon: 1x1 Convolution'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A convolution with a kernel size of 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a ResBlock using these tricks to handle changing shape in the skip connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that we’re using the `noop` function here, which simply returns its input
    unchanged (*noop* is a computer science term that stands for “no operation”).
    In this case, `idconv` does nothing at all if `nf==nf`, and `pool` does nothing
    if `stride==1`, which is what we wanted in our skip connection.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you’ll see that we’ve removed the ReLU (`act_cls=None`) from the final
    convolution in `convs` and from `idconv`, and moved it to *after* we add the skip
    connection. The thinking behind this is that the whole ResNet block is like a
    layer, and you want your activation to be after your layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s replace our `block` with `ResBlock` and try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.973174 | 1.845491 | 0.373248 | 00:08 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.678627 | 1.778713 | 0.439236 | 00:08 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.386163 | 1.596503 | 0.507261 | 00:08 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.177839 | 1.102993 | 0.644841 | 00:09 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.052435 | 1.038013 | 0.667771 | 00:09 |'
  prefs: []
  type: TYPE_TB
- en: 'It’s not much better. But the whole point of this was to allow us to train
    *deeper* models, and we’re not really taking advantage of that yet. To create
    a model that’s, say, twice as deep, all we need to do is replace our `block` with
    two `ResBlock`s in a row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.964076 | 1.864578 | 0.355159 | 00:12 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.636880 | 1.596789 | 0.502675 | 00:12 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.335378 | 1.304472 | 0.588535 | 00:12 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.089160 | 1.065063 | 0.663185 | 00:12 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.942904 | 0.963589 | 0.692739 | 00:12 |'
  prefs: []
  type: TYPE_TB
- en: Now we’re making good progress!
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of the ResNet paper went on to win the 2015 ImageNet challenge.
    At the time, this was by far the most important annual event in computer vision.
    We have already seen another ImageNet winner: the 2013 winners, Zeiler and Fergus.
    It is interesting to note that in both cases, the starting points for the breakthroughs
    were experimental observations: observations about what layers actually learn,
    in the case of Zeiler and Fergus, and observations about which kinds of networks
    can be trained, in the case of the ResNet authors. This ability to design and
    analyze thoughtful experiments, or even just to see an unexpected result, say,
    “Hmmm, that’s interesting,” and then, most importantly, set about figuring out
    what on earth is going on, with great tenacity, is at the heart of many scientific
    discoveries. Deep learning is not like pure mathematics. It is a heavily experimental
    field, so it’s important to be a strong practitioner, not just a theoretician.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the ResNet was introduced, it’s been widely studied and applied to many
    domains. One of the most interesting papers, published in 2018, is [“Visualizing
    the Loss Landscape of Neural Nets”](https://oreil.ly/C9cFi) by Hao Li et al. It
    shows that using skip connections helps smooth the loss function, which makes
    training easier as it avoids falling into a very sharp area. [Figure 14-3](#resnet_surface)
    shows a stunning picture from the paper, illustrating the difference between the
    bumpy terrain that SGD has to navigate to optimize a regular CNN (left) versus
    the smooth surface of a ResNet (right).
  prefs: []
  type: TYPE_NORMAL
- en: '![Impact of ResNet on loss landscape](Images/dlcf_1403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-3\. Impact of ResNet on loss landscape (courtesy of Hao Li et al.)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our first model is already good, but further research has discovered more tricks
    we can apply to make it better. We’ll look at those next.
  prefs: []
  type: TYPE_NORMAL
- en: A State-of-the-Art ResNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [“Bag of Tricks for Image Classification with Convolutional Neural Networks”](https://oreil.ly/n-qhd),
    Tong He et al. study variations of the ResNet architecture that come at almost
    no additional cost in terms of number of parameters or computation. By using a
    tweaked ResNet-50 architecture and Mixup, they achieved 94.6% top-5 accuracy on
    ImageNet, in comparison to 92.2% with a regular ResNet-50 without Mixup. This
    result is better than that achieved by regular ResNet models that are twice as
    deep (and twice as slow, and much more likely to overfit).
  prefs: []
  type: TYPE_NORMAL
- en: 'Jargon: Top-5 Accuracy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A metric testing how often the label we want is in the top 5 predictions of
    our model. It was used in the ImageNet competition because many of the images
    contained multiple objects, or contained objects that could be easily confused
    or may even have been mislabeled with a similar label. In these situations, looking
    at top-1 accuracy may be inappropriate. However, recently CNNs have been getting
    so good that top-5 accuracy is nearly 100%, so some researchers are using top-1
    accuracy for ImageNet too now.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use this tweaked version as we scale up to the full ResNet, because it’s
    substantially better. It differs a little bit from our previous implementation,
    in that instead of just starting with ResNet blocks, it begins with a few convolutional
    layers followed by a max pooling layer. This is what the first layers, called
    the *stem* of the network, look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Jargon: Stem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first few layers of a CNN. Generally, the stem has a different structure
    than the main body of the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason that we have a stem of plain convolutional layers, instead of ResNet
    blocks, is based on an important insight about all deep convolutional neural networks:
    the vast majority of the computation occurs in the early layers. Therefore, we
    should keep the early layers as fast and simple as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: To see why so much computation occurs in the early layers, consider the very
    first convolution on a 128-pixel input image. If it is a stride-1 convolution,
    it will apply the kernel to every one of the 128×128 pixels. That’s a lot of work!
    In the later layers, however, the grid size could be as small as 4×4 or even 2×2,
    so there are far fewer kernel applications to do.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the first-layer convolution has only 3 input features and
    32 output features. Since it is a 3×3 kernel, this is 3×32×3×3 = 864 parameters
    in the weights. But the last convolution will have 256 input features and 512
    output features, resulting in 1,179,648 weights! So the first layers contain the
    vast majority of the computation, but the last layers contain the vast majority
    of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: A ResNet block takes more computation than a plain convolutional block, since
    (in the stride-2 case) a ResNet block has three convolutions and a pooling layer.
    That’s why we want to have plain convolutions to start off our ResNet.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re now ready to show the implementation of a modern ResNet, with the “bag
    of tricks.” It uses the four groups of ResNet blocks, with 64, 128, 256, then
    512 filters. Each group starts with a stride-2 block, except for the first one,
    since it’s just after a `MaxPooling` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `_make_layer` function is just there to create a series of `n_layers` blocks.
    The first one is going from `ch_in` to `ch_out` with the indicated `stride`, and
    all the others are blocks of stride 1 with `ch_out` to `ch_out` tensors. Once
    the blocks are defined, our model is purely sequential, which is why we define
    it as a subclass of `nn.Sequential`. (Ignore the `expansion` parameter for now;
    we’ll discuss it in the next section. For now, it’ll be `1`, so it doesn’t do
    anything.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The various versions of the models (ResNet-18, -34, -50, etc.) just change
    the number of blocks in each of those groups. This is the definition of a ResNet-18:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s train it for a little bit and see how it fares compared to the previous
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.673882 | 1.828394 | 0.413758 | 00:13 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.331675 | 1.572685 | 0.518217 | 00:13 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.087224 | 1.086102 | 0.650701 | 00:13 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.900428 | 0.968219 | 0.684331 | 00:12 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.760280 | 0.782558 | 0.757197 | 00:12 |'
  prefs: []
  type: TYPE_TB
- en: Even though we have more channels (and our model is therefore even more accurate),
    our training is just as fast as before thanks to our optimized stem.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make our model deeper without taking too much compute or memory, we can
    use another kind of layer introduced by the ResNet paper for ResNets with a depth
    of 50 or more: the bottleneck layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Bottleneck Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instead of stacking two convolutions with a kernel size of 3, bottleneck layers
    use three convolutions: two 1×1 (at the beginning and the end) and one 3×3, as
    shown on the right in [Figure 14-4](#resnet_compare).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparison of regular and bottleneck ResNet blocks](Images/dlcf_1404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-4\. Comparison of regular and bottleneck ResNet blocks (courtesy of
    Kaiming He et al.)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Why is that useful? 1×1 convolutions are much faster, so even if this seems
    to be a more complex design, this block executes faster than the first ResNet
    block we saw. This then lets us use more filters: as we see in the illustration,
    the number of filters in and out is four times higher (256 instead of 64). The
    1×1 convs diminish then restore the number of channels (hence the name *bottleneck*).
    The overall impact is that we can use more filters in the same amount of time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try replacing our `ResBlock` with this bottleneck design:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use this to create a ResNet-50 with group sizes of `(3,4,6,3)`. We now
    need to pass `4` into the `expansion` parameter of `ResNet`, since we need to
    start with four times fewer channels and we’ll end with four times more channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deeper networks like this don’t generally show improvements when training for
    only 5 epochs, so we’ll bump it up to 20 epochs this time to make the most of
    our bigger model. And to really get great results, let’s use bigger images too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We don’t have to do anything to account for the larger 224-pixel images; thanks
    to our fully convolutional network, it just works. This is also why we were able
    to do *progressive resizing* earlier in the book—the models we used were fully
    convolutional, so we were even able to fine-tune models trained with different
    sizes. We can now train our model and see the effects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.613448 | 1.473355 | 0.514140 | 00:31 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.359604 | 2.050794 | 0.397452 | 00:31 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.253112 | 4.511735 | 0.387006 | 00:31 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.133450 | 2.575221 | 0.396178 | 00:31 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.054752 | 1.264525 | 0.613758 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.927930 | 2.670484 | 0.422675 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.838268 | 1.724588 | 0.528662 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.748289 | 1.180668 | 0.666497 | 00:31 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.688637 | 1.245039 | 0.650446 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.645530 | 1.053691 | 0.674904 | 00:31 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.593401 | 1.180786 | 0.676433 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.536634 | 0.879937 | 0.713885 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 0.479208 | 0.798356 | 0.741656 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 0.440071 | 0.600644 | 0.806879 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 0.402952 | 0.450296 | 0.858599 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 0.359117 | 0.486126 | 0.846369 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 0.313642 | 0.442215 | 0.861911 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | 0.294050 | 0.485967 | 0.853503 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | 0.270583 | 0.408566 | 0.875924 | 00:32 |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | 0.266003 | 0.411752 | 0.872611 | 00:33 |'
  prefs: []
  type: TYPE_TB
- en: We’re getting a great result now! Try adding Mixup, and then training this for
    a hundred epochs while you go get lunch. You’ll have yourself a very accurate
    image classifier, trained from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: The bottleneck design we’ve shown here is typically used in only ResNet-50,
    -101, and -152 models. ResNet-18 and -34 models usually use the non-bottleneck
    design seen in the previous section. However, we’ve noticed that the bottleneck
    layer generally works better even for the shallower networks. This just goes to
    show that the little details in papers tend to stick around for years, even if
    they’re not quite the best design! Questioning assumptions and “stuff everyone
    knows” is always a good idea, because this is still a new field, and lots of details
    aren’t always done well.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have now seen how the models we have been using for computer vision since
    the first chapter are built, using skip connections to allow deeper models to
    be trained. Even though there has been a lot of research into better architectures,
    they all use one version or another of this trick to make a direct path from the
    input to the end of the network. When using transfer learning, the ResNet is the
    pretrained model. In the next chapter, we will look at the final details of how
    the models we used were built from it.
  prefs: []
  type: TYPE_NORMAL
- en: Questionnaire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How did we get to a single vector of activations in the CNNs used for MNIST
    in previous chapters? Why isn’t that suitable for Imagenette?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do we do for Imagenette instead?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is adaptive pooling?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is average pooling?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need `Flatten` after an adaptive average pooling layer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a skip connection?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do skip connections allow us to train deeper models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does [Figure 14-1](#resnet_depth) show? How did that lead to the idea of
    skip connections?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is identity mapping?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the basic equation for a ResNet block (ignoring batchnorm and ReLU layers)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do ResNets have to do with residuals?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we deal with the skip connection when there is a stride-2 convolution?
    How about when the number of filters changes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we express a 1×1 convolution in terms of a vector dot product?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a 1×1 convolution with `F.conv2d` or `nn.Conv2d` and apply it to an image.
    What happens to the shape of the image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does the `noop` function return?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain what is shown in [Figure 14-3](#resnet_surface).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When is top-5 accuracy a better metric than top-1 accuracy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the “stem” of a CNN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we use plain convolutions in the CNN stem instead of ResNet blocks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a bottleneck block differ from a plain ResNet block?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is a bottleneck block faster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do fully convolutional nets (and nets with adaptive pooling in general)
    allow for progressive resizing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Try creating a fully convolutional net with adaptive average pooling for MNIST
    (note that you’ll need fewer stride-2 layers). How does it compare to a network
    without such a pooling layer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In [Chapter 17](ch17.xhtml#chapter_foundations), we introduce *Einstein summation
    notation*. Skip ahead to see how this works, and then write an implementation
    of the 1×1 convolution operation using `torch.einsum`. Compare it to the same
    operation using `torch.conv2d`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a top-5 accuracy function using plain PyTorch or plain Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a model on Imagenette for more epochs, with and without label smoothing.
    Take a look at the Imagenette leaderboards and see how close you can get to the
    best results shown. Read the linked pages describing the leading approaches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
