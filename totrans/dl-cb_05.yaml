- en: Chapter 5\. Generating Text in the Style of an Example Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we’ll look at how we can use recurrent neural networks (RNNs)
    to generate text in the style of a body of text. This makes for fun demos. People
    have used this type of network to generate anything from names of babies to descriptions
    of colors. These demos are a good way to get comfortable with recurrent networks.
    RNNs have their practical uses too—later in the book we’ll use them to train a
    chatbot and build a recommender system for music based on harvested playlists,
    and RNNs have been used in production to track objects in video.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: The recurrent neural network is a type of neural network that is helpful when
    working with time or sequences. We’ll first look at Project Gutenberg as a source
    of free books and download the collected works of William Shakespeare using some
    simple code. Next, we’ll use an RNN to produce texts that seem Shakespearean (if
    you don’t pay too much attention) by training the network on downloaded text.
    We’ll then repeat the trick on Python code, and see how to vary the output. Finally,
    since Python code has a predictable structure, we can look at which neurons fire
    on which bits of code and visualize the workings of our RNN.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found in the following Python notebook:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 5.1 Acquiring the Text of Public Domain Books
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to download the full text of some public domain books to use to train
    your model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the Python API for Project Gutenberg.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Project Gutenberg contains the complete texts of over 50,000 books. There is
    a handy Python API available to browse and download these books. We can download
    any book if we know the ID:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can get a book’s ID either by browsing the website and extracting it from
    the book’s URL or by querying [*http://www.gutenberg.org/*](http://www.gutenberg.org/)
    by author or title. Before we can query, though, we need to populate the metainformation
    cache. This will create a local database of all books available. It takes a bit
    of time, but only needs to be done once:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now discover all works by Shakespeare:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Discussion
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Project Gutenberg is a volunteer project to digitize books. It focuses on making
    available the most important books in English that are out of copyright in the
    United States, though it also has books in other languages. It was started in
    1971, long before the invention of the World Wide Web by Michael Hart.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Any work published in the US before 1923 is in the public domain, so most books
    found in the Gutenberg collection are older than that. This means that the language
    can be somewhat dated, but for natural language processing the collection remains
    an unrivalled source of training data. Going through the Python API not only makes
    access easy but also respects the restrictions that the site puts up for automatic
    downloading of texts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Generating Shakespeare-Like Texts
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you generate text in a specific style?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a character-level RNN.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by acquiring Shakespeare’s collected works. We’ll drop the poems,
    so we’re left with a more consistent set of just the plays. The poems happen to
    be collected in the first entry:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We’re going to feed the text in character by character and we’ll one-hot encode
    each character—that is, every character will be encoded as a vector containing
    all 0s and one 1\. For this, we need to know which characters we’re going to encounter:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s create our model that will take a sequence of characters and predict
    a sequence of characters. We’ll feed the sequence into a number of LSTM layers
    that do the work. The `TimeDistributed` layer lets our model output a sequence
    again:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We are going to feed in random fragments from the plays to the network, so
    a generator seems appropriate. The generator will yield blocks of pairs of sequences,
    where the sequences of the pairs are just one character apart:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we’ll train the model. We’ll set `steps_per_epoch` such that each character
    should have a decent chance to be seen by the network:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After training we can generate some output. We pick a random fragment from
    the plays and let the model guess what the next character is. We then add the
    next character to the fragment and repeat until we’ve reached the required number
    of characters:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After 10 epochs we should see some text that reminds us of Shakespeare, but
    we need around 30 for it to start to look like it could fool a casual reader that
    is not paying too close attention:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It’s somewhat suspicious that both Cleopatra and the Second Lord have a son
    of Greece, but the present winter and the world being slain are appropriately
    *Game of Thrones*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe we saw how we can use RNNs to generate text in a certain style.
    The results are quite convincing, especially given the fact that the model predicts
    on a character-by-character level. Thanks to the LSTM architecture, the network
    is capable of learning relationships that span quite large sequences—not just
    words, but sentences, and even the basic structure of the layout of Shakespeare’s
    plays.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Even though the example shown here isn’t very practical, RNNs can be. Any time
    we want a network to learn a sequence of items, an RNN is probably a good choice.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Other toy apps people have built using this technique have generated baby names,
    names for paint colors, and even recipes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: More practical RNNs can be used to predict the next character a user is going
    to type for a smartphone keyboard app, or predict the next move in a chess game
    when trained on a set of openings. This type of network has also been used to
    predict sequences like weather patterns or even stock market prices.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent networks are quite fickle, though. Seemingly small changes to the
    network architecture can lead to a situation where they no longer converge because
    of the so-called *exploding gradient problem*. Sometimes during training, after
    making progress for a number of epochs, the network seems to collapse and starts
    forgetting what it learns. As always, it is best to start with something simple
    that works and add complexity step by step, while keeping track of what was changed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: For a slightly more in-depth discussion of RNNs, see [Chapter 1](ch01.html#tools_techniques).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Writing Code Using RNNs
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you generate Python code using a neural network?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train a recurrent neural network over the Python code that comes with the Python
    distribution that runs your scripts.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'We can in fact use pretty much the same model as in the previous recipe for
    this task. As is often the case with deep learning, the key thing is to get the
    data. Python ships with the source code of many modules. Since they are stored
    in the directory where the *random.py* module sits, we can collect them using:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We could then read in all these source files and concatenate them into one document
    and start generating new snippets, just as we did with the Shakespearean text
    in the previous recipe. This works reasonably well, but when generating snippets,
    it becomes clear that a good chunk of Python source code is actually English.
    English appears both in the form of comments and the contents of strings. We want
    our model to learn Python, not English!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Stripping out the comments is easy enough:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Removing the contents of strings is slightly more involved. Some strings contain
    useful patterns, rather than English. As a rough rule, we’re going to replace
    any bit of text that has more than six letters and at least one space with `"MSG"`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finding the occurrences of string literals can be done concisely with a regular
    expression. Regular expressions are rather slow though, and we’re running them
    over a sizeable amount of code. In this case it’s better to just scan the strings:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Even cleaned up this way, we end up with megabytes of pure Python code. We can
    now train the model as before, but on Python code rather than on plays. After
    30 epochs or so, we should have something workable and can generate code.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generating Python code is no different from writing a Shakespearean-style play—at
    least for a neural network. We’ve seen that cleaning up the input data is an important
    aspect of data processing for neural networks. In this case we made sure to remove
    most traces of English from the source code. This way the network can focus on
    learning Python and not be distracted by also having to allocate neurons to learning
    English.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: We could further regularize the input. For example, we could pipe all the source
    code first through a “pretty printer” so that it would all have the same layout
    and our network could focus on learning that, rather than the diversity found
    in the current code. One step further would be to tokenize the Python code using
    the built-in tokenizer, and then let the network learn this parsed version and
    use `untokenize` to generate the code.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Controlling the Temperature of the Output
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to control the variability of the generated code.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the predictions as a probability distribution, rather than picking the highest
    value.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In the Shakespeare example, we picked the character in the predictions that
    had the highest score. This approach results in the output that is the best liked
    by the model. The drawback is that we get the same output for every start. Since
    we picked a random start sequence from the actual Shakespearean texts that didn’t
    matter much. But if we want to generate Python functions, it would be nice to
    always start in the same way—let’s say with `/ndef`—and look at various solutions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The predictions of our network are the result of a softmax activation function
    and can therefore be seen as a probability distribution. So, rather than picking
    the maximum value, we can let `numpy.random.multinomial` give us an answer. `multinomial`
    runs *n* experiments and takes the probability of how likely the outcomes are.
    By running it with *n* = 1, we get what we want.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point we can introduce the notion of temperature in how we draw the
    outcomes. The idea is that the higher the temperature is, the more random the
    outcomes are, while lower temperatures are closer to the pure deterministic outcomes
    we saw earlier. We do this by scaling the logs of the predictions accordingly
    and then applying the softmax function again to get back to probabilities. Putting
    this all together we get:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We’re finally ready to have some fun. At `diversity=1.0` the following code
    is produced. Note how the model generated our `"MSG"` placeholder and, apart from
    confusing `val` and `value`, almost got us running code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Discussion
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the output of the softmax activation function as a probability distribution
    allows us to get a variety of results that correspond to what the model “intends.”
    An added bonus is that it allows us to introduce the notion of temperature, so
    we can control how “random” the output is. In [Chapter 13](ch13.html#autoencoders)
    we’ll look at how *variational* *autoencoders* use a similar technique to control
    the randomness of what is generated.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The generated Python code can certainly pass for the real thing if we don’t
    pay attention to the details. One way to improve the results further would be
    to call the `compile` function on the generated code and only keep code that compiles.
    That way we can make sure that it is at least syntactically correct. A slight
    variation of that approach would be to not start over on a syntax error, but just
    drop the line where the error occurs and everything that follows and try again.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Visualizing Recurrent Network Activations
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you gain insight into what a recurrent network is doing?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Extract the activations from the neurons while they process text. Since we’re
    going to visualize the neurons, it makes sense to reduce their number. This will
    degrade the performance of the model a bit, but makes things simpler:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 提取神经元处理文本时的激活。由于我们将要可视化神经元，将它们的数量减少是有意义的。这会稍微降低模型的性能，但使事情变得更简单：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This model is a bit simpler and gets us slightly less accurate results, but
    it is good enough for visualizations. Keras has a handy method called `function`
    that allows us to specify an input and an output layer and will then run whatever
    part of the network is needed to convert from one to the other. The following
    method provides the network with a bit of text (a sequence of characters) and
    gets the activations for a specific layer back:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型稍微简单一些，得到的结果略微不够准确，但足够用于可视化。Keras有一个方便的方法叫做`function`，它允许我们指定一个输入和一个输出层，然后运行网络中需要的部分来进行转换。以下方法为网络提供了一段文本（一系列字符），并获取特定层的激活值：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now the question is which neurons to look at. Even our simplified model has
    512 neurons. Activations in an LSTM are between –1 and 1, so a simple way to find
    interesting neurons is to just pick the highest value corresponding to each character.
    `np.argmax(act, axis=1)` will get us that. We can visualize those neurons using:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是要看哪些神经元。即使我们简化的模型有512个神经元。LSTM中的激活值在-1到1之间，因此找到有趣的神经元的一个简单方法就是选择与每个字符对应的最高值。`np.argmax(act,
    axis=1)`将帮助我们实现这一点。我们可以使用以下方式可视化这些神经元：
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will produce a small bitmap. After we enlarge the bitmap and plot the
    code on top, we get:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个小的位图。当我们放大位图并在顶部绘制代码时，我们得到：
- en: '![neuron activations in an RNN](assets/dlcb_05in01.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![RNN中的神经元激活](assets/dlcb_05in01.png)'
- en: This looks interesting. The top neuron seems to keep track of where new statements
    start. The one with the green bars keeps track of spaces, but only in as far as
    they are used for indentation. The last-but-one neuron seems to fire when there
    is an `=` sign, but not when there is a `==`, suggesting the network learned the
    difference between assignment and equality.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很有趣。顶部的神经元似乎跟踪新语句的开始位置。带有绿色条的神经元跟踪空格，但仅限于用于缩进。倒数第二个神经元似乎在有`=`符号时激活，但在有`==`时不激活，这表明网络学会了赋值和相等之间的区别。
- en: Discussion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Deep learning models can be very effective, but their results are notoriously
    hard to explain. We more or less understand the mechanics of the training and
    inference, but it is often difficult to explain a concrete result, other than
    pointing to the actual calculations. Visualizing activations is one way of making
    what the network learned a little clearer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型可能非常有效，但它们的结果很难解释。我们更多或少了解训练和推理的机制，但往往很难解释一个具体的结果，除了指向实际的计算。可视化激活是使网络学到的内容更清晰的一种方式。
- en: Looking at the neurons with the highest activation for each character quickly
    gets us a set of neurons that might be of interest. Alternatively, we could explicitly
    try to look for neurons that fire in specific circumstances, for example inside
    brackets.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 查看每个字符的最高激活的神经元很快地得到了一组可能感兴趣的神经元。或者，我们可以明确地尝试寻找在特定情况下激活的神经元，例如在括号内。
- en: Once we have a specific neuron that looks interesting, we can use the same coloring
    technique to highlight larger chunks of code.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有一个看起来有趣的特定神经元，我们可以使用相同的着色技术来突出显示更大的代码块。
