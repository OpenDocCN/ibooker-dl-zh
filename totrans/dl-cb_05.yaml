- en: Chapter 5\. Generating Text in the Style of an Example Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 生成类似示例文本风格的文本
- en: In this chapter we’ll look at how we can use recurrent neural networks (RNNs)
    to generate text in the style of a body of text. This makes for fun demos. People
    have used this type of network to generate anything from names of babies to descriptions
    of colors. These demos are a good way to get comfortable with recurrent networks.
    RNNs have their practical uses too—later in the book we’ll use them to train a
    chatbot and build a recommender system for music based on harvested playlists,
    and RNNs have been used in production to track objects in video.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看看如何使用递归神经网络（RNN）生成类似文本体的文本。这将产生有趣的演示。人们已经使用这种类型的网络生成从婴儿姓名到颜色描述等各种内容。这些演示是熟悉递归网络的好方法。RNN也有它们的实际用途——在本书的后面，我们将使用它们来训练聊天机器人，并基于收集的播放列表构建音乐推荐系统，RNN已经被用于生产中跟踪视频中的对象。
- en: The recurrent neural network is a type of neural network that is helpful when
    working with time or sequences. We’ll first look at Project Gutenberg as a source
    of free books and download the collected works of William Shakespeare using some
    simple code. Next, we’ll use an RNN to produce texts that seem Shakespearean (if
    you don’t pay too much attention) by training the network on downloaded text.
    We’ll then repeat the trick on Python code, and see how to vary the output. Finally,
    since Python code has a predictable structure, we can look at which neurons fire
    on which bits of code and visualize the workings of our RNN.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络是一种在处理时间或序列时有帮助的神经网络类型。我们将首先查看Project Gutenberg作为免费书籍的来源，并使用一些简单的代码下载威廉·莎士比亚的作品集。接下来，我们将使用RNN生成似乎是莎士比亚风格的文本（如果您不太注意的话），通过训练网络下载的文本。然后，我们将在Python代码上重复这一技巧，并看看如何改变输出。最后，由于Python代码具有可预测的结构，我们可以查看哪些神经元在哪些代码位上激活，并可视化我们的RNN的工作原理。
- en: 'The code for this chapter can be found in the following Python notebook:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下Python笔记本中找到：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 5.1 Acquiring the Text of Public Domain Books
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.1 获取公共领域书籍的文本
- en: Problem
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to download the full text of some public domain books to use to train
    your model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要下载一些公共领域书籍的完整文本，以用于训练您的模型。
- en: Solution
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the Python API for Project Gutenberg.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Project Gutenberg的Python API。
- en: 'Project Gutenberg contains the complete texts of over 50,000 books. There is
    a handy Python API available to browse and download these books. We can download
    any book if we know the ID:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Project Gutenberg包含超过5万本书的完整文本。有一个方便的Python API可用于浏览和下载这些书籍。如果我们知道ID，我们可以下载任何一本书：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can get a book’s ID either by browsing the website and extracting it from
    the book’s URL or by querying [*http://www.gutenberg.org/*](http://www.gutenberg.org/)
    by author or title. Before we can query, though, we need to populate the metainformation
    cache. This will create a local database of all books available. It takes a bit
    of time, but only needs to be done once:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过浏览网站并从书籍的URL中提取书籍的ID，或者通过作者或标题查询[*http://www.gutenberg.org/*](http://www.gutenberg.org/)
    来获取书籍的ID。但在查询之前，我们需要填充元信息缓存。这将创建一个包含所有可用书籍的本地数据库。这需要一点时间，但只需要做一次：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now discover all works by Shakespeare:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以发现莎士比亚的所有作品：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Discussion
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Project Gutenberg is a volunteer project to digitize books. It focuses on making
    available the most important books in English that are out of copyright in the
    United States, though it also has books in other languages. It was started in
    1971, long before the invention of the World Wide Web by Michael Hart.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Project Gutenberg是一个志愿者项目，旨在数字化图书。它专注于提供美国版权已过期的英语中最重要的书籍，尽管它也有其他语言的书籍。该项目始于1971年，早于迈克尔·哈特发明万维网之前。
- en: Any work published in the US before 1923 is in the public domain, so most books
    found in the Gutenberg collection are older than that. This means that the language
    can be somewhat dated, but for natural language processing the collection remains
    an unrivalled source of training data. Going through the Python API not only makes
    access easy but also respects the restrictions that the site puts up for automatic
    downloading of texts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在1923年之前在美国出版的任何作品都属于公共领域，因此古腾堡收藏中的大多数书籍都比这个年代更久远。这意味着语言可能有些过时，但对于自然语言处理来说，该收藏仍然是无与伦比的训练数据来源。通过Python
    API进行访问不仅使访问变得容易，而且还尊重该网站为自动下载文本设置的限制。
- en: 5.2 Generating Shakespeare-Like Texts
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.2 生成类似莎士比亚的文本
- en: Problem
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you generate text in a specific style?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如何以特定风格生成文本？
- en: Solution
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a character-level RNN.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用字符级RNN。
- en: 'Let’s start by acquiring Shakespeare’s collected works. We’ll drop the poems,
    so we’re left with a more consistent set of just the plays. The poems happen to
    be collected in the first entry:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先获取莎士比亚的作品集。我们将放弃诗歌，这样我们就只剩下戏剧的一组更一致的作品。诗歌恰好被收集在第一个条目中：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We’re going to feed the text in character by character and we’ll one-hot encode
    each character—that is, every character will be encoded as a vector containing
    all 0s and one 1\. For this, we need to know which characters we’re going to encounter:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐个字符地输入文本，并对每个字符进行独热编码——也就是说，每个字符将被编码为一个包含所有0和一个1的向量。为此，我们需要知道我们将遇到哪些字符：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s create our model that will take a sequence of characters and predict
    a sequence of characters. We’ll feed the sequence into a number of LSTM layers
    that do the work. The `TimeDistributed` layer lets our model output a sequence
    again:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个模型，该模型将接收一个字符序列并预测一个字符序列。我们将把序列输入到多个LSTM层中进行处理。`TimeDistributed`层让我们的模型再次输出一个序列：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We are going to feed in random fragments from the plays to the network, so
    a generator seems appropriate. The generator will yield blocks of pairs of sequences,
    where the sequences of the pairs are just one character apart:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向网络中随机提供剧本片段，因此生成器似乎是合适的。生成器将产生成对序列块，其中成对的序列仅相差一个字符：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we’ll train the model. We’ll set `steps_per_epoch` such that each character
    should have a decent chance to be seen by the network:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将训练模型。我们将设置`steps_per_epoch`，以便每个字符都有足够的机会被网络看到：
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After training we can generate some output. We pick a random fragment from
    the plays and let the model guess what the next character is. We then add the
    next character to the fragment and repeat until we’ve reached the required number
    of characters:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以生成一些输出。我们从剧本中随机选择一个片段，让模型猜测下一个字符是什么。然后我们将下一个字符添加到片段中，并重复，直到达到所需的字符数：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After 10 epochs we should see some text that reminds us of Shakespeare, but
    we need around 30 for it to start to look like it could fool a casual reader that
    is not paying too close attention:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 经过10个时期，我们应该看到一些让我们想起莎士比亚的文本，但我们需要大约30个时期才能开始看起来像是可以愚弄一个不太注意的读者：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It’s somewhat suspicious that both Cleopatra and the Second Lord have a son
    of Greece, but the present winter and the world being slain are appropriately
    *Game of Thrones*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 克利奥帕特拉和第二勋爵都有一个希腊的儿子，但目前的冬天和世界被杀死是合适的*权力的游戏*。
- en: Discussion
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In this recipe we saw how we can use RNNs to generate text in a certain style.
    The results are quite convincing, especially given the fact that the model predicts
    on a character-by-character level. Thanks to the LSTM architecture, the network
    is capable of learning relationships that span quite large sequences—not just
    words, but sentences, and even the basic structure of the layout of Shakespeare’s
    plays.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们看到了如何使用RNN生成特定风格的文本。结果相当令人信服，尤其是考虑到模型是基于字符级别进行预测的。由于LSTM架构，网络能够学习跨越相当大序列的关系——不仅仅是单词，还有句子，甚至是莎士比亚戏剧布局的基本结构。
- en: Even though the example shown here isn’t very practical, RNNs can be. Any time
    we want a network to learn a sequence of items, an RNN is probably a good choice.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这里展示的示例并不是非常实用，但RNN可能是实用的。每当我们想要让网络学习一系列项目时，RNN可能是一个不错的选择。
- en: Other toy apps people have built using this technique have generated baby names,
    names for paint colors, and even recipes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其他人使用这种技术构建的玩具应用程序生成了婴儿姓名、油漆颜色名称，甚至食谱。
- en: More practical RNNs can be used to predict the next character a user is going
    to type for a smartphone keyboard app, or predict the next move in a chess game
    when trained on a set of openings. This type of network has also been used to
    predict sequences like weather patterns or even stock market prices.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 更实用的RNN可以用于预测用户在智能手机键盘应用程序中要输入的下一个字符，或者在训练了一组开局后预测下一步棋。这种类型的网络还被用来预测诸如天气模式或甚至股市价格等序列。
- en: Recurrent networks are quite fickle, though. Seemingly small changes to the
    network architecture can lead to a situation where they no longer converge because
    of the so-called *exploding gradient problem*. Sometimes during training, after
    making progress for a number of epochs, the network seems to collapse and starts
    forgetting what it learns. As always, it is best to start with something simple
    that works and add complexity step by step, while keeping track of what was changed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 循环网络相当易变。看似对网络架构的微小更改可能会导致它们不再收敛，因为所谓的*梯度爆炸问题*。有时在训练过程中，经过多个时期取得进展后，网络似乎会崩溃并开始忘记它所学到的东西。一如既往，最好从一些简单的有效方法开始，逐步增加复杂性，同时跟踪所做的更改。
- en: For a slightly more in-depth discussion of RNNs, see [Chapter 1](ch01.html#tools_techniques).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有关RNN的稍微深入的讨论，请参阅[第1章](ch01.html#tools_techniques)。
- en: 5.3 Writing Code Using RNNs
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN编写代码
- en: Problem
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you generate Python code using a neural network?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用神经网络生成Python代码？
- en: Solution
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Train a recurrent neural network over the Python code that comes with the Python
    distribution that runs your scripts.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个循环神经网络，使用Python分发的Python代码运行您的脚本。
- en: 'We can in fact use pretty much the same model as in the previous recipe for
    this task. As is often the case with deep learning, the key thing is to get the
    data. Python ships with the source code of many modules. Since they are stored
    in the directory where the *random.py* module sits, we can collect them using:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以为这个任务使用与上一个配方中几乎相同的模型。就深度学习而言，关键是获取数据。Python附带许多模块的源代码。由于它们存储在*random.py*模块所在的目录中，我们可以使用以下方法收集它们：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We could then read in all these source files and concatenate them into one document
    and start generating new snippets, just as we did with the Shakespearean text
    in the previous recipe. This works reasonably well, but when generating snippets,
    it becomes clear that a good chunk of Python source code is actually English.
    English appears both in the form of comments and the contents of strings. We want
    our model to learn Python, not English!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以读取所有这些源文件，并将它们连接成一个文档，并开始生成新的片段，就像我们在上一个配方中对莎士比亚文本所做的那样。这种方法效果相当不错，但是在生成片段时，很明显很大一部分Python源代码实际上是英语。英语既出现在注释中，也出现在字符串的内容中。我们希望我们的模型学习Python，而不是英语！
- en: 'Stripping out the comments is easy enough:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 去除注释很容易：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Removing the contents of strings is slightly more involved. Some strings contain
    useful patterns, rather than English. As a rough rule, we’re going to replace
    any bit of text that has more than six letters and at least one space with `"MSG"`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 删除字符串的内容稍微复杂一些。一些字符串包含有用的模式，而不是英语。作为一个粗略的规则，我们将用`"MSG"`替换任何具有超过六个字母和至少一个空格的文本片段：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finding the occurrences of string literals can be done concisely with a regular
    expression. Regular expressions are rather slow though, and we’re running them
    over a sizeable amount of code. In this case it’s better to just scan the strings:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则表达式可以简洁地找到字符串文字的出现。但是正则表达式速度相当慢，而且我们正在对大量代码进行运行。在这种情况下，最好只扫描字符串：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Even cleaned up this way, we end up with megabytes of pure Python code. We can
    now train the model as before, but on Python code rather than on plays. After
    30 epochs or so, we should have something workable and can generate code.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 即使以这种方式清理，我们最终会得到几兆字节的纯Python代码。现在我们可以像以前一样训练模型，但是在Python代码而不是戏剧上。经过大约30个时代，我们应该有可行的东西，并且可以生成代码。
- en: Discussion
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Generating Python code is no different from writing a Shakespearean-style play—at
    least for a neural network. We’ve seen that cleaning up the input data is an important
    aspect of data processing for neural networks. In this case we made sure to remove
    most traces of English from the source code. This way the network can focus on
    learning Python and not be distracted by also having to allocate neurons to learning
    English.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 生成Python代码与编写莎士比亚风格的戏剧没有什么不同——至少对于神经网络来说是这样。我们已经看到，清理输入数据是神经网络数据处理的一个重要方面。在这种情况下，我们确保从源代码中删除了大部分英语的痕迹。这样，网络就可以专注于学习Python，而不会被学习英语分散注意力。
- en: We could further regularize the input. For example, we could pipe all the source
    code first through a “pretty printer” so that it would all have the same layout
    and our network could focus on learning that, rather than the diversity found
    in the current code. One step further would be to tokenize the Python code using
    the built-in tokenizer, and then let the network learn this parsed version and
    use `untokenize` to generate the code.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步规范输入。例如，我们可以首先将所有源代码通过“漂亮的打印机”传递，以便它们都具有相同的布局，这样我们的网络就可以专注于学习这一点，而不是当前代码中的多样性。更进一步的一步是使用内置的标记器对Python代码进行标记化，然后让网络学习这个解析版本，并使用`untokenize`生成代码。
- en: 5.4 Controlling the Temperature of the Output
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.4 控制输出的温度
- en: Problem
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to control the variability of the generated code.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 您想控制生成代码的变化性。
- en: Solution
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the predictions as a probability distribution, rather than picking the highest
    value.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测作为概率分布，而不是选择最高值。
- en: In the Shakespeare example, we picked the character in the predictions that
    had the highest score. This approach results in the output that is the best liked
    by the model. The drawback is that we get the same output for every start. Since
    we picked a random start sequence from the actual Shakespearean texts that didn’t
    matter much. But if we want to generate Python functions, it would be nice to
    always start in the same way—let’s say with `/ndef`—and look at various solutions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在莎士比亚的例子中，我们选择了预测中得分最高的字符。这种方法会导致模型最喜欢的输出。缺点是我们对每个开始都得到相同的输出。由于我们从实际的莎士比亚文本中选择了一个随机的开始序列，这并不重要。但是如果我们想要生成Python函数，最好总是以相同的方式开始——比如以`/ndef`开始，并查看各种解决方案。
- en: The predictions of our network are the result of a softmax activation function
    and can therefore be seen as a probability distribution. So, rather than picking
    the maximum value, we can let `numpy.random.multinomial` give us an answer. `multinomial`
    runs *n* experiments and takes the probability of how likely the outcomes are.
    By running it with *n* = 1, we get what we want.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络的预测是通过softmax激活函数得出的结果，因此可以看作是一个概率分布。因此，我们可以让`numpy.random.multinomial`给出答案，而不是选择最大值。`multinomial`运行*n*次实验，并考虑结果的可能性。通过将*n*
    = 1运行，我们得到了我们想要的结果。
- en: 'At this point we can introduce the notion of temperature in how we draw the
    outcomes. The idea is that the higher the temperature is, the more random the
    outcomes are, while lower temperatures are closer to the pure deterministic outcomes
    we saw earlier. We do this by scaling the logs of the predictions accordingly
    and then applying the softmax function again to get back to probabilities. Putting
    this all together we get:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以引入在如何绘制结果时引入温度的概念。这个想法是，温度越高，结果就越随机，而较低的温度则更接近我们之前看到的纯确定性结果。我们通过相应地缩放预测的对数，然后再次应用softmax函数来获得概率。将所有这些放在一起，我们得到：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We’re finally ready to have some fun. At `diversity=1.0` the following code
    is produced. Note how the model generated our `"MSG"` placeholder and, apart from
    confusing `val` and `value`, almost got us running code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好玩一些了。在`diversity=1.0`时，生成了以下代码。请注意模型生成了我们的“MSG”占位符，并且除了混淆了`val`和`value`之外，几乎让我们运行了代码：
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Discussion
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Using the output of the softmax activation function as a probability distribution
    allows us to get a variety of results that correspond to what the model “intends.”
    An added bonus is that it allows us to introduce the notion of temperature, so
    we can control how “random” the output is. In [Chapter 13](ch13.html#autoencoders)
    we’ll look at how *variational* *autoencoders* use a similar technique to control
    the randomness of what is generated.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将softmax激活函数的输出作为概率分布使我们能够获得与模型“意图”相对应的各种结果。一个额外的好处是，它使我们能够引入温度的概念，因此我们可以控制输出的“随机性”程度。在[第13章](ch13.html#autoencoders)中，我们将看到*变分*
    *自动编码器*如何使用类似的技术来控制生成的随机性。
- en: The generated Python code can certainly pass for the real thing if we don’t
    pay attention to the details. One way to improve the results further would be
    to call the `compile` function on the generated code and only keep code that compiles.
    That way we can make sure that it is at least syntactically correct. A slight
    variation of that approach would be to not start over on a syntax error, but just
    drop the line where the error occurs and everything that follows and try again.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不注意细节，生成的Python代码肯定可以通过真实代码。进一步改进结果的一种方法是在生成的代码上调用`compile`函数，并且仅保留编译的代码。这样我们可以确保它至少在语法上是正确的。稍微变化这种方法的一种方法是在语法错误时不重新开始，而只是删除发生错误的行以及其后的所有内容，然后重试。
- en: 5.5 Visualizing Recurrent Network Activations
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.5 可视化循环网络激活
- en: Problem
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you gain insight into what a recurrent network is doing?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如何获得对循环网络正在执行的操作的洞察？
- en: Solution
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Extract the activations from the neurons while they process text. Since we’re
    going to visualize the neurons, it makes sense to reduce their number. This will
    degrade the performance of the model a bit, but makes things simpler:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 提取神经元处理文本时的激活。由于我们将要可视化神经元，将它们的数量减少是有意义的。这会稍微降低模型的性能，但使事情变得更简单：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This model is a bit simpler and gets us slightly less accurate results, but
    it is good enough for visualizations. Keras has a handy method called `function`
    that allows us to specify an input and an output layer and will then run whatever
    part of the network is needed to convert from one to the other. The following
    method provides the network with a bit of text (a sequence of characters) and
    gets the activations for a specific layer back:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型稍微简单一些，得到的结果略微不够准确，但足够用于可视化。Keras有一个方便的方法叫做`function`，它允许我们指定一个输入和一个输出层，然后运行网络中需要的部分来进行转换。以下方法为网络提供了一段文本（一系列字符），并获取特定层的激活值：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now the question is which neurons to look at. Even our simplified model has
    512 neurons. Activations in an LSTM are between –1 and 1, so a simple way to find
    interesting neurons is to just pick the highest value corresponding to each character.
    `np.argmax(act, axis=1)` will get us that. We can visualize those neurons using:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是要看哪些神经元。即使我们简化的模型有512个神经元。LSTM中的激活值在-1到1之间，因此找到有趣的神经元的一个简单方法就是选择与每个字符对应的最高值。`np.argmax(act,
    axis=1)`将帮助我们实现这一点。我们可以使用以下方式可视化这些神经元：
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will produce a small bitmap. After we enlarge the bitmap and plot the
    code on top, we get:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个小的位图。当我们放大位图并在顶部绘制代码时，我们得到：
- en: '![neuron activations in an RNN](assets/dlcb_05in01.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![RNN中的神经元激活](assets/dlcb_05in01.png)'
- en: This looks interesting. The top neuron seems to keep track of where new statements
    start. The one with the green bars keeps track of spaces, but only in as far as
    they are used for indentation. The last-but-one neuron seems to fire when there
    is an `=` sign, but not when there is a `==`, suggesting the network learned the
    difference between assignment and equality.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很有趣。顶部的神经元似乎跟踪新语句的开始位置。带有绿色条的神经元跟踪空格，但仅限于用于缩进。倒数第二个神经元似乎在有`=`符号时激活，但在有`==`时不激活，这表明网络学会了赋值和相等之间的区别。
- en: Discussion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Deep learning models can be very effective, but their results are notoriously
    hard to explain. We more or less understand the mechanics of the training and
    inference, but it is often difficult to explain a concrete result, other than
    pointing to the actual calculations. Visualizing activations is one way of making
    what the network learned a little clearer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型可能非常有效，但它们的结果很难解释。我们更多或少了解训练和推理的机制，但往往很难解释一个具体的结果，除了指向实际的计算。可视化激活是使网络学到的内容更清晰的一种方式。
- en: Looking at the neurons with the highest activation for each character quickly
    gets us a set of neurons that might be of interest. Alternatively, we could explicitly
    try to look for neurons that fire in specific circumstances, for example inside
    brackets.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 查看每个字符的最高激活的神经元很快地得到了一组可能感兴趣的神经元。或者，我们可以明确地尝试寻找在特定情况下激活的神经元，例如在括号内。
- en: Once we have a specific neuron that looks interesting, we can use the same coloring
    technique to highlight larger chunks of code.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有一个看起来有趣的特定神经元，我们可以使用相同的着色技术来突出显示更大的代码块。
