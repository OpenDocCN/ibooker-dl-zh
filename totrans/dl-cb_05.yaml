- en: Chapter 5\. Generating Text in the Style of an Example Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we’ll look at how we can use recurrent neural networks (RNNs)
    to generate text in the style of a body of text. This makes for fun demos. People
    have used this type of network to generate anything from names of babies to descriptions
    of colors. These demos are a good way to get comfortable with recurrent networks.
    RNNs have their practical uses too—later in the book we’ll use them to train a
    chatbot and build a recommender system for music based on harvested playlists,
    and RNNs have been used in production to track objects in video.
  prefs: []
  type: TYPE_NORMAL
- en: The recurrent neural network is a type of neural network that is helpful when
    working with time or sequences. We’ll first look at Project Gutenberg as a source
    of free books and download the collected works of William Shakespeare using some
    simple code. Next, we’ll use an RNN to produce texts that seem Shakespearean (if
    you don’t pay too much attention) by training the network on downloaded text.
    We’ll then repeat the trick on Python code, and see how to vary the output. Finally,
    since Python code has a predictable structure, we can look at which neurons fire
    on which bits of code and visualize the workings of our RNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found in the following Python notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 5.1 Acquiring the Text of Public Domain Books
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to download the full text of some public domain books to use to train
    your model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the Python API for Project Gutenberg.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project Gutenberg contains the complete texts of over 50,000 books. There is
    a handy Python API available to browse and download these books. We can download
    any book if we know the ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can get a book’s ID either by browsing the website and extracting it from
    the book’s URL or by querying [*http://www.gutenberg.org/*](http://www.gutenberg.org/)
    by author or title. Before we can query, though, we need to populate the metainformation
    cache. This will create a local database of all books available. It takes a bit
    of time, but only needs to be done once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now discover all works by Shakespeare:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Project Gutenberg is a volunteer project to digitize books. It focuses on making
    available the most important books in English that are out of copyright in the
    United States, though it also has books in other languages. It was started in
    1971, long before the invention of the World Wide Web by Michael Hart.
  prefs: []
  type: TYPE_NORMAL
- en: Any work published in the US before 1923 is in the public domain, so most books
    found in the Gutenberg collection are older than that. This means that the language
    can be somewhat dated, but for natural language processing the collection remains
    an unrivalled source of training data. Going through the Python API not only makes
    access easy but also respects the restrictions that the site puts up for automatic
    downloading of texts.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Generating Shakespeare-Like Texts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you generate text in a specific style?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a character-level RNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by acquiring Shakespeare’s collected works. We’ll drop the poems,
    so we’re left with a more consistent set of just the plays. The poems happen to
    be collected in the first entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re going to feed the text in character by character and we’ll one-hot encode
    each character—that is, every character will be encoded as a vector containing
    all 0s and one 1\. For this, we need to know which characters we’re going to encounter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s create our model that will take a sequence of characters and predict
    a sequence of characters. We’ll feed the sequence into a number of LSTM layers
    that do the work. The `TimeDistributed` layer lets our model output a sequence
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to feed in random fragments from the plays to the network, so
    a generator seems appropriate. The generator will yield blocks of pairs of sequences,
    where the sequences of the pairs are just one character apart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll train the model. We’ll set `steps_per_epoch` such that each character
    should have a decent chance to be seen by the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After training we can generate some output. We pick a random fragment from
    the plays and let the model guess what the next character is. We then add the
    next character to the fragment and repeat until we’ve reached the required number
    of characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After 10 epochs we should see some text that reminds us of Shakespeare, but
    we need around 30 for it to start to look like it could fool a casual reader that
    is not paying too close attention:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: It’s somewhat suspicious that both Cleopatra and the Second Lord have a son
    of Greece, but the present winter and the world being slain are appropriately
    *Game of Thrones*.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe we saw how we can use RNNs to generate text in a certain style.
    The results are quite convincing, especially given the fact that the model predicts
    on a character-by-character level. Thanks to the LSTM architecture, the network
    is capable of learning relationships that span quite large sequences—not just
    words, but sentences, and even the basic structure of the layout of Shakespeare’s
    plays.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the example shown here isn’t very practical, RNNs can be. Any time
    we want a network to learn a sequence of items, an RNN is probably a good choice.
  prefs: []
  type: TYPE_NORMAL
- en: Other toy apps people have built using this technique have generated baby names,
    names for paint colors, and even recipes.
  prefs: []
  type: TYPE_NORMAL
- en: More practical RNNs can be used to predict the next character a user is going
    to type for a smartphone keyboard app, or predict the next move in a chess game
    when trained on a set of openings. This type of network has also been used to
    predict sequences like weather patterns or even stock market prices.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent networks are quite fickle, though. Seemingly small changes to the
    network architecture can lead to a situation where they no longer converge because
    of the so-called *exploding gradient problem*. Sometimes during training, after
    making progress for a number of epochs, the network seems to collapse and starts
    forgetting what it learns. As always, it is best to start with something simple
    that works and add complexity step by step, while keeping track of what was changed.
  prefs: []
  type: TYPE_NORMAL
- en: For a slightly more in-depth discussion of RNNs, see [Chapter 1](ch01.html#tools_techniques).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Writing Code Using RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you generate Python code using a neural network?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train a recurrent neural network over the Python code that comes with the Python
    distribution that runs your scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can in fact use pretty much the same model as in the previous recipe for
    this task. As is often the case with deep learning, the key thing is to get the
    data. Python ships with the source code of many modules. Since they are stored
    in the directory where the *random.py* module sits, we can collect them using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We could then read in all these source files and concatenate them into one document
    and start generating new snippets, just as we did with the Shakespearean text
    in the previous recipe. This works reasonably well, but when generating snippets,
    it becomes clear that a good chunk of Python source code is actually English.
    English appears both in the form of comments and the contents of strings. We want
    our model to learn Python, not English!
  prefs: []
  type: TYPE_NORMAL
- en: 'Stripping out the comments is easy enough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Removing the contents of strings is slightly more involved. Some strings contain
    useful patterns, rather than English. As a rough rule, we’re going to replace
    any bit of text that has more than six letters and at least one space with `"MSG"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finding the occurrences of string literals can be done concisely with a regular
    expression. Regular expressions are rather slow though, and we’re running them
    over a sizeable amount of code. In this case it’s better to just scan the strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Even cleaned up this way, we end up with megabytes of pure Python code. We can
    now train the model as before, but on Python code rather than on plays. After
    30 epochs or so, we should have something workable and can generate code.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generating Python code is no different from writing a Shakespearean-style play—at
    least for a neural network. We’ve seen that cleaning up the input data is an important
    aspect of data processing for neural networks. In this case we made sure to remove
    most traces of English from the source code. This way the network can focus on
    learning Python and not be distracted by also having to allocate neurons to learning
    English.
  prefs: []
  type: TYPE_NORMAL
- en: We could further regularize the input. For example, we could pipe all the source
    code first through a “pretty printer” so that it would all have the same layout
    and our network could focus on learning that, rather than the diversity found
    in the current code. One step further would be to tokenize the Python code using
    the built-in tokenizer, and then let the network learn this parsed version and
    use `untokenize` to generate the code.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Controlling the Temperature of the Output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to control the variability of the generated code.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the predictions as a probability distribution, rather than picking the highest
    value.
  prefs: []
  type: TYPE_NORMAL
- en: In the Shakespeare example, we picked the character in the predictions that
    had the highest score. This approach results in the output that is the best liked
    by the model. The drawback is that we get the same output for every start. Since
    we picked a random start sequence from the actual Shakespearean texts that didn’t
    matter much. But if we want to generate Python functions, it would be nice to
    always start in the same way—let’s say with `/ndef`—and look at various solutions.
  prefs: []
  type: TYPE_NORMAL
- en: The predictions of our network are the result of a softmax activation function
    and can therefore be seen as a probability distribution. So, rather than picking
    the maximum value, we can let `numpy.random.multinomial` give us an answer. `multinomial`
    runs *n* experiments and takes the probability of how likely the outcomes are.
    By running it with *n* = 1, we get what we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point we can introduce the notion of temperature in how we draw the
    outcomes. The idea is that the higher the temperature is, the more random the
    outcomes are, while lower temperatures are closer to the pure deterministic outcomes
    we saw earlier. We do this by scaling the logs of the predictions accordingly
    and then applying the softmax function again to get back to probabilities. Putting
    this all together we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re finally ready to have some fun. At `diversity=1.0` the following code
    is produced. Note how the model generated our `"MSG"` placeholder and, apart from
    confusing `val` and `value`, almost got us running code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the output of the softmax activation function as a probability distribution
    allows us to get a variety of results that correspond to what the model “intends.”
    An added bonus is that it allows us to introduce the notion of temperature, so
    we can control how “random” the output is. In [Chapter 13](ch13.html#autoencoders)
    we’ll look at how *variational* *autoencoders* use a similar technique to control
    the randomness of what is generated.
  prefs: []
  type: TYPE_NORMAL
- en: The generated Python code can certainly pass for the real thing if we don’t
    pay attention to the details. One way to improve the results further would be
    to call the `compile` function on the generated code and only keep code that compiles.
    That way we can make sure that it is at least syntactically correct. A slight
    variation of that approach would be to not start over on a syntax error, but just
    drop the line where the error occurs and everything that follows and try again.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Visualizing Recurrent Network Activations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you gain insight into what a recurrent network is doing?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Extract the activations from the neurons while they process text. Since we’re
    going to visualize the neurons, it makes sense to reduce their number. This will
    degrade the performance of the model a bit, but makes things simpler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This model is a bit simpler and gets us slightly less accurate results, but
    it is good enough for visualizations. Keras has a handy method called `function`
    that allows us to specify an input and an output layer and will then run whatever
    part of the network is needed to convert from one to the other. The following
    method provides the network with a bit of text (a sequence of characters) and
    gets the activations for a specific layer back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the question is which neurons to look at. Even our simplified model has
    512 neurons. Activations in an LSTM are between –1 and 1, so a simple way to find
    interesting neurons is to just pick the highest value corresponding to each character.
    `np.argmax(act, axis=1)` will get us that. We can visualize those neurons using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce a small bitmap. After we enlarge the bitmap and plot the
    code on top, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![neuron activations in an RNN](assets/dlcb_05in01.png)'
  prefs: []
  type: TYPE_IMG
- en: This looks interesting. The top neuron seems to keep track of where new statements
    start. The one with the green bars keeps track of spaces, but only in as far as
    they are used for indentation. The last-but-one neuron seems to fire when there
    is an `=` sign, but not when there is a `==`, suggesting the network learned the
    difference between assignment and equality.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning models can be very effective, but their results are notoriously
    hard to explain. We more or less understand the mechanics of the training and
    inference, but it is often difficult to explain a concrete result, other than
    pointing to the actual calculations. Visualizing activations is one way of making
    what the network learned a little clearer.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the neurons with the highest activation for each character quickly
    gets us a set of neurons that might be of interest. Alternatively, we could explicitly
    try to look for neurons that fire in specific circumstances, for example inside
    brackets.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a specific neuron that looks interesting, we can use the same coloring
    technique to highlight larger chunks of code.
  prefs: []
  type: TYPE_NORMAL
