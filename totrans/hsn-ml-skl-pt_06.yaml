- en: Chapter 5\. Decision Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 章\. 决策树
- en: '*Decision trees* are versatile machine learning algorithms that can perform
    both classification and regression tasks, and even multioutput tasks. They are
    powerful algorithms, capable of fitting complex datasets. For example, in [Chapter 2](ch02.html#project_chapter)
    you trained a `DecisionTreeRegressor` model on the California housing dataset,
    fitting it perfectly (actually, overfitting it).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树*是通用的机器学习算法，可以执行分类和回归任务，甚至多输出任务。它们是强大的算法，能够拟合复杂的数据集。例如，在第 2 章中，您在加利福尼亚住房数据集上训练了一个`DecisionTreeRegressor`模型，并完美地（实际上，过度拟合）地拟合了它。'
- en: Decision trees are also the fundamental components of random forests (see [Chapter 6](ch06.html#ensembles_chapter)),
    which are among the most powerful machine learning algorithms available today.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树也是随机森林（见第 6 章）的基本组成部分，而随机森林是目前最强大的机器学习算法之一。
- en: In this chapter we will start by discussing how to train, visualize, and make
    predictions with decision trees. Then we will go through the CART training algorithm
    used by Scikit-Learn, and we will explore how to regularize trees and use them
    for regression tasks. Finally, we will discuss some of the limitations of decision
    trees.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先讨论如何使用决策树进行训练、可视化和预测。然后我们将介绍 Scikit-Learn 使用的 CART 训练算法，并探讨如何正则化树以及如何将它们用于回归任务。最后，我们将讨论决策树的一些局限性。
- en: Training and Visualizing a Decision Tree
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和可视化决策树
- en: 'To understand decision trees, let’s build one and take a look at how it makes
    predictions. The following code trains a `DecisionTreeClassifier` on the iris
    dataset (see [Chapter 4](ch04.html#linear_models_chapter)):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解决策树，让我们构建一个并看看它是如何进行预测的。以下代码在玫瑰数据集上训练了一个`DecisionTreeClassifier`（见第 4 章）：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can visualize the trained decision tree by first using the `export_graphviz()`
    function to output a graph definition file called *iris_tree.dot*:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过首先使用`export_graphviz()`函数输出一个名为*iris_tree.dot*的图形定义文件来可视化训练好的决策树：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then you can use `graphviz.Source.from_file()` to load and display the file
    in a Jupyter notebook:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用`graphviz.Source.from_file()`来加载和显示文件在一个 Jupyter 笔记本中：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Graphviz](https://graphviz.org) is an open source graph visualization software
    package. It also includes a `dot` command-line tool to convert *.dot* files to
    a variety of formats, such as PDF or PNG.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[Graphviz](https://graphviz.org)是一个开源的图形可视化软件包。它还包括一个`dot`命令行工具，可以将*.dot*文件转换为多种格式，如PDF或PNG。'
- en: Your first decision tree looks like [Figure 5-1](#iris_tree).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您的第一个决策树看起来像[图 5-1](#iris_tree)。
- en: '![A diagram of a decision tree for classifying iris species based on petal
    length and width, showing split nodes and leaf nodes with classification results
    for setosa, versicolor, and virginica.](assets/hmls_0501.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![基于花瓣长度和宽度的玫瑰物种分类决策树图，显示分割节点和叶节点，以及对于伊丽莎白、多色和维吉尼亚的分类结果。](assets/hmls_0501.png)'
- en: Figure 5-1\. Iris decision tree
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 玫瑰决策树
- en: Making Predictions
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行预测
- en: 'Let’s see how the tree represented in [Figure 5-1](#iris_tree) makes predictions.
    Suppose you find an iris flower and you want to classify it based on its petals.
    You start at the *root node* (depth 0, at the top): this node asks whether the
    flower’s petal length is smaller than 2.45 cm. If it is, then you move down to
    the root’s left child node (depth 1, left). In this case, it is a *leaf node*
    (i.e., it does not have any child nodes), so it does not ask any questions: simply
    look at the predicted class for that node, and the decision tree predicts that
    your flower is an *Iris setosa* (`class=setosa`).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看[图 5-1](#iris_tree)中展示的树是如何进行预测的。假设您发现了一朵玫瑰，并想根据其花瓣对其进行分类。您从*根节点*（深度 0，位于顶部）开始：此节点询问花朵的花瓣长度是否小于
    2.45 厘米。如果是，那么您将向下移动到根的左子节点（深度 1，左侧）。在这种情况下，它是一个*叶节点*（即它没有子节点），所以它不会提出任何问题：只需查看该节点的预测类别，决策树预测您的花朵是*玫瑰伊丽莎白*（`class=setosa`）。
- en: 'Now suppose you find another flower, and this time the petal length is greater
    than 2.45 cm. You again start at the root but now move down to its right child
    node (depth 1, right). This is not a leaf node, it’s a *split node*, so it asks
    another question: is the petal width smaller than 1.75 cm? If it is, then your
    flower is most likely an *Iris versicolor* (depth 2, left). If not, it is likely
    an *Iris virginica* (depth 2, right). It’s really that simple.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设你找到了另一朵花，这次瓣长大于 2.45 厘米。你再次从根节点开始，但这次移动到其右侧子节点（深度 1，右侧）。这不是一个叶节点，它是一个 *分裂节点*，因此它会问另一个问题：瓣宽是否小于
    1.75 厘米？如果是，那么你的花很可能是 *Iris versicolor*（深度 2，左侧）。如果不是，它很可能是 *Iris virginica*（深度
    2，右侧）。这真的很简单。
- en: Note
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: One of the many qualities of decision trees is that they require very little
    data preparation. In fact, they don’t require feature scaling or centering at
    all.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一个许多优点是它们需要很少的数据准备。事实上，它们根本不需要特征缩放或中心化。
- en: 'A node’s `samples` attribute counts how many training instances it applies
    to. For example, 100 training instances have a petal length greater than 2.45
    cm (depth 1, right), and of those 100, 54 have a petal width smaller than 1.75
    cm (depth 2, left). A node’s `value` attribute tells you how many training instances
    of each class this node applies to: for example, the bottom-right node applies
    to 0 *Iris setosa*, 1 *Iris versicolor*, and 45 *Iris virginica*. Finally, a node’s
    `gini` attribute measures its *Gini impurity*: a node is “pure” (`gini=0`) if
    all training instances it applies to belong to the same class. For example, since
    the depth-1 left node applies only to *Iris setosa* training instances, its Gini
    impurity is 0\. Conversely, the other nodes all apply to instances of multiple
    classes, so they are “impure”. [Equation 5-1](#gini_impurity) shows how the training
    algorithm computes the Gini impurity *G*[*i*] of the *i*^(th) node. The more classes
    and the more mixed they are, the larger the impurity. For example, the depth-2
    left node has a Gini impurity equal to 1 – (0/54)² – (49/54)² – (5/54)² ≈ 0.168.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个节点的 `samples` 属性计算它应用了多少个训练实例。例如，100 个训练实例的瓣长大于 2.45 厘米（深度 1，右侧），其中 100 个中有
    54 个瓣宽小于 1.75 厘米（深度 2，左侧）。一个节点的 `value` 属性告诉你这个节点应用到了每个类别的多少个训练实例：例如，右下角的节点应用到了
    0 个 *Iris setosa*，1 个 *Iris versicolor* 和 45 个 *Iris virginica*。最后，一个节点的 `gini`
    属性衡量其 *Gini 不纯度*：如果一个节点应用到的所有训练实例都属于同一类别，则该节点是“纯净的”（`gini=0`）。例如，由于深度 1 的左侧节点只应用到了
    *Iris setosa* 的训练实例，其吉尼不纯度为 0。相反，其他节点都应用到了多个类别的实例，因此它们是“不纯净的”。[方程式 5-1](#gini_impurity)
    展示了训练算法如何计算第 *i* 个节点的吉尼不纯度 *G*[*i*]。类别越多，混合程度越高，不纯度就越大。例如，深度 2 的左侧节点的吉尼不纯度等于 1
    – (0/54)² – (49/54)² – (5/54)² ≈ 0.168。
- en: Equation 5-1\. Gini impurity
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 5-1\. 吉尼不纯度
- en: $upper G Subscript i Baseline equals 1 minus sigma-summation Underscript k equals
    1 Overscript n Endscripts p Subscript i comma k Baseline Superscript 2$
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: $upper G Subscript i Baseline equals 1 minus sigma-summation Underscript k equals
    1 Overscript n Endscripts p Subscript i comma k Baseline Superscript 2$
- en: 'In this equation:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*G*[*i*] is the Gini impurity of the *i*^(th) node.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*G*[*i*] 是第 *i* 个节点的吉尼不纯度。'
- en: '*p*[*i*,*k*] is the ratio of class *k* instances among the training instances
    in the *i*^(th) node.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p*[*i*,*k*] 是第 *i* 个节点中训练实例中类别 *k* 的实例比率。'
- en: Note
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Scikit-Learn uses the CART algorithm (discussed shortly), which produces only
    *binary trees*, meaning trees where split nodes always have exactly two children
    (i.e., questions only have yes/no answers). However, other algorithms, such as
    ID3, can produce decision trees with nodes that have more than two children.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 使用 CART 算法（稍后讨论），它只产生 *二叉树*，这意味着分裂节点总是有两个子节点（即问题只有是/否的答案）。然而，其他算法，如
    ID3，可以产生具有超过两个子节点的决策树。
- en: '[Figure 5-2](#decision_tree_decision_boundaries_plot) shows this decision tree’s
    decision boundaries. The thick vertical line represents the decision boundary
    of the root node (depth 0): petal length = 2.45 cm. Since the lefthand area is
    pure (only *Iris setosa*), it cannot be split any further. However, the righthand
    area is impure, so the depth-1 right node splits it at petal width = 1.75 cm (represented
    by the dashed line). Since `max_depth` was set to 2, the decision tree stops right
    there. If you set `max_depth` to 3, then the two depth-2 nodes would each add
    another decision boundary (represented by the two vertical dotted lines).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-2](#decision_tree_decision_boundaries_plot) 显示了这个决策树的决策边界。粗垂直线表示根节点（深度
    0）的决策边界：花瓣长度 = 2.45 厘米。由于左侧区域是纯的（只有 *Iris setosa*），它不能再进一步分割。然而，右侧区域是不纯的，因此深度为1的右节点在花瓣宽度
    = 1.75 厘米处将其分割（由虚线表示）。由于 `max_depth` 设置为 2，决策树就在那里停止。如果你将 `max_depth` 设置为 3，那么这两个深度为2的节点将各自添加另一个决策边界（由两条垂直虚线表示）。'
- en: '![Diagram illustrating decision tree decision boundaries with depth levels
    and data points for different Iris flower species.](assets/hmls_0502.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![说明具有深度级别和不同 Iris 花种数据点的决策树决策边界的图](assets/hmls_0502.png)'
- en: Figure 5-2\. Decision tree decision boundaries
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. 决策树决策边界
- en: Tip
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The tree structure, including all the information shown in [Figure 5-1](#iris_tree),
    is available via the classifier’s `tree_` attribute. Type **`help(tree_clf.tree_)`**
    for details, and see [this chapter’s notebook](https://homl.info/colab-p) for
    an example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 树结构，包括 [图 5-1](#iris_tree) 中显示的所有信息，都可通过分类器的 `tree_` 属性获得。输入 **`help(tree_clf.tree_)`**
    获取详细信息，并查看 [本章的笔记本](https://homl.info/colab-p) 以获取示例。
- en: Estimating Class Probabilities
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计类概率
- en: 'A decision tree can also estimate the probability that an instance belongs
    to a particular class *k*. First it traverses the tree to find the leaf node for
    this instance, and then it returns the proportion of instances of class *k* among
    the training instances that would also reach this leaf node. For example, suppose
    you have found a flower whose petals are 5 cm long and 1.5 cm wide. The corresponding
    leaf node is the depth-2 left node, so the decision tree outputs the following
    probabilities: 0% for *Iris setosa* (0/54), 90.7% for *Iris versicolor* (49/54),
    and 9.3% for *Iris virginica* (5/54). And if you ask it to predict the class,
    it outputs *Iris versicolor* (class 1) because it has the highest probability.
    Let’s check this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树还可以估计一个实例属于特定类别 *k* 的概率。首先，它遍历树以找到该实例的叶节点，然后返回在训练实例中达到该叶节点的类别 *k* 的实例比例。例如，假设你发现一朵花瓣长5厘米、宽1.5厘米的花。相应的叶节点是深度为2的左节点，因此决策树输出以下概率：*Iris
    setosa* 为 0%（0/54），*Iris versicolor* 为 90.7%（49/54），*Iris virginica* 为 9.3%（5/54）。如果你要求它预测类别，它输出
    *Iris versicolor*（类别 1），因为它具有最高的概率。让我们检查一下：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`` `Perfect! Notice that the estimated probabilities would be identical anywhere
    else in the bottom-right rectangle of [Figure 5-2](#decision_tree_decision_boundaries_plot)—for
    example, if the petals were 6 cm long and 1.5 cm wide (even though it seems obvious
    that it would most likely be an *Iris virginica* in this case).` ``  [PRE4]``
    [PRE5]` # The CART Training Algorithm    Scikit-Learn uses the *Classification
    and Regression Tree* (CART) algorithm to train decision trees (also called “growing”
    trees). The algorithm works by first splitting the training set into two subsets
    using a single feature *k* and a threshold *t*[*k*] (e.g., “petal length ≤ 2.45
    cm”). How does it choose *k* and *t*[*k*]? It searches for the pair (*k*, *t*[*k*])
    that produces the purest subsets, weighted by their size. [Equation 5-2](#classification_cart_cost_function)
    gives the cost function that the algorithm tries to minimize.    ##### Equation
    5-2\. CART cost function for classification  $StartLayout 1st Row 1st Column upper
    J left-parenthesis k comma t Subscript k Baseline right-parenthesis 2nd Column
    equals StartFraction m Subscript left Baseline Over m EndFraction upper G Subscript
    left Baseline plus StartFraction m Subscript right Baseline Over m EndFraction
    upper G Subscript right Baseline 2nd Row 1st Column where 2nd Column StartLayout
    Enlarged left-brace 1st Row  upper G Subscript left slash right Baseline measures
    the impurity of the left slash right subset 2nd Row  m Subscript left slash right
    Baseline is the number of instances in the left slash right subset 3rd Row  m
    equals m Subscript left Baseline plus m Subscript right EndLayout EndLayout$  Once
    the CART algorithm has successfully split the training set in two, it splits the
    subsets using the same logic, then the sub-subsets, and so on, recursively. It
    stops recursing once it reaches the maximum depth (defined by the `max_depth`
    hyperparameter), or if it cannot find a split that will reduce impurity. A few
    other hyperparameters (described in a moment) control additional stopping conditions:
    `min_samples_split`, `min_samples_leaf`, `max_leaf_nodes`, and more.    ######
    Warning    As you can see, the CART algorithm is a *greedy algorithm*: it greedily
    searches for an optimum split at the top level, then repeats the process at each
    subsequent level. It does not check whether the split will lead to the lowest
    possible impurity several levels down. A greedy algorithm often produces a solution
    that’s reasonably good but not guaranteed to be optimal.    Unfortunately, finding
    the optimal tree is known to be an *NP-complete* problem.⁠^([1](ch05.html#id1673))
    It requires *O*(exp(*m*)) time,⁠^([2](ch05.html#id1674)) making the problem intractable
    even for small training sets. This is why we must settle for a “reasonably good”
    solution when training decision trees.    # Computational Complexity    Making
    predictions requires traversing the decision tree from the root to a leaf. Decision
    trees are generally approximately balanced, so traversing the decision tree requires
    going through roughly *O*(log[2](*m*)) nodes, where *m* is the number of training
    instances, and log[2](*m*) is the *binary logarithm* of *m*, equal to log(*m*)
    / log(2). Since each node only requires checking the value of one feature, the
    overall prediction complexity is *O*(log[2](*m*)), independent of the number of
    features. So predictions are very fast, even when dealing with large training
    sets.    By default, the training algorithm compares all features on all samples
    at each node, which results in a training complexity of *O*(*n* × *m* log[2](*m*)).    It’s
    possible to set a maximum tree depth using the `max_depth` hyperparameter, and/or
    set a maximum number of features to consider at each node (the features are then
    chosen randomly). Doing so will help speed up training considerably, and it can
    also reduce the risk of overfitting (but as always, going too far would result
    in underfitting).    # Gini Impurity or Entropy?    By default, the `DecisionTreeClassifier`
    class uses the Gini impurity measure, but you can select the *entropy* impurity
    measure instead by setting the `criterion` hyperparameter to `"entropy"`. The
    concept of entropy originated in thermodynamics as a measure of molecular disorder:
    entropy approaches zero when molecules are still and well ordered. Entropy later
    spread to a wide variety of domains, including in Shannon’s information theory,
    where it measures the average information content of a message, as we saw in [Chapter 4](ch04.html#linear_models_chapter).
    Entropy is zero when all messages are identical. In machine learning, entropy
    is frequently used as an impurity measure: a set’s entropy is zero when it contains
    instances of only one class. [Equation 5-3](#entropy_function) shows the definition
    of the entropy of the *i*^(th) node. For example, the depth-2 left node in [Figure 5-1](#iris_tree)
    has an entropy equal to –(49/54) log[2] (49/54) – (5/54) log[2] (5/54) ≈ 0.445.    #####
    Equation 5-3\. Entropy  <mrow><msub><mi>H</mi> <mi>i</mi></msub> <mo>=</mo> <mo>-</mo>
    <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub>
    <mo>≠</mo><mn>0</mn></mrow></mfrac> <mi>n</mi></munderover> <mrow><msub><mi>p</mi>
    <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <mrow><mo>(</mo> <msub><mi>p</mi> <mrow><mi>i</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <mo>)</mo></mrow></mrow></mrow>  So,
    should you use Gini impurity or entropy? The truth is, most of the time it does
    not make a big difference: they lead to similar trees. Gini impurity is slightly
    faster to compute, so it is a good default. However, when they differ, Gini impurity
    tends to isolate the most frequent class in its own branch of the tree, while
    entropy tends to produce slightly more balanced trees.⁠^([3](ch05.html#id1691))    #
    Regularization Hyperparameters    Decision trees make very few assumptions about
    the training data (as opposed to linear models, which assume that the data is
    linear, for example). If left unconstrained, the tree structure will adapt itself
    to the training data, fitting it very closely—indeed, most likely overfitting
    it. Such a model is often called a *nonparametric model*, not because it does
    not have any parameters (it often has a lot) but because the number of parameters
    is not determined prior to training, so the model structure is free to stick closely
    to the data. In contrast, a *parametric model*, such as a linear model, has a
    predetermined number of parameters, so its degree of freedom is limited, reducing
    the risk of overfitting (but increasing the risk of underfitting).    To avoid
    overfitting the training data, you need to restrict the decision tree’s freedom
    during training. As you know by now, this is called regularization. The regularization
    hyperparameters depend on the algorithm used, but generally you can at least restrict
    the maximum depth of the decision tree. In Scikit-Learn, this is controlled by
    the `max_depth` hyperparameter. The default value is `None`, which means unlimited.
    Reducing `max_depth` will regularize the model and thus reduce the risk of overfitting.    The
    `DecisionTreeClassifier` class has a few other parameters that similarly restrict
    the shape of the decision tree:    `max_features`      Maximum number of features
    that are evaluated for splitting at each node      `max_leaf_nodes`      Maximum
    number of leaf nodes      `min_samples_split`      Minimum number of samples a
    node must have before it can be split      `min_samples_leaf`      Minimum number
    of samples a leaf node must have to be created      `min_weight_fraction_leaf`      Same
    as `min_samples_leaf` but expressed as a fraction of the total number of weighted
    instances      `min_impurity_decrease`      Only split a node if this split results
    in at least this reduction in impurity      `ccp_alpha`      Controls *minimal
    cost-complexity pruning* (MCCP), i.e., pruning subtrees that don’t reduce impurity
    enough compared to their number of leaves; a larger `ccp_alpha` value leads to
    more pruning, resulting in a smaller tree (the default is 0—no pruning)      To
    limit the model’s complexity and thereby regularize the model, you can increase
    `min_*` hyperparameters or `ccp_alpha`, or decrease `max_*` hyperparameters. Tuning
    `max_depth` is usually a good default: it provides effective regularization, and
    it keeps the tree small and easy to interpret. Setting `min_samples_leaf` is also
    a good idea, especially for small datasets. And `max_features` is great when working
    with high-dimensional datasets.    ###### Note    Other algorithms work by first
    training the decision tree without restrictions, then *pruning* (deleting) unnecessary
    nodes. A node whose children are all leaf nodes is considered unnecessary if the
    purity improvement it provides is not statistically significant. Standard statistical
    tests, such as the *χ*² *test* (chi-squared test), are used to estimate the probability
    that the improvement is purely the result of chance (which is called the *null
    hypothesis*). If this probability, called the *p-value*, is higher than a given
    threshold (typically 5%, controlled by a hyperparameter), then the node is considered
    unnecessary and its children are deleted. The pruning continues until all unnecessary
    nodes have been pruned.    Let’s test regularization on the moons dataset: this
    is a toy dataset for binary classification in which the data points are shaped
    as two interleaving crescent moons (see [Figure 5-3](#min_samples_leaf_plot)).
    You can generate this dataset using the `make_moons()` function.    We’ll train
    one decision tree without regularization, and another with `min_samples_leaf=5`.
    Here’s the code; [Figure 5-3](#min_samples_leaf_plot) shows the decision boundaries
    of each tree:    [PRE6]  ![Comparison of decision boundaries: the left diagram
    shows an unregularized decision tree''s complex boundaries, while the right depicts
    a regularized tree with smoother boundaries indicating potentially better generalization.](assets/hmls_0503.png)  ######
    Figure 5-3\. Decision boundaries of an unregularized tree (left) and a regularized
    tree (right)    The unregularized model on the left is clearly overfitting, and
    the regularized model on the right will probably generalize better. We can verify
    this by evaluating both trees on a test set generated using a different random
    seed:    [PRE7]`` `...` [PRE8] `0.898` `>>>` `tree_clf2``.``score``(``X_moons_test``,`
    `y_moons_test``)` `` `0.92` `` [PRE9]` [PRE10]   [PRE11]  [PRE12] [PRE13]`py [PRE14]``'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`` `完美！注意，在[图5-2](#decision_tree_decision_boundaries_plot)右下角的任何地方，估计的概率都是相同的——例如，如果花瓣长度为6厘米，宽度为1.5厘米（尽管在这种情况下似乎很明显它很可能是*Iris
    virginica*）。` `` [PRE4]`` [PRE5]` # CART训练算法 Scikit-Learn使用**分类和回归树**（CART）算法来训练决策树（也称为“生长”树）。该算法通过首先使用单个特征*k*和阈值*t*[*k*]（例如，“花瓣长度≤2.45厘米”）将训练集分成两个子集来工作。它是如何选择*k*和*t*[*k*]的呢？它寻找产生最纯净子集的(*k*,
    *t*[*k*])对，并按其大小进行加权。[方程5-2](#classification_cart_cost_function)给出了算法试图最小化的成本函数。    #####
    方程5-2\. 分类CART成本函数  $StartLayout 1st Row 1st Column upper J left-parenthesis k
    comma t Subscript k Baseline right-parenthesis 2nd Column equals StartFraction
    m Subscript left Baseline Over m EndFraction upper G Subscript left Baseline plus
    StartFraction m Subscript right Baseline Over m EndFraction upper G Subscript
    right Baseline 2nd Row 1st Column where 2nd Column StartLayout Enlarged left-brace
    1st Row  upper G Subscript left slash right Baseline measures the impurity of
    the left slash right subset 2nd Row  m Subscript left slash right Baseline is
    the number of instances in the left slash right subset 3rd Row  m equals m Subscript
    left Baseline plus m Subscript right EndLayout EndLayout$  一旦CART算法成功地将训练集分成两个部分，它就会使用相同的逻辑将子集分成两个部分，然后是子子集，依此类推，递归地进行。一旦达到最大深度（由`max_depth`超参数定义），或者找不到将减少纯度的分割，它就会停止递归。一些其他超参数（稍后描述）控制了额外的停止条件：`min_samples_split`、`min_samples_leaf`、`max_leaf_nodes`等。    ######
    警告    如您所见，CART算法是一种**贪婪算法**：它在顶层贪婪地搜索最优分割，然后在每个后续级别重复此过程。它不会检查分割是否会降低几层以下的最低可能纯度。贪婪算法通常会产生一个相当好的解决方案，但并不保证是最优的。    不幸的是，找到最优树是一个已知的**NP完全**问题。⁠^([1](ch05.html#id1673))
    它需要*O*(exp(*m*))时间，⁠^([2](ch05.html#id1674)) 即使对于小的训练集，这个问题也是不可行的。这就是为什么我们在训练决策树时必须满足于“相当好”的解决方案。    #
    计算复杂度    做出预测需要从根节点遍历决策树到叶子节点。决策树通常是近似平衡的，因此遍历决策树需要通过大约*O*(log[2](*m*))个节点，其中*m*是训练实例的数量，log[2](*m*)是*m*的**二进制对数**，等于log(*m*)
    / log(2)。由于每个节点只需要检查一个特征值，因此整体预测复杂度为*O*(log[2](*m*))，与特征数量无关。因此，即使处理大型训练集，预测也非常快。    默认情况下，训练算法在节点上的每个样本上比较所有特征，这导致训练复杂度为*O*(*n*
    × *m* log[2](*m*))。    可以使用`max_depth`超参数设置最大树深度，并且/或者设置每个节点要考虑的最大特征数（然后特征被随机选择）。这样做将大大加快训练速度，并且还可以减少过拟合的风险（但像往常一样，做得太过分会导致欠拟合）。    #
    Gini不纯度或熵？    默认情况下，`DecisionTreeClassifier`类使用Gini不纯度度量，但可以通过将`criterion`超参数设置为`"entropy"`来选择*熵*不纯度度量。熵的概念起源于热力学，作为分子无序的度量：当分子静止且有序时，熵接近零。熵后来扩展到广泛的领域，包括香农的信息理论，其中它衡量消息的平均信息内容，正如我们在[第4章](ch04.html#linear_models_chapter)中看到的。当所有消息都相同的时候，熵为零。在机器学习中，熵经常被用作不纯度度量：当集合只包含一个类的实例时，集合的熵为零。[方程5-3](#entropy_function)显示了*i*^(th)节点的熵的定义。例如，[图5-1](#iris_tree)中深度为2的左节点熵等于
    –(49/54) log[2] (49/54) – (5/54) log[2] (5/54) ≈ 0.445。    ##### 方程5-3\. 熵  <mrow><msub><mi>H</mi>
    <mi>i</mi></msub> <mo>=</mo> <mo>-</mo> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub>
    <mo>≠</mo><mn>0</mn></mrow></mfrac> <mi>n</mi></munderover> <mrow><msub><mi>p</mi>
    <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <mrow><mo>(</mo> <msub><mi>p</mi> <mrow><mi>i</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <mo>)</mo></mrow></mrow></mrow>  因此，您应该使用Gini不纯度还是熵？事实是，大多数时候它们没有太大区别：它们会导致类似的树。Gini不纯度计算速度略快，因此是一个好的默认值。然而，当它们不同时，Gini不纯度倾向于将最频繁的类隔离在树的自己的分支中，而熵倾向于产生稍微更平衡的树。⁠^([3](ch05.html#id1691))    #
    正则化超参数    决策树对训练数据所做的假设非常少（与假设数据是线性的线性模型相反）。如果不受约束，树结构将适应训练数据，非常紧密地拟合它——实际上，很可能会过拟合它。这样的模型通常被称为**非参数模型**，并不是因为它没有任何参数（它通常有很多）而是因为参数的数量在训练之前没有确定，因此模型结构可以自由地紧密适应数据。相比之下，**参数模型**，如线性模型，具有预定的参数数量，因此其自由度有限，这降低了过拟合的风险（但增加了欠拟合的风险）。    为了避免过拟合训练数据，您需要在训练过程中限制决策树的自由度。如您现在所知，这被称为正则化。正则化超参数取决于所使用的算法，但通常您至少可以限制决策树的最大深度。在Scikit-Learn中，这由`max_depth`超参数控制。默认值是`None`，这意味着无限制。减少`max_depth`将正则化模型并因此降低过拟合的风险。    `DecisionTreeClassifier`类有几个其他参数，这些参数以类似的方式限制决策树的结构：    `max_features`      每个节点上评估分割的最大特征数      `max_leaf_nodes`      最大叶子节点数      `min_samples_split`      节点在可以分割之前必须拥有的最小样本数      `min_samples_leaf`      创建叶子节点之前叶子节点必须拥有的最小样本数      `min_weight_fraction_leaf`      与`min_samples_leaf`相同，但以加权实例总数的分数表示      `min_impurity_decrease`      只有当此分割导致至少这种程度的纯度降低时才分割节点      `ccp_alpha`      控制**最小成本复杂度剪枝**（MCCP），即剪枝那些与叶子数相比没有足够减少纯度的子树；较大的`ccp_alpha`值会导致更多的剪枝，从而得到更小的树（默认值为0——不剪枝）    为了限制模型的复杂度并因此正则化模型，您可以增加`min_*`超参数或`ccp_alpha`，或减少`max_*`超参数。调整`max_depth`通常是一个好的默认值：它提供了有效的正则化，并保持树小且易于解释。设置`min_samples_leaf`也是一个好主意，特别是对于小型数据集。并且当处理高维数据集时，`max_features`非常出色。    ######
    注意    其他算法首先在不加限制的情况下训练决策树，然后*剪枝*（删除）不必要的节点。如果一个节点的所有子节点都是叶子节点，并且它提供的纯度改进在统计上不显著，则该节点被认为是多余的。标准统计测试，如**χ**²
    **测试**（卡方测试），用于估计改进纯粹是偶然结果（这被称为**零假设**）的概率。如果这个概率，称为**p值**，高于给定的阈值（通常为5%，由超参数控制），则该节点被认为是多余的，并且其子节点被删除。剪枝会继续进行，直到所有不必要的节点都被剪枝。    让我们在moons数据集上测试正则化：这是一个用于二元分类的玩具数据集，其中数据点呈两个交错的新月形状（参见[图5-3](#min_samples_leaf_plot)）。您可以使用`make_moons()`函数生成此数据集。    我们将训练一个不带正则化的决策树，另一个带有`min_samples_leaf=5`。以下是代码；[图5-3](#min_samples_leaf_plot)显示了每个树的决策边界：    [PRE6]  ![决策边界的比较：左图显示未正则化的决策树的复杂边界，而右图显示具有更平滑边界的正则化树，这表明可能具有更好的泛化能力。](assets/hmls_0503.png)  ######
    图5-3\. 未正则化树（左）和正则化树（右）的决策边界    左侧的未正则化模型显然是过拟合的，而右侧的正则化模型可能会更好地泛化。我们可以通过使用不同随机种子生成的测试集来验证这一点：    [PRE7]``
    `...` [PRE8] `0.898` `>>>` `tree_clf2``.``score``(``X_moons_test``,` `y_moons_test``)`
    `` `0.92` `` [PRE9]` [PRE10]   [PRE11]  [PRE12] [PRE13]`py [PRE14]``'
