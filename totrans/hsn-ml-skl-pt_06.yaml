- en: Chapter 5\. Decision Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Decision trees* are versatile machine learning algorithms that can perform
    both classification and regression tasks, and even multioutput tasks. They are
    powerful algorithms, capable of fitting complex datasets. For example, in [Chapter 2](ch02.html#project_chapter)
    you trained a `DecisionTreeRegressor` model on the California housing dataset,
    fitting it perfectly (actually, overfitting it).'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are also the fundamental components of random forests (see [Chapter 6](ch06.html#ensembles_chapter)),
    which are among the most powerful machine learning algorithms available today.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will start by discussing how to train, visualize, and make
    predictions with decision trees. Then we will go through the CART training algorithm
    used by Scikit-Learn, and we will explore how to regularize trees and use them
    for regression tasks. Finally, we will discuss some of the limitations of decision
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Visualizing a Decision Tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand decision trees, let’s build one and take a look at how it makes
    predictions. The following code trains a `DecisionTreeClassifier` on the iris
    dataset (see [Chapter 4](ch04.html#linear_models_chapter)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can visualize the trained decision tree by first using the `export_graphviz()`
    function to output a graph definition file called *iris_tree.dot*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can use `graphviz.Source.from_file()` to load and display the file
    in a Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Graphviz](https://graphviz.org) is an open source graph visualization software
    package. It also includes a `dot` command-line tool to convert *.dot* files to
    a variety of formats, such as PDF or PNG.'
  prefs: []
  type: TYPE_NORMAL
- en: Your first decision tree looks like [Figure 5-1](#iris_tree).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a decision tree for classifying iris species based on petal
    length and width, showing split nodes and leaf nodes with classification results
    for setosa, versicolor, and virginica.](assets/hmls_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Iris decision tree
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Making Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s see how the tree represented in [Figure 5-1](#iris_tree) makes predictions.
    Suppose you find an iris flower and you want to classify it based on its petals.
    You start at the *root node* (depth 0, at the top): this node asks whether the
    flower’s petal length is smaller than 2.45 cm. If it is, then you move down to
    the root’s left child node (depth 1, left). In this case, it is a *leaf node*
    (i.e., it does not have any child nodes), so it does not ask any questions: simply
    look at the predicted class for that node, and the decision tree predicts that
    your flower is an *Iris setosa* (`class=setosa`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now suppose you find another flower, and this time the petal length is greater
    than 2.45 cm. You again start at the root but now move down to its right child
    node (depth 1, right). This is not a leaf node, it’s a *split node*, so it asks
    another question: is the petal width smaller than 1.75 cm? If it is, then your
    flower is most likely an *Iris versicolor* (depth 2, left). If not, it is likely
    an *Iris virginica* (depth 2, right). It’s really that simple.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One of the many qualities of decision trees is that they require very little
    data preparation. In fact, they don’t require feature scaling or centering at
    all.
  prefs: []
  type: TYPE_NORMAL
- en: 'A node’s `samples` attribute counts how many training instances it applies
    to. For example, 100 training instances have a petal length greater than 2.45
    cm (depth 1, right), and of those 100, 54 have a petal width smaller than 1.75
    cm (depth 2, left). A node’s `value` attribute tells you how many training instances
    of each class this node applies to: for example, the bottom-right node applies
    to 0 *Iris setosa*, 1 *Iris versicolor*, and 45 *Iris virginica*. Finally, a node’s
    `gini` attribute measures its *Gini impurity*: a node is “pure” (`gini=0`) if
    all training instances it applies to belong to the same class. For example, since
    the depth-1 left node applies only to *Iris setosa* training instances, its Gini
    impurity is 0\. Conversely, the other nodes all apply to instances of multiple
    classes, so they are “impure”. [Equation 5-1](#gini_impurity) shows how the training
    algorithm computes the Gini impurity *G*[*i*] of the *i*^(th) node. The more classes
    and the more mixed they are, the larger the impurity. For example, the depth-2
    left node has a Gini impurity equal to 1 – (0/54)² – (49/54)² – (5/54)² ≈ 0.168.'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-1\. Gini impurity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $upper G Subscript i Baseline equals 1 minus sigma-summation Underscript k equals
    1 Overscript n Endscripts p Subscript i comma k Baseline Superscript 2$
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*G*[*i*] is the Gini impurity of the *i*^(th) node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p*[*i*,*k*] is the ratio of class *k* instances among the training instances
    in the *i*^(th) node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scikit-Learn uses the CART algorithm (discussed shortly), which produces only
    *binary trees*, meaning trees where split nodes always have exactly two children
    (i.e., questions only have yes/no answers). However, other algorithms, such as
    ID3, can produce decision trees with nodes that have more than two children.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-2](#decision_tree_decision_boundaries_plot) shows this decision tree’s
    decision boundaries. The thick vertical line represents the decision boundary
    of the root node (depth 0): petal length = 2.45 cm. Since the lefthand area is
    pure (only *Iris setosa*), it cannot be split any further. However, the righthand
    area is impure, so the depth-1 right node splits it at petal width = 1.75 cm (represented
    by the dashed line). Since `max_depth` was set to 2, the decision tree stops right
    there. If you set `max_depth` to 3, then the two depth-2 nodes would each add
    another decision boundary (represented by the two vertical dotted lines).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating decision tree decision boundaries with depth levels
    and data points for different Iris flower species.](assets/hmls_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Decision tree decision boundaries
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The tree structure, including all the information shown in [Figure 5-1](#iris_tree),
    is available via the classifier’s `tree_` attribute. Type **`help(tree_clf.tree_)`**
    for details, and see [this chapter’s notebook](https://homl.info/colab-p) for
    an example.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating Class Probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A decision tree can also estimate the probability that an instance belongs
    to a particular class *k*. First it traverses the tree to find the leaf node for
    this instance, and then it returns the proportion of instances of class *k* among
    the training instances that would also reach this leaf node. For example, suppose
    you have found a flower whose petals are 5 cm long and 1.5 cm wide. The corresponding
    leaf node is the depth-2 left node, so the decision tree outputs the following
    probabilities: 0% for *Iris setosa* (0/54), 90.7% for *Iris versicolor* (49/54),
    and 9.3% for *Iris virginica* (5/54). And if you ask it to predict the class,
    it outputs *Iris versicolor* (class 1) because it has the highest probability.
    Let’s check this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Perfect! Notice that the estimated probabilities would be identical anywhere
    else in the bottom-right rectangle of [Figure 5-2](#decision_tree_decision_boundaries_plot)—for
    example, if the petals were 6 cm long and 1.5 cm wide (even though it seems obvious
    that it would most likely be an *Iris virginica* in this case).
  prefs: []
  type: TYPE_NORMAL
- en: The CART Training Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scikit-Learn uses the *Classification and Regression Tree* (CART) algorithm
    to train decision trees (also called “growing” trees). The algorithm works by
    first splitting the training set into two subsets using a single feature *k* and
    a threshold *t*[*k*] (e.g., “petal length ≤ 2.45 cm”). How does it choose *k*
    and *t*[*k*]? It searches for the pair (*k*, *t*[*k*]) that produces the purest
    subsets, weighted by their size. [Equation 5-2](#classification_cart_cost_function)
    gives the cost function that the algorithm tries to minimize.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-2\. CART cost function for classification
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $StartLayout 1st Row 1st Column upper J left-parenthesis k comma t Subscript
    k Baseline right-parenthesis 2nd Column equals StartFraction m Subscript left
    Baseline Over m EndFraction upper G Subscript left Baseline plus StartFraction
    m Subscript right Baseline Over m EndFraction upper G Subscript right Baseline
    2nd Row 1st Column where 2nd Column StartLayout Enlarged left-brace 1st Row  upper
    G Subscript left slash right Baseline measures the impurity of the left slash
    right subset 2nd Row  m Subscript left slash right Baseline is the number of instances
    in the left slash right subset 3rd Row  m equals m Subscript left Baseline plus
    m Subscript right EndLayout EndLayout$
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the CART algorithm has successfully split the training set in two, it
    splits the subsets using the same logic, then the sub-subsets, and so on, recursively.
    It stops recursing once it reaches the maximum depth (defined by the `max_depth`
    hyperparameter), or if it cannot find a split that will reduce impurity. A few
    other hyperparameters (described in a moment) control additional stopping conditions:
    `min_samples_split`, `min_samples_leaf`, `max_leaf_nodes`, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see, the CART algorithm is a *greedy algorithm*: it greedily searches
    for an optimum split at the top level, then repeats the process at each subsequent
    level. It does not check whether the split will lead to the lowest possible impurity
    several levels down. A greedy algorithm often produces a solution that’s reasonably
    good but not guaranteed to be optimal.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, finding the optimal tree is known to be an *NP-complete* problem.⁠^([1](ch05.html#id1673))
    It requires *O*(exp(*m*)) time,⁠^([2](ch05.html#id1674)) making the problem intractable
    even for small training sets. This is why we must settle for a “reasonably good”
    solution when training decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Computational Complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making predictions requires traversing the decision tree from the root to a
    leaf. Decision trees are generally approximately balanced, so traversing the decision
    tree requires going through roughly *O*(log[2](*m*)) nodes, where *m* is the number
    of training instances, and log[2](*m*) is the *binary logarithm* of *m*, equal
    to log(*m*) / log(2). Since each node only requires checking the value of one
    feature, the overall prediction complexity is *O*(log[2](*m*)), independent of
    the number of features. So predictions are very fast, even when dealing with large
    training sets.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the training algorithm compares all features on all samples at each
    node, which results in a training complexity of *O*(*n* × *m* log[2](*m*)).
  prefs: []
  type: TYPE_NORMAL
- en: It’s possible to set a maximum tree depth using the `max_depth` hyperparameter,
    and/or set a maximum number of features to consider at each node (the features
    are then chosen randomly). Doing so will help speed up training considerably,
    and it can also reduce the risk of overfitting (but as always, going too far would
    result in underfitting).
  prefs: []
  type: TYPE_NORMAL
- en: Gini Impurity or Entropy?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, the `DecisionTreeClassifier` class uses the Gini impurity measure,
    but you can select the *entropy* impurity measure instead by setting the `criterion`
    hyperparameter to `"entropy"`. The concept of entropy originated in thermodynamics
    as a measure of molecular disorder: entropy approaches zero when molecules are
    still and well ordered. Entropy later spread to a wide variety of domains, including
    in Shannon’s information theory, where it measures the average information content
    of a message, as we saw in [Chapter 4](ch04.html#linear_models_chapter). Entropy
    is zero when all messages are identical. In machine learning, entropy is frequently
    used as an impurity measure: a set’s entropy is zero when it contains instances
    of only one class. [Equation 5-3](#entropy_function) shows the definition of the
    entropy of the *i*^(th) node. For example, the depth-2 left node in [Figure 5-1](#iris_tree)
    has an entropy equal to –(49/54) log[2] (49/54) – (5/54) log[2] (5/54) ≈ 0.445.'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-3\. Entropy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mrow><msub><mi>H</mi> <mi>i</mi></msub> <mo>=</mo> <mo>-</mo> <munderover><mo>∑</mo>
    <mfrac linethickness="0pt"><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mrow><msub><mi>p</mi>
    <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <mo>≠</mo><mn>0</mn></mrow></mfrac>
    <mi>n</mi></munderover> <mrow><msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%"
    rspace="0%">,</mo><mi>k</mi></mrow></msub> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mrow>
  prefs: []
  type: TYPE_NORMAL
- en: 'So, should you use Gini impurity or entropy? The truth is, most of the time
    it does not make a big difference: they lead to similar trees. Gini impurity is
    slightly faster to compute, so it is a good default. However, when they differ,
    Gini impurity tends to isolate the most frequent class in its own branch of the
    tree, while entropy tends to produce slightly more balanced trees.⁠^([3](ch05.html#id1691))'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees make very few assumptions about the training data (as opposed
    to linear models, which assume that the data is linear, for example). If left
    unconstrained, the tree structure will adapt itself to the training data, fitting
    it very closely—indeed, most likely overfitting it. Such a model is often called
    a *nonparametric model*, not because it does not have any parameters (it often
    has a lot) but because the number of parameters is not determined prior to training,
    so the model structure is free to stick closely to the data. In contrast, a *parametric
    model*, such as a linear model, has a predetermined number of parameters, so its
    degree of freedom is limited, reducing the risk of overfitting (but increasing
    the risk of underfitting).
  prefs: []
  type: TYPE_NORMAL
- en: To avoid overfitting the training data, you need to restrict the decision tree’s
    freedom during training. As you know by now, this is called regularization. The
    regularization hyperparameters depend on the algorithm used, but generally you
    can at least restrict the maximum depth of the decision tree. In Scikit-Learn,
    this is controlled by the `max_depth` hyperparameter. The default value is `None`,
    which means unlimited. Reducing `max_depth` will regularize the model and thus
    reduce the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DecisionTreeClassifier` class has a few other parameters that similarly
    restrict the shape of the decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum number of features that are evaluated for splitting at each node
  prefs: []
  type: TYPE_NORMAL
- en: '`max_leaf_nodes`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum number of leaf nodes
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_split`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of samples a node must have before it can be split
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_leaf`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of samples a leaf node must have to be created
  prefs: []
  type: TYPE_NORMAL
- en: '`min_weight_fraction_leaf`'
  prefs: []
  type: TYPE_NORMAL
- en: Same as `min_samples_leaf` but expressed as a fraction of the total number of
    weighted instances
  prefs: []
  type: TYPE_NORMAL
- en: '`min_impurity_decrease`'
  prefs: []
  type: TYPE_NORMAL
- en: Only split a node if this split results in at least this reduction in impurity
  prefs: []
  type: TYPE_NORMAL
- en: '`ccp_alpha`'
  prefs: []
  type: TYPE_NORMAL
- en: Controls *minimal cost-complexity pruning* (MCCP), i.e., pruning subtrees that
    don’t reduce impurity enough compared to their number of leaves; a larger `ccp_alpha`
    value leads to more pruning, resulting in a smaller tree (the default is 0—no
    pruning)
  prefs: []
  type: TYPE_NORMAL
- en: 'To limit the model’s complexity and thereby regularize the model, you can increase
    `min_*` hyperparameters or `ccp_alpha`, or decrease `max_*` hyperparameters. Tuning
    `max_depth` is usually a good default: it provides effective regularization, and
    it keeps the tree small and easy to interpret. Setting `min_samples_leaf` is also
    a good idea, especially for small datasets. And `max_features` is great when working
    with high-dimensional datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other algorithms work by first training the decision tree without restrictions,
    then *pruning* (deleting) unnecessary nodes. A node whose children are all leaf
    nodes is considered unnecessary if the purity improvement it provides is not statistically
    significant. Standard statistical tests, such as the *χ*² *test* (chi-squared
    test), are used to estimate the probability that the improvement is purely the
    result of chance (which is called the *null hypothesis*). If this probability,
    called the *p-value*, is higher than a given threshold (typically 5%, controlled
    by a hyperparameter), then the node is considered unnecessary and its children
    are deleted. The pruning continues until all unnecessary nodes have been pruned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test regularization on the moons dataset: this is a toy dataset for binary
    classification in which the data points are shaped as two interleaving crescent
    moons (see [Figure 5-3](#min_samples_leaf_plot)). You can generate this dataset
    using the `make_moons()` function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll train one decision tree without regularization, and another with `min_samples_leaf=5`.
    Here’s the code; [Figure 5-3](#min_samples_leaf_plot) shows the decision boundaries
    of each tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Comparison of decision boundaries: the left diagram shows an unregularized
    decision tree''s complex boundaries, while the right depicts a regularized tree
    with smoother boundaries indicating potentially better generalization.](assets/hmls_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. Decision boundaries of an unregularized tree (left) and a regularized
    tree (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The unregularized model on the left is clearly overfitting, and the regularized
    model on the right will probably generalize better. We can verify this by evaluating
    both trees on a test set generated using a different random seed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, the second tree has a better accuracy on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Decision trees are also capable of performing regression tasks. While linear
    regression only works well with linear data, decision trees can fit all sorts
    of complex datasets. Let’s build a regression tree using Scikit-Learn’s `DecisionTreeRegressor`
    class, training it on a noisy quadratic dataset with `max_depth=2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The resulting tree is represented in [Figure 5-4](#regression_tree).
  prefs: []
  type: TYPE_NORMAL
- en: '![A decision tree diagram for regression, showing how input variables are split
    at different nodes with squared error, sample size, and predicted value at each
    node.](assets/hmls_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. A decision tree for regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This tree looks very similar to the classification tree you built earlier. The
    main difference is that instead of predicting a class in each node, it predicts
    a value. For example, suppose you want to make a prediction for a new instance
    with *x*[1] = 0.2\. The root node asks whether *x*[1] ≤ 0.343\. Since it is, the
    algorithm goes to the left child node, which asks whether *x*[1] ≤ –0.302\. Since
    it is not, the algorithm goes to the right child node. This is a leaf node, and
    it predicts `value=0.038`. This prediction is the average target value of the
    133 training instances associated with this leaf node, and it results in a mean
    squared error equal to 0.002 over these 133 instances.
  prefs: []
  type: TYPE_NORMAL
- en: This model’s predictions are represented on the left in [Figure 5-5](#tree_regression_plot).
    If you set `max_depth=3`, you get the predictions represented on the right. Notice
    how the predicted value for each region is always the average target value of
    the instances in that region. The algorithm splits each region in a way that makes
    most training instances as close as possible to that predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: '![Two plots show decision tree regression predictions: the left with max depth
    2 having simpler splits, and the right with max depth 3 showing more detailed
    splits.](assets/hmls_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Predictions of two decision tree regression models
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The CART algorithm works as described earlier, except that instead of trying
    to split the training set in a way that minimizes impurity, it now tries to split
    the training set in a way that minimizes the MSE. [Equation 5-4](#regression_cart_cost_function)
    shows the cost function that the algorithm tries to minimize.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-4\. CART cost function for regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $upper J left-parenthesis k comma t Subscript k Baseline right-parenthesis equals
    StartFraction m Subscript left Baseline Over m EndFraction MSE Subscript left
    Baseline plus StartFraction m Subscript right Baseline Over m EndFraction MSE
    Subscript right Baseline where StartLayout Enlarged left-brace 1st Row  MSE Subscript
    node Baseline equals StartFraction sigma-summation Underscript i element-of node
    Endscripts left-parenthesis ModifyingAbove y With caret Subscript node Baseline
    minus y Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    squared Over m Subscript node Baseline EndFraction 2nd Row  ModifyingAbove y With
    caret Subscript node Baseline equals StartFraction sigma-summation Underscript
    i element-of node Endscripts y Superscript left-parenthesis i right-parenthesis
    Baseline Over m Subscript node Baseline EndFraction EndLayout$
  prefs: []
  type: TYPE_NORMAL
- en: Just like for classification tasks, decision trees are prone to overfitting
    when dealing with regression tasks. Without any regularization (i.e., using the
    default hyperparameters), you get the predictions on the left in [Figure 5-6](#tree_regression_regularization_plot).
    These predictions are obviously overfitting the training set very badly. Just
    setting `min_samples_leaf=10` results in a much more reasonable model, represented
    on the right in [Figure 5-6](#tree_regression_regularization_plot).
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparison of regression tree predictions showing overfitting with no restrictions
    and improved fit with `min_samples_leaf=10`.](assets/hmls_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. Predictions of an unregularized regression tree (left) and a regularized
    tree (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sensitivity to Axis Orientation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hopefully by now you are convinced that decision trees have a lot going for
    them: they are relatively easy to understand and interpret, simple to use, versatile,
    and powerful. However, they do have a few limitations. First, as you may have
    noticed, decision trees love orthogonal decision boundaries (all splits are perpendicular
    to an axis), which makes them sensitive to the data’s orientation. For example,
    [Figure 5-7](#sensitivity_to_rotation_plot) shows a simple linearly separable
    dataset: on the left, a decision tree can split it easily, while on the right,
    after the dataset is rotated by 45°, the decision boundary looks unnecessarily
    convoluted. Although both decision trees fit the training set perfectly, it is
    very likely that the model on the right will not generalize well.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing decision trees'' sensitivity to data orientation, with a
    clear boundary before rotation and a complex boundary after rotation by 45 degrees.](assets/hmls_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. Sensitivity to training set rotation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One way to limit this problem is to scale the data, then apply a principal component
    analysis (PCA) transformation. We will look at PCA in detail in [Chapter 7](ch07.html#dimensionality_chapter),
    but for now you only need to know that it rotates the data in a way that reduces
    the correlation between the features, which often (not always) makes things easier
    for trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a small pipeline that scales the data and rotates it using PCA,
    then train a `DecisionTreeClassifier` on that data. [Figure 5-8](#pca_preprocessing_plot)
    shows the decision boundaries of that tree: as you can see, the rotation makes
    it possible to fit the dataset pretty well using only one feature, *z*[1], which
    is a linear function of the original petal length and width. Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `DecisionTreeClassifier` and `DecisionTreeRegressor` classes both support
    missing values natively, no need for an imputer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating decision boundaries of a decision tree on the scaled
    and PCA-rotated iris dataset, with separate regions for Iris setosa, versicolor,
    and virginica.](assets/hmls_0508.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. A tree’s decision boundaries on the scaled and PCA-rotated iris
    dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Decision Trees Have a High Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More generally, the main issue with decision trees is that they have quite
    a high variance: small changes to the hyperparameters or to the data may produce
    very different models. In fact, since the training algorithm used by Scikit-Learn
    is stochastic—it randomly selects the set of features to evaluate at each node—even
    retraining the same decision tree on the exact same data may produce a very different
    model, such as the one represented in [Figure 5-9](#decision_tree_high_variance_plot)
    (unless you set the `random_state` hyperparameter). As you can see, it looks very
    different from the previous decision tree ([Figure 5-2](#decision_tree_decision_boundaries_plot)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing decision boundaries for petal width and length in a decision
    tree, illustrating high variance due to changes in depth and classification regions.](assets/hmls_0509.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-9\. Retraining the same model on the same data may produce a very different
    model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Luckily, by averaging predictions over many trees, it’s possible to reduce variance
    significantly. Such an *ensemble* of trees is called a *random forest*, and it’s
    one of the most powerful types of models available today, as you will see in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the approximate depth of a decision tree trained (without restrictions)
    on a training set with one million instances?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is a node’s Gini impurity generally lower or higher than its parent’s? Is it
    *generally* lower/higher, or *always* lower/higher?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a decision tree is overfitting the training set, is it a good idea to try
    decreasing `max_depth`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a decision tree is underfitting the training set, is it a good idea to try
    scaling the input features?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If it takes one hour to train a decision tree on a training set containing
    one million instances, roughly how much time will it take to train another decision
    tree on a training set containing ten million instances? Hint: consider the CART
    algorithm’s computational complexity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If it takes one hour to train a decision tree on a given training set, roughly
    how much time will it take if you double the number of features?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train and fine-tune a decision tree for the moons dataset by following these
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `make_moons(n_samples=10000, noise=0.4)` to generate a moons dataset.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `train_test_split()` to split the dataset into a training set and a test
    set.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use grid search with cross-validation (with the help of the `GridSearchCV`
    class) to find good hyperparameter values for a `DecisionTreeClassifier`. Hint:
    try various values for `max_leaf_nodes`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train it on the full training set using these hyperparameters, and measure your
    model’s performance on the test set. You should get roughly 85% to 87% accuracy.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Grow a forest by following these steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Continuing the previous exercise, generate 1,000 subsets of the training set,
    each containing 100 instances selected randomly. Hint: you can use Scikit-Learn’s
    `ShuffleSplit` class for this.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train one decision tree on each subset, using the best hyperparameter values
    found in the previous exercise. Evaluate these 1,000 decision trees on the test
    set. Since they were trained on smaller sets, these decision trees will likely
    perform worse than the first decision tree, achieving only about 80% accuracy.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now comes the magic. For each test set instance, generate the predictions of
    the 1,000 decision trees, and keep only the most frequent prediction (you can
    use SciPy’s `mode()` function for this). This approach gives you *majority-vote
    predictions* over the test set.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evaluate these predictions on the test set: you should obtain a slightly higher
    accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you
    have trained a random forest classifier!'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.html#id1673-marker)) P is the set of problems that can be solved
    in *polynomial time* (i.e., a polynomial of the dataset size). NP is the set of
    problems whose solutions can be verified in polynomial time. An NP-hard problem
    is a problem that can be reduced to a known NP-hard problem in polynomial time.
    An NP-complete problem is both NP and NP-hard. A major open mathematical question
    is whether P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm
    will ever be found for any NP-complete problem (except perhaps one day on a quantum
    computer).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch05.html#id1674-marker)) This *big O notation* means that as *m* (i.e.,
    the number of training instances) gets larger, the computation time becomes proportional
    to the exponential of *m* (it’s actually an upper bound, but we make it as small
    as we can). This tells us how “fast” the computation grows with *m*, and *O*(exp(*m*))
    is very fast.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.html#id1691-marker)) See Sebastian Raschka’s [interesting analysis](https://homl.info/19)
    for more details.
  prefs: []
  type: TYPE_NORMAL
