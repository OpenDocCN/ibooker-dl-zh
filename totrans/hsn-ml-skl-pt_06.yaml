- en: Chapter 5\. Decision Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Decision trees* are versatile machine learning algorithms that can perform
    both classification and regression tasks, and even multioutput tasks. They are
    powerful algorithms, capable of fitting complex datasets. For example, in [Chapter 2](ch02.html#project_chapter)
    you trained a `DecisionTreeRegressor` model on the California housing dataset,
    fitting it perfectly (actually, overfitting it).'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are also the fundamental components of random forests (see [Chapter 6](ch06.html#ensembles_chapter)),
    which are among the most powerful machine learning algorithms available today.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will start by discussing how to train, visualize, and make
    predictions with decision trees. Then we will go through the CART training algorithm
    used by Scikit-Learn, and we will explore how to regularize trees and use them
    for regression tasks. Finally, we will discuss some of the limitations of decision
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Visualizing a Decision Tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand decision trees, let’s build one and take a look at how it makes
    predictions. The following code trains a `DecisionTreeClassifier` on the iris
    dataset (see [Chapter 4](ch04.html#linear_models_chapter)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can visualize the trained decision tree by first using the `export_graphviz()`
    function to output a graph definition file called *iris_tree.dot*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can use `graphviz.Source.from_file()` to load and display the file
    in a Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Graphviz](https://graphviz.org) is an open source graph visualization software
    package. It also includes a `dot` command-line tool to convert *.dot* files to
    a variety of formats, such as PDF or PNG.'
  prefs: []
  type: TYPE_NORMAL
- en: Your first decision tree looks like [Figure 5-1](#iris_tree).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a decision tree for classifying iris species based on petal
    length and width, showing split nodes and leaf nodes with classification results
    for setosa, versicolor, and virginica.](assets/hmls_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Iris decision tree
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Making Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s see how the tree represented in [Figure 5-1](#iris_tree) makes predictions.
    Suppose you find an iris flower and you want to classify it based on its petals.
    You start at the *root node* (depth 0, at the top): this node asks whether the
    flower’s petal length is smaller than 2.45 cm. If it is, then you move down to
    the root’s left child node (depth 1, left). In this case, it is a *leaf node*
    (i.e., it does not have any child nodes), so it does not ask any questions: simply
    look at the predicted class for that node, and the decision tree predicts that
    your flower is an *Iris setosa* (`class=setosa`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now suppose you find another flower, and this time the petal length is greater
    than 2.45 cm. You again start at the root but now move down to its right child
    node (depth 1, right). This is not a leaf node, it’s a *split node*, so it asks
    another question: is the petal width smaller than 1.75 cm? If it is, then your
    flower is most likely an *Iris versicolor* (depth 2, left). If not, it is likely
    an *Iris virginica* (depth 2, right). It’s really that simple.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One of the many qualities of decision trees is that they require very little
    data preparation. In fact, they don’t require feature scaling or centering at
    all.
  prefs: []
  type: TYPE_NORMAL
- en: 'A node’s `samples` attribute counts how many training instances it applies
    to. For example, 100 training instances have a petal length greater than 2.45
    cm (depth 1, right), and of those 100, 54 have a petal width smaller than 1.75
    cm (depth 2, left). A node’s `value` attribute tells you how many training instances
    of each class this node applies to: for example, the bottom-right node applies
    to 0 *Iris setosa*, 1 *Iris versicolor*, and 45 *Iris virginica*. Finally, a node’s
    `gini` attribute measures its *Gini impurity*: a node is “pure” (`gini=0`) if
    all training instances it applies to belong to the same class. For example, since
    the depth-1 left node applies only to *Iris setosa* training instances, its Gini
    impurity is 0\. Conversely, the other nodes all apply to instances of multiple
    classes, so they are “impure”. [Equation 5-1](#gini_impurity) shows how the training
    algorithm computes the Gini impurity *G*[*i*] of the *i*^(th) node. The more classes
    and the more mixed they are, the larger the impurity. For example, the depth-2
    left node has a Gini impurity equal to 1 – (0/54)² – (49/54)² – (5/54)² ≈ 0.168.'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-1\. Gini impurity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $upper G Subscript i Baseline equals 1 minus sigma-summation Underscript k equals
    1 Overscript n Endscripts p Subscript i comma k Baseline Superscript 2$
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*G*[*i*] is the Gini impurity of the *i*^(th) node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p*[*i*,*k*] is the ratio of class *k* instances among the training instances
    in the *i*^(th) node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scikit-Learn uses the CART algorithm (discussed shortly), which produces only
    *binary trees*, meaning trees where split nodes always have exactly two children
    (i.e., questions only have yes/no answers). However, other algorithms, such as
    ID3, can produce decision trees with nodes that have more than two children.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-2](#decision_tree_decision_boundaries_plot) shows this decision tree’s
    decision boundaries. The thick vertical line represents the decision boundary
    of the root node (depth 0): petal length = 2.45 cm. Since the lefthand area is
    pure (only *Iris setosa*), it cannot be split any further. However, the righthand
    area is impure, so the depth-1 right node splits it at petal width = 1.75 cm (represented
    by the dashed line). Since `max_depth` was set to 2, the decision tree stops right
    there. If you set `max_depth` to 3, then the two depth-2 nodes would each add
    another decision boundary (represented by the two vertical dotted lines).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating decision tree decision boundaries with depth levels
    and data points for different Iris flower species.](assets/hmls_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Decision tree decision boundaries
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The tree structure, including all the information shown in [Figure 5-1](#iris_tree),
    is available via the classifier’s `tree_` attribute. Type **`help(tree_clf.tree_)`**
    for details, and see [this chapter’s notebook](https://homl.info/colab-p) for
    an example.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating Class Probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A decision tree can also estimate the probability that an instance belongs
    to a particular class *k*. First it traverses the tree to find the leaf node for
    this instance, and then it returns the proportion of instances of class *k* among
    the training instances that would also reach this leaf node. For example, suppose
    you have found a flower whose petals are 5 cm long and 1.5 cm wide. The corresponding
    leaf node is the depth-2 left node, so the decision tree outputs the following
    probabilities: 0% for *Iris setosa* (0/54), 90.7% for *Iris versicolor* (49/54),
    and 9.3% for *Iris virginica* (5/54). And if you ask it to predict the class,
    it outputs *Iris versicolor* (class 1) because it has the highest probability.
    Let’s check this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`` `Perfect! Notice that the estimated probabilities would be identical anywhere
    else in the bottom-right rectangle of [Figure 5-2](#decision_tree_decision_boundaries_plot)—for
    example, if the petals were 6 cm long and 1.5 cm wide (even though it seems obvious
    that it would most likely be an *Iris virginica* in this case).` ``  [PRE4]``
    [PRE5]` # The CART Training Algorithm    Scikit-Learn uses the *Classification
    and Regression Tree* (CART) algorithm to train decision trees (also called “growing”
    trees). The algorithm works by first splitting the training set into two subsets
    using a single feature *k* and a threshold *t*[*k*] (e.g., “petal length ≤ 2.45
    cm”). How does it choose *k* and *t*[*k*]? It searches for the pair (*k*, *t*[*k*])
    that produces the purest subsets, weighted by their size. [Equation 5-2](#classification_cart_cost_function)
    gives the cost function that the algorithm tries to minimize.    ##### Equation
    5-2\. CART cost function for classification  $StartLayout 1st Row 1st Column upper
    J left-parenthesis k comma t Subscript k Baseline right-parenthesis 2nd Column
    equals StartFraction m Subscript left Baseline Over m EndFraction upper G Subscript
    left Baseline plus StartFraction m Subscript right Baseline Over m EndFraction
    upper G Subscript right Baseline 2nd Row 1st Column where 2nd Column StartLayout
    Enlarged left-brace 1st Row  upper G Subscript left slash right Baseline measures
    the impurity of the left slash right subset 2nd Row  m Subscript left slash right
    Baseline is the number of instances in the left slash right subset 3rd Row  m
    equals m Subscript left Baseline plus m Subscript right EndLayout EndLayout$  Once
    the CART algorithm has successfully split the training set in two, it splits the
    subsets using the same logic, then the sub-subsets, and so on, recursively. It
    stops recursing once it reaches the maximum depth (defined by the `max_depth`
    hyperparameter), or if it cannot find a split that will reduce impurity. A few
    other hyperparameters (described in a moment) control additional stopping conditions:
    `min_samples_split`, `min_samples_leaf`, `max_leaf_nodes`, and more.    ######
    Warning    As you can see, the CART algorithm is a *greedy algorithm*: it greedily
    searches for an optimum split at the top level, then repeats the process at each
    subsequent level. It does not check whether the split will lead to the lowest
    possible impurity several levels down. A greedy algorithm often produces a solution
    that’s reasonably good but not guaranteed to be optimal.    Unfortunately, finding
    the optimal tree is known to be an *NP-complete* problem.⁠^([1](ch05.html#id1673))
    It requires *O*(exp(*m*)) time,⁠^([2](ch05.html#id1674)) making the problem intractable
    even for small training sets. This is why we must settle for a “reasonably good”
    solution when training decision trees.    # Computational Complexity    Making
    predictions requires traversing the decision tree from the root to a leaf. Decision
    trees are generally approximately balanced, so traversing the decision tree requires
    going through roughly *O*(log[2](*m*)) nodes, where *m* is the number of training
    instances, and log[2](*m*) is the *binary logarithm* of *m*, equal to log(*m*)
    / log(2). Since each node only requires checking the value of one feature, the
    overall prediction complexity is *O*(log[2](*m*)), independent of the number of
    features. So predictions are very fast, even when dealing with large training
    sets.    By default, the training algorithm compares all features on all samples
    at each node, which results in a training complexity of *O*(*n* × *m* log[2](*m*)).    It’s
    possible to set a maximum tree depth using the `max_depth` hyperparameter, and/or
    set a maximum number of features to consider at each node (the features are then
    chosen randomly). Doing so will help speed up training considerably, and it can
    also reduce the risk of overfitting (but as always, going too far would result
    in underfitting).    # Gini Impurity or Entropy?    By default, the `DecisionTreeClassifier`
    class uses the Gini impurity measure, but you can select the *entropy* impurity
    measure instead by setting the `criterion` hyperparameter to `"entropy"`. The
    concept of entropy originated in thermodynamics as a measure of molecular disorder:
    entropy approaches zero when molecules are still and well ordered. Entropy later
    spread to a wide variety of domains, including in Shannon’s information theory,
    where it measures the average information content of a message, as we saw in [Chapter 4](ch04.html#linear_models_chapter).
    Entropy is zero when all messages are identical. In machine learning, entropy
    is frequently used as an impurity measure: a set’s entropy is zero when it contains
    instances of only one class. [Equation 5-3](#entropy_function) shows the definition
    of the entropy of the *i*^(th) node. For example, the depth-2 left node in [Figure 5-1](#iris_tree)
    has an entropy equal to –(49/54) log[2] (49/54) – (5/54) log[2] (5/54) ≈ 0.445.    #####
    Equation 5-3\. Entropy  <mrow><msub><mi>H</mi> <mi>i</mi></msub> <mo>=</mo> <mo>-</mo>
    <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub>
    <mo>≠</mo><mn>0</mn></mrow></mfrac> <mi>n</mi></munderover> <mrow><msub><mi>p</mi>
    <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <mrow><mo>(</mo> <msub><mi>p</mi> <mrow><mi>i</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <mo>)</mo></mrow></mrow></mrow>  So,
    should you use Gini impurity or entropy? The truth is, most of the time it does
    not make a big difference: they lead to similar trees. Gini impurity is slightly
    faster to compute, so it is a good default. However, when they differ, Gini impurity
    tends to isolate the most frequent class in its own branch of the tree, while
    entropy tends to produce slightly more balanced trees.⁠^([3](ch05.html#id1691))    #
    Regularization Hyperparameters    Decision trees make very few assumptions about
    the training data (as opposed to linear models, which assume that the data is
    linear, for example). If left unconstrained, the tree structure will adapt itself
    to the training data, fitting it very closely—indeed, most likely overfitting
    it. Such a model is often called a *nonparametric model*, not because it does
    not have any parameters (it often has a lot) but because the number of parameters
    is not determined prior to training, so the model structure is free to stick closely
    to the data. In contrast, a *parametric model*, such as a linear model, has a
    predetermined number of parameters, so its degree of freedom is limited, reducing
    the risk of overfitting (but increasing the risk of underfitting).    To avoid
    overfitting the training data, you need to restrict the decision tree’s freedom
    during training. As you know by now, this is called regularization. The regularization
    hyperparameters depend on the algorithm used, but generally you can at least restrict
    the maximum depth of the decision tree. In Scikit-Learn, this is controlled by
    the `max_depth` hyperparameter. The default value is `None`, which means unlimited.
    Reducing `max_depth` will regularize the model and thus reduce the risk of overfitting.    The
    `DecisionTreeClassifier` class has a few other parameters that similarly restrict
    the shape of the decision tree:    `max_features`      Maximum number of features
    that are evaluated for splitting at each node      `max_leaf_nodes`      Maximum
    number of leaf nodes      `min_samples_split`      Minimum number of samples a
    node must have before it can be split      `min_samples_leaf`      Minimum number
    of samples a leaf node must have to be created      `min_weight_fraction_leaf`      Same
    as `min_samples_leaf` but expressed as a fraction of the total number of weighted
    instances      `min_impurity_decrease`      Only split a node if this split results
    in at least this reduction in impurity      `ccp_alpha`      Controls *minimal
    cost-complexity pruning* (MCCP), i.e., pruning subtrees that don’t reduce impurity
    enough compared to their number of leaves; a larger `ccp_alpha` value leads to
    more pruning, resulting in a smaller tree (the default is 0—no pruning)      To
    limit the model’s complexity and thereby regularize the model, you can increase
    `min_*` hyperparameters or `ccp_alpha`, or decrease `max_*` hyperparameters. Tuning
    `max_depth` is usually a good default: it provides effective regularization, and
    it keeps the tree small and easy to interpret. Setting `min_samples_leaf` is also
    a good idea, especially for small datasets. And `max_features` is great when working
    with high-dimensional datasets.    ###### Note    Other algorithms work by first
    training the decision tree without restrictions, then *pruning* (deleting) unnecessary
    nodes. A node whose children are all leaf nodes is considered unnecessary if the
    purity improvement it provides is not statistically significant. Standard statistical
    tests, such as the *χ*² *test* (chi-squared test), are used to estimate the probability
    that the improvement is purely the result of chance (which is called the *null
    hypothesis*). If this probability, called the *p-value*, is higher than a given
    threshold (typically 5%, controlled by a hyperparameter), then the node is considered
    unnecessary and its children are deleted. The pruning continues until all unnecessary
    nodes have been pruned.    Let’s test regularization on the moons dataset: this
    is a toy dataset for binary classification in which the data points are shaped
    as two interleaving crescent moons (see [Figure 5-3](#min_samples_leaf_plot)).
    You can generate this dataset using the `make_moons()` function.    We’ll train
    one decision tree without regularization, and another with `min_samples_leaf=5`.
    Here’s the code; [Figure 5-3](#min_samples_leaf_plot) shows the decision boundaries
    of each tree:    [PRE6]  ![Comparison of decision boundaries: the left diagram
    shows an unregularized decision tree''s complex boundaries, while the right depicts
    a regularized tree with smoother boundaries indicating potentially better generalization.](assets/hmls_0503.png)  ######
    Figure 5-3\. Decision boundaries of an unregularized tree (left) and a regularized
    tree (right)    The unregularized model on the left is clearly overfitting, and
    the regularized model on the right will probably generalize better. We can verify
    this by evaluating both trees on a test set generated using a different random
    seed:    [PRE7]`` `...` [PRE8] `0.898` `>>>` `tree_clf2``.``score``(``X_moons_test``,`
    `y_moons_test``)` `` `0.92` `` [PRE9]` [PRE10]   [PRE11]  [PRE12] [PRE13]`py [PRE14]``'
  prefs: []
  type: TYPE_NORMAL
