<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">9 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/><a id="idTextAnchor006"/><a id="idTextAnchor007"/><a id="idTextAnchor008"/>A line-by-line implementation of attention and Transformer</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="marker-194"/><a id="idIndexMarker002"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">The architecture and functionalities of encoders and decoders in Transformers</li>
<li class="co-summary-bullet">How the attention mechanism uses query, key, and value to assign weights to elements in a sequence</li>
<li class="co-summary-bullet">Different types of Transformers</li>
<li class="co-summary-bullet">Building a Transformer from scratch for language translation</li>
</ul>
<p class="body">Transformers are advanced deep learning models that excel in handling sequence-to-sequence prediction challenges, outperforming older models like recurrent neural networks (RNNs) and convolutional neural networks (CNNs). Their strength lies in effectively understanding the relationships between elements in input and output sequences over long distances, such as two words far apart in the text. Unlike RNNs, Transformers are capable of parallel training, significantly cutting down training times and enabling the handling of vast datasets. This transformative architecture has been pivotal in the development of large language models (LLMs) like ChatGPT, BERT, and T5, marking a significant milestone in AI progress.<a id="idIndexMarker003"/><a id="idIndexMarker004"/><a id="idIndexMarker005"/><a id="idIndexMarker006"/><a id="idIndexMarker007"/></p>
<p class="body">Prior to the introduction of Transformers in the groundbreaking 2017 paper “Attention Is All You Need” by a group of Google researchers,<sup class="footnotenumber" id="footnote-001-backlink"><a class="url1" href="#footnote-001">1</a></sup> natural language processing (NLP) and similar tasks primarily relied on RNNs, including long short-term memory (LSTM) models. RNNs, however, process information sequentially, limiting their speed due to the inability to train in parallel and struggling with maintaining information about earlier parts of a sequence, thus failing to capture long-term dependencies.<a id="idIndexMarker008"/><a id="idIndexMarker009"/><a id="idIndexMarker010"/><a id="marker-195"/></p>
<p class="body">The revolutionary aspect of the Transformer architecture is its attention mechanism. This mechanism assesses the relationship between words in a sequence by assigning weights, determining the degree of relatedness in meaning among words based on the training data. This enables models like ChatGPT to comprehend relationships between words, thus understanding human language more effectively. The nonsequential processing of inputs allows for parallel training, reducing training time and facilitating the use of large datasets, thereby powering the rise of knowledgeable LLMs and the current surge in AI advancements.</p>
<p class="body">In this chapter, we will implement, line by line, the creation of a Transformer from the ground up, based on the paper “Attention Is All You Need.” The Transformer, once trained, can handle translations between any two languages (such as German to English or English to Chinese). In the next chapter, we’ll focus on training the Transformer developed here to perform English to French translations.</p>
<p class="body">To build the Transformer from scratch, we’ll explore the inner workings of the self-attention mechanism, including the roles of query, key, and value vectors, and the computation of scaled dot product attention (SDPA). We’ll construct an encoder layer by integrating layer normalization and residual connection into a multihead attention layer and combining it with a feed-forward layer. We’ll then stack six of these encoder layers to form the encoder. Similarly, we’ll develop a decoder in the Transformer that is capable of generating translation one token at a time, based on previous tokens in the translation and the encoder’s output.<a id="idIndexMarker011"/></p>
<p class="body">This groundwork will equip you to train the Transformer for translations between any two languages. In the next chapter, you’ll learn to train the Transformer using a dataset containing more than 47,000 English-to-French translations. You’ll witness the trained model translating common English phrases to French with an accuracy comparable to using Google Translate.</p>
<h2 class="fm-head" id="heading_id_3">9.1 Introduction to attention and Transformer</h2>
<p class="body">To grasp the concept of Transformers in machine learning, it’s essential to first understand the attention mechanism. This mechanism allows Transformers to recognize long-range dependencies between sequence elements, a feature that sets them apart from earlier sequence prediction models like RNNs. With this mechanism, Transformers can simultaneously focus on every element in a sequence, comprehending the context of each word.<a id="idIndexMarker012"/><a id="idIndexMarker013"/></p>
<p class="body">Consider the word “bank” to illustrate how the attention mechanism interprets words based on context. In the sentence “I went fishing by the river yesterday, remaining near the bank the whole afternoon,” the word ”bank“ is linked to “fishing” because it refers to the area beside a river. Here, a Transformer understands “bank” as part of the river’s terrain.</p>
<p class="body">By contrast, in “Kate went to the bank after work yesterday and deposited a check there,” “bank” is connected to “check,” leading the Transformer to identify “bank” as a financial institution. This example showcases how Transformers discern word meanings based on their surrounding context.</p>
<p class="body">In this section, you’ll dive deeper into the attention mechanism, exploring how it works. This process is crucial for determining the importance, or weights, of various words within a sentence. After that, we’ll examine the structure of different Transformer models, including one that can translate between any two languages.</p>
<h3 class="fm-head1" id="heading_id_4">9.1.1 The attention mechanism</h3>
<p class="body">The attention mechanism is a method used to determine the interconnections between elements in a sequence. It calculates scores to indicate how one element relates to others in the sequence, with higher scores denoting a stronger relationship. In NLP, this mechanism is instrumental in linking words within a sentence meaningfully. This chapter will guide you through implementing the attention mechanism for language translation.<a id="idIndexMarker014"/><a id="idIndexMarker015"/><a id="marker-196"/></p>
<p class="body">We’ll construct a Transformer composed of an encoder and a decoder for that purpose. We’ll then train the Transformer to translate English to French in the next chapter. The encoder transforms an English sentence, such as “How are you?”, into vector representations that capture its meaning. The decoder then uses these vector representations to generate the French translation.</p>
<p class="body">To transform the phrase “How are you?” into vector representations, the model first breaks it down into tokens <code class="fm-code-in-text">[how, are, you, ?]</code>, a process similar to what you have done in chapter 8. These tokens are each represented by a 256-dimensional vector known as word embeddings, which capture the meaning of each token. The encoder also employs positional encoding, a method to determine the positions of tokens in the sequence. This positional encoding is added to the word embeddings to create input embeddings, which are then used to calculate self-attention. The input embedding for “How are you?” forms a tensor with dimensions (4, 256), where 4 represents the number of tokens and 256 is the dimensionality of each embedding.</p>
<p class="body">While there are different ways to calculate attention, we’ll use the most common method, SDPA. This mechanism is also called self-attention because the algorithm calculates how a word attends to all words in the sequence, including the word itself. Figure 9.1 provides a diagram of how to calculate SDPA.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="551" src="../../OEBPS/Images/CH09_F01_Liu.png" width="698"/></p>
<p class="figurecaption">Figure 9.1 A diagram of the self-attention mechanism. To calculate attention, the input embedding <span class="times">X</span> is first passed through three neural layers with weights, <span class="times">W<sup class="fm-superscript">Q</sup>, W<sup class="fm-superscript">K</sup></span>, and <span class="times">W<sup class="fm-superscript">V</sup></span>, respectively. The outputs are query <span class="times">Q</span>, key <span class="times">K</span>, and value <span class="times">V</span>. The scaled attention score is the product of <span class="times">Q</span> and <span class="times">K</span> divided by the square root of the dimension of <span class="times">K, d<sub class="fm-subscript">k</sub></span>. We apply the softmax function on the scaled attention score to obtain the attention weight. The attention is the product of the attention weight and value V.</p>
</div>
<p class="body"><a id="marker-197"/>The utilization of query, key, and value in calculating attention is inspired by retrieval systems. Consider visiting a public library to find a book. If you search for “machine learning in finance” in the library’s search engine, this phrase becomes your query. The book titles and descriptions in the library serve as the keys. Based on the similarity between your query and these keys, the library’s retrieval system suggests a list of books (values). Books containing “machine learning,” “finance,” or both in their titles or descriptions are likely to rank higher. In contrast, books unrelated to these terms will have a lower matching score and thus are less likely to be recommended.</p>
<p class="body">To calculate SDPA, the input embedding <span class="times">X</span> is processed through three distinct neural network layers. The corresponding weights for these layers are <span class="times">W<sup class="fm-superscript">Q</sup></span>, <span class="times">W<sup class="fm-superscript">K</sup></span>, and <span class="times">W<sup class="fm-superscript">V</sup></span>; each has a dimension of <span class="times">256 <span class="cambria">×</span> 256</span>. These weights are learned from data during the training phase. Thus, we can calculate query <span class="times">Q</span>, key <span class="times">K</span>, and value <span class="times">V</span> as <span class="times">Q = X * W<sup class="fm-superscript">Q</sup>, K = X * Q<sup class="fm-superscript">K</sup></span>, and <span class="times">V = X * W<sup class="fm-superscript">V</sup></span>. The dimensions of <span class="times">Q, K</span>, and <span class="times">V</span> match those of the input embedding <span class="times">X</span>, which are <span class="times">4 <span class="cambria">×</span> 256</span>.</p>
<p class="body">Similar to the retrieval system example we mentioned earlier, in the attention mechanism, we assess the similarities between the query and key vectors using the SDPA approach. SDPA involves calculating the dot product of the query (<span class="times">Q</span>) and key (<span class="times">K</span>) vectors. A high dot product indicates a strong similarity between the two vectors and vice versa. For instance, in the sentence “How are you?”, the scaled attention score is computed as follows:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="90%"/>
<col class="contenttable-0-col" span="1" width="10%"/>
</colgroup>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre4" height="48" src="../../OEBPS/Images/CH09_F01_Liu_EQ01.png" width="239"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="fm-equation-caption">(9.1)</p>
</td>
</tr>
</tbody>
</table>
<p class="body"><a id="marker-198"/>where <span class="times">d<sub class="fm-subscript">k</sub></span> represents the dimension of the key vector <span class="times">K</span>, which in our case is 256. We scale the dot product of <span class="times">Q</span> and <span class="times">K</span> by the square root of <span class="times">d<sub class="fm-subscript">k</sub></span> to stabilize training. This scaling is done to prevent the dot product from growing too large in magnitude. The dot product between the query and key vectors can become very large when the dimension of these vectors (i.e., the depth of the embedding) is high. This is because each element of the query vector is multiplied by each element of the key vector, and these products are then summed up.</p>
<p class="body">The next step is to apply the softmax function to these attention scores, converting them into attention weights. This ensures that the total attention a word gives to all words in the sentence sums to 100%.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="320" src="../../OEBPS/Images/CH09_F02_Liu.png" width="611"/></p>
<p class="figurecaption">Figure 9.2 Steps to calculate attention weights. The input embedding is passed through two neural networks to obtain query Q and key K. The scaled attention scores are calculated as the dot product of <span class="times">Q</span> and <span class="times">K</span> divided by the square root of the dimension of K. Finally, we apply the softmax function on the scaled attention scores to obtain attention weights, which demonstrate how each element is related to all other elements in the sequence.</p>
</div>
<p class="body">Figure 9.2 shows how this is done. For the sentence “How are you?”, the attention weights form a <span class="times">4 <span class="cambria">×</span> 4</span> matrix, which shows how each token in <code class="fm-code-in-text">["How", "are," "you," "?"]</code> is related to all other tokens (including itself). The numbers in figure 9.2 are made-up numbers to illustrate the point. For example, the first row in the attention weights shows that the token <code class="fm-code-in-text">"How"</code> gives 10% of its attention to itself and 40%, 40%, and 10% to the other three tokens, respectively.</p>
<p class="body">The final attention is then calculated as the dot product of these attention weights and the value vector V (also illustrated in figure 9.3):</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="90%"/>
<col class="contenttable-0-col" span="1" width="10%"/>
</colgroup>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre4" height="56" src="../../OEBPS/Images/CH09_F02_Liu_EQ02.png" width="354"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="fm-equation-caption">(9.2)</p>
</td>
</tr>
</tbody>
</table>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="266" src="../../OEBPS/Images/CH09_F03_Liu.png" width="722"/></p>
<p class="figurecaption">Figure 9.3 Use attention weights and the value vector to calculate the attention vector. The input embedding is passed through a neural network to obtain value <span class="times">V</span>. The final attention is the dot product of the attention weights that we calculated earlier and the value vector <span class="times">V</span>.</p>
</div>
<p class="body">This output also maintains a dimension of <span class="times">4 <span class="cambria">×</span> 256</span>, consistent with our input dimensions.</p>
<p class="body"><a id="marker-199"/>To summarize, the process begins with the input embedding X of the sentence “How are you?”, which has a dimension of <span class="times">4 <span class="cambria">×</span> 256</span>. This embedding captures the meanings of the four individual tokens but lacks contextualized understanding. The attention mechanism ends with the output <code class="fm-code-in-text">attention(Q,K,V)</code>, which maintains the same dimension of <span class="times">4 <span class="cambria">×</span> 256</span>. This output can be viewed as a contextually enriched combination of the original four tokens. The weighting of the original tokens varies based on the contextual relevance of each token, granting more significance to words that are more important within the sentence’s context. Through this procedure, the attention mechanism transforms vectors representing isolated tokens into vectors imbued with contextualized meanings, thereby extracting a richer, more nuanced understanding from the sentence.</p>
<p class="body">Further, instead of using one set of query, key, and value vectors, Transformer models use a concept called multihead attention. For example, the 256-dimensional query, key, and value vectors can be split into say, 8, heads, and each head has a set of query, key, and value vectors with dimensions of 32 (because <span class="times">256/8 = 32</span>). Each head pays attention to different parts or aspects of the input, enabling the model to capture a broader range of information and form a more detailed and contextual understanding of the input data. Multihead attention is especially useful when a word has multiple meanings in a sentence, such as in a pun. Let’s continue the “bank” example we mentioned earlier. Consider the pun joke, “Why is the river so rich? Because it has two banks.” In the project of translating English to French in the next chapter, you’ll implement first-hand splitting <span class="times">Q</span>, <span class="times">K</span>, and <span class="times">V</span> into multiple heads to calculate attention in each head before concatenating them back into one single attention vector. <a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="idIndexMarker018"/></p>
<h3 class="fm-head1" id="heading_id_5">9.1.2 The Transformer architecture</h3>
<p class="body"><a id="marker-200"/>The concept of the attention mechanism was introduced by Bahdanau, Cho, and Bengio in 2014.<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">2</a></sup> It became widely used after the groundbreaking paper “Attention Is All You Need,” which focused on creating a model for machine language translation. The architecture of this model, known as the Transformer, is depicted in figure 9.4. It features an encoder-decoder structure that relies heavily on the attention mechanism. In this chapter, you’ll build this model from scratch, coding it line by line, intending to train it for translation between any two languages. <a id="idIndexMarker019"/><a id="idIndexMarker020"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="582" src="../../OEBPS/Images/CH09_F04_Liu.png" width="584"/></p>
<p class="figurecaption">Figure 9.4 The Transformer architecture. The encoder in the Transformer (left side of the diagram), which consists of N identical encoder layers, learns the meaning of the input sequence and converts it into vectors that represent its meaning. It then passes these vectors to the decoder (right side of the diagram), which consists of N identical decoder layers. The decoder constructs the output (e.g., the French translation of an English phrase) by predicting one token at a time, based on previous tokens in the sequence and vector representations from the encoder. The generator on the top right is the head attached to the output from the decoder so that the output is the probability distribution over all tokens in the target language (e.g., the French vocabulary).</p>
</div>
<p class="body">Let’s use English-to-French translation as our example. The Transformer’s encoder transforms an English sentence like “I don’t speak French” into vector representations that store its meaning. The Transformer’s decoder then processes them to produce the French translation “Je ne parle pas français.” The encoder’s role is to capture the essence of the original English sentence. For instance, if the encoder is effective, it should translate both “I don’t speak French” and “I do not speak French” into similar vector representations. Consequently, the decoder will interpret these vectors and generate similar translations. Interestingly, when using ChatGPT, these two English phrases indeed result in the same French translation.</p>
<p class="body">The encoder in the Transformer approaches the task by first tokenizing both the English and French sentences. This is similar to the process described in chapter 8 but with a key difference: it employs subword tokenization. Subword tokenization is a technique used in NLP to break words into smaller components, or subwords, allowing for more efficient and nuanced processing. For example, as you’ll see in the next chapter, the English phrase “I do not speak French” is divided into six tokens: (<code class="fm-code-in-text">i, do, not, speak, fr, ench</code>). Similarly, its French counterpart “Je ne parle pas français” is tokenized into six parts: (<code class="fm-code-in-text">je, ne, parle, pas, franc, ais</code>). This method of tokenization enhances the Transformer’s ability to handle language variations and complexities.</p>
<p class="body">Deep learning models, including Transformers, can’t directly process text, so tokens are indexed using integers before being fed to the model. These tokens are typically first represented using one-hot encoding, as we discussed in chapter 8. We then pass them through a word embedding layer to compress them into vectors with continuous values of a much smaller size, such as a length of 256. Thus, after applying word embedding, the sentence “I do not speak French” is represented by a <span class="times">6 <span class="cambria">×</span> 256 matrix</span>.</p>
<p class="body"><a id="marker-201"/>Transformers process input data such as sentences in parallel, unlike RNNs, which handle data sequentially. This parallelism enhances their efficiency but doesn’t inherently allow them to recognize the sequence order of the input. To address this, Transformers add positional encodings to the input embeddings. These positional encodings are unique vectors assigned to each position in the input sequence and align in dimension with the input embeddings. The vector values are determined by a specific positional function, particularly involving sine and cosine functions of varying frequencies, defined as</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="90%"/>
<col class="contenttable-0-col" span="1" width="10%"/>
</colgroup>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre4" height="46" src="../../OEBPS/Images/CH09_F04_Liu_EQ03.png" width="328"/></p>
</div>
</td>
<td class="contenttable-0-td"/>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre4" height="46" src="../../OEBPS/Images/CH09_F04_Liu_EQ04.png" width="360"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="fm-equation-caption">(9.3)</p>
</td>
</tr>
</tbody>
</table>
<p class="body">In these equations, vectors are calculated using the sine function for even indexes and the cosine function for odd indexes. The two parameters <i class="fm-italics">pos</i> and <i class="fm-italics">i</i> represent the position of a token within the sequence and the index within the vector, respectively. As an illustration, consider the positional encoding for the phrase “I do not speak French.” This is depicted as a <span class="times">6 <span class="cambria">×</span> 256</span> matrix, the same size as the word embedding for the sentence. Here, <i class="fm-italics">pos</i> ranges from 0 to 5, and the indexes <span class="times">2i</span> and <span class="times">2i + 1</span> collectively span 256 distinct values (from 0 to 255). A beneficial aspect of this positional encoding approach is that all values are constrained wit<a id="idTextAnchor009"/>hin the range of <span class="times">–1</span> to <span class="times">1</span>.</p>
<p class="body">It’s important to note that each token position is uniquely identified by a 256-dimensional vector, and these vector values remain constant throughout training. Before being input to the attention layers, these positional encodings are added to the word embeddings of the sequence. In the example of the sentence “I do not speak French,” the encoder generates both word embedding and positional encoding, each having dimensions of <span class="times">6 <span class="cambria">×</span> 256</span>, before combining them into a single <span class="times">6 <span class="cambria">×</span> 256</span>-dimensional representation. Subsequently, the encoder applies the attention mechanism to refine this embedding into more sophisticated vector representations that capture the overall meaning of the phrase, before passing them to the decoder.</p>
<p class="body"><a id="marker-202"/>The Transformer’s encoder, as depicted in figure 9.5, is made up of six identical layers <span class="times">(N = 6)</span>. Each of these layers comprises two distinct sublayers. The first sublayer is a multihead self-attention layer, similar to what was discussed earlier. The second sublayer is a basic, position-wise, fully connected feed-forward network. This network treats each position in the sequence independently rather than as sequential elements. In the model’s architecture, each sublayer incorporates layer normalization and a residual connection. Layer normalization normalizes observations to have zero mean and unit standard deviation. Such normalization helps stabilize the training process. After the normalization layer, we perform the residual connection. This means the input to each sublayer is added to its output, enhancing the flow of information through the network.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="489" src="../../OEBPS/Images/CH09_F05_Liu.png" width="740"/></p>
<p class="figurecaption">Figure 9.5 The structure of the encoder in the Transformer. The encoder consists of <span class="times">N = 6</span> identical encoder layers. Each encoder layer contains two sublayers. The first sublayer is a multihead self-attention layer and the second is a feed-forward network. Each sublayer uses layer normalization and residual connection.</p>
</div>
<p class="body">The decoder of the Transformer model, as seen in figure 9.6, is comprised of six identical decoder layers <span class="times">(N = 6)</span>. Each of these decoder layers features three sublayers: a multihead self-attention sublayer, a sublayer that performs multihead cross attention between the output from the first sublayer and the encoder’s output, and a feed-forward sublayer. Note that the input to each sublayer is the output from the previous sublayer. Further, the second sublayer in the decoder layer also takes the output from the encoder as input. This design is crucial for integrating information from the encoder: this is how the decode generates translations based on the output from the encoder.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="625" src="../../OEBPS/Images/CH09_F06_Liu.png" width="746"/></p>
<p class="figurecaption">Figure 9.6 The structure of the decoder in the Transformer. The decoder consists of <span class="times">N = 6</span> identical decoder layers. Each decoder layer contains three sublayers. The first sublayer is a masked multihead self-attention layer. The second is a multihead cross-attention layer to calculate the cross attention between the output from the first sublayer and the output from the encoder. The third sublayer is a feed-forward network. Each sublayer uses layer normalization and residual connection.</p>
</div>
<p class="body"><a id="marker-203"/>A key aspect of the decoder’s self-attention sublayer is the masking mechanism. This mask prevents the model from accessing future positions in the sequence, ensuring that predictions for a particular position can only depend on previously known elements. This sequential dependency is vital for tasks like language translation or text generation.</p>
<p class="body">The decoding process begins with the decoder receiving an input phrase in French. The decoder transforms the French tokens into word embeddings and positional encodings before combining them into a single embedding. This step ensures that the model not only understands the semantic content of the phrase but also maintains the sequential context, which is crucial for accurate translation or generation tasks.</p>
<p class="body">The decoder operates in an autoregressive manner, generating the output sequence one token at a time. At the first time step, it starts with the <code class="fm-code-in-text">"BOS"</code> token, which indicates the beginning of a sentence. Using this start token as its initial input, the decoder examines vector representations of the English phrase “I do not speak French” and attempts to predict the first token following <code class="fm-code-in-text">"BOS"</code>. Suppose the decoder’s first prediction is <code class="fm-code-in-text">"Je"</code>. In the next time step, it then uses the sequence <code class="fm-code-in-text">"BOS Je"</code> as its new input to predict the following token. This process continues iteratively, with the decoder adding each newly predicted token to its input sequence for the subsequent prediction. <a id="idIndexMarker021"/></p>
<p class="body">The translation process is designed to conclude when the decoder predicts the <code class="fm-code-in-text">"EOS"</code> token, signifying the end of the sentence. When preparing for the training data, we add EOS to the end of each phrase, so the model has learned that it means the end of a sentence. Upon reaching this token, the decoder recognizes the completion of the translation task and ceases its operation. This autoregressive approach ensures that each step in the decoding process is informed by all previously predicted tokens, allowing for coherent and contextually appropriate translations.<a id="idIndexMarker022"/><a id="idIndexMarker023"/><a id="marker-204"/><a id="idIndexMarker024"/></p>
<h3 class="fm-head1" id="heading_id_6">9.1.3 Different types of Transformers</h3>
<p class="body">There are three types of Transformers: encoder-only Transformers, decoder-only Transformers, and encoder-decoder Transformers. We are using an encoder-decoder Transformer in this chapter and the next, but you’ll get a chance to explore firsthand decoder-only Transformers later in the book. <a id="idIndexMarker025"/><a id="idIndexMarker026"/></p>
<p class="body">An encoder-only Transformer consists of N identical encoder layers as shown on the left side of figure 9.4 and is capable of converting a sequence into abstract continuous vector representations. For example, BERT is an encoder-only Transformer that contains 12 encoder layers. An encoder-only Transformer can be used for text classification, for example. If two sentences have similar vector representations, we can classify the two sentences into one category. On the other hand, if two sequences have very different vector representations, we can put them in different categories.</p>
<p class="body">A decoder-only Transformer also consists of N identical layers, and each layer is a decoder layer as shown on the right side of figure 9.4. For example, ChatGPT is a decoder-only Transformer that contains many decoder layers. The decoder-only Transformer can generate text based on a prompt, for example. It extracts the semantic meaning of the words in the prompt and predicts the most likely next token. It then adds the token to the end of the prompt and repeats the process until the text reaches a certain length.</p>
<p class="body">The machine language translation Transformer we discussed earlier is an example of an encoder-decoder Transformer. They are needed for handling complicated tasks, such as text-to-image generation or speech recognition. Encoder-decoder Transformers combine the strengths of both encoders and decoders. Encoders are efficient in processing and understanding input data, while decoders excel in generating output. This combination allows the model to effectively understand complex inputs (like text or speech) and generate intricate outputs (like images or transcribed text).<a id="idIndexMarker027"/><a id="idIndexMarker028"/></p>
<h2 class="fm-head" id="heading_id_7">9.2 Building an encoder</h2>
<p class="body">We’ll develop and train an encoder-decoder Transformer designed for machine language translation. The coding in this project is adapted from the work of Chris Cui in translating Chinese to English (<a class="url" href="https://mng.bz/9o1o">https://mng.bz/9o1o</a>) and Alexander Rush’s German-to-English translation project (<a class="url" href="https://mng.bz/j0mp">https://mng.bz/j0mp</a>). <a id="idIndexMarker029"/><a id="idIndexMarker030"/><a id="marker-205"/><a id="idIndexMarker031"/></p>
<p class="body">This section discusses how to construct an encoder in the Transformer. Specifically, we’ll dive into the process of building various sublayers within each encoder layer and implementing the multihead self-attention mechanism.</p>
<h3 class="fm-head1" id="heading_id_8">9.2.1 The attention mechanism</h3>
<p class="body">While there are different attention mechanisms, we’ll use the SDPA because it’s widely used and effective. The SDPA attention mechanism uses query, key, and value to calculate the relationships among elements in a sequence. It assigns scores to show how an element is related to all elements in a sequence (including the element itself). <a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/></p>
<p class="body">Instead of using one set of query, key, and value vectors, the Transformer model uses a concept called multihead attention. Our 256-dimensional query, key, and value vectors are split into 8 heads, and each head has a set of query, key, and value vectors with dimensions of 32 (because <span class="times">256/8 = 32</span>). Each head pays attention to different parts or aspects of the input, enabling the model to capture a broader range of information and form a more detailed and contextual understanding of the input data. For example, multihead attention allows the model to capture the multiple meanings of the word “bank” in the pun joke, “Why is the river so rich? Because it has two banks.”</p>
<p class="body">To implement this, we define an <code class="fm-code-in-text">attention()</code> function in the local module ch09util. Download the file ch09util.py from the book’s GitHub repository (<a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>) and store it in the /utils/ directory on your computer. The attention() function is defined as shown in the following listing.<a id="idIndexMarker035"/></p>
<p class="fm-code-listing-caption">Listing 9.1 Calculating attention based on query, key, and value</p>
<pre class="programlisting">def attention(query, key, value, mask=None, dropout=None):
    d_k = query.size(-1)
    scores = torch.matmul(query, 
              key.transpose(-2, -1)) / math.sqrt(d_k)       <span class="fm-combinumeral">①</span>
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)        <span class="fm-combinumeral">②</span>
    p_attn = nn.functional.softmax(scores, dim=-1)          <span class="fm-combinumeral">③</span>
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn              <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Scaled attention score is the dot product of query and key, scaled by the square root of d<sub class="fm-subscript1">k</sub>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> If there is a mask, hides future elements in the sequence</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates attention weights</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Returns both attention and attention weights</p>
<p class="body">The <code class="fm-code-in-text">attention()</code> function takes query, key, and value as inputs and calculates attention and attention weights as we discussed earlier in this chapter. The scaled attention score is the dot product of query and key, scaled by the square root of the dimension of the key, <i class="timesitalic">d<sub class="fm-subscript">k</sub></i>. We apply the softmax function on the scaled attention score to obtain attention weights. Finally, attention is calculated as the dot product of attention weights and value. <a id="idIndexMarker036"/></p>
<p class="body">Let’s use our running example to show how multihead attention works (see figure 9.7). The embedding for “How are you?” is a tensor with a size of <span class="times">(1, 6, 256)</span>, as we explained in the last section (after we add positional encoding to word embedding). Note that 1 means there is one sentence in the batch, and there are six tokens in the sentence instead of four because we add BOS and EOS to the beginning and the end of the sequence. This embedding is passed through three linear layers to obtain query <span class="times">Q</span>, key <span class="times">K</span>, and value <span class="times">V</span>, each of the same size <span class="times">(1, 6, 256)</span>. These are divided into eight heads, resulting in eight distinct sets of <span class="times">Q, K</span>, and <span class="times">V</span>, now sized <span class="times">(1, 6, 256/8 = 32)</span> each. The attention function, as defined earlier, is applied to each of these sets, yielding eight attention outputs, each also sized <span class="times">(1, 6, 32)</span>. We then concatenate the eight attention outputs into one single attention, and the result is a tensor with a size of <span class="times">(1, 6, 32 <span class="cambria">×</span> 8 = 256)</span>. Finally, this combined attention passes through another linear layer sized <span class="times">256 <span class="cambria">×</span> 256</span>, leading to the output from the <code class="fm-code-in-text">MultiHeadAttention()</code> class. This output maintains the original input’s dimensions, which are <span class="times">(1, 6, 256)</span>.<a id="idIndexMarker037"/><a id="marker-206"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="527" src="../../OEBPS/Images/CH09_F07_Liu.png" width="681"/></p>
<p class="figurecaption">Figure 9.7 An example of multihead attention. This diagram uses the calculation of the multihead self attention for the phrase “How are you?” as an example. We first pass the embedding through three neural networks to obtain query <span class="times">Q</span>, key <span class="times">K</span>, and value <span class="times">V</span>, each with a size of <span class="times">(1, 6, 256)</span>. We split them into eight heads, each with a set of <span class="times">Q, k</span>, and <span class="times">V</span>, with a size of <span class="times">(1, 6, 32)</span>. We calculate the attention in each head. The attention vectors from the eight heads are then joined back into one single attention vector, with a size of <span class="times">(1, 6, 256)</span>.</p>
</div>
<p class="body">This is implemented in the following code listing in the local module.</p>
<p class="fm-code-listing-caption">Listing 9.2 Calculating multihead attention</p>
<pre class="programlisting">from copy import deepcopy
class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        super().__init__()
        assert d_model % h == 0
        self.d_k = d_model // h
        self.h = h
        self.linears = nn.ModuleList([deepcopy(
            nn.Linear(d_model, d_model)) for i in range(4)])
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)
  
    def forward(self, query, key, value, mask=None):
        if mask is not None:
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)  
        query, key, value = [l(x).view(nbatches, -1, self.h,
           self.d_k).transpose(1, 2)    
         for l, x in zip(self.linears, (query, key, value))]    <span class="fm-combinumeral">①</span>
        x, self.attn = attention(
            query, key, value, mask=mask, dropout=self.dropout) <span class="fm-combinumeral">②</span>
        x = x.transpose(1, 2).contiguous().view(
            nbatches, -1, self.h * self.d_k)                    <span class="fm-combinumeral">③</span>
        output = self.linears[-1](x)                            <span class="fm-combinumeral">④</span>
        return output</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Passes input through three linear layers to obtain Q, K, V, and splits them into multiheads</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Calculates attention and attention weights for each head</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Concatenates attention vectors from multiheads into one single attention vector</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Passes the output through a linear layer</p>
<p class="body"><a id="marker-207"/>Each encoder layer and decoder layer also contain a feed-forward sublayer, which is a two-layer fully connected neural network, with the purpose of enhancing the model’s ability to capture and learn intricate features in the training dataset. Further, the neural network processes each embedding independently. It doesn’t treat the sequence of embeddings as a single vector. Therefore, we often call it a position-wide feed-forward network (or a 1D convolutional network). For that purpose, we define a <code class="fm-code-in-text">PositionwiseFeedForward()</code> class in the local module as follows:<a id="idIndexMarker038"/></p>
<pre class="programlisting">class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        h1 = self.w_1(x)
        h2 = self.dropout(h1)
        return self.w_2(h2) </pre>
<p class="body">The <code class="fm-code-in-text">PositionwiseFeedForward()</code> class is defined with two key parameters: <code class="fm-code-in-text">d_ff</code>, the dimensionality of the feed-forward layer, and <code class="fm-code-in-text">d_model</code>, representing the model’s dimension size. Typically, <code class="fm-code-in-text">d_ff</code> is chosen to be four times the size of <code class="fm-code-in-text">d_model</code>. In our example, <code class="fm-code-in-text">d_model</code> is 256, and we therefore set <code class="fm-code-in-text">d_ff</code> to <span class="times">256 * 4 = 1024</span>. This practice of enlarging the hidden layer in comparison to the model size is a standard approach in Transformer architectures. It enhances the network’s ability to capture and learn intricate features in the training dataset.<a id="idIndexMarker039"/><a id="idIndexMarker040"/><a id="idIndexMarker041"/><a id="idIndexMarker042"/></p>
<h3 class="fm-head1" id="heading_id_9">9.2.2 Creating an encoder</h3>
<p class="body"><a id="marker-208"/>To create an encoder layer, we first define the following <code class="fm-code-in-text">EncoderLayer()</code> class and <code class="fm-code-in-text">SublayerConnection()</code> class.<a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="idIndexMarker045"/><a id="idIndexMarker046"/><a id="idIndexMarker047"/></p>
<p class="fm-code-listing-caption">Listing 9.3 A class to define an encoder layer</p>
<pre class="programlisting">class EncoderLayer(nn.Module):
    def __init__(self, size, self_attn, feed_forward, dropout):
        super().__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = nn.ModuleList([deepcopy(
        SublayerConnection(size, dropout)) for i in range(2)])
        self.size = size  
    def forward(self, x, mask):
        x = self.sublayer[0](
            x, lambda x: self.self_attn(x, x, x, mask))     <span class="fm-combinumeral">①</span>
        output = self.sublayer[1](x, self.feed_forward)     <span class="fm-combinumeral">②</span>
        return output
class SublayerConnection(nn.Module):
    def __init__(self, size, dropout):
        super().__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x, sublayer):
        output = x + self.dropout(sublayer(self.norm(x)))   <span class="fm-combinumeral">③</span>
        return output  </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The first sublayer in each encoder layer is a multihead self-attention network.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The second sublayer in each encoder layer is a feed-forward network.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Each sublayer goes through residual connection and layer normalization.</p>
<p class="body">Each encoder layer is composed of two distinct sublayers: one is a multihead self-attention layer, as outlined in the <code class="fm-code-in-text">MultiHeadAttention()</code> class, and the other is a straightforward, position-wise, fully connected feed-forward network, as specified in the <code class="fm-code-in-text">PositionwiseFeedForward()</code> class. Additionally, both of these sublayers incorporate layer normalization and residual connections. As explained in chapter 6, a residual connection involves passing the input through a sequence of transformations (either the attention or the feed-forward layer in this context) and then adding the input back to these transformations’ output. The method of residual connection is employed to combat the problem of vanishing gradients, which is a common challenge in very deep networks. Another benefit of residual connections in Transformers is to provide a passage to pass the positional encodings (which are calculated only before the first layer) to subsequent layers. <a id="idIndexMarker048"/><a id="idIndexMarker049"/></p>
<p class="body">Layer normalization is somewhat similar to the batch normalization we implemented in chapter 4. It standardizes the observations in a layer to have a zero mean and a unit standard deviation. To achieve this within the local module, we define the <code class="fm-code-in-text">LayerNorm()</code> class, which executes layer normalization as follows:<a id="idIndexMarker050"/></p>
<pre class="programlisting">class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        super().__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps
    def forward(self, x):
        mean = x.mean(-1, keepdim=True) 
        std = x.std(-1, keepdim=True)
        x_zscore = (x - mean) / torch.sqrt(std ** 2 + self.eps)
        output = self.a_2*x_zscore+self.b_2
        return output </pre>
<p class="body">The <code class="fm-code-in-text">mean</code> and <code class="fm-code-in-text">std</code> values in the preceding <code class="fm-code-in-text">LayerNorm()</code> class are the mean and standard deviation of the inputs in each layer. The <code class="fm-code-in-text">a_2</code> and <code class="fm-code-in-text">b_2</code> layers in the <code class="fm-code-in-text">LayerNorm()</code> class expand <code class="fm-code-in-text">x_zscore</code> back to the shape of the input <code class="fm-code-in-text">x</code>. <a id="idIndexMarker051"/><a id="idIndexMarker052"/><a id="marker-209"/></p>
<p class="body">We can now create an encoder by stacking six encoder layers together. For that purpose, we define the <code class="fm-code-in-text">Encoder()</code> class in the local module:<a id="idIndexMarker053"/></p>
<pre class="programlisting">from copy import deepcopy
class Encoder(nn.Module):
    def __init__(self, layer, N):
        super().__init__()
        self.layers = nn.ModuleList(
            [deepcopy(layer) for i in range(N)])
        self.norm = LayerNorm(layer.size)
    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        <a id="idTextAnchor010"/>    output = self.norm(x)
        return output</pre>
<p class="body">Here, the <code class="fm-code-in-text">Encoder()</code> class is defined with two arguments: <code class="fm-code-in-text">layer</code>, which is an encoder layer as defined in the <code class="fm-code-in-text">EncoderLayer()</code> class in listing 9.3, and <code class="fm-code-in-text">N</code>, the number of encoder layers in the encoder. The <code class="fm-code-in-text">Encoder()</code> class takes input <code class="fm-code-in-text">x</code> (for example, a batch of English phrases) and the mask (to mask out sequence padding, as I’ll explain in chapter 10) to generate output (vector representations that capture the meanings of the English phrases). <a id="idIndexMarker054"/><a id="idIndexMarker055"/><a id="idIndexMarker056"/></p>
<p class="body">With that, you have created an encoder. Next, you’ll learn how to create a decoder. <a id="idIndexMarker057"/><a id="idIndexMarker058"/><a id="idIndexMarker059"/><a id="idIndexMarker060"/><a id="idIndexMarker061"/><a id="idIndexMarker062"/></p>
<h2 class="fm-head" id="heading_id_10">9.3 Building an encoder-decoder Transformer</h2>
<p class="body">Now that you understand how to create an encoder in the Transformer, let’s move on to the decoder. You’ll first learn how to create a decoder layer in this section. You’ll then stack <span class="times">N = 6</span> identical decoder layers to form a decoder. <a id="idIndexMarker063"/><a id="idIndexMarker064"/><a id="marker-210"/></p>
<p class="body">We then create an encoder-decoder transformer with five components: <code class="fm-code-in-text">encoder</code>, <code class="fm-code-in-text">decoder</code>, <code class="fm-code-in-text">src_embed, tgt_embed</code>, and <code class="fm-code-in-text">generator</code>, which I’ll explain in this section.</p>
<h3 class="fm-head1" id="heading_id_11">9.3.1 Creating a decoder layer</h3>
<p class="body">Each decoder layer consists of three sublayers: (1) a multihead self-attention layer, (2) the cross attention between the output from the first sublayer and the encoder’s output, and (3) a feed-forward network. Each of these three sublayers incorporates a layer normalization and the residual connection, similar to what we have done in encoder layers. Furthermore, the decoder stack’s multihead self-attention sublayer is masked to prevent positions from attending to subsequent positions. The mask forces the model to use previous elements in a sequence to predict later elements. I’ll explain how masked multihead self-attention works in a moment. To implement this, we define the <code class="fm-code-in-text">DecoderLayer()</code> class in the local module.<a id="idIndexMarker065"/><a id="idIndexMarker066"/><a id="idIndexMarker067"/></p>
<p class="fm-code-listing-caption">Listing 9.4 Creating a decoder layer</p>
<pre class="programlisting">class DecoderLayer(nn.Module):
    def __init__(self, size, self_attn, src_attn,
                 feed_forward, dropout):
        super().__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = nn.ModuleList([deepcopy(
        SublayerConnection(size, dropout)) for i in range(3)])
    def forward(self, x, memory, src_mask, tgt_mask):
        x = self.sublayer[0](x, lambda x: 
                 self.self_attn(x, x, x, tgt_mask))             <span class="fm-combinumeral">①</span>
        x = self.sublayer[1](x, lambda x:
                 self.src_attn(x, memory, memory, src_mask))    <span class="fm-combinumeral">②</span>
        output = self.sublayer[2](x, self.feed_forward)         <span class="fm-combinumeral">③</span>
        return output</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The first sublayer is a masked multihead self-attention layer.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The second sublayer is a cross-attention layer between the target language and the source language.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The third sublayer is a feed-forward network.</p>
<p class="body">To illustrate the operation of a decoder layer, let’s consider our ongoing example. The decoder takes in tokens <code class="fm-code-in-text">['BOS', 'comment', 'et', 'es-vous', '?']</code>, along with the output from the encoder (referred to as <code class="fm-code-in-text">memory</code> in the preceding code block), to predict the sequence <code class="fm-code-in-text">['comment', 'et', 'es-vous', '?', 'EOS']</code>. The embedding of <code class="fm-code-in-text">['BOS', 'comment', 'et', 'es-vous', '?']</code> is a tensor of size <span class="times">(1, 5, 256): 1</span> is the number of sequences in the batch, 5 is the number of tokens in the sequence, and 256 means each token is represented by a 256-value vector. We pass this embedding through the first sublayer, a masked multihead self-attention layer. This process is similar to the multihead self-attention calculation you saw earlier in the encoder layer. However, the process utilizes a mask, designated as <code class="fm-code-in-text">tgt_mask</code> in the preceding code block, which is a <span class="times">5 <span class="cambria">×</span> 5</span> tensor with the following values in the ongoing example:<a id="idIndexMarker068"/></p>
<pre class="programlisting">tensor([[ True, False, False, False, False],
        [ True,  True, False, False, False],
        [ True,  True,  True, False, False],
        [ True,  True,  True,  True, False],
        [ True,  True,  True,  True,  True]], device='cuda:0')</pre>
<p class="body"><a id="marker-211"/>As you may have noticed, the lower half of the mask (values below the main diagonal in the tensor) is turned on as <code class="fm-code-in-text">True</code>, and the upper half of the mask (values above the main diagonal) is turned off as <code class="fm-code-in-text">False</code>. When this mask is applied to the attention scores, it results in the first token attending only to itself during the first time step. In the second time step, attention scores are calculated exclusively between the first two tokens. As the process continues, for exa<a id="idTextAnchor011"/>mple, in the third time step, the decoder uses tokens <code class="fm-code-in-text">['BOS', 'comment', 'et']</code> to predict the token <code class="fm-code-in-text">'es-vous'</code>, and the attention scores are computed only among these three tokens, effectively hiding the future tokens <code class="fm-code-in-text">['es-vous', '?']</code></p>
<p class="body">Following this process, the output generated from the first sublayer, which is a tensor of size <span class="times">(1, 5, 256)</span>, matches the input’s size. This output, which we can refer to as <span class="times">x</span>, is then fed into the second sublayer. Here, cross attention is computed between x and the output of the encoder stack, termed <code class="fm-code-in-text">memory</code>. You may remember that <code class="fm-code-in-text">memory</code> has a dimension of <span class="times">(1, 6, 256)</span> since the English phrase “How are you?” is converted to six tokens <code class="fm-code-in-text">['BOS', 'how', 'are', 'you', '?', 'EOS']</code>.</p>
<p class="body">Figure 9.8 shows how cross-attention weights are calculated. To calculate the cross attention between x and <code class="fm-code-in-text">memory</code>, we first pass x through a neural network to obtain query, which has a dimension of <span class="times">(1, 5, 256)</span>. We then pass <code class="fm-code-in-text">memory</code> through two neural networks to obtain key and value, each having a dimension of <span class="times">(1, 6, 256)</span>. The scaled attention score is calculated using the formula as specified in equation 9.1. This scaled attention score has a dimension of <span class="times">(1, 5, 6):</span> the query <span class="times">Q</span> has a dimension of <span class="times">(1, 5, 256)</span> and the transposed key <span class="times">K</span> has a dimension of <span class="times">(1, 256, 6)</span>. Therefore, the scaled attention score, which is the dot product of the two, scaled by <span class="times">√<i class="timesitalic">d<sub class="fm-subscript">k</sub></i></span>, has a size of <span class="times">(1, 5, 6)</span>. After applying the softmax function to the scaled attention score, we obtain attention weights, which is a <span class="times">5 <span class="cambria">×</span> 6</span> matrix. This matrix tells us how the five tokens in the French input <code class="fm-code-in-text">['BOS', 'comment', 'et', 'es-vous', '?']</code> attend to the six tokens in the English phrase <code class="fm-code-in-text">['BOS', 'how', 'are', 'you', '?', 'EOS']</code>. This is how the decoder captures the meaning of the English phrase when translating. <a id="idIndexMarker069"/><a id="idIndexMarker070"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="359" src="../../OEBPS/Images/CH09_F08_Liu.png" width="748"/></p>
<p class="figurecaption">Figure 9.8 An example of how cross-attention weights are calculated between the input to the decoder and the output from the encoder. The input to the decoder is passed through a neural network to obtain query <span class="times">Q</span>. The output from the encoder is passed through a different neural network to obtain key <span class="times">K</span>. The scaled cross-attention scores are calculated as the dot product of <span class="times">Q</span> and <span class="times">K</span> divided by the square root of the dimension of <span class="times">K</span>. Finally, we apply the softmax function on the scaled cross-attention scores to obtain the cross-attention weights, which demonstrate how each element in <span class="times">Q</span> is related to all elements in <span class="times">K</span>.</p>
</div>
<p class="body">The final cross attention in the second sublayer is then calculated as the dot product of attention weights and the value vector <span class="times">V</span>. The attention weights have a dimension of <span class="times">(1, 5, 6)</span> and the value vector has a dimension of <span class="times">(1, 6, 256)</span>, so the final cross attention, which is the dot product of the two, has a size of <span class="times">(1, 5, 256)</span>. Therefore, the input and output of the second sublayer have the same dimension of <span class="times">(1, 5, 256)</span>. After processing through this second sublayer, the output is then directed through the third sublayer, which is a feed-forward network.<a id="marker-212"/></p>
<h3 class="fm-head1" id="heading_id_12">9.3.2 Creating an encoder-decoder Transformer</h3>
<p class="body">The decoder consists of N = 6 identical decoder layers. <a id="idIndexMarker071"/><a id="idIndexMarker072"/></p>
<p class="body">The <code class="fm-code-in-text">Decoder()</code> class is defined in the local module as follows:<a id="idIndexMarker073"/></p>
<pre class="programlisting">class Decoder(nn.Module):
    def __init__(self, layer, N):
        super().__init__()
        self.layers = nn.ModuleList(
            [deepcopy(layer) for i in range(N)])
        self.norm = LayerNorm(layer.size)
    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        output = self.norm(x)
        return output</pre>
<p class="body">To create an encoder-decoder transformer, we first define a <code class="fm-code-in-text">Transformer()</code> class in the local module. Open the file ch09util.py, and you’ll see the definition of the class as shown in the following listing.<a id="idIndexMarker074"/></p>
<p class="fm-code-listing-caption">Listing 9.5 A class to represent an encoder-decoder Transformer</p>
<pre class="programlisting">class Transformer(nn.Module):
    def __init__(self, encoder, decoder,
                 src_embed, tgt_embed, generator):
        super().__init__()
        self.encoder = encoder                                <span class="fm-combinumeral">①</span>
        self.decoder = decoder                                <span class="fm-combinumeral">②</span>
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator
    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)
    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), 
                            memory, src_mask, tgt_mask)
    def forward(self, src, tgt, src_mask, tgt_mask):
        memory = self.encode(src, src_mask)                   <span class="fm-combinumeral">③</span>
        output = self.decode(memory, src_mask, tgt, tgt_mask) <span class="fm-combinumeral">④</span>
        return output</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines the encoder in the Transformer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines the decoder in the Transformer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Source language is encoded into abstract vector representations.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The decoder uses these vector representations to generate translation in the target language.</p>
<p class="body">The <code class="fm-code-in-text">Transformer()</code> class is constructed with five key components: <code class="fm-code-in-text">encoder</code>, <code class="fm-code-in-text">decoder</code>, <code class="fm-code-in-text">src_embed</code>, <code class="fm-code-in-text">tgt_embed</code>, and <code class="fm-code-in-text">generator</code>. The encoder and decoder are represented by the <code class="fm-code-in-text">Encoder()</code> and <code class="fm-code-in-text">Decoder()</code> classes defined previously. In the next chapter, you’ll learn to generate the source language embedding: we’ll process numerical representations of English phrases using word embedding and positional encoding, combining the results to form the <code class="fm-code-in-text">src_embed</code> component. Similarly, for the target language, we process numerical representations of French phrases in the same manner, using the combined output as the <code class="fm-code-in-text">tgt_embed</code> component. The generator produces predicted probabilities for each index that corresponds to the tokens in the target language. We’ll define a <code class="fm-code-in-text">Generator()</code> class in the next section for this purpose.<a id="idIndexMarker075"/><a id="idIndexMarker076"/><a id="idIndexMarker077"/><a id="idIndexMarker078"/><a id="idIndexMarker079"/><a id="idIndexMarker080"/><a id="idIndexMarker081"/><a id="marker-213"/><a id="idIndexMarker082"/></p>
<h2 class="fm-head" id="heading_id_13">9.4 Putting all the pieces together</h2>
<p class="body">In this section, we’ll put all the pieces together to create a model that can translate between any two languages. <a id="idIndexMarker083"/><a id="idIndexMarker084"/></p>
<h3 class="fm-head1" id="heading_id_14">9.4.1 Defining a generator</h3>
<p class="body">First, we define a <code class="fm-code-in-text">Generator()</code> class in the local module to generate the probability distribution of the next token (see figure 9.9). The idea is to attach a head to the decoder for downstream tasks. In our example in the next chapter, the downstream task is to predict the next token in the French translation. <a id="idIndexMarker085"/><a id="idIndexMarker086"/><a id="idIndexMarker087"/><a id="idIndexMarker088"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="299" src="../../OEBPS/Images/CH09_F09_Liu.png" width="370"/></p>
<p class="figurecaption">Figure 9.9 The structure of the generator in the Transformer. The generator converts the output from the decoder stack to a probability distribution over the target language’s vocabulary, so that the Transformer can use the distribution to predict the next token in the French translation of an English phrase. The generator contains a linear layer so that the number of outputs is the same as the number of tokens in the French vocabulary. The generator also applies a softmax activation to the output so that the output is a probability distribution.</p>
</div>
<p class="body">The class is defined as follows:</p>
<pre class="programlisting">class Generator(nn.Module):
    def __init__(self, d_model, vocab):
        super().__init__()
        self.proj = nn.Linear(d_model, vocab)
  
    def forward(self, x):
        out = self.proj(x)
        probs = nn.functional.log_softmax(out, dim=-1)
        return probs  </pre>
<p class="body">The <code class="fm-code-in-text">Generator()</code> class produces predicted probabilities for each index that corresponds to the tokens in the target language. This enables the model to sequentially predict tokens in an autoregressive manner, utilizing previously generated tokens and the encoder’s output.<a id="idIndexMarker089"/></p>
<h3 class="fm-head1" id="heading_id_15">9.4.2 Creating a model to translate between two languages</h3>
<p class="body">Now we are ready to create a Transformer model to translate between any two languages (e.g., English to French or Chinese to English). The <code class="fm-code-in-text">create_model()</code> function defined in the local module accomplishes that.<a id="idIndexMarker090"/><a id="idIndexMarker091"/><a id="idIndexMarker092"/><a id="marker-214"/></p>
<p class="fm-code-listing-caption">Listing 9.6 Creating a Transformer to translate between two languages</p>
<pre class="programlisting">def create_model(src_vocab, tgt_vocab, N, d_model,
                 d_ff, h, dropout=0.1):
    attn=MultiHeadedAttention(h, d_model).to(DEVICE)
    ff=PositionwiseFeedForward(d_model, d_ff, dropout).to(DEVICE)
    pos=PositionalEncoding(d_model, dropout).to(DEVICE)
    model = Transformer(
        Encoder(EncoderLayer(d_model,deepcopy(attn),deepcopy(ff),
                             dropout).to(DEVICE),N).to(DEVICE),  <span class="fm-combinumeral">①</span>
        Decoder(DecoderLayer(d_model,deepcopy(attn),
             deepcopy(attn),deepcopy(ff), dropout).to(DEVICE),
                N).to(DEVICE),                                   <span class="fm-combinumeral">②</span>
        nn.Sequential(Embeddings(d_model, src_vocab).to(DEVICE),
                      deepcopy(pos)),                            <span class="fm-combinumeral">③</span>
        nn.Sequential(Embeddings(d_model, tgt_vocab).to(DEVICE),
                      deepcopy(pos)),                            <span class="fm-combinumeral">④</span>
        Generator(d_model, tgt_vocab)).to(DEVICE)                <span class="fm-combinumeral">⑤</span>
    for p in model.parameters():
        if p.dim() &gt; 1:
            nn.init.xavier_uniform_(p)    
    return model.to(DEVICE)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates an encoder by instantiating the Encoder() class</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a decoder by instantiating the Decoder() class</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates src_embed by passing source language through word embedding and positional encoding</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Creates tgt_embed by passing target language through word embedding and positional encoding</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates a generator by instantiating the Generator() class</p>
<p class="body">The primary element of the <code class="fm-code-in-text">create_model()</code> function is the <code class="fm-code-in-text">Transformer()</code> class, which was previously defined. Recall that the <code class="fm-code-in-text">Transformer()</code> class is built with five essential elements: <code class="fm-code-in-text">encoder</code>, <code class="fm-code-in-text">decoder</code>, <code class="fm-code-in-text">src_embed</code>, <code class="fm-code-in-text">tgt_embed</code>, and <code class="fm-code-in-text">generator</code>. Within the create_model() function, we sequentially construct these five co<a id="idTextAnchor012"/>mponents, using the recently defined <code class="fm-code-in-text">Encoder()</code>, <code class="fm-code-in-text">Decoder()</code>, and <code class="fm-code-in-text">Generator()</code> classes. In the next chapter, we’ll discuss in detail how to generate the source and target language embeddings, <code class="fm-code-in-text">src_embed</code> and <code class="fm-code-in-text">tgt_embed</code>.<a id="idIndexMarker093"/><a id="idIndexMarker094"/><a id="idIndexMarker095"/><a id="idIndexMarker096"/><a id="idIndexMarker097"/><a id="idIndexMarker098"/><a id="idIndexMarker099"/><a id="idIndexMarker100"/><a id="idIndexMarker101"/><a id="idIndexMarker102"/><a id="marker-215"/><a id="idIndexMarker103"/></p>
<p class="body">In the next chapter, you’ll apply the Transformer you created here to English-to-French translation. You’ll train the model using more than 47,000 pairs of English-to-French translations. You’ll then use the trained model to translate common English phrases into French. <a id="idIndexMarker104"/><a id="idIndexMarker105"/></p>
<h2 class="fm-head" id="heading_id_16">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">Transformers are advanced deep-learning models that excel in handling sequence-to-sequence prediction challenges. Their strength lies in effectively understanding the relationships between elements in input and output sequences over long distances.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The revolutionary aspect of the Transformer architecture is its attention mechanism. This mechanism assesses the relationship between words in a sequence by assigning weights, determining how closely words are related based on the training data. This enables Transformer models like ChatGPT to comprehend relationships between words, thus understanding human language more effectively.</p>
</li>
<li class="fm-list-bullet">
<p class="list">To calculate SDPA, the input embedding <span class="times">X</span> is processed through three distinct neural network layers, query <span class="times">(Q)</span>, key <span class="times">(K)</span>, and value <span class="times">(V)</span>. The corresponding weights for these layers are <span class="times">W<sup class="fm-superscript">Q</sup></span>, <span class="times">W<sup class="fm-superscript">K</sup></span>, and <span class="times">W<sup class="fm-superscript">V</sup></span>. We can calculate <span class="times">Q, K</span>, and <span class="times">V</span> as <span class="times">Q = X * W<sup class="fm-superscript">Q</sup></span>, <span class="times">K = X * Q<sup class="fm-superscript">K</sup></span>, and <span class="times">V = X * W<sup class="fm-superscript">V</sup></span>. SDPA is calculated as follows:</p>
<div class="figure2">
<p class="figure1"><img alt="" class="calibre4" height="60" src="../../OEBPS/Images/CH09_F09_Liu_EQ05.png" width="360"/></p>
</div>
</li>
</ul>
<p class="body-ind">where <span class="times">d<sub class="fm-subscript">k</sub></span> represents the dimension of the key vector <span class="times">K</span>. The softmax function is applied to the attention scores, converting them into attention weights. This ensures that the total attention a word gives to all words in the sentence sums to 100%. The final attention is the dot product of these attention weights and the value vector <span class="times">V</span>.<a id="marker-216"/></p>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">Instead of using one set of query, key, and value vectors, Transformer models use a concept called multihead attention. The query, key, and value vectors are split into multiple heads. Each head pays attention to different parts or aspects of the input, enabling the model to capture a broader range of information and form a more detailed and contextual understanding of the input data. Multihead attention is especially useful when a word has multiple meanings in a sentence.</p>
</li>
</ul>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-001"/><sup class="footnotenumber1"><a class="url1" href="#footnote-001-backlink">1</a></sup>  Vaswani et al., 2017, “Attention Is All You Need.” <a class="url" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.</p>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">2</a></sup>  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, 2014, “Neural Machine Translation by Jointly Learning to Align and Translate.” h<a class="url" href="https://arxiv.org/abs/1409.0473">ttps://arxiv.org/abs/1409.0473</a>.</p>
</div></body></html>