- en: Chapter 8\. Features of Generative AI Workloads on Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: About 15%–20% of the AI-900 exam is about features and scenarios for generative
    AI, which we’ll cover in this chapter. This includes understanding how models
    generate responses, create images, and write code snippets. We’ll also look at
    typical scenarios for generative AI, so you can connect theoretical knowledge
    with practical applications. This part of the exam isn’t just about knowing what
    generative AI is—it’s about recognizing its impact and relevance in real-world
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Another key area on the exam involves responsible AI. This includes understanding
    the ethical implications of AI-generated content and the measures that Microsoft
    Azure takes to ensure its generative AI tools are used safely and responsibly.
    By mastering these topics, you’ll be well prepared to answer questions on how
    Azure’s generative AI services can be effectively and responsibly applied across
    various domains.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, generative AI relies on models trained to understand and respond
    to language in ways that feel intuitive and humanlike. When you ask generative
    AI to generate content, it draws on massive amounts of data and applies mathematical
    algorithms to make this possible.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI has quickly become a centerpiece of the business world. It has
    attracted attention from industries across the board for its potential to streamline
    tasks, foster creativity, and drive productivity. Breakthrough applications like
    OpenAI’s ChatGPT and Microsoft’s Copilot platform have shown just how transformative
    this technology can be. ChatGPT, for instance, has opened up new ways to handle
    customer service, content creation, and brainstorming, all with remarkable efficiency
    and personalization. Meanwhile, Microsoft’s Copilot integrates directly into widely
    used tools like Word and Excel, empowering professionals to handle complex data,
    draft reports, and automate routine processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the easiest ways to work with generative AI is by simply typing in natural
    language. You type a prompt, which is a straightforward request, and the AI responds
    with what you’re looking for. [Figure 8-1](#i08_chapter8_figure_1_1742068263824651)
    shows an example of this when using [Microsoft Copilot](https://oreil.ly/2Oy0y).
    This is the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a professional bio introducing me as a project manager with a background
    in software development and a passion for team leadership.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Microsoft Copilot then writes up a good response.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. A text response to a prompt when using Microsoft Copilot
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Certain generative AI tools can take a simple request and transform it into
    a unique image. For instance, you might type: “Design an inviting book cover for
    a cozy mystery novel set in a small town.” [Figure 8-2](#i08_chapter8_figure_2_1742068263824686)
    shows the image generated by Microsoft Copilot.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. An image created by Microsoft Copilot in response to a prompt
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Finally, generative AI can create, debug, and review code for dozens of languages,
    including C++, Java, and Python. Here’s a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Write Python code to calculate the area of a circle, given its radius.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Figure 8-3](#i08_chapter8_figure_3_1742068263824712) shows the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. How Microsoft Copilot creates code in response to a prompt
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Microsoft Copilot generates the code that you request. You can click on the
    top right to copy the code and use it in your integrated development environment
    (IDE), like Visual Studio Code. Copilot also provides a brief description of how
    the program works.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generative AI applications leverage advanced language models—specialized ML
    systems crafted for NLP tasks. These models bring a powerful, flexible toolkit
    that goes well beyond generating text and images. Here’s a snapshot of their diverse
    capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Pattern recognition through unsupervised learning
  prefs: []
  type: TYPE_NORMAL
- en: AI can detect patterns and structures in large datasets without labeled data,
    allowing it to uncover insights autonomously.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretation of ambiguity
  prefs: []
  type: TYPE_NORMAL
- en: Models decipher ambiguous language by analyzing context. This makes them suitable
    for nuanced tasks like sentiment analysis or handling complex customer inquiries.
  prefs: []
  type: TYPE_NORMAL
- en: Summarization
  prefs: []
  type: TYPE_NORMAL
- en: Models rapidly distill lengthy content, pulling out essential points—key for
    making dense reports, legal briefs, or news summaries accessible.
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual translation
  prefs: []
  type: TYPE_NORMAL
- en: With a grasp of cultural nuances and contextual subtleties, these models accurately
    translate languages.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual responses to questions
  prefs: []
  type: TYPE_NORMAL
- en: The models respond to questions based on context—for example, to bolster customer
    support and make helpdesk interactions smoother.
  prefs: []
  type: TYPE_NORMAL
- en: Lifelike dialogue creation
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI produces realistic conversations, ideal for virtual assistants,
    chatbots, and dynamic character interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs: []
  type: TYPE_NORMAL
- en: By identifying irregular patterns, these models catch inconsistencies, whether
    in financial data, health care records, or security logs.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search
  prefs: []
  type: TYPE_NORMAL
- en: Rather than relying solely on keywords, AI performs searches based on meaning.
    This helps users find relevant information with minimal effort.
  prefs: []
  type: TYPE_NORMAL
- en: Personalized recommendations
  prefs: []
  type: TYPE_NORMAL
- en: AI tailors content and recommendations based on user profiles, which is valuable
    for marketing, customer service, and enhancing user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Content moderation
  prefs: []
  type: TYPE_NORMAL
- en: These models can identify and flag inappropriate or sensitive content, which
    can promote safer and more compliant digital spaces.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML models for NLP have evolved dramatically, leading to today’s advanced systems
    built on the transformer architecture. This architecture has changed how machines
    handle language. Using the large amount of data they’re trained on, these models
    learn the subtle connections between words, which allows them to predict sequences
    that feel natural and make sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers are built with two key components:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder block
  prefs: []
  type: TYPE_NORMAL
- en: Examines the input text and identifies meaningful relationships within the vocabulary
  prefs: []
  type: TYPE_NORMAL
- en: Decoder block
  prefs: []
  type: TYPE_NORMAL
- en: Uses the encoder’s output to generate relevant and contextually appropriate
    language
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at how it all comes together. During training, the
    model processes vast amounts of text from books, websites, and other sources.
    It breaks down this information into tokens (small units like words or parts of
    words) and feeds these to the encoder. With a process called *attention*, the
    encoder figures out how each token relates to others, recognizing patterns such
    as the difference between *bat* as in a flying mammal versus a piece of sports
    equipment. These relationships are stored as embeddings, which are mathematical
    vectors representing each token’s meaning. The decoder then takes these embeddings
    and generates a new sequence of text. For instance, if you provide the input “The
    mysterious package arrived,” the model might continue with “at my doorstep,” based
    on similar sentences it learned during training.
  prefs: []
  type: TYPE_NORMAL
- en: Different transformer models use these blocks in specialized ways. Google’s
    BERT model, for example, leverages the encoder to understand context in search
    results. OpenAI’s GPT model focuses on the decoder, which makes it a powerful
    tool for generating creative content and answering questions in natural, conversational
    language.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we’ll look deeper at the key components of the transformer
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization for transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous chapter, we dove into the concept of tokenization. Let’s continue
    building on that. Tokenization is the essential first step in training a transformer
    model.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI uses the [Tokenizer](https://oreil.ly/PGIwn), an example of which is
    shown in [Figure 8-4](#i08_chapter8_figure_4_1742068263824741). The Tokenizer
    converts text into tokens and IDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top, you can select different types of models to use: GPT-4o/GPT-4o
    mini, GPT-3.5/GPT-4, or GPT-3 (legacy). You’ll need to know what model you’re
    using because tokenization is different based on the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the example in [Figure 8-4](#i08_chapter8_figure_4_1742068263824741), we
    used this sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence ![](assets/robot_1f916.png) like machine learning and
    natural language processing, is transforming industries from healthcare to finance,
    sparking innovation in data-driven decision-making.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](assets/aaif_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. An example of OpenAI’s Tokenizer, which converts text into tokens
    for an LLM
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that a token can include a space with a word. It can also be for a punctuation
    mark or an emoji. In some cases, one word may be composed of two or more tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click Token IDs, you will get the vector for the tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is what the transformer model will process.
  prefs: []
  type: TYPE_NORMAL
- en: As the model undergoes further training, each additional token from the training
    data is incorporated into the vocabulary and assigned a unique token ID. Over
    time, with an expansive enough dataset, the vocabulary can grow to encompass thousands
    of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine trying to navigate a large library with nothing but a list of book
    titles, each assigned a unique number. Sure, the numbers help you locate individual
    books, but they offer no clues about the content of each book, its genre, or any
    connections it might have with other books. When it comes to representing words
    as token IDs in a vocabulary, it’s a similar situation: yes, it’s useful for indexing,
    but it lacks any insight into meaning or relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: To build a more meaningful map of language, we use embeddings, which we learned
    about in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In a vector space, words with similar meanings end up near one another, with
    directions in the space signifying their relationships. For instance, words like
    *king* and *queen* may point in nearly identical directions but differ slightly
    in their dimensions, distinguishing masculine and feminine concepts. This semantic
    proximity enables embeddings to capture subtle relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Creating these embeddings is no small feat, though. Algorithms like word2vec,
    which use neural networks to process massive amounts of text, or the transformer
    models powering advanced AI today identify patterns and context from countless
    words used in sentences. For instance, word2vec calculates embeddings by predicting
    word contexts whereas transformers use more sophisticated architectures to represent
    relationships at various levels of abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The transformer model’s encoder and decoder blocks ​are like layers of a sandwich,
    stacked up to build the neural network. While we don’t need to go over every ingredient
    in that sandwich, there’s one that’s essential in both blocks: the attention layers.
    This is where the model pays attention to relationships between words in a sequence.
    It’s about reading a sentence and figuring out which words have the strongest
    connections to one another.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the encoder block. ​This is the part that takes each word
    in a sentence and looks at it in the context of the words around it. The result?
    Each word gets an *encoding*—basically, a unique representation based on the company
    it keeps. Think of it this way: the word *cell* means one thing in the phrase
    “cell phone” and something totally different in “jail cell.” In this encoder,
    the model adjusts the encoding for *cell* based on which other words it’s hanging
    out with.'
  prefs: []
  type: TYPE_NORMAL
- en: In the decoder block, attention is used a bit differently. Here, the model is
    generating text one word at a time. For every new word, the decoder block considers
    all the words it’s generated so far to figure out what comes next. For instance,
    if you start with “He grabbed his umbrella,” the decoder might zero in on *grabbed*
    and *umbrella* to predict that the next word could be *and* or *opened*. It’s
    like the model is trying to complete a sentence by keeping an eye on the context
    it has already built.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there’s something special going on under the hood: *self-attention*. This
    is where each word gets weighed against others in the sentence to find out how
    much influence it should have on the meaning. In practical terms, self-attention
    gives different weights (or importance) to words depending on how they relate
    to one another. *Multihead* *attention* takes this to the next level by analyzing
    several relationships at once, enabling the model to capture all the nuances in
    a sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the model doesn’t actually “see” words; it’s working with numeric
    vectors (essentially lists of numbers) that represent the words. At the start,
    each word gets an initial value based on where it is in the sequence—think of
    it as a first guess. Then, the attention layers refine these vectors by applying
    weights, which let the model zero in on the most relevant bits of information
    for predicting the next word.
  prefs: []
  type: TYPE_NORMAL
- en: When training this model, we know the full sequence of words, so the model can
    learn from the actual outcome by comparing its predictions to the real sequence.
    This process—where it adjusts its own “attention” to get better over time—is called
    *minimizing loss*.
  prefs: []
  type: TYPE_NORMAL
- en: So what does this mean in action? A transformer model like GPT-4, which powers
    tools like ChatGPT, takes in a piece of text (a prompt) and generates a response
    that sounds coherent and natural. While it doesn’t understand or know things the
    way humans do, it uses massive amounts of data and complex patterns to predict
    what comes next in a way that often sounds spot-on.
  prefs: []
  type: TYPE_NORMAL
- en: Language Models on Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building your own language model from scratch is certainly possible, but the
    costs are huge—not just in time but in dollars. Training an LLM from scratch can
    easily cost millions. You’d need hundreds of powerful GPUs running nonstop for
    weeks (or even months), plus funding to cover the cost of storing and processing
    vast amounts of data. Most organizations find it far more practical to start with
    an existing *foundation model* and fine-tune it with their own data if needed.
    With so many options available, you can skip the heavy lifting and the financial
    burden.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re using Microsoft Azure, you’ll find these foundation models in two
    main places: the Azure OpenAI Service and the Azure AI Model Catalog. Think of
    the Model Catalog as a curated library of models that Azure has handpicked for
    data scientists and developers working with Azure AI Foundry and Azure Machine
    Learning. Plus, when you’re using models from the Azure OpenAI service, you get
    the full benefit of Azure’s secure, scalable infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Model Catalog is also loaded with open source models from a growing list
    of Azure’s partners, including:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mistral AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meta
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anthropic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI21 Labs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohere
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EleutherAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stability AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the popular models you’ll find in Azure OpenAI include:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5 Turbo, GPT-4, and GPT-4o
  prefs: []
  type: TYPE_NORMAL
- en: These are the go-to models for conversation-based applications. Just feed them
    some text, and they’ll give you well-formed responses.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 Turbo with Vision
  prefs: []
  type: TYPE_NORMAL
- en: This one does more than just text—it can analyze images and respond with detailed
    descriptions or answers. It combines language processing with visual understanding.
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Looking to create images from scratch or make edits to existing images? DALL-E
    is your tool for generating unique images, and adding variations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond these, Azure’s Model Catalog includes a range of other models suited
    for various applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion by Stability AI
  prefs: []
  type: TYPE_NORMAL
- en: Generates photorealistic images from text prompts
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  prefs: []
  type: TYPE_NORMAL
- en: Focuses on understanding text, making it great for applications like sentiment
    analysis and question answering
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive Language-Image Pretraining (CLIP)
  prefs: []
  type: TYPE_NORMAL
- en: Connects images with text descriptions, such as for multimedia applications
  prefs: []
  type: TYPE_NORMAL
- en: Large Language and Small Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two main types of language models you can choose from to power your
    generative AI applications: *large language models (LLMs)* and *small language
    models (SLMs)*. [Table 8-1](#i08_chapter8_table_1_1742068263829815) highlights
    the differences.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. Differences between LLMs and SLMs
  prefs: []
  type: TYPE_NORMAL
- en: '| Features | LLMs | SLMs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Training costs | Training an LLM is a major investment as it requires extensive
    compute power and specialized equipment to handle large datasets. | Training SLMs
    is generally more affordable as the smaller datasets and simpler model structures
    reduce resource demands. |'
  prefs: []
  type: TYPE_TB
- en: '| Inference speed | The large parameter count of LLMs can slow their response
    times, which may affect their suitability for real-time applications. | With fewer
    parameters, SLMs often respond faster, making them ideal for real-time use cases.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Memory requirements | LLMs require high memory and storage capacity, especially
    with very large parameter counts. | SLMs need less memory, making them easier
    to deploy on devices with limited resources, such as mobile or edge devices. |'
  prefs: []
  type: TYPE_TB
- en: '| Scalability | LLMs are frequently deployed in the cloud, allowing them to
    scale for larger or more intensive applications. | SLMs are easier to scale with
    fewer resources and often don’t need cloud hosting, which can save on costs and
    enhance privacy. |'
  prefs: []
  type: TYPE_TB
- en: '| Data privacy | Cloud-based deployment of LLMs often involves transferring
    data off-device, which could be a concern for privacy-sensitive applications.
    | SLMs can be deployed on devices or on premises, allowing data to remain securely
    within the organization, which can improve privacy. |'
  prefs: []
  type: TYPE_TB
- en: '| Energy consumption | LLMs are more power intensive, leading to higher operating
    costs and a larger environmental impact. | SLMs consume less energy, making them
    more suitable for eco-friendly applications and improving battery life on portable
    devices. |'
  prefs: []
  type: TYPE_TB
- en: Copilots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Copilots* are changing the way we interact with software. They bring AI-powered
    assistance directly into applications to help with everything from quick tasks
    to complex processes. Built as chat-based tools, these copilots are designed to
    provide on-demand, tailored support that’s ready whenever you need it.'
  prefs: []
  type: TYPE_NORMAL
- en: Copilots are also a key part of Microsoft’s AI strategy, which is important
    to know for the exam. This technology is embedded across various Microsoft products
    and built with an open architecture that allows for flexibility and customization.
    Developers can add their own plug-ins or even create completely new copilots to
    shape unique user experiences. Whether you’re working with an out-of-the-box copilot
    or crafting a custom one, you can adjust it to fit with your business processes.
    This ensures that the copilot responds just the way you need.
  prefs: []
  type: TYPE_NORMAL
- en: These copilots do more than just automate—they assist with drafting, summarizing,
    planning, and more, aiming to boost productivity, encourage creativity, and keep
    teams connected. Whether you’re using prebuilt tools, customizing them for specific
    workflows, or designing unique copilots, these AI assistants adapt to your needs
    and redefine the way you work and collaborate.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we’ll take a look at the different copilot systems
    from Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: Web-based copilots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Microsoft has different ways to access Microsoft Copilot using web-based applications.
    Earlier in the chapter, we saw one, which is the Microsoft Copilot site. You can
    also access Microsoft Copilot in the Bing search engine as shown in [Figure 8-5](#i08_chapter8_figure_5_1742068263824767).
    To access Copilot in Bing, click the icon to the right side of the search box.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Access Microsoft Copilot in Bing by clicking the icon on the right
    side of the search box
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also access this feature in the Microsoft Edge browser. When you open
    the browser, you will see a search box at the top, and Copilot will be on the
    right side.
  prefs: []
  type: TYPE_NORMAL
- en: But the Edge browser can also embed Copilot into the sidebar. You activate this
    by clicking the icon on the top right side of the browser. You can see the sidebar
    in [Figure 8-6](#i08_chapter8_figure_6_1742068263824787).
  prefs: []
  type: TYPE_NORMAL
- en: 'The interface has many useful options. At the top of the Copilot sidebar, you
    can choose between a chat mode and one to create content, such as for emails,
    blogs, or brainstorming. You can write your content in different tones, ranging
    from professional to casual to funny. You can also specify the length. The Chat
    feature is similar to ChatGPT, but you have the option to specify the conversation
    style, which is either: more creative, balanced, or precise.'
  prefs: []
  type: TYPE_NORMAL
- en: In the input box at the bottom, you can click the top to indicate what you want
    to chat about. This can be certain webpages, such as the current page of the site
    you have visited or other sites. The icons at the bottom enable you to upload
    images, add a screenshot, and use the voice system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. The Copilot sidebar in the Edge browser
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Microsoft Copilot for Microsoft 365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Microsoft Copilot for Microsoft 365 puts powerful AI right into the tools you
    already know—Word, PowerPoint, Outlook, Excel, and Teams—so you can get more done
    without breaking your workflow. Here’s how Copilot can help you in each of these
    apps:'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Word
  prefs: []
  type: TYPE_NORMAL
- en: With Copilot, you can keep improving your document without starting from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft PowerPoint
  prefs: []
  type: TYPE_NORMAL
- en: Got a report or email that you need to turn into a presentation? Copilot will
    create the slides from the content you already have. You can then adjust the format,
    add images, and fine-tune the slides.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Outlook
  prefs: []
  type: TYPE_NORMAL
- en: Copilot can help by summarizing emails, finding important details, and even
    gathering what you need to prep for meetings.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Excel
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis doesn’t have to be intimidating. Copilot can suggest formulas,
    uncover insights, and build visualizations. Need to make predictions or analyze
    risks? Copilot is on it, so you can work smarter, even if you’re not an Excel
    power user.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Teams
  prefs: []
  type: TYPE_NORMAL
- en: In meetings, Copilot can keep track of the conversation, summarize the key points,
    and even highlight questions that still need answers. This way, you can stay fully
    engaged, knowing that Copilot is capturing the details.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Dynamics 365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Microsoft Dynamics 365 is a platform for running your business more smoothly.
    It’s Microsoft’s answer to uniting different parts of your operations, from sales
    and customer service to supply chain management. Copilot has boosted the capabilities
    of Microsoft Dynamics 365 with the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: Copilot for Dynamics 365 Sales
  prefs: []
  type: TYPE_NORMAL
- en: Copilot pulls up customer and industry information from your CRM and other sources.
    Whether you’re qualifying a new lead, preparing a proposal, or scheduling a follow-up,
    Copilot makes the process faster and can provide insights. This helps to close
    more deals.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot for Dynamics 365 Supply Chain
  prefs: []
  type: TYPE_NORMAL
- en: Copilot keeps things running smoothly by managing those changes at scale, assessing
    potential impacts, and suggesting your next steps. Say an order update comes in—Copilot
    analyzes how it might affect the rest of the process, so you can make procurement
    decisions without disrupting your workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot for Dynamics 365 Customer Service
  prefs: []
  type: TYPE_NORMAL
- en: In customer service, speed and accuracy are important. Copilot steps in to help
    agents analyze support tickets, find similar issues, and offer solutions. Agents
    have what they need to resolve customer issues quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Azure AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Azure AI system has its own Copilot, which is called the Microsoft Copilot
    in Azure. You can activate it by clicking a button at the top of the screen next
    to the search box. When you do this, you will see the Copilot on the sidebar as
    shown in [Figure 8-7](#i08_chapter8_figure_7_1742068263824807).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. The Copilot in Azure AI
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Copilot can answer your questions using the latest documentation, suggest the
    best Azure services for your specific needs, and help you perform basic tasks
    in your environment. It even recommends script code to carry out those tasks.
    Plus, everything is tailored to fit your role and permissions, so you’re always
    in control.
  prefs: []
  type: TYPE_NORMAL
- en: Other Microsoft Copilots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Microsoft has rolled out quite a few copilots—and it’s coming up with new ones
    regularly. Here are some are some others:'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Security Copilot
  prefs: []
  type: TYPE_NORMAL
- en: Security Copilot steps up for the cybersecurity crowd. With some of the threat
    response automated, your security operations become faster and more precise.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot in Microsoft Fabric
  prefs: []
  type: TYPE_NORMAL
- en: Copilot can generate code for analyzing, manipulating, and visualization data
    in Spark Notebooks, letting you shift your focus away from coding and onto making
    insights.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Copilot
  prefs: []
  type: TYPE_NORMAL
- en: This AI-assisted programming tool not only generates code but also helps with
    unit tests and debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot in Microsoft Power BI
  prefs: []
  type: TYPE_NORMAL
- en: When you’re working on Power BI reports, Copilot can analyze your data and suggest
    useful visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Copilots are incredibly powerful tools, but they can sometimes fall short with
    their responses. That’s why it’s essential to understand *prompt engineering*:
    the skill of crafting specific inputs to guide AI in delivering better answers.
    Think of it as giving the AI just the right nudge to hit the mark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt engineering is a skill you can build with practice and a few best practices
    in mind. To get the most out of AI, start by being as specific as possible in
    your prompts. Vague prompts lead to vague responses, so always try to be precise.
    For example, if you’re asking for a summary, specify what to focus on: “Summarize
    the main arguments and key statistics from this report,” rather than just “Summarize
    this.” This little tweak makes a big difference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding context is another key to better prompts. If you’re working with a technical
    document, mention the field or intended audience, such as: “Explain this scientific
    study on climate change impacts for a high school audience.” Context acts as a
    guiding light, especially when dealing with complex topics, ensuring that the
    AI adapts its response to fit your needs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For prompts with a specific outcome in mind, it’s best to focus on one task
    or goal per prompt. Asking the AI to both “summarize this article and write three
    questions for discussion” can lead to mixed or incomplete responses. Instead,
    break it down: start with “Summarize this article in three bullet points,” then
    follow up with “Now, write three questions based on the summary.” This approach
    keeps the AI focused and the results cleaner.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Including source documents where possible also refines results. If you want
    the AI to pull from specific texts, provide them directly in the prompt or as
    attachments—for instance: “Based on the following paragraph from the *State of
    AI Report*, list three main insights.” This practice ensures that the AI responds
    based on relevant, specific information rather than generalizing from prior data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, setting guidelines on the format or tone also makes a big impact.
    If you need bullet points, a specific style, or a particular tone, just say so.
    You might write: “Explain the key points from this article in bullet points, using
    a conversational tone.” Or, if you’re crafting a professional email, you could
    add, “Use a formal tone and include a call to action at the end.”'
  prefs: []
  type: TYPE_NORMAL
- en: Even with all these best practices, a prompt might still fall short. In such
    cases, don’t hesitate to iterate. AI doesn’t always respond perfectly on the first
    try, so rephrasing or tweaking the prompt can yield better results.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing Copilots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you decide to customize Microsoft Copilot or build a copilot tailored
    for your organization’s needs, Microsoft provides two powerful tools: Copilot
    Studio and Azure AI Foundry. Each tool offers unique features to help you build
    an AI-driven copilot that aligns with your specific goals and user requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Copilot Studio is ideal for low-code development. This enables business users
    and developers with moderate technical skills to design conversational AI experiences
    without extensive coding. The platform provides a fully managed solution, which
    means you don’t have to worry about the infrastructure or deployment details.
    For example, imagine a health care provider using Copilot Studio to build a copilot
    that assists employees with managing patient intake by guiding them through a
    series of routine questions and steps in Microsoft Teams. Since Copilot Studio
    is hosted within the Microsoft 365 environment, users can rely on Teams as the
    familiar chat channel, making the transition to this new tool seamless.
  prefs: []
  type: TYPE_NORMAL
- en: Azure AI Foundry, on the other hand, is a tool built for developers seeking
    full control over their copilot’s underlying model. This platform-as-a-service
    (PaaS) portal gives you the freedom to fine-tune language models with your proprietary
    data, making it perfect for more advanced customization needs. Suppose a financial
    services company wants to build a copilot that provides personalized investment
    recommendations based on each client’s unique portfolio. With Azure AI Foundry,
    developers can integrate custom data augmentation and prompt engineering, crafting
    a copilot that understands complex financial terminology and client profiles.
    You also get control over deployment, so the copilot can integrate seamlessly
    into the company’s existing apps and services.
  prefs: []
  type: TYPE_NORMAL
- en: Azure OpenAI Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure OpenAI Service allows you to access advanced language models like GPT-4,
    GPT-3.5 Turbo, and specialized embeddings for targeted applications. You have
    several ways to get started, whether through REST APIs, the Python SDK, or the
    models in the Azure AI Studio, which has become part of Azure AI Foundry.
  prefs: []
  type: TYPE_NORMAL
- en: Each model in Azure OpenAI Service brings unique strengths. For instance, GPT-4
    and GPT-4 Turbo are designed for complex tasks, with Turbo additionally equipped
    to understand images. GPT-3.5 Turbo, on the other hand, is great for efficient
    content creation and fast response generation. The embeddings model is perfect
    for enhancing semantic search, letting you match queries with relevant data even
    when they don’t use the same words.
  prefs: []
  type: TYPE_NORMAL
- en: But what if you need the model to perform better on your specific data? That’s
    where fine-tuning comes in. *Fine-tuning* lets you customize models, teaching
    them to better respond based on the particular vocabulary, style, or preferences
    you need. It’s about optimizing the model’s performance by training it further
    on relevant examples, so it learns what’s important to you. This customization
    can improve results significantly. It makes the AI feel like a true extension
    of your team rather than a one-size-fits-all tool.
  prefs: []
  type: TYPE_NORMAL
- en: Using Azure OpenAI Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use Azure OpenAI Studio, you will first need to go to the Azure Portal and
    create an OpenAI resource. Then, open it and scroll down. Click on “Go to Azure
    OpenAI Studio.” [Figure 8-8](#i08_chapter8_figure_8_1742068263824827) shows the
    dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. The dashboard for Azure OpenAI Studio
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the top are details about the service, including the API key for the AI models.
    Then there are options for how to use Azure OpenAI Studio, such as by using chat
    or creating images.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll see the models available. Select “Model catalog” on the left menu
    bar. You can click on any of the models and get comprehensive information about
    them. This can be quite helpful to get a sense of which model is the best for
    your task.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s go to Chat, which is on the left menu bar. [Figure 8-9](#i08_chapter8_figure_9_1742068263824845)
    shows the screen for this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0809.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. The Chat system for Azure OpenAI Studio
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Click “Create new deployment” and select “From base models.” Then choose the
    GPT-4 model and click Confirm. You will see a menu box pop up. Then, press Deploy.
    You’ll be taken back to the Chat screen. Here, select Save.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see an input box for “Give the model instructions and context.” This
    refers to the system message. This is the context for each of the prompts when
    you chat. It essentially provides guidance for the AI model to create better responses.
    To get ideas for this, you can select “Prompt samples” at the top of the screen.
    There will be a list of suggestions. Let’s select “Shakespearean Writing Assistant.”
    Then, go to the query box at the bottom and enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a generative AI model?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The response will be in the style of Shakespeare!
  prefs: []
  type: TYPE_NORMAL
- en: A generative AI, thou asketh, is a wondrous creation,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Crafted by minds keen and sharp, a marvel in computation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It learns from vast texts of old and new, a scholar true,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To generate content fresh as morning dew.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Besides chat, the Studio has other applications, such as for using the OpenAI
    Assistants API, real-time audio, and image creation. There are also capabilities
    for fine-tuning the models as well as allowing for batch jobs when you want to
    work on tasks in the background, not in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Responsible Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI has remarkable capabilities, but it also comes with certain risks.
    If you’re working with this technology—whether as a data scientist, developer,
    or otherwise—it’s essential to approach it with responsibility. This means taking
    steps to spot, assess, and lessen any potential harms it might cause.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft provides a hands-on framework for doing just that. Microsoft suggests
    a four-stage process to guide you in building and implementing responsible generative
    AI solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: Spot potential harms
  prefs: []
  type: TYPE_NORMAL
- en: Start by identifying any risks relevant to your AI solution.
  prefs: []
  type: TYPE_NORMAL
- en: Assess the risks
  prefs: []
  type: TYPE_NORMAL
- en: Measure how frequently these risks show up in the AI’s outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Build in safeguards
  prefs: []
  type: TYPE_NORMAL
- en: Reduce these risks by layering protections throughout your solution. Be transparent
    with users about any potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy responsibly
  prefs: []
  type: TYPE_NORMAL
- en: Create a solid deployment and operational plan to keep things on track.
  prefs: []
  type: TYPE_NORMAL
- en: These steps align well with the functions outlined in the [National Institute
    of Standards and Technology’s AI Risk Management Framework](https://oreil.ly/OfcRU),
    offering a reliable structure for managing AI responsibly. In the next few sections,
    we’ll look more deeply into the four principles.
  prefs: []
  type: TYPE_NORMAL
- en: Spot Potential Harms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you’re designing a generative AI solution, the first step to building
    it responsibly is identifying any potential harms it could cause. This step actually
    contains four key actions to take:'
  prefs: []
  type: TYPE_NORMAL
- en: Assess harm
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by looking at all the ways your AI could produce unwanted results. The
    specific harms depend on what services, models, and data you’re working with,
    whether you’re using prebuilt models, fine-tuning a model, or using custom data.
    Common issues include:'
  prefs: []
  type: TYPE_NORMAL
- en: Producing offensive or biased language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing inaccurate information as facts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suggesting harmful or illegal activities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if you’re creating an AI tool for customer service, it might misinterpret
    certain phrases and respond with unintentional insensitivity. To get a good sense
    of these risks, review the documentation from your providers. For instance, OpenAI
    has a system card for GPT-4 that lays out specific model considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Rank the risks
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve identified potential harms, prioritize them. Which risks are most
    likely to occur? Which would have the highest impact? Think about both typical
    use and possible misuse scenarios. Let’s say you’re developing a health app. A
    minor error could suggest an incorrect meal plan while a more severe error might
    accidentally recommend an exercise that could harm someone with a heart condition.
    Here, the higher-impact harm would likely take priority, but frequency matters,
    too. This step often benefits from input from policy or legal advisers who can
    help weigh the implications.
  prefs: []
  type: TYPE_NORMAL
- en: Test and validate risks
  prefs: []
  type: TYPE_NORMAL
- en: With your list of prioritized risks, you can now test them. Red team testing
    is a popular method where you try to push the model to reveal its weaknesses.
    *Red teaming* is borrowed from the cybersecurity field, where it’s used to uncover
    software vulnerabilities. Testing your AI this way can help you uncover potentially
    harmful outputs that might otherwise go unnoticed. For example, if your AI tool
    gives home improvement advice, the testing team might ask it for instructions
    on wiring that could lead to unsafe practices. The goal is to confirm under which
    scenarios these harms appear and if there are any additional ones you hadn’t considered.
  prefs: []
  type: TYPE_NORMAL
- en: Record and share harms
  prefs: []
  type: TYPE_NORMAL
- en: After testing, document your findings. Keep an updated list of potential risks,
    along with the evidence and testing data behind each one. This is crucial for
    stakeholder awareness and future updates. As your AI evolves, you’ll want a clear
    record to ensure that new risks are managed effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Assess the Risks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s move on to the second stage in building responsible generative AI: assessing
    the risks. Once you’ve set up a prioritized list of possible harmful outputs,
    it’s time to really test your AI and see how often these issues pop up and how
    serious they are. The first step is to create a *baseline*: a snapshot that captures
    the current state of harmful outputs in different situations. This baseline gives
    you a solid point of reference so that you can measure improvements as you make
    tweaks to reduce those risks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple, three-step plan for assessing potential harms:'
  prefs: []
  type: TYPE_NORMAL
- en: Create targeted prompts
  prefs: []
  type: TYPE_NORMAL
- en: Start by designing prompts aimed at highlighting each potential harm you’ve
    identified. If one concern is that your AI might give unsafe advice, make prompts
    that test for that. Say you’re working with a health assistant AI. You might ask,
    “What’s a quick way to treat a deep cut with items I have at home?” These prompts
    are designed to reveal any weak spots in the AI’s responses.
  prefs: []
  type: TYPE_NORMAL
- en: Run the prompts and gather outputs
  prefs: []
  type: TYPE_NORMAL
- en: Feed these targeted prompts into your AI and collect the responses. This is
    where you’ll see how the AI performs when faced with tricky, real-world questions.
    The responses you collect here give you the raw data you need for a clear picture
    of the AI’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Classify the results
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the outputs, evaluate each one based on predefined criteria. You
    could go simple, labeling responses as “harmful” or “safe,” or use a more nuanced
    scale like “low,” “medium,” and “high” risk. Setting these categories ahead of
    time ensures that you’re consistent when reviewing the results. Documenting these
    findings is essential and sharing them with stakeholders keeps everyone aligned
    and builds transparency.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re just starting, it’s smart to test a small group of prompts manually.
    This approach helps you fine-tune your criteria and catch any inconsistencies
    before you move on to a larger scale. Once you’re confident, consider automating
    the testing with a classification model. This lets you quickly review large volumes
    of responses, saving time. But remember that even with automation, it’s a good
    idea to check in periodically with manual reviews. Manual checks can catch new
    issues that automated systems might miss, keeping your AI aligned with your safety
    goals.
  prefs: []
  type: TYPE_NORMAL
- en: Build In Safeguards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With a baseline for harmful outputs and a way to track improvements, you’re
    ready to dive into the third stage of building responsible generative AI: reducing
    those risks. Mitigating potential harms in generative AI isn’t a one-and-done
    fix. It takes layers of safeguards, each adding its own level of protection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a look at four essential layers that all require safeguards:'
  prefs: []
  type: TYPE_NORMAL
- en: Model layer
  prefs: []
  type: TYPE_NORMAL
- en: 'This layer is about the model itself. Choosing the right model is the first
    step in managing risks. If your AI solution only needs to handle straightforward
    tasks, a smaller, more targeted model might be the best choice. For example, if
    you’re working on simple text classification, a streamlined model could be just
    as effective as something like GPT-4, but with a lower risk of producing unintended
    content. Fine-tuning is also a smart option: by training the model on specific
    data, you help it stay focused on what’s relevant for your needs, minimizing off-topic
    or risky outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Safety system layer
  prefs: []
  type: TYPE_NORMAL
- en: Next up is the safety system layer, which includes platform-level controls and
    filters. Many AI platforms—like Azure AI Foundry—offer real-time content filtering.
    They categorize responses by risk level (safe, low, medium, or high) based on
    categories like hate speech or self-harm. Beyond filtering, some platforms have
    abuse-detection features that can flag suspicious activity patterns (such as bots
    making tons of requests) and alert your team to potential misuse.
  prefs: []
  type: TYPE_NORMAL
- en: Metaprompt and grounding layer
  prefs: []
  type: TYPE_NORMAL
- en: This layer focuses on shaping the prompts your model sees. *Metaprompts*—statements
    that set tone or style—can guide the AI’s behavior. Think of it as setting boundaries.
    You can tell the model to keep its responses “helpful” or “neutral.” Techniques
    like prompt engineering and adding grounding data (relevant context from reliable
    sources) are valuable here, too. For high-stakes applications, consider using
    an RAG approach to pull in verified information, which helps keep responses accurate
    and safe.
  prefs: []
  type: TYPE_NORMAL
- en: User experience layer
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there’s the UX layer. Here, it’s about creating a safe and intuitive
    interface for users. You can reduce risks by limiting inputs to certain categories,
    helping prevent users from entering risky or off-topic prompts. And good documentation
    is key. When users understand what the AI can and can’t reliably handle, they’re
    more likely to use it safely. A bit of clarity goes a long way in setting realistic
    expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Operate a Responsible Generative AI Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve identified potential harms, set up ways to measure them, and added safeguards
    to your solution. Now, it’s time to get ready for launch during the fourth and
    final stage. But before hitting the release button, there are a few things to
    keep in mind to make sure everything goes as smoothly as possible—and stays that
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Check all compliance boxes'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before you roll out your generative AI solution, make sure it checks all the
    necessary compliance boxes. Different teams in your organization might need to
    give it a thumbs-up, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Legal
  prefs: []
  type: TYPE_NORMAL
- en: Ensures everything is on the right side of regulations
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  prefs: []
  type: TYPE_NORMAL
- en: Keeps user data safe and sound
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs: []
  type: TYPE_NORMAL
- en: Confirms that there are no backdoors or weak spots
  prefs: []
  type: TYPE_NORMAL
- en: Accessibility
  prefs: []
  type: TYPE_NORMAL
- en: Makes sure it’s usable for everyone, including those with disabilities
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Plan your release and stay ready to operate'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Releasing a generative AI solution takes a bit of planning, so here are a few
    ideas to help you get set up:'
  prefs: []
  type: TYPE_NORMAL
- en: Phased rollout
  prefs: []
  type: TYPE_NORMAL
- en: Instead of jumping straight to a full release, try starting with a smaller group
    of users. This way, you can get feedback and work out any kinks before a wider
    launch.
  prefs: []
  type: TYPE_NORMAL
- en: Incident response plan
  prefs: []
  type: TYPE_NORMAL
- en: Things don’t always go as planned, so make sure you have a plan to handle unexpected
    issues. Outline how to respond and specify who’s in charge if something goes wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Rollback plan
  prefs: []
  type: TYPE_NORMAL
- en: Prepare a quick way to revert to an earlier version if needed. This can save
    you a lot of headaches if an issue arises.
  prefs: []
  type: TYPE_NORMAL
- en: On-the-spot blocking
  prefs: []
  type: TYPE_NORMAL
- en: Build in a way to stop harmful responses the moment they’re spotted, so you’re
    always in control.
  prefs: []
  type: TYPE_NORMAL
- en: User-blocking options
  prefs: []
  type: TYPE_NORMAL
- en: Set up tools to restrict certain users or IP addresses if the system is being
    misused.
  prefs: []
  type: TYPE_NORMAL
- en: User feedback
  prefs: []
  type: TYPE_NORMAL
- en: Let users report issues easily. For example, include options for flagging responses
    as “inaccurate,” “offensive,” or “harmful.” These insights help you keep improving.
  prefs: []
  type: TYPE_NORMAL
- en: Telemetry tracking
  prefs: []
  type: TYPE_NORMAL
- en: Use telemetry to see how people are using your solution and spot areas for improvement.
    Just make sure it’s privacy compliant.
  prefs: []
  type: TYPE_NORMAL
- en: 'For even more security, Azure AI offers built-in tools to help monitor and
    control the content. Some key features include:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt shields
  prefs: []
  type: TYPE_NORMAL
- en: To screen for risky inputs
  prefs: []
  type: TYPE_NORMAL
- en: Groundedness detection
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that responses stick to user-provided information
  prefs: []
  type: TYPE_NORMAL
- en: Protected material detection
  prefs: []
  type: TYPE_NORMAL
- en: To flag restricted or copyrighted content
  prefs: []
  type: TYPE_NORMAL
- en: Custom categories
  prefs: []
  type: TYPE_NORMAL
- en: To monitor any emerging risks specific to your application
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI on Azure offers transformative capabilities, but effectively harnessing
    its power requires a solid understanding of the features, ethical considerations,
    and safety mechanisms built into these tools. By exploring the essential aspects
    of generative AI—such as model functions, responsible use, and the practical application
    of services like Microsoft Copilot and Azure OpenAI—you’re better equipped to
    leverage these tools responsibly. Mastering these topics prepares you to tackle
    AI-900 exam questions confidently and apply Azure’s generative AI solutions in
    meaningful, secure ways across diverse use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Quiz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To check your answers, please refer to the [“Chapter 8 Answer Key”](app02.html#answers_chapter_8_sample_questions_1745932457451977).
  prefs: []
  type: TYPE_NORMAL
- en: Which feature of generative AI on Azure allows for generating unique images
    based on text prompts?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Semantic search
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: DALL-E
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Content moderation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Lifelike dialogue creation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of embeddings in transformer models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To identify harmful content
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To translate languages
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To encode semantic relationships between words
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To generate recommendations
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which component of a transformer model interprets the context of input text?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decoder block
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Self-attention
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Encoder block
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which workload requires the ability to respond naturally to customers’ questions
    and inquiries?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image generation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Summarization
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Contextual question answering
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Personalized recommendations
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In the context of Azure’s AI, what does *multihead attention* refer to?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating new tokens
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Detecting anomalies
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyzing relationships between words from multiple perspectives
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Translating between languages
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which feature is common to large language models (LLMs) but not small language
    models (SLMs)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fast response time
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: High memory and storage requirements
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Low energy consumption
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Easy on-premises deployment
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the primary advantage of using Azure OpenAI’s Model Catalog?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fast training times for custom models
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Access to a variety of pretrained, high-performance models
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Exclusive use of OpenAI models
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Only for image-generation tasks
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the safety system layer help mitigate risks in Azure’s generative AI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It sets user expectations.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It filters out harmful or inappropriate content in real time.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It improves model embeddings.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It provides semantic search capabilities.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What function does the decoder block serve in a transformer model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To interpret the context of input
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To generate the output sequence based on the encoded input
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To embed words into vectors
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To attend to specific input tokens
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which strategy is recommended by Microsoft for responsible deployment of generative
    AI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rely solely on automated testing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid documenting potential risks
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a full-scale rollout immediately
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a phased rollout with an incident response plan
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
