- en: Chapter 8\. Features of Generative AI Workloads on Azure
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: About 15%–20% of the AI-900 exam is about features and scenarios for generative
    AI, which we’ll cover in this chapter. This includes understanding how models
    generate responses, create images, and write code snippets. We’ll also look at
    typical scenarios for generative AI, so you can connect theoretical knowledge
    with practical applications. This part of the exam isn’t just about knowing what
    generative AI is—it’s about recognizing its impact and relevance in real-world
    use cases.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Another key area on the exam involves responsible AI. This includes understanding
    the ethical implications of AI-generated content and the measures that Microsoft
    Azure takes to ensure its generative AI tools are used safely and responsibly.
    By mastering these topics, you’ll be well prepared to answer questions on how
    Azure’s generative AI services can be effectively and responsibly applied across
    various domains.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Generative AI
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, generative AI relies on models trained to understand and respond
    to language in ways that feel intuitive and humanlike. When you ask generative
    AI to generate content, it draws on massive amounts of data and applies mathematical
    algorithms to make this possible.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI has quickly become a centerpiece of the business world. It has
    attracted attention from industries across the board for its potential to streamline
    tasks, foster creativity, and drive productivity. Breakthrough applications like
    OpenAI’s ChatGPT and Microsoft’s Copilot platform have shown just how transformative
    this technology can be. ChatGPT, for instance, has opened up new ways to handle
    customer service, content creation, and brainstorming, all with remarkable efficiency
    and personalization. Meanwhile, Microsoft’s Copilot integrates directly into widely
    used tools like Word and Excel, empowering professionals to handle complex data,
    draft reports, and automate routine processes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the easiest ways to work with generative AI is by simply typing in natural
    language. You type a prompt, which is a straightforward request, and the AI responds
    with what you’re looking for. [Figure 8-1](#i08_chapter8_figure_1_1742068263824651)
    shows an example of this when using [Microsoft Copilot](https://oreil.ly/2Oy0y).
    This is the prompt:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Write a professional bio introducing me as a project manager with a background
    in software development and a passion for team leadership.
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Microsoft Copilot then writes up a good response.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0801.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. A text response to a prompt when using Microsoft Copilot
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Certain generative AI tools can take a simple request and transform it into
    a unique image. For instance, you might type: “Design an inviting book cover for
    a cozy mystery novel set in a small town.” [Figure 8-2](#i08_chapter8_figure_2_1742068263824686)
    shows the image generated by Microsoft Copilot.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0802.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. An image created by Microsoft Copilot in response to a prompt
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Finally, generative AI can create, debug, and review code for dozens of languages,
    including C++, Java, and Python. Here’s a prompt:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Write Python code to calculate the area of a circle, given its radius.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Figure 8-3](#i08_chapter8_figure_3_1742068263824712) shows the result.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0803.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. How Microsoft Copilot creates code in response to a prompt
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Microsoft Copilot generates the code that you request. You can click on the
    top right to copy the code and use it in your integrated development environment
    (IDE), like Visual Studio Code. Copilot also provides a brief description of how
    the program works.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Language Models
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generative AI applications leverage advanced language models—specialized ML
    systems crafted for NLP tasks. These models bring a powerful, flexible toolkit
    that goes well beyond generating text and images. Here’s a snapshot of their diverse
    capabilities:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Pattern recognition through unsupervised learning
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: AI can detect patterns and structures in large datasets without labeled data,
    allowing it to uncover insights autonomously.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Interpretation of ambiguity
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Models decipher ambiguous language by analyzing context. This makes them suitable
    for nuanced tasks like sentiment analysis or handling complex customer inquiries.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Summarization
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Models rapidly distill lengthy content, pulling out essential points—key for
    making dense reports, legal briefs, or news summaries accessible.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual translation
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: With a grasp of cultural nuances and contextual subtleties, these models accurately
    translate languages.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Contextual responses to questions
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The models respond to questions based on context—for example, to bolster customer
    support and make helpdesk interactions smoother.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Lifelike dialogue creation
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI produces realistic conversations, ideal for virtual assistants,
    chatbots, and dynamic character interactions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: By identifying irregular patterns, these models catch inconsistencies, whether
    in financial data, health care records, or security logs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Rather than relying solely on keywords, AI performs searches based on meaning.
    This helps users find relevant information with minimal effort.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Personalized recommendations
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: AI tailors content and recommendations based on user profiles, which is valuable
    for marketing, customer service, and enhancing user experience.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Content moderation
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: These models can identify and flag inappropriate or sensitive content, which
    can promote safer and more compliant digital spaces.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer Model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML models for NLP have evolved dramatically, leading to today’s advanced systems
    built on the transformer architecture. This architecture has changed how machines
    handle language. Using the large amount of data they’re trained on, these models
    learn the subtle connections between words, which allows them to predict sequences
    that feel natural and make sense.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers are built with two key components:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Encoder block
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Examines the input text and identifies meaningful relationships within the vocabulary
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Decoder block
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Uses the encoder’s output to generate relevant and contextually appropriate
    language
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at how it all comes together. During training, the
    model processes vast amounts of text from books, websites, and other sources.
    It breaks down this information into tokens (small units like words or parts of
    words) and feeds these to the encoder. With a process called *attention*, the
    encoder figures out how each token relates to others, recognizing patterns such
    as the difference between *bat* as in a flying mammal versus a piece of sports
    equipment. These relationships are stored as embeddings, which are mathematical
    vectors representing each token’s meaning. The decoder then takes these embeddings
    and generates a new sequence of text. For instance, if you provide the input “The
    mysterious package arrived,” the model might continue with “at my doorstep,” based
    on similar sentences it learned during training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Different transformer models use these blocks in specialized ways. Google’s
    BERT model, for example, leverages the encoder to understand context in search
    results. OpenAI’s GPT model focuses on the decoder, which makes it a powerful
    tool for generating creative content and answering questions in natural, conversational
    language.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we’ll look deeper at the key components of the transformer
    model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization for transformers
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous chapter, we dove into the concept of tokenization. Let’s continue
    building on that. Tokenization is the essential first step in training a transformer
    model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI uses the [Tokenizer](https://oreil.ly/PGIwn), an example of which is
    shown in [Figure 8-4](#i08_chapter8_figure_4_1742068263824741). The Tokenizer
    converts text into tokens and IDs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top, you can select different types of models to use: GPT-4o/GPT-4o
    mini, GPT-3.5/GPT-4, or GPT-3 (legacy). You’ll need to know what model you’re
    using because tokenization is different based on the model.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'For the example in [Figure 8-4](#i08_chapter8_figure_4_1742068263824741), we
    used this sentence:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence ![](assets/robot_1f916.png) like machine learning and
    natural language processing, is transforming industries from healthcare to finance,
    sparking innovation in data-driven decision-making.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](assets/aaif_0804.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. An example of OpenAI’s Tokenizer, which converts text into tokens
    for an LLM
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that a token can include a space with a word. It can also be for a punctuation
    mark or an emoji. In some cases, one word may be composed of two or more tokens.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click Token IDs, you will get the vector for the tokenization:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is what the transformer model will process.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: As the model undergoes further training, each additional token from the training
    data is incorporated into the vocabulary and assigned a unique token ID. Over
    time, with an expansive enough dataset, the vocabulary can grow to encompass thousands
    of tokens.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine trying to navigate a large library with nothing but a list of book
    titles, each assigned a unique number. Sure, the numbers help you locate individual
    books, but they offer no clues about the content of each book, its genre, or any
    connections it might have with other books. When it comes to representing words
    as token IDs in a vocabulary, it’s a similar situation: yes, it’s useful for indexing,
    but it lacks any insight into meaning or relationships.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: To build a more meaningful map of language, we use embeddings, which we learned
    about in the previous chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: In a vector space, words with similar meanings end up near one another, with
    directions in the space signifying their relationships. For instance, words like
    *king* and *queen* may point in nearly identical directions but differ slightly
    in their dimensions, distinguishing masculine and feminine concepts. This semantic
    proximity enables embeddings to capture subtle relationships.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Creating these embeddings is no small feat, though. Algorithms like word2vec,
    which use neural networks to process massive amounts of text, or the transformer
    models powering advanced AI today identify patterns and context from countless
    words used in sentences. For instance, word2vec calculates embeddings by predicting
    word contexts whereas transformers use more sophisticated architectures to represent
    relationships at various levels of abstraction.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Attention
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The transformer model’s encoder and decoder blocks ​are like layers of a sandwich,
    stacked up to build the neural network. While we don’t need to go over every ingredient
    in that sandwich, there’s one that’s essential in both blocks: the attention layers.
    This is where the model pays attention to relationships between words in a sequence.
    It’s about reading a sentence and figuring out which words have the strongest
    connections to one another.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the encoder block. ​This is the part that takes each word
    in a sentence and looks at it in the context of the words around it. The result?
    Each word gets an *encoding*—basically, a unique representation based on the company
    it keeps. Think of it this way: the word *cell* means one thing in the phrase
    “cell phone” and something totally different in “jail cell.” In this encoder,
    the model adjusts the encoding for *cell* based on which other words it’s hanging
    out with.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: In the decoder block, attention is used a bit differently. Here, the model is
    generating text one word at a time. For every new word, the decoder block considers
    all the words it’s generated so far to figure out what comes next. For instance,
    if you start with “He grabbed his umbrella,” the decoder might zero in on *grabbed*
    and *umbrella* to predict that the next word could be *and* or *opened*. It’s
    like the model is trying to complete a sentence by keeping an eye on the context
    it has already built.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'But there’s something special going on under the hood: *self-attention*. This
    is where each word gets weighed against others in the sentence to find out how
    much influence it should have on the meaning. In practical terms, self-attention
    gives different weights (or importance) to words depending on how they relate
    to one another. *Multihead* *attention* takes this to the next level by analyzing
    several relationships at once, enabling the model to capture all the nuances in
    a sentence.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the model doesn’t actually “see” words; it’s working with numeric
    vectors (essentially lists of numbers) that represent the words. At the start,
    each word gets an initial value based on where it is in the sequence—think of
    it as a first guess. Then, the attention layers refine these vectors by applying
    weights, which let the model zero in on the most relevant bits of information
    for predicting the next word.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: When training this model, we know the full sequence of words, so the model can
    learn from the actual outcome by comparing its predictions to the real sequence.
    This process—where it adjusts its own “attention” to get better over time—is called
    *minimizing loss*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: So what does this mean in action? A transformer model like GPT-4, which powers
    tools like ChatGPT, takes in a piece of text (a prompt) and generates a response
    that sounds coherent and natural. While it doesn’t understand or know things the
    way humans do, it uses massive amounts of data and complex patterns to predict
    what comes next in a way that often sounds spot-on.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Language Models on Azure
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building your own language model from scratch is certainly possible, but the
    costs are huge—not just in time but in dollars. Training an LLM from scratch can
    easily cost millions. You’d need hundreds of powerful GPUs running nonstop for
    weeks (or even months), plus funding to cover the cost of storing and processing
    vast amounts of data. Most organizations find it far more practical to start with
    an existing *foundation model* and fine-tune it with their own data if needed.
    With so many options available, you can skip the heavy lifting and the financial
    burden.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re using Microsoft Azure, you’ll find these foundation models in two
    main places: the Azure OpenAI Service and the Azure AI Model Catalog. Think of
    the Model Catalog as a curated library of models that Azure has handpicked for
    data scientists and developers working with Azure AI Foundry and Azure Machine
    Learning. Plus, when you’re using models from the Azure OpenAI service, you get
    the full benefit of Azure’s secure, scalable infrastructure.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'The Model Catalog is also loaded with open source models from a growing list
    of Azure’s partners, including:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mistral AI
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meta
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anthropic
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI21 Labs
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohere
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EleutherAI
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stability AI
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the popular models you’ll find in Azure OpenAI include:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5 Turbo, GPT-4, and GPT-4o
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: These are the go-to models for conversation-based applications. Just feed them
    some text, and they’ll give you well-formed responses.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 Turbo with Vision
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: This one does more than just text—it can analyze images and respond with detailed
    descriptions or answers. It combines language processing with visual understanding.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Looking to create images from scratch or make edits to existing images? DALL-E
    is your tool for generating unique images, and adding variations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond these, Azure’s Model Catalog includes a range of other models suited
    for various applications:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion by Stability AI
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Generates photorealistic images from text prompts
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Focuses on understanding text, making it great for applications like sentiment
    analysis and question answering
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive Language-Image Pretraining (CLIP)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Connects images with text descriptions, such as for multimedia applications
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Large Language and Small Language Models
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two main types of language models you can choose from to power your
    generative AI applications: *large language models (LLMs)* and *small language
    models (SLMs)*. [Table 8-1](#i08_chapter8_table_1_1742068263829815) highlights
    the differences.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. Differences between LLMs and SLMs
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '| Features | LLMs | SLMs |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: '| Training costs | Training an LLM is a major investment as it requires extensive
    compute power and specialized equipment to handle large datasets. | Training SLMs
    is generally more affordable as the smaller datasets and simpler model structures
    reduce resource demands. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
- en: '| Inference speed | The large parameter count of LLMs can slow their response
    times, which may affect their suitability for real-time applications. | With fewer
    parameters, SLMs often respond faster, making them ideal for real-time use cases.
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| Memory requirements | LLMs require high memory and storage capacity, especially
    with very large parameter counts. | SLMs need less memory, making them easier
    to deploy on devices with limited resources, such as mobile or edge devices. |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| Scalability | LLMs are frequently deployed in the cloud, allowing them to
    scale for larger or more intensive applications. | SLMs are easier to scale with
    fewer resources and often don’t need cloud hosting, which can save on costs and
    enhance privacy. |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| Data privacy | Cloud-based deployment of LLMs often involves transferring
    data off-device, which could be a concern for privacy-sensitive applications.
    | SLMs can be deployed on devices or on premises, allowing data to remain securely
    within the organization, which can improve privacy. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| Energy consumption | LLMs are more power intensive, leading to higher operating
    costs and a larger environmental impact. | SLMs consume less energy, making them
    more suitable for eco-friendly applications and improving battery life on portable
    devices. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: Copilots
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Copilots* are changing the way we interact with software. They bring AI-powered
    assistance directly into applications to help with everything from quick tasks
    to complex processes. Built as chat-based tools, these copilots are designed to
    provide on-demand, tailored support that’s ready whenever you need it.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Copilots are also a key part of Microsoft’s AI strategy, which is important
    to know for the exam. This technology is embedded across various Microsoft products
    and built with an open architecture that allows for flexibility and customization.
    Developers can add their own plug-ins or even create completely new copilots to
    shape unique user experiences. Whether you’re working with an out-of-the-box copilot
    or crafting a custom one, you can adjust it to fit with your business processes.
    This ensures that the copilot responds just the way you need.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: These copilots do more than just automate—they assist with drafting, summarizing,
    planning, and more, aiming to boost productivity, encourage creativity, and keep
    teams connected. Whether you’re using prebuilt tools, customizing them for specific
    workflows, or designing unique copilots, these AI assistants adapt to your needs
    and redefine the way you work and collaborate.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we’ll take a look at the different copilot systems
    from Microsoft.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Web-based copilots
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Microsoft has different ways to access Microsoft Copilot using web-based applications.
    Earlier in the chapter, we saw one, which is the Microsoft Copilot site. You can
    also access Microsoft Copilot in the Bing search engine as shown in [Figure 8-5](#i08_chapter8_figure_5_1742068263824767).
    To access Copilot in Bing, click the icon to the right side of the search box.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0805.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Access Microsoft Copilot in Bing by clicking the icon on the right
    side of the search box
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also access this feature in the Microsoft Edge browser. When you open
    the browser, you will see a search box at the top, and Copilot will be on the
    right side.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: But the Edge browser can also embed Copilot into the sidebar. You activate this
    by clicking the icon on the top right side of the browser. You can see the sidebar
    in [Figure 8-6](#i08_chapter8_figure_6_1742068263824787).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'The interface has many useful options. At the top of the Copilot sidebar, you
    can choose between a chat mode and one to create content, such as for emails,
    blogs, or brainstorming. You can write your content in different tones, ranging
    from professional to casual to funny. You can also specify the length. The Chat
    feature is similar to ChatGPT, but you have the option to specify the conversation
    style, which is either: more creative, balanced, or precise.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: In the input box at the bottom, you can click the top to indicate what you want
    to chat about. This can be certain webpages, such as the current page of the site
    you have visited or other sites. The icons at the bottom enable you to upload
    images, add a screenshot, and use the voice system.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0806.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. The Copilot sidebar in the Edge browser
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Microsoft Copilot for Microsoft 365
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Microsoft Copilot for Microsoft 365 puts powerful AI right into the tools you
    already know—Word, PowerPoint, Outlook, Excel, and Teams—so you can get more done
    without breaking your workflow. Here’s how Copilot can help you in each of these
    apps:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Word
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: With Copilot, you can keep improving your document without starting from scratch.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft PowerPoint
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Got a report or email that you need to turn into a presentation? Copilot will
    create the slides from the content you already have. You can then adjust the format,
    add images, and fine-tune the slides.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Outlook
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Copilot can help by summarizing emails, finding important details, and even
    gathering what you need to prep for meetings.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Excel
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis doesn’t have to be intimidating. Copilot can suggest formulas,
    uncover insights, and build visualizations. Need to make predictions or analyze
    risks? Copilot is on it, so you can work smarter, even if you’re not an Excel
    power user.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Teams
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In meetings, Copilot can keep track of the conversation, summarize the key points,
    and even highlight questions that still need answers. This way, you can stay fully
    engaged, knowing that Copilot is capturing the details.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Dynamics 365
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Microsoft Dynamics 365 is a platform for running your business more smoothly.
    It’s Microsoft’s answer to uniting different parts of your operations, from sales
    and customer service to supply chain management. Copilot has boosted the capabilities
    of Microsoft Dynamics 365 with the following features:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Copilot for Dynamics 365 Sales
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Copilot pulls up customer and industry information from your CRM and other sources.
    Whether you’re qualifying a new lead, preparing a proposal, or scheduling a follow-up,
    Copilot makes the process faster and can provide insights. This helps to close
    more deals.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Copilot for Dynamics 365 Supply Chain
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Copilot keeps things running smoothly by managing those changes at scale, assessing
    potential impacts, and suggesting your next steps. Say an order update comes in—Copilot
    analyzes how it might affect the rest of the process, so you can make procurement
    decisions without disrupting your workflow.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Copilot for Dynamics 365 Customer Service
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: In customer service, speed and accuracy are important. Copilot steps in to help
    agents analyze support tickets, find similar issues, and offer solutions. Agents
    have what they need to resolve customer issues quickly.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Azure AI
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Azure AI system has its own Copilot, which is called the Microsoft Copilot
    in Azure. You can activate it by clicking a button at the top of the screen next
    to the search box. When you do this, you will see the Copilot on the sidebar as
    shown in [Figure 8-7](#i08_chapter8_figure_7_1742068263824807).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0807.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. The Copilot in Azure AI
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Copilot can answer your questions using the latest documentation, suggest the
    best Azure services for your specific needs, and help you perform basic tasks
    in your environment. It even recommends script code to carry out those tasks.
    Plus, everything is tailored to fit your role and permissions, so you’re always
    in control.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Other Microsoft Copilots
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Microsoft has rolled out quite a few copilots—and it’s coming up with new ones
    regularly. Here are some are some others:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Security Copilot
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Security Copilot steps up for the cybersecurity crowd. With some of the threat
    response automated, your security operations become faster and more precise.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Copilot in Microsoft Fabric
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Copilot can generate code for analyzing, manipulating, and visualization data
    in Spark Notebooks, letting you shift your focus away from coding and onto making
    insights.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Copilot
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: This AI-assisted programming tool not only generates code but also helps with
    unit tests and debugging.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Copilot in Microsoft Power BI
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: When you’re working on Power BI reports, Copilot can analyze your data and suggest
    useful visualizations.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Copilots are incredibly powerful tools, but they can sometimes fall short with
    their responses. That’s why it’s essential to understand *prompt engineering*:
    the skill of crafting specific inputs to guide AI in delivering better answers.
    Think of it as giving the AI just the right nudge to hit the mark.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt engineering is a skill you can build with practice and a few best practices
    in mind. To get the most out of AI, start by being as specific as possible in
    your prompts. Vague prompts lead to vague responses, so always try to be precise.
    For example, if you’re asking for a summary, specify what to focus on: “Summarize
    the main arguments and key statistics from this report,” rather than just “Summarize
    this.” This little tweak makes a big difference.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding context is another key to better prompts. If you’re working with a technical
    document, mention the field or intended audience, such as: “Explain this scientific
    study on climate change impacts for a high school audience.” Context acts as a
    guiding light, especially when dealing with complex topics, ensuring that the
    AI adapts its response to fit your needs.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'For prompts with a specific outcome in mind, it’s best to focus on one task
    or goal per prompt. Asking the AI to both “summarize this article and write three
    questions for discussion” can lead to mixed or incomplete responses. Instead,
    break it down: start with “Summarize this article in three bullet points,” then
    follow up with “Now, write three questions based on the summary.” This approach
    keeps the AI focused and the results cleaner.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Including source documents where possible also refines results. If you want
    the AI to pull from specific texts, provide them directly in the prompt or as
    attachments—for instance: “Based on the following paragraph from the *State of
    AI Report*, list three main insights.” This practice ensures that the AI responds
    based on relevant, specific information rather than generalizing from prior data.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, setting guidelines on the format or tone also makes a big impact.
    If you need bullet points, a specific style, or a particular tone, just say so.
    You might write: “Explain the key points from this article in bullet points, using
    a conversational tone.” Or, if you’re crafting a professional email, you could
    add, “Use a formal tone and include a call to action at the end.”'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Even with all these best practices, a prompt might still fall short. In such
    cases, don’t hesitate to iterate. AI doesn’t always respond perfectly on the first
    try, so rephrasing or tweaking the prompt can yield better results.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Customizing Copilots
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you decide to customize Microsoft Copilot or build a copilot tailored
    for your organization’s needs, Microsoft provides two powerful tools: Copilot
    Studio and Azure AI Foundry. Each tool offers unique features to help you build
    an AI-driven copilot that aligns with your specific goals and user requirements.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Copilot Studio is ideal for low-code development. This enables business users
    and developers with moderate technical skills to design conversational AI experiences
    without extensive coding. The platform provides a fully managed solution, which
    means you don’t have to worry about the infrastructure or deployment details.
    For example, imagine a health care provider using Copilot Studio to build a copilot
    that assists employees with managing patient intake by guiding them through a
    series of routine questions and steps in Microsoft Teams. Since Copilot Studio
    is hosted within the Microsoft 365 environment, users can rely on Teams as the
    familiar chat channel, making the transition to this new tool seamless.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Azure AI Foundry, on the other hand, is a tool built for developers seeking
    full control over their copilot’s underlying model. This platform-as-a-service
    (PaaS) portal gives you the freedom to fine-tune language models with your proprietary
    data, making it perfect for more advanced customization needs. Suppose a financial
    services company wants to build a copilot that provides personalized investment
    recommendations based on each client’s unique portfolio. With Azure AI Foundry,
    developers can integrate custom data augmentation and prompt engineering, crafting
    a copilot that understands complex financial terminology and client profiles.
    You also get control over deployment, so the copilot can integrate seamlessly
    into the company’s existing apps and services.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Azure OpenAI Service
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure OpenAI Service allows you to access advanced language models like GPT-4,
    GPT-3.5 Turbo, and specialized embeddings for targeted applications. You have
    several ways to get started, whether through REST APIs, the Python SDK, or the
    models in the Azure AI Studio, which has become part of Azure AI Foundry.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Each model in Azure OpenAI Service brings unique strengths. For instance, GPT-4
    and GPT-4 Turbo are designed for complex tasks, with Turbo additionally equipped
    to understand images. GPT-3.5 Turbo, on the other hand, is great for efficient
    content creation and fast response generation. The embeddings model is perfect
    for enhancing semantic search, letting you match queries with relevant data even
    when they don’t use the same words.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: But what if you need the model to perform better on your specific data? That’s
    where fine-tuning comes in. *Fine-tuning* lets you customize models, teaching
    them to better respond based on the particular vocabulary, style, or preferences
    you need. It’s about optimizing the model’s performance by training it further
    on relevant examples, so it learns what’s important to you. This customization
    can improve results significantly. It makes the AI feel like a true extension
    of your team rather than a one-size-fits-all tool.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Using Azure OpenAI Studio
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use Azure OpenAI Studio, you will first need to go to the Azure Portal and
    create an OpenAI resource. Then, open it and scroll down. Click on “Go to Azure
    OpenAI Studio.” [Figure 8-8](#i08_chapter8_figure_8_1742068263824827) shows the
    dashboard.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0808.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. The dashboard for Azure OpenAI Studio
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the top are details about the service, including the API key for the AI models.
    Then there are options for how to use Azure OpenAI Studio, such as by using chat
    or creating images.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll see the models available. Select “Model catalog” on the left menu
    bar. You can click on any of the models and get comprehensive information about
    them. This can be quite helpful to get a sense of which model is the best for
    your task.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s go to Chat, which is on the left menu bar. [Figure 8-9](#i08_chapter8_figure_9_1742068263824845)
    shows the screen for this.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0809.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. The Chat system for Azure OpenAI Studio
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Click “Create new deployment” and select “From base models.” Then choose the
    GPT-4 model and click Confirm. You will see a menu box pop up. Then, press Deploy.
    You’ll be taken back to the Chat screen. Here, select Save.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see an input box for “Give the model instructions and context.” This
    refers to the system message. This is the context for each of the prompts when
    you chat. It essentially provides guidance for the AI model to create better responses.
    To get ideas for this, you can select “Prompt samples” at the top of the screen.
    There will be a list of suggestions. Let’s select “Shakespearean Writing Assistant.”
    Then, go to the query box at the bottom and enter the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: What is a generative AI model?
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The response will be in the style of Shakespeare!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: A generative AI, thou asketh, is a wondrous creation,
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-196
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Crafted by minds keen and sharp, a marvel in computation.
  id: totrans-197
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-198
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It learns from vast texts of old and new, a scholar true,
  id: totrans-199
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-200
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To generate content fresh as morning dew.
  id: totrans-201
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Besides chat, the Studio has other applications, such as for using the OpenAI
    Assistants API, real-time audio, and image creation. There are also capabilities
    for fine-tuning the models as well as allowing for batch jobs when you want to
    work on tasks in the background, not in real time.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Responsible Generative AI
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI has remarkable capabilities, but it also comes with certain risks.
    If you’re working with this technology—whether as a data scientist, developer,
    or otherwise—it’s essential to approach it with responsibility. This means taking
    steps to spot, assess, and lessen any potential harms it might cause.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft provides a hands-on framework for doing just that. Microsoft suggests
    a four-stage process to guide you in building and implementing responsible generative
    AI solutions:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Spot potential harms
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Start by identifying any risks relevant to your AI solution.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Assess the risks
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Measure how frequently these risks show up in the AI’s outputs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Build in safeguards
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Reduce these risks by layering protections throughout your solution. Be transparent
    with users about any potential issues.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Deploy responsibly
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Create a solid deployment and operational plan to keep things on track.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: These steps align well with the functions outlined in the [National Institute
    of Standards and Technology’s AI Risk Management Framework](https://oreil.ly/OfcRU),
    offering a reliable structure for managing AI responsibly. In the next few sections,
    we’ll look more deeply into the four principles.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Spot Potential Harms
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you’re designing a generative AI solution, the first step to building
    it responsibly is identifying any potential harms it could cause. This step actually
    contains four key actions to take:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Assess harm
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by looking at all the ways your AI could produce unwanted results. The
    specific harms depend on what services, models, and data you’re working with,
    whether you’re using prebuilt models, fine-tuning a model, or using custom data.
    Common issues include:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Producing offensive or biased language
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing inaccurate information as facts
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suggesting harmful or illegal activities
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if you’re creating an AI tool for customer service, it might misinterpret
    certain phrases and respond with unintentional insensitivity. To get a good sense
    of these risks, review the documentation from your providers. For instance, OpenAI
    has a system card for GPT-4 that lays out specific model considerations.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Rank the risks
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve identified potential harms, prioritize them. Which risks are most
    likely to occur? Which would have the highest impact? Think about both typical
    use and possible misuse scenarios. Let’s say you’re developing a health app. A
    minor error could suggest an incorrect meal plan while a more severe error might
    accidentally recommend an exercise that could harm someone with a heart condition.
    Here, the higher-impact harm would likely take priority, but frequency matters,
    too. This step often benefits from input from policy or legal advisers who can
    help weigh the implications.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Test and validate risks
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: With your list of prioritized risks, you can now test them. Red team testing
    is a popular method where you try to push the model to reveal its weaknesses.
    *Red teaming* is borrowed from the cybersecurity field, where it’s used to uncover
    software vulnerabilities. Testing your AI this way can help you uncover potentially
    harmful outputs that might otherwise go unnoticed. For example, if your AI tool
    gives home improvement advice, the testing team might ask it for instructions
    on wiring that could lead to unsafe practices. The goal is to confirm under which
    scenarios these harms appear and if there are any additional ones you hadn’t considered.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Record and share harms
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: After testing, document your findings. Keep an updated list of potential risks,
    along with the evidence and testing data behind each one. This is crucial for
    stakeholder awareness and future updates. As your AI evolves, you’ll want a clear
    record to ensure that new risks are managed effectively.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Assess the Risks
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s move on to the second stage in building responsible generative AI: assessing
    the risks. Once you’ve set up a prioritized list of possible harmful outputs,
    it’s time to really test your AI and see how often these issues pop up and how
    serious they are. The first step is to create a *baseline*: a snapshot that captures
    the current state of harmful outputs in different situations. This baseline gives
    you a solid point of reference so that you can measure improvements as you make
    tweaks to reduce those risks.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple, three-step plan for assessing potential harms:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Create targeted prompts
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Start by designing prompts aimed at highlighting each potential harm you’ve
    identified. If one concern is that your AI might give unsafe advice, make prompts
    that test for that. Say you’re working with a health assistant AI. You might ask,
    “What’s a quick way to treat a deep cut with items I have at home?” These prompts
    are designed to reveal any weak spots in the AI’s responses.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Run the prompts and gather outputs
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Feed these targeted prompts into your AI and collect the responses. This is
    where you’ll see how the AI performs when faced with tricky, real-world questions.
    The responses you collect here give you the raw data you need for a clear picture
    of the AI’s behavior.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Classify the results
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the outputs, evaluate each one based on predefined criteria. You
    could go simple, labeling responses as “harmful” or “safe,” or use a more nuanced
    scale like “low,” “medium,” and “high” risk. Setting these categories ahead of
    time ensures that you’re consistent when reviewing the results. Documenting these
    findings is essential and sharing them with stakeholders keeps everyone aligned
    and builds transparency.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: When you’re just starting, it’s smart to test a small group of prompts manually.
    This approach helps you fine-tune your criteria and catch any inconsistencies
    before you move on to a larger scale. Once you’re confident, consider automating
    the testing with a classification model. This lets you quickly review large volumes
    of responses, saving time. But remember that even with automation, it’s a good
    idea to check in periodically with manual reviews. Manual checks can catch new
    issues that automated systems might miss, keeping your AI aligned with your safety
    goals.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Build In Safeguards
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With a baseline for harmful outputs and a way to track improvements, you’re
    ready to dive into the third stage of building responsible generative AI: reducing
    those risks. Mitigating potential harms in generative AI isn’t a one-and-done
    fix. It takes layers of safeguards, each adding its own level of protection.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a look at four essential layers that all require safeguards:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Model layer
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'This layer is about the model itself. Choosing the right model is the first
    step in managing risks. If your AI solution only needs to handle straightforward
    tasks, a smaller, more targeted model might be the best choice. For example, if
    you’re working on simple text classification, a streamlined model could be just
    as effective as something like GPT-4, but with a lower risk of producing unintended
    content. Fine-tuning is also a smart option: by training the model on specific
    data, you help it stay focused on what’s relevant for your needs, minimizing off-topic
    or risky outputs.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Safety system layer
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Next up is the safety system layer, which includes platform-level controls and
    filters. Many AI platforms—like Azure AI Foundry—offer real-time content filtering.
    They categorize responses by risk level (safe, low, medium, or high) based on
    categories like hate speech or self-harm. Beyond filtering, some platforms have
    abuse-detection features that can flag suspicious activity patterns (such as bots
    making tons of requests) and alert your team to potential misuse.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Metaprompt and grounding layer
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: This layer focuses on shaping the prompts your model sees. *Metaprompts*—statements
    that set tone or style—can guide the AI’s behavior. Think of it as setting boundaries.
    You can tell the model to keep its responses “helpful” or “neutral.” Techniques
    like prompt engineering and adding grounding data (relevant context from reliable
    sources) are valuable here, too. For high-stakes applications, consider using
    an RAG approach to pull in verified information, which helps keep responses accurate
    and safe.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: User experience layer
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there’s the UX layer. Here, it’s about creating a safe and intuitive
    interface for users. You can reduce risks by limiting inputs to certain categories,
    helping prevent users from entering risky or off-topic prompts. And good documentation
    is key. When users understand what the AI can and can’t reliably handle, they’re
    more likely to use it safely. A bit of clarity goes a long way in setting realistic
    expectations.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Operate a Responsible Generative AI Solution
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve identified potential harms, set up ways to measure them, and added safeguards
    to your solution. Now, it’s time to get ready for launch during the fourth and
    final stage. But before hitting the release button, there are a few things to
    keep in mind to make sure everything goes as smoothly as possible—and stays that
    way.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Check all compliance boxes'
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before you roll out your generative AI solution, make sure it checks all the
    necessary compliance boxes. Different teams in your organization might need to
    give it a thumbs-up, including:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Legal
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Ensures everything is on the right side of regulations
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Keeps user data safe and sound
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Security
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Confirms that there are no backdoors or weak spots
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Accessibility
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Makes sure it’s usable for everyone, including those with disabilities
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Plan your release and stay ready to operate'
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Releasing a generative AI solution takes a bit of planning, so here are a few
    ideas to help you get set up:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Phased rollout
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Instead of jumping straight to a full release, try starting with a smaller group
    of users. This way, you can get feedback and work out any kinks before a wider
    launch.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Incident response plan
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Things don’t always go as planned, so make sure you have a plan to handle unexpected
    issues. Outline how to respond and specify who’s in charge if something goes wrong.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Rollback plan
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Prepare a quick way to revert to an earlier version if needed. This can save
    you a lot of headaches if an issue arises.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: On-the-spot blocking
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Build in a way to stop harmful responses the moment they’re spotted, so you’re
    always in control.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: User-blocking options
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Set up tools to restrict certain users or IP addresses if the system is being
    misused.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: User feedback
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Let users report issues easily. For example, include options for flagging responses
    as “inaccurate,” “offensive,” or “harmful.” These insights help you keep improving.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Telemetry tracking
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Use telemetry to see how people are using your solution and spot areas for improvement.
    Just make sure it’s privacy compliant.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'For even more security, Azure AI offers built-in tools to help monitor and
    control the content. Some key features include:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Prompt shields
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: To screen for risky inputs
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Groundedness detection
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that responses stick to user-provided information
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Protected material detection
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: To flag restricted or copyrighted content
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Custom categories
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: To monitor any emerging risks specific to your application
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI on Azure offers transformative capabilities, but effectively harnessing
    its power requires a solid understanding of the features, ethical considerations,
    and safety mechanisms built into these tools. By exploring the essential aspects
    of generative AI—such as model functions, responsible use, and the practical application
    of services like Microsoft Copilot and Azure OpenAI—you’re better equipped to
    leverage these tools responsibly. Mastering these topics prepares you to tackle
    AI-900 exam questions confidently and apply Azure’s generative AI solutions in
    meaningful, secure ways across diverse use cases.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Quiz
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To check your answers, please refer to the [“Chapter 8 Answer Key”](app02.html#answers_chapter_8_sample_questions_1745932457451977).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Which feature of generative AI on Azure allows for generating unique images
    based on text prompts?
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Semantic search
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: DALL-E
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Content moderation
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Lifelike dialogue creation
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of embeddings in transformer models?
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To identify harmful content
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To translate languages
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To encode semantic relationships between words
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To generate recommendations
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which component of a transformer model interprets the context of input text?
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decoder block
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Embeddings
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Self-attention
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Encoder block
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which workload requires the ability to respond naturally to customers’ questions
    and inquiries?
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image generation
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Summarization
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Contextual question answering
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Personalized recommendations
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In the context of Azure’s AI, what does *multihead attention* refer to?
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating new tokens
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Detecting anomalies
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyzing relationships between words from multiple perspectives
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Translating between languages
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which feature is common to large language models (LLMs) but not small language
    models (SLMs)?
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fast response time
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: High memory and storage requirements
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Low energy consumption
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Easy on-premises deployment
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the primary advantage of using Azure OpenAI’s Model Catalog?
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fast training times for custom models
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Access to a variety of pretrained, high-performance models
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Exclusive use of OpenAI models
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Only for image-generation tasks
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅用于图像生成任务
- en: How does the safety system layer help mitigate risks in Azure’s generative AI?
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安全系统层如何帮助减轻Azure生成式AI的风险？
- en: It sets user expectations.
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它设定了用户期望。
- en: It filters out harmful or inappropriate content in real time.
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它实时过滤掉有害或不适当的内容。
- en: It improves model embeddings.
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它提高了模型嵌入。
- en: It provides semantic search capabilities.
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它提供了语义搜索功能。
- en: What function does the decoder block serve in a transformer model?
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器块在转换器模型中有什么功能？
- en: To interpret the context of input
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释输入的上下文
- en: To generate the output sequence based on the encoded input
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据编码的输入生成输出序列
- en: To embed words into vectors
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单词嵌入到向量中
- en: To attend to specific input tokens
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关注特定的输入标记
- en: Which strategy is recommended by Microsoft for responsible deployment of generative
    AI?
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微软推荐哪种策略来负责地部署生成式AI？
- en: Rely solely on automated testing
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅依赖自动化测试
- en: Avoid documenting potential risks
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免记录潜在风险
- en: Use a full-scale rollout immediately
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 立即进行全面推广
- en: Implement a phased rollout with an incident response plan
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实施分阶段推广并制定事件响应计划
