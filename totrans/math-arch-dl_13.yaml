- en: 14 Latent space and generative modeling, autoencoders, and variational autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Representing inputs with latent vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometrical view, smoothness, continuity, and regularization for latent spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA and linear latent spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders and reconstruction loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variational autoencoders (VAEs) and regularizing latent spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapping input vectors to a transformed space is often beneficial in machine
    learning. The transformed vector is called a *latent vector*‚Äîlatent because it
    is not directly observable‚Äîwhile the input is the underlying *observed vector*.
    The latent vector (aka embedding) is a simpler representation of the input vector
    where only features that help accomplish the ultimate goal (such as estimating
    the probability of an input
  prefs: []
  type: TYPE_NORMAL
- en: 'belonging to a specific class) are retained, and other features are forgotten.
    Typically, the latent representation has fewer dimensions than the input: that
    is, encoding an input into a latent vector results in *dimensionality reduction*.'
  prefs: []
  type: TYPE_NORMAL
- en: The mapping from input to latent space (and vice versa) is usually learned‚Äîwe
    train a machine, such as a neural network, to do it. The latent vector needs to
    be as faithful a representation as possible of the input within the dimensionality
    allocated to it. So, the neural network is incentivized to minimize the loss of
    information caused by the transformation. Later, we see that in autoencoders,
    this is achieved by reconstructing the input from the latent vector and trying
    to minimize the difference between the actual and reconstructed input. However,
    given the reduced number of dimensions, the network does not have the luxury of
    retaining everything in the input. It has to learn what is essential to the end
    goal and retain only that. Thus the embedding is a compact representation of the
    input that is streamlined to achieve the ultimate goal.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1 Geometric view of latent spaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider the space of all digital images of height *H*, width *W*, with each
    pixel representing a 24-bit RGB color value. This is a gigantic space with (2^(24))*^(HW)*
    points. Every possible RGB √ó *H* √ó *W* image is a point in this space. But if
    an image is a natural image, neighboring points tend to have similar colors. This
    means points corresponding to natural images are correlated: they are not distributed
    uniformly over the space of possible images. Furthermore, if the images have a
    common property (say, they all giraffes), the corresponding points form clusters
    in the (2^(24))*^(HW)*-sized input space. In stochastic parlance, the probability
    distribution of natural images with a common property over the space of possible
    images is highly non-uniform (low entropy).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [14.1a](#fig-linear-latent-subspace) illustrates points with some common
    property clustered around a planar manifold. Similarly, figure [14.1b](#fig-non-inear-latent-subspace)
    illustrates points with some common property clustered around a curved manifold.
    These points have a common property. At the moment, we are not interested in what
    that property is or whether the manifold is planar or curved. All we care about
    is that these points of interest are distributed around a manifold. The manifold
    captures the essence of that common property, whatever it is. If the common property
    is, say, the presence of a giraffe in the image, then the manifold captures *giraffeness*:
    the points on or near the manifold all correspond to images with giraffes. If
    we travel along the manifold, we encounter various flavors of giraffe photos.
    If we go far from the manifold‚Äîthat is, travel a long distance in a direction
    orthogonal to the manifold‚Äîthe probability of the point representing a photo with
    a giraffe is low.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F01a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Planar latent subspaces
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F01b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Curved latent subspace
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 Two examples of latent subspaces, with planar and curved manifolds,
    respectively. The solid line shows the latent vector, and the dashed line represents
    the information lost by projecting onto the latent subspace.
  prefs: []
  type: TYPE_NORMAL
- en: Given training data consisting of sampled points of interest (such as many giraffe
    photos), we can train a neural network to learn this manifold‚Äîit is the optimal
    manifold that minimizes the average distance of all the training data points from
    the manifold. Then, at inference time, given an arbitrary input point, we can
    estimate its distance from the manifold, giving us the probability of that input
    satisfying the property represented by the manifold.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the input vector can be decomposed into an in-manifold component (solid
    line in figure [14.1](#fig-latent-subspaces)) and an orthogonal-to-manifold component
    (dashed line in figure [14.1](#fig-latent-subspaces)). Latent space modeling effectively
    eliminates the orthogonal component and retains the in-manifold component as the
    latent vector (aka embedding). Equivalently, we are projecting the input vector
    onto the manifold. This is the core idea of latent space modeling‚Äîwe learn a manifold
    that represents a property of interest and represents all inputs by a latent vector,
    which is the input point‚Äôs projection onto this manifold. The latent vector is
    a more compact representation of the input where only information related to the
    property of interest is retained.
  prefs: []
  type: TYPE_NORMAL
- en: Latent space modeling in a nutshell
  prefs: []
  type: TYPE_NORMAL
- en: In latent space modeling, we train a neural network to represent a manifold
    around which the input points satisfying a property of interest are distributed.
    The property of interest could be membership in a specific class, such as images
    containing a giraffe. Thus, the learned manifold is a collection of points that
    satisfy the property. The input point is projected onto this manifold to obtain
    a latent vector representation of the input (aka embedding). This is equivalent
    to throwing away the input vector component that is orthogonal to the manifold.
    The eliminated component is orthogonal to the manifold and hence unrelated to
    the property of interest (may represent background pixels of the image), so the
    information loss caused by the projection does not hurt. We have created a less
    noisy, more compact representation of the input that focuses on the things we
    care about.
  prefs: []
  type: TYPE_NORMAL
- en: Training data consists of a set of sampled data inputs, all satisfying the property
    of interest. The system essentially learns the manifold, which is optimally located
    to minimize its average distance from all the training data points. During inferencing,
    given an arbitrary input point, its distance from the manifold is an indicator
    of the probability of that input satisfying the property of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'A subtle point is that the latent vector is the in-manifold component of the
    original point‚Äôs position vector. By switching to the latent vector representation,
    we lose the location of the point in the original higher-dimensional input space.
    We can go back to the higher-dimensional space by providing the location of the
    manifold for the lost orthogonal component, but doing so does not recover the
    original point: it recovers only the projection of the original point onto the
    subspace. We are replacing the individual orthogonal components with an aggregate
    entity the location of a manifold) but do not recover the exact original point.
    Some information is irretrievably lost during projection.'
  prefs: []
  type: TYPE_NORMAL
- en: A special case of latent space representation is principal component analysis
    (PCA), introduced in section [4.4](../Text/04.xhtml#sec-pca) (section [14.4](#sec-pca-recap)
    provides a contextual recap of PCAs). It projects input points to an optimal planar
    latent subspace (as in figure [14.1a](#fig-linear-latent-subspace)). But except
    for some lucky special cases, the best latent subspace is not a hyperplane. It
    is a complex curved surface (see figure [14.1b](#fig-non-inear-latent-subspace)).
    Neural networks, such as autoencoders, can learn such nonlinear projections.
  prefs: []
  type: TYPE_NORMAL
- en: 14.2 Generative classifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During inferencing, the supervised classifiers we have encountered in previous
    chapters typically emit the class to which an input belongs, perhaps along with
    a bounding box. This is somewhat black-box-like behavior. We do not know how well
    the classifier has mastered the space except through the quantized end results.
    Such classifiers are called *discriminative* classifiers. On the other hand, latent
    space models map arbitrary input points to probabilities of belonging to the class
    of interest. Such models are called *generative* models, and they have some desirable
    properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F02a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) A good discriminative classifier‚Äîsmooth decision boundary
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F02b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) A bad discriminative classifier‚Äîirregular decision boundary
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F02c_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Generative model‚Äîno decision boundary (heat map indicates the probability
    density)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.2 Solid circles indicate training data points (all belonging to the
    class of interest). The dashed curve indicates the decision boundary separating
    the class of interest from the class of non-interest. In a generative model, there
    is no decision boundary. Every point in the space is associated with a probability
    of belonging to the class of interest (indicated as a heat map in figure [14.2c](#fig-generative-model))
  prefs: []
  type: TYPE_NORMAL
- en: NOTE We can always create a discriminative classifier from a generative classifier
    by putting a threshold on the probability.
  prefs: []
  type: TYPE_NORMAL
- en: '*Smoother, denser manifolds*‚ÄîDiscriminative models learn decision boundaries
    separating data points of interest from those not of interest in the input space.
    On the other hand, generative models try to model the distribution of the data
    points of interest in the input space using smooth probability density functions.
    As such, the generative model can''t learn a very irregularly shaped function
    that overfits the training data. This is illustrated in figure [14.2](#fig-discriminative-generative-model),
    whereas the discriminative model may converge to a manifold that follows the nooks
    and bends of the training data too closely (overfits) as in figure~\ref{fig-discriminative-bad-model}.
    This difference between discriminative and generative classifiers becomes especially
    significant when we have less training data. We can always create a discriminative
    classifier from a generative classifier by putting a threshold on the probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Extra insight*‚ÄîGenerative models offer more insight into the inner workings
    of the model. Consider a model that recognizes horses. Suppose we feed some horse
    images to the model, and it calls them horses (good). Then we feed the model some
    zebra images, and it calls them horses, too (bad). Do we have a useless~model
    that calls everything a horse? If it is a discriminative model, we must test it
    with totally different images (say, bird images) to get the answer. But if we
    have a generative model, it says the probabilities of the true horse images are,
    say, 0.9 and above, while the probabilities for the zebra images are around 0.7\.
    We begin to see that the model is behaving reasonably and does realize that zebras
    are less "horsy" than real horses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*New class instances*‚ÄîA generative model learns the *distribution* of input
    points belonging to the class. An advantage related to learning the distribution
    is that we can sample the distribution to generate new members of the class (for
    example, to generate artificial horse images). This leads to the name *generative*
    modes. If we train a generative model with writings of Shakespeare, it will emit
    Shakespeare-like text pieces. Believe it or not, this has been tried with some
    success.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.3 Benefits and applications of latent-space modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs recap at a high level why we want to do latent-space modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Generative models are often based on latent space models*‚Äîall the benefits
    of generative modeling as outlined in section 14.2 apply to latent space modeling
    too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attention to what matters*‚ÄîRedundant information that does not contribute
    to the end goal is eliminated, and the system focuses on truly discriminative
    information. To visualize this, imagine an input data set of police mugshots consisting
    of people standing in front of the same background. Latent-space modeling trained
    to recognize people typically eliminates the common background from the representation
    and focuses on the photograph‚Äôs subject matter (people).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Streamlined representation of data*‚ÄîThe latent vector is a more compact representation
    of the input vector (reduced dimensions and hence smaller) with no meaningful
    information lost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Noise elimination*‚ÄîLatent-space modeling eliminates the low-variance orthogonal-to-latent-subspace
    component of the data. This is mostly data that does not help in the problem of
    interest and hence is noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transformation to a manifold that is friendlier toward the end goal*‚ÄîWe have
    seen this notion previously, but here let‚Äôs look at an interesting simple example.
    Consider a set of 2D points in Cartesian coordinates (*x*, *y*). Suppose we want
    to classify the points into two sets: those that lie *inside* the circle *x*¬≤
    + *y*¬≤ = *a*¬≤ and those that lie *outside* the circle. In the original Cartesian
    space, the decision boundary is not linear (it is circular). But if we transform
    the Cartesian input points to a latent space in polar coordinates‚Äîthat is, each
    (*x*, *y*) is mapped to (*r*, *Œ∏*) such that *x* = *rcos*(*Œ∏*), *y* = *rsin*(*Œ∏*)‚Äîthe
    circle transforms into a line *r* = *a* in the latent space . A simple linear
    classifier *r* = *a* in the latent space can achieve the desired classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some applications of latent-space modeling are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating artificial images or text (as explained in the context of generative
    modeling).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Similarity estimation between inputs*‚ÄîIf we map inputs to latent vectors,
    we can assess the similarity between inputs by computing the Euclidean distance
    between the latent vectors. Why is this better than taking the Euclidean distance
    between the input vectors? Suppose we are building a recommendation engine that
    suggests other clothing items ‚Äúsimilar‚Äù to the one a potential buyer is currently
    browsing. We want to retrieve other clothing items that look similar but not identical
    to the one viewed. But similarity is a subjective concept, not quite measurable
    via the similarity of the inputs‚Äô pixel colors. Consider a shirt with black vertical
    stripes on a white base. If we switch the stripe color with the base color, we
    get a shirt with white vertical stripes on a black base. If we do pixel-to-pixel
    color matching, these are very different, yet they are considered similar by humans.
    For this problem, we have to train the latent space model, creating neural networks
    so that images perceived to be similar by humans map to points in latent space
    that are close to each other. For example, both white-on-black and black-on-white
    shirts should map to latent vectors that are close to each other in the latent
    space even though they are far apart in the input space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Image or other data compression*‚ÄîThe latent vector approximates the data with
    a smaller-dimensional vector that mimics the original vector as faithfully as
    possible. Thus the latent vector is a lossy compressed representation of the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Denoising*‚ÄîThe latent vector eliminates the non-meaningful part of the input
    information, which is noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this chapter, executable via Jupyter Notebook,
    can be found at [http://mng.bz/6XG6](http://mng.bz/6XG6).
  prefs: []
  type: TYPE_NORMAL
- en: 14.4 Linear latent space manifolds and PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCAs (which we discussed in section [4.4](../Text/04.xhtml#sec-pca)) project
    input data onto linear hyperplanar manifolds. Revisiting this topic will set up
    the correct context for the rest of this chapter. Consider a set of 3D input data
    points clustered closely around the *X*[0] = *X*[2] plane, as shown in figure
    [14.3](#fig-pca-3d-recap).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F03a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Original 3D data
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F03b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Lower-dimensional 2D representation obtained by setting the third principal
    value to zero
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F03c_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) The principal vectors of the original data. The third principal vector is
    normal to *X*[0] = *X*[2] plane; the other two are in-plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.3 The original 3D data in figure [14.3a](#fig-pca-3d-recap-original-data)
    shows high correlation: points are clustered around the *X*[0] = *X*[2] plane.
    The first principal component corresponds to the direction of maximum variance.
    The last (third) principal reduced to a 2D latent vector.'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE We denote the successive axes (dimensions) as *X*[0], *X*[1], *X*[2] instead
    of the more traditional *X*, *Y*, *Z* for easy extension to higher dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using PCA, we can recognize that the data has low variation along some dimensions.
    When we do PCA, we get the principal value and principal vector pairs. The largest
    principal value corresponds to the direction of maximum variance in the data.
    The corresponding principal vector yields that direction, and that principal value
    indicates the magnitude of the variance along that direction. The next principal
    value, the principal vector pair, is the orthogonal direction with the next-highest
    variance, and so on. For instance, in figure [14.3](#fig-pca-3d-recap), the principal
    vectors corresponding to the larger two principal values lie in the *X*[0] = *X*[2]
    plane, while the smallest principal value corresponds to the normal-to-plane vector.
    The third principal value is significantly smaller than the others. This tells
    us that variance along that axis is low, and components along that axis can be
    dropped with relatively little loss of information: that is, low reconstruction
    loss. The variations along the small principal value axes are likely noise, so
    eliminating them cleans up the data. In figure [14.3](#fig-pca-3d-recap), this
    effectively projects the data onto the *X*[0] = *X*[2] plane.'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs: []
  type: TYPE_NORMAL
- en: PCA essentially projects inputs to the *best-fit plane* for the training data.
    Assuming all the training data points are sampled with a common property, this
    plane represents that common property. By projecting, we eliminate that common
    property and retain only the discriminating aspects of the data. The eliminated
    information is remembered *approximately* in the parameters of the plane and supplied
    during reconstruction (aka decoding) to map us back to the same dimensionality
    as the input (but not exactly the same point). This is the essence of dimensionality
    reduction via PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Following are the steps involved in PCA-based dimensionality reduction. This
    was described in detail with proofs in section [4.5](../Text/04.xhtml#sec-svd);
    here we recap the main steps without proof.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE This treatment is similar but not identical to that in section [4.5](../Text/04.xhtml#sec-svd).
    Here we have switched the variables *m* and *n* to be consistent with our use
    of *n* to denote the data instance count. We have also switched to a slightly
    different flavor of the SVD.
  prefs: []
  type: TYPE_NORMAL
- en: Represent the data as a matrix *X*, where each row is an individual data instance.
    The number of rows *n* is the size of the data set. The number of columns *d*
    is the original (input) dimensionality of the data. Thus *X* is a *n* √ó *d* matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the mean data vector
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-a.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: where ![](../../OEBPS/Images/AR_x.png)^((*i*)) for *i* = 1 to *i* = *n* denote
    the training data vector instances (which form rows of the matrix *X*).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Shift the origin of the coordinate system to the mean by subtracting the mean
    vector from each data vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png)^((*i*)) = ![](../../OEBPS/Images/AR_x.png)^((*i*))
    ‚Äì ![](../../OEBPS/Images/AR_micro.png) for all *i*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The data matrix *X* now has the mean-subtracted data instances as rows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The matrix *X^TX* where *X* is the mean-subtracted data matrix) is the covariance
    matrix (as discussed in detail in section [5.7.2](../Text/05.xhtml#sec-var-covar-std)).
    The eigenvalue, eigenvector pairs of the matrix *X^TX* are known as principal
    values and principal vectors (together referred to as principal components). Since
    *X^TX* is a *d* √ó *d* matrix, there are *d* scalar eigenvalues and *d* eigenvectors,
    each of dimension *d* √ó 1. Let‚Äôs denote the principal components as (*Œª*[1], ![](../../OEBPS/Images/AR_v.png)[1]),
    (*Œª*[2], ![](../../OEBPS/Images/AR_v.png)[2]), ‚ãØ, (*Œª[dm]*, ![](../../OEBPS/Images/AR_v.png)*[d]*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can assume *Œª*[1] ‚â• *Œª*[2] ‚â• ‚ãØ ‚â• *Œª[d]* if necessary, we can make this true
    by renumbering the principal components). Then the first principal component corresponds
    to the direction of maximum variance in the data (proof with geometrical intuition
    can be found in section [5.7.2](../Text/05.xhtml#sec-var-covar-std)). The corresponding
    principal value yields the actual variance. The next principal value corresponds
    to the second-highest variance (among directions orthogonal to the first principal
    direction), and so forth. For every component, the principal value yields the
    actual variance, and the principal vector yields the direction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider the matrix of principal vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*V* = [![](../../OEBPS/Images/AR_v.png)[1] ![](../../OEBPS/Images/AR_v.png)[2]
    ‚Ä¶ *![](../../OEBPS/Images/AR_v.png)[d]*]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If we want the data to be a space with *m* dimensions with minimal loss of information,
    we should drop the last *m* vectors of *V*. This eliminates the *m* least-variance
    dimensions. Dropping the last *m* vectors from *V* yields a matrix
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*V[d‚Äìm]* = [![](../../OEBPS/Images/AR_v.png)[1] ![](../../OEBPS/Images/AR_v.png)[2]
    ‚Ä¶ *![](../../OEBPS/Images/AR_v.png)[d‚Äìm]*]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that the best way to obtain the *V* matrix is to perform SVD on the mean-subtracted
    *X* (see section [4.5](../Text/04.xhtml#sec-svd)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Premultiplying *V[d‚àím]*, the truncated principal vectors matrix, with the original
    data matrix *X* projects the data onto a space corresponding to the first *d*
    ‚àí *m* principal components. Thus, to create *d* ‚àí *m*-dimensional linearly encoded
    latent vectors from *d*-dimensional data,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*X[d‚àím]* = *XV[d‚àím]*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*X[d‚àím]* is the reduced dimension data set. Its dimensionality is *n* √ó (*d*‚àí*m*).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It can be shown that
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*XV[d‚àím]* = *UŒ£[d‚àím]*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where *U* is from SVD (see section [4.5](../Text/04.xhtml#sec-svd)) and *Œ£[d‚àím]*
    is a truncated version of the diagonal matrix Œ£ from SVD with its smallest *m*
    elements chopped off. This offers an alternative way to do PCA-based dimensionality
    reduction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'How do we reconstruct? In other words, what is the decoder? Well, to reconstruct,
    we need to save the original principal vectors: that is, the *V* matrix. If we
    have that, we can introduce *m* zeros at the right of every row in *X[d‚àím]* to
    make it a *n* √ó *d* matrix again. Then we post-multiply by *V^T*, which rotates
    the coordinate system back from one with principal vectors as axes to one with
    the original input axes. Finally, we add the mean ![](../../OEBPS/Images/AR_micro.png)
    to each row to shift the origin back to its original position, which yields the
    reconstructed data matrix *XÃÉ*. The reconstruction loss is ||*X* ‚àí *XÃÉ*||¬≤. Note
    that, in effect, *XÃÉ* is *UŒ£V^T* with the last *m* diagonal elements of Œ£ set
    to zero.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reconstructed data *XÃÉ* is *not* identical to the original data. The information
    we lost during dimensionality reduction the normal-to-plane components) is lost
    permanently. Nonetheless, this principled way of dropping information ensures
    that the reconstruction loss is minimal in some sense, at least among all *XÃÉ*
    linearly related to *X*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 14.4.1 PyTorch code for dimensionality reduction using PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let‚Äôs implement dimensionality reduction in PyTorch. Let *X* be a data
    matrix representing points clustered around the *X*[0] = *X*[2] plane. *X* is
    of shape [1000,3], with each row of *X* representing a three-dimensional data
    point. The following listing shows how to project *X* into a lower-dimensional
    space with minimal loss of information. It also shows how to reconstruct the original
    data points from the lower-dimensional representations. Note that the reconstructions
    are approximate because we have lost information (albeit minimal) in the dimensionality-reduction
    process.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for dimensionality reduction using PCA, executable
    via Jupyter Notebook, can be found at [http://mng.bz/7yJg](http://mng.bz/7yJg).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.1 PyTorch- PCA revisited
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Data matrix of shape (1000, 3)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Stores the mean so we can reconstruct the original data points later
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Subtracts the mean before performing SVD
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Runs SVD
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Columns of V are the principal vectors.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Removes the last principal vector. This is along the direction of least variance
    perpendicular to *X*[0] = *X*[2] plane).
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Projects the input data points into the lower-dimensional space
  prefs: []
  type: TYPE_NORMAL
- en: ‚ëß Pads with zeros to make an *n* √ó *d* matrix
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë® Post-multiplies with *V^T* to project back to the original space
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë© Adds the mean
  prefs: []
  type: TYPE_NORMAL
- en: 14.5 Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Autoencoders are neural network systems trained to generate latent-space representations
    corresponding to specified inputs. They can do nonlinear projections and hence
    are more powerful than PCA systems see figure [14.4](#fig-curved-underlying-pattern-recap)).
    The neural network mapping the input vector to a latent vector is called an *encoder*.
    We also train a neural network called a *decoder* that maps the latent vector
    back to the input space. The decoder output is the reconstructed input from the
    latent vector. The reconstructed input (that is, the output of the decoder) will
    never match the original input exactly‚Äîinformation was lost during encoding and
    cannot be brought back‚Äîbut we can try to ensure that they match as closely as
    possible within the constraints of the system. The reconstruction loss is a measure
    of the difference between the original input and the reconstructed input. The
    encoder-decoder pair is trained end to end to minimize reconstruction loss (along
    with, potentially, some other losses). This is an example of *representation learning*,
    whereby we learn to represent input vectors with smaller latent vectors representing
    the input as closely as possible in the stipulated size budget. The budgeted size
    of the latent space is a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F04_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 A 2 data distribution with a curved underlying pattern. It is impossible
    to find a straight line or vector such that all points are near it. PCA will not
    do well.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE A hyperparameter is a neural network parameter that is *not* learned. Its
    value is set based on our knowledge of the system and held constant during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The desired output is implicitly known in autoencoders: it is the input. Consequently,
    no human labeling is needed to train autoencoders; they are *unsupervised*. An
    autoencoder is shown schematically in figure [14.5](#fig-ae-schematic).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F05_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.5 Schematic representation of an autoencoder. The encoder transforms
    input into a latent vector. The decoder transforms the latent vector into reconstructed
    input. We minimize the reconstruction loss‚Äîthe distance between the reconstructed
    input and the original input.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder takes an input ![](../../OEBPS/Images/AR_x.png) and maps it to a
    lower-dimensional latent vector ![](../../OEBPS/Images/AR_z.png). An example of
    an encoding neural network for image inputs is shown in listing [14.2](#code-ae-encoder).
    Note how the image height and width keep decreasing with each successive sequence
    of convolution, ReLU, and max pool layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decoder is a neural network that generates reconstructed image ![](../../OEBPS/Images/AR_x3.png)
    from the latent vector ![](../../OEBPS/Images/AR_z.png). Listing [14.3](#code-ae-decoder)
    shows an example of a decoder neural network. Note the transposed convolutions
    and how the height and width of the image keep increasing with each successive
    sequence of transposed convolution, batch normalization, and ReLU. Transposed
    convolutions are discussed in section [10.5](../Text/10.xhtml#sec-transposed-conv).)
    The decoder essentially remembers‚Äînot exactly, but in an average sense‚Äîthe information
    discarded during encoding. Equivalently, it remembers the position of the latent
    space manifold in the overall input space. Adding that back to the latent space
    representation takes us back to the same dimensionality as the input vector but
    not to the same input point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The system minimizes the information loss from the encoding (the reconstruction
    loss). We are ensuring that for each input, the corresponding latent vector produced
    by the encoder can be mapped back to a reconstructed value by the decoder that
    is as close as possible to the input. Equivalently, each latent vector is a faithful
    representation of the input, and there is a 1 : 1 mapping between inputs and latent
    vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder and decoder need not be symmetric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematically,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-b.png)'
  prefs: []
  type: TYPE_IMG
- en: The end-to-end system is trained to minimize the loss ‚Ñí.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for autoencoders, executable via Jupyter Notebook,
    can be found at [http://mng.bz/mOzM](http://mng.bz/mOzM).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.2 PyTorch- Autoencoder encoder
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Input image size in (c, h, w) format
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Reduces to a (32, 16, 16)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Reduces to a (128, 8, 8)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Reduces to a (256, 4, 4)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Flattens to a 4096-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Reduces the 4096-sized tensor to an nz-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.3 PyTorch- Autoencoder decoder
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Converts (nz, 1, 1) to a 256, 4, 4)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Increases to a 128, 8, 8)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Increases to a 32, 16, 16)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Increases to a 1, 32, 32)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.4 PyTorch- Autoencoder training
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Passes the input image through the convolutional encoder
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Reduces to nz dimensions
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Reconstructs the image using z via the decoder
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Computes the reconstruction loss
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.1 Autoencoders and PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is important to realize that autoencoders perform a much more powerful dimensionality
    reduction than PCA. PCA is a linear process; it can only project data points to
    best-fit hyperplanes. Autoencoders can fit arbitrary complex nonlinear hypersurfaces
    to the data, limited only by the expressive powers of the encoder-decoder pair.
    If the encoder and decoder have only a single linear layer (no ReLU or other nonlinearity),
    then the autoencoder projects the data points to a hyperplane like PCA not necessarily
    the same hyperplane).
  prefs: []
  type: TYPE_NORMAL
- en: 14.6 Smoothness, continuity, and regularization of latent spaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Minimizing the reconstruction loss does not yield a unique solution. For instance,
    figure [14.6](#fig-latent-space-regularization) shows two examples of transforming
    2D inputs into 1D latent-space representations, linear and curved, respectively.
    Both the regularized solid line) and the non-regularized zigzag manifold (dashed
    line) fit the training data well with low reconstruction error. But the former
    is smoother and more desirable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F06a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Linear latent space
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F06b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Curved latent space
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.6 Two examples of mapping from a 2D input space to a 1D latent space.
    Both show regularized solid) vs. unregularized (dashed) latent space manifolds.
    Solid little circles depict training data points.
  prefs: []
  type: TYPE_NORMAL
- en: Note the pair of points marked *p*[1], *p*[2] and *p*[3], *p*[4](square markers).
    The distance between them is more or less the same in the input space. But when
    projected on the dashed curve (unregularized latent space), their distances (measured
    along the curve) become quite different. This is undesirable and does not happen
    in the regularized latent space (here, the distance is measured along a solid
    line). It becomes much more pronounced in high dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The zigzag curve segment containing the training data set is longer than the
    smooth one. A good latent manifold typically has fewer twists and turns (is smooth)
    and hence has a ‚Äúlength‚Äù that is minimal in a sense. This is reminiscent of the
    minimum descriptor length (MDL) principle, which we discussed in section [9.3.1](../Text/09.xhtml#sec-MDL).
  prefs: []
  type: TYPE_NORMAL
- en: How do we ensure that this smoothest latent space is chosen over others that
    also minimize the reconstruction loss? By putting additional constraints (losses)
    over and above the ubiquitous reconstruction loss. Recall the notion of regularization,
    which we looked at in sections [6.6.3](../Text/06.xhtml#sec-MAP_estimation) and
    [9.3](../Text/09.xhtml#sec-regularization). There we introduced an explicit loss
    that penalizes longer solutions (which was equivalent to maximizing the a posteriori
    probability of parameter values as opposed to the likelihood). A related approach
    that we explore in this chapter is to model the latent space as a probability
    distribution belonging to a known family (for example, Gaussian) and minimize
    the difference (KL divergence) of this estimated distribution from a zero-mean
    univariance Gaussian. The encoder-decoder neural network pair is trained end to
    end to minimize a loss that is a weighted sum of the reconstruction loss and this
    KL divergence. Trying to remain close to the zero-mean unit-variance Gaussian
    penalizes departure from compactness and smoothness. This is the basic idea of
    variational autoencoders VAEs).
  prefs: []
  type: TYPE_NORMAL
- en: The overall effect of regularization is to create a latent space that is more
    compact. If we only minimize the reconstruction loss, the system can achieve that
    by mapping points very far from each other (space being infinite). Regularization
    combats that and incentivizes the system to not map the training points too far
    from one another. It tries to limit the total latent-space volume occupied by
    the points corresponding to the training inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 14.7 Variational autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'VAEs are a special case of autoencoders. They have the same architecture: a
    pair of neural networks that encode and decode the input vector, respectively.
    They also have the reconstruction loss term. But they have an additional loss
    term called KL divergence loss that we explain shortly.'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Throughout this chapter, we denote latent variables with ![](../../OEBPS/Images/AR_z.png)
    and input variables with ![](../../OEBPS/Images/AR_x.png).
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.1 Geometric overview of VAEs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Figure [14.7](#fig-latent_space_distributions) attempts to provide a geometrical
    view of VAE latent-space modeling. During training, given an input ![](../../OEBPS/Images/AR_x.png),
    the encoder does not directly emit the corresponding latent-space representation
    ![](../../OEBPS/Images/AR_z.png). Instead, the encoder emits the parameters of
    a distribution from a prechosen family. For instance, if the prechosen family
    is Gaussian, the encoder emits a pair of parameter values ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)),
    **Œ£**(![](../../OEBPS/Images/AR_x.png)). These are the mean and covariance matrix
    of a specific Gaussian distribution ùí©(![](../../OEBPS/Images/AR_z.png); ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)),
    **Œ£**(![](../../OEBPS/Images/AR_x.png))) in the latent space. The latent-space
    representation ![](../../OEBPS/Images/AR_z.png) corresponding to the input ![](../../OEBPS/Images/AR_x.png)
    is obtained by sampling this distribution emitted by the encoder. Thus in the
    Gaussian case, we have ![](../../OEBPS/Images/AR_z.png) ‚àº ùí©(![](../../OEBPS/Images/AR_z.png);
    ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)), **Œ£**(![](../../OEBPS/Images/AR_x.png))).
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The symbol ‚àº indicates sampling from a distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F07_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.7 Geometric depiction of VAE latent-space modeling distributions
  prefs: []
  type: TYPE_NORMAL
- en: This distribution, which we call the *latent-space map* of the input ![](../../OEBPS/Images/AR_x.png),
    is shown by hollow circles with dark borders in figure [14.7](#fig-latent_space_distributions).
    Such mapping is called *stochastic mapping*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The latent-space map distribution should have a narrow, single-peaked probability
    density function (for example, a Gaussian with small variance: that is, small
    ||**Œ£**||). The narrow-peakedness of the probability density function implies
    that the cloud of sample points forms a tight, small cluster‚Äîany random sample
    from the distribution will likely be close to the mean. So, sampling ![](../../OEBPS/Images/AR_z.png)
    from such a distribution is not very different from a deterministic mapping from
    ![](../../OEBPS/Images/AR_x.png) to ![](../../OEBPS/Images/AR_z.png) = ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)).
    This sampling to obtain the latent vector is done only during training. During
    inferencing, we use the mean emitted by the encoder directly as the latent-space
    representation of the input: that is, ![](../../OEBPS/Images/AR_z.png) = ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)).'
  prefs: []
  type: TYPE_NORMAL
- en: The decoder maps the latent vector representation ![](../../OEBPS/Images/AR_z.png)
    back to a point, say *xÃÉ*, in the input space. This is the reconstructed version
    of the input vector (shown by a little white square with a black border in figure
    [14.7](#fig-latent_space_distributions)). The decoder is thus estimating (reconstructing)
    the input given the latent vector.
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.2 VAE training, losses, and inferencing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Training comprises the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a simple distribution family for *q*(![](../../OEBPS/Images/AR_z.png)
    | ![](../../OEBPS/Images/AR_x.png)). Gaussian is a popular choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each input ![](../../OEBPS/Images/AR_x.png) maps to a separate distribution.
    The encoder neural network emits the parameters of this distribution. For the
    Gaussian case, the encoder emits *Œº*(![](../../OEBPS/Images/AR_x.png)), Œ£(![](../../OEBPS/Images/AR_x.png)).
    The latent vector ![](../../OEBPS/Images/AR_z.png) is sampled from this emitted
    distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder neural network takes ![](../../OEBPS/Images/AR_z.png) as input and
    emits the reconstructed input *xÃÉ*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given the input, reconstructed input, and latent vector we can compute the reconstruction
    loss and KL Divergence loss described below. The goal of the training process
    is to iteratively minimize these losses. Thus, the VAE is trained to minimize
    a weighted sum of the following two loss terms on each input batch-
  prefs: []
  type: TYPE_NORMAL
- en: '*Reconstruction Loss*‚ÄîJust as in an autoencoder, in a properly trained VAE,
    the reconstruction *xÃÉ* should be close to the original input ![](../../OEBPS/Images/AR_x.png).
    So, reconstruction loss is'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-c.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '*KL divergence loss*‚Äî In VAE, we also have a loss term proportional to the
    KL divergence between the distribution emitted by the encoder and the zero mean
    unit variance Gaussian. KL divergence measured the dissimilarity between two probability
    distributions and was discussed in detail in section [6.4](../Text/06.xhtml#sec-kld).
    Here we state (following equation [6.13](../Text/06.xhtml#eq-kld-cont-multivar))
    that the KL divergence loss for VAE is'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-d.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: where *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    denotes the latent-space map probability distribution and *p*(![](../../OEBPS/Images/AR_z.png))
    is a fixed target distribution. We want our global distribution of latent vectors
    to mimic the target distribution. The target is typically chosen to be a compact
    distribution so that the global latent vector distribution is also compact.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The popular choice for the prechosen distribution family is Gaussian and for
    the fixed distribution is the zero-mean unit covariance matrix Gaussian:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-e.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: It should be noted that for the above choice of prior, we can evaluate the KLD
    loss via a closed form formula as described in section 14.7.7.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Minimizing ‚Ñí*[kld]* essentially demands that *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    is high‚Äîthat is, close to one‚Äîat the ![](../../OEBPS/Images/AR_z.png) values where
    *p*(![](../../OEBPS/Images/AR_z.png)) is high (see figure [14.8](#fig-vae-kld-loss)),
    because then their ratio is close to one and the logarithm is close to zero. The
    values of *p*(![](../../OEBPS/Images/AR_z.png)) at the places where *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    is low close to zero) do not matter because *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    appears as a factor in ‚Ñí*[kld]*‚Äîthe contributions to the loss by these terms are
    close to zero anyway.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Thus, KLD loss essentially tries to ensure that most of the sample point cloud
    of *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)) falls
    on a densely populated region of the sample point cloud of *p*(![](../../OEBPS/Images/AR_z.png)).
    Geometrically, this means the cloud of little hollow circles with black borders
    has a lot of overlapping mass with the target distribution. If every training
    data point is like that, the overall global cloud of latent vectors will also
    have significant overlap with the target distribution. Since the target distribution
    is typically chosen to be compact, this in turn ensures that the overall latent
    vector distribution (dark filled circles in figure [14.7](#fig-latent_space_distributions))
    is compact. For instance, in the case when the target distribution is the zero-mean
    unit covariance matrix Gaussian ùí©(![](../../OEBPS/Images/AR_z.png); ![](../../OEBPS/Images/AR_0.png),
    **I**), most of the mass of the latent vectors is contained within the unit radius
    ball. Without the KL divergence term, the latent vectors will spread throughout
    the latent space. In short, the KLD loss *regularizes the latent space*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F08a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Low KL divergence loss (high *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    coincides with high *p*(![](../../OEBPS/Images/AR_z.png))
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F08b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) High KL divergence loss (high *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    coincides with low *p*(![](../../OEBPS/Images/AR_z.png))
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.8 between the encoder-generated distribution (ùí©(![](../../OEBPS/Images/AR_z.png);
    ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)), **Œ£**(![](../../OEBPS/Images/AR_x.png)))
    ) and the target distribution *p*(![](../../OEBPS/Images/AR_z.png)) (here *p*(![](../../OEBPS/Images/AR_z.png))
    ‚â° ùí©(![](../../OEBPS/Images/AR_z.png); ![](../../OEBPS/Images/AR_0.png), **I**))
    or, equivalently, low KL divergence between them.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder-decoder pair of neural networks is trained end to end to minimize
    the weighted sum of reconstruction loss and KLD loss. In particular, the encoder
    learns to emit the parameters of the *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: During inferencing, only the encoder is used. The encoder takes an input ![](../../OEBPS/Images/AR_x.png)
    and outputs ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png))
    and **Œ£**(![](../../OEBPS/Images/AR_x.png)). We do not sample here. Instead, we
    use the mean directly as the latent-space representation of the input.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that each input point ![](../../OEBPS/Images/AR_x.png) maps to a separate
    Gaussian distribution *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)
    = *N*(![](../../OEBPS/Images/AR_z.png); ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)),
    ‚àë(![](../../OEBPS/Images/AR_x.png))). The overall distribution *p*(![](../../OEBPS/Images/AR_x.png))
    modeled by all of these together can be very complex. Yet that complexity does
    not affect our computation which involves only *q*(*z*|*x*) and *p*(*z*). This
    is what makes the approach powerful.
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.3 VAEs and Bayes‚Äô theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During training, the encoder neural network stochastically maps a specific
    input data instance, a point ![](../../OEBPS/Images/AR_x.png) in the input space,
    to a latent-space point ![](../../OEBPS/Images/AR_z.png) ~ ùí©(![](../../OEBPS/Images/AR_z.png);
    ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)), **‚àë**(![](../../OEBPS/Images/AR_x.png))).
    Thus the latent-space map effectively models the posterior probability *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    Note that we are using the symbol *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    to denote the actual distribution emitted by the encoder, while we are using the
    symbol *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    to denote the true (unknown) posterior probability distribution. Of course, we
    want these two to be as close as possible to each other: that is, we want the
    KL divergence between them to be minimal. Later in this section, we see how minimizing
    the KL divergence between *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    and *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)) leads
    to the entire VAE algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: The decoder maps this point (![](../../OEBPS/Images/AR_z.png)) in latent space
    back to the input space point *xÃÉ*. As such, it models the probability distribution
    *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The global distributions of the latent vectors ![](../../OEBPS/Images/AR_z.png)
    effectively model *p*(![](../../OEBPS/Images/AR_z.png)) (shown by dark-shaded
    filled little circles in figure [14.7](#fig-latent_space_distributions)). These
    probabilities are connected by our old friend, Bayes‚Äô theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-f.png)'
  prefs: []
  type: TYPE_IMG
- en: 14.7.4 Stochastic mapping leads to latent-space smoothness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sampling the encoder‚Äôs output from a narrow distribution is similar, but not
    identical, to deterministic mapping. It has a rather unexpected advantage over
    direct encoding. A specific input point is mapped to a slightly different point
    in the latent space every time it is encountered during training‚Äîall these points
    have to decode back to the same region in the input space. This enforces an overall
    smoothness over the latent space: nearby ![](../../OEBPS/Images/AR_z.png) values
    all correspond to nearby ![](../../OEBPS/Images/AR_x.png) values.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.5 Direct minimization of the posterior requires prohibitively expensive
    normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Bayes‚Äô theorem expression of a VAE in section [14.7.3](#sec-vae-bayes) gives
    us an idea. Why not train the neural network to directly maximize the posterior
    probability *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_Xc.png)),
    where *X* denotes the training data set? It certainly makes theoretical sense;
    we are choosing the latent space whose posterior probability is maximum given
    the training data. Of course, we must optimize one batch at a time, as we always
    do with neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we evaluate the posterior probability? The formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-g.png)'
  prefs: []
  type: TYPE_IMG
- en: The denominator contains a sum over all values of ![](../../OEBPS/Images/AR_z.png).
    Remember, with every iteration, the neural network weights change, and all previously
    computed latent vectors become invalid. This means we have to recompute all latent
    vectors every iteration, which is intractable. Each iteration is ùí™(*n*), and each
    epoch then is ùí™(*n*¬≤), where *n* is the number of training data instances (could
    be on the order of millions). We have to look for other methods. That takes us
    to *evidence lower bound* (ELBO) types of approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.6 ELBO and VAEs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We do not know the true probability distribution *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    Let‚Äôs try to learn an approximate probability distribution *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    that is as close as possible to *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    In other words, we want to minimize the KL divergence between the two (KL divergence
    was introduced in section [6.4](../Text/06.xhtml#sec-kld)). This KL divergence
    is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-h.png)'
  prefs: []
  type: TYPE_IMG
- en: We can expand this as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-i.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *D* is the domain of ![](../../OEBPS/Images/AR_z.png): that is, the latent
    space, *H[q]* is the entropy of the probability distribution (entropy was introduced
    in section [6.2](../Text/06.xhtml#sec-entropy)), and *E[q]*(*ln*(*p*(![](../../OEBPS/Images/AR_x.png),
    ![](../../OEBPS/Images/AR_z.png)))) is the expected value of *ln*(*p*(![](../../OEBPS/Images/AR_x.png),
    ![](../../OEBPS/Images/AR_z.png))) under the probability density *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    Rearranging terms, we get'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-j.png)'
  prefs: []
  type: TYPE_IMG
- en: where the right-hand side is constant because it is a property of the data and
    cannot be adjusted during optimization. Defining the evidence lower bound (ELBO)
    as
  prefs: []
  type: TYPE_NORMAL
- en: '*ELBO* = *H[q]* + *E[q]*(*ln*(*p*(![](../../OEBPS/Images/AR_x.png), ![](../../OEBPS/Images/AR_z.png))))'
  prefs: []
  type: TYPE_NORMAL
- en: we get
  prefs: []
  type: TYPE_NORMAL
- en: '*KLD*(*q*, *p*) + *ELBO* = *constant*'
  prefs: []
  type: TYPE_NORMAL
- en: So, *minimizing* the KL divergence between *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    and its approximation *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    is equivalent to *maximizing* the ELBO. We soon see that this leads to a technique
    for optimizing variational autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: Significance of the name ELBO
  prefs: []
  type: TYPE_NORMAL
- en: Why do we call it *evidence lower bound*? Well, the answer is hidden in the
    relation *KLD*(*q*, *p*) + *ELBO* = *ln*(*p*(![](../../OEBPS/Images/AR_x.png))).
    The right-hand side is the evidence log-likelihood. Remember, KL divergence is
    always non-negative. So, the lowest value of *ln*(*p*(![](../../OEBPS/Images/AR_x.png)))
    happens when KL divergence is zero when *ln*(*p*(![](../../OEBPS/Images/AR_x.png)))
    = *ELBO*. This means the evidence log-likelihood cannot be lower than the ELBO
    value. Thus the ELBO is the lower bound of the evidence log-likelihood; in short,
    it is the evidence lower bound.
  prefs: []
  type: TYPE_NORMAL
- en: Physical significance of the ELBO
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs look at the physical significance of ELBO maximization:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ELBO* = *H[q]* + *E[q]*(*ln*(*p*(![](../../OEBPS/Images/AR_x.png), ![](../../OEBPS/Images/AR_z.png))))'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first term is entropy. As we saw in section [6.2](../Text/06.xhtml#sec-entropy),
    this is a measure of the diffuseness of the distribution. If the points are evenly
    spread out in the distribution‚Äîthe probability density is flat with no high peak‚Äîthe
    entropy is high. When the distribution has few tall peaks and low values elsewhere,
    entropy is low (remember, for a probability density, having tall peaks implies
    low values elsewhere since the total volume under the function is constant: one).
    Thus, maximizing the ELBO means we are looking for a diffuse distribution *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    This, in turn, encourages smoothness in the latent space since we are effectively
    saying an input point ![](../../OEBPS/Images/AR_x.png) can map to any point around
    the mean ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png))
    (as emitted by the encoder) with almost equal high probability. Note that this
    fights a bit with the notion that each input should map to a unique point in the
    latent space. The solution tries to optimize between these conflicting requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: The other term‚Äîexpectation of the log of joint density *p*(![](../../OEBPS/Images/AR_x.png),
    ![](../../OEBPS/Images/AR_z.png)) under the probability density *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))‚Äîeffectively
    measures the overlap between the two. Maximizing it is equivalent to saying *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    must be high where *p*(![](../../OEBPS/Images/AR_x.png), ![](../../OEBPS/Images/AR_z.png))
    is high. This seems intuitively true. The joint density *p*(![](../../OEBPS/Images/AR_x.png),
    ![](../../OEBPS/Images/AR_z.png)) = *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))*p*(![](../../OEBPS/Images/AR_z.png)).
    It is high where *both* the posterior *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    and prior *p*(![](../../OEBPS/Images/AR_z.png)) are high. If *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    approximates the posterior, it should be high where the joint is high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs continue to explore the ELBO. More physical significances will emerge
    along with an algorithm for VAE optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-k.png)'
  prefs: []
  type: TYPE_IMG
- en: Rearranging terms and simplifying
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-l.png)'
  prefs: []
  type: TYPE_IMG
- en: This last expression yields more physical interpretation and leads to the VAE
    algorithm. Let‚Äôs examine the two terms in the final ELBO expression in detail.
    The first term is *E[q]*(*ln*(*p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png)))).
    This is high when *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    and *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png)) are
    both high at the same ![](../../OEBPS/Images/AR_z.png) values. For a given ![](../../OEBPS/Images/AR_x.png),
    *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)) is high
    at those ![](../../OEBPS/Images/AR_z.png) values that are likely encoder outputs
    (that is, latent representations) of input ![](../../OEBPS/Images/AR_x.png). High
    *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png)) at these
    same ![](../../OEBPS/Images/AR_z.png) locations implies a high probability of
    decoding back to the same ![](../../OEBPS/Images/AR_x.png) value from those ![](../../OEBPS/Images/AR_z.png)
    locations. Thus, this term basically says if ![](../../OEBPS/Images/AR_x.png)
    *encodes* to ![](../../OEBPS/Images/AR_z.png) with a high probability, then ![](../../OEBPS/Images/AR_z.png)
    should *decode* back to ![](../../OEBPS/Images/AR_x.png) with a high probability,
    too. Stated differently, a round trip from input to latent space back to input
    space should not take us far from the original input. In figure [14.7](#fig-latent_space_distributions),
    this means the input point marked ![](../../OEBPS/Images/AR_x.png) lies close
    to the output point marked ![](../../OEBPS/Images/AR_x3.png). In other words,
    *minimizing reconstruction loss leads to ELBO maximization*.
  prefs: []
  type: TYPE_NORMAL
- en: Now consider the second term. It comes with a minus sign. Maximizing this is
    equivalent to minimizing the KL divergence between *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    and *p*(![](../../OEBPS/Images/AR_z.png)). This is the regularizing term. Viewed
    in another way, this is the term through which we inject our belief about the
    basic organization of the latent space into the system. Remember that the KL divergence
    *KLD*(*q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)),
    *p*(![](../../OEBPS/Images/AR_z.png))) sees very little contribution from the
    small values of *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    It is dominated by the large values of *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    In terms of figure [14.7](#fig-latent_space_distributions), minimizing this KL
    divergence essentially ensures that most of the hollow circles fall on an area
    highly populated with filled circles.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, overall, maximization of ELBO is equivalent to minimizing reconstruction
    loss with regularization in the form of minimizing KL divergence from a specific
    prior distribution. This is what we do in VAEs. In every iteration, we minimize
    the reconstruction loss (as in ordinary AEs) and also minimize divergence from
    a known (or guessed) prior. Note that this does not require us to encode all training
    inputs per iteration. The approach is *incremental*‚Äîone input or input batch at
    a time‚Äîlike any other neural network optimization. Also, although we started from
    finding an approximation to *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)),
    the final expression does not have that anywhere. There is only the prior *p*(![](../../OEBPS/Images/AR_z.png))
    for which we can use some suitable fixed distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '14.7.7 Choice of prior: Zero-mean, unit-covariance Gaussian'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The popular choice for the known prior is a zero-mean, unit-covariance matrix
    Gaussian, ùí©(![](../../OEBPS/Images/AR_0.png), **I**) , where I is the *d* √ó *d*
    identity matrix *d* is the dimensionality of the latent space), ![](../../OEBPS/Images/AR_0.png)
    is *d* √ó 1 vector of all zeros. Note that minimizing the KL divergence from ùí©(![](../../OEBPS/Images/AR_0.png),
    **I**) is equivalent to restricting most of the mass within the unit ball (a hypersphere
    with its center at the origin and radius 1). In other words, this KL divergence
    term restrains the latent vectors from spreading over the ‚Ñú*[d]* and remains mostly
    within the unit ball. Remember that a compact set of latent vectors translates
    in a sense to the simplest (minimum descriptor length) representations for the
    input vectors: that is, a regularized latent space (section [14.6](#sec-latent-space-regularizn)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'KL divergence from a Gaussian has a closed-form expression that we derive in
    section [6.4.1](../Text/06.xhtml#sec-kld-gaussians). We first repeat equation
    [6.14](../Text/06.xhtml#eq-kld-univar-gauss) for KL divergence between two Gaussians
    and then obtain the expression for the special case where one of the Gaussians
    is a zero-mean, unit-covariance Gaussian:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 14.1
  prefs: []
  type: TYPE_NORMAL
- en: 'where the operator *tr* denotes the *trace* of a matrix (sum of diagonal elements)
    and operator *det* denotes the determinant. By assumption, *p*(![](../../OEBPS/Images/AR_z.png))
    = ùí©(![](../../OEBPS/Images/AR_0.png), **I**): that is, *![](../../OEBPS/Images/AR_micro.png)[p]*
    = ![](../../OEBPS/Images/AR_0.png) and **Œ£***[p]* = **I**. Thus,'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-01-a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we introduce another simplifying assumption: that *the covariance
    matrix **Œ£***[q]* is a diagonal matrix*. This means the matrix can be expressed
    compactly as'
  prefs: []
  type: TYPE_NORMAL
- en: '**Œ£***[q]* = *![](../../OEBPS/Images/AR_sigma.png)[q]*'
  prefs: []
  type: TYPE_NORMAL
- en: where *![](../../OEBPS/Images/AR_sigma.png)[q]* contains the elements of the
    main diagonal and we are not redundantly expressing the zeros in the off-diagonal
    elements. Note that this is not an outlandish assumption to make. We are approximating
    *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)) with a
    Gaussian *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    whose axes are uncorrelated.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this assumption,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-01-b.png)'
  prefs: []
  type: TYPE_IMG
- en: It is easy to see the expression (*![](../../OEBPS/Images/AR_sigma.png)[q]*¬≤[*i*]
    ‚àí 2*log*(*![](../../OEBPS/Images/AR_sigma.png)[q]*[*i*])) reaches a minimum when
    *![](../../OEBPS/Images/AR_sigma.png)[q]*[*i*] = 1. Thus, overall, KL divergence
    with the zero-mean, unit-covariance Gaussian is minimized when the mean is at
    the origin and the variances are all ones. This is equivalent to minimizing the
    spread of the latent vectors outside the ball of unit radius centered on the origin.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative choice for the prior is a Gaussian mixture with as many components
    as the known number of classes. We do not discuss that here.
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.8 Reparameterization trick
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have avoided talking about one nasty problem so far. We said that in VAEs,
    the encoder emits the mean and variance of the probability density function *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    from which we *sample* the encoder output. There is a problem, however. The encoder-decoder
    pair are neural networks that learn via backpropagation. That is based on differentiation.
    Sampling is *not* differentiable. How do we deal with this?
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use a neat trick: the so-called *reparameterization trick*. Let‚Äôs first
    explain it in the univariate case. Sampling from a Gaussian ùí©(*Œº*, *œÉ*) can be
    viewed as a combination of the following two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Take a random sample from *x* from ùí©(0,1). Note that there is no learnable parameter
    here; it‚Äôs a sample from a constant density function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Translate the sample (add *Œº*), and scale it (multiply by *œÉ*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This essentially takes the sampling part out of the path for backpropagation.
    The encoder emits *Œº* and *œÉ*, which are differentiable entities that we learn.
    Sampling is done separately from a constant density function.
  prefs: []
  type: TYPE_NORMAL
- en: The idea can be extended to a multivariate Gaussian. Sampling from ùí©(![](../../OEBPS/Images/AR_micro.png),
    **Œ£**) can be broken down into sampling from ùí©(![](../../OEBPS/Images/AR_0.png),
    **I**) and scaling the vector by multiplying by the matrix Œ£ and translating by
    ![](../../OEBPS/Images/AR_micro.png). Thus, we have a multivariate encoder that
    can learn via backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for VAEs, executable via Jupyter Notebook, can be
    found at [http://mng.bz/5QYD](http://mng.bz/5QYD).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.5 PyTorch- Reparameterization trick
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Converts the log variance to the standard deviation
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Samples from ùí©(![](../../OEBPS/Images/AR_0.png), **I**)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Scales by multiplying by Œ£ and translates by ![](../../OEBPS/Images/AR_micro.png)
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.6 PyTorch- VAE
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Input image size in (c, h, w) format
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Reduces to a (32, 16, 16)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Reduces to a (128, 8, 8)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Reduces to a (256, 4, 4)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Flattens to a 4096-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Reduces a 4096-sized tensor to an nz-sized *Œº* tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Reduces a 4096-sized tensor to an nz-sized *log*(*œÉ*¬≤) tensor
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.7 PyTorch- VAE decoder
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Converts (nz, 1, 1) to a (256, 4, 4)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Increases to a 128, 8, 8)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Increases to a 32, 16, 16)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Increases to a 1, 32, 32)-sized tensor
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.8 PyTorch- VAE loss
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Binary cross-entropy loss
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° *KLD*(*q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)),
    *p*(![](../../OEBPS/Images/AR_z.png))) where ![](../../OEBPS/Images/AR_z.png)
    ~ ùí©(![](../../OEBPS/Images/AR_0.png), **I**)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Computes the total loss
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.9 PyTorch- VAE training
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Passes the input image through the convolutional encoder
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Computes *Œº*, an nz-dimensional tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Computes *log*(*œÉ*¬≤), an nz-dimensional tensor
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Samples *z* via the reparameterization trick
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Reconstructs the image using z via the decoder
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Computes the total loss
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders vs. VAEs
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs revisit the familiar MNIST digits data set. It contains a training set
    of 60,000 images and a test set of 10,000 images. Each image is 28 √ó 28 in size
    and contains a center crop of a single digit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, we used this data set for classification. Here, we use it an unsupervised
    manner: we ignore the labels during training/testing. We train both the autoencoder
    and the VAE end to end on this data set and look at the results (see figures [14.9](#fig-ae-vae-recon)
    and [14.10](#fig-ae-vae-latent-space)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F09a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Autoencoder- reconstructed images
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F09b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) VAE-reconstructed images
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.9 Comparing the reconstructed images on the test set for the autoencoder
    and VAE trained end to end. On a autoencoder and VAE do a pretty good job of reconstructing
    images from the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F10a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Autoencoder latent space nz=2)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F10b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) VAE latent space (nz=2)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.10 The difference between the learned latent spaces of the autoencoder
    and VAE. We train an autoencoder and a VAE with *nz* = 2 on MNIST and plot the
    latent space for the test set. Autoencoders only minimize the reconstruction loss,
    so any latent space is equally acceptable as long as the reconstruction loss is
    low. As expected, the learned latent space is sparse and has a very high spread.
    VAEs, in contrast, minimize reconstruction loss with regularization. This is done
    by minimizing the KL divergence between the learned latent space and a known prior
    distribution ùí©(![](../../OEBPS/Images/AR_0.png), **I**). Adding this regularization
    term ensures that the latent space is constrained within a unit ball. This can
    be seen in figure [14.10b](#fig-vae-latent-space), where the learned latent space
    is much more compact.
  prefs: []
  type: TYPE_NORMAL
- en: The autoencoder is trained to minimize the MSE between the input image and the
    reconstructed image. There is no other restriction on the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The VAE is trained to maximize the ELBO. As we saw in the previous section,
    we can maximize the ELBO by minimizing the reconstruction loss with regularization
    in the form of minimizing KL divergence from a specific prior distribution: ùí©(![](../../OEBPS/Images/AR_0.png),
    **I**) in the case of VAE. So the network is incentivized to ensure that the latent
    space learned is constrained within the unit ball.'
  prefs: []
  type: TYPE_NORMAL
- en: One minor implementation detail to note is that we use binary cross-entropy
    instead of MSE when training VAEs. In practice, this leads to better convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In latent-space modeling, we map input data points onto a lower-dimensional
    latent space. The latent space is typically a manifold consisting of points that
    have a property of interest in common. The property of interest can be membership
    to a specific class, such as all paragraphs written by Shakespeare. The latent
    vectors are simpler, more compact representations of the input data in which only
    information related to the property of interest is retained and other information
    is eliminated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In latent space modeling, all training data input satisfies the property of
    interest. For instance, we can train a latent space model on paragraphs written
    by Shakespeare. Then the learned manifold contains points corresponding to various
    Shakespeare like paragraphs. Points far away from the manifold are less Shakespeare-like.
    By inspecting this distance, we can estimate the probability of a paragraph being
    written by Shakespeare. By sampling the probability distribution, we may even
    be able to emit pseudo-Shakespeare paragraphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometrically speaking, we project the input point onto the manifold. PCA performs
    a special form of latent space modeling where the manifold is a best-fit hyperplane
    for the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Autoencoders can perform a much more powerful dimensionality reduction than
    PCA. An autoencoder consists of an encoder *E*), which maps the input data point
    into the lower-dimensional space, and a decoder (*D*), which maps the lower-dimensional
    representation back into the input space. It is trained to minimize the reconstruction
    loss: that is, the distance between the input and reconstructed (encoded, then
    decoded) vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variational autoencoders (VAEs) model latent spaces as probability distributions
    to impose additional constraints (over and above reconstruction loss) so that
    we can generate more regularized latent spaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In VAEs, the encoder maps the input to a latent representation via a stochastic
    process (rather than a deterministic one). It emits *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    as opposed to directly emitting ![](../../OEBPS/Images/AR_z.png). ![](../../OEBPS/Images/AR_z.png)
    is obtained by sampling *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    The decoder maps a point in latent space ![](../../OEBPS/Images/AR_z.png) back
    to the input space. It is also modeled as a probability distribution *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png)).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The latent space learned by a VAE is much more compact and smoother (and hence
    more desirable) than that learned by an autoencoder.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
