- en: 14 Latent space and generative modeling, autoencoders, and variational autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14 潜空间和生成建模、自编码器以及变分自编码器
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Representing inputs with latent vectors
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用潜向量表示输入
- en: Geometrical view, smoothness, continuity, and regularization for latent spaces
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜空间的几何视图、平滑性、连续性和正则化
- en: PCA and linear latent spaces
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）和线性潜空间
- en: Autoencoders and reconstruction loss
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器和重建损失
- en: Variational autoencoders (VAEs) and regularizing latent spaces
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分自编码器（VAEs）和正则化潜空间
- en: Mapping input vectors to a transformed space is often beneficial in machine
    learning. The transformed vector is called a *latent vector*—latent because it
    is not directly observable—while the input is the underlying *observed vector*.
    The latent vector (aka embedding) is a simpler representation of the input vector
    where only features that help accomplish the ultimate goal (such as estimating
    the probability of an input
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入向量映射到转换空间在机器学习中通常是有益的。转换向量被称为*潜向量*——潜的，因为它不是直接可观察的——而输入是基础的*观测向量*。潜向量（也称为嵌入）是输入向量的简化表示，其中只包含有助于实现最终目标（例如，估计输入的概率）的特征。
- en: 'belonging to a specific class) are retained, and other features are forgotten.
    Typically, the latent representation has fewer dimensions than the input: that
    is, encoding an input into a latent vector results in *dimensionality reduction*.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: （属于特定类别）被保留，其他特征被遗忘。通常，潜表示的维度少于输入：即，将输入编码到潜向量中会导致*维度降低*。
- en: The mapping from input to latent space (and vice versa) is usually learned—we
    train a machine, such as a neural network, to do it. The latent vector needs to
    be as faithful a representation as possible of the input within the dimensionality
    allocated to it. So, the neural network is incentivized to minimize the loss of
    information caused by the transformation. Later, we see that in autoencoders,
    this is achieved by reconstructing the input from the latent vector and trying
    to minimize the difference between the actual and reconstructed input. However,
    given the reduced number of dimensions, the network does not have the luxury of
    retaining everything in the input. It has to learn what is essential to the end
    goal and retain only that. Thus the embedding is a compact representation of the
    input that is streamlined to achieve the ultimate goal.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入到潜空间（反之亦然）的映射通常是通过学习得到的——我们训练一个机器，例如神经网络，来完成这个任务。潜向量需要尽可能忠实地表示分配给它的输入维度内的输入。因此，神经网络被激励去最小化由转换引起的信息损失。后来，我们看到在自编码器中，这是通过从潜向量重建输入并尝试最小化实际输入和重建输入之间的差异来实现的。然而，由于维度的减少，网络没有保留输入中所有内容的奢侈。它必须学习对最终目标至关重要的内容，并仅保留那些内容。因此，嵌入是输入的紧凑表示，它被精简以达到最终目标。
- en: 14.1 Geometric view of latent spaces
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.1 潜空间几何视图
- en: 'Consider the space of all digital images of height *H*, width *W*, with each
    pixel representing a 24-bit RGB color value. This is a gigantic space with (2^(24))*^(HW)*
    points. Every possible RGB × *H* × *W* image is a point in this space. But if
    an image is a natural image, neighboring points tend to have similar colors. This
    means points corresponding to natural images are correlated: they are not distributed
    uniformly over the space of possible images. Furthermore, if the images have a
    common property (say, they all giraffes), the corresponding points form clusters
    in the (2^(24))*^(HW)*-sized input space. In stochastic parlance, the probability
    distribution of natural images with a common property over the space of possible
    images is highly non-uniform (low entropy).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑所有高度为*H*、宽度为*W*的数字图像空间，每个像素代表一个24位RGB颜色值。这是一个巨大的空间，有(2^(24))*^(HW)*个点。这个空间中的每一个可能的RGB
    × *H* × *W*图像都是一个点。但如果一个图像是自然图像，相邻的点往往具有相似的颜色。这意味着对应于自然图像的点相关：它们不是均匀分布在可能图像的空间中。此外，如果图像具有共同属性（例如，它们都是长颈鹿），相应的点在(2^(24))*^(HW)*大小的输入空间中形成簇。在随机论中，具有共同属性的自然图像在可能图像空间中的概率分布高度非均匀（低熵）。
- en: 'Figure [14.1a](#fig-linear-latent-subspace) illustrates points with some common
    property clustered around a planar manifold. Similarly, figure [14.1b](#fig-non-inear-latent-subspace)
    illustrates points with some common property clustered around a curved manifold.
    These points have a common property. At the moment, we are not interested in what
    that property is or whether the manifold is planar or curved. All we care about
    is that these points of interest are distributed around a manifold. The manifold
    captures the essence of that common property, whatever it is. If the common property
    is, say, the presence of a giraffe in the image, then the manifold captures *giraffeness*:
    the points on or near the manifold all correspond to images with giraffes. If
    we travel along the manifold, we encounter various flavors of giraffe photos.
    If we go far from the manifold—that is, travel a long distance in a direction
    orthogonal to the manifold—the probability of the point representing a photo with
    a giraffe is low.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图[14.1a](#fig-linear-latent-subspace)展示了具有某些共同属性的点围绕平面流形聚集。类似地，图[14.1b](#fig-non-inear-latent-subspace)展示了具有某些共同属性的点围绕曲面流形聚集。这些点具有共同属性。目前，我们并不关心这个属性是什么，或者流形是平面还是曲面。我们只关心这些感兴趣的点分布在一个流形周围。流形捕捉了这种共同属性的精髓，无论它是什么。如果共同属性是，比如说，图像中存在长颈鹿，那么流形就捕捉了*长颈鹿特性*：流形上或附近的点都对应着有长颈鹿的图像。如果我们沿着流形移动，我们会遇到各种风格的长颈鹿照片。如果我们远离流形——即沿着垂直于流形的方向移动一段距离——那么点代表有长颈鹿照片的概率就低。
- en: '![](../../OEBPS/Images/CH14_F01a_Chaudhury.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH14_F01a_Chaudhury.png)'
- en: (a) Planar latent subspaces
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 平面潜在子空间
- en: '![](../../OEBPS/Images/CH14_F01b_Chaudhury.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH14_F01b_Chaudhury.png)'
- en: (b) Curved latent subspace
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 曲面潜在子空间
- en: Figure 14.1 Two examples of latent subspaces, with planar and curved manifolds,
    respectively. The solid line shows the latent vector, and the dashed line represents
    the information lost by projecting onto the latent subspace.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1展示了两个潜在子空间的例子，分别对应平面和曲面的流形。实线表示潜在向量，虚线代表投影到潜在子空间所丢失的信息。
- en: Given training data consisting of sampled points of interest (such as many giraffe
    photos), we can train a neural network to learn this manifold—it is the optimal
    manifold that minimizes the average distance of all the training data points from
    the manifold. Then, at inference time, given an arbitrary input point, we can
    estimate its distance from the manifold, giving us the probability of that input
    satisfying the property represented by the manifold.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 给定由感兴趣点的样本（如许多长颈鹿照片）组成的训练数据，我们可以训练一个神经网络来学习这个流形——这是使所有训练数据点到流形的平均距离最小化的最优流形。然后，在推理时间，给定一个任意的输入点，我们可以估计它到流形的距离，这给了我们该输入满足流形所表示属性的概率。
- en: Thus, the input vector can be decomposed into an in-manifold component (solid
    line in figure [14.1](#fig-latent-subspaces)) and an orthogonal-to-manifold component
    (dashed line in figure [14.1](#fig-latent-subspaces)). Latent space modeling effectively
    eliminates the orthogonal component and retains the in-manifold component as the
    latent vector (aka embedding). Equivalently, we are projecting the input vector
    onto the manifold. This is the core idea of latent space modeling—we learn a manifold
    that represents a property of interest and represents all inputs by a latent vector,
    which is the input point’s projection onto this manifold. The latent vector is
    a more compact representation of the input where only information related to the
    property of interest is retained.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，输入向量可以被分解为一个在流形内的分量（图[14.1](#fig-latent-subspaces)中的实线）和一个垂直于流形的分量（图[14.1](#fig-latent-subspaces)中的虚线）。潜在空间建模有效地消除了垂直分量，并保留了在流形内的分量作为潜在向量（也称为嵌入）。等价地，我们是在将输入向量投影到流形上。这是潜在空间建模的核心思想——我们学习一个表示感兴趣属性的流形，并通过潜在向量表示所有输入，这是输入点到该流形上的投影。潜在向量是输入的更紧凑表示，其中只保留了与感兴趣属性相关的信息。
- en: Latent space modeling in a nutshell
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间建模概述
- en: In latent space modeling, we train a neural network to represent a manifold
    around which the input points satisfying a property of interest are distributed.
    The property of interest could be membership in a specific class, such as images
    containing a giraffe. Thus, the learned manifold is a collection of points that
    satisfy the property. The input point is projected onto this manifold to obtain
    a latent vector representation of the input (aka embedding). This is equivalent
    to throwing away the input vector component that is orthogonal to the manifold.
    The eliminated component is orthogonal to the manifold and hence unrelated to
    the property of interest (may represent background pixels of the image), so the
    information loss caused by the projection does not hurt. We have created a less
    noisy, more compact representation of the input that focuses on the things we
    care about.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在潜在空间建模中，我们训练一个神经网络来表示一个流形，输入点满足感兴趣属性分布在这个流形周围。感兴趣的属性可能是属于特定类别，例如包含长颈鹿的图像。因此，学习到的流形是一组满足该属性的点。输入点被投影到这个流形上，以获得输入的潜在向量表示（也称为嵌入）。这相当于丢弃了与流形正交的输入向量分量。被消除的分量与流形正交，因此与感兴趣属性无关（可能代表图像的背景像素），所以投影引起的信息损失不会造成伤害。我们创建了一个更少噪声、更紧凑的输入表示，专注于我们关心的事情。
- en: Training data consists of a set of sampled data inputs, all satisfying the property
    of interest. The system essentially learns the manifold, which is optimally located
    to minimize its average distance from all the training data points. During inferencing,
    given an arbitrary input point, its distance from the manifold is an indicator
    of the probability of that input satisfying the property of interest.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据由一组采样数据输入组成，所有这些输入都满足感兴趣的属性。系统本质上学习的是流形，该流形位于最优位置以最小化其与所有训练数据点的平均距离。在推理过程中，给定一个任意输入点，其与流形的距离是该输入满足感兴趣属性的概率的指示器。
- en: 'A subtle point is that the latent vector is the in-manifold component of the
    original point’s position vector. By switching to the latent vector representation,
    we lose the location of the point in the original higher-dimensional input space.
    We can go back to the higher-dimensional space by providing the location of the
    manifold for the lost orthogonal component, but doing so does not recover the
    original point: it recovers only the projection of the original point onto the
    subspace. We are replacing the individual orthogonal components with an aggregate
    entity the location of a manifold) but do not recover the exact original point.
    Some information is irretrievably lost during projection.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个微妙之处在于，潜在向量是原始点位置向量的流形内分量。通过切换到潜在向量表示，我们失去了点在原始高维输入空间中的位置。我们可以通过提供丢失的正交分量的流形位置来回到高维空间，但这样做并不能恢复原始点：它只能恢复原始点在子空间上的投影。我们用流形的位置（一个聚合实体）替换了单个正交分量，但并不能恢复原始点。在投影过程中，一些信息不可避免地丢失了。
- en: A special case of latent space representation is principal component analysis
    (PCA), introduced in section [4.4](../Text/04.xhtml#sec-pca) (section [14.4](#sec-pca-recap)
    provides a contextual recap of PCAs). It projects input points to an optimal planar
    latent subspace (as in figure [14.1a](#fig-linear-latent-subspace)). But except
    for some lucky special cases, the best latent subspace is not a hyperplane. It
    is a complex curved surface (see figure [14.1b](#fig-non-inear-latent-subspace)).
    Neural networks, such as autoencoders, can learn such nonlinear projections.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间表示的一个特殊情况是主成分分析（PCA），在第 [4.4](../Text/04.xhtml#sec-pca) 节中介绍（第 [14.4](#sec-pca-recap)
    节提供了 PCA 的上下文回顾）。它将输入点投影到最优的平面潜在子空间（如图 [14.1a](#fig-linear-latent-subspace)）。但除了某些幸运的特殊情况外，最佳的潜在子空间不是一个超平面。它是一个复杂的曲面（见图
    [14.1b](#fig-non-inear-latent-subspace)）。神经网络，如自动编码器，可以学习这种非线性投影。
- en: 14.2 Generative classifiers
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2 生成分类器
- en: 'During inferencing, the supervised classifiers we have encountered in previous
    chapters typically emit the class to which an input belongs, perhaps along with
    a bounding box. This is somewhat black-box-like behavior. We do not know how well
    the classifier has mastered the space except through the quantized end results.
    Such classifiers are called *discriminative* classifiers. On the other hand, latent
    space models map arbitrary input points to probabilities of belonging to the class
    of interest. Such models are called *generative* models, and they have some desirable
    properties:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，我们在前几章中遇到的监督分类器通常会输出输入所属的类别，可能还会附带一个边界框。这种行为有点像黑盒。我们不知道分类器是否很好地掌握了空间，除非通过量化的最终结果。这样的分类器被称为*判别分类器*。另一方面，潜在空间模型将任意输入点映射到属于感兴趣类别的概率。这样的模型被称为*生成模型*，它们具有一些理想的特性：
- en: '![](../../OEBPS/Images/CH14_F02a_Chaudhury.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH14_F02a_Chaudhury.png)'
- en: (a) A good discriminative classifier—smooth decision boundary
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 一个好的判别分类器——平滑的决策边界
- en: '![](../../OEBPS/Images/CH14_F02b_Chaudhury.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH14_F02b_Chaudhury.png)'
- en: (b) A bad discriminative classifier—irregular decision boundary
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 一个不好的判别分类器——不规则的决策边界
- en: '![](../../OEBPS/Images/CH14_F02c_Chaudhury.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH14_F02c_Chaudhury.png)'
- en: (c) Generative model—no decision boundary (heat map indicates the probability
    density)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 生成模型——没有决策边界（热图表示概率密度）
- en: Figure 14.2 Solid circles indicate training data points (all belonging to the
    class of interest). The dashed curve indicates the decision boundary separating
    the class of interest from the class of non-interest. In a generative model, there
    is no decision boundary. Every point in the space is associated with a probability
    of belonging to the class of interest (indicated as a heat map in figure [14.2c](#fig-generative-model))
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 实心圆表示训练数据点（所有属于感兴趣类别）。虚线曲线表示将感兴趣类别与非感兴趣类别分开的决策边界。在生成模型中，没有决策边界。空间中的每个点都与属于感兴趣类别的概率相关联（如图[14.2c](#fig-generative-model)中的热图所示）
- en: NOTE We can always create a discriminative classifier from a generative classifier
    by putting a threshold on the probability.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：我们可以通过在概率上设置阈值，总是从生成分类器创建一个判别分类器。
- en: '*Smoother, denser manifolds*—Discriminative models learn decision boundaries
    separating data points of interest from those not of interest in the input space.
    On the other hand, generative models try to model the distribution of the data
    points of interest in the input space using smooth probability density functions.
    As such, the generative model can''t learn a very irregularly shaped function
    that overfits the training data. This is illustrated in figure [14.2](#fig-discriminative-generative-model),
    whereas the discriminative model may converge to a manifold that follows the nooks
    and bends of the training data too closely (overfits) as in figure~\ref{fig-discriminative-bad-model}.
    This difference between discriminative and generative classifiers becomes especially
    significant when we have less training data. We can always create a discriminative
    classifier from a generative classifier by putting a threshold on the probability.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更平滑、更密集的流形*——判别模型学习在输入空间中分离感兴趣数据点和非感兴趣数据点的决策边界。另一方面，生成模型试图使用平滑的概率密度函数来模拟输入空间中感兴趣数据点的分布。因此，生成模型不能学习到一个非常不规则形状的函数，该函数过度拟合训练数据。这如图[14.2](#fig-discriminative-generative-model)所示，而判别模型可能会收敛到一个与训练数据弯曲和角落过于接近的流形（过度拟合），如图~\ref{fig-discriminative-bad-model}所示。当我们拥有的训练数据较少时，这种判别分类器和生成分类器之间的差异变得特别显著。我们可以通过在概率上设置阈值，总是从生成分类器创建一个判别分类器。'
- en: '*Extra insight*—Generative models offer more insight into the inner workings
    of the model. Consider a model that recognizes horses. Suppose we feed some horse
    images to the model, and it calls them horses (good). Then we feed the model some
    zebra images, and it calls them horses, too (bad). Do we have a useless~model
    that calls everything a horse? If it is a discriminative model, we must test it
    with totally different images (say, bird images) to get the answer. But if we
    have a generative model, it says the probabilities of the true horse images are,
    say, 0.9 and above, while the probabilities for the zebra images are around 0.7\.
    We begin to see that the model is behaving reasonably and does realize that zebras
    are less "horsy" than real horses.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*额外洞察*—生成模型对模型的内部运作提供了更多的洞察。考虑一个识别马匹的模型。假设我们向模型提供一些马匹图像，并且它将它们识别为马（好的）。然后我们向模型提供一些斑马图像，它也将它们识别为马（坏的）。我们是否有一个将所有东西都称为马的无效模型？如果它是一个判别模型，我们必须用完全不同的图像（比如鸟类图像）来测试它以获得答案。但是如果我们有一个生成模型，它说真实马匹图像的概率是，比如说，0.9以上，而斑马图像的概率大约是0.7。我们开始看到模型的行为是合理的，并且确实意识到斑马比真正的马“马性”要少。'
- en: '*New class instances*—A generative model learns the *distribution* of input
    points belonging to the class. An advantage related to learning the distribution
    is that we can sample the distribution to generate new members of the class (for
    example, to generate artificial horse images). This leads to the name *generative*
    modes. If we train a generative model with writings of Shakespeare, it will emit
    Shakespeare-like text pieces. Believe it or not, this has been tried with some
    success.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*新类实例*—生成模型学习属于该类的输入点的*分布*。与学习分布相关的一个优点是，我们可以采样分布以生成该类的新成员（例如，生成人工马匹图像）。这导致了“生成”模型的名字。如果我们用莎士比亚的作品训练一个生成模型，它将发出类似莎士比亚的文本片段。信不信由你，这已经尝试过并且取得了一些成功。'
- en: 14.3 Benefits and applications of latent-space modeling
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3 潜在空间建模的好处和应用
- en: 'Let’s recap at a high level why we want to do latent-space modeling:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下为什么我们想要进行潜在空间建模：
- en: '*Generative models are often based on latent space models*—all the benefits
    of generative modeling as outlined in section 14.2 apply to latent space modeling
    too.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成模型通常基于潜在空间模型*—第14.2节中概述的生成模型的所有好处也适用于潜在空间建模。'
- en: '*Attention to what matters*—Redundant information that does not contribute
    to the end goal is eliminated, and the system focuses on truly discriminative
    information. To visualize this, imagine an input data set of police mugshots consisting
    of people standing in front of the same background. Latent-space modeling trained
    to recognize people typically eliminates the common background from the representation
    and focuses on the photograph’s subject matter (people).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关注重要事项*—不贡献于最终目标的冗余信息被消除，系统专注于真正有区分性的信息。为了可视化这一点，想象一个由站在相同背景前的人的警察肖像组成的数据集。通常用于识别人的潜在空间建模会从表示中消除共同背景，并专注于照片的主题（人）。'
- en: '*Streamlined representation of data*—The latent vector is a more compact representation
    of the input vector (reduced dimensions and hence smaller) with no meaningful
    information lost.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据简化表示*—潜在向量是输入向量的更紧凑表示（减少了维度，因此更小），没有丢失任何有意义的信息。'
- en: '*Noise elimination*—Latent-space modeling eliminates the low-variance orthogonal-to-latent-subspace
    component of the data. This is mostly data that does not help in the problem of
    interest and hence is noise.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*噪声消除*—潜在空间建模消除了数据中与潜在子空间正交的低方差成分。这部分数据大多对感兴趣的问题没有帮助，因此是噪声。'
- en: '*Transformation to a manifold that is friendlier toward the end goal*—We have
    seen this notion previously, but here let’s look at an interesting simple example.
    Consider a set of 2D points in Cartesian coordinates (*x*, *y*). Suppose we want
    to classify the points into two sets: those that lie *inside* the circle *x*²
    + *y*² = *a*² and those that lie *outside* the circle. In the original Cartesian
    space, the decision boundary is not linear (it is circular). But if we transform
    the Cartesian input points to a latent space in polar coordinates—that is, each
    (*x*, *y*) is mapped to (*r*, *θ*) such that *x* = *rcos*(*θ*), *y* = *rsin*(*θ*)—the
    circle transforms into a line *r* = *a* in the latent space . A simple linear
    classifier *r* = *a* in the latent space can achieve the desired classification.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*转换到对最终目标更友好的流形*—我们之前已经看到过这个概念，但在这里让我们看看一个有趣的简单例子。考虑一组在笛卡尔坐标系中的二维点（*x*，*y*）。假设我们想要将点分类到两个集合中：那些位于圆内*x*²
    + *y*² = *a*²的点以及那些位于圆外的点。在原始的笛卡尔空间中，决策边界不是线性的（它是圆形的）。但如果我们将笛卡尔输入点转换到极坐标的潜在空间中——也就是说，每个(*x*，*y*)被映射到(*r*，*θ*)，使得*x*
    = *rcos*(*θ*)，*y* = *rsin*(*θ*)——圆在潜在空间中变成了线*r* = *a*。在潜在空间中一个简单的线性分类器*r* = *a*可以实现所需的分类。'
- en: 'Some applications of latent-space modeling are as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间建模的一些应用如下：
- en: Generating artificial images or text (as explained in the context of generative
    modeling).
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成人工图像或文本（如生成模型中的解释所述）。
- en: '*Similarity estimation between inputs*—If we map inputs to latent vectors,
    we can assess the similarity between inputs by computing the Euclidean distance
    between the latent vectors. Why is this better than taking the Euclidean distance
    between the input vectors? Suppose we are building a recommendation engine that
    suggests other clothing items “similar” to the one a potential buyer is currently
    browsing. We want to retrieve other clothing items that look similar but not identical
    to the one viewed. But similarity is a subjective concept, not quite measurable
    via the similarity of the inputs’ pixel colors. Consider a shirt with black vertical
    stripes on a white base. If we switch the stripe color with the base color, we
    get a shirt with white vertical stripes on a black base. If we do pixel-to-pixel
    color matching, these are very different, yet they are considered similar by humans.
    For this problem, we have to train the latent space model, creating neural networks
    so that images perceived to be similar by humans map to points in latent space
    that are close to each other. For example, both white-on-black and black-on-white
    shirts should map to latent vectors that are close to each other in the latent
    space even though they are far apart in the input space.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入之间的相似度估计*—如果我们将输入映射到潜在向量，我们可以通过计算潜在向量之间的欧几里得距离来评估输入之间的相似度。为什么这比计算输入向量之间的欧几里得距离更好？假设我们正在构建一个推荐引擎，该引擎建议其他与潜在买家当前浏览的服装项目“相似”的服装项目。我们希望检索看起来相似但并非完全相同的其他服装项目。但相似性是一个主观概念，不能通过输入像素颜色的相似性来衡量。考虑一件在白色底色上有黑色竖条纹的衬衫。如果我们交换条纹颜色和底色颜色，我们得到一件在黑色底色上有白色竖条纹的衬衫。如果我们进行像素到像素的颜色匹配，这些衬衫非常不同，但人类认为它们是相似的。对于这个问题，我们必须训练潜在空间模型，创建神经网络，使得人类感知为相似的图像映射到潜在空间中彼此靠近的点。例如，白色底色上的黑色条纹衬衫和黑色底色上的白色条纹衬衫应该在潜在空间中映射到彼此靠近的潜在向量，尽管它们在输入空间中相距甚远。'
- en: '*Image or other data compression*—The latent vector approximates the data with
    a smaller-dimensional vector that mimics the original vector as faithfully as
    possible. Thus the latent vector is a lossy compressed representation of the input.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像或其他数据压缩*—潜在向量用一个尽可能忠实于原始向量的较小维度的向量来近似数据。因此，潜在向量是输入的损失压缩表示。'
- en: '*Denoising*—The latent vector eliminates the non-meaningful part of the input
    information, which is noise.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降噪*—潜在向量消除了输入信息中的非有意义部分，即噪声。'
- en: NOTE Fully functional code for this chapter, executable via Jupyter Notebook,
    can be found at [http://mng.bz/6XG6](http://mng.bz/6XG6).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章的完整功能代码，可通过Jupyter Notebook执行，可在[http://mng.bz/6XG6](http://mng.bz/6XG6)找到。
- en: 14.4 Linear latent space manifolds and PCA
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4 线性潜在空间流形和PCA
- en: PCAs (which we discussed in section [4.4](../Text/04.xhtml#sec-pca)) project
    input data onto linear hyperplanar manifolds. Revisiting this topic will set up
    the correct context for the rest of this chapter. Consider a set of 3D input data
    points clustered closely around the *X*[0] = *X*[2] plane, as shown in figure
    [14.3](#fig-pca-3d-recap).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PCA（我们在第 [4.4](../Text/04.xhtml#sec-pca) 节中讨论过）将输入数据投影到线性超平面流形上。重新审视这个主题将为本章的其余部分设定正确的背景。考虑一组围绕
    *X*[0] = *X*[2] 平面紧密聚集的 3D 输入数据点，如图 [14.3](#fig-pca-3d-recap) 所示。
- en: '![](../../OEBPS/Images/CH14_F03a_Chaudhury.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH14_F03a_Chaudhury.png)'
- en: (a) Original 3D data
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始 3D 数据
- en: '![](../../OEBPS/Images/CH14_F03b_Chaudhury.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH14_F03b_Chaudhury.png)'
- en: (b) Lower-dimensional 2D representation obtained by setting the third principal
    value to zero
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 通过将第三个主值设为零获得的低维二维表示
- en: '![](../../OEBPS/Images/CH14_F03c_Chaudhury.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH14_F03c_Chaudhury.png)'
- en: (c) The principal vectors of the original data. The third principal vector is
    normal to *X*[0] = *X*[2] plane; the other two are in-plane.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 原始数据的主向量。第三个主向量垂直于 *X*[0] = *X*[2] 平面；其他两个在平面内。
- en: 'Figure 14.3 The original 3D data in figure [14.3a](#fig-pca-3d-recap-original-data)
    shows high correlation: points are clustered around the *X*[0] = *X*[2] plane.
    The first principal component corresponds to the direction of maximum variance.
    The last (third) principal reduced to a 2D latent vector.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3 图 [14.3a](#fig-pca-3d-recap-original-data) 中的原始 3D 数据显示高度相关性：点围绕 *X*[0]
    = *X*[2] 平面聚集。第一个主成分对应于最大方差的方向。最后一个（第三个）主成分被简化为二维潜在向量。
- en: NOTE We denote the successive axes (dimensions) as *X*[0], *X*[1], *X*[2] instead
    of the more traditional *X*, *Y*, *Z* for easy extension to higher dimensions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们用 *X*[0]、*X*[1]、*X*[2] 表示连续的轴（维度），而不是更传统的 *X*、*Y*、*Z*，以便于扩展到更高维度。
- en: 'Using PCA, we can recognize that the data has low variation along some dimensions.
    When we do PCA, we get the principal value and principal vector pairs. The largest
    principal value corresponds to the direction of maximum variance in the data.
    The corresponding principal vector yields that direction, and that principal value
    indicates the magnitude of the variance along that direction. The next principal
    value, the principal vector pair, is the orthogonal direction with the next-highest
    variance, and so on. For instance, in figure [14.3](#fig-pca-3d-recap), the principal
    vectors corresponding to the larger two principal values lie in the *X*[0] = *X*[2]
    plane, while the smallest principal value corresponds to the normal-to-plane vector.
    The third principal value is significantly smaller than the others. This tells
    us that variance along that axis is low, and components along that axis can be
    dropped with relatively little loss of information: that is, low reconstruction
    loss. The variations along the small principal value axes are likely noise, so
    eliminating them cleans up the data. In figure [14.3](#fig-pca-3d-recap), this
    effectively projects the data onto the *X*[0] = *X*[2] plane.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PCA，我们可以识别出数据在某些维度上的变化很小。当我们进行 PCA 时，我们得到主值和主向量对。最大的主值对应于数据中最大方差的方向。对应的主向量给出该方向，而该主值表示该方向上方差的大小。下一个主值、主向量对是具有下一个最高方差的正交方向，依此类推。例如，在图
    [14.3](#fig-pca-3d-recap) 中，对应于较大两个主值的主向量位于 *X*[0] = *X*[2] 平面上，而最小的主值对应于垂直于平面的向量。第三个主值显著小于其他值。这告诉我们该轴上的方差很低，并且沿该轴的分量可以相对少地丢失信息：即低重建损失。小主值轴上的变化很可能是噪声，因此消除它们可以清理数据。在图
    [14.3](#fig-pca-3d-recap) 中，这实际上将数据投影到 *X*[0] = *X*[2] 平面上。
- en: Dimensionality reduction
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 维度降低
- en: PCA essentially projects inputs to the *best-fit plane* for the training data.
    Assuming all the training data points are sampled with a common property, this
    plane represents that common property. By projecting, we eliminate that common
    property and retain only the discriminating aspects of the data. The eliminated
    information is remembered *approximately* in the parameters of the plane and supplied
    during reconstruction (aka decoding) to map us back to the same dimensionality
    as the input (but not exactly the same point). This is the essence of dimensionality
    reduction via PCA.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 实质上是将输入投影到训练数据的最佳拟合平面。假设所有训练数据点都是具有共同属性的样本，这个平面代表了这种共同属性。通过投影，我们消除了这种共同属性，只保留了数据的判别方面。消除的信息被近似地保存在平面的参数中，并在重建（也称为解码）过程中提供，以将我们映射回与输入相同的维度（但不是完全相同的点）。这是
    PCA 降维的本质。
- en: Following are the steps involved in PCA-based dimensionality reduction. This
    was described in detail with proofs in section [4.5](../Text/04.xhtml#sec-svd);
    here we recap the main steps without proof.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 PCA 基于的降维步骤。这在第 [4.5](../Text/04.xhtml#sec-svd) 节中已详细描述，并附有证明；这里我们回顾主要步骤而不进行证明。
- en: NOTE This treatment is similar but not identical to that in section [4.5](../Text/04.xhtml#sec-svd).
    Here we have switched the variables *m* and *n* to be consistent with our use
    of *n* to denote the data instance count. We have also switched to a slightly
    different flavor of the SVD.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这种处理与第 [4.5](../Text/04.xhtml#sec-svd) 节中的处理类似，但并不完全相同。在这里，我们将变量 *m* 和 *n*
    交换，以与我们的使用习惯一致，即用 *n* 表示数据实例的数量。我们还采用了稍微不同的 SVD 表达方式。
- en: Represent the data as a matrix *X*, where each row is an individual data instance.
    The number of rows *n* is the size of the data set. The number of columns *d*
    is the original (input) dimensionality of the data. Thus *X* is a *n* × *d* matrix.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据表示为矩阵 *X*，其中每一行是一个单独的数据实例。行数 *n* 是数据集的大小。列数 *d* 是数据的原始（输入）维度。因此 *X* 是一个 *n*
    × *d* 矩阵。
- en: Compute the mean data vector
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算均值数据向量
- en: '![](../../OEBPS/Images/eq_14-00-a.png)'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_14-00-a.png)'
- en: where ![](../../OEBPS/Images/AR_x.png)^((*i*)) for *i* = 1 to *i* = *n* denote
    the training data vector instances (which form rows of the matrix *X*).
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 ![](../../OEBPS/Images/AR_x.png)^((*i*)) 对于 *i* = 1 到 *i* = *n* 表示训练数据向量实例（这些实例构成了矩阵
    *X* 的行）。
- en: 'Shift the origin of the coordinate system to the mean by subtracting the mean
    vector from each data vector:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将坐标系的原点平移到均值处，通过从每个数据向量中减去均值向量来实现：
- en: '![](../../OEBPS/Images/AR_x.png)^((*i*)) = ![](../../OEBPS/Images/AR_x.png)^((*i*))
    – ![](../../OEBPS/Images/AR_micro.png) for all *i*'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_x.png)^((*i*)) = ![](../../OEBPS/Images/AR_x.png)^((*i*))
    – ![](../../OEBPS/Images/AR_micro.png) 对于所有 *i*'
- en: The data matrix *X* now has the mean-subtracted data instances as rows.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据矩阵 *X* 现在的行是减去均值的数据实例。
- en: The matrix *X^TX* where *X* is the mean-subtracted data matrix) is the covariance
    matrix (as discussed in detail in section [5.7.2](../Text/05.xhtml#sec-var-covar-std)).
    The eigenvalue, eigenvector pairs of the matrix *X^TX* are known as principal
    values and principal vectors (together referred to as principal components). Since
    *X^TX* is a *d* × *d* matrix, there are *d* scalar eigenvalues and *d* eigenvectors,
    each of dimension *d* × 1. Let’s denote the principal components as (*λ*[1], ![](../../OEBPS/Images/AR_v.png)[1]),
    (*λ*[2], ![](../../OEBPS/Images/AR_v.png)[2]), ⋯, (*λ[dm]*, ![](../../OEBPS/Images/AR_v.png)*[d]*).
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 矩阵 *X^TX*（其中 *X* 是减去均值的数据矩阵）是协方差矩阵（如第 [5.7.2](../Text/05.xhtml#sec-var-covar-std)
    节中详细讨论的）。矩阵 *X^TX* 的特征值和特征向量对被称为主值和主向量（统称为主成分）。由于 *X^TX* 是一个 *d* × *d* 矩阵，因此有
    *d* 个标量特征值和 *d* 个特征向量，每个特征向量的维度为 *d* × 1。让我们将主成分表示为 (*λ*[1]，![](../../OEBPS/Images/AR_v.png)[1])，(*λ*[2]，![](../../OEBPS/Images/AR_v.png)[2])，⋯，(*λ[dm]*，![](../../OEBPS/Images/AR_v.png)*[d]*)。
- en: We can assume *λ*[1] ≥ *λ*[2] ≥ ⋯ ≥ *λ[d]* if necessary, we can make this true
    by renumbering the principal components). Then the first principal component corresponds
    to the direction of maximum variance in the data (proof with geometrical intuition
    can be found in section [5.7.2](../Text/05.xhtml#sec-var-covar-std)). The corresponding
    principal value yields the actual variance. The next principal value corresponds
    to the second-highest variance (among directions orthogonal to the first principal
    direction), and so forth. For every component, the principal value yields the
    actual variance, and the principal vector yields the direction.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有必要，我们可以假设 *λ*[1] ≥ *λ*[2] ≥ ⋯ ≥ *λ[d]*。如果需要，我们可以通过重新编号主成分来实现这一点）。第一个主成分对应于数据中最大方差的方向（带有几何直觉的证明可以在第
    [5.7.2](../Text/05.xhtml#sec-var-covar-std) 节找到）。对应的主值给出了实际的方差。下一个主值对应于第二高的方差（在第一个主方向正交的方向中），以此类推。对于每个成分，主值给出了实际的方差，而主向量给出了方向。
- en: 'Consider the matrix of principal vectors:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑主向量的矩阵：
- en: '*V* = [![](../../OEBPS/Images/AR_v.png)[1] ![](../../OEBPS/Images/AR_v.png)[2]
    … *![](../../OEBPS/Images/AR_v.png)[d]*]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*V* = [![](../../OEBPS/Images/AR_v.png)[1] ![](../../OEBPS/Images/AR_v.png)[2]
    … *![](../../OEBPS/Images/AR_v.png)[d]*]'
- en: If we want the data to be a space with *m* dimensions with minimal loss of information,
    we should drop the last *m* vectors of *V*. This eliminates the *m* least-variance
    dimensions. Dropping the last *m* vectors from *V* yields a matrix
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们希望数据是一个具有 *m* 维度的空间，并且信息损失最小，我们应该丢弃 *V* 的最后 *m* 个向量。这消除了 *m* 个最小方差维度。从 *V*
    中丢弃最后 *m* 个向量得到一个矩阵
- en: '*V[d–m]* = [![](../../OEBPS/Images/AR_v.png)[1] ![](../../OEBPS/Images/AR_v.png)[2]
    … *![](../../OEBPS/Images/AR_v.png)[d–m]*]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*V[d–m]* = [![](../../OEBPS/Images/AR_v.png)[1] ![](../../OEBPS/Images/AR_v.png)[2]
    … *![](../../OEBPS/Images/AR_v.png)[d–m]*]'
- en: Note that the best way to obtain the *V* matrix is to perform SVD on the mean-subtracted
    *X* (see section [4.5](../Text/04.xhtml#sec-svd)).
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，获取 *V* 矩阵的最佳方式是对减去均值的 *X* 执行奇异值分解（见第 [4.5](../Text/04.xhtml#sec-svd) 节）。
- en: Premultiplying *V[d−m]*, the truncated principal vectors matrix, with the original
    data matrix *X* projects the data onto a space corresponding to the first *d*
    − *m* principal components. Thus, to create *d* − *m*-dimensional linearly encoded
    latent vectors from *d*-dimensional data,
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预乘截断的主向量矩阵 *V[d−m]* 与原始数据矩阵 *X* 相乘，将数据投影到对应于前 *d* − *m* 个主成分的空间。因此，要从 *d*-维数据创建
    *d* − *m*-维线性编码的潜在向量，
- en: '*X[d−m]* = *XV[d−m]*'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*X[d−m]* = *XV[d−m]*'
- en: '*X[d−m]* is the reduced dimension data set. Its dimensionality is *n* × (*d*−*m*).'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*X[d−m]* 是降维后的数据集。其维度是 *n* × (*d*−*m*).'
- en: It can be shown that
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以证明：
- en: '*XV[d−m]* = *UΣ[d−m]*'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*XV[d−m]* = *UΣ[d−m]*'
- en: where *U* is from SVD (see section [4.5](../Text/04.xhtml#sec-svd)) and *Σ[d−m]*
    is a truncated version of the diagonal matrix Σ from SVD with its smallest *m*
    elements chopped off. This offers an alternative way to do PCA-based dimensionality
    reduction.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 *U* 来自奇异值分解（见第 [4.5](../Text/04.xhtml#sec-svd) 节），而 *Σ[d−m]* 是奇异值分解中Σ对角矩阵的截断版本，其最小的
    *m* 个元素被截掉。这为基于PCA的降维提供了一种替代方法。
- en: 'How do we reconstruct? In other words, what is the decoder? Well, to reconstruct,
    we need to save the original principal vectors: that is, the *V* matrix. If we
    have that, we can introduce *m* zeros at the right of every row in *X[d−m]* to
    make it a *n* × *d* matrix again. Then we post-multiply by *V^T*, which rotates
    the coordinate system back from one with principal vectors as axes to one with
    the original input axes. Finally, we add the mean ![](../../OEBPS/Images/AR_micro.png)
    to each row to shift the origin back to its original position, which yields the
    reconstructed data matrix *X̃*. The reconstruction loss is ||*X* − *X̃*||². Note
    that, in effect, *X̃* is *UΣV^T* with the last *m* diagonal elements of Σ set
    to zero.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何重建？换句话说，解码器是什么？好吧，为了重建，我们需要保存原始的主向量：即 *V* 矩阵。如果我们有那个，我们可以在 *X[d−m]* 的每一行的右侧引入
    *m* 个零，使其再次成为 *n* × *d* 矩阵。然后我们后乘以 *V^T*，这会将坐标系从以主向量为轴的坐标系旋转回以原始输入轴为轴的坐标系。最后，我们将均值
    ![](../../OEBPS/Images/AR_micro.png) 添加到每一行，将原点移回到其原始位置，从而得到重建的数据矩阵 *X̃*。重建损失是
    ||*X* − *X̃*||²。请注意，实际上，*X̃* 是 *UΣV^T*，其中Σ的最后一个 *m* 个对角元素被设置为零。
- en: The reconstructed data *X̃* is *not* identical to the original data. The information
    we lost during dimensionality reduction the normal-to-plane components) is lost
    permanently. Nonetheless, this principled way of dropping information ensures
    that the reconstruction loss is minimal in some sense, at least among all *X̃*
    linearly related to *X*.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重构的数据 *X̃* 与原始数据并不相同。我们在降维过程中丢失的信息（尽管是微小的）是永久丢失的。尽管如此，这种删除信息的原则性方法确保了重建损失在某种意义上是最小的，至少在所有与
    *X* 线性相关的 *X̃* 中。
- en: 14.4.1 PyTorch code for dimensionality reduction using PCA
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4.1 使用PCA进行降维的PyTorch代码
- en: Now, let’s implement dimensionality reduction in PyTorch. Let *X* be a data
    matrix representing points clustered around the *X*[0] = *X*[2] plane. *X* is
    of shape [1000,3], with each row of *X* representing a three-dimensional data
    point. The following listing shows how to project *X* into a lower-dimensional
    space with minimal loss of information. It also shows how to reconstruct the original
    data points from the lower-dimensional representations. Note that the reconstructions
    are approximate because we have lost information (albeit minimal) in the dimensionality-reduction
    process.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在PyTorch中实现降维。设 *X* 为表示围绕 *X*[0] = *X*[2] 平面聚类的数据矩阵。*X* 的形状为 [1000,3]，其中
    *X* 的每一行代表一个三维数据点。以下列表展示了如何以最小信息损失将 *X* 投影到低维空间，同时也展示了如何从低维表示中重建原始数据点。请注意，由于我们在降维过程中丢失了信息（尽管是微小的），重建是近似的。
- en: NOTE Fully functional code for dimensionality reduction using PCA, executable
    via Jupyter Notebook, can be found at [http://mng.bz/7yJg](http://mng.bz/7yJg).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：使用PCA进行降维的完整功能代码，可通过Jupyter Notebook执行，可在[http://mng.bz/7yJg](http://mng.bz/7yJg)找到。
- en: Listing 14.1 PyTorch- PCA revisited
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.1 PyTorch- PCA回顾
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Data matrix of shape (1000, 3)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ① 数据矩阵的形状为（1000，3）
- en: ② Stores the mean so we can reconstruct the original data points later
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ② 存储均值，以便我们稍后重建原始数据点
- en: ③ Subtracts the mean before performing SVD
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在执行SVD之前减去均值
- en: ④ Runs SVD
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 执行SVD
- en: ⑤ Columns of V are the principal vectors.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ V的列是主向量。
- en: ⑥ Removes the last principal vector. This is along the direction of least variance
    perpendicular to *X*[0] = *X*[2] plane).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 移除最后一个主向量。这是沿着垂直于 *X*[0] = *X*[2] 平面的最小方差方向。
- en: ⑦ Projects the input data points into the lower-dimensional space
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 将输入数据点投影到低维空间
- en: ⑧ Pads with zeros to make an *n* × *d* matrix
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 用零填充以形成一个 *n* × *d* 矩阵
- en: ⑨ Post-multiplies with *V^T* to project back to the original space
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 后乘以 *V^T* 以将数据投影回原始空间
- en: ⑩ Adds the mean
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 添加均值
- en: 14.5 Autoencoders
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.5 自动编码器
- en: Autoencoders are neural network systems trained to generate latent-space representations
    corresponding to specified inputs. They can do nonlinear projections and hence
    are more powerful than PCA systems see figure [14.4](#fig-curved-underlying-pattern-recap)).
    The neural network mapping the input vector to a latent vector is called an *encoder*.
    We also train a neural network called a *decoder* that maps the latent vector
    back to the input space. The decoder output is the reconstructed input from the
    latent vector. The reconstructed input (that is, the output of the decoder) will
    never match the original input exactly—information was lost during encoding and
    cannot be brought back—but we can try to ensure that they match as closely as
    possible within the constraints of the system. The reconstruction loss is a measure
    of the difference between the original input and the reconstructed input. The
    encoder-decoder pair is trained end to end to minimize reconstruction loss (along
    with, potentially, some other losses). This is an example of *representation learning*,
    whereby we learn to represent input vectors with smaller latent vectors representing
    the input as closely as possible in the stipulated size budget. The budgeted size
    of the latent space is a hyperparameter.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是经过训练以生成对应于指定输入的潜在空间表示的神经网络系统。它们可以进行非线性投影，因此比 PCA 系统更强大，见图 [14.4](#fig-curved-underlying-pattern-recap))。将输入向量映射到潜在向量的神经网络称为
    *编码器*。我们还训练了一个称为 *解码器* 的神经网络，它将潜在向量映射回输入空间。解码器的输出是从潜在向量重构的输入。重构输入（即解码器的输出）永远不会与原始输入完全匹配——在编码过程中丢失了信息，无法恢复——但我们可以在系统的约束条件下尽量确保它们尽可能接近。重构损失是原始输入和重构输入之间差异的度量。编码器-解码器对从头到尾训练以最小化重构损失（可能还有其他损失）。这是一个
    *表示学习* 的例子，其中我们学习用较小的潜在向量来表示输入向量，以尽可能接近在规定的尺寸预算内表示输入。潜在空间的预算大小是一个超参数。
- en: '![](../../OEBPS/Images/CH14_F04_Chaudhury.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图像 14.4](../../OEBPS/Images/CH14_F04_Chaudhury.png)'
- en: Figure 14.4 A 2 data distribution with a curved underlying pattern. It is impossible
    to find a straight line or vector such that all points are near it. PCA will not
    do well.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4 具有弯曲潜在模式的 2 维数据分布。不可能找到一个直线或向量，使得所有点都靠近它。PCA 将表现不佳。
- en: NOTE A hyperparameter is a neural network parameter that is *not* learned. Its
    value is set based on our knowledge of the system and held constant during training.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：超参数是一个 *未学习* 的神经网络参数。其值基于我们对系统的了解设置，并在训练过程中保持不变。
- en: 'The desired output is implicitly known in autoencoders: it is the input. Consequently,
    no human labeling is needed to train autoencoders; they are *unsupervised*. An
    autoencoder is shown schematically in figure [14.5](#fig-ae-schematic).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在自编码器中，期望的输出是隐含已知的：它是输入。因此，不需要人工标记来训练自编码器；它们是 *无监督的*。自编码器在图 [14.5](#fig-ae-schematic)
    中以示意图的形式展示。
- en: '![](../../OEBPS/Images/CH14_F05_Chaudhury.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图像 14.5](../../OEBPS/Images/CH14_F05_Chaudhury.png)'
- en: Figure 14.5 Schematic representation of an autoencoder. The encoder transforms
    input into a latent vector. The decoder transforms the latent vector into reconstructed
    input. We minimize the reconstruction loss—the distance between the reconstructed
    input and the original input.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5 自编码器的示意图。编码器将输入转换为一个潜在向量。解码器将潜在向量转换回重构输入。我们最小化重构损失——重构输入和原始输入之间的距离。
- en: The encoder takes an input ![](../../OEBPS/Images/AR_x.png) and maps it to a
    lower-dimensional latent vector ![](../../OEBPS/Images/AR_z.png). An example of
    an encoding neural network for image inputs is shown in listing [14.2](#code-ae-encoder).
    Note how the image height and width keep decreasing with each successive sequence
    of convolution, ReLU, and max pool layers.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器接收一个输入 ![编码器输入](../../OEBPS/Images/AR_x.png) 并将其映射到一个低维潜在向量 ![潜在向量](../../OEBPS/Images/AR_z.png)。一个用于图像输入的编码神经网络示例在列表
    [14.2](#code-ae-encoder) 中展示。注意图像的高度和宽度在每次连续的卷积、ReLU 和最大池化层之后都会逐渐减小。
- en: The decoder is a neural network that generates reconstructed image ![](../../OEBPS/Images/AR_x3.png)
    from the latent vector ![](../../OEBPS/Images/AR_z.png). Listing [14.3](#code-ae-decoder)
    shows an example of a decoder neural network. Note the transposed convolutions
    and how the height and width of the image keep increasing with each successive
    sequence of transposed convolution, batch normalization, and ReLU. Transposed
    convolutions are discussed in section [10.5](../Text/10.xhtml#sec-transposed-conv).)
    The decoder essentially remembers—not exactly, but in an average sense—the information
    discarded during encoding. Equivalently, it remembers the position of the latent
    space manifold in the overall input space. Adding that back to the latent space
    representation takes us back to the same dimensionality as the input vector but
    not to the same input point.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器是一个神经网络，它从潜在向量 ![](../../OEBPS/Images/AR_z.png) 生成重构图像 ![](../../OEBPS/Images/AR_x3.png)。列表
    [14.3](#code-ae-decoder) 展示了一个解码器神经网络的示例。注意转置卷积以及图像的高度和宽度如何随着每个连续的转置卷积、批量归一化和
    ReLU 序列而不断增加。转置卷积在第 [10.5](../Text/10.xhtml#sec-transposed-conv) 节中讨论。解码器本质上记得——不是完全准确，但平均意义上——编码过程中丢弃的信息。等价地，它记得潜在空间流形在整体输入空间中的位置。将这一点加回到潜在空间表示中，使我们回到与输入向量相同的维度，但不是相同的输入点。
- en: 'The system minimizes the information loss from the encoding (the reconstruction
    loss). We are ensuring that for each input, the corresponding latent vector produced
    by the encoder can be mapped back to a reconstructed value by the decoder that
    is as close as possible to the input. Equivalently, each latent vector is a faithful
    representation of the input, and there is a 1 : 1 mapping between inputs and latent
    vectors.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统最小化编码（重构损失）的信息损失。我们确保对于每个输入，编码器产生的对应潜在向量可以通过解码器映射回一个重构值，该值尽可能接近输入。等价地，每个潜在向量是输入的忠实表示，输入和潜在向量之间存在
    1:1 的映射。
- en: The encoder and decoder need not be symmetric.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器和解码器不必是对称的。
- en: Mathematically,
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，
- en: '![](../../OEBPS/Images/eq_14-00-b.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_14-00-b.png)'
- en: The end-to-end system is trained to minimize the loss ℒ.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端系统被训练以最小化损失 ℒ。
- en: NOTE Fully functional code for autoencoders, executable via Jupyter Notebook,
    can be found at [http://mng.bz/mOzM](http://mng.bz/mOzM).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：完全功能的自动编码器代码，可通过 Jupyter Notebook 执行，可在 [http://mng.bz/mOzM](http://mng.bz/mOzM)
    找到。
- en: Listing 14.2 PyTorch- Autoencoder encoder
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.2 PyTorch- 自动编码器编码器
- en: '[PRE1]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Input image size in (c, h, w) format
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ① 以 (c, h, w) 格式输入图像大小
- en: ② Reduces to a (32, 16, 16)-sized tensor
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ② 减少到 (32, 16, 16)-大小的张量
- en: ③ Reduces to a (128, 8, 8)-sized tensor
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 减少到 (128, 8, 8)-大小的张量
- en: ④ Reduces to a (256, 4, 4)-sized tensor
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 减少到 (256, 4, 4)-大小的张量
- en: ⑤ Flattens to a 4096-sized tensor
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 展平为 4096 大小的张量
- en: ⑥ Reduces the 4096-sized tensor to an nz-sized tensor
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将 4096 大小的张量减少到 nz 大小的张量
- en: Listing 14.3 PyTorch- Autoencoder decoder
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.3 PyTorch- 自动编码器解码器
- en: '[PRE2]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Converts (nz, 1, 1) to a 256, 4, 4)-sized tensor
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将 (nz, 1, 1) 转换为 256, 4, 4)-大小的张量
- en: ② Increases to a 128, 8, 8)-sized tensor
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ② 增加到 128, 8, 8)-大小的张量
- en: ③ Increases to a 32, 16, 16)-sized tensor
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 增加到 32, 16, 16)-大小的张量
- en: ④ Increases to a 1, 32, 32)-sized tensor
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 增加到 1, 32, 32)-大小的张量
- en: Listing 14.4 PyTorch- Autoencoder training
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.4 PyTorch- 自动编码器训练
- en: '[PRE3]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Passes the input image through the convolutional encoder
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将输入图像通过卷积编码器传递
- en: ② Reduces to nz dimensions
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ② 减少到 nz 维度
- en: ③ Reconstructs the image using z via the decoder
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 通过解码器使用 z 重构图像
- en: ④ Computes the reconstruction loss
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算重构损失
- en: 14.5.1 Autoencoders and PCA
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.5.1 自动编码器和 PCA
- en: It is important to realize that autoencoders perform a much more powerful dimensionality
    reduction than PCA. PCA is a linear process; it can only project data points to
    best-fit hyperplanes. Autoencoders can fit arbitrary complex nonlinear hypersurfaces
    to the data, limited only by the expressive powers of the encoder-decoder pair.
    If the encoder and decoder have only a single linear layer (no ReLU or other nonlinearity),
    then the autoencoder projects the data points to a hyperplane like PCA not necessarily
    the same hyperplane).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要认识到，自动编码器执行的降维比 PCA 更强大。PCA 是一个线性过程；它只能将数据点投影到最佳拟合超平面。自动编码器可以将任意复杂的非线性超曲面拟合到数据中，仅受编码器-解码器对的表达能力限制。如果编码器和解码器只有一个线性层（没有
    ReLU 或其他非线性），那么自动编码器将数据点投影到超平面，类似于 PCA，但不一定是同一个超平面）。
- en: 14.6 Smoothness, continuity, and regularization of latent spaces
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.6 潜在空间的平滑性、连续性和正则化
- en: Minimizing the reconstruction loss does not yield a unique solution. For instance,
    figure [14.6](#fig-latent-space-regularization) shows two examples of transforming
    2D inputs into 1D latent-space representations, linear and curved, respectively.
    Both the regularized solid line) and the non-regularized zigzag manifold (dashed
    line) fit the training data well with low reconstruction error. But the former
    is smoother and more desirable.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化重建损失并不产生唯一解。例如，图[14.6](#fig-latent-space-regularization)展示了将2D输入转换为1D潜在空间表示的两个示例，分别是线性和曲线的。经过正则化的实线（solid
    line）和非正则化的锯齿形流形（dashed line）都很好地拟合了训练数据，具有低重建误差。但前者更平滑，更令人满意。
- en: '![](../../OEBPS/Images/CH14_F06a_Chaudhury.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH14_F06a_Chaudhury.png)'
- en: (a) Linear latent space
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 线性潜在空间
- en: '![](../../OEBPS/Images/CH14_F06b_Chaudhury.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH14_F06b_Chaudhury.png)'
- en: (b) Curved latent space
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 曲线潜在空间
- en: Figure 14.6 Two examples of mapping from a 2D input space to a 1D latent space.
    Both show regularized solid) vs. unregularized (dashed) latent space manifolds.
    Solid little circles depict training data points.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6 从2D输入空间映射到1D潜在空间的两个示例。两者都显示了正则化（实线）与非正则化（虚线）的潜在空间流形。实心小圆表示训练数据点。
- en: Note the pair of points marked *p*[1], *p*[2] and *p*[3], *p*[4](square markers).
    The distance between them is more or less the same in the input space. But when
    projected on the dashed curve (unregularized latent space), their distances (measured
    along the curve) become quite different. This is undesirable and does not happen
    in the regularized latent space (here, the distance is measured along a solid
    line). It becomes much more pronounced in high dimensions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意标记为*p*[1]、*p*[2]和*p*[3]、*p*[4]（正方形标记）的点对。在输入空间中，它们之间的距离大致相同。但当投影到虚线曲线（非正则化潜在空间）上时，它们的距离（沿曲线测量）变得相当不同。这是不希望的，并且在正则化潜在空间中不会发生（在这里，距离是沿实线测量的）。在高维情况下，这种情况变得更加明显。
- en: The zigzag curve segment containing the training data set is longer than the
    smooth one. A good latent manifold typically has fewer twists and turns (is smooth)
    and hence has a “length” that is minimal in a sense. This is reminiscent of the
    minimum descriptor length (MDL) principle, which we discussed in section [9.3.1](../Text/09.xhtml#sec-MDL).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 包含训练数据集的锯齿形曲线段比平滑的曲线段更长。一个好的潜在流形通常有更少的扭曲和转弯（更平滑），因此在某种意义上“长度”是最小的。这让人想起了我们在第[9.3.1](../Text/09.xhtml#sec-MDL)节中讨论的最小描述符长度（MDL）原则。
- en: How do we ensure that this smoothest latent space is chosen over others that
    also minimize the reconstruction loss? By putting additional constraints (losses)
    over and above the ubiquitous reconstruction loss. Recall the notion of regularization,
    which we looked at in sections [6.6.3](../Text/06.xhtml#sec-MAP_estimation) and
    [9.3](../Text/09.xhtml#sec-regularization). There we introduced an explicit loss
    that penalizes longer solutions (which was equivalent to maximizing the a posteriori
    probability of parameter values as opposed to the likelihood). A related approach
    that we explore in this chapter is to model the latent space as a probability
    distribution belonging to a known family (for example, Gaussian) and minimize
    the difference (KL divergence) of this estimated distribution from a zero-mean
    univariance Gaussian. The encoder-decoder neural network pair is trained end to
    end to minimize a loss that is a weighted sum of the reconstruction loss and this
    KL divergence. Trying to remain close to the zero-mean unit-variance Gaussian
    penalizes departure from compactness and smoothness. This is the basic idea of
    variational autoencoders VAEs).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何确保选择最平滑的潜在空间，而不是其他也能最小化重建损失的潜在空间？通过在无处不在的重建损失之上添加额外的约束（损失）。回想一下我们在第[6.6.3](../Text/06.xhtml#sec-MAP_estimation)和[9.3](../Text/09.xhtml#sec-regularization)节中讨论的正则化概念。在那里，我们引入了一个显式的损失，它惩罚较长的解（这与最大化参数值的后验概率而不是似然性相当）。在本章中，我们探索的一个相关方法是，将潜在空间建模为属于已知家族（例如高斯）的概率分布，并最小化此估计分布与零均值单变量高斯分布之间的差异（KL散度）。编码器-解码器神经网络对从头到尾训练以最小化一个损失，它是重建损失和此KL散度的加权总和。试图保持接近零均值单位方差高斯分布惩罚了从紧凑性和平滑性中偏离。这是变分自编码器（VAEs）的基本思想）。
- en: The overall effect of regularization is to create a latent space that is more
    compact. If we only minimize the reconstruction loss, the system can achieve that
    by mapping points very far from each other (space being infinite). Regularization
    combats that and incentivizes the system to not map the training points too far
    from one another. It tries to limit the total latent-space volume occupied by
    the points corresponding to the training inputs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的总体效果是创建一个更紧凑的潜在空间。如果我们只最小化重建损失，系统可以通过将点映射得非常远（空间是无限的）来实现这一点。正则化对抗这种情况，并激励系统不要将训练点映射得太远。它试图限制对应于训练输入的点在潜在空间中占据的总体积。
- en: 14.7 Variational autoencoders
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.7 变分自编码器
- en: 'VAEs are a special case of autoencoders. They have the same architecture: a
    pair of neural networks that encode and decode the input vector, respectively.
    They also have the reconstruction loss term. But they have an additional loss
    term called KL divergence loss that we explain shortly.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 是自编码器的一种特殊情况。它们具有相同的架构：一对神经网络分别对输入向量进行编码和解码。它们也有重建损失项。但它们还有一个额外的损失项，称为KL散度损失，我们将在下面简要解释。
- en: NOTE Throughout this chapter, we denote latent variables with ![](../../OEBPS/Images/AR_z.png)
    and input variables with ![](../../OEBPS/Images/AR_x.png).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本章中，我们用 ![](../../OEBPS/Images/AR_z.png) 表示潜在变量，用 ![](../../OEBPS/Images/AR_x.png)
    表示输入变量。
- en: 14.7.1 Geometric overview of VAEs
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.7.1 VAEs 的几何概述
- en: Figure [14.7](#fig-latent_space_distributions) attempts to provide a geometrical
    view of VAE latent-space modeling. During training, given an input ![](../../OEBPS/Images/AR_x.png),
    the encoder does not directly emit the corresponding latent-space representation
    ![](../../OEBPS/Images/AR_z.png). Instead, the encoder emits the parameters of
    a distribution from a prechosen family. For instance, if the prechosen family
    is Gaussian, the encoder emits a pair of parameter values ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)),
    **Σ**(![](../../OEBPS/Images/AR_x.png)). These are the mean and covariance matrix
    of a specific Gaussian distribution 𝒩(![](../../OEBPS/Images/AR_z.png); ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)),
    **Σ**(![](../../OEBPS/Images/AR_x.png))) in the latent space. The latent-space
    representation ![](../../OEBPS/Images/AR_z.png) corresponding to the input ![](../../OEBPS/Images/AR_x.png)
    is obtained by sampling this distribution emitted by the encoder. Thus in the
    Gaussian case, we have ![](../../OEBPS/Images/AR_z.png) ∼ 𝒩(![](../../OEBPS/Images/AR_z.png);
    ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)), **Σ**(![](../../OEBPS/Images/AR_x.png))).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [14.7](#fig-latent_space_distributions) 尝试提供 VAE 潜在空间建模的几何视图。在训练过程中，给定一个输入
    ![](../../OEBPS/Images/AR_x.png)，编码器不会直接发出相应的潜在空间表示 ![](../../OEBPS/Images/AR_z.png)。相反，编码器发出来自预先选择的分布族的一组参数。例如，如果预先选择的族是高斯分布，编码器会发出一对参数值
    ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)), **Σ**(![](../../OEBPS/Images/AR_x.png))。这些是特定高斯分布
    𝒩(![](../../OEBPS/Images/AR_z.png); ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)),
    **Σ**(![](../../OEBPS/Images/AR_x.png))) 在潜在空间中的均值和协方差矩阵。与输入 ![](../../OEBPS/Images/AR_x.png)
    对应的潜在空间表示 ![](../../OEBPS/Images/AR_z.png) 是通过采样编码器发出的这个分布获得的。因此，在高斯情况下，我们有 ![](../../OEBPS/Images/AR_z.png)
    ∼ 𝒩(![](../../OEBPS/Images/AR_z.png); ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)),
    **Σ**(![](../../OEBPS/Images/AR_x.png)))。
- en: NOTE The symbol ∼ indicates sampling from a distribution.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：符号 ∼ 表示从分布中进行采样。
- en: '![](../../OEBPS/Images/CH14_F07_Chaudhury.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH14_F07_Chaudhury.png)'
- en: Figure 14.7 Geometric depiction of VAE latent-space modeling distributions
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7 VAE 潜在空间建模分布的几何描述
- en: This distribution, which we call the *latent-space map* of the input ![](../../OEBPS/Images/AR_x.png),
    is shown by hollow circles with dark borders in figure [14.7](#fig-latent_space_distributions).
    Such mapping is called *stochastic mapping*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分布，我们称之为输入 ![](../../OEBPS/Images/AR_x.png) 的 *潜在空间映射*，在图 [14.7](#fig-latent_space_distributions)
    中用带有深色边框的空心圆表示。这种映射称为 *随机映射*。
- en: 'The latent-space map distribution should have a narrow, single-peaked probability
    density function (for example, a Gaussian with small variance: that is, small
    ||**Σ**||). The narrow-peakedness of the probability density function implies
    that the cloud of sample points forms a tight, small cluster—any random sample
    from the distribution will likely be close to the mean. So, sampling ![](../../OEBPS/Images/AR_z.png)
    from such a distribution is not very different from a deterministic mapping from
    ![](../../OEBPS/Images/AR_x.png) to ![](../../OEBPS/Images/AR_z.png) = ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)).
    This sampling to obtain the latent vector is done only during training. During
    inferencing, we use the mean emitted by the encoder directly as the latent-space
    representation of the input: that is, ![](../../OEBPS/Images/AR_z.png) = ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间映射分布应该具有狭窄的单峰概率密度函数（例如，方差很小的高斯分布：即小的||**Σ**||）。概率密度函数的狭窄峰度意味着样本点云形成一个紧密的小簇——从该分布中随机抽取的任何样本很可能接近均值。因此，从这样的分布中采样![](../../OEBPS/Images/AR_z.png)与从![](../../OEBPS/Images/AR_x.png)到![](../../OEBPS/Images/AR_z.png)
    = ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png))的确定性映射没有太大区别。这种采样以获得潜在向量仅在训练期间进行。在推理期间，我们直接使用编码器发出的均值作为输入的潜在空间表示：即，![](../../OEBPS/Images/AR_z.png)
    = ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png))。
- en: The decoder maps the latent vector representation ![](../../OEBPS/Images/AR_z.png)
    back to a point, say *x̃*, in the input space. This is the reconstructed version
    of the input vector (shown by a little white square with a black border in figure
    [14.7](#fig-latent_space_distributions)). The decoder is thus estimating (reconstructing)
    the input given the latent vector.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器将潜在向量表示![](../../OEBPS/Images/AR_z.png)映射回输入空间中的一个点，例如*x̃*。这是输入向量的重建版本（在图[14.7](#fig-latent_space_distributions)中由一个带有黑色边框的小白方块表示）。因此，解码器是根据潜在向量估计（重建）输入的。
- en: 14.7.2 VAE training, losses, and inferencing
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.7.2 VAE训练、损失和推理
- en: 'Training comprises the following steps:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 训练包括以下步骤：
- en: Choose a simple distribution family for *q*(![](../../OEBPS/Images/AR_z.png)
    | ![](../../OEBPS/Images/AR_x.png)). Gaussian is a popular choice.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为*q*(![](../../OEBPS/Images/AR_z.png) | ![](../../OEBPS/Images/AR_x.png))选择一个简单的分布族。高斯是一个流行的选择。
- en: Each input ![](../../OEBPS/Images/AR_x.png) maps to a separate distribution.
    The encoder neural network emits the parameters of this distribution. For the
    Gaussian case, the encoder emits *μ*(![](../../OEBPS/Images/AR_x.png)), Σ(![](../../OEBPS/Images/AR_x.png)).
    The latent vector ![](../../OEBPS/Images/AR_z.png) is sampled from this emitted
    distribution.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个输入![](../../OEBPS/Images/AR_x.png)映射到一个单独的分布。编码器神经网络发出该分布的参数。对于高斯情况，编码器发出*μ*(![](../../OEBPS/Images/AR_x.png))，Σ(![](../../OEBPS/Images/AR_x.png))。潜在向量![](../../OEBPS/Images/AR_z.png)是从该发出的分布中采样的。
- en: The decoder neural network takes ![](../../OEBPS/Images/AR_z.png) as input and
    emits the reconstructed input *x̃*.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器神经网络以![](../../OEBPS/Images/AR_z.png)作为输入并发出重建输入*x̃*。
- en: Given the input, reconstructed input, and latent vector we can compute the reconstruction
    loss and KL Divergence loss described below. The goal of the training process
    is to iteratively minimize these losses. Thus, the VAE is trained to minimize
    a weighted sum of the following two loss terms on each input batch-
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入、重建输入和潜在向量，我们可以计算以下描述的重建损失和KL散度损失。训练过程的目的是迭代最小化这些损失。因此，变分自编码器（VAE）被训练以在每个输入批次上最小化以下两个损失项的加权总和。
- en: '*Reconstruction Loss*—Just as in an autoencoder, in a properly trained VAE,
    the reconstruction *x̃* should be close to the original input ![](../../OEBPS/Images/AR_x.png).
    So, reconstruction loss is'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重建损失*——正如在自编码器中一样，在适当训练的VAE中，重建*x̃*应该接近原始输入![](../../OEBPS/Images/AR_x.png)。因此，重建损失是'
- en: '![](../../OEBPS/Images/eq_14-00-c.png)'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_14-00-c.png)'
- en: '*KL divergence loss*— In VAE, we also have a loss term proportional to the
    KL divergence between the distribution emitted by the encoder and the zero mean
    unit variance Gaussian. KL divergence measured the dissimilarity between two probability
    distributions and was discussed in detail in section [6.4](../Text/06.xhtml#sec-kld).
    Here we state (following equation [6.13](../Text/06.xhtml#eq-kld-cont-multivar))
    that the KL divergence loss for VAE is'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*KL散度损失*—在变分自编码器（VAE）中，我们还有一个与编码器发出的分布与零均值单位方差高斯分布之间的KL散度成比例的损失项。KL散度衡量了两个概率分布之间的差异，并在第[6.4](../Text/06.xhtml#sec-kld)节中进行了详细讨论。在此，我们声明（根据方程[6.13](../Text/06.xhtml#eq-kld-cont-multivar)），VAE的KL散度损失为'
- en: '![](../../OEBPS/Images/eq_14-00-d.png)'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_14-00-d.png)'
- en: where *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    denotes the latent-space map probability distribution and *p*(![](../../OEBPS/Images/AR_z.png))
    is a fixed target distribution. We want our global distribution of latent vectors
    to mimic the target distribution. The target is typically chosen to be a compact
    distribution so that the global latent vector distribution is also compact.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)) 表示潜在空间映射概率分布，而
    *p*(![](../../OEBPS/Images/AR_z.png)) 是一个固定的目标分布。我们希望我们的潜在向量全局分布能够模仿目标分布。目标通常被选为紧凑分布，以便全局潜在向量分布也是紧凑的。
- en: 'The popular choice for the prechosen distribution family is Gaussian and for
    the fixed distribution is the zero-mean unit covariance matrix Gaussian:'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预先选择的分布族中流行的选择是高斯分布，而对于固定分布则是零均值单位协方差矩阵的高斯分布：
- en: '![](../../OEBPS/Images/eq_14-00-e.png)'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_14-00-e.png)'
- en: It should be noted that for the above choice of prior, we can evaluate the KLD
    loss via a closed form formula as described in section 14.7.7.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应该注意的是，对于上述先验选择，我们可以通过第14.7.7节中描述的闭式公式来评估KLD损失。
- en: Minimizing ℒ*[kld]* essentially demands that *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    is high—that is, close to one—at the ![](../../OEBPS/Images/AR_z.png) values where
    *p*(![](../../OEBPS/Images/AR_z.png)) is high (see figure [14.8](#fig-vae-kld-loss)),
    because then their ratio is close to one and the logarithm is close to zero. The
    values of *p*(![](../../OEBPS/Images/AR_z.png)) at the places where *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    is low close to zero) do not matter because *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    appears as a factor in ℒ*[kld]*—the contributions to the loss by these terms are
    close to zero anyway.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最小化ℒ*[kld]*实际上要求 *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    很高——即接近一——在 *p*(![](../../OEBPS/Images/AR_z.png)) 很高的 ![](../../OEBPS/Images/AR_z.png)
    值处（参见图[14.8](#fig-vae-kld-loss)），因为那时它们的比率接近一，对数接近零。在 *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    很低（接近零）的地方的 *p*(![](../../OEBPS/Images/AR_z.png)) 的值并不重要，因为 *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    作为ℒ*[kld]*中的一个因子出现——这些项对损失的贡献无论如何都接近零。
- en: Thus, KLD loss essentially tries to ensure that most of the sample point cloud
    of *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)) falls
    on a densely populated region of the sample point cloud of *p*(![](../../OEBPS/Images/AR_z.png)).
    Geometrically, this means the cloud of little hollow circles with black borders
    has a lot of overlapping mass with the target distribution. If every training
    data point is like that, the overall global cloud of latent vectors will also
    have significant overlap with the target distribution. Since the target distribution
    is typically chosen to be compact, this in turn ensures that the overall latent
    vector distribution (dark filled circles in figure [14.7](#fig-latent_space_distributions))
    is compact. For instance, in the case when the target distribution is the zero-mean
    unit covariance matrix Gaussian 𝒩(![](../../OEBPS/Images/AR_z.png); ![](../../OEBPS/Images/AR_0.png),
    **I**), most of the mass of the latent vectors is contained within the unit radius
    ball. Without the KL divergence term, the latent vectors will spread throughout
    the latent space. In short, the KLD loss *regularizes the latent space*.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F08a_Chaudhury.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: (a) Low KL divergence loss (high *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    coincides with high *p*(![](../../OEBPS/Images/AR_z.png))
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F08b_Chaudhury.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: (b) High KL divergence loss (high *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    coincides with low *p*(![](../../OEBPS/Images/AR_z.png))
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.8 between the encoder-generated distribution (𝒩(![](../../OEBPS/Images/AR_z.png);
    ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)), **Σ**(![](../../OEBPS/Images/AR_x.png)))
    ) and the target distribution *p*(![](../../OEBPS/Images/AR_z.png)) (here *p*(![](../../OEBPS/Images/AR_z.png))
    ≡ 𝒩(![](../../OEBPS/Images/AR_z.png); ![](../../OEBPS/Images/AR_0.png), **I**))
    or, equivalently, low KL divergence between them.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: The encoder-decoder pair of neural networks is trained end to end to minimize
    the weighted sum of reconstruction loss and KLD loss. In particular, the encoder
    learns to emit the parameters of the *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    distribution.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: During inferencing, only the encoder is used. The encoder takes an input ![](../../OEBPS/Images/AR_x.png)
    and outputs ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png))
    and **Σ**(![](../../OEBPS/Images/AR_x.png)). We do not sample here. Instead, we
    use the mean directly as the latent-space representation of the input.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Notice that each input point ![](../../OEBPS/Images/AR_x.png) maps to a separate
    Gaussian distribution *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)
    = *N*(![](../../OEBPS/Images/AR_z.png); ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)),
    ∑(![](../../OEBPS/Images/AR_x.png))). The overall distribution *p*(![](../../OEBPS/Images/AR_x.png))
    modeled by all of these together can be very complex. Yet that complexity does
    not affect our computation which involves only *q*(*z*|*x*) and *p*(*z*). This
    is what makes the approach powerful.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.3 VAEs and Bayes’ theorem
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During training, the encoder neural network stochastically maps a specific
    input data instance, a point ![](../../OEBPS/Images/AR_x.png) in the input space,
    to a latent-space point ![](../../OEBPS/Images/AR_z.png) ~ 𝒩(![](../../OEBPS/Images/AR_z.png);
    ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png)), **∑**(![](../../OEBPS/Images/AR_x.png))).
    Thus the latent-space map effectively models the posterior probability *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    Note that we are using the symbol *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    to denote the actual distribution emitted by the encoder, while we are using the
    symbol *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    to denote the true (unknown) posterior probability distribution. Of course, we
    want these two to be as close as possible to each other: that is, we want the
    KL divergence between them to be minimal. Later in this section, we see how minimizing
    the KL divergence between *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    and *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)) leads
    to the entire VAE algorithm.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The decoder maps this point (![](../../OEBPS/Images/AR_z.png)) in latent space
    back to the input space point *x̃*. As such, it models the probability distribution
    *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png)).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'The global distributions of the latent vectors ![](../../OEBPS/Images/AR_z.png)
    effectively model *p*(![](../../OEBPS/Images/AR_z.png)) (shown by dark-shaded
    filled little circles in figure [14.7](#fig-latent_space_distributions)). These
    probabilities are connected by our old friend, Bayes’ theorem:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-f.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: 14.7.4 Stochastic mapping leads to latent-space smoothness
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sampling the encoder’s output from a narrow distribution is similar, but not
    identical, to deterministic mapping. It has a rather unexpected advantage over
    direct encoding. A specific input point is mapped to a slightly different point
    in the latent space every time it is encountered during training—all these points
    have to decode back to the same region in the input space. This enforces an overall
    smoothness over the latent space: nearby ![](../../OEBPS/Images/AR_z.png) values
    all correspond to nearby ![](../../OEBPS/Images/AR_x.png) values.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.5 Direct minimization of the posterior requires prohibitively expensive
    normalization
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Bayes’ theorem expression of a VAE in section [14.7.3](#sec-vae-bayes) gives
    us an idea. Why not train the neural network to directly maximize the posterior
    probability *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_Xc.png)),
    where *X* denotes the training data set? It certainly makes theoretical sense;
    we are choosing the latent space whose posterior probability is maximum given
    the training data. Of course, we must optimize one batch at a time, as we always
    do with neural networks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we evaluate the posterior probability? The formula is as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-g.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: The denominator contains a sum over all values of ![](../../OEBPS/Images/AR_z.png).
    Remember, with every iteration, the neural network weights change, and all previously
    computed latent vectors become invalid. This means we have to recompute all latent
    vectors every iteration, which is intractable. Each iteration is 𝒪(*n*), and each
    epoch then is 𝒪(*n*²), where *n* is the number of training data instances (could
    be on the order of millions). We have to look for other methods. That takes us
    to *evidence lower bound* (ELBO) types of approaches.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.6 ELBO and VAEs
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We do not know the true probability distribution *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    Let’s try to learn an approximate probability distribution *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    that is as close as possible to *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    In other words, we want to minimize the KL divergence between the two (KL divergence
    was introduced in section [6.4](../Text/06.xhtml#sec-kld)). This KL divergence
    is
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-h.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: We can expand this as
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-i.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'where *D* is the domain of ![](../../OEBPS/Images/AR_z.png): that is, the latent
    space, *H[q]* is the entropy of the probability distribution (entropy was introduced
    in section [6.2](../Text/06.xhtml#sec-entropy)), and *E[q]*(*ln*(*p*(![](../../OEBPS/Images/AR_x.png),
    ![](../../OEBPS/Images/AR_z.png)))) is the expected value of *ln*(*p*(![](../../OEBPS/Images/AR_x.png),
    ![](../../OEBPS/Images/AR_z.png))) under the probability density *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    Rearranging terms, we get'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-j.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: where the right-hand side is constant because it is a property of the data and
    cannot be adjusted during optimization. Defining the evidence lower bound (ELBO)
    as
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '*ELBO* = *H[q]* + *E[q]*(*ln*(*p*(![](../../OEBPS/Images/AR_x.png), ![](../../OEBPS/Images/AR_z.png))))'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: we get
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '*KLD*(*q*, *p*) + *ELBO* = *constant*'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: So, *minimizing* the KL divergence between *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    and its approximation *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    is equivalent to *maximizing* the ELBO. We soon see that this leads to a technique
    for optimizing variational autoencoders.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Significance of the name ELBO
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Why do we call it *evidence lower bound*? Well, the answer is hidden in the
    relation *KLD*(*q*, *p*) + *ELBO* = *ln*(*p*(![](../../OEBPS/Images/AR_x.png))).
    The right-hand side is the evidence log-likelihood. Remember, KL divergence is
    always non-negative. So, the lowest value of *ln*(*p*(![](../../OEBPS/Images/AR_x.png)))
    happens when KL divergence is zero when *ln*(*p*(![](../../OEBPS/Images/AR_x.png)))
    = *ELBO*. This means the evidence log-likelihood cannot be lower than the ELBO
    value. Thus the ELBO is the lower bound of the evidence log-likelihood; in short,
    it is the evidence lower bound.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Physical significance of the ELBO
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the physical significance of ELBO maximization:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '*ELBO* = *H[q]* + *E[q]*(*ln*(*p*(![](../../OEBPS/Images/AR_x.png), ![](../../OEBPS/Images/AR_z.png))))'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'The first term is entropy. As we saw in section [6.2](../Text/06.xhtml#sec-entropy),
    this is a measure of the diffuseness of the distribution. If the points are evenly
    spread out in the distribution—the probability density is flat with no high peak—the
    entropy is high. When the distribution has few tall peaks and low values elsewhere,
    entropy is low (remember, for a probability density, having tall peaks implies
    low values elsewhere since the total volume under the function is constant: one).
    Thus, maximizing the ELBO means we are looking for a diffuse distribution *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    This, in turn, encourages smoothness in the latent space since we are effectively
    saying an input point ![](../../OEBPS/Images/AR_x.png) can map to any point around
    the mean ![](../../OEBPS/Images/AR_micro.png)(![](../../OEBPS/Images/AR_x.png))
    (as emitted by the encoder) with almost equal high probability. Note that this
    fights a bit with the notion that each input should map to a unique point in the
    latent space. The solution tries to optimize between these conflicting requirements.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: The other term—expectation of the log of joint density *p*(![](../../OEBPS/Images/AR_x.png),
    ![](../../OEBPS/Images/AR_z.png)) under the probability density *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))—effectively
    measures the overlap between the two. Maximizing it is equivalent to saying *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    must be high where *p*(![](../../OEBPS/Images/AR_x.png), ![](../../OEBPS/Images/AR_z.png))
    is high. This seems intuitively true. The joint density *p*(![](../../OEBPS/Images/AR_x.png),
    ![](../../OEBPS/Images/AR_z.png)) = *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))*p*(![](../../OEBPS/Images/AR_z.png)).
    It is high where *both* the posterior *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    and prior *p*(![](../../OEBPS/Images/AR_z.png)) are high. If *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    approximates the posterior, it should be high where the joint is high.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue to explore the ELBO. More physical significances will emerge
    along with an algorithm for VAE optimization:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-k.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Rearranging terms and simplifying
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-00-l.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: This last expression yields more physical interpretation and leads to the VAE
    algorithm. Let’s examine the two terms in the final ELBO expression in detail.
    The first term is *E[q]*(*ln*(*p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png)))).
    This is high when *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    and *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png)) are
    both high at the same ![](../../OEBPS/Images/AR_z.png) values. For a given ![](../../OEBPS/Images/AR_x.png),
    *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)) is high
    at those ![](../../OEBPS/Images/AR_z.png) values that are likely encoder outputs
    (that is, latent representations) of input ![](../../OEBPS/Images/AR_x.png). High
    *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png)) at these
    same ![](../../OEBPS/Images/AR_z.png) locations implies a high probability of
    decoding back to the same ![](../../OEBPS/Images/AR_x.png) value from those ![](../../OEBPS/Images/AR_z.png)
    locations. Thus, this term basically says if ![](../../OEBPS/Images/AR_x.png)
    *encodes* to ![](../../OEBPS/Images/AR_z.png) with a high probability, then ![](../../OEBPS/Images/AR_z.png)
    should *decode* back to ![](../../OEBPS/Images/AR_x.png) with a high probability,
    too. Stated differently, a round trip from input to latent space back to input
    space should not take us far from the original input. In figure [14.7](#fig-latent_space_distributions),
    this means the input point marked ![](../../OEBPS/Images/AR_x.png) lies close
    to the output point marked ![](../../OEBPS/Images/AR_x3.png). In other words,
    *minimizing reconstruction loss leads to ELBO maximization*.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Now consider the second term. It comes with a minus sign. Maximizing this is
    equivalent to minimizing the KL divergence between *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    and *p*(![](../../OEBPS/Images/AR_z.png)). This is the regularizing term. Viewed
    in another way, this is the term through which we inject our belief about the
    basic organization of the latent space into the system. Remember that the KL divergence
    *KLD*(*q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)),
    *p*(![](../../OEBPS/Images/AR_z.png))) sees very little contribution from the
    small values of *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    It is dominated by the large values of *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    In terms of figure [14.7](#fig-latent_space_distributions), minimizing this KL
    divergence essentially ensures that most of the hollow circles fall on an area
    highly populated with filled circles.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Thus, overall, maximization of ELBO is equivalent to minimizing reconstruction
    loss with regularization in the form of minimizing KL divergence from a specific
    prior distribution. This is what we do in VAEs. In every iteration, we minimize
    the reconstruction loss (as in ordinary AEs) and also minimize divergence from
    a known (or guessed) prior. Note that this does not require us to encode all training
    inputs per iteration. The approach is *incremental*—one input or input batch at
    a time—like any other neural network optimization. Also, although we started from
    finding an approximation to *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)),
    the final expression does not have that anywhere. There is only the prior *p*(![](../../OEBPS/Images/AR_z.png))
    for which we can use some suitable fixed distribution.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '14.7.7 Choice of prior: Zero-mean, unit-covariance Gaussian'
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The popular choice for the known prior is a zero-mean, unit-covariance matrix
    Gaussian, 𝒩(![](../../OEBPS/Images/AR_0.png), **I**) , where I is the *d* × *d*
    identity matrix *d* is the dimensionality of the latent space), ![](../../OEBPS/Images/AR_0.png)
    is *d* × 1 vector of all zeros. Note that minimizing the KL divergence from 𝒩(![](../../OEBPS/Images/AR_0.png),
    **I**) is equivalent to restricting most of the mass within the unit ball (a hypersphere
    with its center at the origin and radius 1). In other words, this KL divergence
    term restrains the latent vectors from spreading over the ℜ*[d]* and remains mostly
    within the unit ball. Remember that a compact set of latent vectors translates
    in a sense to the simplest (minimum descriptor length) representations for the
    input vectors: that is, a regularized latent space (section [14.6](#sec-latent-space-regularizn)).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'KL divergence from a Gaussian has a closed-form expression that we derive in
    section [6.4.1](../Text/06.xhtml#sec-kld-gaussians). We first repeat equation
    [6.14](../Text/06.xhtml#eq-kld-univar-gauss) for KL divergence between two Gaussians
    and then obtain the expression for the special case where one of the Gaussians
    is a zero-mean, unit-covariance Gaussian:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-01.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: Equation 14.1
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'where the operator *tr* denotes the *trace* of a matrix (sum of diagonal elements)
    and operator *det* denotes the determinant. By assumption, *p*(![](../../OEBPS/Images/AR_z.png))
    = 𝒩(![](../../OEBPS/Images/AR_0.png), **I**): that is, *![](../../OEBPS/Images/AR_micro.png)[p]*
    = ![](../../OEBPS/Images/AR_0.png) and **Σ***[p]* = **I**. Thus,'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-01-a.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we introduce another simplifying assumption: that *the covariance
    matrix **Σ***[q]* is a diagonal matrix*. This means the matrix can be expressed
    compactly as'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '**Σ***[q]* = *![](../../OEBPS/Images/AR_sigma.png)[q]*'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: where *![](../../OEBPS/Images/AR_sigma.png)[q]* contains the elements of the
    main diagonal and we are not redundantly expressing the zeros in the off-diagonal
    elements. Note that this is not an outlandish assumption to make. We are approximating
    *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)) with a
    Gaussian *q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    whose axes are uncorrelated.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Because of this assumption,
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_14-01-b.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: It is easy to see the expression (*![](../../OEBPS/Images/AR_sigma.png)[q]*²[*i*]
    − 2*log*(*![](../../OEBPS/Images/AR_sigma.png)[q]*[*i*])) reaches a minimum when
    *![](../../OEBPS/Images/AR_sigma.png)[q]*[*i*] = 1. Thus, overall, KL divergence
    with the zero-mean, unit-covariance Gaussian is minimized when the mean is at
    the origin and the variances are all ones. This is equivalent to minimizing the
    spread of the latent vectors outside the ball of unit radius centered on the origin.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: An alternative choice for the prior is a Gaussian mixture with as many components
    as the known number of classes. We do not discuss that here.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.8 Reparameterization trick
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have avoided talking about one nasty problem so far. We said that in VAEs,
    the encoder emits the mean and variance of the probability density function *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    from which we *sample* the encoder output. There is a problem, however. The encoder-decoder
    pair are neural networks that learn via backpropagation. That is based on differentiation.
    Sampling is *not* differentiable. How do we deal with this?
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use a neat trick: the so-called *reparameterization trick*. Let’s first
    explain it in the univariate case. Sampling from a Gaussian 𝒩(*μ*, *σ*) can be
    viewed as a combination of the following two steps:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Take a random sample from *x* from 𝒩(0,1). Note that there is no learnable parameter
    here; it’s a sample from a constant density function.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Translate the sample (add *μ*), and scale it (multiply by *σ*).
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This essentially takes the sampling part out of the path for backpropagation.
    The encoder emits *μ* and *σ*, which are differentiable entities that we learn.
    Sampling is done separately from a constant density function.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The idea can be extended to a multivariate Gaussian. Sampling from 𝒩(![](../../OEBPS/Images/AR_micro.png),
    **Σ**) can be broken down into sampling from 𝒩(![](../../OEBPS/Images/AR_0.png),
    **I**) and scaling the vector by multiplying by the matrix Σ and translating by
    ![](../../OEBPS/Images/AR_micro.png). Thus, we have a multivariate encoder that
    can learn via backpropagation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for VAEs, executable via Jupyter Notebook, can be
    found at [http://mng.bz/5QYD](http://mng.bz/5QYD).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.5 PyTorch- Reparameterization trick
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Converts the log variance to the standard deviation
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: ② Samples from 𝒩(![](../../OEBPS/Images/AR_0.png), **I**)
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: ③ Scales by multiplying by Σ and translates by ![](../../OEBPS/Images/AR_micro.png)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.6 PyTorch- VAE
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Input image size in (c, h, w) format
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: ② Reduces to a (32, 16, 16)-sized tensor
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: ③ Reduces to a (128, 8, 8)-sized tensor
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: ④ Reduces to a (256, 4, 4)-sized tensor
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Flattens to a 4096-sized tensor
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Reduces a 4096-sized tensor to an nz-sized *μ* tensor
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Reduces a 4096-sized tensor to an nz-sized *log*(*σ*²) tensor
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.7 PyTorch- VAE decoder
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Converts (nz, 1, 1) to a (256, 4, 4)-sized tensor
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: ② Increases to a 128, 8, 8)-sized tensor
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: ③ Increases to a 32, 16, 16)-sized tensor
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: ④ Increases to a 1, 32, 32)-sized tensor
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.8 PyTorch- VAE loss
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Binary cross-entropy loss
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: ② *KLD*(*q*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)),
    *p*(![](../../OEBPS/Images/AR_z.png))) where ![](../../OEBPS/Images/AR_z.png)
    ~ 𝒩(![](../../OEBPS/Images/AR_0.png), **I**)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: ③ Computes the total loss
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.9 PyTorch- VAE training
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Passes the input image through the convolutional encoder
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: ② Computes *μ*, an nz-dimensional tensor
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: ③ Computes *log*(*σ*²), an nz-dimensional tensor
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: ④ Samples *z* via the reparameterization trick
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Reconstructs the image using z via the decoder
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Computes the total loss
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders vs. VAEs
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Let’s revisit the familiar MNIST digits data set. It contains a training set
    of 60,000 images and a test set of 10,000 images. Each image is 28 × 28 in size
    and contains a center crop of a single digit.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, we used this data set for classification. Here, we use it an unsupervised
    manner: we ignore the labels during training/testing. We train both the autoencoder
    and the VAE end to end on this data set and look at the results (see figures [14.9](#fig-ae-vae-recon)
    and [14.10](#fig-ae-vae-latent-space)).'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F09a_Chaudhury.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: (a) Autoencoder- reconstructed images
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F09b_Chaudhury.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: (b) VAE-reconstructed images
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.9 Comparing the reconstructed images on the test set for the autoencoder
    and VAE trained end to end. On a autoencoder and VAE do a pretty good job of reconstructing
    images from the test set.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F10a_Chaudhury.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: (a) Autoencoder latent space nz=2)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F10b_Chaudhury.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: (b) VAE latent space (nz=2)
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.10 The difference between the learned latent spaces of the autoencoder
    and VAE. We train an autoencoder and a VAE with *nz* = 2 on MNIST and plot the
    latent space for the test set. Autoencoders only minimize the reconstruction loss,
    so any latent space is equally acceptable as long as the reconstruction loss is
    low. As expected, the learned latent space is sparse and has a very high spread.
    VAEs, in contrast, minimize reconstruction loss with regularization. This is done
    by minimizing the KL divergence between the learned latent space and a known prior
    distribution 𝒩(![](../../OEBPS/Images/AR_0.png), **I**). Adding this regularization
    term ensures that the latent space is constrained within a unit ball. This can
    be seen in figure [14.10b](#fig-vae-latent-space), where the learned latent space
    is much more compact.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: The autoencoder is trained to minimize the MSE between the input image and the
    reconstructed image. There is no other restriction on the latent space.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'The VAE is trained to maximize the ELBO. As we saw in the previous section,
    we can maximize the ELBO by minimizing the reconstruction loss with regularization
    in the form of minimizing KL divergence from a specific prior distribution: 𝒩(![](../../OEBPS/Images/AR_0.png),
    **I**) in the case of VAE. So the network is incentivized to ensure that the latent
    space learned is constrained within the unit ball.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: One minor implementation detail to note is that we use binary cross-entropy
    instead of MSE when training VAEs. In practice, this leads to better convergence.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In latent-space modeling, we map input data points onto a lower-dimensional
    latent space. The latent space is typically a manifold consisting of points that
    have a property of interest in common. The property of interest can be membership
    to a specific class, such as all paragraphs written by Shakespeare. The latent
    vectors are simpler, more compact representations of the input data in which only
    information related to the property of interest is retained and other information
    is eliminated.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In latent space modeling, all training data input satisfies the property of
    interest. For instance, we can train a latent space model on paragraphs written
    by Shakespeare. Then the learned manifold contains points corresponding to various
    Shakespeare like paragraphs. Points far away from the manifold are less Shakespeare-like.
    By inspecting this distance, we can estimate the probability of a paragraph being
    written by Shakespeare. By sampling the probability distribution, we may even
    be able to emit pseudo-Shakespeare paragraphs.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometrically speaking, we project the input point onto the manifold. PCA performs
    a special form of latent space modeling where the manifold is a best-fit hyperplane
    for the training data.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Autoencoders can perform a much more powerful dimensionality reduction than
    PCA. An autoencoder consists of an encoder *E*), which maps the input data point
    into the lower-dimensional space, and a decoder (*D*), which maps the lower-dimensional
    representation back into the input space. It is trained to minimize the reconstruction
    loss: that is, the distance between the input and reconstructed (encoded, then
    decoded) vectors.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variational autoencoders (VAEs) model latent spaces as probability distributions
    to impose additional constraints (over and above reconstruction loss) so that
    we can generate more regularized latent spaces.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In VAEs, the encoder maps the input to a latent representation via a stochastic
    process (rather than a deterministic one). It emits *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png))
    as opposed to directly emitting ![](../../OEBPS/Images/AR_z.png). ![](../../OEBPS/Images/AR_z.png)
    is obtained by sampling *p*(![](../../OEBPS/Images/AR_z.png)|![](../../OEBPS/Images/AR_x.png)).
    The decoder maps a point in latent space ![](../../OEBPS/Images/AR_z.png) back
    to the input space. It is also modeled as a probability distribution *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png)).
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The latent space learned by a VAE is much more compact and smoother (and hence
    more desirable) than that learned by an autoencoder.
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
