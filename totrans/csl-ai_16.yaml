- en: 13 Causality and large language models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 因果关系与大型语言模型
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using causal information in LLMs to enhance a causal analysis
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LLMs中使用因果信息来增强因果分析
- en: Connecting the components of an LLM to a causal ideas
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将LLM的组件与因果理念相连接
- en: Building a causal LLM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建因果LLM
- en: Large language models (LLMs) represent a significant advancement in the field
    of artificial intelligence. These models are large neural networks designed to
    generate and understand human-readable text. They are “large” because their scale
    is truly impressive—cutting-edge LLMs have parameters numbering in the billions
    and trillions. As generative models, their main function is to generate coherent
    and contextually relevant natural language. They can also generate structured
    text, such as programming code, markup languages, mathematical symbols, database
    queries, and many other useful things in text form.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在人工智能领域取得了重大进步。这些模型是设计用来生成和理解人类可读文本的大型神经网络。它们之所以被称为“大型”，是因为它们的规模确实令人印象深刻——最前沿的LLMs拥有数十亿甚至数万亿的参数。作为生成模型，它们的主要功能是生成连贯且与上下文相关的自然语言。它们还可以生成结构化文本，如编程代码、标记语言、数学符号、数据库查询以及许多其他以文本形式的有用事物。
- en: LLMs are just one example of a broad class of generative AI. For example, we
    can use the neural network architecture underlying cutting-edge LLMs to model
    other sequences, such as time-series or DNA. LLMs are a type of *foundation model*,
    meaning large-scale models that serve as a base or foundation upon which more
    specialized models or applications can be built. Some LLMs are multimodal, meaning
    they work with text as well as other content modalities, such as images. In this
    chapter, we’ll focus specifically on LLMs, but much of what we discuss can be
    generalized to these related ideas.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）只是广泛类别的生成式AI的一个例子。例如，我们可以使用最前沿LLM的神经网络架构来模拟其他序列，如时间序列或DNA。LLMs是一种**基础模型**，意味着这些大规模模型作为基础或基石，在此基础上可以构建更专业的模型或应用。一些LLMs是多模态的，这意味着它们不仅与文本一起工作，还与其他内容模态一起工作，如图像。在本章中，我们将特别关注LLMs，但我们讨论的大部分内容可以推广到这些相关概念。
- en: To start, let’s explore some use cases for using LLMs to enhance a causal analysis.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们探讨一些使用LLMs来增强因果分析用例。
- en: 13.1 LLMs as a causal knowledgebase
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 LLMs作为因果知识库
- en: As I’ve mentioned several times in this book, in causal modeling, we don’t model
    the data, we model the data generating process (DGP). In other words, causal modeling
    requires causal information about the DGP that we can use as assumptions that
    enable identification of our causal queries. Cutting-edge LLMs are excellent resources
    for sourcing and validating that causal information. In this section, I’ll demonstrate
    some proven use cases for using LLMs to query causal information that augments
    a causal analysis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在本书中多次提到的，在因果建模中，我们不建模数据，我们建模数据生成过程（DGP）。换句话说，因果建模需要关于DGP的因果信息，我们可以将其作为假设，这些假设能够使我们识别因果查询。最前沿的LLMs是获取和验证这些因果信息的优秀资源。在本节中，我将展示一些使用LLMs查询因果信息以增强因果分析的已验证用例。
- en: The following results come from OpenAI’s LLM GPT-4 via the ChatGPT interface.
    Because of the random generation from the model, and OpenAI’s periodic updates
    to the model and ChatGPT software, your results will vary from mine to some degree.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果来自OpenAI的LLM GPT-4通过ChatGPT界面。由于模型的随机生成，以及OpenAI对模型和ChatGPT软件的定期更新，你的结果可能会在一定程度上与我有所不同。
- en: 13.1.1 Building a causal DAG
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 构建因果DAG
- en: 'Let’s start with using the LLM to augment the first workflow we discussed in
    this book: building the causal DAG. Specifying assumptions in the form of a causal
    DAG can be daunting. What if your DAG is wrong? Modern LLMs are invaluable as
    a way of leveraging public expert knowledge to build a DAG.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用LLM来增强本书中讨论的第一个工作流程开始：构建因果DAG。以因果DAG的形式指定假设可能会令人望而却步。如果你的DAG是错误的怎么办？现代LLMs作为利用公共专家知识构建DAG的一种方式，是无价的。
- en: '**![image](../Images/init.png)**You are a helpful causal analysis assistant.
    I’m interested in modeling the relationship between smoking and lung cancer. Provide
    me a causal DAG that captures this causal relationship.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**您是一位有用的因果分析助手。我对建模吸烟与肺癌之间的关系感兴趣。请提供一个捕捉这种因果关系的因果DAG。'
- en: '****![image](../Images/Response.png)****Certainly! A Directed Acyclic Graph
    (DAG) is a powerful tool to represent causal relationships. Here’s a basic causal
    DAG to represent the relationship between smoking and lung cancer:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '****![image](../Images/Response.png)****当然！有向无环图（DAG）是表示因果关系的有力工具。以下是一个基本的因果DAG，用于表示吸烟和肺癌之间的关系：'
- en: 'Nodes:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点：
- en: '**S**: Smoking behavior (e.g., smoker, non-smoker, or frequency of smoking)'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S**: 吸烟行为（例如，吸烟者、非吸烟者或吸烟频率）'
- en: '**LC**: Lung Cancer (presence or absence)'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LC**: 肺癌（存在或不存在）'
- en: 'Directed Edge:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有向边：
- en: A directed edge from **S** to **LC** represents that smoking behavior can cause
    or influence the occurrence of lung cancer.
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从**S**到**LC**的有向边表示吸烟行为可以导致或影响肺癌的发生。
- en: 'Potential Confounders:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能的混淆因素：
- en: 'Other nodes can be introduced into the DAG to account for potential confounders.
    Some potential confounders might be:'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以将其他节点引入DAG，以考虑潜在的混淆因素。一些可能的混淆因素可能包括：
- en: '**G**: Genetics (a family history of lung cancer)'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**G**: 遗传学（有肺癌家族史）'
- en: '**A**: Age (older age might be associated with a higher risk)'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**A**: 年龄（年龄较大可能意味着更高的风险）'
- en: '**E**: Environmental exposure (like asbestos, pollution, etc.)'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**E**: 环境暴露（如石棉、污染等）'
- en: '**O**: Other health behaviors (like diet, exercise)'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**O**: 其他健康行为（如饮食、锻炼）'
- en: 'Potential Mediators:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能的中介变量：
- en: Mediators are intermediate variables through which a cause exerts its effect
    on an outcome.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中介变量是原因通过其影响结果的中介变量。
- en: 'For instance, one could hypothesize that smoking might cause lung damage, which
    in turn causes lung cancer. In this case:'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，可以假设吸烟可能引起肺损伤，进而导致肺癌。在这种情况下：
- en: '**LD**: Lung Damage'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LD**: 肺损伤'
- en: 'The DAG might look like this with the confounders and the mediator:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DAG可能看起来是这样的，包括混淆因素和中介变量：
- en: '**S** <- **G** –> **LC**'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**S** <- **G** –> **LC**'
- en: '**S** <- **A** –> **LC**'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**S** <- **A** –> **LC**'
- en: '**S** <- **E** –> **LC**'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**S** <- **E** –> **LC**'
- en: '**S** <- **O** –> **LC**'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**S** <- **O** –> **LC**'
- en: '**S** –> **LC**'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**S** –> **LC**'
- en: '**S** –> **LD** –> **LC**'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**S** –> **LD** –> **LC**'
- en: 'In the process of modeling, it’s important to:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模过程中，重要的是：
- en: '**Identify all potential confounders** to avoid biased results. **Remember
    that correlation does not imply causation**. Just because two nodes are correlated
    does not mean one causes the other.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别所有可能的混淆因素**以避免结果偏差。**记住，相关性不等于因果关系**。仅仅因为两个节点相关并不意味着一个导致另一个。'
- en: '**Validate your DAG with domain experts**. They can provide insights into relationships
    you might not have considered.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与领域专家验证你的DAG**。他们可以提供你可能没有考虑到的见解。'
- en: '**Use data** to test and validate the causal relationships in your DAG, e.g.,
    through controlled experiments or statistical methods like instrumental variables
    or propensity score matching when experiments are not feasible.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用数据**来测试和验证DAG中的因果关系，例如，通过控制实验或当实验不可行时，使用工具变量或倾向得分匹配等统计方法。'
- en: Bear in mind that this is a simplified DAG for the relationship between smoking
    and lung cancer. Depending on the granularity and depth of your study, you might
    need to adjust and expand this framework.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请记住，这是一个简化的吸烟和肺癌之间关系的DAG。根据你研究的粒度和深度，你可能需要调整和扩展这个框架。
- en: Continuing, I can prompt the LLM to provide potential instrumental variables.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 继续下去，我可以提示LLM提供潜在的工具变量。
- en: '**![image](../Images/init.png)**Suggest two good candidates for instrumental
    variables. Be brief in your response.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**建议两个合适的工具变量的候选者。请简要回答。'
- en: '**![image](../Images/Response.png)**Certainly. For the relationship between
    smoking and lung cancer, two potential instrumental variables (IVs) could be:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**当然。对于吸烟和肺癌之间的关系，两个可能的工具变量（IVs）可以是：'
- en: '**Cigarette Tax or Price**: If certain regions or periods have higher cigarette
    prices or taxes, this can influence smoking behavior without directly influencing
    lung cancer rates except through the effect of reduced smoking. The assumption
    here is that the only way cigarette prices could affect lung cancer is by influencing
    smoking behavior.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**烟草税或价格**：如果某些地区或时期烟草价格或税收较高，这可以影响吸烟行为，而不直接影响肺癌率，除非通过减少吸烟的影响。这里的假设是，烟草价格影响肺癌的唯一方式是通过影响吸烟行为。'
- en: '**Anti-Smoking Campaigns or Legislation**: Introduction or intensity of anti-smoking
    campaigns or legislation in certain regions or times can be used as an instrument.
    If these campaigns or laws are exogenously determined (not based on current lung
    cancer rates), they can affect smoking rates without having a direct effect on
    lung cancer, except through their impact on smoking behavior.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**反吸烟运动或立法**：某些地区或时间反吸烟运动或立法的介绍或强度可以用作工具变量。如果这些运动或法律是外生决定的（不是基于当前的肺癌率），它们可以影响吸烟率，而不会对肺癌产生直接影响，除非通过其对吸烟行为的影响。'
- en: 'For both IVs, it’s crucial to validate the two key assumptions of instrument
    validity:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个工具变量，验证两个关键假设的工具有效性至关重要：
- en: The instrument is related to the exposure (e.g., cigarette tax affects smoking
    behavior).
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工具变量与暴露相关（例如，香烟税影响吸烟行为）。
- en: The instrument affects the outcome (e.g., lung cancer) only through its effect
    on the exposure and not through other pathways.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工具变量只通过其对暴露的影响影响结果（例如，肺癌），而不是通过其他途径。
- en: Prompting best practice
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示最佳实践
- en: The prompt preface “You are a helpful causal analysis assistant” is an example
    of what is sometimes called a “system prompt.” System prompts provide general
    context about what is expected from the LLM. System prompting, and other prompt
    engineering techniques (e.g., chain-of-thought prompting, in-context learning)
    are best practices for getting good generations from LLMs. For the sake of brevity,
    I’ll use system prompts but omit some of the other prompt engineering best practices
    in this section—it is smart to include them in practical settings.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 提示前缀“你是一个有用的因果分析助手”是所谓的“系统提示”的一个例子。系统提示提供了关于从LLM期望得到的内容的一般背景。系统提示和其他提示工程技术（例如，思维链提示、情境学习）是获得LLM良好输出的最佳实践。为了简洁起见，我将使用系统提示，但在此部分省略了一些其他提示工程最佳实践——在实际环境中包含它们是明智的。
- en: Continuing to interact with the LLM in this way, perhaps through multiple sessions,
    you can come up with a candidate DAG that makes sense to you and your stakeholders.
    Next, you can prompt the LLM to give you code to run an analysis.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式继续与LLM互动，可能通过多个会话，你可以提出一个对你和你的利益相关者有意义的候选DAG。接下来，你可以提示LLM为你提供运行分析的代码。
- en: 13.1.2 Generating code for DAGs, models, and causal analyses
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.2 生成DAG、模型和因果分析的代码
- en: Modern LLMs are trained on vast corpora of code from a wide variety of programming
    languages. This greatly speeds up causal analyses, particularly if the libraries
    you’re using are represented in the training data. For example, we can prompt
    ChatGPT to implement its suggested lung cancer DAG in DoWhy and estimate the causal
    effect.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现代LLM是在广泛的各种编程语言的代码语料库上训练的。这大大加快了因果分析的速度，尤其是如果你使用的库在训练数据中有代表。例如，我们可以提示ChatGPT使用DoWhy实现其建议的肺癌DAG并估计因果效应。
- en: '**![image](../Images/init.png)**Provide me with code that uses the DoWhy library
    in Python to implement the causal DAG that you suggested above. Include cigarette
    price as an instrumental variable and provide code for estimating the causal effect.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**请提供使用Python中的DoWhy库实现你上面建议的因果DAG的代码。包括香烟价格作为工具变量，并提供估计因果效应的代码。'
- en: '**![image](../Images/Response.png)**Certainly! Here’s a basic example using
    the DoWhy library in Python for implementing the causal DAG for the relationship
    between smoking and lung cancer, using cigarette price as an instrumental variable.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**当然！以下是一个使用Python中的DoWhy库实现吸烟与肺癌之间关系因果DAG的基本示例，使用香烟价格作为工具变量。'
- en: Setting up your environment
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设置你的环境
- en: The code in this chapter was written with DoWhy version 0.11, Transformers version
    4.38.2, Accelerate version 0.28.0, pandas 2.0.3, NumPy 1.25.2, and pyro-ppl (Pyro)
    version 1.9.0\. Visit the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for a link to a notebook containing the chapter’s code.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码是用DoWhy版本0.11、Transformers版本4.38.2、Accelerate版本0.28.0、pandas 2.0.3、NumPy
    1.25.2和pyro-ppl (Pyro)版本1.9.0编写的。访问章节注释[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)以获取包含本章代码的笔记本链接。
- en: Chat GTP then went on to generate the following code.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Chat GTP接着生成了以下代码。
- en: Listing 13.1 DoWhy code generated by ChatGPT (verbatim)
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表13.1 ChatGPT生成的DoWhy代码（原文）
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 This line is wrong, but the code runs if you remove it.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 这一行是错误的，但如果你删除它，代码就可以运行。'
- en: '#2 Simulate smoker status: 0 for non-smoker, 1 for smoker.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 模拟吸烟状态：0表示非吸烟者，1表示吸烟者。'
- en: '#3 Simulate lung cancer status: 0 for no lung cancer, 1 for lung cancer.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 模拟肺癌状态：0表示无肺癌，1表示有肺癌。'
- en: '#4 Simulate cigarette price with a mean price of 5.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 以5美元的平均价格模拟香烟价格。'
- en: '**![image](../Images/Response.png)**This is a simplified example. In a real-world
    application:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**这是一个简化的例子。在实际应用中：'
- en: Your data would be imported rather than simulated as above.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的数据将导入而不是像上面那样模拟。
- en: Ensure that your DAG correctly models the relationships based on domain knowledge.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你的DAG正确地根据领域知识建模关系。
- en: Assumptions behind the instrumental variables need to be rigorously checked.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工具变量背后的假设需要严格检查。
- en: Further analytical steps (e.g., statistical tests, checking robustness) would
    be necessary.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步的分析步骤（例如，统计测试，检查稳健性）将是必要的。
- en: When you generate code, you may get errors. For example, DoWhy has gone through
    several iterations, and there is nothing forcing the LLM to keep up with the latest
    version of the library. In this case, there is an error in the line `from dowhy.do_why
    import CausalModel`. There is no module called `do_why`, and `CausalModel` is
    already imported in a previous line. We’ll address errors produced by the LLM
    in section 13.1.7\. For now, if you delete that line, the code will run.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当你生成代码时，可能会出现错误。例如，DoWhy已经经过几次迭代，没有任何东西迫使LLM跟上库的最新版本。在这种情况下，`from dowhy.do_why
    import CausalModel`这一行有错误。没有名为`do_why`的模块，并且`CausalModel`已经在之前的行中导入。我们将在第13.1.7节中解决LLM产生的错误。现在，如果你删除那一行，代码就可以运行。
- en: ChatGPT will work well with pgmpy and Pyro, the libraries we’ve worked with
    extensively in the book. As an exercise, I suggest taking one of the models you
    built in pgmpy, and prompting the LLM to convert it to a model in Pyro.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT将与我们在书中广泛使用的pgmpy和Pyro库很好地工作。作为一个练习，我建议你将你在pgmpy中构建的一个模型提示LLM将其转换为Pyro中的模型。
- en: Next, we’ll look at what the LLM can tell us beyond the structure of the DAG.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看LLM能告诉我们关于DAG结构之外的内容。
- en: 13.1.3 Explanations and mechanism
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.3 解释和机制
- en: The advantage of DAGs is that graphs are an intuitive way to represent what
    causes what. But they do not tell us anything about the nature of the mechanism
    by which a cause drives an effect.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: DAG的优势在于，图是一种直观的方式来表示什么导致什么。但它们并没有告诉我们关于导致效应的机制的性质。
- en: Structural causal models provide a mathematical way of characterizing the mechanism
    underlying a causal relationship. Characterizing mechanisms in mathematical terms
    makes sense in natural sciences, where we have plenty of equations describing
    natural laws. It is harder in, for example, business domains where we lack equations
    beyond mere approximations (what is the equation relating ad spending and engagement?).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 结构性因果模型提供了一种数学方法来描述因果关系的潜在机制。在自然科学中，我们有大量的方程描述自然法则，用数学术语描述机制是有意义的。例如，在商业领域，我们缺乏方程，除了近似（广告支出和参与度之间有什么方程？）。
- en: LLMs leverage common and expert knowledge to provide mechanistic explanations
    that go beyond the DAG but that are in a form more accessible than formal math.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LLM利用常见和专业知识提供超越DAG的机制解释，但形式比正式数学更易于理解。
- en: '**![image](../Images/init.png)**You are a helpful causal explanation generator.
    Provide a characterization of the causal relationship between ad spending and
    brand awareness. Be concise enough to fit a single slide in a slide deck.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**你是一个有用的因果解释生成器。提供广告支出与品牌知名度之间因果关系的描述。要简洁到足以适应幻灯片中的单张幻灯片。'
- en: '**![image](../Images/Response.png)**Causal Relationship Between Ad Spending
    and Brand Awareness'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**广告支出与品牌知名度之间的因果关系'
- en: '**Direct Causality**: As ad spending increases, brand awareness typically rises.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接因果关系**：随着广告支出的增加，品牌知名度通常会上升。'
- en: '**Mechanism**: Larger ad spends mean more frequent and widespread exposure
    of the brand to the target audience.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**机制**：更大的广告支出意味着品牌对目标受众的曝光更加频繁和广泛。'
- en: '**Diminishing Returns**: After a certain point, additional ad spending might
    not lead to proportionally higher brand awareness.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收益递减**：在某个点上，额外的广告支出可能不会导致品牌知名度的成比例增加。'
- en: '**Reason**: Market saturation, ad fatigue, or redundancy in audience reach.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原因**：市场饱和、广告疲劳或受众覆盖的冗余。'
- en: '**External Factors**: Ad efficiency isn’t solely based on spend.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部因素**：广告效率并不仅仅基于支出。'
- en: '**Influences**: Ad quality, placement, target audience relevancy, competition,
    and market conditions can affect the outcome.'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**影响因素**：广告质量、位置、目标受众的相关性、竞争和市场条件都可能影响结果。'
- en: 'Consider that some counterfactual queries can be identified, with simple categorical
    assumptions about the functional relationship between the cause and effect. For
    example, the probability of necessity is identifiable from intervention data and
    partially identifiable from observational data if the relationship between *X*
    and *Y* is monotonic:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到可以识别出一些反事实查询，它们对原因和效果之间功能关系的简单分类假设。例如，从干预数据中可以识别出必要性的概率，如果X和Y之间的关系是单调的，那么从观察数据中部分可识别：
- en: '**![image](../Images/init.png)**Is this a monotonic relationship? Limit your
    answer to one sentence.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**这是单调关系吗？请将您的答案限制在一句话内。'
- en: '**![image](../Images/Response.png)**Yes, the relationship between ad spending
    and brand awareness is generally monotonic, as increased ad spending will not
    decrease brand awareness, though the rate of increase might diminish.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**是的，广告支出与品牌知名度之间的关系通常是单调的，因为增加广告支出不会降低品牌知名度，尽管增加的速度可能会减缓。'
- en: We needn’t limit ourselves to one explanation; we can use the LLM as a generator
    of explanatory hypotheses. Consider, for example*, abductive reasoning tasks*
    where one infers likely causes of a particular observed set of effects (e.g.,
    in science, criminal investigation, and medical diagnosis).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必局限于一个解释；我们可以将LLM用作解释假设的生成器。例如，考虑*推论推理任务*，在这些任务中，一个人推断出特定观察到的效应集的可能原因（例如，在科学、刑事调查和医学诊断中）。
- en: 'To illustrate, let’s pose a modified version of a medical diagnostic question
    sourced from the US Medical License Exam to ChatGPT. I’ll prompt for multiple
    hypotheses about the cause of the observed symptoms, which is what a doctor would
    call a *differential diagnosis*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，让我们提出一个源自美国医学执照考试的医疗诊断问题的修改版，用于ChatGPT。我将要求提出关于观察到的症状原因的多个假设，这就是医生所说的*鉴别诊断*：
- en: '**![image](../Images/init.png)**You are a causal explanation assistant. Your
    job is to generate 3 one sentence-long hypotheses that explain the observations.
    This is a medical diagnosis example.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**您是一位因果解释助手。您的任务是生成3个一句话长的假设，用以解释观察到的现象。这是一个医学诊断示例。'
- en: 'Observations: A 48-year-old woman presents with diplopia, ocular pain, amenorrhea,
    hot flashes, weight loss, tachycardia, hypertension, moist palms, enlarged thyroid,
    prominent eyes, lid retraction, conjunctival injection, and inability to converge
    eyes. Visual acuity is 20/20 bilaterally.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**观察结果**：一位48岁的女性出现复视、眼部疼痛、闭经、潮热、体重减轻、心动过速、高血压、湿润的手掌、甲状腺肿大、突出眼睛、眼睑退缩、结膜充血和无法聚视。视力为双眼20/20。'
- en: '**![image](../Images/Response.png)** The patient may have Graves’ disease,
    an autoimmune disorder affecting the thyroid, leading to hyperthyroidism and its
    associated symptoms, including ocular manifestations.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**患者可能患有格雷夫斯病，这是一种影响甲状腺的自身免疫性疾病，导致甲状腺功能亢进及其相关症状，包括眼部表现。'
- en: 2\. The woman’s symptoms, including amenorrhea and hot flashes, suggest a possible
    concomitant early menopause or another endocrine disorder affecting her reproductive
    hormones.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 女性的症状，包括闭经和潮热，表明可能存在早期更年期或影响其生殖激素的另一种内分泌疾病。
- en: 3\. The ocular symptoms, including diplopia, lid retraction, and inability to
    converge eyes, hint at a potential thyroid eye disease or orbital myopathy.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 眼部症状，包括复视、眼睑退缩和无法聚视，暗示可能存在甲状腺眼病或眶周肌病。
- en: One of the challenges of causal modeling is working with domain experts to encode
    domain assumptions into the structure of the model. These types of generated explanations
    and hypotheses are valuable assets for communicating with domain experts who might
    agree with, or better yet, iterate upon these generated explanations.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因果建模的一个挑战是与领域专家合作，将领域假设编码到模型的框架中。这类生成的解释和假设是宝贵的资产，可用于与可能同意或更好地迭代这些生成解释的领域专家沟通。
- en: 13.1.4 The causal frame problem and AI alignment
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.4 因果框架问题与AI对齐
- en: 'Given a query of interest, we have to decide what variables are relevant to
    the problem. Philosoper and AI researcher Clark Glymour does a good job of describing
    the challenge behind this task: “Given an enormous amount of stuff, and some task
    to be done using some of the stuff, what is the relevant stuff for the task?”[¹](#footnote-305)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个感兴趣的查询，我们必须决定哪些变量与问题相关。哲学家和人工智能研究员 Clark Glymour 很好地描述了这项任务背后的挑战：“给定大量的东西，以及使用其中一些东西来完成某个任务，对于这个任务来说，什么是相关的？”[¹](#footnote-305)
- en: LLMs are useful at establishing a *causal frame*—the problem of selecting the
    causal variables relevant to a causal query. In chapter 3’s discussion on building
    the DAG, I mentioned *causal sufficiency* as a d-separation–based criteria for
    selecting variables for inclusion in the DAG. But when we prompted the LLM to
    provide a DAG containing variables relevant to smoking’s effect on lung cancer,
    it is highly doubtful that the LLM was using causal sufficiency to select those
    variables. More likely, it was using common knowledge found in its training data
    about this relationship. That knowledge was sufficient to provide a reasonably
    minimal set of relevant variables.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在建立**因果关系框架**方面很有用——选择与因果查询相关的因果变量的问题。在第3章关于构建 DAG 的讨论中，我提到了**因果充分性**作为基于
    d-separation 的标准，用于选择包含在 DAG 中的变量。但当我们提示 LLM 提供一个包含与吸烟对肺癌影响相关的变量的 DAG 时，我们高度怀疑
    LLM 是否在使用因果充分性来选择这些变量。更有可能的是，它正在使用其训练数据中关于这种关系的常识。这种知识足以提供一组合理的最小相关变量集。
- en: 'To illustrate the utility of this ability, consider the following ChatGPT interaction:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种能力的实用性，考虑以下 ChatGPT 交互：
- en: '**![image](../Images/init.png)**You are a helpful causal reasoning assistant.
    Consider the following story.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**您是一个有用的因果推理助手。考虑以下故事。'
- en: Jake is on an office network. There was an open shared folder on the server.
    Jake is transferring files across the network and accidentally clicks on the folder,
    starting a process that consumes a significant amount of bandwidth. This causes
    the network to slow down. How responsible is Jake for the network slowdown?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Jake 在一个办公室网络中。服务器上有一个开放的共享文件夹。Jake 正在通过网络传输文件，并意外点击了文件夹，启动了一个消耗大量带宽的过程。这导致网络变慢。Jake
    对网络减速有多大的责任？
- en: '**![image](../Images/Response.png)**Based on the provided story, let’s analyze
    the situation using causal reasoning:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**基于提供的故事，让我们使用因果推理来分析这种情况：'
- en: '**Direct Cause**: Jake started a process that consumes a significant amount
    of bandwidth by accidentally clicking on a file.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**直接原因**：Jake 通过意外点击一个文件启动了一个消耗大量带宽的过程。'
- en: '**Effect**: The network slowed down due to the consumed bandwidth.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**影响**：由于消耗了带宽，网络变慢了。'
- en: Given these facts, we can establish that Jake’s action was the direct cause
    of the network slowdown.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些事实，我们可以确定 Jake 的行为是网络减速的直接原因。
- en: 'However, responsibility can be considered from different dimensions:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可以从不同的维度来考虑责任：
- en: '**Intent**: Jake’s action was accidental. He did not intend to slow down the
    network.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**意图**：Jake 的行为是意外的。他并没有意图减慢网络。'
- en: '**Prevention Mechanisms**: The story does not specify if there were any warnings
    or user prompts before starting such a process that consumes significant bandwidth.
    If there were no warnings, one could argue that the system was not designed to
    prevent such accidental actions.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预防机制**：故事没有说明在启动这种消耗大量带宽的过程之前是否有任何警告或用户提示。如果没有警告，有人可能会认为系统没有设计来防止这种意外行为。'
- en: '**Knowledge & Training**: Was Jake trained or informed about the implications
    of transferring large files or about the particular file that consumes a lot of
    bandwidth? If not, his knowledge gap contributed to the accident.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**知识与培训**：Jake 是否接受了关于传输大文件或关于消耗大量带宽的特定文件的培训或通知？如果没有，他的知识差距导致了这次事故。'
- en: '**System Design**: The open shared folder on the server was accessible to Jake.
    If the file was known to be a bandwidth hog, it’s worth asking why such a file
    was placed in an open shared folder without any warnings or restrictions.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**系统设计**：服务器上的开放共享文件夹对 Jake 可访问。如果文件已知是带宽消耗者，那么值得问的是为什么这样的文件会被放在一个没有任何警告或限制的开放共享文件夹中。'
- en: 'To directly answer the question: Jake is responsible for the network slowdown
    in the sense that his action was the direct cause. However, the degree of his
    responsibility might be mitigated if there were systemic design flaws, lack of
    training, or absence of preventive measures. If other factors, like the ones mentioned
    above, played a role in the incident, then the responsibility would be shared
    among Jake and those other factors.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 直接回答问题：杰克对网络速度减慢负有责任，因为他的行为是直接原因。然而，如果存在系统设计缺陷、缺乏培训或缺乏预防措施，他的责任程度可能会减轻。如果上述提到的其他因素在事件中发挥了作用，那么责任将在杰克和其他因素之间分担。
- en: Here, the response includes the systemic design flaws, lack of training, or
    absence of preventive measures as factors relevant to the causal frame, despite
    the fact that I made no mention of these factors in the prompt.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，响应包括系统设计缺陷、缺乏培训或缺乏预防措施作为与因果关系框架相关的因素，尽管我在提示中并未提及这些因素。
- en: 'More importantly, consider the many factors the LLM could have included but
    did not: perhaps Jake’s mouse had a physical glitch that caused him to click on
    the file, or perhaps a hacker removed security features on that file, or any other
    possible mitigating factors. A human would likely exclude these from the causal
    frame unless explicitly mentioned, and so does the LLM.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，考虑LLM可能包括但未包括的许多因素：或许杰克的鼠标出现了物理故障，导致他点击了文件，或许黑客移除了该文件的安全功能，或者任何其他可能的减轻因素。人类可能会排除这些因素，除非明确提及，LLM也是如此。
- en: 'If humans are already good at establishing the causal frame, why would we care
    whether the LLM can do so? Consider the problem of root cause analysis from network
    logs. Suppose the same story about Jake were rewritten in the form of network
    logs, such as in the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果人类已经擅长建立因果关系框架，我们为什么还要关心LLM能否做到这一点呢？考虑从网络日志中进行根本原因分析的问题。假设关于杰克的相同故事被改写成网络日志的形式，如下所示：
- en: '[PRE1]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is harder for a human to parse than the natural language story, even for
    an experienced network administrator. Further, there would likely be many other
    network events unrelated to the events of this narrative that the human would
    have to read through and disregard. But the LLM has plenty of network logs in
    its training data, and extracting relevant events from a longer body of text is
    an established capability of LLMs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于人类来说比自然语言故事更难解析，即使是经验丰富的网络管理员也是如此。此外，人类可能还需要阅读并忽略与这个叙述无关的许多其他网络事件。但LLM在其训练数据中拥有大量的网络日志，从较长的文本中提取相关事件是LLM的一项既定能力。
- en: In most of this book, we’ve focused on using causal models to infer objective
    causal truth about the world, such as whether smoking causes lung cancer. In this
    example, the objective truth, that Jake caused the network slowdown, is trivial.
    The question of how responsible Jake is for the slowdown is highly subjective.
    But this is still a valuable question to answer, especially if you are trying
    to find a root cause in a large network log file.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分内容中，我们专注于使用因果关系模型来推断关于世界的客观因果关系，例如吸烟是否导致肺癌。在这个例子中，客观事实，即杰克导致了网络速度减慢，是微不足道的。关于杰克对速度减慢的责任程度是一个高度主观的问题。但回答这个问题仍然很有价值，尤其是如果你试图在一个大型网络日志文件中找到根本原因。
- en: '*AI alignment* is the task of making sure an AI system’s behaviors are consistent
    with human goals and preferences. Here, we want a specific type of AI alignment.
    We want the LLM to go beyond confirming obvious causal facts to aligning with
    the ways humans tend to make causal judgments. In this case, I asked the LLM an
    open question about Jake’s responsibility, and it gave a response that many humans
    would agree with. I could also have instructed the LLM to follow certain criteria
    when generating a judgement. For example, an attorney could prompt the LLM to
    apply certain legal theories and case law in generating a legal opinion.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能对齐*是确保人工智能系统行为与人类目标和偏好一致的任务。在这里，我们希望实现一种特定类型的人工智能对齐。我们希望LLM不仅确认明显的因果关系事实，还与人类倾向于做出的因果关系判断相一致。在这种情况下，我向LLM提出了关于杰克责任的一个开放式问题，它给出了许多人类都会同意的回应。我也可以指示LLM在生成判断时遵循某些标准。例如，律师可以提示LLM在生成法律意见时应用某些法律理论和案例法。'
- en: Next, we’ll look at how to use the LLM to understand and contextualize more
    nuanced causal concepts.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何使用LLM来理解和情境化更细微的因果关系概念。
- en: 13.1.5 Understanding and contextualizing causal concepts
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.5 理解和情境化因果关系概念
- en: It can be challenging to understand formal causal concepts and apply them within
    your domain. The LLM can serve as a tool for explaining and contextualizing these
    concepts. Let’s illustrate with the counterfactual concept of *necessary causation*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 理解正式的因果概念并在你的领域内应用它们可能会具有挑战性。LLM可以作为解释和情境化这些概念的工具。让我们以*必要因果关系*的反事实概念为例进行说明。
- en: '**![image](../Images/init.png)**You are a helpful explainer of causal inference
    topics. Provide a one paragraph explanation of the concept of necessary causation.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**你是一位有益的因果推理主题解释者。请提供一段关于必要因果关系的概念解释。'
- en: '**![image](../Images/Response.png)**Certainly! Necessary causation refers to
    a causal relationship where the cause is a necessary condition for the effect
    to occur. In other words, without the presence of the cause, the effect cannot
    happen. However, the presence of the cause does not guarantee the effect will
    occur, as there may be other conditions that also need to be met. Think of it
    like a key to a lock: the key (cause) is necessary to open the lock (effect),
    but just having the key doesn’t mean the lock will open automatically; you still
    need to insert and turn it. If the lock remains locked without the key, then the
    key is a necessary cause for unlocking.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**当然！必要因果关系指的是一种因果关系，其中原因的存在是结果发生的必要条件。换句话说，如果没有原因的存在，结果就不会发生。然而，原因的存在并不保证结果会发生，因为可能还有其他条件需要满足。把它想象成一把钥匙开一把锁：钥匙（原因）是打开锁（结果）的必要条件，但仅仅拥有钥匙并不意味着锁会自动打开；你仍然需要插入并转动它。如果锁没有钥匙仍然锁着，那么钥匙就是解锁的必要原因。'
- en: A cutting-edge LLM would be trained on a wide variety of theory and research
    from across domains. It can be useful to contextualize an idea within various
    domains.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个前沿的LLM会在跨领域的各种理论和研究中进行训练。将一个想法置于各种领域中进行情境化可能是有用的。
- en: '**![image](../Images/init.png)**In one paragraph, explain how necessary causation
    is conceptualized in legal theory.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**在一句话中，解释在法律理论中如何概念化必要因果关系。'
- en: '**![image](../Images/Response.png)**In legal theory, necessary causation, often
    termed “but-for causation,” is a foundational concept used to establish a direct
    link between a defendant’s actions and the harm suffered by a plaintiff. For an
    action to be a but-for cause of an outcome, the outcome would not have occurred
    in the absence of the action. In other words, if one can say “but for the defendant’s
    action, the harm would not have occurred,” then necessary causation is established.
    However, it’s worth noting that while but-for causation is a critical component
    in many legal contexts, it’s not always sufficient on its own to establish liability.
    Other factors, such as foreseeability and proximate cause, often come into play
    to determine the full scope of legal responsibility.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**在法律理论中，必要因果关系，通常被称为“若无因果关系”，是一个用于建立被告行为与原告所受损害之间直接联系的基础性概念。一个行为要成为导致结果的“若无”原因，如果没有这个行为，结果就不会发生。换句话说，如果可以说“若非被告的行为，损害就不会发生”，那么必要因果关系就成立了。然而，值得注意的是，尽管“若无”因果关系在许多法律情境中是一个关键组成部分，但它本身并不总是足以确立责任。其他因素，如可预见性和近因，通常也会发挥作用，以确定法律责任的全部范围。'
- en: '**![image](../Images/init.png)**From the perspective of cognitive science,
    explain in one brief paragraph the role necessary causation plays in human judgments.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**从认知科学的角度来看，用一句话简要说明必要因果关系在人类判断中的作用。'
- en: '**![image](../Images/Response.png)**From a cognitive science perspective, necessary
    causation plays a pivotal role in human judgments and decision-making. Our brains
    are wired to seek patterns and causal relationships in the environment to predict
    and navigate future events. When we perceive an event as a necessary cause of
    another, it simplifies our cognitive processing by creating a deterministic link
    between the two events. This means that when we encounter the cause, we can anticipate
    the effect, allowing for more efficient planning and response. However, this predisposition
    can also lead to cognitive biases, where we might mistakenly attribute an effect
    to a perceived necessary cause, even when other contributing factors are at play.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**从认知科学的角度来看，必然因果关系在人类的判断和决策中起着至关重要的作用。我们的大脑被设计成在环境中寻找模式和因果关系，以预测和导航未来的事件。当我们把一个事件视为另一个事件的必然原因时，它通过在两个事件之间建立决定性的联系，简化了我们的认知处理。这意味着当我们遇到原因时，我们可以预测结果，从而允许更有效的规划和反应。然而，这种倾向也可能导致认知偏差，我们可能会错误地将一个效果归因于一个感知到的必然原因，即使其他影响因素也在发挥作用。'
- en: '**![image](../Images/init.png)**From the perspective of philosophy, explain
    in only one sentence how Kant and Hume differed in their beliefs about necessary
    causation.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**从哲学的角度来看，用一句话解释康德和休谟在关于必然因果关系的信仰上的不同。'
- en: '**![image](../Images/Response.png)**Kant believed in the existence of a priori
    synthetic knowledge and that our minds impose causal order on our experiences,
    while Hume argued that causation is not a feature of the world itself but a habit
    of thought based on the regular succession of events.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**康德相信先验综合知识的存在，并认为我们的心灵在我们的经验上强加因果秩序，而休谟则认为因果关系不是世界本身的特征，而是基于事件规律性连续性的思维习惯。'
- en: 'Now that we’ve unpacked the concept of necessary causation, let’s apply it
    to a practical problem in our domain of interest. Suppose you are a data scientist
    at Netflix:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解构了必然因果关系的概念，让我们将其应用到我们感兴趣领域的一个实际问题中。假设你是Netflix的一名数据科学家：
- en: '**![image](../Images/init.png)**Give me an example of necessary causation that
    involves a Netflix user unsubscribing from Netflix.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**给我一个涉及Netflix用户取消订阅的必然因果关系的例子。'
- en: '**![image](../Images/Response.png)**A necessary causation for a Netflix user
    unsubscribing from Netflix might be the exclusive content that is only available
    on the platform. If a particular show or movie that the user is deeply interested
    in is removed from Netflix while remaining available somewhere else, then the
    removal of that content becomes a necessary cause for that user’s decision to
    unsubscribe. Without the removal of that content, the user might not have unsubscribed.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**一个Netflix用户取消订阅的可能必然原因是该平台上仅有的独家内容。如果用户深感兴趣的一部特定剧集或电影从Netflix上被移除，而该内容在其他地方仍然可用，那么该内容的移除就成为了该用户决定取消订阅的必然原因。如果没有该内容的移除，用户可能不会取消订阅。'
- en: Now that the LLM has helped us understand necessary causation, we can get its
    help in formalizing the concept.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在LLM已经帮助我们理解了必然因果关系，我们可以请它帮助我们形式化这个概念。
- en: 13.1.6 Formalization of causal queries
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.6 因果查询的形式化
- en: For humans, natural language is the medium of our causal reasoning. But we need
    to convert that natural language into formal terms so that we can apply mathematical
    and algorithmic analysis. This can be a challenge. Fortunately, LLMs can help
    greatly in this respect.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类来说，自然语言是我们因果推理的媒介。但我们需要将这种自然语言转换为形式术语，以便我们可以应用数学和算法分析。这可能是一个挑战。幸运的是，LLM在这方面可以提供极大的帮助。
- en: 'For example, in chapter 8, I introduced several examples of the language of
    parallel world counterfactuals and how to convert them into counterfactual notation.
    Necessary causation, more specifically the *probability of necessity*,was one
    of these counterfactuals. Let’s put this task to the LLM in our Netflix context:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在第8章中，我介绍了几个平行世界反事实语言的例子以及如何将它们转换为反事实符号。必然因果关系，更具体地说，是*必然性的概率*，是这些反事实之一。让我们将这个任务放在我们的Netflix环境中交给LLM：
- en: '**![image](../Images/init.png)**You are a helpful causal inference assistant.
    I’m considering a counterfactual inference problem in the context of Netflix.
    I have the following query: “There is a set of users who unsubscribed after content
    was removed from the platform. What is the probability a given member of this
    set of users would not have unsubscribed had the content not been removed?” Write
    this query as a probabilistic expression with Pearl’s counterfactual notation.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**您是一位有用的因果推断助手。我在 Netflix 的背景下考虑一个反事实推断问题。我有一个以下查询：“有一组用户在平台上的内容被删除后取消了订阅。给定这个用户组中的某个成员，如果内容未被删除，他们不会取消订阅的概率是多少？”请使用
    Pearl 的反事实符号将此查询写成概率表达式。'
- en: '**![image](../Images/Response.png)**Certainly! Pearl’s counterfactual notation
    offers a concise way to express such queries.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**当然！Pearl 的反事实符号提供了一种简洁的方式来表达此类查询。'
- en: 'Let’s define some variables:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一些变量：
- en: '*X* =removal of the content (binary, 1 if removed, 0 if not removed).'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*X* = 内容的删除（二元变量，删除为 1，未删除为 0）。'
- en: '*Y* =whether user unsubscribes (binary, 1 if user unsubscribes, 0 if they did
    not unsubscribe).'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Y* = 用户是否取消订阅（二元变量，用户取消订阅为 1，未取消订阅为 0）。'
- en: Your query is essentially asking for the probability that a user would not have
    subscribed, had the content not been removed, given that it was removed and they
    did unsubscribe.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您的查询实际上是在询问，在内容被删除且用户确实取消订阅的情况下，如果内容未被删除，用户不会订阅的概率。
- en: 'Using Pearl’s counterfactual notation, this can be written as:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Pearl 的反事实符号，这可以写成：
- en: '![figure](../Images/ness-ch13-eqs-0x.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch13-eqs-0x.png)'
- en: 'Where:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*Y*[*x*][=0] is the counterfactual outcome for *Y* had *X* been set to 0 (i.e.,
    the content is not removed).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Y*[*x*][=0] 是在 *X* 被设置为 0（即内容未被删除）时 *Y* 的反事实结果。'
- en: '*X*=1 denotes that the content was removed.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X*=1 表示内容已被删除。'
- en: '*Y*=1 denotes that the user unsubscribed.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Y*=1 表示用户已取消订阅。'
- en: So, *P*(*Y**[x=]*[0] = 0| *X* = 1, *Y* = 1) is the probability that a user would
    not have unsubscribed had the content not been removed, given that it was removed
    and they did unsubscribe.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*P*(*Y**[x=]*[0] = 0| *X* = 1, *Y* = 1) 是在内容被删除且用户确实取消订阅的情况下，如果内容未被删除，用户不会取消订阅的概率。
- en: The LLM can take us from conceptualization to formalization. But beware, as
    it can also “hallucinate.”
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型可以将我们从概念化带到形式化。但请注意，它也可能“产生幻觉”。
- en: '13.1.7 Beware: LLMs hallucinate'
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.7 小心：LLM 可能产生幻觉
- en: We’ve seen that LLMs can aid in code generation. Let’s ask for code that implements
    the probability of necessity.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，大型语言模型（LLM）可以帮助生成代码。让我们请求实现必要性概率的代码。
- en: '**![image](../Images/init.png)**Provide Python code that, for a given pair
    of treatment and outcome variables, calculates the counterfactual query “probability
    of necessity.”'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**提供 Python 代码，对于给定的处理变量和结果变量对，计算反事实查询“必要性概率”。'
- en: '**![image](../Images/Response.png)**The Probability of Necessity (PN) is the
    proportion of cases where the treatment is present, and the outcome is achieved
    ONLY when the treatment is present. In simpler terms, it indicates the probability
    that the outcome occurs because of the treatment.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**必要性概率（PN）是指在处理存在的情况下，并且只有在处理存在时才实现结果的情况的比例。简单来说，它表示结果发生是因为处理。'
- en: 'Given that definition, you can calculate PN using the following formula:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个定义，你可以使用以下公式计算 PN：
- en: '![figure](../Images/ness-ch13-eqs-1x.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch13-eqs-1x.png)'
- en: 'Where:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*P*(*Treatment*∩*Outcome*) is the probability of both the treatment and outcome
    occurring.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*Treatment*∩*Outcome*) 是处理和结果同时发生的概率。'
- en: '*P*(¬*Treatment*∩*Outcome*) is the probability of the outcome occurring but
    not the treatment.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(¬*Treatment*∩*Outcome*) 是结果发生但处理未发生的概率。'
- en: The response goes on to provide Python code that is well formatted, well commented,
    and compiles. *But this answer is wrong*. This is what is commonly called a “hallucination,”
    a convincing lie. This example is especially convincing—that formula is true *only*
    when the relationship between the treatment and outcome is monotonic, when there
    are no confounders, and when *P*(*Treatment*) = *P*(*Outcome*). Those assumptions
    are so strong that we can’t forgive ChatGPT for not mentioning them. Other generations
    from the model were even more off base.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 响应继续提供格式良好、注释清晰且可编译的Python代码。*但这个答案是错误的*。这通常被称为“幻觉”，一个令人信服的谎言。这个例子尤其令人信服——该公式仅在治疗与结果之间的关系是单调的、没有混杂因素，以及*P*(*治疗*)
    = *P*(*结果*)时才成立。这些假设如此强烈，以至于我们无法原谅ChatGPT没有提及它们。模型的其他生成结果甚至更加偏离正确方向。
- en: We can use the LLM to augment our causal analysis, particularly with common
    and expert causal knowledge as well as generated code. But hallucination means
    this isn’t a panacea. We need to rely on our own causal expertise to spot when
    hallucination occurs and understand when it threatens the quality of our analysis.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用LLM来增强我们的因果分析，特别是利用常见和专家因果知识以及生成的代码。但幻觉意味着这并不是万能的。我们需要依赖我们自己的因果专业知识来识别何时发生幻觉，并理解何时它威胁到我们分析的质量。
- en: To understand why this hallucination occurred, let’s first examine how LLMs
    work.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这种幻觉发生的原因，让我们首先考察LLMs是如何工作的。
- en: 13.2 A causality-themed LLM primer
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 因果主题的LLM入门
- en: To understand how to deploy LLMs for causal applications, it is important to
    understand how they work, as well as their limitations. This section provides
    a quick high-level causality-themed tour of the core ideas.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何部署LLMs进行因果应用，了解它们的工作原理以及它们的局限性非常重要。本节提供了一个关于核心思想的因果主题快速高级概述。
- en: 13.2.1 A probabilistic ML view of LLMs
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 LLMs的概率机器学习视角
- en: In the context of LLMs, a “token” refers to a sequence of characters that the
    model reads, which can be as short as one character or as long as one word. Tokens
    are the units into which input text is divided into manageable pieces for the
    model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs（大型语言模型）的背景下，“token”指的是模型读取的字符序列，它可以短至一个字符，也可以长至一个单词。Token是输入文本被分割成模型可以管理的片段的单位。
- en: Hugging Face’s Transformers library has a publicly available version of GPT-2,
    which is far inferior to cutting-edge models but has a similar *Transformer* architecture.
    The Transformer architecture is a type of deep learning model designed to process
    and understand text and other sequential data, by focusing on the relationships
    between words in a sentence regardless of their position. Let’s tokenize the expression
    “Can LLMs reason counterfactually?”
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face的Transformers库有一个公开可用的GPT-2版本，它远不如最前沿的模型，但具有类似的*Transformer*架构。Transformer架构是一种深度学习模型，旨在通过关注句子中词语之间的关系（而不考虑它们的位置）来处理和理解文本以及其他序列数据。让我们分词表达式“LLMs能否进行反事实推理？”
- en: Listing 13.2 Viewing example tokens that an LLM operates upon
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.2 查看LLM操作的示例token
- en: '[PRE2]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Initialize the GPT-2 tokenizer.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化GPT-2分词器。'
- en: '#2 Tokenize the sequence.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 分词序列。'
- en: '#3 Print out the tokens.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 打印出token。'
- en: 'This prints out the following tokens:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出以下token：
- en: '[PRE3]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The “Ġ” corresponds to a space. Note that punctuation marks are tokens, and
    that words like “counterfactual” are broken up into multiple tokens. Each token
    corresponds to an integer indexing the token in a large “vocabulary.” GPT-2 has
    a vocabulary size of 50,257.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: “Ġ”对应于一个空格。请注意，标点符号也是token，像“counterfactual”这样的单词被分割成多个token。每个token对应于一个整数，该整数索引了在大型“词汇表”中的token。GPT-2的词汇表大小为50,257。
- en: Listing 13.3 Converting tokens to integers
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.3 将token转换为整数
- en: '[PRE4]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 “Encode” the tokens into integers that index the token in a list of tokens
    called the “vocabulary”.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 “编码”token为整数，这些整数在称为“词汇表”的token列表中索引token。'
- en: 'This *encodes* the tokens into a sequence of integers:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这将token*编码*为一个整数序列：
- en: '[PRE5]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The Transformer architecture works with these numeric values.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构与这些数值值一起工作。
- en: 'LLMs define a joint probability distribution on sequences of tokens. For the
    phrase “Can LLMs reason counterfactually?” the model defines a probability distribution:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在token序列上定义了一个联合概率分布。对于短语“LLMs能否进行反事实推理？”模型定义了一个概率分布：
- en: '![figure](../Images/ness-ch13-eqs-2x.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ness-ch13-eqs-2x.png)'
- en: 'The models will also consider the chances that this sequence ended at the question
    mark, rather than continuing. For that, the LLM’s vocabulary includes a special
    token to mark the end of a sequence. For GPT-2, this is token is `<|endoftext|>`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 模型还将考虑这种序列以问号结束而不是继续下去的可能性。为此，LLM 的词汇表中包含一个特殊标记来标记序列的结束。对于 GPT-2，这个标记是 `<|endoftext|>`：
- en: '![figure](../Images/ness-ch13-eqs-3x.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch13-eqs-3x.png)'
- en: 'Further, autoregressive LLMs, such as the GPT and Llama series of Transformer
    models, model text in the order of the text sequence, so they factorize this joint
    probability as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，自回归语言模型（LLM），如 GPT 和 Llama 系列的 Transformer 模型，按照文本序列的顺序对文本进行建模，因此它们将这个联合概率分解如下：
- en: '![figure](../Images/ness-ch13-eqs-4x.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch13-eqs-4x.png)'
- en: We can calculate each of these probabilities on the log scale with the Transformers
    library. In generating the log probability, we first calculate *logits* for each
    term in the vocabulary. For a probability value *p*, the corresponding logit is
    log(*p* / (1–*p*)).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Transformers 库在对数尺度上计算这些概率中的每一个。在生成对数概率时，我们首先计算词汇表中的每个术语的 *logits*。对于概率值
    *p*，相应的 logits 是 log(*p* / (1–*p*))。
- en: Listing 13.4 Calculate the log probability of each token in the sequence
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.4 计算序列中每个标记的对数概率
- en: '[PRE6]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Initialize the GPT-2 model and set to evaluation mode.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化 GPT-2 模型并将其设置为评估模式。'
- en: '#2 Tokenize and encode the phrase, including the end-of-sequence token.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将短语分词并编码，包括序列结束标记。'
- en: '#3 Given the phrase, the model produces logits for every element in the vocabulary.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 给定短语，模型为词汇表中的每个元素生成 logits。'
- en: '#4 For each position in the sequence, get the log probability corresponding
    to the token that was actually present in that position.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 对于序列中的每个位置，获取对应于实际出现在该位置的标记的对数概率。'
- en: 'This prints the following output:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下输出：
- en: '[PRE7]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Summing these together provides the joint probability of the sequence under
    the model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些相加提供了模型下序列的联合概率。
- en: Of course, as a generative model, GPT-2 can generate the next token conditional
    on the tokens that came before it. The *prompt* the user provides is the beginning
    of the sequence, and the model’s response extends the sequence.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，作为生成模型，GPT-2 可以根据前面的标记生成下一个标记。用户提供的 *prompt* 是序列的开始，模型的响应扩展了序列。
- en: Listing 13.5 Generation from the LLM
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.5 从 LLM 生成
- en: '[PRE8]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Specify and encode the prompt.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 指定并编码提示。'
- en: '#2 Generate from the model. The “do_sample=True” argument means we’re doing
    random selection from the probability distribution of the next token, given all
    the previous tokens.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 从模型生成。参数“do_sample=True”表示我们在给定所有前面的标记的情况下，从下一个标记的概率分布中进行随机选择。'
- en: '#3 Decode and print the output.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 解码并打印输出。'
- en: 'This prints out the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出以下内容：
- en: '[PRE9]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Again, note that ChatGPT generation has a random element, so this will likely
    produce something different for you.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，ChatGPT 的生成具有随机性，因此这可能会产生不同的结果。
- en: Note about confusing terminology
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于混淆术语的说明
- en: Models like the GPT models are often called “causal language models,” but these
    are not causal models in the way we’ve discussed in this book. They are not a
    causal model of a DGP. “Causal” here refers to the autoregressive nature of the
    model—the model evaluates the probability of a token in a sequence conditional
    only on the tokens that came before it.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 GPT 模型的模型通常被称为“因果语言模型”，但它们并不是本书中讨论的那种因果模型。它们不是 DGP 的因果模型。“因果”在这里指的是模型的自回归性质——模型仅根据前面的标记来评估序列中标记的概率。
- en: All this is to say that the LLM is at a basic level a probability model of the
    joint probability of a sequence of tokens. The canonical training procedures for
    these models attempt to fit the joint probability distribution of the tokens.
    Models like GPT optimize the model’s ability to predict a given token in a training
    document given the previous tokens. Understanding that the LLM is a probability
    model over tokens doesn’t explain why LLMs can generate *coherent* text (meaning
    text with logical and consistent interrelation of ideas that forms a comprehensible
    whole). For that, we need to understand *attention*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都意味着 LLM 在基本层面上是一个标记序列的联合概率模型。这些模型的典型训练程序试图拟合标记的联合概率分布。像 GPT 这样的模型优化了在训练文档中根据前面的标记预测给定标记的能力。理解
    LLM 是一个标记的概率模型并不能解释为什么 LLM 可以生成 *连贯* 的文本（即具有逻辑和一致思想相互关系、形成可理解整体的文本）。为此，我们需要理解
    *注意力*。
- en: 13.2.2 The attention mechanism
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 注意力机制
- en: One of the main drivers behind the success of LLMs is use of *Transformer architectures*
    and other neural network architectures that rely on a mechanism called *attention*.
    The attention mechanism allows the model to weigh the importance of different
    parts of an input sequence differently. That allows the model to learn to “focus”
    on specific parts of a sequence that are more relevant to a given task, while
    “ignoring” or assigning lesser weight to less pertinent parts.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）成功的主要驱动力之一是使用*Transformer架构*和其他依赖称为*注意力机制*的神经网络架构。注意力机制允许模型对输入序列的不同部分赋予不同的重要性。这使得模型能够学习“聚焦”于与特定任务更相关的序列的特定部分，同时“忽略”或赋予不太相关的部分较小的权重。
- en: 'Consider, for example, the following conditional counterfactual statement about
    leaves being a necessary cause of a fire:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以以下关于树叶是火灾必要原因的条件反事实陈述为例：
- en: The Labor Day weekend wildfire started in the forest and spread rapidly due
    to the dry leaves on the ground. Had there been a controlled burn, the fire wouldn’t
    have spread so rapidly.
  id: totrans-215
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 劳动节周末的森林大火始于森林，由于地面上干燥的树叶而迅速蔓延。如果进行了可控燃烧，火势就不会如此迅速地蔓延。
- en: The attention mechanism helps the model recognize that “leaves” refers to foliage,
    not a departure, by weighing the relevance of surrounding words like “ground,”
    “dry,” and “forest.”
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制帮助模型认识到“leaves”指的是树叶，而不是离开，通过权衡周围词语如“ground”、“dry”和“forest”的相关性。
- en: Modern LLMs have attention mechanisms stacked over many neural network layers.
    This enables the LLM to attend to concepts at different levels of granularity.
    For example, while the first layer of attention focuses on immediate word-to-word
    relationships, such as “leaves” with “ground,” the next few layers connect broader
    phrases, treating “The Labor Day weekend wildfire” as a single entity connected
    to the phrase “spread rapidly.”
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现代LLMs在许多神经网络层上堆叠了注意力机制。这使得LLM能够关注不同粒度级别上的概念。例如，注意力的第一层关注的是直接的词与词之间的关系，如“leaves”与“ground”，而接下来的几层则连接更广泛的短语，将“劳动节周末的森林大火”视为一个与短语“迅速蔓延”相连的单一实体。
- en: Latter layers can represent the overarching theme or subject of the sentence
    and the broader text, connecting “The Labor Day weekend wildfire” to information
    about how it spread.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 后续层可以表示句子的主题或主题以及更广泛文本的更广泛主题，将“劳动节周末的森林大火”与有关其蔓延的信息联系起来。
- en: 13.2.3 From tokens to causal representation
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.3 从标记符到因果表示
- en: The ability to talk about how attention enables the LLM to learn higher-level
    abstractions becomes of interest to us from the standpoint of causality. Recall
    figure 13.1, which first appeared in chapter 5 (as figure 5.4).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 从因果关系的角度来看，关于注意力如何使LLM学习高级抽象的能力对我们来说变得有趣。回想一下图13.1，它首次出现在第5章（如图5.4）。
- en: '![figure](../Images/CH13_F01_Ness.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F01_Ness.png)'
- en: Figure 13.1 Example from chapter 5, where *digit* and *is-handwritten* are high-level
    causal drivers of low-level *X**[i]* pixels
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.1 来自第5章的示例，其中*digit*和*is-handwritten*是低级*X**[i]*像素的高级因果驱动因素
- en: The small squares represent pixels in an image, while the squares *digit* and
    *is-handwritten* represent the digit depicted in the image and whether it was
    handwritten, respectively. In that example (section 5.1.2), I suggested that whatever
    causal relations exist between individual pixels doesn’t matter to us; we’re interested
    in reasoning at the level of the objects depicted in the image.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 小方块代表图像中的像素，而*digit*和*is-handwritten*方块分别代表图像中描绘的数字以及它是否是手写的。在那个例子（第5.1.2节）中，我建议个体像素之间存在的任何因果关系对我们来说并不重要；我们感兴趣的是在图像中描绘的物体层面上进行推理。
- en: There is a similar thing going on with tokens, as shown in figure 13.2.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 与此类似的情况也发生在标记符上，如图13.2所示。
- en: '![figure](../Images/CH13_F02_Ness.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F02_Ness.png)'
- en: Figure 13.2 *X*[1] through *X*[12] are a sequence of tokens. Whatever structure
    (causal or otherwise) exists between the tokens is of passing interest. We are
    interested in the causal relations between concepts described by the tokens.
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.2 *X*[1]至*X*[12]是一系列标记符。标记符之间存在的任何结构（因果或其他）对我们来说都是短暂的兴趣。我们感兴趣的是由标记符描述的概念之间的因果关系。
- en: In figure 13.2, as with the pixels, there is some structure at the level of
    the tokens. But that structure is beside the point—we’re interested in the causal
    relationships between the concepts that form the meaning behind the tokens.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在图13.2中，与像素一样，在标记级别存在一些结构。但这个结构与问题无关——我们感兴趣的是构成标记背后意义的概念之间的因果关系。
- en: The question becomes, under what circumstances could attention, insofar as it
    can learn higher-level representations, learn a *causal* representation. For example,
    could an attention-based model, perhaps under some set of architectural or learning
    constraints, or use of interventions in training data, learn the parallel world
    structure and abstractions in the Labor Day counterfactual statement?
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 问题变成了，在什么情况下，注意力，只要它能学习高级表示，能够学习一个*因果*表示。例如，一个基于注意力的模型，可能在某些架构或学习约束下，或者在训练数据中使用干预措施，能够学习劳动节反事实陈述中的并行世界结构和抽象？
- en: To consider this, we can revisit LLM hallucinations in our question about the
    probability of necessity, and connect it to causal identification.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑这一点，我们可以回顾LLM的幻觉问题，并将其与因果识别联系起来。
- en: 13.2.4 Hallucination, attention, and causal identification
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.4 幻觉、注意力和因果识别
- en: The hallucination about probability of necessity was generated with GPT-4\.
    The same model got the Netflix question about probability of necessity correct.
    Indeed, the hallucinated answer would have been right if it merely stated the
    correct identifying assumptions. I believe future versions of GPT and similar
    models will likely get this question right on the first try.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 关于必要性的概率的幻觉是由GPT-4生成的。同样的模型在Netflix关于必要性的概率问题上也给出了正确的答案。事实上，如果幻觉的答案仅仅陈述了正确的识别假设，那么这个答案就是正确的。我相信GPT和类似模型未来的版本可能会在第一次尝试时就正确回答这个问题。
- en: But for someone who is unfamiliar with the definition of probability of necessity,
    how would they know if the model were right or if it were hallucinating? First,
    the causal hierarchy tells us that in order to be capable of generating a right
    answer beyond a random guess, the query would need to be identified with level
    3 information. Perhaps that information is provided by the user in the prompt.
    Perhaps the LLM has somehow learned level 3 representations (such a claim would
    require hard proof).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于不熟悉必要性概率定义的人来说，他们如何知道模型是正确的还是产生了幻觉？首先，因果层次告诉我们，为了能够生成一个超出随机猜测的正确答案，查询需要与第3级信息相对应。也许这些信息是由用户在提示中提供的。也许LLM以某种方式学习了第3级表示（这样的主张需要硬性证据）。
- en: If the user were providing that identifying information in the prompt, how would
    the user know if the model was successfully using that information to respond
    to the prompt? Suppose instead that the requirements for identification exist
    and are buried in the learned representations or data, and that they were being
    successfully leveraged by the model in answering the causal query, how could the
    user know for sure this was happening?
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户在提示中提供了那个识别信息，用户如何知道模型是否成功使用了这些信息来响应提示？假设识别的要求存在并且被隐藏在学习的表示或数据中，并且模型在回答因果查询时成功地利用了这些信息，用户如何确信这种情况发生了？
- en: We need to engineer solutions that answer these and other desiderata to build
    toward a causal AI future. In the next section, we’ll start on this path with
    a simple causal LLM.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设计解决方案来回答这些问题和其他期望，以构建向因果AI未来的发展。在下一节中，我们将从这个简单的因果LLM开始。
- en: 13.3 Forging your own causal LLM
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 打造你自己的因果LLM
- en: In this section, we’ll sidestep the question of “can cutting-edge LLMs reason
    causally?” and move on to building a causal LLM that can reason causally. We’ll
    build for causality from the ground up, rather than as an afterthought.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将绕过“最前沿的LLM能否进行因果推理？”这个问题，转而构建一个能够进行因果推理的因果LLM。我们将从零开始构建因果性，而不是作为事后考虑。
- en: 13.3.1 An LLM for script writing
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.1 用于脚本编写的LLM
- en: Often our data has some implicit causal structure. When we make that structure
    explicit during training, the foundation model can learn better causal representations.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据通常具有一些隐含的因果结构。当我们将这种结构在训练中明确化时，基础模型可以学习更好的因果表示。
- en: 'To illustrate, suppose a prolific film production studio has historically insisted
    their writers use script-writing software that required following a three-act
    narrative archetype, which is common for romantic comedies: “boy meets girl, boy
    loses girl, boy gets girl back.” For this archetype, they have a corpus of many
    scripts. In causal terms, the events in act 1 cause the events of act 2, and the
    events of acts 1 and 2 cause the events of act 3\. We can draw the DAG in figure
    13.3.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，假设一个历史悠久的电影制作工作室一直坚持要求他们的编剧使用需要遵循三幕叙事原型的剧本编写软件，这在浪漫喜剧中很常见：“男孩遇见女孩，男孩失去女孩，男孩找回女孩。”对于这个原型，他们有一系列许多剧本。在因果术语中，第一幕的事件导致第二幕的事件，第一幕和第二幕的事件导致第三幕的事件。我们可以在图13.3中绘制DAG。
- en: '![figure](../Images/CH13_F03_Ness.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH13_F03_Ness.png)'
- en: Figure 13.3 A causal DAG for a three-act archetype
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.3 三幕原型的因果有向图
- en: 'The studio works with many such archetypes, and the company has many scripts
    that follow a given archetype template. Suppose that a set of archetypes involve
    a king acting a certain way in act 1, a prince acting a certain way in act 2,
    and these two actions having an effect on a kingdom in act 3\. For example, one
    possible archetype is “King declares war, Prince leads army, kingdom experiences
    prosperity.” But there are multiple outcomes for each act:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 工作室与许多这样的原型一起工作，公司有许多遵循特定原型模板的剧本。假设一组原型涉及国王在第一幕中以某种方式行事，王子在第二幕中以某种方式行事，这两者在第三幕中对王国产生影响。例如，一个可能的原型是“国王宣战，王子领导军队，王国繁荣”。但每个幕都有多种结果：
- en: 'King in act 1: {king declares war; king negotiates peace; king falls ill}'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一幕中的国王：{国王宣战；国王谈判和平；国王生病}
- en: 'Prince in act 2: {prince leads army; prince abdicates throne; prince marries
    foreigner}'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二幕中的王子：{王子领导军队；王子退位；王子与外国人结婚}
- en: 'Kingdom in act 3: {kingdom wins battle; kingdom falls into poverty; kingdom
    experiences prosperity}'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三幕中的王国：{王国赢得战斗；王国陷入贫困；王国繁荣}
- en: '![figure](../Images/CH13_F04_Ness.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH13_F04_Ness.png)'
- en: Figure 13.4 A causal DAG representing various King-Prince-Kingdom archetypes
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.4 表示各种国王-王子-王国原型的因果有向图
- en: Figure 13.4 shows this space of archetypes in the form of a causal DAG.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4以因果有向图的形式显示了这种原型空间。
- en: This describes only 3 × 3 × 3 = 27 possible archetypes, but as you might expect,
    some archetypes are more common and some are less common. We could easily model
    these archetypes and the joint probability distribution by explicitly coding causal
    Markov kernels in pgmpy or Pyro. But that would only be a causal generative model
    on the archetypes. If we want a script generator, we want a causal generative
    model on the scripts.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '这只描述了3 × 3 × 3 = 27种可能的原型，但正如你所预期的，一些原型更常见，而一些原型则不太常见。我们可以很容易地通过在pgmpy或Pyro中显式编码因果马尔可夫核来对这些原型和联合概率分布进行建模。但这将只是一种基于原型的因果生成模型。如果我们想要一个剧本生成器，我们想要一个基于剧本的因果生成模型。 '
- en: To demonstrate a proof-of-concept for this idea, we’ll work with a training
    dataset of short vignettes, rather than full scripts. Let’s load and examine the
    training data.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这个想法的证明概念，我们将使用短故事片段的训练数据集，而不是完整的剧本。让我们加载并检查训练数据。
- en: Listing 13.6 Load causal narrative data
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.6 加载因果叙事数据
- en: '[PRE10]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 The data has 21,000 stories, broken up into three short vignettes.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 数据包含21,000个故事，分为三个简短的故事片段。'
- en: '#2 First, the king acts.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 首先，国王采取行动。'
- en: '#3 Then the prince acts.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 然后王子采取行动。'
- en: '#4 Finally, the kingdom experiences the consequences of the royals’ actions.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 最后，王国经历了皇室行动的后果。'
- en: 'This code prints the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码打印以下内容：
- en: '[PRE11]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There are 21,000 sets of three vignettes. The preceding output shows the first
    three sets in the dataset.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 有21,000组三个故事片段。前面的输出显示了数据集中的前三个组。
- en: 13.3.2 Using pretrained models for causal Markov kernels
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.2 使用预训练模型进行因果马尔可夫核
- en: To train the causal Markov kernels for each node in our DAG, we’ll take pretrained
    models from the Hugging Face Transformers library, and then further train (aka
    “fine-tune”) the models using our vignettes. The pretraining took care of the
    heavy lifting in terms of learning to generate coherent natural language text.
    The fine-tuning will align the models toward representing our causal Markov kernels.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们DAG中每个节点的因果马尔可夫核，我们将从Hugging Face Transformers库中获取预训练模型，然后进一步使用我们的故事片段进行训练（也称为“微调”）。预训练处理了学习生成连贯自然语言文本的重活。微调将使模型朝着表示我们的因果马尔可夫核的方向调整。
- en: First, we’ll use a GPT-2 variant to model the King’s action vignettes. As a
    text-completion model, it typically takes a prompt as input, but we’ll train it
    to generate with an empty prompt and produce vignettes according to the marginal
    probabilities of the King’s action texts in the training data, as in figure 13.5.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用GPT-2的一个变体来模拟国王的行动片段。作为一个文本补全模型，它通常接受一个提示作为输入，但我们将训练它使用空提示生成片段，并根据训练数据中国王行动文本的边缘概率生成片段，如图13.5所示。
- en: '![figure](../Images/CH13_F05_Ness.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F05_Ness.png)'
- en: Figure 13.5 GPT-2 is fine-tuned to represent the distribution of King’s action
    vignettes.
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.5 GPT-2经过微调，以表示国王行动片段的分布。
- en: Next, we’ll use a BART model for the causal Markov kernel Prince’s action. BART
    is a Transformer model released in 2019 designed specifically to take an input
    sequence and generate a corresponding output sequence, such as with translation
    or summarization. Large models like GPT-4 can handle sequence-to-sequence tasks
    quite well, but we’ll use a version of BART with roughly 4,000-times fewer parameters
    than GPT-4, making it easier for you to load and train on your laptop or basic
    Python development environment. Given the King’s action vignette as input, it
    will generate a Prince’s action vignette, as illustrated in figure 13.6.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用BART模型来模拟王子行动的因果马尔可夫核。BART是2019年发布的一个Transformer模型，专门设计用来接受一个输入序列并生成相应的输出序列，例如翻译或摘要。像GPT-4这样的大型模型可以很好地处理序列到序列的任务，但我们将使用一个参数数量大约比GPT-4少4000倍的BART版本，这使得它在您的笔记本电脑或基本的Python开发环境中加载和训练变得更加容易。给定国王的行动片段作为输入，它将生成一个王子的行动片段，如图13.6所示。
- en: We’ll also use a BART model to model the causal Markov kernel for the Kingdom’s
    fate, as shown in figure 13.7\. The model will map the King’s and Prince’s actions
    to the Kingdom’s fate.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用BART模型来模拟王国命运的因果马尔可夫核，如图13.7所示。该模型将国王和王子行动映射到王国的命运。
- en: '![figure](../Images/CH13_F06_Ness.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F06_Ness.png)'
- en: Figure 13.6 A BART sequence-to-sequence model is fine-tuned to represent the
    Prince’s action given the King’s action.
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.6 一个BART序列到序列模型经过微调，以表示在国王行动的情况下王子的行动。
- en: '![figure](../Images/CH13_F07_Ness.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F07_Ness.png)'
- en: Figure 13.7 A BART sequence-to-sequence model is also used to model the Kingdom’s
    fate given the King’s action and Prince’s action.
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.7 一个BART序列到序列模型也用于模拟在国王行动和王子行动的情况下王国的命运。
- en: Jumping ahead, we’re interested in the conditional probability distribution
    of the Kingdom’s fate, given a certain action by the Prince. Since that will require
    inference of the King’s actions given the Prince, we’ll additionally train one
    more BART model that generates a King’s action vignette given a Prince’s action
    vignette, as shown in figure 13.8.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 提前跳过，我们感兴趣的是在王子采取特定行动的情况下，王国命运的条件概率分布。由于这需要根据王子的行动推断国王的行动，因此我们还将额外训练一个BART模型，该模型根据王子的行动片段生成国王的行动片段，如图13.8所示。
- en: '![figure](../Images/CH13_F08_Ness.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F08_Ness.png)'
- en: Figure 13.8 A BART sequence-to-sequence model is also fine-tuned to model the
    Kingdom’s fate, given the King’s and Prince’s actions.
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.8 一个BART序列到序列模型也经过微调，以模拟国王和王子行动情况下的王国命运。
- en: Let’s run the training procedure. First, we’ll set up our imports and our tokenizer.
    We’ll use Bart as the tokenizer for all of our models.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行训练过程。首先，我们将设置我们的导入和分词器。我们将使用Bart作为我们所有模型的分词器。
- en: Listing 13.7 Training the causal LLM
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.7 训练因果LLM
- en: '[PRE12]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Set up the tokenizer.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置分词器。'
- en: '#2 The pad token is used to make all the sequences the same length to facilitate
    matrix operations. It is common to set it to the “end-of-sequence (EOS)” token.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用填充标记（pad token）使所有序列具有相同的长度，以便进行矩阵运算。通常将其设置为“序列结束（EOS）”标记。'
- en: '#3 The max length of the token is set to 40, as all of the vignettes are less
    than 40 tokens.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将标记的最大长度设置为40，因为所有的片段都少于40个标记。'
- en: Next we’ll create a class and a function that tokenizes the King dataset. We’ll
    create a custom subclass of the PyTorch `Dataset` class called `ModelDataset`
    that will store token encodings and their corresponding labels. When accessed
    by index, it returns a dictionary containing token encodings for that index and
    the associated label, and it provides the total number of examples via its `__len__`
    method.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个类和一个函数来分词国王数据集。我们将创建一个名为`ModelDataset`的PyTorch `Dataset`类的自定义子类，该子类将存储标记编码及其对应的标签。当通过索引访问时，它返回一个包含该索引的标记编码及其相关标签的字典，并通过其`__len__`方法提供示例总数。
- en: Listing 13.8 Tokenizing the King vignettes
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.8 对King片段进行分词
- en: '[PRE13]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 When accessed by index, ModelDataset returns a dictionary containing token
    encodings for that index and the associated label.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 当通过索引访问时，ModelDataset返回一个包含该索引的标记编码及其相关标签的字典。'
- en: '#2 Create a ModelDataset instance for the king vignettes.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为King片段创建一个ModelDataset实例。'
- en: Next we’ll tokenize the Prince and Kingdom vignettes. This code will also produce
    a validation dataset used in training sequence-to-sequence models.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对Prince和Kingdom的片段进行分词。此代码还将生成用于训练序列到序列模型的验证数据集。
- en: Listing 13.9 Tokenizing the Prince and Kingdom vignettes
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.9 对Prince和Kingdom片段进行分词
- en: '[PRE14]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Split input and target phrases into training and validation sets.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将输入和目标短语分割成训练集和验证集。'
- en: '#2 Encode the training and validation sets.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 对训练集和验证集进行编码。'
- en: Next, we’ll write a training algorithm for the King model. This function initializes
    a GPT-2 model with the specified parameters, sets up the training arguments, and
    trains the model on the provided dataset, finally saving the trained model to
    the specified directory.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编写King模型的训练算法。此函数使用指定的参数初始化GPT-2模型，设置训练参数，并在提供的数据集上训练模型，最后将训练好的模型保存到指定的目录。
- en: Listing 13.10 Training the King model
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.10 训练King模型
- en: '[PRE15]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Initialize and configure model with the specified parameters.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用指定的参数初始化和配置模型。'
- en: '#2 Configure the training settings.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 配置训练设置。'
- en: '#3 Train the model.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 训练模型。'
- en: Next, we’ll write a training algorithm for the sequence-to-sequence models.
    The function will split the provided input and target phrases into training and
    validation sets, tokenize them, and then create and return PyTorch `Dataset` objects
    for both sets using the `ModelDataset` class. The `train_seq2seq_model` function
    initializes a sequence-to-sequence model with the specified parameters, configures
    its training settings, and then trains the model using both training and validation
    datasets, finally returning the trained model.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编写序列到序列模型的训练算法。该函数将提供的输入和目标短语分割成训练集和验证集，对它们进行分词，然后使用`ModelDataset`类创建并返回两个集合的PyTorch
    `Dataset`对象。`train_seq2seq_model`函数使用指定的参数初始化序列到序列模型，配置其训练设置，然后使用训练集和验证数据集训练模型，最后返回训练好的模型。
- en: Listing 13.11 Function for training the sequence-to-sequence models
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.11 训练序列到序列模型的函数
- en: '[PRE16]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 Initialize and configure sequence-to-sequence model with the specified parameters.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用指定的参数初始化和配置序列到序列模型。'
- en: '#2 Configure the training settings.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 配置训练设置。'
- en: '#3 Train the model using both training and validation datasets, finally returning
    the trained model.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用训练集和验证数据集训练模型，最后返回训练好的模型。'
- en: Now we’ll use this function to train the models. We’ll specify some directories
    for saving checkpoints.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用此函数来训练模型。我们将指定一些用于保存检查点的目录。
- en: Note  In listing 13.14, I’ll provide code that downloads a pretrained model
    from Hugging Face, so if you don’t wish train the model, you can skip ahead to
    that step.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在列表13.14中，我将提供从Hugging Face下载预训练模型的代码，所以如果你不想训练模型，可以跳过此步骤。
- en: Listing 13.12 Training the King, Prince, and Kingdom models
  id: totrans-304
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.12 训练King、Prince和Kingdom模型
- en: '[PRE17]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Provide the output directories where you want to save your model.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 提供你想要保存模型的输出目录。'
- en: '#2 Train the King model using Seq2Seq.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用Seq2Seq训练King模型。'
- en: '#3 Train the Prince model using Seq2Seq. The King vignettes are used to predict
    the Prince vignettes.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用Seq2Seq训练Prince模型。King片段用于预测Prince片段。'
- en: '#4 Train the Prince model using Seq2Seq. The King vignettes are used to predict
    the Prince vignettes.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用Seq2Seq训练Prince模型。King片段用于预测Prince片段。'
- en: '#5 Train the Kingdom model using Seq2Seq. The combined King and Prince vignettes
    are used to predict the Kingdom vignettes.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 使用Seq2Seq训练Kingdom模型。结合King和Prince的片段用于预测Kingdom片段。'
- en: Finally, we’ll train another model for inferring the King vignette given a Prince
    vignette. We’ll use this in inference later.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将训练另一个模型，用于根据Prince片段推断King片段。我们将在推理阶段使用这个模型。
- en: Listing 13.13 Function to train the Prince to King model
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.13 用于训练Prince到King模型的函数
- en: '[PRE18]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Running the preceding training procedure will take some time, especially if
    you’re not using GPU. Fortunately, there are saved versions of the trained models
    in the Hugging Face Hub. The following code pulls the Transformer models from
    the Hugging Face Hub and generates from them. It also provides a function that
    calculates the log probability of each generated sequence.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的训练过程将需要一些时间，尤其是如果你没有使用GPU。幸运的是，Hugging Face Hub中有训练好的模型的保存版本。以下代码从Hugging
    Face Hub获取Transformer模型并从中生成。它还提供了一个计算每个生成序列对数概率的函数。
- en: Listing 13.14 Pull Transformer models from the Hugging Face Hub and generate
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.14 从Hugging Face Hub获取Transformer模型并生成
- en: '[PRE19]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Load the components of our model.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载我们模型的组件。'
- en: '#2 Load the components of our model.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 加载我们模型的组件。'
- en: '#3 Load the Bart-base tokenizer and set the pad token to end-of-sequence tokens.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 加载Bart-base分词器，并将填充标记设置为序列结束标记。'
- en: Next, we’ll write some functions to encode text to tokens, decode tokens to
    text, and generate text from the model given input text.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编写一些函数来编码文本为标记，解码标记为文本，以及根据输入文本从模型生成文本。
- en: Listing 13.15 Helper functions for encoding, decoding, and generation
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.15 编码、解码和生成辅助函数
- en: '[PRE20]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 Encode text into tensor.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将文本编码成张量。'
- en: '#2 Decode tensor into text.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将张量解码为文本。'
- en: '#3 Get the encoding for empty text, for convenience.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 获取空文本的编码，以便于使用。'
- en: '#4 A function for generating from models. These parameters do slightly different
    things for the GPT-2 and BART models, but they more or less overlap.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 一个从模型生成内容的函数。这些参数对于GPT-2和BART模型来说执行不同的功能，但它们大致上是重叠的。'
- en: We want to use our probabilistic ML approach, so we need a way of computing
    the log probabilities of generated sequences so we can use these in inference.
    The following function computes the log probability of a generated sequence based
    on related values produced by the GPT-2 and BART models.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用我们的概率机器学习方法，因此我们需要一种计算生成序列对数概率的方法，这样我们就可以在推理中使用这些值。以下函数基于GPT-2和BART模型产生的相关值计算生成序列的对数概率。
- en: Listing 13.16 Computing log probabilities of generated sequences
  id: totrans-328
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.16 计算生成序列的对数概率
- en: '[PRE21]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#1 Convert logits to logprobs for GPT-2.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将GPT-2的logits转换为logprobs。'
- en: '#2 Convert logits to logprobs from BART cross-entropy.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将logits从BART交叉熵转换为logprobs。'
- en: Finally, we’ll put these pieces together to generate a full story from our three
    models.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将把这些部分组合起来，从我们的三个模型生成一个完整的故事。
- en: Listing 13.17 Generating a full story
  id: totrans-333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.17 生成完整故事
- en: '[PRE22]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#1 Generate from the GPT-based model of vignettes about the King and calculate
    the log probabilities of the generated sequence.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从基于GPT的模型生成关于国王的短篇故事，并计算生成序列的对数概率。'
- en: '#2 Generate from the BART-based sequence-to-sequence model that generates vignettes
    about the Prince given vignettes about the King, and then calculate the log probability
    of the generated sequence.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 从基于BART的序列到序列模型生成关于王子的短篇故事，给定关于国王的短篇故事，然后计算生成序列的对数概率。'
- en: '#3 Generate from the BART-based sequence-to-sequence model that generates vignettes
    about the Kingdom given vignettes about the King and the Prince, and then calculate
    the log probability of the generated sequence.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从基于BART的序列到序列模型生成关于王国的短篇故事，给定关于国王和王子的短篇故事，然后计算生成序列的对数概率。'
- en: '#4 Another BART-based sequence-to-sequence model that maps a vignette about
    the Prince to a vignette about the King. We’ll use this to infer the vignette
    about the King from a vignette about the Prince.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 另一个基于BART的序列到序列模型，将关于王子的短篇故事映射到关于国王的短篇故事。我们将使用这个模型从关于王子的短篇故事中推断出关于国王的短篇故事。'
- en: 'The output is nondeterministic, but one example of the output you’ll get is
    as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是非确定性的，但你会得到的一个输出示例如下：
- en: '[PRE23]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that the generated output isn’t perfect—for example, the first generated
    statement ideally should have stopped after “…kingdom’s territories”. We could
    try to train it more or switch to a more powerful model, but this is pretty good
    for a start.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，生成的输出并不完美——例如，第一个生成的陈述理想情况下应该在“…王国的领土”之后停止。我们可以尝试进一步训练它或切换到一个更强大的模型，但作为开始来说这已经相当不错了。
- en: Next, we’ll use these Transformers library models to define distributions in
    Pyro, and then use Pyro to build a causal generative model. First, we’ll use Pyro’s
    `TorchDistributionMixin` to model the causal Markov kernels with the language
    models. We’ll use the GPT-2 model of the King vignettes to create the causal Markov
    kernel of the `King` variable.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用这些 Transformers 库模型在 Pyro 中定义分布，然后使用 Pyro 构建一个因果生成模型。首先，我们将使用 Pyro
    的 `TorchDistributionMixin` 来使用语言模型对因果马尔可夫核进行建模。我们将使用国王短篇故事的 GPT-2 模型来创建 `King`
    变量的因果马尔可夫核。
- en: Next, we’ll use the BART model to create the causal Markov kernel for the `Prince`
    variable. The `King` variable causes this variable, so the seq2seq model uses
    the `King` variable’s value to generate a value for this model.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 BART 模型为 `Prince` 变量创建因果马尔可夫核。`King` 变量导致这个变量，因此 seq2seq 模型使用 `King`
    变量的值为此模型生成一个值。
- en: Finally, we’ll create the causal Markov kernel for the `Kingdom` variable. The
    `King` and `Prince` variables are causal parents, so we concatenate their generated
    outputs into one string, and use that string to generate the `Kingdom` output,
    again using a BART seq2seq model. We rely on a mixin called `TorchDistributionMixin`,
    which is useful for wrapping PyTorch distributions for use in Pyro.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将为 `Kingdom` 变量创建因果马尔可夫核。`King` 和 `Prince` 变量是因果父变量，因此我们将它们的生成输出连接成一个字符串，并使用该字符串生成
    `Kingdom` 输出，再次使用 BART seq2seq 模型。我们依赖于一个名为 `TorchDistributionMixin` 的混合器，它对于将
    PyTorch 分布包装用于 Pyro 非常有用。
- en: Listing 13.18 Building a Torch distribution from a Transformer model
  id: totrans-345
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.18 从 Transformer 模型构建 Torch 分布
- en: '[PRE24]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#1 Use TorchDistributionMixin to turn a Transformers model into a Pyro distribution.
    TorchDistributionMixin is used to make PyTorch distributions compatible with Pyro’s
    utilities.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用 TorchDistributionMixin 将 Transformers 模型转换为 Pyro 分布。TorchDistributionMixin
    用于使 PyTorch 分布与 Pyro 的实用工具兼容。'
- en: '#2 The log_prob method returns the log probabilities used in inference algorithms.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 log_prob 方法返回用于推理算法的对数概率。'
- en: Now we’ll use that distribution in Pyro.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用该分布于 Pyro 中。
- en: Listing 13.19 Incorporating Transformer models into a causal model with Pyro
  id: totrans-350
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.19 使用 Pyro 将 Transformer 模型集成到因果模型中
- en: '[PRE25]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 Build the causal LLM.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 构建因果 LLM。'
- en: '#2 Create the causal Markov kernel for the King variable.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为国王变量创建因果马尔可夫核。'
- en: '#3 Create the causal Markov kernel for the Prince variable.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 为王子变量创建因果马尔可夫核。'
- en: '#4 Create the causal Markov kernel for the Kingdom variable.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 为王国变量创建因果马尔可夫核。'
- en: '#5 Concatenate all the generated vignettes into one overall vignette and return
    the result.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 将所有生成的短篇故事连接成一个整体故事并返回结果。'
- en: '#6 Confirm our causal model generates the full vignette.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 确认我们的因果模型生成了完整的短篇故事。'
- en: 'The preceding code generates and prints two vignettes, such as the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码生成并打印了两个短篇故事，如下所示：
- en: '[PRE26]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We see that the generated texts are pretty good, though they seem to cut off
    a bit early. This, and other issues with the generations, can be addressed by
    tweaking the generation parameters.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到生成的文本相当不错，尽管它们似乎提前截断了。通过调整生成参数，可以解决这些问题以及其他生成问题。
- en: And just like that, we’ve built a causal LLM, an LLM build on a causal DAG scaffold.
    Let’s prove we have a causal model by comparing the observational and interventional
    distributions entailed by the DAG.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们已经构建了一个因果 LLM，这是一个基于因果 DAG 框架构建的 LLM。让我们通过比较 DAG 所蕴含的观察分布和干预分布来证明我们有一个因果模型。
- en: 13.3.3 Sampling from the interventional and observational distributions
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.3 从干预和观察分布中进行采样
- en: By now, you know the distribution *P*(*Kingdom*|*Prince*=*x*) will be different
    from *P*(*Kingdom*[*Prince*][=][*x*]), but let’s demonstrate the fact with this
    causal LLM. First, we’ll model *P*(*Kingdom*|*Prince*=*x*), where *x* is
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你知道分布 *P*(*Kingdom*|*Prince*=*x*) 将与 *P*(*Kingdom*[*Prince*][=][*x*])
    不同，但让我们通过这个因果 LLM 来证明这个事实。首先，我们将建模 *P*(*Kingdom*|*Prince*=*x*)，其中 *x* 是
- en: His courageous Prince takes command, leading the kingdom’s army to victory in
    battle after battle
  id: totrans-364
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 他的勇敢的王子指挥军队，在一场又一场的战斗中取得胜利
- en: To infer *P*(*Kingdom*|*Prince*=*x*), we’ll have to infer the distribution of
    the latent confounder, *King*. We’ll do this using the `prince2king_model` we
    trained. We’ll use a probabilistic inference algorithm called “importance resampling.”
    We’ll start by creating a proposal function (what Pyro calls a “guide function”)
    that will generate samples of *King* and *Kingdom*, given *Prince*.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 要推断 *P*(*Kingdom*|*Prince*=*x*)，我们必须推断潜在混杂因子 *King* 的分布。我们将使用我们训练的 `prince2king_model`
    来完成这项工作。我们将使用一种名为“重要性重采样”的概率推理算法。我们将首先创建一个提议函数（Pyro 称之为“引导函数”），该函数将根据王子生成 *King*
    和 *Kingdom* 的样本。
- en: Listing 13.20 Proposal distribution for *P*(*Kingdom*|*Prince*=*x*)
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.20 *P*(*Kingdom*|*Prince*=*x*) 的提议分布
- en: '[PRE27]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#1 We condition the model on this value of the Prince variable.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们将模型的条件设置为王子变量的这个值。'
- en: '#2 We’ll use a proposal function to generate from our target distribution P(King,
    Kingdom|Prince=PRINCE_STORY).'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们将使用提议函数从我们的目标分布 P(King, Kingdom|Prince=PRINCE_STORY) 中生成样本。'
- en: '#3 The proposal uses the prince2king_model to infer values of King given Prince=PRINCE_STORY.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 提议使用 prince2king_model 来推断王子=PRINCE_STORY 时的国王值。'
- en: '#4 Given the value of Prince, and the inferred value of King, use the king_and_prince
    model to sample Kingdom.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 给定王子值和推断出的国王值，使用 king_and_prince 模型来抽取王国。'
- en: '#5 Concatenate the generated king tokens and provided prince tokens to return
    a generated vignette so we can inspect what is sampled.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 将生成的国王标记和提供的王子标记连接起来，以返回一个生成的短篇故事，这样我们就可以检查所抽取的内容。'
- en: Now we’ll weigh each sample by the ratio of the probability of the sample under
    the conditioned model, over the probability of the sample under the proposal.
    Resampling the samples using these weights will generate samples from the target
    distribution. Pyro provides a utility for importance sampling, but because of
    the varying length of the generated sequences, it will be easier to implement
    importance sampling directly.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将根据在条件模型下样本的概率与在提议下样本的概率的比率来权衡每个样本。使用这些权重重采样样本将生成来自目标分布的样本。Pyro 提供了重要性采样的实用工具，但由于生成的序列长度不一，直接实现重要性采样将更容易。
- en: First, we’ll write a function to process a sample and get its importance weight.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将编写一个函数来处理样本并获取其重要性权重。
- en: Listing 13.21 Function to draw a sample for resampling
  id: totrans-375
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.21 用于抽取重采样样本的函数
- en: '[PRE28]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#1 Extract a sample from the proposal.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从提议中提取一个样本。'
- en: '#2 Calculate the total log probability of the sampled values of King and Kingdom.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 计算国王和王国样本值的总对数概率。'
- en: '#3 Calculate the total log probability of the sample values of King and Kingdom
    under the original model.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 计算在原始模型下国王和王国样本值的总对数概率。'
- en: '#4 Calculate the log importance weight.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 计算对数重要性权重。'
- en: Now we’ll run the importance resampling.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将运行重要性重采样。
- en: Listing 13.22 Listing 13.22 Importance resampling of *P*(*Kingdom*|*Prince*=*x*)
  id: totrans-382
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.22 列表 13.22 *P*(*Kingdom*|*Prince*=*x*) 的重要性重采样
- en: '[PRE29]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#1 Use importance resampling as our inference procedure.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用重要性重采样作为我们的推理过程。'
- en: '#2 Resample using the importance weights. Pass in the log weights to the “logits”
    argument.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用重要性权重进行重采样。将对数权重传递给“logits”参数。'
- en: Next, we’ll infer *P*(*Kingdom*[*Prince*][=][*x*]). Given our causal model in
    Pyro, we can use Pyro’s do-operator to apply the intervention. We know that given
    the intervention on Prince, the edge from King to Prince is removed, so we don’t
    need to use `prince2king_model`. We can simply do ordinary forward generation
    from our intervention model.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将推断 *P*(*Kingdom*[*Prince*][=][*x*])。鉴于我们在 Pyro 中的因果模型，我们可以使用 Pyro 的 do-operator
    来应用干预。我们知道，在王子干预下，国王到王子的边被移除，因此我们不需要使用 `prince2king_model`。我们可以简单地从我们的干预模型中进行普通的前向生成。
- en: Listing 13.23 Inferring *P*(*KingdomPrince*=*x*) using vanilla forward Monte
    Carlo sampling
  id: totrans-387
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.23 使用纯前向蒙特卡洛采样推断 *P*(*KingdomPrince*=*x*)
- en: '[PRE30]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '#1 Forward sample from the interventional distribution.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从干预分布中进行前向样本抽取。'
- en: '#2 Label the samples, and combine them with the observational samples.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 标记样本，并将它们与观察样本结合起来。'
- en: 'Generating the samples will take some time. Since we’re working directly with
    the encoded sequence tensors in Pyro, we could leverage the potentially faster
    gradient-based inference algorithm. For convenience, you can access presaved samples
    in the book’s directory of the GitHub repo: [https://github.com/altdeep/causalml](https://github.com/altdeep/causalml).'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 生成样本将需要一些时间。由于我们直接在Pyro中使用编码序列张量工作，我们可以利用潜在的更快基于梯度的推理算法。为了方便，您可以在GitHub仓库的书目录中访问预先保存的样本：[https://github.com/altdeep/causalml](https://github.com/altdeep/causalml)。
- en: Next, let’s visualize the difference in the distributions. We need a way to
    visualize sampled text from the interventional and observational distributions.
    We can do so using TF-IDF (term frequency-inverse document frequency), a numerical
    statistic that reflects how important a word is to a sample within the collection
    of samples, emphasizing words that are unique to specific samples.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们可视化分布之间的差异。我们需要一种方法来可视化来自干预和观察分布的样本文本。我们可以使用TF-IDF（词频-逆文档频率），这是一种数值统计量，反映了单词在样本集合中对样本的重要性，强调了特定样本中独特的单词。
- en: Listing 13.24 Get TF-IDF of generations for *P*(*Kingdom**[Prince=x]*) and *P*(*Kingdom*|*Prince*=*x*)
  id: totrans-393
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.24 获取*P*(*Kingdom**[Prince=x]*)和*P*(*Kingdom*|*Prince*=*x*)的生成TF-IDF。
- en: '[PRE31]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#1 Extract generated Kingdom vignettes from observational and interventional
    distributions.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从观察和干预分布中提取生成的王国场景。'
- en: '#2 Compute the TF-IDF values for generated Kingdom vignettes in each group.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 计算每个组中生成的王国场景的TF-IDF值。'
- en: '#3 Get the top k=7 words by TF-IDF for each set.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 获取每个集合中TF-IDF评分最高的k=7个单词。'
- en: Finally, we’ll visualize the two distributions.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将可视化这两个分布。
- en: Listing 13.25 Visually contrast *P*(*Kingdom**[Prince=x]*) and *P*(*Kingdom*|*Prince*=*x*)
  id: totrans-399
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.25 可视化对比*P*(*Kingdom**[Prince=x]*)和*P*(*Kingdom*|*Prince*=*x*)。
- en: '[PRE32]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '#1 Prepare data for the bar plot.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 准备条形图的数据。'
- en: '#2 Produce the plot.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 生成图表。'
- en: This produces figure 13.9.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了图13.9。
- en: Figures 13.9 shows similar TF-IDF scores for words in the observational case.
    This is due to the lack of variation in the observational case, since observing
    the Prince constrains the likely values of King. When we intervene on Prince,
    King can vary more, leading to more variation in the results.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9显示了观察案例中单词的相似TF-IDF评分。这是由于观察案例中缺乏变化，因为观察王子限制了国王的可能值。当我们对王子进行干预时，国王可以变化更多，导致结果有更多的变化。
- en: '![figure](../Images/CH13_F09_Ness.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F09_Ness.png)'
- en: Figure 13.9 Visualization of the difference between samples from *P*(*Kingdom**[Prince]*[=]*[x]*)
    and *P*(*Kingdom*|*Prince*=*x*) using TF-IDF, where *x* is the Prince taking the
    army to battle. The observational values are flat because of little variation
    in the inferred King vignettes. The intervention enables more variation in the
    King vignettes and, consequently, the Kingdom vignettes.
  id: totrans-406
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.9使用TF-IDF可视化来自*P*(*Kingdom**[Prince]*[=]*[x]*)和*P*(*Kingdom*|*Prince*=*x*)的样本之间的差异，其中*x*是王子带领军队去战斗。由于推断的国王场景变化很小，观察值是平的。干预使国王场景有更多的变化，从而王国场景也有更多的变化。
- en: 13.3.4 Closing thoughts
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.4 结束语
- en: This is a toy problem with a simple DAG trained on simple data with simple LLMs.
    But we could extend it to more complicated DAGs and fine-tuning more advanced
    models. There may also be other ways to combine causal assumptions in foundation
    models. We’re only at the beginning of exploring this exciting space.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的玩具问题，使用简单的LLM在简单数据上训练了一个简单的DAG。但我们可以将其扩展到更复杂的DAG和微调更先进的模型。在基础模型中结合因果假设也可能有其他方法。我们只是探索这个令人兴奋空间的开始。
- en: Summary
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) are powerful AI models that generate text and other
    modalities and achieve high performance across a variety of benchmarks.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）是强大的AI模型，可以生成文本和其他模态，并在各种基准测试中实现高性能。
- en: LLMs have proven use cases for supporting causal analysis.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM在支持因果分析方面已经证明了其用途。
- en: LLMs can help build a causal DAG. Moreover, they can leverage common and expert
    knowledge about causal relations and mechanisms.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM可以帮助构建因果DAG。此外，它们可以利用关于因果关系和机制的共同和专业知识。
- en: The causal frame problem is the challenge of selecting the causal variables
    relevant to a given problem and excluding the irrelevant. Cutting-edge LLMs emulate
    how humans set the causal frame, which is useful for applications such as building
    DAGs and root cause analysis.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果框架问题是在给定问题中选择相关因果变量并排除无关变量的挑战。最前沿的LLM模拟人类设置因果框架的方式，这对于构建DAG和根本原因分析等应用很有用。
- en: LLMs can help us understand nuanced causal concepts and how to contextualize
    them within our domain of interest.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs可以帮助我们理解细微的因果关系概念以及如何在我们的兴趣领域内对其进行语境化。
- en: LLMs can help us put causal queries into formal terms.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs可以帮助我们将因果关系查询转化为形式化术语。
- en: LLMs are prone to hallucinations—convincing yet incorrect responses to our queries.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs容易产生幻觉——对我们查询的令人信服但错误的响应。
- en: At their core, LLMs are probabilistic machine learning models that model a joint
    probability distribution on sequences of tokens.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其核心，LLMs是概率机器学习模型，它们对标记序列的联合概率分布进行建模。
- en: The attention mechanism enables the LLM to learn higher-level representations
    that make cutting-edge LLMs so powerful.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制使得LLMs能够学习高级表示，这使得最前沿的LLMs如此强大。
- en: Just because an LLM learns a higher-level representation doesn’t mean it learns
    a causal representation. Even if that did work in some special cases, it would
    be hard for the user to verify that it is working.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使LLMs学习到了高级表示，并不意味着它学习到了因果关系表示。即使在某些特殊情况下这确实有效，用户也很难验证它是否在起作用。
- en: We can build our own causal LLM by composing fine-tuned LLMs over a causal DAG
    scaffold. This allows us to work with cutting-edge LLMs while admitting causal
    operations, such as a do-operator.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过在因果DAG框架上组合微调的LLMs来构建自己的因果LLMs。这允许我们在承认因果操作，如do运算符的同时，使用最前沿的LLMs。
- en: Use the causal hierarchy theory as your North Star in your exploration of how
    to combine causality with LLMs and multimodal models, as well as exploring how
    well these models can learn causal representations on their own.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将因果层次理论作为你在探索如何将因果关系与LLMs和多模态模型相结合的指南星，同时探索这些模型在自身学习因果关系表示方面的能力。
- en: '[[1]](#footnote-source-1) C. Glymour, “Android epistemology and the frame problem,”
    in Z.W. Pylyshyn, ed., *The robot’s dilemma: The frame problem in artificial intelligence*
    (Praeger, 1987), pp. 63–75\.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#footnote-source-1) C. Glymour, “Android epistemology and the frame problem,”
    in Z.W. Pylyshyn, ed., *《机器人的困境：人工智能中的框架问题》* (Praeger, 1987), pp. 63–75。'
