- en: 13 Causality and large language models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using causal information in LLMs to enhance a causal analysis
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting the components of an LLM to a causal ideas
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a causal LLM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large language models (LLMs) represent a significant advancement in the field
    of artificial intelligence. These models are large neural networks designed to
    generate and understand human-readable text. They are “large” because their scale
    is truly impressive—cutting-edge LLMs have parameters numbering in the billions
    and trillions. As generative models, their main function is to generate coherent
    and contextually relevant natural language. They can also generate structured
    text, such as programming code, markup languages, mathematical symbols, database
    queries, and many other useful things in text form.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are just one example of a broad class of generative AI. For example, we
    can use the neural network architecture underlying cutting-edge LLMs to model
    other sequences, such as time-series or DNA. LLMs are a type of *foundation model*,
    meaning large-scale models that serve as a base or foundation upon which more
    specialized models or applications can be built. Some LLMs are multimodal, meaning
    they work with text as well as other content modalities, such as images. In this
    chapter, we’ll focus specifically on LLMs, but much of what we discuss can be
    generalized to these related ideas.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: To start, let’s explore some use cases for using LLMs to enhance a causal analysis.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 13.1 LLMs as a causal knowledgebase
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I’ve mentioned several times in this book, in causal modeling, we don’t model
    the data, we model the data generating process (DGP). In other words, causal modeling
    requires causal information about the DGP that we can use as assumptions that
    enable identification of our causal queries. Cutting-edge LLMs are excellent resources
    for sourcing and validating that causal information. In this section, I’ll demonstrate
    some proven use cases for using LLMs to query causal information that augments
    a causal analysis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The following results come from OpenAI’s LLM GPT-4 via the ChatGPT interface.
    Because of the random generation from the model, and OpenAI’s periodic updates
    to the model and ChatGPT software, your results will vary from mine to some degree.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.1 Building a causal DAG
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start with using the LLM to augment the first workflow we discussed in
    this book: building the causal DAG. Specifying assumptions in the form of a causal
    DAG can be daunting. What if your DAG is wrong? Modern LLMs are invaluable as
    a way of leveraging public expert knowledge to build a DAG.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**You are a helpful causal analysis assistant.
    I’m interested in modeling the relationship between smoking and lung cancer. Provide
    me a causal DAG that captures this causal relationship.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '****![image](../Images/Response.png)****Certainly! A Directed Acyclic Graph
    (DAG) is a powerful tool to represent causal relationships. Here’s a basic causal
    DAG to represent the relationship between smoking and lung cancer:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Nodes:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S**: Smoking behavior (e.g., smoker, non-smoker, or frequency of smoking)'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LC**: Lung Cancer (presence or absence)'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Directed Edge:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A directed edge from **S** to **LC** represents that smoking behavior can cause
    or influence the occurrence of lung cancer.
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Potential Confounders:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Other nodes can be introduced into the DAG to account for potential confounders.
    Some potential confounders might be:'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**G**: Genetics (a family history of lung cancer)'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A**: Age (older age might be associated with a higher risk)'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**E**: Environmental exposure (like asbestos, pollution, etc.)'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**O**: Other health behaviors (like diet, exercise)'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Potential Mediators:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mediators are intermediate variables through which a cause exerts its effect
    on an outcome.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, one could hypothesize that smoking might cause lung damage, which
    in turn causes lung cancer. In this case:'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LD**: Lung Damage'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The DAG might look like this with the confounders and the mediator:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '**S** <- **G** –> **LC**'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S** <- **A** –> **LC**'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S** <- **E** –> **LC**'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S** <- **O** –> **LC**'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S** –> **LC**'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S** –> **LD** –> **LC**'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the process of modeling, it’s important to:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify all potential confounders** to avoid biased results. **Remember
    that correlation does not imply causation**. Just because two nodes are correlated
    does not mean one causes the other.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validate your DAG with domain experts**. They can provide insights into relationships
    you might not have considered.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use data** to test and validate the causal relationships in your DAG, e.g.,
    through controlled experiments or statistical methods like instrumental variables
    or propensity score matching when experiments are not feasible.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bear in mind that this is a simplified DAG for the relationship between smoking
    and lung cancer. Depending on the granularity and depth of your study, you might
    need to adjust and expand this framework.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuing, I can prompt the LLM to provide potential instrumental variables.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**Suggest two good candidates for instrumental
    variables. Be brief in your response.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Certainly. For the relationship between
    smoking and lung cancer, two potential instrumental variables (IVs) could be:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '**Cigarette Tax or Price**: If certain regions or periods have higher cigarette
    prices or taxes, this can influence smoking behavior without directly influencing
    lung cancer rates except through the effect of reduced smoking. The assumption
    here is that the only way cigarette prices could affect lung cancer is by influencing
    smoking behavior.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Anti-Smoking Campaigns or Legislation**: Introduction or intensity of anti-smoking
    campaigns or legislation in certain regions or times can be used as an instrument.
    If these campaigns or laws are exogenously determined (not based on current lung
    cancer rates), they can affect smoking rates without having a direct effect on
    lung cancer, except through their impact on smoking behavior.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For both IVs, it’s crucial to validate the two key assumptions of instrument
    validity:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The instrument is related to the exposure (e.g., cigarette tax affects smoking
    behavior).
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The instrument affects the outcome (e.g., lung cancer) only through its effect
    on the exposure and not through other pathways.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prompting best practice
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The prompt preface “You are a helpful causal analysis assistant” is an example
    of what is sometimes called a “system prompt.” System prompts provide general
    context about what is expected from the LLM. System prompting, and other prompt
    engineering techniques (e.g., chain-of-thought prompting, in-context learning)
    are best practices for getting good generations from LLMs. For the sake of brevity,
    I’ll use system prompts but omit some of the other prompt engineering best practices
    in this section—it is smart to include them in practical settings.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Continuing to interact with the LLM in this way, perhaps through multiple sessions,
    you can come up with a candidate DAG that makes sense to you and your stakeholders.
    Next, you can prompt the LLM to give you code to run an analysis.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.2 Generating code for DAGs, models, and causal analyses
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern LLMs are trained on vast corpora of code from a wide variety of programming
    languages. This greatly speeds up causal analyses, particularly if the libraries
    you’re using are represented in the training data. For example, we can prompt
    ChatGPT to implement its suggested lung cancer DAG in DoWhy and estimate the causal
    effect.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**Provide me with code that uses the DoWhy library
    in Python to implement the causal DAG that you suggested above. Include cigarette
    price as an instrumental variable and provide code for estimating the causal effect.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Certainly! Here’s a basic example using
    the DoWhy library in Python for implementing the causal DAG for the relationship
    between smoking and lung cancer, using cigarette price as an instrumental variable.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The code in this chapter was written with DoWhy version 0.11, Transformers version
    4.38.2, Accelerate version 0.28.0, pandas 2.0.3, NumPy 1.25.2, and pyro-ppl (Pyro)
    version 1.9.0\. Visit the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for a link to a notebook containing the chapter’s code.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Chat GTP then went on to generate the following code.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.1 DoWhy code generated by ChatGPT (verbatim)
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 This line is wrong, but the code runs if you remove it.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Simulate smoker status: 0 for non-smoker, 1 for smoker.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Simulate lung cancer status: 0 for no lung cancer, 1 for lung cancer.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Simulate cigarette price with a mean price of 5.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**This is a simplified example. In a real-world
    application:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Your data would be imported rather than simulated as above.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that your DAG correctly models the relationships based on domain knowledge.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assumptions behind the instrumental variables need to be rigorously checked.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further analytical steps (e.g., statistical tests, checking robustness) would
    be necessary.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you generate code, you may get errors. For example, DoWhy has gone through
    several iterations, and there is nothing forcing the LLM to keep up with the latest
    version of the library. In this case, there is an error in the line `from dowhy.do_why
    import CausalModel`. There is no module called `do_why`, and `CausalModel` is
    already imported in a previous line. We’ll address errors produced by the LLM
    in section 13.1.7\. For now, if you delete that line, the code will run.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT will work well with pgmpy and Pyro, the libraries we’ve worked with
    extensively in the book. As an exercise, I suggest taking one of the models you
    built in pgmpy, and prompting the LLM to convert it to a model in Pyro.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at what the LLM can tell us beyond the structure of the DAG.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.3 Explanations and mechanism
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The advantage of DAGs is that graphs are an intuitive way to represent what
    causes what. But they do not tell us anything about the nature of the mechanism
    by which a cause drives an effect.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Structural causal models provide a mathematical way of characterizing the mechanism
    underlying a causal relationship. Characterizing mechanisms in mathematical terms
    makes sense in natural sciences, where we have plenty of equations describing
    natural laws. It is harder in, for example, business domains where we lack equations
    beyond mere approximations (what is the equation relating ad spending and engagement?).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: LLMs leverage common and expert knowledge to provide mechanistic explanations
    that go beyond the DAG but that are in a form more accessible than formal math.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**You are a helpful causal explanation generator.
    Provide a characterization of the causal relationship between ad spending and
    brand awareness. Be concise enough to fit a single slide in a slide deck.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Causal Relationship Between Ad Spending
    and Brand Awareness'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct Causality**: As ad spending increases, brand awareness typically rises.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mechanism**: Larger ad spends mean more frequent and widespread exposure
    of the brand to the target audience.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Diminishing Returns**: After a certain point, additional ad spending might
    not lead to proportionally higher brand awareness.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reason**: Market saturation, ad fatigue, or redundancy in audience reach.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**External Factors**: Ad efficiency isn’t solely based on spend.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Influences**: Ad quality, placement, target audience relevancy, competition,
    and market conditions can affect the outcome.'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider that some counterfactual queries can be identified, with simple categorical
    assumptions about the functional relationship between the cause and effect. For
    example, the probability of necessity is identifiable from intervention data and
    partially identifiable from observational data if the relationship between *X*
    and *Y* is monotonic:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**Is this a monotonic relationship? Limit your
    answer to one sentence.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Yes, the relationship between ad spending
    and brand awareness is generally monotonic, as increased ad spending will not
    decrease brand awareness, though the rate of increase might diminish.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: We needn’t limit ourselves to one explanation; we can use the LLM as a generator
    of explanatory hypotheses. Consider, for example*, abductive reasoning tasks*
    where one infers likely causes of a particular observed set of effects (e.g.,
    in science, criminal investigation, and medical diagnosis).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, let’s pose a modified version of a medical diagnostic question
    sourced from the US Medical License Exam to ChatGPT. I’ll prompt for multiple
    hypotheses about the cause of the observed symptoms, which is what a doctor would
    call a *differential diagnosis*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**You are a causal explanation assistant. Your
    job is to generate 3 one sentence-long hypotheses that explain the observations.
    This is a medical diagnosis example.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'Observations: A 48-year-old woman presents with diplopia, ocular pain, amenorrhea,
    hot flashes, weight loss, tachycardia, hypertension, moist palms, enlarged thyroid,
    prominent eyes, lid retraction, conjunctival injection, and inability to converge
    eyes. Visual acuity is 20/20 bilaterally.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)** The patient may have Graves’ disease,
    an autoimmune disorder affecting the thyroid, leading to hyperthyroidism and its
    associated symptoms, including ocular manifestations.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. The woman’s symptoms, including amenorrhea and hot flashes, suggest a possible
    concomitant early menopause or another endocrine disorder affecting her reproductive
    hormones.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. The ocular symptoms, including diplopia, lid retraction, and inability to
    converge eyes, hint at a potential thyroid eye disease or orbital myopathy.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the challenges of causal modeling is working with domain experts to encode
    domain assumptions into the structure of the model. These types of generated explanations
    and hypotheses are valuable assets for communicating with domain experts who might
    agree with, or better yet, iterate upon these generated explanations.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.4 The causal frame problem and AI alignment
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given a query of interest, we have to decide what variables are relevant to
    the problem. Philosoper and AI researcher Clark Glymour does a good job of describing
    the challenge behind this task: “Given an enormous amount of stuff, and some task
    to be done using some of the stuff, what is the relevant stuff for the task?”[¹](#footnote-305)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个感兴趣的查询，我们必须决定哪些变量与问题相关。哲学家和人工智能研究员 Clark Glymour 很好地描述了这项任务背后的挑战：“给定大量的东西，以及使用其中一些东西来完成某个任务，对于这个任务来说，什么是相关的？”[¹](#footnote-305)
- en: LLMs are useful at establishing a *causal frame*—the problem of selecting the
    causal variables relevant to a causal query. In chapter 3’s discussion on building
    the DAG, I mentioned *causal sufficiency* as a d-separation–based criteria for
    selecting variables for inclusion in the DAG. But when we prompted the LLM to
    provide a DAG containing variables relevant to smoking’s effect on lung cancer,
    it is highly doubtful that the LLM was using causal sufficiency to select those
    variables. More likely, it was using common knowledge found in its training data
    about this relationship. That knowledge was sufficient to provide a reasonably
    minimal set of relevant variables.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在建立**因果关系框架**方面很有用——选择与因果查询相关的因果变量的问题。在第3章关于构建 DAG 的讨论中，我提到了**因果充分性**作为基于
    d-separation 的标准，用于选择包含在 DAG 中的变量。但当我们提示 LLM 提供一个包含与吸烟对肺癌影响相关的变量的 DAG 时，我们高度怀疑
    LLM 是否在使用因果充分性来选择这些变量。更有可能的是，它正在使用其训练数据中关于这种关系的常识。这种知识足以提供一组合理的最小相关变量集。
- en: 'To illustrate the utility of this ability, consider the following ChatGPT interaction:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种能力的实用性，考虑以下 ChatGPT 交互：
- en: '**![image](../Images/init.png)**You are a helpful causal reasoning assistant.
    Consider the following story.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**您是一个有用的因果推理助手。考虑以下故事。'
- en: Jake is on an office network. There was an open shared folder on the server.
    Jake is transferring files across the network and accidentally clicks on the folder,
    starting a process that consumes a significant amount of bandwidth. This causes
    the network to slow down. How responsible is Jake for the network slowdown?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Jake 在一个办公室网络中。服务器上有一个开放的共享文件夹。Jake 正在通过网络传输文件，并意外点击了文件夹，启动了一个消耗大量带宽的过程。这导致网络变慢。Jake
    对网络减速有多大的责任？
- en: '**![image](../Images/Response.png)**Based on the provided story, let’s analyze
    the situation using causal reasoning:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**基于提供的故事，让我们使用因果推理来分析这种情况：'
- en: '**Direct Cause**: Jake started a process that consumes a significant amount
    of bandwidth by accidentally clicking on a file.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**直接原因**：Jake 通过意外点击一个文件启动了一个消耗大量带宽的过程。'
- en: '**Effect**: The network slowed down due to the consumed bandwidth.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**影响**：由于消耗了带宽，网络变慢了。'
- en: Given these facts, we can establish that Jake’s action was the direct cause
    of the network slowdown.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些事实，我们可以确定 Jake 的行为是网络减速的直接原因。
- en: 'However, responsibility can be considered from different dimensions:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可以从不同的维度来考虑责任：
- en: '**Intent**: Jake’s action was accidental. He did not intend to slow down the
    network.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**意图**：Jake 的行为是意外的。他并没有意图减慢网络。'
- en: '**Prevention Mechanisms**: The story does not specify if there were any warnings
    or user prompts before starting such a process that consumes significant bandwidth.
    If there were no warnings, one could argue that the system was not designed to
    prevent such accidental actions.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预防机制**：故事没有说明在启动这种消耗大量带宽的过程之前是否有任何警告或用户提示。如果没有警告，有人可能会认为系统没有设计来防止这种意外行为。'
- en: '**Knowledge & Training**: Was Jake trained or informed about the implications
    of transferring large files or about the particular file that consumes a lot of
    bandwidth? If not, his knowledge gap contributed to the accident.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**知识与培训**：Jake 是否接受了关于传输大文件或关于消耗大量带宽的特定文件的培训或通知？如果没有，他的知识差距导致了这次事故。'
- en: '**System Design**: The open shared folder on the server was accessible to Jake.
    If the file was known to be a bandwidth hog, it’s worth asking why such a file
    was placed in an open shared folder without any warnings or restrictions.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**系统设计**：服务器上的开放共享文件夹对 Jake 可访问。如果文件已知是带宽消耗者，那么值得问的是为什么这样的文件会被放在一个没有任何警告或限制的开放共享文件夹中。'
- en: 'To directly answer the question: Jake is responsible for the network slowdown
    in the sense that his action was the direct cause. However, the degree of his
    responsibility might be mitigated if there were systemic design flaws, lack of
    training, or absence of preventive measures. If other factors, like the ones mentioned
    above, played a role in the incident, then the responsibility would be shared
    among Jake and those other factors.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Here, the response includes the systemic design flaws, lack of training, or
    absence of preventive measures as factors relevant to the causal frame, despite
    the fact that I made no mention of these factors in the prompt.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'More importantly, consider the many factors the LLM could have included but
    did not: perhaps Jake’s mouse had a physical glitch that caused him to click on
    the file, or perhaps a hacker removed security features on that file, or any other
    possible mitigating factors. A human would likely exclude these from the causal
    frame unless explicitly mentioned, and so does the LLM.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'If humans are already good at establishing the causal frame, why would we care
    whether the LLM can do so? Consider the problem of root cause analysis from network
    logs. Suppose the same story about Jake were rewritten in the form of network
    logs, such as in the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is harder for a human to parse than the natural language story, even for
    an experienced network administrator. Further, there would likely be many other
    network events unrelated to the events of this narrative that the human would
    have to read through and disregard. But the LLM has plenty of network logs in
    its training data, and extracting relevant events from a longer body of text is
    an established capability of LLMs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: In most of this book, we’ve focused on using causal models to infer objective
    causal truth about the world, such as whether smoking causes lung cancer. In this
    example, the objective truth, that Jake caused the network slowdown, is trivial.
    The question of how responsible Jake is for the slowdown is highly subjective.
    But this is still a valuable question to answer, especially if you are trying
    to find a root cause in a large network log file.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '*AI alignment* is the task of making sure an AI system’s behaviors are consistent
    with human goals and preferences. Here, we want a specific type of AI alignment.
    We want the LLM to go beyond confirming obvious causal facts to aligning with
    the ways humans tend to make causal judgments. In this case, I asked the LLM an
    open question about Jake’s responsibility, and it gave a response that many humans
    would agree with. I could also have instructed the LLM to follow certain criteria
    when generating a judgement. For example, an attorney could prompt the LLM to
    apply certain legal theories and case law in generating a legal opinion.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at how to use the LLM to understand and contextualize more
    nuanced causal concepts.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.5 Understanding and contextualizing causal concepts
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It can be challenging to understand formal causal concepts and apply them within
    your domain. The LLM can serve as a tool for explaining and contextualizing these
    concepts. Let’s illustrate with the counterfactual concept of *necessary causation*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 理解正式的因果概念并在你的领域内应用它们可能会具有挑战性。LLM可以作为解释和情境化这些概念的工具。让我们以*必要因果关系*的反事实概念为例进行说明。
- en: '**![image](../Images/init.png)**You are a helpful explainer of causal inference
    topics. Provide a one paragraph explanation of the concept of necessary causation.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**你是一位有益的因果推理主题解释者。请提供一段关于必要因果关系的概念解释。'
- en: '**![image](../Images/Response.png)**Certainly! Necessary causation refers to
    a causal relationship where the cause is a necessary condition for the effect
    to occur. In other words, without the presence of the cause, the effect cannot
    happen. However, the presence of the cause does not guarantee the effect will
    occur, as there may be other conditions that also need to be met. Think of it
    like a key to a lock: the key (cause) is necessary to open the lock (effect),
    but just having the key doesn’t mean the lock will open automatically; you still
    need to insert and turn it. If the lock remains locked without the key, then the
    key is a necessary cause for unlocking.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**当然！必要因果关系指的是一种因果关系，其中原因的存在是结果发生的必要条件。换句话说，如果没有原因的存在，结果就不会发生。然而，原因的存在并不保证结果会发生，因为可能还有其他条件需要满足。把它想象成一把钥匙开一把锁：钥匙（原因）是打开锁（结果）的必要条件，但仅仅拥有钥匙并不意味着锁会自动打开；你仍然需要插入并转动它。如果锁没有钥匙仍然锁着，那么钥匙就是解锁的必要原因。'
- en: A cutting-edge LLM would be trained on a wide variety of theory and research
    from across domains. It can be useful to contextualize an idea within various
    domains.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个前沿的LLM会在跨领域的各种理论和研究中进行训练。将一个想法置于各种领域中进行情境化可能是有用的。
- en: '**![image](../Images/init.png)**In one paragraph, explain how necessary causation
    is conceptualized in legal theory.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**在一句话中，解释在法律理论中如何概念化必要因果关系。'
- en: '**![image](../Images/Response.png)**In legal theory, necessary causation, often
    termed “but-for causation,” is a foundational concept used to establish a direct
    link between a defendant’s actions and the harm suffered by a plaintiff. For an
    action to be a but-for cause of an outcome, the outcome would not have occurred
    in the absence of the action. In other words, if one can say “but for the defendant’s
    action, the harm would not have occurred,” then necessary causation is established.
    However, it’s worth noting that while but-for causation is a critical component
    in many legal contexts, it’s not always sufficient on its own to establish liability.
    Other factors, such as foreseeability and proximate cause, often come into play
    to determine the full scope of legal responsibility.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**在法律理论中，必要因果关系，通常被称为“若无因果关系”，是一个用于建立被告行为与原告所受损害之间直接联系的基础性概念。一个行为要成为导致结果的“若无”原因，如果没有这个行为，结果就不会发生。换句话说，如果可以说“若非被告的行为，损害就不会发生”，那么必要因果关系就成立了。然而，值得注意的是，尽管“若无”因果关系在许多法律情境中是一个关键组成部分，但它本身并不总是足以确立责任。其他因素，如可预见性和近因，通常也会发挥作用，以确定法律责任的全部范围。'
- en: '**![image](../Images/init.png)**From the perspective of cognitive science,
    explain in one brief paragraph the role necessary causation plays in human judgments.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/init.png)**从认知科学的角度来看，用一句话简要说明必要因果关系在人类判断中的作用。'
- en: '**![image](../Images/Response.png)**From a cognitive science perspective, necessary
    causation plays a pivotal role in human judgments and decision-making. Our brains
    are wired to seek patterns and causal relationships in the environment to predict
    and navigate future events. When we perceive an event as a necessary cause of
    another, it simplifies our cognitive processing by creating a deterministic link
    between the two events. This means that when we encounter the cause, we can anticipate
    the effect, allowing for more efficient planning and response. However, this predisposition
    can also lead to cognitive biases, where we might mistakenly attribute an effect
    to a perceived necessary cause, even when other contributing factors are at play.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**From the perspective of philosophy, explain
    in only one sentence how Kant and Hume differed in their beliefs about necessary
    causation.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Kant believed in the existence of a priori
    synthetic knowledge and that our minds impose causal order on our experiences,
    while Hume argued that causation is not a feature of the world itself but a habit
    of thought based on the regular succession of events.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve unpacked the concept of necessary causation, let’s apply it
    to a practical problem in our domain of interest. Suppose you are a data scientist
    at Netflix:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**Give me an example of necessary causation that
    involves a Netflix user unsubscribing from Netflix.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**A necessary causation for a Netflix user
    unsubscribing from Netflix might be the exclusive content that is only available
    on the platform. If a particular show or movie that the user is deeply interested
    in is removed from Netflix while remaining available somewhere else, then the
    removal of that content becomes a necessary cause for that user’s decision to
    unsubscribe. Without the removal of that content, the user might not have unsubscribed.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Now that the LLM has helped us understand necessary causation, we can get its
    help in formalizing the concept.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.6 Formalization of causal queries
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For humans, natural language is the medium of our causal reasoning. But we need
    to convert that natural language into formal terms so that we can apply mathematical
    and algorithmic analysis. This can be a challenge. Fortunately, LLMs can help
    greatly in this respect.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in chapter 8, I introduced several examples of the language of
    parallel world counterfactuals and how to convert them into counterfactual notation.
    Necessary causation, more specifically the *probability of necessity*,was one
    of these counterfactuals. Let’s put this task to the LLM in our Netflix context:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**You are a helpful causal inference assistant.
    I’m considering a counterfactual inference problem in the context of Netflix.
    I have the following query: “There is a set of users who unsubscribed after content
    was removed from the platform. What is the probability a given member of this
    set of users would not have unsubscribed had the content not been removed?” Write
    this query as a probabilistic expression with Pearl’s counterfactual notation.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Certainly! Pearl’s counterfactual notation
    offers a concise way to express such queries.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define some variables:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '*X* =removal of the content (binary, 1 if removed, 0 if not removed).'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Y* =whether user unsubscribes (binary, 1 if user unsubscribes, 0 if they did
    not unsubscribe).'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your query is essentially asking for the probability that a user would not have
    subscribed, had the content not been removed, given that it was removed and they
    did unsubscribe.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Pearl’s counterfactual notation, this can be written as:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch13-eqs-0x.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '*Y*[*x*][=0] is the counterfactual outcome for *Y* had *X* been set to 0 (i.e.,
    the content is not removed).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X*=1 denotes that the content was removed.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Y*=1 denotes that the user unsubscribed.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, *P*(*Y**[x=]*[0] = 0| *X* = 1, *Y* = 1) is the probability that a user would
    not have unsubscribed had the content not been removed, given that it was removed
    and they did unsubscribe.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: The LLM can take us from conceptualization to formalization. But beware, as
    it can also “hallucinate.”
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '13.1.7 Beware: LLMs hallucinate'
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen that LLMs can aid in code generation. Let’s ask for code that implements
    the probability of necessity.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**Provide Python code that, for a given pair
    of treatment and outcome variables, calculates the counterfactual query “probability
    of necessity.”'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**The Probability of Necessity (PN) is the
    proportion of cases where the treatment is present, and the outcome is achieved
    ONLY when the treatment is present. In simpler terms, it indicates the probability
    that the outcome occurs because of the treatment.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that definition, you can calculate PN using the following formula:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch13-eqs-1x.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*Treatment*∩*Outcome*) is the probability of both the treatment and outcome
    occurring.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(¬*Treatment*∩*Outcome*) is the probability of the outcome occurring but
    not the treatment.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The response goes on to provide Python code that is well formatted, well commented,
    and compiles. *But this answer is wrong*. This is what is commonly called a “hallucination,”
    a convincing lie. This example is especially convincing—that formula is true *only*
    when the relationship between the treatment and outcome is monotonic, when there
    are no confounders, and when *P*(*Treatment*) = *P*(*Outcome*). Those assumptions
    are so strong that we can’t forgive ChatGPT for not mentioning them. Other generations
    from the model were even more off base.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: We can use the LLM to augment our causal analysis, particularly with common
    and expert causal knowledge as well as generated code. But hallucination means
    this isn’t a panacea. We need to rely on our own causal expertise to spot when
    hallucination occurs and understand when it threatens the quality of our analysis.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: To understand why this hallucination occurred, let’s first examine how LLMs
    work.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 13.2 A causality-themed LLM primer
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand how to deploy LLMs for causal applications, it is important to
    understand how they work, as well as their limitations. This section provides
    a quick high-level causality-themed tour of the core ideas.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.1 A probabilistic ML view of LLMs
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of LLMs, a “token” refers to a sequence of characters that the
    model reads, which can be as short as one character or as long as one word. Tokens
    are the units into which input text is divided into manageable pieces for the
    model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face’s Transformers library has a publicly available version of GPT-2,
    which is far inferior to cutting-edge models but has a similar *Transformer* architecture.
    The Transformer architecture is a type of deep learning model designed to process
    and understand text and other sequential data, by focusing on the relationships
    between words in a sentence regardless of their position. Let’s tokenize the expression
    “Can LLMs reason counterfactually?”
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.2 Viewing example tokens that an LLM operates upon
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Initialize the GPT-2 tokenizer.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Tokenize the sequence.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Print out the tokens.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'This prints out the following tokens:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The “Ġ” corresponds to a space. Note that punctuation marks are tokens, and
    that words like “counterfactual” are broken up into multiple tokens. Each token
    corresponds to an integer indexing the token in a large “vocabulary.” GPT-2 has
    a vocabulary size of 50,257.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.3 Converting tokens to integers
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 “Encode” the tokens into integers that index the token in a list of tokens
    called the “vocabulary”.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'This *encodes* the tokens into a sequence of integers:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The Transformer architecture works with these numeric values.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs define a joint probability distribution on sequences of tokens. For the
    phrase “Can LLMs reason counterfactually?” the model defines a probability distribution:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch13-eqs-2x.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: 'The models will also consider the chances that this sequence ended at the question
    mark, rather than continuing. For that, the LLM’s vocabulary includes a special
    token to mark the end of a sequence. For GPT-2, this is token is `<|endoftext|>`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch13-eqs-3x.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 'Further, autoregressive LLMs, such as the GPT and Llama series of Transformer
    models, model text in the order of the text sequence, so they factorize this joint
    probability as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch13-eqs-4x.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: We can calculate each of these probabilities on the log scale with the Transformers
    library. In generating the log probability, we first calculate *logits* for each
    term in the vocabulary. For a probability value *p*, the corresponding logit is
    log(*p* / (1–*p*)).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.4 Calculate the log probability of each token in the sequence
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Initialize the GPT-2 model and set to evaluation mode.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Tokenize and encode the phrase, including the end-of-sequence token.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Given the phrase, the model produces logits for every element in the vocabulary.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '#4 For each position in the sequence, get the log probability corresponding
    to the token that was actually present in that position.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'This prints the following output:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Summing these together provides the joint probability of the sequence under
    the model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Of course, as a generative model, GPT-2 can generate the next token conditional
    on the tokens that came before it. The *prompt* the user provides is the beginning
    of the sequence, and the model’s response extends the sequence.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.5 Generation from the LLM
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Specify and encode the prompt.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Generate from the model. The “do_sample=True” argument means we’re doing
    random selection from the probability distribution of the next token, given all
    the previous tokens.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Decode and print the output.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'This prints out the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Again, note that ChatGPT generation has a random element, so this will likely
    produce something different for you.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Note about confusing terminology
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Models like the GPT models are often called “causal language models,” but these
    are not causal models in the way we’ve discussed in this book. They are not a
    causal model of a DGP. “Causal” here refers to the autoregressive nature of the
    model—the model evaluates the probability of a token in a sequence conditional
    only on the tokens that came before it.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: All this is to say that the LLM is at a basic level a probability model of the
    joint probability of a sequence of tokens. The canonical training procedures for
    these models attempt to fit the joint probability distribution of the tokens.
    Models like GPT optimize the model’s ability to predict a given token in a training
    document given the previous tokens. Understanding that the LLM is a probability
    model over tokens doesn’t explain why LLMs can generate *coherent* text (meaning
    text with logical and consistent interrelation of ideas that forms a comprehensible
    whole). For that, we need to understand *attention*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.2 The attention mechanism
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the main drivers behind the success of LLMs is use of *Transformer architectures*
    and other neural network architectures that rely on a mechanism called *attention*.
    The attention mechanism allows the model to weigh the importance of different
    parts of an input sequence differently. That allows the model to learn to “focus”
    on specific parts of a sequence that are more relevant to a given task, while
    “ignoring” or assigning lesser weight to less pertinent parts.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider, for example, the following conditional counterfactual statement about
    leaves being a necessary cause of a fire:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: The Labor Day weekend wildfire started in the forest and spread rapidly due
    to the dry leaves on the ground. Had there been a controlled burn, the fire wouldn’t
    have spread so rapidly.
  id: totrans-215
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The attention mechanism helps the model recognize that “leaves” refers to foliage,
    not a departure, by weighing the relevance of surrounding words like “ground,”
    “dry,” and “forest.”
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Modern LLMs have attention mechanisms stacked over many neural network layers.
    This enables the LLM to attend to concepts at different levels of granularity.
    For example, while the first layer of attention focuses on immediate word-to-word
    relationships, such as “leaves” with “ground,” the next few layers connect broader
    phrases, treating “The Labor Day weekend wildfire” as a single entity connected
    to the phrase “spread rapidly.”
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Latter layers can represent the overarching theme or subject of the sentence
    and the broader text, connecting “The Labor Day weekend wildfire” to information
    about how it spread.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.3 From tokens to causal representation
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ability to talk about how attention enables the LLM to learn higher-level
    abstractions becomes of interest to us from the standpoint of causality. Recall
    figure 13.1, which first appeared in chapter 5 (as figure 5.4).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F01_Ness.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 Example from chapter 5, where *digit* and *is-handwritten* are high-level
    causal drivers of low-level *X**[i]* pixels
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The small squares represent pixels in an image, while the squares *digit* and
    *is-handwritten* represent the digit depicted in the image and whether it was
    handwritten, respectively. In that example (section 5.1.2), I suggested that whatever
    causal relations exist between individual pixels doesn’t matter to us; we’re interested
    in reasoning at the level of the objects depicted in the image.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: There is a similar thing going on with tokens, as shown in figure 13.2.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F02_Ness.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 *X*[1] through *X*[12] are a sequence of tokens. Whatever structure
    (causal or otherwise) exists between the tokens is of passing interest. We are
    interested in the causal relations between concepts described by the tokens.
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In figure 13.2, as with the pixels, there is some structure at the level of
    the tokens. But that structure is beside the point—we’re interested in the causal
    relationships between the concepts that form the meaning behind the tokens.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The question becomes, under what circumstances could attention, insofar as it
    can learn higher-level representations, learn a *causal* representation. For example,
    could an attention-based model, perhaps under some set of architectural or learning
    constraints, or use of interventions in training data, learn the parallel world
    structure and abstractions in the Labor Day counterfactual statement?
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: To consider this, we can revisit LLM hallucinations in our question about the
    probability of necessity, and connect it to causal identification.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.4 Hallucination, attention, and causal identification
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The hallucination about probability of necessity was generated with GPT-4\.
    The same model got the Netflix question about probability of necessity correct.
    Indeed, the hallucinated answer would have been right if it merely stated the
    correct identifying assumptions. I believe future versions of GPT and similar
    models will likely get this question right on the first try.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: But for someone who is unfamiliar with the definition of probability of necessity,
    how would they know if the model were right or if it were hallucinating? First,
    the causal hierarchy tells us that in order to be capable of generating a right
    answer beyond a random guess, the query would need to be identified with level
    3 information. Perhaps that information is provided by the user in the prompt.
    Perhaps the LLM has somehow learned level 3 representations (such a claim would
    require hard proof).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: If the user were providing that identifying information in the prompt, how would
    the user know if the model was successfully using that information to respond
    to the prompt? Suppose instead that the requirements for identification exist
    and are buried in the learned representations or data, and that they were being
    successfully leveraged by the model in answering the causal query, how could the
    user know for sure this was happening?
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: We need to engineer solutions that answer these and other desiderata to build
    toward a causal AI future. In the next section, we’ll start on this path with
    a simple causal LLM.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 13.3 Forging your own causal LLM
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll sidestep the question of “can cutting-edge LLMs reason
    causally?” and move on to building a causal LLM that can reason causally. We’ll
    build for causality from the ground up, rather than as an afterthought.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.1 An LLM for script writing
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often our data has some implicit causal structure. When we make that structure
    explicit during training, the foundation model can learn better causal representations.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, suppose a prolific film production studio has historically insisted
    their writers use script-writing software that required following a three-act
    narrative archetype, which is common for romantic comedies: “boy meets girl, boy
    loses girl, boy gets girl back.” For this archetype, they have a corpus of many
    scripts. In causal terms, the events in act 1 cause the events of act 2, and the
    events of acts 1 and 2 cause the events of act 3\. We can draw the DAG in figure
    13.3.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F03_Ness.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 A causal DAG for a three-act archetype
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The studio works with many such archetypes, and the company has many scripts
    that follow a given archetype template. Suppose that a set of archetypes involve
    a king acting a certain way in act 1, a prince acting a certain way in act 2,
    and these two actions having an effect on a kingdom in act 3\. For example, one
    possible archetype is “King declares war, Prince leads army, kingdom experiences
    prosperity.” But there are multiple outcomes for each act:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'King in act 1: {king declares war; king negotiates peace; king falls ill}'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prince in act 2: {prince leads army; prince abdicates throne; prince marries
    foreigner}'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingdom in act 3: {kingdom wins battle; kingdom falls into poverty; kingdom
    experiences prosperity}'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F04_Ness.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 A causal DAG representing various King-Prince-Kingdom archetypes
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 13.4 shows this space of archetypes in the form of a causal DAG.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: This describes only 3 × 3 × 3 = 27 possible archetypes, but as you might expect,
    some archetypes are more common and some are less common. We could easily model
    these archetypes and the joint probability distribution by explicitly coding causal
    Markov kernels in pgmpy or Pyro. But that would only be a causal generative model
    on the archetypes. If we want a script generator, we want a causal generative
    model on the scripts.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate a proof-of-concept for this idea, we’ll work with a training
    dataset of short vignettes, rather than full scripts. Let’s load and examine the
    training data.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.6 Load causal narrative data
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 The data has 21,000 stories, broken up into three short vignettes.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '#2 First, the king acts.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Then the prince acts.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Finally, the kingdom experiences the consequences of the royals’ actions.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'This code prints the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There are 21,000 sets of three vignettes. The preceding output shows the first
    three sets in the dataset.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.2 Using pretrained models for causal Markov kernels
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train the causal Markov kernels for each node in our DAG, we’ll take pretrained
    models from the Hugging Face Transformers library, and then further train (aka
    “fine-tune”) the models using our vignettes. The pretraining took care of the
    heavy lifting in terms of learning to generate coherent natural language text.
    The fine-tuning will align the models toward representing our causal Markov kernels.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll use a GPT-2 variant to model the King’s action vignettes. As a
    text-completion model, it typically takes a prompt as input, but we’ll train it
    to generate with an empty prompt and produce vignettes according to the marginal
    probabilities of the King’s action texts in the training data, as in figure 13.5.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F05_Ness.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 GPT-2 is fine-tuned to represent the distribution of King’s action
    vignettes.
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next, we’ll use a BART model for the causal Markov kernel Prince’s action. BART
    is a Transformer model released in 2019 designed specifically to take an input
    sequence and generate a corresponding output sequence, such as with translation
    or summarization. Large models like GPT-4 can handle sequence-to-sequence tasks
    quite well, but we’ll use a version of BART with roughly 4,000-times fewer parameters
    than GPT-4, making it easier for you to load and train on your laptop or basic
    Python development environment. Given the King’s action vignette as input, it
    will generate a Prince’s action vignette, as illustrated in figure 13.6.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also use a BART model to model the causal Markov kernel for the Kingdom’s
    fate, as shown in figure 13.7\. The model will map the King’s and Prince’s actions
    to the Kingdom’s fate.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F06_Ness.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 A BART sequence-to-sequence model is fine-tuned to represent the
    Prince’s action given the King’s action.
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F07_Ness.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 A BART sequence-to-sequence model is also used to model the Kingdom’s
    fate given the King’s action and Prince’s action.
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Jumping ahead, we’re interested in the conditional probability distribution
    of the Kingdom’s fate, given a certain action by the Prince. Since that will require
    inference of the King’s actions given the Prince, we’ll additionally train one
    more BART model that generates a King’s action vignette given a Prince’s action
    vignette, as shown in figure 13.8.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F08_Ness.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 A BART sequence-to-sequence model is also fine-tuned to model the
    Kingdom’s fate, given the King’s and Prince’s actions.
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s run the training procedure. First, we’ll set up our imports and our tokenizer.
    We’ll use Bart as the tokenizer for all of our models.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.7 Training the causal LLM
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Set up the tokenizer.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The pad token is used to make all the sequences the same length to facilitate
    matrix operations. It is common to set it to the “end-of-sequence (EOS)” token.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The max length of the token is set to 40, as all of the vignettes are less
    than 40 tokens.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Next we’ll create a class and a function that tokenizes the King dataset. We’ll
    create a custom subclass of the PyTorch `Dataset` class called `ModelDataset`
    that will store token encodings and their corresponding labels. When accessed
    by index, it returns a dictionary containing token encodings for that index and
    the associated label, and it provides the total number of examples via its `__len__`
    method.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.8 Tokenizing the King vignettes
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 When accessed by index, ModelDataset returns a dictionary containing token
    encodings for that index and the associated label.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Create a ModelDataset instance for the king vignettes.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Next we’ll tokenize the Prince and Kingdom vignettes. This code will also produce
    a validation dataset used in training sequence-to-sequence models.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.9 Tokenizing the Prince and Kingdom vignettes
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Split input and target phrases into training and validation sets.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Encode the training and validation sets.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll write a training algorithm for the King model. This function initializes
    a GPT-2 model with the specified parameters, sets up the training arguments, and
    trains the model on the provided dataset, finally saving the trained model to
    the specified directory.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.10 Training the King model
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Initialize and configure model with the specified parameters.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Configure the training settings.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Train the model.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll write a training algorithm for the sequence-to-sequence models.
    The function will split the provided input and target phrases into training and
    validation sets, tokenize them, and then create and return PyTorch `Dataset` objects
    for both sets using the `ModelDataset` class. The `train_seq2seq_model` function
    initializes a sequence-to-sequence model with the specified parameters, configures
    its training settings, and then trains the model using both training and validation
    datasets, finally returning the trained model.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.11 Function for training the sequence-to-sequence models
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 Initialize and configure sequence-to-sequence model with the specified parameters.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Configure the training settings.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Train the model using both training and validation datasets, finally returning
    the trained model.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll use this function to train the models. We’ll specify some directories
    for saving checkpoints.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Note  In listing 13.14, I’ll provide code that downloads a pretrained model
    from Hugging Face, so if you don’t wish train the model, you can skip ahead to
    that step.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.12 Training the King, Prince, and Kingdom models
  id: totrans-304
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Provide the output directories where you want to save your model.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Train the King model using Seq2Seq.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Train the Prince model using Seq2Seq. The King vignettes are used to predict
    the Prince vignettes.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Train the Prince model using Seq2Seq. The King vignettes are used to predict
    the Prince vignettes.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Train the Kingdom model using Seq2Seq. The combined King and Prince vignettes
    are used to predict the Kingdom vignettes.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll train another model for inferring the King vignette given a Prince
    vignette. We’ll use this in inference later.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.13 Function to train the Prince to King model
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Running the preceding training procedure will take some time, especially if
    you’re not using GPU. Fortunately, there are saved versions of the trained models
    in the Hugging Face Hub. The following code pulls the Transformer models from
    the Hugging Face Hub and generates from them. It also provides a function that
    calculates the log probability of each generated sequence.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.14 Pull Transformer models from the Hugging Face Hub and generate
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Load the components of our model.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Load the components of our model.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Load the Bart-base tokenizer and set the pad token to end-of-sequence tokens.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll write some functions to encode text to tokens, decode tokens to
    text, and generate text from the model given input text.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.15 Helper functions for encoding, decoding, and generation
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 Encode text into tensor.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Decode tensor into text.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Get the encoding for empty text, for convenience.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '#4 A function for generating from models. These parameters do slightly different
    things for the GPT-2 and BART models, but they more or less overlap.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: We want to use our probabilistic ML approach, so we need a way of computing
    the log probabilities of generated sequences so we can use these in inference.
    The following function computes the log probability of a generated sequence based
    on related values produced by the GPT-2 and BART models.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.16 Computing log probabilities of generated sequences
  id: totrans-328
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#1 Convert logits to logprobs for GPT-2.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Convert logits to logprobs from BART cross-entropy.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll put these pieces together to generate a full story from our three
    models.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.17 Generating a full story
  id: totrans-333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#1 Generate from the GPT-based model of vignettes about the King and calculate
    the log probabilities of the generated sequence.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Generate from the BART-based sequence-to-sequence model that generates vignettes
    about the Prince given vignettes about the King, and then calculate the log probability
    of the generated sequence.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Generate from the BART-based sequence-to-sequence model that generates vignettes
    about the Kingdom given vignettes about the King and the Prince, and then calculate
    the log probability of the generated sequence.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Another BART-based sequence-to-sequence model that maps a vignette about
    the Prince to a vignette about the King. We’ll use this to infer the vignette
    about the King from a vignette about the Prince.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is nondeterministic, but one example of the output you’ll get is
    as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that the generated output isn’t perfect—for example, the first generated
    statement ideally should have stopped after “…kingdom’s territories”. We could
    try to train it more or switch to a more powerful model, but this is pretty good
    for a start.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll use these Transformers library models to define distributions in
    Pyro, and then use Pyro to build a causal generative model. First, we’ll use Pyro’s
    `TorchDistributionMixin` to model the causal Markov kernels with the language
    models. We’ll use the GPT-2 model of the King vignettes to create the causal Markov
    kernel of the `King` variable.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll use the BART model to create the causal Markov kernel for the `Prince`
    variable. The `King` variable causes this variable, so the seq2seq model uses
    the `King` variable’s value to generate a value for this model.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll create the causal Markov kernel for the `Kingdom` variable. The
    `King` and `Prince` variables are causal parents, so we concatenate their generated
    outputs into one string, and use that string to generate the `Kingdom` output,
    again using a BART seq2seq model. We rely on a mixin called `TorchDistributionMixin`,
    which is useful for wrapping PyTorch distributions for use in Pyro.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.18 Building a Torch distribution from a Transformer model
  id: totrans-345
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#1 Use TorchDistributionMixin to turn a Transformers model into a Pyro distribution.
    TorchDistributionMixin is used to make PyTorch distributions compatible with Pyro’s
    utilities.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The log_prob method returns the log probabilities used in inference algorithms.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll use that distribution in Pyro.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.19 Incorporating Transformer models into a causal model with Pyro
  id: totrans-350
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 Build the causal LLM.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Create the causal Markov kernel for the King variable.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Create the causal Markov kernel for the Prince variable.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Create the causal Markov kernel for the Kingdom variable.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Concatenate all the generated vignettes into one overall vignette and return
    the result.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Confirm our causal model generates the full vignette.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code generates and prints two vignettes, such as the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We see that the generated texts are pretty good, though they seem to cut off
    a bit early. This, and other issues with the generations, can be addressed by
    tweaking the generation parameters.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: And just like that, we’ve built a causal LLM, an LLM build on a causal DAG scaffold.
    Let’s prove we have a causal model by comparing the observational and interventional
    distributions entailed by the DAG.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.3 Sampling from the interventional and observational distributions
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By now, you know the distribution *P*(*Kingdom*|*Prince*=*x*) will be different
    from *P*(*Kingdom*[*Prince*][=][*x*]), but let’s demonstrate the fact with this
    causal LLM. First, we’ll model *P*(*Kingdom*|*Prince*=*x*), where *x* is
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: His courageous Prince takes command, leading the kingdom’s army to victory in
    battle after battle
  id: totrans-364
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To infer *P*(*Kingdom*|*Prince*=*x*), we’ll have to infer the distribution of
    the latent confounder, *King*. We’ll do this using the `prince2king_model` we
    trained. We’ll use a probabilistic inference algorithm called “importance resampling.”
    We’ll start by creating a proposal function (what Pyro calls a “guide function”)
    that will generate samples of *King* and *Kingdom*, given *Prince*.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.20 Proposal distribution for *P*(*Kingdom*|*Prince*=*x*)
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#1 We condition the model on this value of the Prince variable.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We’ll use a proposal function to generate from our target distribution P(King,
    Kingdom|Prince=PRINCE_STORY).'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The proposal uses the prince2king_model to infer values of King given Prince=PRINCE_STORY.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Given the value of Prince, and the inferred value of King, use the king_and_prince
    model to sample Kingdom.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Concatenate the generated king tokens and provided prince tokens to return
    a generated vignette so we can inspect what is sampled.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll weigh each sample by the ratio of the probability of the sample under
    the conditioned model, over the probability of the sample under the proposal.
    Resampling the samples using these weights will generate samples from the target
    distribution. Pyro provides a utility for importance sampling, but because of
    the varying length of the generated sequences, it will be easier to implement
    importance sampling directly.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll write a function to process a sample and get its importance weight.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.21 Function to draw a sample for resampling
  id: totrans-375
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#1 Extract a sample from the proposal.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calculate the total log probability of the sampled values of King and Kingdom.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Calculate the total log probability of the sample values of King and Kingdom
    under the original model.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Calculate the log importance weight.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll run the importance resampling.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.22 Listing 13.22 Importance resampling of *P*(*Kingdom*|*Prince*=*x*)
  id: totrans-382
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#1 Use importance resampling as our inference procedure.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Resample using the importance weights. Pass in the log weights to the “logits”
    argument.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll infer *P*(*Kingdom*[*Prince*][=][*x*]). Given our causal model in
    Pyro, we can use Pyro’s do-operator to apply the intervention. We know that given
    the intervention on Prince, the edge from King to Prince is removed, so we don’t
    need to use `prince2king_model`. We can simply do ordinary forward generation
    from our intervention model.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.23 Inferring *P*(*KingdomPrince*=*x*) using vanilla forward Monte
    Carlo sampling
  id: totrans-387
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '#1 Forward sample from the interventional distribution.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Label the samples, and combine them with the observational samples.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating the samples will take some time. Since we’re working directly with
    the encoded sequence tensors in Pyro, we could leverage the potentially faster
    gradient-based inference algorithm. For convenience, you can access presaved samples
    in the book’s directory of the GitHub repo: [https://github.com/altdeep/causalml](https://github.com/altdeep/causalml).'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s visualize the difference in the distributions. We need a way to
    visualize sampled text from the interventional and observational distributions.
    We can do so using TF-IDF (term frequency-inverse document frequency), a numerical
    statistic that reflects how important a word is to a sample within the collection
    of samples, emphasizing words that are unique to specific samples.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.24 Get TF-IDF of generations for *P*(*Kingdom**[Prince=x]*) and *P*(*Kingdom*|*Prince*=*x*)
  id: totrans-393
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#1 Extract generated Kingdom vignettes from observational and interventional
    distributions.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Compute the TF-IDF values for generated Kingdom vignettes in each group.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Get the top k=7 words by TF-IDF for each set.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll visualize the two distributions.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.25 Visually contrast *P*(*Kingdom**[Prince=x]*) and *P*(*Kingdom*|*Prince*=*x*)
  id: totrans-399
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '#1 Prepare data for the bar plot.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Produce the plot.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: This produces figure 13.9.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Figures 13.9 shows similar TF-IDF scores for words in the observational case.
    This is due to the lack of variation in the observational case, since observing
    the Prince constrains the likely values of King. When we intervene on Prince,
    King can vary more, leading to more variation in the results.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F09_Ness.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 Visualization of the difference between samples from *P*(*Kingdom**[Prince]*[=]*[x]*)
    and *P*(*Kingdom*|*Prince*=*x*) using TF-IDF, where *x* is the Prince taking the
    army to battle. The observational values are flat because of little variation
    in the inferred King vignettes. The intervention enables more variation in the
    King vignettes and, consequently, the Kingdom vignettes.
  id: totrans-406
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 13.3.4 Closing thoughts
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a toy problem with a simple DAG trained on simple data with simple LLMs.
    But we could extend it to more complicated DAGs and fine-tuning more advanced
    models. There may also be other ways to combine causal assumptions in foundation
    models. We’re only at the beginning of exploring this exciting space.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) are powerful AI models that generate text and other
    modalities and achieve high performance across a variety of benchmarks.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs have proven use cases for supporting causal analysis.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can help build a causal DAG. Moreover, they can leverage common and expert
    knowledge about causal relations and mechanisms.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The causal frame problem is the challenge of selecting the causal variables
    relevant to a given problem and excluding the irrelevant. Cutting-edge LLMs emulate
    how humans set the causal frame, which is useful for applications such as building
    DAGs and root cause analysis.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can help us understand nuanced causal concepts and how to contextualize
    them within our domain of interest.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can help us put causal queries into formal terms.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are prone to hallucinations—convincing yet incorrect responses to our queries.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At their core, LLMs are probabilistic machine learning models that model a joint
    probability distribution on sequences of tokens.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attention mechanism enables the LLM to learn higher-level representations
    that make cutting-edge LLMs so powerful.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just because an LLM learns a higher-level representation doesn’t mean it learns
    a causal representation. Even if that did work in some special cases, it would
    be hard for the user to verify that it is working.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can build our own causal LLM by composing fine-tuned LLMs over a causal DAG
    scaffold. This allows us to work with cutting-edge LLMs while admitting causal
    operations, such as a do-operator.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the causal hierarchy theory as your North Star in your exploration of how
    to combine causality with LLMs and multimodal models, as well as exploring how
    well these models can learn causal representations on their own.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) C. Glymour, “Android epistemology and the frame problem,”
    in Z.W. Pylyshyn, ed., *The robot’s dilemma: The frame problem in artificial intelligence*
    (Praeger, 1987), pp. 63–75\.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
