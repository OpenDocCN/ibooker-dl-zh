- en: 13 Causality and large language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using causal information in LLMs to enhance a causal analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting the components of an LLM to a causal ideas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a causal LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large language models (LLMs) represent a significant advancement in the field
    of artificial intelligence. These models are large neural networks designed to
    generate and understand human-readable text. They are “large” because their scale
    is truly impressive—cutting-edge LLMs have parameters numbering in the billions
    and trillions. As generative models, their main function is to generate coherent
    and contextually relevant natural language. They can also generate structured
    text, such as programming code, markup languages, mathematical symbols, database
    queries, and many other useful things in text form.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are just one example of a broad class of generative AI. For example, we
    can use the neural network architecture underlying cutting-edge LLMs to model
    other sequences, such as time-series or DNA. LLMs are a type of *foundation model*,
    meaning large-scale models that serve as a base or foundation upon which more
    specialized models or applications can be built. Some LLMs are multimodal, meaning
    they work with text as well as other content modalities, such as images. In this
    chapter, we’ll focus specifically on LLMs, but much of what we discuss can be
    generalized to these related ideas.
  prefs: []
  type: TYPE_NORMAL
- en: To start, let’s explore some use cases for using LLMs to enhance a causal analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 13.1 LLMs as a causal knowledgebase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I’ve mentioned several times in this book, in causal modeling, we don’t model
    the data, we model the data generating process (DGP). In other words, causal modeling
    requires causal information about the DGP that we can use as assumptions that
    enable identification of our causal queries. Cutting-edge LLMs are excellent resources
    for sourcing and validating that causal information. In this section, I’ll demonstrate
    some proven use cases for using LLMs to query causal information that augments
    a causal analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The following results come from OpenAI’s LLM GPT-4 via the ChatGPT interface.
    Because of the random generation from the model, and OpenAI’s periodic updates
    to the model and ChatGPT software, your results will vary from mine to some degree.
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.1 Building a causal DAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start with using the LLM to augment the first workflow we discussed in
    this book: building the causal DAG. Specifying assumptions in the form of a causal
    DAG can be daunting. What if your DAG is wrong? Modern LLMs are invaluable as
    a way of leveraging public expert knowledge to build a DAG.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**You are a helpful causal analysis assistant.
    I’m interested in modeling the relationship between smoking and lung cancer. Provide
    me a causal DAG that captures this causal relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: '****![image](../Images/Response.png)****Certainly! A Directed Acyclic Graph
    (DAG) is a powerful tool to represent causal relationships. Here’s a basic causal
    DAG to represent the relationship between smoking and lung cancer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S**: Smoking behavior (e.g., smoker, non-smoker, or frequency of smoking)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LC**: Lung Cancer (presence or absence)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Directed Edge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A directed edge from **S** to **LC** represents that smoking behavior can cause
    or influence the occurrence of lung cancer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Potential Confounders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Other nodes can be introduced into the DAG to account for potential confounders.
    Some potential confounders might be:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**G**: Genetics (a family history of lung cancer)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A**: Age (older age might be associated with a higher risk)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**E**: Environmental exposure (like asbestos, pollution, etc.)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**O**: Other health behaviors (like diet, exercise)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Potential Mediators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mediators are intermediate variables through which a cause exerts its effect
    on an outcome.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, one could hypothesize that smoking might cause lung damage, which
    in turn causes lung cancer. In this case:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LD**: Lung Damage'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The DAG might look like this with the confounders and the mediator:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S** <- **G** –> **LC**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S** <- **A** –> **LC**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S** <- **E** –> **LC**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S** <- **O** –> **LC**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S** –> **LC**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S** –> **LD** –> **LC**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the process of modeling, it’s important to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify all potential confounders** to avoid biased results. **Remember
    that correlation does not imply causation**. Just because two nodes are correlated
    does not mean one causes the other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validate your DAG with domain experts**. They can provide insights into relationships
    you might not have considered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use data** to test and validate the causal relationships in your DAG, e.g.,
    through controlled experiments or statistical methods like instrumental variables
    or propensity score matching when experiments are not feasible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bear in mind that this is a simplified DAG for the relationship between smoking
    and lung cancer. Depending on the granularity and depth of your study, you might
    need to adjust and expand this framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuing, I can prompt the LLM to provide potential instrumental variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**Suggest two good candidates for instrumental
    variables. Be brief in your response.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Certainly. For the relationship between
    smoking and lung cancer, two potential instrumental variables (IVs) could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cigarette Tax or Price**: If certain regions or periods have higher cigarette
    prices or taxes, this can influence smoking behavior without directly influencing
    lung cancer rates except through the effect of reduced smoking. The assumption
    here is that the only way cigarette prices could affect lung cancer is by influencing
    smoking behavior.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Anti-Smoking Campaigns or Legislation**: Introduction or intensity of anti-smoking
    campaigns or legislation in certain regions or times can be used as an instrument.
    If these campaigns or laws are exogenously determined (not based on current lung
    cancer rates), they can affect smoking rates without having a direct effect on
    lung cancer, except through their impact on smoking behavior.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For both IVs, it’s crucial to validate the two key assumptions of instrument
    validity:'
  prefs: []
  type: TYPE_NORMAL
- en: The instrument is related to the exposure (e.g., cigarette tax affects smoking
    behavior).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The instrument affects the outcome (e.g., lung cancer) only through its effect
    on the exposure and not through other pathways.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prompting best practice
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The prompt preface “You are a helpful causal analysis assistant” is an example
    of what is sometimes called a “system prompt.” System prompts provide general
    context about what is expected from the LLM. System prompting, and other prompt
    engineering techniques (e.g., chain-of-thought prompting, in-context learning)
    are best practices for getting good generations from LLMs. For the sake of brevity,
    I’ll use system prompts but omit some of the other prompt engineering best practices
    in this section—it is smart to include them in practical settings.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing to interact with the LLM in this way, perhaps through multiple sessions,
    you can come up with a candidate DAG that makes sense to you and your stakeholders.
    Next, you can prompt the LLM to give you code to run an analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.2 Generating code for DAGs, models, and causal analyses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern LLMs are trained on vast corpora of code from a wide variety of programming
    languages. This greatly speeds up causal analyses, particularly if the libraries
    you’re using are represented in the training data. For example, we can prompt
    ChatGPT to implement its suggested lung cancer DAG in DoWhy and estimate the causal
    effect.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**Provide me with code that uses the DoWhy library
    in Python to implement the causal DAG that you suggested above. Include cigarette
    price as an instrumental variable and provide code for estimating the causal effect.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Certainly! Here’s a basic example using
    the DoWhy library in Python for implementing the causal DAG for the relationship
    between smoking and lung cancer, using cigarette price as an instrumental variable.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The code in this chapter was written with DoWhy version 0.11, Transformers version
    4.38.2, Accelerate version 0.28.0, pandas 2.0.3, NumPy 1.25.2, and pyro-ppl (Pyro)
    version 1.9.0\. Visit the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for a link to a notebook containing the chapter’s code.
  prefs: []
  type: TYPE_NORMAL
- en: Chat GTP then went on to generate the following code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.1 DoWhy code generated by ChatGPT (verbatim)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 This line is wrong, but the code runs if you remove it.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Simulate smoker status: 0 for non-smoker, 1 for smoker.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Simulate lung cancer status: 0 for no lung cancer, 1 for lung cancer.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Simulate cigarette price with a mean price of 5.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**This is a simplified example. In a real-world
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: Your data would be imported rather than simulated as above.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that your DAG correctly models the relationships based on domain knowledge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assumptions behind the instrumental variables need to be rigorously checked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further analytical steps (e.g., statistical tests, checking robustness) would
    be necessary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you generate code, you may get errors. For example, DoWhy has gone through
    several iterations, and there is nothing forcing the LLM to keep up with the latest
    version of the library. In this case, there is an error in the line `from dowhy.do_why
    import CausalModel`. There is no module called `do_why`, and `CausalModel` is
    already imported in a previous line. We’ll address errors produced by the LLM
    in section 13.1.7\. For now, if you delete that line, the code will run.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT will work well with pgmpy and Pyro, the libraries we’ve worked with
    extensively in the book. As an exercise, I suggest taking one of the models you
    built in pgmpy, and prompting the LLM to convert it to a model in Pyro.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at what the LLM can tell us beyond the structure of the DAG.
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.3 Explanations and mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The advantage of DAGs is that graphs are an intuitive way to represent what
    causes what. But they do not tell us anything about the nature of the mechanism
    by which a cause drives an effect.
  prefs: []
  type: TYPE_NORMAL
- en: Structural causal models provide a mathematical way of characterizing the mechanism
    underlying a causal relationship. Characterizing mechanisms in mathematical terms
    makes sense in natural sciences, where we have plenty of equations describing
    natural laws. It is harder in, for example, business domains where we lack equations
    beyond mere approximations (what is the equation relating ad spending and engagement?).
  prefs: []
  type: TYPE_NORMAL
- en: LLMs leverage common and expert knowledge to provide mechanistic explanations
    that go beyond the DAG but that are in a form more accessible than formal math.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**You are a helpful causal explanation generator.
    Provide a characterization of the causal relationship between ad spending and
    brand awareness. Be concise enough to fit a single slide in a slide deck.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Causal Relationship Between Ad Spending
    and Brand Awareness'
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct Causality**: As ad spending increases, brand awareness typically rises.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mechanism**: Larger ad spends mean more frequent and widespread exposure
    of the brand to the target audience.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Diminishing Returns**: After a certain point, additional ad spending might
    not lead to proportionally higher brand awareness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reason**: Market saturation, ad fatigue, or redundancy in audience reach.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**External Factors**: Ad efficiency isn’t solely based on spend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Influences**: Ad quality, placement, target audience relevancy, competition,
    and market conditions can affect the outcome.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider that some counterfactual queries can be identified, with simple categorical
    assumptions about the functional relationship between the cause and effect. For
    example, the probability of necessity is identifiable from intervention data and
    partially identifiable from observational data if the relationship between *X*
    and *Y* is monotonic:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**Is this a monotonic relationship? Limit your
    answer to one sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Yes, the relationship between ad spending
    and brand awareness is generally monotonic, as increased ad spending will not
    decrease brand awareness, though the rate of increase might diminish.'
  prefs: []
  type: TYPE_NORMAL
- en: We needn’t limit ourselves to one explanation; we can use the LLM as a generator
    of explanatory hypotheses. Consider, for example*, abductive reasoning tasks*
    where one infers likely causes of a particular observed set of effects (e.g.,
    in science, criminal investigation, and medical diagnosis).
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, let’s pose a modified version of a medical diagnostic question
    sourced from the US Medical License Exam to ChatGPT. I’ll prompt for multiple
    hypotheses about the cause of the observed symptoms, which is what a doctor would
    call a *differential diagnosis*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**You are a causal explanation assistant. Your
    job is to generate 3 one sentence-long hypotheses that explain the observations.
    This is a medical diagnosis example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observations: A 48-year-old woman presents with diplopia, ocular pain, amenorrhea,
    hot flashes, weight loss, tachycardia, hypertension, moist palms, enlarged thyroid,
    prominent eyes, lid retraction, conjunctival injection, and inability to converge
    eyes. Visual acuity is 20/20 bilaterally.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)** The patient may have Graves’ disease,
    an autoimmune disorder affecting the thyroid, leading to hyperthyroidism and its
    associated symptoms, including ocular manifestations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. The woman’s symptoms, including amenorrhea and hot flashes, suggest a possible
    concomitant early menopause or another endocrine disorder affecting her reproductive
    hormones.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. The ocular symptoms, including diplopia, lid retraction, and inability to
    converge eyes, hint at a potential thyroid eye disease or orbital myopathy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the challenges of causal modeling is working with domain experts to encode
    domain assumptions into the structure of the model. These types of generated explanations
    and hypotheses are valuable assets for communicating with domain experts who might
    agree with, or better yet, iterate upon these generated explanations.
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.4 The causal frame problem and AI alignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given a query of interest, we have to decide what variables are relevant to
    the problem. Philosoper and AI researcher Clark Glymour does a good job of describing
    the challenge behind this task: “Given an enormous amount of stuff, and some task
    to be done using some of the stuff, what is the relevant stuff for the task?”[¹](#footnote-305)'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are useful at establishing a *causal frame*—the problem of selecting the
    causal variables relevant to a causal query. In chapter 3’s discussion on building
    the DAG, I mentioned *causal sufficiency* as a d-separation–based criteria for
    selecting variables for inclusion in the DAG. But when we prompted the LLM to
    provide a DAG containing variables relevant to smoking’s effect on lung cancer,
    it is highly doubtful that the LLM was using causal sufficiency to select those
    variables. More likely, it was using common knowledge found in its training data
    about this relationship. That knowledge was sufficient to provide a reasonably
    minimal set of relevant variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the utility of this ability, consider the following ChatGPT interaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**You are a helpful causal reasoning assistant.
    Consider the following story.'
  prefs: []
  type: TYPE_NORMAL
- en: Jake is on an office network. There was an open shared folder on the server.
    Jake is transferring files across the network and accidentally clicks on the folder,
    starting a process that consumes a significant amount of bandwidth. This causes
    the network to slow down. How responsible is Jake for the network slowdown?
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Based on the provided story, let’s analyze
    the situation using causal reasoning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct Cause**: Jake started a process that consumes a significant amount
    of bandwidth by accidentally clicking on a file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Effect**: The network slowed down due to the consumed bandwidth.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given these facts, we can establish that Jake’s action was the direct cause
    of the network slowdown.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, responsibility can be considered from different dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intent**: Jake’s action was accidental. He did not intend to slow down the
    network.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prevention Mechanisms**: The story does not specify if there were any warnings
    or user prompts before starting such a process that consumes significant bandwidth.
    If there were no warnings, one could argue that the system was not designed to
    prevent such accidental actions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Knowledge & Training**: Was Jake trained or informed about the implications
    of transferring large files or about the particular file that consumes a lot of
    bandwidth? If not, his knowledge gap contributed to the accident.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**System Design**: The open shared folder on the server was accessible to Jake.
    If the file was known to be a bandwidth hog, it’s worth asking why such a file
    was placed in an open shared folder without any warnings or restrictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To directly answer the question: Jake is responsible for the network slowdown
    in the sense that his action was the direct cause. However, the degree of his
    responsibility might be mitigated if there were systemic design flaws, lack of
    training, or absence of preventive measures. If other factors, like the ones mentioned
    above, played a role in the incident, then the responsibility would be shared
    among Jake and those other factors.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the response includes the systemic design flaws, lack of training, or
    absence of preventive measures as factors relevant to the causal frame, despite
    the fact that I made no mention of these factors in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'More importantly, consider the many factors the LLM could have included but
    did not: perhaps Jake’s mouse had a physical glitch that caused him to click on
    the file, or perhaps a hacker removed security features on that file, or any other
    possible mitigating factors. A human would likely exclude these from the causal
    frame unless explicitly mentioned, and so does the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If humans are already good at establishing the causal frame, why would we care
    whether the LLM can do so? Consider the problem of root cause analysis from network
    logs. Suppose the same story about Jake were rewritten in the form of network
    logs, such as in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This is harder for a human to parse than the natural language story, even for
    an experienced network administrator. Further, there would likely be many other
    network events unrelated to the events of this narrative that the human would
    have to read through and disregard. But the LLM has plenty of network logs in
    its training data, and extracting relevant events from a longer body of text is
    an established capability of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In most of this book, we’ve focused on using causal models to infer objective
    causal truth about the world, such as whether smoking causes lung cancer. In this
    example, the objective truth, that Jake caused the network slowdown, is trivial.
    The question of how responsible Jake is for the slowdown is highly subjective.
    But this is still a valuable question to answer, especially if you are trying
    to find a root cause in a large network log file.
  prefs: []
  type: TYPE_NORMAL
- en: '*AI alignment* is the task of making sure an AI system’s behaviors are consistent
    with human goals and preferences. Here, we want a specific type of AI alignment.
    We want the LLM to go beyond confirming obvious causal facts to aligning with
    the ways humans tend to make causal judgments. In this case, I asked the LLM an
    open question about Jake’s responsibility, and it gave a response that many humans
    would agree with. I could also have instructed the LLM to follow certain criteria
    when generating a judgement. For example, an attorney could prompt the LLM to
    apply certain legal theories and case law in generating a legal opinion.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at how to use the LLM to understand and contextualize more
    nuanced causal concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.5 Understanding and contextualizing causal concepts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It can be challenging to understand formal causal concepts and apply them within
    your domain. The LLM can serve as a tool for explaining and contextualizing these
    concepts. Let’s illustrate with the counterfactual concept of *necessary causation*.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**You are a helpful explainer of causal inference
    topics. Provide a one paragraph explanation of the concept of necessary causation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Certainly! Necessary causation refers to
    a causal relationship where the cause is a necessary condition for the effect
    to occur. In other words, without the presence of the cause, the effect cannot
    happen. However, the presence of the cause does not guarantee the effect will
    occur, as there may be other conditions that also need to be met. Think of it
    like a key to a lock: the key (cause) is necessary to open the lock (effect),
    but just having the key doesn’t mean the lock will open automatically; you still
    need to insert and turn it. If the lock remains locked without the key, then the
    key is a necessary cause for unlocking.'
  prefs: []
  type: TYPE_NORMAL
- en: A cutting-edge LLM would be trained on a wide variety of theory and research
    from across domains. It can be useful to contextualize an idea within various
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**In one paragraph, explain how necessary causation
    is conceptualized in legal theory.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**In legal theory, necessary causation, often
    termed “but-for causation,” is a foundational concept used to establish a direct
    link between a defendant’s actions and the harm suffered by a plaintiff. For an
    action to be a but-for cause of an outcome, the outcome would not have occurred
    in the absence of the action. In other words, if one can say “but for the defendant’s
    action, the harm would not have occurred,” then necessary causation is established.
    However, it’s worth noting that while but-for causation is a critical component
    in many legal contexts, it’s not always sufficient on its own to establish liability.
    Other factors, such as foreseeability and proximate cause, often come into play
    to determine the full scope of legal responsibility.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**From the perspective of cognitive science,
    explain in one brief paragraph the role necessary causation plays in human judgments.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**From a cognitive science perspective, necessary
    causation plays a pivotal role in human judgments and decision-making. Our brains
    are wired to seek patterns and causal relationships in the environment to predict
    and navigate future events. When we perceive an event as a necessary cause of
    another, it simplifies our cognitive processing by creating a deterministic link
    between the two events. This means that when we encounter the cause, we can anticipate
    the effect, allowing for more efficient planning and response. However, this predisposition
    can also lead to cognitive biases, where we might mistakenly attribute an effect
    to a perceived necessary cause, even when other contributing factors are at play.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**From the perspective of philosophy, explain
    in only one sentence how Kant and Hume differed in their beliefs about necessary
    causation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Kant believed in the existence of a priori
    synthetic knowledge and that our minds impose causal order on our experiences,
    while Hume argued that causation is not a feature of the world itself but a habit
    of thought based on the regular succession of events.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve unpacked the concept of necessary causation, let’s apply it
    to a practical problem in our domain of interest. Suppose you are a data scientist
    at Netflix:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**Give me an example of necessary causation that
    involves a Netflix user unsubscribing from Netflix.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**A necessary causation for a Netflix user
    unsubscribing from Netflix might be the exclusive content that is only available
    on the platform. If a particular show or movie that the user is deeply interested
    in is removed from Netflix while remaining available somewhere else, then the
    removal of that content becomes a necessary cause for that user’s decision to
    unsubscribe. Without the removal of that content, the user might not have unsubscribed.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that the LLM has helped us understand necessary causation, we can get its
    help in formalizing the concept.
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.6 Formalization of causal queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For humans, natural language is the medium of our causal reasoning. But we need
    to convert that natural language into formal terms so that we can apply mathematical
    and algorithmic analysis. This can be a challenge. Fortunately, LLMs can help
    greatly in this respect.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in chapter 8, I introduced several examples of the language of
    parallel world counterfactuals and how to convert them into counterfactual notation.
    Necessary causation, more specifically the *probability of necessity*,was one
    of these counterfactuals. Let’s put this task to the LLM in our Netflix context:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**You are a helpful causal inference assistant.
    I’m considering a counterfactual inference problem in the context of Netflix.
    I have the following query: “There is a set of users who unsubscribed after content
    was removed from the platform. What is the probability a given member of this
    set of users would not have unsubscribed had the content not been removed?” Write
    this query as a probabilistic expression with Pearl’s counterfactual notation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Certainly! Pearl’s counterfactual notation
    offers a concise way to express such queries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define some variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X* =removal of the content (binary, 1 if removed, 0 if not removed).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Y* =whether user unsubscribes (binary, 1 if user unsubscribes, 0 if they did
    not unsubscribe).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your query is essentially asking for the probability that a user would not have
    subscribed, had the content not been removed, given that it was removed and they
    did unsubscribe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Pearl’s counterfactual notation, this can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch13-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y*[*x*][=0] is the counterfactual outcome for *Y* had *X* been set to 0 (i.e.,
    the content is not removed).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X*=1 denotes that the content was removed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Y*=1 denotes that the user unsubscribed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, *P*(*Y**[x=]*[0] = 0| *X* = 1, *Y* = 1) is the probability that a user would
    not have unsubscribed had the content not been removed, given that it was removed
    and they did unsubscribe.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM can take us from conceptualization to formalization. But beware, as
    it can also “hallucinate.”
  prefs: []
  type: TYPE_NORMAL
- en: '13.1.7 Beware: LLMs hallucinate'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen that LLMs can aid in code generation. Let’s ask for code that implements
    the probability of necessity.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/init.png)**Provide Python code that, for a given pair
    of treatment and outcome variables, calculates the counterfactual query “probability
    of necessity.”'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**The Probability of Necessity (PN) is the
    proportion of cases where the treatment is present, and the outcome is achieved
    ONLY when the treatment is present. In simpler terms, it indicates the probability
    that the outcome occurs because of the treatment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that definition, you can calculate PN using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch13-eqs-1x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*Treatment*∩*Outcome*) is the probability of both the treatment and outcome
    occurring.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(¬*Treatment*∩*Outcome*) is the probability of the outcome occurring but
    not the treatment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The response goes on to provide Python code that is well formatted, well commented,
    and compiles. *But this answer is wrong*. This is what is commonly called a “hallucination,”
    a convincing lie. This example is especially convincing—that formula is true *only*
    when the relationship between the treatment and outcome is monotonic, when there
    are no confounders, and when *P*(*Treatment*) = *P*(*Outcome*). Those assumptions
    are so strong that we can’t forgive ChatGPT for not mentioning them. Other generations
    from the model were even more off base.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the LLM to augment our causal analysis, particularly with common
    and expert causal knowledge as well as generated code. But hallucination means
    this isn’t a panacea. We need to rely on our own causal expertise to spot when
    hallucination occurs and understand when it threatens the quality of our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: To understand why this hallucination occurred, let’s first examine how LLMs
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2 A causality-themed LLM primer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand how to deploy LLMs for causal applications, it is important to
    understand how they work, as well as their limitations. This section provides
    a quick high-level causality-themed tour of the core ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.1 A probabilistic ML view of LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of LLMs, a “token” refers to a sequence of characters that the
    model reads, which can be as short as one character or as long as one word. Tokens
    are the units into which input text is divided into manageable pieces for the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face’s Transformers library has a publicly available version of GPT-2,
    which is far inferior to cutting-edge models but has a similar *Transformer* architecture.
    The Transformer architecture is a type of deep learning model designed to process
    and understand text and other sequential data, by focusing on the relationships
    between words in a sentence regardless of their position. Let’s tokenize the expression
    “Can LLMs reason counterfactually?”
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.2 Viewing example tokens that an LLM operates upon
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Initialize the GPT-2 tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Tokenize the sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Print out the tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This prints out the following tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The “Ġ” corresponds to a space. Note that punctuation marks are tokens, and
    that words like “counterfactual” are broken up into multiple tokens. Each token
    corresponds to an integer indexing the token in a large “vocabulary.” GPT-2 has
    a vocabulary size of 50,257.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.3 Converting tokens to integers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 “Encode” the tokens into integers that index the token in a list of tokens
    called the “vocabulary”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This *encodes* the tokens into a sequence of integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The Transformer architecture works with these numeric values.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs define a joint probability distribution on sequences of tokens. For the
    phrase “Can LLMs reason counterfactually?” the model defines a probability distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch13-eqs-2x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The models will also consider the chances that this sequence ended at the question
    mark, rather than continuing. For that, the LLM’s vocabulary includes a special
    token to mark the end of a sequence. For GPT-2, this is token is `<|endoftext|>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch13-eqs-3x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Further, autoregressive LLMs, such as the GPT and Llama series of Transformer
    models, model text in the order of the text sequence, so they factorize this joint
    probability as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch13-eqs-4x.png)'
  prefs: []
  type: TYPE_IMG
- en: We can calculate each of these probabilities on the log scale with the Transformers
    library. In generating the log probability, we first calculate *logits* for each
    term in the vocabulary. For a probability value *p*, the corresponding logit is
    log(*p* / (1–*p*)).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.4 Calculate the log probability of each token in the sequence
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Initialize the GPT-2 model and set to evaluation mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Tokenize and encode the phrase, including the end-of-sequence token.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Given the phrase, the model produces logits for every element in the vocabulary.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 For each position in the sequence, get the log probability corresponding
    to the token that was actually present in that position.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Summing these together provides the joint probability of the sequence under
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, as a generative model, GPT-2 can generate the next token conditional
    on the tokens that came before it. The *prompt* the user provides is the beginning
    of the sequence, and the model’s response extends the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.5 Generation from the LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Specify and encode the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Generate from the model. The “do_sample=True” argument means we’re doing
    random selection from the probability distribution of the next token, given all
    the previous tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Decode and print the output.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This prints out the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Again, note that ChatGPT generation has a random element, so this will likely
    produce something different for you.
  prefs: []
  type: TYPE_NORMAL
- en: Note about confusing terminology
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Models like the GPT models are often called “causal language models,” but these
    are not causal models in the way we’ve discussed in this book. They are not a
    causal model of a DGP. “Causal” here refers to the autoregressive nature of the
    model—the model evaluates the probability of a token in a sequence conditional
    only on the tokens that came before it.
  prefs: []
  type: TYPE_NORMAL
- en: All this is to say that the LLM is at a basic level a probability model of the
    joint probability of a sequence of tokens. The canonical training procedures for
    these models attempt to fit the joint probability distribution of the tokens.
    Models like GPT optimize the model’s ability to predict a given token in a training
    document given the previous tokens. Understanding that the LLM is a probability
    model over tokens doesn’t explain why LLMs can generate *coherent* text (meaning
    text with logical and consistent interrelation of ideas that forms a comprehensible
    whole). For that, we need to understand *attention*.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.2 The attention mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the main drivers behind the success of LLMs is use of *Transformer architectures*
    and other neural network architectures that rely on a mechanism called *attention*.
    The attention mechanism allows the model to weigh the importance of different
    parts of an input sequence differently. That allows the model to learn to “focus”
    on specific parts of a sequence that are more relevant to a given task, while
    “ignoring” or assigning lesser weight to less pertinent parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider, for example, the following conditional counterfactual statement about
    leaves being a necessary cause of a fire:'
  prefs: []
  type: TYPE_NORMAL
- en: The Labor Day weekend wildfire started in the forest and spread rapidly due
    to the dry leaves on the ground. Had there been a controlled burn, the fire wouldn’t
    have spread so rapidly.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The attention mechanism helps the model recognize that “leaves” refers to foliage,
    not a departure, by weighing the relevance of surrounding words like “ground,”
    “dry,” and “forest.”
  prefs: []
  type: TYPE_NORMAL
- en: Modern LLMs have attention mechanisms stacked over many neural network layers.
    This enables the LLM to attend to concepts at different levels of granularity.
    For example, while the first layer of attention focuses on immediate word-to-word
    relationships, such as “leaves” with “ground,” the next few layers connect broader
    phrases, treating “The Labor Day weekend wildfire” as a single entity connected
    to the phrase “spread rapidly.”
  prefs: []
  type: TYPE_NORMAL
- en: Latter layers can represent the overarching theme or subject of the sentence
    and the broader text, connecting “The Labor Day weekend wildfire” to information
    about how it spread.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.3 From tokens to causal representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ability to talk about how attention enables the LLM to learn higher-level
    abstractions becomes of interest to us from the standpoint of causality. Recall
    figure 13.1, which first appeared in chapter 5 (as figure 5.4).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F01_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 Example from chapter 5, where *digit* and *is-handwritten* are high-level
    causal drivers of low-level *X**[i]* pixels
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The small squares represent pixels in an image, while the squares *digit* and
    *is-handwritten* represent the digit depicted in the image and whether it was
    handwritten, respectively. In that example (section 5.1.2), I suggested that whatever
    causal relations exist between individual pixels doesn’t matter to us; we’re interested
    in reasoning at the level of the objects depicted in the image.
  prefs: []
  type: TYPE_NORMAL
- en: There is a similar thing going on with tokens, as shown in figure 13.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F02_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 *X*[1] through *X*[12] are a sequence of tokens. Whatever structure
    (causal or otherwise) exists between the tokens is of passing interest. We are
    interested in the causal relations between concepts described by the tokens.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In figure 13.2, as with the pixels, there is some structure at the level of
    the tokens. But that structure is beside the point—we’re interested in the causal
    relationships between the concepts that form the meaning behind the tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The question becomes, under what circumstances could attention, insofar as it
    can learn higher-level representations, learn a *causal* representation. For example,
    could an attention-based model, perhaps under some set of architectural or learning
    constraints, or use of interventions in training data, learn the parallel world
    structure and abstractions in the Labor Day counterfactual statement?
  prefs: []
  type: TYPE_NORMAL
- en: To consider this, we can revisit LLM hallucinations in our question about the
    probability of necessity, and connect it to causal identification.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.4 Hallucination, attention, and causal identification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The hallucination about probability of necessity was generated with GPT-4\.
    The same model got the Netflix question about probability of necessity correct.
    Indeed, the hallucinated answer would have been right if it merely stated the
    correct identifying assumptions. I believe future versions of GPT and similar
    models will likely get this question right on the first try.
  prefs: []
  type: TYPE_NORMAL
- en: But for someone who is unfamiliar with the definition of probability of necessity,
    how would they know if the model were right or if it were hallucinating? First,
    the causal hierarchy tells us that in order to be capable of generating a right
    answer beyond a random guess, the query would need to be identified with level
    3 information. Perhaps that information is provided by the user in the prompt.
    Perhaps the LLM has somehow learned level 3 representations (such a claim would
    require hard proof).
  prefs: []
  type: TYPE_NORMAL
- en: If the user were providing that identifying information in the prompt, how would
    the user know if the model was successfully using that information to respond
    to the prompt? Suppose instead that the requirements for identification exist
    and are buried in the learned representations or data, and that they were being
    successfully leveraged by the model in answering the causal query, how could the
    user know for sure this was happening?
  prefs: []
  type: TYPE_NORMAL
- en: We need to engineer solutions that answer these and other desiderata to build
    toward a causal AI future. In the next section, we’ll start on this path with
    a simple causal LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3 Forging your own causal LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll sidestep the question of “can cutting-edge LLMs reason
    causally?” and move on to building a causal LLM that can reason causally. We’ll
    build for causality from the ground up, rather than as an afterthought.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.1 An LLM for script writing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often our data has some implicit causal structure. When we make that structure
    explicit during training, the foundation model can learn better causal representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, suppose a prolific film production studio has historically insisted
    their writers use script-writing software that required following a three-act
    narrative archetype, which is common for romantic comedies: “boy meets girl, boy
    loses girl, boy gets girl back.” For this archetype, they have a corpus of many
    scripts. In causal terms, the events in act 1 cause the events of act 2, and the
    events of acts 1 and 2 cause the events of act 3\. We can draw the DAG in figure
    13.3.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F03_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 A causal DAG for a three-act archetype
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The studio works with many such archetypes, and the company has many scripts
    that follow a given archetype template. Suppose that a set of archetypes involve
    a king acting a certain way in act 1, a prince acting a certain way in act 2,
    and these two actions having an effect on a kingdom in act 3\. For example, one
    possible archetype is “King declares war, Prince leads army, kingdom experiences
    prosperity.” But there are multiple outcomes for each act:'
  prefs: []
  type: TYPE_NORMAL
- en: 'King in act 1: {king declares war; king negotiates peace; king falls ill}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prince in act 2: {prince leads army; prince abdicates throne; prince marries
    foreigner}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingdom in act 3: {kingdom wins battle; kingdom falls into poverty; kingdom
    experiences prosperity}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F04_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 A causal DAG representing various King-Prince-Kingdom archetypes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 13.4 shows this space of archetypes in the form of a causal DAG.
  prefs: []
  type: TYPE_NORMAL
- en: This describes only 3 × 3 × 3 = 27 possible archetypes, but as you might expect,
    some archetypes are more common and some are less common. We could easily model
    these archetypes and the joint probability distribution by explicitly coding causal
    Markov kernels in pgmpy or Pyro. But that would only be a causal generative model
    on the archetypes. If we want a script generator, we want a causal generative
    model on the scripts.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate a proof-of-concept for this idea, we’ll work with a training
    dataset of short vignettes, rather than full scripts. Let’s load and examine the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.6 Load causal narrative data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The data has 21,000 stories, broken up into three short vignettes.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 First, the king acts.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Then the prince acts.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Finally, the kingdom experiences the consequences of the royals’ actions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This code prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: There are 21,000 sets of three vignettes. The preceding output shows the first
    three sets in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.2 Using pretrained models for causal Markov kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train the causal Markov kernels for each node in our DAG, we’ll take pretrained
    models from the Hugging Face Transformers library, and then further train (aka
    “fine-tune”) the models using our vignettes. The pretraining took care of the
    heavy lifting in terms of learning to generate coherent natural language text.
    The fine-tuning will align the models toward representing our causal Markov kernels.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll use a GPT-2 variant to model the King’s action vignettes. As a
    text-completion model, it typically takes a prompt as input, but we’ll train it
    to generate with an empty prompt and produce vignettes according to the marginal
    probabilities of the King’s action texts in the training data, as in figure 13.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F05_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 GPT-2 is fine-tuned to represent the distribution of King’s action
    vignettes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next, we’ll use a BART model for the causal Markov kernel Prince’s action. BART
    is a Transformer model released in 2019 designed specifically to take an input
    sequence and generate a corresponding output sequence, such as with translation
    or summarization. Large models like GPT-4 can handle sequence-to-sequence tasks
    quite well, but we’ll use a version of BART with roughly 4,000-times fewer parameters
    than GPT-4, making it easier for you to load and train on your laptop or basic
    Python development environment. Given the King’s action vignette as input, it
    will generate a Prince’s action vignette, as illustrated in figure 13.6.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also use a BART model to model the causal Markov kernel for the Kingdom’s
    fate, as shown in figure 13.7\. The model will map the King’s and Prince’s actions
    to the Kingdom’s fate.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F06_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 A BART sequence-to-sequence model is fine-tuned to represent the
    Prince’s action given the King’s action.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F07_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 A BART sequence-to-sequence model is also used to model the Kingdom’s
    fate given the King’s action and Prince’s action.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Jumping ahead, we’re interested in the conditional probability distribution
    of the Kingdom’s fate, given a certain action by the Prince. Since that will require
    inference of the King’s actions given the Prince, we’ll additionally train one
    more BART model that generates a King’s action vignette given a Prince’s action
    vignette, as shown in figure 13.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F08_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 A BART sequence-to-sequence model is also fine-tuned to model the
    Kingdom’s fate, given the King’s and Prince’s actions.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s run the training procedure. First, we’ll set up our imports and our tokenizer.
    We’ll use Bart as the tokenizer for all of our models.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.7 Training the causal LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Set up the tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The pad token is used to make all the sequences the same length to facilitate
    matrix operations. It is common to set it to the “end-of-sequence (EOS)” token.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The max length of the token is set to 40, as all of the vignettes are less
    than 40 tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Next we’ll create a class and a function that tokenizes the King dataset. We’ll
    create a custom subclass of the PyTorch `Dataset` class called `ModelDataset`
    that will store token encodings and their corresponding labels. When accessed
    by index, it returns a dictionary containing token encodings for that index and
    the associated label, and it provides the total number of examples via its `__len__`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.8 Tokenizing the King vignettes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 When accessed by index, ModelDataset returns a dictionary containing token
    encodings for that index and the associated label.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Create a ModelDataset instance for the king vignettes.'
  prefs: []
  type: TYPE_NORMAL
- en: Next we’ll tokenize the Prince and Kingdom vignettes. This code will also produce
    a validation dataset used in training sequence-to-sequence models.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.9 Tokenizing the Prince and Kingdom vignettes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Split input and target phrases into training and validation sets.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Encode the training and validation sets.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll write a training algorithm for the King model. This function initializes
    a GPT-2 model with the specified parameters, sets up the training arguments, and
    trains the model on the provided dataset, finally saving the trained model to
    the specified directory.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.10 Training the King model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Initialize and configure model with the specified parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Configure the training settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Train the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll write a training algorithm for the sequence-to-sequence models.
    The function will split the provided input and target phrases into training and
    validation sets, tokenize them, and then create and return PyTorch `Dataset` objects
    for both sets using the `ModelDataset` class. The `train_seq2seq_model` function
    initializes a sequence-to-sequence model with the specified parameters, configures
    its training settings, and then trains the model using both training and validation
    datasets, finally returning the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.11 Function for training the sequence-to-sequence models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Initialize and configure sequence-to-sequence model with the specified parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Configure the training settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Train the model using both training and validation datasets, finally returning
    the trained model.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll use this function to train the models. We’ll specify some directories
    for saving checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Note  In listing 13.14, I’ll provide code that downloads a pretrained model
    from Hugging Face, so if you don’t wish train the model, you can skip ahead to
    that step.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.12 Training the King, Prince, and Kingdom models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Provide the output directories where you want to save your model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Train the King model using Seq2Seq.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Train the Prince model using Seq2Seq. The King vignettes are used to predict
    the Prince vignettes.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Train the Prince model using Seq2Seq. The King vignettes are used to predict
    the Prince vignettes.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Train the Kingdom model using Seq2Seq. The combined King and Prince vignettes
    are used to predict the Kingdom vignettes.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll train another model for inferring the King vignette given a Prince
    vignette. We’ll use this in inference later.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.13 Function to train the Prince to King model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding training procedure will take some time, especially if
    you’re not using GPU. Fortunately, there are saved versions of the trained models
    in the Hugging Face Hub. The following code pulls the Transformer models from
    the Hugging Face Hub and generates from them. It also provides a function that
    calculates the log probability of each generated sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.14 Pull Transformer models from the Hugging Face Hub and generate
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Load the components of our model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Load the components of our model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Load the Bart-base tokenizer and set the pad token to end-of-sequence tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll write some functions to encode text to tokens, decode tokens to
    text, and generate text from the model given input text.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.15 Helper functions for encoding, decoding, and generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Encode text into tensor.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Decode tensor into text.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Get the encoding for empty text, for convenience.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 A function for generating from models. These parameters do slightly different
    things for the GPT-2 and BART models, but they more or less overlap.'
  prefs: []
  type: TYPE_NORMAL
- en: We want to use our probabilistic ML approach, so we need a way of computing
    the log probabilities of generated sequences so we can use these in inference.
    The following function computes the log probability of a generated sequence based
    on related values produced by the GPT-2 and BART models.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.16 Computing log probabilities of generated sequences
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Convert logits to logprobs for GPT-2.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Convert logits to logprobs from BART cross-entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll put these pieces together to generate a full story from our three
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.17 Generating a full story
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Generate from the GPT-based model of vignettes about the King and calculate
    the log probabilities of the generated sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Generate from the BART-based sequence-to-sequence model that generates vignettes
    about the Prince given vignettes about the King, and then calculate the log probability
    of the generated sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Generate from the BART-based sequence-to-sequence model that generates vignettes
    about the Kingdom given vignettes about the King and the Prince, and then calculate
    the log probability of the generated sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Another BART-based sequence-to-sequence model that maps a vignette about
    the Prince to a vignette about the King. We’ll use this to infer the vignette
    about the King from a vignette about the Prince.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is nondeterministic, but one example of the output you’ll get is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that the generated output isn’t perfect—for example, the first generated
    statement ideally should have stopped after “…kingdom’s territories”. We could
    try to train it more or switch to a more powerful model, but this is pretty good
    for a start.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll use these Transformers library models to define distributions in
    Pyro, and then use Pyro to build a causal generative model. First, we’ll use Pyro’s
    `TorchDistributionMixin` to model the causal Markov kernels with the language
    models. We’ll use the GPT-2 model of the King vignettes to create the causal Markov
    kernel of the `King` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll use the BART model to create the causal Markov kernel for the `Prince`
    variable. The `King` variable causes this variable, so the seq2seq model uses
    the `King` variable’s value to generate a value for this model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll create the causal Markov kernel for the `Kingdom` variable. The
    `King` and `Prince` variables are causal parents, so we concatenate their generated
    outputs into one string, and use that string to generate the `Kingdom` output,
    again using a BART seq2seq model. We rely on a mixin called `TorchDistributionMixin`,
    which is useful for wrapping PyTorch distributions for use in Pyro.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.18 Building a Torch distribution from a Transformer model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Use TorchDistributionMixin to turn a Transformers model into a Pyro distribution.
    TorchDistributionMixin is used to make PyTorch distributions compatible with Pyro’s
    utilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The log_prob method returns the log probabilities used in inference algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll use that distribution in Pyro.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.19 Incorporating Transformer models into a causal model with Pyro
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Build the causal LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Create the causal Markov kernel for the King variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Create the causal Markov kernel for the Prince variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Create the causal Markov kernel for the Kingdom variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Concatenate all the generated vignettes into one overall vignette and return
    the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Confirm our causal model generates the full vignette.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code generates and prints two vignettes, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We see that the generated texts are pretty good, though they seem to cut off
    a bit early. This, and other issues with the generations, can be addressed by
    tweaking the generation parameters.
  prefs: []
  type: TYPE_NORMAL
- en: And just like that, we’ve built a causal LLM, an LLM build on a causal DAG scaffold.
    Let’s prove we have a causal model by comparing the observational and interventional
    distributions entailed by the DAG.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.3 Sampling from the interventional and observational distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By now, you know the distribution *P*(*Kingdom*|*Prince*=*x*) will be different
    from *P*(*Kingdom*[*Prince*][=][*x*]), but let’s demonstrate the fact with this
    causal LLM. First, we’ll model *P*(*Kingdom*|*Prince*=*x*), where *x* is
  prefs: []
  type: TYPE_NORMAL
- en: His courageous Prince takes command, leading the kingdom’s army to victory in
    battle after battle
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To infer *P*(*Kingdom*|*Prince*=*x*), we’ll have to infer the distribution of
    the latent confounder, *King*. We’ll do this using the `prince2king_model` we
    trained. We’ll use a probabilistic inference algorithm called “importance resampling.”
    We’ll start by creating a proposal function (what Pyro calls a “guide function”)
    that will generate samples of *King* and *Kingdom*, given *Prince*.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.20 Proposal distribution for *P*(*Kingdom*|*Prince*=*x*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We condition the model on this value of the Prince variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We’ll use a proposal function to generate from our target distribution P(King,
    Kingdom|Prince=PRINCE_STORY).'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The proposal uses the prince2king_model to infer values of King given Prince=PRINCE_STORY.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Given the value of Prince, and the inferred value of King, use the king_and_prince
    model to sample Kingdom.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Concatenate the generated king tokens and provided prince tokens to return
    a generated vignette so we can inspect what is sampled.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll weigh each sample by the ratio of the probability of the sample under
    the conditioned model, over the probability of the sample under the proposal.
    Resampling the samples using these weights will generate samples from the target
    distribution. Pyro provides a utility for importance sampling, but because of
    the varying length of the generated sequences, it will be easier to implement
    importance sampling directly.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll write a function to process a sample and get its importance weight.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.21 Function to draw a sample for resampling
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Extract a sample from the proposal.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calculate the total log probability of the sampled values of King and Kingdom.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Calculate the total log probability of the sample values of King and Kingdom
    under the original model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Calculate the log importance weight.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll run the importance resampling.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.22 Listing 13.22 Importance resampling of *P*(*Kingdom*|*Prince*=*x*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Use importance resampling as our inference procedure.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Resample using the importance weights. Pass in the log weights to the “logits”
    argument.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll infer *P*(*Kingdom*[*Prince*][=][*x*]). Given our causal model in
    Pyro, we can use Pyro’s do-operator to apply the intervention. We know that given
    the intervention on Prince, the edge from King to Prince is removed, so we don’t
    need to use `prince2king_model`. We can simply do ordinary forward generation
    from our intervention model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.23 Inferring *P*(*KingdomPrince*=*x*) using vanilla forward Monte
    Carlo sampling
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Forward sample from the interventional distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Label the samples, and combine them with the observational samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating the samples will take some time. Since we’re working directly with
    the encoded sequence tensors in Pyro, we could leverage the potentially faster
    gradient-based inference algorithm. For convenience, you can access presaved samples
    in the book’s directory of the GitHub repo: [https://github.com/altdeep/causalml](https://github.com/altdeep/causalml).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s visualize the difference in the distributions. We need a way to
    visualize sampled text from the interventional and observational distributions.
    We can do so using TF-IDF (term frequency-inverse document frequency), a numerical
    statistic that reflects how important a word is to a sample within the collection
    of samples, emphasizing words that are unique to specific samples.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.24 Get TF-IDF of generations for *P*(*Kingdom**[Prince=x]*) and *P*(*Kingdom*|*Prince*=*x*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Extract generated Kingdom vignettes from observational and interventional
    distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Compute the TF-IDF values for generated Kingdom vignettes in each group.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Get the top k=7 words by TF-IDF for each set.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll visualize the two distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.25 Visually contrast *P*(*Kingdom**[Prince=x]*) and *P*(*Kingdom*|*Prince*=*x*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Prepare data for the bar plot.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Produce the plot.'
  prefs: []
  type: TYPE_NORMAL
- en: This produces figure 13.9.
  prefs: []
  type: TYPE_NORMAL
- en: Figures 13.9 shows similar TF-IDF scores for words in the observational case.
    This is due to the lack of variation in the observational case, since observing
    the Prince constrains the likely values of King. When we intervene on Prince,
    King can vary more, leading to more variation in the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH13_F09_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 Visualization of the difference between samples from *P*(*Kingdom**[Prince]*[=]*[x]*)
    and *P*(*Kingdom*|*Prince*=*x*) using TF-IDF, where *x* is the Prince taking the
    army to battle. The observational values are flat because of little variation
    in the inferred King vignettes. The intervention enables more variation in the
    King vignettes and, consequently, the Kingdom vignettes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 13.3.4 Closing thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a toy problem with a simple DAG trained on simple data with simple LLMs.
    But we could extend it to more complicated DAGs and fine-tuning more advanced
    models. There may also be other ways to combine causal assumptions in foundation
    models. We’re only at the beginning of exploring this exciting space.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) are powerful AI models that generate text and other
    modalities and achieve high performance across a variety of benchmarks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs have proven use cases for supporting causal analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can help build a causal DAG. Moreover, they can leverage common and expert
    knowledge about causal relations and mechanisms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The causal frame problem is the challenge of selecting the causal variables
    relevant to a given problem and excluding the irrelevant. Cutting-edge LLMs emulate
    how humans set the causal frame, which is useful for applications such as building
    DAGs and root cause analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can help us understand nuanced causal concepts and how to contextualize
    them within our domain of interest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can help us put causal queries into formal terms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are prone to hallucinations—convincing yet incorrect responses to our queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At their core, LLMs are probabilistic machine learning models that model a joint
    probability distribution on sequences of tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attention mechanism enables the LLM to learn higher-level representations
    that make cutting-edge LLMs so powerful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just because an LLM learns a higher-level representation doesn’t mean it learns
    a causal representation. Even if that did work in some special cases, it would
    be hard for the user to verify that it is working.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can build our own causal LLM by composing fine-tuned LLMs over a causal DAG
    scaffold. This allows us to work with cutting-edge LLMs while admitting causal
    operations, such as a do-operator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the causal hierarchy theory as your North Star in your exploration of how
    to combine causality with LLMs and multimodal models, as well as exploring how
    well these models can learn causal representations on their own.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) C. Glymour, “Android epistemology and the frame problem,”
    in Z.W. Pylyshyn, ed., *The robot’s dilemma: The frame problem in artificial intelligence*
    (Praeger, 1987), pp. 63–75\.'
  prefs: []
  type: TYPE_NORMAL
