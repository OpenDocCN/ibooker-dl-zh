- en: Chapter 16\. Using LLMs with Custom Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 16 章\. 使用自定义数据与 LLM 结合
- en: In [Chapter 15](ch15.html#ch15_transformers_and_transformers_1748549808974580),
    we looked at Transformers and how their encoder, decoder, and encoder-decoder
    architectures work. The results of their revolutionizing NLP can’t be disputed!
    Then, we looked at transformers, which form the Python library from Hugging Face
    that’s designed to make it easier to use Transformers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 15 章（ch15.html#ch15_transformers_and_transformers_1748549808974580）中，我们探讨了
    Transformer 以及它们的编码器、解码器和编码器-解码器架构的工作原理。它们在革命性 NLP 方面的成果无可争议！然后，我们探讨了 Hugging
    Face 的 transformers 库，这是一个设计用来简化 Transformer 使用的 Python 库。
- en: Large Transformer-based models, which are trained on vast amounts of text, are
    very powerful, but they aren’t always ideal for specific tasks or domains. In
    this chapter, we’ll look at how you can use transformers and other APIs to adapt
    these models to your specific needs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大量文本训练的大型 Transformer 模型非常强大，但它们并不总是适用于特定任务或领域。在本章中，我们将探讨如何使用 Transformer
    和其他 API 来适应这些模型以满足您的特定需求。
- en: Fine-tuning allows you to customize pretrained models with your specific data.
    You could use this approach to create a chatbot, improve classification accuracy,
    or develop text generation for a more specific domain.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 微调允许您使用特定数据自定义预训练模型。您可以使用这种方法创建聊天机器人、提高分类准确度或为特定领域开发文本生成。
- en: There are several techniques for doing this, including traditional fine-tuning
    and parameter-efficient tuning with methods like LoRA and parameter-efficient
    fine-tuning (PEFT). You can also get more out of your LLMs with retrieval-augmented
    generation (RAG), which we’ll explore in [Chapter 18](ch18.html#ch18_introduction_to_rag_1748550073472936).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以实现这一点，包括传统的微调以及使用 LoRA 和参数高效微调（PEFT）等方法的参数高效微调。您还可以通过检索增强生成（RAG）从您的 LLM
    中获得更多收益，我们将在第 18 章（ch18.html#ch18_introduction_to_rag_1748550073472936）中探讨这一方法。
- en: In this chapter, we’ll explore some hands-on examples, starting with traditional
    fine-tuning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索一些实际示例，从传统的微调开始。
- en: Fine-Tuning an LLM
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调 LLM
- en: Let’s take a look, step by step, at how to fine-tune an LLM like BERT. We’ll
    take the IMDb database and fine-tune the model on it to be better at detecting
    sentiment in movie reviews. There are a number of steps involved in doing this,
    so we’ll look at each one in detail.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地看看如何微调一个像 BERT 这样的 LLM。我们将使用 IMDb 数据库，并在其上微调模型以更好地检测电影评论中的情感。这个过程涉及多个步骤，因此我们将逐一详细探讨。
- en: Setup and Dependencies
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置和依赖项
- en: 'We’ll start by setting up everything that we need to do fine-tuning with PyTorch.
    In addition to the basics, there are three new things that you’ll need to include:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先设置所有使用 PyTorch 进行微调所需的内容。除了基础知识外，您还需要包括以下三个新内容：
- en: Datasets
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集
- en: We covered datasets in [Chapter 4](ch04.html#ch04_using_data_with_pytorch_1748548966496246).
    We’re going to use these to load the IMDb dataset and the built-in splits for
    training and testing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第 4 章（ch04.html#ch04_using_data_with_pytorch_1748548966496246）中介绍了数据集。我们将使用这些数据集来加载
    IMDb 数据集和内置的训练和测试分割。
- en: Evaluate
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 评估
- en: This library provides metrics for measuring load performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库提供了用于衡量加载性能的指标。
- en: transformers
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: transformers
- en: As we covered in Chapters [14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)
    and [15](ch15.html#ch15_transformers_and_transformers_1748549808974580), the transformers
    Hugging Face library is designed to make using LLMs much easier.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第 14 章（ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797）和第
    15 章（ch15.html#ch15_transformers_and_transformers_1748549808974580）中所述，Hugging
    Face 的 transformers 库旨在使使用 LLM 变得更加容易。
- en: 'We’ll use some classes from the Hugging Face transformers library for this
    chapter’s fine-tuning exercise. These include the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Hugging Face transformers 库的一些类来完成本章的微调练习。以下是一些类：
- en: AutoModelForSequenceClassification
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AutoModelForSequenceClassification
- en: This class loads pretrained models for classification tasks and adds a classification
    head to the top of the base model. This classification head is then optimized
    for the specific classification scenario you are fine-tuning for, instead of being
    a generic model. If we specify the checkpoint name, it will automatically handle
    the model architecture for us. So, to use the BERT model with a linear classifier
    layer, we’ll use `bert-base-uncased`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类加载用于分类任务的预训练模型，并在基础模型顶部添加一个分类头。然后，这个分类头将针对您正在微调的特定分类场景进行优化，而不是一个通用模型。如果我们指定了检查点名称，它将自动为我们处理模型架构。因此，要使用带有线性分类器层的
    BERT 模型，我们将使用 `bert-base-uncased`。
- en: AutoTokenizer
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: AutoTokenizer
- en: This class automatically initializes the appropriate tokenizer. This converts
    text to the appropriate tokens and adds the appropriate special tokens, padding,
    truncation, etc.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类会自动初始化适当的分词器。它将文本转换为适当的标记，并添加适当的特殊标记、填充、截断等。
- en: TrainingArguments
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: TrainingArguments
- en: This class lets us configure the training settings and all the hyperparameters,
    as well as setting up things like the device to use.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类让我们配置训练设置和所有超参数，以及设置使用设备等。
- en: Trainer
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Trainer
- en: This class manages the training loop on your behalf, handling batching, optimization,
    loss, backpropagation, and everything you need to retrain the model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类代表你管理训练循环，处理批处理、优化、损失、反向传播以及你需要重新训练模型的所有事情。
- en: DataCollatorWithPadding
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DataCollatorWithPadding
- en: The number of records in the dataset doesn’t always line up with the batch size.
    This class therefore efficiently batches examples to the appropriate batch sizes
    while also handling details like attention masks and other model-specific inputs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的记录数并不总是与批处理大小相匹配。因此，这个类有效地将示例批处理到适当的批处理大小，同时处理诸如注意力掩码和其他模型特定输入等细节。
- en: 'We can see this in code here:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在代码中看到这一点：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that the dependencies are in place, we’ll load the data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在依赖项已经就绪，我们将加载数据。
- en: Loading and Examining the Data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和检查数据
- en: 'Next up, let’s load our data by using the datasets API. We’ll also explore
    the test and training dataset sizes. You can use the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用数据集API加载数据。我们还将探索测试和训练数据集的大小。你可以使用以下代码：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'That will output the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The next step is to initialize the model and the tokenizer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是初始化模型和分词器。
- en: Initializing the Model and Tokenizer
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化模型和分词器
- en: 'We’ll use the `bert-base-uncased` model in this example, so we need to initialize
    it by using `AutoModelForSequenceClassification` and getting its associated tokenizer:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用`bert-base-uncased`模型，因此我们需要通过使用`AutoModelForSequenceClassification`并获取其相关的分词器来初始化它：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note the `AutoModelForSequenceClassification` needs to be initialized with the
    number of labels that we want to classify for. This defines the new classification
    head with two labels. The IMDb database that we’ll be using has two labels for
    positive and negative sentiment, so we’ll retrain for that.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`AutoModelForSequenceClassification`需要用我们想要分类的标签数量进行初始化。这定义了具有两个标签的新分类头。我们将使用的IMDb数据库有两个标签，分别表示正面和负面情感，因此我们将为此进行重新训练。
- en: At this point, it’s also a good idea to specify the device that the model will
    run on. Training with this model is computationally intensive, and if you’re using
    Colab, you’ll likely need a high-RAM GPU like an A100\. Training with that will
    take a couple of minutes, but it can take many hours on a CPU!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，指定模型将运行的设备也是一个好主意。使用此模型进行训练计算量很大，如果你使用Colab，你可能会需要一个高RAM的GPU，比如A100。使用它将需要几分钟，但在CPU上可能需要几个小时！
- en: Preprocessing the Data
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Once we have the data, we want to preprocess it just to get what we need to
    train. The first step in this, of course, will be to tokenize the text, and the
    `preprocess` function here handles that, giving a sequence length of 512 characters
    with padding:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了数据，我们想要对其进行预处理，以便仅获取我们需要的用于训练的数据。这一步的第一步当然是分词文本，这里的`preprocess`函数处理这一点，给出512个字符的序列长度，并进行填充：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: One important note here is that the original data came with columns for `text`
    denoting the review and `label` being 0 or 1 for negative or positive sentiment.
    However, we don’t *need* the `text` column to train the data, and the Hugging
    Face Trainer (which we will see in a moment) expects the column containing the
    label to be called `labels` (plural). Therefore, you’ll see that we remove all
    of the columns in the original dataset, and the tokenized dataset will have the
    tokenized data and a column called `labels` instead of `label`, with the original
    values copied over.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个重要的注意事项，即原始数据包含表示评论的`text`列和表示情感为0或1的`label`列。然而，我们不需要`text`列来训练数据，并且Hugging
    Face Trainer（我们稍后将看到）期望包含标签的列被命名为`labels`（复数）。因此，你会看到我们移除了原始数据集中的所有列，并且分词后的数据集将包含分词后的数据和名为`labels`的列，而不是`label`列，原始值将被复制过来。
- en: Collating the Data
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集
- en: When we’re dealing with passing sequenced, tokenized data into a model in batches,
    there can be differences in batch or sequence size that need processing. In our
    case, we shouldn’t have to worry about the sequence size because we used a tokenizer
    that forces the length to be 512 (in the previous set). However, as part of the
    transformers library, the collator classes are still equipped to deal with it,
    and we’ll be using them to ensure consistent batch sizing.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将经过序列化和分词的数据批量输入模型时，可能会存在批量和序列大小差异，这些差异需要处理。在我们的案例中，我们不需要担心序列大小，因为我们使用了一个将长度强制设置为512（在之前的集合中）的分词器。然而，作为transformers库的一部分，collator类仍然能够处理这种情况，我们将使用它们来确保批量大小的统一。
- en: So ultimately, the role of the `DataCollatorWithPadding` class is to take multiple
    examples of different lengths, provide padding if and when necessary, convert
    the inputs into tensors, and create attention masks if necessary.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`DataCollatorWithPadding`类的最终作用是接受不同长度的多个示例，在必要时提供填充，将输入转换为张量，并在必要时创建注意力掩码。
- en: In our case, we’re really only getting the conversion to tensors for input to
    the model, but it’s still good practice to use `DataCollatorWithPadding` if we
    want to change anything in the tokenization process later.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们实际上只得到了模型输入的转换为张量的过程，但如果我们希望在以后更改分词过程，使用`DataCollatorWithPadding`仍然是一个好的实践。
- en: 'Here’s the code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码示例：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Defining Metrics
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义指标
- en: 'Now, let’s define some metrics that we want to capture as we’re training the
    model. We’ll just do accuracy, where we compare the predicted value to the actual
    value. Here’s some simple code to achieve that:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一些我们希望在训练模型时捕捉的指标。我们将只做准确性，即比较预测值和实际值。以下是一些简单的代码来实现这一点：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It’s using `evaluate.load` from Hugging Face’s evaluate library, which provides
    a simple standardized interface that’s specifically designed for tasks like this
    one. It can handle the heavy lifting for us, instead of requiring us to roll our
    own metrics, and for an evaluate task, we simply pass it the set of predictions
    and the set of labels and have it do the computation. The evaluate library is
    prebuilt to handle a number of metrics, including f1, BLEU, and many others.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用的是Hugging Face的evaluate库中的`evaluate.load`，该库提供了一个简单标准化的接口，专门为这类任务设计。它可以为我们处理繁重的工作，而不是要求我们自行创建指标，对于评估任务，我们只需传递一组预测和一组标签，然后由它进行计算。evaluate库预先构建，可以处理包括f1、BLEU在内的多种指标。
- en: Configuring Training
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置训练
- en: Next up, we can configure *how* the model will retrain by using the `TrainingArguments`
    object. This offers a large variety of hyperparameters you can set—including those
    for the learning rate, weight decay, etc., as used by the `optimizer` and `loss`
    function. It’s designed to give you granular control over the learning process
    while abstracting away the complexity.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过使用`TrainingArguments`对象来配置模型如何重新训练。它提供了大量可以设置的超参数——包括那些用于`optimizer`和`loss`函数的学习率、权重衰减等——旨在在抽象复杂性的同时，给您提供对学习过程的精细控制。
- en: 'Here’s the set that I used for fine-tuning with IMDb:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我用于在IMDb上微调时使用的集合：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It’s important to note and tweak hyperparameters for different results. In addition
    to the aforementioned ones for the optimizer, you’ll want to consider the batch
    sizes. You can set different parameters for training or evaluation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意并调整超参数以获得不同的结果非常重要。除了上述用于优化器的参数外，您还希望考虑批大小。您可以针对训练或评估设置不同的参数。
- en: One very useful parameter—in particular for training sessions that are longer
    than the three epochs here—is `load_best_model_at_end`. Instead of always using
    the final checkpoint, it will keep track of the best checkpoint according to the
    specified metric (in this case, accuracy) and will load that one when it’s done.
    And because I set the `evaluation` and `save` strategies to `epoch`, it will only
    do this at the end of an epoch.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有用的参数——特别是对于超过这里三个epoch的训练会话——是`load_best_model_at_end`。它不会总是使用最后的检查点，而是会根据指定的指标（在这种情况下，准确性）跟踪最佳的检查点，并在完成后加载它。由于我将`evaluation`和`save`策略设置为`epoch`，它只会在epoch结束时这样做。
- en: 'Note also the `report_to` parameter: the training uses `weights and biases`
    as the backend for reporting by default. I set `report_to` to `none` to turn off
    this reporting. If you want to keep it, you’ll need a Weights and Biases API key.
    You can get this very easily from the status window or by going to the [Weights
    and Biases website](https://oreil.ly/yMX1A). As you train, you’ll be asked to
    paste in this API key. Be sure to do so before you walk away, particularly if
    you are paying for compute units on Colab.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意还有`report_to`参数：默认情况下，训练使用`weights and biases`作为报告的后端。我将`report_to`设置为`none`以关闭此报告。如果您想保留它，您将需要一个Weights
    and Biases API密钥。您可以从状态窗口或通过访问[Weights and Biases网站](https://oreil.ly/yMX1A)非常容易地获得这个密钥。在训练过程中，您将被要求粘贴此API密钥。确保在离开之前这样做，尤其是如果您在Colab上支付计算单元费用。
- en: There’s a wealth of parameters to experiment with, and being able to parameterize
    easily like this also allows you easily to do a neural architecture search with
    tools like [Ray Tune](https://oreil.ly/fDAhG).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多参数可以实验，并且能够像这样轻松参数化也允许您使用像[Ray Tune](https://oreil.ly/fDAhG)这样的工具轻松地进行神经架构搜索。
- en: Initializing the Trainer
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化训练器
- en: As with the training parameters, transformers give you a trainer class that
    you can use alongside them to encapsulate a full training cycle.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练参数一样，transformers提供了一个trainer类，您可以使用它来封装完整的训练周期。
- en: 'You initialize it with the model, the training arguments, the data, the collator,
    and the metrics strategy that you’ve previously initialized. All the previous
    steps build up to this. Here’s the code you’ll need:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要用模型、训练参数、数据、collator以及您之前初始化的度量策略来初始化它。所有之前的步骤都是为了达到这一步。以下是您需要的代码：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training and Evaluation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和评估
- en: With everything now set up, it becomes as simple as calling the `train()` method
    on the trainer to do the training and the `evaluate()` method to do the evaluation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切准备就绪，调用trainer上的`train()`方法进行训练，以及`evaluate()`方法进行评估就变得非常简单了。
- en: 'Here’s the code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As an example, while you could train this model with the free tiers in Google
    Colab, your experience in timing might vary. With the CPU alone, it can take many
    hours. I trained this model with the T4 High Ram GPU, which costs 1.6 compute
    units per hour. The entire training process was about 50 minutes, but I’ll round
    that up to an hour to include all the downloading and setup. At the time of writing,
    a pro Colab subscription gets one hundred compute Units with the US$9.99 per month
    subscription. You could also choose the A100 GPU, which is much faster (training
    took me about 12 minutes with it) but also more expensive, at about 6.8 compute
    units per hour.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，虽然您可以在Google Colab的免费层上训练此模型，但您的计时体验可能会有所不同。仅使用CPU，可能需要好几个小时。我用T4高RAM GPU训练了这个模型，每小时需要1.6个计算单元。整个训练过程大约需要50分钟，但我将其四舍五入到一小时，包括所有下载和设置。在撰写本文时，专业Colab订阅每月9.99美元可以获得100个计算单元。您还可以选择A100
    GPU，它要快得多（使用它训练我大约需要12分钟），但价格也更贵，每小时大约6.8个计算单元。
- en: 'After training, the results looked like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，结果看起来是这样的：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see quite high accuracy on the evaluation dataset (about 94%) after only
    three epochs, which is a good sign—but of course, there may be overfitting going
    on that would require a separate evaluation. But after about 12 minutes of work
    fine-tuning an LLM, we’re clearly moving in the right direction!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在仅经过三个epoch后，评估数据集上的准确率相当高（大约94%），这是一个好兆头——但当然，可能存在过拟合，这需要单独的评估。但在大约12分钟的LLM微调工作后，我们显然正在朝着正确的方向前进！
- en: Saving and Testing the Model
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和测试模型
- en: 'Once we’ve trained the model, it’s a good idea to save it out for future use,
    and the `trainer` object makes this easy:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了模型，保存它以供将来使用是个好主意，`trainer`对象使这变得很容易：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once we’ve saved the model, we can start using it. To that end, let’s create
    a helper function that takes in the input text, tokenizes it, and then turns those
    tokens into a set of input vectors of keys and values (k, v):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 保存模型后，我们就可以开始使用它了。为此，让我们创建一个辅助函数，该函数接受输入文本，对其进行标记化，然后将这些标记转换为键值（k, v）的一组输入向量：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can then use PyTorch in inference mode to get the outputs from those inputs
    and turn them into a set of predictions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用PyTorch的推理模式从这些输入中获取输出，并将它们转换成一系列预测：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The returned predictions will be a tensor with two dimensions. Neuron 0 is
    the probability that the prediction is negative, and neuron 1 is the probability
    that the prediction is positive. Therefore, we can look at the positive probability
    and return a sentiment and confidence with its values. We could also have done
    the same with the negative one; it’s purely arbitrary:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的预测结果将是一个具有两个维度的张量。神经元0是预测为负面的概率，神经元1是预测为正面的概率。因此，我们可以查看正面的概率，并返回一个带有其值的情感和置信度。我们也可以用同样的方式处理负面的预测；这完全是任意的：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now test the prediction with code like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以用如下代码测试预测：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And the output would look something like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果可能看起来像这样：
- en: '[PRE16] `Sentiment``:` `positive` `Confidence``:` `99.16``%` [PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE16] `情感`：`积极` `置信度`：`99.16%` [PRE17]'
- en: '`` `We can see that this statement is positive, with high confidence!    In
    this process, you can see how, step by step, you can fine-tune an existing LLM
    on new data to turn it into a classification engine! In many circumstances, this
    may be overkill (and training your own model instead of fine-tuning an LLM may
    be quicker and cheaper), but it’s certainly worth evaluating this process. Sometimes,
    even untuned LLMs will work well for classification! In my experience, using the
    general artificial-understanding nature of LLMs will lead to the creation of far
    more effective classifiers with stronger results.` ``  [PRE18]`# Prompt-Tuning
    an LLM    A lightweight alternative to fine-tuning is *prompt tuning,* in which
    you can adapt a model to specific tasks. With prompt tuning, you do this by prepending
    trainable *soft prompts* to each input instead of modifying the model weights.
    These soft prompts will then be optimized during training.    These soft prompts
    are like learned instructions that guide the model’s behavior. Unlike discrete
    text prompts (such as `Classify the sentiment`), the idea of soft prompts is that
    they exist in the model’s embedding space as continuous vectors. So, for example,
    when processing “This movie was great,” the model would see “[V1][V2]…[V20]This
    movie was great.” In this case, [V1][V2]...[V20] are vectors that will help steer
    the model toward the desired classification.    Ultimately, the advantage here
    is efficiency. So instead of fine-tuning a model, amending its weights for each
    task, and saving the entire model for reuse, you only need to save the soft prompt
    vectors. These are much smaller, and they can help you have a suite of fine-tunes
    that you can easily use to guide the model to a specific task without needing
    to manage multiple models.    Prompt tuning like this can actually match or exceed
    the performance of full fine-tuning, particularly with larger models, and it’s
    significantly more efficient.    Now, let’s explore how to prompt-tune the BART
    LLM with the IMDb dataset in direct comparison to the fine-tuning earlier in this
    chapter.    ## Preparing the Data    Let’s start by preparing our data, loading
    it from the IMDb dataset, and setting up the virtual tokens. Here’s the code you’ll
    need:    [PRE19]    This will also tokenize our incoming examples so you should
    note that the maximum length of any example will now be reduced by the number
    of virtual tokens. So, for example, with BERT, we have a maximum length of 512,
    but if we’re going to have 20 virtual tokens, then the sequence maximum length
    will now be 492.    We’ll now load a subset of the data and try with 5,000 examples,
    instead of 25,000\. You can experiment with this number and trade off smaller
    amounts for faster training against larger amounts for better accuracy.    First,
    we’ll create the indices that we want to take from the dataset for training, and
    we’ll test them. Think of these as pointers to the records we’re interested in.
    We’re randomly sampling here:    [PRE20]    Then, the last two lines define the
    mapping function, which simply takes the values from the dataset we’re interested
    in and tokenizes them. We’ll see that in the next step.    ## Creating the Data
    Loaders    Now that we have sets of tokenized training and test data, we want
    to turn them into data loaders. We’ll do this by first selecting the raw examples
    from the underlying data that match the content in our indices:    [PRE21]    Then,
    we’ll set the format of the data that we’re interested in. There may be many columns
    in a dataset, but you won’t use them all for training. In this case, we’ll want
    the `input_ids`, which are the tokenized versions of our input content; the `attention_mask`,
    which is a set of vectors that tells us which tokens in the `input_ids` we should
    be interested in (this has the effect of filtering out padding or other nonsemantic
    tokens); and the label:    [PRE22]    Now, we can specify the DataLoader that
    takes these training and test sets. I have a large batch size here because I was
    testing on a 40Gb GRAM GPU in Colab. In your environment, you may need to adjust
    these:    [PRE23]    Now that the data is processed and loaded into DataLoaders,
    we can go to the next step: defining the model.    ## Defining the Model    First,
    let’s see how to instantiate the model, and then we can go back to the raw definition.
    Typically, in our code, once we’ve set up our DataLoaders, we’ll want to create
    an instance of the model. We’ll use code like this:    [PRE24]    This keeps it
    nice and simple, and we’ll encapsulate the underlying BERT in an override for
    a prompt-tuning version. Now, as nice as it would be for transformers to have
    one, they don’t, so we need to create this class for ourselves.    As we would
    with any PyTorch class that defines a model, we’ll create it with an `__init__`
    method to set it up and a forward method that PyTorch’s training loop will call
    during the forward pass. So, let’s start with the `__init__` method and the class
    definition:    [PRE25]    There’s a lot going on here, so let’s break it down
    little by little. First of all, I set the defaults for the `num_virtual_tokens`
    to 50 and the `max_length` default to 512\. If you don’t specify your own defaults
    when you instantiate the class, you’ll get these values. In this case, the calling
    code sets them to 20 and 512, respectively, but you’re free to experiment.    Next,
    the code sets up the transformers `AutoModelForSequenceClassification` class to
    get BERT:    [PRE26]    As with fine-tuning for IMDb, we’re interested in training
    the model to recognize two labels, so they’re set up here. However, one difference
    from fine-tuning is that we’re not going to change any of the weights within the
    BERT model itself, so we set that we don’t want gradients and freeze it like this:    [PRE27]    The
    secret sauce in generating the soft prompts that we’re going to use comes at the
    end of the init. We’ll create a vector to contain our number of virtual tokens,
    and I just initialized it with random tokens from the vocabulary. There are smarter
    things that we might do here to make training more efficient over time, but for
    the sake of simplicity, let’s go with this:    [PRE28]    The pretrained BERT
    model in transformers comes with embeddings, so we can use them to turn our list
    of random tokens into embeddings:    [PRE29]    Importantly, we should now specify
    that the `prompt_embeddings` are parameters of the neural network. This will be
    important later, when we define the optimizer. We recently specified that all
    of the BERT parameters were frozen, but *these* parameters are not part of that
    and thus are not frozen, so they will be tweaked by the optimizer during training:    [PRE30]    We
    have now initialized a subclassed version of the tunable BERT, specified that
    we don’t want to amend its gradients, and created a set of soft prompts that we
    will append to the examples as we’re training—and we’ll tweak only those soft
    prompts to soft-tune the two output neurons.    Now, let’s look at the `forward`
    function that will be called during the forward pass at training time. Given that
    we’ve set up everything, this is pretty straightforward:    [PRE31]    Let’s look
    at it step-by-step. During the forward pass in training, this function will be
    passed batches of data. Therefore, we need to understand what the size of this
    batch is and then extract the `input_ids` (the tokens for the values read from
    the dataset) and the attention mask for that particular ID:    [PRE32]    We’ll
    also need to convert the `input_ids` into embeddings:    [PRE33]    Our soft prompts
    are also tokenized sentences. Originally, they were initialized to random words,
    and we’ll see over time that they’ll adjust appropriately. But for this step,
    these tokens need to be converted to embeddings:    [PRE34]    The `expand` method
    just adds the batch size to the prompt embeddings. When we defined the class,
    we didn’t know how large each batch coming in would be (and the code is written
    to let you tweak that based on the size of your available memory), so using `expand(batch_size,
    –1, –1)` turns the vector of prompt embeddings, which was of shape `[1, num_prompt_tokens,
    embedding_dimensions]`, into `[batch_size, num_prompt_tokens, embedding_dimensions]`.    Our
    soft prompt tuning involved prepending the soft embeddings to the embeddings for
    the actual input data, so we do that with this:    [PRE35]    BERT uses an `attention_mask`
    to filter out the tokens we don’t want to worry about at training or inference
    time, which are usually the padding tokens. But we want BERT to pay attention
    to all of the soft prompt tokens, so we’ll set the attention mask for them to
    be all 1s and then append that to the incoming attention mask(s) for the training
    data. Here’s the code:    [PRE36]    Now that we’ve done all our tuning, we need
    to pass the data to the model to have it optimize and calculate the loss:    [PRE37]    We
    will see how this data is used in the training loop, next.    ## Training the
    Model    The key to this training is that we’re going to do a full, normal training
    loop but in a special circumstance. In this case, we previously froze *everything*
    in the BERT model, *except* for the soft prompts that we defined as model parameters.
    Therefore, say we define the optimizer like this:    [PRE38]    In this case,
    we’re using standard code, telling it to tweak the model’s parameters. But the
    only ones that are available to tune are the soft prompts, so this should be quick!    Note
    that the value for the learning rate is quite large. This helps the system learn
    quickly, but in a real system, you’d likely want the value to be smaller—or at
    least adjustable, starting large and then shrinking in later epochs.    So now,
    let’s get into training. First, we’ll set up the training loop:    [PRE39]    ###
    Managing data batches    For each batch, we’ll get the columns (`input_ids` and
    `attention_masks`) as well as the labels and pass them to the model:    [PRE40]    This
    looks a little different from ones earlier in this book, but it’s pretty much
    doing the same thing. The `tqdm` code just gives us a status bar because we’re
    training. We read the data batch by batch, but we want the data to be on the same
    device as the model. So, for example, if the model is running on a GPU, we want
    it to access data in the GPU’s memory. Therefore, this line will iterate through
    each column, reading the key and passing the value to the device:    [PRE41]    It
    redefines the batch that was read in to ensure that the data is on the same device
    as the model. But we don’t want the labels to be in the batch because the model
    expects them to be fed in separately, so we remove them from the batch with the
    `pop()` method:    [PRE42]    Now, we can use the shorthand of `**batch` to pass
    the set of input values (in this case, the `input_ids` and the `attention_mask`)
    to the forward method of the model and unpack the dictionary along with the labels,
    like this:    [PRE43]    ### Handling the loss    The forward pass sends the data
    to the model and gets the loss back. We use this to update our overall loss, and
    we can then backward pass:    [PRE44]    With the gradients flowing back, the
    optimizer can now do its job.    ### Optimizing for loss    Remembering that the
    `model.parameters()` will only manage the *trainable unfrozen* parameters, we
    can now call the optimizer. I added something called *gradient clipping* here
    to make the training a little more efficient, but the rest is just calling the
    optimizer’s next step and then zeroing out the gradients so we can use them next
    time:    [PRE45]    ###### Note    The idea behind *gradient clipping* is that
    sometimes, during backpropagation, the gradients can be too large and the optimizer
    might take very large steps. This can lead to a problem called *exploding gradients*,
    in which the value changes hide the nuances of what might be learned. But clipping
    scales the gradients down if their values grow too large, and in a situation like
    this one, they may not even be necessary.    ## Evaluation During Training    We
    also have a set of test data, so we can evaluate how the model performs during
    the training cycle. In each epoch, once the forward and backward passes are done
    and the model parameters are reset, we can switch the model into evaluation mode
    and then start passing all of the test data through it to get inference. We’ll
    also compare the results of the inference against the actual labels to calculate
    accuracy:    [PRE46]    Then, we’ll have similar code—but this time, it will be
    to read the eval batches, turn them into outputs with labels, and get predictions
    and loss values from the model:    [PRE47]    Once we’ve calculated these values,
    then at the end of each epoch, we can report on them and on training loss.    ##
    Reporting Training Metrics    During training in each epoch, we calculated the
    training loss, so we can now get the average across all records. We can do the
    same thing with the validation loss and (of course) with the validation accuracy
    and then report on them:    [PRE48]    Running this training for three epochs
    gives us this:    [PRE49]    This was done on an A100 in Colab with 40 Gb of GRAM,
    and as you can see, each epoch only took about 1 minute to train and 30 seconds
    to evaluate.    By the end, the average training loss had dropped from about 0.65
    to 0.58\. The accuracy was 0.8736\. So, it’s likely overfitting because we only
    trained for three epochs.    ## Saving the Prompt Embeddings    What’s really
    nice about this approach is that you can simply save out the prompt embeddings
    when you’re done. You can also load them back in for inference later, as you’ll
    see in the next section:    [PRE50]    What I find really cool about this is that
    this file is relatively small (61 K), and it doesn’t require you to amend the
    underlying model in any way. Thus, in an application, you could potentially have
    a number of these prompt-tuning files and hot-swap and replace them as needed
    so that you can have multiple models that you can orchestrate, which is the basis
    for an agentic solution.    ## Performing Inference with the Model    To perform
    inference with a prompt-tuned model, you’ll simply define the model with the soft
    prompts and then, instead of training them, load the pretrained soft prompts back
    from disk. We’ll explore that in this section. If you don’t want to train your
    own model, then [in the download](https://github.com/lmoroney/PyTorch-Book-FIles),
    I’ve provided soft prompts from a version of the model that was trained for 30
    epochs instead of 3.    For tidier encapsulation, I created a class that is similar
    to the one we used for training but that is just for inference. I call it `PromptTunedBERTInference`,
    and here’s its initializer:    [PRE51]    It’s very similar to the initializer
    for the trainable one, except for a couple of important points. The first is that
    because we’re *only* using it for inference, I’ve set it into eval mode:    [PRE52]    The
    second is that we don’t need to train the embeddings and do all the associated
    plumbing—instead, we just load them from the specified path:    [PRE53]    And
    that’s it! As you can see, it’s quite lightweight and pretty straightforward.
    It won’t have a `forward` function because we’re not training it, but let’s add
    a `predict` function that encapsulates doing inference with it.    ### The predict
    function    The job of the `predict` function is to take in the string(s) that
    we want to perform inference with, tokenize it (them), and then pass it to the
    model with the soft tokens prepended. Let’s take a look at the code, piece by
    piece.    First, let’s define it and have it accept text that it will then tokenize:    [PRE54]    The
    text will be tokenized up to the maximum length, less the size of the soft prompt,
    and then each of the items in the input will be loaded into a dictionary. Note
    that the tokenizer will return multiple columns for the text—usually, the tokens
    and the attention mask—so we’ll follow this approach to turn them into a set of
    key-value pairs that are easy for us to work with later.    Now that we have our
    inputs, it’s time to pass them to the model. We’ll start by putting `torch` into
    `no_grad()` mode because we’re not interested in training gradients. We’ll then
    get the embeddings for each of our tokens:    [PRE55]    We have an attention
    mask for the input that’s generated by the tokenizer, but we don’t have one for
    the soft prompt. So, let’s create one and then append it to the input attention
    mask:    [PRE56]    Now that we have everything in place, we can pass our data
    to the model to get our inferences back:    [PRE57]    The outputs will be the
    logits from the two neurons, one representing positive sentiment and the other
    negative. We can then Softmax these to get the prediction:    [PRE58]    ### Usage
    example    Using this class for a prediction is then pretty straightforward. We
    create an instance of the class and pass a string to it to get results. The results
    will contain a prediction and a confidence value, which we can then output:    [PRE59]    One
    note you might see with prompt tuning is low confidence values that can lead to
    mis-predictions, especially with binary classifiers like this one. It’s good to
    explore your inference to make sure that it’s working well, and there are also
    techniques that you could explore to ensure that the logits are giving the values
    you want. These include setting the temperature of the Softmax, using more prompt
    tokens to give the model more capacity, and initializing the prompt tokens with
    sentiment-related words (instead of random tokens, like we did here).    # Summary    In
    this chapter, we explored different methods for customizing LLMs with our own
    data. We looked at two main approaches: traditional fine-tuning and prompt tuning.    Using
    the IMDb dataset, you saw how to fine-tune BERT for sentiment analysis and walked
    through all the steps—from data preparation, to model configuration, to training
    and evaluation. The model achieved an impressive 95% accuracy in sentiment classification
    in just a few epochs.    However, fine-tuning may not be appropriate in all cases,
    and to that end, you explored a lightweight alternative called prompt tuning.
    Instead of modifying model weights, the idea here was to prepend trainable soft
    prompts to inputs, which are optimized during training. This approach provides
    significant advantages in that it can be much faster and it doesn’t change the
    underlying model. In this case, the tuned prompts could be saved (and they were
    only a few Kb) and then reloaded to program the model to perform the desired task.
    You then went through a full implementation, showing you how to create, train,
    and save these soft prompts, plus load them back to perform inference.    In the
    next chapter, we’ll explore how you can serve LLMs, including customized ones.
    I’ll explain how to do this in your own data center by using Ollama, which is
    a powerful tool for handling the serving and management of LLMs. You’ll learn
    how to take models and turn them into services, and we’ll also explore how to
    set up Ollama and use it over HTTP to talk with models in your data center.[PRE60]``'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
