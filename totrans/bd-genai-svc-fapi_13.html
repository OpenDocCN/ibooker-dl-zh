<html><head></head><body><section data-pdf-bookmark="Chapter 9. Securing AI Services" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch09">
<h1><span class="label">Chapter 9. </span>Securing AI Services</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1031">
<h1>Chapter Goals</h1>
<p><a data-primary="securing AI services" data-type="indexterm" id="ix_ch09-asciidoc0"/>In this chapter, you will learn about:</p>

<ul>
<li>
<p>How to protect against adversarial attempts and the misuse and abuse of your GenAI services</p>
</li>
<li>
<p>How to implement your own ethical and safety guardrails, evaluation, and content filtering layers to moderate usage</p>
</li>
<li>
<p>How to implement usage moderation, stream throttling, and various API rate-limiting strategies to reduce server load</p>
</li>
</ul>
</div></aside>

<p>In earlier chapters, you learned how to build GenAI services that serve various AI generators while supporting concurrency and data streaming in real time.
Additionally, you integrated external systems like databases and implemented your own authentication and authorization mechanisms.
Finally, you wrote a test suite to verify the functionality and performance of your entire system.</p>

<p>In this chapter, you’ll learn how to implement usage moderation and abuse-protection mechanisms to secure your GenAI services.</p>






<section data-pdf-bookmark="Usage Moderation and Abuse Protection" data-type="sect1"><div class="sect1" id="id135">
<h1>Usage Moderation and Abuse Protection</h1>

<p><a data-primary="securing AI services" data-secondary="usage moderation and abuse protection" data-type="indexterm" id="ix_ch09-asciidoc1"/>When deploying your GenAI services, you’ll need to consider how your services will be misused and abused by malicious users. This is essential to protect user safety and your own reputation.
You won’t know how the users will use your system, so you need to assume the worst and implement <em>guardrails</em> to protect against any misuse or abuse.</p>

<p>According to a <a href="https://oreil.ly/ihmzR">recent study on nefarious applications of GenAI</a>, your services may potentially be used with <em>malicious intents</em>, as described in <a data-type="xref" href="#malicious_intents">Table 9-1</a>.</p>
<table class="striped" id="malicious_intents">
<caption><span class="label">Table 9-1. </span>Malicious intents behind abusing GenAI services</caption>
<thead>
<tr>
<th>Intent</th>
<th>Examples</th>
<th>Real-world cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><strong>Dishonesty</strong></p><p>Supporting lies and untruthfulness</p></td>
<td><p>Plagiarism, faking competency and knowledge, document forgery, cheating in exams and in interviews, etc.</p></td>
<td><p>Increasing cases of students cheating with AI at UK and Australian universities<sup><a data-type="noteref" href="ch09.html#id1032" id="id1032-marker">a</a></sup></p></td>
</tr>
<tr>
<td><p><strong>Propaganda</strong></p><p>Skewing perceptions of reality to advance an agenda</p></td>
<td><p>Impersonating others, promoting extremism, influencing campaigns, etc.</p></td>
<td><p>Fake AI news anchors spreading misinformation or propaganda<sup><a data-type="noteref" href="ch09.html#id1033" id="id1033-marker">b</a></sup></p></td>
</tr>
<tr>
<td><p><strong>Deception</strong></p><p>Misleading others and creating false impressions</p></td>
<td><p>Generating fake reviews, scam ads and phishing emails, and synthetic profiles (i.e., sockpuppeting), etc.</p></td>
<td><p>Engineering firm Arup revealed as a victim of a $25 million deepfake scam<sup><a data-type="noteref" href="ch09.html#id1034" id="id1034-marker">c</a></sup></p></td>
</tr>
</tbody>
<tbody><tr class="footnotes"><td colspan="3"><p data-type="footnote" id="id1032"><sup><a href="ch09.html#id1032-marker">a</a></sup> Sources: <em>Times Higher Education</em> and <em>The Guardian</em></p><p data-type="footnote" id="id1033"><sup><a href="ch09.html#id1033-marker">b</a></sup> Sources: <em>The Guardian</em>, <em>MIT Technology Review</em>, and <em>The Washington Post</em></p><p data-type="footnote" id="id1034"><sup><a href="ch09.html#id1034-marker">c</a></sup> Sources: CNN and <em>The Guardian</em></p></td></tr></tbody></table>

<p>The same study categorizes GenAI application abuse into the following:</p>

<ul>
<li>
<p><em>Misinformation and disinformation</em> to spread propaganda and fake news</p>
</li>
<li>
<p><em>Bias amplification and discrimination</em> to advance racist agendas and societal 
<span class="keep-together">discrimination</span></p>
</li>
<li>
<p><em>Malicious content generation</em> by creating toxic, deceptive, and radicalizing content</p>
</li>
<li>
<p><em>Data privacy attacks</em> to fill in gaps in stolen private data and leak sensitive 
<span class="keep-together">information</span></p>
</li>
<li>
<p><em>Automated cyberattacks</em> to personalize phishing and ransomware attacks</p>
</li>
<li>
<p><em>Identity theft and social engineering</em> to increase the success rate of scams</p>
</li>
<li>
<p><em>Deepfakes and multimedia manipulation</em> to make a profit and skew perceptions of reality and social beliefs</p>
</li>
<li>
<p><em>Scam and fraud</em> by manipulating stock markets and crafting targeted scams</p>
</li>
</ul>

<p>This may not be an exhaustive list but should give you a few ideas on what usage moderation measures to consider.</p>

<p><a href="https://oreil.ly/jbG01">Another study on the taxonomy of GenAI misuse tactics</a> investigated abuse by modality and found that:</p>

<ul class="less_space pagebreak-before">
<li>
<p><em>Audio and video generators</em> were used for the majority of impersonation attempts.</p>
</li>
<li>
<p><em>Image and text generators</em> were used for the majority of sockpuppeting, content farming for opinion manipulation at scale, and falsification attempts.</p>
</li>
<li>
<p><em>Image and video generators</em> were used for the majority of steganography, (i.e., hiding coded messages in model outputs), and nonconsensual intimate content (NCII) generation attempts.</p>
</li>
</ul>

<p>If you’re building services supporting such modalities, you should consider their associated forms of abuse and implement relevant protection mechanisms.</p>

<p>Aside from misuse and abuse, you’ll also need to consider security vulnerabilities.</p>

<p>Securing GenAI services is still an area of research at the time of writing.
For instance, if your services leverage LLMs, OWASP has categorized the <a href="https://oreil.ly/4zob2">top 10 LLM vulnerabilities</a>, as shown in <a data-type="xref" href="#llm_vulnerabilities">Table 9-2</a>.</p>
<table class="striped" id="llm_vulnerabilities">
<caption><span class="label">Table 9-2. </span>OWASP top 10 LLM vulnerabilities</caption>
<thead>
<tr>
<th>Risk</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Prompt injection</p></td>
<td><p>Manipulating inputs to control the LLM’s responses leading to unauthorized access, data breaches, and compromised decision-making.</p></td>
</tr>
<tr>
<td><p>Insecure output handling</p></td>
<td><p>Failing to sanitize or validate LLM outputs causing remote code execution on downstream systems.</p></td>
</tr>
<tr>
<td><p>Training data poisoning</p></td>
<td><p>Injecting data in sources that models get trained on to compromise security, accuracy, or ethical behavior. Open source models and RAG services that rely on web data are most prone to these attacks.</p></td>
</tr>
<tr>
<td><p>Model denial of service</p></td>
<td><p>Causing service disruption and cost explosions by overloading the LLMs with heavy payloads and concurrent requests.</p></td>
</tr>
<tr>
<td><p>Supply chain vulnerabilities</p></td>
<td><p>Causing various components, including data sources, to be compromised, undermining system integrity.</p></td>
</tr>
<tr>
<td><p>Sensitive information leakage</p></td>
<td><p>Leading to accidental exposure of private data, legal liabilities and loss of competitive advantage.</p></td>
</tr>
<tr>
<td><p>Insecure plug-in design</p></td>
<td><p>Vulnerabilities in third-party integrations cause remote code execution.</p></td>
</tr>
<tr>
<td><p>Excessive agency</p></td>
<td><p>Where LLMs have too much autonomy to take actions can lead to unintended consequences and harmful actions.</p></td>
</tr>
<tr>
<td><p>Overreliance on LLM</p></td>
<td><p>Compromising decision-making, contributing to security vulnerabilities and legal liabilities.</p></td>
</tr>
<tr>
<td><p>Model theft</p></td>
<td><p>Related to unauthorized copying or usage of your models.</p></td>
</tr>
</tbody>
</table>
<div data-type="tip"><h6>Tip</h6>
<p>Similar vulnerabilities exist for other types of GenAI systems such as image, audio, video, and geometry generators.</p>

<p>I recommend researching and identifying software vulnerabilities relevant to your own use cases.</p>
</div>

<p>Without guardrails, your services can be abused to cause personal and financial harm, identity theft, economic damage, spread misinformation, and contribute to societal problems.
As a result, it’s crucial to implement several safety measures and guardrails to protect your services against such attacks.</p>

<p>In the next section, you’ll learn usage moderation and security measures you can implement to protect your GenAI services prior to deployment.<a data-startref="ix_ch09-asciidoc1" data-type="indexterm" id="id1035"/></p>
</div></section>






<section data-pdf-bookmark="Guardrails" data-type="sect1"><div class="sect1" id="id136">
<h1>Guardrails</h1>

<p><a data-primary="guardrails" data-type="indexterm" id="ix_ch09-asciidoc2"/><a data-primary="securing AI services" data-secondary="guardrails" data-type="indexterm" id="ix_ch09-asciidoc3"/><em>Guardrails</em> refer to <em>detective controls</em> that aim to guide your application toward the intended outcomes.
They are incredibly diverse and can be configured to fit any situation that may go wrong with your GenAI systems.</p>

<p>As an example, <em>I/O guardrails</em> are designed to verify data entering a GenAI model and outputs sent to the downstream systems or users.
Such guardrails can flag inappropriate user queries and validate output content against toxicity, hallucinations, or banned topics.
<a data-type="xref" href="#guardrails">Figure 9-1</a> shows how an LLM system looks once you add I/O guardrails to it.</p>

<figure><div class="figure" id="guardrails">
<img alt="bgai 0901" src="assets/bgai_0901.png"/>
<h6><span class="label">Figure 9-1. </span>Comparison of an LLM system without and with guardrails</h6>
</div></figure>

<p>You don’t have to implement guardrails from scratch.
At the time of writing, prebuilt open source guardrail frameworks exist like NVIDIA NeMo Guardrails, LLM-Guard, and Guardrails AI to protect your services.
However, they may require learning framework-related languages and have a trade-off of slowing down your services and bloating your application due to various external dependencies.</p>

<p>Other commercial guardrails available on the market, such as Open AI’s Moderation API, Microsoft Azure AI Content Safety API, and Google’s Guardrails API are either not open source or lack details and contents to measure quality constraints.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Guardrails remain an active area of research.
While such defenses can counter some attacks, powerful attacks backed by AI can still bypass them.
This may lead to an <a href="https://oreil.ly/xlUmw">ongoing and endless loop of assaults and defenses</a>.</p>
</div>

<p>While engineering application-level I/O guardrails may not provide perfect protection, upcoming GenAI models may include baked-in guardrails inside the model to improve security guarantees.
However, such guardrails may have a performance impact on response times by introducing latency to the system.</p>








<section data-pdf-bookmark="Input Guardrails" data-type="sect2"><div class="sect2" id="id137">
<h2>Input Guardrails</h2>

<p><a data-primary="guardrails" data-secondary="input guardrails" data-type="indexterm" id="ix_ch09-asciidoc4"/><a data-primary="input guardrails" data-type="indexterm" id="ix_ch09-asciidoc5"/><a data-primary="securing AI services" data-secondary="guardrails" data-tertiary="input guardrails" data-type="indexterm" id="ix_ch09-asciidoc6"/>The purpose of input guardrails is to prevent malicious or inappropriate content from reaching your model. <a data-type="xref" href="#guardrails_input">Table 9-3</a> shows common input guardrails.</p>
<table class="striped" id="guardrails_input">
<caption><span class="label">Table 9-3. </span>Common input guardrails</caption>
<thead>
<tr>
<th>Input guardrails</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><strong>Topical</strong></p><p>Steer inputs away from off-topic or sensitive content.</p></td>
<td><p>Preventing a user from discussing political topics and explicit content.</p></td>
</tr>
<tr>
<td><p><strong>Direct prompt injection</strong>
(jail-breaking)</p><p>Prevent users from revealing or overriding system prompts and secrets.
The longer the input content, the more prone your system will be to these attacks.</p></td>
<td><p>Blocking attempts to override system prompts and manipulating the system into revealing internal API keys or configuration settings.<sup><a data-type="noteref" href="ch09.html#id1036" id="id1036-marker">a</a></sup></p></td>
</tr>
<tr>
<td><p><strong>Indirect prompt injection</strong></p><p>Prevent acceptance of malicious content from external sources such as files or websites that may cause model confusion or remote code execution on downstream systems.</p><p>Malicious content may be invisible to the human eye and encoded within input text or images.</p></td>
<td><p>Sanitizing encoded payloads in upload images, hidden characters or prompt overrides in uploaded documents, hidden scripts in remote URLs or even YouTube video transcripts.</p></td>
</tr>
<tr>
<td><p><strong>Moderation</strong></p><p>Comply with brand guidelines, legal, and branding requirements.</p></td>
<td><p>Flag and refuse invalid user queries if user queries include mentions of profanity, competitor, explicit content, personally identifiable information (PII), self-harm, etc.</p></td>
</tr>
<tr>
<td><p><strong>Attribute</strong></p><p>Validate input properties.</p></td>
<td><p>Check query length, file size, choices, range, data format and structure, etc.</p></td>
</tr>
</tbody>
<tbody><tr class="footnotes"><td colspan="2"><p data-type="footnote" id="id1036"><sup><a href="ch09.html#id1036-marker">a</a></sup> Although guardrails are useful, best practice is to avoid giving your GenAI models direct knowledge of secrets or sensitive configuration settings in the first place.</p></td></tr></tbody></table>

<p>The input guardrails can also be combined with content sanitizers to clean bad inputs.</p>

<p>If you want to implement your own guardrails, you can start off with using advanced prompt engineering techniques within your system prompts.
Additionally, you can use auto-evaluation techniques, (i.e., AI models).</p>

<p><a data-type="xref" href="#guardrail_topical_prompt">Example 9-1</a> shows an example system prompt for an AI guardrail auto-evaluator to reject off-topic queries.</p>
<div data-type="example" id="guardrail_topical_prompt">
<h5><span class="label">Example 9-1. </span>Topical input guardrail system prompt</h5>

<pre data-code-language="python" data-type="programlisting"><code class="n">guardrail_system_prompt</code> <code class="o">=</code> <code class="s2">"""</code>

<code class="s2">Your role is to assess user queries as valid or invalid</code>

<code class="s2">Allowed topics include:</code>

<code class="s2">1. API Development</code>
<code class="s2">2. FastAPI</code>
<code class="s2">3. Building Generative AI systems</code>

<code class="s2">If a topic is allowed, say 'allowed' otherwise say 'disallowed'</code>
<code class="s2">"""</code></pre></div>

<p>You can see an implementation of an input topical guardrail in <a data-type="xref" href="#guardrail_topical">Example 9-2</a> using the LLM auto-evaluation technique.</p>
<div data-type="example" id="guardrail_topical">
<h5><span class="label">Example 9-2. </span>Topical input guardrail</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">re</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Annotated</code>
<code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">AsyncOpenAI</code>
<code class="kn">from</code> <code class="nn">pydantic</code> <code class="kn">import</code> <code class="n">AfterValidator</code><code class="p">,</code> <code class="n">BaseModel</code><code class="p">,</code> <code class="n">validate_call</code>

<code class="n">guardrail_system_prompt</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">...</code><code class="s2">"</code>

<code class="k">class</code> <code class="nc">LLMClient</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">system_prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code><code class="p">:</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">client</code> <code class="o">=</code> <code class="n">AsyncOpenAI</code><code class="p">(</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">system_prompt</code> <code class="o">=</code> <code class="n">system_prompt</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">invoke</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">user_query</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code> <code class="o">|</code> <code class="kc">None</code><code class="p">:</code>
        <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
            <code class="n">model</code><code class="o">=</code><code class="s2">"</code><code class="s2">gpt-4o</code><code class="s2">"</code><code class="p">,</code>
            <code class="n">messages</code><code class="o">=</code><code class="p">[</code>
                <code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">system</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="bp">self</code><code class="o">.</code><code class="n">system_prompt</code><code class="p">}</code><code class="p">,</code>
                <code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="n">user_query</code><code class="p">}</code><code class="p">,</code>
            <code class="p">]</code><code class="p">,</code>
            <code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
        <code class="p">)</code>
        <code class="k">return</code> <code class="n">response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code>

<code class="nd">@validate_call</code>
<code class="k">def</code> <code class="nf">check_classification_response</code><code class="p">(</code><code class="n">value</code><code class="p">:</code> <code class="nb">str</code> <code class="o">|</code> <code class="kc">None</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code> <a class="co" href="#callout_securing_ai_services_CO1-1" id="co_securing_ai_services_CO1-1"><img alt="1" src="assets/1.png"/></a>
    <code class="k">if</code> <code class="n">value</code> <code class="ow">is</code> <code class="kc">None</code> <code class="ow">or</code> <code class="ow">not</code> <code class="n">re</code><code class="o">.</code><code class="n">match</code><code class="p">(</code><code class="sa">r</code><code class="s2">"</code><code class="s2">^(allowed|disallowed)$</code><code class="s2">"</code><code class="p">,</code> <code class="n">value</code><code class="p">)</code><code class="p">:</code>
        <code class="k">raise</code> <code class="ne">ValueError</code><code class="p">(</code><code class="s2">"</code><code class="s2">Invalid topical guardrail response received</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">value</code>

<code class="n">ClassificationResponse</code> <code class="o">=</code> <code class="n">Annotated</code><code class="p">[</code>
    <code class="nb">str</code> <code class="o">|</code> <code class="kc">None</code><code class="p">,</code> <code class="n">AfterValidator</code><code class="p">(</code><code class="n">check_classification_response</code><code class="p">)</code>
<code class="p">]</code>

<code class="k">class</code> <code class="nc">TopicalGuardResponse</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">)</code><code class="p">:</code>
    <code class="n">classification</code><code class="p">:</code> <code class="n">ClassificationResponse</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">is_topic_allowed</code><code class="p">(</code><code class="n">user_query</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">TopicalGuardResponse</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">LLMClient</code><code class="p">(</code><code class="n">guardrail_system_prompt</code><code class="p">)</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">user_query</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">TopicalGuardResponse</code><code class="p">(</code><code class="n">classification</code><code class="o">=</code><code class="n">response</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_securing_ai_services_CO1-1" id="callout_securing_ai_services_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Handle cases where the LLM doesn’t return a valid classification</p></dd>
</dl></div>
<div data-type="tip"><h6>Tip</h6>
<p>Using the technique shown in <a data-type="xref" href="#guardrail_topical">Example 9-2</a>, you can implement auto-evaluators to check for jail-breaking and prompt injection attempts or even detect the presence of PII and profanity in the inputs.</p>
</div>

<p>As discussed in <a data-type="xref" href="ch05.html#ch05">Chapter 5</a>, you can leverage async programming as much as possible even when using auto-evaluation techniques in your guardrails.
This is because AI guardrails require sending multiple model API calls per user query.
To improve user experience, you can run these guardrails in parallel to the model inference process.</p>

<p>Once you have an auto-evaluator guardrail for checking allowed topics, you can execute it in parallel to your data generation<sup><a data-type="noteref" href="ch09.html#id1037" id="id1037-marker">1</a></sup>  using <code>asyncio.wait</code>, as shown in <a data-type="xref" href="#guardrail_concurrent_execution">Example 9-3</a>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Be mindful that implementing async guardrails may trigger model provider API rate-limiting and throttling mechanisms.
Depending on your application requirements, you may want to request higher rate limits or reduce the rate of API calls within a short time frame.</p>
</div>
<div data-type="example" id="guardrail_concurrent_execution">
<h5><span class="label">Example 9-3. </span>Running AI guardrails in parallel to response generation</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">asyncio</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Annotated</code>
<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">Depends</code>
<code class="kn">from</code> <code class="nn">loguru</code> <code class="kn">import</code> <code class="n">logger</code>

<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">invoke_llm_with_guardrails</code><code class="p">(</code><code class="n">user_query</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="n">topical_guardrail_task</code> <code class="o">=</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">create_task</code><code class="p">(</code><code class="n">is_topic_allowed</code><code class="p">(</code><code class="n">user_query</code><code class="p">)</code><code class="p">)</code>
    <code class="n">chat_task</code> <code class="o">=</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">create_task</code><code class="p">(</code><code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">user_query</code><code class="p">)</code><code class="p">)</code>

    <code class="k">while</code> <code class="kc">True</code><code class="p">:</code>
        <code class="n">done</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="k">await</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">wait</code><code class="p">(</code>
            <code class="p">[</code><code class="n">topical_guardrail_task</code><code class="p">,</code> <code class="n">chat_task</code><code class="p">]</code><code class="p">,</code>
            <code class="n">return_when</code><code class="o">=</code><code class="n">asyncio</code><code class="o">.</code><code class="n">FIRST_COMPLETED</code><code class="p">,</code>
        <code class="p">)</code> <a class="co" href="#callout_securing_ai_services_CO2-1" id="co_securing_ai_services_CO2-1"><img alt="1" src="assets/1.png"/></a>
        <code class="k">if</code> <code class="n">topical_guardrail_task</code> <code class="ow">in</code> <code class="n">done</code><code class="p">:</code>
            <code class="n">topic_allowed</code> <code class="o">=</code> <code class="n">topical_guardrail_task</code><code class="o">.</code><code class="n">result</code><code class="p">(</code><code class="p">)</code>
            <code class="k">if</code> <code class="ow">not</code> <code class="n">topic_allowed</code><code class="p">:</code>
                <code class="n">chat_task</code><code class="o">.</code><code class="n">cancel</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_securing_ai_services_CO2-2" id="co_securing_ai_services_CO2-2"><img alt="2" src="assets/2.png"/></a>
                <code class="n">logger</code><code class="o">.</code><code class="n">warning</code><code class="p">(</code><code class="s2">"</code><code class="s2">Topical guardrail triggered</code><code class="s2">"</code><code class="p">)</code>
                <code class="k">return</code> <code class="p">(</code>
                    <code class="s2">"</code><code class="s2">Sorry, I can only talk about </code><code class="s2">"</code>
                    <code class="s2">"</code><code class="s2">building GenAI services with FastAPI</code><code class="s2">"</code>
                <code class="p">)</code>
            <code class="k">elif</code> <code class="n">chat_task</code> <code class="ow">in</code> <code class="n">done</code><code class="p">:</code>
                <code class="k">return</code> <code class="n">chat_task</code><code class="o">.</code><code class="n">result</code><code class="p">(</code><code class="p">)</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="k">await</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">sleep</code><code class="p">(</code><code class="mf">0.1</code><code class="p">)</code> <a class="co" href="#callout_securing_ai_services_CO2-3" id="co_securing_ai_services_CO2-3"><img alt="3" src="assets/3.png"/></a>

<code class="nd">@router</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">/text/generate</code><code class="s2">"</code><code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">generate_text_controller</code><code class="p">(</code>
    <code class="n">response</code><code class="p">:</code> <code class="n">Annotated</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Depends</code><code class="p">(</code><code class="n">invoke_llm_with_guardrails</code><code class="p">)</code><code class="p">]</code> <a class="co" href="#callout_securing_ai_services_CO2-4" id="co_securing_ai_services_CO2-4"><img alt="4" src="assets/4.png"/></a>
<code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="k">return</code> <code class="n">response</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_securing_ai_services_CO2-1" id="callout_securing_ai_services_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create two asyncio tasks to run in parallel using <code>asyncio.wait</code>.
The operation returns as soon as a task is completed.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO2-2" id="callout_securing_ai_services_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>If the guardrail is triggered, cancel the chat operation and return a hard-coded response.
You can log the trigger in a database and send notification emails here.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO2-3" id="callout_securing_ai_services_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Keep checking in with the asyncio event loop every 100 ms until a task is done.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO2-4" id="callout_securing_ai_services_CO2-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Leverage dependency injection to return the model response if guardrails aren’t triggered.</p></dd>
</dl></div>

<p class="less_space pagebreak-before">Since GenAI-enabled guardrails like those you implemented in <a data-type="xref" href="#guardrail_concurrent_execution">Example 9-3</a> remain probabilistic, your GenAI services can still be vulnerable to prompt injection and jail-breaking attacks.
For instance, attackers can use more advanced prompt injection techniques to get around your AI guardrails too.
On the other hand, your guardrails may also incorrectly over-refuse valid user queries, leading to false positives that can downgrade your user experience.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Combining guardrails with rules-based or traditional machine learning models for detection can help mitigate some of the aforementioned risks.</p>

<p>Additionally, you can use guardrails that only consider the latest message to reduce the risk of the model being confused by a long conversation.</p>
</div>

<p>When designing guardrails, you need to consider trade-offs between <em>accuracy</em>, <em>latency</em>, and <em>cost</em> to balance user experience with your required security controls.<a data-startref="ix_ch09-asciidoc6" data-type="indexterm" id="id1038"/><a data-startref="ix_ch09-asciidoc5" data-type="indexterm" id="id1039"/><a data-startref="ix_ch09-asciidoc4" data-type="indexterm" id="id1040"/></p>
</div></section>








<section data-pdf-bookmark="Output Guardrails" data-type="sect2"><div class="sect2" id="id138">
<h2>Output Guardrails</h2>

<p><a data-primary="guardrails" data-secondary="output guardrails" data-type="indexterm" id="id1041"/><a data-primary="output guardrails" data-type="indexterm" id="id1042"/><a data-primary="securing AI services" data-secondary="guardrails" data-tertiary="output guardrails" data-type="indexterm" id="id1043"/>The purpose of output guardrails is to validate GenAI-produced content before it’s passed to users or downstream systems. <a data-type="xref" href="#guardrails_output">Table 9-4</a> shows common output guardrails.</p>
<table class="striped" id="guardrails_output">
<caption><span class="label">Table 9-4. </span>Common output guardrails</caption>
<thead>
<tr>
<th>Output guardrails</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><strong>Hallucination/fact-checking</strong></p><p>Block hallucinations and return canned responses such as “I don’t know.”</p></td>
<td><p>Measuring metrics such as <em>relevancy</em>, <em>coherence</em>, <em>consistency</em>, <em>fluency</em>, etc., on the model outputs against a corpus of ground truth in RAG applications.</p></td>
</tr>
<tr>
<td><p><strong>Moderation</strong></p><p>Apply brand and corporate guidelines to govern the model outputs, either filtering or rewriting responses that breach them.</p></td>
<td><p>Checking against metrics such as <em>readability</em>, <em>toxicity</em>, <em>sentiment</em>, <em>count of competitor mentions</em>, etc.</p></td>
</tr>
<tr>
<td><p><strong>Syntax checks</strong></p><p>Verify the structure and content of model outputs.
These guardrails can either detect and retry or gracefully handle exceptions to prevent failures in the downstream systems.</p></td>
<td><p>Validating JSON schemas and function parameters in <em>function calling</em> workflows when models invoke functions.</p><p>Checking tool/agent selections in <em>agentic workflows</em>.</p></td>
</tr>
</tbody>
</table>

<p>Any of the aforementioned output guardrails will rely on <em>threshold value</em> to detect invalid responses.</p>
</div></section>








<section class="less_space pagebreak-before" data-pdf-bookmark="Guardrail Thresholds" data-type="sect2"><div class="sect2" id="id252">
<h2>Guardrail Thresholds</h2>

<p><a data-primary="guardrails" data-secondary="thresholds" data-type="indexterm" id="id1044"/><a data-primary="securing AI services" data-secondary="guardrails" data-tertiary="thresholds" data-type="indexterm" id="id1045"/>Guardrails can use various metrics such as <em>readability</em>, <em>toxicity</em>, etc., to measure and validate the quality of the model outputs.
For each metric, you’ll need to experiment to identify the appropriate
<em>threshold value</em> for your use case, bearing in mind that:</p>

<ul>
<li>
<p>More <em>false positives</em> can annoy your users and reduce the usability of your 
<span class="keep-together">services.</span></p>
</li>
<li>
<p>More <em>false negatives</em> can cause lasting harm to your reputation and explode costs since malicious users can abuse the system or perform prompt injection/jail-breaking attacks.</p>
</li>
</ul>

<p>Normally, you should assess the risks and worst cases of having false negatives and whether you’re happy to trade off a few false negatives in your use case for enhanced user experience.
For instance, you can reduce instances of blocking outputs if they include more jargon and aren’t as readable.</p>
</div></section>








<section data-pdf-bookmark="Implementing a Moderation Guardrail" data-type="sect2"><div class="sect2" id="id139">
<h2>Implementing a Moderation Guardrail</h2>

<p><a data-primary="G-Eval framework" data-type="indexterm" id="ix_ch09-asciidoc7"/><a data-primary="guardrails" data-secondary="implementing a moderation guardrail" data-type="indexterm" id="ix_ch09-asciidoc8"/><a data-primary="securing AI services" data-secondary="guardrails" data-tertiary="implementing a moderation guardrail" data-type="indexterm" id="ix_ch09-asciidoc9"/>Let’s implement a moderation guardrail using a version of the <a href="https://oreil.ly/7Nent"><em>G-Eval</em> evaluation method</a> to measure the presence of unwanted content in the model output.</p>

<p>The G-Eval framework uses the following components to score invalid content:</p>

<ul>
<li>
<p>A <em>domain</em> name specifying the type of content to be moderated</p>
</li>
<li>
<p>A set of <em>criteria</em> to clearly outline what is considered valid versus invalid content</p>
</li>
<li>
<p>An ordered list of <em>instruction steps</em> for grading the content</p>
</li>
<li>
<p>The <em>content</em> to grade between a discrete score of 1 to 5</p>
</li>
</ul>

<p><a data-type="xref" href="#guardrail_moderation_prompt">Example 9-4</a> shows a system prompt implementing the <em>G-Eval</em> framework that an LLM auto-evaluator will use.</p>
<div data-type="example" id="guardrail_moderation_prompt">
<h5><span class="label">Example 9-4. </span>Moderation guardrail system prompt</h5>

<pre data-code-language="python" data-type="programlisting"><code class="n">domain</code> <code class="o">=</code> <code class="s2">"Building GenAI Services"</code>

<code class="n">criteria</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">Assess the presence of explicit guidelines for API development for GenAI models.</code>
<code class="s2">The content should contain only general evergreen advice</code>
<code class="s2">not specific tools and libraries to use</code>
<code class="s2">"""</code>

<code class="n">steps</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">1. Read the content and the criteria carefully.</code>
<code class="s2">2. Assess how much explicit guidelines for API development</code>
<code class="s2">for GenAI models is contained in the content.</code>
<code class="s2">3. Assign an advice score from 1 to 5,</code>
<code class="s2">with 1 being evergreen general advice and 5 containing explicit</code>
<code class="s2">mentions of various tools and libraries to use.</code>
<code class="s2">"""</code>


<code class="sa">f</code><code class="s2">"""</code>
<code class="s2">You are a moderation assistant.</code>
<code class="s2">Your role is to detect content about </code><code class="si">{</code><code class="n">domain</code><code class="si">}</code><code class="s2"> in the text provided,</code>
<code class="s2">and mark the severity of that content.</code>

<code class="s2">## </code><code class="si">{</code><code class="n">domain</code><code class="si">}</code><code class="s2"/>

<code class="s2">### Criteria</code>

<code class="si">{</code><code class="n">criteria</code><code class="si">}</code><code class="s2"/>

<code class="s2">### Instructions</code>

<code class="si">{</code><code class="n">steps</code><code class="si">}</code><code class="s2"/>

<code class="s2">### Evaluation (score only!)</code>
<code class="s2">"""</code></pre></div>

<p>Using the system prompt implemented in <a data-type="xref" href="#guardrail_moderation_prompt">Example 9-4</a>, you can now implement a moderation guardrail following <a data-type="xref" href="#guardrail_topical">Example 9-2</a>.</p>

<p>Next, let’s integrate the moderation guardrail with your existing chat invocation logic, as shown in <a data-type="xref" href="#guardrail_moderation">Example 9-5</a>.</p>
<div data-type="example" id="guardrail_moderation">
<h5><span class="label">Example 9-5. </span>Integrating moderation guardrail</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">asyncio</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Annotated</code>
<code class="kn">from</code> <code class="nn">loguru</code> <code class="kn">import</code> <code class="n">logger</code>
<code class="kn">from</code> <code class="nn">pydantic</code> <code class="kn">import</code> <code class="n">BaseModel</code><code class="p">,</code> <code class="n">Field</code>

<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="k">class</code> <code class="nc">ModerationResponse</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">)</code><code class="p">:</code>
    <code class="n">score</code><code class="p">:</code> <code class="n">Annotated</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="n">Field</code><code class="p">(</code><code class="n">ge</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">le</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code><code class="p">]</code> <a class="co" href="#callout_securing_ai_services_CO3-1" id="co_securing_ai_services_CO3-1"><img alt="1" src="assets/1.png"/></a>

<code class="k">async</code> <code class="k">def</code> <code class="nf">g_eval_moderate_content</code><code class="p">(</code>
    <code class="n">chat_response</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">threshold</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">3</code>
<code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">bool</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">LLMClient</code><code class="p">(</code><code class="n">guardrail_system_prompt</code><code class="p">)</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">chat_response</code><code class="p">)</code>
    <code class="n">g_eval_score</code> <code class="o">=</code> <code class="n">ModerationResponse</code><code class="p">(</code><code class="n">score</code><code class="o">=</code><code class="n">response</code><code class="p">)</code><code class="o">.</code><code class="n">score</code>
    <code class="k">return</code> <code class="n">g_eval_score</code> <code class="o">&gt;</code><code class="o">=</code> <code class="n">threshold</code> <a class="co" href="#callout_securing_ai_services_CO3-2" id="co_securing_ai_services_CO3-2"><img alt="2" src="assets/2.png"/></a>

<code class="k">async</code> <code class="k">def</code> <code class="nf">invoke_llm_with_guardrails</code><code class="p">(</code><code class="n">user_request</code><code class="p">)</code><code class="p">:</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code>
    <code class="k">while</code> <code class="kc">True</code><code class="p">:</code>
        <code class="o">.</code><code class="o">.</code><code class="o">.</code>
        <code class="k">if</code> <code class="n">topical_guardrail_task</code> <code class="ow">in</code> <code class="n">done</code><code class="p">:</code>
            <code class="o">.</code><code class="o">.</code><code class="o">.</code>
        <code class="k">elif</code> <code class="n">chat_task</code> <code class="ow">in</code> <code class="n">done</code><code class="p">:</code> <a class="co" href="#callout_securing_ai_services_CO3-3" id="co_securing_ai_services_CO3-3"><img alt="3" src="assets/3.png"/></a>
            <code class="n">chat_response</code> <code class="o">=</code> <code class="n">chat_task</code><code class="o">.</code><code class="n">result</code><code class="p">(</code><code class="p">)</code>
            <code class="n">has_passed_moderation</code> <code class="o">=</code> <code class="k">await</code> <code class="n">g_eval_moderate_content</code><code class="p">(</code><code class="n">chat_response</code><code class="p">)</code>
            <code class="k">if</code> <code class="ow">not</code> <code class="n">has_passed_moderation</code><code class="p">:</code>
                <code class="n">logger</code><code class="o">.</code><code class="n">warning</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Moderation guardrail flagged</code><code class="s2">"</code><code class="p">)</code>
                <code class="k">return</code> <code class="p">(</code>
                    <code class="s2">"</code><code class="s2">Sorry, we can</code><code class="s2">'</code><code class="s2">t recommend specific </code><code class="s2">"</code>
                    <code class="s2">"</code><code class="s2">tools or technologies at this time</code><code class="s2">"</code>
                <code class="p">)</code>
            <code class="k">return</code> <code class="n">chat_response</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="k">await</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">sleep</code><code class="p">(</code><code class="mf">0.1</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_securing_ai_services_CO3-1" id="callout_securing_ai_services_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Use a Pydantic constrained integer type to validate LLM auto-evaluator G-Eval score.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO3-2" id="callout_securing_ai_services_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Flag content that is scored above the threshold as not passing moderation.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO3-3" id="callout_securing_ai_services_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Integrate and run the output moderation guardrail with other guardrails.</p></dd>
</dl></div>
<div data-type="tip"><h6>Tip</h6>
<p>Beyond the novel <em>G-Eval</em> framework implemented using an LLM auto-evaluator, you can also use more traditional automatic evaluation frameworks such as <a href="https://oreil.ly/_9Q9g">ROUGE</a>, <a href="https://oreil.ly/jRTeL">BERTScore</a>, and <a href="https://oreil.ly/5YtJG">SummEval</a> for moderating output content.</p>
</div>

<p>Well done.
You have now implemented two I/O guardrails, one to verify topics of user queries and another to moderate the LLM outputs.</p>

<p>To improve your guardrail system even further, you can:</p>

<ul>
<li>
<p>Adopt the <em>fast failure</em> approach by exiting early if a guardrail is triggered to optimize response times.</p>
</li>
<li>
<p>Only select <em>appropriate guardrails</em> for your use cases instead of using them all together, which could overwhelm your services.</p>
</li>
<li>
<p>Run guardrails <em>asynchronously</em> instead of sequentially to optimize latency.</p>
</li>
<li>
<p>Implement <em>request sampling</em> by running slower guardrails on a sample of requests to reduce overall latency when your services are under a heavy load.</p>
</li>
</ul>

<p class="less_space pagebreak-before">You should now feel more confident implementing your own guardrails using classical or LLM auto-evaluation techniques without relying on external tools and libraries.<a data-startref="ix_ch09-asciidoc9" data-type="indexterm" id="id1046"/><a data-startref="ix_ch09-asciidoc8" data-type="indexterm" id="id1047"/><a data-startref="ix_ch09-asciidoc7" data-type="indexterm" id="id1048"/></p>

<p>In the next section, you’ll learn about API rate limiting so that you can protect your services against model overloading and scraping attempts.<a data-startref="ix_ch09-asciidoc3" data-type="indexterm" id="id1049"/><a data-startref="ix_ch09-asciidoc2" data-type="indexterm" id="id1050"/></p>
</div></section>
</div></section>






<section data-pdf-bookmark="API Rate Limiting and Throttling" data-type="sect1"><div class="sect1" id="id140">
<h1>API Rate Limiting and Throttling</h1>

<p><a data-primary="securing AI services" data-secondary="API rate limiting and throttling" data-type="indexterm" id="ix_ch09-asciidoc10"/>When <a data-primary="rate limiting" data-type="indexterm" id="ix_ch09-asciidoc11"/><a data-primary="securing AI services" data-secondary="API rate limiting and throttling" data-tertiary="rate limiting" data-type="indexterm" id="ix_ch09-asciidoc12"/>deploying GenAI services, you will need to consider service exhaustion and model overloading issues in production.
Best practice is to implement rate limiting and potentially throttling into your services.</p>

<p><em>Rate limiting</em> controls the amount of incoming and outgoing traffic to and from a network to prevent abuse, ensure fair usage, and avoid overloading the server.
On the other hand, <em>throttling</em> controls the API throughput by temporarily slowing down the rate of request processing to stabilize the server.</p>

<p>Both techniques can help you:</p>

<ul>
<li>
<p><em>Prevent abuse</em> by blocking malicious users or bots from overwhelming your services from data scraping and brute-force attacks that involve too many requests or large payloads.</p>
</li>
<li>
<p><em>Enforce fair usage policies</em> so that capacity is shared among multiple users and a handful of users are prevented from monopolizing server resources.</p>
</li>
<li>
<p><em>Maintain server stability</em> by regulating incoming traffic to maintain consistent performance and prevent crashes during peak periods.</p>
</li>
</ul>

<p>To implement rate limiting, you will need to monitor incoming requests within a time period and use a queue to balance the load.</p>

<p><a data-primary="rate limiting" data-secondary="strategies for" data-type="indexterm" id="id1051"/>There are several rate-limiting strategies you can choose from, which are compared in <a data-type="xref" href="#rate_limiting_strategies">Table 9-5</a> and shown in <a data-type="xref" href="#rate_limiting_strategies_comparison">Figure 9-2</a>.</p>
<table class="striped" id="rate_limiting_strategies">
<caption><span class="label">Table 9-5. </span>Rate-limiting strategies</caption>
<thead>
<tr>
<th>Strategy</th>
<th>Benefits</th>
<th>Limitations</th>
<th>Use cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><strong>Token Bucket</strong></p><p>A list is filled with tokens at a constant rate, and every incoming request consumes a token.
If there aren’t enough tokens for incoming requests, they’ll be rejected.</p></td>
<td><ul>
<li><p>Handles temporary bursts and dynamic traffic patterns</p></li>
<li><p>Granular control over request processing</p></li>
</ul></td>
<td><p>Complex to implement</p></td>
<td><p>Commonly used in most APIs and services, and interactive or event-driven GenAI systems where request rates can be irregular</p></td>
</tr>
<tr>
<td><p><strong>Leaky Bucket</strong></p><p>Incoming requests are added to a queue and processed at a constant rate to smooth the traffic. If the queue overflows, any new incoming requests are rejected.</p></td>
<td><ul><li><p>Simple to implement</p></li>
<li><p> Maintains consistent traffic flow</p></li></ul></td>
<td><ul><li><p>Less flexible to dynamic traffic</p></li>
<li><p> May reject valid requests during sudden spikes</p></li></ul></td>
<td><p>Services that require maintaining consistent response times in AI inference services</p></td>
</tr>
<tr>
<td><p><strong>Fixed Window</strong></p><p>Limits requests within fixed time windows (e.g., 100 requests per minute).</p></td>
<td><p>Simple to implement</p></td>
<td><p>Does not handle burst traffic well</p></td>
<td>
<ul><li><p>Enforcing strict usage policies for expensive AI inferences and API calls</p></li>
<li><p>
 Ideal for free  tier users or batch-processing systems with predictable usage patterns</p></li>
 <li><p>Each request is treated equally</p></li></ul></td>
</tr>
<tr>
<td><p><strong>Sliding Window</strong></p><p>Counts requests over a rolling time frame.</p></td>
<td><p>Provides better flexibility, granularity, and burst traffic smoothing</p></td>
<td><ul><li><p>More complex to implement</p></li>
<li><p> Requires higher memory usage for tracking requests</p></li></ul></td>
<td><ul><li><p>Much better at handling burst traffic</p></li>
<li><p> Ideal for conversational AI or premium-tier users who expect flexible, high-frequency access over time</p></li></ul></td>
</tr>
</tbody>
</table>

<figure><div class="figure" id="rate_limiting_strategies_comparison">
<img alt="bgai 0902" src="assets/bgai_0902.png"/>
<h6><span class="label">Figure 9-2. </span>Comparison of rate-limiting strategies</h6>
</div></figure>

<p>Now that you’re more familiar with rate-limiting concepts, let’s try to implement rate limiting in FastAPI.</p>








<section data-pdf-bookmark="Implementing Rate Limits in FastAPI" data-type="sect2"><div class="sect2" id="id253">
<h2>Implementing Rate Limits in FastAPI</h2>

<p><a data-primary="rate limiting" data-secondary="implementing rate limits in FastAPI" data-type="indexterm" id="ix_ch09-asciidoc13"/><a data-primary="securing AI services" data-secondary="API rate limiting and throttling" data-tertiary="implementing rate limits in FastAPI" data-type="indexterm" id="ix_ch09-asciidoc14"/>The fastest approach to add rate limiting within FastAPI is to use a library such as <code>slowapi</code> that is a wrapper over the <code>limits</code> package, supporting most of the strategies mentioned in <a data-type="xref" href="#rate_limiting_strategies">Table 9-5</a>. First, install the <code>slowapi</code> library:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install slowapi<code class="w"/></pre>

<p>Once you’ve installed the <code>slowapi</code> package, you can follow <a data-type="xref" href="#rate_limiting_slowapi_configurations">Example 9-6</a> to apply global API or endpoint rate limiting.
You can also track and limit usage per IP address.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Without configuring an external data store, <code>slowapi</code> stores and tracks IP addresses in the application memory for rate limiting.</p>
</div>
<div data-type="example" id="rate_limiting_slowapi_configurations">
<h5><span class="label">Example 9-6. </span>Configuring global rate limits</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">fastapi</code><code class="nn">.</code><code class="nn">responses</code> <code class="kn">import</code> <code class="n">JSONResponse</code>
<code class="kn">from</code> <code class="nn">slowapi</code> <code class="kn">import</code> <code class="n">Limiter</code>
<code class="kn">from</code> <code class="nn">slowapi</code><code class="nn">.</code><code class="nn">errors</code> <code class="kn">import</code> <code class="n">RateLimitExceeded</code>
<code class="kn">from</code> <code class="nn">slowapi</code><code class="nn">.</code><code class="nn">middleware</code> <code class="kn">import</code> <code class="n">SlowAPIMiddleware</code>
<code class="kn">from</code> <code class="nn">slowapi</code><code class="nn">.</code><code class="nn">util</code> <code class="kn">import</code> <code class="n">get_remote_address</code>

<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="n">limiter</code> <code class="o">=</code> <code class="n">Limiter</code><code class="p">(</code>
    <code class="n">key_func</code><code class="o">=</code><code class="n">get_remote_address</code><code class="p">,</code>
    <code class="n">default_limits</code><code class="o">=</code><code class="p">[</code><code class="s2">"</code><code class="s2">200 per day</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">60 per hour</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">2/5seconds</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code>
<code class="p">)</code> <a class="co" href="#callout_securing_ai_services_CO4-1" id="co_securing_ai_services_CO4-1"><img alt="1" src="assets/1.png"/></a>

<code class="n">app</code><code class="o">.</code><code class="n">state</code><code class="o">.</code><code class="n">limiter</code> <code class="o">=</code> <code class="n">limiter</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">exception_handler</code><code class="p">(</code><code class="n">RateLimitExceeded</code><code class="p">)</code> <a class="co" href="#callout_securing_ai_services_CO4-2" id="co_securing_ai_services_CO4-2"><img alt="2" src="assets/2.png"/></a>
<code class="k">def</code> <code class="nf">rate_limit_exceeded_handler</code><code class="p">(</code><code class="n">request</code><code class="p">,</code> <code class="n">exc</code><code class="p">)</code><code class="p">:</code>
    <code class="n">retry_after</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="n">exc</code><code class="o">.</code><code class="n">description</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"</code> <code class="s2">"</code><code class="p">)</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="p">)</code>
    <code class="n">response_body</code> <code class="o">=</code> <code class="p">{</code>
        <code class="s2">"</code><code class="s2">detail</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">Rate limit exceeded. Please try again later.</code><code class="s2">"</code><code class="p">,</code>
        <code class="s2">"</code><code class="s2">retry_after_seconds</code><code class="s2">"</code><code class="p">:</code> <code class="n">retry_after</code><code class="p">,</code>
    <code class="p">}</code>
    <code class="k">return</code> <code class="n">JSONResponse</code><code class="p">(</code>
        <code class="n">status_code</code><code class="o">=</code><code class="mi">429</code><code class="p">,</code>
        <code class="n">content</code><code class="o">=</code><code class="n">response_body</code><code class="p">,</code>
        <code class="n">headers</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">Retry-After</code><code class="s2">"</code><code class="p">:</code> <code class="nb">str</code><code class="p">(</code><code class="n">retry_after</code><code class="p">)</code><code class="p">}</code><code class="p">,</code>
    <code class="p">)</code>

<code class="n">app</code><code class="o">.</code><code class="n">add_middleware</code><code class="p">(</code><code class="n">SlowAPIMiddleware</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_securing_ai_services_CO4-1" id="callout_securing_ai_services_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create rate limiter that tracks usage across each IP address and rejects requests if they exceed specified limits across the application.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO4-2" id="callout_securing_ai_services_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Add a custom exception handler for rate-limited requests to compute and provide waiting times before requests are accepted again.</p></dd>
</dl></div>

<p>With the <code>limiter</code> decorator configured, you can now use it on your API handlers, as shown in <a data-type="xref" href="#rate_limiting_slowapi">Example 9-7</a>.</p>
<div data-type="example" id="rate_limiting_slowapi">
<h5><span class="label">Example 9-7. </span>Setting API rate limits for each API handler</h5>

<pre data-code-language="python" data-type="programlisting"><code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">/generate/text</code><code class="s2">"</code><code class="p">)</code>
<code class="nd">@limiter</code><code class="o">.</code><code class="n">limit</code><code class="p">(</code><code class="s2">"</code><code class="s2">5/minute</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_securing_ai_services_CO5-1" id="co_securing_ai_services_CO5-1"><img alt="1" src="assets/1.png"/></a>
<code class="k">def</code> <code class="nf">serve_text_to_text_controller</code><code class="p">(</code><code class="n">request</code><code class="p">:</code> <code class="n">Request</code><code class="p">,</code> <code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">)</code><code class="p">:</code>
    <code class="k">return</code> <code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">/generate/image</code><code class="s2">"</code><code class="p">)</code>
<code class="nd">@limiter</code><code class="o">.</code><code class="n">limit</code><code class="p">(</code><code class="s2">"</code><code class="s2">1/minute</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_securing_ai_services_CO5-2" id="co_securing_ai_services_CO5-2"><img alt="2" src="assets/2.png"/></a>
<code class="k">def</code> <code class="nf">serve_text_to_image_controller</code><code class="p">(</code><code class="n">request</code><code class="p">:</code> <code class="n">Request</code><code class="p">,</code> <code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_securing_ai_services_CO5-3" id="co_securing_ai_services_CO5-3"><img alt="3" src="assets/3.png"/></a>
    <code class="k">return</code> <code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">/health</code><code class="s2">"</code><code class="p">)</code>
<code class="nd">@limiter</code><code class="o">.</code><code class="n">exempt</code> <a class="co" href="#callout_securing_ai_services_CO5-4" id="co_securing_ai_services_CO5-4"><img alt="4" src="assets/4.png"/></a>
<code class="k">def</code> <code class="nf">check_health_controller</code><code class="p">(</code><code class="n">request</code><code class="p">:</code> <code class="n">Request</code><code class="p">)</code><code class="p">:</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"</code><code class="s2">status</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">healthy</code><code class="s2">"</code><code class="p">}</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_securing_ai_services_CO5-1" id="callout_securing_ai_services_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Specify more granular rate limits at endpoint level using a rate-limiting decorator. The <code>limiter</code> decorator must be ordered last.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO5-2" id="callout_securing_ai_services_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Pass the <code>Request</code> object to each controller so that the <code>slowapi</code> limiter decorator can hook into the incoming request.
Otherwise, rate limiting will not function.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO5-3" id="callout_securing_ai_services_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Exclude the <code>/health</code> endpoint from rate-limiting logic as cloud providers or Docker daemons may ping this endpoint continually to check the status of your application.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO5-4" id="callout_securing_ai_services_CO5-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Avoid rate limiting the <code>/health</code> endpoint as external systems may frequently trigger it to check the current status of your service.</p></dd>
</dl></div>

<p>Now that you’ve implemented the rate limits, you can run load tests using the <code>ab</code> (Apache Benchmarking) CLI tool, as shown in <a data-type="xref" href="#rate_limiting_load_testing">Example 9-8</a>.</p>
<div data-type="example" id="rate_limiting_load_testing">
<h5><span class="label">Example 9-8. </span>API load testing with Apache Benchmark CLI</h5>

<pre data-code-language="bash" data-type="programlisting"><code>$</code> <code>ab</code> <code>-n</code> <code class="m">100</code> <code>-p</code> <code class="m">2</code> <code>http://localhost:8000</code> <a class="co" href="#callout_securing_ai_services_CO6-1" id="co_securing_ai_services_CO6-1"><img alt="1" src="assets/1.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_securing_ai_services_CO6-1" id="callout_securing_ai_services_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Send 100 requests with a rate of 2 parallel requests per second.</p></dd>
</dl></div>

<p class="less_space pagebreak-before">Your terminal outputs should show the following:</p>

<pre data-code-language="text" data-type="programlisting">200 OK
200 OK
429 Rate limited Exceeded
...</pre>

<p>Your global and local limiting system should now be working as intended based on incoming IPs.</p>










<section data-pdf-bookmark="User-based rate limits" data-type="sect3"><div class="sect3" id="id254">
<h3>User-based rate limits</h3>

<p><a data-primary="rate limiting" data-secondary="user-based rate limits" data-type="indexterm" id="id1052"/>With an IP rate limit, you’re limiting excess usage based on IP, but users can get around IP rate limiting by using VPNs, proxies, or rotating IP addresses.
Instead, you want each user to have a dedicated quota to prevent a single user from consuming all available resources.
Adding user-based limits can help you prevent abuse, as shown in <a data-type="xref" href="#rate_limiting_slowapi_users">Example 9-9</a>.</p>
<div data-type="example" id="rate_limiting_slowapi_users">
<h5><span class="label">Example 9-9. </span>User-based rate limiting</h5>

<pre data-code-language="python" data-type="programlisting"><code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"/generate/text"</code><code class="p">)</code>
<code class="nd">@limiter</code><code class="o">.</code><code class="n">limit</code><code class="p">(</code><code class="s2">"10/minute"</code><code class="p">,</code> <code class="n">key_func</code><code class="o">=</code><code class="n">get_current_user</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">serve_text_to_text_controller</code><code class="p">(</code><code class="n">request</code><code class="p">:</code> <code class="n">Request</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"message"</code><code class="p">:</code> <code class="sa">f</code><code class="s2">"Hello User"</code><code class="p">}</code></pre></div>

<p>Your system will now be limiting users based on their account IDs alongside their IP addresses.</p>
</div></section>










<section data-pdf-bookmark="Rate limits across instances in production" data-type="sect3"><div class="sect3" id="id255">
<h3>Rate limits across instances in production</h3>

<p><a data-primary="rate limiting" data-secondary="rate limits across instances in production" data-type="indexterm" id="id1053"/>Since you may run multiple instances of your application in production as you scale your services, you’ll also want to centralize your usage tracking.
Otherwise, each instance will provide their own counters to users, and a load balancer distributes requests between instances; usage won’t be capped as you’d expect.
To rectify this issue, you can switch the <code>slowapi</code> in-memory storage backend with a centralized in-memory database such as Redis, as shown in <a data-type="xref" href="#rate_limiting_slowapi_redis">Example 9-10</a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>To run <a data-type="xref" href="#rate_limiting_slowapi_redis">Example 9-10</a>, you will need a Redis database to store user API usage data:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install coredis<code class="w"/>
$ docker pull redis<code class="w"/>
$ docker run <code class="se">\</code>
  --name rate-limit-redis-cache <code class="se">\</code>
  -d <code class="se">\</code>
  -p <code class="m">6379</code>:6379 <code class="se">\</code>
  redis<code class="w"/></pre>
</div>
<div class="less_space pagebreak-before" data-type="example" id="rate_limiting_slowapi_redis">
<h5><span class="label">Example 9-10. </span>Adding a centralized usage memory store (Redis) across multiple instances</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">slowapi</code> <code class="kn">import</code> <code class="n">Limiter</code>
<code class="kn">from</code> <code class="nn">slowapi.middleware</code> <code class="kn">import</code> <code class="n">SlowAPIMiddleware</code>

<code class="n">app</code><code class="o">.</code><code class="n">state</code><code class="o">.</code><code class="n">limiter</code> <code class="o">=</code> <code class="n">Limiter</code><code class="p">(</code><code class="n">storage_uri</code><code class="o">=</code><code class="s2">"redis://localhost:6379"</code><code class="p">)</code>
<code class="n">app</code><code class="o">.</code><code class="n">add_middleware</code><code class="p">(</code><code class="n">SlowAPIMiddleware</code><code class="p">)</code></pre></div>

<p>You now have a working rate-limited API that functions as intended across multiple instances.</p>

<p>You can get around this issue by implementing your own limiter supported by the <code>limits</code> package instead.
Alternatively, you can apply rate limiting via a <em>load balancer</em>, a <em>reverse proxy</em>, or an <em>API gateway</em> instead.</p>

<p>Each solution can route requests while performing rate limits, protocol translation, and traffic monitoring at an infrastructure layer.
Applying rate limiting externally may be more suitable for your use case if you don’t require a customized rate-limiting logic.</p>
</div></section>










<section data-pdf-bookmark="Limiting WebSocket connections" data-type="sect3"><div class="sect3" id="id141">
<h3>Limiting WebSocket connections</h3>

<p><a data-primary="rate limiting" data-secondary="limiting WebSocket connections" data-type="indexterm" id="ix_ch09-asciidoc15"/><a data-primary="WebSocket (WS)" data-secondary="rate-limiting" data-type="indexterm" id="ix_ch09-asciidoc16"/>Unfortunately the <code>slowapi</code> package also doesn’t support limiting async and WebSocket endpoints at the time of writing.</p>

<p>Because WebSocket connections are likely to be long-lived, you may want to limit the data transition rate sent over the socket.
You can rely on external packages such as <code>fastapi-limiter</code> to rate limit WebSocket connections, as shown in <a data-type="xref" href="#rate_limiting_websocket">Example 9-11</a>.</p>
<div data-type="example" id="rate_limiting_websocket">
<h5><span class="label">Example 9-11. </span>Rate-limiting WebSocket connections with the <code>fastapi_limiter</code> package</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">contextlib</code> <code class="kn">import</code> <code class="n">asynccontextmanager</code>
<code class="kn">import</code> <code class="nn">redis</code>
<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">Depends</code><code class="p">,</code> <code class="n">FastAPI</code>
<code class="kn">from</code> <code class="nn">fastapi</code><code class="nn">.</code><code class="nn">websockets</code> <code class="kn">import</code> <code class="n">WebSocket</code>
<code class="kn">from</code> <code class="nn">fastapi_limiter</code> <code class="kn">import</code> <code class="n">FastAPILimiter</code>
<code class="kn">from</code> <code class="nn">fastapi_limiter</code><code class="nn">.</code><code class="nn">depends</code> <code class="kn">import</code> <code class="n">WebSocketRateLimiter</code>

<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="nd">@asynccontextmanager</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">lifespan</code><code class="p">(</code><code class="n">_</code><code class="p">:</code> <code class="n">FastAPI</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_securing_ai_services_CO7-1" id="co_securing_ai_services_CO7-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">redis_connection</code> <code class="o">=</code> <code class="n">redis</code><code class="o">.</code><code class="n">from_url</code><code class="p">(</code><code class="s2">"</code><code class="s2">redis://localhost:6379</code><code class="s2">"</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s2">"</code><code class="s2">utf8</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">await</code> <code class="n">FastAPILimiter</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">redis_connection</code><code class="p">)</code>
    <code class="k">yield</code>
    <code class="k">await</code> <code class="n">FastAPILimiter</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="n">lifespan</code><code class="o">=</code><code class="n">lifespan</code><code class="p">)</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">websocket</code><code class="p">(</code><code class="s2">"</code><code class="s2">/ws</code><code class="s2">"</code><code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">websocket_endpoint</code><code class="p">(</code>
    <code class="n">websocket</code><code class="p">:</code> <code class="n">WebSocket</code><code class="p">,</code> <code class="n">user_id</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="n">Depends</code><code class="p">(</code><code class="n">get_current_user</code><code class="p">)</code> <a class="co" href="#callout_securing_ai_services_CO7-2" id="co_securing_ai_services_CO7-2"><img alt="2" src="assets/2.png"/></a>
<code class="p">)</code><code class="p">:</code>
    <code class="n">ratelimit</code> <code class="o">=</code> <code class="n">WebSocketRateLimiter</code><code class="p">(</code><code class="n">times</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">seconds</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
    <code class="k">await</code> <code class="n">ws_manager</code><code class="o">.</code><code class="n">connect</code><code class="p">(</code><code class="n">websocket</code><code class="p">)</code>
    <code class="k">try</code><code class="p">:</code>
        <code class="k">while</code> <code class="kc">True</code><code class="p">:</code>
            <code class="n">prompt</code> <code class="o">=</code> <code class="k">await</code> <code class="n">ws_manager</code><code class="o">.</code><code class="n">receive</code><code class="p">(</code><code class="n">websocket</code><code class="p">)</code>
            <code class="k">await</code> <code class="n">ratelimit</code><code class="p">(</code><code class="n">websocket</code><code class="p">,</code> <code class="n">context_key</code><code class="o">=</code><code class="n">user_id</code><code class="p">)</code> <a class="co" href="#callout_securing_ai_services_CO7-3" id="co_securing_ai_services_CO7-3"><img alt="3" src="assets/3.png"/></a>
            <code class="k">async</code> <code class="k">for</code> <code class="n">chunk</code> <code class="ow">in</code> <code class="n">azure_chat_client</code><code class="o">.</code><code class="n">chat_stream</code><code class="p">(</code><code class="n">prompt</code><code class="p">,</code> <code class="s2">"</code><code class="s2">ws</code><code class="s2">"</code><code class="p">)</code><code class="p">:</code>
                <code class="k">await</code> <code class="n">ws_manager</code><code class="o">.</code><code class="n">send</code><code class="p">(</code><code class="n">chunk</code><code class="p">,</code> <code class="n">websocket</code><code class="p">)</code>
    <code class="k">except</code> <code class="n">WebSocketRateLimitException</code><code class="p">:</code>
        <code class="k">await</code> <code class="n">websocket</code><code class="o">.</code><code class="n">send_text</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Rate limit exceeded. Try again later</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">finally</code><code class="p">:</code>
        <code class="k">await</code> <code class="n">ws_manager</code><code class="o">.</code><code class="n">disconnect</code><code class="p">(</code><code class="n">websocket</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_securing_ai_services_CO7-1" id="callout_securing_ai_services_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Configure the <code>FastAPILimiter</code> application lifespan with a Redis storage backend.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO7-2" id="callout_securing_ai_services_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Configure a WebSocket rate limiter to allow one request per second.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO7-3" id="callout_securing_ai_services_CO7-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Use the user’s ID as the unique identifier for rate limiting.</p></dd>
</dl></div>

<p><a data-type="xref" href="#rate_limiting_websocket">Example 9-11</a> shows how to limit the number of active WebSocket connections for a given user.</p>

<p>Beyond rate-limiting WebSocket endpoints, you may also want to limit the data streaming rate of your GenAI models<a data-startref="ix_ch09-asciidoc16" data-type="indexterm" id="id1054"/><a data-startref="ix_ch09-asciidoc15" data-type="indexterm" id="id1055"/>.<a data-startref="ix_ch09-asciidoc14" data-type="indexterm" id="id1056"/><a data-startref="ix_ch09-asciidoc13" data-type="indexterm" id="id1057"/>
Let’s look at how you can throttle real-time data streams next.<a data-startref="ix_ch09-asciidoc12" data-type="indexterm" id="id1058"/><a data-startref="ix_ch09-asciidoc11" data-type="indexterm" id="id1059"/></p>
</div></section>
</div></section>








<section data-pdf-bookmark="Throttling Real-Time Streams" data-type="sect2"><div class="sect2" id="id142">
<h2>Throttling Real-Time Streams</h2>

<p><a data-primary="securing AI services" data-secondary="API rate limiting and throttling" data-tertiary="throttling real-time streams" data-type="indexterm" id="ix_ch09-asciidoc17"/><a data-primary="streams, throttling" data-type="indexterm" id="ix_ch09-asciidoc18"/><a data-primary="throttling" data-type="indexterm" id="ix_ch09-asciidoc19"/>When working with real-time streams, you may need to slow down the streaming rate to give clients enough time to consume the stream and improve streaming throughput across multiple clients.
In addition, throttling can help you manage the network bandwidth, server load, and resource utilization.</p>

<p>Applying a <em>throttle</em> at the stream generation layer, as shown in <a data-type="xref" href="#throttling_stream">Example 9-12</a>, is an effective approach to managing throughput if your services are under pressure.</p>
<div data-type="example" id="throttling_stream">
<h5><span class="label">Example 9-12. </span>Throttling streams</h5>

<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">AzureOpenAIChatClient</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">throttle_rate</code> <code class="o">=</code> <code class="mf">0.5</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_securing_ai_services_CO8-1" id="co_securing_ai_services_CO8-1"><img alt="1" src="assets/1.png"/></a>
        <code class="bp">self</code><code class="o">.</code><code class="n">aclient</code> <code class="o">=</code> <code class="o">.</code><code class="o">.</code><code class="o">.</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">throttle_rate</code> <code class="o">=</code> <code class="n">throttle_rate</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">chat_stream</code><code class="p">(</code>
            <code class="bp">self</code><code class="p">,</code> <code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">mode</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">sse</code><code class="s2">"</code><code class="p">,</code> <code class="n">model</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">gpt-3.5-turbo</code><code class="s2">"</code>
    <code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">AsyncGenerator</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="kc">None</code><code class="p">]</code><code class="p">:</code>
        <code class="n">stream</code> <code class="o">=</code> <code class="o">.</code><code class="o">.</code><code class="o">.</code>  <code class="c1"># OpenAI chat completion stream</code>
        <code class="k">async</code> <code class="k">for</code> <code class="n">chunk</code> <code class="ow">in</code> <code class="n">stream</code><code class="p">:</code>
            <code class="k">await</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">sleep</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">throttle_rate</code><code class="p">)</code> <a class="co" href="#callout_securing_ai_services_CO8-2" id="co_securing_ai_services_CO8-2"><img alt="2" src="assets/2.png"/></a>
            <code class="k">if</code> <code class="n">chunk</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">delta</code><code class="o">.</code><code class="n">content</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
                <code class="k">yield</code> <code class="p">(</code>
                    <code class="sa">f</code><code class="s2">"</code><code class="s2">data: </code><code class="si">{</code><code class="n">chunk</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">delta</code><code class="o">.</code><code class="n">content</code><code class="si">}</code><code class="se">\n</code><code class="se">\n</code><code class="s2">"</code>
                    <code class="k">if</code> <code class="n">mode</code> <code class="o">==</code> <code class="s2">"</code><code class="s2">sse</code><code class="s2">"</code>
                    <code class="k">else</code> <code class="n">chunk</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">delta</code><code class="o">.</code><code class="n">content</code>
                <code class="p">)</code>
                <code class="k">await</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">sleep</code><code class="p">(</code><code class="mf">0.05</code><code class="p">)</code>

        <code class="k">if</code> <code class="n">mode</code> <code class="o">==</code> <code class="s2">"</code><code class="s2">sse</code><code class="s2">"</code><code class="p">:</code>
            <code class="k">yield</code> <code class="sa">f</code><code class="s2">"</code><code class="s2">data: [DONE]</code><code class="se">\n</code><code class="se">\n</code><code class="s2">"</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_securing_ai_services_CO8-1" id="callout_securing_ai_services_CO8-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Set a fixed throttling rate or dynamically adjust based on usage.</p></dd>
<dt><a class="co" href="#co_securing_ai_services_CO8-2" id="callout_securing_ai_services_CO8-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Slow down the streaming rate without blocking the event loop.</p></dd>
</dl></div>

<p>You can then use the throttled stream within an SSE or WebSocket endpoint.
Or, you can limit the number of active WebSocket connections per your own custom policies.</p>

<p>Alongside the application-level throttling for real-time streams, you can also leverage <em>traffic shaping</em> at the infrastructure layer.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1060">
<h1>Traffic Shaping</h1>
<p><a data-primary="traffic shaping" data-type="indexterm" id="id1061"/>While rate-limiting approaches can help you manage incoming requests, you can also use throttling techniques like <em>traffic shaping</em> to control the rate of data transmission.</p>

<p>Traffic shaping prioritizes certain types of data transfer to help you prevent congestion, smooth out traffic bursts, and maintain a consistent data flow to optimize application bandwidth usage, as shown in <a data-type="xref" href="#traffic_shaping">Figure 9-3</a>.
This is especially useful for GenAI services requiring real-time data transmission including chat and video streaming.</p>

<p>To implement traffic shaping, you can use the <code>tc</code> command-line tool within Linux to configure control rules on targets such as network interfaces and Docker containers.
These control rules set bandwidth limits, intentional latency delays, packet loss, and IP limits on specific targets to regulate the application throughput.</p>

<p>While the traffic shaping technique can help you manage the network bandwidth, bear in mind that it involves complex algorithms and requires constant monitoring and dynamic control of the packet flow.
It may also introduce delays due to its queuing methods when the network is heavily loaded.</p>

<figure><div class="figure" id="traffic_shaping">
<img alt="bgai 0903" src="assets/bgai_0903.png"/>
<h6><span class="label">Figure 9-3. </span>Traffic shaping</h6>
</div></figure>
</div></aside>

<p>Using safeguards, rate limits, and throttles should provide enough barriers in protecting your services from abuse and misuse.<a data-startref="ix_ch09-asciidoc19" data-type="indexterm" id="id1062"/><a data-startref="ix_ch09-asciidoc18" data-type="indexterm" id="id1063"/><a data-startref="ix_ch09-asciidoc17" data-type="indexterm" id="id1064"/></p>

<p>In the next section, you’ll learn more about optimization techniques that can help you reduce latency, increase response quality, and throughput alongside reducing the costs of your GenAI services.<a data-startref="ix_ch09-asciidoc10" data-type="indexterm" id="id1065"/></p>
</div></section>
</div></section>






<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id143">
<h1>Summary</h1>

<p>This chapter provided a comprehensive summary of attack vectors for GenAI services and how to safeguard them against adversarial attempts, misuse, and abuse.</p>

<p>You learned to implement input and output guardrails alongside evaluation and content filtering mechanisms to moderate service usage.
Alongside guardrails, you also developed API rate-limiting and throttling protections to manage server load and prevent abuse.<a data-startref="ix_ch09-asciidoc0" data-type="indexterm" id="id1066"/></p>

<p>In the next chapter, we will learn about optimizing AI services through various techniques such as caching, batch processing, model quantizing, prompt engineering, and model fine-tuning.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1037"><sup><a href="ch09.html#id1037-marker">1</a></sup> Inspired by <a href="https://oreil.ly/UQV6i">OpenAI Cookbook’s “How to Implement LLM Guardrails”</a>.</p></div></div></section></body></html>