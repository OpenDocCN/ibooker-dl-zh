- en: Chapter 12\. Not Hotdog on iOS with Core ML and Create ML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。在iOS上使用Core ML和Create ML的Not Hotdog
- en: “I’m a rich,” said Jian-Yang, a newly minted millionaire in an interview with
    Bloomberg ([Figure 12-1](part0014.html#jian_yang_being_interviewed_by_bloomberg)).
    What did he do? He created the Not Hotdog app ([Figure 12-2](part0014.html#the_not_hotdog_app_in_action_left_parent))
    and made the world “a better place.”
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “我是富人，”新晋百万富翁简阳在接受彭博社采访时说（[图12-1]（part0014.html＃jian_yang_being_interviewed_by_bloomberg））。他做了什么？他创建了Not
    Hotdog应用程序（[图12-2]（part0014.html＃the_not_hotdog_app_in_action_left_parent）），让世界“变得更美好”。
- en: '![Jian Yang being interviewed by Bloomberg News after Periscope acquires his
    “Not Hotdog” technology (source: HBO’s Silicon Valley)](../images/00307.jpeg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: ！[简阳在Periscope收购他的“Not Hotdog”技术后接受彭博新闻采访（来源：HBO的硅谷）]（../images/00307.jpeg）
- en: 'Figure 12-1\. Jian-Yang being interviewed by Bloomberg News after Periscope
    acquires his “Not Hotdog” technology (image source: From HBO’s Silicon Valley)'
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1。简阳在Periscope收购他的“Not Hotdog”技术后接受彭博新闻采访（图片来源：来自HBO的硅谷）
- en: To the few of us who may be confused (including a third of the authors of this
    book), we are making a reference to HBO’s *Silicon Valley*, a show in which one
    of the characters is tasked with making SeeFood—the “Shazam for food.” It was
    meant to classify pictures of food and give recipes and nutritional information.
    Hilariously, the app ends up being good only for recognizing hot dogs. Anything
    else would be classified as “Not Hotdog.”
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们中的少数人可能会感到困惑（包括本书的三分之一作者），我们正在参考HBO的*硅谷*，这是一部节目，其中一个角色被要求制作SeeFood - “食物的Shazam”。它旨在对食物图片进行分类，并提供食谱和营养信息。令人发笑的是，该应用程序最终只能识别热狗。其他任何东西都将被分类为“Not
    Hotdog”。
- en: 'There are a few reasons we chose to reference this fictitious app. It’s very
    much a part of popular culture and something many people can easily relate to.
    It’s an exemplar: easy enough to build, yet powerful enough to see the magic of
    deep learning in a real-world application. It is also very trivially generalizable
    to recognize more than one class of items.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择引用这个虚构的应用程序有几个原因。它在流行文化中非常重要，许多人可以轻松地与之联系。它是一个典范：足够简单，但又足够强大，可以在现实世界的应用中看到深度学习的魔力。它也非常容易地可以推广到识别多个类别的物品。
- en: '![The Not Hotdog app in action (image source: Apple App Store listing for the
    Not Hotdog app)](../images/00273.jpeg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: ！[Not Hotdog应用程序的操作（图片来源：苹果应用商店中的Not Hotdog应用程序列表）]（../images/00273.jpeg）
- en: 'Figure 12-2\. The Not Hotdog app in action (image source: Apple App Store listing
    for the Not Hotdog app)'
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2。Not Hotdog应用程序的操作（图片来源：苹果应用商店中的Not Hotdog应用程序）
- en: 'In this chapter, we work through a few different approaches to building a Not
    Hotdog clone. The general outline of the end-to-end process is as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过几种不同的方法来构建一个Not Hotdog克隆。整个端到端过程的一般概述如下：
- en: Collect relevant data.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集相关数据。
- en: Train the model.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Convert to Core ML.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换为Core ML。
- en: Build the iOS app.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建iOS应用程序。
- en: '[Table 12-1](part0014.html#various_approaches_to_get_a_model_ready) presents
    the different options available for steps 1 through 3\. Further along in the chapter,
    we do a deep dive into each of them.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[表12-1]（part0014.html＃various_approaches_to_get_a_model_ready）介绍了步骤1到3的不同选项。在本章后面，我们将深入研究每个选项。'
- en: Table 12-1\. Various approaches to getting a model ready for mobile deployment,
    right from scratch
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表12-1。从头开始为移动部署准备模型的各种方法
- en: '| **Data collection** | **Training mechanism** | **Model conversion** |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **数据收集** | **训练机制** | **模型转换** |'
- en: '| --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Find or collect a dataset
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找或收集数据集
- en: Fatkun Chrome browser extension
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fatkun Chrome浏览器扩展
- en: Web scraper using Bing Image Search API
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Bing Image Search API的网络爬虫
- en: '|'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Web-based GUI: CustomVision.ai, IBM Watson, Clarifai, Google AutoML'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Web的GUI：CustomVision.ai，IBM Watson，Clarifai，Google AutoML
- en: Create ML
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建ML
- en: Fine-tune using any framework of choice like Keras
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用任何选择的框架，如Keras进行微调
- en: '|'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Create ML, CustomVision.ai, and other GUI tools generate .*mlmodel*.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建ML，CustomVision.ai和其他GUI工具生成.*mlmodel*。
- en: For Keras, use Core ML Tools.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Keras，请使用Core ML工具。
- en: For TensorFlow trained models, use `tf-coreml`.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于TensorFlow训练的模型，请使用`tf-coreml`。
- en: '|'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Let’s dive right in!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Collecting Data
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集数据
- en: To begin solving any computer-vision task using deep learning, we first need
    to have a dataset of images to train on. In this section, we use three different
    approaches to collecting the images of the relevant categories in increasing order
    of time required, from minutes to days.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用深度学习解决任何计算机视觉任务，我们首先需要有一组图像数据集进行训练。在本节中，我们使用三种不同的方法来收集相关类别的图像，所需时间逐渐增加，从几分钟到几天。
- en: 'Approach 1: Find or Collect a Dataset'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法1：查找或收集数据集
- en: The fastest way to get our problem solved is to have an existing dataset in
    hand. There are tons of publicly available datasets for which a category or a
    subcategory might be relevant to our task. For example, Food-101 ([*https://www.vision.ee.ethz.ch/datasets_extra/food-101/*](https://oreil.ly/dkS6X))
    from ETH Zurich contains a class of hot dogs. Alternatively, ImageNet contains
    1,257 images of hot dogs. We can use a random sample of images from the remaining
    classes as “Not Hotdog.”
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 解决问题的最快方法是手头上有现有的数据集。有大量公开可用的数据集，其中一个类别或子类别可能与我们的任务相关。例如，ETH Zurich的Food-101（[*https://www.vision.ee.ethz.ch/datasets_extra/food-101/*]（https://oreil.ly/dkS6X））包含一个热狗类别。另外，ImageNet包含1,257张热狗的图像。我们可以使用剩余类别的随机样本作为“Not
    Hotdog”。
- en: 'To download images from a particular category, you can use the [ImageNet-Utils](https://oreil.ly/ftyOU)
    tool:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要从特定类别下载图像，可以使用[ImageNet-Utils]（https://oreil.ly/ftyOU）工具：
- en: Search for the relevant category on the ImageNet website; for example, “Hot
    dog.”
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在ImageNet网站上搜索相关类别；例如，“热狗”。
- en: 'Note the `wnid` (WordNet ID)in the URL: [*http://image-net.org/synset?wnid=n07697537*](http://image-net.org/synset?wnid=n07697537).'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意URL中的`wnid`（WordNet ID）：[*http://image-net.org/synset?wnid=n07697537*]（http://image-net.org/synset?wnid=n07697537）。
- en: 'Clone the ImageNet-Utils repository:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆ImageNet-Utils存储库：
- en: '[PRE0]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Download the images for the particular category by specifying the `wnid`:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过指定`wnid`下载特定类别的图像：
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In case we can’t find a dataset, we can also build our own dataset by taking
    pictures ourselves with a smartphone. It’s essential that we take pictures representative
    of how our application would be used in the real world. Alternatively, crowdsourcing
    this problem, like asking friends, family, and coworkers, can generate a diverse
    dataset. Another approach used by large companies is to hire contractors who are
    tasked with collecting images. For example, Google Allo released a feature to
    convert selfies into stickers. To build it, they hired a team of artists to take
    an image and create the corresponding sticker so that they could train a model
    on it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们找不到数据集，我们也可以通过使用智能手机自己拍照来构建自己的数据集。我们拍摄的照片必须代表我们的应用程序在现实世界中的使用方式。另外，通过向朋友、家人和同事提问，可以生成一个多样化的数据集。大公司使用的另一种方法是雇佣承包商负责收集图像。例如，Google
    Allo发布了一个功能，可以将自拍照片转换为贴纸。为了构建这个功能，他们雇佣了一组艺术家拍摄照片并创建相应的贴纸，以便他们可以对其进行训练。
- en: Note
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure to check the license under which the images in the dataset are released.
    It’s best to use images released under permissive licenses such as Creative Commons.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 确保检查数据集中图像发布的许可证。最好使用根据宽松许可证发布的图像，如知识共享许可证。
- en: 'Approach 2: Fatkun Chrome Browser Extension'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法2：Fatkun Chrome浏览器扩展
- en: There are several browser extensions that allow us to batch download multiple
    images from a website. One such example is the [Fatkun Batch Download Image](https://oreil.ly/7T4JU),
    a browser extension available on the Chrome browser.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个浏览器扩展可以让我们从网站批量下载多个图像。一个例子是[Fatkun批量下载图像](https://oreil.ly/7T4JU)，这是Chrome浏览器上可用的浏览器扩展。
- en: We can have the entire dataset ready in the following short and quick steps.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下简短快速的步骤准备好整个数据集。
- en: Add the extension to our browser.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将扩展添加到我们的浏览器。
- en: Search for the keyword either on Google or Bing Image search.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google或必应图像搜索中搜索关键字。
- en: Select the appropriate filter for image licenses in the search settings.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择搜索设置中适合图像许可证的过滤器。
- en: After the page reloads, scroll to the bottom of it a few times repeatedly to
    ensure more thumbnails are loaded on the page.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 页面重新加载后，多次向下滚动几次，以确保页面上加载更多缩略图。
- en: Open the extension and select “This Tab” option, as demonstrated in [Figure 12-3](part0014.html#bing_search_results_for_quotation_markho).
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开扩展并选择“此标签”选项，如[图12-3](part0014.html#bing_search_results_for_quotation_markho)所示。
- en: '![Bing Search results for “hot dog”](../images/00312.jpeg)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![“热狗”的必应搜索结果](../images/00312.jpeg)'
- en: Figure 12-3\. Bing Search results for “hot dog”
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3\. “热狗”的必应搜索结果
- en: Notice that all the thumbnails are selected by default. At the top of the screen,
    click the Toggle button to deselect all the thumbnails and select only the ones
    we need. We can set the minimum width and height to be 224 (most pretrained models
    take 224x224 as input size).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，默认情况下选择了所有缩略图。在屏幕顶部，单击切换按钮以取消选择所有缩略图并仅选择我们需要的缩略图。我们可以设置最小宽度和高度为224（大多数预训练模型将224x224作为输入尺寸）。
- en: '![Selecting images through the Fatkun extension](../images/00195.jpeg)'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![通过Fatkun扩展选择图像](../images/00195.jpeg)'
- en: Figure 12-4\. Selecting images through the Fatkun extension
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4\. 通过Fatkun扩展选择图像
- en: In the upper-right corner, click Save Image to download all of the selected
    thumbnails to our computer.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在右上角，单击“保存图像”以将所有选定的缩略图下载到我们的计算机上。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the images shown in the screenshots are iconic images (i.e., the main
    object is in direct focus with a clean background). Chances are that using such
    images exclusively in our model will cause it to not generalize well to real-world
    images. For example, in images with a clean white background (like on an ecommerce
    website), the neural network might incorrectly learn that white background equals
    hot dog. Hence, while performing data collection, ensure that your training images
    are representative of the real world.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，屏幕截图中显示的图像是标志性图像（即主要对象直接聚焦在干净的背景上）。只使用这样的图像在我们的模型中可能导致其无法很好地推广到真实世界的图像。例如，在具有干净白色背景的图像中（如在电子商务网站上），神经网络可能会错误地学习到白色背景等于热狗。因此，在进行数据收集时，请确保您的训练图像代表真实世界。
- en: Tip
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For the negative class of “Not Hotdog,” we want to collect random images that
    are abundantly available. Additionally, collect items that look similar to a hot
    dog but are not; for example, a submarine sandwich, bread, plate, hamburgers,
    and so on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“非热狗”的负类别，我们希望收集大量可用的随机图像。此外，收集看起来类似于热狗但实际上不是的物品；例如，潜水艇三明治、面包、盘子、汉堡等等。
- en: An absence of commonly co-occurring items with hot dogs like plates with food,
    tissues, ketchup bottles or packets, can mistakenly lead the model to think that
    those are the real hot dogs. So be sure to add these to the negative class.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 热狗的常见共现物品缺失，如盘子上的食物、纸巾、番茄酱瓶或包装袋，可能会误导模型认为那些是真正的热狗。因此，请确保将这些添加到负类别中。
- en: When you install a browser extension like Fatkun, it will request permissions
    to read and modify data on all the websites we visit. It might be a good idea
    to disable the extension when you’re not using it for downloading images.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当您安装像Fatkun这样的浏览器扩展时，它将请求权限读取和修改我们访问的所有网站上的数据。当您不使用它下载图像时，最好禁用该扩展。
- en: 'Approach 3: Web Scraper Using Bing Image Search API'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法3：使用必应图像搜索API的网络爬虫
- en: For building larger datasets, using Fatkun to collect images can be a tedious
    process. Additionally, images returned by the Fatkun browser extension are thumbnails
    and not the original size images. For large-scale image collections, we can use
    an API for searching images, like the Bing Image Search API, where we can establish
    certain constraints such as the keyword, image size, and license. Google used
    to have the Image Search API, but it was discontinued in 2011.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于构建更大的数据集，使用Fatkun收集图像可能是一个繁琐的过程。此外，Fatkun浏览器扩展返回的图像是缩略图，而不是原始大小的图像。对于大规模图像集合，我们可以使用搜索图像的API，比如必应图像搜索API，其中我们可以建立一定的约束，如关键字、图像大小和许可证。谷歌曾经有图像搜索API，但在2011年停止了。
- en: Bing’s Search API is an amalgamation of its AI-based image understanding and
    traditional information retrieval methods (i.e., using tags from fields like “alt-text,”
    “metadata,” and “caption”). Many times, we can end up with some number of irrelevant
    images because of misleading tags from these fields. As a result, we want to manually
    parse the collected images to make sure that they are actually relevant to our
    task.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 必应的搜索API是其基于AI的图像理解和传统信息检索方法（即使用来自“alt-text”、“metadata”和“caption”等字段的标签）的融合。由于这些字段的误导性标签，我们往往会得到一些不相关的图像。因此，我们希望手动解析收集的图像，以确保它们实际上与我们的任务相关。
- en: 'When we have a very large image dataset, it can be a daunting task to have
    to go through it manually and filter out all of the poor training examples. It’s
    easier to approach this in an iterative manner, slowly improving the quality of
    the training dataset with each iteration. Here are the high-level steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个非常庞大的图像数据集时，手动筛选出所有质量差的训练示例可能是一项艰巨的任务。以迭代的方式逐步改进训练数据集的质量会更容易。以下是高层次的步骤：
- en: Create a subset of the training data by manually reviewing a small number of
    images. For example, if we have 50k images in our original dataset, we might want
    to manually select around 500 good training examples for the first iteration.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过手动审查少量图像创建训练数据的子集。例如，如果我们的原始数据集中有50k张图像，我们可能希望手动选择大约500个良好的训练示例进行第一次迭代。
- en: Train the model on those 500 images.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对这500张图像进行训练。
- en: Test the model on the remaining images and get the confidence value for each
    image.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在剩余的图像上测试模型，并为每个图像获取置信度值。
- en: Among the images with the least confidence values (i.e., often mispredictions),
    review a subset (say, 500) and discard the irrelevant images. Add the remaining
    images from this subset to the training set.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在置信度值最低的图像中（即经常错误预测的图像），审查一个子集（比如500个）并丢弃不相关的图像。将此子集中剩余的图像添加到训练集中。
- en: Repeat steps 1 through 4 for a few iterations until we are happy with the quality
    of the model.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1到4几次，直到我们对模型的质量感到满意。
- en: This is a form of semisupervised learning.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种半监督学习形式。
- en: Tip
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can improve the model accuracy further by reusing the discarded images as
    negative training examples.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将被丢弃的图像重新用作负训练示例来进一步提高模型的准确性。
- en: Note
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For a large set of images that don’t have labels, you might want to use co-occurrence
    of other defining text as labels; for example, hashtags, emojis, alt-text, etc.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一组没有标签的大量图像，您可能希望使用其他定义文本的共现作为标签；例如，标签、表情符号、alt文本等。
- en: Facebook built a dataset of 3.5 billion images using the hashtags from the text
    of corresponding posts as weak labels, training them, and eventually fine tuning
    them on the ImageNet dataset. This model beat the state-of-the-art result by 2%
    (85% top 1% accuracy).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook利用帖子文本中的标签构建了一个包含35亿张图像的数据集，将它们作为弱标签进行训练，并最终在ImageNet数据集上进行微调。这个模型比最先进的结果提高了2%（85%的前1%准确率）。
- en: Now that we have collected our image datasets, let’s finally begin training
    them.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经收集了图像数据集，让我们最终开始训练它们。
- en: Training Our Model
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们的模型
- en: Broadly speaking there are three easy ways to train, all of which we have discussed
    previously. Here, we provide a brief overview of a few different approaches.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上说，有三种简单的训练方式，我们之前已经讨论过。在这里，我们提供了几种不同方法的简要概述。
- en: 'Approach 1: Use Web UI-based Tools'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法1：使用基于Web UI的工具
- en: As discussed in [Chapter 8](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b),
    there are several tools to build custom models by supplying labeled images and
    performing training using the web UI. Microsoft’s CustomVision.ai, Google AutoML,
    IBM Watson Visual Recognition, Clarifai, and Baidu EZDL are a few examples. These
    methods are code free and many provide simple drag-and-drop GUIs for training.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第8章](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b)中讨论的，有几种工具可以通过提供带标签的图像并使用Web
    UI进行训练来构建自定义模型。微软的CustomVision.ai、Google AutoML、IBM Watson Visual Recognition、Clarifai和Baidu
    EZDL是几个例子。这些方法无需编码，许多提供简单的拖放GUI进行训练。
- en: 'Let’s look at how we can have a mobile-friendly model ready in less than five
    minutes using CustomVision.ai:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在不到五分钟的时间内使用CustomVision.ai创建一个适合移动设备的模型：
- en: Go to [*http://customvision.ai*](http://customvision.ai), and make a new project.
    Because we want to export the trained model to a mobile phone, select a compact
    model type. Since our domain is food related, select “Food (Compact),” as shown
    in [Figure 12-5](part0014.html#define_a_new_project_on_customvisiondota).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问[*http://customvision.ai*](http://customvision.ai)，并创建一个新项目。因为我们想要将训练好的模型导出到手机上，所以选择一个紧凑型模型类型。由于我们的领域与食品相关，选择“食品（紧凑型）”，如[图12-5](part0014.html#define_a_new_project_on_customvisiondota)所示。
- en: '![Define a new project on CustomVision.ai](../images/00026.jpeg)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![在CustomVision.ai上定义一个新项目](../images/00026.jpeg)'
- en: Figure 12-5\. Define a new project on CustomVision.ai
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-5\. 在CustomVision.ai上定义一个新项目
- en: Upload the images and assign tags (labels), as depicted in [Figure 12-6](part0014.html#uploading_images_on_the_customvisiondota).
    Upload at least 30 images per tag.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传图像并分配标签（标签），如[图12-6](part0014.html#uploading_images_on_the_customvisiondota)所示。每个标签至少上传30张图像。
- en: '![Uploading images on the CustomVision.ai dashboard. Note that the tags have
    been populated as Hotdog and Not Hotdog](../images/00110.jpeg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![在CustomVision.ai仪表板上上传图像。请注意，标签已填充为Hotdog和Not Hotdog](../images/00110.jpeg)'
- en: Figure 12-6\. Uploading images on the CustomVision.ai dashboard. Note that the
    tags have been populated as Hotdog and Not Hotdog
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-6. 在CustomVision.ai仪表板上上传图片。请注意，标签已填充为Hotdog和Not Hotdog
- en: Click the Train button. A dialog box opens, as shown in [Figure 12-7](part0014.html#options_for_training_type).
    Fast Training essentially trains the last few layers, whereas Advanced Training
    can potentially tune the full network giving even higher accuracy (and obviously
    take more time and money). The Fast Training option should be sufficient for most
    cases.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击训练按钮。一个对话框会打开，如[图12-7](part0014.html#options_for_training_type)所示。快速训练主要训练最后几层，而高级训练可能会调整整个网络，从而获得更高的准确性（显然需要更多时间和金钱）。对于大多数情况，快速训练选项应该足够了。
- en: '![Options for training type](../images/00072.jpeg)'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![训练类型选项](../images/00072.jpeg)'
- en: Figure 12-7\. Options for training type
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-7. 训练类型选项
- en: In under a minute, a screen should appear, showing the precision and recall
    for the newly trained model per category, as shown in [Figure 12-8](part0014.html#precisioncomma_recallcomma_and_average_p).
    (This should ring a bell because we had discussed precision and recall earlier
    in the book.)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不到一分钟，一个屏幕应该出现，显示每个类别新训练模型的精度和召回率，如[图12-8](part0014.html#precisioncomma_recallcomma_and_average_p)所示。（这应该让你想起我们之前在书中讨论过的精度和召回率。）
- en: '![Precision, Recall, and Average Precision of the newly trained model](../images/00031.jpeg)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![新训练模型的精度、召回率和平均精度](../images/00031.jpeg)'
- en: Figure 12-8\. Precision, Recall, and Average Precision of the newly trained
    model
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-8. 新训练模型的精度、召回率和平均精度
- en: Play with the probability threshold to see how it changes the model’s performance.
    The default 90% threshold achieves pretty good results. The higher the threshold,
    the more precise the model becomes, but at the expense of reduced recall.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整概率阈值，看看它如何改变模型的性能。默认的90%阈值可以取得相当不错的结果。阈值越高，模型变得越精确，但召回率会降低。
- en: Press the Export button and select the iOS platform ([Figure 12-9](part0014.html#the_model_exporter_options_in_customvisi)).
    Internally, CustomVision.ai converts the model to Core ML (or TensorFlow Lite
    if you’re exporting for Android).
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击导出按钮，选择iOS平台（[图12-9](part0014.html#the_model_exporter_options_in_customvisi)）。在内部，CustomVision.ai将模型转换为Core
    ML（如果要导出到Android，则转换为TensorFlow Lite）。
- en: '![The model exporter options in CustomVision.ai](../images/00318.jpeg)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CustomVision.ai中的模型导出选项](../images/00318.jpeg)'
- en: Figure 12-9\. The model exporter options in CustomVision.ai
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-9. CustomVision.ai中的模型导出选项
- en: And we’re done, all without writing a single line of code! Now let’s look at
    an even more convenient way of training without coding.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了，而且完全不需要编写一行代码！现在让我们看看更方便的无编码训练方式。
- en: 'Approach 2: Use Create ML'
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法2：使用Create ML
- en: In 2018, Apple launched Create ML as a way for developers within the Apple ecosystem
    to train computer-vision models natively. Developers could open a *playground*
    and write a few lines of Swift code to train an image classifier. Alternatively,
    they could use `CreateMLUI` import to display a limited GUI training experience
    within the playground. It was a good way to get Swift developers to deploy Core
    ML models without requiring much experience in machine learning.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，苹果推出了Create ML，作为苹果生态系统内开发者训练计算机视觉模型的一种方式。开发者可以打开一个*playground*，写几行Swift代码来训练图像分类器。或者，他们可以使用`CreateMLUI`导入在playground内显示有限的GUI训练体验。这是一个让Swift开发者能够部署Core
    ML模型而无需太多机器学习经验的好方法。
- en: A year later, at the Apple Worldwide Developers Conference (WWDC) 2019, Apple
    lowered the barrier even further by announcing the standalone Create ML app on
    macOS Catalina (10.15). It provides an easy-to-use GUI to train neural networks
    without needing to write any code at all. Training a neural network simply became
    a matter of dragging and dropping files into this UI. In addition to supporting
    the image classifier, they also announced support for object detectors, NLP, sound
    classification, activity classification (classify activities based on motion sensor
    data from Apple Watch and iPhone), as well as tabular data (including recommendation
    systems).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一年后，在2019年的苹果全球开发者大会（WWDC）上，苹果通过在macOS Catalina（10.15）上宣布独立的Create ML应用，进一步降低了门槛。它提供了一个易于使用的GUI，可以训练神经网络而无需编写任何代码。训练神经网络只需将文件拖放到此UI中。除了支持图像分类器外，他们还宣布支持目标检测器、NLP、声音分类、活动分类（根据来自Apple
    Watch和iPhone的运动传感器数据对活动进行分类）以及表格数据（包括推荐系统）。
- en: And, it’s fast! Models can be trained in under a minute. This is because it
    uses transfer learning, so it doesn’t need to train all of the layers in the network.
    It also supports various data augmentations such as rotations, blur, noise, and
    so on, and all you need to do is click checkboxes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，速度很快！模型可以在不到一分钟内训练完成。这是因为它使用迁移学习，所以不需要训练网络中的所有层。它还支持各种数据增强，如旋转、模糊、噪声等，你只需要点击复选框。
- en: Before Create ML came along, it was generally a given that anyone seeking to
    train a serious neural network in a reasonable amount of time had to own an NVIDIA
    GPU. Create ML took advantage of the onboard Intel and/or Radeon graphics cards,
    which allowed faster training on MacBooks without the need to purchase additional
    hardware. Create ML allows us to train multiple models, from different data sources,
    all at the same time. It can benefit particularly from powerful hardware such
    as the Mac Pro or even an external GPU (eGPU).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在Create ML出现之前，通常认为任何想在合理时间内训练一个严肃的神经网络的人都必须拥有NVIDIA GPU。Create ML利用了内置的Intel和/或Radeon显卡，使MacBook上的训练速度更快，而无需购买额外的硬件。Create
    ML允许我们同时训练多个模型，来自不同的数据源。它特别受益于强大的硬件，如Mac Pro甚至外部GPU（eGPU）。
- en: One major motivation to use Create ML is the size of the models it outputs.
    A full model can be broken down into a base model (which emits features) and lighter
    task-specific classification layers. Apple ships the base models into each of
    its operating systems. So, Create ML just needs to output the task-specific classifier.
    How small are these models? As little as a few kilobytes (compared to more than
    15 MB for a MobileNet model, which is already pretty small for a CNN). This is
    important in a day and age when more and more app developers are beginning to
    incorporate deep learning into their apps. The same neural networks do not need
    to be unnecessarily replicated across several apps consuming valuable storage
    space.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: In short, Create ML is easy, speedy, and tiny. Sounds too good to be true. Turns
    out the flip-side of having full vertical integration is that the developers are
    tied into the Apple ecosystem. Create ML exports only *.mlmodel* files, which
    can be used exclusively on Apple operating systems such as iOS, iPadOS, macOS,
    tvOS, and watchOS. Sadly, Android integration is not yet a reality for Create
    ML.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we build the Not Hotdog classifier using Create ML:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Open the Create ML app, click New Document, and select the Image Classifier
    template from among the several options available (including Sound, Activity,
    Text, Tabular), as shown in [Figure 12-10](part0014.html#choosing_a_template_for_a_new_project).
    Note that this is only available on Xcode 11 (or greater), on macOS 10.15 (or
    greater).
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Choosing a template for a new project](../images/00277.jpeg)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 12-10\. Choosing a template for a new project
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: In the next screen, enter a name for the project, and then select Done.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to sort the data into the correct directory structure. As [Figure 12-11](part0014.html#train_and_test_data_in_separate_director)
    illustrates, we place images in directories that have the names of their labels.
    It is useful to have separate train and test datasets with their corresponding
    directories.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Train and test data in separate directories](../images/00235.jpeg)'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 12-11\. Train and test data in separate directories
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Point the UI to the training and test data directories, as shown in [Figure 12-12](part0014.html#training_interface_in_create_ml).
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Training interface in Create ML](../images/00200.jpeg)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 12-12\. Training interface in Create ML
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 12-12](part0014.html#training_interface_in_create_ml) shows the UI
    after you select the train and test data directories. Notice that the validation
    data was automatically selected by Create ML. Additionally, notice the augmentation
    options available. It is at this point that we can click the Play button (the
    right-facing triangle; see [Figure 12-13](part0014.html#create_ml_screen_that_opens_after_loadin))
    to start the training process.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Create ML screen that opens after loading train and test data](../images/00164.jpeg)'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 12-13\. Create ML screen that opens after loading train and test data
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As you experiment, you will quickly notice that each augmentation that we add
    will make the training slower. To set a quick baseline performance metric, we
    should avoid using augmentations in the first run. Subsequently, we can experiment
    with adding more and more augmentations to assess how they affect the quality
    of the model.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When the training completes, we can see how the model performed on the training
    data, (auto-selected) validation data, and the test data, as depicted in [Figure 12-14](part0014.html#the_create_ml_screen_after_training_comp).
    At the bottom of the screen, we can also observe how long the training process
    took and the size of the final model. 97% test accuracy in under two minutes.
    And all that with a 17 KB output. Not too shabby.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![The Create ML screen after training completes](../images/00113.jpeg)'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 12-14\. The Create ML screen after training completes
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We’re so close now—we just need to export the final model. Drag the Output button
    (highlighted in [Figure 12-14](part0014.html#the_create_ml_screen_after_training_comp))
    to the desktop to create the *.mlmodel* file.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在非常接近了，我们只需要导出最终模型。将输出按钮（在[图12-14](part0014.html#the_create_ml_screen_after_training_comp)中突出显示）拖到桌面上创建*.mlmodel*文件。
- en: We can double-click on the newly exported *.mlmodel* file to inspect the input
    and output layers, as well as test drive the model by dropping images into it,
    as shown in [Figure 12-15](part0014.html#the_model_inspector_ui_within_xcode).
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以双击新导出的*.mlmodel*文件，检查输入和输出层，以及通过将图像拖放到其中来测试模型，如[图12-15](part0014.html#the_model_inspector_ui_within_xcode)所示。
- en: '![The model inspector UI within Xcode](../images/00076.jpeg)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Xcode中的模型检查器UI](../images/00076.jpeg)'
- en: Figure 12-15\. The model inspector UI within Xcode
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-15\. Xcode中的模型检查器UI
- en: The model is now ready to be plugged into apps on any Apple device.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型现在已准备好插入到任何苹果设备的应用程序中。
- en: Note
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Create ML uses transfer learning, training only the last few layers. Depending
    on your use case, the underlying model that Apple provides you might be insufficient
    to make high-quality predictions. This is because you are unable to train the
    earlier layers in the model, thereby restricting the potential to which the model
    can be tuned. For most day-to-day problems, this should not be an issue. However,
    for very domain-specific applications like X-rays, or very similar-looking objects
    for which the tiniest of details matter (like distinguishing currency notes),
    training a full CNN would be a better approach. We look at doing so in the following
    section.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Create ML使用迁移学习，仅训练最后几层。根据您的用例，苹果提供的底层模型可能不足以进行高质量的预测。这是因为您无法训练模型的早期层，从而限制了模型可以调整的潜力。对于大多数日常问题，这不应该是一个问题。但是，对于非常特定领域的应用，如X射线，或者外观非常相似的对象，细微的细节很重要（如区分货币票据），训练完整的CNN将是一个更好的方法。我们将在下一节中探讨如何做到这一点。
- en: 'Approach 3: Fine Tuning Using Keras'
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法3：使用Keras进行微调
- en: By now, we have become experts in using Keras. This option can get us even higher
    accuracy if we are up for experimentation and are willing to spend more time training
    the model. Let’s reuse the code from [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)
    and modify parameters such as directory and filename, batch size, and number of
    images. You will find the code on the book’s GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-12/1-keras-custom-classifier-with-transfer-learning.ipynb*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经成为使用Keras的专家。如果我们愿意进行实验并愿意花更多时间训练模型，这个选项可以让我们获得更高的准确性。我们可以重用[第3章](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)中的代码，并修改参数，如目录和文件名，批量大小和图像数量。您可以在书的GitHub网站上找到代码（请参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)），位于*code/chapter-12/1-keras-custom-classifier-with-transfer-learning.ipynb*。
- en: The model training should take a few minutes to complete, depending on the hardware,
    and at the end of the training, we should have a *NotHotDog.h5* file ready on
    the disk.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练应该需要几分钟才能完成，具体取决于硬件，在训练结束时，我们应该在磁盘上准备好一个*NotHotDog.h5*文件。
- en: Model Conversion Using Core ML Tools
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Core ML工具进行模型转换
- en: As discussed in [Chapter 11](part0013.html#CCNA3-13fa565533764549a6f0ab7f11eed62b),
    there are several ways of converting our models to the Core ML format.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第11章](part0013.html#CCNA3-13fa565533764549a6f0ab7f11eed62b)中所讨论的，有几种将我们的模型转换为Core
    ML格式的方法。
- en: 'Models generated from CustomVision.ai are directly available in Core ML format,
    hence no conversion is necessary. For models trained in Keras, Core ML Tools can
    help convert as follows. Note that because we are using a MobileNet model, which
    uses a custom layer called `relu6`, we need to import `CustomObjectScope`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从CustomVision.ai生成的模型直接以Core ML格式可用，因此无需转换。对于在Keras中训练的模型，Core ML工具可以帮助进行转换。请注意，因为我们使用了一个使用名为`relu6`的自定义层的MobileNet模型，我们需要导入`CustomObjectScope`：
- en: '[PRE2]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that we have a Core ML model ready, all we need to do is build the app.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个准备好的Core ML模型，我们只需要构建应用程序。
- en: Building the iOS App
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建iOS应用程序
- en: We can use the code from [Chapter 11](part0013.html#CCNA3-13fa565533764549a6f0ab7f11eed62b)
    and simply replace the *.mlmodel* with the newly generated model file, as demonstrated
    in [Figure 12-16](part0014.html#loading_the_dotmlmodel_into_xcode).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用[第11章](part0013.html#CCNA3-13fa565533764549a6f0ab7f11eed62b)中的代码，只需用新生成的模型文件替换*.mlmodel*，如[图12-16](part0014.html#loading_the_dotmlmodel_into_xcode)所示。
- en: '![Loading the .mlmodel into Xcode](../images/00035.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![将.mlmodel加载到Xcode中](../images/00035.jpeg)'
- en: Figure 12-16\. Loading the `.mlmodel` into Xcode
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-16\. 将`.mlmodel`加载到Xcode中
- en: Now, compile and run the app and you’re done! [Figure 12-17](part0014.html#our_app_identifying_the_hot_dog)
    presents the awesome results.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，编译并运行应用程序，完成了！[图12-17](part0014.html#our_app_identifying_the_hot_dog)展示了令人惊叹的结果。
- en: '![Our app identifying the hot dog](../images/00323.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![我们的应用程序识别热狗](../images/00323.jpeg)'
- en: Figure 12-17\. Our app identifying the hot dog
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-17\. 我们的应用程序识别热狗
- en: Further Exploration
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索
- en: Can we make this application more interesting? We can build an actual “Shazam
    for food” by training for all the categories in the Food-101 dataset, which we
    cover in the next chapter. Additionally, we can enhance the UI compared to the
    barebones percentages our current app shows. And, to make it viral just like “Not
    Hotdog,” provide a way to share the classifications to social media platforms.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使这个应用程序更有趣吗？我们可以通过在下一章中涵盖的Food-101数据集中训练所有类别来构建一个真正的“Shazam for food”。此外，我们可以改进UI，与我们当前应用程序显示的基本百分比相比。为了使其像“Not
    Hotdog”一样病毒传播，提供一种将分类分享到社交媒体平台的方式。
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we worked through an end-to-end pipeline of collecting data,
    training and converting a model, and using it in the real world on an iOS device.
    For each step of the pipeline, we have explored a few different options in varying
    degrees of complexity. And, we have placed the concepts covered in the previous
    chapters in the context of a real-world application.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们通过一个端到端的流程，收集数据、训练和转换模型，并在iOS设备上实际应用中使用它。对于流程的每一步，我们探索了一些不同复杂程度的选项。而且，我们将前几章涵盖的概念放在了一个真实应用的背景下。
- en: And now, like Jian-Yang, go make your millions!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就像建阳一样，去赚取你的百万美元吧！
