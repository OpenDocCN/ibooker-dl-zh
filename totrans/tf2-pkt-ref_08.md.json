["```py\n    strategy = tf.distribute.MirroredStrategy()\n    ```", "```py\n    print('Number of devices: {}'.format(\n     strategy.num_replicas_in_sync))\n    ```", "```py\n    Number of devices: 2\n    ```", "```py\n    with strategy.scope():\n      model = tf.keras.Sequential([\n      \u2026\n      ])\n      model.build(\u2026)\n      model.compile(\n        loss=tf.keras.losses\u2026,\n        optimizer=\u2026,\n        metrics=\u2026)\n    ```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport numpy as np\nimport os\nfrom datetime import datetime\n```", "```py\nstrategy = tf.distribute.MirroredStrategy()\n```", "```py\nINFO:tensorflow:Using MirroredStrategy with devices \n('/job:localhost/replica:0/task:0/device:GPU:0', \n'/job:localhost/replica:0/task:0/device:GPU:1')\n```", "```py\nprint('Number of devices: {}'.format(\nstrategy.num_replicas_in_sync))\nNumber of devices: 2\n```", "```py\n(train_images, train_labels), (test_images, test_labels) = \ndatasets.cifar10.load_data()\n# Normalize pixel values to be between 0 and 1\ntrain_images, test_images = train_images / 255.0, \ntest_images / 255.0\n```", "```py\n# Plain-text name in alphabetical order. See\nhttps://www.cs.toronto.edu/~kriz/cifar.html\nCLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat', \n               'deer','dog', 'frog', 'horse', 'ship', 'truck']\n```", "```py\nvalidation_dataset = tf.data.Dataset.from_tensor_slices(\n(test_images[:500], \n test_labels[:500]))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n(test_images[500:], \n test_labels[500:]))\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n(train_images,\n train_labels))\n```", "```py\ntrain_dataset_size = len(list(train_dataset.as_numpy_iterator()))\nprint('Training data sample size: ', train_dataset_size)\n\nvalidation_dataset_size = len(list(validation_dataset.\nas_numpy_iterator()))\nprint('Validation data sample size: ', validation_dataset_size)\n\ntest_dataset_size = len(list(test_dataset.as_numpy_iterator()))\nprint('Test data sample size: ', test_dataset_size)\n```", "```py\nTraining data sample size:  50000\nValidation data sample size:  500\nTest data sample size:  9500\n```", "```py\nTRAIN_BATCH_SIZE = 128\ntrain_dataset = train_dataset.shuffle(50000).batch(\nTRAIN_BATCH_SIZE, drop_remainder=True)\n\nvalidation_dataset = validation_dataset.batch(validation_dataset_size)\ntest_dataset = test_dataset.batch(test_dataset_size)\n```", "```py\nSTEPS_PER_EPOCH = train_dataset_size // TRAIN_BATCH_SIZE\nVALIDATION_STEPS = 1\n```", "```py\nwith strategy.scope():\n  model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\nactivation='relu', name = 'conv_1',\n      kernel_initializer='glorot_uniform',\npadding='same', input_shape = (32,32,3)),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\nactivation='relu', name = 'conv_2',\n      kernel_initializer='glorot_uniform', padding='same'),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Flatten(name = 'flat_1'),\n    tf.keras.layers.Dense(256, activation='relu',\nkernel_initializer='glorot_uniform', name = 'dense_64'),\n    tf.keras.layers.Dense(10, activation='softmax',\nname = 'custom_class')\n  ])\n  model.build([None, 32, 32, 3])\n\n  model.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True),\n    optimizer='adam',\n    metrics=['accuracy'])\n```", "```py\nMODEL_NAME = 'myCIFAR10-{}'.format(datetime.now().strftime(\n\"%Y%m%d-%H%M%S\"))\n\ncheckpoint_dir = './' + MODEL_NAME\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt-{epoch}\")\nprint(checkpoint_prefix)\n```", "```py\nmyCheckPoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    monitor='val_accuracy',\n    mode='max',\n    save_weights_only=True,\n    save_best_only = True)\n```", "```py\nmyCallbacks = [\n    myCheckPoint\n]\n```", "```py\nhist = model.fit(\n    train_dataset,\n    epochs=12,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=validation_dataset,\n    validation_steps=VALIDATION_STEPS,\n    callbacks=myCallbacks).history\n```", "```py\nhist['val_accuracy']\n```", "```py\nmax_value = max(hist['val_accuracy'])\nmax_index = hist['val_accuracy'].index(max_value)\nprint('Best epoch: ', max_index + 1)\n```", "```py\ntf.train.latest_checkpoint(checkpoint_dir)\n```", "```py\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n```", "```py\neval_loss, eval_acc = model.evaluate(test_dataset)\nprint('Eval loss: {}, Eval Accuracy: {}'.format(\neval_loss, eval_acc))\n```", "```py\n1/1 [==============================] - 0s 726us/step\n loss: 1.7533\n accuracy: 0.7069 \nEval loss: 1.753335952758789, \n Eval Accuracy: 0.706947386264801\n```", "```py\nfrom sparkdl import HorovodRunner\nhr = HorovodRunner(np=2)\nhr.run(train_hvd, checkpoint_path=CHECKPOINT_PATH, \nlearning_rate=LEARNING_RATE)\n```", "```py\ndef get_model(num_classes):\n  from tensorflow.keras import models\n  from tensorflow.keras import layers\n\n  model = tf.keras.Sequential([\n  tf.keras.layers.Conv2D(32, \n    kernel_size=(3, 3), \n    activation='relu', \n    name = 'conv_1',\n    kernel_initializer='glorot_uniform', \n    padding='same', \n    input_shape = (32,32,3)),\n  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n  tf.keras.layers.Conv2D(64, \n    kernel_size=(3, 3), \n    activation='relu', \n    name = 'conv_2',\n    kernel_initializer='glorot_uniform', \n    padding='same'),\n  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n  tf.keras.layers.Flatten(name = 'flat_1'),\n  tf.keras.layers.Dense(256, \n    activation='relu', \n    kernel_initializer='glorot_uniform', \n    name = 'dense_64'),\n  tf.keras.layers.Dense(num_classes, \n    activation='softmax', \n    name = 'custom_class')\n  ])\n  model.build([None, 32, 32, 3])\n  return model\n```", "```py\ndef get_dataset(num_classes, rank=0, size=1):\n  from tensorflow.keras import backend as K\n  from tensorflow.keras import datasets, layers, models\n  from tensorflow.keras.models import Sequential\n  import tensorflow as tf\n  from tensorflow import keras\n  import horovod.tensorflow.keras as hvd\n  import numpy as np\n\n  (train_images, train_labels), (test_images, test_labels) =\n      datasets.cifar10.load_data()\n\n  #50000 train samples, 10000 test samples.\n  train_images = train_images[rank::size]\n  train_labels = train_labels[rank::size]\n\n  test_images = test_images[rank::size]\n  test_labels = test_labels[rank::size]\n\n  # Normalize pixel values to be between 0 and 1\n  train_images, test_images = train_images / 255.0, \n    test_images / 255.0\n\n  return train_images, train_labels, test_images, test_labels\n```", "```py\nhvd.callbacks.BroadcastGlobalVariablesCallback(0)\n```", "```py\nhvd.callbacks.MetricAverageCallback()\n```", "```py\nhvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5)\n```", "```py\ntf.keras.callbacks.ReduceLROnPlateau(patience=10, factor = 0.2)\n```", "```py\ncallbacks = [\n   hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n   hvd.callbacks.MetricAverageCallback(),\n   hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, \n     verbose=1),\n   tf.keras.callbacks.ReduceLROnPlateau(patience=10, verbose=1)\n]\n```", "```py\nif hvd.rank() == 0:\n   callbacks.append(keras.callbacks.ModelCheckpoint(\n   filepath=checkpoint_path,\n   monitor='val_accuracy',\n   mode='max',\n   save_best_only = True\n   ))\n```", "```py\noptimizer = tf.keras.optimizers.Adadelta(\nlr=learning_rate * hvd.size())\noptimizer = hvd.DistributedOptimizer(optimizer)\n```", "```py\nimport tensorflow as tf\nimport horovod.tensorflow.keras as hvd\nimport os\nimport time\n\ndef get_dataset(num_classes, rank=0, size=1):\n  from tensorflow.keras import backend as K\n  from tensorflow.keras import datasets, layers, models\n  from tensorflow.keras.models import Sequential\n  import tensorflow as tf\n  from tensorflow import keras\n  import horovod.tensorflow.keras as hvd\n  import numpy as np\n\n  (train_images, train_labels), (test_images, test_labels) = \ndatasets.cifar10.load_data()\n  #50000 train samples, 10000 test samples.\n  train_images = train_images[rank::size]\n  train_labels = train_labels[rank::size]\n\n  test_images = test_images[rank::size]\n  test_labels = test_labels[rank::size]\n\n  # Normalize pixel values to be between 0 and 1\n  train_images, test_images = train_images / 255.0, \n    test_images / 255.0\n\n  return train_images, train_labels, test_images, test_labels\n```", "```py\ndef get_model(num_classes):\n  from tensorflow.keras import models\n  from tensorflow.keras import layers\n\n  model = tf.keras.Sequential([\n  tf.keras.layers.Conv2D(32, \n    kernel_size=(3, 3), \n    activation='relu', \n    name = 'conv_1',\n    kernel_initializer='glorot_uniform', \n    padding='same', \n    input_shape = (32,32,3)),\n  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n  tf.keras.layers.Conv2D(64, kernel_size=(3, 3), \n    activation='relu', \n    name = 'conv_2',\n    kernel_initializer='glorot_uniform', \n    padding='same'),\n  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n  tf.keras.layers.Flatten(name = 'flat_1'),\n  tf.keras.layers.Dense(256, activation='relu', \n     kernel_initializer='glorot_uniform', \n     name = 'dense_64'),\n  tf.keras.layers.Dense(num_classes, \n     activation='softmax', \n     name = 'custom_class')\n  ])\n  model.build([None, 32, 32, 3])\n  return model\n```", "```py\ndef train_hvd(checkpoint_path, learning_rate=1.0):\n\n  # Import tensorflow modules to each worker\n  from tensorflow.keras import backend as K\n  from tensorflow.keras.models import Sequential\n  import tensorflow as tf\n  from tensorflow import keras\n  import horovod.tensorflow.keras as hvd\n  import numpy as np\n```", "```py\n\n# Initialize Horovod\nhvd.init()\n\n# Pin GPU to be used to process local rank (one GPU per process)\n# These steps are skipped on a CPU cluster\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n  tf.config.experimental.set_memory_growth(gpu, True)\nif gpus:\n  tf.config.experimental.set_visible_devices(\n  gpus[hvd.local_rank()], 'GPU')\n\nprint(' Horovod size (processes): ', hvd.size())\n```", "```py\n\n  # Call the get_dataset function you created, this time with the\n    Horovod rank and size\n  num_classes = 10\n  train_images, train_labels, test_images, test_labels = get_dataset(\nnum_classes, hvd.rank(), hvd.size())\n\n  validation_dataset = tf.data.Dataset.from_tensor_slices(\n    (test_images, test_labels))\n  train_dataset = tf.data.Dataset.from_tensor_slices(\n    (train_images, train_labels))\n```", "```py\n  NUM_CLASSES = len(np.unique(train_labels))\n  BUFFER_SIZE = 10000\n  BATCH_SIZE_PER_REPLICA = 64\n  validation_dataset_size = len(test_labels)\n  BATCH_SIZE = BATCH_SIZE_PER_REPLICA * hvd.size()\n  train_dataset = train_dataset.repeat().shuffle(BUFFER_SIZE).\n    batch(BATCH_SIZE\n  validation_dataset = validation_dataset.\n  repeat().shuffle(BUFFER_SIZE).\nbatch(BATCH_SIZE, drop_remainder = True)\n\n  train_dataset_size = len(train_labels)\n\n  print('Training data sample size: ', train_dataset_size)\n\n  validation_dataset_size = len(test_labels)\n  print('Validation data sample size: ', validation_dataset_size)\n```", "```py\nTRAIN_DATASET_SIZE = len(train_labels)\nSTEPS_PER_EPOCH = TRAIN_DATASET_SIZE // BATCH_SIZE_PER_REPLICA\nVALIDATION_STEPS = validation_dataset_size // \n  BATCH_SIZE_PER_REPLICA\nEPOCHS = 20\n```", "```py\nmodel = get_model(10)\n\n# Adjust learning rate based on number of GPUs\noptimizer = tf.keras.optimizers.Adadelta(\n             lr=learning_rate * hvd.size())\n\n# Use the Horovod Distributed Optimizer\noptimizer = hvd.DistributedOptimizer(optimizer)\n\nmodel.compile(optimizer=optimizer,\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(\n            from_logits=True),\n            metrics=['accuracy'])\n```", "```py\n# Create a callback to broadcast the initial variable states from\n  rank 0 to all other processes.\n# This is required to ensure consistent initialization of all \n# workers when training is started with random weights or \n# restored from a checkpoint.\ncallbacks = [\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n    hvd.callbacks.MetricAverageCallback(),\n    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, \n      verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(patience=10, \n      verbose=1)\n  ]\n```", "```py\n# Save checkpoints only on worker 0 to prevent conflicts between \n# workers\nif hvd.rank() == 0:\n    callbacks.append(keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_path,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only = True\n    ))\n```", "```py\nmodel.fit(train_dataset,\n            batch_size=BATCH_SIZE,\n            epochs=EPOCHS,\n            steps_per_epoch=STEPS_PER_EPOCH,\n            callbacks=callbacks,\n            validation_data=validation_dataset,\n            validation_steps=VALIDATION_STEPS,\n           verbose=1)\nprint('DISTRIBUTED TRAINING DONE')\n```", "```py\n# Create directory\ncheckpoint_dir = '/dbfs/ml/CIFAR10DistributedDemo/train/{}/'.\nformat(time.time())\nos.makedirs(checkpoint_dir)\nprint(checkpoint_dir)\n```", "```py\nfrom sparkdl import HorovodRunner\n\ncheckpoint_path = checkpoint_dir + '/checkpoint-{epoch}.ckpt'\nlearning_rate = 0.1\nhr = HorovodRunner(np=2)\nhr.run(train_hvd, checkpoint_path=checkpoint_path, \n       learning_rate=learning_rate)\n```", "```py\nls -lrt  /dbfs/ml/CIFAR10DistributedDemo/train/1615074200.2146788/\n```", "```py\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-9.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-8.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-7.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-6.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-5.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-4.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-3.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-2.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-20.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-1.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-19.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-17.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-16.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-15.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-14.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-13.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-11.ckpt/\ndrwxrwxrwx 2 root root 4096 Mar 6 23:18 checkpoint-10.ckpt/\n```"]