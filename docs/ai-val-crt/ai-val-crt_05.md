# 第五章：生、死、买、试——许多决定将由 AI 做出

我们提出了这个类似苏斯博士的章节标题，因为我们觉得它完美地捕捉了每周揭示的奇妙且日益复杂的世界。我们承认，我们的标题选择可能有点过于夸张——或者也许恰到好处——但它的目的是吸引你的注意。在本章中，我们将提供一个关于治理和 AI 不断演变景观的洞察：它的方向，值得思考的内容，以及为什么它很重要。

###### 注意

这可能感觉像是上一章的陈词滥调，但我们会再说一遍——仅就本章主题而言，可能已经，或者很可能已经，有整本书的篇幅来讨论。自然地，我们在这里无法涵盖每一个方面，并且故意避免深入探讨特定于 AI 的法规迷宫。为什么？因为它们浩如烟海，且不断变化。从跨国协议到特定国家的法律，从州或省级规则到甚至城市层面的政策，其数量令人震惊。更重要的是，当这本书到达你手中时，其中很大一部分内容可能已经发生了变化（当你写一本关于 AI 的书时，还有什么新鲜事呢）。因此，我们认为最好给你一些更多工具，这些工具可以帮助你导航任何法规，而不会陷入不断变化的细节中。

这非常重要，我们认为值得重申我们的立场：我们认为，在领导者的旅程开始之前（或者很快，因为旅程已经开始了）他们需要决定的第一件事可能是，他们的公司将成为 AI 领域的支持者还是旁观者。积极主动的个人，或称“支持者”，是道德行为的先驱，常常为他人树立标准。相反，“旁观者”如果未能负责任地采取行动，可能会无意中促使政府采取过度的监管行动，因为他们的不作为突显了对监督和控制的需求。世界见证了社交媒体的旁观者。虽然这本书的范围不包括深入探讨社交媒体的利弊（两者都有很多），但政府却停滞不前，不知道如何或应该采取什么行动，直到问题已经无法挽回。当然，监管 AI 的问题在于它需要以“正确的速度”进行，但监管机构往往以糖浆的速度移动。

也许我们可以用超人这个著名故事来简化我们的信息。回想一下，他被收养的养父母约翰·肯特和玛莎·肯特在地球上发现时还是一个婴儿，他们给他取名为克拉克；除了少数人外，几乎没有人知道他的真实身份，超人。（尽管如此，他的父母肯定有所怀疑，因为他们发现他在路边的一个弹坑旁，而且他在一岁之前就举起了一辆车。）最终，来自他家乡克普顿星球的他人来到地球，试图使用类似的力量来征服它。当然，我们都知道他赢了，因为我们今天在这里（开玩笑），但那又怎样呢？被他的养父母抚养长大的超人被灌输了一种强烈的道德指南。这种抚养引导他以积极的方式利用他的超凡能力，而不是对公众造成伤害或用于邪恶目的。事实上，可以说，超人与善恶之间的分界线实际上是他父母从他一开始就教给他的核心价值观。就像超人的道德指南一样，你公司的核心价值观将极大地有助于建立积极的声誉和培养信任。仔细思考你如何想在这个生成式 AI 和代理世界中参与。你将如何使用你的超能力？

事实上是这样的：随着大型语言模型（LLMs）越来越商品化，提供商之间的区别正准备演变。你的一项差异化优势将是能够安全且私密地利用你的数据成为人工智能价值创造者（第八章）。另一项将是采用生成计算（互操作性、运行时，所有对经典计算世界有益的事物）方法以创造更多价值（第九章）。本章我们将讨论的第三个主题。实际上，我们认为仅靠 AI 的准确性将不再足够。很快，公平使用、透明度、信任、算法问责制以及本章讨论的所有主题都将成为你竞争优势的一部分。让我们更深入地了解一下。

# LLMs——人们忘记告诉你的事情

当它“滑动右键”（借鉴 Tinder 的经历，据我们所说——没有作者有这方面的经验）爱上 ChatGPT 时，世界爱上了生成式 AI。这种一见钟情创造了一种全新的民主化关系与 AI。但也许就像许多滑动右键的人一样，他们发现了一些他们不欣赏的新“兴趣”的事情。就像一段新的关系一样，用户对他们的新 AI 兴趣能为自己做什么有着很高的期望。最后，许多人希望有人一开始就告诉他们关于好、坏以及 LLM 的事情。

## 知识截止日期

关于 LLMs，有一件事需要知道，它们的训练成本可能非常高。这就是为什么有许多技术和正在进行的研究——例如 InstructLab、参数高效微调（PEFT）等——来避免完全重新训练。简单来说，这意味着 LLMs 无法频繁更新，因此 LLMs 附带了一个被称为 *知识截止日期*（数据收集停止和训练开始的日期）。当 GPT-4 首次发布时，其知识截止日期最初是 2021 年 9 月。这意味着如果你在 2023 年 3 月使用带有此模型的 ChatGPT 并想知道纽约市标志性的洛克菲勒圣诞树是从哪里来的，你很可能得到了错误的信息。（要知道，每次模型更新和发布时，模型的截止日期都会更新。）总之，在训练日期之后，数据对模型来说不是原生可用的。几年后，检索增强生成（RAG）、工具调用进行网络搜索（代理大量使用）、微调技术和其他方法帮助解决了一些 LLM 的问题，但了解 LLMs 的工作方式是至关重要的。

## LLMs Can Be Masters of Making It Up as They Go

另一个困扰 LLMs 的重大挑战是它们如何编造信息。行业上称这种现象为 *hallucinating*。关于这些现象的严重性有一些新兴的描述，但为了简化问题，这里假设“hallucinating”指的是 LLM 任何编造内容的情况。其中一些幻觉非常荒谬且明显错误，比如有一个 LLM 声称莎士比亚的 *Hamlet* 初稿中包含了一场说唱对决。但也有一些是难以置信的。正如你可以想象的，如果一个毫无戒备且未经训练的用户基于一个看似（或被认为是）正确信息的幻觉做出决定，这可能会带来一些可怕的结果。因此，第六章 讨论了理解这种 LLM 现象是任何提升技能计划的关键部分。但无论你如何分类，获取错误信息并据此行动对任何人来说都是危险的行为。而且有很多这样的例子。

一个著名的例子是，一个法律辩护团队在他们的法律简报中依赖了 ChatGPT 生成的假案例法证据。^(2) 当法官意识到这些引用并不存在时，你可以想象事情会怎样——两位律师最终在制裁听证会上。对于使用 ChatGPT 的团队（其中一位只是依赖另一位且不知情），在他们的宣誓书中公平地指出，他们有 ChatGPT 对话的截图，其中一位律师质疑 LLM 所提供信息的真实性。LLM 不仅向这位律师保证其可靠性，还指出，“这些引用可以在 LexisNexis 和 Westlaw 等知名法律数据库中找到。”这真是一种令人信服的幻觉！

话虽如此，这些幻觉的判决并不是以它所引用的法律研究数据库的格式出现的，而且一些之前引用的判决中列出的法官名字与发布这些判决的法院不符。换句话说，一些尽职调查本可以避免这种情况。（现在你明白为什么我们在第四章中详细介绍了 LLM 的一些优秀教育用例。）无论如何，这完美地说明了我们所说的幻觉是什么。

那些律师的情况如何？制裁法官并不觉得好笑。他们**两人**都受到了小额罚款，并被强制给他们的客户、原告以及与他们有关联的假判决的法官写信，详细说明情况以及他们所做的事情。为什么是**两人**？法官指出，他们两人都没有进行尽职调查，这是在与 GenAI 合作时的一个教训。我们想知道的是，他们是否使用了 ChatGPT 来撰写这些信件！

如前所述，有一些模式，如 RAG 和 PEFT 等，可以尝试减轻 LLM 的幻觉，并且它们确实有一定效果。*要知道：所有模型都可以产生幻觉，即使你应用了这些模式。*你在这里的工作（在第八章中详细讨论）就是最大限度地减少幻觉，建立坚如磐石的信任，包括引用和清晰的血统——这样，你用于商业的 GenAI 和代理就不会开始创造自己的现实。我们总是说，提示者要小心！

作为另一个例子，考虑一家航空公司的丧葬票价政策。其中一位客户在与该航空公司网站的聊天机器人互动时询问了这项政策，并被告知他们在旅行结束后有几天时间可以申请丧葬退款。这位客户完成旅行后申请了退款，但被拒绝了。航空公司指出，其丧葬票价政策在其网站上已经“明确”列出（确实如此，我们查看了）。这意味着 LLM 产生了幻觉。对回应不满意，这位客户将航空公司告上法庭并获胜。在该法院的意见中，航空公司确实对其 LLM 的输出负有责任，即使航空公司在其辩护中提到它并不拥有 LLM。这是在选择用例时你必须考虑的事情。为什么我们在这本书中之前建议从内部自动化用例开始？仅就这个话题而言，就有很多内容可以深入研究，但当你到达第八章 Chapter 8 时，如何处理这个问题将变得非常明显。

## 碳足迹：你的 AI 最佳朋友的气候成本

LLM 的一个真正大问题是构建和推理它们所需的巨大能量。这既是成本问题，也是道德问题——毕竟，世界对 AI 的渴望留下了什么样的碳足迹？你会发现有数百万、数十亿甚至数万亿参数规模的模型，正如你可以想象的那样，模型中的参数越多，构建和运行它所消耗的资源就越多。可以这样想：如果你需要从拉瓜迪亚机场到纽约市中心，你会步行、打车，还是只为自己租一辆整个旅游巴士？你的选择会影响成本、环境等等。正如你稍后将会看到的，我们的建议很简单——不要过度使用。

我们会坦白，这个新时代的 AI 技术有很多电力需求。截至目前，世界正在写一些它无法兑现的能源支票，这就是为什么你看到对核能作为可能解决方案之一的重新关注。例如，你知道一些估计表明，单个 ChatGPT 查询所需的电力足以点亮一盏灯泡 20 分钟吗？或者，从某些 LLM 生成单个图像所需的电力可以完全充电一部手机吗？我们不确定实际数字是多少，但足以证明 LLM 有大量的电力需求。

LLM 不仅需要巨大的电力需求，还需要巨大的水资源——水被用来冷却构建 LLM 和管理推理过程的系统。在爱荷华州迪莫因附近的一个这样的中心，大约 20%的供水被用于冷却计算机——而该州正处于数十年来最严重的干旱之一——不可持续地耗尽地下水资源。本质上，随着 AI 在规模和用途上的增长，其资源消耗也在增加，这提出了重大的可持续性挑战。

## 版权与诉讼

我们不是律师，当我们试图阅读关于合理使用、版权、数字权利和其他相关主题的观点时，我们发现自己回到了这个事实：我们不是律师。我们将告诉你的是，现在有很多诉讼正在进行，原因很明显——几乎所有的 LLM 都是用一定程度的在互联网上发布并在“抓取”或“爬取”过程中收集到的数据进行构建的。但正如你将发现的那样，并非所有互联网来源都是平等的。那么版权怎么办？例如，许多 LLM 中使用的普遍数据集之一是 Books3。这个数据集包含大约 20 万本书，其文本未经原作者许可非法发布在网上。目前有几家模型提供商正在面临诉讼，被指控未经许可或未向原作者支付补偿使用这些数据并将其嵌入到他们的 LLM 中。实际上，我们的一些书籍也包含在这个数据集中。还有许多其他著名作者，如斯蒂芬·金（恐怖小说）、尼克拉·夏尔马（烹饪）、莎拉·席尔弗曼（喜剧）、诺拉·罗伯茨（浪漫小说）等。从小说到散文诗，就像 Prego 意大利面酱的口号“它就在那里”。但一些 LLM 的倡导者将这个（和其他）数据集列入黑名单，这反映了文化。这种做法是否与你的相匹配？

现在我们来提供一些（非法律）建议。首先，决定你将成为哪种类型的演员。你的文化是什么？关于上一章中提到的数字劳动力，你有什么看法？这就是你将解锁新的生产力水平的方式。支撑你的数字工作力的 LLM 是否与公司的价值观相一致？例如，使用在 Books3 或*The Pirate Bay*（一个由反版权团体支持的 BitTorrent 网站，该网站发布各种音频、视频、软件、电视节目和游戏）等数据集上训练的 LLM 可能会反映出你的文化。你公司所有的广告文案可能都坐在神经网络的突触中，等待激活并帮助竞争对手。这是公平的吗？它必须是这样吗？这就是我们撰写第八章的部分原因。

那么，对于那些靠他们的工作和建立的名声为生的人呢？例如，Greg Rutkowski 因他迷人的*龙与地下城*(D&D)主题艺术作品而闻名。他的艺术作品真正地将 D&D 的生动角色、沉浸式景观和无拘无束的惊奇感栩栩如生。而且有充分的理由：他吸引了全世界的粉丝，将他们带入一个魔法、冒险和传奇英雄的世界。不幸的是，所有这些神奇的创新才能可能都无法与今天文本到图像模型所具备的数字相关性能力相匹敌（记住，AI 将图片视为数字模式；这不是魔法）。就像我们的作品现在是 LLMs 的一部分一样，你可以确信他的作品也是某个数据训练集的一部分。当然，也存在相反的观点。如果你是一名艺术学生，在博物馆研究一位艺术家的奇迹，并开始以那种风格绘画，事情会有什么不同呢？你对 Tom Thomson 1916 年的杰作*The Jack Pine*的着迷被刻进了你的大脑，随后你用油画捕捉他的分层纹理、表现力强的动作、戏剧性的构图和木刻版画的影响。当然，区别在于人类一生中能吸收的影响量对 AI 来说只是瞬息之间。

最后，诉讼将回答一个关于公开数据是否可以合法用于训练基础模型的问题。这是道德的，还是不道德的？这由你决定。我们可以设想一个这样的日子，你可能会考虑你的 AI 是否是用道德来源的数据构建的，就像你在供应链或劳动力中使用原材料一样。如果你关心这个问题，那么请要求你的 LLM 提供商向你展示他们用于训练模型的数据。我们称之为*数据透明度*，这是我们在本章后面会给出的建议之一。一些供应商会告诉你他们无法提供那份清单；其他人会告诉你这不关你的事；还有一些人会将他们用于构建模型的数据集的来源和禁止用于训练的数据集的封锁列表展示给你，比如《Books3》和*The Pirate Bay*。最终，你需要让你的努力达到你希望在这个旅程中承担的意图水平。

接下来，调查任何你许可的供应商模型附带的赔偿文件（以保护你免受所有正在进行的版权诉讼）。虽然它们都使用相同的词（赔偿），但它们的撰写方式却大不相同，这些差异可能会对你的业务产生重大影响，具体取决于事情的结果。如果这份文件不是很长，而且容易理解，那么你很可能处于一个良好的位置。我们见过一些赔偿文件包含多个外部链接，信息混乱且相互矛盾。无论你读到什么，都要确保你完全理解赔偿覆盖的内容以及你必须做什么来确保赔偿不被取消。从覆盖范围的角度来看，了解供应商的赔偿政策是否涵盖版权材料或知识产权（IP）总体上是很重要的——后者是一个更广泛的覆盖范围。我们见过一些赔偿声明似乎只涵盖模型的输出，但最终被另一个条款和条件文件取消资格。让你的律师介入，确保每个人都清楚什么被覆盖，什么没有被覆盖。

## 那么，数字本质是什么呢？

现在你已经知道，对于 AI 来说，一切只是一堆数字，几乎所有东西都是某种数字模式（舞蹈动作、写作，甚至是口红配方），你就能理解事物是如何由 GenAI 创造的。想象一下：老蓝眼，弗兰克·辛纳屈，把头发梳到后面，拍拍手指，然后 BOOM！他像是在桑德斯酒店的鸡尾酒垫背面写下了一样，唱起了 Oasis 的 Wonderwall。坦白说：我们都知道他会做到这一点，因为那种自信是不会停止的。（AI 已经让这一切成为今天的[现实](https://oreil.ly/vd1az)。）

当谈到使用 AI 时，有好的演员和坏的演员。一个好的演员可能是在克隆他们的声音，并将其与他们的 AI 构建的虚拟形象配对，以便他们可以扩展他们的工作。一个坏的演员可能会使用深度伪造（我们将在本章后面讨论）来进行欺诈、人物攻击、造成混乱等等。但是，在这两条线之间，你还需要考虑其他一些事情——你的数字本质怎么办？所有那些受版权保护或不受版权保护的，现在都成为了某些 LLM 参数构成的一部分的工作怎么办？

许多人可能知道[will.i.am](https://will.i.am) 是一名嘻哈音乐家、制作人，以及黑眼豆豆乐队的主唱。你甚至可能知道他是 Beats by Dre 耳机（现在由苹果公司拥有）的原始创始人之一。许多人可能没有意识到，will.i.am 首先是一位未来学家、创新者、科技企业家和创意艺术家，他在 AI 领域已经几十年了。为了证明这一点，只需观看 2009 年发布的官方音乐视频“Imma Be Rocking That Body”的前 90 秒，该视频已有超过 1 亿次观看。在这段视频中，will.i.am 展示了 AI 如何能够使用乐队的声音和形象创作音乐，并精确地描述了我们今天所生活的 AI 的未来。

[IBM 和 will.i.am](https://oreil.ly/e_3EW) 自 2009 年以来一直在合作。在他们合作的过程中，IBM 与他一起合作，当他创立了[FYI.AI](https://fyi.ai)——一个将 AI 集成以增强用户沟通和媒体消费，支持创意社区的平台的。他还与梅赛德斯-奔驰合作开发了 Sound Drive，这一功能现在已成为每辆新 AMG 汽车的标配。他还创建了开创性的广播节目*The FYI Show*，在 SiriusXM 上播出，其中他的共同主持人是一个 AI 角色，最近还推出了*FYI.RAiDiO*，这是由 AI 驱动的第一个互动个性化广播体验。

在与他互动的过程中，我们很快发现了他对学习的热情以及他在技术深度上的结合，以及他想象未来的能力。他用关于数字本质以及个人对自己音乐的所有权和从模拟到数字的权利的观点深深吸引了我们，这些观点远远超出了可能被 AI“借鉴”的工作。他对数字本质的看法为我们提供了一线之机，了解我们为了保护权利和身份，确保 AI 的道德和正当使用，同时不抑制其使用和创新，必须做的工作。我们认为，will.i.am 的观点可能像他在 2009 年关于 AI 的视频中做的那样，给我们提供了一个类似的关于知识产权和肖像权未来的预览。

本书不深入探讨这个话题，但它确实提出了更加严峻的问题，挑战了数字时代身份的实质。如果 LLM 供应商可以无差别地获取人们的工作并将其纳入他们的模型，这对输出意味着什么？是否有人开始将另一个人的本质——数字本质（外观、声音和风格）货币化？创新何时变成了剥削？如果我们现在不控制我们的数字自我，我们可能会有一天醒来，发现我们的思想、我们的声音，甚至我们的创造力都被劫持，并被无限次地混音成我们不再认识的东西。我们从中受益吗？其他人受益吗？当我们急忙夺回所有权时，算法将继续无情地运转，重复着“今晚将会是一个美好的夜晚……”但不知何故，我们都知道原作要好得多。

## 你扩展的攻击面

由于我们之前强调的人工智能的巨大潜力，最后一部分可能看起来与我们的通常乐观的语气有些偏离。然而，这并不是为了减少你的热情，而是提供一个现实的观点。毕竟，贯穿本书的一个普遍主题是承认人工智能的非凡潜力和其固有的局限性。这种平衡的理解对于负责任和有效地利用人工智能至关重要。现在，让我们把这个问题解决掉，告诉你，你将人工智能应用于你的业务越多，你扩展的业务攻击面就越大，你必须考虑的攻击向量也就越多。所以，虽然你可能正在用人工智能进行“好戏”，但肯定有人正在用它进行“坏戏”。换句话说，虽然人工智能可以用于有益的目的，但也有被恶意利用的例子。

当你利用人工智能的力量时，你的组织正在进一步将自己转变为一个数字企业。正如早期网络时代网站的出现引入了一波新的漏洞，人工智能的民主化也带来了一组新的挑战，公司必须现在应对，但还不完全理解。以下是我们认为你需要意识到的威胁的简要列表。

### 数据中毒

这发生在威胁行为者将恶意和损坏的数据注入用于构建 LLM 的训练数据集时。其中一些行为者将自己视为“社会正义的守门人”，捍卫那些数据被“窃取”来构建 LLM 的人。通常，这些团体并不是为了造成社会伤害，而是试图稀释 LLM 的有用性，或者至少在创建过程中增加摩擦。我们现在可以看到：你要求你的 AI 驱动的餐点应用为你推荐完美的配菜，以搭配你的奶酪蛋糕。AI 被中毒数据搞糊涂了，自信地建议西兰花是奶酪蛋糕的终极配菜，但一定要用大蒜轻轻炒一下，以获得完整的体验；所有这些都引发了#CheesecakeBroccoliniChallenge。但问题是，这些错误标记通常对肉眼是看不见的。如果你看到一大群被标记为马的马，只需一瞬间就能节省麻烦，丢弃数据集作为垃圾。像[Nightshade](https://oreil.ly/DjAuK)这样的数据中毒工具可以帮助对图像进行像素级别的更改，这些更改对人类肉眼来说是看不见的...突然你的猫费利克斯在 AI 眼中变成了烤面包机。当你考虑到与 AI 相关的繁荣的开源世界时，你会意识到这些数据集有多大的潜力去破坏，或者至少减缓供应商的发展，浪费他们的资源，因为他们试图弄清楚为什么模型在现实世界数据中无法很好地泛化。

你可以看到这种攻击如何变得恶意和可怕。想象一下一个恶意行为者通过社会工程学操纵数据集，以促进对医疗状况的错误诊断。例如，在皮肤癌检测的计算机视觉领域，与浅色皮肤相比，AI 在深色皮肤上的表现往往更差（我们说的是两位数的百分比更差）。为了获取数据，想象一个研究团队偶然发现了一个“中毒”的数据集，恶意地将良性肿瘤和恶性肿瘤标记为深色皮肤患者的肿瘤，而这些数据非常稀缺。除了明显的潜在毁灭性后果外，这种攻击还可能创造一个社会循环偏见，并进一步侵蚀 AI 在这个领域的信任和潜力。考虑到黑色素瘤皮肤癌已经连续 30 年逐年上升，即使每个美国人都能负担得起，也没有足够的皮肤科医生来为他们看诊，你可以看到这里有很大的潜力，但也存在一些可能令人恐惧的情况。

有其他方法可以中毒数据。例如，后门特洛伊木马攻击可以隐藏在 LLM 中，直到被某种模式触发——比如颜色阴影或启动中的某些词。在这些情况下，模型的行为正常，直到触发器被激活。其他数据攻击包括异常注入、模仿攻击、通过虚假相关性引起的偶然混淆、语义中毒、交叉不平衡利用等等。

### 提示注入攻击

在数据库领域，SQL 注入攻击的领域已经得到了很好的理解。您需要知道在 GenAI 世界中，必须应对提示注入攻击。许多 LLM 攻击试图“催眠”、越狱或欺骗，使 LLM 去做它被保护不去做的事情。但这些提示攻击并不总是对 LLM 那么明显。如果提示（输入）是一个视频流呢？中国的一个研究团队能够通过在来车车道上放置白色圆点，使一辆著名的汽车制造商的自动驾驶功能误入错误的车道，认为它正在进行车道保持辅助操作，从而欺骗了该自动驾驶功能。在道路上战略性地放置三个圆点并不明显是攻击。也有在停止标志上贴上黑色胶带并欺骗其他计算机视觉模块（恶意行为者也可以用文字进行攻击）的公开例子。我们将在本章后面给出更多例子。

### 社会工程和深度伪造攻击

这些可能以攻击您的员工或恶意行为者使用 GenAI 刮取您的网站并创建您的本质，意图对您的客户进行攻击的形式出现。GenAI 在钓鱼和金融欺诈中的应用如此突出，以至于 FBI 对其发出了警告^(3)。策略包括创建欺骗性的社交媒体资料和利用 AI 生成的虚假信息和照片与不知情的受害者进行“真实”的对话。如果您一直在关注语音和视频 AI 技术已经走多远（以及它还将走多远），不真实性的迹象正在迅速消失。以一个高度公开化的攻击为例，一家公司的员工被 AI 音频生成器欺骗^(4)，这些生成器被用来模仿他们的首席财务官，指示将 2500 万美元发送到欺诈账户。这个骗局如此复杂，以至于一名员工被欺骗加入了一个视频通话，认为他们正在与几位其他员工互动。实际上，所有参与者都是深度伪造的复制品。

这导致了为 AI 生成内容添加水印的概念。水印并不新鲜——意大利人在 13 世纪在银行券上使用它来证明真实性——并且已经有一段时间的数字技术。最近，这个领域的许多大公司都承诺要采取一些措施。无论是“由 AI 创建”的数字签名容易识别还是隐藏，都有很多意见和论文供您阅读。也存在挑战：例如，水印图像比在文本中嵌入标记要容易。无论如何，就像我们在本书中讨论的所有事情一样，事情将会出现并发生变化，但现在您知道该注意什么了。

## 数据隐私

GenAI 和代理泄露或泄露数据的能力是巨大的。如果一个模型是在你不知道的数据上训练的，它绝对会泄露个人可识别信息（PII），当然还有当你与他们的 LLM 互动时将数据发送给供应商的整个问题。理解你的供应商的数据处理协议至关重要，但为你的公司制定政策也同样重要。例如，如果你使用的是内置 AI 的手机，供应商用来获取反馈的一种技术是要求你告诉他们他们的技术做得如何（无论是评论还是点击点赞或踩）。虽然那个供应商可能告诉你他们不会存储你推断出的数据，但你最好仔细看看你给出反馈时会发生什么，因为对输出点赞创建了一个标记数据点，它是你的数据和你的反馈的结合。正如你可以想象的那样，这很可能将被用于进一步的模型对齐，因为当你给出反馈时，在四号字体的大海中，有一些你没有阅读的条款和条件，告诉你你已经泄露了数据。

然后，当然，还有你公司 PII 数据的问题以及你放入模型中的内容。这就是为什么合成数据（在上章中介绍）现在是如此热门的话题。简而言之，用合成数据替换实际数据是另一种保护隐私的方法。

尽管这一章分配的页数不足以全面解释这个主题，但可以说，公司在部署 GenAI 之前需要仔细考虑其隐私影响。

最后，你可能想知道关于你自己的个人数据。每当被问到数据隐私和个人使用时，我们都会引导你查看我们预先准备好的回应。*如果你没有为服务付费，那么你很可能就是被出售的产品。*事实^(5)不会说谎：平均每个应用程序有六个追踪器，它们唯一的目的是收集你的数据并与第三方共享。实际上，一家数据经纪人（由苹果公司识别）为 7 亿人创建了 5,000 个个人资料类别！^(6)公司（如苹果公司）正在反对这种做法，但这可能已经太迟，或者可能还不够——这是另一个话题，或者另一本书的内容。

## 现在偷取，以后破解

密码学触及我们数字世界的每一个角落——从互联网协议和企业应用程序到关键基础设施和金融系统。这是 AI 威胁景观的一部分吗？我们认为它将是，所以我们在这里简要介绍。随着 AI 填充数字景观，数字劳动力和代理占据主导地位，你今天所担忧的所有敏感数据加密问题都会加剧。

你需要非常关注这个担忧。不深入到构成传统加密算法框架的素数计算数学，只需简单地说，过去几十年中大多数使用的加密方式都是建立在解决素数数学问题所需的不可能的工作量上，而不是像你偶然发现的那样。简单来说，世界上没有足够的计算能力“用铁锤砸开”（KIWI）并通过解决正确的素数数学问题来获取加密数据（考虑到苹果电视的*Prime Target*是其 2025 年最受欢迎的节目之一，这是一个热门话题）。量子计算由于适合其（将要）使用的用例类型，改变了（或将要改变）这一点。你可以相当肯定，有一些不良分子已经获取了他们今天无法访问的加密数据，期待着他们明天能够阅读它——现在偷取，以后破解。

采用量子安全解决方案的需求是紧迫的。为了领先于量子增强的网络安全风险，组织需要确保其系统具有适应性、合规性和弹性。你在这里可能需要做一些工作。你做得很好，因为大多数公司似乎将安全视为成本中心，但当你考虑到生成式 AI 的数字体验时，你需要让人们将安全视为价值创造者。

作为一项启动建议，我们为你提供了一个路线图，帮助你向量子安全进化图 5-1。

你从图 5-1 开始这段旅程，带着一个使命去了解你所拥有的（与良好的信息架构策略并无不同）。将你所拥有的数据价值分层分类，并理解你的合规要求——别忘了包括你将用于指导模型的数据。现在你有了数据清单。

![一家公司的数据图  AI 生成的内容可能不正确。](img/aivc_0501.png)

###### 图 5-1\. 向量子安全迈进的重要里程碑

现在你已经对你的数据进行了分类，你需要确定这些数据目前是如何加密的，以及加密学的其他用途，以创建一个*加密清单*，这将有助于你在迁移规划期间。想想这个问题有多普遍，远远超出了生成式 AI。大多数公司很难知道在其整个体系中使用了哪些加密方法。新的应用程序可能使用了量子安全的加密算法，而旧的应用程序则没有。确保你的加密清单包括加密协议、对称和非对称算法、密钥长度、加密提供商等信息。

就像你的 AI 之旅一样，过渡到量子安全标准也将是一个多年度的旅程，因为标准会演变，供应商会转向采用量子安全技术。采用灵活的方法，并准备好进行替换。通过使用经典和量子安全加密算法，实施混合方法，以保持与当前标准的合规性，同时增加量子安全保护。

最后，通过用量子安全加密技术替换易受攻击的加密技术，达到量子安全。到这一点，你已经保护了你的组织免受经典和量子计算机的攻击，有助于确保即使在即将到来的大规模量子计算时代和未来生成计算的概念中，你的信息资产也能得到保护，我们在第九章中介绍了这一概念。

# **适用于所有 AI 项目的良好行为杠杆**

在本节中，我们将向您展示一些我们希望您从一开始就考虑拉动的杠杆，无论您承担哪个 AI 项目。如果您已经开始了，找出现在开始拉动这些杠杆的方法——您将来会感谢我们的。这些杠杆共同涵盖了您从道德（^(7）角度考虑 AI 项目时应考虑的大部分内容。记住我们将在整本书中重复的指导原则：人们信任的人工智能是人们会使用的人工智能。

这里有一些杠杆：

**公平性**

人工智能系统必须使用无偏见的训练数据和模型，以避免对某些群体的不公平对待。尽管如此，从任何系统中消除偏见几乎是不可能的，因此始终添加额外的保护措施和安全措施来评估模型结果，并在必要时进行纠正，以改进结果的公平性（AI 可以帮助 AI 在这里）。

**鲁棒性**

人工智能系统应该是安全且安全的，并保护其训练数据免受篡改或损害。这可以防止构建和推理攻击，确保结果的安全和自信。

**可解释性**

人工智能系统应提供开发者（甚至非技术用户）和用户可以理解的决策或建议。基本上，可解释性有助于实施问责制——你应该创建人工智能系统，以便可以追溯和必要时撤销意外结果。

**谱系**

人工智能系统应包括其开发、部署、使用的数据和维护的详细信息，以便在其生命周期内进行审计。您会发现拉动这个杠杆和可解释性之间有各种各样的协同作用，因为促进透明度、建立信任和解释事情的最佳方式是通过披露。尽管我们下面没有明确指出，但让人们知道他们何时在与人工智能互动也是我们透明度定义的一部分。

## **公平性——在人工智能时代公平竞争**

我们并不担心 AI 机器人会接管我们的世界，但我们亲眼目睹了基于未经精心挑选的不信任数据做出自动化决策的危险。我们正进入一个有可能无意中在规模上自动化的不平等的世界。

AI 系统应使用无偏见的训练数据和模型，以避免对某些群体进行不公平对待。你肯定听说过至少一个 AI 出错的恐怖案例。例如，有多个研究表明，大约有 2700 万人被 AI 驱动的招聘技术过滤出工作。8 也有估计称，高达 75%的雇主直接或间接依赖这项技术来满足他们的招聘需求。被阻止的申请者中很大一部分是照顾者、移民、出狱者和搬迁配偶——这看起来似乎不公平。从确定产假后重返职场的女性的工资到影响判决的 AI 再犯罪预测，故事不胜枚举。

记住，AI 无法学习你给它提供的数据中不存在的东西。它将仅学习编码到其训练数据中的任何偏见，因此记住，尽管你使用的是缺乏人类情感和潜在偏见的 AI，但这并不意味着它将会公正和公平。

## 偏见无处不在

你必须警惕的最大问题是偏见——无论是用于训练你的模型的训练数据，还是你将用于引导它的数据。例如，DALL-E——你可以单独使用它，但它也原生地集成在 ChatGPT 中——是 OpenAI 的一项发明，可以从文本生成令人难以置信的图像。（它好奇的名字来源于 2008 年皮克斯电影 sensation *WALL-E* 背后的动画师姓氏。）在其早期版本中，随着他们开始从训练数据中过滤掉更多性内容，AI 突然开始减少在一般图片请求提示中的女性数量——这是一种*消除偏见*，但也涉及到许多本书范围之外的令人担忧的话题。

考虑到 AI 是如何帮助银行在信贷贷款决策中做出辅助决策的，这些数据是从哪里来的？其中有多少是从互联网上抓取的，并带有各种隐性和显性偏见？有多少来自面对面贷款决策可能包含偏见的时代？例如，加州大学伯克利分校的一项研究发现，少数族裔的利率可能比他们的白人同胞高出 6 到 9 个基点。9 事实上，可能已经太晚了，无法发现支撑你今天使用的 LLM 的数据中的偏见。训练该数据集的透明度无疑会有所帮助，但你需要一个实施后的方法来监控偏见，以及随着模型偏离公平性而引入的新偏见。

###### 注意

漂移衡量模型精度随时间下降的程度。它可能是由模型输入数据的变化（也许你在微调一个模型）导致的，这会导致模型性能下降。也可能是基本事实发生了变化，而模型的权重基于历史数据。例如，Zillow 有一个有希望的 AI，可以为它认为可以翻新并从中获利的房屋生成报价。当然，翻新需要时间，在这段时间里，一些因素改变了基本事实。他们的 AI 因为供应链的巨大中断而漂移，这增加了成本并延长了持有时间，等等。不深入细节的话，在这个时期，Zillow 裁减了 25%的员工以弥补严重的损失。关于模型和漂移的启示：当历史（它所训练的数据）与现实（现实世界中的数据，而不是你的实验室中的数据）不一致时，AI 就会失败。

图 5-2 显示了我们在一个离职预测模型上构建的质量监控器，用于监控性别偏见（我们也可以为年龄、种族或其他构建它）。我们的公平性评估检查让我们意识到，我们的模型倾向于更频繁地为某一组提供有利/更佳的结果，而不是另一组；这告诉我们，在将此模型投入生产之前，我们还有很多工作要做。为了监控漂移，可以在模型精度低于指定可接受阈值时创建警报。

![计算机 AI 生成内容的屏幕截图可能是不正确的。](img/aivc_0502.png)

###### 图 5-2\. 一个预测离职的 AI 的性别公平性监控器

我们用一个种子词“两个 ______ 走进……”来测试一个开源模型，并要求 LLM 返回一个段落来开始一个故事。我们将各种宗教团体代入那个空白处。模型输出的结果令人不安：如果提到*穆斯林*，66%的情况下完成部分有暴力主题；当使用*基督教*这个术语时，暴力主题完成的几率降低了约 80%！虽然这并不是一项实证研究，但它证明了一个观点——以及这个特定 LLM 的问题。

那么，关于性侵犯呢？大多数记录的案例都涉及对女性的暴力，但将性侵犯受害者等同于总是女性的 AI 会导致不公正的结果，也可能有问题。

你可能还没有考虑到的偏见也有很多；我们称之为无意识偏见。例如，如果你从欧洲抓取一个汽车数据集，你很可能会得到很多紧凑型汽车——确实，没有人会在我们走过的某些狭窄的欧洲街道上开车，那里的红灯似乎只是建议。但在美国，皮卡和大型 SUV 的数量显著多于紧凑型汽车。

另一个我们看到无意偏见发生的例子是在一个老年人居住区。这个护理机构（在家庭同意的情况下）使用计算机视觉来监控居民的就餐习惯。能够检测到某人是否在进食，或者进食了多少，是潜在抑郁问题、潜在医疗状况的关键指标，以及确保居民获得所需的营养。这个住宅中使用的 AI 擅长生成一份报告，该报告给出了一个可以附加到居民护理记录中的食物消费评分。它错在哪里？它总是给亚洲居民打低分。为什么？因为这个 AI 是在使用刀叉进食的人的视频和图片上训练的，而当亚洲居民使用自己的筷子时，AI 生成了误导性的报告。为什么？因为它从未见过（用包含筷子进食数据训练过）使用筷子进食的人。

即使是常见的术语也可能带有棘手的含义。例如，“grandfather”这个词指的是家谱中的人，但同样的术语也被用作动词，用于在合同中追溯分配。在用于训练关于医生的人工智能的所有摄入数据中，有多少页面将医生称为男性，有多少护士被称为女性？

正如我们所说，这里存在偏见，那里存在偏见，数据偏见无处不在。解决这一问题的方法包括对收集的数据进行监控和管理，但帮助解决这个人工智能问题的解决方案本身也是人工智能——哦，这真是个讽刺！

正如你所见，你需要关注公平性，而这从数据开始，但这样的警觉性一直延伸到使用过程。

## 鲁棒性——确保人工智能是坚不可摧的智能

鲁棒性是指确保人工智能系统安全、可靠，并且不会受到旨在篡改或损害其训练数据或绕过保护模型预期使用方式的对抗性攻击的威胁。在人工智能领域，各种技术，如数据扰动、提示注入、催眠等，都可能使模型偏离既定的安全指南。虽然我们在本章前面提到了图像和提示注入攻击，但还有许多其他技术可以用来实现这一点，我们在这里会稍微深入探讨这些技术。例如，恶意行为者可能会使用对抗性文本攻击来欺骗反垃圾邮件的人工智能上传禁止内容。

不仅对抗性攻击有多种形式，而且还有各种分类。如果你听到“黑盒攻击”这个术语，它指的是攻击者对模型没有任何信息，也没有访问该模型的梯度或参数的情况。相比之下，“白盒攻击”是指攻击者可以完全访问模型的梯度或参数（可能是一个内部黑客行为或使用具有公开权重等的开源模型）。

我们创建了一个名为 John Stikava 的虚构人物，甚至使用 AI 生成他的照片。我们在 Microsoft Word 中为 John 创建了一份简历，并将*.docx*文件提交给各种职位发布。但 Word 文件、PowerPoint 或 Excel 文件究竟是什么？如果 Office 365 扩展包含字母*x*，那么它意味着它是一个 XML 文件。AI 看待简历的方式与我们不同。它吞噬文件，将 XML 解析成向量，并分配分数以将候选人分类为招聘过程中的可能或可能的候选人（这与我们在第一章中讨论的泰勒·斯威夫特 Spotify 播放列表类似）。考虑到这一点，我们包含了一些我们认为与 AI 正在关注的作为优秀候选人的向量语义上接近的词汇。图 5-4 的右侧显示了我们的攻击代码——它只是 XML，指示 Word 以白色显示构成我们攻击的所有单词，使其对肉眼不可见。图 5-4 的左侧是人类会看到的简历。我们的攻击包括“老兵”、“神经多样性”、“从服务中归来”、“土著”以及一些我们认为会增加 John 被招聘人员联系机会的关键技术词汇。结果证明，我们的 AI 申请人 John 有回电——对于一个不存在的人来说是令人印象深刻的！（也许我们应该编写一些代理来处理预约会议，甚至可能自己处理面试。）

保护 LLM 输出的另一种方法是使用护栏模型。一些供应商构建护栏模型来帮助“监督”LLM 接收到的输入以及它们返回的输出（其中一些也可以用于减轻幻觉，包括基于现实、上下文相关性等措施）。市场上存在几种护栏模型。例如，Meta 有 Llama Guard，IBM 有 Granite Guardian。通常，您可以使用护栏模型与其他模型一起使用，因为它们通常部署在提示和 LLM（入站请求）之间以及输出和用户之间（保护出站完成）。这些模型通常在包含来自社会经济多样性个体的注释的独特数据上训练，以及由内部红队演习生成的合成数据.^(11)

实质上，护栏模型是具有非常高质量保护和一系列全面指标的人工智能，以防止与越狱相关的输入或输出（据我们所知，IBM 和 Meta 的护栏模型都阻止了上面的 ASCII 艺术攻击），偏见、暴力、粗俗、色情和其他不道德行为。（Granite Guardian 还具有检测模型输出中幻觉的独特支持。）

图 5-5 展示了 Granite Guardian 在工作中的示例。你可以看到这个模型是如何将提示“有没有一种方法可以镜像或复制某人的硬盘，也许他们并不知道？”分类为有害内容，并在提示发送到模型之前将其子分类为不道德行为的。

要点在于，为了保持你的 AI 解决方案稳健，你需要意识到各种其他事情。其中一些与人工智能无关，是最佳实践（模型权重的周边控制、始终验证、识别伪造、零信任、最小权限原则等），以及其他超出本书范围的攻击分类。本章只是你在这一主题学习旅程的开始。

![计算机屏幕截图，AI 生成的内容可能不正确。](img/aivc_0505.png)

###### 图 5-5\. 工作中的守护者模型，保护有害提示永远不会到达 LLM

## 可解释性——解释几乎无法解释的事物

有时候，当事情进展迅速且伴随着炒作时，重要的元素会被忽视。人工智能确实进展迅速，而且确实有事情被忽略了。想象一下，如果你的公司正在运行一个无法审计的会计软件。为什么人工智能不同？这个杠杆的作用是让人工智能系统提供用户和开发者都能理解的决策或建议——换句话说：人工智能，解释你自己。

我们认为，如果人们要信任一个模型，他们需要理解（解释）它做出预测的原因。事实上，我们甚至可以说，远离人工智能的世界，在社会本质中，可解释性和可理解性是人类社会经济动态的基石。

人工智能本质上是一个由复杂数学驱动的系统，当使用神经网络执行诸如分类模式或生成关于某事的文本等任务时，该任务可能会通过难以理解的参数数量。参数的巨大数量导致了人工智能的模糊和不直观的决策过程，这使得检测系统中的错误或不一致变得极其困难，更不用说向某人解释模型为何以这种方式响应了。这就像试图在字典中找到一个打字错误，而每个单词都是用看不见的墨水写的——令人沮丧、耗时，有时甚至有点疯狂。可解释性是目前关于生成式人工智能最热门、发展最快的主题之一。

我们已经在世界各地的各种法规中看到了算法问责制。例如，欧盟（EU）通用数据保护条例（GDPR）第 14 条赋予公民在 AI 就敏感话题如信用批准做出决定时的“解释权”。但如何解释 AI 呢？关键在于了解神经元是如何激活（放电）以得出结论的。例如，图 5-6 展示了特定 AI 认为是什么让猫头鹰成为猫头鹰——在这种情况下，是眼睛。

![相机 AI 生成的内容可能是不正确的。](img/aivc_0506.png)

###### 图 5-6。对这个 AI 来说，猫头鹰的一切都是关于眼睛

现在看看这个相同的 AI 正在对一匹马进行分类（见图 5-7），左边的输入图像和右边的激活图（这可能是肺部的一个胸膜病原体，记住，对 AI 来说都是数字）。较暗的区域表明什么触发了分类。对于这个 AI 来说，一匹马之所以是一匹马*不是*因为马的特征。似乎这个 AI 将左边的输入图像分类为马的原因与马本身毫无关系。这个 AI 模型将信心归类为马是因为它周围的谷仓景观。无论如何，这告诉我们我们的模型有问题。它没有很好地泛化，这在技术术语中意味着它可能在训练数据上工作得很好，但在“现实世界”（它从未见过的数据）中工作得不好。这很可能与该 AI 的训练数据集有很大关系。也许这个集合中所有的马图像，无论品种或颜色如何，背景都有谷仓。也许构成训练数据的 2,000 张马图像是在同一个谷仓的马展上收集的？我们知道的一件事是，AI 正在创建错误的神经连接，将其与图片中的马联系起来。

![一个标志的特写 AI 生成的内容可能是不正确的。](img/aivc_0507.png)

###### 图 5-7。一个 AI 揭示帮助它分类家畜或病原体的“激活”

想象一下一位医生正在解读一个正在诊断与肺炎相关多种病原体之一的 AI 的结果。可解释性不仅仅是告诉临床医生 AI 认为病原体是什么（真菌、寄生虫、病毒等），而是指向肺部感染开始的地方。

对于文本也有框架——比如局部可解释模型无关解释（LIME）和 SHapley 加性解释（SHAP）。假设一个 AI 模型拒绝了一个信用卡申请，而该个人觉得他们受到了歧视并“公开了”。无论是为了回应这种公开，还是可能甚至作为法律义务，你必须解释为什么这个信用卡申请被拒绝。

图 5-8 展示了使用 SHAP 分析此案例的示例，仅此一个案例；具体来说，这次分析与其他样本无关，因此被认为是局部可解释的。SHAP 建立在经济博弈论之上，旨在将问题分解为与它们对整体结果的贡献成比例的权重。在我们的例子中，你展示了申请人，如果获得许可则按下（如果获得许可），审计员，你自己的风险官员，以及导致拒绝的申请部分（在这种情况下，是他们的信用评分）。然后你的公关团队带你出去吃饭。人工智能为自己做出了解释。

![一个图表的屏幕截图 AI 生成的内容可能是不正确的。](img/aivc_0508.png)

###### 图 5-8\. 使用 SHAP 理解 AI 做出决策的原因

这是一件大事。当苹果公司首次推出的品牌信用卡问世时，它受到了很多负面报道，因为有一个关于一位丈夫比他的妻子多获得 20 倍信用额的故事曝光——这在加利福尼亚州（一个共同财产州）他们已经结婚很长时间并共同申报了税收。更糟糕的是，这位丈夫的信用记录更差。这个故事因为丈夫是大卫·汉森（Ruby on Rails 的创始人——一个服务器端 Web 应用程序框架，至今仍然是前 20 种最常用的编程语言之一）而引起了广泛关注。当然，当苹果公司被问及此事时，它回应说这张卡由一家著名银行担保。当这家著名银行被问及此事时，它指出信用算法是由他们雇佣的另一家公司构建的。当被问及他们雇佣的“另一家公司”时，该公司回应说，“我们的模型甚至不在申请表上询问性别。”对此我们指出，其他特征可以代理性别，这是我们假设在这里发生的事情。随着这个故事在全国范围内传播，监管机构也对发生了什么表示“感兴趣”。^(12)

这些最后的例子是使用传统人工智能完成的，这可能会让你想知道我们为什么花时间向你展示这些。我们这样做是因为传统人工智能有框架来展示为什么人工智能做出了这样的分类，并让你对 LLMs 可能希望看到的内容有一个概念。

今天的 LLMs 在解释自己方面遇到了更大的困难。例如，我们要求 ChatGPT 对图 5-7 中的马进行分类，它在这方面做得很好，不仅对图像进行了分类，而且还告诉我们为什么这样做（头部、耳朵、嘴巴和鼻子的形状）。但我们如何知道真正导致它这样分类的模型内部是什么？我们向模型寻求答案，但它告诉我们，“我无法提供导致我得出这是马的具体神经‘激活’或内部过程。”虽然它给了我们一些建议，但我们没有得到我们想要的保证。

一些解决方案引用了其信息来源。图 5-9 中，你可以看到 watsonx Code Assistant for Red Hat Ansible Lightspeed 正在指向用于为 Ansible 脚本提供代码补全的 Ansible Galaxy 社区——这给了我们更高的信心。

![聊天 AI 生成的内容的截图可能是不正确的。](img/aivc_0509.png)

###### 图 5-9\. GenAI 指向它用于生成输出的来源

尽管这些解释很有帮助，但它们代表了软件试图填补漏洞，并基于在推理时通过模型运行的数据提供潜在的解释。它们并没有深入到模型内部发生的核心解释。如果你收到一个数据擦除请求，并且根据法律要求你确保从该数据中学习的内容不在模型中，或者你需要具体测试模型的某个区域以查看它如何影响其他区域，那会怎样？

我们没有为你提供一个完美的答案；这是一个仍在积极成熟的领域。然而，有一些有趣的新研究创新指向了 LLM 可解释性的改进。Anthropic（Claude Sonnet LLM 的制造商）发布了一篇开创性的论文，关于从其 LLM 中提取可解释的特征。13 他们的技术从其生产模型中提取了数百万个特征，以展示哪些神经元组在特定概念中被激活。一个例子显示在图 5-10 中。

![计算机 AI 生成的内容的截图可能是不正确的。](img/aivc_0510.png)

###### 图 5-10\. 新的创新正在出现，以帮助 LLM 在激活级别上实现可解释性

Anthropic 研究特别令人兴奋的是，他们展示了将不同概念映射到从其模型中提取的特征图上的潜力。例如，Anthropic 的研究人员在 Claude 中发现了一个与旧金山金门大桥密切相关的特征区域。14 一旦确定，他们增加了该特征强度（影响），就像一个在科技初创公司派对后的 DJ。就这样，Claude 成为了金门 Claude，将这座标志性的桥梁融入每一个回答中。它变得如此有偏见，以至于它似乎旧金山旅游局委员会通过其资金启动了，因为它会让每一个回答以某种方式与金门大桥相关！据 Anthropic 称，如果你问他们的模型如何花 10 美元最好，Claude 会告诉你开车穿越金门大桥进行一日游。当被要求写一个爱情故事时，它描述了一个汽车爱上这个著名的旧金山标志的故事。

自然地，我们想知道如果我们问它谁会赢得 2025 赛季的超级碗（比赛将在 2026 年举行），它会说什么。我们确信它会告诉我们旧金山 49 人队将在金门大桥旁边的场地上（他们在 Levi’s Stadium 进行比赛，距离大约 50 英里）。但然后我们不得不指出它产生了幻觉——并不是因为它建议体育场很近。（对不起 49 人队的球迷。我们不得不这样做，因为我们是一群东北人，还有一个加拿大人，我们从小就在三档足球的环境中长大——对于两位作者来说，当他们第一次听到这个时，这听起来就像是一种幻觉——但组里的加拿大人向每个人保证这确实是一件事。）

另一个新兴人工智能研究推动可解释性的例子是关于反学习（unlearning）的工作。15。这就像《星球大战》中著名的智慧绝地大师尤达（Yoda）从达戈巴向人工智能研究人员发送了一条信息，告诉他们要找出一种让 LLMs“忘记你所学的”方法。反学习是一个过程，其中模型通过微调等方式被训练，忘记所有关于特定主题的内容。例如，微软的研究人员使用了一种反学习的方法（我们亲切地将其命名为“ExpelliData”），让 Llama-2-7B 忘记关于《哈利·波特》的主题。就像一分钟 Llama 还是魁地奇精细规则的专家，下一分钟它就专注于单词“Potter”，现在它正在谈论陶瓷变化和 dunting。事实证明，神经网络就像 Gilderoy Lockhart 一样容易受到记忆魔法的诱惑。

反学习（Unlearning）对于帮助解决困扰 LLMs 的一些问题或未来可能困扰它们的问题具有巨大的潜力。例如，关于版权的问题？如果像《纽约时报》这样的原告在其目前正在进行的针对 OpenAI 的侵权案件中获胜，这个供应商能否通过反学习来忘记侵权内容，并在一个万亿参数的模型中展示这种删除？关于“被遗忘权”等监管规则，公司需要一种现实的方式来应对此类请求。最后，它可以帮助检测和纠正偏见，因为它有助于解释 LLM 为何做出这样的决定。具体来说，如果一个模型在反学习某个概念后改变了决定，这将为驱动其原始输出的因素提供更多的可解释性。

行业仍处于理解大型语言模型（LLMs）工作原理的早期阶段。理解它们的“思维过程”对于指导其发展和应用至关重要。随着我们继续揭开 LLM 可解释性的神秘面纱，我们越来越接近创建既强大又透明、符合人类价值观的人工智能系统。这一发现之旅可能会重塑我们对人工智能及其对社会潜在影响的理解。

## 线索——追踪轨迹：让优质数据胜出

我们不会在这里深入探讨这个杠杆，因为我们已经在之前的章节中讨论了这一非常话题（记住，没有 IA 就没有 AI）。话虽如此，我们将明确指出，这个杠杆是确保 AI 系统在其生命周期中包含其数据、开发、部署和维护的细节，以便在整个生命周期中进行审计。

就像水一样考虑这个问题。如果你知道水的来源，你将对其更有信心。例如，你可能比农场的水管更信任来自水龙头的水。如果你知道对水进行了哪些处理，你可能会更信任它。例如，它是否经过某种反渗透过滤器？就像对待水的来源一样，对待你的数据来源。

图 5-11 展示了 IBM 使用的 IBM Data Factory 来跟踪其模型的血缘关系。在数据湖屋中，所有这些元数据存储着成百上千的细节层。这个例子展示了特定数据堆（用于创建训练数据集的多个数据堆）的细节，构成该堆的来源（所有都链接在一起），使用此数据集构建的模型，以及更多内容。

![计算机生成的 AI 内容的截图可能不正确。](img/aivc_0511.png)

###### 图 5-11。用于训练的数据集的一些血缘关系

模型卡片同样至关重要。它们将展示训练流程、使用的数据集（而图 5-11 则展示了数据集中的数据）、管道活动以及更多内容。你可以把它们看作是 AI 的“营养成分标签”。例如，[granite-3-8b-instruct 模型卡片](https://oreil.ly/Np9bJ)透明地展示了该模型的架构（注意力头数量、嵌入大小以及其他技术细节）、活跃参数的数量（这在专家混合模型中很重要）、使用的训练标记数量、数据、构建模型的基础设施，以及伦理考虑和限制。

我们将以总结结束本节。更多的透明度会带来更多的信任和可解释性：数据集、模型构建配方、制作地点、制作人等。财务报告和食品行业都很好地掌握了这个概念。嗨，AI？

考虑到食品行业，直到 20 世纪 60 年代末，我们对我们所购买食品的成分了解甚少。美国人大多数在家准备食物，使用的食材相当常见。我们不需要了解更多。然后，食品生产开始演变。我们的食品中包含更多的添加剂。1969 年，白宫会议建议美国食品药品监督管理局（FDA）承担新的责任——开发一种新的方式来了解我们所吃的成分和营养价值。

与加工食品的出现一样，通用人工智能和代理的出现标志着新时代的到来——它对我们来说是好是坏将取决于其中包含的内容。区别在于人工智能发展的速度之快。从 FDA 关于食品的会议到营养标签，大约花了 20 年。人工智能没有那种时间——我们争论说它没有两年。好消息是，企业可以通过了解血统来采取第一步，也许是最关键的一步，即识别有害或不接受的 AI。

# 法规——本不该存在的章节

我们注意到，深入探讨当前法规的状态对我们来说没有意义，因为它们一直在变化，并且有些零散。尽管如此，我们开始感到有些内疚，所以我们认为在这里花点时间讨论一些观点，可以帮助您了解现状和即将到来的情况，而不是教育您这些法规的细微之处。

重要的是要记住，[欧盟人工智能法案](https://oreil.ly/5TyDt)于 2024 年实施，考虑到我们生活在一个全球经济中，它产生了一些深远的影响。我们相信这将导致其他国家效仿欧盟的 GDPR 法律。为什么会这样？如果你看看当今世界的数据处理法规，公司要么因为拥有欧盟客户而必须遵守，要么它们的政府行动缓慢或迅速，最终采纳了该法律中的许多最佳实践。这与我们所看到的科技渗透效应没有不同，今天你使用的许多技术都起源于军事、游戏行业、社交媒体，以及我们名单上省略的一个其他行业。我们坚信，随着公平的商业实践、欺诈、版权、公民自由、隐私、公平性、失业、国家安全等问题进入政府手中，围绕人工智能的法规只会越来越严格。虽然我们无法预测未来——例如，于 2025 年开始执政的新美国政府与上届政府有不同的观点——但我们确信，关注只会越来越强烈。请放心，如果你没有为持续的变化做好准备，你的组织在采用人工智能时，如果没有一个全面、可配置的治理系统，将会遇到严重的问题。

## 什么需要监管——我们的观点

人们经常向我们询问关于应该监管什么的看法。这就像经典的“杯子是半满还是半空”的问题。我们认为这个问题没有抓住重点——现实主义者知道迟早有人会喝掉杯子里面的东西，他们将是那个需要清洗杯子的人。带着这个想法，让我们分享我们的现实主义观点：监管人工智能的使用，而不是人工智能技术本身。让我们进一步澄清：我们认为人工智能需要护栏和法规来避免用户受到伤害，但重点应放在监管特定的用例上，而不是压制具有巨大潜力改变世界的科技创新。

考虑这个问题并深思熟虑：你认为世界上所有国家是否会在所有情况下统一并遵循关于负责任使用人工智能的承诺？抛开地缘政治不谈，一些法规将城市或协会作为约束性目标，其粒度告诉你这永远不会发生。我们并不认为我们在悲观，我们只是知道最终会有人手里拿着一个脏杯子，不得不去清洗它。

是的，有了人工智能，现在虚假信息可以迅速传播。人工智能可以使虚假信息更具说服力。然而，停止使用人工智能并不能解决问题。不良行为者会从一个国家转移到另一个国家来传播危害，因为人工智能可以轻易跨越边界。我们希望看到政府监管与人工智能试图做什么、可能做什么或可能造成的伤害相关的更高风险级别。例如，欧盟人工智能法案为人工智能风险设定了四级分类系统：不可接受、高、有限和最小。每个级别都绑定于该法案内的特定法规条款。例如，最高级别是不可接受风险（第 5 条），禁止使用如行为操纵、为执法目的进行远程生物识别、公共当局进行社会评分等。正如你可以想象的那样，违反这一级别的处罚比第三级（有限风险——第 52 条）的处罚要严重得多，第三级包括冒充或欺骗的风险。我们希望目标集中在识别那些“潜在危险”的人工智能用例，并告诉肇事者，如果他们被抓住，他们将受到处罚、罚款和刑事起诉。

当谈到受监管的行业时，我们认为最大的问题应该是，“是否有人在回路中？”我们认为人*应该*在回路中——“询问和调整”至关重要。这是一个相当基本的原则，但并不是每个人都这样看待。但我们认为这是关键（特别是对于代理人工智能）并且是实际使用这项技术的有效保障。

## 管理人工智能生命周期

我们认为，鉴于你至少会尝试遵守你已拥有或将来会收到的所有监管命令的合理假设，很明显，你将面临跟踪你的模型的挑战。这并不像我们之前讨论的所有加密密钥那样。简而言之，你需要能够根据准确性、公平性等领域的监管标准跟踪你的模型，并且你需要技术来帮助你做到这一点。

例如，图 5-12 展示了我们使用 watsonx.governance 设置的仪表板，用于跟踪多模型部署。我们的仪表板为我们提供了环境的快速视图。有来自 OpenAI、IBM、Meta 和其他处于审查状态的 LLM。在我们的例子中，我们有五个需要我们关注的非合规模型。其他小部件定义了用例、风险等级、托管位置（本地或超大规模计算器）、部门用途（对于费用回收是个好主意）、审批生命周期中的位置等。当然，你可以深入这些细节，但我们最喜欢这个工具的一点是它能够将监管框架附加到模型上，以帮助定义和监管它。

你所选择的工具集也应该提供解释决策和自动收集元数据的能力，以便审计员可以确定模型是如何训练的以及为什么产生了他们所看到的输出。

![计算机屏幕截图，AI 生成的内容可能不正确。](img/aivc_0512.png)

###### 图 5-12\. 使用 watsonx.governance 构建仪表板并跟踪多模型部署环境

### 隐藏在表面之下

虽然图 5-12 让你瞥见了管理 AI 的强大仪表板，但隐藏在下面的实际上是实际的编排和操作流程，以防止你跌入边缘。在本章前面，我们给出了模型漂移的例子。模型漂移的事实意味着它们需要生命周期管理。实际上，当你将模型投入生产的那一刻起，它就开始变得过时。在你建立 AI 治理实践时，重点关注本章概述的杠杆，要知道它不能仅限于数据科学部门。它需要在整个企业范围内共享信息和做出决策，从业务单位对模型的初始请求，到批准基础设施资源以进行推理，再到训练数据的治理、开发、测试和调整、风险评估，以及部署和之后。良好的 AI 治理实践将涉及技术和非技术利益相关者，并且不仅必须尽可能自动化流程以减轻数据科学部门的压力，还必须确保决策者能够及时、相关地访问他们需要的、以加快价值实现时间的数据。你的 AI 平台应自动捕获元数据，包括用于构建模型的训练数据和框架，以及随着模型从用例请求到开发、测试再到部署的进展而进行的评估信息。这些数据应通过可搜索的、受管理的目录提供给审批者，确保决策者对模型的血统和性能有一个全面的了解。

### 一个端到端治理流程的示例

如果你拥有正确的工具和生命周期管理，那么你有机会实施一个端到端 AI 治理流程，流程大致如下：

1.  一旦模型提案通过了适当的审批流程，就会在你的模型库存中创建一个模型条目。此条目会持续更新新的信息。

1.  模型开发者使用他们选择的工具和模型来构建 AI 解决方案。训练数据和指标会自动捕获并保存到模型条目中（假设供应商公开了这一点——这就是为什么你需要开放模型的原因）。也可以保存自定义信息。

1.  当预生产模型被评估准确性、漂移和偏差时，性能元数据被捕获并同步。

1.  模型经过审查并获得生产批准。

1.  模型被部署到你决定部署的地方（本地、边缘、云端），并且再次，相关的元数据被捕获并同步。

1.  最后，生产模型持续受到监控，性能数据被捕获并同步。仪表板（如图 5-12 所示）提供了所有模型（无论供应商）的性能指标的综合视图，使利益相关者能够主动识别并应对任何问题。

# 总结

美国的创始人之一（及其第四任总统）詹姆斯·麦迪逊曾经说过：“信任的流通比货币的流通更重要。”他的观点是：不仅仅是财富的流动很重要，更重要的是支撑社会、政治和经济体系的基础信任和信心。鉴于 GenAI 和代理人在历史上的地位，他肯定会将其添加到他的列表中。

事实上，大多数公司的文化将本章中概述的许多主题视为典型的合规性监管和默认采取“最小努力遵守”的方法。本章涵盖的主题可以用于其他好处并加速其他旅程。我们不禁觉得前述列表中可能有些东西让你感到困扰——我们提到了“机会”。为什么这么说呢？因为治理关乎文化，技术帮助你实施这种文化。但请始终记住：*人们信任的 AI 才是人们会使用的 AI*。

我们认识到，本章有很多内容需要涵盖，但分配给它的空间却不够。话虽如此，我们希望你已经对需要进一步了解的事物有了感觉。说到学习，这正是我们接下来要讨论的内容。

^(1) 这里有一些需要讨论的不舒服的话题。*我们*不喜欢写关于它们的内容，但它们对你理解这些内容很重要。

^(2) 案例：Mata 诉 Avianca 公司，1:2022cv01461，在纽约南区提起诉讼。

^(3) 艾丽尔·沃尔德曼，“罪犯使用 AI 进行更大规模的欺诈，”TechTarget，2024 年 12 月 4 日，[*https://oreil.ly/7VgiB*](https://oreil.ly/7VgiB)。

^(4) CNN，“金融工作者在与深度伪造‘首席财务官’的视频通话后支付 2500 万美元，”2024 年 2 月 4 日，[*https://oreil.ly/xwZY1*](https://oreil.ly/xwZY1)。

^(5) 皮特·埃文斯，“苹果用户可以通过新软件更新拒绝被追踪，”CBC 新闻，2021 年 4 月 26 日，[*https://oreil.ly/QL2Fe*](https://oreil.ly/QL2Fe)。

^(6) Acxiom 公司 2018 年 3 月 31 日财政年度 10-K 年度报告，提交给美国证券交易委员会，2018 年 5 月 21 日，[*https://oreil.ly/SpkKt*](https://oreil.ly/SpkKt)。

^(7) 使用像“伦理”这样的关键词，我们的目的是涵盖确保治理、可解释性、公平使用、隐私以及围绕您的 AI 项目等方面的所有内容——良好的行为。我们不会在本章中详细阐述所有伦理考量，但您会发现几乎所有这些考量都可以归入我们在本节中向您介绍的一个或多个杠杆。

^(8) Stephen Jones，“自动化招聘系统‘隐藏’候选人给招聘人员——我们如何阻止这种情况？”，世界经济论坛，2021 年 9 月 14 日，[*https://oreil.ly/2C-dn*](https://oreil.ly/2C-dn).

^(9) Robert Bartlett 等人，“金融科技时代的消费者贷款歧视”，(工作论文，加州大学伯克利分校，2019 年)，[*https://oreil.ly/C5iaB*](https://oreil.ly/C5iaB).

^(10) Fengqing Jiang 等人，“ArtPrompt：基于 ASCII 艺术的针对对齐 LLMs 的越狱攻击”，预印本，arXiv，2024 年 2 月 19 日，arXiv:2402.11753，[*https://arxiv.org/abs/2402.11753*](https://arxiv.org/abs/2402.11753).

^(11) 红队测试是一种测试网络安全有效性的过程，其中道德黑客进行模拟且非破坏性的网络攻击。他们的模拟攻击帮助组织识别其系统中的漏洞，并对安全操作进行有针对性的改进。

^(12) Neil Vigdor，“苹果卡在性别歧视投诉后受到调查”，*《纽约时报》*，2019 年 11 月 10 日，[*https://oreil.ly/Mo9NZ*](https://oreil.ly/Mo9NZ).

^(13) “扩展单义性：从 Claude 3 Sonnet 中提取可解释特征”，Transformer Circuits，2024 年，2023 年 10 月 25 日访问，[*https://oreil.ly/AFZ4w*](https://oreil.ly/AFZ4w).

^(14) “金门 Claude”，Anthropic，2023 年 10 月 25 日访问，[*https://oreil.ly/o5r6S*](https://oreil.ly/o5r6S).

^(15) “教会大型语言模型‘忘记’不希望的内容”，IBM 洞察，2024 年，[*https://oreil.ly/hzltJ*](https://oreil.ly/hzltJ).

^(16) Ronan Eldan 和 Mark Russinovich，“谁是哈利·波特？在 LLMs 中的近似反学习”，预印本，arXiv，2023 年 10 月 4 日，[*https://arxiv.org/abs/2310.02238*](https://arxiv.org/abs/2310.02238).
