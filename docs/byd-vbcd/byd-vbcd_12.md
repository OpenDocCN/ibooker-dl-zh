# 第九章\. 感应编码的伦理影响

随着人工智能辅助开发变得越来越普遍，解决这一新范式在伦理和社会方面的含义至关重要。本章从技术细节中抽身，通过伦理视角来审视感应编码：这些新的开发方法可能有效，但它们也需要负责任地实施，并使个人和整个社会受益。

我从知识产权（IP）的问题开始。谁拥有人工智能生成的代码，并且在不注明出处的情况下使用可能源自开源代码的人工智能输出是否允许？从那里，我考虑偏见和公平性。透明度也是另一个焦点：开发者是否应该披露代码库中哪些部分是由人工智能生成的，团队如何确保代码质量和错误的责任？

我概述了人工智能使用中的负责任开发实践，从建立透明度和问责制到避免在提示中使用敏感数据，再到确保可访问性和包容性。本章以一组使用人工智能工具的负责任指南结束。

# 法律免责声明

下文将涉及复杂法律主题，尤其是从美国视角出发的版权和知识产权法。全球法律体系和解释正在演变，尤其是在人工智能方面。此信息仅用于教育目的，并不构成法律建议。在基于此信息做出任何决定之前，您应咨询合格的知识产权律师，尤其是如果您对代码的所有权或许可有疑问，或者代码是由人工智能工具生成的。

# 知识产权考虑事项

谁拥有人工智能生成的代码？使用它是否尊重了人工智能训练所依据的源材料的许可证和版权？像 GPT 这样的 AI 模型已经在互联网上大量代码上进行训练，包括具有各种许可证的开源存储库（MIT、GPL、Apache 等）。如果人工智能生成了一段与 GPL 许可项目非常相似（或相同）的片段，那么在专有代码库中使用该片段可能会无意中违反 GPL，通常要求[共享衍生代码](https://oreil.ly/8inJc)。

根据开源规范和一般版权原则，如果几行代码片段缺乏足够的独创性，不能被视为独立创意作品，或者其使用*可能*被视为微不足道（过于微不足道，不值得法律关注），那么它们可能*不*受版权保护。然而，任何实质性的或表达独特创意选择的内容更有可能受到版权保护。重要的是要理解，“开源”并不意味着“公共领域”。默认情况下，包括代码在内的创意作品由其作者享有独家版权。开源许可证明确授予了版权法通常限制的权限。

如果你想了解更多关于开源规范的信息，以下是一些好的起点：

开源倡议

[OSI](https://oreil.ly/hmJVN)定义和推广开源软件，维护开源定义，并批准符合其标准的许可证。

自由软件基金会（FSF）

[FSF](https://fsf.org)倡导“免费软件”（与开源原则有强烈重叠）并管理着像 GNU 通用公共许可证（GPL）这样的许可证。

项目特定文档

个体开源项目通常包括*LICENSE*文件、*README*文件和*CONTRIBUTING*指南，详细说明了该特定项目的使用条款和贡献。

社区和法律资源

类似于 GitHub 的网站提供了关于开源实践的广泛文档和讨论。像 Linux 基金会和法律信息网站这样的组织也提供了关于开源合规性和法律方面的宝贵资源。

使用小代码片段是否与[合理使用原则](https://oreil.ly/d0ZK8)（在美国；许多其他司法管辖区称为“公平使用”）重叠的问题复杂且高度依赖事实。合理使用[允许](https://oreil.ly/EwrJ2)在未经许可的情况下有限使用受版权保护的材料，用于批评、评论、新闻报道、教学、学术研究或研究等目的。美国法院通常考虑以下四个因素来确定合理使用：

+   使用目的和性质（商业与非营利，变革性对复制性）

+   受版权保护作品的性质（高度创意与事实性）

+   所用部分的数量和实质与整个受版权保护作品的关系

+   使用对受版权保护作品潜在市场或价值的影响

虽然有些人可能会认为，为了互操作性或访问不受版权保护的想法而复制非常小的、功能性的代码片段可能属于合理使用，尤其是如果使用具有变革性，但这并不是代码法律中一个明确确定的领域，也没有一个普遍认同的行数数量可以明确地界定为“合理使用”或最小损害。最安全的做法通常是获得许可或理解底层想法，并以自己的方式重写代码。美国最高法院案件 *Google LLC v. Oracle America, Inc.* 在软件 API 的背景下讨论了合理使用，认为谷歌对 Java API 声明代码的重实现属于合理使用，但这是一个针对 API 声明特定且复杂的裁决，并不适用于所有代码。通常认为，版权保护的是想法的具体表达，而不是想法、程序或操作方法本身。

通常，使用 AI 的开发者被视为“作者”，因为 AI 是一个工具，类似于编译器或文字处理器。因此，如果在工作环境中生成代码，那么使用该工具的开发者所在的公司可能会拥有由开发者生成的代码，但需遵守 AI 工具的服务条款和底层知识产权问题。然而，AI 工具的服务条款（ToS）至关重要。大多数 ToS 授予用户对其生成的输出拥有权利。例如，OpenAI 的 ToS 规定：“您拥有使用 GPT-4 创建的输出，包括代码。”

然而，这种“所有权”需要仔细考虑。这通常意味着 AI 提供商并不声称拥有你使用他们的工具所创建的内容的所有权。但这假设你拥有你提供的输入的权利，并且这并不意味着输出本身有资格获得版权保护，或者它不受第三方知识产权主张的影响。如果你将你自己的原创代码输入到工具中进行修改或扩展，输出很可能是你的（或你雇主的），再次强调，这取决于 AI 如何处理它以及它从训练数据中吸收了什么。但如果你输入他人的版权代码进行修复或转换，输出*可能*被视为[该第三方代码的衍生作品](https://oreil.ly/mBPyq)。

在美国和许多其他司法管辖区，AI 生成的输出与训练数据实质上相似，或者基于受版权保护输入的输出是否构成衍生作品，是一个持续的法律辩论主题，且缺乏完全的清晰度。不要将不属于你（或适当许可的）版权代码的大量片段输入到 AI 工具中，因为输出可能被视为[衍生作品](https://oreil.ly/O4ktq)，从而落入原始版权代码的许可之下。

在这些不确定性面前，为了安全起见，将 AI 生成的代码视为处于模糊许可之下，并且只有在你确信它不会侵犯现有版权并且你能够遵守任何潜在的开源许可义务时才使用它。关于 AI 输出本身的版权状态，[美国版权局已声明](https://oreil.ly/Y0PYG) ，仅由 AI 生成且缺乏足够人类作者的作品不可获得版权。如果人类以创造性的方式显著修改或编排 AI 生成的材料，那么人类的贡献可能具有版权性，但 AI 生成的元素本身则不一定。因此，通常明智的做法是假设纯粹由 AI 生成的输出可能无人拥有版权，或者版权仅限于人类的创造性贡献。

这不是一种假设的担忧。事实上，存在持续的司法辩论。一项[著名的集体诉讼案，*Doe v. GitHub, Inc.*](https://githubcopilotlitigation.com) 已对 GitHub、微软和 OpenAI 提起，声称 GitHub Copilot 生成的代码与受许可的开源代码过于相似，而没有适当的归属或遵守许可条款。虽然此案中的一些主张已被驳回或正在上诉（截至 2025 年中，该案件涉及持续的诉讼，包括对第九巡回法院关于 DMCA 主张和剩余违约诉讼的上诉），但它凸显了一个真正的担忧：AI 可以，有时确实会从其训练数据中重复或紧密释义受版权保护的代码。（^(1）

GitHub 本身的一项（虽然较旧但仍然相关且后来得到证实）[研究](https://oreil.ly/fFUUd) 指出，在某些情况下，Copilot 的输出包括与训练数据匹配的建议，包括罕见的长句直接引用。尽管大多数 AI 工具设计用来避免直接、广泛的复制可识别的代码，除非被明确提示或处理非常标准的算法，但风险是存在的。此外，问题不仅限于开源代码；作者、艺术家和媒体公司已提起众多诉讼，声称他们的完全版权、私有知识产权未经许可或未获补偿就被用于训练大型语言模型和其他生成性 AI 系统。与专有代码相比，挑战在于，与开源不同，它通常不是公开可见的，这使得最终用户更难确认 AI 的输出是否无意中与这样的私有代码相似。

尽管如此，道德和谨慎的做法是*假定你从 AI 工具接受的任何代码都是你的责任*。在将任何 AI 生成的代码纳入你的项目之前，彻底审查、测试和理解它，并确保其使用符合所有适用的许可和版权法律。

## 如果你得到可疑的输出，应该怎么做

如果 AI 的输出看起来是已知代码的逐字或近似逐字复制（特别是如果它包括独特的注释或作者姓名），请谨慎处理。考虑使用抄袭检测工具进行相似性检查，或者进行网络搜索以查看是否有任何匹配项可以表明复制。

另一个需要遵循的原则是*如有疑问，则弃之不用*。要么避免使用输出，要么确保它符合兼容的许可协议，并在需要时提供归属。例如，如果 Copilot 输出一个你从 Stack Overflow 或开源项目中识别出的知名算法实现，请引用来源或以你自己的方式重写它，将 AI 的答案作为指导，但不要直接引用。

如果你怀疑输出与现有的库解决方案相匹配，考虑包含库本身（带有适当的许可证）。你也可以提示 AI：

> 请提供原始实现，而不是从库中复制的实现。

它可能会合成一个更独特的解决方案。（无法保证它不会被训练代码所影响，但至少它会尝试不直接复制）。

这里的伦理问题还涉及到不使用 AI 故意去除归属。例如，通过 AI 复制 Stack Overflow 上的代码而不提供归属以规避你应该归功于答案的政策是不道德的。这会破坏开放知识生态系统的信任。更好的做法是以适当的信用融入材料。根据情况，这可能意味着以下内容：

+   如果 AI 从某个有作者姓名的来源（如注释中包含“John Doe 2018”的片段）编写了代码注释，你应该保留它或将它移至适当的归属部分，并附上完整的引用，而不是删除它。这尊重了原始作者的信用。

+   如果 AI 提供了一个你已知来自已知算法或代码片段的解决方案，你应该像自己查找那样引用该来源。

+   如果 AI 工具创造了一些具有争议的创意内容（如独特的文档方法或文本），请承认其贡献。尽管它没有权利，但这关乎透明度（也许是对技术的认可）。

一些开源许可证（如 MIT）足够宽容，包含带有归属的复制的代码就能满足许可证的要求。其他一些，如 GPL 或 AGPL，如果你包含这些代码，就会“感染”你的整个代码库，这对封闭项目来说是不理想的。

简而言之：如果你怀疑 AI 给你提供了可能引起知识产权问题的东西，要么避免使用它，要么充分转换它以确保你遵守任何可能的许可证。

## 模糊区域

即使在我撰写这篇文章的时候，AI 工具仍在提出关于知识产权、版权和伦理的新问题。例如：

+   如果你的编码风格包括使用 AI 生成非代码资产，如文档文本、配置文件或图像，类似的知识产权问题也会出现。例如，如果你通过一个在受版权保护的图像上训练的 AI 工具生成图标图像，那么新的图像归谁所有？

+   如果 AI 编写了软件产品的很大一部分，那么 AI 训练代码的原始作者应该得到认可吗？

+   是否有人声称你的 AI 生成的代码侵犯了他们的版权，因为它看起来与他们的一样？如果非重要部分的长度可能相同，这就是相似性检查介入的地方。

正在出现一种观点，即 AI 公司可能需要实施尊重许可的过滤器或允许团队选择不将其代码包含在 AI 训练数据中。这正在演变，但地面上的开发者应该谨慎行事，以免侵犯权利。

法院需要时间来解决所有法律问题，但在此期间，我们应该以诚信和尊重为指导。如果 AI 使用了一个已知的、来自已发表论文的算法，请在注释中引用该论文。如果它使用了常见的开源辅助代码，请为该项目提供信用。这是关于尊重作者的问题。如果你认识到某物来自何处，就倾向于给予信用。这是一种促进透明度的良好做法。

记住，在底层，AI 的知识来自成千上万的公开分享代码的开发者。从道德上讲，软件行业应该尊重开源许可和规范，尊重这个社区。给予应得的信用，不要以“AI 写了它，不是我”为借口滥用他人的工作。

# 透明度和归属

**透明度**指的是在开发过程和输出中公开使用 AI，**归属**指的是当 AI 生成的代码来自可识别的来源时给予适当的信用。

透明度对于问责制也很重要。例如，如果 AI 生成的代码引入了错误或安全漏洞，明确指出“这段代码是 AI 建议的”可能有助于你分析根本原因——可能需要重写模糊的提示。在代码注释或项目的 README 或文档中，你可能会提到“该项目是在 ChatGPT 等 AI 工具的帮助下构建的。”或者更具体：“添加了一个解析 CSV 的功能（在 ChatGPT 的帮助下生成，然后进行了修改）。”这有点像承认你使用了框架或库。

透明度也是建立信任的关键：利益相关者（你的团队、客户、最终用户或行业监管机构）可能想知道你的软件是如何开发和验证的。如果 AI 参与了代码生成，一些利益相关者可能会错误地过分或不足地信任它。透明度允许就可靠性进行对话：“是的，我们使用了 AI，但我们已经彻底测试了它”或“这部分很棘手——我们让 AI 生成了初始代码，但我们后来已经验证了它。”

在许多学术场合，也需要或期望提供归属信息。一些开源项目由于知识产权的担忧，限制或甚至禁止 AI 贡献，因此在使用 AI 之前请检查贡献指南。如果补丁是由 AI 生成的，向维护者保持透明可以帮助他们评估它，尤其是如果版权是一个担忧的话。

事实上，一些高度监管的行业要求软件供应商披露任何 AI 使用情况，以供审计目的。[欧盟的 AI 法案](https://oreil.ly/wDNKs)要求对影响个人的自动化决策（如信用评分算法）进行透明度管理。如果 vibe 编码导致这样的系统，通知用户“推荐是自动生成的，可能反映了数据中的模式”成为一种法律/道德上的必要性。

同样，如果你的产品将用户数据或用户提供的代码示例等专有数据输入到 AI 模型中，以微调它并帮助编程其分析，你可能需要在隐私政策中说明用户数据可能会在许可的情况下用于改进 AI 模型（当然，在法律事务上，始终要咨询律师）。透明度在这里与隐私相交。

一般而言，承认你使用的工具和来源也是道德上的要求。如果你的 30%代码是由 Copilot 生成的，那么在文档或内部沟通中提及这一点是公平的——不是为了贬低自己的角色，而是为了对过程保持诚实。

一些开发者可能害怕承认 AI 的帮助，担心这可能会削弱他们感知到的贡献或技能，或者被视为“作弊”。随着 vibe 编码变得更加普遍，这种耻辱感应该会减少；最终，如果你不使用可用的 AI，你可能会被视为落后于时代。我们需要将 AI 作为一种工具来正常化——它并不比使用 Stack Overflow 或 IDE 更“作弊”。

从另一方面来看，提供过多的免责声明可能会引起不必要的担忧。如果你告诉客户，“我们使用了 AI 来编码这个产品”，他们可能会质疑其安全性（即使这是由于误解）。你如何措辞很重要。强调质量措施： “我们利用高级编码助手加快了开发速度，所有 AI 生成的代码都经过严格审查和测试，以满足我们的质量标准。”

总的来说，透明度和归属感可以培养信任和社区价值观。它们确保信用流向人类创作者，并且我们保持对软件构建过程的诚实。这就像艺术家列出他们的工具或灵感一样；它并不减少艺术的价值；它为艺术提供了背景。如果你像我一样，希望 vibe 编码被广泛接受，那么公开使用 AI 以及如何减轻其风险是很重要的。

# 偏见与公平

如你所知，到这本书的这一部分，AI 模型的输出反映了它们训练的数据。如果这些数据包含偏见或排他性模式，模型可以产生有偏见或不公平的输出。

你可能会问：“代码怎么会有偏见？LLM 又不是在做招聘决定或类似的事情。”但偏见可能会以微妙的方式渗透到你的编码中：

+   代码通常反映了其创作者的假设。AI 生成的面向用户的文本或内容可能会反映其训练数据中的文化偏见或无礼语言。例如，2016 年早期聊天机器人[微软的 Tay](https://oreil.ly/d8wxO)臭名昭著地学会了在启动后几小时内从 Twitter 互动中模仿种族主义和性别歧视的侮辱性语言。

+   假设也可能针对特定的文化规范，例如中产阶级北美生活方式（例如假设拥有汽车或对某些技术的普遍访问）。一个未经验证的假设导致排他性产品的显著例子是苹果公司在 2014 年的[首次发布 Health 应用](https://oreil.ly/67sZG)，该应用缺少经期追踪器——这是一个重大的疏忽，很可能是由于设计团队缺乏多样性和视角。即使在示例代码、注释或合成数据中，模型也可能始终使用*他/他*代词，从而强化性别偏见。

+   众所周知，代码库和更广泛的软件开发领域主要反映了西方观点和英语使用者的视角。因此，在这些代码库上训练的 AI 可能会忽视关键的国际化方面，例如对 Unicode 和多字节字符的适当支持（对于中文、日语、韩语、阿拉伯语、印地语以及许多使用非拉丁或音节文字的语言至关重要），或者它可能会默认使用以英语为中心的示例，例如类型名称。开发者必须提高意识，并设计和编码以实现国际化，即使 AI 没有自发这样做。

+   如果编写算法，要警惕某些变量，如种族、性别、年龄等。AI 可能不会自发地包括它们，除非被要求，但如果它产生了某些标准或如果你在一个数据集上使用像 Code Assistant 这样的 AI，应用公平约束；AI 本身不会天生知道道德或法律背景。

不仅仅是编码，模型还可以在其内容领域反映*数据偏差*：它们训练数据中存在的历史偏差。例如，考虑一个被赋予为贷款审批算法编写代码的 AI。在美国，信用评分系统有记录的历史表明，它们反映了并持续着种族偏见。这些偏见源于历史上的做法，如红线划分和其他形式的系统性歧视，这些做法对黑人社区和其他边缘化群体产生了持久的经济影响。（参见理查德·罗思斯坦的*《法律的颜色》* [经济政策研究所，2017]，了解政府政策如何将美国隔离的历史。）

如果训练数据反映了这些历史偏见，AI 可能会包含歧视性变量，例如使用邮政编码（由于住房隔离模式，它可以作为种族人口统计的代理）或其他看似中性的数据点，这些数据点与受保护的特征相关。如果没有得到适当的指导，AI 可能会生成导致银行做出不公平贷款决定的代码，从而延续历史不平等并影响真实人们的生活。类似的问题也出现在预测警务算法等领域，其中历史逮捕数据（本身可能存在偏见）可能导致 AI 系统[不成比例地针对某些社区](https://oreil.ly/H4rmr)。

类似地，如果您正在使用专用模型（例如针对医疗软件进行微调的 AI 代码助手），请确保该模型没有陷入该领域数据的偏见。例如，历史上，一些医疗指南由于主要使用男性受试者的研究而存在偏见，导致对其他性别误诊或治疗效果不佳。如果 AI 正在推荐用于医疗诊断的代码或解决方案，您需要确保它没有无意中编码这些偏见。

虽然有工具可以检测 AI 输出的偏见，但这些工具在用于生成内容的 GPT 模型中更为常见，AI 提供商本身也试图过滤明显有偏见或有毒的输出。以代码为导向的 AI 很少会自发产生仇恨言论，但它们有内容过滤器是好事。在许多 AI 工具中，内置道德约束意味着，如果用户试图让 AI 创建恶意软件或歧视性算法，它将拒绝。不要试图绕过这些过滤器来获取不道德的输出。

尽管如此，在开发过程的各个阶段，还有许多其他方法可以识别和减轻偏见。这些包括：

使用多样化的示例进行测试

如果您的 AI 生成面向用户的组件或处理与人类相关数据的逻辑，请使用多样化的输入进行测试。例如，如果 AI 生成的表单验证期望“名字”和“姓氏”，它是否允许单名，这在某些文化中很常见？如果不允许，那就是假设上的偏见。如果它生成示例用户名，它们是否都是像“JohnDoe”这样的？如果是这样，考虑在示例中增加多样性。

提示包容性

您可以明确指示 AI 保持中立或包容：“生成使用来自不同文化的各种名字的示例。”如果它总是用“他”来称呼用户，您可能会提示：

> 在此代码注释中避免使用性别语言；使用中性措辞或他们/他们代词。

此外，对 AI 可能产生的可能文化不敏感的笑话或示例要谨慎；您可以提示它使用专业语气以避免这种情况。AI 通常会遵守。它没有议程；它只是输出它认为正常的内容，除非被告知否则。我们塑造那个“正常”。

聘请多元化的团队

让一个多元化的团队审查输出可以捕捉到问题。例如，有人可能会说，“嘿，我们的 AI 总是选择像 foo/bar 这样的变量名，这没问题，但在文档中，它的所有角色都是男性化的。”然后你可以系统地纠正这一点。如果所有开发者都来自相似背景，他们可能不会捕捉到微妙的偏见。如果可能的话，在审查 AI 使用指南时，涉及来自代表性不足的群体的人——或者至少考虑他们的观点。

总结来说，偏见和公平性是关于使用 vibe-coding 工具来生成对所有背景用户都公平的代码，并且不会反映——或者更糟，延续——历史歧视。我们在团队中使用这些工具的方式也应该对各种水平和背景的开发者和其他同事公平。参见第四章，讨论了 AI 工具如何改变工作场所的道德影响，特别是对初级开发者的影响。

# 负责任使用 AI 的黄金法则

将我们讨论的许多内容综合起来，值得阐述一套 vibe coding 的负责任实践：

1.  *始终将人类纳入循环。*

    再次强调：绝不要让 AI 无监督地工作。负责任的 AI 辅助开发意味着你，作为开发者，正在审查每一行并做出决定，而不是在没有人类验证的情况下部署原始 AI 输出。

1.  *对你的代码负责。*

    如果出现问题，这不是 AI 的错——这是开发团队的责任。保持这种心态可以避免自满。无论你是从头开始编写代码还是接受 AI 代码，都要准备好为自己的代码辩护。如果有人问你，“为什么代码会这样做？”不要说，“我不知道；Copilot 做了那件事。”这就是为什么第三章的黄金法则之一是“永远不要提交你不完全理解的代码。”这是负责任工程。

1.  *保护用户的隐私并征求他们的同意。*

    在道德上，你有责任保护用户的秘密数据。在使用 AI 工具时，尤其是基于云的工具时，要小心不要在提示或对话中暴露敏感数据。例如，如果你正在调试用户数据库的问题，不要将实际的用户记录输入到 ChatGPT 中。使用清洗过的或合成的数据代替。

    许多工具现在允许用户（至少是商业用户）选择不使用他们的输入数据用于训练。如果你是企业用户，使用那些设置或使用本地解决方案来处理敏感代码。如果你向模型提供了任何用户数据，或者如果任何 AI 功能直接接触用户（例如，你的应用程序中使用的 LLM 的聊天机器人），获取用户的同意，并在适当的情况下允许他们选择退出。像“此功能使用 AI 服务；您的输入将被发送到它进行处理”这样的警告是透明的，并让对隐私敏感的用户自己做出决定。

1.  *遵守法律和法规。*

    关注 AI 相关的法律要求，这些要求不断演变。例如，像欧盟的通用数据保护条例（GDPR）和 AI 法案这样的数据保护法律，如果 AI 输出包含任何个人数据，则将其视为个人数据。在用户数据上训练模型可能需要这些用户的同意。监管机构可能会将代码生成归类为“通用 AI”，并强制执行透明度或风险管理义务。保持信息畅通，并与您的法律和合规专业人员密切合作，以避免违反任何规定。

    虽然这应该是不言而喻的，但不要使用 AI 生成恶意软件、未经道德正当性证明的代码，或自动化不道德或非法的行为。2 虽然 AI 可能能够编写非常有效的钓鱼邮件或代码注入攻击，但将其用于此目的违反了伦理、大多数国家的法律，以及可能违反 AI 的服务条款。专注于建设性的使用。

1.  *在您的组织中培养负责任的 AI 文化。*

    如果您的团队采用情绪编码，鼓励关于伦理的讨论，并提供相关的伦理培训。考虑让开发人员和代码审查人员使用图 9-1 中的简短清单。

    ![](img/bevc_0901.png)

    ###### 图 9-1\. 负责任的 AI 开发清单：在将 AI 生成的代码集成到生产系统之前，包括知识产权审查、偏见评估和安全审计等基本验证步骤。

    每个人都应对 AI 的道德使用负责；这是一个集体努力，而不仅仅是任何时刻使用该工具的个人的负担。为了正式化这一点，考虑在您的团队或组织中指定一个“伦理倡导者”或一个小型的伦理委员会。这个个人或团体不会是伦理的唯一所有者（因为这种责任仍然是共享的），但他们会在以下方面带头：

    +   关注 AI 伦理的最新发展、新兴的最佳实践和新的监管环境

    +   促进关于特定项目中伦理考虑的讨论

    +   推动将伦理原则融入开发生命周期

    +   帮助整理和传播相关的资源和培训材料给更广泛的团队

    +   作为有伦理问题或担忧的团队成员的联系人

    由于这个领域发展极其迅速，与团队一起工作以保持对 AI 工具的新版本及其功能、限制和负责任使用的发展最佳实践的更新至关重要。

    由于这个领域发展迅速，作为团队工作以保持对 AI 工具的新版本和最佳实践的更新至关重要。要整合到您的工作流程中的一个重要概念是使用模型卡。*模型卡*基本上是提供关于机器学习模型透明度的标准化文档。把它们想象成 AI 模型的营养标签。它们通常包括以下详细信息：

    +   模型的定义、版本以及开发时间

    +   模型设计和测试的具体用例

    +   由于限制或潜在危害，模型不应使用的场景

    +   模型在各种基准测试中的表现如何，包括对不同人口群体公平性和偏差的评估

    +   用于训练模型的数据库信息，包括数据中任何已知的局限性或偏差

    +   潜在风险和社会影响以及采取的任何缓解策略

    无论何时您在使用预训练模型或评估模型用于使用时，都要寻找其模型卡片。如果您正在微调或开发模型，创建自己的模型卡片是最佳实践。

1.  *创建护栏和安全网。*

    实践负责任的设计意味着您的 AI 生成系统应该有安全网。例如，如果 AI 建议一个超出范围的索引修复，可能会掩盖潜在问题，那么系统安全失败比造成静默错误更好。如果一个 AI 生成的推荐系统可能是错误的，提供用户纠正或覆盖它的方法表明对他们的自主权的尊重。努力构建在 AI 组件行为不当时能够优雅降级的系统。

1.  *在您的团队中记录文档化的 AI 使用决策。*

    记录您使用某些 AI 建议（或未使用）的原因：“我们尝试使用 AI 进行模块 X，但它倾向于产生过多的重复代码，所以我们手动编写了那部分。”这可以帮助您完善流程，向新团队成员提供关于 AI 在代码库历史中的作用的背景信息，并增强团队的集体记忆。在审计期间也可能很有用。

1.  *积极努力避免偏见、歧视和不公平。*

    保持警惕，注意您的 AI 使用可能导致歧视的迹象，并在这些情况发生之前努力避免。例如，如果您的应用是全球性的，您的 AI 是否是多语言的，或者它是否偏袒讲英语的人？您的团队成员是否都能平等地访问 AI 工具和培训？

随着 AI 领域的持续变化和增长，软件行业可能会引入 AI 标准或认证。虽然还处于早期阶段，但您的公司可以通过参与标准化工作，如 IEEE 或 ISO 的 AI 软件工程工作组，甚至可以帮助塑造这些指南。从伦理的角度来看，开发社区帮助制定规则比完全由监管机构或法院制定更好。

# 摘要和下一步行动

负责任的设计意味着将 AI 集成到软件开发生命周期中，以尊重所有利益相关者：原始创作者（通过尊重其知识产权）、同事（通过透明度和公平性）、用户（通过隐私、安全和公平的结果）、以及社会（通过不让滥用造成伤害）。这是关于利用 AI 的优势，同时勤奋地防范其弱点。

我经常说，氛围编码不是低质量工作的借口。它也不是道德捷径的借口。作为负责的人类，开发者必须确保速度不会损害价值观。

接下来，第十章探讨了正在改变我们与 AI 模型合作方式的新技术：自主编码代理。

^(1) 案件信息通常可以在[法庭案卷](https://oreil.ly/BdDiV)中找到，例如美国加利福尼亚北部地区地方法院和第九巡回上诉法院的案卷，或者通过[法律新闻机构和案例追踪器](https://oreil.ly/AZrc-)。

^(2) 存在一些在道德上可以接受的例外情况。渗透测试员和安全研究人员可以在负责任的披露协议下，道德地使用 AI 来发现应该修复的漏洞。
