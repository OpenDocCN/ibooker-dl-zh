# 第三章. 词汇和标记化

在第二章中，我们深入探讨了用于训练今天语言模型的语料库，包括创建它们的过程。希望这次探索强调了预训练数据对最终模型的影响。在本章中，我们将讨论语言模型的另一个基本组成部分：其词汇。

# 词汇

当你开始学习一门新语言时，你首先做什么？你开始获取它的词汇，随着你对语言掌握程度的提高，不断扩大它。让我们在这里定义词汇为：

> 一个语言中所有被特定人理解的所有单词。

平均而言，母语为英语的人的词汇量在[20,000–35,000 个单词](https://oreil.ly/bkc2C)之间。同样，每个语言模型都有自己的词汇表，其中大多数词汇表的大小在 5,000 到 500,000 个*标记*之间。

以 GPT-NeoX-20B 模型为例，让我们来探索其词汇表。打开文件[*tokenizer.json*](https://oreil.ly/Kages)，使用 Ctrl+F 搜索“vocab”，这是一个包含模型词汇表的字典。你可以看到构成语言模型词汇的单词并不完全像字典中出现的英语单词。这些类似单词的单位被称为“类型”，而类型的实例化（当它在文本序列中出现时）被称为标记。

###### 注意

最近，尤其是在工业界，我很少听到有人除了在较老的 NLP 教科书中使用“类型”这个词之外，其他情况下都使用“标记”这个词来广泛地指代词汇单元以及它们在文本序列中出现的情况。因此，我们将继续使用“标记”这个词来描述这两个概念，尽管我个人并不是特别喜欢这种用法。

在词汇表中，我们可以看到每个标记旁边都有一个数字，这被称为*输入 ID*或*标记索引*。GPT-NeoX 的词汇量正好超过 50,000。

仔细查看词汇表文件，我们会发现前几百个标记都是单字符标记，例如特殊字符、数字、大写字母、小写字母和带重音的字符。较长的单词出现在词汇表的后面。许多标记对应于单词的一部分，称为*子词*，例如“impl”、“inated”等等。

让我们使用 Ctrl+F 搜索“office”。我们得到了九个结果：

```py
"Ġoffice": 3906
"Ġofficer": 5908
"Ġofficers": 6251
"ĠOffice": 7454
"ĠOfficer": 12743
"Ġoffices": 14145
"office": 30496
"Office": 33577
"ĠOfficers": 37209
```

Ġ字符表示单词前的空格。例如，在句子“他停止去办公室”中，单词“office”中字母“o”前的空格被认为是标记的一部分。你可以看到标记是区分大小写的：有“office”和“Office”两个不同的标记。如今，大多数模型都有区分大小写的词汇表。BERT 模型最初发布时，既有带大小写的版本，也有不带大小写的版本。

###### 注意

语言模型为每个这些标记学习向量表示，称为嵌入，它们反映了它们的句法和语义意义。我们将在第四章中介绍学习过程，并在第十一章中更深入地探讨嵌入。

有大小写的词汇表几乎总是更好的，尤其是当你在一个如此庞大的文本体上进行训练，以至于大多数标记被模型足够多次地看到，以便学习它们的独特表示时。例如，“web”和“Web”之间有明显的语义差异，为它们分别保留单独的标记是很好的。

让我们搜索一些数字。使用 Ctrl+F 搜索“93。”只有三个结果：

```py
"93": 4590
"937": 47508
"930": 48180
```

看起来并不是所有的数字都有自己的标记！934 的标记在哪里？给每个数字分配一个标记是不切实际的，尤其是如果你想将词汇表的大小限制在，比如说，50,000 个单词以内。在本章的后面部分，我们将讨论词汇表大小是如何确定的。流行的人名和地名有自己的标记。有一个标记代表波士顿、多伦多和阿姆斯特丹，但没有代表梅萨或钦奈的标记。有一个标记代表艾哈迈德和唐纳德，但没有代表苏哈斯或玛丽亚姆的标记。

你可能已经注意到像这样的标记：

```py
"]);": 9259
```

存在，这表明 GPT-NeoX 也准备好处理编程语言。

词汇是如何确定的？当然，肯定没有执行委员会在紧急会议上熬夜，成员们为了在词汇表中包含数字 937 而向 934 做出激动的请求。

让我们重新审视词汇的定义：

> 一个特定的人能理解的语言中的所有单词。

由于我们希望我们的语言模型成为英语的专家，我们可以将其词汇表包括所有英语词典中的单词。问题解决了吗？

远远不够。当你用模型从未见过的词与语言模型进行交流时，你会怎么做？这种情况比你想象的要频繁得多。新词不断被创造出来，有些词有多种形式（“理解”，“理解力”，“可理解的”），多个词可以组合成一个词，等等。此外，还有数百万个特定领域的词汇（生物医学、化学等）。

###### 注意

社交媒体平台 X 上的账户[@NYT_first_said](https://oreil.ly/FzfI9)在《纽约时报》首次出现时发布单词，除了专有名词。每天，平均有五个新词首次出现在美国的记录报纸上。在我写这一部分的那天，这些词是“unflippant”、“dumbeyed”、“dewdrenched”、“faceflat”、“saporous”和“dronescape”。其中许多词可能永远不会被添加到词典中。

词汇表中不存在的标记被称为 OOV 标记。传统上，OOV 标记使用特殊的<UNK>标记表示。<UNK>标记是所有不在词汇表中标记的占位符。所有 OOV 标记共享相同的嵌入（并编码相同的意义），这是不理想的。此外，<UNK>标记不能用于生成模型。你不想你的模型输出类似以下内容：

```py
'As a language model, I am trained to <UNK> sequences, and output <UNK> text'.
```

为了解决 OOV（Out-of-Vocabulary）问题，一个可能的解决方案是将标记表示为字符而不是单词。每个字符都有自己的嵌入表示，只要所有有效的字符都包含在词汇表中，就永远不会遇到 OOV 标记。然而，这种方法有很多缺点。表示平均句子所需的标记数量变得非常大。例如，句子“表示平均句子所需的标记数量变得非常大”当每个单词作为一个标记时包含 13 个标记，但当每个字符作为一个标记时包含 81 个标记。这减少了在固定序列长度内可以表示的内容量，这会使模型训练和推理速度变慢，我们将在第四章中进一步展示。模型支持有限的序列长度，这也减少了可以在单个提示中放入的内容量。在本章的后面部分，我们将讨论像 CANINE、ByT5 和 Charformer 这样的模型，它们试图使用基于字符的标记。

因此，折中方案和两者之优（或者两者之劣——该领域尚未达成共识）是使用子词。子词是目前在语言模型空间中表示词汇单位的主要方式。我们之前探索的 GPT-NeoX 词汇表使用子词标记。图 3-1 显示了 OpenAI 分词器游乐场，展示了 OpenAI 模型如何将单词分割成它们的构成子词。

![Subword tokens](img/dllm_0301.png)

###### 图 3-1. 子词标记

# 分词器

接下来，让我们深入了解分词器，它是人类和模型之间作为文本处理接口的软件。

分词器有两个职责：

1.  在分词器预训练阶段，分词器在文本体上运行以生成词汇表。

1.  在模型训练和推理过程中处理输入时，自由形式的原始文本会通过分词器算法进行处理，将其分割成有效标记的序列。图 3-2 描述了分词器所扮演的角色。

![Tokenizer Workflows](img/dllm_0302.png)

###### 图 3-2. 分词器工作流程

当我们将原始文本输入到分词器时，它会将文本分割成词汇表中的标记，并将标记映射到它们的标记索引。然后，标记索引的序列（输入 ID）被输入到语言模型，在那里它们被映射到相应的嵌入表示。让我们详细探讨这个过程。

这次，让我们实验 FLAN-T5 模型。你需要一个 Google Colab Pro 或等效系统才能运行它：

```py
!pip install transformers accelerate sentencepiece
from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-largel")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large",
    device_map="auto")

input_text = "what is 937 + 934?"
encoded_text = tokenizer.encode(input_text)
tokens = tokenizer.convert_ids_to_tokens(encoded_text)
print(encoded_text)
print(tokens)
```

输出是：

```py
[125, 19, 668, 4118, 1768, 668, 3710, 58, 1]
['▁what', '▁is', '▁9', '37', '▁+', '▁9', '34', '?', '</s>']
```

`encode()`函数将输入文本分词，并返回相应的标记索引。使用`convert_ids_to_tokens()`函数将标记索引映射到它们所代表的标记。

如你所见，FLAN-T5 分词器没有为数字 937 或 934 分配专门的标记。因此，它将数字拆分为“9”和“37”。`</s>`标记是一个特殊标记，表示字符串的结束。`_`表示标记前面有一个空格。

让我们再试一个例子：

```py
input_text = "Insuffienct adoption of corduroy pants is the reason this

economy is in the dumps!!!"
encoded_text = tokenizer.encode(input_text)
tokens = tokenizer.convert_ids_to_tokens(encoded_text)
print(tokens)
```

输出是：

```py
['▁In', 's', 'uff', 'i', 'en', 'c', 't', '▁adoption', '▁of', '▁cord', 'u',
'roy', '▁pants', '▁is', '▁the', '▁reason', '▁this', '▁economy', '▁is', '▁in',
'▁the', '▁dump', 's', '!!!', '</s>']
```

我故意在单词“Insufficient”上犯了一个拼写错误。请注意，子词分词对拼写错误相当脆弱。但至少通过将单词分解为子词，已经解决了 OOV（Out-of-Vocabulary）问题。词汇表似乎也没有“corduroy”这个词的条目，从而证实了它糟糕的时尚感。同时，请注意，有三个连续感叹号有一个独特的标记，这与表示单个感叹号的标记不同。从语义上看，它们确实传达了略微不同的含义。

###### 注意

在大量文本上训练的非常大的模型对拼写错误更稳健。训练集中已经有很多拼写错误。例如，即使是罕见的拼写错误“Insuffienct”在 C4 预训练数据集中也出现了 14 次。更常见的拼写错误“insufficent”出现了超过 1,100 次。较大的模型还可以从上下文中推断出拼写错误的单词。像 BERT 这样的较小模型对拼写错误非常敏感。

如果你使用的是 OpenAI 的模型，你可以使用[tiktoken 库](https://oreil.ly/2QByi)（与社交媒体网站无关）来探索它们的分词方案。

使用 tiktoken，让我们看看 OpenAI 生态系统中可用的不同词汇表：

```py
!pip install tiktoken

import tiktoken
tiktoken.list_encoding_names()
```

输出是：

```py
['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base']
```

像这样的 50K/100K 数字被认为是词汇表大小。OpenAI 没有透露太多关于这些词汇表的信息。他们的文档确实指出，o200k_base 被 GPT-4o 使用，而 cl100k_base 被 GPT-4 使用：

```py
encoding = tiktoken.encoding_for_model("gpt-4")
input_ids = encoding.encode("Insuffienct adoption of corduroy pants is the

reason this economy is in the dumps!!!")
tokens = [encoding.decode_single_token_bytes(token) for token in input_ids]
```

输出是：

```py
[b'Ins', b'uff', b'ien', b'ct', b' adoption', b' of', b' cord', b'uro', b'y',
b' pants', b' is', b' the', b' reason', b' this', b' economy', b' is', b' in',
b' the', b' dumps', b'!!!']
```

如你所见，GPT-4 和 FLAN-T5 使用的分词之间没有太大的区别。

###### 小贴士

对于给定的任务，如果你只在你输入的一部分上观察到 LLM（大型语言模型）的异常行为，检查它们是如何被分词的将是有益的。虽然仅通过分析分词你无法确定地诊断你的问题，但在分析中这通常是有帮助的。根据我的经验，相当数量的 LLM 失败可以归因于文本的分词方式。这在你目标领域与预训练领域不同的情况下尤其如此。

# Tokenization Pipeline

图 3-3 描述了分词器执行的步骤序列。

![Hugging Face 分词器管道](img/dllm_0303.png)

###### 图 3-3\. Hugging Face 分词器管道

如果你使用的是 Hugging Face 的`tokenizers`库，你的输入文本将通过一个[多阶段分词管道](https://oreil.ly/CcOKV)。这个管道由四个组件组成：

+   规范化

+   预分词

+   分词

+   后处理

注意，不同的模型将在这四个组件中执行不同的步骤。

## 规范化

应用了不同类型的规范化包括：

+   将文本转换为小写（如果你使用的是无大小写模型）

+   从字符中去除重音，例如从单词 Peña 中去除

+   Unicode 规范化

让我们看看对 BERT 的无大小写版本应用了什么样的规范化：

```py
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(tokenizer.backend_tokenizer.normalizer.normalize_str(
    'Pédrò pôntificated at üs:-)')
```

输出是：

```py
pedro pontificated at us:-)
```

如我们所见，重音符号已被移除，文本已被转换为小写。

对于更近期的模型，分词器中进行的规范化并不多。

## 预分词

在我们对文本运行分词器之前，我们可以选择执行预分词步骤。如前所述，大多数分词器今天都采用子词分词。一个常见的步骤是首先执行词分词，然后将输出传递给子词分词算法。这一步骤被称为预分词。

相比于许多其他语言，预分词在英语中是一个相对简单的步骤，因为你可以通过在空白处分割文本来获得一个非常强大的基线。需要做出一些异常决策，例如如何处理标点符号、多个空格、数字等。在 Hugging Face 中，正则表达式：

```py
\w+|[^\w\s]+
```

用于在空白处分割。

让我们运行 T5 分词器的预分词步骤：

```py
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-xl")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("I'm starting to

suspect - I am 55 years old!   Time to vist New York?")
```

输出是：

```py
[("▁I'm", (0, 3)),
 ('▁starting', (3, 12)),
 ('▁to', (12, 15)),
 ('▁suspect', (15, 23)),
 ('▁-', (23, 25)),
 ('▁I', (25, 27)),
 ('▁am', (27, 30)),
 ('▁55', (30, 33)),
 ('▁years', (33, 39)),
 ('▁old!', (39, 44)),
 ('▁', (44, 45)),
 ('▁', (45, 46)),
 ('▁Time', (46, 51)),
 ('▁to', (51, 54)),
 ('▁vist', (54, 59)),
 ('▁New', (59, 63)),
 ('▁York?', (63, 69))]
```

除了预标记（或词标记）之外，还返回字符偏移量。

T5 预分词器仅在空白处分割，不会将多个空格合并为一个，也不会在标点符号或数字上分割。其他分词器的行为可能会有很大不同。

## 分词

在可选的预分词步骤之后，实际的分词步骤被执行。这个领域的一些重要算法包括字节对编码（BPE）、字节级 BPE、WordPiece 和单语 LM。分词器包含一组在预训练阶段通过预训练数据集学习到的规则。现在让我们详细地了解一下这些算法。

## 字节对编码

这个算法是最简单且最广泛使用的分词算法。

### 训练阶段

我们取一个训练数据集，运行它通过前面讨论的规范化和预分词步骤，并记录结果输出中的唯一标记及其频率。然后我们构建一个由构成这些标记的唯一字符组成的初始词汇表。从这个初始词汇表开始，我们继续使用*合并*规则添加新的标记。合并规则很简单；我们使用最频繁的连续标记对创建一个新的标记。合并会一直持续到达到所需的词汇量。

让我们用一个例子来探讨这个问题。假设我们的训练数据集由六个单词组成，每个单词只出现一次：

```py
'bat', 'cat', 'cap', 'sap', 'map', 'fan'
```

初始词汇表由以下内容组成：

```py
'b', 'a', 't', 'c', 'p', 's', 'm', 'f', 'n'
```

连续标记对的频率如下：

```py
'ba' - 1, 'at' - 2, 'ca' - 2, 'ap' - 3, 'sa' - 1, 'ma' - 1, 'fa' - 1, 'an' - 1
```

最频繁的配对是“ap”，因此第一个合并规则是将“a”和“p”合并。现在的词汇表是：

```py
'b', 'a', 't', 'c', 'p', 's', 'm', 'f', 'n', 'ap'
```

新的频率如下：

```py
'ba' - 1, 'at' - 2, 'cap' - 1, 'sap' - 1, 'map' - 1, 'fa' - 1, 'an' - 1
```

现在，最频繁的配对是“at”，因此下一个合并规则是将“a”和“t”合并。这个过程一直持续到达到词汇表大小。

### 推理阶段

在分词器训练完成后，它可以用来将文本划分为适当的子词标记，并将文本输入到模型中。这个过程与训练步骤类似。在输入文本进行归一化和预分词后，生成的标记被分解为单个字符，并按顺序应用所有合并规则。应用所有合并规则后的标记是最终的标记，然后这些标记被输入到模型中。

您可以再次打开 GPT-NeoX 的[词汇文件](https://oreil.ly/7JAyY)，并使用 Ctrl+F 搜索“merges”来查看合并规则。如预期，初始合并规则将单个字符彼此连接。在合并列表的末尾，您可以看到像“out”和“comes”这样的较大子词被合并成一个标记。

###### 注意

由于分词器训练集中所有唯一的单个字符都将获得自己的标记，因此只要未来推理过程中看到的所有标记都由训练集中存在的字符组成，就可以保证不会有 OOV 标记。但 Unicode 包含超过一百万个代码点，大约有 150,000 个有效字符，这不会适合 30,000 个词汇量的词汇表。这意味着如果输入文本中包含训练集中没有的字符，该字符将被分配一个 <UNK> 标记。为了解决这个问题，使用了一种名为字节级 BPE 的 BPE 变体。字节级 BPE 从 256 个标记开始，代表所有可以用字节表示的字符。这确保了每个 Unicode 字符都可以通过构成字节标记的串联来编码。因此，它还确保我们永远不会遇到 <UNK> 标记。GPT 系列模型使用这种分词器。

## WordPiece

WordPiece 与 BPE 类似，所以我们只突出它们之间的差异。

WordPiece 不像 BPE 那样使用频率方法，而是使用最大似然方法。数据集中标记对的频率通过单个标记的频率乘积进行归一化。然后合并具有最高得分的配对：

```py
score = freq(a,b)/(freq(a) * freq(b))
```

这意味着如果一个标记对由具有低频率的单个标记组成，它们将被首先合并。

图 3-4 展示了合并优先级以及如何通过个体频率的归一化影响合并顺序。

![WordPiece 分词](img/dllm_0304.png)

###### 图 3-4\. WordPiece 分词

在推理过程中，不使用合并规则。相反，对于输入文本中的每个预分词标记，分词器会在标记中找到词汇表中的最长子词，并在此基础上进行分割。例如，如果标记是“understanding”，并且在这个标记内字典中的最长子词是“understand”，那么它将被分割成“understand”和“ing”。

### 后处理

现在我们已经查看了一些分词器算法，接下来让我们进入管道的下一阶段，即后处理阶段。这是添加特定模型特殊标记的地方。常见的标记包括 [CLS]，这是许多语言模型中使用的分类标记，以及 [SEP]，这是一个用于分隔输入部分的分隔符标记。

## 特殊标记

根据模型的不同，词汇表中会添加一些特殊标记以方便处理。这些标记可以包括：

<PAD>

为了表示填充，以防输入的大小小于最大序列长度。

<EOS>

为了表示序列的结束。生成模型在输出此标记后停止生成。

<UNK>

为了表示一个 OOV（Out-of-Vocabulary）术语。

<TOOL_CALL>, </TOOL_CALL>

这些标记之间的内容用作外部工具（如 API 调用或数据库查询）的输入。

<TOOL_RESULT>, </TOOL_RESULT>

这些标记之间的内容用于表示调用上述工具的结果。

正如我们所见，如果我们的数据是特定领域的，如医疗保健、科学文献等，使用通用分词器进行分词将不会令人满意。Meta 公司的 GALACTICA 模型在其模型中引入了几个特定领域的标记以及特殊的分词规则：

+   [START_REF] 和 [END_REF] 用于封装引用。

+   <WORK> 用于封装构成内部工作记忆的标记，该记忆用于推理和代码生成。

+   数字通过为每个数字分配其自己的标记来处理。

+   [START_SMILES], [START_DNA], [START_AMINO], [END_SMILES], [END_DNA], [END_AMINO] 分别用于蛋白质序列、DNA 序列和氨基酸序列。

如果你正在使用在通用数据上训练的分词器对特定领域的数据（如医疗保健、金融、法律、生物医学等）进行建模，那么压缩比将相对较低，因为特定领域的单词没有自己的标记，将被分割成多个标记。将模型适应特定领域的一种方法是为特定领域的术语学习良好的向量表示。

为了达到这个目的，我们可以向现有的分词器添加新标记，并继续在特定领域数据上预训练模型，以便这些新领域特定标记学习有效的表示。我们将在第七章（Chapter 7）中了解更多关于持续预训练的内容。

现在，让我们看看如何使用 Hugging Face 向词汇表中添加新标记。

考虑以下句子，“CAR-T 细胞和反义寡核苷酸的添加降低了发病率。”FLAN-T5 分词器将此文本分割如下：

```py
from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large",
    device_map="auto")

tokenizer.add_tokens(["CAR-T", "antisense", "oligonucleotides"])
model.resize_token_embeddings(len(tokenizer))
```
