# 第一章. 简介

人工智能不再是科幻小说和反乌托邦好莱坞电影的领域。它正迅速成为人们生活的一个基本组成部分。我们大多数人每天都在与人工智能互动，有时甚至没有意识到这一点。

人工智能当前的进步在很大程度上是由语言模型方面的进步所驱动的。大型语言模型（LLMs）代表了最近时期最重大的技术进步之一，标志着科技界的一个新时代。过去类似的转折点包括计算机的诞生，它引领了数字革命，互联网和万维网的诞生，为超连接的世界奠定了基础，以及智能手机的出现，重塑了人类沟通方式。正在进行的 AI 革命有望产生类似的变革性影响。

LLMs 属于一类被称为生成式 AI 的模型。区分因素是这些模型能够对用户查询生成响应，称为*提示*。生成式 AI 包括生成图像、视频、语音、音乐，当然还有文本的模型。虽然越来越关注将这些所有模态结合到一个单一模型中，但在本书中，我们将坚持语言和 LLMs。

在本章中，我们将介绍语言模型，并定义什么使语言模型成为*大型*。我们将提供 LLMs 的简要历史，将其置于自然语言处理（NLP）领域的位置，并讨论其演变。我们将强调 LLMs 已经在世界上产生的影响，展示关键用例，同时讨论它们的优点和局限性。我们还将介绍 LLM 提示，并展示如何有效地与 LLM 互动，无论是通过用户界面还是通过 API。最后，我们将以一个关于构建*Chat with my PDF*聊天机器人原型的小教程结束本章。然后我们将讨论原型的局限性以及限制其适用于生产用例的因素，从而为本书的其余部分奠定基础。

# 定义大型语言模型（LLMs）

模型是对现实世界概念或现象的一种近似。一个忠实模型将能够对其近似的概念做出预测。语言模型近似人类语言，是通过在大规模文本上训练而构建的，从而赋予它各种语言属性，包括语法（句法）和意义（语义）的方面。

训练语言模型的一种方法是将它教会预测已知文本序列中的下一个标记（这相当于一个词或一个子词，但我们现在将忽略这种区别）。模型在大量此类序列上训练，其*参数*通过迭代更新，以便它能够更好地进行预测。

例如，考虑以下出现在训练数据集中的文本序列：

```py
After a physical altercation with the patrons of a restaurant, Alex was feeling
extremely pleased with himself. He walked out with a swagger and confidence
that betrayed his insecurities. Smiling from ear to ear, he noticed rain drops
grazing his face and proceeded to walk toward the hostel.
```

并且语言模型预测了“…​然后继续走向**_*”*之后的下一个词。

*这个文本序列有大量的有效后续内容。它可能是“建筑”或“避难所”，也可能是“堤坝”或“地下墓穴”。然而，它绝对不可能是“the”或“is”，因为这会违反英语语言的规则。在足够大的文本语料库上进行训练后，模型会学习到“the”和“is”都不是有效的后续内容。因此，你可以看到，像学习预测文本序列中的下一个词这样的简单任务，如何使模型在其参数中学习到语言的语法，以及甚至更复杂的技能。

###### 注意

在实践中，语言模型并不精确地输出文本序列中的下一个标记词或子词。它们输出整个词汇表上的概率分布。（我们将在第三章中探讨这个词汇表是如何定义和构建的）。一个训练良好的模型将对有效的后续内容有高概率，而对于无效的后续内容则有非常低的概率。

图 1-1 简要描述了模型训练过程。模型预测的输出是语言整个词汇表上的概率分布。这与原始序列进行比较，并根据算法更新模型的参数，以便在将来做出更好的预测。这在一个非常大的数据集上重复进行。我们将在接下来的三章中详细描述模型训练过程。

![下一个标记词预测](img/dllm_0101.png)

###### 图 1-1\. 使用下一个标记词预测进行模型训练

模型仅从下一个标记词预测中能学到什么是有极限的吗？这是一个非常重要的问题，决定了 LLM 最终能有多强大。在研究界存在很多分歧，[一些研究人员](https://oreil.ly/sUAcl)认为仅通过下一个标记词预测就足以在模型中实现人类水平的智能，而[其他人指出](https://oreil.ly/7QG-l)这种范式的不足。我们将在整本书中回到这个问题，特别是在第八章，我们将讨论推理等技能。

当代语言模型基于神经网络。用于训练大型语言模型（LLM）的神经网络架构有多种类型，其中最突出的是 Transformer。我们将在第四章中详细了解神经网络、Transformer 和其他架构。

语言模型可以被训练来模拟不仅限于人类语言，还可以是像 Python 或 Java 这样的编程语言。实际上，Transformer 架构和下一个标记词预测目标可以应用于根本不是语言的序列，例如棋步表示、DNA 序列或航班时刻表。

例如，Adam Karvonen 训练了[Chess-GPT](https://oreil.ly/oluZN)，这是一个仅使用便携式游戏表示法（PGN）字符串表示的棋局进行训练的模型。棋局的 PGN 字符串看起来像“1. e4 d5 2. exd5 Qxd5...”等等。即使没有明确提供游戏规则，只需训练模型预测 PGN 序列中的下一个字符，该模型也能学习到游戏规则，包括王车易位、将军和将军胜等走法；甚至能够战胜专家的棋局。这展示了下一个标记预测目标以及构成模型基础的 Transformer 架构的力量。在第四章中，我们将学习如何从头开始训练自己的 Chess-GPT。

另一个这样的例子是[Geneformer](https://oreil.ly/31DXq)，这是一个在数百万个单细胞转录组（单个细胞中 RNA 分子的表示）上训练的模型，可以用于网络生物学的预测，包括疾病进展、基因剂量敏感性和治疗候选者。

因此，我鼓励你在构思语言模型的创新用例时超越人类语言的范畴。如果你有一个可以用有限词汇编码的概念或现象（我们将在第三章中更正式地定义词汇），那么我们可能可以在它上面训练出一个有用的模型。

###### 注意

语言的结构有什么特别之处，使得它适合使用下一个标记预测目标进行建模？或者，语言模型中的“语言”一词只是一个历史巧合，任何标记流都可以使用这种范式进行建模？尽管这在研究社区中仍然是一个[争论的话题](https://oreil.ly/nJiQW)，但直接使用这种范式对语音、视频等进行建模并没有那么有效，这可能表明文本的离散性质以及语言提供结构，无论是人类语言如英语，编程语言如 Python，还是特定领域的代码如 DNA 序列，对于建模成功至关重要。

大约在 2019 年，研究人员意识到，通过增加语言模型的大小（通常通过参数数量来衡量）可以可预测地提高性能，而且似乎没有饱和点。这导致了 Kaplan 等人关于 LLM 缩放定律的研究工作（见下文边栏），该研究推导出一个数学公式，描述了训练模型所需的计算量（以下简称“计算”）、训练数据集大小和模型大小之间的关系。从那时起，公司和组织一直在训练越来越大的模型。

关于何时将语言模型视为“大型”的，没有公认的标准。事实上，随着最大模型变得更大，一些几年前还被指定为 LLM 的模型现在被称为小型语言模型（SLM）。在这本书中，我们将保持宽容，继续将超过十亿参数的所有语言模型称为“大型”。

另一种“大型”语言模型与较小模型不同的方式是它所拥有的涌现能力。这种能力最初由[Wei 等人](https://oreil.ly/RQfii)提出，涌现能力是那些较大模型展现但较小模型不具备的能力。

根据这一理论，对于需要这些能力的任务，较小模型的性能接近随机。然而，当模型大小达到阈值时，性能突然开始随着大小增加而提高。例如，包括多位数算术运算、算术和逻辑推理等。这也表明，当前模型中完全缺失的某些能力可能在未来更大的模型中展现出来。

这些阈值不是绝对的，随着我们在语言建模、数据质量等方面的更多进步，我们可以期待阈值会降低。

###### 注意

[Schaeffer 等人](https://oreil.ly/OXk6I)声称，在特定模型大小阈值处某些任务性能的突然提升只是用于评估性能的指标所造成的伪象。这是因为许多指标不分配部分分数，只奖励完全解决任务，因此模型改进可能无法追踪。另一方面，有人可以争辩说，对于多步算术等任务，部分正确地得到答案与完全错误一样无用。

关于哪些能力是涌现的，研究社区仍在探索。在第五章中，我们将讨论其对选择适合我们所需用例的正确模型的影响。

###### 警告

不幸的是，“涌现性质”这个短语在文献中有多重含义。在一些论文中，这个短语被用来描述模型没有明确训练的能力。在这本书中，我们将坚持[Wei 等人](https://oreil.ly/bkVoj)的定义。

要理解当前 LLM 是如何形成的，回顾它们简短的历史是有益的。由于本书的范围不包括更多历史细节，我们将在本节中提供外部资源的链接，以便进一步阅读。*  *# LLM 的简要历史

要介绍 LLM 的历史，我们需要从 LLM 起源的领域——自然语言处理（NLP）的历史开始。关于 NLP 的更详细历史，请参阅丹尼尔·朱拉夫斯基的奠基性著作[*《语音与语言处理》，第二版*](https://oreil.ly/zzU9R)。

## 早期

这个领域可以追溯到 20 世纪 50 年代，由对*机器翻译*的需求推动，即自动将一种语言翻译成另一种语言的任务。早期主要由符号方法主导；这些是基于[语言理论](https://oreil.ly/ELKSe)的算法，这些理论受到了诺姆·乔姆斯基等语言学家作品的影响。

在 20 世纪 60 年代中期，约瑟夫·魏岑鲍姆发布了 ELIZA，这是一个聊天机器人程序，它使用[正则表达式](https://oreil.ly/rIAWY)在用户的输入上应用模式匹配，并选择响应模板来生成输出。ELIZA 由几个脚本组成，其中最著名的是 DOCTOR，它模拟了一位心理治疗师。这个变体会通过将用户输入重新表述为问题来回答，类似于治疗师的做法。这种重新表述是通过用从输入中匹配到的模式匹配词填充预定义模板来完成的。

例如：

```py
User: 'I am not feeling well'
```

```py
ELIZA: 'Do you believe it is normal to be not feeling well?'
```

你可以尝试与[ELIZA 在线聊天](https://oreil.ly/5g0e_)。即使在 ChatGPT 的时代，ELIZA 也能进行一些令人信服的对话，尽管它仅仅是基于规则的。

基于规则的系统是脆弱的，难以构建，维护起来也是一个噩梦。随着几十年的推移，符号方法的局限性变得越来越明显，而统计方法相对的有效性确保了它们变得更加普遍。NLP 研究员[弗雷德里克·杰利内克](https://oreil.ly/AmtvE)曾著名地打趣说：“每次我解雇一个语言学家，语音识别器的性能就会提高。”

基于机器学习的方法在 1990 年代和 2000 年代得到了更广泛的应用。传统的机器学习依赖于人类驱动的特征工程和特征选择，这是一个识别特征（输入的特征）的过程，这些特征对解决任务具有预测性。这些特征可以是统计性的，如平均词长，或者是语言学的，如词性。要了解更多关于传统统计 NLP 的信息，我推荐阅读 Christopher Manning 的书籍[*统计自然语言处理基础*](https://oreil.ly/MIC70)。

语言学与现代自然语言处理应用开发的相关性是一个有争议的话题。许多大学的 NLP 课程已经完全放弃了与语言学相关的内容。尽管我直接工作中并不直接使用语言学，但我发现我比预期的更多地依赖它们来发展对模型行为的直觉。因此，我推荐 Emily Bender 关于[句法](https://oreil.ly/hWR8S)和[语义](https://oreil.ly/7liiS)的书籍，以了解这个领域的基础知识。

2010 年代见证了深度学习的出现及其对自然语言处理（NLP）的广泛影响。深度学习以其多层神经网络模型为特征，这些模型仅通过原始输入自行学习信息性特征，从而消除了繁琐的特征工程需求。深度学习构成了现代 NLP 和大型语言模型（LLMs）的基础。要深入了解深度学习和神经网络的原则，我推荐阅读[Goodfellow 等人所著的书籍](https://oreil.ly/0gv0D)。对于更多实践性的深度学习培训，我推荐张等人所著的[*《深度学习入门》*](https://oreil.ly/YN_3Y)。

在深度学习的早期，构建特定任务的架构来解决每个任务是一种惯例。所使用的神经网络架构类型包括多层感知器、卷积神经网络、循环神经网络和递归神经网络。要了解更多关于这个 NLP 时代的知识，我推荐 Yoav Goldberg 所著的[*《自然语言处理中的神经网络方法》*](https://oreil.ly/MCOp4)（Springer Cham）。

## 现代大型语言模型（LLM）时代

2017 年，发明了[Transformer 架构](https://oreil.ly/AAuvL)，随后[Howard 等人](https://oreil.ly/E15Yn)等先驱发明了高效的*迁移学习*技术，以及基于 Transformer 的语言模型如[BERT](https://oreil.ly/-Yhwz)。这些进步消除了构建复杂特定任务架构的需求。相反，可以使用相同的 Transformer 模型来训练各种任务。这种新范式将训练步骤分为两个阶段：*预训练*和*微调*。一个初始的大规模预训练步骤用通用语言能力初始化 Transformer 模型。随后，预训练模型可以通过微调过程在更具体的任务上进行训练，例如信息提取或情感检测。我们将在本书中广泛介绍微调。

虽然学术界和开源团体对语言模型做出了关键和重要的贡献，但像 OpenAI、Google、Meta 和 Anthropic 这样的大型科技公司则在训练和发布越来越大的 LLMs 方面处于领先地位。特别是 OpenAI 在推进语言模型技术方面发挥了开创性的作用。现代时代 LLMs 演变的轨迹可以通过 OpenAI 训练的 GPT（生成预训练 Transformer）系列模型每个版本的进步来追踪：

[GPT-1](https://oreil.ly/dFPSE)

这个版本展示了在大规模数据上的无监督预训练，随后进行特定任务的监督微调。

[GPT-2](https://oreil.ly/JL-VO)

这个版本是第一个在大型规模网络数据上训练的模型。这个版本也标志着自然语言提示作为与语言模型交互手段的兴起。它表明，预训练模型可以在没有任何特定任务微调的情况下解决各种任务*零样本*（无需任何示例即可解决问题）。我们将在本章后面详细讨论零样本和提示。

[GPT-3](https://oreil.ly/lIwad)

受到扩展定律的启发，这个模型比 GPT-2 大一百倍，并普及了上下文/少样本学习，其中模型通过在提示中提供几个如何解决给定任务的示例来喂养，而无需微调模型。我们将在本章后面更深入地了解少样本学习。

[GPT-4](https://oreil.ly/gY1HL)

本发布的关键方面是用于使模型更具可控性和遵循模型训练者原则和价值观的*对齐训练*。我们将在第八章中了解对齐训练。

[o1](https://oreil.ly/XJSMN)

这是一个由 OpenAI 发布的新系列模型，专注于提高推理能力。这是第一个专注于扩展推理时间计算规模的模型。我们将在第八章中更详细地讨论推理时间计算。

你可能已经注意到了一个趋势：多年来，该领域一直在经历整合效应，越来越多的 NLP 任务管道部分由单个模型端到端执行。在本书中，我们将指出明显的整合效应，并讨论其对 LLM 未来的影响。

LLM 的历史如果不提及开源对这个领域的影响就不完整。开源模型、数据集、模型架构以及各种开发者库和工具都对这一领域的发展产生了重大影响。本书特别重视开源，提供了对开源 LLM 景观的全面调查，并展示了众多开源模型和数据集。

接下来，让我们探索 LLM 的采用情况及其对社会的影响。

# LLM 的影响

科技界长期以来一直容易受到炒作周期的影响，经历着令人振奋的繁荣和令人沮丧的萧条。最近，我们见证了加密/区块链和 Web3 的繁荣，但这两者都尚未实现其承诺。AI 是否正走向类似的命运？我们有确凿的证据表明它并非如此。

在我的公司 Hudson Labs，我们分析了美国 4000 家最大上市公司季度收益电话会议中的[讨论](https://oreil.ly/_mTAs)，以追踪企业对加密、Web3 和 AI 的采用情况。

我们观察到，有 85 家公司在其收益电话中讨论了 Web3，甚至更少的公司真正在从事相关工作。加密领域表现较好，有 313 家公司讨论了它。与此同时，有 2,195 家公司讨论并采用了 LLMs，这意味着至少 50%的美国最大上市公司正在使用 LLMs 来驱动价值，这对他们来说战略上如此重要，以至于在他们的季度收益电话中被讨论。无论有效与否，企业中 LLMs 的采用已经成为现实。

图 1-2（Web3）显示了在其收益电话中讨论 Web3 的公司随时间的变化。如您所见，Web3 的炒作似乎正在减弱。

![web3](img/dllm_0102.png)

###### 图 1-2\. 在其收益电话中讨论 Web3 的公司随时间的变化

类似地，图 1-3（加密）显示了在其收益电话中讨论加密/区块链的公司随时间的变化。如您所见，在其顶峰时期，只有 5%的公司讨论了加密。

![crypto](img/dllm_0103.png)

###### 图 1-3\. 在其收益电话中讨论加密/区块链的公司随时间的变化

最后，让我们来看看 AI。如前所述，AI 在企业的采用程度是近期其他任何技术趋势都无法比拟的。这一趋势正在加速，如图 1-4（AI 采用）所示，该图显示了分析师在仅今年前两个月收益电话中询问 AI 问题的公司数量。2024 年的急剧上升没有减缓的迹象。

![ai-adoption](img/dllm_0104.png)

###### 图 1-4\. 在其收益电话中在年初前两个月被询问 AI 问题的公司

注意，这些统计数据仅包括生成式 AI/LLMs 的采用，而不包括数据科学/数据分析，其采用在企业的普及程度更高。AI 的采用也不限于科技公司，从房地产公司到保险公司等各行各业的公司都在参与其中。

# 企业中 LLMs 的应用

从同样的分析中，我们观察到了企业中 LLMs（大型语言模型）的关键应用方式：

员工生产力

通过 LLMs 的使用提高员工生产力的主要方式是通过像 GitHub Copilot 这样的编码助手。LLMs 也被广泛用于帮助撰写营销和推广文本以及自动化营销活动。事实上，第一个主要的 LLMs 商业成功故事是营销初创公司[Jasper AI](https://oreil.ly/Byw26)和[Copy.ai](https://oreil.ly/QmhJC)。另一个关键的由 LLMs 驱动的生产力提升是通过公司从异构数据源中抽取的广泛知识库上的问答助手。

报告生成

这些包括总结文档、完成日常文书工作，甚至起草合同。总结用例包括总结财务报告、研究论文，甚至音频或通话记录中的会议纪要。

聊天机器人

LLM 驱动的聊天机器人越来越多地被部署为客户服务代表。它们也被用作公司文档或产品页面的接口。

信息提取和序列标记

近年来，许多企业已经为语言处理任务开发了复杂的 NLP 管道。其中许多管道正在被 LLM 完全或部分取代。这些管道用于解决常见的 NLP 任务，如情感分析、信息提取任务，如实体提取和关系提取，以及序列标记任务，如命名实体识别（NER）。有关 NLP 任务及其详细描述的列表，请参阅[Fabio Chiusano 的博客](https://oreil.ly/_11rN)。

翻译

翻译任务包括将文本从一种语言翻译成另一种语言，以及将文本转换为同一语言的不同形式的任务，例如，将非正式文本转换为正式文本、侮辱性文本转换为礼貌文本等。实时翻译应用如[Erudite 的即时语音翻译](https://oreil.ly/xxENs)承诺让游客尴尬的语言障碍时刻成为过去式。

工作流程

LLM 正逐渐被用于促进工作流程自动化，其中一系列任务可以通过 LLM 驱动的软件系统（称为代理）执行。代理可以与其环境交互（搜索和检索数据、运行代码、连接到其他系统），并且可能自主操作。我们将在第十章（ch10.html#ch10）中更正式地定义代理，并探讨如何构建它们。

# 提示

现在我们已经建立了基础，让我们开始学习如何有效地使用 LLM。

与 LLM 交互的过程称为提示。尽管一些公司试图通过给 LLM 起名字或赋予一个角色来拟人化 LLM，但记住当你与 LLM 交互时，你是在*提示*它们，而不是像与人交谈那样聊天。记住 LLM 是下一词预测器。这意味着它们生成的文本高度依赖于它们所接收的文本，包括输入（称为*提示*）和模型迄今为止生成的输出 token。这统称为*上下文*。

通过在上下文中向 LLM 提供正确的文本，你正在为它提供生成所需输出类型的启动。理想的提示将是这个问题的答案：“什么是最合适的 N 个 token 的前缀，当将其输入到 LLM 中时，将最有可能引导它生成正确的答案？”

到本书写作时为止，语言模型还不够智能，无法让你以与人类交谈的方式提示模型并期望获得最佳结果。随着语言模型随着时间的推移而变得更好，提示可以变得更加像人类的对话。那些还记得搜索引擎早期日子的人可能会记得，通过输入正确的查询形式来有效地使用搜索引擎被认为是一项非同小可的技能，但随着搜索引擎的改进，搜索查询可以变得更加自由形式。

当我开始写这本书时，我征求了目标读者群对希望涵盖主题的意见。我收到了最多的关于提示（prompting）主题的请求，实践者希望了解如何有效地为他们特定的用例创建提示。

提示是现代 LLM 的一个重要方面。实际上，你可能会在任何一个基于 LLM 的项目上花费大量时间迭代提示，这通常被不准确地称为*提示工程*。

###### 小贴士

已有尝试自动优化提示，如[自动提示优化（APO）](https://oreil.ly/SekPA)和[AutoPrompt](https://oreil.ly/upVKC)。我们将在第十三章中进一步讨论这个问题。

管理对提示工程有效性的期望是很重要的。提示并不是解锁隐藏 LLM 能力的魔法咒语。不太可能存在仅通过使用其他公司不知晓的优越提示技术就具有显著优势的公司。另一方面，不遵循基本的提示原则可能会严重损害你 LLM 的性能。

在线有大量的提示教程。我特别推荐[学习提示的指南](https://oreil.ly/CQrzi)。你不需要了解所有的提示技巧就能熟练掌握提示。关于提示，你所需了解的大部分内容可以在几个小时的学习中掌握。更重要的是，与经常使用的 LLM 进行互动，观察它们的输出，并发展对它们行为的直觉。

如果你具有编程经验，我建议从编程的角度来看待提示。在编程中，指令需要明确，不能有歧义。提示的挑战在于它是用自然语言进行的，而自然语言本质上是有歧义的。尽管如此，最好的提示都明确、详细且结构化，几乎不留任何歧义的空间。我们将在第五章和第十三章中学习更多关于提示的细微差别。

###### 注意

一个有趣的事实：语言模型对词序不敏感。这一特性甚至在[早期模型](https://oreil.ly/qI_IZ)如 BERT 中也被观察到。[观察](https://oreil.ly/gtDFg)。例如，向 ChatGPT 或你喜欢的 LLM 提供商提出“我该如何系鞋带？”这样的问题，以混乱的形式，比如说“鞋带我如何系鞋？”ChatGPT 会回答“当然！以下是如何系鞋带的逐步说明：…”，就像你提出了一个直接的问题。

接下来，让我们讨论几种提示模式。

## 零样本提示

这是标准的提示方法，其中你向 LLM 提供指令，以及可选的输入文本。术语*零样本*指的是没有提供如何解决任务的示例或演示。

考虑一个例子，你的任务是评估餐厅评论中表达的情感。通过零样本提示实现这一点，你可以发出以下提示：

> *提示:* 根据给定的段落情感进行分类。输出可以是积极、消极或中立之一。
> 
> 段落: “土豆泥让我想起了我童年的学校午餐。我非常期待再次享用它们。不！”
> 
> 情感：

一个好的零样本提示应该：

+   以精确和明确的方式提供指令。

+   描述输出空间或可接受的输出范围和输出格式。在这个例子中，我们声明输出应该是三个值之一。

+   通过在提示中结束“Sentiment:”来引导它生成正确的文本。通过这样做，我们增加了 LLM 生成情感值作为下一个标记的概率。

模型越好，你越不需要担心这些事情的正确性。

在现实世界的设置中，你的输出格式需要高度可控，以便它能够适应自动化系统。我们将在第五章中讨论更多确保输出可控性的技术。

###### 警告

提示对模型变化敏感。你可能费尽心思构建了一个似乎效果很好的提示，但你可能会注意到，同样的提示对不同的模型不起作用。实际上，如果在此期间底层模型被更新，同样的提示在同一个 API 端点上的性能可能会下降。我们称之为*提示漂移*。对提示进行版本控制是个好主意。

## 少样本提示

在我们的零样本提示示例中，LLM 能够解决任务，而无需解释如何解决它。这是因为任务简单且定义明确。在许多情况下，任务可能不容易用自然语言描述。然后我们可以在提示中添加一些示例，包括输出或输入-输出对。虽然这被称为少样本学习，但语言模型通过这种提示技术并没有以任何方式更新。

这里是一个少样本提示的例子：

> *提示:* 回文是一个在从左到右拼写时字母相同的词
> 
> 或者从右到左。
> 
> 是回文的单词示例：kayak, civic, madam, radar
> 
> 不是回文的单词示例：kayla, civil, merge, moment
> 
> 用*是*或*否*回答问题
> 
> 单词*rominmor*是回文吗？
> 
> 答案：

## 思维链提示

如果你只打算学习一种提示技术，那就让它是思维链（CoT）提示，因为它是现有最具影响力的提示技术之一。

如前所述，LLM 的上下文决定了它预测的下一个 token。因此，我们需要优化上下文中的内容（用户提示+迄今为止生成的输出 token）以最大化 LLM 生成正确未来 token 的概率。一种方法是通过提示 LLM 在生成之前进行*思考*。这促使 LLM 生成获取答案的过程而不是直接生成答案。这可能涉及将输入任务分解为子任务并依次解决。

当 LLM 最终接近生成答案时，它可以依赖一个更相关的上下文，这增加了它生成正确答案的概率。

考虑以下示例：

> *提示:* 解这个方程。34 + 44 + 3 * 23 / 3 * 2。逐步思考。

在接收到“逐步思考”的指令后，LLM 随后将问题分解并按顺序解决每个步骤：

```py
Given equation:

34 + 44 + 3 * 23 / 3 * 2

First, perform multiplication and division from left to right:

= 34 + 44 + (3 * 23 / 3 * 2)
= 34 + 44 + (69 / 3 * 2)
= 34 + 44 + (23 * 2)
= 34 + 44 + 46

Now, perform addition:

= 78 + 46

Finally, complete the addition:

= 124

So, the solution to the equation is 124.
```

###### 小贴士

许多 LLM 在未明确提示的情况下会逐步解决任务。这是因为它们已经被*指令微调*以这种方式解决任务。我们将在第五章和第六章中了解更多关于指令微调的内容。经过指令微调的 LLM 更容易被提示。

在用户界面可访问的 LLM 的情况下，LLM 提供商可能通过一个隐藏的提示（称为系统提示）将 CoT 提示应用于相关用户提示。

我们是否应该在每个提示中添加“逐步思考”的 CoT 指令，就像游戏中的作弊码一样？[Sprague 等人](https://oreil.ly/3zJDC)评估了 CoT 提示在广泛任务中的应用，发现 CoT 主要有助于需要数学或逻辑推理的任务。对于涉及常识推理的任务，他们发现 CoT 的收益有限。对于基于知识的任务，CoT 甚至可能有害。

注意，算术和逻辑推理也可以通过委托给外部工具（如符号求解器和代码解释器）来完成。我们将在第十章中详细讨论这一点。

###### 警告

使用 CoT 提示可以显著增加模型解决任务时生成的 token 数量，从而导致更高的成本。

## 提示链

通常，你的任务需要多个步骤和大量指令。一种处理方式是将所有指令塞入一个单独的提示中。另一种方法是把任务分解成多个子任务，并将提示链式连接，使得一个提示的输出成为另一个提示的输入。我观察到，提示链式连接通常比通过单个提示管理整个任务表现更好。

例如，考虑从表单中提供的文本中提取信息并将输出格式化的任务。如果有缺失或异常值，则需要应用一些特殊的后处理规则。在这种情况下，将任务分成两个提示是一个好习惯，第一个提示执行信息提取，第二个提示处理提取信息的后处理。

## 对抗性提示

你可能会注意到，对于某些查询，LLM 拒绝执行你的请求。这是因为它被专门训练成拒绝某些类型的请求（我们将在第八章中学习如何实现这种行为）。这种我们称之为*对齐训练*的训练，是为了使模型与开发该模型的实体的价值观和偏好保持一致。

例如，直接要求任何合理的 LLM 构建炸弹的指令将导致拒绝。然而，截至今天，对齐训练只提供了一层薄弱的安全保障，因为它可以通过巧妙地提示 LLM 来绕过，这被称为*对抗性提示*。对抗性提示可以手动生成或使用算法生成。这些巧妙措辞的提示会诱使 LLM 生成响应，即使它被训练成不应该这样做。

这些巧妙的提示方案不仅对非法用途有用。在许多情况下，LLM 可能不会以你期望的方式响应，巧妙的提示方案可能有所帮助。这些巧妙的提示方案包括让 LLM 采取特定的角色，甚至直接进行情感勒索（“如果你不正确响应这个查询，许多孩子将遭受痛苦！”）。尽管有一些研究表明，在提示中添加情感可能会带来更好的性能，但没有确凿的、持续的证据表明这对特定模型是普遍有效的。因此，我不建议在生产应用中使用这些方法。

# 通过 API 访问 LLMs

你很可能已经通过 ChatGPT、Gemini 或 Claude 等聊天界面与 LLM 互动过。现在让我们探索如何使用 API 访问它们。我们将以 OpenAI API 为例，访问其 GPT 系列模型。大多数其他专有模型也通过它们的 API 公开了类似的参数。

GPT-4o mini 和 GPT-4o 可以通过 OpenAI 的 Chat Completion API 访问。以下是一个示例：

```py
import os
import openai
openai.api_key = <INSERT YOUR KEY HERE>

output = openai.ChatCompletion.create(
  model="gpt-4o-mini",
  messages=[
    {"role": "system", "content": "You are an expert storywriter."},
    {"role": "user", "content": "Write me a short children's story
    about a dog and an elephant stopping
    being friends with each other."}
  ]
)

print(output.choices[0].message)
```

角色可以是系统、用户、助手或工具。

+   系统角色用于指定一个总体的提示。

+   用户角色指的是用户输入。

+   助手角色指的是模型响应。

+   工具角色用于与外部软件工具交互。

我们将在第十章第十章中更详细地讨论工具。

###### 注意

系统角色和用户角色之间的区别是什么？哪些指令应该放入系统提示，哪些应该放入用户提示？系统提示用于规定大型语言模型（LLM）的高级总体行为，例如：“您是一位精通撰写正式报告的金融专家。”如果您允许用户直接与 LLM 交互，那么系统提示可以用来向 LLM 提供您的指令以及用户请求。我的实验表明，将指令放在系统提示还是用户提示中并不重要。重要的是指令的长度和数量。LLM 通常一次只能处理少量指令。提示中的末尾或开头的指令更有可能被遵守。

这里是 OpenAI 提供的部分参数：

`n`

模型为每个输入必须生成的完成次数。例如，如果我们使用给定的示例中的 n = 5，它将生成五个不同的儿童故事。

###### 小贴士

对于可靠性要求高的任务，我建议生成多个完成项，即 n > 1，然后使用后处理函数（这可能涉及 LLM 调用）来选择最佳项。这是因为 LLM 从概率分布中采样生成的文本，在某些情况下，答案可能只是由于不幸的标记采样而错误/不好。您可能需要在预算限制与这个过程之间进行权衡。

`stop` 和 `max_completion_tokens`

用于限制生成输出的长度。`stop` 允许您指定结束标记，如果生成这些标记，则将停止生成过程。一个示例结束序列是换行符。如果您要求模型遵循特定的输出格式，例如句子的编号列表，那么在输出特定数量的句子后停止生成，只需提供最终数字作为停止参数即可。

`presence_penalty` 和 `frequency_penalty`

用于限制生成输出的重复性。通过惩罚已出现在输出中的标记的概率，我们可以确保模型不会过于重复。这些参数可以在执行更具创造性任务时使用。

`logit_bias`

使用 `logit_bias`，我们可以指定我们想要增加或减少生成概率的标记。

`top_p` 和 `temperature`

这两个参数都与解码策略相关。LLMs 会生成一个标记概率分布，并从这个分布中采样以生成下一个标记。在给定标记概率分布的情况下，有许多策略可以选择生成下一个标记。我们将在第五章中详细讨论这些策略。现在，只需记住，更高的温度设置会产生更多创意和多样化的输出，而更低的温度设置会产生更可预测的输出。这份[速查表](https://oreil.ly/DAa66)为各种用例提供了一些推荐值。

`logprobs`

为每个输出标记提供最可能的标记及其对数概率。OpenAI 将其限制为前 20 个最可能的标记。在后面的章节中，我们将讨论如何利用各种形式的`logprobs`信息。

# LLMs 的优缺点

理解 LLMs（大型语言模型）的优缺点是构建有用 LLMs 应用的关键技能。通过使用本书中的信息和充分的动手实践，你将能够培养这种直觉。一般来说，LLMs 在语言任务方面非常擅长。你几乎不会看到它们犯拼写或语法错误。它们在理解用户指令和意图方面比之前的技术有了巨大的改进。它们在大多数 NLP 任务（如实体和关系抽取以及命名实体识别）上也表现出最先进的性能。而且，它们在生成代码方面特别强大，这也是 LLMs 通过像[GitHub Copilot](https://oreil.ly/qvriE)这样的工具找到其最大成功的地方。

大多数 LLMs 的局限性归结为 LLMs 本身并不足够智能。即使是最先进的模型也遭受着推理方面的重大限制，包括算术推理、逻辑推理和常识推理。（我们将在第八章中更正式地定义推理。）LLMs 也无法坚持事实性，因为它们与现实世界缺乏联系。因此，它们倾向于生成可能与现实世界事实和原则不一致的文本，俗称为*幻觉*。幻觉是 LLMs 的噩梦，也是人们犹豫采用它们的关键原因之一。在第八章中，我们将深入探讨各种应对幻觉和解决推理局限性的方法。

每天都有大量由 LLM 生成的文章被上传到网络上，其中许多文章登上了搜索引擎结果页的顶部。例如，对于“你能融化鸡蛋吗？”这个查询，[谷歌一度显示](https://oreil.ly/ivvv_)“是的，鸡蛋可以被融化”，这是因为一个包含错误答案的 AI 生成的网页。这类文本通常被称为 AI 垃圾。因此，搜索引擎有很强的动力准确检测 AI 生成的文本。请注意，由于 LLM 主要是基于网络文本进行训练的，未来的 LLM 也可能被污染文本所污染。

尽管大型语言模型（LLM）常被用于辅助创意任务，但它们与专业作者的水平相去甚远。目前由 LLM 创作的小说仍不太可能成为畅销书。LLM 生成的文本缺乏人类作者所拥有的纯粹的创新性和唤起人类情感的能力。一旦你阅读了足够多的 LLM 生成的文本，就并不难发现它们。

每个 LLM 生成的文本都有独特的特征，有些比其他更容易被人类察觉。例如，你可能已经注意到 ChatGPT 倾向于过度使用某些词汇，如“深入研究”、“锦缎”、“繁忙”等。ChatGPT 还倾向于生成带有解释性结尾句子的句子，例如：“他吃掉了整个披萨，这表明他很饿。”或者“吸血鬼在一个月内发送了一千条短信，这表明有效利用了数字技术。”然而，[极其困难](https://oreil.ly/4skjI)以 100%的准确率检测到 AI 生成的文本。不良分子也在使用规避技术，例如让另一个 LLM 重新措辞 LLM 生成的文本，以稀释原始 LLM 的特征。

因此，抄袭检测变得更加困难，包括由于 AI 文本检测器不准确而导致学生被[不公平地指控抄袭](https://oreil.ly/hetca)的情况。这些趋势促使全球大学重新思考如何评估学生，减少对论文的依赖。正如夏季月份 ChatGPT 使用量的下降所示，学生是 LLM 产品的一些主要使用者。

虽然已知“深入研究”这类词汇被 LLM 过度使用，但不应依赖单词频率作为检测 LLM 生成文本的手段。作为一个在印度长大、学习印度英语的人，单词“深入研究”在我的词汇中出现的频率比普通西方人高得多，这可以在 ChatGPT 推出之前我的写作和出版物中找到。这些细微差别表明，需要开发更稳健的技术来发现 LLM 生成的文本。

一种有希望的方法是使用句法模板，一系列具有特定词性（POS）标签顺序的标记，通常长度为 5-8 个标记。[Shaib 等人](https://oreil.ly/n_dXJ)表明，即使在使用旨在增加标记多样性的文本生成策略（也称为解码策略，在第五章中详细描述）时，这些模板也会出现在生成的文本中。他们表明，这些模板是在预训练过程的早期阶段学习的。

一个示例模板是：

VBN IN JJ NNS: VBN (过去分词动词) + IN (介词) + JJ (形容词) + NNS (复数名词)。

符合此模板的短语示例包括：

+   从事复杂任务

+   接受过高级技术的训练

+   深陷于深切的情感中

+   沉浸在生动的记忆中

你有没有注意到 LLM 经常使用或过度使用这个模板？

# 构建你的第一个聊天机器人原型

让我们深入细节，开始构建！

在过去几年中，一个健康的库生态系统使得实验和原型化 LLM 应用变得容易得多。事实上，你可以在大约一百行代码内构建一个“用你的 PDF 聊天”问答聊天机器人！

让我们实现一个简单的应用程序，允许用户上传 PDF 文档，并通过聊天界面提问关于 PDF 内容，并接收对话式响应。

此应用的预期工作流程是：

1.  用户可以通过用户界面上传他们选择的 PDF。

1.  应用程序使用 PDF 解析库解析 PDF，并将提取的文本分割成可管理的块。

1.  将块转换为向量形式，称为嵌入。

1.  当用户通过聊天界面提出查询时，查询也被转换为向量形式。

1.  计算查询向量与每个块向量的向量相似度。

1.  获取与最相似的 k 个向量对应的文本。

1.  检索到的文本，连同查询和任何其他附加指令，一起输入到 LLM 中。

1.  LLM 使用给定的信息来生成对用户查询的答案。

1.  响应将在用户界面上显示。

    用户现在可以做出回应（澄清问题、新问题、感谢等）。

1.  在每次对话回合中，将整个对话历史反馈给 LLM。

图 1-5 说明了这个工作流程。

![chatbot-prototype](img/dllm_0105.png)

###### 图 1-5\. 聊天机器人应用程序的工作流程

让我们先安装所需的库。为此设置，我们将使用：

[LangChain](https://oreil.ly/g833p)

这个非常流行的框架使得构建 LLM 应用管道变得容易。

[Gradio](https://oreil.ly/XHqfT)

这个库允许你构建由 LLM 驱动的用户界面。

[Unstructured](https://oreil.ly/sIFEX)

这是一个支持从 PDF 中提取文本的多种方法的 PDF 解析套件。

[Sentence Transformers](https://oreil.ly/UyN1k)

这个库简化了从文本生成嵌入的过程。

[OpenAI](https://oreil.ly/zbroe)

此 API 提供了对 OpenAI 的 GPT 系列模型的访问。

让我们导入所需的库和函数：

```py
!pip install openai langchain gradio unstructured

from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
import gradio as gr
```

接下来，让我们实现 PDF 加载和解析功能。LangChain 支持多个 PDF 解析库。PDF 解析可以通过多种方式执行，包括使用 LLM（大型语言模型）。在这个例子中，我们将选择 Unstructured 库：

```py
loader = UnstructuredPDFLoader(input_file.name)
data = loader.load()
```

`data`变量包含已分割成段落的解析 PDF。我们将每个段落称为一个块。现在，每个块都使用嵌入模型转换为它的向量表示。LangChain 支持广泛的嵌入模型。在这个例子中，我们将使用通过 Hugging Face 平台提供的*all-MiniLM-L6-V2*嵌入模型：

```py
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
```

现在我们已经加载了嵌入模型，我们可以从数据中生成向量并将它们存储在向量数据库中。LangChain 上有几个向量数据库集成。在这个例子中，我们将使用 Chroma，因为它使用起来最简单：

```py
db = Chroma.from_documents(data, embeddings)
```

向量数据库已经准备好了，带有向量！我们可以提出查询并获得响应。例如：

```py
query = "How do I request a refund?"
docs = db.similarity_search(query)
print(docs[0].page_content)
```

此代码检索了 PDF 中与表示用户查询的向量最相似的段落。由于向量编码了文本的意义，这意味着表示相似向量的段落内容与查询内容相似。

注意，不能保证段落包含查询的答案。使用嵌入，我们只能得到与查询相似的文字。匹配的文本不一定包含答案，甚至可能不与回答查询相关。

我们将依赖 LLM 来区分无关和相关的上下文。我们向 LLM 提供查询和检索到的文本，并要求它根据提供的信息回答查询。这个工作流程可以使用 LangChain 中的`chain`实现：

```py
conversational_chain =

ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0.1),
    retriever=pdfsearch.as_retriever(search_kwargs={"k": 3}))
```

我们使用`ConversationalRetrievalChain`，它支持以下工作流程：

1.  如果存在，从先前的对话历史中获取并创建一个独立的查询。

1.  使用选择的检索方法检索与问题最相似的 top-k 个块。

1.  从检索到的块、对话历史、当前用户/响应查询和指令中获取数据，并将其输入到 LLM 中。LLM 生成答案。

我们可以调用链并追加结果到聊天历史中：

```py
output = conversational_chain({'question': query, 'chat_history':

conversational_history})
conversational_history += [(query, output['answer'])]
```

我们的聊天机器人已经准备好了。让我们通过连接用户界面来结束它。我们将使用[Gradio](https://oreil.ly/dzYJv)，这是一个用于构建 LLM 驱动用户界面的轻量级 Python 框架：

```py
with gr.Blocks() as app:

    with gr.Row():

       chatbot = gr.Chatbot(value=[], elem_id='qa_chatbot').style(height=500)

    with gr.Row():
        with gr.Column(scale=0.80):
            textbox = gr.Textbox(
                placeholder="Enter text"
            ).style(container=False)

        with gr.Column(scale=0.10):
        upload_button = gr.UploadButton("Upload a PDF",
          file_types=[".pdf"]).style()
```

我们需要更多的代码来编写等待用户事件的处理器。请参阅书籍的[GitHub 仓库](https://oreil.ly/llm-playbooks)中的完整代码。

最后，我们初始化应用程序：

```py
if __name__ == "__main__":
    app.launch()
```

我们的聊天机器人应用程序已经准备好了！

###### 注意

为什么我们不能将整个 PDF 直接喂给 LLM，而不是将其拆分成块并只检索相关信息？这取决于 LLM 支持的最大有效上下文长度，这限制了它可以接受的输入大小。较大的模型支持足够长的上下文长度，可以容纳多个 PDF 作为输入，在这种情况下，你可能根本不需要执行分块和嵌入过程。

# 从原型到生产

构建 LLM 应用程序真的这么容易吗？不幸的是，并非如此。我们已经构建了一个原型，而且是一个相当不错的原型。对于许多非关键用例，该应用程序的性能可能甚至足够。然而，大量用例需要准确性和可靠性保证，而该应用程序无法满足这些要求。本书旨在填补原型与生产之间的差距。

在原型教程中，我们将 LLM 视为一个黑盒。但如果你正在使用 LLM 构建严肃的应用程序，了解底层发生的事情很重要，即使你可能永远不会自己训练 LLM。因此，在第二章、3 章和 4 章中，我们将逐一介绍构成 LLM 的各个组成部分，并展示它们是如何被训练的。对 LLM 的构成和训练过程有深入的理解，在调试故障模式时将非常有用。

在教程中，我们使用了来自 OpenAI 的专有 LLM，并没有过多考虑它是否是适用于该应用的理想 LLM。如今，数百甚至数千种 LLM 可供商业使用。在第五章中，我们将探讨 LLM 的生态系统，涵盖开源和专有模型，以及模型之间差异的相关维度，以及如何选择满足特定用例标准的正确模型。例如，我们 PDF 聊天机器人的一个标准可能是在严格的预算限制下运行。我们将学习如何评估 LLM，评估它们在特定用例中的局限性和能力，开发评估指标和基准数据集，并了解自动评估和人工评估中可能遇到的陷阱。

如果我们打算上传到 PDF 聊天机器人的 PDF 属于 LLM 似乎不太擅长的专业领域怎么办？如果 LLM 无法遵循用户查询中的指示怎么办？我们可能需要通过在专业领域的数据上微调模型来更新模型的参数。在第六章中，我们将介绍模型微调，了解可能有用的情况，并演示如何构建微调数据集。

标准微调可能不适合我们的目的。也许它太昂贵或效果不佳。在第七章中，我们将了解像参数高效微调这样的技术，它只更新模型参数的一小部分。

我们可能会注意到我们的聊天机器人正在进行幻觉，或者它因为推理错误而难以回答问题。在第八章中，我们将讨论检测和减轻幻觉的方法，以及增强推理能力的方法，包括各种推理时计算技术。

一个生产级别的 PDF 聊天机器人需要满足许多非功能性需求，包括最小化延迟（用户等待模型响应的时间）和成本。在第九章中，我们将讨论推理优化的技术，包括缓存、蒸馏和量化。

我们可能希望通过将 LLM 连接到代码解释器、数据库和 API 来扩展聊天机器人的功能。我们可能还希望聊天机器人能够回答需要分解成多个步骤的复杂查询。在第十章中，我们将探讨如何将 LLM 与外部工具和数据源接口，并使 LLM 能够分解任务、做出自主决策并与环境接口。

在教程中，我们演示了一种基本的解析、分块和嵌入文档的方法。但在使用过程中，我们可能会注意到向量相似度度量可能无效，并且经常返回不相关的文档块。或者检索到的块不包含回答查询所需的所有信息。在第十一章中，我们将更详细地探讨嵌入，并学习如何微调我们自己的嵌入。我们还将展示如何更有效地分块数据。

PDF 聊天机器人遵循一种称为检索增强生成（RAG）的范式。RAG 指的是 LLM 连接到外部数据源的系统，例如我们聊天机器人用例中用户上传的 PDF。在第十二章中，我们将定义一个全面的 RAG 管道，并学习如何构建健壮的 RAG 系统。

最后，在第十三章中，我们将讨论开发 LLM 应用程序的设计模式和编程范式。

这些主题以及更多内容将在本书的其余部分进行讨论。我很高兴与您一起踏上这段旅程，希望为您提供开发生产级 LLM 应用程序所需的工具、技术和直觉！

# 摘要

在本章中，我们介绍了语言模型，提供了简要的历史背景，并讨论了它们对世界产生的影响。我们展示了如何使用各种提示技术有效地与模型互动。我们还概述了语言模型的优点和局限性。我们展示了构建原型应用是多么容易，并强调了将它们推向生产所涉及到的挑战。在下一章中，我们将通过介绍构成大型语言模型（LLM）的要素，开始我们的 LLM 世界之旅。*
