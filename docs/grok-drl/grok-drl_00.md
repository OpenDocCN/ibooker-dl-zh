# 前置材料

## 前言

那么，关于强化学习，这里有一些事情。由于多种原因，它既难以学习，也难以教授。首先，它是一个非常技术性的主题。它背后有大量的数学和理论。在不陷入其中时传达正确数量的背景知识本身就是一项挑战。

其次，强化学习鼓励了一种概念错误。强化学习既是思考决策问题的一种方式，也是解决这些问题的工具集。我所说的“一种方式”是指强化学习提供了一个决策框架：它讨论了状态和强化信号等细节。我所说的“一套工具”是指当我们讨论强化学习时，我们会发现自己使用诸如*马尔可夫决策过程*和*贝尔曼更新*等术语。将思维方式与我们对这种思维方式所使用的数学工具混淆是极其容易的。

最后，强化学习可以用多种方式实现。因为强化学习是一种思维方式，我们可以通过尝试以非常抽象的方式实现框架来讨论它，或者将其基于代码，或者，就事论事，基于神经元。一个人决定使用的底层使得这两个困难变得更加具有挑战性——这把我们带到了深度强化学习。

专注于深度强化学习很好地将这些问题一次性放大。关于强化学习（RL）和深度神经网络都有背景知识。两者都值得单独研究，并且以完全不同的方式发展。在开发工具的背景下解释这两者的方法并不容易。此外，不要忘记，理解强化学习不仅需要理解工具及其在深度网络中的实现，还需要理解关于强化学习的思维方式；否则，你无法将所学内容推广到直接研究的例子之外。再次强调，教授强化学习是困难的，而且有那么多错误的方法可以用来教授深度强化学习——这把我们带到了米格尔·莫雷莱斯和这本书。

这本书组织得非常好。它用技术但清晰的语言解释了机器学习是什么，深度学习是什么，以及强化学习是什么。它使读者能够理解该领域的更大背景，以及你可以使用深度强化学习的技巧做什么，同时也理解了机器学习（ML）、强化学习（RL）和深度强化学习所呈现的思维方式。它清晰简洁。因此，它既是一本学习指南，也是一本参考书，至少对我来说，它还是一些灵感的来源。

我对这一切并不感到惊讶。我已经认识米格尔好几年了。他从学习机器学习课程到教授它们。他作为佐治亚理工学院在线硕士学位课程《强化学习与决策制定》的主要助教，比我能够数得上的学期还要多。在那段时间里，他影响了成千上万的学生。我看着他作为一个从业者、一个研究人员和一个教育者的成长。他帮助使佐治亚理工学院的 RL 课程比最初更好，而且在我写这篇文章的时候，还在继续使强化学习的理解对学生来说更加深入。他是一个天生的教师。

这篇文章反映了他的才华。我很高兴能够与他合作，并且我很高兴他决定写这本书。享受吧。我想你会学到很多东西。我自己也学到了一些东西。

查尔斯·伊斯贝尔，小

教授和约翰·P·伊莱，小

计算机学院

乔治亚理工学院

## 前言

强化学习是一个充满活力、有可能对人类历史产生深远影响的领域。几种技术已经影响了我们世界的历史，并改变了人类历史的进程，从火、轮子、电到互联网。每一次技术发现都以累积的方式推动下一次发现。没有电，个人电脑就不会存在；没有它，互联网就不会存在；没有它，搜索引擎就不会存在。

对我来说，强化学习和人工智能最令人兴奋的方面，与其说是仅仅在我们身边有其他智能实体，这确实很令人兴奋，不如说是那之后的事情。我相信强化学习作为一个用于自主优化特定任务的稳健框架，有可能改变世界。除了任务自动化之外，智能机器的创造可能会推动我们对人类智能的理解达到前所未有的地方。可以说，如果你能够确定地找到每个问题的最优决策，你很可能就理解了找到这些最优决策的算法。我有一种感觉，通过创造智能实体，人类可以成为更加智能的存在。

但我们离这个目标还远着呢，要实现这些宏伟的梦想，我们需要更多的智慧。强化学习不仅处于起步阶段，而且已经处于这种状态有一段时间了，所以前方还有许多工作要做。我写这本书的原因是让更多的人理解深度强化学习，以及一般的强化学习，并帮助你做出贡献。

尽管强化学习框架直观易懂，但大多数资源对于新手来说都难以理解。我的目标不是写一本只提供代码示例的书，更不是创建一本教授强化学习理论的资源。相反，我的目标是创建一个能够弥合理论与实践之间差距的资源。正如你很快就会看到的，我不回避方程式；如果你想要深入理解一个研究领域，它们是必不可少的。即使你的目标是实际应用，构建高质量的强化学习解决方案，你仍然需要这个理论基础。然而，我也不会仅仅依赖于方程式，因为并不是所有对强化学习感兴趣的人都喜欢数学。有些人更习惯于代码和具体的例子，所以这本书提供了这个精彩领域的实际应用方面。

在这个为期三年的项目中，我大部分的努力都投入到了弥合这个差距上；我不回避直观地解释理论，也不只是简单地列出代码示例。我两者都做，而且非常注重细节。对于那些理解教科书和讲座有困难的人来说，更容易掌握顶尖研究者使用的词汇：为什么用这些特定的词汇，而不是其他词汇。而对于那些知道这些词汇并喜欢阅读方程式，但难以看到这些方程式在代码中的表现形式以及它们如何连接的人来说，更容易理解强化学习的实际应用方面。

最后，我希望你喜欢这份工作，更重要的是，它确实实现了你的目标。我希望你能深入理解深度强化学习，并能够回馈并贡献于这个我逐渐热爱的出色社区。正如我之前提到的，如果不是因为一系列相对较新的技术创新，你不会读到这本书。但书后的世界取决于你，所以勇敢地去影响世界吧。

## 致谢

我想感谢乔治亚理工学院的那些人，他们承担了风险，为世界上任何人都提供了第一个在线计算机科学硕士学位，让他们能够获得高质量的毕业教育。如果不是那些使这一切成为可能的人，我可能就不会写这本书。

我想感谢查尔斯·伊斯贝尔教授和迈克尔·利特曼教授，他们共同开设了一门优秀的强化学习课程。我对伊斯贝尔院长特别感激，他给了我很多成长和学习强化学习的机会。此外，我教授强化学习的方式——通过将问题分为三种类型的反馈——是从利特曼教授那里学到的。我非常感激能够从他们那里接受指导。

我要感谢乔治亚理工学院 CS 7642 课程的充满活力的教学团队，他们一起努力探讨如何帮助学生学得更多并享受与我们在一起的时光。特别感谢 Tim Bail、Pushkar Kolhe、Chris Serrano、Farrukh Rahman、Vahe Hagopian、Quinn Lee、Taka Hasegawa、Tianhang Zhu 和 Don Jacob。你们都是如此优秀的队友。我还要感谢那些之前对该课程做出重大贡献的人。我从我们的互动中学到了很多：Alec Feuerstein、Valkyrie Felso、Adrien Ecoffet、Kaushik Subramanian 和 Ashley Edwards。我还要感谢我们的学生，是他们的提问帮助我识别了那些试图学习强化学习的人的知识盲点。这本书是为你们写的。特别感谢那位推荐我给 Manning 写这本书的匿名学生；我仍然不知道你是谁，但你很清楚你是谁。谢谢。

我要感谢 Lockheed Martin 团队在我写这本书期间的所有反馈和互动。特别感谢 Chris Aasted、Julia Kwok、Taylor Lopez 和 John Haddon。John 是第一个审阅我早期草稿的人，他的反馈帮助我将写作提升到下一个层次。

我要感谢 Manning 团队为使这本书成为现实提供的框架。感谢 Brian Sawyer 主动联系并打开大门；感谢 Bert Bates 在早期设定方向并帮助我专注于教学；感谢 Candace West 帮助我从零开始有所成就；感谢 Susanna Kline 在我忙碌时帮助我加快进度；感谢 Jennifer Stout 在我冲过终点线时为我加油；感谢 Rebecca Rinehart 处理紧急事务；感谢 Al Krinker 为我提供可操作的反馈并帮助我区分信号和噪音；感谢 Matko Hrvatin 跟进 MEAP 发布并给我施加额外的写作压力；感谢 Candace Gillhoolley 将这本书推广出去；感谢 Stjepan Jurekovic´帮助我推广；感谢 Ivan Martinovic 为改进文本提供必要的反馈；感谢 Lori Weidert 两次将书籍调整为生产就绪；感谢 Jennifer Houle 在设计变更上保持温和；感谢 Katie Petito 耐心地处理细节；感谢 Katie Tennant 进行细致和最终的润色；以及任何我遗漏的人，或者那些在幕后使这本书成为现实的人。我知道还有更多：感谢你们的辛勤工作。

向所有审稿人——Al Rahimi、Alain Couniot、Alberto Ciarlanti、David Finton、Doniyor Ulmasov、Edisson Reinozo、Ezra Joel Schroeder、Hank Meisse、Hao Liu、Ike Okonkwo、Jie Mei、Julien Pohie、Kim Falk Jørgensen、Marc-Philippe Huget、Michael Haller、Michel Klomp、Nacho Ormeño、Rob Pacheco、Sebastian Maier、Sebastian Zaba、Swaminathan Subramanian、Tyler Kowallis、Ursin Stauss 和 Xiaohu Zhu——表示感谢，你们的建议帮助使这本书变得更好。

我要感谢 Udacity 的团队，他们让我有机会与他们学生分享我对这个领域的热情，并为他们深度强化学习纳米学位录制演员-评论家讲座。特别感谢 Alexis Cook、Mat Leonard 和 Luis Serrano。

我要感谢强化学习社区帮助我澄清文本并提高我的理解。特别感谢 David Silver、Sergey Levine、Hado van Hasselt、Pascal Poupart、John Schulman、Pieter Abbeel、Chelsea Finn、Vlad Mnih，感谢他们的讲座；Rich Sutton 将这个领域的黄金副本集中在一个地方（他的教科书）；以及 James MacGlashan 和 Joshua Achiam，感谢他们的代码库、在线资源和在我不知道去哪里寻找问题的答案时的指导。我要感谢 David Ha，他让我了解到下一步该去哪里。

特别感谢 Silvia Mora，她帮助使本书中的所有图表都显得可接受，并在我几乎承担的每一个副项目中给予我帮助。

最后，我要感谢我的家人，他们是我整个项目的基础。我知道写一本书是一个挑战，然后我学会了。但我的妻子和孩子始终在那里，在周末每隔两小时左右等待我的 15 分钟休息时间。感谢 Solo，在本书中途照亮了我的生活。感谢 Rosie，分享你的爱和美丽，感谢 Danelle，我亲爱的妻子，因为你的一切和你所做的一切。你是这个有趣的游戏（生活）中的完美队友。我很高兴我找到了你。

## 关于这本书

*掌握深度强化学习*架起了深度强化学习理论与实践之间的桥梁。本书的目标读者是熟悉机器学习技术、希望学习强化学习的人。本书从深度强化学习的基础开始。然后深入探讨了深度强化学习的算法和技术。最后，它提供了一项具有潜在影响力的高级技术的概述。

### 谁应该阅读这本书

对于一个研究领域感到舒适、熟悉 Python 代码、偶尔有一些数学知识、大量的直观解释和有趣且具体的例子来推动学习的人来说，这本书会很有趣。然而，任何只熟悉 Python 的人，只要对学习有足够的兴趣，都能从中获得很多。尽管假设了基本的深度学习知识，但本书提供了关于神经网络、反向传播和相关技术的简要复习。总之，这本书是自包含的，任何想要尝试与 AI 代理互动并掌握深度强化学习的人都可以使用这本书来实现目标。

### 本书是如何组织的：一个路线图

本书分为两部分，共 13 章。

在第一部分，第一章介绍了深度强化学习领域，并为未来的旅程设定了期望。第二章介绍了一个框架，用于设计 RL 代理能够理解的问题。第三章包含了当代理知道世界动态时解决 RL 问题的算法细节。第四章包含了当代理不知道世界动态时解决简单 RL 问题的算法细节。第五章介绍了解决预测问题的方法，这是高级 RL 方法的基础。

在第二部分，第六章介绍了解决控制问题的方法，这些方法是纯粹通过试错学习来优化策略的。第七章教授了更高级的 RL 方法，包括使用规划来提高样本效率的方法。第八章通过实现一个简单的使用神经网络进行函数近似的 RL 算法，介绍了在 RL 中使用函数近似的方法。第九章深入探讨了使用函数近似解决强化学习问题的更高级技术。第十章教授了迄今为止介绍的方法的一些最佳改进技术。第十一章介绍了一种使用深度学习模型与 RL 结合的不同技术，该技术在多个深度 RL 基准测试中证明了达到最先进性能的能力。第十二章深入探讨了深度 RL 的更高级方法、最先进的算法以及常用于解决现实世界问题的技术。第十三章概述了 RL 的高级研究领域，这些领域表明了向通用人工智能进步的最佳路径。

### 关于代码

本书包含了许多源代码示例，既有标题为“我说 Python”的框中的示例，也有文本中的示例。源代码以固定宽度字体格式化，如这样，以将其与普通文本区分开来，并具有语法高亮，以便更容易阅读。

在许多情况下，原始源代码已经被重新格式化；我们添加了换行符，重命名了变量，并重新调整了缩进以适应书中的可用页面空间。在极少数情况下，即使这样也不够，代码中包含 Python 中的行续符（\），以指示语句将在下一行继续。

此外，源代码中的注释通常已从框中移除，并在文本中描述了代码。代码注释指出了重要概念。

本书中的示例代码可以从 Manning 网站[`www.manning.com/books/grokking-deep-reinforcement-learning`](https://www.manning.com/books/grokking-deep-reinforcement-learning)和 GitHub[`github.com/mimoralea/gdrl`](https://github.com/mimoralea/gdrl)下载。

### liveBook 讨论论坛

购买 *掌握深度强化学习* 包括免费访问由 Manning Publications 运营的私人网络论坛，您可以在论坛上对书籍发表评论、提出技术问题，并从作者和其他用户那里获得帮助。要访问论坛，请访问 [`livebook.manning.com/#!/book/grokking-deep-reinforcement-learning/discussion`](https://livebook.manning.com/#!/book/grokking-deep-reinforcement-learning/discussion)。您还可以在 [`livebook.manning.com/#!/discussion`](https://livebook.manning.com/discussion) 了解更多关于 Manning 的论坛和行为准则。

Manning 对我们读者的承诺是提供一个场所，让读者之间以及读者与作者之间可以进行有意义的对话。这不是对作者参与特定数量活动的承诺，作者对论坛的贡献仍然是自愿的（且未付费）。我们建议您尝试向他提出一些挑战性的问题，以免他的兴趣转移！只要书籍在印刷中，论坛和先前讨论的存档将从出版社的网站提供访问。

## 关于作者

*米格尔·莫雷莱斯* 在科罗拉多州丹佛的洛克希德·马丁公司，导弹和火控，自主系统部门从事强化学习工作。他是乔治亚理工学院强化学习和决策课程的部分时间制教学助理。米格尔曾作为机器学习项目审稿人、自动驾驶汽车纳米学位导师和深度强化学习纳米学位内容开发者为 Udacity 工作。他从乔治亚理工学院计算机科学硕士毕业，专攻交互智能。
