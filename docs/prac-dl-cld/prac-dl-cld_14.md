# 第十四章：使用 TensorFlow Object Detection API 构建完美的猫定位器应用

鲍勃经常受到附近的流浪猫的访问。这些访问导致了不太愉快的结果。你看，鲍勃有一个相当大的花园，他非常用心地照料。然而，这只毛茸茸的小家伙每天晚上都会来到他的花园，并开始咬一堆植物。几个月的辛勤工作在一夜之间被摧毁。显然对这种情况不满意，鲍勃渴望采取一些行动。

发挥他内心的宠物侦探艾斯·文图拉（Ace Ventura）的精神，他试图在夜间保持清醒，赶走猫，但显然，这在长期内是不可持续的。毕竟，红牛也有其限制。达到一个临界点后，他决定使用核选项：将 AI 与他的洒水系统结合起来，从而真正“打开水管”对付猫。

在他广阔的花园里，他设置了一个可以跟踪猫的运动并打开最近的洒水器来吓跑猫的摄像头。他周围有一部旧手机，他所需要的就是一种实时确定猫位置的方法，这样他就可以相应地控制哪个洒水器触发。

希望经过几次不受欢迎的浴，如图 14-1 所示，猫会不再去鲍勃的花园里捣乱。

![建立一个 AI 猫喷水系统，就像这个 Havahart Spray Away Motion Detector](img/00166.jpeg)

###### 图 14-1\. 建立一个 AI 猫喷水系统，就像这个 Havahart Spray Away Motion Detector

###### 提示

敏锐的观察者可能已经注意到，我们试图在二维空间中找到猫的位置，而猫本身存在于三维空间中。我们可以做出一些假设来简化在真实世界坐标中定位猫的问题，假设摄像头始终处于固定位置。为了确定猫的距离，我们可以使用一个平均猫的大小来测量它在图片上的表观大小，这些图片是在距离摄像头的固定距离处定期拍摄的。例如，一个平均猫在距离摄像头两英尺处可能看起来有一个 300 像素的边界框高度，而当它离摄像头镜头三英尺时，它可能占据 250 像素。

在这一章中，我们帮助鲍勃制作他的猫探测器。（动物权益倡导者请注意：在制作本章时没有伤害到任何猫。）我们沿途回答以下问题：

+   有哪些不同类型的计算机视觉任务？

+   如何重用为现有对象类别预训练的模型？

+   我能否在不编写任何代码的情况下训练一个物体检测器模型？

+   我想要更精细的控制，以获得更高的准确性和速度。如何训练一个自定义物体检测器？

# 计算机视觉任务的类型

在前面的大部分章节中，我们基本上看了一种问题：物体分类。在这种情况下，我们找出了图像中是否包含某个特定类别的对象。在鲍勃的情况下，除了知道摄像头是否看到了一只猫之外，还需要确切地知道猫的位置，以触发最近的洒水器。确定图像中对象的位置是一种不同类型的问题：*物体检测*。在我们深入研究物体检测之前，让我们看看频繁的计算机视觉任务的种类以及它们试图回答的问题。

## 分类

在本书中，我们已经看过几个分类的例子。简单来说，分类的任务是将图像分配给图像中存在的对象类别。它回答了这样一个问题：“图像中是否有 X 类对象？”分类是最基本的计算机视觉任务之一。在一张图像中，可能有多个类别的对象，称为多类别分类。在这种情况下，一张图像的标签将是图像包含的所有对象类别的列表。

## 定位

分类的缺点是它不告诉我们图像中特定对象的位置在哪里，它有多大或多小，或者有多少个对象。它只告诉我们特定类别的对象存在于图像中的某个地方。*定位*，又称*带定位的分类*，可以告诉我们图像中有哪些类别以及它们在图像中的位置。这里的关键词是*类别*，而不是个别对象，因为定位每个类别只给出一个边界框。就像分类一样，它无法回答“这个图像中有多少个 X 类对象？”的问题。

正如您马上会看到的，只有在可以保证每个类别只有一个实例时，定位才能正常工作。如果一个类别有多个实例，一个边界框可能包含该类别的一些或所有对象（取决于概率）。但每个类别只有一个边界框似乎相当受限制，不是吗？

## 检测

当我们在同一图像中有多个属于多个类别的对象时，定位就不够了。对象检测会为每个类的每个实例提供一个边界矩形，对于所有类别。它还会告诉我们每个检测到的边界框中的对象的类别。例如，在自动驾驶汽车的情况下，对象检测会为汽车看到的每辆车、每个人、每个路灯等返回一个边界框。由于这种能力，我们也可以将其用于需要计数的应用程序。例如，计算人群中的人数。

与定位不同，检测在图像中某个类的实例数量上没有限制。这就是为什么它在现实世界的应用中被使用的原因。

###### 警告

通常人们在说“对象定位”时实际上是指“对象检测”。重要的是要注意这两者之间的区别。

## 分割

对象分割是将类标签分配给整个图像中的每个像素的任务。我们使用蜡笔给各种对象上色的儿童涂色书是一个现实世界的类比。考虑到图像中的每个像素都被分配一个类，这是一个高度计算密集的任务。虽然对象定位和检测提供边界框，分割产生像素组，也被称为*掩码*。与检测相比，分割在对象的边界方面显然更加精确。例如，一个可以实时改变用户头发颜色的化妆应用程序会使用分割。根据掩码的类型，分割有不同的风格。

### 语义分割

给定一幅图像，*语义分割*为每个类分配一个掩码。如果有多个相同类别的对象，则它们都分配给同一个掩码。同一类别的实例之间没有区分。就像定位一样，语义分割无法计数。

### 实例级分割

给定一幅图像，*实例级分割*识别每个类别每个实例占据的区域。同一类别的不同实例将被唯一分割。

表 14-1 列出了不同的计算机视觉任务。

表 14-1。使用图像 ID [120524](https://oreil.ly/ieJ2d) 从 MS COCO 数据集说明的计算机视觉任务类型

| **任务** | **图像** | **通俗地说** | **输出** |
| --- | --- | --- | --- |
| 对象分类 | ![](img/00118.jpeg) | 这个图像中有一只羊吗？ | 类概率 |
| 对象定位 | ![](img/00015.jpeg) | 图像中是否有“一只”羊，它在哪里？ | 边界框和类概率 |
| 对象检测 | ![](img/00147.jpeg) | 这个图像中的“所有”对象是什么？ | 边界框，类概率，类 ID 预测 |
| 语义分割 | ![](img/00325.jpeg) | 这幅图像中哪些像素属于不同的类别；例如，“羊”，“狗”，“人”？ | 每个类别一个掩码 |
| 实例级别分割 | ![](img/00064.jpeg) | 这幅图像中每个类别的每个实例属于哪些像素；例如，“羊”，“狗”，“人”？ | 每个类别的每个实例一个掩码 |

# 对象检测方法

根据情景、需求和技术知识，有几种方法可以实现在应用程序中获取对象检测功能。表 14-2 介绍了一些方法。

表 14-2\. 对象检测的不同方法及其权衡

|  | **自定义类别** | **工作量** | **云端或本地** | **优缺点** |
| --- | --- | --- | --- | --- |
| 基于云的对象检测 API | 否 | <5 分钟 | 仅限云端 | + 成千上万种类别+ 处于技术前沿+ 快速设置+ 可扩展 API– 无定制化– 由于网络而产生的延迟 |
| 预训练模型 | 否 | <15 分钟 | 本地 | + ~100 个类别+ 选择适合速度和准确性需求的模型+ 可在边缘运行– 无定制化 |
| 基于云的模型训练 | 是 | <20 分钟 | 云端和本地 | + 基于 GUI，无需编码进行训练+ 可定制性+ 选择强大模型（用于云端）和高效模型（用于边缘）+ 可扩展 API– 使用迁移学习，可能不允许完整模型重新训练 |
| 自定义训练模型 | 是 | 4 小时至 2 天 | 本地 | + 高度可定制+ 提供各种速度和准确性要求的模型– 耗时且复杂 |

值得注意的是，“云”标记的选项已经为可扩展性而设计。同时，标记为“本地”推断的选项也可以部署在云端。为了实现高规模，我们可以将它们部署在云端，类似于我们在第九章中为分类模型所做的方式。

# 调用预构建的基于云的对象检测 API

正如我们在第八章中已经看到的，调用基于云的 API 相对简单。该过程涉及设置账户、获取 API 密钥、阅读文档以及编写 REST API 客户端。为了简化这个过程，许多服务提供基于 Python（和其他语言）的 SDK。主要的基于云的对象检测 API 提供商是 Google 的 Vision AI（图 14-2）和微软的认知服务。

使用基于云的 API 的主要优势在于它们具有可扩展性，并支持识别成千上万种对象类别。这些云巨头使用大型专有数据集构建了他们的模型，这些数据集不断增长，导致了非常丰富的分类法。考虑到最大的公共数据集中带有对象检测标签的类别数量通常只有几百个，目前还没有非专有解决方案能够检测成千上万种类别。

基于云的 API 的主要限制当然是由于网络请求而产生的延迟。无法通过这种延迟实现实时体验。在下一节中，我们将看看如何在没有任何数据或训练的情况下实现实时体验。

![在 Google 的 Vision AI API 上运行熟悉的照片以获取对象检测结果](img/00328.jpeg)

###### 图 14-2\. 在 Google 的 Vision AI API 上运行熟悉的照片以获取对象检测结果

# 重用预训练模型

在这一部分，我们将探讨如何轻松地在预训练模型上在手机上运行目标检测器。希望您已经熟悉了在之前的一些章节中在移动设备上运行神经网络的方法。在书的 GitHub 网站上引用了在 iOS 和 Android 上运行目标检测模型的代码（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）位于*code/chapter-14*。更改功能只是简单地用新的*.tflite*模型文件替换现有的模型文件。

在之前的章节中，我们经常使用 MobileNet 来进行分类任务。尽管 MobileNet 系列，包括 MobileNetV2（2018）和 MobileNetV3（2019）本身只是分类网络，但它们可以作为 SSD MobileNetV2 等目标检测架构的骨干，我们在本章中使用。

## 获取模型

揭示幕后魔术的最佳方法是深入研究一个模型。我们需要获取我们的手上的[TensorFlow Models 仓库](https://oreil.ly/9CsHM)，其中包含 50 多个深度学习任务的模型，包括音频分类、文本摘要和目标检测。这个仓库包含模型以及我们在本章中使用的实用脚本。话不多说，让我们在我们的机器上克隆这个仓库：

```py
$ git clone https://github.com/tensorflow/models.git && cd models/research
```

然后，我们更新`PYTHONPATH`以使所有脚本对 Python 可见：

```py
$ export PYTHONPATH="${PYTHONPATH}:`pwd`:`pwd`/slim"
```

接下来，我们将从我们的 GitHub 仓库（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）复制协议缓冲区（protobuf）编译器命令脚本到我们当前目录，以使.proto 文件对 Python 可用：

```py
$ cp {path_to_book_github_repo}/code/chapter-14/add_protoc.sh . && \
chmod +x add_protoc.sh && \
./add_protoc.sh
```

最后，我们按照以下步骤运行*setup.py*脚本：

```py
$ python setup.py build
$ python setup.py install
```

现在是下载我们预构建模型的时候了。在我们的情况下，我们使用 SSD MobileNetV2 模型。您可以在 TensorFlow 的[目标检测模型动物园](https://oreil.ly/FwSlM)找到一个大型模型列表，其中包含按照它们训练的数据集、推理速度和平均精度（mAP）进行分类的模型。在那个列表中，下载名为`ssd_mobilenet_v2_coco`的模型，正如其名称所示，该模型是在 MS COCO 数据集上训练的。在 NVIDIA GeForce GTX TITAN X 上以 600x600 分辨率输入运行时，该模型在 22 mAP 上运行，耗时 31 毫秒。下载完该模型后，我们将其解压缩到 TensorFlow 仓库中的*models/research/object_detection*目录中。我们可以按照以下方式检查目录的内容：

```py
$ cd object_detection/
$ ls ssd_mobilenet_v2_coco_2018_03_29
checkpoint                      model.ckpt.data-00000-of-00001  model.ckpt.meta
saved_model
frozen_inference_graph.pb       model.ckpt.index                pipeline.config
```

## 测试我们的模型

在将我们的模型插入移动应用程序之前，验证模型是否能够进行预测是一个好主意。TensorFlow Models 仓库包含一个 Jupyter Notebook，我们可以简单地插入一张照片进行预测。您可以在*models/research/object_detection/object_detection_tutorial.ipynb*找到这个笔记本。图 14-3 展示了来自该笔记本的一张熟悉照片的预测。

![来自 TensorFlow Models 仓库的可直接使用的 Jupyter Notebook 中的目标检测预测](img/00022.jpeg)

###### 图 14-3。来自 TensorFlow Models 仓库的可直接使用的 Jupyter Notebook 中的目标检测预测

## 部署到设备

现在我们已经验证了模型的工作原理，是时候将其转换为适合移动设备的格式了。为了将其转换为 TensorFlow Lite 格式，我们使用了我们熟悉的`tfite_convert`工具，来自第十三章。值得注意的是，该工具操作*.pb*文件，而 TensorFlow 目标检测模型动物园仅提供模型检查点。因此，我们首先需要从检查点和图生成*.pb*模型。让我们使用 TensorFlow Models 仓库附带的方便脚本来做到这一点。您可以在*models/research/object_detection*中找到*export_tflite_ssd_graph.py*文件：

```py
$ python export_tflite_ssd_graph.py \
--pipeline_config_path=ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \
--trained_checkpoint_prefix=ssd_mobilenet_v2_coco_2018_03_29/
model.ckpt.data-00000-of-00001 \
--output_directory=tflite_model \
--add_postprocessing_op=true
```

如果前面的脚本执行成功，我们将在*tflite_model*目录中看到以下文件：

```py
$ ls tflite_model
tflite_graph.pb
tflite_graph.pbtxt
```

我们将使用`tflite_convert`工具将其转换为 TensorFlow Lite 格式。

```py
$ tflite_convert --graph_def_file=tflite_model/tflite_graph.pb \
--output_file=tflite_model/model.tflite
```

现在唯一剩下的事情就是将模型插入应用程序中。我们将使用的应用程序在书的 GitHub 网站中引用（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）位于*code/chapter-14*。我们已经看过如何在第 11、12 和 13 章中如何在应用程序中更换模型，因此我们不会在这里再次讨论。图 14-4 显示了将对象检测器模型插入 Android 应用程序时的结果。

![在 Android 设备上运行实时对象检测模型](img/00287.jpeg)

###### 图 14-4\. 在 Android 设备上运行实时对象检测模型

我们能够立即看到“猫”预测的原因是因为“猫”是用于训练模型的 MS COCO 数据集中已经存在的 80 个类别之一。这个模型可能足以供 Bob 部署在他的设备上（请记住，尽管 Bob 可能不使用手机来跟踪他的花园，但将 TensorFlow Lite 模型部署到边缘硬件的过程是非常相似的）。然而，Bob 最好通过在花园内生成的数据对模型进行微调以提高精度。在接下来的部分中，我们将探讨如何使用迁移学习来构建一个只使用基于网络的工具的对象检测器。如果你已经阅读了第八章，接下来的内容应该感觉很熟悉。

# 构建一个无需任何代码的自定义检测器

> 我的猫的呼吸闻起来像猫粮！
> 
> 拉尔夫·威格姆

让我们面对现实吧。你们的作者可以四处走动，在花园里拍摄很多猫的照片来进行这个实验。这将是乏味的，我们可能只会获得几个额外百分点的精度和召回率。或者我们可以看一个真正有趣的例子，也测试 CNN 的极限。当然，我们指的是*辛普森一家*。鉴于大多数模型没有使用卡通图像的特征进行训练，看看我们的模型在这里的表现将是有趣的。

首先，我们需要获取一个数据集。幸运的是，我们在[Kaggle](https://oreil.ly/-3wvj)上有一个可用的数据集。让我们下载数据集并使用 CustomVision.ai（类似于第八章）来训练我们的辛普森分类器，以下是使用的步骤。*太棒了！*

###### 注意

与一些人可能认为的相反，我们这些作者并没有数百万美元散落在周围。这意味着我们无法负担从福克斯购买*辛普森一家*的版权。版权法律的结果是阻止我们发布该节目的任何图像。因此，在这一部分中，我们使用了下一个最好的选择——来自美国国会的公共领域图像。我们使用国会议员的照片，这些议员与辛普森一家有相同的名字：*霍默*·霍奇、*玛吉*·鲁克玛、*巴特*·戈登和*丽莎*·布伦特·罗切斯特。请记住，这仅用于出版目的；我们仍然在原始卡通数据集上进行了训练。

1.  前往 CustomVision.ai 并开始一个新的目标检测项目（图 14-5）。

    ![在 CustomVision.ai 中创建一个新的目标检测项目](img/00154.jpeg)

    ###### 图 14-5\. 在 CustomVision.ai 中创建一个新的目标检测项目

1.  为霍默、玛吉、巴特和丽莎创建标签。为每个角色上传 15 张图像，并为这些图像中的每一个绘制一个边界框。仪表板现在应该类似于图 14-6。

    ![带有边界框和类名的仪表板](img/00207.jpeg)

    ###### 图 14-6.带有边界框和类名的仪表板

1.  点击“训练”按钮，让训练开始。仪表板上有滑块来控制概率和重叠阈值。我们可以尝试调整它们，使其达到我们喜欢的效果，并获得良好的精度和召回率。

1.  点击“快速测试”按钮，并使用辛普森家族的任意随机图像（之前未用于训练）来测试模型的性能。结果可能还不太好，但没关系：我们可以通过使用更多数据进行训练来解决这个问题。

1.  让我们尝试增加多少图像以提高精度、召回率和 mAP。让我们从为每个类别添加五张图像并重新训练模型开始。

1.  重复此过程，直到我们获得可接受的结果。图 14-7 显示了我们实验的结果。

    ![随着每个类别图像数量的增加，百分比平均精度改善的测量](img/00193.jpeg)

    ###### 图 14-7.随着每个类别图像数量的增加，百分比平均精度改善的测量

有了我们的最终模型，我们能够对互联网上的随机图像进行相当好的检测，如图 14-8 所示。尽管不是预期的，但令人惊讶的是，一个在自然图像上预训练的模型能够在相对少量数据上对卡通进行微调。

![使用最终模型检测到的辛普森角色，由同名的美国国会议员代表（请参见本节开头的注释）](img/00128.jpeg)

###### 图 14-8.使用最终模型检测到的辛普森角色，由同名的美国国会议员代表（请参见本节开头的注释）

正如我们现在已经知道的那样，CustomVision.ai 允许将此模型导出为各种格式，包括 Core ML、TensorFlow Lite、ONNX 等。我们可以简单地下载到我们想要的格式（在我们的情况下为*.tflite*），并将此文件插入到应用程序中，类似于我们在上一节中使用预训练模型的方式。有了实时的辛普森检测，我们可以在下次斯金纳校长在电视上谈论蒸汽火腿时向朋友和家人展示。

###### 注意

CustomVision.ai 并不是唯一一个允许您在不编写任何代码的情况下在线标记、训练和部署的平台。Google 的 Cloud AutoML（截至 2019 年 10 月仍处于测试阶段）和 Apple 的 Create ML（仅适用于 Apple 生态系统）提供非常相似的功能集。此外，Matroid 允许您从视频源构建自定义对象检测器，这对于训练网络而不需要花费太多精力来构建数据集非常有用。

到目前为止，我们已经看过三种快速的方法来运行目标检测，只需很少的代码。我们估计大约 90%的用例可以通过这些选项之一来处理。如果您可以使用这些解决您的情况，您可以停止阅读本章，直接跳转到“案例研究”部分。

在极少数情况下，我们可能希望对准确性、语音、模型大小和资源使用等因素进行进一步细化控制。在接下来的章节中，我们将更详细地了解目标检测的世界，从标记数据一直到部署模型。

# 目标检测的演变

多年来，随着深度学习革命的发生，不仅分类，还有其他计算机视觉任务，包括目标检测，都经历了一次复兴。在这段时间里，提出了几种用于目标检测的架构，通常是在其他架构的基础上构建的。图 14-9 展示了其中一些的时间轴。

![不同目标检测架构的时间轴（图片来源：吴雄伟等人的“深度学习目标检测的最新进展”）](img/00059.jpeg)

###### 图 14-9\. 不同目标检测架构的时间轴（图片来源：吴雄伟等人的“深度学习目标检测的最新进展”）

在目标分类中，我们的 CNN 架构从图像中提取特征并计算特定数量类别的概率。这些 CNN 架构（ResNet，MobileNet）作为目标检测网络的*骨干*。在目标检测网络中，我们的最终结果是边界框（由矩形中心，高度，宽度定义）。尽管涵盖许多这些内部细节可能需要更深入的探索（您可以在本书的 GitHub 网站上找到，链接在[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)），但我们可以广泛地将它们分为两类，如表 14-3 所示。

表 14-3\. 目标检测器的类别

|  | **描述** | **优缺点** |
| --- | --- | --- |
| **两阶段检测器** | 使用区域建议网络（RPN）生成类别不可知的边界框候选区域（感兴趣区域）。在这些区域提议上运行 CNN 以为每个分配一个类别。示例：Faster R-CNN，Mask R-CNN。 | + 高精度- 较慢 |
| **一阶段检测器** | 直接在特征图的每个位置上对对象进行分类预测；可以进行端到端训练。示例：YOLO，Single-Shot Detector (SSD)。 | + 速度快，更适合实时应用- 精度较低 |

###### 注意

在目标检测领域，Ross Girshik 是一个你在阅读论文时经常会遇到的名字。他在深度学习时代之前和之后都有作品，包括可变形部件模型（DPM），R-CNN，Fast R-CNN，Faster R-CNN，You Only Look Once（YOLO），Mask R-CNN，Feature Pyramid Network（FPN），RetinaNet 和 ResNeXt 等等，经常在公共基准上年复一年地打破自己的记录。

> 我参与了几次 PASCAL VOC 目标检测挑战的第一名入选，并因我在可变形部件模型上的工作而获得了“终身成就”奖。我认为这指的是 PASCAL 挑战的终身，而不是我的！
> 
> —Ross Girschik

## 性能考虑

从从业者的角度来看，主要关注点往往是精度和速度。这两个因素通常相互呈反比关系。我们希望找到在我们的场景中达到理想平衡的检测器。表 14-4 总结了一些在 TensorFlow 目标检测模型库中可用的预训练模型。报告的速度是在一个 600x600 像素分辨率的图像输入上，使用 NVIDIA GeForce GTX TITAN X 卡进行处理的。

表 14-4\. TensorFlow 目标检测模型库网站上一些可用架构的速度与平均精度百分比

|  | **推理速度（毫秒）** | **在 MS COCO 上的 mAP 百分比** |
| --- | --- | --- |
| **ssd_mobilenet_v1_coco** | 30 | 21 |
| **ssd_mobilenet_v2_coco** | 31 | 22 |
| **ssdlite_mobilenet_v2_coco** | 27 | 22 |
| **ssd_resnet_50_fpn_coco** | 76 | 35 |
| **faster_rcnn_nas** | 1,833 | 43 |

谷歌在 2017 年进行的一项研究（参见图 14-10）揭示了以下关键发现：

+   一阶段检测器比两阶段检测器更快，但精度较低。

+   骨干 CNN 架构在分类任务（如 ImageNet）上的准确性越高，对象检测器的精度也越高。

+   小尺寸物体对对象检测器具有挑战性，表现较差（通常在 10% mAP 以下）。这种影响在单阶段检测器上更为严重。

+   在大尺寸物体上，大多数检测器表现相似。

+   更高分辨率的图像会产生更好的结果，因为小物体在网络中看起来更大。

+   两阶段对象检测器内部生成大量建议，用作对象的潜在位置。建议数量可以进行调整。减少建议数量可以在准确性略微降低的情况下加快速度（阈值将取决于使用情况）。

对象检测架构以及骨干架构（特征提取器）对平均精度和预测时间的影响（请注意，在灰度打印中颜色可能不清晰；请参考书籍的 GitHub 获取彩色图像）（图片来源：Jonathan Huang 等人的现代卷积对象检测器速度/准确性权衡）

###### 图 14-10。对象检测架构以及骨干架构（特征提取器）对平均精度和预测时间的影响（请注意，在灰度打印中颜色可能不清晰；请参考书籍的 GitHub 网站[参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)]获取彩色图像）（图片来源：Jonathan Huang 等人的现代卷积对象检测器速度/准确性权衡）

# 对象检测中的关键术语

对象检测中经常使用的术语包括交并比、平均精度、非极大值抑制和锚点。让我们逐个解释这些术语的含义。
