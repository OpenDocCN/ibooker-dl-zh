# 第十四章：使用 TensorFlow Object Detection API 构建完美的猫定位器应用

鲍勃经常受到附近的流浪猫的访问。这些访问导致了不太愉快的结果。你看，鲍勃有一个相当大的花园，他非常用心地照料。然而，这只毛茸茸的小家伙每天晚上都会来到他的花园，并开始咬一堆植物。几个月的辛勤工作在一夜之间被摧毁。显然对这种情况不满意，鲍勃渴望采取一些行动。

发挥他内心的宠物侦探艾斯·文图拉（Ace Ventura）的精神，他试图在夜间保持清醒，赶走猫，但显然，这在长期内是不可持续的。毕竟，红牛也有其限制。达到一个临界点后，他决定使用核选项：将 AI 与他的洒水系统结合起来，从而真正“打开水管”对付猫。

在他广阔的花园里，他设置了一个可以跟踪猫的运动并打开最近的洒水器来吓跑猫的摄像头。他周围有一部旧手机，他所需要的就是一种实时确定猫位置的方法，这样他就可以相应地控制哪个洒水器触发。

希望经过几次不受欢迎的浴，如图 14-1 所示，猫会不再去鲍勃的花园里捣乱。

![建立一个 AI 猫喷水系统，就像这个 Havahart Spray Away Motion Detector](img/00166.jpeg)

###### 图 14-1\. 建立一个 AI 猫喷水系统，就像这个 Havahart Spray Away Motion Detector

###### 提示

敏锐的观察者可能已经注意到，我们试图在二维空间中找到猫的位置，而猫本身存在于三维空间中。我们可以做出一些假设来简化在真实世界坐标中定位猫的问题，假设摄像头始终处于固定位置。为了确定猫的距离，我们可以使用一个平均猫的大小来测量它在图片上的表观大小，这些图片是在距离摄像头的固定距离处定期拍摄的。例如，一个平均猫在距离摄像头两英尺处可能看起来有一个 300 像素的边界框高度，而当它离摄像头镜头三英尺时，它可能占据 250 像素。

在这一章中，我们帮助鲍勃制作他的猫探测器。（动物权益倡导者请注意：在制作本章时没有伤害到任何猫。）我们沿途回答以下问题：

+   有哪些不同类型的计算机视觉任务？

+   如何重用为现有对象类别预训练的模型？

+   我能否在不编写任何代码的情况下训练一个物体检测器模型？

+   我想要更精细的控制，以获得更高的准确性和速度。如何训练一个自定义物体检测器？

# 计算机视觉任务的类型

在前面的大部分章节中，我们基本上看了一种问题：物体分类。在这种情况下，我们找出了图像中是否包含某个特定类别的对象。在鲍勃的情况下，除了知道摄像头是否看到了一只猫之外，还需要确切地知道猫的位置，以触发最近的洒水器。确定图像中对象的位置是一种不同类型的问题：*物体检测*。在我们深入研究物体检测之前，让我们看看频繁的计算机视觉任务的种类以及它们试图回答的问题。

## 分类

在本书中，我们已经看过几个分类的例子。简单来说，分类的任务是将图像分配给图像中存在的对象类别。它回答了这样一个问题：“图像中是否有 X 类对象？”分类是最基本的计算机视觉任务之一。在一张图像中，可能有多个类别的对象，称为多类别分类。在这种情况下，一张图像的标签将是图像包含的所有对象类别的列表。

## 定位

分类的缺点是它不告诉我们图像中特定对象的位置在哪里，它有多大或多小，或者有多少个对象。它只告诉我们特定类别的对象存在于图像中的某个地方。*定位*，又称*带定位的分类*，可以告诉我们图像中有哪些类别以及它们在图像中的位置。这里的关键词是*类别*，而不是个别对象，因为定位每个类别只给出一个边界框。就像分类一样，它无法回答“这个图像中有多少个 X 类对象？”的问题。

正如您马上会看到的，只有在可以保证每个类别只有一个实例时，定位才能正常工作。如果一个类别有多个实例，一个边界框可能包含该类别的一些或所有对象（取决于概率）。但每个类别只有一个边界框似乎相当受限制，不是吗？

## 检测

当我们在同一图像中有多个属于多个类别的对象时，定位就不够了。对象检测会为每个类的每个实例提供一个边界矩形，对于所有类别。它还会告诉我们每个检测到的边界框中的对象的类别。例如，在自动驾驶汽车的情况下，对象检测会为汽车看到的每辆车、每个人、每个路灯等返回一个边界框。由于这种能力，我们也可以将其用于需要计数的应用程序。例如，计算人群中的人数。

与定位不同，检测在图像中某个类的实例数量上没有限制。这就是为什么它在现实世界的应用中被使用的原因。

###### 警告

通常人们在说“对象定位”时实际上是指“对象检测”。重要的是要注意这两者之间的区别。

## 分割

对象分割是将类标签分配给整个图像中的每个像素的任务。我们使用蜡笔给各种对象上色的儿童涂色书是一个现实世界的类比。考虑到图像中的每个像素都被分配一个类，这是一个高度计算密集的任务。虽然对象定位和检测提供边界框，分割产生像素组，也被称为*掩码*。与检测相比，分割在对象的边界方面显然更加精确。例如，一个可以实时改变用户头发颜色的化妆应用程序会使用分割。根据掩码的类型，分割有不同的风格。

### 语义分割

给定一幅图像，*语义分割*为每个类分配一个掩码。如果有多个相同类别的对象，则它们都分配给同一个掩码。同一类别的实例之间没有区分。就像定位一样，语义分割无法计数。

### 实例级分割

给定一幅图像，*实例级分割*识别每个类别每个实例占据的区域。同一类别的不同实例将被唯一分割。

表 14-1 列出了不同的计算机视觉任务。

表 14-1。使用图像 ID [120524](https://oreil.ly/ieJ2d) 从 MS COCO 数据集说明的计算机视觉任务类型

| **任务** | **图像** | **通俗地说** | **输出** |
| --- | --- | --- | --- |
| 对象分类 | ![](img/00118.jpeg) | 这个图像中有一只羊吗？ | 类概率 |
| 对象定位 | ![](img/00015.jpeg) | 图像中是否有“一只”羊，它在哪里？ | 边界框和类概率 |
| 对象检测 | ![](img/00147.jpeg) | 这个图像中的“所有”对象是什么？ | 边界框，类概率，类 ID 预测 |
| 语义分割 | ![](img/00325.jpeg) | 这幅图像中哪些像素属于不同的类别；例如，“羊”，“狗”，“人”？ | 每个类别一个掩码 |
| 实例级别分割 | ![](img/00064.jpeg) | 这幅图像中每个类别的每个实例属于哪些像素；例如，“羊”，“狗”，“人”？ | 每个类别的每个实例一个掩码 |

# 对象检测方法

根据情景、需求和技术知识，有几种方法可以实现在应用程序中获取对象检测功能。表 14-2 介绍了一些方法。

表 14-2\. 对象检测的不同方法及其权衡

|  | **自定义类别** | **工作量** | **云端或本地** | **优缺点** |
| --- | --- | --- | --- | --- |
| 基于云的对象检测 API | 否 | <5 分钟 | 仅限云端 | + 成千上万种类别+ 处于技术前沿+ 快速设置+ 可扩展 API– 无定制化– 由于网络而产生的延迟 |
| 预训练模型 | 否 | <15 分钟 | 本地 | + ~100 个类别+ 选择适合速度和准确性需求的模型+ 可在边缘运行– 无定制化 |
| 基于云的模型训练 | 是 | <20 分钟 | 云端和本地 | + 基于 GUI，无需编码进行训练+ 可定制性+ 选择强大模型（用于云端）和高效模型（用于边缘）+ 可扩展 API– 使用迁移学习，可能不允许完整模型重新训练 |
| 自定义训练模型 | 是 | 4 小时至 2 天 | 本地 | + 高度可定制+ 提供各种速度和准确性要求的模型– 耗时且复杂 |

值得注意的是，“云”标记的选项已经为可扩展性而设计。同时，标记为“本地”推断的选项也可以部署在云端。为了实现高规模，我们可以将它们部署在云端，类似于我们在第九章中为分类模型所做的方式。

# 调用预构建的基于云的对象检测 API

正如我们在第八章中已经看到的，调用基于云的 API 相对简单。该过程涉及设置账户、获取 API 密钥、阅读文档以及编写 REST API 客户端。为了简化这个过程，许多服务提供基于 Python（和其他语言）的 SDK。主要的基于云的对象检测 API 提供商是 Google 的 Vision AI（图 14-2）和微软的认知服务。

使用基于云的 API 的主要优势在于它们具有可扩展性，并支持识别成千上万种对象类别。这些云巨头使用大型专有数据集构建了他们的模型，这些数据集不断增长，导致了非常丰富的分类法。考虑到最大的公共数据集中带有对象检测标签的类别数量通常只有几百个，目前还没有非专有解决方案能够检测成千上万种类别。

基于云的 API 的主要限制当然是由于网络请求而产生的延迟。无法通过这种延迟实现实时体验。在下一节中，我们将看看如何在没有任何数据或训练的情况下实现实时体验。

![在 Google 的 Vision AI API 上运行熟悉的照片以获取对象检测结果](img/00328.jpeg)

###### 图 14-2\. 在 Google 的 Vision AI API 上运行熟悉的照片以获取对象检测结果

# 重用预训练模型

在这一部分，我们将探讨如何轻松地在预训练模型上在手机上运行目标检测器。希望您已经熟悉了在之前的一些章节中在移动设备上运行神经网络的方法。在书的 GitHub 网站上引用了在 iOS 和 Android 上运行目标检测模型的代码（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）位于*code/chapter-14*。更改功能只是简单地用新的*.tflite*模型文件替换现有的模型文件。

在之前的章节中，我们经常使用 MobileNet 来进行分类任务。尽管 MobileNet 系列，包括 MobileNetV2（2018）和 MobileNetV3（2019）本身只是分类网络，但它们可以作为 SSD MobileNetV2 等目标检测架构的骨干，我们在本章中使用。

## 获取模型

揭示幕后魔术的最佳方法是深入研究一个模型。我们需要获取我们的手上的[TensorFlow Models 仓库](https://oreil.ly/9CsHM)，其中包含 50 多个深度学习任务的模型，包括音频分类、文本摘要和目标检测。这个仓库包含模型以及我们在本章中使用的实用脚本。话不多说，让我们在我们的机器上克隆这个仓库：

```py
$ git clone https://github.com/tensorflow/models.git && cd models/research
```

然后，我们更新`PYTHONPATH`以使所有脚本对 Python 可见：

```py
$ export PYTHONPATH="${PYTHONPATH}:`pwd`:`pwd`/slim"
```

接下来，我们将从我们的 GitHub 仓库（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）复制协议缓冲区（protobuf）编译器命令脚本到我们当前目录，以使.proto 文件对 Python 可用：

```py
$ cp {path_to_book_github_repo}/code/chapter-14/add_protoc.sh . && \
chmod +x add_protoc.sh && \
./add_protoc.sh
```

最后，我们按照以下步骤运行*setup.py*脚本：

```py
$ python setup.py build
$ python setup.py install
```

现在是下载我们预构建模型的时候了。在我们的情况下，我们使用 SSD MobileNetV2 模型。您可以在 TensorFlow 的[目标检测模型动物园](https://oreil.ly/FwSlM)找到一个大型模型列表，其中包含按照它们训练的数据集、推理速度和平均精度（mAP）进行分类的模型。在那个列表中，下载名为`ssd_mobilenet_v2_coco`的模型，正如其名称所示，该模型是在 MS COCO 数据集上训练的。在 NVIDIA GeForce GTX TITAN X 上以 600x600 分辨率输入运行时，该模型在 22 mAP 上运行，耗时 31 毫秒。下载完该模型后，我们将其解压缩到 TensorFlow 仓库中的*models/research/object_detection*目录中。我们可以按照以下方式检查目录的内容：

```py
$ cd object_detection/
$ ls ssd_mobilenet_v2_coco_2018_03_29
checkpoint                      model.ckpt.data-00000-of-00001  model.ckpt.meta
saved_model
frozen_inference_graph.pb       model.ckpt.index                pipeline.config
```

## 测试我们的模型

在将我们的模型插入移动应用程序之前，验证模型是否能够进行预测是一个好主意。TensorFlow Models 仓库包含一个 Jupyter Notebook，我们可以简单地插入一张照片进行预测。您可以在*models/research/object_detection/object_detection_tutorial.ipynb*找到这个笔记本。图 14-3 展示了来自该笔记本的一张熟悉照片的预测。

![来自 TensorFlow Models 仓库的可直接使用的 Jupyter Notebook 中的目标检测预测](img/00022.jpeg)

###### 图 14-3。来自 TensorFlow Models 仓库的可直接使用的 Jupyter Notebook 中的目标检测预测

## 部署到设备

现在我们已经验证了模型的工作原理，是时候将其转换为适合移动设备的格式了。为了将其转换为 TensorFlow Lite 格式，我们使用了我们熟悉的`tfite_convert`工具，来自第十三章。值得注意的是，该工具操作*.pb*文件，而 TensorFlow 目标检测模型动物园仅提供模型检查点。因此，我们首先需要从检查点和图生成*.pb*模型。让我们使用 TensorFlow Models 仓库附带的方便脚本来做到这一点。您可以在*models/research/object_detection*中找到*export_tflite_ssd_graph.py*文件：

```py
$ python export_tflite_ssd_graph.py \
--pipeline_config_path=ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \
--trained_checkpoint_prefix=ssd_mobilenet_v2_coco_2018_03_29/
model.ckpt.data-00000-of-00001 \
--output_directory=tflite_model \
--add_postprocessing_op=true
```

如果前面的脚本执行成功，我们将在*tflite_model*目录中看到以下文件：

```py
$ ls tflite_model
tflite_graph.pb
tflite_graph.pbtxt
```

我们将使用`tflite_convert`工具将其转换为 TensorFlow Lite 格式。

```py
$ tflite_convert --graph_def_file=tflite_model/tflite_graph.pb \
--output_file=tflite_model/model.tflite
```

现在唯一剩下的事情就是将模型插入应用程序中。我们将使用的应用程序在书的 GitHub 网站中引用（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）位于*code/chapter-14*。我们已经看过如何在第 11、12 和 13 章中如何在应用程序中更换模型，因此我们不会在这里再次讨论。图 14-4 显示了将对象检测器模型插入 Android 应用程序时的结果。

![在 Android 设备上运行实时对象检测模型](img/00287.jpeg)

###### 图 14-4\. 在 Android 设备上运行实时对象检测模型

我们能够立即看到“猫”预测的原因是因为“猫”是用于训练模型的 MS COCO 数据集中已经存在的 80 个类别之一。这个模型可能足以供 Bob 部署在他的设备上（请记住，尽管 Bob 可能不使用手机来跟踪他的花园，但将 TensorFlow Lite 模型部署到边缘硬件的过程是非常相似的）。然而，Bob 最好通过在花园内生成的数据对模型进行微调以提高精度。在接下来的部分中，我们将探讨如何使用迁移学习来构建一个只使用基于网络的工具的对象检测器。如果你已经阅读了第八章，接下来的内容应该感觉很熟悉。

# 构建一个无需任何代码的自定义检测器

> 我的猫的呼吸闻起来像猫粮！
> 
> 拉尔夫·威格姆

让我们面对现实吧。你们的作者可以四处走动，在花园里拍摄很多猫的照片来进行这个实验。这将是乏味的，我们可能只会获得几个额外百分点的精度和召回率。或者我们可以看一个真正有趣的例子，也测试 CNN 的极限。当然，我们指的是*辛普森一家*。鉴于大多数模型没有使用卡通图像的特征进行训练，看看我们的模型在这里的表现将是有趣的。

首先，我们需要获取一个数据集。幸运的是，我们在[Kaggle](https://oreil.ly/-3wvj)上有一个可用的数据集。让我们下载数据集并使用 CustomVision.ai（类似于第八章）来训练我们的辛普森分类器，以下是使用的步骤。*太棒了！*

###### 注意

与一些人可能认为的相反，我们这些作者并没有数百万美元散落在周围。这意味着我们无法负担从福克斯购买*辛普森一家*的版权。版权法律的结果是阻止我们发布该节目的任何图像。因此，在这一部分中，我们使用了下一个最好的选择——来自美国国会的公共领域图像。我们使用国会议员的照片，这些议员与辛普森一家有相同的名字：*霍默*·霍奇、*玛吉*·鲁克玛、*巴特*·戈登和*丽莎*·布伦特·罗切斯特。请记住，这仅用于出版目的；我们仍然在原始卡通数据集上进行了训练。

1.  前往 CustomVision.ai 并开始一个新的目标检测项目（图 14-5）。

    ![在 CustomVision.ai 中创建一个新的目标检测项目](img/00154.jpeg)

    ###### 图 14-5\. 在 CustomVision.ai 中创建一个新的目标检测项目

1.  为霍默、玛吉、巴特和丽莎创建标签。为每个角色上传 15 张图像，并为这些图像中的每一个绘制一个边界框。仪表板现在应该类似于图 14-6。

    ![带有边界框和类名的仪表板](img/00207.jpeg)

    ###### 图 14-6.带有边界框和类名的仪表板

1.  点击“训练”按钮，让训练开始。仪表板上有滑块来控制概率和重叠阈值。我们可以尝试调整它们，使其达到我们喜欢的效果，并获得良好的精度和召回率。

1.  点击“快速测试”按钮，并使用辛普森家族的任意随机图像（之前未用于训练）来测试模型的性能。结果可能还不太好，但没关系：我们可以通过使用更多数据进行训练来解决这个问题。

1.  让我们尝试增加多少图像以提高精度、召回率和 mAP。让我们从为每个类别添加五张图像并重新训练模型开始。

1.  重复此过程，直到我们获得可接受的结果。图 14-7 显示了我们实验的结果。

    ![随着每个类别图像数量的增加，百分比平均精度改善的测量](img/00193.jpeg)

    ###### 图 14-7.随着每个类别图像数量的增加，百分比平均精度改善的测量

有了我们的最终模型，我们能够对互联网上的随机图像进行相当好的检测，如图 14-8 所示。尽管不是预期的，但令人惊讶的是，一个在自然图像上预训练的模型能够在相对少量数据上对卡通进行微调。

![使用最终模型检测到的辛普森角色，由同名的美国国会议员代表（请参见本节开头的注释）](img/00128.jpeg)

###### 图 14-8.使用最终模型检测到的辛普森角色，由同名的美国国会议员代表（请参见本节开头的注释）

正如我们现在已经知道的那样，CustomVision.ai 允许将此模型导出为各种格式，包括 Core ML、TensorFlow Lite、ONNX 等。我们可以简单地下载到我们想要的格式（在我们的情况下为*.tflite*），并将此文件插入到应用程序中，类似于我们在上一节中使用预训练模型的方式。有了实时的辛普森检测，我们可以在下次斯金纳校长在电视上谈论蒸汽火腿时向朋友和家人展示。

###### 注意

CustomVision.ai 并不是唯一一个允许您在不编写任何代码的情况下在线标记、训练和部署的平台。Google 的 Cloud AutoML（截至 2019 年 10 月仍处于测试阶段）和 Apple 的 Create ML（仅适用于 Apple 生态系统）提供非常相似的功能集。此外，Matroid 允许您从视频源构建自定义对象检测器，这对于训练网络而不需要花费太多精力来构建数据集非常有用。

到目前为止，我们已经看过三种快速的方法来运行目标检测，只需很少的代码。我们估计大约 90%的用例可以通过这些选项之一来处理。如果您可以使用这些解决您的情况，您可以停止阅读本章，直接跳转到“案例研究”部分。

在极少数情况下，我们可能希望对准确性、语音、模型大小和资源使用等因素进行进一步细化控制。在接下来的章节中，我们将更详细地了解目标检测的世界，从标记数据一直到部署模型。

# 目标检测的演变

多年来，随着深度学习革命的发生，不仅分类，还有其他计算机视觉任务，包括目标检测，都经历了一次复兴。在这段时间里，提出了几种用于目标检测的架构，通常是在其他架构的基础上构建的。图 14-9 展示了其中一些的时间轴。

![不同目标检测架构的时间轴（图片来源：吴雄伟等人的“深度学习目标检测的最新进展”）](img/00059.jpeg)

###### 图 14-9\. 不同目标检测架构的时间轴（图片来源：吴雄伟等人的“深度学习目标检测的最新进展”）

在目标分类中，我们的 CNN 架构从图像中提取特征并计算特定数量类别的概率。这些 CNN 架构（ResNet，MobileNet）作为目标检测网络的*骨干*。在目标检测网络中，我们的最终结果是边界框（由矩形中心，高度，宽度定义）。尽管涵盖许多这些内部细节可能需要更深入的探索（您可以在本书的 GitHub 网站上找到，链接在[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)），但我们可以广泛地将它们分为两类，如表 14-3 所示。

表 14-3\. 目标检测器的类别

|  | **描述** | **优缺点** |
| --- | --- | --- |
| **两阶段检测器** | 使用区域建议网络（RPN）生成类别不可知的边界框候选区域（感兴趣区域）。在这些区域提议上运行 CNN 以为每个分配一个类别。示例：Faster R-CNN，Mask R-CNN。 | + 高精度- 较慢 |
| **一阶段检测器** | 直接在特征图的每个位置上对对象进行分类预测；可以进行端到端训练。示例：YOLO，Single-Shot Detector (SSD)。 | + 速度快，更适合实时应用- 精度较低 |

###### 注意

在目标检测领域，Ross Girshik 是一个你在阅读论文时经常会遇到的名字。他在深度学习时代之前和之后都有作品，包括可变形部件模型（DPM），R-CNN，Fast R-CNN，Faster R-CNN，You Only Look Once（YOLO），Mask R-CNN，Feature Pyramid Network（FPN），RetinaNet 和 ResNeXt 等等，经常在公共基准上年复一年地打破自己的记录。

> 我参与了几次 PASCAL VOC 目标检测挑战的第一名入选，并因我在可变形部件模型上的工作而获得了“终身成就”奖。我认为这指的是 PASCAL 挑战的终身，而不是我的！
> 
> —Ross Girschik

## 性能考虑

从从业者的角度来看，主要关注点往往是精度和速度。这两个因素通常相互呈反比关系。我们希望找到在我们的场景中达到理想平衡的检测器。表 14-4 总结了一些在 TensorFlow 目标检测模型库中可用的预训练模型。报告的速度是在一个 600x600 像素分辨率的图像输入上，使用 NVIDIA GeForce GTX TITAN X 卡进行处理的。

表 14-4\. TensorFlow 目标检测模型库网站上一些可用架构的速度与平均精度百分比

|  | **推理速度（毫秒）** | **在 MS COCO 上的 mAP 百分比** |
| --- | --- | --- |
| **ssd_mobilenet_v1_coco** | 30 | 21 |
| **ssd_mobilenet_v2_coco** | 31 | 22 |
| **ssdlite_mobilenet_v2_coco** | 27 | 22 |
| **ssd_resnet_50_fpn_coco** | 76 | 35 |
| **faster_rcnn_nas** | 1,833 | 43 |

谷歌在 2017 年进行的一项研究（参见图 14-10）揭示了以下关键发现：

+   一阶段检测器比两阶段检测器更快，但精度较低。

+   骨干 CNN 架构在分类任务（如 ImageNet）上的准确性越高，对象检测器的精度也越高。

+   小尺寸物体对对象检测器具有挑战性，表现较差（通常在 10% mAP 以下）。这种影响在单阶段检测器上更为严重。

+   在大尺寸物体上，大多数检测器表现相似。

+   更高分辨率的图像会产生更好的结果，因为小物体在网络中看起来更大。

+   两阶段对象检测器内部生成大量建议，用作对象的潜在位置。建议数量可以进行调整。减少建议数量可以在准确性略微降低的情况下加快速度（阈值将取决于使用情况）。

对象检测架构以及骨干架构（特征提取器）对平均精度和预测时间的影响（请注意，在灰度打印中颜色可能不清晰；请参考书籍的 GitHub 获取彩色图像）（图片来源：Jonathan Huang 等人的现代卷积对象检测器速度/准确性权衡）

###### 图 14-10。对象检测架构以及骨干架构（特征提取器）对平均精度和预测时间的影响（请注意，在灰度打印中颜色可能不清晰；请参考书籍的 GitHub 网站[参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)]获取彩色图像）（图片来源：Jonathan Huang 等人的现代卷积对象检测器速度/准确性权衡）

# 对象检测中的关键术语

对象检测中经常使用的术语包括交并比、平均精度、非极大值抑制和锚点。让我们逐个解释这些术语的含义。

## 交并比

在目标检测训练流程中，通常使用诸如交并比（IoU）之类的指标作为过滤特定质量检测的阈值。为了计算 IoU，我们使用预测的边界框和实际边界框来计算两个不同的数字。第一个是预测和实际情况重叠的区域——交集。第二个是预测和实际情况覆盖的范围——并集。正如名称所示，我们简单地将交集的总面积除以并集的面积来获得 IoU。图 14-11 直观地展示了两个 2x2 正方形的 IoU 概念，它们在中间的一个 1x1 正方形中相交。

![IoU 比率的可视化表示](img/00007.jpeg)

###### 图 14-11。IoU 比率的可视化表示

在理想情况下，预测的边界框与实际情况完全匹配，IoU 值将为 1。在最坏的情况下，预测与实际情况没有重叠，IoU 值将为 0。正如我们所看到的，IoU 值将在 0 和 1 之间变化，较高的数字表示较高质量的预测，如图 14-12 所示。

为了帮助我们过滤和报告结果，我们会设置一个最小 IoU 阈值。将阈值设置为非常激进的值（如 0.9）会导致丢失许多可能在后续流程中变得重要的预测。相反，将阈值设置得太低会导致太多虚假的边界框。通常用于报告目标检测模型精度的最小 IoU 为 0.5。

![IoU 的图示；更好模型的预测往往与实际情况有更重叠，导致更高的 IoU](img/00293.jpeg)

###### 图 14-12。IoU 的图示；更好模型的预测往往与实际情况有更重叠，导致更高的 IoU

值得在这里重申的是，IoU 值是针对每个类别实例计算的，而不是针对每个图像。然而，为了计算检测器在更大一组图像上的质量，我们会将 IoU 与其他指标（如平均精度）结合使用。

## 平均精度

在研究目标检测的论文中，我们经常遇到诸如 AP@0.5 的数字。该术语表示 IoU=0.5 时的平均精度。另一个更详细的表示方法是 AP@[0.6:0.05:0.75]，这是从 IoU=0.6 到 IoU=0.75 的平均精度，间隔为 0.05。平均精度（或 mAP）简单地是所有类别的平均精度。对于 COCO 挑战，使用的 MAP 指标是 AP@[0.5:0.05:0.95]。

## 非极大值抑制

在内部，目标检测算法对可能在图像中的对象的潜在位置提出了许多建议。对于图像中的每个对象，预期会有多个具有不同置信度值的边界框建议。我们的任务是找到最能代表对象真实位置的建议。一种天真的方法是只考虑置信度值最大的建议。如果整个图像中只有一个对象，这种方法可能有效。但如果图像中有多个类别，每个类别中有多个实例，这种方法就不起作用了。

非极大值抑制（NMS）来拯救我们（图 14-13）。NMS 背后的关键思想是，同一对象的两个实例不会有严重重叠的边界框；例如，它们的边界框的 IoU 将小于某个 IoU 阈值（比如 0.5）。实现这一点的一种贪婪方法是对每个类别重复以下步骤：

1.  过滤掉所有置信度低于最小阈值的建议。

1.  接受置信度值最大的提议。

1.  对于所有按其置信度值降序排序的剩余提议，检查当前框与先前接受的提议之一的 IoU 是否≥0.5。如果是，则过滤掉；否则，接受它作为提议。

    ![使用 NMS 找到最能代表图像中对象位置的边界框](img/00223.jpeg)

    ###### 图 14-13。使用 NMS 找到最能代表图像中对象位置的边界框

# 使用 TensorFlow 目标检测 API 构建自定义模型

在本节中，我们将通过一个完整的端到端示例来构建一个对象检测器。我们将看到该过程中的几个步骤，包括收集、标记和预处理数据，训练模型，并将其导出为 TensorFlow Lite 格式。

首先，让我们看一些收集数据的策略。

## 数据收集

我们现在知道，在深度学习的世界中，数据至关重要。我们可以通过几种方式获取我们想要检测的对象的数据：

使用现成的数据集

有一些公共数据集可用于训练对象检测模型，如 MS COCO（80 个类别）、ImageNet（200 个类别）、Pascal VOC（20 个类别）和更新的 Open Images（600 个类别）。MS COCO 和 Pascal VOC 在大多数对象检测基准测试中使用，基于 COCO 的基准测试对于真实世界场景更具现实性，因为图像更复杂。

从网上下载

我们应该已经熟悉这个过程，因为在第十二章中，我们为“热狗”和“非热狗”类别收集了图像。浏览器扩展程序如 Fatkun 对于快速从搜索引擎结果中下载大量图像非常方便。

手动拍照

这是更耗时（但潜在有趣）的选项。为了构建一个强大的模型，重要的是用在各种不同环境中拍摄的照片来训练它。拿着要检测的对象，我们应该至少拍摄 100 到 150 张不同背景下的照片，具有各种构图和角度，并在许多不同的光照条件下。图 14-14 展示了用于训练可乐和百事对象检测模型的一些照片示例。考虑到模型可能会学习到红色代表可乐，蓝色代表百事这种虚假相关性，最好混合对象和可能会混淆它的背景，以便最终实现更强大的模型。

![在各种不同环境中拍摄的对象照片，用于训练对象检测模型](img/00212.jpeg)

###### 图 14-14。在各种不同环境中拍摄的对象照片，用于训练对象检测模型

值得在对象不太可能使用的环境中拍摄对象的照片，以使数据集多样化并提高模型的稳健性。这也有一个额外的好处，可以使原本乏味和繁琐的过程变得有趣。我们可以通过挑战自己想出独特和创新的照片来使其有趣。图 14-15 展示了在构建货币检测器过程中拍摄的一些创意照片的示例。

![在构建多样化货币数据集过程中拍摄的一些创意照片](img/00178.jpeg)

###### 图 14-15。在构建多样化货币数据集过程中拍摄的一些创意照片

因为我们想要制作一个猫检测器，我们将重复使用“猫和狗”Kaggle 数据集中的猫图片，该数据集在第三章中使用过。我们随机选择了该数据集中的图片，并将它们分成训练集和测试集。

为了保持输入到我们网络的大小的统一性，我们将所有图像调整为固定大小。我们将使用相同的大小作为网络定义的一部分，并在转换为*.tflite*模型时使用。我们可以使用 ImageMagick 工具一次调整所有图像的大小：

```py
$ apt-get install imagemagick
$ mogrify -resize 800x600 *.jpg
```

###### 提示

如果您在当前目录中有大量图像（即数万张图像），上述命令可能无法列出所有图像。解决方法是使用`find`命令列出所有图像，并将输出传输到`mogrify`命令：

```py
$ find . -type f | awk -F. '!a[$NF]++{print $NF}' |
xargs -I{} mogrify -resize 800x600 *.jpg
```

## 标记数据

在获得数据之后，下一步是对其进行标记。与分类不同，仅将图像放入正确的目录中就足够了，我们需要手动为感兴趣的对象绘制边界矩形。我们选择此任务的工具是 LabelImg（适用于 Windows、Linux 和 Mac），原因如下：

+   您可以将注释保存为 PASCAL VOC 格式的 XML 文件（也被 ImageNet 采用）或 YOLO 格式。

+   它支持单标签和多标签。

###### 注意

如果您像我们一样是好奇的猫，您可能想知道 YOLO 格式和 PASCAL VOC 格式之间的区别是什么。 YOLO 恰好使用简单的*.txt*文件，每个图像一个文件，文件名相同，用于存储有关图像中类别和边界框的信息。以下是 YOLO 格式中*.txt*文件的典型外观：

```py
class_for_box1 box1_x box1_y box1_w box1_h
class_for_box2 box2_x box2_y box2_w box2_h
```

请注意，此示例中的 x、y、宽度和高度是使用图像的完整宽度和高度进行归一化的。

另一方面，PASCAL VOC 格式是基于 XML 的。与 YOLO 格式类似，我们每个图像使用一个 XML 文件（最好与相同名称）。您可以在 Git 存储库的*code/chapter-14/pascal-voc-sample.xml*中查看示例格式。

首先，从[GitHub](https://oreil.ly/jH31E)下载应用程序到计算机上的一个目录。该目录将包含一个可执行文件和一个包含一些示例数据的数据目录。因为我们将使用自己的自定义数据，所以我们不需要提供的数据目录中的数据。您可以通过双击可执行文件来启动应用程序。

此时，我们应该有一组准备用于训练过程的图像。我们首先将数据随机分成训练集和测试集，并将它们放在不同的目录中。我们可以通过简单地将图像随机拖放到任一目录中来进行此拆分。创建训练和测试目录后，我们通过单击“打开目录”来加载训练目录，如图 14-16 所示。

![单击“打开目录”按钮，然后选择包含训练数据的目录](img/00133.jpeg)

###### 图 14-16\. 单击“打开目录”按钮，然后选择包含训练数据的目录

LabelImg 加载训练目录后，我们可以开始标记过程。我们必须逐个图像地为每个对象（在我们的情况下仅为猫）手动绘制边界框，如图 14-17 所示。绘制边界框后，我们将提示提供标签以配合边界框。对于此练习，请输入对象的名称“cat”。输入标签后，我们只需选择复选框，以指定再次为后续图像选择该标签。对于具有多个对象的图像，我们将制作多个边界框并添加相应的标签。如果存在不同类型的对象，我们只需为该对象类别添加新标签。

![从左侧面板中选择 Create RectBox 以创建覆盖猫的边界框](img/00068.jpeg)

###### 图 14-17\. 从左侧面板中选择 Create RectBox 以创建覆盖猫的边界框

现在，对所有训练和测试图像重复这一步骤。我们希望为每个对象获取紧密的边界框，以便对象的所有部分都被框住。最后，在训练和测试目录中，我们看到每个图像文件都有一个*.xml*文件，如图 14-18 所示。我们可以在文本编辑器中打开`.xml`文件并检查元数据，如图像文件名、边界框坐标和标签名称。

![每个图像都附带一个包含标签信息和边界框信息的 XML 文件](img/00049.jpeg)

###### 图 14-18\. 每个图像都附带一个包含标签信息和边界框信息的 XML 文件

## 数据预处理

到目前为止，我们有方便的 XML 数据，提供了所有对象的边界框。但是，为了使用 TensorFlow 对数据进行训练，我们必须将其预处理成 TensorFlow 理解的格式，即 TFRecords。在将数据转换为 TFRecords 之前，我们必须经历一个中间步骤，将所有 XML 文件中的数据合并到一个单独的逗号分隔值（CSV）文件中。TensorFlow 提供了帮助脚本来协助我们进行这些操作。

现在我们的环境已经设置好，是时候开始做一些真正的工作了：

1.  使用来自 Dat Tran 的[racoon_dataset](https://oreil.ly/k8QGl)存储库中的`xml_to_csv`工具，将我们的 cats 数据集目录转换为一个单独的 CSV 文件。我们在我们的存储库中提供了这个文件的稍作编辑副本，位于*code/chapter-14/xml_to_csv.py*中：

    ```py
    $ python xml_to_csv.py -i {path to cats training dataset} -o {path to output
    train_labels.csv}
    ```

1.  对测试数据做同样的操作：

    ```py
    $ python xml_to_csv.py -i {path to cats test dataset} -o {path to
    test_labels.csv}
    ```

1.  创建*label_map.pbtxt*文件。该文件包含所有类别的标签和标识符映射。我们使用这个文件将文本标签转换为整数标识符，这是 TFRecord 所期望的。因为我们只有一个类别，所以这个文件相对较小，如下所示：

    ```py
    item {
        id: 1
        name: 'cat'
    }
    ```

1.  生成 TFRecord 格式的文件，其中包含以后用于训练和测试模型的数据。这个文件也来自 Dat Tran 的[racoon_dataset](https://oreil.ly/VNwwE)存储库。我们在我们的存储库中提供了这个文件的稍作编辑副本，位于*code/chapter-14/generate_tfrecord.py*中。（值得注意的是，参数中的`image_dir`路径应与 XML 文件中的路径相同；LabelImg 使用绝对路径。）

    ```py
    $ python generate_tfrecord.py \
    --csv_input={path to train_labels.csv} \
    --output_path={path to train.tfrecord} \
    --image_dir={path to cat training dataset}

    $ python generate_tfrecord.py \
    --csv_input={path to test_labels.csv} \
    --output_path={path to test.tfrecord} \
    --image_dir={path to cat test dataset}
    ```

有了准备好的*train.tfrecord*和*test.tfrecord*文件，我们现在可以开始训练过程了。

# 检查模型

（此部分仅供信息，对训练过程并非必需。如果您愿意，可以直接跳转到“训练”。）

我们可以使用`saved_model_cli`工具检查我们的模型：

```py
$ saved_model_cli show --dir ssd_mobilenet_v2_coco_2018_03_29/saved_model \
--tag_set serve \
--signature_def serving_default
```

该脚本的输出如下：

```py
The given SavedModel SignatureDef contains the following input(s):
  inputs['inputs'] tensor_info:
      dtype: DT_UINT8
      shape: (-1, -1, -1, 3)
      name: image_tensor:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['detection_boxes'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 100, 4)
      name: detection_boxes:0
  outputs['detection_classes'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 100)
      name: detection_classes:0
  outputs['detection_scores'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 100)
      name: detection_scores:0
  outputs['num_detections'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1)
      name: num_detections:0
  outputs['raw_detection_boxes'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, -1, 4)
      name: raw_detection_boxes:0
  outputs['raw_detection_scores'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, -1, 2)
      name: raw_detection_scores:0
Method name is: tensorflow/serving/predict
```

以下是我们如何解释前一个命令的输出：

1.  `inputs`的形状为`(-1, -1, -1, 3)`表明训练过的模型可以接受包含三个通道的任意大小的图像输入。由于输入大小的固有灵活性，转换后的模型会比具有固定大小输入的模型更大。当我们在本章后面训练自己的目标检测器时，我们将把输入大小固定为 800 x 600 像素，以使得生成的模型更紧凑。

1.  在转向输出时，第一个输出是`detection_boxes (-1, 100, 4)`。这告诉我们有多少个检测框可能存在，以及它们的样子。特别是，第一个数字（即-1）表示我们可以根据所有 100 个类别的检测结果拥有任意数量的框（第二个数字），每个框有四个坐标（第三个数字）——x、y、宽度和高度——定义每个框。换句话说，我们不限制每个 100 个类别的检测数量。

1.  对于`detection_classes`，我们有一个包含两个元素的列表：第一个定义了我们检测到多少个对象，第二个是一个独热编码向量，其中检测到的类别被启用。

1.  `num_detections`是图像中检测到的对象数量。它是一个单一的浮点数。

1.  `raw_detection_boxes`定义了每个对象检测到的每个框的坐标。所有这些检测都是在对所有检测应用 NMS 之前进行的。

1.  `raw_detection_scores`是两个浮点数的列表，第一个描述了总检测数，第二个描述了总类别数，包括背景（如果将其视为单独的类别）。

## 训练

因为我们使用的是 SSD MobileNetV2，所以我们需要创建一个*pipeline.config*文件的副本（来自 TensorFlow 存储库），并根据我们的配置参数进行修改。首先，复制配置文件并在文件中搜索字符串`PATH_TO_BE_CONFIGURED`。该参数指示需要使用正确路径（最好是绝对路径）更新的所有位置。我们还需要编辑配置文件中的一些参数，例如类别数（`num_classes`）、步数（`num_steps`）、验证样本数（`num_examples`）以及训练和测试/评估部分的标签映射路径。 （您可以在本书的 GitHub 网站上找到我们版本的*pipeline.config*文件（请参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)））。

```py
$ cp object_detection/samples/configs/ssd_mobilenet_v2_coco.config
./pipeline.config
$ vim pipeline.config
```

###### 注意

在我们的示例中，我们使用 SSD MobileNetV2 模型来训练我们的目标检测器。在不同条件下，您可能希望选择不同的模型来进行训练。因为每个模型都附带自己的管道配置文件，您将修改该配置文件。配置文件与每个模型捆绑在一起。您将使用类似的过程，识别需要更改的路径并使用文本编辑器相应地更新它们。

除了修改路径，您可能还想修改配置文件中的其他参数，例如图像大小、类别数、优化器、学习率和迭代次数。

到目前为止，我们一直在 TensorFlow 存储库中的*models/research*目录中。现在让我们进入*object_detection*目录，并从 TensorFlow 运行*model_main.py*脚本，根据我们刚刚编辑的*pipeline.config*文件提供的配置来训练我们的模型：

```py
$ cd object_detection/
$ python model_main.py \
--pipeline_config_path=../pipeline.config \
--logtostderr \
--model_dir=training/
```

当我们看到以下输出行时，我们就知道训练正在正确进行：

```py
Average Precision (AP) @[ IoU=0.50:0.95 | area=all | maxDets=100 ] = 0.760
```

根据我们进行训练的迭代次数，训练将需要几分钟到几个小时，所以计划一下，吃点零食，洗洗衣服，清理猫砂盆，或者更好的是，提前阅读下一章。训练完成后，我们应该在 training/目录中看到最新的检查点文件。训练过程还会生成以下文件：

```py
$ ls training/

checkpoint                                   model.ckpt-13960.meta
eval_0                                       model.ckpt-16747.data-00000-of-00001
events.out.tfevents.1554684330.computername  model.ckpt-16747.index
events.out.tfevents.1554684359.computername  model.ckpt-16747.meta
export                                       model.ckpt-19526.data-00000-of-00001
graph.pbtxt                                model.ckpt-19526.index
label_map.pbtxt                            model.ckpt-19526.meta
model.ckpt-11180.data-00000-of-00001       model.ckpt-20000.data-00000-of-00001
model.ckpt-11180.index                     model.ckpt-20000.index
model.ckpt-11180.meta                      model.ckpt-20000.meta
model.ckpt-13960.data-00000-of-00001       pipeline.config
model.ckpt-13960.index
```

我们将在下一节将最新的检查点文件转换为*.TFLite*格式。

###### 提示

有时，您可能希望对模型进行更深入的检查，以进行调试、优化和/或信息目的。可以将以下命令与指向检查点文件、配置文件和输入类型的参数一起运行，以查看所有不同参数、模型分析报告和其他有用信息：

```py
$ python export_inference_graph.py \
--input_type=image_tensor \
--pipeline_config_path=training/pipeline.config \
--output_directory=inference_graph \
--trained_checkpoint_prefix=training/model.ckpt-20000
```

这个命令的输出如下：

```py
=========================Options=========================
-max_depth                  10000
-step                       -1
...

==================Model Analysis Report==================
...
Doc:
scope: The nodes in the model graph are organized by
their names, which is hierarchical like filesystem.
param: Number of parameters (in the Variable).

Profile:
node name | # parameters
_TFProfRoot (--/3.72m params)
 BoxPredictor_0 (--/93.33k params)
    BoxPredictor_0/BoxEncodingPredictor
    (--/62.22k params)
      BoxPredictor_0/BoxEncodingPredictor/biases
      (12, 12/12 params)
 ...     
FeatureExtractor (--/2.84m params)
...
```

这份报告可以帮助您了解您拥有多少参数，从而帮助您找到模型优化的机会。

## 模型转换

现在我们有了最新的检查点文件（后缀应该与*pipeline.config*文件中的迭代次数匹配），我们将像之前在本章中使用我们的预训练模型一样，将其输入到`export_tflite_ssd_graph`脚本中：

```py
$ python export_tflite_ssd_graph.py \
--pipeline_config_path=training/pipeline.config \
--trained_checkpoint_prefix=training/model.ckpt-20000 \
--output_directory=tflite_model \
--add_postprocessing_op=true
```

如果前面的脚本执行成功，我们将在*tflite_model*目录中看到以下文件：

```py
$ ls tflite_model
tflite_graph.pb
tflite_graph.pbtxt
```

我们还剩下最后一步：将冻结的图形文件转换为*.TFLite*格式。我们可以使用`tflite_convert`工具来完成这个操作：

```py
$ tflite_convert --graph_def_file=tflite_model/tflite_graph.pb \
--output_file=tflite_model/cats.tflite \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLi
te_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
--input_shape=1,800,600,3 \
--allow_custom_ops
```

在这个命令中，请注意我们使用了一些可能一开始不直观的不同参数：

+   对于`--input_arrays`，参数简单地表示在预测期间提供的图像将是归一化的`float32`张量。

+   提供给`--output_arrays`的参数表明每个预测中将有四种不同类型的信息：边界框的数量、检测分数、检测类别和边界框本身的坐标。这是可能的，因为我们在上一步的导出图脚本中使用了参数`--add_postprocessing_op=true`。

+   对于`--input_shape`，我们提供与*pipeline.config*文件中相同的维度值。

其余的都相当琐碎。我们现在应该在*tflite_model/*目录中有一个全新的*cats.tflite*模型文件。现在它已经准备好插入到 Android、iOS 或边缘设备中，以进行猫的实时检测。这个模型正在为拯救鲍勃的花园而努力！

###### 提示

在浮点 MobileNet 模型中，'`normalized_image_tensor`'的值在`[-1,1)`之间。这通常意味着将每个像素（线性地）映射到一个值在[–1,1]之间。输入图像的值在 0 到 255 之间被缩放为(1/128)，然后将-1 的值添加到它们中以确保范围是`[-1,1)`。

在量化的 MobileNet 模型中，'`normalized_image_tensor`'的值在`[0, 255]`之间。

通常，在 TensorFlow Models 存储库的*models/research/object_detection/models*目录中定义的特征提取器类中查看`preprocess`函数。

# 图像分割

在超越目标检测的基础上，为了获得更精确的物体位置，我们可以执行物体分割。正如我们在本章前面看到的那样，这涉及对图像帧中的每个像素进行类别预测。像 U-Net、Mask R-CNN 和 DeepLabV3+这样的架构通常用于执行分割任务。与目标检测类似，有一个越来越流行的趋势是在实时运行分割网络，包括在资源受限的设备上，如智能手机上。实时性为许多消费者应用场景打开了大门，比如面部滤镜（图 14-19），以及工业场景，比如为自动驾驶汽车检测可行驶道路（图 14-20）。

![使用 ModiFace 应用程序为头发上色，准确映射属于头发的像素](img/00012.jpeg)

###### 图 14-19。使用 ModiFace 应用程序为头发上色，准确映射属于头发的像素

![从行车记录仪（CamVid 数据集）的帧上执行的图像分割](img/00300.jpeg)

###### 图 14-20。从行车记录仪（CamVid 数据集）的帧上执行的图像分割

正如你现在可能已经习惯在这本书中听到的，你可以在不需要太多代码的情况下完成很多工作。像 Supervisely、LabelBox 和 Diffgram 这样的标注工具不仅可以帮助标注数据，还可以加载先前注释过的数据并在预训练的对象分割模型上进一步训练。此外，这些工具提供了 AI 辅助标注，可以大大加快（约 10 倍）原本费时费力的昂贵标注过程。如果这听起来令人兴奋，那你很幸运！我们在书的 GitHub 网站上包含了一个额外的资源指南，介绍如何学习和构建分割项目（请参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）。

# 案例研究

现在让我们看看目标检测是如何被用于推动行业中的实际应用的。

## 智能冰箱

智能冰箱近几年来越来越受欢迎，因为它们开始变得更加实惠。人们似乎喜欢知道冰箱里有什么，即使他们不在家也能了解。微软与瑞士制造业巨头利勃海尔合作，在其新一代 SmartDeviceBox 冰箱中使用深度学习（图 14-21）。该公司使用 Fast R-CNN 在其冰箱内执行目标检测，以检测不同的食物项目，维护库存，并让用户了解最新状态。

![从 SmartDeviceBox 冰箱检测到的对象及其类别（图片来源）](img/00257.jpeg)

###### 图 14-21。从 SmartDeviceBox 冰箱检测到的对象及其类别（图片来源）

## 人群计数

人群计数不仅仅是猫在窗户外凝视时才会做的事情。在许多情况下都很有用，包括管理大型体育赛事、政治集会和其他高流量区域的安全和后勤。人群计数，顾名思义，可以用于计算任何类型的对象，包括人和动物。野生动物保护是一个极具挑战性的问题，因为缺乏标记数据和强烈的数据收集策略。

### 野生动物保护

包括格拉斯哥大学、开普敦大学、芝加哥自然历史博物馆和坦桑尼亚野生动物研究所在内的几个组织联合起来，统计了地球上最大的陆地动物迁徙：在塞伦盖蒂和肯尼亚马赛马拉国家保护区之间的 1,300,000 只蓝色角马和 25 万只斑马（图 14-22）。他们通过 2200 名志愿公民科学家使用 Zooniverse 平台进行数据标记，并使用像 YOLO 这样的自动化目标检测算法来统计角马和斑马的数量。他们观察到志愿者和目标检测算法的计数相差不到 1%。

![从一架小型勘测飞机拍摄的角马的航拍照片（图片来源）](img/00218.jpeg)

###### 图 14-22。从一架小型勘测飞机拍摄的角马的航拍照片（图片来源）

### 昆布梅拉

另一个密集人群计数的例子来自印度的普拉亚格拉吉，大约每 12 年有 2.5 亿人参加昆布梅拉节（图 14-23）。对于如此庞大的人群进行控制一直是困难的。事实上，在 2013 年，由于糟糕的人群管理，一场踩踏事件导致 42 人不幸丧生。

2019 年，当地政府与拉森和图博签约，利用人工智能进行各种后勤任务，包括交通监控、垃圾收集、安全评估以及人群监测。当局使用了一千多个闭路电视摄像头，分析了人群密度，并设计了一个基于固定区域人群密度的警报系统。在人群密度较高的地方，监测更加密集。这使得它成为有史以来建立的最大人群管理系统，进入了吉尼斯世界纪录！

![2013 年昆布梅拉，由一名参与者拍摄](img/00144.jpeg)

###### 图 14-23。2013 年昆布梅拉，由一名参与者拍摄（图片来源）

## Seeing AI 中的人脸检测

微软的 Seeing AI 应用程序（为盲人和低视力社区）提供了实时人脸检测功能，通过该功能，它会告知用户手机摄像头前的人员、他们的相对位置以及与摄像头的距离。一个典型的引导可能会听起来像“左上角有一个脸，距离四英尺”。此外，该应用程序使用人脸检测引导来对已知人脸列表进行人脸识别。如果脸部匹配，它还会宣布该人的姓名。一个语音引导的例子可能是“伊丽莎白靠近顶部边缘，距离三英尺”，如图 14-24 所示。

为了在摄像头视频中识别一个人的位置，系统正在运行一个快速的移动优化对象检测器。然后将这张脸的裁剪图传递给微软认知服务中的年龄、情绪、发型等识别算法进行进一步处理。为了以一种尊重隐私的方式识别人，该应用程序要求用户的朋友和家人拍摄他们脸部的三张自拍照，并生成脸部的特征表示（嵌入），该表示存储在设备上（而不是存储任何图像）。当未来在摄像头实时视频中检测到一张脸时，将计算一个嵌入并与嵌入数据库和相关名称进行比较。这是基于一次性学习的概念，类似于我们在第四章中看到的孪生网络。不存储图像的另一个好处是，即使存储了大量的人脸，应用程序的大小也不会急剧增加。

![Seeing AI 上的人脸检测功能](img/00142.jpeg)

###### 图 14-24。Seeing AI 上的人脸检测功能

## 自动驾驶汽车

包括 Waymo、Uber、特斯拉、Cruise、NVIDIA 等在内的几家自动驾驶汽车公司使用目标检测、分割和其他深度学习技术来构建他们的自动驾驶汽车。行人检测、车辆类型识别和速限标志识别等高级驾驶辅助系统（ADAS）是自动驾驶汽车的重要组成部分。

在自动驾驶汽车的决策中，NVIDIA 使用专门的网络来执行不同的任务。例如，WaitNet 是一个用于快速、高级检测交通灯、十字路口和交通标志的神经网络。除了快速外，它还能够抵抗噪音，并能够在雨雪等恶劣条件下可靠工作。WaitNet 检测到的边界框然后被馈送到更详细分类的专门网络中。例如，如果在一帧中检测到交通灯，那么它将被馈送到 LightNet——一个用于检测交通灯形状（圆形与箭头）和状态（红、黄、绿）的网络。此外，如果在一帧中检测到交通标志，它将被馈送到 SignNet，一个用于对美国和欧洲几百种交通标志进行分类的网络，包括停车标志、让行标志、方向信息、速限信息等。从更快的网络到专门网络的级联输出有助于提高性能并模块化不同网络的开发。

![使用 NVIDIA Drive 平台在自动驾驶汽车上检测交通灯和标志（图片来源）](img/00097.jpeg)

###### 图 14-25。使用 NVIDIA Drive 平台在自动驾驶汽车上检测交通灯和标志（[图片来源](https://oreil.ly/vcJY6)）

# 总结

在这一章中，我们探讨了计算机视觉任务的类型，包括它们之间的关系以及它们之间的区别。我们深入研究了目标检测，管道的工作原理以及它随时间的演变。然后我们收集数据，标记数据，训练模型（有或没有代码），并将模型部署到移动设备上。接着我们看了看目标检测在工业、政府和非政府组织（NGOs）中如何被使用。然后我们偷偷看了一眼目标分割的世界。目标检测是一个非常强大的工具，其巨大潜力仍在每天在我们生活的许多领域中被发现。你能想到哪些创新的目标检测用途？在 Twitter 上与我们分享[@PracticalDLBook](https://www.twitter.com/PracticalDLBook)。
