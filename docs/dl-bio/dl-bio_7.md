# 第七章. 生物学的深度学习技巧与窍门

这最后一章汇集了前面章节的常见主题，并提炼了将深度学习技术应用于生物问题的实用策略。在机器学习中，事情第一次尝试就能完美工作的情况很少——甚至第十次尝试也很少。调试是过程的一个预期部分，而不是失败的标志。不要气馁。

在这里，我们分享了一系列帮助我们（以及其他人）在生物学的深度学习挑战中导航的技巧。有些是通过艰难的方式学到的，有些则是从撰写这本书中产生的。这份清单并不全面，但我们希望它能缩短你开发工作模型的道路——并使你在事情出错时更加敏锐。

###### 警告

不要期望你的项目会有稳定的、渐进的改进。深度学习的进展——尤其是与生物数据相关——通常是高度非线性的。你可能会花费几周的时间调试，却看不到明显的进展，只有当你做出一个小小的改变，突然一切都解开时。这是正常的——并不是一个值得关注的问题。

# 简化

当事情不再有意义时，简化。将你的问题简化到最基本的形式——更小的数据集、更浅的模型或更简单的损失函数。在复杂的管道中容易迷失方向，但当你一次隔离一个事物时，调试会容易得多。

一旦你让系统再次工作，你可以逐步引入复杂性。把这想象成逐个转动旋钮，而不是一次性全部转动。这并不华丽，但这是实现进步最可靠的策略之一。

## 简化你的模型

当你的模型有太多花哨的功能时，确定出错的地方变得困难。通常，最有效的调试策略是将模型简化到最基本的形式——更小的数据集、更浅的模型或更简单的损失函数。在复杂的管道中容易迷失方向，但当你一次隔离一个事物时，调试会容易得多。

简化你的架构

复杂的架构可能会使得推理出错的原因变得困难。为了帮助解决这个问题：

消除不必要的层

如果某一层不直接贡献于输入-输出映射——例如那些仅仅增加模型容量而不改变维度的层——在调试时最好将其移除。

直接调用基本层

使用像`nn.Conv`和`nn.Dense`这样的层，而不是自定义块，因为自定义块可能会掩盖错误和内部行为。

减少深度和宽度

如果你的模型有很多层或每层的单元很多，考虑同时减少这两者。较浅的模型更容易调试和理解，尤其是在开发的早期阶段。

移除残差连接

这些连接可能会通过引入层之间的依赖关系以及掩盖它们连接的层中的问题（如初始化不良或梯度问题）来使调试复杂化。

###### 小贴士

最简单的简化方法是减少模型到从输入到输出的直接映射。例如，将输入通过单个 `nn.Dense` 层，看看它是否能过拟合一小批数据。如果这失败了，问题可能不是架构问题，而是你的数据管道、损失函数或优化器的问题。

关闭额外的功能，如归一化和 dropout

这些增加了在早期调试中通常不必要的复杂性。在 Flax 中，你必须显式管理模型状态（例如，批归一化统计）和随机数生成器（RNGs），这很容易导致微妙的问题。

不要使用批归一化

批归一化的复杂性有三重。首先，它在训练和推理期间表现不同。其次，它引入了必须在外部标准梯度更新之外更新的额外状态（运行均值和方差）。第三，它破坏了一个关键假设：大多数层独立地对每个批处理元素进行操作，但批归一化计算整个批次的统计信息。这使得它在与 `vmap` 或分片训练（`pmap`、`pjit`）等工具不兼容，除非你特别小心地同步设备间的统计信息。

跳过 dropout

Dropout 引入了随机性，使得难以确定性能不佳是由于随机性还是更深层次的问题。

简化你的优化器

在基本功能正常工作之前，不要担心尝试不同的优化器或学习率计划。选择一个合理的默认值——比如老式的 Adam，学习率为 `1e-3`，并首先专注于解决更基本的问题。

避免混合精度

虽然像 `bfloat16`、`float16` 或 `TensorFloat32` 这样的低精度数据类型可以提高性能和内存使用（本书范围之外），但它们可能导致难以调试的微妙数值不稳定性。请注意，即使你传递了 `float32` 输入，JAX 也可能默认使用低精度矩阵乘法。为了全局强制使用完整的 `float32` 精度（尤其是在调试期间），请添加 `jax.config.update("jax_default_matmul_precision", "float32")`。

###### 注意

尽管关闭这些更高级的功能可能会导致性能下降（由于欠拟合或过拟合）并感觉像是退步，但一旦基本设置运行正确，你可以系统地重新启用功能以评估它们对模型性能的影响。

## 简化和控制你的环境

你的模型可能没有问题，但你的技术环境可能引入了意外的问题。许多看似深度学习失败的问题实际上只是环境中的小问题——这意味着清理你的设置通常是前进的最快方式。以下是一些需要记住的提示：

整理确定性和可重复性

如果你的实验是可重复的，那么隔离问题会更容易。除了关闭像 dropout 这样的随机组件外，还应考虑：

设置明确的随机种子

对于 JAX 的可重复性，你需要设置`jax.random.PRNGKey(...)`的种子——这控制了模型初始化、dropout 和其他基于 JAX 的操作中的所有随机性。请注意，除非你也在单独使用它们（例如，在数据预处理或非 JAX 组件中），否则你不需要设置 Python 的`random.seed(...)`或 NumPy 的`np.random.seed(...)`。

关闭数据集洗牌

不要对你的训练数据进行洗牌，或者使用固定的随机种子进行洗牌，以保持运行之间示例的一致顺序。

保持环境恒定

避免由外部因素引起的不一致性（例如，使用相同的硬件、库版本和配置）。

简化你的训练循环

尤其是在 JAX 和 Flax 中，训练循环需要手动控制 RNGs、状态和更新——这使得它们功能强大但容易出错。

仅对单个批次进行几步训练

这通常足以捕捉到主要问题。

禁用额外的功能，如日志记录、指标或学习率计划

这些可能会掩盖训练过程中实际发生的事情。

使用固定的输入和随机种子

每次都在相同的批次上运行你的训练步骤。这消除了数据变化引起的可变性，使得错误更容易隔离。

暂时避免高级抽象

如果你正在使用`TrainState`等工具，尝试在核心逻辑工作之前用原始变量更新替换它们。

使你的代码自包含

尤其是在 Colab 或 Jupyter 笔记本等交互式环境中工作时，你的环境很容易在不经意间变得杂乱，充斥着旧的变量或状态。重启内核并将你的代码组织成自包含的函数，这真的有助于调试。

谨慎关闭 JIT 编译

禁用`@jax.jit`可以使训练步骤中的问题更容易识别，因为这使得堆栈跟踪更清晰，行为更明确。然而，请注意：关闭 JIT 可能会导致内存使用量激增和执行速度大幅下降，这使得这种方法对于较大的运行来说不切实际。

用单个 GPU 而不是多个 GPU 进行训练

如果你正在使用 GPU，从只使用一个开始。通过分片（例如，使用`jax.pjit`）的多 GPU 训练引入了许多额外的复杂性。（注意：`pmap`正在被弃用，以`pjit`取而代之，因此最好不要依赖它。）

使用 CPU 进行简单的调试设置

如果你的模型很小，你正在尝试隔离一个错误，使用 CPU 可以去除设备放置和驱动程序问题的复杂性。它速度较慢，但通常更容易推理。只是要注意，一些错误可能仅在加速器上出现。

回到 NumPy

虽然`jax.numpy`与 NumPy 非常相似，但原始的 NumPy 库具有更简单的内部结构，并且通常有更好的错误消息。你不能训练模型或使用自动微分，但为了测试数据转换或验证计算，将 numpy-only 代码隔离和调试在 JAX 之外可能很有用。

这些步骤可能感觉繁琐，但一个干净且受控的环境通常是几天徒劳和 10 分钟找到错误之间的区别。

###### 警告

简要说明：不同的硬件后端在运行 JAX 代码时表现略有不同，尤其是在可重现性方面。

+   *TPU* 在 JAX 中是完全一致的。如果你使用相同的随机种子，你将每次都得到相同的结果。

+   *GPU* 主要是一致的，但某些操作（如卷积或矩阵乘法）在运行中可能略有差异，除非你禁用了某些性能优化。

+   *CPU* 通常是一致的，但细微的非确定性来源（如线程调度）仍然可能存在。

在 JAX 的早期调试中，在 CPU 上运行可以简化事情——但请注意，当你迁移到 GPU 或 TPU 时，错误可能会以不同的方式出现。

## 简化数据和问题

使你的数据集和预测任务更容易简化调试。以下是一些在早期实验中使数据更易于管理的几种方法：

可视化单个示例

实际绘制或打印原始输入和标签——而不仅仅是摘要。你通常会通过这种方式捕捉到诸如编码错误、偏移量错误或图像-标签对不匹配等问题。跳过这一步骤似乎很有吸引力——不要这样做。简单地查看原始数据可以揭示在之后节省数小时的问题。

检查类别平衡

当模型只是进行朴素预测（预测多数类）时，不平衡的数据集可能会让模型看起来像是有问题。在调试过程中考虑子采样或重新平衡。

移除数据增强

增强如裁剪、翻转或添加噪声可能会隐藏潜在问题或使任务变得不必要地困难。在确信核心管道工作正常之前，请关闭它们。

减少类别的数量

不要预测许多类别，将你的任务重新定义为二元分类，首先关注最清晰的信号。

简化输出空间

在类似的情况下，如果你的目标是复杂的（例如，回归标签或结构化输出），尝试将其简化为更简单的东西。例如，预测二进制类别或标签的阈值版本。这有助于在解决完整问题之前验证管道。

使你的数据集更小

大数据集会减慢一切。使用一个小的、具有代表性的子集来捕捉关键结构。

限制你的数据范围

使用你数据集的一个自然切片。例如，限制为单一物种、组织类型、年份或患者群体。这减少了变异性并有助于隔离错误。

检查标签泄漏

尤其是在生物数据集中，标签泄漏可能通过患者 ID、批次号或实验日期等元数据悄悄进入。这可能导致你的模型通过学习捷径而表现出可疑的良好性能。务必检查没有特征或分割意外泄漏目标相关的信息。

使用合成或模拟数据

如果可行，从模拟数据开始，这些数据模仿了你的真实数据集的关键特征，但更容易理解和追踪。你还可以添加受控的噪声（例如，`np.random.normal(0, 1)`）来测试模型的鲁棒性。

简化数据并不是放弃——而是在创建一个受控的测试平台，你可以快速调试、消除不确定性，并在扩展之前建立信心。

## 对单个数据批次进行过拟合

我们之前简要提到过这一点，但值得明确指出：如果你的模型似乎根本学不到什么，一个经典的调试策略是尝试对单个批次进行过拟合——这意味着，在同一个批次上反复运行训练循环，看看模型是否能够记住它。这个测试有助于确认训练循环、损失函数和优化器是否正确连接。

而不是通常的训练循环：

```py
for step in range(num_steps):
  batch = next(training_data)
  state, loss = update_step(state, batch)

```

你可以尝试这样做：

```py
num_debugging_steps = 10
batch = next(training_data)

for step in range(num_debugging_steps):
  state, loss = update_step(state, batch)
  print(f"Loss at step {step}: {loss}")

```

如果你的设置工作正常，模型应该会迅速记住批次，损失在几步之后应该会显著下降。如果不这样，考虑检查以下内容：

学习率问题：

学习率可能太高（导致发散）或太低（导致无法学习）。

冻结的参数或坏的梯度

有时参数根本不会被更新——例如，如果它们由于命名不匹配或作用域问题意外地被排除在`params`字典之外。还要检查梯度——如果它们都是零或 NaN，那是一个线索。

损失函数错误

确保你正在使用适合你任务的正确损失函数（例如，对于分类使用交叉熵），并且它在数值上表现如预期。至少在调试期间，优先使用标准实现而不是自定义的。

模型初始化问题

糟糕或不一致的权重初始化可能会阻碍学习，尤其是在深度网络中。如果你使用自定义模块，请务必检查它们的初始化。

批次大小太小

非常小的批次（例如，1-2 个示例）可能会导致梯度噪声和不稳定的更新。对于调试，使用 8-32 这样的小但合理的批次大小。

静默的形状不匹配或广播错误

这些错误并不总是会崩溃你的代码，但它们可能会无声地破坏你的损失或梯度。打印张量形状并检查中间输出以确认一切如预期的那样对齐。

这是最快且最有信息量的调试步骤之一——如果你的模型无法学习一个小批次，那么现在就别忙着扩展了。

## 回归基础知识

如果这些调试技巧都不起作用，你快要失去理智了，最有效的策略之一是回到一个简单、易于理解的例子，你知道它是有效的：

从一个已建立的例子开始

在一个知名数据集上训练一个简单的模型——例如，在 MNIST 上使用基本 CNN 进行图像问题。这些例子被广泛使用且文档齐全，使它们成为实现端到端工作的可靠方式。

重新生成已知结果

确保你的设置可以成功训练模型并达到预期的性能（例如，MNIST 上的~99%准确率）。这证实了你的训练循环、模型和损失函数是正常工作的。

更换你的数据集

一旦基线工作正常，开始用你自己的数据集替换它。逐步进行，并检查一切是否仍然按预期工作。

逐步增加复杂性

在数据集成后，逐步引入更复杂的组件——如更深的架构或新的训练策略。注意每次更改后的故障情况。

回到教程并训练一个简单的线性模型绝对没有问题——有时那是最快确认你的设置并获得方向的方法。

## 记录一切

良好的日志记录是有效调试和盲目猜测之间的区别。当出现问题的时候，清晰的日志可以帮助你追溯确切发生了什么——而当事情顺利时，它们帮助你理解为什么会这样。

随时间记录训练损失和关键指标

至少，跟踪损失、准确率以及任何相关的任务特定指标（如 auROC 或 auPRC）。这有助于更容易地发现过拟合、不稳定或性能不足。

在固定的时间间隔记录验证性能

观察你的模型在训练过程中的泛化能力有助于早期检测过拟合，并且可以捕捉到验证性能出现偏差但原因不明显的错误。

记录输入、预测和错误

在每个步骤（或 epoch）保存一些输入样本、预测输出和错误。这对于发现系统性的失败（例如，总是错误地分类某个类别）特别有用。

记录配置和超参数

将每次运行的学习率、批量大小、优化器类型和模型架构保存下来。你会忘记的。每个人都会忘记。

使用结构化日志记录器或跟踪工具

使用 TensorBoard、Weights & Biases 或仅仅是结构化的 JSON 日志等工具可以更容易地比较运行情况并理解发生了什么变化。

###### 小贴士

记录日志在当下可能感觉像是额外的负担，但这是你可以做的最好的时间投资之一。即使是最少的日志也能帮助你更快地调试并避免不必要的重新训练。

有一个经验法则：每个新的日志都会揭示你不知道的 bug。

## 寻求帮助

如果你仍然遇到困难，不要害怕寻求帮助。社区论坛如 Stack Overflow 或 GitHub Discussions 是宝贵的资源。与同事或朋友交谈也可能有所帮助——有时只是大声解释问题（橡皮鸭法）就能带来突破。

你还可以使用像 ChatGPT、Gemini 或 Claude 这样的 LLM 来帮助排查问题或探索想法。但请记住，虽然这些模型非常有帮助，但它们的建议并不总是正确的，可能会引入新的错误——所以请务必检查它们生成的任何代码。

# 常见数据问题

通常，问题不在于你的模型有错误——而是数据。数据集中的微妙问题可能会悄无声息地破坏你的学习流程，你可能会发现自己花费数小时调试模型设置，而问题实际上在上游。

## 数据泄漏

如著名物理学家理查德·费曼所说：“第一条原则是，你必须不要欺骗自己——而你是最容易欺骗的人。”当你的模型实际上在作弊时，相信你的模型表现良好是非常容易的。数据泄漏发生在训练期间应该隐藏的信息意外地被模型访问，导致过度乐观的性能指标。

+   明显的情况

    +   在训练集本身上评估模型。这听起来有点愚蠢，但出奇地常见——尤其是在 Kaggle 笔记本这样的非正式环境中。

    +   在验证集或测试集上评估，其中一些示例与训练集重叠。

+   微妙的情况

    +   通过预处理泄漏，例如，在将数据集拆分为训练/验证/测试集之前对整个数据集进行归一化。

    +   泄露未来信息的特征——仅因为它们在预测时不可用，才与目标相关。

这里有一些现实世界的微妙泄漏的例子：

+   你希望将皮肤病变分类为恶性或良性。癌症患者在一家诊所拍照，健康患者在另一家诊所拍照。如果一家诊所使用更亮的照明，你的模型可能会学会将亮度作为癌症的代理。

+   你正在预测蛋白质与基因的结合，并将基因表达水平作为模型的输入。由于基因的表达可能会受到结合的影响，你的模型可能会学会依赖这个代理，而不是你打算让它学习的 DNA 序列特征。

###### 警告

如果你的模型在测试集上表现良好，但无法推广到新的数据集或现实世界场景，数据泄漏是首先要调查的问题之一。

为了避免数据泄漏：

+   总是确保测试数据完全隔离，并且未被训练流程所触及。

+   在项目中途添加新的训练数据时，检查这些数据是否已经出现在你的验证集或测试集中。

+   问问自己：*这个特征在推理时是否可用？* 如果不可用，不要使用它。

+   使用模型解释工具来查看你的模型在做出预测时依赖于哪些数据方面。你所看到的是否符合你的预期，或者模型是否在捕捉到伪影？

## 错误的数据标签

虽然很明显，但错误标记的数据是模型性能不佳的最常见和最令人沮丧的原因之一。一些高风险场景包括：

将标签存储在输入之外

如果标签存储在单独的文件中（例如，包含文件名和类别的 CSV 文件），它们在预处理过程中很容易出现不匹配或错误连接。我们在这方面确实有一些令人尴尬的故事。

独立地洗牌输入和标签

如果你分别对数据和标签进行洗牌，它们将失去同步——默默地。

TensorFlow 数据集中的形状不匹配：

`tf.data.Dataset`不会因为你的标签和输入具有不匹配的形状（例如，数据形状为`(100, 10)`但标签形状为`(43,)`）而必然抱怨。这可能导致沉默的失败，只有在很久以后才会显现。

错误合并表格数据集

在没有验证对齐的情况下（例如，通过 pandas 中的`merge`）合并数据集可能会错误地标记数据行而不会抛出错误。

数据增强管道错误修改标签

增强实际上增加了你的数据集——因此，它也是引入标签损坏的高风险区域。

###### 注意

标签问题的常见警告信号：训练损失在训练过程中确实略有下降，但早期就达到一个高值并趋于平稳，准确率（或其他指标）保持在随机机会水平附近。

例如，如果标签被打乱，你的准确率将徘徊在平衡二元分类的随机基线值（如 50%）附近。

解决标签问题的最佳方法就是花时间检查你的输入-标签对，无论是开始时的原始数据，还是在数据管道的不同阶段。手动检查几个批次。绘制示例并验证标签。这可能感觉有些繁琐——但它是最快捕捉沉默错误的方法之一。

## 不平衡的类别

在生物学中，一个类别远远多于其他类别的情况很常见——比如检测罕见的突变或识别患病细胞。这种类别不平衡本身并不是问题，但训练在不平衡数据上的模型可能会学会总是预测多数类，从而实现欺骗性的高准确率。

警告信号：

+   准确率很高，但少数类的精确率或召回率很差。

+   混淆矩阵显示模型很少预测少数类。

为了解决这个问题：

+   使用类别权重或焦点损失来惩罚模型对少数类的错误更多。焦点损失降低简单示例的权重，并专注于学习困难、被错误分类的示例——特别是当稀有类别容易被压倒时非常有用。

+   重新采样数据——要么对少数类进行过采样，要么对多数类进行欠采样，以减少不平衡。当数据稀缺时，过采样通常更安全，但如果不小心进行，可能会导致过拟合。

+   使用分层抽样来确保在训练、验证和测试分割中保持类别平衡。这意味着分割数据，以便每个子集保持原始的类别比例，避免性能估计偏差。

## 分布偏移

训练数据和测试数据通常来自不同的来源——不同的实验室、物种、测序协议或成像设置。这些偏移可能导致模型学习特定于数据集的伪象，而不是可推广的生物学。

警告信号：

+   强大的验证性能并不能转移到现实世界或外部数据集。

+   训练来预测数据集标签（例如，实验室或批次 ID）的模型表现惊人。

为了捕捉和纠正这个问题：

+   通过数据源着色可视化嵌入（例如，通过 PCA 或 UMAP）以发现聚类。

+   如有必要，使用批量效应校正或领域自适应方法。

+   在混合来自不同来源的数据时要谨慎——明确测试对新环境的泛化能力。

# 生物学特有的陷阱

生物学是一个广阔的领域，充满了复杂的系统和不断发展的数据集。我们无法在此涵盖每一个陷阱，但以下是我们反复遇到的一些常见错误和故障源，这些在基于生物数据构建模型时值得牢记：

版本控制问题

许多生物数据集都与参考版本（例如，基因组构建、基因 ID、转录注释）相关联。混淆基因组版本（例如，GRCh37 与 GRCh38）、基因 ID 版本，甚至生物体存取编号非常危险。确保您的管道所有部分都使用一致的版本——或者明确地在它们之间进行映射。

数据集成挑战

从不同来源组合数据集在生物学中很常见，但可能导致微妙的矛盾：不匹配的标识符、不同的文件格式或不相容的测量单位（例如，读取计数与 TPM 与 RPKM）。在合并数据集之前仔细检查对齐。

生物异质性

生物系统差异很大——跨越个体、细胞类型、种群和物种。在欧洲血统样本上训练的模型可能无法推广到其他血统。同样，在永生化癌细胞系上训练的模型在应用于正常原代细胞时可能会失败。始终考虑您训练数据的范围和限制。

模糊或软标签

生物类别通常没有干净的定义：细胞类型可以是分级或过渡性的，蛋白质结合通常是一个连续的分数，而不是二元的肯定/否定。如果存在硬标签，可能会过分简化实际上是一个连续谱。在这些情况下，性能上限可能反映了标签的模糊性，而不是模型失败。

实验噪声

仅增加更多数据并不总是更好的——低质量的实验数据可能会引入噪声，从而掩盖信号。寻找过滤或去噪的方法。

使用质量指标

许多实验包括内置的质量分数。基于这些分数进行过滤可能会有所帮助。

利用重复

使用实验重复（相同的精确设置，多次进行）或生物重复（相同的方案，但不同的样本）以减少变异性。您可以平均重复信号或使用它们来量化不确定性。

批量效应

实验室条件、试剂批次、测序机器或协议的差异可能会引入强烈的混杂信号。如果不加以考虑，这些技术伪影往往会掩盖真实的生物信号。通过可视化数据（例如，使用按批次着色的 PCA 或 UMAP）来评估批次之间的聚类程度。您还可以训练一个模型来预测批次标签——如果它在实际任务上的表现优于您的模型，那么您的模型可能是在学习特定于批次的噪声。如果需要，可以应用如分位数归一化等归一化技术来减轻这些影响。

# 常见模型问题

并非所有训练失败都源于数据质量差——有时，问题出在模型本身。在本节中，我们将突出模型训练过程中出现的常见问题，从过拟合到梯度不稳定。

在网上有更多详细的深度学习调试指南——例如，谷歌的[深度学习调优剧本](https://oreil.ly/-t96r)。但在这里，我们将回顾一些最常见且实用的故障模式，重点关注如何高效地识别和修复它们。

## 过拟合和泛化能力差

过拟合是深度学习中常见的问题之一。深度神经网络通常具有很高的容量，并且只被要求最小化训练损失——没有任何内置的泛化概念。因此，它们通常在训练数据上表现良好，但在未见过的数据上表现较差。

幸运的是，有一套成熟的正则化技术可以帮助减少过拟合——其中许多我们在本书中已经讨论过：

Dropout

在训练过程中随机禁用网络中的单元，以防止过度依赖任何一条路径。

Weight decay

通过惩罚大权重（L1 或 L2 正则化）来鼓励更简单且泛化能力更强的模型。

提前停止

监控验证性能，并在性能开始下降时停止训练，即使训练损失仍在下降。

数据增强

通过应用小的、有意义的变换（例如，图像的旋转、翻转；序列中的抖动或裁剪）来扩展您的数据集。

Ensembling

将使用不同种子或分割训练的多个模型的预测结果结合起来，作为一种错误校正的形式。即使结合相同的架构也可以显著提高鲁棒性。

此外，检查您的验证数据或测试数据是否与训练数据有根本性的不同（参见上一节）。如果分布确实发生了变化，那么模型可能不是过拟合，而是遇到了它从未训练过的数据。

###### 注意

一个在训练数据上表现良好但在验证数据上表现差的模型很可能是过拟合。一个在两者上都表现不佳的模型可能是欠拟合或设置有误。

## Vanishing or Exploding Gradients

*消失的梯度*，即梯度值接近零，以及*爆炸的梯度*，即梯度值变得过大，可能会严重干扰训练。幸运的是，这些问题相对容易检测。

监控梯度的简单方法是通过计算它们的 L2 范数（也称为欧几里得范数），这会将梯度的整体幅度总结为一个单一的标量。你可以在训练期间记录这个值，并与损失值一起记录。

在 Flax 中计算梯度 L2 范数的方法如下：

```py
@jax.jit
def compute_gradients_l2_norm(grads):
  """Compute L2 norm of gradients."""
  grads_flat = jax.tree_util.tree_leaves(grads)  # Flatten.
  return jnp.sqrt(sum([jnp.sum(jnp.square(g)) for g in grads_flat]))

# Example usage inside a training step:
loss, grads = jax.value_and_grad(loss_fn)(state.params)
grad_norm = compute_gradients_l2_norm(grads)

```

你可以在训练过程中记录这个`grad_norm`值，并将其与损失值一起可视化，以检查梯度行为：

+   如果`grad_norm`接近 0，梯度很可能是消失的。

+   如果它迅速增长或异常波动，你可能遇到了梯度爆炸。

可以尝试的常见修复方法包括：

+   降低学习率或使用学习率调度。

+   使用更好的权重初始化器：根据你的激活函数尝试 Xavier（Glorot）或 He 初始化。

+   归一化激活：批归一化或层归一化有助于稳定梯度流。

+   添加残差连接：这些有助于梯度在深度网络中传播而不会退化。

+   裁剪梯度：这是一个简单但有效的工具，可以限制极端值并防止不稳定。

解决这个问题的方法通常是我们之前提到的，比如降低学习率或使用学习率调度，使用不同的权重初始化器，添加批归一化或层归一化。在块之间添加残差连接也可能有所帮助。最后，明确地将梯度裁剪到固定阈值，以避免过大的值，这可能听起来是一个非常粗略的方法，但它是常见且非常有效的。以下是一个示例实现：

```py
def clip_gradients(grads, threshold):
  """Clip gradients."""
  return jax.tree_map(lambda g: jnp.clip(g, -threshold, threshold), grads)

```

或者，如果你正在使用`optax`，你还可以使用以下方式裁剪梯度：

```py
tx = optax.chain(optax.clip(threshold), optax.adam(learning_rate))

```

## 训练不稳定

与梯度问题相关的一个问题是*训练不稳定*，它可能以多种方式表现出来，包括训练损失的异常波动、验证损失的突然上升，甚至完全发散。通过*发散*，我们指的是模型未能*收敛*到一个稳定的解；相反，损失可能会剧烈波动或变为`NaN`。

训练不稳定通常由几个常见原因引起：

学习率过高

高学习率可能导致优化器超过最小值，导致不稳定。尝试降低学习率或使用从小开始逐渐增加的学习率预热计划。

使用非自适应优化器

自适应优化器如 Adam、RMSProp 或 Adagrad 会根据每个参数调整学习率，并且通常开箱即用就更加稳健。虽然传统的随机梯度下降（SGD）可能有效，但它通常需要更仔细的调整，尤其是在较大的模型或噪声数据的情况下。

梯度爆炸

在深度网络中，梯度可能变得过大并导致更新不稳定。如前所述，应用*梯度裁剪*或使用归一化层（如批归一化或层归一化）来控制这一点。

不适当的批量大小

非常小的批量可能导致梯度估计噪声，使训练不稳定。较大的批量提供更稳定的梯度——通常，尽量使用硬件允许的最大尺寸，尤其是在早期调试期间。

糟糕的权重初始化

不当的初始化可能导致梯度消失或爆炸。Flax 使用 LeCun 正态分布作为`nn.Dense`和`nn.Conv`的默认初始化器，这对于 ReLU 激活函数来说效果很好。但对于非常深的网络或特定的架构，Xavier 或 He 初始化可能表现更好。

激活爆炸

随着网络的加深，中间激活值可能会变得过大，尤其是在 ReLU 或未归一化的输入下。为了防止这种情况，保持激活值居中并限制在合理范围内，最常见的方法是应用批量归一化。

# 模型性能不佳

模型训练并且数据集看起来不错。你已经解决了过拟合问题。一切看起来都很正常。唯一的问题是模型本身并不那么好。

## 你应该做得有多好？

我们在引言中提到了这一点，但值得重申：要判断性能，你需要有上下文。以下是一些锚定你期望的方法：

随机机会性能

随机猜测会达到什么效果？对于回归，你总是预测训练集的均值或中位数会做得有多好？

基准模型

尝试简单的线性回归或逻辑回归。有时这些模型表现出人意料的出色——如果你的深度模型没有打败它们，那么可能有问题。

其他已发表的模型

如果其他人已经在这个任务上工作过，检查他们报告的性能。你可以从他们的工作中获得架构或预处理想法。但要注意——已发表的指标并不总是可信的，它们可能无法直接与你的设置进行比较。

人类表现

人类能完成这个任务吗？专家会做得有多好？这可以帮助你校准期望。

实验重复

生物测量通常由于采样变异性、测量误差或生物本身的变异性而包含噪声。估计你模型性能上限的一种方法是通过检查原始信号在重复实验中的一致性。如果两个重复实验的相关性为 0.85，你的模型不太可能超过这个值。不要期望你的模型比生物学本身更一致。

## 解决模型性能不佳问题

虽然对于表现不佳的模型没有万能的解决方案，但以下策略可以帮助识别问题并提出前进的道路：

检查数据质量

许多模型问题实际上是数据问题。深入研究具体例子——特别是模型出错的地方——寻找不一致性、噪声或标签错误。如果数据高度专业且你不是专家，请咨询专家。

运行错误分析

模型在哪里表现良好？在哪里持续失败？是否存在错误模式——特定的类别、边缘情况或混淆条件？系统错误通常指向缺失的特征或错误的假设。

添加更多数据

如果模型欠拟合或难以处理罕见情况，更多的数据可能会有所帮助。你也可以尝试合成增强、从已知示例中进行引导或生成模拟。观察性能如何随着数据集大小而扩展——平台期可能表明其他瓶颈。

调整超参数

一些超参数比其他参数更重要——从学习率、批量大小、模型深度和正则化强度开始学习。在小范围内使用网格搜索或随机搜索来找到更好的性能设置。

尝试迁移学习

如果存在类似的数据库或任务，可以使用预训练模型作为起点。你可以微调整个模型，或者冻结其特征提取器，在顶部训练一个更小的模型。或者，使用相关模型学习到的嵌入作为输入特征。

###### 小贴士

像往常一样，如果你遇到困难，请回顾基础知识：简化模型，过度拟合单个批次，检查你的标签是否合理，并与基线性能进行比较。许多帮助修复损坏模型的策略也可以阐明为什么一个正在工作的模型还不是一个好的模型。

# 最后的想法

生物学中的深度学习很困难。数据很混乱，目标通常是开放式的，训练有用的模型可能很棘手。但正是这一点使得它令人兴奋。每一次实验，你不仅是在解决技术挑战，你还在帮助推动我们对生命本身理解的边界。

旅程不会总是顺利——模型可能会以令人惊讶的方式失败，你可能会编写灾难性的错误，数据可能包含重大的错误。但如果你保持好奇心，保持模块化，在不确定时简化，保持耐心，你将找到自己的路。

不论你是构建模型来解码基因组、预测蛋白质结构，还是解释显微镜图像，我们都希望这本书能帮助你更有信心地接近这项工作——也许还能让你在过程中享受更多乐趣。

祝你好运，继续前进！
