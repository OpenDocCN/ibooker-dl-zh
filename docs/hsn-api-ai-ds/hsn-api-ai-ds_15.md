# 第十二章。使用 API 与人工智能

> 更多的人工智能意味着更多的 API。
> 
> Frank Kilcommins，SmartBear

在技术圈中，人工智能和 API 有时被视为不同的专业领域。但它们密切相关，并且正在不断接近。在本章中，你将了解人工智能和 API 的重叠方式，你应该培养的一些技能，以及如何构建与人工智能兼容的 API。然后，你将设置你的第三部分项目组合，你将在剩余的章节中使用它。

# 人工智能和 API 的重叠

首先，API 是训练 AI 模型的重要数据源，与数据库和文件一样。一旦模型训练完成，REST API 是使其对用户可用的常见方法。你将在第十三章中训练一个机器学习模型，并使用 REST API 部署它。

同样，基于云的人工智能工具是部署时使用 API 的高级机器学习模型。生成式人工智能、自然语言处理等 AI 工具通常托管在云端，并以 API 的形式提供。你将在第十四章中通过 REST API 调用 Anthropic 大型语言模型（LLM）。

人工智能和 API 之间一个新兴的交叉领域是直接从*生成式人工智能*应用中调用 API，这些应用是使用 LLM 构建的，并通过自然语言与用户交互。这类应用中的一种被称为检索增强生成（RAG）。在 RAG 应用中，程序调用 API 和其他数据源，然后将检索到的信息连同用户提示一起喂给 LLM。这有助于克服 LLM 仅拥有训练信息的知识差距。

另一种生成式人工智能应用使用 LLM 来确定使用哪些 API 端点，在这本书中我们将这类应用称为*代理*应用。LLM 通过解释 OAS 文件、Python 代码或 API 文档中的定义来做出这个决定。你将在第十四章中使用 LangChain，在第十五章中使用 ChatGPT 创建调用 API 的代理人工智能应用。

# 设计与生成式人工智能和 LLM 一起使用的 API

那么，你应该如何设计一个 API，以便生成式人工智能应用可以使用它，正如 Doerrfield 所建议的？这个领域正在迅速变化，但以下是一些适用于 LangChain（第十四章）和 ChatGPT（第十五章）的初步建议。

首先，你需要考虑在没有额外安全措施的情况下，是否可以使用 API 或端点与代理生成式人工智能应用一起使用。LLM 的提供者提供了警告，例如 LLM 不应在“高风险情况下单独使用”（Anthropic Claude 3）以及它们“有时可能提供不准确的信息”（Google NotebookLM）。ChatGPT 的文档只是告诫用户“检查重要信息”。

###### 注意

研究人员和机器学习工程师正在探索额外的方法来应对使用 LLM 进行 API 调用和其他业务任务的风险。一些潜在的安全措施包括在执行前要求人工批准 LLM 推荐的任务，在执行前结合多个 AI 代理审查任务，审查和过滤模型输入输出，以及审查系统运行日志。此外，当 LLM 使用 API 时，需要遵循 API 管理和安全的基础实践——就像传统软件使用 API 时一样。一项关键的安全实践是限制包括 LLM 在内的系统所提供的权限。

对于合适的 API，以下是一些基于我的经验和 Blobr 的[“您的 API 是否为 AI 准备好了？我们的指南和最佳实践”](https://oreil.ly/jxllA)的建议：

限制数据结果的大小。

这对成本和准确性都很重要。从成本角度来看，模型提供商按处理*令牌*收费，这些令牌是文本块。这些令牌的大小不同，但底线是相同的：模型处理的数据越多，成本就越高。除了成本之外，使用 ChatGPT 的开发者发现，它难以从 API 返回的非常大的数据集中进行计算。如果您使用模型进行计算，限制数据结果的大小可以提高其准确性。

实现这一点有几种方法。与其返回与实体相关的所有字段，不如只返回关键字段。如果一个 API 在集合中返回子记录（例如，`product.orders`），则从结果中排除这些记录，并在单独的端点中提供它们。添加参数、过滤器以及分页来缩小 API 调用中特定记录的范围。

在整个 API 中保持数据结构的一致性。

API 越可预测，AI 就能越准确地使用它。通过在您的 API 中重用模式并在 OAS 文件中定义它们，您将帮助 LLM 了解结果中可以期待什么。您在第一部分中创建的 API 使用了 Pydantic，它强制执行标准模式并在您的 OAS 文件中发布。

提供软件开发工具包（SDK）。

提供 SDK 是一种提供适合 AI 应用程序的端点子集和定制 API 调用的方法。SDK 还可以包括 API 调用和参数的详细说明，帮助 LLM 理解其用法。在第十四章中，您将使用 swcpy SDK 与 LangChain 和 LangGraph 一起使用。

定制您的 OpenAPI 规范（OAS）。

一些使用生成式 AI 的方法支持读取 OAS 文件以推断 API 端点进行调用。您可以使用 AI 合适的端点和每个端点及参数的详细描述来创建定制的 OAS 文件，这些描述有助于 LLM 推断其含义。OAS 文件中的端点应具有独特且清晰的操作 ID。

提供一个用于摘要统计的单独端点。

如果用户询问人工智能关于摘要信息和计数的问题，LLM 的行为可能会不稳定。它可能会尝试扫描 API 中的每条记录，它可能会只查看记录标识符并推断这是计数，或者它可能会尝试完全不同的事情。提供专用端点可以减少一些猜测。

提供一个不依赖于记录标识符的搜索端点。

LLM 更擅长使用语言而不是数字。它们喜欢根据用户可能提出的信息进行搜索。

# 定义人工智能

> 人工智能是一种使计算机和机器能够模拟人类学习、理解、问题解决、决策、创造力和自主性的技术。
> 
> “什么是人工智能（AI）？”，Cole Stryker 和 Eda Kavlakoglu，IBM 公司，2024

除了人工智能的正式定义之外，今天的非正式定义是：人工智能是一种能够进行类似人类对话和完成类似人类任务的计算机程序。人工智能包括*专家系统*，这些系统已经存在了几十年。这些是复杂的基于规则的系统，可以执行类似人类任务。现代人工智能专注于*机器学习*，其中研究人员指导模型的训练，但并不明确编程它们。使用大型语言模型（LLM）的生成式人工智能是机器学习的一个重要应用。

图 12-1 展示了这些术语是如何相互关联的。

![人工智能术语图](img/haad_1201.png)

###### 图 12-1\. 人工智能术语图

# 生成式人工智能和大型语言模型（LLM）

尽管机器学习在许多不同的应用中使用，但近年来公众的关注主要集中在生成式人工智能上。这些应用能够根据文本提示生成文本、音乐和视频的能力，导致了在 OpenAI 的 ChatGPT、微软的 Copilot、谷歌的 Gemini 以及其他许多软件应用中的快速采用。

尽管这些应用的能力令人印象深刻，但生成式人工智能也伴随着许多风险和限制。一些流行模型的提供者包括关于偏见、幻觉、错误和有害内容的警告。这些是使用它们的开发者应该认真对待的主要风险。第十四章和 15 章有关于这些章节中展示的模型的风险和限制的更多细节。

# 创建具有代理性的人工智能应用

如 Doerrfield 所提到的，人工智能代理处于人工智能研究和开发的前沿。*代理*是一种使用 LLM 控制应用程序流程的软件。LLM 越能自主地控制系统，系统就越具有*代理性*。

使用大型语言模型 (LLM) 创建 AI 代理是一个新兴领域，已经发布了各种不同的工具来创建代理或编排多个代理以执行任务。表 12-1 列出了几个用于开发代理和基于 LLM 的应用程序的开源框架。

表 12-1\. AI 代理框架

| 软件 | 支持的编程语言 |
| --- | --- |
| Autogen | Python, dotnet |
| CrewAI | Python |
| LangChain/LangGraph | Python |
| LlamaIndex | Python, Typescript |
| PydanticAI | Python |
| Vercel AI SDK | Typescript |

您将在 第十四章 中使用 LangChain 和 LangGraph 来调用 API。

# 介绍您的第三部分投资组合项目

您将创建一个投资组合项目，以展示您使用 API 和 AI 的能力。以下是您接下来工作的概述：

+   第十三章：部署机器学习 API

+   第十四章：使用 LangChain 与 API 交互

+   第十五章：使用 ChatGPT 调用您的 API

这些任务中的每一个都将使您以独特的方式展示您的 API 和 AI 技能。

# 开始使用您的 GitHub Codespace

您将继续使用 GitHub Codespaces 来开发第三部分的所有代码。如果您还没有创建 GitHub 账户，请现在创建一个。

## 克隆第三部分仓库

所有 第三部分 的代码示例都包含在这本书的 GitHub 仓库中 [this book’s GitHub repository](https://github.com/handsonapibook/api-book-part-three)。

要克隆仓库，请登录到 GitHub 并转到 [GitHub 导入仓库页面](https://github.com/new/import)。在此页面的字段中输入以下信息：

+   您源代码仓库的 URL：**`https://github.com/handsonapibook/api-book-part-three`**

+   您源代码仓库的用户名：留空。

+   您源代码仓库的访问令牌或密码：留空。

+   仓库名称：**`ai-project`**

+   公开：选择此选项，以便您可以分享您正在进行的工作的结果。

点击“开始导入”。导入过程将开始，并显示“准备您的新的仓库”消息。几分钟后，您将收到一封电子邮件通知您导入已完成。点击链接访问您新克隆的仓库。

## 启动您的 GitHub Codespace

在您的新仓库中，点击“代码”按钮并选择“Codespaces”标签。点击“在主分支上创建 codespace”。您应该会看到一个状态为“设置您的 codespace”的页面。在设置过程中，您的 Codespace 窗口将被打开。当设置完成时，您的显示将类似于 图 12-2。

![第三部分的 GitHub Codespace](img/haad_1202.png)

###### 图 12-2\. 第三部分的 GitHub Codespace

您的 Codespace 现在已通过克隆的仓库创建。这将是你使用本书第三部分的环境。打开 [GitHub Codespaces 页面](https://oreil.ly/nLbqH)，滚动到页面底部找到这个新的 Codespace，点击名称右侧的省略号，选择重命名。输入名称 **`Part 3 Portfolio project codespace`** 并点击保存。您应该会看到消息“您的 codespace *Part 3 Portfolio project codespace* 已更新。”再次点击省略号，然后点击“自动删除 codespace”旁边的彩带以关闭自动删除功能。

###### 注意

为了在页面上节省空间，我已经在我的 Codespace 的终端提示符中修剪了目录列表。您可以在您的 Codespace 中通过编辑 VS Code 中的 */home/codespace/.bashrc* 文件来实现这一点。找到 `export PROMPT_DIRTRIM` 语句并将其设置为 `export PROMPT_DIRTRIM=1`。要首次加载这些值，请执行以下终端命令：`source ~/.bashrc`。

# 其他资源

要查看 API 人工智能兼容性的 100 分评分卡，请阅读 Blobr 的文章“Is Your API AI-ready? Our Guidelines and Best Practices” [链接](https://oreil.ly/JLaK1)。

要了解如何绕过 GPT 的限制的示例，请阅读 Kade Halabuza 的文章“Syntax Sunday: Custom API Wrapper for GPTs” [链接](https://oreil.ly/khh0J)。

要了解更多关于 AI 和 API 可能的未来，请阅读 Peter Schroeder 的文章“AI + APIs — What 12 Experts Think The Future Holds” [链接](https://oreil.ly/t2lxH)。

# 摘要

本章介绍了人工智能的基础知识，并解释了它与 API 的关系。

在 第十三章 中，您将创建一个机器学习模型并使用 FastAPI 进行部署。
