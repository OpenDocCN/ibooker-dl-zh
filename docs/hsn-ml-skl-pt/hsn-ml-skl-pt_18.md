# 第十六章. 视觉和多模态转换器

在上一章中，我们从零开始实现了一个转换器并将其转变为翻译系统，然后我们探讨了仅编码器模型用于 NLU，仅解码器模型用于 NLG，我们甚至构建了一个小型的聊天机器人——这是一段相当漫长的旅程！然而，关于转换器还有很多话要说。特别是，我们迄今为止只处理了文本，但转换器实际上在处理各种输入方面表现出色。在本章中，我们将介绍能够处理图像的 *视觉转换器*（ViTs），然后是能够处理包括文本、图像、音频、视频、机器人传感器和执行器以及任何类型数据的 *多模态转换器*。

在本章的第一部分，我们将讨论一些最有影响力的纯视觉转换器：

DETR (检测转换器)

一个用于目标检测的早期编码器-解码器转换器。

原始的 ViT (视觉转换器)

这个里程碑式的仅编码器转换器将图像块视为词标记，如果在大数据集上训练，可以达到最先进的状态。

DeiT (数据高效图像转换器)

使用蒸馏在规模上训练的更具数据效率的 ViT。

PVT (金字塔视觉转换器)

一个可以生成多尺度特征图用于语义分割和其他密集预测任务分层模型。

Swin Transformer (移动窗口转换器)

一个速度更快的分层模型。

DINO (无标签的自蒸馏)

这引入了一种新颖的自监督视觉表示学习方法。

在本章的第二部分，我们将深入探讨多模态转换器：

VideoBERT

一种训练用于处理文本和视频标记的 BERT 模型。

ViLBERT (视觉语言 BERT)

一个用于图像加文本的双编码器模型，它引入了共注意力（即双向交叉注意力）。

CLIP (对比语言-图像预训练)

这又是一个使用对比预训练训练的图像加文本双编码器模型。

DALL·E （对艺术家萨尔瓦多·达利和皮克斯角色瓦力名字的恶搞）

一种能够从文本提示生成图像的模型。

Perceiver

它使用交叉注意力技巧有效地将任何高分辨率模态压缩成短序列。

Perceiver IO (输入/输出)

为 Perceiver 添加了一个灵活的输出机制，使用类似的交叉注意力技巧。

Flamingo

它不是从头开始，而是重用了两个大型预训练模型——一个用于视觉，一个用于语言（两者都冻结）——并使用名为 Resampler 的 Perceiver 风格适配器将它们连接起来。这种架构使得开放式的视觉对话成为可能。

BLIP-2 (自举语言-图像预训练)

这又是一个开放式的视觉对话模型，它重用了两个大型预训练模型，使用轻量级的查询转换器（Q-Former）将它们连接起来，并采用了一种强大的两阶段训练方法，具有多个训练目标。

所以，打开灯光，变换器即将睁开眼睛。

# 视觉变换器

视觉变换器并非凭空出现：在它们被发明之前，已经有了带视觉注意力的 RNN 和混合 CNN-Transformer 模型。在我们深入研究一些最有影响力的 ViT 之前，让我们看看这些 ViT 的祖先。

## 带视觉注意力的 RNN

注意力机制在 NLP 之外的第一项应用之一是使用[视觉注意力](https://homl.info/visualattention)生成图像标题。⁠^(1) 在这里，卷积神经网络首先处理图像并输出一些特征图，然后一个配备注意力机制的解码器 RNN 逐个生成标题。

解码器在每个解码步骤使用一个注意力层来专注于图像的恰当部分。例如，在图 16-1 中，模型生成了标题“一位女士在公园里扔飞盘”，你可以看到当解码器即将输出“飞盘”这个词时，它关注了输入图像的哪个部分：很明显，大部分注意力都集中在飞盘上。

![一位穿绿色衣服的女士在公园里扔一个红色的飞盘，当在标题中生成“飞盘”这个词时，注意力层突出了飞盘。](img/hmls_1601.png)

###### 图 16-1\. 视觉注意力：输入图像（左）和模型在输出“飞盘”这个词之前关注的焦点（右）⁠^(2)

变换器被发明后，它们很快就被应用于视觉任务，通常是通过替换现有架构中的 RNN（例如，用于图像标题）。然而，大部分视觉工作仍然由 CNN 完成，因此尽管它们是用于视觉任务的变换器，我们通常不把它们视为 ViT。*检测变换器*（DETR）是这种类型的典型例子。

## DETR：用于目标检测的 CNN-Transformer 混合模型

2020 年 5 月，一组 Facebook 研究人员提出了一种用于目标检测的混合 CNN-变换器架构，命名为[*检测变换器*](https://homl.info/detr)（DETR，见图 16-2）。⁠^(4) CNN 首先处理输入图像并输出一系列特征图，然后这些特征图被转换成一系列视觉标记，这些标记被输入到编码器-解码器变换器中，最后变换器输出一系列边界框预测。

在某个时刻，肯定有人会想知道我们是否可以完全去掉 CNN。毕竟，注意力就是一切，对吧？在 DETR 之后几个月，原始的 ViT 诞生了。

![检测变换器（DETR）过程图解，展示了如何使用视觉标记和位置编码来以置信度识别寺庙和树木等物体。](img/hmls_1602.png)

###### 图 16-2\. 用于目标检测的检测变换器（DETR）

## 原始 ViT

2020 年 10 月，一组谷歌研究人员发布了一篇[论文](https://homl.info/vit)⁠^(5)，介绍了第一个没有 CNN 的视觉变换器（参见图 16-3)。它简单地被命名为*视觉变换器*（ViT）。这个想法非常简单：将图像切割成小的 16 × 16 补丁，并将补丁序列视为一个词表示序列。实际上，这篇论文的标题是“一张图片值 16 × 16 个词”。

要更精确地说，这些补丁首先被展平成 16 × 16 × 3 = 768 维度的向量（3 代表 RGB 颜色通道）。例如，一个 224 × 224 的图像被切割成 14 × 14 = 196 个补丁，因此我们得到了 196 个每个维度为 768 的向量。这些向量随后通过一个线性层，将向量投影到变换器的嵌入大小。得到的向量序列可以像处理词嵌入序列一样处理：添加可学习的位置嵌入，然后将结果传递给变换器，这是一个常规的仅编码器模型。在序列的开始插入一个具有可训练表示的类别标记，并在相应的输出上方添加一个分类头（即这是 BERT 风格的分类）。

就这样！这个模型在 ImageNet 图像分类上击败了当时的最佳水平，但公平地说，作者们不得不使用超过 3 亿张额外的图像进行训练。这很合理，因为变换器不像卷积神经网络那样具有许多*归纳偏差*，因此它们需要额外的数据来学习 CNN 隐含假设的东西。

###### 注意

归纳偏差是模型由于其架构而做出的隐含假设。例如，线性模型隐含地假设数据是线性的。CNN 具有平移不变性，因此它们隐含地假设在一个位置学习到的模式在其他位置也可能有用。它们还强烈倾向于局部性。RNN 隐含地假设输入是有序的，并且最近的标记比旧的标记更重要。一个模型具有的归纳偏差越多，假设它们是正确的，那么模型所需的训练数据就越少。但如果隐含的假设是错误的，那么即使在大数据集上训练，模型也可能表现不佳。

![说明视觉变换器（ViT）模型图，展示了将图像补丁转换为分类标记嵌入的过程。](img/hmls_1603.png)

###### 图 16-3\. 用于分类的视觉变换器（ViT）

现在你已经知道了一切，可以从头开始实现一个 ViT 了！

### 使用 PyTorch 从头开始实现 ViT

我们将首先实现一个自定义模块来处理补丁嵌入。为此，我们可以实际使用一个 `nn.Conv2d` 模块，将 `kernel_size` 和 `stride` 都设置为补丁大小（16）。这相当于将图像切割成补丁，展平它们，并通过一个线性层（然后重塑结果）。这正是我们所需要的！

```py
import torch
import torch.nn as nn

class PatchEmbedding(nn.Module):
    def __init__(self, in_channels, embed_dim, patch_size=16):
        super().__init__()
        self.conv2d = nn.Conv2d(embed_dim, in_channels,
                                kernel_size=patch_size, stride=patch_size)
    def forward(self, X):
        X = self.conv2d(X)  # shape [B=Batch, C=Channels, H=Height, W=Width]
        X = X.flatten(start_dim=2)  # shape [B, C, H * W]
        return X.transpose(1, 2)  # shape [B, H * W, C]
```

在卷积层之后，我们必须展平空间维度并转置最后两个维度，以确保嵌入维度最终位于最后，这是 `nn.TransformerEncoder` 模块所期望的。现在我们已准备好实现我们的 ViT 模型：

```py
class ViT(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_channels=3,
                 num_classes=1000, embed_dim=768, depth=12, num_heads=12,
                 ff_dim=3072, dropout=0.1):
        super().__init__()
        self.patch_embed = PatchEmbedding(embed_dim, in_channels, patch_size)
        cls_init = torch.randn(1, 1, embed_dim) * 0.02
        self.cls_token = nn.Parameter(cls_init)  # shape [1, 1, E=embed_dim]
        num_patches = (img_size // patch_size) ** 2  # num_patches (denoted L)
        pos_init = torch.randn(1, num_patches + 1, embed_dim) * 0.02
        self.pos_embed = nn.Parameter(pos_init)  # shape [1, 1 + L, E]
        self.dropout = nn.Dropout(p=dropout)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim,
            dropout=dropout, activation="gelu", batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)
        self.layer_norm = nn.LayerNorm(embed_dim)
        self.output = nn.Linear(embed_dim, num_classes)

    def forward(self, X):
        Z = self.patch_embed(X)  # shape [B, L, E]
        cls_expd = self.cls_token.expand(Z.shape[0], -1, -1)  # shape [B, 1, E]
        Z = torch.cat((cls_expd, Z), dim=1)  # shape [B, 1 + L, E]
        Z = Z + self.pos_embed
        Z = self.dropout(Z)
        Z = self.encoder(Z)  # shape [B, 1 + L, E]
        Z = self.layer_norm(Z[:, 0])  # shape [B, E]
        logits = self.output(Z) # shape [B, C]
        return logits
```

让我们来看一下这段代码：

+   构造函数首先创建 `PatchEmbedding` 模块。

+   然后它创建类别标记的可训练嵌入，使用具有小标准差（0.02 是常见的）的正态分布进行初始化。其形状为 [1, 1, *E*]，其中 *E* 是嵌入维度。

+   接下来，我们初始化位置嵌入，形状为 [1, 1 + *L*, *E*]，其中 *L* 是补丁标记的数量。我们需要一个额外的位置嵌入来处理类别标记，因此是 1 + *L*。同样，我们使用具有小标准差的正态分布来初始化它。

+   接下来，我们创建其他模块：`nn.Dropout`、`nn.TransformerEncoder`（基于 `nn.TransformerEncoderLayer`）、`nn.LayerNorm` 以及我们将用作分类头的输出线性层。

+   在 `forward()` 方法中，我们首先创建补丁标记。

+   然后我们使用 `expand()` 方法在批处理轴上复制类别标记，并将补丁标记连接起来。这确保了每个补丁标记序列都以类别标记开头。

+   其余部分都很直接：我们添加位置嵌入，应用一些 dropout，运行编码器，只保留类别标记的输出 (`Z[:, 0]`) 并对其进行归一化，最后通过输出层，该层产生 logits。

您可以创建模型并使用随机的一批图像进行测试，如下所示：

```py
vit_model = ViT(
    img_size=224, patch_size=16, in_channels=3, num_classes=1000, embed_dim=768,
    depth=12, num_heads=12, ff_dim=3072, dropout=0.1)
batch = torch.randn(4, 3, 224, 224)
logits = vit_model(batch)  # shape [4, 1000]
```

您可以使用 `nn.CrossEntropyLoss` 训练这个模型，就像通常一样。然而，这会花费相当长的时间，所以除非您的图像数据集非常具有领域特定性，否则您通常更好的做法是使用 Transformers 库下载预训练的 ViT，然后在您的数据集上对其进行微调。让我们看看如何操作。

### 使用 Transformers 库微调预训练的 ViT

让我们下载一个小型的预训练的 ViT 并在牛津-IIIT Pet 数据集上对其进行微调，该数据集包含超过 7,000 张宠物图片，分为 37 个不同的类别。首先，让我们下载数据集：

```py
from datasets import load_dataset

pets = load_dataset("timm/oxford-iiit-pet")
```

接下来，让我们下载 ViT：

```py
from transformers import ViTForImageClassification, AutoImageProcessor

model_id = "google/vit-base-patch16-224-in21k"
vit_model = ViTForImageClassification.from_pretrained(model_id, num_labels=37)
vit_processor = AutoImageProcessor.from_pretrained(model_id, use_fast=True)
```

我们正在加载一个在 ImageNet-21k 数据集上预训练的基础 ViT 模型。这个数据集包含大约 1400 万张图片，跨越 21,800 多个类别。我们使用 `ViTForImageClassification` 类，该类会自动用一个新的（未训练的）分类头替换原始的分类头，以适应所需的类别数量。这就是我们现在需要训练的部分。

我们还加载了该模型的图像处理器。我们将使用它来预处理每个图像，正如模型所期望的那样：它将被缩放到 224 × 224，像素值将被归一化到-1 和 1 之间，通道维度将被移动到空间维度之前。我们还设置了`use_fast=True`，因为有一个快速的图像处理器实现可用，所以我们不妨使用它。处理器接收一个图像作为输入，并返回一个包含“pixel_values”条目的字典，该条目等于预处理后的图像。

接下来，我们需要一个数据收集器，它将预处理一批中的所有图像，并将图像和标签作为 PyTorch 张量返回：

```py
def vit_collate_fn(batch):
    images = [example["image"] for example in batch]
    labels = [example["label"] for example in batch]
    inputs = vit_processor(images, return_tensors="pt", do_convert_rgb=True)
    inputs["labels"] = torch.tensor(labels)
    return inputs
```

我们设置了`do_convert_rgb=True`，因为模型期望 RGB 图像，但数据集中的一些图像是 RGBA（即它们有一个额外的透明度通道），因此我们必须强制转换为 RGB 以避免训练过程中出现错误。现在我们准备好使用熟悉的 Hugging Face 训练 API 来训练我们的模型：

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments("my_pets_vit", per_device_train_batch_size=16,
                         eval_strategy="epoch", num_train_epochs=3,
                         remove_unused_columns=False)
trainer = Trainer(model=vit_model, args=args, data_collator=vit_collate_fn,
                  train_dataset=pets["train"], eval_dataset=pets["test"])
train_output = trainer.train()
```

###### 警告

默认情况下，训练器会自动移除`forward()`方法未使用的输入属性：我们的模型期望`pixel_values`和可选的`labels`，但任何其他内容都将被丢弃，包括`"image"`属性。由于未使用的属性在调用`collate_fn()`函数之前被丢弃，因此`example["image"]`代码将导致错误。这就是为什么我们必须设置`remove_unused_columns=False`。

仅经过 3 个 epoch，我们的 ViT 模型就达到了约 91.8%的准确率。通过一些数据增强和更多的训练，你可能会达到 93%到 95%的准确率，这接近了当前的最佳水平。太棒了！但我们才刚刚开始：自 2020 年以来，ViT 在许多方面都得到了改进。特别是，现在可以使用蒸馏以更高效的方式从头开始训练它们。让我们看看如何。

## 数据高效图像转换器

在谷歌的 ViT 论文发布仅仅两个月后，一支 Facebook 的研究团队发布了[*数据高效图像转换器*](https://homl.info/deit) (DeiT)。⁠^(6) 他们的 DeiT 模型在 ImageNet 上取得了有竞争力的结果，而无需为训练提供任何额外的数据。该模型的架构几乎与原始 ViT 相同（参见图 16-4），但作者们使用了一种蒸馏技术，将知识从教师模型传递到他们的学生 ViT 模型（蒸馏技术在第十五章中介绍）。

作者使用了一个冻结的、最先进的 CNN 作为教师模型。在训练过程中，他们向学生 ViT 模型添加了一个特殊的蒸馏标记。就像类别标记一样，蒸馏标记的表示是可训练的，其输出通过一个专门的分类头。两个分类头（类别标记和蒸馏标记）同时训练，都使用交叉熵损失，但类别标记的分类头使用正常的硬目标（即 one-hot 向量）进行训练，而蒸馏头使用教师模型输出的软目标进行训练。最终的损失是两个分类损失（通常权重相等）的加权总和。在推理时间，蒸馏标记及其分类头被丢弃。这就是全部内容！如果你在相同的 pets 数据集上微调一个 DeiT 模型，使用`model_id = "facebook/deit-base-distilled-patch16-224"`和`DeiTForImageClassification`，你只需经过三个 epoch 就应该能达到大约 94.4%的验证准确率。

![说明数据高效图像 Transformer (DeiT)过程的图，展示了从图像块通过线性变换、编码位置和类别标记、蒸馏标记包含，到具有来自 CNN 的硬和软目标输出的分类头的过程。](img/hmls_1604.png)

###### 图 16-4\. 数据高效图像 Transformer (DeiT) = ViT + 蒸馏

到目前为止，我们只使用了 ViTs 进行分类任务，那么对于密集预测任务，如目标检测或语义分割（在第十二章中介绍）怎么办呢？为此，ViT 架构需要稍作调整；欢迎来到分层视觉 Transformer。

## 用于密集预测任务的 Pyramid Vision Transformer

2021 年是 ViTs（视觉 Transformer）丰收的一年：几乎每隔一周就有新的模型推动了技术的进步。一个重要的里程碑是 2021 年 2 月发布的[Pyramid Vision Transformer (PVT)](https://homl.info/pvt)，由南京大学、香港大学、IIAI 和 SenseTime Research 的研究团队开发。他们指出，原始的 ViT 架构在分类任务上表现良好，但在需要精细分辨率的大规模预测任务上表现不佳。为了解决这个问题，他们提出了一种金字塔架构，其中图像被处理成越来越小但越来越深的图像（即语义上更丰富），这与 CNN 非常相似。图 16-5 展示了如何将一个 256 × 192 像素、3 个通道（RGB）的图像首先转换成一个 64 × 48 像素、64 个通道的图像，然后转换成一个 32 × 24 像素、128 个通道的图像，接着是一个 16 × 12 像素、320 个通道的图像，最后是一个 8 × 6 像素、512 个通道的图像。

![说明金字塔视觉 Transformer 过程的图解，展示了图像如何通过多个阶段进行转换，通道数增加，空间分辨率降低，最终用于密集预测任务。](img/hmls_1605.png)

###### 图 16-5\. 用于密集预测任务的金字塔视觉 Transformer

在金字塔的每一层，输入图像的处理方式与常规 ViT 非常相似。它首先被切割成补丁，并转换成一系列补丁标记，然后添加可训练的位置嵌入，最后将得到的标记通过一个仅编码器的 transformer，由多个编码器层组成。

由于编码器输出一系列向量（即，上下文化的嵌入），这个序列必须被重塑成一个 *网格* 的向量，然后可以将其视为一个图像（具有许多通道）并传递到金字塔的下一层。例如，第一层的编码器接收一个包含 3,072 个补丁标记的序列，因为图像被切割成一个 64 × 48 的 4 × 4 补丁网格（64 × 48 = 3,072）。每个补丁标记表示为一个 64 维的向量。编码器还输出 3,072 个向量（即，上下文化的嵌入），每个 64 维，并且它们被组织成一个 64 × 48 的网格。这给我们提供了一个 64 × 48 的图像，具有 64 个通道，可以传递到下一层。在金字塔的第二、三、四层，补丁标记分别是 128 维、320 维和 512 维。

重要的是，这些补丁的大小比原始的 ViT 小得多：在第一层，它们只是 4 × 4，而在第二、三、四层则是 2 × 2。这些微小的补丁提供了更高的空间分辨率，这对于密集预测任务至关重要。然而，这也带来了代价：较小的补丁意味着需要更多的它们，而且由于多头注意力的复杂度是二次的，对 ViT 的简单适配将需要大量的计算。这就是为什么 PVT 作者引入了 *空间减少注意力*（SRA）：它就像 MHA 一样，只是键和值首先在空间上进行了减少（但不是查询）。为此，作者提出了一系列操作，通常实现为一个步长卷积层，随后是层归一化（尽管一些实现使用平均池化层代替）。

让我们看看 SRA 在金字塔第一层的影响。这里有 3,072 个补丁标记。在常规 MHA 中，每个标记都会关注每个标记，因此我们需要计算 3,072²个注意力分数：这超过 900 万个分数！在 SRA 中，查询保持不变，因此仍然涉及 3,072 个标记，但键和值在空间上减少了 8 倍，水平和垂直方向都是如此（在金字塔的 2、3 和 4 层，减少因子分别是 4、2 和 1）。因此，不再是 3,072 个标记代表一个 64 × 48 的网格，键和值仅由 48 个标记组成，代表一个 8 × 6 的网格（因为 64 / 8 = 8 和 48 / 8 = 6）。因此，我们只需要计算 3,072 × 48 = 147,456 个注意力分数：这比计算成本降低了 64 倍。而且好消息是，这不会影响输出分辨率，因为我们根本就没有减少查询：编码器仍然输出 3,072 个标记，代表一个 64 × 48 的图像。

好的，所以 PVT 模型接收一个图像并输出四个逐渐变小且更深的图像。那么接下来呢？我们如何使用这些多尺度特征图来实现目标检测或其他密集预测任务呢？嗯，没有必要重新发明轮子：现有的解决方案通常涉及一个生成多尺度特征图的 CNN 骨干网络，因此我们可以简单地将其替换为 PVT（通常在 ImageNet 上预训练）。例如，我们可以使用 FCN 方法进行语义分割（在第十二章末尾介绍），通过上采样和组合 PVT 输出的多尺度特征图，并添加一个最终的分类头以输出每个像素的一个类别。同样，我们可以使用 Mask R-CNN 进行目标检测和实例分割，用 PVT 替换其 CNN 骨干网络。

简而言之，PVT 的层次结构是视觉 Transformer 的一个重大里程碑，尽管有空间减少注意力，但它仍然计算成本高昂。Swin Transformer 在一个月后发布，具有更高的可扩展性。让我们看看原因。

## Swin Transformer：一种快速且通用的 ViT

2021 年 3 月，一支微软研究团队发布了[Swin Transformer](https://homl.info/swin)。⁠^(8) 就像 PVT 一样，它具有层次结构，生成可用于密集预测任务的多尺度特征图。但 Swin 使用了一种非常不同的多头注意力变体：每个补丁只关注同一窗口内的补丁。这被称为*基于窗口的多头自注意力*（W-MSA），它允许自注意力的成本与图像大小（即面积）线性缩放，而不是平方缩放。

例如，在图 16-6 的左侧，图像被切割成 28 × 28 的补丁网格，这些补丁被分组到非重叠的窗口中。在 Swin 金字塔的第一层，补丁通常是 4 × 4 像素，每个窗口包含一个 7 × 7 的补丁网格。因此总共有 784 个补丁标记（28 × 28），但每个标记只关注 49 个标记（7 × 7），所以 W-MSA 层只需要计算 784 × 49 = 38,416 个关注分数，而不是常规 MHA 的 784² = 614,656 个分数。

最重要的是，如果我们将图像的宽度和高度加倍，补丁标记的数量将增加到四倍，但每个标记仍然只关注 49 个标记，所以我们只需要计算 4 倍的关注分数：Swin Transformer 的计算成本与图像面积成线性关系，因此它可以处理大图像。相反，ViT、DeiT 和 PVT 都呈二次方增长：如果你将图像的宽度和高度加倍，面积将增加到四倍，计算成本将乘以 16！因此，这些模型对于非常大的图像来说速度太慢，这意味着你必须首先下采样图像，这可能会损害模型的准确性，尤其是在密集预测任务中。

![Swin Transformer 架构图，展示了常规多头自注意力（S-MSA）、移位窗口多头自注意力（SW-MSA）和优化 SW-MSA，突出显示了不同的窗口配置如何覆盖相同的图像。](img/hmls_1606.png)

###### 图 16-6. Swin Transformer：交替使用 W-MSA（左）和 SW-MSA（中）；SW-MSA 可以被优化为需要与 W-MSA 相同数量的窗口（右）

但等等！如果每个标记只关注同一窗口内的补丁，我们如何希望捕捉到长距离模式呢？答案就在架构的名字中，Swin，代表*移位窗口*：每隔一个编码器层使用*移位 W-MSA*（SW-MSA），这就像 W-MSA 一样，只是窗口偏移了半个窗口大小。正如你在图 16-6 的中间部分可以看到的，窗口向右下角偏移了 3 个补丁（因为 7 的一半是 3.5，我们将其四舍五入到 3）。这有什么帮助呢？好吧，在上一层中分别位于不同窗口的邻近补丁现在在同一窗口中，因此它们可以相互看到。通过交替使用 W-MSA 和 SW-MSA，图像任何部分的信息可以逐渐传播到整个图像。此外，由于架构是分层的，随着我们向上金字塔移动，补丁变得越来越粗糙，因此信息可以传播得越来越快。

SW-MSA 的简单实现需要处理许多额外的窗口。例如，如果你比较图 16-6 中的 W-MSA 和 SW-MSA，你可以看到 W-MSA 使用 16 个窗口，而 SW-MSA 使用 25 个（至少在这个例子中）。为了避免这种额外成本，作者提出了一个优化的实现：不是移动窗口，而是移动图像本身并将其包裹在边缘，如图图 16-6 的右侧所示。这样，我们回到了 16 个窗口。然而，这需要对包含包裹补丁的边缘窗口进行仔细的掩码处理；例如，标记为①、②、③、④的区域不应该相互看到，即使它们位于同一个窗口内，因此必须应用适当的注意力掩码。

总体来说，Swin 比 PVT 更难实现，但它的线性扩展和优秀性能使其成为最好的视觉 Transformer 之一。但 2021 年还没有结束：[Swin v2](https://homl.info)于 2021 年 11 月发布。⁠^(9)它在各个方面都改进了 Swin：对大型 ViT 的更稳定训练，更容易在大图像上进行微调，减少了对标记数据的需要，等等。Swin v2 至今仍在视觉任务中得到广泛应用。

我们的工具箱现在包含了用于分类（例如，ViT 和 DeiT）和密集预测任务（例如，PVT 和 Swin）的视觉 Transformer。现在让我们探索最后一个纯视觉 Transformer，DINO，它引入了一种革命性的自监督技术，用于视觉表示学习。

## DINO：自监督视觉表示学习

2021 年 4 月，Mathilde Caron 等人介绍了[DINO](https://homl.info/dino)，^(10)一种令人印象深刻的自监督训练技术，能够生成能够生成优秀图像表示的模型。这些表示可以用于分类和其他任务。

这就是它的工作原理：在训练过程中，模型被复制，一个网络充当教师，另一个网络充当学生（参见图 16-7）。梯度下降只影响学生，而教师的权重只是学生权重的指数移动平均（EMA）。这被称为*动量教师*。学生被训练以匹配教师的预测：由于它们几乎是同一个模型，这被称为*自蒸馏*（因此模型的名称是 self-**di**stillation with **no** labels）。

在每个训练步骤中，输入图像以各种方式进行增强：颜色抖动、灰度、高斯模糊、水平翻转等。重要的是，它们以不同的方式增强教师和学生：教师总是看到完整图像，仅略有增强，而学生通常只看到图像的放大部分，增强更强烈。简而言之，教师和学生看到的原始图像的变体不同，但他们的预测必须仍然匹配。这迫使他们就高级表示达成一致。

然而，使用这种机制存在*模式坍塌*的强烈风险。这是当学生和教师始终输出完全相同的东西，完全忽略输入图像时。为了防止这种情况，DINO 跟踪教师预测对数的移动平均值，并从预测对数中减去这个平均值。这被称为*居中*，迫使教师将其预测均匀地分布在所有类别上（平均而言，随着时间的推移）。

但仅居中可能会导致教师始终为每个类别输出相同的概率，始终忽略图像。为了避免这种情况，DINO 还迫使教师对其最高预测有很高的信心：这被称为*锐化*。它是通过将教师的对数几率（即，除以小于 1 的温度）应用低温度来实现的。居中和锐化共同保留了教师输出的多样性；这给模型留下了没有捷径。它必须基于图像的实际内容进行预测。

![说明 DINO 模型使用教师和学生视觉 Transformer (ViTs) 的图，通过居中和锐化来增强无标签的预测准确性。](img/hmls_1607.png)

###### 图 16-7\. DINO，或无标签的自蒸馏

训练完成后，你可以丢弃教师：学生就是最终的 DINO 模型。如果你给它一个新图像，它将输出一系列上下文化的补丁嵌入。这些嵌入可以用各种方式使用。例如，你可以在类别标记的输出嵌入之上训练一个分类头。实际上，你甚至不需要一个新的分类头：你可以在每个训练图像上运行 DINO 来获取它们的表示（即类别标记的输出），然后计算每个类别的平均表示。然后，当给定一个新图像时，使用 DINO 来计算其表示，并寻找与最近平均表示最接近的类别。这种方法在 ImageNet 上达到了 78.3% 的 top-1 准确率，相当令人印象深刻。

但这不仅仅是关于分类！有趣的是，DINO 的作者们注意到，在最后一层的类别标记的注意力图中，通常关注图像中感兴趣的主要对象，尽管它们完全是未经标签训练的！实际上，每个注意力头似乎都关注对象的不同部分，正如你在图 16-8 中可以看到。⁠^(11) 请参阅笔记本，其中包含使用 DINO 绘制类似注意力图的代码示例。

![使用 DINO 进行无监督图像分割的示意图，不同的注意力头突出显示蔬菜、斑马和卡车等对象的不同部分。](img/hmls_1608.png)

###### 图 16-8\. 使用 DINO 进行无监督图像分割——不同的注意力头关注主对象的不同部分

后续技术，如[TokenCut](https://homl.info/tokencut)⁠^(12)，在 DINO 的基础上检测和分割图像和视频中的对象。然后在 2023 年 4 月，Meta 发布了[DINOv2](https://homl.info/dino2)⁠^(13)，它在精心策划的更大数据集上进行了训练，并调整以输出每个块的特性，使其不仅成为分类的强大基础模型，而且对于密集预测任务也非常有用。

让我们回顾一下：我们已经介绍了基于 CNN 的 transformers，如 DETR，接着是原始的 ViT（通过编码器处理图像块），DeiT（蒸馏 ViT），PVT（具有空间减少注意力的分层 ViT），Swin（基于窗口的注意力的分层 ViT），以及 DINO（无标签的自蒸馏）。在我们继续到多模态 transformers 之前，让我们快速浏览一些更多的纯视觉 transformer 模型和技术。

## 其他主要视觉模型和技术

视觉 transformers 的进展一直持续到今天。以下是关于一些里程碑论文的简要概述：

[“扩展视觉 Transformer”](https://homl.info/scalingvits),⁠^(14) 2021 年 6 月

Google 研究人员展示了如何根据可用数据量扩展或缩小 ViT。他们成功地创建了一个包含 20 亿个参数的巨大模型，在 ImageNet 上达到了超过 90.4%的 top-1 准确率。相反，他们还训练了一个缩小后的模型，在 ImageNet 上达到了超过 84.8%的 top-1 准确率，仅使用了 10,000 张图片：每个类别的图片只有 10 张！

[“BEiT：图像 Transformer 的 BERT 预训练”](https://homl.info/beit),⁠^(15) 2021 年 6 月

Hangbo Bao 等人提出了一种受 BERT 的掩码语言建模（MLM）启发的*掩码图像建模*（MIM）方法。BEiT 被预训练以从可见图像块重建掩码图像块。这种预训练技术显著提高了下游任务。

注意，BEiT 并未训练来预测遮挡补丁的原始像素；相反，它必须预测遮挡标记 ID。但这些标记 ID 从何而来？嗯，原始图像通过一个 *离散变分自动编码器*（dVAE，见第十八章）传递，将每个补丁编码成一个视觉标记 ID（一个整数），来自一个固定词汇表。这些就是 BEiT 尝试预测的 ID。目标是避免在不必要的细节上浪费模型的能力。

[“Masked Autoencoders Are Scalable Vision Learners”](https://homl.info/mae)，⁠^(16) 2021 年 11 月

这篇由 Facebook 研究团队（由多产的 Kaiming He 领导）撰写的论文也提出了一种基于遮挡图像建模的预训练技术，但它去除了 BEiT 的 dVAE 的复杂性：遮挡自动编码器（MAE）直接预测原始像素值。关键的是，它使用非对称的编码器-解码器架构：一个大型编码器仅处理可见补丁，而一个轻量级的解码器重建整个图像。由于 75% 的补丁被遮挡，这种设计大大降低了计算成本，并允许 MAE 在非常大的数据集上进行预训练。这导致了在下游任务上的强大性能。

[“Model Soups”](https://homl.info/modelsoups)，⁠^(17) 2022 年 3 月

本文证明了首先训练多个变压器，然后平均它们的权重以创建一个新且改进的模型是可能的。这与集成（见第六章）类似，但最终只有一个模型，这意味着没有推理成本。

[“EVA: Exploring the Limits of Masked Visual Representation Learning at Scale”](https://homl.info/eva)，⁠^(18) 2022 年 5 月

EVA 是一系列在规模上预训练的大型 ViT，使用增强的 MAE 和强大的增强。它是 ViT 的领先基础模型之一。2023 年 3 月发布的 EVA-02，尽管参数较少，但表现同样出色甚至更好。大型变体有 304M 个参数，在 ImageNet 上达到了令人印象深刻的 90.0%。

[I-JEPA](https://homl.info/ijepa)，⁠^(19) 2023 年 1 月

Yann LeCun 在一篇 2022 年的论文中提出了联合嵌入预测架构（JEPA），⁠^(20) 作为他世界模型框架的一部分，旨在加深 AI 对世界的理解并提高其预测的可靠性。I-JEPA 是 JEPA 的图像实现。2024 年很快推出了 [V-JEPA](https://homl.info/vjepa)，2025 年推出了 [V-JEPA 2](https://homl.info/vjepa2)，两者都处理视频。

在训练过程中，JEPA 涉及两个编码器和预测器：教师编码器看到完整的输入（例如，一张猫的照片），而学生编码器只看到输入的一部分（例如，同一张猫的照片但没有耳朵）。两个编码器都将它们的输入转换为嵌入，然后预测器试图根据输入其余部分的学生嵌入（例如，没有耳朵的猫）预测缺失部分（例如，耳朵）的教师嵌入。学生编码器和预测器联合训练，而教师编码器只是学生编码器的一个移动平均值（类似于 DINO）。JEPA 主要在嵌入空间而不是像素空间中工作，这使得它快速、参数高效且更具语义性。

训练完成后，教师编码器和预测器不再需要，但学生编码器可以用来为下游任务生成优秀且具有意义的表示。

列表可以一直继续：

+   NesT 或 DeiT-III 用于图像分类

+   MobileViT、EfficientFormer、EfficientViT 或 TinyViT，用于小型高效的图像分类模型（例如，用于移动设备）

+   类似于 Twins-SVT、FocalNet、MaxViT 和 InternImage 这样的分层 Transformer，常被用作密集预测任务的主干网络

+   Mask2Former 或 OneFormer 用于通用分割，SEEM 用于通用分割，SAM 或 MobileSAM 用于交互式分割

+   ViTDet 或 RT-DETR 用于目标检测

+   TimeSformer、VideoMAE 或 OmniMAE 用于视频理解

此外，还有一些技术，如*token merging*（ToMe），通过在运行时合并相似标记来加速推理，*token pruning*在处理过程中去除不重要的标记（即，具有低注意力分数的标记），*early exiting*只计算最重要的标记的深层，*patch selection*只选择最具有信息量的块进行处理，以及像 SimMIM、iBOT、CAE 或 DINOv2 这样的自监督训练技术，等等。

希望我们已经涵盖了足够多的模型和技术，以便您能够进一步探索。

###### 小贴士

其中一些仅用于视觉的模型是在多模态数据上预训练的（例如，图像-文本对或输入提示）：OmniMAE、SEEM、SAM、MobileSAM 和 DINOv2。这很自然地引出了本章的第二部分。

我们已经有了能够阅读和写作（以及聊天！）的 Transformer，现在我们有了能够看到的视觉 Transformer。是时候构建能够同时处理文本和图像，以及其他模态的 Transformer 了。

# 多模态 Transformer

人类是多模态生物：我们通过多种感官感知世界——视觉、听觉、嗅觉、味觉、触觉、平衡感、本体感觉（即身体位置感）以及其他几种——并通过运动、言语、书写等方式作用于世界。这些模态可以被视为非常低级（例如，声波）或高级（例如，单词、语调、旋律）。重要的是，模态是异质的：一个模态可能是连续的，而另一个是离散的；一个可能是时间的，而另一个是空间的；一个可能是高分辨率的（例如，48 kHz 音频），而另一个则不是（例如，文本）；一个可能是嘈杂的，而另一个是干净的，等等。

此外，模态可能以各种方式相互作用。例如，当我们与人交谈时，我们可能同时听到他们的声音并观察他们嘴唇的运动：这两种模态（听觉和视觉）携带重叠的信息，这有助于我们的大脑更好地解析单词。但多模态不仅仅是提高信号/噪声比：面部表情可能携带它们自己的意义（例如，微笑和皱眉），不同的模态可能结合产生新的意义。例如，如果你说“他是个专家”同时翻白眼或做出引号手势，你显然是在讽刺，这颠倒了你的句子的意义并传达了额外信息（例如，幽默或轻蔑），这些信息任何一个模态单独都不具备。

因此，多模态机器学习需要设计能够处理非常异质数据并捕捉它们之间相互作用的模型。这有两个主要挑战。第一个被称为**融合**，它涉及到找到一种方法来结合不同的模态，例如，通过将它们编码到同一个表示空间中。第二个被称为**对齐**，其目标是发现模态之间的关系。例如，你可能有一个语音的录音，以及相应的文本转录，你想要找到每个单词的时间戳。或者，你想要根据文本查询“树旁的狗”找到图像中最相关的对象（这被称为**视觉定位**）。许多其他常见任务涉及两个或更多模态，例如图像标题、图像搜索、视觉问答（VQA）、语音转文本（STT）、文本转语音（TTS）、具身人工智能（即能够与物理环境进行物理交互的模型）等等。

多模态机器学习已经存在了几十年，但由于深度学习，尤其是自变压器兴起以来，最近进展加速。确实，只要你能将其切割成一系列有意义的标记（例如，将文本切割成单词，将图像切割成小块，将音频或视频切割成短剪辑等），变压器几乎可以处理任何模态。一旦你准备好了标记嵌入的序列，你就可以将其输入到变压器中。不同模态的嵌入可以通过各种方式融合：相加、连接、通过融合编码器传递，等等。这可以解决融合问题。而且，变压器还有多头注意力，这是一个强大的工具，可以检测和利用复杂模式，无论是模态内部还是跨模态。这可以解决对齐问题。

研究人员很快理解了变压器在多模态架构中的潜力。第一个多模态变压器是在 2018 年初原 Transformer 论文发布后的几个月内发布的，包括图像字幕、视频字幕等。让我们看看一些最有影响力的多模态变压器架构，从 VideoBERT 开始。

## VideoBERT：一种用于文本加视频的 BERT 变体

2019 年 4 月，谷歌研究人员发布了[VideoBERT](https://homl.info/videobert)。⁠^(21) 如其名所示，这个模型与 BERT 非常相似，但它可以处理文本和视频。事实上，作者只是将预训练的 BERT-large 模型扩展到允许额外的视频标记（稍后会有更多介绍），并继续使用文本加视频训练集上的自监督训练模型。这个数据集是从大量教学 YouTube 视频中构建的，特别是烹饪视频。这些视频通常涉及某人描述执行一系列动作的同时进行（例如，“像这样将番茄切成薄片”）。为了将这些视频输入到 VideoBERT 中，作者必须将视频编码成文本和视觉序列（参见图 16-9)）：

+   对于视觉模态，他们提取了非重叠的 1.5 秒剪辑，每秒 20 帧（即每个剪辑 30 帧），并将这些剪辑通过一个名为 S3D 的 3D CNN。这个 CNN 基于 Inception 模块和可分离卷积（参见第十二章)，并在包含许多 YouTube 视频中人们执行各种动作的 Kinetics 数据集上进行了预训练。作者在 S3D 之上添加了一个 3D 平均池化层，为每个视频剪辑得到一个 1,024 维的向量。每个向量编码了关于视频剪辑相当高级的信息。

+   为了从视频中提取文本，作者使用了 YouTube 的内部语音到文本软件，之后他们从视频中删除了音频轨道。然后，他们通过添加标点符号使用现成的 LSTM 模型将文本分离成句子。最后，他们像对 BERT 一样预处理和标记化文本。

![说明 VideoBERT 过程的图解，展示了如何使用 S3D 对视频剪辑进行视频到文本转换，以及使用语音到文本对音频进行转换，将视频中的动作转换为文本和视觉标记。](img/hmls_1609.png)

###### 图 16-9\. VideoBERT—将视频编码为文本序列和视觉序列

太好了！我们现在有一个描述一些动作的文本标记序列，以及代表这些动作视频剪辑的向量序列。然而，我们遇到了一个问题。回想一下，BERT 是使用 MLM 进行预训练的，其中模型必须从固定词汇表中预测被掩码的标记。我们确实有一个用于文本标记的固定词汇表，但没有用于视频标记的。所以，让我们构建一个吧！为此，作者收集了 S3D 在他们的训练集中产生的所有视觉向量，并使用 k-均值（见第八章第八章）将这些向量聚类成 k = 12 个簇。然后，他们在每个簇上再次使用 k-均值，得到 12² = 144 个簇，然后又再次这样做，得到 12⁴ = 20,736 个簇。这个过程被称为**层次 k-均值**，它比只使用 k = 20,736 运行 k-均值一次要快得多，而且通常会产生更好的簇。现在，每个向量都可以用其簇 ID 来替换：这样，每个视频剪辑都由固定视觉词汇中的一个单一 ID 来表示，因此整个视频现在由一个视觉标记 ID 序列（例如，194，3912，…）来表示，就像标记化文本一样。简而言之，我们已经从连续的 1,024 维空间下降到只有 20,736 个可能值的离散空间。在这个步骤中会有大量的信息损失，但 VideoBERT 出色的性能表明，大部分重要信息仍然保留。

###### 注意

由于作者使用了预训练的 BERT-large 模型，因此在 VideoBERT 的额外训练开始之前，文本标记嵌入已经非常优秀了。对于视觉标记嵌入，作者没有使用从零开始初始化的可训练嵌入，而是使用了使用 k-均值簇质心的 1,024 维向量表示初始化的冻结嵌入。

作者使用了三种不同的训练模式：仅文本、仅视频和文本加视频。在仅文本和仅视频模式下，VideoBERT 只接受单一模态输入，并训练预测掩码标记（文本标记或视频标记）。对于文本加视频，模型同时接受文本标记和视频标记，简单拼接（中间加上一个不重要的分隔标记），并需要预测文本标记和视频标记是否来自原始视频的同一部分。这被称为*语言-视觉对齐*。为此，作者在类别标记输出之上添加了一个二分类头（这取代了 BERT 的下一句预测头）。对于负例，作者只是随机采样句子和视频片段。图 16-10 同时显示了所有三种模式，但请注意，它们实际上是分开的。

![说明 VideoBERT 模型，突出掩码标记预测和语言-视觉对齐过程的图。](img/hmls_1610.png)

###### 图 16-10\. VideoBERT—使用掩码标记预测和语言-视觉对齐进行预训练（显示在一起但实际上是分开的）

语言-视觉对齐是一个有噪声的任务，因为厨师可能会解释他们已经完成或将要做的某事，因此作者将随机相邻的句子拼接起来，以给模型提供更多上下文。作者还有一些其他的技巧，例如随机改变视频采样率，使模型对不同的动作速度更加鲁棒，因为有些厨师比其他人更快；更多细节请参阅论文。

这是一项大量工作，但作者最终完成了：他们拥有一个完全训练好的 VideoBERT 模型。为了展示其有效性，他们在一些下游任务上评估了 VideoBERT，包括：

零样本动作分类

给定一个视频片段，确定执行了哪个动作，而不需要微调 VideoBERT。作者通过将视频输入到 VideoBERT 中，并附带以下掩码句子：“现在让我给你演示如何

视频字幕

给定一个视频片段，生成一个字幕。为此，作者使用了最早的[视频字幕 Transformer 架构](https://homl.info/videocaption)⁠^(22)，但他们用 VideoBERT 输出的视觉特征替换了编码器的输入。更具体地说，他们取了 VideoBERT 最终输出表示的平均值，包括所有视觉标记的表示和被遮蔽的文本标记的表示。他们使用的遮蔽句子是：“现在让我们把刀切向番茄，然后切向土豆，最后切向洋葱”。在微调这个新模型后，他们获得了比原始字幕模型更好的结果。

使用类似的方法，VideoBERT 可以适应许多其他任务，例如多选题视觉问答：给定一个图像、一个问题以及多个可能的答案，模型必须找到正确的答案。例如：“厨师在做什么？”→“切番茄”。为此，一种方法是在每个可能的答案上简单地运行 VideoBERT，包括视频，并比较语言-视觉对齐得分：正确的答案应该有最高的得分。

VideoBERT 的成功激励了许多基于 BERT 的多模态 Transformer，其中许多在 2019 年 8 月和 9 月发布：ViLBERT、VisualBERT、Unicoder-VL、LXMERT、VL-BERT 和 UNITER。其中大多数是单流模型，类似于 VideoBERT，意味着模态在网络的早期就融合在一起，通常是通过简单地连接序列。然而，ViLBERT 和 LXMERT 是双流 Transformer，意味着每个模态都由自己的编码器处理，并有一个机制允许编码器相互影响。这使得模型在尝试理解它们之间的相互作用之前，能更好地理解每个模态。ViLBERT 尤其有影响力，因此让我们更详细地看看它。

## ViLBERT：一种用于文本加图像的双流 Transformer

ViLBERT 是由乔治亚理工学院、Facebook AI Research 和俄勒冈州立大学的研究团队在 2019 年 8 月[提出](https://homl.info/vilbert)⁠^(23)的。他们指出，单流方法（VideoBERT 和许多其他方法所使用）对两种模态的处理是相同的，尽管它们可能需要不同级别的处理。例如，如果视觉特征来自深度 CNN，那么我们已经有很好的高级视觉特征，而文本在模型能够访问高级文本特征之前需要更多的处理。此外，研究人员假设“图像区域可能比句子中的单词关系更弱”。⁠^(24) 最后，BERT 最初仅使用文本进行预训练，因此强迫它处理其他模态可能会得到次优结果，甚至可能在多模态训练期间损坏其权重。

因此，作者选择了一种双流方法：每个模态都通过自己的编码器，在上层，两个编码器通过一个新的双向交叉注意力机制连接并交换信息，称为*共注意力*（见图 16-11）。具体来说，在每一对连接的编码层中，一个编码器的 MHA 查询被另一个编码器用作 MHA 键/值。

![使用共注意力连接的视觉和文本共编码层图，其中一个编码器的多头注意力（MHA）查询被另一个编码器用作键/值。](img/hmls_1611.png)

###### 图 16-11\. 通过共注意力连接的两个编码层：一个编码器的 MHA 查询被另一个编码器用作 MHA 键/值

文本编码器的底层使用 BERT 的权重初始化（作者使用了一个 BERT 基础模型，它有 12 层），并在其上方放置了 6 个共注意力层（见图 16-12 的右下象限）。视觉特征由一个预训练并冻结的 Faster R-CNN 模型产生，并假设这些特征足够高级，因此不需要进一步处理；因此，视觉编码器仅由六个共注意力层组成，与文本编码器的六个共注意力层配对（见图的左下象限）。Faster R-CNN 模型的输出为每个检测到的区域通过一个均值池化层，因此我们得到每个区域的特征向量，并且低置信度区域被丢弃：每个图像最终由 10 到 36 个向量表示。

![说明 ViLBERT 模型使用掩码标记预测和语言-视觉对齐进行预训练的图，突出视觉编码器和文本编码器之间的交互以及它们在分类任务中的集成。](img/hmls_1612.png)

###### 图 16-12\. ViLBert 使用掩码标记预测和语言-视觉对齐进行预训练（再次，一起显示但实际上是分开的）

由于区域没有像单词那样的自然顺序，视觉编码器不使用位置编码。相反，它使用如下计算的空间编码：每个区域的边界框被编码为一个包含归一化左上角和右下角坐标以及边界框覆盖的图像比例的 5D 向量。然后，这个 5D 向量被线性投影到与视觉向量相同的维度，并简单地添加到它上面。

最后，一个特殊的[IMG]标记被添加到视觉序列的开头：它具有与类别标记相同的作用（即，生成整个序列的表示），但它不是一个可训练的嵌入，而是计算为特征向量的平均值（在空间编码之前），加上覆盖整个图像的边界框的空间编码。

现在转到训练！与 VideoBERT 类似，作者使用了掩码标记预测和语言-视觉对齐：

+   对于掩码标记预测，作者在文本编码器中使用了类似于 BERT 的常规 MLM。然而，对于视觉编码器，由于 ViLBERT 不使用固定大小的视觉词汇表（没有聚类步骤），模型被训练来预测 CNN 对给定图像区域预测的类别分布（这是一个软目标）。作者选择这个任务而不是预测原始像素，因为区域可能相当大，并且通常周围区域和文本中的信息不足以正确重建掩码区域：目标是追求更高层次的目标。

+   对于语言-视觉对齐，模型取[IMG]和[CLS]标记的输出，然后计算它们的逐项乘积，并将结果传递给必须预测文本和图像是否匹配的二分类头。乘法优于加法，因为它放大了在两种表示中都很强的特征（有点像逻辑 AND 门），因此更好地捕捉了对齐。

就这样。该模型在包括图像定位、基于描述的图像检索（甚至零样本）、视觉问答和*视觉常识推理*（VCR）在内的多个下游任务上显著超越了现有技术。VCR 涉及回答关于图像的多项选择题（如 VQA），然后选择适当的理由。例如，给定一张服务员在桌子上为一些煎饼服务的图像，以及问题“为什么第 4 个人指着第 1 个人”，模型必须选择正确的答案“他正在告诉第 3 个人第 1 个人点了煎饼”，然后它必须选择理由“第 3 个人正在上菜，他们可能不知道谁的订单是谁的”。

ViLBERT 由于其双流架构、协同注意力的发明以及在许多下游任务上的出色结果，对多模态机器学习领域产生了强烈影响。这是大规模自监督预训练使用 transformers 的又一伟大展示。下一个重大里程碑出现在 2021 年，它以非常不同的方式处理这个问题，使用了对比预训练：遇见 CLIP。

## CLIP：使用对比预训练训练的双编码器文本加图像模型

OpenAI 于 2021 年 1 月发布的[对比语言-图像预训练（CLIP）](https://homl.info/clip)⁠^(25)是一个重大突破，不仅因为其惊人的能力，还因为其基于*对比学习*的出人意料简单的方法：模型学习将文本和图像编码成向量表示，当文本和图像匹配时相似，不匹配时不相似。

一旦训练完成，该模型可以用于许多任务，尤其是零样本图像分类。例如，CLIP 可以用作昆虫分类器，而无需任何额外训练：只需将所有可能的类名输入到 CLIP 中，例如“蟋蟀”、“瓢虫”、“蜘蛛”等等，为每个类名获得一个向量表示。然后，无论何时你想对图像进行分类，只需将其输入到 CLIP 中，以获得一个向量表示，然后使用余弦相似度找到最相似类名表示。这通常在文本类似于网络上常见的典型图像标题时效果更好，因为 CLIP 就是这样训练的，例如，“这是一张瓢虫的照片”而不是仅仅“瓢虫”。一点提示工程可以帮助（即，尝试各种提示模板）。

好消息是，CLIP 是完全开源的⁠^(26)，Hugging Face Hub 上有几个预训练模型可用，Transformers 库提供了一个方便的零样本图像分类管道：

```py
from transformers import pipeline

model_id = "openai/clip-vit-base-patch32"
clip_pipeline = pipeline(task="zero-shot-image-classification", model=model_id,
                         device_map="auto", dtype="auto")
candidate_labels = ["cricket", "ladybug", "spider"]
image_url = "https://homl.info/ladybug"  # a photo of a ladybug on a dandelion
results = clip_pipeline(image_url, candidate_labels=candidate_labels,
                        hypothesis_template="This is a photo of a {}.")
```

注意，我们提供了一个提示模板，因此模型实际上会编码“这是一张瓢虫的照片”，而不仅仅是“瓢虫”（如果你没有提供任何模板，该管道实际上默认为“这是一张{}的照片”。）现在让我们看看结果，这些结果按分数排序：

```py
[{'score': 0.9972853660583496, 'label': 'ladybug'},
 {'score': 0.0016511697322130203, 'label': 'spider'},
 {'score': 0.0010634352220222354, 'label': 'cricket'}]
```

太棒了！CLIP 以超过 99.7%的置信度预测了瓢虫。现在，如果你想有一个花卉分类器，只需将候选标签替换为花卉的名称。如果你在列表中包含“蒲公英”并分类相同的图像，模型应该以高置信度选择“蒲公英”（忽略瓢虫）。令人印象深刻！

那么，这个魔法是如何工作的呢？好吧，CLIP 的架构基于一个常规文本编码器和常规视觉编码器，没有共注意力或任何花哨的东西（见图 16-13）。实际上，你可以使用几乎任何你想要的文本和视觉编码器，只要它们可以产生文本或图像的向量表示。作者尝试了各种编码器，包括几个 ResNet 和 ViT 模型用于视觉，以及一个类似 GPT-2 的模型用于文本，所有这些都是从零开始训练的。你听到我说 GPT-2 不是一个编码器吗？这是真的，它是一个仅解码器模型，但我们不是为下一个标记预测进行预训练，所以最后一个标记的输出可以自由地用作整个输入序列的表示，这正是 CLIP 所做的事情。你可能想知道为什么我们不使用像 BERT 这样的常规文本编码器？好吧，我们可以，但 OpenAI 创建了 GPT——Alex Radford 是 GPT 和 CLIP 的首席作者——所以这很可能是选择 GPT-2 的原因：作者只是对这个模型有更多的经验，并且已经建立了一个良好的训练基础设施。使用因果编码器还使得在多个文本以相同方式开始时缓存模型的中间状态成为可能；例如，“这是一张瓢虫的照片”。

![说明 CLIP 模型中文本和视觉编码器处理图像-文本对以将它们转换为向量以匹配对应对并区分不匹配对。](img/hmls_1613.png)

###### 图 16-13\. CLIP：将一批图像-文本对编码为向量，然后匹配的向量被拉近，不匹配的向量被推开

还要注意，在视觉编码器之上添加了一个池化层，以确保它输出整个图像的单个向量，而不是特征图。此外，在每个编码器之上添加了一个线性层，将最终表示投影到相同的输出空间（即具有相同数量的维度）。因此，给定*m*个图像-文本对批次，我们得到*m*个图像的向量表示和*m*个文本的向量表示，所有向量具有相同数量的维度。图 16-13 显示了*m* = 4，但在训练期间，作者使用了令人震惊的大批次大小*m* = 2¹⁵ = 32,768。

该模型随后在从互联网上抓取的包含 4 亿个图像-文本对的庞大数据集上进行了预训练，使用对比损失⁠^(27)将匹配对的表示拉近，同时将不匹配对的表示推开。以下是它是如何工作的：

+   所有向量首先进行ℓ[2]归一化，这意味着它们被缩放为单位向量：我们只关心它们的方向，而不是它们的长度。

+   接下来，我们计算每个可能的图像-文本对图像表示和文本表示之间的余弦相似度。结果是包含介于-1（对于相反向量）和+1（对于相同向量）之间的数字的*m* × *m*矩阵。在图 16-13 中，这个矩阵由 4 × 4 网格表示（黑色为+1，白色为-1）。每一列衡量每个批次中的图像与同一批次中给定文本的匹配程度，而每一行衡量每个文本与给定图像的匹配程度。

+   由于第*i*个图像对应于第*i*个文本，我们希望这个矩阵的主对角线包含接近+1 的相似度分数，而所有其他分数应接近 0。为什么不是接近-1 呢？好吧，如果一个图像和文本完全无关，我们可以将它们的表示视为两个随机向量。回想一下，两个随机的高维向量很可能接近正交（如第七章所述），因此它们的余弦相似度将接近 0，而不是-1。换句话说，假设不匹配对的文本和图像表示无关（分数接近 0），而不是相反（分数接近-1）是有意义的。

+   在第 *i*^(th) 行中，我们知道匹配的标题在第 *i*^(th) 列，因此我们希望模型在该列产生高相似度分数，在其他地方产生低分数。这类似于一个分类任务，其中目标类别是第 *i*^(th) 类。实际上，我们可以将每个相似度分数视为类别 logit，并简单地计算该行的交叉熵损失，其中 *i* 作为目标。我们可以对每一列采用完全相同的推理。如果我们为每一行和每一列计算交叉熵损失（使用类别 *i* 作为第 *i*^(th) 行和第 *i*^(th) 列的目标），并计算平均值，我们得到最终的损失。

+   只有一个额外的技术细节：相似度分数的范围在 –1 和 +1 之间，这不太可能是任务的理想 logit 尺度，因此 CLIP 在计算损失之前将所有相似度分数除以一个可训练的温度（一个标量）。

###### 警告

这个损失需要一个大的批量大小以确保模型看到足够的负例来与正例进行对比，否则它可能会过度拟合正例的细节。CLIP 的成功部分归因于作者能够实现的巨大的批量大小。

作者在许多图像分类数据集上评估了 CLIP，对于其中大约 60% 的数据集，它在没有任何额外训练（即零样本）的情况下表现优于在 ResNet-50 特征上训练的 *线性探针*（这是一个在预训练并冻结的 ResNet-50 模型输出的特征上训练的线性分类器），包括在 ImageNet 上，尽管 ResNet-50 模型实际上是在 ImageNet 上预训练的。CLIP 在每个类别只有少量示例的数据集上特别强大，例如日常场景的图片（即你在网上找到的那种图片）。事实上，CLIP 甚至在斯坦福汽车数据集上击败了当时最先进的 ViTs，因为汽车图片在网络上非常常见，而且数据集每个类别的示例并不多。然而，CLIP 在特定领域的图像上表现不佳，例如卫星或医学图像。

重要的是，CLIP 输出的视觉特征对扰动也非常鲁棒，这使得它们非常适合下游任务，例如图像检索：如果你将图像存储在向量数据库中，通过它们的 CLIP 编码的视觉特征进行索引，那么你可以根据文本查询或图像查询来搜索它们。为此，只需将查询通过 CLIP 获取向量表示，然后在具有相似表示的图像中进行数据库搜索。

要使用 Transformers 库获取文本和视觉特征，您必须直接运行 CLIP 模型，而不是通过管道：

```py
import PIL
import urllib.request
from transformers import CLIPProcessor, CLIPModel

clip_processor = CLIPProcessor.from_pretrained(model_id)
clip_model = CLIPModel.from_pretrained(model_id)
image = PIL.Image.open(urllib.request.urlopen(image_url)).convert("RGB")
captions = [f"This is a photo of a {label}." for label in candidate_labels]
inputs = clip_processor(text=captions, images=[image], return_tensors="pt",
                        padding=True)
with torch.no_grad():
    outputs = clip_model(**inputs)

text_features = outputs.text_embeds    # shape [3, 512]  # 3 captions
image_features = outputs.image_embeds  # shape [1, 512]  # 1 image (ladybug)
```

###### 小贴士

如果你需要分别对图像和文本进行编码，可以使用 CLIP 模型的`get_image_features()`和`get_text_features()`方法。你必须首先使用`CLIPTokenizer`对文本进行标记化，并使用`CLIPImageProcessor`处理图像。生成的特征没有进行ℓ[2]归一化，因此你必须将它们除以`features.norm(dim=1, keepdim=True)`（请参阅笔记本中的代码示例）。

特征已经进行了ℓ[2]归一化，因此如果你要计算相似度分数，只需要一次矩阵乘法即可：

```py
>>> similarities = image_features @ text_features.T  # shape [1, 3]
>>> similarities
tensor([[0.2337, 0.3021, 0.2381]])
```

这之所以有效，是因为矩阵乘法计算了第一个矩阵中每一行向量与第二个矩阵中每一列向量的点积，每个点积等于向量之间角度的余弦值乘以向量的范数。由于在这种情况下向量已经进行了ℓ[2]归一化，范数等于 1，所以结果只是角度的余弦值，这正是我们想要的相似度分数。正如你所见，最相似的表现是第二个，对于瓢虫类别。如果你更喜欢估计概率而不是相似度分数，你必须首先使用模型学习到的温度对相似度进行缩放，然后将结果通过 softmax 函数（很高兴看到我们得到了与管道相同的结果）：

```py
>>> temperature = clip_model.logit_scale.detach().exp()
>>> rescaled_similarities = similarities * temperature
>>> probabilities = torch.nn.functional.softmax(rescaled_similarities , dim=1)
>>> probabilities
tensor([[0.0011, 0.9973, 0.0017]])
```

CLIP 并不是 OpenAI 在 2021 年唯一的惊喜。就在下一个月，OpenAI 宣布了 DALL·E，它可以根据文本描述生成令人印象深刻的图像。现在让我们来讨论它。

## DALL·E：从文本提示生成图像

OpenAI 于 2021 年 2 月发布的[DALL·E](https://homl.info/dalle),⁠^(28)是一个能够根据文本提示生成图像的模型，例如“一个形状像鳄梨的扶手椅”。其架构相当简单（参见图 16-14 的左侧）：一个类似于 GPT 的模型，经过训练以预测下一个标记，但与 GPT 不同，它是在数百万个图像-标题对上预训练的，并输入由文本标记后跟视觉标记组成的序列。在推理时，你只需提供文本标记，然后模型逐个生成视觉标记，直到生成完整的图像。视觉标记是由一个 dVAE 模型生成的，它接受一个图像并输出一个来自固定词汇表的标记序列。遗憾的是，该模型从未向公众发布，但论文描述得足够详细，因此有一些开源复制品可用，例如[DALL·E mini](https://huggingface.co/dalle-mini)，也称为 Craiyon。

一年后，即 2022 年 4 月，OpenAI 发布了[DALL·E 2](https://homl.info/dalle2)，⁠^(29)，能够生成更高质量的图像。其架构实际上非常不同：文本被输入到 CLIP 模型中，该模型输出文本嵌入，然后这个文本嵌入被输入到一个*扩散模型*中，该模型使用它来指导其图像生成过程（我们将在第十八章中讨论扩散模型）。该模型不是开源的，但可以通过付费 API 获取，并通过一些产品，如 Microsoft Designer、Bing Image Creator、Canva、ChatGPT 等。

![比较 DALL·E 和 DALL·E 2 架构的图表，展示了从通过解码器和 dVAE（对于 DALL·E）到 CLIP 和扩散模型（对于 DALL·E 2）生成中国塔图像的文本嵌入的过渡](img/hmls_1614.png)

###### 图 16-14。DALL·E（左）和 DALL·E 2（右）

DALL·E 3 于 2023 年 10 月发布。遗憾的是，到那时，OpenAI 已经完全转变了其最初的开源策略：没有经过同行评审的论文，没有代码，没有权重，没有数据。与上一版本一样，DALL·E 3 可以通过 API 和一些产品获取。我们知道它是基于扩散的，不使用 CLIP，并且与 GPT-4 紧密集成，GPT-4 在生成图像之前会重写提示。它的工作效果令人印象深刻：它输出的图像令人惊叹，与提示的匹配程度比之前的版本更加精确。这种差异对于*组合提示*（例如，“一只毛茸茸的白色猫坐在一个红色的天鹅绒垫子上，后面是一个盛满向日葵的花瓶，沐浴在金色时光中。猫正直视观众”。）尤其明显。DALL·E 1 和 2 通常只会遵循此类提示中的一个或两个元素，而 DALL·E 3 则更紧密地遵循指令。图像质量、逼真度、艺术风格和一致性令人惊叹。最后，DALL·E 3 还集成了某些监管能力。

我们多模态旅程的下一个里程碑是在第一个 DALL·E 模型发布后一个月出现的：Perceiver。

## Perceiver：通过潜在空间连接高分辨率模态

到目前为止，每个 Transformer 都需要将输入切割成有意义的标记。在文本的情况下，标记代表单词或子单词。在 ViTs 的情况下，它们代表 16×16 像素的补丁。在 VideoBERT 中，是短 1.5 秒的剪辑。在音频 Transformer 中，是短音频剪辑。如果我们直接将单个字符、像素或音频帧输入到 Transformer 中，输入序列会非常长，我们会遇到二次注意力问题。此外，我们还会失去重要的归纳偏差：例如，通过将图像切割成补丁，我们强制执行了强烈的归纳偏差，即邻近性（即，附近的像素被认为比远处的像素具有更强的相关性）。

然而，这种标记化是模态特定的，这使得处理新的模态或将其混合到模型中变得更加困难。此外，当您没有大量训练数据时（假设偏差是正确的），归纳偏差非常有用，但如果您的数据集很大，您通常会通过使用具有非常少隐含假设的无偏模型获得更好的性能。当然，模型将不得不自己找出附近的像素通常相关，但另一方面，它将足够灵活，可以发现可能被忽视的模式。

正因如此，DeepMind 在 2021 年 3 月推出了 [*Perceiver*](https://homl.info/perceiver)⁠^(30)。这种架构能够在最低级别直接处理任何模态：字符、像素、音频帧等。此外，它以模态无关的设计来实现这一点，因此相同的模型可以处理不同的模态。Perceiver 架构在 图 16-15 中展示。

![Perceiver 架构图，展示了输入像素通过傅里叶位置编码、线性编码和转换为像素标记的流程，这些标记通过交叉注意力层与潜在标记交互，最终导致分类头。](img/hmls_1615.png)

###### 图 16-15\. Perceiver 架构：输入通过交叉注意力层摄取，而主要输入是一系列学习到的潜在标记

让我们来看看这个架构：

+   输入首先被分割成其最小的组成部分。在这个例子中，输入是一个图像，因此它被分割成单个像素：我们现在有一个 3D 向量序列（红色、绿色、蓝色）。

+   位置编码被连接到这些特征向量中。Perceiver 使用傅里叶位置编码，这与原始 Transformer 的正弦位置编码非常相似，但它们编码了输入的所有维度。由于图像是二维的，每个像素的水平坐标和垂直坐标都会被编码；例如，如果一个像素位于坐标 *x* 和 *y*（归一化在 –1 和 1 之间），那么位置编码向量将包括 *x* 和 *y*，然后是 sin(π_fx_)，sin(π_fy_)，cos(π_fx_)，cos(π_fy_)，重复 *K* 次（通常是 6 次）频率 *f* 从 1 开始，增加到 *μ* / 2（均匀分布），其中 *μ* 是目标分辨率（例如，如果图像是 224 × 224 像素，则 *μ* = 224）。⁠^(31) 位置编码向量的维度是 *d*(2^K_ + 1)，其中 *d* 是输入维度的数量（即音频为 1，图像为 2，视频为 3 等）。

+   现在像素标记有 3 + 2 × (2 × 6 + 1) = 29 维。然后我们通过一个线性层将它们投影到 Perceiver 的维度（例如，512）。

+   感知器的架构本身由重复的处理块（例如，八个）组成，其中每个块由一个交叉注意力多头注意力层（MHA）和一个常规的变换器编码器（例如，具有六个编码器层）组成。最后的块由一个交叉注意力 MHA 层和一个平均池化层组成，以将输入序列减少到一个单一的向量，然后将其输入到分类头（即线性加 softmax）。

+   像素标记仅通过 MHA 层输入到感知器中，并扮演键和值的角色。换句话说，感知器仅通过交叉注意力关注像素标记。

+   关键的是，感知器的主要输入是一个相当短的潜在标记序列（例如，512）。这些标记类似于 RNN 的隐藏状态：一个初始序列（在训练期间学习）被输入到感知器中，并且随着模型通过交叉注意力学习越来越多的关于像素标记的信息，它逐渐得到更新。由于这是一个短序列，它不会受到二次注意力问题的很大影响。这被称为“潜在瓶颈技巧”，是感知器成功的关键。

+   作者们尝试了在处理块之间共享权重（不包括第一个交叉注意力层），并且取得了良好的结果。当处理块共享相同的权重时，感知器实际上是一个循环神经网络，而潜在标记确实是其隐藏状态。

###### 注意

正如我们在第七章中看到的，流形假设指出，大多数现实世界的数据都生活在一个低维流形附近，就像一张卷起的纸张生活在三维空间中，但本质上是一个二维对象。这个二维空间是潜在的（即隐藏的、可能的），直到我们展开纸张。同样，感知器的目标是将其高维输入“展开”，以便模型可以在潜在空间中工作，使用低维表示。

重要的是，这种架构可以有效地处理高分辨率输入。例如，一个 224 × 224 的图像有 50,176 个像素，如果我们尝试直接将如此长的像素标记序列输入到常规编码器中，每个自注意力层将不得不计算 50,176² ≈ 25 亿个注意力分数！但是，由于感知器仅通过交叉注意力关注像素标记，它只需要计算 50,176 乘以潜在标记的数量。即使是最大的感知器变体，这也只是总共 50,176 × 512 ≈ 2570 万个注意力分数，这大约是 100 倍的计算量。

###### 注意

多亏了潜在瓶颈，感知器的扩展与像素标记的数量呈线性关系，而不是二次关系。

作者们使用常规的监督学习在多个模态的分类任务上训练了 Perceiver，包括仅图像（ImageNet）、音频加视频（AudioSet）⁠^(32) 或点云（ModelNet40）⁠^(33)，所有这些都使用了相同的模型架构。他们得到了有竞争力的结果，在某些情况下甚至达到了现有技术的水平。

AudioSet 数据集中的视频被下采样到每秒 25 帧的 224 × 224 像素，音频采样率为 48 kHz。理论上，你可以单独将每个像素和每个音频帧输入到 Perceiver 中，但这会有些极端，因为每个 10 秒的视频将表示为一个由 224 × 224 × 25 × 10 ≈ 12.5 百万像素标记组成的序列，以及 48,000 × 10 = 480,000 音频标记。

因此，作者们不得不妥协。他们在 32 帧剪辑（每秒 25 帧，每段 1.28 秒，而不是 10 秒）上进行了训练，并将视频切割成 2 × 8 × 8 个块（即 2 帧 × 8 × 8 像素），从而得到每个包含 128 个 RGB 像素的 12,544 个视频标记（加上位置编码）。他们还将音频切割成每段 128 帧的剪辑，从而得到 480 个音频标记。他们还尝试将音频转换为梅尔频谱图（这产生了 4,800 个音频标记）。使用频谱图而不是原始音频是音频处理中的标准做法，但这几乎没有对模型性能产生影响，这表明 Perceiver 能够在没有任何帮助的情况下从原始数据中提取有用的特征。

然后他们简单地连接了视频和音频标记序列（在位置编码之后），还连接了一个模态嵌入，以帮助模型区分模态。

Perceiver 架构的一个局限性是它仅设计用于多模态分类。话虽如此，我们不是简单地平均潜在标记并将它们输入到分类头中，而是可以尝试将它们用于其他下游任务。当然，DeepMind 的研究人员想到了这一点，仅仅几个月后，他们就发布了 Perceiver IO 架构。

## Perceiver IO：Perceiver 的灵活输出机制

DeepMind 于 2021 年 7 月发布了[Perceiver IO](https://homl.info/perceiverio)。它能够执行与 Perceiver 类似的分类任务，但比 BERT 更好地执行许多其他任务，如掩码语言建模（MLM）、*光流*（即预测每个像素在下一个视频帧中的移动位置），实际上超越了现有技术，甚至可以玩星际争霸 II。

模型在输出潜在标记之前与 Perceiver 相同，但池化层和分类头被一个非常灵活的输出机制所取代（见图 16-16）：

因此，Google 和 DeepMind 的研究人员在 2022 年 2 月发布了[Perceiver AR 架构](https://homl.info/perceiverar)来解决这一限制（AR 代表自回归）。该模型的工作方式与感知器非常相似，除了输入序列的最后几个标记被用作潜在标记，模型对这些潜在标记是因果的，并且使用下一个标记预测进行训练。Perceiver AR 并没有像 Perceiver 和 Perceiver IO 那样产生同样大的影响，但它由于其线性扩展能力，在非常长的输入序列上取得了出色的结果。

但 DeepMind 的研究人员并没有停止多模态机器学习；他们很快又发布了一个基于感知器的另一个令人惊叹的多模态模型：Flamingo。

## Flamingo：开放式视觉对话

DeepMind 于 2022 年 4 月发布的[Flamingo 论文](https://homl.info/flamingo)，介绍了一种视觉语言模型（VLM），它可以接受任意序列的文本和图像作为输入，并生成连贯的自由形式文本。最重要的是，它在各种任务上的少样本性能都非常出色。

例如，假设你想构建一个模型，该模型接受一张图片并输出关于该图片的诗篇：无需训练新的模型；你只需向 Flamingo 提供几个示例，并在最后添加新的图像，它就会愉快地生成关于这张新图像的诗篇。如果你想让它检测汽车照片上的车牌号码，只需提供几张带有相应车牌号码（作为文本）的照片，然后添加一张新的汽车照片，Flamingo 就会输出车牌号码。你同样可以轻松地使用 Flamingo 进行图像标题生成。或者视觉问答。或者你可以要求它比较两张图像。实际上，你甚至可以给模型提供视频的几个帧，并要求它描述动作。这是一个功能极其强大且无需微调的模型。

让我们看看 Flamingo 的架构（见图 16-17）：

+   Flamingo 不是从头开始，而是基于两个大型预训练模型，这两个模型都是冻结的：一个是视觉模型，另一个是仅解码的语言模型。作者分别使用了 Chinchilla 和 CLIP，但许多其他强大的模型也能很好地工作。

+   每个输入图像都会被输入到视觉模型中，其输出经过一个名为 *Resampler* 的感知器模型，该模型生成一系列潜在标记表示。这确保了每个图像都能以相当短的潜在表示序列（通常比视觉模型的输出短得多）来表示。这解决了二次注意力问题。

+   Resampler 输出的序列被用作许多 *gated xattn-dense* 模块的键/值，这些模块被插入到冻结的 LLM 中的每个块之前：

    +   每个门控 xattn-dense 模块由一个掩码多头注意力层和一个前馈模块组成，每个都有跳跃连接，就像标准 Transformer 解码器层的前馈交叉注意力部分一样。

    +   然而，掩码 MHA 层和前馈模块后面都跟着一个*tanh 门控器*。这些门控器将它们的输入乘以 tanh(*α*)，其中*α*是一个可学习的标量参数，初始化为 0（每个门控器一个）。由于 tanh(0) = 0，训练开始时所有门控器都是关闭的，因此输入只能通过跳跃连接流动，门控的 xattn-dense 模块对 LLM 没有影响。但随着训练的进行，模型逐渐学会打开门控器，允许门控模块影响 LLM 的输出。

    +   在门控 xattn-dense 模块中，每个文本标记只能关注位于其之前的最近图像中的视觉标记；来自所有其他图像的视觉标记都被掩码了。例如，最后一个文本标记（“是”）只能关注中国塔的照片，它不能直接关注花朵照片。然而，由于前面的文本标记有关于花朵照片的信息，最后一个标记确实通过冻结的 LLM 的自注意力层间接访问了花朵照片。

+   文本被标记化成 LLM 期望的形式（例如，Chinchilla 期望序列开始和结束标记，我标记为<s>和</s>

![说明 Flamingo 模型架构的图解，展示了图像和文本输入通过视觉编码器、重采样器和门控 xattn-dense 模块的流动，这些模块集成到 LLM 块中。](img/hmls_1617.png)

###### 图 16-17\. Flamingo 可以接受任何文本和图像的序列，并输出连贯的自由形式文本

坏消息是 DeepMind 没有将 Flamingo 公开。好消息是开源复制品和变体是可用的：

+   [OpenFlamingo](https://homl.info/openflamingo)，由 MLFoundations 团队创建，该团队是非营利组织 LAION 的一部分。它是完全开源的，可在 Hugging Face Hub 上获得（例如，openflamingo/OpenFlamingo-9B-vitl-mpt7b，基于 CLIP ViT-L/14 视觉编码器和 MPT-7B LLM）。

+   Hugging Face 的[IDEFICS](https://homl.info/idefics)，在名为 OBELICS 的巨大数据集上训练，该数据集由来自 Common Crawl 的 1.41 亿个交错文本-图像文档组成（包括 3.5 亿张图像和 1150 亿个文本标记）。IDEFICS 和 OBELICS 都可在平台上找到（例如，Idefics3-8B-Llama3 和 HuggingFaceM4 的 OBELICS）。该架构在 Flamingo 的基础上进行了一些改进；例如，你可以更容易地替换不同的 LLM 或视觉编码器。IDEFICS 本身是开源的，但基于它的模型可能存在许可限制。特别是，IDEFICS 1 和 3 基于 Llama，它在商业使用上存在一些限制，而 IDEFICS 2 基于 Mistral，它是完全开源的。

+   Nvidia 的[AudioFlamingo](https://homl.info/audioflamingo)，与 Flamingo 非常相似，但处理的是音频而不是图像。

+   其他变体也很有用，例如针对特定领域的模型，如[Med-Flamingo](https://homl.info/medflamingo)，这是一个在医疗文档上训练的 OpenFlamingo 模型。

我们将要讨论的最后一种多模态架构是由 Salesforce 提出的语言-图像预训练，或称 BLIP。它的第二个版本，BLIP-2，也成功地重用了两个大型预训练模型——一个视觉模型和一个 LLM——来创建一个能够同时处理图像和文本的 VLM，并生成自由形式的文本。让我们看看它是如何做到的。

## BLIP 和 BLIP-2

Salesforce 在 2022 年 1 月发布的原始[BLIP 模型](https://homl.info/blip)是一个优秀的视觉-语言模型。其架构是一个由纯文本编码器、纯视觉编码器、基于图像的文本编码器和基于图像的文本解码器组成的*编码器-解码器混合体*（MED），它们共享许多层。这种灵活的架构使得模型能够同时针对三个不同的目标进行训练：*图像-文本匹配*（ITM），一个*图像-文本对比损失*（ITC）来对齐图像和文本表示（类似于 CLIP），以及语言建模（LM），其中模型必须尝试使用下一个标记预测来生成标题。

BLIP 成功的一个重要原因是它在一个非常大且干净的数据集上进行了预训练。为了构建这个数据集，作者同时训练了一个*标题生成模块*来为图像生成合成标题，以及一个*过滤模块*来移除噪声数据。这种方法被称为*CapFilt*，它从原始的网页抓取数据集中移除了低质量的标题，并添加了许多新的高质量合成标题。在这个自举阶段之后，作者在刚刚构建的大且干净的数据集上训练了最终的模型。这是一个两阶段的过程，因此得名 BLIP：*自举语言-图像预训练*。

一年后，2023 年 1 月，Salesforce 发布了[BLIP-2](https://homl.info/blip2)，⁠^(37)，它基于相同的核心思想，但通过重用两个大型预训练模型（一个视觉模型和一个语言模型，两者都冻结）大大提高了模型性能。BLIP-2 甚至以更小的模型超越了 Flamingo。

训练分为两个阶段。BLIP-2 在第一阶段的结构如图 16-18 所示。

![图示 BLIP-2 第一阶段预训练架构，重点关注通过各种注意力和处理层整合视觉和文本序列的 Q-Former 模块。](img/hmls_1618.png)

###### 图 16-18\. BLIP-2 预训练，第一阶段：训练 Q-Former

+   核心组件被称为*Q-Former*（查询转换器）。其架构与 BERT-base 相同，实际上它甚至使用 BERT-base 的预训练权重进行初始化，但它也有一些额外的交叉注意力层，使其能够关注由预训练视觉编码器产生的视觉标记。交叉注意力层插入到每个编码器层的每隔一层，在自注意力层和前馈模块之间，并且它们是随机初始化的。

+   Q-Former 处理三个序列：一个文本标记序列（使用 BERT 标记化和标记嵌入），一个由预训练视觉编码器产生的视觉标记序列，最后是一个可训练的 Perceiver 风格的潜在标记序列。在 BLIP-2 中，这些潜在标记被称为*查询标记*，因为它们的输出表示将后来用于查询预训练的 LLM。

+   Q-Former 使用与 BLIP 相同的三个目标进行训练：ITM、ITC 和 LM。对于每个目标，使用不同的掩码：

    +   对于 ITM，查询标记和文本标记可以相互关注。换句话说，查询标记的输出表示代表基于文本的视觉特征，而文本标记的输出表示代表基于图像的文本特征。查询标记的输出通过一个线性层，每个查询标记产生两个 logits（图像-文本匹配或不匹配），然后模型计算所有查询标记的平均 logits，然后计算二元交叉熵。

    +   对于 ITC，查询标记和文本标记不能相互关注。换句话说，Q-Former 的输出代表仅视觉特征和仅文本特征。对于批次中每个可能的图像/标题对，模型计算查询标记输出和类别标记输出之间的最大相似度。我们得到一个最大相似度的矩阵，损失函数推动主对角线上的值趋向于+1，推动其他值趋向于 0，类似于 CLIP。

    +   对于 LM，文本标记只能关注前面的标记（即，我们使用因果掩码），但它们可以关注所有查询标记。然而，查询标记不能关注任何文本标记。换句话说，查询标记输出代表仅视觉特征，而文本标记输出代表基于图像的因果文本特征。该模型使用下一个标记预测进行训练：每个文本标记的输出通过一个分类头，该头必须预测标题中的下一个标记。

你可能会惊讶，Q-Former 既用于编码文本（用于 ITM 和 ITC），也用于生成文本（用于 LM）。由于 Q-Former 是使用预训练的 BERT-base 模型的权重初始化的，因此它从一开始就非常擅长文本编码，但它最初并不知道它必须预测 LM 任务中的下一个标记。幸运的是，它可以相当快地学习，因为它不是从零开始；它有很好的 BERT 特征可以工作。然而，我们需要告诉它我们想要它编码文本还是预测下一个标记。为此，我们在 LM 期间用*解码标记*替换了类别标记。⁠^(38)

一旦第一阶段完成，Q-Former 已经是一个强大的模型，可以将图像和文本编码到同一个空间，因此一只黑猩猩的照片会产生与标题“一只黑猩猩的照片”非常相似的结果表示。但它甚至更好：查询标记输出被训练为对下一个标记预测最有帮助。

###### 小贴士

为了产生 ITM 的负例，一种策略是从同一个批次中随机选择一个标题，排除图像的真实标题。然而，这使得任务过于简单，因此模型学不到很多东西。相反，作者使用了一种*硬负例挖掘*策略，其中困难的标题更有可能被采样。例如，给定一只黑猩猩的照片，标题“一只大猩猩”比“一艘宇宙飞船”更有可能被采样。为了找到困难的标题，算法使用 ITC 任务的相似度分数。

因此，现在是训练的第二阶段（见图 16-19）：

+   我们保留了视觉转换器和 Q-Former，但丢弃了其余部分，并在 Q-Former 之上添加了一个新的随机初始化的线性层。

+   对于每个图像/标题对，Q-Former 关注由预训练的视觉编码器产生的视觉特征，输出通过线性层产生一系列视觉查询标记。

+   将视觉查询标记和文本标记表示连接起来，然后输入到（冻结的）预训练的 LLM 中。我们训练 BLIP-2 来预测下一个标题标记。

在第二阶段，模型学习将视觉查询标记正确映射到 LLM 的输入空间。一旦训练完成，该模型就可以像第二阶段一样使用，生成视觉基础文本。

![BLIP-2 预训练过程的示意图，展示了视觉编码器中的视觉特征如何被 Q-former 和线性层处理以生成视觉查询标记，然后这些标记与文本标记结合并输入到大型语言模型（LLM）中。](img/hmls_1619.png)

###### 图 16-19。BLIP-2 预训练，阶段 2：训练线性层将查询标记映射到 LLM 的输入空间

让我们使用 BLIP-2 为一张图片生成标题：

```py
from transformers import Blip2Processor, Blip2ForConditionalGeneration

model_id = "Salesforce/blip2-opt-2.7b"
blip2_processor = Blip2Processor.from_pretrained(model_id)
blip2_model = Blip2ForConditionalGeneration.from_pretrained(
    model_id, device_map=device, dtype=torch.float16)

image_url = "http://images.cocodataset.org/val2017/000000039769.jpg"  # two cats
image = Image.open(urllib.request.urlopen(image_url))
inputs = blip2_processor(images=image, return_tensors="pt")
inputs = inputs.to(device, dtype=torch.float16)
with torch.no_grad():
    generated_ids = blip2_model.generate(**inputs)

generated_text = blip2_processor.batch_decode(generated_ids)
```

BLIP-2 看到了什么？

```py
>>> generated_text
['<image><image><image><image>[...]<image></s>two cats laying on a couch\n']
```

这是对照片的良好描述，但没有特殊标记会更好，所以让我们在解码模型输出时去掉它们：

```py
>>> generated_text = blip2_processor.batch_decode(generated_ids,
...                                               skip_special_tokens=True)
...
>>> generated_text
>>>
['two cats laying on a couch\n']
```

完美！

###### 小贴士

还可以查看 InstructBLIP，这是一个具有视觉语言指令调整的 BLIP-2 模型。

# 其他多模态模型

我们已经介绍了很多多模态模型，它们具有非常不同的架构和预训练技术，但当然还有很多其他模型。以下是一些最引人注目的模型的快速概述：

LayoutLM (Microsoft, Dec. 2019)

基于文本、视觉和文档布局的文档理解。第 3 版于 2022 年 4 月发布。

GLIP (Microsoft, Dec. 2021)

用于视觉定位和目标检测的视觉语言模型。GLIP-2 于 2022 年发布。

Stable Diffusion (Stability AI, Dec. 2021)

一个强大的文本到图像模型。

OFA (Microsoft, Feb. 2022)

统一（适用于所有）视觉语言预训练框架，处理各种视觉语言任务。

CoCa (Google, May 2022)

使用对比和标题目标预训练的视觉语言模型。CoCa 影响了后来的模型，如 PaLI-X 和 Flamingo-2。

PaLI (Google, Sep. 2022)

用于视觉语言任务（如 VQA 和标题）的多语言多模态模型，具有强大的零样本性能。下一版本 PaLI-X 和 PaLI-3 于 2023 年发布，PaliGemma 于 2024 年 5 月发布。

Kosmos-1 (Microsoft, Feb. 2023)

一个具有强大视觉定位支持的视觉语言模型。Kosmos-2 和 Kosmos-2.5 于 2023 年发布。

PaLM-E (Google, Mar. 2023)

PaLM-E 通过视觉输入和具身传感器数据扩展了 Google 的 PaLM 系列。一个仅解码器的大型语言模型生成文本命令，如“拿起锤子”，这些命令通过下游系统被解释并由机器人执行。

LLaVA (H. Liu et al., Apr. 2023)

在最好的开源视觉语言聊天模型中。

ImageBind (Meta, May 2023)

一种扩展到六个模态（图像、文本、音频、IMU、深度和热成像）的 CLIP 风格模型。

RT-2 (DeepMind, Jul. 2023)

一个能够进行机器人控制的视觉语言模型，在大型指令遵循数据集上训练。

SeamlessM4T (Meta, Aug. 2023)

一个可以执行语音转文本、语音转语音、文本转语音和文本转文本翻译的单一模型，支持近 100 种语言。

Qwen-VL (Alibaba, Sep. 2023)

开放视觉语言家族（7B 到 72B），成为最强的开源多模态基线之一。导致了 Qwen2-VL（2024 年 8 月）和 Qwen3-Omni（2025 年 9 月），扩展到视频和音频并达到万亿参数规模。

Fuyu（Adept AI，2023 年 10 月）

使用统一的 Transformer 实时处理交织的图像和文本。

EMO（阿里巴巴，2024 年 2 月）

捕捉一个人的图像，加上某人说话或唱歌的音频记录，模型生成匹配音频的该人的视频。EMO-2 于 2025 年 1 月发布。

GLaMM（H. Rasheed 等人，2024 年 6 月）

一种视觉对话模型，生成包含对象分割掩码的文本响应。

LaViDa（加州大学洛杉矶分校，松下，Adobe，Salesforce，2025 年 5 月）

一系列基于扩散的开源视觉-语言模型。

###### 小贴士

我为这一章中讨论的所有模型创建了 homl.info 短链接；只需使用不带连字符的小写名称，例如，[*https://homl.info/qwen2vl*](https://homl.info/qwen2vl)。

还有几个商业多模态模型，其详细架构尚未公开，例如 OpenAI 的 GPT-4.1 和 Sora，Google 的 Gemini 2.5 Pro，DeepMind 的 Veo-3，以及 Anthropic 的 Claude 4 Opus。要访问这些模型，你首先需要创建一个账户并获取订阅（或使用免费层），然后你可以使用提供的应用程序（例如，Google AI Studio，[*https://aistudio.google.com*](https://aistudio.google.com))，或者通过 API 查询模型。以下是一个简短的代码示例，展示如何通过 API 查询 Gemini 2.5 Pro。你首先需要在 Google AI Studio 中获取一个 API 密钥，然后你可以使用你喜欢的任何秘密管理方法来存储它并在你的代码中加载它（例如，如果你正在使用 Colab，我建议你使用 Colab 的秘密管理器，正如我们在第十五章中看到的）。

```py
from google import genai

gemini_api_key = [...]  # load from Colab secrets, or from a file, or hardcode
gemini_client = genai.Client(api_key=gemini_api_key)
cats_photo = gemini_client.files.upload(file="my_cats_photo.jpg")
question = "What animal and how many? Format: [animal, number]"
response = gemini_client.models.generate_content(
    model="gemini-2.5-flash",  # or "gemini-2.5-pro"
    contents=[cats_photo, question])
print(response.text)  # prints: "[cat, 2]"
```

这段代码使用了已安装在 Colab 上的`google-genai`库。它还假设在笔记本所在的目录中存在一个名为*my_cats_photo.jpg*的文件。

这就结束了这一章；希望你喜欢。Transformer 现在可以看、听、触摸等等！在下一章中，我们将探讨一些旨在加快和扩展 Transformer 的相当高级的技术。正如 Daft Punk 所说：更难、更好、更快、更强。

# 练习

1.  你能描述一下原始 ViT 的架构吗？为什么这很重要？

1.  常规 ViT（即非分层）最适合哪些任务？它们的局限性是什么？

1.  DeiT 的主要创新是什么？这个想法是否可以推广到其他架构？

1.  有哪些分层 ViT 的例子？它们适合哪些任务？

1.  如何通过 PVTs 和 Swin Transformers 降低处理高分辨率图像的计算成本？

1.  DINO 是如何工作的？DINOv2 中有什么变化？你会在什么情况下想使用 DINOv2？

1.  JEPA 架构的目标是什么？它是如何工作的？

1.  什么是多模态模型？你能给出五个多模态任务的例子吗？

1.  解释在多模态学习中的融合和对齐问题是什么。为什么 Transformer 非常适合解决这些问题？

1.  你能为一行总结 VideoBERT、ViLBERT、CLIP、DALL·E、Perceiver IO、Flamingo 和 BLIP-2 的主要思想吗？

1.  如果您正在使用 Perceiver IO 模型，并且将输入和输出的长度加倍，大约需要多少额外的计算？

1.  尝试在[Food 101 数据集](https://homl.info/food101)（`torchvision.datasets.Food101`）上微调预训练的 ViT 模型。您能达到多少准确率？使用 CLIP 模型，零样本呢？

1.  为您自己的照片创建一个简单的搜索引擎：首先，编写一个函数，使用 CLIP 模型嵌入所有照片并保存结果向量。接下来，编写一个函数，它接受一个搜索查询（文本或图像），使用 CLIP 进行嵌入，然后找到最相似的图像嵌入并显示相应的照片。您可以手动实现相似性搜索算法，或者使用专门的库，如[FAISS 库](https://github.com/facebookresearch/faiss)，甚至是一个完整的向量数据库。

1.  使用 BLIP-2 自动为您的所有照片添加标题。

这些练习的解决方案可在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。

^(1) 凯文·许等，“展示、关注和讲述：具有视觉注意力的神经图像标题生成”，*第 32 届国际机器学习会议论文集*（2015 年）：2048–2057。

^(2) 这部分内容来自论文中的图 3。它是在作者们的友好授权下复制的。

^(3) 马可·图利奥·里贝罗等，“‘我为什么要相信你？’：解释任何分类器的预测”，*第 22 届 ACM SIGKDD 国际知识发现和数据挖掘会议论文集*（2016 年）：1135–1144。

^(4) 尼古拉斯·卡里翁等，“使用 Transformer 进行端到端目标检测”，arXiv 预印本 arXiv:2005.12872（2020 年）。

^(5) 亚历克谢·多索夫斯基等，“一张图片等于 16x16 个单词：用于大规模图像识别的 Transformer”，arXiv 预印本 arXiv:2010.11929（2020 年）。

^(6) 雨果·图尔弗朗等，“训练数据高效图像 Transformer 与通过注意力进行蒸馏”，arXiv 预印本 arXiv:2012.12877（2020 年）。

^(7) 王文海等，“金字塔视觉 Transformer：无需卷积的密集预测的多功能骨干网络”，arXiv 预印本 arXiv:2102.12122（2021 年）。

^(8) 刘泽等，“Swin Transformer：使用平移窗口的层次视觉 Transformer”，arXiv 预印本 arXiv:2103.14030（2021 年）。

^(9) 刘泽等，“Swin Transformer V2：提升容量和分辨率”，arXiv 预印本 arXiv:2111.09883（2021 年）。

^(10) 玛蒂尔德·卡隆等，“自监督视觉 Transformer 中的新兴特性”，arXiv 预印本 arXiv:2104.14294（2021 年）。

^(11) 这是 DINO 论文图 3 的右侧部分，在作者们的友好授权下复制。

^(12) 王扬涛等人，“TokenCut: 使用自监督 Transformer 和归一化 Cut 在图像和视频中分割对象”，arXiv 预印本 arXiv:2209.00383 (2022)。

^(13) “DINOv2: 无监督学习鲁棒视觉特征”，arXiv 预印本 arXiv:2304.07193 (2023)。

^(14) 赵晓华等人，“扩展视觉 Transformer”，arXiv 预印本 arXiv:2106.04560 (2021)。

^(15) 包航波等人，“BEiT：图像 Transformer 的 BERT 预训练”，arXiv 预印本 arXiv:2106.08254 (2021)。

^(16) 何凯明等人，“掩码自编码器是可扩展的视觉学习者”，arXiv 预印本 arXiv:2111.06377 (2021)。

^(17) Mitchell Wortsman 等人，“模型汤：平均多个微调模型的权重可以提高准确率，而不会增加推理时间”，arXiv 预印本 arXiv:2203.05482 (2022)。

^(18) 方宇新等人，“EVA: 在大规模上探索掩码视觉表示学习的极限”，arXiv 预印本 arXiv:2211.07636 (2022)。

^(19) “使用联合嵌入预测架构从图像中进行自监督学习”，arXiv 预印本 arXiv:2301.08243 (2023)。

^(20) Yann LeCun，“通往自主机器智能之路”（2022）。

^(21) 陈孙等人，“VideoBERT：视频和语言表示学习的联合模型”，arXiv 预印本 arXiv:1904.01766 (2019)。

^(22) 周磊，周勇等人，“使用掩码 Transformer 进行端到端密集视频字幕生成”，*IEEE 计算机视觉与模式识别会议论文集* (2018)。

^(23) 陆嘉森等人，“ViLBERT: 针对视觉和语言任务的预训练任务无关的视语言表示”，*神经信息处理系统进展* 32 (2019)。

^(24) 曹继泽等人后来在他们的论文[“幕后：揭示预训练视觉和语言模型之谜”](https://homl.info/probing)中提供了一些支持这一观点的经验证据：特别是，他们发现更多的注意力头集中在文本模态上，而不是视觉模态上。

^(25) Alec Radford 等人，“从自然语言监督中学习可迁移的视觉模型”，arXiv 预印本 arXiv:2103.00020 (2021)。

^(26) 训练代码和数据并未由 OpenAI 发布，但 Gabriel Ilharco 等人创建了[OpenCLIP](https://homl.info/openclip)，这是一个具有完整训练代码和数据的灵活开源 CLIP 复制品。

^(27) 这种对比损失最初是在 2016 年由 Kihyuk Sohn 的 [一篇论文](https://homl.info/npairloss) 中作为 *多类 n 对损失* 介绍，然后用于对比表示学习，并在 2018 年由 Aaron van den Oord 等人发表的 [一篇论文](https://homl.info/infonce) 中更名为 *InfoNCE*（信息噪声对比估计）。

^(28) 阿迪亚·拉梅什等人，“零样本文本到图像生成”，arXiv 预印本 arXiv:2102.12092（2021）。

^(29) 阿迪亚·拉梅什等人，“使用 CLIP 潜在的分层文本条件图像生成”，arXiv 预印本 arXiv:2204.06125（2022）。

^(30) 安德鲁·耶格尔等人，“Perceiver：具有迭代注意力的通用感知”，arXiv 预印本 arXiv:2103.03206（2021）。

^(31) 如果 Δ 是样本之间的间距，那么奈奎斯特-香农采样定理告诉我们，我们可以测量的最大频率是 *f* = 1 / 2Δ。这就是为什么 *f* 停在 *μ* / 2 而不是 *μ*：以更高的分辨率采样不会增加任何信息，还可能引入混叠伪影。

^(32) [AudioSet](https://homl.info/audioset) 包含超过 200 万个 10 秒的视频片段，分为超过 500 个类别。

^(33) [ModelNet40](https://homl.info/modelnet) 是一个包含各种形状的 3D 点云合成数据集，例如飞机或汽车。现实生活中点云的常见来源是激光雷达传感器。

^(34) 安德鲁·耶格尔等人，“Perceiver IO：一种用于结构化输入和输出的通用架构”，arXiv 预印本 arXiv:2107.14795（2021）。

^(35) 在法国漫画系列《阿斯特 rix》中，奥贝利克斯是一个高大友善的高卢人，而伊德菲克斯是他的聪明小狗。

^(36) 李俊南等人，“BLIP：用于统一视觉语言理解和生成的自举语言-图像预训练”，arXiv 预印本 arXiv:2201.12086（2022）。

^(37) 李俊南等人，“BLIP-2：使用冻结图像编码器和大型语言模型的自举语言-图像预训练”，arXiv 预印本 arXiv:2301.12597（2023）。

^(38) 能够同时编码和生成文本的单个模型的想法是在 2019 年由微软研究人员李东等人提出的，他们推出了 [UniLM 模型](https://homl.info/unilm)。

^(39) 大多数现代智能手机都包含一个惯性测量单元（IMU）传感器：它测量加速度、角速度，以及通常的磁场强度。
