# 第十章：参考文献

## 第一章

1 布朗斯坦，M. M.，布卢纳，J.，莱克恩，Y.，斯兹拉姆，A.，范德盖恩施，P. (2017)。几何深度学习：超越欧几里得数据。*IEEE 信号处理杂志，* 34(4)，18–42。

2 吴，Z.，石瑞，P.，陈，F.，等 (2020)。图神经网络综合调查。*IEEE 神经科学和机器学习杂志，* 32，4–24。

3 德奥，N. (1974)。*图论及其在工程和计算机科学中的应用*，印度普伦蒂斯-霍尔出版社。

4 卢斯，D.，佩里，A. D. (1949)。矩阵分析群体结构的方法。*心理测量学，14*，95–116。

5 贾佳，J.，拜卡尔，C.，波特拉鲁，V. K.，本森，A. R. (2021)。图信念传播网络。arXiv 预印本 arXiv:2106.03033。

6 加特纳，T.，黎，Q. V.，斯莫拉，A. (2005)。图核方法的简要游览。[`api.semanticscholar.org/CorpusID:4854202`](https://api.semanticscholar.org/CorpusID:4854202)

7 库科夫，L. (2015 年 5 月 19 日)。网络分析：第 17 讲（第一部分）。图上的标签传播 [视频]。[`youtu.be/hmashUPJwSQ`](https://youtu.be/hmashUPJwSQ)

8 基恩，B. A. (2017 年 5 月 9 日)。Python 中的 Isomap 降维 [博客文章]。[`mng.bz/ey8P`](https://mng.bz/ey8P)。

9 英，R.，何，R.，陈，K.，等 (2018)。图卷积神经网络在 Web 规模推荐系统中的应用。在*第 24 届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*。ACM。

10 桑切斯-伦格林，B.，魏，J. N.，李，B. K.，等 (2019)。机器学习用于气味：学习小分子的可泛化感知表示。arXiv 预印本 arXiv:1910.10685。

11 里戈尼，D.，纳瓦林，N.，斯佩杜蒂，A. (2020). 用于分子设计的条件约束图变分自编码器。*2020 年 IEEE 计算智能系列研讨会（SSCI）*。IEEE。

12 江伟，罗，J.，何，M.，等 (2023)。图神经网络在交通预测中的应用：研究进展。*ISPRS 国际地理信息期刊，12*(3)，100。 [`doi.org/10.3390/ijgi12030100`](https://doi.org/10.3390/ijgi12030100)

13 桑切斯-冈萨雷斯，A.，赫斯，N.，斯普林伯格，J. T.，等 (2018)。图网络作为可学习的物理引擎用于推理和控制。在*国际机器学习会议*。PMLR。

14 拉马斯佩克，L.，沃尔夫，G. (2021)。分层图神经网络可以捕捉长距离相互作用。”在*2021 年 IEEE 第 31 届国际信号处理机器学习研讨会*（第 1-6 页）。IEEE。

15 汉密尔顿，W. (2020). *图表示学习*。摩根与克莱普顿出版社。

## 第二章

1 汉密尔顿，W. L. (2020). 图表示学习。*人工智能与机器学习综合讲座，* 14(3), 1–159。

2 格罗弗，A.，莱斯克维茨，J. (2016)。Node2Vec：网络的缩放特征学习。在*第 22 届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*（第 855-864 页）。ACM。

3 Krebs, V. （未注明）。Pol Books 数据集。[www.orgnet.com](http://www.orgnet.com)

4 Krebs, V. （未注明）。关于美国政治的书籍。[www.orgnet.com/divided.xhtml](http://www.orgnet.com/divided.xhtml)

5 McInnes, L., Healy, J. 和 Melville, J. (2018)。UMAP：统一流形逼近和投影用于降维。arXiv 预印本 arXiv:1802.03426。

6 Rossi, A., Tiezzi, M., Dimitri, G. M., et al. (2018). 基于图神经网络的归纳-转导学习。在《模式识别中的人工神经网络》（第 201-212 页）。Springer-Verlag.

7 Perozzi, B., Al-Rfou, R. 和 Skiena, S. (2014). DeepWalk：社交表示的在线学习。在*第 20 届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*（第 701-710 页）。ACM。

## 第三章

1 Kipf, T. N. 和 Welling, M. (2016). 基于图卷积网络的半监督分类。arXiv 预印本 arXiv:1609.02907。

2 Hamilton, W. L., Ying, R. 和 Leskovec, J. (2017)。在大图上的归纳表示学习。在*第 31 届国际神经信息处理系统会议论文集*（第 1025-1035 页）。Springer。

3 Li, G. (2022 年 11 月)。聚合的原理方法。*PyTorch Geometric*。[`mng.bz/ga8x`](https://mng.bz/ga8x)

4 Xu, K., Li, C., Tian, Y. 等. (2018)。跳跃知识网络上的图表示学习。在*国际机器学习会议*（第 5453-5462 页）。PMLR。

5 Niepert, M., Ahmed, M. 和 Kutzkov, K. (2016)。为图学习卷积神经网络。在*国际机器学习会议*（第 2014-2023 页）。PMLR。

6 Shuman, D. I., Narang, S. K., Frossard, P., et al. (2012). 图信号处理领域的兴起：将高维数据分析扩展到网络和其他不规则领域。*IEEE 信号处理杂志，* 第 30 卷（第 3 期），第 83-98 页。

7 Hamilton, W. L. (2020). 图表示学习。*人工智能与机器学习综合讲座，* 第 14 卷（第 3 期），第 51-89 页。

8 Gao, H. 和 Ji, S. (2019)。图 U-nets。在*国际机器学习会议*（第 2083-2092 页）。PMLR。

9 McAuley, J., Pandey, R. 和 Leskovec, J. (2015). 推断可替代和互补产品的网络。在*第 21 届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*（第 785-794 页）。AMC。

## 第四章

1 Veličković, P., Cucurull, G., Casanova, A. 等. (2017). 图注意力网络。arXiv 预印本 arXiv:1710.10903。

2 Brody, S., Alon, U. 和 Yahav, E. (2021)。图注意力网络有多关注？arXiv 预印本 arXiv:2105.14491。

3 网络犯罪对策联盟。 (2020)。虚假评论欺诈。[www.counteringcrime.org/review-fraud](http://www.counteringcrime.org/review-fraud)。

4 Howarth, Josh. 2023。 “81 Online Review Statistics。” ExplodingTopics。[`explodingtopics.com/blog/online-review-stats`](https://explodingtopics.com/blog/online-review-stats)

5 假评论统计数据。（2024 年 9 月 26 日）。CapitalOne 购物。[`mng.bz/ga0x`](https://mng.bz/ga0x)

6 YelpChi 数据集。GitHub。[`mng.bz/aveo`](https://mng.bz/aveo)

7 拉扬纳，S.，和阿科古卢，L.（2015 年）。集体意见垃圾邮件检测：连接评论网络和元数据。在第 21 届 ACM SIGKDD 国际知识发现和数据挖掘会议论文集（第 985-994 页）。ACM。

8 杜，Y.，刘，Z.，孙，L.，等。（2020 年）。增强基于图神经网络的欺诈检测器以对抗伪装的欺诈者。”在*第 29 届 ACM 国际信息与知识管理会议论文集*（第 315-324 页）。ACM。

9 GATConv 消耗大量 GPU 内存：问题编号#527。（2019 年）。GitHub。[`mng.bz/Zl05`](https://mng.bz/Zl05)

10 汉密尔顿，W. L.，英格，R.，和莱斯克维茨，J.（2017 年）。在大图上进行归纳表示学习。在*神经信息处理系统进展*（第 1025-1035 页）。ACM。

11 马宇，田宇，莫尼兹，N.，和查瓦拉，N. V.（2023 年）。图上的类不平衡学习：综述。arXiv 预印本 arXiv:2304.04300。

12 赵天，张翔，王帅（2021 年）。GraphSMOTE：使用图神经网络在图上进行不平衡节点分类。在第 14 届 ACM 国际网络搜索和数据挖掘会议论文集（第 833-841 页）。ACM。

13 克里斯蒂娜，S.，和赛义德，M.（2022 年）。*使用注意力构建 Transformer 模型*。机器学习精通。

14 阿拉马尔，J. 图像化的 Transformer。[`jalammar.github.io/illustrated-transformer/`](http://jalammar.github.io/illustrated-transformer/)

15 茹施，T. K.，布罗尼斯坦，M. M.，和米什拉，S.（2023 年）。关于图神经网络中过度平滑的综述。arXiv 预印本 arXiv:2303.10993。

## 第五章

1 卡拉斯，T.，莱奈，S.，和艾拉，T.（2019 年）。用于生成对抗网络的基于风格的生成器架构。”在第 IEEE/CVF 计算机视觉和模式识别会议论文集（第 4217-4228 页）。IEEE。

2 麦考利，J.，潘德，R.，和莱斯克维茨，J.（2015 年）。推断可替代和互补产品的网络。在第 21 届 ACM SIGKDD 国际知识发现和数据挖掘会议论文集（第 785-794 页）。ACM。

3 吉普夫，T. N.，和韦林，M.（2016 年）。变分图自动编码器。arXiv 预印本 arXiv:1611.07308。

4 德·高，N.，和吉普夫，T.（2018 年）。MolGAN：用于小分子图的隐式生成模型。arXiv 预印本 arXiv:1805.11973。

5 戈麦斯-博姆巴拉里，R.，魏，J. N.，杜文纳德，D.，等（2018 年）。使用分子数据驱动的连续表示自动进行化学设计。*ACS 中央科学，4*，268-276。

6 Basu, V. (2024 年 12 月 17 日). 使用 VAE 生成药物分子。Keras。[`mng.bz/rKve`](https://mng.bz/rKve)

7 阿隆，U.，和亚哈夫，E.（2020 年）。关于图神经网络瓶颈及其实际影响。arXiv 预印本 arXiv:2006.05205。

## 第六章

1 Carnegie Mellon University. (n.d.). CMU Graphics Lab Motion Capture Database. [`mocap.cs.cmu.edu/`](http://mocap.cs.cmu.edu/)

2 Kipf, T., Fetaya, E., Wang, K-C., Welling, M., and Zemel, R. (2018). Neural relational inference for interacting systems. In *International Conference on Machine Learning* (pp. 2688–2697). PMLR.

3 Raff, E. (2022). *Inside Deep Learning*. Manning.

4 Xu, D., Ruan, C., Korpeoglu, E., Kumar, S., and Achan, K. (2020). Inductive representation learning on temporal graphs. arXiv preprint arXiv:2002.07962.

5 Rossi, E., Chamberlain, B., Frasca, F., et al. (2020). Temporal graph networks for deep learning on dynamic graphs. arXiv preprint arXiv:2006.10637.

6 Zheng, Y., Yi, L., and Wei, Z. (2024). A survey of dynamic graph neural networks. arXiv preprint arXiv:2404.18211.

7 Veličković, P., Cucurull, G., Casanova, A., et al. (2017). Graph attention networks. arXiv preprint arXiv:1710.10903.

## Chapter 7

1 Khatua, A., Mailthody, V. S., Taleka, B., et al. (2023). IGB: Addressing the gaps in labeling, features, heterogeneity, and size of public graph datasets for deep learning research. arXiv preprint arXiv:2302.13522.

2 McAuley, J., Pandey, R., and Leskovec, J. (2015). Inferring networks of substitutable and complementary products. In *Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining* (pp. 785–794). ACM.

3 Bronstein, M., Frasca, F., and Rossi, E. (2020, August 8). Simple scalable graph neural networks. Towards Data Science. [`mng.bz/N1Q7`](https://mng.bz/N1Q7)

4 Chiang, W-L., Liu, X., Xiaoqing, S., et al. (2019). Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks. In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining* (pp. 257–266). ACM.

5 PyTorch. (n.d.). PyTorch Profiler with TensorBoard. [`mng.bz/EaAj`](https://mng.bz/EaAj)

6 PyTorch. (n.d.). PyTorch Profiler. [`mng.bz/8OBB`](https://mng.bz/8OBB)

7 Heim, L. (2023, September 11). FLOP for quantity, FLOP/s for performance [blogpost]. [`mng.bz/KGnZ`](https://mng.bz/KGnZ)

8 Wu, Z., Pan, S., Chen, F., et al. (2020). A comprehensive survey on graph neural networks. *IEEE Transactions on Neural Networks and Learning Systems,* *32,* 4–24.

9 PyTorch Geometric. (n.d.). Torch_Geometric.Loader. [`mng.bz/BXBr`](https://mng.bz/BXBr)

10 PyTorch Geometric. (n.d.). Torch_Geometric.Sampler. [`mng.bz/dX8v`](https://mng.bz/dX8v)

11 Kipf, T. N., and Welling, M. (2016). Variational graph auto-encoders. arXiv preprint arXiv:1611.07308.

12 PyTorch Geometric. (n.d.). Memory-efficient aggregations. [`mng.bz/9Ymo`](https://mng.bz/9Ymo)

13 namespace-PT. (2021, August 15). A comprehensive tutorial to PyTorch DistributedDataParallel. [`mng.bz/YDzK`](https://mng.bz/YDzK)

14 PyTorch Geometric. (n.d.). Distributed batching. GitHub. [`mng.bz/GeBR`](https://mng.bz/GeBR)

15 林，H.，颜，M.，杨，X.，等. (2022). 分布式 GNN 在 GPU 上的特性和理解.” *IEEE 计算机架构信函,* *21,* 21–24.

16 德费拉德，M.，布雷斯松，X.，和范德盖恩施，P. (2016). 基于快速局部频谱滤波的图上的卷积神经网络. 在 *神经信息处理系统进展* (第 3844–3852 页). ACM.

17 PyTorch Geometric. (n.d.) 通过远程后端扩展 GNN. [`mng.bz/jpdp`](https://mng.bz/jpdp)

18 PyTorch Geometric. (n.d.) Graph_store.py. GitHub. [`mng.bz/W2Yw`](https://mng.bz/W2Yw)

19 KuzuDB. (n.d.). KuzuDB 图存储实现. GitHub. [`mng.bz/OBXo`](https://mng.bz/OBXo)

20 ArangoDB. (n.d.). ArangoDB 远程后端模块，FastGraphML. GitHub. [`github.com/arangoml/fastgraphml#fastgraphml`](https://github.com/arangoml/fastgraphml#fastgraphml)

21 黄，Z.，张，S.，西，C.，刘，T.，和周，M. (2021). 通过图粗化扩展图神经网络. 在 *第 27 届 ACM SIGKDD 知识发现与数据挖掘会议* (第 675–684 页). ACM.

## 第八章

1 亚历克索普洛斯，P. (2020). *数据语义建模*. O’Reilly 媒体.

2 巴克，R. (1990). *CaseMethod: 实体关系建模*. 埃迪生·韦斯利出版社.

3 波科尼，J. (2016). 图数据库的概念和数据库建模. 在 *第 20 届国际数据库工程与应用研讨会(IDEAS '16)* (第 370–377 页). ACM.

4 戈斯尼尔，D.，和布罗切勒，M. (2020). *图数据实践指南*. O’Reilly 媒体.

5 波科尼，J.，和科瓦奇奇，J. (2017). 图数据库中的完整性约束. *计算机科学进展, 109,* 975–981.

6 内戈，A. (2021). *图增强机器学习*. 曼宁出版社.

7 Neo4j GraphAcademy. (n.d.). 图数据建模基础. [`mng.bz/pKo2`](https://mng.bz/pKo2)

## 附录 A

1 迪奥，N. (2017). *图论及其在工程和计算机科学中的应用*. 多佛出版社.

2 科尔曼，T. H.，利瑟森，C. E.，里维斯，R. L.，和斯坦，C. (2009). *算法导论*. 第 3 版. MIT 出版社.

3 斯基纳，S. (1997). *算法设计手册*. Springer-Verlag.

4 德雷夫斯，S. E. (1969). 一些最短路径算法的评价. *运筹学, 17,* 395–412.

# 索引

A

aggr 参数

激活函数

邻接表, 第 2 版

Avro

准确度指标

属性

Amazon 产品数据集, 第 2 版, 第 3 版

聚合运算符

平均路径长度

邻接矩阵, 第 2 版, 第 3 版

aggr 参数, 第二章, 第三章

Adam 优化器

APIs（应用程序编程接口）

图对象的属性

聚合操作

相邻节点

Add_self_loops

聚合方法, 第二章

高级聚合工具

应用中的实际考虑因素

ASIN（亚马逊标准识别号）

Add_Node 方法

注意力

系数计算

B

使用采样方法进行批处理

选择合适的采样器

BalancedNodeSampler 类

BFS（广度优先搜索）

books_graph 数据集, 第二章

BPTT（时间反向传播）

大数据

中介中心性

基线

二部图, 第二章, 第三章, 第四章

BCE（二元交叉熵）

双向性

books_graph 变量

books_gml 数据集

C

循环图

CNNs（卷积神经网络）

卷积 GNNs（卷积图神经网络）

connected_components 方法

Cypher 语言

calculate_loss 方法

连通图

连通性

常数时间复杂度

收敛速度

卷积方法, 第二章

聚类

CSV（逗号分隔值）, 第二章

团方法

连续性

conv2 层

Connected_Graph 方法

cat 参数

CPUs（中央处理单元）, 第二章

CUDA（统一计算设备架构）

概念模式, 第二章

D

将数据对象转换为迭代器

Delete_Node 方法

DFS（深度优先搜索）, 第 2 次, 第 3 次

数据集对象

方向性

解码

多样化操作

DAG（有向无环图）

DataParallel

有向边

data.x 节点特征矩阵

DataBatch

数据隐私

Dataloader 类

dataloader 模块

深度学习方法

DDP（分布式数据并行）

主函数

多进程 spawn

准备模型和数据

运行训练, 第 2 次

为分布式训练设置

训练函数

动态图, 第 2 次

将自动编码器与 RNN 结合

针对的图注意力网络, 第 2 次

姿态估计

循环神经网络

数据集对象

dataloader, 第 2 次

DGL（深度图库）, 第 2 次

数据集表示

直径

判别模型

度

降维

调试

度分布

数据准备和项目规划

项目定义

项目目标和范围

数据大小和速度

数据结构

数据类

Directed_Graph 方法

data.x 节点特征对象

数据模型

数据

数据模块（torch_geometric.data）

度矩阵, 第 2 次

数据集类, 第 2 次

数据集模块（torch_geometric_datasets）

有向图

E

边属性

编码

ER（实体-关系）图

边索引张量

边预测, 第二部分

边到节点矩阵

边列表, 第二部分

边到节点函数, 第二部分, 第三部分

ETL（提取、转换、加载）步骤

创建邻接列表

创建边列表

ELU（指数线性单元）

边索引

EDA（探索性数据分析）

嵌入

可解释性

边权重

周期

F

滤波器（核操作）

拟合方法

from_networkx 方法

FLOP, 第二部分

F1 分数

特征存储

G

GraphSAGE（图采样和聚合）, 第二部分, 第三部分

聚合函数

图自动编码器（GAEs）, 第二部分

使用 GNN 生成图

生成模型

概述

GATConv 层

GATs（图注意力网络）, 第二部分, 第三部分, 第四部分

概述, 第二部分

训练模型

Gremlin 语言

GeoGrid, 第二部分

Graph_State 方法

图粗化

GNN 架构

Gumbel 温度

图对象

GRUs（门控循环单元）, 第二部分, 第三部分

graclus(edge_index) 函数

Gloo

GPU（图形处理单元）, 第二章, 第三章

GCNs（图卷积网络）, 第二章

聚合函数, 第二章

基于图的学习

定义

Get_Node 方法

GDPR（通用数据保护条例）

GraphStore, 第二章

G2GNN（图图神经网络）

图, 第二章

算法, 第二章

类别

聚类系数

定义, 第二章

维度

暴露

基础

属性, 第二章

表示

系统

类型

gcn_norm 函数, 第二章

GNNs（图神经网络）, 第二章, 第三章, 第四章, 第五章

应用

项目考虑因素

使用创建嵌入, 第二章

数据管道示例

定义

设计图模型

训练的心理模型

概述, 第二章

独特机制

何时使用, 第二章

图大小

图数据

查找位置

图数据集

GATv2Conv 层

图预测任务

GCNConv 类, 第二章

获取方法

图嵌入

使用 Node2Vec 创建, 第二章

理论基础

Gumbel-Softmax

图创建方法

H

硬件需求

超图, 第 2 次

同质图

heads 参数

硬件配置

异质/同质图

硬件速度和容量

局部化模式层次

异质图, 第 2 次

I

归纳偏差

推理速度

集成测试

输入, 第 2 次

关联矩阵, 第 2 次

标识符

归纳学习

index_to_key 属性

InMemoryDataset 类

实现细节

I/O 延迟

实例模型, 第 2 次

隐含关系和相互依赖，关键指标

IPUs（智能处理单元）

内积解码器, 第 2 次

正则化潜在空间

索引数组

入度

J

联合概率

JumpingKnowledge 层

JSON

json 模块

JumpingKnowledge 聚合方法

JK（跳跃知识）网络, 第 2 次

K

KL 散度（Kullback-Liebler 散度）

知识图谱

k-分割图

关键词

Kamada-Kawai 算法

L

lstm 参数

logits

线性时间复杂度

潜在空间

语言和系统无关的编码格式

层输出

标签, 第 2 次

LSTM（长短期记忆）

大规模学习和推理

使用采样方法进行批处理

GNN 算法的选择

数据表示的选择, 第 2 次

硬件配置的选择

规模问题的框架

解决规模问题的技术, 第 2 次

标记的掩码布尔掩码

长短期记忆 (LSTM) 网络

拉普拉斯矩阵, 第 2 次

损失函数 (loss_fn) 函数

链接预测任务

库

图神经网络 (GNN)

LSTM 聚合

对数时间复杂度

loadmat 函数

日志记录

标签 NumPy 数组

链接预测，用于图自动编码器的

亚马逊产品数据集, 第 2 次

定义, 第 2 次

训练, 第 2 次

特定语言

M

均方误差 (MSE), 第 2 次

model.eval() 方法

多头注意力

机械推理

分子图, 第 2 次

多层感知器 (MLP), 第 2 次, 第 3 次

内存到数据比率

多重图

矩阵乘法

max_pool() 函数

手动转换

分子科学

模型前向传递

基于矩阵的

内存大小

掩码

模式参数

最大参数

最大聚合

Matplotlib 库

平均聚合, 第 2 次

单部分图

消息传递, 第 2 次

N

神经处理单元 (NPUs)

NeighborSampler 函数

邻域聚合

节点分类

node2vec 方法

邻域加载器, 第 2 次

Node_List 方法

Node_Neighbors 方法

节点

特性

num_nodes

节点嵌入

数据预处理

端到端模型中

随机森林分类

名称

负样本

非结构化属性

NLP (自然语言处理), 第 2 次

节点

N2V (node2vec), 第 2 次

应用和考虑因素

嵌入

加载数据，设置参数，创建嵌入

转换和可视化嵌入, 第 2 次

N2V (node2vec) 嵌入

对新图的可适应性

增强特征集成

特定任务优化

Number_of_Nodes 构造函数

邻域

NRI (神经关系推理) 模型, 第 2 次

训练

num_vars 变量

NetworkX 库, 第 2 次, 第 3 次

node2edge 函数, 第 2 次

节点表示更新

node_embeddings 对象

NCCL (NVIDIA 集体通信库)

NeighborLoader 数据加载器

Node2Vec Python 库

非局部交互，关键指标

O

优化器初始化

本体

ogbn-products 数据集, 第 2 次

OGB (开放图基准), 第 2 次

OLAP (在线应用处理)

过度压缩

OOD (分布外) 泛化

过度平滑, 第 2 次

输出

OLTP (在线事务处理)

P

Pinterest

参数类

产品类别预测

创建模型类

加载数据和处理数据

模型性能分析，第 2 次

模型训练，第 2 次

产品捆绑演示，第 2 次

并行性

项目目标

预测

Pickle 数据编码

属性

PyG（PyTorch Geometric），第 2 次

GCN 在

GraphSAGE 在

在中实现的批处理

使用原始文件创建数据对象

在数据加载器中使用数据对象创建，不使用数据集对象

使用自定义类和输入文件创建数据集对象

GATs 的实现

导入和准备图数据

安装和配置，第 2 次

邻域聚合在

处理边和节点特征

采样器，第 2 次

项目需求和范围，第 2 次

排列不变性

路径

psutil 库

prediction_steps 变量

pred_adj 邻接矩阵

并行和分布式处理

DDP 的代码示例

使用分布式数据并行（DDP）

并行边，第 2 次

池化，图粗化

属性图

PinSage

问题半径

姿态估计

使用内存构建模型

设置问题

Q

二次时间复杂度

QED（药物相似性定量估计）

R

排名

远程存储，使用进行训练，第 2 次

示例

return_edge_mask 选项

RNNs (循环神经网络), 第 2 版, 第 3 版, 第 4 版, 第 5 版, 第 6 版

推荐引擎

随机森林分类

接收系统

表示, 第 2 版

节点相似性和上下文

随机游走

原始数据, 第 2 版

到邻接表和边列表

RDF (资源描述框架) 图

RandomForestClassifier

细化, 第 2 版

基线模型性能, 第 2 版

dropout

模型深度

ReLU (修正线性单元), 第 2 版

read_edgelist 方法

评论垃圾邮件数据集

探索性数据分析

图结构, 第 2 版

节点特征, 第 2 版

阅读 GNN 文献

常见的图表示法

远程后端, 第 2 版

ROC (接收者操作特征), 第 2 版

RefMLPEncoder

关系数据库, 第 2 版, 第 3 版

RDF 图数据模型

知识图谱

简约图数据模型

节点和边类型

非图数据模型

属性图数据模型

远程数据源

S

子图方法

子图, 第 2 版

结构属性

系统驱动，编码选择

模式, 第 2 版, 第 3 版

SMOTE (合成少数类过采样技术)

垃圾邮件和欺诈评论检测

稀疏性，关键指标

系统架构

序列化，2 次

缩放 GNNs，图粗化，2 次

自环，2 次，3 次

对称归一化

谱方法

稀疏矩阵

空间时序 GNNs

使用 GRU 解码姿态数据

编码姿态数据

自注意力

最短路径

采样，2 次

SAGEConv 类

SMILES（简化分子输入行系统）

求和聚合

空间方法

浅层方法

空间复杂度

合成数据

大小属性

采样器

简单图

最短路径长度

SPARQL 语言

SAS（合成可访问性分数）

空间卷积

T

torch_geometric.profile 模块

时序邻接矩阵

to_edge_index 实用函数

时序模型

测试和故障排除

训练

GCN 基线

MLP，2 次

基线模型

非 GNN 基线

为...准备数据

训练掩码

团队分析

test_train_split 函数

平移不变性

测试，2 次

torch.no_grad()方法

TGAT（时序 GAT）

树

TPUs（张量处理单元）

torch_geometric.transforms.ToSparseTensor 函数，2 次

归纳方法

遍历

故障排除

技术债务

表格数据, 第 2 次

图数据以及增加深度和意义

将泰坦尼克数据集重铸为图

时间和空间复杂度, 第 2 次

torch_geometric.data.GraphStore 对象

transformers

归纳学习

torch-scatter 依赖

torch.no_grad()函数

thop 库, 第 2 次

torch_geometric.data.FeatureStore 对象

tpr（真正例率）

遍历算法

U

无权图

单元测试

util 模块

唯一方法

无向边

用例

无向图

Utils 模块（torch_geometric.utils）

Update_Node 方法

V

可视化

可视化库

VGAEs（变分图自动编码器）, 第 2 次

构建, 第 2 次

用于生成图, 第 2 次

何时使用

顶点

VariationalGCNEncoder 层

W

在 Windows 上安装 PyTorch Geometric

权重

wv 方法

加权图

X

X_train_gnn 嵌入

Xavier 初始化

X_n2v NumPy 数组

XGBoost（极端梯度提升）

XML

Z

梯度归零
