# ç¬¬å…«ç« ï¼šä½¿ transformers åœ¨ç”Ÿäº§ä¸­æ›´é«˜æ•ˆ

åœ¨ä¹‹å‰çš„ç« èŠ‚ä¸­ï¼Œæ‚¨å·²ç»çœ‹åˆ°äº† transformers å¦‚ä½•è¢«å¾®è°ƒä»¥åœ¨å„ç§ä»»åŠ¡ä¸Šäº§ç”Ÿå‡ºè‰²çš„ç»“æœã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå‡†ç¡®æ€§ï¼ˆæˆ–è€…æ‚¨æ­£åœ¨ä¼˜åŒ–çš„ä»»ä½•æŒ‡æ ‡ï¼‰æ˜¯ä¸å¤Ÿçš„ï¼›å¦‚æœæ‚¨çš„æœ€å…ˆè¿›æ¨¡å‹å¤ªæ…¢æˆ–å¤ªå¤§ï¼Œæ— æ³•æ»¡è¶³åº”ç”¨ç¨‹åºçš„ä¸šåŠ¡éœ€æ±‚ï¼Œé‚£ä¹ˆå®ƒå°±ä¸æ˜¯å¾ˆæœ‰ç”¨ã€‚ä¸€ä¸ªæ˜æ˜¾çš„æ›¿ä»£æ–¹æ¡ˆæ˜¯è®­ç»ƒä¸€ä¸ªæ›´å¿«ã€æ›´ç´§å‡‘çš„æ¨¡å‹ï¼Œä½†æ¨¡å‹å®¹é‡çš„å‡å°‘é€šå¸¸ä¼šä¼´éšç€æ€§èƒ½çš„ä¸‹é™ã€‚é‚£ä¹ˆå½“æ‚¨éœ€è¦ä¸€ä¸ªå¿«é€Ÿã€ç´§å‡‘ä½†é«˜åº¦å‡†ç¡®çš„æ¨¡å‹æ—¶ï¼Œæ‚¨è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å››ç§äº’è¡¥çš„æŠ€æœ¯ï¼Œå¯ä»¥ç”¨æ¥åŠ é€Ÿé¢„æµ‹å¹¶å‡å°‘æ‚¨çš„ transformer æ¨¡å‹çš„å†…å­˜å ç”¨ï¼š*çŸ¥è¯†è’¸é¦*ã€*é‡åŒ–*ã€*ä¿®å‰ª*å’Œä½¿ç”¨ Open Neural Network Exchange (ONNX)æ ¼å¼å’Œ ONNX Runtime (ORT)è¿›è¡Œ*å›¾ä¼˜åŒ–*ã€‚æˆ‘ä»¬è¿˜å°†çœ‹åˆ°å…¶ä¸­ä¸€äº›æŠ€æœ¯å¦‚ä½•ç»“åˆèµ·æ¥äº§ç”Ÿæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯ Roblox å·¥ç¨‹å›¢é˜Ÿåœ¨ä»–ä»¬çš„æ–‡ç« [â€œæˆ‘ä»¬å¦‚ä½•åœ¨ CPU ä¸Šæ‰©å±• BERT ä»¥å¤„ç† 10 äº¿+æ—¥è¯·æ±‚â€](https://oreil.ly/QdNIk)ä¸­é‡‡å–çš„æ–¹æ³•ï¼Œæ­£å¦‚å›¾ 8-1 æ‰€ç¤ºï¼Œä»–ä»¬å‘ç°ç»“åˆçŸ¥è¯†è’¸é¦å’Œé‡åŒ–ä½¿ä»–ä»¬çš„ BERT åˆ†ç±»å™¨çš„å»¶è¿Ÿå’Œååé‡æé«˜äº† 30 å€ä»¥ä¸Šï¼

![åœ¨ Roblox æ‰©å±• BERT](img/nlpt_0801.png)

###### å›¾ 8-1\. Roblox å¦‚ä½•é€šè¿‡çŸ¥è¯†è’¸é¦ã€åŠ¨æ€å¡«å……å’Œæƒé‡é‡åŒ–æ‰©å±• BERTï¼ˆç…§ç‰‡ç”± Roblox å‘˜å·¥ Quoc N. Le å’Œ Kip Kaehler æä¾›ï¼‰

ä¸ºäº†è¯´æ˜ä¸æ¯ç§æŠ€æœ¯ç›¸å…³çš„å¥½å¤„å’Œæƒè¡¡ï¼Œæˆ‘ä»¬å°†ä»¥æ„å›¾æ£€æµ‹ä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼›è¿™æ˜¯åŸºäºæ–‡æœ¬çš„åŠ©æ‰‹çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½å»¶è¿Ÿå¯¹äºå®æ—¶ç»´æŒå¯¹è¯è‡³å…³é‡è¦ã€‚åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰è®­ç»ƒå™¨ï¼Œæ‰§è¡Œé«˜æ•ˆçš„è¶…å‚æ•°æœç´¢ï¼Œå¹¶äº†è§£å®æ–½æœ€å‰æ²¿ç ”ç©¶æ‰€éœ€çš„å†…å®¹ï¼Œä½¿ç”¨![nlpt_pin01](img/nlpt_pin01.png) Transformersã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

# ä»¥æ„å›¾æ£€æµ‹ä¸ºæ¡ˆä¾‹ç ”ç©¶

å‡è®¾æˆ‘ä»¬æ­£åœ¨å°è¯•ä¸ºå…¬å¸çš„å‘¼å«ä¸­å¿ƒæ„å»ºä¸€ä¸ªåŸºäºæ–‡æœ¬çš„åŠ©æ‰‹ï¼Œä»¥ä¾¿å®¢æˆ·å¯ä»¥åœ¨ä¸éœ€è¦ä¸äººç±»ä»£ç†äº¤è°ˆçš„æƒ…å†µä¸‹è¯·æ±‚å…¶è´¦æˆ·ä½™é¢æˆ–è¿›è¡Œé¢„è®¢ã€‚ä¸ºäº†ç†è§£å®¢æˆ·çš„ç›®æ ‡ï¼Œæˆ‘ä»¬çš„åŠ©æ‰‹éœ€è¦èƒ½å¤Ÿå°†å„ç§è‡ªç„¶è¯­è¨€æ–‡æœ¬åˆ†ç±»ä¸ºä¸€ç»„é¢„å®šä¹‰çš„åŠ¨ä½œæˆ–*æ„å›¾*ã€‚ä¾‹å¦‚ï¼Œå®¢æˆ·å¯èƒ½ä¼šå‘é€ä»¥ä¸‹å…³äºå³å°†åˆ°æ¥çš„æ—…è¡Œçš„æ¶ˆæ¯ï¼š

> å˜¿ï¼Œæˆ‘æƒ³åœ¨ 11 æœˆ 1 æ—¥åˆ° 11 æœˆ 15 æ—¥åœ¨å·´é»ç§Ÿä¸€è¾†è½¦ï¼Œæˆ‘éœ€è¦ä¸€è¾† 15 åº§ä½çš„é¢åŒ…è½¦ã€‚

æˆ‘ä»¬çš„æ„å›¾åˆ†ç±»å™¨å¯ä»¥è‡ªåŠ¨å°†æ­¤åˆ†ç±»ä¸º*ç§Ÿè½¦*æ„å›¾ï¼Œç„¶åè§¦å‘ä¸€ä¸ªåŠ¨ä½œå’Œå“åº”ã€‚ä¸ºäº†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å…·æœ‰é²æ£’æ€§ï¼Œæˆ‘ä»¬çš„åˆ†ç±»å™¨è¿˜éœ€è¦èƒ½å¤Ÿå¤„ç†*è¶…å‡ºèŒƒå›´*çš„æŸ¥è¯¢ï¼Œå³å®¢æˆ·æå‡ºä¸å±äºä»»ä½•é¢„å®šä¹‰æ„å›¾çš„æŸ¥è¯¢ï¼Œç³»ç»Ÿåº”è¯¥äº§ç”Ÿä¸€ä¸ªå›é€€å“åº”ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾ 8-2 ä¸­æ˜¾ç¤ºçš„ç¬¬äºŒç§æƒ…å†µä¸­ï¼Œå®¢æˆ·è¯¢é—®æœ‰å…³ä½“è‚²çš„é—®é¢˜ï¼ˆè¶…å‡ºèŒƒå›´ï¼‰ï¼Œæ–‡æœ¬åŠ©æ‰‹é”™è¯¯åœ°å°†å…¶åˆ†ç±»ä¸ºå·²çŸ¥çš„èŒƒå›´å†…æ„å›¾ä¹‹ä¸€ï¼Œå¹¶è¿”å›å‘è–ªæ—¥çš„å“åº”ã€‚åœ¨ç¬¬ä¸‰ç§æƒ…å†µä¸‹ï¼Œæ–‡æœ¬åŠ©æ‰‹å·²ç»è¢«è®­ç»ƒæ¥æ£€æµ‹è¶…å‡ºèŒƒå›´çš„æŸ¥è¯¢ï¼ˆé€šå¸¸æ ‡è®°ä¸ºä¸€ä¸ªå•ç‹¬çš„ç±»ï¼‰ï¼Œå¹¶å‘ŠçŸ¥å®¢æˆ·å®ƒå¯ä»¥å›ç­”å…³äºå“ªäº›ä¸»é¢˜çš„é—®é¢˜ã€‚

![è¶…å‡ºèŒƒå›´çš„æŸ¥è¯¢](img/nlpt_0802.png)

###### å›¾ 8-2\. äººç±»ï¼ˆå³ï¼‰å’ŒåŸºäºæ–‡æœ¬çš„åŠ©æ‰‹ï¼ˆå·¦ï¼‰ä¹‹é—´çš„ä¸‰æ¬¡äº¤æµï¼Œæ¶‰åŠä¸ªäººç†è´¢ï¼ˆç”± Stefan Larson ç­‰äººæä¾›ï¼‰

ä½œä¸ºåŸºå‡†ï¼Œæˆ‘ä»¬å¾®è°ƒäº†ä¸€ä¸ª BERT-base æ¨¡å‹ï¼Œåœ¨ CLINC150 æ•°æ®é›†ä¸Šè¾¾åˆ°äº†çº¦ 94%çš„å‡†ç¡®æ€§ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…æ‹¬ 150 ä¸ªæ„å›¾å’Œ 10 ä¸ªé¢†åŸŸï¼ˆå¦‚é“¶è¡Œå’Œæ—…è¡Œï¼‰ä¸­çš„ 22,500 ä¸ªèŒƒå›´å†…æŸ¥è¯¢ï¼Œè¿˜åŒ…æ‹¬å±äº`oos`æ„å›¾ç±»åˆ«çš„ 1,200 ä¸ªèŒƒå›´å¤–æŸ¥è¯¢ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬è¿˜ä¼šæ”¶é›†è‡ªå·±çš„å†…éƒ¨æ•°æ®é›†ï¼Œä½†ä½¿ç”¨å…¬å…±æ•°æ®æ˜¯å¿«é€Ÿè¿­ä»£å’Œç”Ÿæˆåˆæ­¥ç»“æœçš„å¥½æ–¹æ³•ã€‚

è®©æˆ‘ä»¬ä» Hugging Face Hub ä¸‹è½½æˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹ï¼Œå¹¶å°†å…¶åŒ…è£…æˆæ–‡æœ¬åˆ†ç±»çš„ç®¡é“ï¼š

```py
from transformers import pipeline

bert_ckpt = "transformersbook/bert-base-uncased-finetuned-clinc"
pipe = pipeline("text-classification", model=bert_ckpt)
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªç®¡é“ï¼Œæˆ‘ä»¬å¯ä»¥ä¼ é€’ä¸€ä¸ªæŸ¥è¯¢ä»¥ä»æ¨¡å‹è·å–é¢„æµ‹çš„æ„å›¾å’Œç½®ä¿¡åº¦åˆ†æ•°ï¼š

```py
query = """Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in
Paris and I need a 15 passenger van"""
pipe(query)
```

```py
[{'label': 'car_rental', 'score': 0.549003541469574}]
```

å¾ˆå¥½ï¼Œ`car_rental`æ„å›¾æ˜¯æœ‰æ„ä¹‰çš„ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹åˆ›å»ºä¸€ä¸ªåŸºå‡†ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ¥è¯„ä¼°æˆ‘ä»¬åŸºå‡†æ¨¡å‹çš„æ€§èƒ½ã€‚

# åˆ›å»ºæ€§èƒ½åŸºå‡†

ä¸å…¶ä»–æœºå™¨å­¦ä¹ æ¨¡å‹ä¸€æ ·ï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½² transformers æ¶‰åŠåœ¨å‡ ä¸ªçº¦æŸæ¡ä»¶ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œæœ€å¸¸è§çš„æ˜¯ï¼š

*æ¨¡å‹æ€§èƒ½*

æˆ‘ä»¬çš„æ¨¡å‹åœ¨åæ˜ ç”Ÿäº§æ•°æ®çš„ç²¾å¿ƒè®¾è®¡çš„æµ‹è¯•é›†ä¸Šè¡¨ç°å¦‚ä½•ï¼Ÿå½“é”™è¯¯çš„æˆæœ¬å¾ˆé«˜æ—¶ï¼ˆæœ€å¥½é€šè¿‡äººä¸ºå¹²é¢„æ¥å‡è½»ï¼‰ï¼Œæˆ–è€…å½“æˆ‘ä»¬éœ€è¦å¯¹æ•°ç™¾ä¸‡ä¸ªç¤ºä¾‹è¿›è¡Œæ¨æ–­ï¼Œå¹¶ä¸”æ¨¡å‹æŒ‡æ ‡çš„å°å¹…æ”¹è¿›å¯ä»¥è½¬åŒ–ä¸ºå¤§å¹…å¢ç›Šæ—¶ï¼Œè¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦ã€‚

*å»¶è¿Ÿ*

æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå¤šå¿«åœ°æä¾›é¢„æµ‹ï¼Ÿæˆ‘ä»¬é€šå¸¸å…³å¿ƒå®æ—¶ç¯å¢ƒä¸­çš„å»¶è¿Ÿï¼Œè¿™äº›ç¯å¢ƒå¤„ç†å¤§é‡æµé‡ï¼Œå°±åƒ Stack Overflow éœ€è¦ä¸€ä¸ªåˆ†ç±»å™¨æ¥å¿«é€Ÿ[æ£€æµ‹ç½‘ç«™ä¸Šä¸å—æ¬¢è¿çš„è¯„è®º](https://oreil.ly/cf7QX)ä¸€æ ·ã€‚

*å†…å­˜*

æˆ‘ä»¬å¦‚ä½•éƒ¨ç½²åƒ GPT-2 æˆ– T5 è¿™æ ·éœ€è¦å ç”¨å‡  GB ç£ç›˜å­˜å‚¨å’Œå†…å­˜çš„ç™¾äº¿å‚æ•°æ¨¡å‹ï¼Ÿå†…å­˜åœ¨ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¾å¤‡ä¸­æ‰®æ¼”ç€ç‰¹åˆ«é‡è¦çš„è§’è‰²ï¼Œå› ä¸ºæ¨¡å‹å¿…é¡»åœ¨æ²¡æœ‰å¼ºå¤§çš„äº‘æœåŠ¡å™¨çš„æƒ…å†µä¸‹ç”Ÿæˆé¢„æµ‹ã€‚

æœªèƒ½è§£å†³è¿™äº›çº¦æŸæ¡ä»¶å¯èƒ½ä¼šå¯¹åº”ç”¨ç¨‹åºçš„ç”¨æˆ·ä½“éªŒäº§ç”Ÿè´Ÿé¢å½±å“ã€‚æ›´å¸¸è§çš„æ˜¯ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¿è¡Œæ˜‚è´µçš„äº‘æœåŠ¡å™¨çš„æˆæœ¬æ¿€å¢ï¼Œè€Œè¿™äº›æœåŠ¡å™¨å¯èƒ½åªéœ€è¦å¤„ç†å°‘é‡è¯·æ±‚ã€‚ä¸ºäº†æ¢ç´¢å¦‚ä½•ä½¿ç”¨å„ç§å‹ç¼©æŠ€æœ¯ä¼˜åŒ–è¿™äº›çº¦æŸæ¡ä»¶ï¼Œè®©æˆ‘ä»¬ä»åˆ›å»ºä¸€ä¸ªç®€å•çš„åŸºå‡†å¼€å§‹ï¼Œè¯¥åŸºå‡†å¯ä»¥æµ‹é‡ç»™å®šç®¡é“å’Œæµ‹è¯•é›†çš„æ¯ä¸ªæ•°é‡ï¼š

```py
class PerformanceBenchmark:
    def __init__(self, pipeline, dataset, optim_type="BERT baseline"):
        self.pipeline = pipeline
        self.dataset = dataset
        self.optim_type = optim_type

    def compute_accuracy(self):
        # We'll define this later
        pass

    def compute_size(self):
        # We'll define this later
        pass

    def time_pipeline(self):
        # We'll define this later
        pass

    def run_benchmark(self):
        metrics = {}
        metrics[self.optim_type] = self.compute_size()
        metrics[self.optim_type].update(self.time_pipeline())
        metrics[self.optim_type].update(self.compute_accuracy())
        return metrics
```

æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª`optim_type`å‚æ•°ï¼Œä»¥è·Ÿè¸ªæˆ‘ä»¬åœ¨æœ¬ç« ä¸­å°†æ¶µç›–çš„ä¸åŒä¼˜åŒ–æŠ€æœ¯ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`run_benchmark()`æ–¹æ³•å°†æ‰€æœ‰æŒ‡æ ‡æ”¶é›†åˆ°ä¸€ä¸ªå­—å…¸ä¸­ï¼Œé”®ç”±`optim_type`ç»™å‡ºã€‚

è®©æˆ‘ä»¬ç°åœ¨é€šè¿‡åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—æ¨¡å‹çš„å‡†ç¡®æ€§æ¥ä¸ºè¿™ä¸ªç±»æ·»åŠ ä¸€äº›å…·ä½“å†…å®¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›æ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä¸‹è½½ç”¨äºå¾®è°ƒåŸºå‡†æ¨¡å‹çš„ CLINC150 æ•°æ®é›†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä» Hub è·å–æ•°æ®é›†ï¼š![nlpt_pin01](img/nlpt_pin01.png)ã€‚

```py
from datasets import load_dataset

clinc = load_dataset("clinc_oos", "plus")
```

åœ¨è¿™é‡Œï¼Œ`plus`é…ç½®æ˜¯æŒ‡åŒ…å«è¶…å‡ºèŒƒå›´çš„è®­ç»ƒç¤ºä¾‹çš„å­é›†ã€‚CLINC150 æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹éƒ½åŒ…æ‹¬`text`åˆ—ä¸­çš„æŸ¥è¯¢åŠå…¶å¯¹åº”çš„æ„å›¾ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æµ‹è¯•é›†æ¥å¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ•°æ®é›†çš„ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
sample = clinc["test"][42]
sample
```

```py
{'intent': 133, 'text': 'transfer $100 from my checking to saving account'}
```

æ„å›¾ä»¥ ID çš„å½¢å¼æä¾›ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¿é—®æ•°æ®é›†çš„`features`å±æ€§è½»æ¾è·å–åˆ°å­—ç¬¦ä¸²çš„æ˜ å°„ï¼ˆåä¹‹äº¦ç„¶ï¼‰ï¼š

```py
intents = clinc["test"].features["intent"]
intents.int2str(sample["intent"])
```

```py
'transfer'
```

ç°åœ¨æˆ‘ä»¬å¯¹ CLINC150 æ•°æ®é›†çš„å†…å®¹æœ‰äº†åŸºæœ¬çš„äº†è§£ï¼Œè®©æˆ‘ä»¬å®ç°`PerformanceBenchmark`çš„`compute_accuracy()`æ–¹æ³•ã€‚ç”±äºæ•°æ®é›†åœ¨æ„å›¾ç±»åˆ«ä¸Šæ˜¯å¹³è¡¡çš„ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‡†ç¡®æ€§ä½œä¸ºæˆ‘ä»¬çš„åº¦é‡æ ‡å‡†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨![nlpt_pin01](img/nlpt_pin01.png)æ•°æ®é›†åŠ è½½è¿™ä¸ªåº¦é‡æ ‡å‡†ï¼š

```py
from datasets import load_metric

accuracy_score = load_metric("accuracy")
```

å‡†ç¡®åº¦æŒ‡æ ‡æœŸæœ›é¢„æµ‹å’Œå‚è€ƒï¼ˆå³ï¼ŒçœŸå®æ ‡ç­¾ï¼‰æ˜¯æ•´æ•°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç®¡é“ä»`text`å­—æ®µä¸­æå–é¢„æµ‹ï¼Œç„¶åä½¿ç”¨æˆ‘ä»¬çš„`intents`å¯¹è±¡çš„â€œstr2intï¼ˆï¼‰â€æ–¹æ³•å°†æ¯ä¸ªé¢„æµ‹æ˜ å°„åˆ°å…¶ç›¸åº”çš„ IDã€‚ä»¥ä¸‹ä»£ç åœ¨è¿”å›æ•°æ®é›†çš„å‡†ç¡®åº¦ä¹‹å‰æ”¶é›†æ‰€æœ‰çš„é¢„æµ‹å’Œæ ‡ç­¾ã€‚è®©æˆ‘ä»¬ä¹Ÿå°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„â€œPerformanceBenchmarkâ€ç±»ä¸­ï¼š

```py
def compute_accuracy(self):
    """This overrides the PerformanceBenchmark.compute_accuracy() method"""
    preds, labels = [], []
    for example in self.dataset:
        pred = self.pipeline(example["text"])[0]["label"]
        label = example["intent"]
        preds.append(intents.str2int(pred))
        labels.append(label)
    accuracy = accuracy_score.compute(predictions=preds, references=labels)
    print(f"Accuracy on test set - {accuracy['accuracy']:.3f}")
    return accuracy

PerformanceBenchmark.compute_accuracy = compute_accuracy
```

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ PyTorch çš„â€œtorch.saveï¼ˆï¼‰â€å‡½æ•°æ¥è®¡ç®—æˆ‘ä»¬æ¨¡å‹çš„å¤§å°ï¼Œå°†æ¨¡å‹åºåˆ—åŒ–åˆ°ç£ç›˜ä¸Šã€‚åœ¨å†…éƒ¨ï¼Œâ€œtorch.saveï¼ˆï¼‰â€ä½¿ç”¨ Python çš„`pickle`æ¨¡å—ï¼Œå¯ä»¥ç”¨æ¥ä¿å­˜ä»æ¨¡å‹åˆ°å¼ é‡åˆ°æ™®é€š Python å¯¹è±¡çš„ä»»ä½•ä¸œè¥¿ã€‚åœ¨ PyTorch ä¸­ï¼Œä¿å­˜æ¨¡å‹çš„æ¨èæ–¹å¼æ˜¯ä½¿ç”¨å®ƒçš„`state_dict`ï¼Œè¿™æ˜¯ä¸€ä¸ª Python å­—å…¸ï¼Œå°†æ¨¡å‹ä¸­çš„æ¯ä¸€å±‚æ˜ å°„åˆ°å®ƒçš„å¯å­¦ä¹ å‚æ•°ï¼ˆå³ï¼Œæƒé‡å’Œåç½®ï¼‰ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åŸºå‡†æ¨¡å‹çš„`state_dict`ä¸­å­˜å‚¨äº†ä»€ä¹ˆï¼š

```py
list(pipe.model.state_dict().items())[42]
```

```py
('bert.encoder.layer.2.attention.self.value.weight',
 tensor([[-1.0526e-02, -3.2215e-02,  2.2097e-02,  ..., -6.0953e-03,
           4.6521e-03,  2.9844e-02],
         [-1.4964e-02, -1.0915e-02,  5.2396e-04,  ...,  3.2047e-05,
          -2.6890e-02, -2.1943e-02],
         [-2.9640e-02, -3.7842e-03, -1.2582e-02,  ..., -1.0917e-02,
           3.1152e-02, -9.7786e-03],
         ...,
         [-1.5116e-02, -3.3226e-02,  4.2063e-02,  ..., -5.2652e-03,
           1.1093e-02,  2.9703e-03],
         [-3.6809e-02,  5.6848e-02, -2.6544e-02,  ..., -4.0114e-02,
           6.7487e-03,  1.0511e-03],
         [-2.4961e-02,  1.4747e-03, -5.4271e-02,  ...,  2.0004e-02,
           2.3981e-02, -4.2880e-02]]))
```

æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°æ¯ä¸ªé”®/å€¼å¯¹å¯¹åº”äº BERT ä¸­çš„ç‰¹å®šå±‚å’Œå¼ é‡ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬ç”¨ä»¥ä¸‹æ–¹å¼ä¿å­˜æˆ‘ä»¬çš„æ¨¡å‹ï¼š

```py
torch.save(pipe.model.state_dict(), "model.pt")
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python çš„`pathlib`æ¨¡å—ä¸­çš„â€œPath.statï¼ˆï¼‰â€å‡½æ•°æ¥è·å–æœ‰å…³åº•å±‚æ–‡ä»¶çš„ä¿¡æ¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œâ€œPathï¼ˆ"model.â€‹pt"ï¼‰.â€‹statï¼ˆï¼‰.â€‹st_sizeâ€å°†ç»™å‡ºæ¨¡å‹çš„å¤§å°ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚è®©æˆ‘ä»¬å°†æ‰€æœ‰è¿™äº›æ”¾åœ¨â€œcompute_â€‹sizeï¼ˆï¼‰â€å‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°`PerformanceBenchmark`ä¸­ï¼š

```py
import torch
from pathlib import Path

def compute_size(self):
    """This overrides the PerformanceBenchmark.compute_size() method"""
    state_dict = self.pipeline.model.state_dict()
    tmp_path = Path("model.pt")
    torch.save(state_dict, tmp_path)
    # Calculate size in megabytes
    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)
    # Delete temporary file
    tmp_path.unlink()
    print(f"Model size (MB) - {size_mb:.2f}")
    return {"size_mb": size_mb}

PerformanceBenchmark.compute_size = compute_size
```

æœ€åï¼Œè®©æˆ‘ä»¬å®ç°â€œtime_pipelineï¼ˆï¼‰â€å‡½æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªæŸ¥è¯¢çš„å¹³å‡å»¶è¿Ÿæ—¶é—´ã€‚å¯¹äºè¿™ä¸ªåº”ç”¨ç¨‹åºï¼Œå»¶è¿Ÿæ—¶é—´æŒ‡çš„æ˜¯å°†æ–‡æœ¬æŸ¥è¯¢è¾“å…¥åˆ°ç®¡é“ä¸­å¹¶ä»æ¨¡å‹è¿”å›é¢„æµ‹æ„å›¾æ‰€éœ€çš„æ—¶é—´ã€‚åœ¨å†…éƒ¨ï¼Œç®¡é“è¿˜ä¼šå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œä½†è¿™æ¯”ç”Ÿæˆé¢„æµ‹å¿«äº†å¤§çº¦ä¸€åƒå€ï¼Œå› æ­¤å¯¹æ•´ä½“å»¶è¿Ÿæ—¶é—´çš„è´¡çŒ®å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚è¡¡é‡ä»£ç ç‰‡æ®µçš„æ‰§è¡Œæ—¶é—´çš„ä¸€ä¸ªç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨ Python çš„`time`æ¨¡å—ä¸­çš„â€œperf_counterï¼ˆï¼‰â€å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°æ¯”â€œtime.timeï¼ˆï¼‰â€å‡½æ•°å…·æœ‰æ›´å¥½çš„æ—¶é—´åˆ†è¾¨ç‡ï¼Œéå¸¸é€‚åˆè·å–ç²¾ç¡®çš„ç»“æœã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨â€œperf_counterï¼ˆï¼‰â€é€šè¿‡ä¼ é€’æˆ‘ä»¬çš„æµ‹è¯•æŸ¥è¯¢æ¥è®¡æ—¶æˆ‘ä»¬çš„ç®¡é“ï¼Œå¹¶è®¡ç®—å¼€å§‹å’Œç»“æŸä¹‹é—´çš„æ¯«ç§’æ—¶é—´å·®ï¼š

```py
from time import perf_counter

for _ in range(3):
    start_time = perf_counter()
    _ = pipe(query)
    latency = perf_counter() - start_time
    print(f"Latency (ms) - {1000 * latency:.3f}")
```

```py
Latency (ms) - 85.367
Latency (ms) - 85.241
Latency (ms) - 87.275
```

è¿™äº›ç»“æœå±•ç¤ºäº†å»¶è¿Ÿæ—¶é—´çš„ç›¸å½“å¤§çš„å·®å¼‚ï¼Œå¹¶ä¸”è¡¨æ˜é€šè¿‡ç®¡é“çš„å•æ¬¡è®¡æ—¶å¯èƒ½æ¯æ¬¡è¿è¡Œä»£ç æ—¶éƒ½ä¼šå¾—åˆ°å®Œå…¨ä¸åŒçš„ç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ”¶é›†å¤šæ¬¡è¿è¡Œçš„å»¶è¿Ÿæ—¶é—´ï¼Œç„¶åä½¿ç”¨å¾—åˆ°çš„åˆ†å¸ƒæ¥è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼Œè¿™å°†è®©æˆ‘ä»¬å¯¹æ•°å€¼çš„å·®å¼‚æœ‰ä¸€ä¸ªæ¦‚å¿µã€‚ä»¥ä¸‹ä»£ç å®ç°äº†æˆ‘ä»¬éœ€è¦çš„åŠŸèƒ½ï¼Œå¹¶åŒ…æ‹¬äº†åœ¨æ‰§è¡Œå®é™…è®¡æ—¶è¿è¡Œä¹‹å‰é¢„çƒ­ CPU çš„é˜¶æ®µï¼š

```py
import numpy as np

def time_pipeline(self, query="What is the pin number for my account?"):
    """This overrides the PerformanceBenchmark.time_pipeline() method"""
    latencies = []
    # Warmup
    for _ in range(10):
        _ = self.pipeline(query)
    # Timed run
    for _ in range(100):
        start_time = perf_counter()
        _ = self.pipeline(query)
        latency = perf_counter() - start_time
        latencies.append(latency)
    # Compute run statistics
    time_avg_ms = 1000 * np.mean(latencies)
    time_std_ms = 1000 * np.std(latencies)
    print(f"Average latency (ms) - {time_avg_ms:.2f} +\- {time_std_ms:.2f}")
    return {"time_avg_ms": time_avg_ms, "time_std_ms": time_std_ms}

PerformanceBenchmark.time_pipeline = time_pipeline
```

ä¸ºäº†ç®€åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„`query`å€¼æ¥å¯¹æˆ‘ä»¬æ‰€æœ‰çš„æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå»¶è¿Ÿæ—¶é—´å°†å–å†³äºæŸ¥è¯¢é•¿åº¦ï¼Œä¸€ä¸ªå¥½çš„åšæ³•æ˜¯ä½¿ç”¨æ¨¡å‹å¯èƒ½åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é‡åˆ°çš„æŸ¥è¯¢æ¥å¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

ç°åœ¨æˆ‘ä»¬çš„`PerformanceBenchmark`ç±»å·²ç»å®Œæˆï¼Œè®©æˆ‘ä»¬æ¥è¯•ä¸€è¯•å§ï¼è®©æˆ‘ä»¬ä»å¯¹æˆ‘ä»¬çš„ BERT åŸºå‡†æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•å¼€å§‹ã€‚å¯¹äºåŸºå‡†æ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è¦ä¼ é€’ç®¡é“å’Œæˆ‘ä»¬å¸Œæœ›è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å°†åœ¨`perf_metrics`å­—å…¸ä¸­æ”¶é›†ç»“æœï¼Œä»¥è·Ÿè¸ªæ¯ä¸ªæ¨¡å‹çš„æ€§èƒ½ï¼š

```py
pb = PerformanceBenchmark(pipe, clinc["test"])
perf_metrics = pb.run_benchmark()
```

```py
Model size (MB) - 418.16
Average latency (ms) - 54.20 +\- 1.91
Accuracy on test set - 0.867
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå‚è€ƒç‚¹ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå‹ç¼©æŠ€æœ¯ï¼šçŸ¥è¯†è’¸é¦ã€‚

###### æ³¨æ„

å¹³å‡å»¶è¿Ÿå€¼å°†å–å†³äºæ‚¨æ‰€è¿è¡Œçš„ç¡¬ä»¶ç±»å‹ã€‚ä¾‹å¦‚ï¼Œé€šå¸¸å¯ä»¥é€šè¿‡åœ¨ GPU ä¸Šè¿è¡Œæ¨æ–­æ¥è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œå› ä¸ºå®ƒå¯ä»¥å®ç°æ‰¹å¤„ç†ã€‚å¯¹äºæœ¬ç« çš„ç›®çš„ï¼Œé‡è¦çš„æ˜¯æ¨¡å‹ä¹‹é—´å»¶è¿Ÿæ—¶é—´çš„ç›¸å¯¹å·®å¼‚ã€‚ä¸€æ—¦ç¡®å®šäº†æ€§èƒ½æœ€ä½³çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥æ¢ç´¢ä¸åŒçš„åç«¯æ¥å‡å°‘ç»å¯¹å»¶è¿Ÿæ—¶é—´ï¼ˆå¦‚æœéœ€è¦ï¼‰ã€‚

# é€šè¿‡çŸ¥è¯†è’¸é¦ä½¿æ¨¡å‹å˜å¾—æ›´å°

çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒä¸€ä¸ªè¾ƒå°çš„â€œå­¦ç”Ÿâ€æ¨¡å‹æ¥æ¨¡ä»¿é€Ÿåº¦è¾ƒæ…¢ã€æ›´å¤§ä½†æ€§èƒ½æ›´å¥½çš„â€œæ•™å¸ˆâ€æ¨¡å‹çš„è¡Œä¸ºã€‚æœ€åˆæ˜¯åœ¨ 2006 å¹´åœ¨é›†æˆæ¨¡å‹çš„èƒŒæ™¯ä¸‹å¼•å…¥çš„ï¼Œåæ¥åœ¨ä¸€ç¯‡è‘—åçš„ 2015 å¹´è®ºæ–‡ä¸­å°†è¯¥æ–¹æ³•æ¨å¹¿åˆ°æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¹¶å°†å…¶åº”ç”¨äºå›¾åƒåˆ†ç±»å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚

é‰´äºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å‚æ•°æ•°é‡ä¸æ–­å¢åŠ çš„è¶‹åŠ¿ï¼ˆæ’°å†™æ—¶æœ€å¤§çš„æ¨¡å‹å‚æ•°è¶…è¿‡ä¸€ä¸‡äº¿ï¼‰ï¼ŒçŸ¥è¯†è’¸é¦ä¹Ÿæˆä¸ºå‹ç¼©è¿™äº›åºå¤§æ¨¡å‹å¹¶ä½¿å…¶æ›´é€‚åˆæ„å»ºå®é™…åº”ç”¨çš„æµè¡Œç­–ç•¥ã€‚

## å¾®è°ƒçš„çŸ¥è¯†è’¸é¦

é‚£ä¹ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒçŸ¥è¯†å®é™…ä¸Šæ˜¯å¦‚ä½•ä»æ•™å¸ˆä¼ é€’ç»™å­¦ç”Ÿçš„å‘¢ï¼Ÿå¯¹äºå¾®è°ƒç­‰ç›‘ç£ä»»åŠ¡ï¼Œä¸»è¦æ€æƒ³æ˜¯ç”¨æ•™å¸ˆçš„â€œè½¯æ¦‚ç‡â€åˆ†å¸ƒæ¥å¢å¼ºåœ°é¢çœŸå®æ ‡ç­¾ï¼Œä¸ºå­¦ç”Ÿæä¾›è¡¥å……ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çš„ BERT-base åˆ†ç±»å™¨ä¸ºå¤šä¸ªæ„å›¾åˆ†é…é«˜æ¦‚ç‡ï¼Œé‚£ä¹ˆè¿™å¯èƒ½è¡¨æ˜è¿™äº›æ„å›¾åœ¨ç‰¹å¾ç©ºé—´ä¸­ç›¸äº’é è¿‘ã€‚é€šè¿‡è®­ç»ƒå­¦ç”Ÿæ¨¡ä»¿è¿™äº›æ¦‚ç‡ï¼Œç›®æ ‡æ˜¯è’¸é¦æ•™å¸ˆå­¦åˆ°çš„ä¸€äº›â€œæš—çŸ¥è¯†â€â€”â€”ä¹Ÿå°±æ˜¯ï¼Œä»…ä»æ ‡ç­¾ä¸­æ— æ³•è·å¾—çš„çŸ¥è¯†ã€‚

ä»æ•°å­¦ä¸Šè®²ï¼Œè¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚å‡è®¾æˆ‘ä»¬å°†è¾“å…¥åºåˆ—*x*æä¾›ç»™æ•™å¸ˆï¼Œä»¥ç”Ÿæˆä¸€ä¸ªå¯¹æ•°å‘é‡<math alttext="bold z left-parenthesis x right-parenthesis"><mrow><mi>ğ³</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> = [ <math alttext="z 1 left-parenthesis x right-parenthesis comma ellipsis comma z Subscript upper N Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>z</mi> <mn>1</mn></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>z</mi> <mi>N</mi></sub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> ]ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åº”ç”¨ softmax å‡½æ•°å°†è¿™äº›å¯¹æ•°è½¬æ¢ä¸ºæ¦‚ç‡ï¼š

<math alttext="StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis right-parenthesis Over sigma-summation Underscript j Endscripts exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis right-parenthesis EndFraction" display="block"><mrow><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow> <mrow><msub><mo>âˆ‘</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></math>

ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Œå› ä¸ºåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæ•™å¸ˆä¼šä¸ºä¸€ä¸ªç±»åˆ†é…é«˜æ¦‚ç‡ï¼Œè€Œå…¶ä»–ç±»çš„æ¦‚ç‡æ¥è¿‘äºé›¶ã€‚å½“å‘ç”Ÿè¿™ç§æƒ…å†µæ—¶ï¼Œæ•™å¸ˆé™¤äº†åœ°é¢çœŸå®æ ‡ç­¾å¤–å¹¶æ²¡æœ‰æä¾›å¤ªå¤šé¢å¤–ä¿¡æ¯ï¼Œå› æ­¤æˆ‘ä»¬ä¼šåœ¨åº”ç”¨ softmax ä¹‹å‰ï¼Œé€šè¿‡ä¸€ä¸ªæ¸©åº¦è¶…å‚æ•°*T*æ¥ç¼©æ”¾å¯¹æ•°ï¼Œä»è€Œâ€œè½¯åŒ–â€æ¦‚ç‡ã€‚

<math alttext="p Subscript i Baseline left-parenthesis x right-parenthesis equals StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis slash upper T right-parenthesis Over sigma-summation Underscript j Endscripts exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis slash upper T right-parenthesis EndFraction" display="block"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow> <mrow><msub><mo>âˆ‘</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>

å¦‚å›¾ 8-3 æ‰€ç¤ºï¼Œ*T*çš„å€¼è¶Šé«˜ï¼Œç±»åˆ«ä¸Šçš„è½¯åŒ–æ¦‚ç‡åˆ†å¸ƒå°±è¶Šè½¯ï¼Œå¯ä»¥æ›´å¤šåœ°æ­ç¤ºè€å¸ˆå¯¹æ¯ä¸ªè®­ç»ƒç¤ºä¾‹å­¦ä¹ çš„å†³ç­–è¾¹ç•Œã€‚å½“<math alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>æ—¶ï¼Œæˆ‘ä»¬æ¢å¤äº†åŸå§‹çš„ softmax åˆ†å¸ƒã€‚

![è½¯æ¦‚ç‡](img/nlpt_0803.png)

###### å›¾ 8-3ã€‚ä¸€ä¸ªä½¿ç”¨ one-hot ç¼–ç çš„ç¡¬æ ‡ç­¾ï¼ˆå·¦ï¼‰ã€softmax æ¦‚ç‡ï¼ˆä¸­ï¼‰å’Œè½¯åŒ–ç±»åˆ«æ¦‚ç‡ï¼ˆå³ï¼‰çš„æ¯”è¾ƒã€‚

ç”±äºå­¦ç”Ÿè¿˜äº§ç”Ÿäº†è‡ªå·±çš„è½¯åŒ–æ¦‚ç‡<math alttext="q Subscript i Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨[Kullback-Leiblerï¼ˆKLï¼‰](https://oreil.ly/8nKQG)æ•£åº¦æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼š

<math alttext="upper D Subscript upper K upper L Baseline left-parenthesis p comma q right-parenthesis equals sigma-summation Underscript i Endscripts p Subscript i Baseline left-parenthesis x right-parenthesis log StartFraction p Subscript i Baseline left-parenthesis x right-parenthesis Over q Subscript i Baseline left-parenthesis x right-parenthesis EndFraction" display="block"><mrow><msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub> <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo> <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>âˆ‘</mo> <mi>i</mi></munder> <msub><mi>p</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>

é€šè¿‡ KL æ•£åº¦ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å½“æˆ‘ä»¬ç”¨å­¦ç”Ÿæ¥è¿‘ä¼¼è€å¸ˆçš„æ¦‚ç‡åˆ†å¸ƒæ—¶æŸå¤±äº†å¤šå°‘ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå®šä¹‰çŸ¥è¯†è’¸é¦æŸå¤±ï¼š

<math alttext="upper L Subscript upper K upper D Baseline equals upper T squared upper D Subscript upper K upper L" display="block"><mrow><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub> <mo>=</mo> <msup><mi>T</mi> <mn>2</mn></msup> <msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub></mrow></math>

å…¶ä¸­<math alttext="upper T squared"><msup><mi>T</mi> <mn>2</mn></msup></math>æ˜¯ä¸€ä¸ªå½’ä¸€åŒ–å› å­ï¼Œç”¨äºè€ƒè™‘è½¯æ ‡ç­¾äº§ç”Ÿçš„æ¢¯åº¦å¤§å°æŒ‰<math alttext="1 slash upper T squared"><mrow><mn>1</mn> <mo>/</mo> <msup><mi>T</mi> <mn>2</mn></msup></mrow></math>ç¼©æ”¾çš„äº‹å®ã€‚å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œå­¦ç”Ÿçš„æŸå¤±æ˜¯è’¸é¦æŸå¤±å’Œåœ°é¢çœŸå®æ ‡ç­¾çš„äº¤å‰ç†µæŸå¤±<math alttext="upper L Subscript upper C upper E"><msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub></math>çš„åŠ æƒå¹³å‡ï¼š

<math alttext="upper L Subscript normal s normal t normal u normal d normal e normal n normal t Baseline equals alpha upper L Subscript upper C upper E Baseline plus left-parenthesis 1 minus alpha right-parenthesis upper L Subscript upper K upper D" display="block"><mrow><msub><mi>L</mi> <mi>student</mi></msub> <mo>=</mo> <mi>Î±</mi> <msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>Î±</mi> <mo>)</mo></mrow> <msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></mrow></math>

å…¶ä¸­<math alttext="alpha"><mi>Î±</mi></math>æ˜¯ä¸€ä¸ªæ§åˆ¶æ¯ä¸ªæŸå¤±ç›¸å¯¹å¼ºåº¦çš„è¶…å‚æ•°ã€‚æ•´ä¸ªè¿‡ç¨‹çš„å›¾è¡¨å¦‚å›¾ 8-4 æ‰€ç¤ºï¼›åœ¨æ¨æ–­æ—¶ï¼Œæ¸©åº¦è¢«è®¾ç½®ä¸º 1ï¼Œä»¥æ¢å¤æ ‡å‡†çš„ softmax æ¦‚ç‡ã€‚

![çŸ¥è¯†è’¸é¦](img/nlpt_0804.png)

###### å›¾ 8-4ã€‚çŸ¥è¯†è’¸é¦è¿‡ç¨‹

## é¢„è®­ç»ƒçš„çŸ¥è¯†è’¸é¦

çŸ¥è¯†è’¸é¦ä¹Ÿå¯ä»¥åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨ï¼Œä»¥åˆ›å»ºä¸€ä¸ªé€šç”¨çš„å­¦ç”Ÿæ¨¡å‹ï¼Œéšåå¯ä»¥åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ•™å¸ˆæ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚ BERTï¼Œå®ƒå°†å…¶å…³äºæ©ç è¯­è¨€å»ºæ¨¡çš„çŸ¥è¯†è½¬ç§»åˆ°å­¦ç”Ÿèº«ä¸Šã€‚ä¾‹å¦‚ï¼Œåœ¨ DistilBERT è®ºæ–‡ä¸­ï¼Œâ¸æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±<math alttext="upper L Subscript m l m"><msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub></math>è¢«çŸ¥è¯†è’¸é¦çš„ä¸€ä¸ªé¡¹å’Œä½™å¼¦åµŒå…¥æŸå¤±<math alttext="upper L Subscript c o s Baseline equals 1 minus cosine left-parenthesis h Subscript s Baseline comma h Subscript t Baseline right-parenthesis"><mrow><msub><mi>L</mi> <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub> <mo>=</mo> <mn>1</mn> <mo>-</mo> <mo form="prefix">cos</mo> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>s</mi></msub> <mo>,</mo> <msub><mi>h</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>æ¥å¯¹é½æ•™å¸ˆå’Œå­¦ç”Ÿä¹‹é—´çš„éšè—çŠ¶æ€å‘é‡çš„æ–¹å‘ï¼š

<math alttext="upper L Subscript normal upper D normal i normal s normal t normal i normal l normal upper B normal upper E normal upper R normal upper T Baseline equals alpha upper L Subscript m l m Baseline plus beta upper L Subscript upper K upper D Baseline plus gamma upper L Subscript c o s" display="block"><mrow><msub><mi>L</mi> <mi>DistilBERT</mi></msub> <mo>=</mo> <mi>Î±</mi> <msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub> <mo>+</mo> <mi>Î²</mi> <msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub> <mo>+</mo> <mi>Î³</mi> <msub><mi>L</mi> <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>

ç”±äºæˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªç»è¿‡ç²¾ç»†è°ƒæ•´çš„ BERT-base æ¨¡å‹ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨çŸ¥è¯†è’¸é¦æ¥å¯¹ä¸€ä¸ªæ›´å°æ›´å¿«çš„æ¨¡å‹è¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥å°†äº¤å‰ç†µæŸå¤±ä¸<math alttext="upper L Subscript upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>é¡¹ç›¸ç»“åˆã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ›å»ºè‡ªå·±çš„è®­ç»ƒå™¨æ¥å®ç°è¿™ä¸€ç‚¹ï¼

## åˆ›å»ºçŸ¥è¯†è’¸é¦è®­ç»ƒå™¨

è¦å®ç°çŸ¥è¯†è’¸é¦ï¼Œæˆ‘ä»¬éœ€è¦å‘`Trainer`åŸºç±»æ·»åŠ ä¸€äº›å†…å®¹ï¼š

+   æ–°çš„è¶…å‚æ•°<math alttext="alpha"><mi>Î±</mi></math>å’Œ*T*ï¼Œå®ƒä»¬æ§åˆ¶è’¸é¦æŸå¤±çš„ç›¸å¯¹æƒé‡ä»¥åŠæ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒåº”è¯¥è¢«å¹³æ»‘çš„ç¨‹åº¦

+   ç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ•™å¸ˆæ¨¡å‹ï¼Œæˆ‘ä»¬çš„æƒ…å†µä¸‹æ˜¯ BERT-base

+   ç»“åˆäº¤å‰ç†µæŸå¤±å’ŒçŸ¥è¯†è’¸é¦æŸå¤±çš„æ–°æŸå¤±å‡½æ•°

æ·»åŠ æ–°çš„è¶…å‚æ•°éå¸¸ç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦å¯¹`TrainingArguments`è¿›è¡Œå­ç±»åŒ–ï¼Œå¹¶å°†å®ƒä»¬åŒ…å«ä¸ºæ–°çš„å±æ€§ï¼š

```py
from transformers import TrainingArguments

class DistillationTrainingArguments(TrainingArguments):
    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.alpha = alpha
        self.temperature = temperature
```

å¯¹äºè®­ç»ƒå™¨æœ¬èº«ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–°çš„æŸå¤±å‡½æ•°ã€‚å®ç°è¿™ä¸€ç‚¹çš„æ–¹æ³•æ˜¯é€šè¿‡å¯¹`Trainer`è¿›è¡Œå­ç±»åŒ–ï¼Œå¹¶è¦†ç›–`compute_loss()`æ–¹æ³•ï¼Œä»¥åŒ…æ‹¬çŸ¥è¯†è’¸é¦æŸå¤±é¡¹<math alttext="upper L Subscript upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>ï¼š

```py
import torch.nn as nn
import torch.nn.functional as F
from transformers import Trainer

class DistillationTrainer(Trainer):
    def __init__(self, *args, teacher_model=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.teacher_model = teacher_model

    def compute_loss(self, model, inputs, return_outputs=False):
        outputs_stu = model(**inputs)
        # Extract cross-entropy loss and logits from student
        loss_ce = outputs_stu.loss
        logits_stu = outputs_stu.logits
        # Extract logits from teacher
        with torch.no_grad():
            outputs_tea = self.teacher_model(**inputs)
            logits_tea = outputs_tea.logits
        # Soften probabilities and compute distillation loss
        loss_fct = nn.KLDivLoss(reduction="batchmean")
        loss_kd = self.args.temperature ** 2 * loss_fct(
            F.log_softmax(logits_stu / self.args.temperature, dim=-1),
            F.softmax(logits_tea / self.args.temperature, dim=-1))
        # Return weighted student loss
        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd
        return (loss, outputs_stu) if return_outputs else loss
```

è®©æˆ‘ä»¬è§£å¼€ä¸€ä¸‹è¿™æ®µä»£ç ã€‚å½“æˆ‘ä»¬å®ä¾‹åŒ–`DistillationTrainer`æ—¶ï¼Œæˆ‘ä»¬ä¼ é€’äº†ä¸€ä¸ªå·²ç»åœ¨æˆ‘ä»¬çš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¾®è°ƒçš„è€å¸ˆæ¨¡å‹ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨`compute_loss()`æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä»å­¦ç”Ÿå’Œè€å¸ˆé‚£é‡Œæå– logitsï¼Œé€šè¿‡æ¸©åº¦å¯¹å®ƒä»¬è¿›è¡Œç¼©æ”¾ï¼Œç„¶ååœ¨ä¼ é€’ç»™ PyTorch çš„`nn.KLDivLoss()`å‡½æ•°ä¹‹å‰ï¼Œä½¿ç”¨ softmax å¯¹å®ƒä»¬è¿›è¡Œå½’ä¸€åŒ–ä»¥è®¡ç®— KL æ•£åº¦ã€‚`nn.KLDivLoss()`çš„ä¸€ä¸ªæ€ªç™–æ˜¯ï¼Œå®ƒæœŸæœ›è¾“å…¥ä»¥å¯¹æ•°æ¦‚ç‡çš„å½¢å¼ï¼Œæ ‡ç­¾ä»¥æ­£å¸¸æ¦‚ç‡çš„å½¢å¼ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨`F.log_softmax()`å‡½æ•°å¯¹å­¦ç”Ÿçš„ logits è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œè€å¸ˆçš„ logits åˆ™ä½¿ç”¨æ ‡å‡† softmax è½¬æ¢ä¸ºæ¦‚ç‡ã€‚`nn.KLDivLoss()`ä¸­çš„`reduction=batchmean`å‚æ•°æŒ‡å®šæˆ‘ä»¬åœ¨æ‰¹ç»´åº¦ä¸Šå¹³å‡æŸå¤±ã€‚

###### æç¤º

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨![nlpt_pin01](img/nlpt_pin01.png) Transformers åº“çš„ Keras API è¿›è¡ŒçŸ¥è¯†è’¸é¦ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„`Distiller`ç±»ï¼Œè¦†ç›–`tf.keras.Model()`çš„`train_step()`ã€`test_step()`å’Œ`compile()`æ–¹æ³•ã€‚è¯·å‚é˜…[Keras æ–‡æ¡£](https://oreil.ly/6qp0F)äº†è§£å¦‚ä½•å®ç°ã€‚

## é€‰æ‹©ä¸€ä¸ªå¥½çš„å­¦ç”Ÿåˆå§‹åŒ–

ç°åœ¨æˆ‘ä»¬æœ‰äº†è‡ªå®šä¹‰çš„è®­ç»ƒå™¨ï¼Œæ‚¨å¯èƒ½ä¼šé—®çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬åº”è¯¥ä¸ºå­¦ç”Ÿé€‰æ‹©å“ªä¸ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Ÿä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åº”è¯¥ä¸ºå­¦ç”Ÿé€‰æ‹©ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼Œä»¥å‡å°‘å»¶è¿Ÿå’Œå†…å­˜å ç”¨ã€‚ä»æ–‡çŒ®ä¸­å¾—å‡ºçš„ä¸€ä¸ªå¾ˆå¥½çš„ç»éªŒæ³•åˆ™æ˜¯ï¼Œå½“è€å¸ˆå’Œå­¦ç”Ÿæ˜¯ç›¸åŒçš„*æ¨¡å‹ç±»å‹*æ—¶ï¼ŒçŸ¥è¯†è’¸é¦æ•ˆæœæœ€å¥½ã€‚â¹è¿™æ ·åšçš„ä¸€ä¸ªå¯èƒ½åŸå› æ˜¯ï¼Œä¸åŒçš„æ¨¡å‹ç±»å‹ï¼Œæ¯”å¦‚ BERT å’Œ RoBERTaï¼Œå¯èƒ½å…·æœ‰ä¸åŒçš„è¾“å‡ºåµŒå…¥ç©ºé—´ï¼Œè¿™ä¼šå¦¨ç¢å­¦ç”Ÿæ¨¡ä»¿è€å¸ˆçš„èƒ½åŠ›ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œè€å¸ˆæ˜¯ BERTï¼Œå› æ­¤ DistilBERT æ˜¯ä¸€ä¸ªè‡ªç„¶çš„å€™é€‰ï¼Œå› ä¸ºå®ƒçš„å‚æ•°å°‘äº† 40%ï¼Œå¹¶ä¸”å·²ç»åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½çš„ç»“æœã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¯¹æˆ‘ä»¬çš„æŸ¥è¯¢è¿›è¡Œæ ‡è®°åŒ–å’Œç¼–ç ï¼Œå› æ­¤è®©æˆ‘ä»¬å®ä¾‹åŒ–æ¥è‡ª DistilBERT çš„æ ‡è®°å™¨ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªç®€å•çš„`tokenize_text()`å‡½æ•°æ¥å¤„ç†é¢„å¤„ç†ï¼š

```py
from transformers import AutoTokenizer

student_ckpt = "distilbert-base-uncased"
student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)

def tokenize_text(batch):
    return student_tokenizer(batch["text"], truncation=True)

clinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=["text"])
clinc_enc = clinc_enc.rename_column("intent", "labels")
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»åˆ é™¤äº†`text`åˆ—ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†éœ€è¦å®ƒï¼Œæˆ‘ä»¬è¿˜å°†`intent`åˆ—é‡å‘½åä¸º`labels`ï¼Œä»¥ä¾¿è®­ç»ƒå™¨å¯ä»¥è‡ªåŠ¨æ£€æµ‹åˆ°å®ƒã€‚Â¹â°

ç°åœ¨æˆ‘ä»¬å·²ç»å¤„ç†äº†æˆ‘ä»¬çš„æ–‡æœ¬ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ä¸ºæˆ‘ä»¬çš„`DistillationTrainer`å®šä¹‰è¶…å‚æ•°å’Œ`compute_metrics()`å‡½æ•°ã€‚æˆ‘ä»¬è¿˜å°†æŠŠæ‰€æœ‰çš„æ¨¡å‹æ¨é€åˆ° Hugging Face Hubï¼Œæ‰€ä»¥è®©æˆ‘ä»¬é¦–å…ˆç™»å½•åˆ°æˆ‘ä»¬çš„è´¦æˆ·ï¼š

```py
from huggingface_hub import notebook_login

notebook_login()
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®šä¹‰è®­ç»ƒè¿‡ç¨‹ä¸­è¦è·Ÿè¸ªçš„æŒ‡æ ‡ã€‚å°±åƒæˆ‘ä»¬åœ¨æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸­æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‡†ç¡®æ€§ä½œä¸ºä¸»è¦æŒ‡æ ‡ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥åœ¨`compute_metrics()`å‡½æ•°ä¸­é‡ç”¨æˆ‘ä»¬çš„`accuracy_score()`å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°å°†åŒ…å«åœ¨`DistillationTrainer`ä¸­ï¼š

```py
def compute_metrics(pred):
    predictions, labels = pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy_score.compute(predictions=predictions, references=labels)
```

åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œåºåˆ—å»ºæ¨¡å¤´éƒ¨çš„é¢„æµ‹ä»¥ logits çš„å½¢å¼å‡ºç°ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨`np.argmax()`å‡½æ•°æ‰¾åˆ°æœ€æœ‰ä¿¡å¿ƒçš„ç±»åˆ«é¢„æµ‹ï¼Œå¹¶å°†å…¶ä¸åœ°é¢çœŸç›¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦å®šä¹‰è®­ç»ƒå‚æ•°ã€‚ä¸ºäº†çƒ­èº«ï¼Œæˆ‘ä»¬å°†è®¾ç½®<math alttext="alpha equals 1"><mrow><mi>Î±</mi> <mo>=</mo> <mn>1</mn></mrow></math>ï¼Œä»¥æŸ¥çœ‹ DistilBERT åœ¨æ²¡æœ‰æ¥è‡ªæ•™å¸ˆçš„ä»»ä½•ä¿¡å·çš„æƒ…å†µä¸‹çš„è¡¨ç°ã€‚Â¹Â¹ç„¶åæˆ‘ä»¬å°†æˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹æ¨é€åˆ°ä¸€ä¸ªåä¸º`distilbert-base-uncased-finetuned-clinc`çš„æ–°å­˜å‚¨åº“ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦åœ¨`DistillationTrainingArguments`çš„`output_dir`å‚æ•°ä¸­æŒ‡å®šå®ƒï¼š

```py
batch_size = 48

finetuned_ckpt = "distilbert-base-uncased-finetuned-clinc"
student_training_args = DistillationTrainingArguments(
    output_dir=finetuned_ckpt, evaluation_strategy = "epoch",
    num_train_epochs=5, learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size, alpha=1, weight_decay=0.01,
    push_to_hub=True)
```

æˆ‘ä»¬è¿˜è°ƒæ•´äº†ä¸€äº›é»˜è®¤è¶…å‚æ•°å€¼ï¼Œæ¯”å¦‚ epochs çš„æ•°é‡ï¼Œæƒé‡è¡°å‡å’Œå­¦ä¹ ç‡ã€‚æ¥ä¸‹æ¥è¦åšçš„æ˜¯åˆå§‹åŒ–ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ã€‚ç”±äºæˆ‘ä»¬å°†ä½¿ç”¨è®­ç»ƒå™¨è¿›è¡Œå¤šæ¬¡è¿è¡Œï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª`student_init()`å‡½æ•°ï¼Œä»¥ä¾¿åœ¨æ¯æ¬¡è°ƒç”¨`train()`æ–¹æ³•æ—¶åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡å‹ã€‚å½“æˆ‘ä»¬å°†è¿™ä¸ªå‡½æ•°ä¼ é€’ç»™`DistillationTrainer`æ—¶ï¼Œè¿™å°†ç¡®ä¿æˆ‘ä»¬æ¯æ¬¡è°ƒç”¨`train()`æ–¹æ³•æ—¶åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡å‹ã€‚

æˆ‘ä»¬è¿˜éœ€è¦åšçš„å¦ä¸€ä»¶äº‹æ˜¯ä¸ºå­¦ç”Ÿæ¨¡å‹æä¾›æ¯ä¸ªæ„å›¾å’Œæ ‡ç­¾ ID ä¹‹é—´çš„æ˜ å°„ã€‚è¿™äº›æ˜ å°„å¯ä»¥ä»æˆ‘ä»¬åœ¨æµæ°´çº¿ä¸­ä¸‹è½½çš„ BERT-base æ¨¡å‹ä¸­è·å¾—ï¼š

```py
id2label = pipe.model.config.id2label
label2id = pipe.model.config.label2id
```

æœ‰äº†è¿™äº›æ˜ å°„ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨`AutoConfig`ç±»åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹é…ç½®ï¼Œè¿™æ˜¯æˆ‘ä»¬åœ¨ç¬¬ä¸‰ç« å’Œç¬¬å››ç« ä¸­é‡åˆ°çš„ã€‚è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªä¸ºæˆ‘ä»¬çš„å­¦ç”Ÿåˆ›å»ºä¸€ä¸ªåŒ…å«æ ‡ç­¾æ˜ å°„ä¿¡æ¯çš„é…ç½®ï¼š

```py
from transformers import AutoConfig

num_labels = intents.num_classes
student_config = (AutoConfig
                  .from_pretrained(student_ckpt, num_labels=num_labels,
                                   id2label=id2label, label2id=label2id))
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¿˜æŒ‡å®šäº†æˆ‘ä»¬çš„æ¨¡å‹åº”è¯¥æœŸæœ›çš„ç±»çš„æ•°é‡ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªé…ç½®æä¾›ç»™`AutoModelForSequenceClassification`ç±»çš„`from_pretrained()`å‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
import torch
from transformers import AutoModelForSequenceClassification

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def student_init():
    return (AutoModelForSequenceClassification
            .from_pretrained(student_ckpt, config=student_config).to(device))
```

ç°åœ¨æˆ‘ä»¬å·²ç»æ‹¥æœ‰äº†æˆ‘ä»¬çš„è’¸é¦è®­ç»ƒå™¨æ‰€éœ€çš„æ‰€æœ‰è¦ç´ ï¼Œè®©æˆ‘ä»¬åŠ è½½æ•™å¸ˆå¹¶è¿›è¡Œå¾®è°ƒï¼š

```py
teacher_ckpt = "transformersbook/bert-base-uncased-finetuned-clinc"
teacher_model = (AutoModelForSequenceClassification
                 .from_pretrained(teacher_ckpt, num_labels=num_labels)
                 .to(device))
```

```py
distilbert_trainer = DistillationTrainer(model_init=student_init,
    teacher_model=teacher_model, args=student_training_args,
    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],
    compute_metrics=compute_metrics, tokenizer=student_tokenizer)

distilbert_trainer.train()
```

| Epoch | Training Loss | Validation Loss | Accuracy |
| --- | --- | --- | --- |
| --- | --- | --- | --- |
| 1 | 4.2923 | 3.289337 | 0.742258 |
| 2 | 2.6307 | 1.883680 | 0.828065 |
| 3 | 1.5483 | 1.158315 | 0.896774 |
| 4 | 1.0153 | 0.861815 | 0.909355 |
| 5 | 0.7958 | 0.777289 | 0.917419 |

éªŒè¯é›†ä¸Šçš„ 92%å‡†ç¡®ç‡çœ‹èµ·æ¥ç›¸å½“ä¸é”™ï¼Œä¸ BERT-base æ•™å¸ˆå®ç°çš„ 94%ç›¸æ¯”ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»å¯¹ DistilBERT è¿›è¡Œäº†å¾®è°ƒï¼Œè®©æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° Hubï¼Œä»¥ä¾¿ä»¥åé‡ç”¨ï¼š

```py
distilbert_trainer.push_to_hub("Training completed!")
```

ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å·²ç»å®‰å…¨åœ°å­˜å‚¨åœ¨ Hub ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ç«‹å³åœ¨æ€§èƒ½åŸºå‡†æµ‹è¯•çš„æµæ°´çº¿ä¸­ä½¿ç”¨å®ƒï¼š

```py
finetuned_ckpt = "transformersbook/distilbert-base-uncased-finetuned-clinc"
pipe = pipeline("text-classification", model=finetuned_ckpt)
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæµæ°´çº¿ä¼ é€’ç»™æˆ‘ä»¬çš„`PerformanceBenchmark`ç±»ï¼Œä»¥è®¡ç®—ä¸è¿™ä¸ªæ¨¡å‹ç›¸å…³çš„æŒ‡æ ‡ï¼š

```py
optim_type = "DistilBERT"
pb = PerformanceBenchmark(pipe, clinc["test"], optim_type=optim_type)
perf_metrics.update(pb.run_benchmark())
```

```py
Model size (MB) - 255.89
Average latency (ms) - 27.53 +\- 0.60
Accuracy on test set - 0.858
```

ä¸ºäº†å°†è¿™äº›ç»“æœä¸æˆ‘ä»¬çš„åŸºå‡†è¿›è¡Œæ¯”è¾ƒï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ•£ç‚¹å›¾ï¼Œæ˜¾ç¤ºå‡†ç¡®æ€§ä¸å»¶è¿Ÿä¹‹é—´çš„å…³ç³»ï¼Œæ¯ä¸ªç‚¹çš„åŠå¾„å¯¹åº”äºç£ç›˜ä¸Šæ¨¡å‹çš„å¤§å°ã€‚ä»¥ä¸‹å‡½æ•°å¯ä»¥æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚ï¼Œå¹¶å°†å½“å‰ä¼˜åŒ–ç±»å‹æ ‡è®°ä¸ºè™šçº¿åœ†åœˆï¼Œä»¥ä¾¿ä¸ä»¥å‰çš„ç»“æœè¿›è¡Œæ¯”è¾ƒï¼š

```py
import pandas as pd

def plot_metrics(perf_metrics, current_optim_type):
    df = pd.DataFrame.from_dict(perf_metrics, orient='index')

    for idx in df.index:
        df_opt = df.loc[idx]
        # Add a dashed circle around the current optimization type
        if idx == current_optim_type:
            plt.scatter(df_opt["time_avg_ms"], df_opt["accuracy"] * 100,
                        alpha=0.5, s=df_opt["size_mb"], label=idx,
                        marker='$\u25CC$')
        else:
            plt.scatter(df_opt["time_avg_ms"], df_opt["accuracy"] * 100,
                        s=df_opt["size_mb"], label=idx, alpha=0.5)

    legend = plt.legend(bbox_to_anchor=(1,1))
    for handle in legend.legendHandles:
        handle.set_sizes([20])

    plt.ylim(80,90)
    # Use the slowest model to define the x-axis range
    xlim = int(perf_metrics["BERT baseline"]["time_avg_ms"] + 3)
    plt.xlim(1, xlim)
    plt.ylabel("Accuracy (%)")
    plt.xlabel("Average latency (ms)")
    plt.show()

plot_metrics(perf_metrics, optim_type)
```

![](img/nlpt_08in01.png)

ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡ä½¿ç”¨ä¸€ä¸ªæ›´å°çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æˆåŠŸåœ°æ˜¾è‘—é™ä½äº†å¹³å‡å»¶è¿Ÿã€‚è€Œè¿™ä¸€åˆ‡åªéœ€ç‰ºç‰²äº†ç•¥å¾®è¶…è¿‡ 1%çš„å‡†ç¡®æ€§ï¼è®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦å¯ä»¥é€šè¿‡åŒ…æ‹¬æ•™å¸ˆçš„è’¸é¦æŸå¤±å¹¶æ‰¾åˆ°<math alttext="alpha"><mi>Î±</mi></math>å’Œ*T*çš„è‰¯å¥½å€¼æ¥ç¼©å°æœ€åçš„å·®è·ã€‚

## ä½¿ç”¨ Optuna æ‰¾åˆ°è‰¯å¥½çš„è¶…å‚æ•°

ä¸ºäº†æ‰¾åˆ°<math alttext="alpha"><mi>Î±</mi></math>å’Œ*T*çš„è‰¯å¥½å€¼ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ 2D å‚æ•°ç©ºé—´ä¸Šè¿›è¡Œç½‘æ ¼æœç´¢ã€‚ä½†ä¸€ä¸ªæ›´å¥½çš„é€‰æ‹©æ˜¯ä½¿ç”¨*Optuna*ï¼ŒÂ¹Â²è¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¿™ç§ä»»åŠ¡è®¾è®¡çš„ä¼˜åŒ–æ¡†æ¶ã€‚Optuna é€šè¿‡å¤šæ¬¡*trials*ä¼˜åŒ–ç›®æ ‡å‡½æ•°æ¥åˆ¶å®šæœç´¢é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬å¸Œæœ›æœ€å°åŒ– Rosenbrock çš„[â€œé¦™è•‰å‡½æ•°â€](https://oreil.ly/hPk8h)ï¼š

<math alttext="f left-parenthesis x comma y right-parenthesis equals left-parenthesis 1 minus x right-parenthesis squared plus 100 left-parenthesis y minus x squared right-parenthesis squared" display="block"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>x</mi><mo>)</mo></mrow> <mn>2</mn></msup> <mo>+</mo> <mn>100</mn> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><msup><mi>x</mi> <mn>2</mn></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>

è¿™æ˜¯ä¸€ä¸ªè‘—åçš„ä¼˜åŒ–æ¡†æ¶çš„æµ‹è¯•æ¡ˆä¾‹ã€‚å¦‚å›¾ 8-5 æ‰€ç¤ºï¼Œè¯¥å‡½æ•°å› å…¶æ›²çº¿è½®å»“è€Œå¾—åï¼Œå¹¶ä¸”åœ¨<math alttext="left-parenthesis x comma y right-parenthesis equals left-parenthesis 1 comma 1 right-parenthesis"><mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo> <mo>=</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>å¤„æœ‰ä¸€ä¸ªå…¨å±€æœ€å°å€¼ã€‚æ‰¾åˆ°è¿™ä¸ªè°·æ˜¯ä¸€ä¸ªç®€å•çš„ä¼˜åŒ–é—®é¢˜ï¼Œä½†æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼å´ä¸æ˜¯ã€‚

![é¦™è•‰å›¾](img/nlpt_0805.png)

###### å›¾ 8-5ã€‚ä¸¤ä¸ªå˜é‡çš„ Rosenbrock å‡½æ•°çš„ç»˜å›¾

åœ¨ Optuna ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å®šä¹‰ä¸€ä¸ª`objective()`å‡½æ•°æ¥æ‰¾åˆ°<math alttext="f left-parenthesis x comma y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math>çš„æœ€å°å€¼ï¼Œè¯¥å‡½æ•°è¿”å›<math alttext="f left-parenthesis x comma y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math>çš„å€¼ï¼š

```py
def objective(trial):
    x = trial.suggest_float("x", -2, 2)
    y = trial.suggest_float("y", -2, 2)
    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2
```

`trial.suggest_float`å¯¹è±¡æŒ‡å®šè¦å‡åŒ€é‡‡æ ·çš„å‚æ•°èŒƒå›´ï¼›Optuna è¿˜æä¾›`suggest_int`å’Œ`suggest_categorical`ç”¨äºæ•´æ•°å’Œåˆ†ç±»å‚æ•°ã€‚Optuna å°†å¤šä¸ªè¯•éªŒæ”¶é›†ä¸ºä¸€ä¸ª*study*ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€å°†`objective()`å‡½æ•°ä¼ é€’ç»™`study.optimize()`æ¥åˆ›å»ºä¸€ä¸ªå¦‚ä¸‹ï¼š

```py
import optuna

study = optuna.create_study()
study.optimize(objective, n_trials=1000)
```

ä¸€æ—¦ç ”ç©¶å®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼æ‰¾åˆ°æœ€ä½³å‚æ•°ï¼š

```py
study.best_params
```

```py
{'x': 1.003024865971437, 'y': 1.00315167589307}
```

é€šè¿‡ä¸€åƒæ¬¡è¯•éªŒï¼ŒOptuna å·²ç»æˆåŠŸæ‰¾åˆ°äº†* x *å’Œ* y *çš„å€¼ï¼Œè¿™äº›å€¼ä¸å…¨å±€æœ€å°å€¼ç›¸å½“æ¥è¿‘ã€‚è¦åœ¨![nlpt_pin01](img/nlpt_pin01.png) Transformers ä¸­ä½¿ç”¨ Optunaï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰è¦ä¼˜åŒ–çš„è¶…å‚æ•°ç©ºé—´ã€‚é™¤äº†<math alttext="alpha"> <mi>Î±</mi> </math>å’Œ*T*ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å°†åŒ…æ‹¬è®­ç»ƒå‘¨æœŸçš„æ•°é‡å¦‚ä¸‹ï¼š

```py
def hp_space(trial):
    return {"num_train_epochs": trial.suggest_int("num_train_epochs", 5, 10),
        "alpha": trial.suggest_float("alpha", 0, 1),
        "temperature": trial.suggest_int("temperature", 2, 20)}
```

ä½¿ç”¨`Trainer`è¿›è¡Œè¶…å‚æ•°æœç´¢éå¸¸ç®€å•ï¼›æˆ‘ä»¬åªéœ€è¦æŒ‡å®šè¦è¿è¡Œçš„è¯•éªŒæ¬¡æ•°å’Œè¦ä¼˜åŒ–çš„æ–¹å‘ã€‚å› ä¸ºæˆ‘ä»¬å¸Œæœ›è·å¾—æœ€ä½³å‡†ç¡®åº¦ï¼Œæ‰€ä»¥åœ¨è®­ç»ƒå™¨çš„`hyperâ€‹paraâ meter_â€‹search()`æ–¹æ³•ä¸­æŒ‡å®š`direction="maximize"`ï¼Œå¹¶æŒ‰å¦‚ä¸‹æ–¹å¼ä¼ é€’è¶…å‚æ•°æœç´¢ç©ºé—´ï¼š

```py
best_run = distilbert_trainer.hyperparameter_search(
    n_trials=20, direction="maximize", hp_space=hp_space)
```

`hyperparameter_search()`æ–¹æ³•è¿”å›ä¸€ä¸ª`BestRun`å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«äº†è¢«æœ€å¤§åŒ–çš„ç›®æ ‡å€¼ï¼ˆé»˜è®¤ä¸ºæ‰€æœ‰æŒ‡æ ‡çš„æ€»å’Œï¼‰å’Œè¯¥è¿è¡Œæ‰€ä½¿ç”¨çš„è¶…å‚æ•°ï¼š

```py
print(best_run)
```

```py
BestRun(run_id='1', objective=0.927741935483871,
hyperparameters={'num_train_epochs': 10, 'alpha': 0.12468168730193585,
'temperature': 7})
```

è¿™ä¸ª<math alttext="alpha"><mi>Î±</mi></math>çš„å€¼å‘Šè¯‰æˆ‘ä»¬ï¼Œå¤§éƒ¨åˆ†çš„è®­ç»ƒä¿¡å·æ¥è‡ªçŸ¥è¯†è’¸é¦é¡¹ã€‚è®©æˆ‘ä»¬ä½¿ç”¨è¿™äº›å€¼æ›´æ–°æˆ‘ä»¬çš„è®­ç»ƒå‚æ•°ï¼Œå¹¶è¿è¡Œæœ€ç»ˆçš„è®­ç»ƒï¼š

```py
for k,v in best_run.hyperparameters.items():
    setattr(student_training_args, k, v)

# Define a new repository to store our distilled model
distilled_ckpt = "distilbert-base-uncased-distilled-clinc"
student_training_args.output_dir = distilled_ckpt

# Create a new Trainer with optimal parameters
distil_trainer = DistillationTrainer(model_init=student_init,
    teacher_model=teacher_model, args=student_training_args,
    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],
    compute_metrics=compute_metrics, tokenizer=student_tokenizer)

distil_trainer.train();
```

| Epoch | Training Loss | Validation Loss | Accuracy |
| --- | --- | --- | --- |
| --- | --- | --- | --- |
| 1 | 0.9031 | 0.574540 | 0.736452 |
| 2 | 0.4481 | 0.285621 | 0.874839 |
| 3 | 0.2528 | 0.179766 | 0.918710 |
| 4 | 0.1760 | 0.139828 | 0.929355 |
| 5 | 0.1416 | 0.121053 | 0.934839 |
| 6 | 0.1243 | 0.111640 | 0.934839 |
| 7 | 0.1133 | 0.106174 | 0.937742 |
| 8 | 0.1075 | 0.103526 | 0.938710 |
| 9 | 0.1039 | 0.101432 | 0.938065 |
| 10 | 0.1018 | 0.100493 | 0.939355 |

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡å‚æ•°æ•°é‡å‡ ä¹å‡å°‘äº†ä¸€åŠï¼Œæˆ‘ä»¬å·²ç»æˆåŠŸè®­ç»ƒå‡ºå­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹çš„å‡†ç¡®åº¦ç›¸åŒ¹é…ï¼è®©æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° Hub ä»¥ä¾›å°†æ¥ä½¿ç”¨ï¼š

```py
distil_trainer.push_to_hub("Training complete")
```

## åŸºå‡†æµ‹è¯•æˆ‘ä»¬çš„ç²¾ç‚¼æ¨¡å‹

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå‡†ç¡®çš„å­¦ç”Ÿï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæµæ°´çº¿ï¼Œå¹¶é‡æ–°è¿›è¡Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ï¼Œçœ‹çœ‹æˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å¦‚ä½•ï¼š

```py
distilled_ckpt = "transformersbook/distilbert-base-uncased-distilled-clinc"
pipe = pipeline("text-classification", model=distilled_ckpt)
optim_type = "Distillation"
pb = PerformanceBenchmark(pipe, clinc["test"], optim_type=optim_type)
perf_metrics.update(pb.run_benchmark())
```

```py
Model size (MB) - 255.89
Average latency (ms) - 25.96 +\- 1.63
Accuracy on test set - 0.868
```

ä¸ºäº†å°†è¿™äº›ç»“æœæ”¾åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè®©æˆ‘ä»¬è¿˜ç”¨æˆ‘ä»¬çš„`plot_metrics()`å‡½æ•°å°†å®ƒä»¬å¯è§†åŒ–ï¼š

```py
plot_metrics(perf_metrics, optim_type)
```

![](img/nlpt_08in02.png)

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œä¸ DistilBERT åŸºå‡†ç›¸æ¯”ï¼Œæ¨¡å‹å¤§å°å’Œå»¶è¿ŸåŸºæœ¬ä¿æŒä¸å˜ï¼Œä½†å‡†ç¡®æ€§å¾—åˆ°äº†æ”¹å–„ï¼Œç”šè‡³è¶…è¿‡äº†æ•™å¸ˆçš„è¡¨ç°ï¼è§£é‡Šè¿™ä¸€ä»¤äººæƒŠè®¶çš„ç»“æœçš„ä¸€ç§æ–¹å¼æ˜¯ï¼Œæ•™å¸ˆå¾ˆå¯èƒ½æ²¡æœ‰åƒå­¦ç”Ÿé‚£æ ·ç³»ç»Ÿåœ°è¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚è¿™å¾ˆå¥½ï¼Œä½†æˆ‘ä»¬å®é™…ä¸Šå¯ä»¥ä½¿ç”¨ä¸€ç§ç§°ä¸ºé‡åŒ–çš„æŠ€æœ¯è¿›ä¸€æ­¥å‹ç¼©æˆ‘ä»¬çš„ç²¾ç‚¼æ¨¡å‹ã€‚è¿™æ˜¯ä¸‹ä¸€èŠ‚çš„ä¸»é¢˜ã€‚

# ä½¿ç”¨é‡åŒ–ä½¿æ¨¡å‹æ›´å¿«

æˆ‘ä»¬ç°åœ¨å·²ç»çœ‹åˆ°ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å°†ä¿¡æ¯ä»æ•™å¸ˆä¼ é€’åˆ°æ›´å°çš„å­¦ç”Ÿæ¥å‡å°‘è¿è¡Œæ¨æ–­çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚é‡åŒ–é‡‡ç”¨äº†ä¸åŒçš„æ–¹æ³•ï¼›å®ƒä¸æ˜¯å‡å°‘è®¡ç®—çš„æ•°é‡ï¼Œè€Œæ˜¯é€šè¿‡ä½¿ç”¨ä½ç²¾åº¦æ•°æ®ç±»å‹ï¼ˆå¦‚ 8 ä½æ•´æ•°ï¼ˆINT8ï¼‰ï¼‰ä»£æ›¿é€šå¸¸çš„ 32 ä½æµ®ç‚¹æ•°ï¼ˆFP32ï¼‰æ¥ä½¿å®ƒä»¬æ›´åŠ é«˜æ•ˆã€‚å‡å°‘ä½æ•°æ„å‘³ç€ç»“æœæ¨¡å‹éœ€è¦æ›´å°‘çš„å†…å­˜å­˜å‚¨ï¼Œå¹¶ä¸”åƒçŸ©é˜µä¹˜æ³•è¿™æ ·çš„æ“ä½œå¯ä»¥é€šè¿‡æ•´æ•°è¿ç®—æ›´å¿«åœ°æ‰§è¡Œã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æ€§èƒ½å¢ç›Šå¯ä»¥åœ¨å‡ ä¹æ²¡æœ‰å‡†ç¡®æ€§æŸå¤±çš„æƒ…å†µä¸‹å®ç°ï¼

é‡åŒ–èƒŒåçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å°†å¼ é‡ä¸­çš„æµ®ç‚¹å€¼*f*çš„èŒƒå›´[ <math alttext="f Subscript normal m normal a normal x Baseline comma f Subscript normal m normal i normal n Baseline"><mrow><msub><mi>f</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>f</mi> <mi>min</mi></msub></mrow></math> ]æ˜ å°„åˆ°ä¸€ä¸ªè¾ƒå°çš„èŒƒå›´[ <math alttext="q Subscript normal m normal a normal x Baseline comma q Subscript normal m normal i normal n Baseline"><mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>q</mi> <mi>min</mi></msub></mrow></math> ]ä¸­çš„å›ºå®šç‚¹æ•°<math alttext="q"><mi>q</mi></math>ï¼Œå¹¶çº¿æ€§åˆ†å¸ƒæ‰€æœ‰å€¼ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œè¿™ç§æ˜ å°„ç”±ä»¥ä¸‹æ–¹ç¨‹æè¿°ï¼š

<math alttext="f equals left-parenthesis StartFraction f Subscript normal m normal a normal x Baseline minus f Subscript normal m normal i normal n Baseline Over q Subscript normal m normal a normal x Baseline minus q Subscript normal m normal i normal n Baseline EndFraction right-parenthesis left-parenthesis q minus upper Z right-parenthesis equals upper S left-parenthesis q minus upper Z right-parenthesis" display="block"><mrow><mi>f</mi> <mo>=</mo> <mfenced open="(" close=")"><mfrac><mrow><msub><mi>f</mi> <mi>max</mi></msub> <mo>-</mo><msub><mi>f</mi> <mi>min</mi></msub></mrow> <mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>-</mo><msub><mi>q</mi> <mi>min</mi></msub></mrow></mfrac></mfenced> <mrow><mo>(</mo> <mi>q</mi> <mo>-</mo> <mi>Z</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>S</mi> <mrow><mo>(</mo> <mi>q</mi> <mo>-</mo> <mi>Z</mi> <mo>)</mo></mrow></mrow></math>

ç¼©æ”¾å› å­<math alttext="upper S"><mi>S</mi></math>æ˜¯ä¸€ä¸ªæ­£çš„æµ®ç‚¹æ•°ï¼Œå¸¸æ•°<math alttext="upper Z"><mi>Z</mi></math>ä¸<math alttext="q"><mi>q</mi></math>å…·æœ‰ç›¸åŒçš„ç±»å‹ï¼Œè¢«ç§°ä¸º*é›¶ç‚¹*ï¼Œå› ä¸ºå®ƒå¯¹åº”äºæµ®ç‚¹å€¼<math alttext="f equals 0"><mrow><mi>f</mi> <mo>=</mo> <mn>0</mn></mrow></math>çš„é‡åŒ–å€¼ã€‚è¯·æ³¨æ„ï¼Œæ˜ å°„éœ€è¦æ˜¯*ä»¿å°„*çš„ï¼Œè¿™æ ·å½“æˆ‘ä»¬å°†å®šç‚¹æ•°åé‡åŒ–ä¸ºæµ®ç‚¹æ•°æ—¶ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°æµ®ç‚¹æ•°ã€‚Â¹Â³ è½¬æ¢çš„ç¤ºä¾‹æ˜¾ç¤ºåœ¨å›¾ 8-6 ä¸­ã€‚

![å°†æµ®ç‚¹æ•°æ˜ å°„ä¸º 8 ä½æ•´æ•°](img/nlpt_0806.png)

###### å›¾ 8-6ã€‚å°†æµ®ç‚¹æ•°é‡åŒ–ä¸ºæ— ç¬¦å· 8 ä½æ•´æ•°ï¼ˆç”± Manas Sahni æä¾›ï¼‰

ç°åœ¨ï¼ŒTransformerï¼ˆä»¥åŠæ·±åº¦ç¥ç»ç½‘ç»œæ›´æ™®éåœ°ï¼‰æˆä¸ºé‡åŒ–çš„ä¸»è¦å€™é€‰å¯¹è±¡çš„ä¸€ä¸ªä¸»è¦åŸå› æ˜¯æƒé‡å’Œæ¿€æ´»å€¾å‘äºåœ¨ç›¸å¯¹è¾ƒå°çš„èŒƒå›´å†…å–å€¼ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¸å¿…å°†æ‰€æœ‰å¯èƒ½çš„ FP32 æ•°å­—èŒƒå›´å‹ç¼©åˆ° INT8 è¡¨ç¤ºçš„ 256 ä¸ªæ•°å­—ä¸­ã€‚ä¸ºäº†çœ‹åˆ°è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬ä»æˆ‘ä»¬ç²¾ç®€æ¨¡å‹ä¸­æŒ‘é€‰å‡ºä¸€ä¸ªæ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼Œå¹¶ç»˜åˆ¶å€¼çš„é¢‘ç‡åˆ†å¸ƒï¼š

```py
import matplotlib.pyplot as plt

state_dict = pipe.model.state_dict()
weights = state_dict["distilbert.transformer.layer.0.attention.out_lin.weight"]
plt.hist(weights.flatten().numpy(), bins=250, range=(-0.3,0.3), edgecolor="C0")
plt.show()
```

![](img/nlpt_08in03.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæƒé‡çš„å€¼åˆ†å¸ƒåœ¨æ¥è¿‘é›¶çš„å°èŒƒå›´å†…[<math alttext="è´Ÿ 0.1ï¼Œ0.1"><mrow><mo>-</mo><mn>0</mn><mo>.</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>.</mo><mn>1</mn></mrow></math>]ã€‚ç°åœ¨ï¼Œå‡è®¾æˆ‘ä»¬æƒ³è¦å°†è¿™ä¸ªå¼ é‡é‡åŒ–ä¸ºå¸¦ç¬¦å·çš„ 8 ä½æ•´æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ•´æ•°çš„å¯èƒ½å€¼èŒƒå›´æ˜¯[<math alttext="q ä¸‹æ ‡æœ€å¤§ï¼Œq ä¸‹æ ‡æœ€å°"><mrow><msub><mi>q</mi><mi>max</mi></msub><mo>,</mo><msub><mi>q</mi><mi>min</mi></msub></mrow></math>] = [<math alttext="è´Ÿ 128ï¼Œ127"><mrow><mo>-</mo><mn>128</mn><mo>,</mo><mn>127</mn></mrow></math>]ã€‚é›¶ç‚¹ä¸ FP32 çš„é›¶ç‚¹é‡åˆï¼Œæ¯”ä¾‹å› å­æ ¹æ®å‰é¢çš„æ–¹ç¨‹è®¡ç®—ï¼š

```py
zero_point = 0
scale = (weights.max() - weights.min()) / (127 - (-128))
```

ä¸ºäº†è·å¾—é‡åŒ–å¼ é‡ï¼Œæˆ‘ä»¬åªéœ€è¦åè½¬æ˜ å°„<math alttext="q ç­‰äº f é™¤ä»¥ S åŠ  Z"><mrow><mi>q</mi><mo>=</mo><mi>f</mi><mo>/</mo><mi>S</mi><mo>+</mo><mi>Z</mi></mrow></math>ï¼Œå°†å€¼å¤¹ç´§ï¼Œå››èˆäº”å…¥åˆ°æœ€è¿‘çš„æ•´æ•°ï¼Œå¹¶ä½¿ç”¨`Tensor.char()`å‡½æ•°å°†ç»“æœè¡¨ç¤ºä¸º`torch.int8`æ•°æ®ç±»å‹ï¼š

```py
(weights / scale + zero_point).clamp(-128, 127).round().char()
```

```py
tensor([[ -5,  -8,   0,  ...,  -6,  -4,   8],
        [  8,   3,   1,  ...,  -4,   7,   0],
        [ -9,  -6,   5,  ...,   1,   5,  -3],
        ...,
        [  6,   0,  12,  ...,   0,   6,  -1],
        [  0,  -2, -12,  ...,  12,  -7, -13],
        [-13,  -1, -10,  ...,   8,   2,  -2]], dtype=torch.int8)
```

å¤ªå¥½äº†ï¼Œæˆ‘ä»¬åˆšåˆšé‡åŒ–äº†æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå¼ é‡ï¼åœ¨ PyTorch ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`quantize_per_tensor()`å‡½æ•°å’Œé‡åŒ–æ•°æ®ç±»å‹`torch.qint`æ¥ç®€åŒ–è½¬æ¢ï¼Œè¯¥æ•°æ®ç±»å‹é’ˆå¯¹æ•´æ•°ç®—æœ¯æ“ä½œè¿›è¡Œäº†ä¼˜åŒ–ï¼š

```py
from torch import quantize_per_tensor

dtype = torch.qint8
quantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)
quantized_weights.int_repr()
```

```py
tensor([[ -5,  -8,   0,  ...,  -6,  -4,   8],
        [  8,   3,   1,  ...,  -4,   7,   0],
        [ -9,  -6,   5,  ...,   1,   5,  -3],
        ...,
        [  6,   0,  12,  ...,   0,   6,  -1],
        [  0,  -2, -12,  ...,  12,  -7, -13],
        [-13,  -1, -10,  ...,   8,   2,  -2]], dtype=torch.int8)
```

å›¾ 8-7 ä¸­çš„å›¾è¡¨æ¸…æ¥šåœ°æ˜¾ç¤ºäº†åªæ˜ å°„ä¸€äº›æƒé‡å€¼å¹¶å¯¹å…¶ä½™å€¼è¿›è¡Œå››èˆäº”å…¥æ‰€å¼•èµ·çš„ç¦»æ•£åŒ–ã€‚

![é‡åŒ–å¯¹ Transformer æƒé‡çš„å½±å“](img/nlpt_0807.png)

###### å›¾ 8-7ã€‚é‡åŒ–å¯¹ Transformer æƒé‡çš„å½±å“

ä¸ºäº†å®Œæˆæˆ‘ä»¬çš„å°åˆ†æï¼Œè®©æˆ‘ä»¬æ¯”è¾ƒä½¿ç”¨ FP32 å’Œ INT8 å€¼è®¡ç®—ä¸¤ä¸ªæƒé‡å¼ é‡çš„ä¹˜æ³•éœ€è¦å¤šé•¿æ—¶é—´ã€‚å¯¹äº FP32 å¼ é‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ PyTorch çš„`@`è¿ç®—ç¬¦è¿›è¡Œç›¸ä¹˜ï¼š

```py
%%timeit
weights @ weights
```

```py
393 Âµs Â± 3.84 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
```

å¯¹äºé‡åŒ–å¼ é‡ï¼Œæˆ‘ä»¬éœ€è¦`QFunctional`åŒ…è£…ç±»ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç‰¹æ®Šçš„`torch.qint8`æ•°æ®ç±»å‹æ‰§è¡Œæ“ä½œï¼š

```py
from torch.nn.quantized import QFunctional

q_fn = QFunctional()
```

è¿™ä¸ªç±»æ”¯æŒå„ç§åŸºæœ¬æ“ä½œï¼Œæ¯”å¦‚åŠ æ³•ï¼Œåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¡ç®—é‡åŒ–å¼ é‡çš„ä¹˜æ³•æ—¶é—´ï¼š

```py
%%timeit
q_fn.mul(quantized_weights, quantized_weights)
```

```py
23.3 Âµs Â± 298 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
```

ä¸æˆ‘ä»¬çš„ FP32 è®¡ç®—ç›¸æ¯”ï¼Œä½¿ç”¨ INT8 å¼ é‡å‡ ä¹å¿« 100 å€ï¼é€šè¿‡ä½¿ç”¨ä¸“é—¨çš„åç«¯è¿è¡Œé‡åŒ–è¿ç®—ç¬¦ï¼Œè¿˜å¯ä»¥è·å¾—æ›´å¤§çš„æ”¶ç›Šã€‚æˆªè‡³æœ¬ä¹¦ç¼–å†™æ—¶ï¼ŒPyTorch æ”¯æŒï¼š

+   å…·æœ‰ AVX2 æ”¯æŒæˆ–æ›´é«˜ç‰ˆæœ¬çš„ x86 CPU

+   ARM CPUï¼ˆé€šå¸¸ç”¨äºç§»åŠ¨/åµŒå…¥å¼è®¾å¤‡ï¼‰

ç”±äº INT8 æ•°å­—çš„ä½æ•°æ¯” FP32 æ•°å­—å°‘å››å€ï¼Œé‡åŒ–è¿˜å°†å†…å­˜å­˜å‚¨éœ€æ±‚å‡å°‘äº†å¤šè¾¾å››å€ã€‚åœ¨æˆ‘ä»¬çš„ç®€å•ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨`Tensor.storage()`å‡½æ•°å’Œ Python çš„`sys`æ¨¡å—ä¸­çš„`getsizeof()`å‡½æ•°æ¥æ¯”è¾ƒæƒé‡å¼ é‡åŠå…¶é‡åŒ–ç‰ˆæœ¬çš„åº•å±‚å­˜å‚¨å¤§å°æ¥éªŒè¯è¿™ä¸€ç‚¹ï¼š

```py
import sys

sys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())
```

```py
3.999633833760527
```

å¯¹äºä¸€ä¸ªå¤§è§„æ¨¡çš„ Transformerï¼Œå®é™…çš„å‹ç¼©ç‡å–å†³äºå“ªäº›å±‚è¢«é‡åŒ–ï¼ˆæ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚çœ‹åˆ°çš„ï¼Œé€šå¸¸åªæœ‰çº¿æ€§å±‚è¢«é‡åŒ–ï¼‰ã€‚

é‚£ä¹ˆé‡åŒ–æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿæ”¹å˜æ¨¡å‹ä¸­æ‰€æœ‰è®¡ç®—çš„ç²¾åº¦ä¼šåœ¨æ¨¡å‹çš„è®¡ç®—å›¾ä¸­çš„æ¯ä¸ªç‚¹å¼•å…¥å°çš„æ‰°åŠ¨ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚é‡åŒ–æ¨¡å‹æœ‰å‡ ç§æ–¹æ³•ï¼Œå„æœ‰åˆ©å¼Šã€‚å¯¹äºæ·±åº¦ç¥ç»ç½‘ç»œï¼Œé€šå¸¸æœ‰ä¸‰ç§ä¸»è¦çš„é‡åŒ–æ–¹æ³•ï¼š

åŠ¨æ€é‡åŒ–

ä½¿ç”¨åŠ¨æ€é‡åŒ–æ—¶ï¼Œåœ¨è®­ç»ƒæœŸé—´ä¸ä¼šå‘ç”Ÿä»»ä½•å˜åŒ–ï¼Œè°ƒæ•´åªä¼šåœ¨æ¨æ–­æœŸé—´è¿›è¡Œã€‚ä¸æˆ‘ä»¬å°†è®¨è®ºçš„æ‰€æœ‰é‡åŒ–æ–¹æ³•ä¸€æ ·ï¼Œæ¨¡å‹çš„æƒé‡åœ¨æ¨æ–­æ—¶é—´ä¹‹å‰è¢«è½¬æ¢ä¸º INT8ã€‚é™¤äº†æƒé‡ï¼Œæ¨¡å‹çš„æ¿€æ´»ä¹Ÿè¢«é‡åŒ–ã€‚è¿™ç§æ–¹æ³•æ˜¯åŠ¨æ€çš„ï¼Œå› ä¸ºé‡åŒ–æ˜¯å³æ—¶å‘ç”Ÿçš„ã€‚è¿™æ„å‘³ç€æ‰€æœ‰çŸ©é˜µä¹˜æ³•éƒ½å¯ä»¥ä½¿ç”¨é«˜åº¦ä¼˜åŒ–çš„ INT8 å‡½æ•°è¿›è¡Œè®¡ç®—ã€‚åœ¨è¿™é‡Œè®¨è®ºçš„æ‰€æœ‰é‡åŒ–æ–¹æ³•ä¸­ï¼ŒåŠ¨æ€é‡åŒ–æ˜¯æœ€ç®€å•çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œä½¿ç”¨åŠ¨æ€é‡åŒ–æ—¶ï¼Œæ¿€æ´»ä»¥æµ®ç‚¹æ ¼å¼å†™å…¥å’Œè¯»å–åˆ°å†…å­˜ä¸­ã€‚æ•´æ•°å’Œæµ®ç‚¹ä¹‹é—´çš„è½¬æ¢å¯èƒ½æˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚

é™æ€é‡åŒ–

æˆ‘ä»¬å¯ä»¥é¿å…åœ¨æ¨æ–­æœŸé—´å°†æ¿€æ´»é‡åŒ–ä¸ºæµ®ç‚¹æ•°ï¼Œè€Œæ˜¯é¢„å…ˆè®¡ç®—é‡åŒ–æ–¹æ¡ˆã€‚é™æ€é‡åŒ–é€šè¿‡è§‚å¯Ÿæ•°æ®çš„ä»£è¡¨æ€§æ ·æœ¬ä¸Šçš„æ¿€æ´»æ¨¡å¼æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç†æƒ³çš„é‡åŒ–æ–¹æ¡ˆè¢«è®¡ç®—ç„¶åä¿å­˜ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè·³è¿‡ INT8 å’Œ FP32 å€¼ä¹‹é—´çš„è½¬æ¢ï¼Œå¹¶åŠ å¿«è®¡ç®—é€Ÿåº¦ã€‚ç„¶è€Œï¼Œè¿™éœ€è¦è®¿é—®ä¸€ä¸ªè‰¯å¥½çš„æ•°æ®æ ·æœ¬ï¼Œå¹¶ä¸”åœ¨ç®¡é“ä¸­å¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„æ­¥éª¤ï¼Œå› ä¸ºç°åœ¨æˆ‘ä»¬éœ€è¦åœ¨æ‰§è¡Œæ¨æ–­ä¹‹å‰è®­ç»ƒå’Œç¡®å®šé‡åŒ–æ–¹æ¡ˆã€‚é™æ€é‡åŒ–æ²¡æœ‰è§£å†³çš„ä¸€ä¸ªæ–¹é¢æ˜¯ï¼šè®­ç»ƒå’Œæ¨æ–­æœŸé—´ç²¾åº¦ä¹‹é—´çš„å·®å¼‚ï¼Œè¿™å¯¼è‡´æ¨¡å‹æŒ‡æ ‡ï¼ˆä¾‹å¦‚å‡†ç¡®æ€§ï¼‰ä¸‹é™ã€‚

é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ

é€šè¿‡â€œä¼ªâ€é‡åŒ– FP32 å€¼æ¥æœ‰æ•ˆæ¨¡æ‹Ÿè®­ç»ƒæœŸé—´çš„é‡åŒ–æ•ˆæœã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œä¸ä½¿ç”¨ INT8 å€¼ï¼Œè€Œæ˜¯å°† FP32 å€¼å››èˆäº”å…¥ä»¥æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœã€‚è¿™åœ¨å‰å‘å’Œåå‘ä¼ é€’è¿‡ç¨‹ä¸­éƒ½ä¼šè¿›è¡Œï¼Œå¯ä»¥æ”¹å–„æ¨¡å‹æŒ‡æ ‡çš„æ€§èƒ½ï¼Œè¶…è¿‡é™æ€å’ŒåŠ¨æ€é‡åŒ–ã€‚

ä½¿ç”¨ transformers è¿›è¡Œæ¨æ–­çš„ä¸»è¦ç“¶é¢ˆæ˜¯ä¸è¿™äº›æ¨¡å‹ä¸­åºå¤§æ•°é‡çš„æƒé‡ç›¸å…³çš„è®¡ç®—å’Œå†…å­˜å¸¦å®½ã€‚å› æ­¤ï¼ŒåŠ¨æ€é‡åŒ–ç›®å‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­åŸºäº transformer çš„æ¨¡å‹çš„æœ€ä½³æ–¹æ³•ã€‚åœ¨è¾ƒå°çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸­ï¼Œé™åˆ¶å› ç´ æ˜¯æ¿€æ´»çš„å†…å­˜å¸¦å®½ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆé€šå¸¸ä½¿ç”¨é™æ€é‡åŒ–ï¼ˆæˆ–è€…åœ¨æ€§èƒ½ä¸‹é™å¤ªæ˜¾è‘—çš„æƒ…å†µä¸‹ä½¿ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼‰çš„åŸå› ã€‚

åœ¨ PyTorch ä¸­å®ç°åŠ¨æ€é‡åŒ–éå¸¸ç®€å•ï¼Œå¯ä»¥ç”¨ä¸€è¡Œä»£ç å®Œæˆï¼š

```py
from torch.quantization import quantize_dynamic

model_ckpt = "transformersbook/distilbert-base-uncased-distilled-clinc"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = (AutoModelForSequenceClassification
         .from_pretrained(model_ckpt).to("cpu"))

model_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å®Œæ•´ç²¾åº¦æ¨¡å‹ä¼ é€’ç»™`quantize_dynamic()`ï¼Œå¹¶æŒ‡å®šæˆ‘ä»¬è¦é‡åŒ–çš„ PyTorch å±‚ç±»çš„é›†åˆã€‚`dtype`å‚æ•°æŒ‡å®šç›®æ ‡ç²¾åº¦ï¼Œå¯ä»¥æ˜¯`fp16`æˆ–`qint8`ã€‚ä¸€ä¸ªå¥½çš„åšæ³•æ˜¯é€‰æ‹©æ‚¨çš„è¯„ä¼°æŒ‡æ ‡æ‰€èƒ½å®¹å¿çš„æœ€ä½ç²¾åº¦ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ INT8ï¼Œå¾ˆå¿«å°±ä¼šçœ‹åˆ°å®ƒå¯¹æˆ‘ä»¬æ¨¡å‹çš„å‡†ç¡®æ€§å‡ ä¹æ²¡æœ‰å½±å“ã€‚

# å¯¹æˆ‘ä»¬çš„é‡åŒ–æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•

æˆ‘ä»¬çš„æ¨¡å‹ç°åœ¨å·²ç»é‡åŒ–ï¼Œè®©æˆ‘ä»¬é€šè¿‡åŸºå‡†æµ‹è¯•å¹¶å¯è§†åŒ–ç»“æœï¼š

```py
pipe = pipeline("text-classification", model=model_quantized,
                tokenizer=tokenizer)
optim_type = "Distillation + quantization"
pb = PerformanceBenchmark(pipe, clinc["test"], optim_type=optim_type)
perf_metrics.update(pb.run_benchmark())
```

```py
Model size (MB) - 132.40
Average latency (ms) - 12.54 +\- 0.73
Accuracy on test set - 0.876
```

```py
plot_metrics(perf_metrics, optim_type)
```

![](img/nlpt_08in04.png)

ä¸é”™ï¼Œé‡åŒ–æ¨¡å‹å‡ ä¹æ˜¯æˆ‘ä»¬ç²¾ç®€æ¨¡å‹å¤§å°çš„ä¸€åŠï¼Œç”šè‡³è¿˜ç•¥å¾®æé«˜äº†å‡†ç¡®æ€§ï¼è®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦å¯ä»¥é€šè¿‡ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ ONNX Runtime å°†æˆ‘ä»¬çš„ä¼˜åŒ–æ¨å‘æé™ã€‚

# ä½¿ç”¨ ONNX å’Œ ONNX Runtime ä¼˜åŒ–æ¨æ–­

[ONNX](https://onnx.ai)æ˜¯ä¸€ä¸ªå¼€æ”¾æ ‡å‡†ï¼Œå®šä¹‰äº†ä¸€ç»„é€šç”¨çš„æ“ä½œç¬¦å’Œä¸€ç§é€šç”¨çš„æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºåœ¨å„ç§æ¡†æ¶ä¸­è¡¨ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬ PyTorch å’Œ TensorFlowã€‚Â¹â´å½“æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼æ—¶ï¼Œè¿™äº›æ“ä½œç¬¦ç”¨äºæ„å»ºä¸€ä¸ªè®¡ç®—å›¾ï¼ˆé€šå¸¸ç§°ä¸º*ä¸­é—´è¡¨ç¤º*ï¼‰ï¼Œè¡¨ç¤ºæ•°æ®é€šè¿‡ç¥ç»ç½‘ç»œçš„æµåŠ¨ã€‚ä¾‹å¦‚ï¼ŒBERT-base çš„è¿™æ ·ä¸€ä¸ªå›¾ç¤ºä¾‹æ˜¾ç¤ºåœ¨å›¾ 8-8 ä¸­ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶ä¸€äº›è¾“å…¥ï¼Œåº”ç”¨æ“ä½œå¦‚`Add`æˆ–`Squeeze`ï¼Œç„¶åå°†è¾“å‡ºé¦ˆé€åˆ°ä¸‹ä¸€ç»„èŠ‚ç‚¹ã€‚

![ONNX å›¾ç¤ºä¾‹](img/nlpt_0808.png)

###### å›¾ 8-8\. BERT-base çš„ ONNX å›¾çš„ä¸€ä¸ªéƒ¨åˆ†ï¼Œåœ¨ Netron ä¸­å¯è§†åŒ–

é€šè¿‡å…¬å¼€å…·æœ‰æ ‡å‡†åŒ–æ“ä½œç¬¦å’Œæ•°æ®ç±»å‹çš„å›¾ï¼ŒONNX ä½¿å¾—åœ¨ä¸åŒæ¡†æ¶ä¹‹é—´åˆ‡æ¢å˜å¾—å®¹æ˜“ã€‚ä¾‹å¦‚ï¼Œåœ¨ PyTorch ä¸­è®­ç»ƒçš„æ¨¡å‹å¯ä»¥å¯¼å‡ºä¸º ONNX æ ¼å¼ï¼Œç„¶ååœ¨ TensorFlow ä¸­å¯¼å…¥ï¼ˆåä¹‹äº¦ç„¶ï¼‰ã€‚

å½“ ONNX ä¸ä¸“ç”¨åŠ é€Ÿå™¨å¦‚[ONNX Runtime](https://onnxruntime.ai)æˆ– ORT é…åˆä½¿ç”¨æ—¶ï¼Œå®ƒçš„ä¼˜åŠ¿å°±æ˜¾ç°å‡ºæ¥äº†ã€‚Â¹âµORT é€šè¿‡æ“ä½œç¬¦èåˆå’Œå¸¸é‡æŠ˜å ç­‰æŠ€æœ¯æä¾›äº†ä¼˜åŒ– ONNX å›¾çš„å·¥å…·ï¼ŒÂ¹â¶å¹¶å®šä¹‰äº†ä¸€ä¸ªæ¥å£ï¼Œå…è®¸æ‚¨åœ¨ä¸åŒç±»å‹çš„ç¡¬ä»¶ä¸Šè¿è¡Œæ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æŠ½è±¡ã€‚å›¾ 8-9 æ˜¾ç¤ºäº† ONNX å’Œ ORT ç”Ÿæ€ç³»ç»Ÿçš„é«˜çº§æ¶æ„ã€‚

![ONNX å’Œ ONNX Runtime ç”Ÿæ€ç³»ç»Ÿçš„æ¶æ„](img/nlpt_0809.png)

###### å›¾ 8-9\. ONNX å’Œ ONNX Runtime ç”Ÿæ€ç³»ç»Ÿçš„æ¶æ„ï¼ˆç”± ONNX Runtime å›¢é˜Ÿæä¾›ï¼‰

è¦çœ‹åˆ° ORT çš„è¿è¡Œæƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯å°†æˆ‘ä»¬çš„ç²¾ç‚¼æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼ã€‚![nlpt_pin01](img/nlpt_pin01.png) Transformers åº“æœ‰ä¸€ä¸ªå†…ç½®å‡½æ•°å«åš`conâ vert_graph_to_onnx.convert()`ï¼Œå®ƒç®€åŒ–äº†è¿™ä¸ªè¿‡ç¨‹ï¼Œé‡‡å–ä»¥ä¸‹æ­¥éª¤ï¼š

1.  å°†æ¨¡å‹åˆå§‹åŒ–ä¸º`Pipeline`ã€‚

1.  é€šè¿‡ç®¡é“è¿è¡Œå ä½ç¬¦è¾“å…¥ï¼Œä»¥ä¾¿ ONNX å¯ä»¥è®°å½•è®¡ç®—å›¾ã€‚

1.  å®šä¹‰åŠ¨æ€è½´ä»¥å¤„ç†åŠ¨æ€åºåˆ—é•¿åº¦ã€‚

1.  ä¿å­˜å…·æœ‰ç½‘ç»œå‚æ•°çš„å›¾ã€‚

è¦ä½¿ç”¨è¿™ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸º ONNX è®¾ç½®ä¸€äº›[OpenMP](https://openmp.org)ç¯å¢ƒå˜é‡ï¼š

```py
import os
from psutil import cpu_count

os.environ["OMP_NUM_THREADS"] = f"{cpu_count()}"
os.environ["OMP_WAIT_POLICY"] = "ACTIVE"
```

OpenMP æ˜¯ä¸€ä¸ªä¸ºå¼€å‘é«˜åº¦å¹¶è¡ŒåŒ–åº”ç”¨ç¨‹åºè€Œè®¾è®¡çš„ APIã€‚`OMP_NUM_THREADS`ç¯å¢ƒå˜é‡è®¾ç½®å¹¶è¡Œè®¡ç®—ä¸­ä½¿ç”¨çš„çº¿ç¨‹æ•°ï¼Œåœ¨ ONNX Runtime ä¸­ï¼Œ`OMP_WAIT_POLICY=ACTIVE`æŒ‡å®šç­‰å¾…çº¿ç¨‹åº”å¤„äºæ´»åŠ¨çŠ¶æ€ï¼ˆå³ä½¿ç”¨ CPU å¤„ç†å™¨å‘¨æœŸï¼‰ã€‚

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å°†æˆ‘ä»¬çš„ç²¾ç‚¼æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éœ€è¦æŒ‡å®šå‚æ•°`pipeline_name="text-classification"`ï¼Œå› ä¸º`convert()`åœ¨è½¬æ¢è¿‡ç¨‹ä¸­å°†æ¨¡å‹åŒ…è£…åœ¨ä¸€ä¸ª![nlpt_pin01](img/nlpt_pin01.png) Transformers `pipeline()`å‡½æ•°ä¸­ã€‚é™¤äº†`model_ckpt`ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ä¼ é€’äº† tokenizer æ¥åˆå§‹åŒ–ç®¡é“ï¼š

```py
from transformers.convert_graph_to_onnx import convert

model_ckpt = "transformersbook/distilbert-base-uncased-distilled-clinc"
onnx_model_path = Path("onnx/model.onnx")
convert(framework="pt", model=model_ckpt, tokenizer=tokenizer,
        output=onnx_model_path, opset=12, pipeline_name="text-classification")
```

ONNX ä½¿ç”¨*æ“ä½œç¬¦é›†*æ¥å°†ä¸å¯å˜çš„æ“ä½œç¬¦è§„èŒƒåˆ†ç»„åœ¨ä¸€èµ·ï¼Œå› æ­¤`opset=12`å¯¹åº”äº ONNX åº“çš„ç‰¹å®šç‰ˆæœ¬ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»ä¿å­˜äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª`InferenceSession`å®ä¾‹æ¥å‘æ¨¡å‹è¾“å…¥æ•°æ®ï¼š

```py
from onnxruntime import (GraphOptimizationLevel, InferenceSession,
                         SessionOptions)

def create_model_for_provider(model_path, provider="CPUExecutionProvider"):
    options = SessionOptions()
    options.intra_op_num_threads = 1
    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL
    session = InferenceSession(str(model_path), options, providers=[provider])
    session.disable_fallback()
    return session
```

```py
onnx_model = create_model_for_provider(onnx_model_path)
```

ç°åœ¨å½“æˆ‘ä»¬è°ƒç”¨`onnx_model.run()`æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä» ONNX æ¨¡å‹ä¸­è·å–ç±»åˆ«å¯¹æ•°ã€‚è®©æˆ‘ä»¬ç”¨æµ‹è¯•é›†ä¸­çš„ä¸€ä¸ªä¾‹å­æ¥æµ‹è¯•ä¸€ä¸‹ã€‚ç”±äº`convert()`çš„è¾“å‡ºå‘Šè¯‰æˆ‘ä»¬ ONNX åªæœŸæœ›`input_ids`å’Œ`attention_mask`ä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬éœ€è¦ä»æˆ‘ä»¬çš„æ ·æœ¬ä¸­åˆ é™¤`label`åˆ—ï¼š

```py
inputs = clinc_enc["test"][:1]
del inputs["labels"]
logits_onnx = onnx_model.run(None, inputs)[0]
logits_onnx.shape
```

```py
(1, 151)
```

ä¸€æ—¦æˆ‘ä»¬æœ‰äº†å¯¹æ•°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å– argmax è½»æ¾è·å¾—é¢„æµ‹çš„æ ‡ç­¾ï¼š

```py
np.argmax(logits_onnx)
```

```py
61
```

è¿™ç¡®å®ä¸åœ°é¢çœŸå®æ ‡ç­¾ä¸€è‡´ï¼š

```py
clinc_enc["test"][0]["labels"]
```

```py
61
```

ONNX æ¨¡å‹ä¸`text-classification`ç®¡é“ä¸å…¼å®¹ï¼Œå› æ­¤æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ¨¡ä»¿æ ¸å¿ƒè¡Œä¸ºçš„è‡ªå®šä¹‰ç±»ï¼š

```py
from scipy.special import softmax

class OnnxPipeline:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def __call__(self, query):
        model_inputs = self.tokenizer(query, return_tensors="pt")
        inputs_onnx = {k: v.cpu().detach().numpy()
                       for k, v in model_inputs.items()}
        logits = self.model.run(None, inputs_onnx)[0][0, :]
        probs = softmax(logits)
        pred_idx = np.argmax(probs).item()
        return [{"label": intents.int2str(pred_idx), "score": probs[pred_idx]}]
```

ç„¶åæˆ‘ä»¬å¯ä»¥æµ‹è¯•è¿™ä¸ªç®€å•çš„æŸ¥è¯¢ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦æ¢å¤äº†`car_rental`æ„å›¾ï¼š

```py
pipe = OnnxPipeline(onnx_model, tokenizer)
pipe(query)
```

```py
[{'label': 'car_rental', 'score': 0.7848334}]
```

å¾ˆå¥½ï¼Œæˆ‘ä»¬çš„æµæ°´çº¿æŒ‰é¢„æœŸå·¥ä½œã€‚ä¸‹ä¸€æ­¥æ˜¯ä¸º ONNX æ¨¡å‹åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å€Ÿé‰´æˆ‘ä»¬ä¸`Perâ formanceBenchmark`ç±»ä¸€èµ·å®Œæˆçš„å·¥ä½œï¼Œåªéœ€é‡å†™`compute_size()`æ–¹æ³•ï¼Œä¿ç•™`compute_accuracy()`å’Œ`time_pipeline()`æ–¹æ³•ã€‚æˆ‘ä»¬éœ€è¦é‡å†™`compute_size()`æ–¹æ³•çš„åŸå› æ˜¯ï¼Œæˆ‘ä»¬ä¸èƒ½ä¾èµ–`state_dict`å’Œ`torch.save()`æ¥æµ‹é‡æ¨¡å‹çš„å¤§å°ï¼Œå› ä¸º`onnx_model`åœ¨æŠ€æœ¯ä¸Šæ˜¯ä¸€ä¸ª ONNX`InferenceSession`å¯¹è±¡ï¼Œæ— æ³•è®¿é—® PyTorch çš„`nn.Module`çš„å±æ€§ã€‚æ— è®ºå¦‚ä½•ï¼Œç»“æœé€»è¾‘å¾ˆç®€å•ï¼Œå¯ä»¥å®ç°å¦‚ä¸‹ï¼š

```py
class OnnxPerformanceBenchmark(PerformanceBenchmark):
    def __init__(self, *args, model_path, **kwargs):
        super().__init__(*args, **kwargs)
        self.model_path = model_path

    def compute_size(self):
        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)
        print(f"Model size (MB) - {size_mb:.2f}")
        return {"size_mb": size_mb}
```

é€šè¿‡æˆ‘ä»¬çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„è’¸é¦æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼åçš„æ€§èƒ½ï¼š

```py
optim_type = "Distillation + ORT"
pb = OnnxPerformanceBenchmark(pipe, clinc["test"], optim_type,
                              model_path="onnx/model.onnx")
perf_metrics.update(pb.run_benchmark())
```

```py
Model size (MB) - 255.88
Average latency (ms) - 21.02 +\- 0.55
Accuracy on test set - 0.868
```

```py
plot_metrics(perf_metrics, optim_type)
```

![](img/nlpt_08in05.png)

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè½¬æ¢ä¸º ONNX æ ¼å¼å¹¶ä½¿ç”¨ ONNX Runtime ä¸ºæˆ‘ä»¬çš„è’¸é¦æ¨¡å‹ï¼ˆå³å›¾ä¸­çš„â€œè’¸é¦â€åœˆï¼‰æä¾›äº†å»¶è¿Ÿå¢ç›Šï¼è®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦å¯ä»¥é€šè¿‡æ·»åŠ é‡åŒ–æ¥æŒ¤å‡ºæ›´å¤šæ€§èƒ½ã€‚

ä¸ PyTorch ç±»ä¼¼ï¼ŒORT æä¾›äº†ä¸‰ç§æ¨¡å‹é‡åŒ–çš„æ–¹å¼ï¼šåŠ¨æ€é‡åŒ–ã€é™æ€é‡åŒ–å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒã€‚ä¸ PyTorch ä¸€æ ·ï¼Œæˆ‘ä»¬å°†å¯¹æˆ‘ä»¬çš„è’¸é¦æ¨¡å‹åº”ç”¨åŠ¨æ€é‡åŒ–ã€‚åœ¨ ORT ä¸­ï¼Œé‡åŒ–æ˜¯é€šè¿‡`quanâ tize_dynamic()`å‡½æ•°åº”ç”¨çš„ï¼Œè¯¥å‡½æ•°éœ€è¦ä¸€ä¸ª ONNX æ¨¡å‹çš„è·¯å¾„è¿›è¡Œé‡åŒ–ï¼Œä¸€ä¸ªç›®æ ‡è·¯å¾„æ¥ä¿å­˜é‡åŒ–åçš„æ¨¡å‹ï¼Œä»¥åŠè¦å°†æƒé‡å‡å°‘åˆ°çš„æ•°æ®ç±»å‹ï¼š

```py
from onnxruntime.quantization import quantize_dynamic, QuantType

model_input = "onnx/model.onnx"
model_output = "onnx/model.quant.onnx"
quantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)
```

ç°åœ¨æ¨¡å‹å·²ç»è¢«é‡åŒ–ï¼Œè®©æˆ‘ä»¬é€šè¿‡æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è¿è¡Œå®ƒï¼š

```py
onnx_quantized_model = create_model_for_provider(model_output)
pipe = OnnxPipeline(onnx_quantized_model, tokenizer)
optim_type = "Distillation + ORT (quantized)"
pb = OnnxPerformanceBenchmark(pipe, clinc["test"], optim_type,
                              model_path=model_output)
perf_metrics.update(pb.run_benchmark())
```

```py
Model size (MB) - 64.20
Average latency (ms) - 9.24 +\- 0.29
Accuracy on test set - 0.877
```

```py
plot_metrics(perf_metrics, optim_type)
```

![](img/nlpt_08in06.png)

ä¸ PyTorch é‡åŒ–è·å¾—çš„æ¨¡å‹ç›¸æ¯”ï¼ŒORT é‡åŒ–å·²ç»å°†æ¨¡å‹å¤§å°å’Œå»¶è¿Ÿå‡å°‘äº†çº¦ 30%ï¼ˆè’¸é¦+é‡åŒ– blobï¼‰ã€‚å…¶ä¸­ä¸€ä¸ªåŸå› æ˜¯ PyTorch åªä¼˜åŒ–`nn.Linear`æ¨¡å—ï¼Œè€Œ ONNX è¿˜é‡åŒ–äº†åµŒå…¥å±‚ã€‚ä»å›¾ä¸­æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°ï¼Œå°† ORT é‡åŒ–åº”ç”¨äºæˆ‘ä»¬çš„è’¸é¦æ¨¡å‹ä¸æˆ‘ä»¬çš„ BERT åŸºçº¿ç›¸æ¯”ï¼Œæä¾›äº†è¿‘ä¸‰å€çš„å¢ç›Šï¼

è¿™ç»“æŸäº†æˆ‘ä»¬å¯¹åŠ é€Ÿ Transformer è¿›è¡Œæ¨æ–­çš„æŠ€æœ¯çš„åˆ†æã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œè¯¸å¦‚é‡åŒ–ä¹‹ç±»çš„æ–¹æ³•é€šè¿‡é™ä½è¡¨ç¤ºçš„ç²¾åº¦æ¥å‡å°æ¨¡å‹å¤§å°ã€‚å¦ä¸€ç§å‡å°å¤§å°çš„ç­–ç•¥æ˜¯å½»åº•åˆ é™¤ä¸€äº›æƒé‡ã€‚è¿™ç§æŠ€æœ¯ç§°ä¸º*æƒé‡ä¿®å‰ª*ï¼Œå¹¶ä¸”æ˜¯ä¸‹ä¸€èŠ‚çš„é‡ç‚¹ã€‚

# ä½¿ç”¨æƒé‡ä¿®å‰ªä½¿æ¨¡å‹æ›´ç¨€ç–

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°çŸ¥è¯†è’¸é¦å’Œæƒé‡é‡åŒ–åœ¨äº§ç”Ÿæ›´å¿«çš„æ¨æ–­æ¨¡å‹æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½è¿˜å¯¹æ¨¡å‹çš„å†…å­˜å ç”¨æœ‰å¾ˆå¼ºçš„çº¦æŸã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çš„äº§å“ç»ç†çªç„¶å†³å®šæˆ‘ä»¬çš„æ–‡æœ¬åŠ©æ‰‹éœ€è¦éƒ¨ç½²åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦æˆ‘ä»¬çš„æ„å›¾åˆ†ç±»å™¨å°½å¯èƒ½å°‘åœ°å ç”¨å­˜å‚¨ç©ºé—´ã€‚ä¸ºäº†å®Œæˆæˆ‘ä»¬å¯¹å‹ç¼©æ–¹æ³•çš„è°ƒæŸ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•é€šè¿‡è¯†åˆ«å’Œåˆ é™¤ç½‘ç»œä¸­æœ€ä¸é‡è¦çš„æƒé‡æ¥å‡å°‘æ¨¡å‹å‚æ•°çš„æ•°é‡ã€‚

## æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„ç¨€ç–æ€§

å¦‚å›¾ 8-10 æ‰€ç¤ºï¼Œä¿®å‰ªçš„ä¸»è¦æ€æƒ³æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸ç§»é™¤æƒé‡è¿æ¥ï¼ˆå¯èƒ½è¿˜æœ‰ç¥ç»å…ƒï¼‰ï¼Œä½¿æ¨¡å‹é€æ¸å˜å¾—æ›´ç¨€ç–ã€‚ç»“æœä¿®å‰ªåçš„æ¨¡å‹å…·æœ‰æ›´å°‘çš„éé›¶å‚æ•°ï¼Œç„¶åå¯ä»¥ä»¥ç´§å‡‘çš„ç¨€ç–çŸ©é˜µæ ¼å¼å­˜å‚¨ã€‚ä¿®å‰ªä¹Ÿå¯ä»¥ä¸é‡åŒ–ç»“åˆä»¥è·å¾—è¿›ä¸€æ­¥çš„å‹ç¼©ã€‚

![ç½‘ç»œä¿®å‰ª](img/nlpt_0810.png)

###### å›¾ 8-10ã€‚ä¿®å‰ªå‰åçš„æƒé‡å’Œç¥ç»å…ƒï¼ˆç”± Song Han æä¾›ï¼‰

## æƒé‡ä¿®å‰ªæ–¹æ³•

åœ¨æ•°å­¦ä¸Šï¼Œå¤§å¤šæ•°æƒé‡ä¿®å‰ªæ–¹æ³•çš„å·¥ä½œæ–¹å¼æ˜¯è®¡ç®—ä¸€ä¸ª*é‡è¦æ€§åˆ†æ•°*çŸ©é˜µ<math alttext="bold upper S"><mi>ğ’</mi></math>ï¼Œç„¶åæŒ‰é‡è¦æ€§é€‰æ‹©å‰<math alttext="k"><mi>k</mi></math>ç™¾åˆ†æ¯”çš„æƒé‡ï¼š

<math alttext="normal upper T normal o normal p Subscript k Baseline left-parenthesis bold upper S right-parenthesis Subscript i j Baseline equals StartLayout Enlarged left-brace 1st Row 1st Column 1 2nd Column Blank 3rd Column normal i normal f upper S Subscript i j Baseline normal i normal n normal t normal o normal p k percent-sign 2nd Row 1st Column 0 2nd Column Blank 3rd Column normal o normal t normal h normal e normal r normal w normal i normal s normal e EndLayout" display="block"><mrow><msub><mi>Top</mi> <mi>k</mi></msub> <msub><mrow><mo>(</mo><mi>ğ’</mi><mo>)</mo></mrow> <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mn>1</mn></mrow></mtd> <mtd><mrow><mi>if</mi> <msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mi>in</mi> <mi>top</mi> <mi>k</mi> <mo>%</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd> <mtd><mi>otherwise</mi></mtd></mtr></mtable></mfenced></mrow></math>

å®é™…ä¸Šï¼Œ<math alttext="k"><mi>k</mi></math> ä½œä¸ºä¸€ä¸ªæ–°çš„è¶…å‚æ•°ï¼Œç”¨æ¥æ§åˆ¶æ¨¡å‹ä¸­ç¨€ç–æ€§çš„ç¨‹åº¦ï¼Œå³æƒé‡ä¸ºé›¶å€¼çš„æ¯”ä¾‹ã€‚è¾ƒä½çš„ <math alttext="k"><mi>k</mi></math> å€¼å¯¹åº”ç€æ›´ç¨€ç–çš„çŸ©é˜µã€‚ä»è¿™äº›åˆ†æ•°ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ª*æ©ç çŸ©é˜µ* <math alttext="bold upper M"><mi>ğŒ</mi></math>ï¼Œåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œç”¨ä¸€äº›è¾“å…¥ <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> æ©ç›–æƒé‡ <math alttext="upper W Subscript i j"><msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ›å»ºä¸€ä¸ªç¨€ç–çš„æ¿€æ´»ç½‘ç»œ <math alttext="a Subscript i"><msub><mi>a</mi> <mi>i</mi></msub></math>ï¼š

<math alttext="a Subscript i Baseline equals sigma-summation Underscript k Endscripts upper W Subscript i k Baseline upper M Subscript i k Baseline x Subscript k" display="block"><mrow><msub><mi>a</mi> <mi>i</mi></msub> <mo>=</mo> <munder><mo>âˆ‘</mo> <mi>k</mi></munder> <msub><mi>W</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mi>M</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mi>x</mi> <mi>k</mi></msub></mrow></math>

æ­£å¦‚â€œæœ€ä½³è„‘å¤–ç§‘åŒ»ç”Ÿâ€è®ºæ–‡ä¸­æ‰€è®¨è®ºçš„é‚£æ ·ï¼Œæ¯ç§å‰ªææ–¹æ³•çš„æ ¸å¿ƒéƒ½æ˜¯ä¸€ç»„éœ€è¦è€ƒè™‘çš„é—®é¢˜ï¼š

+   å“ªäº›æƒé‡åº”è¯¥è¢«æ¶ˆé™¤ï¼Ÿ

+   å‰©ä½™çš„æƒé‡åº”è¯¥å¦‚ä½•è°ƒæ•´ä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼Ÿ

+   å¦‚ä½•ä»¥è®¡ç®—æœ‰æ•ˆçš„æ–¹å¼è¿›è¡Œç½‘ç»œå‰ªæï¼Ÿ

è¿™äº›é—®é¢˜çš„ç­”æ¡ˆå‘Šè¯‰äº†æˆ‘ä»¬å¦‚ä½•è®¡ç®—å¾—åˆ†çŸ©é˜µ <math alttext="bold upper S"><mi>ğ’</mi></math>ï¼Œå› æ­¤è®©æˆ‘ä»¬é¦–å…ˆçœ‹ä¸€ä¸‹æœ€æ—©å’Œæœ€æµè¡Œçš„å‰ªææ–¹æ³•ä¹‹ä¸€ï¼šå¹…åº¦å‰ªæã€‚

### å¹…åº¦å‰ªæ

é¡¾åæ€ä¹‰ï¼Œå¹…åº¦å‰ªææ ¹æ®æƒé‡çš„å¹…åº¦è®¡ç®—å¾—åˆ† <math alttext="bold upper S equals left-parenthesis bar upper W Subscript i j Baseline bar right-parenthesis Subscript 1 less-than-or-equal-to j comma j less-than-or-equal-to n"><mrow><mi>ğ’</mi> <mo>=</mo> <msub><mfenced separators="" open="(" close=")"><mo>âˆ£</mo> <msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>âˆ£</mo></mfenced> <mrow><mn>1</mn><mo>â‰¤</mo><mi>j</mi><mo>,</mo><mi>j</mi><mo>â‰¤</mo><mi>n</mi></mrow></msub></mrow></math>ï¼Œç„¶åä» <math alttext="bold upper M equals normal upper T normal o normal p Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>ğŒ</mi> <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>ğ’</mi> <mo>)</mo></mrow></mrow></math> ä¸­å¾—å‡ºæ©ç ã€‚åœ¨æ–‡çŒ®ä¸­ï¼Œé€šå¸¸é€šè¿‡è¿­ä»£çš„æ–¹å¼åº”ç”¨å¹…åº¦å‰ªæï¼Œé¦–å…ˆè®­ç»ƒæ¨¡å‹å­¦ä¹ å“ªäº›è¿æ¥æ˜¯é‡è¦çš„ï¼Œç„¶åå‰ªææœ€ä¸é‡è¦çš„æƒé‡ã€‚ç¨€ç–æ¨¡å‹ç„¶åè¢«é‡æ–°è®­ç»ƒï¼Œå¹¶ä¸”é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°æœŸæœ›çš„ç¨€ç–åº¦ã€‚

è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯è®¡ç®—éœ€æ±‚é‡å¤§ï¼šåœ¨æ¯ä¸€æ­¥ä¿®å‰ªä¸­ï¼Œæˆ‘ä»¬éƒ½éœ€è¦å°†æ¨¡å‹è®­ç»ƒåˆ°æ”¶æ•›ã€‚å› æ­¤ï¼Œé€šå¸¸æœ€å¥½é€æ¸å¢åŠ åˆå§‹ç¨€ç–åº¦<math alttext="s Subscript i"><msub><mi>s</mi> <mi>i</mi></msub></math>ï¼ˆé€šå¸¸ä¸ºé›¶ï¼‰åˆ°ä¸€å®šæ­¥æ•°<math alttext="upper N"><mi>N</mi></math>åçš„æœ€ç»ˆå€¼<math alttext="s Subscript f"><msub><mi>s</mi> <mi>f</mi></msub></math>ã€‚Â¹â¹

<math alttext="s Subscript t Baseline equals s Subscript f Baseline plus left-parenthesis s Subscript i Baseline minus s Subscript f Baseline right-parenthesis left-parenthesis 1 minus StartFraction t minus t 0 Over upper N normal upper Delta t EndFraction right-parenthesis cubed normal f normal o normal r t element-of StartSet t 0 comma t 0 plus normal upper Delta t comma ellipsis comma t 0 plus upper N normal upper Delta t EndSet" display="block"><mrow><msub><mi>s</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>s</mi> <mi>f</mi></msub> <mo>+</mo> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>i</mi></msub> <mo>-</mo> <msub><mi>s</mi> <mi>f</mi></msub> <mo>)</mo></mrow> <msup><mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mfrac><mrow><mi>t</mi><mo>-</mo><msub><mi>t</mi> <mn>0</mn></msub></mrow> <mrow><mi>N</mi><mi>Î”</mi><mi>t</mi></mrow></mfrac></mfenced> <mn>3</mn></msup> <mi>for</mi> <mi>t</mi> <mo>âˆˆ</mo> <mrow><mo>{</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>+</mo> <mi>Î”</mi> <mi>t</mi> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>+</mo> <mi>N</mi> <mi>Î”</mi> <mi>t</mi> <mo>}</mo></mrow></mrow></math>

è¿™é‡Œçš„æƒ³æ³•æ˜¯æ¯éš”<math alttext="normal upper Delta t"><mrow><mi>Î”</mi> <mi>t</mi></mrow></math>æ­¥æ›´æ–°ä¸€æ¬¡äºŒè¿›åˆ¶æ©ç <math alttext="bold upper M"><mi>ğŒ</mi></math>ï¼Œä»¥å…è®¸è¢«å±è”½çš„æƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡æ–°æ¿€æ´»ï¼Œå¹¶ä»ä¿®å‰ªè¿‡ç¨‹ä¸­å¯èƒ½å¯¼è‡´çš„ä»»ä½•ç²¾åº¦æŸå¤±ä¸­æ¢å¤è¿‡æ¥ã€‚å¦‚å›¾ 8-11 æ‰€ç¤ºï¼Œç«‹æ–¹å› å­æ„å‘³ç€æƒé‡ä¿®å‰ªçš„é€Ÿç‡åœ¨æ—©æœŸé˜¶æ®µæœ€é«˜ï¼ˆå½“å†—ä½™æƒé‡æ•°é‡è¾ƒå¤§æ—¶ï¼‰ï¼Œå¹¶é€æ¸å‡å°ã€‚

![ç¨€ç–è°ƒåº¦å™¨](img/nlpt_0811.png)

###### å›¾ 8-11ã€‚ç”¨äºä¿®å‰ªçš„ç«‹æ–¹ç¨€ç–è°ƒåº¦å™¨ã€‚

å¹…åº¦ä¿®å‰ªçš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå®ƒå®é™…ä¸Šæ˜¯ä¸ºçº¯ç›‘ç£å­¦ä¹ è€Œè®¾è®¡çš„ï¼Œå…¶ä¸­æ¯ä¸ªæƒé‡çš„é‡è¦æ€§ä¸æ‰‹å¤´çš„ä»»åŠ¡ç›´æ¥ç›¸å…³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨è¿ç§»å­¦ä¹ ä¸­ï¼Œæƒé‡çš„é‡è¦æ€§ä¸»è¦ç”±é¢„è®­ç»ƒé˜¶æ®µç¡®å®šï¼Œå› æ­¤å¹…åº¦ä¿®å‰ªå¯èƒ½ä¼šç§»é™¤å¯¹å¾®è°ƒä»»åŠ¡é‡è¦çš„è¿æ¥ã€‚æœ€è¿‘ï¼ŒHugging Face çš„ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§ç§°ä¸ºç§»åŠ¨ä¿®å‰ªçš„è‡ªé€‚åº”æ–¹æ³•â€”â€”è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ã€‚Â²â°

### ç§»åŠ¨ä¿®å‰ª

ç§»åŠ¨ä¿®å‰ªèƒŒåçš„åŸºæœ¬æ€æƒ³æ˜¯*é€æ¸*åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ç§»é™¤æƒé‡ï¼Œä½¿æ¨¡å‹é€æ¸å˜å¾—*æ›´ç¨€ç–*ã€‚å…³é”®çš„æ–°é¢–ä¹‹å¤„åœ¨äºï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæƒé‡å’Œåˆ†æ•°éƒ½æ˜¯å¯å­¦ä¹ çš„ã€‚å› æ­¤ï¼Œä¸å¹…åº¦ä¿®å‰ªç›´æ¥ä»æƒé‡æ´¾ç”Ÿï¼ˆå¦‚å¹…åº¦ä¿®å‰ªï¼‰ä¸åŒï¼Œç§»åŠ¨ä¿®å‰ªä¸­çš„åˆ†æ•°æ˜¯ä»»æ„çš„ï¼Œå¹¶ä¸”é€šè¿‡æ¢¯åº¦ä¸‹é™å­¦ä¹ ï¼Œå°±åƒä»»ä½•å…¶ä»–ç¥ç»ç½‘ç»œå‚æ•°ä¸€æ ·ã€‚è¿™æ„å‘³ç€åœ¨åå‘ä¼ æ’­ä¸­ï¼Œæˆ‘ä»¬è¿˜è¦è·Ÿè¸ªæŸå¤±<math alttext="upper L"><mi>L</mi></math>ç›¸å¯¹äºåˆ†æ•°<math alttext="upper S Subscript i j"><msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>çš„æ¢¯åº¦ã€‚

ä¸€æ—¦å­¦ä¹ äº†åˆ†æ•°ï¼Œå°±å¾ˆå®¹æ˜“ä½¿ç”¨<math alttext="bold upper M equals normal upper T normal o normal p Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>ğŒ</mi> <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>ğ’</mi> <mo>)</mo></mrow></mrow></math>ç”ŸæˆäºŒè¿›åˆ¶æ©ç ã€‚Â²Â¹

è¿åŠ¨å‰ªæèƒŒåçš„ç›´è§‰æ˜¯ï¼Œâ€œç§»åŠ¨â€ç¦»é›¶æœ€è¿œçš„æƒé‡æ˜¯æœ€é‡è¦çš„ã€‚æ¢å¥è¯è¯´ï¼Œæ­£æƒé‡åœ¨ç²¾ç»†è°ƒæ•´æœŸé—´å¢åŠ ï¼ˆè´Ÿæƒé‡ç›¸åï¼‰ï¼Œè¿™ç›¸å½“äºè¯´åˆ†æ•°éšç€æƒé‡è¿œç¦»é›¶è€Œå¢åŠ ã€‚å¦‚å›¾ 8-12 æ‰€ç¤ºï¼Œè¿™ç§è¡Œä¸ºä¸å¹…å€¼å‰ªæä¸åŒï¼Œåè€…é€‰æ‹©ç¦»é›¶æœ€è¿œçš„æƒé‡ä½œä¸ºæœ€é‡è¦çš„æƒé‡ã€‚

![å¹…å€¼ä¸è¿åŠ¨å‰ªæ](img/nlpt_0812.png)

###### å›¾ 8-12ã€‚å¹…å€¼å‰ªæï¼ˆå·¦ï¼‰å’Œè¿åŠ¨å‰ªæï¼ˆå³ï¼‰ä¸­ç§»é™¤çš„æƒé‡çš„æ¯”è¾ƒ

è¿™ä¸¤ç§å‰ªææ–¹æ³•ä¹‹é—´çš„å·®å¼‚ä¹Ÿåœ¨å‰©ä½™æƒé‡çš„åˆ†å¸ƒä¸­æ˜¾è€Œæ˜“è§ã€‚å¦‚å›¾ 8-13 æ‰€ç¤ºï¼Œå¹…å€¼å‰ªæäº§ç”Ÿä¸¤ä¸ªæƒé‡ç°‡ï¼Œè€Œè¿åŠ¨å‰ªæäº§ç”Ÿæ›´å¹³æ»‘çš„åˆ†å¸ƒã€‚

æˆªè‡³æœ¬ä¹¦æ’°å†™æ—¶ï¼Œ![nlpt_pin01](img/nlpt_pin01.png) Transformers ä¸æ”¯æŒå¼€ç®±å³ç”¨çš„å‰ªææ–¹æ³•ã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰ä¸€ä¸ªåä¸º[*ç¥ç»ç½‘ç»œå—è¿åŠ¨å‰ªæ*](https://oreil.ly/aHEvD)çš„å·§å¦™åº“å®ç°äº†è®¸å¤šè¿™äº›æƒ³æ³•ï¼Œå¦‚æœå†…å­˜é™åˆ¶æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æŸ¥çœ‹å®ƒã€‚

![å‰ªæåˆ†å¸ƒ](img/nlpt_0813.png)

###### å›¾ 8-13ã€‚å‰©ä½™æƒé‡çš„åˆ†å¸ƒï¼Œç”¨äºå¹…å€¼å‰ªæï¼ˆMaPï¼‰å’Œè¿åŠ¨å‰ªæï¼ˆMvPï¼‰

# ç»“è®º

æˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œä¼˜åŒ– Transformer ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­æ¶‰åŠæ²¿ä¸¤ä¸ªç»´åº¦çš„å‹ç¼©ï¼šå»¶è¿Ÿå’Œå†…å­˜å ç”¨ã€‚ä»ç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹å¼€å§‹ï¼Œæˆ‘ä»¬åº”ç”¨äº†è’¸é¦ã€é‡åŒ–å’Œ ORT ä¼˜åŒ–ï¼Œæ˜¾è‘—å‡å°‘äº†è¿™ä¸¤è€…ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é‡åŒ–å’Œ ORT ä¸­çš„è½¬æ¢ç»™å‡ºäº†æœ€å¤§çš„æ”¶ç›Šï¼Œè€Œä»˜å‡ºçš„åŠªåŠ›æœ€å°ã€‚

å°½ç®¡å‰ªææ˜¯å‡å°‘ Transformer æ¨¡å‹å­˜å‚¨å¤§å°çš„æœ‰æ•ˆç­–ç•¥ï¼Œä½†å½“å‰çš„ç¡¬ä»¶å¹¶æœªé’ˆå¯¹ç¨€ç–çŸ©é˜µè¿ç®—è¿›è¡Œä¼˜åŒ–ï¼Œè¿™é™åˆ¶äº†è¿™ç§æŠ€æœ¯çš„å®ç”¨æ€§ã€‚ç„¶è€Œï¼Œè¿™æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œåˆ°æœ¬ä¹¦ä¸Šå¸‚æ—¶ï¼Œè®¸å¤šè¿™äº›é™åˆ¶å¯èƒ½å·²ç»å¾—åˆ°è§£å†³ã€‚

é‚£ä¹ˆæ¥ä¸‹æ¥å‘¢ï¼Ÿæœ¬ç« ä¸­çš„æ‰€æœ‰æŠ€æœ¯éƒ½å¯ä»¥åº”ç”¨åˆ°å…¶ä»–ä»»åŠ¡ä¸­ï¼Œæ¯”å¦‚é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«æˆ–è¯­è¨€å»ºæ¨¡ã€‚å¦‚æœæ‚¨å‘ç°è‡ªå·±éš¾ä»¥æ»¡è¶³å»¶è¿Ÿè¦æ±‚ï¼Œæˆ–è€…æ‚¨çš„æ¨¡å‹å ç”¨äº†æ‰€æœ‰çš„è®¡ç®—é¢„ç®—ï¼Œæˆ‘ä»¬å»ºè®®å°è¯•å…¶ä¸­ä¹‹ä¸€ã€‚

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ‘†è„±æ€§èƒ½ä¼˜åŒ–ï¼Œæ¢è®¨æ¯ä¸ªæ•°æ®ç§‘å­¦å®¶çš„å™©æ¢¦ï¼šå¤„ç†å°‘é‡æˆ–æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µã€‚

Â¹ S. Larson ç­‰äººï¼Œ[â€œæ„å›¾åˆ†ç±»å’Œè¶…å‡ºèŒƒå›´é¢„æµ‹çš„è¯„ä¼°æ•°æ®é›†â€](https://arxiv.org/abs/1909.02027)ï¼Œï¼ˆ2019 å¹´ï¼‰ã€‚

Â² æ­£å¦‚ Emmanuel Ameisen åœ¨*æ„å»ºæœºå™¨å­¦ä¹ é©±åŠ¨çš„åº”ç”¨*ï¼ˆO'Reillyï¼‰ä¸­æ‰€æè¿°çš„ï¼Œä¸šåŠ¡æˆ–äº§å“æŒ‡æ ‡æ˜¯*æœ€*é‡è¦çš„è€ƒè™‘å› ç´ ã€‚æ¯•ç«Ÿï¼Œå¦‚æœæ‚¨çš„æ¨¡å‹ä¸èƒ½è§£å†³ä¸šåŠ¡å…³å¿ƒçš„é—®é¢˜ï¼Œé‚£ä¹ˆå®ƒçš„å‡†ç¡®æ€§å°±æ— å…³ç´§è¦ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å‡è®¾æ‚¨å·²ç»ä¸ºåº”ç”¨ç¨‹åºå®šä¹‰äº†é‡è¦çš„æŒ‡æ ‡ï¼Œå¹¶ä¸“æ³¨äºä¼˜åŒ–æ¨¡å‹æŒ‡æ ‡ã€‚

Â³ C. BuciluÄƒç­‰äººï¼Œâ€œæ¨¡å‹å‹ç¼©â€ï¼Œ*ç¬¬ 12 å±Š ACM SIGKDD å›½é™…çŸ¥è¯†å‘ç°å’Œæ•°æ®æŒ–æ˜ä¼šè®®è®ºæ–‡é›†*ï¼ˆ2006 å¹´ 8 æœˆï¼‰ï¼š535-541ï¼Œ[*https://doi.org/10.1145/1150402.1150464*](https://doi.org/10.1145/1150402.1150464)ã€‚

â´ G. Hinton, O. Vinyals å’Œ J. Deanï¼Œ[â€œè’¸é¦ç¥ç»ç½‘ç»œä¸­çš„çŸ¥è¯†â€](https://arxiv.org/abs/1503.02531)ï¼Œï¼ˆ2015 å¹´ï¼‰ã€‚

âµ W. Fedus, B. Zoph, and N. Shazeerï¼Œ[â€œSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsityâ€](https://arxiv.org/abs/2101.03961)ï¼Œ(2021)ã€‚

â¶ Geoff Hinton åœ¨ä¸€æ¬¡[æ¼”è®²](https://oreil.ly/OkHGp)ä¸­åˆ›é€ äº†è¿™ä¸ªæœ¯è¯­ï¼Œç”¨æ¥æŒ‡ä»£è½¯åŒ–æ¦‚ç‡æ­ç¤ºäº†æ•™å¸ˆçš„éšè—çŸ¥è¯†çš„è§‚å¯Ÿã€‚

â· æˆ‘ä»¬åœ¨ç¬¬äº”ç« ä¸­ä¹Ÿé‡åˆ°äº†ä¸æ–‡æœ¬ç”Ÿæˆç›¸å…³çš„æ¸©åº¦ã€‚

â¸ V. Sanh ç­‰äººï¼Œ[â€œDistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighterâ€](https://arxiv.org/abs/1910.01108)ï¼Œ(2019)ã€‚

â¹ Y. Kim and H. Awadallaï¼Œ[â€œFastFormers: Highly Efficient Transformer Models for Natural Language Understandingâ€](https://arxiv.org/abs/2010.13382)ï¼Œ(2020)ã€‚

Â¹â° é»˜è®¤æƒ…å†µä¸‹ï¼Œ`Trainer` åœ¨è¿›è¡Œåˆ†ç±»ä»»åŠ¡å¾®è°ƒæ—¶ä¼šå¯»æ‰¾åä¸º `labels` çš„åˆ—ã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡æŒ‡å®š `TrainingArguments` çš„ `label_names` å‚æ•°æ¥è¦†ç›–æ­¤è¡Œä¸ºã€‚

Â¹Â¹ å¯¹é€šç”¨çš„ç²¾ç‚¼è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ–¹æ³•æœ‰æ—¶è¢«ç§°ä¸ºâ€œä»»åŠ¡ä¸å¯çŸ¥â€ç²¾ç‚¼ã€‚

Â¹Â² T. Akiba ç­‰äººï¼Œ[â€œOptuna: A Next-Generation Hyperparameter Optimization Frameworkâ€](https://arxiv.org/abs/1907.10902)ï¼Œ(2019)ã€‚

Â¹Â³ ä»¿å°„æ˜ å°„åªæ˜¯ç¥ç»ç½‘ç»œçº¿æ€§å±‚ä¸­ä½ ç†Ÿæ‚‰çš„ <math alttext="y equals upper A x plus b"><mrow><mi>y</mi> <mo>=</mo> <mi>A</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math> æ˜ å°„çš„ä¸€ä¸ªèŠ±å“¨çš„åå­—ã€‚

Â¹â´ è¿˜æœ‰ä¸€ä¸ªåä¸º ONNX-ML çš„æ ‡å‡†ï¼Œä¸“é—¨ä¸ºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚éšæœºæ£®æ—ï¼‰å’Œ Scikit-learn ç­‰æ¡†æ¶è®¾è®¡ã€‚

Â¹âµ å…¶ä»–æµè¡Œçš„åŠ é€Ÿå™¨åŒ…æ‹¬[NVIDIA çš„ TensorRT](https://oreil.ly/HnNZx)å’Œ[Apache TVM](https://oreil.ly/7KUyt)ã€‚

Â¹â¶ èåˆæ“ä½œæ¶‰åŠå°†ä¸€ä¸ªè¿ç®—ç¬¦ï¼ˆé€šå¸¸æ˜¯æ¿€æ´»å‡½æ•°ï¼‰åˆå¹¶åˆ°å¦ä¸€ä¸ªè¿ç®—ç¬¦ä¸­ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥ä¸€èµ·æ‰§è¡Œã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æƒ³å°†æ¿€æ´»å‡½æ•° *f* åº”ç”¨äºçŸ©é˜µä¹˜ç§¯ *A* Ã— *B*ã€‚é€šå¸¸ï¼Œä¹˜ç§¯çš„ç»“æœéœ€è¦å†™å›åˆ° GPU å­˜å‚¨å™¨ï¼Œç„¶åå†è®¡ç®—æ¿€æ´»å‡½æ•°ã€‚è¿ç®—ç¬¦èåˆå…è®¸æˆ‘ä»¬ä¸€æ­¥è®¡ç®— <math alttext="f left-parenthesis upper A times upper B right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>A</mi> <mo>Ã—</mo> <mi>B</mi> <mo>)</mo></mrow></math>ã€‚å¸¸é‡æŠ˜å æ˜¯æŒ‡åœ¨ç¼–è¯‘æ—¶è¯„ä¼°å¸¸é‡è¡¨è¾¾å¼ï¼Œè€Œä¸æ˜¯åœ¨è¿è¡Œæ—¶ã€‚

Â¹â· B. Hassibi and D. Storkï¼Œâ€œSecond Order Derivatives for Network Pruning: Optimal Brain Surgeon,â€ *Proceedings of the 5th International Conference on Neural Information Processing Systems* (November 1992): 164â€“171ï¼Œ[*https://papers.nips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html*](https://papers.nips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html)ã€‚

Â¹â¸ S. Han ç­‰äººï¼Œ[â€œLearning Both Weights and Connections for Efficient Neural Networksâ€](https://arxiv.org/abs/1506.02626)ï¼Œ(2015)ã€‚

Â¹â¹ M. Zhu and S. Guptaï¼Œ[â€œTo Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compressionâ€](https://arxiv.org/abs/1710.01878)ï¼Œ(2017)ã€‚

Â²â° V. Sanh, T. Wolf, and A.M. Rushï¼Œ[â€œMovement Pruning: Adaptive Sparsity by Fine-Tuningâ€](https://arxiv.org/abs/2005.07683)ï¼Œ(2020)ã€‚

Â²Â¹ è¿˜æœ‰ä¸€ç§â€œè½¯â€ç‰ˆæœ¬çš„ç§»åŠ¨ä¿®å‰ªï¼Œå…¶ä¸­ä¸æ˜¯é€‰æ‹©æƒé‡çš„å‰<math alttext="k"><mi>k</mi></math> %ï¼Œè€Œæ˜¯ä½¿ç”¨å…¨å±€é˜ˆå€¼<math alttext="tau"><mi>Ï„</mi></math>æ¥å®šä¹‰äºŒè¿›åˆ¶æ©ç ï¼š<math alttext="bold upper M equals left-parenthesis bold upper S greater-than tau right-parenthesis"><mrow><mi>ğŒ</mi> <mo>=</mo> <mo>(</mo> <mi>ğ’</mi> <mo>></mo> <mi>Ï„</mi> <mo>)</mo></mrow></math>ã€‚
