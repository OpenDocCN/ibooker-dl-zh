# 2 深度神经网络

本章节涵盖

+   分析神经网络和深度神经网络的架构

+   在训练过程中使用前向和反向传播来学习模型权重

+   在 TF.Keras 顺序和功能 API 中编码神经网络模型

+   理解各种模型任务类型

+   使用策略防止过拟合

本章节从神经网络的一些基础知识开始。一旦你掌握了基础知识，我将向你介绍如何使用 TF.Keras 轻松地编码深度神经网络（DNNs），TF.Keras 提供了两种编码神经网络的风格：顺序 API 和功能 API。我们将使用这两种风格来编写示例代码。

本章还涵盖了模型的基本类型。每种模型类型，如回归和分类，都学习不同类型的任务。你想要学习的任务决定了你将设计的模型类型。你还将学习权重、偏差、激活和优化器的根本原理，以及它们如何有助于提高模型的准确性。

为了总结本章内容，我们将编写一个图像分类器。最后，我将介绍在训练过程中出现的过拟合问题，以及使用 dropout 的早期解决过拟合的方法。

## 2 神经网络基础知识

让我们从神经网络的一些基础知识开始。首先，本节涵盖了神经网络的输入层，然后是如何将其连接到输出层，接着是如何在之间添加隐藏层以形成一个深度神经网络。从那里，我们将介绍层由节点组成，节点的作用，以及层如何相互连接以形成全连接神经网络。

### 2.1.1 输入层

神经网络输入层接受数字！所有输入数据都被转换为数字。一切都是数字。文本变成数字，语音变成数字，图片变成数字，已经是数字的东西仍然是数字。

神经网络以向量、矩阵或张量作为输入。这些只是表示数组中维度数量的名称。一个*向量*是一维数组，例如数字列表。一个*矩阵*是二维数组，例如黑白图像中的像素。而一个*张量*是三维或更多维度的数组——例如，矩阵堆叠，其中每个矩阵的维度相同。就是这样。图 2.1 展示了这些概念。

![](img/CH02_F01_Ferlitsch.png)

图 2.1 深度学习中数组的类型

说到数字，你可能听说过像*规范化*或*标准化*这样的术语。在标准化中，数字被转换成以零为中心，每侧的均值有一个标准差。如果你现在正在说，“我不做统计学”，我知道你的感受。但别担心。像 scikit-learn([`scikit-learn.org`](https://scikit-learn.org))和 NumPy([`numpy.org`](https://numpy.org))这样的包提供了库调用，为你完成这项工作。标准化基本上是一个可以按的按钮，甚至不需要杠杆，所以没有需要设置的参数。

说到包，你将大量使用 NumPy。NumPy 是什么，为什么它如此受欢迎？鉴于 Python 的解释性特性，该语言处理大型数组不佳——就像真正的大、超级大的数字数组——成千上万的、数以万计的、数以百万计的数字。想想卡尔·萨根关于宇宙大小的著名引言：亿万个星星。那是一个张量！

有一天，一个 C 程序员有了这样的想法，用低级的 C 编写一个处理超级大数组的性能实现，然后添加了一个外部 Python 包装器。NumPy 就这样诞生了。如今，NumPy 是一个包含许多有用方法和属性的库，如`shape`属性，它告诉你数组的形状（或维度），以及`where()`方法，它允许你在超级大数组上进行类似 SQL 的查询。

所有 Python 机器学习框架，如 TensorFlow 和 PyTorch，都会将 NumPy 多维数组作为输入层输入。至于 C、Java 或 C++等，神经网络中的输入层就像在编程语言中传递给函数的参数一样。就是这样。

让我们开始安装你需要的 Python 包。我假设你已经安装了 Python 3.*x*版本([www.python.org/downloads/](https://www.python.org/downloads/))。无论你是直接安装它，还是作为 Anaconda([www.anaconda.com/products/enterprise](https://www.anaconda.com/products/enterprise))等更大包的一部分安装它，你都会得到一个叫做`pip`的便捷命令行工具。这个工具用于安装你将来需要的任何 Python 包，只需一个命令调用。你使用`pip install`然后是包名。它会去 Python 包索引（PyPI），Python 包的全局存储库，为你下载并安装包。这相当简单。

我们想从下载和安装 TensorFlow 框架和 NumPy 包开始。猜猜看？它们的名称在注册表中是*tensorflow*和*numpy*——幸运的是，非常明显。让我们一起来做。打开命令行，输入以下命令：

```
pip install tensorflow
pip install numpy
```

在 TensorFlow 2.0 中，Keras 被内置为推荐的模型 API，现在被称为*TF.Keras*。TF.Keras 基于面向对象编程，包含一系列类及其相关的方法和属性。

让我们从简单开始。假设我们有一个房屋数据集。每一行有 14 个数据列。一列表示房屋的销售价格。我们将称之为*标签*。其他 13 列包含有关房屋的信息，如面积和财产税。这些都是数字。我们将称之为*特征*。我们想要做的是学习从特征中预测（或估计）标签。

现在，在我们拥有所有这些计算能力和这些令人惊叹的机器学习框架之前，数据分析师是通过手工或使用 Microsoft Excel 电子表格中的公式来处理这些事情的，这些电子表格包含一定量的数据和大量的线性代数。然而，我们将使用 Keras 和 TensorFlow。

我们将首先从 TensorFlow 导入 Keras 模块，然后实例化一个`Input`类对象。对于这个类对象，我们定义输入的形状或维度。在我们的例子中，输入是一个包含 13 个元素的向量（数组），每个元素对应一个特征：

```
from tensorflow.keras import Input

Input(shape=(13,))
```

当你在笔记本中运行这两行代码时，你会看到这个输出：

```
<tf.Tensor 'input_1:0' shape=(?, 13) dtype=float32>
```

这个输出显示了`Input(shape=(13,))`的评估结果。它产生了一个名为`input_1:0`的张量对象。这个名称将在稍后帮助你调试模型时很有用。`shape`中的`?`表示输入对象可以接受任意数量的条目（示例或行），每个条目包含 13 个元素。也就是说，在运行时，它将绑定 13 个元素的向量数量到实际传递的示例（行）数量，这被称为（迷你）*批量大小*。`dtype`显示了元素的默认数据类型，在这种情况下是一个 32 位的浮点数（单精度）。

### 2.1.2 深度神经网络

DeepMind, 深度学习，深，深，深。哦，这一切都是什么意思？在这里，“深”的意思是指神经网络在输入层和输出层之间有一层或多层。正如你稍后将要读到的，通过深入隐藏层，研究人员已经能够获得更高的准确率。

将有向图可视化成深度层。根节点是输入层，终端节点是输出层。中间的层被称为*隐藏层*或*深层层*。所以一个四层的深度神经网络（DNN）架构看起来会是这样：

+   输入层

+   隐藏层

+   隐藏层

+   输出层

要开始，我们将假设除了输出层之外，每一层的每个神经网络节点都是相同类型的神经网络节点。我们还将假设每一层的每个节点都与下一层的每个节点相连。这被称为*全连接神经网络*（FCNN），如图 2.2 所示。例如，如果输入层有三个节点，下一个（隐藏）层有四个节点，那么第一层的每个节点都与下一层的所有四个节点相连——总共 12 个连接（3 × 4）。

![](img/CH02_F02_Ferlitsch.png)

图 2.2 深度神经网络在输入层和输出层之间有一个或多个隐藏层。这是一个全连接网络，因此每个层的节点都相互连接。

### 2.1.3 前馈网络

DNN 和 CNN（你将在第三章中了解更多关于 CNN 的内容）被称为*前馈神经网络*。*前馈*意味着数据按顺序通过网络，单向地从输入层流向输出层。这与过程式编程中的函数类似。输入作为参数在输入层中传递，函数根据输入（在隐藏层中）执行一系列按顺序的动作，然后输出结果（输出层）。

当在 TF.Keras 中编码前馈网络时，你会在博客和其他教程中看到两种不同的风格。我将简要介绍这两种风格，这样当你看到一种风格的代码片段时，你可以将其翻译成另一种风格。

### 2.1.4 顺序 API 方法

*顺序 API 方法*对于初学者来说更容易阅读和遵循，但代价是它的灵活性较低。本质上，你使用`Sequential`类对象创建一个空的 feed-forward 神经网络，然后逐个“添加”一层，直到输出层。在以下示例中，省略号代表伪代码：

```
from tensorflow.keras import Sequential

model = Sequential()                    ❶
model.add( ...the first layer... )      ❷
model.add( ...the next layer... )       ❷
model.add( ...the output layer... )     ❷
```

❶ 创建一个空模型

❷ 按顺序添加层的占位符

或者，可以在实例化`Sequential`类对象时，以列表的形式按顺序指定层，作为参数传递：

```
model = Sequential([ ...the first layer...,
                     ...the next layer...,
                     ...the output layer...
                   ])
```

因此，你可能会问，何时使用`add()`方法与在`Sequential`对象实例化时指定列表？嗯，两种方法生成相同的模型和行为，所以这是一个个人偏好的问题。我倾向于在教程和演示材料中使用更详尽的`add()`方法以提高清晰度。但如果是编写生产代码，我使用更简洁的列表方法，这样我可以更容易地可视化和编辑代码。

### 2.1.5 功能 API 方法

*功能 API 方法*更为高级，允许你构建非顺序流模型——例如分支、跳转链接、多个输入和输出（你将在第 2.4 节中看到多个输入和输出的工作方式）。你分别构建层，然后将它们连接起来。这一步骤给你提供了以创新方式连接层的自由。本质上，对于前馈神经网络，你创建层，将它们绑定到另一个或多个层，然后在`Model`类对象的最终实例化中拉取所有层。

```
input = layers.(...the first layer...)                                 ❶
hidden = layers.(...the next layer...)( ...the layer to bind to... )   ❷
output = layers.(...the output layer...)( /the layer to bind to... )   ❸
model = Model(input, output)                                           ❹
```

❶ 构建输入层

❷ 构建隐藏层并将其绑定到输入层

❸ 构建输出层并将其绑定到隐藏层

❹ 按照从输入层到输出层的绑定组装模型

### 2.1.6 输入形状与输入层

输入 *形状* 和输入 *层* 在一开始可能会让人困惑。它们不是同一件事。更具体地说，输入层中的节点数量不需要与输入向量的形状相匹配。这是因为输入向量中的每个元素都将传递到输入层中的每个节点，如图 2.3 所示。

![图片](img/CH02_F03_Ferlitsch.png)

图 2.3 输入（形状）和输入层不同。输入中的每个元素都与输入层中的每个节点相连。

例如，如果我们的输入层有 10 个节点，并且我们使用之前提到的 13 元素输入向量作为例子，我们将在输入向量和输入层之间有 130 个连接（10 × 13）。

输入向量中的每个元素与输入层中的每个节点之间的每个连接都有一个 *权重*，输入层中的每个节点都有一个 *偏差*。将输入向量与输入层之间的每个连接以及层之间的连接视为发送一个信号，表示它对输入值将如何对模型的预测做出贡献的强烈信念。我们需要有一个测量这个信号强度的指标，这就是权重的作用。它是一个系数，用于乘以输入层的输入值和后续层的先前值。

现在，这些连接中的每一个都像是在 x-y 平面上的一个向量。理想情况下，我们希望这些向量在 y 轴上的同一个中心点（例如，0 原点）相交。但它们并没有。为了使这些向量相互关联，偏差是每个向量从 y 轴中心点的偏移量。

权重和偏差是神经网络在训练过程中将“学习”的内容。权重和偏差也被称为 *参数*。这些值在模型训练后将与模型一起保留。否则，这个操作对你来说是不可见的。

### 2.1.7 密集层

在 TF.Keras 中，FCNN（全连接神经网络）的层被称为 *密集层*。一个密集层有 *n* 个节点，并且与前一层完全连接。

让我们继续，通过在 TF.Keras 中定义一个三层神经网络，使用 sequential API 方法，来构建我们的例子。我们的输入层有 10 个节点，接受一个 13 元素向量（13 个特征）作为输入，该向量连接到第二个（隐藏）层，该层有 10 个节点，然后连接到第三个（输出）层，该层只有一个节点。由于输出层将输出单个实数值（例如，房屋的预测价格），因此输出层只需要一个节点。在这个例子中，我们将使用神经网络作为 *回归器*，这意味着神经网络将输出一个单一的实数：

输入层 = 10 个节点

隐藏层 = 10 个节点

输出层 = 1 个节点

对于输入层和隐藏层，我们可以选择任意数量的节点。节点越多，神经网络的学习能力越强。但节点越多也意味着复杂性增加，训练和预测所需的时间也越长。

在下面的代码示例中，我们对 `Dense` 类对象进行了三次 `add()` 调用。`add()` 方法以我们指定的相同顺序添加层。第一个（位置）参数是节点的数量，第一层和第二层是 10 个，第三层是 1 个。请注意，在第一个 `Dense` 层中，我们添加了（关键字）参数 `input_shape`。这就是我们定义输入向量并将其连接到 `Dense` 层的第一个（输入）层的地方：

```
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(10, input_shape=(13,)))    ❶
model.add(Dense(10))                       ❷
model.add(Dense(1))                        ❸
```

❶ 在顺序模型中，第一层需要 `input_shape` 参数。

❷ 构建隐藏层

❸ 将输出层构建为一个回归器——单个节点

或者，我们可以在实例化 `Sequential` 类对象时，将层的顺序序列定义为列表参数：

```
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
                   Dense(10, input_shape=(13,)),    ❶
                   Dense(10),                       ❶
                   Dense(1)                         ❶
                   ])
```

❶ 层以顺序列表的形式指定。

让我们现在做同样的事情，但使用功能 API 方法。首先，我们通过实例化一个 `Input` 类对象来创建一个输入向量。`Input` 对象的（位置）参数是输入的形状，它可以是一个向量、矩阵或张量。在我们的例子中，我们有一个 13 个元素的向量。因此，我们的形状是 `(13,)`。我确信你已经注意到了尾随的逗号。这是为了克服 Python 中的一个怪癖。如果没有逗号，`(13)` 将被评估为一个表达式：一个由括号包围的整数值 13。添加一个逗号告诉解释器这是一个 *元组*（一个有序值集）。

接下来，我们通过实例化一个 `Dense` 类对象来创建输入层。`Dense` 对象的位置参数是节点的数量，在我们的例子中是 10 个。注意以下特殊的语法：`(inputs)`。`Dense` 对象是一个可调用的对象；通过实例化 `Dense` 返回的对象可以作为函数调用。因此，我们将其作为函数调用，在这种情况下，该函数将输入向量（或层输出）作为（位置）参数来连接它；因此我们传递给它 `inputs`，这样输入向量就被绑定到 10 节点的输入层。

接下来，我们通过实例化另一个具有 10 个节点的 `Dense` 对象来创建隐藏层。使用它作为可调用的对象，我们将它（完全）连接到输入层。

然后我们通过实例化另一个具有一个节点的 `Dense` 对象来创建输出层。使用它作为可调用的对象，我们将它（完全）连接到隐藏层。

最后，我们通过实例化一个 `Model` 类对象，传递输入向量和输出层的（位置）参数来将这些全部组合起来。记住，所有其他层之间已经连接，因此在实例化 `Model()` 对象时我们不需要指定它们：

```
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense

inputs = Input((13,))           ❶
input = Dense(10)(inputs)       ❷
hidden = Dense(10)(input)       ❸
output = Dense(1)(hidden)       ❹
model = Model(inputs, output)   ❺
```

❶ 构建输入向量（13 个元素）

❷ 构建第一个（输入）层（10 个节点）并将其连接到输入向量

❸ 构建下一个（隐藏）层（10 个节点）并将其连接到输入层

❹ 构建输出层（1 个节点）并将其连接到前一个（隐藏）层

❺ 构建神经网络，指定输入和输出层

### 2.1.8 激活函数

在训练或预测（通过推理）时，层中的每个节点都会向下一层的节点输出一个值。我们不想原样传递值，而有时希望以特定方式更改值。这个过程称为*激活函数*。

想象一个返回结果的函数，比如`return result`。在激活函数的情况下，我们不会返回`result`，而是会返回将结果值传递到另一个（激活）函数的结果，例如`return A(result``)`，其中`A()`是激活函数。从概念上讲，你可以这样想：

```
def layer(params):
    """ inside are the nodes """
    result = some_calculations
    return A(result)

def A(result):
    """ modifies the result """
    return some_modified_value_of_result
```

激活函数帮助神经网络更快、更好地学习。默认情况下，当未指定激活函数时，将一层中的值原样（未改变）传递到下一层。最基本的激活函数是*阶跃函数*。如果值大于 0，则输出 1；否则，输出 0。阶跃函数已经很久没有使用了。

让我们暂停一下，讨论激活函数的目的。你可能已经听说过*非线性*这个术语。这是什么？对我来说，更重要的是，它不是什么？

在传统统计学中，我们在低维空间中工作，输入和输出之间存在强烈的线性相关性。这种相关性可以通过输入的多项式变换来计算，当变换后，与输出有线性相关性。最基本的一个例子是直线的斜率，表示为*y = mx + b*。在这种情况下，*x*和*y*是直线的坐标，我们想要拟合*m*的值，即斜率，以及*b*，即直线与 y 轴的交点。

在深度学习中，我们在高维空间中工作，输入和输出之间存在显著的非线性。这种非线性意味着输入不是基于输入的多项式变换均匀地与输出相关（不接近）。例如，假设财产税是房屋价值的固定百分比率（*r*）。财产税可以用一个函数表示，该函数将税率乘以房屋价值。因此，我们有一个线性（直线）的关系，即价值（输入）与财产税（输出）之间的关系：

tax = *f* (*value*) = *r* × *value*

让我们看看用于测量地震的对数刻度，其中增加 1 表示释放的能量是 10 倍。例如，4 级地震比 3 级地震强 10 倍。通过对输入功率应用对数变换，我们得到功率和刻度之间的线性关系：

scale = *f* (*power*) = log(*power*)

在非线性关系中，输入序列中的序列与输出有不同的线性关系，在深度学习中，我们希望学习到每个输入序列的分离点以及线性函数。例如，考虑年龄与收入的关系来展示非线性关系。一般来说，幼儿没有收入，小学生有零花钱，青少年有零花钱加上家务钱，稍大一点的青少年通过工作赚钱，然后当他们上大学时，他们的收入降到零！大学毕业后，他们的收入逐渐增加，直到退休，这时收入变得固定。我们可以将这种非线性建模为年龄序列，并学习每个序列的线性函数，如图所示：

| 收入 = F1(年龄) = 0 | 对于年龄 [0..5] |
| --- | --- |
| 收入 = F2(年龄) = c1 | 对于年龄 [6..9] |
| 收入 = F3(年龄) = c1 + (w1 × 年龄) | 对于年龄 [10..15] |
| 收入 = F4(年龄) = (w2 × 年龄) | 对于年龄 [16..18] |
| 收入 = F5(年龄) = 0 | 对于年龄 [19..22] |
| 收入 = F6(年龄) = (w3 × 年龄) | 对于年龄 [23..64] |
| 收入 = F7(年龄) = c2 | 对于年龄 [65+] |

激活函数有助于在输入序列中找到非线性分离以及节点的对应聚类，然后学习到与输出的（近）线性关系。大多数情况下，你会使用三种激活函数：修正线性单元（ReLU）、Sigmoid 和 Softmax。

我们将从 ReLU 开始，因为它是除了输出层之外所有模型中最常用的。Sigmoid 和 Softmax 激活将在第 2.2 节和第 2.3 节中介绍。ReLU 如图 2.4 所示，将大于零的值原样通过（不变）；否则，它通过零（无信号）。

![图像](img/CH02_F04_Ferlitsch.png)

图 2.4 矩形线性单元函数将所有负值裁剪为零。本质上，任何负值都等同于无信号，或~零。

ReLU 通常用于层之间。虽然早期研究人员在层之间使用了不同的激活函数（例如双曲正切函数），但研究人员发现 ReLU 在训练模型时产生了最佳结果。在我们的例子中，我们将在每一层之间添加一个 ReLU：

```
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, ReLU

model = Sequential()
model.add(Dense(10, input_shape=(13,)))
model.add(ReLU())                        ❶
model.add(Dense(10))
model.add(ReLU())                        ❶
model.add(Dense(1))
```

❶ 习惯上，将 ReLU 激活添加到每个非输出层

让我们来看看我们的模型对象内部，看看我们是否构建了我们认为的那样。你可以通过使用`summary()`方法来完成这个操作。这个方法对于可视化你构建的层以及验证你打算构建的内容是否实际构建了非常有用。它将按顺序显示每个层的摘要：

```
model.summary()
Layer (type)                 Output Shape              Param #   
=================================================================
dense_56 (Dense)             (None, 10)                140       
___________________________________________________________________
re_lu_18 (ReLU)              (None, 10)                0         
_________________________________________________________________
dense_57 (Dense)             (None, 10)                110       
_________________________________________________________________
re_lu_19 (ReLU)              (None, 10)                0         
_________________________________________________________________
dense_58 (Dense)             (None, 1)                 11        
=================================================================
Total params: 261
Trainable params: 261
Non-trainable params: 0
_________________________________________________________________
```

对于这个代码示例，你可以看到总结从 10 个节点的密集层（输入层）开始，接着是一个 ReLU 激活函数，然后是一个 10 个节点的第二个密集层（隐藏层），接着是一个 ReLU 激活函数，最后是一个 1 个节点的密集层（输出层）。所以，是的，我们得到了我们预期的结果。

接下来，让我们看看摘要中的参数字段。输入层显示 140 个参数。这是如何计算的？我们有 13 个输入和 10 个节点，所以 13 × 10 是 130。140 是从哪里来的？输入和每个节点之间的每个连接都有一个权重，这些权重加起来是 130。但是每个节点还有一个额外的偏差。这是 10 个节点，所以 130 + 10 = 140。正如我所说的，这是神经网络在训练期间将“学习”的权重和偏差。*偏差*是一个学习到的偏移量，在概念上等同于线的斜率中的 y 截距（*b*），这是线与 y 轴相交的地方：

*y* = *b* + *mx*

在下一个（隐藏）层，您可以看到 110 个参数。这是从输入层连接到隐藏层每个节点的 10 个输出（10 × 10）加上隐藏层中节点的 10 个偏差，总共 110 个参数需要学习。

### 2.1.9 简写语法

TF.Keras 在指定层时提供了简写语法。实际上，您不需要像上一个例子那样在层之间单独指定激活函数。相反，您可以在实例化`Dense`层时将激活函数指定为一个（关键字）参数。

您可能会问，为什么不总是使用简写语法？正如您将在第三章中看到的，在今天的模型架构中，激活函数位于另一个中间层（批归一化）之前，或者根本位于层之前（预激活批归一化）。以下代码示例与上一个示例完全相同：

```
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(10, input_shape=(13,), activation='relu'))     ❶
model.add(Dense(10, activation='relu'))                        ❶
model.add(Dense(1))
```

❶ 激活函数在层中指定为关键字参数。

让我们调用这个模型的`summary()`方法：

```
model.summary()
Layer (type)                Output Shape              Param #   
=================================================================
dense_1 (Dense)             (None, 10)                140       
_________________________________________________________________
dense_2 (Dense)             (None, 10)                110       
_________________________________________________________________
dense_3 (Dense)             (None, 1)                 11        
=================================================================
Total params: 261
Trainable params: 261
Non-trainable params: 0
_________________________________________________________________
```

嗯，您没有看到层之间的激活，就像在之前的例子中那样。为什么？这是因为`summary()`方法显示输出的方式有点奇怪。它们仍然存在。

### 2.1.10 使用优化器提高准确度

一旦您完成了神经网络前馈部分的构建，就像我们在简单示例中所做的那样，您需要添加一些东西来训练模型。这是通过`compile()`方法完成的。这一步在训练期间添加了*反向传播*。让我们定义并探讨这个概念。

每次我们将数据（或数据批次）通过神经网络向前传递时，它都会计算预测结果中的误差（称为*损失*），这些误差与实际值（称为*标签*）进行比较，并使用这些信息来逐步调整节点的权重和偏差。对于模型来说，这个过程就是学习的过程。

如我所说，误差的计算称为**损失**。它可以以多种方式计算。由于我们设计的示例神经网络是一个**回归器**（意味着输出，房价，是一个实数值），我们希望使用最适合回归器的损失函数。通常，对于这种类型的神经网络，我们使用**均方误差**方法来计算损失。在 Keras 中，`compile()`方法接受一个（关键字）参数`loss`，用于指定我们想要如何计算损失。我们将传递给它`mse`（代表**均方误差**）的值。

过程中的下一步是使用在反向传播过程中出现的优化器来最小化损失。优化器基于**梯度下降**；可以选择不同的**梯度下降**算法变体。这些术语一开始可能难以理解。本质上，每次我们通过神经网络传递数据时，我们都会使用计算出的损失来决定如何改变层中的权重和偏置。目标是逐渐接近权重和偏置的正确值，以准确预测或估计每个示例的标签。这个过程被称为**收敛**。优化器的任务是计算权重的更新，以逐渐接近正确的值并达到收敛。

随着损失的逐渐降低，我们正在**收敛**。当损失达到平台期后，我们实现了**收敛**。结果是神经网络的准确率。在梯度下降之前，早期 AI 研究人员使用的方法可能需要超级计算机数年才能在非平凡问题上找到收敛。梯度下降算法的发现之后，这个时间缩短到了几天、几小时，甚至在普通计算能力上只需几分钟。让我们跳过数学，只说梯度下降是数据科学家的一种魔法，它使得在良好的局部最优解上收敛成为可能。

对于我们的回归器神经网络，我们将使用`rmsprop`方法（**均方根属性**）：

```
model.compile(loss='mse', optimizer='rmsprop')
```

现在我们已经完成了您第一个可训练神经网络的构建。在开始准备数据和训练模型之前，我们将介绍更多关于神经网络的设计。这些设计使用了之前提到的两种激活函数：sigmoid 和 softmax。

## 2.2 DNN 二分类器

DNN 的另一种形式是**二分类器**，也称为**逻辑分类器**。当我们使用二分类器时，我们希望神经网络预测输入是否是某物。输出可以有两种状态或类别：是/否、真/假、0/1 等等。

例如，假设我们有一个信用卡交易数据集，每个交易都被标记为欺诈或非欺诈。记住，标签是我们想要预测的内容。

总体而言，我们迄今为止学到的设计方法没有改变，除了单节点输出层的激活函数以及损失/优化器方法。与回归器不同，我们将在输出节点上使用一个**sigmoid**激活函数，而不是**线性**激活函数。sigmoid 函数将所有值压缩到 0 和 1 之间，如图 2.5 所示。随着值远离中心，它们会迅速移动到 0 和 1 的极端（渐近线）。

![图像](img/CH02_F05_Ferlitsch.png)

图 2.5 sigmoid 函数

我们现在将使用我们讨论过的两种风格来编写这段代码。让我们从之前的代码示例开始，其中我们将激活函数指定为一个（关键字）参数。在这个例子中，我们在输出`Dense`层中添加了参数`activation= 'sigmoid'`，以便将最终节点的输出结果通过 sigmoid 函数传递。

接下来，我们将我们的损失参数更改为`binary_crossentropy`。这是在二分类器中通常使用的损失函数：

```
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(10, input_shape=(13,), activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))      ❶

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])            ❷
```

❶ Sigmoid 函数用于二分类。

❷ 二分类器中损失函数和优化器的常见约定

并非所有激活函数都有自己的类，例如`ReLU`。这是 TF.Keras 框架中的另一个特性。相反，一个名为`Activation`的类可以创建任何受支持的激活函数。参数是预定义的激活函数名称。在我们的例子中，`relu`代表修正线性单元，而`sigmoid`代表 sigmoid。以下代码与前面的代码执行相同的操作：

```
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Activation

model = Sequential()
model.add(Dense(10, input_shape=(13,)))
model.add(Activation('relu'))     ❶
model.add(Dense(10))
model.add(Activation('relu'))     ❶
model.add(Dense(1))
model.add(Activation('sigmoid')   ❶

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
```

❶ 可以使用 Activation()方法指定激活函数。

现在，我们将使用功能 API 方法重写相同的代码。请注意，我们反复使用了变量`x`。这是一个常见的做法。我们希望避免创建大量的一次性使用变量。由于我们知道在这种类型的神经网络中，除了输入和输出之外，每一层的输出都是下一层的输入（或激活），因此我们持续使用`x`作为连接变量。

到现在为止，你应该开始熟悉这两种方法：

```
from tensorflow.keras import Model, Input
from tensorflow.keras.layers import Dense, ReLU, Activation

inputs = Input((13,))
x = Dense(10)(inputs)
x = Activation('relu')(x)              ❶
x = Dense(10)(x)
x = Activation('relu')(x)              ❶
x = Dense(1)(x)
output = Activation('sigmoid')(x)      ❶
model = Model(inputs, output)

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
```

❶ 使用功能 API 指定的激活函数

## 2.3 DNN 多分类器

假设我们有一组身体测量值（例如身高和体重）以及与每组测量值相关的性别，我们想要预测某人是否是婴儿、幼儿、儿童、青少年或成人。我们希望我们的模型能够从多个类别或标签中进行分类或预测——在这个例子中，我们有总共五个年龄类别的类别。为此，我们可以使用 DNN 的另一种形式，称为**多分类器**。

我们已经可以看到我们将会有一些复杂性。例如，成年男性平均身高比女性高。但在青春期前，女孩往往比男孩高。我们知道男性在成年早期比在青少年时期体重增加得更多，但女性平均来说不太可能增重。因此，我们应该预料到在青春期前预测女孩、青春期预测男孩和成年预测女性时会出现问题。

这些问题是**非线性**的例子；特征与预测之间的关系不是线性的。相反，这种关系可以被分解为不连续的线性段。这正是神经网络擅长的类型的问题。

让我们增加一个第四个测量值，即鼻子的表面积。例如，《整形外科学年鉴》*（Annals of Plastic Surgery）*的研究([`pubmed.ncbi.nlm.nih.gov/3579170/`](https://pubmed.ncbi.nlm.nih.gov/3579170/))表明，对于女孩和男孩来说，鼻子的表面积从 6 岁到 18 岁持续增长，并在 18 岁时基本停止增长。

因此，现在我们有四个特征，一个由五个类别组成的标签。在下一个例子中，我们将改变我们的输入向量到 4，以匹配特征的数量，并将我们的输出层改变为 5 个节点，以匹配类别的数量。在这种情况下，每个输出节点对应一个独特的类别（婴儿、幼儿等等）。我们希望训练神经网络，使每个输出节点输出一个介于 0 到 1 之间的值作为预测。例如，0.75 意味着该节点有 75%的信心认为预测是相应的类别。

每个输出节点将独立学习和预测其对应类别的置信度。然而，这个过程导致了一个问题：因为值是独立的，它们不会加起来等于 1（100%）。这就是 softmax 函数有用的地方。这个数学函数将一组值（输出层的输出）压缩到 0 到 1 的范围内，同时确保所有值加起来等于 1。完美。这样，我们可以选择具有最高值的输出节点，并说出预测的内容以及该预测的置信度。所以如果最高值是 0.97，我们可以说我们在预测中估计的置信度为 97%。

图 2.6 是一个多类模型的图示。在这个例子中，输出层有两个节点，每个节点对应预测一个不同的类别。每个节点独立地预测它对输入属于相应类别的信念强度。这两个独立的预测随后通过 softmax 激活函数，将值压缩到总和为 1（100%）。在这个例子中，一个类别以 97%的置信度被预测，另一个以 3%的置信度被预测。

![图片](img/CH02_F06_Ferlitsch.png)

图 2.6 为多类分类器的输出层添加 softmax 激活有助于提高模型预测的置信度。

以下代码展示了构建多类分类器 DNN 的示例。我们首先设置我们的输入层和输出层，分别使用多个特征和多个类别。然后我们将激活函数从`sigmoid`改为`softmax`。接下来，我们将损失函数设置为`categorical_crossentropy`。这通常是最推荐用于多类分类的。我们不会深入探讨交叉熵背后的统计学原理，除了交叉熵计算多个概率分布的损失。在二元分类器中，我们有两个概率分布并使用`binary_crossentropy`计算；而在多类分类器中，我们使用`categorical_crossentropy`来计算多个（多于两个）概率分布的损失。

最后，我们将使用一种流行且广泛使用的梯度下降变体，称为*Adam 优化器*（`adam`）。*Adam*结合了其他方法的一些方面，如`rmsprop`（*均方根*）和`adagrad`（*自适应梯度*），以及自适应学习率。它通常被认为是适用于各种神经网络的最佳优化器：

```
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(10, input_shape=(4,), activation='relu'))    ❶
model.add(Dense(10, activation='relu'))
model.add(Dense(5, activation='softmax'))                    ❷

model.compile(loss='categorical_crossentropy', 
        optimizer='adam', 
        metrics=['accuracy'])                                ❸
```

❶ 输入层为四个特征的 1D 向量

❷ 在输出层，多类分类器使用 softmax 激活函数。

❸ 多类分类器损失函数和优化器的常用约定

## 2.4 DNN 多标签多类分类器

现在，让我们看看预测每个输入的两个或更多类别（标签）。让我们使用我们之前的例子，预测某人是否是婴儿、幼儿、儿童、青少年或成人。这次，我们将从特征中移除性别，并将其作为要预测的一个标签。我们的输入将是身高、体重和鼻表面面积，我们的输出将是两个类别：年龄类别（婴儿、幼儿等）和性别（男性或女性）。一个预测示例可能看起来像这样：

[身高，体重，鼻表面面积] -> 神经网络 -> [儿童，女性]

为了从多个输入中预测两个或更多标签，正如我们在这里所做的那样，我们使用——你猜对了——一个*多标签多类分类器*。为此，我们需要对我们之前的多类分类器做一些修改。在我们的输出层，我们的输出类别数是所有输出类别的总和。在这种情况下，我们之前有五个，现在我们再增加两个用于性别，总共七个。我们还希望将每个输出类别视为二元分类器，这意味着我们希望得到一个是/否类型的答案，因此我们将激活函数改为`sigmoid`。对于我们的编译语句，我们模仿了本章中更简单的 DNN 所做的事情，将损失函数设置为`binary_crossentropy`，并将优化器设置为`rmsprop`。你可以在这里看到每个步骤的实现：

```
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(10, input_shape=(3,), activation='relu'))    ❶
model.add(Dense(10, activation='relu'))
model.add(Dense(7, activation='sigmoid'))                    ❷

model.compile(loss='binary_crossentropy',                    ❸
        optimizer='rmsprop', 
        metrics=['accuracy'])
```

❶ 输入向量仅仅是身高、体重和鼻表面面积

❷ 将年龄人口统计和性别类别合并为一个分类器输出

❸ 使用 sigmoid 激活函数和二元交叉熵损失函数来独立预测每个类别为 0 或 1，或者接近 0 或 1。

你看到这个设计可能存在潜在问题吗？让我们假设我们输出两个具有最高值（从 0 到 1）的类别（标签），即最自信的预测。如果在一个预测中，神经网络以高置信度预测了青少年和青少年，以及男性和女性以较低置信度，会怎样呢？嗯，我们可以通过一些后逻辑来修复这个问题，通过从前五个输出类别（年龄人口统计）中选择最高置信度，并从最后两个类别（性别）中选择最高置信度。换句话说，我们将七个输出类别分为两个相应的类别，并从每个类别中选择置信度最高的输出。

功能 API 使我们能够在不添加任何后逻辑的情况下修复这个问题。在这种情况下，我们想要用*两个并行输出层*替换结合两个类别集的输出层，一个用于第一组类别（年龄类别），另一个用于第二组类别（性别）。您可以在图 2.7 中看到这个设置。

![图片](img/CH02_F07_Ferlitsch.png)

图 2.7 多标签多类分类器的两个并行输出层

在下面的代码示例中，只有最终输出层与之前的代码列表不同。在这里，我们不是只有一个输出层，而是有两个并行层。

然后当我们使用`Model`类将所有内容组合在一起时，我们传递的是一个输出层的列表：`[output1, output2]`。最后，由于每个输出层都做出独立的预测，我们可以将它们作为*多类分类器*来处理——这意味着我们返回使用`categorical_crossentropy`作为损失函数和`adam`作为优化器。

这种多标签多类分类器的设计也可以称为*具有多个输出的神经网络*，其中每个输出学习不同的任务。由于我们将训练这个模型进行多个独立的预测，这也被称为*多任务模型*：

```
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense

inputs = Input((3,))
x = Dense(10, activation='relu')(inputs)
x = Dense(10, activation='relu')(x)
output1 = Dense(5, activation='softmax')(x)     ❶
output2 = Dense(2, activation='softmax')(x)     ❶
model = Model(inputs, [output1, output2])

model.compile(loss='categorical_crossentropy', 
        optimizer='adam', 
        metrics=['accuracy'])
```

❶ 每个类别都有一个独立的输出层，并得到相同输入的副本。

那么哪种设计对于多标签多类分类器是正确的（或更好的）？这取决于应用。如果所有类别都属于单个类别——例如年龄人口统计——使用第一个模式，即单一任务。如果类别来自不同的类别——例如年龄人口统计和性别——使用第二个模式，即多任务。在这个例子中，我们使用多任务模式，因为我们想要学习两个类别作为输出。

## 2.5 简单图像分类器

你现在已经看到了 DNN 的基本类型以及如何使用 TF.Keras 来编码它们。所以现在让我们构建我们的第一个简单的图像分类模型。

在计算机视觉中，神经网络被用于图像分类。让我们从基础知识开始。对于小灰度图像，如图 2.8 所示，我们可以使用一个类似于我们之前描述的多类分类器的深度神经网络来预测年龄人口统计。这种类型的 DNN 已经在使用修改后的国家标准与技术研究院（MNIST）数据集的文献中广泛发表，这是一个用于识别手写数字的数据集。该数据集由 28 × 28 像素大小的灰度图像组成。每个像素由一个从 0 到 255 的整数值表示（0 是黑色，255 是白色，中间的值是灰色）。

![](img/CH02_F08_Ferlitsch.png)

图 2.8 灰度图像的矩阵表示

尽管如此，我们需要进行一个修改。灰度图像是一个*矩阵*（二维数组）。将矩阵想象成一个网格，大小为高度 × 宽度，其中宽度代表列，高度代表行。然而，深度神经网络将*向量*作为输入，这是一个一维数组。那么我们能做什么呢？我们可以将二维矩阵展平成一维向量。

### 2.5.1 展平

我们将通过将每个像素视为一个特征来进行分类。以 MNIST 数据集为例，28 × 28 的图像将有 784 个像素，因此有 784 个特征。我们通过展平将矩阵（二维）转换为向量（一维）。

*展平*是将每一行按顺序放入向量的过程。因此，向量从像素的第一行开始，然后是第二行像素，以此类推，最后以最后一行像素结束。图 2.9 展示了将矩阵展平成向量的过程。

![](img/CH02_F09_Ferlitsch.png)

图 2.9 将矩阵展平成向量

你可能会在这个时候问，为什么我们需要将二维矩阵展平成一维向量？这是因为在一个深度神经网络中，密集层的输入必须是一维向量。在下一章中，当我们介绍卷积神经网络（CNN）时，你会看到一些使用二维输入的卷积层示例。

在下一个示例中，我们在神经网络的开头添加一个层来展平输入，使用`Flatten`类。剩余的层和激活对于 MNIST 数据集来说是典型的。请注意，`Flatten`对象的输入形状是二维形状`(28, 28)`。该对象输出的形状将是一维的`(784,)`：

```
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten, ReLU, Activation

model = Sequential()
model.add(Flatten(input_shape=(28,28)))      ❶
model.add(Dense(512, activation='relu'))     ❷
model.add(Dense(512, activation='relu'))     ❷
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])
```

❶ 将二维灰度图像展平成一维向量作为 DNN 的输入。

❷ MNIST 通常作为输入，并在 128、256 和 512 个节点之间有一个隐藏的密集层。

现在我们通过使用`summary()`方法来查看层。正如你所见，摘要中的第一层是展平层，显示该层的输出是 784 个节点。这正是我们想要的。同时注意网络在训练过程中需要学习的参数数量，接近 70 万个：

```
model.summary()
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 784)               0         
_________________________________________________________________
dense_69 (Dense)             (None, 512)               401920    
_________________________________________________________________
re_lu_20 (ReLU)              (None, 512)               0         
_________________________________________________________________
dense_70 (Dense)             (None, 512)               262656    
_________________________________________________________________
re_lu_21 (ReLU)              (None, 512)               0         
_________________________________________________________________
dense_71 (Dense)             (None, 10)                5130      
_________________________________________________________________
activation_10 (Activation)   (None, 10)                0         
=================================================================
Total params: 669,706
Trainable params: 669,706
Non-trainable params: 0
_________________________________________________________________
```

### 2.5.2 过拟合和 dropout

在训练过程中，数据集被分为训练数据和测试数据（也称为*保留数据*）。在神经网络的训练过程中只使用训练数据。一旦神经网络达到收敛*，*，我们在第四章中详细讨论了这一点，训练就会停止，如图 2.10 所示。

![图片](img/CH02_F10_Ferlitsch.png)

图 2.10 收敛发生在损失曲线平缓时。

之后，为了获得模型在训练数据上的准确率，训练数据再次正向传递，但不启用反向传播，因此没有学习。这也被称为在训练好的神经网络中运行*推理*或*预测模式*。在训练/测试分割中，之前保留并未作为训练一部分的测试数据再次正向传递，不启用反向传播，以获得准确率。

为什么我们要将测试数据从训练数据中分割出来并保留？理想情况下，训练数据和测试数据的准确率将几乎相同。实际上，测试数据的准确率总是略低。这有一个原因。

一旦达到收敛，持续将训练数据通过神经网络，会导致神经元越来越多地记住训练样本，而不是泛化到训练过程中从未见过的样本。这被称为*过拟合*。当神经网络对训练数据过拟合时，你将获得很高的训练准确率，但在测试/评估数据上的准确率会显著降低。

即使没有训练到收敛，你也会有一些过拟合。数据集/问题可能存在非线性（这就是为什么你使用神经网络）。因此，单个神经元将以不同的速率收敛。在测量收敛时，你是在看整个系统。在此之前，一些神经元已经收敛，而持续的训练将导致它们过拟合。这就是为什么测试/评估准确率总是至少略低于训练数据的准确率。

为了解决训练神经网络时的过拟合问题，我们可以使用*正则化*。这会在训练过程中添加少量的随机噪声，以防止模型记住样本，并在模型训练后更好地泛化到未见过的样本。

最基本的正则化类型被称为*dropout*。Dropout 就像遗忘。当我们教小孩子时，我们使用死记硬背，就像我们要求他们记住 1 到 12 的乘法表。我们让他们反复练习，反复练习，直到他们能够 100%正确地按任何顺序背诵出正确的答案。但如果我们问他们“13 乘以 13 等于多少？”他们可能会给我们一个茫然的表情。在这个时候，乘法表已经在他们的记忆中过拟合了。每个乘法对的答案，即样本，被硬编码在大脑的记忆细胞中，他们没有方法将那种知识扩展到 1 到 12 之外。

随着孩子们的成长，我们转向抽象。我们不是教孩子记忆答案，而是教他们如何计算答案——尽管他们可能会犯计算错误。在这个第二个教学阶段，一些与死记硬背相关的神经元会死亡。这些神经元的死亡（意味着遗忘）与抽象的结合使得孩子的头脑能够进行概括，现在可以解决任意的乘法问题，尽管有时他们会犯错误，甚至在 12×12 乘法表中，以某种概率分布。

神经网络中的 dropout 技术模拟了这种向抽象和学习的迁移过程，通过概率分布的不确定性进行学习。在任意层之间，你可以添加一个 dropout 层，在那里你指定一个百分比（介于 0 和 1 之间）来遗忘。节点本身不会被丢弃，而是在训练过程中，每个前向传递的随机选择不会传递信号。随机选择的节点的信号将被遗忘。例如，如果你指定 50%的 dropout（0.5），在每次数据的前向传递中，随机选择一半的节点将不会发送信号。

优势在于，我们最小化了局部过拟合的影响，同时持续训练神经网络以实现整体收敛。dropout 的常见做法是设置 20%到 50%之间的值。

在下面的代码示例中，我们在输入层和隐藏层中添加了 50%的 dropout。注意，我们在激活函数（ReLU）之前放置了它。由于 dropout 会导致节点信号为零，因此添加`Dropout`层在激活函数之前或之后无关紧要：

```
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten, ReLU, Activation, Dropout

model = Sequential()
model.add(Flatten(input_shape=(28,28)))
model.add(Dense(512))
model.add(Dropout(0.5))     ❶
model.add(ReLU())

model.add(Dense(512))
model.add(Dropout(0.5))     ❶
model.add(ReLU())

model.add(Dense(10))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])
```

❶ 添加 dropout 是为了防止过拟合。

## 摘要

+   神经网络的输入和输入层不是同一回事，也不需要相同的大小。输入是样本的特征，而输入层是学习预测相应标签的第一个权重和偏差层。

+   深度神经网络在输入层和输出层之间有一个或多个层，这些层被称为隐藏层。用编程函数作类比，输入层是函数的参数，输出层是函数的返回值，而隐藏层是函数内部的代码，它将输入参数转换为输出返回值。

+   神经网络是有向无环图，数据从输入层正向传递到输出层。

+   激活函数，如 ReLU 和 softmax，压缩了层的输出信号，研究人员发现这有助于模型更好地学习。

+   优化器的作用是更新权重，从当前批次的损失中，以便后续批次的损失更小。

+   回归器使用线性激活函数来预测连续的实数值，例如预测房屋的销售价格。

+   二元分类器使用 sigmoid 激活函数来预测二元状态：真/假，1/0，是/否。

+   多类分类器使用 softmax 激活函数从一组类别中预测一个类别，例如预测一个人的年龄人口统计。

+   顺序型 API 易于入门，但它的局限性在于不支持模型中的分支。

+   功能型 API 比顺序型 API 更适合用于生产模型。

+   当模型在训练过程中记住训练样本时，会发生过拟合，这阻止了模型泛化到它未训练过的样本。正则化方法在训练过程中注入少量随机噪声，这已被证明在防止记忆化方面是有效的。
