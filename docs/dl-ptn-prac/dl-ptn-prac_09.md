# 7 种交替连接模式

本章涵盖

+   理解更深更宽层的交替连接模式

+   通过特征图重用、进一步重构卷积和 squeeze-excitation 来提高准确性

+   使用过程设计模式编码交替连接的模型（DenseNet、Xception、SE-Net）

到目前为止，我们已经研究了具有深层卷积网络的卷积网络和具有宽层卷积网络的卷积网络。特别是，我们看到了相应的连接模式如何在卷积块之间和内部解决梯度消失和爆炸以及过度容量导致的记忆问题。

这些增加深度和宽度层的方法，以及深层层中的正则化（添加噪声以减少过拟合），减少了记忆问题，但当然并没有消除它。因此，研究人员探索了残差卷积块内部和之间的其他连接模式，以进一步减少记忆，而不会显著增加参数数量和计算操作。

在本章中，我们将介绍其中的三种交替连接模式：DenseNet、Xception 和 SE-Net。这些模式都有相似的目标：减少连接组件的计算复杂性。但它们在解决问题的方法上有所不同。让我们首先概述这些差异。然后，我们将在本章的剩余部分查看每种模式的细节。

2017 年，康奈尔大学、清华大学和 Facebook 人工智能研究部门的学者们认为，传统残差块中的残差连接只部分允许深层层使用早期层的特征提取。通过将输入与输出进行矩阵相加，输入的特征信息在向深层层进展的过程中逐渐稀释。作者们提出使用特征图拼接，他们称之为*特征重用*，来代替矩阵相加。他们的理由是，每个残差块输出的特征图将在所有剩余的（深层）层中重用。为了防止模型参数随着特征图在深层层中累积而爆炸性增长，他们在卷积组之间引入了激进的降维特征图。在他们进行的消融研究中，DenseNet 比之前的残差块网络获得了更好的性能。

在同年，Keras 的创造者 François Chollet 引入了 Xception，它将 Inception v3 模型重新设计成新的流程模式。新的模式由入口、中间和出口组成，与之前的 Inception 设计不同。虽然其他研究人员没有采用这种新的流程模式，但他们确实采用了 Chollet 对正常和可分离卷积进一步重构为深度可分离卷积的改进。这个过程减少了矩阵操作的数量，同时保持了表示等价性（稍后将有更多介绍）。这种重构继续出现在许多 SOTA 模型中——尤其是那些为内存和计算受限设备（如移动设备）设计的模型。

2017 年稍后，中国科学院和牛津大学的研究人员为残差块引入了另一种连接模式，该模式可以集成到传统的残差网络中。SE-Net 的连接模式，正如其名称所示，在残差块的输出和与块输入的矩阵加法操作之间插入了一个微块（称为*SE 链接*）。这个微块对输出特征图进行了积极的维度缩减，或*挤压*，随后是维度扩展，或*激励*。研究人员假设这个挤压-激励步骤会使特征图变得更加通用。他们将 SE 链接插入到 ResNet 和 ResNeXt 中，并在测试中未看到的示例上（保留数据）展示了平均 2%的性能提升。

现在我们有了整体图景，让我们来看看这三种模式如何解决在连接级别降低复杂性的问题。

## 7.1 DenseNet：密集连接卷积神经网络

*DenseNet*模型引入了密集连接卷积网络的概念。相应的论文，“Densely Connected Convolutional Networks”，由 Gao Huang 等人撰写([`arxiv.org/abs/1608.06993`](https://arxiv.org/abs/1608.06993))，获得了 2017 年计算机视觉和模式识别会议（CVPR）最佳论文奖。该设计基于每个残差块层输出连接到每个后续残差块层输入的原则。

这扩展了残差块中身份链接的概念（在第四章中介绍）。本节提供了关于宏观架构、组和块组件以及相应设计原则的详细信息。

### 7.1.1 密集组

在 DenseNet 之前，残差块的输入和输出之间的身份链接是通过矩阵加法组合的。相比之下，在密集块中，残差块的输入被连接到残差块的输出。这种变化引入了*特征（映射）重用*的概念。

在图 7.1 中，你可以看到残差块和密集残差块之间的连接性差异。在残差块中，输入特征图中的值被加到输出特征图上。虽然这保留了一些块中的信息，但它可以被看作是通过加法操作稀释了。在 DenseNet 残差块版本中，输入特征图被完全保留，因此没有发生稀释。

![图片](img/CH07_F01_Ferlitsch.png)

图 7.1 残差块与密集块对比：密集块使用矩阵连接而不是矩阵加法操作。

将矩阵加法替换为连接的优点：

+   进一步缓解深层网络中的消失梯度问题

+   通过较窄的特征图进一步降低计算复杂度（参数）

使用连接，输出（分类器）和特征图之间的距离更短。缩短的距离减少了消失梯度问题，允许构建更深层的网络，从而产生更高的精度。

特征图的复用与矩阵加法的前一操作具有表示等价性，但具有显著更少的过滤器。作者将这种配置称为*较窄*的层。使用较窄的层，可以减少训练所需的总体参数数量。作者理论认为，特征复用允许模型在更深层的网络中达到更高的精度，而不会暴露于消失梯度或记忆化。

这里有一个比较的例子。假设某层的输出是大小为 28 × 28 × 10 的特征图。经过矩阵加法后，输出仍然是 28 × 28 × 10 的特征图。它们内部的价值是残差块输入和输出的加和，因此没有保留原始值——换句话说，它们已经被合并。在密集块中，输入特征图是连接到残差块输出的，而不是合并，从而保留了恒等连接的原始值。在我们的例子中，输入和输出为 28 × 28 × 10，连接后的输出将是 28 × 28 × 20。继续到下一个块，输出将是 28 × 28 × 40。

以这种方式，每一层的输出都连接到下一层的输入，从而产生了描述这类模型的短语*密集连接*。图 7.2 展示了密集组中残差块的一般构造和恒等连接。

![图片](img/CH07_F02_Ferlitsch.png)

图 7.2 在这个密集的组微架构中，残差块输出和输入（恒等连接）之间使用矩阵连接操作。

如您所见，一个密集组由多个密集块组成。每个密集块由一个残差块（没有身份链接）和从输入到残差块再到输出的身份链接组成。然后，输入和输出特征图被连接成一个单一的输出，这成为下一个密集块的输入。这样，每个密集块输出的特征图都会被后续的每个密集块重用（共享）。

DenseNet 研究人员引入了一个元参数*k*，它指定了每个卷积组中的滤波器数量。他们尝试了*k* = 12, 24, 和 32。对于 ImageNet，他们使用了*k* = 32，并设置了四个密集组。他们发现，他们可以用一半的参数得到与 ResNet 网络相当的结果。例如，他们训练了一个与 ResNet50 相当参数的 DenseNet，参数数量为 2000 万，并得到了与更深层的 ResNet101 相当的结果，参数数量为 4000 万。

以下代码是一个密集组的示例实现。密集残差块的数量由参数`n_blocks`指定，输出滤波器的数量由`n_filters`指定，压缩因子由`compression`指定。对于最后一组，由于缺少过渡块（我们将在下一节中讨论），将参数`compression`设置为`None`来表示：

```
def group(x, n_blocks, n_filters, compression=None):
    """ Construct a Dense Group
        x           : input to the group
        n_blocks    : number of residual blocks in dense block
        n_filters   : number of filters in convolution layer in residual block
        compression : amount to reduce feature maps by
    """
    for _ in range(n_blocks):            ❶
        x = dense_block(x, n_filters)

    if compression is not None:          ❷
        x = trans_block(x, reduction)
    return x
```

❶ 构建一组密集连接的残差块

❷ 构建中间过渡块

让我们再次讨论为什么 DenseNet 和其他 SOTA 模型在任务组件（例如，分类器）之前没有对特征图进行最终池化。这些模型在块内进行特征提取，并在组末进行特征汇总，我们称这个过程为*特征学习*。每个组总结它所学习的特征，以减少后续组对特征图进一步处理的计算复杂度。最后一组（非池化）的特征图在大小上进行了优化，以表示在潜在空间中的高维编码。在此提醒，在多任务模型中，例如在目标检测中，潜在空间是在任务之间（或模型融合的情况下，在模型接口之间）共享的。

一旦最终特征图进入任务组件，它们将进行最后一次池化——但这次池化的方式是为了学习任务而不是特征汇总。在任务组件中的这个最后池化步骤是瓶颈层，输出被称为潜在空间的*低维嵌入*，这也可能与其他任务和模型共享。

DenseNet 架构有四个密集组，每个组都由可配置数量的密集块组成。现在让我们来看看密集块的结构和设计。

### 7.1.2 密集块

DenseNet 中的残差块使用 B(1, 3)模式，这是一个 1×1 卷积后跟一个 3×3 卷积。然而，1×1 卷积是一个线性投影而不是瓶颈：1×1 通过 4 倍的扩展因子扩展了输出特征图（滤波器）的数目。然后 3×3 执行维度缩减，将输出特征图的数目恢复到与输入特征图相同的数目。

图 7.3 展示了残差密集块中特征图的空间扩展和缩减。请注意，输入和输出特征图的数目和大小保持不变。在块内部，1×1 线性投影扩展了特征图的数目，而随后的 3×3 卷积则同时进行特征提取和特征图缩减。正是这个最后的卷积将输出特征图的数目和大小恢复到与输入相同——这个过程被称为*维度恢复***。

![图像](img/CH07_F03_Ferlitsch.png)

图 7.3 在残差密集块的卷积层中进行维度扩展和缩减时，输入和输出特征图的数目和大小保持不变。

图 7.4 说明了残差密集块，它由以下部分组成：

+   一个将特征图数目增加四倍的 1×1 线性投影卷积

+   一个既执行特征提取又恢复特征图数目的 3×3 卷积

+   一个将残差块的输入特征图和输出特征图连接的操作

![图像](img/CH07_F04_Ferlitsch.png)

图 7.4 使用连接操作进行特征重用的具有恒等快捷方式的残差密集块

DenseNet 还采用了现代惯例，使用前激活批量归一化（BN-ReLU-Conv）来提高准确度。在后激活中，ReLU 激活和批量归一化发生在卷积之后。在前激活中，批量归一化和 ReLU 发生在卷积之前。

前人研究者发现，通过从后激活转换为前激活，模型在准确度上提高了 0.5 到 2 个百分点。（例如，ResNet v2 研究者，如“深度残差网络中的恒等映射”[[`arxiv.org/abs/1603.05027`](https://arxiv.org/abs/1603.05027)]中所述。）

以下代码是一个密集残差块的示例实现，它包括以下步骤：

1.  在变量`shortcut`中保存输入特征图的副本。

1.  一个将特征图数目增加四倍的前激活 1×1 线性投影

1.  一个用于特征提取和恢复特征图数目的前激活 3×3 卷积

1.  将保存的输入特征图与输出特征图进行连接，以实现特征重用

```
shortcut = x                                                       ❶
x = BatchNormalization()(x)                                        ❷
x = ReLU()(x)                                                      ❷
x = Conv2D(4 * n_filters, (1, 1), strides=(1, 1))(x)               ❷

x = BatchNormalization()(x)                                        ❸
x = ReLU()(x)                                                      ❸
x = Conv2D(n_filters, (3, 3), strides=(1, 1), padding='same')(x)   ❸

x = Concatenate()([shortcut, x])                                   ❹
```

❶ 记录输入

❷ 维度扩展，通过 4 倍扩展滤波器（DenseNet-B）

❸ 使用 padding='same'填充的 3×3 瓶颈卷积以保持特征图形状

❹ 将输入（恒等变换）与残差块的输出连接起来，其中连接在层之间提供了特征重用。

### 7.1.3 DenseNet 宏观架构

在学习组件中，在每个密集组之间插入一个*过渡块*以进一步减少计算复杂度。过渡块是一个步长卷积，也称为*特征池化*，用于在从一个密集组移动到下一个密集组时减少连接特征图的整体大小（特征重用）。如果没有这种减少，特征图的整体大小会随着每个密集块的逐步增加而逐渐翻倍，这将导致训练参数数量的爆炸性增长。通过减少参数数量，DenseNet 可以在参数数量仅线性增加的情况下更深入地扩展层。

在我们查看过渡块的架构之前，让我们首先看看它在学习组件中的位置。如图 7.5 所示，学习组件由四个密集组组成，过渡块位于每个密集组之间。

![](img/CH07_F05_Ferlitsch.png)

图 7.5 展示了密集组之间的过渡块的大规模 DenseNet 架构

现在，让我们近距离观察每个密集组之间的过渡块。

### 7.1.4 密集过渡块

过渡块由两个步骤组成：

+   一个 1 × 1 瓶颈卷积，通过压缩因子*C*减少了输出特征图（通道）的数量。

+   在瓶颈之后跟随的步长平均池化，将每个特征图的大小减少 75%。当我们说*步长*时，我们通常指的是步长为 2。步长为 2 将特征图的高度和宽度维度减少一半，这将像素数量减少四分之一（25%）。

图 7.6 描述了此过程。在这里，过滤器/*C*代表 1 × 1 瓶颈卷积中减少特征图*数量*的特征图压缩。随后的平均池化是步长的，它减少了减少数量后的特征图*大小*。

![](img/CH07_F06_Ferlitsch.png)

图 7.6 在密集过渡块中，特征图维度通过 1 × 1 瓶颈卷积和步长平均池化层同时减少。

现在，这种压缩是如何实际工作的呢？如图 7.7 所示，我们开始时有八个特征图，每个大小为*H* × *W*，总共可以表示为*H* × *W* × 8。1 × 1 瓶颈卷积中的压缩比是 2。因此，瓶颈将输入并输出一半数量的特征图，在这个例子中是 4。我们可以将其表示为*H* × *W* × *4*。然后步长平均池化将 4 个特征图的维度减少一半，最终输出大小为 0.5*H* × 0.5*W* × 4。

![](img/CH07_F07_Ferlitsch.png)

图 7.7 展示了过渡块中特征图（压缩）减少的过程

要压缩特征图的数量，我们需要知道进入过渡块的特征图数量（通道数）。在下面的代码示例中，这是通过 `x.shape[-1]` 获得的。我们使用索引-1 来引用输入张量（*B*, *H*, *W*, *C*）的最后一个维度，这是通道数。输入张量中的特征图数量然后乘以`compression`因子（范围从 0 到 1）。请注意，在 Python 中，乘法操作以浮点值执行，因此我们将结果转换回整数：

```
n_filters = int(x.shape[-1]) * compression )           ❶
x = BatchNormalization()(x)
x = Conv2D(n_filters, (1, 1), strides=(1, 1))(x)       ❷
x = AveragePooling2D((2, 2), strides=(2, 2))(x)        ❸
```

❶ 计算特征图数量（DenseNet-C）的减少（压缩）

❷ 使用 BN-LI-Conv 形式的批量归一化进行 1 × 1 瓶颈卷积

❸ 在池化时使用平均值（平均）来减少 75%

在 GitHub 上提供了使用 Idiomatic procedure reuse 设计模式为 DenseNet 编写的完整代码示例 ([`mng.bz/6N0o`](http://mng.bz/6N0o)).

## 7.2 Xception：极端 Inception

如前所述，*Xception* (*极端 Inception*) 架构是由 Keras 的创造者 François Chollet 于 2017 年在谷歌提出的，作为对 Inception v3 架构的进一步改进建议。在他的论文“Xception: Deep Learning with Depthwise Separable Convolutions” ([`arxiv.org/pdf/1610.02357.pdf`](https://arxiv.org/pdf/1610.02357.pdf)) 中，Chollet 认为 Inception 风格模块的成功基于一个将空间相关性从通道相关性中大量解耦的因子化。这种解耦导致参数数量减少，同时仍然保持表示能力。他提出，我们可以通过完全解耦空间和通道相关性来进一步减少参数，同时保持表示能力。如果你觉得这些关于解耦的想法有点复杂，你会在 7.2.5 节中找到一个更详细的解释。

Chollet 在他的论文中做出了另一个重要的声明：他声称他对 Xception 架构的重设计实际上比 Inception 架构更简单，并且可以使用像 Keras 这样的高级库仅用 30 到 40 行代码来实现。

Chollet 的结论基于比较 Inception v3 和 Xception 在 ImageNet 和谷歌的内部 Joint Foto Tree (JFT)数据集上的准确性的实验。他在两个模型中使用了相同数量的参数，因此他认为任何准确性的提高都是由于*更有效地使用参数*。JFT 数据集包含 3.5 亿张图片和 17,000 个类别；Xception 在 JFT 数据集上比 Inception 高出 4.3%。在他的 ImageNet 实验中，该数据集包含 1.2 百万张图片和 1000 个类别，准确性的差异可以忽略不计。

从 Inception v3 到 Xception 有两个主要变化：

+   将 Inception 架构中使用的三个 Inception 风格的残差组（A、B 和 C）重新组织为入口、中间和出口流。在这种新方法下，主干组成为入口的一部分，分类器成为出口的一部分，这降低了 Inception 风格残差块的结构复杂性。

+   在 Inception v3 块中将卷积分解为空间可分离卷积的操作被替换为深度可分离卷积，这减少了 83%的矩阵乘法操作。

与 Inception v3 一样，Xception 使用后激活批量归一化（Conv-BN-ReLU）。

让我们看一下整体宏架构，然后看看重新设计组件（入口、出口和中间流）的细节。在本节末尾，我们将回到起点，我会解释空间卷积分解为深度可分离卷积的过程。

### 7.2.1 Xception 架构

Chollet 将传统的主干-学习者-分类器排列重新组合为入口流、中间流和出口流。您可以在图 7.8 中看到这一点，该图显示了重新组合并回溯到过程重用设计模式的 Xception 架构。入口和中间代表特征学习，出口流代表分类学习。

虽然我已经多次阅读了 Chollet 的论文，但我找不到描述架构为具有入口、中间和出口流的理由。我认为直接将这些残差组在学习者组件中的三种*风格*称为 A、B 和 C 会更清晰。论文似乎暗示他的决定是为了简化他所说的 Inception 的复杂架构。他希望这种简化使得架构可以用 30 到 40 行高级库（如 Keras 或 TensorFlow-Slim）的代码实现，同时保持参数数量相当。无论如何，后续的研究者并没有采用 Chollet 关于入口、中间和出口流的术语。

![图片](img/CH07_F08_Ferlitsch.png)

图 7.8 Xception 宏架构将主要组件重新组合为入口、中间和出口流。以下是它们如何适应主干、学习者和任务组件。

如您所见，主干组件被纳入入口流，分类器组件被纳入出口流。从入口到出口流的残差卷积组共同构成了学习者组件的等效部分。

Xception 架构的骨架实现显示了代码是如何分为输入流程、中间流程和输出流程部分的。输入流程进一步细分为主干和主体，输出流程进一步细分为分类器和主体。这些部分在代码模板中用三个顶级函数表示：`entryFlow()`、`middleFlow()`和`exitFlow()`。`entryFlow()`函数包含嵌套的`stem()`函数，表示主干包含在输入流程中，而`exitFlow()`包含嵌套的函数`classifier()`，表示分类器包含在输出流程中。

为了简洁，省略了函数体细节。使用 Idiomatic procedure reuse 设计模式为 Xception 提供的完整代码版本可在 GitHub 上找到 ([`mng.bz/5WzB`](https://shortener.manning.com/5WzB))。

```
def entryFlow(inputs):
    """ Create the entry flow section
        inputs : input tensor to neural network
    """
    def stem(inputs):                                         ❶
        """ Create the stem entry into the neural network
            inputs : input tensor to neural network
        """
                                                            ❷
        return x

    x = stem(inputs)                                        ❸

    for n_filters in [128, 256, 728]:                       ❹
        x = projection_block(x, n_filters)

    return x

def middleFlow(x):
    """ Create the middle flow section
        inputs : input tensor into section
    """
    for _ in range(8):                                      ❺
        x = residual_block(x, 728)
    return x

def exitFlow(x, n_classes):
    """ Create the exit flow section
        x         : input to the exit flow section
        n_classes : number of output classes
    """
    def classifier(x, n_classes):                           ❻
        """ The output classifier
            x         : input to the classifier
            n_classes : number of output classes
        """
                                                            ❼
        return x
                                                            ❼

    x = classifier(x, n_classes)                            ❽
    return x

inputs = Input(shape=(299, 299, 3))                         ❾

x = entryFlow(inputs)                                       ❿

x = middleFlow(x)                                           ⓫

outputs = exitFlow(x, 1000)                                 ⓬

model = Model(inputs, outputs)
```

❶ 主干组件是输入流程的一部分。

❷ 为了简洁，代码已被删除。

❸ 主干组件是输入流程的一部分。

❹ 使用线性投影构建三个残差块。

❺ 中间流程构建 8 个相同的残差块。

❻ 分类器组件是输出流程的一部分。

❼ 为了简洁，代码已被删除。

❽ 中间流程构建 8 个相同的残差块。

❾ 创建形状为(229, 229, 3)的输入向量。

❿ 构建输入流程。

⓫ 构建中间流程。

⓬ 构建用于 1000 个类别的输出流程。

### 7.2.2 Xception 的输入流程

输入流程组件包括主干卷积组，随后是三个 Xception 输入流程风格的残差块，依次输出 128、256 和 728 个特征图。图 7.9 显示了输入流程以及主干组如何作为子组件嵌入其中。

![](img/CH07_F09_Ferlitsch.png)

图 7.9 Xception 输入流程的微架构。

主干部分由两个 3 × 3 卷积层的堆叠组成，如图 7.10 所示。第二个 3 × 3 卷积层将输出特征图的数量加倍（维度扩展），其中一个卷积层采用步进以进行特征池化（维度减少）。对于 Xception，堆叠中的滤波器数量分别为 32 和 64，这是一个常见的约定。堆叠中的第一个卷积层，采用步进，原本是为了减少堆叠中第二个 3 × 3 卷积层的参数数量。另一种约定是在第二个 3 × 3 卷积层，采用步进进行特征汇总，并放弃了参数减少。

![](img/CH07_F10_Ferlitsch.png)

图 7.10 Xception 主干组的层构建，用于两个 3 × 3 卷积层的堆叠。

接下来是入口流式残差块，如图 7.11 所示。入口流式使用一个 B(3, 3)残差块，随后在身份链路上进行最大池化和 1 × 1 线性投影。3 × 3 卷积是深度可分离卷积（`SeparableConv2D`），与 Inception v3 不同，后者使用了正常和空间可分离卷积的组合。最大池化使用 3 × 3 的池化大小，因此从 9 像素窗口中输出最大值（与 2 × 2 的 4 像素相比）。请注意，1 × 1 线性投影也是步进的，以减少特征图的大小，以匹配最大池化层从残差路径中减少的特征图大小。

![图片](img/CH07_F11_Ferlitsch.png)

图 7.11 带线性投影快捷方式的 Xception 残差块

现在让我们看看一个入口流式残差块的示例实现。以下代码表示：

1.  一个 1 × 1 线性投影来增加特征图的数量并减小大小以匹配残差路径的输出（`shortcut`）

1.  两个 3 × 3 深度可分离卷积

1.  线性投影链路（`shortcut`）的特征图与残差路径输出的矩阵加法操作

```
def projection_block(x, n_filters):
    """ Create a residual block using Depthwise Separable Convolutions with  
        Projection shortcut
        x        : input into residual block
        n_filters: number of filters
    """

    shortcut = Conv2D(n_filters, (1, 1), strides=(2, 2), padding='same')
               (x)                                                 ❶
    shortcut = BatchNormalization()(shortcut)                      ❶

    x = SeparableConv2D(n_filters, (3, 3), padding='same')(x)      ❷
    x = BatchNormalization()(x)
    x = ReLU()(x)

    x = SeparableConv2D(n_filters, (3, 3), padding='same')(x)      ❸
    x = BatchNormalization()(x)
    x = ReLU()(x)

    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)    ❹

    x = Add()([x, shortcut])                                       ❺
    return x
```

❶ 投影快捷方式使用步进卷积来减小特征图的大小，同时将滤波器数量加倍以匹配块的输出，以便进行矩阵加法操作。

❷ 首个深度可分离卷积

❸ 第二个深度可分离卷积

❹ 将特征图的大小减少 75%

❺ 将投影快捷方式添加到块的输出

### 7.2.3 Xception 的中间流

中间流由八个中间流式残差块组成，每个块输出 728 个特征图。在组内保持输入/输出特征图数量相同是惯例；而在组之间，特征图的数量逐渐增加。相比之下，Xception 中入口和中间流的输出特征图数量保持不变，而不是增加。

中间流式残差块，如图 7.12 所示，使用八个 B(3, 3, 3)残差块。与入口流残差块不同，在所有块之间输入和输出特征图的数量保持相同，因此没有池化操作，也没有在身份链路上进行 1 × 1 步进线性投影。

![图片](img/CH07_F12_Ferlitsch.png)

图 7.12 Xception 中间流微架构由八个相同的残差块组成。

现在让我们看看每个残差块中发生了什么。图 7.13 显示了三个 3 × 3 卷积，它们是深度可分离卷积（`SeparableConv2D`）。（我们很快就会了解到深度可分离卷积究竟是什么。）

![图片](img/CH07_F13_Ferlitsch.png)

图 7.13 带身份快捷方式的残差块中间流：矩阵加法操作中输入特征图和残差路径的数量和大小相同。

以下代码是中间流样式残差块的示例实现，其中 B(3, 3, 3)样式使用深度可分离卷积(`SeparableConv2D`)实现：

```
def residual_block(x, n_filters):
    """ Create a residual block using Depthwise Separable Convolutions
        x        : input into residual block
        n_filters: number of filters
    """

    shortcut = x

    x = SeparableConv2D(n_filters, (3, 3), padding='same')(x)     ❶
    x = BatchNormalization()(x)
    x = ReLU()(x)

    x = SeparableConv2D(n_filters, (3, 3), padding='same')(x)     ❶
    x = BatchNormalization()(x)
    x = ReLU()(x)

    x = SeparableConv2D(n_filters, (3, 3), padding='same')(x)     ❶
    x = BatchNormalization()(x)
    x = ReLU()(x)

    x = Add()([x, shortcut])                                      ❷
    return x
```

❶ 三个 3 × 3 深度可分离卷积的序列

❷ 将恒等连接添加到块的输出

### 7.2.4 Xception 的出口流

现在来看出口流。它由一个出口流样式的残差块组成，后面跟着一个卷积块（非残差块），然后是分类器。如图 7.14 所示，分类器组是出口流的子组件。

![](img/CH07_F14_Ferlitsch.png)

图 7.14 Xception 出口流逐步增加特征图的数量。

出口流以 728 个特征图和中间流的输出作为输入，并在分类器之前逐步增加特征图的数量到 2048。与 Inception v3 和 ResNet 等大型 CNN 的常规做法相比，这些 CNN 在瓶颈层之前生成 2048 个最终特征图，形成了所谓的*高维编码*。

让我们仔细看看那个单独的出口流样式残差块，如图 7.15 所示。这个残差块是一个 B(3,3)，两个卷积分别输出 728 和 1024 个特征图。两个卷积后面跟着一个 3 × 3 的最大池化，然后是一个 1 × 1 的线性投影用于恒等连接。与中间流相比，出口流块增加了特征图的数量，并在出口流中的单个残差块和卷积块之间延迟了池化。

![](img/CH07_F15_Ferlitsch.png)

图 7.15 Xception 出口流残差块带有线性投影快捷连接延迟了最终特征图数量增加和池化的进程。

注意，出口流残差块结构与入口流相同，除了出口流样式在块中进行维度扩展，从 728 个特征图扩展到 1024 个特征图，而入口流不进行任何维度扩展。

现在来看出口流卷积块，它位于残差块之后，如图 7.16 所示。此块由两个 3 × 3 深度可分离卷积组成，每个卷积都进行维度扩展。这种扩展将特征图的数量增加到 1156 和 2048，分别，这完成了在瓶颈层之前增加最终特征图数量的延迟增长。

![](img/CH07_F16_Ferlitsch.png)

图 7.16 Xception 出口流卷积块完成了最终特征图数量的延迟增长。

出口流的最后一个组是分类器，由一个`GlobalAveragePooling2D`层组成，该层将最终特征图池化和展平成一个 1D 向量，然后是一个具有 softmax 激活的分类`Dense`层。

### 7.2.5 深度可分离卷积

如承诺的那样，我们终于要深入探讨 Xception 架构中的深度可分离卷积了。自从它们被引入以来，深度可分离卷积在卷积神经网络中得到了广泛的应用，因为它们能够在保持表示能力的同时降低计算成本。深度可分离卷积最初由 Laurent Sifre 和 Stéphane Mallat 于 2014 年在 Google Brain 工作期间提出（参见[`arxiv.org/abs/1403.1687`](https://arxiv.org/abs/1403.1687)），自那时起，深度可分离卷积在各种 SOTA 模型中得到了研究和应用，包括 Xception、MobileNet 和 ShuffleNet。

简而言之，深度可分离卷积将一个 2D 核分解为两个 2D 核；第一个是深度卷积，第二个是点卷积。为了完全理解这一点，我们首先需要了解两个相关概念：深度卷积和点卷积，深度卷积就是由这两个概念构建的。

### 7.2.6 深度卷积

在*深度卷积*中，核被分割成一个单独的*H* × *W* × 1 核，每个通道一个，每个核只对一个通道进行操作，而不是对所有通道进行操作。在这种安排中，跨通道关系与空间关系解耦。正如 Chollet 所建议的，完全解耦空间和通道卷积会导致更少的 matmul 操作，并且精度与没有解耦和正常卷积的模型相当，以及与部分解耦和空间可分离卷积的模型相当。

因此，在图 7.17 中展示的 RGB 示例中，使用 3 × 3 核，深度卷积将会有三个 3 × 3 × 1 的核。当核移动时，乘法操作的次数与正常卷积相同（例如，在三个通道上为 27）。然而，输出是一个 D 深度的特征图，而不是一个 2D（`depth=1`）的特征图。

![图片](img/CH07_F17_Ferlitsch.png)

图 7.17 在这个深度卷积中，核被分割成单个*H* × *W* × 1 核。

### 7.2.7 点卷积

深度卷积的输出随后作为*点卷积*的输入，形成深度可分离卷积。点卷积执行解耦的空间卷积。点卷积结合深度卷积的输出，并将特征图的数量扩展到匹配指定的过滤器（特征图）数量。组合输出与正常或可分离卷积（89）相同数量的特征图，但矩阵乘法操作更少（减少了 83%）。

点卷积，如图 7.18 所示，具有 1 × 1 × *D*（通道数）。它将遍历每个像素，生成一个*N* × *M* × 1 的特征图，这取代了*N* × *M* × *D*的特征图。

![图片](img/CH07_F18_Ferlitsch.png)

图 7.18 点卷积

在点卷积中，我们使用 1 × 1 × D 核，每个输出一个。正如图 7.17 中的前一个例子一样，如果我们的输出是 256 个滤波器（特征图），我们将使用 256 个 1 × 1 × D 核。

在图 7.17 中使用的 RGB 示例中，深度卷积使用了 3 × 3 × 3 核，每次核移动时都有 27 次乘法操作。这将随后是一个 1 × 1 × 3 × 256（其中 256 是输出滤波器的数量）——这是 768。总的乘法操作数将是 795，而不是正常卷积的 6912 次和空间可分离卷积的 4608 次。

在 Xception 架构中，inception 模块中的空间可分离卷积被深度可分离卷积所取代，通过 83%的计算复杂度（乘法操作数）减少。使用 Idiomatic procedure reuse 设计模式对 Xception 的完整代码实现可在 GitHub 上找到（[`mng.bz/5WzB`](http://mng.bz/5WzB)）。

## 7.3 SE-Net：挤压和激励

现在我们将转向另一种替代连接设计，即*squeeze-excitation-scale pattern*或*SE-Net*，它可以通过仅添加少量参数来添加到现有的残差网络中以提高准确率。

在“Squeeze-and-Excitation Networks”中引入了这种模式（[`arxiv.org/abs/1709.01507`](https://arxiv.org/abs/1709.01507)），Jie Hu 等人解释说，之前对模型的改进主要集中在卷积层之间的空间关系上。因此，他们决定采取不同的方法，并研究基于*通道*之间关系的新网络设计。他们的想法是，特征重校准可以使用全局信息来选择性地强调重要特征并降低不太重要的特征的重要性。

为了实现选择性强调特征的能力，作者提出了在残差块内添加一个*squeeze-excitation* (*SE*) *链接*的概念。这个块将位于卷积层（或多个层）的输出和与恒等链接的矩阵加法操作之间。这个概念赢得了 2017 年 ILSVRC 竞赛的 ImageNet。

他们的消融研究指出了 SE-Net 方法的一些好处，包括以下这些：

+   可以添加到现有的 SOTA 架构中，例如 ResNet、ResNeXt 和 Inception。

+   在实现更高准确率的同时，参数增加最小。例如：

    +   ResNet50 的 ImageNet top-5 错误率为 7.48%，SE-ResNet50 为 6.62%

    +   ResNeXt50 的 ImageNet top-5 错误率为 5.9%，SE-ResNeXt50 为 5.49%

    +   Inception 的 ImageNet top-5 错误率为 7.89%，SE-Inception 为 7.14%

### 7.3.1 SE-Net 的架构

SE-Net 架构，如图 7.19 所示，由一个现有的残差网络组成，然后通过在残差块中插入 SE 链接进行改造。改造后的 ResNet 和 ResNeXt 架构分别称为*SE-ResNet*和*SE-ResNeXt*。

![](img/CH07_F19_Ferlitsch.png)

图 7.19 SE-Net 宏架构展示了将 SE-链接添加到每个残差块

### 7.3.2 SE-Net 的组和块

如果我们分解宏观架构，我们会发现图 7.19 中的每个卷积组都由一个或多个残差块组成，构成一个残差组。每个残差块都有一个 SE 链接。这种对残差组的近距离观察在图 7.20 中展示。

![图片](img/CH07_F20_Ferlitsch.png)

图 7.20 在残差组中，每个残差块都插入了一个 SE 链接。

现在，让我们分解残差组。图 7.21 展示了 SE 链接是如何插入到残差块中，位于卷积层（s）输出和矩阵加法操作之间。

![图片](img/CH07_F21_Ferlitsch.png)

图 7.21 残差块展示了 SE 链接是如何插入到残差路径和矩阵加法操作（标记为 *Add*）之间的（对于身份链接）

以下代码是向 ResNet 残差块添加 SE 链接的示例实现。在块的末尾，在 B(3,3) 输出和矩阵加法操作（`Add()`）之间插入对 `squeeze_excite_link()` 的调用。在 `squeeze_excite_link()` 函数中，我们实现了 SE 链接（在下一小节中详细介绍）。

参数 `ratio` 是在激励操作之前对输入进行挤压操作的维度降低量（比率）。

```
def identity_block(x, n_filters, ratio=16):
    """ Create a Bottleneck Residual Block with Identity Link
        x        : input into the block
        n_filters: number of filters
        ratio    : amount of filter reduction during squeeze
    """
    shortcut = x

    x = Conv2D(n_filters, (1, 1), strides=(1, 1))(x)                    ❶
    x = BatchNormalization()(x)
    x = ReLU()(x)

    x = Conv2D(n_filters, (3, 3), strides=(1, 1), padding="same")(x)    ❷
    x = BatchNormalization()(x)
    x = ReLU()(x)

    x = Conv2D(n_filters * 4, (1, 1), strides=(1, 1))(x)                ❸
    x = BatchNormalization()(x)

    x = squeeze_excite_link(x, ratio)                                   ❹

    x = Add()([shortcut, x])                                            ❺
    x = ReLU()(x)
    return x
```

❶ 用于维度降低的 1 × 1 卷积

❷ 瓶颈层使用 3 × 3 卷积

❸ 1 × 1 卷积通过增加过滤器数量 4 倍来实现维度恢复

❹ 将输出通过挤压-激励链接

❺ 将身份链接（输入）添加到残差块的输出

### 7.3.3 SE 链接

现在，让我们详细探讨 SE 链接（图 7.22）。该链接由三个层组成。前两层执行挤压操作。使用全局平均池化将每个输入特征图（通道）减少到一个单一值，输出一个大小为 *C*（通道）的 1D 向量，然后将其重塑为一个大小为 *C*（通道）的 1-×-1 像素的 2D 矩阵。密集层随后通过减少比率 *r* 进一步减少输出，结果是一个大小为 *C* / *r*（通道）的 1-×-1 像素的 2D 矩阵。

![图片](img/CH07_F22_Ferlitsch.png)

图 7.22 挤压-激励块展示了挤压、激励然后缩放操作

挤压后的输出随后传递到第三层，该层通过恢复到链接输入的通道数（*C*）来进行激励。请注意，这与使用 1 × 1 线性投影卷积相当，但这里使用的是密集层。

最后一步是一个缩放操作，它由一个从输入的身份链接组成，其中来自挤压-激励操作的 1 × 1 × *C* 向量与输入（*H* × *W* × *C*）进行矩阵乘法。缩放操作之后，输出维度（特征图的数量和大小）恢复到输入的原始维度（缩放）。

现在让我们看看 SE 链接的一个示例实现，包括挤压、激励和缩放操作。注意在`GlobalAveragePooling2D`之后的`Reshape`操作，将池化的 1D 向量转换为 1-×-1 像素的 2D 向量，以便后续的两个`Dense`层执行挤压和激励操作。激励产生的 1 × 1 × *C*矩阵随后与输入（`shortcut`）进行矩阵乘法，以进行缩放操作：

```
def squeeze_excite_link(x, ratio=16):
    """ Create a Squeeze and Excite link
        x    : input to the link
        ratio : amount of filter reduction during squeeze
    """
    shortcut = x
    n_filters = x.shape[-1]                                  ❶
    x = GlobalAveragePooling2D()(x)                          ❷
    x = Reshape((1, 1, n_filters))(x)                        ❸
    x = Dense(n_filters // ratio, activation='relu')(x)      ❹
    x = Dense(n_filters, activation='sigmoid')(x)            ❺
    x = Multiply()([shortcut, x])                            ❻
    return x
```

❶ 获取 SE 链接输入中的特征图（滤波器）数量

❷ 使用全局平均池化进行维度减少的挤压操作，将输出一个 1D 向量

❸ 将输出重塑为 1 × 1 特征图（1 × 1 × C）

❹ 通过减少比例将滤波器数量（1 × 1 × C / r）减少

❺ 通过恢复滤波器数量（1 × 1 × C）进行维度恢复的激励操作

❻ 缩放操作，将挤压/激励输出与输入（W × H × C）相乘

使用 Idiomatic 程序重用设计模式在 GitHub 上提供了一个 SE-Net 的完整代码实现（[`mng.bz/vea7`](https://shortener.manning.com/vea7)）。

## 摘要

+   DenseNet 中的特征重用通过将残差块的输入到输出的特征图拼接代替矩阵加法。这种分类提高了现有 SOTA 模型的准确性。

+   在 DenseNet 中使用 1 × 1 卷积来学习是针对特定数据集进行上采样和下采样的最佳方式。

+   在 Xception 中将空间可分离卷积进一步重构为深度可分离卷积，进一步降低计算成本，同时保持表示等价。

+   在 SE-Net 中添加 squeeze-excite-scale 模式，同时仅增加少量参数，可以提高准确性。
