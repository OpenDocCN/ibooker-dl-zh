# 6 宽卷积神经网络

本章涵盖

+   介绍宽卷积层设计模式

+   理解宽层与深层的优势

+   将微架构模式重构以降低计算复杂性

+   使用过程设计模式编码前 SOTA 宽卷积模型

到目前为止，我们一直专注于具有更深层、块层和残差网络中的捷径的网络，用于图像相关的任务，如分类、目标定位和图像分割。现在我们将探讨具有宽而不是深的卷积层的网络。从 2014 年的 Inception v1（GoogLeNet）开始，到 2015 年的 ResNeXt 和 Inception v2，神经网络设计转向了宽层，减少了深层层的需求。本质上，宽层设计意味着并行进行多个卷积，然后将它们的输出连接起来。相比之下，深层层具有顺序卷积并聚合它们的输出。

那么，是什么导致了宽层设计模式的实验？当时，研究人员了解到，为了提高模型的准确性，它们需要更多的容量。更具体地说，它们需要具有冗余的过量容量。

早期与 VGG ([`arxiv.org/pdf/1409.1556.pdf`](https://arxiv.org/pdf/1409.1556.pdf)) 和 ResNet v1 ([`arxiv.org/abs/1512.03385`](https://arxiv.org/abs/1512.03385)) 的合作工作表明，更深层的容量确实增加了准确性。例如，AlexNet（2012）是第一个卷积神经网络提交作品，也是 ILSVRC 挑战赛的获胜者，实现了 5 类错误率为 15.3%，比 2011 年的获胜者提高了 10%。ZFNet 基于 AlexNet，并在 2013 年成为获胜者，5 类错误率为 14.8%。然后，在 2014 年，VGG 在层中进一步加深，以 7.3%的 5 类错误率成为第一名，而 ResNet 在 2015 年甚至更深，以 3.57%的 5 类错误率获得第一名。

但这些设计都遇到了瓶颈，限制了层的深度，从而限制了增加容量的能力。一个主要问题是梯度消失和梯度爆炸。当向模型添加更深层时，这些深层层的权重更有可能变得太小（消失）或太大（爆炸）。在训练过程中，模型会崩溃，就像计算机程序崩溃一样。

2015 年批量归一化的引入在某种程度上解决了这个问题。该开创性论文的作者 ([`arxiv.org/abs/1502.03167`](https://arxiv.org/abs/1502.03167)) 假设，在训练过程中将每层的权重重新分配到正态分布中，可以解决深层层中权重变得太小或太大的问题。其他研究人员验证了这一假设，批量归一化成为今天仍在继续的惯例。

但在加深层数的过程中，仍然存在一个问题：记忆化。结果证明，更深层的网络，通过增加过载来提高准确性，比浅层网络更容易记忆数据。也就是说，在过载的情况下，训练数据中的例子可能会直接映射到节点上，而不是从训练数据中泛化。当你在训练数据上看到准确率提高，但在训练过程中未看到的例子上准确率急剧下降时，我们说模型是*过拟合*的。而过拟合表明模型是在记忆而不是在学习。在深层添加一些噪声，如 dropout 和高斯噪声，可以减少记忆化，但并不能完全消除。

但如果我们通过在浅层中使卷积更宽来增加容量会怎样呢？这增加的容量将减少在发生记忆化的深层中进一步深化的需求。以 ResNeXt 为例，它在 2016 年 ILSVRC 竞赛中获得了第二名。它将残差块中的顺序卷积替换为并行卷积，以在浅层增加容量。

本章涵盖了宽层设计演变的过程，从原始的 inception 模块原理开始，该原理在 Inception v1 中被重新设计，然后是 Inception v2 和 v3 中的宽层块改进。我们还将探讨 Facebook AI Research 在 ResNeXt 中以及巴黎理工学院在 Wide Residual Network 中的宽层设计并行演变。

## 6.1 Inception v1

*Inception v1* ([`arxiv.org/abs/1409.4842`](https://arxiv.org/abs/1409.4842))，在原名 GoogLeNet 下赢得了 2014 年 ILSVRC 竞赛中的目标检测奖项，引入了*inception 模块*。这个卷积层具有不同滤波器尺寸的并行卷积，每个卷积的输出被连接在一起。这里的想法是，而不是试图为某一层选择最佳的滤波器尺寸，每一层都有多个并行滤波器尺寸，层“学习哪个尺寸是最佳的”。

例如，假设你设计了一个具有多层卷积的模型，但你不知道哪个滤波器尺寸会给你最佳的结果。比如说，你想知道三个尺寸——3 × 3、5 × 5 或 7 × 7——哪个能给你最好的准确率。为了比较准确率，你将不得不制作三个版本的模型，每个滤波器尺寸一个，并训练每一个。但假设你现在想知道每个层的最佳滤波器尺寸。也许第一层应该是 7 × 7，下一层是 5 × 5，其余的是 3 × 3（或任何其他组合）。根据深度，这可能意味着数百甚至数千种可能的组合。训练每一种组合将是一项巨大的工作。

相反，inception 模块的设计通过在每个卷积层让每个特征图通过不同滤波器大小的并行卷积来解决该问题。这种创新使得模型能够通过单个模型实例的版本和训练来学习适当的滤波器大小。

### 6.1.1 朴素 inception 模块

图 6.1 展示了 *朴素 inception 模块*，展示了这种方法。

![](img/CH06_F01_Ferlitsch.png)

图 6.1 作为实验各种 inception 模块变体的理论基础的朴素 inception 模块

原生的 inception 模块是一个卷积块。块输入通过四个分支：一个用于降维的池化层，以及 1 × 1、3 × 3 和 5 × 5 的卷积层。然后，池化和其他卷积层的输出被连接在一起。

不同的滤波器大小捕捉不同的细节级别。1 × 1 卷积捕捉特征的细微细节，而 5 × 5 捕捉更抽象的特征。您可以在朴素 inception 模块的示例实现中看到这个过程。来自先前块（层）`x` 的输入分支并通过最大池化层、1 × 1、3 × 3 和 5 × 5 卷积，然后这些卷积被连接在一起：

```
x1 = MaxPooling2D((3, 3), strides=(1,1), padding='same')(x)              ❶
x2 = Conv2D(64, (1, 1), strides=(1, 1), padding='same', activation='relu')(x)                                              
x3 = Conv2D(96, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)                                              
x4 = Conv2D(48, (5, 5), strides=(1, 1), padding='same', activation='relu')(x)                                              

output = Concatenate()([x1, x2, x3, x4])                                 ❷
```

❶ inception 分支，其中 x 是前一层

❷ 将四个分支的输出连接在一起

通过设置 `padding='same'`，输入的高度和宽度维度得到保留。这允许将每个分支的相应输出连接在一起。例如，如果输入是 256 个大小为 28 × 28 的特征图，分支层的维度如下，其中 `?` 是批处理大小的占位符：

```
x1 (pool)       :  (?, 28, 28, 256)
x2 (1x1)        :  (?, 28, 28, 64)
x2 (3x3)        :  (?, 28, 28, 96)
x3 (5x5)        :  (?, 28, 28, 48)
```

连接后，输出结果如下：

```
x (concat)      : (?, 28, 28, 464)
```

让我们看看我们是如何得到这些数字的。首先，卷积和最大池化都是非步进（意味着它们有步长 1），所以特征图没有下采样。其次，由于我们设置了 `padding='same'`，我们不会在边缘丢失任何像素宽度/高度。因此，输出的特征图大小将与输入相同，因此在输出中是 28 × 28。

现在，让我们看看最大池化分支，它输出的特征图数量与输入相同，因此我们得到 256。三个卷积分支的特征图数量等于滤波器的数量，因此会是 64、96、48。然后如果我们把所有分支的特征图加起来，我们得到 464。

对于一个朴素 inception 模块，`summary()` 显示有 544,000 个参数需要训练：

```
max_pooling2d_1 (MaxPooling2D (None, 28, 28, 256)  0      input_1[0][0]
_______________________________________________________________________________
conv2d_1 (Conv2D)            (None, 28, 28, 64)    16448  input_1[0][0]
_______________________________________________________________________________
conv2d_2 (Conv2D)            (None, 28, 28, 96)    221280 input_1[0][0]
_______________________________________________________________________________
conv2d_3 (Conv2D)            (None, 28, 28, 48)    307248 input_1[0][0]
_______________________________________________________________________________
concatenate_1 (Concatenate)  (None, 28, 28, 464)   0      max_pooling2d_1[0][0]
conv2d_1[0][0]               
conv2d_2[0][0]               
conv2d_3[0][0]               
===============================================================================
Total params: 544,976
Trainable params: 544,976
```

如果省略 `padding='same'` 参数（默认为 `padding='valid'`），形状将如下所示：

```
x1 (pool)       :  (?, 26, 26, 256)
x2 (1x1)        :  (?, 28, 28, 64)
x2 (3x3)        :  (?, 26, 26, 96)
x3 (5x5)        :  (?, 24, 24, 48)
```

由于宽度和高度维度不匹配，如果您尝试连接这些层，您将得到以下错误：

```
ValueError: A Concatenate layer requires inputs with matching shapes except
➥ for the concat axis. Got inputs shapes: [(None, 26, 26, 256), (None, 28, 
➥ 28, 64), (None, 26, 26, 96), (None, 24, 24, 48)]
```

原始的 Inception 模块是 Inception v1 作者的原理。当作者参加 ILSVRC 比赛时，他们通过使用瓶颈残差块设计重构了模块，称为 Inception v1 模块。该模块保持了准确性，并且在训练时计算成本更低。

### 6.1.2 Inception v1 模块

Inception v1 通过在池化、3 × 3 和 5 × 5 分支中添加 1 × 1 瓶颈卷积，进一步实现了维度降低。这种维度降低将整体计算复杂度降低了近三分之二。

在这一点上，你可能想知道，为什么使用 1 × 1 卷积？一个 1 像素的过滤器如何能够学习到任何特征？1 × 1 卷积的使用就像粘合代码。1 × 1 卷积要么用于扩展或减少输出中的通道数，同时保持通道大小（形状）。扩展通道数被称为*线性投影*；我们在 5.3.1 节中讨论了这一点。

减少，也称为*瓶颈*，用于减少块输入和卷积层输入之间的通道数。线性投影和瓶颈卷积类似于上采样和下采样，除了我们不是扩展或减少通道的*大小*，而是减少通道的*数量*。在这种情况下，因为我们正在减少通道数，所以我们可以说我们正在压缩数据——这就是我们使用术语*维度降低*的原因。我们可以使用静态算法来完成这项工作，或者，正如这个例子中，我们*学习最优方法*来减少通道数。这与最大池化和特征池化类似；在最大池化中，我们使用静态算法来减少通道的大小，而在特征池化中，我们*学习最优方法来减少大小*。

图 6.2 展示了 Inception v1 模块。3 × 3 和 5 × 5 分支之前有一个 1 × 1 瓶颈卷积，而池化分支之后有一个 1 × 1 瓶颈卷积。

![图片](img/CH06_F02_Ferlitsch.png)

图 6.2 Inception v1 模块（模块）的设计，该模块被用于 2014 年 ILSVRC 提交

下面是一个 Inception v1 模块（模块）的示例，其中池化、3 × 3 和 5 × 5 卷积分支增加了额外的 1 × 1 瓶颈卷积：

```
x1 = MaxPooling2D((3, 3), strides=(1,1), padding='same')(x)                   ❶
x1 = Conv2D(64, (1, 1), strides=(1, 1), padding='same',                       ❶
➥ activation='relu')(x1)                                                     ❶
x2 = Conv2D(64, (1, 1), strides=(1, 1), padding='same', activation='relu')(x) ❶
x3 = Conv2D(64, (1, 1), strides=(1, 1), padding='same', activation='relu')(x) ❶
x3 = Conv2D(96, (3, 3), strides=(1, 1), padding='same',                       ❶
➥ activation='relu')(x3)                                                     ❶
x4 = Conv2D(64, (1, 1), strides=(1, 1), padding='same', activation='relu')(x) ❶
x4 = Conv2D(48, (5, 5), strides=(1, 1), padding='same',                       ❶
➥ activation='relu')(x4)                                                     ❶

x = Concatenate([x1, x2, x3, x4])                                             ❷
```

❶ Inception 分支，其中 x 是前一层

❷ 将分支的特征图连接在一起

对于这些层的`summary()`显示需要训练 198,000 个参数，相比之下，使用瓶颈卷积进行维度降低的原生 Inception 模块有 544,000 个参数。模型的设计者能够保持与原始 Inception 模块相同的准确度水平，但训练速度更快，预测（推理）性能得到改善：

```
max_pooling2d_1 (MaxPooling2D (None, 28, 28, 256)  0           input_1[0][0]        
____________________________________________________________________________________
conv2d_1 (Conv2D)             (None, 28, 28, 64)   16448       input_1[0][0]
____________________________________________________________________________________
conv2d_2 (Conv2D)             (None, 28, 28, 64)   16448       input_1[0][0]
____________________________________________________________________________________
conv2d_3 (Conv2D)             (None, 28, 28, 64)   16448       max_pooling2d_1[0][0]
____________________________________________________________________________________
conv2d_4 (Conv2D)             (None, 28, 28, 64)   16448       input_1[0][0]
____________________________________________________________________________________
conv2d_5 (Conv2D)             (None, 28, 28, 96)   55392       conv2d_4[0][0]
____________________________________________________________________________________
conv2d_6 (Conv2D)             (None, 28, 28, 48)   76848       conv2d_2[0][0]
____________________________________________________________________________________
concatenate_430 (Concatenate) (None, 28, 28, 272)  0           conv2d_3[0][0]
conv2d_1[0][0]               
conv2d_5[0][0]               
conv2d_6[0][0]               
====================================================================================
Total params: 198,032
Trainable params: 198,032
```

正如您在图 6.3 中看到的，当 Inception v1 架构被重构为程序设计模式时，它由四个组件组成：主干、学习器、分类器和辅助分类器。总体而言，程序设计模式表示的宏观架构与之前我展示的 SOTA 模型相同，只是增加了辅助分类器。如图所示，学习器组件由五个卷积组组成，每个组有不同的卷积块数量。第二和第四个卷积组是唯一只有一个卷积块的组，并且是与辅助分类器相连的组。

![图片](img/CH06_F03_Ferlitsch.png)

图 6.3 在 Inception v1 宏观架构中，辅助分类器被添加到第二和第四个 Inception 组之后。

Inception v1 在 2014 年 ILSVRC 挑战中对象检测项目中获得第一名的成绩，证明了在深度层的同时探索宽层设计模式的有用性。

注意，我一直在将模块称为块，这是在此设计模式中使用的术语。接下来，我们将更详细地探讨每个组件。

### 6.1.3 主干

主干是进入神经网络的人口。输入（图像）通过一系列（深度）卷积和最大池化进行处理，就像传统的 ConvNet 一样。

让我们更深入地探讨一下主干部分，以便您可以看到其结构与当时传统 SOTA 主干的不同（见图 6.4）。Inception 使用了一个非常粗略的 7 × 7 初始滤波器，随后是一个非常激进的特性图减少，包括两个步进卷积和两个最大池化。另一方面，它逐渐将特性图的数量从 64 增加到 192。Inception 在卷积中无法移动滤波器到边缘的情况下进行编码。因此，为了在减少过程中保持高度和宽度减半，添加了零填充。

![图片](img/CH06_F04_Ferlitsch.png)

图 6.4 Inception v1 主干由一个粗略的 7 × 7 滤波器和一个详细的 3 × 3 滤波器组成，每个卷积后都进行最大池化以降低维度。

### 6.1.4 学习器

学习器是由五个组中的九个 Inception 块组成的集合，如图 6.3 和图 6.5 所示。图中较宽的组代表两个或三个 Inception 块的一组，而较细的组是一个单独的 Inception 块，总共九个 Inception 块。第四和第七个块（单个块）被单独列出，以突出它们有一个额外的组件，即辅助分类器。

![图片](img/CH06_F05_Ferlitsch.png)

图 6.5 Inception v1 学习组件的组配置和块数量。

### 6.1.5 辅助分类器

辅助分类器是一组两个分类器，作为训练神经网络的辅助工具。每个辅助分类器由一个卷积层、一个密集层和一个最终的 softmax 激活函数（图 6.6）组成。softmax（你可能已经知道）是统计学中的一个方程，它接受一组独立的概率（从 0 到 1）作为输入，并将该组压缩，使得所有概率加起来等于 1。在一个每个类别有一个节点的最终密集层中，每个节点都会做出独立的预测（从 0 到 1），通过将值传递给 softmax 函数，所有类别的预测概率之和将等于 1。

![](img/CH06_F06_Ferlitsch.png)

图 6.6 Inception v1/v2 辅助分类器组

Inception v1 架构引入了辅助分类器的概念。这里的原理是，随着神经网络层数的加深（随着前层离最终分类器越来越远），前层更容易受到梯度消失和增加的训练时间（增加的 epoch 数量）的影响，以训练最前层的权重。图 6.7 展示了这一过程。当权重更新从后层传播过来时，更新往往会逐渐变小。

![](img/CH06_F07_Ferlitsch.png)

图 6.7 在反向传播过程中，通过层传递的权重更新逐渐变小。

如 Inception v1 的作者们所理论化的，这可能导致两个他们想要解决的问题。首先，如果更新变得太小，乘法运算可能会导致一个太小以至于无法由浮点计算机硬件表示的数字（这被称为*梯度消失*）。

另一个问题是在早期层的更新远小于后期层时，它们将需要更长的时间来收敛并增加训练时间。此外，如果后期层提前收敛而早期层较晚收敛，那么后期层可能会开始记住数据，而早期层仍在学习泛化。

Inception v1 的作者们认为，在半深层，有一些信息可以用来预测或分类输入，尽管其准确性不如最终分类器。这些早期的分类器更靠近前层，因此不太容易受到梯度消失的影响。在训练过程中，损失函数成为辅助分类器和最终分类器损失的组合。换句话说，作者们认为，将辅助和最终分类器的损失组合起来，将导致所有层权重的更新更加均匀，从而缓解梯度消失并减少训练时间。

图 6.6 中展示的辅助分类器在 Inception v1 和 Inception v2 中都被使用。在 Inception 之后，由于两个原因，没有继续使用辅助分类器的做法。首先，随着模型层级的加深，梯度消失（和爆炸）的问题在深层比在前面层更为明显，因此该理论在深层神经网络中并未得到验证。其次，2015 年引入的批量归一化在所有层面上统一解决了这个问题。

与 VGG 设计相比，Inception v1 在分类器中（无论是辅助还是最终分类器）消除了添加额外的密集层，而 VGG 则有两个额外的 4096 节点密集层。作者理论认为，额外的密集节点对于训练最终的分类密集层是不必要的。这大大降低了计算复杂度，而没有在分类器上降低准确性。在随后的 SOTA 模型中，研究人员发现他们可以消除瓶颈和最终分类器层之间的所有先前密集层，而不会降低准确性。

Inception 是最后一批使用分类器中的 dropout 层进行正则化以减少过拟合的 SOTA 模型之一。在随后的批量归一化引入后，研究人员观察到归一化在每个层上添加了少量的正则化，并且比 dropout 更有效地促进了泛化。最终，研究人员引入了称为*权重正则化*的显式逐层正则化，进一步提高了正则化效果。因此，随后 dropout 的使用逐渐被淘汰。

### 6.1.6 分类器

图 6.8 展示了在训练神经网络和预测中使用的最终（非辅助）分类器。请注意，在预测时，辅助分类器被移除。该分类器通过两层实现全局平均池化步骤；第一层（`AveragePooling2D`）对每个特征图进行平均池化，形成 1 × 1 的特征图，随后通过一个展平层将其展平成一个一维向量。在分类的`Dense`层之前，使用了一个`Dropout`层进行正则化——这是当时的一种常见做法。

![图片](img/CH06_F08_Ferlitsch.png)

图 6.8 在 Inception 最终分类器中，池化到 1 × 1 像素图和展平是作为两个步骤完成的。

使用 Idiomatic 过程重用设计模式为 Inception v1 编写的完整代码可以在 GitHub 上找到([`mng.bz/oGnd`](https://shortener.manning.com/oGnd))。

接下来，让我们看看 Inception v2 如何引入了计算密集型卷积分解的概念。

## 6.2 Inception v2：分解卷积

在卷积中，滤波器（核）的大小越大，计算成本就越高。提出 *Inception v2* 架构的论文计算出，inception 模块中的 5 × 5 卷积比 3 × 3 卷积计算成本高 2.78 倍。换句话说，5 × 5 滤波器需要几乎三倍的 matmul 操作，需要更多的时间进行训练和预测。作者的目标是找到一种方法，用 3 × 3 滤波器替换 5 × 5 滤波器，以减少训练/预测时间，同时不牺牲模型的准确率。

Inception v2 引入了**分解**来降低 inception 模块中更昂贵的卷积的计算复杂度，并减少从表示瓶颈中损失的信息。图 6.9 描述了表示损失。在这个描述中，我们展示了一个覆盖 25 像素区域的 5 × 5 滤波器。在每次滤波器滑动时，25 像素区域被（表示为）一个单独的像素所替代。在后续池化操作中，对应特征图中的这个单独像素将被减半。表示损失是压缩比，在这种情况下为 50（25 到 0.5）。对于较小的 3 × 3 滤波器，表示损失为 18（9 到 0.5）。

![图像](img/CH06_F09_Ferlitsch.png)

图 6.9 滤波器与后续池化后输出的像素值之间的表示损失

在 Inception v2 模块中，5 × 5 滤波器被两个 3 × 3 滤波器的堆栈所取代，这导致替换的 5 × 5 滤波器的计算复杂度降低了 33%。

此外，当存在滤波器大小的大差异时，会发生表示瓶颈损失。通过用两个 3 × 3 滤波器替换 5 × 5 滤波器，所有非瓶颈滤波器现在具有相同的大小，并且 Inception v2 架构的整体准确率超过 Inception v1。

图 6.10 展示了 Inception v2 块：v1 中的 5 × 5 卷积被两个 3 × 3 卷积所取代。

![图像](img/CH06_F10_Ferlitsch.png)

图 6.10 在 Inception v2 块中，5 × 5 卷积被更高效的两个 3 × 3 卷积堆栈所取代。

Inception v2 还增加了在每个卷积层使用后激活批量归一化（Conv-BN-ReLU）。由于批量归一化直到 2015 年才被引入，因此 2014 年的 Inception v1 没有使用这种技术的优势。图 6.11 展示了添加批量归一化（Conv-ReLU）前后的前一个卷积层和激活之间的差异。

![图像](img/CH06_F11_Ferlitsch.png)

图 6.11 添加批量归一化前后卷积层和激活的比较

以下是一个 Inception v2 模块的代码示例，它与 v1 的不同之处如下：

+   每个卷积层后面都跟着一个批量归一化。

+   v1 的 5 × 5 卷积被两个 3 × 3 卷积所取代，将更昂贵的 5 × 5 分解为更便宜的 3 × 3 卷积对，从而降低了计算复杂性和信息损失，减少了表示瓶颈。

```
x1 = MaxPooling2D((3, 3), strides=(1,1), padding='same')(x)    ❶
x1 = Conv2D(64, (1, 1), strides=(1, 1), padding='same')(x1)
x1 = BatchNormalization()(x1)                                  ❷
x1 = ReLU()(x1)

x2 = Conv2D(64, (1, 1), strides=(1, 1), padding='same')(x)     ❶
x2 = BatchNormalization()(x2)
x2 = ReLU()(x2)

x3 = Conv2D(64, (1, 1), strides=(1, 1), padding='same')(x)     ❶
x3 = BatchNormalization()(x3)
x3 = ReLU()(x3)
x3 = Conv2D(96, (3, 3), strides=(1, 1), padding='same')(x3)
x3 = BatchNormalization()(x3)
x3 = ReLU()(x3)

x4 = Conv2D(64, (1, 1), strides=(1, 1), padding='same')(x)     ❶
x4 = BatchNormalization()(x4)
x4 = ReLU()(x4)
x4 = Conv2D(48, (3, 3), strides=(1, 1), padding='same')(x4)
x4 = BatchNormalization()(x4)
x4 = ReLU()(x4)
x4 = Conv2D(48, (3, 3), strides=(1, 1), padding='same')(x4)
x4 = BatchNormalization()(x4)
x4 = ReLU()(x4)

x = Concatenate([x1, x2, x3, x4])                              ❸
```

❶ 使用后激活批量归一化

❷ Inception 分支，其中 x 是前一层

❸ 将分支的特征图连接在一起

与 Inception v1 模块的 198,000 个参数相比，这些层的 `summary()` 显示需要训练 169,000 个参数。

使用 Idiomatic 程序重用设计模式对 Inception v2 的完整代码实现位于 GitHub 上 ([`mng.bz/oGnd`](https://shortener.manning.com/oGnd))。接下来，我们将描述 Inception 架构在 Inception v3 中的重设计过程。

## 6.3 Inception v3：架构重设计

*Inception v3* 引入了新的宏观架构设计，同时重新设计了主干组，并仅使用一个辅助分类器。Christian Szegedy 等人在其论文标题中提到了这种安排，“重新思考 Inception 架构” ([`arxiv.org/abs/1512.00567`](https://arxiv.org/abs/1512.00567))。

作者指出，近年来在提高准确率和降低参数大小方面取得了显著进展，这既涉及更深层次的架构，也涉及更宽的架构。AlexNet 有 6000 万个参数，VGG 的参数是其三倍，而 Inception v1 的参数只有 500 万。作者强调了需要高效架构，这些架构可以将模型应用于现实世界，并在参数数量减少的同时实现更高的准确率提升。

在他们看来，Inception v1/v2 架构对于这个目的来说过于复杂。例如，为了增加容量而将滤波器组的尺寸加倍，会使参数数量增加四倍。重设计的动机是简化架构，同时在扩展时保持计算收益，并提高当时 SOTA 模型的准确性。

在重设计中，卷积块被重构，以便架构可以高效地扩展。图 6.12 显示了宏观架构：学习组件由三个组（A、B 和 C）组成，用于特征学习，以及两个用于特征图减少的网格减少组。此外，辅助分类器的数量从两个减少到一个。

![](img/CH06_F12_Ferlitsch.png)

图 6.12 重设计的 Inception v3 宏观架构简化了 Inception v1 和 v2 架构。

让我们接下来更详细地看看这些重设计的组件。

### 6.3.1 Inception 组和块

四个设计原则塑造了 Inception 架构的重设计。

1.  避免表示损失。

1.  高维表示更容易在网络中局部处理。

1.  在低维嵌入上执行空间聚合不会在表示能力上造成很大损失。

1.  平衡网络的宽度和深度。

Inception 的微架构反映了第一个设计原则，即通过实现特征图大小的更渐进式减少来减少从表示瓶颈中损失的信息。它还解决了原则 4，即平衡卷积层的宽度和深度。作者观察到，当宽度和深度的增加是并行进行，并且计算预算在这两者之间平衡时，最优的改进发生了。因此，该模型在宽度和深度上同时增加，以贡献于更高品质的网络。

现在让我们进一步放大，看看 A、B 和 C 组的重新设计。A 组的块与早期版本相同，而 B 组和 C 组有所不同。A、B 和 C 组的输出特征图大小分别为 35 × 35、17 × 17 和 8 × 8。注意特征图大小的逐渐减少，因为每个组将*H* × *W*减半。这种在三组中的逐渐减少反映了设计原则 1，即减少表示损失，因为网络变得更深。

图 6.13 和 6.14 分别显示了 B 组和 C 组的块。在这两个组中，一些*N* × *N*卷积被分解为*N* × 1 和 1 × N 的空间可分离卷积。这种调整反映了设计原则 3，即当在特征图的较低维度上执行时，空间可分离卷积不会丢失表示能力。

![图像](img/CH06_F13_Ferlitsch.png)

图 6.13 使用空间可分离卷积的 Inception v3 块 17 × 17（组 B）

![图像](img/CH06_F14_Ferlitsch.png)

图 6.14 使用并行空间可分离卷积的 Inception v3 块 8 × 8（组 C）

在 Inception 的早期版本中，为了减少维度，卷积组之间的特征图进行了池化，数量加倍，研究人员认为这导致了表示损失（设计原则 1）。他们提出，组之间的特征图减少阶段可以用并行卷积和池化来完成，如图 6.15 所示。然而，在 Inception v3 之后，这种在减少过程中消除表示损失的方法没有进一步追求。

![图像](img/CH06_F15_Ferlitsch.png)

图 6.15 使用特征图并行池化以减少表示损失

A 组和 B 组之后的并行池化分别显示在图 6.16 和 6.17 中。被称为*网格减少*的这些池化块将前一个组的输出特征图（或通道）的数量减少，以匹配下一个组的输入。因此，网格减少块 A 从 35 × 35 减少到 17 × 17，网格减少块 B 从 17 × 17 减少到 8 × 8（满足设计原则 1）。

![图像](img/CH06_F16_Ferlitsch.png)

图 6.16 Inception v3 网格减少块 17 × 17（组 A）

此外，反映设计原则 3，7 × 7 卷积，以及 B 组和 C 组中的一些 3 × 3 卷积，以及 B 组中的网格减少，分别被(7 × 1, 1 × 7)和(3 × 1, 1 × 3)的空间卷积所取代。

![](img/CH06_F17_Ferlitsch.png)

图 6.17 Inception v3 网格减少块 8 × 8（组 B）

现在我们来比较 Inception v1 和 v2 中的正常卷积，我们将其简单地称为正常卷积，与 v3 中的空间可分离卷积。

### 6.3.2 正常卷积

在*正常卷积*中，核（例如，3 × 3）应用于高度(*H*)、宽度(*W*)和深度(*D*)通道。每次核移动时，矩阵乘法操作的次数等于像素的数量，即*H* × *W* × *D*。

例如，一个具有三个通道的 RGB 图像（应用 3 × 3 核到所有三个通道）使用了 3 × 3 × 3 = 27 次矩阵乘法（matmul）操作，生成一个*N* × *M* × 1（例如，8 × 8 × 1）的特征图（每个核），其中*N*和*M*是特征图的结果高度和宽度；参见图 6.18

![](img/CH06_F18_Ferlitsch.png)

图 6.18 带有一个滤波器的填充卷积

如果我们指定卷积的输出为 256 个滤波器，我们就有 256 个核需要训练。在 RGB 示例中使用 256 个 3 × 3 核的情况下，这意味着每次核移动时需要进行 6912 次矩阵乘法操作；参见图 6.19。因此，即使核大小很小（3 × 3），随着我们增加输出特征图的数量以获得更强的表示能力，正常卷积的计算成本也会变得很高。

![](img/CH06_F19_Ferlitsch.png)

图 6.19 带有多个滤波器的填充卷积

### 6.3.3 空间可分离卷积

相比之下，*空间可分离卷积*将 2D 核（例如，3 × 3）分解为两个较小的 1D 核。如果我们用*H* × *W*表示 2D 核，那么分解的两个较小的 1D 核将是*H* × 1 和 1 × *W*。这种分解将计算总量减少了一半。虽然这种分解并不总是保持表示等价性，但研究人员证明了他们在 Inception v3 中能够保持表示等价性。图 6.20 比较了正常卷积和可分离卷积。

在使用 3 × 3 核的 RGB 示例中，每次核移动时，正常卷积会有 3 × 3 × 3（通道）= 27 次矩阵乘法操作。在相同的 RGB 示例中，使用因式分解的 3 × 3 核，空间可分离卷积每次核移动时会有(3 × 1 × 3) + (1 × 3 × 3) = 18 次矩阵乘法操作。因此，矩阵乘法操作的数量减少了三分之一（18 / 27）。

![](img/CH06_F20_Ferlitsch.png)

图 6.20 正常卷积与空间可分离卷积的比较

在使用 256 个 3 × 3 核的 RGB 示例中，每次核移动时我们有 4608 次矩阵乘法操作，与正常卷积相比，正常卷积会有 6912 次矩阵乘法操作。

### 6.3.4 茎重新设计和实现

到 Inception v3 被设计出来的时候，用两个 3 × 3 的卷积堆叠替换粗略的 5 × 5 滤波器已经成为一种常见的做法，这比单个 5 × 5 滤波器计算量更小（18 次矩阵乘法操作与 25 次相比）并且保留了表示能力。使用同样的原理，作者理论化地认为，一个计算成本较高的 7 × 7 粗略级卷积（每次移动 49 次矩阵乘法），可以被三个 3 × 3 卷积的堆叠（每次移动 27 次矩阵乘法）所替代。这减少了茎组件中的参数数量，同时保留了表示能力。

![](img/CH06_F21_Ferlitsch.png)

图 6.21 Inception v3 茎组，由替换 v1/v2 中 7 × 7 卷积的三个 3 × 3 卷积堆叠组成

图 6.21 展示了茎卷积组中的 7 × 7 卷积是如何分解并替换为三个 3 × 3 卷积的堆叠的，如下所示：

1.  第一个 3 × 3 是一个步长卷积（`strides=2, 2`），它执行特征图减少。

1.  第二个 3 × 3 是一个常规卷积。

1.  第三个 3 × 3 将滤波器的数量翻倍。

Inception v3 将是基于分解（或非分解）的 7 × 7 粗滤波器的最后一批 SOTA 模型之一。今天的当前做法是分解（或非分解）的 5 × 5。

下一个代码示例是实现 Inception v3 的茎组，它包括以下内容：

1.  三个 3 × 3 卷积的堆叠（分解的 7 × 7），其中第一个卷积用于特征池化（输入形状的 25%大小）。

1.  一个最大池化层用于进一步减少特征图的维度（输入形状的 6%大小）。

1.  1 × 1 线性投影卷积将特征图的数量从 64 扩展到 80。

1.  一个 3 × 3 的卷积用于进一步将维度扩展到 192 个特征图。

1.  第二个最大池化层用于进一步减少特征图的维度（输入形状的 1.5%大小）。

```
x = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(input)     ❶
x = BatchNormalization()(x)
x = ReLU()(x)
x = Conv2D(32, (3, 3), strides=(1, 1), padding='same')(x)
x = BatchNormalization()(x)
x = ReLU()(x)
x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)
x = BatchNormalization()(x)
x = ReLU()(x)

# max pooling layer
x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x) 

x = Conv2D(80, (1, 1), strides=(1, 1), padding='same')(x)         ❷
x = BatchNormalization()(x)
x = ReLU()(x)

x = Conv2D(192, (3, 3), strides=(1, 1), padding='same')(x)        ❸
x = BatchNormalization()(x)
x = ReLU()(x)

x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)       ❹
```

❶ Inception v3 茎，7 × 7 被替换为一个 3 × 3 卷积的堆叠

❷ 一个 1 × 1 线性投影卷积

❸ 特征图扩展（维度扩展）

❹ 特征图池化（维度减少）

对于茎组的一个`summary()`显示有 614,000 个参数用于输入（229, 229, 3）训练。

### 6.3.5 辅助分类器

Inception v3 的另一个变化是将两个辅助分类器减少到一个，并进一步简化，如图 6.22 所示。作者解释说，他们做出这些改变是因为“他们发现辅助分类器在训练初期并没有导致收敛性的改善。”通过保留单个分类器，他们似乎瞄准了中间点。

![](img/CH06_F22_Ferlitsch.png)

图 6.22 Inception v3 辅助组

同样，他们采用了当时的一种惯例，在最终的分类器之前移除额外的密集层，进一步减少参数。早期的研究人员已经确立，移除额外的密集层（在分类密集层之前）不会导致准确度下降。

辅助分类器进一步简化为以下内容：

+   一个平均池化层（`AveragePooling2D`），将每个特征图减少到一个 1 × 1 的矩阵

+   一个 3 × 3 卷积层（`Conv2D`），输出 768 个 1 × 1 特征图

+   一个展平层（`Flatten`）将特征图展平为 768 个元素的 1D 向量

+   一个用于分类的最终密集层（`Dense`）

使用 Idiomatic 过程重用设计模式为 Inception v3 编写的完整代码可以在 GitHub 上找到（[`mng.bz/oGnd`](http://mng.bz/oGnd)）。

## 6.4 ResNeXt：宽残差神经网络

Facebook AI Research 的*ResNeXt*，是 2016 年 ILSVRC 竞赛 ImageNet 的第二名，引入了一个使用分割-变换-合并模式进行并行卷积的宽残差块。这种并行卷积的架构被称为*组卷积*。

并行卷积的数量构成了宽度，称为*基数*。例如，在 2016 年的比赛中，ResNeXt 架构使用了 32 个基数，这意味着每个 ResNeXt 层由 32 个并行卷积组成。

这里的想法是添加并行卷积可以帮助模型在不加深层的情况下提高准确性，而加深层更容易导致记忆化。在他们的消融研究中（[`arxiv.org/abs/1611.05431`](https://arxiv.org/abs/1611.05431)），Saining Xie 等人比较了 ResNeXt 与 ResNet 的 50、101 和 200 层，以及 Inception v3 的 101 和 200 层在 ImageNet 数据集上的表现。在所有情况下，相同深度层的 ResNeXt 架构都实现了更高的准确性。

如果你查看预训练模型存储库，例如 TensorFlow Hub，你可以看到 SE-ResNeXt 变体具有略高的计算量和更高的准确性，并且被选为图像分类的主干。

### 6.4.1 ResNeXt 块

在每个 ResNeXt 层中，从前一层的输入被分配到并行卷积中，每个卷积的输出（特征图）被连接回一起。最后，将层的输入矩阵加到连接的输出（恒等连接）上，形成残差块。这一系列层被称为*分割-变换-合并*和*缩放*操作。定义这些术语将有助于阐明操作：

+   *分割*指的是根据基数将特征图分成组。

+   *变换*是指每个组中并行卷积中发生的事情。

+   *合并*指的是结果特征图的连接操作。

+   *缩放*表示恒等连接中的加法操作。

分割-变换-合并操作的目标是在不增加参数的情况下提高准确性。它是通过将基本变换（*w* × *x*）转换为网络内神经元聚合变换来实现的。

现在来看实现这些概念的架构。如图 6.23 所示，ResNeXt 的宽残差块组包括以下内容：

+   一个第一瓶颈卷积（1 × 1 核）

+   一个基数*N*的分割-分支-连接卷积（组卷积）

+   一个最终的瓶颈卷积（1 × 1 核）

+   输入和最终卷积输出之间的恒等连接（快捷连接）

![图像](img/CH06_F23_Ferlitsch.png)

图 6.23 实现分割-变换-合并和缩放操作的具有恒等快捷连接的残差块

让我们更详细地看看组卷积的分割-变换-合并操作（图 6.24）。以下是三个主要步骤的应用方式：

1.  分割：输入（特征图）被均匀分割成*N*组（其中*N*是基数）。

1.  变换：每个组通过一个独立的 3 × 3 卷积。

1.  合并：将所有变换后的组连接在一起。

![图像](img/CH06_F24_Ferlitsch.png)

图 6.24 实现分割-变换-合并操作的 ResNeXt 组卷积

第一个瓶颈卷积通过减少（压缩）输入特征图的数量来执行维度降低。我们在第五章查看瓶颈残差块以及在 6.1 至 6.3 节查看 Inception 模块时看到了瓶颈卷积的类似用途。

在瓶颈卷积之后，特征图根据基数分配到并行卷积中。例如，如果输入特征图的数量（或通道数）为 128，基数是 32，每个并行卷积将获得 4 个特征图，这是特征图数量除以基数，即 128 除以 32。

然后将并行卷积的输出连接回完整的特征图集合，然后通过最终的瓶颈卷积进行另一轮维度降低。与残差块类似，输入到 ResNeXt 块和从 ResNeXt 块输出的输入之间存在恒等连接，然后进行矩阵加法。

以下是一个 ResNeXt 块的编码示例，它由四个代码序列组成：

1.  块输入（快捷连接）通过一个 1 × 1 瓶颈卷积进行维度降低。

1.  分割-变换操作（组卷积）。

1.  合并操作（连接）。

1.  输入与合并操作的输出（恒等连接）矩阵相加作为缩放操作。

```
shortcut = x                                                               ❶
                                                                           ❶
x = Conv2D(filters_in, (1, 1), strides=(1, 1), padding='same')(shortcut)   ❶
x = BatchNormalization()(x)
x = ReLU()(x)

filters_card = filters_in // cardinality                                   ❷

groups = []                                                                ❸
for i in range(cardinality):                                               ❸
    group = Lambda(lambda z: z[:, :, :, i * filters_card:i *               ❸
                             filters_card + filters_card])(x)              ❸
    groups.append(Conv2D(filters_card, (3, 3), strides=(1, 1),             ❸
                         padding='same')(group))                           ❸

x = Concatenate()(groups)                                                  ❹
x = BatchNormalization()(x)
x = ReLU()(x)

x = Conv2D(filters_out, (1, 1), strides=(1, 1), padding='same')(x)         ❺
x = BatchNormalization()(x)

x = Add()([shortcut, x])                                                   ❻
x = ReLU()(x)
return x
```

❶ 短路连接是一个用于维度降低的 1 × 1 瓶颈卷积

❷ 通过除以基数（宽度）大小来计算每个组的通道数

❸ 执行分割-变换步骤

❹ 通过连接组卷积的输出执行合并步骤

❺ 1 × 1 线性投影以恢复维度

❻ 将快捷连接添加到块的输出中

注意：在此代码列表中，`Lambda()`方法执行特征图的分割。序列`z[:, :, :, i * filters_card:i * filters_card + filters_card]`是一个滑动窗口，它沿着第四维分割输入特征图；第四维是通道 *B* × *H* × *W* × *C*。

### 6.4.2 ResNeXt 架构

如图 6.25 所示，该架构从输入的茎卷积组开始，包括一个 7×7 的卷积，然后通过最大池化层来减少数据。

![](img/CH06_F25_Ferlitsch.png)

图 6.25 展示了 ResNeXt 学习组件在卷积组之间的特征池化

在茎卷积组之后是四个 ResNeXt 块组。每个组相对于输入逐步将输出的滤波器数量翻倍。在每个块之间是一个步长卷积，它有两个作用：

+   它将数据减少 75%（特征池化）。

+   它将前一层输出的滤波器数量翻倍，因此当在当前层的输入和输出之间建立恒等连接时，滤波器的数量与矩阵加法操作匹配。

在最终的 ResNeXt 组之后，输出被传递到分类组件。分类器由一个最大池化层和一个展平层组成，展平层将输入展平成一个一维向量，然后将其传递到一个单层密集层进行分类。

在 GitHub 上（[`mng.bz/my6r`](http://mng.bz/my6r)）有一个使用 Idiomatic procedure reuse 设计模式为 ResNeXt 编写的完整代码示例。

## 6.5 宽残差网络

2016 年由巴黎理工学院的学者们提出的**宽残差网络**（*WRN*），对宽卷积神经网络采取了另一种方法。研究者们从理论出发，认为随着模型层级的加深，特征复用减少，因此训练时间更长。他们使用残差网络进行了一项研究，并为每个残差块中的滤波器数量（宽度）添加了一个乘数参数。这减少了网络的深度。当他们对这种设计进行测试时，发现只有 16 层的 WRN 就能超越其他 SOTA 架构。

很快，一个名为 DenseNet 的设计将展示另一种处理深层特征复用的方法。与 WRN 类似，DenseNet 基于增加特征复用将导致更强的表示能力和更高的准确性的假设。然而，DenseNet 通过将输入与每个残差块的输出进行特征图拼接来实现复用。

在他们的消融研究中，“Wide Residual Networks”([`arxiv.org/pdf/1605.07146.pdf`](https://arxiv.org/pdf/1605.07146.pdf))，Sergey Zagoruyko 和 Nikos Komodakis 将他们的加宽原则应用于 ResNet50，他们称之为 WRN-50-2，并发现它优于更深层的 ResNet101。今天的 SOTA 模型采用使用宽层和深层的原则来实现更高的性能、更快的训练和更少的记忆化。

### 6.5.1 WRN-50-2 架构

该 WRN 模型采用了以下设计考虑：

1.  使用预激活批量归一化（BN-RE-Conv）以实现更快的训练，就像在 ResNet v2 中一样。

1.  使用两个 3 × 3 卷积（B(3, 3)），如 ResNet34 中所示，而不是 ResNet50 中更不具表现力的瓶颈残差块（B(1,3,1)）。这里的理由是基于瓶颈设计有助于减少参数以增加准确性的事实，*随着网络的加深*。通过*更宽*来提高准确性，网络变得更浅，因此可以保留更具表现力的堆叠。

1.  用 l 表示每个组中卷积层的数量，用*k*表示乘以过滤器数量的宽度因子。

1.  将 dropout 操作从顶层（这是惯例）移动到残差块中的每个卷积层之间以及 ReLU 之后。这里的推理是为了扰动批量归一化。

在图 6.26 的宏观架构中，你可以看到这三个原则在起作用，因此每个卷积组将输出特征的数量翻倍。每个卷积都使用预激活批量归一化（设计原则 1）。组内的每个残差块使用 B(3,3)残差块（设计原则 2）。并且元参数*k*用于每个卷积的过滤器数量宽度乘数（设计原则 3）。未展示的是残差块中的 dropout（设计原则 4）。

![](img/CH06_F26_Ferlitsch.png)

图 6.26 在 WRN 宏观架构中，每个卷积组逐渐将输出特征图的数量翻倍。

### 6.5.2 宽残差块

让我们关注宽残差块，它由各种残差组组成。图 6.27 显示，两个 3 × 3 卷积（B(3,3)）的过滤器数量都乘以一个可配置的宽度因子(*k*)。在 3 × 3 卷积之间是一个用于块级正则化的 dropout 层。否则，宽残差块的设计与 ResNet34 残差块相同。

![](img/CH06_F27_Ferlitsch.png)

图 6.27 带有标识快捷方式的宽残差块

这里是一个宽残差块的编码示例：

```
shortcut = x                                 ❶

x = BatchNormalization()(x)                  ❷
x = ReLU()(x)
x = Conv2D(filters_out, (3, 3), strides=(1, 1), padding='same')(x) 

x = BatchNormalization()(x)                  ❸
x = ReLU()(x)
x = Dropout(rate)(x)                         ❹
x = Conv2D(filters_out, (3, 3), strides=(1, 1), padding='same')(x)

x = Add()([shortcut, x])                     ❺
return x
```

❶ 记住输入

❷ 第一个 3 × 3 卷积使用预激活批量归一化

❸ 第二个 3 × 3 卷积使用预激活批量归一化

❹ ReLU 之后的 dropout 以扰动批量归一化

❺ 标识链接，将输入添加到块的输出

使用 Idiomatic 过程重用设计模式为 WRN 提供完整代码实现可在 GitHub 上找到 ([`mng.bz/n2oa`](https://shortener.manning.com/n2oa)).

## 6.6 超越计算机视觉：结构化数据

让我们看看结构化数据模型中宽度和深度层概念是如何演变的。在 2016 年之前，大多数结构化数据的应用继续使用经典机器学习方法，而不是深度学习。与计算机视觉中使用的非结构化数据不同，结构化数据具有多种输入，包括数值、分类和特征工程。这种输入范围意味着在密集层的层中深入挖掘并不那么有效，以使模型学习输入特征和相应标签之间的非线性关系。

图 6.28 展示了在 2016 年之前将深度学习应用于结构化数据的方法。在这种方法中，所有特征输入都通过一系列密集层进行处理——深入其中，隐藏的密集层本质上就是学习器。最后密集层的输出随后传递到任务组件。任务组件与计算机视觉中的任务组件相当。最后密集层的输出已经是一个一维向量。该向量可能还会进行一些额外的池化操作，然后传递到一个具有对应于任务的激活函数的最终密集层：对于回归，使用线性或 ReLU；对于二分类，使用 sigmoid；对于多分类，使用 softmax。

![图片](img/CH06_F28_Ferlitsch.png)

图 6.28 在 2016 年之前，结构化数据模型的方法使用了深层 DNN。

对于结构化数据，你希望学习记忆和泛化。记忆是学习特征值的共现（协变关系）。泛化是学习在训练数据分布中没有看到但部署时会在数据分布中看到的新特征组合。

我将使用人脸检测来阐述记忆和泛化的区别。在记忆中，网络的某些部分学会锁定特定样本群（例如，特定的眼睛模式或肤色）的模式。一旦锁定，这部分网络将在相似示例上发出极高的置信度信号，但在可比示例上置信度较低。随着越来越多的神经网络被锁定，神经网络退化成决策树。这类似于经典 AI 中专家系统的规则集。如果一个示例与专家编码的模式匹配，它就会被识别。否则，它无法被识别。

例如，假设神经网络锁定在眼睛、肤色、穿孔、眼镜、帽子、头发遮挡和面部毛发等模式上。然后我们提交一张有面部彩绘的孩子的图像，模型无法识别出人脸。然后你可以使用面部彩绘的图像重新训练，但由于锁定，你需要进一步增加模型的参数容量来记忆新的模式。然后还有另一个模式和另一个模式——这就是专家系统的问题所在。

在泛化过程中，冗余的节点簇弱信号识别模式，并在模型中集体作为集成。模型中冗余弱信号簇越多，模型泛化以识别未训练过的模式的可能性就越大。

然后，在 2016 年，谷歌研究发布了宽度和深度网络模型架构及其相应的论文，“推荐系统中的宽度和深度学习”由 Heng-Tze Cheng 等人撰写([`arxiv.org/pdf/1606.07792.pdf`](https://arxiv.org/pdf/1606.07792.pdf))。虽然这篇论文是针对改进推荐模型而特定的，但这种模型已被广泛应用于不同结构化数据模型类型。推荐是一个有趣的挑战，因为它同时利用了泛化和记忆化。目标是进行具有高价值转换的利基推荐，这需要泛化，除了记忆化广泛常见的共现之外。宽度和深度架构在一个模型中结合了记忆化和泛化。它本质上是由两个在任务组件中结合的模型组成。

图 6.29 展示了宽度和深度架构。这个架构也是一个*多模态*架构，因为它接受两种不同类型的两个单独输入。

![](img/CH06_F29_Ferlitsch.png)

图 6.29 展示了输入在宽层和深度层之间的分配，以及层的输出结合到任务组件中。

让我们更深入地探讨这个架构。学习组件由两部分组成，一个多层深度神经网络和一个单层宽密集层。宽密集层充当线性回归器并记忆高频共现。深度神经网络学习非线性并推广到低频（稀疏）共现以及训练数据中未出现的共现。宽密集层的输入是基础特征（非交叉特征），这些特征已经过特征预处理，并转换为转换特征（例如，分类特征的独热编码）。它们直接输入到宽密集层，因此没有主干。多层密集神经网络的输入是基础特征和交叉特征。在这种情况下，一个主干组件通过使用编码器将组合特征转换为嵌入。

然后，从宽密集层和多层深度神经网络输出的结果在任务组件中结合，并且可能还会进行额外的池化。任务组件基本上与计算机视觉模型中的相同。宽密集层和多层深度神经网络层一起训练。

## 摘要

+   减少深层网络中记忆化暴露的一种方法是使用并行卷积。这允许使用更浅的卷积神经网络来解决过拟合问题。

+   当卷积设计模式可以被重构为计算成本更低且更小（在参数数量上）的另一种模式时，就会发生表示等价。通过分解，模型在更少的计算需求下保持了相同水平的信息（或特征）提取。这允许模型更小、训练更快，并减少预测的延迟。

+   在 Inception 设计中引入了将常规卷积重构为计算量更小的空间可分离卷积的概念。Inception 在 ImageNet 数据集上展示了在保持性能目标方面的表示等价性。

+   ResNeXt 在并行分组卷积中引入了分割-变换-合并模式。这种模式在不加深层的情况下提高了先前残差网络的准确性。

+   将批归一化从 WRN 的后激活阶段移动到前激活阶段的目的是提高模型准确性。前激活批归一化进一步减少了需要加深层的必要性，从而减少了防止过拟合的正则化需求。前激活方法提高了训练速度，使得可以使用略高的学习率来实现可比的收敛。

+   在 WRN 中添加了一个宽度乘数作为元参数，用于在浅层宽残差网络中寻找宽度，从而得到一个在准确性方面（即准确度）与更深层的残差网络表现相当（或一样好）的模型。

+   现代用于结构化数据的深度学习模型同时使用宽层和深层；宽层负责记忆，而深层负责泛化。
