# 六、机器学习的通用工作流程

本章涵盖

+   制定机器学习问题的步骤

+   开发一个可工作模型的步骤

+   部署模型到生产环境并进行维护的步骤

我们先前的示例假设我们已经有一个标记的数据集可以开始训练模型。在现实世界中，这通常不是这样。您不是从数据集开始，而是从问题开始。

想象一下，您正在开设自己的机器学习咨询公司。您注册公司，建立一个漂亮的网站，通知您的网络。项目开始涌入：

+   为图片分享社交网络设计的个性化照片搜索引擎——输入“婚礼”并检索您在婚礼上拍摄的所有照片，无需任何手动标记。

+   在新兴聊天应用的帖子中标记垃圾邮件和冒犯性文本内容。

+   为在线广播用户构建音乐推荐系统。

+   为电子商务网站检测信用卡欺诈。

+   预测显示广告的点击率，以决定在特定时间向特定用户提供哪个广告。

+   在饼干制造线上的传送带上标记异常的饼干。

+   使用卫星图像预测尚未知晓的考古遗址的位置。

伦理注意事项

有时您可能会被提供道德上可疑的项目，比如“构建一个从某人面部照片中评估其信任度的 AI”。首先，该项目的有效性存疑：不清楚为什么信任度会反映在某人的脸上。其次，这样的任务打开了各种道德问题的大门。为这个任务收集数据集将等同于记录标记图片的人的偏见和成见。您将在这些数据上训练的模型只会将这些偏见编码到一个黑盒算法中，给予它们一层薄薄的合法性外衣。在我们这样一个技术素养较低的社会中，“AI 算法说这个人不可信”似乎比“约翰·史密斯说这个人不可信”更具权威性和客观性，尽管前者只是对后者的学习近似。您的模型将在规模上洗钱和操作化人类判断的最糟糕的方面，对真实人们的生活产生负面影响。

技术从来都不是中立的。如果您的工作对世界产生任何影响，这种影响具有道德方向：技术选择也是伦理选择。始终要谨慎选择您希望您的工作支持的价值观。

如果您可以从`keras.datasets`导入正确的数据集并开始拟合一些深度学习模型将会非常方便。不幸的是，在现实世界中，您将不得不从头开始。

在本章中，您将学习一个通用的逐步蓝图，您可以使用它来处理和解决任何机器学习问题，就像前面列表中的问题一样。这个模板将汇集和巩固您在第四章和第五章中学到的一切，并为您提供更广泛的背景，以便扎实您将在接下来的章节中学到的内容。

机器学习的通用工作流程大致分为三个部分：

1.  *定义任务*—理解问题领域和客户要求背后的业务逻辑。收集数据集，了解数据代表什么，并选择如何衡量任务的成功。

1.  *开发模型*—准备数据以便机器学习模型处理，选择一个模型评估协议和一个简单的基准来超越，训练一个具有泛化能力且可以过拟合的第一个模型，然后正则化和调整您的模型，直到实现最佳的泛化性能。

1.  *部署模型*—向利益相关者展示您的工作，将模型部署到 Web 服务器、移动应用程序、网页或嵌入式设备，监控模型在实际环境中的性能，并开始收集构建下一代模型所需的数据。

让我们深入了解。

## 6.1 定义任务

没有对您正在做的事情的背景有深刻的理解，您无法做出好的工作。您的客户为什么要解决这个特定的问题？他们将从解决方案中获得什么价值——您的模型将如何使用，它将如何融入客户的业务流程？有哪些可用的数据，或者可以收集哪些数据？什么样的机器学习任务可以映射到业务问题？

### 6.1.1 界定问题

构建机器学习问题通常需要与利益相关者进行许多详细的讨论。以下是您应该牢记的问题：

+   您的输入数据将是什么？您试图预测什么？只有在有训练数据可用的情况下，您才能学会预测某些东西：例如，只有在有电影评论和情感注释可用时，您才能学会对电影评论的情感进行分类。因此，在这个阶段，数据可用性通常是限制因素。在许多情况下，您将不得不自己收集和注释新的数据集（我们将在下一节中介绍）。

+   您面临的是什么类型的机器学习任务？是二元分类？多类分类？标量回归？向量回归？多类别、多标签分类？图像分割？排名？还是其他类型，如聚类、生成或强化学习？在某些情况下，可能机器学习甚至不是理解数据的最佳方式，您应该使用其他方法，比如传统的统计分析。

    +   照片搜索引擎项目是一个多类别、多标签分类任务。

    +   垃圾邮件检测项目是一个二元分类任务。如果将“具有攻击性内容”设置为单独的类别，则它是一个三分类任务。

    +   音乐推荐引擎最终最好不是通过深度学习来处理，而是通过矩阵分解（协同过滤）。

    +   信用卡欺诈检测项目是一个二元分类任务。

    +   点击率预测项目是一个标量回归任务。

    +   异常饼干检测是一个二元分类任务，但它还需要一个对象检测模型作为第一阶段，以正确裁剪原始图像中的饼干。请注意，被称为“异常检测”的一组机器学习技术在这种情况下并不适用！

    +   从卫星图像中寻找新的考古遗址项目是一个图像相似度排名任务：您需要检索看起来最像已知考古遗址的新图像。

+   现有解决方案是什么样的？也许您的客户已经有一个手工制作的算法来处理垃圾邮件过滤或信用卡欺诈检测，其中有很多嵌套的`if`语句。也许目前是一个人手动处理正在考虑的过程——监控饼干工厂的传送带并手动移除坏饼干，或者制作歌曲推荐播放列表以发送给喜欢特定艺术家的用户。您应该确保了解已经存在的系统以及它们是如何工作的。

+   您是否需要处理特定的约束？例如，您可能会发现您正在为其构建垃圾邮件检测系统的应用程序严格端到端加密，因此垃圾邮件检测模型将不得不存在于最终用户的手机上，并且必须在外部数据集上进行训练。也许饼干过滤模型有延迟约束，因此它将需要在工厂的嵌入式设备上运行，而不是在远程服务器上运行。您应该了解您的工作将适用的完整背景。

一旦你完成了研究，你应该知道你的输入是什么，你的目标是什么，以及问题映射到什么类型的机器学习任务。在这个阶段要注意你所做的假设：

+   你假设可以根据输入来预测目标。

+   你假设可用的数据（或者你即将收集的数据）足够信息丰富，可以学习输入和目标之间的关系。

在你有一个可用的模型之前，这些只是等待验证或无效的假设。并不是所有问题都可以通过机器学习解决；仅仅因为你已经收集了输入 X 和目标 Y 的示例并不意味着 X 包含足够的信息来预测 Y。例如，如果你试图根据股票市场的最近价格历史来预测股票的走势，你很可能不会成功，因为价格历史并不包含太多预测信息。

### 6.1.2 收集数据集

一旦你了解了任务的性质，知道你的输入和目标将是什么，就是数据收集的时候了——大多数机器学习项目中最费力、耗时和昂贵的部分。

+   图片搜索引擎项目要求你首先选择你想要分类的标签集——你选择了 1 万个常见的图像类别。然后你需要手动为数十万张过去用户上传的图片打上这个标签集中的标签。

+   对于聊天应用的垃圾信息检测项目，由于用户聊天是端到端加密的，你无法使用其内容来训练模型。你需要获得一个包含数万条未经过滤的社交媒体帖子的独立数据集，并手动将其标记为垃圾信息、冒犯性内容或可接受内容。

+   对于音乐推荐引擎，你可以只使用用户的“喜欢”。不需要收集新数据。同样对于点击率预测项目：你有过去多年广告的点击率记录。

+   对于饼干标记模型，你需要在传送带上方安装摄像头，收集数万张图片，然后需要有人手动标记这些图片。目前知道如何做这件事的人在饼干工厂工作，但似乎并不太困难。你应该能够训练人员来做这件事。

+   卫星图像项目将需要一个考古学家团队收集一个现有感兴趣地点的数据库，对于每个地点，你需要找到在不同天气条件下拍摄的现有卫星图像。为了得到一个好的模型，你需要成千上万个不同的地点。

你在第五章学到，模型的泛化能力几乎完全来自于它所训练的数据的属性——你拥有的数据点数量，标签的可靠性，特征的质量。一个好的数据集是值得关注和投资的资产。如果你有额外的 50 个小时用于项目，最有效的分配方式很可能是收集更多数据，而不是寻找渐进的建模改进。

数据比算法更重要的观点最著名的是由谷歌研究人员在 2009 年发表的一篇名为“数据的不合理有效性”的论文中提出的（标题是对尤金·维格纳 1960 年文章“数学在自然科学中的不合理有效性”进行的改编）。这是在深度学习流行之前，但令人惊讶的是，深度学习的兴起只使数据的重要性更加突出。

如果你正在进行监督学习，那么一旦你收集了输入（如图片），你将需要为它们提供*注释*（如这些图片的标签）—你将训练模型来预测的目标。有时，注释可以自动获取，比如音乐推荐任务或点击率预测任务的注释。但通常你需要手动为数据进行注释。这是一个劳动密集型的过程。

投资于数据标注基础设施

你的数据标注流程将决定你的目标的质量，进而决定你的模型的质量。仔细考虑你可用的选项：

+   你应该自己标注数据吗？

+   你应该使用类似 Mechanical Turk 这样的众包平台来收集标签吗？

+   是否应该使用专门的数据标注公司的服务？

外包可能会节省你时间和金钱，但会带走控制权。使用类似 Mechanical Turk 这样的东西可能会便宜且扩展性好，但你的标注可能会变得很嘈杂。

要选择最佳选项，考虑你正在处理的限制条件：

+   数据标注员是否需要是专业领域专家，还是任何人都可以标注数据？猫狗图像分类问题的标签可以由任何人选择，但狗品种分类任务的标签需要专业知识。同时，标注骨折的 CT 扫描几乎需要医学学位。

+   如果标注数据需要专业知识，你能培训人员来做吗？如果不能，你如何获得相关专家的帮助？

+   你自己是否了解专家如何进行标注？如果不了解，你将不得不将数据集视为黑匣子，你将无法进行手动特征工程—这并非关键，但可能会有限制。

如果决定在内部标注数据，问问自己你将使用什么软件记录标注。你可能需要自己开发这个软件。高效的数据标注软件会为你节省大量时间，因此在项目早期投资于此是值得的。

警惕非代表性数据

机器学习模型只能理解与之前看到的输入类似的内容。因此，训练使用的数据应该*代表*生产数据。这个问题应该是所有数据收集工作的基础。

假设你正在开发一个应用，用户可以拍摄一盘食物的照片以找出菜名。你使用来自食客常用的图片分享社交网络的图片来训练模型。在部署时，愤怒用户的反馈开始涌入：你的应用在 10 次中有 8 次答错！怎么回事？你在测试集上的准确率远远超过 90%！快速查看用户上传的数据发现，随机餐馆的随机菜品用随机手机拍摄的移动图片看起来与你训练模型的专业质量、光线充足、诱人的图片完全不同：*你的训练数据不代表生产数据*。这是一个致命的错误—欢迎来到机器学习地狱。

如果可能的话，直接从模型将要使用的环境中收集数据。电影评论情感分类模型应该用于新的 IMDB 评论，而不是 Yelp 餐厅评论，也不是 Twitter 状态更新。如果你想评价一条推文的情感，首先收集和标注来自与你预期在生产中的用户类似的一组用户的实际推文。如果无法在生产数据上进行训练，那么确保你充分了解你的训练和生产数据的差异，并积极纠正这些差异。

你应该注意的一个相关现象是*概念漂移*。你几乎会在所有涉及用户生成数据的真实世界问题中遇到概念漂移。概念漂移发生在生产数据的属性随时间变化，导致模型准确性逐渐下降的情况下。2013 年训练的音乐推荐引擎可能在今天效果不佳。同样，你使用的 IMDB 数据集是在 2011 年收集的，基于它训练的模型可能在处理 2020 年的评论时表现不如处理 2012 年的评论，因为词汇、表达和电影类型随时间演变。概念漂移在像信用卡欺诈检测这样的对抗性环境中尤为严重，那里的欺诈模式几乎每天都在变化。处理快速概念漂移需要不断进行数据收集、标注和模型重新训练。

请记住，机器学习只能用于记忆训练数据中存在的模式。你只能识别以前见过的内容。使用过去数据训练的机器学习来预测未来是假设未来会像过去一样。这通常并非如此。

抽样偏差问题

一个特别阴险且常见的非代表性数据案例是*抽样偏差*。当你的数据收集过程与你试图预测的内容相互作用，导致测量结果出现偏差时，就会发生抽样偏差。一个著名的历史例子发生在 1948 年的美国总统选举中。选举当晚，芝加哥论坛报刊登了“杜威击败杜鲁门”的头条新闻。第二天早上，杜鲁门被宣布为胜者。论坛报的编辑信任了一项电话调查的结果——但 1948 年的电话用户并不是选民群体的随机代表样本。他们更有可能是富裕、保守派的人，更有可能投票给共和党候选人杜威。

![](img/06-UN01.png)

“杜威击败杜鲁门”：抽样偏差的一个著名例子

如今，每次电话调查都会考虑到抽样偏差。这并不意味着在政治民意调查中抽样偏差已经成为过去的事情——相反，与 1948 年不同，民意调查员现在已经意识到这一点并采取措施加以纠正。

### 6.1.3 理解你的数据

将数据集视为黑匣子是相当糟糕的做法。在开始训练模型之前，你应该探索和可视化数据，以获取关于数据预测性的见解，这将为特征工程提供信息，并筛选潜在问题。

+   如果你的数据包含图像或自然语言文本，请直接查看一些样本（及其标签）。

+   如果你的数据包含数值特征，绘制特征值的直方图是个好主意，以了解取值范围和不同值的频率。

+   如果你的数据包含位置信息，请在地图上绘制出来。是否出现了明显的模式？

+   一些样本是否缺少某些特征的值？如果是，你在准备数据时需要处理这个问题（我们将在下一节介绍如何处理）。

+   如果你的任务是一个分类问题，请打印出数据中每个类别的实例数。这些类别是否大致平均表示？如果不是，你将需要考虑这种不平衡。

+   检查*目标泄漏*：数据中存在的特征提供了关于目标的信息，这些信息在生产中可能不可用。如果你正在训练一个模型来预测未来某人是否会接受癌症治疗，并且记录包括“这个人被诊断患有癌症”这个特征，那么你的目标就会人为地泄漏到你的数据中。始终要问自己，你的数据中的每个特征是否都是在生产中以相同形式可用的？

### 6.1.4 选择成功的衡量标准

要控制某物，你需要能够观察它。要在项目上取得成功，你必须首先定义你所理解的成功。准确率？精确率和召回率？客户保留率？你对成功的度量标准将指导你在整个项目中做出的所有技术选择。它应直接与你的更高级别目标（例如客户的业务成功）保持一致。

对于平衡分类问题，每个类别等可能出现的情况下，准确率和*受试者工作特征曲线*（ROC）下的面积，简称为 ROC AUC，是常见的度量标准。对于类别不平衡的问题、排名问题或多标签分类问题，可以使用精确率和召回率，以及加权准确率或 ROC AUC 的形式。而且，定义自己的成功度量标准也并不罕见。要了解机器学习成功度量标准的多样性以及它们与不同问题领域的关系，浏览 Kaggle（[`kaggle.com`](https://kaggle.com)）上的数据科学竞赛是很有帮助的；它们展示了各种问题和评估指标。

## 6.2 开发模型

一旦你知道如何衡量你的进展，就可以开始模型开发。大多数教程和研究项目假设这是唯一的步骤——跳过问题定义和数据集收集，这些被假定已经完成，并跳过模型部署和维护，这些被假定由其他人处理。事实上，模型开发只是机器学习工作流程中的一个步骤，如果你问我，这并不是最困难的步骤。机器学习中最困难的事情是明确问题并收集、注释和清理数据。所以振作起来——接下来的事情相对容易！

### 6.2.1 准备数据

正如你之前学到的，深度学习模型通常不会直接处理原始数据。数据预处理旨在使手头的原始数据更适合神经网络。这包括向量化、归一化或处理缺失值。许多预处理技术是领域特定的（例如，特定于文本数据或图像数据）；在接下来的章节中，当我们在实际示例中遇到它们时，我们将介绍这些技术。目前，我们将回顾所有数据领域通用的基础知识。

向量化

神经网络中的所有输入和目标通常必须是浮点数据张量（或在特定情况下是整数或字符串张量）。无论你需要处理的数据是声音、图像、文本，你都必须首先将其转换为张量，这一步称为*数据向量化*。例如，在第四章的两个文本分类示例中，我们开始时将文本表示为整数列表（代表单词序列），然后使用独热编码将其转换为`float32`数据张量。在分类数字和预测房价的示例中，数据以向量化形式呈现，因此我们可以跳过这一步。

值归一化

在第二章的 MNIST 数字分类示例中，我们开始时将图像数据编码为 0-255 范围内的整数，表示灰度值。在将这些数据馈送到网络之前，我们必须将其转换为`float32`并除以 255，以便最终得到 0-1 范围内的浮点值。类似地，在预测房价时，我们开始时的特征具有各种范围——一些特征具有较小的浮点值，而其他特征具有相当大的整数值。在将这些数据馈送到网络之前，我们必须独立地对每个特征进行归一化，使其具有标准差为 1 和均值为 0。

一般来说，将相对较大的值（例如，多位整数，远大于网络权重初始值的值）或异构数据（例如，一个特征在 0-1 范围内，另一个在 100-200 范围内）输入神经网络是不安全的。这样做可能会触发大的梯度更新，阻止网络收敛。为了让你的网络学习更容易，你的数据应具有以下特征：

+   *取小值*—通常，大多数值应该在 0-1 范围内。

+   *保持同质性*—所有特征的取值应该在大致相同的范围内。

此外，以下更严格的归一化实践是常见的，可以帮助，尽管并不总是必要的（例如，在数字分类示例中我们没有这样做）：

+   将每个特征独立归一化为均值为 0。

+   将每个特征独立归一化为标准差为 1。

这在 NumPy 数组中很容易实现：

```py
x -= x.mean(axis=0)      # ❶
x /= x.std(axis=0)
```

❶ 假设 x 是一个形状为（样本数，特征数）的 2D 数据矩阵

处理缺失值

有时你的数据中可能会有缺失值。例如，在房价示例中，第一个特征（数据中索引为 0 的列）是人均犯罪率。如果这个特征并非所有样本都有呢？那么你的训练或测试数据中就会有缺失值。

你可以选择完全丢弃该特征，但不一定必须这样做。

+   如果特征是分类的，创建一个新类别表示“该值缺失”是安全的。模型将自动学习这对目标意味着什么。

+   如果特征是数值的，避免输入像`"0"`这样的任意值，因为它可能会在特征形成的潜在空间中创建不连续，使得模型更难泛化。相反，考虑用数据集中该特征的平均值或中位数替换缺失值。你也可以训练一个模型，根据其他特征的值来预测该特征的值。

请注意，如果你预期测试数据中会有缺失的分类特征，但网络是在没有任何缺失值的数据上训练的，那么网络就不会学会忽略缺失值！在这种情况下，你应该人为生成带有缺失条目的训练样本：多次复制一些训练样本，并丢弃你预计在测试数据中可能缺失的一些分类特征。

### 6.2.2 选择评估协议

正如你在前一章中学到的，模型的目的是实现泛化，你在整个模型开发过程中做出的每个建模决策都将由寻求衡量泛化性能的*验证指标*指导。你的验证协议的目标是准确估计你选择的成功指标（如准确率）在实际生产数据上的表现。这个过程的可靠性对于构建一个有用的模型至关重要。

在第五章中，我们回顾了三种常见的评估协议：

+   *保持留出验证集*—当你有大量数据时，这是一个好方法。

+   *进行 K 折交叉验证*—当你的样本量太少，无法依靠留出验证来进行可靠评估时，这是正确的选择。

+   *进行迭代的 K 折验证*—这是在数据有限的情况下进行高精度模型评估的方法。

选择其中之一。在大多数情况下，第一个就足够好了。但是要记住，始终要注意你的验证集的*代表性*，并小心不要在训练集和验证集之间有重复的样本。

### 6.2.3 击败基准

当你开始着手模型本身时，你的初始目标是实现*统计功效*，就像你在第五章看到的那样：也就是说，开发一个能够击败简单基准的小模型。

在这个阶段，你应该专注于以下三件最重要的事情：

+   *特征工程*—过滤掉无信息的特征（特征选择）并利用你对问题的了解开发可能有用的新特征。

+   *选择正确的架构先验*—你将使用什么类型的模型架构？密集连接网络、卷积网络、循环神经网络、Transformer？深度学习是否是解决任务的好方法，还是应该使用其他方法？

+   *选择足够好的训练配置*—你应该使用什么损失函数？什么批量大小和学习率？

选择正确的损失函数

往往不可能直接优化衡量问题成功的指标。有时没有简单的方法将指标转化为损失函数；毕竟，损失函数需要仅通过一个小批量数据就能计算（理想情况下，损失函数应该能够计算一个数据点）并且必须可微分（否则，你无法使用反向传播来训练你的网络）。例如，广泛使用的分类指标 ROC AUC 不能直接优化。因此，在分类任务中，通常会优化 ROC AUC 的代理指标，如交叉熵。一般来说，你可以希望交叉熵越低，ROC AUC 就越高。

以下表格可以帮助你为几种常见问题类型选择最后一层激活函数和损失函数。

为你的模型选择正确的最后一层激活函数和损失函数

| 问题类型 | 最后一层激活函数 | 损失函数 |
| --- | --- | --- |
| 二元分类 | sigmoid | binary_crossentropy |
| 多类单标签分类 | softmax | categorical_crossentropy |
| 多类多标签分类 | sigmoid | binary_crossentropy |

对于大多数问题，你可以从现有的模板开始。你不是第一个尝试构建垃圾邮件检测器、音乐推荐引擎或图像分类器的人。确保你研究先前的技术以识别在你的任务上表现良好的特征工程技术和模型架构。

注意，并非总是能够达到统计功效。如果在尝试多个合理的架构后仍无法超越简单的基准线，可能是你所问的问题在输入数据中没有答案。记住，你正在提出两个假设：

+   你假设可以根据输入预测输出。

+   你假设可用数据足够信息丰富，可以学习输入和输出之间的关系。

很可能这些假设是错误的，如果是这样，你必须回到起点。

### 6.2.4 扩展规模：开发一个过拟合的模型

一旦你获得了具有统计功效的模型，问题就变成了，你的模型是否足够强大？它是否有足够的层和参数来正确地建模手头的问题？例如，逻辑回归模型在 MNIST 上具有统计功效，但不足以很好地解决问题。记住，机器学习中的普遍张力在于优化和泛化之间。理想的模型是一个恰好处于欠拟合和过拟合、欠容量和过容量之间的模型。要弄清楚这个边界在哪里，首先你必须跨越它。

要弄清楚你需要多大的模型，你必须开发一个过拟合的模型。这相当容易，正如你在第五章学到的那样：

1.  添加层。

1.  让层变得更大。

1.  训练更多的 epochs。

始终监控训练损失和验证损失，以及你关心的任何指标的训练和验证值。当你看到模型在验证数据上的表现开始下降时，你已经过拟合了。

### 6.2.5 正则化和调整你的模型

一旦您达到了统计功效并且能够过度拟合，您就知道自己走在正确的道路上。此时，您的目标是最大化泛化性能。

这个阶段将花费最多的时间：您将反复修改您的模型，训练它，在验证数据上评估（此时不是测试数据），再次修改它，并重复，直到模型达到最佳状态。以下是您应该尝试的一些事项：

+   尝试不同的架构；添加或删除层。

+   添加丢弃（dropout）。

+   如果您的模型很小，添加 L1 或 L2 正则化。

+   尝试不同的超参数（例如每层的单元数或优化器的学习率）以找到最佳配置。

+   可选地，迭代数据整理或特征工程：收集和注释更多数据，开发更好的特征，或删除看起来不具信息量的特征。

可以通过使用*自动化超参数调整软件*（如 KerasTuner）来自动化大部分工作。我们将在第十三章中介绍这个内容。

要注意以下事项：每次您使用验证过程的反馈来调整模型时，都会泄漏有关验证过程的信息到模型中。这样做几次是无害的；但如果在许多迭代中系统地重复这样做，最终会导致您的模型过度拟合验证过程（即使没有任何模型直接在任何验证数据上训练）。这会使评估过程变得不太可靠。

一旦您开发出令人满意的模型配置，您可以在所有可用数据（训练和验证）上训练最终的生产模型，并最后一次在测试集上评估它。如果测试集上的性能明显低于在验证数据上测量的性能，这可能意味着您的验证程序毕竟不可靠，或者在调整模型参数时开始过拟合验证数据。在这种情况下，您可能需要切换到更可靠的评估协议（如迭代 K 折验证）。

## 6.3 部署模型

您的模型已成功通过了测试集上的最终评估，它已准备好部署并开始其生产生活。

### 6.3.1 向利益相关者解释您的工作并设定期望

成功和客户信任在于始终满足或超越人们的期望。您交付的实际系统只是这个画面的一半；另一半是在推出前设定适当的期望。

非专业人士对人工智能系统的期望往往是不切实际的。例如，他们可能期望系统“理解”其任务，并能在任务背景下行使类似人类常识的能力。为了解决这个问题，您应该考虑展示一些模型的*失败模式*的示例（例如，展示被错误分类的样本是什么样子，特别是那些误分类看起来令人惊讶的样本）。

他们可能还期望人类水平的表现，特别是对以前由人类处理的流程。大多数机器学习模型，因为它们（不完美地）训练来逼近人类生成的标签，并没有真正达到那里。您应该清楚地传达模型表现的期望。避免使用抽象的陈述，如“模型的准确率为 98%”（大多数人心理上会四舍五入到 100%），而更倾向于谈论假阴性率和假阳性率。您可以说，“使用这些设置，欺诈检测模型将有 5%的假阴性率和 2.5%的假阳性率。每天，平均有 200 笔有效交易会被标记为欺诈并送去人工审核，平均会漏掉 14 笔欺诈交易。平均会正确捕捉 266 笔欺诈交易。”清楚地将模型的性能指标与业务目标联系起来。

你还应该确保与利益相关者讨论关键启动参数的选择——例如，应该在哪个概率阈值下标记交易（不同的阈值会产生不同的假阴性和假阳性率）。这些决策涉及权衡，只有深刻理解业务背景才能处理。

### 6.3.2 发布推理模型

当你到达一个可以保存训练模型的 Colab 笔记本时，一个机器学习项目并没有结束。你很少会将在训练过程中操作的完全相同的 Python 模型对象投入生产。

首先，你可能希望将模型导出为除 Python 之外的其他形式：

+   你的生产环境可能根本不支持 Python——例如，如果是移动应用程序或嵌入式系统。

+   如果应用程序的其余部分不是用 Python 编写的（可能是 JavaScript、C++ 等），使用 Python 来提供模型可能会引入显著的开销。

其次，由于你的生产模型只用于输出预测（一个称为*推理*的阶段），而不是用于训练，你有机会进行各种优化，使模型更快速并减少其内存占用。

让我们快速看一下你可以使用的不同模型部署选项。

将模型部署为 REST API

这可能是将模型转化为产品的常见方式：在服务器或云实例上安装 TensorFlow，并通过 REST API 查询模型的预测。你可以使用类似 Flask（或任何其他 Python Web 开发库）的东西构建自己的服务应用程序，或者使用 TensorFlow 的自己的库来将模型作为 API 进行部署，称为*TensorFlow Serving*（[www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving)）。使用 TensorFlow Serving，你可以在几分钟内部署一个 Keras 模型。

当你需要使用这种部署设置时

+   将消费模型预测的应用程序将可靠地访问互联网（显然）。例如，如果你的应用程序是一个移动应用程序，从远程 API 提供预测意味着应用程序在飞行模式或低连接环境下无法使用。

+   应用程序没有严格的延迟要求：请求、推理和答复往返通常需要大约 500 毫秒。

+   发送用于推理的输入数据并不高度敏感：数据需要以解密形式在服务器上可用，因为模型需要查看它（但请注意，应该使用 SSL 加密进行 HTTP 请求和答复）。

例如，图像搜索引擎项目、音乐推荐系统、信用卡欺诈检测项目和卫星图像项目都非常适合通过 REST API 进行服务。

将模型部署为 REST API 时一个重要的问题是，你是想自己托管代码，还是想使用完全托管的第三方云服务。例如，Google 的产品 Cloud AI Platform 允许你简单地将你的 TensorFlow 模型上传到 Google Cloud Storage（GCS），并提供一个 API 端点来查询它。它会处理许多实际细节，如批处理预测、负载平衡和扩展。

在设备上部署模型

有时，你可能需要让你的模型与使用它的应用程序运行在同一设备上——也许是智能手机、机器人上的嵌入式 ARM CPU，或者微型设备上的微控制器。你可能见过一款相机，能够自动检测你对准的场景中的人和面孔：那很可能是一款直接在相机上运行的小型深度学习模型。

当你需要使用这种设置时

+   你的模型具有严格的延迟约束或需要在低连接环境中运行。如果你正在构建一款沉浸式增强现实应用程序，查询远程服务器不是一个可行的选择。

+   您的模型可以足够小，以便在目标设备的内存和功率约束下运行。您可以使用 TensorFlow 模型优化工具包来帮助实现此目标（[www.tensorflow.org/model_optimization](https://www.tensorflow.org/model_optimization)）。

+   获得最高可能的准确性对您的任务并非至关重要。运行时效率和准确性之间总是存在权衡，因此内存和功率约束通常要求您部署一个不如在大型 GPU 上运行的最佳模型好的模型。

+   输入数据非常敏感，因此不应在远程服务器上解密。

我们的垃圾邮件检测模型将需要在最终用户的智能手机上运行，作为聊天应用程序的一部分，因为消息是端到端加密的，因此无法由远程托管模型读取。同样，坏 cookie 检测模型具有严格的延迟约束，并且需要在工厂中运行。幸运的是，在这种情况下，我们没有任何功率或空间约束，因此我们实际上可以在 GPU 上运行模型。

要在智能手机或嵌入式设备上部署 Keras 模型，您的首选解决方案是 TensorFlow Lite（[www.tensorflow.org/lite](https://www.tensorflow.org/lite)）。这是一个用于在设备上高效进行深度学习推断的框架，可在 Android 和 iOS 智能手机以及基于 ARM64 的计算机、树莓派或某些微控制器上运行。它包括一个转换器，可以直接将您的 Keras 模型转换为 TensorFlow Lite 格式。

在浏览器中部署模型

深度学习经常用于基于浏览器或桌面的 JavaScript 应用程序。虽然通常可以让应用程序通过 REST API 查询远程模型，但在浏览器中直接运行模型（利用 GPU 资源（如果可用））可能具有关键优势。

当以下情况发生时，请使用此设置

+   您希望将计算卸载到最终用户，这可以大大降低服务器成本。

+   输入数据需要保留在最终用户的计算机或手机上。例如，在我们的垃圾邮件检测项目中，Web 版本和桌面版聊天应用程序（作为用 JavaScript 编写的跨平台应用程序实现）应使用本地运行的模型。

+   您的应用程序具有严格的延迟约束。虽然在最终用户的笔记本电脑或智能手机上运行的模型可能比在您自己服务器上的大型 GPU 上运行的模型慢，但您没有额外的 100 毫秒的网络往返时间。

+   在模型已被下载并缓存后，您需要确保您的应用程序在没有连接的情况下继续工作。

只有在您的模型足够小，不会占用用户笔记本电脑或智能手机的 CPU、GPU 或 RAM 时，才应选择此选项。此外，由于整个模型将被下载到用户设备上，因此您应确保模型的任何内容都不需要保密。请注意，鉴于训练过的深度学习模型，通常可以恢复有关训练数据的一些信息：如果模型是在敏感数据上训练的，则最好不要将训练过的模型公开。

要在 JavaScript 中部署模型，TensorFlow 生态系统包括 TensorFlow.js（[www.tensorflow.org/js](https://www.tensorflow.org/js)），这是一个用于深度学习的 JavaScript 库，几乎实现了所有 Keras API（最初以 WebKeras 为工作名称开发）以及许多较低级别的 TensorFlow API。您可以轻松地将保存的 Keras 模型导入到 TensorFlow.js 中，以便在基于浏览器的 JavaScript 应用程序或桌面 Electron 应用程序中查询它。

推断模型优化

在部署在具有严格可用功率和内存约束（智能手机和嵌入式设备）或具有低延迟要求的环境中时，优化您的模型以进行推断尤为重要。在将模型导入到 TensorFlow.js 或导出到 TensorFlow Lite 之前，您应始终寻求优化您的模型。

有两种流行的优化技术可以应用：

+   *权重剪枝*—权重张量中的每个系数并不都对预测产生相同的贡献。通过仅保留最重要的系数，可以显著降低模型层中的参数数量。这减少了模型的内存和计算占用，但在性能指标上略有损失。通过决定要应用多少剪枝，你可以控制大小和准确性之间的权衡。

+   *权重量化*—深度学习模型是使用单精度浮点(`float32`)权重训练的。然而，可以将权重*量化*为 8 位有符号整数(`int8`)，以获得一个仅用于推断的模型，其大小是原始模型的四分之一，但保持接近原始模型的准确性。

TensorFlow 生态系统包括一个权重剪枝和量化工具包([www.tensorflow.org/model_optimization](https://www.tensorflow.org/model_optimization))，它与 Keras API 深度集成。

### 6.3.3 在实际环境中监控你的模型

你已经导出了一个推断模型，将其集成到你的应用程序中，并在生产数据上进行了干扰测试—模型的行为与你预期的完全一致。你已经编写了单元测试以及日志记录和状态监控代码—完美。现在是时候按下大红按钮，部署到生产环境了。

即使这还不是结束。一旦部署了模型，你需要继续监控其行为，对新数据的性能，与应用程序其他部分的交互以及对业务指标的最终影响。

+   在部署新的音乐推荐系统后，你的在线广播的用户参与度是上升还是下降？切换到新的点击率预测模型后，平均广告点击率是否增加？考虑使用*随机 A/B 测试*来分离模型本身的影响和其他变化：一部分案例应该经过新模型，而另一部分控制案例应该继续使用旧流程。一旦处理了足够多的案例，两者之间的结果差异很可能归因于模型。

+   如果可能的话，定期对模型在生产数据上的预测进行手动审核。通常可以重复使用与数据标注相同的基础设施：将一部分生产数据发送到手动标注，然后将模型的预测与新的标注进行比较。例如，你应该绝对为图像搜索引擎和恶意 cookie 标记系统做这个工作。

+   当无法进行手动审核时，考虑替代的评估途径，比如用户调查（例如，在垃圾邮件和有害内容标记系统的情况下）。

### 6.3.4 维护你的模型

最后，没有模型能永远持续。你已经了解了*概念漂移*：随着时间的推移，你的生产数据的特征将发生变化，逐渐降低模型的性能和相关性。你的音乐推荐系统的寿命将以周计算。对于信用卡欺诈检测系统，将以天计算。最好情况下，图像搜索引擎的寿命为几年。

一旦你的模型启动，你应该准备好训练下一个将取代它的新一代。因此，

+   注意生产数据的变化。是否有新特征可用？是否应该扩展或编辑标签集？

+   继续收集和标注数据，并随着时间的推移改进你的标注流水线。特别要注意收集那些对当前模型分类困难的样本—这些样本最有可能帮助提高性能。

这就结束了机器学习的通用工作流程——需要记住的事情很多。成为专家需要时间和经验，但不要担心，您现在比几章前聪明多了。您现在熟悉了整体情况——机器学习项目所涉及的整个范围。虽然本书的大部分内容将集中在模型开发上，但您现在意识到这只是整个工作流程的一部分。始终牢记整体情况！

## 摘要

+   当您开始新的机器学习项目时，首先定义手头的问题：

    +   理解您要做的事情的更广泛背景——最终目标是什么，有哪些约束条件？

    +   收集和注释数据集；确保深入了解数据。

    +   选择如何衡量问题的成功——您将在验证数据上监视哪些指标？

+   一旦您理解了问题并拥有适当的数据集，就开发一个模型：

    +   准备您的数据。

    +   选择评估协议：留出验证？K 折验证？应该使用数据的哪一部分进行验证？

    +   实现统计功效：击败一个简单的基准。

    +   扩展：开发一个可以过拟合的模型。

    +   正则化您的模型并调整其超参数，基于验证数据的性能。许多机器学习研究往往只关注这一步，但要牢记整体情况。

+   当您的模型准备就绪并在测试数据上表现良好时，就是部署的时候了：

    +   首先，确保与利益相关者设定适当的期望。

    +   优化最终模型以进行推断，并将模型部署到选择的部署环境——Web 服务器、移动设备、浏览器、嵌入式设备等。

    +   监控模型在生产中的性能，并继续收集数据，以便开发下一代模型。
