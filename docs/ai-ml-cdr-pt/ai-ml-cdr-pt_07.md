# 第六章\. 通过使用嵌入使情感可编程

在第五章中，你看到了如何将单词编码成标记。然后，你看到了如何将充满单词的句子编码成充满标记的序列，根据需要填充或截断它们，以得到一个形状良好的数据集，你可以用它来训练神经网络。然而，在那一切中，并没有对单词的*意义*进行任何类型的建模。虽然确实没有能够封装意义的绝对数值编码，但存在相对的编码。

在本章中，你将了解封装意义的技术，特别是*嵌入*的概念，其中在多维空间中创建向量来表示单词。这些向量的方向可以根据语料库中单词的使用情况随时间学习。然后，当你得到一个句子时，你可以研究单词向量的方向，将它们相加，并从总和的整体方向中建立句子的情感，作为其单词的乘积。此外，与此相关，当模型扫描句子时，句子中单词的位置也可以帮助训练适当的嵌入。

在本章中，我们还将探讨它是如何工作的。使用来自第五章的讽刺检测新闻标题数据集，你将构建嵌入以帮助模型检测句子中的讽刺。你还将使用一些酷炫的可视化工具，帮助你理解语料库中的单词如何映射到向量，以便你可以看到哪些单词决定了整体的分类。

# 从单词中建立意义

在我们深入探讨用于嵌入的高维向量之前，让我们用一些简单的例子来尝试可视化如何从数值中推导出意义。考虑这个例子：使用第五章中的讽刺数据集，如果你用正数编码构成讽刺标题的所有单词，而用负数编码构成现实标题的所有单词，会发生什么？

## 简单示例：正面和负面

以数据集中的这个讽刺标题为例：

```py
christian bale given neutered male statuette named oscar
```

假设我们词汇表中的所有单词的初始值都是 0，我们可以为这个句子中的每个单词的值加 1，最终我们会得到这个：

```py
{ "christian" : 1, "bale" : 1, "given" : 1, "neutered": 1, "male" : 1, 
  "statuette": 1, "named" : 1, "oscar": 1}
```

###### 注意

这与你在上一章中进行的单词*分词*不同。你可以考虑将每个单词（例如，*christian*）替换为其从语料库中编码的标记表示，但为了使代码更容易阅读，我现在将保留这些单词。

然后，在下一步中，考虑一个普通的标题（不是一个讽刺的标题），如下所示：

```py
gareth bale scores wonder goal against germany
```

因为这是一个不同的情感，我们可以从每个词语的当前值中减去 1，所以我们的值集将看起来像这样：

```py
{ "christian" : 1, "bale" : 0, "given" : 1, "neutered": 1, "male" : 1,
  "statuette": 1, "named" : 1, "oscar": 1, "gareth" : –1, "scores": –1,
  "wonder" : –1, "goal" : –1, "against" : –1, "germany" : –1}
```

注意，讽刺性的`bale`（来自`christian bale`）被非讽刺性的`bale`（来自`gareth bale`）所抵消，因此其得分最终为 0。重复这个过程数千次，你将得到一个巨大的词汇表，其中的词语根据其使用情况进行了评分。

现在，想象一下我们想要确定这个句子的情感：

```py
neutered male named against germany, wins statuette!
```

使用我们现有的值集，我们可以查看每个词语的评分并将它们加起来。我们会得到一个 2 分的评分，这表明（因为它是一个正数）这是一个讽刺的句子。

###### 注意

值得注意的是，在讽刺数据集中，“bale”这个词语出现了五次，其中两次出现在正常标题中，三次出现在讽刺标题中。因此，在这种模型中，“bale”这个词语在整个数据集中会被评分为一 1。

## 深入一点：向量

希望之前的例子已经帮助你理解了通过与其他同“方向”的词语关联来为某个词语建立某种形式**相对**意义的心理模型。在我们的例子中，尽管计算机无法理解单个词语的含义，但它可以将标记的词语从一个已知的讽刺性标题向一个方向（通过加 1）移动，并将标记的词语从一个已知的正常标题向另一个方向（通过减 1）移动。这让我们对词语的含义有了基本理解，但这也失去了一些细微差别。

但如果我们增加方向的维度以尝试捕捉更多信息呢？例如，假设我们要查看简·奥斯汀的小说《傲慢与偏见》中的字符，考虑性别和贵族的维度。我们可以在*x*轴上绘制前者，在*y*轴上绘制后者，向量的长度表示每个角色的财富（见图 6-1）。

![](img/aiml_0601.png)

###### 图 6-1\. 《傲慢与偏见》中的角色作为向量

从对图表的检查中，你可以得出关于每个角色的相当多的信息。其中三个是男性。达西先生非常富有，但他的贵族身份并不明确（他被称为“Mister”，与不那么富有但显然更贵族的威廉·卢卡斯爵士不同）。另一位“Mister”，本内特先生，显然不是贵族，并且经济上很困难。伊丽莎白·本内特，他的女儿，与他相似，但她是女性。凯瑟琳夫人，我们例子中的另一位女性角色，是贵族并且极其富有。达西先生和伊丽莎白之间的浪漫关系引起了紧张——向量的贵族方面对不那么贵族的方面产生了**偏见**。

正如这个例子所示，通过考虑多个维度，我们可以开始看到单词（在这里是字符名称）的真实意义。我们在这里讨论的不是具体的定义，而是基于轴和单词向量与其他向量之间关系的 *相对* 意义。

这引出了 *嵌入* 的概念，它简单地说是在训练神经网络时学习到的单词的向量表示。我们将在下一节中探讨这一点。

# PyTorch 中的嵌入

与你之前看到的 `Linear` 和 `Conv2D` 类似，PyTorch 通过使用一个层来实现嵌入。这创建了一个查找表，它将整数映射到嵌入表中，其中包含的系数是表示该整数所标识单词的向量的系数。因此，在上一节中的 *《傲慢与偏见》* 示例中，*x* 和 *y* 坐标将给我们提供书中特定字符的嵌入。当然，在真实的自然语言处理问题中，我们将使用远超过两个维度的数据。因此，向量在向量空间中的方向可以看作是编码单词的“意义”，而具有相似向量的单词（即，大致指向同一方向）可以被认为是与该单词相关的。

嵌入层将被随机初始化——也就是说，向量的坐标最初是完全随机的，将在训练过程中通过反向传播进行学习。当训练完成后，嵌入将大致编码单词之间的相似性，使我们能够根据这些单词的向量方向识别出某种程度上相似的单词。

这一切都很抽象，所以我认为理解如何使用嵌入的最佳方式是卷起袖子亲自尝试。让我们从一个使用 第五章 中的 Sarcasm 数据集的讽刺检测器开始。

## 通过使用嵌入构建讽刺检测器

在 第五章 中，你加载并预处理了一个名为 Sarcasm Detection 新闻标题数据集的 JSON 数据集。在你完成时，你有了训练数据和测试数据以及标签的列表：

```py
training_size = 28000
training_sentences = sentences[0:training_size]
testing_sentences = sentences[training_size:]
training_labels = labels[0:training_size]
testing_labels = labels[training_size:]
```

对于训练数据，你创建了一个 `build_vocab` 辅助函数来创建一个单词频率的字典，按频率排序。这个字典的大小是 `vocab_size`。

要在 PyTorch 中获取嵌入层，你可以使用 `nn.Embedding` 层类型，如下所示，通过指定所需的词汇大小和嵌入维度数：

```py
nn.Embedding(vocab_size, embedding_dim)
```

这将为每个单词初始化一个具有 `embedding_dim` 轴的向量。例如，如果 `embedding_dim` 是 `16`，那么词汇表中的每个单词都将被分配一个 16 维的向量。

随着时间的推移，每个标记的属性（编码为每个维度中向量的值）将通过反向传播在学习过程中匹配训练数据与其标签时被学习。

重要的下一步是将嵌入层的输出输入到一个密集层。最简单的方法，类似于使用卷积神经网络时的方式，是使用池化。在这种情况下，嵌入的维度被平均化以产生一个固定长度的输出向量，`AdaptiveAvePool1d(1)`将输入沿序列长度减少到固定向量大小 1。

例如，考虑以下模型架构：

```py
self.embedding = nn.Embedding(vocab_size, embedding_dim)
self.global_pool = nn.AdaptiveAvgPool1d(1)
self.fc1 = nn.Linear(embedding_dim, 24)
self.fc2 = nn.Linear(24, 1)
self.relu = nn.ReLU()
self.sigmoid = nn.Sigmoid()
```

在这里，定义了一个嵌入层，并给出了词汇大小和嵌入维度。让我们看看使用`torchinfo.summary`的网络中可训练参数的数量：

```py
==========================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================
TextClassificationModel                  [32, 1]                   --
├─Embedding: 1-1                         [32, 100, 100]            2,429,200
├─AdaptiveAvgPool1d: 1-2                 [32, 100, 1]              --
├─Linear: 1-3                            [32, 24]                  2,424
├─ReLU: 1-4                              [32, 24]                  --
├─Linear: 1-5                            [32, 1]                   25
├─Sigmoid: 1-6                           [32, 1]                   --
==========================================================================
Total params: 2,431,649
Trainable params: 2,431,649
Non-trainable params: 0
Total mult-adds (M): 77.81
==========================================================================
Input size (MB): 0.03
Forward/backward pass size (MB): 2.57
Params size (MB): 9.73
Estimated Total Size (MB): 12.32
==========================================================================
```

词汇表大小为 24,292 个单词，由于嵌入有 100 个维度，嵌入层中的可训练参数总数将为 2,429,200。第一层线性有 100 个输入值，24 个输出值，因此总共有 2,400 个权重，但每个神经元也有一个偏差，所以加上 24 得到 24, 24\。

同样，最后一层线性有 24 个输入值，只有一个输出神经元。总共有 24 个参数，加上一个偏差，等于 25。整个网络有 2,431,649 个参数需要学习。注意，平均池化层没有可训练参数，因为它只是对嵌入层之前的参数进行平均，以得到一个单一的 16 值向量。

如果我们训练这个模型，经过 30 个 epoch 后，我们将获得相当不错的训练准确率 99%+，但我们的验证准确率将低于 80%（见图 6-2）。

![模型架构](img/aiml_0602.png)

###### 图 6-2\. 训练准确率与验证准确率

这个曲线看起来可能似乎是合理的，因为验证数据很可能包含许多在训练数据中不存在的单词。然而，如果你检查训练与验证在一百个 epoch 中的损失曲线，你会看到一个问题。虽然你可能会期望看到训练准确率高于验证准确率，但过拟合的一个明显指标是，尽管验证准确率随着时间的推移略有下降（如图 6-2 所示），但其损失却在急剧增加，如图 6-3 所示。

![训练损失与验证损失](img/aiml_0603.png)

###### 图 6-3\. 训练损失与验证损失

由于语言的某种不可预测性，这种过拟合在 NLP 模型中很常见。在接下来的几节中，我们将探讨如何通过使用多种技术来减少这种影响。

## 降低语言模型中的过拟合

过拟合发生在网络对训练数据过度专业化的情况下，这包括网络在训练集中匹配“噪声”数据中的模式变得非常好，而这些模式在其他任何地方都不存在。由于这种特定的噪声在验证集中不存在，网络匹配它的能力越强，验证集的损失就会越差。这可能导致你在图 6-3 中看到的损失不断上升。

在本节中，我们将探讨几种泛化模型和减少过拟合的方法。

### 调整学习率

优化器的超参数是学习率（LR）。这个参数的细节超出了本章的范围，但可以将其视为一个值，如果太高，可能会导致网络学习得太快而错过细节。另一方面，如果设置得太低，网络可能无法有效学习。

可能导致过拟合的最大因素之一是优化器的 LR 是否过高。如果是，那么你的网络学习得太快了。对于这个例子，定义优化器的代码如下：

```py
optimizer = optim.Adam(model.parameters(), lr=0.001, 
                       betas=(0.9, 0.999), amsgrad=False)
```

这些是`Adam`优化器的默认值。可以实验的一个参数是`学习率`（`lr`），在下面的代码中，你会看到我将它降低了一个数量级到 0.0001 的结果，如下所示：

```py
optimizer = optim.Adam(model.parameters(), `lr``=``0.0001`, 
                       betas=(0.9, 0.999), amsgrad=False)
```

`betas`值保持默认，`amsgrad`也是如此。请注意，两个`beta`值必须在 0 到 1 之间，通常两者都接近 1。Amsgrad 是 Adam 优化器的另一种实现，它是在 Sashank Reddi、Satyen Kale 和 Sanjiv Kumar 发表的论文《On the Convergence of Adam and Beyond》中引入的。[“On the Convergence of Adam and Beyond” by Sashank Reddi, Satyen Kale, and Sanjiv Kumar](https://oreil.ly/FhTDi)。

这个更低的 LR 对网络有深远的影响。图 6-4 显示了网络在一百个 epoch 中的准确率。在最初的 10 个 epoch 左右，可以看到网络似乎没有学习，然后“突破”并开始快速学习。

探索损失（如图 6-5 所示），我们可以看到，尽管在前几个 epoch 中准确率没有上升，但损失却在下降。因此，如果你逐个 epoch 地观察，可以确信网络最终会开始学习。

![图片](img/aiml_0604.png)

###### 图 6-4\. 使用较低学习率的准确率

![图片](img/aiml_0605.png)

###### 图 6-5\. 使用较低学习率的损失

虽然损失确实开始显示出你在图 6-3 中看到的相同的过拟合曲线，但请注意，它发生得晚得多，并且速率低得多。到第 30 个 epoch 时，损失约为 0.49，而图 6-3 中的更高学习率（LR）时，损失量超过两倍。虽然网络需要更长的时间才能达到良好的准确率，但它以更低的损失率做到这一点，因此你可以对结果更有信心。使用这些超参数，验证集上的损失在约第 60 个 epoch 时开始增加，此时训练集的准确率为 90%，验证集的准确率约为 81%，这表明我们有一个相当有效的网络。

当然，只是调整优化器然后宣布胜利很容易，但你可以使用许多其他方法来改进你的模型。你将在接下来的几节中了解这些方法，并且为了这些方法，我已恢复使用默认的 Adam 优化器，这样调整 LR 的效果就不会隐藏这些其他技术提供的优势。

### 探索词汇量大小

意图数据集处理单词，所以如果你探索数据集中的单词及其频率，你可能会得到一个有助于解决过拟合问题的线索。

我提供了一个`word_frequency`辅助函数，让你可以探索词汇表中的单词频率。它看起来像这样：

```py
def word_frequency(sentences, word_dict):
    frequency = {word: 0 for word in word_dict}

    for sentence in sentences:
        words = sentence.lower().split()
        for word in words:
            if word in frequency:
                frequency[word] += 1

    return frequency
```

你可以用以下代码运行它：

```py
word_freq = word_frequency(training_sentences, word_index)
print(word_freq)
```

你将看到这样的结果：一个包含每个单词频率的字典，从最常用的单词开始，然后继续下去。以下是一些单词：

```py
{'new': 1318, 'trump': 1117, 'man': 1075, 'not': 634, 'just': 501, 
 'will': 484, 'one': 469, 'year': 440, …
```

如果你想绘制这个图表，你可以遍历列表中的每个项目，并将*x*值设置为你的位置序号（第一个项目为 1，第二个项目为 2，以此类推）。然后*y*值将是`newlist[item]`，你可以使用`matplotlib`来绘制。以下是代码：

```py
import matplotlib.pyplot as plt
from collections import OrderedDict
newlist = (OrderedDict(sorted(word_freq.items(), key=lambda t: t[1], 
                       reverse=True)))

xs=[]
ys=[]
curr_x = 1
for item in newlist:
  xs.append(curr_x)
  curr_x=curr_x+1
  ys.append(newlist[item])

print(ys)
plt.plot(xs,ys)

```

结果显示在图 6-6 中。

![](img/aiml_0606.png)

###### 图 6-6\. 探索单词频率

这条“曲棍球棒”曲线告诉我们，很少的单词被多次使用，而大多数单词被很少使用。但每个单词都被有效地同等加权，因为每个单词都有一个“条目”在嵌入中。鉴于我们的训练集相对于验证集来说相对较大，我们最终处于一个情况，即训练集中有许多单词在验证集中不存在。

你可以通过更改在调用`plt.show`之前图表的轴来放大数据。例如，要查看*x*轴上 300 到 10,000 的单词量，以及*y*轴上 0 到 100 的刻度，你可以使用以下代码：

```py
plt.plot(xs,ys)
plt.axis([300,10000,0,100])
plt.show()
```

结果显示在图 6-7 中。

![单词频率 300–10,000](img/aiml_0607.png)

###### 图 6-7\. 300 到 10,000 的单词频率

语料库中大约有 25,000 个单词，代码被设置为对所有这些单词进行训练！但如果我们查看位置 2,000 及以后的单词，这超过了我们词汇量的 90%，我们会发现它们在整个语料库中每个单词的使用次数都不到 20 次！

这可能解释了过拟合，所以合理的下一步是看看我们是否可以减少我们正在训练的词汇量。在`build_vocab`辅助函数中，我们可以添加一个参数来指定我们感兴趣的词汇量的最大值，如下所示：

```py
def build_vocab(sentences, max_vocab_size=10000):
    counter = Counter()
    for text in sentences:
        counter.update(tokenize(text))

# Take only the top max_vocab_size-1 most frequent words 
# (leave room for special tokens)
    most_common = counter.most_common(max_vocab_size – 2)  
    # -2 for <pad> and <unk>

    # Create vocabulary with indices starting from 2
    vocab = {word: idx + 2 for idx, (word, _) in enumerate(most_common)}
    vocab['<pad>'] = 0  # Add padding token
    vocab['<unk>'] = 1  # Add unknown token
    return vocab
```

然后，在构建我们的`word_index`时，我们可以指定一个我们感兴趣探索的最大词汇量：

```py
vocab_size = 2000
word_index = build_vocab(training_sentences, max_vocab_size=vocab_size)
```

嵌入层已经根据词汇量初始化，因此模型架构不需要改变。实际上，随着词汇量的减少，学习到的参数数量急剧下降，给我们提供了一个更简单的网络，它学习得更快：

```py

==========================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================
TextClassificationModel                  [32, 1]                   --
├─Embedding: 1-1                         [32, 100, 100]            200,100
├─AdaptiveAvgPool1d: 1-2                 [32, 100, 1]              --
├─Linear: 1-3                            [32, 24]                  2,424
├─ReLU: 1-4                              [32, 24]                  --
├─Linear: 1-5                            [32, 1]                   25
├─Sigmoid: 1-6                           [32, 1]                   --
==========================================================================
Total params: 202,549
Trainable params: 202,549
Non-trainable params: 0
Total mult-adds (M): 6.48
==========================================================================
Input size (MB): 0.03
Forward/backward pass size (MB): 2.57
Params size (MB): 0.81
Estimated Total Size (MB): 3.40
==========================================================================

```

模型从 240 万个参数缩减到只有 202,549 个。

在重新训练和探索较小的模型后，我们可以看到结果已经改变。

图 6-8 显示了准确率指标。现在，训练集的准确率大约是 82%，验证集的准确率大约是 76%。它们彼此更接近，没有发散，这是一个好兆头，表明我们已经消除了大部分过拟合。

![](img/aiml_0608.png)

###### 图 6-8。两千个单词词汇量的准确率

这在一定程度上得到了图 6-9 中的损失图的加强。验证集上的损失在上升，但比之前慢得多，所以减少词汇量以防止训练集对可能在训练集中仅出现一次的低频单词过拟合似乎已经奏效。

值得尝试不同的词汇量大小，但请记住，词汇量太小也可能导致过拟合。你需要找到一个平衡点。在这种情况下，我选择出现 20 次或以上的单词纯粹是随意的。

![](img/aiml_0609.png)

###### 图 6-9。两千个单词词汇量的损失

### 探索嵌入维度

对于这个例子，我随意选择了 16 维的嵌入维度。在这种情况下，单词被编码为 16 维空间中的向量，其方向表示它们的整体意义。但 16 是一个好数字吗？在我们的词汇量中只有两千个单词的情况下，它可能有点高，导致方向的高稀疏度。

###### 注意

我认为思考稀疏性的最佳方式是将它们投影到三维空间中。想象一下地球，有一千个向量从核心指向地表上的一个点。这些向量在三维空间中，*x*，*y*，和*z*。它们覆盖的表面积很大，但如果许多向量缺少*x*和*y*，这意味着它们只是零，那么许多向量将指向(0, 0, *z*)，而地球表面的大部分将保持未触及！因此，将缺乏独特性。

研究表明，嵌入大小的最佳实践是使其成为词汇大小的四次方根。2000 的四次方根是 6.687，所以让我们探索如果我们将其四舍五入并将嵌入维度更改为 7 会发生什么。

你可以在图 6-10 中看到训练 100 个 epoch 的结果。训练集的准确率稳定在大约 83%，验证集的准确率稳定在大约 77%。尽管有一些波动，但线条相当平稳，表明模型已经收敛。这与图 6-8 中的结果没有太大不同，但减少嵌入维度使得模型训练速度显著加快。

![](img/aiml_0610.png)

###### 图 6-10\. 七维度的训练与验证准确率

图 6-11 显示了训练和验证中的损失。虽然最初看起来损失在约 20 个 epoch 时上升，但它很快平缓下来。再次，这是一个好兆头！

![](img/aiml_0611.png)

###### 图 6-11\. 七维度的训练与验证损失

现在维度已经减少，我们可以对模型架构进行一些更多的调整。

### 探索模型架构

在前几节的优化之后，模型架构看起来是这样的：

```py
class TextClassificationModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim=24):
        super(TextClassificationModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.fc1 = nn.Linear(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

```

想到的一件事是维度——`GlobalAveragePooling1D`层现在只发出 7 个维度，但它们被输入到 24 个神经元的隐藏层中，这是过度的。让我们看看当这减少到 8 个神经元并训练 100 个 epoch 时会发生什么。

你可以在图 6-12 中看到训练与验证的准确率。与使用 24 个神经元的图 6-7 相比，整体结果相当相似，但模型的训练速度有所提高。

![](img/aiml_0612.png)

###### 图 6-12\. 减少密集架构准确率结果

图 6-13 中的损失曲线显示了类似的结果。

通过遵循这些练习，我们能够显著减少模型架构，在提高质量和减轻过拟合的同时减少参数数量。但还有一些事情我们可以做——从 dropout 开始。

![](img/aiml_0613.png)

###### 图 6-13\. 减少密集架构损失结果

### 使用 dropout

减少过拟合的常见技术是在密集神经网络中添加 dropout。我们早在第三章中探讨了卷积神经网络，所以在这里直接探讨它对过拟合的影响似乎很吸引人。但在这个案例中，我想等到词汇量、嵌入大小和架构复杂性都得到解决。这些变化往往比使用 dropout 有更大的影响，而且我们已经看到了一些很好的结果。

现在我们已经将架构简化到中间密集层只有八个神经元，dropout 的影响可能已经被最小化——但让我们还是探索一下。以下是更新后的模型架构代码，添加了 0.25 的 dropout（相当于八个神经元中的两个）：

```py
class TextClassificationModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim=8, 
                       `dropout_rate``=``0.25``)``:`
        super(TextClassificationModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.fc1 = nn.Linear(embedding_dim, hidden_dim)
        self.dropout = nn.Dropout(p=dropout_rate)  # Add dropout layer
        self.fc2 = nn.Linear(hidden_dim, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.embedding(x)
        x = x.transpose(1, 2)  # Change for pooling layer
        x = self.global_pool(x).squeeze(2)
        `x` `=` `self``.``dropout`(self.relu(self.fc1(x)))
        x = self.sigmoid(self.fc2(x))
        return x

```

图 6-14 显示了训练 100 个 epoch 时的准确率结果。这次，我们看到训练准确率和验证准确率正在收敛，训练准确率现在低于之前。同样，图 6-15 中的损失曲线也显示了收敛，所以虽然 dropout 使我们的网络略微*降低*了准确率，但它似乎更好地泛化了。

但在宣布胜利之前，请务必谨慎！仔细检查曲线显示，损失已经很好地收敛，但它们*确实*比之前更高。使用 dropout 时，训练损失高于 0.5，而没有 dropout 时大约是 0.3。它也呈下降趋势，所以值得实验看看更长的训练时间是否能产生更好的结果。

![](img/aiml_0614.png)

###### 图 6-14\. 添加 dropout 后的准确率

![](img/aiml_0615.png)

###### 图 6-15\. 添加 dropout 后的损失

你还可以看到，模型正在回归到之前随时间增加验证损失的模式。这并不像之前那么糟糕，但它正朝着错误的方向发展。

在这种情况下，当神经元非常少时，引入 dropout 可能并不是一个好的主意。尽管如此，这个工具仍然值得保留在你的工具箱中，所以请确保记住它，用于比这个更复杂的架构。

### 使用正则化

*正则化*是一种通过减少权重的极化来帮助防止过拟合的技术。如果某些神经元的权重太重，正则化会有效地惩罚它们。广义而言，有两种类型的正则化：

L1 正则化

这通常被称为*最小绝对收缩*和*选择算子*（lasso）正则化。它有效地帮助我们计算层中的结果时忽略零或接近零的权重。

L2 正则化

这通常被称为*岭回归*，因为它通过取它们的平方来推动值分开。这往往放大了非零值与零或接近零值之间的差异，从而产生脊效应。

这两种方法也可以结合成有时被称为*弹性*正则化的方法。

对于我们正在考虑的 NLP 问题，L2 正则化最常使用。它可以作为`optimizer`的`weight_decay`属性添加。以下是一个示例：

```py
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), 
                       amsgrad=False, `weight_decay``=``0.01`)
```

这将应用`weight_decay`为`0.01`。（通常，这里的值会在 0.01 和 0.001 之间）。另外，使用 PyTorch 你可以定义不同层的不同权重衰减，通过在`Adam`声明调用中指定它们，如下所示：

```py
# Different weight decay for different layers
optimizer = torch.optim.Adam([
# L2 reg on fc1
        {'params': model.fc1.parameters(), 'weight_decay': 0.01},      
    # No L2 reg on other layers
{'params': [p for name, p in model.named_parameters() 
            if 'fc1' not in name]}  
], lr=0.0001)
```

在这样一个简单模型中添加正则化的影响并不特别大，但它确实在一定程度上平滑了我们的训练损失和验证损失。对于这种情况来说可能有些过度，但就像 dropout 一样，了解如何使用正则化来防止模型过度专业化是一个好主意。

### 其他优化考虑因素

虽然我们做出的修改已经使我们得到了一个拟合度更低的改进模型，但还有其他超参数你可以进行实验。例如，我们选择将最大句子长度设为 100 个单词，但这完全是随意的，可能不是最优的。探索语料库并查看更好的句子长度可能是一个好主意。以下是一段代码，它查看句子并按从低到高的顺序绘制每个句子的长度：

```py
xs=[]
ys=[]
current_item=1
for item in sentences:
  xs.append(current_item)
  current_item=current_item+1
  ys.append(len(item))
newys = sorted(ys)

import matplotlib.pyplot as plt
plt.plot(xs,newys)
plt.show()
```

见图 6-16 以了解此结果。

![图片](img/aiml_0616.png)

###### 图 6-16\. 探索句子长度

在 26,000+的总语料库中，少于 200 个句子的长度超过 100 个单词，因此选择这个作为最大长度，我们引入了大量的不必要的填充，从而影响了模型的表现。将最大长度减少到 85 个单词仍然可以保留 26,000 个句子（99%以上），同时大幅减少填充。

## 整合所有内容

将所有前面的优化措施生效，并重新训练模型三百个 epoch，你可以在图 6-17 中看到训练和验证准确率的结果。鉴于它们的曲线大致匹配，这表明我们已朝着避免过拟合迈出了巨大的步伐，并且我们有一个正在有效学习的网络。

同样，训练和验证损失曲线在三百个 epoch 中显示出显著相似性，如图 6-18 所示，这表明优化是防止模型过拟合的正确方向。

![图片](img/aiml_0617.png)

###### 图 6-17\. 优化后的训练和验证准确率

![图片](img/aiml_0618.png)

###### 图 6-18\. 优化后的训练和验证损失

## 使用模型对句子进行分类

现在你已经创建了模型，训练了它，并优化了它以消除导致过拟合的许多问题，下一步就是运行模型并检查其结果。为此，你需要创建一个新句子的数组。例如，考虑以下内容：

```py
test_sentences = [
             "granny starting to fear spiders in the garden might be real", 
             "game of thrones season finale showing this sunday night", 
             "PyTorch book will be a best seller"]
```

你可以使用创建词汇表时使用的相同分词器来对这些序列进行编码：

```py
print(texts_to_sequences(test_sentences, word_index))

```

使用这个分词器很重要，因为它包含了网络训练时使用的单词的标记！

打印语句的输出将是前面句子的序列：

```py
[
[1, 803, 753, 1, 1, 312, 97], 
[123, 1183, 160, 1, 1, 1543, 152], 
[1, 235, 7, 47, 1]
]
```

这里有很多`1`标记（“<OOV>”），因为像*奶奶*和*蜘蛛*这样的词没有出现在字典中。由于停用词已被删除，序列也变短了。

接下来，在你可以将序列传递给模型之前，你需要将它们放入模型期望的形状——即期望的长度。你可以用与训练模型时相同的方式使用`pad_sequences`来完成这个操作：

```py
padded = pad_sequences(sequences, max_len)
```

这将输出长度为`85`的句子序列，因此第一个序列的输出如下：

```py
[1, 803, 753, 1, 1, 312, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
 0, 0, 0, 0, 0, 0, 0]
```

这是一个非常短的句子，所以它被填充到了 85 个字符，其中包含了很多零！

现在你已经填充并分词了句子，使其符合模型对输入维度的期望，现在是时候将它们传递给模型并获取预测结果了。

这涉及多个步骤。首先，将填充的序列转换为输入张量：

```py
# Convert to tensor
input_ids = torch.tensor(padded, dtype=torch.long).to(device)
```

接下来，将模型置于评估模式以获取预测，然后只需将`input_ids`传递给它以获取输出：

```py
# Get predictions
model.eval()
with torch.no_grad():
    outputs = model(input_ids)
```

结果将以列表的形式返回并打印出来，高值表示可能存在讽刺。以下是我们的样本句子的结果：

```py
tensor([[0.5516],
        [0.0765],
        [0.0987]], device='cuda:0')
```

尽管第一个句子（“奶奶开始害怕花园里的蜘蛛可能是真的”）有很多停用词，并且被填充了很多零，但它的得分很高，这表明其中存在一定程度的讽刺。其他两个句子的得分要低得多，表明它们中讽刺的可能性较低。

要获取概率，你可以调用`squeeze()`方法来检索张量值。如果你想将阈值与比较以获得预测——例如，高于 0.5 表示讽刺，低于 0.5 表示没有讽刺——则可以使用如下代码：

```py
probabilities = outputs.squeeze().cpu().numpy()
predictions = (probabilities >= threshold).astype(int)
```

根据你的网络调整，你也可以确定你认为适当的阈值应该是什么。以 0.5 的阈值运行，我们得到以下结果：

```py
Text: granny starting to fear spiders in the garden might be real
Probability: 0.5516
Classification: `Sarcastic`
Confidence: 0.5516
--------------------------------------------------------------------------

Text: game of thrones season finale showing this sunday night
Probability: 0.0765
Classification: Not Sarcastic
Confidence: 0.9235
--------------------------------------------------------------------------

Text: PyTorch book will be a best seller
Probability: 0.0987
Classification: `Not` `Sarcastic`
Confidence: 0.9013
--------------------------------------------------------------------------
```

因此，通过这些测试句子，我们开始得到一个很好的迹象，表明我们的网络正在按预期运行。你应该用其他数据来测试它，看看你是否能将其破坏，如果你能持续地破坏它，那么就是时候尝试不同的模型架构，从现有的工作网络中利用迁移学习，或者探索使用预训练的嵌入体了。

我们将在下一节中了解这一点，但在那之前，我想向你展示你可以如何可视化这个网络学习到的自定义嵌入。

# 可视化嵌入

要可视化嵌入，你可以使用一个名为[嵌入投影仪](http://projector.tensorflow.org)的在线工具。它预先加载了许多现有数据集，但在本节中，你将看到如何使用这个工具通过模型训练的数据来可视化。

但首先，你需要一个函数来反转单词索引。它目前将单词作为标记，将键作为值，但你需要将其反转，以便你可以在投影仪上绘制单词值。以下是执行此操作的代码：

```py
reverse_word_index = dict([(value, key)
for (key, value) in word_index.items()])

```

你还需要提取嵌入中的向量权重：

```py
embedding_weights = model.embedding.weight.data.cpu().numpy()
print(embedding_weights.shape)

```

如果你遵循了本章中的优化，这个输出的结果将是`(2000,7)`，因为我们使用了 2,000 个单词的词汇量和 7 个维度的嵌入。如果你想探索一个单词及其向量细节，你可以使用如下代码：

```py
print(reverse_word_index[2])
print(embedding_weights[2])

```

这将产生以下输出：

```py
new
[–0.27116913 –1.3026129   1.6390767   0.4922502  –0.6025921   1.4584142
  0.05054485]

```

因此，单词“new”由一个在轴上有那些七个系数的向量表示。

嵌入投影仪使用两个制表符分隔的值（TSV）文件，一个用于向量维度，一个用于元数据。此代码将为你生成它们：

```py
import io
out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for word_num in range(1, vocab_size):
  word = reverse_word_index[word_num]
  embeddings = embedding_weights[word_num]
  out_m.write(word + "\n")
  out_v.write('\t'.join([str(x) for x in embeddings]) + "\n")
out_v.close()
out_m.close()

```

或者，如果你使用的是 Google Colab，你可以使用以下代码或从文件面板下载 TSV 文件：

```py
try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download('vecs.tsv')
  files.download('meta.tsv')

```

一旦你有这些文件，你可以在投影仪上按下“加载”按钮来可视化嵌入（见图 6-19）。

![](img/aiml_0619.png)

###### 图 6-19\. 使用嵌入投影仪

你还可以在结果对话框中推荐的位置使用向量和元 TSV 文件，然后点击投影仪上的“球化数据”。这将导致单词在球体中聚集，并将为你提供这个分类器二进制性质的清晰可视化。它只训练了讽刺和非讽刺句子，所以单词倾向于聚集在某个标签附近（见图 6-20）。

![](img/aiml_0620.png)

###### 图 6-20\. 可视化讽刺嵌入

屏幕截图并不能完全展示这些内容——你应该亲自尝试！你可以旋转中心球体，探索每个“极点”上的单词，看看它们对整体分类的影响，你还可以在右侧面板中选择单词并显示相关的单词。玩一玩，实验一下！

# 使用预训练嵌入

使用自己训练的嵌入的一个替代方案是使用别人代为预训练的嵌入。你可以在许多来源找到这些嵌入，包括 Kaggle 和 Hugging Face。你甚至可以在研究结果的旁边找到预训练的嵌入。这样一组预训练的嵌入是[斯坦福 GloVe 嵌入](https://oreil.ly/s1YWw)，我们将在下面探讨这些内容。

注意，然而，当使用预训练的嵌入时，你也应该考虑更新和更改你的分词器，以匹配预训练嵌入所使用的任何规则。

例如，对于 GloVE 预训练嵌入——它仅仅是一个包含单词及其在 50 到 300 个维度上预训练嵌入的大型文本文件——用于分词单词的规则与我们所使用的原始数据的手工分词器略有不同。因此，对于 GloVe，你应该考虑所有单词都转换为小写或数字归一化到 0 这样的规则。

一旦你完成这个操作（我在下载中提供了 GloVe 的代码，并在下一章中对其进行了更详细的讨论），那么只需将预训练嵌入的权重加载到你的模型定义中即可，如下所示：

```py
# Initialize embedding layer
self.embedding = nn.Embedding(vocab_size, embedding_dim)

# Load pretrained embeddings if provided
if pretrained_embeddings is not None:
    self.embedding.weight.data.copy_(pretrained_embeddings)
    if freeze_embeddings:
        self.embedding.weight.requires_grad = False
```

如果你不想从这些嵌入中学习，只想使用它们，那么你应该将`freeze_embeddings`设置为`True`。否则，网络将使用预加载的嵌入权重作为起点进行微调。

这个模型在训练中会迅速达到峰值准确度，并且它不会像我们之前看到的那样过度拟合。经过三百个 epoch 的准确度显示，训练和验证非常一致（参见图 6-22）。损失值也是一致的，这表明我们在前几百个 epoch 中拟合得非常好。然而，它们也开始发散（参见图 6-22）。

另一方面，值得注意的是，整体准确度（大约 70%）相当低，考虑到抛硬币有 50%的机会猜对！因此，虽然使用预训练嵌入可以加快训练速度并减少过度拟合，但你应该了解它们有什么用，以及它们可能并不总是最适合你的场景。因此，你可能需要探索适当的优化方法或替代方案。

![图片](img/aiml_0621.png)

###### 图 6-21\. 使用 GloVe 嵌入的准确度指标

![图片](img/aiml_0622.png)

###### 图 6-22\. 使用 GloVe 嵌入的损失指标

# 摘要

在本章中，你构建了第一个能够理解文本情感的模型。它是通过从第五章获取分词文本并将其映射到向量来实现的。然后，使用反向传播，它根据包含该句子的标签学习每个向量的适当“方向”。最后，它能够使用一组单词的所有向量来构建句子中情感的概念。

你还探索了优化模型以避免过拟合的方法，并看到了表示你单词的最终向量的整洁可视化。但尽管这是一种很好的句子分类方法，它只是简单地将每个句子视为一堆单词。其中没有固有的序列，而在确定句子的真正含义时，单词出现的顺序非常重要。

因此，考虑如果我们能通过考虑序列来改进我们的模型，那将是个不错的想法。我们将在下一章中通过介绍一种新的层类型：一个*循环*层，这是循环神经网络的基础，来探讨这一点。
