# 第三章：代理的经济学、模型使用和选择

大型语言模型（LLMs）作为驱动人工智能代理的认知引擎——就像不同的汽车引擎为不同的目的而设计，从全地形车到校车。在[*什么是人工智能代理？何时以及如何使用 LLM 代理*](https://learning.oreilly.com/library/view/what-are-ai/9781098159726/)（O'Reilly）中概述的框架基础上，为您的代理选择合适的 LLM 不仅关乎性能——它还关乎能力和经济之间的权衡。您选择的基座模型将直接影响您的代理能做什么，以及运行它的成本、其操作的效率，以及最终，您的项目是否具有财务意义。理解这些经济权衡对于任何构建人工智能代理的人来说至关重要，无论您是关注每一美元的初创公司，还是希望扩展的企业。

# 代理采用的经济学

关于人工智能代理的第一点要注意的是，随着它们在通过协议（如模型上下文协议 MCP）或通过 LLM 内部的函数调用等方式获取和操作更多工具的能力扩大，最终用户从代理中获得的利益也更大。但就像经济学中的所有事物一样，存在权衡。随着代理提供的任务复杂性的增加，特定行动的边际成本也增加——无论我们通过完成任务所需的小时数、API 调用的成本，还是底层代理解决特定任务的能力来衡量这种成本。

图 3-1 中显示的图表将任务复杂度（*x*轴）与边际收益和边际成本（*y*轴）进行对比。随着复杂度的增加，人工智能模型增加的额外价值（边际收益）下降，而额外的成本或风险（边际成本）上升。它们的交点（*）标志着经济最优点：任何复杂度在*左侧的任务仍然提供正的净收益（MB > MC），使得人工智能部署是有价值的；右侧的任务成本超过其价值。

![说明当前人工智能能力如何将任务复杂性与边际收益和成本相关联的图表，显示了人工智能部署的经济最优点，其中收益超过成本。](img/mmai_0301.png)

###### 图 3-1：当前人工智能能力

但还有更多。当突破性模型或新的集成能力出现时——想想 GPT-5 或前沿的开源基座模型——边际成本曲线会向下并向右移动。从实际意义上讲，现在每个额外的任务复杂度单位成本更低，因此与边际收益曲线的交点向右移动。这扩大了 MB > MC 的区域，意味着更复杂的任务对人工智能自动化来说在经济上变得可行。换句话说，随着模型效率的提高，您的“部署前沿”变宽——人工智能可以为更广泛的项目增加正的净收益（图 3-2）。

![说明引入新 AI 模型如何改变边际成本（MC）和边际价值（MV）的交叉点，扩大经济可行的 AI 自动化的前沿的图表](img/mmai_0302.png)

###### 图 3-2。当新的 AI 模型出现时，前沿发生移动

# 模型选择矩阵：多因素分析

构建一个 AI 代理需要一个相应的 LLM 来运行它。选择一个 LLM 并不像看起来那么简单。默认选择最新的模型，如 Claude Sonnet 4 或 GPT-5，可能是一个快速、通用项目的有效启发式方法，但当涉及到专门的应用或可能成为你业务或产品核心的应用时，就需要更多的思考、计划和解释。你是否只需要基于文本的代理，或者你需要能够处理声音或视觉输入的多模态代理？将有多少用户使用你的代理？你的延迟需求是什么？这些问题只是基本条件。

有效做出这些决策需要了解不同模型在几个关键维度上的表现。一些因素——如质量和准确性——似乎很明显，但其他因素——如上下文窗口大小与成本之间的关系——当团队达到生产规模时可能会让他们措手不及。

## 核心性能指标

当涉及到 AI 代理时，速度和推理代理的输出与推理之间存在一种普遍的权衡。许多行业人士已经开始区分他们所说的“系统 1”与“系统 2”思考——借鉴了丹尼尔·卡尼曼和阿莫斯·特沃斯基在行为经济学平行领域发展的人类心智理论。1 系统 1 的处理速度快、自动且本能：我们用来在日常生活中导航而不消耗太多认知能量的心理启发式和快速判断。系统 2 的处理是有意为之的，需要意识努力，这种理性思考和批判性思维与我们关联的复杂问题解决相联系。

### 质量和准确性

在大型语言模型（LLMs）中，像思维链提示这样的技术本质上迫使模型进入系统 2 模式，在该模式下，模型会迭代地重新审视其逻辑。这个推理过程可以显著提高准确性和可靠性，但可能会付出代价：更多的处理时间和更高的计算费用。这种权衡是真实的——系统 2 的思考可以带来更好的结果，但运行成本更高。

好消息是？聪明的工程策略有时可以恢复思维链处理的大部分质量增益，同时将延迟或令牌使用量减少到原来的二到三倍。2 技术如早期退出推理和推测解码使得在不完全付出系统 2 成本的情况下，更接近系统 2 的质量成为可能。

### 成本

最知名的 LLM（OpenAI 的 GPTs、Claude 的 Sonnet、Google 的 Gemini）提供按需付费的专属 API。价格因模型而异，并且根据市场和最新模型不断修订，但**不要**让大多数模型按字符计价的低廉成本欺骗了你。平均项目通常需要用户传递他们的代码、写作或文档的上下文以启动模型——每天积累数百万甚至数千万的字符上下文。这些成本会迅速增加。图 3-3 展示了来自一些最受欢迎的提供商的 21 个 LLM 的每百万字符成本。随着模型规模和质量的增加，价格也随之上升。

![条形图比较 Google、OpenAI 和 Anthropic 的 21 个 AI 模型的输入和输出字符定价，展示了随着模型质量提高成本增加的情况。](img/mmai_0303.png)

###### 图 3-3\. AI 模型定价比较

实际上，你提供给模型的内容的成本和模型提供给你的内容的成本之间存在显著差异。LLM 之间的入口成本相对较低，但从 LLM 返回给你的出口成本可能相当高。你要求模型返回的内容越多，你的成本就会越高。因此，对于那些希望将自己定位为产品核心的公司，仔细考虑这些模型的使用程度以及你愿意花费多少是值得的。

# 多模型策略的兴起

随着代理系统变得更加复杂，组织和行业专家越来越放弃“一刀切”的模型选择方法。相反，一种更复杂的多模型策略正在成为最佳实践。这涉及到使用不同 LLM 的组合，而不是坚持使用单一 LLM 供应商。在这个多模型策略中，模型选择取决于上下文，并针对更大工作流中特定任务的特定要求进行优化：

不同模型的专用角色

常见且有效的一种模式是创建一个模型层级。一个功能强大但价格更高且可能运行速度更慢的模型，例如 o3 Pro 或 Claude Opus 4，用于最复杂的认知任务，如高级规划、复杂推理或把一个难题分解成子任务。一旦计划制定完成，更直接、更简单的子任务的执行就委托给一支由更小、更快、成本效益更高的模型组成的队伍，例如 Claude Sonnet 4、Gemini Flash 2.5 或 GPT-5 mini。这种方法使得系统在推理和成本效益执行之间达到平衡。

可扩展的编排

对于部署数百甚至数千个代理的大型企业来说，依赖每个操作的单个强大、单一模型可能会非常昂贵且难以扩展。更可扩展的架构可能涉及使用稀疏模型，如 Mixtral 这样的专家混合（MoE）模型，或者编排大量专门轻量级代理。这允许系统以成本效益的方式扩展，同时保持高性能。

# 实证评估框架

在这样一个快速变化的行业中，许多人将行业基准视为有用的信号，以确定他们应该使用哪些模型。虽然此类公共基准对于比较模型是一个有用的起点，但它们不能保证模型在你的特定项目或专有用例上的表现。因此，对于所有代理和 LLM 的开发者和架构师来说，开发他们自己的特定领域评估框架来测试模型与他们的独特数据和需求是至关重要的。以下是方法。

## 创建特定用例的测试标准

开发内部评估框架的第一步是为代理任务的成功定义清晰、客观和可衡量的标准——毕竟，你不能管理你不能衡量的东西。这些评估标准应超越仅仅检查单个“正确答案”，而应评估整个代理过程的整体质量。评估的关键领域包括：

任务完成

最基本的指标：代理是否成功实现了其高级目标？具体来说，是否有实质性工作按时并以相对正确的方式完成？

工具的正确性和效率

代理是否选择了正确的工具来完成工作？它是否以正确的参数调用了它们？工具的使用是否高效，或者是否存在冗余或不必要的调用？

推理的连贯性和相关性

代理的内部思维链是否逻辑清晰？其推理步骤是否直接有助于解决问题，或者它们是否无关紧要？

## “LLM 作为裁判”的方法论

从实践者的角度来说：评估数千个代理的输出是不切实际的，并且无法扩展。为了应对这一挑战，出现了一种强大的技术，即“LLM 作为裁判”的方法。这种方法涉及使用一个强大、最先进的 LLM 来充当一个公正的评估者。裁判 LLM 被提供代理的输出、原始提示、一组事实数据以及一个详细的评估标准或评分模板。然后，它被提示根据定义的标准评估代理的表现，并为它的判断提供分数和定性解释。为了提高这种方法的可信度，通常使用高级提示技术，如思维链提示，它指导裁判 LLM 逐步思考其评估，使其推理更加透明，并减少随机变异性。这种方法使得对代理性能的评估具有可扩展性、可重复性和细微差别，尽管它需要仔细的提示工程和对裁判模型自身潜在偏差的认识。

^(1) Daniel Kahneman, *Thinking, Fast and Slow* (Farrar, Straus, and Giroux, 2011).

^(2) Yaniv Leviathan, Matan Kalman, and Yossi Matias, “Fast Inference from Transformers via Speculative Decoding,” in *Proceedings of the 40th International Conference on Machine Learning*, PMLR 202, 2023; Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, et al., “LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding,” in *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 12622–12642*, 2024.
