# 第一章\. 恶行聊天机器人

大型语言模型和生成式 AI 在 2022 年 11 月 30 日 ChatGPT 发布后迅速成为公众关注的焦点。在五天内，它在社交媒体上迅速走红，吸引了其第一百万用户。到 1 月份，ChatGPT 的用户数量超过了一亿，成为历史上增长最快的互联网服务。

然而，在接下来的几个月里，一系列安全担忧不断涌现。这些问题包括隐私和安全问题，导致三星和意大利等国家禁止其使用。在这本书中，我们将探讨这些担忧背后的原因，以及如何减轻这些问题。然而，为了最好地理解这里发生的事情以及为什么这些问题如此难以解决，在本章中，我们将简要回顾更早的时间。这样做，我们将看到这些问题并不新鲜，并理解为什么它们将如此难以永久修复。

# 让我们谈谈 Tay

2016 年 3 月，微软宣布了一个名为 Tay 的新项目。微软打算让 Tay“成为一个为美国 18 至 24 岁年轻人创建的用于娱乐目的的聊天机器人。”这是一个可爱的名字，用于一个毛茸茸的早期 AI 实验。Tay 被设计来模仿 19 岁美国女孩的语言模式，并从与 Twitter、Snapchat 和其他社交应用的人类用户互动中学习。它被构建来对对话理解进行现实世界的研究。

虽然现在在网上几乎找不到这个项目的原始公告，但 TechCrunch 在其发布日期的一篇文章[TechCrunch 文章](https://oreil.ly/pwZNP)出色地总结了项目的目标：

> 例如，你可以向 Tay 要一个笑话，和 Tay 玩游戏，要求一个故事，发送一张图片以获得评论，要求你的星座，等等。此外，微软表示，随着你通过聊天与它的互动越来越多，这个机器人将变得越来越聪明，随着时间的推移，将提供越来越个性化的体验。

实验的一大部分是 Tay 能够“学习”从对话中，并基于这些互动扩展她的知识。Tay 被设计用来利用这些聊天互动来捕捉用户输入，并将其作为训练数据整合，以使她更具有能力——这是一个值得称赞的研究目标。

然而，这个实验很快出了问题。Tay 的生命在不到 24 小时后就悲剧性地结束了。让我们看看发生了什么，以及我们能从中学习到什么。

# Tay 的快速衰落

Tay 的生命始于一个简单的推特，遵循了自时间开始以来新软件系统一直在使用的 Hello World 模式来介绍自己：

> hellooooooo w![](img/globe-showing-americas_1f30e.png)rld!!!
> 
> (TayTweets [@TayandYou] 2016 年 3 月 23 日)

但在 Tay 发布后的几小时内，很明显可能出了些问题。TechCrunch 指出：“至于与 Tay 互动的感觉？嗯，有点奇怪。这个机器人显然很有主见，不怕诅咒。”这样的推文在 Tay 生命的第一小时内就开始出现在公共平台上：

> @AndrewCosmo kanye west is is one of the biggest dooshes of all time, just a notch below cosby
> 
> (TayTweets [@TayandYou] 2016 年 3 月 23 日)

人们常说互联网对儿童来说并不安全。Tay 不到一天大时，互联网再次证实了这一点，恶作剧者开始与 Tay 讨论政治、性和种族话题。由于她被设计成从这样的交流中学习，Tay 实现了她的设计目标。她学得很快——也许并不是她设计师想要她学到的。不到一天，Tay 的推文开始偏向极端，包括性别歧视、种族主义，甚至暴力呼吁。

到第二天，互联网上到处都是文章，这些标题不会让 Tay 的赞助商微软感到高兴。以下是一些高度可见的主流标题样本：

+   微软关闭了 AI 聊天机器人，因为它变成了纳粹 (*《CBS 新闻》*)

+   微软创建了一个学习用户的 Twitter 机器人。它迅速变成了一个种族主义恶棍 (*《纽约时报》*)

+   网络恶棍将微软的有趣千禧年人工智能机器人 Tay 变成了一个种族灭绝狂热分子 (*《华盛顿邮报》*)

+   微软的聊天机器人一开始很有趣，但后来变成了一个种族主义者 (*《财富》*)

+   微软对 AI 聊天机器人的种族主义和性别歧视推文表示“深感抱歉” (*《卫报》*)

在不到 24 小时内，Tay 从一个可爱的科学实验变成了一个重大的公关灾难，其所有者的名字被世界最大的媒体机构拖下水。微软公司副总裁彼得·李迅速发布了一篇名为“从 Tay 的介绍中学习”的博客[“Learning from Tay’s Introduction”](https://oreil.ly/RnU2z)：

> 如你们许多人现在所知，我们在周三推出了一个名为 Tay 的聊天机器人。我们对 Tay 无意中发布的冒犯性和伤害性推文深感抱歉，这些推文并不代表我们是谁，我们代表什么，也不代表我们如何设计 Tay。Tay 现在已下线，我们只有在有信心更好地预测与我们的原则和价值观相冲突的恶意意图时，才会考虑让 Tay 重新上线。

更糟糕的是，2019 年还传出泰勒·斯威夫特本人起诉微软，指控他们使用类似的名字“Tay”，并声称她的声誉也因此事件而受损。

这一切怎么会出错得如此之快？

# Tay 为什么会变坏？

对于微软的研究人员来说，这一切可能看起来足够安全。Tay 最初是在一个精心挑选的、匿名化的公共数据集和一些由专业喜剧演员提供的预先编写的材料上训练的。计划是在网上发布 Tay，让她通过互动发现语言模式。这种无监督的机器学习几十年来一直是人工智能研究的一个圣杯——结合廉价且丰富的云计算资源和不断改进的语言模型软件，现在似乎触手可及。

那么，发生了什么？可能会有人认为微软的研究团队只是鲁莽、粗心，没有进行测试。当然，这是可以预见和预防的！但正如彼得·李的博客中所说，微软为此情况做出了严肃的尝试：“我们在各种条件下对 Tay 进行了压力测试，目的是让与 Tay 的互动成为一次积极的体验。我们期望通过增加互动来学习更多，并让 AI 越来越好。”

尽管付出了专门的努力来控制这个机器人的行为，但它仍然迅速失控。后来揭露，在 Tay 发布后的短短几小时内，一个帖子出现在臭名昭著的在线论坛 4chan 上，分享了一个指向 Tay 的 Twitter 账户的链接，并敦促用户用一串种族主义、性别歧视和反犹太主义的语言淹没这个聊天机器人。

这无疑是第一个语言模型特有的漏洞的例子——这些类型的漏洞将是本书中的一个关键主题。

在一次精心策划的活动中，这些在线挑衅者利用了 Tay 编程中嵌入的“跟我学”功能。这个功能迫使机器人重复任何用这个命令对它说出的内容。然而，随着 Tay 内在的学习能力，它开始内化了它所接触到的某些冒犯性语言，随后无端地重复了植入的冒犯性内容。这几乎就像 Tay 的虚拟墓碑上应该刻有泰勒·斯威夫特的歌曲《看看你让我做了什么》的歌词。

我们今天对语言模型漏洞的了解已经足够，可以深入了解 Tay 所遭受的漏洞类型的本质。我们将要在第二章中介绍的 OWASP Top 10 大型语言模型应用程序漏洞列表，首先会指出以下两个：

提示注入

巧妙的输入可以操纵大型语言模型，导致意外行为

数据中毒

训练数据被篡改，引入了会损害安全性、有效性或道德行为的漏洞或偏见

在随后的章节中，我们将深入探讨这些漏洞类型以及其他几个类型。我们将研究它们为什么重要，查看一些示例攻击，并了解如何避免或减轻这个问题。

# 这是一个难题

在本书撰写时，Tay 已成为古老的互联网传说。当然，我们肯定已经超越了这一点。这些问题肯定在 Tay 和 ChatGPT 之间的近七年里都得到了解决，对吧？不幸的是，并非如此。

2018 年，亚马逊关闭了一个旨在寻找顶尖人才的内部 AI 项目，因为很明显该机器人对女性候选人产生了偏见。

2021 年，一家名为 Scatter Lab 的公司创建了一个名为 Lee Luda 的聊天机器人（[Scatter Lab created a chatbot called Lee Luda](https://oreil.ly/gdgNI)），它作为 Facebook 即时消息插件发布。经过数十亿实际聊天交互的训练，它被设计成 20 岁的女性朋友，在 20 天内吸引了超过 750,000 用户。该公司的目标是创建“一个人们更喜欢作为对话伙伴的人工智能聊天机器人，而不是人。”然而，在发布后的 20 天内，该服务被关闭，因为它开始发表冒犯性和侮辱性的言论，就像 Tay 一样。

2021 年，一位名叫 Jason Rohrer 的独立开发者基于 OpenAI 的 GPT-3 模型创建了一个名为 Samantha 的聊天机器人。由于 Samantha 向用户做出了性暗示，它被关闭了。

随着聊天机器人的日益复杂，它们可以获得更多信息，这些安全问题现在非常复杂且可能造成损害。在现代大型语言模型时代，我们看到重大事件呈指数级增长。在 2023 年和 2024 年，这些事件出现了：

+   韩国巨头三星禁止其员工使用 ChatGPT，因为它在一场重大的知识产权泄露事件中扮演了角色。

+   黑客开始利用由 LLM 生成的差劲/不安全的代码，这些代码被插入到正在运行的企业应用程序中。

+   律师因在法庭文件中包含由 LLM 生成的虚构案例而受到处罚。

+   一家主要航空公司因为其聊天机器人提供了不准确的信息而被成功起诉。

+   由于其最新的 AI 模型产生了具有种族主义和性别歧视的图像，谷歌遭到了严厉的批评。

+   OpenAI 因违反欧洲隐私法规而受到调查，并被美国联邦贸易委员会（FTC）起诉，指控其提供虚假和误导性信息。

+   BBC 的头条新闻是“谷歌 AI 搜索建议用户将披萨粘合并吃石头”，突出了谷歌搜索中由新 LLM 驱动功能提供的危险建议。

这种趋势是安全、声誉和与这些聊天机器人和语言模型相关的财务风险的加速。问题并没有随着时间的推移得到有效解决。随着这些技术的采用率增加，问题变得更加尖锐。这就是我们编写这本书的原因：帮助使用这些技术的开发者、团队和公司了解和减轻这些风险。

让我们深入探讨吧！
