# 第十一章\. 信任过程

> 如果你不能将你所做的事情描述为一个过程，你就不知道你在做什么。
> 
> W. Edwards Deming

我们在这本书的大部分内容中探讨了在生产中应用 LLM 技术的危险。虽然技术具有巨大的力量，但风险也很多。安全、隐私、财务、法律和声誉风险似乎无处不在。有了这种理解，你如何能够有信心地前进？是时候讨论可操作、持久、可重复的解决方案了。虽然我们已经讨论了针对每个风险的实用缓解策略，但将它们作为补丁组合单独处理不太可能奏效。你必须将安全构建到你的开发过程中，以确保你的成功。

本章将讨论两个在成功项目中成为关键要素的过程元素。首先，我们将讨论 DevSecOps 运动的演变以及它如何成为任何大型软件项目应用安全的核心。我们将探讨它如何演变以涵盖与 AI/ML 和 LLM 相关的具体挑战。作为这次讨论的一部分，我们将查看开发时用于扫描安全漏洞的工具以及运行时工具（称为护栏），这些工具可以帮助保护你的 LLM 在生产中的安全。

我们还将探讨安全测试的演变以及新兴的 AI 红队领域。红队在网络安全领域已经存在很长时间了，但随着特定技术的发展，这些技术适用于 LLM 项目，AI 红队最近获得了更多的关注。

# DevSecOps 的演变

*DevOps* 的起源可以追溯到 2000 年代初，当时它作为对软件开发（Dev）和 IT 运维（Ops）团队之间日益增长的更好协作和整合需求的回应而出现。这种需求源于对传统软件开发方法论的局限性观察，这些方法论往往导致团队孤立、发布延迟，以及开发目标与运营稳定性之间需要更多一致性的需求。DevOps 运动旨在通过推广协作、自动化、持续集成和持续交付（CI/CD）的文化，从而提高软件部署的速度和质量。

随着 DevOps 实践的成熟和更广泛的采用，将安全原则整合到开发生命周期中的关键需求变得越来越明显。这种认识导致了将安全（Sec）整合到 DevOps 过程中，形成了*DevSecOps*。DevSecOps 通过在软件开发过程的每个阶段嵌入安全，从设计到部署，丰富了 DevOps 实践。目标是确保安全考虑不是事后想法，而是集成到工作流程中，从而实现早期发现和缓解漏洞，从而构建更安全的软件。

我们希望在开发和使用 LLM 的应用程序部署中实现同样的主动安全立场。为此，DevOps 和 DevSecOps 的原则进一步激发了*MLOps*和*LLMOps*的出现，以解决部署和管理 AI/ML 系统的独特挑战和需求。

MLOps（机器学习运维）专注于自动化和优化机器学习生命周期（包括数据准备、模型训练、部署和可观察性），以确保机器学习模型开发和维护的一致性和效率。LLMOps（大型语言模型运维）明确针对大型语言模型的运营需求，重点关注提示工程、模型微调和 RAG 等方面。这些专业实践展示了 DevOps 哲学的持续扩展，该哲学已经适应了新兴技术的运营和安全需求，从而确保它们能够有效地集成到更广泛的软件开发和部署生态系统中。结合 MLOps 和 LLMOps 的概念将有助于您将组织的 DevSecOps 流程扩展到考虑添加高级 AI 技术到您的堆栈的具体需求。

## MLOps

MLOps 是一套最佳实践，旨在简化和自动化机器学习生命周期，从数据准备和模型开发到部署和监控。MLOps 的关键要素包括模型和数据的版本控制，确保可重复性和可追溯性，以及为选择最佳模型候选者进行的模型训练和验证。

CI/CD 管道针对机器学习工作流程进行定制，以自动化模型的测试和部署，并在生产中监控模型性能，以捕捉和解决由于模型或数据漂移而导致的模型退化。此外，MLOps 强调数据科学家、机器学习工程师和运维团队之间的协作，以促进更高效和无缝的开发过程，确保机器学习模型准确、可扩展和可维护。

MLOps 基础设施在机器学习系统的安全格局中扮演着至关重要的角色。通过在整个机器学习生命周期中整合安全实践，MLOps 可以帮助在开发早期识别和缓解风险。这包括确保数据隐私和符合 GDPR 等法规，管理对敏感数据集的访问，以及保护模型端点免受对抗性攻击。自动漏洞扫描和将安全检查纳入 CI/CD 管道有助于在部署前发现安全问题。此外，监控已部署模型的不寻常行为可以检测潜在的网络安全漏洞，从而为机器学习应用提供更稳健的安全态势。

## LLMOps

MLOps 虽然在为任何利用机器学习的应用程序建立实践方面至关重要，但它并没有解决 LLMs 提出的所有独特挑战。LLMs 引入了特定的挑战，例如提示工程、强大的监控来捕捉细微的性能，以及生成输出的潜在滥用。这意味着我们必须利用 DevSecOps 和 MLOps 所能教给我们的最好方法，然后添加更多针对 LLMs 的特定技术。

LLMOps 作为一门解决这些挑战的专业学科而发展起来。它包括针对在生产环境中部署、监控和维护 LLMs 的特定实践。LLMOps 在模型版本化和管理方面处理更大规模的方面，采用高级部署策略来处理高计算负载，以及针对评估模型输出定性方面的特定监控技术。此外，LLMOps 强调提示工程和反馈循环的重要性，以优化模型性能并减轻与模型生成内容相关的风险。这种专业化的关注确保了 LLM 部署的高效性、道德性以及与用户期望和监管要求的对齐。

现在，让我们探讨如何将安全实践最佳地整合到 LLMOps 中，以确保交付更安全应用程序的可重复过程。

# 将安全性融入 LLMOps

所有关于 DevSecOps、MLOps 和 LLMOps 的讨论可能听起来令人畏惧。然而，为了确保我们构建安全 LLM 应用的过程安全，所需的关键任务可以分解为五个简单的步骤：基础模型选择、数据准备、验证、部署和监控，如表 11-1 所示。

表 11-1\. LLMOps 步骤

| 任务 | LLMOps 安全措施 |
| --- | --- |
| 基础模型选择 | 选择具有强大安全功能的基座模型。评估模型的来源的安全历史和漏洞报告。审查与基座模型一起提供的模型卡和安全特定信息。审查有关用于训练基座模型的训练数据集的信息。实施监控基座模型新版本的流程，这些新版本可能添加安全或对齐改进。 |
| 数据准备 | 如果您计划使用微调或 RAG 来增强应用程序可用的特定领域知识，您必须准备您的数据。仔细评估您的数据集来源。确保数据被清理、匿名化，且不包含非法或不适当的内容。评估您的数据以确定可能的偏见。在微调或嵌入生成期间实施安全数据处理和访问控制。 |
| 验证 | 将您的安全测试扩展到包括针对 LLM 的特定漏洞扫描器和 AI 红队演习。（我们将在本章后面更多地讨论 AI 红队。）将您的验证步骤扩展到检查非传统安全威胁，如毒性和偏见。 |
| 部署 | 确保你有适当的运行时防护措施来筛选进入模型和输出的提示。自动化你的构建过程，以确保每次更改后都重新生成并存储你的机器学习物料清单（ML-BOM）。 |
| 监控 | 记录所有活动并监控异常，这些异常可能表明越狱、拒绝服务尝试或其他对基础设施的破坏。 |

# LLM 开发过程中的安全

现在是时候超越流程抽象，采取实际步骤使你的安全开发流程可重复。我们将探讨涵盖整个开发生命周期的主题。我们将首先查看如何确保你的开发环境和管道是安全的。然后，我们将探讨在部署前可以使用哪些针对 LLM 的特定安全测试工具来检查你的安全程序。我们还将回顾你必须采取的步骤，以确保你的软件供应链的安全性。

## 保护你的 CI/CD

开发管道的安全性对于防止你的项目成为供应链中的薄弱环节至关重要。在第九章中，我们回顾了 SolarWinds 案例研究，它展示了如果你的管道被破坏，对你和你的下游客户来说可能会多么灾难性。本节探讨了加强管道以抵御威胁的策略，确保你的 LLM 应用不会受到损害或无意中导致下游用户的网络安全漏洞。

### 实施稳健的安全实践

让我们看看你需要实施安全计划的几个关键实践：

CI/CD 安全

将安全检查集成到持续集成/持续部署（CI/CD）管道中，以便在开发早期自动检测漏洞或配置错误。

依赖关系管理

定期审计和更新项目中使用的依赖项，以减轻与过时或受损库相关的漏洞。最近报告了 PyTorch 等特定于 ML 的开源构建管道组件的严重零日安全漏洞，这证明了这一步骤的重要性。

访问控制和监控

限制对 CI/CD 环境的访问，并监控活动，以便及时检测和响应可疑行为。像保护源代码一样保护你的训练数据存储库，以帮助防止可能的数据中毒攻击。

### 培养安全意识文化

训练你的团队成员与训练你的大型语言模型（LLM）在构建安全应用中同样重要。以下是一些关于如何培训和准备你的团队成员的事项：

培训和意识

教育开发团队成员关于供应链安全的重要性以及他们在维护中的作用。确保你的团队了解必须作为应用程序供应链的一部分管理的新的组件，例如基础模型和训练数据集。

事件响应计划

制定并定期更新一个事件响应计划，包括应对供应链威胁的程序，包括零日漏洞披露。

## LLM 特定安全测试工具

应用安全测试工具可以有多种形式，例如静态应用安全测试（SAST）、动态应用安全测试（DAST）和交互式应用安全测试（IAST）。所有这些都已经确立了自己在开发传统 Web 应用中的不可或缺的地位。虽然每种方法都有其优势和劣势，但它们都帮助自动化漏洞和安全缺陷的识别，促进早期检测和修复。将它们集成到软件开发生命周期中，使组织能够采取主动的安全立场，确保应用在设计时就具备功能和安全性。

LLMs 带来了独特的安全挑战，这些挑战尚未被传统的安全测试方法完全解决。它们的复杂性、新颖性和易受数据偏差、幻觉和对抗性攻击等问题的影响，需要针对其特定环境定制化的工具。尽管该领域相对较新，但旨在加强 LLM 应用以抵御一系列漏洞的新工具正在开始出现。让我们看看几个例子。

### TextAttack

TextAttack 自 2020 年以来以某种形式存在。它是一个复杂的 Python 框架，旨在为 NLP 模型（包括 LLMs）进行对抗性测试。它是免费且开源的，遵循 MIT 许可证发布，便于探索语言模型中的漏洞并开发针对对抗性攻击的强大防御措施。

TextAttack 脱颖而出，提供了一种模块化架构，允许对各种模型和数据集的攻击策略进行定制和测试。它模拟对抗性示例以揭示 NLP 应用中的潜在弱点，从而指导模型弹性的改进。该工具提供了关于攻击方法、成功率以及模型响应的详细报告，对于安全评估来说价值连城。其适应性和对攻击技术的全面覆盖使 TextAttack 成为开发者和研究人员增强 LLM 应用安全和可靠性的强大工具。

### Garak

以一个鲜为人知的*星际迷航*角色命名的 Garak 是一个 LLM 漏洞扫描器。Garak 由 Leon Derczynski 开发，他是 LLM 应用 OWASP Top 10 第一版的重要贡献者。Garak 免费使用，并遵循宽松的 Apache 开源许可证发布。

Garak 采用与 DAST 工具类似的模式，它在运行时探测应用程序并检查其行为，寻找漏洞。该工具向模型发送各种提示，使用检测器分析多个输出以识别不受欢迎的内容。结果未经科学验证，但更高的通过百分比表明更好的性能。它可以通过插件进行定制，以添加额外的提示或漏洞。它生成包含所有测试参数、提示、响应和分数的详细报告。根据用户贡献和请求，有潜力扩展到不同的模型和漏洞。

### 负责任 AI 工具箱

由微软开发的[负责任 AI 工具箱](https://oreil.ly/6hpZE)是一个开源工具套件，它使开发人员和数据科学家能够将道德原则、公平性和透明度融入他们的 AI 系统中。此工具箱遵循 MIT 许可证分发，并提供一个综合环境来评估、改进和监控负责任 AI 的各个维度的模型，包括公平性、可解释性和隐私。

### Giskard LLM 扫描

Giskard LLM 扫描是一个开源工具，用于评估 LLM 的道德考虑和安全。在 Apache 2.0 许可证下可用，Giskard AI 套件的这一部分旨在识别偏见、检测有害内容的实例，并促进 LLM 的负责任部署。它采用各种指标和测试，旨在评估 LLM 在公平性、毒性和包容性方面的行为。通过其界面，Giskard LLM 扫描提供详细报告，突出关注领域，帮助开发人员和研究人员理解和可能减轻其 AI 模型中的道德风险。

### 将安全工具集成到 DevOps 中

将自动化的、针对 LLM 特定的安全测试工具和传统的 AST（应用程序安全测试）工具集成到 LLMOps 流程中不仅有益，而且是强制性的。将这些工具嵌入到 CI/CD 管道中确保安全不是事后考虑，而是应用开发的基础部分。这种方法使得每次构建时都能进行自动化的、可重复的安全检查，显著降低了生产中的漏洞风险。此外，它还培养了开发团队的安全意识文化，确保从项目的开始到部署，安全考虑始终是首要的。

## 管理您的供应链

如第九章所述，供应链不仅代表组件和工具的采购，还涉及开发工件（如模型卡和 ML-BOM）的细致生成、存储和可访问性。

模型卡片是 LLMs 的必要文档，概述了模型的目的、性能和潜在偏差。同样，ML-BOMs 详细说明了使用 LLM 等机器学习技术开发应用程序所涉及的组件、数据集和依赖项。这些工件共同构成了 LLMs 开发中透明度和问责制的基石。

为了有效地管理它们，开发者必须实施生成、存储和使这些工件易于搜索的系统。这有助于合规性监管并增强利益相关者的协作和信任。通过将这些实践整合到更广泛的 SBOM 策略中，团队可以确保对其应用程序的 AI 和非 AI 组件有一个全面的视角，从而加强供应链的安全性和完整性。

您需要关注三个支柱，以确保您的工件得到适当跟踪，从而帮助确保您对供应链有控制权：

自动生成

在关键开发里程碑处实现自动生成模型卡片和 ML-BOM 的工具和工作流程。

安全存储

将这些工件存储在安全、版本控制的存储库中，以确保它们防篡改且可检索。

可访问性

使这些工件对相关利益相关者可访问，并整合搜索功能以促进快速检索和审查。

在 LLMs 应用开发中，供应链是一个复杂的生态系统，需要勤勉的管理以确保开发工件和开发管道的安全性和完整性。通过优先考虑关键工件如模型卡片和 ML-BOMs 的生成和存储，并通过保护开发管道，组织可以防范供应链漏洞，增强其 LLM 应用程序的信任和可靠性。

# 使用护栏保护您的应用程序

如同网络应用防火墙（WAFs）和运行时应用自我保护（RASP）等工具已成为在运行时防御 Web 应用程序攻击的基本工具。与在构建和测试时分析代码以查找漏洞的 AST 工具不同，WAFs 和 RASP 在应用程序运行时提供持续的保护。它们作为警觉的守护者，实时识别和缓解威胁，从而增加了一个关键的安全层。

在 LLMs 的背景下，可以将“护栏”概念与之类比。护栏有助于确保 LLMs 在定义的道德、法律和安全参数内运行，防止滥用并引导模型生成适当和安全的输出。最初，护栏的实现相对简单，通常是在内部构建并针对特定用例定制。在第七章中，我们探讨了构建一些简单护栏的过程，以帮助筛选 LLM 的输出，防止毒性和 PII。这项练习是理解一些护栏如何工作的基础知识的好方法。

然而，随着基于 LLM 的应用变得更加复杂，对更高级的安全和安全性框架的需求也在增加。如今，一个蓬勃发展的生态系统正在形成，其中包括开源和商业工具，为 LLM 提供更全面的护栏框架。这些工具作为运行时安全措施，持续监控和引导 LLM 的行为，以防止生成有害、有偏见或其他不希望的内容。它们类似于网络应用空间中的 WAF 和 RASP，提供一种动态的防护盾，能够适应新兴的威胁和挑战。

## 在 LLM 安全策略中，护栏的作用

将高级护栏解决方案纳入 LLM 部署不仅是一个建议，而正在成为一种必要性。随着这些模型越来越深入地集成到关键和面向消费者的应用中，它们误用或故障的潜在影响呈指数增长。护栏提供了一种减轻这些风险的方法。护栏框架提供了一系列功能，但以下是一些在评估选项时您希望寻找的典型功能。

### 输入验证

实施扫描输入到您的 LLM 的护栏有一些好处：

预防提示注入

监控即时注入的迹象，例如不寻常的短语、隐藏的字符和奇特的编码，以防止恶意操纵 LLM。

领域限制

通过限制或忽略不相关的提示，使 LLM 专注于相关主题。这通过减少生成不适当或不相关内容的风险和降低幻觉的可能性来增强安全性。

匿名化和秘密检测

在与 LLM 交互时，用户可能会输入机密数据，如电子邮件地址、电话号码或 API 密钥。如果数据被记录、存储或传输到第三方 LLM 提供商，或者如果数据可能被用于训练目的，这就会成为一个问题。在 LLM 处理之前匿名化 PII 和删除敏感数据至关重要。

### 输出验证

对您 LLM 的所有输出进行筛选是您零信任策略的关键部分。以下是一些好处：

道德审查

过滤输出内容，以避免被认为是具有毒性、不适当或仇恨的内容，确保 LLM 的交互符合道德准则。这本来可以拯救可怜的 Tay 从第一章以及其他无数项目免受未检查的毒性等漏洞的侵害。

敏感信息保护

实施措施以防止通过 LLM 的输出泄露 PII 或其他敏感数据。

代码输出

寻找可能导致下游攻击的意外代码生成，例如 SQL 注入、服务器端请求伪造（SSRF）和 XSS。

合规性保证

在具有严格监管标准的行业，如医疗保健或法律，调整输出以满足特定的合规要求，并确保 LLM 的响应在其预期用途范围内。

事实核查和幻觉检测

将 LLM 输出的准确性与可信来源进行验证，以确保提供的信息是事实性和可靠的。识别并减轻 LLM 生成虚构或不相关内容的情况，以确保输出保持相关性并扎根于现实。

## 开源与商业护栏解决方案的比较

在开源和商业护栏解决方案之间进行选择取决于多个因素，包括组织的具体需求、所需的定制程度以及预算考虑。

开源工具提供了灵活性和社区支持的好处，允许组织根据其独特需求定制解决方案。然而，它们可能需要大量的内部专业知识和资源才能有效部署和维护。您可能希望评估的一些开源护栏工具包括 NVIDIA NeMo-Guardrails、Meta Llama Guard、Guardrails AI 和 Protect AI。

另一方面，商业解决方案可能提供更多开箱即用的功能，并附带专业支持、定期更新和高级功能的好处。一些商业护栏选项的例子包括 Prompt Security、Lakera Guard、WhyLabs LangKit、Lasso Security、PromptArmor 和 Cloudflare Firewall for AI。

## 混合自定义和包装的护栏

在第七章中，我们手动实现了一些基本的护栏。虽然预构建的护栏框架的出现可以在安全性方面提供显著的提升，但这些手工制作的护栏仍然有其作用。用您自己的定制、领域或应用程序特定的护栏补充护栏框架是有意义的。这类深度防御策略在网络安全中通常是最成功的。

# 监控您的应用程序

在 LLM 应用的生命周期中，有效的监控不仅包括传统的组件——如网络服务器、中间件、应用程序代码和数据库——还包括 LLM 本身及其用于 RAG 的相关向量数据库的独特元素。这种全面的方法对于在整个应用生命周期中保持操作完整性和安全性至关重要。

## 记录每个提示和响应

监控 LLM 应用程序的基础实践之一是记录每个提示和响应。这种详细的记录具有多重目的：它提供了用户如何与应用程序互动的见解，使识别潜在的滥用或问题输出成为可能，并形成了解模型性能随时间变化的基线。这种细粒度的数据收集对于诊断问题、优化模型行为和确保符合数据治理标准至关重要。

## 集中式日志和事件管理

将日志和应用程序事件聚合到*安全信息和事件管理*（SIEM）系统中是至关重要的。SIEM 系统使整个应用程序堆栈的数据整合成为可能，提供了一个对所有活动的统一视图。这使得你的组织能够轻松存储应用程序对每个用户输入的响应的历史记录。这些集中式日志可以随后用于合规目的存储。此外，SIEM 系统提供高级搜索工具，使你的团队能够快速搜索大量提示和响应中的模式。这可以使你的安全运营团队能够在生产中寻找威胁。

## 用户和实体行为分析

为了进一步增强监控能力，可以在 SIEM（安全信息和事件管理）系统之上叠加*用户和实体行为分析*（UEBA）技术。UEBA 通过利用机器学习和分析来理解用户和实体通常如何与应用程序交互，从而能够检测出偏离常规的活动。对于 LLM 应用，将 UEBA 框架扩展到包括模型特定的行为——例如不寻常的提示-响应模式或对向量数据库的非典型访问——可以提供关于安全漏洞、数据泄露或需要模型重新训练的早期预警信号。此外，使用模式的重大变化可以帮助你识别拒绝服务、拒绝钱包和模型克隆攻击，如第八章第八章中所述。

# 构建你的 AI 红队

到目前为止，在本章中，我们探讨了如何确保你的开发管道安全，以可重复的方式使用安全测试工具，并在生产中保护和监控你的应用程序。这些都是关键步骤，但它们反复被证明是理解你的应用程序在现实世界中的行为的必要但不充分的步骤。新兴的*AI 红队*领域正是为此而设计的。让我们看看 AI 红队如何成为验证你的应用程序安全性的重要部分。

AI 红队是一组采用对抗性方法，使用 AI 技术（如 LLM）严格挑战应用程序安全性和安全性的安全专业人士。他们的目标是识别和利用 AI 系统中的弱点，就像外部攻击者可能会做的那样，但目的是提高安全性而不是造成损害。

###### 注意

当美国总统拜登于 2023 年 10 月发布其“关于安全、可靠和值得信赖的人工智能开发和使用行政命令”时，AI 红队被推到了 AI 和 LLM 安全讨论的前沿，该命令包含以下内容：

+   *“人工智能红队”一词意味着一种结构化测试工作，旨在发现人工智能系统中的缺陷和漏洞，通常在受控环境中进行，并与人工智能的开发者合作。人工智能红队通常由采用对抗性方法的专用“红队”执行，以识别缺陷和漏洞，例如人工智能系统的有害或歧视性输出、不可预见或不受欢迎的系统行为、局限性或与系统误用相关的潜在风险.*

因此，美国国家标准与技术研究院（NIST）的人工智能安全研究所，作为国家研究院的一部分，已经创建了一个专门的红队最佳实践工作组。

人工智能红队基于人工智能系统具有传统软件可能不具备的独特漏洞的前提，例如对抗性输入攻击、数据中毒和模型窃取攻击。人工智能红队通过模拟现实世界的特定人工智能威胁，帮助组织预测和缓解安全漏洞。

人工智能红队的关键功能包括：

对抗性攻击模拟

设计和执行利用人工智能系统弱点的攻击，例如输入欺骗性输入以操纵结果或提取敏感数据。

漏洞评估

系统性地审查人工智能系统，以识别可能被攻击者利用的漏洞，包括底层基础设施、训练数据和模型输出中的漏洞。

风险分析

评估已识别漏洞的潜在影响，并提供基于风险的评估，以优先处理修复工作。

缓解策略开发

建议防御和反制措施，以保护人工智能系统免受已识别的威胁和漏洞。

意识和培训

教育开发者、安全团队和利益相关者关于人工智能安全威胁和最佳实践，以培养安全意识的人工智能开发文化。

人工智能红队对于强大的人工智能安全框架至关重要。它确保人工智能系统被安全地设计和开发，持续进行测试，并针对野外不断发展的威胁进行加固。

## 人工智能红队的优势

尽管传统安全措施是必要的，但它们通常不足以解决复杂的长短期记忆（LLM）特定漏洞。一个采用全面和对抗性方法的红队对于识别和缓解这些威胁至关重要，这不仅通过技术手段，还通过审视人类和组织行为的更广泛影响。

例如，幻觉代表了一个重大的风险。通过模拟高级测试场景，红队可以识别出可能导致此类行为的潜在触发因素，使开发者能够理解和缓解这些风险，这是自动化测试无法做到的。

数据偏差构成了一种更微妙但更深刻的威胁，因为它可能导致不公平或不道德的结果。红队可以评估数据收集和处理实践中的技术偏差和系统性问题。团队的外部视角可以揭示数据处理和算法训练中的盲点，这些盲点可能被专注于功能性的内部团队忽视。

在大型语言模型（LLM）中，模型可以超出其预期范围进行操作，这需要持续和创造性的测试来识别。红队可以探测 LLM 行为的极限，以确保针对意外自主行为的保护措施是强大和有效的。

提示注入攻击利用 LLM 处理输入以产生意外结果，突出了红队创新思维的需求。团队可以模拟复杂的攻击向量，挑战 LLM 安全处理对抗性输入的能力。

此外，过度依赖 LLM 等风险涉及技术、人类和组织因素。红队可以评估 LLM 集成到决策过程中的更广泛影响，突出自动化可能损害批判性思维或操作安全性的领域。

红队在 LLM 应用安全中的必要性不仅仅是增加另一层防御；它关乎采用一种全面和主动的安全方法，该方法解决从技术到人的全面风险。这种方法确保 LLM 应用能够抵御当前威胁，并准备好应对新兴漏洞。

## 红队与渗透测试

红队和传统的渗透测试经常被放在一起讨论，但它们在组织的网络安全态势中扮演着不同的角色。当我们分析这两种方法之间的差异时，我们必须认识到它们不是相互排斥的，而是在加强防御网络威胁方面是互补的。*渗透测试*是一种在特定时间点进行的评估，用于识别可利用的漏洞。相比之下，红队是一种持续、动态的过程，它模拟了组织整个数字和物理防御范围内的现实世界攻击。

在保护 LLM 应用完整性的情况下，红队尤其重要，因为攻击面广泛，与传统的应用在质量上截然不同。红队以与潜在对手心态一致的方式，进行更广泛和更灵活的安全测试。这包括技术漏洞以及安全性的组织、行为和心理方面。通过这种方式，红队还可以包括检查负责任和道德的结果，这对于完全自动化的测试来说极为困难。

表 11-2 总结了渗透测试和红队之间的差异。

表 11-2\. 渗透测试与红队对比

| 方面 | 渗透测试 | 红队 |
| --- | --- | --- |
| 目标 | 识别和利用特定的漏洞 | 模拟真实的网络攻击以测试响应能力 |
| 范围 | 专注于特定的系统、网络或应用程序 | 广泛的，包括各种攻击向量，如社会工程学、物理安全和网络安全 |
| 持续时间 | 短期，通常为几天到几周 | 长期，可能持续几周到几个月以模拟持续威胁 |
| 频率 | 定期间隔，或作为合规性评估的一部分 | 频繁或持续 |
| 方法 | 战术性的，旨在发现特定的技术漏洞 | 战略性的，旨在揭示系统性的弱点和组织响应 |
| 报告 | 详细列出漏洞及其修复步骤 | 对安全态势的全面评估以及整体改进的建议 |

## 工具和方法

虽然你可以完全独立地组建一个红队，但也有一些新兴的工具和服务可以帮助你。这个领域将迅速发展，但我们将回顾一些新兴的选项，以便你知道该寻找什么。

### 红队自动化工具

PyRIT（Python 生成式 AI 风险识别工具包）于 2024 年 2 月推出，是微软开源计划的一部分，旨在增强 AI 红队的功能。PyRIT 是从微软早期内部开发的一些工具演变而来的，旨在支持识别和分析生成式 AI 系统中的漏洞。该工具包作为人类红队人员的辅助工具，而不是替代品，强调工具包在增强以人为核心的安全努力中的作用。

PyRIT 自动化了红队测试过程中的某些方面，使安全专业人士能够高效地发现可能被利用在生成式 AI 系统中的潜在弱点。PyRIT 通过简化对对抗性攻击和数据中毒等问题检测，使人类红队人员能够将更多时间投入到战略性的复杂攻击模拟和创造性漏洞探索中。这种自动化与人类专业知识的结合旨在深化 AI 系统的安全测试，确保它们能够抵御广泛的网络威胁。

### 红队作为服务

HackerOne 的 AI 安全红队服务为那些缺乏时间、资源或专业知识来开发和维持内部红队以保护其 AI 系统的组织提供了一个可能的解决方案。这项服务提供了一种灵活的“作为服务”的方法，允许组织在不进行重大内部投资的情况下，访问进行全面的 AI 安全评估所需的专门技能和见解。

通过利用 HackerOne 的众包安全专业人士网络，公司可以从针对 AI 技术独特漏洞的彻底和创造性的对抗性测试中受益。这种外部专业知识支持识别和缓解潜在威胁，以灵活性和可扩展性来增强 AI 系统的安全态势，这与组织的需求和能力相一致。

# 持续改进

LLM 应用的安全部署不是一个一次性努力，而是一个持续改进和适应的旅程。从记录的提示和响应、用户和实体行为分析（UEBA）以及 AI 红队练习中获得的经验见解是这个过程中的宝贵资产。它们提供了一个丰富的数据集，从中学习并制定增强你的 LLM 应用安全和功能性的路线图。根据从这些来源看到的结果，有许多活动你可以持续执行以改善你的整体安全和安全态势。

## 建立和调整边界

在本章的早期，我们讨论了边界的重要性以及它们如何灵活实施。你应该将维护和更新你的边界作为你的 DevOps 流程的一部分。无论你是手动构建自己的边界还是使用前面讨论的框架之一，你仍然需要持续更新和调整它们：

自适应边界

利用你的监控和测试活动的见解来微调围绕你的 LLM 操作的现有边界。这可能涉及调整可接受行为的阈值、完善内容过滤器或增强数据隐私措施。

新的边界

除了调整之外，收集到的情报可以揭示需要全新的边界。这些可能解决新兴威胁、新的滥用模式或之前未注意到的意外模型行为。

## 管理数据访问和质量

在前两个章节中，我们讨论了在给你的大型语言模型（LLM）提供过多或过少数据之间的微妙平衡。在第五章中，我们讨论了敏感信息泄露的风险。在第六章中，我们讨论了幻觉的风险。通过将这些经验教训融入我们的流程中，我们可以帮助控制这些风险。现在是时候将新的专业知识添加到你的整体 DevSecOps 方法中。当你包括 MLOps 和 LLMOps 方法时，你希望将数据科学家和行为分析师纳入你的工作流程中：

数据访问

定期审查和管理你的 LLM 可以访问的数据。这包括移除对敏感或不相关数据的访问，并纳入新的数据集以帮助模型避免幻觉或偏差，从而提高其可靠性和输出质量。

质量控制

确保输入到您的 LLM 中的数据质量高且具有代表性。这降低了在误导性或有害信息上训练模型的风险，这会直接影响到其安全性和有效性。

## 利用 RLHF 进行对齐和安全

*从人类反馈中进行强化学习*（RLHF）是一种复杂的机器学习技术，显著提高了 LLMs（大型语言模型）的性能和对人类价值观和期望的对齐。其核心在于，RLHF 涉及使用由人类评估者生成的反馈来训练 LLMs，而不是仅仅依赖于预定义的奖励函数或数据集。这个过程从人类审查模型对特定输入或提示产生的输出开始。评估者随后提供反馈，这包括排名、评分，以及直接的纠正或偏好。这些由人类生成的反馈被用来创建或完善奖励模型，指导 LLM 生成更接近人类判断和道德标准的响应。RLHF 的迭代性质允许模型在准确性、相关性和安全性方面持续改进，这使得它成为开发以用户为中心的 AI 应用的关键工具。

RLHF 通过将人类洞察力整合到训练过程中，弥合了原始计算输出与人类交流中特有的语言和上下文细微理解的差距。这种方法提高了模型生成连贯且上下文适当的响应的能力，并确保这些输出符合道德指南和社会规范。随着 AI 应用越来越多地融入日常生活，RLHF 在确保这些技术以有益且无害的方式对人类行为发挥关键作用。

诚然，将 RLHF 纳入流程比简单的干预措施，如调整护栏、微调或增强 RAG 数据更为复杂、涉及更多且成本更高。然而，对于准确度、与人类价值观的对齐和道德考量至关重要的应用，RLHF 脱颖而出，成为最强大的工具之一。它通过直接的人类反馈迭代地完善和调整模型输出，使其成为开发不仅技术先进而且深刻理解人类互动和期望的 LLM 应用的无价资产。

###### 警告

虽然 RLHF 在使 LLM 与人类价值观保持一致并提高其性能方面提供了显著优势，但了解其局限性和潜在陷阱至关重要。首先，将人类反馈引入训练过程可能会无意中引入或放大偏见，反映了评估者的主观观点或无意识的偏见。此外，RLHF 本身并不能保护免受对抗性攻击；复杂的对手可能仍然找到利用模型响应中漏洞的方法。另一个担忧是潜在的*政策过度拟合*，其中模型在生成满足反馈的响应方面变得过于专业化，但失去了在更广泛背景下的泛化能力和性能。开发者需要仔细权衡这些因素，并考虑实施补充策略来缓解这些限制，并确保 AI 技术的负责任开发。

# 结论

将 LLM 集成到生产中是复杂的，需要一种对安全和运营的复杂方法。向 DevSecOps、MLOps 和 LLMOps 的转变代表了在开发、部署和保障软件中的关键演变，这突出了在开发生命周期中深入嵌入安全的重要性。这个基础对于应对 LLM 技术相关的风险至关重要，从隐私和安全到伦理和监管问题。

人工智能红队测试的作用提供了一种通过模拟对抗性攻击来主动识别和缓解潜在漏洞的方法。红队测试，与持续监控和改进原则相结合，为 LLM 应用安全提供了一个动态和有弹性的方法。它强调了在技术集成中保持警惕、适应性立场的重要性，其中持续评估和改进是防范不断演变威胁的关键。

保护 LLM 应用是一项强调持续、迭代过程重要性的旅程。通过严格应用开发、部署、监控和改进的循环，组织可以创建出无与伦比的稳健和安全系统。这种对持续改进的承诺，由每个循环中最新的安全实践和洞察力引导，确保每次迭代，应用都变得更加安全、更加符合道德标准。这种不懈的追求改进将导致最具有弹性的 LLM 应用，自信地迎接明天的挑战。
