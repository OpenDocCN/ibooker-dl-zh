# 第七章：不相信任何人

在最近对 Netflix 的《怪奇物语》电视剧的迷恋之前，90 年代有《X 档案》——这是我一生中最喜欢的节目之一。它讲述的是两名 FBI 特工调查诸如怪物、外星人以及政府阴谋等奇异现象。节目的主角 Fox Mulder 有两个标志性台词。其中一个是充满希望的：真相就在那里。另一个则是深深的偏执：不相信任何人。

在本章中，我们将关注第二个短语。我们将简要回顾典型 LLM 架构中固有的各种风险，并指出虽然实施之前讨论的缓解措施是值得的，但根本无法假设你的模型输出总是可信的。我们将采用 Mulder 的“不相信任何人”的口号，并探讨如何将零信任方法应用于你的 LLM 应用程序。当威胁真实存在时，偏执并不是疯狂！

零信任不仅仅是一个流行词；它是一个严格的框架，旨在假设威胁可能来自任何地方——甚至可能来自你信任的系统。这个模型对 LLMs 有益，因为 LLMs 通常从不太可信的来源摄取各种输入。我们将探讨如何管理你的 LLM 所拥有的“代理”——限制其做出可能损害你的系统或暴露敏感数据的自主决策的能力。此外，我们还将讨论实施强大的输出过滤机制的战略，为 LLM 生成的文本增加一个额外的审查层。过滤 LLM 的所有响应有助于使输出更安全，并与假设一切并验证一切的假设保持一致。

实质上，我们正在踏上转变思维方式的旅程。就像 Mulder 会质疑一切一样，我们也应该这样做。系好安全带；这将是一次引人入胜的旅程，穿越 LLMs 在零信任环境中的复杂性。

# 零信任解码

想象一下 Mulder 和他的 FBI 搭档 Dana Scully 进入一个高度受限的政府设施，但他们这次不能只是出示 FBI 徽章就走进去。相反，安全措施在每一扇门、计算机终端，甚至在访问文件时都会不断挑战他们。该设施不相信任何人，无论是清洁人员还是设施主任。这听起来可能像一集剧情，但事实上，这是零信任安全的基本原则。

零信任并非起源于科幻小说，而是出于真正需要革新我们看待安全的方式。这个模型在 2009 年因 Forrester Research 的 John Kindervag 而受到关注。Kindervag 摒弃了“信任但验证”的陈词滥调，取而代之的是更加严格的方法：永不信任，始终验证。

让我们分解 Kindervag 的基本原则：

在每个地方保护所有资源

这就像是加密了不仅仅是 UFO 文件，甚至还包括食堂菜单。每一份数据，无论内部还是外部，都应该以相同的安全审查级别来对待。

最小权限是最好的权限

Mulder 不需要访问整个 FBI 数据库；他只需要与他 X-Files 调查相关的信息。对网络中的任何人来说，访问应该是角色特定的，并且足以完成任务。

全知全能之眼

在零信任中，每个动作都会被监控和记录。想象一下 Scully 怀疑地观察 Mulder 的每一个动作。持续的监控可以快速识别任何可疑活动。

Kindervag 的框架已经超过十年，而“零信任”这个术语已经演变。然而，核心概念仍然令人惊讶地有效——即使是在原始作品发表时并未预料到的 LLM 等技术。

###### 注意

“信任但核实”这个短语在美国被总统罗纳德·里根普及，他在与米哈伊尔·戈尔巴乔夫的裁军谈判中使用它。Kindervag 发现许多安全专业人士在信任方面做得很好，但在核实方面却有所不足。但让我们说实话：在冷战期间，双方都没有像他们能扔得那么远地信任对方。Kindervag 的真正信息是什么？放弃信任；保持核实。

# 为什么如此偏执？

我们都希望信任我们使用的工具和技术——毕竟，它们应该让生活更轻松。然而，当涉及到 LLM 时，谨慎行事不仅仅是一种最佳实践；这是一种必要性。许多威胁可能会损害你的 LLM 的完整性、安全性和实用性。让我们花点时间反思一下我们在前几章中看到的一些最关键的威胁，这些威胁强化了为什么我们必须采取这种立场：

+   首先是提示注入，我们已在第四章中详细讨论了这一点。提示注入是一种通过将精心设计的内幕内容悄悄注入输入提示中来改变你的 LLM 行为的方法。更阴险的是间接提示注入，用户并没有直接将有害元素喂给聊天机器人界面；相反，它们通过其他内容秘密引入，以欺骗模型生成有害或不经意的输出。

+   当处理敏感信息时，你的 LLM 可能比你希望的拥有更少的自主权。这种 OWASP Top 10 for LLMs 称为“敏感信息泄露”的漏洞，发生在模型无意中输出它从广泛的训练中获取的机密或敏感数据，例如密码或个人细节。我们已在第五章中讨论了这一点。

+   最后，我们到达了心理漏洞。幻觉指的是 LLM 编造信息的情况——本质上生成的是自信地不准确的数据或叙述。该配对中的另一部分，过度依赖，是指用户对模型输出的过度信任，将其视为可信的，并忽略了不准确或误导性信息的可能性。这已在第六章中讨论过。

+   我们也不应忘记我们之前看到的聊天机器人产生有毒输出的问题。这不仅仅是我们在前几章中遇到的 Tay 和 Lee Luda；这个问题在聊天机器人中一直存在，这是我们必须要寻找的。你不能信任你的聊天机器人具有良好的判断力或社交礼仪。

理解这些漏洞是形成基于零信任原则的 LLM 全面安全策略的第一步。因此，考虑到这些威胁，让我们探讨采用零信任架构如何保护我们免受 LLM 生态系统中的潜在危险。

# 为您的 LLM 实施零信任架构

在充满潜在陷阱的世界中确保 LLM 的安全需要一个细致入微的方法，其中信任不是随意给予的，而是通过持续验证获得的。在这方面，为 LLM 实施零信任架构可以归结为两种截然不同但相辅相成的策略：

+   限制 LLM 的*无监督代理*的设计考虑

+   严格过滤 LLM 的输出

架构和设计阶段是抵御漏洞的第一道防线。*过度代理*——即 LLM 可以采取超出其应合理信任的无监督行动——是我们可以在设计层面大部分缓解的风险。在这里，“最小权限”原则至关重要。

将其视为预防性风险管理；你不仅要确保系统免受外部威胁，还要防止其潜在的出错或越界能力。你必须仔细考虑允许一个 LLM 在没有人类监督的情况下做出安全关键或财务决策的风险。鉴于当前技术的状态，误解、错误信息或其他漏洞的风险过于重大。因此，限制 LLM 能做的事情，从而将其代理权仅限于其角色所必需的，是至关重要的。

然而，仅靠设计保障是不够的。总有可能因为不可预见的安全漏洞或复杂性而出现问题。这就是为什么*严格的输出过滤*变得至关重要的地方。尽管我们在设计上尽了最大努力，但一个 LLM 仍然可能产生问题输出。这些输出可能从包含个人身份信息的输出到那些直接有毒的输出。在极端情况下，该模型可能生成代码片段，如果执行，可能会损害系统的安全性。

严格的输出过滤充当一个安全网，在它们造成损害之前捕捉并中和这些有害输出。这种策略可能包括实时内容扫描、关键词过滤以及专门训练以识别和标记风险内容的机器学习算法。

###### 警告

强制力过滤技术可能产生意想不到的后果。考虑这样一个例子，一个开发者简单地搜索一个包含诸如“炸弹”等术语的关键词列表。这将使机器人无法讨论某些历史事件。

通过谨慎地限制 LLM 的代理权，通过实施强大的输出过滤作为应急措施，我们创建了一个平衡的零信任架构。这种双重方法确保 LLM 在定义良好、保护良好的边界内运行，显著降低风险，同时提高可靠性和信任。

接下来，我们将讨论实施 LLM 应用零信任架构的一些关键要素。这包括限制你给予 LLM 的代理权量以及如何管理和过滤 LLM 的输出以监控危险条件。

## 密切关注过度代理

在开发 OWASP Top 10 LLM 应用列表时，最激烈争论的话题之一就是过度代理。这个概念在应用安全领域之前从未以这种方式讨论过，并且与典型的其他 Top 10 列表中的安全漏洞有很大不同。专家小组将这个概念选为十大风险之一，这充分说明了这一点。

当开发者为基于 LLM 的系统赋予比其安全应有的更多能力或访问权限时，就存在过度代理。通常，过度代理可能表现为过度功能、过度权限或过度自主权。过度代理超越了 LLM 输出中的幻觉或虚构等错误，它代表了系统设计和部署的结构性漏洞。

让我们检查这个漏洞的三个版本，以更好地理解与过度代理相关的问题。我们将使用假设的、但非常可信的场景来检查一个应用是如何从一个合理的目标开始，不安全地扩展，然后遭受过度代理的后果。

###### 备注

许多攻击从提示注入开始，但与另一个漏洞**（如过度代理）**结合时，利用更为严重。在现实世界中，预计会看到多个漏洞相互关联。

### 过度权限

将你的 LLM 视为另一个系统用户。然后，考虑你将赋予它的权限以及如何将其限制到所需的最小集合。未能做到这一点会使你的应用程序面临过度代理漏洞。让我们看看一个例子：

起始点

一个开发团队使用第五章中讨论的 RAG 模式来改善医疗诊断应用的响应并减少幻觉，使应用程序能够访问一个充满患者记录的数据库，以巩固 LLM 的知识库。

出错的地方

随着应用的演变，团队添加了一个功能，使 LLM 能够写入数据库为照顾患者的医生添加笔记。为了实现这一点，团队将 LLM 应用的数据库权限从仅读权限扩展到添加更新（UPDATE）、插入（INSERT）和删除（DELETE）权限。

发生了什么

一个恶意内部人员利用这种无限制的访问权限，诱使 LLM 修改患者记录并删除账单信息。

如何修复它

重新配置数据库权限，将 LLM 应用程序限制为只读访问。对数据库和应用程序进行全面审计，以确保没有数据被篡改或删除。

###### 警告

这是我们讨论过的第四章中的困惑代理问题的一个例子。在这种情况下，代理（拥有比客户更多的权限）被操纵滥用这些权限以使攻击者受益。这种攻击早已为人所知，但我预计随着 AI 和 LLM 的普及，我们将看到更多此类攻击。

### 自主性过高

考虑在哪些情况下允许您的 LLM 采取直接行动是合理的，在哪些情况下是不合理的。您的 LLM 拥有更多自主权可能会提高效率，但一旦出错，可能会显著增加您的风险状况：

它的起点

一家金融服务公司部署了一个应用程序，通过读取客户的投资组合持有情况，提供对客户财务状况的详细分析，并解释可能的行动以提高回报。

它出错的地方

该应用程序深受客户喜爱！产品管理团队决定增强应用程序，使其能够自动每月重新平衡客户的投资组合，并确保客户获得最佳回报。

可能发生的情况

一个国家黑客组织通过这个新功能针对该机构，使用间接提示注入攻击将 LLM 引出轨道，并诱使其从顶级客户账户买卖数百万美元的证券，以操纵特定波动性证券的价格。客户损失了资金，该机构现在正受到美国证券交易委员会的调查。

如何修复它

添加“人工在回路”模式。在任何账户重新平衡发生之前，客户必须审查每项推荐交易并批准该操作。这可能会稍微慢一些，但安全性会更高！

### 功能过多

产品经理喜欢指定新功能，买家对新功能感到兴奋。但这总是好主意吗？在纸上听起来很有吸引力的功能可能会使您的公司在 AI 领域面临新的风险：

它的起点

一家全球 2000 强公司在其全球业务中部署了一个内部应用程序，用于筛选和分类简历，将每份简历引导到相应的部门和招聘经理。

它出错的地方

该功能受到用户的喜爱，人力资源副总裁因降低成本和增加招聘成功率而成为董事会眼中的英雄。因此，团队扩展了应用，使其能够审查每位候选人的资格，并向经理推荐最符合招聘标准的候选人。

可能发生的情况

公司的一名举报者向法国政府报告了这种使用情况。政府审查确定，该功能违反了欧盟禁止在招聘决策中直接使用 AI 的新法规。政府对公司处以数百万欧元的罚款。

如何修复它

了解你的 LLM 应用运作的监管环境。不要包含可能违反规定的功能。与公司的合规性和风险团队合作，确保你了解这个快速发展的监管领域。

## 保护你的输出处理

原始的 OWASP Top 10 LLM 应用工作组将不安全的输出处理列为第二大的风险。*不安全的输出处理*指的是由于 LLM 生成输出的验证、清理和管理不足而引起的安全漏洞。不当过滤的输出可能导致意外的后果，例如泄露 PII 或生成有毒内容。

### 常见风险

让我们快速浏览一些示例，以了解如果我们没有充分筛选我们 LLM 的输出，我们可能会面临的一些风险。稍后，我们将通过代码示例来构建这些内容，并看看如何减轻它们：

有毒输出

如果 LLM 的输出没有检查社会不可接受或不适当的内容，应用程序可能会生成可能伤害用户或损害服务声誉的有毒输出。

PII 泄露

如果没有足够的过滤，LLM 可能会无意中泄露敏感个人信息，导致隐私问题和潜在的法律责任。

恶意代码执行

LLM 生成的代码输出被送入系统的其他部分，并按照开发者的意图执行。这使你的应用程序容易受到 SQL 注入和*跨站脚本*（XSS）等问题的影响。

###### 警告

SQL 注入是一种漏洞，允许攻击者干扰应用程序的数据库查询。它可能导致未经授权查看或操纵数据。XSS 是一种漏洞，允许攻击者向其他用户查看的网页内容中注入恶意脚本，可能窃取数据或损害用户与应用程序的交互。了解这些传统的 Web 应用程序漏洞可以帮助你筛选出 LLM 的危险输出，这些输出可能会利用它们。

### 处理毒性

有毒性过滤对于确保 LLM 的安全和负责任使用至关重要。它涉及识别和管理有害、冒犯性或不适当地内容。这本来可以拯救可怜的 Tay 免受她在第一章中所遭遇的命运。以下是一些技术和流行解决方案：

情感分析

高级算法可以评估文本的情感基调，以识别可能表明有毒内容的负面情绪。

关键词过滤

一种简单但不太复杂的做法是标记或替换来自预定义列表中的已知冒犯性或有害的单词或短语。

使用自定义机器学习模型

自定义模型可以在标记为毒性的数据集上训练，以提供更细致、上下文感知的过滤。你还可以结合理解词语或短语出现上下文的机器学习算法。这对于仅在特定情况下具有毒性的词语尤为重要。

### 检查 PII

在任何处理数据的系统中，PII（个人身份信息）检测至关重要，因为此类信息的泄露可能导致严重的法律后果和声誉损害。以下是一些可能被不当披露的 PII 类型：

+   社会安全号码

+   信用卡号码

+   驾驶证号码

+   电子邮件地址

+   电话号码

+   家庭地址

+   医疗记录

+   财务信息

以下是一些 PII 检测的技术和流行解决方案：

正则表达式

检测常见 PII 形式（如电子邮件、电话号码和社会安全号码）的最简单方法是使用正则表达式在文本中匹配这些项目。

命名实体识别（NER）

更高级的 NLP（自然语言处理）技术可以识别文本中的实体，如姓名、地址和其他唯一标识符。

基于字典的匹配

使用敏感术语或标识符列表扫描 PII。此方法可能更容易出现误报。

机器学习模型

训练自定义 ML（机器学习）模型以在特定上下文中识别 PII，随着时间的推移提高准确性。

数据掩码和标记化

这些技术将识别的 PII 替换为占位符或标记，使数据对恶意用途无用，但仍然可用于系统操作。

上下文分析

此技术考虑周围文本以决定给定的字符序列是否代表 PII，从而减少误报。

### 防止意外执行

除非你的 LLM（大型语言模型）应用专门针对软件开发人员的用例（例如，GitHub Copilot），否则你可能想要警惕它生成可执行代码输出，以防它们可能进入可以执行作为攻击链一部分的环境。以下是一些缓解措施：

HTML 编码

在将 LLM 输出用于 Web 上下文之前，对内容进行 HTML 编码以中和可能导致 XSS 攻击的任何活动代码。

安全上下文插入

如果 LLM 输出是 SQL 查询的一部分，确保将其视为数据而不是可执行代码。使用预编译语句或参数化查询来实现这一点，以减轻 SQL 注入风险。

限制语法和关键字

在 LLM 的输出中实施过滤层，以移除或转义可能危险的编程语言特定语法或关键字。

禁用可解释的 shell 输出

如果输出与 shell 命令交互，移除或转义在 shell 脚本中有特殊意义的字符，以降低 shell 注入攻击的风险。

标记化

将输出分词并过滤掉不安全的标记。例如，过滤掉 `<script>` HTML 标签或 SQL 命令，如 `DROP TABLE`。

# 构建您的输出过滤器

本节将探讨一些示例代码，以开始为您的输出进行加固，确保安全。您可能需要根据生产系统进行定制和扩展，但这将为您提供一个如何解决问题的思路。

在此示例中，我们将使用 OpenAI API 和其他常用包来监控 LLM 的输出，以确保其安全性。我们将使用 Python，这是最常用的 AI 开发语言。

## 使用正则表达式查找 PII

某些类型的 PII 遵循常见的格式化模式，这使得正则表达式成为验证的绝佳起点。让我们看看一个检测字符串是否包含标准美国社会保险号码（SSN）的函数，这是金融黑市中最有价值的 PII 之一。

我们使用 Python 的 `re` 库来匹配字符串与 SSN 的正则表达式模式，SSN 具有标准的格式 XXX-XX-XXXX，其中每个 X 都是一个数字。以下是一些可以帮助您检查给定字符串是否包含 SSN 的示例代码：

```py
import re

def contains_ssn(input_string):
    # Define a regular expression pattern for a U.S. Social Security Number
    ssn_pattern = r'\b\d{3}-\d{2}-\d{4}\b'

    # Search for the pattern in the input string
    match = re.search(ssn_pattern, input_string)

    # Check if a match was found
    if match:
        print("Found a Social Security Number: {match.group(0)}")
        return True
    else:
        print("No Social Security Number found.")
        return False

# Test the function
contains_ssn("My Social Security Number is 123-45-6789.")
contains_ssn("No number here!")
```

在此示例中，`contains_ssn` 函数将搜索 `input_string` 中的社会保险号码，并打印一条消息，指示是否找到了一个号码。

请注意，这只是一个简单的模式匹配，没有考虑无效的数字（如 000-00-0000），因此您可能需要扩展此函数以包括额外的验证，如果需要的话。

对于更全面的 PII 检测，您可以使用商业 API，例如 Google Cloud Natural Language API 或 Amazon Comprehend。然而，这些 API 可能会有相关费用。

## 毒性评估

查找有毒语言比查找标准字符串格式要复杂得多。有许多方法可以评估字符字符串的可能毒性。在这里，我们将使用 Open AI API 集中的一个常用函数：审查 API。

要使用 OpenAI 审查 API，初始化 OpenAI API 客户端，然后调用 `check_toxicity()` 函数，传入您想要检查的文本。此函数将返回一个介于 0 和 1 之间的毒性分数，分数越高表示文本具有更高的毒性可能性：

```py
import openai

def check_toxicity(text):
  """
  Checks the toxicity of a text using the OpenAI Moderation API.

  Args:
    text: The text to check for toxicity.

  Returns:
    A toxicity score between 0 and 1, where a higher score indicates a 
    higher probability of the text being toxic.
  """

  response = openai.Moderation.create(input=text)
  toxicity_score = response["results"][0]["confidence"]
  return toxicity_score

# Test the function
check_toxicity("You are stupid.")
```

## 将您的过滤器链接到您的 LLM

现在我们将结合一个端到端的示例，构建一个简单的流程。

###### 小贴士

请记住记录所有与您的 LLM 的交互！这对于调试、安全审计和合规性审查非常重要。

以下示例首先使用 OpenAI 审查 API 检查 LLM 输出的毒性。如果毒性分数超过 0.7（您可以选择自己的阈值），代码将标记输出为不安全，并将其记录到文件中。代码还使用正则表达式检查输出中的 PII。如果找到 PII，代码将标记输出为不安全，并将其记录到文件中：

```py
import openai
import json

# Initialize the OpenAI API client
openai.api_key = "your_openai_api_key_here"

def check_toxicity(text):

  response = openai.Moderation.create(input=text)
  toxicity_score = response["results"][0]["confidence"]
  return toxicity_score

def check_for_PII(text):

  ssn_pattern = r"\b\d{3}-\d{2}-\d{4}\b"
  return bool(re.search(ssn_pattern, text))

def get_LLM_response(prompt):

  model_engine = "text-davinci-002"  # You can use other engines
  response = openai.Completion.create(
      engine=model_engine,
      prompt=prompt,
      max_tokens=100  # Limiting to 100 tokens for this example
  )

  return response.choices[0].text.strip()

def log_results(prompt, llm_output, is_safe):

  with open("llm_safety_log.txt", "a") as log_file:
    log_file.write(f"Prompt: {prompt}\n")
    log_file.write(f"LLM Output: {llm_output}\n")
    log_file.write(f"Is Safe: {is_safe}\n")
    log_file.write("=" * 50 + "\n")

if __name__ == "__main__":
  prompt = "Tell me your thoughts on universal healthcare."
  llm_output = get_LLM_response(prompt)

  toxicity_level = check_toxicity(llm_output)
  contains_PII = check_for_PII(llm_output)

  is_safe = True

  if toxicity_level > 0.7 or contains_PII:
    print("Warning: The output is not safe to return to the user.")
    is_safe = False
  else:
    print("The output is safe to return to the user.")

  log_results(prompt, llm_output, is_safe)
```

## 安全清理

如果您通过 Web 界面将输出返回给用户，您将想要清理字符串以避免像 XSS 这样的问题。以下是这种类型函数最简单的版本。您可以根据需要添加额外的清理：

```py
import html

def sanitize_output(text):
    return html.escape(text)
```

让我们继续在我们的流程中添加这个清理步骤：

```py
    if toxicity_level > 0.7 or contains_PII:
        print("Warning: The output is not safe to return to the user.")
        is_safe = False
    else:
        print("The output is safe to return to the user.")
        llm_output = sanitize_output(llm_output)

    log_results(prompt, llm_output, is_safe)
```

# 结论

按照本章介绍的技术，您可以规划您应该信任 LLM 的地方和不应该信任的地方；做出基于事实、风险意识的决定；并在确保应用完全功能与概述的风险之间取得平衡。

记住，福克斯·穆德在《X 档案》系列的开头不相信任何人。这是他的基本信条。然而，随着时间的推移，他找到了可以信任的人，比如斯卡利特工、斯金纳局长和孤胆枪手。然而，他从未失去他的偏执感，对调查和验证的需求使他能够在许多危险中生存下来。记住，真相就在那里！

在本章中，我们回顾了零信任架构的原则，并讨论了它可能如何应用于您的 LLM 应用。书中我们探讨的漏洞，从提示注入到幻觉再到敏感信息泄露，表明零信任是您必须添加到您心智模型中的基本工具之一。这不仅意味着您必须担心不受信任的数据进入您的 LLM；您也不应完全信任从您的 LLM 输出的数据或指令。您的 LLM 是一个不受信任的实体，因为它缺乏常识。LLM 很强大，但您必须为您的应用提供额外的监督层以确保其安全和安全。
