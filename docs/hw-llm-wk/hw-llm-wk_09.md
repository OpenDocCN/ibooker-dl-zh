# 第十章：参考文献

## 第一章

1.  Young, B. (2023). AI 专家对 GPT-4 架构进行推测。Weights & Biases。[`api.wandb.ai/links/byyoung3/8zxbl12q`](https://api.wandb.ai/links/byyoung3/8zxbl12q)

1.  Micikevicius, P. (2017)。深度神经网络混合精度训练。NVIDIA 开发者。[`mng.bz/6eaA`](https://mng.bz/6eaA)

1.  利用 Google Cloud TPUs 加速 AI 开发。[`cloud.google.com/`](https://cloud.google.com/tpu)[tpu](https://cloud.google.com/tpu)

1.  Metz, C. (2023 年 7 月 23 日)。研究人员质疑 ChatGPT 和其他聊天机器人的安全控制。*纽约时报*。

1.  Hu, K. (2023 年 2 月 2 日)。ChatGPT 创下用户增长最快的记录——分析师评论。路透社。[`mng.bz/XxKv`](https://mng.bz/XxKv)

## 第二章

1.  Friederici, A. D. (2011)。语言处理的大脑基础：从结构到功能。*生理学评论，91*，1357-1392。[`doi.org/10.1152/physrev`](https://doi.org/10.1152/physrev.00006.2011)[.00006.2011](https://doi.org/10.1152/physrev.00006.2011)

1.  Nation, P. 和 Waring, R. (1997)。词汇量、文本覆盖率和词汇表。在：N. Schmitt 和 M. McCarthy 编著，《词汇：描述、获取和教学法》（第 6-19 页）。剑桥大学出版社。

1.  Brown, T. B.，Mann, B.，Ryder, N. 等. (2020)。语言模型是少样本学习者。[`arxiv.org/abs/2005.14165`](https://arxiv.org/abs/2005.14165)

1.  Google/SentencePiece。[`github.com/google/sentencepiece`](https://github.com/google/sentencepiece)

1.  Petrov, A.，La Malfa, E.，Torr, P. H. S. 和 Bibi, A. (2023)。语言模型分词器在语言间引入不公平。[`arxiv.org/abs/2305.15425`](https://arxiv.org/abs/2305.15425)

## 第三章

1.  Denk, T. (2019)。Transformer 的位置编码中的线性关系。[`mng.bz/oKxd`](https://mng.bz/oKxd)

1.  Raff, E. (2022)。*深度学习内部*。Manning。

## 第四章

1.  Yong, E. (2012)。模拟大脑获得最高测试分数。*自然*。[`www.nature`](https://www.nature.com/articles/nature.2012.11914)[.com/articles/nature.2012.11914](https://www.nature.com/articles/nature.2012.11914)

1.  Forsyth, J. A. 和 Mongrut, S. (2022)。竞争优势的持续时间是否推动股市的长期回报？*Revista Contabilidade & Finanças, 33*(89)，329–342。[`doi.org/10.1590/1808-057x202113660`](https://doi.org/10.1590/1808-057x202113660)

1.  Lin, S.，Hilton, J. 和 Evans, O. (2022)。TruthfulQA：衡量模型模仿人类错误的方式。[`arxiv.org/abs/2109.07958`](https://arxiv.org/abs/2109.07958)

1.  Parrish, A.，Chen, A.，Nangia, N. 等. (2022) BBQ：问答中的手建偏差基准。[`arxiv.org/abs/2110.08193`](https://arxiv.org/abs/2110.08193))

1.  Chen, M.，Tworek, J.，Jun, H. 等. (2021)。评估在代码上训练的大型语言模型。[`arxiv.org/abs/2107.03374`](https://arxiv.org/abs/2107.03374)

1.  我们如何画一只鸭子（为了创建 tikzducks 包并将其存储在 CTAN 中）？[`mng.bz/W2Jg`](https://mng.bz/W2Jg)

1.  Sutton, R. (2019)。苦涩的教训。[`mng.bz/EaJq`](https://mng.bz/EaJq)

## 第五章

1.  Barber, R. G., Oza, A., Carlson, R. 和 Ramirez, R. (2023 年 10 月 18 日)。为什么科学家们为了研究而重新激活蜘蛛尸体。NPR。[`mng.bz/lYgj`](https://mng.bz/lYgj)

1.  OpenAI。微调。[`mng.bz/dXnD`](https://mng.bz/dXnD)

1.  Hugging Face。微调预训练模型。[`huggingface.co/docs/`](https://huggingface.co/docs/transformers/training)[transformers/training](https://huggingface.co/docs/transformers/training)

1.  Luo, Y, Yang, Z., Meng, F. 等人 (2025)。在持续微调期间大型语言模型中灾难性遗忘的实证研究。[`arxiv.org/abs/`](https://arxiv.org/abs/2308.08747)[2308.08747](https://arxiv.org/abs/2308.08747)

1.  McCloskey, M. 和 Cohen, N. J. (1989)。连接主义网络中的灾难性干扰。《学习与动机心理学》，第 24 卷，109-165。[`doi.org/`](https://doi.org/10.1016/S0079-7421(08)60536-8)[10.1016/S0079-7421(08)60536-8](https://doi.org/10.1016/S0079-7421(08)60536-8)

1.  Belrose, N., Schneider-Joseph, D., Ravfogel, S. 等人 (2023). LEACE：闭形式下的完美线性概念擦除。[`arxiv.org/abs/2306.03819`](https://arxiv.org/abs/2306.03819)

1.  Phung, D. V., Thakur, A., Castricato, L., Tow, J. 和 Havrilla, A. (2025). 实施 RLHF：使用 trlX 学习总结。度量与衡。[`mng.bz/rKzg`](https://mng.bz/rKzg)

1.  Kolter, Z. 和 Madry, M. (未注明日期)。对抗鲁棒性：理论与实践。[`adversarial-ml-tutorial.org/`](https://adversarial-ml-tutorial.org/)

1.  OpenAI。 (2023 年 3 月 27 日)。GPT-4 技术报告。[`cdn.openai.com/`](https://cdn.openai.com/papers/gpt-4.pdf)[papers/gpt-4.pdf](https://cdn.openai.com/papers/gpt-4.pdf)

1.  Chowdhery, A., Narang, S., Devlin, J. 等人 (2022)。PaLM：通过路径扩展语言建模。[`arxiv.org/abs/2204.02311`](https://arxiv.org/abs/2204.02311)

1.  Liang, W., Izzo, Z., Zhang, Y., 等人 (2024). 在大规模上监控 AI 修改内容：ChatGPT 对 AI 会议同行评审影响的案例研究。[`arxiv.org/abs/2403.07183`](https://arxiv.org/abs/2403.07183)

1.  Li, C. 和 Flanigan, J. (2023)。任务污染：语言模型可能不再是少样本了。[`arxiv.org/abs/2312.16337`](https://arxiv.org/abs/2312.16337)

1.  Near, J. P. 和 Abuah, C. (2021)。*编程差分隐私*。[`prog`](https://programming-dp.com/)[ramming-dp.com/](https://programming-dp.com/)

## 第六章

1.  Albergotti, R. 和 Matsakis, L. (2023 年 1 月 23 日)。OpenAI 雇佣了一支承包军，使基础编码变得过时。Semafor。[`mng.bz/MDGQ`](https://mng.bz/MDGQ)

1.  介绍 Code Llama，一种用于编码的顶尖大型语言模型。(2023 年 8 月 24 日)。Meta。[`mng.bz/av2j`](https://mng.bz/av2j)

1.  von Werra, L., and Ben Allal, L. (2023, May 4). StarCoder: A state-of-the-art LLM for code. Hugging Face. [`huggingface.co/blog/starcoder`](https://huggingface.co/blog/starcoder)

1.  Biderman, S., and Raff, E. (2022). Fooling MOSS detection with pretrained language models. [`arxiv.org/abs/2201.07406`](https://arxiv.org/abs/2201.07406).

1.  Dyer, E., and Gur-Ari, G. (2020, June 30). Minerva: Solving quantitative reasoning problems with language models. Google Research. [`mng.bz/gane`](https://mng.bz/gane).

1.  Azerbayev, Z., Schoelkopf, H., Paster, K., et al. (2023, October 16). Llemma: An open language model for mathematics. EleutherAI. [`blog.eleuther.ai/`](https://blog.eleuther.ai/llemma/)[llemma/](https://blog.eleuther.ai/llemma/)

1.  Richardson, D. (1968). Some undecidable problems involving elementary functions of a real variable. *Journal of Symbolic Logic, 33,* 514–520.

1.  Nogueira, R., Jiang, Z., and Lin, J. (2021). Investigating the limitations of transformers with simple arithmetic tasks. [`arxiv.org/abs/2102.13019v3`](https://arxiv.org/abs/2102.13019v3)

1.  Golkar, S., Pettee, M., Eickenberg, M., et al. (2024). Investigating the limitations of transformers with simple arithmetic tasks. [`arxiv.org/abs/2310.02989`](https://arxiv.org/abs/2310.02989)

## 第七章

1.  Romeo, R. R., Leonard, J. A., Robinson, S. T., et al. (2018). Beyond the 30-million-word gap: Children’s conversational exposure is associated with language-related brain function. *Psychological Science, 29,* 700–710\. [`doi.org/10.1177/`](https://doi.org/10.1177/0956797617742725)[0956797617742725](https://doi.org/10.1177/0956797617742725)

1.  Gilkerson, J., Richards, J. A., Warren, S. F., et al. (2017). Mapping the early language environment using all-day recordings and automated analysis. *American Journal of Speech-Language Pathology, 26,* 248-265\. [`doi.org/10.1044/2016_AJSLP-15-0169`](https://doi.org/10.1044/2016_AJSLP-15-0169)

1.  Shumailov, I., Shumaylov, Z., Zhao, Y., et al. (2024). The curse of recursion: Training on generated data makes models forget. [`arxiv.org/abs/2305.17493`](https://arxiv.org/abs/2305.17493)

1.  Stanovich K. E. (2009). *What Intelligence Tests Miss: The Psychology of Rational Thought*. Yale University Press.

1.  改进合成图像的真实性。 (2017, July 7). Apple Machine Learning Research. [`machinelearning.apple.com/research/gan`](https://machinelearning.apple.com/research/gan)

1.  Dai, D., Sun, Y., Dong, L., et al. (2023). Why can GPT learn in-context? Language models secretly perform gradient descent as meta-optimizers. In *Findings of the Association for Computational Linguistics: ACL 2023* (pp. 4005–4019). Association for Computational Linguistics.

1.  Hiller, J. (2023, December 12). Microsoft targets nuclear to power AI operations. *Wall Street Journal*. [`mng.bz/pKe5`](https://mng.bz/pKe5)

1.  Disavino, S. (2023 年 9 月 8 日). 德克萨斯州电力价格飙升，电网在热浪中通过可靠性测试。路透社。[`mng.bz/OB0K`](https://mng.bz/OB0K)

1.  最近添加的 emoji，v15.1。 (未注明日期)。Unicode。[`www.unicode.org/emoji/charts-15.1/emoji-released.html`](https://www.unicode.org/emoji/charts-15.1/emoji-released.html)

1.  Wei, J., Wang, X., Schuurmans, D. 等. (2023 年)。思维链提示引发大型语言模型的推理。[`arxiv.org/abs/2201.11903`](https://arxiv.org/abs/2201.11903)

1.  Wang, L., Xu, W., Lan, Y. 等. (2023 年)。计划-解决提示：通过大型语言模型提高零样本思维链推理。[第 61 届计算语言学年会*（第 1 卷：长篇论文，第 2609-2634 页）。计算语言学协会。

1.  Guan, L., Valmeekam, K., Sreedharan, S. 和 Kambhampati, S. (2023 年)。利用预训练的大型语言模型构建和利用世界模型以进行基于模型的任务规划。[`arxiv.org/abs/2305.14909`](https://arxiv.org/abs/2305.14909)

1.  Bhargava, A. Y. (2015 年). *Grokking Algorithms: An illustrated Guide for Programmers and Other Curious People*。Manning Publications。

1.  Merrill, W. 和 Sabharwal, S. (2024 年)。具有思维链的 Transformer 的表达能力。在*2024 年学习表示国际会议*。[`openreview.net/forum?id=NjNGlPh8Wh`](https://openreview.net/forum?id=NjNGlPh8Wh)

1.  Carlini, N. (2023 年 9 月 22 日). 与大型语言模型下棋。[`nicholas.carlini.com/writing/2023/chess-llm.html`](https://nicholas.carlini.com/writing/2023/chess-llm.html)

1.  Edwards B. (2022 年 11 月 7 日). 新的围棋技巧击败了世界级围棋 AI——但输给了业余爱好者。*Ars Technica*。[`mng.bz/dW6O`](https://mng.bz/dW6O)

## 第八章

1.  Yagoda, M. (2024 年 2 月 23 日). 航空公司因其聊天机器人给出糟糕的建议而承担责任——这对旅客意味着什么。BBC。[`mng.bz/xK7W`](https://mng.bz/xK7W)

1.  Notopoulos, K. (2023 年 12 月 18 日)。一家汽车经销商在其网站上添加了 AI 聊天机器人：然后一切乱套了。[`mng.bz/AQPz`](https://mng.bz/AQPz)

1.  Suresh, H., Lao, N., 和 Liccardi, I. (2020 年). 误置的信任：衡量机器学习对人类决策的干扰。在*第 12 届 ACM 网络科学会议（WebSci '20）*（第 315-324 页）。计算机制造协会。[`doi.org/10.1145/3394231.3397922`](https://doi.org/10.1145/3394231.3397922)

## 第九章

1.  Hofmann, V., Kalluri, P. R., Jurafsky, D., 和 King, S. (2024 年). 方言偏见预测 AI 对人们性格、就业能力和犯罪性的判断。[`arxiv.org/abs/2403.00742`](https://arxiv.org/abs/2403.00742)

1.  Omiye, J. A., Lester, J. C., Spichak, S. 等人 (2023). 大型语言模型传播基于种族的医学。npj 数字医学，6，195。[`doi.org/10.1038/`](https://doi.org/10.1038/s41746-023-00939-z)[s41746-023-00939-z](https://doi.org/10.1038/s41746-023-00939-z)

1.  农业劳动力。 (2025 年 1 月 8 日)。经济研究服务。[`www.ers.usda`](https://www.ers.usda.gov/topics/farm-economy/farm-labor/)[.gov/topics/farm-economy/farm-labor/](https://www.ers.usda.gov/topics/farm-economy/farm-labor/)

1.  Verma, P. 和 De Vync, G. (2023 年 6 月 2 日)。ChatGPT 夺走了他们的工作：现在他们遛狗和修理空调。*华盛顿邮报*。[`mng.bz/EwQd`](https://mng.bz/RVYK)

1.  Marr, B. (2024 年 4 月 18 日)。生成式 AI 在视频游戏开发中的作用。*Forbes*。[`mng.bz/Pdpn`](https://mng.bz/Pdpn)

1.  Lev-Ram, M. (2023 年 1 月 26 日)。科技巨头裁员受害者发现其他公司争相雇佣他们。*Forbes*。[`mng.bz/JYXV`](https://mng.bz/JYXV)

1.  Lohr, S. (2024 年 2 月 1 日)。报告称，生成式 AI 的最大影响将在银行和科技领域。*纽约时报*。[`mng.bz/wJ7P`](https://mng.bz/wJ7P)

1.  Pethokoukis, J. (2016 年 6 月 16 日)。ATM 和银行柜员的故事揭示了“机器人崛起”和就业的情况。美国企业研究所。[`mng.bz/qx7r`](https://mng.bz/qx7r)

1.  Hunter, L. W., Bernhardt, A., Hughes, K. L. 和 Skuratowicz, E. (2001)。不仅仅是 ATM：零售银行业的科技、企业策略、就业和收入。*ILR 评论，54*(2A)，402-424。[`doi.org/10.1177/001979390105400222`](https://doi.org/10.1177/001979390105400222)

1.  Rosalsky, G. (2024 年 6 月 18 日)。如果 AI 如此出色，为什么翻译工作仍然如此之多？NPR。[`mng.bz/7pBv`](https://mng.bz/7pBv)

1.  Marr, B. (2024 年 5 月 28 日)。生成式 AI 将如何改变艺术家和设计师的工作。*Forbes*。[`mng.bz/mG7a`](https://mng.bz/mG7a)

1.  Autor, D.，Chin, C.，Salomons, A. 和 Seegmiller, B. (2024)。新前沿：1940-2018 年新工作的起源和内容。*经济学季刊，139*，1399-1465。[`doi.org/10.1093/qje/qjae008`](https://doi.org/10.1093/qje/qjae008)

1.  Dave, P. (2023 年 4 月 8 日)。StackOverflow 将对 AI 巨头收取训练数据费用。*Wired*。[`mng.bz/5gDO`](https://mng.bz/5gDO)

1.  Grimm, D. (2024 年 5 月 8 日)。Stack Overflow 因用户反抗 OpenAI 合作而大规模封禁用户——封禁用户删除答案以防止其被用于训练 ChatGPT。Tom’s Hardware。[`mng.bz/nR75`](https://mng.bz/nR75)

1.  Bishop, T. (2020 年 10 月 20 日)。Expedia 集团首席执行官谈谷歌反垄断案：“非常高兴看到政府最终采取行动。”Geek Wire。[`mng.bz/vK7p`](https://mng.bz/vK7p)

1.  Siddiqui, T. (2023 年 6 月 29 日)。随着技术的发展，必须考虑人工智能的风险：杰弗里·辛顿。多伦多大学。[`mng.bz/4aNR`](https://mng.bz/4aNR)

1.  Bengio, Y. (2023 年 6 月 24 日)。关于灾难性 AI 风险的常见问题解答。[`mng.bz/QDO6`](https://mng.bz/QDO6)

1.  介绍 Llama 3.1：我们迄今为止最强大的模型。(2024 年 7 月 23 日)。Meta。[`ai.meta.com/blog/meta-llama-3-1/`](https://ai.meta.com/blog/meta-llama-3-1/)

1.  Min, S., Gururangan, S., Wallace, E., et al. (2023). SILO 语言模型：在非参数数据集中隔离法律风险。[`arxiv.org/abs/2308.04430`](https://arxiv.org/abs/2308.04430)

1.  Rivero, N. (2022 年 9 月 21 日)。低背景金属：纯净、未被污染的宝藏。《Quartz》。[`mng.bz/eyXZ`](https://mng.bz/eyXZ)

1.  Shumailov, I., Shumaylov, Z., Zhao, Y. 等. (2024)。当在递归生成数据上训练时，AI 模型会崩溃。《自然》，631 期，第 755–759 页。[`doi.org/10.1038/s41586-024-07566-y`](https://doi.org/10.1038/s41586-024-07566-y)

1.  Coffey, L. (2024 年 2 月 9 日)。教授们对检测 AI 生成写作的工具持谨慎态度。《高等教育内参》。[`mng.bz/Xxj9`](https://mng.bz/Xxj9)

1.  ChatGPT 推出后，Stack Exchange 的流量是否有所下降？(2023)。Stack Exchange。[`mng.bz/yW7p`](https://mng.bz/yW7p)

1.  Dhamani, N. 和 Engler, M. (2024)。*生成式 AI 导论*。Manning。[`www.manning.com/books/introduction-to-generative-ai`](https://www.manning.com/books/introduction-to-generative-ai)

# 索引
