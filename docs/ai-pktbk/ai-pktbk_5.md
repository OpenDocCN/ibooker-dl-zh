# 第六章：详细说明

### 本章涵盖

+   围绕人工智能的争议性和及时性讨论

+   训练数据相关的版权争议

+   人工智能的经济学

+   对人工智能性能和进步的夸张

+   人工智能监管

+   训练和使用人工智能模型所需的资源消耗，如电力和水

+   围绕人工智能、生物大脑和意识的哲学辩论

本章探讨了人工智能的一些重大问题。它还揭示了一个不太令人愉快的 AI 方面——该领域经常受到夸张、推测甚至欺骗的影响。我认为了解这些话题很重要，这样你可以批判性地分析人工智能的公告和讨论。此外，如果你正在开发基于人工智能的产品或大量使用人工智能，你可能想了解你的工作可能产生的更广泛的影响和潜在争议。

## 版权

大型人工智能模型，如 LLM 和文本到图像模型，是使用从互联网收集的数据或 *抓取* 训练的，通常未经所有者授权。这包括数百万份文档、图片和书籍，这让许多人感到愤怒，并对人工智能提供商提起了许多诉讼。一个例子是 Getty Images（一个销售库存图片的网站）对 Stability AI（创建 Stable Diffusion 文本到图像模型的公司）提起的诉讼。Getty Images 认为 Stability AI 在未经授权的情况下使用从 Getty 网站收集的图片来训练其模型。投诉展示了 Stability AI 模型生成的图片，这些图片与 Getty 销售的图片相似。在某些情况下，AI 模型甚至生成了带有 Getty Images 水印粗糙仿制的图片（见图 6.1）。

![](img/CH06_F01_Maggiori.jpg)

##### 图 6.1  左：在 Getty Images 网站上销售的图片。右：由 Stable Diffusion 模型生成的图片。注意图片中的水印。这些图片是从 Getty Images (US), Inc. v. Stability AI, Inc., 1:23-cv-00135, (D. Del.)*.* 中复制的。

*《纽约时报》* 对 OpenAI 提起了一项类似的诉讼，理由是未经授权抓取报纸文章来训练 OpenAI 的模型。投诉中包含了 GPT-4 输出的大量文本示例，这些文本是 *《纽约时报》* 中找到的文本的逐字复制（见图 6.2）。

![](img/CH06_F02_Maggiori.png)

##### 图 6.2  GPT-4 输出的示例（几乎）逐字文本，如《纽约时报》所发布。图来自 The New York Times Company v. Microsoft Corporation, 1:23-cv-11195, (S.D.N.Y.)。

此外，一群艺术家起诉了 Midjourney、Stability AI 和其他图像生成提供商，因为他们使用了原告作品的图片来训练他们的模型。他们认为这允许模型生成“原告风格”的图片 ([`mng.bz/ZlKa`](https://mng.bz/ZlKa))。未来很可能会对人工智能提供商提出许多其他版权侵权指控。

版权侵权通常基于两个理由：

+   人工智能模型有时会逐字复制内容。

+   人工智能服务提供商未经授权使用受版权保护的数据来训练模型。

当模型记住训练数据，这是过度拟合的结果时，就会发生逐字复制。人工智能服务提供商可能会通过使用防止过度拟合的技术来尽量减少这种情况。很难保证不会发生记忆，但可能成功减轻。

第二点——未经授权使用数据来训练模型——更具争议性，似乎也是问题的核心。人工智能服务提供商的支持者认为这并不构成版权侵犯。他们认为从数据中抓取信息来训练模型是合法的，因为目标是让模型从数据中学习模式和关联，而不是复制数据的逐字逐句（即使不幸地发生了这种情况）。

我甚至听说有些人认为我们人类通过阅读公开可用的数据来学习，然后我们使用这些知识来创造我们自己的作品。那么，为什么人工智能服务提供商不能做同样的事情呢？

解决这个难题的关键在于*合理使用*这个话题。在版权法中，在某些情况下，未经授权复制数据被认为是合理的。这包括复制数据以帮助构建不取代或与原始产品竞争的产品。例如，在这本书中，我引用了其他书籍的引文。我从未联系过这些书籍的作者请求许可。这被认为是合理使用，因为我的引用并没有使这本书与其他书籍竞争，并且引用的原始作者被明确标注。例如，在第一章中，我引用了《统计学习元素》一书的段落。然而，这本书涉及不同的主题，所以它并不打算与之竞争，窃取其客户。事实上，我可能通过提及它实际上为那本书带来了宣传。然而，如果复制了那本书的整个章节，这就不被认为是合理使用，因为我的书可能成为它的替代品。关于什么构成合理使用没有确切的规定，例如引用中精确的单词数，所以这通常在争议解决中逐案确定。

Getty Images 和《纽约时报》提起的诉讼试图确立，人工智能服务提供商抓取他们的数据不是合理使用，因为他们用它来构建竞争产品。在图像生成的情况下，这一点尤其容易可视化——可以想象 Getty Images 的客户可能会使用 Stable Diffusion 来创建图像。

如 Getty Images 所辩称（[`mng.bz/RVgO`](https://mng.bz/RVgO)），

> Stability AI 已从 Getty Images 网站上复制了至少 1200 万张受版权保护的图片……Stability AI 现在与 Getty Images 直接竞争。

《纽约时报》的诉讼还试图确立 OpenAI 的模型作为报纸网站的替代品([`mng.bz/2yvm`](https://mng.bz/2yvm))：

> 被告坚持认为，他们的行为应被视为“合理使用”受到保护，因为他们的未经许可使用受版权保护的内容来训练 GenAI 模型服务于一个新的“转换”目的。但是，使用《泰晤士报》的内容而不支付费用来创建替代《泰晤士报》并从其那里吸引受众的产品，并没有什么“转换”之处。

到我写这篇文章的时候，这些争议还没有得到解决。我们将看看法庭上会发生什么。

我认为连续的争议可能的一个结果是，监管机构将要求 AI 提供商尊重退出的请求——如果数据所有者不希望他们的数据用于训练 AI 模型，则不应使用。数据所有者必须通过在约定格式中的机器可读元数据中表明他们希望退出的意愿。这就是如果你不希望搜索引擎抓取和索引你的内容时的工作方式。你必须在一个名为 robots.txt 的文本文件中指定这一点，该文件在请求你的根域名时返回（例如，[example.com/robots.txt](http://example.com/robots.txt)）。该文件以特殊格式描述了哪些网站部分允许被抓取以及由谁抓取。所有主要搜索引擎都遵守此协议。

另一个争议是 AI 生成的内容本身是否受版权法保护。例如，如果你使用 Midjourney 生成一个图像，你能阻止他人复制 AI 生成的图像吗，因为这侵犯了你的版权？版权联盟认为，仅由 AI 生成的作品不受版权保护。然而，它澄清了([`mng.bz/1Xnn`](https://mng.bz/1Xnn))：

> 如果一部作品既包含由 AI 生成的元素，也包含受版权法保护的人类创作元素——例如人类创作的文本或人类对作品各部分的最小创造性安排、选择和协调——那么受版权保护的作品元素将归人类作者所有。

我不太确定这是什么意思。如果我使用 Midjourney 生成一个图像，这个作品是完全由 AI 生成的，还是因为我编写和改进了提示而我是作品的组织者？也许版权联盟还不知道，因为它在那一部分之后补充说，“AI 和版权问题将继续发展”，并邀请您订阅 AI 版权通讯以保持最新。

## 人工智能的经济

自从生成式 AI 的兴起以来，我们听到了很多关于其潜在经济回报的消息。听起来，很多人将因为 AI 而赚很多钱。但情况真的是这样吗？

一些 AI 提供商已经收入数百亿美元。例如，到 2024 年，OpenAI 创造了 37 亿美元。对于一个如此年轻的公司来说，这相当令人印象深刻。

但从长远来看，收入并不足以建立一个成功的企业。为了做到这一点，企业必须盈利——它必须收集比产生收入所花费的更多的收入。否则，除非有投资者持续注入资金来补贴其亏损，否则它无法支付账单。

2024 年，OpenAI 亏损了 50 亿美元。虽然其 37 亿美元的营收令人印象深刻，但这并不足以覆盖其更令人印象深刻的支出（[`mng.bz/PdMv`](https://mng.bz/PdMv)）。这很可能与训练和提供大型 AI 模型的高成本有关。有人估计，运行 ChatGPT 可能每天要花费 OpenAI 70 万美元（[`mng.bz/JYna`](https://mng.bz/JYna)）。据说训练 GPT-4 的成本为该公司 1 亿美元（[`mng.bz/wJma`](https://mng.bz/wJma)）。请注意，一些模型会定期用新数据进行重新训练，因此模型训练并不总是一次性支出。其他主要 AI 提供商，如 Anthropic 和 Mistral，也仍然没有盈利。

除了盈利之外，一个企业如果能够产生**良好**的利润，那么它才算得上是成功的——投资者希望他们的投资能够得到良好的回报。在竞争激烈的市场中，利润往往会随着时间的推移而逐渐被侵蚀，因为模仿者进入市场，推高了成本并降低了价格，因此很难持续获得良好的利润。后者需要一道**护城河**，也称为**竞争优势**，这是一种保护公司市场份额不受竞争对手侵害的特性。当一个企业从护城河中获益时，竞争对手无法在同等条件下进入市场，因此他们很难或成本过高来侵蚀你的市场份额。

似乎 AI 提供商并没有强大的护城河来保护他们的市场份额。特别是，LLMs（第一章中讨论的变换器架构）背后的方法论是公开的，因此其他人可以构建自己的竞争模型。AI 提供商对此感到有些不安。2023 年 5 月，一份泄露的谷歌备忘录表示：“我们没有护城河，OpenAI 也没有。…令人不舒服的真相是，我们并没有处于赢得这场军备竞赛的优势地位，OpenAI 也没有。…虽然我们的 AI 在质量上仍然略占优势，但差距正在惊人地迅速缩小。开源 AI 更快，更可定制，更私密，并且按性能和功能来看，每一点都更强大。”备忘录还承认：“我们没有秘密配方”，并建议：“当有免费、无限制的替代品在质量上相当时，人们不会为受限的 AI 付费。我们应该考虑我们的真正价值在哪里”（埃马纽埃尔·马吉奥里，2024 年，《硅化》）。

由于没有秘密配方，不同提供商创建的模型在性能和能力方面已经趋于一致，包括开源模型。可以预见，在某个时刻可能会发生市场动荡——一些公司可能会倒闭或停止其产品。开发大型 AI 模型的经济案例并不像最初看起来那么清晰。

除了大公司之外，许多较小的公司正在构建基于 AI 的产品，这些产品建立在基础模型之上——有些人称它们为“AI 包装器”。例如，有数十家公司提供 AI 工具，可以将你的普通照片转换成专业的头像。这些工具可能只是添加在公开可用的 AI 模型（如 Stable Diffusion）之上的一层薄薄的外壳，或者可能是其中之一经过微调的版本。这最初可能看起来像是一个成功的商业想法，因为你确实让人们创建头像变得容易。然而，这里没有护城河——“这些应用的秘诀”是提示词，除非它非常特别，否则其他人可能也能想出来。因此，竞争加剧，正如我们从提供类似功能的多个应用程序中已经看到的那样。这些薄利的企业要产生显著的利润将非常困难。

最后，关于使用 AI 工具大幅提高商业生产力的说法已经很多。2023 年，麦肯锡分享了以下估计（[`mng.bz/qxz6`](https://mng.bz/qxz6)）：

> 生成式 AI 对生产力的影响可能为全球经济增加数万亿美元的价值。我们最新的研究估计，在 63 个我们分析的应用案例中，生成式 AI 每年可能增加 2.6 万亿美元至 4.4 万亿美元的价值——相比之下，2021 年英国的国内生产总值（GDP）为 3.1 万亿美元。

但生产力的增长到目前为止几乎无法检测到。2024 年《经济学人》的一篇文章解释说：

> 宏观经济数据……几乎没有显示出生产力的激增迹象……在美国，作为全球 AI 中心的美国，每小时的产出仍然低于 2020 年之前的趋势。即使在采购经理调查产生的全球数据中，这些数据滞后较短，也没有显示出生产力激增的迹象。（“人工智能革命怎么了？”2024 年 7 月 2 日，《经济学人》）

文章还解释说，由于“对数据安全、有偏见的算法和幻觉”的担忧，商业世界中 AI 的采用率一直非常缓慢。它总结道：“到目前为止，这项技术几乎没有任何经济影响。”确实，似乎在商业中实施 AI 比最初看起来要困难得多。有人最近告诉我，问题是“最后一公里”——虽然 AI 可以帮助你完成工作的前 80%，但由于幻觉或需要痛苦的定制，很难让它很好地完成剩余的 20%。这使得生产力的提升不如承诺的那样引人注目。

因此，我建议你在听到关于 AI 经济收益的大话时要谨慎。结果尚未明朗。

## 烟雾和幻影

2023 年 11 月，有消息透露，Cruise 公司生产的自动驾驶汽车并不能完全自动驾驶。当汽车遇到问题时，远程控制室里的一群人类操作员会手动干预。这种情况每行驶 2.5 到 5 英里就会发生一次。该公司为每辆在路上的汽车配备了 1.5 名员工来做这项工作（[`mng.bz/7pM7`](https://mng.bz/7pM7)）。商业教授托马斯·W·马龙说：“可能只是支付司机坐在车里驾驶会更便宜” ([`mng.bz/mGpW`](https://mng.bz/mGpW))。

几个月后，Cruise 的主要竞争对手 Waymo 在一篇博客文章中解释说：“就像拨打朋友电话一样，当 Waymo 的车辆在道路上遇到特定情况时，自动驾驶驾驶员可以联系人类车队响应代理以获取更多信息，以对其环境进行上下文化” ([`waymo.com/blog/2024/05/fleet-response/`](https://waymo.com/blog/2024/05/fleet-response/))。

亚马逊的“直接出门”技术也发生了类似的事情，这项技术被安装在了亚马逊的超市里。据称，这项技术利用 AI 根据安装在天花板上的摄像头拍摄的画面自动准备购物收据。2024 年 4 月，一名记者揭露，印度有 1000 名远程工作人员在观看视频，并手动准备或调整至少 70%的收据（[`mng.bz/5gX8`](https://mng.bz/5gX8)）。

使用人类秘密地驱动 AI 的做法常被比作 1770 年建造的机械 Turk，这个欺诈性的机器似乎能自己下棋，但实际上是由人类秘密地操作的。这台机器在巡展中展出了 84 年。

人工智能领域充斥着大承诺、炒作和夸张。机械 Turk 只是其中的一个例子——夸张和欺骗以不同的形式出现。例如，2023 年 4 月，谷歌高管声称他们的一款 AI 模型已经学会了孟加拉语，尽管它并没有在孟加拉语文本上进行过训练。其中一位高管解释说：“我们发现，在孟加拉语中只有很少的提示，现在它可以翻译所有的孟加拉语” ([`futurism.com/the-byte/google-ai-bengali`](https://futurism.com/the-byte/google-ai-bengali))。他们认为这是一个 AI 具有“涌现特性”的例子。

这条新闻迅速传播开来。一家印度报纸思考道：“AI 自己学会了孟加拉语，我们应该担心吗？” ([`mng.bz/6eyp`](https://mng.bz/6eyp))。一位听到这个新闻的人联系我，问我是否认为我们可能很快会面临一个“奇点事件”——AI 能力的戏剧性爆炸——因为现在 AI 可以自己学习新东西。

在了解了当前 AI 的工作原理（见第一章）后，很难相信它能够轻易地学习不属于其训练数据的新语言。结果证明，孟加拉语确实是该模型训练的语言之一，这与谷歌高管所说的相反（[`mng.bz/oKYy`](https://mng.bz/oKYy)）。

最近，在 2024 年 9 月，OpenAI 推出了一种名为 OpenAI o1 的新模型。公司将其定位为一种能够“思考”和“推理”的模型。通告解释说：“我们推出了 OpenAI o1，这是一种使用强化学习训练以执行复杂推理的新大型语言模型。o1 在回答之前会思考——在回应用户之前，它可以产生一个长的内部思维链。”（[`mng.bz/nROV`](https://mng.bz/nROV)）。文章中使用了“思考”这个词 9 次，使用了“推理”这个词 17 次。这种定位让人听起来像是一个重大的改进，也许是从通常的自动补全 LLM 中的一次转变。它还听起来像是朝着更类似人类的 AI 迈进了一步——通告说该模型在回答之前可以花更多的时间思考，“就像一个人一样”。

但一旦我们超越营销材料，我们会意识到 o1 系统并不像看起来那么新颖。它的工作原理如下：首先，使用 LLM 生成一段包含建议步骤列表的文本，以解决问题。然后，将这些指令添加到原始提示的末尾。因此，新的提示包含了原始任务，后面跟着一个建议的逐步执行方案。之后，这个扩展提示像往常一样通过 LLM 运行。这模仿了流行的思维链提示技术，其中用户将解决问题的逐步指南添加到提示中。

通告强调使用了强化学习来训练系统：“我们的大规模强化学习算法教导模型如何通过其思维链在高度数据高效的训练过程中进行富有成效的思考。”这听起来可能很令人印象深刻，但可能并没有什么新意。OpenAI 已经使用带有人类反馈的强化学习（RLHF）来改进其所有模型已有相当长一段时间（见第一章）。很可能是他们所说的“强化学习”是指人类手动编写了一小部分示例数据集，其中包含了他们希望 LLM 生成的逐步指令，然后 LLM 被优化以更准确地生成这样的指令。

我建议你在听到令人印象深刻的 AI 公告时要谨慎。我建议你在分析公告时牢记当前 AI 的工作原理，这使你更容易读出言外之意，区分精华和糟粕。

## 规章制度

在 2024 年 8 月，欧盟实施了关于人工智能的法规，被称为 AI 法案。AI 法案适用于在欧盟内使用或其输出被使用的 AI，即使它是在其他地方开发和运行的。这项法规引起了争议，有些人认为它不足，而有些人认为它过度。无论如何，让我们快速讨论一下它，因为你可能受到影响（例如，你可能开发了一个在欧盟使用的基于 AI 的产品），因为它可能成为未来其他地方 AI 法规的蓝图。

AI 法案包含四个特别章节，这些章节对 AI 系统的开发者和用户特别相关。我们下面简要评论每个章节。您可以在网上阅读全文([`mng.bz/vKWm`](https://mng.bz/vKWm))或查看官方的高级摘要([`mng.bz/4aQ5`](https://mng.bz/4aQ5))。

### 禁止的人工智能实践

本法案的这一部分描述了一系列被明令禁止的人工智能实践，因为这些实践被视为严重的违规行为。这些包括用于操纵或欺骗人们的 AI、利用人们因年龄、残疾或特定的社会或经济状况而存在的脆弱性的 AI，以及用于社会评分的 AI，以及其他类别。

### 高风险系统

本部分包含适用于高风险产品的规定。这些产品已经受到欧盟监管，需要第三方合格评定，例如某些车辆、机械和医疗设备。它还增加了几个新的类别，例如用于针对性招聘广告的 AI 和用于签证申请的 AI。法案对这些高风险系统提出了多项要求，包括启用人工监督“以了解其能力和局限性，检测和解决问题，避免过度依赖系统，解释其输出，决定不使用它或停止其操作。”

### 透明度义务

本部分要求公司在与 AI 系统互动时通知用户（除非很明显或 AI 用于法律目的，如犯罪侦查），这特别适用于“生成或操纵构成深度伪造的图像、音频或视频内容的 AI 系统。”即使对于被认为不是高风险的系统也是如此。请注意，如果你使用 AI 生成内容，但随后你彻底审查了内容并对其承担编辑责任，你不再需要通知他人使用 AI。

顺便说一句，不用担心 AI 法案会破坏你的 AI 艺术——你可以表明你正在以“不损害作品展示或欣赏的方式”使用 AI。

### 基础模型

本部分对基础模型提出了要求，这些模型被称为“通用人工智能模型”。法案要求 AI 提供商编写详细说明模型开发的文档，包括“用于训练、测试和验证的数据信息”以及“模型的已知或估计能耗。”

此外，法案还特别提到了一类非常大的基础模型，这些模型被认为可能带来“系统性风险”。这些模型在训练量方面超过了一定阈值（当前设定为训练过程中的 1025 次浮点运算）。这些模型的创作者必须通知欧盟他们的工作，欧盟可能会对降低风险提出额外的要求。

此外，该法案批准了未经授权从抓取的数据中训练模型，只要尊重退出权。这是通过引用一个允许为了数据分析目的进行网络抓取的指令间接批准的。有些人批评了这个指令，认为“数据挖掘”过于宽泛，几乎可以涵盖任何内容（[`mng.bz/QDa1`](https://mng.bz/QDa1)）。

## 资源消耗

训练和使用 AI 模型消耗电力和其他资源，其规模受到了批评。例如，一位记者称 AI“是气候的灾难”（[`mng.bz/Xxzl`](https://mng.bz/Xxzl)）。

评估人工智能的电力消耗是困难的，因为供应商尚未持续地报告这一数据。因此，我们必须依赖其他人进行的研究。这些研究并不完全标准化，因此它们有些混乱且难以追踪。其中一些研究甚至在同一报告中以混乱的方式混合了不同的单位，例如千瓦时（kWh）、二氧化碳排放量和“相当于智能手机充电次数”（[`arxiv.org/pdf/2311.16863`](https://arxiv.org/pdf/2311.16863)）。有时研究人员会依赖传闻和松散的逻辑联系来计算消耗量。例如，一位研究人员通过一位谷歌高管在一次采访中提到 LLM（大型语言模型）可能比执行一次谷歌搜索消耗 10 倍多的电力这一事实，间接地推断出 LLM 的能源消耗量（[`mng.bz/yW57`](https://mng.bz/yW57)）。

在以下内容中，我将分享一组来自 Hugging Face 和卡内基梅隆大学的研究人员所展示的研究结果。研究人员使用了多个开源模型以及他们自己的 GPU，并测量了消耗量。

表 6.1 显示了研究人员报告的文本和图像生成的电力消耗情况（[`arxiv.org/pdf/2311.16863`](https://arxiv.org/pdf/2311.16863)）。消耗数据是研究人员研究的不同模型平均得出的（每个模型的个人消耗量没有以一致的方式报告）。

##### 表 6.1  不同模型平均电力消耗与典型家庭消耗量的比较

|  | 千瓦时/1,000 次响应 | 每日家庭千瓦时（美国）的百分比 | 每日家庭千瓦时（英国）的百分比 |
| --- | --- | --- | --- |
| 文本生成 | 0.047 | 0.15% | 0.5% |
| 图像生成 | 2.907 | 9.83% | 30% |

注意，这些数据是按每千次模型使用计算的，例如使用 LLM 生成整个响应 1000 次或使用文本到图像模型生成 1000 张图像。1000 次 AI 使用可能看起来很多，但密集用户可能在一天内就轻松达到这个数字。例如，使用 GitHub Copilot 的程序员每小时可能生成数百个基于 LLM 的自动完成。此外，许多我们常规的在线行为，如进行谷歌搜索或浏览在线商店，可能会触发 LLM 查询（谷歌已经在搜索中显示 AI 结果），即使用户没有直接使用 LLM，也会增加 LLM 的使用量。我们还可以想象，一小群图形设计师可能会在短时间内通过反复提示系统创建图像并调整结果来生成 1000 张图像。

在这些实验中，图像生成比文本生成更耗电。然而，研究人员没有透露用于文本生成的提示或每次生成多少文本。此外，他们只使用了较小范围的文本生成模型，如 GPT-2 模型，这些模型比随后的生成大 100 倍。作者报告了模型之间的显著差异。特别是，最大的图像生成模型消耗的电量是最小的模型的 6000 倍。

注意，AI 模型正在不断优化，因此未来的消耗可能会减少——有时在不显著降低其能力的情况下，可以将模型做得小得多。为了参考，我在表 6.1 中添加了两列，比较了 AI 消耗与典型美国家庭（[`www.eia.gov/tools/faqs/faq.php?id=97&t=3`](https://www.eia.gov/tools/faqs/faq.php?id=97&t=3)）和英国家庭（[`mng.bz/MDQE`](https://mng.bz/MDQE)）的每日总电力消耗。

最令人担忧的不是电力消耗本身，而是为了产生电力而排放的 CO2。*碳强度*衡量每千瓦时消耗产生的 CO2 克数，这取决于电力是如何产生的。表 6.2 根据美国（[`mng.bz/av6x`](https://mng.bz/av6x)）和英国（[`mng.bz/gaXZ`](https://mng.bz/gaXZ)；这两个国家从不同的来源发电，因此它们的碳强度不同）典型的碳强度重新表述了上述结果。

##### 表 6.2  表 6.1 中电力消耗与汽油车等效 CO2 排放的比较

|  | 每千次使用产生的 CO2 克数（美国） | 等效 CO2 排放的行驶里程（美国） | 每千次使用产生的 CO2 克数（英国） | 等效 CO2 排放的行驶里程（英国） |
| --- | --- | --- | --- | --- |
| 文本生成 | 20 | 0.05 英里 | 7.6 | 0.02 英里 |
| 图像生成 | 1,200 | 3.1 英里 | 470 | 1.2 英里 |

为了更清楚地说明问题，表中还包括了您需要驾驶汽车行驶多少英里才能排放相同数量的 CO2（[`mng.bz/av6x`](https://mng.bz/av6x)）。

除了使用 AI 模型所需的电力外，许多人强调，*训练*它们是一项耗能的活动。据估计，训练 GPT-3 消耗了 1,287 MWh ([`arxiv.org/pdf/2104.10350`](https://arxiv.org/pdf/2104.10350))。这相当于 43000 个美国家庭或 134000 个英国家庭一天所消耗的电量。请注意，尽管模型只是偶尔进行训练，但 AI 提供商每年会训练或重新训练多个模型。

使用和训练 AI 模型也消耗其他资源，例如用于冷却数据中心的水。一篇发表在《财富》杂志上的文章解释说：“微软披露，其全球用水量从 2021 年到 2022 年增长了 34%（达到近 17 亿加仑，或超过 2500 个奥运标准游泳池），与往年相比有显著增长，外部研究人员将其与 AI 研究联系起来” ([`mng.bz/eyNw`](https://mng.bz/eyNw))。

当你使用 AI 时，我建议你记住，“云计算”实际上是在地球上，在大型冷藏建筑内进行的，这可能非常耗费资源并对环境产生影响。

## 大脑与意识

让我们以轻松和哲学的笔调结束。将 AI 模型的结构与我们自己的生物大脑进行比较是很常见的。如果你还记得第一章，LLMs 执行了许多投影，这些投影是涉及矩阵乘法的数学运算。生物神经元传统上被描述为执行类似的计算，因此许多 ML 模型，包括 LLMs，被归类为*人工神经网络*。此外，一些 ML 模型架构与大脑特定部分的构造进行了比较。例如，卷积神经网络（CNNs）通常与大脑的视觉皮层进行比较。

事实上，我们仍然不完全理解大脑是如何工作的。例如，对神经元进行计算的传统理解过于简单（Penrose, R., 1989, *The Emperor's New Mind: Concerning Computers, Mind, and the Laws in Physics*. Oxford University Press, p. 511）。多年来，已经开发出更加复杂的模型。然而，这些模型仍然无法预测科学家在研究真实神经元工作时所观察到的现象。例如，2020 年，一组研究人员发现，传递信号从一个神经元到另一个神经元的树突实际上可能执行复杂的计算（Gidon, A. et al., 2020, “Dendritic action potentials and computation in human layer 2/3 cortical neurons,” *Science*, *367*[6473], pp. 83–87）。因此，它们不仅仅是像以前认为的那样只是传递信号的电线。更复杂的是，包围神经元的液体中含有被称为神经调节剂的分子，它们以不完全理解的方式影响神经元的行为。虽然已经取得了一些进展，但我们对于神经元和大脑的理解仍然相当有限。

到目前为止，只有一种生物的大脑被完全绘制出来，这意味着研究人员可以创建一个所有神经元之间连接的图谱，即*连接组*。这种生物是一种名为 C. Elegans 的小型蠕虫，它大约有 300 个神经元和 7000 个连接。然而，由于图谱只告诉我们哪些神经元连接到哪些神经元，但没有确切说明它们是如何工作的，因此无法模拟观察到的蠕虫行为。神经科学家安东尼·莫夫申得出结论，"连接组本身并没有解释任何事情"（Jabr, F., 2012, “连接组辩论：绘制蠕虫的大脑图值得吗？” *Scientific American*，[`mng.bz/pKaE`](https://mng.bz/pKaE)）。

在某些情况下，AI 模型的设计没有考虑到大脑结构，后来才强行引入大脑类比。例如，最初描述 CNNs 的文章并没有说这些是由大脑启发的。研究人员声称他们的设计决策是“受我们对形状识别的先验知识指导”（LeCun, Y. et al., 1989, “使用反向传播网络进行手写数字识别”，*Advances in Neural Information Processing Systems*，2）。多年以后，当 CNNs 变得流行时，同样的研究人员声称他们是“直接受到视觉神经科学中经典概念——简单细胞和复杂细胞——的启发，整体架构与视觉皮层腹侧通路中的 LGN-V1-V2-V4-IT 层次结构相似”（LeCun, Y., Bengio, Y., & Hinton, G., 2015, “深度学习”，*Nature, 521*[7553]，第 436–444 页）。

此外，类比通常相当松散。例如，如果我们忽略视觉皮层中不属于 CNNs 的一些已知事实，那么 CNNs 和视觉皮层的比较才成立（参见 *Smart Until It’s Dumb*，第二章）。

因此，每当您听到 AI 和大脑之间的类比时，都要谨慎。我们仍然不了解大脑，所以这种联系很可能是高度推测性的。

除了与大脑相关的推测之外，最新的 AI 热潮也重新点燃了意识辩论。仅举一个例子，2022 年，有报道称一位 AI 工程师声称谷歌的聊天机器人已经具有了意识（[`mng.bz/OBn2`](https://mng.bz/OBn2)）。

但是，就像大脑一样，我们对意识并不完全了解。我们知道大脑的一些部分负责无意识行为（如控制心跳），而其他部分与有意识感知（如视觉）有关，但我们不明白为什么有些部分有助于我们的意识，而其他部分则不然。我们也不了解全身麻醉是如何工作的；我们只知道从经验中得知，麻醉剂可以暂时关闭意识，但我们不知道其背后的机制。

此外，关于意识存在许多哲学问题，这些问题没有简单的答案。例如，有些人认为任何计算都会产生意识。根据这种观点，恒温器是有意识的，但以不同的方式。其他人，如物理学家罗杰·彭罗斯，认为意识根本不是由计算产生的，因此不能通过数字计算机创造。这场辩论仍在继续，我不确定我们是否能够确定恒温器是否有意识。

因此，我建议你在任何人声称对人工智能与意识之间的联系有明确答案时要谨慎。我们还有很多不知道的东西。

现在我们已经到达了这本书的结尾，让我们快速回顾一下所涵盖的内容。在这本书中，我们讨论了人工智能的力量——机器学习创新如何推动人工智能能够做到的边界。我们也讨论了人工智能的限制——有时人工智能会产生幻觉，或者不像乍看之下那么有用。因为人工智能不是万能的，其影响将根据上下文而变化——有时人工智能可能会自动化工作，但有时则不会；有时它可能是完成任务的最好工具，但有时则不是；等等。在这本书中，我试图涵盖辩论的两面，并据此提供建议。最后一章通过讨论围绕人工智能的一些更大的问题来完成我们的分析，其中许多问题仍然没有答案，并且很可能是未来的热门话题。

## 摘要

+   版权争议取决于对*合理使用*的解释。人工智能提供商认为，他们抓取数据是为了让他们的模型学习一般模式，并且他们没有意图复制原始数据，这意味着这是该数据的合理使用。数据所有者则认为，人工智能提供商使用这些数据来构建竞争产品并窃取他们的客户，所以这不是合理使用。

+   人工智能的经济案例并不那么明确。人工智能提供商仍然大部分是无利可图的，面临着激烈的竞争。创建薄人工智能包装的小公司也面临着激烈的竞争，可能难以收支平衡。由于人工智能，更广泛的经济中的生产力增长尚未观察到。

+   人工智能领域有夸大或甚至欺骗的倾向。许多声称使用了人工智能的产品被揭露实际上依赖于远程人类操作员手动完成工作。大型人工智能公告往往是不正确的（比如谷歌说一个模型学会了训练数据中没有的语言）或者被粉饰了（比如 OpenAI 说它的模型“思考”和“推理”）。

+   训练和运行人工智能模型所需的电量消耗（以及其他资源如水）受到了很多批评，有些人认为这将对环境产生有害的影响。关于人工智能资源消耗的研究和报告仍然很少，但我们可以看到这并不是一个可以忽略的数量。

+   人工智能模型与大脑结构的比较非常具有推测性。我们还没有完全理解大脑是如何工作的，因此比较往往显得牵强。对于人工智能与意识的关系也是如此——这仍然是一个没有明确答案的持续争论。
