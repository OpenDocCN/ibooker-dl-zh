# 术语和缩写词表

| 缩写/术语 | 定义/含义 |
| --- | --- |
| 随机不确定性 | 无法进一步减少的数据固有的不确定性。例如，你无法预测硬币会落在哪一边。 |
| API | 应用程序编程接口。 |
| 贝叶斯咒语 | 后验与似然乘以先验成正比。 |
| BNN | 贝叶斯神经网络。一种其权重被分布替换的神经网络。通过变分推断（VI）或蒙特卡洛 dropout（MC dropout）来解决。 |
| 贝叶斯概率模型 | 可以通过描述分布的所有参数来表示其认知不确定性的概率模型。 |
| 贝叶斯统计学观点 | 在贝叶斯统计学的观点中，参数θ不是固定的，而是遵循一个分布。 |
| 贝叶斯定理 | P(A | B) = *P*(B | A) · *P*(A) / *P*(B)。这个著名的公式说明了如何逆转条件概率。 |
| 贝叶斯学习 | P(θ | *D*) = *P*(*D* | θ) · *P*(θ) / *P*(*D*)。这个公式告诉你如何从似然*P*(*D* | θ)，先验*P*(θ)和边缘似然（也称为证据）*P*(*D*)中确定后验*P*(θ | *D*)。它是贝叶斯定理的一种特殊形式，其中*a* = θ，*b* = D，θ是模型的参数，D 是数据。 |
| 反向传播 | 一种高效计算损失函数相对于神经网络权重梯度的方法。 |
| Bijector | TFP 包中用于正则化流（NF）所需的可逆（双射）函数。 |
| CIFAR-10 | 包含 60,000 个 10 类 32 × 32 彩色图像的流行基准数据集。 |
| CNN | 卷积神经网络。一种特别适合视觉应用的神经网络。 |
| 计算图 | 编码神经网络中所有计算的图。 |
| 缩写/术语 | 定义/含义 |
| CPD | 条件概率分布。我们也不严谨地称一个结果（例如，一个人的年龄）的密度*P*(*y* | *x*）（例如，一个人的图像）为给定某些输入（例如，一个人的图像）的 CPD。 |
| 交叉熵 | 分类任务中负对数似然（NLL）的另一个名称。 |
| 确定性模型 | 一种非概率模型，它不返回结果分布，而只返回一个最佳猜测。 |
| Dropout | Dropout 指的是随机删除神经网络中的节点。训练期间的 Dropout 通常会产生表现出较少过拟合的神经网络。在测试时间（参见 MC dropout）期间执行 Dropout 也被解释为 BNN 的近似。 |
| DL | 深度学习。 |
| 外推法 | 超出模型训练数据范围。 |
| 认知不确定性 | 由模型参数的不确定性引起的模型不确定性。原则上，可以通过提供更多数据来减少这种不确定性。 |
| fcNN | 全连接神经网络。 |
| Glow | 基于 NF 的特定 CNN 网络，用于生成逼真的面部。 |
| ImageNet | 一个包含 1,000 个类别的 1 百万个标记图像的著名数据集。 |
| 雅可比矩阵 | 多维函数或多个变量变换的雅可比矩阵是其所有一阶偏导数的矩阵。 |
| 雅可比行列式 | 雅可比矩阵的行列式。它用于计算变换过程中的体积变化，并且对于 NF 是必需的。 |
| Keras | Keras 是一个高级神经网络 API，我们在本书中使用它配合 TensorFlow。 |
| KL 散度 | 一种衡量两个概率密度函数（PDFs）之间距离的度量。 |
| 似然 | 从由参数值θ指定的密度函数中采样的概率 P(D | θ)，即产生数据的概率。 |
| 损失函数 | 一种量化模型不良程度的函数，以及在深度学习模型训练过程中优化的函数。 |
| MAE | 均绝对误差。MAE 是一种性能度量，它是残差的绝对值的平均值。它不足以量化概率模型（这里应使用 NLL 作为性能度量）。 |
| MaxLike | 最大似然。 |
| MaxLike 学习 | 一种基于似然的方法，用于确定模型的参数值θ（例如，神经网络中的权重）。目标是最大化观察数据的似然 P(D | θ)。这对应于最小化 NLL。 |
| ML | 机器学习。 |
| MC dropout | 摩尔卡洛 dropout。这指的是测试时间中的 dropout。这是一种被解释为贝叶斯神经网络（BNN）近似的方法。 |
| MNIST | 更确切地说，是手写数字的 MNIST 数据库。一个包含 60,000 个 28×28 灰度 10 类（数字 0-9）的数据集。 |
| MSE | 均方误差。MSE 是一种性能度量，它是残差的平方的平均值。它不足以量化概率模型（这里应使用 NLL 作为性能度量）。 |
| NF | 归一化流。NF 是一种基于神经网络的拟合复杂概率分布的方法。 |
| NLL | 负对数似然。在拟合概率模型时用作损失函数。验证集上的 NLL 是量化概率模型预测性能的最佳度量。 |
| NN | 神经网络。 |
| 观察到的结果 | 对于某个实例 i 测量的观察到的结果或*y*[i]值。在概率模型中，我们旨在根据一些描述实例 i 的特征来预测*y*的条件概率分布。有时*y*[i]也被称为“真实”值。我们不赞成这种说法，因为在存在随机不确定性的情况下，没有真正的结果。 |
| PDF | 概率密度函数。PDF 有时也被称为概率密度分布。参见 CPD 以了解条件版本。 |
| PixelCNN++ | 一种捕捉像素值概率分布的特定卷积神经网络模型。“++版本”使用高级条件概率分布（CPD）以提高性能。 |
| Posterior | 在看到数据 D 后参数 θ 的分布 *P*(θ | *D*)。 |
| Posterior predictive distribution | 给定数据 D，由贝叶斯概率模型产生的 CPD *P*(*y* | *x*, *D*)。 |
| Prediction interval | 预期包含所有数据中一定比例的区间，通常是 95%。 |
| Prior | 在看到任何数据 D 之前分配给模型参数 θ 的分布 *P*(θ )。 |
| Probabilistic model | 返回结果分布的模型。 |
| Residuals | 观测值 *y**[i]* 与确定性模型输出 yˆi（结果的期望值）之间的差异。 |
| RMSE | 均方根误差。均方误差的平方根。 |
| RealNVP | 一种称为真实非体积保持的特定 NF 模型。 |
| softmax | 一种强制神经网络输出总和为 1 的激活函数，它可以被解释为概率。 |
| softplus | 一种激活函数，应用后确保值为正。 |
| SGD | 随机梯度下降。 |
| Tensor | 多维数组。这是深度学习中的主要数据结构。 |
| TF | TensorFlow 是本书中用于深度学习的低级库。 |
| The big lie of DL | 假设 *P*(train) = *P*(test)，意味着测试数据来自与训练数据相同的分布。在许多深度学习/机器学习应用中，这被假设但往往并不真实。 |
| TFP | TensorFlow Probability。一个用于促进深度学习概率建模的 TF 扩展。 |
| VGG16 | 一种具有特定架构的传统卷积神经网络，在 2014 年 ImageNet 竞赛中排名第二。它通常与在 ImageNet 数据上训练后从图像中提取特征的权重一起使用。 |
| VI | 变分推断。一种可以证明它产生近似贝叶斯神经网络的方法。 |
| w.r.t. | 缩写，意为“关于”。 |
| WaveNet | 一种用于文本到语音的特定神经网络模型。 |
| ZIP | 零膨胀泊松分布。一种针对计数数据且关注值 0 过多的特殊分布。 |
