# 第一章。引言

人工智能，尤其是生成式人工智能，遵循了许多新技术相似的采用周期。那些生产尖端技术以及位于或接近科技空间的组织通常更快地采用这项技术，而其他行业则采用了较慢的采用率。然而，最近的发展帮助商业领导者意识到，他们必须找出如何利用生成式人工智能来推动他们的业务，否则可能会被竞争对手抛在后面。

现在，企业正在投入更多资源以利用人工智能来推动他们的业务。这通常表现为数据科学家团队为他们的业务实施基于人工智能的应用程序的验证（POCs）。

在企业中，一个特别常见的验证项目是构建聊天机器人。通常，这些项目会使用检索增强生成（RAG），将专有数据与现成的大语言模型（LLMs）相结合，为聊天机器人提供特定问题领域的专业知识。

但这些企业通常面临着一项艰巨的挑战：他们有针对其业务的基于人工智能的应用程序的 POCs，但他们无法将其转化为生产。事实上，[这些项目中绝大多数从未进入生产阶段](https://oreil.ly/X1kML)。

为了提高这些 POCs 的成功率并提升企业中人工智能项目的投资回报率，企业必须更好地理解在生产环境中运行人工智能应用程序时出现的挑战。有了这种理解，领导者将能够更好地设计解决方案，以促进 POCs 转化为生产，并管理其产品生命周期。

解决和克服这些挑战是相对较新的学科 *机器学习运营（MLOps）* 的核心。本出版物将向您介绍为什么这是关键的一步，以及如何在 Kubernetes 上利用 MLOps。

在本报告中，我们将探讨构建人工智能应用程序的四个基本要素：

+   在实验阶段训练模型

+   使模型创建可重复和声明性

+   作为人工智能应用程序一部分的生产运营模式

+   确保您创建的模型是可信的并且是负责任地构建的

本报告将采用以 Kubernetes 为中心的视角，突出那些旨在成为 Kubernetes 原生项目，并且当它们一起使用时，能够帮助您将 MLOps 原则和实践应用于构建人工智能应用程序。

# 什么是 MLOps？

MLOps 的起源在于 *DevOps* 世界，这是一个寻求快速将高质量软件投入生产的最佳实践开发模型。它通过将开发和运营角色更紧密地结合起来来实现这一点。这种方法促进了软件开发和生产线生命周期中的协作和知识共享，同时将生产问题意识带到最擅长解决这些问题的团队和个人。这种方法要求开发者关注他们编写的软件在生产线上的表现，并且他们还积极参与在生产线上的软件运营。

随着人工智能/机器学习的普及以及创建的模型数量不断增加，MLOps 已成为快速将高质量模型投入生产的新范式，将 [DevOps 原则](https://oreil.ly/0QBop)应用于人工智能模型和人工智能驱动应用程序，而不是传统的软件应用程序。然而，MLOps 不仅将 DevOps 原则应用于人工智能开发生命周期，而且在此基础上构建，以定义构建和运行人工智能驱动应用程序的基础最佳实践。实施 MLOps 实践的团队应遵守以下核心原则，这些原则在 Chip Huyen（O’Reilly，2022 年）所著的 [*设计机器学习系统*](https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956) 一书中进行了扩展和深入解释：

持续集成和交付 (CI/CD)

一套强大的 CI/CD 自动化工具，可以重复构建、测试和部署人工智能驱动应用程序。

探索性工作流程编排

一个强大的数据科学工作流程编排工具，可以自动化从数据准备到模型训练、调优和评估的端到端模型开发生命周期。

可重现的工件

智能应用程序的给定版本的工件必须可重现，用于创建这些工件的所有组件都必须进行版本控制和良好记录。

跨团队协作

构建人工智能驱动应用程序需要多个角色之间的紧密协作，至少包括数据工程师、数据科学家、应用程序开发人员和运营团队。MLOps 强调这些群体之间的密切沟通和协作。

模型和数据血缘

智能应用程序的模型和数据血缘以及其他关键元数据必须得到良好跟踪，特别是为了在人工智能应用程序中建立信任，但也为了调试和可解释性。

监控

MLOps 要求在整个生产生命周期中对人工智能驱动应用程序进行监控。除了传统的应用程序监控之外，人工智能驱动应用程序还必须监控数据分布漂移、模型退化、偏差、合规性等问题。由于许多模型还使用昂贵的专用硬件，如 GPU 集群，因此监控这种硬件的效率也至关重要。

支持迭代的流程

MLOps 流程必须在智能应用的开发和生命周期中允许频繁迭代。数据科学家必须能够训练一个模型、评估其性能，并根据评估结果快速重新训练模型。同样，模型在发布到生产后必须定期重新训练，以便将新数据纳入其训练集中，因为新数据可能与原始训练数据不同。这种差异可以通过遵循之前提到的监控原则来捕捉。

现在我们已经了解了基础原则，让我们考虑人工智能开发生命周期。虽然这个生命周期可以有多种形式，但所有这些形式都大致遵循这个模式（如图 1-1 所示）：

项目启动

业务利益相关者、应用开发者、数据科学家和数据工程师协作，以确定智能应用所需的目标业务成果、可用于构建实现该成果的模型的原始数据，以及将生成的 AI 模型集成到软件应用中以提供所需解决方案的架构。

数据准备

数据工程师和数据科学家生成构建模型所需的必要训练和调整数据工件。这些工件不需要是静态的：它们可以是实时数据或数据库条目，这些条目可以从数据摄取管道中不断更新，例如。

模型实验

数据科学家消耗准备好的特征数据以创建一个性能足够的模型，经常迭代不同的模型架构、训练超参数和特征数据组合。

应用集成

应用开发者与数据科学家紧密合作，将训练好的模型集成到应用代码中，该代码将通过 API 消费训练好的模型。

生产服务

应用被推广到生产环境中，在那里它增加价值，并持续迭代以改进其性能和添加新功能。

![图片](img/skia_0101.png)

###### 图 1-1\. 此流程图展示了人工智能开发生命周期的迭代特性

由于对能够实现这个生命周期和 MLOps 最佳实践的平台的强烈需求，今天市场上存在大量 AI 开发平台并不令人惊讶。本报告将深入研究开源 Kubernetes 容器编排平台，突出它如何在人工智能开发生命周期中应用基础 MLOps 原则到您的流程中。

# 为什么要在您的 MLOps 平台上使用 Kubernetes？

Kubernetes 拥有许多优势，使其成为构建和运行遵循 MLOps 原则的 AI 应用的优秀平台。由于 Kubernetes 应用程序以声明性方式编写，它允许团队在构建 AI 模型时始终如一地产生可重复的结果。这一点，结合内置的强大[GitOps 工具](https://oreil.ly/Fjk7z)，使得版本控制模型训练工件变得非常容易。

创建模型需要访问专用硬件，而硬件的使用往往是不可预测的，具有突发性。Kubernetes 能够抽象出硬件资源分配的过程，使得数据科学家可以专注于模型开发，而不是配置硬件环境。在生产方面，运行 AI 应用需要分别扩展应用程序的不同部分，包括服务于后端模型和提供用户访问模型的计算资源以及前端 API。由于 Kubernetes 抽象出了硬件资源分配，因此扩展部署应用程序的不同部分变得更加容易。

除了手动扩展之外，Kubernetes 还允许用户自动扩展高度专业化和昂贵的计算资源。这种细粒度的资源管理对于正确管理成本至关重要。

另一个考虑因素是专用硬件，如加速器，尤其是在大规模训练和调优作业中，这些硬件可能相当脆弱。在一个需要执行数天的模型训练或微调作业中，硬件故障迫使你重新开始训练可能会造成相当大的损失。Kubernetes 具有自我修复功能，与常见训练库中的检查点支持相结合，消除了这个问题，使 Kubernetes 成为一个健壮的容错平台。

虽然许多 AI 开发平台都与特定的云平台绑定，但 Kubernetes 能够在任何你需要的地方运行。这包括云提供商、私有数据中心、边缘位置和混合配置，这使得 Kubernetes 能够作为单一部署平台，用于构建你的应用程序。

在监控方面，Kubernetes 与 Prometheus、DataDog 和 Grafana 等监控工具集成，可以帮助跟踪模型性能和资源使用情况。这对于 LLM 尤为重要，因为它们体积庞大且运营成本高。这些深度集成为 MLOps 管理员提供了主动监控和警报，以确保模型在关键 AI 工作负载中运行优化。

最后，推出模型更新，尤其是 LLM（大型语言模型），可能是一项困难和昂贵的实践。Kubernetes 通过滚动更新（逐步推送更新）和金丝雀部署等特性简化了这一过程，有助于最小化这些模型的停机时间。
