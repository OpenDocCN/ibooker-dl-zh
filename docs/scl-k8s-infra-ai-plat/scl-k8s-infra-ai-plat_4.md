# 第五章\. 负责任的 AI

迄今为止，我们专注于构建模型并为其部署做准备。然而，在上线之前，还有一些额外的、重要的考虑事项需要涵盖。

大型语言模型（LLMs）不仅在技术层面上强大，在社会层面上也是如此。它们被用来放大垃圾邮件、帮助骗子，甚至大规模生成看似合理的宣传。它们在庞大的数据集上进行训练，这些数据集包括我们希望从中汲取的内容以及各种偏见、种族主义、性别歧视，以及可能非法或有害的内容（例如，ChatGPT 的早期版本愉快地向用户解释如何制造炸弹）。因此，对 LLMs 的道德和负责任训练及使用的重视和审查有所增加。

# 数据安全和透明度

与任何其他机器学习模型一样，训练数据对于大型语言模型（LLMs）至关重要。然而，与其他机器学习模型不同的是，LLMs 生成文本，这为恶意行为者创造了一个新的攻击向量。为了应对这种情况并帮助防止不道德的数据使用，一种方法就是公开关于 LLM 是如何训练的、它使用了哪些数据以及谁开发了它的明确信息。这些信息使用户、研究人员和监管机构能够有效地审查模型的行为，并报告或管理伤害或滥用事件。有几个平台和框架可用于实现这一目标，例如联合国教科文组织（UNESCO）的[全球人工智能伦理与治理观测站](https://oreil.ly/HSfmk)，还有更多由政府团体、私人智库、构建人工智能的组织和学术实验室开发并迭代。

由于 LLM 可能产生各种输出，因此模型可能生成的输出可能存在安全漏洞（如泄露个人身份信息[PII]）、安全或道德违规（例如生成种族主义或性别歧视内容）或有害信息（例如自杀或制造炸弹的方法）。这些话题很难考虑，而 LLM 的开放性性质又增加了挑战，但由于这项技术的广泛部署和新型能力，考虑这些问题非常重要。

随着人工智能监管的加强，透明度将是确保遵守与数据隐私、使用和安全相关的法律和标准的关键。透明度高的数据实践有助于公司证明它们正在履行其法律义务，例如美国的健康数据 HIPAA、欧盟的 GDPR 以及审查个人身份信息（PII）。全球的监管框架处于不断变化的状态，因此联合国贸易和发展署的[全球数据隐私法律](https://oreil.ly/gCUls)页面是一个咨询最新全球监管格局的好资源。

为了评估安全性，目前一个流行的工具是 LM-Eval。LM-Eval 允许您在广泛的特定任务基准上测试生成 AI 模型，还可以构建自己的基准来测试您的训练模型。此工具通过 TrustyAI Kubernetes Operator 与 Kubernetes 兼容，并在[Red Hat OpenShift AI](https://oreil.ly/4s0C_)中默认安装。查看 TrustyAI 网站了解有关在 Kubernetes 中使用[LM-Eval](https://oreil.ly/ZeZyV)的更多信息，并了解[LM-Eval 如何与集群中的其他部分相匹配](https://oreil.ly/dsTgo)。

# AI 安全护栏

AI 安全护栏是一个新的软件概念，作为 LLM 的安全机制。这些解决方案作为入站请求和 LLM 之间的中间件，并添加限制以防止产生有害或违反某些规范的结果。例如，安全护栏可以检测输出中的错误 PII 并将其删除，然后再将其返回给用户。安全护栏应该能够执行企业的政策和指南，实现上下文理解，并且需要定期更新。

一个正在构建整个开源 AI 安全工具套件的社区是[TrustyAI](https://oreil.ly/hMSM-)，他们还创建了之前提到的 LM-Eval。他们还提供了 AI 安全护栏工具，例如`trustyai-detoxify`，这是 TrustyAI Python 库中的一个 Python 模块，它提供了围绕有毒语言的护栏，以及其他以有毒语言为中心的安全工具。此外，他们维护[TrustyAI Guardrails](https://oreil.ly/AUOiB)，它作为一个服务器，用于调用检测器（如有毒语言检测器），以帮助开发者实施自己的安全护栏。

LLM 安全护栏检测器是确保 LLM 在预定义的道德、安全和操作边界内运行的工具。这些工具旨在识别和减轻不良结果，例如生成有害或不适当的内容，确保 AI 系统负责任地行为。目前有各种各样的检测器可用，您可以通过[Guardrails AI Hub](https://hub.guardrailsai.com)访问许多公共的检测器。一些广泛的检测器示例包括：

PII 检测器

这些屏幕输出用于识别 PII 并防止其到达用户。

仇恨、滥用、粗话（HAP）检测器

这些屏幕输出用于检测有毒内容、偏见或有害内容，如仇恨言论、歧视或错误信息。

偏见检测器

这些分析输出，以寻找与种族、性别、宗教或其他属性相关的公平偏见迹象。

提示注入检测器

这些识别并对抗潜在的对抗性提示或试图操纵模型生成有害内容的行为。

TrustyAI 提供了一个[Kubernetes Operator](https://oreil.ly/kB0fi)，允许您无缝地将它与您的集群集成，以及一个[KServe 的自定义解释器](https://oreil.ly/FzfM_)。

尽管这些工具包并不全面，但它们是目前可用的资源，有助于解决 LLMs 的滥用问题，确保您的用户和企业的安全。要深入了解在整个 AI 开发生命周期中减轻偏见和伤害的方法，请参阅 Aileen Nielsen 的书籍[*《实用公平性》*](https://www.oreilly.com/library/view/practical-fairness/9781492075721/)（O’Reilly，2020 年）。
