# 第四章. 模型部署和监控

在前面的章节中，你学习了模型定制技术，包括微调和训练，以及使训练和评估可重复的技术。一旦你使用模型获得了你想要的结果，就是时候将你的模型部署到生产环境中了。

本章将通过概述使用 Kubernetes 部署和监控机器学习模型的主要技术和技术，为你准备模型部署和服务的知识。虽然我们将关注与大型语言模型（LLMs）和生成式 AI 相关的特定技术，但本章的大部分内容也适用于传统的机器学习模型。

# LLM 服务概述

*模型服务*是指实时处理推理请求的行为，这需要将已训练的模型部署到适合接收这些请求的位置。在较高层次上，模型服务涉及打包模型，在硬件加速器（如 GPU 或 CPU）上部署它，公开 API 供用户查询模型，并启用监控和警报的指标。模型服务系统的组件包括模型服务平台、模型服务运行时和指标收集与监控系统。通常，还包括用于处理模型查询流量突增的 API 网关和负载均衡器。

模型服务平台组件从存储（如 Amazon S3 或本地持久卷）检索模型，然后执行各种预处理任务，例如更改模型格式或后处理步骤，如收集指标。它包含一个模型服务运行时，以帮助其提供服务。它还公开 REST 或 gRPC API，以便用户可以与模型交互，同时通过网关和负载均衡器提供模型访问安全性。

模型服务运行时组件将模型加载到 GPU 或 CPU 内存中，将任何传入的查询或提示从其网络表示形式反序列化，将其转换为适合模型的格式，然后在模型上执行推理以获取响应。此响应通常序列化为 JSON 对象或其他格式，并返回给调用应用程序。

指标和监控组件通常聚合请求指标，如请求时间、错误代码、令牌计数、跟踪等，并将它们存储到像[Prometheus](https://prometheus.io)这样的指标服务器中。这些指标允许 MLOps 从业者确保生产中模型的健康和性能，并通过警报诊断可能出现的任何问题。

虽然大多数开发和计算时间都花在训练和调整模型上，但[模型的生命周期中近 90% 的时间都花在服务上](https://oreil.ly/7rEbn)，这就是为什么优化服务对于从模型中获得商业价值至关重要。在接下来的章节中，您将使用 Kubernetes 特定的工具了解这些基本组件，这些工具将帮助您扩展您的生成性 AI 推理工作负载。

# 使用模型服务平台

模型服务平台是任何推理系统的核心组件，根据传入的推理请求数量管理模型部署和扩展。目前有许多平台可用于在 Kubernetes 上提供服务模型。它们的目的在于简化并扩展模型部署和推理服务过程。

为了在 Kubernetes 上以可扩展的方式服务 LLM，服务平台应满足以下要求：

+   支持不同类型的模型架构

+   通过添加新的模型架构进行扩展

+   支持为不同模态生成嵌入，例如文本或图像

+   支持推理中的多种模态

+   支持跨多个模型的推理链（*模型组合*）

+   广泛的硬件加速器支持

+   与标准 Kubernetes API 和工具的集成

+   对不同模型工件格式的强大支持

+   支持广泛的存储技术

+   与 API 网关的集成

+   能够以 A/B 测试或金丝雀发布的方式部署模型

+   提供与预处理和后处理系统的灵活集成选项

+   支持自动扩展推理基础设施

+   模型监控解决方案的集成

部署 LLM（大型语言模型）到 Kubernetes 最受欢迎的工具之一是 [KServe](https://oreil.ly/volxt)。KServe 是 Kubernetes 的控制器，它使 Kubernetes 能够服务预测性和生成性 AI 模型，同时维护推理请求的预处理和后处理管道。它不仅满足之前提到的要求，而且还提供了几个帮助企业广泛采用的好处：

+   一个活跃且繁荣的开源社区

+   支持流量路由和自动扩展，包括扩展到零

+   支持批量和实时推理工作负载

+   支持使用标准协议 Open Inference Protocol 进行预测性和生成性 AI 推理

与其他控制器一样，KServe 由一组自定义资源组成，这些资源是基础 Kubernetes API 的扩展。其中之一是 ServingRuntime。这本质上是一个部署模板，它定义了模型将从其中提供的环境。KServe 自带一些开箱即用的 ServingRuntimes，但其他 ServingRuntimes 也可以轻松添加到系统中。每个 ServingRuntime 定义了诸如用于运行时的容器镜像以及 ServingRuntime 支持的模型格式等内容，并且可以通过在容器中设置的环境变量进一步自定义。这使用户能够轻松添加对新模型架构的支持。

KServe 的核心是 InferenceService。这是一个自定义资源定义，您在其中定义预测器、存储位置、模型格式、用于渐进式部署的金丝雀、部署模式以及为提供服务所需的任何其他内容。模型通常从云存储，如 Amazon S3 存储桶初始化，但也可以使用符合 OCI 规范的容器作为 KServe 的云存储替代方案[“Modelcars”](https://oreil.ly/gDOnq)。

要使用此组件，必须创建一个符合 OCI 规范的容器镜像，并将其添加到容器注册库，如 Quay。当使用 KServe 进行部署时，您可以引用包含容器的存储库。由于 Kubernetes 集群会缓存下载的容器镜像，因此模型不需要多次下载，这可以减少启动时间，同时仍然减少整体磁盘使用量。

###### 注意

KServe 提供三种部署模式：`RawDeployment`模式使用标准的 Kubernetes 部署和入口网关（仅用于路由入站请求的 API 网关）；无服务器模式使用[Knative](https://knative.dev)对象来启用无服务器部署；ModelMesh 允许在 Pod 中部署多个模型，以使用较少的计算资源来扩展较小的模型。

在 InferenceService 中使用服务运行时，InferenceService 由 KServe 控制器管理(图 4-1)。这确保了部署的应用程序状态与 InferenceService 的定义相匹配，为每个推理端点创建部署，并启用自动扩展等功能。

每个端点由三个组件组成：

预测器

这是端点所需唯一组件。它由一个模型和模型服务器组成，使得模型在端点可用。

Transformer

此组件允许用户根据需要定义预处理和后处理步骤，以管理传入的请求数据和输出的推理数据。

解释器

这使得一个提供预测和模型解释的替代工作流程成为可能。KServe 提供 API，使用户可以编写和配置自己的解释容器。

当用户使用 `:predict` 或 `:explain` 调用 KServe 端点时，该请求会被路由到三个组件。对于任一调用，转换器组件是请求的第一个目的地。如果用户调用了 `:predict`，则请求会被路由到预测器。如果用户调用了 `:explain`，则请求从转换器路由到解释器组件，然后解释器在预测器组件上调用 `:predict` (图 4-1)。

![skia_0401](img/skia_0401.png)

###### 图 4-1\. 用户调用带有 `:predict` 或 `:explain` 调用的 KServe 端点的请求流程

虽然 KServe 有许多功能使其成为部署机器学习模型的理想平台，但 LLMs 通常需要一些额外的工作。

# 深入了解 vLLM 中的 LLM-Serving 运行时

因为 KServe 允许你定义自己的 ServingRuntime 资源，所以可以使用它来配合替代模型服务运行时。其中一个这样的运行时是 [vLLM](https://docs.vllm.ai)，这是一个针对 LLMs 定制的服务系统，旨在提高推理效率和可扩展性。它通过优化内存使用和执行速度来解决部署 LLMs 的挑战，使其适合需要高吞吐量和低延迟的实时应用。

###### 注意

KServe 还包括一个现成的 LLM 运行时，即 [Hugging Face LLM Serving Runtime](https://oreil.ly/K3Bsx)，它使用 vLLM 作为其默认后端。

vLLM 提供了一个基于 FastAPI 的服务器，用于在线模型服务，它与 OpenAI API 兼容，也与流行的机器学习框架（如 PyTorch）兼容，允许与现有的机器学习管道无缝集成，并促进使用这些框架训练的模型的部署。

vLLM 还支持动态批处理，它将多个推理请求组合成一个批次以改进处理效率。这在高流量场景中特别有益，因为它可以显著提高吞吐量。

vLLM 服务速度的关键是一些核心架构特性：

分页关注

这是一个允许在非连续内存块中存储大连续键值对的算法，从而优化内存使用。

并行执行

该架构通过利用模型并行性和张量切片来支持模型组件的并行执行。这允许模型的不同部分在多个硬件单元上同时处理，优化资源利用并加快推理速度。

张量缓存

vLLM 具有内存中的张量存储，它缓存频繁访问的张量以避免重复计算。通过提供对必要数据的快速访问，这显著减少了推理所需的时间。

尽管许多模型服务运行时都可以与 KServe 一起使用，但由于这些特性，vLLM 是服务 LLMs 的一个强有力的竞争者。一旦你的模型部署完成，了解模型随时间推移的性能表现以及跟踪系统中可用的模型家族和版本变得至关重要。在下一节中，你将了解如何监控 LLMs，需要监控哪些指标，以及如何使用注册表来跟踪已部署的模型。

# 监控和跟踪你的模型

无论你有一个还是数百个模型在生产环境中，实时监控它们的性能并跟踪你有哪些模型在生产中是至关重要的。为了监控性能，你必须知道你应该跟踪哪些指标。这部分取决于你的用例和基础设施，但也有一些通用的指标你可以跟踪。一旦你对部署的 LLMs 需要跟踪的指标有了了解，我们将继续探讨在 KServe 中监控这些指标，然后跟踪你的模型在注册表中的情况。

## LLM 指标

LLM 评估是一个快速发展的领域。毕竟，你如何判断一个 LLM 是否在按照你的意愿执行？你是否担心*幻觉*或事实性错误输出？或者，对于你的用例来说，机器创造力是否很重要？或者，也许你不太关心内容，但想确保你的模型以可接受的速率处理推理请求，并且请求不会在队列中停滞。

对于基于任务的指标，例如衡量摘要或翻译的指标，有无数可用的指标可供选择。然而，你应该谨慎行事，因为这些任务是开放式的，不太可能有一个单一的指标能准确反映你模型的表现。你将不得不评估适用于你用例的指标，并选择一个能够准确传达模型在其特定任务上表现的组合。在 Neptune 发布的文章《“LLM 文本摘要评估”》（[“LLM Evaluation for Text Summarization”](https://oreil.ly/qX9dx)）中可以找到广泛的各种摘要任务特定指标。

模型服务运行时通常自带指标，这些指标衡量服务器在处理每个阶段的请求处理效果。例如，vLLM 自带一个[大型指标类](https://oreil.ly/J57gs)，其中包含许多对 LLMs 有用的不同指标。其中一些特别重要的指标是`gauge_gpu_cache_usage`和`gauge_cpu_cache_usage`，它们显示了本章前面提到的键值缓存的使用情况，`num_requests_waiting`显示了等待处理的请求数量，以及`num_requests_running`显示了正在处理的请求数量。所有这些指标都通过`/metrics`端点公开。

vLLM 的指标以及 KServe 公开的指标可以集成到 KServe 中，并在[Prometheus](https://prometheus.io)中进行可视化。在 KServe 中，所有模型服务运行时都能够以 Prometheus 兼容的格式导出指标。

## 预测日志

预测日志在传统机器学习中很重要，在 LLM 中也是如此。有时这也被称为*生成日志*，因为生成的结果会被记录。当与输入提示、输入和输出令牌以及其他生成指标结合时，预测日志成为审计模型使用和准确性的强大工具。

这些可以存储在任何现有的日志存储解决方案中，而像[MLflow](https://oreil.ly/BRA7H)这样的集成解决方案则将预测日志和查看功能整合到其平台中。

## 生产模型注册表

在第三章中，我们讨论了在整个 AI 生命周期中保持单一集中式模型注册表的整体重要性，以及为什么生命周期的前几个阶段需要一个注册表。然而，我们还没有讨论模型注册表为生产带来的好处。

准备部署模型时，了解哪个版本的模型适合生产，以及是否有任何版本正在部署，这一点非常重要。这些信息将存储在模型注册表中，包括每个模型的元数据，如评估结果和超参数，这些信息有助于做出部署决策或配置服务环境。

一旦部署，模型注册表仍然可以提供重要的好处，主要围绕监控和可观察性。注册表可以帮助用户轻松找到模型工件，以跟踪特定部署模型版本的性能指标，了解这些版本的流量模式，获取训练和其他细节以诊断生产中发现的故障，等等。

###### 注意

与模型注册表一样，一些团队开始尝试使用*提示注册表*。虽然这些目前并不广泛可用，但这将是值得关注的创新领域，因为提示注册表可以为提示本身提供许多相同的益处。

不仅部署的模型需要指标监控，还需要内置的安全保障和合规性，以防止滥用。由于 LLM 能够产生令人信服的自然语言，并且与传统 API 相比具有开放的接口，这创造了巨大的攻击面，因此这一点在 LLM 中变得尤为紧迫。这为安全且负责任地提供 LLM 带来了额外的挑战。
