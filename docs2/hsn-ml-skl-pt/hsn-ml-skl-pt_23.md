# 附录 B. 混合精度和量化

默认情况下，PyTorch 使用 32 位浮点数来表示模型参数：每个参数 4 个字节。如果你的模型有 10 亿个参数，那么你至少需要 4GB 的 RAM 来存储模型。在推理时，你还需要足够的 RAM 来存储激活，而在训练时，你需要足够的 RAM 来存储所有中间激活（用于反向传播），以及存储优化器参数（例如，Adam 为每个模型参数需要两个额外的参数——这额外需要 8GB）。这需要大量的 RAM，而且还需要大量的时间在 CPU 和 GPU 之间传输数据，更不用说存储空间、下载时间和能耗了。

那么，我们如何减小模型的大小呢？一个简单的选择是使用降低精度的浮点数表示——通常是 16 位浮点数而不是 32 位浮点数。如果你训练一个 32 位的模型，然后在训练后将其缩小到 16 位，其大小将减半，对质量的影响很小。太棒了！

然而，如果你尝试使用 16 位浮点数来训练模型，你可能会遇到收敛问题，正如我们将看到的。因此，一种常见的策略是*混合精度训练*（MPT），在训练过程中保持权重和权重更新为 32 位精度，但其余的计算使用 16 位精度。在训练后，我们将权重缩小到 16 位。

最后，为了进一步缩小模型的大小，你可以使用*量化*：参数被离散化，并以 8 位整数表示，甚至可以更少，如 4 位整数。这比较困难，并且会稍微降低模型的质量，但它可以将模型大小减少 4 倍以上，并显著提高其速度。

在本附录中，我们将介绍降低精度、混合精度训练和量化。但要完全理解这些概念，我们首先必须讨论机器学习中常见的数字表示方法。

# 常见数字表示方法

默认情况下，PyTorch 根据*IEEE 浮点算术标准*（IEEE 754）使用 32 位浮点数来表示权重和激活，该标准指定了浮点数在内存中的表示方式。它是一种灵活且高效的格式，可以表示极小的值和极大的值，以及特殊的值，如±0、±infinity 和 NaN（即非数字）。

float32 数据类型（简称 fp32）可以存储从±1.4e^(–45)到±3.4e³⁸的数字。它在图 B-1 的顶部表示。第一个比特位确定*符号* *S*：0 表示正数，1 表示负数。接下来的 8 位位表示*指数* *E*，范围从 0 到 255。最后 23 位表示*分数* *F*，范围从 0 到 2²³ – 1。以下是计算值的方法：

+   如果 *E* 在 1 和 254 之间，则该数字称为 *normalized*：这是最常见的情况。在这种情况下，值 *v* 可以通过 *v* = (–1)^(*S*)⋅2^(*E*–127)⋅(1 + *F*⋅2^(–23)) 来计算。最后一个项 (1 + *F*⋅2^(–23)) 对应于最高有效位，因此称为 *significand*。

+   如果 *E* = 0 且 *F* > 0，则该数字称为 *subnormal*：它用于表示极小的值。⁠^(2) 在这种情况下，*v* = (–1)^(*S*)⋅2^(*E*+1–127)⋅(0 + *F*⋅2^(–23)) = (–1)^(*S*)⋅*F*⋅2^(–149)。

+   如果 *E* = 0 且 *F* = 0，则 *v* = ±0。

+   如果 *E* = 255 且 *F* > 0，则 *v* = NaN。

+   如果 *E* = 255 且 *F* = 0，则 *v* = ±infinity。

图 B-1 中表示的其他浮点数格式仅通过用于指数和小数部分的位数不同而有所不同。例如，float16 使用 5 位用于指数（即范围从 0 到 31）和 10 位用于小数（范围从 0 到 1,023），而 float8 使用 4 位用于指数（从 0 到 15）和 3 位用于小数，因此通常表示为 fp8 E4M3。⁠^(3) 计算值的方程相应调整，例如，归一化的 float16 值通过 *v* = (–1)^(*S*)⋅2^(*E*–15)⋅(1 + *F*⋅2^(–10)) 来计算。

![图示机器学习中常见的数字表示，重点关注各种浮点数和整数格式（如 float32、float16、float8 和 int8）的位结构，以及它们的相应范围和组成部分。](img/hmls_ab01.png)

###### 图 B-1\. 机器学习中常见的数字表示

bfloat16 和 bfloat8 格式是由 Google Brain 提出的（因此有 *b*），它们提供了更宽的值范围，但代价是精度显著降低。我们稍后会回到这一点。

整数通常使用 64 位表示，无符号整数的值范围从 0 到 2⁶⁴ – 1（约 1.8e¹⁹），或有符号整数的值范围从 –2³² 到 2³² – 1（约 ±4.3e⁹）。根据用例，整数也经常使用 32 位、16 位或 8 位表示。在 图 B-1 中，我只表示了经常用于量化的整数类型，例如 8 位整数（可以是无符号或带符号）。

当量化到 4 位时，我们通常每字节打包 2 个权重，当量化到 2 位时，我们每字节打包 4 个权重。甚至可以将量化降低到三进制值，其中每个权重只能等于 –1、0 或 +1。在这种情况下，通常每字节存储 5 个权重。例如，字节 178 可以写成 20121（三进制），因为 178 = 2⋅3⁴ + 0⋅3³ + 1⋅3² + 2⋅3¹ + 1⋅3⁰），如果我们从每个数字中减去 1，我们得到 1、–1、0、1、0：这些是存储在这个单一字节中的 5 个三进制权重。由于 3⁵ = 243，小于 256，我们可以将五个三进制值放入一个字节中。这种格式平均每个权重只使用 1.6 位，这比使用 32 位浮点数少 20 倍！

技术上可以将权重量化到每个位只有一位，每字节存储 8 个权重：每个位代表一个等于-1 或+1（有时是 0 或 1）的权重。然而，使用这种极端的量化很难获得合理的精度。

如你所见，PyTorch 的默认权重表示（32 位浮点数）与其他表示相比占用*很多*空间：我们有相当大的空间来缩小我们的模型！让我们从将精度从 32 位降低到 16 位开始。

# 精度降低的模型

如果你有一个 32 位的 PyTorch 模型，你可以通过调用模型的`half()`方法将所有参数转换为 16 位浮点数——这被称为*半精度*。

```py
import torch
import torch.nn as nn

model = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1))
# [...] pretend the 32-bit model is trained here
model.half()  # convert the model parameters to half precision (16 bits)
```

这是一个快速简单的方法，可以将训练模型的尺寸减半，通常对质量的影响不大。此外，由于许多 GPU 具有 16 位浮点优化，并且 CPU 和 GPU 之间的数据传输量将减少，因此模型通常可以运行得快近两倍。

###### 小贴士

当使用 Transformers 库的`from_pretrained()`方法下载预训练模型时，你可以设置`dtype="auto"`以让库为你的硬件选择最佳的浮点表示。

要使用该模型，你现在需要提供 16 位输入，它也将输出 16 位输出：

```py
X = torch.rand(3, 10, dtype=torch.float16)  # some 16-bit input
y_pred = model(X)  # 16-bit output
```

但如果你想在一开始就构建和训练一个 16 位模型呢？在这种情况下，你可以在创建张量或具有参数的模块时设置`dtype=torch.float16`，例如：

```py
model = nn.Sequential(nn.Linear(10, 100, dtype=torch.float16), nn.ReLU(),
                      nn.Linear(100, 1, dtype=torch.float16))
```

###### 小贴士

如果你希望避免在所有地方重复`dtype=torch.float16`，那么你可以通过使用`torch.set_default_dtype(torch.float16)`将默认数据类型设置为`torch.float16`。请注意：这将应用于之后创建的所有张量和模块。

然而，降低精度可能会在训练期间引起一些问题。实际上，16 位浮点数具有有限的*动态范围*（即，最大和最小可表示正值的比率）：最小的可表示正值约为 0.00000006（即，6.0e^(–8)），而最大值为 65,504（即，~6.5e⁴）。这意味着任何小于~6.0e^(–8)的梯度更新将*下溢*，这意味着它将被舍入到零，因此被忽略。相反，任何大于~6.5e⁴的值将*溢出*，这意味着它将被舍入到无穷大，导致训练失败（一旦某些权重变为无穷大，损失将变为无穷大或 NaN）。

为了避免下溢，一种解决方案是将损失放大一个很大的因子（例如，乘以 256）：这将自动在反向传播过程中以相同的因子放大梯度，从而防止它们小于最小的 16 位可表示值。然而，在执行优化器步骤之前，你必须将梯度缩小回原来的规模，这时你可能会遇到下溢。此外，如果你将损失放大得太多，你可能会遇到溢出。

如果你找不到一个既能避免下溢又能避免上溢的合适缩放因子，你可以尝试使用 `torch.bfloat16` 而不是 `torch.float16`，因为 bfloat16 的指数位更多：最小值约为 ~9.2e^(–41)，最大值约为 ~3.4e³⁸，因此不太可能忽略任何显著的梯度更新，或者将合理的值四舍五入到无穷大。

然而，bfloat16 历史上硬件支持较少（尽管情况正在改善），并且它提供的分数位更少，这可能导致当相应的参数值很大时，一些梯度更新被忽略，导致训练停滞。例如，如果梯度更新是 4.5e^(–2)（即 0.045）且相应的参数值等于 1.23e²（即 123），那么总和应该是 1.23045e²（即 123.045），但 bfloat16 没有足够的分数位来存储所有这些数字，因此它必须将结果四舍五入到 1.23e²（即 123）：正如你所看到的，梯度更新被完全忽略。使用常规 16 位浮点数时，结果将是 123.0625，这并不完全正确，因为浮点精度错误，但至少参数向正确的方向迈出了一步。话虽如此，如果梯度更新稍微小一点（例如，0.03），即使在常规 16 位浮点精度下也会被忽略。

因此，如果你尝试了 float16 和 bfloat16 但在训练过程中仍然遇到收敛问题，那么你可以尝试进行 *混合精度训练*。

# 混合精度训练

[*混合精度训练*（MPT）](https://homl.info/mpt) 由百度和英伟达的研究人员在 2017 年提出，以解决通常观察到的 16 位训练问题。以下是它是如何工作的：

+   MPT 存储模型参数的主副本为 32 位浮点数，并在每次训练迭代中创建这些模型参数的 16 位副本（参见图 B-2 中的步骤 1），然后使用它们进行正向传递（步骤 2）。

+   然后，损失被放大一个很大的因子（步骤 3），以避免下溢，正如我们之前讨论的那样。

+   最后，我们切换回 32 位精度以将梯度缩放回原来的大小：这种更高的精度避免了下溢的风险。接下来，我们使用梯度执行一个优化器步骤，改进主参数（步骤 5）。在 32 位精度下执行实际的优化器步骤确保了当应用于较大的参数值时，不会忽略小的权重更新，因为 32 位浮点数有非常大的分数部分（23 位）。

![展示混合精度训练过程的图，包括将 32 位参数复制到 16 位，执行正向和反向传递，缩放损失，以及完成优化器步骤。](img/hmls_ab02.png)

###### 图 B-2\. 混合精度训练

MPT 提供了几乎与 16 位训练相同的所有好处，但没有不稳定性。然而，由于每次训练迭代中的 16 位复制，模型参数比 32 位训练多占用 50%的空间，那么这有什么优势呢？好吧，在训练过程中，大部分 RAM 用于存储激活，而不是模型参数，所以实际上 MPT 只需要比常规 32 位训练多一点点 RAM。而且它通常运行速度要快两倍。此外，一旦训练完成，我们就不再需要 32 位参数，我们可以将它们转换为 16 位，从而得到一个纯 16 位模型。

###### 警告

MPT 并不总是加速训练：这取决于模型、批量大小和硬件。话虽如此，大多数大型 Transformer 都是使用 MPT 进行训练的。

而不是通过试错来找到最佳缩放因子，您可以在 32 位精度下运行训练一段时间（假设您有足够的 RAM）并测量梯度统计信息，以找到适合您任务的最佳缩放因子：它应该足够大以避免下溢，同时足够小以避免上溢。

或者，您的训练脚本可以在训练过程中动态调整因子：如果某些梯度是无穷大或 NaN，这意味着发生了溢出，因此因子必须减少（例如减半），并且必须跳过训练步骤，但如果未检测到溢出，则缩放因子可以逐渐增加（例如，每 2,000 个训练步骤加倍）。PyTorch 提供了一个`torch.amp.GradScaler`类来实现这种方法，并且还会适当地降低学习率。

PyTorch 还提供了一个`torch.autocast()`函数，它返回一个上下文，在该上下文中许多操作将自动以 16 位精度运行。这包括通常从 16 位精度中受益最大的操作，如矩阵乘法和卷积，但不包括像 reductions（例如`torch.sum()`）这样的操作，因为这些操作在半精度下运行不会带来显著的好处，并且可能会损害精度。

让我们更新我们的训练函数，使其在 autocast 上下文中运行前向传递，并使用`GradScaler`动态缩放损失：

```py
from torch.amp import GradScaler

def train_mpt(model, optimizer, criterion, train_loader, n_epochs,
              dtype=torch.float16, init_scale=2.0**16):
    grad_scaler = GradScaler(device=device, init_scale=init_scale)
    model.train()
    for epoch in range(n_epochs):
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            with torch.autocast(device_type=device, dtype=dtype):
                y_pred = model(X_batch)
                loss = criterion(y_pred, y_batch)
            grad_scaler.scale(loss).backward()
            grad_scaler.step(optimizer)
            grad_scaler.update()
            optimizer.zero_grad()
```

###### 小贴士

当使用 Hugging Face Transformers 库微调 Transformer 时，您可以在`TrainingArguments`中将`fp16=True`或`bf16=True`设置为激活混合精度训练。

将精度降低到 16 位通常效果很好，但我们能否进一步缩小我们的模型？是的，我们可以，使用量化。

# 量化

量化意味着将连续值映射到离散值。在深度学习中，这通常涉及将参数，以及通常激活，从浮点数转换为整数——通常是 32 位浮点数到 8 位整数。更普遍地说，目标是通过减少参数（以及通常激活）中使用的位数来缩小和加速我们的模型。此外，一些嵌入式设备（例如，ARM Cortex-M0）根本不支持浮点运算（部分是为了降低其成本和能耗），因此模型必须在设备上使用之前完全量化（包括权重和激活）。现代智能手机支持浮点运算，但仍然可以从量化中受益良多：int8 运算比 FP32 快 2 到 4 倍，并且使用的能量少 5 到 10 倍。

最简单的方法是 *线性量化*，因此我们现在将讨论它。我们将在本附录的后面讨论几种非线性量化方法。

## 线性量化

线性量化可以追溯到 20 世纪 50 年代的数字信号处理，但自从模型变得巨大，而我们又希望在手机和其他有限设备上运行它们以来，在过去十年中它在机器学习中变得尤为重要。它有两个变体：非对称和对称。在 *非对称线性量化* 中，浮点值简单地线性映射到无符号字节，其值范围从 0 到 255（或者更普遍地，当量化到 *n*-位整数时，从 0 到 2^(*n*) – 1）。例如，如果权重范围在 *a* = –0.1 和 *b* = 0.6 之间，那么浮点数 –0.1 将映射到字节 0，浮点数 0.0 映射到整数 36，0.1 映射到 72，……，0.6 映射到 255，并且更普遍地，浮点张量 **w** 将使用 方程 B-1 映射到整数张量 **q**。

##### 方程 B-1\. 非对称线性量化

$StartLayout 1st Row 1st Column q Subscript i 2nd Column equals round left-parenthesis StartFraction w Subscript i Baseline Over s EndFraction right-parenthesis plus z 2nd Row 1st Column with s 2nd Column equals StartFraction b minus a Over 2 Superscript n Baseline minus 1 EndFraction and z equals minus round left-parenthesis StartFraction a Over s EndFraction right-parenthesis 3rd Row 1st Column where a 2nd Column equals min Underscript i Endscripts w Subscript i Baseline and b equals max Underscript i Endscripts w Subscript i Baseline EndLayout$

在这个方程中：

+   *w*[*i*] 是原始张量 **w** 中的第 *i* 个浮点数。

+   *q*[i] 是量化张量 **q** 中的第 *i* 个整数。它在 0 和 2^(*n*) – 1 之间夹紧（例如，对于 8 位量化，为 255）。

+   *s* 是 *量化尺度*。请注意，一些作者将其定义为 1 / *s* 并相应地调整方程（即，他们乘以而不是除以）。

+   *z* 是 *量化偏差* 或 *零点*。

+   *a* 是 **w** 的最小值，*b* 是 **w** 的最大值。

对于权重，[*a*, *b*] 的范围是已知的，因为它们的值在训练后不会改变。然而，激活值的范围取决于我们输入到模型中的输入。因此，对于我们要量化的每个激活（例如，每层的输入），我们可能必须为每个新的输入批次动态计算 *a* 和 *b*（这称为动态量化）或者运行一个校准数据集一次通过模型来确定激活值的典型范围，然后使用这个范围来量化所有后续批次的激活（这称为静态量化）。静态量化更快但精度较低。

为了从量化值 *q*[*i*] 近似恢复原始值 *w*[*i*]，我们可以计算 *w*[*i*] ≈ s × (*q*[*i*] – *z*)。这被称为反量化。例如，如果 *q*[*i*] = 72，那么我们得到 *w*[*i*] ≈ 0.0988，这确实接近 0.1。反量化值（0.0988）与原始值（0.1）之间的差异称为量化噪声：在 8 位量化中，量化噪声通常会导致略微降低的精度。在 6 位、4 位或更少的情况下，量化噪声的伤害可能更大，尤其是因为它具有累积效应：网络越深，影响越强。

###### 注意

方程 B-1 保证任何等于 0.0 的浮点数都可以精确量化并反量化回 0.0：确实，如果 *w*[*i*] = 0.0，则 *q*[*i*] = *z*，反量化 *q*[*i*] 会返回 *w*[*i*] = *s* × (*z* – *z*) = 0.0。这对于稀疏模型特别有用，其中许多权重等于零。当使用如 ReLU 这样的激活函数时，它会产生许多零激活，这也非常重要。

在 PyTorch 中，`torch.quantize_per_tensor()` 函数允许你创建一个量化张量：这是一种特殊的张量，它包含量化值（即整数），以及量化参数（即缩放和零点）。让我们使用这个函数来量化一个张量，然后进行反量化。在这个例子中，我们将使用数据类型 `torch.quint8`，它使用 8 位无符号整数：

```py
>>> w = torch.tensor([0.1, -0.1, 0.6, 0.0])  # 32-bit floats `>>>` `s` `=` `(``w``.``max``()` `-` `w``.``min``())` `/` `255.`  `# compute the scale` ``````py `>>>` `z` `=` `-``(``w``.``min``()` `/` `s``)``.``round``()`  `# compute the zero point` ````` `>>>` `qw` `=` `torch``.``quantize_per_tensor``(``w``,` `scale``=``s``,` `zero_point``=``z``,` `dtype``=``torch``.``quint8``)` ```py` `>>>` `qw`  `# this is a quantized tensor internally represented using integers` ``` `tensor([ 0.0988, -0.0988,  0.6012,  0.0000], size=(4,), dtype=torch.quint8,`  `quantization_scheme=torch.per_tensor_affine, scale=0.002745098201557994,`  `zero_point=36)` `>>>` `qw``.``dequantize``()`  `# 返回 32 位浮点数（接近原始张量)` `` `tensor([ 0.0988, -0.0988,  0.6012,  0.0000])` `` ```py ```` ```py`` ``````py
```

```py```` ```py``` ````` Quantizing a model to 8-bits divides its size by almost 4\. For example, suppose we have a convolutional layer with 64 kernels, 3 × 3 each, and it has 32 input channels. This layer requires 64 × 32 × 3 × 3 = 18,432 parameters (ignoring the bias terms). That’s 18,432 × 4 = 73,728 bytes before quantization, and just 18,432 bytes after quantization, plus 2 × 4 = 8 bytes to store *s* and *z* (indeed, they are both stored as 32-bit floats, so 4 bytes each).    ###### Tip    PyTorch also has a `torch.quantize_per_channel()` function which quantizes each channel separately: this offers better precision but requires a bit more space for the additional quantization parameters.    When the float values are approximately symmetric around zero, we can use *symmetric linear quantization*, where the values are mapped between –127 and +127, or more generally between –*r* and +*r* with *r* = 2^(*n*–1) – 1, using Equation B-2.    ##### Equation B-2\. Symmetric linear quantization  $q Subscript i Baseline equals round left-parenthesis StartFraction w Subscript i Baseline Over s EndFraction right-parenthesis with s equals StartFraction max Underscript i Endscripts StartAbsoluteValue w Subscript i Baseline EndAbsoluteValue Over 2 Superscript n minus 1 Baseline minus 1 EndFraction$  To implement symmetric linear quantization in PyTorch, we can use the `torch.quantize_per_tensor()` function again, but using a zero point equal to 0, and data type `qint8` (quantized signed 8-bit integer):    ```py >>> w = torch.tensor([0.0, -0.94, 0.92, 0.93])  # 32-bit floats `>>>` `s` `=` `w``.``abs``()``.``max``()` `/` `127.` ```` `>>>` `qw` `=` `torch``.``quantize_per_tensor``(``w``,` `scale``=``s``,` `zero_point``=``0``,` `dtype``=``torch``.``qint8``)` ```py `>>>` `qw` `` `tensor([ 0.0000, -0.9400,  0.9178,  0.9326], size=(4,), dtype=torch.qint8,`  `quantization_scheme=torch.per_tensor_affine, scale=0.007401574868708849,`  `zero_point=0)` `` ``` ```py` ```   ```py` ``` ``Figure B-3 shows some floats ranging between –0.94 and +0.93, quantized to signed bytes (i.e., 8-bits) ranging between –127 and +127,⁠^(5) using symmetric linear quantization. Notice that float 0.0 is always mapped to integer 0.  ![Diagram showing the mapping of weights from floats between -0.94 and 0.93 to quantized bytes ranging from -127 to 127 using symmetric linear quantization.](img/hmls_ab03.png)  ###### Figure B-3\. Symmetric linear quantization    Symmetric mode is often a bit faster than asymmetric mode, because there’s no zero point *z* to worry about. However, if the values are not symmetric, part of the integer range will be wasted. For example, if all the weights are positive, then symmetric mode will only use bytes 0 to 127 (rather than –127 to 127). As a result, symmetric mode can be a bit less precise than asymmetric mode. In practice, symmetric mode is generally preferred for weights (which are often fairly symmetric), and asymmetric mode for activations (especially when using ReLU, since it outputs only nonnegative values).    Let’s now see how to quantize your models in practice using PyTorch’s `torch.​ao.quantization` package. The first approach is to quantize a trained model, which is called *post-training quantization* (PTQ). The second is to train (or fine-tune) your model with some fake quantization to get it used to the noise: this is called *quantization-aware training* (QAT). Let’s start with PTQ.`` ```py ```` ```py`` ``````py ``````py`  ``````py ````` ```py`## Post-Training Quantization Using torch.ao.quantization    The `torch.ao` package contains tools for architecture optimization (hence the name), including pruning, sparsity, and quantization. The `torch.ao.quantization` package offers two solutions to quantize trained models: dynamic quantization and static quantization. Let’s see how to implement both.    ### Dynamic quantization    Dynamic quantization is best for MLPs, RNNs, and transformers. To implement it using PyTorch’s `torch.ao.quantization` package, you must first choose a quantization engine: PyTorch currently supports the *Facebook General Matrix Multiplication* (FBGEMM) engine for x86 CPUs, plus a newer x86 engine that supports recent x86 CPUs but is less battle-tested, and finally the *Quantized Neural Networks Package* (QNNPACK) engine for ARM/mobile. This code will pick the appropriate engine depending on the platform:    ``` import platform  machine = platform.machine().lower() engine = "qnnpack" if ("arm" in machine or "aarch64" in machine) else "x86" ```py    ###### Warning    PyTorch does not offer an engine for CUDA or other hardware accelerators, but other libraries do, such as the bitsandbytes library (as we will see shortly).    Once you have selected an engine, you can use the `quantize_dynamic()` function from the `torch.ao.quantization` package; just pass it your trained model, tell it the types of layers to quantize (typically just the `Linear` and RNN layers), specify the quantized data type, and boom, you have a ready-to-use quantized model:    ``` from torch.ao.quantization import quantize_dynamic  model = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1)) # [...] pretend the 32-bit model is trained here torch.backends.quantized.engine = engine qmodel = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8) X = torch.randn(3, 10) y_pred = qmodel(X)  # float inputs and outputs, but quantized internally ```py    The `quantize_dynamic()` function replaces each `Linear` layer with a `DynamicQuantizedLinear` layer, with int8 weights. This layer behaves just like a regular linear layer, with float inputs and outputs, but it quantizes its inputs on the fly (recomputing the zero points and scales for each batch), performs matrix multiplication using integers only (with 32-bit integer accumulators), and dequantizes the result so the next layer gets float inputs. Now let’s look at static quantization.    ### Static quantization    This option is best for CNNs, and max inference speed. It’s also compulsory for edge devices without a *floating-point unit* (FPU), as they don’t support floats at all. Both the weights and activations are prepared for quantization ahead of time, for all layers. As we discussed earlier, weights are constant so they can be quantized once, while activations require a calibration step to determine their typical range. The model is then converted to a fully quantized model. Here is how to implement it:    ``` from torch.ao.quantization import get_default_qconfig, QuantStub, DeQuantStub  model = nn.Sequential(QuantStub(),                       nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1),                       DeQuantStub()) # [...] pretend the 32-bit model is trained here model.qconfig = get_default_qconfig(engine) torch.ao.quantization.prepare(model, inplace=True) for X_batch, _ in calibration_loader:     model(X_batch) torch.ao.quantization.convert(model, inplace=True) ```py    Let’s go through this code step by step:    *   After the imports, we create our 32-bit model, but this time we add a `QuantStub` layer as the first layer, and a `DeQuantStub` layer as the last. Both layers are just passthrough for now.           *   Next, the model can be trained normally (another option would be to take a pretrained model and place it between a `QuantStub` layer and a `DeQuantStub` layer).           *   Next, we set the model’s `qconfig` to the output of the `get_default_qconfig()` function: this function takes the name of the desired quantization engine and returns a `QConfig` object containing a default quantization configuration for this engine. It specifies the quantization data type (e.g., `torch.qint8`), the quantization scheme (e.g., symmetric linear quantization per tensor), and two functions that will observe the weights and activations to determine their ranges.           *   Next we call the `torch.ao.quantization.prepare()` function: it uses the weight observer specified in the configuration to determine the weights range, which it immediately uses to compute the zero points and scales for the weights. Since we don’t know what the input data looks like at this point, the function cannot compute the quantization parameters for the activations yet, so it inserts activation observers in the model itself: these are attached to the outputs of the `QuantStub` and `Linear` layers. The observer appended to the `QuantStub` layer is responsible for tracking the input range.           *   Next, we take a representative sample of input batches (i.e., the kind the model will get in production), and we pass these batches through the model: this allows the activation observers to track the activations.           *   Once we have given the model enough data, we finally call the `torch.ao.​quanti⁠zation.convert()` function, which removes the observers from the model and replaces the layers with quantized versions. The `QuantStub` layer is replaced with a `Quantize` layer which will quantize the inputs. The `Linear` layers are replaced with `QuantizedLinear` layers. And the `DeQuantStub` layer is replaced with a `DeQuantize` layer which will dequantize the outputs.              ###### Note    There are a few observers to choose from: they can just keep track of the minimum and maximum values for each tensor (`MinMaxObserver`), or for each channel (`PerChannelMinMaxObserver`), or they can compute an exponential moving average of the min/max values, which reduces the impact of a few outliers. Finally, they can even record a histogram of the observed values (`HistogramObserver`), making it possible to find an optimal quantization range that minimizes the quantization error. That said, the default observers are usually fine.    We now have a model that we can use normally, with float inputs and outputs, but which works entirely with integers internally, making it lightweight and fast. To deploy it to mobile or embedded devices, there are many options to choose from (which are beyond the scope of this book), including:    *   Use ExecuTorch, which is PyTorch’s lightweight edge runtime           *   Export the model to ONNX and run it with ONNX Runtime (cross-platform)           *   Convert it to TFLite or TFLite Micro           *   Compile it for the target device using TVM or microTVM              Moreover, the PyTorch team has released a separate library named [*PyTorch-native Architecture Optimization* (TorchAO)](https://homl.info/torchao), designed to be a robust and extensible model optimization framework. Over time, many features in PyTorch’s `torch.ao` package are expected to be migrated to—or superseded by—TorchAO. The library already includes advanced features such as 4-bit weight support and *per-block quantization*, in which each tensor is split into small blocks and each block is quantized independently, trading space for improved precision.    Post-training quantization (either dynamic or static) can shrink and speed up your models significantly, but it will also degrade their accuracy. This is particularly the case when quantizing down to 4 bits or less, and it’s worse for static quantization than for dynamic quantization (which can at least adapt to each input batch independently). When the accuracy drop is unacceptable, you can try quantization-aware training, as we will discuss now.    ## Quantization-Aware Training (QAT)    QAT was introduced in a [2017 paper](https://homl.info/qat) by Google researchers.⁠^(6) It rests upon a simple idea: why not introduce some fake quantization noise during training so the model can learn to cope with it? After training, we can then quantize the model for real, and it should remain fairly accurate. QAT also makes it possible to quantize more aggressively without losing too much accuracy, down to 4 bits, or even less. Sound promising? Let’s see how it can be done.    To add fake quantization noise to weights, we can simply quantize them and immediately dequantize them. For example, a weight equal to 0.42 might be quantized to the 4-bit integer 7, and immediately dequantized back to 0.39: we’ve successfully introduced quantization noise, and it’s precisely the quantization noise that we would get if the model were really quantized. This fake quantization operation can be executed at each training step, and it can also be applied to some of the activations (e.g., to each layer output).    However, there is one little problem: quantization involves rounding to the nearest integer, and the rounding operation has gradients equal to zero (or undefined at integer boundaries), so gradient descent cannot make any progress. Luckily, we can sidestep this issue by using the *straight-through estimator* (STE) trick: during the backward phase, we pretend that the fake quantization operation was just the identity function, so the gradients flow straight through it untouched. This works because the loss landscape is generally fairly smooth locally, so gradients are likely to be similar within a small region around the quantized value, including at the original value.    Implementing QAT in PyTorch is fairly straightforward:    ``` from torch.ao.quantization import get_default_qat_qconfig  model = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1)) model.qconfig = get_default_qat_qconfig(engine) torch.ao.quantization.prepare_qat(model, inplace=True) train(model, optimizer, [...])  # train the model normally torch.ao.quantization.convert(model.eval(), inplace=True) ```py    After the import, we create our model, set its `qconfig` attribute to the default QAT configuration object for the chosen quantization engine, then we call the `prepare_qat()` function to add fake quantization operations to the model. This step also adds observers to determine the usual range of activation values. Next, we can train the model normally. Lastly, we switch the model to eval mode, and we call the `convert()` function to truly quantize it.    ###### Tip    QAT doesn’t have to be used during all of training: you can take a pretrained model and just fine-tune it for a few epochs using QAT, typically using a lower learning rate to avoid damaging the pretrained weights.    We’ve seen how to implement PTQ and QAT using PyTorch’s `torch.ao` package. However, it’s primarily designed for CPUs. What if you want to run an LLM on a GPU that doesn’t quite have enough RAM? One option is to use the TorchAO library, which has growing GPU support. Another is to use the bitsandbytes library: let’s discuss it now.    ## Quantizing LLMs Using the bitsandbytes Library    The bitsandbytes library (bnb), created by Tim Dettmers, is designed to make it easier to train and run large models on GPUs with limited VRAM. For this, it offers:    *   Quantization tools, including 4-bit quantization, block-wise quantization, and more           *   Memory-efficient versions of popular optimizers such as Adam or AdamW, that operate on 8-bit tensors           *   Custom CUDA kernels written specifically for 8-bit or 4-bit quantized models, for maximum speed              ###### Warning    The bitsandbytes library is designed for Nvidia GPUs. It also has some limited support for CPUs and AMD GPUs.    For example, let’s see how to implement post-training static quantization down to 4 bits. If you are using Colab, you must first install the bitsandbytes library using `%pip install bitsandbytes`, then run this code:    ``` from transformers import AutoModelForCausalLM, BitsAndBytesConfig  model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0" bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4",                                 bnb_4bit_compute_dtype=torch.bfloat16) model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto",                                              quantization_config=bnb_config) ```py    This code starts by importing the necessary classes from the Transformers library (introduced in Chapter 14), then it creates a `BitsAndBytesConfig` object, which I will explain shortly. Lastly, it downloads a pretrained model (in this case a 1.1 billion parameter version of Llama named TinyLlama, fine-tuned for chat), specifying the desired quantization configuration.    Under the hood, the Transformers library uses the bitsandbytes library to quantize the model weights down to 4 bits just as they are loaded into the GPU: no extra step is required. You can now use this model normally to generate text (see Chapter 15). During inference, whenever some weights are needed, they are dequantized on the fly to the type specified by the `bnb_4bit_compute_dtype` argument (`bfloat16` in this case), and the computations are performed in this higher precision. As soon as the dequantized weights are no longer needed, they are dropped, so memory usage remains low.    In this example, the `BitsAndBytesConfig` object specifies *4-bit Normal Float* (NF4) quantization using `bfloat16` for computations. NF4 is a nonlinear 4-bit scheme where each of the 16 possible integer values represents a specific float value between –1 and +1\. Instead of being equally spaced (as in linear quantization), these values correspond to the quantiles of the normal distribution centered on zero: this means that they are closer together near zero. This improves accuracy because model weights tend to follow a normal distribution centered on zero, so having more precision near zero is helpful.    NF4 was introduced as part of [QLoRA](https://homl.info/qlora),⁠^(7) a technique that quantizes a frozen pretrained model with NF4, then uses LoRA adapters (see Chapter 17) for fine-tuning, along with activation checkpointing (see Chapter 12). This approach drastically reduces VRAM usage and compute: the authors managed to fine-tune a 65-billion parameter model using a single GPU with 48 GB of RAM, with only a small accuracy drop. Although activation checkpointing reduces VRAM usage overall, it can lead to memory spikes when processing batches with long sequences. To deal with such spikes, the QLoRA authors also introduced *paged optimizers* which take advantage of Nvidia unified memory: the CUDA driver automatically moves pages of data from GPU VRAM to CPU RAM whenever needed. Lastly, the authors also used *double quantization*, meaning that the quantization parameters themselves were quantized to save a bit more VRAM.    For more details on 4-bit quantization in the Hugging Face ecosystem, check out this [great post by the QLoRA authors and other contributors](https://huggingface.co/blog/4bit-transformers-bitsandbytes).```` ```py`` ``````py  ```` ```py ``# Using Pre-Quantized Models    Many popular pretrained models have already been quantized and published online, in particular on the Hugging Face Hub. For example, Tom Jobbins, better known by his Hugging Face username TheBloke, has published thousands of quantized models available at [*https://huggingface.co/TheBloke*](https://huggingface.co/TheBloke). Many of these models were quantized using one of the following modern methods:    *Generative pre-training quantization* (GPTQ)      [GPTQ](https://homl.info/gptq)⁠^(8) is a post-training quantization method, usually down to 4 bits, that treats quantization as an optimization problem. GPTQ goes through each layer, one by one, and optimizes the 4-bit weights to minimize the MSE between the layer’s original outputs (i.e., using the full precision weights) and the approximate outputs (i.e., using the 4-bit weights). Once the optimal 4-bit weights are found, the approximate outputs are passed to the next layer, and the process is repeated all the way to the output layer. During inference, the weights are dequantized whenever they are needed. GPTQ only quantizes the weights, not the activations: this is called *weight-only quantization*, which is great for inference, not for training. You can use the [Hugging Face Optimum library](https://huggingface.co/docs/optimum) or the [GPTQModel library](https://github.com/ModelCloud/GPTQModel) to quantize your models with GPTQ.      *Activation-aware Weight Quantization* (AWQ)      [AWQ](https://homl.info/awq)⁠^(9) aims to improve the accuracy of block-wise weight-only quantization (typically 4-bit quantization). The idea is to preserve the precision of the most important weights. To identify these so-called *salient weights*, the algorithm runs a calibration dataset through the model and finds the largest activations for each quantization group (e.g., the largest 0.1% to 1% activations), and the corresponding weights are considered salient. The authors observed that storing the salient weights using float16 greatly reduces the model’s *perplexity* (a common metric equal to the exponential of the cross-entropy). However, mixing 4-bit and 16-bit weights is not hardware-friendly, so AWQ uses another method to preserve the salient weight’s precision: they simply scale them up by some factor and add an operation in the model to scale down the corresponding activations (but this operation can generally be fused into the previous operation). Rather than using a fixed scaling factor, AWQ performs a search for the optimal factor, leading to the lowest quantization error. To implement AWQ, you can use the Hugging Face Optimum library.      Llama.cpp quantization using the *GPT-Generated Unified Format* (GGUF)      [GGUF](https://homl.info/gguf) is a binary file format designed to store LLMs efficiently. It was introduced by Georgi Gerganov, the creator of llama.cpp, and it supersedes previous file formats such as GGML, GGMF, and GGJT. A GGUF file includes the weights, the tokenizer, special tokens, the model architecture, the vocabulary size, and other metadata. Llama.cpp offers quantizers (e.g., using the `quantize` tool) to convert the model weights to one of GGUF’s supported quantized formats, such as Q4_K_M. Q4 stands for 4-bit quantization, K stands for per-block quantization (typically 32 or 64 weights per block depending on the chosen format), and M means medium size and precision for this quantization level (other options are S = Small and L = Large). There are also more recent and efficient quantization options such as Importance-aware Quantization (IQ), which uses various techniques to improve accuracy (e.g., nonlinear quantization), and Ternary Quantization (TQ).      ###### Note    On the Hugging Face Hub, every repository is backed by Git, so it has branches and commits. When you call `from_pretrained()`, the model is fetched from the default branch, which is almost always `main`. But quantized models are often placed in a different branch. When calling `from_pretrained()`, you can choose a branch, a tag, or even a commit hash, by using the `revision` argument. Check the model card for the list of available files and versions. For GGUF models, you must specify the filename using the `gguf_file` argument.    In conclusion, reduced precision, mixed-precision training, and quantization are arguably the most important tools to allow large models to run on limited hardware. But there are many more, including the following:    *   You could tweak the model’s architecture before training, by reducing the number of layers, or the number of neurons per layer, or by sharing weights across layers (e.g., as in the ALBERT model, introduced in Chapter 15).           *   If you have a large trained model, you can shrink it by removing some of its weights, for example the ones with the smallest magnitude, or the ones with the smallest effect on the loss. You can also remove whole channels, layers, or attention heads. This is called *model pruning*, and you can implement it using the `torch.nn.utils.prune` module, or the Hugging Face Optimum library.           *   As we saw in Chapter 15, you can also use a large trained model as a teacher to train a smaller model: this is called distillation.           *   A trained model can also be shrunk by fusing some of its layers, removing redundancy. For example, a batch-norm layer (introduced in Chapter 11) performs a linear operation, so if it comes immediately after a linear layer, you can fuse both layers into a single linear layer. Similarly, you can fuse a convolutional layer followed by a batch-norm layer into a single convolutional layer. This only works after training, since the batch-norm layer must compute running averages during training. You can implement layer fusion with the `torch.quantization.fuse_modules()` function, or with the Hugging Face Optimum library. In any case, make sure to fuse layers *before* quantizing your model: less layers means less quantization noise.           *   You can use low-rank approximations, where a large matrix is replaced by the product of two smaller ones. For example, replace a large linear layer such as `Linear(10_000, 20_000)` with two linear layers `Linear(10_000, 100)` and `Linear(100, 20_000)`. This reduces the number of parameters from about 200 million down to just three million, and also drastically reduces computations. The intermediate dimensionality (100 in this example) is a hyperparameter you can tune to balance accuracy and model size. This technique can be performed after training by factorizing the weight matrix using SVD (see the notebook for an example).              Give these techniques a try: shrink the models!    ###### Note    Chapter 17 and Appendices C, D, and E are available online at [*https://homl.info*](https://homl.info).    ^(1) In general, –0 and +0 are considered equal, but some operations give different results, for example 1 / –0 = –infinity, while 1 / +0 = +infinity.    ^(2) Some high-performance computing applications deactivate subnormal numbers because they slow down computations, and normalized numbers are generally sufficient (e.g., normalized fp32 can represent numbers as small as ±1.2e^(–38)).    ^(3) The *M* stands for *mantissa*, which is a term often used as a synonym for fraction. Unfortunately, it’s also used as a synonym for significand, leading to some confusion. This is why IEEE 754 no longer uses the term mantissa.    ^(4) P. Micikevicius et al., “Mixed Precision Training”, arXiv preprint 2017, ICLR (2018).    ^(5) PyTorch implements *restricted symmetric quantization*, meaning that it excludes the lowest possible signed integer (e.g., –128 for 8-bit integers) to ensure that the range is symmetric (e.g., –127 to +127). Some other implementations allow the full signed byte range (from –128 to +127): this is called *unrestricted symmetric quantization*. These implementations also subtract 0.5 instead of 1 in the denominator of Equation B-2.    ^(6) Benoit Jacob et al., “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference”, arXiv preprint arXiv:1712.05877 (2017)”.    ^(7) Tim Dettmers et al., “QLORA: Efficient Finetuning of Quantized LLMs”, arXiv preprint arXiv:2305.14314 (2023).    ^(8) Elias Frantar et al., “GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers”, arXiv preprint arXiv:2210.17323 (2022).    ^(9) Ji Lin et al., “AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration”, arXiv preprint arXiv:2306.00978 (2023).`` ``` ````
