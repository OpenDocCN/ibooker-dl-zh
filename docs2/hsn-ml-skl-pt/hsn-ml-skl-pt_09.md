# 第八章。无监督学习技术

图灵奖获得者、Meta 的首席人工智能科学家 Yann LeCun 曾著名地说：“如果智能是一块蛋糕，无监督学习就是蛋糕本身，监督学习就是蛋糕上的糖霜，强化学习就是蛋糕上的樱桃”（NeurIPS 2016）。换句话说，无监督学习具有巨大的潜力，我们才刚刚开始涉足。确实，大部分可用的数据都是未标记的：我们拥有输入特征**X**，但没有标签**y**。

假设你想创建一个系统，该系统可以自动拍摄制造生产线上的每个物品的几张图片，并检测哪些物品是次品。你可以相当容易地创建一个自动拍摄图片的系统，这可能会每天给你成千上万张图片。然后你可以在几周内构建一个相当大的数据集。但是等等，没有标签！如果你想训练一个常规的二分类器来预测一个物品是否是次品，你需要将每张图片都标记为“次品”或“正常”。这通常需要人类专家坐下来手动检查所有图片。这是一项漫长、昂贵且繁琐的任务，因此通常只会对可用图片的一小部分进行操作。结果，标记的数据集将非常小，分类器的性能将令人失望。此外，每次公司对其产品进行任何更改时，整个流程都需要从头开始。如果算法能够仅利用未标记的数据而不需要人类为每张图片标记，那岂不是很好？这就是无监督学习的用武之地。

在第七章中，我们探讨了最常见的无监督学习任务：降维。在本章中，我们将探讨更多无监督任务：

聚类

目标是将相似的实例分组到*簇*中。聚类是数据分析、客户细分、推荐系统、搜索引擎、图像分割、半监督学习、降维等领域的强大工具。

异常检测（也称为*离群值检测*）

目标是学习“正常”数据看起来是什么样子，然后使用这些知识来检测异常实例。这些实例被称为*异常*或*离群值*，而正常实例被称为*内群值*。异常检测在广泛的领域中都有用，例如欺诈检测、在制造中检测次品、识别时间序列中的新趋势，或在训练另一个模型之前从数据集中移除离群值，这可以显著提高最终模型的表现。

密度估计

这是一项估计生成数据集的随机过程的*概率密度函数*（PDF）的任务。⁠^(1) 密度估计通常用于异常检测：位于非常低密度区域的实例很可能是异常。它也适用于数据分析和可视化。

准备好享受一些蛋糕了吗？我们将从两种聚类算法开始，*k*-均值和 DBSCAN，然后我们将讨论高斯混合模型，并看看它们如何被用于密度估计、聚类和异常检测。

# 聚类算法：k-均值和 DBSCAN

当你在山中徒步时，你发现了一种你以前从未见过的植物。你四处看了看，注意到还有几株。它们并不完全相同，但足够相似，让你知道它们很可能属于同一物种（或者至少同一属）。你可能需要一个植物学家告诉你这是什么物种，但你当然不需要专家来识别外观相似的物体群体。这被称为*聚类*：它是识别相似实例并将它们分配到*簇*或相似实例群体的任务。

就像在分类中一样，每个实例都被分配到一个组。然而，与分类不同，聚类是一个无监督的任务，没有标签，因此算法需要自己找出如何分组实例。考虑图 8-1：左边的图是鸢尾花数据集（在第四章线性模型中介绍），其中每个实例的物种（即其类别）用不同的标记表示。这是一个带标签的数据集，对于逻辑回归、SVM 或随机森林分类器等分类算法非常适合。右边的图是相同的数据集，但没有标签，因此不能再使用分类算法。这就是聚类算法介入的地方：许多算法可以轻松地检测到左下角的簇。用我们的肉眼也很容易看出，但右上角的簇由两个不同的子簇组成这一点并不明显。尽管如此，数据集还有两个额外的特征（花瓣长度和宽度）在这里没有表示，聚类算法可以利用所有特征，因此实际上它们相当好地识别了三个簇（例如，使用高斯混合模型，只有 150 个实例中的 5 个被分配到错误的簇）。

![比较分类（左）与聚类（右）在鸢尾花数据集上的图表，突出聚类如何在不带先验标签的情况下识别群体](img/hmls_0801.png)

###### 图 8-1. 分类（左）与聚类（右）：在聚类中，数据集未标记，因此算法必须在没有指导的情况下识别簇

聚类在众多应用中被广泛使用，包括：

客户细分

你可以根据客户的购买和他们在你网站上的活动来聚类你的客户。这有助于了解你的客户是谁以及他们需要什么，因此你可以根据每个细分市场调整你的产品和营销活动。例如，客户细分在 *推荐系统* 中很有用，可以建议其他同一聚类中的用户喜欢的相关内容。

数据分析

当你分析一个新的数据集时，运行聚类算法并分别分析每个聚类可能会有所帮助。

维度约简

一旦数据集被聚类，通常可以测量每个实例与每个聚类的 *亲和度*；亲和度是衡量实例如何适合聚类的任何度量。然后，每个实例的特征向量 **x** 可以用其聚类亲和度的向量替换。如果有 *k* 个聚类，那么这个向量是 *k*-维的。新向量通常比原始特征向量低得多维，但它可以保留足够的信息以供进一步处理。

特征工程

聚类亲和度通常可以作为额外的特征很有用。例如，我们在第二章中使用了 *k*-means 来向加利福尼亚住房数据集添加地理聚类亲和度特征，并且这有助于我们获得更好的性能。

*异常检测*（也称为 *离群值检测*）

任何与所有聚类亲和度低的实例很可能是一个异常。例如，如果你根据用户的行为将你的网站用户进行了聚类，你可以检测到具有不寻常行为的用户，例如每秒请求的数量异常。

半监督学习

如果你只有少数标签，你可以执行聚类并将标签传播到同一聚类的所有实例。这种技术可以大大增加后续监督学习算法可用的标签数量，从而提高其性能。

搜索引擎

一些搜索引擎允许你搜索与参考图像相似的图像。要构建这样的系统，你首先需要将聚类算法应用于数据库中的所有图像；相似的图像最终会进入同一个聚类。然后，当用户提供一个参考图像时，你所需要做的就是使用训练好的聚类模型找到这个图像的聚类，然后你可以简单地返回这个聚类中的所有图像。

图像分割

通过根据颜色对像素进行聚类，然后用其聚类的平均颜色替换每个像素的颜色，可以显著减少图像中的不同颜色数量。图像分割在许多目标检测和跟踪系统中使用，因为它使得检测每个对象的轮廓变得更容易。

对于什么是团块，并没有一个普遍的定义：这实际上取决于上下文，不同的算法会捕捉到不同类型的团块。一些算法寻找围绕特定点的实例，称为*质心*。其他算法寻找密集实例的连续区域：这些团块可以呈现任何形状。一些算法是层次化的，寻找团块的团块。等等。

在本节中，我们将探讨两种流行的聚类算法，*k*均值和 DBSCAN，并探索它们的一些应用，例如非线性降维、半监督学习和异常检测。

## k-Means 聚类

考虑到图 8-2 中展示的无标签数据集：你可以清楚地看到五个实例的团块。*k*均值算法是一种简单的算法，能够非常快速且高效地对这类数据集进行聚类，通常只需几轮迭代即可。它在 1957 年由贝尔实验室的 Stuart Lloyd 提出，作为一种脉冲编码调制的技巧，但直到 1982 年才在公司外部[发表](https://homl.info/36)。⁠^(2) 1965 年，Edward W. Forgy 发表了几乎相同的算法，因此*k*均值有时被称为 Lloyd-Forgy 算法。

![展示五个不同数据点团块的散点图，说明适合*k*均值聚类的无标签数据集。](img/hmls_0802.png)

###### 图 8-2\. 由五个实例团块组成的无标签数据集

让我们在该数据集上训练一个*k*均值聚类器。它将尝试找到每个团块的中心，并将每个实例分配到最近的团块：

```py
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

X, y = make_blobs([...])  # make the blobs: y contains the cluster IDs, but we
                          # will not use them; that's what we want to predict
k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)
```

注意，你必须指定算法必须找到的团块数量*k*。在这个例子中，从数据中观察可以很明显地看出*k*应该设置为 5，但通常并不那么容易。我们将在稍后讨论这个问题。

每个实例将被分配到五个团块中的某一个。在聚类的上下文中，一个实例的*标签*是算法分配给该实例的团块的索引；这不要与分类中的类标签混淆，类标签用作目标（记住聚类是一个无监督学习任务）。`KMeans`实例保留了它在训练过程中预测的实例标签，这些标签可通过`labels_`实例变量获取：

```py
>>> y_pred `array([4, 0, 1, ..., 2, 1, 0], dtype=int32)`
`>>>` `y_pred` `is` `kmeans``.``labels_` `` `True` ``
```

```py```````py`` ``````py````` We can also take a look at the five centroids that the algorithm found:    ```py >>> kmeans.cluster_centers_ `array([[-2.80389616,  1.80117999],`  `[ 0.20876306,  2.25551336],`  `[-2.79290307,  2.79641063],`  `[-1.46679593,  2.28585348],`  `[-2.80037642,  1.30082566]])` ```   ```py```````py 你可以轻松地将新实例分配给质心最近的簇：``` >>> import numpy as np `>>>` `X_new` `=` `np``.``array``([[``0``,` `2``],` `[``3``,` `2``],` `[``-``3``,` `3``],` `[``-``3``,` `2.5``]])` ```py `>>>` `kmeans``.``predict``(``X_new``)` `` `array([1, 1, 2, 2], dtype=int32)` `` ``` ```py   ``````py``` ``````py`` ``````py` If you plot the cluster’s decision boundaries, you get a Voronoi tessellation: see Figure 8-3, where each centroid is represented with an ⓧ.  ![Diagram showing Voronoi tessellation with k-means centroids marked, highlighting decision boundaries between clusters.](img/hmls_0803.png)  ###### Figure 8-3\. k-means decision boundaries (Voronoi tessellation)    The vast majority of the instances were clearly assigned to the appropriate cluster, but a few instances were probably mislabeled, especially near the boundary between the top-left cluster and the central cluster. Indeed, the *k*-means algorithm does not behave very well when the blobs have very different diameters because all it cares about when assigning an instance to a cluster is the distance to the centroid.    Instead of assigning each instance to a single cluster, which is called *hard clustering*, it can be useful to give each instance a score per cluster, which is called *soft clustering*. The score can be the distance between the instance and the centroid or a similarity score (or affinity), such as the Gaussian radial basis function we used in Chapter 2. In the `KMeans` class, the `transform()` method measures the distance from each instance to every centroid:    ``` >>> kmeans.transform(X_new).round(2) `array([[2.81, 0.33, 2.9 , 1.49, 2.89],`  `[5.81, 2.8 , 5.85, 4.48, 5.84],`  `[1.21, 3.29, 0.29, 1.69, 1.71],`  `[0.73, 3.22, 0.36, 1.55, 1.22]])` ```py   ``````py 在这个例子中，`X_new`中的第一个实例距离第一个质心约为 2.81，距离第二个质心 0.33，距离第三个质心 2.90，距离第四个质心 1.49，距离第五个质心 2.89。如果你有一个高维数据集并且以这种方式转换它，你最终会得到一个*k*-维数据集：这种转换可以是一个非常有效的非线性降维技术。或者，你可以使用这些距离作为额外的特征来训练另一个模型，如第二章中所述。### k-means 算法
