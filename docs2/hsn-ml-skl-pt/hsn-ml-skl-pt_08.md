# 第七章\. 维度降低

许多机器学习问题涉及每个训练实例成千上万个特征。不仅所有这些特征使得训练变得极其缓慢，而且它们还可以使找到好的解决方案变得更加困难，正如你将看到的。这个问题通常被称为*维度诅咒*。

幸运的是，在现实世界的问题中，通常可以显著减少特征的数量，将一个不可解的问题转化为可解的问题。例如，考虑 MNIST 图像（在第三章中介绍）：图像边界的像素几乎总是白色，因此你可以完全从训练集中删除这些像素，而不会丢失太多信息。正如我们在上一章中看到的，图 6-6 证实了这些像素对于分类任务来说完全不重要。此外，相邻的两个像素通常高度相关：如果你将它们合并成一个像素（例如，通过取两个像素强度的平均值），你不会丢失太多信息，从而去除冗余，有时甚至去除噪声。

###### 警告

降低维度也可能丢失一些有用的信息，就像将图像压缩成 JPEG 会降低其质量一样：它可能会使你的系统性能略有下降，尤其是如果你过度降低维度。此外，一些模型——如神经网络——可以有效地处理高维数据，并学习在保留任务所需的有用信息的同时降低其维度。简而言之，添加一个额外的预处理步骤进行维度降低并不总是有帮助。

除了加快训练速度并可能提高你的模型性能之外，维度降低对于数据可视化也非常有用。将维度降低到两个（或三个）使得在图上绘制高维训练集的浓缩视图成为可能，并且通过视觉检测模式，如簇，通常可以获得一些重要的见解。此外，数据可视化对于向非数据科学家的人传达你的结论至关重要——特别是那些将使用你的结果做决策的人。

在本章中，我们将首先讨论维度诅咒，并了解高维空间中发生的情况。然后我们将考虑维度降低的两种主要方法（投影和流形学习），并介绍三种最流行的维度降低技术：主成分分析（PCA）、随机投影和局部线性嵌入（LLE）。

# 维度诅咒

我们已经习惯了生活在三维空间中⁠^(1)，以至于当我们试图想象高维空间时，直觉就会失效。即使是一个基本的 4 维超立方体在我们心中也很难想象（参见图 7-1），更不用说在一个 1000 维空间中弯曲的 200 维椭球体了。

![展示从点到四维体的演变，说明从 0D 到 4D 的超立方体概念的图](img/hmls_0701.png)

###### 图 7-1\. 点、线段、正方形、立方体和四维体（从 0D 到 4D 的超立方体）⁠^(2)

结果表明，许多事物在高维空间中的表现非常不同。例如，如果你在单位正方形（1 × 1 的正方形）中随机选择一个点，它位于距离边界小于 0.001 的概率只有大约 0.4%（换句话说，随机点在任何维度上“极端”的可能性非常小）。但在一个 10,000 维的单位超立方体中，这个概率超过 99.999999%。一个高维超立方体中的大多数点都非常接近边界。⁠^(3)

这里有一个更麻烦的差异：如果你在单位正方形中随机选择两个点，这两个点之间的距离平均大约是 0.52。如果你在 3D 单位立方体中随机选择两个点，平均距离大约是 0.66。那么，在 1,000,000 维单位超立方体中随机选择的两个点呢？平均距离，信不信由你，大约是 408.25（大约是$StartRoot StartFraction 1 comma 000 comma 000 Over 6 EndFraction EndRoot$）！这是反直觉的：当两个点都位于同一个单位超立方体中时，它们怎么可能相距如此之远？好吧，高维度中空间很大。

因此，高维数据集通常非常稀疏：大多数训练实例可能彼此相距很远，因此基于距离或相似性的训练方法（如*k*-最近邻）将不太有效。而且，某些类型的模型可能根本无法使用，因为它们与数据集的维度扩展不佳（例如，SVMs 或密集神经网络）。新实例可能远离任何训练实例，这使得预测在低维度的可靠性大大降低，因为它们将基于更大的外推。由于数据中的模式将变得更加难以识别，模型将倾向于比低维度更频繁地拟合噪声；正则化将变得更加重要。最后，模型将变得更加难以解释。

理论上，通过增加训练集的大小以达到足够的训练实例密度，可以解决这些问题。不幸的是，在实践中，达到给定密度所需的训练实例数量会随着维数的增加而呈指数增长。在只有 100 个特征的情况下——这比 MNIST 问题中的特征少得多——所有特征的范围都在 0 到 1 之间，你需要比可观测宇宙中的原子还多的训练实例，才能使训练实例平均距离在 0.1 以内，假设它们在所有维度上均匀分布。

# 维度约简的主要方法

在深入研究特定的降维算法之前，让我们看看降低维度的两种主要方法：投影和流形学习。

## 投影

在大多数实际问题中，训练实例并不是均匀地分布在所有维度上。许多特征几乎恒定不变，而其他特征高度相关（如前面讨论的 MNIST）。因此，所有训练实例都位于（或接近）高维空间中一个低得多的*子空间*内。这听起来很抽象，所以让我们看一个例子。在图 7-2 中，一个 3D 数据集由小圆球表示（我将在接下来的几节中多次提到这个图）。

注意到所有训练实例都靠近一个平面：这是高维（3D）空间中的低维（2D）子空间。如果我们将每个训练实例垂直投影到这个子空间上（如通过连接实例和平面的短虚线所示），我们就会得到图 7-3 中显示的新 2D 数据集。哇！我们刚刚将数据集的维度从 3D 降低到 2D。注意，轴对应于新的特征*z*[1]和*z*[2]：它们是平面上的投影坐标。

![一个 3D 散点图，显示数据点聚集在 2D 平面上附近，说明了高维空间中的低维子空间。](img/hmls_0702.png)

###### 图 7-2\. 一个靠近 2D 子空间的 3D 数据集

![散点图显示 2D 数据集，轴标注为新的特征 z1 和 z2，说明了从 3D 到 2D 的降维。](img/hmls_0703.png)

###### 图 7-3\. 投影后的新 2D 数据集

## 流形学习

虽然投影速度快且通常效果良好，但并非总是降维的最佳方法。在许多情况下，子空间可能会扭曲和旋转，例如在图 7-4 中展示的瑞士卷数据集中：这是一个包含瑞士卷形状 3D 点的玩具数据集。

![瑞士卷数据集的 3D 散点图，显示点以螺旋形状排列，用于说明降维中的挑战。](img/hmls_0704.png)

###### 图 7-4\. 瑞士卷数据集

简单地将投影到平面上（例如，通过丢弃*x*[3]）会将瑞士卷的不同层挤压在一起，如图 7-5 左侧所示。你可能更希望将瑞士卷展开，以获得图 7-5 右侧的 2D 数据集。

![显示左侧挤压的瑞士卷数据集，层重叠，与右侧展开版本对比，数据在两个维度上展开的示意图。](img/hmls_0705.png)

###### 图 7-5\. 将投影到平面上（左侧）与展开瑞士卷（右侧）的挤压

瑞士卷是 2D *流形*的一个例子。简单来说，一个 2D 流形是在更高维空间中可以弯曲和扭曲的 2D 形状。更普遍地说，*d*-维流形是*n*-维空间（其中*d* < *n*）的一部分，它在局部类似于*d*-维超平面。在瑞士卷的情况下，*d* = 2，*n* = 3：它在局部类似于 2D 平面，但在第三维上被卷曲。

许多降维算法（例如，LLE、Isomap、t-SNE 或 UMAP）通过在训练实例所在的流形上建模来工作；这被称为*流形学习*。它依赖于*流形假设*，也称为*流形假设*，该假设认为大多数现实世界的高维数据集都靠近一个低维流形。这个假设通常在经验观察中是成立的。

再次思考 MNIST 数据集：所有手写数字图像都有一些相似之处。它们由连接的线条组成，边缘是白色的，并且它们或多或少是居中的。如果你随机生成图像，只有极小的一部分看起来像手写数字。换句话说，如果你尝试创建数字图像，可用的自由度比你被允许生成任何想要的图像时的自由度要低得多。这些约束往往会将数据集挤压到一个低维流形中。

流形假设通常伴随着另一个隐含的假设：如果用流形的低维空间表达，手头的任务（例如，分类或回归）将变得更加简单。例如，在图 7-6 的顶部行中，瑞士卷被分为两类：在 3D 空间（左侧）中，决策边界将相当复杂，但在 2D 展开流形空间（右侧）中，决策边界是一条直线。

然而，这个隐含的假设并不总是成立。例如，在图 7-6 的底部行中，决策边界位于 *x*[1] = 5。这个决策边界在原始的 3D 空间（一个垂直平面）中看起来非常简单，但在展开的流形（由四个独立的线段组成）中看起来更复杂。

简而言之，在训练模型之前降低训练集的维度通常会加快训练速度，但它并不总是导致更好的或更简单的解决方案；这完全取决于数据集。当数据集相对于特征数量较小时，降维通常更有效，特别是如果它是嘈杂的，或者许多特征彼此高度相关（即冗余）。如果你对生成数据的流程有领域知识，并且你知道它是简单的，那么流形假设肯定成立，降维很可能会有所帮助。

希望你现在对维度灾难的概念以及降维算法如何对抗它有了很好的理解，尤其是在流形假设成立的情况下。本章的其余部分将介绍一些最流行的降维算法。

![图表说明降维如何影响决策边界，左侧显示一个复杂的螺旋结构，右侧显示其简化的低维投影。](img/hmls_0706.png)

###### 图 7-6\. 决策边界在低维情况下不一定更简单

# PCA

*主成分分析*（PCA）是迄今为止最流行的降维算法。首先，它确定离数据最近的超平面，然后将数据投影到该平面上，如图 7-2 所示。

## 保留方差

在你能够将训练集投影到低维超平面之前，你首先需要选择正确的超平面。例如，一个简单的二维数据集在 图 7-7 的左侧表示，包括三个不同的轴（即 1D 超平面）。右侧是数据集在每个这些轴上的投影结果。正如你所看到的，实线上的投影保留了最大的方差（顶部），而点线上的投影保留了非常少的方差（底部），虚线上的投影保留了中等数量的方差（中间）。

![PCA 图表显示左侧的二维数据集投影到三个不同的 1D 轴上。实线保留了最多的方差，虚线保留了中等数量的方差，点线保留了最少的方差，如右侧点云的分布所示。](img/hmls_0707.png)

###### 图 7-7\. 选择投影的子空间

选择保留最大方差量的轴似乎是合理的，因为它与其他投影相比可能丢失的信息更少。考虑当太阳正午直射时你在地面的影子：它是一个小而模糊的形状，看起来根本不像你。但你在日出时墙上的影子要大得多，*确实*像你。选择最大化方差轴的另一种理由是，它也是最小化原始数据集与其在该轴上投影之间的平均平方距离的轴。这是主成分分析（PCA）背后的简单想法，PCA 早在 [1901 年](https://homl.info/pca)!⁠^(4) 就被提出了！

## 主成分

PCA 识别出在训练集中解释最大方差量的轴。在图 7-7 中，它是实线。它还找到一个与第一个轴正交的第二个轴，解释了剩余的最大方差量。在这个 2D 示例中，没有选择：它是虚线。如果是一个更高维度的数据集，PCA 还会找到一个与前面两个轴都正交的第三个轴，第四个轴，第五个轴，等等——与数据集维度数量一样多的轴。

第*i*轴被称为数据的第*i*个**主成分**（PC）。在图 7-7 中，第一个 PC 是向量**c**[**1**]所在的轴，第二个 PC 是向量**c**[**2**]所在的轴。在图 7-2 中，前两个 PC 位于投影平面上，第三个 PC 是垂直于该平面的轴。在投影后，回到图 7-3，第一个 PC 对应于*z*[1]轴，第二个 PC 对应于*z*[2]轴。

###### 注意

对于每个主成分，PCA 找到一个以零为中心的单位向量，指向 PC 的方向。不幸的是，它的方向没有保证：如果你稍微扰动训练集并再次运行 PCA，单位向量可能会指向相反的方向。事实上，如果这两个轴上的方差非常接近，一对单位向量甚至可能会旋转或交换。所以如果你在模型之前使用 PCA 作为预处理步骤，确保每次更新 PCA 转换器时都完全重新训练模型：如果你不这样做，并且 PCA 的输出与之前版本不匹配，模型会非常困惑。

那么你如何找到训练集的主成分呢？幸运的是，有一个标准的矩阵分解技术叫做**奇异值分解**（SVD），可以将训练集矩阵**X**分解为三个矩阵**U** **Σ** **V**^⊺的乘积，其中**V**包含了定义你正在寻找的所有主成分的单位向量，按照正确的顺序排列，如方程 7-1 所示。⁠^(5)

##### 方程 7-1\. 主成分矩阵

**粗体上标 V 等于 3x4 矩阵，第一行第一列是 2，第二列是 2，第三列是空，第四列是 2，第二行第一列是粗体 c，第二列是粗体 c，第三列是中横线省略号，第四列是粗体 c 下标 n 基线，第三行第一列是 2，第二列是 2，第三列是空，第四列是 2**。

以下 Python 代码使用 NumPy 的`svd()`函数获取图 7-2 中代表的所有 3D 训练集的主成分，然后它提取定义前两个 PC 的两个单位向量：

```py
import numpy as np

X = [...]  # create a small 3D dataset
X_centered = X - X.mean(axis=0)
U, s, Vt = np.linalg.svd(X_centered)
c1 = Vt[0]
c2 = Vt[1]
```

###### 警告

PCA 假设数据集围绕原点中心化。正如你将看到的，Scikit-Learn 的 PCA 类会为你处理数据的中心化。如果你自己实现 PCA（如前例所示），或者如果你使用其他库，别忘了首先中心化数据。

## 投影到 d 维

一旦确定了所有主成分，就可以通过将其投影到由前 *d* 个主成分定义的超平面上来将数据集的维度降低到 *d* 维（我们将在稍后讨论如何选择维度数 *d*）。选择这个超平面确保投影将尽可能保留方差。例如，在 图 7-2 中，3D 数据集被投影到由前两个主成分定义的 2D 平面上，保留了数据集的大部分方差。因此，2D 投影看起来非常像原始的 3D 数据集。

要将训练集投影到超平面并获取一个维度为 *d* 的降低数据集 **X**[*d*-proj]，计算训练集矩阵 **X** 与矩阵 **W**[*d*] 的矩阵乘法，其中 **W**[*d*] 是包含 **V** 的前 *d* 列的矩阵，如 方程式 7-2 所示。

##### 方程式 7-2\. 将训练集投影到 *d* 维

$ \mathbf{X}^{d-\text{proj}}_{\text{Baseline}} = \mathbf{X} \mathbf{W}^{d} $

以下 Python 代码将训练集投影到由前两个主成分定义的平面上：

```py
W2 = Vt[:2].T
X2D = X_centered @ W2
```

就这样！你现在知道如何通过将数据集投影到任意数量的维度来降低其维度，同时尽可能保留方差。

## 使用 Scikit-Learn

Scikit-Learn 的 `PCA` 类使用 SVD 来实现 PCA，就像我们在本章前面所做的那样。以下代码将 PCA 应用到数据集上，将其维度降低到二维（请注意，它自动处理了数据的中心化）：

```py
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X2D = pca.fit_transform(X)
```

在将 `PCA` 转换器拟合到数据集后，其 `components_` 属性包含 **W**[*d*] 的转置：它包含每个前 *d* 个主成分的一行。

## 解释方差比

另一个有用的信息是每个主成分的解释方差比，通过 `explained_variance_ratio_` 变量提供。该比率表示数据集方差中沿每个主成分的比例。例如，让我们看看 图 7-2 中表示的 3D 数据集的前两个成分的解释方差比：

```py
>>> pca.explained_variance_ratio_ `array([0.82279334, 0.10821224])`
```
