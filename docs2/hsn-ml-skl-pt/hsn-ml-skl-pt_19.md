# 第十七章\. 加速变压器

在第十五章和第十六章中，我们构建了各种变压器，从分类器、翻译器和聊天机器人，到视觉和多模态变压器。虽然变压器非常灵活且强大，但它们远非完美。特别是，它们可能非常慢，尤其是在处理长输入序列时。

幸运的是，已经开发了许多技术来加速任何规模的变压器：

+   为了加速生成式变压器的解码，我们将使用键/值缓存和推测性解码，然后我们将简要介绍几种并行化文本生成的方案。

+   为了加速多头注意力（MHA），这是变压器中最计算密集的组件之一，我们将探讨稀疏注意力、近似注意力、共享投影和 FlashAttention。

+   为了加速具有高达万亿参数的巨型变压器，我们将讨论专家混合（MoE）。

+   为了高效训练大型变压器，我们将讨论使用适配器如低秩适配（LoRA）、激活检查点、序列打包、梯度累积和并行性进行的参数高效微调（PEFT）。

###### 小贴士

加速变压器的另一种方法是使其变得更小。这可以通过使用降低精度和量化来实现，这些内容在第 B 附录中讨论。

要介绍的技术有很多，而且它们相当高级，所以如果你是变压器的新手，现在可以安全地跳过这一章，并在需要时再回来。这也是为什么这一章仅在线上提供，可在[*https://homl.info*](https://homl.info)找到，为其他章节留出空间。
