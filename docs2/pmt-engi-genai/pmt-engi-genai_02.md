# 第二章. 文本生成的大语言模型简介

在人工智能领域，最近的研究重点是大语言模型的演变。与它们不那么灵活的前辈相比，LLM 能够处理和从大量数据中学习，从而产生了产生与人类语言输出非常相似文本的能力。这些模型已经应用于各种不同的应用，从撰写内容到自动化软件开发，以及实现实时交互式聊天机器人体验。

# 文本生成模型是什么？

文本生成模型利用高级算法来理解文本中的含义，并产生通常与人类工作难以区分的输出。如果你曾经与[ChatGPT](https://chat.openai.com)互动或对其构建连贯且上下文相关的句子能力感到惊奇，你见证了 LLM 的实际力量。

在自然语言处理（NLP）和 LLM 中，基本的语言单位是*标记*。[标记](https://oreil.ly/3fOsM)可以代表句子、单词，甚至子词，如一组字符。理解文本数据大小的一个有用方法是查看它包含的标记数量；例如，100 个标记的文本大约相当于 75 个单词。这种比较对于管理 LLM 的处理限制至关重要，因为不同的模型可能有不同的标记容量。

*标记化*，即将文本分解成标记的过程，是准备数据用于 NLP 任务的关键步骤。可以用于标记化的方法包括[字节对编码（BPE）](https://oreil.ly/iSOp7)，WordPiece 和 SentencePiece。每种方法都有其独特的优势，适用于特定的用例。BPE 因其处理广泛词汇量的效率而常用，同时保持标记数量可管理。

BPE（字节对编码）首先将文本视为一系列单个字符。随着时间的推移，它将频繁一起出现的字符组合成单个单元，或称为标记。为了更好地理解这一点，可以考虑单词*apple*。最初，BPE 可能将其视为*a*，*p*，*p*，*l*和*e*。但注意到*p*在数据集中经常出现在*a*之后和*l*之前，它可能会将它们组合起来，并在未来的实例中将*appl*视为一个单独的标记。

这种方法有助于 LLM 识别和生成单词或短语，即使它们在训练数据中并不常见，这使得模型更具适应性和多功能性。

理解大型语言模型（LLM）的工作原理需要掌握支撑这些系统的底层数学原理。尽管计算可能很复杂，但我们仍可以将核心元素简化，以提供对这些模型如何运作的直观理解。特别是在商业环境中，LLM 的准确性和可靠性至关重要。

实现这种可靠性的重要部分在于 LLM 开发的前训练和微调阶段。最初，模型在前训练阶段在庞大的数据集上进行训练，获得对语言的广泛理解。随后，在微调阶段，模型被适应于特定任务，磨练其能力，为专门的应用提供准确和可靠的结果。

## 向量表示：语言的数值本质

在 NLP 领域，单词不仅仅是字母符号。它们可以被标记化，然后以数值形式表示，称为*向量*。这些向量是多维数组，包含数字，捕捉语义和句法关系：

$w right-arrow bold v equals left-bracket v 1 comma v 2 comma ellipsis comma v Subscript n Baseline right-bracket$

创建单词向量，也称为*词嵌入*，依赖于语言中的复杂模式。在密集的训练阶段，模型被设计用来识别和学习这些模式，确保具有相似意义的单词在多维空间中彼此靠近（图 2-1）。

![Word Embeddings](img/pega_0201.png)

###### 图 2-1\. 单词嵌入空间中单词向量的语义邻近性

这种方法的美丽之处在于它能够捕捉单词之间的细微关系并计算它们的距离。当我们检查词嵌入时，很明显，具有相似或相关意义的单词，如*virtue*和*moral*或*walked*和*walking*，都位于彼此附近。这种嵌入空间中的空间接近性成为各种 NLP 任务中的强大工具，使模型能够理解上下文、语义以及构成语言的复杂关系网。

## Transformer 架构：协调上下文关系

在我们深入探讨 transformer 架构的机制之前，让我们建立一个基础的理解。简单来说，当我们有一个句子，比如说，*The cat sat on the mat*，这个句子中的每个单词都会被转换成其数值向量表示。所以，*cat*可能变成一系列数字，*sat*、*on*和*mat*也是如此。

正如你将在本章后面详细探索的那样，transformer 架构将这些单词向量理解它们之间的关系——在结构（句法）和意义（语义）上。有许多类型的 transformer；图 2-2 展示了 BERT 和 GPT 的架构。此外，transformer 不仅孤立地看待单词；它观察*cat*并知道在这个句子中以特定方式与*sat*和*mat*相关。

![BERT 和 GPT 架构](img/pega_0202.png)

###### 图 2-2\. BERT 使用编码器处理输入数据，而 GPT 有解码器处理输出

当 transformer 处理这些向量时，它使用数学运算来理解单词之间的关系，从而产生具有丰富上下文信息的新向量：

$bold v prime Subscript i Baseline equals Transformer left-parenthesis bold v 1 comma bold v 2 comma ellipsis comma bold v Subscript m Baseline right-parenthesis$

Transformer 的一个显著特点是它们理解词语细微语境意义的能力。Transformer 中的 [自注意力](https://oreil.ly/xuovP) 机制允许句子中的每个词查看所有其他词，以更好地理解其语境。想象一下，每个词都在为其他词对其意义的重要性投票。通过考虑整个句子，Transformer 可以更准确地确定每个词的角色和意义，使它们的*解释更加语境丰富*。

## 概率文本生成：决策机制

在 Transformer 理解了给定文本的语境之后，它继续生成新的文本，受似然或概率概念指导。从数学的角度来看，模型计算每个可能的下一个词跟随当前词序列的可能性，并选择最有可能的那个词：

$w Subscript next Baseline equals argmax upper P left-parenthesis w vertical-bar w 1 comma w 2 comma ellipsis comma w Subscript m Baseline right-parenthesis$

通过重复此过程，如图 图 2-3 所示，模型生成一个连贯且与语境相关的文本字符串作为其输出。

![使用类似 GPT-4 的 Transformer 模型生成文本的说明性概述](img/pega_0203.png)

###### 图 2-3\. 使用类似 GPT-4 的 Transformer 模型生成文本的方式

驱动大型语言模型（LLM）的机制根植于向量数学、线性变换和概率模型。虽然底层的操作计算密集，但核心概念建立在这些数学原理之上，提供了一个基础理解，架起了技术复杂性与商业应用之间的桥梁。

# 历史背景：Transformer 架构的兴起

类似 ChatGPT（*GPT* 代表 *生成预训练 Transformer*）的语言模型并非凭空出现。它们是 NLP 领域多年进步的结晶，特别是自 2010 年代后期以来的加速发展。这一进步的核心是引入了 transformer 架构，这在谷歌 Brain 团队开创性的论文 [“Attention Is All You Need”](https://oreil.ly/6NNbg) 中有详细阐述。

变换器架构的真正突破是“注意力”的概念。传统模型按顺序处理文本，这限制了它们对语言结构的理解，尤其是在文本的较长距离上。注意力通过允许模型直接将文本中位置较远的词语联系起来，而不管它们在文本中的位置，从而改变了这一点。这是一个开创性的观点。这意味着词语及其上下文不必在整个模型中移动才能相互影响。这不仅显著提高了模型的文本理解能力，而且使它们变得更加高效。

这种注意力机制在扩展模型检测文本中长距离依赖关系的能力方面发挥了至关重要的作用。这对于生成不仅上下文准确和流畅，而且在更长段落中也是连贯的输出至关重要。

根据人工智能先驱和教育家[安德鲁·吴](https://oreil.ly/JQd53)的说法，早期的许多自然语言处理研究，包括对变换器的基本研究，都得到了美国军事情报机构的重大资助。他们对机器翻译和语音识别等工具的浓厚兴趣，主要是出于情报目的，无意中为超越翻译本身的发展开辟了道路。

训练 LLM 需要大量的计算资源。这些模型被大量数据喂养，从千兆字节到拍字节不等，包括互联网内容、学术论文、书籍以及为特定目的定制的更窄数据集。然而，需要注意的是，用于训练 LLM 的数据可能携带其来源的*固有偏差*。因此，在使用这些模型时，用户应谨慎行事，并最好采用人工监督，以确保负责任和道德的 AI 应用。

例如，OpenAI 的 GPT-4 拥有估计的[1.7 万亿参数](https://oreil.ly/pZvMo)，相当于一个覆盖三万足球场的 Excel 电子表格。在神经网络中，“参数”是指在训练过程中调整的权重和偏差，使模型能够根据其训练数据表示和生成复杂模式。GPT-4 的训练成本估计为[6300 万美元](https://oreil.ly/_NAq5)，而训练数据将填满大约[650 公里书架的书](https://oreil.ly/D7jL5)。

为了满足这些要求，像微软、Meta 和谷歌这样的大型科技公司投入了大量资金，使得大型语言模型（LLM）的开发成为一场高风险的竞赛。

大型语言模型（LLM）的兴起为硬件行业带来了更高的需求，尤其是专注于图形处理单元（GPU）的公司。例如，英伟达几乎成了高性能 GPU 的同义词，这对于训练 LLM 至关重要。

随着公司努力构建更大、更复杂的模型，对强大、高效的 GPU 的需求急剧上升。人们追求的不仅仅是原始的计算能力。GPU 还需要针对机器学习中的特定任务进行微调，如张量运算。在机器学习环境中，“张量”是多维数据数组，对它们的操作是神经网络计算的基础。这种对专用能力的重视催生了定制的硬件，例如 NVIDIA 的 H100 张量核心 GPU，它专门设计用来加速机器学习工作负载。

此外，对顶级 GPU 的需求往往超过了供应，导致价格呈上升趋势。这种供需互动已经将 GPU 市场转变为一个竞争激烈且有利可图的领域。在这里，从科技巨头到学术研究人员，各种客户都在争相采购最先进的硬件。

这种需求的激增引发了一股创新浪潮，而不仅仅是针对 GPU。公司现在正专注于创建专门的 AI 硬件，例如谷歌的 Tensor 处理单元（TPUs），以满足 AI 模型不断增长的计算需求。

这个不断演变的地形不仅强调了人工智能领域软件与硬件之间的共生关系，而且也突出了 LLM“淘金热”的连锁反应。它正引导创新并将投资引导到各个领域，尤其是那些提供构建这些模型的基本组件的领域。

# OpenAI 的生成预训练转换器

[OpenAI](https://openai.com)的成立宗旨是确保通用人工智能的益处惠及全人类，最近它一直是人工智能革命的先锋。他们最具有突破性的贡献之一是 GPT 系列模型，这些模型在很大程度上重新定义了 LLM 能够实现的范围。

OpenAI 的原始 GPT 模型不仅仅是一个研究成果；它是对基于转换器架构潜力的有力展示。这个模型展示了机器理解并生成类似人类语言的初步步骤，为未来的进步奠定了基础。

GPT-2 的发布引起了人们的期待和谨慎。OpenAI 在认识到该模型强大能力的同时，由于担心其潜在的误用，最初犹豫是否发布。GPT-2 的强大力量使得伦理问题成为焦点，与今天模型的威力相比可能显得有些过时。然而，当 OpenAI 决定将项目作为[开源](https://oreil.ly/evOQE)发布时，并不仅仅意味着公开代码。它允许企业和研究人员将这些预训练模型作为构建块使用，将 AI 融入他们的应用中而无需从头开始。这一举措使高级自然语言处理能力更加民主化，促进了各个领域的创新。

在 GPT-2 之后，OpenAI 决定专注于发布付费的闭源模型。GPT-3 的推出标志着 LLM 进步中的一个重大步伐。它不仅因其技术实力而受到媒体的关注，还因其能力的社会影响而备受瞩目。该模型能够生成如此逼真的文本，以至于它常常与人类撰写的文本难以区分。从创作复杂的文学作品到生成操作代码片段，GPT-3 展示了 AI 看似无限的潜力。

## GPT-3.5-turbo 和 ChatGPT

在微软对其公司进行重大投资的支持下，OpenAI 推出了 GPT-3.5-turbo，这是其卓越前身的优化版本。2019 年，微软注入了 10 亿美元，后来增加到 130 亿美元以获得 OpenAI 营利性部门的 49%股份，OpenAI 利用这些资源开发了 GPT-3.5-turbo，它提供了更高的效率和更低的成本，使得大型语言模型（LLM）对更广泛的用例更加可及。

OpenAI 希望收集更多世界反馈以进行微调，因此[ChatGPT](https://chat.openai.com)应运而生。与通用型兄弟不同，[ChatGPT 经过微调](https://oreil.ly/6ib-Q)以在对话环境中表现出色，使人类与机器之间的对话既自然又富有意义。

图 2-4 展示了 ChatGPT 的训练过程，涉及三个主要步骤：

演示数据的收集

在这一步骤中，人类标注员提供了一系列提示的期望模型行为的示例。标注员在项目上进行培训，并遵循具体指令以准确标注提示。

训练监督策略

上一步收集的演示数据用于使用监督学习微调预训练的 GPT-3 模型。在监督学习中，模型在提供正确答案的标记数据集上进行训练。这一步骤有助于模型学习遵循给定的指令并产生符合期望行为的输出。

比较数据的收集和强化学习

在这一步中，收集了一个模型输出的数据集，并且人类标注者根据他们的偏好对这些输出进行排名。然后训练一个奖励模型来预测标注者会偏好哪些输出。最后，使用强化学习技术，特别是近端策略优化（PPO）算法，来优化监督策略，以最大化奖励模型中的奖励。

这个训练过程允许 ChatGPT 模型使其行为与人类意图保持一致。使用带有人类反馈的强化学习有助于创建一个比预训练的 GPT-3 模型更有帮助、更诚实、更安全的模型。

![ChatGPT 微调方法](img/pega_0204.png)

###### 图 2-4\. ChatGPT 的微调过程

根据一份[UBS 研究](https://oreil.ly/2Ivq2)，到 2023 年 1 月，ChatGPT 设定了新的基准，积累了 1 亿活跃用户，成为互联网历史上增长最快的消费应用。ChatGPT 现在已成为客户服务、虚拟助手和许多其他需要类似人类对话技巧的应用的首选。

# GPT-4

2024 年，OpenAI 发布了 GPT-4，它在理解复杂查询和生成上下文相关且连贯的文本方面表现出色。例如，GPT-4 在律师资格考试中得分达到 90 分，总分 400 分。目前，GPT-3.5-turbo 在 ChatGPT 中免费使用，但 GPT-4 需要[每月支付](https://oreil.ly/UOEBM)。

GPT-4 采用[mixture-of-experts 方法](https://oreil.ly/v45LZ)；它不仅超越了依赖单个模型的推理来产生更准确和有洞察力的结果。

2024 年 5 月 13 日，OpenAI 推出了[GPT-4o](https://oreil.ly/4ttmq)，这是一个能够实时处理和推理文本、音频和视觉输入的高级模型。该模型在视觉和音频理解方面提供了增强的性能；由于其能够在单个神经网络中处理所有三种模态，因此它比其前辈更快、更经济。

# 谷歌的 Gemini

由于 ChatGPT 的使用导致谷歌失去了搜索市场份额，它最初于 2023 年 3 月 21 日发布了 Bard。Bard 起初有些[粗糙](https://oreil.ly/Sj24h)，并且最初并没有提供与 ChatGPT 相同的高质量 LLM 响应（图 2-5）。

谷歌一直在为 Bard 添加额外功能，包括代码生成、视觉 AI、实时搜索和语音，使其在质量上更接近 ChatGPT。

2023 年 3 月 14 日，谷歌发布了[PaLM API](https://oreil.ly/EbI8-)，允许开发者在谷歌云平台上访问它。2023 年 4 月，亚马逊网络服务（AWS）发布了类似的服务，如[Amazon Bedrock](https://oreil.ly/4fNQX)和[亚马逊的 Titan FMs](https://oreil.ly/FJ-7D)。谷歌在 2024 年 2 月的 v1.5 版本中将 Bard 重新命名为 Gemini，并开始获得与 GPT-4 类似的结果。

![谷歌的 Bard，这是一个类似于 ChatGPT 的应用。](img/pega_0205.png)

###### 图 2-5\. Bard 对詹姆斯·韦伯太空望远镜的幻想结果

此外，谷歌还发布了基于与 Gemini 相同架构的两个较小的 [开源模型](https://oreil.ly/LWIwv)。OpenAI 终于不再是软件工程师将最先进的 LLM 集成到其应用程序中的唯一明显选择。

# Meta 的 Llama 和开源

Meta 对语言模型的方法与行业中的其他竞争对手存在显著差异。通过依次发布开源模型 [Llama](https://oreil.ly/LroPn)，[Llama 2](https://oreil.ly/NeZLw) 和 [Llama 3](https://oreil.ly/Vwlo-)，Meta 旨在培养一个更加包容和协作的人工智能开发生态系统。

Llama 2 和 Llama 3 的开源特性对更广泛的科技行业具有重大影响，尤其是对大型企业而言。透明度和协作精神鼓励快速创新，因为问题和漏洞可以由全球开发者社区迅速识别和解决。随着这些模型变得更加稳健和安全，大型企业可以更加有信心地采用它们。

Meta 的开源策略不仅使最先进的 AI 技术的访问民主化，而且有可能在整个行业中产生重大影响。通过为协作、透明和去中心化的开发过程奠定基础，Llama 2 和 Llama 3 是开创性的模型，它们很可能定义了生成式 AI 的未来。这些模型在 AWS、Google Cloud、Hugging Face 和其他平台上有 70 亿、80 亿和 700 亿参数版本。

这些模型的开放源代码特性是一把双刃剑。一方面，它使竞争更加公平。这意味着即使是较小的开发者也有机会为创新做出贡献，改进和应用开源模型到实际商业应用中。这种去中心化的创新可能导致在单一组织围墙花园内可能不会发生的突破，从而增强模型的能力和应用。

然而，正是这种使这一切成为可能的开源性也带来了潜在的风险，因为它可能允许恶意行为者利用这项技术达到有害的目的。这确实是像 OpenAI 这样的组织所关注的问题，他们建议一定程度上的控制和限制实际上可以用来减轻这些强大工具的危险应用。

# 利用量化（Quantization）和 LoRA（Low-Rank Adaptation）

这些开源模型中具有颠覆性的一个方面是[量化](https://oreil.ly/bkWXk)和[LoRA](https://oreil.ly/zORsB)（低秩近似）的潜力。这些技术允许开发者将模型适配到更小的硬件占用空间。量化有助于降低模型参数的数值精度，从而在不显著影响性能的情况下缩小模型的整体大小。同时，LoRA 有助于优化网络的架构，使其在消费级硬件上运行更加高效。

这样的优化使得在消费级硬件上微调这些 LLM 变得越来越可行。这是一个关键的发展，因为它允许进行更多的实验和适应性调整。不再局限于高性能数据中心，个人开发者、小型企业和初创公司现在可以在资源受限的环境中工作这些模型。

# Mistral

Mistral 7B，法国初创公司[Mistral AI](https://mistral.ai)的杰作，在生成式 AI 领域崭露头角，其 73 亿个参数产生了重大影响。这个模型不仅仅关乎规模；它关乎效率和能力，为开源大型语言模型及其在众多用例中的应用前景提供了光明。其效率的关键在于滑动窗口注意力的实现，这是一种在宽松的 Apache 开源许可证下发布的技巧。许多 AI 工程师以此模型为基础进行了微调，包括令人印象深刻的[Zephr 7b beta](https://oreil.ly/Lg6_r)模型。还有[Mixtral 8x7b](https://oreil.ly/itsJG)，这是一个专家混合模型（类似于 GPT-4 的架构），其结果与 GPT-3.5-turbo 相似。

想要更详细和最新的开源模型及其性能指标的比较，请访问由 Hugging Face 主办的聊天机器人[排行榜](https://oreil.ly/ttiji)。

# Anthropic: Claude

2023 年 7 月 11 日发布的[Claude 2](https://claude.ai/login)通过其开创性的[宪法 AI](https://oreil.ly/Tim9W)方法在 AI 安全和一致性方面脱颖而出——使用一系列规则或价值观来训练模型。Claude 2 的一个显著改进是其扩展的上下文窗口，达到 10 万个标记，以及上传文件的能力。在生成式 AI 领域，*上下文窗口*指的是模型在生成响应时可以积极考虑或记住的文本或数据量。拥有更大的上下文窗口，模型可以基于更广泛的环境进行理解和生成。

这一进步引起了 AI 工程师的极大热情，因为它为新的更复杂的用例开辟了道路。例如，Claude 2 同时处理更多信息的能力使其擅长总结大量文档或维持深入对话。然而，这一优势很快就被 OpenAI 发布的 GPT-4 128K 版本所取代，仅[六个月后](https://oreil.ly/BWxrn)。然而，竞争对手之间的激烈竞争正在推动该领域向前发展。

Claude 的下一代包括了[Opus](https://oreil.ly/NH0jh)，这是第一个在智能方面与 GPT-4 相媲美的模型，以及 Haiku，这是一个更小的模型，速度极快，价格为每百万个标记 0.25 美元（当时是 GPT-3.5-turbo 成本的一半）。

# GPT-4V(视觉)

在一个重大的飞跃中，2023 年 9 月 23 日，OpenAI 通过引入视觉功能扩展了 GPT-4 的能力，使用户能够指令 GPT-4 分析图像和文本。这一创新也体现在 ChatGPT 界面的更新中，现在它支持包含图像和文本作为用户输入。这一发展标志着向*多模态模型*的重大趋势转变，这些模型可以在单一语境中无缝处理和理解多种类型的数据，如图像和文本。

# 模型比较

在撰写本文时，LLM 市场由 OpenAI 主导，其最先进的 GPT-4 模型被广泛认为具有显著领先优势。最接近的竞争对手是 Anthropic，人们对 Llama 和 Mistral 等小型开源模型在微调方面的潜力感到兴奋。尽管评论家们预计 OpenAI 未来将继续推出世界领先的模型，但随着开源模型在更多任务上达到*足够好*的水平，AI 工作负载可能会转向本地微调模型。随着模型性能和量化（在精度、大小和计算成本之间权衡的方法）的进步，未来某一天在您的手机或其他设备上运行 LLM 可能成为可能。

目前，了解模型擅长什么最好的方法是在多个模型上运行相同的提示并比较响应。在我们工作中经常突出的一点是，GPT-4 在遵循指令方面表现得更好，如下面的例子所示，它是唯一一个以正确格式、以字母*i*开头并符合要求的名称响应的模型。

输入：

```py
Brainstorm a list of product names for a shoe that fits any foot size,
in the style of Steve Jobs.

Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of three product names]

## Examples
Product description: A refrigerator that dispenses beer
Product names: iBarFridge, iFridgeBeer, iDrinkBeerFridge

Product description: A watch that can tell accurate time in space
Product names: iNaut, iSpace, iTime

Product description: A home milkshake maker
Product names: iShake, iSmoothie, iShake Mini
```

输出（GPT-4）：

```py
Product description: A shoe that fits any foot size
Product names: iFit, iShoe, iFlexible
```

输出（Claude 3）：

```py
Product description: A shoe that fits any foot size
Product names: iFit, iComfort, iSole
```

输出（Llama 3 70b）：

```py
Here is the list of product names for a shoe that fits any foot size,
in the style of Steve Jobs:

Product description: A shoe that fits any foot size Product names:
iFit, OneSize, iWalkFree
```

# 摘要

从 LLM 的诞生到其当前的高级状态，这是一个关于不懈创新、合作和激烈竞争的故事。随着这些模型继续进化，它们很可能会成为我们日常生活中的重要组成部分，改变我们与技术以及彼此互动的方式。

通过理解这些模型的历史背景和功能，您可以更好地欣赏我们可用于各种应用的工具，从提示工程到智能虚拟代理的开发。然而，需要注意的是，尽管这些模型提供了广阔的可能性，数据隐私仍然是一个关键的关注点。如果这些模型使用您的数据进行再训练或微调，请谨慎行事，并避免输入敏感信息。

在下一章中，您将学习所有基本的提示工程技术，用于与文本 LLMs（大型语言模型）一起工作。您将学习到充分利用像 GPT-4 这样的强大语言模型所需的基本技能。随着您解锁生成式 AI 的真正潜力，您将迎来令人兴奋的见解和实用的方法。
