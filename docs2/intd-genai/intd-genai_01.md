# 2 训练大型语言模型

本章涵盖

+   解释 LLMs 是如何被训练的

+   介绍 LLMs 的涌现特性

+   探索训练 LLMs 带来的危害和风险

几十年来，数字经济一直依赖于数据这一货币。收集和交易关于我们在网上是谁以及我们在做什么的信息的数字经济价值数万亿美元，随着我们越来越多的日常活动转移到互联网上，磨坊的磨盘上磨的谷物越来越多。大型语言模型（LLMs）是互联网时代的发明，通过收集在线发现的数以千计的文本数据来模拟人类语言。

这个过程产生了可预测和不可预测的结果。值得注意的是，关于 LLMs 使用的数据集中有什么以及如何防止模型复制他们在训练集中持有的某些更令人反感的文本，都存在重大问题。在如此规模的数据收集下，收集个人信息、低质量、垃圾邮件或攻击性内容是预期的，但如何解决这个问题是另一个挑战。我们现在看到的 LLMs 规模已经展现出了许多似乎不属于较小语言模型的能力。这些特性使 LLMs 在各种用途上更具吸引力，并确保了向更多数据和更大模型的竞赛不会很快结束。

在本章中，你将了解更多关于如何训练大型语言模型（LLMs）以理解它们与先前模型相比的独特之处，以及这些特性如何导致新的能力和潜在的风险。

## LLMs 是如何被训练的？

在第一章中，我们介绍了训练 LLMs 涉及的一些概念。我们涵盖了 Transformer 架构，这是一种在 LLMs 中使用的特定类型的神经网络，并讨论了 LLMs 使用的一些数据来源。我们还解释了他们被训练完成的自我监督任务——生成下一个最可能的单词或字符，也称为标记预测。在这里，我们将更详细地检查训练过程，并讨论 LLMs 最令人惊讶和激动人心的方面——它们的涌现特性，即他们没有被训练去做，但仍然做得很好的事情。

创建一个大型语言模型（LLM）的第一步，通常被称为**预训练**步骤，是在一个庞大的数据语料库上对某些标记预测任务（对于生成模型，是自回归或因果标记预测）进行训练。之所以称为预训练，是因为尽管这是一个训练阶段，但模型在此阶段编码的知识是任何后续自然语言任务的基础。然后，模型在单个或多个附加任务上进行微调，即使用标记数据和特定目标进行训练。例如，对话代理如 ChatGPT 可能会在对话数据上进行微调；许多生成模型会在指令数据集上进行微调，以提高其遵循指令的能力（例如，“为我写一首诗”）；其他模型可能会针对代码生成进行微调。这个过程在图 2.1 中有所展示，但深入探讨每个阶段都是值得的。

![图片](img/CH02_F01_Dhamani.png)

图 2.1 LLMs 的高级训练过程

### 探索公开网络数据收集

为了模拟自然语言并生成令人信服的语言，LLM 需要大量的例子。让我们考虑所有进入问答任务的问题。首先，模型必须对问题和上下文（问题所涉及的内容）有一个准确的表现，这反过来又意味着需要对问题和上下文中的每个标记有一个表现——类似于知道单词本身的意思。模型还必须能够从句法上解析问题，以确定所问的内容，然后产生一个答案，无论是从上下文（开放式书籍案例）还是从其对外部概念的内部分代（封闭式书籍案例）中产生。由于 LLM 已经看到了互联网上的大量文本，大多数 LLM 能够正确回答像“谁是美国的第一个总统？”这样的问题，而无需任何提供的上下文。更难以捉摸的信息可能会导致错误的或虚构的答案，因为模型不会有一个高概率的响应。值得注意的是，如果我们向 ChatGPT 提问“谁是第一个总统？”而没有指定我们是在询问美国，ChatGPT 会回答，“美国的第一个总统是乔治·华盛顿。”

LLM 使用来自公开网络的数据，这指的是互联网上所有公开网页，包括维基百科和 Reddit 等网站，也可能包括非密码保护的博客、新闻聚合器和非私人论坛。为什么 ChatGPT 假设我们在询问美国？公平地说，如果请求来自另一个国家的 IP 地址，答案可能不同，但这种假设也掩盖了一个关于互联网数据的不可否认的事实——其中大部分是英语，而且不成比例的大量数据来自美国和西欧。在第一章中，我们提到维基百科是 LLM 的经典数据来源之一。虽然该百科的全球地理覆盖范围持续改善，但英语维基百科有超过 660 万篇文章，而下一个最高的总数是法语维基百科的 250 万篇文章。这种影响的下游效应是，LLM 在理解、生成和完成英语任务方面表现得更好。它们也更好地理解与北美和西欧相关的话题，因此更好地服务于这些受众。

要了解正在使用的其他类型文本数据集，我们可以查看公开数据存储库，例如开源 AI 公司 Hugging Face 的数据存储库（见[`huggingface.co/datasets`](https://huggingface.co/datasets)）。任何人都可以下载并使用这些公开数据为其项目服务，尽管有时数据的使用类型可能受到数据许可的限制；例如，数据集提供者可能指定数据集仅用于学术或研究目的，不得用于商业应用。一个用于语言模型的数据集包含数百万条 Reddit 帖子（过滤掉了非英语帖子）。其他数据集包括新闻文章集合、来自亚马逊和烂番茄（电影和电视剧的评论聚合网站）的评论，或来自社区问答网站 Stack Exchange 的问题和答案。Common Crawl 是一个非营利组织，维护着一个庞大的网页数据存储库，并向公众提供这些数据（见[`commoncrawl.org/`](https://commoncrawl.org/)）。简而言之，任何人们在线写作的地方都是一个潜在的数据来源。

开发大型语言模型（LLM）的公司可能会使用多种数据集的组合，例如来自 Hugging Face 等公开数据集，从第三方供应商购买的数据集，通过爬取网络自行收集的数据集，或者通过为模型编写学习示例自行创建的数据集。尽管 LLM 的初始训练可能不需要任何人工干预，但正如我们将看到的，众包和对话收集对于提高模型在特定领域（如聊天机器人的对话）的性能至关重要。

### 揭秘自回归和双向标记预测

一些早期的 LLMs，例如谷歌的 BERT，相比聊天机器人等生成用例，更侧重于自然语言理解。正因为这个目标，BERT 被称为双向模型，这意味着 BERT 被训练来预测句子中缺失的单词（标记）并能够访问左右两个上下文（双向部分）。这对于自然语言理解来说是非常理想的，因为模型能够获取更多关于特定单词使用上下文的信息。然而，如果一个模型用于文本生成，它不应该在缺失标记之后的内容上进行训练，因为它只能访问它之前的文本。这种类型的模型被称为**自回归**模型，因为未来的预测依赖于模型的历史数据。GPT 家族中的所有模型，以及谷歌的 Pathways 语言模型（PaLM），都是自回归的。

**自回归**意味着未来的预测依赖于模型的历史数据。

例如，考虑这个句子，“为了他们的蜜月，他们飞往 ______ 并在埃菲尔铁塔前享用浪漫晚餐。”模型需要预测的正确单词是“巴黎”。在这种情况下，正确的上下文（缺失单词之后发生的事情）特别具有信息性，双向模型很可能给出正确的答案。但是，当模型被要求生成文本，例如，“一个浪漫蜜月的理想地点是 ______，”任务的结构是这样的，即模型的补全位于上下文的末尾。因此，模型的训练应该只使用左上下文（缺失单词之前的内容）来预测缺失的标记。模型通过自我监督学习，反复从文本中的数十亿个例子中猜测最终的标记，并根据正确的标记调整其权重，直到模型在训练数据中猜测缺失标记的性能达到最优。当我们与 ChatGPT 聊天时，这看起来对用户来说不是一个正式的任务，但在底层，模型正在预测每条消息之后应该出现的内容。当我输入“嘿！怎么了？”时，逻辑上最有可能的补全是回答问题并返回问候。

### 微调大型语言模型（LLMs）

一旦在标记补全任务上训练，模型可以生成单词、短语或完整的句子。在这个阶段，这些模型被称为基础或基模型，因为它们提供了基础的知识，这是由于它们对数千个不同单词和概念的复杂表示，从而能够执行自然语言处理（NLP）任务。

虽然这些基础模型一开始并不那么令人印象深刻，但它们可以通过微调轻松适应以在特定任务上表现良好，即收集展示模型需要改进的特定任务或任务的标记数据集。这些任务可能非常狭窄，例如需要特定领域专业知识的分类问题，或者相当广泛。许多商业 LLMs 在遵循指令的数据上进行微调，以便模型能更好地响应“写一首歌”或“讲一个笑话”等输入。其他微调任务也是 LLMs 的常见用途，如摘要和问答。从技术角度来看，微调以监督方式训练神经网络，但不是从头开始，而是使用基础模型的权重初始化神经网络。而训练基础模型需要数周时间并使用大量计算资源，而微调可以在几分钟内完成。微调模型使用原始模型的表示，然后调整自己的权重和参数以最佳地适应新数据。

## 意外的：LLMs 的演化特性

在某些方面，LLMs 是前辈神经网络模型的自然扩展。在变压器架构使构建更大模型变得高效之前，众所周知，模型大小与模型在一系列常见 NLP 任务上的性能相关联，并且在许多情况下，这种性能提升可以根据经验推导的缩放定律进行预测。然而，LLMs 还产生了被称为演化特性的行为，这些行为无法通过缩放定律进行预测。在 2022 年关于 LLMs 演化能力的一项调查中，演化被定义为“当系统中的数量变化导致行为在本质上发生变化时” [[1]](http://arxiv.org/abs/2206.07682)。换句话说，我们可能预期，对于特定任务，一个拥有 1000 亿参数的模型会比一个拥有 1 亿参数的模型实现 10%更高的准确率。但是，拥有 1000 亿参数的模型——即 LLM——现在可以执行较小模型无法执行的任务，并且以某种不可预测和出乎意料的方式。

演化特性是指 LLMs 在非常大的模型尺寸下开始展现的能力，这些能力与较小模型的行为在本质上有所不同。

### 快速学习：通过少量示例进行学习

当谈论 LLMs 的演化能力时，将其与上一节中描述的过程产生的能力进行比较是有用的。在标准情况下，模型是预训练并微调以具备一个或多个自然语言能力，如翻译或类比完成。这些能力是训练流程的一部分，被认为是可预测的——不是指模型将如何表现，而是指模型在训练过程中的改进。

另一方面，涌现能力的首要例子是零样本和少样本学习。*零样本*和*少样本*这两个术语指的是在要求模型执行任务之前，模型所获得的示例数量。例如，假设一位餐馆老板想在他们的菜单上添加视觉指示，以标示素食菜肴。使用 ChatGPT，他们可能会写一些像这样的事情：“请重新编写这个菜单，并在所有不含任何肉的菜肴旁边加上星号，”然后复制并粘贴菜单。这可能对人类来说似乎是一个微不足道的小任务，但模型必须首先解释请求，然后根据每一项书面菜单内容是否含有肉类进行分类，最后以相应的格式生成输出。完成这样一个任务所需的自然语言理解和生成能力，在没有先前示例的情况下（我们可以安全地假设模型从未被明确训练来完成这项任务），在先前的语言模型中并未观察到，然而，大型语言模型（LLMs）在许多这样的零样本任务上可以产生令人印象深刻的成果，在这些任务中，模型之前从未见过这个任务。

零样本或少样本指的是在要求模型执行任务之前，模型所获得的示例数量。

在少样本情况下，模型在*提示*中给出几个任务的示例，即模型作为输入文本来确定它应该生成什么输出。在之前的零样本示例中，用户的请求构成了模型提示的一部分或全部（模型有时会部署一个基础提示，这可能提供关于如何响应输入的通用指令，但与本次讨论无关）。另一个用户可能希望模型执行一个稍微复杂一些的任务。比如说，一个自由职业的作家正在撰写三篇不同的文章——一篇关于狗的繁殖，一篇关于系外行星，还有一篇关于匹兹堡——并希望按主题组织文章列表。在这种情况下，他们可能会写一些像这样的事情：

以下每一篇文章都与“狗的繁殖”、“系外行星”或“匹兹堡”中的一个相关。对于每一篇文章，写出最可能的相关主题。

这也可以被结构化为一个零样本任务。然而，通常来说，提供几个示例对模型性能是有益的，所以如果响应并不完全符合作者的期望，他们可能会尝试提供额外的指导：

示例：“最新太空望远镜的发现”：系外行星；示例：“为什么斗牛犬有呼吸问题”：狗的繁殖；等等。

图 2.2 展示了零样本和少量样本提示与为任务微调模型的不同之处。如果你使用 LLM 执行这些任务之一，你可能已经尝试了零样本和少量样本学习，甚至没有意识到或考虑过。这是 LLM 的巨大优势之一：因为这些聊天机器人的界面仅仅是自然语言，我们通常可以更直观地调整输入以实现所需的输出，这比我们可能使用其他模型的方式要直观得多。

![图片](img/CH02_F02_Dhamani.png)

图 2.2 在机器翻译任务中，微调、零样本学习和单样本学习的比较

除了模型提示中的零样本和少量样本示例之外，对模型提示的其他更改还揭示了额外的新兴能力。一种称为思维链提示的技术，或者指导模型将具有挑战性的问题分解成多个步骤，已被证明可以提高模型性能（在其最简单的版本中，在提示前加上“让我们一步步思考”已被证明可以使模型在推理问题上的生成更加准确）。人们还测试了零样本任务上的详细指令，以及询问模型对其自身响应的信心水平，这些都可以在某些设置中改善响应。

在之前提到的关于探索大型语言模型（LLM）新兴能力的研究中，作者们检查了不同大小 LLM 在少量样本任务上的表现。特别是，研究人员寻找的是“小型”LLM 表现随机，但随后在更大规模上急剧提升的任务。他们发现，语言模型进行加法、减法和乘法的能力是新兴的，GPT-3 在几乎没有任何情况下得到正确答案，直到 13 亿参数规模的模型；同样，GPT-3 和其他模型在达到大约 700 亿或更多参数后，发现它们在回答关于各种学术主题的问题（包括数学、历史和法律）的能力显著提高。因为这些新兴能力不遵循规模定律，所以很难确定更大的规模是否会促进更大的能力，在什么规模下改进会停止，甚至如何与那些准确性可预测地映射到模型规模的任务相比来推理这些任务。

人工通用智能的火花？

根据微软团队的一项评估，“除了对语言的精通之外，GPT-4 还能够解决跨越数学、编码、视觉、医学、法律、心理学等多个领域的创新和困难任务，而无需任何特殊提示” [[2](http://arxiv.org/abs/2304.15004)]。这些涌现能力使他们大胆地将论文标题定为“人工智能的火花”，并写道：“鉴于 GPT-4 的能力深度和广度，我们认为它可以合理地被视为一个人工通用智能（AGI）系统的早期（尽管仍不完整）版本。”AGI 一直是许多 AI 科学家长期追求的目标，它被理解为可以像人类一样学习，而人类在历史上一直擅长于概括知识和适应未见问题。关于 AGI 的问题，以及任何 LLMs 是否拥有它，超出了本章的范围，但我们将它在第九章中讨论和相关问题。

### 涌现是幻觉吗？

尽管有几项研究记录了涌现能力的证据，但在机器学习社区中关于涌现的共识尚未形成。斯坦福大学的一组计算机科学家认为，这些所谓的涌现能力之所以出现，更多的是因为模型在特定尺度上的行为发生了某些定性变化，而不是因为研究人员评估模型的方式 [[2](http://arxiv.org/abs/2304.15004)]。特别是，某些任务中涌现的特征是性能的急剧增加，这似乎至少部分归因于任务中使用的度量标准的选择、用于评估的测试数据量（因为使用较少的数据会对模型性能的估计造成更多的噪声），以及评估中大规模模型的数量（因为可用的较大规模模型比小规模模型少）。换句话说，作者并不否认 LLMs 在这些任务上的实际性能，只是认为在声称涌现能力的情况下，LLMs 与之前的版本相比代表了一种根本性的变化。涌现行为取决于选定的性能指标，而且虽然不清楚哪个指标更好，但在我们假设*其他*能力可能会随着更多或不同的数据以及更大的模型而轻易出现之前，我们应该保持谨慎。

## 训练数据中有什么？

如我们之前所讨论的，大型语言模型（LLMs）是在来自网络的*大量*非精选数据上训练的。这些 LLMs 到底被喂食了多少信息？相当多。通用 LLM GPT-3 是在 45 太字节（TB）的文本数据上训练的 [[3]](https://arxiv.org/pdf/2005.14165.pdf)，其中 1TB 通常估计包含 7500 万页 [[4]](https://cloudnine.com/ediscoverydaily/electronic-discovery/ediscovery-best-practices-perspective-on-the-amount-of-data-contained-in-1-gigabyte/)。当与难以估量的非精选和未记录的训练数据一起工作时，没有人确切知道数据包含什么，这导致 LLMs 编码和放大了刻板和贬义的联系，有时甚至包含敏感数据，如个人可识别信息（PII）。在本节中，我们将更多地讨论在难以衡量的文本数据上训练语言模型所面临的挑战。

### 编码偏差

沿着性别、性取向、种族、民族、宗教、年龄和残疾状态等路线持续有害的刻板印象和歧视性语言是 LLMs 中已记录的一种伤害形式 [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)。基于互联网的数据集由于不同的原因编码了偏见和有害的刻板印象。首先，这些联系在很大程度上是训练数据中发现的特征的反映。在这里，当 LLM 为了生成类似人类的文本而学习语言的特性和模式时，它也继承了类似人类的偏见、历史不公和文化联系，这些可能是有害和冒犯性的。其次，训练数据缺乏多样性。数据集可能存在偏见，因为某些社区可能比其他社区有更好的代表性，而且数据集可能不能广泛地代表不同群体如何看待世界。第三，社会观点的发展和变化可能导致 LLMs 错误地代表社会运动。

在第一章中，我们简要讨论了词嵌入如何反映社会中存在的差异。在一项关于词嵌入偏见的早期研究中，作者考虑了使用词嵌入来确定这种潜在影响的 NLP 应用 [[6]](https://doi.org/10.1126/science.aal4230)。首先，他们研究了情感分析，它将文本分类为积极、消极或中性。任务是计算电影评论的情感得分，这有助于营销目的。他们的结果显示，与非洲裔美国人名字相比，包含欧洲裔美国人名字的电影评论平均具有更积极的情感得分，即使评论在其他方面相似；也就是说，情感得分在电影评论中表现出对角色和演员名字的种族偏见。接下来，他们研究了机器翻译，他们得出结论，从许多性别中性的语言翻译成英语会导致性别刻板印象的句子。在他们的论文中，他们展示了 Google 翻译如何将土耳其语中无性别代词的句子翻译成英语：“*O bir doktor. O bir hemşire.*” 到 “He is a doctor. She is a nurse.”

同样，LLMs 不仅强化了刻板印象，还放大了它们。在一项探讨语言模型中宗教偏见的研究中，作者确定 OpenAI 的 GPT-3 捕获了穆斯林-暴力偏见以及反犹太偏见 [[7]](https://arxiv.org/pdf/2101.05783.pdf)。他们表明，包含“穆斯林”一词的提示词中有 23% 的时间会映射到“恐怖分子”，而“犹太人”有 5% 的时间会映射到“金钱”。他们进一步表明，将提示词中的“穆斯林”替换为其他宗教群体可以显著减少 GPT-3 包含与暴力相关的关键词和短语。

在大型语言模型（LLMs）中，歧视性的性别、种族、职业和宗教偏见也被夸大了。在由 GPT-3 生成的虚构故事中，发现女性角色与男性角色相比，被描述为力量较弱，以及更可能与家庭和外表相关 [[8]](https://aclanthology.org/2021.nuse-1.5.pdf)。其他 LLMs，如 BERT 和 GPT-2，也表现出强烈的刻板印象偏见。例如，对 *非洲* 的属性词被发现是 *贫穷* 和 *黑暗*，而 *软件开发者* 的属性词则是 *极客* 和 *书呆子* [[9]](https://aclanthology.org/2021.acl-long.416.pdf)。

现在，让我们看看 LLMs 中持续存在偏见的第二个案例：训练数据集中缺乏多样性。正如我们之前讨论的，数量并不等于质量。为了全面代表不同个人或群体的观点和价值观，训练数据集必须多样化，并且广泛地代表来自不同社区的观点。在论文“关于随机鹦鹉的危险：语言模型可以太大吗？”中，作者们探讨了几个因素，他们认为在这些因素中，人们的声音在语言模型的训练数据集中没有得到平等的代表 [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)。正如我们所知，Reddit 和维基百科是训练 LLMs 的两个广泛使用的数据集。作者们讨论了 Reddit 用户中有 67%是男性，其中 64%年龄在 18 至 29 岁之间，而类似地，只有 8.8%至 15%的维基百科编辑是女性或女孩。他们还讨论了过滤数据集的常见做法，例如 Common Crawl 数据集，这进一步削弱了代表性不足的社区的声音。例如，在 GPT-3 的训练中，Common Crawl 数据集通过寻找与 Reddit 和维基百科数据集相似的文档进行过滤，然后通过移除包含与性、种族诽谤或白人至上主义相关的 400 个单词列表的任何页面进行额外的过滤。作者们认为，虽然这可能是一种有效过滤某些类型色情和仇恨言论的策略，但它无意中压制了边缘化群体，如 LGBTQ 人群的言论。

作者们还讨论了随着社会运动的不断变化所带来的挑战，在这些运动中，观点可能在线讨论中被过度代表或根本未被捕捉到，而这最终是 LLMs 训练所依赖的数据。在具体的一个例子中，研究人员发现，维基百科上对“黑人的命也是命”（BLM）运动的“加强文档”强化了 BLM 关于警察暴力是美国系统性问题的主张 [[10]](https://dl.acm.org/doi/pdf/10.1145/2998181.2998232)。在运动将新的关注点引向这个问题之前，关于警察暴力的维基百科数据，由孤立案例组成，可能讲述了一个不同的故事。当然，当训练数据不经常更新时，这尤其令人担忧，考虑到 LLMs 的训练既耗时又计算量大，这很可能是不切实际的。

在巴斯大学和普林斯顿大学的一项联合研究中，研究人员展示了为什么解决机器学习中的偏见是一个具有挑战性的问题 [[6](https://doi.org/10.1126/science.aal4230)]。首先，他们表明偏见与意义相同，因此不包含人类偏见就无法有意义地使用语言。其次，他们讨论了为什么算法上定义偏见同样是不可能的，因为我们对它的社会理解是不断演变的，并且在不同文化之间存在差异。最后，他们展示了偏见也可能是历史不平等的结果，在某些情况下这可能很重要地表示出来。

为了消除词嵌入和语言模型中的偏见，人们已经做出了努力，最常见的是关于性别。为了减少词嵌入中的偏见，你可以通过去除它们的性别关联来改变中性词的表示。例如，如果我们有“护士”这个词，它更可能与“女性”相关联，那么它将在“男性”和“女性”之间被平等地移动 [[11](https://arxiv.org/pdf/1607.06520.pdf)]。在 2022 年，一组研究人员调查了针对性别、宗教和种族偏见的五种语言模型去偏技术，他们确定，不仅当前的去偏技术对非性别偏见的效果不佳，而且它们还导致了对语言建模能力的下降 [[12](https://arxiv.org/pdf/2110.08527.pdf)]。尽管这是一个崇高的努力，但算法上从语言模型中消除偏见是极其困难的，因为它也消除了意义和信息，给模型提供了一个不完整的世界图景，将去偏变成了“盲目中的公平” [[6](https://doi.org/10.1126/science.aal4230)]。

如 Bender 和 Gebru 等人所论证，一个具体的路径是整理和记录语言模型的训练数据集 [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)。截至目前，大多数大型语言模型（LLM）都是基于未向最终用户提供的专有数据集混合进行训练。文档记录对于理解数据特征、减轻一些风险以及允许潜在的责任归属至关重要。我们可以通过为数据集文档编制预算，只收集可以记录的数据，来构建具有代表性且无偏见的数据库。专注于构建开源机器学习工具的公司 Hugging Face 已经开发了数据集卡片，这是数据集文档的一个良好起点，包括数据集内容、数据集中存在的任何潜在偏见以及数据集应如何使用的背景信息 [[13]](https://huggingface.co/docs/hub/datasets-cards)。Hugging Face 还发布了一个用于 ROOTS 的搜索工具，ROOTS 是一个 1.6TB 的多语言文本语料库，用于训练 BLOOM 这个 LLM [[14]](https://arxiv.org/pdf/2302.14035.pdf)。为了鼓励研究人员对大型数据集进行特征描述，该工具允许您在数据集中进行定性分析以研究训练数据。同样，通过哈佛大学 Berkman Klein 中心的 Assembly 奖学金项目成立的 Data Nutrition 项目，从食品的营养标签中汲取灵感，突出数据集中的关键成分，如元数据和人口代表性（参见[`datanutrition.org/`](https://datanutrition.org/))）。

最后，与 AI 不同，人类有特定情境的记忆和社会例子可以借鉴，这些可以用来克服种族和性别偏见。人类可以对抗他们的隐性偏见，这些偏见不需要永远根植于我们的社会中。

### 敏感信息

由于 LLM 是在来自互联网上广泛来源的难以想象的大量数据上训练的，它们有时可能包含个人可识别信息（PII），如姓名、地址、社会保障号码、生物识别数据、性取向等，即使是在公共数据上训练也是如此。一个潜在的风险是模型可能无意中“记住”它所训练的数据中的细节；也就是说，模型中的敏感信息可能反映在其输出中。如果基于专有数据集训练的模型被公开，自然会有额外的担忧。

LLM 的一个巨大漏洞是，对手可以执行*训练数据提取攻击*，恶意行为者可以查询模型以恢复敏感和可识别的信息。与大多数安全和隐私研究一样，考虑进行攻击以进行研究的风险和伦理问题很重要，因此在这个领域公开可用和发表的工作通常有限。

Google 与 OpenAI、Apple、斯坦福大学、东北大学和加州大学伯克利分校合作，展示了他们对 GPT-2 的“攻击”，以表明有可能提取模型无意中“记住”的敏感训练数据。在这里，攻击者可以向语言模型查询，从训练数据中提取*原文信息*。研究人员指出，当基于专有数据集训练的模型被公开时，训练数据提取攻击具有最大的潜在危害，但他们承认，在这样一个数据集上进行研究目的的攻击也可能产生有害后果。考虑到这一点，他们选择了 GPT-2，因为其训练数据集收集过程有记录，并且仅使用公共互联网资源。他们能够提取数百条原文信息，包括个人身份信息（姓名、电话号码、电子邮件地址）、即时通讯对话、代码和通用唯一标识符（UUID）。尽管这些例子在训练数据集中出现的频率很低，甚至只有一份文档中出现过，但大多数例子仍然被记住，而且发现更大的模型比小模型更容易受到这些攻击[[15]](https://arxiv.org/pdf/2012.07805.pdf)。另一项研究“秘密分享者”表明，对于大型语言模型（LLM）来说，无意中的记忆是持续的，难以避免[[16]](https://arxiv.org/pdf/1802.08232.pdf)。他们演示了对安然电子邮件数据集的攻击（见[`mng.bz/K9AZ`](http://mng.bz/K9AZ)），该数据集包含安然公司员工之间发送的五十万封电子邮件。该数据集在联邦能源监管委员会调查期间被公开并在线发布。研究人员使用安然电子邮件数据集来训练语言模型，并表明他们能够轻松地提取信用卡号和社会安全号码。

减缓此类问题的最直接方法是在实践中确保模型不训练任何敏感或 PII（个人身份信息）数据。然而，这极为困难，并回到了我们之前提到的为语言模型整理和记录数据集的问题。其他解决方案包括*隐私保护*或*隐私增强技术*（PETs），这些技术可以帮助缓解数据隐私和安全风险 [[17]](https://royalsociety.org/-/media/policy/projects/privacy-enhancing-technologies/Protecting-privacy-in-practice.pdf)。PETs 的例子包括匿名化、混淆、净化和数据屏蔽的方法。在实践中使用这些技术的例子是为可能的敏感序列创建黑名单，以从训练数据集中过滤出可能包含的个人信息。然而，正如“秘密分享者”所展示的，黑名单永远不是一种完整的方法，并且不会显著减少任何已出现序列的意外记忆效果。2000 年代初引入的差分隐私是一种流行的 PET，它试图通过数据集进行训练而不透露任何单个训练样本的细节。在这里，想法是为给定数据集中的个体身份添加统计*噪声*以掩盖其身份。但这项技术也有其局限性，因为它无法防止对在数据集中不常重复的内容的记忆。在《超越数据：元宇宙黎明时的人权恢复》一书中，作者指出，PETs 不仅技术含量高、使用复杂、昂贵且资源密集，而且对立法者和政策制定者来说，审计或监管也具有挑战性 [[18]](https://books.google.com/books/about/Beyond_Data.xhtml?hl=&id=zJZuEAAAQBAJ)。

隐私保护或隐私增强技术（PET）是用于描述可以帮助缓解隐私和安全风险的方法的通用术语。

考虑到当前 PET 方法的局限性，我们希望提高对此挑战的认识将鼓励研究人员开发新技术来解决这个问题，并在此基础上测试 LLMs（大型语言模型）的意外记忆，以便我们能够适当地应对这个问题。

## 摘要

+   LLMs 可能是在开源或公共数据集、从第三方供应商购买的数据集、公司通过爬取网络自行收集的数据集，或公司通过为模型编写学习示例自行创建的数据集上训练的。

+   *自回归*模型指的是未来预测依赖于模型的历史数据。GPT 家族中的所有模型以及谷歌的 PaLM 都是自回归模型，这些模型被训练在给定一些输入的情况下预测下一个标记。

+   *零样本*和*少样本*指的是在要求模型执行任务之前，模型被给出的示例数量。它们是 LLMs（大型语言模型）涌现能力的典型例子。

+   大型语言模型（LLMs）往往编码和放大刻板和贬义的联系；它们还包含敏感数据，包括个人可识别信息（PII）。

+   一个具体的途径是整理和记录语言模型的训练数据集，这对于理解数据特征以减轻风险和允许潜在的责任至关重要。

+   攻击者可以使用 LLM 执行一种**训练数据提取攻击**，恶意行为者可以通过查询模型来恢复敏感和可识别的信息。

+   **隐私保护**或**增强隐私技术**（PETs）可以帮助缓解数据隐私和安全风险。PETs 存在一些局限性，我们希望看到研究人员在这个领域的集中努力，以便有 LLM 开发者可以轻松采用的技巧。
