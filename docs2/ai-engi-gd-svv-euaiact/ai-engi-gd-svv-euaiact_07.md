# 第七章\. 向着可信赖的通用人工智能和生成人工智能迈进

欧盟人工智能法案（EU AI Act），于 2024 年 8 月生效，是全球首个全面的人工智能法律框架。到目前为止，本书主要关注该法案基于风险的途径如何应用于预测人工智能系统。然而，该法案还包括了针对**通用人工智能**（GPAI）的规定——这一补充是在大语言模型（LLMs）如 GPT 和 BERT 的快速崛起以及生成人工智能技术在该法案起草期间更广泛出现之后相对较晚加入的。这一补充旨在解决一个关键的监管挑战：如何治理那些与特定任务无关且可以适应广泛下游用途的人工智能模型。

欧盟人工智能法案旨在通过促进研究和开发来平衡创新和风险缓解，同时保护基本权利。因此，它不适用于在投放市场或投入使用之前仅用于原型设计或研究和开发目的的 GPAI 模型。一般来说，GPAI 模型和系统的规则不如高风险人工智能系统严格——除非它们被认为存在系统性风险（定义为“特定于通用人工智能模型的高影响能力，由于它们的范围或由于对公共健康、安全、公共安全、基本权利或整个社会产生的实际或合理可预见的负面影响，这些影响可以在价值链上大规模传播”）。

本章阐述了欧盟人工智能法案对通用人工智能和生成人工智能合规性的要求。它还介绍了 GenAIOps 的概念——将 MLOps 原则应用于生成人工智能应用的开发和部署。与前面的章节一样，重点将放在如何将人工智能工程原理和实践应用于满足欧盟人工智能法案在此快速发展的领域中的透明度义务。

# 欧盟人工智能法案与生成人工智能

“通用”AI 的概念（也称为“基础模型”）在欧盟 AI 法的谈判过程中相对较晚被引入。在立法过程中，像 ChatGPT、DALL·E 和 Midjourney 这样的生成式 AI 系统的快速出现和广泛应用揭示了，该法案最初对具有特定、可识别风险的 AI 系统的关注过于狭窄，这些风险源于明确定义的使用案例。通用 AI 被添加为一个独特的类别，主要是为了解决与训练数据的质量控制和版权保护相关的担忧，同时允许评估系统性风险，如法案第 V 章所述。这些模型的管理规定范围相对有限，主要关注对文档和透明度的要求（第 53 条）以及与相关当局合作的义务（第 91 条）。

生成式 AI，或 GenAI，提供了许多潜在的好处，包括增强决策能力、提高生产力和生成新颖内容的能力。GenAI 技术可用于广泛的领域，如教育、研究和客户服务，以自动化任务并提高运营效率。

然而，这些技术也带来了显著的风险。这些风险包括潜在的偏见和歧视、生成不准确或误导性信息，以及各种形式的滥用（例如，创建欺骗性内容或深度伪造）。滥用往往分为两大类：^(1)

利用 GenAI 能力

利用 GenAI 模型的特性来创建有害输出或支持恶意活动的策略。例如，AI 机器人电话可以模仿真实人物并代表他们采取行动，合成内容可以生成以创建虚假社交媒体账户来推广特定议程或伪造身份证明文件。

对 GenAI 系统的妥协

涉及攻击或操纵 GenAI 系统、针对模型或数据完整性漏洞的策略。例如，包括提示注入（操纵模型提示以启用非预期或未经授权的输出）；侵犯训练数据的隐私以提取个人信息；绕过对模型安全措施的限制（称为越狱）；以及模型提取（逆向工程以获取有关模型架构、参数或训练数据的详细信息）。

欧盟 AI 法旨在通过要求生成式和通用 AI 系统的提供者承担某些义务来减轻这些风险，以确保它们负责任地开发和部署。

## GPAI 系统和透明度义务

与人类直接交互的 GPAI 系统，包括生成合成内容的那部分，根据欧盟 AI 法第 50 条的规定，需要遵守以下透明度义务：

通知用户 AI 交互

当用户与 AI 系统互动时，提供者必须通知用户，除非这是明显合理的或系统用于执法目的（例如，检测、预防、调查或起诉犯罪）。这适用于聊天机器人和内容生成工具。

标记合成内容

生成合成内容（音频、图像、视频、文本）的 AI 系统提供者必须明确标记这些输出为人工生成或操纵的。标签应该是机器可读的，并且易于检测，表明内容的非真实性。此要求不适用于辅助编辑工具或未显著改变原始输入的系统。

深伪影的披露

生成深伪影（根据第 3(60)条描述为“AI 生成或操纵的图像、音频或视频内容，类似于现有的人、物体、地点、实体或事件，并会向个人虚假地呈现为真实或真实”）的 AI 系统提供者必须明确标记这些输出为人工制品。标签应该是机器可读的，并且易于检测。

## 监管通用人工智能

欧盟人工智能法案为 GPAI 模型和系统引入了特定的法规，重点关注透明度、文档和风险管理。在本节中，我将首先定义法案下构成 GPAI 模型或系统的要素，并解释确定 GPAI 模型是否呈现系统性风险的准则。然后，我将概述 GPAI 模型提供者和 GPAI 系统部署者的义务，包括被认为可能带来系统性风险的 GPAI 模型提供者的增加义务。

### 定义和范围

欧盟人工智能法案将**GPAI 模型**定义为一种具有显著通用性的 AI 模型，能够执行广泛的独立任务。这些模型通常在大量数据上训练，经常使用自监督学习技术，并设计成可以集成到各种下游系统或应用中。

法案将**GPAI 系统**定义为基于 GPAI 模型的 AI 系统，可以服务于各种目的，无论是作为独立应用程序还是作为集成到其他 AI 系统中的组件。提醒一下，法案将 AI 系统定义为“一种基于机器的系统，旨在以不同的自主程度运行，并在部署后可能表现出适应性，并且为了明确或隐含的目标，从它接收的输入中推断出如何生成输出，如预测、内容、推荐或决策，这些输出可以影响物理或虚拟环境。”

区分模型及其所驱动的系统非常重要。法案规范了 GPAI 模型的提供者和使用或集成这些模型到 AI 系统中的部署者。GPAI 系统部署者的义务通常与其他 AI 系统相同，欧洲委员会的 AI 办公室被授予特定的监管权力。

### 系统性风险准则

欧盟 AI 法案为 GPAI 模型建立了两级监管方法：

一般义务

所有 GPAI 模型的提供者都必须满足基线透明度和兼容性要求。

系统风险

如果 GPAI 模型被归类为具有系统风险，则将触发额外的监管监督和额外义务。此类分类的标准在表 7-1 中概述。

表 7-1\. 系统风险 GPAI 模型的标准

| 标准 | 系统风险的描述 |
| --- | --- |
| 高影响能力 | 该模型由于其广泛的功能范围，具有造成重大伤害的显著潜力。风险包括重大事故、关键基础设施的中断、对公共健康或安全的威胁、对民主进程的影响、国家或经济安全风险、生成非法或歧视性内容、降低化学、生物、辐射和核（CBRN）武器技术发展的障碍，或模型复制自身或自主训练其他模型的能力。 |
| FLOPs 阈值 | 模型在训练期间需要每秒超过 10²⁵ 次浮点运算（FLOPs）。这表明具有高影响能力的潜力。 |
| 基于等效能力或影响的委员会决定 | 即使没有达到 FLOPs 阈值，如果欧洲委员会根据附件 XIII 中的评估标准证明模型具有高影响能力的潜力，则可以将其归类为具有系统风险。 |
| 委员会考虑的额外因素 | 考虑的额外因素包括参数数量、训练数据集的质量和大小（例如，令牌数量）、用于训练的计算资源（FLOPs、成本、时间、能耗）、输入和输出模式，以及自主程度或可扩展性。 |
| 对系统风险假设的质疑 | 提供者可以通过提出充分证明的论据来质疑系统风险的假设，证明即使满足 FLOPs 标准，由于模型的特定特征，该模型实际上并不呈现系统风险。一般论据是不够的。 |

### GPAI 模型提供者的义务

所有 GPAI 模型的提供者，无论其系统风险分类如何，都受欧盟 AI 法案下的一系列一般义务约束。这些义务旨在促进强大 AI 模型的透明度、问责制和负责任的发展。对于在免费和开放许可证下提供 GPAI 模型的提供者，如果这些模型不呈现系统风险，则有一些简化的要求。核心义务（如[第 53 条](https://oreil.ly/Xn8iw)所述）包括：

技术文档

提供者必须创建和维护全面的技术文档，包括关于模型设计规格、培训和测试过程以及评估结果的详细信息。这些文档必须使监管机构能够评估模型是否符合欧盟 AI 法案，并在请求时提供给 AI 办公室和国家主管部门。

向下游提供者的文档

由于 GPAI 模型通常集成到其他 AI 系统中，欧盟 AI 法案要求提供者向下游提供者提供文档，使他们能够了解模型的能力、局限性和已知风险。例如，如果模型在特定环境中已知存在偏差，则必须披露此信息，以便下游提供者在其 AI 系统中采取适当的缓解措施。

版权合规

GPAI 模型是在大量数据上训练的，通常包括受版权保护的材料。因此，提供者必须实施一项政策，以遵守欧盟版权法（特别是[欧盟版权指令](https://oreil.ly/6l3Hq)），包括识别和尊重用于训练模型的数据的任何版权限制的措施，以及记录如何解决这些限制的文档。

训练数据摘要

为促进透明度和问责制，提供者必须发布其 GPAI 模型训练所使用数据的详细摘要。这应提供有关使用数据的类型和来源的信息，以及数据中任何已知的局限性或偏差。

欧盟代表

欧盟以外地区的提供者必须在欧盟内指定一名授权代表。此代表作为当局的联系人，并负责确保提供者遵守欧盟 AI 法案下的义务。

### GPAI 模型提供者的额外义务

由于 GPAI 模型可能造成重大危害，具有系统性风险的 GPAI 模型需遵守更严格的要求。除了上一节中概述的一般义务外，这些模型的提供者必须遵守以下义务，具体规定在[第 55 条](https://oreil.ly/2fYcc)中：

模型评估

定期使用标准协议和工具对模型进行严格的评估。这包括对抗性测试以识别漏洞和潜在风险。目标是确保模型稳健、可靠，且不构成不可接受的风险。

风险缓解

积极评估和减轻与模型使用相关的潜在系统性风险。识别与基本权利、健康和安全或安全相关的潜在危害，并实施记录在案的措施以最小化这些风险。

事件报告

建立一个跟踪、记录和报告与模型使用相关的严重事件的系统。事件必须报告给人工智能办公室和相关的国家当局，并采取适当的纠正措施并记录在案。

网络安全

实施强有力的保障措施来保护模型及其运行的基础设施。这对于防止未经授权的访问、数据泄露和恶意使用至关重要。

### GPAI 模型部署者的义务

欧盟人工智能法案适用于所有在其专业能力下自主运营人工智能系统的部署者。当部署者将 GPAI 模型集成到其人工智能系统中时，他们可能根据该法案承担额外的责任，尤其是如果生成的系统被归类为高风险时。这些义务包括：

按预期使用系统

系统必须按照提供商的说明和预期目的使用。

人类监督

必须建立适当的人类监督机制——例如，对系统输出的人类审查、干预系统操作的能力，以及对系统决策责任的明确分配。

数据质量

提供给系统的输入数据必须是相关、准确和具有代表性的。这对于防止产生有偏见或不准确的结果，并确保系统按预期运行至关重要。

监控

必须监控系统性能，并向提供商报告任何问题。这包括对意外输出、性能下降或其他表明系统未正确运行的迹象进行监控。

记录保存

必须维护系统日志，用于事故调查和证明合规性。

透明度

生成或操纵发布以向公众通报公共事务内容的人工智能系统必须明确披露该内容是合成的。

# 预测性机器学习与 GPAI

在我们转向实施欧盟人工智能法案对 GPAI 的合规性之前，让我们简要地反思预测性和生成性人工智能之间的差异，以便更好地理解生成性人工智能如何偏离以及为什么会出现 GenAIOps 这一学科。

机器学习是一个广泛的领域，但就其核心而言，大多数模型可以分为两个不同的范例：预测性机器学习和 GPAI（参见 7-1 和 7-2 图中的具体示例）。虽然两者都依赖于统计学习，但它们在目标、方法和应用上有所不同。

![图片](img/taie_0701.png)

###### 图 7-1\. 预测性机器学习的一个可视化——一个训练用来预测给定图像是否由文森特·梵高绘制的判别模型。图片来自大卫·福斯特所著的《生成深度学习，第 2 版》一书（https://oreil.ly/L5njI）。经许可使用。

![图片](img/taie_0702.png)

###### 图 7-2\. 生成式 AI 的视觉展示——一个训练生成马的真实照片的生成模型。图片来自 David Foster 所著的《生成深度学习，第 2 版》（https://oreil.ly/L5njI），经 O’Reilly 授权使用。

在其核心，预测机器学习是关于回答“可能是什么？”的问题。它专注于估计、预测和模式识别。预测模型通过分析历史数据来做出明智的预测或决策。它们从结构化数据集中学习模式，并旨在为输入特征提供数值输出、分类或推荐。

GPAI，另一方面，是一个包含许多生成模型（尤其是基础模型如 LLMs）的范畴。因此，生成式 AI 可以被视为 GPAI 的一个子集。大多数最先进的生成模型（如 GPT-4、DALL·E 和 Stable Diffusion）根据欧盟 AI 法案被归类为 GPAI。这包括通用模型（用于许多任务或系统）和可供他人用于下游使用或微调的模型。

在其核心，生成式 AI 是关于回答“可能是什么？”的问题。它超越了预测，转向创造新的内容。生成模型不是将输入映射到预定义的输出，而是学习其训练数据的统计分布，并生成新的、合理的、落在该学习分布内的数据点。

表 7-2 提供了预测机器学习和 GPAI 的结构化比较。

表 7-2\. 预测机器学习和 GPAI——并列比较

| 特征 | 预测机器学习 | GPAI |
| --- | --- | --- |
| 主要目标 | 解决一个定义明确、范围狭窄的任务（例如，欺诈检测、贷款评分） | 提供广泛的能力，可以适应许多任务和用例 |
| 数据输入 | 领域特定的结构化或非结构化数据（例如，表格数据、图像） | 巨大且多样化的跨领域数据集（例如，互联网文本、代码、音频、图像） |
| 输出 | 对特定任务的预测、分类、回归或推荐 | 多样化的输出：文本、代码、推理、视觉任务、语言理解等 |
| 下游使用 | 为已知用户和应用程序量身定制的系统 | 可由第三方适应和微调，用于多样化的和不断发展的应用 |
| 例子 | 预测性维护模型、欺诈检测系统 | GPT-4、Gemini、LLaMA、Mistral、DALL·E |
| 算法 | 决策树、XGBoost、CNNs、RNNs | 基于大型 Transformer 架构、基础模型（例如，LLMs、视觉语言模型） |
| 评估 | 任务特定指标：准确率、F1 分数、AUC-ROC、MAE/RMSE | 在广泛的任务中使用通用基准（例如，MMLU、HELΜ），评估鲁棒性、偏差、毒性 |

# GenAIOps——实施欧盟 AI 法案对 GPAI 的合规性

GenAIOps 是 DevOps 和 MLOps 原则的系统扩展，以应对开发、部署和维护 GPAI 模型和系统（包括 LLMs、图像和视频生成器以及其他基础模型）的独特挑战。虽然传统的 MLOps 侧重于管理预测模型，但 GenAIOps 必须支持用于广泛下游任务的强大生成模型的适应和治理。这包括提示工程、检索增强生成（RAG）、基础模型微调和针对特定下游用例的实时监控，如幻觉、偏差和安全风险。这些需求在数据治理到基础设施、安全监控和负责任使用等方面引入了额外的复杂性——特别是在符合欧盟 AI 法案等监管框架的背景下。

图 7-3 展示了 GPAI 模型的操作阶段以及它们如何映射到 CRISP-ML(Q)开发阶段。

![](img/taie_0703.png)

###### 图 7-3\. GPAI 模型和系统的 CRISP-ML(Q)框架

## 关键组件

GenAIOps 将自动化管道、模型版本控制、可观察性与推理优化、安全防护和合规工作流程集成，以创建一个全面的责任 GPAI 部署框架。这种方法确保 GPAI 模型在生产环境中以透明和可审计的方式运行，同时解决关于虚假信息、合成内容归属和符合适用监管要求的问题。

GenAIOps 的关键组件包括以下内容（总结于图 7-4）：

提示工程基础设施

管理提示创建、测试、版本控制和编排的工具，特别是针对复杂的推理链和 RAG 系统。

推理优化

通过量化、缓存和模型蒸馏等技术来减少部署环境中的延迟、成本和资源使用。

持续评估

实时监控系统以检测幻觉和性能漂移，以及收集人类反馈以持续改进模型的机制。

安全和内容控制

自动内容过滤、水印、红队协议和风险评估工作流程，以确保负责任和安全的 AI 输出。

管理和合规

透明文档（例如，模型卡片）、内容溯源系统和全面的审计跟踪，以满足不断变化的监管要求和道德标准。

基础模型适应

微调工作流程、从人类反馈中进行强化学习（RLHF）和参数高效方法（例如 LoRA）以定制特定下游用例的模型。

![](img/taie_0704.png)

###### 图 7-4\. GenAIOps 的关键组件

## GenAIOps 如何扩展 MLOps

MLOps 的核心目标，主要关注预测性机器学习模型的生命周期，包括可重复性、可扩展性和持续交付。GenAIOps 保留了这些目标，但将其适应于 GPAI 的具体需求。例如，生成式 AI 应用必须在规模上处理非结构化数据，整合提示工程和人工反馈机制，并确保输出不仅质量高，而且安全、透明且适当标记。表 7-3 概述了 MLOps 和 GenAIOps 在多个领域的关键区别，例如数据管理、模型训练和适应、模型部署和推理、监控以及伦理和法规。

表 7-3\. MLOps 与 GenAIOps 的关键区别

| Aspect | MLOps（预测性 ML） | GenAIOps（GPAI） |
| --- | --- | --- |
| 数据管理 | 专注于精心整理、标记的数据集、特征工程和训练数据管道的版本控制。数据通常是结构化和表格化的，或限于特定任务的语料库。 | 处理大规模、非结构化数据集（文本、图像、音频）。接受合成数据生成以增强数据，并需要强大的数据过滤来从训练语料库中移除有毒或偏见的内容（以避免放大危害）。使用嵌入管理（数据的向量表示）代替传统的特征存储，以实现 RAG，通常通过向量数据库实现。 |
| 模型训练和适应 | 从头开始训练模型或使用迁移学习，基于特定任务的数据进行超参数调整和模型选择。模型通常较小，并训练以收敛到标记数据。 | 从大型、预训练的基础模型开始。使用参数高效的微调（例如，LoRA、适配器层）、提示工程和少样本学习，而不是完全重新训练。应用直接偏好优化（DPO）和 RLHF 等技术，以使模型行为与人类偏好一致。实验跟踪必须包括提示版本和链配置，而不仅仅是模型参数。 |
| 模型部署和推理 | 模型作为微服务或 REST API 背后的批处理作业部署，使用标准扩展、CI/CD 和版本控制。推理通常涉及单个模型的预测调用（例如，分类 API）。 | 部署需要专门的硬件基础设施（GPU 或 TPU，高内存使用）。专注于生成式输出的实时推理，通常由于输出较长（例如，聊天机器人）而有流式响应。经常涉及协调多个模型和工具，以及管理提示管道和会话上下文。应用模型量化、输出缓存和负载均衡等优化技术来管理重推理负载。 |
| 监控和持续改进 | 在生产中监控模型性能（准确性、延迟）和数据漂移。用户反馈可能触发定期重新训练或模型更新。回滚机制在出现故障时最小化停机时间。当使用第三方模型时，可能需要自定义评估集来检测版本之间的行为变化。 | 除了标准性能指标外，还监控输出质量和安全风险（幻觉、毒性、偏见）。需要持续评估循环和人工反馈来指导改进。 |
| 监管和伦理考量 | 重点关注决策环境（例如，贷款审批）中的公平性、偏见缓解、可解释性和合规性。模型伴随全面的文档（例如，模型卡片、偏见审计）。 | 面临新的伦理和监管挑战：GPAI 模型可能产生错误信息、有害内容、深度伪造，并且可能被用于冒名顶替。必须支持新兴要求，例如标记 AI 生成内容，并确保透明性和可追溯性。 |

## 支持透明度的 GenAIOps 工具、工作流程和框架

GenAIOps 在 MLOps 的基础上增加了提示管理、思维链编排、合成数据生成和严格的输出控制等功能，以应对 GPAI 的独特需求。这对于确保 GPAI 模型和系统的透明性也至关重要，这是欧盟 AI 法案等法规所要求的。法案第 50 条中概述的透明度义务，我们在第六章中详细讨论过，适用于生成式 AI 系统的提供者和部署者（以及预测性机器学习系统）。满足这些义务并有效管理与 GenAI 相关的风险需要数据工程师、模型开发者、用户体验设计师和伦理合规团队之间的跨学科合作。以下 GenAIOps 工具和工作流程可以支持这一努力：

即时和链式管理

可以使用 LangChain 和 NVIDIA NeMo 等框架来编排复杂的提示工作流程，并确保所需系统提示（包括 AI 自我识别）在用户交互中始终一致。这些工具还支持提示版本控制、测试和可追溯性。

内容水印和元数据

符合 C2PA（内容溯源和真实性）标准的库和 API 正越来越多地集成到图像和视频生成管道中。例如，OpenAI 的 DALL·E API [自动添加 C2PA 元数据](https://oreil.ly/djcGb)到其输出中。还有第三方服务，如 Stability 的 Stable Diffusion 插件或[Steg.AI](https://oreil.ly/PGrpq)，它们将不可见的水印嵌入到图像中，以便稍后进行验证。GenAIOps 管道可以在生成后调用这些服务。对于基于文本的内容，统计水印的研究也在不断进步。

监控和检测服务

为了补充水印，GenAIOps 可以将 deepfake 检测模型和内容审核 AI 集成到监控堆栈中。例如，企业可能会部署一个视觉模型，用于扫描新上传的视频中的操纵迹象。如果 AI 生成的媒体缺少必要的披露，检测系统可以将其标记为需要审查。云服务提供商开始提供用于此目的的 API（例如，Azure 的视频验证器和 AWS 的 Rekognition）。

防护栏和政策执行

可以使用 NVIDIA NeMo Guardrails 或开源 Guardrails AI 库等防护栏框架，通过在模型输出到达用户之前拦截模型输出来在运行时执行策略。它们可以自动添加免责声明或阻止应标记为合成但未标记的内容。例如，如果用户提示 GPAI 模型生成一个人的假图像，防护栏可以将“假图像”标签附加到输出上。同样，可以在音频内容中注入可听提示。这些执行步骤直接在推理管道中配置。

记录和版本控制

使用支持 GenAIOps 功能的平台（如 MLflow、Weights & Biases 或 Arize AI）可以全面记录提示、响应和内容工件。这些日志提供了强大的审计跟踪，可以验证，例如，水印功能是否运行以及它产生了什么输出。

在本章的剩余部分，我们将探讨上一章中引入的 SMACTR 框架如何集成到 CRISP-ML(Q)生命周期的不同阶段，为 GPAI 模型和系统的开发和部署中的合规性操作提供实际基础。

# 将 AI 工程与 SMACTR 和 CRISP-ML(Q)对齐以实现透明度

第六章为 AI 工程师提供了实现欧盟 AI 法案第 50 条规定的透明度要求的主动合规性的详细指南。本节将相同的做法应用于 GPAI 模型和系统，并纳入 GenAIOps 原则。通过将 SMACTR 框架与 CRISP-ML(Q)方法相结合，团队可以建立一套稳健、可审计的过程，以负责地开发和部署通用 AI。

## 商业和数据理解阶段

GenAIOps 通常分为三个主要操作阶段（见图 7-3）。初始阶段包括与用例定义和数据需求规划相关的任务，这些任务在全面数据工程之前进行。它还包括早期数据工程活动，如明确应用程序的目的和确定数据来源。例如，在构建一个由 LLM 驱动的系统之前，团队必须定义业务问题（例如，“客户支持聊天机器人”与“代码生成助手”），并评估解决该问题所需的数据和模型能力。这与 CRISP-ML(Q)方法中的业务和数据理解阶段紧密一致。

第一步是将业务目标转化为机器学习/大型语言模型（LLM）目标——也就是说，确定你希望模型做什么，以及需要哪些数据或基础模型。例如，如果目标是开发一个问答聊天机器人，GenAIOps 团队必须决定是使用现有的问答模型还是微调基础 LLM，并确定相关的领域数据。这反映了 CRISP-ML(Q)强调业务目标和数据约束应共同考虑，以避免构建“对错问题的正确答案”。

在目标、成功标准和约束方面的早期一致，塑造了所有后续工作。CRISP-ML(Q)要求在业务、机器学习和系统层面定义成功指标。GenAIOps 通过要求从开始就解决用户体验目标和伦理考量来扩展这一点——包括对模型行为、语气和安全性的预期。

### 实施

在这个阶段将 CRISP-ML(Q)实践整合到 GenAIOps 中意味着严格记录需求、约束和成功标准。在实践中，工程团队应在开发开始之前召集领域专家、数据科学家和 LLM 工程师，定义 GenAI 应用程序的范围。成功标准将超越经典指标如准确度或 ROI，包括输出质量（例如，流畅性、相关性）和安全性的衡量（例如，不存在有害或偏见的内容）。早期可行性评估至关重要；如果所需的 dataset 不可用或任务不适合 LLM，这一点应在前期确定。

在 GenAIOps 中，此阶段通常涉及选择一个适合任务的初始基础模型。例如，选择专有的大型语言模型（LLM）如 GPT-4 或较小的开源模型将取决于业务需求和约束，如成本、延迟和数据隐私。与预测性机器学习项目中的算法选择类似，GenAI 中的基础模型选择是一个关键决策，应受问题范围和数据需求指导。例如，如果应用涉及敏感或特定领域的内 容，则可能更倾向于使用在专有数据上训练的小型微调模型，而不是通用的大型语言模型。记录决策及其理由将有助于在项目进展过程中保持清晰和透明。

### SMACTR 集成

SMACTR 框架中的范围界定和映射阶段在此初始阶段最为相关。

在范围界定阶段，团队进行初步风险评估和对预期用例的伦理审查。对于一个 GenAI 项目来说，这包括询问应用可能如何影响用户或利益相关者（例如，“聊天机器人可能会给出有害的建议或泄露私人数据吗？”）。定义项目将遵循的伦理人工智能原则也是此阶段的一部分。关键输出，如社会影响评估，有助于确保应用的设计考虑并采取措施减轻潜在危害，并与组织的价值观和人工智能伦理指南保持一致。

在映射阶段，重点转向识别利益相关者和合作者。在 GenAIOps 的背景下，这意味着确定需要参与的人员——例如，数据所有者以获取数据源，如果涉及用户数据或知识产权，则可能需要合规或法律专家，以及最终用户代表。此利益相关者图通过明确谁负责哪些输入和决策，促进了问责制和可追溯性。

将范围界定和映射纳入业务和数据理解阶段有助于 GenAI 项目团队在收集任何数据或开始模型工作之前减轻战略风险，减少构建不匹配或不安全产品的可能性。

表 7-4 提供了在此阶段应产生的关键工件的概述。

表 7-4\. 业务和数据理解阶段产生的工件总结

| SMACTR 阶段 | 关键工件 |
| --- | --- |
| 范围界定 |

+   初步风险评估

+   对预期用例的伦理审查

+   定义项目中的伦理人工智能原则

+   与企业价值观和人工智能伦理指南的一致性声明

|

| 映射 |
| --- |

+   识别利益相关者和合作者（例如，数据所有者、合规/法律专家、最终用户）

+   利益相关者图

|

## 数据准备阶段

在 GenAI 的背景下，CRISP-ML(Q) 的数据准备阶段包括收集大规模文本数据、清理和过滤它以及为模型输入做准备（例如，执行分词和格式化）等步骤。如果 GenAI 解决方案涉及微调 LLM 或训练特定领域的模型，此阶段将包括组装微调数据集和任何提示模板。它还可能涉及设置用于检索增强生成的知识库，这是 GenAIOps 中独特的数据组件，在传统的机器学习管道中通常不存在。数据准备阶段的目标保持不变——确保输入到模型中的数据是高质量的、相关的且适当准备好的——无论是用于经典机器学习的表格数据还是用于 LLMs 的非结构化文本和提示数据。

数据质量直接影响模型的成功。CRISP-ML(Q) 强调通过选择合适的数据、清理以去除噪声、归一化、特征工程和标准化格式来稳健地处理数据。GenAIOps 将 MLOps 扩展到解决与 GenAI 模型一起工作的独特挑战。一个主要区别是，对于大型语言模型 (LLMs) 来说，传统的特征工程是不必要的。相反，GenAIOps 专注于提示工程和数据整理——例如，制作提示示例或结构化输入/输出对以教会模型一项任务。另一个关键区别是规模和多样性：GenAI 应用通常需要大规模数据收集，强调多样性和代表性，以避免偏差或盲点。如果需要为微调或 RLHF 数据进行标记，GenAIOps 可能会使用半自动技术，例如使用另一个模型进行预标记或主动学习。

### 实施

数据准备涉及实施系统性的数据质量检查和创建详尽的文档。在实践中，工程团队应将文本语料库或提示数据集视为与精心制作的机器学习数据集相同的学科。关键活动包括：

数据选择和来源

从多个来源（例如，内部文档、公共数据集、网络抓取的文本）识别和收集与用例相关的数据。GenAIOps 强调自动化此过程，并在需要最新信息的应用中持续摄取新数据。

数据清洗

过滤掉有问题的内容，例如粗俗语言、个人可识别信息或冒犯性文本，以防止模型学习到不希望的模式。MLOps 和 GenAIOps 都强调数据集异质性的重要性，以减少偏差和过拟合。像 [Data-Juicer](https://oreil.ly/5wr6H) 这样的工具可以帮助审计数据集的多样性。

数据结构

在传统的机器学习中，这包括诸如归一化值或格式化逗号分隔值等任务。在生成式 AI 中，这包括对原始文本进行分词、分割文档以及添加提示前缀或后缀。它还可能涉及数据集级别的提示工程——即构建用于微调或评估的任务输入/输出对。

嵌入

GenAI 数据准备应考虑动态数据，这意味着如果应用程序将在运行时检索文档（RAG），团队将需要设置索引向量数据库和管道以更新新内容。这是 GenAIOps 中额外的运营考虑因素，以保持知识库的更新。

文档

对于 GenAIOps，记录数据来源、预处理步骤和数据集已知限制的细节至关重要。这支持透明度（例如，通过数据集的数据表）并有助于确保在以后出现问题时可审计性。

### SMACTR 集成

SMACTR 的工件收集阶段在数据准备期间尤其相关。随着团队整理数据集，他们还应编制审计工件，包括一个全面的清单以验证所有必需的文档都已到位，以及捕获数据来源、假设等的数据表。这通过设计实现质量保证。在建模开始之前，审计员或内部质量保证审查员应确认数据集符合既定标准，以及任何潜在的偏差或数据限制都已记录。对于 GenAI 项目，这一步骤有助于揭示训练数据中的偏差问题（例如，聊天机器人训练数据中某些用户群体的代表性不足），以便加以解决（例如，通过增加更具代表性的样本来扩充数据集）。

范围界定阶段的见解也反馈到数据准备中。例如，如果识别出隐私风险，数据管道应纳入适当的缓解步骤，如数据最小化或匿名化。

最后，团队应开始规划如何在测试中处理与数据相关的风险。SMACTR 的测试阶段通常借鉴已记录的问题和失败案例，因此，在数据准备阶段识别出的任何担忧都应予以记录（例如，“我们的数据集可能不包含俚语——聊天机器人可能的失败模式”）。

表 7-5 总结了在数据准备阶段应产生的关键工件。通过将数据视为可审计的工件，并支持清单、数据表和偏差分析，GenAIOps 可以在管道早期就纳入 SMACTR 的问责制。

表 7-5\. 数据准备阶段产生的关键工件概要

| SMACTR 阶段 | 输出 |
| --- | --- |
| 工件收集 |

+   详细说明数据来源、假设等的数据表（数据文档）。

+   数据集中识别出的偏差或限制日志

|

| 范围界定 |
| --- |

+   在准备或训练期间可能需要额外数据隐私控制的隐私风险文档

|

| 测试（在数据准备期间规划） |
| --- |

+   关于潜在数据相关风险的注意事项

+   如何在测试中处理这些风险的初步计划

|

## 建模阶段

在 GenAIOps 中，建模阶段会选择或构建基础模型，并将其适应于任务。基础模型选择和领域适应步骤包括提示工程、微调或 RAG。对于传统的机器学习项目，建模包括选择算法、调整超参数，并在准备好的数据上从头开始训练模型。在 GenAI 项目中，你通常从一个预存在的模型开始，例如一个大型预训练的 Transformer，因此“建模”更多地关于*适应*该模型：选择哪个 LLM 作为起点，决定是否微调它或通过提示使用它，并实施这些适应。例如，如果构建一个定制的聊天机器人，团队可能会选择一个基础模型如 GPT，在其领域数据上微调它，并制定提示策略。

CRISP-ML(Q)强调了建模实践，例如确保实验的可重复性、尝试多种建模技术以及将模型选择与业务目标对齐。在 GenAI 应用开发中，模型的选择以及相应的提示或微调策略必须与用例约束（延迟、准确性等）相一致，并且应该跟踪实验。生成建模中的一个关键区别是使用提示进行迭代实验。在结构化机器学习中，一旦选择了算法和特征，训练相对简单。但与 LLMs 相比，达到期望的输出通常需要交互式提示调整和调整参数，如温度设置。这意味着 GenAIOps 中的建模阶段可以非常迭代和探索性，通常与评估交织在一起。你可能提示模型，观察其输出，调整提示或使用上下文中的少量示例，然后重复。尽管如此，基础的关注点仍然是优化模型的行为以满足预定义的成功标准。

与传统机器学习相比，另一个不同之处在于涉及的规模和工具：训练一个经典的机器学习模型可能涉及 scikit-learn 管道或自定义代码，而在 GenAIOps 中，你可能可以利用专门的框架，如 Transformers 和参数高效的微调库，并在微调大型模型时利用分布式训练或服务。此外，LLMs 的复杂性意味着实验跟踪和版本控制变得更加关键。每个提示模板或微调检查点都是一个需要仔细管理的变体（以避免“管道丛林”问题）。

GenAIOps 将模型视为一个需要配置（选择和调整）的工件，而不是从头开始发明，强调配置管理和性能优化。例如，模型压缩和 GPU 优化是 GenAIOps 中“建模”的一部分。

### 实施方案

将 CRISP-ML(Q)的纪律引入通用人工智能（GenAI）应用开发意味着采用系统实验和稳健的模型管理实践，以支持符合欧盟人工智能法案等法规。实际上，这涉及几个关键活动：

模型选择和建立基线

正如数据科学家可能会测试多个算法一样，GenAIOps 团队应该在代表性样本的任务上评估不同的基础大型语言模型（LLM），例如 GPT-5、Mistral 或 LLaMA-2。目标是选择在性能与业务和技术约束之间取得最佳平衡的模型，由初始阶段定义的成功标准指导。CRISP-ML(Q)方法强调定义基线模型性能。在 GenAI 中，这可能意味着使用现成的模型生成一些输出，并在定制之前评估这些输出。

提示和微调实验

如果计划进行微调，每次训练运行都应被视为一个实验，对训练数据、代码和结果模型检查点进行适当的版本控制。同样，提示设计（如少样本或思维链设计）也应进行版本控制和文档记录。这将确保结果可以追溯到产生它们的精确模型和配置。可以将像 Weights & Biases 或 MLflow 这样的工具集成到 GenAIOps 管道中，以跟踪提示参数和模型版本。

质量保证技术

CRISP-ML(Q)建议在建模期间整合特定的质量保证实践，例如可重复性检查和模型稳定性评估。对于 GenAI，这可能包括对提示进行单元测试，以验证一组固定的测试输入是否始终产生预期的输出格式，或进行静态分析以验证微调是否没有降低关键模型能力。

解决模型设计中的风险

如果早期阶段标识了某些风险，应应用风险缓解技术，例如嵌入对齐、将调节模型纳入生成管道，或选择一个更小、更可控的模型而不是一个更大、更不可解释的模型。此外，GenAIOps 通常涉及在模型接口处添加护栏，例如基于规则的过滤器或拒绝采样，以确保输出满足质量和合规性要求。这与 CRISP-ML(Q)在开发过程中缓解风险的焦点相一致。

在这些步骤中，SMACTR 鼓励保持“透明度跟踪”。这意味着记录设计决策，例如为什么选择特定的模型或提示策略，如何设置配置，以及任何已知的权衡。例如，你可能想要创建模型卡的早期草案，以捕捉模型的使用意图、架构和局限性。

到建模阶段结束时，工程团队不仅应该有一个调优好的模型和管道准备评估，还应该有一套全面的文档和工件，确保建模过程可重复且易于理解。

### SMACTR 整合

在建模阶段，需要关注的 SMACTR 关键阶段是**工件收集和测试（规划**）。这些工件应包括设计历史文件或类似的文档，详细说明建模决策。这些文档可能包含架构图（特别是如果应用程序包含多个组件，如 LLM 和向量数据库），超参数或提示脚本的记录，以及融入模型设计中的任何伦理考量（例如，“我们决定不对包含敏感数据的用户聊天日志进行微调，以保护隐私”）。

SMACTR 建议使用审计清单来验证关键模型文档，如模型卡和相关数据文档是否完整且可访问。到建模阶段结束时，这些工件应基本完成。

虽然测试阶段在下一个阶段（评估）正式开始，但测试的准备应该在建模阶段开始。团队应根据早期的风险映射优先考虑哪些风险进行测试。例如，如果关注的故障模式是模型生成有害语言，建模阶段可能包含毒性分类头或计划进行对抗性提示测试。本质上，SMACTR 鼓励通过确保模型包含钩子或支持工具来促进即将到来的密集测试。

为了确保质量的责任和所有权，应指派个人工程师或研究人员负责各种模型组件或实验，追踪谁微调了哪个版本，谁设计了哪个提示集等。

通过将 SMACTR 整合到建模阶段，GenAI 团队将模型视为一个透明开发的组件，而不是一个黑盒，准备接受严格的审计和测试。表 7-6 总结了此阶段产生的核心工件。

表 7-6\. 建模阶段产生的关键工件总结

| SMACTR 阶段 | 输出 |
| --- | --- |
| 工件收集 |

+   建模决策设计历史文件或文档（架构图、超参数、提示脚本、伦理考量）

+   模型文档（模型卡）

+   几乎完成的数据文档

|

| 测试（建模期间的规划） |
| --- |

+   根据早期风险映射确定的优先测试风险

+   **测试性功能集成（例如，毒性分类头，对抗性提示测试计划**）

+   密集测试计划

|

| 映射 |
| --- |

+   负责模型组件或实验的个人识别（责任）

|

## 评估阶段

此阶段直接与 GenAIOps 生命周期中的模型评估阶段相一致。在模型或提示开发完成后，应对其性能与之前定义的成功标准进行彻底评估。在 GenAIOps 中，评估是一个持续的多方面过程。它包括适用时的定量评估（例如，基准测试的准确性或翻译任务的 BLEU 分数）以及定性评估，如对输出质量的个人判断、用户反馈循环等。它还可能扩展到通过红队或使用对抗性或异常输入探测模型来评估系统行为。在实践中，GenAI 的评估阶段通常与部署重叠，涉及影子部署或与真实用户进行的 A/B 测试。在此，我们将重点关注核心的预部署评估步骤。

CRISP-ML(Q)将评估视为通过评估其满足定义的成功标准的能力来验证模型的目的适应性。同样，GenAIOps 必须确定 GenAI 应用程序是否满足业务需求、质量标准和技术指标的阈值。两种方法都涉及在保留数据集或场景上进行测试。

传统的机器学习评估通常使用定义良好的指标，如准确性、精确度、均方根误差（RMSE）和静态测试数据集。GenAI 的评估更为复杂，因为生成性输出不容易被单个标量指标所捕捉。GenAI 的评估仍然是一个未解决的问题。例如，给定相同的输入，模型的行为可能会随时间变化，而保持一致的输出可能需要提示调整。GenAI 的评估通常涉及对输出进行人为评分以判断正确性或偏好，进行对抗性测试以暴露故障模式或不良行为，以及使用 BLEU 或 ROUGE 分数或困惑度等专门的生成质量指标。

GenAI 的评估也必须考虑伦理和安全维度：一个模型可能具有很高的准确性，但由于存在偏见或有毒的输出而失败。因此，必须纳入公平性和安全性审计。在 CRISP-ML(Q)中，评估是迭代的；如果模型未能满足定义的标准，则返回建模甚至数据准备。但在 GenAIOps 中，由于非确定性输出和不断变化的用途，评估通常是连续的。像“黄金集”这样的技术——精心策划的测试提示，其中包含回归测试的预期答案——正成为标准。

CRISP-ML(Q)要求对目标进行严格的验证。GenAI 将评估范围扩展到包括开放性输出质量、伦理风险测试以及传统的性能指标。

### 实现

在评估阶段，工程团队应实施一个稳健的评估策略，包括：

基准测试

在已知答案的问题或任务数据集上评估 LLM。例如，对于代码生成模型，维护一套已知正确输出的编程问题，并测量成功率。这类似于传统机器学习中的测试集。基准应该进行版本控制，以便随着时间的推移跟踪改进和回归。

质量指标

定义适用于用例的定量指标。这些可能包括如果可用参考输出，则包括 BLEU 或 ROUGE 分数用于文本相似度，多样性指标或响应延迟。如果应用程序可以收集反馈，将用户满意度评分或评级纳入评估循环通常很有用。

人类和对抗性评估

由于生成模型的开放性，人类应该评估输出样本的正确性、连贯性和安全性等因素。在测试阶段，强烈建议进行对抗性测试，这涉及故意使用具有挑战性或敏感的提示来观察模型的行为。例如，测试人员可能会向聊天机器人提出不适当或违反政策的提问，以查看它是否会以不允许的内容回应。结果可以总结在 SMACTR 建议的道德风险分析图表中，按严重性和可能性对故障模式进行评级。

评估中的自动化监控

使用自动化工具在规模上评估输出。一种新兴的 GenAIOps 实践是使用“监视器模型”或分类器来标记问题内容。例如，毒性检测器如 Llama Guard 或 Azure 的 Prompt Shields 可以扫描大量输出样本，以估计可能有害或不适当的百分比。这些工具在简单指标之外增加了一层质量保证。

比较和验证

如果开发了多个模型版本或提示变体，应将它们并列评估。A/B 测试可以离线进行（评估者对哪个模型产生了哪个输出一无所知）以选择最佳版本。这类似于 CRISP-ML(Q)中的集成/超参数选择，但在 GenAI 中可能涉及比较两个提示模板。

文档

在评估过程中，文档仍然至关重要。团队应记录所进行的测试、结果以及基于这些结果所做的决策。如果模型未能通过某些测试，应记录问题是否将被解决或接受缓解措施。例如，如果大型语言模型（LLM）偶尔产生略微不正确但无害的输出，团队可能会决定这在商业环境中是可以接受的；然而，如果它有时产生隐私违规，则可能触发模型修订。

在 GenAIOps 中，评估阶段也是做出“部署或放弃”部署决策的地方。工程团队应定义明确的发布标准，例如“输出中标记为冒犯性的少于 1%”或“至少 85%的测试问题回答正确。”如果模型未能达到这些阈值，则过程将回溯到建模甚至数据准备阶段，通过微调、提示重新设计或数据增强来解决不足之处。

### SMACTR 集成

SMACTR 的测试阶段与评估阶段直接对应。该框架建议使用对抗性测试等技术来评估性能，并生成如伦理风险分析图等工件。在通用人工智能（GenAI）的背景下，这意味着特别关注验证模型是否符合在范围阶段确定的伦理和风险相关要求。团队应使用如失效模式和影响分析（FMEA）等风险评估工具来列出潜在失效模式，例如“模型提供法律建议”或“模型输出带有偏见的语言”，并实施测试以覆盖每个识别的风险。

测试阶段还关注伦理合规性。为了评估高优先级的伦理风险或来自训练数据或模型设计的已知问题，团队可以创建并执行伦理测试清单，包括诸如“模型是否产生事实幻觉？”或“它是否具有刻板印象？”等问题。

这个阶段也为反思阶段奠定了基础。收集所有评估结果和见解，以便纳入缓解计划。例如，如果测试显示模型在特定类别的输入上表现不佳，请记录下来，以便在部署后监控或重新训练时参考。

在这个阶段，工件收集持续进行。评估报告、风险图表和测试结果应作为透明度轨迹的一部分保存。对于通用人工智能（GenAI）应用的评估阶段实际上成为了对模型准备情况的审计。除了验证准确性和性能外，还应测试安全性、公平性和鲁棒性，并提供书面证据，以向监管机构等利益相关者表明在部署前已尽职调查。表 7-7 总结了此阶段的关键 SMACTR 工件。

表 7-7. 评估阶段产生的关键工件摘要

| SMACTR 阶段 | 输出 |
| --- | --- |
| 测试 |

+   使用对抗性测试等方法进行性能评估

+   伦理风险分析图

+   FMEA 或类似的风险评估

+   对识别的失效模式进行测试

+   伦理测试清单（例如，幻觉、刻板印象）

|

| 反思 |
| --- |

+   从评估中收集的结果和见解，以制定缓解计划

|

| 工件收集 |
| --- |

+   评估报告、风险图表和测试结果

|

## 部署阶段

CRISP-ML(Q)部署阶段侧重于将模型投入生产并确保其在实际操作中满足业务需求。其他关键活动包括用户验收测试和生成文档。在 GenAIOps 中，此阶段涵盖多个操作步骤，如集成和编排（CI/CD）、安全和可靠性工程以及实际模型部署。一旦模型被认为准备就绪，它必须集成到应用基础设施中，服务于最终用户，并使用适当的 MLOps 和 GenAIOps 实践进行维护。

与传统的 MLOps 一样，GenAIOps 中的部署涉及通过设置 API 端点或将模型嵌入到应用中来将模型或流水线推送到生产环境。它还包括持续交付提示或模型更新、实施如速率限制等安全控制、监控滥用以及确保系统在负载下可以扩展并保持可靠。

由于通用人工智能应用的动态性，团队可能会频繁更新提示或替换为改进的模型，这使得 CI/CD 流水线变得至关重要。这一阶段的安全和可靠性方面反映了 CRISP-ML(Q)的质量保证思维。主要目标是确保部署的模型不会暴露漏洞（如提示注入）并保持稳健。

### Implementation

在实践中，根据 CRISP-ML(Q)原则部署通用人工智能应用意味着在模型上线时关注工程最佳实践和安全。部署阶段的关键考虑因素包括：

CI/CD for prompts and models

建立自动化流水线，将更新的提示或模型版本通过测试并推入生产。例如，类似于软件部署中的单元测试，如果提示模板更新以提高性能，CI 流水线应运行评估套件，并且只有通过测试后才能推广更改。同样，应使用基础设施即代码来部署模型服务器或服务。这通过创建一致的环境确保可重复性，并提供了关于正在使用的模型或提示版本的可追溯性。

系统集成和性能优化

与简单的机器学习部署不同，通用人工智能（GenAI）应用通常包含多个组件（前端、后端、模型 API、数据库、向量存储等）。集成涉及将这些组件连接起来，并确保数据正确流动——例如，用户查询路由到大型语言模型（LLM），在返回响应之前可能调用知识库。根据非功能性需求，如成本、速度和可扩展性，您可能需要优化延迟和吞吐量。由于大型模型可能成本高昂或运行缓慢，您可能希望应用模型量化、缓存频繁响应或为特定请求使用较小的蒸馏模型等技术。

Monitoring setup

虽然监控是 CRISP-ML(Q)中的下一个正式阶段，但基础工作是在部署期间进行的。实现模型输入和输出的日志记录，为关键指标（错误率、响应时间等）设置仪表板和警报，并跟踪使用模式。GenAIOps 最佳实践建议从模型部署的那一刻起就集成可观察性。Prometheus 和 Grafana 等工具常用于基础设施监控，以及针对模型行为的自定义监控器。

安全机制

对于通用人工智能（GenAI）应用，应在部署时实施诸如内容过滤器、用户身份验证和授权、速率限制和回退机制等护栏。例如，如果大型语言模型（LLM）是面向用户的 APP 的一部分，您可能需要集成一个审查 API 或简单的规则引擎来检查模型输出。如果输出违反政策（例如，包含仇恨言论），系统可以阻止或清理响应。您还应计划故障转移：如果 LLM 服务不可用，系统应默认回退到更安全的选项，例如更简单的响应或静态消息。这些安全和可靠性措施有助于确保实时系统具有弹性并符合欧盟人工智能法案的要求。

分阶段推出

团队可以选择进行影子部署，在这种部署中，模型在用户看不到的情况下根据真实用户查询生成输出，以在生产类似条件下收集性能数据。这类似于 CRISP-ML(Q)中的试点或 beta 测试，在全面推出之前确保模型按预期工作。

文档和培训

确保操作团队（可能和开发团队相同）有明确的文档说明如何回滚部署，如何在出现问题的情况下介入，以及已知的问题是什么。在 CRISP-ML(Q)中，部署以将文档交付给维护者结束；在 GenAIOps 中，这可能采取运行手册或剧本的形式。例如，一个剧本可能说：“如果监控显示慢响应激增，考虑禁用复杂功能或增加计算资源。”

### SMACTR 集成

SMACTR 框架鼓励“负责任地部署”的心态，将部署视为一个验证到目前为止所做的一切是否符合伦理、质量和安全标准的时刻，并在模型投入生产后建立持续警惕的机制。

在这个阶段，应激活 SMACTR 的反思阶段。在上线之前，团队应反思开发过程是否满足了最初的伦理和风险相关目标。具体来说，这意味着审查评估结果，并确定是否有任何剩余的风险是不可接受的。例如，反思阶段可能导致风险缓解计划：“我们观察到模型有时会提供过时的信息。缓解措施：显示免责声明或定期重新训练。”

SMACTR 的反思阶段还要求生成一个算法审计总结报告，这是对整个项目期间的关键发现、风险决策和伦理考量的总结。对于一个通用人工智能部署，这可以提炼成一个类似于模型卡片加上一个负责的 AI 部署清单，用于治理目的。

表 7-8 概述了在部署阶段应产生的工件，特别是作为 SMACTR 框架反思阶段的一部分。

表 7-8。部署阶段产生的关键工件摘要

| SMACTR 阶段 | 输出 |
| --- | --- |
| 反思 |

+   评估结果的审查

+   对残余风险的接受性决策

+   风险缓解计划（例如，免责声明、再培训时间表）

+   算法审计总结报告（或模型卡片和负责的 AI 部署清单）

+   定期发布后审计计划（内部审查）

+   维护可追溯性（模型版本、部署日期、评估）

|

## 模型监控和维护阶段

在 GenAIOps 中，一旦部署了基于 LLM 的应用程序，团队必须持续监控其性能，收集现实世界使用的数据，并随着时间的推移维护或改进模型。这个阶段，通常被称为*模型运维*，类似于 CRISP-ML(Q)的监控和维护阶段。

在通用人工智能的背景下，维护可以采取多种形式，例如在新数据上微调模型、切换到新的模型版本、更新提示、刷新 RAG 管道中的知识库，或者在领域发生重大变化时重新训练。监控和维护阶段特别旨在解决模型在演变环境中退化的风险，其中数据漂移、用户需求的变化或概念漂移可能在部署后发生。

在传统的机器学习中，监控通常集中在数据漂移、模型漂移和核心性能指标上——例如，检测输入数据的分布是否发生了足够的变化，以至于降低了准确性并触发了再培训。其中一些概念也适用于通用人工智能：用户查询可能会随时间演变，使得原始模型或提示变得不那么相关。然而，通用人工智能引入了额外的挑战，例如监控输出内容以检测不适当或事实错误的答案，以及跟踪用户交互模式以检测潜在的滥用。

反馈循环在通用人工智能中尤其关键：可以使用 RLHF 或基于记录的错误和失败的微调技术来改进模型。CRISP-ML(Q)强调持续质量控制，但更侧重于技术退化和计划中的再培训，而通用人工智能的维护涉及对模型行为的积极、有时是实时管理。

最后，在整个流程中整合监控至关重要。没有适当的监控和维护，模型的表现和效用可能会随着时间的推移而下降，安全性也可能迅速恶化。生成式 AI 应用的动态、非确定性特性使得这一阶段更加关键和复杂。

### 实施

对于生成式 AI 应用，监控和维护阶段涉及多个持续实践，其中许多与经典的 MLOps（机器学习运营）并行，但针对生成式 AI 特定的挑战和指标进行了调整：

实时性能监控

在生产环境中跟踪模型的输出，监控其质量和可靠性。这可能包括诸如成功回答查询的百分比或用户评分（如果可用）等统计指标，以及如延迟和错误率等技术指标。例如，在一个基于生成式 AI 的聊天机器人中，用户频繁地重述问题可能表明初始响应不佳。设置警报阈值非常重要。例如，如果内容标记的速率在夜间翻倍，团队应被提醒调查潜在的模型漂移或误用。

数据和用法漂移

与传统的机器学习不同，其中输入特征可以直接比较，生成式 AI 可能需要监控用户查询的嵌入或对它们进行聚类以检测主题变化。例如，在一个基于 RAG 的系统（Retrieval-Augmented Generation）中，监控可能揭示用户对知识库中尚未出现的新产品的兴趣。

日志和审计

维护详细的交互日志，同时采取适当的隐私保护措施，以支持回顾性分析和审计。这些日志可以输入到反馈数据集中。此外，团队可能定期对随机样本的交互进行标注，以评估满意度或准确性，帮助构建用于质量跟踪的持续评估数据集。

重新训练或模型更新

确定重新训练或更新模型的时间表或标准。在基模型通过 API（如 OpenAI 的 GPT）提供的情况下，重新训练可能不可行。然而，团队仍然可以在新版本可用时微调适配器并切换到更新的模型版本（例如，从 GPT-3.5 升级到 GPT-4）。每次更新都应该通过简化的评估和部署生命周期。

提示和系统维护

维护对于周围系统也很重要。团队可能根据观察到的性能优化提示模板或根据新出现的故障案例引入新的安全规则。由于生成式 AI 应用对提示变化非常敏感，因此应像对待代码更改一样谨慎对待这些更改——在推出前进行测试并密切监控性能。

用户反馈和迭代

提供用户反馈的渠道（如对响应的点赞/踩、报告问题等），并建立定期分析和采取行动的反馈流程。

风险管理

如果出现新的滥用模式——例如，如果用户发现一种新的方法让模型产生不允许的内容——团队应迅速响应，更新过滤器，修改提示或调整模型行为以阻止这些漏洞。

### SMACTR 集成

监控和维护对应于 SMACTR 术语中的持续循环的反思和范围更新。虽然反思阶段最初被界定为部署前的审计，但其核心活动——评估结果与初始目标和风险——应在部署后定期进行。

对于一个生成式人工智能应用，这可能意味着安排定期的内部审计或事后分析。例如，在一个月的生产使用后，团队可能会审查以下问题：

+   模型的使用方式是否产生了新的伦理担忧？

+   我们的缓解计划是否有效，或者我们需要采取新的措施？

这种持续的反思可能导致风险缓解计划的更新，甚至可能触发一轮新的范围规划，如果应用程序的目的或使用环境发生变化。例如，如果系统被用于比最初预期风险更高的环境中，团队应重新审视和重新规划伦理和运营风险。

SMACTR 还要求记录事件，这意味着团队应维护设计变更的历史记录，并记录任何重大失败及其解决方式。这类似于将算法使用相关的风险分析和审计总结报告视为活文件，随着时间的推移更新以反映现实世界的发现。

此外，如果出现新的利益相关者，可能需要重新访问映射阶段。例如，客户支持可能需要介入处理关于系统的用户报告，或法律团队可能需要审查符合欧盟人工智能法案等法规的情况。维护流程应确保这些利益相关者被识别并纳入治理和监督活动。

最后，监控在有效运作中相当于生产中的持续测试。SMACTR 框架强调监控和审计有助于识别新兴威胁并迅速采取行动。工程团队应将监控数据不仅视为运营信息，还视为审计证据。每个异常都应调查其根本原因，每个重要的输出错误都应分析其潜在的更广泛影响，并明确分配监督责任。这对于生成式人工智能（GenAI）的风险缓解尤为重要，其中模型偏差或不当输出可能导致声誉或法律后果。

每个 SMACTR 阶段监控和维护阶段的显著输出都在表 7-9 中概述。

表 7-9. 监控和维护阶段产生的关键工件摘要

| SMACTR 阶段 | 输出 |
| --- | --- |
| 反思（持续） |

+   定期内部审计或事后分析

+   与原始目标和风险相对的结果评估

+   风险缓解计划的更新（活文档）

+   事件记录（设计历史、故障日志）

+   维护与算法使用相关的风险分析及审计摘要报告（活文档）

|

| 重新界定（更新） |
| --- |

+   如果应用程序的范围扩大或改变，重新界定和评估风险

|

| 重新审视映射 |
| --- |

+   包括新的利益相关者（例如，客户支持、法律团队）并明确他们的监督角色

|

| 测试（持续） |
| --- |

+   监控作为生产中的持续测试

+   确定新兴威胁

+   异常和重大输出错误的调查

+   明确分配监督责任

|

# 结论

本章探讨了欧盟人工智能法案的核心方面，这些方面适用于通用人工智能模型和系统，以及 GenAIOps 在支持合规性和培养可信赖人工智能中的实际作用。正如这里所概述的，法案为通用人工智能（GPAI）建立了一个结构化的监管框架，要求将文档编制、披露和风险缓解措施嵌入到提供商的工作流程中。GenAIOps 将传统的 MLOps 实践扩展到满足 GPAI 的独特挑战，使工程团队能够构建符合监管要求、具有弹性、可扩展并与社会期望一致的系统。通过整合 SMACTR 框架，组织可以进一步通过嵌入解决 GPAI 特定风险（如幻觉和道德不匹配）的问责机制来增强 CRISP-ML(Q)。

然而，实现合规性并非一次性任务。它需要一种积极主动、持续的方法，在人工智能开发的生命周期中整合透明度、问责制和道德反思。从数据准备和模型微调到部署和持续监控，团队必须采用结构化、可审计的过程，以确保他们的系统保持可靠和可解释，并符合道德标准。这对于管理诸如偏见、错误信息和意外模型行为等风险尤为重要。GenAIOps 提供了一个操作框架，允许组织系统地实施这些保障措施，弥合法规合规与实际人工智能开发部署之间的差距。

# 最后的话和人工智能政策制定的未来

随着我们结束对人工智能治理、工程实践以及欧盟人工智能法案的探索之旅，回顾所涵盖的内容并展望未来显得尤为重要。我们首先探讨了欧盟人工智能法案基于风险的方法论的基础格局，以及它如何引入了关键义务，即使是那些未被归类为高风险的系统也不例外。一个关键的见解是，法案要求所有旨在直接与自然人互动的人工智能系统透明度，无论其风险水平如何（主要通过第 50 条）。

然后，我们转向了人工智能开发和部署的实际世界，认识到合规性不是部署后的后顾之忧，而是一个必须整合到人工智能系统生命周期中的过程。采用结构化方法，如 CRISP-ML(Q)，以及像 SMACTR 这样的伦理框架，成为推动负责任和合规发展的主动催化剂。通过将 CRISP-ML(Q)的阶段与 SMACTR 原则对齐，从业务和数据理解到监控和维护，组织可以系统地解决伦理考量、管理风险并在每个阶段满足透明度要求。

本书贯穿的一个主题是文档和稳健元数据管理的重要性。这些是可追溯性、内部审计、证明合规性和促进透明度的基石。在整个开发过程中生成的关键工件，如项目范围文档、伦理审查报告、利益相关者图、数据质量评估和初始透明度要求文档，作为尽职调查和伦理考量的证据。进一步的文档，如数据质量报告、功能文档、评估报告、合规性验证记录、部署配置和监控日志，有助于建立全面的合规轨迹。

在立法过程中，GPAI 和生成式 AI 模型的快速出现和广泛应用引入了新的挑战，促使欧盟 AI 法案纳入具体规定来应对这些问题。在这个不断发展的景观中，MLOps 的原则——扩展到 GenAIOps——对于管理这些复杂系统至关重要。它们支持透明度、日志记录和版本控制，以创建可审计的轨迹，使团队能够在保持灵活性和性能的同时实现合规性。

正如你所看到的，在人工智能监管的景观中导航需要不仅仅是理解规则，它要求在人工智能系统的工程和治理方面有一个根本性的转变。主动合规，嵌入到工程工作流程中，并得到全面文档的支持，是前进的道路。

人工智能技术的景观正在以惊人的速度发展。这一点的例子就是检索增强生成和基于代理架构的出现。为了跟上步伐，我们必须不断学习新技术，并关注监管更新、最佳实践以及人工智能治理的新兴方法。

在序言中，我引用了斯坦福大学以人为本的人工智能研究所（HAI）的联合创始人和世界实验室的首席执行官兼联合创始人李飞飞的话，她[评论道](https://oreil.ly/_PUIb)：“现在比以往任何时候都更需要一个治理框架。”她为人工智能政策制定的未来概述了三个基本原则：

1.  人工智能工程应**优先考虑经验验证而非推测**，确保模型使用真实世界的数据和透明的基准进行严格测试。这意味着工程师应依赖科学依据的方法，避免炒作，专注于实际应用而非推测性场景。可以将标准化框架如 ISO/IEC 人工智能指南、NIST 的人工智能风险管理框架或 SMACTR 整合到开发工作流程中，以促进可靠性、公平性和鲁棒性。

1.  工程师必须通过整合持续监控和遵守伦理标准，采取**风险意识、务实的开发实践**，在创新与负责任的部署之间取得平衡。这包括在开发早期进行风险评估，利用迭代原型来揭示和解决意外后果，以及持续部署后的监控以检测偏差或滥用。人工智能系统应设计内置的安全保障和治理机制，特别是在高风险领域如国防或医疗保健领域。

1.  最后，**协作和开放访问**应该是核心，促进开源贡献、知识共享和跨行业合作，以加强人工智能生态系统同时保持问责制。工程师可以通过分享研究成果、构建可重用框架以及创建或贡献降低初创企业和学术机构进入壁垒的工具来支持这一点。

最终，构建值得信赖的人工智能既是技术挑战，也是整个组织共同承担的责任。作为工程师、研究人员和从业者，我们不仅塑造了人工智能能够做什么，还塑造了它如何影响世界。通过将伦理、透明度和问责制嵌入到开发的每个阶段，我们可以确保我们创建的系统不仅强大，而且值得人们对其所赋予的信任。

^(1) 更多信息，请参阅 Nahema Marchal 等人撰写的论文《“生成式人工智能滥用：策略分类和来自真实世界数据的见解”》（[“Generative AI Misuse: A Taxonomy of Tactics and Insights from Real-World Data”](https://oreil.ly/DDpPc)）。
