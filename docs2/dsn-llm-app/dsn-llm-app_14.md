# 第十一章\. 表示学习和嵌入

在上一章中，我们学习了如何将语言模型与外部工具接口，包括数据存储。外部数据可以以文本文件、数据库表和知识图谱的形式存在。数据可以跨越广泛的内容类型，从专有领域的知识库到由 LLM 生成的中间结果和输出。

如果数据是有结构的，例如存储在关系型数据库中，语言模型可以发出 SQL 查询来检索所需的数据。但如果是非结构化形式的数据呢？

从非结构化文本数据集中检索数据的一种方法是通过关键词搜索或使用正则表达式。对于上一章中提到的苹果公司 CFO 的例子，我们可以从包含财务披露的语料库中检索包含 CFO 提及的文本，希望其中包含加入日期或任期信息。例如，你可以使用以下正则表达式：

```py
pattern = r"(?i)\b(?:C\.?F\.?O|Chief\s+Financial\s+Officer)\b"
```

关键词搜索在有效性上存在局限性。如果语料库中存在 CFO 加入日期或任期，那么表达这些信息的方式有非常多种。尝试使用上述通用的正则表达式可能会导致大量误报。

因此，我们需要超越关键词搜索。在过去几十年中，信息检索领域已经发展出多种方法，如 BM25，这些方法塑造了搜索系统。我们将在第十二章中了解更多关于这些方法的内容。在 LLM 时代，基于嵌入的搜索系统正迅速成为实现搜索的标准方式。

在本章中，我们将学习嵌入是如何工作的。我们将探讨语义相似性的概念，并检查各种相似度度量。我们将学习如何使用流行的嵌入模型并评估其性能。我们还将展示如何微调嵌入模型以适应特定的用例和领域。我们将展示如何使用稀疏自编码器（SAEs）来解释这些嵌入。最后，我们将讨论优化嵌入以减少存储需求和计算开销的技术。

# 嵌入简介

表示学习是机器学习的一个子领域，它涉及学习以捕捉数据有意义特征的方式表示数据，通常是在低维空间中。在 NLP 的背景下，这涉及到将文本单元如单词、句子或段落转换为向量形式，称为嵌入。嵌入捕捉输入的语义（与意义相关）和语用（与社会语境相关）特征。

嵌入可以使用开源库和付费 API 生成。[Sentence Transformers](https://oreil.ly/4OSVd)是一个非常著名的开源库，用于生成嵌入，它提供了与专有模型竞争的嵌入模型。

让我们使用`Sentence Transformers`库来生成嵌入：

```py`` ```` 输出：    ```py Embedding size: 768  tensor([-3.9256e-01,  1.0734e-01,  1.3579e-01,  7.6147e-02,  5.2521e-02, -6.5887e-03,  1.9225e-01,  3.5374e-01,  2.5725e-01,  5.6408e-02,...]) ```    对于这个模型，嵌入大小为 768，这意味着每个向量有 768 个维度。这个特定模型的序列长度为 512，这意味着输入文本被限制在 512 个标记内，超出部分将被截断。嵌入向量由浮点数组成，这些浮点数本身是不可解释的。我们将在本章后面讨论解释嵌入的技术。    目前使用的多数嵌入模型都是基于仅编码器语言模型，我们在第四章中介绍了这种模型。底层模型包括 BERT、RoBERTa、MPNet 等，通常在释义/问答/自然语言推理数据集上进行微调。让我们看看如何从这些类型的模型中提取嵌入（这是`sentence_transformers.encode()`函数在底层所做的事情）：    ```py from transformers import AutoTokenizer, AutoModel import torch  tokenizer= AutoTokenizer.from_pretrained(   "sentence-transformers/msmarco-distilbert-base-tas-b") model = AutoModel.from_pretrained("sentence-transformers/msmarco-distilbert-base-tas-b")  input = tokenizer(   'American pizza is one of the nation's greatest cultural exports', `padding``=``True``,` `truncation``=``True``,` `return_tensors``=``'pt'``)`  `with` `torch``.``no_grad``():`         `output` `=` `model``(``**``input``,` `return_dict``=``True``)`        `embedding` `=` `output``.``last_hidden_state``[:,` `0``]` `print``(``embedding``)` ```   `在这个例子中，嵌入是从 DistilBERT 模型的最后一层的[CLS]标记中提取的。从模型中提取嵌入的其他方法包括：    *   均值池化，即在序列中所有标记输出上取平均值           *   最大池化，即在所有标记的每个维度上取最大值           *   加权均值，即给予最后几个标记更多的权重           *   最后一个标记，其中嵌入只是最后一个标记的编码器输出              ###### 小贴士    最后一个标记（或第一个标记）是否包含整个序列的良好表示，很大程度上取决于预训练和微调目标。BERT 的预训练目标（下一句预测）确保了[CLS]标记比，比如说，不使用下一句预测目标的 RoBERTa 更丰富，因此其<s>起始序列标记的信息量不是很大。    最近，基于解码器的嵌入模型开始变得突出，如[SGPT 模型系列](https://oreil.ly/AztT9)。OpenAI 为搜索和相似度暴露了一个单一的嵌入端点。OpenAI 嵌入具有更大的最大序列长度（8,192 个标记）和更大的维度大小（1,536–3,072）。Cohere 和 Jina 是其他嵌入提供商的例子。    选择适合您任务的正确模型取决于成本、延迟、存储限制、性能以及您用例的数据域。我建议从 Sentence Transformers 库中提供的有效但小巧的 all-mpnet-base-v2 模型开始，我认为它是 NLP 领域的“工作马”。像往常一样，尝试不同的模型永远不会有害。本章的其余部分将提供有关选择正确模型的更多提示。稍后在本章中，我们还将展示如何评估嵌入模型并介绍流行的基准。    ###### 警告    没有无限压缩这回事！嵌入大小是固定的，所以您的输入越长，其嵌入中可以编码的信息就越少。管理这种权衡因用例而异。` ```py` `````  ```py```````py ``````py``` ``````py`` # Semantic Search    The true value of embeddings can be appreciated when we use them for representing a large text corpus. The vectors representing the data occupy what we call an embedding space. Similar texts are located closer to each other in the embedding space. This property allows us to use similarity measures to accomplish meaningful tasks like clustering or semantic search. Semantic search refers to techniques that take into account the meaning and context of queries and documents to identify documents that are most relevant to a given query.    We can visualize the embedding space by using dimensionality reduction techniques like [PCA](https://oreil.ly/Rk1M9) or [t-SNE](https://oreil.ly/0xNrB).    Figure 11-1 depicts the visualization of embeddings of posts on X (formerly Twitter) by members of the US Congress created by [Nomic AI](https://oreil.ly/XsXls) using its Atlas tool. You can view a detailed version of the visualization at [Nomic’s blog](https://oreil.ly/AORpk).    Let’s explore how we can use embeddings for semantic search. For a given user query, we can generate an embedding of the query and then identify document embeddings closest to it in the vector space. The texts corresponding to the top-k (k can be as small as 1 but can vary according to application needs) closest vectors are provided as a response to the search query. This process is called *retrieval*. The texts are then fed into the LLM prompt along with the user query, and the LLM uses the information provided in the context to answer the user query. This two-step process has traditionally been called the *retriever-reader* framework, with the LLM playing the role of the reader in this example.  ![embedding-visualization](img/dllm_1101.png)  ###### Figure 11-1\. Embedding space visualization    As a simple illustrative example, consider two sentences that make up our corpus:    ``` chunks = ['The President of the U.S is Joe Biden', 'Ramen consumption has increased in the last 5 months'] ```py    Given the query “president of usa,” we can encode the query and the chunks using Sentence Transformers:    ``` 从 sentence_transformers 导入 SentenceTransformer, util sbert_model = SentenceTransformer('msmarco-distilbert-base-tas-b') chunk_embeddings = sbert_model.encode(chunks, show_progress_bar=True, device='cuda', normalize_embeddings=True, convert_to_tensor=True) query_embedding = sbert_model.encode(query, device='cuda', normalize_embeddings=True, convert_to_tensor=True) matches = util.semantic_search(query_embedding, chunk_embeddings, score_function=util.dot_score) ```py    The output is:    ``` [[{'corpus_id': 0, 'score': 0.8643729090690613},   {'corpus_id': 1, 'score': 0.6223753690719604}]] ```py    As you can see, the similarity score is much higher for the first sentence, and thus we return the first sentence as the query response.    ###### Note    There is a distinction between symmetric semantic search and asymmetric semantic search. In symmetric search, the query text is of similar size as the document text. In asymmetric search, the query text is much shorter than the document text, as with search engine and question-answering assistant queries. There are models available that are specialized for only symmetric or asymmetric search. In some models, the query and chunk texts are encoded using separate models.    # Similarity Measures    Commonly used similarity measures include dot product, cosine similarity, and Euclidean distance. Refer to the [Pinecone](https://oreil.ly/X_qcD) tutorial on similarity measures if you need a backgrounder. While using embedding models, use the similarity measure that was used to train the model. You will find this information in the model card or Hugging Face model hub page.    ###### Tip    If you set `normalize_embeddings` to `True` as an argument in the `encode()` function, it will normalize the embeddings to unit length. This will ensure that both dot product and cosine similarity will have the same values. Note that dot product is a faster operation than cosine similarity. Sentence Transformers provides [separate models](https://oreil.ly/LOu75) trained on dot product and cosine similarity and mentions that models trained on dot product tend to prefer longer chunks during retrieval.    While the notion of semantic similarity is powerful, it is not a panacea for all applications. The semantic similarity task is underspecified. To start with, there are several notions of similarity. Similarity refers to the sameness or alikeness of the entities being compared. But for the same two entities, some dimensions are similar and some are different.    For example, consider the three sentences:    > After his 25th anniversary at the company, Mr. Pomorenko confirmed that he is not retiring. >  > Mr. Pomorenko announced his retirement yesterday. >  > Mr. Pomorenko did not announce his retirement yesterday.    Now let’s use the Sentence Transformers all-mpnet-base-v2 embedding model to encode these sentences and calculate their similarity:    ``` !pip install sentence-transformers  from sentence_transformers import SentenceTransformer, util model = SentenceTransformer('all-mpnet-base-v2')  sentences = ['After his 25th anniversary at the company, Mr. Pomorenko `confirmed` `that` `he` `is` `not` `retiring``',  '``Mr``.` `Pomorenko` `announced` `his` `retirement` `yesterday``']` ```py `embeddings` `=` `model``.``encode``(``sentences``)` `cosine_scores` `=` `util``.``cos_sim``(``embeddings``[``0``],` `embeddings``[``1``])` `print``(``"Cosine Similarity:"``,` `cosine_scores``.``item``())` ``` ```py   ````` ```py`Output:    ``` 余弦相似度：0.7870 ```py    If you replace the second sentence with “Mr. Pomorenko did not announce his retirement yesterday,” the output is:    ``` 余弦相似度：0.7677! ```py    As you can see, both these sentences are perceived as equally similar to the first sentence. In some aspects, this is true. They are similar because they both talk about Mr. Pomorenko. They are also similar because both deal with the subject of retirement. On the other hand, one sentence conveys the opposite meaning to the other, by suggesting a retirement is happening versus not happening.    ###### Tip    One way to handle the false positives arising due to the model using undesirable similarity dimensions (like negation) is to just increase the k value in the top-k results that are returned as a response to the query. Then, the LLM can distinguish between false positives and use the correct information for answering the query. However, increasing the top-k also increases the context length of the prompt, increasing latency and cost.    Our application requirements determine which similarity dimensions are important to us. If negation is an important relation for our application to distinguish, it might be a good idea to reflect that in our embedding space. This is where fine-tuning embedding models can come in handy. Fine-tuning embedding models allows you to “edit” your embedding space to your own liking. The process is relatively simple and can be potentially quite beneficial.    Fine-tuning embeddings can also be very useful when you are working with specialized data domains whose token distribution deviates from general-purpose data. Let’s now discuss how to fine-tune embedding models.```` ```py``  ``````py` ``````py # Fine-Tuning Embedding Models    The Sentence Transformers library facilitates fine-tuning embedding models using the [`SentenceTransformerTrainer` class](https://oreil.ly/Jahep). To fine-tune an embedding model, we need a base model to fine-tune on, a training dataset, and a learning objective.    ## Base Models    You can fine-tune a fine-tuned model like all-mpnet-base-v2, or you can fine-tune a base model like MPNet, from which all-mpnet-base-v2 is defined. You will need more training data to fine-tune a base model than to further fine-tune an already fine-tuned model. Other candidates’ models for fine-tuning include [BGE-M3](https://oreil.ly/Sh8pZ) and [jina-embeddings-v3](https://oreil.ly/lFiWX). A full list of models available through Sentence Transformers can be accessed [online](https://oreil.ly/Onyuv). Remember to check the licenses for a given model before using it for commercial purposes.    Some of the factors to keep in mind while choosing a base model include the performance of the base model, the size of the embedding models (which determines how fast the model can encode text), the number of dimensions of the model (which determines the amount of storage taken up by the embeddings), and the licensing implications. The MPNet or all-mpnet-base-v2 is a solid first choice that has served me well on many projects.    ###### Tip    If a model has been fine-tuned for a particular task like semantic search, it is not optimal to further fine-tune it on a different task.    ## Training Dataset    There are many different ways to structure your dataset. The most common way is in the form of triplets consisting of (anchor, positive, negative) examples. For a given anchor sentence, the positive sentence is a sentence we would like to be closer to the anchor sentence in embedding space, and the negative sentence is a sentence we would like to be farther apart from the anchor in embedding space. For example, to fine-tune the model to help it distinguish negation sentences, our training set can be composed of triplets where the negative sentence contradicts the anchor and the positive sentences.    Figure 11-2 shows an embedding dataset composed of triplets for helping the model distinguish negation.  ![embed-dataset](img/dllm_1102.png)  ###### Figure 11-2\. Fine-tuning dataset for negation    Datasets can also be composed of sentence pairs, where the sentences could represent a (query, response) pair, or a (passage, summary) pair, or a pair of paraphrases. The downstream use cases determine the type of dataset needed. The [Sentence Transformers website](https://oreil.ly/geI1M) shows all the different ways a dataset can be formatted.    Training datasets can be as small as a few thousand examples, to [billions of tokens](https://oreil.ly/oNI4n) when used for domain adaptation.    Note that certain loss functions require your dataset to be in a specific format. We will discuss loss functions in detail next.    ## Loss Functions    Recall our discussion on loss functions for training LLMs in Chapter 4. The [Sentence Transformers library](https://oreil.ly/9Qaop) supports a wide range of loss functions for training embedding models. Let’s explore a few commonly used ones.    For a triplet dataset, you can compute a [triplet loss](https://oreil.ly/yXHNU). For a training dataset consisting of an (anchor, positive, negative) triplet, the triplet loss minimizes the distance between the anchor sentence and the positive sentence, and maximizes the distance between the anchor sentence and the negative sentence.    Mathematically, the loss is calculated as:  Loss = max(d(a, p) – d(a, n) + margin, 0)  where d is a distance measure, typically Euclidean distance. The margin is a hyperparameter that represents the distance by which the negative example should be farther away from the anchor than the positive example. When using Euclidean distance as the distance measure, I suggest a margin of 5, but make sure to tune it if you are not getting sufficient results.    If you are using a dataset composed of pairs like (query, response), (passage, summary), etc., you can use the [Multiple Negatives Ranking Loss](https://oreil.ly/oNcsQ).    In a batch containing (query, response) pairs (q1, r1), (q2, r2)…​(qn, rn), for each query, there will be a positive pair, e.g., (q1, r1) and n – 1 negative pairs, e.g., (q1, r2), (q1, r3)…​etc. The loss function minimizes the negative log likelihood.    ###### Tip    Use [`CachedMultipleNegativesRankingLoss`](https://oreil.ly/QwBlI), available in Sentence Transformers, which allows you to use larger batch sizes, leading to better performance.    Now that we have discussed all the ingredients needed for fine-tuning, let’s put it all together with the `SentenceTransformerTrainer` class:    ``` 从 datasets 导入 load_dataset 从 sentence_transformers 导入 SentenceTransformer, SentenceTransformerTrainer 从 sentence_transformers.losses 导入 TripletLoss model = SentenceTransformer("'all-mpnet-base-v2'") dataset = load_dataset("csv", data_files="negatives_dataset.csv") loss = TripletLoss(model) trainer = SentenceTransformerTrainer(     model=model,     train_dataset=dataset,     loss=loss    ) trainer.train() model.save_pretrained("mpnet_finetuned_negatives") ```py    The full code is available in the book’s [GitHub repo](https://oreil.ly/llm-playbooks).    ###### Tip    Watch out for overfitting! You can reduce your learning rate if you notice the model overfitting.    ###### Note    [Zhou et al.](https://oreil.ly/BPdRD) show that in the context of embeddings, cosine similarity tends to underestimate the similarity between high-frequency words. This is because high-frequency words occupy distinct regions in the embedding space, leading to larger distances from other words. On the other hand, low-frequency words tend to be more concentrated geometrically.    # Instruction Embeddings    So far we have seen that embedding models are specialized for solving a specific task, like semantic search or paraphrasing. A recent development ties together embedding models and the concept of instruction-tuning, which we discussed in Chapter 6. Imagine if you could use the same embedding model to generate different embeddings for the same document, based on the task it is going to be used for. One such model is called Instructor. [Instructor embeddings](https://oreil.ly/mSIhG) allow you to optionally specify the domain, text type (whether it is a sentence, paragraph, etc.), and task, along with the text during encoding.    Here is an example:    ``` !pip install InstructorEmbedding  from InstructorEmbedding 导入 INSTRUCTOR model = INSTRUCTOR('hkunlp/instructor-large')  customized_embeddings = model.encode( [['Represent the question for retrieving supporting documents:',   'Who is the CEO of Apple'],  ['Represent the sentence for retrieval:',   'Tim Cook is the CEO of Apple'],  ['Represent the sentence for retrieval:',   'He is a musically gifted CEO'], ) ```py    The creators of Instructor recommend using this instruction template:    ``` ‘Represent the {domain} {text_type} for {task_objective}:’ ```py    where `{domain}` represents the domain of the text like law, finance, etc. The optional `{text_type}` represents the unit of text being encoded, like a question, sentence, paragraph, etc. `{task_objective}` represents the task for which we are using the embeddings, like semantic search, paraphrase detection, etc.    In the context of semantic search, they recommend the instruction “Represent the question for retrieving supporting documents” for queries, and “Represent the sentence for retrieval” for documents.    Another way the principle of instruction-tuning can be applied to retrieval is with *description-based retrieval*, where the query can be the description of the text that needs to be retrieved, rather than an instantiation (example) of the text that needs to be retrieved. [Ravfogel et al.](https://oreil.ly/rp8Q-) have published description-based retrieval models that in my experience are very effective. Note that these models have a dual-encoder setup: separate models are used to encode the query and documents.    # Optimizing Embedding Size    Many applications involve generating billions of embeddings. As we have seen, modern embeddings sometimes have as many as thousands of dimensions. If each dimension is represented in float32, then it needs four bytes of memory per dimension. Therefore, storing 100 million vectors generated from the all-mpnet-base-v2 model, which has 768 dimensions, needs close to 300 GB of memory!    It is not uncommon to represent a single sentence, almost always no longer than 40 tokens, with a 768-dimension vector. Do we really need 768 dimensions to represent 40 tokens? The reality is that embedding training is very inefficient, and a large number of dimensions are not really useful.    Therefore, several embedding truncation and quantization approaches have been developed to optimize embedding size and reduce storage and compute requirements. If you are operating in an environment with more than a few million vectors, these techniques are likely to be useful to you. Let’s look at some of these approaches.    ## Matryoshka Embeddings    Matryoshka embeddings are named after [Matryoshka dolls](https://oreil.ly/OC6Yj), which refer to a set of wooden dolls that are placed inside each other in decreasing order of size, originating from Russia. Matryoshka embeddings are trained such that the earlier dimensions of the vector contain more important information than the later dimensions. This allows us to truncate vectors depending on the requirements of the application with respect to cost, latency, and performance.    The technique used to train these embeddings is called Matryoshka Representation Learning (MRL). In MRL, we first choose a set of truncation dimensions. For example a 1,024-dimension vector can have truncation dimensions 128, 256, 512, and 768\. During the training process, we calculate the loss over each of the truncation dimensions as well as the full dimension. The losses are then added and weighted. In our example, the first 128 dimensions learn from the loss calculated over the first 128, 256, 512, 768, and 1,024 dimensions of the vector. The end result is that the initial dimensions of the vector will encode more important information because they learn from richer losses.    Training using MRL is supported by the Sentence Transformers library. Let’s see how it works in practice:    ``` 从 sentence_transformers 导入 SentenceTransformer 从 sentence_transformers 导入 SentenceTransformerTrainer, losses 从 datasets 导入 load_dataset model = SentenceTransformer("all-mpnet-base-v2") train_dataset = load_dataset("csv", data_files="finetune_dataset.csv") loss = losses.MultipleNegativesRankingLoss(model) loss = losses.MatryoshkaLoss(model, loss, [768, 512, 256, 128]])  trainer = SentenceTransformerTrainer(     model=model,     train_dataset=train_dataset,     loss=loss, ) trainer.train() ```py    [Tom Aarsen](https://oreil.ly/sA5fo) observed in his experiments that even at 8.3% of the original embedding size, the Matryoshka model preserves 98.37% of the original performance. This makes it a very effective technique that will come in handy when you are working with large datasets.    Similar to how we can reduce the effective dimension of our embeddings using MRL, we can also reduce the effective number of layers of the embedding model, leading to faster inference. This is done by extracting embeddings from the lower layers of the model. To facilitate the lower layers of the model aligning high-quality embeddings with the embeddings of the last layer of the model, a K-L divergence loss is employed between the final layer and each of the lower layers. This technique was first introduced by [Li et al.’s](https://oreil.ly/fzIPD) Espresso Sentence Embeddings.    [Tom Aarsen](https://oreil.ly/DIoTe) observed in his experiments that removing half the layers leads to a 2x improvement in speed with 85% of the original performance preserved.    The Sentence Transformers library allows you to combine Matryoshka representations with layer reduction using the [Matryoshka2dLoss](https://oreil.ly/xzG-a).    ## Binary and Integer Embeddings    An alternative to truncation is quantization. With binary and integer quantization, the number of vector dimensions remains the same, but each dimension is represented by fewer bits. Recall that typically embedding vectors are represented in float32, thus taking four bytes of memory per dimension.    At the extreme level, the four bytes can be represented with just one bit, resulting in a 32x reduction in storage requirements. This type of compression is generally done by sacrificing the precision of the vector values.    A simple way to convert a four-byte vector to a one-bit vector is to assign a value of 1 if the original value is positive, and 0 if it is negative. Note that you might need to perform some scaling to achieve best results. After packing these bits into bytes, a 512-dimension vector can be represented in just 512 / 8 = 64 bytes, instead of 512 × 4 = 2,048 bytes.    Another advantage with using binary embeddings is that computing similarity only needs simple bitwise operations, thus vastly speeding up retrieval. However, quantization negatively affects performance.    You can use the `Sentence Transformers` library to quantize embeddings:    ``` 从 sentence_transformers.quantization 导入 quantize_embeddings model = SentenceTransformer("all-mpnet-base-v2") embeddings = model.encode(["I heard the horses are excited for Halloween.", "Dalmatians are the most patriotic of dogs.", "This restaurant is making me `nostalgic``.``"])` ```py `binary_embeddings` `=` `quantize_embeddings``(``embeddings``,` `precision``=``"binary"``)` ``` ```py   ``` `` `quantize_embeddings` 也支持 int8 量化。在这种方案中，代表每个维度的四个字节被转换成一个整数值，用一个字节表示。这个整数可以是带符号的或无符号的，因此可以表示介于-127 和 127 或介于 0 和 255 之间的值。转换过程使用嵌入的校准数据集指导，从中我们计算每个维度的最小值和最大值。然后，这些值用于归一化公式，将数字从一种范围转换为另一种范围。    ###### 小贴士    已有研究表明，对于某些[嵌入模型](https://oreil.ly/Mp3pu)，二进制嵌入比 int8 嵌入表现更好，尽管精度有所降低！这很大程度上是因为使用的校准数据集和将浮点值映射到 int8 值桶的挑战。 `` ```py  `` `## Product Quantization    Another promising quantization method is called [*product quantization*](https://oreil.ly/aJq2C). In this technique, a vector is divided into chunks of equal size. The chunks are then clustered. The number of clusters is set to the number of values that can be represented by the quantized embedding. For example, if we aim to quantize to int8, then the number of values that can be represented is 256, and thus the number of clusters is 256\. Each cluster is associated with an identifier, which is a unique value between 0 and 255\. Each chunk belongs to the cluster whose centroid the chunk is closest to.    Thus, the original float32 vector can now be represented by a list of cluster identifiers corresponding to the clusters the chunks belong to. The larger the chunk size, the more the compression. Thus if the vector is divided into five chunks, the resulting embedding will have only five dimensions. Unlike int8 and binary quantization, product quantization also reduces the number of dimensions needed to represent a vector. However, the performance drop is higher.    Choose your quantization technique by determining your relative product priorities for criteria like cost, performance, and speed.    ###### Tip    Optimizing embeddings for storage come with a performance hit. However, if there is plenty of redundancy in the document corpus, answers to typical user queries might be found in several documents, and hence the user may not feel this performance drop.    Now that we have seen various techniques to practically implement embedding-based retrieval, let’s next figure out the textual units we need to embed into distinct vectors.` ``  ````` ```py`# Chunking    As noted in “Introduction to Embeddings”, embedding models support very limited context lengths, and the effectiveness of embedding similarity matching decreases as the text length increases. Therefore, it is natural to split documents into manageable units called chunks and embed each chunk into one or more vectors.    A chunk can be defined as a semantically coherent and not necessarily contiguous part of a document. The average chunk length depends on the context length supported by the language model, and the number of chunks returned to the model (the top-k) in response to a user query. As models become increasingly affordable to operate and support ever-larger context lengths, the permissible chunk size grows.    Each chunk can either be represented by a single vector or can be further broken down into units, with each unit being represented by a separate vector. A unit could be a sentence, a paragraph, or even a section. Typically, the smaller the unit, the better. For your application, test your expected user queries against different granularities and see what works best.    Consider a scenario where a document corpus has been broken down into units represented by embeddings. For a given user query, we can calculate the cosine similarity between the user query vector and each of the document vectors. The chunks corresponding to the most similar vectors are then retrieved. This ensures that the embedding matching happens at a lower granularity, like a sentence, but the model receives the entirety of the chunk the sentence belongs to, thus providing sufficient background context to the model.    A question I am frequently asked by ML practitioners is, “What is the ideal chunk size and what are some effective chunking strategies?” Determining the right chunk size and boundaries are key challenges practitioners face when using embedding-based retrieval. In this section, we will discuss a few chunking strategies, introduced in order of increasing complexity.    In the basic implementation of embedding-based retrieval, each vector is a distinct island, disconnected from all other islands. The text represented by Vector A is not able to influence text represented by Vector B in any way. Therefore, we need to connect these islands in some way or make these islands as self-contained as possible. With these objectives in mind, let’s look at some chunking strategies that go beyond naive paragraph or section splitting.    ## Sliding Window Chunking    Consider a situation where the embedding similarity function returns a unit in Chunk 45 as the most similar vector to your query vector. However, text in Chunk 44, which immediately precedes Chunk 45 in the document, contains relevant information contextualizing Chunk 45\. The vectors in Chunk 44 have a very low similarity score with the query, and as a result, Chunk 44 is not selected for retrieval. One way to fix this is by using sliding window chunking, where each text can be present in multiple chunks, thus allowing neighboring context to be effectively represented in a coherent block.    ## Metadata-Aware Chunking    Any metadata that you have about the document can be leveraged to determine chunking boundaries. Useful metadata information includes paragraph boundaries, section and subsection boundaries, etc. If the metadata isn’t already available, you might need to use document parsing techniques to extract this information. Several libraries can facilitate this, including [Unstructured](https://oreil.ly/CoX46).    ## Layout-Aware Chunking    A more involved form of metadata-aware chunking is layout-aware chunking. In this approach we use computer vision techniques to extract layout information about the document, including the placement and scope of textual elements, the titles, subtitles, font size of text, etc.; use this metadata to inform the chunking process. Both open source and proprietary tools can facilitate layout extraction. They include tools like [Amazon Textractor](https://oreil.ly/fvkiT), [Unstructured](https://oreil.ly/CoX46), and layout-aware language models like [LayoutLMv3](https://oreil.ly/Od5fA).    For example, using this approach we can know the scope of a subsection, and thus insert the subsection title at the beginning of each chunk comprising text from that subsection.    You can also use techniques like ColPali that employ vision models to directly embed a page or section of the document and perform retrieval over it. This may remove the need for chunking entirely but might be more expensive overall.    ## Semantic Chunking    The principle behind semantic chunking is that similar information should be grouped into coherent chunks. Paragraph boundaries provide a weak signal for semantic chunking, but more advanced methods can be employed. One approach is to cluster the document based on topics, with each chunk containing information pertaining to the same topic. The chunks need not necessarily be built from contiguous text if it makes sense for the application. A more advanced approach is to use [Bollinger bands-based chunking](https://oreil.ly/1MwK1). The book’s [GitHub repository](https://oreil.ly/llm-playbooks) contains an experimental implementation of this form of chunking.    Semantic chunking can also be employed to connect different chunks with each other. Once the chunks have been assigned, similar chunks can be grouped based on embedding similarity, allowing them to be retrieved along with the chunk having the highest similarity score. Each chunk does not necessarily need to consist of content from the same document, as long as the metadata associated with each sub-chunk is retained.    A basic implementation of semantic chunking is available in [LangChain](https://oreil.ly/tm8tk).    ###### Note    Highly performant semantic chunking can be performed through LLMs. But it will be a huge cost overhead if the size of your data corpus is very large. Sometimes good old regex can be enough. Jina AI created a complex 50-line [regex-based chunker](https://oreil.ly/x5UO8) that you can try as an initial option.    Despite using all these techniques, effective chunking still remains a problem. Consider the following real-world example from a financial document:    > Page 5: *All numbers in the document are in millions* >  > Page 84: *The related party transaction amounts to $213.45*    In this case the related party transaction actually amounts to $213M dollars but the LLM would never know this because the text from page 5 is not likely to be part of the same chunk.    A related problem is the difficulty in understanding scope boundaries. When does a subsection end and a new subsection begins? What is the scope of the rule in page 5 in the given example? What if it is overridden in the middle of a document? Not all documents have perfect visual cues or structure. Not all documents are well structured into sections, subsections, and paragraphs. These are unsolved problems and are the cause of a sizable proportion of RAG failure modes.    ## Late Chunking    One way of supporting long-range dependencies in text is to use [late chunking](https://oreil.ly/IxTQx), a method introduced by Jina AI. Recall from earlier in the chapter that embeddings are generated by typically pooling the vectors from the last layer of the underlying language model.    Given that we have access to long-context language models that can accept an entire long document in a single input, we can use such a long-context model as our underlying model for generating embeddings. We feed an entire document (or as large a part as the model can handle) to the long-context model, so that vectors are generated for each of the input tokens. As explained in Chapter 4, each token vector encapsulates its meaning based on its relationship with all other tokens in the sequence. This enables long-context dependencies to be captured.    The pooling operation to extract the embeddings is performed on smaller segments of the input, where the segment boundaries can be determined by any of the chunking algorithms. Thus, we can have several embeddings representing the same document but each of them representing distinct parts of the input.    # Vector Databases    Depending on your application, you may have to deal with millions or billions of vectors, with the need to generate and store new vectors and their associated metadata tags every day. Vector databases facilitate this. Both self-hosted and cloud-based, open source, and proprietary options are available. Weviate, Milvus, Pinecone, Chroma, Qdrant, and LanceDB are some of the popular vector databases. More established players like ElasticSearch, Redis, and Postgres also provide vector database support.    These days, the features provided by vector databases are converging, given the prevalence of a small set of very popular retrieval use cases.    Let’s now look at how vector databases work. Probably the simplest one to get started with is Chroma, which is open source and can run locally on your machine or can be deployed on AWS:    ``` !pip install chromadb  导入 chromadb chroma_client = chromadb.Client()  collection = chroma_client.create_collection(name="mango_science") chunks = ['353 varieties of mangoes are now extinct', 'Mangoes are grown in the tropics'] metadata = [{"topic": "extinction", "chapter": "2"}, {"topic": "regions",   "chapter": "5"}] unique_ids = [str(i) for i in range(len(chunks))]  collection.add(    documents=chunks,    metadatas=metadata,    ids=unique_ids   ) results = collection.query(    query_texts=["Where are mangoes grown?"],    n_results=2,    where={"chapter": { "$ne": "2"}},    where_document={"$contains":"grown"} ) ```py    Most vector databases offer:    *   Approximate nearest neighbor search in addition to exact search, to reduce latency           *   Ability to filter using metadata, like the *where* clause in SQL           *   Ability to integrate keyword search or algorithms like BM25           *   Support Boolean search operations, so that multiple search clauses can be combined with AND or OR operations           *   Ability to update or delete entries in the database in real time              # Interpreting Embeddings    What features of text do embeddings learn? Why are two sentences sometimes closer to/farther from each other in the embedding space than we expect? Can we know what each dimension of an embedding vector represents?    A key limitation in embedding-based retrieval compared to traditional techniques is the lack of interpretability in ranking decisions. There is a whole body of research dedicated to improving interpretability of neural networks, LLMs, and embeddings. In Chapter 5, we introduced some interpretability techniques for understanding LLMs. In this section, we will focus on embedding interpretability in particular. One benefit of understanding the features represented in embedding space is that we could leverage that knowledge to steer embeddings for our own purposes.    One promising technique for imparting interpretability is to use SAEs. Let’s understand what they mean and how they are trained and used to enhance interpretability.    A language model may learn millions of features, but for any given input, only a few of those features are relevant or activated. This is what we mean by sparsity. Even as they learn lots of features, there are only a limited number of dimensions in an embedding vector. Therefore, each dimension contributes to many features that can interfere with each other. If you train a [sparse autoencoder](https://oreil.ly/oiXb7) over these embeddings, you can derive independent interpretable features.    In his [Prism project](https://oreil.ly/efzz1), Linus Lee uses SAEs to explore the features of a T5-based embedding model.    Some of the identified features include:    *   Presence of negation           *   Expression of possibility or speculation           *   Employment and labor concepts           *   Possessive syntax at sentence start              For a longer list of identified features, refer to [Linus Lee’s blog post](https://oreil.ly/efzz1).    # Summary    In this chapter, we introduced the concept of embeddings, examined their internals, and showed various techniques for generating them. We also discussed techniques for fine-tuning embeddings on our own data. We learned how to determine the data granularities at which we construct embeddings, discussing several chunking techniques in the process. Finally, we explored techniques to visualize and interpret embeddings.    In the next chapter, we will explore RAG, an application paradigm that is by far the most popular use case for embeddings today. We will present the steps involved in a typical RAG workflow and review each of these steps in detail. We will also discuss the technical decisions involved in building a RAG application and provide pointers on how to think through various tradeoffs.```` ```py`` ``````py ``````py` ``````py`` ``````py``` ``````py````
