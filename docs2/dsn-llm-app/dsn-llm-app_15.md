# 第十二章. 检索增强生成

在第十章中，我们展示了如何通过将 LLM 与外部数据和软件接口来大幅扩展 LLM 的功能。在第十一章中，我们介绍了基于嵌入的检索概念，这是一种从数据存储中检索相关数据的基础技术。掌握了这些知识，让我们全面地探讨通过外部数据增强 LLM 的应用范式，即检索增强生成（RAG）。

在本章中，我们将全面审视 RAG 管道，深入探讨构成典型 RAG 应用程序工作流程的每个步骤。我们将探索实施 RAG 所涉及的各项决策，包括我们可以检索什么类型的数据、如何检索以及何时检索。我们将强调 RAG 如何帮助不仅在模型推理期间，而且在模型训练和微调期间。我们还将比较 RAG 与其他范式，并讨论 RAG 在与其他替代方案相比时表现优异的场景。

# RAG 的需求

如第十章中所述，RAG 是一个术语，用于描述使用外部数据源增强 LLM 能力的一系列技术。以下是我们可能想要使用 RAG 的一些原因：

+   我们需要 LLM 访问我们的私有/专有数据，或者 LLM 预训练数据集之外的数据。使用 RAG 是一个比在私有数据上预训练 LLM 更轻量级的选择。

+   为了减少幻觉的风险，我们希望 LLM 通过检索机制提供的数据而不是依赖其自身的内部知识。RAG 促进了这一点。RAG 还使数据引用更加准确，将 LLM 的输出与其真实来源相连接。

+   我们希望 LLM 能够回答关于 LLM 预训练后出现的近期事件和概念的问题。虽然存在像[MEMIT](https://oreil.ly/kxI3j)这样的记忆编辑技术，用于用新知识更新 LLM 参数，但它们目前还不可靠或可扩展。正如第七章中讨论的，持续训练 LLM 以保持其知识更新是昂贵且风险较高的。

+   我们希望 LLM 能够回答涉及长尾实体的查询，这些实体在预训练数据集中出现的频率很低。

# 典型的 RAG 场景

既然我们已经看到了*为什么*我们需要 RAG，那么让我们来探讨*哪里*我们可以利用它。最流行的四种场景是：

检索外部知识

这是已经看到很多成功案例并实现商业化的主要用例。正如本章前面所讨论的，我们可以使用 RAG 来填补 LLM 的知识空白或降低幻觉风险。

检索上下文历史

LLMs 的上下文窗口有限，但通常我们需要比上下文窗口能容纳的更多上下文来回答查询。我们也希望与 LLM 进行比上下文窗口能容纳的更长时间的对话。在这些情况下，当需要时，我们可以检索对话历史或会话上下文的一部分。

检索上下文训练示例

少样本学习是一种有效的途径，可以帮助 LLMs 熟悉任务的输入-输出映射。您可以通过根据当前输入动态选择少样本示例来提高少样本学习的有效性。在推理时，这些少样本示例可以从训练示例数据存储库中检索。

检索工具相关信息

如第十章所述，LLMs 可以将软件工具作为其工作流程的一部分来调用。可用的工具列表及其描述存储在工具存储库中。然后 LLM 可以使用检索来进行工具选择，选择最适合任务的工具。工具相关信息也可以包括 API 文档，例如。

# 决定何时检索

在一个代理工作流程的每个步骤中，LLM 可以使用以下步骤之一来推进其任务：

+   使用其内部能力

+   从几个数据存储库中选择

+   从几种软件工具中选择

有可能 LLM 可以使用其参数化记忆完全解决的任务，但一个或多个数据存储库也可能包含解决这些任务所需的数据。在这些情况下，鉴于我们之前提出的所有好处，我们应该默认使用 RAG 吗？

我们在本章前面看到，LLMs 在处理长尾信息方面存在困难，而 RAG 可以成为回答关于长尾实体的有效手段。然而，[Mallen 等人](https://oreil.ly/MF7Y1)表明，对于关于更常见实体的查询，LLM 有时可能比 RAG 更好地回答查询。这是由于检索模型的不可避免限制，它可能会检索到无关或不正确的信息，这可能会误导 LLM。

对于给定的查询，您可以动态确定是使用检索还是依赖 LLM 的参数化记忆。决定采取正确方法的规则包括：

+   查询是否关于更频繁出现的实体。例如，LLM 更有可能记住泰勒·斯威夫特的生日，而不是一个地方乐队的替补鼓手的生日，其维基百科页面是一个简短条目。

+   查询是否有时效性限制，即所需的数据可能在 LLM 的知识截止日期之前不存在。

+   无论模型是否如第七章所述进行了持续预训练或记忆调整，以及给定的查询与训练所涉及的概念相关。

如果您使用 LLM 进行通用问答，Mallen 等人表明，您可以使用像维基百科这样的来源作为实体的伪流行度指标。如果您的输入中存在的实体在维基百科中的计数超过一个阈值，那么 LLM 可以选择在没有使用 RAG 的情况下自行回答问题。请注意，阈值可能在不同 LLM 之间有所不同。这种策略仅在您对 LLM 预先训练的数据集有良好理解的情况下才有效。

动态决定何时检索数据也可以帮助优化模型的延迟和响应性，因为 RAG 管道将引入额外的开销。

###### 小贴士

动态检索在您使用非常大的 LLM 时非常有用。对于较小的模型（7B 或以下），几乎总是有益于优先使用 RAG 而不是依赖 LLM 的内部内存。

# RAG 管道

典型的 RAG 应用遵循*检索-读取*框架，如第十一章中讨论的那样。在响应查询时，检索模型会识别与回答查询相关的文档。然后，这些文档被传递给 LLM 作为上下文，LLM 可以依赖这些上下文以及其内部能力来生成响应。在实践中，我们通常需要添加很多功能才能在生产环境中使 RAG 工作。这涉及到在检索-读取框架中添加几个额外的可选阶段。在实践中，您的管道阶段可能包括一个*重写-检索-读取-精炼-插入-生成*工作流程，其中一些步骤可能包含多个阶段。在本章的后面部分，我们将更详细地介绍每个步骤。

图 12-1 展示了 RAG 管道的各个阶段和涉及的组件。

![RAG-pipeline](img/dllm_1201.png)

###### 图 12-1\. RAG 管道

###### 小贴士

正如本书的其他部分一样，我们将用户或 LLM 请求检索数据称为查询，并将从数据存储中检索到的文本单元称为文档。

让我们用一个例子来说明。考虑一个关于加拿大政治和议会活动的 RAG 应用。该应用可以访问包含议会会议记录的知识库。我们将假设数据使用第十一章中描述的表示技术进行表示。

当用户提出查询时，我们可能希望在将其发送给检索器之前对其进行重新表述。在信息检索（IR）领域，这种做法传统上被称为查询扩展。查询扩展特别有用，因为查询空间和文档空间之间存在词汇不匹配。用户可能在查询中使用与文档中使用的不同的术语。重新表述查询可以帮助弥合词汇差距。一般来说，我们希望以这种方式重新表述查询，以提高检索器检索最相关文档的机会。这一阶段被称为*重写*阶段。

接下来，在*检索*阶段，使用检索模型来检索与查询相关的文档。在第十一章中，我们讨论了基于嵌入的检索，这是 LLM 时代流行的检索范式。检索阶段可能是一个多阶段的复杂管道。

检索可能发生在非常大的文档空间中。在这种情况下，使用更高级的检索模型在计算上是不可行的。因此，检索通常分为两个步骤，第一步使用更快的检索方法（这些天通常是基于嵌入的方法）来检索一份可能相关的文档列表（优化召回率），第二步根据相关性重新排序检索到的列表，以便将排名前 k 的文档作为传递给 LLM 的上下文。这一阶段被称为*重新排序*阶段。

在确定与查询相关的 top-k 文档后，需要将它们传递给 LLM。然而，文档可能不适合上下文窗口，因此需要缩短。它们还可能被重新表述，以便更有可能让 LLM 使用上下文来生成答案。这是在*精炼*阶段完成的。

接下来，我们将精炼步骤的输出提供给 LLM。默认方法是将提示中的所有文档连接起来。然而，您也可以一次传递一个，然后汇总结果。提示中文档的顺序也可能产生影响。几种这样的技术决定了上下文如何被输入到 LLM 中。这被称为*插入*阶段。

最后，在*生成*阶段，LLM 读取包含查询和上下文的提示并生成响应。生成可以一次性完成，或者检索过程可以与生成过程交织进行，即模型可以生成一些标记，然后再次调用检索模型来检索更多内容，生成更多标记，然后再次调用检索模型，依此类推。

每一阶段的输出都可以通过一个*验证*阶段来评估输出质量，甚至采取纠正措施。验证阶段可以采用启发式方法或基于 AI 的方法。

在这个例子中，查询是由人类用户生成的。但如果我们将 RAG 放在代理工作流程的背景下考虑，查询可能是由一个 LLM 生成的。在代理工作流程中，代理可以在任何给定时刻确定它需要检索数据以推进其任务，这就会启动上述流程。

除了检索和生成步骤之外，其余的流程是可选的，是否包含其他步骤取决于你的性能和延迟权衡。

###### 注意

我们的例子涉及在推理时间使用 RAG。RAG 也可以在预训练或微调模型时应用，我们将在本章后面进行描述。

让我们详细检查流程中的每一步。

## 重写

在发出查询后，可能需要对其进行重写以使其更易于检索。重写过程取决于所使用的检索模型。如前所述，查询空间和文档空间之间通常存在不匹配，因为查询所使用的词汇、措辞和语义可能与文档中传达的相关概念大相径庭。

例如，考虑以下查询：“哪些政治家抱怨预算不平衡？”

数据存储中包含以下文本：“参议员帕克斯顿：‘我实在无法忍受我们巨大的赤字。’”

如果你正在使用依赖关键词的传统检索方法，那么在检索过程中，这段文本可能不会被选为相关内容。使用基于嵌入的方法可以缩小这一差距，因为相似句子的嵌入在嵌入空间中彼此更接近，但这并不能完全解决问题。

###### 小贴士

如果查询来自用户，用户可能会在查询中添加指令，例如，“哪些政治家抱怨预算不平衡？请以表格的形式提供结果。”在这种情况下，你将不得不在将查询输入到检索流程之前将查询与指令分开。这可以通过 LLM 使用提示技术如 CoT、ReAct 等来完成，我们分别在第五章 5 和第十章 10 中进行了讨论。

对于使用传统检索技术的系统，查询重写通常是通过查询扩展技术来执行的，其中查询被添加了相似的关键词。基本的查询扩展技术包括在查询中添加关键词的同义词和其他主题信息。

查询扩展的一个经过良好测试的方法是伪相关性反馈（PRF）。在 PRF 中，原始查询用于检索文档，并从这些文档中提取显著术语并添加到原始查询中。

让我们看看 PRF 如何帮助我们查询，“哪些政治家抱怨预算不平衡？”我们使用类似 BM25（本章后面将解释）的检索技术来返回一组 k 个候选文档。然后我们使用诸如词频或更有效的方法[Tf-IDf](https://oreil.ly/5be9z)来提取这些返回文档中出现的显著术语。对于这个例子，显著的短语是“财政政策”、“赤字”、“财务管理不善”和“预算改革”。将这些短语添加到原始查询中，将导致以下文本：

> “参议员帕克斯顿：‘我简直无法忍受我们巨大的赤字！’”检索成功。

近年来，由 LLM 驱动的查询扩展方法越来越受到重视。两个这样的例子是[Query2doc](https://oreil.ly/BDJAN)和[假设文档嵌入（HyDE）](https://oreil.ly/tZyUV)。

让我们探索基于 LLM 的查询扩展是如何工作的。我们将给定的查询传递给 LLM，并要求它生成一个包含查询答案的假设文档。

> *提示:* 对于提供的查询，用 2 句话或更少的文字生成一个假设文档来回答查询。假设文档是加拿大议会程序的记录，包含下议院和参议院的代表的直接陈述。
> 
> *查询:* 哪些政治家抱怨预算不平衡？

提示包含：

+   生成假设文档的说明

+   生成文档的期望长度限制

+   描述包含相关文档的文档语料库

+   原始查询

LLM 生成的假设文档如下：

> *LLM:* “尊敬的马克·韦林顿参议员表示，*政府未能平衡预算是不可接受的，这危及我们的经济未来。*同样，埃米莉·弗莱塞议员评论说，*我们不能继续这种不负责任的支出之路而不解决赤字——是时候采取真正的财政责任了。*”

使用这个假设文档作为查询将返回句子“参议员帕克斯顿：‘我简直无法忍受我们巨大的赤字！’”并具有更高的相似度分数。

虽然这个假设文档在事实上并不准确，甚至有点可笑（加拿大没有名叫马克·韦林顿和埃米莉·弗莱塞的参议员或议员），但它确实包含与实际政治家所说非常相似的措辞和语义。如果我们使用这个文档作为查询（可选地与原始查询结合），那么它与实际政治家谈论该主题的实例在语义上相似的可能性比仅与查询匹配时更高。

假设文档的长度可能与检索单元的典型长度相似。您可以使用较小的 LLM 来生成假设文档，因为我们在这个设置中并不关心事实保证。然而，较小的模型在生成高质量的假设文档方面也不够熟练，因此您将不得不管理这种权衡。LangChain 和 LlamaIndex 都提供了基于假设文档的查询重写的实现。

如果模型已经在包含相关数据的语料库上预训练或微调，那么在提示中添加语料库的描述，如示例所示，将更有可能使生成的文档遵循该数据语料库的结构、格式和语言学。

###### 警告

查询重写技术的另一个陷阱是主题漂移的风险。在假设文档的情况下，文档可能在最初的几个标记之后漂移到不相关的主题。增加查询中标记的 logits 偏差权重可以部分解决这个问题。PRF 技术也容易受到主题漂移的影响。

您还可以将 PRF 风格技术与假设文档相结合。不是生成假设文档来替换或增强查询，而是可以使用它们来提取可以添加到原始查询中的关键词。[Li 等人](https://oreil.ly/cOnMs)提出了一种称为*query2document2keyword*的技术。在这种技术中，LLM 使用查询生成一个假设文档，类似于 HyDE。然后，LLM 被提示从这个文档中提取显著关键词。

然后，我们可以通过过滤步骤进一步改进提取出的关键词的质量。作者提出了使用*自洽性*方法，我们已在第五章中讨论过。概括来说，在自洽性方法中，我们多次重复关键词生成，然后根据它们出现的生成次数选择顶级关键词。

将传统检索与 LLM 驱动的查询重写相结合的另一种方法是首先从初始检索步骤返回顶级-k 文档，然后使用 LLM 从返回的文档中生成显著关键词并将其添加到查询中。

到目前为止，我们讨论了通过修改查询并将其带到文档空间附近来桥接查询文档不匹配问题的技术。解决不匹配问题的另一种方法是按照使文档更接近查询空间的方式表示文档。这种方法包括[doc2query](https://oreil.ly/CGUtP)和[上下文检索](https://oreil.ly/ZJuIu)。虽然如果数据存储非常大，文档重写技术最初可能会有很大的成本，但它们可以在推理时间减少延迟，因为不需要或只需要很少的查询重写。另一方面，查询重写技术易于实现并集成到 RAG 工作流程中。

另一种查询重写的形式称为查询分解。对于代理工作流程中的复杂查询，我们可以让 LLM 将任务分解成多个可以顺序或并行执行的查询，具体取决于查询是如何分解的。我们在第十章中讨论了查询分解技术。

###### 注意

如果你的外部数据以结构化形式存在，如数据库，那么查询需要被重写为一个 SQL 查询或等效的查询，正如在第十章中讨论的那样。

既然我们已经讨论了管道中的查询重写步骤，让我们继续讨论检索步骤。

## 检索

检索步骤是 RAG 管道中最关键的阶段。为什么这样说很容易理解：所有 RAG 应用都受检索质量的限制。即使你使用的是世界上最好的语言模型，如果检索步骤没有检索到回答查询所需的正确文档，你也不会得到正确的结果。因此，这个管道步骤应专注于提高召回率。

我们在第十一章中详细讨论的基于嵌入的检索非常受欢迎。然而，传统的信息检索技术不应被忽视。使用哪种技术取决于查询预期的性质（是否可以通过仅关键词或正则表达式匹配来回答其中很大一部分？），查询与文档词汇不匹配的预期程度，延迟和计算限制，以及性能要求。

###### 注意

信息检索（IR）研究领域已经研究了这些问题很长时间。现在，随着检索在 NLP 中的相关性日益增强，我注意到很多人在重新发明轮子而不是重用 IR 见解。关于检索研究的见解，可以查看 SIGIR、ECIR、TREC 等领先 IR 研究会议的论文。

当你希望检索包含特定单词或短语的全部文档时，基于嵌入的检索方法并不总是合适的。因此，通常将基于关键词的方法与嵌入方法相结合，称为混合搜索。两种方法的结果被合并并输入到检索管道的下一步。大多数向量数据库以某种形式支持混合搜索。

图 12-2 展示了使用混合搜索的检索阶段，展示了其作用。

![混合-搜索](img/dllm_1202.png)

###### 图 12-2\. 混合搜索

我还强烈推荐使用元数据过滤器来提高检索效果。在数据表示和存储阶段收集的元数据越多，检索结果就越好。例如，如果你已经提前对你的数据存储进行了主题建模，你可以通过使用硬编码的规则集或由一个 LLM 确定来限制你的搜索结果只包含特定主题的子集。

接下来，让我们讨论检索方面的有希望的近期进展。

### 生成式检索

如果 LLM 能够识别出需要检索的正确的文档（s），从而消除检索技术的需求，这被称为生成检索。

生成检索通过为文档分配称为 docID 的标识符，并教授 LLM 文档与 docID 之间的关联来实现。一个文档可以与一个或多个 docID 相关联。典型的 docID 可以是：

单个标记

每个文档都可以由词汇表中的一个新标记表示。这意味着，在推理过程中，模型只需要为它想要检索的每个文档输出一个标记。[Pradeep 等人](https://oreil.ly/7JYOM)使用了一个 T5 模型，其中编码器词汇表是标准的 T5 词汇表，但解码器词汇表包含 docID。这种方法仅适用于小型文档语料库。

前缀/子集标记

[Tay 等人](https://oreil.ly/1p1C8)使用文档的前 64 个标记作为 docID，而[Wang 等人](https://oreil.ly/lg3g3)使用从文档中随机选择的 64 个连续标记。

聚类标记

您也可以根据其语义（例如使用嵌入）对文档语料库进行分层聚类，docID 可以是层次结构中每个级别的聚类 ID 的连接。

显著关键词标记

docID 也可以包含代表文档中主题和主题的显著关键词。例如，关于 Transformer 架构的文档可以用 docID“transformer_self-attention_architecture”表示。

教授 LLM 文档与 docID 之间关联的一种方法是通过微调模型。这被称为基于训练的索引。然而，微调需要大量资源，并且不适用于频繁向语料库添加新文档的场景。

[Askari 等人](https://oreil.ly/K5TAB)表明我们可以使用少量样本学习来构建一个不需要训练模型的生成检索系统。首先，对于语料库中的每个文档，使用语言模型生成伪查询。这些伪查询是答案存在于文档中的查询。然后，将这些伪查询在少量样本设置中输入到语言模型，并要求生成 docID。图 12-3 显示了无训练的生成检索的实际应用。

![生成-检索](img/dllm_1203.png)

###### 图 12-3. 生成检索

在推理过程中，模型被提供与图 12-3 中设置类似的查询，并被要求生成与查询相关的正确 docID（s）。使用约束束搜索以确保模型生成的 docID 对应于语料库中的有效 docID。

###### 小贴士

您还可以使用生成式检索根据文档的元数据检索文档。例如，模型可以要求检索苹果公司的 2024 年年度报告。模型可以通过微调模型或使用本节中展示的少样本学习来生成正确的标识符。

最终，生成式检索仅适用于您的文档语料库相对较小、语料库内冗余有限或文档属于一组定义良好的类别（例如，美国所有上市公司的年度报告）的情况。

接下来，让我们讨论紧耦合检索器，这是检索空间中的另一个新话题。

### 紧耦合检索器

如第十一章第十一章中所示，在基于嵌入的检索中，嵌入模型通常独立于检索结果输入的语言模型。我们将它们称为*松耦合*检索器。

相反，一个*紧耦合*检索器被训练成从 LLM 反馈中学习；模型学会检索文本，以最佳方式定位 LLM 生成给定查询的正确输出。紧耦合检索器可以作为单一架构的一部分与生成 LLM 一起训练，或者它们可以单独使用训练好的 LLM 的反馈进行训练。

后者的一个例子是[张等人提出的 LLM-Embedder](https://oreil.ly/Q__8M)，这是一个统一的嵌入模型，可以在单个模型中支持各种检索需求，从知识检索到检索最优的少样本示例。该模型从两种类型的信号中进行训练：一种对比学习设置，通常用于训练嵌入模型（在第十一章第十一章中介绍）和 LLM 反馈。如果一个检索候选者通过改进 LLM 回答查询的性能而获得更大的奖励，那么它将从 LLM 反馈中获得更大的奖励。

紧耦合检索器是您工具箱中用于改进检索的另一个工具。它们绝不是 RAG 管道中的必要步骤。一如既往，实验将显示它们为您的应用提供了多少提升（如果有的话）。

最后，让我们讨论 GraphRAG，这是一种新兴的检索范式，它利用知识图谱来提高检索效果。

### GraphRAG

我们之前讨论的检索方法的一个关键限制是它们无法促进回答需要在不同部分之间建立联系的问题，以及涉及在数据集上总结高级主题的问题。例如，我们之前讨论的所有检索技术在对“在这个数据集中讨论的关键主题是什么？”这样的查询上表现不佳。

解决这些限制的一种方法是通过使用知识图谱。微软发布了 [GraphRAG](https://oreil.ly/V4n_S)，这是一个基于图的 RAG 系统。GraphRAG 通过从底层数据语料库中提取实体和关系来创建知识图谱。然后，该图被用于执行层次语义聚类，并为每个聚类生成摘要。这些摘要使得回答诸如“在这个数据集中讨论的关键主题是什么？”之类的主题性问题成为可能。

GraphRAG 需要大量的初始计算来准备知识图谱。这对于较大的数据集来说可能是限制性的。虽然提取实体相对容易，但提取相关关系则更为困难。

现在我们已经探讨了 RAG 流程中的检索阶段，接下来让我们转向重排序阶段。

## 重排序

检索过程可以被分解为两阶段或多阶段过程，其中初始阶段检索与查询相关的文档列表，随后是一或多个 *重排序* 阶段，这些阶段将文档按相关性排序。重排序器通常比检索器更复杂，因此仅在检索结果上运行（否则我们就会直接使用重排序器作为检索器）。

重排序器通常是在特定用例上微调的语言模型。您可以使用类似 BERT 的模型来构建相关性分类器，其中给定一个查询和一个文档，模型输出文档与回答查询的相关概率。这些模型被称为 *跨编码器*，在这些模型中，查询和文档一起编码，与我们所讨论的基于嵌入的检索模型（称为双编码器）不同，在双编码器中，查询和文档被编码为单独的向量。

作为跨编码器作用的 BERT 模型的输入格式为：

```py
[CLS] query_text [SEP] document_text [SEP]
```

Sentence Transformers 库提供了访问跨编码器的方法，这些跨编码器可以用作 RAG 流程中的重排序器：

```py
from sentence_transformers import CrossEncoder
model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-12-v2", num_labels=1)

query = 'When was the Apple iPhone 15 launched?'
documents = ['Apple iPhone 15 launched with great fanfare in New York',
'He was foolish enough to believe that gifting an iPhone would
  save the relationship',
'On September 22, 2023, I lined up at the Central Park store for the launch of
  the iPhone 15']

ranks = model.rank(query, documents)
for rank in ranks:
   print(rank['score'], documents[rank['corpus_id']])
```

由于我们已将 `num_labels` 设置为 `1`，该模型将将其视为回归任务，使用 sigmoid 激活函数输出介于 0 和 1 之间的分数。

这些天，更先进的模型如 [基于 BERT 的上下文晚期交互 (ColBERT)](https://oreil.ly/N3fOv) 被用于重排序。与刚才讨论的跨编码器设置不同，ColBERT 风格的模型允许预先计算文档表示，从而实现更快的推理。

在 ColBERT 中，查询和文档分别使用 BERT 进行编码，为查询和文档中的每个标记生成标记级嵌入向量。对于查询中的每个标记，相应的嵌入与文档中每个标记嵌入的嵌入进行比较，生成相似度分数。每个查询标记的最大相似度分数相加，得到最终的关联度分数。这种架构称为*后期交互*，因为查询和文档不是一起编码，而是在处理过程中的后期才进行交互。与传统的交叉编码器相比，后期交互可以节省时间，因为文档嵌入可以预先创建和存储。

图 12-4 展示了 ColBERT 模型在运行中的情况，说明了查询和文档之间的后期交互。

![cross-encodersl](img/dllm_1204.png)

###### 图 12-4\. ColBERT

接下来，让我们探索一些高级的重排序技术。

### 查询似然模型（QLM）

QLM（查询语言模型）估计给定候选文档作为输入生成查询的概率。你可以将一个 LLM（大型语言模型）视为一个 QLM，利用其零样本能力根据查询标记的概率对候选文档进行排序。或者，你可以对 LLM 进行微调，以查询生成任务来提高其作为 QLM 的适用性。

一个典型的 QLM 提示可能如下所示：“生成一个与给定文档<文档内容>最相关的疑问”。

在检索阶段从查询中获取与查询相关的 top-k 文档后，每个文档都使用此提示输入到 LLM 中。然后使用模型的对数几率计算查询标记的似然性。然后根据似然性对文档进行排序，提供相关性排名。

###### 警告

[庄等人](https://oreil.ly/QnWWh)表明，在其指令微调训练集中不包含查询生成任务的指令微调模型会失去成为有效零样本 QLM 的能力。这是指令微调模型在未训练的任务上与基模型相比性能下降的又一案例。

注意，为了计算查询标记的概率，我们需要访问模型的对数几率。截至本书编写时，大多数专有模型提供商，包括 OpenAI，尚未提供对模型对数几率的完全访问。因此，LLM 作为 QLM 的方法只能使用开源模型来实现。

为了减少延迟，你理想情况下希望 QLM 尽可能小。然而，较小的模型作为 QLM 的效果较差。有效地微调较小的 LLM 以进行查询生成可能是最佳选择。

### LLM 蒸馏用于排序

在本章的早期，我们看到了仅编码器模型如 BERT 可以作为重排序器。最近，解码器 LLM 也被训练以三种方式直接对候选文档进行排序：

点对点排序

每个候选文档单独输入到大型语言模型（LLM）中。LLM 提供对其相关性的布尔判断。或者，它也可以提供一个数值分数，尽管这不太可靠。

成对排名

对于每一对候选文档，LLM 指示哪篇文档更相关。为了得到完整的排名，需要进行 N² 次这样的比较。

列表排名

所有候选文档都贴上标识符并输入到 LLM 中，并要求 LLM 根据对应文档的相关性递减顺序生成一个排序的标识符列表。

通常，点对排名是最容易使用的，但可能不是[最有效的](https://oreil.ly/DvmtC)。列表排名可能需要一个大的上下文窗口，而成对排名需要大量的比较。成对排名是这些技术中最有效的，因为它涉及直接比较。图 12-5 展示了点对、成对和列表排名是如何工作的。

排名 LLM 的例子包括 [RankGPT](https://oreil.ly/6XoOG)、[RankVicuna](https://oreil.ly/00Dan) 和 [RankZephyr](https://oreil.ly/AAbUE)。

这些模型是通过从更大的 LLM 中提取知识来训练的，这是我们首次在第九章中学习的。例如，训练 RankVicuna 的过程如下：

+   训练集中的查询通过一级检索器（如 BM25）生成候选文档列表。

+   这个列表被传递给一个更大的 LLM，该 LLM 生成一个按候选者排序的列表。

+   查询和排序列表用于微调较小的 LLM。

[RankVicuna](https://oreil.ly/cFLSc) 的创造者表明，随着一级检索的有效性提高，RankVicuna 可能的性能提升因回报递减而降低。他们还报告说，通过打乱候选文档的输入顺序来扩充数据集可以提升模型性能。

![llm-rerankers](img/dllm_1205.png)

###### 图 12-5\. 解码器 LLM 重新排序器

###### 小贴士

你可以将检索和重新排序阶段的结果结合起来，以获得候选文档的最终相关性排名。这需要执行关键字加权，例如。你也可以通过元数据（如发布日期）来加权你的相关性排名（更近期的文档权重更高）。

既然我们已经讨论了重新排序阶段，让我们继续到 RAG 管道的精细调整步骤。

## 精细调整

一旦检索并选择了与给定查询相关的候选文本，它们可以被输入到 LLM 中。然而，LLM 的上下文窗口是有限的，所以我们可能想要缩短检索到的文本的长度。我们可能还想重新措辞，使其更适合 LLM 处理。另一个可能的操作是根据某些规则过滤掉一些检索到的文本。所有这些都是在*精炼*阶段进行的。在本节中，我们将讨论两种这样的技术，即摘要化和笔记链。让我们先讨论如何对检索到的文本进行摘要化。

###### 小贴士

精炼阶段可以是一个独立的阶段，也可以与最终的生成阶段配对，在精炼检索到的文档后立即提供最终响应，作为同一提示或提示链的一部分。

### 摘要化

如果检索到的片段相对较大，摘要化是有用的。它可以是提取式或抽象式。提取式摘要从原始文本中提取关键句子，而不对其进行修改。抽象式摘要从头开始生成，借鉴原始文本的内容。摘要器还可以作为质量过滤器；如果文档与查询不相关，它可以输出一个空摘要。摘要应该是相关的、简洁的，并且忠实于原始文本。

###### 注意

这些摘要不是为了人类消费，而是为了被 LLM 消费。因此，它们不一定与传统的摘要器具有相同的目标。这里的主要目标是生成一个帮助 LLM 输出正确答案的摘要。

你应该选择提取式还是抽象式摘要？提取式摘要几乎总是忠实于原文，因为它保留了原文的意义。抽象式摘要存在幻觉的风险。另一方面，由于它们能够结合文档内部和跨文档不同位置的信息，抽象式摘要可能更具相关性。

虽然你可以利用 LLM 的零样本能力进行提取式和抽象式摘要，但微调它们以使生成的摘要专门优化以使 LLM 能够生成正确答案会更有效（尽管成本较高）。我们将这些紧密耦合的摘要器称为。

[Xu 等人](https://oreil.ly/XCpyr)介绍了训练提取式和抽象式摘要器的技术。让我们详细地了解一下。

对于提取式摘要，我们希望从检索到的文档中提取一个子句集作为其摘要。这是通过为输入查询和检索文档中的每个句子生成嵌入来完成的。在嵌入空间中最相似于输入查询的前 k 个句子被选为摘要。嵌入距离是衡量文档句子在使 LLM 生成正确输出方面有效性的度量。

提取式摘要器是通过对比学习进行训练的，这一点我们在第十一章中讨论过。在对比学习中，每个训练示例都是一个三元组：锚文本、与锚文本相似的正面示例，以及与锚文本不相似的负面示例。为了生成训练示例，对于检索到的文档中的每一句话，我们在输入查询前加上这句话，并计算生成黄金真实输出标记的概率。概率最高的句子被选为正面示例。对于负面示例，我们选择最多五个概率低于阈值的句子。这个数据集随后被用来训练模型。

对于抽象式摘要，我们可以提炼一个更大的 LLM，即使用它的输出微调一个较小的 LLM。

为了生成训练数据集，我们可以构建一些提示模板，并使用更大的 LLM 来生成检索文档的无样本摘要。请注意，我们正在生成所有检索文档的单个摘要。类似于提取式摘要技术，对于每个生成的摘要，我们将它添加到输入文本前，并计算正确输出标记的概率。我们选择概率最高的摘要作为我们的训练集的一部分。

在推理过程中，如果给定的摘要前缀生成正确输出的概率低于不添加任何摘要的情况，那么我们认为摘要所代表的文本是不相关的，并生成一个空摘要。这允许我们过滤掉不相关的文档。

图 12-6 展示了紧耦合抽象式摘要器在训练过程中的工作流程。

![抽象式摘要](img/dllm_1206.png)

###### 图 12-6\. 抽象式摘要

###### 小贴士

如果你计划更改目标 LLM，你可能需要重新训练摘要模型。虽然摘要器可以在模型之间迁移，但仍然会有轻微的性能下降。

紧耦合的摘要器，虽然初始训练成本较高，但可以是一种有效的方法，在重新表述文本的同时，从检索文本中移除不相关信息，并降低 LLM 的歧义性。

### 笔记链

另一种重新表述检索文本的方法是生成*笔记*。

如果检索到的文本包含可能误导模型的不相关信息，这将是有害的。本质上，LLM 必须应对三种类型的场景：

+   检索到的文档包含用户查询的答案，LLM 可以使用它来生成正确的输出。

+   检索到的文档不包含用户查询的答案，但它们提供了 LLM 可以结合其内部知识来得出答案的有价值背景信息。

+   检索到的文档与用户查询不相关，应该被忽略。

LLMs 在区分相关和不相关上下文方面并不擅长。解决这一问题的方法是为每个检索到的文档生成笔记，其中包含检索文档的摘要，并指明它是否包含用户查询的答案，或者只包含相关上下文但不是直接的答案，或者是不相关的。这种技术被称为笔记链（CoN），由 [Yu 等人](https://oreil.ly/hPkKm) 提出。

这里是这些笔记的示例：

> *提示：* 1993 年谁接替了加拿大总理布莱恩·马尔罗尼？
> 
> 文档 1：金·坎贝尔在 1993 年接管了加拿大的领导权，标志着她作为该国第一位女性总理进入历史。然而，她的任期很短，因为她带领进步保守党在那年晚些时候的联邦选举中面临挑战。
> 
> 文档 2：在布莱恩·马尔罗尼的任期之后，加拿大见证了其第一位女性总理的历史性任命，标志着该国政治格局的一次突破性转变。

虽然第一段文本说金·坎贝尔在 1993 年成为总理，但并不明显她是否立即接替了布莱恩·马尔罗尼，或者中间是否有其他总理。然而，第二段文本清楚地表明确实是金·坎贝尔，因为文本提到布莱恩·马尔罗尼被一位女性总理接替。

模型生成的相应笔记如下：

> *LLM:* 第一句话提到金·坎贝尔在 1993 年接管了加拿大的领导权，第二句话指的是布莱恩·马尔罗尼任期之后的时期，导致加拿大第一位女总理的任命。这一序列暗示了金·坎贝尔担任总理的角色直接接替了布莱恩·马尔罗尼的任期，使她成为他的继任者。

使用笔记作为上下文，LLM 可以然后生成正确的答案。

当上下文中包含无关或不充分的信息时，笔记链的真正价值变得明显。生成的笔记可以帮助 LLM 输出“我不知道”。

考虑以下查询：

> *提示：* 加拿大绿党是否支持提出的 2023 年公共交通法案？

以及检索到的文档是：

> 文档 1：加拿大绿党历史上一直强调可持续交通的重要性，将其作为其环境平台的关键组成部分，倡导促进绿色基础设施的政策。
> 
> 文档 2：提出的 2023 年公共交通法案受到了广泛关注和欢迎，反映了公众对可持续交通解决方案重要性的日益增长的共识。

检索到的文档中没有包含查询的答案。生成的相应笔记是：

> *LLM:* 虽然第一段提到该党历史上对可持续交通的重视，第二段提到该法案的受欢迎程度，但都没有确认该党对 2023 年法案的支持或反对。

使用这个笔记，当检索到的文档不包含查询的答案时，LLM 可以生成“未知”。

一个 CoN 提示的例子可以是：

> *提示:* 你将获得一个查询以及{K}个段落，这些段落可能包含可用于回答查询的信息。写笔记总结这些段落的关键点。讨论每个段落与给定问题的相关性，并说明查询的答案是否可以从这些段落的内容中推断出来。

再次，我们可以训练紧密耦合的 CoN 模型以提高其有效性。这可以通过微调 LLM 以引发 CoN 行为来实现。

要生成微调数据集，你可以提示 LLM 为示例查询生成候选笔记。然后，人工评估可以筛选出错误或质量差的笔记。最终数据集包括 CoN 提示、输入查询和检索到的文档作为输入，以及相应的笔记和查询答案作为输出。LLM 可以在此基础上进行微调。

作者（Yu 等人）在训练期间引入了一种加权损失方案。笔记可以比答案长得多，因此在整个标记上均匀加权损失会导致笔记在训练期间获得显著的重要性。这会损害模型收敛。加权损失方案涉及 50%的时间内计算答案标记的损失。

使用 CoN 步骤非常有用，特别是如果检索结果已知包含大量噪声，或者有更高的可能性没有相关文档可供查询服务。CoN 行为对较小的模型来说更难，因此应该使用足够大的模型。

既然我们已经讨论了 RAG 管道的精炼步骤，让我们转到插入步骤。

## 插入

一旦我们确定了要提供给 LLM 以生成查询最终响应的内容，无论是原始检索到的文档、它们的摘要还是笔记，我们需要决定如何在提示中安排它。

标准的方法是将所有内容，或者至少尽可能多的内容，放入上下文窗口中。另一种方法是分别将检索到的每个文档/摘要/笔记作为输入前缀添加到 LLM 中，然后合并输出。

[刘等人](https://oreil.ly/LFR8r)表明，与中间部分相比，语言模型在回忆上下文窗口开始和结束处的信息方面更为擅长。我们可以利用这一知识来重新排序提示中的检索文档。

假设我们为给定查询检索了 10 个文档。这些文档根据相关性排序：Doc1, Doc2,…Doc10。现在，这些文档可以在提示中按以下顺序排列：

> Doc1, Doc3, Doc5, Doc7, Doc9, Doc10, Doc8, Doc6, Doc4, Doc2

因此，最不相关的文档存在于上下文窗口的中间，它们更有可能因为当前长上下文召回限制而被模型忽略。

不同的方法包括按相关性排序文档，例如：

> Doc1, Doc2, Doc3, Doc4, Doc5, Doc6, Doc7, Doc8, Doc9, Doc10

或者按相关性逆序，例如：

> Doc10, Doc9, Doc8, Doc7, Doc6, Doc5, Doc4, Doc3, Doc2, Doc1

这些排序方案仅在输入上下文非常长（超过 5,000 个标记）时才有用。

最后，让我们讨论 RAG 管道中的生成步骤。

## 生成

在这一步，LLM 生成对给定查询的最终响应。标准方法是一次性生成输出。然而，您也可以通过生成一些输出并检索更多上下文，然后再生成一些输出，再检索更多上下文，以此类推，来交错生成和检索过程。

这种方法在维护长文本生成中的连贯性方面可能很有用。生成的文本决定了接下来需要检索的内容。这个过程被称为主动检索。

我们如何决定何时停止生成并开始新的检索步骤？我们可以：

+   每生成 N 个标记后进行检索。

+   每生成每个文本单元后进行检索。（文本单元可以是句子、段落、部分等。）

+   当当前可用上下文被认为不足以生成时进行检索。

有几种方法可以实现这一点。其中之一是向前看主动检索增强生成（FLARE）。[FLARE](https://oreil.ly/eZRdy)的作者介绍了两种主动检索方法：FLARE-Instruct 和 FLARE-Direct。

在 FLARE-Instruct 中，当 LLM 需要更多信息以继续连贯生成时，会提示它以特定语法生成查询。

在 FLARE-Direct 中，LLM 生成一个候选下一句。如果生成的句子中的任何标记的概率低于一个阈值，则激活检索过程。如果没有，则候选句子被视为有效的后续句子，生成过程继续。如果需要进行检索，则可以使用生成的句子作为查询，通过屏蔽低概率标记（因为如果它们不相关/不正确，可能会混淆检索器）。您还可以将句子重新表述为关于低概率标记的问题。

让我们通过 FLARE-Instruct 看看一个例子：

> *提示:* 写一篇关于乌干达奥运金牌得主 Peruth Chemutai 的文章。
> 
> *FLARE-Instruct:* Peruth Chemutai [搜索(Peruth Chemutai 的出生日期)] 是一位专注于 3000 米障碍赛的乌干达长跑运动员。她在 2020 年夏季奥运会上赢得女子 3000 米障碍赛金牌后获得了国际认可，成为第一位赢得奥运金牌的乌干达女性。
> 
> 早年生活
> 
> Chemutai 出生于[Search(Peruth Chemutai 的出生地)]，一个以[Search(Peruth Chemutai 的出生地以什么而闻名？)]而闻名的地区。’

这是一个虚构的例子，因为 FLARE 的真实好处可以在更长的输出中更好地欣赏。如输出所示，模型生成可用于从数据源检索事实性信息的搜索查询。

对于相同的查询，使用 FLARE-Direct，模型生成了候选文章：

> *FLARE-Direct:* Peruth Chemutai（出生于 1999 年 7 月 10 日）是一位乌干达长跑运动员，擅长 3000 米障碍赛。她在 2020 年夏季奥运会上获得女子 3000 米障碍赛金牌后获得了国际认可，成为第一位获得奥运金牌的乌干达女性。
> 
> 早年生活
> 
> Chemutai 出生于乌干达的布库沃区，这是一个以地形崎岖和热情的长跑运动员而闻名的地区。

下划线的标记是低概率标记，可以通过检索相关文本来补充。我们可以要么屏蔽低概率标记并将它们用作检索查询，要么根据屏蔽的标记生成独立的查询，例如，“Peruth Chemutai 什么时候出生？”

生成的一个关键方面是添加适当的引用到事实来源。LLM 可以被微调以使其在响应用户查询时提供与答案一起的引用。其中一个这样的模型是[Cohere 的 Command-R](https://oreil.ly/v0KUs)模型。

如我们所见，知识检索的 RAG 管道可能相当长。然而，对于许多 RAG 应用来说，延迟是一个关键考虑因素。这增加了小型语言模型或更快、非 LLM 基础的方案的重要性。

让我们通过回顾本章开头首次介绍的 RAG 管道图来汇总所有内容。图 12-7 展示了全面 RAG 管道的工作流程。

![RAG-pipeline](img/dllm_1207.png)

###### 图 12-7\. 全面 RAG 管道

到目前为止，我们一直专注于使用 RAG 进行知识检索。现在让我们讨论一些其他用例。

# RAG 用于内存管理

RAG 的一个被低估的应用是扩展 LLM 的上下文窗口。为了回顾，LLM 提示通常包含以下类型的（可选）内容：

预提示或*系统提示*

这些是在每个查询开头提供的总体指令，包括给 LLM 的。根据您的定制需求，系统提示可能占据上下文窗口的很大一部分。

输入提示

这包括当前输入和指令，可选的几步训练示例，以及可能通过检索获取的附加上下文。

对话历史

这包括用户和 LLM 之间对话/交互的历史。将此包含在上下文窗口中，使用户能够与 LLM 进行长时间、连贯的对话。

记事本

这包括由 LLM（在第八章中讨论）生成的中间输出，当生成未来输出时，LLM 可以引用这些输出。Scratchpad 内容是某些提示技术（如 CoT）的产物。

在许多情况下，LLM 有限的上下文窗口不足以包含所有这些数据。此外，我们可能希望通过永久性使对话历史对模型可用，这意味着它随着时间的推移而持续增长。使所有对话历史对 LLM 可用是实现个性化的关键方面。

RAG（Retrieval-Augmented Generation）拯救了！RAG 可以通过在提示中交换相关内容来促进 LLM 的内存管理。这让人联想到操作系统中的内存管理方式。让我们进一步探讨这个抽象。

在操作系统中，内存以层次结构组织，快速（且昂贵）的内存可以直接被处理器访问，而层次结构的高层包含更大、更慢（但相对便宜）的内存。当处理器需要访问某些数据时，它会尝试从内存层次结构的最低层访问。如果数据不在那里，它会搜索层次结构的下一层。如果数据存在，它会将所需数据交换到低层，并交换出当前不需要的数据。这样，操作系统可以支持一个处理器可以直接访问的快速主内存，以及一个可以随时交换的更大的虚拟内存。

这是对操作系统内存管理的一个非常简化的解释。要了解更多详细信息，请查看 Tony 的[“操作系统 — 内存层次结构”](https://oreil.ly/vcciM)。

图 12-8 显示了典型操作系统的内存层次结构。

![os-hierarchy](img/dllm_1208.png)

###### 图 12-8\. 典型操作系统的内存层次结构

类似地，在 LLMs 中，上下文窗口类似于主内存，因为它可以直接被 LLM 访问。然而，我们可以通过实现类似于操作系统虚拟内存的内存系统来无限扩展上下文窗口。这有助于个性化 LLMs，为它们提供对用户对话历史和他们的隐含和显式偏好的完全访问。

支持 LLM 内存管理的库示例包括[Letta（原名 MemGPT）](https://oreil.ly/1p8Vu)和[Mem0](https://oreil.ly/dgJaZ)。

###### 注意

交换内存的另一种或补充方法是递归地总结对话历史。然而，总结是一个有损过程，可能无法保留文本的语义。在总结过程中可能会丢失有价值的信息，如作者的语气。

# RAG 用于选择上下文中的训练示例

如本章开头所述，RAG 的另一个应用是通过从包含一系列训练示例的数据存储中检索最佳示例来动态选择少量学习的训练示例。对于给定的输入，检索出的少量示例应该最大化 LLM 生成用户查询正确答案的概率。

一种简单的方法是生成输入的嵌入，并检索与输入嵌入最相似的示例。虽然这项技术是一个有希望的起点，但我们能做得更好。

[王等人](https://oreil.ly/r8735)介绍了一种名为 LLM Retriever (LLM-R)的方法，该方法通过使用 LLM 反馈来训练模型，检索出少量示例训练样本，其包含将增加 LLM 生成正确答案的概率。图 12-9 展示了 LLM-R 技术。

![llm-r](img/dllm_1209.png)

###### 图 12-9\. LLM-R 工作流程

对于训练集中的每个输入查询，我们使用类似 BM25 的检索模型检索出 top-k 的少量示例。然后，我们使用 LLM 反馈对这些示例进行重新排序。每个示例都添加到输入之前，并计算真实输出标记的概率。然后，根据它们的对数概率递减顺序对这些示例进行排序。排序后的示例随后用于训练一个奖励模型，该模型被蒸馏以训练最终的检索模型。

# RAG 用于模型训练

到目前为止，我们探索的所有 RAG 应用都是在 LLM 推理期间应用的。我们能否在模型预训练和微调期间也使用 RAG？是的，我们可以！这是一个被低估的研究领域，我预计在未来几年中会有更多的 LLM 利用这一点。让我们详细看看一个例子。

检索增强语言模型（REALM）是 RAG 领域中的开创性工作之一。REALM 将检索和生成任务集成到一个单一模型中。图 12-10 显示了 REALM 的预训练和微调框架。

![realm](img/dllm_1210.png)

###### 图 12-10\. REALM 架构

REALM 架构由两个组件组成：一个知识检索器和一个知识增强编码器，这是一个类似于 BERT 的仅编码器模型。这两个组件都是可微分的，因此一起训练。

知识检索器用于为外部知识库中的所有文档生成嵌入。检索是通过找到与输入具有最大嵌入相似度的文档来进行的。在掩码语言模型预训练阶段，检索器损失函数鼓励它检索有助于预测掩码标记的文本。然后，通过关注输入文本和检索到的文本来预测掩码标记。检索到的文本应该包含使预测掩码标记变得容易的相关上下文。

REALM 也采用这些策略来优化训练：

+   命名实体或日期被掩码，以便模型可以学习使用检索到的上下文来预测它们。

+   并非所有掩码标记的预测都需要外部知识。为了适应这一点，总是向检索到的文档中添加一个空文档。

+   理想的检索文档应包含预测掩码标记所需的上下文，而不是掩码标记本身。因此，包含检索文本中掩码标记的简单检索不包括在内。

# RAG 的局限性

虽然 RAG 是一个强大的范式，它扩展了 LLM 的有用性并减少了幻觉，但它并没有解决 LLM 的所有局限性。使用 RAG 的一些陷阱包括：

+   依赖于文本片段的检索可能导致 LLM 依赖于表面信息来回答查询，而不是对问题空间的深入理解。

+   检索成为管道的瓶颈。如果检索过程未能提取合适的候选文本，LLM 强大的能力都将毫无用处。

+   有时检索过程可能会提取出与 LLM 参数化记忆中包含的知识相矛盾的文档。没有访问到事实真相，LLM 很难解决这些矛盾。

# RAG 与长上下文

如第五章所述，LLM 的一个局限性是它们可用的有效上下文窗口有限。然而，这是最近取得快速进展的一个领域。最多几千个标记的上下文窗口是标准，直到 2023 年初，此后像[Anthropic](https://oreil.ly/ucbD-)这样的公司宣布支持跨越 10 万多个标记的上下文窗口。到 2024 年初，谷歌宣布了[Gemini 1.5 Pro](https://oreil.ly/rp7pi)，支持一百万个标记的上下文。

为了评估上下文大小增加对 LLM 性能的影响，已经设计了几种“大海捞针”测试。其中一种由[Greg Kamradt](https://oreil.ly/M8Jc9)实施，它便于将一个随机事实或陈述（“针”）添加到上下文（“草堆”）的中间，然后向 LLM 提出问题，其中“针”是答案。

然而，明智的做法是以批判的眼光看待这些测试，因为它们通常只评估 LLM 的信息回忆能力。此外，现实世界中的问题很少是“大海捞针”的问题；LLM 可能也不是解决这些问题的正确工具。更便宜、更快的检索模型可以充分执行大多数“大海捞针”检索任务。

在许多“大海捞针”的测试中，随机句子或段落被添加到上下文窗口中作为“针”，而上下文窗口中的其余内容与“针”正交。但这并不反映现实世界的情况，在现实世界中，大多数共现文本以某种方式相关。相关的文本往往充当干扰项，阻止 LLM 得出正确的结论。事实上，这也是在 RAG 管道中开发严格的重排序和细化步骤的原因之一！

长上下文模型可以用于分析非常长的文档，还可以减少重新排序和细化步骤的复杂性。我建议在可行的情况下进行经验计算以权衡利弊。

最后，成本也是长上下文与检索辩论中的一个重要考虑因素。毫无疑问，未来长上下文模型的成本将显著降低，但检索仍然相对便宜。完全放弃检索而选择使用长上下文模型，就像购买一台笔记本电脑并将所有文件存储在 RAM 中而不是磁盘上一样。

# RAG 与微调的比较

使用 RAG 与微调的辩论归结为一个更基本的问题：我可以用 LLM 执行哪些任务方面，而不是依赖外部来源？

在需要外部知识来解决任务的情况下，可以使用检索和微调。检索可用于按需集成知识，缺点是 LLM 仅接触到表面信息，没有机会从数据之间的联系中学习。另一方面，持续的预训练或微调也可以用于集成外部知识，尽管需要昂贵的训练步骤。

[Ovadia 等人](https://oreil.ly/Agodo)比较了 RAG 和微调在需要外部知识任务上的表现。他们表明，对于知识密集型任务，RAG 始终优于微调。正如本章前面所展示的，LLM 需要大量样本来记忆一个概念或事实。因此，通过重复或增强微调数据集可以提高微调的有效性。

即使对于知识密集型任务，RAG 与微调也不必是二选一的决定。如果你正在从事一个专业领域或需要以特定风格或格式输出结果，你可以在领域和任务特定数据上微调你的 LLM，并使用微调后的模型进行下游应用。在大量用例中，RAG 应该是足够的，微调不应成为首选解决方案。

RAG 和微调可以互补。在本章前面，我们看到了如何使用微调来优化 RAG 管道的每一步。同样，我们也看到了如何使用 RAG 来优化微调过程。因此，检索和微调都是你 LLM 工具箱中的强大部分，我希望这些章节已经充分准备了你将它们实施和部署到实际应用中。

# 摘要

在本章中，我们深入探讨了 RAG 管道，详细介绍了*重写-检索-重新排序-细化-插入-生成*管道。我们强调了 RAG 在各种场景下的有效性，包括外部知识的集成、过去对话历史的检索、动态选择少样本学习示例以及工具选择。我们还探讨了 RAG 的局限性以及 RAG 可能无效的场景。

在最后一章中，我们将探讨如何利用我们迄今为止学到的所有概念来设计和打包由 LLM（大型语言模型）驱动的产品，这些产品能为最终用户提供价值。在 LLM 时代，有效的产品设计变得尤为重要，因为一个成功的 LLM 产品能够最大限度地利用 LLM 在擅长领域的功能，同时通过巧妙的产品设计来限制最终用户对 LLM 局限性的接触。我们还将探讨几种 LLM 设计模式，这些模式将我们学到的所有概念整合成可重用、可调试的抽象。
