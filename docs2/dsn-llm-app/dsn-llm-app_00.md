# 前言

在过去几年里，人工智能领域的进步速度之快，主要是由 LLMs 的进步推动的。LLMs 不久前还是一种新兴技术，难以生成连贯的段落；而如今，它们能够解决复杂的数学问题，撰写令人信服的论文，并与人类进行长时间的深入对话。

随着人工智能从强到强的发展，它正迅速融入社会的织锦中，触及我们生活的许多方面。学习如何有效地使用 LLMs 等 AI 模型可能是这个十年中最有用的技能之一。LLMs 正在改变软件世界，使得以前被认为不可能的应用程序的开发成为可能。

尽管 LLMs（大型语言模型）带来了巨大的希望，但现实是它们仍然不是一个成熟的技术，并且存在许多限制，如推理能力不足、缺乏事实性、出现“幻觉”、难以引导它们实现我们的目标、偏见和公平性问题等。尽管存在这些限制，我们仍然可以利用 LLMs 进行有益的应用，并构建各种有用的应用程序，前提是我们能够有效地解决它们的不足。

已经出现了许多软件框架，它们能够快速原型开发 LLM 应用程序。然而，从原型发展到生产级应用程序是一条很少人走的路，仍然是一个非常具有挑战性的任务。这正是这本书的作用所在——提供一个对 LLM 领域的全面概述，为你提供构建复杂 LLM 应用程序的直觉和工具。

通过这本书，我的目标是让你对 LLMs 的工作原理有一个直观的理解，以及你可以用来利用它们的工具，以及它们可以构建的各种应用范式。这本书的独特之处在于练习；书中穿插了 80 多个练习，帮助你巩固直觉，并加深你对底层发生事情的理解。在准备这本书的内容时，我阅读了 800 多篇研究论文，其中许多在书中适当位置引用并链接，为你提供了进一步探索的起点。总的来说，如果你完整地阅读这本书，完成所有练习，并探索推荐的参考文献，我坚信你将成为一个 LLM 专家。

# 这本书面向的对象

这本书面向广泛的读者，包括转向 AI 应用开发的软件工程师、机器学习实践者和科学家，以及产品经理。这本书的大部分内容源于我对 LLMs 的实验，所以即使你是经验丰富的科学家，我也期待你能在其中找到价值。同样，即使你对 AI 世界接触很少，我也期待你仍然能从这本书中理解这项技术的根本。

这本书的唯一先决条件是了解 Python 编码知识和对基本机器学习和深度学习原理的理解。在需要的情况下，我提供了外部资源的链接，您可以使用这些资源来提高或发展您的先决条件。

# 本书结构

本书分为 3 部分，共 13 章。第一部分涉及理解语言模型的成分。我强烈认为，即使您可能永远不会从头开始训练语言模型，了解其构成也是至关重要的。第二部分讨论了各种利用语言模型的方法，无论是直接提示模型，还是以各种方式微调它。它还解决了诸如幻觉和推理限制等问题，以及减轻这些问题的方法。最后，本书的第三部分涉及检索增强生成（RAG）和代理等应用范式，将 LLM 定位于整个软件系统的更广泛背景下。

要查看扩展的目录，请参阅我的 [Substack 博客文章](https://oreil.ly/-2zkH)。

# 本书不涉及的内容

为了保持本书的合理长度，某些主题被认为超出了范围。我已经注意不要涵盖那些我不确定能否经得起时间考验的主题。这个领域发展非常快，因此编写一本能够保持其相关性的书籍极具挑战性。

本书仅关注英语语言的 LLM，大部分情况下省略了对多语言模型的讨论。我也不同意将世界上所有非英语语言都归入“多语言”这一概念。每种语言都有其独特的细微差别，都值得有自己的书籍。

本书也没有涵盖多模态模型。新的模型越来越多地是多模态的，即单个模型支持多种模态，如文本、图像、视频、语音等。然而，文本仍然是最重要的模态，并且是这些模型的结合基础。因此，阅读这本书仍然可以帮助您为多模态的未来做好准备。

本书不专注于理论或深入数学。有很多其他书籍涵盖了这些内容，我在需要的地方慷慨地提供了链接。本书包含最少的数学方程式，而是专注于建立直觉。

本书仅对推理模型进行了基本的介绍，这是最新的 LLM 范式。在本书写作时，推理模型仍处于起步阶段，关于哪些技术将证明是最有效的，人们仍在争论。

# 如何阅读本书

阅读本书的最佳方式是按顺序阅读，同时完成练习并探索参考链接。尽管如此，根据您的兴趣，还有一些替代路径：

+   如果你更感兴趣的是了解 LLM 的格局，而不是一定要用它们来构建应用程序，你可以关注第 1、2、3、4、5、10 和 11 章。

+   如果你是一名产品经理，想要了解 LLM 应用的可能范围，第 1、2、3、5、8、10、11、12 和 13 章是一个不错的选择。

+   如果你是一名机器学习科学家，那么第 7、8、9、10、11 和 12 章一定会给你带来思考的食物和新研究挑战。

+   如果你想要从头开始训练自己的 LLM，第 2、3、4、5 和 7 章将为你提供基础原理。

# 本书使用的约定

本书使用的以下排版约定：

*斜体*

表示新术语、URL、电子邮件地址、文件名和文件扩展名。

`常宽`

用于程序列表，以及段落中引用程序元素，如变量或函数名称、数据库、数据类型、环境变量、语句和关键字。

**`常宽粗体`**

显示用户应直接输入的命令或其他文本。

*`常宽斜体`*

显示应替换为用户提供的值或由上下文确定的值的文本。

###### 提示

此元素表示提示或建议。

###### 注意

此元素表示一般性说明。

###### 警告

此元素表示警告或注意。

# 使用代码示例

补充材料（代码示例、练习等）可在[*https://oreil.ly/llm-playbooks*](https://oreil.ly/llm-playbooks)下载。

如果你在使用代码示例时遇到技术问题或问题，请发送电子邮件至*support@oreilly.com*。

这本书旨在帮助您完成工作。一般来说，如果这本书提供了示例代码，您可以在您的程序和文档中使用它。除非您正在复制代码的很大一部分，否则您不需要联系我们获取许可。例如，编写一个使用这本书中几个代码片段的程序不需要许可。通过引用这本书并引用示例代码来回答问题不需要许可。将这本书的大量示例代码纳入您产品的文档中则需要许可。

我们感激，但通常不需要注明出处。注明出处通常包括标题、作者、出版社和 ISBN。例如：“*《设计大型语言模型应用》* 由苏哈斯·帕伊（O’Reilly）著。版权所有 2025 年苏哈斯·帕伊，978-1-098-15050-1。”

如果您认为您对代码示例的使用超出了合理使用或上述许可的范围，请随时通过*permissions@oreilly.com*联系我们。

# O’Reilly 在线学习

###### 注意

40 多年来，[*O’Reilly Media*](https://oreilly.com)一直提供技术和商业培训、知识和见解，以帮助公司成功。

我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专长。O’Reilly 的在线学习平台为您提供按需访问实时培训课程、深入的学习路径、交互式编码环境以及来自 O’Reilly 和 200 多家其他出版商的大量文本和视频。更多信息，请访问[*https://oreilly.com*](https://oreilly.com)。

# 如何联系我们

请将有关这本书的评论和问题寄给出版社：

+   O’Reilly Media, Inc.

+   1005 Gravenstein Highway North

+   Sebastopol, CA 95472

+   800-889-8969（美国或加拿大）

+   707-827-7019（国际或本地）

+   707-829-0104（传真）

+   *support@oreilly.com*

+   [*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)

我们为这本书有一个网页，上面列出了勘误表、示例和任何其他附加信息。您可以通过[*https://oreil.ly/designing-llm-applications-1e*](https://oreil.ly/designing-llm-applications-1e)访问此页面。

想了解我们书籍和课程的新闻和信息，请访问[*https://oreilly.com*](https://oreilly.com)。

在 LinkedIn 上找到我们：[*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)。

在 YouTube 上关注我们：[*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)。

# 致谢

他们常说，养育一个孩子需要整个村庄的力量；我现在意识到，写一本书则需要一个都市的力量。

首先，我要感谢奥莱利团队在整个书籍的开发和发布过程中与我合作时的细致专业和优雅。难怪他们是世界上最好的技术书籍出版商。我特别想感谢妮可·巴特菲尔德，是她邀请我成为作者，以及世界最好的编辑米歇尔·克罗宁，她频繁的审阅确保了书籍发展出连贯的结构。我会怀念我们定期的检查！感谢艾什利·斯图西、克莉丝滕·布朗以及整个制作团队为将书籍投入生产所付出的辛勤工作。

我非常感谢我的朋友艾米伯·腾格，她帮助我绘制书籍插图并设置书籍的 GitHub 仓库。我也要深深地感谢我的技术审稿人塞伦娜·麦克唐纳尔、杨森·刘、苏珊·舒·张、戈登·吉布森和努尔·法赫米，他们各自花费了数十小时撰写了极其详细和深思熟虑的技术审阅。这本书因此变得更好。

我感谢多伦多 AI 生态系统，特别是聚合智力、TMLS（多伦多机器学习峰会）和 SharpestMinds 社区，为我提供了与社区互动的空间，并确保我始终能够把握行业的脉搏。特别感谢我的朋友们马达夫·辛哈尔、杰伊·阿拉马尔和梅根·里斯达尔（他们帮助我创造了“token etymology”这个短语），我们定期进行关于 LLMs 的富有启发性的对话，并且是这本书的首批读者。我还要向我的开源合作者胡安·阮致敬，我们在多个开源 LLM 项目上合作，对于在 LLM 研究中最大胆的想法上进行的数十次深夜讨论表示感谢。

在同时作为一家 AI 初创公司的联合创始人写作书籍是可能的，这都得益于我的商业和犯罪伙伴，克里斯·贝纳蒂（他同时也说服我删除了书中的“orifice”一词）。我将永远感激整个哈德森实验室团队在整段时间里坚定不移和持续的支持，特别感谢小泉，他稳定的手确保了我有足够的时间专注于书籍。此外，我还要感谢我的朋友们卡夫·沙马内什、阿卜杜拉·阿尔-哈亚利、扎克·阮、萨马尔特·巴辛、萨代格·拉伊西和伊恩·余，他们在整个过程中给予我道德支持，并定期检查我是否得到了足够的睡眠。

最后，我想将这本书献给我的母亲，库苏玛·帕伊，我简单地称她为“传奇”，因为她一生的牺牲，确保了我能够长大成人并有机会写这本书。这本书的任何成功都应该主要归功于我的母亲，因为她塑造了我今天成为的人。
