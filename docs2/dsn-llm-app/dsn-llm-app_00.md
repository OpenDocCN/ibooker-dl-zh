# 前言

在过去几年里，人工智能领域的进步速度之快，主要是由 LLMs 的进步推动的。LLMs 曾经是一种新兴技术，难以生成连贯的段落；而如今，它们能够解决复杂的数学问题，撰写令人信服的论文，并与人类进行长时间的深入对话。

随着人工智能从强到强，它正迅速融入社会的织锦中，触及我们生活的许多方面。学习如何有效地使用 LLMs 等 AI 模型可能是这个十年中最有用的技能之一。LLMs 正在改变软件世界，使得以前被认为不可能的应用开发成为可能。

尽管 LLMs 带来了如此多的希望，但现实是它们仍然不是一个成熟的技术，存在许多限制，如推理不足、缺乏事实性、“幻觉”、难以引导它们实现我们的目标、偏见和公平性问题等。尽管存在这些限制，只要我们有效地解决它们的不足，我们仍然可以利用 LLMs 进行有益的应用，并构建各种有用的应用。

已经出现了许多软件框架，它们能够快速原型开发 LLM 应用。然而，从原型发展到生产级应用的道路却鲜有人走，并且仍然是一项极具挑战性的任务。这正是本书的作用所在——提供一个对 LLM 领域的全面概述，为你提供构建复杂 LLM 应用的直觉和工具。

通过这本书，我的目标是帮助你直观地理解 LLMs 的工作原理，你可用于利用它们的工具，以及它们可以构建的各种应用范式。本书的独特之处在于练习；超过 80 个练习散布全书，帮助你巩固直觉并加深对底层发生事情的理解。在准备本书内容的过程中，我阅读了 800 多篇研究论文，其中许多在书中适当位置引用并链接，为你提供了进一步探索的起点。总的来说，如果你完整地阅读本书，完成所有练习，并探索推荐的参考文献，我坚信你将成为 LLM 领域的专家。

# 本书面向对象

这本书旨在面向广泛的读者群体，包括正在转向人工智能应用开发的软件工程师、机器学习实践者和科学家，以及产品经理。本书的大部分内容源于我对大型语言模型（LLMs）的实验，因此即使你是经验丰富的科学家，我也期待你能在其中找到价值。同样，即使你对人工智能领域接触非常有限，我也期待你仍然能从这本书中理解到这项技术的根本。

本书的唯一先决条件是了解 Python 编程和基本机器学习、深度学习原理。在需要的情况下，我提供了外部资源的链接，你可以使用这些资源来提高或发展你的先决条件。

# 本书的结构安排

本书分为 3 部分，共 13 章。第一部分涉及理解语言模型的成分。我坚信，即使你永远不会从头开始训练语言模型，了解其构成也是至关重要的。第二部分讨论了各种利用语言模型的方式，无论是直接提示模型，还是通过各种方式微调它。它还讨论了诸如幻觉和推理限制等局限性，以及缓解这些问题的方法。最后，本书的第三部分涉及检索增强生成（RAG）和代理等应用范式，将 LLM 定位在整个软件系统的更广泛背景下。

要查看扩展的目录，请参阅我的[Substack 博客文章](https://oreil.ly/-2zkH)。

# 本书不涉及的内容

为了保持书籍的合理长度，某些主题被认为超出了范围。我已确保不涉及那些我不确定能经得起时间考验的主题。这个领域发展非常迅速，因此编写一本能够保持其时效性的书籍极具挑战性。

本书仅关注英语语言的 LLM，大部分内容没有讨论多语言模型。我也不同意将世界上所有非英语语言都归入“多语言”这一大类的观点。每种语言都有其独特的细微差别，都应拥有自己的书籍。

本书也没有涵盖多模态模型。新的模型越来越多地是多模态的，即单个模型支持多种模态，如文本、图像、视频、语音等。然而，文本仍然是最重要的模态，并且是这些模型中的粘合剂。因此，阅读本书仍能帮助你为多模态的未来做好准备。

本书不侧重于理论或深入数学。有很多其他书籍涵盖了这些内容，我在需要的地方慷慨地提供了链接。本书包含最少的数学方程式，而是侧重于建立直觉。

本书仅对推理模型进行了基本的介绍，这是最新的 LLM 范式。在本书写作时，推理模型仍处于起步阶段，关于哪些技术将证明是最有效的，意见尚不统一。

# 如何阅读本书

阅读本书的最佳方式是按顺序阅读，同时完成练习并探索参考链接。尽管如此，根据你的兴趣，还有一些替代路径：

+   如果你对了解 LLM 领域感兴趣，而不一定在于构建应用程序，那么你可以专注于第 1、2、3、4、5、10 和 11 章。

+   如果你是一名寻求了解 LLM 应用可能性范围的产品经理，那么第 1、2、3、5、8、10、11、12 和 13 章是一个不错的选择。

+   如果你是一名机器学习科学家，那么第 7、8、9、10、11、12 章一定会给你带来思考和新研究挑战。

+   如果你想要从头开始训练自己的 LLM，第 2、3、4、5、7 章将为你提供基础原理。

# 本书使用的约定

本书使用以下排版约定：

*斜体*

指示新术语、URL、电子邮件地址、文件名和文件扩展名。

`等宽字体`

用于程序列表，以及段落中引用程序元素，如变量或函数名、数据库、数据类型、环境变量、语句和关键字。

**`等宽粗体`**

显示用户应直接输入的命令或其他文本。

*`等宽斜体`*

显示应替换为用户提供的值或由上下文确定的值的文本。

###### 小贴士

此元素表示提示或建议。

###### 注意

此元素表示一般性说明。

###### 警告

此元素表示警告或注意事项。

# 使用代码示例

补充材料（代码示例、练习等）可在[*https://oreil.ly/llm-playbooks*](https://oreil.ly/llm-playbooks)下载。

如果你对技术问题或使用代码示例时遇到的问题有疑问，请发送电子邮件至*support@oreilly.com*。

这本书旨在帮助你完成工作。一般来说，如果本书提供了示例代码，你可以在你的程序和文档中使用它。除非你正在复制代码的很大一部分，否则你不需要联系我们获取许可。例如，编写一个使用本书中几个代码片段的程序不需要许可。通过引用本书并引用示例代码来回答问题不需要许可。将本书的大量示例代码纳入你产品的文档中则需要许可。

我们感谢，但通常不需要署名。署名通常包括标题、作者、出版社和 ISBN。例如：“*《设计大型语言模型应用》* 由苏哈斯·帕伊（O’Reilly）著。版权所有 2025 苏哈斯·帕伊，978-1-098-15050-1。”

如果你认为你对代码示例的使用超出了合理使用或上述许可的范围，请随时通过 *permissions@oreilly.com* 联系我们。

# O’Reilly 在线学习

###### 注意

40 多年来，[*O’Reilly 媒体*](https://oreilly.com) 为公司提供技术培训和商业知识，帮助他们取得成功。

我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专业知识。O’Reilly 的在线学习平台为您提供按需访问实时培训课程、深入的学习路径、交互式编码环境和来自 O’Reilly 和 200 多家其他出版商的大量文本和视频。更多信息，请访问 [*https://oreilly.com*](https://oreilly.com)。

# 如何联系我们

请将有关本书的评论和问题寄给出版社：

+   O’Reilly 媒体公司

+   1005 Gravenstein Highway North

+   加州塞巴斯蒂波尔 95472

+   800-889-8969 (美国或加拿大)

+   707-827-7019 (国际或本地)

+   707-829-0104 (传真)

+   *support@oreilly.com*

+   [*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)

我们为这本书有一个网页，其中列出了勘误表、示例和任何其他信息。您可以通过 [*https://oreil.ly/designing-llm-applications-1e*](https://oreil.ly/designing-llm-applications-1e) 访问此页面。

想了解我们书籍和课程的新闻和信息，请访问 [*https://oreilly.com*](https://oreilly.com)。

在 LinkedIn 上找到我们：[*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)。

在 YouTube 上观看我们：[*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)。

# 致谢

他们说，养育一个孩子需要整个村庄；我现在意识到，写一本书需要一座大都市。

首先，我要感谢 O’Reilly 团队在整个书籍开发和发布过程中与我合作时的细致专业和精湛技艺。难怪他们是世界上最好的技术书籍出版商。我特别想感谢 Nicole Butterfield，她邀请我成为作者，以及 Michele Cronin，世界上最优秀的编辑，她频繁的审阅确保了书籍形成了连贯的结构。我会怀念我们定期的检查！感谢 Ashley Stussy、Kristen Brown 以及整个制作团队为将书籍投入生产所付出的辛勤工作。

我对朋友 Amber Teng 深表感激，她帮助我绘制书籍插图并设置书籍的 Github 仓库。我还对技术审阅员 Serena McDonnell、Yenson Lau、Susan Shu Chang、Gordon Gibson 和 Nour Fahmy 深表感激，他们各自花费了数十小时撰写了极其详细和周到的技术审阅。这本书因此变得更加出色。

我对多伦多人工智能生态系统表示感谢，特别是 Aggregate Intellect、TMLS（多伦多机器学习峰会）和 SharpestMinds 社区，它们为我提供了与社区互动的空间，并确保我始终能够紧跟行业脉搏。特别感谢我的朋友 Madhav Singhal、Jay Alammar 和 Megan Risdal（他们帮助我创造了“token etymology”这个短语），我们经常就 LLMs 进行富有启发性的对话，并且他们是这本书的第一批读者。我还想向我的开源合作者 Huu Nguyen 表示感谢，我们在多个开源 LLM 项目上合作，进行了数十次关于 LLM 研究中最大胆想法的深夜讨论。

在同时作为一家 AI 初创公司的联合创始人的情况下写书，仅因我的商业和犯罪伙伴 Kris Bennatti（他同时也说服我删除了书中的“orifice”一词）坚定不移的支持才成为可能。我将永远感激整个 Hudson Labs 团队在整个过程中的坚定和持续支持，特别感谢 Xiao Quan，他稳定的手确保了我找到时间专注于书籍。此外，我还想感谢我的朋友 Kaaveh Shoamanesh、Abdullah Al-hayali、Zach Nguyen、Samarth Bhasin、Sadegh Raeisi 和 Ian Yu，他们在整个过程中给予了我道德支持，并定期检查我是否得到了足够的睡眠。

最后，我想将这本书献给我的母亲，Kusuma Pai，我简单地称她为“传奇”，因为她一生的牺牲，确保我能够长大成人并有机会写这本书。这本书的任何成功都应该主要归功于我的母亲，因为她塑造了我今天成为的人。
