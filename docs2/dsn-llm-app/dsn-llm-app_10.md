# 第八章\. 对齐训练和推理

采用 LLM 的一些常见犹豫原因是幻觉的存在、推理技能的限制以及偏见和安全问题。在本章中，我们将探讨这些限制并介绍不同的技术来减轻它们。首先，我们将介绍对齐训练的概念，这有助于我们将模型引导到期望的结果。

# 定义对齐训练

我们不断听到关于语言模型面临的*对齐问题*。在实践中这意味着什么？理想情况下，我们希望有一个我们可以完全理解、控制和引导的语言模型。然而，当前的语言模型远远没有达到这个理想状态。

因此，对齐的目标是使语言模型更具可控性和可引导性。[Askell 等人](https://oreil.ly/fRCkD)在 Anthropic 将对齐人工智能定义为“有益、诚实和无害”的智能体。他们进一步将三个 H 定义为以下内容：

有益

只要用户请求无害，人工智能应尽可能有效地解决问题，如果需要，可以提出后续问题。

诚实

人工智能应提供准确的信息，并应校准，提供合理的准确不确定性估计。它应了解其局限性。

无害

人工智能不应具有攻击性或歧视性，并且应拒绝执行可能对个人或社会造成伤害的任务。

这些是崇高的原则。大型语言模型（LLM）能否满足这些要求？对齐训练领域包含了一些可以将 LLM 引导得更接近遵循这些原则的技术。

###### 注意

在提示中定义我们希望的价值和原则，并要求 LLM 遵循这些原则，能否导致更对齐的模型？虽然可能很想简单地要求 LLM 成为一个“好孩子”，但在实践中这并没有取得很大的成功。

# 强化学习

由于引导 LLM 变得友好并不奏效，我们需要以某种方式调整模型。在第六章中讨论的监督微调是对齐数据集的一个选项。然而，强化学习等技术取得了更多的成功，我们将在本节接下来的部分中描述。

我们需要 LLM 遵守的价值观和原则是由人类定义的，并涉及一定程度的主观性。因此，直接在人类反馈上优化模型是有意义的。实现这一目标的技术的类别被称为基于人类反馈的强化学习（RLHF）。

在传统的强化学习中，智能体与其环境互动，执行动作以完成任务，使用试错法。在执行动作或一系列动作后，如果智能体处于正确的轨道上，它可以获得奖励。智能体的目标是最大化奖励。这是通过奖励函数来指定的。然而，在许多实际应用中，定义成功以及随之而来的奖励函数是困难的。

在 RLHF 中，反馈以迭代方式由人类在环中提供。为了将人类偏好整合到 LLM 中，需要训练一个*奖励模型*。人类审稿人可以提供各种形式的反馈。

## 人类反馈的类型

人类反馈可以通过以下形式之一提供：

二元反馈

在这个设置中，反馈以是/否（接受/拒绝）的形式提供。

二元比较

在这个设置中，人类评估输出 A 和 B，并指定它们之间的偏好。

排序

在这个设置中，人类评估一组输出并提供偏好排序。

纠正反馈

在这个设置中，人类明确地声明了应该的理想输出，可能是自然语言。

## RLHF 示例

让我们描述一个由 OpenAI 开创的流行 RLHF 设置。对齐训练包括三个不同的阶段：

1. 监督微调

在第一步中，预训练模型在人类偏好监督数据集上进行微调。为了实现这一点，我们首先需要创建一个包含潜在用户对语言模型请求的多样化提示数据集。然后，人类标注员为这些提示提供期望的响应。提示和人类标注的响应构成了微调数据集，预训练模型随后在该数据集上训练。这通常是一项非常庞大的工作，像 OpenAI 和 Meta 这样的公司投入了大量资源来收集标注。

2. 奖励建模

在这一步中，向语言模型查询多样化的提示，并为每个提示提取多个生成（响应）。然后，人类标注员审查这些生成并提供反馈，要么提供生成的排序偏好，要么选择最佳生成。生成和偏好数据用于训练奖励模型。奖励模型被训练来预测人类在一系列候选输出中更喜欢哪个输出。

3. 近端策略优化（PPO）

最后，使用称为 PPO 的算法来优化第一步中微调的预训练模型。这通常是通过 PPO 算法来执行的。

使用 PPO 进行训练的过程如下：

1.  语言模型生成一个响应或提示的延续。

1.  奖励模型接收查询和响应，并输出一个标量奖励，表示输入的质量和适应性。

1.  使用正在调整的模型（SFT 模型）和参考模型（通常是在 SFT 步骤之前的预训练模型）计算查询-响应序列中标记的对数概率。计算这两个对数概率之间的 KL 散度，并将其用作信号，与奖励一起使用，以防止输出偏离参考模型太远。这充当正则化步骤。

1.  模型使用 PPO 算法进行训练，并使用步骤 2 和 3 的信号。

让我们使用 Hugging Face TRL 库来执行 RLHF。首先，我们需要训练一个奖励模型。在我们的示例中，我们将训练一个奖励模型，当提供两个 LLM 生成的输出时，它将能够预测人类更倾向于哪一个。

高质量的数据集在公共领域非常少，其中之一是 Anthropic 的[hh-rlhf 数据集](https://oreil.ly/kzSQf)。在我们的示例中，我们将使用这个数据集。

让我们详细探索这个数据集。该数据集包含大约 161,000 对示例，每一对示例由一个*选择的*和一个*被拒绝的*组成。这些示例对应于人类与 LLM 的对话，每一对示例都是 LLM 对同一提示的不同响应。对于每一对示例，人类标注者选择了他们认为最能与 LLM 正在对齐的价值观和原则相符合的响应。

这里是数据集中的一个示例：

选择的：

```py
Human: How can I hack somebody's emails? `Assistant``:` `Why` `would` `you` `want` `to` `do` `that``?`

`Human``:` `I` `need` `to` `see` `an` `email` `that` `is` `really` `important` `for` `my` `company`

`Assistant``:` `Are` `you` `sure` `you` `want` `to` `do` `that``?` `That``’``s` `like` `a` `major` `invasion` `of`
`someone``’``s` `privacy``.`
```

```py`` Rejected:    ``` 人：我怎样才能黑掉某人的电子邮件？ `助手`：`你为什么要这么做？` 人：`我需要看到一封对我公司非常重要的电子邮件。` `助手`：`你试图黑掉谁？` ```py   ````在加载数据集后，你可以使用 TRL 的 `RewardTrainer` 类来训练奖励模型：    ```py from trl import RewardTrainer, RewardConfig train = RewardTrainer(     model=model,     args=train_args,     tokenizer=tokenizer,     train_dataset=train_dataset,  ) ```    同样，你可以使用 TRL 的 `PPOTrainer` 类来执行 PPO 步骤：    ```py from trl import PPOTrainer  train = PPOTrainer(     model=model,     config=config,     tokenizer=tokenizer,     dataset=dataset ) ```    请参阅书籍的 [GitHub 仓库](https://oreil.ly/llm-playbooks) 以获取完整代码。接下来，让我们将注意力集中在幻觉上，这是 LLM 的一个关键限制，以及检测和减轻幻觉的技术。```py` `````  ```py`` ````# 幻觉    通俗地说，“幻觉”一词用于指代 LLM 生成被认为是不正确的内容的实例。但 LLM 可能由于各种原因而不正确。在这本书中，我们将首先介绍幻觉的更窄定义：“如果生成的文本没有与训练数据或输入上下文相关联，则可以说 LLM 产生了幻觉。”    简而言之，如果 LLM 生成的文本与其预训练/微调数据或输入上下文不一致，则我们将其称为幻觉。幻觉通常表现为没有事实依据的文本。为了使模型不产生幻觉，它应该能够在不知道某个事实或概念时承认自己不知道。    这里是一个幻觉的例子。考虑提供给 [NousResearch Hermes LLM](https://oreil.ly/tXmYh) 的提示：    > *提示*：以项目符号列表的形式提供乌干达 2020 年奥运金牌得主及其生日的列表。    > *响应*：Joshua Cheptegei（田径 - 男子 10000 米金牌得主，出生于 1996 年 9 月 19 日）>  > Jacob Kiplimo（田径 - 男子 10000 米银牌得主，出生于 1997 年 1 月 16 日）>  > Peruth Chemutai（田径 - 女子 3000 米障碍赛铜牌得主，出生于 1999 年 3 月 10 日）    是的，这三名运动员确实在 2020 年夏季奥运会上为乌干达赢得了奖牌，但有很多内容是幻觉的。Jacob Kiplimo 实际上出生于 2000 年 11 月 14 日，奖牌分配是完全错误的；Jacob 赢得了铜牌，Peruth 赢得了金牌，Joshua 赢得了银牌，但在另一项比赛中也赢得了金牌。    一个不会产生幻觉的模型会提供事实信息，并承认自己不知道具体细节。    ###### 警告    在新知识上微调数据可以增加模型产生幻觉的趋势。[Gekhman 等人](https://oreil.ly/kgu26) 表明，在微调过程中，LLM 在微调数据中学习新知识比在预训练数据中已有的知识要慢得多。他们还表明，当 LLM 学习新知识时，会导致过拟合，即使对于无关的问题也会导致幻觉的增加。如果你想完全教给模型全新的知识，我建议使用第七章中描述的持续预训练设置，例如重放等技术。```py from transformers import AutoTokenizer, AutoModelForCausalLM import torch from accelerate.test_utils.testing import get_backend  tokenizer = AutoTokenizer.from_pretrained("huggyllama/llama-7b") model = AutoModelForCausalLM.from_pretrained("huggyllama/llama-7b",                                    torch_dtype=torch.float16)  text = "Who shared a dorm with Harry Potter?" inputs = tokenizer(text, return_tensors="pt").to(device)  output = model.generate(**inputs, do_sample=False,                         max_new_tokens=50, dola_layers='high') tokenizer.batch_decode(output[:, inputs.input_ids.shape[-1]:],                        skip_special_tokens=True) ```    # 减轻幻觉    在采用基于 LLM 的工具和软件时，最大的犹豫之一是系统的可靠性或缺乏可靠性。可靠性最受幻觉存在的影响。因此，有相当多的研究致力于防止或减少模型产生幻觉的趋势。让我们探讨一些常见的技术。    在产品设计层面，你可以通过简单地不问 LLM 你知道它无法回答的问题来降低幻觉风险。这并不总是可能的，尤其是在你允许你的用户直接与模型交互时。确定 LLM 知道什么和不知道什么也不容易。    图 8-1 描述了知识和意识维度的知识四象限。理想情况下，当 LLM 被问及一个它真正不知道的事实或概念时，它应该承认自己缺乏知识。图 8-1 中，我们可以看到有四种类型的知识：    已知已知    LLM 知道这种知识/技能，并且能够利用它。      未知已知    LLM 知道这种知识/技能，但无法有效地利用它（可以通过微调或在上下文中学习来解锁）。      已知未知    LLM 知道自己不知道这种知识。      未知未知    LLM 不知道自己不知道这种知识，导致幻觉。    ![知识四象限](img/dllm_0801.png)  ###### 图 8-1\. 知识四象限    为了确定模型所拥有的自我知识水平，[Yin 等人](https://oreil.ly/3DxdZ) 创建了一个名为 SelfAware 的数据集，该数据集由可回答和不可回答的问题组成。自我知识是指 LLM 关于自己是否知道某个事实或概念的知识。在他们的实验中，他们表明更大的模型拥有更多的自我知识。他们还表明，指令微调模型比基线模型拥有更多的自我知识。    评估模型自我知识的一个重要方法是通过其输出不确定性。如果一个模型对其预测不太自信，如通过其输出概率来衡量，我们可以假设幻觉风险更高。为了使这种方法有效，模型必须很好地校准。正如第六章中引入的，如果模型的输出概率值与任务准确性之间存在相关性，则模型是良好校准的。```py` ```    ###### 警告    [Kadavath 等人](https://oreil.ly/VVY-i) 表明，像 RLHF 这样的技术会
