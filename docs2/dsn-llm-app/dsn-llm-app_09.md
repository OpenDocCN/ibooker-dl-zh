# 第七章. 高级微调技术

在上一章中，我们介绍了微调典型 LLM 的规范方法。在现实世界中，更新 LLM 有多种动机，同样也有多种更新方法。在本章中，我们将描述几种高级微调技术，并强调每种技术适用的场景。

你为什么想更新 LLM 的参数？我们已经在之前的章节中提到了这一点，但现在让我们更详细地讨论一下：

领域适应

我们处理的数据属于一个 LLM 在预训练期间可能没有熟悉的特定领域。在这种情况下，我们希望通过在特定领域数据上训练来更新模型。

任务适应

我们关注 LLM 在特定下游任务上的性能。为了提高 LLM 在这些任务上的性能，我们可以在特定任务数据上对其进行训练。这可以是监督的或无监督的。

知识更新

我们希望通过不断用新数据训练 LLM 来保持其知识的更新。

可控性/可引导性

我们希望控制 LLM 的行为，包括使其更有可能遵循用自然语言编写的用户请求，拒绝某些类型的请求等。实现这一目标的技巧统称为对齐训练。我们将对对齐训练的讨论推迟到第八章。

在本章中，我们将学习可用于更新 LLM 以实现上述原因的技术。为此，本章分为三个部分：

持续预训练

主要用于领域适应和保持 LLM 的知识更新（后者也称为终身学习）。

参数高效微调（PEFT）

一组微调技术，通过仅更新少量模型参数来使微调过程更高效，从而需要更少的内存和计算。

模型合并/模型融合

LLM 的一个令人兴奋的新子领域，探索结合两个或更多模型的参数。我将其称为 NLP 的“黑暗艺术”，因为它理解得不好，但如果做得正确，效果惊人。

让我们从我最喜欢的开始：持续预训练！

# 持续预训练

持续预训练的前提很简单。取一个预训练模型检查点，并继续用你自己的数据对其进行预训练。但你为什么要这样做呢？以下是一些持续预训练可以帮助的场景。

+   你在一个专业领域工作，如法律、金融或生物医学。在这些案例中，属于这些领域的文本在语言和结构上都与自然发生的英语文本不同。例如，法律文本以正式的语调写成的长句为特征，包含法律领域的特定术语。金融文本中穿插着许多数字。法律和金融文本都包含相当比例的模板文本。生物医学文本包含许多不属于标准英语词汇的科学术语。在这些所有情况下，你希望在你的 LLM 上预训练特定领域的数据，以便 LLM 接触到特定领域文本的细微差别和特征。这被称为*领域自适应预训练（DAPT）*。

+   将 DAPT 进一步发展，你还可以继续在特定于你感兴趣领域的通用文本以及与你的下游任务具体相关的领域文本上预训练你的模型。这被称为*任务自适应预训练（TAPT）*。

+   你的大型语言模型是一个知识的宝库。但这个知识可能会随着时间的推移而变得过时。为了保持其知识的时效性，你会在定期的时间间隔或新数据可用时继续预训练模型。这被称为*终身学习*。

###### 注意

你可能正在想，“如果我想得到一个特定领域的 LLM，为什么我不直接用我的特定领域数据从头开始训练一个 LLM？”好吧，你可以，但你的 LLM 的表现力可能不会那么好，而且这项练习的成本将远远高于持续预训练。LLM 学习了许多广泛的言语能力，这些能力可能无法仅从特定领域文本中学习到。因此，最好使用已经在通用文本上预训练的 LLM，然后使用特定领域文本继续预训练它。

在实践中，持续预训练是一项具有挑战性的练习。这是由于灾难性遗忘现象，当 LLM 继续在新和不同的数据上训练时，它会忘记之前学到的能力和知识。我们将很快探讨各种技术来对抗灾难性遗忘问题。

持续预训练与微调有何不同？这些差异主要在于外观和术语。就像预训练一样，持续预训练是自监督的，而当我们使用监督数据集时，我们通常使用微调这个术语。持续预训练使用与原始预训练设置中使用的相同（但不一定是）学习目标。最后，持续预训练的数据集通常比典型的微调数据集大得多。

图 7-1 展示了通用的持续预训练过程。

![continual-pt](img/dllm_0701.png)

###### 图 7-1\. 持续预训练过程的示意图

本书[GitHub 仓库](https://oreil.ly/llm-playbooks)包含了一个关于持续预训练的教程。这种设置与微调没有不同，只是数据集没有标记（自监督训练），并且数据集的大小比典型的微调数据集大得多。

如前所述，简单的持续预训练会导致先前学习的能力和知识发生灾难性遗忘。存在几种技术可以缓解这个问题：

重放（记忆）

使用原始预训练的训练示例，并将它们与新的训练数据混合。

知识蒸馏

在训练期间，取模型的较旧检查点，并比较较旧和当前表示之间的 KL 散度，并对它进行惩罚。

正则化

在持续训练期间惩罚参数的大幅变化。

参数扩展

在持续预训练过程中，向模型添加更多参数。这可以通过增加模型的宽度或深度来实现。

想要了解更多持续学习技术，请查看[Jin 等人](https://oreil.ly/yNa-H)的论文。在本章中，我们将更深入地探讨重放和参数扩展方法。

## 重放（记忆）

基于重放的技术是缓解灾难性遗忘的最简单技术之一。在这种方法中，我们存储来自原始数据集的预训练示例，并将它们与持续训练数据集交织在一起。因此，数据漂移并不那么明显。

以下公式对我非常有效：从原始预训练数据集的不同子集中采样，并将它们与持续训练数据集混合。在训练开始时，让新数据的比例大约为 25%。在训练步骤中，这个比例可以逐渐增加到最大比例，例如 80%。

如果原始预训练数据集是一个整体，而不是由几个较小的数据集组成，你可能需要自己识别领域，以确保原始预训练集中的所有领域都包括在内。

## 参数扩展

与重放方法相比，另一种方法是使用参数扩展技术。简单的方法是在模型顶部添加一个或两个新层，并在持续预训练期间仅训练这些参数。你还可以在现有层中插入并训练特定领域的参数模块（称为适配器）。我们将在“参数高效微调”（#parameter-efficient-fine-tuning）中讨论基于适配器的方法。

如前所述，持续预训练也可以用来促进终身学习，模型持续更新以包含新的事实和知识。然而，目前这可能不是新知识学习最有效的范式。你可能更适合使用 RAG。我们将在第十二章（ch12.html#ch12）中更详细地探讨 RAG。

###### 小贴士

[任务自适应预训练（TAPT）](https://oreil.ly/H38wF)是领域自适应预训练的有用补充。TAPT 涉及在更小但更特定于任务的未标记数据集上对 LLM 进行持续预训练。为了防止灾难性遗忘，你应该在 TAPT 之前先进行 DAPT，然后随后在下游任务上执行任何监督微调。TAPT 的未标记数据可以选择与 DAPT 类似的方法：通过构建数据的嵌入并选择与黄金真值句子聚类的数据。

总结来说，在拥有大量特定领域文本且该领域由专门的语料库或词汇明确表征的情况下，持续预训练可以非常有效。持续预训练还可以用于帮助 LLM 适应新语言。

###### 提示

领域特定的文本可能包含该领域特有的术语。对我来说，一种有效的方法是添加额外的标记来表示领域特定的术语。

持续预训练可能需要大量的计算资源。在较小的数据集上进行微调所需的资源要少得多。然而，在大语言模型时代，我们必须尽一切努力减少计算和内存需求。因此，接下来让我们讨论一些参数高效的微调技术，这些技术使得在资源受限的环境中微调过程更加可行。

# 参数高效微调

在 PEFT 中，我们不是更新模型的全部参数，而是仅更新一小部分参数。这可以大幅降低计算和存储需求。

我们可以将当前的 PEFT 技术分为三种类型：

添加新参数

这涉及到向 LLM 添加一些额外的参数，并仅对这些参数进行训练。

选择参数子集

这涉及到选择仅更新 LLM 的一小部分参数，无论是通过事先选择子集还是通过学习适当的子集。

低秩方法

这涉及到使用方法来减少训练参数的数量，通过找到一个包含几乎与较大矩阵相同信息的较小矩阵。

让我们现在详细讨论这些方法。

## 添加新参数

可能你的工作需要你对大量任务进行模型微调。或者，你可能需要通过为每个用户微调模型来驱动个性化。维护和部署如此多的微调模型副本将会很麻烦。

避免更新模型所有参数的一种方法是在模型中添加一些额外的参数，并仅训练这些参数。而不是存储和部署每个微调模型的完整副本，你只需存储新添加的参数。

添加新参数进行微调的常见方法包括：

瓶颈适配器

这些是添加到 Transformer 层的轻量级模块。

前缀微调

这些是训练并附加到输入的特定任务向量。

提示调整（软提示）

这与前缀调整类似，但采用了简化的训练方法。

让我们详细讨论这些技术中的每一个。

### 瓶颈适配器

适配器是附加到 LLM 架构的参数模块。适配器可以以多种方式集成到 LLM 架构中，但在 Transformer 中，常见的方式是在每个 Transformer 层插入它们。为了减少参数数量，适配器模块的宽度应该远小于底层 Transformer 模型的宽度。这构成了一个 *下投影*，也称为瓶颈。

因此，瓶颈适配器子层由一个下投影矩阵、一个末尾的上投影矩阵以将数据投影回原始维度，以及中间可以以多种方式配置的参数组成。在微调过程中，仅更新适配器模块。原始预训练模型不会被更新。适配器使用接近单位矩阵的初始化来确保训练平稳。

图 7-2 展示了瓶颈适配器通常在 Transformer 架构中插入的位置。请注意，这只是一个可能的配置。

![适配器](img/dllm_0702.png)

###### 图 7-2\. Transformer 中的适配器模块

这些在实际中是如何工作的呢？[*适配器*库](https://oreil.ly/z05rI)非常有用，可以方便地使用这些高级技术微调大型语言模型。

这就是如何开始使用适配器库中的瓶颈适配器：

```py
from adapters import DoubleSeqBnConfig
adapter_config = DoubleSeqBnConfig()
model.add_adapter("bottleneck_adapter", config=adapter_config)
```

`DoubleSeqBnConfig` 指的是库原生支持的配置，对应于 图 7-2 中所示的适配器架构。但如我之前提到的，你可以根据需要更改适配器的大小和形状。为此，我们需要使用 `BnConfig`：

```py
from adapters import BnConfig
adapter_config = BnConfig(mh_adapter=True, output_adapter=True,

reduction_factor=32, non_linearity="gelu")
```

这里是这些参数代表的意义：

`mh_adapter`

指的是添加到 Transformer 多头注意力子层之后的适配器模块。

`output_adapter`

指的是添加到 Transformer 前馈网络子层之后的适配器模块。

`reduction_factor`

指的是下投影因子：适配器宽度相对于 Transformer 层宽应该缩小多少？

`non_linearity`

指的是所使用的激活函数，例如 RELU 或 GELU。

请参阅适配器库的[文档](https://oreil.ly/n1Pga)以获取更多配置选项。有如此多的配置选项可供选择！

虽然使用瓶颈适配器会导致微调时间和复杂度大幅降低，但将参数添加到 Transformer 的所有层会增加推理延迟。通常，使用常用适配器配置的推理时间预计会增加 6%–8%。

###### 小贴士

在推理过程中，通过删除一些适配器层，可以降低推理延迟。[Rücklé等人提出了 AdapterDrop](https://oreil.ly/GM_1X)，这是一组在训练和推理过程中删除适配器模块的方法。他们建议在推理期间从 Transformer 的前几层删除适配器，或者从每个最少激活的层修剪适配器。

### Prefix-tuning

使用基于适配器的微调技术的缺点是，在推理过程中，每个批次只能支持单个适配器实例，即针对特定任务微调的适配器。相比之下，前缀微调（Prefix-tuning）允许在同一批次中运行多个任务。

在 Prefix-tuning 中，我们在输入的前缀中添加并训练特定于任务的向量。这大大减少了我们需要微调的参数数量。回想一下，提示包含指令、输入以及可选的一些少样本示例。LLM 生成的文本是根据到目前为止生成的输出和提示条件生成的。为此，我们添加了额外的上下文，LLM 可以通过这些前缀向量来关注。添加到输入前缀的新标记被称为**虚拟标记**或**软提示**。

图 7-3 展示了 Prefix-tuning 在 Transformer 中的发生过程。

![prefix-tuning](img/dllm_0703.png)

###### 图 7-3\. Prefix-tuning

如图所示，在每个层中添加了前缀参数。

相比于通常占模型参数 2%或更多的适配器，Prefix-tuning 在参数效率上要高得多，仅占用模型参数的 0.1%或更少。然而，与适配器相比，Prefix-tuning 更难有效地训练。Prefix-tuning 还通过容纳虚拟标记来减少模型的序列长度。

与适配器类似，初始化对于 Prefix-tuning 非常重要。虚拟标记可以通过选择与模型正在微调的任务相关的单词来初始化。

使用适配器库，我们可以实现前缀微调：

```py
from adapters import PrefixTuningConfig
adapter_config = PrefixTuningConfig()
model.add_adapter("prefix_tuning", config=adapter_config)
```

### Prompt tuning

Prompt 微调是前缀微调的简化版本。与前缀微调不同，在每个层中都没有前缀参数。

图 7-4 展示了 Prompt-tuning 在 Transformer 中的发生过程。

![prompt-tuning](img/dllm_0704.png)

###### 图 7-4\. Prompt-tuning

适配器库为提示微调提供了内置的配置：

```py
from adapters import PromptTuningConfig
adapter_config = PromptTuningConfig()
model.add_adapter("prompt_tuning", config=adapter_config)
```

与提示微调相关的某些配置参数包括：

`prompt_length`

提示标记的长度；10-30 是一个不错的起点。

`prompt_init`

初始化这些标记的方法。它们可以通过字符串的嵌入或随机均匀初始化来初始化。

`prompt_init_text`

如果软提示是通过字符串初始化的，则用于初始化它的文本。这可以是当前任务的描述符。

[Lester 等人](https://oreil.ly/BPpRu)，他们引入了提示微调，还利用它来执行软提示集成。对于软提示集成，你为每个任务训练几个软提示。然后，对于给定的输入，你分别将它们用作前缀并生成输出。然后，你可以使用多数投票来从生成的输出中选择正确的输出。

到目前为止，我们已经看到了添加新参数到模型中进行微调的技术。然而，我们可以通过仅微调模型的一小部分参数来实现 PEFT，而不必添加新参数。让我们接下来探索这些方法。

## 子集方法

选择参数子集进行微调的一种天真方法是仅微调 Transformer 的上层，而将其他所有内容冻结。Transformer 的下层已知在语言的基础方面具有专长，如句法，这是我们希望 LLM 保留的。

另一种方法是仅微调 Transformer 的偏置项（在第二章中讨论）。这是由[Zaken 等人](https://oreil.ly/SaWoe)提出的，他们表明，通过仅对偏置项进行微调，你可以获得几乎与完全微调模型相同水平的性能。作者观察到，当你的训练数据有限时，这项技术主要有效。

最终，正如我们在这里所看到的，选择这些微调方法中的每一个都涉及权衡。机器学习社区正在努力开发这一领域的最佳实践。与此同时，实验是关键！

接下来，让我们看看另一种更新 LLM 参数的方法：通过将其与另一个 LLM 的参数合并。

# 结合多个模型

如果你可以访问多个 LLM，它们在能力上相互重叠，但各自具有某些独特的特征，你希望在下游任务中以某种方式利用所有模型的特性。这可以通过多种方式实现，包括模型集成、模型融合或合并。LLM 的这个领域还处于起步阶段，还有更多的工作要做，以充分利用其全部优势。我将其称为 NLP 的黑暗艺术，因为这些技术的理论基础仍然理解不深。然而，我确实相信，即使有这些警告，它也值得包含在这本书中，因为其实际效益已经显而易见。让我们探索其中的一些方法。

## 模型集成

不同的 LLM 可能拥有不同但互补的能力，这是由于它们训练规程、训练超参数等的差异所导致的副作用。这在开源 LLM 中尤其如此，我们拥有大量的模型，其中大多数在大量重叠的数据集上训练，在基准评估指标上非常接近。因此，通过允许利用多个模型的互补能力来生成更好的输出，集成方法可能会带来好处。

在第五章中，我们讨论了对于生成任务，生成相同输入的多个输出并使用启发式方法选择最佳输出是有用的。我们可以将这一原则扩展到多个模型。每个输入都通过*n*个模型。可选地，一个初始步骤可以选择具有最高质量或相关性的前 k 个模型。这些模型的输出可以组合并通过一个模型（可以是 LLM）生成最终输出。

[江等人](https://oreil.ly/Sipzu)提出了一种名为 LLM-Blender 的框架，用于实现 LLM 集成。该框架由两个组件组成：

+   PairRanker 对两个模型的输出进行评分，从而选择一个胜者。

+   GenFuser 接受来自 k 个不同模型的输出以生成最终输出。

图 7-5 显示了 LLM-Blender 框架的工作原理。

![LLMBlender](img/dllm_0705.png)

###### 图 7-5\. LLM-Blender

让我们更深入地了解这些模块中的每一个。

### PairRanker

假设你能够访问*n*个不同的模型。对于给定的输入，你将输入提供给这些模型中的每一个以生成输出。现在，对于每一对输出，你可以将它们与输入结合并输入到 PairRanker 模块中。PairRanker 模块被训练用来为每个输出提供分数。如果你将所有输出对都输入到 PairRanker 模块中，你将找到得分最高的输出（模型）。这个输出可以被视为最终输出。

然而，这仅仅选择了最佳输出，并不一定结合了不同模型的能力。为此，LLM-Blender 框架包含一个名为 GenFuser 的模块。

### GenFuser

对于 GenFuser，我们从 PairRanker 的评分中获取前 k 个结果。然后我们将它们一起输入到 GenFuser 中，生成最终输出。在实践中，GenFuser 只是一个经过微调的 LLM，它被调整以接受多个候选输入并生成一个结合了不同候选者特性的输出。

让我们看看这在实践中是如何工作的。我们可以使用[LLM-Blender 库](https://oreil.ly/F2IcX)：

```py
import llm_blender
from llm_blender.blender.blender_utils import get_topk_candidates_from_ranks

ensemble = llm_blender.Blender()
ensemble.loadranker("llm-blender/PairRM")
ensemnle.loadfuser("llm-blender/gen_fuser_3b")

rank_list = blender.rank(input, candidate_outputs)
top_k = get_topk_candidates_from_ranks(rank_list, candidate_outputs, top_k=4)
final_output = ensemble.fuse(input, top_k)
```

给定一个输入和来自*n*个不同语言模型的`candidate_outputs`列表，我们使用 PairRanker 对输出进行排名，然后选择排名前 k 的输出并将它们融合以生成最终输出。

虽然集成方法可能有效，但最近对模型融合技术产生了很大兴趣。

## 模型融合

在这种方法中，我们以某种方式合并多个模型的参数。想法是，通过合并多个模型的参数，我们可能能够从每个单独模型所拥有的所有互补能力中受益，在一个模型中。

模型融合中常用的某些方法是：

平均

将多个模型合并的最简单方式是对它们的参数进行平均。简单的平均法已被证明非常有效。

加权平均

在平均过程中，某些模型或模型中的某些层可以被加权更多。

插值

每个模型都可以通过一个因子 w1, w2,…wn 加权，其中：

```py
w1 + w2 + w3 +...wn = 1
w1p1 + w2p2 + w3p3 +...wnpn
```

其中 p1, p2, p3…pn 是模型 m1, m2, m3…mn 的参数。

合并多个模型的一个好处是模型重用。比如说，在你的组织中有一个基础 LLM。它被组织中的所有人使用，人们将模型取走并在自己的任务上进行微调。然后他们上传微调后的模型。然后你可以合并所有模型的权重，从而得到一个更强的预训练模型。这个模型可以作为一个新版本的基础模型使用。这个过程被[Don-Yehiya 等人](https://oreil.ly/LTcdf)命名为协作下降（ColD）融合。

我们为什么要这样做呢？想法是，如果我们想在数据集上微调一个 LLM，那么有一个好的起点将使训练更优化。假设是，如果我们已经在另一个任务上微调了 LLM，那么微调后的 LLM 比基础 LLM 是一个更好的起点。这被称为交叉训练。这也是一个相对较新的概念，所以请谨慎行事。

你不需要合并模型的全部参数，只需合并其中一小部分即可。事实上，我们只需合并适配器模块。

## 适配器合并

在本章的早期部分，我们学习了适配器，它们可以用于多种目的，包括领域自适应预训练。虽然你可以为不同的领域训练不同的适配器，但问题仍然在于如何在推理时处理看到的新领域。一个解决方案是对与最接近的领域相关的适配器进行平均，并使用它来处理新领域。这已被[Chronopoulou 等人提出的 AdapterSoup 框架](https://oreil.ly/mKoZ1)证明是有效的。

另一种合并适配器参数的方式是在第四章中介绍的 MoE 框架的上下文中。回想一下，在混合专家模型中，路由函数决定了哪个专家（们）将处理输入。[Wang 等人提出的 AdaMix 框架](https://oreil.ly/pc7Js)将此扩展到适配器模块。我们不是只学习每个层的单个适配器模块，而是学习多个专家模块。在推理期间，所有适配层都被合并。

模型合并是 LLMs（大型语言模型）的一个迷人的子领域。即使你不在你的应用中使用它，我也强烈推荐你尝试一下，因为它同时也是一个真正 neat（整洁）的工具，用来理解 LLMs 的工作原理。

# 摘要

在本章中，我们学习了大量的高级微调技术，包括经验回放和参数扩展等持续预训练策略；瓶颈适配器、前缀调整、提示调整和子集选择等参数高效的微调技术；以及各种类型的模型合并和集成。我们还学习了更新模型权重的原因以及不同方法在不同情况下的适用性。

如前一章和本章所讨论的，微调并不是万能的，它不能学习新的能力或必然消化新的知识。在下一章中，我们将讨论 LLMs 的局限性，如较差的可控性、幻觉和推理问题，以及缓解这些问题的技术。
