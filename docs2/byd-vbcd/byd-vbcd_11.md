# 第八章：安全、可维护性和可靠性

本章面对 vibe 编程和 AI 辅助工程的一个关键方面——确保使用 AI 辅助生成的代码是安全的、可靠的和可维护的。如果生成的软件漏洞百出或容易崩溃，那么速度和生产力就微不足道了。

首先，我将检查在 AI 生成的代码中出现的常见安全陷阱，从注入漏洞到机密泄露。你将学习审计和审查 AI 编写的代码以发现此类问题的技术，有效地作为你的 AI 代码伙伴的安全保障。

接下来，我将讨论围绕 AI 生成的代码构建有效的测试和 QA 框架，以早期捕捉到错误和可靠性问题。性能考虑也将被涵盖。AI 可能会编写正确的代码，但并不总是最有效的代码，因此我将概述如何识别和优化性能瓶颈。我还会探讨确保可维护性的策略，例如强制执行一致的样式或重构 AI 代码，因为 AI 建议有时可能不一致或过于冗长。

我将向你展示如何调整你的代码审查实践以适应 AI 辅助工作流程，强调在审查部分或全部由机器生成的代码时，人类审查员应该关注什么。最后，我将总结部署 AI 辅助项目的最佳实践，从持续集成管道到生产中的监控。到本章结束时，你将拥有一套方法，以确保你的 AI 加速开发既安全又稳健。

# AI 生成的代码中的常见安全漏洞

尽管 AI 编码助手功能强大，但如果使用不当，可能会无意中引入安全问题。它们从大量的公共代码中学习——包括好的和不好的实践——如果提示或上下文没有引导它们远离，它们可能会重复不安全的模式。了解这些常见陷阱对于你来说至关重要，这样你就可以发现并修复它们。这可能包括使用手动和自动手段来检测潜在的安全问题（参见图 8-1）。

![](img/bevc_0801.png)

###### 图 8-1\. AI 引入的安全漏洞：AI 生成的代码可能包含微妙的漏洞，需要仔细审查和自动化的安全扫描来识别和修复。

在 AI 生成的代码中观察到的某些典型安全问题包括：

固定编码的秘密或凭证

有时 AI 会在代码中输出 API 密钥、密码或令牌，尤其是在其训练数据中有类似示例的情况下。例如，如果你要求它与 AWS 集成，它可能会直接在代码中放置一个虚拟的 AWS 密钥。如果留下它，这是危险的——如果代码被共享，可能会泄露敏感信息。始终确保通过环境变量或配置文件正确管理密钥。如果 AI 建议像`api_key = "ABC123SECRET"`这样的内容，将其视为一个警告——真正的密钥不应该出现在源代码中。

SQL 注入漏洞

如果你让 AI 模型生成 SQL 查询或 ORM 使用，请检查它不是通过直接连接用户输入来构建查询的。例如，一个不安全的模式可能是：

```py
sql = "SELECT * FROM users WHERE name = '" + username + "'";
```

这容易受到注入攻击。如果你没有明确告诉它参数化查询，AI 可能会生成这样的内容。始终使用预编译语句或参数绑定。许多 AI 助手会这样做，如果它们记得最佳实践（如使用`?`或占位符在 SQL 中为用户输入），但这并不保证。这取决于你验证并要求 AI 在需要时进行修复：

```py
Modify this query to use parameters to prevent SQL injection.
```

网络应用中的跨站脚本（XSS）

在生成网络代码时，AI 工具并不总是自动在输出中转义用户输入。例如，你的 AI 可能会生成一个模板片段，直接将`{{comment.text}}`插入 HTML 而不进行转义，这可能会允许恶意脚本在注释中运行。如果使用框架，AI 通常默认转义，但如果它们处理原始 HTML 构建，请小心。实现输出编码或清理例程。你可以提示 AI：

```py
Add sanitization for user inputs to prevent XSS.
```

许多现代框架都有内置机制，因此请确保 AI 使用它们，例如在[文档对象模型 (DOM) 操作](https://oreil.ly/5o_2x)中使用`innerText`而不是`innerHTML`。

不正确的身份验证和授权

AI 可以编写身份验证流程，但细微的错误可能会悄悄出现：例如，在没有足够强大的密钥的情况下生成[JSON Web Token (JWT)](https://oreil.ly/rf7JL)或没有正确检查密码散列。

对于授权来说，也是如此：AI 可能不会自动强制执行操作（如删除资源）仅限于拥有该资源的用户。这些逻辑问题很难自动捕捉到——需要仔细思考安全模型。在编写此类代码时，请明确指定：

```py
Ensure that only the owner of the resource can delete it. Add checks for user ID.
```

然后测试这些条件。AI 很容易省略检查，因为它如果不被告知，并不真正“理解”上下文。

不安全默认或配置

AI 可能会选择便利性而不是安全性，除非被提示做其他事情。例如：

+   使用 HTTP 而不是 HTTPS 进行 API 调用（如果未指定 TLS）

+   不验证 SSL 证书（一些互联网上的代码示例在请求中使用`verify=false`，AI 可能会复制）

+   无限制地启用对所有来源和方法的 CORS（这可能会打开应用对任何跨源请求的访问）

+   选择过时的加密（如用于散列的 MD5 或 SHA1，而不是用于密码的 SHA-256/Bcrypt/Argon2）

这些问题通常很微妙，这也是为什么审计您的配置文件和初始化代码是个好主意的原因之一。如果 AI 设置了类似`app.UseCors(allowAll)`或选择了旧加密算法，您应该注意到并纠正它。

揭露敏感信息的错误处理

AI 生成的错误处理可能会打印或返回堆栈跟踪。例如，一个 Node.js API 可能会捕获错误并执行`res.send(err.toString())`，这可能会导致泄露内部细节。确保向用户传达的错误消息经过清理，并且日志得到适当处理。根据需要调整，以避免向攻击者提供如完整错误消息或文件路径之类的线索。

依赖项管理和更新

如果 AI 向您的项目添加依赖项（如库），请确保它们是最新的且来自可信赖的来源。AI 可能会选择在其训练数据中流行的库，但这些库可能不再维护或存在已知漏洞。例如，如果它建议使用较旧版本的包，您应该将其升级到最新稳定版。生成后运行`npm audit`或等效工具也是明智之举。或者询问 AI：

> 这个库是否仍然得到维护且安全？

它可能不完全清楚，但它可以告诉你是否存在已知的弃用。

2023 年对 GitHub Copilot 在现实项目中的一次大规模分析揭示了，根据语言的不同，高达 25%–33%的生成代码可能存在潜在的安全漏洞，包括命令注入、代码注入等严重程度高的 CWEs（公共漏洞和暴露）。这些发现强调了 Copilot 反映了其训练数据中存在的安全模式，而不是有意生成有缺陷的代码。一致的推荐？开发者必须保持警惕：手动审查 AI 生成的代码，使用安全意识工具，并保持严格的代码卫生。特别是在“氛围编码”期间，AI 生成内容的速度和范围需要更多的警惕。在更短的时间内编写更多代码意味着需要审计的表面区域更大。

让我们看看一个简短的例子。

## 不恰当的认证和授权

想象一下您要求 AI 在 Express 应用程序中创建一个登录路由。它可能会生成如下内容：

```py
// Insecure example
app.post('/login', async (req, res) => {
  const { username, password } = req.body;
  const user = await Users.findOne({ username: username });
  if (!user) return res.status(401).send("No such user");
  if (user.password === password) { // plain text password comparison
    res.send("Login successful!");
  } else {
    res.status(401).send("Incorrect password");
  }
});
```

这里存在哪些问题？

+   它直接比较密码，这意味着密码以纯文本形式存储在数据库中——这是绝对不允许的。

+   它发送非常通用的响应，这可能对于安全性来说是合适的，但也可能无意中泄露敏感信息。

考虑认证错误消息作为一个关键示例。一个安全得当的系统在登录失败时应该返回一个通用的消息，如“无效凭证”，无论用户名或密码是否错误。然而，AI 生成的代码可能会产生更具体的错误，例如“用户名未找到”或“此用户的密码不正确。”

这些特定的消息通过确认潜在攻击者哪些信息是正确的，从而创建了一个安全漏洞。如果攻击者收到“密码错误”的错误信息，他们现在知道他们已经发现了系统中有效的用户名。这使得他们能够通过重复尝试构建一个合法用户名列表，然后集中精力破解这些已确认账户的密码。这种被称为*用户枚举*的技术，将猜测游戏转变为更具针对性的攻击。AI 倾向于提供有帮助、具体的错误信息，除非你明确指示它为安全敏感操作保持适当模糊的响应，否则无意中帮助了恶意行为者。

虽然代码中没有显示，但这里没有包含速率限制或其他保护措施。

一个安全的实现将包含几个关键的安全措施：

+   首要的是，密码绝不应该以明文形式存储。相反，系统应该存储经过加密散列的密码，并使用安全的比较函数来验证登录尝试。

+   这个比较过程必须使用恒定时间算法来防止时间攻击，攻击者通过分析响应时间来推断密码正确性的信息。许多以安全为重点的库提供这些恒定时间比较函数，专门用于解决这个漏洞。

+   此外，认证系统应实施速率限制或节流机制，以防止暴力攻击。失败的登录尝试应记录在安全监控中，允许管理员检测并响应可疑模式。这些措施共同作用，创建了一种深度防御方法，即使在某个安全层被破坏的情况下也能保护用户凭据。

你可以要求 AI 帮助修复这个问题：

> 改善登录路由，使用 bcrypt 散列和比较密码，并确保数据库中的密码是散列的。

它可能输出：

```py
const bcrypt = require('bcrypt');

app.post('/login', async (req, res) => {
  const { username, password } = req.body;
  const user = await Users.findOne({ username });
  if (!user) {
    return res.status(401).send("Invalid credentials");
  }
  const match = await bcrypt.compare(password, user.passwordHash);
  if (!match) {
    return res.status(401).send("Invalid credentials");
  }
  res.send("Login successful!");
});
```

这更好：它使用`bcrypt`来比较散列密码（假设变量`user.passwordHash`存储了该密码）。在创建用户时，你还想确保使用`bcrypt.hash`来散列他们的密码。

在一点指导之下，AI 可以做到正确的事情，但它的初始天真输出可能并不安全。这强调了这样一个模式：*审查和改进*。

## 包管理问题

另一个常见的漏洞类别是包管理。AI 有时会发明一个库或忘记一个名称，这被称为*包幻觉*问题。这样的包可能不存在，但理论上，攻击者可以发布以常见幻觉名称为名的包含恶意代码的包。如果你在未确认该包存在且是正确包的情况下安装它，你可能会引入严重风险。如果你对某个特定包不确定，尝试快速网络搜索或直接检查 npm/PyPI。

此外，AI 可能无意中生成与训练数据中授权片段相同的代码。这与其说是安全问题，不如说是知识产权问题，但它需要引起足够的重视。例如，GitHub Copilot 包含一个重复检测功能，可以在生成的代码与公共存储库紧密匹配时发出警告，帮助开发者避免潜在的许可冲突。还有类似工具正在出现，以解决 AI 生成代码来源的特定挑战。第九章 将更详细地探讨许可和知识产权问题，提供全面指导，帮助您应对这些复杂问题。

总结来说，主要信息仍然如此——是的，我意识到我在整本书中都强调了这一点，以至于你可能几乎能在睡梦中背诵它——那就是 *AI 输出需要与审查初级开发人员代码相同的谨慎审查*。这种重复是有意的，因为这个原则几乎支撑着安全有效 AI 辅助开发的各个方面。无论你是在进行原型设计、构建后端还是实现安全功能，这种思维模式提供了正确的信任和验证平衡，使 AI 成为一个强大的盟友而不是一个风险捷径。它可以快速编写大量代码，但你需要将其中的安全最佳实践内化，并双重检查漏洞。小说家弗兰克·赫伯特在《沙丘皇帝》（Putnam，1981 年）中的一句经常被引用的话中这样说道：“它们增加了我们无需思考就能做的事情的数量。我们无需思考就能做的事情——这才是真正的危险。”

使用 AI 可能会让你在处理常规代码时减少思考，你应该有意识地思考如何应用安全审查的思维。捕捉那些“无需思考就能做的事情”至关重要。

# 安全审计

针对上述类型的漏洞，你如何有效地审计和确保我们生成的 AI 代码的安全性？本节探讨了你可以使用的几种技术和工具。

## 利用自动化安全扫描器

静态分析工具（SASTs）可以扫描代码中的已知漏洞模式；例如：

+   [ESLint + 安全插件](https://oreil.ly/55ppH) 可以检测 JavaScript 和 Node 代码中的不安全函数或未清理的输入。

+   [Bandit](https://bandit.readthedocs.io) 用于 Python，可以标记生产环境中 assert 的使用、弱加密、硬编码的秘密等。

+   [GitHub CodeQL](https://github.com/github/codeql) 允许你在代码库中运行查询，以查找 SQL 注入、XSS 和其他常见模式。

+   [Semgrep](https://semgrep.dev) 支持多种语言的规则，包括 JavaScript、Python、Java、Go 等社区维护的规则，并能直接识别出一些常见问题。

你可以将这些工具集成到你的 CI/CD 或开发管道中。在 AI 生成的代码上运行它们——它们不会捕捉到所有内容，但可能会标记出明显的错误（例如，明文密码检查、未清理的 SQL、不安全的加密）。这是一个稳固的安全网。

## 使用独立的 AI 作为审稿人

两种不同的方法可以利用 AI 对生成的代码进行安全审查，每种方法都有其独特的优势。第一种方法涉及使用生成代码相同的 AI 模型，要求它转换视角并审计其输出。在生成代码后，你可以向模型提示如下内容：

> 审查此代码的安全漏洞，并解释你发现的问题。

这种方法往往会产生令人惊讶的有效结果，因为模型可以识别常见的安全问题，如明文密码存储、缺少输入验证或潜在的 SQL 注入漏洞。

第二种方法采用不同的 AI 模型作为独立的审稿人。例如，如果你使用 ChatGPT 生成了代码，你可以将那段代码粘贴到 Claude 或 Gemini 中进行安全分析。这种跨模型审查可以揭示不同的视角并捕捉到原始模型可能忽略的问题，就像不同的安全工具或人类审稿人带来不同的专业知识和关注领域一样。不同的模型可能被训练有不同的重点或数据集，可能捕捉到不同类别的漏洞。

这两种技术都作为有价值的额外安全审查层，补充但不取代适当的安全测试和人类专业知识。虽然 AI 审稿人可能会偶尔标记出误报或错过细微的漏洞，但它们擅长快速捕捉常见的安全反模式。将这个过程视为专注于安全考虑的自动化结对编程。关键在于将这些 AI 生成的安全审查视为你安全评估过程中的另一个输入，而不是作为最终的安全许可。

## 使用安全清单进行人工代码审查

如果你在团队中，应该有一个用于审查代码的安全清单。AI 通常生成的代码在预期情况下“工作”，但并未针对恶意情况进行加固。对于 AI 生成的代码，务必考虑以下方面：

+   认证流程：它们是否稳固？

+   数据进入系统的任何地方：我们是否验证了输入？

+   数据离开系统的任何地方：我们是否清理了输出？我们是否保护了敏感数据？

+   外部 API 的使用：我们是否处理了失败？我们是否暴露了密钥？

+   数据库访问：我们是否安全地使用 ORM？我们是否使用参数化查询？

+   低级代码中的内存管理：如果 AI 正在编写 C/C++或 Rust，是否存在溢出？是否存在任何滥用？

## 渗透测试和模糊测试

使用动态方法。对于模糊测试，将随机或特别定制的输入喂入你的函数或端点，以查看它们是否会崩溃或做奇怪的事情。AI 可以帮助生成模糊案例，或者你可以使用[现有的模糊工具](https://oreil.ly/OoFzT)，例如[谷歌的 OSS Fuzz](https://oreil.ly/FvKSU)。

在你的 AI 制作的 Web 应用上运行像 OWASP 的 ZAP 这样的渗透测试工具，可以自动化扫描诸如 XSS 和 SQL 注入漏洞等问题。例如，ZAP 可能会尝试注入一个脚本并使其反射，并检测到某些输入没有被清理。

如果你正在构建一个 API，可以使用 Postman 或自定义脚本等工具尝试发送格式不正确的数据，以查看系统如何响应：是否会抛出 500 错误或优雅地处理错误？

## 添加以安全为重点的单元测试

对于关键代码片段，编写断言安全属性的测试。例如，你可能测试你的登录速率限制器在 X 次失败尝试后触发，或者某些输入（如`"<script>alert(1)</script>"`）在响应中输出时被转义。为了测试未经授权的用户无法访问受保护资源，模拟授权和未经授权的调用，并确保应用程序的行为正确。

你可以要求 AI 帮助生成这些测试：

> 编写测试以确保未经授权的用户在/deleteUser 端点上收到 403 错误。

然后运行测试。

## 提供更新以补偿训练截止点

AI 模型具有一个直接影响安全的根本性限制：它们的知识在某个时间点冻结。当模型完成训练后，它无法了解之后发现的漏洞、随后发布的安全补丁或出现的新最佳实践。这种知识截止点在 AI 所知和当前安全标准之间造成了关键差距。

考虑在 2023 年训练的模型在 2025 年生成代码。在这段时间里，发现了许多安全漏洞，进行了修补和记录。新的攻击向量出现，框架增加了安全功能，最佳实践也发生了演变。然而，AI 除非你明确在提示中提供更新信息，否则对这些发展一无所知。

这种限制在快速发展的安全标准和漏洞数据库中尤其明显。例如，[OWASP Top 10](https://oreil.ly/US-uh)会定期更新，以反映不断变化的威胁环境。如果你提示 AI“编写一个安全的文件上传函数”，它可能会根据其训练数据实施合理的保护——可能包括文件类型验证、大小限制和存储在 Web 根目录之外。然而，它可能会错过最近发现的攻击向量或未能实施新推荐缓解措施。

解决方案涉及积极补充 AI 的知识，以当前的安全信息。在请求安全敏感的代码时，在您的提示中包含对当前最佳实践的引用。例如，您可能不会仅仅要求安全的代码，而是可能会提示：

> 编写一个文件上传函数，解决 2025 年 OWASP Top 10 中的安全关注点，特别是关注注入攻击和服务器端请求伪造。

这种方法将 AI 的响应建立在当前的安全标准上，而不是可能过时的训练数据。

类似地，框架特定的安全功能通常在 AI 的训练截止日期之后出现。例如，Express.js 应用程序从[Helmet 中间件](https://oreil.ly/WSPar)中受益匪浅，该中间件用于设置安全头。在 Helmet 成为标准实践之前训练的 AI 可能会生成没有这个关键安全层的 Express 应用程序。通过在提示中明确提及当前的安全工具和实践，您帮助 AI 生成与当代安全标准相符合的代码，而不是历史标准。

## 优化您的日志记录实践

确保代码（AI 和人类）有良好的日志记录，尤其是在关键操作或潜在的故障点周围。这有助于在生产中调试问题。如果 AI 编写了一个日志记录很少的部分，考虑添加更多。例如，如果有一个 AI 生成的捕获块只是吞咽错误，将其更改为记录错误（以及可能的一些上下文）以提高可见性。此外，清理日志，确保它们不包含任何敏感信息。

## 使用具有安全重点的更新模型或工具

一些 AI 编码工具旨在将代码生成与内置的安全扫描相结合。Snyk 是一个典型的例子：它采用[混合方法](https://oreil.ly/0ZGFv)，结合 LLM 生成的建议和基于规则的污点分析。根据 Snyk 的说法，当您请求代码（即使是来自 OpenAI、Anthropic 或 Hugging Face 等 LLM 库的代码）时，Snyk Code 跟踪潜在的不安全数据流，并在它们到达敏感的汇点之前标记不受信任的输入。在实践中，这意味着如果 AI 建议一个数据库查询，Snyk 确保它是参数化的，防止 SQL 注入——即使您自己忘记这样做。这类工具特别有用，因为它通过避免通过 AI 生成的建议引入不安全代码。

## 注意上下文中的警告

如果您使用的是 IDE，您通常会看到警告或波浪线来突出可疑代码。具有 IntelliSense 的现代 IDE 有时可以捕获，例如，看起来可疑的 SQL 字符串连接。不要因为 AI 编写了这些警告和标志就忽略它们——解决问题。AI 在生成代码时没有这些实时警告的优势。

## 放慢速度

在使用人工智能快速生成大量代码之后，当审计时间到来时，要放慢速度。当你能够快速生成功能时，追逐下一个功能是有诱惑力的，但请安排时间进行彻底的审查。将其视为“人工智能加速开发，人类加速安全”。Snyk 的[最佳实践](https://oreil.ly/uUExW)建议在 IDE 中直接扫描人工智能代码，并警告不要让人工智能的速度超过你的安全检查。换句话说，将安全扫描集成到你的开发循环中，以便在代码编写后立即发现漏洞。

总结来说，当你审计人工智能生成的代码时，你会使用许多在传统开发中使用的相同工具——静态分析、动态测试、代码审查——但你可能需要更频繁地应用它们，因为代码生成速度更快。*将每个人工智能输出视为需要检查*。

# 为人工智能生成系统构建有效的测试框架

虽然安全性是可靠性的一个支柱，但更广泛的概念涵盖了你的软件系统的基本可靠性。*可靠性*在软件架构术语中，解决了关于系统故障及其后果的关键问题。你的系统是否需要是故障安全的？它是否在影响人类生活或安全的方式上是至关重要的？如果系统失败，是否会对你组织的财务造成重大损失？这些考虑决定了你在开发和测试实践中所需的严谨性。

当你在人工智能辅助下进行构建时，这些可靠性风险仍然保持不变。使用人工智能辅助生成的银行应用程序与完全由人类编写的应用程序一样，对交易准确性和数据完整性有相同的要求。无论其代码的来源如何，医疗系统都必须满足相同的患者安全标准。人工智能在代码生成中的参与并不会降低这些基本可靠性要求。

这种现实强调了为什么在人工智能辅助开发中，全面的测试变得更加关键。强大的测试框架确保你的代码正确执行其预期功能，并在项目演变过程中保持这种正确性。虽然测试人工智能生成的代码遵循与测试人类编写的代码相同的根本原则，但人工智能开发过程中出现了一些细微差别和机会，值得特别注意。

以下部分探讨了如何利用人工智能不仅生成代码，而且创建健壮的测试套件来验证可靠性、维护系统稳定性和提供信心，即当风险最高时，你的软件将正确执行。

首先，尽早并经常拥抱自动化测试。当开发进度缓慢时，你可能会跳过编写测试，因为你想要推动新功能的开发。讽刺的是，当开发速度加快（使用 AI）时，跳过测试也变得容易，因为新功能不断涌现。但是，当代码快速生成时，这正是你最需要测试来捕捉回归或集成问题的时刻。因此，在使用 AI 帮助实现功能后，养成立即为其编写测试的习惯（甚至可以使用 AI 来编写这些测试）。这不仅可以验证功能，还可以在后续更改时保护它。

一项[2022 年的研究](https://oreil.ly/Vc8Gd)发现，使用 AI 助手的开发者对他们所编写的代码的安全性更有信心，即使从客观上讲，这些代码的安全性不如没有 AI 辅助的代码。你需要通过实际测试来对抗这种过度自信。

正如我在第四章中提到的，你可以使用 AI 不仅来生成代码，还可以生成一系列测试。这样，AI 可以帮助自我双重检查。这就像让它同时进行实现和初步验证。例如，在编写一个新的模块后，你可以这样问：

> 为此模块编写单元测试，覆盖边缘情况。

如果它们通过了，那很好。如果失败了，要么是有错误，要么是测试期望了其他内容。根据需要调查并修复代码或测试。

谨慎地对待 AI 可能错误地假设某些输出或行为；像对待其代码一样，将其测试视为建议，而不是事实。你可能需要调整测试的期望以匹配预期的行为——即使这个过程也是有价值的，因为它迫使你清楚地定义预期的行为。

将你的测试套件集成到每次提交时运行的 CI 管道中。这样，每当添加或更改 AI 生成的代码时，所有测试都会自动运行。如果出现问题，你将能够及早发现。有时 AI 可能会引入微妙的破坏性更改（如稍微改变函数签名或输出格式），一个健壮的测试套件将能够检测到这一点。在 CI 中还包括安全扫描（如`npm audit`或静态分析），以便任何引入的风险模式都会被标记出来。可以尝试的测试类型包括：

基于属性的测试和模糊测试

*基于属性的测试*（使用像[Python 的 Hypothesis](https://oreil.ly/JcYBf)或[JavaScript 的 fast-check](https://fast-check.dev)这样的工具）是另一种有价值的技巧。你不需要为特定的输入和预期输出编写单个测试用例，而是定义代码应该始终满足的高级别属性。然后，框架会生成广泛范围的输入来检查这些属性是否成立。

以排序为例。与其断言`sort([3, 1, 2]) === [1, 2, 3]`，你可以定义属性：

+   输出应该是有序的

+   它应该包含与输入相同的元素

工具随后会生成数十或数百个输入数组来测试这些条件——并发现您可能没有手动想到的边缘情况。

这对于人工智能生成的代码尤其有用。如果您的 AI 编写了一个用于规范化电子邮件地址（例如，通过将域名转换为小写）的函数，属性测试可能会检查输出是否是*幂等的*——这意味着运行函数两次与运行一次得到相同的结果。如果边缘情况违反了这个不变性，测试框架将生成一个反例来帮助您诊断错误。

加载和性能测试

人工智能可能编写未优化的代码。在负载下测试您的系统是个好主意。这在性能方面是可靠性的体现。使用 JMeter、Locust 或 k6 等工具模拟大量请求或大量数据，看看系统是否能够承受。如果不能，找出瓶颈。

例如，也许人工智能编写了一个在 100 个元素上运行良好的`O(n²)`算法，但在 10,000 个元素上会崩溃。如果没有性能测试，您可能直到它投入生产时才注意到这一点。因此，如果适用，结合一些性能场景。随着输入大小的增加，对关键操作进行计时，或使用分析工具查看 CPU 时间或内存消耗在重任务中的位置。

错误处理

故意引入错误以确保系统能够优雅地响应，例如：

+   对于 API，关闭数据库并查看 API 是否返回友好的错误或崩溃。如果它崩溃，添加代码（或要求人工智能添加代码）来处理数据库连接错误。

+   对于前端，模拟后端返回 500 错误，并确保 UI 显示错误消息，而不是空白页面或无限加载指示器。

人工智能在编写代码时可能不会自己想到这些故障模式，因此您必须测试它们并加以改进。测试这些场景将通过提示您添加适当的回退逻辑、重试或用户反馈来提高可靠性。

监控和日志记录

结合日志记录，并在测试中可能使用日志进行验证。例如，如果某个操作应该触发审计日志条目，进行测试。人工智能可以生成日志行；验证它们是否按预期打印出来。

此外，考虑设置监控（例如，模拟在生产中如何监控您的服务）。例如，您可能需要跟踪测试运行期间是否记录了任何未捕获的异常。如果是，将其视为测试失败；这意味着某些情况没有得到妥善处理。

可维护性

维护性测试，如确保代码风格和标准，非常重要。使用 linters 和 formatters 来保持代码一致性，因为 AI 可能会根据不同的提示产生略微不同的风格。一个格式化工具，如[Prettier](https://prettier.io)或[Black (for Python)](https://pypi.org/project/black)，可以统一风格。为了获得更多逻辑一致性，并捕捉可能需要重构的过于复杂的 AI 生成代码，考虑添加强制执行诸如函数复杂度限制之类的规则的 linting 规则。（更多信息请参阅“确保 AI 加速代码库的可维护性”）

一旦你的测试就绪，你就可以更有信心地重构 AI 代码。也许 AI 产生了一个工作但笨拙的解决方案；你可以改进它，并依靠测试来确保你没有破坏其行为。你甚至可以要求 AI 重构它自己的代码：

> 重构这个函数以使其更清晰，同时确保它通过当前的测试。

如果你的测试做得好，你可以检查重构是否破坏了任何东西。

理解 AI 系统中的非确定性需要区分两种根本不同的场景。当 AI 在生产系统中运行时，例如聊天机器人响应客户查询或推荐引擎个性化内容，即使输入相同，输出也可能不同。这种可变性源于模型温度设置、随机种子或演变的模型状态等因素。测试此类系统需要专门的方案，这些方案考虑了可接受的变异范围，而不是期望精确匹配。

然而，AI 辅助的代码生成完全是一种不同的范式。一旦 AI 生成代码并将其提交到你的仓库，它就变得和任何人类编写的代码一样确定。计算税率的功能将每次对相同的输入产生相同的输出，无论最初是人为编写还是 AI 编写。这种确定性对于系统可靠性至关重要，使得传统的测试方法完全适用于 AI 生成的代码。

当集成多个 AI 生成的组件时，每个组件可能都是独立创建的，具有不同的隐含假设，这时会浮现出更微妙挑战。考虑一个来自电子商务系统的具体例子。你可能会提示 AI 生成一个订单处理模块，指示它处理国际订单。单独地，你要求 AI 为同一系统创建一个运输计算服务。订单处理模块遵循美国惯例，将日期格式化为“12/25/2024”表示 12 月 25 日。而运输服务，可能受到其生成过程中欧洲示例的影响，期望日期格式化为“25/12/2024”。这两个组件在独立运行时都表现完美，通过了各自的单元测试。

这种不匹配只有在集成测试期间才会显现出来，当订单处理程序将日期传递给运输计算器时。运输服务将“12/01/2024”解释为 1 月 12 日而不是 12 月 1 日，可能会基于错误的月份计算运输时间。这种假设不匹配在 AI 生成的组件中尤其常见，因为 AI 在独立生成每个部分时可能会从不同的示例或惯例中汲取灵感。进行全面的集成测试，以锻炼组件之间的实际数据流，对于在它们导致生产故障之前捕捉这些细微的不兼容性至关重要。

对于 AI 辅助项目的 QA 流程可能需要更多的创造性，因为 AI 可能会引入不寻常的边缘情况。例如，AI 可能会输出你未明确考虑的功能——如果是这样，也要测试这一点。如果它添加了隐藏的行为，要么移除它，要么正确地测试它。

最后，如果可能的话，在类似生产环境的环境中测试你的应用程序，使用真实的数据负载。有时只有在更大的数据量或更高的并发性下才会出现性能问题。使用这些测试结果来定位低效之处。

# 性能优化

尽管 AI 经常编写正确的代码，但它并不总是编写**最优**的代码。大型语言模型（LLMs）本身并不进行性能分析；它们通常复制其在训练数据中常见的模式。因此，对潜在的性能问题保持警惕，尤其是在关键路径或大规模使用时。

你甚至可以与 AI 聊天，获取有关性能优化的提示：

+   这段代码的复杂度是多少？可以改进吗？

+   这个函数运行得很慢——有没有什么办法让它更快？

它可能并不总是正确，但有时它可以提供有用的建议，或者至少确认你的思考。

话虽如此，不要过度优化，也不要过早或在不必要的地方进行优化。有时，如果数据量小或操作不频繁，AI 解决方案可能完全没问题。使用你的分析数据来关注真正的瓶颈，并优化真正需要优化的部分。Vibe 编码的优势在于你并没有花费大量时间从头开始手工编写代码，因此你可以允许一些非关键部分简单且不是超级优化，只要它们不影响用户体验或成本。这种方法与敏捷实践相一致：先让它工作，然后再让它变得更快（如果需要）。

在确保你的 AI 增强项目高效运行时，以下是一些需要覆盖的区域：

复杂性分析

当 AI 生成算法时，花点时间考虑其复杂性。有时它可能会使用暴力解法，尽管存在更有效的算法。例如，它可能会对列表进行双重排序，因为它没有回忆起单步方法，导致 O(n log n × 2)，而 O(n log n)就可以做到（大写*O*代表内存使用）。或者它可能会使用嵌套循环，使得操作达到 O(n²)，而存在已知的 O(n)方法。如果你发现类似的情况，请要求改进：

> 我们能否优化以避免嵌套循环？也许可以使用集合进行查找。

如果你暗示了方法，AI 通常会遵从并给出更好的解决方案。如果没有，你可能需要手动实现那部分。

要识别慢速函数，运行分析器或使用代表性或最坏情况数据测量关键代码路径的执行时间。如果某些操作太慢，你可以尝试手动或使用 AI 辅助进行优化：

> 优化这个当前作为瓶颈的功能；尽量减少其复杂性。

AI 可能会为了性能重构代码。使用测试来确保它仍然可以工作。

对于关键算法，编写一个小型基准测试套件。如果 AI 给你一段代码，比如计算某个东西，测试它与其他方法，或者至少测量它随着输入大小的扩展情况。如果需要，你可能决定以更有效的方式重写。

内存使用、泄漏和保留

AI 生成的解决方案可能比必要的使用更多内存：例如，将整个文件读入内存而不是流式传输，从而保留大量数据结构。如果你的用例涉及大数据，检查系统的内存使用情况，并在需要时通过流式传输或分块进行优化。例如，如果你需要处理数百万条记录，你希望重构 AI 生成的函数`loadAllRecords()`以批量处理它们或从数据库中流式传输。

还要检查 AI 生成的代码是否释放了资源。在像 Java 或 C#这样的语言中，可能它打开了一个文件或数据库连接而没有关闭。在前端单页应用中，可能事件监听器没有被移除，导致泄漏。工具可以帮助（如前端 Chrome 开发者工具的内存检查器或 C++的 Valgrind 泄漏检测），但通常只是阅读代码就能帮助。识别这些问题并修复它们。如果你看到一个未关闭的打开文件句柄，在`finally`块中添加一个关闭操作。

并发与并行

如果你使用支持线程或异步的语言，寻找 AI 代码可能单线程的地方，而它本可以是并行的。AI 可能不会在适当的地方自动使用 async/await，并且可能不知道将重负载 CPU 任务卸载到工作线程。识别这样的机会。例如，对于 Node 或 Python 中的 I/O 密集型任务，确保异步使用，以便系统不会阻塞。对于 CPU 密集型任务，AI 可能在代码上帮助不大，但你可能决定在更高效的语言中实现或将其卸载到后台任务。

缓存

AI 不总是自动添加的一种常见性能优化是将昂贵操作的结果进行缓存。看看你的代码：是否反复计算某些内容？如果是这样，实现缓存（内存中的或使用外部缓存如 Redis）。你可以提示 AI：

> 将缓存添加到这个函数中，以避免重复计算。

它可能实现简单的记忆功能或建议使用缓存库。

数据库查询优化

如果你的应用程序使用数据库，检查 AI 创建的查询。它们是否正确使用索引？也许 AI 写了`SELECT *`，但实际上只需要几个列。或者它正在检索大量数据以在代码中进行过滤，从而创建性能瓶颈，如 N + 1 查询问题。这些低效需要通过将更多工作推送到数据库或利用适当的索引进行优化。

例如，如果生成的代码在循环中反复调用`findOne`，导致多次数据库往返，你可以将其重构为使用`WHERE id IN (...)`的单个批量查询。同样，如果 AI 在迁移中省略了频繁查询字段的索引创建，添加这些索引对于保持可接受的性能变得至关重要。AI 通常生成功能上正确但次优的数据库交互，需要人类专业知识来识别和解决。

为了说明，让我们举一个例子。假设 AI 为你编写了一个函数，该函数通过简单地将两个排序数组连接并排序来合并它们：（O(n log n)）——即使有一个已知的线性算法可以用来合并两个排序列表（如合并步骤或归并排序，O(n)）。在代码审查中，你意识到这可能是大型数组的瓶颈，因此你提示 AI 实现线性合并：

> 优化 mergeSortedArrays 函数，以线性时间执行合并操作，而不使用内置排序。

人工智能将其识别为经典的合并算法并将其写入。解决方案通过了你的测试，所以恭喜：你在不牺牲正确性的情况下提高了性能。

AI 辅助开发并不消除对性能调优的需求；它只是改变了你进行调优的时间。你通常会首先得到一个正确的解决方案（这非常有价值），然后转向关注测量和优化目标部分。当你需要优化某些内容时，AI 可以帮助你，只要你引导它了解你需要什么。

# 确保在 AI 加速的代码库中的可维护性

代码库的*可维护性*描述了随着时间的推移修改、扩展和理解的难易程度。有些人担心 AI 生成的代码可能杂乱无章或不一致，特别是如果多个建议有不同的风格或模式。本节涵盖了你可以使用的几个实践来处理这些担忧，并保持你的项目整洁且可维护。

## 在提示过程中

在准备你的提示时，以下是一些需要注意的事项：

使用一致的编码标准

使用 linters 和 formatters 来强制执行一致的风格。如前所述，AI 有时可能会在不同的输出中使用不同的命名约定或格式。在生成所有代码后运行格式化器（如 JS 的 Prettier、Python 的 Black、Go 的 gofmt 等）确保其符合统一风格。这使得阅读代码变得更加容易（无需在风格之间切换认知负荷）。此外，为你的项目定义命名约定并坚持使用。如果 AI 在一个地方输出`get_user_data`而在另一个地方输出`fetchUserData`，决定你更喜欢哪种约定（`snake_case`与`camelCase`等）并重构为一种风格。

使用架构模式来鼓励模块化并避免蔓延。

通过提示 AI 分离关注点来鼓励它编写模块化代码。例如，不要让它编写一个实现所有内容的巨大文件，而是将工作分解成任务：

+   为用户逻辑创建一个 UserService 类。

+   为发送电子邮件创建一个单独的模块。

这导致代码库在逻辑上被划分。当每个模块都有明确的职责时，它更容易维护。你可以引导架构：

> 将数据库访问代码放在与 API 路由代码分开的文件或类中。

由于使用 AI 添加功能非常容易，因此防止功能蔓延和代码蔓延至关重要。如果没有纪律性的架构思维，你的代码库可能会演变成软件架构师所说的“大泥球”：一种代码缺乏清晰结构或边界的反模式。这种风险在 AI 辅助下加剧，因为添加功能时传统上所关联的摩擦消失了，可能会加速架构的退化。

为了应对这一问题，将你的 AI 辅助开发建立在经过验证的架构模式和原则之上。在指导 AI 时，明确指出你的项目遵循的模式：

+   按照项目中使用的仓库/服务模式添加此新功能。

+   使用我们在领域层建立的六边形架构来实现这一点。

这种具体性有助于在功能快速积累的同时保持一致性。

对于寻求更深入架构基础的开发者，几本基础文本提供了必要的指导：

+   由 Erich Gamma、Richard Helm、Ralph Johnson 和 John Vlissides（“四人帮”）所著的《*设计模式：可复用面向对象软件元素*》（Addison-Wesley，1994）仍然是可复用设计解决方案的权威目录。

+   [《*软件架构基础：工程方法*》](https://learning.oreilly.com/library/view/fundamentals-of-software/9781098175504/) 由 Mark Richards 和 Neal Ford 所著，全面涵盖了技术栈中的架构模式和原则。

+   《*领域驱动设计：软件核心的复杂性应对*》（Eric Evans 著，Addison-Wesley，2003 年）提供了将软件设计与业务领域对齐的关键技术——尤其是当 AI 生成的代码必须反映复杂的业务逻辑时，这一点尤为宝贵。

这些资源使你能够有效地指导 AI 工具，确保生成的代码遵循良好的架构原则，而不是增加技术债务。记住：AI 擅长实现模式，但不能确定哪些模式适合你的特定环境。那种架构判断本质上仍然是人类的。

## 与代码输出一起工作

一旦 AI 响应并生成代码，以下是一些可用的维护技术：

持续重构

需要时不要犹豫重构 AI 生成的代码。有时第一次尝试可能正确，但结构并不理想：例如，AI 可能会编写一个非常长的函数或在两个地方重复其逻辑。一个常见的挑战是不经意间重复的代码：AI 可能没有意识到两个函数做的是类似的事情，因此创建了两个。如果你注意到类似的代码块，就重构为一个。像代码检查器这样的工具可以检测重复（有针对过于相似代码的检查器）。运行这些工具可能会突出显示需要“DRY out”（不要重复自己）的地方。

要让 AI 帮助重构，你可以提示：

> 重构此代码以消除重复并提高清晰度。

它可能会创建辅助函数或简化一些逻辑。重构后始终要测试。

测试

本章已经涵盖了测试，所以这里只提一下，一个好的测试套件可以使维护更容易。当你或其他人将来修改代码（可能再次使用 AI）时，你的测试将捕捉到是否有任何更改破坏了任何东西，这样你就可以安心地进行重构或更改实现。测试将“做什么”与“如何做”解耦，这让你在不改变“做什么”的情况下，有灵活性来维护或改进“如何”。

避免过度复杂或过度依赖 AI 特定的结构

有时 AI 可能会使用一些巧妙的技巧或不太常见的函数，其他开发者可能不知道。虽然这本身并不坏，但考虑可维护性：如果一个普通开发者看到代码会挠头，也许可以简化它。例如，如果 AI 使用了一些正则表达式的魔法或过于简略的列表解析，为了清晰起见，可以将其重写为更明确的循环（或者至少注释它）。

同样，一个试图提供帮助的 AI 可能会过度设计解决方案，比如添加不必要的层。例如，可能直接的方法就足够了，但 AI 引入了一个没有发挥作用的抽象。移除它以保持事情简单。通常，更简单的代码更容易维护。

增强代码的弹性和回退机制

考虑在出现故障时采取后备策略。例如，如果一个 AI 编写的组件调用外部 API，而这个 API 出现故障或返回意外数据，我们是否有后备方案（如使用缓存数据或默认响应）？实现这种弹性模式（断路器、带有退避的重试等）可以使系统更健壮。除非被要求，否则 AI 可能不会自己这样做。确保系统可以优雅地处理部分故障。如果可能的话，一个微服务崩溃不应该导致整个应用程序崩溃。使用超时和后备逻辑。

## 后续

一旦你对代码感到满意，一些额外的做法可以帮助保持其可维护性：

提供详尽的文档和注释

确保代码得到适当的文档化。AI 通常除非被提示，否则只会写最少的注释。你可以通过提示请求文档字符串或注释：

+   为这段代码的每个部分添加注释以解释其目的。

+   为这个函数编写文档字符串。

这些可以节省未来读者的时间。AI 通常可以生成相当好的解释，但有时会误解细微之处，因此请进行审查以确保准确性。

还应考虑为项目维护高级文档（如 README 或设计文档），描述其架构、主要组件等。你可以大部分自己编写，但如果需要，AI 可以通过总结代码库来提供帮助。

如果你遇到一些怪癖，比如“AI 总是以奇怪的方式命名这个参数”，请在你的开发笔记中提及它，以便其他人知道。这是新协作环境的一部分。如果你是唯一使用 AI 生成代码的人，一些怪癖是可以接受的——但如果其他人加入项目，他们可能会想知道，“为什么这个命名是这样的？”也许只是标准化那些名称。

在维护性方面，了解哪些代码是 AI 生成的，哪些是人工编写的，也是一个方面。这并不是严格必要的，但一些团队可能会注释，“由 GPT-4 于 2025-05-01 生成”以供追溯。理想情况下，在 PR 描述中标记任何你不确定的事情：“使用 ChatGPT 帮助完成此功能；它似乎可以工作，但请仔细检查错误处理逻辑。”

这并不是一个普遍的做法。在代码审查期间可能会有所帮助，但如果已经有人类审查过代码，现在只是代码的话，你可能不需要它。如果你保留任何转录或提示，你可以在复杂代码的注释中链接它们：“此算法通过 GPT-4 推导，基于提示 X；请参阅文档以了解推导过程。”审查者不需要在审查时对待它有所不同（你应该审查所有代码），但它有助于理解上下文。例如，如果代码存在某种风格不匹配或奇特的习语，知道它来自 AI 可能会让审查者意识到这并不是作者故意的选择，而是 AI 的产物。

代码审查和团队规范

如果你在一个团队中工作，让所有团队成员审查代码——即使是一个人和 AI 共同编写的。他们可能会发现尴尬的模式或违反团队规范的事情。随着时间的推移，你会发展出一种感觉，如何提示 AI 以匹配你团队的风格（可能包括在系统提示或初始指南中具体说明）。如果有多个开发者使用 AI，确保每个人都了解期望的风格模式，以便他们可以相应地提示（例如，“用函数式风格编写这个”或“使用 async/await，而不是回调”）。下一节将提供一些关于使用 AI 代码进行代码审查的技巧。

跟踪技术债务

如果在开发过程中，你接受了一个你知道并不理想的 AI 解决方案，请在你的注释或项目待办事项中将其跟踪为技术债务：“待办：这个解决方案工作正常，但效率是 O(n²)；如果数据增长，优化这一点，”或者“待办：为了简单起见，这里使用了全局变量；稍后进行细化。”如果你要求，AI 甚至可以自己插入待办注释：

> 如果有任何需要未来改进的区域，添加待办事项注释。

只需最终解决这些待办事项。

从 AI 模式中学习

如果 AI 引入了一个你不太熟悉的设计模式或库，花时间了解更多关于它的信息，而不是忽略它。了解特定的缓存方法或它使用的库将有助于你在未来自信地维护或修改这部分内容。如果它过于晦涩，你可能会决定用你已知的东西替换它——但有时 AI 会以你不知道的有用库或模式让你感到惊喜。如果这是一个你和你团队都可以学习的知名解决方案，这甚至可以提高可维护性。

在实践中，可维护性归结为始终应用相同的良好软件工程原则——只是将这些原则应用到部分由 AI 编写的代码上。幸运的是，由于 AI 减少了繁琐的工作，你可能会有更多的时间来整理代码和编写文档，这*提高了*可维护性。

一些公司[报告](https://oreil.ly/2lrTW)，在用 AI 生成代码的初始爆发之后，他们会投入时间进行“加固冲刺”来重构和记录所有内容。考虑在生成密集型冲刺和清理冲刺之间交替，这可能是一种潜在策略。

# 代码审查策略

如第四章所述，代码审查是传统开发中的关键过程，在 AI 辅助开发中也是如此。本节讨论了在审查的代码块由机器建议时需要考虑的一些细微差别。由于 AI 可以快速生成代码，因此有理由担心代码审查可能会成为瓶颈——但不要让这种担忧阻碍审查过程。为审查分配适当的时间至关重要。不要因为“我们写得快，就合并得快”的假设而节省时间。相反，更频繁地提交较小的更改，以便更容易进行审查（这通常也是一项良好的实践）。频繁的小型拉取请求（PR）比一个巨大的 PR 更容易彻底审查。如果计划得当，AI 还可以帮助将任务分解成更小的 PR。

不要仅仅因为“AI 编写了代码并且测试通过”就假设代码是正确的。要批判性地思考，并尝试通过逻辑推理。如果可能的话，通过心理测试或使用提供的测试之外的其他案例来测试它，因为测试可能无法涵盖所有内容。你还可以运行代码，甚至通过运行具有复杂输入的代码片段来实验，看看它是否表现正常。

代码审查也可以是重要的学习时刻。如果 AI 引入了一种实际上很好的新解决方案，审查者可以在验证其正确性的同时学到新东西。同样，如果 AI/人类组合做出了次优决策，审查者可以解释一个更好的方法。随着时间的推移，这个反馈循环可以提高团队使用 AI 的方式（例如帮助每个人了解哪些事情要避免或以不同的方式询问）。从某种意义上说，代码审查有助于关闭人类学习循环，因为人类作者应该学习和理解 AI 为他们编写的新内容。

当你审查代码时，你的首要任务应该是确保它符合要求和预期设计。这段代码是否完成了功能/错误修复应该完成的工作？它是否涵盖了规格说明中提到的任何边缘情况？如果提示不正确，AI 可能会解决一个稍微不同的问题：它可能处理了不需要的情况，或者遗漏了某个情况。这是正常的，但要注意开发者是否只是接受了 AI 输出，该输出仅部分解决了问题。例如，AI 可能会生成格式化日期的代码，但假设一个特定时区，这可能与要求一致或不一致。

如果代码中的某些内容不明显，请要求作者解释它是如何工作的或为什么这样做。如果他们难以解释或寻求“AI 做了，我假设它是正确的”，那么这是一个红旗。团队应该理解代码库中的所有内容。鼓励作者与 AI 或文档进行双重检查，并提供适当的解释，可能是在代码中的注释。

还要注意本章前面讨论过的安全和性能漏洞，如果违反了任何已知最佳实践，请指出——比如输出没有转义（在 Web 开发中）或你在代码中找到凭证。

如果你看到代码虽然能工作，但可以更简单或更符合团队风格，请请求更改或重构：

> AI 为不同的用户角色创建了 3 个不同的函数，这些函数大部分是重复的。我们能将它们合并成一个带有角色参数的函数吗？

代码的作者可以这样做（也许需要 AI 的帮助）。如果 AI 的建议没有使用团队的一致风格或标准库，也要指出这一点：

> 我们通常使用 requests 库来进行 HTTP 调用，但这段代码使用了 http.client。为了保持一致性，我们还是坚持使用 requests 库。

然后，作者可以提示 AI 使用首选库进行重写。

如果 AI 编写了一些非常复杂的代码，比如一个棘手的算法，考虑与另一位审查者或团队讨论，进行更深入的审查。

你可能想尝试一些使用 AI 辅助代码审查的新兴工具——比如 GitHub 的 Copilot 用于 Pull Requests，它可以生成摘要并标记潜在的 bug 和其他问题。这样的工具可能会突出显示“这个代码片段与模块 X 中的一个类似，但有细微差别”（指出可能的重复）。这些提示可以补充人工审查，但不应该取代它。

最后，即使代码由于 AI 而存在缺陷，也要在审查中保持尊重和建设性。避免责怪开发者可能是由 AI 产生的错误：虽然他们仍然对其代码负责，但也要认识到背景。AI 是一个工具，作者和审查者都在使用它。目标是改进代码和分享知识，而不是指责。例如：“这部分似乎存在安全问题——可能是 AI 建议的疏忽；让我们修复它。”

最终，在 vibe coding 中，代码审查是我们完全发挥人/AI 伙伴关系中的*人类智能*方面。这是监督和专业知识介入以捕捉 AI 可能遗漏的内容并保持质量标准高的地方。它也是团队的知识共享时刻，因为审查中的代码讨论可以加深对领域和如何最好地使用 AI 的理解。

代码审查也正式化了“开发者作为编辑”的概念[由 Grant Gross 在*CIO*](https://oreil.ly/INPFV)提出：审查者是一个编辑，确保代码经过打磨且适合生产。这与作为概念的 vibe coding 完美契合，其中 vibe（AI 建议）存在，但人类的判断力对其进行了细化。

# 可靠部署的最佳实践

一旦你知道你的代码是安全的、经过测试的且可维护的，你需要将其部署并确保在生产环境中可靠运行。

尽管人工智能辅助开发并没有改变软件部署的核心原则，但它确实引入了关于部署速度和运营复杂性的考虑。对于那些寻求全面覆盖部署基础的人来说，Gene Kim、Jez Humble、Patrick Debois、John Willis 和 Nicole Forsgren 著的《DevOps 手册》（IT Revolution Press，2016 年），提供了权威指南，涵盖了从持续集成和部署管道到监控、安全和组织转型的各个方面。当人工智能加速你生成可部署代码的能力时，这种基础性知识变得更加关键，因为这些原则确保你的部署实践可以随着你的开发速度的增加而扩展。

## 在部署前和部署期间

当你准备部署时，考虑以下最佳实践：

自动化你的 CI/CD 管道

由于人工智能发展的快速步伐，一个健壮的持续集成/持续部署（CI/CD）管道非常有价值。每个提交（无论是否有人工智能生成的代码）都应该通过自动化管道进行构建、测试，并可能部署。这减少了人为错误，并确认所有部署步骤（测试、代码审查、安全扫描）都是一致运行的。如果人工智能代码引入了破坏构建或测试失败的内容，CI 将立即捕捉到它。此外，自动化的 CI/CD 管道允许快速迭代，因此你可以快速修复人工智能引入的问题并部署修复程序。

基础设施即代码

使用基础设施即代码（Terraform、CloudFormation 等）来定义你的部署环境。虽然它与人工智能编码没有直接关系，但它构成了可靠的部署的一部分。你甚至可以使用人工智能来帮助编写 Terraform 脚本，但要以与其他人工智能代码相同的谨慎和测试来对待它们，包括在应用到生产之前在沙盒中测试它们。一个有价值的起点是 Yevgeniy Brikman 著的书籍 [*Terraform: Up & Running*](https://learning.oreilly.com/library/view/terraform-up-and/9781098116736/)（O’Reilly，2022 年），它提供了对使用 Terraform 的基础设施即代码原则和实践的全面介绍。

使用分阶段推出，并制定回滚计划

使用分阶段推出策略，如在生产全面推出之前部署到测试环境或金丝雀发布。这样，你可以在它影响所有用户之前捕捉到你忽略的任何内容。例如，你可能会将新的 AI 编码功能部署给 5% 的用户，并监控（使用指标和日志）任何错误或性能问题。如果一切顺利，将其推广到 100% 的用户。

总是制定回滚计划。尽管经过所有测试和审查，有时事情还是会出错。如果新版本出现问题，准备好回滚到最后一个稳定版本。如果你使用像 Kubernetes 这样的容器化策略，保持之前的部署以快速切换回。如果是一个无服务器函数，保持上一个版本运行，直到你对新版本有信心。

设置可观察性

在生产环境中设置全面的监控，包括系统指标和应用程序日志：

+   使用 Sentry 等工具跟踪错误并捕获异常。如果 AI 代码在生产中抛出意外的错误（可能是因为没有覆盖边缘情况），你会收到警报，以便你可以修复它。

+   使用性能监控工具，如应用程序性能监控（APM），以跟踪响应时间、吞吐量和内存使用情况。这将显示新部署中是否有任何代码引入了减速或内存泄漏。

+   监控可用性：例如，ping 服务端点以确认它们正在运行。如果发生崩溃（可能由于某些未测试的场景），应该触发警报，以便你可以快速反应。

保持对安全的警惕

确保在部署中妥善处理像 API 密钥这样的机密。例如，如果你的 AI 编写的代码期望在环境变量中有一个机密，请在 CI/CD 或云配置中设置该机密，这样就不会意外记录或暴露。使用像[HashiCorp Vault](https://oreil.ly/NqQ-T)（HashiCorp Vault 提供机密管理、密钥管理以及许多集成）或[AWS Secrets Manager](https://oreil.ly/LlYX-)（AWS Secrets Manager 允许你安全地存储和轮换机密，如数据库凭据、API 密钥和令牌，并且可以与 CI/CD 工具如 GitHub 集成）这样的机密管理工具。此外，如果你使用容器镜像，扫描它们以查找漏洞。

使用蓝色-绿色部署或影子测试等技术进行测试

对于重大变更，考虑使用蓝色-绿色部署。这涉及到设置两个完全相同的生产环境：“蓝色”（当前运行版本）和“绿色”（新版本）。初始流量被引导到蓝色环境。一旦绿色环境准备就绪并经过测试，流量就会切换到它。如果绿色环境中出现任何问题，流量可以迅速重新路由回蓝色环境，最小化停机时间和风险。这种方法在将其作为唯一运行版本之前，在完整的生产环境中测试新版本。

或者，如果特定的 AI 编码算法更改有风险或你希望在不对用户产生影响的情况下验证其实际数据的行为，你可以进行影子测试。这涉及到将新版本与当前运行版本一起部署。同时向两个版本提供真实的生产输入。然而，只向用户展示当前版本的输出。收集新（影子）版本的输出并与当前版本的输出进行比较，以评估其性能、准确性和稳定性。如果影子版本的输出令人满意且性能良好，然后你可以自信地将它切换为活动版本。

## 持续最佳实践

部署后，以下策略可以帮助确保一切可靠运行：

创建操作运行手册

为运维团队提供操作手册，描述代码中 AI 生成部分的任何特殊方面：“此服务使用 AI 模型进行 X；如果模型输出看起来有误，尝试重启服务或检查模型的版本。”或者“功能 Y 大量使用缓存以实现良好的性能；如果出现性能问题，请检查缓存命中率。”基本上，记录任何可能不明显的工作考虑因素。如果 AI 引入了依赖（如使用临时文件），请注意这一点，以便运维人员知道要监控磁盘空间等。

在生产中进行测试

除了开发期间和作为部署的一部分进行测试之外，一些公司还会以安全的方式在生产中进行测试（TiP），例如运行连续的小实验。例如，你可能可以使用功能标志为一小部分用户打开 AI 生成的功能，并查看是否有任何错误率的变化。这与金丝雀发布重叠，但你可以使用功能开关使其更加细致。

定期审计

定期对代码库进行安全和性能审计，特别是在更多 AI 贡献积累时。这与管理技术债务类似：它帮助你捕捉到最初可能没问题，但随着规模或环境的变化可能会变得有问题的事情。也要注意“漂移”——如果 AI 代码正在生成 SQL 查询，确保你的迁移和代码保持同步，并且在新的代码开始接收流量之前，部署正确运行迁移。

让人类参与其中

主题继续——人类应该监控自动化。AI 可以帮助你编写代码，但不会在凌晨 2 点修复生产事件。安排一个了解系统的值班人员。随着时间的推移，你可能会让 AI 帮助进行故障排除，例如分析日志（一些新兴工具的功能），但最终，人类应该对修复做出决定。

从失败中学习

没有过程是 100%完美的。如果错误通过了你的防御并导致了事件，进行事后分析。确定问题是否与 AI 使用相关（例如“我们信任这里的 AI 代码，但在场景 X 下失败了”），并更新你的流程和测试以防止这类问题。每次进行这种分析都会持续提高可靠性。

可靠性不仅仅是关于代码，当然；它还涉及代码周围的**基础设施和运维**。AI 主要在代码方面提供帮助。稳健的运维实践（可以部分由 AI 辅助）可以保持整个系统的可靠性。

从本质上讲，在部署方面，将 AI 密集型项目视为任何高质量软件项目一样：进行彻底的测试，逐步推出，严格监控，并确保你可以快速回滚。因为 AI 可以更快地创建变化，你可能会更频繁地部署（如果您的 CI/CD 管道良好，这是可以的）。[频繁的小型部署](https://oreil.ly/ATjYo)实际上[已知可以降低风险](https://oreil.ly/Y5uDn)，与不频繁的大型部署相比。原因是每个单独的变化都更小，这使得识别和修复任何出现的问题更容易。如果出现问题，回滚一个小变化也更简单、更快。这种方法与大型、不频繁的发布形成对比，其中许多变化捆绑在一起，这使得确定任何问题的原因变得困难，并增加了失败部署的潜在影响。

通过遵循这些最佳实践，你可以有信心，尽管其中很大一部分代码是机器生成的，但你的整个系统将可靠地为用户服务。自动化测试、谨慎的部署和监控的组合，可以关闭循环，捕捉到之前阶段遗漏的任何内容。因此，你可以在不牺牲在生产环境中信任你的软件能力的情况下，享受到 AI 开发的速度和生产力优势。

# 摘要和下一步

总结来说，vibe 编码并没有消除对工程严谨性的需求——它放大了那些严谨工程师的生产力。你的座右铭应该是古老的俄罗斯谚语：信任但核实。信任 AI 处理繁琐的工作，但用你的工具和专业知识核实一切。

安全性和可靠性是负责任开发的一个维度；伦理是另一个维度。AI 辅助编码提出了关于知识产权、偏见、对开发者工作的影响等问题。第九章将深入探讨这些更广泛的影响。你如何负责任和公平地使用 AI 编码工具？你如何处理 AI 生成代码的许可，并确保你的模型和提示被道德地使用？
