# 第十一章\. 展望未来

人类历史只有在对数尺度上才有意义。人类花费了无数个时代来了解农业，在那之后的千年发明了书写，再之后的几个世纪发明了蒸汽机，再之后的几十年发明了汽车、计算机和智能手机。在那之后仅仅几年，大约在 2012 年，深度学习出现在了舞台上。

OpenAI 的 GPT-2 在 2019 年宣布，然后 ChatGPT 在 2022 年宣布。这引发了围绕 LLM 的开发爆炸。许多公司都加入了这场混战——Anthropic、Google、Microsoft、Meta、xAI、NVIDIA、Mistral 以及更多——所有这些公司都在构建新的 LLM，这些 LLM 在能力、容量和速度上都超越了之前的版本。仅仅几个月的时间，LLM 就从文档完成引擎变成了聊天引擎，再到可以与外界互动的代理。

读者们，系好安全带。如果你认为现在的变化速度已经很快，那么就再等等，它只会变得更快。（也许那个雷·库兹韦尔家伙真的有所发现！）在本章的最后，让我们展望一下我们眼前的某些发展，以及它们将如何改变你作为提示工程师的工作。

# 多模态

对多模态模型的使用有着巨大的推动力。OpenAI 通过 GPT-4 启动了这一趋势，该模型能够作为提示的一部分处理图像。尽管 OpenAI 没有披露模型具体工作方式的细节，但很可能是它紧密遵循了在[学术文献](https://arxiv.org/abs/2202.10936)中发布的方法。

在这样的方法中，卷积网络被用来将图像特征转换为与用于文本标记的维度相同的嵌入向量。图像向量被赋予了位置信息，以便保留图像中特征之间的关系。然后，将图像和文本向量连接起来。最后，Transformer 架构以与仅处理纯文本的 LLM 相似的方式处理这些信息（参见第二章）。多模态可以天真地扩展到视频输入——你所要做的就是从视频中采样图像，如本[OpenAI 烹饪书](https://oreil.ly/RhK-7)中所示。

随着它们的发展成熟，多模态模型将在无法仅通过文本捕捉的领域变得极其有用。例如，很容易想象这些模型如何被用来使世界对视力受损的人更加易于访问。一个视觉模型可以帮助他们阅读标志、寻找建筑物，以及导航不熟悉的环境。

多模态模型之所以重要，另一个原因是它们为模型提供了访问大量丰富训练数据的机会。在过去的几年里，人们越来越担心我们可能会真正耗尽训练数据！模型足够大，可以学习关于世界的越来越复杂的细节。然而，如果我们用一组过小的数据过度训练，那么这些模型可能会过拟合——实际上是记住文本而不是模拟世界的工作方式。令人惊讶的是，即使是整个公共互联网的文本可能也不足以满足下一代大型模型的需求。

然而，当我们把图像和视频纳入训练时，我们能够接触到大量内容。此外，图像和视频内容携带了一种非常不同的信息类型，这有助于模型更好地理解周围的世界；有了图像的访问权限，模型理解与空间推理、社交线索、物理常识等相关任务应该会变得简单得多。

作为提示工程师，当你构建未来的 LLM 应用时，你可能会在提示中包含图像和视频——尽管它们构成了完全不同的信息形式，但在处理它们时，你可以利用这本书中的一些教训。请记住，只包含与当前对话相关的图像，以免模型分心。用文字为图像设定框架，正确介绍它们在对话中的作用，并利用训练数据中的模式和主题。例如，当互联网上已经有现成的通用格式时，不要引入一种新的图表类型来传达信息。

## 用户体验和用户界面

许多消费应用的 UI 目前正朝着对话交互的方向发展。这合理吗？人类已经相互交谈了 20 万年，但只在屏幕上点击按钮才 40 年。在本节中，我们将关注一个引起我们注意的新对话元素——*文物*，或者我们喜欢称之为*状态化的讨论对象*。

想想看。在日常与人合作中，我们经常谈论一个*事物*——*讨论的对象*。当我们谈论它时，我们可以讨论我们想要如何改变它，我们实际上可以修改它，我们还可以讨论它随时间的变化——这意味着我们可以讨论它的状态。结对编程就是一个很好的例子。文件是讨论的对象，在结对编程过程中，我们可以修改它们并讨论它们是如何变化的。

在当今大多数聊天应用中，助手并没有以有状态的方式处理对话对象。如果你要求 ChatGPT 编写一个函数，然后稍后修改它，它无法回过头来更新函数的内容。相反，它每次都会从头开始重写函数。ChatGPT 不是写一个状态演变的对象，而是将*N*个对象写入对话。

更重要的是，指定你正在谈论的对象很困难，尤其是如果你有多个对象在操作中。你谈论的是哪个函数？哪个版本？这些问题使得与助手一起在某个事物上工作变得困难，而不是仅仅进行一个期望想法飞快而过并超出范围的对话。

当我们完成这本书的时候，Anthropic 引入了*文物*，这代表了向有状态的对话对象迈出的一步。在与 Anthropic 的克劳德对话中，文物*就是*有状态的对话对象。它可以是 SVG 图像、HTML 文件、mermaid 图表、代码或任何其他类型的文本片段。在对话过程中，用户与助手一起修改文物，直到达到用户的期望。虽然来回对话被记录在屏幕左边的记录中，但他们讨论的文物仍然——有状态地——保持在屏幕的右侧（见图 11-1）。如果用户要求修改文物，那么文物的状态将就地更新，而不是在记录中反复重复。

![聊天截图  描述由自动生成](img/pefl_1101.png)

###### 图 11-1\. 与克劳德一起绘制一个戴眼罩的独腿海盗，同时有状态地讨论图像似乎缺少实际腿和眼睛的事实

克劳德的文物范式与我们心中的想法非常接近，但仍有改进的空间。例如，大部分变化只是在于用户界面，而不是在提示工程上。当你要求更改时，克劳德仍然会从头开始重写整个文物；它只是知道将文物放在正确的面板中。这种编辑形式可能不适合较长的文档。

此外，同时与多个文物互动并不容易。克劳德的界面假设一次只处理一个文物。如果你开始谈论不同的文物，那么用户界面会将其视为前一个界面的一个不同版本。多个文物的另一个问题是很难引用它们。如果用户界面和提示都包含项目的缩写名称，那就太好了。

最后，Claude 的界面和提示工程（同样）不允许用户编辑工件。如果你看到一个小问题，你很容易就能修复，唯一解决问题的方法就是告诉助手帮你修复问题（通过重新输入整个文件）。如果用户能够更新工件，并在下一个提示中反映这一更新，以便模型意识到这一变化，那将是一个更好的体验。

当你在自己的 LLM 应用中构建 LLM 界面时，考虑到对话对人类来说非常直观，采用对话界面可能是个不错的选择。但如果是这样的话，你需要投入时间去真正做好它——使用 LLM 可以轻易地制作出基础版本，但如果没有经过深思熟虑和充分完善，它将只会是一个花哨的干扰，而不是真正提供好处的东西。

模型设计者意识到这个需求，并为此进行创新。工具的改进是一个巨大的进步——它们赋予了助手在现实世界中采取行动的能力。类似地，工件也非常有用——它们使得对话可以围绕*事物*（即，有状态的讨论对象）进行。接下来是什么？

对话式用户界面也是让用户保持参与的好方法。正如我们在第八章中讨论的那样，如果让模型自行其是，它们往往会偏离轨道。但在紧密的对话互动中，用户可以及早发现问题，并将助手重新引回正轨。

## 智能化

有没有人注意到 LLM 变得越来越聪明了？是的，而且它们还会继续变得更聪明。让我们看看一些即将到来的发展。

首先，我们在基准测试上变得更聪明了。基准测试是包含已知答案的问题集，它允许我们衡量模型相对于人类和彼此的表现。到目前为止，一些最有用的基准测试已经饱和（见图 11-2），这意味着领先模型往往能轻松通过它们。这使得基准测试在评估模型改进方面变得无用。模型饱和基准测试有两个原因：（1）模型确实变得更聪明了——这是一件好事；（2）模型通过在基准测试上训练来作弊——这是一件非常糟糕的事情。这种“作弊”并非有意为之；只是经过几年后，基准测试的信息在互联网上到处重复（字面或通过描述），并意外地被拉入训练中。

为了解决这个问题，我们这个 AI 社区正在勤奋地升级我们的基准（例如，在[Open LLM Leaderboard 2](https://oreil.ly/zr_z6)上）。我们还开始使用不可记忆的基准，如[ARC-AGI](https://oreil.ly/YTM0M)，它实际上是一套由形状模式组成的心理测量智力测试。它们测试个人或 LLM 理解并再现新模式的能力，由于测试问题属于一个非常大的可能测试空间，它们是算法生成的，你可以总是生成更多。

![不同颜色线条的图表   自动生成的描述](img/pefl_1102.png)

###### 图 11-2\. 流行基准随着时间的推移而饱和，这使得它们作为基准变得无用

我们在模型训练方面也越来越聪明。当你使用 ChatGPT 或其竞争对手时，你可以实际看到这一过程的发生，因为得益于更好的 RLHF 训练（参考第三章），模型在表达它们的思维链推理方面做得更好，这不可避免地导致更有效的响应。

接下来，我们在训练方法上变得更加有创意。例如，大型模型通常没有充分利用它们的全部能力，所以，如果你能想出一种方法将知识传达给小型模型，那么你就可以有效地将大型模型的信息压缩到小型模型中。一种称为*知识蒸馏*的训练方法使用大型模型作为小型模型的“教师”。知识蒸馏不是训练小型模型来预测下一个标记，而是训练小型模型通过预测下一个标记的完整概率集来模仿大型模型。这更丰富的训练数据集允许小型模型快速训练，与它们的教师模型相比，准确度略有下降。作为对准确度下降的补偿，这些小型模型比它们训练的大型模型便宜得多，速度快得多。

除了训练之外，模型改进还将来自架构创新。在这里，我们只列举几个。首先，通过*量化*方法，模型正变得越来越小和更快，在这种方法中，你不需要用 32 位浮点数来表示参数，而是可以用 8 位参数来近似它们，从而大大减少模型大小，相应地降低成本并提高速度。

在你的提示工程工作中，你应该能够预期这些趋势会继续。如果今天某件事太贵，明天就会便宜。如果今天某件事太慢，明天就会更快。如果今天某件事不适合上下文，明天就会适合。如果模型今天不够聪明，明天就会更聪明。然而，始终记住，尽管模型会变得更聪明，但它们永远不会是通灵者。如果提示中没有包含你解决问题所需的信息，那么它可能对模型来说也不充分。

# 结论

如果我们要总结这本书的主要教训，那么会有两个：

1.  LLM 不过是模仿训练期间看到的文本的文本完成引擎。

1.  你应该同情 LLM 并理解它是如何思考的。

关于第一个教训，当我们开始写这本书时，我们所能接触到的唯一模型是完成模型——给他们一部分文档（即*提示*），然后它们会生成合理的文本来完成文档。但是，随后聊天 API 开始主导，工具随之而来，也许，艺术品将是下一个大事件。但即便如此，LLM 的核心仍然是完成文档，以便它们看起来像模型被训练去“喜欢”的其他文档。只是现在，这些文档看起来像聊天记录。

在这里的提示工程教训是要遵循既定的路线（第四章中提到的《小红帽原则》[ch04.html#ch04_designing_llm_applications_1728407230643376]）——让你的提示遵循训练数据中看到的模式和主题，这样你更有可能得到行为良好且易于预测的完成结果。例如，你可以将复杂文本格式化为 Markdown，如果你要传达给 LLM 的信息有标准的文档格式，那么使用该格式比提出一个模型从未见过的格式更有可能成功。

关于第二个教训，即同情，将 LLM 想象成你的一个大而笨拙的机械朋友，碰巧知道互联网的大部分内容。以下是一些帮助你理解的事情：

LLM 很容易分心

不要在提示中填满无用的信息，这些信息——*交叉手指*——可能刚好有帮助。确保每一条信息都至关重要。

LLM 应该能够解读提示

如果你作为一个人类无法理解完全渲染的提示，那么 LLM 同样可能会感到困惑。

LLM 需要被引导

提供明确的指令来完成什么，并在适当的时候提供示例，说明任务应该如何进行。

LLM 不是通灵者

作为提示工程师，你的任务是确保提示包含模型解决问题所需的信息。或者，给模型提供工具和指令来检索它。

LLM 没有内心独白

如果允许 LLM 大声思考问题（在思维链中），那么它找到有用解决方案的过程将容易得多。

希望这本书已经为你提供了跳入提示工程和 LLM 应用开发所需的一切。请放心，我们目前经历的加速变化将继续。由于软件将更容易创建，你会发现更多高度个性化的应用或甚至一次性应用。应用将具有 LLM 的非确定性特征，导致更灵活和开放式的体验。开发将发生变化。你将与人工智能助手协同工作以完成你的工作——如果你还没有这样做的话。

无论世界呈现何种形态，那都将是你塑造的形态。作为一名提示工程师，你手握工具，拥有构建你选择未来的知识和技能。拥抱加速。持续实验。保持灵活。正如已故的特里·普拉切特爵士所说：

> 整个世界都在快速沙上踢踏舞。在这种情况下，奖项授予最佳舞者.^(1))

^(1) 特里·普拉切特，《第五头象》（纽约：道布迪，1999）
