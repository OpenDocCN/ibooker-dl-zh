# 第三章 RAG 第二部分：与数据聊天

在上一章中，你学习了如何处理你的数据，并在向量存储中创建和存储嵌入。在本章中，你将学习如何根据用户的查询高效地检索最相关的嵌入和文档片段。这使你能够构建一个包含相关文档作为上下文的提示，从而提高 LLM 最终输出的准确性。

这个过程——涉及嵌入用户的查询，从数据源检索相似文档，然后将它们作为上下文传递给发送给 LLM 的提示——正式称为 *检索增强生成* (RAG)。

RAG 是构建准确、高效且最新的聊天型 LLM 应用的一个关键组件。在本章中，你将从基础知识进步到高级策略，学习如何为各种数据源（如向量存储和数据库）和数据结构（结构化和非结构化）构建有效的 RAG 系统。

但首先，让我们定义 RAG 并讨论其优势。

# 介绍检索增强生成

RAG 是一种技术，通过提供来自外部来源的上下文来增强 LLM 生成输出的准确性。这个术语最初是由 Meta AI 研究人员在一篇论文中提出的，他们发现 RAG 启用的模型比非 RAG 模型更具有事实性和具体性.^(1)

没有 RAG，LLM 完全依赖于其预训练的数据，这些数据可能已经过时。例如，让我们向 ChatGPT 提出一个关于当前事件的问题，看看它的回答：

*输入*

```py
Which country is the latest winner of the men’s FIFA World Cup?
```

*输出*

```py
The most recent FIFA World Cup winner was France, who won the tournament in 2018.
```

LLM 的响应在事实上是不正确的，并且是过时的。本书出版时最新的赢家是阿根廷，他们在 2022 年赢得了世界杯。虽然这个例子中的问题可能微不足道，但如果 LLM 的答案被用于事实核查或重要决策，LLM 的幻觉可能会产生灾难性的后果。

为了解决这个问题，我们需要向 LLM 提供事实性、最新的信息，以便它可以据此制定准确的响应。继续上一个例子，让我们转到维基百科的[FIFA 世界杯](https://oreil.ly/LpLOV)页面，复制简介段落，然后将它作为 *上下文* 添加到我们的 ChatGPT 提示中：

```py
Which country is the latest winner of the men's FIFA World Cup?

See context below.

The FIFA World Cup, often called the World Cup, is an international association
football competition among the senior men's national teams of the members of
the Fédération Internationale de Football Association (FIFA), the sport's 
global governing body. The tournament has been held every four years since the 
inaugural tournament in 1930, with the exception of 1942 and 1946 due to the 
Second World War. The reigning champions are Argentina, who won their third 
title at the 2022 tournament.
```

注意，最后一句话包含了 LLM 可以使用以提供准确答案的必要上下文。以下是 LLM 的响应：

```py
The latest winner of the men's FIFA World Cup is Argentina, who won their third
title at the 2022 tournament.
```

由于提供了最新的附加上下文，LLM 能够对提示生成准确的响应。但是，将相关信息作为上下文复制粘贴并不适用于生产 AI 应用，也不具备可扩展性。我们需要一个自动化的系统，根据用户的查询获取相关信息，将其作为上下文添加到提示中，然后向 LLM 执行生成请求。

## 检索相关文档

一个 AI 应用的 RAG 系统通常遵循三个核心阶段：

索引

这个阶段涉及预处理外部数据源并将表示数据的嵌入存储在向量存储中，以便可以轻松检索。

检索

这个阶段涉及根据用户的查询检索向量存储中存储的相关嵌入和数据。

生成

这个阶段涉及将检索到的相关文档与原始提示合并为一个最终的提示，并将其发送到模型进行预测。

三个基本阶段看起来像图 3-1。

![文档的示意图  自动生成的描述](img/lelc_0301.png)

###### 图 3-1\. RAG 的关键阶段

在第二章中详细介绍了这个过程的索引阶段，其中你学习了如何使用文档加载器、文本分割器、嵌入和向量存储。

让我们再次从头开始运行一个例子，从索引阶段开始：

*Python*

```py
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_postgres.vectorstores import PGVector

# Load the document, split it into chunks
raw_documents = TextLoader('./test.txt').load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, 
    chunk_overlap=200)
documents = text_splitter.split_documents(raw_documents)

# embed each chunk and insert it into the vector store
model = OpenAIEmbeddings()
connection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'
db = PGVector.from_documents(documents, model, connection=connection)
```

*JavaScript*

```py
import { TextLoader } from "langchain/document_loaders/fs/text";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import { OpenAIEmbeddings } from "@langchain/openai";
import { PGVectorStore } from "@langchain/community/vectorstores/pgvector";

// Load the document, split it into chunks
const loader = new TextLoader("./test.txt");
const raw_docs = await loader.load();
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200,
});
const docs = await splitter.splitDocuments(docs)

// embed each chunk and insert it into the vector store
const model = new OpenAIEmbeddings();
const db = await PGVectorStore.fromDocuments(docs, model, {
  postgresConnectionOptions: {
    connectionString: 'postgresql://langchain:langchain@localhost:6024/langchain'
  }
})
```

第二章中有关于索引阶段的更多细节。

索引阶段现在已完成。为了执行检索阶段，我们需要在用户的查询和存储的嵌入之间执行相似度搜索计算——例如余弦相似度——以检索索引文档中的相关片段（见图 3-2）。

![2024-02-12 上午 1:36:56 的屏幕截图.png](img/lelc_0302.png)

###### 图 3-2\. 从向量存储检索相关文档的同时索引文档的示例流程；HNSW（分层可导航小世界）框表示计算文档与用户查询的相似度

图 3-2 展示了检索过程中的步骤：

1.  将用户的查询转换为嵌入。

1.  计算与用户查询最相似的向量存储中的嵌入。

1.  检索相关文档的嵌入及其对应的文本片段。

我们可以使用 LangChain 以编程方式表示这些步骤如下：

*Python*

```py
# create retriever
retriever `=` db`.`as_retriever()

# fetch relevant documents
docs `=` retriever`.`invoke("""Who are the key figures in the ancient greek 
 history of philosophy?""")
```

*JavaScript*

```py
// create retriever
const retriever = db.asRetriever()

// fetch relevant documents
const docs = await retriever.invoke(`Who are the key figures in the ancient 
 greek history of philosophy?`)
```

注意，我们使用了一个之前未见过的向量存储方法：`as_retriever`。这个函数抽象了将用户的查询嵌入以及向量存储执行的相关相似度搜索计算的底层逻辑。

也有一个参数 `k`，它决定了从向量存储中检索相关文档的数量。例如：

*Python*

```py
# create retriever with k=2
retriever = db.as_retriever(search_kwargs={"k": 2})

# fetch the 2 most relevant documents
docs = retriever.invoke("""Who are the key figures in the ancient greek history 
 of philosophy?""")
```

*JavaScript*

```py
// create retriever with k=2
const retriever = db.asRetriever({k: 2})

// fetch the 2 most relevant documents
const docs = await retriever.invoke(`Who are the key figures in the ancient 
 greek history of philosophy?`)
```

在这个例子中，参数 `k` 被指定为 2。这告诉向量存储根据用户的查询返回两个最相关的文档。

使用低 `k` 值可能看起来不太直观，但检索更多文档并不总是更好的选择。检索的文档越多，你的应用程序运行速度越慢，提示（以及相关的生成成本）越大，检索到包含无关信息的文本片段的可能性也越大，这会导致 LLM 发生幻觉。

现在我们已经完成了 RAG 系统的检索阶段，接下来让我们进入最终的生成阶段。

## 使用相关文档生成 LLM 预测

一旦根据用户的查询检索到相关文档，下一步就是将它们添加到原始提示中作为上下文，然后调用模型生成最终输出 (图 3-3)

![一个图表的图表  描述自动生成](img/lelc_0303.png)

###### 图 3-3\. 一个示例流程，展示了索引文档、从向量存储中检索相关文档，以及将检索到的文档作为 LLM 提示中的上下文

这里是一个代码示例，延续我们之前的例子：

*Python*

```py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

retriever = db.as_retriever()

prompt = ChatPromptTemplate.from_template("""Answer the question based only on 
 the following context:
{context} `Question:` `{question}` ``` `"""``)`  `llm` `=` `ChatOpenAI``(``model_name``=``"gpt-3.5-turbo"``,` `temperature``=``0``)`  `chain` `=` `prompt` `|` `llm`  `# 获取相关文档`  `docs` `=` `retriever``.``get_relevant_documents``(``"""Who are the key figures in the`   `ancient greek history of philosophy?"""``)`  `# 运行` `chain``.``invoke``({``"context"``:` `docs``,``"question"``:` `"""Who are the key figures in the`   `ancient greek history of philosophy?"""``})` ```py
```

```py```` ```py``` *JavaScript*    ``` import {ChatOpenAI} from '@langchain/openai' import {ChatPromptTemplate} from '@langchain/core/prompts'  const retriever = db.asRetriever()  const prompt = ChatPromptTemplate.fromTemplate(`Answer the question based only   on the following context: {context}  Question: {question} `)  const llm = new ChatOpenAI({temperature: 0, modelName: 'gpt-3.5-turbo'})  const chain = prompt.pipe(llm)  // fetch relevant documents const docs = await retriever.invoke(`Who are the key figures in the ancient   greek history of philosophy?`)  await chain.invoke({context: docs, question: `Who are the key figures in the   ancient greek history of philosophy?`}) ```py    注意以下变更：    *   我们将动态的 `context` 和 `question` 变量实现到我们的提示中，这使得我们可以定义一个模型可以使用的 `ChatPromptTemplate` 来生成响应。           *   我们定义了一个 `ChatOpenAI` 接口来作为我们的 LLM。温度设置为 0 以消除模型输出中的创造性。           *   我们创建了一个链来组合提示和 LLM。提醒：`|` 操作符（或 JS 中的 `pipe` 方法）将 `prompt` 的输出用作 `llm` 的输入。           *   我们通过传递 `context` 变量（我们检索到的相关文档）和用户的提问来调用链，以生成最终的输出。              我们可以将这个检索逻辑封装在一个单独的函数中：    *Python*    ``` from langchain_openai import ChatOpenAI from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables import chain  retriever = db.as_retriever()  prompt = ChatPromptTemplate.from_template("""Answer the question based only on   the following context: {context} `Question:` `{question}` ```py `"""``)`  `llm` `=` `ChatOpenAI``(``model``=``"gpt-3.5-turbo"``,` `temperature``=``0``)`  `@chain` `def` `qa``(``input``):`     `# 获取相关文档`      `docs` `=` `retriever``.``get_relevant_documents``(``input``)`     `# 格式化提示`     `formatted` `=` `prompt``.``invoke``({``"context"``:` `docs``,` `"question"``:` `input``})`     `# 生成答案`     `answer` `=` `llm``.``invoke``(``formatted``)`     `return` `answer`  `# 运行` `qa``.``invoke``(``"Who are the key figures in the ancient greek history of philosophy?"``)` ``` ```py   ````` ```py`*JavaScript*    ``` import {ChatOpenAI} from '@langchain/openai' import {ChatPromptTemplate} from '@langchain/core/prompts' import {RunnableLambda} from '@langchain/core/runnables'  const retriever = db.asRetriever()  const prompt = ChatPromptTemplate.fromTemplate(`Answer the question based only   on the following context: {context}  Question: {question} `)  const llm = new ChatOpenAI({temperature: 0, modelName: 'gpt-3.5-turbo'})  const qa = RunnableLambda.from(async input => {   // fetch relevant documents   const docs = await retriever.invoke(input)   // format prompt   const formatted = await prompt.invoke({context: docs, question: input})   // generate answer   const answer = await llm.invoke(formatted)   return answer })  await qa.invoke(`Who are the key figures in the ancient greek history of   philosophy?`) ```py    Notice how we now have a new runnable `qa` function that can be called with just a question and takes care to first fetch the relevant docs for context, format them into the prompt, and finally generate the answer. In the Python code, the `@chain` decorator turns the function into a runnable chain. This notion of encapsulating multiple steps into a single function will be key to building interesting apps with LLMs.    You can also return the retrieved documents for further inspection:    *Python*    ``` @chain def qa(input):     # fetch relevant documents      docs = retriever.get_relevant_documents(input)     # format prompt     formatted = prompt.invoke({"context": docs, "question": input})     # generate answer     answer = llm.invoke(formatted)     return {"answer": answer, "docs": docs} ```py    *JavaScript*    ``` const qa = RunnableLambda.from(async input => {   // fetch relevant documents   const docs = await retriever.invoke(input)   // format prompt   const formatted = await prompt.invoke({context: docs, question: input})   // generate answer   const answer = await llm.invoke(formatted)   return {answer, docs} }) ```py    Congratulations! You’ve now built a basic RAG system to power an AI app for personal use.    However, a production-ready AI app used by multiple users requires a more advanced RAG system. In order to build a robust RAG system, we need to answer the following questions effectively:    *   How do we handle the variability in the quality of a user’s input?           *   How do we route queries to retrieve relevant data from a variety of data sources?           *   How do we transform natural language to the query language of the target data source?           *   How do we optimize our indexing process, i.e., embedding, text splitting?              Next we’ll discuss the latest research-backed strategies to answer these questions and build a production-ready RAG system. These strategies can be summarized in Figure 3-4.  ![A diagram of a software development process  Description automatically generated](img/lelc_0304.png)  ###### Figure 3-4\. Effective strategies to optimize the accuracy of your RAG system    ###### Note    All code blocks in the rest of this chapter use the vector store we set up at the beginning of the chapter.```` ```py`` ``````py ``````py`  ``````py````` ```py```````py # 查询转换    基本 RAG 系统的一个主要问题是它过于依赖用户查询的质量来生成准确的输出。在生产环境中，用户可能以不完整、模糊或措辞不佳的方式构建他们的查询，从而导致模型产生幻觉。    *查询转换* 是一种策略的子集，旨在修改用户的输入以回答第一个 RAG 问题：我们如何处理用户输入质量的变异性？图 3-5 展示了查询转换策略的范围，从那些使用户的输入更抽象或更具体以生成准确的 LLM 输出的策略。下一节从中间策略开始。  ![问题图的描述自动生成](img/lelc_0305.png)  ###### 图 3-5\. 基于抽象级别的各种转换用户查询的方法    ## 重写-检索-阅读    微软研究团队提出的重写-检索-阅读策略简单地提示 LLM 在执行检索之前重写用户的查询。2 为了说明，让我们回到上一节中构建的链，这次使用措辞不佳的用户查询：    *Python*    ``` @chain def qa(input):     # fetch relevant documents      docs = retriever.get_relevant_documents(input)     # format prompt     formatted = prompt.invoke({"context": docs, "question": input})     # generate answer     answer = llm.invoke(formatted)     return answer  qa.invoke("""Today I woke up and brushed my teeth, then I sat down to read the   news. But then I forgot the food on the cooker. Who are some key figures in   the ancient greek history of philosophy?""") ```py    *JavaScript*    ``` const qa = RunnableLambda.from(async input => {   // fetch relevant documents   const docs = await retriever.invoke(input)   // format prompt   const formatted = await prompt.invoke({context: docs, question: input})   // generate answer   const answer = await llm.invoke(formatted)   return answer })  await qa.invoke(`Today I woke up and brushed my teeth, then I sat down to read   the news. But then I forgot the food on the cooker. Who are some key figures   in the ancient greek history of philosophy?`) ```py    *输出*（记住：如果您重新运行它，您的输出可能与这个不同）：    ``` Based on the given context, there is no information provided. ```py    模型未能回答问题，因为它被用户查询中提供的不相关信息所分散。    现在，让我们实现重写-检索-阅读提示：    *Python*    ``` rewrite_prompt = ChatPromptTemplate.from_template("""Provide a better search   query for web search engine to answer the given question, end the queries   with ’**’. Question: {x} Answer:""")  def parse_rewriter_output(message):     return message.content.strip('"').strip("**")  rewriter = rewrite_prompt | llm | parse_rewriter_output  @chain def qa_rrr(input):     # rewrite the query     new_query = rewriter.invoke(input)     # fetch relevant documents      docs = retriever.get_relevant_documents(new_query)     # format prompt     formatted = prompt.invoke({"context": docs, "question": input})     # generate answer     answer = llm.invoke(formatted)     return answer  # run qa_rrr.invoke("""Today I woke up and brushed my teeth, then I sat down to read   the news. But then I forgot the food on the cooker. Who are some key   figures in the ancient greek history of philosophy?""") ```py    *JavaScript*    ``` const rewritePrompt = ChatPromptTemplate.fromTemplate(`Provide a better search   query for web search engine to answer the given question, end the queries   with ’**’. Question: {question} Answer:`)  const rewriter = rewritePrompt.pipe(llm).pipe(message => {   return message.content.replaceAll('"', '').replaceAll('**') })  const qa = RunnableLambda.from(async input => {   const newQuery = await rewriter.invoke({question: input});   // fetch relevant documents   const docs = await retriever.invoke(newQuery)   // format prompt   const formatted = await prompt.invoke({context: docs, question: input})   // generate answer   const answer = await llm.invoke(formatted)   return answer })  await qa.invoke(`Today I woke up and brushed my teeth, then I sat down to read   the news. But then I forgot the food on the cooker. Who are some key   figures in the ancient greek history of philosophy?`) ```py    *输出:*    ``` Based on the given context, some key figures in the ancient greek history of  philosophy include: Themistocles (an Athenian statesman), Pythagoras, and Plato. ```py    注意，我们已经让 LLM 将用户最初的分散查询重写为一个更清晰的查询，并且是那个更集中的查询被传递给检索器以获取最相关的文档。注意：这项技术可以与任何检索方法一起使用，无论是我们这里使用的向量存储，还是例如网络搜索工具。这种方法的一个缺点是它将额外的延迟引入到您的链中，因为我们现在需要连续进行两次 LLM 调用。    ## 多查询检索    单个用户的查询可能不足以捕捉回答查询所需信息的全部范围。多查询检索策略通过指示 LLM 根据用户的初始查询生成多个查询来解决此问题，并行地从数据源检索每个查询，然后将检索到的结果作为提示上下文来生成最终的模型输出。图 3-6 展示了这一点。  ![文档图的描述自动生成，置信度中等](img/lelc_0306.png)  ###### 图 3-6\. 多查询检索策略的演示    这种策略特别适用于需要从多个角度提供全面答案的用例。    下面是一个多查询检索的代码示例：    *Python*    ``` from langchain.prompts import ChatPromptTemplate  perspectives_prompt = ChatPromptTemplate.from_template("""You are an AI language   model assistant. Your task is to generate five different versions of the   given user question to retrieve relevant documents from a vector database.   By generating multiple perspectives on the user question, your goal is to   help the user overcome some of the limitations of the distance-based   similarity search. Provide these alternative questions separated by   newlines. Original question: {question}""")  def parse_queries_output(message):     return message.content.split('\n')  query_gen = perspectives_prompt | llm | parse_queries_output ```py    *JavaScript*    ``` const perspectivesPrompt = ChatPromptTemplate.fromTemplate(`You are an AI   language model assistant. Your task is to generate five different versions   of the given user question to retrieve relevant documents from a vector   database. By generating multiple perspectives on the user question, your   goal is to help the user overcome some of the limitations of the   distance-based similarity search. Provide these alternative questions   separated by newlines. Original question: {question}`)  const queryGen = perspectivesPrompt.pipe(llm).pipe(message => {   return message.content.split('\n') }) ```py    注意，提示模板被设计成根据用户的初始查询生成问题的变体。    接下来，我们获取生成的查询列表，并行检索每个查询的最相关文档，然后将它们组合以获取所有检索到的相关文档的唯一并集：    *Python*    ``` def get_unique_union(document_lists):     # Flatten list of lists, and dedupe them     deduped_docs = {         doc.page_content: doc         for sublist in document_lists for doc in sublist     }     # return a flat list of unique docs     return list(deduped_docs.values())  retrieval_chain = query_gen | retriever.batch | get_unique_union ```py    *JavaScript*    ``` const retrievalChain = queryGen   .pipe(retriever.batch.bind(retriever))   .pipe(documentLists => {     const dedupedDocs = {}     documentLists.flat().forEach(doc => {       dedupedDocs[doc.pageContent] = doc     })     return Object.values(dedupedDocs)   }) ```py    由于我们使用多个（相关）查询从相同的检索器检索文档，因此其中一些很可能重复。在将它们用作回答问题的上下文之前，我们需要去重，以结束每个的唯一实例。在这里，我们通过使用它们的（字符串）内容作为字典（或 JS 中的对象）中的键来去重文档，因为字典只能为每个键包含一个条目。在迭代完所有文档后，我们只需获取所有字典值，现在它们不再有重复。    注意我们同样使用了 `.batch`，它并行运行所有生成的查询，并返回一个结果列表——在这种情况下，一个文档列表的列表，然后我们将其展平并去重，如前所述。    最后一步是构建一个提示，包括用户的问题和组合检索到的相关文档，以及一个模型接口来生成预测：    *Python*    ``` prompt = ChatPromptTemplate.from_template("""Answer the following question based   on this context:  {context} `Question:` `{question}` ```py `"""``)`  `@chain` `def` `multi_query_qa``(``input``):`     `# 获取相关文档`      `docs` `=` `retrieval_chain``.``invoke``(``input``)`     `# 格式化提示`     `formatted` `=` `prompt``.``invoke``({``"context"``:` `docs``,` `"question"``:` `input``})`     `# 生成答案`     `answer` `=` `llm``.``invoke``(``formatted``)`     `return` `answer`  `# 运行` `multi_query_qa``.``invoke``(``"""Who are some key figures in the ancient greek   history of philosophy?"""``)` ``` ```py   ````` ```py`*JavaScript*    ``` const prompt = ChatPromptTemplate.fromTemplate(`Answer the following   question based on this context:  {context}  Question: {question} `)  const multiQueryQa = RunnableLambda.from(async input => {   // fetch relevant documents   const docs = await retrievalChain.invoke(input)   // format prompt   const formatted = await prompt.invoke({context: docs, question: input})   // generate answer   const answer = await llm.invoke(formatted)   return answer })  await multiQueryQa.invoke(`Who are some key figures in the ancient greek   history of philosophy?`) ```py    Notice how this isn’t that different from our previous QA chains, as all the new logic for multi-query retrieval is contained in `retrieval_chain`. This is key to making good use of these techniques—implementing each technique as a standalone chain (in this case, `retrieval_chain`), which makes it easy to adopt them and even to combine them.```` ```py``  ``````py``` ``````py`` ## RAG-Fusion    The RAG-Fusion strategy shares similarities with the multi-query retrieval strategy, except we will apply a final reranking step to all the retrieved documents.^(3) This reranking step makes use of the *reciprocal rank fusion* (RRF) algorithm, which involves combining the ranks of different search results to produce a single, unified ranking. By combining ranks from different queries, we pull the most relevant documents to the top of the final list. RRF is well-suited for combining results from queries that might have different scales or distributions of scores.    Let’s demonstrate RAG-Fusion in code. First, we craft a prompt similar to the multi-query retrieval strategy to generate a list of queries based on the user query:    *Python*    ``` 从 langchain.prompts 导入 ChatPromptTemplate 从 langchain_openai 导入 ChatOpenAI  prompt_rag_fusion = ChatPromptTemplate.from_template("""您是一个有帮助的   助手，根据单个输入   查询生成多个搜索查询。 \n `生成与以下查询相关的多个搜索查询:` `{question}` `\n` ```py `Output (4 queries):"""``)`  `def` `parse_queries_output``(``message``):`     `return` `message``.``content``.``split``(``'``\n``'``)`  `llm` `=` `ChatOpenAI``(``temperature``=``0``)`  `query_gen` `=` `prompt_rag_fusion` `|` `llm` `|` `parse_queries_output` ``` ```py   ``````py` ``````py *JavaScript*    ``` 从 '@langchain/core/prompts' 导入 {ChatPromptTemplate} 从 '@langchain/openai' 导入 {ChatOpenAI} 从 '@langchain/core/runnables' 导入 {RunnableLambda}  const perspectivesPrompt = ChatPromptTemplate.fromTemplate(`您是一个有帮助的   助手，根据单个输入   查询生成多个搜索查询。 \n  生成与以下查询相关的多个搜索查询: {question} \n  输出（4 个查询）:`)  const queryGen = perspectivesPrompt.pipe(llm).pipe(message => {   return message.content.split('\n') }) ```py    Once we’ve generated our queries, we fetch relevant documents for each query and pass them into a function to *rerank* (that is, *reorder* according to relevancy) the final list of relevant documents.    The function `reciprocal_rank_fusion` takes a list of the search results of each query, so a list of lists of documents, where each inner list of documents is sorted by their relevance to that query. The RRF algorithm then calculates a new score for each document based on its ranks (or positions) in the different lists and sorts them to create a final reranked list.    After calculating the fused scores, the function sorts the documents in descending order of these scores to get the final reranked list, which is then returned:    *Python*    ``` def reciprocal_rank_fusion(results: list[list], k=60):     """在多个排序文档列表上执行倒数排名融合，并可选参数 k 用于 RRF 公式  """          # 初始化一个字典来保存每个文档的融合分数     # 文档将通过其内容作为键来确保唯一性     fused_scores = {}     documents = {}      # 遍历每个排序文档列表     for docs in results:         # 遍历列表中的每个文档及其排名（列表中的位置）         for rank, doc in enumerate(docs):             # 使用文档内容作为唯一性的键             doc_str = doc.page_content             # 如果文档尚未被看到，             # - 初始化分数为 0             # - 保存它以供以后使用             if doc_str not in fused_scores:                 fused_scores[doc_str] = 0                 documents[doc_str] = doc             # 使用 RRF 公式更新文档的分数：             # 1 / (rank + k)             fused_scores[doc_str] += 1 / (rank + k)      # 根据文档的融合分数按降序排序文档，以获取最终的重新排序结果     reranked_doc_strs = sorted(         fused_scores, key=lambda d: fused_scores[d], reverse=True     )     # 获取每个 doc_str 对应的文档     return [         documents[doc_str]         for doc_str in reranked_doc_strs     ]  retrieval_chain = generate_queries | retriever.batch | reciprocal_rank_fusion ```py    *JavaScript*    ``` function reciprocalRankFusion(results, k = 60) {   // 初始化一个字典来保存每个文档的融合分数   // 文档将通过其内容作为键来确保唯一性   const fusedScores = {}   const documents = {}    results.forEach(docs => {     docs.forEach((doc, rank) => {       // 使用文档内容作为唯一性的键       const key = doc.pageContent       // 如果文档尚未被看到，       // - 初始化分数为 0       // - 保存它以供以后使用       if (!(key in fusedScores)) {         fusedScores[key] = 0         documents[key] = 0       }       // 使用 RRF 公式更新文档的分数：       // 1 / (rank + k)       fusedScores[key] += 1 / (rank + k)     })   })    // 根据文档的融合分数按降序排序文档，以获取最终的重新排序结果   const sorted = Object.entries(fusedScores).sort((a, b) => b[1] - a[1])   // 获取每个键对应的文档   return sorted.map(([key]) => documents[key]) }  const retrievalChain = queryGen   .pipe(retriever.batch.bind(retriever))   .pipe(reciprocalRankFusion) ```py    Notice that the function also takes a `k` parameter, which determines how much influence documents in each query’s result sets have over the final list of documents. A higher value indicates that lower-ranked documents have more influence.    Finally, we combine our new retrieval chain (now using RRF) with the full chain we’ve seen before:    *Python*    ``` prompt = ChatPromptTemplate.from_template("""根据以下上下文回答以下问题：   {context} `问题:` `{question}` ```py `"""``)`  `llm` `=` `ChatOpenAI``(``temperature``=``0``)`  `@chain` `def` `multi_query_qa``(``input``):`     `# fetch relevant documents`      `docs` `=` `retrieval_chain``.``invoke``(``input``)`     `# format prompt`     `formatted` `=` `prompt``.``invoke``({``"context"``:` `docs``,` `"question"``:` `input``})`     `# generate answer`     `answer` `=` `llm``.``invoke``(``formatted``)`     `return` `answer`  `multi_query_qa``.``invoke``(``"""Who are some key figures in the ancient greek history`   `of philosophy?"""``)` ``` ```py   ````` ```py`*JavaScript*    ``` const rewritePrompt = ChatPromptTemplate.fromTemplate(`根据以下上下文回答以下问题：   {context}  问题: {question} `)  const llm = new ChatOpenAI({temperature: 0})  const multiQueryQa = RunnableLambda.from(async input => {   // 从之前定义的 hyde 检索链中获取相关文档   const docs = await retrievalChain.invoke(input)   // 格式化提示   const formatted = await prompt.invoke({context: docs, question: input})   // 生成答案   const answer = await llm.invoke(formatted)   return answer })  await multiQueryQa.invoke(`Who are some key figures in the ancient greek   history of philosophy?`) ```py    RAG-Fusion’s strength lies in its ability to capture the user’s intended expression, navigate complex queries, and broaden the scope of retrieved documents, enabling serendipitous discovery.```` ```py`` ``````py ``````py`  ``````py` ``````py ## Hypothetical Document Embeddings    *Hypothetical Document Embeddings* (HyDE) is a strategy that involves creating a hypothetical document based on the user’s query, embedding the document, and retrieving relevant documents based on vector similarity.^(4) The intuition behind HyDE is that an LLM-generated hypothetical document will be more similar to the most relevant documents than the original query, as shown in Figure 3-7.  ![Screenshot 2024-02-12 at 1.12.45 PM.png](img/lelc_0307.png)  ###### Figure 3-7\. An illustration of HyDE closer in the vector space to the document embeddings than the plain query embeddings    First, define a prompt to generate a hypothetical document:    *Python*    ``` 从 langchain.prompts 导入 ChatPromptTemplate 从 langchain_core.output_parsers 导入 StrOutputParser 从 langchain_openai 导入 ChatOpenAI  prompt_hyde = ChatPromptTemplate.from_template("""请写一段话来回答以下问题。   \n 问题: {question} \n 文章:""")  generate_doc = (     prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser()  ) ```py    *JavaScript*    ``` 从 '@langchain/openai' 导入 {ChatOpenAI} 从 '@langchain/core/prompts' 导入 {ChatPromptTemplate} 从 '@langchain/core/runnables' 导入 {RunnableLambda}  const prompt = ChatPromptTemplate.fromTemplate(`请写一段话来回答以下问题   问题: {question} 文章:`)  const llm = new ChatOpenAI({temperature: 0})  const generateDoc = prompt.pipe(llm).pipe(msg => msg.content) ```py    Next, we take the hypothetical document and use it as input to the `retriever`, which will generate its embedding and search for similar documents in the vector store:    *Python*    ``` retrieval_chain `=` generate_doc `|` retriever  ```py    *JavaScript*    ``` const retrievalChain = generateDoc.pipe(retriever) ```py    Finally, we take the retrieved documents, pass them as context to the final prompt, and instruct the model to generate an output:    *Python*    ``` prompt = ChatPromptTemplate.from_template("""根据以下上下文回答以下问题：   {context} `问题:` `{question}` ```py `"""``)`  `llm` `=` `ChatOpenAI``(``temperature``=``0``)`  `@chain` `def` `qa``(``input``):`   `# fetch relevant documents from the hyde retrieval chain defined earlier`   `docs` `=` `retrieval_chain``.``invoke``(``input``)`   `# format prompt`   `formatted` `=` `prompt``.``invoke``({``"context"``:` `docs``,` `"question"``:` `input``})`   `# generate answer`   `answer` `=` `llm``.``invoke``(``formatted``)`   `return` `answer`  `qa``.``invoke``(``"""Who are some key figures in the ancient greek history of`   `philosophy?"""``)` ``` ```py   ````` ```py`*JavaScript*    ``` const prompt = ChatPromptTemplate.fromTemplate(`根据以下上下文回答以下问题：   {context}  问题: {question} `)  const llm = new ChatOpenAI({temperature: 0})  const qa = RunnableLambda.from(async input => {   // 从之前定义的 hyde 检索链中获取相关文档   const docs = await retrievalChain.invoke(input)   // 格式化提示   const formatted = await prompt.invoke({context: docs, question: input})   // 生成答案   const answer = await llm.invoke(formatted)   return answer })  await qa.invoke(`Who are some key figures in the ancient greek history of   philosophy?`) ```py    To recap what we covered in this section, query transformation consists of taking the user’s original query and doing the following:    *   Rewriting into one or more queries           *   Combining the results of those queries into a single set of the most relevant results              Rewriting the query can take many forms, but it’s usually done in a similar fashion: take the user’s original query—a prompt you wrote—and ask an LLM to write a new query or queries. Some examples of typical changes made are:    *   Removing irrelevant/unrelated text from the query.           *   Grounding the query with past conversation history. For instance, to make sense of a query such as *and what about in LA,* we need to combine it with a hypothetical past question about the weather in SF, to arrive at a useful query such as *weather in LA*.           *   Casting a wider net for relevant documents by also fetching documents for related queries.           *   Decomposing a complex question into multiple, simpler questions and then including results for all of them in the final prompt to generate an answer.              The right rewriting strategy to use will depend on your use case.    Now that we’ve covered the main query transformation strategies, let’s discuss the second major question to answer in order to build a robust RAG system: How do we route queries to retrieve relevant data from multiple data sources?```` ```py`` ``````py ``````py` ``````py`` ``````py```  ````` ```py`# Query Routing    Although using a single vector store is useful, the required data may live in a variety of data sources, including relational databases or other vector stores.    For example, you may have two vector stores: one for LangChain Python documentation and another for LangChain JS documentation. Given a user’s question, we would like to *route* the query to the appropriate inferred data source to retrieve relevant docs. *Query routing* is a strategy used to forward a user’s query to the relevant data source.    ## Logical Routing    In *logical routing*, we give the LLM knowledge of the various data sources at our disposal and then let the LLM reason which data source to apply based on the user’s query, as shown in Figure 3-8.  ![A diagram of a brain  Description automatically generated](img/lelc_0308.png)  ###### Figure 3-8\. Query routing to relevant data sources    In order to achieve this, we make use of function-calling models like GPT-3.5 Turbo to help classify each query into one of the available routes. A *function call* involves defining a schema that the model can use to generate arguments of a function based on the query. This enables us to generate structured outputs that can be used to run other functions. The following Python code defines the schema for our router based on three docs for different languages:    *Python*    ``` from typing import Literal  from langchain_core.prompts import ChatPromptTemplate from langchain_core.pydantic_v1 import BaseModel, Field from langchain_openai import ChatOpenAI  # Data model class RouteQuery(BaseModel):     """Route a user query to the most relevant datasource."""      datasource: Literal["python_docs", "js_docs"] = Field(         ...,         description="""Given a user question, choose which datasource would be   most relevant for answering their question""",     )  # LLM with function call  llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0) structured_llm = llm.with_structured_output(RouteQuery)  # Prompt  system = """You are an expert at routing a user question to the appropriate data   source.  Based on the programming language the question is referring to, route it to the   relevant data source."""  prompt = ChatPromptTemplate.from_messages(     [         ("system", system),         ("human", "{question}"),     ] )  # Define router  router `=` prompt `|` structured_llm ```py    *JavaScript*    ``` import { ChatOpenAI } from "@langchain/openai"; import { z } from "zod";  const routeQuery = z.object({   datasource: z.enum(["python_docs", "js_docs"]).describe(`Given a user   question, choose which datasource would be most relevant for answering   their question`), }).describe("Route a user query to the most relevant datasource.")  const llm = new ChatOpenAI({model: "gpt-3.5-turbo", temperature: 0}) const structuredLlm = llm.withStructuredOutput(routeQuery, {name: "RouteQuery"})  const prompt = ChatPromptTemplate.fromMessages([   ['system', `You are an expert at routing a user question to the appropriate   data source.  Based on the programming language the question is referring to, route it to   the relevant data source.`],   ['human', '{question}'] ])  const router = prompt.pipe(structuredLlm) ```py    Now we invoke the LLM to extract the data source based on the predefined schema:    *Python*    ``` question = """Why doesn't the following code work:  from langchain_core.prompts import ChatPromptTemplate  prompt = ChatPromptTemplate.from_messages(["human", "speak in {language}"]) prompt.invoke("french") """  result = router.invoke({"question": question})  result.datasource # "python_docs" ```py    *JavaScript*    ``` const question = `Why doesn't the following code work:  from langchain_core.prompts import ChatPromptTemplate  prompt = ChatPromptTemplate.from_messages(["human", "speak in {language}"]) prompt.invoke("french") `  await router.invoke({ question }) ```py    *The output:*    ``` {     datasource: "python_docs" } ```py    Notice how the LLM produced JSON output, conforming to the schema we defined earlier. This will be useful in many other tasks.    Once we’ve extracted the relevant data source, we can pass the value into another function to execute additional logic as required:    *Python*    ``` def choose_route(result):     if "python_docs" in result.datasource.lower():         ### Logic here          return "chain for python_docs"     else:         ### Logic here          return "chain for js_docs"  full_chain = router | RunnableLambda(choose_route) ```py    *JavaScript*    ``` function chooseRoute(result) {   if (result.datasource.toLowerCase().includes('python_docs')) {     return 'chain for python_docs';   } else {     return 'chain for js_docs';   } }   const fullChain = router.pipe(chooseRoute)  ```py    Notice how we don’t do an exact string comparison but instead first turn the generated output to lowercase, and then do a substring match. This makes our chain more resilient to the LLM going off script and producing output that doesn’t quite conform to the schema we asked for.    ###### Tip    Resilience to the random nature of LLM outputs is an important theme to keep in mind when building your LLM applications.    Logical routing is most suitable when you have a defined list of data sources from which relevant data can be retrieved and utilized by the LLM to generate an accurate output. These can range from vector stores to databases and even APIs.    ## Semantic Routing    Unlike logical routing, *semantic routing* involves embedding various prompts that represent various data sources alongside the user’s query and then performing vector similarity search to retrieve the most similar prompt. Figure 3-9 illustrates.  ![A diagram of a diagram of a notepad and a brain  Description automatically generated](img/lelc_0309.png)  ###### Figure 3-9\. Semantic routing to improve the accuracy of retrieved documents    The following is an example of semantic routing:    *Python*    ``` from langchain.utils.math import cosine_similarity from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import PromptTemplate from langchain_core.runnables import chain from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # Two prompts physics_template = """You are a very smart physics professor. You are great at   answering questions about physics in a concise and easy-to-understand manner.   When you don't know the answer to a question, you admit that you don't know.  Here is a question: {query}"""  math_template = """You are a very good mathematician. You are great at answering   math questions. You are so good because you are able to break down hard   problems into their component parts, answer the component parts, and then   put them together to answer the broader question.  Here is a question: {query}"""  # Embed prompts embeddings = OpenAIEmbeddings() prompt_templates = [physics_template, math_template] prompt_embeddings = embeddings.embed_documents(prompt_templates)  # Route question to prompt @chain def prompt_router(query):     # Embed question     query_embedding = embeddings.embed_query(query)     # Compute similarity     similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]     # Pick the prompt most similar to the input question     most_similar = prompt_templates[similarity.argmax()]     return PromptTemplate.from_template(most_similar)  semantic_router = (     prompt_router     | ChatOpenAI()     | StrOutputParser() )  print(semantic_router.invoke("What's a black hole")) ```py    *JavaScript*    ``` import {cosineSimilarity} from '@langchain/core/utils/math' import {ChatOpenAI, OpenAIEmbeddings} from '@langchain/openai' import {PromptTemplate} from '@langchain/core/prompts' import {RunnableLambda} from '@langchain/core/runnables';  const physicsTemplate = `You are a very smart physics professor. You are great   at answering questions about physics in a concise and easy-to-understand   manner. When you don't know the answer to a question, you admit that you   don't know.  Here is a question: {query}`  const mathTemplate = `You are a very good mathematician. You are great at   answering math questions. You are so good because you are able to break down   hard problems into their component parts, answer the component parts, and   then put them together to answer the broader question.  Here is a question: {query}`  const embeddings = new OpenAIEmbeddings()  const promptTemplates = [physicsTemplate, mathTemplate] const promptEmbeddings = await embeddings.embedDocuments(promptTemplates)  const promptRouter = RunnableLambda.from(query => {   // Embed question   const queryEmbedding = await embeddings.embedQuery(query)   // Compute similarity   const similarities = cosineSimilarity([queryEmbedding], promptEmbeddings)[0]   // Pick the prompt most similar to the input question   const mostSimilar = similarities[0] > similarities[1]      ? promptTemplates[0]      : promptTemplates[1]   return PromptTemplate.fromTemplate(mostSimilar) })   const semanticRouter = promptRouter.pipe(new ChatOpenAI())  await semanticRouter.invoke("What's a black hole") ```py    Now that you’ve seen how to route a user’s query to the relevant data source, let’s discuss the third major question when building a robust RAG system: “How do we transform natural language to the query language of the target data source?”    # Query Construction    As discussed earlier, RAG is an effective strategy to embed and retrieve relevant unstructured data from a vector store based on a query. But most data available for use in production apps is structured and typically stored in relational databases. In addition, unstructured data embedded in a vector store also contains structured metadata that possesses important information.    *Query construction* is the process of transforming a natural language query into the query language of the database or data source you are interacting with. See Figure 3-10.  ![A diagram of data types  Description automatically generated](img/lelc_0310.png)  ###### Figure 3-10\. Illustration of query languages for various data sources    For example, consider the query: *What* *are movies about aliens in the year 1980?* This question contains an unstructured topic that can be retrieved via embeddings (*aliens*), but it also contains potential structured components (*year == 1980*).    The following sections dive deeper into the various forms of query construction.    ## Text-to-Metadata Filter    Most vector stores provide the ability to limit your vector search based on metadata. During the embedding process, we can attach metadata key-value pairs to vectors in an index and then later specify filter expressions when you query the index.    LangChain provides a `SelfQueryRetriever` that abstracts this logic and makes it easier to translate natural language queries into structured queries for various data sources. The self-querying utilizes an LLM to extract and execute the relevant metadata filters based on a user’s query and predefined metadata schema:    *Python*    ``` from langchain.chains.query_constructor.base import AttributeInfo from langchain.retrievers.self_query.base import SelfQueryRetriever from langchain_openai import ChatOpenAI  fields = [     AttributeInfo(         name="genre",         description="The genre of the movie",         type="string or list[string]",     ),     AttributeInfo(         name="year",         description="The year the movie was released",         type="integer",     ),     AttributeInfo(         name="director",         description="The name of the movie director",         type="string",     ),     AttributeInfo(         name="rating", description="A 1-10 rating for the movie", type="float"     ), ] description = "Brief summary of a movie"  llm = ChatOpenAI(temperature=0)  retriever = SelfQueryRetriever.from_llm(     llm, db, description, fields, )  print(retriever.invoke(     "What's a highly rated (above 8.5) science fiction film?")) ```py    *JavaScript*    ``` import { ChatOpenAI } from "@langchain/openai"; import { SelfQueryRetriever } from "langchain/retrievers/self_query"; import { FunctionalTranslator } from "@langchain/core/structured_query";  /**  * First, we define the attributes we want to be able to query on.  * in this case, we want to be able to query on the genre, year, director,   * rating, and length of the movie.  * We also provide a description of each attribute and the type of the attribute.  * This is used to generate the query prompts.  */ const fields = [   {     name: "genre",     description: "The genre of the movie",     type: "string or array of strings",   },   {     name: "year",     description: "The year the movie was released",     type: "number",   },   {     name: "director",     description: "The director of the movie",     type: "string",   },   {     name: "rating",     description: "The rating of the movie (1-10)",     type: "number",   },   {     name: "length",     description: "The length of the movie in minutes",     type: "number",   }, ]; const description = "Brief summary of a movie";  const llm = new ChatOpenAI(); const attributeInfos = fields.map((field) => new AttributeInfo(field.name,     field.description, field.type));    const selfQueryRetriever = SelfQueryRetriever.fromLLM({   llm,   db,   description,   attributeInfo: attributeInfos,   /**  * We need to use a translator that translates the queries into a  * filter format that the vector store can understand. LangChain provides one   * here.  */   structuredQueryTranslator: new FunctionalTranslator(), });  await selfQueryRetriever.invoke(   "What's a highly rated (above 8.5) science fiction film?" ); ```py    This results in a retriever that will take a user query, and split it into:    *   A filter to apply on the metadata of each document first           *   A query to use for semantic search on the documents              To do this, we have to describe which fields the metadata of our documents contain; that description will be included in the prompt. The retriever will then do the following:    1.  Send the query generation prompt to the LLM.           2.  Parse metadata filter and rewritten search query from the LLM output.           3.  Convert the metadata filter generated by the LLM to the format appropriate for our vector store.           4.  Issue a similarity search against the vector store, filtered to only match documents whose metadata passes the generated filter.              ## Text-to-SQL    SQL and relational databases are important sources of structured data, but they don’t interact directly with natural language. Although we can simply use the LLM to translate a user’s query to SQL queries, there is little margin for error.    Here are some useful strategies for effective text to SQL translations:    Database description      To ground SQL queries, an LLM must be provided with an accurate description of the database. One common text-to-SQL prompt employs an idea reported in this paper and others: provide the LLM with a `CREATE TABLE` description for each table, including column names and types.^(5) We can also provide a few (for instance, three) example rows from the table.      Few-shot examples      Feeding the prompt with few-shot examples of question-query matches can improve the query generation accuracy. This can be achieved by simply appending standard static examples in the prompt to guide the agent on how it should build queries based on questions.      See Figure 3-11 for a visual of the process.  ![A close-up of a machine  Description automatically generated](img/lelc_0311.png)  ###### Figure 3-11\. A user’s query transformed to a SQL query    Here’s a full code example:    *Python*    ``` from langchain_community.tools import QuerySQLDatabaseTool from langchain_community.utilities import SQLDatabase from langchain.chains import create_sql_query_chain from langchain_openai import ChatOpenAI  # replace this with the connection details of your db db = SQLDatabase.from_uri("sqlite:///Chinook.db") llm = ChatOpenAI(model="gpt-4", temperature=0)  # convert question to sql query write_query = create_sql_query_chain(llm, db)  # Execute SQL query execute_query = QuerySQLDatabaseTool(db=db)  # combined chain = write_query | execute_query  # invoke the chain chain.invoke('How many employees are there?'); ```py    *JavaScript*    ``` import { ChatOpenAI } from "@langchain/openai"; import { createSqlQueryChain } from "langchain/chains/sql_db"; import { SqlDatabase } from "langchain/sql_db"; import { DataSource } from "typeorm"; import { QuerySqlTool } from "langchain/tools/sql";  const datasource = new DataSource({   type: "sqlite",   database: "./Chinook.db", // here should be the details of your database }); const db = await SqlDatabase.fromDataSourceParams({   appDataSource: datasource, }); const llm = new ChatOpenAI({ model: "gpt-4", temperature: 0 });  // convert question to sql query const writeQuery = await createSqlQueryChain({ llm, db, dialect: "sqlite" });  // execute query const executeQuery = new QuerySqlTool(db);  // combined const chain = writeQuery.pipe(executeQuery);  // invoke the chain await chain.invoke('How many employees are there?'); ```py    We first convert the user’s query to a SQL query appropriate to the dialect of our database. Then we execute that query on our database. Note that executing arbitrary SQL queries on your database generated by an LLM from user input is dangerous in a production application. To use these ideas in production, you need to consider a number of security measures to reduce the risk of unintended queries being run in your database. Here are some examples:    *   Run the queries on your database with a user with read-only permissions.           *   The database user running the queries should have access only to the tables you wish to make available for querying.           *   Add a time-out to the queries run by this application; this would ensure that even if an expensive query is generated, it is canceled before taking up too many of your database resources.              This is not an exhaustive list of security considerations. Security of LLM applications is an area currently in development, with more security measures being added to the recommendations as new vulnerabilities are discovered.    # Summary    This chapter discussed various state-of-the-art strategies to efficiently retrieve the most relevant documents based on a user’s query and synthesize them with your prompt to aid the LLM to generate an accurate, up-to-date output.    As discussed, a robust, production-ready RAG system requires a wide range of effective strategies that can execute query transformation, query construction, routing, and indexing optimization.    Query transformation enables your AI app to transform an ambiguous or malformed user query into a representative query that’s optimal for retrieval. Query construction enables your AI app to convert the user’s query into the syntax of the query language of the database or data source where structured data lives. Routing enables your AI app to dynamically route the user’s query to retrieve relevant information from the relevant data source.    In Chapter 4, we’ll build on this knowledge to add memory to your AI chatbot so that it can remember and learn from each interaction. This will enable users to “chat” with the application in multiturn conversations like ChatGPT.    ^(1) Patrick Lewis et al., [“Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”](https://oreil.ly/Qzd2K), arXiv, April 12, 2021\.    ^(2) Xinbei Ma et al., [“Query Rewriting for Retrieval-Augmented Large Language Models”](https://oreil.ly/zyw5E), arXiv, October 23, 2023\. Research commissioned by Microsoft Research Asia.    ^(3) Zackary Rackauckas, [“RAG-Fusion: A New Take on Retrieval-Augmented Generation”](https://oreil.ly/k7TTY), arXiv, February 21, 2024\. From the *International Journal on Natural Language Computing*, vol. 13, no. 1 (February 2024).    ^(4) Luyu Gao et al., [“Precise Zero-Shot Dense Retrieval Without Relevance Labels”](https://oreil.ly/7aTnS), arXiv, December 20, 2022\.    ^(5) Nitarshan Rajkumar et al., [“Evaluating the Text-to-SQL Capabilities of Large Language Models”](https://oreil.ly/WOrzt), arXiv, March 15, 2022\.```` ```py`` ``````py```` ```py```````
