# 第八章\. 负责任的人工智能框架

2014 年，微软在中国推出了 Xiaoice，这是一个由人工智能驱动的聊天机器人，成功吸引了超过 4000 万用户。一年后，微软在 Twitter 上发布了一个名为 Tay 的版本。这次发布是一场灾难。Tay 很快就开始发表种族主义和敌对言论，迫使微软在 24 小时内将其关闭。

这起事件发生是因为微软对其在中国市场的经验有一种错误的安全感，那里有更严格的内容限制，并且低估了 Twitter 的自由放任性质，用户们积极尝试操纵这个机器人。在一篇博客文章中，一位微软高管承认公司学到了宝贵的教训，表示人工智能的挑战“同样多的是社会性的，也是技术性的”，并且在公共论坛上迭代时需要谨慎。

这一事件促使微软为其负责任人工智能制定了自己的原则——一个在安全、值得信赖和道德的方式下开发和部署人工智能系统的框架。随着人工智能越来越多地融入关键领域，如医疗保健和刑事司法，这种对责任的关注从未如此重要，并且是 AIF-C01 考试的关键部分。

# 生成式人工智能的风险

尽管生成式人工智能可能非常强大，但其能力也引入了广泛的风险——有些微妙，有些破坏性极强。从产生有毒内容到侵犯知识产权和导致就业岗位流失，这些风险可能对社会、伦理和法律产生严重后果。这就是为什么负责任人工智能的概念如此重要的原因。它作为识别、减轻和管理这些挑战的基础方法，从一开始就是必不可少的。我们将探讨与生成式人工智能相关的几个关键风险领域——每个领域都说明了为什么负责任框架不是可选择的，而是必需的。然后，我们将继续了解负责任人工智能的核心要素以及它们如何作为解决这些风险的实用工具。

而不是将这些问题视为孤立的技术缺陷，负责任的人工智能强调一种整体策略：确保人工智能系统在构建和使用过程中的公平性、透明度、安全性和人为监督。在接下来的几节中，我们将探讨与生成式人工智能相关的几个关键风险领域——每个领域都说明了为什么负责任框架不是可选择的，而是必需的。然后，我们将继续了解负责任人工智能的核心要素以及它们如何作为解决这些风险的实用工具。

## 毒性

管理毒性是负责任人工智能的一个核心关注点。如果生成式人工智能系统产生冒犯性、有害或不适当的内容，它可能会损害信任、损害品牌声誉，甚至造成现实世界的伤害。确保负责任的人工智能意味着通过深思熟虑的设计、过滤机制和持续的监督来实施保障措施，以最大限度地减少这些风险。

但是，毒性问题有一个主要问题：它非常主观。对某个人可能冒犯的内容，对另一个人来说可能完全没问题。还有与年龄相关的考虑以及文化之间的差异。

因此，对于一个 AI 开发者来说，开发正确的过滤器可能极其困难。不可避免地，似乎不可能不冒犯到某个人。

另一个问题在于识别毒性可能具有挑战性。因为生成式 AI 系统基于复杂的概率系统，内容可能包含一些攻击性的细微差别或微妙之处——这些可能不会被过滤器捕捉到。

事实上，一种方法是为生成式 AI 模型进行人工数据管理。但这可能耗时且远非完美。

## 知识产权

尊重知识产权（IP）是负责任 AI 的基石。未能适当归属、许可或保护创意作品的生成式 AI 系统可能会破坏行业、违反法律保护，并侵蚀公众信任。对 AI 开发的负责任方法意味着在版权、所有权和公平补偿方面采取主动。

2023 年好莱坞编剧罢工凸显了这些担忧，揭示了创意专业人士与 AI 生成内容快速崛起之间的日益紧张关系。争议的核心是担心生成式 AI 可能在不充分补偿或认可的情况下复制作家的作品。^([1)] 美国编剧工会（WGA）成功谈判了确保 AI 生成的内容不能取代人类作家，以及任何在写作过程中使用的 AI 辅助仍需为涉及的人类作家提供全额信用和补偿的条款。这无疑是历史性的——AI 对社会影响的又一例证。

但在生成式 AI 领域，自 ChatGPT 发布之初，知识产权问题就一直是关键。在几个月内，就已经出现了各种诉讼。例如，《纽约时报》对 OpenAI 和微软提起了诉讼，指控他们未经许可使用了报纸文章。

生成式 AI 和知识产权的法律问题复杂，可能需要数年才能解决。最终，它们可能由最高法院裁决。

同时，AI 开发者正在寻找解决这些问题的方法。一种方法是与内容提供商签订许可协议。

另一种处理知识产权问题的方法是提供赔偿保护。这是指 AI 开发者将捍卫并承担诉讼的法律费用。提供这种保护的一些公司包括 OpenAI、微软和 Adobe。

## 抄袭和作弊

促进学术诚信是负责任 AI 的一个基本组成部分。虽然生成式 AI 可以作为强大的教育工具，但必须部署有约束性的措施来阻止滥用——如抄袭。负责任 AI 意味着在学习和工作环境中培养透明度、责任感和道德行为。

类似 ChatGPT 的生成式 AI 工具为学生提供了探索学术主题、提出复杂问题以及获得个性化、按需帮助的能力。这有可能在重大方面提升学习效果。

然而，同样的技术也可以被用来规避真正的努力——撰写论文、完成作业或回答考试问题。这引发了关于公平和学习成果的严重担忧。

教育机构的反应各不相同。一些已经实施了禁令或对其使用施加了限制。在其他情况下，方法是通过推广生成 AI 的使用并将其纳入课程，例如学习如何更好地利用这项技术。

已有尝试检测 AI 生成的内容。但这证明极其困难。这些工具给出假阳性结果并不罕见。此外，学生可以创造性地规避检测。例如，他们可能会重写部分内容。这甚至可以使用 AI 工具来完成！

## 工作性质的改变

对于负责任的 AI 来说，最紧迫的挑战之一是其可能对全球劳动力市场造成破坏。随着 AI 系统能够执行以前只有高技能专业人士才能完成的复杂任务，负责任的 AI 不仅必须考虑安全和公平，还要考虑长期的经济和社会影响。这包括规划劳动力转型、支持工作增强和培养包容性创新。

1930 年，传奇经济学家约翰·梅纳德·凯恩斯撰写了一篇关于技术将取代大量工作的论文。他称之为“技术性失业”。

在过去几十年中，他的预测往往不准确。通常，新技术会导致更多的就业机会。但今天，凯恩斯的担忧似乎更加现实。事实上，生成式 AI 已经在软件开发、金融服务和法律等复杂、知识密集型领域有效地参与其中。

例如，高盛的研究预测，生成式 AI 可能[在全球自动化约 3 亿个全职工作](https://oreil.ly/M1Pxu)，并且大约三分之二的美国工作可能受到 AI 自动化的威胁。

麦肯锡全球研究院发布了一份报告。它预测到 2030 年，美国和欧洲目前 30%的工作时间可能被自动化([30% of hours currently worked in the US and Europe could be automated](https://oreil.ly/e6ZQh))。

毫无疑问，这远非鼓舞人心。如果这些预测最终成为现实，可能会造成重大的经济和社会动荡。

这确实强调了发展和实施负责任的 AI 实践的重要性。这可能意味着考虑重新培训和再培训劳动力，以及看看 AI 如何更好地增强工作——而不是取代工作。

## 准确性

负责任的 AI 的核心是模型的准确性。这对于可靠性、安全性和可信度至关重要。

在第三章中，我们了解了一些测量 AI 模型准确性的技术，包括：

偏差

这是平均预测值与实际值之间的差异。高偏差通常会导致欠拟合，其中模型在训练和未见过的数据上表现不佳，因为它无法表示数据的复杂性。

方差

这是指模型对训练数据中的变化或噪声敏感的地方。高方差会导致过拟合，其中模型将训练数据中的噪声捕获为真实模式。虽然增加更多数据有时可以减少过拟合，但这并不保证。如果模型相对于数据的复杂度过于复杂，或者数据是噪声的，仅仅增加数据集的大小可能不会提高准确性。

最终目标是找到偏差和方差都最小化的平衡点。以下是一些帮助权衡的技术：

交叉验证

通过在可用数据的子集上训练多个其他模型来评估人工智能模型，有助于检测过拟合。

增加数据

添加更多数据样本，特别是那些更具多样性的样本。

正则化

这会惩罚极端值，从而减轻过拟合和方差。

简单的模型

简单的模型可以帮助避免过拟合，因为它们不太可能捕捉到训练数据中的噪声。但若模型过于简单，这可能会导致偏差。

维度约简

通过减少数据集中特征的数量，同时尽可能保留信息，可以减少方差。

超参数调整

调整模型参数可以帮助平衡偏差和方差。

特征选择

这可以简化模型并减少方差。

# 负责任人工智能的要素

除了微软的负责任人工智能原则之外，还有其他公司拥有自己的框架，例如谷歌。这也适用于像联合国教科文组织和联合国这样的组织。甚至梵蒂冈也有自己的指导方针。无论如何，它们通常共享许多相同的概念。

关于 AIF-C01 考试，有一些原则你应该记住。但它们不应孤立考虑。实施一个原则通常需要考虑其他原则。例如，实现人工智能系统的透明度通常需要可解释性、公平性和稳健的治理结构。同样，确保安全和可控性涉及稳健的设计和清晰的治理。换句话说，应该采取整体的方法。

让我们来看看你应该知道的考试原则。

## 公平性

公平性意味着人工智能系统应做出不偏不倚的决定。不应基于种族、性别或社会经济地位等对个人或群体进行歧视。通过在人工智能系统中融入公平性，你可以帮助加强包容性和信任。

趣味的是，苹果和高盛在他们的 AI 模型中没有使用性别作为一个因素，纽约州的一项调查也没有发现固有的偏见。2 尽管如此，算法被更改了，结果变得更为公平。

这一点指出的是——即使你未使用某些数据，模型仍然可能是不公平的。原因是相关数据可能导致相同的结果。例如，信用评分系统可能给教师较低的信用额度，而女性可能在这个群体中有更高的代表性。

## 可解释性

可解释人工智能（XAI）是指开发 AI 系统以使决策过程透明和可理解。这有助于提高信任和问责制。

在监管行业，可解释人工智能（XAI）至关重要。如果一个系统不能满足某些要求和标准，它可能无法通过监管审查。例如，如果一个 AI 系统用于诊断疾病并推荐治疗方案，它必须对底层过程和推理有明确的解释。否则，患者可能会处于危险之中。

不幸的是，XAI 面临着许多挑战。当前的技术通常是通过事后解释来完成的，这些解释可能无法准确反映模型的实际决策过程。此外，缺乏标准化的指标来评估解释的质量和有效性，而使模型更具可解释性的努力有时可能会损害其性能。

但在 XAI 领域已经进行了大量研究，并且仍在持续进步。

请记住，存在各种可解释性框架，如 SHapley Additive exPlanations (SHAP)、Local Interpretable Model-Agnostic Explanations (LIME)和反事实解释。这些框架将总结和解释 AI 系统的决策过程。

对于 AWS 来说，有一些有用的工具，如 SageMaker Clarify，我们将在本章后面介绍。

## 隐私和安全

隐私和安全确保个人数据得到保护，并且他们能够控制自己的信息如何被使用。这包括保护数据免受未经授权的访问，并为用户提供关于其数据使用的明确选择。

实施强大的隐私和安全措施不仅符合法律要求，而且与用户建立信任。当个人对其数据得到负责任的处理有信心时，他们更有可能参与 AI 技术。这有助于促进创新和更广泛的应用。

## 透明度

透明度是分享关于 AI 系统如何开发、使用的数据以及其决策过程的信息。这种开放性使利益相关者——如用户、监管者和开发者——能够理解系统的能力和局限性。例如，透明度可能包括披露训练数据来源、AI 模型的目标以及任何固有的风险或偏见。

虽然透明度和可解释性是人工智能中的相关概念，但它们服务于不同的目的。透明度涉及对人工智能系统设计、数据来源和功能的整体开放性。另一方面，可解释性专注于人工智能就个别决策所提供的具体推理。

## 真实性和鲁棒性

真实性和鲁棒性有助于确保人工智能系统可靠且准确地运行。即使在意外输入或具有挑战性的环境中也是如此。

真实性涉及人工智能输出的真实性和准确性。鲁棒性是关于人工智能系统在输入数据变化、对抗性攻击或不可预见的情况下维持一致性能的能力。

这些属性的重要性不容小觑，尤其是在医疗保健、金融和自主系统等关键应用中。例如，在医疗保健领域，即使患者数据不完整或包含异常，人工智能诊断工具也必须提供准确的评估。一个强大的 AI 系统可以处理这些不规则性，而不会降低其输出的质量。同样，在金融领域，AI 模型必须在市场波动和数据不一致的情况下保持可靠性。

## 治理

人工智能治理是指公司为引导人工智能系统的道德和合规发展而设定的政策和程序。这包括定义明确的角色和责任，实施监督结构，并建立风险评估和缓解的协议。有效的 AI 治理有助于组织管理潜在风险，如偏见、歧视和隐私侵犯，同时促进人工智能操作的透明度和问责制。

人工智能技术的动态性要求持续监控和调整治理策略。建立包括伦理学家、法律专家、技术专家和其他利益相关者的跨职能团队，可以帮助组织主动识别和解决新兴的伦理困境和合规挑战。

## 安全性

确保人工智能的安全性涉及开发和运营人工智能系统以执行其预期功能，而不会对人类或环境造成伤害。这包括解决潜在风险，如意外行为、算法偏见和滥用。

人工智能安全的一个关键方面是严格的测试和验证。这包括在极端条件下对人工智能系统进行压力测试，并使用多样化的数据集以确保在各种场景中的一致性能。这些做法有助于在部署前识别和缓解风险。此外，实施强大的安全措施和监督机制可以防止故障和滥用。

模型的安全性和透明度之间存在重要的权衡。模型安全主要关于保护敏感数据，而模型透明度则是关于使人们更容易看到模型是如何和为什么做出决策的。在这两者之间找到正确的平衡往往具有挑战性。这在隐私和问责制都至关重要的环境中尤其如此。

例如，像深度神经网络这样高度复杂的模型通常提供更强的性能和准确性，但通常难以解释。像线性回归这样的简单模型更容易解释，但在复杂任务上可能表现不佳。

还有一些技术旨在保护数据隐私，例如差分隐私，它有助于防止个人数据点的泄露。然而，这可能会使人们更难理解模型是如何得出结论的——以牺牲透明度为代价提高安全性。同样，在隔离环境中训练的模型——称为断网系统，它们在物理上或逻辑上与外部网络断开连接——通过防止外部访问进一步增强了安全性。但这种隔离可能会使外部各方更难审计或评估模型的行为。为了确保性能和弹性，AWS Bedrock 允许在多种负载和场景下对模型进行压力测试，帮助验证它们在苛刻环境中的运行情况。

## 可控性

可控性是指引导和调节 AI 系统，使其行为始终与人类意图和道德标准保持一致的能力。这涉及到设计允许人类监督的 AI 架构。这允许开发人员和用户监控、干预和调整系统的行为。

“AI 控制问题”解决的是确保高级 AI 系统按照人类价值观和目标行动的困难。随着 AI 系统变得更加自主，它们以意想不到的方式追求目标的风险增加。这可能导致有害的结果。

模型的可控性在透明度和调试中也发挥着关键作用。如果模型对训练数据的调整做出合理的反应，那么理解其功能就变得更容易，当出现问题时也能更容易追踪问题。

可控性的程度受模型类型的影响。像线性回归这样的简单模型通常允许更直接的控制，而更复杂的模型可能会以不可预测的方式表现。为了评估模型的可控性，你可以运行测试，故意修改或增强数据，以查看模型输出是否以预期的方式发生变化。

# 负责任 AI 的好处

负责任的 AI 不仅仅是关于伦理——它也是一项好生意。虽然做正确的事情始终应该是优先事项，但整合负责任的 AI 实践可以显著提升公司的业绩和长期成功。

让我们看看其他原因：

建立信任和提升品牌形象

当用户认为 AI 系统是透明的、公平的和安全的，他们更愿意与之互动。这种信心会建立忠诚度并加强公司的声誉。这也意味着 AI 应用将更有效和有用。

保持在法规之前

随着政府和行业机构围绕 AI 制定新规则，已经建立道德框架的组织将发现适应起来更容易。

降低风险暴露

负责任的 AI 帮助公司主动识别和减轻危险，如算法偏差、数据滥用和安全漏洞。这降低了法律麻烦、声誉损害或因意外后果造成的财务损失的可能性。

在市场上脱颖而出

道德 AI 可以使公司区别于其竞争对手。随着越来越多的消费者关注公司如何使用 AI，那些表现出责任和诚信的公司可以赢得更强的竞争优势。

更智能的结果

当公平和透明是核心设计原则时，AI 系统往往会产生更可靠的见解。这导致更稳健的策略和更明智的决策。

推动创新

负责任的 AI 将更多视角带入对话。这种多样性可以导致更多原创思维，帮助团队创建既有影响力又具有前瞻性的产品和服务。

# Amazon Tools for Responsible AI

对于 AWS AI 平台，有广泛的能力和工具来帮助创建负责任的 AI。这已经成为一个主要优先事项，多年来投入了大量投资。

让我们来看看 Amazon Bedrock、SageMaker Clarify、Amazon A2I 和 SageMaker Model Monitor 的这些功能。

## Amazon Bedrock

使用 Bedrock，您可以轻松评估和比较各种 FM。一些自动指标包括准确性、鲁棒性和毒性。但还有人类评估，这些评估侧重于更主观的类别，如品牌声音的风格和对齐。这可以通过您的员工或由 AWS 管理的员工来完成。

另一个强大的负责任 AI 功能是 Bedrock 的指南，我们在第六章中简要介绍了。这个系统允许控制用户如何与 FM 互动。您可以通过以下方式限制互动：

过滤内容

您可以创建过滤器或使用内置版本来检测仇恨、侮辱、性或暴力内容。对于这些内容，您可以设置阈值。

修改个人识别信息（PII）

指南可以检测敏感数据，如姓名、地址、社会保障号码等。这些信息将在 FM 的输入和输出中被阻止。

实施内容安全和隐私政策

您不需要使用脚本语言来做这件事；您可以使用自然语言。

指南也适用于 AI 代理。这一点尤为重要，因为这些系统可以自主行动。因此，通常需要允许人类进行审批或反馈。

## SageMaker Clarify 和实验

SageMaker Clarify 允许您在无需高级编码的情况下检测数据集和 AI 模型中的偏差。您将指定如性别或年龄等因素，系统将进行分析并生成报告。

Clarify 具有其他功能。例如，它可以提供有关 AI 系统决策的详细信息，说明哪些特征对模型的响应影响最大。

AWS 还提供 SageMaker Experiments。这有助于管理 AI 开发的交互性。您可以组织、跟踪和比较不同的训练运行。为此，您将捕获输入、参数和结果。这有助于更好地评估 FM。

SageMaker 还拥有各种治理工具：

SageMaker 角色管理器

这允许管理员高效地定义用户权限。

SageMaker 模型卡片

这提供了必要模型信息的文档。这包括预期用例、风险评估和训练细节。

SageMaker 模型仪表板

这提供了一个统一的界面来监控模型性能。它集成了来自众多来源的数据，以跟踪数据质量、模型准确性和偏差随时间的变化。

## 亚马逊增强人工智能（Amazon A21）

亚马逊增强人工智能（Amazon A2I）在负责任的 AI 中扮演着关键角色，它允许在自动化决策过程中进行人工监督。这有助于降低有害错误的概率，提高公平性，并在 AI 系统中建立信任。

您可以定义触发人工审查的条件，例如低置信度预测或为审计目的的随机抽样。这种灵活性允许在包括内容审核、文本提取和翻译任务在内的各种场景中融入人类判断。例如，在内容审核中，置信度得分低于特定阈值的图像可以路由到人工审查员进行进一步评估。

Amazon A2I 支持多种劳动力选项。您可以使用自己的私人审查团队，通过 AWS Marketplace 与第三方供应商合作，或通过 Amazon Mechanical Turk 访问超过 500,000 名独立承包商的全球劳动力。

您可以使用 Amazon A2I 与 Amazon SageMaker、Amazon Textract、Amazon Rekognition、Amazon Comprehend、Amazon Transcribe 和 Amazon Translate 一起使用。

## SageMaker 模型监控器

在第三章中，我们简要介绍了 SageMaker 模型监控器。它是一项完全托管的服务，允许持续审查生产中的 AI 模型。它将检测可能影响模型性能的不同类型的漂移，包括以下内容：

数据质量漂移

识别输入数据统计特性的变化，例如均值或方差的偏移

模型质量漂移

通过比较模型预测与实际结果来监控性能指标，如准确性和精确度

偏差漂移

检测模型预测中的无意偏差随时间的变化

特征归因漂移

观察输入特征在影响模型预测中的重要性变化

使用模型监控器，您可以使用训练数据建立基线，以定义可接受的性能阈值。监控作业可以定期安排或按需执行。

# 进一步推进负责任 AI

让我们看看在以下部分中创建负责任 AI 时需要考虑的一些额外因素。

## 可持续性和环境考量

可持续性和环境考量指的是开发那些在长期内（在社会、经济和环境方面）可行的 AI 技术，同时积极最小化生态损害。这包括创建系统，不仅提供性能和创新，而且支持社会福祉并减少对地球的负面影响。它包括管理 AI 系统的整个生命周期，从训练和运行模型所需的能源到硬件中使用的材料，目标是降低环境影响并促进负责任、资源高效的实践。

这些原则是负责任 AI 的核心，强调人工智能的道德、透明和可问责的发展。随着 AI 的不断扩展，其环境影响不能再被视为事后考虑。负责任 AI 的倡议必须确保可持续性被融入 AI 系统的设计、部署和管理中。

一个主要问题是与训练和运行大型 AI 模型相关的能源消耗。这些过程可能需要大量的计算资源，这增加了电力使用并导致温室气体排放。负责任的方法涉及通过更好的模型架构、使用节能硬件和从可再生能源获取电力来提高能源效率。例如，优化训练计划以与低碳能源可用性较低的时期相一致，可以在不牺牲性能的情况下减少环境影响。

另一个问题是与 AI 基础设施的资源强度相关。制造和部署专门的硬件，如 GPU 和 TPU，通常涉及对环境有害的材料和过程。可持续 AI 开发促进现有硬件的再利用，优先考虑可回收或更持久的组件，并限制电子废物的生产。

此外，环境影响评估应该是 AI 开发生命周期的一个组成部分。这些评估评估了部署 AI 系统的直接影响（如能源使用）和间接影响（如促进高排放行业）。在识别到风险时，应实施缓解策略——例如减少模型大小、利用基于云的绿色计算或引入政策保障措施。

## 数据准备

创建负责任的人工智能系统需要仔细准备数据集以确保公平性和准确性。一个关键因素是平衡数据集，以确保人工智能模型不会无意中偏爱某些群体或结果。例如，在招聘或放贷等应用中，不平衡的数据集可能导致有偏见的决策，不公平地损害特定人口群体。

要实现平衡的数据集，收集既包容又多样化的数据至关重要。这意味着确保数据集准确反映了与人工智能系统预期用途相关的各种观点和经验。例如，如果开发一个针对所有年龄段诊断条件的医疗保健人工智能模型，训练数据应包括来自不同年龄段的患者代表性样本。忽视这一点可能导致模型在一个年龄段表现良好，而在其他年龄段表现不佳。

除了收集之外，数据整理在平衡数据集中也发挥着至关重要的作用。这包括预处理步骤，如清理数据以去除不准确之处，归一化数据以确保一致性，以及选择对模型预测有实质性贡献的相关特征。生成合成示例等数据增强技术也有助于实现平衡。定期审计数据集对于识别和纠正随时间出现的任何新兴偏差是必要的。

工具如 Amazon SageMaker Clarify 和 SageMaker Data Wrangler 可以帮助这个过程。SageMaker Clarify 通过分析不同特征和结果分布来识别数据集中的潜在偏差。如果检测到不平衡，SageMaker Data Wrangler 提供随机过采样、随机欠采样和合成少数过采样技术（SMOTE）等方法来重新平衡数据。

## 可解释性与可解释性对比

在负责任人工智能的背景下，可解释性（在第四章第四章中介绍）与可解释性之间的优先级取决于风险、监管环境和涉及的利益相关者：

可解释性

这通常在透明度和问责制至关重要的情况下受到青睐，例如在受监管的行业中。

可解释性

当使用复杂模型且难以解释时，可解释性至关重要，但仍然需要人工监督——例如，在预测诊断或自动化招聘中。

这两者都是负责任人工智能的重要支柱，但在决策必须清晰理解时，可解释性通常被视为黄金标准。表 8-1 展示了这种情况的一些场景。

表 8-1\. 可解释性与可解释性对比：何时使用哪种

| 用例 | 目标 | 优先方法 | 理由 |
| --- | --- | --- | --- |
| 银行贷款审批 | 监管合规性、公平性 | 可解释性 | 需要清晰的规则以实现可审计性和法律合规性 |
| 使用人工智能诊断罕见疾病 | 在人工监督下实现高精度 | 可解释性 | 使用了复杂的模型如深度学习，但需要对决策进行解释 |
| 使用机器学习进行简历筛选 | 防止偏见，人力资源透明度 | 可解释性 | 必须解释为什么候选人被筛选出去；内部逻辑可能不透明 |
| 消费者信用评分预测 | 公众信任，清晰 | 可解释性 | 消费者和监管机构必须了解分数是如何计算的 |

## 以人为中心的设计

以人为中心的设计（HCD）是指技术是以最终用户为中心创建的。它关乎优先考虑清晰度、可用性和公平性。通过使用 HCD，你可以提供增强的决策能力。这些是关键原则：

清晰性

信息必须以清晰的方式呈现——没有术语，没有歧义。例如，一位审查人工智能推荐的治疗方案的医生需要对其建议的原因有一个简单明了的解释。

简单性

少即是多。删除不必要的数据点，突出重点。物流经理不需要模型内部的数学——只需要一个清晰的路线推荐和一个置信水平。

可用性

界面应直观，既适用于技术熟练的用户，也适用于非技术用户。例如，一位贷款官员应该能够在没有特殊培训的情况下导航 AI 工具。

反思性

工具应提示用户对决策进行批判性思考。一个弹出窗口询问“这个系统可能遗漏了额外的背景信息吗？”可以引发深思熟虑的审查。

责任

人工智能辅助决策必须有明确的归属权。如果招聘工具推荐了候选人，人力资源专业人士仍然对最终选择负责。

个性化

根据用户定制体验。例如，客户服务人工智能可以根据代理人的互动风格调整其语气和建议。

认知学徒制

就像初级员工通过跟随专家学习一样，人工智能系统应该通过示例和纠正从经验丰富的用户那里学习。

以用户为中心的工具

使系统包容并易于访问。例如，一个有视觉障碍的初级员工和一个对人工智能知识有限的资深经理都应该能够同样好地使用培训平台。

## RLHF

在第四章中，我们简要介绍了 RLHF。这是模型通过结合人类偏好来学习做出更好决策的地方。RLHF 在负责任的 AI 中发挥着重要作用，通过使模型行为与人类价值观、伦理和期望保持一致，有助于减少有害或偏见的结果。它支持创建不仅更准确，而且更透明、公平，并与社会规范保持一致的 AI 系统。

想象一下开发一个旨在帮助用户管理日常任务的虚拟助手。最初，助手可能会根据一般模式建议提醒或安排会议。然而，用户可能更喜欢某些建议而不是其他建议。通过观察用户接受或拒绝哪些建议以及收集他们对偏好的反馈，助手可以学会更有效地定制其推荐。

这些是强化学习与人类反馈（RLHF）的一些优势：

提高模型性能

模型可以优化其输出以更好地满足用户期望。这可能导致准确性和相关性的提高。

处理复杂场景

在难以定义明确规则的情况下，人类反馈提供了细微的指导。

提高用户满意度

根据用户偏好进行适应的模型往往能提供更个性化和令人满意的体验。这有助于培养更大的用户信任和参与度。

类似于 Amazon SageMaker Ground Truth 这样的平台提供了将强化学习与人类反馈（RLHF）集成到机器学习生命周期中的能力。例如，数据标注员可以审查模型输出，根据质量对其进行排名或分类。这种反馈作为训练模型的有价值输入。这使它们能够更紧密地与人类判断和期望保持一致。

# 结论

负责任的人工智能涉及以道德、安全和透明的方式开发人工智能系统。它关乎管理如毒性、知识产权争议、就业替代和准确性问题等风险。

在本章中，我们学习了负责任人工智能的核心原则及其用例。我们还看到了 AWS 提供的各种工具，如 Amazon Bedrock 和 SageMaker Clarify，可以帮助这个过程。

在下一章中，我们将探讨人工智能解决方案的安全、合规和治理。

# 测验

为了检查你的答案，请参考“第八章答案键”。

1.  以下哪种方法可以帮助减少人工智能模型中的过拟合？

    1.  增加模型的复杂性

    1.  添加更多噪声数据

    1.  提前停止训练

    1.  避免超参数

1.  公司用来解决生成人工智能中知识产权关注的技术是什么？

    1.  从公共来源移除所有训练数据

    1.  限制对人工智能工具的访问

    1.  将人工智能限制为仅限公司内部使用

    1.  与内容提供商创建许可协议

1.  为什么人工智能模型中的准确性被认为是负责任人工智能的关键？

    1.  它提高了可靠性、信任和安全。

    1.  准确的模型需要更少的更新和补丁。

    1.  准确性使模型成本更低。

    1.  准确性仅对视觉模型重要。

1.  人工智能系统中公平性的主要目的是什么？

    1.  提高模型的复杂性

    1.  减少决策中的延迟

    1.  增强个性化功能

    1.  避免对个人或群体进行歧视

1.  可解释性与透明度如何比较？

    1.  可解释性关注用户界面设计。

    1.  可解释性解释模型决策；透明度分享系统细节。

    1.  它们意味着相同的意思。

    1.  透明度仅在开源模型中是必需的。

1.  人工智能中的隐私和安全主要旨在保护什么？

    1.  模型权重和参数

    1.  算法透明度

    1.  个人数据和用法控制

    1.  开发者知识产权

^(1) Jake Coyle, [“在好莱坞编剧与人工智能的斗争中，人类暂时获胜”](https://oreil.ly/KrpWD), 联合社，2023 年 9 月 27 日。

^(2) Sanya Mansoor, [“一条病毒式推文指责苹果的新信用卡具有‘性别歧视’”](https://oreil.ly/lE4hb), *《时代》杂志*, 2019 年 11 月 12 日。
