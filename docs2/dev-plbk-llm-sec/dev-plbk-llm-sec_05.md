# 第五章。你的 LLM 是否知道得太多？

2023 年，许多公司开始禁止或严格限制使用 LLM 服务，如 ChatGPT，基于对可能泄露机密数据的担忧。这些公司的部分名单包括三星、摩根大通、亚马逊、美国银行、花旗集团、德意志银行、富国银行和高盛。这些大型金融和科技公司采取的行动表明，他们对 LLM 泄露机密和敏感信息表示了极大的关注，但这种风险有多严重？作为 LLM 应用程序的开发者，你是否需要关心？

在第一章中提到的泰勒故事第一章，微软的聊天机器人遭到了黑客的攻击。虽然造成的损害很严重，但由于泰勒没有访问到大量敏感数据，因此损害是有限的。然而，大型语言模型（LLM）与真实世界数据的交集可能会带来无意中泄露信息的潜在风险，正如员工不小心将敏感的商业数据输入到 ChatGPT 中，这些数据随后被整合到系统的训练库中，使得其他人能够发现。

本章将深入探讨 LLM 获取数据的各种方式。我们将研究三种主要的知识获取方法以及与 LLM 获取数据相关的风险。在这个过程中，我们将尝试回答“你的 LLM 是否知道得太多？”这个问题，并讨论如何减轻应用程序泄露敏感、私人或机密数据的风险。

# 真实世界案例

让我们考察两个在真实世界中看到的例子的影响。我们将从一个聊天机器人的例子开始，这个例子与泰勒有些相似，但由于聊天机器人可以访问的数据以及数据的披露方式，造成的损害要大得多。然后我们将看看一个机组成员的例子，这个例子使所有者面临了更高的法律和声誉风险。

## 李路达

位于首尔的初创公司 Scatter Lab，在第一章中也简要提到了第一章，由于其不负责任地处理个人数据而面临严重的法律和声誉后果。该公司运营了一个流行的应用程序“爱情科学”，该应用程序通过分析用户的短信来帮助用户分析他们与浪漫伴侣的兼容性。这项服务积累了来自 60 万用户的 940 亿条对话。该公司后来推出了李路达，[“一个人们更喜欢作为对话伙伴的人工智能聊天机器人。”](https://oreil.ly/PDF3e)李路达使用“爱情科学”的庞大数据集作为其训练基础——而没有应用任何适当的净化措施。李路达不仅表现出我们从泰勒那里看到的一些有毒行为，而且更令人担忧的是，她开始泄露敏感数据，例如用户的姓名、私人昵称和家庭地址。

韩国个人信息保护委员会对 Scatter Lab 未能获得适当用户许可处以 1.033 亿韩元（约合 9.3 万美元）的罚款，这在韩国为惩罚 AI 技术公司数据管理不当树立了先例。

这起事件产生了重大影响。让我们来看看各个方面：

敏感数据的公开曝光

敏感数据的公开暴露威胁到用户隐私，揭示了诸如姓名、位置、关系状态和医疗信息等个人信息。

财务罚款

Scatter Lab 因未能负责任地管理用户数据而遭受了巨额罚款。

声誉损害

这起事件严重损害了 Scatter Lab 的声誉，正如主流媒体的报道和 Google Play 上大量负面评论所证明的，特别是针对“爱情科学”应用。

服务中断

在事件发生后，违规的聊天机器人服务 Lee Luda 被关闭，阻碍了公司的扩张计划。

现在，让我们来探讨你可以从中学习并应用到自己的项目中的经验教训：

严格的数据隐私协议

这个事件强调了确保用户数据得到最严格保护和在法律框架内处理的强大数据隐私协议的必要性。

用户同意

在收集和处理用户数据之前获得明确和知情同意是法律规定的，也是道德数据实践的基础。

年龄验证机制

在这种情况下，损害更为严重，因为“爱情科学”收集的一些数据属于未成年人。在许多监管环境中，从未成年人那里进行数据挖掘需要特别注意。

公众意识

公司必须向用户透明地说明他们将如何利用数据，并有效地传达风险。

监控和审计

定期监控和审计数据处理实践可以帮助及时识别和纠正隐私问题，减轻敏感数据泄露的风险。

这个案例强调了在利用用户数据增强 LLM 能力与确保严格保护用户隐私和数据完整性之间的微妙平衡。

## GitHub Copilot 和 OpenAI 的 Codex

2023 年的一起显著事件突出了通过 LLM（大型语言模型）涉及 GitHub Copilot（一个由 OpenAI 的 Codex 模型驱动的工具）的敏感数据披露风险。GitHub 设计 Copilot 旨在通过自动补全代码来帮助开发者，这是通过在 GitHub 公共存储库的大量代码语料库上训练实现的。然而，这个工具很快陷入了法律和伦理挑战的泥潭。一些开发者发现 Copilot 建议了他们的版权代码片段——尽管原始代码处于一个限制此类使用的许可证之下。这种可能的版权侵犯引发了 GitHub、微软和 OpenAI 的诉讼，开发者指控版权、合同和隐私侵犯。

此案在美国地区法院展开。开发者的论点基于两个主要主张：Codex 能够复制其代码的部分内容，违反了软件许可条款，并且通过复制受版权保护的代码而没有必要的版权管理信息，违反了《数字千年版权法案》。法官驳回了驳回这两个主张的动议，使诉讼得以继续。尽管法院驳回了某些指控，但案件的焦点在于由于 Codex 和 Copilot 复制代码，可能侵犯了开发者的知识产权。

到目前为止，诉讼仍在进行中，我们可能需要一段时间才能了解其全部影响。这场诉讼突显了 LLM（大型语言模型）领域的一个关键问题——无意中泄露敏感数据的潜在风险。其影响超出了涉案各方，在整个科技行业引起共鸣，并引发了关于 LLM 访问和使用公开数据的法律和伦理影响的讨论。

尽管此案提出的全部知识产权问题尚未完全解决，但您可以从中学到一些经验教训，并将其应用到自己的项目中：

数据治理

这起事件强调了稳健的数据治理框架的重要性，强调了在数据使用方面需要明确的指南，特别是关于公开可用或开源数据。

法律清晰度

此案阐明了 LLM 与真实世界数据交互周围的合法灰色区域，表明需要更明确的法律和法规来定义可允许的数据使用范围和版权遵守。

伦理参与

除了法律合规性之外，LLM 数据使用的伦理维度要求开发者和组织采取负责任的态度，尊重开源贡献和许可协议的字面意义和精神。

用户意识

此事件还突显了用户对如何利用其数据的意识的重要性，为使用 LLM 的组织提供了更透明披露的先例。

这场诉讼的展开提供了一个现实世界的场景，展示了 LLM 应用领域中法律、伦理和技术因素之间的复杂相互作用。它是 LLM 发展和与各种数据源互动时即将到来的挑战（尤其是关于敏感数据泄露风险的挑战）的预兆。

# 知识获取方法

您的 LLM 应用的力量将随着其可访问的数据量而增长。同时，与这些数据相关的风险也在增加。如果您的 LLM 接触到了特定类型的数据，您将需要管理数据泄露的风险。让我们看看 LLM 获取知识的三种常见方式。

LLM 的知识库的核心是其*模型训练*。这个过程从*基础模型训练*开始，LLM 沉浸在庞大的数据集中，获得对语言、上下文和世界洞察的广泛理解。然后，这种基础知识可以通过*模型微调*进行细化，使用目标数据集将 LLM 调整到更专业化的任务或细分领域。

LLMs 在独特的、不频繁的训练阶段进行学习，这意味着它们的信息通常过时，这限制了它们在需要最新知识的应用中的使用。这就是*检索增强生成*（RAG）发挥作用的地方。LLMs 可以进入广阔的公共网络领域，获取实时更新，或者深入到结构化或非结构化数据库中。通过 API 连接外部系统、数据库或在线平台，LLMs 可以进一步扩展其知识范围，用丰富的外部数据丰富其回答。

一些应用甚至可以更进一步。用户交互，如查询、对话和反馈，使 LLMs 能够持续获取新知识。处理这些输入允许 LLM 扩展其理解，通过每次交互改进其能力，并交付越来越个性化和相关的回答。

这些类别——训练、检索增强生成和用户交互——各自都有细微差别，这些差别可以显著影响您 LLM 应用的网络安全格局。虽然它们作为知识获取的渠道，但也引入了需要仔细考虑的潜在漏洞和挑战。随着我们进入本章，我们将深入探究每个类别，揭示每种方法固有的关键安全影响。通过这次探索，我们的目标是让您全面了解潜在风险及其缓解措施。

# 模型训练

训练是开发和精炼大型语言模型（LLMs）的关键步骤。它包括两个不同的阶段：创建基础模型及其后续的微调。*基础模型训练*建立了广泛的语言和上下文理解，而*微调*则将这种通用知识针对特定任务或领域进行细化。在本节中，我们将探讨这两个阶段的复杂性，强调它们各自的方法。随后，我们将深入探讨每个步骤固有的关键安全影响，为您提供关于潜在漏洞和防范最佳实践的见解。

## 基础模型训练

基础模型训练是构建大型语言模型（LLM）的第一步。在这个阶段，模型在广泛且多样化的数据集上进行训练，通常包括各种主题、语言和文本格式。目标是让模型具备对语言、语境关系和一般世界知识的广泛理解。这种基础训练构成了 LLM 生成连贯、语境相关和有见地回应的基础，类似于人类在特定领域专业化之前对世界的初步理解。

在本质上，训练一个 LLM 的基础模型是一个复杂的模式识别练习。训练涉及使用高级算法分析大量数据集，识别词语之间的关系，理解语境，并根据这种理解生成连贯的回应。让我们看看涉及到的步骤：

1. 模式识别

训练基础模型向模型提供大量的文本数据——有时是数十亿个标记。在处理这些数据时，模型学会识别模式。例如，它开始理解“苹果”这个词可以根据语境与“水果”、“树”、“派”或“技术”相关联。

2. 语境理解

接下来，模型开始根据语境辨别词语使用的细微差别。例如，它学会根据周围的词语和短语，短语“苹果的增长”可以指一家科技公司的扩张或树上水果的发展。训练算法将调整内部参数，通常数量以亿计，以捕捉这些复杂语境关系。

3. 回应生成

模型生成回应的能力是通过反复的训练迭代来发展的，不断精炼其对语言和语境的理解。与人类的记忆回忆不同，模型分析输入，将其与学习到的模式匹配，理解语境，并根据训练数据构建回应。训练数据的多样性和广度至关重要，因为它们直接影响到模型产生准确和语境适当的回应的能力。

## 基础模型的安全考量

前面的步骤说明了为什么训练一个定制的基座模型可能很复杂且成本高昂。这就是为什么今天的大多数项目都是从现有的基座模型开始的。起点可能是一个通过 SaaS（软件即服务）产品访问的专有模型，例如 OpenAI 的 GPT 系列，或者一个私有托管的开源模型，例如 Meta 的 Llama。在任一情况下，基座模型的创建者都希望已经做了一定程度的工作，以确保个人信息（PII）等敏感信息不会出现在训练基础中，尽管这并不总是可能。请仔细选择您的基座模型！即使有最好的意图，也有许多这些基座模型积累敏感信息的例子，这些信息在某些情况下可能是不适当的。以下是一些可能需要留意的问题信息类型示例：

+   他人知识产权，例如受版权保护的文本

+   与武器、毒品或其他相关话题的危险或非法信息

+   在特定情境或讨论中可能不适当的文化或宗教文本

如果您决定训练自己的基座模型，您可以对系统许多方面实现更高程度的控制。这种控制可能非常有优势。然而，您现在需要承担使用在模型中使用的所有训练数据的责任。保持其无敏感信息可能对您来说是一个重大的挑战。我们将在本章后面进一步讨论这个问题。

## 模型微调

模型微调是基座模型训练之后的可选步骤，旨在针对特定任务或领域对通用模型进行专业化。您将使用一个较小的、特定领域的数据集来调整模型在微调期间的权重。这样，您可以细化其响应，以在目标应用中表现良好。这个过程显著提高了模型的表现，使其对预期用途更加相关和准确。用于微调的专用数据允许模型将基座训练期间获得的泛化理解适应到任务的细微差别和具体细节，从而提供更定制和有效的解决方案。

在其核心，微调解决了机器学习中的一个基本挑战：虽然基座模型具有广泛的知识，但它们通常需要更多深度和特定性来完成特定任务。例如，虽然一个通用模型可能使用了一些医疗信息进行训练，但它可能产生的响应精度与医疗专业人员预期的不同。微调通过将基座模型的通用知识适应到特定领域或任务来弥合这一差距。

## 训练风险

无论是从头训练基础模型还是微调现有模型，都必须仔细考虑将敏感数据纳入训练集的风险。用于训练模型的数据可能会成为长期记忆。即使尝试调整模型并提供防止不当披露的护栏，模型仍可能将此类信息泄露给第三方。

在为训练模型构建数据集时，以下是一些您需要考虑的风险：

直接数据泄露

如果在训练过程中将模型暴露于 PII 或机密信息，它可能会生成意外泄露这些数据的输出。

推理攻击

攻击者可能使用提示注入来从模型中提取敏感数据。

违规和合规问题

使用包含个人身份信息（PII）的数据集训练模型，尤其是未经用户同意，可能导致违反数据保护法规，如健康保险可携带性和问责制法案（HIPAA）、通用数据保护条例（GDPR）或加利福尼亚消费者隐私法案（CCPA）。这可能导致巨额罚款和法律责任，更不用说声誉损害。

公众信任的丧失

如果公众得知一家公司使用 PII 或机密数据训练了其模型，并且可以泄露此类数据，该组织可能会面临重大反弹和信任丧失。

数据匿名化受损

即使在训练前将 PII“匿名化”，模型仍可能发现允许数据去匿名化的模式，尤其是如果它们将输入与其他公开可用的数据集相关联。

作为目标吸引力增加

如果恶意行为者认为模型包含机密信息或 PII，他们可能会更有动力对其发起复杂的攻击，旨在提取有价值的数据。

模型回滚和财务影响

如果团队后来发现模型之前曾使用 PII 进行训练，可能需要回滚到早期版本，从而导致财务影响和项目延误。

考虑到这些重大风险，确保用于训练的数据得到彻底清洗至关重要。此外，定期审计、严格的数据审查和高级差分隐私技术可以帮助减轻潜在风险。

# 检索增强生成（RAG）

RAG 是 LLM 数据获取和响应生成的变革性方法。与传统 LLM 仅依赖从训练中获得的庞大内部知识库不同，RAG 首先从外部数据集中检索相关的文档片段或*段落*。然后，LLM 利用这些段落来指导其生成的响应。这种两步法——检索相关信息然后基于检索开发答案——允许模型实时或更及时地获取其原始训练数据中未包含的信息。

RAG 是语言模型处理大量实时数据能力的重大飞跃。无论他们的训练数据多么庞大，传统 LLM 本质上都局限于他们最后的训练截止日期，这使得他们在特定主题或实时事件上可能过时。RAG 通过允许 LLM 无缝访问和整合外部、最新的信息来解决这一限制。这种动态能力提高了模型输出的准确性和相关性，并将 LLM 定位为在快速发展的领域中更具多样性和适应性。融合检索和生成过程的能力预示着更知情和情境感知的对话 AI 的新前沿。

然而，将您的 LLM 连接到大型、实时数据存储会打开一个潘多拉的盒子，涉及安全考虑。一个问题是我们之前在第四章中讨论的间接提示注入。当您将不受信任的数据作为 RAG 提示的一部分提供给 LLM 时，可能会发生提示注入攻击。但，对于本章，我们将关注与敏感数据披露相关的风险，以帮助回答“您的 LLM 知道得太多吗？”这个问题。

让我们回顾一下 RAG 系统获取更大数据存储的常见方式。通过了解您的 LLM 可能如何访问这些知识库，我们可以更好地规划安全风险和考虑因素。在这里，我们将探讨直接从网络访问数据和访问数据库。

## 直接网络访问

为您的 LLM 提供直接连接到网络的能力，可以是一种强大的机制，以便获取实时或更新的信息来增强其知识库。网络连接使模型能够获取最新数据，跟上不断发展的主题，并提供更准确和最新的响应。通过与网络互动，LLM 可以弥合其最后训练截止日期和现在之间的差距，确保其信息的相关性和时效性。这一功能显著提高了 LLM 在动态或快速变化的领域中的实用性。

让我们看看访问网络的几种模式。

### 抓取特定 URL

直接访问预定的 URL 以提取其内容，当您知道您想要 LLM 访问的信息的确切来源时，这是一种特别有用的方法。这种方法适用于许多情况，例如从金融新闻网站提取每日股价，从特定的新闻来源或博客中获取定期更新，或从电子商务网站检索产品详情或评论。

对于这些类型的用例，有几个优点：

精确性

针对目标网页，消除了来自无关来源的潜在噪声。

效率

由于 URL 是预定的，您可以优化该页面的特定结构和内容的抓取过程。

可靠性

持续访问单个或一组已知的 URL 可以在一段时间内提供更稳定的结果。

但也存在一些关键挑战：

页面结构变化

网页经常进行重新设计或结构变化。如果特定 URL 的内容结构发生变化，抓取机制可能需要调整。

访问限制

一些网站使用 CAPTCHA、速率限制或*robots.txt*限制来防止或限制自动化访问。

法律或伦理挑战

如果你没有拥有你正在抓取的网页上的内容，你必须考虑该页面的所有者是否可能反对你在系统中使用这些数据的方式。根据需要考虑版权和其他许可条款。

### 使用搜索引擎后进行内容抓取

在这种方法中，你向谷歌或必应等平台发出搜索查询，根据特定的关键词或主题找到相关内容，然后从一或多个顶级搜索结果中抓取内容。这种方法最适用于以下用例：通过抓取顶级新闻文章或博客来研究特定主题或产品的当前公众情绪，检索特定主题的最近学术出版物或文章，以及通过分析特定行业关键词的顶级结果来了解市场趋势。

对于这些类型的用例，有几个优点：

相关性

搜索引擎根据相关性对内容进行排名，确保 LLM 访问高质量和相关的信息。

及时性

搜索引擎不断索引新内容，使它们成为获取特定主题最新信息的宝贵资源。

多样性

通过访问多个顶级结果，LLMs 可以从不同的角度更全面地了解一个主题。

挑战包括：

间接提示注入

如第四章所述，恶意提示可能不是直接来自用户。它们可能被秘密嵌入到 RAG 系统中的提示数据中。在这种情况下，攻击者可以在网页中嵌入恶意数据，当页面被解析并且数据包含在通过应用程序传递给 LLM 的提示中时，导致间接提示注入攻击。

动态结果

特定查询的搜索结果可能会随时间变化，这引入了 LLM 访问的内容的变异性。

搜索限制

搜索引擎可能有请求限制，尤其是对于自动化查询，这可能会限制搜索次数。

抓取深度

决定抓取多少顶级结果会影响信息的质量和广度。抓取太多可能会稀释相关性；抓取太少可能会错过有价值的观点。

法律和伦理问题

在抓取内容时，重要的是遵守搜索引擎的服务条款，并考虑版权和许可条款。

### 示例风险

直接网络访问或搜索引擎携带各种风险，与无意中获取或披露 PII 和其他敏感信息有关。以下是一些可能发生这种情况的例子：

评论区和论坛

一个模型可能会从一个可信赖的来源抓取技术文章或新闻报道，但在这样做的时候，它也可能无意中拉入了与文章相关的评论或论坛帖子。这些部分通常包含个人轶事、电子邮件地址或其他可识别的细节。例如，一个用户可能会要求 LLM 提供关于特定健康主题的最新讨论。模型可能会从用户分享个人故事、姓名、年龄或甚至特定医疗细节的健康论坛中提取数据。

用户资料

一些网站在文章或帖子末尾包含用户资料或作者简介。抓取此类网站可能会意外收集这些资料中的个人细节或联系信息。例如，一个从博客平台抓取条目的 LLM 可能会也抓取作者的简介，包括他们的全名、位置、工作单位和电子邮件地址。

网页中的隐藏数据

一些网页在后台存储元数据或秘密信息。虽然这些数据可能对人类观众不可见，但具有网络访问权限的 LLM 仍然可以访问和处理它。例如，一个抓取企业网站的 LLM 可能会无意中访问包含内部文档路径或甚至机密修订注释的嵌入式元数据。

不准确或宽泛的搜索查询

当使用搜索引擎时，如果查询过于宽泛或定义不准确，模型可能会拉入包含敏感信息的不相关内容。例如，一个旨在寻找知名人物公开演讲的查询“John Doe 的演示”可能会也产生来自不同 John Doe 的博客的结果，他在那里分享了他的联系电话。

广告和赞助内容

网络抓取可能会无意中收集来自广告或赞助帖子的数据，这些帖子有时可能包含基于先前用户行为或其他针对性标准的个性化内容。例如，一个抓取网页新闻的 LLM 可能会也拉入一个广告，声称“[地点]居民的特别优惠”，揭示了位置数据。

动态内容和弹出窗口

许多网站具有基于用户交互、位置或时间变化的动态内容。弹出调查、聊天机器人或反馈表单可能包含个人信息提示。例如，在抓取服务提供商的网页时，LLM 可能会拉入一个弹出内容询问：“你是来自[城市]的吗？回答这个调查！”这可能会披露地理位置细节。

文档元数据和属性

当访问在线文档或文件时，它们相关的元数据可以包含作者姓名、编辑历史或内部评论。例如，LLM 可能会抓取一家公司的公开财务报告，但与其相关的属性可能会显示“最后由[员工姓名]从[部门]编辑”，揭示了公司内部信息。

## 访问数据库

这种模式涉及一个 LLM 检索存储在结构化或非结构化数据库中的数据。这种方法可以包括查询传统数据库以获取特定数据或访问向量数据库以获取嵌入。通过利用数据库，LLM 可以提供精确且数据驱动的响应，使它们在需要实时或历史数据检索的场景中具有显著的价值。这种知识获取方法允许 LLM 在数据丰富的环境中运行，并基于数据库中可用的数据提供高度准确、上下文感知和个性化的响应。

### 关系数据库

自 20 世纪 70 年代末以来，关系数据库一直是事实上的标准，支撑着无数行业和应用程序的基础设施。它们革命性地改变了开发者使用表格组织和访问数据的方式，并通过建立的关系确保数据完整性。它们对数据管理的结构化方法，加上 SQL（结构化查询语言）在数据处理方面的力量，使组织能够高效且精确地处理复杂的数据集。尽管现代技术进步带来了新型数据库，但关系数据库的稳健性继续使它们成为许多企业的信任之选。

给你的 LLM 访问企业内部庞大的数据存储库是强大且诱人的。优势是明显的：即时访问大量历史和实时数据，可以提供更丰富、更全面的信息，以满足特定组织的需求和情境。LLM 可以提供见解，回答复杂的查询，甚至自动化原本需要数小时人工编译的任务。它可以改变用户体验，在庞大的数据存储库和最终用户之间提供一个无缝的界面，无论是员工、利益相关者还是客户。然而，随着这种巨大的力量，也带来了同样巨大的责任，即保护敏感信息并确保数据访问保持安全且道德地管理。让我们检查与 LLM 应用程序相关的数据库访问风险区域：

复杂关系放大暴露

关系数据库通过关系链接结构化数据集。虽然一张表可能看似无害，但其与其他表的关联可能会无意中揭示敏感模式。例如，一个无辜的产品 ID 列表在关联到特定的客户交易时可能会变得敏感。

非预期查询

误解的命令或措辞不当的问题可能导致 LLM 检索到开发者不希望用户访问的数据。想象一下，一个随意的询问无意中调出了一份详细的记录，揭示了比询问的更多内容。

授权疏忽

关系数据库具有复杂的权限系统。在集成过程中，LLM 可能会由于疏忽或配置错误而获得比必要的更广泛的访问权限，从而打开了应该保持限制的数据的大门。

无意的数据推断

LLMs 识别模式。在多次交互中，它们可能会收集看似不敏感的数据，导致意外的敏感见解。例如，虽然个别购买可能不会透露太多信息，但模式可能会暗示公司即将推出的产品发布或战略转变。

审计性和问责制挑战

关系型数据库传统上提供强大的审计跟踪，将操作与特定用户关联。随着 LLMs（大型语言模型）作为中介，确保每个查询和数据检索都保持可追溯性至关重要。没有清晰的审计跟踪，确定数据泄露的源头或理解意外行为变得复杂。

总之，将 LLMs 与受信任的关系型数据库集成可以提高功能和性能。然而，在使用这些集成时，重要的是要意识到相关的风险。实施严格的保障措施和监督可以发挥 LLM 的能力，同时确保数据完整性和安全性。

### 向量数据库

*向量数据库*代表了我们在处理数据方式上的重大进步，尤其是在机器学习和 AI 操作方面。与将数据组织成行和列的关系型数据库不同，向量数据库将数据存储为高维向量。这些向量是数字数组，能够有效地捕捉对象或数据点的特征或属性的本质。这种结构在执行向量空间中的相似性或邻近性操作时具有优势。

高维向量擅长处理复杂的操作，如*最近邻*搜索，这对于许多 AI 应用至关重要。这些搜索允许数据库快速找到向量空间中与给定查询点最近的点，从而促进依赖于找到最相似项或模式的操作。通过将数据作为*向量*——本质上是对数据项信息的数学表示——管理，向量数据库在快速检索和比较数据方面表现出色，从而在庞大的数据集中实现高效和准确的相似性搜索。

通过 RAG 模式将您的 LLM 与向量数据库集成可以极大地增强其功能。通过将这些数据库与模型链接，您可以利用基于相似性的搜索能力，从而提供更丰富、更符合用户细微查询的响应。模型可以迅速定位并利用与查询意图产生共鸣的嵌入，提供准确和相关的结果。这无疑是革命性的。让我们看看一些将向量数据库与 RAG 模式结合可以产生优秀结果的例子：

问答系统

用户在回答问题时期望得到精确和准确的响应。RAG 系统可以从向量数据库检索相关文档或数据片段，以告知 LLM 的响应，使答案比仅从模型知识生成的答案更准确和详细。

内容推荐

对于需要个性化内容推荐的平台——如新闻聚合器、流媒体服务和电子商务网站——RAG 可以通过从向量数据库检索与用户配置文件或先前交互紧密匹配的内容来增强推荐引擎，从而提高用户参与度和满意度。

学术研究和总结

RAG 系统可以通过从向量数据库检索相关文档并提供它们之间的摘要或关联来显著加快研究过程。

客户支持

聊天机器人可以从常见问题解答、产品手册和客户交互日志中提取信息，为支持代理或自动聊天机器人提供有效且高效回答客户询问所需的信息。

法律和合规性审查

对于需要审查大量法律或监管文件的用例，RAG 可以根据查询快速检索相关文档，从而帮助进行合规性检查或法律研究。

医疗信息系统

在医疗保健领域，RAG 可以通过检索与医生查询或特定医疗状况相关的患者记录、科学研究和临床试验结果来支持诊断过程、患者管理和医学研究。

这种架构具有巨大的力量。然而，向量数据库的动态性质及其独特的数据处理机制带来了安全挑战，开发团队必须解决：

嵌入可逆性

虽然向量数据库中的嵌入是抽象的数值表示，但存在风险，即复杂的技术可能逆向工程这些嵌入，揭示其来源的敏感信息。例如，从机密文件中创建的嵌入可能具有独特的模式，这可能会暗示文档的内容。

通过相似性搜索的信息泄露

相似性搜索，向量数据库的核心优势，在敏感数据泄露的背景下可能构成风险。攻击者通过分析基于邻近度的查询结果，可能会推断出关于数据集的某些敏感方面。例如，如果用户发现特定的查询产生接近匹配的结果，他们可能会推断嵌入背后的数据性质或具体细节。

数据粒度和向量表示

根据嵌入的粒度，向量空间中的特定模式或簇可能会间接泄露关于数据性质的信息。例如，如果特定的数据点总是聚集在一起，这可能会揭示原始数据的关系或特征。

与其他系统的交互

通常，向量数据库不是独立的，而是与其他系统交互。系统之间嵌入或派生向量的流动可能成为暴露点，特别是如果数据来源和流动没有安全地管理。

总之，虽然向量数据库通过提供基于细微差异的相似性方法来增强 LLM 的功能，但密切关注敏感数据泄露的潜在途径至关重要。如果未能充分保护，这些数据库的强大功能可能会被恶意行为者利用。了解这些风险并采取主动措施对于维护他们管理的数据的完整性和机密性至关重要。

### 降低数据库风险

以下是一些最佳实践和缓解策略，用于降低将 LLM 连接到数据库时敏感数据泄露的风险：

基于角色的访问控制（RBAC）

确保 LLM 对数据库的访问受到限制。仅授予必要的权限，避免给予 LLM 全面访问权限。通过使用角色，你可以确保 LLM 只能获取它绝对需要的信息。

数据分类

根据敏感性（公开、内部、机密、受限）对数据进行分类。确保 LLM 无法访问或仅有限、清洁访问高敏感性数据类别。

审计跟踪

记录应用程序发出的每个数据库查询。定期审查这些日志以识别模式、异常或未预期的数据访问。

数据删除和屏蔽

对于敏感字段，考虑使用删除（完全隐藏数据）或屏蔽（模糊部分数据）来限制敏感数据的暴露。

输入清理

确保 LLM 处理以访问数据库的任何查询或输入都经过清理并检查，以防止 SQL 注入或其他数据操纵攻击。

自动数据扫描器

使用自动化工具扫描并标记敏感信息，确保在 LLM 访问之前，此类数据被移除或得到充分保护。

使用视图而不是直接访问表

对于关系型数据库，考虑为 LLM 提供访问经过清理的视图，这些视图是表的清洁版本，而不是直接访问实际原始表。

数据保留政策

实施政策，规定数据库应保留某些数据的时间长度。定期清除不再需要的旧数据，以减少潜在的数据泄露足迹。

# 从用户交互中学习

虽然简单的 LLM 不会根据使用情况修改其行为，但现在我们看到越来越多的场景中，开发者添加了这种能力。通过处理查询、反馈或其他形式的用户输入，LLM 可以完善其理解，提供更准确的响应，甚至随着时间的推移学习新信息。这种动态互动使 LLM 能够保持更新，从用户反馈中学习，并根据个人或集体用户偏好调整其响应，从而提高用户体验和 LLM 在实际应用中的实用性。

在第一章中，我们看到了直接将不受信任的用户输入纳入 LLM 知识库的风险之一。在这种情况下，微软的 Tay 学会了有毒语言和偏见。然而，还存在与敏感数据相关的另一组风险。

当 LLM 持续与不同用户互动时，可能会无意或有意地涌入大量敏感数据。虽然 LLM 的学习能力确保其随着时间的推移不断进化并变得更加高效，但这种持续学习在数据保护方面也可能成为其致命弱点。用户互动的本质，即多样性和不可预测性，意味着用户可能会输入或引用个人、机密或专有信息。

例如，考虑一位商业高管使用 LLM 来撰写信息。他们可能会向系统提供一些机密商业策略的片段，期望得到更精炼的输出。我们在三星和其他大型企业中看到了这种情况的真实案例。或者，用户可能会向 LLM 查询个人医疗症状，希望了解潜在状况。在这两种情况下，用户都与应用程序分享了敏感数据。如果您未来使用这些数据进行训练或存储以供实时访问，这些信息可能会成为 LLM 内部知识结构的一部分，或者您的应用程序可能会将其存储以供将来参考。

此外，用户交互的挑战在于 LLM 可能只在某些时候识别到敏感数据。而人类可能会意识到社会保险号码、专有公式或独特商业策略的重要性，而 LLM 可能会将其视为另一条信息。这种缺乏理解可能导致 LLM 在稍后由另一用户就相关主题进行查询时，无意中泄露之前输入的敏感信息片段。

此外，随着多模态 LLM（能够处理文本、图像、音频和视频）的兴起，敏感数据泄露的潜在风险也随之增加。用户可能输入一张照片进行图像识别，却未意识到背景中包含可识别的信息或受版权保护的材料。

为了解决这些问题，采用以下缓解策略：

清晰的沟通

用户应了解 LLM 的学习能力和数据保留政策。一个关于不分享个人或敏感信息的初始免责声明可能是有帮助的。

数据净化

实施算法，在处理之前识别并删除用户输入中可能存在的 PII 或其他敏感数据。

临时记忆

考虑给 LLM 一个用于用户特定信息的临时记忆，系统在会话结束后自动删除，确保没有长期保留敏感数据。

无持续学习

设计 LLM，使其不会持续地从用户交互中学习，从而最大限度地减少内部化敏感数据的风险。

# 结论

本章的核心问题是“你的 LLM 知道得太多吗？”答案显然是肯定的。我们需要我们的 LLMs 能够访问信息以提供帮助。然而，我们必须仔细评估我们提供给这些系统的信息类型，并通过一个透镜来审视这些信息，问自己，“如果这些信息被披露会发生什么？”如果无意泄露的惩罚过高，那么你必须仔细权衡在训练或装备模型时使用此类数据的风险。

我们研究了 LLMs 获取其庞大知识的三条主要途径：训练、检索增强生成和用户交互。每种方法在防止意外数据泄露时都伴随着其自身的优势和独特的挑战。获得的关键见解包括：

训练

LLM 的基础。虽然训练为 LLMs 配备了大量知识，但仔细审查训练数据、消除任何 PII、专有见解或争议内容的痕迹是至关重要的。定期的审计和采用数据净化策略是不可或缺的。

检索增强生成

LLM 与网络上无结构数据汪洋之间的桥梁。实时数据的强大力量伴随着过滤掉敏感或误导性信息的责任。在访问 API 或数据库时，设置严格的访问控制至关重要。

从用户交互中学习

最动态的知识来源。每个用户查询都可能揭示个人或公司机密。为此，需要明确与用户沟通、数据净化和谨慎使用持续学习。

总之，你的 LLM 处理大量知识库的能力可能具有重大价值，但危险也可能隐藏在这里。关键在于在赋予 LLM 能力的同时确保它们不会无意中“知道太多”。本章致力于理解这种微妙的平衡，希望引导读者负责任地利用 LLMs 的力量，确保它们既是强大的工具也是敏感信息的值得信赖的守护者。
