# 第六章。语言模型会梦见电子羊吗？

在所有关于 LLM 进步的兴奋中，所谓的*幻觉*现象最能吸引和困惑人。几乎就像这些计算实体在其众多层中偶尔会进入一种梦幻般的状态，创造出奇妙而令人困惑的叙述。就像人类的梦境一样，这些幻觉可以是反思性的、荒谬的，甚至是预言性的，为训练数据和模型学习解释之间的复杂相互作用提供了见解。

在 LLM 的世界里，“幻觉”这个术语可能会让人联想到生动而异想天开的创造，但现实中，它指的是一种更常见的统计异常。本质上，幻觉是模型试图利用从其训练数据中汲取的规律来弥合其知识差距。虽然这可能被称为“富有想象力的”，但当面对不熟悉的信息或场景时，LLM 实际上是在做出一个有根据的猜测。然而，这些猜测可能表现为自信但无根据的断言，揭示了模型在区分所学事实和训练数据中的统计噪声方面的努力。

LLM 不提供像某些其他“预测”AI 算法那样易于使用的概率分数。例如，一个视觉分类器算法可能会以百分比的形式返回一个概率。它可能会显示有 79%的可能性，某个特定的图像描绘了一只猴子。因此，该模型的用户可以感受到模型对预测的“感觉”有多强烈。LLM 只是预测序列中的下一个或几个标记。虽然 LLM 使用复杂的统计模型来完成这项任务，但通常整个响应的确定性分数不是输出的一部分。这可能会让最终用户不确定 LLM 返回的是对提示的稳固反应还是薄弱的、基于统计的推断。

###### 注意

对于一些人来说，“幻觉”这个术语不受欢迎，因为它将 LLM 人格化，使得其缺陷看起来不那么关键。现在的一些文献将这种现象称为*虚构*。然而，幻觉更为普遍，所以在这本书中我们将使用它。

在 LLM 输出中，事实与虚构之间的这种微妙舞蹈将我们引向挑战的核心：*过度依赖*。作为人类，我们天生倾向于信任那些自信呈现的结果，尤其是当它们来自复杂的计算机软件时。然而，正是这种信任可能会将我们引入歧途。当 LLM 产生幻觉时，它们通常不会在自信上动摇，这使得很难区分真正的知识和不完美的统计伪象。危险在于幻觉，也在于我们倾向于将这些梦幻般的陈述视为理所当然，这可能导致错误信息、失误，并在现实世界的应用中产生更广泛的影响。

###### 注意

过度依赖指的是对 LLM 阐述的能力和精确度的过度信任。在 LLM 输出中，尤其是当存在幻觉、错误或偏数据输入时，过度自信可能导致有害的输出，尤其是在专业或安全关键的环境中。一个显著的例子是信任 LLM 提供医疗建议，而没有足够的测试。

# 为什么 LLM 会幻觉？

幻觉的核心原因在于 LLM（大型语言模型）的操作机制，该机制侧重于模式匹配和统计外推，而不是事实验证。虽然它们通过在大规模训练数据集上进行训练来获取知识，但 LLM 通常缺乏具体、实际的知识。它们的操作根植于识别输入数据中的模式，并试图将这些模式与训练期间学习到的模式相匹配。这种模式匹配在没有现实世界理解的情况下发生，可能导致生成幻觉文本，尤其是在面对模糊或新颖的输入提示时。

训练数据的质量和性质显著影响幻觉的可能性和程度。训练数据中的偏差、不准确或噪声可能导致模型生成有偏见或不正确的文本。

幻觉在使用 LLM 进行关键或敏感应用时提出了重大挑战。它们突显了 AI 发展的固有复杂性和挑战，聚焦于统计模式匹配与现实世界、事实理解之间的差距。LLM 中的幻觉现象为更广泛地讨论在现实场景中部署大规模 AI 模型（没有强大的事实验证或上下文理解机制）的限制和伦理影响打开了窗口。

# 幻觉的类型

随着我们进一步探讨这个问题，让我们看看我们可能会遇到的一些幻觉类型。这样做将帮助我们理解其影响和缓解措施：

事实不准确

由于模型缺乏具体知识或对训练数据的误解，LLM 可能会产生事实不准确的陈述。

无依据的断言

与事实不准确类似，LLM 可能会生成无根据的断言，这在敏感或关键环境中可能是有害的。

能力误代表

即使 LLM 并不真正理解高级主题，如化学，它们也可能给人以理解这些主题的错觉。它们可以令人信服地就某个主题进行双向讨论，误导用户对其理解程度的认识。

矛盾的陈述

LLM 可能会生成与先前陈述或用户提示相矛盾的句子。例如，它们可能会首先声明，“猫害怕水”，然后又声称，“猫喜欢在水中游泳”。

考虑到这些因素，让我们看看现实世界的例子及其对应用提供商和客户的影响。

# 示例

在本节中，我们将考察四个案例，其中幻觉与过度依赖相交并造成了损害。这些案例应该有助于强调在您的 LLM 应用中解决这些问题的必要性。

## 虚构的法律先例

在 2023 年，在美国联邦法院，一位法官对两位律师及其律师事务所因在法律实践中的疏忽监管进行了罚款。律师在航空伤害案件中提交了虚构的法律研究。结果证明，这些虚构的案件法是由 ChatGPT 生成的。

这个问题是在一次常规法律程序中暴露出来的，当时反对派发现律师提供的法律引用不仅错误，而且完全是虚构的。律师在研究时使用了通用的 LLM，该 LLM 没有特定的法律培训或数据访问权限。他们对 AI 输出的未经验证的依赖导致了在法律简报中提交了六个虚构的案件引用。法官后来判定这一行为是不诚实的行为。这一行为的后果不仅限于法庭，而且在法律和技术社区中产生了共鸣，标志着围绕 AI 在法律实践中的作用的讨论中的一个重大事件。

由于法官对律师及其公司施加了巨额罚款，这一事件成为了一个关于在关键领域过度依赖 AI 的警示故事。它展示了人类验证和尽职调查的必要性，特别是在准确性和真实性至关重要的领域。

让我们看看这一事件对几个不同方面产生的影响，以确保我们能全面了解由此引发的问题：

关于 LLM 提供商

这一事件突出了在关键和正式领域，如法律实践中使用 OpenAI 产品的潜在风险。它提出了关于 ChatGPT 的可靠性和安全使用的疑问，并可能影响了 OpenAI 的声誉。在法律环境中滥用 ChatGPT 可能会促使立法者进一步审查，并要求对 OpenAI 产品在关键领域的使用和部署实施更严格的监管。

关于 LLM 客户

对于涉及的律师来说，后果是立即且严重的。他们面临了罚款，并且他们的专业声誉受到了严重影响。这一事件对其他法律专业人士来说是一个威慑，使他们警惕在没有彻底验证的情况下依赖 AI 工具进行关键任务。

关于法律职业

这一事件在法律职业中产生了回响，强调了人类验证的重要性以及无条件的信任 AI 生成内容的风险。它突显了教育并警告法律专业人士关于 AI 工具在法律实践中的局限性和正确使用的一个迫切需要。

从根本上讲，这一事件强调了验证的不可或缺价值。法律专业人士，以及所有 AI 工具的使用者，都应投资于验证 AI 工具生成的信息。此外，这一事件突显了制定规范 AI 在法律实践和其他关键领域使用的稳健指南的必要性。建立此类政策，包括确保 AI 生成信息准确性和可靠性的验证程序，将作为防止类似事件的防御工事。这个故事还强调了推广 AI 工具道德使用的必要性。提高对潜在滥用的认识，并强调在执行关键任务时遵守专业标准的重要性，成为一项关键教训。

如 OpenAI 之类的 LLM（大型语言模型）提供商应提供更好的指南、警告和教育，关于其 AI 工具的正确使用和限制，以防止滥用并确保用户充分了解其功能和潜在风险。最后，这一事件突显了持续改进的必要性，敦促 AI 软件开发人员和法律界从他们的错误中学习，并提高他们在关键应用中工具的安全性和可靠性。通过这样的反思视角，这一事件为培养基于验证、教育和道德实践的负责任 AI 使用文化提供了路线图。

## 航空公司聊天机器人诉讼

在 2024 年的一项里程碑式裁决中，加拿大最大的航空公司加拿大航空公司（Air Canada）被命令赔偿一位客户，因为聊天机器人提供了关于票价的不正确信息。在此案中，居住在不列颠哥伦比亚省的贾克·摩夫特（Jake Moffatt）向加拿大航空公司的聊天机器人咨询有关丧葬票价所需文件以及能否获得追溯性退款的可能性。根据聊天机器人提供的信息，摩夫特购买了一张全价票，相信他可以在以后获得退款。然而，当他申请退款时，加拿大航空公司拒绝了他的请求，声称丧葬票价不适用于已完成旅行的行程，这与聊天机器人的指导相反。

摩夫特（Moffatt）在加拿大航空公司未能履行聊天机器人的信息后，对其提起了法律诉讼，以追回票价差额。加拿大航空公司的辩护称聊天机器人是一个“独立的法律实体”，应对其自身行为负责，但这一立场被法官判定为不合逻辑且不负责任。

法官命令加拿大航空公司支付摩夫特全价票和丧葬票价之间的差额，以及利息和费用。法官强调，无论通过聊天机器人还是静态页面，在加拿大航空公司的网站上提供的信息都是航空公司的责任。

让我们从几个不同的角度来分析这一案例的影响：

关于加拿大航空公司

这起事件引发了公众和法律的广泛关注，挑战了航空公司对客户互动中人工智能的运用方法。它突显了准确的人工智能生成通信的必要性以及人工智能错误可能带来的声誉损害。

关于人工智能和法律先例

此案在商业运营中人工智能通信的法律责任方面树立了先例。它提出了关于公司可以或应该对人工智能生成内容承担多大或何种责任的问题。

关于消费者和人工智能

这项裁决强化了数字时代的消费者权利，强调公司不能免除对人工智能生成错误的责任。

此案强调了在 LLM 生成内容中准确性至关重要的意义，以及不准确可能导致公司遭受重大财务和声誉惩罚的法律先例。这一裁决强化了企业不能否认其 LLM 应用程序输出的观点，并且必须像对待任何其他官方企业通信一样仔细审查人工智能通信。公司必须确保对其人工智能工具进行严格的测试和持续的监控，以避免潜在的法律责任并维护消费者信任。此外，此案凸显的财务影响提醒我们，此类错误信息直接相关的成本。

## 无意中的人格诽谤

2023 年，澳大利亚赫本郡市长布莱恩·霍德威胁要对 OpenAI 提起诉讼，因为 LLM 生成的一项诽谤性指控。ChatGPT 错误地断言，霍德当时是外国贿赂丑闻中的告密者，曾服过刑。根据诉讼，AI 呈现的这项虚构信息，作为事实呈现，对霍德的声誉造成了重大影响，并给他带来了痛苦。

问题可能源于 ChatGPT 在此领域有限的训练数据。如果没有 LLM 能够访问与用户查询强相关数据，LLM 可能会将不相关的信息片段混淆在一起，从而导致了关于霍德的明显错误指控。这一事件强调了不加批判地依赖人工智能生成信息的潜在危险，尤其是在像公共声誉这样的敏感领域。

我们可以通过从原告和被告的角度来更好地理解此案的影响：

霍德

这项虚假声明给霍德带来了心理痛苦，并威胁到他的政治生涯。这一事件突显了个人容易受到人工智能生成错误信息的影响以及声誉损害的可能性。

OpenAI

公司已经使自己面临昂贵且耗时的诉讼。在此案中，原告在提起诉讼时表示，他可能寻求超过 20 万美元的赔偿。

理解这些影响使我们得出以下三个可以在项目中应用的教训：

验证

无论是通过事实核查工具、人工监督还是两者的结合，强大的验证机制至关重要。用户必须对 AI 生成信息保持健康的怀疑态度。

教育

教育用户关于 LLM 的能力和局限性对于促进负责任和道德的使用至关重要。

监管

在关键领域使用 LLM 可能需要监管框架，以确保数据隐私、算法问责制和用户保护。《休德案》突出了在 AI 责任和责任方面进行法律澄清的潜在需求。

布莱恩·休德案是 LLM 中幻觉和过度依赖潜在陷阱的例证。它呼吁采取更稳健的安全措施、用户教育和负责任地应用这项强大的技术。只有通过多管齐下的方法，我们才能防止未来的伤害并确保 AI 有益地融入社会。

## 开源包幻觉

这起事件围绕使用 LLM 作为编码助手展开。现在，开发者使用 LLM 来辅助他们编写代码已经变得司空见惯。开发者可能会使用通用聊天机器人，如 ChatGPT，或者专门的副驾驶，如 GitHub Copilot。2023 年 6 月，GitHub 进行的一项[调查](https://oreil.ly/tcy1y)显示，92%在大型公司工作的开发者正在使用 LLM 来帮助他们编码。本节将探讨使用这些代码生成工具的风险，包括幻觉和过度依赖的显著例子。

这些天，大量编写的代码使用了开源库。这包括由 AI 编码助手编写的代码，这些助手可能会利用现有的开源库来使代码更加紧凑或高效。通常情况下，这没问题，但有些情况下，这些助手已被证明对各种开源库的存在产生了幻觉。他们想象出一个有用的库来解决问题，并生成使用这个想象中的库的代码。这看起来可能无害，但在 2023 年，Vulcan Cyber 的研究团队展示了[黑客如何利用这个漏洞将恶意代码插入应用程序](https://oreil.ly/oULNb)。他们将这个问题简单地称为“AI 包幻觉”。

在这种情况下，研究团队通过搜索流行的 Stack Overflow 问题并要求 ChatGPT 解决这些问题来构建攻击。他们很快发现了一个由助手机器人建议的超过 100 个幻觉包，这些包尚未在任何流行的代码仓库中发布。因为这些是基于流行问题的，许多其他开发者可能会要求他们的 AI 助手生成类似的代码，这可能包括相同的幻觉。

要利用这种幻觉，攻击者只需要创建幻觉包的恶意版本，将它们上传到流行的代码仓库，然后等待一个毫无戒心的开发者根据 AI 的建议下载并运行此代码。

###### 警告

2024 年 3 月，Lasso Security 团队对此研究进行了跟进，发现他们向一个流行的模型提出的编码问题中，高达 30%的结果至少包含一个幻觉包！

开发者从在线搜索编码解决方案转向向 ChatGPT 等 AI 平台寻求答案，这为攻击者创造了有利可图的机遇。这种情况表明了一个严重的安全问题，因为它展示了攻击者利用 AI 技术传播恶意代码的新途径，从而损害了软件应用的完整性和安全性。尽管这种漏洞已被广泛报道，但其在野外被利用的程度尚不清楚。然而，它是一个重要的例子，说明了幻觉和过度依赖如何结合在一起，使组织面临风险。

这起事件揭示了几个关键教训。首先，它强调了严格验证 AI 生成输出的必要性，尤其是当这些结果可能影响软件开发或其他关键任务操作时。必须要有机制来验证 AI 推荐包的真实性和安全性。其次，它强调了持续监控和更新 AI 系统的重要性，以减轻与过时或不准确训练数据相关的风险。最后，它呼吁 AI 和网络安全社区共同努力，制定检测和预防未来此类利用途径的策略。通过从这些事件中学习，利益相关者可以共同努力构建更强大、更安全的 AI 驱动平台，以抵御不断发展的威胁环境。

# 谁负责？

与大型语言模型（LLMs）合作的开发团队有时会将幻觉造成的损害视为“人为问题”，他们指责用户误解或误用了提供的信息。毫无疑问，用户教育非常重要。正如人们学会了他们不能信任在网络上找到的所有信息一样，人们将变得更加成熟，以检查聊天机器人或辅助驾驶员给他们提供的错误信息。

然而，作为开发者，我们负责确保我们的软件提供的信息尽可能准确。这种错误信息的涟漪效应可能非常深远，尤其是在医疗保健、法律或金融等高风险领域。这强调了开发者投资于识别和纠正幻觉或错误信息，在它们到达用户之前进行纠正的必要性。

作为开发者，我们的职责不仅限于创建复杂的 AI 系统。它包括培养一个安全可靠的环境，让用户可以与 AI 互动，并有一定程度的准确性和可靠性保证。这项责任需要多方面的方法：改进系统以减少幻觉，实施强大的输出过滤机制以捕捉和纠正错误，以及培养持续改进和从过去的错误中学习的文化。此外，教育用户关于 LLM 的潜在局限性和可靠性的程度至关重要。这有助于培养一个了解的用户群体，他们可以明智地与 AI 系统互动，同时注意风险。

本章讨论的案例研究说明了不同的法律责任。在涉及律师使用 ChatGPT 生成的虚构法律先例的案例中，法院将责任明确地放在了专业人士身上。作为高级用户，律师被期望在提交法律文件之前验证信息的真实性。他们未能做到这一点导致了重大的后果，突显了在利用 AI 工具时专业审慎的至关重要性。

相反，加拿大航空聊天机器人的案例导致公司因向消费者提供误导性信息而承担责任。这个案例强调了公司，尤其是在面向消费者的角色中，必须确保其输出是准确可靠的。仲裁庭的决定反映了日益增长的司法共识，即公司不能推卸对 AI 生成内容的责任，强化了企业必须保护其系统与消费者互动的期望。这些案例共同强调了在使用 AI 时需要明确的指南和问责制，无论用户的复杂程度如何。

# 缓解最佳实践

幻觉是不可避免的。这是当前 LLM 技术的固有属性。作为应用开发者，我们的任务有两方面。首先，我们应该努力通过我们的应用来最小化幻觉的可能性，其次，我们希望在幻觉发生时减少其造成的损害。让我们看看有哪些选择。

## 扩展特定领域的知识

在 LLM 的世界里，特定领域的知识不仅仅是锦上添花；它对于最大化效用和最小化幻觉风险通常是至关重要的。当我们将 LLM 聚焦于特定领域——无论是医疗保健、法律、金融还是其他任何领域——它都有潜力提供更准确和上下文相关的信息。这种专业化的关注可以极大地减少模型做出错误或误导性陈述的可能性，这是幻觉的标志。

在前一章中，我们讨论了为你的大型语言模型（LLM）配备危险、有偏见或特权信息的风险。虽然那一章强调了通过最小化数据暴露来避免这些陷阱，但你必须让你的模型访问更多特定领域的、事实性的知识，以减少幻觉。

### 模型微调以实现专业化

微调是 LLM 应用的一个强大工具，可以在利用基础模型中封装的广泛知识的同时，为你的特定用例添加一层专业化。与从头开始训练模型相比，你可以在相对较低的计算和财务成本下实现这种通用和专业化知识的平衡。主要好处是？你获得了一个更可靠、更特定领域的 LLM，量身定制以满足你应用的独特需求。

微调过程有助于缩小 LLM 的范围，使其更符合你的特定领域目标。微调优化了模型的效用，是减轻幻觉的关键缓解策略。模型越专业化，生成错误或不合时宜的幻觉形式的不正确或离题响应的概率就越低。

通过微调你的基础模型，你实际上将其转变为一个专家。这种更高层次的专业化使 LLM 在关键操作中更加值得信赖，无论是医疗诊断、法律解释还是财务分析。微调是实现减轻幻觉风险和减少其影响的双重目标的重要策略，从而使你的 LLM 应用更加稳健和可靠。

### RAG 以增强领域专业知识

RAG 为 LLM 的能力引入了新的复杂层次。它结合了基于检索的模型和序列到序列生成模型的优点。开发者使用一个经过验证、可靠的检索信息技术，如搜索引擎或数据库，来收集与用户需求相关的信息。然后，这些信息可以作为提示的一部分被输入到 LLM 中。这种混合方法增强了模型的环境意识，提高了准确性，并为生成内容的来源提供了一个机制，从而有助于提高可信度。

当你将你的 LLM 微调为特定领域的专家后，下一步合乎逻辑的步骤是为它配备最佳可用参考材料，就像现实世界中的专业人士一样。医生、律师和其他专家很少仅依赖他们的记忆；他们拥有丰富的图书、期刊和数据库，以获取最新和最准确的信息。

在你特定领域的 LLM 应用中实施 RAG（检索增强生成）相当于给它提供了一个虚拟图书馆，里面充满了专业知识。这个精选资源可以包括教科书、研究论文、指南或其他可信材料，这些材料可以指导模型生成响应。RAG 与微调相结合，增强了应用的效用和可靠性，并最大限度地减少了与幻觉和过度依赖相关的风险。

###### 注意

并非所有 LLM 的错误陈述都应该被归类为幻觉。大多数专家用于定义幻觉的核心定义涉及 LLM 的低置信度标记序列预测以一种高置信度的形式被陈述。然而，LLM 的错误陈述也可能源于错误的数据或从数据库或网页检索到的错误数据，甚至在 RAG 过程中也可能源于其他更传统的编码错误。

## 思维链提示以增加准确性

在微调你的模型并使用 RAG 增强其特定领域专业知识之后，减少幻觉并提高可靠性的另一个选项是*思维链*（CoT）推理。正如我们已经建立的，幻觉可能导致误导或危险的结果，而思维链推理通过增强 LLM 的逻辑推理能力，提供了一种结构化的方法来对抗这个问题。

思维链推理鼓励 LLM 遵循逻辑步骤序列或推理路径。开发者不是仅基于即时输入生成响应，而是提示 LLM 考虑中间推理步骤，将复杂问题分解为子问题并系统地解决它们。思维链在复杂任务中特别有益，例如医疗诊断、法律推理或复杂的技术故障排除，在这些任务中，任何失误都可能产生严重后果。

思维链（CoT）推理的好处包括：

减少幻觉

采用结构化的推理方法可以显著降低与幻觉相关的风险。

提高准确性

当一个大型语言模型（LLM）逐步推理问题时，得出准确解决方案的可能性更高。

自我评估

思维链推理使 LLM 能够评估其推理过程，在过程中识别和纠正错误。这种自我评估行为增加了生成内容的可靠性，从而降低了过度依赖模型输出的风险。

让我们通过一个简单的例子来帮助说明这个概念。

```py
Simple Prompt: What is the total cost of 3 notebooks and 2 pencils if 
one notebook costs $2 and one pencil costs $0.50?
```

一个模型可能会错误地加总数字，而没有考虑每个项目的数量和价格，从而导致答案不准确。

```py
Chain of Thought Prompt (CoT): First, calculate the total cost of the 
notebooks by multiplying the cost of one notebook, which is $2, by 3\. 
Then, calculate the total cost of the pencils by multiplying the cost 
of one pencil, which is $0.50, by 2\. Finally, add both totals together 
to get the final cost.
```

通过将问题分解为连续步骤并明确指导模型通过计算的每一部分，CoT 提示有助于确保模型考虑问题的所有部分及其相互作用，从而得出更准确的响应。模型更有可能正确应用乘法来计算每个项目的数量，然后在最后一步将总数相加。

###### 小贴士

如何使用 CoT 的复杂示例越来越多。这包括“零样本”技术，要求 LLM 创建解决复杂问题的详细步骤。研究正在进行且进展迅速，因此请查阅当前文献，了解这一有前景领域的最新进展，以减少幻觉并提高准确性。

CoT 推理作为减少幻觉和最大化可靠性的多方面策略，补充了微调和 RAG。通过分层这些技术，开发者可以显著提高 LLM 应用的鲁棒性，确保它们更适合复杂和关键任务。

## 反馈循环：用户输入在减轻风险中的力量

虽然实施各种技术解决方案，如微调、RAG 和 CoT 推理可以显著提高 LLM 应用的可靠性，但记住，最终用户往往提供了对系统性能最有价值的见解。建立反馈循环允许用户标记问题性或误导性输出，增加一层安全性和质量保证。收集反馈的方法有几种：

标记系统

集成一个简单的界面，用户可以在其中标记不准确、有偏见或问题性回答。您使此过程越简单，用户参与的可能性就越大。

评分量表

除了标记之外，还提供评分量表，让用户评估回答的准确性或有用性。这些定量数据将有助于您对模型进行持续评估。

评论框

为愿意提供更详细反馈的用户提供可选的评论框，描述他们发现输出中哪些部分具有误导性或问题性。

一旦收集到反馈，就需要系统地分析以了解：

重复性问题

在特定领域或查询类型中，是否存在幻觉或不准确性的模式？

严重性

错误是一个小的不便，还是可能引发严重后果？

潜在原因

这些问题可能是什么原因造成的？是缺乏特定领域的知识，还是推理过程有缺陷？

基于这一分析，开发团队可以：

进一步微调

利用反馈来提高模型在特定领域的性能或通用推理能力。

增强 CoT 推理

如果反馈表明模型在逻辑推理方面失败，请考虑使用更针对性的 CoT 提示或监督推理增强。

增强 RAG 中的参考资料

如果模型在特定领域的答案持续不准确，可能需要更新或扩展 RAG（检索增强生成）参考材料。

反馈循环不是一个一次性解决方案，而是一个持续的过程。持续与用户群体互动并根据其反馈调整你的模型确保系统持续改进。这种适应性方法提高了应用程序的可靠性，并有助于维持用户信任。

## 清晰传达预期用途和限制

在我们应对缓解幻觉和改进 LLM（大型语言模型）能力复杂性的过程中，我们必须认识到在应用程序开发中透明度的重要性。一个 LLM 可能是一项技术的奇迹，但它远非完美。关于其预期用途、优势和局限性的清晰、坦率的沟通不仅符合道德规范，而且是建立信任和管理用户群体期望的必要方面。

首先，让我们回顾一下预期用途文档可能重要的领域：

预期用途

明确说明你设计应用程序要实现的目标。它是为法律专业人士设计的专用工具，还是一款通用助手？了解应用程序的范围有助于用户做出明智的决定，以最佳方式使用它。

局限性

承认 LLM 的局限性，包括它可能没有特定领域专业知识或幻觉风险较高的领域。明确说明你从应用程序的预期使用范围内排除的内容。

数据处理

分享你的数据保护和隐私协议。明确说明用户数据将如何存储、处理和保护。

反馈机制

通知用户你有一个用于持续改进的反馈循环，并解释他们如何参与这一过程。

一旦你决定要向用户传达的项目，以下是一些有效的沟通方式：

用户界面

使用应用内的工具提示、弹出窗口或常见问题解答（FAQ）提供关于模型预期用途和限制的快速提醒或解释。

文档

创建详细的指南或手册，用户可以参考以获取有关系统可以做什么和不能做什么的更多信息。

初级教程

当用户首次与应用程序互动时，提供操作演示或教程，重点关注展示其功能和限制。

更新日志

维护一个版本历史或更新日志，让用户可以看到已进行的改进和正在解决的问题。

透明度不仅仅是单次事件。随着你的模型的发展——提高其能力、扩展其特定领域的知识、增强其推理能力——及时向用户社区通报这些进展至关重要。同样，如果发现新的限制或漏洞，应尽可能及时和透明地沟通。

透明度对用户和开发团队都有益，因为它可以培养一个更加投入和宽容的用户群体。当人们了解工具的局限性时，他们不太可能滥用它，更有可能提供可以用于进一步改进的建设性反馈。透明度是一项道德义务，也是应用开发者和用户之间互惠互利关系的基石。

## 用户教育：通过知识赋权用户

就像高级反钓鱼软件本身无法完全防止钓鱼攻击一样，技术缓解措施只能最小化 LLM 幻觉和过度依赖的风险。人类意识和教育是至关重要的额外防御层。企业安全团队培训员工识别钓鱼尝试、双重检查 URL 并对未经请求的通信持怀疑态度。同样，虽然我们努力减少对 LLM 的过度依赖，但我们还必须培养一个知情和警觉的用户群体。教育用户关于真实信任问题并为他们提供交叉验证策略是确保他们理解与使用 LLM 相关的局限性和最佳实践的关键。

在构建您的教育计划时，以下是一些建议的主题：

理解信任问题

让用户知道，虽然大型语言模型（LLM）先进且通常准确，但它们并非完美无缺。幻觉现象可能发生，未经验证的过度依赖可能会产生重大后果。

交叉检查机制

教育用户交叉验证 LLM 提供的信息。根据领域不同，这可能包括检查多个可信来源、咨询专家或进行实证测试。

情境意识

鼓励用户评估信息的紧迫性。对于常规或非关键任务，可能可以接受较高水平的信任。然而，您应该鼓励对关键安全、金融或法律工作进行更严格的验证。

反馈选项

让用户了解您应用程序中的反馈循环功能。他们积极参与报告异常情况可以帮助系统持续改进。

以下是一些您可以用来向用户传递教育内容的建议方法：

应用内指南

简短、互动的指南或视频可以在用户使用应用程序时向他们介绍这些概念。

资源库

创建一个包含文章、常见问题解答和如何指南的存储库，详细说明这些主题。

社区论坛

活跃的用户论坛可以帮助快速传播最佳实践和新闻，提供额外的教育和意识层。

邮件营销

定期向用户发送更新，概述新功能、限制或教育材料，确保即使是不常使用的用户也能保持信息畅通。

虽然开发团队专注于技术缓解措施，如微调、RAG 和 CoT 推理，但记住，一个受过良好教育的用户基础也是抵御 LLM 带来的风险的一个强大防线。因此，结合技术进步和持续用户教育的平衡、综合方法是减轻风险和增强可靠性的最佳策略。

###### 警告

在本章最后的讽刺转折中，似乎 LLM 缺乏幽默感现在也成为你必须考虑的风险因素。最近的例子突出了这一特点：谷歌的 LLM 增强搜索功能提供了可疑的建议，例如建议将胶水作为披萨配料，建议吃石头作为营养建议，甚至建议跳桥来治疗抑郁症。这些奇怪的建议追溯到非权威但流行的网站，如 Reddit 和 The Onion。不幸的是，由于缺乏幽默感，LLM 将这些玩笑的结尾当作事实传递。这又是一个你需要考虑的边缘情况。

# 结论

解决过度依赖幻觉倾向的 LLM 带来的损害风险需要全面、多层次的方法。这个挑战最好通过技术进步、积极的用户参与、透明沟通和彻底的用户教育来解决。

第一步是承认问题。你的第一道防线必须是将幻觉降至最低。考虑将你的应用程序的使用范围缩小到特定领域，然后利用微调、RAG 和 CoT 等技术，让你的 LLM 成为世界级的专家。

通过结合技术保障、用户反馈循环、透明沟通和强大的用户教育，减轻过度依赖 LLM 的风险的策略变得全面。这些元素各自有助于降低幻觉的风险，并协同帮助构建一个更具弹性、透明和用户友好的系统。
