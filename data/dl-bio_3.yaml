- en: Chapter 3\. Learning the Logic of DNA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. 学习DNA的逻辑
- en: 'In this chapter, we’ll build a deep learning model to predict whether a DNA
    sequence is bound by a class of proteins called *transcription factors* (TFs).
    Transcription factors play a central role in gene regulation: they bind to specific
    DNA sequences and influence whether nearby genes are turned on or off. By recognizing
    these sequence patterns, we can begin to decode the regulatory logic embedded
    in the genome.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个深度学习模型来预测DNA序列是否被一类称为*转录因子*（TFs）的蛋白质所结合。转录因子在基因调控中起着核心作用：它们结合到特定的DNA序列上，并影响附近基因是开启还是关闭。通过识别这些序列模式，我们可以开始解码基因组中嵌入的调控逻辑。
- en: Unlike the previous chapter—where we used an off-the-shelf protein model from
    Hugging Face—here we’ll start defining and training our own models from scratch.
    This gives us more control and helps us better understand how deep learning works
    on biological data. We’ll explore both convolutional and transformer-based architectures
    and introduce interpretation techniques to help us understand how our models make
    predictions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一章不同——我们使用了 Hugging Face 的现成蛋白质模型——在这里，我们将从头开始定义和训练自己的模型。这让我们有更多的控制权，并帮助我们更好地理解深度学习在生物数据上的工作原理。我们将探索基于卷积和基于转换器的架构，并介绍解释技术，帮助我们理解模型是如何进行预测的。
- en: 'We will tackle this problem in stages, gradually increasing the complexity:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分阶段解决这个问题，逐步增加复杂性：
- en: 1\. Start simple
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 从简单开始
- en: 'First, we’ll train a basic convolutional network to predict whether a DNA sequence
    binds a single transcription factor called CTCF. Its binding behavior is relatively
    easy to predict, making it a great first target. We’ll build the full pipeline:
    loading data, training the model, and checking whether it captures meaningful
    biological signals.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将训练一个基本的卷积网络来预测DNA序列是否结合一个名为CTCF的单个转录因子。其结合行为相对容易预测，使其成为一个很好的起点。我们将构建完整的管道：加载数据、训练模型，并检查它是否捕捉到有意义的生物信号。
- en: 2\. Increase complexity
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 增加复杂性
- en: Next, we’ll scale up to predicting whether a sequence binds any of 10 different
    TFs. We’ll introduce regularization and normalization, improve our evaluation
    metrics, and begin inspecting individual predictions. We’ll also use mutation
    experiments and input gradients to highlight which parts of the sequence the model
    relies on—offering a first step toward interpretability.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将扩展到预测序列是否结合10种不同的TFs。我们将介绍正则化和归一化，改进我们的评估指标，并开始检查单个预测。我们还将使用突变实验和输入梯度来突出模型所依赖的序列部分——这是可解释性的第一步。
- en: 3\. Incorporate advanced techniques
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 集成高级技术
- en: Finally, we’ll try adding transformer layers to explore whether they improve
    performance and continue dissecting model behavior to understand how different
    architectural choices influence learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将尝试添加转换器层来探索它们是否提高了性能，并继续剖析模型行为，以了解不同的架构选择如何影响学习。
- en: This staged approach—building up from simple to more complex—is one we recommend
    in general. It helps keep models interpretable, makes debugging easier, and builds
    confidence along the way.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分阶段的方法——从简单到复杂逐步构建——是我们普遍推荐的方法。它有助于保持模型的可解释性，使调试更容易，并在过程中建立信心。
- en: Before diving in, we’ll do a quick refresher on the biological and machine learning
    concepts that underpin this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入之前，我们将快速回顾一下支撑本章的生物和机器学习概念。
- en: Tip
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: To get the most out of this chapter, open the companion Colab notebook and run
    the code cells as you follow along. Executing the code interactively will deepen
    your understanding and give you space to experiment with the concepts in real
    time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用本章内容，请打开配套的 Colab 笔记本，并按照说明运行代码单元格。交互式执行代码将加深你的理解，并让你有机会实时实验这些概念。
- en: Biology Primer
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物学入门
- en: It’s astonishing that all the instructions for building an entire human body
    are encoded in the DNA of a single cell. Every human starts as one tiny cell—about
    100 micrometers wide—with its DNA tightly packed into a nucleus just 6 micrometers
    across. This DNA acts as the blueprint for processes like cell division and differentiation,
    eventually giving rise to the diverse tissues and cell types that make up an entire
    human body.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，构建整个人类身体的全部指令都编码在单个细胞的DNA中。每个人类都是从一个小小的细胞开始的——大约100微米宽——其DNA紧密地包装在一个直径仅为6微米的细胞核中。这段DNA充当细胞分裂和分化等过程的蓝图，最终产生构成整个人类身体的多种组织和细胞类型。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'A few terminology clarifications: The *genome* refers to the complete set of
    DNA in an organism, including all of its genes and other genetic material. While
    *genetics* typically studies individual genes or small gene sets, *genomics* takes
    a broader view, often analyzing entire genomes across individuals or even species.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 几个术语的澄清：*基因组*指的是一个生物体中完整的DNA集合，包括其所有基因和其他遗传物质。虽然*遗传学*通常研究单个基因或小基因集，但*基因组学*采取更广泛的视角，通常分析个体或甚至物种的整个基因组。
- en: The human genome is vast—more than 3 billion base pairs long—and carries with
    it billions of years of evolutionary history. But what is this molecule, really,
    and how does it encode biological function?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人类基因组非常庞大——超过30亿个碱基对长——并携带了数十亿年的进化历史。但这个分子究竟是什么，它又是如何编码生物功能的？
- en: What Exactly Is DNA?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DNA究竟是什么？
- en: DNA is the molecule of inheritance—the fundamental code of life. Its double-helix
    structure was first revealed in 1953 by Watson, Crick, and Franklin, marking a
    pivotal moment in the biological sciences. Nearly half a century later, the first
    complete draft of the human genome was published in 2001,^([1](ch03.html#id616))
    laying the foundation for modern genetics and genomics. But these milestones are
    relatively recent, and while we now know a great deal about *what* is in the genome,
    we still understand surprisingly little about *how* it actually works.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DNA是遗传的分子——生命的根本代码。其双螺旋结构最早在1953年由沃森、克里克和富兰克林揭示，标志着生物科学的一个转折点。近半个世纪后，2001年发表了人类基因组的第一份完整草图^([1](ch03.html#id616))，为现代遗传学和基因组学奠定了基础。但这些里程碑相对较新，尽管我们现在对基因组中“有什么”了解很多，但我们仍然对它“如何”实际工作了解得惊人地少。
- en: 'We do know that DNA is built from four chemical letters, or nucleotide bases:
    `A` (adenine), `C` (cytosine), `G` (guanine), and `T` (thymine). These bases form
    long sequences that carry genetic instructions. The full human genome contains
    around 3.2 billion of these letters, packed into 23 pairs of chromosomes. To fit
    inside the tiny nucleus of a cell, this DNA wraps around proteins and coils into
    compact, highly organized structures known as *chromatin*, as shown in [Figure 3-1](#dna-organization).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实知道，DNA由四种化学字母，或称为核苷酸碱基组成：`A`（腺嘌呤）、`C`（胞嘧啶）、`G`（鸟嘌呤）和`T`（胸腺嘧啶）。这些碱基形成长序列，携带遗传指令。完整的人类基因组大约包含320亿个这样的字母，打包在23对染色体中。为了适应细胞微小的细胞核，这段DNA缠绕在蛋白质上，并卷曲成紧凑、高度组织化的结构，称为*染色质*，如图[图3-1](#dna-organization)所示。
- en: Despite decades of research, the genome remains full of unanswered questions.
    Only about 2% of it directly codes for proteins—what is the rest doing? How can
    all the cells in your body share the same DNA, yet behave so differently? What
    controls when a gene is used, and how do changes in the environment or during
    development affect this process?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管进行了数十年的研究，基因组仍然充满了未解之谜。其中只有大约2%直接编码蛋白质——其余的部分在做什么？你的身体中的所有细胞如何共享相同的DNA，却表现出如此不同的行为？什么控制着基因的使用时机，以及环境变化或发育过程中的变化如何影响这一过程？
- en: These mysteries lie at the heart of gene regulation—and increasingly, deep learning
    is helping us explore them.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些谜团位于基因调控的核心，并且越来越明显，深度学习正在帮助我们探索它们。
- en: '![](assets/dlfb_0301.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0301.png)'
- en: 'Figure 3-1\. DNA is packed into the nucleus in multiple layers of structure.
    Starting with the double helix, it first wraps around histone proteins to form
    nucleosomes (beads on a string), which fold into chromatin fibers, and ultimately
    into chromosomes. Source: [National Institute of Environmental Health Sciences](https://oreil.ly/h8M44).'
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\. DNA以多层结构的形式打包到细胞核中。从双螺旋开始，它首先缠绕在组蛋白蛋白上形成核小体（线上的珠子），然后折叠成染色质纤维，最终形成染色体。来源：[国家环境卫生科学研究所](https://oreil.ly/h8M44)。
- en: Coding and Noncoding Regions
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码区和非编码区
- en: The human genome contains around 20,000 protein-coding genes. These make up
    the *coding* regions—stretches of DNA that are *transcribed* into RNA and then
    *translated* into proteins. Each protein carries out specific tasks, from building
    cellular structures to catalyzing chemical reactions. Together, they perform most
    of the cell’s essential functions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 人类基因组包含大约20,000个编码蛋白质的基因。这些构成了*编码区*——DNA片段被转录成RNA，然后翻译成蛋白质。每个蛋白质执行特定的任务，从构建细胞结构到催化化学反应。共同执行细胞的大部分基本功能。
- en: Yet, protein-coding genes account for only about 2% of the genome. The remaining
    98% is noncoding DNA. While it doesn’t produce proteins, noncoding DNA plays critical
    regulatory roles, helping to control when and where genes are used. In fact, most
    genetic variants associated with human disease fall in noncoding regions—though
    we still have limited understanding of how most of them exert their effects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，编码基因只占基因组的大约2%。剩余的98%是非编码DNA。虽然它不产生蛋白质，但非编码DNA发挥着关键的调节作用，帮助控制基因何时何地被使用。事实上，与人类疾病相关的大多数遗传变异都发生在非编码区域——尽管我们对它们如何发挥作用的理解仍然有限。
- en: Some noncoding DNA produces RNAs that regulate gene expression, while other
    regions help organize the 3D structure of the genome or serve as docking sites
    for regulatory proteins. One especially important category of noncoding region
    is transcription factor binding sites. These are short DNA sequences where *transcription
    factors* (TFs) attach to help regulate gene activity—and they’re the central focus
    of this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一些非编码DNA产生调节基因表达的RNA，而其他区域则有助于组织基因组的3D结构或作为调节蛋白的对接位点。一类特别重要的非编码区域是转录因子结合位点。这些是转录因子（TFs）附着以帮助调节基因活性的短DNA序列——它们是本章的核心焦点。
- en: How Transcription Factors Orchestrate Gene Activity
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转录因子如何调控基因活性
- en: TFs are proteins that control which genes are used, when, and in what context.
    They do this by binding to short, specific DNA sequences called *motifs*, often
    located near genes. By binding these motifs, TFs can activate or repress transcription.
    You can think of them as conductors of a genomic orchestra—directing the performance
    by determining which regions get “played” and when.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 转录因子是控制哪些基因被使用、何时使用以及在使用何种背景下使用的蛋白质。它们通过结合称为基序的短、特定DNA序列来实现这一点，这些基序通常位于基因附近。通过结合这些基序，转录因子可以激活或抑制转录。你可以把它们想象为基因组乐队的指挥——通过确定哪些区域被“演奏”以及何时演奏来指导表演。
- en: TFs are involved in nearly every biological process, from guiding development
    to coordinating how cells respond to stress or infection. Humans have around 1,600
    different TFs, each of which has evolved to recognize specific DNA motifs. These
    motifs are short sequence patterns—typically 6 to 15 base pairs long—that TFs
    preferentially bind to. For example, the well-studied transcription factor CTCF
    binds a core motif with the central pattern `CCCTC`. TFs don’t bind randomly across
    the genome—they search for these preferred sequences.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 转录因子参与几乎每一个生物学过程，从指导发育到协调细胞对压力或感染的响应。人类大约有1,600种不同的转录因子，每种转录因子都进化出识别特定DNA基序的能力。这些基序是短序列模式——通常长度为6到15个碱基对——转录因子优先结合。例如，研究广泛的转录因子CTCF结合一个具有中心模式`CCCTC`的核心基序。转录因子不会随机地跨越整个基因组——它们寻找这些首选序列。
- en: These motifs form specific 3D shapes in the DNA helix, and the protein-binding
    domains of TFs are shaped to complement them—much like a key fitting into a lock.
    In reality, the interaction is often more flexible and dynamic than the analogy
    suggests, but the idea of a physical match still holds.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基序在DNA螺旋中形成特定的3D形状，转录因子的蛋白结合域被设计成与之相匹配——就像一把钥匙插入锁中。实际上，这种相互作用通常比类比所暗示的更加灵活和动态，但物理匹配的概念仍然成立。
- en: To make this interaction more concrete, [Figure 3-2](#fig3-2) shows crystallographic
    structures of different TFs bound to DNA.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这种相互作用更加具体，[图3-2](#fig3-2) 展示了不同转录因子与DNA结合的晶体结构。
- en: '![](assets/dlfb_0302.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0302.png)'
- en: 'Figure 3-2\. Crystallographic structures showing how different types of TF
    binding domains interact with DNA. Each gray structure represents a different
    TF binding domain—zinc finger, homeodomain, helix-loop-helix, and forkhead—bound
    to a double-stranded DNA molecule. These protein segments recognize specific DNA
    motifs by the physical shape those sequences form. Like a key fitting into a lock,
    the structure of the protein complements the shape of the DNA at its binding site.
    Shown here are actual resolved structures from the Protein Data Bank (PDB: 6ML2,
    6KKS, 1NKP, 3G73). Source: [Wikipedia](https://oreil.ly/Nd4Tv).'
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. 展示不同类型的转录因子结合域如何与DNA相互作用的晶体结构。每个灰色结构代表一个不同的转录因子结合域——锌指、同源域、螺旋-环-螺旋和叉头——与双链DNA分子结合。这些蛋白段通过这些序列形成的物理形状识别特定的DNA基序。就像一把钥匙插入锁中，蛋白质的结构与结合位点的DNA形状相匹配。这里展示的是来自蛋白质数据银行（PDB：6ML2，6KKS，1NKP，3G73）的实际解析结构。来源：[维基百科](https://oreil.ly/Nd4Tv)。
- en: 'However, not every matching motif is actually bound in a living cell. In fact,
    the genome contains far more motif matches than actual binding events. That’s
    because binding depends on many additional factors:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非每个匹配的基序在活细胞中实际上都结合了。事实上，基因组中基序匹配的数量远多于实际结合事件。这是因为结合取决于许多其他因素：
- en: Chromatin accessibility
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 染色质可及性
- en: DNA that’s tightly packed into chromatin is harder for proteins to access. TFs
    are more likely to bind in regions of open chromatin.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 紧密包装在染色质中的DNA对蛋白质的访问更困难。TF更有可能在开放染色质区域结合。
- en: DNA methylation
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: DNA甲基化
- en: Certain TFs, like CTCF, are sensitive to methylation (a certain chemical modification
    of DNA bases) at their binding sites, which can block binding even if the motif
    is present.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 某些TF，如CTCF，对其结合位点上的甲基化（DNA碱基的特定化学修饰）敏感，即使基序存在，也可能阻止结合。
- en: Cellular signals
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞信号
- en: Signals inside the cell—such as hormones or stress responses—can activate or
    deactivate a TF’s ability to bind.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞内部的信号——例如激素或压力反应——可以激活或抑制转录因子（TF）结合的能力。
- en: Other proteins
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其他蛋白质
- en: Helper or blocking proteins in the local environment can facilitate or inhibit
    binding.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 局部环境中的辅助或阻断蛋白可以促进或抑制结合。
- en: Cooperative binding
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 协同结合
- en: Many TFs work in complexes or recruit others to stabilize binding and control
    gene activity.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 许多TF在复合物中工作或招募其他因子以稳定结合并控制基因活性。
- en: And perhaps most importantly, real cells are dynamic. Molecules move, concentrations
    change, and TF binding events happen on short timescales. Most genomic datasets,
    by contrast, are static snapshots—freeze-frames of a constantly changing scene.
    That’s worth keeping in mind when interpreting binding data in this chapter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 也许是更重要的是，真实细胞是动态的。分子在移动，浓度在变化，TF结合事件在很短的时间尺度上发生。相比之下，大多数基因组数据集是静态快照——一个不断变化的场景的定格。在解释本章中的结合数据时，这一点值得记住。
- en: Measuring Where Transcription Factors Bind
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量转录因子结合的位置
- en: In deep learning, we’re incredibly reliant on experimental data—we need something
    to learn from. When it comes to studying TFs, that data typically comes from laboratory
    experiments that measure where in the genome a particular TF binds.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们极其依赖实验数据——我们需要一些东西来学习。当涉及到研究TF时，这些数据通常来自实验室实验，这些实验测量特定TF在基因组中的结合位置。
- en: The cornerstone wet lab method is [ChIP-seq](https://oreil.ly/IovSO), or chromatin
    immunoprecipitation followed by sequencing. TFs don’t permanently stick to DNA—they
    bind and unbind constantly. ChIP-seq captures a snapshot of this dynamic process
    by chemically cross-linking (gluing) proteins to DNA, essentially freezing them
    in place. The DNA fragments bound by a specific TF can then be isolated, sequenced,
    and mapped back to the genome to determine where the protein was bound.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 基石湿实验室方法是[ChIP-seq](https://oreil.ly/IovSO)，即染色质免疫沉淀后测序。TF不会永久地粘附在DNA上——它们不断地结合和解除结合。ChIP-seq通过化学交联（粘合）蛋白质到DNA上，本质上将其固定在原位，捕捉了这个动态过程的快照。然后可以隔离、测序并将DNA片段映射回基因组，以确定蛋白质的结合位置。
- en: ChIP-seq data is typically visualized as peaks over the DNA sequence—regions
    of the genome where TF binding was enriched. The height of a peak reflects how
    strong or frequent the binding was at that site, while a flat zero signal means
    no detectable binding occurred there.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ChIP-seq数据通常以峰的形式可视化在DNA序列上——基因组中TF结合富集的区域。峰的高度反映了在该位置结合的强度或频率，而平坦的零信号则表示没有检测到结合发生。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'To simplify the data for modeling, ChIP-seq peaks can be binarized: instead
    of retaining the full quantitative signal, we apply a threshold and record only
    whether the TF was bound in a given region. This reduces the task to a binary
    classification problem—does the TF bind to this DNA sequence or not—which is the
    setup we’ll use throughout this chapter.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化数据以进行建模，ChIP-seq峰可以二值化：不是保留完整的定量信号，而是应用一个阈值，并仅记录TF是否在给定区域结合。这将任务简化为二元分类问题——TF是否结合到这个DNA序列上，这是我们将在本章中使用的设置。
- en: Machine Learning Primer
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: With the biology background knowledge in place, we now review some foundational
    machine learning concepts that we’ll use in this chapter. If you’re already familiar
    with deep learning, feel free to skim this section as a refresher. We’ll briefly
    cover *convolutional* and *transformer*-based architectures, how they can be applied
    to biological sequence data like DNA, and what kinds of insights they can provide.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物学背景知识的基础上，我们现在回顾一些在本章中会用到的基本机器学习概念。如果你已经熟悉深度学习，可以自由地浏览这一节作为复习。我们将简要介绍基于*卷积*和*变换器*的架构，它们如何应用于生物序列数据，如DNA，以及它们可以提供哪些见解。
- en: Convolutional Neural Networks
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '*Convolutional neural networks* (CNNs) are one of the most widely used deep
    learning architectures. Their core strength lies in their ability to automatically
    learn useful patterns from raw, grid-like input data—whether that’s pixels in
    an image or bases in a DNA sequence—without the need for hand-engineered features.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积神经网络*（CNN）是最广泛使用的深度学习架构之一。它们的核心优势在于能够从原始的、网格状输入数据中自动学习有用的模式——无论是图像中的像素还是DNA序列中的碱基——而无需手动设计特征。'
- en: CNNs were originally developed for image recognition tasks. In images, low-level
    features like edges and textures appear in small local patches, while higher-level
    concepts like shapes or objects are formed by combining these local features.
    CNNs mirror this structure by using small, learnable *filters* that slide across
    the input, extracting local patterns at each position. As we stack more layers,
    the model combines local features into more abstract and global representations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: CNN最初是为图像识别任务开发的。在图像中，低级特征如边缘和纹理出现在小的局部区域，而高级概念如形状或物体是通过组合这些局部特征形成的。CNN通过使用滑动在输入上提取局部模式的较小、可学习的*滤波器*来反映这种结构。随着我们堆叠更多的层，模型将局部特征组合成更抽象和全局的表示。
- en: This ability to model hierarchical structure turns out to be useful far beyond
    images. Whenever there are meaningful local patterns in data—like substructures
    in molecules, motifs in DNA, or phonemes in speech—CNNs often perform well.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种建模层次结构的能力证明在图像之外也非常有用。每当数据中有有意义的局部模式时——如分子中的子结构、DNA中的基序或语音中的音素——CNN通常表现良好。
- en: Tip
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: We cover CNNs in more detail in Chapter 5, where we build a skin cancer classifier
    and explore common model design patterns. For now, we’ll provide a short overview
    of the key components you’ll need for this chapter.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第5章中更详细地介绍了CNN，在那里我们构建了一个皮肤癌分类器并探讨了常见的模型设计模式。现在，我们将提供本章所需的关键组件的简要概述。
- en: 'Let’s briefly walk through the key components of a typical CNN:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地浏览一下典型CNN的关键组件：
- en: Convolutional layers
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层
- en: These are the heart of the CNN. A convolutional layer contains multiple filters
    (also called *kernels*)—small weight matrices that slide across the input and
    compute dot products. Each filter acts like a pattern detector, lighting up when
    it finds a good match in the input. The result of applying a convolution is a
    *feature map* showing where the pattern occurred.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是CNN的核心。一个卷积层包含多个滤波器（也称为*核*）——在输入上滑动的小权重矩阵，并计算点积。每个滤波器都像一个模式检测器，当它在输入中找到良好的匹配时就会亮起。应用卷积的结果是一个*特征图*，显示了模式发生的位置。
- en: Pooling layers
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层
- en: These downsample the feature maps to reduce dimensionality and computation.
    *Max pooling*, for example, keeps only the strongest signal in each region, helping
    the model focus on the most salient features.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些将特征图下采样以减少维度和计算。例如，*最大池化*只保留每个区域中最强的信号，帮助模型关注最显著的特征。
- en: Normalization
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化
- en: Layers like *batch normalization* rescale activations to make training more
    stable and efficient. They reduce internal *covariate shift*—the tendency for
    activations to drift during training—and often speed up convergence.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于*批量归一化*的层会将激活值重新缩放，使训练更加稳定和高效。它们减少了内部*协变量偏移*——激活值在训练过程中漂移的趋势——并且通常可以加快收敛速度。
- en: Fully connected layers
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层
- en: These sit at the end of the network and use the features extracted by earlier
    layers to make final predictions—such as whether a DNA sequence is bound by a
    transcription factor.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些位于网络的末端，并使用先前层提取的特征进行最终预测——例如，是否一个DNA序列被转录因子结合。
- en: 'One key property of CNNs is that they are *translation equivariant*, meaning
    that a pattern can be recognized regardless of where it appears in the input.
    This is especially useful for DNA: a binding motif is still a binding motif whether
    it’s at position 10 or position 90 of the sequence.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs的一个关键特性是它们是*平移等变*的，这意味着无论模式出现在输入的哪个位置，都可以被识别。这对于DNA来说特别有用：无论结合基序位于序列的第10位还是第90位，它仍然是一个结合基序。
- en: Convolutions for DNA Sequences
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DNA序列卷积
- en: Although CNNs were originally developed for images (which are 2D grids of pixels),
    the architecture can easily be adapted to 1D data like DNA sequences. In genomics,
    DNA is commonly represented as a one-hot encoded matrix, where each base (`A`,
    `C`, `G`, `T`) is turned into a binary vector. A sequence of 100 bases would thus
    become a 100×4 matrix.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CNNs最初是为图像（像素的二维网格）开发的，但该架构可以轻松地适应一维数据，如DNA序列。在基因组学中，DNA通常表示为独热编码矩阵，其中每个碱基（`A`、`C`、`G`、`T`）被转换为二进制向量。因此，100个碱基的序列将变成一个100×4的矩阵。
- en: 'We then apply *1D convolutions*—filters that slide across the sequence in one
    dimension, looking for patterns in short windows of DNA bases. These filters often
    end up learning to identify the presence of DNA motifs: the short sequence patterns
    that have biological meaning we mentioned before, such as transcription factor
    binding sites. For example:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们应用*一维卷积*——这些滤波器在一个维度上滑动，寻找DNA碱基短窗口中的模式。这些滤波器通常学会识别DNA基序的存在：我们之前提到的具有生物学意义的短序列模式，例如转录因子结合位点。例如：
- en: Shallow layers might learn to detect low-level DNA features such as simple GC-rich
    or AT-rich regions in DNA.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浅层可能学会检测低级DNA特征，例如DNA中的简单GC丰富或AT丰富区域。
- en: Mid-level filters may identify known TF motifs.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中层滤波器可能识别已知的TF基序。
- en: Deeper layers might learn higher-order combinations—such as co-occurring motifs
    or long-range dependencies.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深层可能学会学习更高阶的组合——例如共现基序或长距离依赖性。
- en: Importantly, the model learns all of this automatically from labeled data. It
    doesn’t need to be told what motif to look for—it discovers useful patterns by
    optimizing for the task at hand, such as predicting TF binding.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，模型能够从标记数据中自动学习所有这些内容。它不需要被告知要寻找哪种基序——它通过优化当前任务（如预测TF结合）来发现有用的模式。
- en: Note
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'CNNs have become a standard architecture in sequence-based biology tasks because
    they offer a good balance between power, speed, and interpretability. Compared
    to other deep learning architectures, CNNs are relatively lightweight, easy to
    train, and often easier to interpret: you can visualize which motifs a filter
    has learned and where they appear in a sequence.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs已成为基于序列的生物学任务的标准架构，因为它们在能力、速度和可解释性之间提供了良好的平衡。与其他深度学习架构相比，CNNs相对轻量级，易于训练，并且通常更容易解释：你可以可视化一个滤波器学到的基序以及它们在序列中的出现位置。
- en: Their main limitation, however, is that they operate on fixed windows of sequence
    and struggle to model interactions between distant bases. For problems involving
    long-range dependencies—relationships between elements far apart in a sequence—we
    often turn to a different class of models.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们的主要局限性在于它们在固定窗口的序列上操作，并且难以模拟远程碱基之间的相互作用。对于涉及长距离依赖性（序列中相隔较远元素之间的关系）的问题，我们通常转向不同类别的模型。
- en: Transformers
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformers
- en: While CNNs are excellent at detecting local patterns, transformers are particularly
    powerful for modeling relationships across long distances in a sequence. Their
    core mechanism—*self-attention*—allows the model to dynamically determine which
    parts of the input are relevant for predicting any given output, regardless of
    how far apart those parts are.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CNNs 在检测局部模式方面非常出色，但Transformers在建模序列中长距离关系方面特别强大。它们的核心理机制——*自注意力*——允许模型动态确定输入的哪些部分对于预测任何给定输出是相关的，无论这些部分相隔多远。
- en: Transformer-based models have revolutionized deep learning since their debut
    in 2017.^([2](ch03.html#id638)) Originally designed for natural language processing
    tasks like translation and summarization, transformers have since become the state
    of the art in a wide range of domains, including genomics and protein modeling.
    Earlier architectures like RNNs and CNNs can also handle sequences, but transformers
    have largely surpassed them for tasks that require understanding global sequence
    context.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变压器的模型自2017年首次亮相以来已经彻底改变了深度学习。最初是为自然语言处理任务如翻译和摘要而设计的，变压器后来成为包括基因组学和蛋白质建模在内的广泛领域的最先进技术。早期的架构如RNN和CNN也可以处理序列，但变压器在需要理解全局序列上下文的任务上已经很大程度上超越了它们。
- en: 'The key innovation is *self-attention*: a mechanism that lets each token in
    the sequence “attend to” every other token, computing how much influence they
    should have. This is useful for:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 关键创新是**自注意力**：一种机制，允许序列中的每个标记“关注”每个其他标记，计算它们应该有多少影响力。这对于以下方面很有用：
- en: 'Language: Where a word’s meaning depends on faraway context'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言：一个词的意义取决于远处的上下文
- en: 'Genomics: Where a regulatory DNA motif located thousands of bases away might
    affect gene activity'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基因组学：位于数千个碱基之外的控制DNA基序可能会影响基因活性
- en: 'This flexibility allows transformers to learn arbitrary and complex dependencies—something
    CNNs struggle with. The main drawback of transformers is scalability: self-attention
    requires computations that grow quadratically with sequence length. For very long
    inputs like whole-genome sequences, this can become a bottleneck. However, many
    efficient transformer variants (e.g., Linformer, Longformer, Performer) have been
    developed to partially address this weakness.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种灵活性使得变压器能够学习任意和复杂的依赖关系——这是CNN难以做到的。变压器的主要缺点是可扩展性：自注意力需要与序列长度成平方增长的计算。对于像全基因组序列这样的非常长的输入，这可能会成为瓶颈。然而，已经开发了许多高效的变压器变体（例如，Linformer、Longformer、Performer）来部分解决这一弱点。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you’d like to dive deeper into transformers, we recommend the excellent blog
    post [“The Illustrated Transformer”](https://oreil.ly/WxzGl) by Jay Alammar or
    the [3Blue1Brown video](https://oreil.ly/dLW40) on attention. We’ll only touch
    on the basics here.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解变压器，我们推荐阅读Jay Alammar的优秀博客文章“[图解Transformer](https://oreil.ly/WxzGl)”或[3Blue1Brown的视频](https://oreil.ly/dLW40)关于注意力的内容。在这里，我们只简要介绍基础知识。
- en: The foundation of transformers lies in their ability to assign *attention*—a
    mechanism that lets each position in a sequence dynamically focus on other positions.
    This is what enables them to model long-range dependencies.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的基石在于其分配**注意力**的能力——这是一种机制，允许序列中的每个位置动态地关注其他位置。这正是它们能够建模长距离依赖的原因。
- en: Attention
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力
- en: At a high level, attention is a process that enriches the embedding of each
    token by incorporating information from every other token in the sequence. This
    makes each token more context aware, as illustrated in [Figure 3-3](#attention-mechanism).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，注意力是一个过程，通过结合序列中每个其他标记的信息来丰富每个标记的嵌入。这使得每个标记更加上下文感知，如图[图3-3](#attention-mechanism)所示。
- en: '![](assets/dlfb_0303.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0303.png)'
- en: Figure 3-3\. High-level visualization of the attention mechanism transforming
    input token embeddings into context-aware output embeddings. Each of the four
    tokens in the sequence (“the cat is black”) attends to every other token, allowing
    the model to capture relationships and dependencies across the entire sequence.
    The output is also four tokens, each enriched with contextual information from
    the others.
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3. 注意力机制将输入标记嵌入转换为上下文感知输出嵌入的高级可视化。序列中的每个标记（“the cat is black”）都会关注序列中的每个其他标记，使模型能够捕捉整个序列中的关系和依赖。输出也是四个标记，每个标记都富含来自其他标记的上下文信息。
- en: Briefly, here’s how it works. The model first creates three versions of the
    input embeddings—queries (Q), keys (K), and values (V)—via learned linear transformations.
    Each query is compared to all keys using a dot product, producing a score that
    reflects how relevant one token is to another. These scores are then normalized
    via a softmax function, producing the attention weights. Finally, each token’s
    new embedding is computed as a weighted sum of the value vectors, with the attention
    weights determining how much each value contributes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这是它的工作原理。模型首先通过学习线性变换创建三个版本的输入嵌入——查询（Q）、键（K）和值（V）。每个查询通过点积与所有键进行比较，产生一个分数，该分数反映了某个标记与另一个标记的相关性。然后，这些分数通过softmax函数进行归一化，产生注意力权重。最后，每个标记的新嵌入是通过值向量的加权和计算的，注意力权重决定了每个值贡献的大小。
- en: Let’s walk through a simple example. Suppose we have the input sequence `the`,
    `cat`, `is`, `black`. In a transformer model, each token doesn’t just pass through
    the network on its own; it decides how much attention to pay to every other token.
    For example, when processing the word `cat`, the model might assign a high attention
    weight to `the`, recognizing that articles and nouns are often linked. This helps
    the model understand grammatical relationships and contextual meaning.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来了解一下。假设我们有一个输入序列`the`、`cat`、`is`、`black`。在转换器模型中，每个标记不仅仅独立通过网络；它决定对每个其他标记给予多少注意力。例如，当处理单词`cat`时，模型可能会将高注意力权重分配给`the`，认识到冠词和名词通常相关。这有助于模型理解语法关系和上下文意义。
- en: 'In genomics, attention can serve a similar purpose. Imagine a model processing
    a DNA sequence to predict TF binding. An attention mechanism allows the model
    to ask: how relevant is this motif to another element upstream or downstream—perhaps
    thousands of bases away? Just as a word’s meaning is shaped by the words around
    it, the function of a sequence motif may depend on other elements scattered across
    the genome.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在基因组学中，注意力可以起到类似的作用。想象一个处理DNA序列以预测TF结合的模型。注意力机制允许模型询问：这个基序与上游或下游的其他元素（可能相距数千个碱基）的相关性如何？正如一个词的意义是由周围的词塑造的，一个序列基序的功能可能取决于散布在整个基因组中的其他元素。
- en: Once attention has enriched the token embeddings with contextual information,
    the result is passed through a *feedforward network*—typically a small, multilayer
    perceptron applied independently to each token. This network introduces nonlinearity
    and helps the model capture more complex patterns. The output is then passed through
    *residual connections* (which help with gradient flow) and *layer normalization*
    (which stabilizes training).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦注意力通过上下文信息丰富了标记嵌入，结果就会通过一个*前馈网络*——通常是一个小型多层感知器，独立应用于每个标记。这个网络引入了非线性，并帮助模型捕捉更复杂的模式。然后，输出通过*残差连接*（有助于梯度流动）和*层归一化*（有助于稳定训练）。
- en: All together, this sequence—attention, feedforward layers, residual connections,
    and normalization—forms one transformer block. A full transformer model is typically
    built by stacking many of these blocks, allowing information to flow and be refined
    across layers. As tokens pass through successive layers, their representations
    become increasingly rich, capturing everything from local patterns to global structure.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这个序列——注意力、前馈层、残差连接和归一化——形成了一个转换器块。一个完整的转换器模型通常是通过堆叠许多这样的块来构建的，允许信息在层之间流动和细化。随着标记通过连续的层，它们的表示变得越来越丰富，从局部模式到全局结构，一切都被捕捉到。
- en: Query, Key, and Value Intuition
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询、键和值直觉
- en: 'You might wonder: why bother transforming the input into queries, keys, and
    values at all? The key idea is that Q, K, and V aren’t just redundant copies of
    the original token embeddings. They’re learned projections that allow the model
    to look at the same sequence from different perspectives:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：为什么要把输入转换成查询、键和值呢？关键思想是Q、K和V不仅仅是原始标记嵌入的冗余副本。它们是学习到的投影，允许模型从不同的角度看待相同的序列：
- en: The *query* is like a lens that each token uses to express what it wants to
    pay attention to. In our sentence example, the word `black` might use its query
    to find the noun it modifies—`cat`. In DNA, a regulatory region might “look for”
    compatible regulatory motifs elsewhere in the sequence. The query says, “I’m looking
    for X.”
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询*就像一个透镜，每个标记用它来表达它想要关注的。在我们的句子示例中，单词`black`可能使用它的查询来找到它所修饰的名词——`cat`。在DNA中，一个调控区域可能“寻找”在序列其他地方的兼容调控基序。查询说：“我在寻找X。”'
- en: 'The *key* is how each token presents itself to others: it encodes what kind
    of information it offers. Continuing the analogy, each word or DNA element “advertises”
    what kind of content it has, saying: “I contain Y.”'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关键*在于每个标记如何向他人展示自己：它编码了它提供的信息类型。继续这个类比，每个单词或DNA元素“宣传”它拥有的内容，说：“我包含Y。”'
- en: The *value* is the actual content that gets passed along if the query decides
    the key is relevant. In other words, the query compares itself to all keys to
    compute attention weights and then uses those weights to pull a mixture of values
    from across the sequence.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*值*是如果查询决定键相关时实际传递的内容。换句话说，查询将自己与所有键进行比较以计算注意力权重，然后使用这些权重从整个序列中提取一系列值。'
- en: This separation of roles allows the model to reason more flexibly. Instead of
    treating all parts of the sequence as equally relevant, each token decides what
    matters to it right now, based on its query and the other tokens’ keys. The values
    then supply the useful content.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这种角色的分离使得模型能够更灵活地进行推理。而不是将序列的所有部分视为同等相关，每个标记根据其查询和其他标记的键决定当前对其重要的事情。然后，值提供有用的内容。
- en: Note
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'This design makes attention work like a smart lookup system: tokens advertise
    what they contain (keys), queries look for matches, and then the actual content
    (values) is pulled in weighted proportion to how well the key matched the query.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计使得注意力工作像一个智能查找系统：标记宣传它们包含的内容（键），查询寻找匹配项，然后根据键与查询的匹配程度，以加权比例提取实际内容（值）。
- en: Importantly, the Q, K, and V projections are all learned from data. They start
    out random, and the model figures out—through training—how best to shape these
    representations for the task at hand, whether that’s learning grammar, predicting
    regulatory activity, or modeling protein interactions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，Q、K和V投影都是从数据中学习的。它们最初是随机的，模型通过训练找出——如何最好地塑造这些表示以完成当前任务，无论是学习语法、预测调控活性还是模拟蛋白质相互作用。
- en: Multiheaded Attention
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力
- en: '*Multiheaded attention* (MHA) enhances the attention mechanism by running several
    attention operations—called *heads*—in parallel. Each head learns to focus on
    different parts or patterns in the input sequence. For instance, one head might
    focus on local motifs, while another might detect longer-range interactions or
    subtle contextual cues.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*多头注意力*（MHA）通过并行运行多个注意力操作——称为*头*——来增强注意力机制。每个头学习专注于输入序列的不同部分或模式。例如，一个头可能专注于局部基序，而另一个头可能检测更广泛的相互作用或微妙的上下文线索。'
- en: By combining these multiple perspectives, MHA allows the model to capture a
    richer and more diverse set of relationships within the data, beyond what a single
    attention operation could learn.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这些多个视角，MHA允许模型捕捉到更丰富、更多样化的数据关系集，这超出了单个注意力操作所能学习的内容。
- en: Tip
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: While often the different heads learn somewhat redundant patterns, this parallel
    structure increases the model’s expressive power and flexibility. The outputs
    from all heads are concatenated and linearly transformed to produce the final
    representation, which then feeds into subsequent model layers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不同的头通常学习到一些冗余的模式，但这种并行结构增加了模型的表达能力和灵活性。所有头的输出被连接起来，并通过线性变换产生最终的表示，然后输入到后续模型层中。
- en: In essence, MHA lets transformers attend to multiple types of interactions simultaneously,
    which is particularly useful for complex biological sequences where various signals
    and dependencies exist at different scales.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，MHA让transformer能够同时关注多种类型的交互，这对于存在不同尺度上的各种信号和依赖关系的复杂生物序列特别有用。
- en: Representing Positional Information
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示位置信息
- en: 'One final point: basic self-attention is *position invariant*, meaning it does
    not inherently capture the order or position of tokens in a sequence. To address
    this, transformer models include positional encodings or other mechanisms that
    inject information about the relative or absolute positions of tokens, enabling
    the model to understand sequence order. In the original transformer paper, sinusoidal
    functions of the token index were added to the token embeddings to provide this
    positional information.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点：基本的自注意力是*位置不变的*，这意味着它本身不捕获序列中标记的顺序或位置。为了解决这个问题，transformer模型包括位置编码或其他机制，注入关于标记相对或绝对位置的信息，使模型能够理解序列顺序。在原始的transformer论文中，将正弦函数添加到标记嵌入中，以提供这种位置信息。
- en: With this brief overview of transformers complete, let’s move on to some model
    interpretation techniques we’ll apply in this chapter.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成对转换器的简要概述后，让我们继续本章中我们将应用的一些模型解释技术。
- en: Model Interpretation
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型解释
- en: A common criticism of deep learning models is that they act as a *black box*—they
    may produce accurate predictions, but it’s often unclear how exactly those predictions
    are made or what internal reasoning the model uses. While deep learning models
    are generally less interpretable than simpler methods like linear models or decision
    trees, there are several techniques to probe their inner workings. These techniques
    fall under the umbrella of *model interpretation*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的一个常见批评是它们像一个 *黑盒*——它们可能产生准确的预测，但通常不清楚这些预测是如何做出的，或者模型使用了什么内部推理。虽然深度学习模型通常不如线性模型或决策树等简单方法可解释，但有一些技术可以探测其内部工作原理。这些技术属于
    *模型解释* 的范畴。
- en: 'Model interpretation for deep learning is a vast and active research area,
    so here we provide a brief overview of the most commonly used techniques in the
    DNA modeling space:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的模型解释是一个广泛且活跃的研究领域，因此在这里我们简要概述了在DNA建模空间中最常用的技术：
- en: Mutagenesis
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 突变
- en: To understand which input features the model relies on, we systematically alter
    (or *mutate*) parts of the input and observe how the model’s predictions change.
    For example, when predicting gene expression from DNA sequence, we can shuffle,
    replace, or delete certain bases and see if the prediction shifts significantly.
    Large changes indicate that the mutated region is important for the model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解模型依赖哪些输入特征，我们系统地改变（或 *突变*）输入的一部分，并观察模型的预测如何变化。例如，在从DNA序列预测基因表达时，我们可以打乱、替换或删除某些碱基，并观察预测是否发生显著变化。大的变化表明突变区域对模型很重要。
- en: 'Pro: This is direct and intuitive. It provides rich, localized insights.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优点：这是直接且直观的。它提供了丰富且局部的洞察。
- en: 'Con: It is computationally expensive, since each mutation requires a separate
    forward pass through the model.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺点：它计算成本高，因为每个突变都需要模型的一次单独前向传递。
- en: Input gradients
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输入梯度
- en: A faster but approximate method involves computing the gradient of the model’s
    output with respect to each input feature. This gradient shows how sensitive the
    prediction is to small changes in each input element.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更快但近似的方法是计算模型输出相对于每个输入特征的梯度。这个梯度显示了预测对每个输入元素微小变化的敏感性。
- en: 'Pro: It is efficient, as it requires only one backward pass to generate an
    importance map.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优点：它效率高，因为它只需要一次反向传递来生成重要性图。
- en: 'Con: It can be noisy and less precise, making it harder to distinguish signal
    from noise.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺点：它可能噪声大且不够精确，使得区分信号和噪声更困难。
- en: Attention mechanisms
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制
- en: For models that include attention, we can inspect the attention weights to see
    where the model is focusing when making predictions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含注意力的模型，我们可以检查注意力权重，以查看模型在做出预测时关注的地方。
- en: 'Pro: This provides a naturally interpretable visualization of model focus and
    interactions.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优点：这提供了模型关注点和交互的自然可解释的可视化。
- en: 'Con: Attention scales quadratically with input sequence length, meaning that
    in practice, we can’t use attention to model very long DNA strings without first
    condensing them down in some way.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺点：注意力与输入序列长度成平方级增长，这意味着在实践中，我们无法在不以某种方式压缩的情况下使用注意力来建模非常长的DNA字符串。
- en: There are many extensions and refinements of these methods, and model interpretation
    is increasingly standard in deep learning biology research papers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法有许多扩展和改进，模型解释在深度学习生物学研究论文中越来越标准化。
- en: 'Next, we’ll dive deeper into the two interpretation approaches that we will
    implement in this chapter: *in silico* mutagenesis and input gradients.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将更深入地探讨本章中我们将实施的两种解释方法：*in silico* 突变和输入梯度。
- en: In Silico Saturation Mutagenesis
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: In Silico Saturation 突变
- en: '*In silico saturation mutagenesis* (ISM) may sound complex, but it essentially
    involves systematically making every possible alteration (or mutation) to a biological
    sequence—such as DNA or protein—and generating a separate model prediction for
    each mutated sequence. Because this requires many forward passes through the model,
    it is computationally expensive. However, the detailed insights it provides into
    how each possible variation affects the output often justify the cost.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*In silico饱和突变生成*（ISM）可能听起来很复杂，但本质上它涉及系统地改变生物序列（如DNA或蛋白质）的每一个可能的改变（或突变），并为每个突变的序列生成一个单独的模型预测。因为这需要通过模型进行许多正向传递，所以计算成本很高。然而，它提供的关于每个可能的变异如何影响输出的详细见解通常可以证明这种成本是合理的。'
- en: Note
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Terminology breakdown: it’s called *mutagenesis* because we induce mutations,
    *saturation* because every possible mutation is tested, and *in silico* since
    these predictions are done computationally rather than experimentally in the lab.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 术语分解：它被称为*突变生成*，因为我们诱导突变，*饱和*，因为测试了所有可能的突变，以及*in silico*，因为这些预测是通过计算而不是在实验室中进行实验完成的。
- en: '[Figure 3-4](#saturation-mutagenesis) shows an example plot we will generate
    later in this chapter.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-4](#saturation-mutagenesis)显示了本章后面我们将生成的示例图。'
- en: '![](assets/dlfb_0304.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0304.png)'
- en: Figure 3-4\. Example of an in silico saturation mutagenesis plot showing predicted
    probabilities of transcription factor binding across a 200-base DNA sequence.
    The x-axis represents the DNA sequence positions, while the y-axis shows the three
    possible mutations at each position (including the mutation to the original base,
    which has no effect and is set to zero). In this example, most mutation effects
    are negative (darker color), indicating that changing bases generally reduces
    the predicted binding probability. This approach is computationally expensive
    due to the large number of forward passes required to generate this output matrix
    (here, 3 mutations × 200 positions = 600 model predictions).
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4\. 一个in silico饱和突变生成图的示例，显示了跨越200个碱基DNA序列的转录因子结合预测概率。x轴代表DNA序列位置，而y轴显示了每个位置的三种可能的突变（包括对原始碱基的突变，这没有影响，设置为零）。在这个例子中，大多数突变效应是负的（颜色较深），表明改变碱基通常会降低预测的结合概率。这种方法由于需要大量正向传递来生成这个输出矩阵（这里，3种突变×200个位置=600个模型预测）而计算成本高昂。
- en: ISM plots help to quickly highlight which parts of the sequence are most important
    for the model’s predictions. Because they visualize the most salient or impactful
    input regions, they are often referred to as *saliency maps*. In this example,
    the plot suggests that mutating any of the central bases likely disrupts a motif
    that the protein binds to, as these mutations generally lead to lower predicted
    binding probabilities (indicated by the negative values).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ISM图有助于快速突出序列中哪些部分对模型的预测最重要。因为它们可视化最显著或最具影响力的输入区域，它们通常被称为*显著性图*。在这个例子中，图表明突变任何中央碱基可能会破坏蛋白质结合的基序，因为这些突变通常会导致预测的结合概率降低（用负值表示）。
- en: Input Gradients
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入梯度
- en: Input gradients provide a faster way to generate a saliency map, summarizing
    which parts of the sequence most influence the model’s predictions. Conceptually,
    they are the derivatives of the model’s output with respect to its input features.
    If you’ve trained neural networks before, you’ve already encountered gradients—typically
    computed with respect to model parameters to guide weight updates.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 输入梯度提供了一种更快的方法来生成显著性图，总结哪些序列部分对模型的预测影响最大。从概念上讲，它们是模型输出相对于其输入特征的导数。如果你之前训练过神经网络，你已经遇到过梯度——通常与模型参数相关联以指导权重更新。
- en: Input gradients follow the same principle, but they shift the focus from parameters
    to the input itself. By calculating how the output changes in response to tiny
    perturbations at each input position, we can assess the model’s sensitivity. For
    DNA sequences, this means identifying which bases have the greatest influence
    on predictions like TF binding. A large gradient at a given base suggests that
    altering it would significantly impact the model’s output—signaling that the base
    is important.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 输入梯度遵循相同的原则，但它们将重点从参数转移到输入本身。通过计算输出如何对每个输入位置的微小扰动做出反应，我们可以评估模型的敏感性。对于DNA序列，这意味着确定哪些碱基对预测如TF结合有最大的影响。在给定碱基上的大梯度表明，改变它将显著影响模型的输出——表明该碱基很重要。
- en: 'Concretely, for a TF binding prediction:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，对于一个TF结合预测：
- en: A *large negative gradient* at a base suggests that changing it would significantly
    lower the binding probability, perhaps disrupting the motif the TF needs to fit
    properly.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个碱基上的一个*大的负梯度*表明改变它将显著降低结合概率，可能破坏转录因子需要正确拟合的基序。
- en: A *large positive gradient* suggests that changing it would significantly increase
    the binding probability, maybe by strengthening an existing motif or creating
    a new one, thus improving the physical binding affinity between the DNA and the
    TF.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*大的正梯度*表明改变它将显著增加结合概率，可能通过加强现有的基序或创建一个新的基序，从而提高DNA和转录因子之间的物理结合亲和力。
- en: Note
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: What does “making a small change to the input” mean when the input is a one-hot
    encoded DNA sequence, rather than a continuous scalar value? Each DNA sequence,
    each base (A, T, C, G) is generally represented as a binary vector (e.g., `[1,
    0, 0, 0]` for `A`). During gradient calculation, however, we treat these vectors
    as if they could vary continuously—allowing fractional values like `[0.9, 0.1,
    0, 0]`. While such fractional bases aren’t biologically meaningful, this mathematical
    abstraction lets us compute gradients and gain insights into which positions are
    influential for predictions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入是一个独热编码的DNA序列而不是连续标量值时，“对输入进行微小改变”意味着什么？每个DNA序列，每个碱基（A，T，C，G）通常表示为一个二进制向量（例如，`[1,
    0, 0, 0]`表示`A`）。然而，在梯度计算过程中，我们将这些向量视为可以连续变化——允许像`[0.9, 0.1, 0, 0]`这样的分数值。虽然这样的分数碱基在生物学上没有意义，但这种数学抽象使我们能够计算梯度，并深入了解哪些位置对预测有影响。
- en: You can think of input gradients as a faster but more approximate alternative
    to in silico saturation mutagenesis. Input gradients provide a general idea of
    important regions, whereas saturation mutagenesis directly tests every possible
    mutation’s effect but is computationally expensive.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将输入梯度视为一种更快但更近似的替代方法，即模拟饱和突变。输入梯度提供了一个关于重要区域的总体概念，而饱和突变直接测试了每种可能的突变效果，但计算成本高昂。
- en: This concludes the biology and machine learning primers. Now let’s dive into
    exploring and modeling the data to predict transcription factor binding in DNA
    sequences.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着生物学和机器学习入门课程的结束。现在让我们深入探索和建模数据，以预测DNA序列中的转录因子结合。
- en: Building a Simple Prototype
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个简单的原型
- en: 'The modeling task we’ll tackle in this chapter is a *binary classification
    problem*: given a 200-base DNA sequence, can we predict whether it binds to a
    specific TF called CTCF? Among the 1,500+ TFs in humans, CTCF stands out for its
    role in organizing the genome’s 3D architecture—folding DNA into loops and domains
    that regulate gene activity.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将要解决的是一项*二元分类问题*：给定一个200个碱基的DNA序列，我们能否预测它是否与一个称为CTCF的特定转录因子结合？在人类1500多个转录因子中，CTCF因其对基因组3D结构的组织作用而突出——将DNA折叠成环和区域，以调节基因活性。
- en: CTCF is also a great first target because its binding behavior is relatively
    easy to model. It recognizes a well-characterized, highly conserved motif with
    strong sequence specificity, meaning its binding sites are more predictable than
    those of many other TFs. That makes CTCF an ideal entry point for building and
    interpreting sequence-based models of TF binding. Later, we will expand our scope
    to predict the binding of 10 different TFs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: CTCF也是一个很好的起点，因为其结合行为相对容易建模。它识别一个被充分表征的、高度保守的基序，具有强烈的序列特异性，这意味着其结合位点比许多其他转录因子的结合位点更容易预测。这使得CTCF成为构建和解释基于序列的转录因子结合模型的理想起点。稍后，我们将扩大范围，预测10种不同的转录因子结合。
- en: As with other chapters in this book, we’ll begin by exploring the dataset, building
    a simple prototype model, and then iteratively extending and improving it.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中的其他章节一样，我们将从探索数据集、构建一个简单的原型模型开始，然后迭代地扩展和改进它。
- en: Building a Dataset
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建数据集
- en: The dataset we will use looks like [Figure 3-5](#dna-data-task).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的数据库看起来像[图3-5](#dna-data-task)。
- en: '![](assets/dlfb_0305.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0305.png)'
- en: Figure 3-5\. The input dataset consists of DNA sequences, each 200 bases long,
    with an associated binary label indicating whether the protein CTCF binds to it.
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5. 输入数据集由200个碱基长的DNA序列组成，每个序列都有一个相关的二进制标签，表示蛋白质CTCF是否与之结合。
- en: This task is inspired by one of the evaluation challenges presented in a recent
    2024 paper preprint,^([3](ch03.html#id670)) which sourced the dataset from a 2023
    genomics interpretation study.^([4](ch03.html#id671))
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务灵感来源于最近一篇2024年论文预印本中提出的一个评估挑战，该预印本的数据集来源于2023年的一项基因组学解释研究.^([3](ch03.html#id670))，^([4](ch03.html#id671))
- en: Loading the Labeled Sequences
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载标记序列
- en: 'We start by examining the training dataset:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先检查训练数据集：
- en: '[PRE0]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Output:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE1]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The two classes appear fairly balanced (equally represented) in the training
    dataset, so we won’t have to do any rebalancing by downsampling the majority class
    here:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据集中，两个类别出现得相当平衡（均匀表示），因此我们不需要通过下采样多数类别来进行任何重平衡：
- en: '[PRE2]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE3]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To use the DNA sequences numerically, we need to convert them into one-hot
    format. The function `dna_to_one_hot` performs this mapping:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将DNA序列数值化，我们需要将它们转换为单热格式。函数`dna_to_one_hot`执行这个映射：
- en: '[PRE4]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s see what the one-hot encoding looks like on a sample DNA sequence, “AAACGT”:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看单热编码在样本DNA序列“AAACGT”上的样子：
- en: '[PRE5]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Output:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE6]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can apply this converter to the entire training dataset to generate the
    numerical training data `x_train` as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个转换器应用于整个训练数据集，以生成数值训练数据`x_train`，如下所示：
- en: '[PRE7]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here, `y_train` contains the binary target labels: `0` means the sequence does
    not bind CTCF, and `1` means the sequence does bind CTCF.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`y_train`包含二进制目标标签：`0`表示序列不与CTCF结合，`1`表示序列与CTCF结合。
- en: 'The dataset loading code for this problem is fairly straightforward, but we
    can wrap it into a convenient function called `load_dataset` for cleaner use:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，数据集加载代码相当简单，但我们可以将其包装成一个方便的函数`load_dataset`，以便更干净地使用：
- en: '[PRE8]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Converting the Data to a TensorFlow Dataset
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据转换为TensorFlow数据集
- en: 'Next, we convert the training data into a TensorFlow dataset. This format makes
    it easy to efficiently iterate over batches during model training, especially
    when shuffling and repeating the data for multiple epochs:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练数据转换为TensorFlow数据集。这种格式使得在模型训练期间高效迭代批次变得容易，尤其是在对数据进行多次epoch的打乱和重复时：
- en: '[PRE9]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the preceding code, we created the training dataset `train_ds` with batching,
    shuffling, and repetition enabled. Let’s sanity-check by pulling one batch from
    the dataset and inspecting its shape and contents:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们通过启用批处理、打乱和重复创建了一个训练数据集`train_ds`。让我们通过从数据集中抽取一个批次并检查其形状和内容来进行合理性检查：
- en: '[PRE11]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Output:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE12]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This looks sensible. We see the data has shape (`32`, `200`, `4`), indicating
    a batch size of `32`, sequence length of 200, and 4 channels per DNA base as expected.
    The labels have shape (`32`, `1`) since each label is a simple binary `0` or `1`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来是合理的。我们看到数据具有形状（`32`，`200`，`4`），表示批次大小为`32`，序列长度为200，每个DNA碱基有4个通道，正如预期的那样。标签具有形状（`32`，`1`），因为每个标签都是一个简单的二进制`0`或`1`。
- en: 'For validation, since the dataset is smaller, we can structure it as one single
    big batch:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证，由于数据集较小，我们可以将其结构化为一个单独的大批次：
- en: '[PRE13]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With our data now in the correct format, we are ready to build and train our
    first simple model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据格式化为正确的格式，我们准备构建和训练我们的第一个简单模型。
- en: Defining a Simple Convolutional Model
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个简单的卷积模型
- en: 'Next, we define a simple CNN composed of two 1D convolutional layers, followed
    by flattening and fully connected (dense) layers:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个由两个1D卷积层组成、随后是展平和全连接（密集）层的简单CNN：
- en: '[PRE14]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This model architecture is fairly “no frills” but should already be able to
    capture local patterns in DNA sequences. We can instantiate our model like this:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型架构相当“无装饰”，但应该已经能够捕捉DNA序列中的局部模式。我们可以像这样实例化我们的模型：
- en: '[PRE15]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To initialize model parameters in JAX, we need to provide a dummy input tensor
    that matches the expected shape of the model’s input. Although we could use an
    actual batch from our dataset, it is common and simpler to use a tensor of ones
    with batch size 1 and the same shape as a single-encoded DNA sequence. Importantly,
    the batch size used for this dummy input does not affect the model initialization—JAX
    initializes parameters based on the shape of an individual input sample (excluding
    the batch dimension). This means the model can later be trained or used for inference
    with any batch size.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在JAX中初始化模型参数时，我们需要提供一个虚拟输入张量，该张量与模型输入的预期形状相匹配。虽然我们可以使用数据集的实际批次，但通常更简单的是使用一个大小为1的ones张量，其形状与单个编码的DNA序列相同。重要的是，用于这个虚拟输入的批次大小不会影响模型初始化——JAX根据单个输入样本的形状（不包括批次维度）来初始化参数。这意味着模型可以在以后以任何批次大小进行训练或用于推理。
- en: '[PRE16]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Output:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE17]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This dummy input allows JAX to infer the shapes of all model parameters, and
    the random key `rng_init` seeds any stochastic initialization, ensuring reproducibility.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个虚拟输入允许JAX推断所有模型参数的形状，并且随机的键`rng_init`为任何随机初始化提供种子，确保可重复性。
- en: Examining Model Tensor Shapes
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查模型张量形状
- en: Understanding and keeping track of tensor shapes is a crucial part of machine
    learning. Always make it a habit to verify the shapes of your data as it flows
    through the model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 理解并跟踪张量形状是机器学习的关键部分。始终养成验证数据流经模型时数据形状的习惯。
- en: Tip
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: As a practical exercise, try adding `print(x.shape)` statements at various points
    inside the model’s `__call__` method and then rerun the `model.init` step to observe
    how the shapes change as data flows through the model layers.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项实际练习，尝试在模型的`__call__`方法内部的不同位置添加`print(x.shape)`语句，然后重新运行`model.init`步骤，以观察数据流经模型层时形状如何变化。
- en: 'Here are some key operations in the model that change tensor shapes:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是模型中一些改变张量形状的关键操作：
- en: 'Convolutional layers (`nn.Conv`): These layers can modify the channel dimension.
    For example, our input starts with four channels (DNA bases), but the first convolution
    increases this to 64 channels, effectively learning 64 different sequence features
    or motifs. Additionally, the convolution’s `padding` option affects the sequence
    length dimension:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层（`nn.Conv`）：这些层可以修改通道维度。例如，我们的输入开始时有四个通道（DNA碱基），但第一个卷积将通道数增加到64，有效地学习64个不同的序列特征或基序。此外，卷积的`padding`选项会影响序列长度维度：
- en: '`adding=''SAME''` preserves the sequence length.'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adding=''SAME''`保留序列长度。'
- en: '`padding=''VALID''` reduces it depending on the kernel size. Try switching
    between these to see the impact on the sequence length.'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding=''VALID''`根据核大小减少它。尝试在这两者之间切换以查看对序列长度的影响。'
- en: 'Max pooling layers (`nn.max_pool`): These reduce the sequence length by downsampling.
    In our model, each max pooling layer halves the length, from 200 → 100 → 50 bases.
    To reduce the spatial axis more aggressively (e.g., by a factor of 5 each time),
    adjust the `window_shape` and `strides` arguments accordingly (typically, these
    are set to the same value to avoid overlapping windows and simplify downsampling).'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大池化层（`nn.max_pool`）：这些层通过下采样减少序列长度。在我们的模型中，每个最大池化层将长度减半，从200 → 100 → 50碱基。为了更激进地减少空间轴（例如，每次减少5倍），相应地调整`window_shape`和`strides`参数（通常，这些参数设置为相同的值以避免重叠窗口并简化下采样）。
- en: 'Flattening (`reshape`): Before passing data to dense layers, the tensor is
    reshaped from `(batch_size, sequence_length, channels)` to `(batch_size, flattened_features)`.
    This collapses the spatial and channel dimensions into one long vector per example,
    preparing it for fully connected layers.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展平（`reshape`）：在将数据传递到密集层之前，张量从`(batch_size, sequence_length, channels)`重塑为`(batch_size,
    flattened_features)`。这将空间和通道维度折叠为每个示例的一个长向量，为全连接层做准备。
- en: 'Once the model parameters are initialized, you can inspect them to confirm
    the layer structure and shapes. Check the parameter keys (layer names) with:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化了模型参数，您可以检查它们以确认层结构和形状。使用以下方式检查参数键（层名称）：
- en: '[PRE18]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Output:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE19]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, inspect each layer’s kernel shape to verify the expected parameter dimensions:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，检查每个层的核形状以验证预期的参数维度：
- en: '[PRE20]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Output:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE21]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Some notes on these shapes:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些形状的一些说明：
- en: For convolutional layers, the parameter shape is `(kernel_size, input_channels,
    output_channels)`. For example, `Conv_0` has kernel size 10, input channels 4
    (DNA bases), and outputs 64 feature maps.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于卷积层，参数形状为`(kernel_size, input_channels, output_channels)`。例如，`Conv_0`的核大小为10，输入通道为4（DNA碱基），并输出64个特征图。
- en: Dense layers have shapes (`input_features`, `output_units`). For instance, `Dense_0`
    maps 3200 flattened features (50 sequence length * 64) to 128 units.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集层的形状为（`input_features`, `output_units`）。例如，`Dense_0`将3200个展平特征（50序列长度 * 64）映射到128个单元。
- en: With the model initialized and the parameter shapes explored, let’s start setting
    up our training loop.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型初始化并探索参数形状之后，让我们开始设置我们的训练循环。
- en: Making predictions with the model
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用模型进行预测
- en: 'We can actually already obtain predictions from the model using the randomly
    initialized parameters, though the predictions will be random. We make predictions
    by calling `model.apply` on a batch of sequences:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们已经在使用随机初始化的参数从模型中获取预测，尽管预测将是随机的。我们通过在序列的一批上调用`model.apply`来做出预测：
- en: '[PRE22]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE23]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Since the model is untrained, predicted probabilities will be around around
    0.5, reflecting pure guesswork from our randomly-parameterized model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型未经过训练，预测概率将大约为0.5，这反映了我们的随机参数化模型的纯猜测。
- en: Defining a loss function
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义损失函数
- en: 'Let’s now define a binary cross-entropy loss function using the `optax` library
    that we can use to train our model parameters:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`optax`库定义一个二元交叉熵损失函数，我们可以使用它来训练我们的模型参数：
- en: '[PRE24]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can use it to compute the loss for the initial batch:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用它来计算初始批次的损失：
- en: '[PRE25]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE26]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This gives a baseline loss value before training. As the model learns, this
    loss should decrease substantially.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了训练前的基线损失值。随着模型的学习，这个损失应该大幅下降。
- en: Note
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意：
- en: 'Why use binary cross-entropy loss? In this chapter, we’re predicting whether
    a TF binds to a given DNA sequence—a classic binary classification task. Binary
    cross-entropy is the standard loss function for this setting: it measures how
    well the model’s predicted probabilities align with the true binary labels.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用二元交叉熵损失？在本章中，我们预测TF是否绑定到给定的DNA序列——这是一个经典的二元分类任务。二元交叉熵是这种设置的标准损失函数：它衡量模型的预测概率与真实二元标签之间的匹配程度。
- en: 'It penalizes confident but incorrect predictions more heavily, encouraging
    well-calibrated outputs near 0 or 1\. You can also think of it as a signal reconstruction
    problem: the model tries to approximate a hidden binary signal, and cross-entropy
    imposes a sharp cost for noisy or off-target estimates.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 它对自信但错误的预测施加更重的惩罚，鼓励接近0或1的准确输出。你也可以将其视为一个信号重建问题：模型试图逼近一个隐藏的二进制信号，而交叉熵对噪声或偏离目标的估计施加了尖锐的成本。
- en: Defining the TrainState
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义TrainState
- en: 'To start training the model, we first need to define the optimizer. We’ll use
    Adam with a learning rate of 1e-3, which is a common default value that tends
    to work well across diverse problems:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练模型，我们首先需要定义优化器。我们将使用学习率为1e-3的Adam，这是一个常见的默认值，通常适用于各种问题：
- en: '[PRE27]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'With this, we now have all the components to initialize the training state.
    For this, we’ll use Flax’s `TrainState` class, which is a container that bundles
    all the important objects for training (the model, parameters, and optimizer):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个，我们现在有了初始化训练状态的所有组件。为此，我们将使用Flax的`TrainState`类，它是一个包含所有重要训练对象（模型、参数和优化器）的容器：
- en: '[PRE28]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For convenience, let’s define a function to create the train state:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，让我们定义一个函数来创建训练状态：
- en: '[PRE29]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Defining a single training step
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义单个训练步骤
- en: 'Finally, putting everything together, we can write a function to run one training
    iteration, which performs these steps:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将所有这些放在一起，我们可以编写一个函数来运行一个训练迭代，它执行以下步骤：
- en: 1\. Forward pass
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 前向传播
- en: Takes a batch of data, makes model predictions, and computes loss based on the
    current parameters
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 接收一批数据，根据当前参数进行模型预测并计算损失
- en: '2\. Backwards pass:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 向后传播：
- en: Computes gradients of the loss with respect to the parameters
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失相对于参数的梯度
- en: 3\. Update
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 更新
- en: Using the computed gradients, updates the parameters to minimize the model’s
    loss
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用计算出的梯度，更新参数以最小化模型的损失
- en: 'These steps happen in the `train_step` function:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤发生在`train_step`函数中：
- en: '[PRE30]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let’s run one training step:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个训练步骤：
- en: '[PRE31]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If our setup is working well, the training loss should already be lower on
    this batch. Let’s check:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的设置工作良好，训练损失应该已经在这个批次上降低。让我们检查：
- en: '[PRE32]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Output:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE33]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: And indeed, the loss is lower than it was before a `train_step` was run. With
    the model and training loop ready, let’s set up a full training run.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，损失确实低于运行`train_step`之前的损失。随着模型和训练循环就绪，让我们设置一个完整的训练运行。
- en: Training the Simple Model
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练简单模型
- en: 'Let’s now run the preceding `train_step` many times in order to optimize the
    model. Here, we train for 500 steps and periodically evaluate on the validation
    set:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将多次运行前面的`train_step`以优化模型。在这里，我们训练500步，并定期在验证集上进行评估：
- en: '[PRE34]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In [Figure 3-6](#training-simplest-model), we can plot the training and validation
    loss curves resulting from this run:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图3-6](#training-simplest-model)中，我们可以绘制出这次运行产生的训练和验证损失曲线：
- en: '[PRE35]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](assets/dlfb_0306.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0306.png)'
- en: Figure 3-6\. Training and validation loss over learning steps. Both decrease,
    indicating the model is learning signal in the data over time. Note that training
    loss curve is comparatively noisier due to it being computed on a small batch
    of the data at each step (rather than the full training set), which introduces
    variability.
  id: totrans-275
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6. 随着学习步骤的增加，训练和验证损失。两者都在下降，表明模型随着时间的推移在数据中学习信号。请注意，由于每一步都是在数据的小批次上计算的（而不是整个训练集），因此训练损失曲线相对较嘈杂，这引入了变异性。
- en: Reaching this stage—where you have a working model, a dataset, and training
    with decreasing loss—is a major milestone. The rest of this chapter focuses on
    improving and extending this foundation.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 达到这个阶段——你有一个工作的模型、数据集，并且随着损失的下降进行训练——是一个重要的里程碑。本章的其余部分将专注于改进和扩展这个基础。
- en: Sanity-checking the model
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查模型的合理性
- en: 'Before we do anything more complicated, we should first check that the model
    has learned something sensible. One simple check is verifying that the trained
    model behaves as expected on known DNA motifs. For example, from an online search,
    we can see that the CTCF transcription factor is known to prefer binding DNA sequences
    containing motifs similar `CCACCAGGGGGCGC`. Let’s construct the 200-base-long
    DNA string containing repeats of this motif and convert it to the one-hot encoded
    format that our model expects:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行更复杂的操作之前，我们应该首先检查模型是否学到了一些合理的东西。一个简单的检查是验证训练好的模型在已知的DNA基序上的行为是否符合预期。例如，通过在线搜索，我们可以看到CTCF转录因子已知更喜欢结合包含类似`CCACCAGGGGGCGC`基序的DNA序列。让我们构建一个包含这个基序重复的200碱基长的DNA字符串，并将其转换为模型期望的one-hot编码格式：
- en: '[PRE36]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Output:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE37]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We expect that the model will predict a very high probability of CTCF binding
    this sequence, since it’s packed with the relevant motif. Let’s check:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计模型将预测CTCF结合这个序列的高概率，因为它富含相关的基序。让我们检查一下：
- en: '[PRE38]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Output:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE39]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: And this is indeed the case—the predicted probability of binding is very close
    to 1\. This means the model has learned to identify this motif in the DNA sequence
    and associate it with CTCF binding.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 确实如此——预测的结合概率非常接近1。这意味着模型已经学会了在DNA序列中识别这个基序，并将其与CTCF结合相关联。
- en: 'Conversely, if we construct some pseudorandom DNA strings, the model should
    predict a relatively low probability of CTCF binding them:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果我们构建一些伪随机DNA字符串，模型应该预测CTCF结合它们的概率相对较低：
- en: '[PRE40]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Output:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE41]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As expected, these probabilities look relatively low, meaning that the CTCF
    protein is not likely to bind these sequences of random DNA. This completes a
    basic sanity check of our approach, but let’s dig a bit deeper into what this
    model has already learned about DNA sequences.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，这些概率看起来相对较低，这意味着CTCF蛋白不太可能结合这些随机的DNA序列。这完成了对我们方法的基本合理性检查，但让我们进一步深入了解一下这个模型已经学到了关于DNA序列的哪些知识。
- en: Increasing Complexity
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增加复杂性
- en: In this section, we introduce two important extensions to our modeling approach.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了对我们建模方法的两个重要扩展。
- en: First, we focus on *model interpretation*. We’ll apply two techniques from the
    earlier machine learning primer—in silico mutagenesis (ISM) and input gradients—to
    better understand what the model has learned. These methods produce *contribution
    scores* (or *saliency maps*) that assign an importance value to each base in the
    DNA sequence, indicating how much that base influences the model’s prediction
    of TF binding.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们关注**模型解释**。我们将应用早期机器学习入门中提到的两种技术——在硅突变（ISM）和输入梯度，以更好地理解模型学到了什么。这些方法产生**贡献分数**（或**显著性图**），为DNA序列中的每个碱基分配一个重要性值，表明该碱基对模型预测TF结合的影响程度。
- en: Second, we’ll expand the scope of our prediction task. Instead of predicting
    binding for just one transcription factor (CTCF), we train models for *all 10
    transcription factors* in the dataset. This allows us to explore how model performance
    varies across TFs and how motif preferences differ between them.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们将扩大预测任务的范围。我们不仅预测一个转录因子（CTCF）的结合，我们还为数据集中的所有10个转录因子训练模型。这使我们能够探索模型性能在转录因子之间的变化，以及它们之间基序偏好的差异。
- en: Together, these steps deepen both our understanding of the model’s behavior
    and the complexity of the biological task it’s modeling. After this, we’ll turn
    our attention to improving the model architecture itself.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤共同加深了我们对于模型行为和所建模的生物任务复杂性的理解。在此之后，我们将把注意力转向改进模型架构本身。
- en: Conducting In Silico Mutagenesis
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行在硅突变
- en: Recall from the introduction that ISM is a technique in which each base in a
    DNA sequence is systematically mutated to all possible alternative bases one at
    a time, and the effect of each mutation on a given output (in this example, CTCF
    binding probability) is quantified. This allows us to identify which regions are
    important to the output—unimportant regions can be freely mutated without impacting
    predictions, whereas important regions significantly affect the probability of
    TF binding.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下引言中提到的ISM技术，这是一种将DNA序列中的每个碱基系统地逐个突变到所有可能的替代碱基的技术，并量化每个突变对给定输出（在这个例子中，是CTCF结合概率）的影响。这使我们能够确定哪些区域对输出很重要——不重要的区域可以自由突变，而不会影响预测，而重要的区域会显著影响转录因子结合的概率。
- en: 'Before making all possible mutations, let’s first check the effect of making
    just a single mutation. We’ll start by identifying a DNA sequence in the validation
    set that binds the CTCF protein (i.e., has a label of `1`):'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行所有可能的突变之前，让我们首先检查仅进行单个突变的效果。我们将从验证集中识别一个结合CTCF蛋白的DNA序列（即标签为`1`）：
- en: '[PRE42]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Output:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE43]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, let’s examine what the model predicts for this unmodified sequence:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检查模型对这个未修改的序列的预测：
- en: '[PRE44]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Output:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE45]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The original sequence has a predicted binding probability of 95.8%.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 原始序列的预测结合概率为95.8%。
- en: 'Now let’s create a single mutation at position 100\. The original base is a
    `G` (encoded as `[0, 0, 1, 0]`), and we change it to a `C` (encoded as `[0, 1,
    0, 0]`):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在位置100处创建一个单一突变。原始碱基是一个`G`（编码为`[0, 0, 1, 0]`），我们将其更改为一个`C`（编码为`[0, 1, 0,
    0]`）：
- en: '[PRE46]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Output:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE47]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We’ll now run the model again to see if this mutation has a measurable effect
    on the prediction:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将再次运行模型，看看这个突变是否对预测有可测量的影响：
- en: '[PRE48]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Output:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE49]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: After the mutation, the model’s predicted binding probability drops to 93.4%,
    a decrease of over 2.4%. This shows that even a single-base change at a sensitive
    position can substantially impact the model’s output.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 突变后，模型的预测结合概率降至93.4%，下降了超过2.4%。这表明即使在敏感位置的单碱基变化也会对模型的输出产生重大影响。
- en: Now that we’ve observed how a single mutation can influence the prediction,
    let’s extend this approach to systematically mutate every position in the sequence.
    This will give us a more global view of which bases matter most for the model’s
    prediction.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经观察到单个突变如何影响预测，让我们将这种方法扩展到系统地突变序列中的每个位置。这将使我们能够更全面地了解哪些碱基对模型的预测最为重要。
- en: Implementing in silico saturation mutagenesis
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实施计算机模拟饱和突变
- en: 'Here’s the basic plan for our implementation:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们实现的基本计划：
- en: Mutate
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 突变
- en: Start with the original sequence. At each position, change the base to each
    of the other three possible DNA bases.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始序列开始。在每个位置，将碱基更改为其他三种可能的DNA碱基之一。
- en: Predict
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 预测
- en: Run the model on each mutated sequence and record the predicted binding probability.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个突变的序列上运行模型并记录预测的结合概率。
- en: Aggregate
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合
- en: Collect all results to identify which mutations cause meaningful changes.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 收集所有结果以确定哪些突变会引起有意义的改变。
- en: 'We begin by generating all possible single-base mutations:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先生成所有可能的单碱基突变：
- en: '[PRE50]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Output:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE51]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We now have 800 sequences: four variants for each of the 200 positions. Although
    only three of those per position are true mutations (since mutating, say, `A`
    to an `A` is a no-op), we include all four to simplify downstream logic.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有800个序列：每个200个位置有四个变体。尽管每个位置只有三个是真正的突变（因为将`A`突变到`A`是无操作），但我们包括所有四个以简化下游逻辑。
- en: 'We can now run predictions on all these mutated sequences in a single batch:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以一次性对这些突变序列进行预测：
- en: '[PRE52]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let’s visualize the predicted binding probabilities for every mutated sequence
    in [Figure 3-7](#mutation-plot-saturated):'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化每个突变序列的预测结合概率，如图3-7[图3-7](#mutation-plot-saturated)所示：
- en: '[PRE53]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![](assets/dlfb_0307.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0307.png)'
- en: Figure 3-7\. All possible variations of a 200-base pair DNA region and their
    corresponding probabilities of TF binding, represented as a heatmap. The x-axis
    indicates a position in the DNA sequence, and the y-axis represents each possible
    DNA base at that position. The value represents the predicted probability of CTCF
    binding.
  id: totrans-336
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7\. 200碱基对DNA区域的所有可能变体及其对应的TF结合概率，以热图形式表示。x轴表示DNA序列中的位置，y轴表示该位置上每个可能的DNA碱基。值表示CTCF结合的预测概率。
- en: 'This shows us that:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明：
- en: Most mutations don’t change the prediction much. The model predicts close to
    the original 95.8% probability of CTCF binding for sequences containing most mutations.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数突变对预测的影响不大。模型预测含有大多数突变的序列接近原始95.8%的CTCF结合概率。
- en: A central region does appear to significantly affect the binding prediction.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个中心区域似乎确实对结合预测有显著影响。
- en: 'But what we really care about is how much each mutation changes the prediction.
    Let’s subtract the original (unmutated) predicted probability and center the color
    map at zero in [Figure 3-8](#mutation-plot-difference):'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们真正关心的是每个突变对预测的影响程度。让我们减去原始（未突变）的预测概率，并在图3-8[图3-8](#mutation-plot-difference)中将颜色图中心对齐到零：
- en: '[PRE54]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![](assets/dlfb_0308.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0308.png)'
- en: Figure 3-8\. In silico mutagenesis results showing the change in predicted CTCF
    binding probability for each possible mutation. Light and dark shades indicate
    a deviation from the original prediction, with lighter colors showing mutations
    that increase predicted binding and darker colors showing those that decrease
    binding.
  id: totrans-343
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8。在硅突变的结果显示每个可能的突变对预测CTCF结合概率的变化。浅色和深色表示与原始预测的偏差，较浅的颜色表示增加预测结合的突变，较深的颜色表示减少结合的突变。
- en: 'Now the heatmap is clearer:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，热图更清晰了：
- en: Most positions are lighter, meaning their mutations have little effect.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数位置较浅，意味着它们的突变影响很小。
- en: A few bases in the center are dark (decreased binding) or light (increased binding),
    showing meaningful influence.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中心的一些碱基是深色（结合减少）或浅色（结合增加），显示出有意义的影响。
- en: We can quantify mutation effects using a helper function, `describe_change`.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用辅助函数`describe_change`来量化突变效应。
- en: 'It allows us to have a look at the impact of mutating position 100 for all
    bases:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许我们查看突变位置100对所有碱基的影响：
- en: '[PRE55]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Output:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE56]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let’s also summarize the overall importance of each position in the DNA sequence
    by summing the absolute changes across all possible base mutations at each position.
    The result is visualized in [Figure 3-9](#mutation-plot-stacked), highlighting
    which regions the model considers most influential for its prediction:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将每个位置上所有可能的碱基突变的所有绝对变化相加来总结DNA序列中每个位置的总体重要性。结果在[图3-9](#mutation-plot-stacked)中可视化，突出显示模型认为对其预测最有影响力的区域：
- en: '[PRE57]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![](assets/dlfb_0309.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0309.png)'
- en: Figure 3-9\. Positional importance of the TF binding motif. The bottom panel
    is the same as in [Figure 3-8](#mutation-plot-difference) with a linegraph superimposed.
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9。TF结合基序的位置重要性。底部面板与[图3-8](#mutation-plot-difference)相同，叠加了线图。
- en: We can identify the most impactful mutations by ranking the values. In this
    case, the largest *increase* comes from mutating the base at position 92 with
    G→C (3.11% increase), and the biggest decrease comes from mutating the base at
    position 102 with G→T (-20.24% decrease).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对值进行排序来识别最有影响的突变。在这种情况下，最大的*增加*来自将位置92的碱基从G突变为C（增加3.11%），最大的*减少*来自将位置102的碱基从G突变为T（减少20.24%）。
- en: Note
  id: totrans-357
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This is a great moment to reflect on the biological meaning of these plots.
    The central region of the sequence likely contains the CTCF binding motif—mutations
    here have a strong impact on the model’s prediction. In contrast, flanking regions
    show little effect, indicating they contribute less to binding. Interestingly,
    most impactful mutations in the core motif tend to reduce predicted binding, suggesting
    the original sequence already contains a fairly strong CTCF motif that’s difficult
    to strengthen with single-base changes.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们反思这些图生物意义的绝佳时刻。序列的中心区域可能包含CTCF结合基序——这里的突变对模型的预测有强烈的影响。相比之下，侧翼区域影响很小，表明它们对结合的贡献较少。有趣的是，核心基序中最有影响的突变往往减少预测结合，这表明原始序列已经包含了一个相当强的CTCF基序，很难通过单碱基变化来增强。
- en: Verifying motif presence
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证基序的存在
- en: 'To validate whether the region identified by our model corresponds to a known
    CTCF binding motif, we can use an external bioinformatics tool. But first, we
    need to convert the one-hot encoded sequence back into the standard DNA string
    format. This is done using the `one_hot_to_dna` function:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们模型识别的区域是否对应于已知的CTCF结合基序，我们可以使用外部生物信息学工具。但首先，我们需要将one-hot编码的序列转换回标准的DNA字符串格式。这是通过使用`one_hot_to_dna`函数完成的：
- en: '[PRE58]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'which we can use on our sequence:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用它来分析我们的序列：
- en: '[PRE59]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Output:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE60]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: After converting the sequence, we paste it into the FIMO (Find Individual Motif
    Occurrences) tool, which is part of the MEME suite of motif discovery and search
    tools. FIMO allows for fuzzy matching of motifs—meaning the match doesn’t need
    to be exact—which better reflects biological reality, as transcription factors
    often tolerate some variability in their binding sites.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在将序列转换后，我们将其粘贴到FIMO（Find Individual Motif Occurrences）工具中，该工具是MEME套件中的一部分，用于基序发现和搜索工具。FIMO允许基序进行模糊匹配——这意味着匹配不需要完全精确，这更好地反映了生物现实，因为转录因子通常可以容忍其结合位点的一些变化。
- en: For CTCF, the known binding motif we used was CCACCAGGGGGCGC. When we submitted
    our sequence, FIMO reported a match to the CTCF motif (specifically, the subsequence
    GCCTCTGGGGGCGC, spanning positions 93 to 106) with a highly significant p-value
    of 2.6e-05, as shown in [Figure 3-10](#dlfb_0310).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CTCF，我们使用的已知结合基序是 CCACCAGGGGGCGC。当我们提交序列时，FIMO 报告了一个与 CTCF 基序（具体来说，是子序列 GCCTCTGGGGGCGC，跨越位置
    93 到 106）的匹配，具有高度显著的 p 值 2.6e-05，如 [图 3-10](#dlfb_0310) 所示。
- en: '![](assets/dlfb_0310.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0310.png)'
- en: Figure 3-10\. How we used FIMO to search for the known CTCF binding motif within
    a 200 bp DNA sequence labeled as positive. Although transcription factor motifs
    are usually represented more flexibly using position weight matrices (PWMs), a
    simple string-based search for the canonical motif `CCACCAGGGGGCGC` offers a quick
    sanity check to confirm that the expected pattern is present in the sequence.
  id: totrans-369
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-10\. 我们如何使用 FIMO 在标记为阳性的 200 bp DNA 序列中搜索已知的 CTCF 结合基序。尽管转录因子基序通常使用位置权重矩阵
    (PWMs) 更灵活地表示，但基于简单字符串的搜索以验证标准基序 `CCACCAGGGGGCGC` 的存在，可以快速进行合理性检查。
- en: 'To further verify this, we can overlay the motif region found by FIMO on our
    earlier saliency map of mutation impact. This helps us visually compare where
    mutations have the strongest effect on the model’s prediction versus where the
    detected motif actually occurs (see [Figure 3-11](#mutation-plot-stacked-highlighted)):'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步验证这一点，我们可以将 FIMO 找到的基序区域叠加到我们之前突变更显性图上。这有助于我们直观地比较突变对模型预测影响最强的地方与检测到的基序实际出现的地方（参见
    [图 3-11](#mutation-plot-stacked-highlighted)）。
- en: '[PRE61]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![](assets/dlfb_0311.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0311.png)'
- en: Figure 3-11\. Highlight of the TF binding site overlapping neatly with the region
    where mutations have the biggest influence.
  id: totrans-373
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-11\. TF 结合位点与突变影响最大的区域完美重叠的突出显示。
- en: As you can see, the motif returned by FIMO lines up almost exactly with the
    most mutation-sensitive region identified by our model through in silico mutagenesis.
    This alignment gives us confidence that the model has learned to recognize meaningful
    biological signal—in this case, a known CTCF binding motif—directly from sequence
    data.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，FIMO 返回的基序几乎与我们的模型通过 in silico 突变识别的最突变敏感区域完全一致。这种对齐使我们确信模型已经学会了直接从序列数据中识别有意义的生物信号——在这种情况下，一个已知的
    CTCF 结合基序。
- en: Implementing input gradients
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现输入梯度
- en: 'While *in silico* mutagenesis provides highly interpretable insights, it can
    be computationally expensive—requiring multiple forward passes through the model
    for each position in the input sequence. An alternative approach that is much
    cheaper to compute is *input gradients*. This technique, introduced earlier in
    the chapter, relies on a simple idea: how much does the model output change if
    we nudge each input base slightly?'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 *in silico* 突变提供了高度可解释的见解，但它可能计算成本较高——需要通过模型对输入序列中的每个位置进行多次正向传递。一种计算成本更低的可选方法是
    *输入梯度*。这种技术在章节中较早介绍，依赖于一个简单想法：如果我们稍微调整每个输入碱基，模型输出会改变多少？
- en: Mathematically, this corresponds to computing the gradient of the model’s prediction
    with respect to its input sequence. If a small change in a particular base results
    in a large change in the output, that base must be important.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这对应于计算模型预测相对于其输入序列的梯度。如果特定碱基的小幅度变化导致输出的大幅度变化，那么这个碱基一定是重要的。
- en: 'The implementation is straightforward. We use `jax.grad` to take the derivative
    of the model’s predicted binding probability with respect to the one-hot input
    sequence:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 实现方法是直接的。我们使用 `jax.grad` 来计算模型预测的绑定概率相对于单热输入序列的导数：
- en: '[PRE62]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let’s apply it to the same sequence we used for ISM and inspect the output:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其应用于与 ISM 相同的序列，并检查输出：
- en: '[PRE63]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Output:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE64]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The result is a matrix of the same shape as the input sequence, where each value
    indicates how sensitive the model’s prediction is to a small change in a particular
    base at a particular position.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个与输入序列形状相同的矩阵，其中每个值表示模型预测对特定位置特定碱基的小幅度变化的敏感性。
- en: 'We can visualize this as a heatmap in [Figure 3-12](#mutation-plot-input-gradients):'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此可视化为 [图 3-12](#mutation-plot-input-gradients) 中的热图：
- en: '[PRE65]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Just like with ISM, we see that most positions have little influence on the
    prediction, while a central region—roughly positions 90 to 110—exhibits strong
    gradients. This reflects the bases that the model is most sensitive to for making
    its prediction. However, input gradients differ from ISM in key ways: the values
    are not bounded to represent discrete mutations, and even the base currently present
    at each position can have a nonzero gradient. This is because the gradient describes
    how the model’s output would change with an infinitesimal increase in that base’s
    activation—not an actual mutation. This makes gradients a bit more abstract but
    also more flexible and computationally efficient.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 就像ISM一样，我们发现大多数位置对预测的影响很小，而一个中心区域——大约位置90到110——表现出强烈的梯度。这反映了模型在做出预测时最敏感的碱基。然而，输入梯度与ISM在关键方面有所不同：值不是限定在表示离散突变，而且每个位置的当前碱基也可以有一个非零梯度。这是因为梯度描述了模型输出如何随着该碱基激活的无限小增加而变化——而不是实际的突变。这使得梯度更加抽象，但也更加灵活和计算高效。
- en: '![](assets/dlfb_0312.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0312.png)'
- en: Figure 3-12\. Input gradient saliency map for a CTCF-binding sequence, indicating
    the contribution of each DNA base to the model’s prediction of CTCF binding. The
    bottom heatmap shows input gradients per base (A/C/G/T), while the top line shows
    aggregated importance across positions. A clear central region stands out as most
    influential for the model’s decision.
  id: totrans-389
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-12。CTCF结合序列的输入梯度显著性图，表明每个DNA碱基对模型预测CTCF结合的贡献。底部的热图显示了每个碱基（A/C/G/T）的输入梯度，而顶部的一行显示了位置的重要性汇总。一个清晰的中心区域突出显示为对模型决策最有影响的区域。
- en: 'To examine this region more closely, we can zoom in and label the actual bases
    in the 90:110 base region in [Figure 3-13](#mutation-plot-input-gradients-zoomed):'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更仔细地检查这个区域，我们可以放大并标注[图3-13](#mutation-plot-input-gradients-zoomed)中90:110碱基区域的实际碱基：
- en: '[PRE66]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Output:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE67]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This plot confirms what we saw using ISM and the FIMO tool: the model is placing
    high importance on the central region of the sequence, which corresponds to a
    canonical CTCF binding motif.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图证实了我们使用ISM和FIMO工具看到的情况：模型高度重视序列的中心区域，这对应于一个标准的CTCF结合基序。
- en: While input gradients are computationally much cheaper than ISM, they can be
    somewhat noisier or harder to interpret due to model nonlinearities or saturation
    effects. Still, they offer a practical way to rapidly inspect what a model is
    attending to—especially when analyzing many sequences at once.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然输入梯度在计算上比ISM便宜得多，但由于模型的非线性或饱和效应，它们可能会有些嘈杂或难以解释。然而，它们提供了一种快速检查模型关注点的方法——尤其是在同时分析多个序列时。
- en: '![](assets/dlfb_0313.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0313.png)'
- en: Figure 3-13\. Zoomed input gradients over the CTCF motif. In this close-up view
    of the region identified as important by input gradients, each cell shows how
    the model’s predicted CTCF binding probability would change in response to a small
    increase in a specific base at a given position. Unlike in silico mutagenesis,
    even the base that is already present (e.g., G→G) can have a high gradient—indicating
    that the model is highly sensitive to that base and relies on it for its prediction.
    Strongly positive or negative values reflect positions where the model has learned
    a motif and is using it to assess CTCF binding.
  id: totrans-397
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-13。CTCF基序的放大输入梯度。在这个对输入梯度识别为重要区域的近距离观察中，每个单元格显示了模型预测的CTCF结合概率如何对特定位置上特定碱基的小幅度增加做出反应。与在硅突变不同，即使已经存在的碱基（例如，G→G）也可以有高梯度——表明模型对该碱基非常敏感，并依赖于它进行预测。强烈的正或负值反映了模型已经学习到的基序，并使用它来评估CTCF结合。
- en: 'So far, we have been analyzing a single sequence example (the first validation
    example). In [Figure 3-14](#mutation-plot-ten-input-gratient-panels) we extend
    this to the first 10 sequences in the validation set that are labeled 1 (i.e.,
    sequences that are known to bind CTCF), and visualize their input gradients:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在分析单个序列示例（第一个验证示例）。在[图3-14](#mutation-plot-ten-input-gratient-panels)中，我们将此扩展到验证集中标记为1的前10个序列（即已知结合CTCF的序列），并可视化它们的输入梯度：
- en: '[PRE68]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![](assets/dlfb_0314.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0314.png)'
- en: Figure 3-14\. Input gradients for 10 CTCF-bound sequences show consistent central
    regions of high importance, suggesting a shared motif-like structure driving model
    predictions. Each cell shows the gradient value for mutating a specific base at
    a given position. Strongly positive or negative gradients indicate high sensitivity
    to mutations.
  id: totrans-401
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-14。10个CTCF结合序列的输入梯度显示出一致的中心高重要性区域，表明驱动模型预测的共享基序样结构。每个细胞显示在给定位置突变特定碱基的梯度值。强烈正负梯度表示对突变的高度敏感性。
- en: These gradients show a consistent central pattern across the examples, indicating
    that the model has likely learned a motif centered within the sequence. While
    the width of the important region varies slightly, the signal is strong and localized.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 这些梯度在示例中显示出一致的中央模式，表明模型可能已经学习到序列内部的基序。虽然重要区域的宽度略有变化，但信号强烈且集中。
- en: 'By contrast, [Figure 3-15](#dlfb_0315) shows the input gradients for 10 negative
    examples—sequences that the model predicts do not bind CTCF. Here, the picture
    is far more heterogeneous:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，[图3-15](#dlfb_0315) 展示了10个负例的输入梯度——模型预测这些序列不会结合CTCF。在这里，图片的异质性要大得多：
- en: '[PRE69]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '![](assets/dlfb_0315.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0315.png)'
- en: Figure 3-15\. Input gradients for 10 nonbinding sequences reveal more diffuse
    or noisy importance patterns, but often retain a weak central focus—likely reflecting
    the peak-centered sampling strategy used during dataset construction.
  id: totrans-406
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-15。10个非结合序列的输入梯度揭示了更扩散或嘈杂的重要性模式，但通常保留一个微弱的中心焦点——这很可能反映了在数据集构建期间使用的峰值中心采样策略。
- en: In this plot, some sequences show a weaker but still present centered signal.
    Others display no clear motif or have diffuse scores spread across the entire
    sequence.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，一些序列显示出较弱但仍存在的中心信号。其他序列没有明显的基序或在整个序列中扩散的分数。
- en: This variation makes sense given the way the dataset was constructed. While
    these sequences are labeled negative, they were carefully matched to positive
    examples in terms of chromatin accessibility. That means they also come from open
    chromatin regions and may still contain weak or partial motifs—or even motifs
    for other TFs. As a result, the model might still “attend” to the center of the
    sequence, even if it ultimately predicts no binding.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 这种变化符合数据集的构建方式。虽然这些序列被标记为负例，但它们在染色质可及性方面与正例进行了精心匹配。这意味着它们也来自开放染色质区域，可能仍然包含弱或部分基序——甚至可能是其他转录因子的基序。因此，即使模型最终预测没有结合，模型仍然可能“关注”序列的中心。
- en: Note
  id: totrans-409
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: These findings serve as a good reminder that contribution scores like input
    gradients don’t just reflect the presence or absence of strong motifs. They also
    reveal where the model is looking for evidence and can surface subtle or confounding
    biological signals introduced by dataset design choices. In this case, the weak
    central patterns likely reflect a bias introduced by sampling from peak-centered
    open chromatin, even in negative (class 0) examples.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现是一个很好的提醒，即像输入梯度这样的贡献分数不仅反映了强基序的存在或不存在。它们还揭示了模型在寻找证据的地方，并可以揭示由数据集设计选择引入的微妙或混淆的生物信号。在这种情况下，弱中心模式很可能反映了从峰值中心开放染色质采样引入的偏差，即使在负例（类别0）中也是如此。
- en: Now that we’ve explored two complementary interpretation tools—ISM and input
    gradients—we’re in a better position to trust (and debug) what the model is learning.
    Let’s return to the modeling task and increase both the problem complexity and
    model capacity.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了两种互补的解释工具——ISM和输入梯度，我们更有信心（并调试）模型正在学习的内容。让我们回到建模任务，并增加问题复杂性和模型容量。
- en: Modeling Multiple Transcription Factors
  id: totrans-412
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个转录因子的建模
- en: 'So far, we’ve focused on a single transcription factor: CTCF. Let’s now increase
    the biological scope of our modeling by predicting binding for all 10 TFs in the
    dataset.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直关注单个转录因子：CTCF。现在让我们通过预测数据集中的所有10个TF的结合来扩大我们建模的生物范围。
- en: Preparing a multi-TF dataset
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备多TF数据集
- en: 'The dataset includes binding labels for the following 10 transcription factors:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包括以下10个转录因子的结合标签：
- en: '[PRE70]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Each of these TFs has its own distinct DNA-binding preference. For instance:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 每个TF都有自己的独特DNA结合偏好。例如：
- en: CTCF, as we’ve seen, binds motifs like `CCACCAGGGGGCGC`.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如我们所见，CTCF结合基序如`CCACCAGGGGGCGC`。
- en: MAX prefers the E-box motif `CACGTG`, important in regulating cell proliferation.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAX更喜欢调节细胞增殖的E-box基序`CACGTG`。
- en: SRF binds to `CCW6GG` (where W = A or T), a motif involved in muscle-specific
    gene expression.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SRF结合到`CCW6GG`（其中W = A或T），这是一个涉及肌肉特异性基因表达的基序。
- en: These preferences are often conserved across species, and the role of each TF
    can be deeply rooted in specific cell type identity or developmental processes.
    If you’re curious, it’s worth looking up some of these proteins—for example, REST
    is a key repressor in neurons, while MAX has a known role in oncogenesis through
    interaction with MYC.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 这些偏好通常在物种间得到保留，每个转录因子（TF）的作用可能深深植根于特定的细胞类型身份或发育过程中。如果你对此好奇，值得查阅一些这些蛋白质——例如，REST是神经元中的关键抑制因子，而MAX通过与MYC的相互作用在肿瘤发生中已知有作用。
- en: Our task now is to train models that can automatically discover these motif
    patterns and accurately associate them with TF binding.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的任务是训练能够自动发现这些基序模式并将其与TF结合准确关联的模型。
- en: 'There are two common strategies for tackling this problem:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的两种常见策略是：
- en: Multitask learning
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习
- en: Train a single model to output one prediction per TF, potentially learning shared
    representations across tasks.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个模型，使其为每个TF输出一个预测，可能在学习任务间共享表示。
- en: Single-task learning
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 单任务学习
- en: Train separate binary classification models for each TF independently.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 独立地为每个TF训练单独的二分类模型。
- en: In our case, the 10 TFs are not particularly closely related, and their binding
    preferences are quite distinct—ranging from insulator proteins like CTCF to general
    transcriptional regulators like MAX and neuron-specific factors like REST. Additionally,
    the dataset provides separate training sets for each TF, and the original paper
    trained them independently.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，10个TF之间并不特别紧密相关，它们的结合偏好相当不同——从绝缘蛋白如CTCF到通用转录调控因子如MAX和神经元特异性因子如REST。此外，数据集为每个TF提供了单独的训练集，原始论文也是独立训练它们的。
- en: 'Given these factors, we’ll follow a single-task learning approach: train 10
    individual binary classification models, one per TF. This also contrasts nicely
    with the multitask approach you saw in [Chapter 2](ch02.html#learning-the-language-of-proteins).'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些因素，我们将遵循单任务学习方法：训练10个独立的二分类模型，每个TF一个。这也与你在[第2章](ch02.html#learning-the-language-of-proteins)中看到的任务学习方法形成鲜明对比。
- en: Defining a more complex model
  id: totrans-430
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义一个更复杂的模型
- en: Now that we’re training on 10 separate TF binding tasks, it’s a good opportunity
    to improve the training stability and generalization of our model.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们正在对10个独立的TF结合任务进行训练，这是一个提高模型训练稳定性和泛化能力的好机会。
- en: We’ll retain the same core convolutional architecture from earlier, but introduce
    three standard deep learning techniques—batch normalization, dropout, and learning
    rate scheduling—that are widely used in CNNs and often help even for relatively
    shallow models and simple datasets.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保留之前相同的核心卷积架构，但引入三种标准的深度学习技术——批归一化、dropout和学习率调度，这些技术在CNN中广泛使用，并且经常帮助即使是相对较浅的模型和简单的数据集。
- en: Batch normalization
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化
- en: 'We add batch normalization after each convolutional layer to improve training
    stability:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每个卷积层之后添加批归一化以提高训练稳定性：
- en: Batch norm normalizes the activations across the batch, which helps smooth the
    optimization landscape and accelerates convergence.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批归一化在整个批次中归一化激活，这有助于平滑优化景观并加速收敛。
- en: Even though our network is relatively shallow, batch norm can still improve
    performance and robustness.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管我们的网络相对较浅，批归一化仍然可以提高性能和鲁棒性。
- en: 'A few implementation details:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 一些实现细节：
- en: Batch norm behaves differently during training and inference. During training,
    it computes mean and variance from the current batch and updates running averages;
    at inference time, it uses the learned averages.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批归一化在训练和推理期间表现不同。在训练期间，它从当前批次计算均值和方差并更新运行平均值；在推理时间，它使用学习到的平均值。
- en: In Flax, you control this behavior with the `is_training` flag using the `use_running_average=not
    is_training` pattern.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Flax中，你可以使用`is_training`标志通过`use_running_average=not is_training`模式来控制这种行为。
- en: Dropout regularization
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout正则化
- en: 'To reduce overfitting, we add dropout after the dense (fully connected) layers:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少过拟合，我们在密集（全连接）层之后添加dropout：
- en: Dropout randomly sets a portion of activations to zero during training, encouraging
    the model to learn redundant and robust features.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout在训练过程中随机将一部分激活设置为0，这鼓励模型学习冗余和鲁棒的特征。
- en: It is typically used after dense layers, rather than convolutions, because spatially
    shared convolutional filters already generalize well.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通常在密集层之后而不是卷积层之后使用，因为空间共享的卷积滤波器已经很好地泛化了。
- en: In Flax, dropout requires passing a PRNG (pseudorandom number generator) key
    to the forward pass.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Flax中，dropout需要将PRNG（伪随机数生成器）密钥传递给前向传递。
- en: We use a moderate dropout rate of 0.2, which adds some regularization without
    significantly reducing model capacity.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用0.2的中等dropout率，这增加了一些正则化，但不会显著降低模型容量。
- en: Learning rate scheduling
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率调度
- en: 'Rather than using a fixed learning rate, we adopt a learning rate schedule
    to guide training:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用固定学习率不同，我们采用学习率调度来指导训练：
- en: Dynamic learning rates usually start high (encouraging fast exploration) and
    decrease over time to help convergence.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态学习率通常开始较高（鼓励快速探索），随着时间的推移逐渐降低，以帮助收敛。
- en: Popular options include exponential decay, step decay, and cosine annealing.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的选项包括指数衰减、步长衰减和余弦退火。
- en: 'Implemented via `optax.cosine_decay_schedule`, we use a cosine decay schedule
    which gradually reduces the learning rate over the course of training. The shape
    of this learning rate schedule is visualized in [Figure 3-16](#dlfb_0316):'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`optax.cosine_decay_schedule`实现，我们使用余弦衰减调度，该调度在训练过程中逐渐降低学习率。这个学习率调度的形状在[图3-16](#dlfb_0316)中进行了可视化：
- en: '[PRE71]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '![](assets/dlfb_0316.png)'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图3-16](assets/dlfb_0316.png)'
- en: Figure 3-16\. Cosine decay learning rate schedule used during training. The
    learning rate starts at 0.001 and gradually decreases over 1,000 steps, helping
    the model explore early on and converge smoothly toward the end.
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-16。训练过程中使用的余弦衰减学习率调度。学习率从0.001开始，逐渐降低至1000步，帮助模型在早期进行探索，并在后期平滑收敛。
- en: Even though our dataset and architecture are fairly simple, these additions
    should help improve training stability and model generalization across the expanded
    set of transcription factor tasks.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的数据集和架构相当简单，但这些添加应该有助于提高训练稳定性和模型在转录因子任务扩展集上的泛化能力。
- en: 'Let’s now implement the updated model definition:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在实现更新的模型定义：
- en: '[PRE72]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Our `ConvModelV2` implementation is starting to get a bit long and repetitive.
    Later in the chapter, we’ll address this by refactoring out the repeated logic
    into `ConvBlock` and `DenseBlock` components to make the model definition more
    concise and modular.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`ConvModelV2`实现开始变得有点长且重复。在章节的后面，我们将通过将重复的逻辑重构到`ConvBlock`和`DenseBlock`组件中来解决这个问题，从而使模型定义更加简洁和模块化。
- en: 'For now, we have all the pieces needed to create the training state:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们已经拥有了创建训练状态所需的所有组件：
- en: '[PRE73]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The training step looks fairly similar to what we’ve seen before, but now it
    needs to handle both dropout and batch normalization.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 训练步骤看起来与之前看到的相当相似，但现在它需要处理dropout和批量归一化。
- en: 'One small additional change: the loss function is defined directly inside `train_step`.
    This is often done for readability and encapsulation, especially when the loss
    depends on extra mutable model state (like `batch_stats`) or dropout, both of
    which are now required in the forward pass:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小的额外变化：损失函数直接在`train_step`中定义。这通常是为了可读性和封装而做的，尤其是在损失依赖于额外的可变模型状态（如`batch_stats`）或dropout时，这两者现在都是前向传递所必需的：
- en: '[PRE74]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'To confirm that everything is wired up correctly, we can overfit to a single
    batch—that is, run a few steps on the same batch and check that the loss decreases:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认一切连接正确，我们可以对单个批次进行过拟合——也就是说，在同一个批次上运行几个步骤，并检查损失是否下降：
- en: '[PRE75]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Output:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE76]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Looks good—the loss decreases quickly. This indicates the model is capable of
    fitting the data.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错——损失迅速下降。这表明模型能够拟合数据。
- en: 'However, loss alone isn’t the best way to evaluate classification models. We
    want to monitor additional metrics like accuracy and the area under the ROC curve
    (auROC). We compute these in an evaluation step:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅损失并不是评估分类模型的最佳方式。我们希望监控额外的指标，如准确率和ROC曲线下的面积（auROC）。我们在评估步骤中计算这些指标：
- en: '[PRE77]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Note that since `scikit-learn` functions are not JAX compatible, the evaluation
    step is not decorated with `@jax.jit`.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于`scikit-learn`函数与JAX不兼容，评估步骤没有使用`@jax.jit`装饰。
- en: 'Let’s see the output of running `eval_step`:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看运行`eval_step`的输出：
- en: '[PRE78]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Output:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE79]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now that we’ve implemented the training and evaluation steps, let’s define
    a full training loop. The train function takes an initialized model state, training
    and validation datasets, and a few configuration parameters:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了训练和评估步骤，让我们定义一个完整的训练循环。`train`函数接受初始化的模型状态、训练和验证数据集以及一些配置参数：
- en: '[PRE80]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The training loop is quite similar to before, but as a quick recap, here is
    what it does:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环与之前相当相似，但作为一个快速回顾，以下是它所做的工作：
- en: Iterates through training steps using a progress bar
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用进度条遍历训练步骤
- en: Runs `train_step` to update model parameters
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`train_step`来更新模型参数
- en: Periodically evaluates on the validation set using `eval_step`
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期使用`eval_step`在验证集上进行评估
- en: Logs and stores metrics at each step using a custom MetricsLogger
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义MetricsLogger在每个步骤记录和存储指标
- en: 'Since we’re training one model per TF, we’ll use a utility function to load
    datasets split by TF name:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们为每个TF训练一个模型，我们将使用一个实用函数来加载按TF名称分割的数据集：
- en: '[PRE81]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: This function loads training, validation, and test splits for the given `transcription_factor`
    and converts them into TensorFlow datasets ready for batching.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数加载给定`transcription_factor`的训练、验证和测试分割，并将它们转换为TensorFlow数据集，以便进行批处理。
- en: 'We now have everything in place to train one model per TF:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为每个TF训练一个模型做好了所有准备：
- en: '[PRE82]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'With training complete, let’s take a look at just CTCF transcription factor
    performance in [Figure 3-17](#dlfb_0317):'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，让我们看看[图3-17](#dlfb_0317)中仅CTCF转录因子性能的情况：
- en: '[PRE83]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '![](assets/dlfb_0317.png)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0317.png)'
- en: Figure 3-17\. Learning curves for the CTCF transcription factor. The left panel
    shows the training and validation loss over time. The right panel tracks the model’s
    classification performance using validation set accuracy and auROC. Performance
    improves steadily during training and converges smoothly.
  id: totrans-490
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-17\. CTCF转录因子的学习曲线。左侧面板显示了训练和验证损失随时间的变化。右侧面板跟踪使用验证集准确率和auROC的模型分类性能。性能在训练过程中稳步提高并平滑收敛。
- en: Our model performs well on predicting whether CTCF binds a given DNA sequence,
    with training and validation losses decreasing and validation auROC approaching
    a perfect score of 1.0—consistent with the fact that CTCF is relatively easy to
    predict compared to some of the other TFs.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在预测CTCF是否结合给定的DNA序列方面表现良好，训练和验证损失下降，验证auROC接近完美的1.0分——这与CTCF相对于其他一些TFs来说更容易预测的事实一致。
- en: 'Let’s now visualize the training curves across all 10 TFs. We first process
    the logged metrics:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们可视化所有10个TFs的训练曲线。我们首先处理记录的指标：
- en: '[PRE84]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'We can then visualize their learning curves in [Figure 3-18](#learning-curves-10-tfs)
    sorted by auROC performance (best-performing TFs are first):'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以按auROC性能（最佳性能的TFs排在前面）排序，在[图3-18](#learning-curves-10-tfs)中可视化它们的学习曲线：
- en: '[PRE85]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '![](assets/dlfb_0318.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0318.png)'
- en: Figure 3-18\. Learning curves for all 10 TFs. Each panel shows the training
    and validation loss (solid lines), validation accuracy (dashed lines), and validation
    auROC (dotted lines) over training steps. TFs are ordered by peak auROC performance.
    While some TFs, like CTCF and ATF2, reach near-perfect performance quickly, others,
    such as ZNF24 and BACH1, prove more challenging to model.
  id: totrans-497
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-18\. 所有10个TFs的学习曲线。每个面板显示了训练步骤中的训练和验证损失（实线）、验证准确率（虚线）和验证auROC（点线）。TFs按峰值auROC性能排序。虽然一些TFs，如CTCF和ATF2，可以快速达到接近完美的性能，但其他如ZNF24和BACH1则更难以建模。
- en: Are these different TF binding models doing a good job? It can be hard to know
    what constitutes a “good enough” auROC—especially in biological settings where
    label noise and dataset complexity can vary widely. Fortunately, since our dataset
    comes from a published benchmark, we can directly compare our model’s results
    to those reported in the original paper.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的TF结合模型是否做得很好？知道什么构成了“足够好”的auROC可能很困难，特别是在生物环境中，标签噪声和数据集复杂性可能差异很大。幸运的是，由于我们的数据集来自已发表的基准，我们可以直接将我们的模型结果与原始论文中报告的结果进行比较。
- en: In [Figure 3 of the source paper](https://oreil.ly/QciKO), we see that auROC
    scores vary considerably across TFs, and some are challenging to predict well—even
    when using information from more advanced architectures like genomic language
    models (gLMs) trained with large-scale pretraining. The paper’s figure helps establish
    a performance ceiling for baseline CNNs trained on one-hot encoded DNA. Interestingly,
    the paper notes that pretrained gLM representations do not consistently outperform
    conventional approaches using one-hot inputs, suggesting that simple models can
    still be competitive on these TF binding tasks.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在[源论文的图3](https://oreil.ly/QciKO)中，我们可以看到在不同TFs（转录因子）之间，auROC分数差异很大，有些甚至在使用来自更高级架构（如使用大规模预训练的基因组语言模型（gLMs））的信息时也难以准确预测。论文中的图表有助于为基于one-hot编码DNA训练的基线CNNs建立性能上限。有趣的是，论文指出，预训练的gLM表示并不总是优于使用one-hot输入的传统方法，这表明简单模型在这些TF结合任务上仍然具有竞争力。
- en: 'The following are key takeaways from the paper’s figure:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从论文图表中得出的关键要点：
- en: CTCF and ATF2 are the most predictable TFs, with both one-hot and pretrained
    models achieving auROC scores above 0.95\. These TFs have strong, conserved binding
    motifs that are easy for models to learn.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CTCF和ATF2是最可预测的转录因子，一热编码和预训练模型都实现了超过0.95的auROC分数。这些转录因子具有强大的保守结合基序，易于模型学习。
- en: REST, MAX, and ELK1 show intermediate difficulty, with auROCs around 0.83 to
    0.85.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: REST、MAX和ELK1显示中等难度，auROC在0.83到0.85之间。
- en: ZNF24, BACH1, ARID3, and GABPA tend to be more difficult, with auROCs hovering
    in the 0.75 to 0.80 range.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZNF24、BACH1、ARID3和GABPA通常更难预测，其auROC值在0.75到0.80之间徘徊。
- en: 'Let’s now see how our own models compare. The following prints the peak validation
    auROC achieved by our CNNs trained independently for each TF:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看我们的模型是如何比较的。以下打印出我们为每个TF独立训练的CNN达到的峰值验证auROC：
- en: '[PRE86]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Output:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE87]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Our results closely mirror the rankings and scores from the paper—especially
    in terms of which TFs are easier or harder to predict. This consistency offers
    reassuring external validation that our training setup is functioning correctly
    and our models are learning something meaningful.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果与论文中的排名和分数非常接近——特别是在哪些转录因子更容易或更难预测方面。这种一致性提供了令人放心的外部验证，表明我们的训练设置正在正常工作，我们的模型正在学习有意义的内容。
- en: Let’s now explore how we might push our TF-binding prediction results further
    through more expressive architectures.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在探讨如何通过更表达性的架构进一步推动我们的TF结合预测结果。
- en: Advanced Techniques
  id: totrans-510
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级技术
- en: Before we introduce more complex model components, let’s first improve the clarity
    and modularity of our model architecture by refactoring it into reusable building
    blocks. This makes our code easier to read, extend, and maintain.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍更复杂的模型组件之前，让我们首先通过将其重构为可重用构建块来提高我们模型架构的清晰度和模块化。这使得我们的代码更容易阅读、扩展和维护。
- en: 'We will modularize our convolutional and MLP layers by creating two helper
    modules, `ConvBlock` and `MLPBlock`:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过创建两个辅助模块`ConvBlock`和`MLPBlock`来模块化我们的卷积和MLP层：
- en: '[PRE88]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'With these reusable blocks in place, we can now define a more compact and configurable
    model architecture:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些可重用块到位后，我们现在可以定义一个更紧凑和可配置的模型架构：
- en: '[PRE89]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: This architecture supports optional transformer blocks inserted between the
    convolutional feature extractors and the MLP classifier. You’ll also notice the
    introduction of `TransformerBlock`, which we’ll explore next.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 此架构支持在卷积特征提取器和MLP分类器之间插入可选的Transformer块。您还会注意到引入了`TransformerBlock`，我们将在下一节中探讨。
- en: Adding Self-attention and Transformer Blocks
  id: totrans-517
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加自注意力和Transformer块
- en: Our final architecture combines convolutional layers with transformer blocks.
    While convolutional layers are excellent at extracting local motif-like patterns
    from DNA, transformer blocks can capture long-range dependencies—patterns that
    span larger regions of the sequence. These two types of layers are highly complementary.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终架构结合了卷积层和Transformer块。虽然卷积层在从DNA中提取局部基序样模式方面非常出色，但Transformer块可以捕捉长距离依赖关系——跨越更大序列区域的模式。这两种类型的层高度互补。
- en: Note
  id: totrans-519
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In our case, the sequences are relatively short (just 200 base pairs), so the
    benefits of long-range attention may be modest. However, attention mechanisms
    become increasingly useful as input length increases. For longer genomic contexts—such
    as full gene bodies, or interactions between potentially distant regulatory elements
    such as promoter and enhancer sequences—transformers can integrate signals across
    a broader range than is possible with fixed-size convolutional kernels.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，序列相对较短（仅有200个碱基对），因此长距离注意力的好处可能有限。然而，随着输入长度的增加，注意力机制变得越来越有用。对于更长的基因组上下文——例如完整的基因体，或潜在的远距离调控元件之间的相互作用，如启动子和增强子序列——Transformer可以整合比固定大小卷积核更广泛的信号。
- en: 'Flax includes a built-in `SelfAttention` module, which we can use as the foundation
    for our `TransformerBlock`:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: Flax包含一个内置的`SelfAttention`模块，我们可以将其用作`TransformerBlock`的基础：
- en: '[PRE90]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'A few notes on this design:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此设计的几点说明：
- en: Self-attention
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力
- en: The core operation is `nn.SelfAttention`, which enables each position in the
    sequence to attend to every other position, capturing dependencies across the
    input.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 核心操作是`nn.SelfAttention`，它使序列中的每个位置都能关注到其他所有位置，从而捕捉输入中的依赖关系。
- en: Residual connections
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接
- en: The skip connections help stabilize training by allowing gradients to flow more
    easily through deep architectures.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 跳过连接有助于通过允许梯度更容易地通过深层架构来稳定训练。
- en: Layer normalization
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化
- en: Transformers typically use layer norm rather than batch norm, as it tends to
    be more stable for sequence-based models and is invariant to batch size.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器通常使用层归一化而不是批量归一化，因为它对基于序列的模型来说更稳定，并且对批量大小不变。
- en: Position information
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 位置信息
- en: In our current setup, we omit explicit positional encodings—this is a simplification
    that may be acceptable for short sequences (e.g., 200 bp), where the relative
    arrangement of motifs can still be learned through the convolutional layers preceding
    attention. For longer inputs or if attention is applied very early, consider adding
    learned or sinusoidal positional encodings.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的设置中，我们省略了显式的位置编码——这是一个简化，可能适用于短序列（例如，200 bp），其中基序的相对排列仍然可以通过注意力之前的卷积层来学习。对于更长的输入或如果注意力应用得非常早，请考虑添加学习或正弦位置编码。
- en: This modular `TransformerBlock` can now be optionally included between convolutional
    and MLP layers in our `ConvTransformerModel`.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 这个可选的`TransformerBlock`现在可以包含在我们的`ConvTransformerModel`中的卷积和MLP层之间。
- en: Defining Various Model Architectures
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义各种模型架构
- en: Now that our model is modular, we can experiment with different architectural
    settings to better understand their impact on performance. This kind of architectural
    exploration is common when tuning deep learning models—there’s rarely a single
    obvious “best” model, and trying multiple variants can reveal helpful insights.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们模型是模块化的，我们可以尝试不同的架构设置来更好地理解它们对性能的影响。这种架构探索在调整深度学习模型时很常见——很少有一个明显的“最佳”模型，尝试多个变体可以揭示有价值的见解。
- en: 'In the following code, we define several models with different settings:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们定义了具有不同设置的几个模型：
- en: '[PRE91]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'To test these variants, we’ll focus on the most difficult TF from our earlier
    results: ZNF24, which achieved a peak validation auROC of 0.76 in our initial
    model. This makes it a great candidate to explore whether architectural improvements
    can lead to better predictive performance.'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这些变体，我们将重点关注我们之前结果中最困难的TF：ZNF24，它在我们的初始模型中达到了0.76的峰值验证auROC。这使得它成为探索架构改进是否能带来更好的预测性能的理想候选者。
- en: Sweeping Over the Different Models
  id: totrans-538
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查不同的模型
- en: With all our model components now modularized, we can easily run systematic
    experiments to compare architectural choices.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们所有的模型组件都已模块化，我们可以轻松运行系统实验来比较架构选择。
- en: 'We’ll train several model variants—ranging from simpler ablations to transformer-enhanced
    architectures—on the most challenging TF in our benchmark: ZNF24\. This allows
    us to explore which components help or hurt performance on a relatively difficult
    task.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在基准测试中最具挑战性的TF：ZNF24上训练几个模型变体——从更简单的消融到增强的变换器架构——这使我们能够探索哪些组件有助于或损害相对困难的任务上的性能。
- en: 'With all of this in place, we can train the different models in the `model`
    dict:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些准备就绪后，我们可以在`model`字典中训练不同的模型：
- en: '[PRE92]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'After training each model, we extract logged metrics over training time and
    visualize the results:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 训练每个模型后，我们提取训练时间内的记录指标并可视化结果：
- en: '[PRE93]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'With this code, we can plot our learning curves and metrics over time in [Figure 3-19](#learning-curve-panels-model-variants-znf24):'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此代码，我们可以在[图3-19](#learning-curve-panels-model-variants-znf24)中绘制我们的学习曲线和随时间变化的指标：
- en: '[PRE94]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '![](assets/dlfb_0319.png)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0319.png)'
- en: Figure 3-19\. Learning curves for different model architectures on ZNF24 binding
    prediction. Each panel shows training and validation loss, accuracy, and auROC.
    We see that adding transformer blocks slightly improves performance, while removing
    capacity (e.g., fewer conv filters) hurts it.
  id: totrans-548
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-19\. 不同模型架构在ZNF24结合预测上的学习曲线。每个面板显示训练和验证损失、准确率和auROC。我们看到添加transformer块略微提高了性能，而移除容量（例如，更少的卷积滤波器）则损害了性能。
- en: 'To better isolate model differences, we can plot just the validation auROC
    over time in [Figure 3-20](#comparison-of-model-variants-znf24):'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地隔离模型差异，我们可以在[图3-20](#comparison-of-model-variants-znf24)中仅绘制随时间变化的验证auROC：
- en: '[PRE95]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '![](assets/dlfb_0320.png)'
  id: totrans-551
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0320.png)'
- en: Figure 3-20\. Validation auROC across training steps for each model variant.
    Models with transformer blocks outperform simpler baselines, while reducing convolutional
    capacity or removing MLPs tends to hurt performance.
  id: totrans-552
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-20\. 每个模型变体在训练步骤中的验证auROC。具有transformer块的模型优于更简单的基线，而减少卷积容量或移除MLP通常会损害性能。
- en: 'And here are the max auROC values for each model (by step 1,000):'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是每个模型（按步骤1,000）的最大auROC值：
- en: '[PRE96]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Output:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE97]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Some observations and hypotheses based on these results:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些结果的一些观察和假设：
- en: Convolutions are critical
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积至关重要
- en: The `single_conv_only` model underperforms relative to the baseline, and reducing
    the number of convolutional filters degrades performance further. This suggests
    that having multiple convolutional layers with sufficient capacity is important
    for learning meaningful sequence features. It may be worth exploring deeper convolutional
    stacks or wider kernels.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于基线，`single_conv_only` 模型表现不佳，减少卷积滤波器的数量进一步降低了性能。这表明拥有多个具有足够容量的卷积层对于学习有意义的序列特征很重要。探索更深的卷积堆叠或更宽的核可能值得考虑。
- en: Transformer blocks help
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模块有助于
- en: Adding one or two self-attention blocks gives a modest but consistent improvement
    in auROC, despite the input sequences being only 200 bp long. This supports the
    idea that even short DNA windows can benefit from modeling long-range dependencies.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个或两个自注意力模块可以在 auROC 上带来适度但一致的提升，尽管输入序列只有 200 个碱基长。这支持了即使是短的 DNA 窗口也能从建模长距离依赖中受益的观点。
- en: MLPs may be unnecessary
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 可能是不必要的
- en: Surprisingly, removing the MLP blocks doesn’t drastically hurt performance.
    This suggests that most of the representational power is coming from earlier convolutional
    layers, and the additional MLP layers may be redundant.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，移除 MLP 模块并没有大幅损害性能。这表明大部分的表示能力来自早期的卷积层，额外的 MLP 层可能是多余的。
- en: Training dynamics
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 训练动态
- en: Models with transformer blocks exhibit noisier validation loss during early
    training. This instability might be reduced with a smaller initial learning rate.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 带有 transformer 模块的模型在早期训练期间表现出更嘈杂的验证损失。这种不稳定性可能通过较小的初始学习率来减少。
- en: While we won’t exhaustively optimize hyperparameters here, these results demonstrate
    the value of modular model exploration and raise intriguing questions for further
    study.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这里不会全面优化超参数，但这些结果证明了模块化模型探索的价值，并提出了进一步研究的有趣问题。
- en: Evaluating on the Test Split
  id: totrans-567
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试集上进行评估
- en: The final step is to evaluate our best-performing model on the held-out test
    set. This ensures that the model’s performance generalizes to completely unseen
    data and was not overfit to the training or validation distributions.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是对我们的最佳性能模型在保留的测试集上进行评估。这确保了模型的表现可以泛化到完全未见过的数据，并且没有过度拟合训练或验证分布。
- en: 'We reload the checkpoint for the top model—selected based on the highest validation
    auROC—and evaluate it on the test split:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重新加载了基于最高验证 auROC 选择的顶级模型的检查点，并在测试分割上评估它：
- en: '[PRE98]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Output:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE99]'
  id: totrans-572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Success! Our best model achieves a comparable AUC on the test set to what we
    saw on the validation set, demonstrating good generalization to unseen data.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们的最佳模型在测试集上达到了与验证集上相当的 AUC，证明了良好的泛化能力。
- en: Extensions and Improvements
  id: totrans-574
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展和改进
- en: There are many ways to extend this work—in terms of both analyzing model behavior
    and exploring new modeling directions.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以扩展这项工作——无论是分析模型行为还是探索新的建模方向。
- en: 'A few analysis ideas:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 一些分析想法：
- en: Failure analysis
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 失败分析
- en: Inspect misclassified sequences. Do they correspond to originally noisy or lower-magnitude
    ChIP-seq peaks? Are there weak motif matches within them? Trace these sequences
    back to the raw data to understand the source of prediction errors.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 检查被错误分类的序列。它们是否对应于原本噪声较大或较低强度的 ChIP-seq 峰？它们内部是否存在弱基序匹配？将这些序列追踪回原始数据以了解预测错误的来源。
- en: Motif discovery
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 基序发现
- en: Use saliency maps to extract high-signal regions and align them to known motifs
    from databases like JASPAR or HOCOMOCO. You can also explore tools like TF-MoDISco
    for automatic motif discovery based on contribution scores.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 使用显著性图提取高信号区域，并将它们与来自 JASPAR 或 HOCOMOCO 等数据库的已知基序对齐。您还可以探索基于贡献分数的自动基序发现工具，如
    TF-MoDISco。
- en: Attribution comparisons
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 归因比较
- en: Compare in silico mutagenesis and input gradients across TFs. Do easier tasks
    yield more consistent or interpretable saliency patterns?
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 比较不同 TF 之间的计算机模拟突变和输入梯度。更容易的任务会产生更一致或可解释的显著性模式吗？
- en: Cross-TF generalization
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 跨 TF 泛化
- en: Evaluate a model trained on one TF against the validation data for another.
    Which TFs generalize well to others? This could reveal shared motif features or
    similarities in binding logic.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 评估在一个 TF 上训练的模型与另一个验证数据集的对比。哪些 TF 泛化得很好？这可能揭示了共享的基序特征或结合逻辑的相似性。
- en: Saliency reproducibility
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性可重复性
- en: Compare saliency maps across different training runs or architectures to assess
    how reliably motif patterns are captured.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 比较不同训练运行或架构中的显著性图，以评估基序模式被捕获的可靠性。
- en: 'And some potential modeling extensions:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 以及一些潜在建模扩展：
- en: Model optimization
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化
- en: Tune architectural hyperparameters such as the number of convolutional filters,
    kernel width, dropout rate, or transformer head count and MLP size.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 调整架构超参数，如卷积滤波器数量、核宽度、dropout率或transformer头数和MLP大小。
- en: Positional encodings
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码
- en: Add sinusoidal or learned positional embeddings to improve transformer modeling
    of sequence order—especially valuable for longer DNA sequences.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 添加正弦波或学习到的位置嵌入以改善transformer对序列顺序的建模——特别是对于较长的DNA序列非常有价值。
- en: Multitask setup
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务设置
- en: Build a multitask model that predicts binding for all TFs simultaneously. This
    allows shared representations across tasks and may improve performance on lower-data
    TFs with less data.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 建立一个多任务模型，同时预测所有TF的结合。这允许任务之间的共享表示，并可能提高具有较少数据的TF的性能。
- en: Quantitative binding prediction
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 定量结合预测
- en: Instead of binary classification, predict continuous binding intensity (e.g.,
    ChIP-seq signal). This would require adapting the model for sequence-to-sequence
    or dense regression output.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 与二分类不同，预测连续的结合强度（例如，ChIP-seq信号）。这需要调整模型以进行序列到序列或密集回归输出。
- en: Pretraining
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练
- en: Fine-tune pretrained DNA models such as DNABERT or Nucleotide Transformer to
    leverage prior genomic knowledge and improve performance with less data.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 微调预训练的DNA模型，如DNABERT或Nucleotide Transformer，以利用先前的基因组知识，并使用更少的数据提高性能。
- en: Data augmentation
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强
- en: Improve generalization with reverse complements, shifted windows (jittering),
    or synthetic motif insertions.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反向互补、移位窗口（抖动）或合成的基序插入来提高泛化能力。
- en: These directions offer exciting possibilities for improving accuracy, interpretability,
    and generalizability in TF binding prediction.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方向为提高TF结合预测的准确性、可解释性和泛化性提供了令人兴奋的可能性。
- en: Summary
  id: totrans-601
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the fascinating world of gene regulation through
    transcription factor binding. Starting with simple convolutional models, we built
    a foundation for understanding how neural networks can learn to recognize sequence
    motifs in raw DNA sequences. From there, we incrementally increased model complexity—adding
    batch normalization, dropout, and even transformer blocks—to investigate how architectural
    changes impact performance.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过转录因子结合探索了基因调控的迷人世界。从简单的卷积模型开始，我们为理解神经网络如何学习识别原始DNA序列中的序列基序奠定了基础。从那里，我们逐步增加模型复杂性——添加批量归一化、dropout，甚至transformer块——以研究架构变化对性能的影响。
- en: This gradual, modular approach to model development not only helped clarify
    the strengths of each component, but also made debugging and evaluation more manageable.
    Along the way, we touched on model interpretability, performance benchmarking,
    and ideas for deeper analysis and extensions.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 这种逐步的、模块化的模型开发方法不仅有助于阐明每个组件的优势，而且使调试和评估更加容易管理。在这个过程中，我们触及了模型可解释性、性能基准测试以及更深入分析和扩展的想法。
- en: 'Up to this point, we’ve focused on biological data with an inherent sequential
    structure—first with protein sequences in [Chapter 2, “Learning the Language of
    Proteins”](ch02.html#learning-the-language-of-proteins) and now with DNA. In the
    next chapter, we shift gears to a very different kind of data: *graphs*. We’ll
    explore how graph neural networks can help us reason about relationships between
    entities—in particular, interactions between different drugs.'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直关注具有固有顺序结构的生物数据——首先是[第2章，“学习蛋白质的语言”](ch02.html#learning-the-language-of-proteins)中的蛋白质序列，现在又是DNA。在下一章中，我们将转向一种非常不同的数据类型：*图*。我们将探讨图神经网络如何帮助我们推理实体之间的关系——特别是不同药物之间的相互作用。
- en: '^([1](ch03.html#id616-marker)) International Human Genome Sequencing Consortium,
    “Initial Sequencing and Analysis of the Human Genome.” Nature 409 (2001): 860–921.'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch03.html#id616-marker)) 国际人类基因组测序联盟，“人类基因组初步测序和分析。”自然409 (2001): 860–921。'
- en: '^([2](ch03.html#id638-marker)) Vaswani, Ashish, Noam Shazeer, Niki Parmar,
    Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
    2017\. [“Attention Is All You Need”](https://oreil.ly/-Qa6G). *arXiv (Cornell
    University)* 30 (June): 5998–6008.'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch03.html#id638-marker)) Vaswani, Ashish, Noam Shazeer, Niki Parmar,
    Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
    2017\. [“Attention Is All You Need”](https://oreil.ly/-Qa6G). *arXiv (康奈尔大学)* 30
    (六月): 5998–6008。'
- en: ^([3](ch03.html#id670-marker)) Tang, Z., Somia, N., Yu, Y., & Koo, P. K. (2024).
    [Evaluating the representational power of pre-trained DNA language models for
    regulatory genomics](https://doi.org/10.1101/2024.02.29.582810). bioRxiv (Cold
    Spring Harbor Laboratory).
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.html#id670-marker)) 唐泽，索米亚，余宇，与库帕·K. (2024). [评估预训练DNA语言模型在调控基因组学中的表征能力](https://doi.org/10.1101/2024.02.29.582810).
    bioRxiv (冷泉港实验室).
- en: ^([4](ch03.html#id671-marker)) Majdandzic, A., Rajesh, C., & Koo, P. K. (2023).
    [Correcting gradient-based interpretations of deep neural networks for genomics](https://doi.org/10.1186/s13059-023-02956-3).
    Genome Biology, 24(1).
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch03.html#id671-marker)) 马吉丹齐克，拉杰什，与库帕·K. (2023). [纠正基因组学中深度神经网络基于梯度的解释](https://doi.org/10.1186/s13059-023-02956-3).
    基因组生物学，24(1).
