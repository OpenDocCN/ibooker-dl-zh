- en: 2 Exploring tabular datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 探索表格数据集
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Row and column characteristics in a tabular dataset
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格数据集中的行和列特征
- en: Possible pathologies and remedies for tabular datasets
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格数据集的可能病理和补救措施
- en: Finding tabular data externally on the internet and internally in organizations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在互联网上和机构内部寻找表格数据
- en: Exploring data to solve common problems in tabular data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据以解决表格数据中的常见问题
- en: Tabular data may consist of practically anything—from low-level scientific research
    to consumer behavior on a website to the statistics in your fantasy sports league.
    In the end, though, the commonalities in tabular data prevail over differences,
    and you can achieve most of your data analysis job just by applying standard approaches
    and tools even without a lot of domain expertise.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 表格数据可能包含几乎所有内容——从低级科学研究到网站上的消费者行为，再到你幻想体育联盟中的统计数据。然而，最终，表格数据中的共性胜过差异，你只需应用标准方法和工具，即使没有太多的领域专业知识，也能完成大部分数据分析工作。
- en: 'In this chapter, we’ll look at how to gather and prepare tabular datasets.
    We’ll also take on a practical data analysis exploration that shows the steps
    you can take to look at data from different viewpoints: by rows, by columns, under
    the light of the relationship between features, and considering their overall
    distribution in the dataset. For that example, we will use a simple toy dataset,
    the Auto MPG Data Set, a dataset freely available on the UCI Machine Learning
    website ([https://archive.ics.uci.edu/dataset/9/auto+mpg](https://archive.ics.uci.edu/dataset/9/auto+mpg)).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何收集和准备表格数据集。我们还将进行一项实际的数据分析探索，展示你可以采取的步骤来从不同的视角查看数据：按行、按列、在特征之间关系的背景下，以及考虑它们在数据集中的整体分布。为此示例，我们将使用一个简单的玩具数据集，即汽车燃油效率数据集（Auto
    MPG Data Set），这是一个在UCI机器学习网站上免费可用的数据集（[https://archive.ics.uci.edu/dataset/9/auto+mpg](https://archive.ics.uci.edu/dataset/9/auto+mpg)）。
- en: 2.1 Row and column characteristics
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 行和列特征
- en: Depending on the domain, it is incredible how much variety you will find in
    tabular data. That is because tabular data is the rule, not the exception, in
    the world of data, and that has been true since the very beginning. Tabular data
    has been collected for thousands of years into tables and records, from grain
    accounting in ancient Egypt to parish births, weddings, and deaths in medieval
    Europe, up to our days in modern national countries and their bureaucracies. It
    wasn’t until the 1960s that we began collecting data in computerized databases,
    which gave the term *tabular* a more electronic connotation. The widespread adoption
    of relational databases since the 1970s has popularized tabular data, making it
    ubiquitous and used for every possible application. In relational databases, data
    tables can be combined by the values of specific columns acting as joining keys.
    Such an innovation allowed computers to store more information in less disk space,
    guaranteeing the technology’s success and widespread diffusion.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 根据领域不同，你会在表格数据中发现多么丰富的多样性，这真是令人难以置信。这是因为表格数据在数据世界中是规则而非例外，这种情况从一开始就是如此。数千年来，表格数据被收集到表格和记录中，从古埃及的谷物会计到中世纪欧洲的教区出生、婚礼和死亡记录，一直到我们现代国家的官僚机构。直到20世纪60年代，我们才开始在计算机化的数据库中收集数据，这给“表格”这个词赋予了更多的电子含义。自20世纪70年代以来，关系数据库的广泛应用普及了表格数据，使其无处不在，并用于各种可能的应用。在关系数据库中，可以通过作为连接键的特定列的值来组合数据表。这种创新使得计算机能够在更少的磁盘空间中存储更多信息，保证了技术的成功和广泛传播。
- en: You can look at open data repositories or data science competitions, such as
    Kaggle, which routinely feature tabular data competitions to give you an idea
    of the current sheer variety of tabular data. For instance, in the last two years,
    Kaggle organized the Tabular Playground Series ([https://mng.bz/pK2z](https://mng.bz/pK2z)),
    a series of competitions inspired by most common machine learning problems involving
    tabular datasets and using synthetic data devised by generative AI, as we will
    discuss more later in the chapter. Although the Tabular Playground competitions
    use generated data, the examples and the original data they took inspiration from
    are selected from real-world examples such as
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看公开数据存储库或数据科学竞赛，例如Kaggle，它通常会举办表格数据竞赛，以给你一个关于当前表格数据多样性的概念。例如，在过去的两年中，Kaggle组织了表格游乐场系列赛（[https://mng.bz/pK2z](https://mng.bz/pK2z)），这是一系列受最常见的机器学习问题启发的竞赛，这些问题涉及表格数据集和由生成式AI设计的合成数据，我们将在本章后面进一步讨论。尽管表格游乐场竞赛使用生成数据，但它们从中汲取灵感的例子和原始数据是从现实世界的例子中精选出来的，例如
- en: Probability and amount of insurance claims
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保险索赔的概率和金额
- en: Loan defaults in the banking sector
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 银行领域的贷款违约
- en: Product testing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品测试
- en: E-commerce sales
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子商务销售
- en: Environment sensor data
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境传感器数据
- en: Biological and genomic data
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生物和基因组数据
- en: Ecological measurements
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生态测量
- en: However, despite the variety of real-world applications and underlying knowledge
    domains, every tabular dataset shares the same structure of a matrix of rows and
    columns with values in numeric, time or dates, and textual forms. Such applies
    to all tabular datasets, regardless of their features. While domain knowledge
    is essential for devising optimal feature engineering for predictive algorithms,
    the basic structure of tabular data remains consistent across all domains. Such
    universality justifies the need for a book on tabular data, as examples and techniques
    can be easily transferred between domains.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管有各种各样的现实应用和潜在的知识领域，每个表格数据集都共享相同的结构，即行和列的矩阵，其中包含数值、时间和日期以及文本形式的数据。这适用于所有表格数据集，无论它们的特征如何。虽然领域知识对于设计预测算法的最佳特征工程至关重要，但表格数据的基本结构在所有领域都是一致的。这种普遍性证明了编写关于表格数据书籍的必要性，因为示例和技术可以轻松地在领域之间转移。
- en: Diving deeper into details, in a data table, also called a dataset, you have
    rows of values, and each row represents a unit of your analysis, which, in statistical
    terms, can be mentioned as a statistical unit or an observation. If you are analyzing
    DNA samples, for example, each row in your table represents a sample. If you are
    analyzing industrial products, each row will be a product. The principle is the
    same, and the nature of the represented units can significantly vary.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深入细节，在一个数据表中，也称为数据集，你有值行，每一行代表你的分析单位，在统计学上，这可以称为统计单位或观察。例如，如果你正在分析DNA样本，那么你表中的每一行代表一个样本。如果你正在分析工业产品，每一行将代表一个产品。原则是相同的，而所代表单位的性质可以显著不同。
- en: 2.1.1 The ideal criteria for tabular rows
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 表格行的理想标准
- en: The only limit to remember for rows in a tabular dataset is that examples should
    be independent unless you are working with time predictions (time series analysis)
    or other time-related problems. That’s the IID principle that you may have heard
    about before. IID is an acronym for “independent and identically distributed,”
    meaning that your samples should have been drawn independently, where each drawing
    doesn’t influence or carry information of the subsequent ones. Identically, that
    means that you always draw in the same way from the same data distribution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格数据集中，你需要记住的唯一限制是，除非你正在处理时间预测（时间序列分析）或其他与时间相关的问题，否则示例应该是独立的。这就是你可能之前听说过的IID原则。IID是“独立且同分布”的缩写，意味着你的样本应该是独立抽取的，其中每一次抽取都不影响或携带后续抽取的信息。同样，这意味着你总是以相同的方式从相同的数据分布中抽取。
- en: 'Let’s consider a simple example of IID: a coin flip. Each time we flip a coin,
    the outcome is independent of all the previous flips, and the probability of getting
    heads or tails is the same for each flip. In other words, the coin flips are identically
    distributed, according to Bernoulli distribution. If we were to generate a dataset
    by flipping the same coin repeatedly, the resulting data would be IID. The property
    of being identically distributed allows us to simplify the modeling and analysis
    of the data. For example, it allows us to randomly sample the data when creating
    cross-validation folds. It allows us to assume that predictive algorithms will
    not memorize and replicate the order of the data presented. Typical examples of
    non-IID are sales data from multiple stores, where sales from the same store tend
    to be very correlated and not necessarily with other stores’ dynamics or school
    survey data, where each class presents similar characteristics due to shared interests
    or experiences, introducing non-IID characteristics into the data. In these instances,
    the data exhibits distinctive characteristics that deviate from the assumption
    of independence and identical distribution because the samples derive from specific
    groups (stores or school classes), but similar situations also arise when the
    data is hierarchically organized, when you deal with repeated measures when you
    basically measure the same example multiple times with different but correlated
    results, or when there is any temporal dependency, typical of time series, where
    being non-IID is not a problem.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的IID例子：抛硬币。每次我们抛硬币，结果都与所有之前的抛掷无关，每次抛掷得到正面或反面的概率都是相同的。换句话说，硬币抛掷是同分布的，根据伯努利分布。如果我们通过反复抛掷同一枚硬币来生成数据集，那么得到的数据将是IID。同分布的性质允许我们简化数据的建模和分析。例如，它允许我们在创建交叉验证折时随机采样数据。它允许我们假设预测算法不会记住并复制呈现的数据的顺序。非IID的典型例子包括来自多个商店的销售数据，其中同一商店的销售往往高度相关，并且不一定与其他商店的动态相关，或者学校调查数据，由于共同的兴趣或经验，每个班级都表现出相似的特征，这会将非IID特征引入数据中。在这些情况下，数据表现出与独立同分布假设相偏离的独特特征，因为样本来自特定的群体（商店或学校班级），但在数据分层组织时，当你处理重复测量时（基本上是多次以不同但相关的方式测量相同的例子），或者当存在任何时间依赖性时，例如典型的时间序列，其中非IID不是一个问题。
- en: 'In figure 2.1, we compare how non-IID and IID situations differ when you compare
    distributions as a plotted series. You can observe an IID behavior in a feature
    on the left panel. It is based on randomly combining dice rolls with coin flips.
    On the right panel, you can examine a non-IID behavior: just notice the jump appearing
    after a certain number of samples, implying something changed in how the distribution
    was generated or in how you sampled its values.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.1中，我们比较了在将分布作为绘制的序列进行比较时，非IID和IID情况如何不同。你可以在左面板上观察到IID行为：它基于随机组合骰子滚动和抛硬币。在右面板上，你可以检查非IID行为：只需注意在一定的样本数之后出现的跳跃，这表明在生成分布或采样其值的方式上发生了变化。
- en: '![](../Images/CH02_F01_Ryan2.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F01_Ryan2.png)'
- en: Figure 2.1 Comparison of IID and non--IID data sequences. The left chart shows
    IID data, while the right chart illustrates non-IID data, highlighting the sequential
    correlation patterns.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 IID和非IID数据序列的比较。左图显示了IID数据，而右图说明了非IID数据，突出了序列相关性模式。
- en: Even if we don’t know about the details of the distribution we are drawing from,
    which can be safely imagined as the result of some kind of unknown process, to
    have IID data, we should always be picking examples with no relation to each other
    and examples we presume are derived from the same situation or process. Non-IID
    data can affect your analysis in different ways. In particular, it can affect
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们对从其抽取的分布的细节一无所知，这可以安全地想象为某种未知过程的结果，为了得到独立同分布（IID）数据，我们始终应该选择彼此无关的例子，以及我们认为来自相同情境或过程的例子。非独立同分布（Non-IID）数据可以以不同的方式影响你的分析。特别是，它可能会影响
- en: '*Bootstrapping*, which is sampling with repetition from a sample until we obtain
    a new sample of the same size as the original, and *subsampling,* which is sampling
    without repetition from a sample until obtaining a new sample of the desired size.
    Both sampling procedures are affected because you may oversample or distort certain
    data signals, and dependency structures will be messed up. We will return to such
    sampling procedures because some learning algorithms discussed in chapter 4 use
    bootstrapping or subsampling.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重采样（Bootstrapping）*，即从样本中重复采样，直到我们获得与原始样本相同大小的新样本，以及*子采样（subsampling）*，即从样本中无重复地采样，直到获得所需大小的新样本。这两种采样过程都会受到影响，因为你可能会过度采样或扭曲某些数据信号，依赖结构也会被打乱。我们将在第4章讨论的一些学习算法中再次回到这样的采样过程，因为这些算法使用了重采样或子采样。'
- en: '*How your model learns,* because it can pick unwanted relationships among the
    samples that won’t be helpful at prediction time when test samples will be different
    from training one and not related to them. For instance, all learning algorithms
    based on stochasitc gradient descent (SDG) and mini-batch gradient descent, including
    deep learning, are affected by the order you present the samples to the algorithm.
    Consider how non-IID data, which has an intrinsic, hidden order, may affect the
    results from similar algorithms.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你的模型是如何学习的*，因为它可能会选择在预测时间无用的样本之间的不必要关系，当测试样本与训练样本不同且与它们无关时。例如，所有基于随机梯度下降（SGD）和迷你批梯度下降的学习算法，包括深度学习，都会受到你向算法展示样本顺序的影响。考虑一下非独立同分布数据，它具有内在的、隐藏的顺序，可能会如何影响类似算法的结果。'
- en: '*Cross-validation*, which is the most efficient testing method for validating
    your machine learning models and may provide inflated estimates when data is non-IID,
    because your algorithm may learn to cluster cases based on their relationship
    specifically in the training set.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*交叉验证（Cross-validation）*，这是验证你的机器学习模型的最有效测试方法，当数据是非独立同分布时，可能会提供夸大的估计，因为你的算法可能会学会根据它们在训练集中的特定关系来聚类案例。'
- en: Data that is not IID can be detected based on analysis of the data generation
    process and exploration because
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过分析数据生成过程和探索来检测非独立同分布的数据。
- en: 'In non-IID data, each sample is likely to exhibit some form of association
    with at least one other sample found within the same dataset. This often occurs
    because you are thinking of analyzing different units, but the units are the same.
    For instance, in medical analysis, you may be analyzing multiple medical records
    and thinking that your unit of analysis is the records themselves, which are distinct.
    In reality, you are analyzing the health data from the same patients at different
    times: the actual analysis units should be the patients, not the records. A very
    similar situation happened to the team of Prof. Andrew Ng (see [https://mng.bz/OBGE](https://mng.bz/OBGE)
    for details) when they prepared the data for a paper by using a dataset of 100,000
    x-rays from 30,000 patients, and, as they split it separating training and test
    data, they didn’t consider how the x-rays of the same patient could end up a part
    in the training set and a part in the test set, hence inflating all the results
    and distorting all the insights derived from the analysis.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在非独立同分布（non-IID）数据中，每个样本很可能与同一数据集中至少另一个样本存在某种形式的关联。这种情况通常发生是因为你在思考分析不同的单元，但实际上这些单元是相同的。例如，在医学分析中，你可能正在分析多个医疗记录，并认为你的分析单元是这些记录本身，它们是不同的。实际上，你正在分析来自同一患者在不同时间的健康数据：实际的分析单元应该是患者，而不是记录。安德鲁·吴教授的团队也遇到了一个非常类似的情况（详情见[https://mng.bz/OBGE](https://mng.bz/OBGE)），当他们准备一篇论文的数据时，使用了来自30,000名患者的10万张X光片的数据集，在分割数据以分离训练集和测试集时，他们没有考虑到同一患者的X光片可能会同时出现在训练集和测试集中，从而夸大了所有结果并扭曲了从分析中得出的所有见解。
- en: The measures in your data represent heterogeneous distributions. This typically
    happens with the passage of time and its consequent changes that reflect on the
    data at many levels. For instance, when analyzing the balance sheets of different
    companies, it is important to note that even if the companies are distinct, they
    may not represent the same distribution if they are from different years. This
    is because the macroeconomic situation is mutable and can change the characteristics
    of the companies and their sectors over time (i.e., the characteristics of the
    distribution you want to represent).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您数据中的度量代表异质分布。这种情况通常随着时间推移及其在多个层面上反映出的变化而发生。例如，在分析不同公司的资产负债表时，重要的是要注意，即使公司不同，如果它们来自不同的年份，它们可能不代表相同的分布。这是因为宏观经济状况是可变的，并且随着时间的推移可以改变公司及其行业的特征（即您想要表示的分布的特征）。
- en: Since every analysis is based on some expected distribution assumptions, data
    being IID is paramount for proper estimates in statistics and regression analysis.
    In machine learning, even if the methods are data-driven and nonparametric, IID
    data is strongly recommended, though, in practice, it is hardly found in real-world
    datasets. One fundamental limit of machine learning algorithms is that they are
    aware of the relations between features and the target but cannot figure out the
    relations between the rows. Machine learning algorithms are column-aware but not
    row-aware. Hence, it is necessary to adequately provide supplemental features
    to support the work of machine learning algorithms. Therefore, if two features
    are correlated, the algorithm will expect such correlation to derive from their
    relationship, independent of the interference of other features, not because of
    time or because of another hidden feature that affected the sampling. In other
    words, when data is not IID, the learning algorithm will learn patterns based
    on time or relationship among samples as if they were some kind of feature-based
    relationship.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每次分析都是基于一些预期的分布假设，因此数据是独立同分布（IID）对于统计和回归分析中的正确估计至关重要。在机器学习中，即使方法是数据驱动和非参数的，但强烈建议使用IID数据，尽管在实践中，在现实世界的数据集中几乎找不到。机器学习算法的一个基本限制是它们知道特征与目标之间的关系，但无法找出行与行之间的关系。机器学习算法是列感知但不是行感知的。因此，有必要充分提供补充特征以支持机器学习算法的工作。因此，如果两个特征相关，算法将期望这种相关性源于它们之间的关系，而不是其他特征的干扰，不是由于时间或由于影响抽样的另一个隐藏特征。换句话说，当数据不是IID时，学习算法将根据时间或样本之间的关系学习模式，就像它们是某种基于特征的关系一样。
- en: 'Times series and every kind of longitudinal data representing phenomena over
    time are typically not IID. In time series, each observation is autocorrelated,
    meaning that each target value is correlated with the previous ones, as well as
    the features, called covariates in statistics, that can explain the target. Given
    your business data problem and how you collect or assemble the data, you must
    know how time affects your observations and try to control its effect using time-based
    features. Thanks to time-based features, your model can determine how time affects
    the values of the other features and your target. Usually, a time feature and
    lagged features based on time will solve the situation. Essentially, you can have
    two scenarios here:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列和代表随时间推移的现象的任何类型的纵向数据通常不是IID。在时间序列中，每个观测值都是自相关的，这意味着每个目标值都与前一个值以及可以解释目标的特征（在统计学中称为协变量）相关。鉴于您的业务数据问题和您收集或组装数据的方式，您必须了解时间如何影响您的观测值，并尝试使用基于时间的特征来控制其影响。多亏了基于时间的特征，您的模型可以确定时间如何影响其他特征和您的目标值。通常，一个时间特征和基于时间的时间滞后特征将解决这种情况。本质上，您在这里可以有两种情况：
- en: Each row has a date or a time interval. You can use it as a feature after appropriate
    transformation—for instance, converting dates to Unix time, a continuous numeric
    feature. In such a case, you are running a time series analysis and need to use
    proper cross-validation strategies for time series.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一行都有一个日期或时间间隔。您可以使用它作为特征，经过适当的转换后——例如，将日期转换为Unix时间，一个连续的数值特征。在这种情况下，您正在进行时间序列分析，需要使用适当的时间序列交叉验证策略。
- en: You pivot multiple rows related to different times, creating time-based features.
    For instance, you can have the value of a feature at different moments in time
    and create a separate feature for each of such moments (such as sales_month_1,
    sales_month_2, and so on). In this case, you just fall back to having IID data
    and can proceed to analyze without much more formality.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将涉及不同时间的多行数据旋转，创建基于时间的特征。例如，你可以有特征在不同时间点的值，并为这样的每个时刻创建一个单独的特征（例如，sales_month_1、sales_month_2等）。在这种情况下，你只是回到了具有IID数据的情况，可以继续分析而无需更多的正式性。
- en: Cross-sectional data, typically in tabular form, may not be naturally IID even
    when drawn from the same period. This is because interactions among the analysis
    units and their membership in specific groups can create subtle dependencies between
    observations, often called “leakage” of information. This leakage can cause unexpected
    transmission of predictive information from features to the target during training,
    and it probably won’t work the same way during prediction when the represented
    situations are different. To address this problem, it is necessary to create features
    that explicitly capture the different memberships between rows and their relationships.
    For instance, if you are working with data from different companies, failing to
    provide information about their country of origin or operations, as well as their
    sector, may create a leakage for your machine learning algorithm that could learn
    and exploit implicit orderings in the data that won’t be replicable at prediction
    time. In addition, even if you can label the examples in your training data with
    their correct group, if you cannot replicate the same in your test data, it is
    important to segregate the groups in cross-validation so that each group appears
    only in the training or validation folds. Failure to do so can cause inflated
    prediction cross-validation estimates because the machine learning algorithm can
    exploit autocorrelation within groups.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 横截面数据，通常以表格形式呈现，即使是从同一时期抽取，也可能不是自然独立的同分布（IID）。这是因为分析单元之间的相互作用以及它们在特定群体中的成员资格可能会在观测值之间产生微妙的依赖关系，通常称为“信息泄露”。这种泄露可能导致在训练过程中，从特征到目标的不预期的预测信息传输，并且在预测时，当表示的情况不同时，可能不会以相同的方式工作。为了解决这个问题，有必要创建显式捕捉行之间不同成员资格及其关系的特征。例如，如果你正在处理来自不同公司的数据，未能提供关于它们的起源国或运营以及它们所属行业的信息，可能会为你的机器学习算法造成泄露，该算法可能会学习并利用数据中隐含的顺序，而这种顺序在预测时是不可复制的。此外，即使你可以在训练数据中为示例贴上正确的标签，如果你不能在测试数据中复制同样的情况，那么在交叉验证中隔离群体就很重要，以确保每个群体只出现在训练或验证折中。未能这样做可能会导致预测交叉验证估计值膨胀，因为机器学习算法可以利用组内的自相关性。
- en: When cross-sectional data is used to compare different time periods, the data
    may not be independent even if the interactions among units and groups are absent.
    In this case, the order of observations matters, and we need to consider the time
    series dependencies between the observations. This means that the data is non-IID,
    and the usual assumptions of independence may not hold. To deal with this situation,
    we can use time series models to handle the temporal dependencies between observations.
    Time series models consider that observations taken at different points in time
    are likely to be correlated, and they can use this correlation to make better
    predictions. Using appropriate time series models and techniques, we can obtain
    accurate predictions even when dealing with non-IID data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用横截面数据比较不同的时间段时，即使单元和群体之间的相互作用不存在，数据也可能不是独立的。在这种情况下，观测值的顺序很重要，我们需要考虑观测值之间的时序依赖关系。这意味着数据是非IID的，通常的独立性假设可能不成立。为了处理这种情况，我们可以使用时序模型来处理观测值之间的时序依赖关系。时序模型认为在不同时间点进行的观测值很可能是相关的，并且它们可以利用这种相关性来做出更好的预测。使用适当的时间序列模型和技术，我们可以在处理非IID数据时获得准确的预测。
- en: 'As a final general suggestion about how to prepare your data from the point
    of view of rows, consider the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为关于如何从行角度准备你的数据的最后一条一般性建议，考虑以下内容：
- en: Verify how time affects your data. Consider using time features and modeling
    time in your analysis, using lags and moving averages, as customary in time series
    analysis, to control changes due to time alone.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证时间如何影响你的数据。考虑在分析中使用时间特征并对时间进行建模，使用滞后和移动平均，如时间序列分析中通常所做的那样，以控制仅由时间引起的变化。
- en: Be aware of what you represent in rows because groups and relationships among
    them can affect your model results in training, validation, and testing.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意你在行中代表的内容，因为组和它们之间的关系可能会影响你的模型在训练、验证和测试中的结果。
- en: If hidden groups exist in your data, explicitly represent any grouping variable
    in a feature.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的数据中存在隐藏的组，请在特征中明确表示任何分组变量。
- en: In cross-validation, if your data contains groups, just prefer group cross-validation
    so they are never split between training and validation folds ([https://mng.bz/YDgA](https://mng.bz/YDgA)).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在交叉验证中，如果你的数据包含组，则优先选择组交叉验证，这样它们就不会在训练和验证折之间分割（[https://mng.bz/YDgA](https://mng.bz/YDgA)）。
- en: If time is a determinant in your model, use time-based validation ([https://mng.bz/GeGO](https://mng.bz/GeGO)).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果时间是你的模型中的一个决定因素，请使用基于时间的验证（[https://mng.bz/GeGO](https://mng.bz/GeGO)）。
- en: In the next section, we discuss the ideal criteria that should characterize
    tabular columns and what each data type implies regarding data processing.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论应该表征表格列的理想标准以及每种数据类型在数据处理方面的含义。
- en: 2.1.2 The ideal criteria for tabular columns
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 表格列的理想标准
- en: If the sampling of the cases, arranged in rows, presents some challenges, remember
    it is in the columns of your data table, which are also called the features, that
    most information resides and where most of your attention should focus next. A
    column is characterized by carrying homogeneous information related to its represented
    rows. For instance, if you are building a real estate evaluation dataset and your
    rows represent houses, columns may be related to the house’s surface area (i.e.,
    how many square meters and market evaluation expressed by a recent sale price).
    By being homogeneous, you can expect that each of these columns will carry only
    the information it is designated to. For instance, you shouldn’t find sale prices
    or other information in the surface column. In addition, you should also expect
    that the column values are uniquely related to the unit that is represented by
    each row.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果按行排列的案例采样存在一些挑战，请记住它位于你的数据表列中，这些列也称为特征，其中大部分信息都驻留于此，并且你接下来应该集中注意力的地方。一列的特点是携带与其所代表的行相关的同质信息。例如，如果你正在构建一个房地产评估数据集，并且你的行代表房屋，则列可能与房屋的面积相关（即，由最近的销售价格表示的平方米和市场评估）。由于同质性，你可以预期这些列将只携带它们指定的信息。例如，你不应该在面积列中找到销售价格或其他信息。此外，你还应预期列值与每一行所代表的单位唯一相关。
- en: Before delving into data structures suitable for handling a dataset of rows
    and columns, however, you need to first recognize the five types of data that
    can populate columns and the best way to handle each of them (see table 2.1).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在深入研究适合处理行和列的数组的结构之前，你需要首先识别可以填充列的五种数据类型以及处理每种类型的最佳方式（见表2.1）。
- en: Table 2.1 Types of data in a tabular dataset
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 表格数据集中的数据类型
- en: '| Type | Description |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 描述 |'
- en: '| --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Numeric features | Integers for count data, such as when enumerating sales
    by day for a product: [105, 122, 91, … 124]Floats for measurements, such as when
    enumerating sales income by day for a product: [1000.50, 1230.00, 950.80, …, 1200.00
    ] |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 数值特征 | 用于计数数据的整数，例如按日列举产品的销售额：[105, 122, 91, … 124]用于测量的浮点数，例如按日列举产品的销售额：[1000.50,
    1230.00, 950.80, …, 1200.00 ] |'
- en: '| Ordinal features | Integers (or floats without decimals) for rankings or
    ordered levels: [0, 1, 2, 3, …, 999]Sometimes ordinal features can be verbally
    expressed by strings underling some ordered effect such as with Likert-like agreement
    or preference scales: [“strongly disagree,” “disagree,” “neither agree nor disagree,”
    “agree,” “strongly agree”] |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 序数特征 | 用于排名或有序级别的整数（或无小数的浮点数）：[0, 1, 2, 3, …, 999]有时序数特征可以用字符串表达，并带有某种有序效果，例如使用类似Likert的同意或偏好量表：[“强烈不同意”，“不同意”，“既不同意也不反对”，“同意”，“强烈同意”]
    |'
- en: '| Low categorical features | Since they express a quality, they can be a string
    (called “labels”) or integers associated with the original labels; in that case
    you have a conversion dictionary available: {0: “red,” 1:”green,” 2: “blue”} |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 低级分类特征 | 由于它们表达了一种质量，它们可以是字符串（称为“标签”）或与原始标签关联的整数；在这种情况下，你有一个转换字典可用：{0: “红色”，1:”绿色”，2:
    “蓝色”} |'
- en: '| High categorical features | The same as low categorical features but with
    a large number of label (e.g., US area codes or zip codes: [https://postal-codes.net/united-states](https://postal-codes.net/united-states))
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 高级分类特征 | 与低级分类特征相同，但具有大量标签（例如，美国区号或邮政编码：[https://postal-codes.net/united-states](https://postal-codes.net/united-states)）|'
- en: '| Dates | They can be a string or already encoded in data formats such as “2022-03-04”,
    “Feb 15, 1957 11:45 PM” or “6/12/2022” (there are many date standards and conventions)
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 它们可以是字符串，也可以是已经编码在数据格式中的，例如“2022-03-04”、“Feb 15, 1957 11:45 PM”或“6/12/2022”（存在许多日期标准和惯例）|'
- en: Knowing each of them is the basic step in building a dataset and properly processing
    data to be fed into a machine learning algorithm. Each algorithm will require
    each type of data to be specially prepared for its best understanding and consequent
    predictive performance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 了解每一种都是构建数据集和正确处理数据以便输入机器学习算法的基本步骤。每种算法都需要对每种类型的数据进行特殊准备，以便其最佳理解和后续的预测性能。
- en: You have *numeric features* when your data is expressed by a floating number
    or an integer representing a counting of some value. Accordingly to statistics,
    features represented as floating numbers are often distinguished into ratio or
    interval scales. The distinction is that ratio scales, representing a measurement
    of something real, like unit sales or money, have an absolute real zero and are
    only positive. Interval scales are instead abstract or arbitrary measurements;
    their units can represent anything. In interval scales, the zero value is arbitrary,
    and the values can be negative. For instance, interval scales are the measure
    of temperature in Fahrenheit or Celsius degrees. You can turn a ratio scale into
    an interval one by simple transformations like subtracting the mean, which is
    a centering operation, and dividing by the standard deviation, which is a standardization
    operation. In such a fashion, you make the zero value, as well as the scale, arbitrary.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据以浮点数或表示某些值计数的整数表示时，你就有*数值特征*。根据统计学，表示为浮点数的特征通常被区分成比例尺度或区间尺度。区别在于比例尺度，代表对某个真实事物的测量，如单位销售或金钱，有一个绝对的真实零点，并且只有正值。区间尺度则是抽象或任意的测量；它们的单位可以代表任何事物。在区间尺度中，零值是任意的，值可以是负数。例如，区间尺度是华氏度或摄氏度温度的测量。你可以通过简单的转换，如减去平均值（这是一个中心化操作）和除以标准差（这是一个标准化操作），将比例尺度转换为区间尺度。以这种方式，你使零值以及尺度变得任意。
- en: 'Integer numbers, when the feature is a numeric one, are only ratio scales.
    However, integers can also be used for ordinal and even categorical features.
    If you need more clarification about the meaning of a numeric feature expressed
    in integers, start checking if the numbers are not continuous and if there are
    many distinct values. When such conditions are satisfied, you can be almost confident
    that you are dealing with a numeric feature. Numeric features don’t need much
    processing, and for machine learning purposes, you won’t care much if they are
    ratio or interval scale. All you must consider is that they represent a single
    kind of measurement. For instance, if you are measuring monetary values in your
    dataset, you cannot create a column representing dollars, euros, and pounds together:
    for your numeric feature to be usable, you need one measurement type for each
    column. Also, you need to ascertain that you don’t have too many missing numbers
    in your numeric features and that their values vary enough to be useful. You should
    avoid low variance or constant features in your data.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 整数数字，当特征是数值时，仅是比例尺度。然而，整数也可以用于顺序特征甚至分类特征。如果你需要更多关于用整数表示的数值特征含义的澄清，开始检查这些数字是否不连续，以及是否存在许多不同的值。当这些条件得到满足时，你几乎可以确信你正在处理一个数值特征。数值特征不需要太多处理，对于机器学习目的而言，你不会太在意它们是比例尺度还是区间尺度。你必须考虑的是，它们代表了一种单一的测量类型。例如，如果你在你的数据集中测量货币价值，你不能创建一个列来表示美元、欧元和英镑：为了你的数值特征可使用，你需要为每个列提供一个测量类型。此外，你需要确认你的数值特征中缺失的数字不多，并且它们的值变化足够大，以便有用。你应该避免数据中的低方差或恒定特征。
- en: '*Ordinal features* are always made up of integers, usually representing rankings
    and ratings or scores. For instance, as an example of an ordinal feature, just
    think of the ratings in terms of stars for products sold on e-commerce platforms.
    Being numbers, ordinal features are pretty similar to numeric values, yet they
    require different treatment, and you cannot work with them in exactly the same
    way. First, if numeric values are a scale where each value is equidistant to the
    next and previous ones, there is no guarantee that the distance between the value
    points is always constant in an ordinal scale. For instance, as in a long-distance
    running competition, first and second place may differ by a few seconds. However,
    a second place and a third place may differ by many minutes.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*序数特征*总是由整数组成，通常表示排名、评分或分数。例如，作为一个序数特征的例子，只需考虑电子商务平台上产品的星级评分。作为数字，序数特征与数值非常相似，但它们需要不同的处理方式，你不能以完全相同的方式处理它们。首先，如果数值是一个每个值与下一个和前一个值等距离的刻度，那么在序数刻度中，值点之间的距离并不总是恒定的。例如，在长跑比赛中，第一名和第二名可能只差几秒。然而，第二名和第三名可能相差几分钟。'
- en: In the same way, in an ordinal scale representing some underlying numeric measure,
    some adjacent points can be numerically very near to each other, and others may
    be numerically very distant. An ordinal scale just tells you about an order, which
    means that a value comes before the next one and after the previous one. Hence,
    without making a significant approximation, you cannot process it as if it were
    purely numeric, for instance, by computing mean and standard deviation. From a
    certain point of view, you can consider ordinals as categorical features with
    meaningful ordering.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，在一个表示某些潜在数值测量的序数刻度中，一些相邻点在数值上可能非常接近，而其他点可能非常遥远。序数刻度只是告诉你关于一个顺序，这意味着一个值在下一个值之前，在之前一个值之后。因此，不进行重大近似，你不能像处理纯数值一样处理它，例如，通过计算平均值和标准差。从某个角度来看，你可以将序数视为具有有意义排序的分类特征。
- en: '*Categoricals* are features constituted by labels: each unique value in a categorical
    feature represents a quality referred to in the example. We can find these labels
    expressed as strings, in this case, we are unequivocally working with a categorical
    feature, or as integer numbers, and in this case, we shouldn’t mistake it for
    an ordinal. Even the missing value can be treated as a label among the others.
    Hence, missing values are more easily dealt with in categorical features. We distinguish
    between *low cardinality* and *high cardinality* categorical features based on
    the number of unique values found in the column. The distinction is helpful because
    high cardinality categorical features are challenging to handle for deep learning
    and machine learning algorithms and require a more complex treatment than low
    cardinality ones. No clear threshold exists for classifying a categorical as low
    or high cardinality. However, having more than a dozen unique values usually poses
    a challenging time for the analyst. Throughout the book, we will discuss specialized
    strategies to handle categorical features called encodings. For the moment, just
    keep in mind a special type of categorical feature—one with a single label and
    values representing the presence or absence of the label, usually 0 for the absence
    and 1 for the presence of the quality. Such features are called binary features
    or, using a more statistical term, dichotomous variables.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*分类特征*是由标签构成的特性：分类特征中的每个唯一值代表示例中提到的某种质量。我们可以将这些标签表示为字符串，在这种情况下，我们明确地处理的是分类特征，或者作为整数，在这种情况下，我们不应将其误认为是序数。即使是缺失值也可以被视为其他标签之一。因此，在分类特征中处理缺失值更为容易。我们根据列中找到的唯一值的数量区分*低基数*和*高基数*分类特征。这种区分是有帮助的，因为高基数分类特征对于深度学习和机器学习算法来说处理起来更具挑战性，并且需要比低基数特征更复杂的处理。没有明确的阈值来分类分类特征为低基数或高基数。然而，拥有超过十个唯一值通常会给分析师带来挑战。在整本书中，我们将讨论处理分类特征的专门策略，称为编码。目前，只需记住一种特殊的分类特征——只有一个标签且值表示标签的存在或不存在，通常用0表示不存在，用1表示存在该质量。这些特征被称为二元特征，或者使用更统计学的术语，称为二项变量。'
- en: '*Dates* are very commonly found in many business data and databases since time
    is essential information to be monitored for a business to work its processes
    and resources properly. Dates also vary a lot since several date formats depend
    on the country, type of business application, or adoption of particular standards.
    For this reason, some efforts have been made to standardize the dates—for instance,
    proposing the ISO 8601 standard ([https://mng.bz/zZQQ](https://mng.bz/zZQQ)),
    but we are still far from having a common standard. It is essential to know that
    dates may be reported as columns as they are already processed for better clarity
    into separated columns containing their cyclical components, which are days, months,
    hours, days of the week, and noncyclical ones—for instance, years. They can also
    be transformed into a numeric continuous value representing the flow of time;
    Unix time is the best example, representing the number of seconds that have elapsed
    since 00:00:00 UTC on January 1, 1970, excluding leap seconds. Finally, dates
    require some understanding of used conventions for treating missing information
    because, in certain applications, it is customary to avoid void or null values
    and instead prefer applying a date far in the past or future to represent that
    the time value is missing or unknown.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*日期*在许多商业数据和数据库中非常常见，因为时间是确保业务正常运作和资源有效利用的关键信息。日期也变化很大，因为多种日期格式取决于国家、业务应用类型或特定标准的采用。因此，已经做出了一些努力来标准化日期——例如，提出ISO
    8601标准([https://mng.bz/zZQQ](https://mng.bz/zZQQ))，但我们离一个共同的标准还有很长的路要走。重要的是要知道，日期可能作为列报告，因为它们已经被处理，以便更清晰地包含它们的周期性成分，如日、月、小时、星期几，以及非周期性成分——例如，年份。它们也可以转换为表示时间流的数值连续值；Unix时间是最佳例子，表示自1970年1月1日00:00:00
    UTC以来经过的秒数，不包括闰秒。最后，日期需要了解用于处理缺失信息的约定，因为在某些应用中，避免空或空值是惯例，而更倾向于应用一个遥远的过去或未来的日期来表示时间值缺失或未知。'
- en: 2.1.3 Representing rows and columns
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 表示行和列
- en: When organizing a tabular dataset, it is important to ensure that each row is
    identifiable and uses appropriate data types for numeric, ordinal, categorical,
    and date values, which may include float, integer, string, datetime, and sometimes
    even boolean for binary features. To handle this mix of data types efficiently,
    the pandas DataFrame is the best data structure available in Python. It is an
    ordered collection of columns that provides a flexible and efficient way to manage
    and manipulate tabular data. You can learn more about the pandas DataFrame at
    [https://mng.bz/0Qw6](https://mng.bz/0Qw6).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在组织表格数据集时，确保每一行可识别并使用适当的数据类型对于数值、序数、分类和日期值非常重要，这些数据类型可能包括浮点数、整数、字符串、日期时间，有时甚至包括布尔值用于二进制特征。为了有效地处理这种数据类型的混合，pandas
    DataFrame是Python中最好的数据结构。它是一个有序的列集合，提供了一种灵活且高效的方式来管理和操作表格数据。您可以在[https://mng.bz/0Qw6](https://mng.bz/0Qw6)了解更多关于pandas
    DataFrame的信息。
- en: In listing 2.1, we create a small tabular dataset from scratch with four rows,
    representing four individuals, and four columns, representing numeric and categorical
    features that characterize them, using a data dictionary. We also define a list
    containing their names to be used as a reference for accessing their information.
    We then use pandas to convert the dictionary to a DataFrame, a two-dimensional
    table-like data structure, and assign the labels.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表2.1中，我们从头开始创建一个包含四行的小型表格数据集，代表四个个体，以及四列，代表描述他们的数值和分类特征，使用数据字典。我们还定义了一个包含他们名字的列表，用作访问他们信息的参考。然后我们使用pandas将字典转换为DataFrame，这是一个类似二维表的复杂数据结构，并分配标签。
- en: Listing 2.1 Creating a simple tabular dataset
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1 创建一个简单的表格数据集
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Creates a dictionary of data
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个数据字典
- en: ② Creates a row index
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个行索引
- en: ③ Creates a pandas DataFrame from the dictionary
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从字典创建一个pandas DataFrame
- en: ④ Prints row 1
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 打印第1行
- en: ⑤ Prints the row whose label is Alice
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 打印标签为Alice的行
- en: 'The output for the `print(df)` command should look like the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`print(df)`命令的输出应如下所示：'
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Notice that the rows are labeled with the example names and the columns are
    labeled with the feature names. You can access both rows and columns by their
    labels or index numbers, starting from zero. Therefore, we can access Alice’s
    information by both means of its row index, which is 1, or by its name label:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，行用示例名称标记，列用特征名称标记。你可以通过它们的标签或索引号（从零开始）访问行和列。因此，我们可以通过其行索引（1）或其名称标签来访问爱丽丝的信息：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is made possible by pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/)),
    which is a Python package dedicated to data processing that allows you to quickly
    and smoothly load data from multiple sources; slice it based on columns or rows
    or on both at the same time (an operation known as *dicing*); handle missing values;
    add, rename, compute, group, and aggregate features; and pivot, reshape, and finally
    visualize your processed data. Apart from its super helpful data processing functionalities,
    it is also particularly renowned because of its data structures, Series, and DataFrames,
    the most widely used data formats for tabular data operating with Python.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由 pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/)) 实现的，这是一个专门用于数据处理的
    Python 包，它允许你快速而顺畅地从多个来源加载数据；基于列或行或同时基于两者进行切片（这种操作称为 *dicing*）；处理缺失值；添加、重命名、计算、分组和聚合特征；以及旋转、重塑并最终可视化处理后的数据。除了其超级有用的数据处理功能外，它还因其数据结构（Series
    和 DataFrame）而特别闻名，这些是 Python 中处理表格数据最广泛使用的格式。
- en: In a pandas DataFrame, apart from the table of data, you also have an index
    for columns so that you can name them and for rows, helping you in the operations
    of identification and filtering. In addition, you can efficiently perform selections
    and various operations, such as combining columns or replacing missing values.
    Recently, even the popular machine learning package Scikit-learn, which has long
    accepted pandas DataFrames as input for its algorithms, has taken steps to maintain
    such data structure along all its pipelines. Now all outputs, instead of being
    transformed data into Numpy arrays, which are matrices homogeneous by type, can
    be maintained as pandas DataFrames. For more details about how this works and
    how it can affect how you use the package, see [https://mng.bz/nR54](https://mng.bz/nR54).
    A de facto standard for tabular data, pandas DataFrames will be extensively used
    throughout the book, where we will show how to apply more useful transformations
    for dealing with common tabular data characteristics and problems.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pandas DataFrame 中，除了数据表之外，你还有一个用于列的索引，这样你可以给它们命名，以及用于行的索引，这有助于你在识别和过滤操作中。此外，你可以高效地进行选择和多种操作，例如合并列或替换缺失值。最近，甚至流行的机器学习包
    Scikit-learn，它长期以来一直接受 pandas DataFrame 作为其算法的输入，也采取了步骤来在整个管道中维护这种数据结构。现在，所有输出，而不是将转换后的数据转换为类型同质的
    Numpy 数组，都可以保持为 pandas DataFrame。有关如何实现以及它如何影响你使用该包的方式的更多详细信息，请参阅[https://mng.bz/nR54](https://mng.bz/nR54)。pandas
    DataFrame 作为表格数据的事实标准，将在整本书中得到广泛使用，我们将展示如何应用更有用的转换来处理常见的表格数据特性和问题。
- en: After discussing the ideal characteristics of rows and columns in a tabular
    dataset, the next section will explore what can go wrong and the implications
    and remedies.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论表格数据集中行和列的理想特性之后，下一节将探讨可能出错的情况以及其影响和补救措施。
- en: 2.2 Pathologies and remedies
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 病态与补救措施
- en: As a general rule, you must always strive to avoid certain conditions among
    your features, no matter what their kind, which we previously briefly mentioned
    when discussing each type of column you may have in a dataset.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一条一般规则，你必须始终努力避免你特征中的一些条件，无论它们的类型如何，我们在讨论数据集中可能具有的每种类型的列时简要提到了这些条件。
- en: Previous data from data science competitions can help us see what could go wrong
    with a tabular dataset. Let’s take, for instance, the Madelon dataset ([https://archive.ics.uci.edu/ml/datasets/Madelon](https://archive.ics.uci.edu/ml/datasets/Madelon)),
    which is remembered after so many years as a very challenging data problem because
    of its specific characteristics that made prediction hard. The Madelon dataset
    is an artificial dataset generated using an ad hoc algorithm developed by Isabel
    Guyon ([https://guyon.chalearn.org/](https://guyon.chalearn.org/)), who joined
    Google Brain in 2022 as a director. The data has been presented at a contest at
    the NIPS 2003 conference, the seventh Annual Conference on Neural Information
    Processing Systems. She placed errors in data in the form of random noise and
    the target by having a part of the labels flipped. She added redundant and highly
    collinear features, clustered observations along the vertices of a five-dimension
    hypercube without providing information, and finally inserted irrelevant information.
    That made many data scientists strive to wrap their heads around the problem at
    the time. See [https://mng.bz/ga4V](https://mng.bz/ga4V) for more details about
    the generative process of the synthetic dataset if you are interested.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学竞赛的先前数据可以帮助我们了解表格数据集可能出什么问题。例如，Madelon数据集([https://archive.ics.uci.edu/ml/datasets/Madelon](https://archive.ics.uci.edu/ml/datasets/Madelon))，由于其特定的特性使得预测变得困难，因此多年后仍然被记住作为一个非常具有挑战性的数据问题。Madelon数据集是一个使用由Isabel
    Guyon([https://guyon.chalearn.org/](https://guyon.chalearn.org/))开发的专门算法生成的合成数据集，她于2022年加入谷歌大脑担任总监。这些数据在2003年NIPS会议（第七届神经信息处理系统年会）的比赛中展出。她通过翻转部分标签将错误数据以随机噪声和目标的形式呈现，添加了冗余和高度相关的特征，在五维超立方的顶点处聚类观察结果而不提供信息，并最终插入无关信息。这使得许多数据科学家当时都努力去理解这个问题。如果您对合成数据集的生成过程感兴趣，请参阅[https://mng.bz/ga4V](https://mng.bz/ga4V)以获取更多详细信息。
- en: More recently, Kaggle competitions such as Don’t Overfit ([https://www.kaggle.com/competitions/overfitting](https://www.kaggle.com/competitions/overfitting)),
    Don’t Overfit II ([https://www.kaggle.com/competitions/dont-overfit-ii](https://www.kaggle.com/competitions/dont-overfit-ii)),
    Categorical Feature Encoding Challenge I and II ([https://www.kaggle.com/competitions/cat-in-the-dat](https://www.kaggle.com/competitions/cat-in-the-dat)
    and [https://mng.bz/jpyP](https://mng.bz/jpyP)), extending the problem to categorical
    features and missing data, proved that when there are too many problems in data,
    even the most powerful machine learning algorithms can do very little.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Kaggle竞赛如Don’t Overfit([https://www.kaggle.com/competitions/overfitting](https://www.kaggle.com/competitions/overfitting))、Don’t
    Overfit II([https://www.kaggle.com/competitions/dont-overfit-ii](https://www.kaggle.com/competitions/dont-overfit-ii))、Categorical
    Feature Encoding Challenge I and II([https://www.kaggle.com/competitions/cat-in-the-dat](https://www.kaggle.com/competitions/cat-in-the-dat)
    和 [https://mng.bz/jpyP](https://mng.bz/jpyP))，将问题扩展到类别特征和缺失数据，证明了当数据中存在太多问题时，即使是最强大的机器学习算法也几乎无能为力。
- en: Starting from such practical examples, generally speaking, the conditions you
    have to care about the most in tabular data are
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从这样的实际例子开始，一般来说，在表格数据中你必须最关心的条件是
- en: Avoiding constant or quasi-constant columns
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免常数或准常数列
- en: Avoiding duplicated or highly collinear columns
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免重复或高度相关的列
- en: Avoiding irrelevant features and prioritize features that demonstrate high predictive
    power
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免无关特征，并优先考虑具有高预测力的特征
- en: Dealing with rare categories or with far too many labels with categoricals
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理稀有类别或具有过多标签的类别
- en: Spot incongruencies and misplaced, flipped, or distorted values
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找不一致性、错位、翻转或扭曲的值
- en: Avoiding too many missing cases in a column and handling existing ones
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免列中有太多缺失值，并处理现有的缺失值
- en: Excluding leakage features
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除泄漏特征
- en: Let’s go through each of these from a theoretical and practical point of view
    using some artificial data. More real-world examples and commands for the detection
    and resolution of such conditions will be illustrated in our example in the paragraph
    concluding the chapter. It is pretty challenging to present examples for each
    of them or find a single real-world dataset distributed to the public containing
    all such bad data examples (actually, such examples are abundant in private repositories).
    Public datasets are often curated enough to have most of such data traps already
    expunged.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个理论和实践的角度，使用一些人工数据来逐一分析这些内容。在章节结尾的段落中，我们将通过示例展示如何检测和解决此类条件的一些真实世界示例和命令。对于每一个都提供示例或找到一个包含所有这些不良数据示例的单一真实世界数据集（实际上，这样的例子在私人存储库中很丰富）是非常具有挑战性的。公共数据集通常已经足够精心制作，大多数这样的数据陷阱已经清除。
- en: 2.2.1 Constant or quasi-constant columns
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 常数或准常数列
- en: Avoid constant or quasi-constant columns. As a rule of thumb, the variance shouldn’t
    approximate zero for numeric features, and the majority class shouldn’t be over
    99.9% for categorical features. All this is paramount because machine learning
    algorithms can learn only from how empirical conditional expectations of your
    target vary with respect to your features. No change in the features implies no
    conditional change on your target from which to learn. Constant features result
    in more cumber to be handled by numerical processes behind learning algorithms,
    and quasi-constant features may even result in some overfitting because the minimal
    nonconstant part may be deterministically associated with some target output.
    The solution is to drop constant or quasi-constant columns.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 避免常数或准常数列。作为一个经验法则，对于数值特征，方差不应该接近零，对于分类特征，多数类不应该超过99.9%。所有这些都是至关重要的，因为机器学习算法只能从您的目标相对于特征的实证条件期望的变化中学习。特征的任何变化都意味着没有条件变化，从而无法从中学习。常数特征会导致学习算法背后的数值过程处理起来更加繁琐，而准常数特征甚至可能导致一些过拟合，因为最小非常数部分可能与某些目标输出确定性相关联。解决方案是删除常数或准常数列。
- en: Listing 2.2 Dropping zero variance features
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2 删除零方差特征
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① An ordinal encoder will transform your data from string labels to ordered
    numeric ones.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ① 有序编码器会将您的数据从字符串标签转换为有序数值。
- en: ② The VarianceThreshold class will filter all features whose variance is equal
    to or below the selected threshold.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ② VarianceThreshold 类将过滤掉所有方差等于或低于所选阈值的特征。
- en: ③ You can have the variances of all features represented by the .variances_
    attribute,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 您可以通过 .variances_ 属性来表示所有特征的方差，
- en: 2.2.2 Duplicated and highly collinear features
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 重复和高度相关的特征
- en: Avoiding duplicated or highly collinear columns is helpful because if information
    redundancy makes your learning more robust, there are some caveats. Most importantly,
    it renders learning more complicated and computationally expensive. First, duplicated
    features are useless as constant ones and should be discarded immediately because
    they just waste memory space and computation time. The discourse is different
    for highly collinear ones. No matter whether we are talking about numeric features,
    where you measure collinearity by correlations, or categorical ones, where you
    measure collinearity by association measures based on chi-square statistics when
    a set of features are very strongly associated, it is because
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 避免重复或高度相关的列是有帮助的，因为如果信息冗余使您的学习更加鲁棒，也有一些注意事项。最重要的是，它使得学习更加复杂和计算成本更高。首先，重复的特征作为常数是无用的，应该立即丢弃，因为它们只是浪费内存空间和计算时间。对于高度相关的特征，情况不同。无论我们是在谈论数值特征，其中您通过相关性来衡量共线性，还是分类特征，其中您通过基于卡方统计的关联度量来衡量共线性，当一组特征非常强地相关时，这是因为
- en: One is the causative feature of the other. For instance, in a dataset related
    to academic results, time spent studying and test scores are highly correlated
    because time spent is one of the determinants of good test scores, as many studies
    have determined.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个是另一个的因果特征。例如，在一个与学术成绩相关的数据集中，学习时间和考试成绩高度相关，因为学习时间是良好考试成绩的一个决定因素，正如许多研究已经确定的。
- en: They all reflect a latent feature that causes or influences them. For instance,
    in a dataset about cars, performance and emissions are partially determined by
    the type of fuel used. Even if the fuel characteristics are not recorded in the
    data and their relationship is not apparent, the performance and emissions features
    will be strongly associated with each other.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们都反映了导致或影响它们的潜在特征。例如，在一个关于汽车的数据库中，性能和排放部分由使用的燃料类型决定。即使燃料的特性没有记录在数据中，它们之间的关系也不明显，性能和排放特征将强烈相关。
- en: In the first situation, the solution is simple because you just have to keep
    the causative feature and drop all the others. Nothing in the data itself will
    tell you which is the causative feature, and you basically can figure out which
    is the one only by domain knowledge, theoretical deductions, or causal analysis,
    which requires additional modeling and experimentation. The latter case is a bit
    more sophisticated because, even if you can figure out the causative feature,
    you don’t have it among your data and must decide which feature to keep. Again,
    domain knowledge can help you make decisions. Data analysis can also contribute
    with hints based on the association with the target and the data quality of the
    features at hand, using a mix of fewer errors, fewer missing data, and fewer outliers.
    Keeping only the feature most associated with the target or the one with the highest
    quality is the best choice.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，解决方案很简单，因为你只需要保留因果特征并丢弃所有其他特征。数据本身不会告诉你哪个是因果特征，你基本上只能通过领域知识、理论推导或因果分析来找出它是哪一个，这需要额外的建模和实验。后一种情况稍微复杂一些，因为即使你能找出因果特征，它可能不在你的数据中，你必须决定保留哪个特征。再次强调，领域知识可以帮助你做出决定。数据分析也可以通过基于目标与特征之间的关联以及手头特征的
    数据质量来提供提示，使用更少的错误、更少的缺失数据和更少的异常值。只保留与目标最相关的特征或质量最高的特征是最好的选择。
- en: Listing 2.3 Finding multicollinear numeric features
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.3 寻找多重共线性数值特征
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Sets the seed for reproducibility
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ① 设置随机种子以确保可重复性
- en: '② Creates a synthetic dataset: the make_classification command will create
    a sample dataset of twenty slightly correlated features (see [https://mng.bz/eynQ](https://mng.bz/eynQ))'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个合成数据集：make_classification 命令将创建一个包含二十个略微相关的特征的样本数据集（见[https://mng.bz/eynQ](https://mng.bz/eynQ)）
- en: '③ Adds more correlated features to the dataset: we pick the first five features,
    and we duplicate them at the end of the dataset after having added some noise,
    so they are not the same as the original ones.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 向数据集中添加更多相关特征：我们选择前五个特征，并在添加了一些噪声后，将它们复制到数据集的末尾，这样它们就与原始特征不同了。
- en: ④ Computes the variance inflation factor to spot the features that have the
    least unique contribution
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算方差膨胀因子以识别贡献最少的特征
- en: ⑤ Checking for correlated features by iterating through a correlation matrix,
    we compute only the correlation coefficients of the lower triangle of the matrix.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 通过迭代相关矩阵来检查相关特征，我们只计算矩阵下三角的相关系数。
- en: ⑥ The correlation coefficient is computed using the NumPy function np.corcoef
    ([https://mng.bz/8Olw](https://mng.bz/8Olw)).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用 NumPy 函数 np.corcoef 计算相关系数（见[https://mng.bz/8Olw](https://mng.bz/8Olw)）。
- en: ⑦ The threshold to report collinearity is evaluated in absolute value (correlation
    will be negative if a feature is reversed). In addition, depending on the stage
    of the analysis, you can set this to 0.90, 0.95, or even 0.99.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 报告共线性的阈值按绝对值评估（如果特征被反转，相关系数将是负值）。此外，根据分析阶段的不同，你可以将其设置为 0.90、0.95 或甚至 0.99。
- en: 'In the previous example, we deal with highly collinear numeric features in
    a synthetic dataset. We use two approaches: the variance inflation factor and
    the Pearson correlation coefficient. The variance inflation factor (VIF) is a
    typical analysis preparatory to linear models, a family of models to be discussed
    in chapter 4, which aims at quantifying how much of a feature informative continent
    is actually to be found in other features. A VIF is, therefore, more extensive
    in scope than a correlation coefficient, which is a bivariate analysis, an analysis
    between two variables at a time. Instead, the VIF tries to pounder the role of
    a feature with respect to all the others. VIFs are powerful in terms of highlighting
    potentially less contributing features. Still, they won’t reveal what feature
    is related to others or which must be removed. The higher a VIF’s value, which
    starts from 1 and goes to infinity, the more multicollinear the considered feature.
    In our example, since higher VIF values indicate a less unique contribution of
    the feature, we can quickly spot that the first and last five features of the
    dataset are more problematic:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们处理了一个合成数据集中高度相关的数值特征。我们采用了两种方法：方差膨胀因子和皮尔逊相关系数。方差膨胀因子（VIF）是线性模型准备分析的典型方法，线性模型是一系列将在第4章讨论的模型，其目的是量化一个特征的信息含量实际上在其它特征中可以找到的程度。因此，VIF的范围比相关系数更广泛，相关系数是一种双变量分析，即一次分析两个变量。相反，VIF试图分析一个特征相对于所有其他特征的作用。VIF在突出可能贡献较小的特征方面非常强大。然而，它们不会揭示哪些特征与其它特征相关或哪些必须删除。VIF的值越高，从1开始到无穷大，考虑的特征的多重共线性就越高。在我们的例子中，由于较高的VIF值表明特征贡献的独特性较低，我们可以迅速发现数据集的前五个和最后五个特征更有问题：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'However, you need the following one-by-one comparison by correlation analysis
    to determine which features you can narrow down the problem. By using a threshold
    set to 0.99, you will figure out only almost identical features. By setting the
    bar lower to 0.90 or 0.95, you will reveal features whose unique contribution
    is minimal:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你需要通过相关性分析逐一比较，以确定你可以缩小问题的特征。通过将阈值设置为0.99，你将只能发现几乎完全相同的功能。通过将标准降低到0.90或0.95，你将揭示那些独特贡献最小的特征：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: At this point, it is up to you to evaluate what feature to keep between two
    highly collinear ones. In any case, even if you have to decide what features to
    drop based on collinearity, selecting features based on quality is something you
    always consider to later treat only with useful features. Generally speaking,
    when facing multiple errors that cannot be corrected, such as recording or measurement
    errors, a high volume of missing data, and outlying measures, a feature should
    be kept only if it shows an interesting and unique association with the target.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你需要评估在两个高度相关的特征之间保留哪个特征。在任何情况下，即使你必须根据共线性来决定要删除哪些特征，基于质量选择特征始终是你考虑的，以便以后只处理有用的特征。一般来说，当面对无法纠正的多个错误，如记录或测量错误、大量缺失数据和异常值时，只有当特征显示出与目标有趣的独特关联时，才应保留该特征。
- en: 2.2.3 Irrelevant features
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 无关特征
- en: The same goes for irrelevant features, which make little sense for your problem
    and have little or no association with the target. At this stage, you just need
    to rule out improbable features that you know do not apport any information to
    your problem based on domain knowledge or simple statistical univariate tests,
    such as correlation or chi-square analysis, as we will demonstrate in the latter
    part of this chapter.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与问题无关的特征，它们对你的问题意义不大，并且与目标几乎没有或没有关联。在这个阶段，你只需要排除那些根据领域知识或简单的单变量统计测试（如相关分析或卡方分析）可知不会对你的问题提供任何信息的可能性较大的特征，正如我们将在本章的后续部分所展示的。
- en: Selecting features has been widely used to reduce computing requirements during
    training, help in having a more interpretable model, and improve predictive performance
    on test data since irrelevant features create false signals and disturbances to
    machine learning algorithms, both classic and neural networks. Chapter 6 of this
    book will discuss feature selection in-depth and present tools suitable for selecting
    a working subset of your initial features.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中选择特征已被广泛用于减少计算需求，有助于拥有更可解释的模型，并提高测试数据上的预测性能，因为无关特征会为机器学习算法（包括经典和神经网络）产生虚假信号和干扰。本书的第6章将深入讨论特征选择，并介绍适合选择初始特征子集的工具。
- en: We will focus especially on those tools based on testing the behavior of features
    being reshuffled or randomized (by random replacements of their values). The idea
    is to build a model for your problem and then shuffle each feature to check if
    the results change too much and the predictive performances decrease. Features,
    whose randomization does not affect the model performance, are likely to be irrelevant
    or carry redundant information available elsewhere among the features. Hence,
    they can be safely dropped, enhancing memory handling and training/prediction
    times.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将特别关注基于测试特征重新排列或随机化（通过随机替换其值）行为的工具。想法是为你的问题构建一个模型，然后对每个特征进行随机化以检查结果是否变化太大，预测性能是否下降。那些随机化不会影响模型性能的特征，很可能是无关的或携带其他特征中可用的冗余信息。因此，它们可以安全地删除，从而提高内存处理和训练/预测时间。
- en: 2.2.4 Missing data
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 缺失数据
- en: Missing data is a critical factor that must be considered, as it can affect
    any data source and pose nuisances for many learning algorithms. Deep learning
    models and numerous machine learning algorithms cannot handle missing data directly.
    However, certain specialized machine learning algorithms, such as XGBoost and
    LightGBM, can reasonably manage missing information without requiring any intervention.
    These algorithms assume the value that was previously deemed more useful in similar
    situations when confronted with a missing value. For more information, see [https://mng.bz/EaxO](https://mng.bz/EaxO).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据是一个必须考虑的关键因素，因为它可能影响任何数据源，并为许多学习算法带来麻烦。深度学习模型和许多机器学习算法不能直接处理缺失数据。然而，某些专门的机器学习算法，如XGBoost和LightGBM，可以在不要求任何干预的情况下合理地管理缺失信息。这些算法在遇到缺失值时，会假设在类似情况下被认为更有用的值。有关更多信息，请参阅[https://mng.bz/EaxO](https://mng.bz/EaxO)。
- en: Apart from these specialized algorithms, missing values are usually addressed
    through a process known as imputation. This involves using the information present
    in the same column (simple univariate imputation) or in all other available columns
    (multivariate imputation) to determine a reasonable replacement value or class.
    Imputing data by multivariate imputation is sometimes more effective, even for
    XGBoost and LightGBM algorithms. We will discuss multivariate imputation in more
    detail in chapter 6 of the book.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些专门的算法之外，缺失值通常通过一个称为插补的过程来解决。这涉及到使用同一列中存在的（简单单变量插补）或所有其他可用列中的信息来确定一个合理的替代值或类别。对于XGBoost和LightGBM算法，多变量插补有时甚至更有效。我们将在本书的第6章中更详细地讨论多变量插补。
- en: Please take notice now that if you are dealing with missing data at the early
    stages of data preparation, missingness may be a piece of information in itself.
    For instance, if you work with a relational database query and are left or right
    joining (or full outer joining) tables, you will produce missing cases when cases
    don’t match. In such a situation, a missing case will mean no match with the conditions
    expressed in a certain database table, which can be precious information.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果在数据准备的早期阶段处理缺失数据，缺失性本身可能是一条信息。例如，如果你使用关系数据库查询，并且进行左连接或右连接（或全外连接）表，当案例不匹配时，你会产生缺失案例。在这种情况下，缺失案例意味着与某个数据库表中表达的条件不匹配，这可能是一条宝贵的信息。
- en: 'In other cases, a missing case will mean something specific relative to how
    the data has been generated, such as in census data, where you don’t get answers
    relative to income if the answerer is too rich or poor because the necessity of
    being socially acceptable influences answers. Creating a binary feature indicating
    if a value is missing can help keep track of such patterns. See the MissingIndicator
    in the Scikit-learn package for more details on this kind of processing: [https://mng.bz/EaxO](https://mng.bz/EaxO).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，缺失的案例将相对于数据生成方式具有特定的含义，例如在人口普查数据中，如果回答者过于富有或贫穷，那么你将无法得到关于收入问题的答案，因为社会可接受性的必要性影响了回答。创建一个表示值是否缺失的二进制特征可以帮助跟踪这种模式。有关此类处理的更多详细信息，请参阅Scikit-learn包中的MissingIndicator：[https://mng.bz/EaxO](https://mng.bz/EaxO)。
- en: 2.2.5 Rare categories
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.5 稀有类别
- en: On the side of categoricals, dealing with too many labels or rare categories
    in a feature are two conditions that must be addressed as soon as possible, possibly
    at extraction time. We have already mentioned high cardinality categorical features
    in the chapter. Instead, you have rare categories in the context of categorical
    variables when specific categories occur infrequently or have very few instances
    in the dataset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类变量的方面，处理一个特征中的太多标签或稀少类别是必须尽快解决的问题，可能在提取时解决。我们已经在本章中提到了高基数分类特征。相反，在分类变量的上下文中，当特定类别在数据集中出现频率很低或实例很少时，你会有稀少类别。
- en: Rare categories increase the possibility of overfitting at training time and
    can usually be dealt with by aggregating them, thus forming a larger class. Domain
    knowledge may guide such aggregations, suggesting similar rare categories to aggregate
    into larger ones.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 稀少类别在训练时增加了过拟合的可能性，通常可以通过聚合它们来处理，从而形成一个更大的类别。领域知识可能指导这样的聚合，建议将相似的稀少类别聚合到更大的类别中。
- en: When, instead, the problem is that there are way too many categories, the most
    appropriate solution is target encoding, which is mostly effective for gradient
    boosting, or using embeddings, which works the best for deep learning approaches.
    We will discuss how these approaches work and how to effectively implement them
    in chapters 6 and 7.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题相反，即存在太多的类别时，最合适的解决方案是目标编码，这对于梯度提升法通常非常有效，或者使用嵌入，这对于深度学习方法效果最好。我们将在第6章和第7章中讨论这些方法的工作原理以及如何有效地实现它们。
- en: 2.2.6 Errors in data
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.6 数据错误
- en: 'Spotting incongruencies, misplaced, flipped, or distorted values is instead
    a topic of its own because fixing such a problem mostly depends on your knowledge
    of the application domain and the recording procedures for the data you are handling.
    Many errors may happen when a phenomenon is observed in the real world and recorded
    in data. Errors can range from inescapable measurement errors because of the instruments
    and sensors we use to an extensive catalog of possible mismatches, underestimates,
    and overestimates that will make the recording completely unreliable. More than
    a single error here and there, you should look for systematic errors. These errors
    almost always happen in certain situations and bias parts of the information.
    Knowing the data schema and the meaning of the data you are dealing with is the
    only reasonable remedy for such problems. It may sound very general and vague
    advice because it is, in reality, but you can only try to understand your data
    to the best of your ability and not leave anything for granted. Don’t believe
    that machine learning algorithms are robust and can fix all the errors in the
    data: systematic errors can limit the ability of your models to generalize and
    provide reliable predictions.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 发现不一致、放置不当、翻转或扭曲的值实际上是一个独立的话题，因为解决这个问题主要取决于你对应用领域和数据记录程序的了解。当在现实世界中观察到某种现象并将其记录在数据中时，可能会发生许多错误。错误可能从由于我们使用的仪器和传感器而产生的不可避免的测量误差，到可能使记录完全不可靠的广泛可能的错误、低估和过度估计的目录。除了这里和那里的单个错误之外，你应该寻找系统性的错误。这些错误几乎总是在某些情况下发生，并偏颇部分信息。了解数据模式和你要处理的数据的含义是解决此类问题的唯一合理的补救措施。这听起来可能非常普遍和模糊，因为实际上就是这样，但你只能尽力理解你的数据，不要对任何东西视而不见。不要相信机器学习算法是鲁棒的，可以修复数据中的所有错误：系统性的错误可能会限制你模型泛化的能力，并提供可靠的预测。
- en: 2.2.7 Leakage features
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.7 泄漏特征
- en: Finally, the last point of concern you should deal with is the presence of leakage
    features. You have leakage when some predictive information that shouldn’t be
    involved in the model training temporarily inflates results, rendering poor results
    at subsequent prediction time. Leakage can happen at row and feature levels. We
    have already discussed row leakage when dealing with non-IID rows, which occurs
    when some samples are associated with others because of time mediation or the
    mediation of some other feature. Now it is time to discuss feature-level leakage,
    which is a more common problem than expected when extracting data from business
    databases.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你应该处理的最后一个关注点是泄漏特征的存在。当你有一些不应该参与模型训练的预测信息暂时夸大结果时，就会发生泄漏，导致后续预测时间结果不佳。泄漏可能发生在行和特征级别。我们已经讨论了处理非-IID行时的行泄漏，这发生在一些样本由于时间中介或某些其他特征的中介而与其他样本相关联时。现在，是时候讨论特征级别的泄漏了，这在从业务数据库提取数据时比预期的更常见。
- en: The key principle is to ensure temporal consistency in the features used for
    modeling. Ideally, the features should align with or precede the target variable
    in time. In the best scenario, no feature should be created or generated after
    the target variable’s own time point. This temporal alignment helps to avoid potential
    data leakage and ensures that the model is making predictions based on information
    that would have been available at the time of prediction. You need a model able
    to predict the present or the future, but you cannot do that if the necessary
    features you have trained the model on are to be produced at a time point following
    the prediction itself.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 关键原则是确保用于建模的特征在时间上的一致性。理想情况下，特征应该与目标变量在时间上保持一致或先于目标变量。在最佳情况下，不应在目标变量的时间点之后创建或生成任何特征。这种时间上的对齐有助于避免潜在的数据泄露，并确保模型是基于预测时可能可用的信息进行预测。你需要一个能够预测现在或未来的模型，但如果你训练模型所必需的特征是在预测本身之后的时间点产生的，那么你就无法做到这一点。
- en: When training, it is easy to break this constraint because you are taking everything
    from the past, and many information sources used for building your training data
    may not be properly documented in regard to the creation or modification time.
    Take, for instance, the example of a business offering loans using a machine learning
    algorithm. Knowing how late the payments have been may provide a tremendous predictive
    feature at training time, but reasoning how such a feature would be unavailable
    when granting the loan or not—since it is a future behavior near to the time the
    target will be defined—renders it a useless, misleading feature because of future
    leakage. As a proposed solution for this problem, we suggest closely verifying
    the generative times for features and targets, if possible, and checking if the
    target comes after the features or vice versa. Frequently, such information is
    readily available, depending on how data storage is organized in your company.
    For instance, your data could have specific meta-data showing the date and time
    when an insertion or an update has happened, or your database administrator has
    perhaps set up a particular timestamp field to snapshot the moment some changes
    occur.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，很容易打破这个约束，因为你正在从过去获取一切，而你用于构建训练数据的信息源可能没有正确记录创建或修改时间。以一个使用机器学习算法提供贷款的商业为例，了解支付延迟可能会在训练时提供一个非常强大的预测特征，但推理这样一个特征在批准贷款或拒绝贷款时将不可用——因为它是一种接近目标变量定义时间点的未来行为——由于未来泄露，这使得它成为一个无用且具有误导性的特征。作为这个问题的解决方案，我们建议尽可能密切地验证特征和目标变量的生成时间，并检查目标是否在特征之后或反之亦然。通常，这种信息很容易获得，这取决于你公司中数据存储的组织方式。例如，你的数据可能具有特定的元数据，显示插入或更新发生的日期和时间，或者数据库管理员可能已经设置了一个特定的时间戳字段来快照某些变化发生的时刻。
- en: In the next section, after discussing ideal conditions in tabular data and what
    could happen when they are not met, we will get nearer to real-world data by discussing
    how to find tabular data on the internet and in your organization.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，在讨论表格数据的理想条件以及当这些条件不满足时可能发生的情况之后，我们将通过讨论如何在互联网和你的组织中找到表格数据，来更接近现实世界的数据。
- en: 2.3 Finding external and internal data
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 寻找外部和内部数据
- en: Suppose you need to find some dataset for a machine learning project. Following
    the instructions in this section, you are guaranteed to find what you need if
    it is available and accessible on the internet. In fact, not only has the number
    and quality of data repositories increased over time, but now we have new search
    and aggregation tools such as Google Dataset Search and Kaggle Datasets that allow
    us to specifically detail what we are looking for and obtain a list of results
    to choose from. But let’s start from the beginning.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要为机器学习项目找到一些数据集。按照本节中的说明，如果你需要的数据集在互联网上可用且可访问，你一定能找到它。事实上，随着时间的推移，数据仓库的数量和质量都在增加，现在我们有了新的搜索和聚合工具，如Google
    Dataset Search和Kaggle Datasets，这些工具允许我们具体说明我们正在寻找的内容，并获得一个可供选择的列表。但让我们从最基本的地方开始。
- en: Finding the data you need always comes after defining the project and its purposes.
    Whether your objectives are business-related or academic, only after you have
    framed the goals clearly can you decide what kind of data you need to assemble.
    It is not linear because you’ll usually need to reiterate various times between
    objectives, data, and available resources. Still, data always comes after having
    a well-defined purpose and before any engineering, like building a pipeline, which
    is a sequence of data processing steps from the data repository to where the computations
    are executed, or any further modeling action. Necessarily, in the middle between
    provisioning data and processing it, there is the phase of data understanding,
    which is the concluding topic of our chapter.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 找到所需数据总是在定义项目和其目的之后。无论您的目标是商业相关还是学术相关，只有当您明确地设定了目标后，您才能决定需要收集哪种类型的数据。这个过程不是线性的，因为您通常需要在目标、数据和可用资源之间多次迭代。尽管如此，数据总是在明确目的之后，在构建管道（从数据存储库到计算执行的序列数据处理步骤）或任何进一步的建模行动之前。必然的是，在提供数据和处理数据之间，存在数据理解阶段，这是我们章节的最后一个主题。
- en: Since machine learning models need a target—that is, something to predict such
    as a number for a regression problem or a class or label for a classification
    problem—you will first be concerned with finding one or more outputs for your
    problem. Then, all machine learning models need some other information—a set of
    predictors or features called predictors—to be used to predict your target outputs.
    It is a process where you map some data (the predictors) to other data (the outputs).
    Since without any target you cannot even start your project, whereas, on the other
    end, you may begin with an incomplete set of predictors and increase them along
    the way, do always start locating your target first and then be concerned with
    your predictors.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习模型需要一个目标——即预测的内容，比如回归问题中的一个数字或分类问题中的一个类别或标签——您首先需要关注的是为您的问题找到一个或多个输出。然后，所有机器学习模型都需要一些其他信息——一组称为预测器的预测因子或特征——用于预测您的目标输出。这是一个将一些数据（预测器）映射到其他数据（输出）的过程。由于没有目标，您甚至无法开始项目，而在另一端，您可能从一个不完整的预测器集合开始，并在过程中增加它们，因此始终首先定位您的目标，然后再关注您的预测器。
- en: Finding suitable data for your machine learning models and processing it correctly
    in the data preparation and data pipelining phases is one of the activities data
    scientists spend more time and effort on—and actually where they get stuck the
    most. At this time, you will need to invest efforts in terms of getting the data
    out of the repositories where it is stored after having understood how it is organized
    (the data scheme) and how to properly bring it all together into a single dataset
    in a way that is usable for your project.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为您的机器学习模型找到合适的数据，并在数据准备和数据管道阶段正确处理它，是数据科学家花费更多时间和精力的活动之一——实际上也是他们最常遇到瓶颈的地方。在这个时候，您需要投入精力来获取数据，这些数据存储在您已经了解其组织方式（数据架构）以及如何将其全部整合成一个可用于您项目的单一数据集的方式。
- en: 2.3.1 Using pandas to access data stores
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 使用 pandas 访问数据存储
- en: 'Usually, you can find both the target and the predictors in the same data store,
    if not in the same table or file. Still, sometimes you may find all parts of your
    necessary data scattered around and they need to be adequately assembled before
    any usage. You may find such data inside or outside of your organization. A common
    scenario when locating data resources within an organization is to find them scattered
    among Excel files, stored in *normalized form* in a data warehouse (DWH), or left
    unmanaged in data streams. As for DWH, information is organized into multiple
    tables to minimize redundancy and improve data integrity, scalability, performance,
    and ease of maintenance. This condition is called a *normalized schema*, where
    information has been broken down into smaller, more focused tables containing
    a single subject area or entity. Typically, in such a situation, you can find
    three types of tables to be assembled:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你可以在同一个数据存储中找到目标和预测变量，如果不是在同一个表格或文件中。然而，有时你可能发现所有必要的数据都散布在各个地方，在它们被使用之前需要适当组装。你可能会在组织内部或外部找到这样的数据。在组织内部定位数据资源的一个常见场景是在Excel文件中找到它们，这些文件以数据仓库（DWH）中的*规范化形式*存储，或者在不管理的数据流中留下。至于DWH，信息被组织到多个表格中，以最小化冗余并提高数据完整性、可扩展性、性能和维护的简便性。这种情况被称为*规范化模式*，其中信息已被分解成包含单一主题区域或实体的更小、更专注的表格。通常，在这种情况下，你可以找到三种类型的表格需要组装：
- en: '*Event tables (transaction tables or fact tables)* are designed to hold records
    of specific business events measured at a particular moment. Examples of such
    events could be an order placed in an e-commerce platform, a credit card transaction
    in banking, a doctor’s visit in healthcare, or a user’s clickstream on the internet.
    These tables can take various forms depending on the type of business and the
    specific events being tracked.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*事件表（事务表或事实表）* 设计用于存储特定业务事件在特定时刻的记录。此类事件的例子可能包括在电子商务平台上下的订单、银行中的信用卡交易、医疗保健中的医生访问或互联网上的用户点击流。这些表格的形式可能因业务类型和跟踪的具体事件而异。'
- en: '*Item tables* (also known as product tables) are a type of table found in a
    DWH that provides detailed information about specific business products or events.
    For example, an item table might contain information, such as description, prices,
    and stock levels, about the individual product items purchased as part of a customer
    order or the drug prescriptions problems during patient–doctor visits. The purpose
    of an item table is to provide more in-depth data to be used for analysis and
    decision-making purposes.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*项目表*（也称为产品表）是DWH中的一种表格，提供有关特定业务产品或事件的详细信息。例如，项目表可能包含有关单个产品项目的描述、价格和库存水平等信息，这些产品项目是客户订单的一部分，或者在患者-医生访问期间出现的药物处方问题。项目表的目的在于提供更深入的数据，用于分析和决策。'
- en: '*Dimension tables* store additional descriptive information to add context
    to the data in other tables, such as a person’s birth date or location or a product’s
    category. When using a dimension table, caution is essential because its information
    may have been changed or updated over time. Data updating can happen in two ways:
    by maintaining a history of changes by creating a new record for each modification
    with an attached date or by simply overwriting old data with new data. In the
    latter case, you may be using data that can act as noise for your models since
    its rows are temporally misaligned, or it can leak information from the future,
    affecting your model’s capabilities to predict correctly.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*维度表* 存储额外的描述性信息，以增加对其他表中数据的上下文，例如一个人的出生日期或地点，或一个产品的类别。在使用维度表时，必须谨慎，因为其信息可能随着时间的推移而改变或更新。数据更新可以通过两种方式发生：通过维护更改的历史记录，为每次修改创建一个新的记录并附上日期，或者简单地用新数据覆盖旧数据。在后一种情况下，你可能会使用那些可以充当模型噪声的数据，因为其行在时间上是不对齐的，或者它可能会泄露未来的信息，影响模型正确预测的能力。'
- en: 'Similarly, when dealing with Excel files, you often encounter scattered and
    denormalized data. In such cases, as with DWH, you face the challenge of working
    with similar tables and the identical need to consolidate and assemble them into
    a coherent structure. For such an assembling task between event tables, item tables,
    and dimension tables, there are different tools available on the market—some of
    them even of the no-code type, where you operate without scripting but using icons
    and point-and-click actions, for instance. Historically, SQL has been the elective
    tool for programmers to gather and combine multiple data sources. Still, SQL is
    the best choice today when all your data resides in a relational database. It
    can help you structure a series of queries and temporary tables until you arrive
    at a final data table to download or directly feed to a machine learning algorithm.
    When there are multiple sources and we need to check and visualize intermediate
    results, however, pandas, our choice in the first place for handling diverse data
    types, is again the tool we suggest to master. Pandas has a lot of functionalities
    that mimic and extend those of SQL queries. Selecting, filtering, aggregating,
    ordering, and processing are as easy on pandas as on a relational database, and
    they follow the same principles, though sometimes based on different terms. For
    instance, indexes in DataFrames are assimilable to primary keys in relational
    databases. In addition, pandas DataFrames present further characteristics that
    make them more versatile and powerful than SQL for data science tasks:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当处理Excel文件时，你经常会遇到分散和非规范化的数据。在这种情况下，就像在DWH中一样，你面临着处理类似表和将它们合并成一个连贯结构的挑战。对于事件表、项目表和维度表之间的这种组装任务，市场上有很多不同的工具可用——其中一些甚至是零代码类型的，例如，你无需编写脚本，而是使用图标和点击操作来操作。从历史上看，SQL一直是程序员收集和组合多个数据源的首选工具。然而，当所有数据都驻留在关系型数据库中时，SQL仍然是最佳选择。它可以帮助你构建一系列查询和临时表，直到你到达一个最终的数据表以下载或直接输入到机器学习算法中。然而，当存在多个来源并且我们需要检查和可视化中间结果时，我们首先选择用于处理多种数据类型的pandas工具，再次是我们建议掌握的工具。Pandas有很多功能模仿并扩展了SQL查询的功能。选择、过滤、聚合、排序和处理在pandas上与在关系型数据库上一样简单，尽管有时基于不同的术语。例如，DataFrames中的索引可以与关系型数据库中的主键相提并论。此外，pandas
    DataFrames还呈现了使它们比SQL在数据科学任务上更灵活和强大的进一步特征：
- en: An API that is common to other tools makes it easy to switch from pandas, which
    were created for use on a single CPU, to multiprocessing or distributed computing.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个与其他工具通用的API使得从专为单核CPU使用而创建的pandas切换到多进程或分布式计算变得容易。
- en: You can achieve processing and data exploration at the same time. In particular,
    it is straightforward to plot the data you have prepared, as we will see at the
    end of this chapter.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以同时实现处理和数据分析。特别是，绘制你准备好的数据非常简单，正如我们在本章末尾将看到的。
- en: You have more control of all the changes and the necessary manipulation steps
    that should happen on the data, which also means that it is easier to make an
    error, allowing you to store away intermediate results and preserve some data
    characteristics, such as data types and data ordering, which is instead sometimes
    hard to achieve in SQL.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你对所有的更改和应该在数据上发生的必要操作步骤有更多的控制，这也意味着更容易出错，允许你存储中间结果并保留一些数据特征，如数据类型和数据排序，这在SQL中有时很难实现。
- en: Let’s imagine having three tables in a data warehouse, each containing different
    information about a company’s products. These tables need to be joined to create
    an analysis or train a machine learning model, as shown in figure 2.2.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象在一个数据仓库中有三张表，每张表都包含关于公司产品不同信息。这些表需要被连接起来以创建分析或训练机器学习模型，如图2.2所示。
- en: 'The task is a quite simple example that would require a SQL query to return
    a dataset combining the three of them:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简单的示例，需要SQL查询来返回一个结合这三个表的组合数据集：
- en: '[PRE7]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/CH02_F02_Ryan2.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F02_Ryan2.png)'
- en: Figure 2.2 Three simple tables describing products’ features
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 描述产品特征的三个简单表
- en: However, the same could be easily achieved in pandas using the merge function,
    keeping control of the various stages where the data is merged and having it ready
    for further transformations to render it suitable to machine learning.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用pandas的merge函数同样可以轻松实现这一点，同时保持对数据合并各个阶段的控制，并使其准备好进行进一步的转换以适应机器学习。
- en: Listing 2.4 Merging datasets in pandas
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.4 在 pandas 中合并数据集
- en: '[PRE8]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① The first table, containing prices
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ① 包含价格的第一个表格
- en: ② The second table, containing descriptions
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ② 包含描述的第二个表格
- en: ③ The third table, containing makers and characteristics
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 包含制造商和特性的第三个表格
- en: ④ Merges the first two tables
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 合并前两个表格
- en: ⑤ Merges the previous two joined tables with the third one
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将前两个已连接的表格与第三个表格合并
- en: 'One limitation to doing this in pandas for every use case is computational
    efficiency because the package is slow. See, for instance, the following Stack
    Overflow answer: [https://mng.bz/N1x1](https://mng.bz/N1x1). There are also scalability
    problems since pandas cannot handle data larger than your computer’s available
    memory. Such limitations exist because the package has been coded with functionalities
    in mind, not performance. Consequently, most of the functions found on pandas
    are written in plain Python, and they make little access to optimized compiled
    routines—mainly routines written in Fortran and C++, as, for instance, NumPy,
    another popular matrix and array manipulation package, does. However, thanks to
    its popular API, you can learn and start with your projects using just pandas
    and then scale up to more powerful tools. As stated in the previous point, different
    products have more or less compatibility with pandas:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pandas 中进行此类操作的局限性之一是计算效率，因为该包运行较慢。例如，请参阅以下 Stack Overflow 答案：[https://mng.bz/N1x1](https://mng.bz/N1x1)。由于
    pandas 无法处理比计算机可用内存更大的数据，因此也存在可扩展性问题。这些限制存在是因为该包在设计时考虑了功能，而不是性能。因此，pandas 上大多数功能都是用纯
    Python 编写的，并且它们很少访问优化编译例程——主要是 Fortran 和 C++ 编写的例程，例如 NumPy，另一个流行的矩阵和数组操作包。然而，由于其流行的
    API，您可以使用 pandas 学习并开始您的项目，然后扩展到更强大的工具。正如前一点所述，不同的产品与 pandas 的兼容性或多或少：
- en: Dask ([https://www.dask.org/](https://www.dask.org/)) is an open-source Python
    library that presents both low-level and high-level interfaces for the user. It
    is designed to run on computer clusters. Dask can also work in multiprocessing
    mode on multiple CPU processors, and it handles out-of-core tasks easily. In these
    tasks, you process more data than your RAM can handle by working it by chunks
    living on the disk. This data structure copies the pandas DataFrame API but is
    much more capable of handling many rows.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask ([https://www.dask.org/](https://www.dask.org/)) 是一个开源的 Python 库，为用户提供低级和高级接口。它设计用于在计算机集群上运行。Dask
    还可以在多个 CPU 处理器上以多进程模式工作，并且可以轻松处理离核任务。在这些任务中，您通过在磁盘上工作的数据块处理比 RAM 可以处理更多的数据。这种数据结构复制了
    pandas DataFrame API，但能够处理更多的行。
- en: Ray ([https://www.ray.io/](https://www.ray.io/)) is a low-level framework that
    parallelizes Python code across processors or clusters. It is ideal as a backend
    for other high-end solutions, such as Modin.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ray ([https://www.ray.io/](https://www.ray.io/)) 是一个低级框架，可以在处理器或集群之间并行化 Python
    代码。它非常适合作为其他高端解决方案的后端，例如 Modin。
- en: Modin ([https://github.com/modin-project/modin](https://github.com/modin-project/modin))
    is perhaps the most compatible tool with Pandas API. It works just by a simple
    replacement such as `import modin.pandas` `as` `pd`, and it performs best with
    Ray as a backend.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Modin ([https://github.com/modin-project/modin](https://github.com/modin-project/modin))
    可能是与 Pandas API 最兼容的工具。它只需简单的替换即可工作，例如 `import modin.pandas` `as` `pd`，并且以 Ray
    作为后端时表现最佳。
- en: Vaex ([https://vaex.io/](https://vaex.io/)) is a Python library for lazy out-of-core
    DataFrames (similar to pandas). You can also visualize and explore big tabular
    datasets on your stand-alone machine or a server. Being lazy, it can optimize
    its operations and reach a performance of up to a billion objects/rows processed
    per second.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaex ([https://vaex.io/](https://vaex.io/)) 是一个用于懒加载离核 DataFrame 的 Python 库（类似于
    pandas）。您还可以在您的独立机器或服务器上可视化和探索大型表格数据集。由于其懒加载特性，它可以优化其操作，达到每秒处理高达十亿个对象/行的性能。
- en: RAPIDS ([https://rapids.ai/](https://rapids.ai/)) is a collection of libraries
    for using GPUs’ computational capabilities on large matrices. It offers cuDF,
    a partial replacement for pandas. Data processing can improve efficiency (time
    to compute), not scalability, because GPUs must access your memory to determine
    what to calculate.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAPIDS ([https://rapids.ai/](https://rapids.ai/)) 是一组用于在大型矩阵上使用 GPU 计算能力的库。它提供了
    cuDF，这是 pandas 的部分替代品。数据处理可以提高效率（计算时间），而不是可扩展性，因为 GPU 必须访问您的内存以确定要计算的内容。
- en: Spark ([https://spark.apache.org/](https://spark.apache.org/)) is a solution
    for map-reduce, a big data processing technique, graph algorithms, streaming data,
    and SQL queries working on single-node machines or clusters. It offers various
    packages and a DataFrame data structure similar to pandas. It is the solution
    best suited to handling massive quantities of tabular data.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark ([https://spark.apache.org/](https://spark.apache.org/)) 是一种用于 map-reduce
    的解决方案，这是一种大数据处理技术，适用于图算法、流数据以及单节点机器或集群上的 SQL 查询。它提供了各种包和一个类似于 pandas 的 DataFrame
    数据结构。它是处理大量表格数据最合适的解决方案。
- en: Polars ([https://www.pola.rs/](https://www.pola.rs/)) is designed as a high-performance
    DataFrame library. It has been written in Rust, which allows for faster execution,
    comparable to C/C++, and distributable computations. Polars also has better memory
    management and large dataset handling and uses a columnar storage format. Columnar
    storage formats are more efficient for storing and accessing dense data, which
    is the most typical kind of tabular data. In contrast, row-based storage formats
    are more efficient for storing and accessing mostly sparse data (pandas uses a
    row-based storage format). Polars and pandas have similar APIs. However, there
    are differences because Polars can operate both in eager mode, where commands
    are executed immediately, or lazy mode, where commands are executed upon a specific
    command. Still under development, Polars is rapidly gaining traction in the data
    science community.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polars ([https://www.pola.rs/](https://www.pola.rs/)) 被设计为一个高性能的 DataFrame 库。它是用
    Rust 编写的，这使得执行速度更快，与 C/C++ 相当，并且可以进行分布式计算。Polars 还具有更好的内存管理和处理大数据集的能力，并使用列式存储格式。列式存储格式在存储和访问密集型数据时更为高效，这是表格数据最常见的类型。相比之下，基于行的存储格式在存储和访问主要是稀疏数据时更为高效（pandas
    使用基于行的存储格式）。Polars 和 pandas 具有相似的 API。然而，它们之间也存在差异，因为 Polars 可以在即时执行模式（命令立即执行）或懒执行模式（命令在特定命令下执行）下运行。尽管仍在开发中，Polars
    正在迅速获得数据科学社区的青睐。
- en: Based on your usage of pandas and the dimensions of your tabular data, you may
    find each one of these projects interesting. As a general suggestion, we advise
    you to check if your pandas functions are available in each of the products mentioned
    previously so you won’t have to refactor your code and then evaluate the best
    solution for the size of your data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您对 pandas 的使用以及您表格数据的维度，您可能会发现这些项目中的每一个都很有趣。作为一个一般性的建议，我们建议您检查之前提到的每个产品是否都提供了您需要的
    pandas 函数，这样您就不必重构您的代码，然后评估最适合您数据规模的解决方案。
- en: 2.3.2 Internet data
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 互联网数据
- en: Now that we have created a plan for acquiring and assembling your data from
    various data storage sources such as relational databases, DWHs, and data lakes,
    we need to provide you with guidance on where to find additional sources of information
    on the internet. These sources will help you apply the deep learning and machine
    learning algorithms outlined in this book and allow you to test these algorithms
    on different datasets or even benchmark your models. In other words, we will help
    you locate online resources that can be used to increase and enhance the data
    you have already acquired.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为您从各种数据存储源（如关系数据库、DWH 和数据湖）获取和组装数据制定了计划，我们需要向您提供指导，告诉您在哪里可以找到互联网上的更多信息来源。这些来源将帮助您应用本书中概述的深度学习和机器学习算法，并允许您在不同的数据集上测试这些算法，甚至基准测试您的模型。换句话说，我们将帮助您找到可以用来增加和增强您已经获取的数据的在线资源。
- en: Concerning where to look for sources, there are specialized websites where tabular
    data is collected and routinely used by researchers and practitioners. The best
    example is the UCI Machine Learning Repository ([https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)).
    The machine learning community has long used this website for educational purposes
    and research on machine learning algorithms. We can also quote OpenML ([https://www.openml.org/](https://www.openml.org/)),
    the repository used by Scikit-learn, as a source for its examples. It is supported
    by the Open Machine Learning Foundation, a nonprofit organization whose mission
    is to make machine learning simple, accessible, collaborative, and open. It is
    also sponsored by private companies, such as Amazon, and universities. In both
    cases—UCI and OpenML—you can download each dataset following the instructions
    provided or by directly accessing the URL of the data itself. When we use datasets
    from these sources, we will provide the Python code snippet to download the data
    directly for you to work on.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 关于寻找数据来源的地方，有一些专门网站收集并定期由研究人员和实践者使用表格数据。最好的例子是UCI机器学习仓库([https://archive.ics.uci.edu/](https://archive.ics.uci.edu/))。机器学习社区长期以来一直使用这个网站进行机器学习算法的教育和研究。我们还可以引用OpenML([https://www.openml.org/](https://www.openml.org/))，Scikit-learn使用的仓库，作为其示例的来源。它由一个非营利组织Open
    Machine Learning Foundation支持，其使命是使机器学习变得简单、易于访问、协作和开放。它还由像亚马逊这样的私营公司和大学赞助。在UCI和OpenML的两种情况下，您可以根据提供的说明或直接访问数据的URL下载每个数据集。当我们使用这些来源的数据集时，我们将提供Python代码片段，以便您可以直接下载并工作。
- en: Apart from these open repositories, many other websites offer a shorter selection
    of open data—data provided freely for academic, scientific, or even commercial
    usage (but you have to check the usage license they came with). Governments, the
    scientific community, and sometimes even private companies grant such data to
    the public. You can get lucky and get some interesting datasets for your work
    by consulting the Harvard University Dataverse free data repository at [https://dataverse.harvard.edu/](https://dataverse.harvard.edu/)
    or by browsing the Dataset subreddit [https://www.reddit.com/r/datasets/](https://www.reddit.com/r/datasets/).
    In both cases, it is an excellent way to stumble across something interesting,
    but it may also be disappointing if you seek specific data and need help finding
    it. Suppose your search revolves around data relative to the public or macroeconomic
    sphere, such as transportation, energy, political participation, commerce, industrial
    production, consumption, and so on. In that case, the open data portals should
    provide hints on where to look for open datasets. The two best portals are Data
    Portals ([http://dataportals.org/](http://dataportals.org/)), which covers the
    world, and Open Data Monitor ([https://opendatamonitor.eu/](https://opendatamonitor.eu/)),
    which specializes in the European region. Another good source is the National
    Statistical Service. You can browse for specific countries in this comprehensive
    list provided by the United States Census Bureau ([https://mng.bz/eynQ](https://mng.bz/eynQ)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些公开的仓库之外，许多其他网站提供了更短的开源数据选择——这些数据免费提供给学术、科学甚至商业使用（但您必须检查它们附带的使用许可）。政府、科学界有时甚至私营公司会将此类数据授予公众。通过咨询哈佛大学数据仓库免费数据仓库[https://dataverse.harvard.edu/](https://dataverse.harvard.edu/)或浏览Dataset子版块[https://www.reddit.com/r/datasets/](https://www.reddit.com/r/datasets/)，您可能会幸运地找到一些有趣的数据集用于您的工作。在两种情况下，这都是偶然发现有趣事物的好方法，但如果您寻求特定的数据并需要帮助找到它，可能会感到失望。假设您的搜索围绕与公众或宏观经济领域相关的数据，例如交通、能源、政治参与、商业、工业生产、消费等，在这种情况下，开源数据门户应提供有关查找开源数据集的提示。两个最好的门户网站是数据门户([http://dataportals.org/](http://dataportals.org/))，它覆盖全球，以及开放数据监控器([https://opendatamonitor.eu/](https://opendatamonitor.eu/))，它专注于欧洲地区。另一个好来源是国家统计局。您可以在美国人口普查局提供的这个综合列表中浏览特定国家([https://mng.bz/eynQ](https://mng.bz/eynQ))。
- en: Apart from these specific examples, the best way to come into contact with the
    type of open data you search is undoubtedly through the dataset search engine
    provided by Google, which can be found at [https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/).
    The Google Dataset Search, comprising many possible sources and scattered repositories,
    should give you access to what you are looking for through its search results.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些具体的例子，无疑通过Google提供的数据集搜索引擎接触到你搜索的开放数据类型是最佳方式，该搜索引擎可以在[https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/)找到。Google数据集搜索，包含许多可能的来源和分散的存储库，应该可以通过其搜索结果为你提供所需的内容。
- en: Figure 2.3 shows the results from Dataset Search from Google, where we asked
    for a dataset relative to credit scoring data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3显示了Google数据集搜索的结果，我们请求了与信用评分数据相关的数据集。
- en: '![](../Images/CH02_F03_Ryan2.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F03_Ryan2.png)'
- en: 'Figure 2.3 What Google Dataset Search returns for credit scoring data. Search
    input box (top of the page): where you input your search string as you would do
    in a search engine. Result display panel (left panel): where you can browse the
    results found by the search engine. Result display (center right page): where
    key information of the dataset is displayed. You are provided with links to reach
    the data.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 Google数据集搜索返回的信用评分数据。搜索输入框（页面顶部）：在这里输入你的搜索字符串，就像在搜索引擎中做的那样。结果显示面板（左侧面板）：在这里你可以浏览搜索引擎找到的结果。结果显示（页面中心右侧）：在这里显示数据集的关键信息。你将提供链接以访问数据。
- en: It is not unusual to look for outside data to start a project when you don’t
    have similar data available in your systems or want to experiment with some idea
    or machine learning model. For instance, let’s pretend you need to find external
    credit scoring data for your project to build a model from scratch based on external
    data. Credit scoring is a computed quantification of a person’s or business’s
    creditworthiness. Building a credit scoring model helps you run any business successfully,
    such as mortgages, auto loans, credit cards, and private loans, where you have
    to offer, extend, or deny credit. Credit scoring can be used for risk-based pricing—that
    is, for setting a fair price for credit, given the risks of not repaying.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的系统中没有可用的相似数据，或者想要尝试一些想法或机器学习模型时，寻找外部数据来启动一个项目并不罕见。例如，假设你需要为你的项目寻找外部的信用评分数据，以便从头开始基于外部数据构建模型。信用评分是对个人或企业信用价值的计算量化。构建信用评分模型可以帮助你成功运营任何业务，例如抵押贷款、汽车贷款、信用卡和私人贷款，在这些业务中，你必须提供、扩展或拒绝信用。信用评分可以用于基于风险的定价——也就是说，在考虑到不偿还的风险的情况下，为信用设定一个公平的价格。
- en: Looking, for instance, for credit scoring data as you would on the Google search
    engine will open up a panel on the left side of the browser listing a series of
    datasets that could satisfy your request. You can filter the results by recency
    of the data, format, license (a point to be addressed if you want to make commercial
    use of the data), costs, and category of interest. On the right panel, a description
    of the key characteristics of the chosen dataset will appear, with a link allowing
    you to access the original repository where the data is stored and documented.
    To download the data, follow the instructions on each landing page you will be
    pointed to.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，像在Google搜索引擎中搜索信用评分数据一样，将在浏览器左侧打开一个面板，列出一系列可能满足你要求的数据集。你可以通过数据的最新程度、格式、许可（如果你想要商业用途，这是一个需要解决的问题）、成本和感兴趣的分类来过滤结果。在右侧面板中，将出现所选数据集关键特征的描述，以及一个链接，允许你访问存储和记录数据的原始存储库。要下载数据，请遵循你将被引导到的每个着陆页上的说明。
- en: To our knowledge, Google Dataset Search is the best tool for finding the data
    you need or are looking for. In addition, we would like to point out another resource
    made available by Google through Kaggle, a company devoted to data science competitions
    that was acquired by Google in 2017\. As you will surely notice, the search engine
    often returns datasets hosted on the Kaggle platform using Google Dataset Search.
    Apart from offering a competition platform, Kaggle offers a dataset hosting service
    called Kaggle Datasets, found at [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，Google Dataset Search是寻找您所需或正在寻找的数据的最佳工具。此外，我们还想指出谷歌通过Kaggle提供的一项其他资源，Kaggle是一家专注于数据科学竞赛的公司，于2017年被谷歌收购。您肯定会注意到，搜索引擎经常使用Google
    Dataset Search返回托管在Kaggle平台上的数据集。除了提供竞赛平台外，Kaggle还提供了一种名为Kaggle数据集的数据集托管服务，位于[https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)。
- en: Kaggle Datasets is part of a service offered to platform users, allowing them
    to download data freely (sometimes because they open-sourced it) in exchange for
    points and rank in the gamified Kaggle system. The gaming formulation also pushes
    for data to be uploaded soon. Hence, you will find the latest data around, and
    you can expect them to be updated often. The result is an impressive collection
    of data that is not all that easy to find elsewhere, ranging from economic statistics
    to transaction collections that are helpful in building recommender systems from
    scratch. Depending on the engagement of the Kaggle user who posted the dataset
    and that of the community, you may find documentation, data analysis, machine
    learning models, and discussion associated with the data.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle数据集是提供给平台用户的一项服务的一部分，允许他们免费下载数据（有时因为它们是开源的），以换取Kaggle游戏化系统中的积分和排名。游戏化公式也推动数据尽快上传。因此，您会发现最新的数据，并且可以期待它们经常更新。结果是令人印象深刻的数据集集合，这些数据集在其他地方并不容易找到，从经济统计数据到有助于从头开始构建推荐系统的交易集合。
- en: If you can find what you are looking for, you can download the data from the
    Kaggle Dataset directly, using the download button shown in figure 2.4, after
    registering with the website. Registering also allows you to install and use the
    `kaggle-api` command ([https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api))
    to download datasets from the shell. For instance, after having installed it,
    you can download the German Credit Risk data by the command
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您能找到您想要的东西，您可以在注册网站后，使用图2.4中显示的下载按钮直接从Kaggle数据集下载数据。注册还允许您安装和使用`kaggle-api`命令（[https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api)）从shell中下载数据集。例如，安装后，您可以通过以下命令下载德国信用风险数据：
- en: '[PRE9]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/CH02_F04_Ryan2.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F04_Ryan2.png)'
- en: Figure 2.4 Options on the German Credit Risk page at Kaggle Datasets (menu on
    the right). Easily download the entire dataset with the Kaggle API command from
    the top-down menu. You can find the command by requiring it directly on the screen
    from the menu associated with the page, as shown in the figure.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 Kaggle数据集页面上的德国信用风险选项（右侧菜单）。您可以通过从上到下的菜单使用Kaggle API命令轻松下载整个数据集。您可以通过在页面关联的菜单中直接要求它来找到该命令，如图所示。
- en: 2.3.3  Synthetic data
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 合成数据
- en: In the scenario when you have already managed to acquire some datasets but need
    to increment your tabular data available, we have to mention synthetic data generation,
    which won’t create any data from scratch but can effectively increment and improve
    the existing datasets that you have available at hand, making using machine learning
    and deep learning models that require a more significant number of examples to
    run properly feasible. Synthetic data generation is a generative AI application
    and a growing field because it can
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在您已经成功获取了一些数据集但需要增加可用的表格数据的情况下，我们必须提到合成数据生成，这不会从头创建任何数据，但可以有效地增加和改进您手头可用的现有数据集，使得运行需要更多示例才能正常工作的机器学习和深度学习模型成为可能。合成数据生成是一个生成式AI应用，并且是一个不断发展的领域，因为它可以
- en: Overcome data scarcity by inflating the number of tabular examples available
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过增加可用的表格示例数量来克服数据稀缺问题
- en: Provide data diversity by enhancing portions of the data, such as in the case
    of the minority class in an unbalanced classification problem
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过增强数据的一部分来提供数据多样性，例如在不平衡分类问题中的少数类情况
- en: Generate edge cases so that it can provide you with data examples that you cannot
    frequently encounter and that you can use to test your designed systems
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成边缘情况，以便它可以为你提供你不太可能遇到的数据示例，你可以使用这些示例来测试你设计的系统
- en: Preserve privacy because it helps to generate tabular data with the same characteristics
    as the original one but with no privacy problem because all the represented data
    is fictitious
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护隐私，因为它有助于生成具有与原始数据相同特征但无隐私问题的表格数据，因为所有表示的数据都是虚构的
- en: In recent times, data generation has made great leap forwards, thanks to the
    first generative AI experiments with Generative Adversarial Networks (GANs) and
    Variational Autoencoders (VAEs), two deep learning architectures that were the
    spearhead of generative AI a few years ago.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，数据生成取得了巨大进步，这得益于与生成对抗网络（GANs）和变分自编码器（VAEs）的第一批生成性AI实验，这两种深度学习架构几年前是生成性AI的先锋。
- en: GANs were introduced by Ian Goodfellow and his colleagues in 2014\. They consist
    of a couple of deep neural networks—namely a generator and a discriminator—trained
    simultaneously and made to interact by challenging the discriminator to guess
    the generative work of the generator against the original examples. It is an unsupervised
    process, though. Given enough time and computation, the continuous comparison
    between the generator and discriminator should lead the generator to mimic the
    real-world distribution of the original examples used for comparison by the discriminator.
    This is indeed smart and amazing, if you consider that the generator never sees
    any instance of the data it should resemble because it builds its work from purely
    random noise. This process works extremely well both for images and tabular data.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: GANs由Ian Goodfellow及其同事在2014年提出。它们由一对深度神经网络组成——即生成器和判别器——同时训练并相互交互，通过挑战判别器猜测生成器对原始示例的生成工作。尽管这是一个无监督过程。给定足够的时间和计算，生成器和判别器之间的持续比较应该使生成器模仿判别器用于比较的原始示例的真实世界分布。如果你考虑到生成器从未看到任何它应该相似的数据实例，因为它从纯随机噪声中构建其作品，这个过程确实既聪明又神奇。这个过程对于图像和表格数据都工作得非常好。
- en: VAEs consist of an encoder and a decoder. Their function is to compress input
    data using the encoder to a state called the latent space, where the data information
    is highly condensed. Subsequently, the decoder decompresses the data with high
    fidelity and reconstructs the original data. The underlying idea is that if the
    compression-decompression process is effective, then the latent space encapsulates
    the core distributional information of the data, enabling the generation of new
    data. The architecture of VAEs is designed so that inputs are taken by the encoder
    and passed through a sequence of layers, which may consist of the same number
    of neurons or a decreasing number, until reaching a layer representing the latent
    space. This layer serves as the starting point for the decoder.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs由编码器和解码器组成。它们的功能是使用编码器将输入数据压缩到一个称为潜在空间的状态，其中数据信息高度浓缩。随后，解码器以高保真度解压缩数据并重建原始数据。其基本思想是，如果压缩-解压缩过程有效，那么潜在空间封装了数据的核心分布信息，从而能够生成新的数据。VAEs的架构设计使得输入被编码器接收并通过一系列层传递，这些层可能包含相同数量的神经元或递减的数量，直到达到代表潜在空间的层。这一层作为解码器的起点。
- en: How these deep learning approaches generate good data is proven by how a data
    science competition platform such as Kaggle uses synthetic data for its competitions.
    Due to the lack of tabular data competitions, Kaggle has recently launched a series
    of competitions based on synthetic data that don’t have anything to envy from
    traditional ones based on curated original data. The Tabular Playground Series
    has been a feature of Kaggle for the last two years and still continues to this
    day ([https://www.kaggle.com/competitions](https://www.kaggle.com/competitions)).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这些深度学习方法如何生成优质数据，可以通过像Kaggle这样的数据科学竞赛平台如何使用合成数据来证明其竞赛。由于缺乏表格数据竞赛，Kaggle最近推出了一系列基于合成数据的竞赛，这些竞赛与传统基于精选原始数据的竞赛相比，没有任何可羡慕之处。表格游乐场系列是Kaggle过去两年的一个特色，至今仍在继续（[https://www.kaggle.com/competitions](https://www.kaggle.com/competitions)）。
- en: 'In listing 2.5, we use the sdv package to generate 10,000 additional examples
    from the original German Credit Risk dataset, made up of 1,000 samples that we
    presented in the previous section. To install the sdv package on your system,
    use the following shell command:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 2.5 中，我们使用 sdv 包从原始的德国信用风险数据集生成 10,000 个额外的示例，该数据集由我们在上一节中展示的 1,000 个样本组成。要在您的系统上安装
    sdv 包，请使用以下 shell 命令：
- en: '[PRE10]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The sdv package, from an MIT initiative, is a set of open-source tools devised
    to help individuals and enterprises generate synthetic data starting from some
    original one. Such tools use classical statistical methods, such as the Gaussian
    copulas that can reproduce the distribution of multiple variables simultaneously,
    and deep learning. The deep learning tools are based on GANs and VAEs. In our
    example, we found that a VAE architecture can better and more easily mimic the
    data at hand. You can find more information about the sdv package on the GitHub
    page of the project ([https://github.com/sdv-dev/SDV](https://github.com/sdv-dev/SDV))
    or by reading the reference paper illustrating the methodology: Neha Patki, Roy
    Wedge, Kalyan Veeramachaneni, “The Synthetic Data Vault,” IEEE DSAA 2016 ([https://mng.bz/ga4V](https://mng.bz/ga4V)).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: sdv 包是由麻省理工学院发起的一个开源工具集，旨在帮助个人和企业从某些原始数据生成合成数据。这些工具使用经典统计方法，如高斯 Copulas，可以同时复制多个变量的分布，以及深度学习。深度学习工具基于
    GAN 和 VAE。在我们的例子中，我们发现 VAE 架构可以更好地、更简单地模拟手头的数据。您可以在项目的 GitHub 页面（[https://github.com/sdv-dev/SDV](https://github.com/sdv-dev/SDV)）上找到有关
    sdv 包的更多信息，或者通过阅读说明该方法的参考论文：Neha Patki，Roy Wedge，Kalyan Veeramachaneni，“合成数据宝库”，IEEE
    DSAA 2016 ([https://mng.bz/ga4V](https://mng.bz/ga4V))。
- en: Listing 2.5 Generating a synthetic dataset
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.5 生成合成数据集
- en: '[PRE11]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Imports from sdv the function to generate metadata from your original
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从 sdv 导入生成元数据的函数
- en: ② Imports from sdv the VAE to generate synthetic data
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从 sdv 导入 VAE 以生成合成数据
- en: ③ Loads the German Credit Risk dataset
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 加载德国信用风险数据集
- en: ④ Instantiates the metadata detector
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 实例化元数据检测器
- en: ⑤ Detects the metadata from the original data
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 从原始数据中检测元数据
- en: ⑥ Checks if the metadata has been correctly detected for each column
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 检查每列的元数据是否已正确检测
- en: ⑦ Corrects the “Saving accounts” columns metadata
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 修正“储蓄账户”列的元数据
- en: ⑧ Instantiates the VAE model and instructs it to train for 10,000 epochs
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 实例化 VAE 模型并指示其训练 10,000 个周期
- en: ⑨ Fits the VAE model with the original data
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 使用原始数据拟合 VAE 模型
- en: ⑩ Generates a new synthetic dataset of 10,000 examples
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 生成包含 10,000 个示例的新合成数据集
- en: The script takes a while to complete since a certain number of iterations over
    the original data is necessary for the VAE to develop a good latent representation
    of the original characteristics and distributions of the data. Once completed,
    the sampled examples from the new synthetic data resemble the sample from the
    original data, but if you need more formal proof, you must resort to an adversarial
    validation evaluation, where a machine learning algorithm is challenged to distinguish
    between the original data and the generated one. A machine learning algorithm
    is more apt at detecting even subtle patterns in data; hence, if it can be confused
    when distinguishing between original and generated data, the generated data is
    assumed to be of good quality. In the following listing, we show how to proceed
    with setting up adversarial validation for the synthetic data derived from the
    German Credit Risk data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本需要一段时间才能完成，因为 VAE 需要一定次数的原始数据迭代来发展出对原始数据特征和分布的良好潜在表示。一旦完成，从新合成数据中抽取的示例与原始数据的样本相似，但如果您需要更正式的证明，您必须求助于对抗验证评估，其中机器学习算法被挑战区分原始数据和生成数据。机器学习算法更擅长检测数据中的微妙模式；因此，如果它在区分原始和生成数据时被混淆，则假定生成数据质量良好。在下面的列表中，我们展示了如何为从德国信用风险数据派生的合成数据设置对抗验证。
- en: Listing 2.6 Testing a synthetic dataset using adversarial validation
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.6 使用对抗验证测试合成数据集
- en: '[PRE12]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Concatenates original and synthetic data together
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将原始数据和合成数据合并在一起
- en: ② Encodes categorical features into dummy variables
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将分类特征编码为虚拟变量
- en: ③ Classifies if the example is original or synthetic
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 判断示例是原始的还是合成的
- en: ④ Computes the ROC AUC score for the classification
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算分类的 ROC AUC 分数
- en: The area under the receiver operating characteristic (ROC AUC) score, also known
    as the area under the curve (AUC), is a metric used to evaluate the performance
    of a machine learning algorithm in a binary classification problem. It measures
    the algorithm’s ability to distinguish between positive and negative instances
    based on the predicted probabilities assigned to each observation. A score of
    1 indicates a perfect classifier, while a score of 0.5 suggests a model that performs
    no better than random guessing. You can learn more about this metric by consulting
    [https://mlu-explain.github.io/roc-auc/](https://mlu-explain.github.io/roc-auc/),
    part of the Machine Learning University, an education initiative from Amazon to
    teach machine learning theory.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 接收器操作特征（ROC AUC）分数下的面积，也称为曲线下的面积（AUC），是用于评估机器学习算法在二元分类问题中性能的指标。它衡量算法根据分配给每个观察值的预测概率区分正例和负例的能力。得分为1表示完美的分类器，而得分为0.5则表明模型的表现不如随机猜测。你可以通过查阅[https://mlu-explain.github.io/roc-auc/](https://mlu-explain.github.io/roc-auc/)（机器学习大学的一部分，是亚马逊的一项教育倡议，旨在教授机器学习理论）来了解更多关于这个指标的信息。
- en: In this case, the ROC AUC score is approximately 0.567, which indicates that
    the synthetic data is almost indistinguishable from the original data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，ROC AUC分数大约为0.567，这表明合成数据几乎与原始数据无法区分。
- en: In the next section, we will complete our overview of tabular data by exploring
    a tabular dataset retrieved from the UCI Machine Learning Repository and Kaggle
    Datasets. Many of the problems we quoted in the previous paragraphs will be spotted
    as we analyze the data, and some remedies will be implemented.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过探索从UCI机器学习仓库和Kaggle数据集检索到的表格数据集，来完成我们对表格数据的概述。在我们分析数据的过程中，许多在前几段中提到的
    问题将会被发现，并且一些补救措施将会被实施。
- en: 2.4 Exploratory data analysis
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 探索性数据分析
- en: Assembling the dataset into a data matrix arranged by rows and columns is just
    the starting point of a longer process. The data is then examined, explored, transformed,
    and finally fed into a model. Studying and exploring data are crucial stages of
    the process because they give you an idea of what you may have missed in the assembling
    phase and what can be done for your specific predictive problem. *Exploratory
    data analysis* (EDA) is often associated more with the feature engineering phase—more
    in the case of using classical machine learning approaches for tabular. Usually,
    EDA is instead almost completely ignored when using deep learning. We want to
    stress that EDA shouldn’t be limited to helping create features. It is an overall
    process of exploration to discover how the data can be used at its best.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集组装成按行和列排列的数据矩阵只是更长过程的开端。然后，数据将被检查、探索、转换，并最终输入到模型中。研究和探索数据是这个过程的关键阶段，因为它们能让你了解在组装阶段可能遗漏了什么，以及针对你特定的预测问题可以做什么。*探索性数据分析*（EDA）通常与特征工程阶段联系更为紧密——在表格数据中使用经典机器学习方法时更是如此。通常，当使用深度学习时，EDA几乎完全被忽视。我们想强调的是，EDA不应该仅限于帮助创建特征。它是一个整体探索过程，旨在发现数据如何被最佳地使用。
- en: EDA certainly has a statistical flavor since it was suggested by one of the
    most prominent statisticians of the 20th century, John W. Tukey, in his 1977 masterpiece,
    *Exploratory Data Analysis*. In his work, Tukey claimed that statistical work
    doesn’t come from simply modeling and hypothesis testing based on theoretical
    assumptions. According to Tukey, data by means of EDA can also tell you what is
    possible in scientific and engineering problems and hint you at the best ways
    to curate your data. There are no predefined blueprints for an EDA. Still, smart
    usage of statistical description and tests and the graphical representation of
    the features alone and in their relationship with others can tell you how to act
    on the data. In general, first, employing descriptive statistics and plots, you
    examine every single feature, called the univariate approach in statistics. You
    then examine how features relate to each other, called the bivariate approach.
    Finally, you try to get a glimpse of all the features together using multivariate
    techniques and dimensionality reduction ones, such as t-SNE and UMAP.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: EDA 确实带有统计学的味道，因为它是20世纪最杰出的统计学家之一，约翰·W·图基在1977年的杰作《探索性数据分析》中提出的。在他的工作中，图基声称统计工作并不仅仅来自基于理论假设的建模和假设检验。根据图基的说法，通过
    EDA 的数据也可以告诉您在科学和工程问题中可能发生的事情，并提示您整理数据的最佳方式。没有预定义的 EDA 蓝图。然而，智能使用统计描述和测试以及特征及其相互关系的图形表示可以告诉您如何对数据进行操作。一般来说，首先，使用描述性统计和图表，您检查每个单独的特征，这在统计学中被称为单变量方法。然后，您检查特征之间的关系，这被称为双变量方法。最后，您尝试使用多元技术和降维技术，如
    t-SNE 和 UMAP，来一瞥所有特征。
- en: 'In this phase, just after assembling your data into a tabular matrix, EDA can
    do the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，在将您的数据组装成表格矩阵之后，EDA 可以执行以下操作：
- en: Inform you about the characteristics of the data, such as numeric, ordinal,
    high/low categorical, and date features.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通知您有关数据特征的信息，例如数值、有序、高/低类别和日期特征。
- en: Give you an idea of the values of each feature and how they are distributed.
    This is especially useful when working with neural networks, where value scales
    are important.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给您每个特征的值以及它们如何分布的印象。这在与神经网络一起工作时特别有用，因为值尺度很重要。
- en: Tell you if there are missing values because neural networks and some machine
    learning algorithms are deemed to fail in the presence of missing data.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 告诉您是否存在缺失值，因为神经网络和一些机器学习算法被认为在存在缺失数据的情况下会失败。
- en: Locate outliers and clusters of values due to errors and mistakes in extraction
    and assembling.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定位由于提取和组装中的错误和错误导致的异常值和值簇。
- en: Spot rare categories that can be eliminated or aggregated.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找可以消除或合并的罕见类别。
- en: 'We will show you how to achieve such explorations using simple pandas commands
    since we deem an EDA process fully guided by an analyst’s reasoning more effective
    than an automated one. However, you can also use automatic data exploration tools
    if the number of features to explore is vast and you need to save time. Later,
    you can integrate the automatic EDA with specific and focused custom data explorations.
    Among the open-source packages for automatic EDA, there are a few notable ones
    that we would like to suggest you try, and all of them are valid solutions that
    are easy to learn and use:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向您展示如何使用简单的 pandas 命令来实现这种探索，因为我们认为由分析师的推理完全指导的 EDA 流程比自动化的流程更有效。然而，如果您需要探索大量特征并节省时间，您也可以使用自动数据探索工具。稍后，您可以集成自动
    EDA 与特定和专注的定制数据探索。在自动 EDA 的开源软件包中，有一些值得推荐的，我们希望您尝试，它们都是易于学习和使用的有效解决方案：
- en: AutoViz ([https://github.com/AutoViML/AutoViz](https://github.com/AutoViML/AutoViz))
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoViz ([https://github.com/AutoViML/AutoViz](https://github.com/AutoViML/AutoViz))
- en: Sweetviz ([https://github.com/fbdesignpro/sweetviz](https://github.com/fbdesignpro/sweetviz))
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sweetviz ([https://github.com/fbdesignpro/sweetviz](https://github.com/fbdesignpro/sweetviz))
- en: Pandas Profiling ([https://mng.bz/DMxw](https://mng.bz/DMxw))
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas Profiling ([https://mng.bz/DMxw](https://mng.bz/DMxw))
- en: In the following chapters, we will reprise EDA to complete the panorama with
    the further explorations you may need depending on the techniques (machine learning
    or deep learning) you want to apply to your data for predictive purposes.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将重新审视 EDA，以进一步探索您可能需要的全景，这取决于您想要应用于数据以进行预测目的的技术（机器学习或深度学习）。
- en: 2.4.1  Loading the Auto MPG example dataset
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1  加载 Auto MPG 示例数据集
- en: Now, as an example of how a simple EDA can hint at ideas and solutions for further
    processing your data and taking necessary remediation steps when adapting it to
    the model you want to use, we can go for the Auto MPG Data Set, a dataset freely
    available on the UCI Machine Learning repository ([https://archive.ics.uci.edu/ml/datasets/auto+mpg](https://archive.ics.uci.edu/ml/datasets/auto+mpg)).
    The dataset, assembled by Ernesto Ramos and David Donoho, originates from the
    StatLib library maintained at Carnegie Mellon University. Previously, it was featured
    in the 1983 American Statistical Association Exposition ([https://mng.bz/lYa8](https://mng.bz/lYa8))
    and the works of Ross Quinlan. Ross Quinlan is a major contributor to the development
    of the decision tree algorithm and the creator of the C4.5 ID3 algorithms. He
    quotes this dataset in his 1993 paper “Combining Instance-Based and Model-Based
    Learning” ([https://mng.bz/BXx8](https://mng.bz/BXx8)), a true milestone paper
    on regression problems in machine learning. For our purposes, it is a simple,
    manageable example because of its mixed set of features and some missing data
    to be handled in the mpg and horsepower features.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，作为一个简单的EDA如何提示进一步处理您的数据以及适应您想要使用的模型时采取必要的补救步骤的例子，我们可以选择Auto MPG数据集，这是一个在UCI机器学习存储库上免费可用的数据集([https://archive.ics.uci.edu/ml/datasets/auto+mpg](https://archive.ics.uci.edu/ml/datasets/auto+mpg))。该数据集由Ernesto
    Ramos和David Donoho汇编，源自卡内基梅隆大学维护的StatLib库。之前，它曾在1983年美国统计协会展览([https://mng.bz/lYa8](https://mng.bz/lYa8))和Ross
    Quinlan的作品中展出。Ross Quinlan是决策树算法发展的主要贡献者，也是C4.5 ID3算法的创造者。他在1993年的论文“结合实例学习和基于模型学习”([https://mng.bz/BXx8](https://mng.bz/BXx8))中引用了此数据集，这是一篇关于机器学习中回归问题的真正里程碑论文。对于我们来说，由于它具有混合的特征集和一些需要处理在每加仑油耗和马力特征中的缺失数据，它是一个简单、易于管理的例子。
- en: 'The following is a list of the available features in the dataset:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在数据集中可用的功能列表：
- en: 'mpg: continuous'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每加仑油耗：连续
- en: 'cylinders: multivalued discrete'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 气缸数：多值离散
- en: 'displacement: continuous'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排量：连续
- en: 'horsepower: continuous'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马力：连续
- en: 'weight: continuous'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重量：连续
- en: 'acceleration: continuous'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速：连续
- en: 'model year: multivalued discrete'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型年份：多值离散
- en: 'origin: multivalued discrete'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原产地：多值离散
- en: 'car name: string (unique for each example, it can be used as an index in a
    pandas DataFrame)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车辆名称：字符串（每个示例都是唯一的，可以用作pandas DataFrame中的索引）
- en: In our example, the list is short, but the available features can last dozens
    of pages in most complex tabular datasets.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，列表很短，但在大多数复杂的表格数据集中，可用的功能可能长达数十页。
- en: To upload our example dataset, we can refer to the code in the following listing
    that will connect to the UCI repository, and it will dump the data into a pandas
    DataFrame.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 要上传我们的示例数据集，我们可以参考以下列表中的代码，该代码将连接到UCI存储库，并将数据导入pandas DataFrame。
- en: Listing 2.7 Download of Auto MPG Data Set from UCI repository
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.7 从UCI存储库下载Auto MPG数据集
- en: '[PRE13]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① StringIO reads and writes an in-memory string buffer.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ① StringIO读取和写入内存中的字符串缓冲区。
- en: ② Requests is an HTTP library that can help you recover data from the web.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ② Requests是一个HTTP库，可以帮助您从网络中恢复数据。
- en: ③ Derived from the documentation on the UCI machine learning repository
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 该文档来源于UCI机器学习存储库的文档
- en: ④ Fixed-width data requires providing each feature with its start and end position
    in the input.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 固定宽度数据需要为每个特征提供其在输入中的起始和结束位置。
- en: ⑤ The dataset is read from the web by requests.get() and then turned into a
    string buffer.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 数据集通过requests.get()从网络读取，然后转换为字符串缓冲区。
- en: ⑥ pd.read_fwf reads a table of fixed-width formatted lines into a DataFrame.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ pd.read_fwf将固定宽度的格式化行表读入DataFrame。
- en: 'The Auto MPG data set is stored in a fixed-width text file, where values are
    on the same line, separated by a certain number of spaces or a tab. The return
    carriage at the end of the row signals the end of an example. The following is
    a sample of the text file that the request library retries for us from the UCI
    Machine Learning repository:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Auto MPG数据集存储在一个固定宽度的文本文件中，其中值位于同一行，由一定数量的空格或制表符分隔。行尾的回车符表示示例的结束。以下是从UCI机器学习存储库中请求库为我们重试的文本文件的样本：
- en: '[PRE14]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This explains why we must use a fixed-width reader such as pandas `read_fwf`
    and specify the character position where we expect each value to start and end.
    More commonly, you can find other datasets organized as CSV files. If the same
    dataset had been stored in CSV format, it would look like the following:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了为什么我们必须使用固定宽度的读取器，如 pandas `read_fwf`，并指定我们期望每个值开始和结束的字符位置。更常见的是，你可以找到其他以
    CSV 文件组织的数据集。如果相同的数据集以 CSV 格式存储，它将看起来像以下这样：
- en: '[PRE15]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In CSV files, the values of the features are not placed in fixed positions in
    rows; they vary in position but not in order of sequence, and you find them separated
    because of a special character denominated as a separator. In our example, it
    is the comma. Please also notice that textual data, such as strings or dates,
    is often delimited between quotes to avoid confusion if the text contains the
    separator.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CSV 文件中，特征的值不是放在行的固定位置；它们的位置变化，但顺序不变，你之所以能找到它们，是因为有一个称为分隔符的特殊字符。在我们的例子中，它是逗号。请注意，文本数据，如字符串或日期，通常用引号分隔，以避免文本包含分隔符时的混淆。
- en: In such a case, you have to use the pandas `read_csv` reader, which can handle
    CSV files. Depending on the situation, your data may be stored as a JSON file
    or an XML file, but there are readers also for such formats in pandas (see table
    2.2).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你必须使用 pandas 的 `read_csv` 读取器，它可以处理 CSV 文件。根据情况，你的数据可能存储为 JSON 文件或 XML
    文件，但在 pandas 中也有针对这些格式的读取器（见表 2.2）。
- en: Table 2.2 Common data reading methods in pandas
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2 pandas 中常见的数据读取方法
- en: '| Function | Description |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `read_csv`  | Loads delimited data from a file, URL, or file-like objectUses
    a comma as the default delimiter |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| `read_csv`  | 从文件、URL 或类似文件的对象中加载分隔数据，默认分隔符为逗号 |'
- en: '| `read_json`  | Reads data from a JSON string representation |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| `read_json`  | 从 JSON 字符串表示形式中读取数据 |'
- en: '| `read_xml`  | Imports shallow XML documents as a DataFrame |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| `read_xml`  | 将浅 XML 文档导入为 DataFrame |'
- en: '| `read_html`  | Reads all tables found in the given HTML document |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| `read_html`  | 读取给定 HTML 文档中找到的所有表格 |'
- en: '| `read_excel`  | Reads tabular data from an Excel XLS or XLSX file |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| `read_excel`  | 从 Excel XLS 或 XLSX 文件中读取表格数据 |'
- en: 2.4.2  Examining labels, values, distributions
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2  检查标签、值、分布
- en: 'All you have to do is understand what kind of data you are reading from the
    internet or your local disk and use the appropriate readers among the ones offered
    by pandas: each will be automatically handled by the proper function.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做的就是理解你正在从互联网或本地磁盘读取什么类型的数据，并使用 pandas 提供的适当读取器：每个都会由适当的函数自动处理。
- en: 'As we already have a databook for the dataset (just read “attribute information”
    at [https://archive.ics.uci.edu/ml/datasets/Auto%2BMPG](https://archive.ics.uci.edu/ml/datasets/Auto%2BMPG)),
    we can immediately take note in specific variables about what features are numeric,
    ordinal, and categorical:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经有数据集的数据手册（只需阅读 [https://archive.ics.uci.edu/ml/datasets/Auto%2BMPG](https://archive.ics.uci.edu/ml/datasets/Auto%2BMPG)
    上的“属性信息”），我们可以在特定变量中立即注意哪些特征是数值的、序数的和分类的：
- en: '[PRE16]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In different cases with other datasets, you have to discover such information
    through your data exploration. The variables will have to be populated by your
    discoveries. Hence, if we do not know the characteristics of our features, we
    should discover them through first data exploration, which can be quickly accomplished
    by a few manual pandas commands followed by some deductions. For instance, we
    could ask for a sample of the first rows and immediately get an idea of the data
    we are dealing with. We get just the top five rows using the `.head(n)` method:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他数据集的不同情况下，你必须通过数据探索来发现此类信息。变量将由你的发现来填充。因此，如果我们不知道特征的特征，我们应该通过首先进行数据探索来发现它们，这可以通过一些手动
    pandas 命令和一些推理快速完成。例如，我们可以请求前几行的样本，并立即了解我们正在处理的数据。我们使用 `.head(n)` 方法只获取前五行：
- en: '[PRE17]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The results are shown in figure 2.5.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在图 2.5 中。
- en: '![](../Images/CH02_F05_Ryan2.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F05_Ryan2.png)'
- en: Figure 2.5 Results from the `data.head(5)` command
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 `data.head(5)` 命令的结果
- en: 'In this example, many features are floats without decimal parts, which hints
    at them being integers. Integers may also point out a numeric, ordinal, or categorical
    feature. Requiring the number of distinct values for each feature is the second
    step in our example:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，许多特征都是没有小数部分的浮点数，这表明它们可能是整数。整数也可能表示数值、序数或分类特征。在我们的例子中，要求每个特征的唯一值数量是第二步：
- en: '[PRE18]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The returned results are
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的结果是
- en: '[PRE19]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With few distinct numbers, cylinders and origin features can be treated as categorical
    variables. Hence, they could be both embedded, a procedure that transforms them
    into continuous numerical features, or transformed into binaries for each value
    by one-hot encoding. Domain knowledge can help us classify cylinders as an ordinal
    feature since cylinders may have different volumes in different car models, and,
    generally, having more cylinders proportionally corresponds to more engine power.
    Similarly, the feature relative to years may also be dealt with as an ordinal
    representing time progress.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 使用少数几个不同的数字，圆柱体和原点特征可以被当作分类变量处理。因此，它们可以被嵌入，这是一个将它们转换为连续数值特征的过程，或者通过独热编码将每个值转换为二进制。领域知识可以帮助我们将圆柱体分类为有序特征，因为不同车型中的圆柱体可能有不同的体积，而且通常来说，圆柱体数量越多，发动机功率也越大。同样，与年份相关的特征也可以被视为表示时间进程的有序特征。
- en: 'Another quick explorative check, based on the examination of the standard deviation
    of each of your numeric features, could help you identify your data further and
    even select it:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个基于检查每个数值特征标准差的快速探索性检查，可以帮助你进一步了解你的数据，甚至选择它：
- en: '[PRE20]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The returned results are
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的结果是
- en: '[PRE21]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'By doing so, you can be aware of constant or quasi-constant features in your
    data. Excluding missing values, if a feature is set to zero or too low in variance,
    it can be safely excluded from your data because it probably won’t bring any tangible
    advantage to process it further. However, suppose missing values are present in
    good quantity. In that case, you may want to create an indicator variable to keep
    track of the missing patterns in that feature, as they could be predictive. Consider
    checking with a command the number of missing cases:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，你可以了解数据中的常数或准常数特征。排除缺失值后，如果一个特征被设置为零或方差太低，可以安全地从数据中排除，因为它可能不会带来任何实质性的优势来进一步处理。然而，如果存在大量缺失值，你可能想创建一个指示变量来跟踪该特征的缺失模式，因为它们可能是预测性的。考虑使用命令检查缺失案例的数量：
- en: '[PRE22]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `.isna()` method will return a boolean telling us if any feature sample
    is missing. We can count how many missing samples there are by summing the number
    of missing samples in a feature. A True value equates 1 and a False 0\. The obtained
    results are
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '`.isna()` 方法将返回一个布尔值，告诉我们是否有任何特征样本缺失。我们可以通过计算一个特征中缺失样本的数量来统计缺失样本的数量。True 值等于
    1，False 等于 0。得到的结果是'
- en: '[PRE23]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `.isna` method will help you keep track of the NaN values in your features.
    As discussed, features with a large number of missing numbers may conceal some
    interesting predictive pattern and may be turned into missing binary indicator
    variables, You can use the MissingIndicator in Scikit-learn for this purpose:
    [https://mng.bz/dXoO](https://mng.bz/dXoO). Additionally, depending on your predictive
    algorithm, you always have to deal with missing values, whether there are many
    or just a few handfuls. If you use a deep learning solution, you must impute the
    missing values with a numeric value. Usually, you take the mean, median, or mode
    as a replacement for missing values. Still, more sophisticated treatments are
    also based on iterative estimations of the best value to use as a substitute.
    An example is the IterativeImputer in Scikit-learn: [https://mng.bz/rKaD](https://mng.bz/rKaD).'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`.isna` 方法可以帮助你跟踪特征中的 NaN 值。正如讨论的那样，具有大量缺失数字的特征可能隐藏一些有趣的预测模式，并且可能被转换为缺失的二进制指示变量。你可以使用
    Scikit-learn 中的 MissingIndicator 来实现这个目的：[https://mng.bz/dXoO](https://mng.bz/dXoO)。此外，根据你的预测算法，你总是需要处理缺失值，无论是大量还是只有少数几个。如果你使用深度学习解决方案，你必须用数值来填充缺失值。通常，你使用平均值、中位数或众数作为缺失值的替代。然而，更复杂的方法也基于对最佳替代值的迭代估计。Scikit-learn
    中的 IterativeImputer 就是这样一个例子：[https://mng.bz/rKaD](https://mng.bz/rKaD)。'
- en: 'It is helpful to remember that imputing the missing values with the zero value
    is a good strategy for neural networks and generalized linear models when your
    numeric features are standardized when you remove the mean and divide by the standard
    deviation: in such a case, the zero value corresponds to the mean for all the
    numeric features. The same goes for most machine learning algorithms but the approach
    is differentfor the most advanced gradient boosting implementations, such as XGBoost
    and LightGBM. Such algorithms can appropriately treat missing data without any
    further intervention on your side.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，用零值填充缺失值是当你的数值特征标准化（移除均值并除以标准差）时，对于神经网络和广义线性模型的一种好策略：在这种情况下，零值对应于所有数值特征的均值。对于大多数机器学习算法来说也是如此，但对于最先进的梯度提升实现（如XGBoost和LightGBM）来说，方法不同。这样的算法可以适当地处理缺失数据，而无需你进行任何进一步的操作。
- en: 'Sometimes conventional numbers (such as –999) are used in tabular datasets.
    If you know that a certain value points out to a missing value, you have to modify
    your command to consider such information:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 有时在表格数据集中使用传统的数字（如-999）。如果你知道某个值表示缺失值，你必须修改你的命令来考虑这种信息：
- en: '[PRE24]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the previous code snippet, you check all the numeric features against the
    value –999, which is a marker for missing data. Basically, any number could be
    used as a marker, and you have to know in advance what is used in your data to
    set a missing value. Though a number, it won’t cause any error when training your
    model, but it will seriously mislead its learning. Hence, the missing marker should
    always be addressed as a NaN missing value to prevent potential misinterpretation
    by deep learning models and certain machine learning algorithms. Tree-based models,
    like gradient boosting or random forest models, are better equipped to handle
    such situations, especially when the marker is positioned at an extreme end of
    the feature’s data distribution.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，你将所有数值特征与值-999进行比较，-999是一个表示缺失数据的标记。基本上，任何数字都可以用作标记，你必须事先知道你的数据中使用了什么来设置缺失值。虽然是一个数字，但在训练模型时不会引起任何错误，但它会严重误导其学习。因此，缺失标记应该始终被视为NaN缺失值，以防止深度学习模型和某些机器学习算法的潜在误解。基于树的模型，如梯度提升或随机森林模型，更能处理这种情况，尤其是在标记位于特征数据分布的极端端时。
- en: 'Having checked about missing data and standard deviations, you now start checking
    the distribution of your features to spot other useful information that can guide
    your treatment of the dataset:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查了缺失数据和标准差之后，你现在开始检查特征的分布，以发现其他有用的信息，这些信息可以指导你对数据集的处理：
- en: '[PRE25]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Figure 2.6 shows the outputs of the describe method.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6显示了describe方法的输出。
- en: '![](../Images/CH02_F06_Ryan2b.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F06_Ryan2b.png)'
- en: Figure 2.6 Results from the `data.describe()` command
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 `data.describe()`命令的结果
- en: The pandas describe method allows you to represent basic descriptive statistics
    for all your numeric features. Note that missing data is ignored. After checking
    the missing values and the variance or the standard deviation, your attention
    should focus on the minimum and maximum values with respect to the mean. Too large
    or too small values should draw your attention to outliers (extreme values in
    a data distribution) or too skewed distributions. Outliers and skewed distributions
    are not uncommon in data analysis, and sometimes they have to be removed. Sometimes,
    they should be taken at face value without any corrective action, depending on
    the motivation of your analysis. In this phase, outliers and skewed distributions
    may hint at problems in collecting and assembling your data, and corrective actions
    imply removing or correcting them. For instance, an outlying value is due to errors
    in the data. Also, stacked data recorded with different methodologies may have
    generated a skewed distribution. For instance, in a table, your measures are in
    meters, and they are instead expressed in centimeters in another one.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: pandas的describe方法允许你表示所有数值特征的基礎描述性統計。请注意，缺失数据被忽略。在检查缺失值和方差或标准差后，你应该关注与平均值相关的最小值和最大值。过大或过小的值应该引起你对异常值（数据分布中的极端值）或过于倾斜的分布的注意。异常值和倾斜分布在数据分析中并不罕见，有时它们需要被移除。有时，它们应该被当作原始值，而不采取任何纠正措施，这取决于你分析的目的。在这个阶段，异常值和倾斜分布可能表明数据收集和组装中存在问题，纠正措施意味着移除或纠正它们。例如，一个异常值是由于数据错误造成的。此外，使用不同方法记录的堆叠数据可能会产生倾斜分布。例如，在一个表格中，你的测量值是以米为单位的，而在另一个表格中，它们被表示为厘米。
- en: 'The reasons behind all such errors in data vary widely, and it is advised to
    check your data closely with descriptive statistics and charts and then figure
    out a corrective action to remove the errors. Such errors are not limited to numeric
    features but can be easily found in categorical ones. Let’s now examine the only
    string feature available, the `car_name` one, by splitting its elements:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中所有此类错误的原因各不相同，建议你使用描述性统计和图表仔细检查你的数据，然后找出纠正错误的方法。此类错误不仅限于数值特征，但在分类特征中也很容易找到。现在，让我们通过分割其元素来检查可用的唯一字符串特征，即`car_name`：
- en: '[PRE26]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The lambda function in the apply function will split the instances of `car_name`
    into single words. Since the result will be a series of lists, using the `.explore()`
    method, we unroll all the lists into a single feature. By counting the single
    values in this new feature, we get the frequency results for each word:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在`apply`函数中的lambda函数会将`car_name`实例分割成单个单词。由于结果将是一系列列表，使用`.explore()`方法，我们将所有列表展开成一个单一的特征。通过计算这个新特征中的单个值，我们得到每个单词的频率结果：
- en: '[PRE27]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'It looks fine at first sight, with most instances containing a brand name and
    some information on the type of car. However, among the most frequent labels,
    you notice some incongruent information, such as `"(sw)"`, which stands for sport
    wagon, and `custom`, which refers to the customized accessories in the car. At
    a closer inspection, you finally notice that, even in this curated dataset, there
    are problems since many brands are misspelled:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 初看似乎没问题，因为大多数实例包含品牌名称和一些关于车型类型的信息。然而，在最常见的标签中，你注意到一些不一致的信息，例如`"(sw)"`，它代表运动旅行车，而`custom`则指代汽车中的定制配件。在更仔细的检查中，你最终注意到，即使在经过筛选的数据集中，也存在问题，因为许多品牌被拼写错误：
- en: '[PRE28]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The returned results are
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的结果是
- en: '[PRE29]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In this data slice containing the words composing the categorical feature,
    we notice how some brands are misspelled (e.g., Toyota and Toyouta, Volkswagen
    and Vokswagen but also VW as a shortening). Misspells will split the examples
    related to a true category (Volkswagen and Toyota in this case) into multiple
    categories, putting your model at risk of picking up noisy evidence or discarding
    weaker signals. The same happens with other categorical and ordinal features that
    present erroneous additional categories:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在包含组成分类特征的单词的数据切片中，我们注意到一些品牌被拼写错误（例如，Toyota和Toyouta，Volkswagen和Vokswagen，但还有VW作为缩写）。拼写错误会将与真实类别（在这个例子中是Volkswagen和Toyota）相关的示例分割成多个类别，使你的模型面临收集噪声证据或丢弃较弱信号的风险。同样，其他呈现错误额外类别的分类和有序特征也会发生这种情况：
- en: '[PRE30]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The returned results for data origin are
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 数据来源的返回结果是
- en: '[PRE31]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s operate the same for `model_year`, ordering the results based on the
    year:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对`model_year`也进行相同的操作，根据年份对结果进行排序：
- en: '[PRE32]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Figure 2.7 shows how the ordered value results for `model_year` will appear.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7展示了`model_year`的有序值结果将如何显示。
- en: '![](../Images/CH02_F07_Ryan2b.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F07_Ryan2b.png)'
- en: Figure 2.7 Ordered value counts
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 有序值计数
- en: 'Not all these problems with data taken singularly affect the results much.
    However, their combined presence may affect your model performances in an evident
    way when your dataset is composed of tens and tens of features with minor problems.
    Remediation is simple: just remove the cases with the erroneous category labels
    unless you can reasonably correct them or treat them as missing cases and then
    impute them. Since they require inspection and reasoning on every feature, such
    actions may take considerable time and effort. Still, you are doing EDA precisely
    for this purpose, and later, a model that is more confident and performing in
    predictions will repay your efforts in full. Tabular data has stronger requirements
    on the cleanness of data, especially when applying classical machine learning
    models than generally expected from unstructured data processed by deep learning,
    where errors are sometimes considered useful noise to help avoid overfitting.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有这些单独的数据问题都会对结果产生很大影响。然而，当你的数据集由数十个具有小问题的特征组成时，它们的共同存在可能会以明显的方式影响你的模型性能。补救措施很简单：只需移除具有错误类别标签的案例，除非你可以合理地纠正它们或将它们视为缺失案例并进行插补。由于它们需要对每个特征进行审查和推理，这些行动可能需要相当的时间和精力。尽管如此，你进行EDA正是为了这个目的，而且后来，一个在预测中更有信心且表现更好的模型将完全回报你的努力。与深度学习处理的无结构数据相比，表格数据对数据清洁度的要求更强，尤其是在应用经典机器学习模型时，通常期望的比无结构数据要高，在深度学习中，错误有时被认为是有用的噪声，有助于避免过拟合。
- en: 'Being on the lookout for more errors in data, it is now time, after checking
    on the categorical features, to examine closely the numeric ones too. Here, boxplots
    and histograms replace category counts, and the focus is also widened from single
    features (the so-called statistical univariate approach) to more features together.
    Each value in the features of an example may not appear erroneous per se. Still,
    if all the values are taken together, you may realize that their combination is
    highly unlikely, if not the result of some error. You start examining the distributions
    by boxplots. A boxplot, also called a box and whiskers plot, drafts the key characteristics
    of a distribution using the boundaries of a box: you can see the first quartile
    (Q1) and the third quartile (Q3) as the top and bottom sides of the box (the median,
    Q2, is represented inside the box). Its whiskers represent the farthest points
    in both directions, not exceeding Q3 + 1.5 × IQR and Q1 – 15 × IQR (as a reminder,
    IQR is the difference between Q3 and Q1). Outlying observations beyond such boundaries
    are plotted as separate points, and you can immediately spot if there are any
    present and how many there are. Since a boxplot scales to the unit measures of
    the feature you represent, if you are to compare multiple boxplots with the numeric
    features for their distributions, you need first to standardize them by subtracting
    the mean and divide by their standard deviation:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '在检查了分类特征之后，现在应该检查数值特征，以寻找更多数据错误。在这里，箱线图和直方图取代了类别计数，关注的焦点也从单个特征（所谓的统计单变量方法）扩展到更多特征。一个示例的特征中的每个值本身可能并不错误。然而，如果将这些值综合考虑，你可能会意识到它们的组合非常不可能，如果不是某些错误的结果。你开始通过箱线图检查分布。箱线图，也称为箱线和须线图，使用箱子的边界绘制分布的关键特征：你可以看到第一四分位数（Q1）和第三四分位数（Q3）作为箱子的顶部和底部边缘（中位数，Q2，表示在箱子内部）。它的须线代表两个方向上最远的点，不超过Q3
    + 1.5 × IQR和Q1 – 1.5 × IQR（作为一个提醒，IQR是Q3和Q1之间的差值）。超出这些边界的异常观测值被绘制为单独的点，你可以立即发现是否存在任何异常以及有多少个。由于箱线图缩放到你表示的特征的单位度量，如果你要比较多个箱线图以比较数值特征的分布，你需要首先通过减去平均值并除以标准差来标准化它们： '
- en: '[PRE33]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Figure 2.8 shows the chart output.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8展示了图表输出。
- en: '![](../Images/CH02_F08_Ryan2.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F08_Ryan2.png)'
- en: Figure 2.8 Boxplots of numeric features from the Auto MPG Data Set
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 Auto MPG数据集的数值特征的箱线图
- en: 'The purpose is to determine if the data is wrong because of your collection
    or aggregation procedure. Features with outliers such as mpg, horsepower, and
    acceleration require a closer inspection through a histogram to exclude the absence
    of extraneous or erroneous values:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 目的是确定数据是否因为你的收集或聚合过程出现错误。具有异常值（如mpg、马力和加速度）的特征需要通过直方图进行更仔细的检查，以排除存在无关或错误值的情况：
- en: '[PRE34]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Figure 2.9 shows the resulting 64-bin histogram.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9显示了结果的64个桶直方图。
- en: '![](../Images/CH02_F09_Ryan2.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F09_Ryan2.png)'
- en: Figure 2.9 Histogram of horsepower feature
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：马力的特征直方图
- en: 'As for the horsepower feature, there does not seem to be anything relevant
    except a peculiar peak at around value 150: a concentration of examples due to
    fiscal reasons. In fact, since you have to pay more taxes if you own a car exceeding
    a certain horsepower threshold, designers of many cars’ engines just stayed below
    that threshold to make their cars more marketable.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 对于马力特征，似乎没有其他相关内容，除了在约150值附近的奇特峰值：由于财政原因导致的一些示例的集中。实际上，由于拥有超过一定马力阈值的汽车需要支付更多的税款，许多汽车的发动机设计者只是保持在那个阈值以下，以便使他们的汽车更具市场竞争力。
- en: 'As for the acceleration feature, everything is okay. There are just long tails
    on both sides:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 对于加速度特征，一切正常。只是两侧都有长尾：
- en: '[PRE35]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Figure 2.10 shows the resulting 24-bin histogram.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10显示了结果的24个桶直方图。
- en: '![](../Images/CH02_F10_Ryan2.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F10_Ryan2.png)'
- en: Figure 2.10 Histogram of acceleration feature
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：加速度特征的直方图
- en: 'Histograms, which are bar charts of frequencies of values bins, work for numeric
    variables. For ordinal and categorical variables, a simple bar chart on value
    counts obtains the same information:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图，作为值桶频率的条形图，适用于数值变量。对于有序和分类变量，简单的值计数条形图可以获得相同的信息：
- en: '[PRE36]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Figure 2.11 shows the resulting bar plot.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11显示了结果的条形图。
- en: '![](../Images/CH02_F11_Ryan2.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F11_Ryan2.png)'
- en: Figure 2.11 Bar plot of cylinders feature
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11：气缸特征的条形图
- en: Here, you should look for rarer classes because they can have an error or an
    outlying rare observation that you must decide whether to keep. Domain knowledge
    should help you make these decisions. In our example regarding cylinders, we actually
    should keep both the three-cylinder and five-cylinder classes, even if they are
    rare, because a quick check can reveal that we have few three-cylinder cars—cars
    installing this type of engine are smaller and tend to have much less market share
    than the more common four-cylinder vehicles. In addition, car designers underuse
    the five-cylinder layout, similar to other odd layouts, because it presents costs
    similar to the six-cylinder layouts but implies more complexity in many engineering
    aspects and many more shortcomings in terms of performance.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你应该寻找较少见的类别，因为它们可能存在错误或异常的罕见观测值，你必须决定是否保留。领域知识应该帮助你做出这些决定。在我们的气缸例子中，我们实际上应该保留三缸和五缸类别，即使它们很少见，因为快速检查可以揭示我们有三缸汽车的数量很少——安装这种类型发动机的汽车较小，并且往往比更常见的四缸车辆市场份额要小得多。此外，汽车设计师对五缸布局的使用不足，类似于其他奇数布局，因为它具有与六缸布局相似的成本，但在许多工程方面却意味着更多的复杂性，并且在性能方面存在更多的不足。
- en: 2.4.3  Exploring bivariate and multivariate relationships
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 探索双变量和多变量关系
- en: 'Spending time examining how values are distributed using histograms and bar
    plots will provide information for catching errors that may later affect your
    work. Even if the errors on every feature may have a minimal effect, the sum of
    all errors on all the features you will be using may affect your predictive algorithm
    significantly. Once you are done spotting the problems with single features, it
    is time to check how they relate to each other. How features relate to your target
    and how you can exclude part of them, leaving your results unmodified or even
    improving them, is a topic we will discuss later when discussing feature selection.
    Your priority at this point is avoiding redundant features in your dataset. For
    redundant features, we intend duplicated features or features that are extremely
    similar:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用直方图和条形图来检查值的分布情况，将为你提供捕捉可能影响你工作的错误信息。即使每个特征的错误可能只有最小的影响，但你将使用到的所有特征上的错误总和可能会对你的预测算法产生显著影响。一旦你完成了对单个特征的错误识别，就需要检查它们之间的关系。特征如何与你的目标相关联，以及你如何排除其中的一部分，而不会修改或甚至改善你的结果，这是我们将在讨论特征选择时讨论的话题。目前你的首要任务是避免在数据集中出现冗余特征。对于冗余特征，我们指的是重复的特征或极其相似的特征：
- en: Duplicated features with different names.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同名称的重复特征。
- en: Highly collinear numeric features.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度相关的数值特征。
- en: Similar categorical features, apparently different because of level aggregations
    or because of used labels that are different.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似的分类特征，表面上由于级别聚合或使用的标签不同而显得不同。
- en: Similar numeric and categorical features that are both derived from the same
    source. An example is in finance when you have a probability of default and a
    corresponding default rating, which are usually expressed with alphabetic labels
    like AAA or BB.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似的数值和分类特征，它们都来自同一来源。例如，在金融领域，当你有一个违约概率和相应的违约评级时，这些通常用AAA或BB等字母标签表示。
- en: 'We start figuring out how to spot duplications and high collinearity among
    numeric features. Our favored tool for this investigation is the bivariate correlation:
    the correlation between the features, one by another. When the numeric features’
    bivariate correlations are arranged in a symmetrical matrix, we have a correlation
    matrix, which can be immediately visualized as a chart for spotting collinearity
    if there are not too many features:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始探讨如何发现数值特征中的重复和高共线性。我们用于这项调查的有利工具是二元相关性：特征之间的相关性。当数值特征的二元相关性以对称矩阵的形式排列时，我们就有了相关性矩阵，如果特征不多，可以立即将其可视化为一个图表，用于发现共线性：
- en: '[PRE37]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The resulting plot is shown in figure 2.12.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如图2.12所示。
- en: '![](../Images/CH02_F12_Ryan2.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F12_Ryan2.png)'
- en: Figure 2.12 Correlation plot with values heatmap
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 相关性热图
- en: In our example, we only have a few features involved, and the readability of
    the matrix is high. When the number of features is high, it is better to list
    only the bivariate correlations that exceed a certain threshold. In both cases,
    you must pay attention to correlations that exceed the absolute value of 0.98
    to 0.99\. Also, correlations may be negative, and a negative correlation approaching
    minus one is another case of collinearity. When you have such high correlations
    between features, you must try to understand why and then decide which one to
    drop. Dropping some of the collinear features will reduce your dataset and avoid
    problems later, depending on the feature selection method used or the learning
    algorithm. In fact, with collinearity or multicollinearity, if it involves more
    than two features at once, you may experience problems in your dataset in both
    the convergence of the learning algorithm, resulting in suboptimal results and
    interpretability of the solution.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们只涉及少数几个特征，矩阵的可读性很高。当特征数量较多时，最好只列出超过一定阈值的二元相关性。在两种情况下，都必须注意超过0.98到0.99绝对值的关联性。此外，相关性可能是负的，接近负一的负相关性是另一种共线性的情况。当你有特征之间如此高的相关性时，你必须试图理解原因，然后决定保留哪一个。删除一些共线性的特征将减少你的数据集，并避免使用所用的特征选择方法或学习算法时可能出现的后续问题。实际上，在共线性或多重共线性中，如果涉及两个以上的特征，你可能会在数据集的收敛、导致次优结果以及解决方案的可解释性方面遇到问题。
- en: Collinearity, though associated with correlations, also affects categorical
    features and sometimes has to be found even among categorical and numeric ones.
    In this case, you cannot use correlation. Even if correlation for this purpose
    is quite robust, and even if you are comparing ordinal and binary features, when
    working with categorical features, you have to transform them into numbers using
    a procedure called label encoding that implies assigning arbitrary numbers to
    categories. Since label encoding is based on arbitrary value assignment, you cannot
    even establish the identity of the same encoded categorical feature using correlation
    if your encoding is applied differently to the same feature.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 共线性，尽管与相关性相关，但也影响分类特征，有时甚至需要在分类和数值特征之间找到它。在这种情况下，你不能使用相关性。即使为了这个目的相关性相当稳健，即使你在比较有序和二元特征时，当处理分类特征时，你必须使用称为标签编码的程序将它们转换为数字，该程序意味着为类别分配任意数字。由于标签编码基于任意值分配，如果你的编码应用于同一特征的方式不同，甚至不能使用相关性来建立相同编码的分类特征的同一性。
- en: In similar situations, we can recur to associative measures based on chi-square
    statistics or Cramer’s V measure when correlation is not applicable. Cramer’s
    V is a statistic based on the chi-square value divided by the number of labels
    in the features being compared. This operation normalizes the value recorded on
    a feature, making it comparable across all dataset features. The square root of
    the result ranges from 0 to 1, providing a measure of intensity but not directionality
    of the relationship. Regarding intensity, Cramer’s V values close to 0 indicate
    unrelated features, while those near 1 indicate a high association and collinearity
    among the features.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似情况下，当相关性不适用时，我们可以求助于基于卡方统计或克拉美尔V测量的关联度量。克拉美尔V是一种基于比较特征标签数量的卡方值统计量。此操作将特征上的记录值进行归一化，使其在整个数据集特征中具有可比性。结果的平方根范围从0到1，提供关系强度的度量，但不提供方向性。就强度而言，接近0的克拉美尔V值表示特征之间无关，而接近1的值表示特征之间存在高度关联和共线性。
- en: Cramer’s V can also be applied by comparing a categorical feature and a numeric
    one if you previously discretized the numeric one into a categorical one using
    a transformation based on deciles, for instance. Listing 2.8 is a comparison between
    two features from our example dataset. In the example, we first create a function
    to compute `cramerV` from the chi-square score from a table. We apply it to a
    comparison between a categorical and a numeric feature after having discretized
    the numeric feature using deciles.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您之前使用基于十分位数的转换将数值特征离散化为分类特征，则可以应用克拉美尔V，通过比较分类特征和数值特征。列表2.8是我们示例数据集中两个特征的比较。在示例中，我们首先创建一个函数来从表格的卡方分数计算`cramerV`。在将数值特征使用十分位数离散化后，我们将其应用于分类特征和数值特征的比较。
- en: Listing 2.8 Using Cramer’s V to detect association
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.8 使用克拉美尔V检测关联
- en: '[PRE38]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ① Imports from Scipy the function for computing the chi-square test of independence
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从Scipy导入计算卡方独立性检验的函数
- en: ② Prepares a function for calculating Cramer’s V having as input the score chi-square
    test of independence and the table it has been derived from
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ② 准备一个函数，该函数以卡方独立性检验的分数和由此得到的表格作为输入来计算克拉美尔V
- en: ③ Counts all the elements in the table
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算表格中的所有元素数量
- en: ④ Figures out the minimum dimension of the table
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 确定表格的最小维度
- en: ⑤ The result is the root of the chi-square score divided by the number of elements
    in the table and its minimum dimension.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 结果是卡方分数除以表格中的元素数量及其最小维度
- en: ⑥ Transforms the numeric feature mpg into deciles so they can fit into a table
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将数值特征mpg转换为十分位数，以便它们可以适应表格
- en: ⑦ Creates the table between the model_year and mpg deciles to estimate their
    association
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 在模型年份和mpg十分位数之间创建表格以估计它们之间的关联
- en: ⑧ Returns the score, the p-value, the degrees of freedom, and the expected table
    based on marginal probabilities
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 返回基于边缘概率的分数、p值、自由度和期望表格
- en: ⑨ Prints the resulting Cramer’s V
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 打印结果中的克拉美尔V
- en: In the code, by providing the chi-square score and the table itself, we call
    the Cramer-V function to get a 0.855 value for Cramer’s V comparing MPGs and the
    year of the model. Cramer’s V is a reciprocal measure used to identify features
    with a similar role in prediction. Unlike directional measures, Cramer’s V is
    insensitive to feature swapping. This property allows for identifying highly correlated
    features that can be removed from the analysis. A very high positive or negative
    correlation between features may indicate redundancy, and one of the two features
    can be dropped.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，通过提供卡方分数和表格本身，我们调用克拉美尔V函数，得到比较MPGs和模型年份的克拉美尔V值为0.855。克拉美尔V是一种互反度量，用于识别在预测中具有相似角色的特征。与方向性度量不同，克拉美尔V对特征交换不敏感。这一特性允许识别可以从中移除的分析的高度相关特征。特征之间非常高的正或负相关性可能表明冗余，其中两个特征之一可以被删除。
- en: 'As a last step after your EDA has dealt with univariate and bivariate explorations,
    as we pointed out, you need to examine your dataset from a multivariate point
    of view: this will help you detect the presence of any chunk of data that doesn’t
    fit in the data. Again, you are performing such an examination not to find better
    ways to fit your model precisely but as a preliminary operation to validate your
    tabular data as fit for the problem you want to represent. Multivariate approaches
    require all features to be numeric, as previously seen for Cramer’s V; you can
    discretize numeric features for this purpose and operate projecting all your data
    to lower dimensionality, no matter how complex it is, and thus you can visualize
    it and easily spot anomalies.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的探索性数据分析（EDA）处理了单变量和双变量探索之后，正如我们指出的，你需要从多变量的角度检查你的数据集：这将帮助你检测是否存在任何不适合数据的数据块。再次强调，你进行这种检查不是为了找到更精确地拟合你的模型的方法，而是作为一个初步操作来验证你的表格数据是否适合你想要表示的问题。多变量方法要求所有特征都是数值型的，正如之前在Cramer的V中看到的；你可以为了这个目的对数值特征进行离散化，并将所有数据投影到低维，无论其多么复杂，从而可以可视化它并轻松地发现异常值。
- en: The process consists of reducing your data to a few comprehensive summary features
    of dimensions and plotting them on a chart to visually spot patterns and isolated
    clusters of points that could be anomalies. Common multivariate approaches for
    obtaining a lower dimensionality projection are
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程包括将你的数据简化为几个综合性的总结特征维度，并在图表上绘制它们，以直观地发现模式和孤立的数据点集群，这些可能是异常值。常见的用于获得低维投影的多变量方法有：
- en: 'Principal component analysis (PCA) and singular value decomposition (SVD):
    [https://mng.bz/VVl0](https://mng.bz/VVl0)'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '主成分分析（PCA）和奇异值分解（SVD）: [https://mng.bz/VVl0](https://mng.bz/VVl0)'
- en: 'T-distributed stochastic neighbor embedding (t-SNE): [https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'T分布随机邻域嵌入（t-SNE）: [https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/)'
- en: 'Uniform manifold approximation and projection (UMAP): [https://github.com/lmcinnes/umap](https://github.com/lmcinnes/umap)'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '均匀流形近似和投影（UMAP）: [https://github.com/lmcinnes/umap](https://github.com/lmcinnes/umap)'
- en: PCA and SVD have their roots in statistical analysis, and PCA has a long history
    of being applied for EDA purposes. Both are, however, approaches based on linear
    combinations of features (your reduced summary dimensions are a weighted summation
    of your data) and, thus, are only sometimes suitable to catch nonlinear patterns
    that are frequently found in real-world data. The more recent t-SNE and UMAP are
    methods that can reduce the dimensionality of data exceptionally well, allowing
    you to chart a reliable representation of your data where original data characteristics
    are maintained. There are some caveats, though, since tweaking the hyper-parameters
    of both methods may lead to entirely different projections from the same data.
    This is due to the loss of information in the data when passing from multiple
    features to a few ones that may result in different resulting plots—some more,
    some less representative of the data itself. In addition, the resulting plots
    may only sometimes be easily interpretable, especially if there are many data
    points or if the data is highly clustered. That said, t-SNE and UMAP are valuable
    tools for data exploration and visualization, as they can help identify patterns
    and clusters within complex datasets that may not be readily visible through other
    methods. Of course, a core principle of EDA does not uniquely rely solely on them
    but compares their outputs with other multivariate, bivariate, and univariate
    methods, as shown.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: PCA和SVD的根源在于统计分析，PCA在用于EDA目的方面有着悠久的历史。然而，它们都是基于特征线性组合的方法（你的减少的总结维度是你的数据的加权求和），因此，它们只在某些情况下适合捕捉在现实世界数据中经常发现的非线性模式。较新的t-SNE和UMAP是能够非常有效地降低数据维度的方法，允许你绘制出可靠的数据表示，其中原始数据特征得到保留。尽管如此，也有一些注意事项，因为调整这两种方法的超参数可能会导致从相同数据中产生完全不同的投影。这是由于数据从多个特征传递到几个可能产生不同结果的特征时信息丢失所导致的——一些更，一些更少地代表数据本身。此外，生成的图表可能只在某些情况下容易解释，特别是当数据点很多或数据高度聚集时。话虽如此，t-SNE和UMAP是数据探索和可视化的宝贵工具，因为它们可以帮助识别复杂数据集中通过其他方法可能不易察觉的模式和集群。当然，EDA的一个核心原则并不唯一地依赖于它们，而是将它们的输出与其他多变量、双变量和单变量方法进行比较，如图所示。
- en: Before starting working with these methods, read articles such as “How to t-SNE
    Effectively” ([https://distill.pub/2016/misread-tsne/](https://distill.pub/2016/misread-tsne/))
    or “Understanding UMAP’’ ([https://mng.bz/AQxK](https://mng.bz/AQxK)), which will
    provide you with additional confidence on using the methods with the appropriate
    precautions. Another caveat is that both approaches are computationally intensive,
    and it may take quite a long time to obtain a reduction from a large and complex
    dataset. Recently, however, NVIDIA has furthermore developed its RAPIDS suite
    based on CUDA and GPU technology ([https://developer.nvidia.com/rapids](https://developer.nvidia.com/rapids)),
    which can dramatically cut the time necessary before getting results from both
    UMAP and t-SNE, making them even more effective for intensive EDA explorations.
    Listing 2.9 shows the code to analyze our example dataset using the t-SNE implementation
    in the Scikit-learn package. We will present the NVIDIA RAPIDS implementations,
    as well as their other tools for processing tabular data, later in the book.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用这些方法之前，阅读像“如何有效地使用 t-SNE”（[https://distill.pub/2016/misread-tsne/](https://distill.pub/2016/misread-tsne/))）或“理解
    UMAP”（[https://mng.bz/AQxK](https://mng.bz/AQxK)）这样的文章，这些文章将为你提供在使用方法时采取适当预防措施所需的额外信心。另一个注意事项是，这两种方法都是计算密集型的，从大型和复杂的数据集中获得降维可能需要相当长的时间。然而，最近，NVIDIA
    还基于 CUDA 和 GPU 技术开发了其 RAPIDS 套件（[https://developer.nvidia.com/rapids](https://developer.nvidia.com/rapids)），这可以显著减少从
    UMAP 和 t-SNE 获得结果所需的时间，使它们在密集的 EDA 探索中更加有效。列表 2.9 展示了使用 Scikit-learn 包中的 t-SNE
    实现分析示例数据集的代码。我们将在本书的后面部分介绍 NVIDIA RAPIDS 的实现，以及它们用于处理表格数据的其他工具。
- en: Listing 2.9 Plotting a t-SNE low-dimensional projection of a dataset
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.9 绘制数据集的 t-SNE 低维投影
- en: '[PRE39]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ① Imports the t-SNE class available in Scikit-learn
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入 Scikit-learn 中可用的 t-SNE 类
- en: ② Imports pyplot from matplotlib for chart plotting
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ② 导入 matplotlib 的 pyplot 用于图表绘制
- en: ③ t-SNE is set to project results in two dimensions; the other parameters are
    kept at their default settings.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将 t-SNE 设置为在二维投影结果；其他参数保持默认设置。
- en: ④ Only numeric and ordinal features are used; missing values are replaced with
    the mean because the t-SNE class requires a complete input data matrix.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 仅使用数值和有序特征；缺失值用均值替换，因为 t-SNE 类需要完整的输入数据矩阵。
- en: ⑤ By fit_transform, the projection is created based on the provided data and
    applied to the data itself.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 通过 fit_transform，投影基于提供的数据创建，并应用于数据本身。
- en: ⑥ The t-SNE transformed data is plotted as a bidimensional scatterplot.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将 t-SNE 变换后的数据绘制为二维散点图。
- en: Figure 2.13 shows the resulting plot of the t-SNE transformed data.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13 展示了 t-SNE 变换后的数据结果图。
- en: '![](../Images/CH02_F13_Ryan2.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F13_Ryan2.png)'
- en: Figure 2.13 Points distribution resulting from t-SNE two-dimensional transformation
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13 t-SNE 二维变换后的点分布
- en: 'The code snippet works only on the numeric features that, after having all
    the missing values replaced by the mean (neither t-SNE nor UMAP can work with
    missing data), squeeze the data into a two-dimensional representation that a scatter
    plot can plot. In this dataset, the results appear to be very regular and do not
    present much of a surprise if we think this is a selection of curated examples
    collected for a purpose: all the data points have ended up being aligned in a
    curvilinear cloud, offering an overall impression of examples chosen progressively
    and regularly based on specific criteria.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段仅适用于数值特征，在所有缺失值被均值替换后（t-SNE 和 UMAP 都不能处理缺失数据），将数据压缩成散点图可以绘制的二维表示。在这个数据集中，结果看起来非常规律，如果我们认为这是为特定目的收集的精选示例集合，那么就不会有太多惊喜：所有数据点最终都排列在一个曲线云中，整体给人一种根据特定标准逐步、规律选择示例的印象。
- en: This concludes our exploration of the Auto MPG Data Set. The next chapter will
    discuss machine learning algorithms and explore fundamental classical models routinely
    applied to tabular datasets. By proceeding with examples, we will point out each
    algorithm’s strengths and weaknesses regarding the data and features you may encounter
    in a project.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们对 Auto MPG 数据集的探索结束。下一章将讨论机器学习算法，并探讨通常应用于表格数据集的基本经典模型。通过举例，我们将指出每个算法在数据以及你可能在项目中遇到的特征方面的优势和劣势。
- en: Summary
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Despite wild differences in domains and organizations, common characteristics
    in tabular datasets make it possible to outline the best data handling and modeling
    practices.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管在领域和组织之间存在巨大差异，但表格数据集中的共同特征使得可以概述最佳的数据处理和建模实践。
- en: Rows in a tabular dataset relate the units represented, and there are limitations
    and opportunities to be noticed when they are non-IID. Non-IID data can affect
    commonly used procedures in data science, such as bootstrapping, subsampling,
    and cross-validation.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格数据集中的行关联所表示的单位，当它们不是独立同分布（non-IID）时，会有局限性和机会需要注意。非独立同分布数据可能会影响数据科学中常用的程序，如自助法、子采样和交叉验证。
- en: 'There are different types of data to be found in columns: numeric (both floating
    and integer), ordinal (integer), categorical (both low and high cardinality, i.e.,
    with a low or high number of distinct labels), and dates: each one requires a
    different approach for data processing and analysis. We suggest mastering the
    pandas package (and its DataFrame data structure) to handle all such different
    kinds of features found in tabular data.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在列中可以找到不同类型的数据：数值型（包括浮点数和整数）、序数型（整数）、分类型（低和高基数，即具有低或高数量的不同标签），以及日期：每种类型都需要不同的数据处理和分析方法。我们建议掌握
    pandas 包（及其 DataFrame 数据结构）来处理表格数据中发现的这些不同类型的特征。
- en: 'Different data pathologies are connected to the type of data you have in columns:
    constant or quasi-constant features, duplicated or highly collinear features,
    irrelevant features, rare categories and other incongruencies, missing data, and
    information leakage. Also, for each one of these, there are specific remedies.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的数据异常与列中数据的类型有关：常数或准常数特征、重复或高度共线性特征、无关特征、稀有类别和其他不一致性、缺失数据和信息泄露。对于这些中的每一个，都有特定的补救措施。
- en: Finding and obtaining tabular data is relatively easy if you look for open repositories
    (such as the UCI Machine Learning Repository or Kaggle Datasets) or consult the
    Google Dataset search engine.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您寻找公开存储库（如 UCI 机器学习存储库或 Kaggle 数据集）或咨询 Google 数据集搜索引擎，找到和获取表格数据相对容易。
- en: EDA plays an important role in helping you clean the data you have gathered
    internally in your organization or from the web. You can use value counts and
    descriptions, histograms, box plots, correlation matrices, and low-dimensionality
    projections such as t-SNE to reveal the structure and the problems in data.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）在帮助您清理组织内部或从网络收集的数据方面发挥着重要作用。您可以使用值计数和描述、直方图、箱线图、相关矩阵和低维投影（如 t-SNE）来揭示数据的结构和问题。
