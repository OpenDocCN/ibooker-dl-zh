- en: 15 Foundation models and emerging search paradigms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15 基础模型和新兴搜索范式
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Retrieval augmented generation (RAG)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）
- en: Generative search for results summarization and abstractive question answering
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于结果摘要和抽象问答的生成搜索
- en: Integrating foundation models, prompt optimization, and evaluating model quality
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整合基础模型、提示优化和评估模型质量
- en: Generating synthetic data for model training
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为模型训练生成合成数据
- en: Implementing multimodal and hybrid search
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现多模态和混合搜索
- en: The future of AI-powered search
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能搜索的未来
- en: Large language models (LLMs), like the ones we’ve tested and fine-tuned in the
    last two chapters, have been front and center in the advances in AI-powered search
    in recent years. You’ve already seen some of the key ways search quality can be
    enhanced by these models, from improving query interpretation and document understanding
    by mapping content into embeddings for dense vector search, to helping extract
    answers to questions from within documents.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），就像我们在前两章中测试和微调的那些，在近年来人工智能搜索的进步中一直处于中心位置。您已经看到了一些关键方式，这些模型如何通过将这些内容映射到密集向量搜索的嵌入来提高搜索质量，从通过映射内容到嵌入以改善查询解释和文档理解，到帮助从文档中提取问题的答案。
- en: But what additional advanced approaches are emerging on the horizon? In this
    chapter, we’ll cover some of the more recent advances at the intersection of search
    and AI. We’ll cover how foundation models are being used to extend new capabilities
    to AI-powered search, like results summarization, abstractive question answering,
    multimodal search across media types, and even conversational interfaces for search
    and information retrieval. We’ll cover the basics of emerging search paradigms
    like generative search, retrieval augmented generation (RAG), and new classes
    of foundation models that are reinventing some of the ways we’ll soon approach
    the frontier of AI-powered search.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但在未来的地平线上，有哪些额外的先进方法正在出现？在本章中，我们将介绍搜索和人工智能交叉领域的一些最新进展。我们将介绍基础模型如何被用来扩展人工智能搜索的新功能，如结果摘要、抽象问答、跨媒体类型的多模态搜索，甚至是搜索和信息检索的对话界面。我们将介绍新兴搜索范式的基础，如生成搜索、检索增强生成（RAG）以及正在重新定义我们即将接近人工智能搜索前沿的一些方式的新类别基础模型。
- en: 15.1 Understanding foundation models
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 理解基础模型
- en: A *foundation model* is a model that is pretrained on a large amount of broad
    data and is designed to be generally effective at a wide variety of tasks. LLMs
    are a subset of foundation models that are trained on a very large amount of text.
    Foundation models can also be trained on images, audio, or other sources, or even
    on multimodal data incorporating many different input types. Figure 15.1 demonstrates
    common categories of foundation models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**基础模型**是在大量广泛数据上预训练的模型，旨在在各种任务上具有普遍的有效性。大型语言模型（LLMs）是基础模型的一个子集，它们在非常大量的文本上进行了训练。基础模型也可以在图像、音频或其他来源上训练，甚至可以在包含许多不同输入类型的跨模态数据上训练。图15.1展示了基础模型的常见类别。
- en: '![figure](../Images/CH15_F01_Grainger.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F01_Grainger.png)'
- en: Figure 15.1 Foundation model types. LLMs are one of several types of foundation
    models.
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.1 基础模型类型。LLMs是几种基础模型类型之一。
- en: Vision foundation models can be used to map images into embeddings (like how
    we mapped text to embeddings in chapter 13), which can then be searched to enable
    image-to-image search.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉基础模型可以将图像映射到嵌入（就像我们在第13章中将文本映射到嵌入一样），然后可以搜索以实现图像到图像的搜索。
- en: A multimodal foundation model can be built using both text and images (or other
    data types), and it can then enable cross-modal search for images based on text
    queries, or on text based on uploaded images as the query. We’ll implement this
    kind of multimodal text and image search in section 15.3.2\. Generative multimodal
    models, like Stable Diffusion (a text-to-image model), can also be used to generate
    brand-new images based only on text prompts. Multimodal foundation models that
    can learn from both images and text are also commonly referred to as *vision language
    models* (VLMs).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用文本和图像（或其他数据类型）构建多模态基础模型，然后它可以基于文本查询或基于上传的图像作为查询来实现基于文本的图像跨模态搜索。我们将在第15.3.2节中实现这种多模态文本和图像搜索。像Stable
    Diffusion（一种文本到图像模型）这样的生成多模态模型也可以仅根据文本提示生成全新的图像。可以从图像和文本中学习的多模态基础模型也通常被称为**视觉语言模型**（VLMs）。
- en: 15.1.1 What qualifies as a foundation model?
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.1 什么可以称为基础模型？
- en: Foundation models are typically trained on a wide variety of data covering many
    topics so that they are effective at generalized interpretation and prediction
    across domains. These models are called “foundation” models because they can serve
    as a base model (or foundation), which can then be more quickly fine-tuned on
    domain-specific or task-specific training sets to better tackle specific problems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型通常在涵盖众多主题的广泛数据上训练，以便它们在跨领域的泛化解释和预测方面表现出色。这些模型被称为“基础”模型，因为它们可以作为基础模型（或基础），然后可以更快地在特定领域或特定任务的训练集上进行微调，以更好地解决特定问题。
- en: 'Foundation models typically meet the following criteria:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型通常满足以下标准：
- en: They are *large*, typically trained on massive amounts of data, often with billions
    or trillions of parameters.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们是**大型模型**，通常在大量数据上训练，通常具有数十亿或数万亿个参数。
- en: They are *pretrained*, using significant compute capacity to arrive at model
    weights to be saved and deployed (or fine-tuned) later.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们是**预训练的**，使用显著的计算能力来得出要保存和部署（或微调）的模型权重。
- en: They are *generalizable* to many tasks, as opposed to limited to specific tasks.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们具有**可泛化性**，适用于许多任务，而不是局限于特定任务。
- en: They are *adaptable*, using prompts to pull in additional context from their
    trained model to adjust their predicted output. This makes the kinds of queries
    they can accept very flexible.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们是**可适应的**，使用提示从其训练模型中提取额外的上下文来调整其预测输出。这使得它们可以接受的查询类型非常灵活。
- en: They are *self-supervised*, learning automatically from raw data how to relate
    and interpret the data and represent it for future use.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们是**自监督的**，从原始数据中自动学习如何关联和解释数据，并将其表示为未来的使用。
- en: We have already worked with several foundation models in previous chapters,
    including BERT, which is one of the earliest foundation models, and RoBERTa, which
    we used in chapter 13 to generate embeddings and perform semantic search on those
    embeddings. Sentence Transformer models such as SBERT (Sentence-BERT) and SRoBERTa
    (Sentence-RoBERTa) are models that were fine-tuned from the BERT and RoBERTa foundation
    models to excel at the semantic textual similarity (STS) task. We also fine-tuned
    the `deepset/roberta-base-squad2` model in chapter 14; it is a model based upon
    the RoBERTa foundation model that has been fine-tuned for the question-answering
    task. Technically SBERT, SRoBERTa, and `deepset/roberta-base -squad2` are themselves
    fine-tuned foundation models, which can be further used as the foundation for
    more fine-tuning to generate additional models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在之前的章节中与几个基础模型合作过，包括BERT，这是最早的基础模型之一，以及我们在第13章中使用的RoBERTa，我们用它来生成嵌入并在这些嵌入上执行语义搜索。Sentence
    Transformer模型，如SBERT（Sentence-BERT）和SRoBERTa（Sentence-RoBERTa），是从BERT和RoBERTa基础模型微调的模型，以在语义文本相似度（STS）任务上表现出色。我们还在第14章中微调了`deepset/roberta-base-squad2`模型；这是一个基于RoBERTa基础模型的模型，经过微调用于问答任务。技术上讲，SBERT、SRoBERTa和`deepset/roberta-base-squad2`本身也是微调的基础模型，可以进一步用作更多微调的基础以生成额外的模型。
- en: The dominant architecture for foundation models is currently the Transformer
    model, though recurrent neural networks (using architectures like MAMBA) can also
    be used, and additional architectures will inevitably evolve over time. Most Transformer-based
    models can be used to generate embedding vectors or predictive output.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，基础模型的主导架构是Transformer模型，尽管可以使用循环神经网络（使用如MAMBA的架构），并且随着时间的推移，必然会出现更多的架构。大多数基于Transformer的模型可以用于生成嵌入向量或预测输出。
- en: 'The strength of the responses from foundation models reflects the quality of
    three processes: training, fine-tuning, and prompting.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型响应的强度反映了三个过程的质量：训练、微调和提示。
- en: 15.1.2 Training vs. fine-tuning vs. prompting
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.2 训练与微调与提示
- en: '*Training* (or *pretraining*) is the process whereby a massive amount of data
    (often a large portion of the internet) is used to learn model weights for billions
    or trillions of parameters in the foundation model’s deep neural network. This
    process can sometimes be very expensive, take months, and cost millions of dollars
    due to the computing and energy requirements. This process manages to perform
    lossy compression of much of human knowledge into a neural network from which
    facts and relationships (words, linguistics, associations, etc.) can be decompressed
    back out later. Recall from section 13.3 that training a Transformer on text typically
    follows a self-supervised learning process that optimizes for predicting masked
    tokens in text sequences to measure overall comprehension of the text (the Cloze
    test described in section 13.3.1). This training can include any specific datasets
    that may be beneficial for the model’s knowledge base, such as computer code or
    domain-specific content (financial documents, academic papers, foreign languages,
    multimodal content, etc.)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*训练*（或*预训练*）是一个过程，在这个过程中，大量数据（通常是互联网的大部分）被用来学习基础模型深度神经网络中数十亿或数千亿参数的模型权重。这个过程有时可能非常昂贵，可能需要数月时间，并且由于计算和能源需求，可能花费数百万美元。这个过程设法将大量人类知识损失压缩到一个神经网络中，从该网络中可以后来解压缩出事实和关系（单词、语言学、关联等）。回想一下第13.3节，在文本上训练Transformer通常遵循一个自监督学习过程，该过程优化预测文本序列中掩盖的标记，以衡量文本的整体理解（第13.3.1节中描述的Cloze测试）。这种训练可以包括任何可能对模型知识库有益的特定数据集，例如计算机代码或特定领域的内
    容（财务文件、学术论文、外语、多模态内容等）。'
- en: '*Fine-tuning* is the process whereby the foundation model can be supplied with
    additional domain-specific data or instructions. For example, if you want the
    model to follow instructions or to act like a person or a chatbot, you can train
    the model with responses to input that reflect those behaviors. There are several
    approaches to fine-tuning, depending on the task or architecture and compute and
    budgetary requirements. Some types of fine-tuning will change all the weights
    of a model, which is helpful if the domain-tuning needs to be highly specific
    and the task of the original model is the same. More efficient or task-specific
    approaches may leave all the existing weights intact and add one or more additional
    layers to the neural network of the base foundation model. Fine-tuning enables
    these new sources to extend the capabilities of the original model to new data
    or patterns with a much smaller training process focused on specific data or goals.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*微调* 是一个过程，通过这个过程，基础模型可以提供额外的领域特定数据或指令。例如，如果你想使模型遵循指令或表现得像一个人或聊天机器人，你可以通过反映这些行为的输入来训练模型。根据任务或架构以及计算和预算要求，微调有几种方法。某些类型的微调会改变模型的所有权重，这在领域调整需要非常具体且原始模型的任务相同的情况下是有帮助的。更高效或特定于任务的途径可能会保留所有现有权重，并在基础基础模型的神经网络中添加一个或多个额外的层。微调使这些新来源能够通过一个专注于特定数据或目标的较小训练过程，扩展原始模型的能力，以处理新的数据或模式。'
- en: '*Prompting* is the process of supplying input to the trained language model
    to get an output. Prompting is the final “training” step for fine-tuning a model,
    but it occurs at inference time as opposed to training time. We can supply as
    much context in the prompt as the model allows, which means that the prompt can
    be manipulated to use this additional context to affect the output. For example,
    consider the output to the queries in listings 15.1 and 15.2 when they are sent
    to OpenAI’s GPT-4 language model.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示* 是向训练好的语言模型提供输入以获得输出的过程。提示是微调模型的最终“训练”步骤，但它发生在推理时间而不是训练时间。我们可以提供模型允许的尽可能多的上下文，这意味着提示可以被操纵来使用这些额外的上下文来影响输出。例如，考虑列表15.1和15.2中的查询当它们被发送到OpenAI的GPT-4语言模型时的输出。'
- en: Listing 15.1 Query with no prompt engineering
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.1 无提示工程查询
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Listing 15.1 provides a good general description of a unicorn. Contrast this
    with listing 15.2, which uses prompt engineering to give the foundation model
    a “chatbot” persona named AIPS Chat and then tells the foundation model to respond
    as if it had a PhD in biology.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.1提供了一个关于独角兽的良好一般描述。与此相对比的是列表15.2，它使用提示工程给基础模型赋予一个名为AIPS Chat的“聊天机器人”角色，然后告诉基础模型以拥有生物博士学位的人的身份进行回应。
- en: Listing 15.2 Query with additional context from prompt engineering
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.2 提示工程提供的额外上下文查询
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The query in listing 15.2, which includes the context of a chatbot having a
    PhD in biology, uses that additional context to inform its answer. If we had read/write
    access to the language model, we could fine-tune it with inputs and responses
    generated by a PhD in biology and by a chatbot. In the listing, we were able to
    accomplish a similar outcome simply by supplying the model with a prompt to pull
    in that context from its already-trained model. Fine-tuning will typically generate
    a better answer to such questions, but prompting is much more flexible, since
    you can supply any context at inference time, whereas fine-tuning would be more
    appropriate for general characteristics you want the model to learn and represent
    in all future interactions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第15.2列表中的查询，包括一个拥有生物学博士学位的聊天机器人的上下文，利用这个额外的上下文来告知其答案。如果我们能够访问语言模型的读写权限，我们可以使用由生物学博士和聊天机器人生成的输入和响应来微调它。在列表中，我们能够通过向模型提供提示来简单地从其已经训练好的模型中拉取那个上下文，从而实现类似的结果。微调通常会生成更好的答案，但提示更加灵活，因为您可以在推理时提供任何上下文，而微调则更适合于模型需要学习并在所有未来交互中表征的一般特征。
- en: Providing additional context in the prompt can be key to getting the best output
    from a foundation model. Since generative foundation models sequentially predict
    the next token in a sequence, one token at a time, coercing the model to output
    more relevant context in the response can cause the model to generate more relevant
    output. A large pretrained model is capable of *few-shot learning* (being able
    to learn in context without further training by providing two or three examples
    with the prompt). In listing 15.2, for example, we saw that by adding the “biology”
    context to the prompt, the answer thereafter included phrases like “in the natural
    world”, “From a biological perspective”, “actual scientific data or research”,
    and “within the field of biology”.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示中提供额外的上下文可能是获得最佳输出的关键。由于生成式基础模型是按顺序逐个预测序列中的下一个标记，因此强迫模型在响应中输出更多相关上下文可以导致模型生成更相关的输出。一个大型预训练模型能够实现*少量样本学习*（能够在提供两个或三个示例的情况下，无需进一步训练就能在特定上下文中学习）。例如，在第15.2列表中，我们看到通过在提示中添加“生物学”上下文，之后的答案中就包含了诸如“在自然世界中”、“从生物学角度来看”、“实际科学数据或研究”以及“在生物学领域”等短语。
- en: The hardest and most expensive part of training is the initial generation of
    the foundation model. Once it’s trained, the fine-tuning for specific tasks is
    relatively quick and inexpensive and can often be done with a reasonably small
    training set. If we compare training these models with human learning, a foundation
    model may be like a high school student who generally knows how to read and write
    and can answer basic questions about math, science, and world history. If we want
    the student to be able to generate and interpret financial statements (income
    statements, cash flow statements, and balance sheets), they are going to need
    to take an accounting course to learn those skills. After 18 or more years of
    training (life experience and school), however, the student can likely learn sufficient
    accounting within a few months to generate and interpret financial statements.
    Similarly, with foundation models, the initial training phase takes the longest
    and provides the base upon which further knowledge and skills can be much more
    quickly learned.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 训练中最困难且成本最高的部分是基础模型的初始生成。一旦训练完成，针对特定任务的微调就相对快速且成本低廉，通常可以使用一个相当小的训练集完成。如果我们将这些模型与人类学习进行比较，基础模型可能就像一个高中生，他们通常知道如何阅读和写作，并能回答有关数学、科学和世界历史的常识性问题。如果我们想让这个学生能够生成和解读财务报表（收入报表、现金流量报表和资产负债表），他们需要学习会计课程来掌握这些技能。然而，经过18年或更长时间的训练（生活经验和学校教育）后，学生可能在几个月内就能学会足够的会计知识来生成和解读财务报表。同样，对于基础模型来说，初始训练阶段耗时最长，为后续更快地学习更多知识和技能提供了基础。
- en: 15.2 Generative search
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 生成式搜索
- en: Most of our journey into building AI-powered search has focused on finding the
    results, answers, or actions matching the user’s intent. In chapter 14, we went
    so far as to extract specific answers to questions, using our retriever-reader
    pattern. But what if instead of returning real documents or extracted answers,
    our search engine could generate new content on the fly in response to queries?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在构建人工智能搜索的过程中，大部分的旅程都集中在寻找与用户意图相匹配的结果、答案或行动。在第14章中，我们甚至提取了特定问题的具体答案，使用了我们的检索-阅读模式。但如果我们的搜索引擎能够根据查询实时生成新的内容，而不是返回真实文档或提取的答案，会怎么样呢？
- en: '*Generative models* are models that can generate new content based on incoming
    prompts. Their output can be text content, images generated from text input, audio
    emulating specific people or sounds, or videos combining both audio and video.
    Text can be generated to describe an image, or alternatively, an image, audio,
    or video can be generated to “describe” the text in a different modality.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成模型*是能够根据输入提示生成新内容的模型。它们的输出可以是文本内容、从文本输入生成的图像、模拟特定人物或声音的音频，或者结合音频和视频的视频。可以生成文本来描述图像，或者相反，可以生成图像、音频或视频来以不同的方式“描述”文本。'
- en: We are entering a world where someone will be able to enter a search query,
    and the search engine could return entirely made-up content and images, generated
    on the fly, in response to the user’s query or prompts. While this can be amazing
    with queries like `What` `is` `an` `optimal` `agenda` `for` `three` `days` `in`
    `Paris` `assuming` `I` `really like` `fine` `dining,` `historic` `buildings` `and`
    `museums,` `but` `hate` `crowds`, it also introduces severe ethical considerations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正进入一个世界，在这个世界里，有人可以输入一个搜索查询，搜索引擎可以返回完全虚构的内容和图像，这些内容和图像是即时生成的，作为对用户查询或提示的响应。虽然对于像“在巴黎三天内，假设我真的喜欢美食、历史建筑和博物馆，但讨厌人群，一个最佳的日程安排是什么？”这样的查询来说这可能很神奇，但它也引入了严重的伦理考量。
- en: Should a search engine be responsible for synthesizing information for its users
    instead of returning existing content for them to interpret? What if the search
    engine is politically or commercially biased and is trying to influence users’
    thinking or behavior? What if the search engine is being used as a tool for censorship
    or to spread propaganda (such as images or text that were modified on the fly)
    to trick people into false beliefs or to take dangerous actions?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎是否应该为用户综合信息，而不是为他们提供现有内容进行解读？如果搜索引擎在政治或商业上存在偏见，并试图影响用户的思维或行为，会怎样？如果搜索引擎被用作审查的工具或传播宣传（如即时修改的图像或文本）来欺骗人们相信错误或采取危险行动，又会怎样？
- en: Some of the more common use cases for generative search are abstractive question
    answering and results summarization. Whereas traditional search approaches have
    returned “ten blue links” or predetermined info boxes with information matching
    certain queries, generative search focuses on creating search responses on the
    fly based on dynamically analyzing the search results. From left to right, figure
    15.2 shows the progression from these traditional search approaches toward generative
    search.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式搜索的一些更常见的用例包括抽取式问答和结果摘要。而传统的搜索方法返回的是“十个蓝色链接”或与特定查询匹配的预定信息框，生成式搜索则侧重于根据动态分析搜索结果即时创建搜索响应。从左到右，图15.2展示了从这些传统搜索方法向生成式搜索的演变过程。
- en: '![figure](../Images/CH15_F02_Grainger.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH15_F02_Grainger.png)'
- en: Figure 15.2 Spectrum of traditional search approaches (left) to generative search
    (right)
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.2 传统搜索方法（左）到生成式搜索（右）的频谱
- en: Extractive question answering, covered in chapter 14, begins the move toward
    generative search, in that it analyzes search results to return direct answers
    to questions instead of just the search results or predetermined answers. It’s
    not entirely “generative” search, however, in that it still only returns answers
    exactly as presented in the searched documents, without any additional synthesis
    or new content generation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在第14章中介绍的抽取式问答，开始向生成式搜索转变，因为它分析搜索结果以返回直接的问题答案，而不是仅仅提供搜索结果或预定的答案。然而，这并不是完全的“生成式”搜索，因为它仍然只返回与搜索文档中呈现的答案完全一致的答案，没有任何额外的综合或新内容生成。
- en: As we continue toward the right in figure 15.2, results summarization is the
    first technique that can be fully considered generative search. Results summarization,
    which we’ll cover in section 15.2.2, is the process of taking the search results
    and summarizing their content. This can be quite useful for the user, particularly
    if they are researching a topic and don’t have time to read through all the results.
    Instead of the user needing to click on multiple links and read and analyze each
    of the results, a summary of the pages (along with citations, if desired) can
    be returned, saving the user time assessing the content.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们继续向图15.2的右侧移动时，结果摘要是最先可以完全考虑为生成式搜索的技术。结果摘要，我们将在15.2.2节中介绍，是从搜索结果中提取其内容的过程。这对于用户来说非常有用，尤其是当他们正在研究一个主题而没有时间阅读所有结果时。用户不需要点击多个链接并阅读和分析每个结果，而是可以返回页面摘要（如果需要的话，包括引用），从而节省用户评估内容的时间。
- en: Abstractive question answering is very similar to extractive question answering,
    in that it tries to answer a user’s question, but it does so by either analyzing
    search results (like in results summarization) or simply by relying on the underlying
    foundation model to generate the answer. Relying on the foundation model is more
    likely to lead to made-up, or hallucinated, results, so it is often beneficial
    to use the search results as part of the prompt context for in-context learning.
    We’ll cover this process of retrieval augmented generation in the next section.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象式问答与提取式问答非常相似，因为它们都试图回答用户的问题，但它通过分析搜索结果（如结果摘要）或简单地依靠底层基础模型来生成答案来实现。依赖基础模型更有可能导致虚构或幻觉的结果，因此通常有益于将搜索结果作为上下文学习提示的一部分。我们将在下一节中介绍这种检索增强生成的过程。
- en: Many major search engines and startups have integrated these generative models
    and interactive chat sessions into their search experiences, which provides the
    models with access to the internet (or at least a copy of the internet in the
    form of the search index). This allows the models to have a near-real-time view
    of the world’s information. It means that the models may know detailed public
    information about the users they are interacting with, which may shape the results,
    and it also means that anyone can work to inject malicious information or intentions
    into these models by crafting web pages that relate concepts in misleading ways.
    If we’re not careful, the field of search engine optimization (SEO) may go from
    trying to boost a website higher in search results to trying to manipulate the
    AI into providing malicious answers to end users. Applying critical thinking skills
    and validating the output of these models will be essential for anyone using them,
    but unfortunately, the models can be so compelling that many people are likely
    to be fooled into believing misleading output unless significant safeguards are
    put in place.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 许多主要搜索引擎和初创公司已经将这些生成模型和交互式聊天会话整合到他们的搜索体验中，这为模型提供了访问互联网（或至少是搜索索引形式的互联网副本）的机会。这使得模型能够几乎实时地了解世界的信息。这意味着模型可能知道与他们互动的用户详细的公开信息，这可能影响结果，这也意味着任何人都可以通过制作以误导方式关联概念的网页来向这些模型注入恶意信息或意图。如果我们不小心，搜索引擎优化（SEO）领域可能会从试图提高网站在搜索结果中的排名转变为试图操纵AI向最终用户提供恶意答案。对于使用这些模型的人来说，应用批判性思维技能和验证这些模型的输出将是至关重要的，但不幸的是，这些模型可能如此吸引人，以至于许多人可能会被误导的输出所欺骗，除非采取重大的安全措施。
- en: These models will continue to evolve to support searchers by interpreting search
    results. While many people may dream of having an AI-based personal assistant,
    the ethical considerations of having an AI generate the knowledge you consume
    daily will need to be handled carefully.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型将继续进化，通过解释搜索结果来支持搜索者。虽然许多人可能梦想拥有一个基于AI的个人助理，但AI生成你每天消费的知识所涉及的伦理考量需要谨慎处理。
- en: 15.2.1 Retrieval augmented generation
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.1 检索增强生成
- en: The workflow of using a search engine or vector database to find relevant documents
    that can be supplied as context to an LLM is commonly referred to as *retrieval
    augmented generation* (RAG).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用搜索引擎或向量数据库查找可以提供给LLM作为上下文的有关联的文档的工作流程通常被称为*检索增强生成*（RAG）。
- en: 'We’ve mentioned RAG several times before, and it’s a big buzzword as this book
    is being published. There’s a very good reason for this: the training of language
    models is a form of lossy compression of the training data, and the models can’t
    faithfully store the massive amount of data they were trained on without losing
    fidelity. Their information also dates from the last time they were trained—it
    is hard for the models to make decisions on evolving information without some
    continually updated external data source.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前多次提到过RAG，随着这本书的出版，它已经成为了一个热门词汇。这有一个非常好的原因：语言模型的训练是对训练数据的一种有损压缩，而模型无法在不丢失准确性的情况下忠实存储它们训练过的海量数据。它们的信息也只到上次训练时为止——在没有一些持续更新的外部数据源的情况下，模型很难对不断变化的信息做出决策。
- en: No matter how much context a language model can accept in its prompt, it’s computationally
    infeasible or at least cost-prohibitive to rely on an LLM as a source of truth
    for all the information it was trained on. Thankfully, the entire purpose of search
    engines is to find relevant context for any incoming query. In fact, this *entire*
    book is a deep dive into the *retrieval* part of RAG. As we discuss results summarization,
    abstractive question answering, and generative search in this chapter, we’re touching
    on the *generation* part of RAG.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 无论语言模型可以接受多少上下文，将其作为所有训练信息的真理来源都是计算上不可行或至少成本高昂的。幸运的是，搜索引擎的整个目的就是为任何传入的查询找到相关的上下文。事实上，这本书的*整个*内容都是对RAG的*检索*部分的深入研究。在我们讨论本章的结果摘要、抽象问答和生成搜索时，我们正在触及RAG的*生成*部分。
- en: To perform RAG, most current libraries take documents, split them into one or
    more sections (chunks), and then index embeddings for each section into a search
    engine or vector database. Then, at generation time, the application prompting
    the language model will create queries to find supplementary information needed
    to execute the next prompt, generate embeddings for those queries, perform a dense
    vector search to find the highest-scoring sections using vector similarity (usually
    cosine or dot product), and pass the resulting ranked sections along with the
    prompt to the generative AI model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行RAG，大多数当前库都会将文档取来，将其拆分成一个或多个部分（块），然后将每个部分的嵌入索引到搜索引擎或向量数据库中。然后，在生成时，提示语言模型的程序将创建查询以找到执行下一个提示所需的信息补充，为这些查询生成嵌入，执行密集向量搜索以找到使用向量相似度（通常是余弦或点积）得分最高的部分，并将结果排序的部分连同提示一起传递给生成AI模型。
- en: Challenges with chunking
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 块化挑战
- en: 'This process of splitting apart documents into sections is known as *chunking*,
    and it is both necessary and problematic. The problem can be understood as a tension
    between three constraints:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种将文档拆分成部分的过程被称为*块化*，它既是必要的也是问题性的。这个问题可以理解为三个约束之间的张力：
- en: '*Vector database limitations*—Many vector databases are designed to index single
    vectors, but summarizing an entire document into a single vector can lose a lot
    of the specificity and context of the document. It pools the embedding across
    the entire document into a more vague summary embedding.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*向量数据库的限制*——许多向量数据库被设计用来索引单个向量，但将整个文档总结成一个向量可能会丢失文档的很多特异性和上下文。它将整个文档的嵌入汇总成一个更模糊的摘要嵌入。'
- en: '*Loss of context between independent chunks*—Splitting a document into many
    separate documents to preserve the specificity of each section (chapter, paragraph,
    sentence, word, or other useful semantic chunk) can cause a loss of context across
    chunks. If you split a document into 10 chunks and independently generate embeddings,
    the shared context across those embeddings is lost.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*独立块之间的上下文丢失*——为了保留每个部分（章节、段落、句子、单词或其他有用的语义块）的特异性，将文档拆分成许多单独的文档可能会导致块之间的上下文丢失。如果你将一个文档拆分成10个块，并独立生成嵌入，那么这些嵌入之间的共享上下文就会丢失。'
- en: '*Computational complexity of many chunks*—The more chunks you have, the more
    vectors you need to index, and the more you need to search. This can be computationally
    expensive and slow, and it can also be difficult to manage the relevance of the
    results across the chunks when you have to weight many matches on the same initial
    document relative to fewer, but better, matches on other documents.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*许多块的计算复杂性*——你拥有的块越多，你需要索引的向量就越多，你需要搜索的也越多。这可能会非常昂贵且缓慢，而且在你必须对同一初始文档上的多个匹配项进行加权，相对于其他文档上更少但更好的匹配项时，也可能很难管理跨块的结果的相关性。'
- en: At the early stages of implementing RAG for generative AI, many people have
    focused on chunking documents into multiple separate documents to overcome vector
    database limitations. Computational complexity can be managed using good ANN (approximate
    nearest-neighbor) algorithms, but the loss of context across chunks is still problematic
    in that case, and it’s frankly wasteful to create an unbounded number of overlapping
    chunks instead of using better algorithms to model the embeddings.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施RAG用于生成式AI的早期阶段，许多人专注于将文档分块成多个单独的文档，以克服向量数据库的限制。可以使用良好的ANN（近似最近邻）算法来管理计算复杂性，但在那种情况下，上下文在块之间的丢失仍然是问题，而且不使用更好的算法来建模嵌入，而是创建无限数量的重叠块，这显然是浪费的。
- en: As an alternative approach, several search engines and vector databases (such
    as Vespa and Qdrant) already have support for multivalued vector fields, which
    makes it possible to create an arbitrary number of chunks and also to create overlapping
    chunks within one document (so that a single sentence or paragraph can be part
    of multiple chunks, thereby preserving more context across chunks). Such multivector
    support will likely become standard in the coming years to support emerging contextualized
    late interaction approaches like those introduced in the ColBERT family of models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种替代方法，几个搜索引擎和向量数据库（如Vespa和Qdrant）已经支持多值向量字段，这使得创建任意数量的块成为可能，也可以在单个文档内创建重叠的块（这样单个句子或段落可以成为多个块的一部分，从而在块之间保留更多上下文）。这种多向量支持预计在未来几年将成为标准，以支持像ColBERT模型系列中引入的基于上下文的后期交互等新兴方法。
- en: The future of RAG
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RAG的未来
- en: 'RAG, as a discipline, is still in its infancy, but it’s evolving rapidly. Numerous
    libraries and frameworks have been developed to perform RAG, but they are often
    based on an overly simple understanding of information retrieval. Current RAG
    implementations tend to rely entirely on a language model and vector similarity
    for relevance, ignoring most of the other AI-powered retrieval techniques we’ve
    covered in this book. In the context of the dimensions of user intent (figure
    1.7), this has three effects:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: RAG作为一个学科，仍处于起步阶段，但发展迅速。已经开发了许多库和框架来执行RAG，但它们通常基于对信息检索过于简单的理解。当前的RAG实现往往完全依赖于语言模型和向量相似度来决定相关性，而忽略了本书中我们介绍的大多数其他AI驱动的检索技术。在用户意图的维度（见图1.7）的背景下，这有三个影响：
- en: The content context is handled well conceptually (assuming the chosen embedding
    model was trained for query-document retrieval), but not on a specific keyword
    basis (due to the scope of the vectors summarizing multiple keywords).
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容上下文在概念上处理得很好（假设所选嵌入模型是针对查询-文档检索进行训练的），但不是基于特定关键词（由于总结多个关键词的向量的范围）。
- en: The domain context is handled as well as the language model is fine-tuned.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领域上下文处理得和语言模型微调一样好。
- en: The user context is usually ignored entirely.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户上下文通常完全被忽略。
- en: For the content-understanding dimension of user intent (see section 1.2.5),
    promising approaches that use contextualized late interaction with embeddings
    are evolving, and they may eventually overcome the need for chunking altogether.
    These approaches (such as ColBERT, ColBERTv2, ColPali, and successors) involve
    generating an embedding per token in the document, but where each token’s embedding
    uses the context of that token within the entire document (or at least a long
    surrounding context). This prevents the loss of context across chunks and avoids
    the need to index an unbounded number of chunks into the engine. These kinds of
    approaches make much more sense for RAG, and retrieval in general, than some of
    the more naive approaches mentioned in the last subsection. We expect to see similar
    approaches evolve over the coming years and to significantly improve recall over
    the current baseline ranking approaches.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于用户意图的内容理解维度（见第1.2.5节），使用与嵌入的上下文后期交互的具有前景的方法正在发展，它们最终可能克服对分块的需求。这些方法（如ColBERT、ColBERTv2、ColPali及其继任者）涉及为文档中的每个标记生成一个嵌入，但每个标记的嵌入都使用该标记在整个文档中的上下文（或至少是长距离的上下文）。这防止了上下文在块之间的丢失，并避免了将无限数量的块索引到引擎中的需要。这些方法对于RAG和检索来说比上一小节中提到的某些更简单的方法更有意义。我们预计在未来几年将看到类似方法的发展，并将显著提高召回率，超过当前的基线排名方法。
- en: The retrieval concepts you’ve learned in this book, concerning dimensions of
    user intent, reflected intelligence and signals models, semantic knowledge graphs,
    learning to rank, and feedback loops (combined with LLM-based vector search),
    are light years ahead of most current RAG implementations, which only use a subset.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中你学到的检索概念，关于用户意图的维度、反映智能和信号模型、语义知识图谱、学习排序和反馈循环（结合基于 LLM 的向量搜索），在大多数当前的 RAG
    实现中领先数光年，这些实现只使用了一小部分。
- en: That said, the future of RAG is bright, and it’s likely the next few years will
    both see more sophisticated approaches to tackling current challenges, and see
    the integration of better retrieval and ranking techniques into the RAG process.
    RAG techniques are changing and evolving rapidly, and the field is ripe for continued
    innovation and improvement.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，RAG 的未来光明，预计在接下来的几年里，我们将看到解决当前挑战的更复杂方法，以及更好的检索和排序技术整合到 RAG 流程中。RAG 技术正在快速变化和演变，该领域为持续的创新和改进做好了准备。
- en: 'We’ll cover one of the most popular generative search approaches using RAG
    in the coming section: results summarization (with citations).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将介绍使用 RAG 的最受欢迎的生成式搜索方法之一：结果摘要（带引用）。
- en: 15.2.2 Results summarization using foundation models
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.2 使用基础模型进行结果摘要
- en: 'In chapter 3, we stated that the search engine is primarily responsible for
    three things: indexing, matching, and ranking results. Chapter 13 already showed
    how to enhance indexing (with embeddings), matching (with approximate nearest-neighbor
    search), and ranking of results (with similarity comparison on embedding vectors).
    But the way we return results is often just as important as the results themselves.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章中，我们提到搜索引擎主要负责三件事：索引、匹配和排序结果。第十三章已经展示了如何通过嵌入来增强索引、通过近似最近邻搜索来增强匹配以及通过在嵌入向量上进行相似度比较来增强结果的排序。但返回结果的方式往往与结果本身一样重要。
- en: In chapter 14, we demonstrated how to extract answers out of ranked documents
    containing those answers, which can be a drastic improvement over returning full
    documents and forcing users to analyze them individually. But what if the answer
    to a question *requires* analysis of the documents? What if the desired answer
    is actually the *result* of an analysis that combines information from multiple
    documents into a unified, well-sourced answer?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在第十四章中，我们展示了如何从包含这些答案的排序文档中提取答案，这可以比返回完整文档并强制用户逐个分析它们有显著的改进。但如果问题的答案*需要*分析文档呢？如果期望的答案实际上是结合来自多个文档的信息，形成一个统一、有良好来源的答案的分析结果呢？
- en: One of the problems with answers generated directly from LLMs is that they are
    created based on statistical probability distributions from parameters learned
    by the model. Though the model may have been trained on a massive number of data
    sources, it does not store those data sources directly. Instead, the model is
    compressed into a lossy representation of the data. This means that you cannot
    count on the output from an LLM to accurately reflect the input data—only an approximation
    of it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 直接从 LLM 生成的答案的一个问题是，它们是基于模型学习到的参数的统计概率分布创建的。尽管模型可能在大量数据源上进行了训练，但它并没有直接存储这些数据源。相反，模型被压缩成数据的损失表示。这意味着你不能指望
    LLM 的输出能够准确反映输入数据——只能是一个近似值。
- en: As a result, foundation models are well-known to hallucinate—to generate responses
    that make up incorrect facts or misrepresent the topics in the response. The context
    provided in the prompt also heavily influences the answer, to the point where
    foundation models can sometimes be more a reflection of the user’s word choices
    and bias than a legitimate answer to a question. While this can be useful in creative
    endeavors, it makes today’s foundation models quite unreliable for safely generating
    the fact-based responses for which search engines are historically relied upon.
    For users to truly trust the results, they need to be able to verify the sources
    of the information. Thankfully, instead of directly relying on foundation models
    for answers to questions, we can combine the search engine with a foundation model,
    using RAG to create a hybrid output.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基础模型因产生幻觉而闻名——生成包含错误事实或错误表述主题的响应。提示中提供的上下文也极大地影响了答案，以至于基础模型有时可能更多地反映了用户的词汇选择和偏见，而不是对问题的合法回答。虽然这在创造性工作中可能很有用，但它使得今天的基础模型在安全生成搜索引擎历史上依赖的事实性响应方面相当不可靠。为了用户真正信任结果，他们需要能够验证信息的来源。幸运的是，我们不必直接依赖基础模型来回答问题，而是可以将搜索引擎与基础模型结合起来，使用RAG创建混合输出。
- en: Using foundation models like this to summarize search results is a great way
    to infuse your search engine with better AI-powered responses. Let’s walk through
    an example of performing search results summarization with citations using a foundation
    model. We’ll use output from OpenAI’s GPT-4 model, but you can get similar output
    from most current open source or permissively licensed LLMs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此类基础模型来总结搜索结果是一种很好的方法，可以为你的搜索引擎注入更好的AI驱动的响应。让我们通过一个使用基础模型进行带有引用的搜索结果摘要的例子来了解一下。我们将使用OpenAI的GPT-4模型的输出，但你也可以从大多数当前的开源或许可LLMs中获得类似输出。
- en: 'The workflow includes two steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程包括两个步骤：
- en: Execute a search. This can involve converting the query to an embedding and
    performing a dense vector search or using any other technique we’ve discussed
    to find the most relevant results.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行搜索。这可能涉及将查询转换为嵌入并执行密集向量搜索，或使用我们讨论过的任何其他技术来找到最相关的结果。
- en: Construct a prompt instructing your foundation model to take the user’s query
    and read and summarize a set of search results returned from the query.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个提示，指导你的基础模型接收用户的查询，并阅读和总结从查询返回的一组搜索结果。
- en: The following listing demonstrates an example prompt combining the search results
    from step 1 into a prompt from step 2.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表演示了一个示例提示，将步骤1的搜索结果组合成步骤2的提示。
- en: Listing 15.3 Prompt to foundation model to summarize search results
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.3 基础模型总结搜索结果的提示
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When all of listing 15.3 is passed as a prompt to the language model, we got
    the following result.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当将列表15.3的全部内容作为提示传递给语言模型时，我们得到了以下结果。
- en: Listing 15.4 Response to the summarization prompt from listing 15.3
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.4 对列表15.3的总结提示的响应
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This output pulls critical details from the search results to answer the original
    query (`What` `is` `a` `large` `language` `model?`) but does it in a way that
    cites the original articles. If that answer is too verbose, an additional refinement
    to the prompt can be added to the end of the instructions: `Be` `concise.` This
    results in the following output instead:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出从搜索结果中提取关键细节以回答原始查询（“什么是大型语言模型？”），但以引用原始文章的方式进行。如果该答案过于冗长，可以在指令末尾添加额外的改进提示：“Be
    concise.” 这将导致以下输出：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Listing 15.5 Results for listing 15.3’s prompt with `Be concise.`
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.5 列表15.3的提示“Be concise.”的结果
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Many search engines have started relying on foundation models to interpret and
    cite search results like this. By adding in additional instructions like “avoid
    being vague, controversial, or off-topic”, “use an unbiased and journalistic tone”,
    or even “write at a fifth-grade reading level”, you can adjust the quality and
    tone of the summaries to cater to your search engine’s needs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 许多搜索引擎已经开始依赖基础模型来解释和引用此类搜索结果。通过添加额外的指令，如“避免含糊不清、有争议或离题”，“使用无偏见和新闻风格”，甚至“以五年级阅读水平写作”，你可以调整摘要的质量和语气，以满足搜索引擎的需求。
- en: 15.2.3 Data generation using foundation models
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.3 使用基础模型生成数据
- en: In addition to foundation models adding a synthesis and summarization layer
    on top of search results, one of the other emerging use cases for the models is
    the generation of synthetic domain-specific and task-specific training data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在搜索结果之上添加合成和摘要层之外，这些模型的其他新兴用例之一是生成合成领域特定和任务特定的训练数据。
- en: 'As we think back to the numerous AI-powered search techniques we’ve already
    explored, many of them require substantial training data to implement:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们之前已经探索过的众多基于AI的搜索技术，其中许多需要大量的训练数据来实现：
- en: Signals boosting models require user signals data showing which user queries
    correspond with which documents.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号增强模型需要显示哪些用户查询与哪些文档相对应的用户信号数据。
- en: Click models, useful for automated LTR, require an understanding of which documents
    users click on and skip over in their search results.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于自动化LTR的点击模型需要了解用户在搜索结果中点击和跳过的文档。
- en: Semantic knowledge graphs require an index of data to find related terms and
    phrases.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义知识图谱需要一个数据索引来查找相关术语和短语。
- en: But what if we fine-tuned a foundation model to “come up with documents about
    missing topics” or “come up with realistic queries” associated with each document?
    Or better yet, what if we didn’t need to fine-tune at all, but could instead construct
    a prompt to generate great queries for documents? Such data could be used to generate
    synthetic signals to help improve relevance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们微调了一个基础模型来“生成关于缺失主题的文档”或“生成与每个文档相关的现实查询”，会怎么样？或者更好的是，如果我们根本不需要微调，而是可以构建一个提示来为文档生成优秀的查询呢？这样的数据可以用来生成合成信号，以帮助提高相关性。
- en: Listings 15.6 through 15.9 show how we can use a prompt to find queries and
    relevance scores by combining an LLM with searches on our Stack Exchange outdoors
    collection from chapter 14.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.6至15.9展示了我们如何通过将LLM与第14章中的Stack Exchange户外集合的搜索相结合，使用提示来查找查询和相关性分数。
- en: For our exercise, we found it better to give several documents to the prompt,
    as opposed to just one, and even better to return related documents (from the
    same topic or a results list from a previous query). This is important, because
    there are niche topics within each set of documents, so having related documents
    helps to return more fine-grained queries instead of general ones. Additionally,
    while all the documents may not be perfectly relevant to the original query (or
    may be noisy), this still gives the LLM a few-shot chance at understanding our
    corpus, as opposed to basing its context on a single example. The following listing
    shows the template used with the LLM to generate candidate queries for a list
    of documents.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的练习，我们发现给提示提供几份文档比只提供一份文档更好，甚至更好的是返回相关文档（来自同一主题或来自先前查询的结果列表）。这一点很重要，因为每套文档中都有一些细分主题，所以拥有相关文档有助于返回更精细的查询而不是一般的查询。此外，尽管所有文档可能并不完全与原始查询相关（或可能存在噪声），但这仍然给LLM提供了一个几轮的机会来理解我们的语料库，而不是基于单个示例来建立其上下文。以下列表显示了使用LLM生成文档列表的候选查询所使用的模板。
- en: Listing 15.6 A prompt that generates queries describing documents
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.6 生成描述文档的查询的提示
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The following listing shows code that can generate the `{resultset}` to be inserted
    into the prompt from listing 15.6\. With the example query `what` `are` `minimalist
    shoes?` we get the `resultset` using the `retriever` function from listing 14.15\.
    The retriever provides answer contexts for a query.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了可以生成要插入到列表15.6中的提示的`{resultset}`的代码。使用示例查询`what are minimalist shoes?`，我们通过列表14.15中的`retriever`函数获取`resultset`。检索器为查询提供答案上下文。
- en: Listing 15.7 Producing prompt-friendly text search results
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.7 生成提示友好的文本搜索结果
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 We only want the index and context information from the first 5 results
    returned by the retriever, and we prefix each result with its index of 0 to 4.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们只想从检索器返回的前5个结果中获取索引和上下文信息，并且为每个结果添加0到4的索引前缀。'
- en: 'Output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: These five results are then added to the bottom of the prompt from listing 15.6
    by replacing the `{resultset}` value with the response from listing 15.7\. We
    take the final, fully substituted prompt, and pass it into our LLM, yielding the
    following result.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这五个结果添加到列表15.6的提示底部，通过用列表15.7的响应替换`{resultset}`值来实现。我们使用完全替换后的最终提示，并将其传递给我们的LLM，得到以下结果。
- en: Listing 15.8 LLM response relating documents to generating queries
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.8 LLM响应将文档与生成查询相关联
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Notice the nuance in relevance between the candidate queries and the wording
    in each of their relevant results. Also note that the ordering of the relevant
    results is not the same as the ordering of the queries in the output. The context
    of document 0 is more relevant for the specific query `What are the characteristics
    of minimalist shoes?` and the context of document 1 is more relevant for the query
    `What` `is` `the definition of a minimalist shoe?`
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意候选查询与每个相关结果中的措辞之间的细微差别。此外，请注意相关结果的顺序与输出中查询的顺序不同。文档 0 的上下文对于特定查询 `什么是简约鞋的特点？`
    更相关，而文档 1 的上下文对于查询 `什么是简约鞋的定义？` 更相关。
- en: Note that we just use the pandas index position for the context IDs, instead
    of the IDs of the documents. In our experience, the IDs can confuse the model
    by providing irrelevant information. Depending on the LLM you use, this may need
    to be reverse engineered with some code, like we’ve done with our existing `example_contexts`
    from listing 15.7\. Also notice how the model changed the order in listing 15.8
    (the queries are not in the same order as the judgment results), so we’ll need
    to account for this next when we parse the output.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们仅使用 pandas 索引位置作为上下文 ID，而不是文档的 ID。根据我们的经验，ID 可能会通过提供无关信息来混淆模型。根据您使用的 LLM，这可能需要通过一些代码进行逆向工程，就像我们用列表
    15.7 中的现有 `example_contexts` 所做的那样。同时请注意，模型在列表 15.8 中改变了顺序（查询的顺序与判断结果的顺序不同），因此我们将在解析输出时考虑这一点。
- en: The following listing shows how to extract the information and make a nice Python
    dictionary for the queries, documents, and relevant results.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了如何提取信息并为查询、文档和相关结果创建一个漂亮的 Python 字典。
- en: Listing 15.9 Extracting pairwise judgments from the LLM output
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 15.9 从 LLM 输出中提取成对判断
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Regular expressions are used to see which line belongs to which list. You
    can experiment with more robust expressions if you run into some strange output
    from the model.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 正则表达式用于查看哪一行属于哪个列表。如果您从模型中获得一些奇怪的输出，可以尝试更健壮的表达式。'
- en: Next, we pass our output from listing 15.8 into the `extract_pairwise_judgments`
    from listing 15.9.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将列表 15.8 的输出传递给列表 15.9 中的 `extract_pairwise_judgments`。
- en: Listing 15.10 Positive relevance judgments generated from the LLM
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 15.10 由 LLM 生成的正面相关性判断
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 example_contexts, from listing 15.7, contains the search results for the
    query ''What are minimalist shoes?''.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 示例上下文，来自列表 15.7，包含查询 ''什么是简约鞋？'' 的搜索结果。'
- en: 'Response:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Running this process through hundreds of result sets taken from customer queries
    will produce a large list of relevant query-document pairs. Since we are processing
    five results at a time, we’ll also be generating five new positive pairwise judgments
    with every single prompt to the LLM. You can then use these judgments as synthetic
    signals or as training data to train many of the signals-based models explored
    in the earlier chapters.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 将此过程应用于从客户查询中获取的数百个结果集，将生成大量相关查询-文档对。由于我们一次处理五个结果，因此每次向 LLM 发送单个提示时，我们也将生成五个新的正面成对判断。然后，您可以使用这些判断作为合成信号或作为训练数据来训练在早期章节中探讨的许多基于信号的模型。
- en: 15.2.4 Evaluating generative output
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.4 评估生成输出
- en: In previous chapters, we covered the importance of using judgment lists to measure
    the quality of our search algorithms. In chapters 10 and 11, we generated judgment
    lists manually and then later automatically using click models, for training and
    measuring the quality of LTR models. In chapter 14, we generated silver sets and
    golden sets to train and measure the quality of our fine-tuned LLM for extractive
    question answering.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了使用判断列表来衡量我们搜索算法质量的重要性。在第 10 章和第 11 章中，我们手动生成判断列表，然后后来使用点击模型自动生成，用于训练和衡量
    LTR 模型的质量。在第 14 章中，我们生成了银集和金集来训练和衡量我们微调的 LLM 的提取式问答质量。
- en: These are all search use cases for which the query and results pairs are largely
    deterministic. Measuring the quality of generative model output is much trickier,
    however. We’ll explore some of the generative use cases in this section to see
    what can be done to overcome those challenges.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是查询和结果对在很大程度上是确定性的搜索用例。然而，衡量生成模型输出的质量要困难得多。在本节中，我们将探讨一些生成用例，以了解如何克服这些挑战。
- en: Generative output can be subjective. Many generative models produce different
    outputs given the same prompt when you adjust a temperature parameter. *Temperature*
    is a value between `0.0` and `1.0` that controls the randomness in the output.
    The lower the temperature, the more predictable the output; the higher the temperature,
    the more creative the output. We recommend always setting the temperature to `0.0`
    for the evaluation and production of your model so you have a higher confidence
    in its output. However, even a temperature of `0.0` may still produce different
    responses for the same prompt between runs, depending on the model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 生成输出可能具有主观性。许多生成模型在调整温度参数时，给定相同的提示会产生不同的输出。*温度*是一个介于`0.0`和`1.0`之间的值，它控制输出中的随机性。温度越低，输出越可预测；温度越高，输出越具创造性。我们建议在评估和生成您的模型时始终将温度设置为`0.0`，以便您对其输出有更高的信心。然而，即使是`0.0`的温度也可能在运行之间对同一提示产生不同的响应，这取决于模型。
- en: 'The core methodology for evaluating generative output is straightforward: frame
    and evaluate tasks that can be measured objectively. Since the output of the prompt
    can be unpredictable, however, it’s not easy to curate a dataset that can be reused
    to measure different prompts for your exact task. Therefore, we typically rely
    on established metrics to assess the quality of the model chosen first, before
    you start your own evaluation on its downstream output.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 评估生成输出的核心方法很简单：构建和评估可以客观衡量的任务。然而，由于提示的输出可能不可预测，因此很难创建一个可以重复用于衡量您特定任务的不同提示的数据集。因此，我们通常依赖于已建立的指标来评估首先选择的模型的质量，然后再开始对其下游输出进行自己的评估。
- en: 'Various important and common metrics address different language tasks. Each
    metric usually has a leaderboard, which is a competition-style benchmark that
    ranks models by performance on the given tasks. These are some common benchmarks:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的重要和常见指标针对不同的语言任务。每个指标通常都有一个排行榜，这是一个竞争风格的基准，通过在给定任务上的性能对模型进行排名。以下是一些常见的基准：
- en: '*AI2 Reasoning Challenge (ARC)*—A multiple-choice question-answering dataset
    consisting of 7,787 grade-school science exam questions.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AI2推理挑战（ARC）*——一个由7,787个小学科学考试问题组成的多项选择题问答数据集。'
- en: '*HellaSwag*—Common-sense inference tests; for example, a complex reasoning
    question is given with multiple choice.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HellaSwag*——常识推理测试；例如，给出一个复杂推理问题，并附有多个选择题。'
- en: '*Massive Multitask Language Understanding (MMLU)*—A test to measure a text
    model’s multitask accuracy. The test covers 57 tasks, including mathematics, history,
    computer science, law, and more.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*大规模多任务语言理解（MMLU）*——一种用于衡量文本模型多任务准确性的测试。该测试涵盖57个任务，包括数学、历史、计算机科学、法律等。'
- en: '*TruthfulQA*—Measures whether a language model is truthful in generating answers
    to questions. The benchmark comprises 817 questions that span 38 categories. The
    authors crafted questions that some humans would answer falsely due to a false
    belief or misconception.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TruthfulQA*——衡量语言模型在生成问题答案时是否诚实。该基准包括817个问题，涵盖38个类别。作者设计了某些人类可能会由于错误信念或误解而给出错误答案的问题。'
- en: Figure 15.3 shows an example of a HellaSwag question-and-answer test.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3展示了HellaSwag问答测试的示例。
- en: '![figure](../Images/CH15_F03_Grainger.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH15_F03_Grainger.png)'
- en: Figure 15.3 HellaSwag example question and multiple choices, with the correct
    answer highlighted
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.3 HellaSwag示例问题和多项选择题，正确答案已突出显示
- en: Some leaderboards contain multiple metrics, such as the Open LLM Leaderboard
    in figure 15.4.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一些排行榜包含多个指标，如图15.4中的Open LLM排行榜。
- en: '![figure](../Images/CH15_F04_Grainger.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH15_F04_Grainger.png)'
- en: Figure 15.4 HuggingFaceH4 Open LLM Leaderboard space (taken March 2024)
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.4 HuggingFaceH4开放大型语言模型排行榜空间（拍摄于2024年3月）
- en: You should use these metrics to decide which model to use for your specific
    domain and task. For example, if your task is abstractive question answering,
    consider using a model with the best TruthfulQA metric.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您应使用这些指标来决定为您的特定领域和任务使用哪种模型。例如，如果您的任务是抽象式问答，请考虑使用具有最佳TruthfulQA指标的模型。
- en: Warning Models are licensed, so make sure you only use a model with a license
    that aligns with your use case. Some models have licenses similar to open source
    software (like Apache 2.0 or MIT). But be careful, because many models have commercial
    restrictions (such as the LLaMA models and derivatives or models trained on output
    from restrictive models like GPT from OpenAI).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 模型是受许可的，所以请确保你只使用与你的用例相匹配的许可证的模型。一些模型的许可证类似于开源软件（如Apache 2.0或MIT）。但请注意，许多模型有商业限制（例如LLaMA模型及其衍生品或基于OpenAI的GPT等限制性模型输出的模型）。
- en: Once you have chosen your model, you should take steps to evaluate it on your
    prompts. Be rigorous, and track responses to your prompts and how they fare. Some
    purpose-built tools are emerging in the market to do this for you, including automatic
    prompt optimization approaches, but a simple spreadsheet can suffice. In the following
    section, we’ll construct our own metric to use for a new task.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你选择了你的模型，你应该采取步骤在你的提示上评估它。要严谨，并跟踪你的提示及其表现。市场上出现了一些专门为此目的设计的工具，包括自动提示优化方法，但一个简单的电子表格就足够了。在下一节中，我们将构建我们自己的度量标准来用于新的任务。
- en: 15.2.5 Constructing your own metric
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.5 构建自己的度量标准
- en: Let’s explore how a generative task can be construed objectively, allowing you
    to create a dataset and use a metric like precision or recall (both introduced
    in section 5.4.5) to evaluate a model’s accuracy. We’ll use a prompt with unstructured
    data to extract structured information. Since our result will be formatted predictably,
    we can then measure the accuracy of the response based on a human-labeled result.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨如何客观地构建一个生成任务，这样你就可以创建一个数据集，并使用像精确度或召回率（在第5.4.5节中介绍）这样的度量标准来评估模型的准确性。我们将使用一个包含非结构化数据的提示来提取结构化信息。由于我们的结果将格式化得可预测，我们就可以根据人工标记的结果来衡量响应的准确性。
- en: For our example, we’ll test the named-entity recognition accuracy of a generative
    model. Specifically, we’ll measure whether generative output from an LLM correctly
    labels entities of type person, organization, or location.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们将测试一个生成模型的命名实体识别准确性。具体来说，我们将衡量来自LLM的生成输出是否正确地标记了人员、组织或地点类型的实体。
- en: 'We’ll use the following snippet of text from a news article:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一篇新闻文章的以下文本片段中获取：
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If we manually label the entities in this snippet with the tags `<per>` (person),
    `<org>` (organization), and `<loc>` (location), we’ll arrive at the labeled version
    in the following listing.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们手动将此片段中的实体标记为`<per>`（人员）、`<org>`（组织）和`<loc>`（地点），我们将在以下列表中到达标记版本。
- en: Listing 15.11 Article tagged with entity labels
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.11 带有实体标签的文章
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is a format that a generative model should be able to produce, taking text
    and generating a marked-up version as instructed by a prompt. But this response
    is only semi-structured with ad hoc markup. We need to process it further, and
    we can write some Python to extract the entities into a suitable JSON format.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个生成模型应该能够生成的格式，它接受文本并按照提示生成一个标记版本。但这个响应只有半结构化，带有临时标记。我们需要进一步处理它，我们可以编写一些Python代码来将实体提取到合适的JSON格式中。
- en: Listing 15.12 Extracting entities from generative output
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.12 从生成输出中提取实体
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Output:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With the entities now represented in a structured form (as JSON), we can use
    the list to calculate our metric. We can manually label many passages or use the
    techniques from sections 14.2–14.3 to automate the labeling, using a model to
    construct a silver set and then correcting the outputs to produce a golden set.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在实体已经以结构化形式（作为JSON）表示，我们可以使用列表来计算我们的度量标准。我们可以手动标记许多段落，或者使用第14.2-14.3节中的技术来自动化标记，使用模型构建一个银集，然后纠正输出以产生一个金集。
- en: When you have your golden set, you can try a prompt like the following.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有了你的金集时，你可以尝试以下提示。
- en: Listing 15.13 Labeled entities
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.13 标记的实体
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding prompt, `{text}` would be replaced by the paragraph for which
    you want to identify entities. When the model generates the response, it can be
    passed directly into the `extract_entities` function in listing 15.12\.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的提示中，`{text}`将被替换为你想要识别实体的段落。当模型生成响应时，它可以直接传递到列表15.12中的`extract_entities`函数。
- en: The output of the extracted entities can then be compared to the golden set
    to produce the number of true positives, `TP` (correctly identified entities),
    false positives, `FP` (incorrectly identified text that should not be identified),
    and false negatives, `FN` (entities that should have been identified but were
    not).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以将提取的实体输出与黄金集进行比较，以产生真实正例的数量`TP`（正确识别的实体）、假正例`FP`（不应识别但被错误识别的文本）和假负例`FN`（应该被识别但未被识别的实体）。
- en: 'From these numbers, you can calculate the following:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些数字中，您可以计算出以下内容：
- en: '`Precision = (`*TP / ( TP + FP )*`)`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Precision = (`*TP / ( TP + FP )*`)`'
- en: '`Recall = (`*TP / ( TP + FN)*`)`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Recall = (`*TP / ( TP + FN)*`)`'
- en: '`F1 = (`*2 * (Precision * Recall) / (Precision + Recall)*`)`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F1 = (`*2 * (Precision * Recall) / (Precision + Recall)*`)`'
- en: Many of the metrics listed on leaderboards use `F1` scores.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 许多在排行榜上列出的指标使用`F1`分数。
- en: What generative tasks do you have? Be creative and think about how you can shape
    your task to be objectively measurable. For example, if you are generating search
    result summaries, you can perform something similar to the named-entity recognition
    task—check if the summaries show important spans of text for a given search result
    set or show result number citations with titles correctly attributed.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您有哪些生成任务？要富有创意，并思考如何使您的任务具有客观可衡量性。例如，如果您正在生成搜索结果摘要，您可以执行类似于命名实体识别的任务——检查摘要是否显示了给定搜索结果集中的重要文本片段，或者是否正确地引用了结果编号和标题。
- en: 15.2.6 Algorithmic prompt optimization
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.6 算法提示优化
- en: 'Throughout section 15.2, we’ve covered the use of LLM prompts to generate synthetic
    data and to summarize and cite search results for RAG. We also covered metrics
    to quantify the quality of generated data. We discussed this in the context of
    measuring the *model’s* quality, but there’s an important point we glossed over:
    the quality of the *prompt*.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在第15.2节中，我们介绍了使用LLM提示生成合成数据、总结和引用RAG的搜索结果，以及量化生成数据质量的指标。我们讨论了在衡量*模型*质量时的这一内容，但有一个重要的观点我们略过了：*提示*的质量。
- en: 'If you recall, we have three different ways to improve LLM output: improve
    the model during training (pretraining), fine-tune the model, or improve the prompt.
    Pretraining and fine-tuning are direct, programmatic optimizations of the neural
    network model, but we’ve thus far treated prompt creation and improvement (known
    as *prompt engineering*) as a manual process requiring human intervention.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得，我们有三种不同的方法来改进LLM输出：在训练期间改进模型（预训练）、微调模型或改进提示。预训练和微调是直接、程序化的神经网络模型优化，但到目前为止，我们将提示创建和改进（称为*提示工程*）视为需要人工干预的手动过程。
- en: In reality, the prompt can also be fine-tuned programmatically much like the
    model. Libraries like DSPy (Declarative Self-improving Language Programs, Pythonically)
    provide a framework for “programming” language model usage as opposed to manually
    prompting it. Functionally, this is accomplished by defining the desired output
    format from the language model (along with training data showing good responses)
    and letting the library optimize the prompt’s wording to best achieve that output.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，提示也可以像模型一样进行程序化微调。例如，DSPy（Declarative Self-improving Language Programs，Pythonically）库提供了一个框架，用于“编程”语言模型的使用，而不是手动提示它。功能上，这是通过定义从语言模型期望的输出格式（以及显示良好响应的训练数据）并让库优化提示的措辞以最佳实现该输出来完成的。
- en: 'DSPy accomplishes this programmatic prompt optimization using four key components:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: DSPy使用四个关键组件来完成这种程序化提示优化：
- en: '*Signatures*—These are simple strings explaining the goal of the training process,
    as well as the expected input and output. Examples:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*签名*——这些是简单的字符串，解释了训练过程的目标，以及预期的输入和输出。例如：'
- en: '`question → answer` (question answering)'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question → answer` (问答)'
- en: '`document → summary` (document summarization)'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`document → summary` (文档摘要)'
- en: '`context,` `question → answer` (RAG)'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context,` `question → answer` (RAG)'
- en: '*Modules*—These take in a signature and combine it with a prompting technique,
    a configured LLM, and a set of parameters to generate a prompt to achieve the
    desired output.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模块*——这些接受一个签名，并将其与提示技术、配置的LLM和一组参数相结合，以生成一个提示以实现期望的输出。'
- en: '*Metrics*—These are measures of how well the output generated by a module matches
    the desired output.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*指标*——这些是衡量模块生成的输出与期望输出匹配程度的度量。'
- en: '*Optimizers*—These are used to train the model by iteratively testing variations
    to the prompts and parameters to optimize for the specified metric.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器*——这些用于通过迭代测试提示和参数的变体来训练模型，以优化指定的指标。'
- en: 'Programming the language model usage like this provides four key benefits:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式编程语言模型的使用提供了四个关键优势：
- en: It allows for the automatic optimization of the prompt, which can be a time-consuming
    and error-prone process when done manually.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许自动优化提示，当手动进行时可能是一个耗时且易出错的流程。
- en: It allows for the optimization of the prompt to quickly test well-known templates
    (and new techniques or data over time) and best practices for prompting, as well
    as to chain multiple prompting stages together into a more sophisticated pipeline.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许优化提示以快速测试众所周知的模板（以及随着时间的推移新的技术或数据），以及提示的最佳实践，以及将多个提示阶段链接在一起形成一个更复杂的流水线。
- en: It allows you to easily switch out the LLM at any time and “recompile” the DSPy
    program to re-optimize the prompt for the new model.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许您在任何时候轻松切换LLM，并“重新编译”DSPy程序以重新优化新模型的提示。
- en: It allows you to chain multiple modules, models, and workflows together into
    more sophisticated pipelines, and to optimize the prompts for each stage of the
    pipeline and the pipeline as a whole.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许您将多个模块、模型和工作流程链接在一起形成更复杂的流水线，并优化流水线每个阶段的提示以及整个流水线。
- en: These benefits make your application more robust and easier to maintain, as
    compared to manual prompt tuning, and they ensure your prompt is always fine-tuned
    and optimized for your current environment and goals. Once you’ve coded up the
    configured DSPy pipeline, you just run the optimizer to *compile* it, at which
    point all the optimal parameters are learned for each module. Want to then try
    a new model, technique, or set of training data? Just recompile your DSPy program,
    and you’re ready to go, with your prompts automatically optimized to yield the
    best results. You can find lots of “getting started” templates for implementing
    RAG, question answering, summarization, and any number of other LLM-prompt-based
    tasks in the DSPy documentation at [https://github.com/stanfordnlp/dspy](https://github.com/stanfordnlp/dspy).
    If you’re building a non-trivial LLM-based application, we highly recommend you
    opt for such a programmatic approach to prompt optimization.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动提示调整相比，这些优势使您的应用程序更加健壮且易于维护，并确保您的提示始终针对当前环境和目标进行微调和优化。一旦您编写了配置的DSPy流水线，只需运行优化器来*编译*它，此时每个模块的所有最优参数都已学习。想要尝试新的模型、技术或一组训练数据？只需重新编译您的DSPy程序，您就可以出发了，您的提示将自动优化以产生最佳结果。您可以在DSPy文档中找到许多用于实现RAG、问答、摘要以及其他基于LLM提示任务的“入门”模板，文档地址为[https://github.com/stanfordnlp/dspy](https://github.com/stanfordnlp/dspy)。如果您正在构建一个非平凡的基于LLM的应用程序，我们强烈建议您选择这样的程序化方法来优化提示。
- en: 15.3 Multimodal search
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 多模态搜索
- en: 'Let’s now explore one of the most powerful capabilities enabled by foundation-model–based
    embeddings: multimodal search. Multimodal search engines allow you to search for
    one content type using another content type (also called *cross-modal* search),
    or to search multiple content types together. For example, you can search for
    an image using either text, an image, or a hybrid query combining both text and
    an image. We’ll implement each of these use cases in this section.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索由基础模型嵌入提供的一种最强大的功能：多模态搜索。多模态搜索引擎允许您使用另一种内容类型（也称为*跨模态*搜索）来搜索一种内容类型，或者同时搜索多种内容类型。例如，您可以使用文本、图像或结合文本和图像的混合查询来搜索图像。我们将在本节中实现这些用例中的每一个。
- en: Multimodal search is made possible by the fact that vector embeddings can be
    generated for text, images, audio, video, and other content that can be mapped
    into overlapping vector spaces. This enables us to search any content type against
    any other content type without the need for any special indexing or transformation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态搜索之所以成为可能，是因为可以为文本、图像、音频、视频和其他可以映射到重叠向量空间的内容生成向量嵌入。这使得我们能够在不需要任何特殊索引或转换的情况下，对任何内容类型进行任何其他内容类型的搜索。
- en: Multimodal search engines have the potential to revolutionize the way we search
    for information, removing barriers that previously prevented us from using all
    the available context to best understand the incoming queries and rank results.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态搜索引擎有可能彻底改变我们搜索信息的方式，消除之前阻止我们使用所有可用上下文来最好地理解传入查询和排名结果的障碍。
- en: 15.3.1 Common modes for multimodal search
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.1 多模态搜索的常见模式
- en: 'While multimodal search entails many different data types being searchable,
    we’ll briefly discuss the currently most common data modalities in this section:
    natural language, images, audio, and video.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然多模态搜索涉及许多不同的数据类型都可以被搜索，但在这个部分我们将简要讨论目前最常见的数据模态：自然语言、图像、音频和视频。
- en: Natural language search
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自然语言搜索
- en: In chapters 13 and 14, we implemented semantic search and question-answering
    using dense vectors. Both techniques are examples of natural language search.
    A simple query like `shirt` `without` `stripes` will confound every e-commerce
    platform built atop an inverted index, unless special logic is added manually
    or integrated into a search-focused knowledge graph using *semantic functions*
    like we implemented in chapter 7\.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在第13章和第14章中，我们使用了密集向量实现了语义搜索和问答。这两种技术都是自然语言搜索的例子。一个简单的查询，如`shirt` `without`
    `stripes`，将会让建立在倒排索引之上的每一个电子商务平台都感到困惑，除非手动添加特殊逻辑或集成到一个使用*语义函数*的知识图谱中，就像我们在第7章中实现的那样。
- en: Today’s state-of-the-art LLMs are becoming more sophisticated at handling these
    queries and are even able to interpret instructions and synthesize information
    from different sources to generate new responses. Many companies building traditional
    databases and NoSQL data stores are aggressively integrating dense vector support
    as first-class data types to take advantage of all these major advances.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的最先进的LLMs在处理这些查询方面变得更加复杂，甚至能够解释指令并从不同来源综合信息以生成新的响应。许多构建传统数据库和NoSQL数据存储的公司正在积极集成密集向量支持作为一等数据类型，以利用所有这些主要进步。
- en: While it is unlikely that these dense-vector–based approaches will fully replace
    traditional inverted indexes, knowledge graphs, signals boosting, learning to
    rank, click models, and other search techniques, we will see new hybrid approaches
    emerge that combine the best of each of these techniques to continue optimizing
    content and query understanding.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些基于密集向量的方法不太可能完全取代传统的倒排索引、知识图谱、信号增强、排序学习、点击模型和其他搜索技术，但我们将会看到新的混合方法出现，这些方法将结合这些技术的最佳部分，以继续优化内容和查询理解。
- en: Image search
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像搜索
- en: As you saw in chapter 2, images are another form of unstructured data, alongside
    text.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第2章中看到的，图像是另一种非结构化数据形式，与文本并列。
- en: Traditionally, if you wanted to search for images in an inverted index, you
    would search through text descriptions of the image or labels applied to the image.
    With dense vector search, however, searching images for visual or conceptual similarity
    can be implemented almost as easily as text search. By taking input images, encoding
    them into embedding vectors with a vision Transformer, and indexing those vectors,
    it becomes possible to accept another image as a query, encode it into an embedding,
    and then search with that vector to find visually similar images.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，如果你想在倒排索引中搜索图像，你会搜索图像的文本描述或标签。然而，使用密集向量搜索，搜索图像的视觉或概念相似性可以几乎像文本搜索一样容易实现。通过输入图像，使用视觉Transformer将其编码为嵌入向量，并索引这些向量，现在可以接受另一张图像作为查询，将其编码为嵌入，然后使用该向量搜索以找到视觉上相似的图像。
- en: Further, by training a model with images alongside text descriptions of those
    images, it becomes possible to perform a multimodal search for images either by
    inputting an image or by text. Rather than having a caption and an unsearchable
    image, one can now use convolutional neural networks (CNNs) or vision Transformers
    to encode the image to a dense vector space that co-exists in the same vector
    space as text. With image representation and text representation in the same vector
    space, you can search for a more descriptive version of the image than the caption
    provides, yet still match using text to describe the features in the image.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过训练一个同时包含图像及其文本描述的模型，现在可以通过输入图像或文本来进行多模态图像搜索。不再需要有一个标题和一个无法搜索的图像，现在可以使用卷积神经网络（CNNs）或视觉Transformer将图像编码到与文本共存的密集向量空间中。在同一个向量空间中，图像表示和文本表示，你可以搜索比标题更详细的图像版本，同时仍然可以使用文本来描述图像中的特征。
- en: Given this ability to encode images and text into the same dense vector space,
    it is also possible to reverse the search and use the image as the query to match
    any text documents that include similar language to what’s occurring in the image,
    or to combine both an image and text into a hybrid query to find other images
    best matching a combination of the text and image query modalities. We’ll walk
    through examples of some of these techniques in section 15.3.2\.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可以将图像和文本编码到相同的密集向量空间中，因此也可以反向搜索，使用图像作为查询来匹配包含与图像中出现的语言相似的任何文本文档，或者将图像和文本结合成一个混合查询以找到与文本和图像查询模态的最佳匹配的其他图像。我们将在第15.3.2节中通过一些这些技术的示例。
- en: Audio search
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 音频搜索
- en: Searching with audio or for audio has historically been mostly a text-to-speech
    problem. Audio files would be converted to text and indexed, and audio queries
    would be converted to text and searched against the text index. Transcribing voice
    is a very difficult problem. Voice assistants from Google, Apple, Amazon, and
    Microsoft typically do very well with short queries, but this level of accuracy
    has historically been difficult to come by for those wanting to integrate open
    source solutions. Recent breakthroughs with the combinations of speech recognition
    and language models that can error-correct phonetic misunderstandings are bringing
    better technology to the market, however.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用音频或搜索音频在历史上主要是一个文本到语音的问题。音频文件会被转换成文本并建立索引，音频查询也会被转换成文本并与文本索引进行搜索。语音转录是一个非常困难的问题。谷歌、苹果、亚马逊和微软的语音助手在处理短查询时通常表现很好，但对于想要集成开源解决方案的人来说，这种准确度在历史上一直难以实现。然而，最近在语音识别和语言模型结合以纠正语音误解错误方面的突破，正在将更好的技术带入市场。
- en: It’s important to keep in mind that many other kinds of important sounds can
    be present in audio besides just spoken words. If someone searches for `loud train`,
    `rushing stream`, or `Irish accent`, these are all qualities they’d hope to find
    in any returned audio results. Multimodal Transformer models, when trained by
    text and audio mapping to overlapping vector spaces, now make this possible.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，除了说话的词语之外，音频中还可以包含许多其他重要的声音。如果有人搜索“大声的火车”、“急流”或“爱尔兰口音”，这些都是他们希望在返回的音频结果中找到的品质。当多模态Transformer模型通过文本和音频映射到重叠的向量空间进行训练时，现在这使得这一切成为可能。
- en: Video search
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 视频搜索
- en: Video is nothing more than the combination of sequential images overlaid and
    kept in sequence with audio. If audio and images can both be mapped to an overlapping
    vector space with written text, this means that by indexing each frame of a video
    (or maybe a few frames per second, depending on the granularity needed), it’s
    possible to create a video search engine that allows searching by text description
    for any scene in the video, searching by audio clip, or searching by an image
    to find the most similar video. The deep learning models being generated in the
    fields of computer vision, natural language processing, and audio processing are
    all converging, and as long as these different types of media can all be represented
    in overlapping vector spaces, search engines can now search for them.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 视频不过是序列图像的组合，这些图像被叠加并保持序列与音频同步。如果音频和图像都可以映射到与书面文本重叠的向量空间，这意味着通过索引视频的每一帧（或者可能每秒几帧，取决于所需的粒度），可以创建一个视频搜索引擎，允许通过文本描述搜索视频中的任何场景，通过音频片段搜索，或者通过图像搜索以找到最相似的视频。计算机视觉、自然语言处理和音频处理领域的深度学习模型正在趋同，只要这些不同类型的媒体都可以表示在重叠的向量空间中，搜索引擎现在可以搜索它们。
- en: 15.3.2 Implementing multimodal search
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.2 实现多模态搜索
- en: 'In this section, we’ll implement one of the most popular forms of multimodal
    search: text-to-image search. We’ll do this by using embeddings that were jointly
    trained on image and text data pairs from the internet. This results in the latent
    text features learned being in the same vector space as the latent image features
    learned. This also means that in addition to performing text-to-image search,
    we can use the same embedding model to perform an image-to-image visual search
    based on the pixels in any incoming image. We’ll also show an example of combining
    modalities in a hybrid query, searching for results that best match both a text
    query *and* an image at the same time.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现最流行的多模态搜索形式之一：文本到图像搜索。我们将通过使用在互联网上的图像和文本数据对上联合训练的嵌入来实现这一点。这导致学习的潜在文本特征与学习的潜在图像特征处于相同的向量空间。这也意味着，除了执行文本到图像搜索外，我们还可以使用相同的嵌入模型根据任何传入图像的像素执行图像到图像的视觉搜索。我们还将展示一个在混合查询中结合模态的示例，同时搜索与文本查询
    *和* 图像最匹配的结果。
- en: We’ll use the CLIP model, which is a multimodal model that can understand images
    and text in the same vector space. CLIP is a Transformer-based model developed
    by OpenAI that was trained on a large dataset of images and their associated text
    captions. The model was trained to predict which caption goes with which image,
    and vice versa. This means that the model has learned to map images and text into
    the same vector space so that similar images and related text embeddings are located
    close together.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 CLIP 模型，这是一个可以在相同向量空间中理解图像和文本的多模态模型。CLIP 是由 OpenAI 开发的一个基于 Transformer
    的模型，它在大量图像及其相关文本标题的数据集上进行了训练。该模型被训练来预测哪个标题与哪个图像相匹配，反之亦然。这意味着该模型已经学会了将图像和文本映射到相同的向量空间，以便相似的图像和相关的文本嵌入彼此靠近。
- en: We will return to The Movie Database (TMDB) dataset we used in chapter 10 for
    implementing our multimodal search examples. In this case, however, instead of
    searching on the text of movies, we’ll search on images from movies.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回到第 10 章中使用的电影数据库（TMDB）数据集，以实现我们的多模态搜索示例。然而，在这种情况下，我们不是在电影的文本上搜索，而是在电影的图像上搜索。
- en: In the following listing, we define the functionality to calculate normalized
    embeddings and build a movie collection. The collection is composed of the movie’s
    image embeddings and movie metadata, including titles, image source URLs, and
    the URL for the movie’s TMDB page.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们定义了计算归一化嵌入和构建电影集合的功能。该集合由电影的图像嵌入和电影元数据组成，包括标题、图像源 URL 和电影 TMDB 页面的
    URL。
- en: Listing 15.14 Indexing cached movie embeddings
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 15.14 索引缓存的电影嵌入
- en: '[PRE19]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Normalize movie embeddings at index time so we can use the more efficient
    dot product (versus cosine) calculation at query time.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在索引时对电影嵌入进行归一化，以便在查询时使用更高效的点积（相对于余弦）计算。'
- en: '#2 We pregenerated and cached the movie embeddings so you don’t have to download
    and process all the images.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们预先生成并缓存了电影嵌入，这样你就不必下载和处理所有图像。'
- en: '#3 Builds a movie collection containing image embeddings'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 构建包含图像嵌入的电影集合'
- en: Because the CLIP model is a multimodal model jointly trained on text and images,
    the embeddings we generate from either text or images can be used to search the
    same image embeddings.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CLIP 模型是一个在文本和图像上联合训练的多模态模型，因此从文本或图像生成的嵌入可以用来搜索相同的图像嵌入。
- en: With the index now built using normalized embeddings, all that’s left to do
    is to execute a vector search against the collection. The following listing shows
    the key functions needed to search for images using text queries, image queries,
    or hybrid text and image queries.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用归一化嵌入构建的索引后，剩下的工作就是执行对集合的向量搜索。以下列表显示了搜索图像所需的键函数，包括使用文本查询、图像查询或混合文本和图像查询。
- en: Listing 15.15 Multimodal text/image vector search using CLIP
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 15.15 使用 CLIP 的多模态文本/图像向量搜索
- en: '[PRE20]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 Loads the pretrained CLIP model and image preprocessor'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载预训练的 CLIP 模型和图像预处理程序'
- en: '#2 Executes a vector search for a query embedding'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 执行查询嵌入的向量搜索'
- en: '#3 Constructs a search request with a query embedding to search against the
    indexed image embeddings'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用查询嵌入构建搜索请求，以搜索索引图像嵌入'
- en: '#4 The data type used for each embedding feature value, in this case a 32-bit
    float.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 每个嵌入特征值使用的数据类型，在本例中为 32 位浮点数。'
- en: '#5 Computes the normalized embeddings for text'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 计算文本的归一化嵌入'
- en: '#6 Computes the normalized embeddings for an image'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 计算图像的归一化嵌入'
- en: '#7 Computes and combines normalized embeddings for an image and text'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 计算并组合图像和文本的归一化嵌入'
- en: '#8 Averages the text and image vectors to create a multimodal query'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 将文本和图像向量平均以创建一个多模态查询'
- en: 'The `movie_search` function follows a process similar to one we used in chapter
    13: take a query vector and execute a vector search against a collection with
    embeddings. Our `encode_text` and `encode_image` functions calculate normalized
    embeddings based on text or an image. The `encode_text_and_image` function is
    a hybrid of the two, where we generate embeddings from both text and images, unit-normalize
    them, and pool them together by averaging them.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`movie_search` 函数遵循与第13章中使用的类似的过程：获取一个查询向量并对具有嵌入的集合执行向量搜索。我们的 `encode_text`
    和 `encode_image` 函数根据文本或图像计算归一化嵌入。`encode_text_and_image` 函数是两者的混合体，其中我们从文本和图像中生成嵌入，将它们单位归一化，并通过平均它们将它们汇总在一起。'
- en: With the core multimodal embedding calculations in place, we can now implement
    a simple search interface to display the top results for an incoming text, image,
    or text and image query.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心多模态嵌入计算到位后，我们现在可以实施一个简单的搜索界面来显示对传入的文本、图像或文本和图像查询的顶部结果。
- en: Listing 15.16 Multimodal vector search on text and image embeddings
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.16 在文本和图像嵌入上进行多模态向量搜索
- en: '[PRE21]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `search_and_display` function takes in either a text query, an image query,
    or both, and then retrieves the embeddings and executes the search. The function
    then displays the top results in a simple grid. Figure 15.5 shows an example output
    for the query `singing in the rain`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`search_and_display` 函数接受文本查询、图像查询或两者，然后检索嵌入并执行搜索。然后该函数以简单的网格显示顶部结果。图15.5显示了查询“在雨中唱歌”的示例输出：'
- en: '[PRE22]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![figure](../Images/CH15_F05_Grainger.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F05_Grainger.png)'
- en: Figure 15.5 Text query for `singing in the rain`
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.5 对“在雨中唱歌”的文本查询
- en: 'Three of the first four images are from the movie *Singin’ in the Rain*, all
    of the images show someone in the rain or with an umbrella, and many are either
    from musicals or show people actively singing. These results demonstrate the power
    of performing image search using embeddings that were trained on both text and
    images: we now have the ability to search for pictures whose pixels contain the
    meaning expressed by the words!'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 前四幅图像中的三幅来自电影《 Singin’ in the Rain》，所有图像都显示有人在雨中或打着伞，许多图像要么来自音乐剧，要么显示人们积极唱歌。这些结果展示了使用在文本和图像上训练的嵌入执行图像搜索的强大功能：我们现在能够搜索包含由单词表达的意义的像素的图片！
- en: 'To demonstrate some of the nuances of mapping text to meaning in images, let’s
    run two variations of the same query: `superhero flying` versus `superheroes flying`.
    In traditional keyword search, we’d usually throw away the plural, but in the
    context of searching on embeddings, and particularly in the context of multimodal
    search, let’s see how this slight difference affects the results. As you can see
    in figure 15.6, our search results changed from images of a single superhero flying
    (or at least up high off the ground) to pictures of mostly groups of superheroes
    containing similar actions.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示将文本映射到图像中的意义的一些细微差别，让我们运行同一查询的两个变体：“superhero flying”与“superheroes flying”。在传统的关键词搜索中，我们通常会丢弃复数，但在嵌入搜索的上下文中，尤其是在多模态搜索的上下文中，让我们看看这种细微差别如何影响结果。如图15.6所示，我们的搜索结果从单个超级英雄飞行的图像（或至少离地面很高）变为主要包含类似动作的超级英雄群体的图片。
- en: '![figure](../Images/CH15_F06_Grainger.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F06_Grainger.png)'
- en: Figure 15.6 Nuanced difference between `superhero` `flying` vs. `superheroes
    flying` queries
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.6 “superhero flying”与“superheroes flying”查询之间的细微差别
- en: This cross-modal (text-to-image) search is impressive, but let’s also test out
    image-to-image search to demonstrate how reusable these multimodal embeddings
    are. Figure 15.7 shows the results of an image search using a picture of a DeLorean
    (the car famously converted to a time machine in the movie *Back to the Future*).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这种跨模态（文本到图像）搜索令人印象深刻，但让我们也测试一下图像到图像搜索，以展示这些多模态嵌入的可重用性。图15.7显示了使用《回到未来》电影中著名的时光机器变形的德洛瑞安（DeLorean）汽车的图像进行图像搜索的结果。
- en: '[PRE23]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![figure](../Images/CH15_F07_Grainger.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F07_Grainger.png)'
- en: Figure 15.7 Image-to-image search for a car that looks similar to a DeLorean
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.7 与德洛瑞安相似的汽车的图像到图像搜索
- en: 'As you would expect, most of the cars are actually the famous DeLorean from
    *Back to the Future*. The other cars have similar aesthetic features: similar
    shapes, similarly opening doors. Not only have we matched the features of the
    car well, but many of the results also reflect the glowing lighting from the image
    query.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所料，大多数汽车实际上是著名的《回到未来》中的DeLorean。其他汽车有类似的美学特征：相似的形状，类似的开门方式。我们不仅很好地匹配了汽车的特征，而且许多结果还反映了图像查询中的发光照明效果。
- en: To take our DeLorean search one step further, let’s try a multimodal search
    combining our last two query examples. Let’s perform a multimodal query for both
    the previous DeLorean image (image modality) and the text query `superhero` (text
    modality). The output of this query is shown in figure 15.8.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的DeLorean搜索进一步推进，让我们尝试结合我们最后两个查询示例的多模态搜索。让我们对之前的DeLorean图像（图像模态）和文本查询`superhero`（文本模态）进行多模态查询。这个查询的结果如图15.8所示。
- en: '[PRE24]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![figure](../Images/CH15_F08_Grainger.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F08_Grainger.png)'
- en: Figure 15.8 Multimodal search for an image and text query
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.8 图像和文本查询的多模态搜索
- en: Interesting! The first image is of a superhero (Black Panther) on top of a sporty
    car, with lights glowing from the car, similar to the original picture. Most of
    the results now show the heroes or protagonists of their movies, with each of
    the results containing sporty cars and most images containing illuminating lighting
    effects picked up from the image query. This is a great example of how multimodal
    embeddings can be used to infer nuanced intent and to enable querying in new ways
    across diverse types of datasets.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 真是令人兴奋！第一张图片是一位超级英雄（黑豹）站在一辆运动型汽车上，汽车灯光闪烁，与原始图片相似。现在的大部分结果都显示了电影中的英雄或主角，每个结果都包含运动型汽车，大多数图片都包含了从图像查询中获取的照明效果。这是一个很好的例子，说明了如何使用多模态嵌入来推断细微的意图，并允许以新的方式在多种类型的数据集中进行查询。
- en: The implications of multimodal search across text and images are vast. In e-commerce
    search, someone can upload a picture of a part they need and immediately find
    it without knowing the name, or upload a picture of an outfit or furniture that
    they like and find similar styles. Someone can describe a medical problem and
    immediately find images reflecting the symptoms or body systems involved. From
    chapter 9, you may remember our work to learn latent features of items and users
    from user behavior signals. This behavior is yet another modality, like images,
    that can be learned and cross-trained with text and other modalities to increase
    the intelligence of your search engine. As we’ll discuss further in section 15.5,
    many additional data modalities like this will continue to become commonly searchable
    in the future, helping to make search engines even more powerful at understanding
    and finding relevant content and answers.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本和图像之间进行多模态搜索的影响是深远的。在电子商务搜索中，有人可以上传他们需要的零件的图片，立即找到它，即使不知道零件的名称，或者上传他们喜欢的服装或家具的图片，找到类似风格的款式。有人可以描述一个医疗问题，并立即找到反映症状或涉及的器官系统的图片。从第9章，你可能还记得我们从用户行为信号中学习物品和用户潜在特征的工作。这种行为是另一种模态，就像图像一样，可以学习并与文本和其他模态进行交叉训练，以增加搜索引擎的智能。正如我们将在第15.5节中进一步讨论的，许多类似的数据模态将继续在未来变得可搜索，帮助使搜索引擎在理解和查找相关内容和答案方面变得更加强大。
- en: 15.4 Other emerging AI-powered search paradigms
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 其他新兴的AI驱动搜索范式
- en: Throughout this book, you’ve learned techniques in most key AI-powered search
    categories, such as signals processing and crowdsourced relevance, knowledge graphs,
    personalized search, learning to rank, and semantic search using Transformer-based
    embeddings.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，你学习了大多数关键AI驱动搜索类别中的技术，例如信号处理和众包相关性、知识图谱、个性化搜索、学习排序和基于Transformer嵌入的语义搜索。
- en: The introduction of foundation models and the ability to represent language
    as dense vectors has revolutionized how we think about and build search engines
    in recent years. Whereas search used to be primarily text-based, now it is possible
    to search on anything that can be encoded into dense vectors. This includes searching
    on images, audio, video, concepts, or any combination of these or other modalities.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的出现以及将语言表示为密集向量的能力，在近年来彻底改变了我们思考和研究搜索引擎构建的方式。过去搜索主要是基于文本的，现在可以搜索任何可以编码为密集向量的内容。这包括对图像、音频、视频、概念或这些或其他模态的组合进行搜索。
- en: Additionally, the way that users interface with search continues to evolve in
    new and groundbreaking ways. This includes the ability to ask questions, receive
    summarized answers that are a combination of multiple search results, generate
    new content explaining search results, and generate images, prose, code, instructions,
    or other content. Chatbot interfaces, combined with context tracking throughout
    conversations, have enabled iterative refinement of search tasks, where users
    can provide real-time feedback on responses and enable AI to act as an agent to
    search, synthesize, and refine the relevance of search results based upon live
    user feedback.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，用户与搜索的交互方式也在以新的和开创性的方式不断发展。这包括提出问题、接收综合答案，这些答案是多个搜索结果的组合、生成解释搜索结果的新内容、生成图像、散文、代码、说明或其他内容。聊天机器人界面与对话中的上下文跟踪相结合，使得搜索任务的迭代优化成为可能，用户可以对响应提供实时反馈，并使AI能够作为代理进行搜索、综合和根据实时用户反馈优化搜索结果的相关性。
- en: In this final section of our AI-powered search journey, we’ll touch on a few
    of the trends we’re seeing evolve and how they are likely to shape the future
    of search.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们AI驱动的搜索之旅的最后一部分，我们将探讨一些正在发展的趋势以及它们如何可能塑造搜索的未来。
- en: 15.4.1 Conversational and contextual search
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.1 对话和上下文搜索
- en: We’ve spent significant time covering contextual search—understanding the content
    context, the domain context, and the user context when interpreting queries. As
    search engines become more conversational, the personal context begins to take
    even more priority. For example,
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经花费了大量时间来介绍上下文搜索——在解释查询时理解内容上下文、领域上下文和用户上下文。随着搜索引擎变得更加对话化，个人上下文开始变得更加重要。例如，
- en: '[PRE25]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Despite someone having their address bookmarked under the name “Home” and their
    phone tracking their locations daily (including where they sleep at night), some
    highly used digital assistants are still unable to automatically bring in that
    context, like in figure 15.9, and instead require your home location to be explicitly
    configured in the settings. These shortcomings are likely to disappear in the
    coming years as personal assistants build much more precise personalization models
    from all available contexts.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有人将地址保存在名为“家”的书签下，并且他们的手机每天跟踪他们的位置（包括他们晚上睡觉的地方），但一些高度使用的数字助手仍然无法自动引入这种上下文，就像图15.9中所示的那样，并且需要在设置中明确配置您的家庭位置。这些不足之处预计在未来几年中随着个人助手从所有可用上下文中构建更加精确的个性化模型而消失。
- en: '![figure](../Images/CH15_F09_Grainger.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH15_F09_Grainger.png)'
- en: Figure 15.9 Digital assistants will need to pull in many different data sources
    to build out robust personalization models
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.9 数字助手需要从许多不同的数据源中提取信息，以构建强大的个性化模型
- en: 'In addition to these personalization contexts, there’s an additional context
    that is becoming increasingly important: conversational context. Here’s an example
    of a chatbot (virtual assistant) that lacks awareness of the current conversational
    context:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些个性化上下文之外，还有一个越来越重要的上下文：对话上下文。以下是一个缺乏当前对话上下文意识的聊天机器人（虚拟助手）的例子：
- en: '[PRE26]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Good chatbots integrate a strong search index to power their ability to answer
    questions and return information, but many still lack short-term and long-term
    memory of the previous conversational context. They may search each query independently,
    without considering the previous queries and responses. Depending on the application
    and type of query, some previous conversational context makes sense to remember
    indefinitely as part of a personalization model, such as someone’s home address,
    family member names, or frequented places (doctor, dentist, school, etc.). In
    many cases, though, short-term memory just during the current conversation is
    sufficient. As search experiences become more conversational, short-term and long-term
    memory of the conversational context becomes more important.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的聊天机器人整合强大的搜索索引来支持他们回答问题和返回信息的能力，但许多聊天机器人仍然缺乏对先前对话上下文的短期和长期记忆。它们可能会独立地搜索每个查询，而不考虑先前的查询和响应。根据应用和查询类型，一些先前的对话上下文作为个性化模型的一部分有理由无限期地记住，例如某人的家庭地址、家庭成员的名字或常去的地方（医生、牙医、学校等）。然而，在许多情况下，仅在当前对话中的短期记忆就足够了。随着搜索体验变得更加对话化，对话上下文的短期和长期记忆变得更加重要。
- en: Foundation models trained for chat contexts, such as ChatGPT by OpenAI, have
    brought about drastic improvements to the conversational capabilities of chatbots.
    Although they are pretrained models that don’t update in real time, additional
    context can be injected into their prompts, making it entirely possible to capture
    and add personalized contexts into future prompts to improve their ability to
    cater to each user.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 为聊天环境训练的基础模型，如OpenAI的ChatGPT，极大地提高了聊天机器人的对话能力。尽管它们是预训练模型且不实时更新，但可以向它们的提示中注入额外的上下文，使得完全有可能捕捉并添加个性化的上下文到未来的提示中，以改善它们满足每个用户的能力。
- en: 15.4.2 Agent-based search
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.2 基于代理的搜索
- en: Just as we discussed the importance of pipelines in chapter 7 for best interpreting
    and representing the meaning of user queries, pipelines are also an integral component
    for the kind of multistep problem-solving that is emerging with chatbots and foundation
    model interactions.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第7章讨论了管道对于最佳解释和表示用户查询意义的重要性一样，管道也是随着聊天机器人和基础模型交互出现的多步骤问题解决的一个基本组成部分。
- en: With agent-based search, a search engine or other AI interface is given a prompt
    and can then generate new prompts or tasks, chaining them together to achieve
    a desired outcome. Solving a user’s request may require multiple steps, like generating
    new tasks (“now go find the top websites on this topic” or “explain if your last
    answer was correct”), reasoning about data (“now combine lists that were pulled
    from each website”, or “summarize the results”), and other functional steps (“now
    return the search results”).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于代理的搜索中，搜索引擎或其他AI界面被给出一个提示，然后可以生成新的提示或任务，将它们连接起来以实现预期的结果。解决用户的请求可能需要多个步骤，如生成新的任务（“现在去找到这个主题的前十大网站”或“解释你上一个答案是否正确”），对数据进行推理（“现在将每个网站提取的列表合并”，或“总结结果”），以及其他功能性步骤（“现在返回搜索结果”）。
- en: A significant portion of web traffic in the future will originate from AI-based
    agents searching for information to use in their problem-solving. The degree to
    which web search engines act as the data-serving layer for these agents, since
    web search engines typically have a cached copy of much of the web already, remains
    to be seen, but search engines are natural launch points for these AI-based agents.
    Major search engines like Bing, as well as several startups, have already rolled
    out multistep searches involving some level of iterative task-following to compose
    better-researched responses. This is likely to be a growing trend for some time
    to come.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，相当一部分网络流量将来自基于AI的代理搜索信息以用于其问题解决。这些代理在多大程度上将网络搜索引擎作为数据服务层，鉴于网络搜索引擎通常已经缓存了大部分网络内容，这一点尚待观察，但搜索引擎对于这些基于AI的代理来说是自然的启动点。像Bing这样的主要搜索引擎以及几家初创公司已经推出了多步骤搜索，涉及一定程度的迭代任务跟随以生成更深入研究的响应。这很可能是未来一段时间内增长的趋势。
- en: 15.5 Hybrid search
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.5 混合搜索
- en: 'In chapters 2 and 3, we introduced two different search paradigms: keyword-based
    lexical search (based on sparse vector representations, like an inverted index
    and typically ranked using BM25) and dense vector search (based on dense vector
    representations and typically ranked using cosine, dot product, or a similar vector-based
    similarity calculation). In this section, we’ll demonstrate how to combine results
    across these different paradigms.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章和第3章中，我们介绍了两种不同的搜索范式：基于关键词的词汇搜索（基于稀疏向量表示，如倒排索引，通常使用BM25进行排序）和密集向量搜索（基于密集向量表示，通常使用余弦、点积或类似的基于向量的相似度计算进行排序）。在本节中，我们将展示如何结合这些不同范式的结果。
- en: We’ve mostly treated lexical and vector search as orthogonal search approaches,
    but in practice, you’ll often get the best results by using both approaches as
    part of a hybrid search. *Hybrid search* is the process of combining results from
    multiple search paradigms to deliver the most relevant results. Hybrid search
    usually involves combining lexical and vector search results, though the term
    isn’t limited to just these two approaches.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要将词汇和向量搜索视为正交的搜索方法，但在实践中，你通常会通过将这两种方法作为混合搜索的一部分来获得最佳结果。*混合搜索*是将多个搜索范式的结果结合起来的过程，以提供最相关的结果。混合搜索通常涉及结合词汇和向量搜索的结果，尽管这个术语并不局限于这两种方法。
- en: There are a few ways to implement hybrid search. Some engines have direct support
    for combining lexical query syntax and vector search syntax as part of the same
    query, effectively allowing you to swap out specific keywords or filters within
    your query with vectors and combine vector similarity scores, BM25 scores, and
    function scores in arbitrarily complex ways. Some engines don’t support running
    vector and lexical search as part of the same query, but instead support hybrid
    search *fusion* algorithms that enable you to merge results from separate lexical
    and vector queries in a thoughtful way. In other cases, you may be using a vector
    database that is separate from your lexical search engine, in which case you may
    end up using a similar fusion algorithm to combine those results outside of the
    engine.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 实现混合搜索有几种方法。一些搜索引擎直接支持在同一个查询中将词汇查询语法和向量搜索语法结合起来，有效地允许你用向量替换查询中的特定关键词或过滤器，并以任意复杂的方式组合向量相似度分数、BM25分数和函数分数。一些搜索引擎不支持将向量搜索和词汇搜索作为同一个查询的一部分运行，而是支持混合搜索*融合*算法，这些算法允许你以深思熟虑的方式合并来自不同的词汇和向量查询的结果。在其他情况下，你可能在使用一个与你的词汇搜索引擎分开的向量数据库，在这种情况下，你可能需要使用类似的融合算法来在引擎外部组合这些结果。
- en: 15.5.1 Reciprocal rank fusion
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.1 互逆排名融合
- en: 'In this section, we’ll demonstrate a popular hybrid search algorithm for merging
    two sets of search results, called *reciprocal rank fusion* (RRF). RRF combines
    two or more sets of search results by ranking the documents based on their relative
    ranks across the different result sets. The algorithm is simple: for each document,
    it sums the reciprocal of the ranks of the document in each result set and then
    sorts the documents based on this sum. This algorithm is particularly effective
    when the two sets of search results are complementary, as it will rank documents
    higher that are already ranked higher in either set, but highest when they are
    ranked in both sets. The following listing implements the RRF algorithm.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示一种流行的混合搜索算法，用于合并两组搜索结果，称为*互逆排名融合*（RRF）。RRF通过根据文档在不同结果集中的相对排名来对文档进行排名，从而结合两个或多个搜索结果集。该算法很简单：对于每个文档，它将每个结果集中文档的排名的倒数相加，然后根据这个总和对文档进行排序。当两组搜索结果互补时，该算法特别有效，因为它将排名更高的文档排在更高位置，当它们在两个结果集中都排名较高时，排名最高。以下列表实现了RRF算法。
- en: Listing 15.17 RRF for combining multiple sets of search results
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.17 用于组合多个搜索结果集的RRF
- en: '[PRE27]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#1 search_results is a list of sets of ranked documents associated with different
    searches (lexical search, vector search, etc.).'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 `search_results`是一个与不同搜索（词汇搜索、向量搜索等）相关联的排名文档集的列表。'
- en: '#2 ranked_docs is a list of documents for a specific search.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 `ranked_docs`是一个特定搜索的文档列表。'
- en: '#3 A document’s score increases by being in multiple ranked_docs lists and
    by being higher in each list.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 一个文档的分数通过在多个排名文档列表中存在以及在每个列表中的排名更高而增加。'
- en: '#4 Return the docs, sorted from highest RRF score to lowest.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 按从高到低的RRF分数排序返回文档。'
- en: 'In this function, an arbitrary list (`search_results`) of sets of ranked documents
    from each individual search is passed in. For example, you may have two items
    in `search_results`:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，一个任意列表（`search_results`）被传递进来，其中包含每个单独搜索的排名文档集。例如，`search_results`中可能有两个项目：
- en: The ranked docs from a lexical search
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自词汇搜索的排名文档
- en: The ranked docs from a vector search
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自向量搜索的排名文档
- en: The RRF score for each document is then calculated by summing the reciprocal
    ranks of the document (`1` `/` `(k` `+` `rank)`) from each set of search results
    (`ranked_docs`). The final RRF scores are then sorted and returned.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文档的RRF分数是通过将每个搜索结果集（`ranked_docs`）中文档的倒数排名（`1` `/` `(k` `+` `rank)`)相加来计算的。最终的RRF分数随后会被排序并返回。
- en: The `k` parameter is a constant that can be increased to prevent an outlier
    ranking highly in one search result from carrying too much weight. The higher
    the `k`, the more weight is given to the docs that appear in multiple ranked document
    lists, as opposed to docs that rank higher in any given list of ranked documents.
    The `k` parameter is often set to `60` by default, based on research showing that
    this value works well in practice ([https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`k`参数是一个常数，可以增加以防止一个异常值在一个搜索结果中的排名很高。`k`值越高，赋予出现在多个排名文档列表中的文档的权重就越大，而不是赋予在给定排名文档列表中排名更高的文档的权重。默认情况下，`k`参数通常设置为`60`，根据研究表明这个值在实践中效果很好（[https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)）。'
- en: 'We’ve integrated the RRF logic into the `collection.hybrid_search` function,
    which we’ll invoke in listing 15.19\. First, however, let’s see what the initial
    results look like for a lexical search versus the corresponding vector search
    for a sample phrase query: `"singin''` `in` `the` `rain"`.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将RRF逻辑集成到`collection.hybrid_search`函数中，我们将在列表15.19中调用它。然而，首先，让我们看看对于样本短语查询：“singin'”
    `in` “the” `rain`，词汇搜索与相应的向量搜索的初始结果是什么样的。
- en: Listing 15.18 Running lexical and vector searches independently
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.18独立运行词汇和向量搜索
- en: '[PRE28]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This listing takes the lexical phrase query `"singin'` `in` `the` `rain"` and
    encodes it as an embedding using the same `encode_ text` function from listing
    15.15\. It then executes a lexical search and a vector search against the `tmdb_lexical_plus_
    embeddings` collection, which contains both the text fields needed for a lexical
    search and an image embedding for some of the movies in the TMDB dataset. The
    results from the lexical search and vector search are shown in figures 15.10 and
    15.11, respectively.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表将词汇短语查询“singin'” `in` “the” `rain`编码为嵌入，使用与列表15.15中相同的`encode_text`函数。然后，它对包含文本字段（用于词汇搜索）和TMDB数据集中一些电影的图像嵌入的`tmdb_lexical_plus_
    embeddings`集合执行词汇搜索和向量搜索。词汇搜索和向量搜索的结果分别显示在图15.10和15.11中。
- en: '![figure](../Images/CH15_F10_Grainger.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F10_Grainger.png)'
- en: Figure 15.10 A single lexical search result for the phrase query `"singin'`
    `in` `the` `rain"`
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.10短语查询“singin'” `in` “the” `rain`的单个词汇搜索结果
- en: 'The lexical search, in this case, only returns one result: the movie *Singin’
    in the Rain*. Since the user’s query was a phrase (quoted) query for the exact
    name of the title, it matched perfectly and found only the specific item the user
    was likely looking for. Figure 15.11 shows the corresponding vector search results
    for the same query.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，词汇搜索只返回一个结果：电影 *Singin’ in the Rain*。由于用户的查询是一个短语（引号内）查询，用于精确匹配标题名称，因此它完美匹配并找到了用户可能正在寻找的特定项目。图15.11显示了相同查询的对应向量搜索结果。
- en: '![figure](../Images/CH15_F11_Grainger.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F11_Grainger.png)'
- en: Figure 15.11 Vector search results for the query `"singin'` `in` `the` `rain"`
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.11查询“singin'” `in` “the” `rain`的向量搜索结果
- en: You’ll notice that the vector search results generally show images containing
    “rain” or “singing” or matching similar concepts, like umbrellas, weather, and
    musicals. This is because our vector search is on image embeddings, so the results
    are based on the visual content of the images, not the text content of the movies.
    While these images conceptually match the meaning of the words in the query better
    than the lexical search results, they don’t contain the exact title match that
    the lexical search results do, so the ideal document for the movie *Singin’ in
    the Rain* is missing from the top few results.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，向量搜索结果通常显示包含“rain”或“singing”或类似概念的图像，如雨伞、天气和音乐剧。这是因为我们的向量搜索是在图像嵌入上进行的，所以结果是基于图像的视觉内容，而不是电影的文本内容。虽然这些图像在概念上比词汇搜索结果更好地匹配查询中的单词含义，但它们不包含词汇搜索结果中的精确标题匹配，因此理想的*Singin’
    in the Rain*电影文档在顶部几个结果中缺失。
- en: 'By combining these two sets of results using RRF, we can get the best of both
    worlds: the exact title match from the lexical search and the conceptually relevant
    images from the vector search. In the following listing, we demonstrate how to
    combine these two sets of search results by invoking `collection.hybrid_search`.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用RRF结合这两组结果，我们可以得到两者的最佳结合：词汇搜索的精确标题匹配和向量搜索的概念相关图像。在下面的列表中，我们展示了如何通过调用`collection.hybrid_search`来组合这两组搜索结果。
- en: Listing 15.19 Hybrid search function
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.19混合搜索函数
- en: '[PRE29]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We pass in an array of searches to execute—in this case, our `lexical_search`
    and `vector_search` from listing 15.18\. The `collection.hybrid_search` function
    internally defaults to the RRF algorithm with `k=60` set, so if you’d like the
    pass in these parameters explicitly or change them, the full syntax is
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递一个搜索数组来执行——在这种情况下，我们的`lexical_search`和`vector_search`来自列表15.18。`collection.hybrid_search`函数内部默认使用RRF算法，`k=60`已设置，所以如果你想明确传递这些参数或更改它们，完整的语法是
- en: '[PRE30]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The call executes both searches and combines the results using RRF, with the
    output shown in figure 15.12.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 该调用同时执行搜索并使用RRF结合结果，输出显示在图15.12中。
- en: '![figure](../Images/CH15_F12_Grainger.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F12_Grainger.png)'
- en: Figure 15.12 Hybrid search results for lexical and vector searches using RRF
    for the query `"singin'` `in` `the` `rain"`
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.12 使用RRF对查询`"singin'` `in` `the` `rain"`进行的词汇和向量搜索混合搜索结果
- en: 'These results demonstrate how hybrid search can often provide the best of both
    worlds: the exact keyword matches from the lexical search and the conceptually
    relevant results from the vector search. Vector search often struggles with matching
    exact names, specific keywords, and product names or IDs. Likewise, lexical search
    misses conceptually relevant results for the query text. In this case, the top
    result is the exact match the user was likely looking for (previously missing
    from the vector search results), but it has now been supplemented with other results
    conceptually similar to the user’s query (previously missing from the lexical
    search).'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果展示了混合搜索如何经常提供两者的最佳结合：来自词汇搜索的精确关键词匹配和来自向量搜索的概念相关结果。向量搜索通常难以匹配精确名称、特定关键词、产品名称或ID。同样，词汇搜索也会错过查询文本的概念相关结果。在这种情况下，最上面的结果是用户可能正在寻找的精确匹配（之前在向量搜索结果中缺失），但现在它已经补充了与用户查询概念上相似的其他结果（之前在词汇搜索中缺失）。
- en: Of course, you can pass in more than two searches to the hybrid search function
    if you want to. For example, you may want to add an additional vector field containing
    an embedding for the `title` or `overview` so that instead of just lexical search
    on text content, and vector search on images, you also have semantic search on
    the text content. Using RRF with these three sets of search results would allow
    you to combine the best of all three search approaches into a single set of results.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你想向混合搜索函数传递超过两个搜索，你也可以这样做。例如，你可能想添加一个包含`title`或`overview`嵌入的额外向量字段，这样你不仅可以在文本内容上进行词汇搜索，在图像上进行向量搜索，还可以在文本内容上进行语义搜索。使用RRF与这三组搜索结果结合，可以将三种搜索方法的最佳之处结合到一组结果中。
- en: In addition to overcoming the respective functional limitations of keyword and
    vector search, hybrid search often yields better overall search ranking. This
    is because algorithms like RRF are designed to rank documents highest when they’re
    found in multiple sets of search results. When the same items are returned as
    relevant from different searches, the fusion algorithm can use that agreement
    between the sets of results to boost those items’ scores. This is particularly
    useful when one or more sets of search results have irrelevant documents, as the
    agreement between the two result sets can help filter out the noise, and surface
    the most relevant results. Listing 15.20 demonstrates this concept using the query
    `the` `hobbit`.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 除了克服关键词搜索和向量搜索各自的功能限制外，混合搜索通常能提供更好的整体搜索排名。这是因为像RRF这样的算法被设计成在多个搜索结果集中找到文档时给予最高的排名。当相同的项目从不同的搜索中作为相关内容返回时，融合算法可以利用这些结果集之间的这种一致性来提高这些项目的得分。这在搜索结果集中有一或多个集合包含无关文档时尤其有用，因为两个结果集之间的这种一致性可以帮助过滤掉噪声，并展示最相关的结果。列表15.20使用查询`the`
    `hobbit`演示了这一概念。
- en: Listing 15.20 Lexical, vector, and hybrid searches for `the hobbit`
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.20 对`the hobbit`进行的词汇、向量和混合搜索
- en: '[PRE31]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Figures 15.13 through 15.15 show the results from listing 15.20.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.13至15.15显示了列表15.20的结果。
- en: In figure 15.13, you’ll see five relevant results in the top six results (*The
    Hobbit* is part of the *Lord of the Rings* franchise). The text “The Hobbit” appears
    in the title of three of the first four results. There is one clearly bad result
    in the fifth position of the lexical search results, and all results are bad after
    the sixth document, mostly just matching on keywords like “the” and “of” in the
    `title` and `overview` fields.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 15.13 中，您将在前六个结果中看到五个相关结果（*霍比特人* 是 *指环王* 电影系列的一部分）。文本 “The Hobbit” 出现在前四个结果中的三个标题中。词汇搜索结果中的第五位有一个明显的不良结果，并且第六个文档之后的所有结果都是不良的，主要是匹配
    `title` 和 `overview` 字段中的关键词如 “the” 和 “of”。
- en: Figure 15.14 shows the vector search results for the same query. These also
    show five relevant results related to *The Lord of the Rings*, but one of the
    relevant results from the lexical search is missing, and an additional relevant
    movie has been returned in the vector search results.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.14 显示了相同查询的向量搜索结果。这些结果也显示了与 *指环王* 相关的五个相关结果，但词汇搜索中的一个相关结果缺失，向量搜索结果中返回了一个额外的相关电影。
- en: '![figure](../Images/CH15_F13_Grainger.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F13_Grainger.png)'
- en: Figure 15.13 Lexical search results for the query `the` `hobbit`
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 15.13 查询 `the` `hobbit` 的词汇搜索结果
- en: '![figure](../Images/CH15_F14_Grainger.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F14_Grainger.png)'
- en: Figure 15.14 Vector search results for the query `the` `hobbit`
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 15.14 查询 `the` `hobbit` 的向量搜索结果
- en: Many of the remaining results are conceptually related to the query, showing
    mostly movies with fantasy landscapes and magic. Given the heavy overlap of good
    results between the lexical and vector search results, as well as the lack of
    overlap between the less relevant results, we should expect the hybrid search
    results to be quite good.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 许多剩余的结果在概念上与查询相关，主要显示带有奇幻景观和魔法的电影。鉴于词汇搜索和向量搜索结果之间良好的结果重叠，以及不太相关结果之间的缺乏重叠，我们应该期待混合搜索结果相当不错。
- en: Indeed, in figure 15.15, we see that the first five results are all relevant
    and that six of the first seven results are related to *The Lord of the Rings*.
    We’ve moved from two different search results lists that were each missing a different
    document and showing some irrelevant results to a final list of results.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在图 15.15 中，我们看到前五个结果都是相关的，而且前七个结果中有六个与 *指环王* 相关。我们已经从两个不同的搜索结果列表中（每个列表都缺失不同的文档并显示了一些不相关的结果）转变为一个最终的结果列表。
- en: '![figure](../Images/CH15_F15_Grainger.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F15_Grainger.png)'
- en: Figure 15.15 Hybrid search results for the lexical and vector search results
    using RRF for the query `the` `hobbit`
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 15.15 使用 RRF 对查询 `the` `hobbit` 进行词汇和向量搜索结果的混合搜索结果
- en: RRF has allowed us to surface missing results from both lists and push down
    irrelevant results only appearing in one list, truly emphasizing the best qualities
    of each search paradigm (lexical versus vector) while overcoming each of their
    weaknesses.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: RRF 使我们能够从两个列表中提取缺失的结果，并将仅出现在一个列表中的不相关结果向下推，真正强调了每种搜索范式（词汇与向量）的最佳品质，同时克服了各自的弱点。
- en: 15.5.2 Other hybrid search algorithms
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.2 其他混合搜索算法
- en: While RRF is a popular and effective algorithm for combining search results,
    there are, of course, many other algorithms that can be used for similar purposes.
    One popular algorithm is *relative score fusion* (RSF), which is similar to RRF
    but uses the relative scores of documents in each search result set to combine
    the results. Since relevance scores are often not comparable across different
    ranking algorithms, the scores per query modality are typically scaled to the
    same range (often `0.0` to `1.0`) based on the minimum and maximum scores from
    each modality. The relative scores are then combined using a weighted average,
    with the weights often set based on the relative performance of each modality
    on a validation set.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 RRF 是一种流行且有效的组合搜索结果的算法，但当然还有许多其他算法可以用于类似的目的。一种流行的算法是 *相对分数融合* (RSF)，它与 RRF
    类似，但使用每个搜索结果集中文档的相对分数来组合结果。由于相关性分数通常在不同排名算法之间不可比较，因此每个查询模态的分数通常根据每个模态的最小和最大分数缩放到相同的范围（通常是
    `0.0` 到 `1.0`）。然后使用加权平均将相对分数组合起来，权重通常根据每个模态在验证集上的相对性能设置。
- en: Another common way to combine search results is to use one modality for an initial
    match and then rerank the results using another modality. Many engines support
    both lexical search and vector search as separate operations but allow you to
    run one query version (such as a lexical query) and then rerank the resulting
    documents using the other version (such as a vector query).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的组合搜索结果的方法是使用一种模态进行初始匹配，然后使用另一种模态重新排序结果。许多引擎都支持词法搜索和向量搜索作为独立的操作，但允许你运行一个查询版本（如词法查询），然后使用另一个版本（如向量查询）重新排序结果。
- en: For example, if you want to ensure you match on specific keywords but also boost
    results that are conceptually similar to the query, you might run a lexical search
    first and then rerank the results using a vector search. The following listing
    demonstrates this concept.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想确保匹配特定的关键词，同时提高与查询概念上相似的结果，你可能首先运行一个词法搜索，然后使用向量搜索重新排序结果。以下列表展示了这一概念。
- en: Listing 15.21 Hybrid lexical search with vector search reranking
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.21 混合词法搜索与向量搜索重新排序
- en: '[PRE32]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Internally, this translates into a normal lexical search, but with the results
    then re-sorted using the score of a vector search for the same (encoded) query.
    Syntactically, these are the key parts of the search request generated by listing
    15.21:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，这相当于一个正常的词法搜索，但随后使用相同（编码）查询的向量搜索分数对结果进行重新排序。从语法上讲，这些是列表15.21生成的搜索请求的关键部分：
- en: '[PRE33]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The output of this lexical search with vector search reranking is shown in figure
    15.16\. In this particular case, the results look very similar to the RRF results.
    Note that, unlike in figure 15.10 where the default operator for the lexical query
    was `AND`, resulting in a single exact lexical match, here the default operator
    is `OR`, so that more results are returned for the vector search to rerank. If
    you’d like to increase precision and only return more precise lexical matches,
    you can set the default operator to `AND` or add a `min_match` threshold in the
    lexical search request.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 使用向量搜索重新排序的词法搜索输出显示在图15.16中。在这种情况下，结果看起来非常类似于RRF结果。请注意，与图15.10不同，那里的词法查询默认操作符是`AND`，导致只有一个精确的词法匹配，而在这里，默认操作符是`OR`，因此向量搜索返回了更多结果以便重新排序。如果你想提高精确度并只返回更精确的词法匹配，可以将默认操作符设置为`AND`或在词法搜索请求中添加一个`min_match`阈值。
- en: '![figure](../Images/CH15_F16_Grainger.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F16_Grainger.png)'
- en: Figure 15.16 Hybrid search by vector reranking of a lexical query
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.16 通过词法查询的向量重新排序的混合搜索
- en: Functionally, the lexical search is entirely responsible for which documents
    return, while the vector search is entirely responsible for the order of the results.
    This may be preferred if you want to emphasize lexical keyword matching but with
    a more semantically relevant ranking. On the other hand, a fusion approach like
    RRF or RSF will provide a more blended approach, ensuring that the best results
    from each modality are surfaced. There are many other ways to combine search results,
    and the best approach will depend on the specific use case and the relative strengths
    and weaknesses of each search modality.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 从功能上讲，词法搜索完全负责哪些文档返回，而向量搜索完全负责结果的顺序。如果你想要强调词法关键词匹配但具有更语义相关的排序，这可能是一个更好的选择。另一方面，像RRF或RSF这样的融合方法将提供一种更混合的方法，确保每个模态的最佳结果都能呈现出来。还有许多其他方法可以组合搜索结果，最佳方法将取决于具体用例以及每种搜索模态的相对优势和劣势。
- en: 15.6 Convergence of contextual technologies
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.6 上下文技术融合
- en: Just as recommendations, chatbots, and question-answering systems are all types
    of AI-powered information retrieval, many other technologies are also beginning
    to converge with search engines. Vector databases are emerging that function like
    search engines for multimodal data, and many different data sources and data modalities
    are being pulled together as additional contexts for optimal matching, ranking,
    and reasoning over data. Generative models are enabling a base-level understanding
    sufficient to generate new written and artistic works.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 就像推荐、聊天机器人和问答系统都是人工智能驱动的信息检索类型一样，许多其他技术也开始与搜索引擎融合。正在出现一些向量数据库，它们像搜索引擎一样处理多模态数据，并且许多不同的数据源和数据模式正被拉在一起，作为优化匹配、排序和数据推理的额外上下文。生成模型正在使基础理解能力得到提升，足以生成新的书面和艺术作品。
- en: But most of these technologies are still implemented piecemeal into production
    systems. As researchers continue working toward building artificial general intelligence,
    intelligent robots, and smarter search systems, we’re likely to see a continued
    convergence of technologies integrating new and different contexts to build a
    more comprehensive understanding of content, domains, and users. Figure 15.17
    demonstrates what this convergence of technologies is likely to look like over
    the coming years.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 但大多数这些技术仍然以零散的方式集成到生产系统中。随着研究人员继续致力于构建通用人工智能、智能机器人和更智能的搜索系统，我们可能会看到技术不断汇聚，整合新的和不同的上下文，以构建对内容、领域和用户的更全面理解。图15.17展示了未来几年这种技术汇聚可能的样子。
- en: '![figure](../Images/CH15_F17_Grainger.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH15_F17_Grainger.png)'
- en: Figure 15.17 The convergence of contextual technologies to deliver more intelligent
    results
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.17 上下文技术汇聚以提供更智能的结果
- en: In the figure, we see the textual, audio, and video modalities we’ve already
    discussed, as well as a metadata modality that may provide additional context.
    We see a sensory modality, which might be available for a physical network of
    sensor-equipped devices or a robot with direct interactive access to the physical
    world. We also see the temporal modality, as the timing of both the queries and
    the data being searched can influence the relevance of the results.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，我们看到我们已经讨论过的文本、音频和视频模式，以及可能提供额外上下文的元数据模式。我们看到一个感官模式，这可能适用于配备传感器的物理设备网络或具有直接交互访问物理世界的机器人。我们还看到时间模式，因为查询和数据搜索的时间都可以影响结果的相关性。
- en: Just as LLMs learn both the domain and the language structure of text (see chapter
    13), additional cultural contexts and customs can also be learned by foundation
    models based on the geography and the observed behaviors from real-world interactions.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 正如LLMs（大型语言模型）学习文本的领域和语言结构（参见第13章）一样，基于地理和从现实世界交互中观察到的行为，基础模型也可以学习到额外的文化背景和习俗。
- en: Much of this may seem *very* far away from traditional search, or even AI-powered
    search today—and it is! However, the goal of search is to best meet a user’s information
    need, and as more overlapping technologies emerge to solve this problem from different
    angles, we see search being a critical base layer with which many of these technologies
    will integrate.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这其中许多可能看起来与传统的搜索，甚至今天的人工智能搜索相去甚远——确实如此！然而，搜索的目标是最好地满足用户的信息需求，随着更多重叠的技术从不同角度解决这个问题，我们看到搜索正成为一个关键的基础层，许多这些技术都将与之集成。
- en: Many AI practitioners already recognize the importance of using RAG to find
    and supply real-time context to generative AI models, and data security, data
    accuracy, and model size considerations make it almost certain that search engines
    will be a critical backbone for knowledge-based generative AI systems well into
    the future. Large amounts of data coming from many sources will need to be indexed
    and available for real-time retrieval and use by generative AI models.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人工智能从业者已经认识到使用RAG（Retrieval-Augmented Generation）来为生成式人工智能模型查找和提供实时上下文的重要性，而数据安全、数据准确性和模型大小等因素使得搜索引擎在未来很长一段时间内将成为基于知识的生成式人工智能系统的关键支撑。来自多个来源的大量数据需要被索引，以便生成式人工智能模型能够实时检索和使用。
- en: Having these technologies converge into end-to-end systems will enable solving
    these AI-powered search problems better than any of these systems could in isolation.
    Whether this gets labeled “robotics”, “AI”, “general intelligence”, “AI-powered
    search”, or something else entirely is yet to be seen. But many of the AI-powered
    search techniques you’ve learned have been, and will continue to be, central to
    the development of these next-generation contextual reasoning engines.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些技术汇聚成端到端系统将能够比任何单一系统更好地解决这些人工智能搜索问题。无论这被标记为“机器人技术”、“人工智能”、“通用智能”、“人工智能搜索”还是其他完全不同的东西，还有待观察。但你所学习到的许多人工智能搜索技术已经并将继续成为下一代上下文推理引擎发展的核心。
- en: 15.7 All the above, please!
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.7 以上所有内容，请！
- en: Throughout this book, we’ve done a deep dive through the core concepts of building
    AI-powered search. We’ve covered both the theory behind how modern AI-powered
    search relevance works, and we’ve walked through code examples demonstrating each
    topic with real-world use cases.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们深入探讨了构建人工智能搜索的核心概念。我们不仅涵盖了现代人工智能搜索相关性的理论背景，还通过代码示例，结合实际应用案例，展示了每个主题的实践应用。
- en: The techniques and algorithms we’ve looked at won’t be applicable to all use
    cases, but usually by combining multiple approaches, you will deliver a more effective
    and relevant search experience.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所探讨的技术和算法并不适用于所有用例，但通常通过结合多种方法，你将提供更有效和相关的搜索体验。
- en: While the field of AI-powered search is evolving quickly, particularly due to
    the rapid innovation happening with generative foundation models and dense vector
    approaches to search, the core principles remain the same. The job of a search
    engine is to understand user intent and to return the content that best fulfills
    each user’s information needs. This requires a proper understanding of the content
    context, the user context, and the domain context for each interaction. In addition
    to learning language models and knowledge graphs from your content, your search
    engine’s capabilities will be supercharged if it can learn from your users implicitly,
    through their interactions (user signals) with the search engine.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人工智能搜索领域正在快速发展，尤其是由于生成式基础模型和密集向量搜索方法的快速创新，但其核心原则保持不变。搜索引擎的职责是理解用户意图，并返回最能满足每个用户信息需求的内
    容。这需要正确理解每次交互的内容上下文、用户上下文和领域上下文。除了从你的内容中学习语言模型和知识图谱外，如果你的搜索引擎能够通过用户与搜索引擎的交互（用户信号）隐式地学习，那么其能力将会得到极大的提升。
- en: You’ve learned how key AI-powered search algorithms work and how to implement
    and automate these algorithms into a self-learning search engine. Whether you
    need to automate the building of signals boosting models, learning to rank models,
    click models, collaborative filtering models, knowledge graph learning, or fine-tuning
    of deep-learning-based Transformer models for enhanced semantic search and question
    answering, you now have the knowledge and skills needed to implement a world-class
    AI-powered search engine. We look forward to seeing what you build!
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了关键的人工智能搜索算法的工作原理，以及如何将这些算法实现并自动化到自学习搜索引擎中。无论你需要自动化构建信号增强模型、学习排序模型、点击模型、协同过滤模型、知识图谱学习，还是对基于深度学习的Transformer模型进行微调以增强语义搜索和问答，你现在拥有了实施世界级人工智能搜索引擎所需的知识和技能。我们期待看到你将构建出什么！
- en: Summary
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Foundation models serve as base models that are trained and can later be fine-tuned
    for specific domains or tasks.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型作为训练和可以后来针对特定领域或任务进行微调的基模型。
- en: Prompt engineering allows you to inject additional context and data into each
    request, providing a way to perform real-time fine-tuning of the content that
    will be returned for the request. LLM-based applications should ideally be programmed
    to autogenerate and optimize prompts, as opposed to requiring manual prompt verification
    and adjustments, so they can automatically handle model and environment changes
    over time, while still performing optimally.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程允许你向每个请求注入额外的上下文和数据，提供了一种对将返回请求内容进行实时微调的方法。基于LLM的应用程序理想情况下应该被编程来自动生成和优化提示，而不是需要手动提示验证和调整，这样它们可以自动处理模型和环境随时间的变化，同时仍然保持最佳性能。
- en: Search results summarization and training data generation are two key areas
    in which foundation models can help drive relevance improvements in search engines.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索结果摘要和训练数据生成是基础模型可以帮助推动搜索引擎相关性改进的两个关键领域。
- en: Jointly training embedding models on multiple data types enables powerful multimodal
    search capabilities (text-to-image, image-to-image, hybrid text-plus-image-to-image,
    and so on) extending user expressiveness and the search engine’s ability to interpret
    user intent.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多种数据类型上联合训练嵌入模型，可以启用强大的多模态搜索能力（文本到图像、图像到图像、混合文本加图像到图像等），扩展用户的表达能力和搜索引擎解释用户意图的能力。
- en: AI-powered search is rapidly evolving with the rise of large language models,
    dense vector search, multimodal search, conversational and contextual search,
    generative search, and emerging hybrid search approaches.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于人工智能的搜索随着大型语言模型、密集向量搜索、多模态搜索、对话式和上下文搜索、生成式搜索以及新兴的混合搜索方法的兴起而迅速发展。
- en: There are many techniques for implementing AI-powered search, and the best systems
    in the future will be those that can effectively apply multiple relevant approaches
    within hybrid search systems.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现人工智能搜索有许多技术，未来的最佳系统将是那些能够在混合搜索系统中有效应用多种相关方法的系统。
