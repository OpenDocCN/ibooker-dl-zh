- en: 'Chapter 10\. Testing: Evaluation, Monitoring, and Continuous Improvement'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章\. 测试：评估、监控和持续改进
- en: In [Chapter 9](ch09.html#ch09_deployment_launching_your_ai_application_into_pro_1736545675509604),
    you learned how to deploy your AI application into production and utilize LangGraph
    Platform to host and debug your app.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.html#ch09_deployment_launching_your_ai_application_into_pro_1736545675509604)中，您学习了如何将您的AI应用程序部署到生产环境中，并利用LangGraph平台托管和调试您的应用程序。
- en: Although your app can respond to user inputs and execute complex tasks, its
    underlying LLM is nondeterministic and prone to hallucination. As discussed in
    previous chapters, LLMs can generate inaccurate and outdated outputs due to a
    variety of reasons including the prompt, format of user’s input, and retrieved
    context. In addition, harmful or misleading LLM outputs can significantly damage
    a company’s brand and customer loyalty.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您的应用程序可以响应用户输入并执行复杂任务，但其底层的LLM是非确定性的，容易产生幻觉。正如前几章所讨论的，LLM由于各种原因（包括提示、用户输入的格式和检索到的上下文）可能会生成不准确和过时的输出。此外，有害或误导性的LLM输出可能会严重损害公司的品牌和客户忠诚度。
- en: To combat this tendency toward hallucination, you need to build an efficient
    system to test, evaluate, monitor, and continuously improve your LLM applications’
    performance. This robust testing process will enable you to quickly debug and
    fix AI-related issues before and after your app is in production.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对抗这种幻觉倾向，您需要建立一个高效的系统来测试、评估、监控和持续改进您的LLM应用程序的性能。这个强大的测试过程将使您能够在应用程序生产前后快速调试和修复AI相关的问题。
- en: In this chapter, you’ll learn how to build an iterative testing system across
    the key stages of the LLM app development life-cycle and maintain high performance
    of your application.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何构建一个迭代测试系统，跨越LLM应用程序开发生命周期的关键阶段，并保持您应用程序的高性能。
- en: Testing Techniques Across the LLM App Development Cycle
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM应用开发周期中的测试技术
- en: 'Before we construct the testing system, let’s briefly review how testing can
    be applied across the three key stages of LLM app development:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建测试系统之前，让我们简要回顾一下如何在LLM应用程序开发的三个关键阶段应用测试：
- en: Design
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 设计
- en: In this stage, LLM tests are applied directly to your application. These tests
    can be assertions executed at runtime that feed failures back to the LLM for self-correction.
    The purpose of testing at this stage is error handling within your app before
    it affects users.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，LLM测试将直接应用于您的应用程序。这些测试可以是运行时执行的断言，将失败反馈给LLM进行自我纠正。在这个阶段进行测试的目的是在它影响用户之前，在您的应用程序中处理错误。
- en: Preproduction
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 预生产
- en: In this stage, tests are run right before deployment into production. The purpose
    of testing at this stage is to catch and fix any regressions before the app is
    released to real users.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，测试是在部署到生产之前运行的。在这个阶段进行测试的目的是在应用程序发布给真实用户之前捕捉和修复任何回归。
- en: Production
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 生产
- en: In this stage, tests are run while your application is in production to help
    monitor and catch errors affecting real users. The purpose is to identify issues
    and feed them back into the design or preproduction phases.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，测试在生产环境中运行，以帮助监控和捕捉影响真实用户的错误。其目的是识别问题并将它们反馈到设计或预生产阶段。
- en: 'The combination of testing across these stages creates a continuous improvement
    cycle where these steps are repeated: design, test, deploy, monitor, fix, and
    redesign. See [Figure 10-1](#ch10_figure_1_1736545678095728).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些阶段的测试组合，创建了一个持续改进的周期，其中这些步骤被重复：设计、测试、部署、监控、修复和重新设计。参见[图10-1](#ch10_figure_1_1736545678095728)。
- en: '![A diagram of a process  Description automatically generated](assets/lelc_1001.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![一个流程图，描述自动生成](assets/lelc_1001.png)'
- en: Figure 10-1\. The three key stages of the LLM app development cycle
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1\. LLM应用程序开发周期的三个关键阶段
- en: In essence, this cycle helps you to identify and fix production issues in an
    efficient and quick manner.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，这个周期帮助您以高效和快捷的方式识别和修复生产问题。
- en: Let’s dive deeper into testing techniques across each of these stages.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解每个阶段的测试技术。
- en: 'The Design Stage: Self-Corrective RAG'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计阶段：自我纠正的RAG
- en: As discussed previously, your application can incorporate error handling at
    runtime that feeds errors to the LLM for self-correction. Let’s explore a RAG
    use case using LangGraph as the framework to orchestrate error handling.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，您的应用程序可以在运行时进行错误处理，将错误反馈给LLM进行自我纠正。让我们通过使用LangGraph作为框架来编排错误处理，来探讨一个RAG用例。
- en: Basic RAG-driven AI applications are prone to hallucination due to inaccurate
    or incomplete retrieval of relevant context to generate outputs. But you can utilize
    an LLM to grade retrieval relevance and fix hallucination issues.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的RAG驱动的AI应用由于检索相关上下文不准确或不完整，生成输出时容易出现幻觉。但您可以使用LLM来评估检索的相关性并修复幻觉问题。
- en: LangGraph enables you to effectively implement the control flow of this process,
    as shown in [Figure 10-2](#ch10_figure_2_1736545678095764).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph使您能够有效地实现此过程的控制流，如图[图10-2](#ch10_figure_2_1736545678095764)所示。
- en: '![A diagram of a diagram  Description automatically generated](assets/lelc_1002.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图表的图表  自动生成的描述](assets/lelc_1002.png)'
- en: Figure 10-2\. Self-corrective RAG control flow
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2\. 自纠正的RAG控制流
- en: 'The control flow steps are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 控制流步骤如下：
- en: In the routing step, each question is routed to the relevant retrieval method,
    that is, vector store and web search.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在路由步骤中，每个问题都被路由到相关的检索方法，即向量存储和网页搜索。
- en: If, for example, the question is routed to a vector store for retrieval, the
    LLM in the control flow will retrieve and grade the documents for relevancy.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，如果问题被路由到向量存储进行检索，控制流中的LLM将检索并评估文档的相关性。
- en: If the document is relevant, the LLM proceeds to generate an answer.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果文档相关，LLM将继续生成答案。
- en: The LLM will check the answer for hallucinations and only proceed to display
    the answer to the user if the output is accurate and relevant.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM将检查答案中的幻觉，并且只有在输出准确且相关的情况下才会向用户显示答案。
- en: As a fallback, if the retrieved document is irrelevant or the generated answer
    doesn’t answer the user’s question, the flow utilizes web search to retrieve relevant
    information as context.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为后备方案，如果检索到的文档不相关或生成的答案没有回答用户的问题，流程将利用网页搜索检索相关信息作为上下文。
- en: This process enables your app to iteratively generate answers, self-correct
    errors and hallucinations, and improve the quality of outputs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程使您的应用能够迭代生成答案，自我纠正错误和幻觉，并提高输出质量。
- en: Let’s run through an example code implementation of this control flow. First,
    download the required packages and initialize relevant API keys. For these examples,
    you’ll need to set your OpenAI and LangSmith API keys as environment variables.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例代码实现来了解这个控制流的流程。首先，下载所需的包并初始化相关的API密钥。对于这些示例，您需要将您的OpenAI和LangSmith
    API密钥设置为环境变量。
- en: 'First, we’ll create an index of three blog posts:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建三个博客文章的索引：
- en: '*Python*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*JavaScript*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As discussed previously, the LLM will grade the relevancy of the retrieved
    documents from the index. We can construct this instruction in a system prompt:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LLM将评估从索引中检索到的文档的相关性。我们可以在系统提示中构建此指令：
- en: '*Python*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*JavaScript*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*The output:*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notice the use of Pydantic/Zod to help model the binary decision output in a
    format that can be used to programmatically decide which node in the control flow
    to move toward.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Pydantic/Zod的使用，它有助于以编程方式决定控制流中哪个节点需要移动的格式来帮助模型二进制决策输出。
- en: In LangSmith, you can see a trace of the logic flow across the nodes discussed
    previously (see [Figure 10-3](#ch10_figure_3_1736545678095796)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在LangSmith中，您可以看到之前讨论的节点之间的逻辑流程跟踪（见图[图10-3](#ch10_figure_3_1736545678095796)）。
- en: '![A screenshot of a chat  Description automatically generated](assets/lelc_1003.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![聊天截图  自动生成的描述](assets/lelc_1003.png)'
- en: Figure 10-3\. LangSmith trace results
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3\. LangSmith跟踪结果
- en: Let’s test to see what happens when the input question cannot be answered by
    the retrieved documents in the index.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下，当输入问题无法由索引中的检索到的文档回答时会发生什么。
- en: First, utilize LangGraph to make it easier to construct, execute, and debug
    the full control flow. See the full graph definition in the book’s [GitHub repository](https://oreil.ly/v63Vr).
    Notice that we’ve added a `transform_query` node to help rewrite the input query
    in a format that web search can use to retrieve higher-quality results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，利用LangGraph使其更容易构建、执行和调试完整的控制流。请参阅书中[GitHub仓库](https://oreil.ly/v63Vr)中的完整图定义。请注意，我们已添加一个`transform_query`节点，以帮助重写输入查询，使其格式适用于网页搜索以检索更高质量的结果。
- en: As a final step, we set up our web search tool and execute the graph using the
    out-of-context question. The LangSmith trace shows that the web search tool was
    used as a fallback to retrieve relevant information prior to the final LLM generated
    answer (see [Figure 10-4](#ch10_figure_4_1736545678095829)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们设置了我们的网络搜索工具，并使用无上下文问题执行图。LangSmith 追踪显示，网络搜索工具被用作回退来检索在最终 LLM 生成答案之前的相关信息（见[图
    10-4](#ch10_figure_4_1736545678095829)）。
- en: '![A screenshot of a chat  Description automatically generated](assets/lelc_1004.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![聊天截图，描述自动生成](assets/lelc_1004.png)'
- en: Figure 10-4\. LangSmith trace of self-corrective RAG utilizing web search as
    a fallback
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 利用网络搜索作为回退进行自我纠正 RAG 的 LangSmith 追踪
- en: 'Let’s move on to the next stage in LLM app testing: preproduction.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进入 LLM 应用测试的下一阶段：预生产阶段。
- en: The Preproduction Stage
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预生产阶段
- en: The purpose of the preproduction stage of testing is to measure and evaluate
    the performance of your application prior to production. This will enable you
    to efficiently assess the accuracy, latency, and cost of utilizing the LLM.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 测试预生产阶段的目的是在投入生产之前衡量和评估您应用程序的性能。这将使您能够有效地评估利用 LLM 的准确性、延迟和成本。
- en: Creating Datasets
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据集
- en: Prior to testing, you need to define a set of scenarios you’d like to test and
    evaluate. A *dataset* is a collection of examples that provide inputs and expected
    outputs used to evaluate your LLM app.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试之前，您需要定义一组您想要测试和评估的场景。*数据集* 是一组示例，提供输入和预期输出，用于评估您的 LLM 应用程序。
- en: 'These are three common methods to build datasets for valuation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是构建估值数据集的三个常用方法：
- en: Manually curated examples
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 手动整理的示例
- en: These are handwritten examples based on expected user inputs and ideal generated
    outputs. A small dataset consists of between 10 and 50 quality examples. Over
    time, more examples can be added to the dataset based on edge cases that emerge
    in production.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是基于预期用户输入和理想生成输出的手写示例。小型数据集包含 10 到 50 个高质量的示例。随着时间的推移，可以根据生产中出现的边缘情况向数据集中添加更多示例。
- en: Application logs
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 应用日志
- en: Once the application is in production, you can store real-time user inputs and
    later add them to the dataset. This will help ensure the dataset is realistic
    and covers the most common user questions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦应用程序投入生产，您就可以存储实时用户输入，并在以后将它们添加到数据集中。这将有助于确保数据集的现实性和涵盖最常见的问题。
- en: Synthetic data
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据
- en: These are artificially generated examples that simulate various scenarios and
    edge cases. This enables you to generate new inputs by sampling existing inputs,
    which is useful when you don’t have enough real data to test on.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是人工生成的示例，用于模拟各种场景和边缘情况。这使您可以通过采样现有输入来生成新的输入，这在您没有足够真实数据来测试时非常有用。
- en: In LangSmith, you can create a new dataset by selecting Datasets and Testing
    in the sidebar and clicking the “+ New Dataset” button on the top right of the
    app, as shown in [Figure 10-5](#ch10_figure_5_1736545678095862).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LangSmith 中，您可以通过在侧边栏中选择数据集和测试，然后点击应用程序右上角的“+ 新数据集”按钮来创建新的数据集，如图[图 10-5](#ch10_figure_5_1736545678095862)所示。
- en: In the opened window, enter the relevant dataset details, including a name,
    description, and dataset type. If you’d like to use your own dataset, click the
    “Upload a CSV dataset” button.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在打开的窗口中，输入相关的数据集详细信息，包括名称、描述和数据集类型。如果您想使用自己的数据集，请点击“上传 CSV 数据集”按钮。
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1005.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图，描述自动生成](assets/lelc_1005.png)'
- en: Figure 10-5\. Creating a new dataset in the LangSmith UI
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 在 LangSmith UI 中创建新的数据集
- en: 'LangSmith offers three different dataset types:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: LangSmith 提供了三种不同的数据集类型：
- en: '`kv` (key-value) dataset'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`kv` (键值) 数据集'
- en: '*Inputs* and *outputs* are represented as arbitrary key-value pairs.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入* 和 *输出* 以任意键值对的形式表示。'
- en: The `kv` dataset is the most versatile, and it is the default type. The `kv`
    dataset is suitable for a wide range of evaluation scenarios.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kv` 数据集是最通用的，也是默认类型。`kv` 数据集适用于广泛的评估场景。'
- en: This dataset type is ideal for evaluating chains and agents that require multiple
    inputs or generate multiple outputs.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此数据集类型非常适合评估需要多个输入或生成多个输出的链和代理。
- en: '`llm` (large language model) dataset'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`llm` (大型语言模型) 数据集'
- en: The `llm` dataset is designed for evaluating completion style language models.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`llm` 数据集是为评估完成风格语言模型而设计的。'
- en: The inputs dictionary contains a single input key mapped to the prompt string.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入字典包含一个映射到提示字符串的单个输入键。
- en: The outputs dictionary contains a single output key mapped to the corresponding
    response string.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出字典包含一个输出键，映射到相应的响应字符串。
- en: This dataset type simplifies evaluation for LLMs by providing a standardized
    format for inputs and outputs.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此数据集类型通过为输入和输出提供标准化格式来简化LLM的评估。
- en: '`chat` dataset'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`chat` 数据集'
- en: The `chat` dataset is designed for evaluating LLM structured chat messages as
    inputs and outputs.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chat` 数据集是为评估LLM结构化聊天消息作为输入和输出而设计的。'
- en: The *inputs* dictionary contains a single *input* key mapped to a list of serialized
    chat messages.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**inputs** 字典包含一个单个的 **input** 键，映射到一个序列化的聊天消息列表。'
- en: The *outputs* dictionary contains a single *output* key mapped to a list of
    serialized chat messages.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**outputs** 字典包含一个单个的 **output** 键，映射到一个序列化的聊天消息列表。'
- en: This dataset type is useful for evaluating conversational AI systems or chatbots.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此数据集类型对于评估对话式人工智能系统或聊天机器人很有用。
- en: The most flexible option is the key-value data type (see [Figure 10-6](#ch10_figure_6_1736545678095893)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最灵活的选项是键值数据类型（见 [图10-6](#ch10_figure_6_1736545678095893)）。
- en: '![A screenshot of a chat  Description automatically generated](assets/lelc_1006.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![一张聊天屏幕截图  自动生成的描述](assets/lelc_1006.png)'
- en: Figure 10-6\. Selecting a dataset type in the LangSmith UI
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 在LangSmith UI中选择数据集类型
- en: Next, add examples to the dataset by clicking Add Example. Provide the input
    and output examples as JSON objects, as shown in [Figure 10-7](#ch10_figure_7_1736545678095923).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过点击“添加示例”将示例添加到数据集中。提供输入和输出示例作为JSON对象，如图 [图10-7](#ch10_figure_7_1736545678095923)
    所示。
- en: '![A white background with black lines  Description automatically generated](assets/lelc_1007.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![一张白色背景带有黑色线条的图片  自动生成的描述](assets/lelc_1007.png)'
- en: Figure 10-7\. Add key-value dataset examples in the LangSmith UI
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. 在LangSmith UI中添加键值数据集示例
- en: You can also define a schema for your dataset in the “Dataset schema” section,
    as shown in [Figure 10-8](#ch10_figure_8_1736545678095955).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在“数据集模式”部分定义您数据集的模式，如图 [图10-8](#ch10_figure_8_1736545678095955) 所示。
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1008.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![一张计算机屏幕截图  自动生成的描述](assets/lelc_1008.png)'
- en: Figure 10-8\. Adding a dataset schema in the LangSmith UI
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. 在LangSmith UI中添加数据集模式
- en: Defining Your Evaluation Criteria
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义您的评估标准
- en: After creating your dataset, you need to define evaluation metrics to assess
    your application’s outputs before deploying into production. This batch evaluation
    on a predetermined test suite is often referred to as*offline evaluation***.**
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建您的数据集后，您需要在部署到生产之前定义评估指标来评估您的应用程序的输出。这种在预定测试套件上的批量评估通常被称为**离线评估**。
- en: For offline evaluation, you can optionally label expected outputs (that is,
    ground truth references) for the data points you are testing on. This enables
    you to compare your application’s response with the ground truth references, as
    shown in [Figure 10-9](#ch10_figure_9_1736545678095982).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离线评估，您可以可选地为测试的数据点标记预期的输出（即，地面真实参考）。这使您能够将应用程序的响应与地面真实参考进行比较，如图 [图10-9](#ch10_figure_9_1736545678095982)
    所示。
- en: '![A diagram of an application process  Description automatically generated](assets/lelc_1009.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![一个应用程序流程图  自动生成的描述](assets/lelc_1009.png)'
- en: Figure 10-9\. AI evaluation diagram
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-9\. 人工智能评估图
- en: 'There are three main evaluators to score your LLM app performance:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个主要的评估器来评估您的LLM应用程序性能：
- en: Human evaluators
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 人类评估器
- en: If you can’t express your testing requirements as code, you can use human feedback
    to express qualitative characteristics and label app responses with scores. LangSmith
    speeds up the process of collecting and incorporating human feedback with annotation
    queues.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您无法将测试要求表达为代码，您可以使用人类反馈来表达定性特征，并用分数标记应用程序响应。LangSmith通过注释队列加快收集和整合人类反馈的过程。
- en: Heuristic evaluators
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式评估器
- en: These are hardcoded functions and assertions that perform computations to determine
    a score. You can use reference-free heuristics (for example, checking whether
    output is valid JSON) or reference-based heuristics such as accuracy. Reference-based
    evaluation compares an output to a predefined ground truth, whereas reference-free
    evaluation assesses qualitative characteristics without a ground truth. Custom
    heuristic evaluators are useful for code-generation tasks such as schema checking
    and unit testing with hardcoded evaluation logic.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是硬编码的函数和断言，它们执行计算以确定分数。您可以使用无参考启发式（例如，检查输出是否为有效的 JSON）或基于参考的启发式（例如，准确性）。基于参考的评估将输出与预定义的地面真实值进行比较，而无需参考的评估评估定性特征，而不需要地面真实值。自定义启发式评估器对于代码生成任务（如模式检查和单元测试）非常有用，这些任务具有硬编码的评估逻辑。
- en: LLM-as-a-judge evaluators
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 作为裁判的评估器
- en: This evaluator integrates human grading rules into an LLM prompt to evaluate
    whether the output is correct relative to the reference answer supplied from the
    dataset output. As you iterate in preproduction, you’ll need to audit the scores
    and tune the LLM-as-a-judge to produce reliable scores.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此评估器将人工评分规则集成到 LLM 提示中，以评估输出相对于数据集输出提供的参考答案是否正确。在预生产迭代中，您需要审核分数并调整 LLM 作为裁判以生成可靠的分数。
- en: To get started with evaluation, start simple with heuristic evaluators. Then
    implement human evaluators before moving on to LLM-as-a-judge to automate your
    human review. This enables you to add depth and scale once your criteria are well-defined.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始评估，可以从简单的启发式评估器开始。然后，在转向 LLM 作为裁判以自动化人工审查之前，先实施人工评估。这使你能够在标准定义良好后增加深度和规模。
- en: Tip
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When using LLM-as-a-judge evaluators, use straightforward prompts that can easily
    be replicated and understood by a human. For example, avoid asking an LLM to produce
    scores on a range of 0 to 10 with vague distinctions between scores.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 LLM 作为裁判的评估器时，请使用简单明了的提示，这些提示可以轻松复制并由人类理解。例如，避免要求 LLM 在 0 到 10 的范围内生成带有模糊区分的分数。
- en: '[Figure 10-10](#ch10_figure_10_1736545678096002) illustrates LLM-as-a-judge
    evaluator in the context of a RAG use case. Note that the reference answer is
    the ground truth.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-10](#ch10_figure_10_1736545678096002) 展示了在 RAG 用例中 LLM 作为裁判的评估器。请注意，参考答案是地面真实值。'
- en: '![A diagram of a brain  Description automatically generated](assets/lelc_1010.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![大脑的示意图  自动生成的描述](assets/lelc_1010.png)'
- en: Figure 10-10\. LLM-as-a-judge evaluator used in a RAG use case
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-10\. 在 RAG 用例中使用的 LLM 作为裁判的评估器
- en: Improving LLM-as-a-judge evaluators performance
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提高LLM作为裁判评估器的性能
- en: Using an LLM-as-a-judge is an effective method to grade natural language outputs
    from LLM applications. This involves passing the generated output to a separate
    LLM for judgment and evaluation. But how can you trust the results of LLM-as-a-judge
    evaluation?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLM 作为裁判是一种评估来自 LLM 应用程序的自然语言输出的有效方法。这涉及将生成的输出传递给另一个 LLM 进行判断和评估。但您如何信任 LLM
    作为裁判评估的结果？
- en: Often, rounds of prompt engineering are required to improve accuracy, which
    is cumbersome and time-consuming. Fortunately, LangSmith provides a *few-shot*
    prompt solution whereby human corrections to LLM-as-a-judge outputs are stored
    as few-shot examples, which are then fed back into the prompt in future iterations.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通常需要多轮提示工程来提高准确性，这既繁琐又耗时。幸运的是，LangSmith 提供了一种 *几次* 提示解决方案，其中将人类对 LLM 作为裁判输出的更正存储为几次示例，然后在未来的迭代中将这些示例反馈到提示中。
- en: By utilizing few-shot learning, the LLM can improve accuracy and align outputs
    with human preferences by providing examples of correct behavior. This is especially
    useful when it’s difficult to construct instructions on how the LLM should behave
    or be formatted.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用几次学习，LLM 可以通过提供正确行为的示例来提高准确性并使输出与人类偏好保持一致。这在难以构建关于 LLM 应如何行为或格式化的指令时特别有用。
- en: 'The few-shot evaluator follows these steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 几次评估器遵循以下步骤：
- en: The LLM evaluator provides feedback on generated outputs, assessing factors
    such as correctness, relevance, or other criteria.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 评估器对生成的输出提供反馈，评估因素如正确性、相关性或其他标准。
- en: It adds human corrections to modify or correct the LLM evaluator’s feedback
    in LangSmith. This is where human preferences and judgment are captured.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在 LangSmith 中添加了人类更正来修改或纠正 LLM 评估器的反馈。这就是人类偏好和判断被捕捉的地方。
- en: These corrections are stored as few-shot examples in LangSmith, with an option
    to leave explanations for corrections.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些更正被存储为 LangSmith 中的几次示例，并可以选择为更正留下解释。
- en: The few-shot examples are incorporated into future prompts as subsequent evaluation
    runs.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将少量样本示例纳入后续评估运行的未来提示中。
- en: Over time, the few-shot evaluator will become increasingly aligned with human
    preferences. This self-improving mechanism reduces the need for time-consuming
    prompt engineering, while improving the accuracy and relevance of LLM-as-a-judge
    evaluations.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，少量样本评估器将越来越符合人类偏好。这种自我改进机制减少了耗时提示工程的需求，同时提高了 LLM 作为裁判评估的准确性和相关性。
- en: Here’s how to easily set up the LLM-as-a-judge evaluator in LangSmith for offline
    evaluation. First, navigate to the “Datasets and Testing” section in the sidebar
    and select the dataset you want to configure the evaluator for. Click the Add
    Auto-Evaluator button at the top right of the dashboard to add an evaluator to
    the dataset. This will open a modal you can use to configure the evaluator.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何在 LangSmith 中轻松设置 LLM 作为裁判评估器以进行离线评估的方法。首先，导航到侧边栏中的“数据集和测试”部分，并选择您想要配置评估器的数据集。点击仪表板右上角的添加自动评估器按钮，将评估器添加到数据集中。这将打开一个模态，您可以使用它来配置评估器。
- en: Select the LLM-as-a-judge option and give your evaluator a name. You will now
    have the option to set an inline prompt or load a prompt from the prompt hub that
    will be used to evaluate the results of the runs in the experiment. For the sake
    of this example, choose the Create Few-Shot Evaluator option, as shown in [Figure 10-11](#ch10_figure_11_1736545678096023).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 LLM 作为裁判选项，并为您的评估器命名。现在您可以选择设置内联提示或从提示中心加载提示，该提示将用于评估实验中运行的结果。为了本例，请选择创建少量样本评估器选项，如图[图
    10-11](#ch10_figure_11_1736545678096023)所示。
- en: '![A screenshot of a survey  Description automatically generated](assets/lelc_1011.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![一个调查的截图  自动生成的描述](assets/lelc_1011.png)'
- en: Figure 10-11\. LangSmith UI options for the LLM-as-a-judge evaluator
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-11\. LangSmith UI 中的 LLM 作为裁判评估器选项
- en: This option will create a dataset that holds few-shot examples that will autopopulate
    when you make corrections on the evaluator feedback. The examples in this dataset
    will be inserted in the system prompt message.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此选项将创建一个包含少量样本的数据集，当您在评估器反馈中进行更正时将自动填充。此数据集中的示例将被插入到系统提示消息中。
- en: You can also specify the scoring criteria in the Schema field and toggle between
    primitive types—for example, integer and Boolean (see [Figure 10-12](#ch10_figure_12_1736545678096045)).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在 Schema 字段中指定评分标准，并在原始类型之间切换——例如，整数和布尔值（见[图 10-12](#ch10_figure_12_1736545678096045)）。
- en: '![A screenshot of a quiz  Description automatically generated](assets/lelc_1012.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![一个测验的截图  自动生成的描述](assets/lelc_1012.png)'
- en: Figure 10-12\. LLM-as-a-judge evaluator scoring criteria
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-12\. LLM 作为裁判评估器的评分标准
- en: Save the evaluator and navigate back to the dataset details page. Moving forward,
    each subsequent experiment run from the dataset will be evaluated by the evaluator
    you configured.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 保存评估器并导航回数据集详细信息页面。从现在起，每个后续实验运行都将由您配置的评估器进行评估。
- en: Pairwise evaluation
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成对评估
- en: Ranking LLM outputs by preference can be less cognitively demanding for human
    or LLM-as-a-judge evaluators. For example, assessing which output is more informative,
    specific, or safe. Pairwise evaluation compares two outputs simultaneously from
    different versions of an application to determine which version better meets evaluation
    criteria.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 根据偏好对 LLM 输出进行排名可能对人类或 LLM 作为裁判的评估者来说认知要求较低。例如，评估哪个输出更信息丰富、更具体或更安全。成对评估同时比较来自应用程序不同版本的两个输出，以确定哪个版本更好地满足评估标准。
- en: 'LangSmith natively supports running and visualizing pairwise LLM app generations,
    highlighting preference for one generation over another based on guidelines set
    by the pairwise evaluator. LangSmith’s pairwise evaluation enables you to do the
    following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: LangSmith 本地支持运行和可视化成对 LLM 应用生成，根据成对评估器设定的指南突出显示对某一生成版本相对于另一版本的偏好。LangSmith
    的成对评估使您能够执行以下操作：
- en: Define a custom pairwise LLM-as-a-judge evaluator using any desired criteria
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用任何期望的标准定义一个自定义成对 LLM 作为裁判的评估器
- en: Compare two LLM generations using this evaluator
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用此评估器比较两个 LLM 生成版本
- en: As per the LangSmith [docs](https://oreil.ly/ruFvy), you can use custom pairwise
    evaluators in the LangSmith SDK and visualize the results of pairwise evaluations
    in the LangSmith UI.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 LangSmith [文档](https://oreil.ly/ruFvy)，您可以在 LangSmith SDK 中使用自定义成对评估器，并在
    LangSmith UI 中可视化成对评估的结果。
- en: After creating an evaluation experiment, you can navigate to the Pairwise Experiments
    tab in the Datasets & Experiments section. The UI enables you to dive into each
    pairwise experiment, showing which LLM generation is preferred based upon our
    criteria. If you click the RANKED_PREFERENCE score under each answer, you can
    dive deeper into each evaluation trace (see [Figure 10-13](#ch10_figure_13_1736545678096064)).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建评估实验后，您可以在“数据集与实验”部分导航到成对实验选项卡。用户界面允许您深入了解每个成对实验，显示根据我们的标准，哪个 LLM 生成被优先考虑。如果您点击每个答案下的
    RANKED_PREFERENCE 分数，您可以深入了解每个评估跟踪（见[图 10-13](#ch10_figure_13_1736545678096064)）。
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1013.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](assets/lelc_1013.png)'
- en: Figure 10-13\. Pairwise experiment UI evaluation trace
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-13\. 成对实验 UI 评估跟踪
- en: Regression Testing
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归测试
- en: In traditional software development, tests are expected to pass 100% based on
    functional requirements. This ensures stable behavior once the test is validated.
    In contrast, however, AI models’ output performances can vary significantly due
    to model *drift* (degradation due to changes in data distribution or updates to
    the model). As a result, testing AI applications may not always lead to a perfect
    score on the evaluation dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的软件开发中，测试应根据功能要求通过 100%。这确保了测试验证后的稳定行为。然而，由于模型 *漂移*（由于数据分布的变化或模型的更新而导致的退化），AI
    模型的输出性能可能会有很大差异。因此，测试 AI 应用程序可能不会始终导致评估数据集上的完美分数。
- en: This has several implications. First, it’s important to track results and performance
    of your tests over time to prevent regression of your app’s performance. *Regression*
    testing ensures that the latest updates or changes of the LLM model of your app
    do not *regress* (perform worse) relative to the baseline.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这有几个含义。首先，跟踪测试结果和性能随时间的变化，以防止应用程序性能的回归。*回归*测试确保应用程序的最新更新或 LLM 模型的更改相对于基线不会*退化*（表现更差）。
- en: Second, it’s crucial to compare the individual data points between two or more
    experimental runs to see where the model got it right or wrong.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，比较两个或多个实验运行之间的单个数据点非常重要，以查看模型在哪里做得正确或错误。
- en: LangSmith’s comparison view has native support for regression testing, allowing
    you to quickly see examples that have changed relative to the baseline. Runs that
    regressed or improved are highlighted differently in the LangSmith dashboard (see
    [Figure 10-14](#ch10_figure_14_1736545678096104)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: LangSmith 的比较视图原生支持回归测试，允许您快速查看相对于基线发生变化的示例。在 LangSmith 仪表板中，回归或改进的运行以不同的方式突出显示（见[图
    10-14](#ch10_figure_14_1736545678096104)）。
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1014.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](assets/lelc_1014.png)'
- en: Figure 10-14\. LangSmith’s experiments comparison view
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-14\. LangSmith 的实验比较视图
- en: 'In LangSmith’s Comparing Experiments dashboard, you can do the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LangSmith 的比较实验仪表板中，您可以执行以下操作：
- en: Compare multiple experiments and runs associated with a dataset. Aggregate stats
    of runs is useful for migrating models or prompts, which may result in performance
    improvements or regression on specific examples.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较与数据集相关的多个实验和运行。运行聚合统计对于迁移模型或提示很有用，这可能会导致性能改进或特定示例的回归。
- en: Set a baseline run and compare it against prior app versions to detect unexpected
    regressions. If a regression occurs, you can isolate both the app version and
    the specific examples that contain performance changes.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个基线运行，并将其与先前应用程序版本进行比较，以检测意外的回归。如果发生回归，您可以隔离应用程序版本和包含性能变化的特定示例。
- en: Drill into data points that behaved differently between compared experiments
    and runs.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入研究比较实验和运行之间行为不同的数据点。
- en: This regression testing is crucial to ensure that your application maintains
    high performance over time regardless of updates and LLM changes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这种回归测试对于确保您的应用程序在更新和 LLM 变化后保持高性能至关重要。
- en: Now that we’ve covered various preproduction testing strategies, let’s explore
    a specific use case.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了各种预生产测试策略，让我们探索一个具体用例。
- en: Evaluating an Agent’s End-to-End Performance
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估代理的端到端性能
- en: Although agents show a lot of promise in executing autonomous tasks and workflows,
    testing an agent’s performance can be challenging. In previous chapters, you learned
    how agents use tool calling with planning and memory to generate responses. In
    particular, tool calling enables the model to respond to a given prompt by generating
    a tool to invoke and the input arguments required to execute the tool.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代理在执行自主任务和工作流程方面展现出很多潜力，但测试代理的性能可能会很具挑战性。在前几章中，你学习了代理如何通过工具调用、规划和记忆来生成响应。特别是，工具调用使模型能够通过生成一个要调用的工具及其执行工具所需的输入参数来响应给定的提示。
- en: Since agents use an LLM to decide the control flow of the application, each
    agent run can have significantly different outcomes. For example, different tools
    might be called, agents might get stuck in a loop, or the number of steps from
    start to finish can vary significantly.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代理使用 LLM 来决定应用程序的控制流程，每个代理运行的结果可能会有很大差异。例如，可能会调用不同的工具，代理可能会陷入循环，或者从开始到结束的步骤数量可能会有很大差异。
- en: 'Ideally, agents should be tested at three different levels of granularity:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，代理应该在三个不同粒度的级别上进行测试：
- en: Response
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 响应
- en: The agent’s final response to focus on the end-to-end performance. The inputs
    are a prompt and an optional list of tools, whereas the output is the final agent
    response.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的最终响应，重点关注端到端性能。输入是一个提示和一个可选的工具列表，而输出是最终的代理响应。
- en: Single step
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 单步
- en: Any single, important step of the agent to drill into specific tool calls or
    decisions. In this case, the output is a tool call.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的任何单个重要步骤，用于深入特定的工具调用或决策。在这种情况下，输出是一个工具调用。
- en: Trajectory
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹
- en: The full trajectory of the agent. In this case, the output is the list of tool
    calls.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的完整轨迹。在这种情况下，输出是工具调用的列表。
- en: '[Figure 10-15](#ch10_figure_15_1736545678096126) illustrates these levels:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-15](#ch10_figure_15_1736545678096126) 展示了这些级别：'
- en: '![A diagram of a tool call  Description automatically generated](assets/lelc_1015.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![工具调用的图  描述自动生成](assets/lelc_1015.png)'
- en: Figure 10-15\. An example of an agentic app’s flow
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-15\. 一个代理应用程序流程的示例
- en: Let’s dive deeper into each of these three agent-testing granularities.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨这三个代理测试粒度中的每一个。
- en: Testing an agent’s final response
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试代理的最终响应
- en: In order to assess the overall performance of an agent on a task, you can treat
    the agent as a black box and define success based on whether or not it completes
    the task.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估代理在任务上的整体性能，你可以将代理视为一个黑盒，并根据它是否完成任务来定义成功。
- en: 'Testing for the agent’s final response typically involves the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 测试代理的最终响应通常涉及以下内容：
- en: Inputs
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: User input and (optionally) predefined tools
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 用户输入和（可选）预定义的工具
- en: Output
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: Agent’s final response
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的最终响应
- en: Evaluator
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 评估者
- en: LLM-as-a-judge
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 作为裁判
- en: 'To implement this in a programmatic manner, first create a dataset that includes
    questions and expected answers from the agent:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以编程方式实现这一点，首先创建一个包含代理问题和预期答案的数据集：
- en: '*Python*'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE5]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*JavaScript*'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE6]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, as discussed earlier, we can utilize the LLM to compare the generated
    answer with the reference answer:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如前所述，我们可以利用 LLM 来比较生成的答案与参考答案：
- en: '*Python*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE7]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*JavaScript*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE8]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Testing a single step of an agent
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试代理的单步
- en: 'Testing an agent’s individual action or decision enables you to identify and
    analyze specifically where your application is underperforming. Testing for a
    single step of an agent involves the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 测试代理的个别动作或决策可以使你具体识别和分析应用程序性能不佳的地方。测试代理的单步涉及以下内容：
- en: Inputs
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: User input to a single step (for example, user prompt, set of tools). This can
    also include previously completed steps.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 单步的用户输入（例如，用户提示、工具集）。这也可以包括之前完成的步骤。
- en: Output
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: LLM response from the inputs step, which often contains tool calls indicating
    what action the agent should take next.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入步骤的 LLM 响应，通常包含指示代理下一步应采取什么行动的工具调用。
- en: Evaluator
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 评估者
- en: Binary score for correct tool selection and heuristic assessment of the tool
    input’s accuracy.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正确工具选择的二进制分数和对工具输入准确性的启发式评估。
- en: 'The following example checks a specific tool call using a custom evaluator:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用自定义评估器检查特定的工具调用：
- en: '*Python*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE9]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*JavaScript*'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE10]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding code block implements these distinct components:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块实现了这些不同的组件：
- en: Invoke the assistant, `assistant_runnable`, with a prompt and check if the resulting
    tool call is as expected.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提示调用辅助程序 `assistant_runnable` 并检查结果工具调用是否符合预期。
- en: Utilize a specialized agent where the tools are hardcoded rather than passed
    with the dataset input.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用专门代理，其中工具是硬编码的，而不是与数据集输入一起传递。
- en: Specify the reference tool call for the step that we are evaluating for `expected_tool_call`.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为我们正在评估的步骤指定`expected_tool_call`的参考工具调用。
- en: Testing an agent’s trajectory
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试代理的轨迹
- en: It’s important to look back on the steps an agent took in order to assess whether
    or not the trajectory lined up with expectations of the agent—that is, the number
    of steps or sequence of steps taken.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是回顾代理所采取的步骤，以评估轨迹是否与代理的预期一致——即所采取的步骤数量或步骤序列。
- en: 'Testing an agent’s trajectory involves the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 测试代理的轨迹包括以下内容：
- en: Inputs
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: User input and (optionally) predefined tools.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 用户输入和（可选）预定义工具。
- en: Output
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: Expected sequence of tool calls or a list of tool calls in any order.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 工具调用的预期序列或任意顺序的工具调用列表。
- en: Evaluator
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 评估器
- en: Function over the steps taken. To test the outputs, you can look at an exact
    match binary score or metrics that focus on the number of incorrect steps. You’d
    need to evaluate the full agent’s trajectory against a reference trajectory and
    then compile as a set of messages to pass into the LLM-as-a-judge.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 函数覆盖了所采取的步骤。为了测试输出，你可以查看精确匹配的二进制分数或关注错误步骤数量的指标。你需要将完整代理的轨迹与参考轨迹进行比较，然后将其编译为传递给LLM作为裁判的消息集。
- en: 'The following example assesses the trajectory of tool calls using custom evaluators:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例评估了使用自定义评估器的工具调用轨迹：
- en: '*Python*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE11]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*JavaScript*'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE12]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This implementation example includes the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现示例包括以下内容：
- en: Invoking a precompiled LangGraph agent `graph.invoke` with a prompt
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提示调用预编译的LangGraph代理`graph.invoke`
- en: Utilizing a specialized agent where the tools are hardcoded rather than passed
    with the dataset input
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用专门代理，其中工具是硬编码的，而不是与数据集输入一起传递
- en: Extracting of the list of tools called using the function `find_tool_calls`
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用函数`find_tool_calls`提取调用工具的列表
- en: Checking if all expected tools are called in any order using the function `contains_all_tool_calls_any_order`
    or called in order using `contains_all_tool_calls_in_order`
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用函数`contains_all_tool_calls_any_order`或按顺序使用`contains_all_tool_calls_in_order`检查是否以任何顺序调用了所有预期的工具
- en: Checking whether all expected tools are called in the exact order using `contains_all_tool_calls_in_order_exact_match`
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`contains_all_tool_calls_in_order_exact_match`检查是否以精确顺序调用了所有预期的工具
- en: All three of these agent evaluation methods can be observed and debugged in
    LangSmith’s experimentation UI (see [Figure 10-16](#ch10_figure_16_1736545678096148)).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种代理评估方法都可以在LangSmith的实验UI中观察和调试（见[图10-16](#ch10_figure_16_1736545678096148)）。
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1016.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  自动生成的描述](assets/lelc_1016.png)'
- en: Figure 10-16\. Example of an agent evaluation test in the LangSmith UI
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-16\. LangSmith UI中代理评估测试的示例
- en: In general, these tests are a solid starting point to help mitigate an agent’s
    cost and unreliability due to LLM invocations and variability in tool calling.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些测试是帮助减轻代理因LLM调用和工具调用变化而产生的成本和不可靠性的良好起点。
- en: Production
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产
- en: Although testing in the preproduction phase is useful, certain bugs and edge
    cases may not emerge until your LLM application interacts with live users. These
    issues can affect latency, as well as the relevancy and accuracy of outputs. In
    addition, observability and the process of *online evaluation* can help ensure
    that there are guardrails for LLM inputs or outputs. These guardrails can provide
    much-needed protection from prompt injection and toxicity.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在预生产阶段进行测试很有用，但某些错误和边缘情况可能直到你的LLM应用程序与真实用户交互时才会出现。这些问题可能会影响延迟，以及输出的相关性和准确性。此外，可观察性和在线评估的过程可以帮助确保LLM输入或输出有护栏。这些护栏可以提供必要的保护，防止提示注入和毒性。
- en: The first step in this process is to set up LangSmith’s tracing feature.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程的第一步是设置LangSmith的跟踪功能。
- en: Tracing
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪
- en: A *trace* is a series of steps that your application takes to go from input
    to output. LangSmith makes it easy to visualize, debug, and test each trace generated
    from your app.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*跟踪*是一系列步骤，您的应用程序从输入到输出所采取的。LangSmith使可视化、调试和测试从您的应用程序生成的每个跟踪变得容易。'
- en: 'Once you’ve installed the relevant LangChain and LLM dependencies, all you
    need to do is configure the tracing environment variables based on your LangSmith
    account credentials:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您安装了相关的LangChain和LLM依赖项，您需要做的就是根据您的LangSmith账户凭据配置跟踪环境变量：
- en: '[PRE13]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: After the environment variables are set, no other code is required to enable
    tracing. Traces will be automatically logged to their specific project in the
    “Tracing projects” section of the LangSmith dashboard. The metrics provided include
    trace volume, success and failure rates, latency, token count and cost, and more—as
    shown in [Figure 10-17](#ch10_figure_17_1736545678096172).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 环境变量设置后，不需要其他代码即可启用跟踪。跟踪将自动记录到LangSmith仪表板“跟踪项目”部分的特定项目中。提供的指标包括跟踪量、成功和失败率、延迟、令牌计数和成本等——如[图10-17](#ch10_figure_17_1736545678096172)所示。
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1017.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](assets/lelc_1017.png)'
- en: Figure 10-17\. An example of LangSmith’s trace performance metrics
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-17\. LangSmith跟踪性能指标的一个示例
- en: You can review a variety of strategies to implement tracing based on your needs.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据需要审查各种基于跟踪的策略。
- en: Collect Feedback in Production
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在生产中收集反馈
- en: Unlike the preproduction phase, evaluators for production testing don’t have
    grounded reference responses for the LLM to compare against. Instead, evaluators
    need to score performance in real time as your application processes user inputs.
    This reference-free, real-time evaluation is often referred to as *online evaluation*.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 与预生产阶段不同，生产测试的评估器没有为LLM提供基于的参考响应。相反，评估器需要实时评估性能，因为您的应用程序处理用户输入。这种无参考的实时评估通常被称为*在线评估*。
- en: 'There are at least two types of feedback you can collect in production to improve
    app performance:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中，您至少可以收集两种类型的反馈来提高应用程序的性能：
- en: Feedback from users
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 用户反馈
- en: You can directly collect user feedback explicitly or implicitly. For example,
    giving users the ability to click a like and dislike button or provide detailed
    feedback based on the application’s output is an effective way to track user satisfaction.
    In LangSmith, you can attach user feedback to any trace or intermediate run (that
    is, span) of a trace, including annotating traces inline or reviewing runs together
    in an annotation queue.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接显式或隐式地收集用户反馈。例如，给用户点击点赞和不喜欢按钮或根据应用程序的输出提供详细反馈的能力是跟踪用户满意度的有效方法。在LangSmith中，您可以将用户反馈附加到任何跟踪或中间运行（即跨度）上，包括在行内注释跟踪或一起在注释队列中审查运行。
- en: Feedback from LLM-as-a judge evaluators
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 来自LLM作为评委评估器的反馈
- en: As discussed previously, these evaluators can be implemented directly on traces
    to identify hallucination and toxic responses.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这些评估器可以直接在跟踪上实现，以识别幻觉和有毒反应。
- en: The earlier preproduction section already discussed how to set up LangSmith’s
    auto evaluation in the Datasets & Experiments section of the dashboard.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 早期预生产部分已经在仪表板的“数据集和实验”部分讨论了如何设置LangSmith的自动评估。
- en: Classification and Tagging
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类和标记
- en: In order to implement effective guardrails against toxicity or gather insights
    on user sentiment analysis, we need to build an effective system for labeling
    user inputs and generated outputs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实施有效的防止毒性或收集用户情感分析的见解，我们需要建立一个有效的系统来标记用户输入和生成的输出。
- en: This system is largely dependent on whether or not you have a dataset that contains
    reference labels. If you don’t have preset labels, you can use the LLM-as-a-judge
    evaluator to assist in performing classification and tagging based upon specified
    criteria.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统在很大程度上取决于您是否有一个包含参考标签的数据集。如果您没有预设的标签，您可以使用LLM作为评委的评估器来协助根据指定标准进行分类和标记。
- en: If, however, ground truth classification labels are provided, then a custom
    heuristic evaluator can be used to score the chain’s output relative to the ground
    truth class labels.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提供了真实分类标签，则可以使用自定义启发式评估器来根据真实类别标签对链的输出进行评分。
- en: Monitoring and Fixing Errors
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控和修复错误
- en: Once your application is in production, LangSmith’s tracing will catch errors
    and edge cases. You can add these errors into your test dataset for offline evaluation
    in order to prevent recurrences of the same issues.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的应用程序投入生产，LangSmith的跟踪将捕获错误和边缘情况。您可以将这些错误添加到您的测试数据集中，以便进行离线评估，以防止相同问题的再次发生。
- en: Another useful strategy is to release your app in phases to a small group of
    beta users before a larger audience can access its features. This will enable
    you to uncover crucial bugs, develop a solid evaluation dataset with ground truth
    references, and assess the general performance of the app including cost, latency,
    and quality of outputs.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的策略是在更大范围的受众可以访问其功能之前，将你的应用分阶段发布给一小群测试用户。这将使你能够发现关键的错误，开发一个包含真实参考的可靠评估数据集，并评估应用的整体性能，包括成本、延迟和输出质量。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: As discussed in this chapter, robust testing is crucial to ensure that your
    LLM application is accurate, reliable, fast, toxic-free, and cost-efficient. The
    three key stages of LLM app development create a data cycle that helps to ensure
    high performance throughout the lifetime of the application.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章所述，稳健的测试对于确保你的LLM应用准确、可靠、快速、无毒性且成本效益至关重要。LLM应用开发的三个关键阶段创建了一个数据循环，有助于确保应用整个生命周期内的高性能。
- en: During the design phase, in-app error handling enables self-correction before
    the error reaches the user. Preproduction testing ensures each of your app’s updates
    avoids regression in performance metrics. Finally, production monitoring gathers
    real-time insights and application errors that inform the subsequent design process
    and the cycle repeats.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计阶段，应用内错误处理能够在错误到达用户之前进行自我纠正。预生产测试确保你的应用每个更新都不会在性能指标上出现回归。最后，生产监控收集实时洞察和应用程序错误，这些信息将指导后续的设计过程，并使循环重复。
- en: Ultimately, this process of testing, evaluation, monitoring, and continuous
    improvement, will help you fix issues and iterate faster, and most importantly,
    deliver a product that users can trust to consistently deliver their desired results.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这个测试、评估、监控和持续改进的过程将帮助你修复问题并更快地迭代，最重要的是，交付一个用户可以信赖的、能够持续提供预期结果的产物。
