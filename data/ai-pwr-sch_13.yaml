- en: 11 Automating learning to rank with click models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 使用点击模型自动化学习排序
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Automating learning to rank retraining from user behavioral signals (searches,
    clicks, etc.)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化从用户行为信号（搜索、点击等）中重新训练学习排序
- en: Transforming user signals into implicit LTR training data using click models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用点击模型将用户信号转化为隐式LTR训练数据
- en: Overcoming user’s tendency to click items higher in the search results, regardless
    of relevance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克服用户倾向于点击搜索结果中排名更高的项目，而不考虑相关性的倾向
- en: Handling low-confidence documents with fewer clicks when deriving implicit judgments
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推导隐式判断时处理点击次数较少的低置信度文档
- en: In chapter 10, we went step by step through training a learning to rank (LTR)
    model. Like walking through the mechanics of building a car, we saw the underlying
    nuts and bolts of LTR model training. In this chapter, we’ll treat the LTR training
    process as a black box. In other words, we’ll step away from the LTR internals,
    instead treating LTR more like a self-driving car, fine-tuning its trip toward
    a final destination.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10章中，我们一步步地讲解了如何训练一个学习排序（LTR）模型。就像一步步地了解汽车构造的机械原理一样，我们看到了LTR模型训练的底层结构和细节。在这一章中，我们将把LTR训练过程视为一个黑盒。换句话说，我们将不再深入LTR的内部，而是将LTR更像看作一辆自动驾驶汽车，微调其行程以到达最终目的地。
- en: 'Recall that LTR relies on accurate training data in order to be effective.
    LTR training data describes how users expect search results to be optimally ranked;
    it provides the directions we’ll input into our LTR self-driving car. As you’ll
    see, determining what’s relevant based on user interactions comes with many challenges.
    If we can overcome these challenges and gain high confidence in our training data,
    though, we can build *automated learning to rank*: a system that regularly retrains
    LTR to capture the latest user relevance expectations.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，LTR的有效性依赖于准确无误的训练数据。LTR训练数据描述了用户期望搜索结果如何被最优排序；它为我们输入LTR自动驾驶汽车的方向提供了指导。正如您将看到的，根据用户互动来确定相关性带来了许多挑战。如果我们能够克服这些挑战，并对我们的训练数据有很高的信心，那么我们可以构建*自动化的学习排序*：一个定期重新训练LTR以捕捉最新的用户相关性期望的系统。
- en: As training data is so central to automated LTR, the challenges become not “What
    model/features/search engine should we use?” but more fundamentally, “What do
    users want from search?”, “How do we turn that into training data?”, and “How
    do we know whether that training data is any good?”. By improving our confidence
    in the answers to these questions, we can put LTR (re)training on autopilot, as
    shown in figure 11.1\.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练数据在自动化LTR中如此关键，挑战不再仅仅是“我们应该使用什么模型/特征/搜索引擎？”而是更根本的，“用户从搜索中想要什么？”“我们如何将这转化为训练数据？”“我们如何知道这些训练数据是否有效？”通过提高我们对这些问题的答案的信心，我们可以将LTR（再）训练自动化，如图11.1所示。
- en: '![figure](../Images/CH11_F01_Grainger.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH11_F01_Grainger.png)'
- en: Figure 11.1 An automated LTR system automatically learns and retrains from the
    user’s signals. This helps build models based on what actual users consider relevant
    over many queries.
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.1 一个自动化的LTR系统从用户信号中自动学习和再训练。这有助于基于用户在多个查询中认为相关的实际内容来构建模型。
- en: 'Let’s briefly walk through each step in the automated LTR process:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地回顾自动化LTR过程中的每一步：
- en: '*Input new destination*—We input training data into the LTR system describing
    ideal relevance, based on our understanding of user behavioral signals, such as
    searches, clicks, and conversions (covered in this chapter).'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*输入新目的地*——我们将描述理想相关性的训练数据输入LTR系统，这些数据基于我们对用户行为信号的理解，例如搜索、点击和转化（本章涵盖）。'
- en: '*Drive to destination*—Our LTR system retrains an LTR model using the provided
    training data (as covered in chapter 10).'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*驶向目的地*——我们的LTR系统使用提供的训练数据重新训练LTR模型（如第10章所述）。'
- en: '*Are we there yet?*—Is the model truly helping users? And should future models
    perhaps explore alternate routes (covered in chapter 12)?'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*我们到了吗？*——模型真的在帮助用户吗？未来的模型或许应该探索其他路线（第12章涵盖）？'
- en: Automated LTR repeats steps 1–3 continuously to automatically optimize relevance.
    The search team monitors the automated LTR’s performance and intervenes as needed.
    This is the *maintenance* portion of figure 11.1\. During maintenance, we open
    the hood to explore new LTR features and other model adjustments. Maintenance
    could also mean revisiting step 1 to correct our understanding of user behaviors
    and build more reliable, robust training data. After all, without good training
    data, we could follow chapter 10 to a T and still fail to satisfy our users.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化LTR会持续重复步骤1-3以自动优化相关性。搜索团队会监控自动化LTR的性能并在必要时进行干预。这是图11.1中的*维护*部分。在维护期间，我们会打开引擎盖来探索新的LTR功能和其它模型调整。维护也可能意味着重新审视步骤1，以纠正我们对用户行为的理解并构建更可靠、更健壮的训练数据。毕竟，如果没有好的训练数据，我们可能完全遵循第10章的内容，但仍可能无法满足用户的需求。
- en: This chapter starts our exploration of automated LTR by focusing on step 1—inputting
    a new destination. We’ll first define the task of deriving training data from
    user clicks. We’ll then spend the rest of this chapter overcoming some common
    biases and challenges with search click data. By the end of this chapter, you’ll
    be able to build models with more reliable training data derived from user signals.
    Chapter 12 will then finish our automated LTR exploration by observing the model
    interacting with live users, using active learning and Gaussian techniques to
    overcome trickier presentation biases, and integrating all these components into
    a final, end-to-end automated LTR system.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过关注步骤1——输入新的目的地，开始对自动化LTR的探索。我们首先定义从用户点击中推导训练数据的任务。然后，我们将在本章的其余部分克服一些常见的搜索点击数据偏差和挑战。到本章结束时，你将能够构建从用户信号中提取的更可靠训练数据的模型。第12章将通过观察模型与真实用户互动，使用主动学习和高斯技术克服更复杂的展示偏差，并将所有这些组件整合到一个最终的端到端自动化LTR系统中来完成我们的自动化LTR探索。
- en: 11.1 (Re)creating judgment lists from signals
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1（重新）从信号创建判断列表
- en: We mentioned that we need to overcome biases when creating LTR training data
    from clicks. However, before we dig into those biases, we’ll explore the implications
    of using clicks instead of manual labels for LTR training data. We’ll then take
    a naive, first stab at crafting training data in this section, reflecting on what
    went well or not so well. This will set us up for the rest of the chapter, where
    we’ll explore removing bias from these results (in section 11.2 and beyond).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到，在从点击中创建LTR训练数据时需要克服偏差。然而，在我们深入探讨这些偏差之前，我们将探讨使用点击而不是手动标签作为LTR训练数据的含义。然后，在本节中，我们将尝试制作训练数据，回顾哪些做得好，哪些不好。这将为我们设置本章的其余部分，我们将探讨如何从这些结果中去除偏差（在第11.2节及以后）。
- en: 11.1.1 Generating implicit, probabilistic judgments from signals
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 从信号生成隐式、概率性判断
- en: Let’s lay a foundation for how to use behavioral signals as LTR training data.
    Then we’ll dive into the details of constructing reliable judgment lists.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为如何使用行为信号作为LTR训练数据打下基础。然后我们将深入了解构建可靠判断列表的细节。
- en: In chapter 10, we discussed LTR training data, referred to as *judgment lists*
    or *judgments*. These judgements contain labels or *grades* for how relevant potential
    search results are for a given query. In chapter 10, we used movies as our example,
    labeling them with a grade of either `1` (relevant) or `0` (irrelevant), as in
    the following example.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10章中，我们讨论了LTR训练数据，被称为*判断列表*或*判断*。这些判断包含了对给定查询潜在搜索结果相关性的标签或*评分*。在第10章中，我们以电影为例，用`1`（相关）或`0`（不相关）的评分来标记它们，如下面的例子所示。
- en: Listing 11.1 Labeling movies relevant or irrelevant
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.1 标记电影相关或不相关
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There are many techniques for generating judgment lists, and this isn’t a comprehensive
    chapter on judgment lists and their many applications. Instead, we’ll specifically
    focus on LTR training data. For this reason, we will only discuss judgments generated
    from user click signals. We call these *implicit judgments* because they derive
    from user interactions with the search application as users search and click.
    This contrasts with *explicit judgments*, where raters directly label search results
    as relevant/irrelevant.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 生成判断列表有许多技术，但这并不是关于判断列表及其众多应用的全面章节。相反，我们将专门关注LTR训练数据。因此，我们只会讨论从用户点击信号中生成的判断。我们称之为*隐式判断*，因为它们源自用户与搜索应用之间的交互，当用户搜索和点击时。这与*显式判断*形成对比，显式判断中评分者直接将搜索结果标记为相关或不相关。
- en: 'Implicit judgments are ideal for automating LTR for several reasons:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式判断非常适合自动化LTR（学习到排名），原因有以下几点：
- en: '*Recency*—We have ready access to user traffic, so we can automate training
    today’s LTR model on the latest user search expectations.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*近期性*——我们有用户流量的直接访问权限，因此我们可以自动化训练今天的LTR模型，以满足最新的用户搜索期望。'
- en: '*More data at less cost*—Setting up a task to capture explicit judgments, even
    with crowdsourcing, is time consuming and expensive to do well at scale. Deriving
    implicit judgments from live user interactions we’re already collecting allows
    us to use the existing user base to do this work for us.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据更多，成本更低*——设置一个任务来捕获显式判断，即使使用众包，也是耗时且成本高昂的，尤其是在大规模上。从我们正在收集的实时用户交互中推导出隐式判断，使我们能够利用现有的用户基础为我们完成这项工作。'
- en: '*Capturing real use cases*—Implicit judgments capture real users doing actual
    tasks with your search app. Contrast this with an artificial setting where explicit
    raters think carefully, perhaps unrealistically so, about the abstract task of
    choosing the most relevant results.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*捕获真实用例*——隐式判断捕获了实际使用你的搜索应用的用户。与此相对比的是，在人工环境中，显式评分者会仔细思考，可能是不切实际地思考，关于选择最相关结果的抽象任务。'
- en: Unfortunately, click data can be noisy. We don’t know why a user clicked on
    a given search result. Further, users are not homogeneous; some will interpret
    one result as relevant, while others will think otherwise. Search interactions
    also contain biases that need to be overcome, creating additional uncertainty
    around a model’s calculations, which we’ll discuss in detail later in this chapter
    and the next.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，点击数据可能很嘈杂。我们不知道为什么用户点击了特定的搜索结果。此外，用户并不同质；有些人会将一个结果视为相关，而其他人则不然。搜索交互中也包含需要克服的偏见，这会在模型计算周围产生额外的不确定性，我们将在本章和下一章的后面详细讨论。
- en: For these reasons, instead of a binary judgment, click models create *probabilistic
    judgments*. Instead of producing a grade of only `1` (relevant) or `0` (irrelevant),
    the grade represents the probability (between `0.0` and `1.0`) that a random user
    would consider the result to be relevant or not. For example, a good click model
    might restate the judgments from listing 11.1 as something more like the following.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，点击模型不是进行二进制判断，而是创建*概率判断*。它不会只产生`1`（相关）或`0`（不相关）的等级，而是表示一个随机用户认为结果相关或不相关的概率（介于`0.0`和`1.0`之间）。例如，一个好的点击模型可能会将列表11.1中的判断重述为以下类似的内容。
- en: Listing 11.2 Labeling movie query relevance probabilistically
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.2 概率性地标记电影查询的相关性
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice the Star Wars movies in listing 10.2—the `grade` has become quite a bit
    more interesting. *Star Wars* now has a very high probability of relevance (`0.99`).
    The sequel, *Return of the Jedi*, has a slightly lower probability. Other science
    fiction movies (*Star Trek Into Darkness* and *Battlestar Galactica*) have ratings
    a bit higher than `0`, as fans of the Star Wars franchise might also enjoy these
    movies. *The Star* is completely unrelated—it’s a children’s animated movie about
    the first Christmas—so it receives a low `0.01` relevance probability.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意列表10.2中的《星球大战》电影——`grade`变得更有趣了。《星球大战》现在有非常高的相关性概率（`0.99`）。续集《星球大战：绝地归来》的相关性概率略低。其他科幻电影（如《星际迷航：暗黑无界》和《银河系漫游指南》）的评分略高于`0`，因为《星球大战》系列的粉丝也可能喜欢这些电影。《星》与这些完全不相关——它是一部关于第一个圣诞节的儿童动画电影——因此它只获得了低`0.01`的相关性概率。
- en: 11.1.2 Training an LTR model using probabilistic judgments
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.2 使用概率判断训练LTR模型
- en: We just introduced the idea that a relevance grade could be probabilistic. Now
    let’s consider how we can apply the lessons from chapter 10 to train a model using
    these probabilistic judgments (between `0.0` and `1.0`) instead of binary judgments.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚介绍了相关性等级可以是概率性的这一想法。现在让我们考虑如何将第10章的教训应用于训练一个模型，使用这些概率判断（介于`0.0`和`1.0`之间）而不是二进制判断。
- en: 'Generally, you might consider these options when training a model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在训练模型时，你可能需要考虑以下选项：
- en: '*Quantize the grades*—Quite simply, you can set arbitrary cutoffs before training
    to convert the grades to an acceptable format. You might assign a grade greater
    than `0.75` as relevant (or `1.00`). Anything less than `0.75` would be considered
    irrelevant (or `0.00`). Other algorithms, like LambdaMART, accept a range of grades,
    like `1` to `4`, and these could have discrete cutoffs as well, such as assigning
    anything less than `0.25` a grade of `1.00`, anything greater than or equal to
    `0.25` but less than `0.5` a grade of `2.00`, and so on. With these algorithms,
    you could create 100 such labels, assigning `0.00` a grade of `0`, `0.01` a grade
    of `1`, and so on, until `1` is assigned a grade of `100` prior to training.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*量化评分*——简单来说，您可以在训练之前设置任意截止值，将评分转换为可接受的格式。您可以将大于`0.75`的评分视为相关（或`1.00`）。任何小于`0.75`的评分将被视为不相关（或`0.00`）。其他算法，如LambdaMART，接受一系列评分，如`1`到`4`，并且这些算法也可以有离散的截止值，例如将小于`0.25`的评分赋予`1.00`，大于或等于`0.25`但小于`0.5`的评分赋予`2.00`，依此类推。使用这些算法，您可以创建100个这样的标签，将`0.00`赋予`0`，`0.01`赋予`1`，等等，直到将`1`赋予在训练前的`100`。'
- en: '*Just use the floating-point judgments*—The SVMRank algorithm from chapter
    10 subtracted a more relevant item’s features from a less relevant item’s features
    (and vice versa) and built a classifier to tell relevant from irrelevant items.
    We did this with binary judgments, but nothing prevents us from doing this with
    probabilistic judgments. Here, if *Return of the Jedi* (`grade=0.80`) is considered
    more relevant than *Star Trek Into Darkness* (`grade=0.20`), we simply note *Return
    of the Jedi* as more relevant than *Star Trek Into Darkness* (labeling the difference
    as `+1`). Then we perform the same pairwise subtraction we would perform from
    chapter 10, subtracting features of *Star Trek Into Darkness* from those of *Return
    of the Jedi* to create a full training example.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*仅使用浮点判断*——第10章中的SVMRank算法从不太相关的项目的特征中减去更相关项目的特征（反之亦然），并构建一个分类器来区分相关和不相关的项目。我们使用二元判断来做这件事，但没有任何东西阻止我们使用概率判断。在这里，如果认为《星球大战：绝地归来》（评分=0.80）比《星际迷航：暗黑无界》（评分=0.20）更相关，我们只需将《星球大战：绝地归来》标记为比《星际迷航：暗黑无界》更相关（将差异标记为`+1`）。然后我们执行与第10章相同的成对减法，从《星际迷航：暗黑无界》中减去《星球大战：绝地归来》的特征，以创建一个完整的训练示例。'
- en: Retraining the model with judgments in this chapter would mostly repeat the
    code from chapter 10, so we’ll instead focus on the mechanics of training a click
    model. We have included a notebook with a full end-to-end LTR training example
    (see section 11.4) that integrates the click model we’ll arrive at by the end
    of this chapter into the LTR training process you already explored in chapter
    10.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本章中的判断重新训练模型将主要重复第10章中的代码，所以我们将专注于训练点击模型的机制。我们包含了一个笔记本，其中包含一个完整的端到端LTR训练示例（见第11.4节），它将我们将在本章末尾得到的点击模型整合到您在第10章中已经探索过的LTR训练过程中。
- en: Time to get back to the code and see our first click model!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候回到代码，看看我们的第一个点击模型了！
- en: '11.1.3 Click-Through Rate: Your first click model'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.3 点击率：您的第一个点击模型
- en: Now that you’ve seen the judgments format that a click model generates and how
    this format can be integrated to train an LTR model, let’s take a first, naive
    pass at building a click model. After that, we’ll take a step back to focus on
    a more sophisticated, general-purpose click model, and we’ll then explore some
    of the core biases inherent in processing query and click signals.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了点击模型生成的判断格式以及这种格式如何集成到训练LTR模型中，让我们尝试构建一个简单的点击模型。之后，我们将退后一步，专注于一个更复杂、通用的点击模型，然后我们将探讨一些在处理查询和点击信号中固有的核心偏差。
- en: TIP If you’d like to take a deeper dive into this topic, we encourage you to
    read *Click Models for Web Search* by Chuklin, Markov, and Rijke (Springer, 2015).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 如果您想更深入地了解这个主题，我们鼓励您阅读Chuklin、Markov和Rijke所著的《网络搜索点击模型》（Springer，2015年）。
- en: To build our click model, we’ll return to the RetroTech dataset, as it comes
    conveniently bundled with user click signals. From these signals, we’ve also reverse-engineered
    the kind of raw session data you need to build high-quality judgments. We’ll make
    use of the `pandas` library to perform tabular computations on session data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的点击模型，我们将回到RetroTech数据集，因为它方便地包含了用户点击信号。从这些信号中，我们还逆向工程了构建高质量判断所需的原始会话数据。我们将使用`pandas`库对会话数据进行表格计算。
- en: In the following listing, we examine a sample search session for the movie *Transformers
    Dark of The Moon*. This raw session information is your starting point—the bare
    minimum information needed to develop a judgment list from user signals.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们检查了电影《变形金刚：月黑之时》的样本搜索会话。这个原始会话信息是你的起点——从用户信号中开发判断列表所需的最基本信息。
- en: Listing 11.3 Examining a search session
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.3 检查搜索会话
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Selects sessions for the "transformers dark of the moon" query'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 选择“transformers dark of the moon”查询的会话'
- en: '#2 Examines a single search session shown to the user'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 检查展示给用户的单个搜索会话'
- en: 'Output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Listing 11.3 corresponds to a single search session, with `sess_id=3`, for the
    query `transformers` `dark` `of` `the` `moon`. This session includes the query,
    the ranked results seen by the user, and whether each result was clicked. These
    three elements are the core ingredients needed to build a click model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.3对应于一个单一的搜索会话，`sess_id=3`，针对查询`transformers` `dark` `of` `the` `moon`。这个会话包括查询、用户看到的排名结果以及每个结果是否被点击。这三个元素是构建点击模型所需的核心成分。
- en: Search sessions will frequently differ. Another session, even seconds later,
    could have a slightly different ranking presented to the user. The search index
    might have changed, or a new relevance algorithm may have been deployed to production.
    We encourage you to retry listing 11.3 with another `sess_id` to compare sessions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索会话将经常有所不同。另一个会话，即使几秒钟后，也可能向用户展示略有不同的排名。搜索索引可能已更改，或者可能已将新的相关性算法部署到生产中。我们鼓励你使用另一个`sess_id`重试列表11.3，以比较会话。
- en: 'Let’s convert this data into judgments using our first, simple click model:
    Click-Through Rate.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些数据转换为使用我们的第一个简单点击模型：点击率进行判断。
- en: Building judgments from click-through rate
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从点击率构建判断
- en: We’ll start by building a very simple click model to get comfortable with the
    data, and then we can step back to see the flaws in this first pass. This will
    allow us to think carefully about the quality of the generated judgments for automated
    LTR in the rest of this chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先构建一个非常简单的点击模型，以便熟悉数据，然后我们可以退后一步，看看这个初步尝试的缺陷。这将使我们能够仔细思考本章其余部分中自动LTR生成的判断质量。
- en: Our first click model will be based on *click-through rate* (CTR). CTR is the
    number of clicks received on a search result divided by the number of times it
    appeared in search results. If a result is clicked every single time the search
    engine returns the result, the CTR will be `1`. If it’s never clicked, the CTR
    will be `0`. Sounds simple enough—what could go wrong?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个点击模型将基于**点击率**（CTR）。CTR是搜索结果上的点击次数除以它在搜索结果中出现的次数。如果一个结果每次搜索引擎返回时都被点击，CTR将是`1`。如果它从未被点击，CTR将是`0`。听起来很简单——可能出什么问题呢？
- en: We can look over every result for the query `transformers` `dark of the moon`
    and consider clicks with respect to the number of sessions in which the `doc_id`
    was returned. The following listing shows the computation and the resulting CTR
    value per document.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看查询`transformers` `dark` `of` `the` `moon`的每个结果，并考虑与返回`doc_id`的会话数量相关的点击。以下列表显示了计算和每个文档的CTR值。
- en: Listing 11.4 Computing CTR
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.4 计算CTR
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In listing 11.4, for all `sessions` with the query `transformers dark of the
    moon` (per listing 11.3), we sum the clicks for each `doc_id` as `click_counts`.
    We also count the number of unique sessions for that document in `sess_counts`.
    Finally, we compute `ctrs` as `click_counts` `/` `sess_counts`, giving us our
    first click model. We see that document 97360810042 has the highest CTR and 24543750949
    the lowest.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表11.4中，对于所有查询`transformers dark of the moon`的`sessions`（根据列表11.3），我们将每个`doc_id`的点击次数求和为`click_counts`。我们还计算了该文档的独特会话数量为`sess_counts`。最后，我们计算`ctrs`为`click_counts`
    `/` `sess_counts`，从而得到我们的第一个点击模型。我们看到文档97360810042的CTR最高，而24543750949的CTR最低。
- en: The preceding listing outputs the *ideal search results* based on the CTR. That
    is, if our LTR model was trained using this CTR click model to provide the relevance
    judgments, the search engine would produce this ordering as the optimal ranking.
    Throughout this chapter and the next, we’ll frequently visually display this ideal
    ranking to understand whether the click model builds reasonable training data
    (judgments). We can see the CTR-based ideal judgments for `transformers` `dark`
    `of` `the moon` in figure 11.2.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的列表基于CTR输出了*理想搜索结果*。也就是说，如果我们使用这个CTR点击模型来训练LTR模型提供相关性判断，搜索引擎将产生这个排序作为最佳排名。在本章和下一章中，我们将经常通过视觉显示这个理想排名来理解点击模型是否构建了合理的训练数据（判断）。我们可以在图11.2中看到基于CTR的理想判断`transformers
    dark of the moon`。
- en: '![figure](../Images/CH11_F02_Grainger.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F02_Grainger.png)'
- en: Figure 11.2 Search results ranked by CTR for the query `transformers dark` `of`
    `the` `moon`
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.2 按CTR排序的查询“transformers dark of the moon”的搜索结果
- en: 'Examining the results of figure 11.2, a couple of things jump out:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 检查图11.2的结果，有几件事情很突出：
- en: 'The CTR for our top result (the Blu-ray of the movie *Transformers: Dark of
    the Moon*) seems rather low (`0.0824`, only a little better than the next judgment
    at `0.0734`). We might expect the Blu-ray’s relevance grade to be much higher
    than other results.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们最高结果的CTR（电影《变形金刚：月黑之时》的蓝光碟）似乎相当低（`0.0824`，略好于下一个判断的`0.0734`）。我们可能预期蓝光碟的相关性等级要远高于其他结果。
- en: 'The DVD for the movie *Transformers: Dark of The Moon* doesn’t even show up.
    It sits far below seemingly unrelated movies and secondary video games about the
    movie *Dark of The Moon*. We would expect the DVD to rank higher, maybe as high
    or higher than the Blu-ray.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电影《变形金刚：月黑之时》的DVD甚至都没有出现。它似乎与看似无关的电影和关于电影《月黑之时》的次要电子游戏远在下方。我们原本预期DVD的排名应该更高，可能和蓝光碟一样高，甚至更高。
- en: But perhaps `transformers` `dark` `of` `the` `moon` is just a weird query. Let’s
    repeat the process for something completely unrelated, this time for `dryer` in
    figure 11.3.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 但也许“transformers dark of the moon”只是一个奇怪的查询。让我们重复这个过程，这次针对图11.3中的“dryer”进行。
- en: '![figure](../Images/CH11_F03_Grainger.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F03_Grainger.png)'
- en: Figure 11.3 Search results ranked by CTR for the query `dryer`. Here we note
    the strange result for the movie *The Independent* that doesn’t seem relevant.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.3 按CTR排序的查询`dryer`的搜索结果。我们注意到电影《独立者》这个奇怪的结果似乎并不相关。
- en: 'In figure 11.3 we see other odd-looking results:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在图11.3中，我们看到其他一些看起来很奇怪的结果：
- en: The first two results are clothes dryers, which seems good.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前两个结果是衣物烘干机，这似乎是好的。
- en: Following the clothes dryers are clothes-dryer parts. Hmm, OK?
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是衣物烘干机的部件。嗯，好吧？
- en: A movie called *The Independent* shows up. This seems completely random. Why
    would this be rated so highly?
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出现了一部名为《独立者》的电影。这似乎是完全随机的。为什么会被评价得这么高？
- en: Next there’s a washer accessory, which is kind of related.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是一个洗衣机配件，有点相关。
- en: Finally, we see hair dryers, which shows another potential meaning of the word
    “dryer”.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们看到吹风机，这显示了“dryer”一词的另一个潜在含义。
- en: What do you think of the judgments produced by the CTR click model? Think back
    to what you learned in chapter 10\. Remember this is the foundation, the very
    target, of your LTR model. Do you think these judgments would lead to a good LTR
    model that would ultimately succeed if put into production?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为CTR点击模型产生的判断如何？回想一下你在第10章中学到的内容。记住，这是你的LTR模型的基础，也是目标。你认为这些判断会导致一个最终投入生产时能够成功的良好LTR模型吗？
- en: 'We also encourage you to ask yourself a more fundamental question: how could
    we even tell if a judgment list is good? Our subjective interpretation could be
    as flawed as the data in a click model. We’ll consider this more analytically
    in chapter 12\. For this chapter, we’ll let our instincts guide us to possible
    problems.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也鼓励你自己问一个更基本的问题：我们如何才能判断一个判断列表是否良好？我们的主观解释可能和点击模型中的数据一样有缺陷。我们将在第12章中更深入地分析这个问题。对于本章，我们将让我们的直觉引导我们去发现可能的问题。
- en: 11.1.4 Common biases in judgments
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.4 判断中的常见偏差
- en: We’ve seen so far that we can create probabilistic judgments—those with grades
    between 0.00 and 1.00—simply by dividing the number of clicks on a product by
    the number of times that product is returned by search. The output, however, seemed
    to be a bit wanting, as it included movies unrelated to the Transformers franchise.
    We also saw a movie placed in the search results for `dryer`!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止已经看到，我们可以通过将产品点击次数除以该产品被搜索返回的次数来创建概率判断——这些判断的等级在0.00到1.00之间。然而，输出结果似乎有点不足，因为它包括了与变形金刚系列无关的电影。我们还看到一部电影被放在了搜索结果中，搜索词是`dryer`！
- en: It turns out that search click data is full of biases. Here, we’ll briefly define
    what we mean by “bias” before exploring each of these biases in the RetroTech
    click data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，搜索点击数据充满了偏差。在这里，我们将在探索RetroTech点击数据中的每个这些偏差之前，简要定义我们所说的“偏差”。
- en: 'With click models, a *bias* is a reason that raw user click data can have nothing
    to do with the relevance of search results. Instead, biases define how clicks
    (or the lack of clicks) reflect user psychology, search user interface design,
    or noisy data. We can separate biases into two broad groups: nonalgorithmic and
    algorithmic biases. *Algorithmic biases* are those inherent in the ranking, display,
    and interaction with search results. *Nonalgorithmic biases* occur for reasons
    only indirectly related to search ranking.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在点击模型中，*偏差*是原始用户点击数据与搜索结果的相关性无关的原因。相反，偏差定义了点击（或未点击）如何反映用户心理、搜索用户界面设计或噪声数据。我们可以将偏差分为两大类：非算法偏差和算法偏差。*算法偏差*是排名、显示和与搜索结果交互中固有的。*非算法偏差*是由于与搜索排名间接相关的原因发生的。
- en: 'Algorithmic biases can include the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 算法偏差可能包括以下内容：
- en: '*Position bias*—Users click on higher-ranked results more than lower-ranked
    results.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*位置偏差*—用户点击排名较高的结果比排名较低的结果多。'
- en: '*Confidence bias*—Documents with little signal data influence judgments the
    same as documents with much more data.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自信偏差*—信号数据很少的文档与数据量更多的文档对判断的影响相同。'
- en: '*Presentation Bias*—If search never surfaces particular results, users never
    click them, so the click model won’t know whether they’re relevant.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*展示偏差*—如果搜索从未显示特定的结果，用户永远不会点击它们，因此点击模型将不知道它们是否相关。'
- en: 'Nonalgorithmic biases, on the other hand, are biases like the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，非算法偏差是以下这样的偏差：
- en: '*Attractiveness bias*—Some results appear attractive and generate clicks (perhaps
    due to better images or wording selection), but they turn out to be spammy or
    just irrelevant.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*吸引力偏差*—一些结果看起来很有吸引力并产生点击（可能由于更好的图像或措辞选择），但结果却是垃圾或完全不相关。'
- en: '*Performance bias*—Users give up on slow searches, get distracted, and end
    up not clicking anything or clicking only on the earliest-returned results.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性能偏差*—用户放弃缓慢的搜索，分心，最终什么也不点击，或者只点击最早返回的结果。'
- en: Since this book is about *AI-powered* search, we will focus our discussion on
    *algorithmic* biases in search clickstream data. We’ll cover position bias and
    confidence bias in this chapter. Presentation bias will be covered in chapter
    12.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这本书是关于*人工智能驱动*的搜索，我们将重点讨论搜索点击流数据中的*算法*偏差。本章将涵盖位置偏差和自信偏差，展示偏差将在第12章中介绍。
- en: But nonalgorithmic biases matter too! Search is a complex ecosystem that goes
    beyond relevance rankings. If results are frequently clicked, but follow-up actions
    like sales or other conversions don’t occur, it might not be a ranking problem—perhaps
    you have a problem with spammy products. Or you might have a problem with the
    product pages or checkout process. You may find yourself asked to improve “relevance”
    when the limiting factor is actually the user experience, the content, or the
    speed of search.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但非算法偏差也很重要！搜索是一个复杂的生态系统，它超越了相关性排名。如果结果经常被点击，但后续行动如销售或其他转化没有发生，这可能不是一个排名问题——可能你有垃圾产品的问题。或者你可能有问题与产品页面或结账流程。当你发现限制因素实际上是用户体验、内容或搜索速度时，你可能会被要求提高“相关性”。
- en: Now that we’ve reflected on our first click model, let’s work to overcome the
    first bias.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经反思了我们的第一个点击模型，让我们努力克服第一个偏差。
- en: 11.2 Overcoming position bias
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 克服位置偏差
- en: 'In the previous section, we saw our first click model in action: a simple CTR
    click model. This divided the number of times a product was clicked in search
    by the number of times it was returned in the top results. We saw that this was
    quite a flawed approach, noting numerous reasons it could be biased. Specifically,
    we pointed out position bias, confidence bias, and presentation bias as three
    of the algorithmic biases present in our click model. It’s time to begin tackling
    those problems!'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了我们的第一个点击模型在行动：一个简单的点击率点击模型。这个模型将产品在搜索中被点击的次数除以它在顶部结果中出现的次数。我们看到了这是一个相当有缺陷的方法，并指出有多个可能导致偏差的原因。具体来说，我们指出了位置偏差、信心偏差和展示偏差作为我们点击模型中存在的三种算法偏差。是时候开始解决这些问题了！
- en: In this section, we’ll focus on the first of those algorithmic biases, position
    bias, digging into the problem and working on a click model designed to overcome
    it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点关注这些算法偏差中的第一个，即位置偏差，深入探讨这个问题，并致力于设计一个旨在克服它的点击模型。
- en: 11.2.1 Defining position bias
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 定义位置偏差
- en: '*Position bias* is present in most search systems. If users are shown search
    results, they tend to prefer highly ranked search results over lower ones, even
    when those lower results are in fact more relevant. Joachims, et al. in their
    paper “Evaluating the Accuracy of Implicit Feedback from Clicks and Query Reformulations
    in Web Search” ([www.cs.cornell.edu/people/tj/publications/joachims_etal_07a.pdf](http://www.cs.cornell.edu/people/tj/publications/joachims_etal_07a.pdf))
    discuss several reasons for position biases to exist:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*位置偏差*存在于大多数搜索系统中。如果用户看到了搜索结果，他们往往会更倾向于选择排名较高的搜索结果，而不是排名较低的结果，即使那些较低的结果实际上更相关。Joachims等人在其论文“评估Web搜索中点击和查询重构的隐式反馈的准确性”([www.cs.cornell.edu/people/tj/publications/joachims_etal_07a.pdf](http://www.cs.cornell.edu/people/tj/publications/joachims_etal_07a.pdf))中讨论了位置偏差存在的原因：'
- en: '*Trust bias*—Users trust that the search engine must know what it’s doing,
    so they interact with higher results more.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*信任偏差*—用户相信搜索引擎必须知道自己在做什么，因此他们更倾向于与排名较高的结果互动。'
- en: '*Scanning behaviors*—Users examine search results in specific patterns, such
    as top-to-bottom, and often don’t explore everything in front of them.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*扫描行为*—用户以特定的模式检查搜索结果，例如从上到下，并且通常不会探索他们面前的一切。'
- en: '*Visibility*—Higher ranked results are likely to be rendered on the user’s
    screen, so users need to scroll to see the remaining results.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可见性*—排名较高的结果更有可能在用户的屏幕上显示，因此用户需要滚动才能看到剩余的结果。'
- en: With these factors in mind, let’s see if we can detect position bias in the
    RetroTech sessions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些因素，让我们看看我们是否可以在RetroTech会话中检测到位置偏差。
- en: 11.2.2 Position bias in RetroTech data
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.2 RetroTech数据中的位置偏差
- en: How much position bias exists in the sessions in the RetroTech dataset? If we
    can quantify this, we can consider how exactly we can remedy this problem. Let’s
    assess the bias quickly before we consider a new click model for overcoming these
    biases.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: RetroTech数据集中存在多少位置偏差？如果我们能量化这一点，我们就可以考虑如何确切地解决这个问题。在我们考虑一个新的点击模型来克服这些偏差之前，让我们快速评估这个偏差。
- en: By looking at all sessions across all queries, we can compute an average CTR
    per rank. This will tell us how much position bias exists in the RetroTech click
    data. We do this in the following listing.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看所有查询的所有会话，我们可以计算每个排名的平均CTR。这将告诉我们RetroTech点击数据中存在多少位置偏差。我们在以下列表中这样做。
- en: Listing 11.5 CTR per rank in search sessions across all queries
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.5 所有查询的搜索会话中的点击率按排名
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can see in listing 11.5 that users click higher positions more. The CTR
    of results at rank `0` is `0.25`, followed by `0.143` at rank `1`, and so on.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在列表11.5中看到用户点击较高位置的情况更多。排名`0`的结果的点击率是`0.25`，其次是排名`1`的`0.143`，依此类推。
- en: Further, we can see position bias when we compare the CTR judgments from earlier
    to the typical ranking for each product in a query. If position bias is present,
    then our judgment’s ideal ranking will end up resembling the typical ranking shown
    to users. We can analyze this by averaging the rank of each document over every
    session to see where they appear.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当我们比较早期的CTR判断与每个产品在查询中的典型排名时，我们还可以看到位置偏差。如果存在位置偏差，那么我们判断的理想排名最终将类似于显示给用户的典型排名。我们可以通过平均每个会话中每个文档的排名来分析这一点，以查看它们出现在哪里。
- en: The following listing shows the typical search results page for `transformers
    dark of the moon` sessions.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了`transformers dark of the moon`会话的典型搜索结果页面。
- en: Listing 11.6 Examining ranking for `transformers` `dark` `of` `the` `moon`
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.6 检查`transformers` `dark` `of` `the` `moon`的排名
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In listing 11.6, some documents, like 24543701538 and 47875842328, historically
    occur toward the top of the search results for this query. They will be clicked
    more due to position bias. The typical results page, shown in figure 11.4, overlaps
    quite a lot with the CTR rank from figure 11.2.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表11.6中，一些文档，如24543701538和47875842328，在这个查询的历史搜索结果中通常出现在顶部。由于位置偏差，它们将被点击得更多。典型的结果页面，如图11.4所示，与图11.2中的CTR排名有很大的重叠。
- en: '![figure](../Images/CH11_F04_Grainger.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH11_F04_Grainger.png)'
- en: Figure 11.4 Typical search result page for the `transformers` `dark` `of` `the`
    `moon` query. Notice the irrelevant movies like *The A-Team* and *Fast Five* showing
    up. Also note the high ranking of the Wii game. The high position of these results
    and the fact that they get clicked more just by showing up higher in the list
    explains why the CTR model erroneously thinks these are relevant.
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.4 对于查询`transformers dark of the moon`的典型搜索结果页面。注意无关的电影，如*The A-Team*和*Fast
    Five*的出现。还要注意Wii游戏的排名很高。这些结果的高位置以及它们仅仅因为出现在列表中就获得更多点击的事实，解释了为什么CTR模型错误地认为这些是相关的。
- en: Unfortunately, CTR is primarily influenced by position bias. Users click on
    the odd movies in figure 11.4 because the search engine returns them highly for
    this query, not because they are relevant. If we train an LTR model just on CTR,
    we would be asking the LTR model to optimize for what users already see and interact
    with. We must account for position bias when automating LTR.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，CTR主要受到位置偏差的影响。用户点击图11.4中的奇怪电影，是因为搜索引擎为这个查询返回了它们，而不是因为它们相关。如果我们仅仅基于CTR来训练LTR模型，我们就是在要求LTR模型优化用户已经看到和交互的内容。在自动化LTR时，我们必须考虑位置偏差。
- en: Next, let’s see how we can overcome position bias in a more robust click model
    that compensates for position bias.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看我们如何在更健壮的点击模型中克服位置偏差，该模型可以补偿位置偏差。
- en: '11.2.3 Simplified dynamic Bayesian network: A click model that overcomes position
    bias'
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.3 简化的动态贝叶斯网络：一个克服位置偏差的点击模型
- en: You’ve seen the harm position bias can do in action! If we just use clicks directly,
    we will train our LTR model to reinforce the ranking already shown to users. It’s
    time to introduce a click model that can overcome position bias. We’ll start by
    defining an “examine”, a key concept in modeling position bias. We’ll then introduce
    one particular click model that uses this examine concept to adjust raw clicks
    to overcome position bias.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了位置偏差在实际操作中可能造成的危害！如果我们仅仅使用点击数据，我们将训练我们的LTR模型来强化用户已经看到的排名。是时候引入一个能够克服位置偏差的点击模型了。我们将从定义“检查”，一个在建模位置偏差中的关键概念开始。然后，我们将介绍一个特定的点击模型，该模型使用这个“检查”概念来调整原始点击数据以克服位置偏差。
- en: How click models overcome position bias with an “examine” event
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何通过“检查”事件克服位置偏差
- en: The basic CTR calculation doesn’t *really* account for how users scan search
    results. The user likely considers only a few results—biased by position—before
    deciding to click one or two. If we can capture which results users consciously
    consider before clicking, we might be able to overcome position bias. Click models
    do exactly this by defining the concept of an “examine”. We’ll explore this concept
    before building a click model that overcomes position bias.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的点击率（CTR）计算并没有真正考虑到用户是如何扫描搜索结果的。用户在决定点击一个或两个结果之前，可能只会考虑几个结果——这些结果受到位置的影响。如果我们能够捕捉到用户在点击之前有意识地考虑的结果，我们可能能够克服位置偏差。点击模型正是通过定义“检查”的概念来实现这一点的。在我们构建一个能够克服位置偏差的点击模型之前，我们将探讨这个概念。
- en: What is an examine? You may be familiar with an *impressio**n*—when a UI element
    is rendered on the visible part of a user’s screen. In click models, we consider
    instead an *examine*, the probability that a search result was consciously considered
    by the user. As we know, users often fail to notice something right in front of
    their eyes. You may have even been that user! Figure 11.5 captures this concept,
    contrasting impressions with examines.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是“检查”？你可能对“印象”很熟悉——当一个UI元素渲染在用户屏幕的可见部分时。在点击模型中，我们考虑的是“检查”，即用户有意识地考虑搜索结果的可能性。正如我们所知，用户往往无法注意到他们眼前的东西。你可能就是那个用户！图11.5捕捉了这个概念，将“印象”与“检查”进行了对比。
- en: '![figure](../Images/CH11_F05_Grainger.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH11_F05_Grainger.png)'
- en: Figure 11.5 Impressions are whatever is rendered in the viewport (the monitor-shaped
    square) while examines are what the user considers (the results with eyeballs
    adjacent). Modeling what users examine helps correctly account for how users interact
    with search results.
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.5中，印象是视口中渲染的内容（显示器形状的方形），而检查是用户考虑的内容（带有眼睛的搜索结果）。建模用户检查的内容有助于正确地解释用户如何与搜索结果互动。
- en: You can see in figure 11.5 that the user fails to notice the Nintendo game in
    the second position, even though it’s being rendered on their monitor. If the
    user didn’t examine it, a click model shouldn’t penalize the Nintendo game’s relevance.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图11.5中看到，用户没有注意到第二位上的任天堂游戏，尽管它在他们的显示器上被渲染。如果用户没有检查它，点击模型不应该惩罚任天堂游戏的相关性。
- en: Why does tracking examines help overcome position bias? Examines are how a click
    model understands position bias. Another way of saying “position bias” is “we
    think that whether users examine search results depends on the position.” As a
    result, modeling examines correctly is a core activity of most click models. Some
    click models, like the *position-based model* (PBM) attempt to determine an examine
    probability per position across all searches. Others, like the *cascading model*
    or, as we’ll see soon, the *dynamic Bayesian network* (DBN) models, assume that
    if a result was above the last click on the search page, it likely was examined.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么跟踪检查有助于克服位置偏差？检查是点击模型理解位置偏差的方式。另一种说法“位置偏差”是“我们认为用户是否检查搜索结果取决于其位置。”因此，正确建模检查是大多数点击模型的核心活动。一些点击模型，如**基于位置的模型**（PBM），试图确定所有搜索中每个位置上的检查概率。其他，如**级联模型**或，如我们很快将看到的，**动态贝叶斯网络**（DBN）模型，假设如果一个结果位于搜索页面的最后一个点击之上，那么它很可能是被检查过的。
- en: For most click models, the top position usually has a higher examine probability
    than lower ones. This allows click models to adjust for clicks correctly. Items
    examined frequently and clicked are rewarded and seen as more relevant. Those
    examined but not clicked are seen as less relevant.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数点击模型，顶部位置通常比底部位置的检查概率更高。这使得点击模型能够正确调整点击。经常检查并点击的项目会被奖励，并被认为更相关。那些被检查但没有点击的项目被认为不太相关。
- en: To make this more concrete, let’s dive deeper into one of the dynamic Bayesian
    network click models that uses examines to help overcome position bias.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点更具体，让我们深入探讨一个使用检查来帮助克服位置偏差的动态贝叶斯网络点击模型。
- en: Defining a simplified dynamic Bayesian network
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义简化动态贝叶斯网络
- en: A *simplified dynamic Bayesian network* (SDBN) is a slightly less accurate version
    of the more complex dynamic Bayesian network (DBN) click model. These click models
    assume that, within a search session, the probability that a user examined a document
    depends heavily on whether it was positioned at or above the lowest clicked document.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**简化动态贝叶斯网络**（SDBN）是更复杂的动态贝叶斯网络（DBN）点击模型的一个略微不准确版本。这些点击模型假设，在搜索会话中，用户检查一个文档的概率在很大程度上取决于它是否位于或高于最低点击的文档。'
- en: SDBN’s algorithm first marks the last click of each session and then considers
    every document at or above this last click as examined. Finally, it computes a
    relevance grade by simply dividing the total clicks on a document by that document’s
    total examines. We thus get a kind of dynamic CTR, tracking within each user’s
    search session when they likely examined a result, and carefully using this to
    account for how that user evaluated its relevance. We then use these relevance
    evaluations in aggregate across sessions to train the SDBN click model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: SDBN的算法首先标记每个会话的最后一个点击，然后考虑每个会话中或高于此最后一个点击的每个文档为已检查。最后，它通过简单地用文档上的总点击数除以该文档的总检查数来计算相关性等级。因此，我们得到一种动态点击率（CTR），跟踪每个用户的搜索会话中他们可能检查结果的时间，并仔细利用这一点来评估用户对其相关性的评价。然后，我们使用这些相关性评价在会话中汇总以训练SDBN点击模型。
- en: Let’s follow this algorithm step by step. We’ll first mark the last click of
    each session in the following listing.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步跟踪这个算法。我们首先在以下列表中标记每个会话的最后一个点击。
- en: Listing 11.7 Marking which results were examined in each session
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.7 标记每个会话中哪些结果被检查
- en: '[PRE10]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Computes last_click_per_ session, the max rank where clicked is True per
    session.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 计算每个会话的last_click_per_session，即每个会话中点击为真的最大排名。'
- en: '#2 Marks the last click rank in each session'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 标记每个会话的最后一个点击排名'
- en: '#3 Sets every position at or above the last click to True (otherwise False)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将最后一个点击或以上的每个位置设置为True（否则为False）'
- en: 'Output (truncated):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（截断）：
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In listing 11.7, we find the max rank where `clicked` is `True` by storing it
    in `last_click_per_session`. We then mark positions at or above `last_click_rank`
    as examined in our sessions for `dryer`, as you can see in the output for `sess_id=3`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 11.7 中，我们通过在 `last_click_per_session` 中存储它来找到 `clicked` 为 `True` 的最大排名。然后，我们将
    `last_click_rank` 或以上的位置标记为在我们的 `dryer` 会话中已检查，正如你在 `sess_id=3` 的输出中可以看到的那样。
- en: With every session updated with examines set to `True` or `False`, we now sum
    the total clicks and examines per document across all sessions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个会话更新后，我们将所有会话中每个文档的总点击和检查次数进行汇总。
- en: Listing 11.8 Sum clicks and examines per `doc_id` for this query
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.8 为此查询按 `doc_id` 汇总点击和检查次数
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output (truncated):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（截断）：
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In listing 11.8, `sessions[sessions["examined"]]` filters to examined rows only.
    Then, for each `doc_id`, we compute the total `clicked` and `examined` counts.
    You can see that some results, like `doc_id=36172950027`, clearly were examined
    a lot with relatively few clicks from users.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 11.8 中，`sessions[sessions["examined"]]` 仅筛选出已检查的行。然后，对于每个 `doc_id`，我们计算每个文档的总
    `clicked` 和 `examined` 计数。你可以看到一些结果，如 `doc_id=36172950027`，显然被检查了很多次，但用户点击次数相对较少。
- en: Finally, we finish the SDBN algorithm in the following listing by computing
    clicks over examines.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在以下列表中通过计算点击次数来完成 SDBN 算法。
- en: Listing 11.9 Compute final SDBN grades
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.9 计算最终的 SDBN 评分
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Output (truncated):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（截断）：
- en: '[PRE15]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the output of listing 11.9, document 856751002097 is seen as the most relevant,
    with a grade of `0.4118`, or `133` clicks out of `323` examines.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 11.9 的输出中，文档 856751002097 被视为最相关，评分为 `0.4118`，即 `323` 次检查中的 `133` 次点击。
- en: Let’s revisit our two queries to see how the ideal results now look for `dryer`
    and `transformers dark of the moon`. Figure 11.6 shows results for `dryer`, and
    figure 11.7 shows the `transformers dark of the moon` results.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视我们的两个查询，看看现在对于 `dryer` 和 `transformers dark of the moon` 的理想结果看起来如何。图
    11.6 展示了 `dryer` 的结果，而图 11.7 展示了 `transformers dark of the moon` 的结果。
- en: '![figure](../Images/CH11_F06_Grainger.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH11_F06_Grainger.png)'
- en: Figure 11.6 Ideal search results for the query `dryer` according to SDBN. Notice
    how SDBN seems to promote more results related to washing clothes.
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.6 根据 SDBN 的查询 `dryer` 的理想搜索结果。注意 SDBN 似乎促进了更多与洗衣相关的结果。
- en: '![figure](../Images/CH11_F07_Grainger.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH11_F07_Grainger.png)'
- en: Figure 11.7 Ideal search results for the query `transformers` `dark` `of the
    moon` according to SDBN. We’ve now surfaced the DVD, Blu-ray movie, and CD soundtrack.
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.7 根据 SDBN 的查询 `transformers` `dark` `of the moon` 的理想搜索结果。我们现在已经展示了 DVD、蓝光电影和
    CD 原声带。
- en: If we subjectively examine figures 11.6 and 11.7, both sets of judgments appear
    more intuitive than the previous CTR judgments. In our `dryer` example, the emphasis
    appears to be on washing clothes. There are some accessories (such as the dryer
    balls) that score roughly the same as the dryers themselves.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们主观地审视图 11.6 和 11.7，这两组判断似乎比之前的 CTR 判断更直观。在我们的 `dryer` 例子中，重点似乎在洗衣上。有一些配件（如烘干球）的得分与烘干机本身大致相同。
- en: For `transformers dark of the moon`, we note the very high grade for the Blu-ray
    movie. We also see the DVD and CD soundtrack ranking higher than other secondary
    “Dark of the Moon” items such as video games. Somewhat oddly, the soundtrack CD
    is ranked higher than the movie DVD—perhaps we should investigate this more.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `transformers dark of the moon`，我们注意到蓝光电影的评分非常高。我们还看到 DVD 和 CD 原声带在“暗月”的其它二级项目（如视频游戏）中排名更高。有些奇怪的是，原声带
    CD 的排名高于电影 DVD——也许我们应该进一步调查这一点。
- en: Of course, as we’ve said earlier, we’re using our intuition for now. In chapter
    12, we’ll think more objectively about how we might evaluate judgment quality.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，正如我们之前所说的，我们现在正在使用我们的直觉。在第 12 章中，我们将更客观地思考我们如何评估判断质量。
- en: 'With our position bias more under control, we’ll now move on to fine-tune our
    judgments to handle another crucial bias: confidence bias.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的位置偏差得到更好的控制后，我们现在将转向微调我们的判断，以处理另一个关键偏差：置信度偏差。
- en: '11.3 Handling confidence bias: Not upending your model due to a few lucky clicks'
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 处理置信度偏差：不要因为少数幸运点击而颠覆你的模型
- en: In the game of baseball, a player’s batting average tells us the proportion
    of hits they get for every at bat. A great professional player has a batting average
    greater than 0.3\. Consider, however, a lucky little league baseball player stepping
    up to the plate for their first at bat and getting a hit. Their batting average
    is technically 1.0! We can conclude, then, that this young child is a baseball
    prodigy and will certainly have a great baseball career. Right?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在棒球比赛中，一个球员的击球率告诉我们他们在每次击球中得到的击球比例。一位伟大的职业球员的击球率大于0.3。然而，考虑一下这位幸运的小联盟棒球球员第一次击球就击中球的情况。他们的击球率在技术上为1.0！因此，我们可以得出结论，这位年轻的孩子是一个棒球天才，并且肯定会有一个伟大的棒球生涯。对吗？
- en: Not quite! In this section, we’re going to explore the relevance side of this
    lucky little leaguer. What do we do with results that, perhaps simply out of luck,
    have been examined only a few times, each resulting in a click? These likely shouldn’t
    get a perfect grade of 1.0\. We’ll see (and correct) this problem in our data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 不完全是这样！在本节中，我们将探讨这位幸运的小联盟球员的相关性方面。对于那些可能仅仅因为运气好，只被检查过几次，每次都导致点击的结果，我们该怎么办？这些结果可能不应该得到完美的1.0分。我们将在我们的数据中看到（并纠正）这个问题。
- en: 11.3.1 The low-confidence problem in click data
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 点击数据中的低置信度问题
- en: Let’s look at the data to see where low-confidence data points are biasing the
    training data. We’ll then see how we can compensate for low-confidence problems
    in the SDBN results. To define the problem, let’s look at the SDBN results for
    `transformers` `dark of` `the` `moon` and another, rarer, query to see common
    low-confidence situations.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据，看看低置信度数据点在哪里影响了训练数据。然后我们将看到我们如何补偿SDBN结果中的低置信度问题。为了定义这个问题，让我们看看`transformers`
    `dark of` `the` `moon`和另一个更罕见的查询的SDBN结果，看看常见的低置信度情况。
- en: If you recall, it was a bit suspicious that the soundtrack CD for the *Transformers
    Dark of The Moon* movie ranked so highly according to SDBN. When we examine the
    raw data underlying the rankings, we can see a possible problem. In the following
    listing, we reconstruct the SDBN data for `transformers` `dark` `of` `the` `moon`
    to debug this problem, combining listings 11.7–11.9.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，根据SDBN，*变形金刚：月黑之时*电影的原声带CD排名相当高，这有点可疑。当我们检查排名背后的原始数据时，我们可以看到可能存在的问题。在下面的列表中，我们重构了`transformers`
    `dark` `of` `the` `moon`的SDBN数据来调试这个问题，结合了列表11.7-11.9。
- en: Listing 11.10 Recomputing SDBN statistics
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.10 重新计算SDBN统计数据
- en: '[PRE16]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Output (truncated):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（截断）：
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the output of listing 11.10, note that the top result, the Blu-ray movie
    (`doc_id=97360810042`), has far more examines (`642`) than the soundtrack CD (`doc_id=400192926087`
    with `129` examines). The Blu-ray’s grade is more reliable, given it has had many
    times more opportunities for user interaction, making it less likely to be dominated
    by noisy, spurious clicks. The CD, on the other hand, has far fewer examines.
    Shouldn’t the Blu-ray’s relevance grade be weighed higher, given that it’s a more
    reliable data point compared to the CD with more limited data?
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表11.10的输出中，注意最上面的结果，蓝光电影（`doc_id=97360810042`），有更多的检查次数（`642`）比原声带CD（`doc_id=400192926087`，`129`次检查）。鉴于蓝光有更多用户互动的机会，它的评分更可靠，因此它不太可能被噪声点击所主导。另一方面，CD的检查次数要少得多。鉴于CD的数据有限，蓝光的相关性评分不应该更高吗？
- en: Often, this situation is even starker, particularly when dealing with less common
    queries. Regardless of the number of queries your search engine receives, some
    queries are likely to be received many times (*head queries*), some a moderate
    number of times (*torso queries*), and some very rarely (*long-tail queries*,
    or simply *tail queries*). Consider the query `blue ray`. You’ll note that this
    is a common misspelling of “Blu-ray”. As a common mistake, it likely mixes documents
    with a modest number of examines with documents receiving very few. In the following
    listing, we compute the SDBN statistics for `blue ray`, which suffers from this
    data sparsity problem.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，这种状况甚至更为明显，尤其是在处理不太常见的查询时。无论你的搜索引擎接收多少查询，一些查询可能会被多次接收（*头部查询*），一些会被适度次数接收（*躯干查询*），而一些则非常罕见（*长尾查询*，或简称*尾部查询*）。以查询“blue
    ray”为例。你会注意到这实际上是“Blu-ray”的常见误拼。作为一个常见的错误，它可能将包含适度数量检查的文档与接收非常少检查的文档混合在一起。在下面的列表中，我们计算了`blue
    ray`的SDBN统计数据，它受到了这种数据稀疏问题的影响。
- en: Listing 11.11 SDBN judgments for a query with sparse data
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.11 对于稀疏数据的查询的SDBN判断
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#1 Randomly samples a few sessions to simulate a typical long-tail case'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 随机抽取几个会话来模拟典型的长尾案例'
- en: 'Output (truncated):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（截断）：
- en: '[PRE19]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Looking at the output of listing 11.11, we see something unsettling. Like the
    most extreme case of our lucky little league baseball player, the most relevant
    result, doc 600603132872, receives a grade of `1.0` (perfectly relevant) after
    being examined by only one user! This grade of `1.0` ranks higher than the next
    result, which has a grade of `0.411` based on `34` examines. When you consider
    that doc 600603132872 is a set of Blu-ray cases and 827396513927 is a Blu-ray
    player, this feels more troubling. Our subjective interpretation might rank the
    player above the cases. Shouldn’t the fact that the second result was examined
    significantly more count for something?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 查看列表11.11的输出，我们发现了一些令人不安的情况。就像我们幸运的小联盟棒球选手最极端的情况一样，最相关的结果，文档600603132872，在仅被一个用户检查后得到了`1.0`（完全相关）的评分！这个`1.0`的评分高于下一个结果，该结果的评分为`0.411`，基于`34`次检查。当你考虑到文档600603132872是一套蓝光案例，而827396513927是蓝光播放器时，这感觉更加令人不安。我们主观的解释可能会将播放器排在案例之上。第二结果被检查得更多，这不应该算作某种因素吗？
- en: What we’ve seen in these examples is *confidence bias*—when a judgment list
    has many grades based on statistically insignificant, spurious events. We say
    these spurious events with few examines have low confidence, whereas those with
    more examines provide a higher level of confidence. No matter your click model,
    you likely have many situations where queries have only a modest amount of traffic.
    To automate LTR, you’ll need to adjust your training data generation to account
    for the confidence you have in the data.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例中，我们所看到的是**置信度偏差**——当一个判断列表基于统计上不显著、虚假的事件有很多评分时。我们说这些检查次数很少的虚假事件具有低置信度，而那些检查次数更多的提供了更高水平的置信度。无论你的点击模型如何，你很可能有很多查询只有少量流量的情况。为了自动化LTR，你需要调整你的训练数据生成，以考虑你对数据的置信度。
- en: Now that you’ve seen the effect of low-confidence data, we can move on to some
    solutions you can apply when building your click model.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经看到了低置信度数据的影响，我们可以继续讨论在构建你的点击模型时可以应用的一些解决方案。
- en: 11.3.2 Using a beta prior to model confidence probabilistically
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.2 使用beta先验概率建模置信度
- en: We’ve just seen a few problems created by valuing low-confidence data too highly.
    Without adjusting your models based on your confidence in the data, you won’t
    be able to build a reliable automated LTR system. We could just filter these low-confidence
    examples out, but can we perhaps do something smarter? We’ll discuss an approach
    to preserving all the click-stream data in this section as we introduce the concept
    of beta distributions. But first, let’s discuss why using all data is generally
    preferred over simply filtering out the low-confidence examples.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了过分重视低置信度数据所造成的一些问题。如果不根据你对数据的置信度调整你的模型，你将无法构建一个可靠的自动化LTR系统。我们本可以简单地过滤掉这些低置信度示例，但我们可以做些更智能的事情吗？在本节中，我们将介绍beta分布的概念，同时讨论如何保留所有点击流数据。但首先，让我们讨论为什么使用所有数据通常比简单地过滤掉低置信度示例更受欢迎。
- en: Should we filter out low-confidence judgments?
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们是否应该过滤掉低置信度的判断？
- en: In our click model, should we just remove the low-confidence examples? We don’t
    generally recommend throwing data points away like that.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的点击模型中，我们是否应该仅仅移除低置信度示例？我们通常不推荐像那样丢弃数据点。
- en: Filtering training data, such as data points below some minimum threshold of
    examines, reduces the amount of training data you have. Even with a reasonable
    threshold, documents for a query are typically examined on a power law distribution.
    Users examine some documents very frequently, while examining the vast majority
    very infrequently. A threshold can thus remove too many LTR examples and cause
    an LTR model to miss important patterns. Even with a threshold, you would be left
    with the challenge of how to weight medium-confidence examples against high-confidence
    ones, such as with the `transformers` `dark` `of` `the` `moon` query from earlier.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤训练数据，例如低于某些最小检查阈值的检查点数据，会减少你拥有的训练数据量。即使有一个合理的阈值，查询的文档通常是在幂律分布上被检查的。用户频繁检查一些文档，而大量文档则很少被检查。因此，阈值可能会移除过多的LTR示例，导致LTR模型错过重要的模式。即使有阈值，你仍将面临如何权衡中等置信度示例与高置信度示例的挑战，例如与之前提到的`transformers`
    `dark` `of` `the` `moon`查询。
- en: Instead of using a hard cutoff, we advocate keeping low-confidence examples
    and just weighing all examples based on confidence level. We’ll do this using
    a beta distribution on the computed relevance grades. We’ll then apply this solution
    to fix our SDBN click model judgments.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主张保留低置信度示例，并仅根据置信度水平权衡所有示例。我们将使用计算的相关度等级的贝塔分布来完成这项工作。然后我们将应用这个解决方案来修复我们的SDBN点击模型判断。
- en: Using the beta distribution to adjust for confidence
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用贝塔分布来调整置信度
- en: Beta distributions help us draw conclusions from our clicks and examines based
    on probabilities instead of just biased occurrences. However, before we dive straight
    into using the beta distribution for judgments, let’s first examine the usefulness
    of a beta distribution using our previous, intuitive baseball batting-average
    analogy.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 贝塔分布帮助我们根据概率而不是仅仅基于有偏见的出现来从我们的点击和检查中得出结论。然而，在我们直接使用贝塔分布进行判断之前，让我们首先通过我们之前直观的棒球击球率类比来检验贝塔分布的有用性。
- en: 'In baseball, a batting average of 0.295 for a player means that when this player
    goes to bat, there’s roughly a 29.5% chance they will get a hit. But if we wanted
    to know “What’s the batting average for that player, batting in Fenway Park in
    September on rainy days”, we’d probably have very little information to go on.
    The player may have only batted in those conditions a handful of times. Maybe
    they made 2 hits out of 3 tries in those conditions. We would conclude their batting
    average in these cases is 2/3 or 0.67\. We know by now that this conclusion would
    be a mistake: do we really think, based on only 3 chances at bat, we can conclude
    that the player has an improbably high 66.7% chance of making a hit? A better
    approach would be to use the 0.295 general batting average as an initial belief,
    moving slowly away from that assumption as we gradually gain more data on “Fenway
    Park in September on rainy days” at bats.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在棒球中，一名球员的0.295击球率意味着当这名球员击球时，大约有29.5%的几率他们会击中。但如果我们想知道“这名球员在九月雨天在芬威球场击球的击球率是多少”，我们可能几乎没有信息可用。这名球员可能只在这种条件下击球几次。也许他们在那种条件下2次尝试中击中了2次。我们会得出结论，他们在这些情况下的击球率是2/3或0.67。现在我们知道这个结论是错误的：我们真的认为，基于只有3次击球的机会，我们就可以得出这名球员有66.7%的不太可能击中的高概率吗？更好的方法是将0.295的一般击球率作为初始信念，随着我们逐渐获得更多关于“九月雨天在芬威球场击球”的数据，我们逐渐远离这个假设。
- en: 'The *beta distribution* is a tool used to manage beliefs. It turns a probability,
    like a batting average or judgment grade, into two values, `a` and `b`, that represent
    the probability as a distribution. The `a` and `b` values can be interpreted as
    follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*贝塔分布*是一种用于管理信念的工具。它将一个概率，如击球率或判断等级，转换为两个值，`a`和`b`，这些值代表概率分布。`a`和`b`值可以这样解释：'
- en: '`a` *(the successes)*—The number of at bats with hits we’ve observed, or the
    number of examines with clicks'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`a` *(成功次数)*—我们观察到的击中次数，或者点击的检查次数'
- en: '`b` *(the failures)*—The number of at bats without hits we’ve observed, or
    the number of examines without clicks'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b` *(失败次数)*—我们观察到的未击中次数，或者未点击的检查次数'
- en: With the beta distribution, the property `mean` `=` `a` `/` `(a` `+` `b)` holds,
    where `mean` is the initial point value, like a batting average. Given a `mean`,
    we can find many `a` and `b` values that satisfy `mean` `=` `a` `/` `(a` `+` `b)`.
    After all, `0.295` `=` `295` `/` `(295` `+` `705)` as does `0.295` `=` `1475`
    `/` `(1475` `+` `3525)` and so on. Yet each represents a different beta distribution.
    Keep this property in mind as we move along.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝塔分布，属性`均值` `=` `a` `/` `(a` `+` `b)`成立，其中`均值`是初始点值，如击球率。给定一个`均值`，我们可以找到许多满足`均值`
    `=` `a` `/` `(a` `+` `b)`的`a`和`b`值。毕竟，`0.295` `=` `295` `/` `(295` `+` `705)`，同样`0.295`
    `=` `1475` `/` `(1475` `+` `3525)`等等。然而，每个都代表一个不同的贝塔分布。随着我们的进展，请记住这个属性。
- en: Let’s put these pieces together to see how the beta distribution prevents us
    from jumping to conclusions on spurious click (or batting) data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这些部分放在一起，看看贝塔分布是如何防止我们从虚假点击（或击球）数据中得出结论的。
- en: We could declare our initial belief about any document’s relevance grade as
    `0.125`. This is like declaring the baseball player’s batting average to be 0.295
    as our initial belief of their performance. We can use the beta distribution to
    update the initial belief for specific cases like “Fenway Park in September on
    rainy days” or a specific document’s relevance for a search query.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将我们对任何文档相关性等级的初始信念声明为`0.125`。这就像声明棒球运动员的击球率为0.295作为我们对他们表现的初始信念一样。我们可以使用beta分布来更新特定案例的初始信念，例如“9月的雨天在芬威公园”或特定文档对搜索查询的相关性。
- en: The first step is to pick an `a` and a `b` that capture our initial belief.
    For our relevance case, we could choose many values for `a` and `b` that satisfy
    `0.125` `=` `a` `/` `(a` `+` `b)`. Suppose we choose `a=2.5,` `b=17.5` as our
    relevance belief on documents with no clicks. Graphing this, we would see the
    distribution in figure 11.8.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是选择一个`a`和一个`b`来捕捉我们的初始信念。对于我们的相关性案例，我们可以选择许多满足`0.125` `=` `a` `/` `(a` `+`
    `b)`的`a`和`b`值。假设我们选择`a=2.5,` `b=17.5`作为我们对没有点击的文档的相关性信念。绘制这个，我们会看到图11.8中的分布。
- en: '![figure](../Images/CH11_F08_Grainger.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH11_F08_Grainger.png)'
- en: Figure 11.8 Beta distribution for a relevance grade of 0.125\. The mean corresponds
    to our default relevance grade. We see the distribution of most likely relevance
    grades.
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.8 相关性等级为0.125的beta分布。平均值对应于我们的默认相关性等级。我们看到最可能的相关性等级分布。
- en: We can observe now what happens when we see a document’s first click, incrementing
    that document’s `a` to 3.5\. In figure 11.9 we have `a=3.5, b=17.5`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以观察到当我们看到文档的第一次点击，将文档的`a`增加到3.5时会发生什么。在图11.9中，我们有`a=3.5, b=17.5`。
- en: '![figure](../Images/CH11_F09_Grainger.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH11_F09_Grainger.png)'
- en: Figure 11.9 The beta distribution for a relevance grade after adding one click
    is now `0.1667`. Adding a click “pulls” the probability distribution a little
    toward one direction, updating the initial belief.
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.9 在添加一次点击后，相关性等级的beta分布现在是`0.1667`。添加一次点击“拉动”概率分布稍微向一个方向移动，更新初始信念。
- en: The mean relevance grade for the updated distribution is now `3.5` `/` `(17.5`
    `+` `3.5)` or `0.1667`, effectively pulling the initial belief a little higher,
    given its first click. Without the beta distribution, this document would have
    1 click and 1 examine, resulting in a grade of `1`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 更新分布的平均相关性等级现在是`3.5` `/` `(17.5` `+` `3.5)`或`0.1667`，在考虑到其第一次点击的情况下，有效地将初始信念稍微提高。如果没有beta分布，这篇文档将会有1次点击和1次查看，结果等级为`1`。
- en: We refer to the starting point probability distribution (the chosen `a` and
    `b`) as the *prior distribution*, or just a *prior*. This is our initial belief
    in what will happen. The distribution after updating `a` and `b` for a specific
    case, like a document, is a *posterior distribution*, or just a *posterior*. This
    is our updated belief.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将起始点概率分布（所选的`a`和`b`）称为*先验分布*，或简称*先验*。这是我们对于会发生什么的初始信念。更新`a`和`b`后的特定案例（如文档）的分布是*后验分布*，或简称*后验*。这是我们更新的信念。
- en: Recall that we said earlier that many initial `a` and `b` values could be chosen.
    This has significance, as the magnitude of the initial `a` and `b` make our prior
    weaker or stronger. We could choose any value for `a` and `b` where `a` `/` `(a`
    `+` `b)` `=` `0.125`. But note what happens if we choose a very small value `a=0.25,`
    `b=1.75`. Then we go to update `a` by incrementing it by 1\. The new expected
    value of the posterior distribution is `1.25 /` `(1.25` `+` `1.75)` or ~`0.416`.
    That’s a major effect for just one click. Conversely, using very high `a` and
    `b` values would make a prior so strong it would barely budge. When you use the
    beta distribution, you’ll want to tune the magnitude of the prior so updates have
    the desired effect.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们之前提到可以选择许多初始的`a`和`b`值。这很重要，因为初始`a`和`b`的大小使我们的先验更弱或更强。我们可以选择任何满足`a` `/`
    `(a` `+` `b)` `=` `0.125`的`a`和`b`值。但请注意，如果我们选择一个非常小的值`a=0.25,` `b=1.75`会发生什么。然后我们通过增加1来更新`a`。后验分布的新预期值是`1.25
    /` `(1.25` `+` `1.75)`或约`0.416`。这仅仅是一个点击就能产生重大影响。相反，使用非常高的`a`和`b`值会使先验非常强，几乎不会变动。当你使用beta分布时，你将想要调整先验的大小，以便更新产生期望的效果。
- en: Now that you’ve seen this handy tool for capturing SDBN grades in practice,
    let’s see how the beta distribution can help with our SDBN confidence problems.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了捕捉SDBN等级的实用工具，让我们看看beta分布如何帮助我们解决SDBN置信度问题。
- en: Using a beta prior in SDBN click models
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在SDBN点击模型中使用beta先验
- en: Let’s finish the chapter by updating the SDBN click model using the beta distribution.
    If you use other click models, like the ones alluded to earlier in this chapter,
    you’ll need to reflect on how confidence can be solved in those cases. The beta
    distribution might be a useful tool there, as well.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用beta分布更新SDBN点击模型来结束本章。如果你使用其他点击模型，如本章前面提到的那些，你需要思考在这些情况下如何解决置信度问题。beta分布可能在那里也是一个有用的工具。
- en: If you recall, the output of SDBN was a count of `clicks` and `examines` for
    each document. In listing 11.12, we pick up from listing 11.11, which computed
    the SDBN for the `blue ray` query. We’ll choose a prior grade of `0.3` for use
    with our SDBN model. This is our default grade when we don’t have information
    about the document—possibly derived from the typical grade we see in our judgments.
    We’ll then compute a prior beta distribution (`prior_a` and `prior_b`) using this
    prior grade.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得，SDBN的输出是每个文档的`clicks`和`examines`的计数。在列表11.12中，我们从列表11.11继续，它计算了`blue ray`查询的SDBN。我们将为我们的SDBN模型选择`0.3`作为先验等级。这是我们不知道文档信息时的默认等级——可能来自我们在判断中看到的典型等级。然后，我们将使用这个先验等级计算先验beta分布（`prior_a`和`prior_b`）。
- en: Listing 11.12 Computing prior beta distribution
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.12 计算先验beta分布
- en: '[PRE20]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 Resulting a and b satisfying prior_grade = prior_a / (prior_a + prior_b)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 结果满足先验等级 = prior_a / (prior_a + prior_b)'
- en: '#2 Default prior relevance grade'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 默认的相关性等级'
- en: '#3 How much confidence to place in the prior (prior_weight = a + b)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 在先验中放置多少信心（先验权重 = a + b）'
- en: 'Output (truncated):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（截断）：
- en: '[PRE21]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In listing 11.12, with a weight of `100`, you can confirm that `prior_grade`
    `=` `prior_a` `/ (prior_a` `+` `prior_b)` or `0.3` `=` `30 /` `(30` `+` `70)`.
    This has captured an initial probability distribution for our prior.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表11.12中，使用`100`的权重，你可以确认`prior_grade` `=` `prior_a` `/ (prior_a` `+` `prior_b)`
    或 `0.3` `=` `30 /` `(30` `+` `70)`。这已经捕捉到了我们先验的初始概率分布。
- en: In listing 11.13, we need to compute a posterior distribution and corresponding
    relevance grade. We do this by incrementing `prior_a` for clicks (our “successes”),
    and `prior_b` for examines with no clicks (our “failures”). Finally, we compute
    an updated grade as the `beta_grade`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表11.13中，我们需要计算后验分布和相应的相关性等级。我们通过增加点击（我们的“成功”）的`prior_a`和没有点击的检查（我们的“失败”）的`prior_b`来实现这一点。最后，我们计算一个更新的等级作为`beta_grade`。
- en: Listing 11.13 Computing posterior beta distribution
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.13 计算后验beta分布
- en: '[PRE22]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#1 Updates our belief about the document’s relevance, increasing from prior_a
    by the number of clicks'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 更新我们对文档相关性的信念，通过点击的数量从`prior_a`增加'
- en: '#2 Updates our belief about the document’s lack of relevance, increasing from
    prior_b by examines without clicks'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 更新我们对文档缺乏相关性的信念，通过没有点击的检查从`prior_b`增加'
- en: '#3 Computes a new grade from posterior_a and posterior_b'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从后验_a和后验_b计算一个新的等级'
- en: 'Output (truncated):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（截断）：
- en: '[PRE23]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the output of listing 11.13, the column headers `clicked`, `examined`, `prior`
    `a`, and `prior_b` were shortened for space to `cl`, `ex`, `pr_a`, and `pr_b`.
    Notice our new ideal results for the query `blue` `ray` by sorting on `beta` `grade`.
    The `beta` `grade` values remain closer to the prior grade of `0.3`. Notably,
    our Blu-ray cases have slid to the third most relevant slot, with the single click
    not pushing the grade much past `0.3`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表11.13的输出中，列标题`clicked`、`examined`、`prior` `a`和`prior_b`为了节省空间被缩短为`cl`、`ex`、`pr_a`和`pr_b`。注意我们通过按`beta`
    `grade`排序的新理想查询结果`blue` `ray`。值得注意的是，我们的蓝光案例滑到了第三个最相关位置，单次点击并没有将等级推过`0.3`。
- en: When we repeat this calculation of judgments for `dryer` and `transformers`
    `dark of` `the` `moon` in figures 11.10 and 11.11, we note that the order is the
    same, but the grades themselves stay closer to the prior of `0.3` depending on
    our confidence in the data.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在图11.10和11.11中重复对`dryer`和`transformers` `dark of` `the` `moon`的判断计算时，我们注意到顺序是相同的，但等级本身更接近先验的`0.3`，这取决于我们对数据的信心。
- en: '![figure](../Images/CH11_F10_Grainger.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F10_Grainger.png)'
- en: Figure 11.10 Beta-adjusted SDBN results for `dryer`. Notice the grades now are
    more tightly focused around the prior grade `0.3`, with some above or below this
    prior.
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.10 对`dryer`的beta调整后的SDBN结果。注意现在等级更紧密地集中在先验等级`0.3`周围，有些高于或低于这个先验。
- en: '![figure](../Images/CH11_F11_Grainger.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F11_Grainger.png)'
- en: Figure 11.11 Beta-adjusted SDBN results for `transformers` `dark` `of` `the`
    `moon`. In figure 11.7, we noted the soundtrack seemed oddly high (`0.48`) in
    its relevance grade despite fewer clicks than the Blu-ray movie. We now see the
    soundtrack’s relevance closer to the prior of `0.4`.
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.11 `transformers` `dark` `of` `the` `moon`的Beta调整后的SDBN结果。在图11.7中，我们注意到尽管点击次数少于蓝光电影，音轨的相关性评分似乎异常高（`0.48`）。我们现在看到音轨的相关性更接近先前的`0.4`。
- en: Figure 11.11 notably shows less confidence in the soundtrack when compared to
    the SDBN judgments without modeling confidence (figure 11.7). The grade has dropped
    from `0.4806` to `0.4017`. Notably, the DVD grade following the CD has not changed
    much, only changing from `0.3951` to `0.3673`, because of our higher confidence
    in that observation. As more observations come in, it’s likely the CD would even
    move down in ranking if this pattern continues.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11明显显示，与未建模置信度的SDBN判断（图11.7）相比，对音轨的置信度较低。等级从`0.4806`降至`0.4017`。值得注意的是，CD之后的DVD等级没有太大变化，只是从`0.3951`变为`0.3673`，因为我们对该观察结果的置信度更高。随着更多观察结果的到来，如果这种模式持续下去，CD甚至可能降级排名。
- en: Most of your queries won’t be like `dryer` or `transformers` `dark` `of` `the`
    `moon`. They’ll be more like `blue` `ray`. To meaningfully work with these queries
    for LTR, you’ll need to be able to handle these “small data” problems, such as
    having lower confidence.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数查询不会像 `dryer` 或 `transformers` `dark` `of` `the` `moon` 这样。它们更像 `blue` `ray`。为了有意义地处理这些查询进行LTR，你需要能够处理这些“小数据”问题，例如具有较低的置信度。
- en: We are beginning to have a more reasonable training set for automating LTR,
    but there’s still work to do. In the next chapter, we’ll move to look at the complete
    search feedback loop. This includes working on presentation bias. Recall that
    this is the bias where users never examine what search never returns to them.
    How can we add surveillance to the automated LTR feedback loop to both overcome
    presentation bias and ensure that our model—and by extension, the judgments—are
    working as expected?
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '我们开始拥有一个更合理的LTR自动化训练集，但仍有许多工作要做。在下一章中，我们将转向查看完整的搜索反馈循环。这包括处理展示偏差。回想一下，这是用户从未检查搜索从未返回给他们的偏差。我们如何向自动LTR反馈循环添加监督，以克服展示偏差并确保我们的模型——以及由此产生的判断——按预期工作？ '
- en: Before we examine those topics in the next chapter, though, let’s look again
    at training an LTR model end-to-end so you can experiment with what you’ve learned
    so far.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们检查下一章中的这些主题之前，让我们再次查看从头到尾训练LTR模型，这样你可以实验你迄今为止学到的知识。
- en: 11.4 Exploring your training data in an LTR system
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 在LTR系统中探索你的训练数据
- en: Great work! You’ve made it through chapters 10 and 11\. You now have what you
    need to develop reasonable LTR training data and train an LTR model. You’re likely
    eager to train a model from your work. Instead of repeating the extensive code
    from chapter 10 here, we’ve created an “End-to-End Automated Learning to Rank”
    notebook in the ch11 folder of the book’s codebase (4.end-to-end-auto-ltr.ipynb).
    It will allow you to experiment with LTR on the RetroTech data (figure 11.12).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！你已经完成了第10章和第11章。你现在有了开发合理的LTR训练数据和训练LTR模型所需的一切。你很可能急于从你的工作中训练一个模型。为了避免在这里重复第10章中的大量代码，我们在书籍代码库的ch11文件夹中创建了一个“端到端自动学习排序”笔记本（4.end-to-end-auto-ltr.ipynb）。它将允许你在RetroTech数据（图11.12）上实验LTR。
- en: 'In this notebook, you can fine-tune the inner LTR engine—the feature engineering
    and model creation that attempts to satisfy the training data. You can also explore
    the implications of altering the automated inputs to this engine: the training
    data itself. Altogether, this notebook has every step you’ve learned about so
    far:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个笔记本中，你可以微调内部LTR引擎——特征工程和模型创建，试图满足训练数据。你还可以探索改变此引擎自动输入的影响：训练数据本身。总之，这个笔记本包含了你迄今为止学到的所有步骤：
- en: Processing raw click session data into judgments, using the SDBN click model
    and a beta prior
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SDBN点击模型和beta先验将原始点击会话数据转换为判断
- en: Transforming the dataframe into the judgments we used in chapter 10
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将dataframe转换为我们在第10章中使用的判断
- en: Loading a selection of LTR features to use with the search engine’s feature
    store capabilities
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一组LTR特征以用于搜索引擎的特征存储功能
- en: Logging these features from the search engine and then performing a pairwise
    transformation of the data into a suitable training set
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从搜索引擎记录这些特征，然后对数据进行成对转换，以形成一个合适的训练集
- en: Training and uploading a model to the search engine’s model store
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索引擎的模型存储中训练和上传模型
- en: Searching and ranking with the model
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型进行搜索和排序
- en: '![figure](../Images/CH11_F12_Grainger.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH11_F12_Grainger.png)'
- en: Figure 11.12 Notebook exploring the full LTR system. You can take the model
    for a test drive.
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.12 探索完整LTR系统的笔记本。你可以对这个模型进行测试。
- en: We invite you to tune the click model parameters and to think of new features,
    and different ways of arriving at the final LTR model, discovering which ones
    seem to yield the best results. While you do this tuning, please be sure to question
    your own subjective assumptions compared to what the data is showing you.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们邀请您调整点击模型参数，并思考新的特征，以及到达最终LTR模型的不同方式，发现哪些似乎能产生最佳结果。在您进行这些调整时，请务必质疑您自己的主观假设与数据向您展示的内容之间的差异。
- en: With out-of-the box tuning, we’ll leave you with figure 11.13, showing the current
    search results for the query `transformers` `dvd`. Try different queries here.
    How can you help the model better discriminate between relevant and irrelevant
    documents? Are the problems you encounter due to the training data used? Or are
    they due to the features used to construct the model?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 通过开箱即用的调整，我们将向您展示图11.13，显示当前查询`transformers` `dvd`的搜索结果。在这里尝试不同的查询。您如何帮助模型更好地区分相关和不相关的文档？您遇到的问题是由于使用的训练数据引起的吗？还是由于构建模型使用的特征引起的？
- en: '![figure](../Images/CH11_F13_Grainger.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH11_F13_Grainger.png)'
- en: Figure 11.13 How our trained model ranks `transformers dvd`. Do you think you
    could improve on this?
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.13 我们的训练模型如何对`transformers dvd`进行排序。您认为您能在这方面做得更好吗？
- en: In the next chapter, we’ll finalize the automated LTR system by performing surveillance
    on the model. Most crucially, we’ll consider how to overcome *presentation bias*.
    Even with the adjustments in this chapter, users will still only ever have a chance
    to act on what the search shows them. So we still have a feedback loop biased
    heavily by the current relevance ranking. How can we look out for this problem
    and overcome it? In the next chapter, we’ll consider these problems as our LTR
    model continues to iteratively incorporate incoming user interactions and actively
    surface additional promising results.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过监控模型来最终确定自动化的LTR系统。最重要的是，我们将考虑如何克服**展示偏差**。即使在本章的调整之后，用户仍然只能对搜索显示的内容采取行动。因此，我们仍然有一个由当前相关性排名严重偏重的反馈循环。我们如何关注这个问题并克服它？在下一章中，我们将考虑这些问题，因为我们的LTR模型继续迭代地整合用户交互并主动展示更多有希望的结果。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: We can automate learning to rank (LTR) if we can reliably transform user click
    data into relevance judgments using a click model. However, the click model must
    be designed carefully to reduce bias in the data and ensure the reliability of
    the automated LTR system when deployed to live users.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们能可靠地将用户点击数据转换为相关性判断，我们可以自动化学习排序（LTR）。然而，点击模型必须精心设计，以减少数据中的偏差，并确保自动化LTR系统在部署给真实用户时的可靠性。
- en: Learned (implicit) relevance judgment lists can be plugged into existing LTR
    training processes to either replace or augment manually created judgments.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习到的（隐式）相关性判断列表可以插入到现有的LTR训练过程中，以替换或增强手动创建的判断。
- en: Raw clicks are usually problematic in automated LTR models due to common biases
    in how algorithms rank and present search results to users.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在自动化的LTR模型中，原始点击通常存在问题，因为算法在如何对用户排名和展示搜索结果方面存在常见的偏差。
- en: Among the visible search results, position bias says users prefer results ranked
    near the top. We can overcome position bias by using a click model that tracks
    the probability that a user has examined a document or a position in the search
    results.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可见的搜索结果中，位置偏差表明用户更喜欢排名靠前的结果。我们可以通过使用跟踪用户检查文档或搜索结果中位置的点击模型来克服位置偏差。
- en: Most search applications have a lot of spurious click data. When training data
    is biased toward these spurious results, we have confidence bias. We can overcome
    confidence bias by using a beta distribution to create a prior that we update
    gradually with new observations as they come in.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数搜索应用都有大量的虚假点击数据。当训练数据偏向这些虚假结果时，我们会产生信心偏差。我们可以通过使用贝塔分布来创建先验，并随着新观察数据的到来逐步更新，来克服信心偏差。
