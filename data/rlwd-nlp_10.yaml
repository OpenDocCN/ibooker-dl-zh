- en: 7 Convolutional neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 卷积神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Solving text classification by detecting patterns
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过检测模式解决文本分类问题。
- en: Using convolutional layers to detect patterns and produce scores
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积层来检测模式并生成分数。
- en: Using pooling layers to aggregate the scores produced by convolution
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用池化层来聚合由卷积产生的分数。
- en: Building a convolutional neural network (CNN) by combining convolution and pooling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过组合卷积和池化来构建卷积神经网络（CNN）。
- en: Building a CNN-based text classifier using AllenNLP
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AllenNLP构建基于CNN的文本分类器。
- en: In previous chapters, we covered linear layers and RNNs, two main neural network
    architectures commonly used in NLP. In this chapter, we introduce another important
    class of neural networks called *convolutional neural networks* (CNNs). CNNs have
    different characteristics than RNNs that make them suitable for NLP tasks where
    detecting linguistic patterns is important, such as text classification.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们介绍了线性层和RNN，这是NLP中常用的两种主要神经网络体系结构。在本章中，我们介绍了另一种重要的神经网络类别，称为*卷积神经网络*（CNN）。CNN具有与RNN不同的特征，使它们适用于检测语言模式至关重要的NLP任务，例如文本分类。
- en: 7.1 Introducing convolutional neural networks (CNNs)
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 介绍卷积神经网络（CNN）
- en: This section introduces convolutional neural networks (CNNs), another type of
    neural network architecture that operates in a different way from how RNNs work.
    CNNs are particularly good at pattern-matching tasks and are increasingly popular
    in the NLP community.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了另一种类型的神经网络体系结构，称为卷积神经网络（CNN），它以与RNN不同的方式运行。CNN特别擅长于模式匹配任务，在NLP社区中越来越受欢迎。
- en: 7.1.1 RNNs and their shortcomings
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 循环神经网络及其缺点。
- en: In chapter 4, we covered sentence classification, which is an NLP task that
    receives some text as the input and produces a label for it. We also discussed
    how to use recurrent neural networks (RNNs) for that task. As a refresher, an
    RNN is a type of neural network that has a “loop” in it, which processes the input
    sequence one element at a time from the beginning until the end. The internal
    loop variable, which is updated at every step, is called the *hidden state*. When
    the RNN finishes processing the entire sequence, the hidden state at the final
    timestep represents the compressed content of the input sequence, which can be
    used for NLP tasks including sentence classification. Alternatively, you can take
    out the hidden state after every step and use it to assign labels (such as PoS
    and named entity tags) to individual words. The structure that is applied repeatedly
    in the loop is called a *cell*. An RNN with a simple multiplication and nonlinearity
    is called a *vanilla* or an *Elman* RNN. On the other hand, LSTM and GRU-based
    RNNs use more complicated cells that employ memory and gating.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们讨论了句子分类，这是一个自然语言处理任务，接收一些文本作为输入并为其生成标签。我们还讨论了如何使用循环神经网络（RNN）来完成该任务。作为复习，RNN是一种具有“循环”的神经网络，它从开头开始逐个元素地处理输入序列直到结束。在每一步更新的内部循环变量称为*隐藏状态*。当RNN完成处理整个序列时，最终时间步长处的隐藏状态表示输入序列的压缩内容，可用于包括句子分类在内的NLP任务。或者，您可以在每一步之后取出隐藏状态并将其用于为单词分配标签（例如PoS和命名实体标签）。在循环中反复应用的结构称为*单元*。具有简单乘法和非线性的RNN称为*香草*或*埃尔曼*
    RNN。另一方面，基于LSTM和GRU的RNN使用更复杂的单元，这些单元使用存储器和门。
- en: RNNs are a powerful tool in modern NLP with a wide range of applications; however,
    they are not without shortcomings. First, RNNs are slow—they need to scan the
    input sequence element by element no matter what. Their computational complexity
    is proportional to the length of the input sequences. Second, due to their sequential
    nature, RNNs are hard to parallelize. Think of a multilayer RNN where multiple
    RNN layers are stacked on top of each other (as shown in figure 7.1). In a naive
    implementation, each layer needs to wait until all the layers below it finish
    processing the input
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: RNN在现代NLP中是一种强大的工具，具有广泛的应用范围；但是，它们并非没有缺点。首先，RNN速度较慢-无论如何都需要逐个元素地扫描输入序列。它们的计算复杂度与输入序列的长度成正比。其次，由于它们的顺序性质，RNN难以并行化。想象一个多层RNN，其中多个RNN层堆叠在一起（如图7.1所示）。在朴素实现中，每一层都需要等到所有下面的层完成对输入的处理。
- en: '![CH07_F01_Hagiwara](../Images/CH07_F01_Hagiwara.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F01_Hagiwara](../Images/CH07_F01_Hagiwara.png)'
- en: Figure 7.1 Multilayer RNN
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 多层RNN
- en: Third, the RNN structure is simply overkill and inefficient for some tasks.
    For example, recall the task of detecting grammatical English sentences that we
    covered in chapter 4\. In its simplest form, the task is to recognize valid and
    invalid subject-verb agreement in a two-word sentence. If a sentence contains
    phrases such as “I am” and “you are,” it’s grammatical. If it contains “I are”
    or “you am,” it’s not. In chapter 4, we built a simple LSTM-RNN with a nonlinearity
    to recognize the grammaticality of two-word sentences with a vocabulary of four
    words. But what if you need to classify whether an arbitrary long sentence with
    a very large vocabulary is grammatical? Suddenly, this process starts to sound
    very complex. Your LSTM needs to learn to pick up the signal (subject-verb agreement)
    from a large amount of noise (all other words and phrases that have nothing to
    do with agreement), while learning to do all this using the update operation that
    gets repeated for every single element of the input.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，对于某些任务，RNN结构过于复杂和低效。例如，在第四章中我们讲解了检测符合语法的句子的任务。在最简单的形式下，任务是在一个两个词组成的句子中识别主谓一致性的正确与否。如果句子包含诸如“I
    am”和“you are”之类的短语，那么它是符合语法的。如果包含“I are”或“you am”，那么就不符合语法。在第四章，我们构建了一个简单的带非线性的LSTM-RNN来识别有四个单词词汇量的两个词组成句子的语法正确性。但是，如果你需要对一个词汇量非常大的任意长度的句子进行分类，这个过程就开始变得非常复杂了。你的LSTM需要从大量的噪声（与一致性无关的其他单词和短语）中识别出信号（主谓一致性），同时学习使用更新操作来处理输入的每一个元素。
- en: But if you think about it, no matter how long the sentence is or how large the
    vocabulary is, your network’s job should still be quite simple—if the sentence
    contains valid collocations (such as “I am” and “you are”), it’s grammatical.
    Otherwise, it’s not. The task is actually not very far from the “if-then” sentiment
    analyzer that we saw in chapter 1\. It is obvious that the structure of LSTM RNNs
    is overkill for this task, where simple pattern matching over words and phrases
    would suffice.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你仔细考虑，无论句子有多长或者词汇量有多大，你的网络的任务应该还是相当简单——如果句子包含有效的短语（如“I am”和“you are”），那么它符合语法。否则，不符合语法。实际上，这个任务与我们在第一章中看到的“如果-那么”情感分析器非常相似。很明显，LSTM
    RNN的结构对于这个任务来说过于复杂，简单的文字和短语模式匹配就足够了。
- en: 7.1.2 Pattern matching for sentence classification
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 句子分类的模式匹配
- en: If you look at text classification in general, many tasks can be effectively
    solved by this “pattern matching.” Take spam filtering, for example—if you want
    to detect spam emails, simply look for words and phrases such as “v1agra” and
    “business opportunity” without even reading the entire email; it doesn’t matter
    where these patterns appear. If you want to detect sentiment from movie reviews,
    detecting positive and negative words such as “amazing” and “awful” would go a
    long way. In other words, learning and detecting such local linguistic patterns,
    regardless of their location, is an effective and efficient strategy for text-classification
    tasks, and possibly for other NLP tasks as well.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看一下文本分类的一般情况，很多任务可以通过“模式匹配”来有效解决。以垃圾邮件过滤为例：如果你想要检测垃圾邮件，只需要查找像“v1agra”和“商机”这样的词语和短语，甚至不需要读完整封邮件；这些模式出现在什么地方并不重要。如果你想要从电影评论中检测情感，检测到像“amazing”和“awful”这样的积极和消极词语就足够了。换句话说，学习和检测这种本地语言模式，而不考虑它们的位置，对于文本分类任务是一种有效而高效的策略，也可能对其他自然语言处理任务有效。
- en: In chapter 3, we learned the concept of n-grams—contiguous sequences of one
    or more words. They are often used in NLP as proxies for more formally defined
    linguistic units such as phrases and clauses. If there’s some tool that can wade
    through a large amount of noise in text and detect n-grams that serve as signals,
    it would be a great fit for text classification.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章，我们学习了n元语法的概念，即一个或多个词的连续序列。它们经常被用作自然语言处理中更正式定义的语言单位（如短语和从句）的代理。如果有一种工具能够遍历大量的文本噪声并检测作为信号的n元语法，那将非常适合文本分类。
- en: 7.1.3 Convolutional neural networks (CNNs)
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 卷积神经网络（CNNs）
- en: Convolutional neural networks, or CNNs, do exactly this. A CNN is a type of
    neural network that involves a mathematical operation called *convolution*, which,
    put simply, detects local patterns that are useful for the task at hand. A CNN
    usually consists of one or more convolutional layers, which do convolution, and
    pooling layers, which are responsible for aggregating the result of convolution.
    See figure 7.2 for a diagram. Sections 7.2 and 7.3 provide some detail of convolutional
    layers and pooling layers, respectively.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）正是做到这一点的。CNN是一种神经网络类型，它涉及一种称为*卷积*的数学运算，简单来说，它检测有用于当前任务的局部模式。CNN通常由一个或多个卷积层和一个或多个池化层组成，卷积层进行卷积操作，池化层负责聚合卷积结果。请参见图7.2。分别在第7.2节和第7.3节中详细介绍卷积层和池化层。
- en: '![CH07_F02_Hagiwara](../Images/CH07_F02_Hagiwara.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F02_Hagiwara](../Images/CH07_F02_Hagiwara.png)'
- en: Figure 7.2 Convolutional neural network
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 卷积神经网络
- en: CNNs, which are inspired by the visual system in the human brain, have been
    widely used for computer vision tasks such as image classification and object
    detection. In recent years, the use of CNNs has been increasingly popular in NLP,
    especially for tasks such as text classification, sequential labeling, and machine
    translation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CNN受到人脑视觉系统的启发，在计算机视觉任务（如图像分类和目标检测）中被广泛使用。近年来，CNN的使用在自然语言处理领域越来越流行，特别是在文本分类、序列标注和机器翻译等任务中。
- en: 7.2 Convolutional layers
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 卷积层
- en: In this section, we’ll discuss convolutional layers, the essential part of the
    CNN architecture. The term *convolution* may sound a bit scary, but at its essence,
    it’s just pattern matching. We’ll use diagrams and intuitive examples to illustrate
    how it really works.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论卷积层，这是CNN架构的核心部分。术语*卷积*听起来可能有点可怕，但本质上它只是一种模式匹配。我们将使用图示和直观的例子来说明它的工作原理。
- en: 7.2.1 Pattern matching using filters
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 使用滤波器进行模式匹配
- en: Convolutional layers are the most important component in CNNs. As mentioned
    earlier, convolutional layers apply a mathematical operation called convolution
    to input vectors and produce output. But what is convolution? Understanding the
    strict definition of convolution requires knowing linear algebra, so we’ll use
    some analogy and concrete examples to understand it. Imagine holding a rectangular-shaped
    patch of colored glass with complex patterns (like the stained glass you see in
    a church) and sliding it over the input sequence while looking through it. If
    the input pattern matches that of the patch, more light goes through the glass
    and you get larger output values. If the input pattern does not look like that
    of the patch or looks the opposite, you get smaller output values. In other words,
    you are looking for particular patterns in the input sequence using a patch of
    colored glass.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是CNN中最重要的组件。如前所述，卷积层将一种称为卷积的数学运算应用于输入向量，并产生输出。但是什么是卷积？理解卷积的严格定义需要了解线性代数，因此我们将使用类比和具体示例来理解它。想象一下，你手里拿着一个带有复杂图案的矩形玻璃块（就像你在教堂里看到的彩色玻璃），在观察它的同时将其滑动到输入序列上。如果输入模式与玻璃块的模式匹配，更多的光线透过玻璃进去，你会得到更大的输出值。如果输入模式看起来不像玻璃块的模式或者相反，你会得到更小的输出值。换句话说，你正在使用带有彩色玻璃块的道具在输入序列中寻找特定的模式。
- en: This analogy is a little bit too vague, so let’s revisit the grammaticality-detection
    example we used in chapter 4 and see how we’d apply a convolutional layer to the
    task. To recap, our neural network receives a two-word sentence as an input and
    needs to distinguish grammatical sequences from ungrammatical ones. There are
    only four words in the vocabulary—“I,” “you,” “am,” and “are,” which are represented
    by word embeddings. Similarly, there are only four possibilities for the input
    sentence—“I am,” “I are,” “you am,” and “you are.” You want the network to produce
    1s for the first and the last cases and 0s for others. See figure 7.3 for an illustration.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类比比较模糊，所以让我们回顾一下我们在第4章中使用的语法检测的例子，并看看如何将卷积层应用到这个任务上。回顾一下，我们的神经网络接收一个包含两个词的句子作为输入，并需要区分出语法正确的序列和语法错误的序列。词汇表中只有四个词--“I”，“you”，“am”和“are”，它们由单词嵌入表示。类似地，输入句子只有四种可能性--“I
    am”，“I are”，“you am”和“you are”。你希望网络对前两种情况产生1，对其他情况产生0。请参见图7.3进行说明。
- en: '![CH07_F03_Hagiwara](../Images/CH07_F03_Hagiwara.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F03_Hagiwara](../Images/CH07_F03_Hagiwara.png)'
- en: Figure 7.3 Recognizing grammatical English sentences
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 识别英文语法正确的句子
- en: Now, let’s represent word embeddings as patterns. We’ll draw a black circle
    for value -1 and a white one for 1\. Then you can represent each word vector as
    a pair of two circles (see the table on the left in figure 7.3). Similarly, you
    can represent each two-word sentence as a small “patch” of two vectors, or four
    circles (see the table on the right in figure 7.3). Our task is beginning to look
    more like a pattern-recognition task, where the network needs to learn black-and-white
    patterns that correspond to grammatical sentences.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将词嵌入表示为模式。我们用黑色圆表示值-1，白色圆表示1。然后，您可以将每个单词向量表示为两个圆的一对（请参见图7.3左侧的表）。同样，您可以将每个两个词的句子表示为两个向量的小“片段”，或者四个圆（请参见图7.3右侧的表）。我们的任务开始看起来更像是一个模式识别任务，网络需要学习对应于语法句子的黑白模式。
- en: Then, let’s think of a “filter” of the same size (two circles × two circles)
    that acts as the colored glass we talked about earlier. Each circle of this filter
    is also either black or white, corresponding to values -1 and 1\. You are going
    to look at a pattern through this filter and determine whether the pattern is
    the one you are looking for. You do this by putting the filter over a pattern
    and counting the number of color matches between the two. For each one of four
    positions, you get a score of +1 if the colors match (black-black or white-white)
    and a score of -1 if they don’t (black-white or white-black). Your final score
    is the sum of four scores, which varies from -4 (no matches) to +4 (four matches).
    See figure 7.4 for some examples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们想象一个相同大小的“滤波器”（两个圆×两个圆），它充当我们之前讨论过的彩色玻璃。该滤波器的每个圆也是黑色或白色，对应值-1和1。您将通过这个滤波器查看一个模式，并确定是否这是您要找的模式。您可以通过将滤波器放在模式上并计算两者之间的颜色匹配数量来执行此操作。对于四个位置中的每一个，如果颜色匹配（黑色-黑色或白色-白色），则得分+1，如果不匹配（黑色-白色或白色-黑色），则得分-1。您的最终得分是四个分数的总和，从-4（无匹配）到+4（四次匹配）。请参见图7.4中的一些示例。
- en: '![CH07_F04_Hagiwara](../Images/CH07_F04_Hagiwara.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F04_Hagiwara](../Images/CH07_F04_Hagiwara.png)'
- en: Figure 7.4 Examples of convolutional filters
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 卷积滤波器示例
- en: The score you get varies depending on the pattern and the filter, but as you
    can see in the figure, the score becomes larger when the filter looks similar
    to the pattern and becomes smaller when the two are not similar. You get the largest
    score (4) when the two match exactly and the smallest score (-4) when the two
    are exactly opposite. The filter acts as a pattern detector against the input.
    Although this is a very simplified example, it basically shows what a convolutional
    layer is doing. In convolutional neural networks, such filters are called *kernels*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您得到的分数取决于模式和滤波器，但如图所示，当滤波器与模式相似时，分数变大，当两者不相似时，分数变小。当两者完全匹配时，您获得最大分数（4），当两者完全相反时，您获得最小分数（-4）。该滤波器充当输入的模式检测器。虽然这是一个非常简化的例子，但基本上显示了卷积层在做什么。在卷积神经网络中，这种滤波器称为*核*。
- en: In a more general setting, you have an input sentence of arbitrary length, and
    you slide a kernel over the sentence from left to right. See figure 7.5 for an
    illustration of this. The kernel is repeatedly applied to two consecutive words
    to produce a sequence of scores. Because the kernel we are using here covers two
    words, it is said to have a *size* of 2\. Also, because there are two dimensions
    in the input embeddings (which are called *channels*), the number of the kernel’s
    input channels is 2.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在更一般的设置中，您有一个任意长度的输入句子，并且从左到右将一个核滑过句子。请参见图7.5以了解此过程的示意图。该核反复应用于连续的两个词，以生成一系列分数。因为我们在这里使用的核覆盖了两个词，所以它被称为具有*大小*为2的核。而且，因为输入嵌入中有两个维度（称为*通道*），所以核的输入通道数量为2。
- en: '![CH07_F05_Hagiwara](../Images/CH07_F05_Hagiwara.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F05_Hagiwara](../Images/CH07_F05_Hagiwara.png)'
- en: Figure 7.5 Sliding a kernel over the input sentence
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 在输入句子上滑动核
- en: NOTE The reason embedding dimensions are called channels is because CNNs are
    most commonly applied to computer vision tasks where the input is often a 2-D
    image of different channels that correspond to intensities of different colors
    (such as red, green, and blue). In computer vision, kernels are two dimensional
    and move over the input 2-D images, which is also called *2-D convolution*. In
    NLP, however, kernels are usually one-dimensional (1-D convolution) and have only
    one size.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 嵌入维度被称为通道的原因是因为 CNN 最常应用于计算机视觉任务，其中输入通常是不同通道的 2-D 图像，这些通道对应于不同颜色的强度（如红色、绿色和蓝色）。在计算机视觉中，核是二维的，并在输入的
    2-D 图像上移动，这也被称为 *2-D 卷积*。然而，在自然语言处理中，核通常是一维的（1-D 卷积），并且只有一个尺寸。
- en: 7.2.2 Rectified linear unit (ReLU)
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 整流线性单元（ReLU）
- en: As the next step, let’s think about how we can get the desired output (the Desired
    column in figure 7.3) using kernels. How about if we use the filter shown in the
    second column of figure 7.4? The kernel, which we’ll call kernel 1 from now on,
    matches the first pattern exactly and gives it a high score, while giving zero
    or negative scores to others. Figure 7.6 shows the score (called score 1) when
    kernel 1 is applied to each pattern.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，让我们考虑如何使用核来获得期望的输出（图 7.3 中的 Desired 列）。如果我们使用图 7.4 中第二列所示的滤波器会怎样？从现在开始，我们将这个核称为核
    1。这个核完全匹配第一个模式并给它一个高分，同时给其他模式给出零或负分数。图 7.6 显示了将核 1 应用于每个模式时的分数（称为分数 1）。
- en: '![CH07_F06_Hagiwara](../Images/CH07_F06_Hagiwara.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F06_Hagiwara](../Images/CH07_F06_Hagiwara.png)'
- en: Figure 7.6 Applying kernel 1 to patterns
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 对模式应用核 1
- en: Let’s forget the magnitude of the scores for now and focus on their signs (positive
    and negative). The signs for the first three patterns match between Score 1 and
    Desired, but not for the last pattern. To score it correctly—that is, to give
    it a positive score—you need to use another filter that matches the last pattern
    exactly. Let’s call this kernel 2\. Figure 7.7 shows the score (called score 2)
    when kernel 2 is applied to each pattern.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们忘记分数的大小，专注于它们的符号（正数和负数）。前三个模式的符号在分数 1 和所需之间匹配，但对于最后一个模式则不是。要正确评分，即给出正分数，您需要使用另一个与最后一个模式完全匹配的滤波器。我们称这个核为核
    2。图 7.7 显示了应用核 2 到每个模式时的分数（称为分数 2）。
- en: Kernel 2 can give correct scores that match the signs of the desired ones for
    the last three patterns, but not for the first one. But if you observe figures
    7.6 and 7.7 carefully, it looks like you could get closer to the desired scores
    if there was a way to somehow disregard the output when a kernel gives negative
    scores and then combine the scores from multiple kernels.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 核 2 可以为最后三个模式给出与所需符号匹配的正确分数，但不能为第一个模式。但是，如果仔细观察图 7.6 和 7.7，看起来如果有一种方法可以在核给出负分数时忽略输出，然后组合来自多个核的分数，那么就可以更接近所需的分数。
- en: '![CH07_F07_Hagiwara](../Images/CH07_F07_Hagiwara.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F07_Hagiwara](../Images/CH07_F07_Hagiwara.png)'
- en: Figure 7.7 Applying kernel 2 to patterns
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 对模式应用核 2
- en: 'Let’s think of a function that clamps any negative input to zero while passing
    any positive values through unchanged. In Python, this function can be written
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个函数，它将任何负输入夹紧为零，同时保持任何正值不变。在 Python 中，这个函数可以写成如下：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: or even simpler
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 或者更简单
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can disregard negative values by applying this function to score 1 and score
    2, as shown in figures 7.8 and 7.9.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将此函数应用于分数 1 和分数 2 来忽略负值，如图 7.8 和 7.9 所示。
- en: '![CH07_F08_Hagiwara](../Images/CH07_F08_Hagiwara.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F08_Hagiwara](../Images/CH07_F08_Hagiwara.png)'
- en: Figure 7.8 Applying ReLU to score 1
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 对分数 1 应用 ReLU
- en: '![CH07_F09_Hagiwara](../Images/CH07_F09_Hagiwara.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F09_Hagiwara](../Images/CH07_F09_Hagiwara.png)'
- en: Figure 7.9 Applying ReLU to score 2
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 对分数 2 应用 ReLU
- en: This function, which is called a *rectified linear unit*, or ReLU (pronounced
    “rel-you”), is one of the simplest yet most commonly used activation functions
    in deep learning. It is often used with a convolutional layer, and although it
    is very simple (all it does is just clamp negative values to zero), it is still
    an activation function that enables neural networks to learn complex nonlinear
    functions (see chapter 4 for why nonlinear activation functions are important).
    It also has favorable mathematical properties that make it easier to optimize
    the network, although the theoretical details are beyond the scope of this book.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数，被称为*修正线性单元*，或称为 ReLU（发音为“rel-you”），是深度学习中最简单但最常用的激活函数之一。 它通常与卷积层一起使用，虽然它非常简单（它只是将负值夹紧为零），但它仍然是一个激活函数，它使神经网络能够学习复杂的非线性函数（参见第
    4 章，了解为什么非线性激活函数很重要）。 它还具有有利的数学属性，使得优化网络变得更容易，尽管理论细节超出了本书的范围。
- en: 7.2.3 Combining scores
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 合并分数
- en: If you look at both figures 7.8 and 7.9, the “clamped” scores—shown in the f(Score
    1) and f(Score 2) columns—capture the desired scores at least partially. All you
    need to do is combine them together (by summing) and adjust the range (by dividing
    by 4). Figure 7.10 shows the result of this.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看图 7.8 和图 7.9，所谓的“固定”分数—显示在 f(Score 1) 和 f(Score 2) 列中—至少部分地捕捉到了期望的分数。 您所需做的就是将它们结合在一起（通过求和）并调整范围（通过除以
    4）。 图 7.10 展示了这个结果。
- en: '![CH07_F10_Hagiwara](../Images/CH07_F10_Hagiwara.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F10_Hagiwara](../Images/CH07_F10_Hagiwara.png)'
- en: Figure 7.10 Combining the results from two kernels
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 结合两个内核的结果
- en: After combining, the scores match the desired outcomes exactly. All we did so
    far was design kernels that match the patterns we want to detect and then simply
    combine the scores. Compare this to the RNN example we worked on in section 4.1.3,
    where we needed to use some complicated numeric computation to derive the parameters.
    Hopefully this example is enough to show you how simple and powerful CNNs can
    be for text classification!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在合并之后，分数与期望的结果完全匹配。 到目前为止，我们所做的一切都是设计与我们想要检测的模式相匹配的内核，然后简单地组合分数。 比较一下我们在第 4.1.3
    节中处理的 RNN 示例，那里我们需要使用一些复杂的数值计算来推导参数。 希望这个例子足以向您展示 CNN 对于文本分类可以有多简单而强大！
- en: The example we worked on in this section is simply for introducing the basic
    concepts of CNNs, so we cut many corners. First, in practice, patterns and kernels
    are not just black and white but contain real-valued numbers. The score after
    applying a kernel to a pattern is obtained not by counting color matches but through
    a mathematical operation called *inner product*, which captures the similarity
    between the two. Second, the scores produced by kernels aren’t combined by some
    arbitrary operation (like we did in this section) but usually by a linear layer
    (see section 3.4.3), which can learn a linear transformation against the input
    to produce the output. Finally, kernels and the weights (magic constants w and
    b) in the final linear layer are all trainable parameters of a CNN, meaning that
    their values are adjusted so that the CNN can produce the desired scores.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中处理的示例仅用于介绍 CNN 的基本概念，因此我们偷了很多懒。 首先，在实践中，模式和内核不仅仅是黑白的，而是包含实值数字。 应用内核到模式后的分数不是通过计算颜色匹配次数得到的，而是通过一种称为*内积*的数学运算得到的，它捕捉了两者之间的相似性。
    第二，内核产生的分数不是通过某种任意的操作（就像我们在本节中所做的那样）组合在一起的，而通常是通过线性层（见 3.4.3 节）组合在一起的，该线性层可以学习针对输入的线性变换以产生输出。
    最后，内核和最终线性层中的权重（魔法常数 w 和 b）都是 CNN 的可训练参数，这意味着它们的值会被调整，以使 CNN 能够产生期望的分数。
- en: 7.3 Pooling layers
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 池化层
- en: In the previous section, we assumed that the input is just a combination of
    two words—subjects and verbs—although in practice, the input to a CNN can be of
    arbitrary length. Your CNN needs to not only detect patterns but also find them
    in a potentially large amount of noise in the input. As we saw in section 7.2,
    you slide a kernel over the sentence from left to right, and the kernel is repeatedly
    applied to two consecutive words to produce a sequence of scores. The remaining
    question is what to do with these produced scores. Specifically, what operation
    should we use in the “?” position in figure 7.11 to derive the desired score?
    This operation needs to have some properties—it must be something that can be
    applied to an arbitrarily large number of scores, because the sentence can be
    very long. It also needs to aggregate the scores in a way that is agnostic to
    where the target pattern (word embeddings for “I am”) is in the input sentence.
    Can you figure out the answer?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们假设输入只是两个词——主语和动词的组合，尽管在实践中，CNN的输入可以是任意长度的。您的CNN不仅需要检测模式，还需要在输入中可能存在的大量噪声中找到它们。正如我们在第7.2节中看到的，您将一个核从左到右滑过句子，并且核会重复应用于两个连续的单词以产生一系列分数。剩下的问题是如何处理这些产生的分数。具体来说，我们应该在图7.11中的“？”位置使用什么操作来获得所需的分数？这个操作需要具有一些属性——它必须是可以应用于任意数量的分数的东西，因为句子可能非常长。它还需要以一种对输入句子中目标模式（“我是”的单词嵌入）的位置不可知的方式聚合分数。您能想出答案吗？
- en: '![CH07_F11_Hagiwara](../Images/CH07_F11_Hagiwara.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F11_Hagiwara](../Images/CH07_F11_Hagiwara.png)'
- en: Figure 7.11 Aggregating scores to derive the desired score
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 聚合分数以获得所需分数
- en: The simplest thing you can do to aggregate the scores is to take their maximum.
    Because the largest score in figure 7.11 is 4, it will become the output of this
    layer. This aggregation operation is called *pooling*, and the neural network
    substructure that does pooling is called a *pooling layer*. You can also do other
    types of mathematical operations that do aggregation, such as taking the average,
    although taking the maximum (called *max pooling*) is most commonly used.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 汇总分数的最简单方法是取它们的最大值。因为图7.11中的最大分数为4，它将成为该层的输出。这种汇总操作称为*池化*，而执行汇总的神经网络子结构称为*池化层*。您还可以执行其他类型的数学运算来进行聚合，例如取平均值，尽管最常用的是取最大值（称为*最大池化*）。
- en: The pooled score will be fed to a linear layer, optionally combined with scores
    from other kernels, and used as a predicted score. This entire process is illustrated
    in figure 7.12\. Now we have a fully functional CNN!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 汇总分数将被馈送到一个线性层，可选地与其他核的分数结合，并用作预测分数。整个过程如图7.12所示。现在我们有一个完全功能的CNN！
- en: As with other neural networks we’ve seen so far, the output from the linear
    layer is fed to softmax to produce a probability distribution over labels. These
    predicted values are then compared with the true labels to produce the loss and
    used for optimizing the network.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们迄今看到的其他神经网络一样，线性层的输出被馈送到softmax以产生标签上的概率分布。然后将这些预测值与真实标签进行比较，以产生损失并用于优化网络。
- en: 'Before we wrap up, a few more words on CNNs: notice that the CNN in figure
    7.12 produces the same prediction value no matter where the search pattern (“I
    am”) is in the input sentence. This is due to the kernel locality as well as the
    property of the max pooling layer we just added. In general, CNNs produce the
    same prediction, even if the input sentence is modified by shifting by a few words.
    In a technical term, the CNN is called *transformation invariant*, which is an
    important property of CNNs. This property is perhaps more intuitive if you use
    an image recognition example. An image of a cat is still an image of a cat, no
    matter where the cat is in the image. Similarly, a grammatical English sentence
    (e.g., “I am a student”) is still grammatical, even if the sentence is transformed
    by adding a few words (e.g., “that’s right”) to the beginning, making it “That’s
    right, I am a student.”'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束之前，对CNN还有几句话：请注意，图7.12中的CNN无论搜索模式（“我是”）在输入句子中的位置如何，都会产生相同的预测值。这是由于卷积核的局部性以及我们刚刚添加的最大池化层的属性。通常情况下，即使输入句子通过移动几个单词而被修改，CNN也会产生相同的预测。从技术上讲，CNN被称为*变换不变*，这是CNN的一个重要属性。如果您使用图像识别示例，则该属性可能更直观。猫的图像仍然是猫的图像，无论猫在图像中的位置如何。同样，一个语法正确的英文句子（例如，“我是学生”）仍然是语法正确的，即使句子通过在开头添加几个单词（例如，“那是对的”）而被转换为“那是对的，我是学生”。
- en: '![CH07_F12_Hagiwara](../Images/CH07_F12_Hagiwara.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F12_Hagiwara](../Images/CH07_F12_Hagiwara.png)'
- en: Figure 7.12 A full CNN with multiple kernels
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 带有多个卷积核的完整CNN
- en: Because the kernels in a CNN do not depend on each other (unlike RNNs, where
    one cell needs to wait until all the preceding cells finish processing the input),
    CNNs are computationally efficient. GPUs can process these kernels in parallel
    without waiting on other kernels’ output. Due to this property, CNNs are usually
    faster than RNNs of similar size.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因为CNN中的卷积核不相互依赖（与RNN不同，后续单元需要等待所有前面的单元完成输入处理），所以CNN计算效率高。GPU可以并行处理这些卷积核，不用等待其他卷积核的输出。由于这个特性，CNN通常比大小相似的RNN更快。
- en: '7.4 Case study: Text classification'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 案例研究：文本分类
- en: Now that we know the basics of CNNs, in this section we are going to build an
    NLP application using a CNN and see how it works in practice. As mentioned previously,
    one of the most popular and straightforward applications of CNNs in NLP is text
    classification. CNNs are good at detecting patterns (such as salient words and
    phrases in text), which is also the key to accurate text classification.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经了解了CNN的基础知识，在本节中，我们将使用CNN构建一个NLP应用并看看它在实践中的工作原理。正如之前提到的，CNN在NLP中最受欢迎和直接的应用之一就是文本分类。CNN擅长检测文本中的模式（如突出的单词和短语），这也是准确文本分类的关键。
- en: '7.4.1 Review: Text classification'
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 复习：文本分类
- en: We already covered text classification in chapters 2 and 4, but to recap, text
    classification is a task where an NLP system assigns a label to a given piece
    of text. If the text is an email and the label is whether the email is spam, it’s
    spam filtering. If the text is a document (such as a news article) and the label
    is its topic (such as politics, business, technology, or sports), it’s called
    *document classification*. Many other variants of text classification exist, depending
    on what the input and the output are. But the task we’ll be working on in this
    section is again sentiment analysis, where the input is some text in which the
    writer’s subjective opinions are expressed (such as movie and product reviews)
    and the output is the label for the opinion (such as positive or negative, or
    even the number of stars), also called *polarity*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在第2章和第4章中介绍了文本分类，但是为了回顾一下，文本分类是指一个NLP系统给定一段文本分配一个标签的任务。如果文本是一个电子邮件，标签是邮件是否为垃圾邮件，那就是垃圾邮件过滤。如果文本是一个文档（比如新闻文章），标签是它的主题（如政治、商业、技术或体育），那就叫做*文档分类*。根据输入和输出的不同，还存在许多其他变种的文本分类。但是在本节中，我们将再次处理情感分析，它的输入是一些表达作者主观意见的文本（如电影和产品评论），输出是意见的标签（如正面或负面，甚至星级评价），也被称为*极性*。
- en: In chapters 2 and 4, we built an NLP system that detected the polarity given
    a movie review using the Stanford Sentiment Treebank, a dataset containing movie
    reviews and their polarity (strongly positive, positive, neutral, negative, or
    strongly negative). In this section, we will build the same text classifier but
    with a CNN instead of an RNN. The good news is that we can reuse most of the code
    we wrote in chapter 2 in this section—in fact, we need to modify only a few lines
    of code to swap the RNN with a CNN. This is largely thanks to AllenNLP’s powerful,
    well-designed abstractions, which let you work with many modules with different
    architectures through the common interfaces. Let’s see this in action next.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章和第4章中，我们构建了一个NLP系统，使用Stanford Sentiment Treebank检测给定电影评论的情感极性，这是一个包含电影评论及其极性（非常正面，正面，中立，负面，非常负面）的数据集。在本节中，我们将构建同样的文本分类器，但是使用CNN而不是RNN。好消息是，我们可以重用第2章编写的大部分代码，在这一部分只需要修改几行代码将RNN替换为CNN。这在很大程度上归功于AllenNLP强大而设计良好的抽象，它可以让您通过公共接口与许多具有不同架构的模块一起工作。让我们下面看看它的运行。
- en: 7.4.2 Using CnnEncoder
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 使用 CnnEncoder
- en: 'Remember that back in section 4.4, we defined our LstmClassifier for text classification
    as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在第4.4节中，我们定义了文本分类的LstmClassifier如下：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We hadn’t put much thought into what this definition meant, but from this constructor
    we can see that the model is built on top of two subcomponents: a TextFieldEmbedder
    called embedder and a Seq2VecEncoder called encoder, in addition to the vocabulary
    and the string for the positive label, which are not relevant to our discussion
    here. We discussed word embeddings in chapter 3 at length, although we only briefly
    touched on the encoder. What does this Seq2VecEncoder actually mean?'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有对这个定义进行深入的思考，但是从这个构造函数中，我们可以看到这个模型是建立在两个子组件之上的：一个名为`embedder`的`TextFieldEmbedder`和一个名为`encoder`的`Seq2VecEncoder`，除此之外还有词汇表和正标签的字符串，这些对我们的讨论不相关。我们在第3章详细讨论了词嵌入，尽管我们只是简要涉及了编码器。这个`Seq2VecEncoder`到底是什么意思呢？
- en: 'In AllenNLP, Seq2VecEncoder is a class of neural network architectures that
    take a sequence of vectors (or tensors in general) and return a single vector.
    An RNN, one example of this, takes a variable-length input consisting of multiple
    vectors and converts it into a single vector at the last cell. We created an instance
    of Seq2VecEncoder based on an LSTM-RNN using the following code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在AllenNLP中，`Seq2VecEncoder`是一类神经网络架构，它接受一系列向量（或一般张量）并返回一个单个向量。RNN是其中的一个例子，它接受由多个向量组成的可变长度输入，并在最后一个单元格中将其转换为单个向量。我们使用以下代码基于LSTM-RNN创建了一个`Seq2VecEncoder`的实例：
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: But as long as your component has the same input and output specifications,
    you can use any neural network architecture as a Seq2VecEncoder. In programming
    language, Seq2VecEncoder is analogous to an interface in Java (and in many other
    languages)—interfaces define what your class looks like and what it does, but
    they do not care about *how* your class does it. In fact, your model can do something
    as simple as just summing up all the input vectors to produce the output, without
    any complex transformations such as nonlinearities. This is, in fact, what BagOfEmbeddingsEncoder—one
    of the Seq2VecEncoders implemented in AllenNLP—does.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但只要你的组件具有相同的输入和输出规范，你就可以使用任何神经网络架构作为`Seq2VecEncoder`。在编程语言中，`Seq2VecEncoder`类似于Java（以及许多其他语言中的）中的接口——接口定义了你的类是什么样子的，它做什么，但它们不关心你的类是如何做到的。实际上，你的模型可以简单地对所有输入向量求和以产生输出，而不需要任何复杂的变换，比如非线性变换。事实上，这就是`BagOfEmbeddingsEncoder`—AllenNLP中实现的`Seq2VecEncoder`之一的做法。
- en: 'Next, we use a CNN to “squash” a sequence of vectors into a single vector.
    A CNN-based Seq2VecEncoder is implemented as CnnEncoder in AllenNLP, which can
    be instantiated as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用CNN将一系列向量“压缩”为单个向量。在AllenNLP中，基于CNN的`Seq2VecEncoder`实现为`CnnEncoder`，可以如下实例化：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this example, embedding_dim specifies the dimensionality of the input embeddings.
    The second argument, num_filters, tells how many filters (or kernels, as explained
    in section 7.2.1) will be used per n-gram. The final argument, ngram_ filter_sizes,
    specifies the list of n-gram sizes, which are the sizes of these kernels. Here,
    we are using n-gram sizes of 2, 3, 4, and 5, meaning there are 8 kernels for bigrams,
    8 kernels for trigrams, and so on, up to 5-grams. In total, this CNN can learn
    32 different kernels to detect patterns. CnnEncoder runs these results from the
    kernels through a max pooling layer and comes up with a single vector that summarizes
    the input.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`embedding_dim`指定了输入嵌入的维度。第二个参数`num_filters`告诉我们将使用多少个过滤器（或内核，如第7.2.1节所解释的）。最后一个参数`ngram_filter_sizes`指定了n-gram大小的列表，即这些内核的大小。在这里，我们使用2、3、4和5的n-gram大小，这意味着有8个用于bigram的内核，8个用于trigram，以此类推，直到5-gram。总而言之，这个CNN可以学习32个不同的内核来检测模式。`CnnEncoder`通过一个最大池化层运行这些内核的结果，并得出一个总结输入的单个向量。
- en: 'The rest of the training pipeline looks almost identical to the LSTM version
    we saw in chapter 2\. The entire code is available on Google Colab ([http://www.realworld
    nlpbook.com/ch7.html#cnn-nb](http://www.realworldnlpbook.com/ch7.html#cnn-nb)).
    There is one caveat: because some n-gram filters have a wide shape (e.g., 4- and
    5-grams), you need to make sure that each text field is at least that long, even
    when the original text is short (e.g., just one or two words). You need to know
    how batching and padding work in AllenNLP (which we’ll cover in chapter 10) to
    fully understand how to deal with this, but in a nutshell, you need to specify
    the token_min_padding_length parameter when initializing the token indexer as
    follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 训练流水线的其余部分几乎与我们在第 2 章中看到的 LSTM 版本相同。整个代码都可以在 Google Colab 上找到。[http://www.realworldnlpbook.com/ch7.html#cnn-nb](http://www.realworldnlpbook.com/ch7.html#cnn-nb)。但有一个注意事项：由于一些
    n-gram 过滤器具有宽形状（例如，4-gram 和 5-gram），您需要确保每个文本字段至少具有该长度，即使原始文本很短（例如，只有一个或两个单词）。您需要了解
    AllenNLP 中的批处理和填充工作原理（我们将在第 10 章中介绍）才能充分理解如何处理这一问题，但简而言之，您需要在初始化标记索引器时指定 token_min_padding_length
    参数，如下所示：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 7.4.3 Training and running the classifier
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 训练和运行分类器
- en: 'When you run the script, you’ll see something like the following log output
    at the end of the training:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行脚本时，您会在训练结束时看到类似以下日志输出：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This means that the training accuracy reaches ~99%, whereas the validation accuracy
    tops around 40%. Again, this is a typical symptom of overfitting, where your model
    is so powerful that it fits the training data well, but it doesn’t generalize
    to the validation and test datasets as well. Our CNN has many filters that can
    remember salient patterns in the training data, but these patterns are not necessarily
    the ones that help predict the labels for the validation instances. We are not
    worried too much about overfitting in this chapter. See chapter 10 for common
    techniques for avoiding overfitting.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着训练精度达到了约 99%，而验证精度则达到了约 40%。同样，这是过拟合的典型症状，即您的模型非常强大，可以很好地拟合训练数据，但不能很好地泛化到验证和测试数据集。我们的
    CNN 具有许多能够记住训练数据中显著模式的过滤器，但这些模式未必有助于预测验证实例的标签。在本章中，我们不太担心过拟合。有关避免过拟合的常见技术，请参见第
    10 章。
- en: 'If you want to make predictions for new instances, you can use the same Predictor
    as we did in chapter 2\. Predictors in AllenNLP are a thin wrapper around your
    trained model, which take care of formatting the input and output in a JSON format
    and feeding the instance to the model. You can use the following snippet to make
    predictions using your trained CNN model:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想对新实例进行预测，可以使用与第 2 章相同的预测器。AllenNLP 中的预测器是您训练好的模型的一个轻量级包装器，负责将输入和输出格式化为 JSON
    格式并将实例提供给模型。您可以使用以下代码段使用您训练好的 CNN 模型进行预测：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: CNNs use filters called kernels and an operation called convolution to detect
    local linguistic patterns in the input.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 使用称为内核的过滤器和称为卷积的操作来检测输入中的局部语言模式。
- en: An activation function called ReLU, which clamps negative values to zero, is
    used with convolution layers.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层使用的激活函数称为ReLU，它将负值截断为零。
- en: CNNs then use pooling layers to aggregate the result from the convolutional
    layer.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 然后使用池化层来聚合卷积层的结果。
- en: CNN prediction is transformation invariant, meaning it remains unchanged even
    after linear modification of the input.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 预测是转换不变的，意味着即使对输入进行线性修改后也保持不变。
- en: You can use a CNN-based encoder as a Seq2VecEncoder in AllenNLP by modifying
    a few lines of code of your text classifier.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过修改文本分类器的几行代码将基于 CNN 的编码器用作 AllenNLP 中的 Seq2VecEncoder。
