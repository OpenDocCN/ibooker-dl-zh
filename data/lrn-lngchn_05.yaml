- en: Chapter 5\. Cognitive Architectures with LangGraph
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 章\. 基于LangGraph的认知架构
- en: 'So far, we’ve looked at the most common features of LLM applications:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了 LLM 应用程序最常见的特点：
- en: Prompting techniques in the [Preface](preface01.html#pr01_preface_1736545679069216)
    and [Chapter 1](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[序言](preface01.html#pr01_preface_1736545679069216)和[第 1 章](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004)中的提示技术'
- en: RAG in Chapters [2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)
    and [3](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第 2 章](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)和[第 3
    章](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580)中的 RAG'
- en: Memory in [Chapter 4](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第 4 章](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431)中的记忆'
- en: 'The next question should be: How do we assemble these pieces into a coherent
    application that achieves the goal we set out to solve? To draw a parallel with
    the world of bricks and mortar, a swimming pool and a one-story house are built
    of the same materials, but obviously serve very different purposes. What makes
    them uniquely suited to their different purposes is the plan for how those materials
    are combined—that is, their architecture. The same is true when building LLM applications.
    The most important decisions you have to make are how to assemble the different
    components you have at your disposal (such as RAG, prompting techniques, memory)
    into something that achieves your purpose.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的问题应该是：我们如何将这些组件组合成一个连贯的应用程序，以实现我们设定的目标？为了与砖石世界做一个类比，游泳池和一层的房子是由相同的材料建造的，但显然服务于非常不同的目的。使它们各自适合其不同目的的是这些材料组合的计划——即它们的架构。在构建
    LLM 应用程序时也是如此。你必须做出的最重要的决定是如何将你拥有的不同组件（如 RAG、提示技术、记忆）组装成实现你目的的东西。
- en: 'Before we look at specific architectures, let’s walk through an example. Any
    LLM application you might build will start from a purpose: what the app is designed
    to do. Let’s say you want to build an email assistant—an LLM application that
    reads your emails before you do and aims to reduce the amount of emails you need
    to look at. The application might do this by archiving a few uninteresting ones,
    directly replying to some, and marking others as deserving of your attention later.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看具体架构之前，让我们通过一个例子来了解一下。你可能会构建的任何 LLM 应用程序都将从一个目的开始：应用程序被设计来做什么。假设你想构建一个电子邮件助手——一个在你看邮件之前阅读你的邮件的
    LLM 应用程序，目的是减少你需要查看的邮件数量。应用程序可能通过存档一些不感兴趣的邮件，直接回复一些，并将其他标记为需要你稍后注意的邮件来实现这一点。
- en: 'You probably also would want the app to be bound by some constraints in its
    action. Listing those constraints helps tremendously, as they will help inform
    the search for the right architecture. [Chapter 8](ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600)
    covers these constraints in more detail and how to work with them. For this hypothetical
    email assistant, let’s say we’d like it to do the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还希望应用程序在行动上受到一些约束。列出这些约束非常有帮助，因为它们将有助于指导对正确架构的搜索。[第 8 章](ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600)更详细地介绍了这些约束以及如何处理它们。对于这个假设的电子邮件助手，假设我们希望它做以下事情：
- en: Minimize the number of times it interrupts you (after all, the whole point is
    to save time).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化它打扰你的次数（毕竟，整个目的就是节省时间）。
- en: Avoid having your email correspondents receive a reply that you’d never have
    sent yourself.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免让你的电子邮件联系人收到你永远不会自己发送的回复。
- en: 'This hints at the key trade-off often faced when building LLM apps: the trade-off
    between *agency* (or the capacity to act autonomously) and *reliability* (or the
    degree to which you can trust its outputs). Intuitively, the email assistant will
    be more useful if it takes more actions without your involvement, but if you take
    it too far, it will inevitably send emails you wish it hadn’t.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这暗示了在构建 LLM 应用程序时经常面临的关键权衡：在 *自主性*（或自主行动的能力）和 *可靠性*（或你可以信任其输出的程度）之间的权衡。直观地讲，如果电子邮件助手在没有你介入的情况下采取更多行动，它将更有用，但如果你做得太过分，它不可避免地会发送你希望它没有发送的电子邮件。
- en: 'One way to describe the degree of autonomy of an LLM application is to evaluate
    how much of the behavior of the application is determined by an LLM (versus code):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 描述一个 LLM 应用程序的自主程度的一种方法是通过评估应用程序的行为中有多少是由 LLM（而不是代码）决定的：
- en: Have an LLM decide the output of a step (for instance, write a draft reply to
    an email).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让 LLM 决定一个步骤的输出（例如，为电子邮件写一个草稿回复）。
- en: 'Have an LLM decide the next step to take (for instance, for a new email, decide
    between the three actions it can take on an email: archive, reply, or mark for
    review).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让LLM决定下一步要采取的行动（例如，对于一封新邮件，决定它可以在邮件上执行的三种操作：存档、回复或标记为审阅）。
- en: Have an LLM decide what steps are available to take (for instance, have the
    LLM write code that executes a dynamic action you didn’t preprogram into the application).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让LLM决定可以采取哪些步骤（例如，让LLM编写执行你未预先编程到应用程序中的动态操作的代码）。
- en: We can classify a number of popular *recipes* for building LLM applications
    based on where they fall in this spectrum of autonomy, that is, which of the three
    tasks just mentioned are handled by an LLM and which remain in the hands of the
    developer or user. These recipes can be called *cognitive architectures*. In the
    artificial intelligence field, the term *cognitive architecture* has long been
    used to denote models of human reasoning (and their implementations in computers).
    An LLM cognitive architecture (the term was first applied to LLMs, to our knowledge,
    in a paper^([1](ch05.html#id699))) can be defined as a recipe for the steps to
    be taken by an LLM application (see [Figure 5-1](#ch05_figure_1_1736545670023944)).
    A *step* is, for instance, retrieval of relevant documents (RAG), or calling an
    LLM with a chain-of-thought prompt.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据它们在这条自主性谱系中的位置来对构建LLM应用的许多流行**食谱**进行分类，也就是说，三个提到的任务中哪些是由LLM处理的，哪些仍然掌握在开发者或用户手中。这些食谱可以被称为**认知架构**。在人工智能领域，**认知架构**这个术语长期以来一直被用来表示人类推理模型（及其在计算机中的实现）。一个LLM认知架构（据我们所知，这个术语首次应用于LLM，是在一篇论文中提出的）可以被定义为LLM应用要采取的步骤的**食谱**（参见[图5-1](#ch05_figure_1_1736545670023944)）。例如，一个**步骤**可以是检索相关文档（RAG），或者使用思维链提示调用LLM。
- en: '![A screenshot of a computer application  Description automatically generated](assets/lelc_0501.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![计算机应用程序的截图  自动生成的描述](assets/lelc_0501.png)'
- en: Figure 5-1\. Cognitive architectures for LLM applications
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. LLM应用的认知架构
- en: 'Now let’s look at each of the major architectures, or recipes, that you can
    use when building your application (as shown in [Figure 5-1](#ch05_figure_1_1736545670023944)):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看在构建您的应用时可以使用的主要架构或**食谱**（如图[图5-1](#ch05_figure_1_1736545670023944)所示）：
- en: '0: Code'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '0: 代码'
- en: This is not an LLM cognitive architecture (hence we numbered it **0**), as it
    doesn’t use LLMs at all. You can think of this as regular software you’re used
    to writing. The first interesting architecture (for this book, at any rate) is
    actually the next one.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个LLM认知架构（因此我们将其编号为**0**），因为它根本不使用LLM。你可以把它想象成你习惯编写的常规软件。对于这本书来说，第一个有趣的架构实际上是下一个。
- en: '1: LLM call'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '1: LLM调用'
- en: This is the majority of the examples we’ve seen in the book so far, with one
    LLM call only. This is useful mostly when it’s part of a larger application that
    makes use of an LLM for achieving a specific task, such as translating or summarizing
    a piece of text.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们到目前为止在书中看到的绝大多数例子，只有一个LLM调用。这主要在它是更大应用的一部分时有用，该应用使用LLM来完成特定任务，例如翻译或总结一段文本。
- en: '2: Chain'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '2: 链接'
- en: 'The next level up, so to speak, comes with the use of multiple LLM calls in
    a predefined sequence. For instance, a text-to-SQL application (which receives
    as input from the user a natural language description of some calculation to make
    over a database) could make use of two LLM calls in sequence:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 更高级别的是，使用预定义序列中的多个LLM调用。例如，一个文本到SQL应用（它从用户那里接收对数据库中某些计算的自然语言描述作为输入）可以连续使用两个LLM调用：
- en: One LLM call to generate a SQL query, from the natural language query, provided
    by the user, and a description of the database contents, provided by the developer.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LLM调用，从用户提供的自然语言查询和开发者提供的数据库内容描述中生成SQL查询。
- en: And another LLM call to write an explanation of the query appropriate for a
    nontechnical user, given the query generated in the previous call. This one could
    then be used to enable the user to check if the generated query matches his request.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个LLM调用是编写一个适合非技术用户的查询解释，基于前一个调用中生成的查询。然后可以使用这个解释来使用户检查生成的查询是否与他的请求匹配。
- en: '3: Router'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '3: 路由器'
- en: 'This next step comes from using the LLM to define the sequence of steps to
    take. That is, whereas the chain architecture always executes a static sequence
    of steps (however many) determined by the developer, the router architecture is
    characterized by using an LLM to choose between certain predefined steps. An example
    would be a RAG application with multiple indexes of documents from different domains,
    with the following steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个步骤来自使用 LLM 来定义要采取的步骤序列。也就是说，链式架构总是执行由开发者确定的静态步骤序列（无论多少），而路由架构的特点是使用 LLM 在某些预定义步骤之间进行选择。一个例子是一个
    RAG 应用程序，它包含来自不同领域的多个文档索引，以下步骤：
- en: An LLM call to pick which of the available indexes to use, given the user-supplied
    query and the developer-supplied description of the indexes.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据用户提供的查询和开发者提供的索引描述，LLM 调用来选择使用哪个可用的索引。
- en: A retrieval step that queries the chosen index for the most relevant documents
    for the user query.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个检索步骤，查询所选索引以获取用户查询的最相关文档。
- en: Another LLM call to generate an answer, given the user-supplied query and the
    list of relevant documents fetched from the index.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一次 LLM 调用来生成答案，给定用户提供的查询和从索引中检索到的相关文档列表。
- en: That’s as far as we’ll go in this chapter. We will talk about each of these
    architectures in turn. The next chapters discuss the agentic architectures, which
    make even more use of LLMs. But first let’s talk about some better tooling to
    help us on this journey.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本章我们将要讨论的内容。我们将依次讨论这些架构。下一章将讨论代理架构，这些架构会更多地使用 LLM。但首先，让我们谈谈一些更好的工具，以帮助我们在这个旅程中。
- en: 'Architecture #1: LLM Call'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '架构 #1：LLM 调用'
- en: As an example of the LLM call architecture, we’ll return to the chatbot we created
    in [Chapter 4](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431).
    This chatbot will respond directly to user messages.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 LLM 调用架构的例子，我们将回到我们在第 4 章中创建的聊天机器人。这个聊天机器人将直接响应用户的消息。
- en: 'Start by creating a `StateGraph`, to which we’ll add a node to represent the
    LLM call:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个 `StateGraph`，我们将向其中添加一个节点来表示 LLM 调用：
- en: '*Python*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*JavaScript*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can also draw a visual representation of the graph:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制一个图形的视觉表示：
- en: '*Python*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*JavaScript*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The graph we just made looks like [Figure 5-2](#ch05_figure_2_1736545670023979).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才创建的图形看起来像[图 5-2](#ch05_figure_2_1736545670023979)。
- en: '![A diagram of a chatbot  Description automatically generated](assets/lelc_0502.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![聊天机器人的图示  自动生成的描述](assets/lelc_0502.png)'
- en: Figure 5-2\. The LLM call architecture
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. LLM 调用架构
- en: 'You can run it with the familiar `stream()` method you’ve seen in earlier chapters:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用在前面章节中看到的熟悉的 `stream()` 方法来运行它：
- en: '*Python*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*JavaScript*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*The output:*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice how the input to the graph was in the same shape as the `State` object
    we defined earlier; that is, we sent in a list of messages in the `messages` key
    of a dictionary.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到图形的输入与我们在前面定义的 `State` 对象的形状相同；也就是说，我们发送了一个字典中 `messages` 键的消息列表。
- en: 'This is the simplest possible architecture for using an LLM, which is not to
    say that it should never be used. Here are some examples of where you might see
    it in action in popular products, among many others:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用 LLM 的最简单架构，但这并不意味着它永远不会被使用。以下是一些例子，说明你可能在许多流行的产品中看到它在行动：
- en: AI-powered features such as summarize and translate (such as you can find in
    Notion, a popular writing software) can be powered by a single LLM call.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 AI 驱动的功能，如摘要和翻译（例如在流行的写作软件 Notion 中可以找到的），可以通过一次 LLM 调用来实现。
- en: Simple SQL query generation can be powered by a single LLM call, depending on
    the UX and target user the developer has in mind.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的 SQL 查询生成可以通过一次 LLM 调用实现，具体取决于开发者心中所想的 UX 和目标用户。
- en: 'Architecture #2: Chain'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '架构 #2：链式'
- en: This next architecture extends on all that by using multiple LLM calls, in a
    predefined sequence (that is, different invocations of the application do the
    same sequence of LLM calls, albeit with different inputs and results).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个架构通过使用多个 LLM 调用，按照预定义的顺序（即，不同的应用程序调用执行相同的 LLM 调用序列，尽管输入和结果不同）来扩展所有这些内容。
- en: Let’s take as an example a text-to-SQL application, which receives as input
    from the user a natural language description of some calculation to make over
    a database. We mentioned earlier that this could be achieved with a single LLM
    call, to generate a SQL query, but we can create a more sophisticated application
    by making use of multiple LLM calls in sequence. Some authors call this architecture
    *flow engineering*.^([2](ch05.html#id709))
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个文本到SQL应用程序为例，该程序接收用户输入的自然语言描述，用于对数据库进行某些计算。我们之前提到，这可以通过一次LLM调用来实现，生成一个SQL查询，但我们可以通过按顺序使用多个LLM调用创建一个更复杂的应用程序。一些作者称这种架构为*流程工程*。[2](ch05.html#id709)
- en: 'First let’s describe the flow in words:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们用文字描述一下流程：
- en: One LLM call to generate a SQL query from the natural language query, provided
    by the user, and a description of the database contents, provided by the developer.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一次LLM调用，从用户提供的自然语言查询和开发者提供的数据库内容描述中生成SQL查询。
- en: Another LLM call to write an explanation of the query appropriate for a nontechnical
    user, given the query generated in the previous call. This one could then be used
    to enable the user to check if the generated query matches his request.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用另一个LLM调用，根据前一个调用生成的查询，为非技术用户编写查询的解释。这可以用来让用户检查生成的查询是否符合他的要求。
- en: 'You could also extend this even further (but we won’t do that here) with additional
    steps to be taken after the preceding two:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以进一步扩展（但在这里我们不会这么做）在前面两个步骤之后采取的额外步骤：
- en: Executes the query against the database, which returns a two-dimensional table.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据库上执行查询，返回一个二维表。
- en: Uses a third LLM call to summarize the query results into a textual answer to
    the original user question.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第三次LLM调用将查询结果总结为对原始用户问题的文本答案。
- en: 'And now let’s implement this with LangGraph:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用LangGraph来实现这个架构：
- en: '*Python*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*JavaScript*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The visual representation of the graph is shown in [Figure 5-3](#ch05_figure_3_1736545670024001).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图的视觉表示如图[图5-3](#ch05_figure_3_1736545670024001)所示。
- en: '![A diagram of a program  Description automatically generated](assets/lelc_0503.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![程序图，描述自动生成](assets/lelc_0503.png)'
- en: Figure 5-3\. The chain architecture
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3. 链式架构
- en: 'Here’s an example of inputs and outputs:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个输入和输出的例子：
- en: '*Python*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*JavaScript*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*The output:*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: First, the `generate_sql` node is executed, which populates the `sql_query`
    key in the state (which will be part of the final output) and updates the `messages`
    key with the new messages. Then the `explain_sql` node runs, taking the SQL query
    generated in the previous step and populating the `sql_explanation` key in the
    state. At this point, the graph finishes running, and the output is returned to
    the caller.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，执行`generate_sql`节点，填充状态中的`sql_query`键（这将是最终输出的一部分）并更新`messages`键以包含新消息。然后运行`explain_sql`节点，使用前一步生成的SQL查询填充状态中的`sql_explanation`键。此时，图运行完成，输出返回给调用者。
- en: Note also the use of separate input and output schemas when creating the `StateGraph`.
    This lets you customize which parts of the state are accepted as input from the
    user and which are returned as the final output. The remaining state keys are
    used by the graph nodes internally to keep intermediate state and are made available
    to the user as part of the streaming output produced by `stream()`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在创建`StateGraph`时也使用了单独的输入和输出模式。这允许你自定义哪些状态部分被接受为用户的输入，哪些作为最终输出返回。剩余的状态键由图节点内部用于保持中间状态，并作为`stream()`产生的流式输出的一部分提供给用户。
- en: 'Architecture #3: Router'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '架构 #3: 路由器'
- en: 'This next architecture moves up the autonomy ladder by assigning to LLMs the
    next of the responsibilities we outlined before: deciding the next step to take.
    That is, whereas the chain architecture always executes a static sequence of steps
    (however many), the router architecture is characterized by using an LLM to choose
    between certain predefined steps.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构通过将我们之前概述的下一个责任分配给LLM来提高自主性等级：决定下一步要采取的行动。也就是说，链式架构始终执行一系列静态步骤（无论多少），而路由器架构的特点是使用LLM在预定义的步骤之间进行选择。
- en: Let’s use the example of a RAG application with access to multiple indexes of
    documents from different domains (refer to [Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)
    for more on indexing). Usually you can extract better performance from LLMs by
    avoiding the inclusion of irrelevant information in the prompt. Therefore, in
    building this application, we should try to pick the right index to use for each
    query and use only that one. The key development in this architecture is to use
    an LLM to make this decision, effectively using an LLM to evaluate each incoming
    query and decide which index it should use for that *particular* query.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个RAG应用为例，该应用可以访问来自不同领域的多个文档索引（更多关于索引的内容，请参阅[第2章](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)）。通常，通过在提示中避免包含无关信息，可以从中提取更好的LLMs性能。因此，在构建此应用时，我们应该尝试为每个查询选择正确的索引，并仅使用该索引。此架构的关键发展是使用LLM来做出这个决定，有效地使用LLM来评估每个传入的查询并决定该查询应使用哪个*特定*索引。
- en: Note
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Before the advent of LLMs, the usual way of solving this problem would be to
    build a classifier model using ML techniques and a dataset mapping example user
    queries to the right index. This could prove quite challenging, as it requires
    the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs出现之前，解决这个问题的通常方法是通过机器学习技术构建一个分类模型，并使用一个数据集将示例用户查询映射到正确的索引。这可能会相当具有挑战性，因为它需要以下条件：
- en: Assembling that dataset by hand
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动组装该数据集
- en: Generating enough *features* (quantitative attributes) from each user query
    to enable training a classifier for the task
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从每个用户查询中生成足够的*特征*（定量属性），以使能够训练一个用于该任务的分类器
- en: LLMs, given their encoding of human language, can effectively serve as this
    classifier with zero, or very few, examples or additional training.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs，鉴于其对人类语言的编码，可以有效地作为这个分类器，无需或仅需极少的示例或额外训练。
- en: 'First, let’s describe the flow in words:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们用文字描述一下流程：
- en: An LLM call to pick which of the available indexes to use, given the user-supplied
    query, and the developer-supplied description of the indexes
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个LLM调用，根据用户提供的查询和开发者提供的索引描述来选择要使用的可用索引
- en: A retrieval step that queries the chosen index for the most relevant documents
    for the user query
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个检索步骤，查询所选索引以获取用户查询的最相关文档
- en: Another LLM call to generate an answer, given the user-supplied query and the
    list of relevant documents fetched from the index
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个LLM调用，根据用户提供的查询和从索引中检索到的相关文档列表生成答案
- en: 'And now let’s implement it with LangGraph:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用LangGraph来实现它：
- en: '*Python*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*JavaScript*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The visual representation is shown in [Figure 5-4](#ch05_figure_4_1736545670024020).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 可视表示如图[图5-4](#ch05_figure_4_1736545670024020)所示。
- en: '![A diagram of a router  Description automatically generated](assets/lelc_0504.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![一个路由器图  自动生成的描述](assets/lelc_0504.png)'
- en: Figure 5-4\. The router architecture
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4. 路由器架构
- en: Notice how this is now starting to become more useful, as it shows the two possible
    paths through the graph, through `retrieve_medical_records` or through `retrieve_insurance_faqs`,
    and that for both of those, we first visit the `router` node and finish by visiting
    the `generate_answer` node. These two possible paths were implemented through
    the use of a conditional edge, implemented in the function `pick_retriever`, which
    maps the `domain` picked by the LLM to one of the two nodes mentioned earlier.
    The conditional edge is shown in [Figure 5-4](#ch05_figure_4_1736545670024020)
    as dotted lines from the source node to the destination nodes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意现在这开始变得更有用，因为它显示了通过`retrieve_medical_records`或通过`retrieve_insurance_faqs`的两种可能的图路径，并且对于这两者，我们首先访问`router`节点，然后访问`generate_answer`节点结束。这两个可能的路径是通过使用一个条件边实现的，该条件边在`pick_retriever`函数中实现，它将LLM选择的`domain`映射到前面提到的两个节点之一。条件边如图[图5-4](#ch05_figure_4_1736545670024020)中的虚线从源节点到目标节点所示。
- en: 'And now for example inputs and outputs, this time with streaming output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一些示例输入和输出，这次是带有流式输出的：
- en: '*Python*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*JavaScript*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*The output* (the actual answer is not shown, since it would depend on your
    documents):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出*（实际的答案未显示，因为它将取决于您的文档）：'
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This output stream contains the values returned by each node that ran during
    this execution of the graph. Let’s take it one at a time. The top-level key in
    each dictionary is the name of the node, and the value for that key is what that
    node returned:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出流包含在此次图执行期间每个运行节点的返回值。让我们逐个来看。每个字典中的顶级键是节点的名称，该键的值是节点返回的内容：
- en: The `router` node returned an update to `messages` (this would allow us to easily
    continue this conversation using the memory technique described earlier), and
    the `domain` the LLM picked for this user’s query, in this case `insurance`.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`router` 节点返回了对 `messages` 的更新（这将使我们能够轻松地继续使用之前描述的记忆技巧进行这次对话），以及 LLM 为此用户查询选择的
    `domain`，在本例中为 `insurance`。'
- en: Then the `pick_retriever` function ran and returned the name of the next node
    to run, based on the `domain` identified by the LLM call in the previous step.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，`pick_retriever` 函数运行并返回了下一个要运行的节点名称，基于前一步中 LLM 调用所确定的 `domain`。
- en: Then the `retrieve_insurance_faqs` node ran, returning a set of relevant documents
    from that index. This means that on the drawing of the graph seen earlier, we
    took the left path, as decided by the LLM.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，`retrieve_insurance_faqs` 节点运行，从该索引中返回一组相关文档。这意味着在之前看到的图上，我们选择了 LLM 决定的左侧路径。
- en: Finally, the `generate_answer` node ran, which took those documents and the
    original user query and produced an answer to the question, which was written
    to the state (along with a final update to the `messages` key).
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，`generate_answer` 节点运行，它使用那些文档和原始用户查询生成了一个问题的答案，并将其写入状态（以及 `messages` 键的最终更新）。
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter talked about the key trade-off when building LLM applications:
    agency versus oversight. The more autonomous an LLM application is, the more it
    can do—but that raises the need for more mechanisms of control over its actions.
    We moved on to different cognitive architectures that strike different balances
    between agency and oversight.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了构建 LLM 应用程序时的关键权衡：自主性与监管。LLM 应用程序越自主，它能做的事情就越多——但这也提高了对其行为进行更多控制机制的需求。我们继续探讨了不同的认知架构，这些架构在自主性与监管之间取得了不同的平衡。
- en: '[Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341) talks about
    the most powerful of the cognitive architectures we’ve seen so far: the agent
    architecture.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 6 章](ch06.html#ch06_agent_architecture_1736545671750341) 讨论了我们迄今为止所见到的最强大的认知架构：代理架构。'
- en: ^([1](ch05.html#id699-marker)) Theodore R. Sumers et al., [“Cognitive Architectures
    for Language Agents”](https://oreil.ly/cuQnT), arXiv, September 5, 2023, updated
    March 15, 2024\.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#id699-marker)) Theodore R. Sumers 等人，[“Cognitive Architectures
    for Language Agents”](https://oreil.ly/cuQnT)，arXiv，2023 年 9 月 5 日，更新于 2024 年
    3 月 15 日。
- en: '^([2](ch05.html#id709-marker)) Tal Ridnik et al., [“Code Generation with AlphaCodium:
    From Prompt Engineering to Flow Engineering”](https://oreil.ly/0wHX4), arXiv,
    January 16, 2024\.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch05.html#id709-marker)) Tal Ridnik 等人，[“Code Generation with AlphaCodium:
    From Prompt Engineering to Flow Engineering”](https://oreil.ly/0wHX4)，arXiv，2024
    年 1 月 16 日。'
