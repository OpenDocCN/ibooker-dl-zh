- en: Chapter 5\. Cognitive Architectures with LangGraph
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。使用LangGraph的认知架构
- en: 'So far, we’ve looked at the most common features of LLM applications:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了LLM应用程序最常见的特点：
- en: Prompting techniques in the [Preface](preface01.html#pr01_preface_1736545679069216)
    and [Chapter 1](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[序言](preface01.html#pr01_preface_1736545679069216)和[第一章](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004)中介绍了一些提示技巧。
- en: RAG in Chapters [2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)
    and [3](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)和[第三章](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580)中介绍了RAG。
- en: Memory in [Chapter 4](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第四章](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431)中的记忆'
- en: 'The next question should be: How do we assemble these pieces into a coherent
    application that achieves the goal we set out to solve? To draw a parallel with
    the world of bricks and mortar, a swimming pool and a one-story house are built
    of the same materials, but obviously serve very different purposes. What makes
    them uniquely suited to their different purposes is the plan for how those materials
    are combined—that is, their architecture. The same is true when building LLM applications.
    The most important decisions you have to make are how to assemble the different
    components you have at your disposal (such as RAG, prompting techniques, memory)
    into something that achieves your purpose.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个问题应该是：我们如何将这些部分组合成一个连贯的应用程序，以实现我们设定的目标？为了与砖石建筑的领域进行类比，游泳池和一层的房子虽然由相同的材料建造，但显然服务于非常不同的目的。使它们各自适合其不同目的的是这些材料组合的计划——即它们的建筑。在构建LLM应用程序时也是如此。你必须做出的最重要的决定是如何将你拥有的不同组件（如RAG、提示技巧、记忆）组合成实现你目的的东西。
- en: 'Before we look at specific architectures, let’s walk through an example. Any
    LLM application you might build will start from a purpose: what the app is designed
    to do. Let’s say you want to build an email assistant—an LLM application that
    reads your emails before you do and aims to reduce the amount of emails you need
    to look at. The application might do this by archiving a few uninteresting ones,
    directly replying to some, and marking others as deserving of your attention later.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看具体的架构之前，让我们通过一个例子来了解一下。你可能会构建的任何LLM应用程序都将从一个目的开始：应用程序被设计来做什么。假设你想构建一个电子邮件助手——一个LLM应用程序，它在你之前阅读你的电子邮件，并旨在减少你需要查看的电子邮件数量。应用程序可能通过存档一些不感兴趣的邮件，直接回复一些邮件，并将其他邮件标记为稍后需要你注意的邮件来实现这一点。
- en: 'You probably also would want the app to be bound by some constraints in its
    action. Listing those constraints helps tremendously, as they will help inform
    the search for the right architecture. [Chapter 8](ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600)
    covers these constraints in more detail and how to work with them. For this hypothetical
    email assistant, let’s say we’d like it to do the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还希望应用程序在行动上受到一些约束。列出这些约束非常有帮助，因为它们将有助于指导对正确架构的搜索。[第八章](ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600)更详细地介绍了这些约束以及如何处理它们。对于这个假设的电子邮件助手，假设我们希望它执行以下操作：
- en: Minimize the number of times it interrupts you (after all, the whole point is
    to save time).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化它打扰你的次数（毕竟，整个目的就是节省时间）。
- en: Avoid having your email correspondents receive a reply that you’d never have
    sent yourself.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免让你的电子邮件联系人收到你永远不会自己发送的回复。
- en: 'This hints at the key trade-off often faced when building LLM apps: the trade-off
    between *agency* (or the capacity to act autonomously) and *reliability* (or the
    degree to which you can trust its outputs). Intuitively, the email assistant will
    be more useful if it takes more actions without your involvement, but if you take
    it too far, it will inevitably send emails you wish it hadn’t.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这暗示了在构建LLM应用程序时经常面临的关键权衡：在*自主性*（或自主行动的能力）和*可靠性*（或你可以信任其输出的程度）之间的权衡。直观地说，如果电子邮件助手在没有你参与的情况下采取更多行动，它将更有用，但如果你做得太过分，它不可避免地会发送你希望它没有发送的电子邮件。
- en: 'One way to describe the degree of autonomy of an LLM application is to evaluate
    how much of the behavior of the application is determined by an LLM (versus code):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 描述LLM应用程序自主程度的一种方法是通过评估应用程序的行为中有多少是由LLM（而不是代码）决定的：
- en: Have an LLM decide the output of a step (for instance, write a draft reply to
    an email).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让LLM决定一个步骤的输出（例如，写一封电子邮件的草稿回复）。
- en: 'Have an LLM decide the next step to take (for instance, for a new email, decide
    between the three actions it can take on an email: archive, reply, or mark for
    review).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让LLM决定下一步要采取的行动（例如，对于一封新邮件，决定它可以在邮件上执行的三种操作：存档、回复或标记为审阅）。
- en: Have an LLM decide what steps are available to take (for instance, have the
    LLM write code that executes a dynamic action you didn’t preprogram into the application).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让LLM决定可以采取哪些步骤（例如，让LLM编写执行你未预先编程到应用程序中的动态操作的代码）。
- en: We can classify a number of popular *recipes* for building LLM applications
    based on where they fall in this spectrum of autonomy, that is, which of the three
    tasks just mentioned are handled by an LLM and which remain in the hands of the
    developer or user. These recipes can be called *cognitive architectures*. In the
    artificial intelligence field, the term *cognitive architecture* has long been
    used to denote models of human reasoning (and their implementations in computers).
    An LLM cognitive architecture (the term was first applied to LLMs, to our knowledge,
    in a paper^([1](ch05.html#id699))) can be defined as a recipe for the steps to
    be taken by an LLM application (see [Figure 5-1](#ch05_figure_1_1736545670023944)).
    A *step* is, for instance, retrieval of relevant documents (RAG), or calling an
    LLM with a chain-of-thought prompt.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据它们在这个自主性谱系中的位置对构建LLM应用的许多流行**食谱**进行分类，即哪些任务由LLM处理，哪些仍然掌握在开发者或用户手中。这些食谱可以被称为**认知架构**。在人工智能领域，**认知架构**这个术语长期以来一直用来表示人类推理模型（及其在计算机中的实现）。一个LLM认知架构（据我们所知，这个术语最初应用于LLM，在一篇论文中提出）可以被定义为LLM应用要采取步骤的食谱（见[图5-1](#ch05_figure_1_1736545670023944)）。例如，“步骤”可以是检索相关文档（RAG），或者使用思维链提示调用LLM。
- en: '![A screenshot of a computer application  Description automatically generated](assets/lelc_0501.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![计算机应用程序的截图  自动生成的描述](assets/lelc_0501.png)'
- en: Figure 5-1\. Cognitive architectures for LLM applications
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. LLM应用的认知架构
- en: 'Now let’s look at each of the major architectures, or recipes, that you can
    use when building your application (as shown in [Figure 5-1](#ch05_figure_1_1736545670023944)):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看你可以用来构建应用程序的每个主要架构或食谱（如图[图5-1](#ch05_figure_1_1736545670023944)所示）：
- en: '0: Code'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '0: 代码'
- en: This is not an LLM cognitive architecture (hence we numbered it **0**), as it
    doesn’t use LLMs at all. You can think of this as regular software you’re used
    to writing. The first interesting architecture (for this book, at any rate) is
    actually the next one.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个LLM认知架构（因此我们将其编号为**0**），因为它根本不使用LLM。你可以将其视为你习惯编写的常规软件。对于这本书来说，第一个有趣的架构实际上是下一个。
- en: '1: LLM call'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '1: LLM调用'
- en: This is the majority of the examples we’ve seen in the book so far, with one
    LLM call only. This is useful mostly when it’s part of a larger application that
    makes use of an LLM for achieving a specific task, such as translating or summarizing
    a piece of text.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在这本书中迄今为止看到的绝大多数例子，只有一个LLM调用。这主要用于当它是实现特定任务（如翻译或总结一段文本）的更大应用程序的一部分时。
- en: '2: Chain'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '2: 链'
- en: 'The next level up, so to speak, comes with the use of multiple LLM calls in
    a predefined sequence. For instance, a text-to-SQL application (which receives
    as input from the user a natural language description of some calculation to make
    over a database) could make use of two LLM calls in sequence:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 次一级的是，使用预定义序列中的多个LLM调用。例如，一个文本到SQL的应用程序（它从用户那里接收一个自然语言描述，描述对数据库进行某些计算的操作）可以连续使用两个LLM调用：
- en: One LLM call to generate a SQL query, from the natural language query, provided
    by the user, and a description of the database contents, provided by the developer.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LLM调用，根据用户提供的自然语言查询和开发者提供的数据库内容描述，生成一个SQL查询。
- en: And another LLM call to write an explanation of the query appropriate for a
    nontechnical user, given the query generated in the previous call. This one could
    then be used to enable the user to check if the generated query matches his request.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个LLM调用，根据前一个调用生成的查询，为非技术用户编写一个适当的查询解释。这可以用来使用户能够检查生成的查询是否符合他的要求。
- en: '3: Router'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '3: 路由器'
- en: 'This next step comes from using the LLM to define the sequence of steps to
    take. That is, whereas the chain architecture always executes a static sequence
    of steps (however many) determined by the developer, the router architecture is
    characterized by using an LLM to choose between certain predefined steps. An example
    would be a RAG application with multiple indexes of documents from different domains,
    with the following steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是由LLM定义要采取的步骤序列。也就是说，链式架构总是执行由开发者确定的静态步骤序列（无论多少），而路由架构的特点是使用LLM在预定义的某些步骤之间进行选择。一个例子是一个具有来自不同领域多个文档索引的RAG应用，以下步骤如下：
- en: An LLM call to pick which of the available indexes to use, given the user-supplied
    query and the developer-supplied description of the indexes.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个LLM调用，用于根据用户提供的查询和开发者提供的索引描述来选择要使用的可用索引。
- en: A retrieval step that queries the chosen index for the most relevant documents
    for the user query.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个检索步骤，查询所选索引以获取用户查询的最相关文档。
- en: Another LLM call to generate an answer, given the user-supplied query and the
    list of relevant documents fetched from the index.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个LLM调用，根据用户提供的查询和从索引中检索到的相关文档列表生成答案。
- en: That’s as far as we’ll go in this chapter. We will talk about each of these
    architectures in turn. The next chapters discuss the agentic architectures, which
    make even more use of LLMs. But first let’s talk about some better tooling to
    help us on this journey.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本章我们将讨论的内容。我们将依次讨论这些架构。下一章将讨论代理架构，这些架构更多地使用了LLM。但首先让我们谈谈一些更好的工具，以帮助我们在这个旅程中。
- en: 'Architecture #1: LLM Call'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构#1：LLM调用
- en: As an example of the LLM call architecture, we’ll return to the chatbot we created
    in [Chapter 4](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431).
    This chatbot will respond directly to user messages.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作为LLM调用架构的示例，我们将回到我们在[第4章](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431)中创建的聊天机器人。这个聊天机器人将直接响应用户消息。
- en: 'Start by creating a `StateGraph`, to which we’ll add a node to represent the
    LLM call:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个`StateGraph`，我们将添加一个节点来表示LLM调用：
- en: '*Python*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*JavaScript*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can also draw a visual representation of the graph:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制图表的视觉表示：
- en: '*Python*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*JavaScript*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The graph we just made looks like [Figure 5-2](#ch05_figure_2_1736545670023979).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才制作的图表看起来像[图5-2](#ch05_figure_2_1736545670023979)。
- en: '![A diagram of a chatbot  Description automatically generated](assets/lelc_0502.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![聊天机器人图示 自动生成的描述](assets/lelc_0502.png)'
- en: Figure 5-2\. The LLM call architecture
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2. LLM调用架构
- en: 'You can run it with the familiar `stream()` method you’ve seen in earlier chapters:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用在前面章节中看到的熟悉`stream()`方法来运行它：
- en: '*Python*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5] const input = {messages: [new HumanMessage(''hi!)]} for await (const
    chunk of await graph.stream(input)) {   console.log(chunk) } [PRE6] { "chatbot":
    { "messages": [AIMessage("How can I help you?")] } } [PRE7]`  [PRE8] from typing
    import Annotated, TypedDict  from langchain_core.messages import HumanMessage,
    SystemMessage from langchain_openai import ChatOpenAI  from langgraph.graph import
    END, START, StateGraph from langgraph.graph.message import add_messages  # useful
    to generate SQL query model_low_temp = ChatOpenAI(temperature=0.1) # useful to
    generate natural language outputs model_high_temp = ChatOpenAI(temperature=0.7)  class
    State(TypedDict):     # to track conversation history     messages: Annotated[list,
    add_messages]     # input     user_query: str     # output     sql_query: str     sql_explanation:
    str  class Input(TypedDict):     user_query: str  class Output(TypedDict):     sql_query:
    str     sql_explanation: str  generate_prompt = SystemMessage(     """You are
    a helpful data analyst who generates SQL queries for users based   on their questions."""
    )  def generate_sql(state: State) -> State:     user_message = HumanMessage(state["user_query"])     messages
    = [generate_prompt, *state["messages"], user_message]     res = model_low_temp.invoke(messages)     return
    {         "sql_query": res.content,         # update conversation history         "messages":
    [user_message, res],     }  explain_prompt = SystemMessage(     "You are a helpful
    data analyst who explains SQL queries to users." )  def explain_sql(state: State)
    -> State:     messages = [         explain_prompt,         # contains user''s
    query and SQL query from prev step         *state["messages"],     ]     res =
    model_high_temp.invoke(messages)     return {         "sql_explanation": res.content,         #
    update conversation history         "messages": res,     }  builder = StateGraph(State,
    input=Input, output=Output) builder.add_node("generate_sql", generate_sql) builder.add_node("explain_sql",
    explain_sql) builder.add_edge(START, "generate_sql") builder.add_edge("generate_sql",
    "explain_sql") builder.add_edge("explain_sql", END)  graph = builder.compile()
    [PRE9] import {   HumanMessage,   SystemMessage } from "@langchain/core/messages";
    import { ChatOpenAI } from "@langchain/openai"; import {   StateGraph,   Annotation,   messagesStateReducer,   START,   END,
    } from "@langchain/langgraph";  // useful to generate SQL query const modelLowTemp
    = new ChatOpenAI({ temperature: 0.1 }); // useful to generate natural language
    outputs const modelHighTemp = new ChatOpenAI({ temperature: 0.7 });  const annotation
    = Annotation.Root({   messages: Annotation({ reducer: messagesStateReducer, default:
    () => [] }),   user_query: Annotation(),   sql_query: Annotation(),   sql_explanation:
    Annotation(), });  const generatePrompt = new SystemMessage(   `You are a helpful
    data analyst who generates SQL queries for users based on   their questions.`
    );  async function generateSql(state) {   const userMessage = new HumanMessage(state.user_query);   const
    messages = [generatePrompt, ...state.messages, userMessage];   const res = await
    modelLowTemp.invoke(messages);   return {     sql_query: res.content as string,     //
    update conversation history     messages: [userMessage, res],   }; }  const explainPrompt
    = new SystemMessage(   "You are a helpful data analyst who explains SQL queries
    to users." );  async function explainSql(state) {   const messages = [explainPrompt,
    ...state.messages];   const res = await modelHighTemp.invoke(messages);   return
    {     sql_explanation: res.content as string,     // update conversation history     messages:
    res,   }; }  const builder = new StateGraph(annotation)   .addNode("generate_sql",
    generateSql)   .addNode("explain_sql", explainSql)   .addEdge(START, "generate_sql")   .addEdge("generate_sql",
    "explain_sql")   .addEdge("explain_sql", END);  const graph = builder.compile();
    [PRE10] graph.invoke({   "user_query": "What is the total sales for each product?"
    }) [PRE11] await graph.invoke({   user_query: "What is the total sales for each
    product?" }) [PRE12] {   "sql_query": "SELECT product_name, SUM(sales_amount)
    AS total_sales\nFROM        sales\nGROUP BY product_name;",   "sql_explanation":
    "This query will retrieve the total sales for each product        by summing up
    the sales_amount column for each product and grouping the       results by product_name.",
    } [PRE13] from typing import Annotated, Literal, TypedDict  from langchain_core.documents
    import Document from langchain_core.messages import HumanMessage, SystemMessage
    from langchain_core.vectorstores.in_memory import InMemoryVectorStore from langchain_openai
    import ChatOpenAI, OpenAIEmbeddings  from langgraph.graph import END, START, StateGraph
    from langgraph.graph.message import add_messages  embeddings = OpenAIEmbeddings()
    # useful to generate SQL query model_low_temp = ChatOpenAI(temperature=0.1) #
    useful to generate natural language outputs model_high_temp = ChatOpenAI(temperature=0.7)  class
    State(TypedDict):     # to track conversation history     messages: Annotated[list,
    add_messages]     # input     user_query: str     # output     domain: Literal["records",
    "insurance"]     documents: list[Document]     answer: str  class Input(TypedDict):     user_query:
    str  class Output(TypedDict):     documents: list[Document]     answer: str  #
    refer to Chapter 2 on how to fill a vector store with documents medical_records_store
    = InMemoryVectorStore.from_documents([], embeddings) medical_records_retriever
    = medical_records_store.as_retriever()  insurance_faqs_store = InMemoryVectorStore.from_documents([],
    embeddings) insurance_faqs_retriever = insurance_faqs_store.as_retriever()  router_prompt
    = SystemMessage(     """You need to decide which domain to route the user query
    to. You have two   domains to choose from:  - records: contains medical records
    of the patient, such as   diagnosis, treatment, and prescriptions.  - insurance:
    contains frequently asked questions about insurance   policies, claims, and coverage.  Output
    only the domain name.""" )  def router_node(state: State) -> State:     user_message
    = HumanMessage(state["user_query"])     messages = [router_prompt, *state["messages"],
    user_message]     res = model_low_temp.invoke(messages)     return {         "domain":
    res.content,         # update conversation history         "messages": [user_message,
    res],     }  def pick_retriever(     state: State, ) -> Literal["retrieve_medical_records",
    "retrieve_insurance_faqs"]:     if state["domain"] == "records":         return
    "retrieve_medical_records"     else:         return "retrieve_insurance_faqs"  def
    retrieve_medical_records(state: State) -> State:     documents = medical_records_retriever.invoke(state["user_query"])     return
    {         "documents": documents,     }  def retrieve_insurance_faqs(state: State)
    -> State:     documents = insurance_faqs_retriever.invoke(state["user_query"])     return
    {         "documents": documents,     }  medical_records_prompt = SystemMessage(     """You
    are a helpful medical chatbot who answers questions based on the   patient''s
    medical records, such as diagnosis, treatment, and   prescriptions.""" )  insurance_faqs_prompt
    = SystemMessage(     """You are a helpful medical insurance chatbot who answers
    frequently asked   questions about insurance policies, claims, and coverage."""
    )  def generate_answer(state: State) -> State:     if state["domain"] == "records":         prompt
    = medical_records_prompt     else:         prompt = insurance_faqs_prompt     messages
    = [         prompt,         *state["messages"],         HumanMessage(f"Documents:
    {state["documents"]}"),     ]     res = model_high_temp.invoke(messages)     return
    {         "answer": res.content,         # update conversation history         "messages":
    res,     }  builder = StateGraph(State, input=Input, output=Output) builder.add_node("router",
    router_node) builder.add_node("retrieve_medical_records", retrieve_medical_records)
    builder.add_node("retrieve_insurance_faqs", retrieve_insurance_faqs) builder.add_node("generate_answer",
    generate_answer) builder.add_edge(START, "router") builder.add_conditional_edges("router",
    pick_retriever) builder.add_edge("retrieve_medical_records", "generate_answer")
    builder.add_edge("retrieve_insurance_faqs", "generate_answer") builder.add_edge("generate_answer",
    END)  graph = builder.compile() [PRE14] import {   HumanMessage,   SystemMessage
    } from "@langchain/core/messages"; import {   ChatOpenAI,   OpenAIEmbeddings }
    from "@langchain/openai"; import {   MemoryVectorStore } from "langchain/vectorstores/memory";
    import {   DocumentInterface } from "@langchain/core/documents"; import {   StateGraph,   Annotation,   messagesStateReducer,   START,   END,
    } from "@langchain/langgraph";  const embeddings = new OpenAIEmbeddings(); //
    useful to generate SQL query const modelLowTemp = new ChatOpenAI({ temperature:
    0.1 }); // useful to generate natural language outputs const modelHighTemp = new
    ChatOpenAI({ temperature: 0.7 });  const annotation = Annotation.Root({   messages:
    Annotation({ reducer: messagesStateReducer, default: () => [] }),   user_query:
    Annotation(),   domain: Annotation(),   documents: Annotation(),   answer: Annotation(),
    });  // refer to Chapter 2 on how to fill a vector store with documents const
    medicalRecordsStore = await MemoryVectorStore.fromDocuments(   [],   embeddings
    ); const medicalRecordsRetriever = medicalRecordsStore.asRetriever();  const insuranceFaqsStore
    = await MemoryVectorStore.fromDocuments(   [],   embeddings ); const insuranceFaqsRetriever
    = insuranceFaqsStore.asRetriever();  const routerPrompt = new SystemMessage(   `You
    need to decide which domain to route the user query to. You have two   domains
    to choose from:  - records: contains medical records of the patient, such as diagnosis,   treatment,
    and prescriptions.  - insurance: contains frequently asked questions about insurance   policies,
    claims, and coverage.  Output only the domain name.` );  async function routerNode(state)
    {   const userMessage = new HumanMessage(state.user_query);   const messages =
    [routerPrompt, ...state.messages, userMessage];   const res = await modelLowTemp.invoke(messages);   return
    {     domain: res.content as "records" | "insurance",     // update conversation
    history     messages: [userMessage, res],   }; }  function pickRetriever(state)
    {   if (state.domain === "records") {     return "retrieve_medical_records";   }
    else {     return "retrieve_insurance_faqs";   } }  async function retrieveMedicalRecords(state)
    {   const documents = await medicalRecordsRetriever.invoke(state.user_query);   return
    {     documents,   }; }  async function retrieveInsuranceFaqs(state) {   const
    documents = await insuranceFaqsRetriever.invoke(state.user_query);   return {     documents,   };
    }  const medicalRecordsPrompt = new SystemMessage(   `You are a helpful medical
    chatbot who answers questions based on the   patient''s medical records, such
    as diagnosis, treatment, and   prescriptions.` );  const insuranceFaqsPrompt =
    new SystemMessage(   `You are a helpful medical insurance chatbot who answers
    frequently asked   questions about insurance policies, claims, and coverage.`
    );  async function generateAnswer(state) {   const prompt =     state.domain ===
    "records" ? medicalRecordsPrompt : insuranceFaqsPrompt;   const messages = [     prompt,     ...state.messages,     new
    HumanMessage(`Documents: ${state.documents}`),   ];   const res = await modelHighTemp.invoke(messages);   return
    {     answer: res.content as string,     // update conversation history     messages:
    res,   }; }  const builder = new StateGraph(annotation)   .addNode("router", routerNode)   .addNode("retrieve_medical_records",
    retrieveMedicalRecords)   .addNode("retrieve_insurance_faqs", retrieveInsuranceFaqs)   .addNode("generate_answer",
    generateAnswer)   .addEdge(START, "router")   .addConditionalEdges("router", pickRetriever)   .addEdge("retrieve_medical_records",
    "generate_answer")   .addEdge("retrieve_insurance_faqs", "generate_answer")   .addEdge("generate_answer",
    END);  const graph = builder.compile(); [PRE15] input = {     "user_query": "Am
    I covered for COVID-19 treatment?" } for c in graph.stream(input):     print(c)
    [PRE16] const input = {   user_query: "Am I covered for COVID-19 treatment?" }
    for await (const chunk of await graph.stream(input)) { console.log(chunk) } [PRE17]
    {     "router": {         "messages": [             HumanMessage(content="Am I
    covered for COVID-19 treatment?"),             AIMessage(content="insurance"),         ],         "domain":
    "insurance",     } } {     "retrieve_insurance_faqs": {         "documents": [...]     }
    } {     "generate_answer": {         "messages": AIMessage(             content="...",         ),         "answer":
    "...",     } } [PRE18]`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
