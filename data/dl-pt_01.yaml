- en: 1 Introducing deep learning and the PyTorch Library
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 介绍深度学习和PyTorch库
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How deep learning changes our approach to machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习如何改变我们对机器学习的方法
- en: Understanding why PyTorch is a good fit for deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解为什么PyTorch非常适合深度学习
- en: Examining a typical deep learning project
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查典型的深度学习项目
- en: The hardware you’ll need to follow along with the examples
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要的硬件来跟随示例
- en: The poorly defined term *artificial intelligence* covers a set of disciplines
    that have been subjected to a tremendous amount of research, scrutiny, confusion,
    fantastical hype, and sci-fi fearmongering. Reality is, of course, far more sanguine.
    It would be disingenuous to assert that today’s machines are learning to “think”
    in any human sense of the word. Rather, we’ve discovered a general class of algorithms
    that are able to approximate complicated, nonlinear processes very, very effectively,
    which we can use to automate tasks that were previously limited to humans.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*人工智能*的定义模糊，涵盖了一系列经历了大量研究、审查、混乱、夸张和科幻恐慌的学科。现实当然要乐观得多。断言今天的机器在任何人类意义上都在“思考”是不诚实的。相反，我们发现了一类能够非常有效地逼近复杂非线性过程的算法，我们可以利用这些算法来自动化以前仅限于人类的任务。
- en: 'For example, at [https://inferkit.com/](https://inferkit.com/), a language
    model called GPT-2 can generate coherent paragraphs of text one word at a time.
    When we fed it this very paragraph, it produced the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[https://inferkit.com/](https://inferkit.com/)，一个名为GPT-2的语言模型可以逐字生成连贯的段落文本。当我们将这段文字输入时，它生成了以下内容：
- en: Next we’re going to feed in a list of phrases from a corpus of email addresses,
    and see if the program can parse the lists as sentences. Again, this is much more
    complicated and far more complex than the search at the beginning of this post,
    but hopefully helps you understand the basics of constructing sentence structures
    in various programming languages.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将输入一组来自电子邮件地址语料库的短语列表，并查看程序是否能将列表解析为句子。再次强调，这比本文开头的搜索要复杂得多，也更加复杂，但希望能帮助您了解在各种编程语言中构建句子结构的基础知识。
- en: That’s remarkably coherent for a machine, even if there isn’t a well-defined
    thesis behind the rambling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一台机器来说，这是非常连贯的，即使在这些胡言乱语背后没有一个明确定义的论点。
- en: Even more impressively, the ability to perform these formerly human-only tasks
    is acquired *through examples*, rather than encoded by a human as a set of handcrafted
    rules. In a way, we’re learning that intelligence is a notion we often conflate
    with self-awareness, and self-awareness is definitely not required to successfully
    carry out these kinds of tasks. In the end, the question of computer intelligence
    might not even be important. Edsger W. Dijkstra found that the question of whether
    machines could think was “about as relevant as the question of whether Submarines
    Can Swim.” [¹](#pgfId-1011854)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 更令人印象深刻的是，执行这些以前仅限于人类的任务的能力是*通过示例*获得的，而不是由人类编码为一组手工制作的规则。在某种程度上，我们正在学习智能是一个我们经常与自我意识混淆的概念，而自我意识绝对不是成功执行这类任务所必需的。最终，计算机智能的问题甚至可能并不重要。Edsger
    W. Dijkstra发现，机器是否能够思考的问题“与潜艇是否能游泳的问题一样相关”。[¹](#pgfId-1011854)
- en: That general class of algorithms we’re talking about falls under the AI subcategory
    of *deep learning*, which deals with training mathematical entities named *deep
    neural networks* by presenting instructive examples. Deep learning uses large
    amounts of data to approximate complex functions whose inputs and outputs are
    far apart, like an input image and, as output, a line of text describing the input;
    or a written script as input and a natural-sounding voice reciting the script
    as output; or, even more simply, associating an image of a golden retriever with
    a flag that tells us “Yes, a golden retriever is present.” This kind of capability
    allows us to create programs with functionality that was, until very recently,
    exclusively the domain of human beings.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们谈论的那类算法属于*深度学习*的人工智能子类，它通过提供示例来训练名为*深度神经网络*的数学实体。深度学习使用大量数据来逼近输入和输出相距甚远的复杂函数，比如输入图像和输出的一行描述输入的文本；或者以书面脚本为输入，以自然语音朗读脚本为输出；甚至更简单的是将金毛寻回犬的图像与告诉我们“是的，金毛寻回犬在场”的标志相关联。这种能力使我们能够创建具有直到最近为止仅属于人类领域的功能的程序。
- en: 1.1 The deep learning revolution
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 深度学习革命
- en: To appreciate the paradigm shift ushered in by this deep learning approach,
    let’s take a step back for a bit of perspective. Until the last decade, the broader
    class of systems that fell under the label *machine learning* relied heavily on
    *feature engineering*. Features are transformations on input data that facilitate
    a downstream algorithm, like a classifier, to produce correct outcomes on new
    data. Feature engineering consists of coming up with the right transformations
    so that the downstream algorithm can solve a task. For instance, in order to tell
    ones from zeros in images of handwritten digits, we would come up with a set of
    filters to estimate the direction of edges over the image, and then train a classifier
    to predict the correct digit given a distribution of edge directions. Another
    useful feature could be the number of enclosed holes, as seen in a zero, an eight,
    and, particularly, loopy twos.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要欣赏这种深度学习方法带来的范式转变，让我们退后一步，换个角度看一下。直到最近的十年，广义上属于*机器学习*范畴的系统在很大程度上依赖*特征工程*。特征是对输入数据的转换，有助于下游算法（如分类器）在新数据上产生正确的结果。特征工程包括想出正确的转换，以便下游算法能够解决任务。例如，为了在手写数字图像中区分1和0，我们会想出一组滤波器来估计图像上边缘的方向，然后训练一个分类器来预测给定边缘方向分布的正确数字。另一个有用的特征可能是封闭孔的数量，如0、8和尤其是环绕的2。
- en: Deep learning, on the other hand, deals with finding such representations automatically,
    from raw data, in order to successfully perform a task. In the ones versus zeros
    example, filters would be refined during training by iteratively looking at pairs
    of examples and target labels. This is not to say that feature engineering has
    no place with deep learning; we often need to inject some form of prior knowledge
    in a learning system. However, the ability of a neural network to ingest data
    and extract useful representations on the basis of examples is what makes deep
    learning so powerful. The focus of deep learning practitioners is not so much
    on handcrafting those representations, but on operating on a mathematical entity
    so that it discovers representations from the training data autonomously. Often,
    these automatically created features are better than those that are handcrafted!
    As with many disruptive technologies, this fact has led to a change in perspective.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，深度学习处理的是自动从原始数据中找到这样的表示，以便成功执行任务。在二进制示例中，通过在训练过程中迭代地查看示例和目标标签对来逐步改进滤波器。这并不是说特征工程在深度学习中没有地位；我们经常需要在学习系统中注入某种形式的先验知识。然而，神经网络摄取数据并根据示例提取有用表示的能力是使深度学习如此强大的原因。深度学习从业者的重点不是手工制作这些表示，而是操作数学实体，使其自主地从训练数据中发现表示。通常，这些自动生成的特征比手工制作的特征更好！与许多颠覆性技术一样，这一事实导致了观念的变化。
- en: On the left side of figure 1.1, we see a practitioner busy defining engineering
    features and feeding them to a learning algorithm; the results on the task will
    be as good as the features the practitioner engineers. On the right, with deep
    learning, the raw data is fed to an algorithm that extracts hierarchical features
    automatically, guided by the optimization of its own performance on the task;
    the results will be as good as the ability of the practitioner to drive the algorithm
    toward its goal.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.1的左侧，我们看到一个从业者忙于定义工程特征并将其馈送给学习算法；在任务上的结果将取决于从业者工程的特征的好坏。在右侧，通过深度学习，原始数据被馈送给一个自动提取分层特征的算法，该算法受其在任务上性能优化的指导；结果将取决于从业者驱动算法朝着目标的能力。
- en: '![](../Images/CH01_F01_Stevens2_GS.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F01_Stevens2_GS.png)'
- en: Figure 1.1 Deep learning exchanges the need to handcraft features for an increase
    in data and computational requirements.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 深度学习交换了手工制作特征的需求，增加了数据和计算需求。
- en: 'Starting from the right side in figure 1.1, we already get a glimpse of what
    we need to execute successful deep learning:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从图1.1的右侧开始，我们已经可以看到我们需要执行成功的深度学习所需的一瞥：
- en: We need a way to ingest whatever data we have at hand.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要一种方法来摄取手头的任何数据。
- en: We somehow need to define the deep learning machine.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们以某种方式需要定义深度学习机器。
- en: We must have an automated way, *training*, to obtain useful representations
    and make the machine produce desired outputs.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须有一种自动化的方式，*训练*，来获得有用的表示并使机器产生期望的输出。
- en: This leaves us with taking a closer look at this training thing we keep talking
    about. During training, we use a *criterion*, a real-valued function of model
    outputs and reference data, to provide a numerical score for the discrepancy between
    the desired and actual output of our model (by convention, a lower score is typically
    better). Training consists of driving the criterion toward lower and lower scores
    by incrementally modifying our deep learning machine until it achieves low scores,
    even on data not seen during training.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们不得不更仔细地看看我们一直在谈论的这个训练问题。在训练过程中，我们使用一个*标准*，这是模型输出和参考数据的实值函数，为我们的模型期望输出与实际输出之间的差异提供一个数值分数（按照惯例，较低的分数通常更好）。训练包括通过逐步修改我们的深度学习机器来将标准驱向更低的分数，直到它在训练过程中未见的数据上也能获得低分数。
- en: 1.2 PyTorch for deep learning
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 PyTorch用于深度学习
- en: PyTorch is a library for Python programs that facilitates building deep learning
    projects. It emphasizes flexibility and allows deep learning models to be expressed
    in idiomatic Python. This approachability and ease of use found early adopters
    in the research community, and in the years since its first release, it has grown
    into one of the most prominent deep learning tools across a broad range of applications.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是一个用于Python程序的库，有助于构建深度学习项目。它强调灵活性，并允许用惯用Python表达深度学习模型。这种易接近性和易用性在研究界早期的采用者中得到了认可，在其首次发布后的几年里，它已经发展成为广泛应用于各种应用领域的最重要的深度学习工具之一。
- en: As Python does for programming, PyTorch provides an excellent introduction to
    deep learning. At the same time, PyTorch has been proven to be fully qualified
    for use in professional contexts for real-world, high-profile work. We believe
    that PyTorch’s clear syntax, streamlined API, and easy debugging make it an excellent
    choice for introducing deep learning. We highly recommend studying PyTorch for
    your first deep learning library. Whether it ought to be the last deep learning
    library you learn is a decision we leave up to you.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Python用于编程一样，PyTorch为深度学习提供了一个出色的入门。同时，PyTorch已被证明完全适用于在实际工作中的专业环境中使用。我们相信PyTorch清晰的语法、简化的API和易于调试使其成为��入深度学习的绝佳选择。我们强烈推荐学习PyTorch作为你的第一个深度学习库。至于它是否应该是你学习的最后一个深度学习库，这是一个由你决定的问题。
- en: At its core, the deep learning machine in figure 1.1 is a rather complex mathematical
    function mapping inputs to an output. To facilitate expressing this function,
    PyTorch provides a core data structure, the *tensor*, which is a multidimensional
    array that shares many similarities with NumPy arrays. Around that foundation,
    PyTorch comes with features to perform accelerated mathematical operations on
    dedicated hardware, which makes it convenient to design neural network architectures
    and train them on individual machines or parallel computing resources.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.1中的深度学习机器的核心是一个将输入映射到输出的相当复杂的数学函数。为了方便表达这个函数，PyTorch提供了一个核心数据结构，*张量*，它是一个与NumPy数组有许多相似之处的多维数组。在这个基础上，PyTorch提供了在专用硬件上执行加速数学运算的功能，这使得设计神经网络架构并在单台机器或并行计算资源上训练它们变得方便。
- en: This book is intended as a starting point for software engineers, data scientists,
    and motivated students fluent in Python to become comfortable using PyTorch to
    build deep learning projects. We want this book to be as accessible and useful
    as possible, and we expect that you will be able to take the concepts in this
    book and apply them to other domains. To that end, we use a hands-on approach
    and encourage you to keep your computer at the ready, so you can play with the
    examples and take them a step further. By the time we are through with the book,
    we expect you to be able to take a data source and build out a deep learning project
    with it, supported by the excellent official documentation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在成为软件工程师、数据科学家和精通Python的有动力的学生开始使用PyTorch构建深度学习项目的起点。我们希望这本书尽可能易于访问和有用，并且我们期望您能够将本书中的概念应用到其他领域。为此，我们采用了实践方法，并鼓励您随时准备好计算机，这样您就可以尝试示例并进一步深入研究。到本书结束时，我们期望您能够利用数据源构建出一个深度学习项目，并得到优秀的官方文档支持。
- en: Although we stress the practical aspects of building deep learning systems with
    PyTorch, we believe that providing an accessible introduction to a foundational
    deep learning tool is more than just a way to facilitate the acquisition of new
    technical skills. It is a step toward equipping a new generation of scientists,
    engineers, and practitioners from a wide range of disciplines with working knowledge
    that will be the backbone of many software projects during the decades to come.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们强调使用PyTorch构建深度学习系统的实际方面，但我们认为提供一个易于理解的基础深度学习工具的介绍不仅仅是为了促进新技术技能的习得。这是向来自各种学科领域的新一代科学家、工程师和从业者提供工作知识的一步，这些知识将成为未来几十年许多软件项目的支柱。
- en: 'In order to get the most out of this book, you will need two things:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用本书，您需要两样东西：
- en: Some experience programming in Python. We’re not going to pull any punches on
    that one; you’ll need to be up on Python data types, classes, floating-point numbers,
    and the like.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一些在Python中编程经验。我们不会在这一点上有任何保留；您需要了解Python数据类型、类、浮点数等。
- en: A willingness to dive in and get your hands dirty. We’ll be starting from the
    basics and building up our working knowledge, and it will be much easier for you
    to learn if you follow along with us.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有愿意深入并动手实践的态度。我们将从基础开始建立工作知识，如果您跟着我们一起学习，学习将会更容易。
- en: '*Deep Learning with PyTorch* is organized in three distinct parts. Part 1 covers
    the foundations, examining in detail the facilities PyTorch offers to put the
    sketch of deep learning in figure 1.1 into action with code. Part 2 walks you
    through an end-to-end project involving medical imaging: finding and classifying
    tumors in CT scans, building on the basic concepts introduced in part 1, and adding
    more advanced topics. The short part 3 rounds off the book with a tour of what
    PyTorch offers for deploying deep learning models to production.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用PyTorch进行深度学习* 分为三个不同的部分。第一部分涵盖了基础知识，详细介绍了PyTorch提供的设施，以便用代码将图1.1中深度学习的草图付诸实践。第二部分将带您完成一个涉及医学成像的端到端项目：在CT扫描中查找和分类肿瘤，建立在第一部分介绍的基本概念基础上，并添加更多高级主题。简短的第三部分以PyTorch为主题，介绍了将深度学习模型部署到生产环境中的内容。'
- en: 'Deep learning is a huge space. In this book, we will be covering a tiny part
    of that space: specifically, using PyTorch for smaller-scope classification and
    segmentation projects, with image processing of 2D and 3D datasets used for most
    of the motivating examples. This book focuses on practical PyTorch, with the aim
    of covering enough ground to allow you to solve real-world machine learning problems,
    such as in vision, with deep learning or explore new models as they pop up in
    research literature. Most, if not all, of the latest publications related to deep
    learning research can be found in the arXiV public preprint repository, hosted
    at [https://arxiv.org](https://arxiv.org).[²](#pgfId-1012143)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个庞大的领域。在本书中，我们将涵盖其中的一小部分：具体来说，使用PyTorch进行较小范围的分类和分割项目，其中大部分激励示例使用2D和3D数据集的图像处理。本书侧重于实用的PyTorch，旨在涵盖足够的内容，使您能够解决真实世界的机器学习问题，例如在视觉领域使用深度学习，或者随着研究文献中出现新模型而探索新模型。大多数，如果不是全部，与深度学习研究相关的最新出版物都可以在arXiV公共预印本存储库中找到，托管在[https://arxiv.org](https://arxiv.org)。
- en: 1.3 Why PyTorch?
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 为什么选择PyTorch？
- en: As we’ve said, deep learning allows us to carry out a very wide range of complicated
    tasks, like machine translation, playing strategy games, or identifying objects
    in cluttered scenes, by exposing our model to illustrative examples. In order
    to do so in practice, we need tools that are flexible, so they can be adapted
    to such a wide range of problems, and efficient, to allow training to occur over
    large amounts of data in reasonable times; and we need the trained model to perform
    correctly in the presence of variability in the inputs. Let’s take a look at some
    of the reasons we decided to use PyTorch.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，深度学习使我们能够通过向我们的模型展示说明性示例来执行非常广泛的复杂任务，如机器翻译、玩策略游戏或在混乱场景中识别物体。为了在实践中做到这一点，我们需要灵活的工具，以便能够适应如此广泛的问题，并且高效，以便允许在合理时间内对大量数据进行训练；我们需要训练好的模型在输入变化时能够正确执行。让我们看看我们决定使用
    PyTorch 的一些原因。
- en: PyTorch is easy to recommend because of its simplicity. Many researchers and
    practitioners find it easy to learn, use, extend, and debug. It’s Pythonic, and
    while like any complicated domain it has caveats and best practices, using the
    library generally feels familiar to developers who have used Python previously.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 之所以易于推荐，是因为它的简单性。许多研究人员和实践者发现它易于学习、使用、扩展和调试。它符合 Python 的风格，虽然像任何复杂的领域一样，它有注意事项和最佳实践，但使用该库通常对之前使用过
    Python 的开发人员来说感觉很熟悉。
- en: More concretely, programming the deep learning machine is very natural in PyTorch.
    PyTorch gives us a data type, the `Tensor`, to hold numbers, vectors, matrices,
    or arrays in general. In addition, it provides functions for operating on them.
    We can program with them incrementally and, if we want, interactively, just like
    we are used to from Python. If you know NumPy, this will be very familiar.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在 PyTorch 中编程深度学习机器非常自然。PyTorch 给我们提供了一种数据类型，即`Tensor`，用于保存数字、向量、矩阵或一般数组。此外，它提供了用于操作它们的函数。我们可以像在
    Python 中一样逐步编程，并且如果需要，可以交互式地进行，就像我们从 Python 中习惯的那样。如果你了解 NumPy，这将非常熟悉。
- en: 'But PyTorch offers two things that make it particularly relevant for deep learning:
    first, it provides accelerated computation using graphical processing units (GPUs),
    often yielding speedups in the range of 50x over doing the same calculation on
    a CPU. Second, PyTorch provides facilities that support numerical optimization
    on generic mathematical expressions, which deep learning uses for training. Note
    that both features are useful for scientific computing in general, not exclusively
    for deep learning. In fact, we can safely characterize PyTorch as a high-performance
    library with optimization support for scientific computing in Python.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但是 PyTorch 提供了两个使其特别适用于深度学习的特点：首先，它利用图形处理单元（GPU）进行加速计算，通常比在 CPU 上进行相同计算速度提高了
    50 倍。其次，PyTorch 提供了支持数值优化的功能，用于训练深度学习所使用的通用数学表达式。请注意，这两个特点不仅适用于深度学习，而且适用于科学计算。事实上，我们可以将
    PyTorch 安全地描述为一个在 Python 中为科学计算提供优化支持的高性能库。
- en: A design driver for PyTorch is expressivity, allowing a developer to implement
    complicated models without undue complexity being imposed by the library (it’s
    not a framework!). PyTorch arguably offers one of the most seamless translations
    of ideas into Python code in the deep learning landscape. For this reason, PyTorch
    has seen widespread adoption in research, as witnessed by the high citation counts
    at international conferences.[³](#pgfId-1012208)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的设计驱动因素是表达能力，允许开发人员实现复杂模型而不受库施加的复杂性（它不是一个框架！）。可以说 PyTorch 在深度学习领域中最顺畅地将思想转化为
    Python 代码之一。因此，PyTorch 在研究中得到了广泛的采用，这可以从国际会议上的高引用计数看出。
- en: PyTorch also has a compelling story for the transition from research and development
    into production. While it was initially focused on research workflows, PyTorch
    has been equipped with a high-performance C++ runtime that can be used to deploy
    models for inference without relying on Python, and can be used for designing
    and training models in C++. It has also grown bindings to other languages and
    an interface for deploying to mobile devices. These features allow us to take
    advantage of PyTorch’s flexibility and at the same time take our applications
    where a full Python runtime would be hard to get or would impose expensive overhead.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 在从研究和开发转向生产方面也有引人注目的故事。虽然最初它专注于研究工作流程，但 PyTorch 已经配备了一个高性能的 C++ 运行时，可以用于在不依赖
    Python 的情况下��署推断模型，并且可以用于在 C++ 中设计和训练模型。它还增加了对其他语言的绑定和用于部署到移动设备的接口。这些功能使我们能够利用
    PyTorch 的灵活性，同时将我们的应用程序带到完全无法获得或会带来昂贵开销的完整 Python 运行时的地方。
- en: Of course, claims of ease of use and high performance are trivial to make. We
    hope that by the time you are in the thick of this book, you’ll agree with us
    that our claims here are well founded.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，声称易用性和高性能是微不足道的。我们希望当你深入阅读本书时，你会同意我们在这里的声明是有充分根据的。
- en: 1.3.1 The deep learning competitive landscape
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 深度学习竞争格局
- en: While all analogies are flawed, it seems that the release of PyTorch 0.1 in
    January 2017 marked the transition from a Cambrian-explosion-like proliferation
    of deep learning libraries, wrappers, and data-exchange formats into an era of
    consolidation and unification.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有类比都有缺陷，但似乎 PyTorch 0.1 在 2017 年 1 月的发布标志着从深度学习库、包装器和数据交换格式的富集到整合和统一的时代的转变。
- en: '*Note* The deep learning landscape has been moving so quickly lately that by
    the time you read this in print, it will likely be out of date. If you’re unfamiliar
    with some of the libraries mentioned here, that’s fine.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* 深度学习领域最近发展迅速，到您阅读这篇文章时，它可能已经过时。如果您对这里提到的一些库不熟悉，那没关系。'
- en: 'At the time of PyTorch’s first beta release:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 首个 beta 版本发布时：
- en: Theano and TensorFlow were the premiere low-level libraries, working with a
    model that had the user define a computational graph and then execute it.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theano 和 TensorFlow 是首屈一指的低级库，使用用户定义计算图然后执行它。
- en: Lasagne and Keras were high-level wrappers around Theano, with Keras wrapping
    TensorFlow and CNTK as well.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lasagne和Keras是围绕Theano的高级封装，Keras也封装了TensorFlow和CNTK。
- en: Caffe, Chainer, DyNet, Torch (the Lua-based precursor to PyTorch), MXNet, CNTK,
    DL4J, and others filled various niches in the ecosystem.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caffe、Chainer、DyNet、Torch（PyTorch的Lua前身）、MXNet、CNTK、DL4J等填补了生态系统中的各种领域。
- en: 'In the roughly two years that followed, the landscape changed drastically.
    The community largely consolidated behind either PyTorch or TensorFlow, with the
    adoption of other libraries dwindling, except for those filling specific niches.
    In a nutshell:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的大约两年时间里，情况发生了巨大变化。社区在PyTorch或TensorFlow之间大多数集中，其他库的采用量减少，除了填补特定领域的库。简而言之：
- en: Theano, one of the first deep learning frameworks, has ceased active development.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theano，第一个深度学习框架之一，已经停止了活跃开发。
- en: 'TensorFlow:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow：
- en: Consumed Keras entirely, promoting it to a first-class API
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全消化了Keras，将其提升为一流的API
- en: Provided an immediate-execution “eager mode” that is somewhat similar to how
    PyTorch approaches computation
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供了一个立即执行的“急切模式”，与PyTorch处理计算方式有些相似
- en: Released TF 2.0 with eager mode by default
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布了默认启用急切模式的TF 2.0
- en: JAX, a library by Google that was developed independently from TensorFlow, has
    started gaining traction as a NumPy equivalent with GPU, autograd and JIT capabilities.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX是Google开发的一个独立于TensorFlow的库，已经开始获得与GPU、自动微分和JIT功能相当的NumPy等价物。
- en: 'PyTorch:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch：
- en: Consumed Caffe2 for its backend
  id: totrans-57
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消化了Caffe2作为其后端
- en: Replaced most of the low-level code reused from the Lua-based Torch project
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 替换了大部分从基于Lua的Torch项目中重复使用的低级代码
- en: Added support for ONNX, a vendor-neutral model description and exchange format
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加了对ONNX的支持，这是一个供应商中立的模型描述和交换格式
- en: Added a delayed-execution “graph mode” runtime called *TorchScript*
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加了一个延迟执行的“图模式”运行时称为*TorchScript*
- en: Released version 1.0
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布了1.0版本
- en: Replaced CNTK and Chainer as the framework of choice by their respective corporate
    sponsors
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分别由各自公司的赞助商替换了CNTK和Chainer作为首选框架
- en: TensorFlow has a robust pipeline to production, an extensive industry-wide community,
    and massive mindshare. PyTorch has made huge inroads with the research and teaching
    communities, thanks to its ease of use, and has picked up momentum since, as researchers
    and graduates train students and move to industry. It has also built up steam
    in terms of production solutions. Interestingly, with the advent of TorchScript
    and eager mode, both PyTorch and TensorFlow have seen their feature sets start
    to converge with the other’s, though the presentation of these features and the
    overall experience is still quite different between the two.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow拥有强大的生产流水线、广泛的行业社区和巨大的知名度。PyTorch在研究和教学社区中取得了巨大进展，得益于其易用性，并自那时起一直在增长，因为研究人员和毕业生培训学生并转向工业。它在生产解决方案方面也积累了动力。有趣的是，随着TorchScript和急切模式的出现，PyTorch和TensorFlow的功能集开始收敛，尽管这些功能的展示和整体体验在两者之间仍然有很大的不同。
- en: 1.4 An overview of how PyTorch supports deep learning projects
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 PyTorch如何支持深度学习项目的概述
- en: We have already hinted at a few building blocks in PyTorch. Let’s now take some
    time to formalize a high-level map of the main components that form PyTorch. We
    can best do this by looking at what a deep learning project needs from PyTorch.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经暗示了PyTorch中的一些构建模块。现在让我们花点时间来形式化一个构成PyTorch的主要组件的高级地图。我们最好通过查看深度学习项目从PyTorch中需要什么来做到这一点。
- en: First, PyTorch has the “Py” as in Python, but there’s a lot of non-Python code
    in it. Actually, for performance reasons, most of PyTorch is written in C++ and
    CUDA ([www.geforce.com/hardware/technology/cuda](https://www.geforce.com/hardware/technology/cuda)),
    a C++-like language from NVIDIA that can be compiled to run with massive parallelism
    on GPUs. There are ways to run PyTorch directly from C++, and we’ll look into
    those in chapter 15\. One of the motivations for this capability is to provide
    a reliable strategy for deploying models in production. However, most of the time
    we’ll interact with PyTorch from Python, building models, training them, and using
    the trained models to solve actual problems.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，PyTorch中有“Py”代表Python，但其中有很多非Python代码。实际上，出于性能原因，大部分PyTorch是用C++和CUDA（[www.geforce.com/hardware/technology/cuda](https://www.geforce.com/hardware/technology/cuda)）编写的，CUDA是NVIDIA的一种类似C++的语言，可以编译成在GPU上进行大规模并行运行。有方法可以直接从C++运行PyTorch，我们将在第15章中探讨这些方法。这种能力的一个动机是提供一个可靠的部署模型的策略。然而，大部分时间我们会从Python中与PyTorch交互，构建模型，训练它们，并使用训练好的模型解决实际问题。
- en: Indeed, the Python API is where PyTorch shines in term of usability and integration
    with the wider Python ecosystem. Let’s take a peek at the mental model of what
    PyTorch is.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Python API是PyTorch在可用性和与更广泛的Python生态系统集成方面的亮点。让我们来看看PyTorch是什么样的思维模型。
- en: As we already touched on, at its core, PyTorch is a library that provides *multidimensional
    arrays*, or *tensors* in PyTorch parlance (we’ll go into details on those in chapter
    3), and an extensive library of operations on them, provided by the `torch` module.
    Both tensors and the operations on them can be used on the CPU or the GPU. Moving
    computations from the CPU to the GPU in PyTorch doesn’t require more than an additional
    function call or two. The second core thing that PyTorch provides is the ability
    of tensors to keep track of the operations performed on them and to analytically
    compute derivatives of an output of a computation with respect to any of its inputs.
    This is used for numerical optimization, and it is provided natively by tensors
    by virtue of dispatching through PyTorch’s *autograd* engine under the hood.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经提到的，PyTorch 的核心是一个提供*多维数组*或在 PyTorch 术语中称为*张量*的库（我们将在第 3 章详细介绍），以及由`torch`模块提供的广泛的操作库。张量和对它们的操作都可以在
    CPU 或 GPU 上使用。在 PyTorch 中将计算从 CPU 移动到 GPU 不需要更多的函数调用。PyTorch 提供的第二个核心功能是张量能够跟踪对它们执行的操作，并分析地计算与计算输出相对于任何输入的导数。这用于数值优化，并且通过
    PyTorch 的*autograd*引擎在底层提供。
- en: By having tensors and the autograd-enabled tensor standard library, PyTorch
    can be used for physics, rendering, optimization, simulation, modeling, and more--we’re
    very likely to see PyTorch used in creative ways throughout the spectrum of scientific
    applications. But PyTorch is first and foremost a deep learning library, and as
    such it provides all the building blocks needed to build neural networks and train
    them. Figure 1.2 shows a standard setup that loads data, trains a model, and then
    deploys that model to production.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过具有张量和 autograd 启用的张量标准库，PyTorch 可以用于物理、渲染、优化、模拟、建模等领域--我们很可能会在整个科学应用的范围内看到
    PyTorch 以创造性的方式使用。但 PyTorch 首先是一个深度学习库，因此它提供了构建神经网络和训练它们所需的所有构建模块。图 1.2 显示了一个标准设置，加载数据，训练模型，然后将该模型部署到生产环境。
- en: 'The core PyTorch modules for building neural networks are located in `torch.nn`,
    which provides common neural network layers and other architectural components.
    Fully connected layers, convolutional layers, activation functions, and loss functions
    can all be found here (we’ll go into more detail about what all that means as
    we go through the rest of this book). These components can be used to build and
    initialize the untrained model we see in the center of figure 1.2\. In order to
    train our model, we need a few additional things: a source of training data, an
    optimizer to adapt the model to the training data, and a way to get the model
    and data to the hardware that will actually be performing the calculations needed
    for training the model.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 用于构建神经网络的 PyTorch 核心模块位于`torch.nn`中，它提供常见的神经网络层和其他架构组件。全连接层、卷积层、激活函数和损失函数都可以在这里找到（随着我们在本书的后续部分的深入，我们将更详细地介绍��些内容）。这些组件可以用于构建和初始化我们在图
    1.2 中看到的未经训练的模型。为了训练我们的模型，我们需要一些额外的东西：训练数据的来源，一个优化器来使模型适应训练数据，以及一种将模型和数据传输到实际执行训练模型所需计算的硬件的方法。
- en: '![](../Images/CH01_F02_Stevens2_GS.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F02_Stevens2_GS.png)'
- en: Figure 1.2 Basic, high-level structure of a PyTorch project, with data loading,
    training, and deployment to production
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 PyTorch 项目的基本高级结构，包括数据加载、训练和部署到生产环境
- en: 'At left in figure 1.2, we see that quite a bit of data processing is needed
    before the training data even reaches our model.[⁴](#pgfId-1012633) First we need
    to physically get the data, most often from some sort of storage as the data source.
    Then we need to convert each sample from our data into a something PyTorch can
    actually handle: tensors. This bridge between our custom data (in whatever format
    it might be) and a standardized PyTorch tensor is the `Dataset` class PyTorch
    provides in `torch.utils.data`. As this process is wildly different from one problem
    to the next, we will have to implement this data sourcing ourselves. We will look
    in detail at how to represent various type of data we might want to work with
    as tensors in chapter 4.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 1.2 的左侧，我们看到在训练数据到达我们的模型之前需要进行相当多的数据处理。首先，我们需要从某种存储中获取数据，最常见的是数据源。然后，我们需要将我们的数据中的每个样本转换为
    PyTorch 实际可以处理的东西：张量。我们自定义数据（无论其格式是什么）与标准化的 PyTorch 张量之间的桥梁是 PyTorch 在`torch.utils.data`中提供的`Dataset`类。由于这个过程在不同问题之间差异很大，我们将不得不自己实现这个数据获取过程。我们将详细讨论如何将我们想要处理的各种类型的数据表示为张量在第
    4 章。
- en: 'As data storage is often slow, in particular due to access latency, we want
    to parallelize data loading. But as the many things Python is well loved for do
    not include easy, efficient, parallel processing, we will need multiple processes
    to load our data, in order to assemble them into *batches*: tensors that encompass
    several samples. This is rather elaborate; but as it is also relatively generic,
    PyTorch readily provides all that magic in the `DataLoader` class. Its instances
    can spawn child processes to load data from a dataset in the background so that
    it’s ready and waiting for the training loop as soon as the loop can use it. We
    will meet and use `Dataset` and `DataLoader` in chapter 7.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据存储通常较慢，特别是由于访问延迟，我们希望并行化数据加载。但由于 Python 受欢迎的许多功能并不包括简单、高效的并行处理，我们需要多个进程来加载数据，以便将它们组装成*批次*：包含多个样本的张量。这相当复杂；但由于它也相对通用，PyTorch
    在`DataLoader`类中轻松提供了所有这些魔法。它的实例可以生成子进程，后台加载数据集中的数据，以便在训练循环可以使用时，数据已准备就绪。我们将在第
    7 章中遇到并使用`Dataset`和`DataLoader`。
- en: With the mechanism for getting batches of samples in place, we can turn to the
    training loop itself at the center of figure 1.2\. Typically, the training loop
    is implemented as a standard Python `for` loop. In the simplest case, the model
    runs the required calculations on the local CPU or a single GPU, and once the
    training loop has the data, computation can start immediately. Chances are this
    will be your basic setup, too, and it’s the one we’ll assume in this book.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有了获取样本批次的机制，我们可以转向图1.2中心的训练循环本身。通常，训练循环被实现为标准的Python `for` 循环。在最简单的情况下，模型在本地CPU或单个GPU上运行所需的计算，一旦训练循环有了数据，计算就可以立即开始。很可能这也是您的基本设置，这也是我们在本书中假设的设置。
- en: At each step in the training loop, we evaluate our model on the samples we got
    from the data loader. We then compare the outputs of our model to the desired
    output (the targets) using some *criterion* or *loss function*. Just as it offers
    the components from which to build our model, PyTorch also has a variety of loss
    functions at our disposal. They, too, are provided in `torch.nn`. After we have
    compared our actual outputs to the ideal with the loss functions, we need to push
    the model a little to move its outputs to better resemble the target. As mentioned
    earlier, this is where the PyTorch autograd engine comes in; but we also need
    an *optimizer* doing the updates, and that is what PyTorch offers us in `torch.optim`.
    We will start looking at training loops with loss functions and optimizers in
    chapter 5 and then hone our skills in chapters 6 through 8 before embarking on
    our big project in part 2.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环的每一步中，我们使用从数据加载器中获取的样本评估我们的模型。然后，我们使用一些*标准*或*损失函数*将我们模型的输出与期望输出（目标）进行比较。正如它提供了构建模型的组件一样，PyTorch还提供了各种损失函数供我们使用。它们也是在`torch.nn`中提供的。在我们用损失函数比较了实际输出和理想输出之后，我们需要稍微推动模型，使其输出更好地类似于目标。正如前面提到的，这就是PyTorch自动求导引擎的作用所在；但我们还需要一个*优化器*来进行更新，这就是PyTorch在`torch.optim`中为我们提供的。我们将在第5章开始研究带有损失函数和优化器的训练循环，然后在第6至8章中磨练我们的技能，然后开始我们的大型项目。
- en: It’s increasingly common to use more elaborate hardware like multiple GPUs or
    multiple machines that contribute their resources to training a large model, as
    seen in the bottom center of figure 1.2\. In those cases, `torch.nn.parallel.Distributed-DataParallel`
    and the `torch.distributed` submodule can be employed to use the additional hardware.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越普遍的是使用更复杂的硬件，如多个GPU或多台机器共同为训练大型模型提供资源，如图1.2底部中心所示。在这些情况下，可以使用`torch.nn.parallel.Distributed-DataParallel`和`torch.distributed`子模块来利用额外的硬件。
- en: 'The training loop might be the most unexciting yet most time-consuming part
    of a deep learning project. At the end of it, we are rewarded with a model whose
    parameters have been optimized on our task: the *trained model* depicted to the
    right of the training loop in the figure. Having a model to solve a task is great,
    but in order for it to be useful, we must put it where the work is needed. This
    *deployment* part of the process, depicted on the right in figure 1.2, may involve
    putting the model on a server or exporting it to load it to a cloud engine, as
    shown in the figure. Or we might integrate it with a larger application, or run
    it on a phone.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环可能是深度学习项目中最不令人兴奋但最耗时的部分。在此之后，我们将获得一个在我们的任务上经过优化的模型参数：图中训练循环右侧所示的*训练模型*。拥有一个能解决问题的模型很棒，但为了让它有用，我们必须将其放在需要工作的地方。这个过程的*部署*部分在图1.2右侧描述，可能涉及将模型放在服务器上或将其导出以加载到云引擎中，如图所示。或者我们可以将其集成到更大的应用程序中，或在手机上运行。
- en: One particular step of the deployment exercise can be to export the model. As
    mentioned earlier, PyTorch defaults to an immediate execution model (eager mode).
    Whenever an instruction involving PyTorch is executed by the Python interpreter,
    the corresponding operation is immediately carried out by the underlying C++ or
    CUDA implementation. As more instructions operate on tensors, more operations
    are executed by the backend implementation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 部署练习的一个特定步骤可以是导出模型。如前所述，PyTorch默认为即时执行模式（急切模式）。每当涉及PyTorch的指令被Python解释器执行时，相应的操作立即由底层C++或CUDA实现执行。随着更多指令操作张量，更多操作由后端实现执行。
- en: 'PyTorch also provides a way to compile models ahead of time through *TorchScript*.
    Using TorchScript, PyTorch can serialize a model into a set of instructions that
    can be invoked independently from Python: say, from C++ programs or on mobile
    devices. We can think about it as a virtual machine with a limited instruction
    set, specific to tensor operations. This allows us to export our model, either
    as TorchScript to be used with the PyTorch runtime, or in a standardized format
    called *ONNX*. These features are at the basis of the production deployment capabilities
    of PyTorch. We’ll cover this in chapter 15\.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还提供了一种通过*TorchScript*提前编译模型的方法。使用TorchScript，PyTorch可以将模型序列化为一组指令，可以独立于Python调用：比如，从C++程序或移动设备上。我们可以将其视为具有有限指令集的虚拟机，特定于张量操作。这使我们能够导出我们的模型，无论是作为可与PyTorch运行时一起使用的TorchScript，还是作为一种称为*ONNX*的标准化格式。这些功能是PyTorch生产部署能力的基础。我们将在第15章中介绍这一点。
- en: 1.5 Hardware and software requirements
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 硬件和软件要求
- en: This book will require coding and running tasks that involve heavy numerical
    computing, such as multiplication of large numbers of matrices. As it turns out,
    running a pretrained network on new data is within the capabilities of any recent
    laptop or personal computer. Even taking a pretrained network and retraining a
    small portion of it to specialize it on a new dataset doesn’t necessarily require
    specialized hardware. You can follow along with everything we do in part 1 of
    this book using a standard personal computer or laptop.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将需要编写和运行涉及大量数值计算的任务，例如大量矩阵相乘。事实证明，在新数据上运行预训练网络在任何最近的笔记本电脑或个人电脑上都是可以的。甚至拿一个预训练网络并重新训练其中的一小部分以使其在新数据集上专门化并不一定需要专门的硬件。您可以使用标准个人电脑或笔记本电脑跟随本书第1部分的所有操作。
- en: 'However, we anticipate that completing a full training run for the more advanced
    examples in part 2 will require a CUDA-capable GPU. The default parameters used
    in part 2 assume a GPU with 8 GB of RAM (we suggest an NVIDIA GTX 1070 or better),
    but those can be adjusted if your hardware has less RAM available. To be clear:
    such hardware is not mandatory if you’re willing to wait, but running on a GPU
    cuts training time by at least an order of magnitude (and usually it’s 40-50x
    faster). Taken individually, the operations required to compute parameter updates
    are fast (from fractions of a second to a few seconds) on modern hardware like
    a typical laptop CPU. The issue is that training involves running these operations
    over and over, many, many times, incrementally updating the network parameters
    to minimize the training error.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们预计完成第2部分中更高级示例的完整训练运行将需要一个支持CUDA的GPU。第2部分中使用的默认参数假定具有8 GB RAM的GPU（我们建议使用NVIDIA
    GTX 1070或更高版本），但如果您的硬件可用RAM较少，则可以进行调整。明确一点：如果您愿意等待，这样的硬件并非强制要求，但在GPU上运行可以将训练时间缩短至少一个数量级（通常快40-50倍）。单独看，计算参数更新所需的操作速度很快（从几分之一秒到几秒）在现代硬件上，如典型笔记本电脑CPU。问题在于训练涉及一遍又一遍地运行这些操作，逐渐更新网络参数以最小化训练误差。
- en: Moderately large networks can take hours to days to train from scratch on large,
    real-world datasets on workstations equipped with a good GPU. That time can be
    reduced by using multiple GPUs on the same machine, and even further on clusters
    of machines equipped with multiple GPUs. These setups are less prohibitive to
    access than it sounds, thanks to the offerings of cloud computing providers. DAWNBench
    ([https://dawn.cs.stanford.edu/benchmark/index.html](https://dawn.cs.stanford.edu/benchmark/index.html))
    is an interesting initiative from Stanford University aimed at providing benchmarks
    on training time and cloud computing costs related to common deep learning tasks
    on publicly available datasets.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 中等规模的网络在配备良好GPU的工作站上从头开始训练大型真实世界数据集可能需要几小时到几天的时间。通过在同一台机器上使用多个GPU，甚至在配备多个GPU的机器集群上进一步减少时间。由于云计算提供商的提供，这些设置比听起来的要容易访问。DAWNBench（[https://dawn.cs.stanford.edu/benchmark/index.html](https://dawn.cs.stanford.edu/benchmark/index.html)）是斯坦福大学的一个有趣的倡议，旨在提供关于在公开可用数据集上进行常见深度学习任务的训练时间和云计算成本的基准。
- en: So, if there’s a GPU around by the time you reach part 2, then great. Otherwise,
    we suggest checking out the offerings from the various cloud platforms, many of
    which offer GPU-enabled Jupyter Notebooks with PyTorch preinstalled, often with
    a free quota. Google Colaboratory ([https://colab.research.google.com](https://colab.research.google.com))
    is a great place to start.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果在您到达第2部分时有GPU可用，那太好了。否则，我们建议查看各种云平台的提供，其中许多提供预装PyTorch的支持GPU的Jupyter笔记本，通常还有免费配额。Google
    Colaboratory（[https://colab.research.google.com](https://colab.research.google.com)）是一个很好的起点。
- en: The last consideration is the operating system (OS). PyTorch has supported Linux
    and macOS from its first release, and it gained Windows support in 2018\. Since
    current Apple laptops do not include GPUs that support CUDA, the precompiled macOS
    packages for PyTorch are CPU-only. Throughout the book, we will try to avoid assuming
    you are running a particular OS, although some of the scripts in part 2 are shown
    as if running from a Bash prompt under Linux. Those scripts’ command lines should
    convert to a Windows-compatible form readily. For convenience, code will be listed
    as if running from a Jupyter Notebook when possible.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后考虑的是操作系统（OS）。PyTorch从首次发布开始就支持Linux和macOS，并于2018年获得了Windows支持。由于当前的苹果笔记本不包含支持CUDA的GPU，PyTorch的预编译macOS包仅支持CPU。在本书中，我们会尽量避免假设您正在运行特定的操作系统，尽管第2部分中的一些脚本显示为在Linux下的Bash提示符下运行。这些脚本的命令行应该很容易转换为兼容Windows的形式。为了方便起见，尽可能地，代码将被列为从Jupyter
    Notebook运行时的形式。
- en: For installation information, please see the Get Started guide on the official
    PyTorch website ([https://pytorch.org/get-started/locally](https://pytorch.org/get-started/locally)).
    We suggest that Windows users install with Anaconda or Miniconda ([https://www.anaconda.com/distribution](https://www.anaconda.com/distribution)
    or [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)).
    Other operating systems like Linux typically have a wider variety of workable
    options, with Pip being the most common package manager for Python. We provide
    a requirements.txt file that pip can use to install dependencies. Of course, experienced
    users are free to install packages in the way that is most compatible with your
    preferred development environment.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有关安装信息，请参阅官方PyTorch网站上的入门指南（[https://pytorch.org/get-started/locally](https://pytorch.org/get-started/locally)）。我们建议Windows用户使用Anaconda或Miniconda进行安装（[https://www.anaconda.com/distribution](https://www.anaconda.com/distribution)或[https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)）。像Linux这样的其他操作系统通常有更多可行的选项，Pip是Python最常见的包管理器。我们提供一个requirements.txt文件，pip可以使用它来安装依赖项。当然，��经验的用户可以自由选择最符合您首选开发环境的方式来安装软件包。
- en: Part 2 has some nontrivial download bandwidth and disk space requirements as
    well. The raw data needed for the cancer-detection project in part 2 is about
    60 GB to download, and when uncompressed it requires about 120 GB of space. The
    compressed data can be removed after decompressing it. In addition, due to caching
    some of the data for performance reasons, another 80 GB will be needed while training.
    You will need a total of 200 GB (at minimum) of free disk space on the system
    that will be used for training. While it is possible to use network storage for
    this, there might be training speed penalties if the network access is slower
    than local disk. Preferably you will have space on a local SSD to store the data
    for fast retrieval.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第2部分还有一些不容忽视的下载带宽和磁盘空间要求。第2部分癌症检测项目所需的原始数据约为60 GB，解压后需要约120 GB的空间。解压缩后的数据可以在解压缩后删除。此外，由于为了性能原因缓存了一些数据，训练时还需要另外80
    GB。您需要在用于训练的系统上至少有200 GB的空闲磁盘空间。虽然可以使用网络存储进行此操作，但如果网络访问速度慢于本地磁盘，则可能会导致训练速度下降。最好在本地SSD上有空间存储数据以便快速检索。
- en: 1.5.1 Using Jupyter Notebooks
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.1 使用Jupyter笔记本
- en: 'We’re going to assume you’ve installed PyTorch and the other dependencies and
    have verified that things are working. Earlier we touched on the possibilities
    for following along with the code in the book. We are going to be making heavy
    use of Jupyter Notebooks for our example code. A Jupyter Notebook shows itself
    as a page in the browser through which we can run code interactively. The code
    is evaluated by a *kernel*, a process running on a server that is ready to receive
    code to execute and send back the results, which are then rendered inline on the
    page. A notebook maintains the state of the kernel, like variables defined during
    the evaluation of code, in memory until it is terminated or restarted. The fundamental
    unit with which we interact with a notebook is a *cell* : a box on the page where
    we can type code and have the kernel evaluate it (through the menu item or by
    pressing Shift-Enter). We can add multiple cells in a notebook, and the new cells
    will see the variables we created in the earlier cells. The value returned by
    the last line of a cell will be printed right below the cell after execution,
    and the same goes for plots. By mixing source code, results of evaluations, and
    Markdown-formatted text cells, we can generate beautiful interactive documents.
    You can read everything about Jupyter Notebooks on the project website ([https://jupyter.org](https://jupyter.org)).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您已经安装了PyTorch和其他依赖项，并已验证一切正常。之前我们提到了在书中跟随代码的可能性。我们将大量使用Jupyter笔记本来展示我们的示例代码。Jupyter笔记本显示为浏览器中的页面，通过它我们可以交互式地运行代码。代码由一个*内核*评估，这是在服务器上运行的进程，准备接收要执行的代码并发送结果，然后在页面上内联呈现。笔记本保持内核的状态，例如在评估代码期间定义的变量，直到终止或重新启动。我们与笔记本交互的基本单元是*单元格*：页面上的一个框，我们可以在其中输入代码并让内核评估它（通过菜单项或按Shift-Enter）。我们可以在笔记本中添加多个单元格，新单元格将看到我们在早期单元格中创建的变量。单元格的最后一行返回的值将在执行后直接在单元格下方打印出来，绘图也是如此。通过混合源代码、评估结果和Markdown格式的文本单元格，我们可以生成漂亮的交互式文档。您可以在项目网站上阅读有关Jupyter笔记本的所有内容（[https://jupyter.org](https://jupyter.org)）。
- en: At this point, you need to start the notebook server from the root directory
    of the code checkout from GitHub. How exactly starting the server looks depends
    on the details of your OS and how and where you installed Jupyter. If you have
    questions, feel free to ask on the book’s forum.[⁵](#pgfId-1014892) Once started,
    your default browser will pop up, showing a list of local notebook files.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您需要从GitHub代码检出的根目录启动笔记本服务器。启动服务器的确切方式取决于您的操作系统的细节以及您安装Jupyter的方式和位置。如果您有问题，请随时在书的论坛上提问。[⁵](#pgfId-1014892)
    一旦启动，您的默认浏览器将弹出，显示本地笔记本文件列表。
- en: '*Note* Jupyter Notebooks are a powerful tool for expressing and investigating
    ideas through code. While we think that they make for a good fit for our use case
    with this book, they’re not for everyone. We would argue that it’s important to
    focus on removing friction and minimizing cognitive overhead, and that’s going
    to be different for everyone. Use what you like during your experimentation with
    PyTorch.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* Jupyter Notebooks是通过代码表达和探索想法的强大工具。虽然我们认为它们非常适合本书的用例，但并非人人都适用。我们认为专注于消除摩擦和最小化认知负担很重要，对每个人来说都会有所不同。在使用PyTorch进行实验时，请使用您喜欢的工具。'
- en: Full working code for all listings from the book can be found at the book’s
    website ([www.manning.com/books/deep-learning-with-pytorch](https://www.manning.com/books/deep-learning-with-pytorch))
    and in our repository on GitHub ([https://github.com/deep-learning-with-pytorch/dlwpt-code](https://github.com/deep-learning-with-pytorch/dlwpt-code)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 书中所有示例的完整工作代码可以在书的网站([www.manning.com/books/deep-learning-with-pytorch](https://www.manning.com/books/deep-learning-with-pytorch))和我们在GitHub上的存储库中找到([https://github.com/deep-learning-with-pytorch/dlwpt-code](https://github.com/deep-learning-with-pytorch/dlwpt-code))。
- en: 1.6 Exercises
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 练习
- en: Start Python to get an interactive prompt.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Python以获得交互式提示符。
- en: What Python version are you using? We hope it is at least 3.6!
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您正在使用哪个Python版本？我们希望至少是3.6！
- en: Can you `import torch`? What version of PyTorch do you get?
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您能够`import torch`吗？您得到了哪个PyTorch版本？
- en: What is the result of `torch.cuda.is_available()`? Does it match your expectation
    based on the hardware you’re using?
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch.cuda.is_available()`的结果是什么？它是否符合您基于所使用硬件的期望？'
- en: Start the Jupyter notebook server.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Jupyter笔记本服务器。
- en: What version of Python is Jupyter using?
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jupyter使用的Python版本是多少？
- en: Is the location of the `torch` library used by Jupyter the same as the one you
    imported from the interactive prompt?
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jupyter使用的`torch`库的位置与您从交互式提示符导入的位置相同吗？
- en: 1.7 Summary
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 总结
- en: Deep learning models automatically learn to associate inputs and desired outputs
    from examples.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型会自动从示例中学习将输入和期望输出关联起来。
- en: Libraries like PyTorch allow you to build and train neural network models efficiently.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像PyTorch这样的库允许您高效地构建和训练神经网络模型。
- en: PyTorch minimizes cognitive overhead while focusing on flexibility and speed.
    It also defaults to immediate execution for operations.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch专注于灵活性和速度，同时最大限度地减少认知负担。它还默认立即执行操作。
- en: TorchScript allows us to precompile models and invoke them not only from Python
    but also from C++ programs and on mobile devices.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TorchScript允许我们预编译模型，并不仅可以从Python中调用它们，还可以从C++程序和移动设备中调用。
- en: Since the release of PyTorch in early 2017, the deep learning tooling ecosystem
    has consolidated significantly.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自2017年初发布PyTorch以来，深度学习工具生态系统已经显著巩固。
- en: PyTorch provides a number of utility libraries to facilitate deep learning projects.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch提供了许多实用库，以便促进深度学习项目。
- en: '* * *'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.)Edsger W. Dijkstra, “The Threats to Computing Science,” [http://mng.bz/nPJ5](http://mng.bz/nPJ5).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)Edsger W. Dijkstra，“计算科学的威胁”，[http://mng.bz/nPJ5](http://mng.bz/nPJ5)。
- en: ^(2.)We also recommend [www.arxiv-sanity.com](http://www.arxiv-sanity.com/)
    to help organize research papers of interest.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.)我们还推荐[www.arxiv-sanity.com](http://www.arxiv-sanity.com/)来帮助组织感兴趣的研究论文。
- en: ^(3.)At the International Conference on Learning Representations (ICLR) 2019,
    PyTorch appeared as a citation in 252 papers, up from 87 the previous year and
    at the same level as TensorFlow, which appeared in 266 papers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ^(3.)在2019年的国际学习表示会议（ICLR���上，PyTorch在252篇论文中被引用，比前一年的87篇增加了很多，并且与TensorFlow的水平相同，后者在266篇论文中被引用。
- en: ^(4.)And that’s just the data preparation that is done on the fly, not the preprocessing,
    which can be a pretty large part in practical projects.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ^(4.)这只是在运行时进行的数据准备，而不是预处理，后者在实际项目中可能占据相当大的部分。
- en: ^(5.)[https://forums.manning.com/forums/deep-learning-with-pytorch](https://forums.manning.com/forums/deep-learning-with-pytorch)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ^(5.)[https://forums.manning.com/forums/deep-learning-with-pytorch](https://forums.manning.com/forums/deep-learning-with-pytorch)
