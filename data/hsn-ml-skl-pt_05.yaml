- en: Chapter 4\. Training Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章\. 训练模型
- en: 'So far we have treated machine learning models and their training algorithms
    mostly like black boxes. If you went through some of the exercises in the previous
    chapters, you may have been surprised by how much you can get done without knowing
    anything about what’s under the hood: you optimized a regression system, you improved
    a digit image classifier, and you even built a spam classifier from scratch, all
    without knowing how they actually work. Indeed, in many situations you don’t really
    need to know the implementation details.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要将机器学习模型及其训练算法视为黑盒。如果你完成了前几章的一些练习，你可能会惊讶于你可以在不了解内部机制的情况下完成多少工作：你优化了一个回归系统，改进了一个数字图像分类器，甚至从头开始构建了一个垃圾邮件分类器，所有这些都不需要了解它们实际上是如何工作的。确实，在许多情况下，你并不真的需要了解实现细节。
- en: However, having a good understanding of how things work can help you quickly
    home in on the appropriate model, the right training algorithm to use, and a good
    set of hyperparameters for your task. Understanding what’s under the hood will
    also help you debug issues and perform error analysis more efficiently. Lastly,
    most of the topics discussed in this chapter will be essential in understanding,
    building, and training neural networks (discussed in [Part II](part02.html#neural_nets_part)
    of this book).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对事物工作原理的良好理解可以帮助你快速定位到合适的模型、正确的训练算法以及适合你任务的优秀超参数集。了解内部机制也将帮助你调试问题并更有效地进行错误分析。最后，本章讨论的许多主题对于理解、构建和训练神经网络（本书[第二部分](part02.html#neural_nets_part)中讨论）至关重要。
- en: 'In this chapter we will start by looking at the linear regression model, one
    of the simplest models there is. We will discuss two very different ways to train
    it:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先探讨线性回归模型，这是最简单的模型之一。我们将讨论两种非常不同的训练方法：
- en: Using a “closed-form” equation⁠^([1](ch04.html#id1433)) that directly computes
    the model parameters that best fit the model to the training set (i.e., the model
    parameters that minimize the cost function over the training set).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个“闭式”方程⁠^([1](ch04.html#id1433))，该方程直接计算最佳拟合模型到训练集的模型参数（即最小化训练集上成本函数的模型参数）。
- en: 'Using an iterative optimization approach called gradient descent (GD) that
    gradually tweaks the model parameters to minimize the cost function over the training
    set, eventually converging to the same set of parameters as the first method.
    We will look at a few variants of gradient descent that we will use again and
    again when we study neural networks in [Part II](part02.html#neural_nets_part):
    batch GD, mini-batch GD, and stochastic GD.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一种称为梯度下降（GD）的迭代优化方法，该方法逐渐调整模型参数以最小化训练集上的成本函数，最终收敛到与第一种方法相同的一组参数。当我们研究[第二部分](part02.html#neural_nets_part)中的神经网络时，我们将再次查看几种梯度下降的变体：批量GD、小批量GD和随机GD。
- en: Next we will look at polynomial regression, a more complex model that can fit
    nonlinear datasets. Since this model has more parameters than linear regression,
    it is more prone to overfitting the training data. We will explore how to detect
    whether this is the case using learning curves, and then we will look at several
    regularization techniques that can reduce the risk of overfitting the training
    set.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨多项式回归，这是一个更复杂的模型，可以拟合非线性数据集。由于这个模型比线性回归有更多的参数，因此它更容易对训练数据进行过拟合。我们将通过学习曲线来探讨如何检测这种情况，然后我们将探讨几种可以减少对训练集过拟合风险的正则化技术。
- en: 'Finally, we will examine two more models that are commonly used for classification
    tasks: logistic regression and softmax regression.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将检查两种常用于分类任务的模型：逻辑回归和softmax回归。
- en: Warning
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: There will be quite a few math equations in this chapter using basic concepts
    of linear algebra and calculus. To understand these equations, you need to be
    familiar with vectors and matrices—how to transpose them, multiply them, and invert
    them—as well as partial derivatives. If these concepts are unfamiliar, please
    review the introductory Jupyter notebooks on linear algebra and calculus provided
    in the [online supplemental material](https://github.com/ageron/handson-mlp).
    If you are truly allergic to math, you can just skip the equations; the text should
    still help you grasp most of the concepts. That said, learning the mathematical
    formalism is extremely useful, as it will allow you to read ML papers. Although
    it may seem daunting at first, it’s actually not that hard, and this chapter includes
    code that should help you make sense of the equations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用线性代数和微积分的基本概念，包含相当多的数学方程。为了理解这些方程，你需要熟悉向量和矩阵——如何转置它们、乘以它们以及求逆，以及偏导数。如果这些概念不熟悉，请查阅[在线补充材料](https://github.com/ageron/handson-mlp)中提供的线性代数和微积分的入门
    Jupyter 笔记本。如果你对数学真的过敏，可以跳过这些方程；文本应该仍然能帮助你掌握大部分的概念。尽管一开始可能看起来令人畏惧，但实际上并不难，本章还包括代码，可以帮助你理解这些方程。
- en: Linear Regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: In [Chapter 1](ch01.html#landscape_chapter) we looked at a simple linear model
    of life satisfaction ([Equation 4-1](#life_satisfaction_equation)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 1 章](ch01.html#landscape_chapter)中，我们研究了生命满意度的一个简单线性模型([方程 4-1](#life_satisfaction_equation))。
- en: Equation 4-1\. A simple linear model of life satisfaction
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-1\. 生命满意度的一个简单线性模型
- en: $life normal bar satisfaction equals theta 0 plus theta 1 times GDP normal bar
    per normal bar capita$
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: $life normal bar satisfaction equals theta 0 plus theta 1 times GDP normal bar
    per normal bar capita$
- en: This model is just a linear function of the input feature `GDP_per_capita`.
    *θ*[0] and *θ*[1] are the model’s parameters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型只是输入特征 `GDP_per_capita` 的线性函数。*θ*[0] 和 *θ*[1] 是模型的参数。
- en: More generally, a linear model makes a prediction by simply computing a weighted
    sum of the input features, plus a constant called the *bias term* (also called
    the *intercept term*), as shown in [Equation 4-2](#linear_regression_equation).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，线性模型通过简单地计算输入特征的加权总和，加上一个称为 *偏置项*（也称为 *截距项*）的常数来进行预测，如[方程 4-2](#linear_regression_equation)所示。
- en: Equation 4-2\. Linear regression model prediction
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-2\. 线性回归模型预测
- en: $ModifyingAbove y With caret equals theta 0 plus theta 1 x 1 plus theta 2 x
    2 plus midline-horizontal-ellipsis plus theta Subscript n Baseline x Subscript
    n$
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: $通过上标修改 y 等于 theta 0 加上 theta 1 乘以 x 1 加上 theta 2 乘以 x 2 加上中划线水平省略号加上 theta
    下标 n 基线 x 下标 n$
- en: 'In this equation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*ŷ* is the predicted value.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ŷ* 是预测值。'
- en: '*n* is the number of features.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n* 是特征的数量。'
- en: '*x*[*i*] is the *i*^(th) feature value.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[*i*] 是第 *i* 个特征值。'
- en: '*θ*[*j*] is the *j*^(th) model parameter, including the bias term *θ*[0] and
    the feature weights *θ*[1], *θ*[2], ⋯, *θ*[*n*].'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*θ*[*j*] 是第 *j* 个模型参数，包括偏置项 *θ*[0] 和特征权重 *θ*[1]、*θ*[2]、⋯、*θ*[*n*]。'
- en: This can be written much more concisely using a vectorized form, as shown in
    [Equation 4-3](#linear_regression_prediction_vectorized_equation).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过向量形式更简洁地表示，如[方程 4-3](#linear_regression_prediction_vectorized_equation)所示。
- en: Equation 4-3\. Linear regression model prediction (vectorized form)
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-3\. 线性回归模型预测（向量形式）
- en: <mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msub><mi>h</mi><mi
    mathvariant="bold">θ</mi></msub><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>·</mo><mi mathvariant="bold">x</mi>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msub><mi>h</mi><mi
    mathvariant="bold">θ</mi></msub><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>·</mo><mi mathvariant="bold">x</mi>
- en: 'In this equation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*h*[**θ**] is the hypothesis function, using the model parameters **θ**.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h*[**θ**] 是假设函数，使用模型参数 **θ**。'
- en: '**θ** is the model’s *parameter vector*, containing the bias term *θ*[0] and
    the feature weights *θ*[1] to *θ*[*n*].'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**θ** 是模型的 *参数向量*，包含偏置项 *θ*[0] 和特征权重 *θ*[1] 到 *θ*[*n*]。'
- en: '**x** is the instance’s *feature vector*, containing *x*[0] to *x*[*n*], with
    *x*[0] always equal to 1.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**x** 是实例的 *特征向量*，包含 *x*[0] 到 *x*[*n*]，其中 *x*[0] 总是等于 1。'
- en: '**θ** · **x** is the dot product of the vectors **θ** and **x**, which is equal
    to *θ*[0]*x*[0] + *θ*[1]*x*[1] + *θ*[2]*x*[2] + ... + *θ*[*n*]*x*[*n*].'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**θ** · **x** 是向量 **θ** 和 **x** 的点积，等于 *θ*[0]*x*[0] + *θ*[1]*x*[1] + *θ*[2]*x*[2]
    + ... + *θ*[*n*]*x*[*n*]。'
- en: Note
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注记
- en: In machine learning, vectors are often represented as *column vectors*, which
    are 2D arrays with a single column. If **θ** and **x** are column vectors, then
    the prediction is <mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi>, where
    <msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup> is the *transpose* of **θ**
    (a row vector instead of a column vector) and <msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi
    mathvariant="bold">x</mi> is the matrix multiplication of <msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup>
    and **x**. It is of course the same prediction, except that it is now represented
    as a single-cell matrix rather than a scalar value. In this book I will use this
    notation to avoid switching between dot products and matrix multiplications.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，向量通常表示为 *列向量*，它们是只有一列的二维数组。如果 **θ** 和 **x** 是列向量，那么预测是 <mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi>，其中 <msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup> 是 **θ** 的 *转置*（一个行向量而不是列向量）并且 <msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi> 是 <msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup> 和 **x** 的矩阵乘积。当然，这仍然是相同的预测，只是现在它被表示为一个单元素矩阵而不是标量值。在这本书中，我将使用这种符号来避免在点积和矩阵乘法之间切换。
- en: OK, that’s the linear regression model—but how do we train it? Well, recall
    that training a model means setting its parameters so that the model best fits
    the training set. For this purpose, we first need a measure of how well (or poorly)
    the model fits the training data. In [Chapter 2](ch02.html#project_chapter) we
    saw that the most common performance measure of a regression model is the root
    mean squared error ([Equation 2-1](ch02.html#rmse_equation)). Therefore, to train
    a linear regression model, we need to find the value of **θ** that minimizes the
    RMSE. In practice, it is simpler to minimize the mean squared error (MSE) than
    the RMSE, and it leads to the same result (because the value that minimizes a
    positive function also minimizes its square root).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这就是线性回归模型——但是我们是怎样训练它的呢？记住，训练一个模型意味着设置其参数，使得模型最好地拟合训练集。为此，我们首先需要一个衡量模型拟合训练数据好坏的指标。在
    [第二章](ch02.html#project_chapter) 中，我们了解到回归模型最常见的性能指标是均方根误差（[方程 2-1](ch02.html#rmse_equation)）。因此，为了训练一个线性回归模型，我们需要找到使
    RMSE 最小的 **θ** 的值。在实践中，最小化均方误差（MSE）比最小化 RMSE 更简单，并且它会导致相同的结果（因为最小化正函数的值也会最小化其平方根）。
- en: Warning
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Learning algorithms will often optimize a different loss function during training
    than the performance measure used to evaluate the final model. This is generally
    because the function is easier to optimize and/or because it has extra terms needed
    during training only (e.g., for regularization). A good performance metric is
    as close as possible to the final business objective. A good training loss is
    easy to optimize and strongly correlated with the metric. For example, classifiers
    are often trained using a cost function such as the log loss (as you will see
    later in this chapter) but evaluated using precision/recall. The log loss is easy
    to minimize, and doing so will usually improve precision/recall.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 学习算法在训练过程中通常会优化一个不同于用于评估最终模型的性能度量函数。这通常是因为该函数更容易优化，或者因为它在训练期间需要额外的项（例如，用于正则化）。一个好的性能指标应该尽可能接近最终的商业目标。一个好的训练损失函数易于优化，并且与该指标高度相关。例如，分类器通常使用如对数损失（如你将在本章后面看到）的成本函数进行训练，但使用精确度/召回率进行评估。对数损失易于最小化，这样做通常会提高精确度/召回率。
- en: The MSE of a linear regression hypothesis *h*[**θ**] on a training set **X**
    is calculated using [Equation 4-4](#mse_cost_function).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集 **X** 上，线性回归假设 *h*[**θ**] 的 MSE 是通过 [方程 4-4](#mse_cost_function) 计算的。
- en: Equation 4-4\. MSE cost function for a linear regression model
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-4\. 线性回归模型的均方误差（MSE）成本函数
- en: <mrow><mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi> <mo
    lspace="0%" rspace="0%">,</mo> <mi mathvariant="bold">y</mi> <mo lspace="0%" rspace="0%">,</mo>
    <msub><mi>h</mi> <mi mathvariant="bold">θ</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi> <mo
    lspace="0%" rspace="0%">,</mo> <mi mathvariant="bold">y</mi> <mo lspace="0%" rspace="0%">,</mo>
    <msub><mi>h</mi> <mi mathvariant="bold">θ</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow>
- en: Most of these notations were presented in [Chapter 2](ch02.html#project_chapter)
    (see [“Notations”](ch02.html#notations)). The only difference is that we write
    *h*[**θ**] instead of just *h* to make it clear that the model is parametrized
    by the vector **θ**. To simplify notations, we will just write MSE(**θ**) instead
    of MSE(**X**, *h*[**θ**]).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些符号都在 [第二章](ch02.html#project_chapter) (见 [“符号”](ch02.html#notations)) 中介绍过。唯一的区别是我们写
    *h*[**θ**] 而不是仅仅 *h*，以使其清楚模型是由向量 **θ** 参数化的。为了简化符号，我们将只写 MSE(**θ**) 而不是 MSE(**X**,
    *h*[**θ**])。
- en: The Normal Equation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则方程
- en: To find the value of **θ** that minimizes the MSE, there exists a *closed-form
    solution*—in other words, a mathematical equation that gives the result directly.
    This is called the *normal equation* ([Equation 4-5](#equation_four_four)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到使 MSE 最小的 **θ** 值，存在一个 *封闭形式解*——换句话说，一个直接给出结果的数学方程。这被称为 *正则方程* ([方程 4-5](#equation_four_four))。
- en: Equation 4-5\. Normal equation
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-5\. 正则方程
- en: <mrow><mover accent="true"><mi mathvariant="bold">θ</mi> <mo>^</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">y</mi></mrow>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mover accent="true"><mi mathvariant="bold">θ</mi> <mo>^</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">y</mi></mrow>
- en: 'In this equation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover> is the
    value of **θ** that minimizes the cost function.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover> 是使成本函数最小的
    **θ** 值。
- en: '**y** is the vector of target values containing *y*^((1)) to *y*^((*m*)).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**y** 是包含 *y*^((1)) 到 *y*^((*m*) 的目标值向量。'
- en: 'Let’s generate some linear-looking data to test this equation on ([Figure 4-1](#generated_data_plot)):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一些看起来像线性的数据来测试这个方程 ([图 4-1](#generated_data_plot))：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Scatter plot showing a linear dataset with increasing trend, generated to
    illustrate the application of the normal equation in linear regression.](assets/hmls_0401.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示一个具有递增趋势的线性数据集，用于说明线性回归中正则方程的应用。](assets/hmls_0401.png)'
- en: Figure 4-1\. A randomly generated linear dataset
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 随机生成的线性数据集
- en: 'Now let’s compute <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover>
    using the normal equation. We will use the `inv()` function from NumPy’s linear
    algebra module (`np.linalg`) to compute the inverse of a matrix, and the `@` operator
    for matrix multiplication:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用正则方程来计算 <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover>。我们将使用
    NumPy 线性代数模块 (`np.linalg`) 中的 `inv()` 函数来计算矩阵的逆，以及 `@` 运算符进行矩阵乘法：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `@` operator performs matrix multiplication. If `A` and `B` are NumPy arrays,
    then `A @ B` is equivalent to `np.matmul(A, B)`. Many other libraries, like TensorFlow,
    PyTorch, and JAX, support the `@` operator as well. However, you cannot use `@`
    on pure Python arrays (i.e., lists of lists).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`@` 运算符执行矩阵乘法。如果 `A` 和 `B` 是 NumPy 数组，那么 `A @ B` 等价于 `np.matmul(A, B)`。许多其他库，如
    TensorFlow、PyTorch 和 JAX，也支持 `@` 运算符。然而，你不能在纯 Python 数组（即列表的列表）上使用 `@`。'
- en: 'The function that we used to generate the data is *y* = 4 + 3*x*[1] + Gaussian
    noise. Let’s see what the equation found:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来生成数据的函数是 *y* = 4 + 3*x*[1] + 高斯噪声。让我们看看方程找到了什么：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We would have hoped for *θ*[0] = 4 and *θ*[1] = 3 instead of *θ*[0] = 3.6908
    and *θ*[1] = 3.3296\. Close enough, but the noise made it impossible to recover
    the exact parameters of the original function. The smaller and noisier the dataset,
    the harder it gets.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本希望得到*θ*[0] = 4和*θ*[1] = 3，而不是*θ*[0] = 3.6908和*θ*[1] = 3.3296。已经很接近了，但噪声使得无法恢复原始函数的确切参数。数据集越小、噪声越大，问题就越难解决。
- en: 'Now we can make predictions using <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover>:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用<mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover>进行预测：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s plot this model’s predictions ([Figure 4-2](#linear_model_predictions_plot)):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这个模型的预测结果（[图4-2](#linear_model_predictions_plot)）：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Scatter plot showing data points with a fitted linear regression line representing
    predictions.](assets/hmls_0402.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示数据点，并带有表示预测的拟合线性回归线。](assets/hmls_0402.png)'
- en: Figure 4-2\. Linear regression model predictions
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. 线性回归模型预测
- en: 'Performing linear regression using Scikit-Learn is relatively straightforward:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn进行线性回归相对简单：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Notice that Scikit-Learn separates the bias term (`intercept_`) from the feature
    weights (`coef_`). The `LinearRegression` class is based on the `scipy.linalg.lstsq()`
    function (the name stands for “least squares”), which you could call directly:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到Scikit-Learn将偏差项（`intercept_`）与特征权重（`coef_`）分开。`LinearRegression`类基于`scipy.linalg.lstsq()`函数（其名称代表“最小二乘”），你可以直接调用它：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This function computes <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup><mi mathvariant="bold">y</mi>, where
    <msup><mi mathvariant="bold">X</mi><mo>+</mo></msup> is the *pseudoinverse* of
    **X** (specifically, the Moore–Penrose inverse). You can use `np.linalg.pinv()`
    to compute the pseudoinverse directly:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数计算<mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup><mi mathvariant="bold">y</mi>，其中<msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup>是**X**的伪逆（具体来说，是莫雷-彭罗斯逆）。你可以使用`np.linalg.pinv()`直接计算伪逆：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The pseudoinverse itself is computed using a standard matrix factorization
    technique called *singular value decomposition* (SVD) that can decompose the training
    set matrix **X** into the matrix multiplication of three matrices **U** **Σ**
    **V**^⊺ (see `numpy.linalg.svd()`). The pseudoinverse is computed as <msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup><mo>=</mo><mi mathvariant="bold">V</mi><msup><mi
    mathvariant="bold">Σ</mi><mo>+</mo></msup><msup><mi mathvariant="bold">U</mi><mo>⊺</mo></msup>.
    To compute the matrix <msup><mi mathvariant="bold">Σ</mi><mo>+</mo></msup>, the
    algorithm takes **Σ** and sets to zero all values smaller than a tiny threshold
    value, then it replaces all the nonzero values with their inverse, and finally
    it transposes the resulting matrix. This approach is more efficient than computing
    the normal equation, plus it handles edge cases nicely: indeed, the normal equation
    may not work if the matrix **X**^⊺**X** is not invertible (i.e., singular), such
    as if *m* < *n* or if some features are redundant, but the pseudoinverse is always
    defined.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 伪逆本身是通过一种称为奇异值分解（SVD）的标准矩阵分解技术计算的，可以将训练集矩阵**X**分解为三个矩阵**U** **Σ** **V**^⊺的乘积（参见`numpy.linalg.svd()`）。伪逆计算为<msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup><mo>=</mo><mi mathvariant="bold">V</mi><msup><mi
    mathvariant="bold">Σ</mi><mo>+</mo></msup><msup><mi mathvariant="bold">U</mi><mo>⊺</mo></msup>。为了计算矩阵<msup><mi
    mathvariant="bold">Σ</mi><mo>+</mo></msup>，算法将**Σ**中所有小于一个很小的阈值值的值设置为0，然后将所有非零值替换为其倒数，最后转置得到的矩阵。这种方法比计算正则方程更高效，并且很好地处理了边缘情况：实际上，如果矩阵**X**^⊺**X**不可逆（即奇异的），正则方程可能不起作用（例如，如果*m*
    < *n*或某些特征是冗余的），但伪逆总是定义良好的。
- en: Computational Complexity
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算复杂度
- en: The normal equation computes the inverse of **X**^⊺ **X**, which is an (*n*
    + 1) × (*n* + 1) matrix (where *n* is the number of features). The *computational
    complexity* of inverting such a matrix is typically about *O*(*n*^(2.4)) to *O*(*n*³),
    depending on the implementation. In other words, if you double the number of features,
    you multiply the computation time by roughly 2^(2.4) = 5.3 to 2³ = 8.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正则方程计算**X**^⊺ **X**的逆，这是一个(*n* + 1) × (*n* + 1)的矩阵（其中*n*是特征的数量）。逆矩阵的计算复杂度通常是大约*O*(*n*^(2.4))到*O*(*n*³)，具体取决于实现方式。换句话说，如果你将特征的数量加倍，计算时间大约会增加2^(2.4)
    = 5.3到2³ = 8倍。
- en: The SVD approach used by Scikit-Learn’s `LinearRegression` class is about *O*(*n*²).
    If you double the number of features, you multiply the computation time by roughly
    4.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn的`LinearRegression`类使用的SVD方法是关于*O*(*n*²)的。如果你将特征数量加倍，计算时间大约会增加4倍。
- en: Warning
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Both the normal equation and the SVD approach get very slow when the number
    of features grows large (e.g., 100,000). On the positive side, both are linear
    with regard to the number of instances in the training set (they are *O*(*m*)),
    so they handle large training sets efficiently, provided they can fit in memory.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征数量增长很大时（例如，100,000），正规方程和SVD方法都会变得非常慢。从积极的一面来看，它们与训练集中实例的数量都是线性的（它们是*O*(*m*))，因此它们可以有效地处理大型训练集，前提是它们可以适应内存。
- en: 'Also, once you have trained your linear regression model (using the normal
    equation or any other algorithm), predictions are very fast: the computational
    complexity is linear with regard to both the number of instances you want to make
    predictions on and the number of features. In other words, making predictions
    on twice as many instances (or twice as many features) will take roughly twice
    as much time.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一旦你训练好了你的线性回归模型（使用正规方程或任何其他算法），预测非常快：计算复杂度与你要进行预测的实例数量和特征数量都是线性的。换句话说，对两倍多的实例（或两倍多的特征）进行预测将大约需要两倍的时间。
- en: Now we will look at a very different way to train a linear regression model,
    which is better suited for cases where there are a large number of features or
    too many training instances to fit in memory.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探讨一种非常不同的训练线性回归模型的方法，这种方法更适合于特征数量很多或训练实例太多而无法适应内存的情况。
- en: Gradient Descent
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: '*Gradient descent* is a generic optimization algorithm capable of finding optimal
    solutions to a wide range of problems. The general idea of gradient descent is
    to tweak parameters iteratively in order to minimize a cost function.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度下降*是一种通用的优化算法，能够找到广泛问题的最优解。梯度下降的一般思想是通过迭代调整参数以最小化成本函数。'
- en: 'Suppose you are lost in the mountains in a dense fog, and you can only feel
    the slope of the ground below your feet. A good strategy to get to the bottom
    of the valley quickly is to go downhill in the direction of the steepest slope.
    This is exactly what gradient descent does: it measures the local gradient of
    the error function with regard to the parameter vector **θ**, and it goes in the
    direction of descending gradient. Once the gradient is zero, you have reached
    a minimum!'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在浓雾中迷失在山中，你只能感觉到脚下地面的坡度。一个快速到达山谷底部的好策略是沿着最陡的坡度向下走。这正是梯度下降所做的：它测量参数向量**θ**相对于误差函数的局部梯度，并沿着下降梯度方向前进。一旦梯度为零，你就达到了最小值！
- en: In practice, you start by filling **θ** with random values (this is called *random
    initialization*). Then you improve it gradually, taking one baby step at a time,
    each step attempting to decrease the cost function (e.g., the MSE), until the
    algorithm *converges* to a minimum (see [Figure 4-3](#gradient_descent_diagram)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你首先用随机值填充**θ**（这被称为*随机初始化*）。然后你逐步改进它，每次只迈出一小步，每一步都试图减少成本函数（例如，均方误差），直到算法*收敛*到最小值（见[图4-3](#gradient_descent_diagram)）。
- en: '![Diagram illustrating gradient descent where model parameters adjust in decreasing
    steps towards the minimum of a cost function, demonstrating the effect of a small
    learning rate.](assets/hmls_0403.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图示梯度下降，其中模型参数以递减的步长调整，以接近成本函数的最小值，展示了小学习率的影响。](assets/hmls_0403.png)'
- en: Figure 4-3\. In this depiction of gradient descent, the model parameters are
    initialized randomly and get tweaked repeatedly to minimize the cost function;
    the learning step size is proportional to the slope of the cost function, so the
    steps gradually get smaller as the cost approaches the minimum
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3\. 在这个梯度下降的描述中，模型参数被随机初始化，并反复调整以最小化成本函数；学习步长与成本函数的斜率成正比，因此随着成本接近最小值，步长逐渐减小
- en: An important parameter in gradient descent is the size of the steps, determined
    by the *learning rate* hyperparameter. If the learning rate is too small, then
    the algorithm will have to go through many iterations to converge, which will
    take a long time (see [Figure 4-4](#small_learning_rate_diagram)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降中的一个重要参数是步长的大小，由*学习率*超参数确定。如果学习率太小，那么算法将需要经过许多迭代才能收敛，这将花费很长时间（见[图4-4](#small_learning_rate_diagram)）。
- en: '![Diagram illustrating a gradient descent path with a small learning rate,
    showing slow progression towards the minimum cost.](assets/hmls_0404.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图示具有小学习率的梯度下降路径，显示缓慢向最小成本进展。](assets/hmls_0404.png)'
- en: Figure 4-4\. Learning rate too small
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 学习率过低
- en: On the other hand, if the learning rate is too high, you might jump across the
    valley and end up on the other side, possibly even higher up than you were before.
    This might make the algorithm diverge, with larger and larger values, failing
    to find a good solution (see [Figure 4-5](#large_learning_rate_diagram)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果学习率过高，你可能会跳过山谷，最终落在另一边，甚至可能比之前更高。这可能会导致算法发散，值越来越大，无法找到好的解决方案（参见[图 4-5](#large_learning_rate_diagram)）。
- en: '![Diagram illustrating how a high learning rate can cause gradient descent
    to overshoot the optimal point, resulting in divergence.](assets/hmls_0405.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图示高学习率如何导致梯度下降超过最佳点，从而产生发散。](assets/hmls_0405.png)'
- en: Figure 4-5\. Learning rate too high
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 学习率过高
- en: Additionally, not all cost functions look like nice, regular bowls. There may
    be holes, ridges, plateaus, and all sorts of irregular terrain, making convergence
    to the minimum difficult. [Figure 4-6](#gradient_descent_pitfalls_diagram) shows
    the two main challenges with gradient descent. If the random initialization starts
    the algorithm on the left, then it will converge to a *local minimum*, which is
    not as good as the *global minimum*. If it starts on the right, then it will take
    a very long time to cross the plateau. And if you stop too early, you will never
    reach the global minimum.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，并非所有成本函数看起来都像漂亮的、规则的碗。可能会有洞、脊、平台和各种不规则的地形，使得收敛到最小值变得困难。[图 4-6](#gradient_descent_pitfalls_diagram)
    展示了梯度下降的两个主要挑战。如果随机初始化使算法从左侧开始，那么它将收敛到一个**局部最小值**，这不如**全局最小值**好。如果它从右侧开始，那么它将花费很长时间才能越过平台。如果你过早停止，你将永远无法达到全局最小值。
- en: '![Diagram illustrating gradient descent pitfalls, showing a local minimum trap
    and a plateau that slows progress toward the global minimum.](assets/hmls_0406.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图示梯度下降的陷阱，显示局部最小值陷阱和平坦区域，这会减慢向全局最小值的进展。](assets/hmls_0406.png)'
- en: Figure 4-6\. Gradient descent pitfalls
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 梯度下降陷阱
- en: 'Fortunately, the MSE cost function for a linear regression model happens to
    be a *convex function*, which means that if you pick any two points on the curve,
    the line segment joining them is never below the curve. This implies that there
    are no local minima, just one global minimum. It is also a continuous function
    with a slope that never changes abruptly.⁠^([2](ch04.html#id1475)) These two facts
    have a great consequence: gradient descent is guaranteed to approach arbitrarily
    closely the global minimum (if you wait long enough and if the learning rate is
    not too high).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，线性回归模型的均方误差成本函数恰好是一个**凸函数**，这意味着如果你在曲线上选择任意两点，连接这两点的线段永远不会低于曲线。这表明没有局部最小值，只有一个全局最小值。它也是一个斜率永远不会突然改变的连续函数。⁠^([2](ch04.html#id1475))
    这两个事实具有重大意义：梯度下降保证能够任意接近全局最小值（如果你等待足够长的时间，并且学习率不是太高的话）。
- en: While the cost function has the shape of a bowl, it can be an elongated bowl
    if the features have very different scales. [Figure 4-7](#elongated_bowl_diagram)
    shows gradient descent on a training set where features 1 and 2 have the same
    scale (on the left), and on a training set where feature 1 has much smaller values
    than feature 2 (on the right).⁠^([3](ch04.html#id1476))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当成本函数的形状像一个碗时，如果特征具有非常不同的尺度，它可能是一个拉长的碗。[图 4-7](#elongated_bowl_diagram) 展示了在特征
    1 和 2 尺度相同（左侧）的训练集上的梯度下降，以及在特征 1 的值比特征 2 小得多（右侧）的训练集上的梯度下降。⁠^([3](ch04.html#id1476))
- en: As you can see, on the left the gradient descent algorithm goes straight toward
    the minimum, thereby reaching it quickly, whereas on the right it first goes in
    a direction almost orthogonal to the direction of the global minimum, and it ends
    with a long march down an almost flat valley. It will eventually reach the minimum,
    but it will take a long time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在左侧，梯度下降算法直接朝向最小值，因此快速到达，而在右侧，它首先沿着几乎垂直于全局最小值方向移动，最后在一个几乎平坦的山谷中缓慢下降。它最终会到达最小值，但需要很长时间。
- en: 'This diagram also illustrates the fact that training a model means searching
    for a combination of model parameters that minimizes a cost function (over the
    training set). It is a search in the model’s *parameter space*. The more parameters
    a model has, the more dimensions this space has, and the harder the search is:
    searching for a needle in a 300-dimensional haystack is much trickier than in
    3 dimensions. Fortunately, since the cost function is convex in the case of linear
    regression, the needle is simply at the bottom of the bowl.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此图还说明了训练模型意味着寻找一组模型参数的组合，以最小化成本函数（在训练集上）。这是在模型的*参数空间*中的搜索。模型具有的参数越多，这个空间就有更多的维度，搜索就越困难：在300维的稻草堆中寻找针比在3维中要困难得多。幸运的是，由于线性回归中的成本函数是凸的，针就简单地位于碗底。
- en: '![Diagram showing gradient descent paths on cost function contours; left is
    circular indicating equal feature scaling, right is elongated showing uneven scaling.](assets/hmls_0407.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![显示成本函数轮廓上梯度下降路径的图；左侧为圆形，表示特征缩放相同，右侧为细长形，表示缩放不均匀。](assets/hmls_0407.png)'
- en: Figure 4-7\. Gradient descent with (left) and without (right) feature scaling
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 带有（左侧）和没有（右侧）特征缩放的梯度下降
- en: Warning
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When using gradient descent, you should ensure that all features have a similar
    scale (e.g., using Scikit-Learn’s `StandardScaler` class), or else it will take
    much longer to converge.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用梯度下降时，你应该确保所有特征具有相似的尺度（例如，使用 Scikit-Learn 的 `StandardScaler` 类），否则收敛将需要更长的时间。
- en: Batch Gradient Descent
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: Most models have more than one model parameter. Therefore, to implement gradient
    descent, you need to compute the gradient of the cost function with regard to
    each model parameter *θ*[*j*]. In other words, you need to calculate how much
    the cost function will change if you change *θ*[*j*] just a little bit. This is
    called a *partial derivative*. It is like asking, “What is the slope of the mountain
    toward the east?” and then asking the same question facing north (and so on for
    all other dimensions, if you can imagine a universe with more than three dimensions).
    [Equation 4-6](#mse_partial_derivatives) computes the partial derivative of the
    MSE with regard to parameter *θ*[*j*], denoted ∂ MSE(**θ**) / ∂θ[*j*].
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模型都有多个模型参数。因此，要实现梯度下降，你需要计算每个模型参数 *θ*[*j*] 的成本函数梯度。换句话说，你需要计算如果只稍微改变 *θ*[*j*]，成本函数将如何变化。这被称为*偏导数*。就像问，“向东的山坡斜率是多少？”然后面对北方（如果你能想象一个超过三个维度的宇宙，那么对所有其他维度也是如此）。[方程
    4-6](#mse_partial_derivatives) 计算了 MSE 关于参数 *θ*[*j*] 的偏导数，表示为 ∂ MSE(**θ**) / ∂θ[*j*]。
- en: Equation 4-6\. Partial derivatives of the cost function
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-6\. 成本函数的偏导数
- en: <mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi>
    <mi>j</mi></msub></mrow></mfrac></mstyle> <mtext>MSE</mtext> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>2</mn> <mi>m</mi></mfrac></mstyle> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <mrow><mo>(</mo>
    <msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <msubsup><mi>x</mi>
    <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow>
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi>
    <mi>j</mi></msub></mrow></mfrac></mstyle> <mtext>MSE</mtext> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>2</mn> <mi>m</mi></mfrac></mstyle> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <mrow><mo>(</mo>
    <msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <msubsup><mi>x</mi>
    <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow>
- en: Instead of computing these partial derivatives individually, you can use [Equation
    4-7](#mse_gradient_vector) to compute them all in one go. The gradient vector,
    denoted ∇[**θ**]MSE(**θ**), contains all the partial derivatives of the cost function
    (one for each model parameter).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是单独计算这些偏导数，你可以使用[方程 4-7](#mse_gradient_vector)一次性计算它们。梯度向量，表示为 ∇[**θ**]MSE(**θ**)，包含了成本函数的所有偏导数（每个模型参数一个）。
- en: Equation 4-7\. Gradient vector of the cost function
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-7\. 成本函数的梯度向量
- en: <mrow><msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mtext>MSE</mtext>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mrow><mo
    stretchy="true">(</mo> <mtable><mtr><mtd><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi>
    <mn>0</mn></msub></mrow></mfrac> <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi>
    <mn>1</mn></msub></mrow></mfrac> <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mrow><mfrac><mi>∂</mi>
    <mrow><mi>∂</mi><msub><mi>θ</mi> <mi>n</mi></msub></mrow></mfrac> <mtext>MSE</mtext>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr></mtable>
    <mo stretchy="true">)</mo></mrow> <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>2</mn>
    <mi>m</mi></mfrac></mstyle> <msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup>
    <mrow><mo>(</mo> <mi mathvariant="bold">X</mi> <mi mathvariant="bold">θ</mi> <mo>-</mo>
    <mi mathvariant="bold">y</mi> <mo>)</mo></mrow></mrow>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mtext>MSE</mtext>
    <mrow><mo stretchy="true">(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mrow><mo stretchy="true">(</mo> <mtable><mtr><mtd><mrow><mfrac><mi>∂</mi>
    <mrow><mi>∂</mi><msub><mi>θ</mi> <mn>0</mn></msub></mrow></mfrac> <mtext>MSE</mtext>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi> <mn>1</mn></msub></mrow></mfrac>
    <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi>
    <mi>n</mi></msub></mrow></mfrac> <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable> <mo stretchy="true">)</mo></mrow>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>2</mn> <mi>m</mi></mfrac></mstyle>
    <msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi>
    <mi mathvariant="bold">θ</mi> <mo>-</mo> <mi mathvariant="bold">y</mi> <mo>)</mo></mrow></mrow>
- en: Warning
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Notice that this formula involves calculations over the full training set **X**,
    at each gradient descent step! This is why the algorithm is called *batch gradient
    descent*: it uses the whole batch of training data at every step (actually, *full
    gradient descent* would probably be a better name). As a result, it is terribly
    slow on very large training sets (we will look at some much faster gradient descent
    algorithms shortly). However, gradient descent scales well with the number of
    features; training a linear regression model when there are hundreds of thousands
    of features is much faster using gradient descent than using the normal equation
    or SVD decomposition.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个公式涉及到在整个训练集 **X** 上进行计算，在每个梯度下降步骤中！这就是为什么这个算法被称为 *批量梯度下降*：它在每个步骤中使用整个训练数据批次（实际上，*全梯度下降*可能是一个更好的名字）。因此，它在非常大的训练集上非常慢（我们很快就会看到一些更快的梯度下降算法）。然而，梯度下降在特征数量方面具有良好的扩展性；当有数十万个特征时，使用梯度下降训练线性回归模型比使用正常方程或奇异值分解要快得多。
- en: Once you have the gradient vector, which points uphill, just go in the opposite
    direction to go downhill. This means subtracting ∇[**θ**]MSE(**θ**) from **θ**.
    This is where the learning rate *η* comes into play:⁠^([4](ch04.html#id1483))
    multiply the gradient vector by *η* to determine the size of the downhill step
    ([Equation 4-8](#gradient_descent_step)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了指向上坡的梯度向量，只需朝相反方向走，就可以下山。这意味着从 **θ** 中减去 ∇[**θ**]MSE(θ**)。这就是学习率 *η* 发挥作用的地方：⁠^([4](ch04.html#id1483))
    将梯度向量乘以 *η* 以确定下山步长的大小 ([方程 4-8](#gradient_descent_step))。
- en: Equation 4-8\. Gradient descent step
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-8\. 梯度下降步骤
- en: <msup><mi mathvariant="bold">θ</mi><mrow><mo>(</mo><mtext>next step</mtext><mo>)</mo></mrow></msup><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>-</mo><mi>η</mi><msub><mo>∇</mo><mi mathvariant="bold">θ</mi></msub><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <msup><mi mathvariant="bold">θ</mi><mrow><mo>(</mo><mtext>next step</mtext><mo>)</mo></mrow></msup><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>-</mo><mi>η</mi><msub><mo>∇</mo><mi mathvariant="bold">θ</mi></msub><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo>
- en: 'Let’s look at a quick implementation of this algorithm:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个算法的快速实现：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'That wasn’t too hard! Each iteration over the training set is called an *epoch*.
    Let’s look at the resulting `theta`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不太难！对训练集的每次迭代称为一个 *epoch*。让我们看看结果 `theta`：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Hey, that’s exactly what the normal equation found! Gradient descent worked
    perfectly. But what if you had used a different learning rate (`eta`)? [Figure 4-8](#gradient_descent_plot)
    shows the first 20 steps of gradient descent using three different learning rates.
    The line at the bottom of each plot represents the random starting point, then
    each epoch is represented by a darker and darker line.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿，这正是正常方程发现的东西！梯度下降工作得非常完美。但如果你使用了不同的学习率（`eta`）会怎样呢？[图4-8](#gradient_descent_plot)展示了使用三个不同学习率的梯度下降的前20步。每个图底部的线代表随机的起始点，然后每个迭代由越来越深的线表示。
- en: '![Diagram showing the first 20 steps of gradient descent with learning rates
    of 0.02, 0.1, and 0.5, illustrating slow convergence, optimal convergence, and
    divergence, respectively.](assets/hmls_0408.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![展示使用学习率0.02、0.1和0.5的梯度下降前20步的图表，分别说明慢速收敛、最佳收敛和发散。](assets/hmls_0408.png)'
- en: Figure 4-8\. Gradient descent with various learning rates
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-8\. 使用不同学习率的梯度下降
- en: 'On the left, the learning rate is too low: the algorithm will eventually reach
    the solution, but it will take a long time. In the middle, the learning rate looks
    pretty good: in just a few epochs, it has already converged to the solution. On
    the right, the learning rate is too high: the algorithm diverges, jumping all
    over the place and actually getting further and further away from the solution
    at every step.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在左边，学习率太低：算法最终会达到解，但会花费很长时间。在中间，学习率看起来相当不错：在仅仅几个迭代后，它已经收敛到解。在右边，学习率太高：算法发散，到处跳跃，实际上在每一步都离解越来越远。
- en: To find a good learning rate, you can use grid search (see [Chapter 2](ch02.html#project_chapter)).
    However, you may want to limit the number of epochs so that grid search can eliminate
    models that take too long to converge.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到一个好的学习率，你可以使用网格搜索（参见[第2章](ch02.html#project_chapter)）。然而，你可能想限制迭代的次数，这样网格搜索就可以消除那些收敛速度太慢的模型。
- en: You may wonder how to set the number of epochs. If it is too low, you will still
    be far away from the optimal solution when the algorithm stops; but if it is too
    high, you will waste time while the model parameters do not change anymore. A
    simple solution is to set a very large number of epochs but to interrupt the algorithm
    when the gradient vector becomes tiny—that is, when its norm becomes smaller than
    a tiny number *ε* (called the *tolerance*)—because this happens when gradient
    descent has (almost) reached the minimum.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道如何设置迭代的次数。如果迭代次数太低，当算法停止时，你仍然离最优解很远；但如果迭代次数太高，你会浪费时间，因为模型参数已经不再变化。一个简单的解决方案是设置一个非常大的迭代次数，但在梯度向量变得非常小的时候中断算法——也就是说，当其范数小于一个非常小的数
    *ε*（称为*容忍度*）时——因为这时梯度下降（几乎）达到了最小值。
- en: Stochastic Gradient Descent
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: The main problem with batch gradient descent is the fact that it uses the whole
    training set to compute the gradients at every step, which makes it very slow
    when the training set is large. At the opposite extreme, *stochastic gradient
    descent* picks a random instance in the training set at every step and computes
    the gradients based only on that single instance. Obviously, working on a single
    instance at a time makes the algorithm much faster because it has very little
    data to manipulate at every iteration. It also makes it possible to train on huge
    training sets, since only one instance needs to be in memory at each iteration
    (stochastic GD can be implemented as an out-of-core algorithm; see [Chapter 1](ch01.html#landscape_chapter)).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降的主要问题在于它使用整个训练集在每一步计算梯度，这使得当训练集很大时非常慢。在相反的极端情况下，*随机梯度下降*在每一步从训练集中随机选择一个实例，并仅基于该单个实例计算梯度。显然，一次只处理一个实例使得算法快得多，因为它在每次迭代中要处理的数据非常少。这也使得在巨大的训练集上训练成为可能，因为每次迭代只需要一个实例在内存中（随机GD可以作为一个离核算法实现；参见[第1章](ch01.html#landscape_chapter))。
- en: 'On the other hand, due to its stochastic (i.e., random) nature, this algorithm
    is much less regular than batch gradient descent: instead of gently decreasing
    until it reaches the minimum, the cost function will bounce up and down, decreasing
    only on average. Over time it will end up very close to the minimum, but once
    it gets there it will continue to bounce around, never settling down (see [Figure 4-9](#sgd_random_walk_diagram)).
    Once the algorithm stops, the final parameter values will be good, but not optimal.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于其随机（即随机）性质，此算法比批量梯度下降要少得多：它不会逐渐减小直到达到最小值，损失函数会上下波动，平均而言才会下降。随着时间的推移，它最终会非常接近最小值，但一旦到达那里，它将继续上下波动，永远不会稳定下来（参见[图4-9](#sgd_random_walk_diagram)）。一旦算法停止，最终的参数值将很好，但不是最优的。
- en: When the cost function is very irregular (as in [Figure 4-6](#gradient_descent_pitfalls_diagram)),
    this can actually help the algorithm jump out of local minima, so stochastic gradient
    descent has a better chance of finding the global minimum than batch gradient
    descent does.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当损失函数非常不规则（如图4-6所示）时，这实际上可以帮助算法跳出局部最小值，因此随机梯度下降比批量梯度下降有更好的机会找到全局最小值。
- en: Therefore, randomness is good to escape from local optima, but bad because it
    means that the algorithm can never settle at the minimum. One solution to this
    dilemma is to gradually reduce the learning rate. The steps start out large (which
    helps make quick progress and escape local minima), then get smaller and smaller,
    allowing the algorithm to settle at the global minimum. This process is akin to
    *simulated annealing*, an algorithm inspired by the process in metallurgy of annealing,
    where molten metal is slowly cooled down. The function that determines the learning
    rate at each iteration is called the *learning schedule*. If the learning rate
    is reduced too quickly, you may get stuck in a local minimum, or even end up frozen
    halfway to the minimum. If the learning rate is reduced too slowly, you may jump
    around the minimum for a long time and end up with a suboptimal solution if you
    halt training too early.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随机性有助于逃离局部最优，但也很不好，因为它意味着算法永远无法稳定在最小值。解决这个困境的一个方法是通过逐渐降低学习率。步骤一开始很大（这有助于快速进步并逃离局部最小值），然后越来越小，使算法能够稳定在全局最小值。这个过程类似于*模拟退火*，这是一种受冶金中退火过程启发的算法，其中熔融金属会慢慢冷却。确定每次迭代的
    learning rate 的函数称为*学习计划*。如果学习率降得太快，你可能会陷入局部最小值，甚至可能在中途冻结。如果学习率降得太慢，你可能会在最小值周围跳来跳去很长时间，如果在训练太早停止的情况下，最终得到的解决方案可能不是最优的。
- en: '![Diagram illustrating the path of stochastic gradient descent, showing a random
    walk toward the minimum cost area on a contour plot.](assets/hmls_0409.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图解随机梯度下降的路径，显示在等高线图上向最小成本区域随机行走的路径。](assets/hmls_0409.png)'
- en: Figure 4-9\. With stochastic gradient descent, each training step is much faster
    but also much more stochastic than when using batch gradient descent
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9\. 使用随机梯度下降时，每个训练步骤比使用批量梯度下降要快得多，但同时也更加随机
- en: 'This code implements stochastic gradient descent using a simple learning schedule:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码通过简单的学习计划实现了随机梯度下降：
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'By convention we iterate by rounds of *m* iterations; each round is called
    an *epoch*, as earlier. While the batch gradient descent code iterated 1,000 times
    through the whole training set, this code goes through the training set only 50
    times and reaches a pretty good solution:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，我们通过进行*m*次迭代的轮次来迭代；每一轮称为一个*epoch*，如前所述。当批量梯度下降代码在整个训练集上迭代了1,000次时，此代码只通过训练集50次就得到了一个相当不错的解决方案：
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Figure 4-10](#sgd_plot) shows the first 20 steps of training (notice how irregular
    the steps are).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-10](#sgd_plot)显示了训练的前20步（注意步骤的不规则性）。'
- en: '![Scatter plot showing multiple linear regression lines demonstrating the first
    20 steps of stochastic gradient descent, illustrating how the algorithm converges
    towards optimal parameters.](assets/hmls_0410.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示多个线性回归线，展示了随机梯度下降的前20步，说明了算法如何收敛到最优参数。](assets/hmls_0410.png)'
- en: Figure 4-10\. The first 20 steps of stochastic gradient descent
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-10\. 随机梯度下降的前20步
- en: Note that since instances are picked randomly, some instances may be picked
    several times per epoch, while others may not be picked at all. If you want to
    be sure that the algorithm goes through every instance at each epoch, another
    approach is to shuffle the training set (making sure to shuffle the input features
    and the labels jointly), then go through it instance by instance, then shuffle
    it again, and so on. However, this approach is more complex, and it generally
    does not improve the result.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于实例是随机选择的，一些实例在每个epoch可能被选择多次，而另一些可能一次都没有被选择。如果你想确保算法在每个epoch都遍历每个实例，另一种方法是洗牌训练集（确保同时洗牌输入特征和标签），然后逐个实例遍历，然后再洗牌，依此类推。然而，这种方法更复杂，并且通常不会改善结果。
- en: Warning
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When using stochastic gradient descent, the training instances must be independent
    and identically distributed (IID) to ensure that the parameters get pulled toward
    the global optimum, on average. A simple way to ensure this is to shuffle the
    instances during training (e.g., pick each instance randomly, or shuffle the training
    set at the beginning of each epoch). If you do not shuffle the instances—for example,
    if the instances are sorted by label—then SGD will start by optimizing for one
    label, then the next, and so on, and it will not settle close to the global minimum.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用随机梯度下降时，训练实例必须是独立且同分布的（IID），以确保参数平均趋向于全局最优。确保这一点的一个简单方法是训练期间对实例进行洗牌（例如，随机选择每个实例，或在每个epoch开始时洗牌训练集）。如果你没有洗牌实例——例如，如果实例按标签排序——那么SGD将首先优化一个标签，然后是下一个，依此类推，它将不会接近全局最小值。
- en: 'To perform linear regression using stochastic GD with Scikit-Learn, you can
    use the `SGDRegressor` class, which defaults to optimizing the MSE cost function.
    The following code runs for a maximum of 1,000 epochs (`max_iter`) or until the
    loss drops by less than 10^(–5) (`tol`) during 100 epochs (`n_iter_no_change`).
    It starts with a learning rate of 0.01 (`eta0`), using the default learning schedule
    (different from the one we used). Lastly, it does not use any regularization (`penalty=None`;
    more details on this shortly):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Scikit-Learn进行随机梯度下降的线性回归，你可以使用`SGDRegressor`类，它默认优化均方误差成本函数。以下代码运行最多1,000个epoch（`max_iter`）或直到在100个epoch（`n_iter_no_change`）内损失下降小于10的负五次方（`tol`）。它以0.01的学习率（`eta0`）开始，使用默认的学习计划（与我们使用的不一样）。最后，它不使用任何正则化（`penalty=None`；关于这一点稍后详细说明）：
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once again, you find a solution quite close to the one returned by the normal
    equation:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你发现了一个与正常方程返回的解相当接近的解：
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tip
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'All Scikit-Learn estimators can be trained using the `fit()` method, but some
    estimators also have a `partial_fit()` method that you can call to run a single
    round of training on one or more instances (it ignores hyperparameters like `max_iter`
    or `tol`). Repeatedly calling `partial_fit()` will gradually train the model.
    This is useful when you need more control over the training process. Other models
    have a `warm_start` hyperparameter instead (and some have both): if you set `warm_start=True`,
    calling the `fit()` method on a trained model will not reset the model; it will
    just continue training where it left off, respecting hyperparameters like `max_iter`
    and `tol`. Note that `fit()` resets the iteration counter used by the learning
    schedule, while `partial_fit()` does not.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Scikit-Learn估计器都可以使用`fit()`方法进行训练，但一些估计器还有一个`partial_fit()`方法，你可以调用它来对一个或多个实例运行一轮训练（它忽略了像`max_iter`或`tol`这样的超参数）。重复调用`partial_fit()`将逐渐训练模型。这在需要更多控制训练过程时很有用。其他模型有一个`warm_start`超参数（并且一些模型两者都有）：如果你设置`warm_start=True`，在训练模型上调用`fit()`方法不会重置模型；它将只继续从上次停止的地方训练，尊重像`max_iter`和`tol`这样的超参数。注意，`fit()`重置学习计划使用的迭代计数器，而`partial_fit()`不会。
- en: Mini-Batch Gradient Descent
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: 'The last gradient descent algorithm we will look at is called *mini-batch gradient
    descent*. It is straightforward once you know batch and stochastic gradient descent:
    at each step, instead of computing the gradients based on the full training set
    (as in batch GD) or based on just one instance (as in stochastic GD), mini-batch
    GD computes the gradients on small random sets of instances called *mini-batches*.
    The main advantage of mini-batch GD over stochastic GD is that you can get a performance
    boost from hardware acceleration of matrix operations, especially when using *graphical
    processing units* (GPUs).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的最后一种梯度下降算法被称为 *小批量梯度下降*。一旦你了解了批量和小批量梯度下降，它就很简单：在每一步中，小批量GD不是基于完整训练集（如批量GD）或仅基于一个实例（如随机GD）来计算梯度，而是计算在称为
    *小批量* 的小随机实例集上的梯度。小批量GD相对于随机GD的主要优势是你可以从矩阵操作的硬件加速中获得性能提升，尤其是在使用 *图形处理单元*（GPU）时。
- en: The algorithm’s progress in parameter space is less erratic than with stochastic
    GD, especially with fairly large mini-batches. As a result, mini-batch GD will
    end up walking around a bit closer to the minimum than stochastic GD—but it may
    be harder for it to escape from local minima (in the case of problems that suffer
    from local minima, unlike linear regression with the MSE cost function). [Figure 4-11](#gradient_descent_paths_plot)
    shows the paths taken by the three gradient descent algorithms in parameter space
    during training. They all end up near the minimum, but batch GD’s path actually
    stops at the minimum, while both stochastic GD and mini-batch GD continue to walk
    around. However, don’t forget that batch GD takes a lot of time to take each step,
    and stochastic GD and mini-batch GD would also reach the minimum if you used a
    good learning schedule.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在参数空间中的进展比随机GD更平稳，尤其是在相当大的小批量中。因此，小批量GD最终会在比随机GD更接近最小值的地方徘徊——但它可能更难逃离局部最小值（对于有局部最小值的问题，与具有MSE成本函数的线性回归不同）。[图4-11](#gradient_descent_paths_plot)
    展示了三个梯度下降算法在训练过程中在参数空间中采取的路径。它们最终都接近最小值，但批量GD的路径实际上停止在最小值处，而随机GD和小批量GD则继续徘徊。然而，不要忘记批量GD每次移动都需要花费很多时间，而随机GD和小批量GD如果使用一个好的学习计划也会达到最小值。
- en: '![Diagram showing the paths of stochastic, mini-batch, and batch gradient descent
    algorithms in parameter space, illustrating their different approaches to convergence
    near the minimum.](assets/hmls_0411.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![展示随机、小批量和小批量梯度下降算法在参数空间中路径的图表，说明了它们在最小值附近的收敛方法。](assets/hmls_0411.png)'
- en: Figure 4-11\. Gradient descent paths in parameter space
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-11\. 参数空间中的梯度下降路径
- en: '[Table 4-1](#linear_regression_algorithm_comparison) compares the algorithms
    we’ve discussed so far for linear regression⁠^([5](ch04.html#id1509)) (recall
    that *m* is the number of training instances and *n* is the number of features).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4-1](#linear_regression_algorithm_comparison) 比较了我们迄今为止讨论的线性回归算法（回忆一下，*m*
    是训练实例的数量，*n* 是特征的数量）。'
- en: Table 4-1\. Comparison of algorithms for linear regression
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1\. 线性回归算法比较
- en: '| Algorithm | Large *m* | Out-of-core support | Large *n* | Hyperparams | Scaling
    required | Scikit-Learn |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 大 *m* | 核外支持 | 大 *n* | 超参数 | 缩放要求 | Scikit-Learn |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Normal equation | Fast | No | Slow | 0 | No | N/A |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 正则方程 | 快速 | 否 | 慢 | 0 | 否 | N/A |'
- en: '| SVD | Fast | No | Slow | 0 | No | `LinearRegression` |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| SVD | 快速 | 否 | 慢 | 0 | 否 | `LinearRegression` |'
- en: '| Batch GD | Slow | No | Fast | 2 | Yes | N/A |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 批量GD | 慢 | 否 | 快 | 2 | 是 | N/A |'
- en: '| Stochastic GD | Fast | Yes | Fast | ≥2 | Yes | `SGDRegressor` |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 随机GD | 快速 | 是 | 快速 | ≥2 | 是 | `SGDRegressor` |'
- en: '| Mini-batch GD | Fast | Yes | Fast | ≥2 | Yes | N/A |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 小批量GD | 快速 | 是 | 快速 | ≥2 | 是 | N/A |'
- en: 'There is almost no difference after training: all these algorithms end up with
    very similar models and make predictions in exactly the same way.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后几乎没有差异：所有这些算法最终都得到非常相似的模式，并以完全相同的方式进行预测。
- en: Polynomial Regression
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式回归
- en: What if your data is more complex than a straight line? Surprisingly, you can
    use a linear model to fit nonlinear data. A simple way to do this is to add powers
    of each feature as new features, then train a linear model on this extended set
    of features. This technique is called *polynomial regression*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据比直线更复杂怎么办？令人惊讶的是，你可以使用线性模型来拟合非线性数据。这样做的一个简单方法是将每个特征的幂次作为新特征添加，然后在扩展特征集上训练线性模型。这种技术称为
    *多项式回归*。
- en: 'Let’s look at an example. First, we’ll generate some nonlinear data (see [Figure 4-12](#quadratic_data_plot)),
    based on a simple *quadratic equation*—that’s an equation of the form *y* = *ax*²
    + *bx* + *c*—plus some noise:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。首先，我们将生成一些非线性数据（见[图4-12](#quadratic_data_plot)），基于一个简单的*二次方程*——这是一个形式为*y*
    = *ax*² + *bx* + *c*的方程——以及一些噪声：
- en: '[PRE14]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Scatter plot of a nonlinear and noisy dataset showing a quadratic pattern,
    illustrating why a straight line wouldn''t fit well.](assets/hmls_0412.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![非线性且噪声数据集的散点图显示二次模式，说明为什么直线不会很好地拟合。](assets/hmls_0412.png)'
- en: Figure 4-12\. Generated nonlinear and noisy dataset
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12\. 生成的非线性且噪声数据集
- en: 'Clearly, a straight line will never fit this data properly. So let’s use Scikit-Learn’s
    `PolynomialFeatures` class to transform our training data, adding the square (second-degree
    polynomial) of each feature in the training set as a new feature (in this case
    there is just one feature):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，一条直线永远不会恰当地拟合这些数据。因此，让我们使用Scikit-Learn的`PolynomialFeatures`类来转换我们的训练数据，将训练集中每个特征的平方（二次多项式）作为新特征添加（在这种情况下只有一个特征）：
- en: '[PRE15]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`X_poly` now contains the original feature of `X` plus the square of this feature.
    Now we can fit a `LinearRegression` model to this extended training data ([Figure 4-13](#quadratic_predictions_plot)):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_poly`现在包含原始特征`X`以及该特征的平方。现在我们可以将`LinearRegression`模型拟合到这个扩展的训练数据上（[图4-13](#quadratic_predictions_plot)）：'
- en: '[PRE16]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Scatter plot showing polynomial regression model predictions with a red curve
    fitting the data points, illustrating the relationship between x1 and y.](assets/hmls_0413.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示多项式回归模型预测结果，用红色曲线拟合数据点，说明x1和y之间的关系](assets/hmls_0413.png)'
- en: Figure 4-13\. Polynomial regression model predictions
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-13\. 多项式回归模型预测
- en: 'Not bad: the model estimates <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <mn>0.56</mn> <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>0.93</mn> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <mn>1.78</mn></mrow> when in fact the original function was <mrow><mi>y</mi>
    <mo>=</mo> <mn>0.5</mn> <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>1.0</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <mn>2.0</mn> <mo>+</mo> <mtext>Gaussian noise</mtext></mrow> .'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 不错：模型估计 <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mo>=</mo>
    <mn>0.56</mn> <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mn>0.93</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mn>1.78</mn></mrow>
    当实际上原始函数是 <mrow><mi>y</mi> <mo>=</mo> <mn>0.5</mn> <msup><mrow><msub><mi>x</mi>
    <mn>1</mn></msub></mrow> <mn>2</mn></msup> <mo>+</mo> <mn>1.0</mn> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>2.0</mn> <mo>+</mo> <mtext>高斯噪声</mtext></mrow>
    。
- en: Note that when there are multiple features, polynomial regression is capable
    of finding relationships between features, which is something a plain linear regression
    model cannot do. This is made possible by the fact that `PolynomialFeatures` also
    adds all combinations of features up to the given degree. For example, if there
    were two features *a* and *b*, `PolynomialFeatures` with `degree=3` would not
    only add the features *a*², *a*³, *b*², and *b*³, but also the combinations *ab*,
    *a*²*b*, and *ab*².
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当存在多个特征时，多项式回归能够找到特征之间的关系，这是普通线性回归模型无法做到的。这是由于`PolynomialFeatures`还添加了所有给定阶数的特征组合。例如，如果有两个特征*a*和*b*，`PolynomialFeatures`的`degree=3`不仅会添加特征*a*²、*a*³、*b*²和*b*³，还会添加组合*ab*、*a*²*b*和*ab*²。
- en: Warning
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: '`PolynomialFeatures(degree=*d*)` transforms an array containing *n* features
    into an array containing (*n* + *d*)! / *d*!*n*! features, where *n*! is the *factorial*
    of *n*, equal to 1 × 2 × 3 × ⋯ × *n*. Beware of the combinatorial explosion of
    the number of features!'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`PolynomialFeatures(degree=*d*)`将包含*n*个特征的数组转换为一个包含(*n* + *d*)! / *d*!*n*!个特征的数组，其中*n*!是*n*的阶乘，等于1
    × 2 × 3 × ⋯ × *n*。注意特征数量的组合爆炸！'
- en: Learning Curves
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: If you perform high-degree polynomial regression, you will likely fit the training
    data much better than with plain linear regression. For example, [Figure 4-14](#high_degree_polynomials_plot)
    applies a 300-degree polynomial model to the preceding training data, and compares
    the result with a pure linear model and a quadratic model (second-degree polynomial).
    Notice how the 300-degree polynomial model wiggles around to get as close as possible
    to the training instances.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你进行高阶多项式回归，你可能会比使用普通线性回归更好地拟合训练数据。例如，[图4-14](#high_degree_polynomials_plot)将300阶多项式模型应用于前面的训练数据，并将其与纯线性模型和二次模型（二次多项式）的结果进行比较。注意300阶多项式模型是如何扭曲以尽可能接近训练实例的。
- en: '![Graph comparing linear (1-degree), quadratic (2-degree), and 300-degree polynomial
    regression models, illustrating overfitting with the high-degree polynomial as
    it closely follows the data points.](assets/hmls_0414.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![比较线性（1次方）、二次（2次方）和300次方多项式回归模型的图表，展示了高阶多项式如何紧密跟随数据点，从而说明过拟合。](assets/hmls_0414.png)'
- en: Figure 4-14\. High-degree polynomial regression
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-14\. 高阶多项式回归
- en: This high-degree polynomial regression model is severely overfitting the training
    data, while the linear model is underfitting it. The model that will generalize
    best in this case is the quadratic model, which makes sense because the data was
    generated using a quadratic model. But in general you won’t know what function
    generated the data, so how can you decide how complex your model should be? How
    can you tell that your model is overfitting or underfitting the data?
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个高阶多项式回归模型严重地过拟合了训练数据，而线性模型则欠拟合了它。在这种情况下，表现最好的模型是二次模型，这是有道理的，因为数据是使用二次模型生成的。但通常你不会知道生成数据的函数是什么，那么你如何决定你的模型应该有多复杂？你如何判断你的模型是过拟合还是欠拟合数据？
- en: In [Chapter 2](ch02.html#project_chapter) you used cross-validation to get an
    estimate of a model’s generalization performance. If a model performs well on
    the training data but generalizes poorly according to the cross-validation metrics,
    then your model is overfitting. If it performs poorly on both, then it is underfitting.
    This is one way to tell when a model is too simple or too complex.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.html#project_chapter)中，你使用了交叉验证来估计模型的一般化性能。如果一个模型在训练数据上表现良好，但根据交叉验证的指标泛化性能差，那么你的模型就是过拟合。如果它在两者上都表现不佳，那么它就是欠拟合。这是判断模型是否过于简单或过于复杂的一种方法。
- en: 'Another way to tell is to look at the *learning curves*, which are plots of
    the model’s training error and validation error as a function of the training
    iteration: just evaluate the model at regular intervals during training on both
    the training set and the validation set, and plot the results. If the model cannot
    be trained incrementally (i.e., if it does not support `partial_fit()` or `warm_start`),
    then you must train it several times on gradually larger subsets of the training
    set.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种判断方法是查看*学习曲线*，这是模型训练误差和验证误差作为训练迭代次数函数的图表：只需在训练过程中定期评估训练集和验证集上的模型，并绘制结果。如果模型不能增量训练（即，如果它不支持`partial_fit()`或`warm_start`），那么你必须对训练集的逐渐增大的子集进行多次训练。
- en: 'Scikit-Learn has a useful `learning_curve()` function to help with this: it
    trains and evaluates the model using cross-validation. By default it retrains
    the model on growing subsets of the training set, but if the model supports incremental
    learning you can set `exploit_incremental_learning=True` when calling `learning_curve()`
    and it will train the model incrementally instead. The function returns the training
    set sizes at which it evaluated the model, and the training and validation scores
    it measured for each size and for each cross-validation fold. Let’s use this function
    to look at the learning curves of the plain linear regression model (see [Figure 4-15](#underfitting_learning_curves_plot)):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn有一个有用的`learning_curve()`函数可以帮助你完成这个任务：它使用交叉验证来训练和评估模型。默认情况下，它会在训练集的逐渐增大的子集上重新训练模型，但如果模型支持增量学习，你可以在调用`learning_curve()`时设置`exploit_incremental_learning=True`，这样它就会增量训练模型。该函数返回它在哪些训练集大小上评估了模型，以及它为每个大小和每个交叉验证折计算的训练和验证分数。让我们使用这个函数来查看普通线性回归模型的学习曲线（见图[4-15](#underfitting_learning_curves_plot)）：
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Line graph of learning curves shows root mean square error (RMSE) decreasing
    and plateauing for both training and validation sets as training set size increases,
    indicating underfitting.](assets/hmls_0415.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![学习曲线的线图显示了随着训练集大小的增加，训练集和验证集的均方根误差（RMSE）下降并趋于平稳，这表明欠拟合。](assets/hmls_0415.png)'
- en: Figure 4-15\. Learning curves
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-15\. 学习曲线
- en: This model is underfitting, it’s too simple for the data. How can we tell? Well,
    let’s look at the training error. When there are just one or two instances in
    the training set, the model can fit them perfectly, which is why the curve starts
    at zero. But as new instances are added to the training set, it becomes impossible
    for the model to fit the training data perfectly, both because the data is noisy
    and because it is not linear at all. So the error on the training data goes up
    until it reaches a plateau, at which point adding new instances to the training
    set doesn’t make the average error much better or worse. Now let’s look at the
    validation error. When the model is trained on very few training instances, it
    is incapable of generalizing properly, which is why the validation error is initially
    quite large. Then, as the model is shown more training examples, it learns, and
    thus the validation error slowly goes down. However, once again a straight line
    cannot do a good job of modeling the data, so the error ends up at a plateau,
    very close to the other curve.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型欠拟合，对于数据来说太简单了。我们如何判断呢？好吧，让我们看看训练误差。当训练集中只有一个或两个实例时，模型可以完美地拟合它们，这就是为什么曲线从零开始。但随着新实例被添加到训练集中，模型无法完美地拟合训练数据，这既是因为数据有噪声，也是因为数据根本不是线性的。因此，训练数据上的误差会上升，直到达到一个平台期，此时添加新实例到训练集中并不会使平均误差变得更好或更差。现在让我们看看验证误差。当模型在非常少的训练实例上训练时，它无法正确泛化，这就是为什么验证误差最初相当大。然后，随着模型展示了更多的训练示例，它开始学习，因此验证误差逐渐下降。然而，又一次，直线无法很好地模拟数据，所以误差最终达到一个平台期，非常接近另一条曲线。
- en: These learning curves are typical of a model that’s underfitting. Both curves
    have reached a plateau; they are close and fairly high.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这些学习曲线是欠拟合模型的典型特征。两条曲线都已达到平台期；它们接近且相当高。
- en: Tip
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If your model is underfitting the training data, adding more training examples
    will not help. You need to use a better model or come up with better features.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型欠拟合了训练数据，添加更多的训练示例是没有帮助的。你需要使用更好的模型或提出更好的特征。
- en: 'Now let’s look at the learning curves of a 10th-degree polynomial model on
    the same data ([Figure 4-16](#learning_curves_plot)):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来观察同一数据上10次多项式模型的学习曲线（[图4-16](#learning_curves_plot)）：
- en: '[PRE18]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Learning curves for a 10th-degree polynomial model showing root mean square
    error (RMSE) decreasing with larger training set sizes, with validation error
    stabilizing.](assets/hmls_0416.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![10次多项式模型的学习曲线，显示随着训练集大小的增加，均方根误差（RMSE）降低，验证误差稳定。](assets/hmls_0416.png)'
- en: Figure 4-16\. Learning curves for the 10th-degree polynomial model
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-16。10次多项式模型的学习曲线
- en: 'These learning curves look a bit like the previous ones, but there are two
    very important differences:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这些学习曲线看起来有点像之前的那些，但有两个非常重要的区别：
- en: The error on the training data is much lower than before.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据上的误差比之前低得多。
- en: There is a gap between the curves. This means that the model performs better
    on the training data than on the validation data, which is the hallmark of an
    overfitting model. If you used a much larger training set, however, the two curves
    would continue to get closer.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线之间存在差距。这意味着模型在训练数据上的表现优于在验证数据上的表现，这是过拟合模型的标志。然而，如果你使用了一个更大的训练集，这两条曲线会继续接近。
- en: Tip
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: One way to improve an overfitting model is to feed it more training data until
    the validation error gets close enough to the training error.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 提高过拟合模型的一种方法是为它提供更多的训练数据，直到验证误差接近训练误差。
- en: Regularized Linear Models
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化线性模型
- en: 'As you saw in Chapters [1](ch01.html#landscape_chapter) and [2](ch02.html#project_chapter),
    a good way to reduce overfitting is to regularize the model (i.e., to constrain
    it): the fewer degrees of freedom it has, the harder it will be for it to overfit
    the data. A simple way to regularize a polynomial model is to reduce the number
    of polynomial degrees.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第[1](ch01.html#landscape_chapter)章和第[2](ch02.html#project_chapter)章中看到的，减少过拟合的一个好方法是对模型进行正则化（即约束它）：它拥有的自由度越少，它就越难过拟合数据。正则化多项式模型的一个简单方法就是减少多项式的次数。
- en: 'What about linear models? Can we regularize them too? You may wonder why we
    may want to do that: aren’t linear models constrained enough already? Well, linear
    regression makes a few assumptions, including the fact that the true relationship
    between the inputs and the outputs is linear, the noise has zero mean, constant
    variance, and is independent of the inputs, plus the input matrix has full rank,
    meaning that the inputs are not colinear⁠^([7](ch04.html#id1539)) and there at
    least as many samples as parameters. In practice, some assumptions don’t hold
    perfectly. For example, some inputs may be close to colinear, which makes linear
    regression numerically unstable, meaning that very small differences in the training
    set can have a big impact on the trained model. Regularization can stabilize linear
    models and make them more accurate.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 关于线性模型呢？我们也能对它们进行正则化吗？你可能想知道我们为什么要这样做：线性模型不是已经足够受约束了吗？嗯，线性回归做出了一些假设，包括输入和输出之间真实关系的线性，噪声具有零均值，方差恒定，并且与输入无关，以及输入矩阵具有满秩，这意味着输入不是共线的⁠^([7](ch04.html#id1539))，并且至少有与参数一样多的样本。在实践中，一些假设并不完全成立。例如，一些输入可能接近共线，这使得线性回归数值不稳定，意味着训练集中非常小的差异可能会对训练模型产生重大影响。正则化可以稳定线性模型并使它们更准确。
- en: So how can we regularize a linear model? This is usually done by constraining
    its weights. In this section, we will discuss ridge regression, lasso regression,
    and elastic net regression, which implement three different ways to do that.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何对线性模型进行正则化呢？这通常是通过约束其权重来实现的。在本节中，我们将讨论岭回归、Lasso 回归和弹性网络回归，它们实现了三种不同的正则化方法。
- en: Ridge Regression
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 岭回归
- en: '*Ridge regression* (also called *Tikhonov regularization*) is a regularized
    version of linear regression: a *regularization term* equal to <mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup>
    is added to the MSE. This forces the learning algorithm to not only fit the data
    but also keep the model weights as small as possible. This constraint makes the
    model less flexible, preventing it from stretching itself too much to fit every
    data point: this reduces the risk of overfitting. Note that the regularization
    term should only be added to the cost function during training. Once the model
    is trained, you want to use the unregularized MSE (or the RMSE) to evaluate the
    model’s performance.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*岭回归*（也称为 *Tikhonov 正则化*）是线性回归的正则化版本：将一个等于 <mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup>
    的正则化项添加到均方误差（MSE）中。这迫使学习算法不仅要拟合数据，还要尽可能保持模型权重最小。这个约束使得模型更不灵活，防止它过度拉伸以适应每个数据点：这减少了过拟合的风险。请注意，正则化项应在训练期间添加到成本函数中。一旦模型训练完成，你希望使用未正则化的
    MSE（或 RMSE）来评估模型性能。'
- en: The hyperparameter *α* controls how much you want to regularize the model. If
    *α* = 0, then ridge regression is just linear regression. If *α* is very large,
    then all weights end up very close to zero and the result is a flat line going
    through the data’s mean. [Equation 4-9](#ridge_cost_function) presents the ridge
    regression cost function.⁠^([8](ch04.html#id1544))
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数 *α* 控制你想要对模型进行多少正则化。如果 *α* = 0，则岭回归就是线性回归。如果 *α* 非常大，那么所有权重最终都会非常接近于零，结果是一条穿过数据平均值的平坦线。[方程
    4-9](#ridge_cost_function) 展示了岭回归成本函数。⁠^([8](ch04.html#id1544))
- en: Equation 4-9\. Ridge regression cost function
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-9\. 岭回归成本函数
- en: <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup>
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup>
- en: Note that the bias term *θ*[0] is not regularized (the sum starts at *i* = 1,
    not 0). If we define **w** as the vector of feature weights (*θ*[1] to *θ*[*n*]),
    then the regularization term is equal to *α*(∥**w**∥[2])² / *m*, where ∥**w**∥[2]
    represents the ℓ[2] norm of the weight vector.⁠^([9](ch04.html#id1545)) For batch
    gradient descent, just add 2*α***w** / *m* to the part of the MSE gradient vector
    that corresponds to the feature weights, without adding anything to the gradient
    of the bias term (see [Equation 4-7](#mse_gradient_vector)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，偏差项 *θ*[0] 没有进行正则化（求和从 *i* = 1 开始，而不是 0）。如果我们定义 **w** 为特征权重向量（*θ*[1] 到 *θ*[*n*]），则正则化项等于
    *α*(∥**w**∥[2])² / *m*，其中 ∥**w**∥[2] 表示权重向量的 ℓ[2] 范数。⁠^([9](ch04.html#id1545))
    对于批量梯度下降，只需将 2*α***w** / *m* 添加到对应于特征权重的 MSE 梯度向量部分，而不添加任何内容到偏差项的梯度（参见 [方程 4-7](#mse_gradient_vector)）。
- en: Warning
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It is important to scale the data (e.g., using a `StandardScaler`) before performing
    ridge regression, as it is sensitive to the scale of the input features. This
    is true of most regularized models.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行岭回归之前对数据进行缩放（例如，使用 `StandardScaler`）是很重要的，因为它对输入特征的规模很敏感。大多数正则化模型都如此。
- en: '[Figure 4-18](#ridge_regression_plot) shows several ridge models that were
    trained on some very noisy linear data using different *α* values. On the left,
    plain ridge models are used, leading to linear predictions. On the right, the
    data is first expanded using `PolynomialFeatures(degree=10)`, then it is scaled
    using a `StandardScaler`, and finally the ridge models are applied to the resulting
    features: this is polynomial regression with ridge regularization. Note how increasing
    *α* leads to flatter (i.e., less extreme, more reasonable) predictions, thus reducing
    the model’s variance but increasing its bias.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-18](#ridge_regression_plot) 展示了在非常嘈杂的线性数据上使用不同 *α* 值训练的几个岭回归模型。在左侧，使用了普通的岭回归模型，导致线性预测。在右侧，数据首先使用
    `PolynomialFeatures(degree=10)` 进行扩展，然后使用 `StandardScaler` 进行缩放，最后将岭回归模型应用于生成的特征：这是具有岭回归正则化的多项式回归。注意，随着
    *α* 的增加，预测结果变得更加平缓（即，不那么极端，更合理），从而减少了模型的方差但增加了偏差。'
- en: '![Graphs showing linear and polynomial ridge regression models applied to noisy
    linear data, with increasing alpha values resulting in flatter predictions.](assets/hmls_0418.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![展示线性回归和多项式回归模型应用于噪声线性数据，随着 alpha 值的增加，预测结果变得更加平缓的图表。](assets/hmls_0418.png)'
- en: Figure 4-18\. Linear (left) and polynomial (right) models, both with various
    levels of ridge regularization
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-18\. 线性（左）和多项式（右）模型，都具有不同级别的岭回归正则化
- en: As with linear regression, we can perform ridge regression either by computing
    a closed-form equation or by performing gradient descent. The pros and cons are
    the same. [Equation 4-10](#ridge_regression_solution) shows the closed-form solution,
    where **A** is the (*n* + 1) × (*n* + 1) *identity matrix*,⁠^([10](ch04.html#id1552))
    except with a 0 in the top-left cell, corresponding to the bias term.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归一样，我们可以通过计算闭式方程或执行梯度下降来执行岭回归。优缺点相同。[方程 4-10](#ridge_regression_solution)
    展示了闭式解，其中 **A** 是 (*n* + 1) × (*n* + 1) 的 *单位矩阵*，⁠^([10](ch04.html#id1552)) 除了左上角单元格为
    0，对应于偏差项。
- en: Equation 4-10\. Ridge regression closed-form solution
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-10\. 岭回归闭式解
- en: <mrow><mover accent="true"><mi mathvariant="bold">θ</mi> <mo>^</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">X</mi><mo>+</mo><mi>α</mi><mi mathvariant="bold">A</mi><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">y</mi></mrow>
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mover accent="true"><mi mathvariant="bold">θ</mi> <mo>^</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">X</mi><mo>+</mo><mi>α</mi><mi mathvariant="bold">A</mi><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">y</mi></mrow>
- en: 'Here is how to perform ridge regression with Scikit-Learn using a closed-form
    solution (a variant of [Equation 4-10](#ridge_regression_solution) that uses a
    matrix factorization technique by André-Louis Cholesky):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何使用闭式解（[方程 4-10](#ridge_regression_solution) 的一个变体，使用安德烈-路易斯·高斯-约当矩阵分解技术）在
    Scikit-Learn 中执行岭回归的：
- en: '[PRE19]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: And using stochastic gradient descent:⁠^([11](ch04.html#id1553))
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机梯度下降：⁠^([11](ch04.html#id1553))
- en: '[PRE20]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `penalty` hyperparameter sets the type of regularization term to use. Specifying
    `"l2"` indicates that you want SGD to add a regularization term to the MSE cost
    function equal to `alpha` times the square of the ℓ[2] norm of the weight vector.
    This is just like ridge regression, except there’s no division by *m* in this
    case; that’s why we passed `alpha=0.1 / m`, to get the same result as `Ridge(alpha=0.1)`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`penalty`超参数设置要使用的正则化项的类型。指定`"l2"`表示您希望SGD向MSE损失函数添加一个等于`alpha`乘以权重向量ℓ[2]范数平方的正则化项。这与岭回归类似，只是在这种情况下没有除以*m*；这就是为什么我们传递`alpha=0.1
    / m`，以获得与`Ridge(alpha=0.1)`相同的结果。'
- en: Tip
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `RidgeCV` class also performs ridge regression, but it automatically tunes
    hyperparameters using cross-validation. It’s roughly equivalent to using `GridSearchCV`,
    but it’s optimized for ridge regression and runs *much* faster. Several other
    estimators (mostly linear) also have efficient CV variants, such as `LassoCV`
    and `ElasticNetCV`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`RidgeCV`类也执行岭回归，但它使用交叉验证自动调整超参数。它大致等同于使用`GridSearchCV`，但针对岭回归进行了优化，并且运行速度*快得多*。其他几个估计器（主要是线性）也有高效的CV变体，例如`LassoCV`和`ElasticNetCV`。'
- en: Lasso Regression
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lasso回归
- en: '*Least absolute shrinkage and selection operator regression* (usually simply
    called *lasso regression*) is another regularized version of linear regression:
    just like ridge regression, it adds a regularization term to the cost function,
    but it uses the ℓ[1] norm of the weight vector instead of the square of the ℓ[2]
    norm (see [Equation 4-11](#lasso_cost_function)). Notice that the ℓ[1] norm is
    multiplied by 2*α*, whereas the ℓ[2] norm was multiplied by *α* / *m* in ridge
    regression. These factors were chosen to ensure that the optimal *α* value is
    independent from the training set size: different norms lead to different factors
    (see [Scikit-Learn issue #15657](https://github.com/scikit-learn/scikit-learn/issues/15657)
    for more details).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*最小绝对收缩和选择算子回归*（通常简称为*lasso回归*）是线性回归的另一种正则化版本：就像岭回归一样，它将正则化项添加到损失函数中，但它使用权重向量的ℓ[1]范数而不是ℓ[2]范数的平方（参见[方程4-11](#lasso_cost_function)）。请注意，ℓ[1]范数乘以2*α*，而在岭回归中ℓ[2]范数乘以*α*
    / *m*。这些因素被选择以确保最优的*α*值与训练集大小无关：不同的范数导致不同的因素（有关更多详细信息，请参见[Scikit-Learn问题#15657](https://github.com/scikit-learn/scikit-learn/issues/15657)）。'
- en: Equation 4-11\. Lasso regression cost function
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-11。lasso回归损失函数
- en: <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mrow><mn>2</mn><mi>α</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="|" close="|"><msub><mi>θ</mi><mi>i</mi></msub></mfenced></mrow>
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mrow><mn>2</mn><mi>α</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="|" close="|"><msub><mi>θ</mi><mi>i</mi></msub></mfenced></mrow>
- en: '[Figure 4-19](#lasso_regression_plot) shows the same thing as [Figure 4-18](#ridge_regression_plot)
    but replaces the ridge models with lasso models and uses different *α* values.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-19](#lasso_regression_plot)显示了与[图4-18](#ridge_regression_plot)相同的内容，但用lasso模型替换了岭模型，并使用了不同的*α*值。'
- en: 'An important characteristic of lasso regression is that it tends to eliminate
    the weights of the least important features (i.e., set them to zero). For example,
    the dashed line in the righthand plot in [Figure 4-19](#lasso_regression_plot)
    (with *α* = 0.01) looks roughly cubic: all the weights for the high-degree polynomial
    features are equal to zero. In other words, lasso regression automatically performs
    feature selection and outputs a *sparse model* with few nonzero feature weights.
    Of course, there’s a trade-off: if you increase *α* too much, the model will be
    very sparse, but its performance will plummet.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: lasso回归的一个重要特征是它倾向于消除最不重要的特征（即将其设置为0）。例如，[图4-19](#lasso_regression_plot)中右手边图表中的虚线（*α*
    = 0.01）看起来大致是三次的：所有高次多项式特征的权重都等于0。换句话说，lasso回归自动执行特征选择，并输出一个具有很少非零特征权重的*稀疏模型*。当然，这里有一个权衡：如果您增加*α*太多，模型将非常稀疏，但性能会大幅下降。
- en: '![Plots showing linear (left) and polynomial (right) models using lasso regression
    with different α values, illustrating varying levels of regularization.](assets/hmls_0419.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![展示使用不同α值进行lasso回归的线性（左）和多项式（右）模型的图表](assets/hmls_0419.png)'
- en: Figure 4-19\. Linear (left) and polynomial (right) models, both using various
    levels of lasso regularization
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-19\. 线性（左）和多项式（右）模型，都使用不同级别的lasso正则化
- en: 'You can get a sense of why the ℓ[1] norm induces sparsity by looking at [Figure 4-20](#lasso_vs_ridge_plot):
    the axes represent two model parameters, and the background contours represent
    different loss functions. In the top-left plot, the contours represent the ℓ[1]
    loss (|*θ*[1]| + |*θ*[2]|), which drops linearly as you get closer to any axis.
    For example, if you initialize the model parameters to *θ*[1] = 2 and *θ*[2] =
    0.5, running gradient descent will decrement both parameters equally (as represented
    by the dashed yellow line); therefore *θ*[2] will reach 0 first (since it was
    closer to 0 to begin with). After that, gradient descent will roll down the gutter
    until it reaches *θ*[1] = 0 (with a bit of bouncing around, since the gradients
    of ℓ[1] never get close to 0: they are either –1 or 1 for each parameter). In
    the top-right plot, the contours represent lasso regression’s cost function (i.e.,
    an MSE cost function plus an ℓ[1] loss). The small white circles show the path
    that gradient descent takes to optimize some model parameters that were initialized
    around *θ*[1] = 0.25 and *θ*[2] = –1: notice again how the path quickly reaches
    *θ*[2] = 0, then rolls down the gutter and ends up bouncing around the global
    optimum (represented by the red square). If we increased *α*, the global optimum
    would move left along the dashed yellow line, while if we decreased *α*, the global
    optimum would move right (in this example, the optimal parameters for the unregularized
    MSE are *θ*[1] = 2 and *θ*[2] = 0.5).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过查看[图4-20](#lasso_vs_ridge_plot)来理解为什么ℓ[1]范数会导致稀疏性：轴代表两个模型参数，背景等高线代表不同的损失函数。在左上角的图中，等高线代表ℓ[1]损失（|*θ*[1]|
    + |*θ*[2]|），随着你接近任何轴而线性下降。例如，如果你将模型参数初始化为*θ*[1] = 2和*θ*[2] = 0.5，运行梯度下降将使两个参数等量减少（如虚线黄色线所示）；因此*θ*[2]将首先达到0（因为它一开始就接近0）。之后，梯度下降将沿着沟槽滚下，直到它达到*θ*[1]
    = 0（会有一些弹跳，因为ℓ[1]的梯度永远不会接近0：每个参数的梯度要么是-1，要么是1）。在右上角的图中，等高线代表lasso回归的成本函数（即均方误差成本函数加上ℓ[1]损失）。小白色圆圈显示了梯度下降在优化一些初始化在*θ*[1]
    = 0.25和*θ*[2] = –1附近的模型参数时采取的路径：再次注意路径如何快速达到*θ*[2] = 0，然后滚下沟槽，最终在全局最优（用红色方块表示）周围弹跳。如果我们增加*α*，全局最优将沿着虚线黄色线向左移动，而如果我们减少*α*，全局最优将向右移动（在这个例子中，未正则化的MSE的最佳参数是*θ*[1]
    = 2和*θ*[2] = 0.5）。
- en: The two bottom plots show the same thing but with an ℓ[2] penalty instead. In
    the bottom-left plot, you can see that the ℓ[2] loss decreases as we get closer
    to the origin, so gradient descent just takes a straight path toward that point.
    In the bottom-right plot, the contours represent ridge regression’s cost function
    (i.e., an MSE cost function plus an ℓ[2] loss). As you can see, the gradients
    get smaller as the parameters approach the global optimum, so gradient descent
    naturally slows down. This limits the bouncing around, which helps ridge converge
    faster than lasso regression. Also note that the optimal parameters (represented
    by the red square) get closer and closer to the origin when you increase *α*,
    but they never get eliminated entirely.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 两个底部的图展示了相同的内容，但使用了ℓ[2]惩罚。在左下角的图中，你可以看到ℓ[2]损失随着我们接近原点而减少，因此梯度下降只是沿着一条直线向那个点移动。在右下角的图中，等高线代表岭回归的成本函数（即均方误差成本函数加上ℓ[2]损失）。正如你所见，当参数接近全局最优时，梯度会变小，因此梯度下降自然会减慢。这限制了来回弹跳，有助于岭回归比lasso回归更快地收敛。此外，请注意，当增加*α*时，最佳参数（用红色方块表示）会越来越接近原点，但它们永远不会完全消失。
- en: '![Diagram comparing lasso and ridge regularization techniques, showing the
    paths of gradient descent in ℓ~1~ and ℓ~2~ penalty spaces, and illustrating how
    lasso encourages sparsity by reaching parameter zero faster.](assets/hmls_0420.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![比较lasso和岭回归技术，展示ℓ~1~和ℓ~2~惩罚空间中梯度下降路径，并说明lasso如何通过更快地达到参数零来鼓励稀疏性。](assets/hmls_0420.png)'
- en: Figure 4-20\. Lasso versus ridge regularization
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-20\. Lasso与岭回归正则化比较
- en: Tip
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: To keep gradient descent from bouncing around the optimum at the end when using
    lasso regression, you need to gradually reduce the learning rate during training.
    It will still bounce around the optimum, but the steps will get smaller and smaller,
    so it will converge.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止在使用lasso回归时梯度下降在训练结束时在最优解周围弹跳，你需要逐渐降低学习率。它仍然会在最优解周围弹跳，但步长会越来越小，因此它会收敛。
- en: The lasso cost function is not differentiable at *θ*[*i*] = 0 (for *i* = 1,
    2, ⋯, *n*), but gradient descent still works if you use a *subgradient vector*
    **g**⁠^([12](ch04.html#id1564)) instead when any *θ*[*i*] = 0\. [Equation 4-12](#lasso_subgradient_vector)
    shows a subgradient vector equation you can use for gradient descent with the
    lasso cost function.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *θ*[*i*] = 0 (对于 *i* = 1, 2, ⋯, *n*) 时，lasso 代价函数在 *θ*[*i*] 处不可导，但如果你使用一个
    *subgradient vector* **g**⁠^([12](ch04.html#id1564)) 替代时，梯度下降仍然有效。[式4-12](#lasso_subgradient_vector)
    展示了一个你可以用于使用 lasso 代价函数进行梯度下降的子梯度向量方程。
- en: Equation 4-12\. Lasso regression subgradient vector
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 式4-12\. Lasso 回归子梯度向量
- en: <mrow><mi>g</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><msub><mo>∇</mo><mi
    mathvariant="bold">θ</mi></msub><mtext>MSE</mtext><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mrow><mn>2</mn><mi>α</mi><mfenced><mtable><mtr><mtd><mtext>sign</mtext><mo>(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>)</mo></mtd></mtr><mtr><mtd><mtext>sign</mtext><mo>(</mo><msub><mi>θ</mi><mn>2</mn></msub><mo>)</mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mtext>sign</mtext><mo>(</mo><msub><mi>θ</mi><mi>n</mi></msub><mo>)</mo></mtd></mtr></mtable></mfenced></mrow><mtext>where </mtext><mrow><mtext>sign</mtext><mo>(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>)</mo></mrow><mo>=</mo><mrow><mfenced
    open="{" close=""><mtable><mtr><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo><</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mo>+</mo><mn>1</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo>></mo><mn>0</mn></mtd></mtr></mtable></mfenced></mrow>
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>g</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><msub><mo>∇</mo><mi
    mathvariant="bold">θ</mi></msub><mtext>MSE</mtext><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mrow><mn>2</mn><mi>α</mi><mfenced><mtable><mtr><mtd><mtext>sign</mtext><mo>(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>)</mo></mtd></mtr><mtr><mtd><mtext>sign</mtext><mo>(</mo><msub><mi>θ</mi><mn>2</mn></msub><mo>)</mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mtext>sign</mtext><mo>(</mo><msub><mi>θ</mi><mi>n</mi></msub><mo>)</mo></mtd></mtr></mtable></mfenced></mrow><mtext>其中 
    </mtext><mrow><mtext>sign</mtext><mo>(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>)</mo></mrow><mo>=</mo><mrow><mfenced
    open="{" close=""><mtable><mtr><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mtext>if 
    </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo><</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mtext>if 
    </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mo>+</mo><mn>1</mn></mtd><mtd><mtext>if 
    </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo>></mo><mn>0</mn></mtd></mtr></mtable></mfenced></mrow>
- en: 'Here is a small Scikit-Learn example using the `Lasso` class:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个使用 `Lasso` 类的 Scikit-Learn 小示例：
- en: '[PRE21]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that you could instead use `SGDRegressor(penalty="l1", alpha=0.1)`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你也可以使用 `SGDRegressor(penalty="l1", alpha=0.1)`。
- en: Elastic Net Regression
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性网络回归
- en: '*Elastic net regression* is a middle ground between ridge regression and lasso
    regression. The regularization term is a weighted sum of both ridge and lasso’s
    regularization terms, and you can control the mix ratio *r*. When *r* = 0, elastic
    net is equivalent to ridge regression, and when *r* = 1, it is equivalent to lasso
    regression ([Equation 4-13](#elastic_net_cost_function)).'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*弹性网络回归* 是岭回归和 lasso 回归之间的折中方案。正则化项是岭回归和 lasso 正则化项的加权总和，你可以控制混合比例 *r*。当 *r*
    = 0 时，弹性网络等同于岭回归，当 *r* = 1 时，它等同于 lasso 回归 ([式4-13](#elastic_net_cost_function))。'
- en: Equation 4-13\. Elastic net cost function
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 式4-13\. 弹性网络代价函数
- en: <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mi>r</mi><mfenced><mrow><mn>2</mn><mi>α</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="|" close="|"><msub><mi>θ</mi><mi>i</mi></msub></mfenced></mrow></mfenced><mo>+</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>r</mi><mo>)</mo><mfenced><mrow><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>θ</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfenced>
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mi>r</mi><mfenced><mrow><mn>2</mn><mi>α</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="|" close="|"><msub><mi>θ</mi><mi>i</mi></msub></mfenced></mrow></mfenced><mo>+</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>r</mi><mo>)</mo><mfenced><mrow><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>θ</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfenced>
- en: So when should you use elastic net regression, or ridge, lasso, or plain linear
    regression (i.e., without any regularization)? It is almost always preferable
    to have at least a little bit of regularization, so generally you should avoid
    plain linear regression. Ridge is a good default, but if you suspect that only
    a few features are useful, you should prefer lasso or elastic net because they
    tend to reduce the useless features’ weights down to zero, as discussed earlier.
    In general, elastic net is preferred over lasso because lasso may behave erratically
    when the number of features is greater than the number of training instances or
    when several features are strongly correlated.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你何时应该使用弹性网络回归、岭回归、Lasso回归或普通线性回归（即没有任何正则化）？几乎总是更可取至少有一些正则化，所以通常你应该避免使用普通线性回归。岭回归是一个好的默认选择，但如果你怀疑只有少数特征是有用的，你应该选择Lasso或弹性网络，因为它们倾向于将无用的特征权重降低到零，如前所述。一般来说，弹性网络比Lasso更受欢迎，因为当特征数量大于训练实例数量或当几个特征高度相关时，Lasso可能会表现出不规律的行为。
- en: 'Here is a short example that uses Scikit-Learn’s `ElasticNet` (`l1_ratio` corresponds
    to the mix ratio *r*):'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个使用Scikit-Learn的`ElasticNet`（`l1_ratio`对应于混合比例*r*）的简短示例：
- en: '[PRE22]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Early Stopping
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 早期停止
- en: 'A different way to regularize iterative learning algorithms such as gradient
    descent is to stop training as soon as the validation error reaches a minimum.
    This popular technique is called *early stopping*. [Figure 4-21](#early_stopping_plot)
    shows a complex model (in this case, a high-degree polynomial regression model)
    being trained with batch gradient descent on the quadratic dataset we used earlier.
    As the epochs go by, the algorithm learns, and its prediction error (RMSE) on
    the training set goes down, along with its prediction error on the validation
    set. After a while, though, the validation error stops decreasing and starts to
    go back up. This indicates that the model has started to overfit the training
    data. With early stopping you just stop training as soon as the validation error
    reaches the minimum. It is such a simple and efficient regularization technique
    that Geoffrey Hinton called it a “beautiful free lunch”.⁠^([13](ch04.html#id1576))
    That said, the validation error sometimes comes back down after a while: this
    is called *double descent*. It’s fairly common with large neural networks, and
    is an area of active research.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一种不同的方法来正则化迭代学习算法，如梯度下降，是在验证误差达到最小值时立即停止训练。这种流行的技术被称为**早期停止**。[图4-21](#early_stopping_plot)展示了使用批量梯度下降在之前使用的二次数据集上训练的复杂模型（在这种情况下，是一个高次多项式回归模型）。随着时代的推移，算法学习，其在训练集上的预测误差（RMSE）下降，同时其在验证集上的预测误差也下降。然而，过了一段时间后，验证误差停止下降并开始上升。这表明模型已经开始过拟合训练数据。使用早期停止，你只需在验证误差达到最小值时立即停止训练。这是一个如此简单且高效的正则化技术，以至于杰弗里·辛顿称其为“美丽的免费午餐”。⁠^([13](ch04.html#id1576))
    话虽如此，验证误差有时会在一段时间后再次下降：这被称为**双重下降**。这在大型神经网络中相当常见，是一个活跃的研究领域。
- en: '![Graph illustrating early stopping regularization with RMSE on the y-axis
    and epochs on the x-axis; the validation error is shown to increase after reaching
    the minimum, marked as the "Best model."](assets/hmls_0421.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图示早期停止正则化，y轴为RMSE，x轴为时代；验证误差在达到最小值后增加，标记为“最佳模型”](assets/hmls_0421.png)'
- en: Figure 4-21\. Early stopping regularization
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-21\. 早期停止正则化
- en: Tip
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: With stochastic and mini-batch gradient descent, the curves are not so smooth,
    and it may be hard to know whether you have reached the minimum or not. One solution
    is to stop only after the validation error has been above the minimum for some
    time (when you are confident that the model will not do any better), then roll
    back the model parameters to the point where the validation error was at a minimum.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机和迷你批梯度下降中，曲线并不那么平滑，可能很难知道你是否已经达到了最小值。一个解决方案是在验证误差已经高于最小值一段时间后（当你确信模型不会变得更好时）停止，然后将模型参数回滚到验证误差达到最小值时的点。
- en: 'Here is a basic implementation of early stopping:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是早期停止的基本实现：
- en: '[PRE23]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This code first adds the polynomial features and scales all the input features,
    both for the training set and for the validation set (the code assumes that you
    have split the original training set into a smaller training set and a validation
    set). Then it creates an `SGDRegressor` model with no regularization and a small
    learning rate. In the training loop, it calls `partial_fit()` instead of `fit()`,
    to perform incremental learning. At each epoch, it measures the RMSE on the validation
    set. If it is lower than the lowest RMSE seen so far, it saves a copy of the model
    in the `best_model` variable. This implementation does not actually stop training,
    but it lets you revert to the best model after training. Note that the model is
    copied using `copy.deepcopy()`, because it copies both the model’s hyperparameters
    *and* the learned parameters. In contrast, `sklearn.base.clone()` only copies
    the model’s hyperparameters.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码首先添加多项式特征，并将训练集和验证集的所有输入特征进行缩放（代码假设您已将原始训练集分割成较小的训练集和验证集）。然后，它创建一个没有正则化且学习率较小的`SGDRegressor`模型。在训练循环中，它调用`partial_fit()`而不是`fit()`，以执行增量学习。在每个epoch，它测量验证集上的RMSE。如果它低于迄今为止看到的最低RMSE，它将在`best_model`变量中保存模型的副本。此实现实际上并没有停止训练，但它让您在训练后回退到最佳模型。请注意，使用`copy.deepcopy()`复制模型，因为它复制了模型的超参数和学习的参数。相比之下，`sklearn.base.clone()`只复制模型的超参数。
- en: Logistic Regression
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: As discussed in [Chapter 1](ch01.html#landscape_chapter), some regression algorithms
    can be used for classification (and vice versa). *Logistic regression* (also called
    *logit regression*) is commonly used to estimate the probability that an instance
    belongs to a particular class (e.g., what is the probability that this email is
    spam?). If the estimated probability is greater than a given threshold (typically
    50%), then the model predicts that the instance belongs to that class (called
    the *positive class*, labeled “1”), and otherwise it predicts that it does not
    (i.e., it belongs to the *negative class*, labeled “0”). This makes it a binary
    classifier.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#landscape_chapter)所述，一些回归算法可以用于分类（反之亦然）。*逻辑回归*（也称为*logit回归*）通常用于估计实例属于特定类别的概率（例如，这封电子邮件是垃圾邮件的概率是多少？）。如果估计的概率大于给定的阈值（通常是50%），则模型预测该实例属于该类别（称为*正类*，标记为“1”），否则它预测它不属于（即，它属于*负类*，标记为“0”）。这使得它成为一个二元分类器。
- en: Estimating Probabilities
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 估计概率
- en: So how does logistic regression work? Just like a linear regression model, a
    logistic regression model computes a weighted sum of the input features (plus
    a bias term), but instead of outputting the result directly like the linear regression
    model does, it outputs the *logistic* of this result (see [Equation 4-14](#logisticregression_model_estimated_probability_vectorized_form)).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，逻辑回归是如何工作的呢？就像线性回归模型一样，逻辑回归模型计算输入特征的加权总和（加上一个偏差项），但它不像线性回归模型那样直接输出结果，而是输出这个结果的*逻辑值*（参见[方程式
    4-14](#logisticregression_model_estimated_probability_vectorized_form)）。
- en: Equation 4-14\. Logistic regression model estimated probability (vectorized
    form)
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4-14\. 逻辑回归模型估计概率（向量形式）
- en: <mrow><mover accent="true"><mi>p</mi> <mo>^</mo></mover> <mo>=</mo> <msub><mi>h</mi>
    <mi mathvariant="bold">θ</mi></msub> <mrow><mo>(</mo> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">θ</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">x</mi> <mo>)</mo></mrow></mrow>
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: \[mrow\]\[mover accent="true"\]\[mi\]p\[mo\]^\[mo\]\[msub\]\[mi\]h\[mi mathvariant="bold"\]θ\[msub\]\[mrow\]\[mo\](\[mi
    mathvariant="bold"\]x\[mo\])\[mo\]\[mrow\]\[mo\]=\[mi\]σ\[mrow\]\[mo\](\[msup\]\[mi
    mathvariant="bold"\]θ\[mo\]⊺\[mi mathvariant="bold"\]x\[mo\])\[mo\]\[mrow\]\[mo\]\]
- en: The logistic—denoted *σ*(·)—is a *sigmoid function* (i.e., *S*-shaped) that
    outputs a number between 0 and 1\. It is defined as shown in [Equation 4-15](#equation_four_fourteen)
    and [Figure 4-22](#logistic_function_plot).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数——表示为*σ*(·)—是一个*S型函数*（即，*S*形），输出一个介于0和1之间的数字。它定义如[方程式 4-15](#equation_four_fourteen)和[图
    4-22](#logistic_function_plot)所示。
- en: Equation 4-15\. Logistic function
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4-15\. 逻辑函数
- en: <mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mo
    form="prefix">exp</mo><mo>(</mo><mo lspace="0%" rspace="0%">-</mo><mi>t</mi><mo>)</mo></mrow></mfrac></mstyle></mrow>![Graph
    illustrating the logistic function, showing an S-shaped curve that rises from
    0 to 1 as the input value increases.](assets/hmls_0422.png)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mo
    form="prefix">exp</mo><mo>(</mo><mo lspace="0%" rspace="0%">-</mo><mi>t</mi><mo>)</mo></mrow></mfrac></mstyle></mrow>![说明逻辑函数的图形，显示输入值增加时从
    0 到 1 上升的 S 形曲线。](assets/hmls_0422.png)
- en: Figure 4-22\. Logistic function
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-22\. 逻辑函数
- en: Once the logistic regression model has estimated the probability <mover><mi>p</mi><mo>^</mo></mover>
    = *h*[**θ**](**x**) that an instance **x** belongs to the positive class, it can
    make its prediction *ŷ* easily (see [Equation 4-16](#equation_four_fifteen)).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦逻辑回归模型估计了实例 **x** 属于正类的概率 <mover><mi>p</mi><mo>^</mo></mover> = *h*[**θ**](**x**)，它就可以轻松地做出预测
    *ŷ*（参见 [方程 4-16](#equation_four_fifteen)）。
- en: Equation 4-16\. Logistic regression model prediction using a 50% threshold probability
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-16\. 使用 50% 阈值概率的逻辑回归模型预测
- en: <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mo>=</mo> <mfenced
    separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mo><</mo> <mn>0.5</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mo>≥</mo> <mn>0.5</mn></mrow></mtd></mtr></mtable></mfenced></mrow>
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mo>=</mo> <mfenced
    separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mo><</mo> <mn>0.5</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mo>≥</mo> <mn>0.5</mn></mrow></mtd></mtr></mtable></mfenced></mrow>
- en: Notice that *σ*(*t*) < 0.5 when *t* < 0, and *σ*(*t*) ≥ 0.5 when *t* ≥ 0, so
    a logistic regression model using the default threshold of 50% probability predicts
    1 if **θ**^⊺ **x** is positive and 0 if it is negative.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到当 *t* < 0 时，*σ*(*t*) < 0.5，而当 *t* ≥ 0 时，*σ*(*t*) ≥ 0.5，因此使用默认 50% 概率阈值的逻辑回归模型在
    **θ**^⊺ **x** 为正时预测 1，为负时预测 0。
- en: Note
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The score *t* is often called the *logit*. The name comes from the fact that
    the logit function, defined as logit(*p*) = log(*p* / (1 – *p*)), is the inverse
    of the logistic function. Indeed, if you compute the logit of the estimated probability
    *p*, you will find that the result is *t*. The logit is also called the *log-odds*,
    since it is the log of the ratio between the estimated probability for the positive
    class and the estimated probability for the negative class.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 分数 *t* 通常被称为 *logit*。这个名字来源于 logit 函数，定义为 logit(*p*) = log(*p* / (1 – *p*))，它是逻辑函数的逆函数。确实，如果你计算估计概率
    *p* 的 logit，你会发现结果是 *t*。logit 也被称为 *log-odds*，因为它是在正类估计概率与负类估计概率之间的比值的对数。
- en: Training and Cost Function
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和损失函数
- en: Now you know how a logistic regression model estimates probabilities and makes
    predictions. But how is it trained? The objective of training is to set the parameter
    vector **θ** so that the model estimates high probabilities for positive instances
    (*y* = 1) and low probabilities for negative instances (*y* = 0). This idea is
    captured by the cost function shown in [Equation 4-17](#cost_function_of_a_single_training_instance)
    for a single training instance **x**.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了逻辑回归模型如何估计概率和做出预测。但是它是如何训练的呢？训练的目标是设置参数向量 **θ**，使得模型对正实例（*y* = 1）估计高概率，对负实例（*y*
    = 0）估计低概率。这个想法由 [方程 4-17](#cost_function_of_a_single_training_instance) 中单个训练实例
    **x** 的损失函数所体现。
- en: Equation 4-17\. Cost function of a single training instance
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-17\. 单个训练实例的损失函数
- en: <mrow><mi>c</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mfenced
    open="{" close=""><mtable><mtr><mtd><mo>-</mo><mi>log</mi><mo>(</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>y</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd><mo>-</mo><mi>log</mi><mo>(</mo><mn>1</mn><mo>-</mo><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>y</mi><mo>=</mo><mn>0</mn></mtd></mtr></mtable></mfenced></mrow>
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>c</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mfenced
    open="{" close=""><mtable><mtr><mtd><mo>-</mo><mi>log</mi><mo>(</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>y</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd><mo>-</mo><mi>log</mi><mo>(</mo><mn>1</mn><mo>-</mo><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>y</mi><mo>=</mo><mn>0</mn></mtd></mtr></mtable></mfenced></mrow>
- en: This cost function makes sense because –log(*t*) grows very large when *t* approaches
    0, so the cost will be large if the model estimates a probability close to 0 for
    a positive instance, and it will also be large if the model estimates a probability
    close to 1 for a negative instance. On the other hand, –log(*t*) is close to 0
    when *t* is close to 1, so the cost will be close to 0 if the estimated probability
    is close to 0 for a negative instance or close to 1 for a positive instance, which
    is precisely what we want.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代价函数是有意义的，因为当 *t* 接近 0 时，-log(*t*) 会变得非常大，所以如果模型估计一个正例的概率接近 0，代价就会很大；同样，如果模型估计一个负例的概率接近
    1，代价也会很大。另一方面，当 *t* 接近 1 时，-log(*t*) 接近 0，所以如果估计的概率对于一个负例接近 0 或对于一个正例接近 1，代价就会接近
    0，这正是我们想要的。
- en: The cost function over the whole training set is the average cost over all training
    instances. It can be written in a single expression called the *log loss*, shown
    in [Equation 4-18](#logistic_regression_cost_function).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 整个训练集上的代价函数是所有训练实例的平均代价。它可以写成单个表达式，称为 *对数损失*，如[方程 4-18](#logistic_regression_cost_function)所示。
- en: Equation 4-18\. Logistic regression cost function (log loss)
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-18\. 逻辑回归代价函数（对数损失）
- en: <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mfenced
    open="[" close="]"><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mi>l</mi><mi>o</mi><mi>g</mi><mfenced><msup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced><mo>+</mo><mo>(</mo><mn>1</mn><mo>-</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mfenced><mrow><mn>1</mn><mo>-</mo><msup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfenced></mrow></mfenced></mrow>
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mfenced
    open="[" close="]"><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mi>l</mi><mi>o</mi><mi>g</mi><mfenced><msup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced><mo>+</mo><mo>(</mo><mn>1</mn><mo>-</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mfenced><mrow><mn>1</mn><mo>-</mo><msup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfenced></mrow></mfenced></mrow>
- en: Warning
  id: totrans-286
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The log loss was not just pulled out of a hat. It can be shown mathematically
    (using Bayesian inference) that minimizing this loss will result in the model
    with the *maximum likelihood* of being optimal, assuming that the instances follow
    a Gaussian distribution around the mean of their class. When you use the log loss,
    this is the implicit assumption you are making. The more wrong this assumption
    is, the more biased the model will be. Similarly, when we used the MSE to train
    linear regression models, we were implicitly assuming that the data was purely
    linear, plus some Gaussian noise. So, if the data is not linear (e.g., if it’s
    quadratic) or if the noise is not Gaussian (e.g., if outliers are not exponentially
    rare), then the model will be biased.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失并不是随意提出的。可以通过数学方法（使用贝叶斯推理）证明，最小化这个损失会导致模型具有最大的似然性，假设实例围绕其类别的均值呈高斯分布。当你使用对数损失时，这就是你隐含的假设。这个假设越错误，模型就会越有偏差。同样，当我们使用均方误差（MSE）来训练线性回归模型时，我们隐含地假设数据是纯线性的，加上一些高斯噪声。所以，如果数据不是线性的（例如，如果是二次的）或者噪声不是高斯分布的（例如，如果异常值不是指数级稀少的），那么模型就会存在偏差。
- en: The bad news is that there is no known closed-form equation to compute the value
    of **θ** that minimizes this cost function (there is no equivalent of the normal
    equation). But the good news is that this cost function is convex, so gradient
    descent (or any other optimization algorithm) is guaranteed to find the global
    minimum (if the learning rate is not too large and you wait long enough). The
    partial derivatives of the cost function with regard to the *j*^(th) model parameter
    *θ*[*j*] are given by [Equation 4-19](#logistic_cost_function_partial_derivatives).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 坏消息是，没有已知的闭式方程来计算最小化此成本函数的 **θ** 值（没有正常方程的等价物）。但好消息是，这个成本函数是凸的，所以梯度下降（或任何其他优化算法）保证能够找到全局最小值（如果学习率不是太大，并且等待足够长的时间）。关于第
    *j* 个模型参数 *θ*[*j*] 的成本函数的偏导数由[公式 4-19](#logistic_cost_function_partial_derivatives)给出。
- en: Equation 4-19\. Logistic cost function partial derivatives
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 公式 4-19\. 逻辑成本函数的偏导数
- en: <mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi>
    <mi>j</mi></msub></mrow></mfrac></mstyle> <mtext>J</mtext> <mrow><mo>(</mo> <mi
    mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <mfenced separators=""
    open="(" close=")"><mi>σ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">θ</mi>
    <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>-</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow>
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi>
    <mi>j</mi></msub></mrow></mfrac></mstyle> <mtext>J</mtext> <mrow><mo>(</mo> <mi
    mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <mfenced separators=""
    open="(" close=")"><mi>σ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">θ</mi>
    <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>-</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow>
- en: 'This equation looks very much like [Equation 4-6](#mse_partial_derivatives):
    for each instance it computes the prediction error and multiplies it by the *j*^(th)
    feature value, and then it computes the average over all training instances. Once
    you have the gradient vector containing all the partial derivatives, you can use
    it in the batch gradient descent algorithm. That’s it: you now know how to train
    a logistic regression model. For stochastic GD you would take one instance at
    a time, and for mini-batch GD you would use a mini-batch at a time.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式看起来非常像[公式 4-6](#mse_partial_derivatives)：对于每个实例，它计算预测误差并将其乘以第 *j* 个特征值，然后计算所有训练实例的平均值。一旦你有了包含所有偏导数的梯度向量，你就可以在批量梯度下降算法中使用它。就是这样：你现在知道如何训练一个逻辑回归模型。对于随机梯度下降，你会一次取一个实例，而对于小批量梯度下降，你会一次使用一个小批量。
- en: Decision Boundaries
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策边界
- en: 'We can use the iris dataset to illustrate logistic regression. This is a famous
    dataset that contains the sepal and petal length and width of 150 iris flowers
    of three different species: *Iris setosa*, *Iris versicolor*, and *Iris virginica*
    (see [Figure 4-23](#iris_dataset_diagram)).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用鸢尾花数据集来说明逻辑回归。这是一个著名的包含三种不同物种（*Iris setosa*，*Iris versicolor*，和 *Iris
    virginica*）的 150 朵鸢尾花萼片和花瓣长度和宽度的数据集（见[图 4-23](#iris_dataset_diagram)）。
- en: '![Photographs of three iris species: _Iris virginica_, _Iris versicolor_, and
    _Iris setosa_, demonstrating petal and sepal features.](assets/hmls_0423.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![三种鸢尾花物种的照片：_Iris virginica_，_Iris versicolor_，和 _Iris setosa_，展示了花瓣和萼片的特征。](assets/hmls_0423.png)'
- en: Figure 4-23\. Flowers of three iris plant species⁠^([14](ch04.html#id1611))
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-23\. 三种鸢尾花植物的鲜花⁠^([14](ch04.html#id1611))
- en: 'Let’s try to build a classifier to detect the *Iris virginica* type based only
    on the petal width feature. The first step is to load the data and take a quick
    peek:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试构建一个分类器，仅基于花瓣宽度特征来检测 *Iris virginica* 类型。第一步是加载数据并快速查看：
- en: '[PRE24]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next we’ll split the data and train a logistic regression model on the training
    set:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将分割数据，并在训练集上训练一个逻辑回归模型：
- en: '[PRE25]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let’s look at the model’s estimated probabilities for flowers with petal widths
    varying from 0 cm to 3 cm ([Figure 4-24](#logistic_regression_plot)):⁠^([15](ch04.html#id1612))
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型对花瓣宽度从 0 厘米到 3 厘米的鲜花估计的概率([图 4-24](#logistic_regression_plot))⁠^([15](ch04.html#id1612))
- en: '[PRE26]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Graph showing estimated probabilities of a classifier predicting _Iris virginica_
    versus not _Iris virginica_ based on petal width, with a decision boundary at
    1.6 cm where probabilities are equal.](assets/hmls_0424.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![显示基于瓣宽，决策边界在 1.6 厘米处概率相等的分类器预测 _Iris virginica_ 与非 _Iris virginica_ 的估计概率的图表](assets/hmls_0424.png)'
- en: Figure 4-24\. Estimated probabilities and decision boundary
  id: totrans-303
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-24\. 估计概率和决策边界
- en: 'The petal width of *Iris virginica* flowers (represented as triangles) ranges
    from 1.4 cm to 2.5 cm, while the other iris flowers (represented by squares) generally
    have a smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is
    a bit of overlap. Above about 2 cm the classifier is highly confident that the
    flower is an *Iris virginica* (it outputs a high probability for that class),
    while below 1 cm it is highly confident that it is not an *Iris virginica* (high
    probability for the “Not Iris virginica” class). In between these extremes, the
    classifier is unsure. However, if you ask it to predict the class (using the `predict()`
    method rather than the `predict_proba()` method), it will return whichever class
    is the most likely. Therefore, there is a *decision boundary* at around 1.6 cm
    where both probabilities are equal to 50%: if the petal width is greater than
    1.6 cm the classifier will predict that the flower is an *Iris virginica*, and
    otherwise it will predict that it is not (even if it is not very confident):'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 爱丽丝·弗吉尼亚（Iris virginica）花的瓣宽（以三角形表示）从 1.4 厘米到 2.5 厘米不等，而其他鸢尾花（以正方形表示）的瓣宽通常较小，范围从
    0.1 厘米到 1.8 厘米。请注意，这里有一些重叠。当瓣宽超过 2 厘米时，分类器高度确信这朵花是 *Iris virginica*（为该类别输出高概率），而当瓣宽低于
    1 厘米时，它高度确信这朵花不是 *Iris virginica*（“非 Iris virginica”类别的概率高）。在这两个极端之间，分类器不确定。然而，如果你要求它预测类别（使用
    `predict()` 方法而不是 `predict_proba()` 方法），它将返回最可能的类别。因此，在约 1.6 厘米处存在一个 *决策边界*，此时两个概率都等于
    50%：如果瓣宽大于 1.6 厘米，分类器将预测这朵花是 *Iris virginica*，否则它将预测它不是（即使它不太确定）：
- en: '[PRE27]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Figure 4-25](#logistic_regression_contour_plot) shows the same dataset, but
    this time displaying two features: petal width and length. Once trained, the logistic
    regression classifier can, based on these two features, estimate the probability
    that a new flower is an *Iris virginica*. The dashed line represents the points
    where the model estimates a 50% probability: this is the model’s decision boundary.
    Note that it is a linear boundary.⁠^([16](ch04.html#id1613)) Each parallel line
    represents the points where the model outputs a specific probability, from 15%
    (bottom left) to 90% (top right). All the flowers beyond the top-right line have
    over a 90% chance of being *Iris virginica*, according to the model.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-25](#logistic_regression_contour_plot) 显示了相同的数据库，但这次显示两个特征：瓣宽和长度。一旦训练，逻辑回归分类器可以根据这两个特征估计一朵新花是
    *Iris virginica* 的概率。虚线表示模型估计 50% 概率的点：这是模型的决策边界。请注意，它是一个线性边界。⁠^([16](ch04.html#id1613))
    每条平行线代表模型输出特定概率的点，从左下角的 15% 到右上角的 90%。根据模型，所有超出右上角线的花朵有超过 90% 的可能性是 *Iris virginica*。'
- en: '![Contour plot of logistic regression showing decision boundary and probability
    lines for classifying Iris virginica based on petal width and length.](assets/hmls_0425.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归轮廓图显示基于瓣宽和长度的决策边界和概率线，用于分类 Iris virginica。](assets/hmls_0425.png)'
- en: Figure 4-25\. Linear decision boundary
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-25\. 线性决策边界
- en: Note
  id: totrans-309
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The hyperparameter controlling the regularization strength of a Scikit-Learn
    `LogisticRegression` model is not `alpha` (as in other linear models), but its
    inverse: `C`. The higher the value of `C`, the *less* the model is regularized.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 控制 Scikit-Learn `LogisticRegression` 模型正则化强度的超参数不是 `alpha`（如其他线性模型），而是其倒数：`C`。`C`
    的值越高，模型的正则化程度就越低。
- en: Just like the other linear models, logistic regression models can be regularized
    using ℓ[1] or ℓ[2] penalties. Scikit-Learn actually adds an ℓ[2] penalty by default.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 就像其他线性模型一样，逻辑回归模型可以使用 ℓ[1] 或 ℓ[2] 惩罚进行正则化。Scikit-Learn 实际上默认添加了 ℓ[2] 惩罚。
- en: Softmax Regression
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Softmax 回归
- en: The logistic regression model can be generalized to support multiple classes
    directly, without having to train and combine multiple binary classifiers (as
    discussed in [Chapter 3](ch03.html#classification_chapter)). This is called *softmax
    regression*, or *multinomial logistic regression*.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型可以推广以直接支持多个类别，而无需训练和组合多个二元分类器（如第 3 章所述）。这被称为 *softmax 回归*，或 *多项式逻辑回归*。
- en: 'The idea is simple: when given an instance **x**, the softmax regression model
    first computes a score *s*[*k*](**x**) for each class *k*, then estimates the
    probability of each class by applying the *softmax function* (also called the
    *normalized exponential*) to the scores. The equation to compute *s*[*k*](**x**)
    should look familiar, as it is just like the equation for linear regression prediction
    (see [Equation 4-20](#softmax_score_for_class_k)).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很简单：当给定一个实例 **x** 时，softmax回归模型首先为每个类别 **k** 计算一个得分 *s*[*k*](**x**)，然后通过应用
    *softmax 函数*（也称为 *归一化指数*）到得分上来估计每个类别的概率。计算 *s*[*k*](**x**) 的方程应该看起来很熟悉，因为它与线性回归预测的方程类似（参见
    [方程 4-20](#softmax_score_for_class_k)）。
- en: Equation 4-20\. Softmax score for class k
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-20\. 类别 k 的 softmax 得分
- en: <mrow><msub><mi>s</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi>
    <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>⊺</mo></msup>
    <mi mathvariant="bold">x</mi></mrow>
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msub><mi>s</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi>
    <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>⊺</mo></msup>
    <mi mathvariant="bold">x</mi></mrow>
- en: Note that each class has its own dedicated parameter vector **θ**^((*k*)). All
    these vectors are typically stored as rows in a *parameter matrix* **Θ**.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个类别都有自己的专用参数向量 **θ**^((*k*)）。所有这些向量通常存储为 *参数矩阵* **Θ** 的行。
- en: Once you have computed the score of every class for the instance **x**, you
    can estimate the probability <msub><mover><mi>p</mi><mo>^</mo></mover><mi>k</mi></msub>
    that the instance belongs to class *k* by running the scores through the softmax
    function ([Equation 4-21](#softmax_function)). The function computes the exponential
    of every score, then normalizes them (dividing by the sum of all the exponentials).
    The scores are generally called logits or log-odds (although they are actually
    unnormalized log-odds).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你计算了实例 **x** 的每个类别的得分，你就可以通过将得分通过 softmax 函数（[方程 4-21](#softmax_function)）来估计实例属于类别
    *k* 的概率 <msub><mover><mi>p</mi><mo>^</mo></mover><mi>k</mi></msub>。该函数计算每个得分的指数，然后对它们进行归一化（除以所有指数的总和）。得分通常被称为
    logits 或 log-odds（尽管它们实际上是未归一化的 log-odds）。
- en: Equation 4-21\. Softmax function
  id: totrans-319
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-21\. Softmax 函数
- en: <mrow><msub><mover accent="true"><mi>p</mi> <mo>^</mo></mover> <mi>k</mi></msub>
    <mo>=</mo> <mi>σ</mi> <msub><mfenced separators="" open="(" close=")"><mi mathvariant="bold">s</mi><mo>(</mo><mi
    mathvariant="bold">x</mi><mo>)</mo></mfenced> <mi>k</mi></msub> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mrow><mtext>exp</mtext><mfenced separators=""
    open="(" close=")"><msub><mi>s</mi> <mi>k</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></mfenced></mrow>
    <mrow><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi></munderover>
    <mrow><mtext>exp</mtext><mfenced separators="" open="(" close=")"><msub><mi>s</mi>
    <mi>j</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></mfenced></mrow></mrow></mfrac></mstyle></mrow>
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msub><mover accent="true"><mi>p</mi> <mo>^</mo></mover> <mi>k</mi></msub>
    <mo>=</mo> <mi>σ</mi> <msub><mfenced separators="" open="(" close=")"><mi mathvariant="bold">s</mi><mo>(</mo><mi
    mathvariant="bold">x</mi><mo>)</mo></mfenced> <mi>k</mi></msub> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mrow><mtext>exp</mtext><mfenced separators=""
    open="(" close=")"><msub><mi>s</mi> <mi>k</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></mfenced></mrow>
    <mrow><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi></munderover>
    <mrow><mtext>exp</mtext><mfenced separators="" open="(" close=")"><msub><mi>s</mi>
    <mi>j</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></mfenced></mrow></mrow></mfrac></mstyle></mrow>
- en: 'In this equation:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*K* is the number of classes.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*K* 是类别的数量。'
- en: '**s**(**x**) is a vector containing the scores of each class for the instance
    **x**.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**s**(**x**) 是一个包含实例 **x** 每个类别得分的向量。'
- en: '*σ*(**s**(**x**))[*k*] is the estimated probability that the instance **x**
    belongs to class *k*, given the scores of each class for that instance.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*σ*(**s**(**x**))[*k*] 是在给定该实例每个类别的得分的情况下，实例 **x** 属于类别 **k** 的估计概率。'
- en: Just like the logistic regression classifier, by default the softmax regression
    classifier predicts the class with the highest estimated probability (which is
    simply the class with the highest score), as shown in [Equation 4-22](#softmax_regression_classifier_prediction).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 就像逻辑回归分类器一样，默认情况下，softmax回归分类器预测具有最高估计概率的类别（这仅仅是得分最高的类别），如 [方程 4-22](#softmax_regression_classifier_prediction)
    所示。
- en: Equation 4-22\. Softmax regression classifier prediction
  id: totrans-326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-22\. Softmax 回归分类器预测
- en: <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mo>=</mo> <munder><mo
    form="prefix">argmax</mo> <mi>k</mi></munder> <mi>σ</mi> <msub><mfenced separators=""
    open="(" close=")"><mi mathvariant="bold">s</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mfenced>
    <mi>k</mi></msub> <mo>=</mo> <munder><mo form="prefix">argmax</mo> <mi>k</mi></munder>
    <msub><mi>s</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo form="prefix">argmax</mo> <mi>k</mi></munder>
    <mfenced separators="" open="(" close=")"><msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi>
    <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>⊺</mo></msup>
    <mi mathvariant="bold">x</mi></mfenced></mrow>
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mo>=</mo> <munder><mo
    form="prefix">argmax</mo> <mi>k</mi></munder> <mi>σ</mi> <msub><mfenced separators=""
    open="(" close=")"><mi mathvariant="bold">s</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mfenced>
    <mi>k</mi></msub> <mo>=</mo> <munder><mo form="prefix">argmax</mo> <mi>k</mi></munder>
    <msub><mi>s</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo form="prefix">argmax</mo> <mi>k</mi></munder>
    <mfenced separators="" open="(" close=")"><msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi>
    <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>⊺</mo></msup>
    <mi mathvariant="bold">x</mi></mfenced></mrow>
- en: The *argmax* operator returns the value of a variable that maximizes a function.
    In this equation, it returns the value of *k* that maximizes the estimated probability
    *σ*(**s**(**x**))[*k*].
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '*argmax* 运算符返回使函数最大化的变量的值。在这个方程中，它返回使估计概率 *σ*(**s**(**x**))[*k*] 最大的 *k* 的值。'
- en: Tip
  id: totrans-329
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The softmax regression classifier predicts only one class at a time (i.e., it
    is multiclass, not multioutput), so it should be used only with mutually exclusive
    classes, such as different species of plants. You cannot use it to recognize multiple
    people in one picture.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 回归分类器一次只预测一个类别（即它是多类，而不是多输出），因此它只能与互斥的类别一起使用，例如不同的植物种类。你不能用它来识别一张图片中的多个人。
- en: Now that you know how the model estimates probabilities and makes predictions,
    let’s take a look at training. The objective is to have a model that estimates
    a high probability for the target class (and consequently a low probability for
    the other classes). Minimizing the cost function shown in [Equation 4-23](#cross_entropy_cost_function),
    called the *cross entropy*, should lead to this objective because it penalizes
    the model when it estimates a low probability for a target class. Cross entropy
    is frequently used to measure how well a set of estimated class probabilities
    matches the target classes.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了模型如何估计概率和做出预测，让我们来看看训练。目标是拥有一个能够为目标类别估计高概率（从而为其他类别估计低概率）的模型。最小化显示在[方程式
    4-23](#cross_entropy_cost_function)中的损失函数，称为 *交叉熵*，应该会导致这个目标，因为它在模型估计目标类别的概率低时会对模型进行惩罚。交叉熵经常用来衡量一组估计的类别概率与目标类别匹配得有多好。
- en: Equation 4-23\. Cross entropy cost function
  id: totrans-332
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4-23\. 交叉熵损失函数
- en: <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">Θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mi>y</mi><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mi>log</mi><mfenced><msubsup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mfenced></mrow>
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">Θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mi>y</mi><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mi>log</mi><mfenced><msubsup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mfenced></mrow>
- en: In this equation, <msubsup><mi>y</mi><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    is the target probability that the *i*^(th) instance belongs to class *k*. In
    general, it is either equal to 1 or 0, depending on whether the instance belongs
    to the class or not.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，<msubsup><mi>y</mi><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    是目标概率，表示第 *i* 个实例属于类别 *k*。一般来说，它要么等于 1，要么等于 0，这取决于实例是否属于该类别。
- en: Notice that when there are just two classes (*K* = 2), this cost function is
    equivalent to the logistic regression cost function (log loss; see [Equation 4-18](#logistic_regression_cost_function)).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当只有两个类别（*K* = 2）时，这个损失函数等同于逻辑回归损失函数（对数损失；参见[方程式 4-18](#logistic_regression_cost_function)）。
- en: The gradient vector of this cost function with regard to **θ**^((*k*)) is given
    by [Equation 4-24](#cross_entropy_gradient_vector_for_class_k).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 关于**θ**^((*k*))的成本函数的梯度向量由[方程4-24](#cross_entropy_gradient_vector_for_class_k)给出。
- en: Equation 4-24\. Cross entropy gradient vector for class k
  id: totrans-337
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-24\. 类k的交叉熵梯度向量
- en: <mrow><msub><mi>∇</mi> <msup><mi mathvariant="bold">θ</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup></msub>
    <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">Θ</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <mrow><mrow><mo fence="true" stretchy="true">(</mo></mrow> <msubsup><mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mi>k</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mo>-</mo> <msubsup><mi>y</mi> <mi>k</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mrow><mo fence="true" stretchy="true">)</mo></mrow> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mrow>
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msub><mi>∇</mi> <msup><mi mathvariant="bold">θ</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup></msub>
    <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">Θ</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <mrow><mrow><mo fence="true" stretchy="true">(</mo></mrow> <msubsup><mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mi>k</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mo>-</mo> <msubsup><mi>y</mi> <mi>k</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mrow><mo fence="true" stretchy="true">)</mo></mrow> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mrow>
- en: Now you can compute the gradient vector for every class, then use gradient descent
    (or any other optimization algorithm) to find the parameter matrix **Θ** that
    minimizes the cost function.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以计算每个类别的梯度向量，然后使用梯度下降法（或任何其他优化算法）来找到最小化成本函数的参数矩阵**Θ**。
- en: 'Let’s use softmax regression to classify the iris plants into all three classes.
    Scikit-Learn’s `LogisticRegression` classifier uses softmax regression automatically
    when you train it on more than two classes (assuming you use `solver="lbfgs"`,
    which is the default). It also applies ℓ[2] regularization by default, which you
    can control using the hyperparameter `C`: decrease `C` to increase regularization,
    as mentioned earlier.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用softmax回归将鸢尾花植物分类到三个类别中。当你在超过两个类别上训练Scikit-Learn的`LogisticRegression`分类器时，它会自动使用softmax回归（假设你使用`solver="lbfgs"`，这是默认设置）。它还默认应用ℓ[2]正则化，你可以通过超参数`C`来控制它：如前所述，减小`C`会增加正则化。
- en: '[PRE28]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'So the next time you find an iris with petals that are 5 cm long and 2 cm wide,
    you can ask your model to tell you what type of iris it is, and it will answer
    *Iris virginica* (class 2) with 96% probability (or *Iris versicolor* with 4%
    probability):'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 所以下次当你发现一个花瓣长5厘米、宽2厘米的鸢尾花时，你可以让模型告诉你它是什么类型的鸢尾花，它将以96%的概率回答*Iris virginica*（类别2），或者以4%的概率回答*Iris
    versicolor*：
- en: '[PRE29]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[Figure 4-26](#softmax_regression_contour_plot) shows the resulting decision
    boundaries, represented by the background colors. Notice that the decision boundaries
    between any two classes are linear. The figure also shows the probabilities for
    the *Iris versicolor* class, represented by the curved lines (e.g., the line labeled
    with 0.30 represents the 30% probability boundary). Notice that the model can
    predict a class that has an estimated probability below 50%. For example, at the
    point where all decision boundaries meet, all classes have an equal estimated
    probability of 33%.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-26](#softmax_regression_contour_plot)显示了由此产生的决策边界，用背景颜色表示。请注意，任意两个类别之间的决策边界是线性的。该图还显示了*Iris
    versicolor*类的概率，用曲线表示（例如，标记为0.30的线代表30%的概率边界）。请注意，模型可以预测一个估计概率低于50%的类别。例如，在所有决策边界相交的点，所有类别的估计概率都是33%。'
- en: '![Diagram showing the decision boundaries of softmax regression on the Iris
    dataset, with linear boundaries and probability contours for the Iris versicolor
    class.](assets/hmls_0426.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![展示softmax回归在Iris数据集上的决策边界，包括线性边界和Iris versicolor类的概率等高线的图](assets/hmls_0426.png)'
- en: Figure 4-26\. Softmax regression decision boundaries
  id: totrans-346
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-26\. Softmax回归决策边界
- en: In this chapter, you learned various ways to train linear models, both for regression
    and for classification. You used a closed-form equation to solve linear regression,
    as well as gradient descent, and you learned how various penalties can be added
    to the cost function during training to regularize the model. Along the way, you
    also learned how to plot learning curves and analyze them, and how to implement
    early stopping. Finally, you learned how logistic regression and softmax regression
    work. We’ve opened up the first machine learning black boxes! In the next chapters
    we will open many more, starting with support vector machines.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了各种训练线性模型的方法，无论是回归还是分类。你使用闭式方程来解决线性回归，以及梯度下降，并学习了如何在训练过程中将各种惩罚添加到损失函数中以正则化模型。在这个过程中，你还学习了如何绘制学习曲线并分析它们，以及如何实现早期停止。最后，你学习了逻辑回归和softmax回归的工作原理。我们打开了第一个机器学习黑盒！在接下来的章节中，我们将打开更多，从支持向量机开始。
- en: Exercises
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Which linear regression training algorithm can you use if you have a training
    set with millions of features?
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有包含数百万个特征的训练集，你可以使用哪种线性回归训练算法？
- en: Suppose the features in your training set have very different scales. Which
    algorithms might suffer from this, and how? What can you do about it?
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你的训练集中的特征具有非常不同的尺度。哪些算法可能会受到影响，以及如何受到影响？你能做些什么？
- en: Can gradient descent get stuck in a local minimum when training a logistic regression
    model?
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练逻辑回归模型时，梯度下降是否会陷入局部最小值？
- en: Do all gradient descent algorithms lead to the same model, provided you let
    them run long enough?
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你让它们运行足够长的时间，所有梯度下降算法都会导致相同的模型吗？
- en: Suppose you use batch gradient descent and you plot the validation error at
    every epoch. If you notice that the validation error consistently goes up, what
    is likely going on? How can you fix this?
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你使用批量梯度下降并在每个epoch绘制验证误差。如果你注意到验证误差持续上升，可能发生了什么？你该如何解决这个问题？
- en: Is it a good idea to stop mini-batch gradient descent immediately when the validation
    error goes up?
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当验证误差上升时，立即停止小批量梯度下降是否是一个好主意？
- en: Which gradient descent algorithm (among those we discussed) will reach the vicinity
    of the optimal solution the fastest? Which will actually converge? How can you
    make the others converge as well?
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们讨论的梯度下降算法中，哪个算法最快达到最优解的附近？哪个实际上会收敛？你如何使其他算法也收敛？
- en: Suppose you are using polynomial regression. You plot the learning curves and
    you notice that there is a large gap between the training error and the validation
    error. What is happening? What are three ways to solve this?
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你正在使用多项式回归。你绘制了学习曲线，并注意到训练误差和验证误差之间存在很大的差距。发生了什么？有三种方法可以解决这个问题。
- en: Suppose you are using ridge regression and you notice that the training error
    and the validation error are almost equal and fairly high. Would you say that
    the model suffers from high bias or high variance? Should you increase the regularization
    hyperparameter *α* or reduce it?
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你正在使用ridge回归，并且注意到训练误差和验证误差几乎相等且相当高。你会说模型受到高偏差还是高方差的影响？你应该增加正则化超参数 *α* 还是减少它？
- en: 'Why would you want to use:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么你会想使用：
- en: Ridge regression instead of plain linear regression (i.e., without any regularization)?
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ridge回归而不是普通线性回归（即没有任何正则化）？
- en: Lasso instead of ridge regression?
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用lasso而不是ridge回归？
- en: Elastic net instead of lasso regression?
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用弹性网络而不是lasso回归？
- en: Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.
    Should you implement two logistic regression classifiers or one softmax regression
    classifier?
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你想将图片分类为户外/室内和白天/夜间。你应该实现两个逻辑回归分类器还是使用一个softmax回归分类器？
- en: Implement batch gradient descent with early stopping for softmax regression
    without using Scikit-Learn, only NumPy. Use it on a classification task such as
    the iris dataset.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NumPy而不使用Scikit-Learn实现带有早期停止的批量梯度下降法进行softmax回归。将其应用于如鸢尾花数据集的分类任务。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解答可以在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: '^([1](ch04.html#id1433-marker)) A closed-form equation is only composed of
    a finite number of constants, variables, and standard operations: for example,
    *a* = sin(*b* – *c*). No infinite sums, no limits, no integrals, etc.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#id1433-marker)) 闭式方程仅由有限数量的常数、变量和标准运算组成：例如，*a* = sin(*b* – *c*)。没有无限求和，没有极限，没有积分等。
- en: ^([2](ch04.html#id1475-marker)) Technically speaking, its derivative is *Lipschitz
    continuous*.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#id1475-marker)) 从技术上来说，它的导数是*Lipschitz连续*。
- en: ^([3](ch04.html#id1476-marker)) Since feature 1 is smaller, it takes a larger
    change in *θ*[1] to affect the cost function, which is why the bowl is elongated
    along the *θ*[1] axis.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.html#id1476-marker)) 由于特征1较小，因此需要更大的*θ*[1]的变化来影响成本函数，这就是为什么碗沿着*θ*[1]轴拉长的原因。
- en: ^([4](ch04.html#id1483-marker)) Eta (*η*) is the seventh letter of the Greek
    alphabet.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.html#id1483-marker)) Eta (*η*) 是希腊字母表的第七个字母。
- en: ^([5](ch04.html#id1509-marker)) While the normal equation can only perform linear
    regression, the gradient descent algorithms can be used to train many other models,
    as you’ll see.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.html#id1509-marker)) 虽然正规方程只能执行线性回归，但梯度下降算法可以用来训练许多其他模型，正如你将看到的。
- en: ^([6](ch04.html#id1524-marker)) This notion of bias is not to be confused with
    the bias term of linear models.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.html#id1524-marker)) 这种偏差的概念不要与线性模型的偏差项混淆。
- en: ^([7](ch04.html#id1539-marker)) Inputs are colinear when one input is equal
    to a linear combination of some other inputs. For example, the temperature in
    Celsius degrees is colinear with the temperature in Fahrenheit degrees.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.html#id1539-marker)) 当一个输入等于其他一些输入的线性组合时，输入是共线的。例如，摄氏度温度与华氏度温度是共线的。
- en: ^([8](ch04.html#id1544-marker)) It is common to use the notation *J*(**θ**)
    for cost functions that don’t have a short name; I’ll often use this notation
    throughout the rest of this book. The context will make it clear which cost function
    is being discussed.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch04.html#id1544-marker)) 对于没有简短名称的成本函数，通常使用*J*(**θ**)的符号；我将在本书的其余部分经常使用这个符号。上下文将清楚地表明正在讨论哪个成本函数。
- en: ^([9](ch04.html#id1545-marker)) Norms are discussed in [Chapter 2](ch02.html#project_chapter).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch04.html#id1545-marker)) 规范在[第2章](ch02.html#project_chapter)中讨论。
- en: ^([10](ch04.html#id1552-marker)) A square matrix full of 0s except for 1s on
    the main diagonal (top left to bottom right).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch04.html#id1552-marker)) 一个除主对角线上的1之外全为0的方阵。
- en: ^([11](ch04.html#id1553-marker)) Alternatively, you can use the `Ridge` class
    with the `"sag"` solver. Stochastic average GD is a variant of stochastic GD.
    For more details, see the presentation [“Minimizing Finite Sums with the Stochastic
    Average Gradient Algorithm”](https://homl.info/12) by Mark Schmidt et al. from
    the University of British Columbia.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch04.html#id1553-marker)) 或者，你可以使用带有`"sag"`求解器的`Ridge`类。随机平均GD是随机GD的一个变体。更多细节，请参阅不列颠哥伦比亚大学的Mark
    Schmidt等人发表的演示文稿“使用随机平均梯度算法最小化有限和”（[“Minimizing Finite Sums with the Stochastic
    Average Gradient Algorithm”](https://homl.info/12)）。
- en: ^([12](ch04.html#id1564-marker)) You can think of a subgradient vector at a
    nondifferentiable point as an intermediate vector between the gradient vectors
    around that point.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch04.html#id1564-marker)) 你可以将非可微点处的子梯度向量视为该点周围梯度向量之间的中间向量。
- en: '^([13](ch04.html#id1576-marker)) Slide #63 of the [NeurIPS 2015 Deep Learning
    Tutorial](https://homl.info/freelunch).'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch04.html#id1576-marker)) [NeurIPS 2015深度学习教程](https://homl.info/freelunch)的第63张幻灯片。
- en: ^([14](ch04.html#id1611-marker)) Photos reproduced from the corresponding Wikipedia
    pages. *Iris virginica* photo by Frank Mayfield ([Creative Commons BY-SA 2.0](https://oreil.ly/O2fAq)),
    *Iris versicolor* photo by D. Gordon E. Robertson ([Creative Commons BY-SA 3.0](https://oreil.ly/pMbrK)),
    *Iris setosa* photo public domain.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch04.html#id1611-marker)) 图片来自相应的维基百科页面。*Iris virginica* 图片由Frank Mayfield提供
    ([Creative Commons BY-SA 2.0](https://oreil.ly/O2fAq))，*Iris versicolor* 图片由D.
    Gordon E. Robertson提供 ([Creative Commons BY-SA 3.0](https://oreil.ly/pMbrK))，*Iris
    setosa* 图片为公有领域。
- en: '^([15](ch04.html#id1612-marker)) NumPy’s `reshape()` function allows one dimension
    to be –1, which means “automatic”: the value is inferred from the length of the
    array and the remaining dimensions.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch04.html#id1612-marker)) NumPy的`reshape()`函数允许一个维度为-1，这意味着“自动”：该值从数组的长度和剩余维度推断出来。
- en: ^([16](ch04.html#id1613-marker)) It is the set of points **x** such that *θ*[0]
    + *θ*[1]*x*[1] + *θ*[2]*x*[2] = 0, which defines a straight line.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch04.html#id1613-marker)) 它是满足*θ*[0] + *θ*[1]*x*[1] + *θ*[2]*x*[2] =
    0的点集，这定义了一条直线。
