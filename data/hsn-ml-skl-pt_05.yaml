- en: Chapter 4\. Training Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章\. 训练模型
- en: 'So far we have treated machine learning models and their training algorithms
    mostly like black boxes. If you went through some of the exercises in the previous
    chapters, you may have been surprised by how much you can get done without knowing
    anything about what’s under the hood: you optimized a regression system, you improved
    a digit image classifier, and you even built a spam classifier from scratch, all
    without knowing how they actually work. Indeed, in many situations you don’t really
    need to know the implementation details.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要将机器学习模型及其训练算法视为黑盒。如果你完成了前几章的一些练习，你可能对在不了解内部结构的情况下能完成多少工作感到惊讶：你优化了一个回归系统，改进了一个数字图像分类器，甚至从头开始构建了一个垃圾邮件分类器，所有这些都不需要了解它们实际是如何工作的。确实，在许多情况下，你并不真的需要了解实现细节。
- en: However, having a good understanding of how things work can help you quickly
    home in on the appropriate model, the right training algorithm to use, and a good
    set of hyperparameters for your task. Understanding what’s under the hood will
    also help you debug issues and perform error analysis more efficiently. Lastly,
    most of the topics discussed in this chapter will be essential in understanding,
    building, and training neural networks (discussed in [Part II](part02.html#neural_nets_part)
    of this book).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对事物工作原理的良好理解可以帮助你快速找到合适的模型、正确的训练算法以及适合你任务的良好的超参数集。了解内部结构也将帮助你更有效地调试问题和进行错误分析。最后，本章讨论的许多主题对于理解、构建和训练神经网络（本书第II部分[part02.html#neural_nets_part]中讨论）是必不可少的。
- en: 'In this chapter we will start by looking at the linear regression model, one
    of the simplest models there is. We will discuss two very different ways to train
    it:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先探讨线性回归模型，这是最简单的模型之一。我们将讨论两种非常不同的训练方法：
- en: Using a “closed-form” equation⁠^([1](ch04.html#id1433)) that directly computes
    the model parameters that best fit the model to the training set (i.e., the model
    parameters that minimize the cost function over the training set).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个“闭式”方程⁠^([1](ch04.html#id1433))，该方程直接计算最适合训练集（即最小化训练集上成本函数的模型参数）的模型参数。
- en: 'Using an iterative optimization approach called gradient descent (GD) that
    gradually tweaks the model parameters to minimize the cost function over the training
    set, eventually converging to the same set of parameters as the first method.
    We will look at a few variants of gradient descent that we will use again and
    again when we study neural networks in [Part II](part02.html#neural_nets_part):
    batch GD, mini-batch GD, and stochastic GD.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一种称为梯度下降（GD）的迭代优化方法，该方法逐渐调整模型参数以最小化训练集上的成本函数，最终收敛到与第一种方法相同的一组参数。当我们研究第II部分[part02.html#neural_nets_part]中的神经网络时，我们将再次查看几种梯度下降的变体：批量GD、小批量GD和随机GD。
- en: Next we will look at polynomial regression, a more complex model that can fit
    nonlinear datasets. Since this model has more parameters than linear regression,
    it is more prone to overfitting the training data. We will explore how to detect
    whether this is the case using learning curves, and then we will look at several
    regularization techniques that can reduce the risk of overfitting the training
    set.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨多项式回归，这是一个更复杂的模型，可以拟合非线性数据集。由于这个模型比线性回归有更多的参数，它更容易过拟合训练数据。我们将通过学习曲线探索如何检测这种情况，然后我们将探讨几种可以减少过拟合训练集风险的正则化技术。
- en: 'Finally, we will examine two more models that are commonly used for classification
    tasks: logistic regression and softmax regression.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将考察两种常用于分类任务的模型：逻辑回归和softmax回归。
- en: Warning
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: There will be quite a few math equations in this chapter using basic concepts
    of linear algebra and calculus. To understand these equations, you need to be
    familiar with vectors and matrices—how to transpose them, multiply them, and invert
    them—as well as partial derivatives. If these concepts are unfamiliar, please
    review the introductory Jupyter notebooks on linear algebra and calculus provided
    in the [online supplemental material](https://github.com/ageron/handson-mlp).
    If you are truly allergic to math, you can just skip the equations; the text should
    still help you grasp most of the concepts. That said, learning the mathematical
    formalism is extremely useful, as it will allow you to read ML papers. Although
    it may seem daunting at first, it’s actually not that hard, and this chapter includes
    code that should help you make sense of the equations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用线性代数和微积分的基本概念，包含相当多的数学方程。要理解这些方程，你需要熟悉向量和矩阵——如何转置它们，如何乘法，以及如何求逆，以及偏导数。如果这些概念不熟悉，请查阅
    [在线补充材料](https://github.com/ageron/handson-mlp) 中提供的线性代数和微积分的入门 Jupyter 笔记本。如果你真的对数学过敏，可以跳过这些方程；文本应该仍然能帮助你掌握大部分概念。尽管一开始可能觉得令人畏惧，但实际上并不难，本章还包括代码，可以帮助你理解这些方程。
- en: Linear Regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: In [Chapter 1](ch01.html#landscape_chapter) we looked at a simple linear model
    of life satisfaction ([Equation 4-1](#life_satisfaction_equation)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第一章](ch01.html#landscape_chapter) 中，我们研究了生活满意度的简单线性模型（[方程 4-1](#life_satisfaction_equation)）。
- en: Equation 4-1\. A simple linear model of life satisfaction
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-1\. 生活满意度的简单线性模型
- en: $life normal bar satisfaction equals theta 0 plus theta 1 times GDP normal bar
    per normal bar capita$
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: $life normal bar satisfaction equals theta 0 plus theta 1 times GDP normal bar
    per normal bar capita$
- en: This model is just a linear function of the input feature `GDP_per_capita`.
    *θ*[0] and *θ*[1] are the model’s parameters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型仅仅是输入特征 `GDP_per_capita` 的线性函数。*θ*[0] 和 *θ*[1] 是模型的参数。
- en: More generally, a linear model makes a prediction by simply computing a weighted
    sum of the input features, plus a constant called the *bias term* (also called
    the *intercept term*), as shown in [Equation 4-2](#linear_regression_equation).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，线性模型通过简单地计算输入特征的加权总和，加上一个称为 *偏差项*（也称为 *截距项*）的常数来进行预测，如 [方程 4-2](#linear_regression_equation)
    所示。
- en: Equation 4-2\. Linear regression model prediction
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-2\. 线性回归模型预测
- en: $ModifyingAbove y With caret equals theta 0 plus theta 1 x 1 plus theta 2 x
    2 plus midline-horizontal-ellipsis plus theta Subscript n Baseline x Subscript
    n$
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: $y$ 上方的 caret 等于 theta 0 加上 theta 1 乘以 x 1 加上 theta 2 乘以 x 2 加上中划线水平省略号加上 theta
    下标 n 基线 x 下标 n$
- en: 'In this equation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*ŷ* is the predicted value.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ŷ* 是预测值。'
- en: '*n* is the number of features.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n* 是特征的数量。'
- en: '*x*[*i*] is the *i*^(th) feature value.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[*i*] 是 *i*^(th) 特征值。'
- en: '*θ*[*j*] is the *j*^(th) model parameter, including the bias term *θ*[0] and
    the feature weights *θ*[1], *θ*[2], ⋯, *θ*[*n*].'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*θ*[*j*] 是 *j*^(th) 模型参数，包括偏差项 *θ*[0] 和特征权重 *θ*[1]，*θ*[2]，⋯，*θ*[*n*]。'
- en: This can be written much more concisely using a vectorized form, as shown in
    [Equation 4-3](#linear_regression_prediction_vectorized_equation).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过向量化的形式更加简洁地表示，如 [方程 4-3](#linear_regression_prediction_vectorized_equation)
    所示。
- en: Equation 4-3\. Linear regression model prediction (vectorized form)
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-3\. 线性回归模型预测（向量化形式）
- en: <mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msub><mi>h</mi><mi
    mathvariant="bold">θ</mi></msub><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>·</mo><mi mathvariant="bold">x</mi>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msub><mi>h</mi><mi
    mathvariant="bold">θ</mi></msub><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>·</mo><mi mathvariant="bold">x</mi>
- en: 'In this equation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*h*[**θ**] is the hypothesis function, using the model parameters **θ**.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h*[**θ**] 是假设函数，使用模型参数 **θ**。'
- en: '**θ** is the model’s *parameter vector*, containing the bias term *θ*[0] and
    the feature weights *θ*[1] to *θ*[*n*].'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**θ** 是模型的 *参数向量*，包含偏差项 *θ*[0] 和特征权重 *θ*[1] 到 *θ*[*n*]。'
- en: '**x** is the instance’s *feature vector*, containing *x*[0] to *x*[*n*], with
    *x*[0] always equal to 1.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**x** 是实例的 *特征向量*，包含 *x*[0] 到 *x*[*n*]，其中 *x*[0] 总是等于 1。'
- en: '**θ** · **x** is the dot product of the vectors **θ** and **x**, which is equal
    to *θ*[0]*x*[0] + *θ*[1]*x*[1] + *θ*[2]*x*[2] + ... + *θ*[*n*]*x*[*n*].'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**θ** · **x** 是向量 **θ** 和 **x** 的点积，等于 *θ*[0]*x*[0] + *θ*[1]*x*[1] + *θ*[2]*x*[2]
    + ... + *θ*[*n*]*x*[*n*]。'
- en: Note
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In machine learning, vectors are often represented as *column vectors*, which
    are 2D arrays with a single column. If **θ** and **x** are column vectors, then
    the prediction is <mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi>, where
    <msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup> is the *transpose* of **θ**
    (a row vector instead of a column vector) and <msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi
    mathvariant="bold">x</mi> is the matrix multiplication of <msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup>
    and **x**. It is of course the same prediction, except that it is now represented
    as a single-cell matrix rather than a scalar value. In this book I will use this
    notation to avoid switching between dot products and matrix multiplications.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，向量通常表示为 *列向量*，它们是只有一列的二维数组。如果 **θ** 和 **x** 是列向量，那么预测是 <mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi>，其中 <msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup> 是 **θ** 的 *转置*（一个行向量而不是列向量），而 <msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi> 是 <msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup> 和 **x** 的矩阵乘积。当然，这仍然是相同的预测，只是现在它被表示为一个单元素矩阵而不是标量值。在这本书中，我将使用这种符号来避免在点积和矩阵乘法之间切换。
- en: OK, that’s the linear regression model—but how do we train it? Well, recall
    that training a model means setting its parameters so that the model best fits
    the training set. For this purpose, we first need a measure of how well (or poorly)
    the model fits the training data. In [Chapter 2](ch02.html#project_chapter) we
    saw that the most common performance measure of a regression model is the root
    mean squared error ([Equation 2-1](ch02.html#rmse_equation)). Therefore, to train
    a linear regression model, we need to find the value of **θ** that minimizes the
    RMSE. In practice, it is simpler to minimize the mean squared error (MSE) than
    the RMSE, and it leads to the same result (because the value that minimizes a
    positive function also minimizes its square root).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这就是线性回归模型——但是我们是怎样训练它的呢？嗯，回想一下，训练一个模型意味着设置其参数，使得模型最好地拟合训练集。为此，我们首先需要一个衡量模型如何（或不好）拟合训练数据的指标。在
    [第二章](ch02.html#project_chapter) 中，我们看到了回归模型最常见的性能指标是均方根误差（[方程 2-1](ch02.html#rmse_equation)）。因此，为了训练线性回归模型，我们需要找到使
    RMSE 最小的 **θ** 的值。在实践中，最小化均方误差（MSE）比最小化 RMSE 更简单，并且它会导致相同的结果（因为最小化正函数的值也会最小化其平方根）。
- en: Warning
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Learning algorithms will often optimize a different loss function during training
    than the performance measure used to evaluate the final model. This is generally
    because the function is easier to optimize and/or because it has extra terms needed
    during training only (e.g., for regularization). A good performance metric is
    as close as possible to the final business objective. A good training loss is
    easy to optimize and strongly correlated with the metric. For example, classifiers
    are often trained using a cost function such as the log loss (as you will see
    later in this chapter) but evaluated using precision/recall. The log loss is easy
    to minimize, and doing so will usually improve precision/recall.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 学习算法在训练过程中通常会优化一个不同的损失函数，而不是用于评估最终模型的性能指标。这通常是因为该函数更容易优化，或者因为它在训练期间需要额外的项（例如，用于正则化）。一个好的性能指标应该尽可能接近最终的商业目标。一个好的训练损失函数易于优化，并且与指标高度相关。例如，分类器通常使用如对数损失（你将在本章后面看到）之类的成本函数进行训练，但使用精确度/召回率进行评估。对数损失易于最小化，并且这样做通常会提高精确度/召回率。
- en: The MSE of a linear regression hypothesis *h*[**θ**] on a training set **X**
    is calculated using [Equation 4-4](#mse_cost_function).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归假设 *h*[**θ**] 在训练集 **X** 上的均方误差（MSE）是使用 [方程 4-4](#mse_cost_function) 计算的。
- en: Equation 4-4\. MSE cost function for a linear regression model
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-4\. 线性回归模型的均方误差（MSE）成本函数
- en: <mrow><mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi> <mo
    lspace="0%" rspace="0%">,</mo> <mi mathvariant="bold">y</mi> <mo lspace="0%" rspace="0%">,</mo>
    <msub><mi>h</mi> <mi mathvariant="bold">θ</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi> <mo
    lspace="0%" rspace="0%">,</mo> <mi mathvariant="bold">y</mi> <mo lspace="0%" rspace="0%">,</mo>
    <msub><mi>h</mi> <mi mathvariant="bold">θ</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow>
- en: Most of these notations were presented in [Chapter 2](ch02.html#project_chapter)
    (see [“Notations”](ch02.html#notations)). The only difference is that we write
    *h*[**θ**] instead of just *h* to make it clear that the model is parametrized
    by the vector **θ**. To simplify notations, we will just write MSE(**θ**) instead
    of MSE(**X**, *h*[**θ**]).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些符号都在 [第 2 章](ch02.html#project_chapter)（见 [“符号”](ch02.html#notations)）中介绍过。唯一的区别是我们写
    *h*[**θ**] 而不是仅仅 *h*，以使其清楚模型是由向量 **θ** 参数化的。为了简化符号，我们将只写 MSE(**θ**) 而不是 MSE(**X**,
    *h*[**θ**])。
- en: The Normal Equation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则方程
- en: To find the value of **θ** that minimizes the MSE, there exists a *closed-form
    solution*—in other words, a mathematical equation that gives the result directly.
    This is called the *normal equation* ([Equation 4-5](#equation_four_four)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到使 MSE 最小的 **θ** 值，存在一个 *封闭形式解*——换句话说，一个直接给出结果的数学方程。这被称为 *正则方程* ([方程 4-5](#equation_four_four))。
- en: Equation 4-5\. Normal equation
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-5\. 正则方程
- en: <mrow><mover accent="true"><mi mathvariant="bold">θ</mi> <mo>^</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">y</mi></mrow>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mover accent="true"><mi mathvariant="bold">θ</mi> <mo>^</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">y</mi></mrow>
- en: 'In this equation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover> is the
    value of **θ** that minimizes the cost function.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover> 是使成本函数最小的
    **θ** 的值。
- en: '**y** is the vector of target values containing *y*^((1)) to *y*^((*m*)).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**y** 是包含 *y*^((1)) 到 *y*^((*m*)）的目标值的向量。'
- en: 'Let’s generate some linear-looking data to test this equation on ([Figure 4-1](#generated_data_plot)):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一些看起来像线性的数据来测试这个方程（[图 4-1](#generated_data_plot)）：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Scatter plot showing a linear dataset with increasing trend, generated to
    illustrate the application of the normal equation in linear regression.](assets/hmls_0401.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![展示线性数据集和增长趋势的散点图，用于说明线性回归中正则方程的应用](assets/hmls_0401.png)'
- en: Figure 4-1\. A randomly generated linear dataset
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 随机生成的线性数据集
- en: 'Now let’s compute <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover>
    using the normal equation. We will use the `inv()` function from NumPy’s linear
    algebra module (`np.linalg`) to compute the inverse of a matrix, and the `@` operator
    for matrix multiplication:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用正则方程来计算 <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover>。我们将使用
    NumPy 线性代数模块（`np.linalg`）中的 `inv()` 函数来计算矩阵的逆，并使用 `@` 运算符进行矩阵乘法：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `@` operator performs matrix multiplication. If `A` and `B` are NumPy arrays,
    then `A @ B` is equivalent to `np.matmul(A, B)`. Many other libraries, like TensorFlow,
    PyTorch, and JAX, support the `@` operator as well. However, you cannot use `@`
    on pure Python arrays (i.e., lists of lists).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`@` 运算符执行矩阵乘法。如果 `A` 和 `B` 是 NumPy 数组，则 `A @ B` 等同于 `np.matmul(A, B)`。许多其他库，如
    TensorFlow、PyTorch 和 JAX，也支持 `@` 运算符。然而，你不能在纯 Python 数组（即列表的列表）上使用 `@`。'
- en: 'The function that we used to generate the data is *y* = 4 + 3*x*[1] + Gaussian
    noise. Let’s see what the equation found:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来生成数据的函数是 *y* = 4 + 3*x*[1] + 高斯噪声。让我们看看找到的方程：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3][PRE4] [PRE5][PRE6]`` [PRE7][PRE8]``py[PRE9] import matplotlib.pyplot
    as plt  plt.plot(X_new, y_predict, "r-", label="Predictions") plt.plot(X, y, "b.")
    [...]  # beautify the figure: add labels, axis, grid, and legend plt.show() [PRE10]
    >>> from sklearn.linear_model import LinearRegression `>>>` `lin_reg` `=` `LinearRegression``()`
    [PRE11]` `>>>` `lin_reg``.``intercept_``,` `lin_reg``.``coef_` [PRE12] [PRE13]``
    [PRE14][PRE15][PRE16][PRE17] >>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b,
    y, rcond=1e-6) `>>>` `theta_best_svd` `` `array([[3.69084138],`  `[3.32960458]])`
    `` [PRE18]`` [PRE19] >>> np.linalg.pinv(X_b) @ y `array([[3.69084138],`  `[3.32960458]])`
    [PRE20]` [PRE21][PRE22][PRE23][PRE24][PRE25] [PRE26]`py` [PRE27]  [PRE28] ``##
    Computational Complexity    The normal equation computes the inverse of **X**^⊺
    **X**, which is an (*n* + 1) × (*n* + 1) matrix (where *n* is the number of features).
    The *computational complexity* of inverting such a matrix is typically about *O*(*n*^(2.4))
    to *O*(*n*³), depending on the implementation. In other words, if you double the
    number of features, you multiply the computation time by roughly 2^(2.4) = 5.3
    to 2³ = 8.    The SVD approach used by Scikit-Learn’s `LinearRegression` class
    is about *O*(*n*²). If you double the number of features, you multiply the computation
    time by roughly 4.    ###### Warning    Both the normal equation and the SVD approach
    get very slow when the number of features grows large (e.g., 100,000). On the
    positive side, both are linear with regard to the number of instances in the training
    set (they are *O*(*m*)), so they handle large training sets efficiently, provided
    they can fit in memory.    Also, once you have trained your linear regression
    model (using the normal equation or any other algorithm), predictions are very
    fast: the computational complexity is linear with regard to both the number of
    instances you want to make predictions on and the number of features. In other
    words, making predictions on twice as many instances (or twice as many features)
    will take roughly twice as much time.    Now we will look at a very different
    way to train a linear regression model, which is better suited for cases where
    there are a large number of features or too many training instances to fit in
    memory.`` [PRE29]`  [PRE30][PRE31][PRE32]` [PRE33][PRE34][PRE35] [PRE36]`py[PRE37]`py[PRE38]py[PRE39][PRE40][PRE41]py
    rng = np.random.default_rng(seed=42) m = 200  # number of instances X = 6 * rng.random((m,
    1)) - 3 y = 0.5 * X ** 2 + X + 2 + rng.standard_normal((m, 1)) [PRE42]py >>> from
    sklearn.preprocessing import PolynomialFeatures `>>>` `poly_features` `=` `PolynomialFeatures``(``degree``=``2``,`
    `include_bias``=``False``)` [PRE43] `X_poly` now contains the original feature
    of `X` plus the square of this feature. Now we can fit a `LinearRegression` model
    to this extended training data ([Figure 4-13](#quadratic_predictions_plot)):    [PRE44]
    `>>>` `lin_reg``.``intercept_``,` `lin_reg``.``coef_` `` `(array([2.00540719]),
    array([[1.11022126, 0.50526985]]))` `` [PRE45]  [PRE46] ``![Scatter plot showing
    polynomial regression model predictions with a red curve fitting the data points,
    illustrating the relationship between x1 and y.](assets/hmls_0413.png)  ######
    Figure 4-13\. Polynomial regression model predictions    Not bad: the model estimates
    <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mo>=</mo> <mn>0.56</mn>
    <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow> <mn>2</mn></msup> <mo>+</mo>
    <mn>0.93</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mn>1.78</mn></mrow>
    when in fact the original function was <mrow><mi>y</mi> <mo>=</mo> <mn>0.5</mn>
    <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow> <mn>2</mn></msup> <mo>+</mo>
    <mn>1.0</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mn>2.0</mn> <mo>+</mo>
    <mtext>Gaussian noise</mtext></mrow> .    Note that when there are multiple features,
    polynomial regression is capable of finding relationships between features, which
    is something a plain linear regression model cannot do. This is made possible
    by the fact that `PolynomialFeatures` also adds all combinations of features up
    to the given degree. For example, if there were two features *a* and *b*, `PolynomialFeatures`
    with `degree=3` would not only add the features *a*², *a*³, *b*², and *b*³, but
    also the combinations *ab*, *a*²*b*, and *ab*².    ###### Warning    `PolynomialFeatures(degree=*d*)`
    transforms an array containing *n* features into an array containing (*n* + *d*)!
    / *d*!*n*! features, where *n*! is the *factorial* of *n*, equal to 1 × 2 × 3
    × ⋯ × *n*. Beware of the combinatorial explosion of the number of features!``
    [PRE47]` [PRE48][PRE49][PRE50][PRE51][PRE52]``py[PRE53]py # Learning Curves    If
    you perform high-degree polynomial regression, you will likely fit the training
    data much better than with plain linear regression. For example, [Figure 4-14](#high_degree_polynomials_plot)
    applies a 300-degree polynomial model to the preceding training data, and compares
    the result with a pure linear model and a quadratic model (second-degree polynomial).
    Notice how the 300-degree polynomial model wiggles around to get as close as possible
    to the training instances.  ![Graph comparing linear (1-degree), quadratic (2-degree),
    and 300-degree polynomial regression models, illustrating overfitting with the
    high-degree polynomial as it closely follows the data points.](assets/hmls_0414.png)  ######
    Figure 4-14\. High-degree polynomial regression    This high-degree polynomial
    regression model is severely overfitting the training data, while the linear model
    is underfitting it. The model that will generalize best in this case is the quadratic
    model, which makes sense because the data was generated using a quadratic model.
    But in general you won’t know what function generated the data, so how can you
    decide how complex your model should be? How can you tell that your model is overfitting
    or underfitting the data?    In [Chapter 2](ch02.html#project_chapter) you used
    cross-validation to get an estimate of a model’s generalization performance. If
    a model performs well on the training data but generalizes poorly according to
    the cross-validation metrics, then your model is overfitting. If it performs poorly
    on both, then it is underfitting. This is one way to tell when a model is too
    simple or too complex.    Another way to tell is to look at the *learning curves*,
    which are plots of the model’s training error and validation error as a function
    of the training iteration: just evaluate the model at regular intervals during
    training on both the training set and the validation set, and plot the results.
    If the model cannot be trained incrementally (i.e., if it does not support `partial_fit()`
    or `warm_start`), then you must train it several times on gradually larger subsets
    of the training set.    Scikit-Learn has a useful `learning_curve()` function
    to help with this: it trains and evaluates the model using cross-validation. By
    default it retrains the model on growing subsets of the training set, but if the
    model supports incremental learning you can set `exploit_incremental_learning=True`
    when calling `learning_curve()` and it will train the model incrementally instead.
    The function returns the training set sizes at which it evaluated the model, and
    the training and validation scores it measured for each size and for each cross-validation
    fold. Let’s use this function to look at the learning curves of the plain linear
    regression model (see [Figure 4-15](#underfitting_learning_curves_plot)):    [PRE54]py  ![Line
    graph of learning curves shows root mean square error (RMSE) decreasing and plateauing
    for both training and validation sets as training set size increases, indicating
    underfitting.](assets/hmls_0415.png)  ###### Figure 4-15\. Learning curves    This
    model is underfitting, it’s too simple for the data. How can we tell? Well, let’s
    look at the training error. When there are just one or two instances in the training
    set, the model can fit them perfectly, which is why the curve starts at zero.
    But as new instances are added to the training set, it becomes impossible for
    the model to fit the training data perfectly, both because the data is noisy and
    because it is not linear at all. So the error on the training data goes up until
    it reaches a plateau, at which point adding new instances to the training set
    doesn’t make the average error much better or worse. Now let’s look at the validation
    error. When the model is trained on very few training instances, it is incapable
    of generalizing properly, which is why the validation error is initially quite
    large. Then, as the model is shown more training examples, it learns, and thus
    the validation error slowly goes down. However, once again a straight line cannot
    do a good job of modeling the data, so the error ends up at a plateau, very close
    to the other curve.    These learning curves are typical of a model that’s underfitting.
    Both curves have reached a plateau; they are close and fairly high.    ######
    Tip    If your model is underfitting the training data, adding more training examples
    will not help. You need to use a better model or come up with better features.    Now
    let’s look at the learning curves of a 10th-degree polynomial model on the same
    data ([Figure 4-16](#learning_curves_plot)):    [PRE55]py  ![Learning curves for
    a 10th-degree polynomial model showing root mean square error (RMSE) decreasing
    with larger training set sizes, with validation error stabilizing.](assets/hmls_0416.png)  ######
    Figure 4-16\. Learning curves for the 10th-degree polynomial model    These learning
    curves look a bit like the previous ones, but there are two very important differences:    *   The
    error on the training data is much lower than before.           *   There is a
    gap between the curves. This means that the model performs better on the training
    data than on the validation data, which is the hallmark of an overfitting model.
    If you used a much larger training set, however, the two curves would continue
    to get closer.              ###### Tip    One way to improve an overfitting model
    is to feed it more training data until the validation error gets close enough
    to the training error.    # Regularized Linear Models    As you saw in Chapters
    [1](ch01.html#landscape_chapter) and [2](ch02.html#project_chapter), a good way
    to reduce overfitting is to regularize the model (i.e., to constrain it): the
    fewer degrees of freedom it has, the harder it will be for it to overfit the data.
    A simple way to regularize a polynomial model is to reduce the number of polynomial
    degrees.    What about linear models? Can we regularize them too? You may wonder
    why we may want to do that: aren’t linear models constrained enough already? Well,
    linear regression makes a few assumptions, including the fact that the true relationship
    between the inputs and the outputs is linear, the noise has zero mean, constant
    variance, and is independent of the inputs, plus the input matrix has full rank,
    meaning that the inputs are not colinear⁠^([7](ch04.html#id1539)) and there at
    least as many samples as parameters. In practice, some assumptions don’t hold
    perfectly. For example, some inputs may be close to colinear, which makes linear
    regression numerically unstable, meaning that very small differences in the training
    set can have a big impact on the trained model. Regularization can stabilize linear
    models and make them more accurate.    So how can we regularize a linear model?
    This is usually done by constraining its weights. In this section, we will discuss
    ridge regression, lasso regression, and elastic net regression, which implement
    three different ways to do that.    ## Ridge Regression    *Ridge regression*
    (also called *Tikhonov regularization*) is a regularized version of linear regression:
    a *regularization term* equal to <mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup>
    is added to the MSE. This forces the learning algorithm to not only fit the data
    but also keep the model weights as small as possible. This constraint makes the
    model less flexible, preventing it from stretching itself too much to fit every
    data point: this reduces the risk of overfitting. Note that the regularization
    term should only be added to the cost function during training. Once the model
    is trained, you want to use the unregularized MSE (or the RMSE) to evaluate the
    model’s performance.    The hyperparameter *α* controls how much you want to regularize
    the model. If *α* = 0, then ridge regression is just linear regression. If *α*
    is very large, then all weights end up very close to zero and the result is a
    flat line going through the data’s mean. [Equation 4-9](#ridge_cost_function)
    presents the ridge regression cost function.⁠^([8](ch04.html#id1544))    #####
    Equation 4-9\. Ridge regression cost function  <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup>  Note
    that the bias term *θ*[0] is not regularized (the sum starts at *i* = 1, not 0).
    If we define **w** as the vector of feature weights (*θ*[1] to *θ*[*n*]), then
    the regularization term is equal to *α*(∥**w**∥[2])² / *m*, where ∥**w**∥[2] represents
    the ℓ[2] norm of the weight vector.⁠^([9](ch04.html#id1545)) For batch gradient
    descent, just add 2*α***w** / *m* to the part of the MSE gradient vector that
    corresponds to the feature weights, without adding anything to the gradient of
    the bias term (see [Equation 4-7](#mse_gradient_vector)).    ###### Warning    It
    is important to scale the data (e.g., using a `StandardScaler`) before performing
    ridge regression, as it is sensitive to the scale of the input features. This
    is true of most regularized models.    [Figure 4-18](#ridge_regression_plot) shows
    several ridge models that were trained on some very noisy linear data using different
    *α* values. On the left, plain ridge models are used, leading to linear predictions.
    On the right, the data is first expanded using `PolynomialFeatures(degree=10)`,
    then it is scaled using a `StandardScaler`, and finally the ridge models are applied
    to the resulting features: this is polynomial regression with ridge regularization.
    Note how increasing *α* leads to flatter (i.e., less extreme, more reasonable)
    predictions, thus reducing the model’s variance but increasing its bias.  ![Graphs
    showing linear and polynomial ridge regression models applied to noisy linear
    data, with increasing alpha values resulting in flatter predictions.](assets/hmls_0418.png)  ######
    Figure 4-18\. Linear (left) and polynomial (right) models, both with various levels
    of ridge regularization    As with linear regression, we can perform ridge regression
    either by computing a closed-form equation or by performing gradient descent.
    The pros and cons are the same. [Equation 4-10](#ridge_regression_solution) shows
    the closed-form solution, where **A** is the (*n* + 1) × (*n* + 1) *identity matrix*,⁠^([10](ch04.html#id1552))
    except with a 0 in the top-left cell, corresponding to the bias term.    #####
    Equation 4-10\. Ridge regression closed-form solution  <mrow><mover accent="true"><mi
    mathvariant="bold">θ</mi> <mo>^</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">X</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">X</mi><mo>+</mo><mi>α</mi><mi
    mathvariant="bold">A</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">y</mi></mrow>  Here
    is how to perform ridge regression with Scikit-Learn using a closed-form solution
    (a variant of [Equation 4-10](#ridge_regression_solution) that uses a matrix factorization
    technique by André-Louis Cholesky):    [PRE56]py` `>>>` `ridge_reg``.``fit``(``X``,`
    `y``)` [PRE57]py [PRE58]``py`` [PRE59]py And using stochastic gradient descent:⁠^([11](ch04.html#id1553))    [PRE60]py``
    `...` [PRE61]` [PRE62]` [PRE63] [PRE64][PRE65][PRE66][PRE67][PRE68][PRE69]py[PRE70]py`
    [PRE71]  [PRE72][PRE73]py[PRE74]`py `>>>` `list``(``iris``)` [PRE75]`py [PRE76]py[PRE77]``
    [PRE78] from sklearn.linear_model import LogisticRegression from sklearn.model_selection
    import train_test_split  X = iris.data[["petal width (cm)"]].values y = iris.target_names[iris.target]
    == ''virginica'' X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)  log_reg
    = LogisticRegression(random_state=42) log_reg.fit(X_train, y_train) [PRE79] X_new
    = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector y_proba
    = log_reg.predict_proba(X_new) decision_boundary = X_new[y_proba[:, 1] >= 0.5][0,
    0]  plt.plot(X_new, y_proba[:, 0], "b--", linewidth=2,          label="Not Iris
    virginica proba") plt.plot(X_new, y_proba[:, 1], "g-", linewidth=2, label="Iris
    virginica proba") plt.plot([decision_boundary, decision_boundary], [0, 1], "k:",
    linewidth=2,          label="Decision boundary") [...] # beautify the figure:
    add grid, labels, axis, legend, arrows, and samples plt.show() [PRE80] >>> decision_boundary
    `np.float64(1.6516516516516517)` `>>>` `log_reg``.``predict``([[``1.7``],` `[``1.5``]])`
    `` `array([ True, False])` `` [PRE81] ``[Figure 4-25](#logistic_regression_contour_plot)
    shows the same dataset, but this time displaying two features: petal width and
    length. Once trained, the logistic regression classifier can, based on these two
    features, estimate the probability that a new flower is an *Iris virginica*. The
    dashed line represents the points where the model estimates a 50% probability:
    this is the model’s decision boundary. Note that it is a linear boundary.⁠^([16](ch04.html#id1613))
    Each parallel line represents the points where the model outputs a specific probability,
    from 15% (bottom left) to 90% (top right). All the flowers beyond the top-right
    line have over a 90% chance of being *Iris virginica*, according to the model.  ![Contour
    plot of logistic regression showing decision boundary and probability lines for
    classifying Iris virginica based on petal width and length.](assets/hmls_0425.png)  ######
    Figure 4-25\. Linear decision boundary    ###### Note    The hyperparameter controlling
    the regularization strength of a Scikit-Learn `LogisticRegression` model is not
    `alpha` (as in other linear models), but its inverse: `C`. The higher the value
    of `C`, the *less* the model is regularized.    Just like the other linear models,
    logistic regression models can be regularized using ℓ[1] or ℓ[2] penalties. Scikit-Learn
    actually adds an ℓ[2] penalty by default.`` [PRE82]` [PRE83][PRE84][PRE85]`` [PRE86]
    X = iris.data[["petal length (cm)", "petal width (cm)"]].values y = iris["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)  softmax_reg
    = LogisticRegression(C=30, random_state=42) softmax_reg.fit(X_train, y_train)
    [PRE87] >>> softmax_reg.predict([[5, 2]]) `array([2])` `>>>` `softmax_reg``.``predict_proba``([[``5``,`
    `2``]])``.``round``(``2``)` `` `array([[0\.  , 0.04, 0.96]])` `` [PRE88]` [PRE89][PRE90]`
    [PRE91] [PRE92][PRE93][PRE94][PRE95] [PRE96]`py [PRE97]`py` [PRE98]`py`` [PRE99]`py[PRE100][PRE101][PRE102]
    [PRE103][PRE104][PRE105][PRE106][PRE107]`` [PRE108][PRE109][PRE110] [PRE111]`py[PRE112]`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3][PRE4] [PRE5][PRE6]`` [PRE7][PRE8]``py[PRE9] 导入matplotlib.pyplot模块作为plt，并使用plt.plot绘制新数据X_new和预测值y_predict的红色实线，并添加标签“Predictions”。接着使用plt.plot绘制原始数据X和y的蓝色点。
    [...]  # 美化图形：添加标签、坐标轴、网格和图例 plt.show() [PRE10] >>> from sklearn.linear_model
    import LinearRegression `>>>` `lin_reg` `=` `LinearRegression()` [PRE11]` `>>>`
    `lin_reg` `. `intercept_` `, `lin_reg` `. `coef_` [PRE12] [PRE13]`` [PRE14][PRE15][PRE16][PRE17]
    >>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6) `>>>`
    `theta_best_svd` `` `array([[3.69084138],`  `[3.32960458]])` `` [PRE18]`` [PRE19]
    >>> np.linalg.pinv(X_b) @ y `array([[3.69084138],`  `[3.32960458]])` [PRE20]`
    [PRE21][PRE22][PRE23][PRE24][PRE25] [PRE26]`py` [PRE27]  [PRE28] ``## 计算复杂度    正则方程计算**X**^⊺
    **X**的逆，这是一个(*n* + 1) × (*n* + 1)的矩阵（其中*n*是特征的数量）。逆矩阵的计算复杂度通常是大约*O*(*n*^(2.4))到*O*(*n*³)，具体取决于实现方式。换句话说，如果你将特征的数量加倍，计算时间大约会增加2^(2.4)
    = 5.3到2³ = 8。    Scikit-Learn的`LinearRegression`类使用的SVD方法大约是*O*(*n*²)。如果你将特征的数量加倍，计算时间大约会增加4。    ######
    警告    当特征数量增长很大时（例如，100,000），正则方程和SVD方法都会变得非常慢。从积极的一面来看，它们与训练集实例的数量呈线性关系（它们是*O*(*m*))，因此它们可以有效地处理大型训练集，前提是它们可以适应内存。    此外，一旦你训练了你的线性回归模型（使用正则方程或任何其他算法），预测就会非常快：计算复杂度与你要进行预测的实例数量和特征数量呈线性关系。换句话说，对两倍多的实例（或两倍多的特征）进行预测将花费大约两倍的时间。    现在我们将探讨一种非常不同的训练线性回归模型的方法，这种方法更适合于特征数量很大或训练实例太多而无法适应内存的情况。``
    [PRE29]`  [PRE30][PRE31][PRE32]` [PRE33][PRE34][PRE35] [PRE36]`py[PRE37]`py[PRE38]`py[PRE39][PRE40][PRE41]`py
    rng = np.random.default_rng(seed=42) m = 200  # 实例数量 X = 6 * rng.random((m, 1))
    - 3 y = 0.5 * X ** 2 + X + 2 + rng.standard_normal((m, 1)) [PRE42]` >>> from sklearn.preprocessing
    import PolynomialFeatures `>>>` `poly_features` `=` `PolynomialFeatures(``degree``=``2``,`
    `include_bias``=``False``)` [PRE43] `X_poly`现在包含原始特征`X`及其平方。现在我们可以将`LinearRegression`模型拟合到扩展后的训练数据（[图4-13](#quadratic_predictions_plot)）：    [PRE44]
    `>>>` `lin_reg` `. `intercept_` `, `lin_reg` `. `coef_` `` `(array([2.00540719]),
    array([[1.11022126, 0.50526985]]))` `` [PRE45]  [PRE46] ``![散点图显示多项式回归模型的预测结果，用红色曲线拟合数据点，说明x1和y之间的关系。](assets/hmls_0413.png)  ######
    图4-13\. 多项式回归模型预测    模型估计 <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <mn>0.56</mn> <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>0.93</mn> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <mn>1.78</mn></mrow> ，而实际上原始函数是 <mrow><mi>y</mi> <mo>=</mo> <mn>0.5</mn>
    <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow> <mn>2</mn></msup> <mo>+</mo>
    <mn>1.0</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mn>2.0</mn> <mo>+</mo>
    <mtext>高斯噪声</mtext></mrow> 。    注意，当存在多个特征时，多项式回归能够找到特征之间的关系，这是普通线性回归模型无法做到的。这是由于`PolynomialFeatures`还添加了给定度数的所有特征组合。例如，如果有两个特征*a*和*b*，`PolynomialFeatures`的`degree=3`不仅添加了特征*a*²、*a*³、*b*²和*b*³，还添加了组合*ab*、*a*²*b*和*ab*²。    ######
    警告    `PolynomialFeatures(degree=*d*)`将包含*n*个特征的数组转换为包含(*n* + *d*)! / *d*!*n*!个特征的数组，其中*n*!是*n*的阶乘，等于1
    × 2 × 3 × ⋯ × *n*，注意特征数量的组合爆炸！`` [PRE47]` [PRE48][PRE49][PRE50][PRE51][PRE52]`py[PRE53]py
    # 学习曲线    如果你执行高阶多项式回归，你可能会比使用普通线性回归更好地拟合训练数据。例如，[图4-14](#high_degree_polynomials_plot)将300次多项式模型应用于前面的训练数据，并将其与纯线性模型和二次模型（二次多项式）进行比较。注意300次多项式模型是如何围绕训练实例进行摆动，以尽可能接近训练实例的。  ![比较线性（1次）、二次（2次）和300次多项式回归模型的图表，说明高阶多项式模型如何紧密跟随数据点，导致过拟合。](assets/hmls_0414.png)  ######
    图4-14\. 高阶多项式回归    这种高阶多项式回归模型严重过拟合了训练数据，而线性模型则欠拟合了数据。在这种情况下，表现最好的模型是二次模型，这是有道理的，因为数据是使用二次模型生成的。但一般来说，你不会知道生成数据的功能是什么，那么你如何决定你的模型应该有多复杂？你如何判断你的模型是否过拟合或欠拟合数据？    在[第2章](ch02.html#project_chapter)中，你使用了交叉验证来估计模型的一般化性能。如果一个模型在训练数据上表现良好，但在交叉验证指标上泛化性能差，那么你的模型就是过拟合。如果它在两者上都表现不佳，那么它就是欠拟合。这是判断模型是否过于简单或过于复杂的一种方法。    另一种判断方法是查看*学习曲线*，这是模型训练误差和验证误差作为训练迭代函数的图表：只需在训练过程中定期评估模型在训练集和验证集上的性能，并绘制结果。如果模型不能增量训练（即如果它不支持`partial_fit()`或`warm_start`），那么你必须多次在训练集的逐渐增大的子集上训练它。    Scikit-Learn有一个有用的`learning_curve()`函数可以帮助你完成这项工作：它使用交叉验证来训练和评估模型。默认情况下，它会在训练集的增大小子集上重新训练模型，但如果模型支持增量学习，你可以在调用`learning_curve()`时设置`exploit_incremental_learning=True`，这样它就会增量训练模型。该函数返回评估模型的训练集大小，以及它为每个大小和每个交叉验证折数测量的训练和验证分数。让我们使用这个函数来查看普通线性回归模型的学习曲线（见[图4-15](#underfitting_learning_curves_plot)）：    [PRE54]`py  ![学习曲线的线形图显示随着训练集大小的增加，训练集和验证集的均方根误差（RMSE）逐渐降低并趋于平稳，表明欠拟合。](assets/hmls_0415.png)  ######
    图4-15\. 学习曲线    这个模型欠拟合，它对数据来说太简单了。我们如何判断呢？好吧，让我们看看训练误差。当训练集中只有一个或两个实例时，模型可以完美地拟合它们，这就是为什么曲线从零开始。但随着新实例被添加到训练集中，模型无法完美地拟合训练数据，这既是因为数据有噪声，也是因为它根本不是线性的。因此，训练数据的误差会上升，直到达到一个平台期，此时添加新实例到训练集中不会使平均误差变得更好或更差。现在让我们看看验证误差。当模型在非常少的训练实例上训练时，它无法正确泛化，这就是为什么验证误差最初相当大。然后，随着模型被展示更多的训练示例，它就会学习，因此验证误差会慢慢下降。然而，一旦再次使用直线无法很好地模拟数据，所以误差最终会达到一个平台期，非常接近另一个曲线。    这些学习曲线是欠拟合模型的典型特征。两个曲线都达到了平台期；它们很接近，而且相当高。    ######
    小贴士    如果你发现模型欠拟合训练数据，添加更多的训练实例不会有所帮助。你需要使用更好的模型或提出更好的特征。    现在我们来看看同一数据上10次多项式模型的学习曲线（[图4-16](#learning_curves_plot)）：    [PRE55]`py  ![10次多项式模型的学习曲线显示随着训练集大小的增加，均方根误差（RMSE）逐渐降低，验证误差趋于稳定。](assets/hmls_0416.png)  ######
    图4-16\. 10次多项式模型的学习曲线    这些学习曲线看起来有点像之前的那些，但有两个非常重要的区别：    *   训练数据的误差比之前低得多。           *   曲线之间存在差距。这意味着模型在训练数据上的表现比在验证数据上好，这是过拟合模型的标志。然而，如果你使用更大的训练集，这两个曲线将继续接近。              ######
    小贴士    改善过拟合模型的一种方法是将更多的'
