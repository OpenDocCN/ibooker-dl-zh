- en: 6 Beyond natural language processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 超越自然语言处理
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How transformer layers work on data other than text
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器层如何在除文本以外的数据上工作
- en: Helping LLMs to write working software
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助LLMs编写可工作的软件
- en: Tweaking LLMs so they understand mathematical notation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整LLMs以便它们理解数学符号
- en: How transformers replace the input and output steps to work with images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何让变压器替换输入和输出步骤以处理图像
- en: While modeling natural language was the transformers’ primary purpose, machine
    learning researchers quickly discovered they could predict anything involving
    data sequences. Transformers view a sentence as a sequence of tokens and either
    produce a related sequence of tokens, such as a translation from one language
    to another, or predict the following tokens in a sequence, such as when answering
    questions or acting like a chatbot. While sequence modeling and prediction are
    potent tools for interpreting and generating natural language, natural language
    is the only domain where LLMs can be helpful.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然建模自然语言是变压器的首要目的，但机器学习研究人员很快发现他们可以预测涉及数据序列的任何事物。变压器将一个句子视为一个标记序列，并产生一个相关的标记序列，例如从一种语言到另一种语言的翻译，或者预测序列中的下一个标记，例如在回答问题或充当聊天机器人时。虽然序列建模和预测是解释和生成自然语言的强大工具，但自然语言是LLMs唯一能够提供帮助的领域。
- en: Many data types, other than human language, can be represented as a sequence
    of tokens. Source code used to implement software is one example. Instead of the
    words and syntax you would expect to see in English, source code is written in
    a computer programming language like Python. Source code has its own structure
    that describes the operations a software developer wants a computer to perform.
    Like human language, the tokens in the source code have meaning according to the
    language used and the context in which they appear. If anything, source code is
    more highly structured and specific than human language. A programming language
    with shades of ambiguity and meaning would be challenging for a computer to interpret
    and harder for others to modify and maintain.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据类型，除了人类语言，都可以表示为标记序列。用于实现软件的源代码就是一个例子。与您期望在英语中看到的单词和语法不同，源代码是用Python这样的计算机编程语言编写的。源代码有其自己的结构，描述了软件开发人员希望计算机执行的操作。与人类语言一样，源代码中的标记根据所使用的语言和它们出现的上下文具有意义。如果有什么不同的话，源代码比人类语言更加结构化和具体。具有模糊性和意义的编程语言对计算机的解释以及其他人对其进行修改和维护都会更具挑战性。
- en: 'Source code, or simply “code” (which is how we’ll refer to it from here on),
    is just one example of how LLMs and transformers work with data that is not natural
    language. Almost any data you can recast as a sequence of tokens can use transformers
    and the many lessons we have learned about how LLMs work. This chapter will review
    three examples that become progressively less like natural language: code, mathematics,
    and computer vision.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码，或者简单地说“代码”（这是我们接下来将如何称呼它的），只是LLMs和变压器如何处理非自然语言数据的例子之一。几乎任何可以重新铸造成标记序列的数据都可以使用变压器以及我们关于LLMs工作方式所学到的大量经验。本章将回顾三个例子，这些例子逐渐不像自然语言：代码、数学和计算机视觉。
- en: Each of these three different types of data, known as data *modalities*, will
    require a new way of looking at a transformer’s inputs or outputs. However, in
    all cases, the transformer itself will remain unchanged. We will still stack multiple
    transformer layers on top of each other to build a model, and we will continue
    to train the transformer layers using gradient descent. Code, being the most similar
    to natural language, does not require too many changes. To make a code LLM work
    well, though, we will change how the outputs of the LLM generate subsequent tokens.
    Next, we will look at mathematics, where we need to change tokenization to get
    an LLM to succeed at basic operations such as addition. Finally, for computer
    vision, which concerns working with images and performing tasks such as object
    detection and identification, we will modify both the inputs and outputs, showing
    how you can convert a very different type of data into a sequence by replacing
    the concept of tokens entirely. We show the parts of LLMs that you must modify
    to work with each data modality in figure [6.1](#fig__codeMathCV).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种不同的数据类型，被称为数据*模态*，将需要一种新的方式来观察transformer的输入或输出。然而，在所有情况下，transformer本身将保持不变。我们仍然会在彼此之上堆叠多个transformer层来构建模型，并且我们将继续使用梯度下降来训练transformer层。代码与自然语言最相似，不需要太多改变。但是，为了使代码LLM工作得更好，我们将改变LLM生成后续标记的方式。接下来，我们将探讨数学，我们需要改变标记化以使LLM在基本操作，如加法中取得成功。最后，对于计算机视觉，它涉及处理图像并执行诸如对象检测和识别等任务，我们将修改输入和输出，展示如何通过完全替换标记的概念来将非常不同的数据类型转换为序列。我们在图[6.1](#fig__codeMathCV)中展示了必须修改以与每种数据模态一起工作的LLM的部分。
- en: '![figure](../Images/CH06_F01_Boozallen.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH06_F01_Boozallen.png)'
- en: Figure 6.1 If we break an LLM into three primary components—input (tokenization),
    transformation (transformers), and output generation (unembedding)—we can use
    new data modalities by changing at least one of the input or output components.
    Meanwhile, the transformer does not require modification for most use cases because
    it is general-purpose.
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1 如果我们将LLM分解为三个主要组件——输入（标记化）、转换（transformers）和输出生成（反嵌入）——我们可以通过更改至少一个输入或输出组件来使用新的数据模态。同时，对于大多数用例，transformer本身不需要修改，因为它是一般性的。
- en: In working with all three new types of data, we must solve a common problem.
    How do we give the LLM or transformer the ability to use knowledge related to
    that specific subject area? We commonly handle this by integrating external software
    into the LLM. You can think of these external software components as tools. Similarly
    to how you need a hammer to drive a nail through a piece of wood, LLMs can benefit
    from using tools to achieve end goals. Tools built for code will help us improve
    coding LLMs. Knowing how humans do math and the tools we use to automate math
    will help us make better math LLMs. Understanding how we represent images as pixels
    (which we ultimately transform into sequences of numbers representing the amount
    of red, green, and blue in one part of an image) will allow us to convert them
    into sequences for the LLM. As you think about the specific knowledge related
    to your work where LLMs have not yet been applied, you will be able to identify
    the unique characteristics of the data you work with to modify an LLM to better
    operate with data from that domain of knowledge.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理所有三种新的数据类型时，我们必须解决一个共同的问题。我们如何赋予LLM或transformer使用与特定学科领域相关的知识的能力？我们通常通过将外部软件集成到LLM中来处理这个问题。你可以将这些外部软件组件视为工具。类似于你需要一把锤子来将钉子敲入一块木头，LLM也可以通过使用工具来实现最终目标。为代码构建的工具将帮助我们改进编码LLM。了解人类如何进行数学以及我们用来自动化数学的工具将帮助我们制作更好的数学LLM。理解我们如何将图像表示为像素（我们最终将它们转换为表示图像某一部分中红色、绿色和蓝色数量的数字序列）将使我们能够将它们转换为LLM的序列。当你思考与你工作相关的特定知识，而LLM尚未应用到的领域时，你将能够识别你所处理数据的独特特征，以便修改LLM以更好地与该知识领域的数据进行交互。
- en: 6.1 LLMs for software development
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 软件开发的LLM
- en: We’ve already briefly discussed that LLMs can write source code for software.
    In chapter [4](../Text/chapter-4.html), we asked ChatGPT to write some Python
    code for calculating the mathematical constant ![equation image](../Images/eq-chapter-6-13-1.png).
    Next, we asked it to convert that code into an obscure language called Modula-3\.
    Software was one of the first things people discovered LLMs could help with as
    a relatively natural consequence of how programming works. Programming languages
    are designed to be read and written by humans like text! Consequently, we can
    generate code without changing the tokenization process. Everything we have discussed
    about constructing LLMs applies equally to code and human languages.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经简要讨论过，LLMs 可以编写软件的源代码。在第 [4](../Text/chapter-4.html) 章中，我们要求 ChatGPT 编写一些用于计算数学常数
    ![equation image](../Images/eq-chapter-6-13-1.png) 的 Python 代码。接下来，我们要求它将这段代码转换成一种名为
    Modula-3 的晦涩语言。软件是人们最早发现 LLMs 可以帮助解决的事情之一，这是编程工作相对自然的结果。编程语言被设计成像文本一样由人类阅读和编写！因此，我们可以在不改变标记化过程的情况下生成代码。我们讨论的所有关于构建
    LLMs 的内容同样适用于代码和人类语言。
- en: We can see this by looking at ChatGPT’s tokenization of two similar code segments
    for Python and Java in figure [6.2](#fig__pythonJavaTokens). Here, we use shades
    of grey to show the OpenAI tokenizer ([https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)),
    which breaks code into different tokens. While the same token might have a different
    color in each example, we can focus on how the tokenizer breaks code into tokens
    and the similarities between both examples. These include things like
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看 ChatGPT 对 Python 和 Java 两种类似代码段的标记化来了解这一点，如图 [6.2](#fig__pythonJavaTokens)
    所示。在这里，我们使用不同灰度来表示 OpenAI 标记化器 ([https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer))，它将代码分解成不同的标记。虽然每个例子中相同的标记可能颜色不同，但我们关注的是标记化器如何将代码分解成标记，以及两个例子之间的相似性。这些包括如下内容：
- en: The indentation for each line of code
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每行代码的缩进
- en: The `x` and `i` variables (in most cases)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量 `x` 和 `i`（在大多数情况下）
- en: The function name and return statement
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数名和返回语句
- en: The operators, such as `+=`
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作符，例如 `+=`
- en: These similarities make it far easier for an LLM to correlate the similarity
    between each piece of code. The similarities also mean that the LLM shares information
    between programming languages with common naming, syntax, and coding practices
    during training.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些相似性使得 LLM 能够更容易地关联每段代码之间的相似性。相似性还意味着，在训练过程中，LLM 在具有共同命名、语法和编码实践的编程语言之间共享信息。
- en: '![figure](../Images/CH06_F02_Boozallen.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F02_Boozallen.png)'
- en: Figure 6.2 Two similar samples of code written in the programming languages
    Python (left) and Java (right). These show how byte-pair encoding can identify
    similar tokens across different languages. The boxes show individual tokens. Standard
    tokenization methods for human languages do a reasonable job on code since it
    has many similarities to natural language.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.2 以编程语言 Python（左）和 Java（右）编写的两个类似代码样本。这展示了字节对编码如何在不同语言中识别相似的标记。方框显示了单个标记。标准的人类语言标记化方法在代码上做得相当合理，因为代码与自然语言有很多相似之处。
- en: Software developers are encouraged to use meaningful variable names that reflect
    a variable’s role or purpose in the programs they write. Variables named like
    `initValue` are broken up into two tokens for `init` and `Value`, using the same
    tokens to represent natural language text where the prefix “init” of the word
    “Value” occurs. So not only do we share information between programming languages
    with similar syntax, but we also share information about the context and intention
    of code via variable names. LLMs also benefit from the code comments that programmers
    add to describe complex parts of the code for themselves or other programmers.
    In figure [6.3](#fig__javaCommented), we have the Java version repeated with a
    change in the variable name and a descriptive (but unnecessary in real life) comment
    at the top of the function.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励软件开发者使用有意义的变量名，这些变量名反映了变量在编写的程序中的作用或目的。像 `initValue` 这样的变量名被拆分为两个标记 `init`
    和 `Value`，使用相同的标记来表示自然语言文本，其中单词 “Value” 的前缀 “init” 出现。因此，我们不仅可以在具有相似语法的编程语言之间共享信息，还可以通过变量名共享关于代码上下文和意图的信息。LLMs
    也可以从程序员添加的代码注释中受益，这些注释用于描述代码的复杂部分，以便于自己或其他程序员理解。在图 [6.3](#fig__javaCommented)
    中，我们重复了 Java 版本，并更改了变量名，并在函数顶部添加了一个描述性（但在现实生活中并非必需）的注释。
- en: '![figure](../Images/CH06_F03_Boozallen.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F03_Boozallen.png)'
- en: Figure 6.3 Code written in Java, including a comment describing what the code
    does. Because (good) code (hopefully) has a lot of comments, there is a natural
    mix of natural language and code for the LLM to use to obtain information. When
    variables have descriptive names, it becomes easier for the model to correlate
    information between the code and the intent described in comments and variable
    names.
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3 用Java编写的代码，包括描述代码功能的注释。由于（好的）代码（希望）有很多注释，因此自然语言和代码的自然混合为LLM提供了信息。当变量有描述性的名称时，模型更容易在代码和注释及变量名称中描述的意图之间关联信息。
- en: In most cases, we get the same tokens between code and comments, linking human
    and programming languages together since they use the same representation. Whether
    we are working with a programming language or natural language, we get the same
    tokens and embeddings. The beauty of this is that an LLM will reuse information
    about natural languages to capture the meaning of the source code, much like human
    programmers do.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，代码和注释之间得到相同的标记，将人类和编程语言联系起来，因为它们使用相同的表示。无论是处理编程语言还是自然语言，我们都得到相同的标记和嵌入。这种美妙的性质是，LLM将重用关于自然语言的信息来捕捉源代码的意义，就像人类程序员一样。
- en: In each case, we see that the tokenization is not perfect for the code. There
    are edge cases where the LLM’s tokenizer does not convert the data types in the
    code to the same token. For example, you can see that the token for `(double`
    in the function argument is handled differently from the token for `double` in
    the function body. However, these differences are similar to the problems we already
    see in LLMs for natural language, where different cases of punctuation around
    a word like “hello ”, “hello.”, and “hello!” are interpreted as different tokens.
    Since LLMs can handle these minor differences, it makes sense that they can also
    handle the same problem for code. The problem is, in many ways, easier for an
    LLM to handle in code because code is case sensitive, so we do not need to worry
    about textual situations like “hello” and “Hello” being inappropriately mapped
    to different tokens. In code, “hello” and “Hello” would be separate and distinct
    variable or function names. Treating them as separate tokens is correct because
    the programming language treats them as different elements.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，我们都看到代码的标记化并不完美。存在一些边缘情况，LLM的标记化器没有将代码中的数据类型转换为相同的标记。例如，你可以看到函数参数中的`(double`标记与函数体中的`double`标记的处理方式不同。然而，这些差异与我们在自然语言LLM中已经看到的问题类似，即围绕像“hello
    ”、“hello.”和“hello!”这样的单词的不同标点符号被解释为不同的标记。由于LLM可以处理这些细微的差异，因此它们也可以处理相同的问题，即代码问题。问题是，在许多方面，LLM处理代码问题更容易，因为代码是区分大小写的，所以我们不需要担心像“hello”和“Hello”这样的文本情况被不适当地映射到不同的标记。在代码中，“hello”和“Hello”将是不同的、独立的变量或函数名。将它们视为不同的标记是正确的，因为编程语言将它们视为不同的元素。
- en: Code generation is particularly interesting from an application perspective
    because of the various opportunities for self-validation. We can apply all the
    lessons on supervised fine tuning (SFT) and reinforcement learning with human
    feedback (RLHF) from chapter [5](../Text/chapter-5.html) to make an LLM an effective
    coding agent.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用角度来看，代码生成特别有趣，因为存在各种自我验证的机会。我们可以应用第[5](../Text/chapter-5.html)章中关于监督微调（SFT）和强化学习（RLHF）的所有教训，使LLM成为一个有效的编码代理。
- en: 6.1.1 Improving LLMs to work with code
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 改进LLM以处理代码
- en: 'The first step to improving an LLM for code is ensuring that code examples
    are present within the initial training data. Due to the nature of the internet,
    most LLM developers have already done this: code examples are frequent online
    and naturally make their way into everyone’s training datasets.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 提高代码型大型语言模型（LLM）的第一步是确保初始训练数据中包含代码示例。由于互联网的特性，大多数LLM开发者已经完成了这一步：代码示例在网络上很常见，并自然地融入了每个人的训练数据集中。
- en: Improving the results then becomes an opportunity to apply SFT, where we collect
    additional code examples and fine-tune our LLM on the given code examples. Open
    source repositories like GitHub, which contain significant volumes of code, make
    obtaining a large amount of code especially easy. Code collected from sources
    such as GitHub forms the basis of a fine-tuning dataset for LLMs that interpret
    and produce code.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 提高结果成为应用SFT的机会，我们收集额外的代码示例，并在给定的代码示例上微调我们的LLM。像GitHub这样的开源仓库，包含大量的代码，使得获取大量代码变得特别容易。从GitHub等来源收集的代码构成了用于解释和生成代码的LLM微调数据集的基础。
- en: The more interesting case is using RLHF to improve a model’s utility for writing
    code. Again, there are many tools and datasets available that make it possible
    to build a decent RLHF dataset for a coding assistant. Sources like Stack Overflow
    allow users to enter questions, provide a facility for other people to give answers
    to these questions, and include a system where other users vote on the best answers.
    Data sources include coding competitions like CodeJam, which provide many example
    solutions to a specific coding problem. Incorporating information from data sources
    like these is shown in figure [6.4](#fig__RLHF_for_code).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的情况是使用RLHF来提高模型编写代码的实用性。同样，有许多工具和数据集可用，使得为编码助手构建一个不错的RLHF数据集成为可能。像Stack Overflow这样的来源允许用户提出问题，为其他人提供回答这些问题的便利，并包括一个其他用户对最佳答案进行投票的系统。数据来源包括像CodeJam这样的编码竞赛，它们为特定的编码问题提供了许多示例解决方案。如图[6.4](#fig__RLHF_for_code)所示，将这些数据源的信息纳入其中。
- en: 'Like all good machine learning solutions, you get the best results if you create
    and label your own data specific to your task. It is rumored that OpenAI did this
    for generating code, hiring contractors to complete coding tasks as part of creating
    the data for their system [1]. Regardless of how training and fine-tuning data
    is collected, the overall strategy remains the same: use standard tokenizers and
    SFT with RLHF to make an LLM tailored to generate code. This recipe has been used
    successfully to produce LLMs such as Code Llama [2] and StarCoder [3].'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 就像所有优秀的机器学习解决方案一样，如果你为你的任务创建并标记自己的数据，你将获得最佳结果。据传闻，OpenAI就是这样做的，为了生成代码，他们雇佣承包商完成编码任务，作为创建他们系统数据的一部分
    [1]。无论训练和微调数据是如何收集的，整体策略保持不变：使用标准的标记化工具和强化学习与人类反馈（RLHF）来定制一个用于生成代码的LLM。这个方法已经被成功用于生成像Code
    Llama [2]和StarCoder [3]这样的LLM。
- en: '![figure](../Images/CH06_F04_Boozallen.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH06_F04_Boozallen.png)'
- en: Figure 6.4 Developing an LLM for code applies multiple rounds of fine-tuning.
    Standard training procedures, such as those described in chapter [4](../Text/chapter-4.html),
    produce an initial base LLM. Using a large amount of code, SFT creates an LLM
    that works well with code. Including RLHF as a second fine-tuning step improves
    the LLM’s ability to produce code.
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4 开发用于代码的LLM需要多轮微调。标准的训练程序，如第[4](../Text/chapter-4.html)章所述，产生一个初始的基础LLM。使用大量的代码，SFT创建了一个与代码配合良好的LLM。将RLHF作为第二次微调步骤可以提升LLM生成代码的能力。
- en: 6.1.2 Validating code generated by LLMs
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 验证LLM生成的代码
- en: 'LLMs are particularly useful for code generation because there is an objective
    and easy-to-run verification step: attempting to compile the code into an executable
    program [4]. When generating natural language, it is challenging to check the
    correctness of the output generated by an LLM because natural language can be
    subjective. There isn’t an automated way to check the truthfulness or veracity
    of the output generated by an LLM. However, when generating code, simply checking
    whether the code compiles successfully into an executable is a good first step
    and catches a large portion of the incorrect code. Some commercial products take
    this a step further and integrate tools such as compilers (software that transforms
    source code into executables) and visualization tools into their backend. For
    example, ChatGPT can check whether the code it writes compiles before returning
    it to the user. If the code doesn’t pass this verification step, ChatGPT will
    try to generate different code for the prompt it received. If the model cannot
    create valid code to compile, it will warn the user of this fact.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）在代码生成方面特别有用，因为存在一个客观且易于运行的验证步骤：尝试将代码编译成可执行程序[4]。在生成自然语言时，检查由LLM生成的输出是否正确具有挑战性，因为自然语言具有主观性。没有自动化的方法来检查LLM生成的输出的真实性或准确性。然而，在生成代码时，简单地检查代码是否成功编译成可执行程序是一个很好的第一步，并且可以捕捉到大量错误代码。一些商业产品更进一步，将编译器（将源代码转换为可执行文件的软件）和可视化工具等工具集成到其后端。例如，ChatGPT可以在将代码返回给用户之前检查它所编写的代码是否可编译。如果代码未通过这一验证步骤，ChatGPT将尝试为它收到的提示生成不同的代码。如果模型无法创建可以编译的有效代码，它将警告用户这一事实。
- en: Beyond checking whether code can compile, LLMs are increasingly able to create
    methods for validating functional correctness. Many code generation tools utilize
    LLM to generate unit tests, which are tiny programs that provide sample input
    into generated code and validate that it produces the correct result. In some
    cases, these capabilities require the developer to describe the test cases that
    they want the LLM to generate, and the LLM creates an initial implementation as
    a starting point for further testing.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了检查代码是否可编译之外，LLM越来越能够创建验证功能正确性的方法。许多代码生成工具利用LLM生成单元测试，这些是提供样本输入到生成的代码中的小型程序，并验证它是否产生正确的结果。在某些情况下，这些功能需要开发者描述他们希望LLM生成的测试用例，然后LLM创建一个初始实现作为进一步测试的起点。
- en: Code is particularly special because multiple ways exist to validate its output
    beyond just compilation. For example, code compilation can’t happen until the
    LLM finishes generating its response.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 代码特别特殊，因为除了编译之外，还有多种方式可以验证其输出。例如，LLM完成生成响应之前，代码编译是无法发生的。
- en: Considering that LLMs are expensive to run, and we don’t want to keep a user
    waiting too long for output, it would be ideal if the LLM could correct errors
    before completing a large generation. Again, applying the lessons from chapter
    5, we can use a syntax parser to check whether the code is incorrect before completing
    the entire generation process. If portions of the output code fail a basic syntax
    check, we can instruct the LLM to regenerate just that faulty portion of code.
    We show the basic process behind this in figure [6.5](#fig__codeSyntaxCheck),
    where the LLM performs a check on a per-token basis instead of waiting for the
    generation to complete before checking the code using compilation. The syntax
    check is less expensive and can happen faster than compilation, but it does not
    validate that a compiler can turn the code into a working executable program.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到LLM运行成本高昂，我们不想让用户等待太长时间才能得到输出，如果LLM能够在完成大量生成之前纠正错误，那就太理想了。再次应用第5章的教训，我们可以在完成整个生成过程之前使用语法解析器来检查代码是否错误。如果输出代码的部分未能通过基本的语法检查，我们可以指示LLM仅重新生成那个有缺陷的代码部分。我们在图[6.5](#fig__codeSyntaxCheck)中展示了这一基本过程，其中LLM在基于每个标记的基础上进行检查，而不是在生成完成之前使用编译来检查代码。语法检查成本较低，可以更快地发生，但它并不能验证编译器能否将代码转换为可工作的可执行程序。
- en: '![figure](../Images/CH06_F05_Boozallen.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F05_Boozallen.png)'
- en: Figure 6.5 A Python code example where the current tokens `if(A > B)` have been
    generated. If the next token produced by the LLM is a newline, a syntax error
    will occur because an `if` statement must end in a colon to be valid. Running
    a syntax checker on each new token allows us to catch this error and force the
    LLM to pick an alternative token that doesn’t cause a syntax error.
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5 一个Python代码示例，其中生成了当前的标记`if(A > B)`。如果LLM接下来生成的标记是换行符，将会发生语法错误，因为一个`if`语句必须以冒号结尾才是有效的。对每个新标记运行语法检查，使我们能够捕捉到这个错误并迫使LLM选择一个不会导致语法错误的替代标记。
- en: 6.1.3 Improving code via formatting
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 通过格式化改进代码
- en: Using parsers for syntax checking and compilers to produce working executables
    makes it far easier to adapt LLMs to the new problem domain of generating code.
    However, one additional trick is helpful. We can use tools known as *code formatters*
    (also known by programmers as *linters*) to change tokenization and improve performance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用解析器进行语法检查和使用编译器生成可执行的程序，使得将LLM适应生成代码的新问题领域变得容易得多。然而，还有一个额外的技巧是有帮助的。我们可以使用被称为*代码格式化工具*（程序员也称之为*代码检查器*）的工具来改变标记化并提高性能。
- en: The problem is that there can be many ways to write code that performs the same
    functions yet is tokenized differently. Applying a linter to adjust source code
    formatting helps remove differences between two functionally equivalent, yet different
    pieces of code. While reformatting code is not a requirement to make code LLMs
    function well, it helps to avoid unnecessary redundancy that can occur. For example,
    consider the Java programming language that uses brackets to begin and end a new
    scope in a program. Various forms of white space are now nonimportant but would
    be tokenized differently, especially since the brackets are optional for a scope
    that only uses a single line of code! Figure [6.6](#fig__codeForamtter) shows
    how these different legal formats exist for the code that performs the same functions
    and how we could, ideally, convert code to a single canonical representation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，可能有多种编写执行相同功能但标记化不同的代码的方式。应用代码检查器调整源代码格式有助于消除两个功能等效但不同的代码片段之间的差异。虽然重新格式化代码不是使代码LLM良好运行的要求，但它有助于避免可能发生的不必要冗余。例如，考虑使用括号在程序中开始和结束新作用域的Java编程语言。现在各种形式的空白不再重要，但会被不同地标记化，特别是由于对于只使用单行代码的作用域来说，括号是可选的！图[6.6](#fig__codeForamtter)展示了执行相同功能的代码存在不同的合法格式，以及我们理想上如何将这些代码转换为单一的标准表示形式。
- en: '![figure](../Images/CH06_F06_Boozallen.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F06_Boozallen.png)'
- en: Figure 6.6 A Java code example of how multiple ways to format the same code
    will lead to different tokenizations, even though each is semantically identical.
    Linters are a common tool to force code to follow a specific formatting rule.
    Instead, a linter can be used to create an identical “base” form, thus avoiding
    representing unnecessary information (like spaces versus tabs).
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6 一个Java代码示例，展示了多种格式化相同代码的方式会导致不同的标记化，尽管每种方式在语义上都是相同的。代码检查器是一种常见的工具，用于强制代码遵循特定的格式化规则。相反，可以使用代码格式化工具创建一个相同的“基础”形式，从而避免表示不必要的信息（如空格与制表符）。
- en: Removing nonfunctional aspects of code is called *canonicalization*, meaning
    we convert code with formatting variations into a standard or “canonical” form.
    Here, we demonstrated a robust method of canonicalization by adding special tokens
    like `<NEW SCOPE>` that capture the fact that a new context exists for the `if`
    statement, regardless of whether it’s a single-line or multiline statement. Instead
    of adding special tokens, we can use formatting that is consistent across the
    code (e.g., always use spaces versus tabs, a newline before `{` or not). Both
    special parsing and formatting will improve the performance of a code LLM. The
    robust method, where we add special tokens, will yield better performance over
    formatting but has the added cost of writing and maintaining a custom parser for
    code that adds those special tokens. The problem of altering the tokenizer will
    be more critical in the next section when we discuss using LLMs for mathematics.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 移除代码的非功能性部分称为*规范化*，这意味着我们将具有格式变化的代码转换为标准或“规范”形式。在这里，我们通过添加特殊标记如`<NEW SCOPE>`来展示了一种稳健的规范方法，这些特殊标记捕捉到`if`语句存在新上下文的事实，无论它是单行还是多行语句。我们不仅可以添加特殊标记，还可以使用在代码中一致的格式（例如，始终使用空格而不是制表符，在`{`之前或之后换行）。特殊解析和格式化都将提高代码LLM的性能。添加特殊标记的稳健方法在格式化方面将产生更好的性能，但需要编写和维护一个用于添加这些特殊标记的自定义解析器。在下一节讨论使用LLM进行数学时，更改标记化器的问题将更加关键。
- en: 6.2 LLMs for formal mathematics
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 用于形式数学的LLM
- en: LLMs can also perform mathematical tasks that are usually quite challenging
    for humans to do successfully. These tasks are more than just performing operations
    like addition and subtraction to calculate numbers; they include formal and symbolic
    mathematics. We give an example of the kinds of formal math we are talking about
    in figure [6.7](#fig__minerva_example). You can ask these LLMs to calculate derivatives,
    limits, and integrals and write proofs. They can produce shockingly reasonable
    results.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: LLM还可以执行人类通常难以成功完成的数学任务。这些任务不仅仅是执行加法和减法等操作来计算数字；还包括形式和符号数学。我们在图[6.7](#fig__minerva_example)中给出了我们所说的形式数学的例子。你可以要求这些LLM计算导数、极限和积分，并撰写证明。它们可以产生令人震惊的合理结果。
- en: LLMs for code are practical because we can use parsers and compilers to partially
    validate their outputs. Proper tokenization is paramount for making a helpful
    LLM for mathematics. Using LLMs for math is still a particularly active area of
    research [6], so the best ways to get an LLM to perform math are not yet known.
    However, researchers have identified some problems that cluster around the tokenization
    stage of building and running an LLM.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 代码LLM是实用的，因为我们可以使用解析器和编译器来部分验证它们的输出。适当的标记化对于创建一个有用的数学LLM至关重要。使用LLM进行数学研究仍然是一个特别活跃的研究领域[6]，因此，让LLM执行数学操作的最佳方法尚不清楚。然而，研究人员已经确定了一些围绕LLM构建和运行过程中的标记化阶段的问题。
- en: '![figure](../Images/CH06_F07_Boozallen.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F07_Boozallen.png)'
- en: Figure 6.7 A symbolic math problem that the Minerva LLM can solve correctly.
    While this example mixes natural language with mathematical content, the standard
    tokenization used by many LLMs would not allow this kind of mathematical output
    and can cause some surprising problems. (Image Creative Commons licensed from
    [5])
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7 Minerva LLM可以正确解决的符号数学问题。虽然这个例子将自然语言与数学内容混合在一起，但许多LLM使用的标准标记化方法不允许这种数学输出，并可能导致一些令人惊讶的问题。（图片由Creative
    Commons授权自[5]）
- en: Note In chapter 5, we mentioned that fine-tuning can be applied multiple times,
    and math LLMs are a great example of this. Researchers often create math LLMs
    by fine-tuning code LLMs, which are created by fine-tuning general-purpose text
    LLMs. Between SFT and RLHF at each stage, as many as three to six rounds of fine-tuning
    are applied to the original downstream LLM for math LLMs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在第5章中，我们提到微调可以多次应用，数学LLM（大型语言模型）是这一点的绝佳例子。研究人员通常通过微调通用文本LLM来创建数学LLM，而通用文本LLM是通过微调代码LLM创建的。在数学LLM的每个阶段，SFT（监督式微调）和RLHF（基于人类反馈的强化学习）之间，对原始下游LLM进行多达三到六轮的微调。
- en: 6.2.1 Sanitized input
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 清理后的输入
- en: Math LLMs often suffer from input preparation that may work well for natural
    language text but degrade representations of mathematical concepts. In text, formatted
    mathematics representations often involve symbols like `{}<>;^`. Special symbols
    like these are commonly removed from training data when working with regular text.
    Preserving this information requires rewriting input parsers for tokenization
    to ensure you do not remove the data you are trying to get your model to learn
    from.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Math LLMs通常受到输入准备的影响，这些准备对于自然语言文本可能效果良好，但会降低数学概念的表示。在文本中，格式化的数学表示通常涉及像`{}<>;^`这样的符号。当处理常规文本时，这些特殊的符号通常会被从训练数据中移除。保留这些信息需要重新编写用于标记化的输入解析器，以确保您不会移除您希望模型从中学习的那些数据。
- en: Multiple representations for equivalent mathematical equations further complicate
    training LLMs to understand math in a similar way that multiple formatting may
    cause problems when processing programming languages. Several formats like TeX,
    asciimath, and MathML allow mathematical notation to be expressed using plain
    text but provide instructions for a typesetter to render equations correctly.
    These formats offer many different ways to represent the same equation. We show
    an example of this problem in figure [6.8](#fig__mathRepresentation). There are
    problems with the method of typesetting the math (i.e., how to draw the equation
    by picking TeX versus MathML) and the representation of the math (i.e., two mathematically
    equivalent ways of expressing the same thing).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于等价的数学方程的多种表示形式进一步增加了训练LLMs以类似方式理解数学的难度，就像多种格式在处理编程语言时可能引起问题一样。像TeX、asciimath和MathML这样的多种格式允许使用纯文本表达数学符号，但提供了为排版者正确渲染方程的指令。这些格式提供了许多表示同一方程的不同方式。我们在图[6.8](#fig__mathRepresentation)中展示了这个问题的一个例子。数学的排版方法（即通过选择TeX还是MathML来绘制方程）和数学的表示（即表达同一事物的两种数学上等价的方式）都存在问题。
- en: 'These are both forms of a problem that has come up a few times in our discussion
    of LLMs: different ways to represent the same thing. In the case of mathematics,
    the current preference is to keep math formatted using TeX and very similar but
    less-frequent alternatives like asciimath and to discard verbose content like
    MathML. We base this motivation on three factors:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是在我们讨论LLMs时多次出现的问题的形式：表示同一事物的不同方式。在数学的情况下，目前的偏好是保持使用TeX格式，以及非常相似但使用频率较低的替代方案，如asciimath，并丢弃像MathML这样的冗长内容。我们基于以下三个因素来支持这一动机：
- en: '![figure](../Images/CH06_F08_Boozallen.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F08_Boozallen.png)'
- en: Figure 6.8 A mathematical equation in the top-left demonstrates two different
    representation problems that occur with math. The nicely formatted math requires
    a typesetting language. TeX and MathML are two different typesetting languages
    that have vastly different text and, thus, tokenization. Separate from the typesetting
    language, there are many ways to represent the same mathematical statement.
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8 左上角的数学方程展示了数学中出现的两种不同的表示问题。格式良好的数学需要一种排版语言。TeX和MathML是两种不同的排版语言，它们在文本和标记化方面有极大的不同。除了排版语言之外，还有许多方式可以表示相同的数学陈述。
- en: TeX-based formatted math is the most common and available form of math thanks
    to publicly available sources like arXiv, which consistently uses TeX formatting.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于TeX的格式化数学是数学最常见和可用的形式，这得益于像arXiv这样的公开资源，它始终使用TeX格式。
- en: Keeping all TeX-like representations mitigates the challenge of learning multiple
    formats and, thus, very different token sets.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持所有类似TeX的表示形式可以减轻学习多个格式以及因此非常不同的标记集的挑战。
- en: The more verbose MathML uses a larger variety of tokens; thus, more computing
    resources are required to store the data associated with each unique token.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MathML的冗长性使用了更多种类的标记；因此，存储与每个唯一标记相关的数据需要更多的计算资源。
- en: Choosing TeX as a single preferred representation for math in LLMs doesn’t solve
    the fact that there are multiple ways to write equivalent equations. Determining
    which equations are the same is so difficult that researchers have proven that
    no single algorithm can determine the equivalence of two mathematical expressions.
    (We are being a little loose with our words here, given that this section is on
    *formal* mathematics, so we will point you to the source [7].) So far, the best
    answer for LLMs appears to be “let the model try to figure that out,” which has
    been reasonably successful thus far. But we wouldn’t be surprised if the developers
    of future math LLMs invest heavily in improving preprocessing by creating more
    consistent canonical representations for mathematical equations that reduce the
    variety of possible expressions for equivalent expressions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs中将TeX作为数学表示的唯一首选格式并不能解决存在多种等效方程书写方式的事实。确定哪些方程是相同的非常困难，以至于研究人员已经证明没有单个算法可以确定两个数学表达式的等价性。（鉴于本节讨论的是*形式*数学，我们在用词上稍微宽松一些，所以我们将指向来源[7]。）到目前为止，对于LLMs来说，最好的答案似乎是由模型尝试自己找出答案，这在迄今为止已经相当成功。但如果我们看到未来数学LLMs的开发者大量投资于通过创建更一致的规范表示来改进预处理，从而减少等效表达式的可能表达方式的多样性，我们也不会感到惊讶。
- en: 6.2.2 Helping LLMs understand numbers
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 帮助LLMs理解数字
- en: For most people, numbers are the more accessible part of math. You can put them
    in a calculator and get the result. Although it may be tedious, you can perform
    calculations by hand if you do not have a calculator. One follows a fixed set
    of rules to get the result. Somewhat surprisingly, LLMs have a lot of trouble
    doing that sort of rote calculation, but developers have worked to improve tokenizers’
    ability to work better with numbers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数人来说，数字是数学中更容易接触的部分。你可以把它们放入计算器中并得到结果。尽管可能有些繁琐，如果你没有计算器，你也可以手动进行计算。遵循一组固定的规则就可以得到结果。有些令人惊讶的是，LLMs在那种死记硬背的计算上有很多困难，但开发者们已经努力提高分词器与数字更好地协同工作的能力。
- en: The first problem is that the standard byte-pair encoding (BPE) algorithm produces
    tokenizers that create inconsistent tokens for numbers. For example, “1812” will
    likely be tokenized as a single token because there are references to the War
    of 1812 in thousands of documents; tokenizers will possibly break up 1811 and
    1813 into smaller numbers. To further explore why this happens, consider the initial
    string `3252+3253` and how GPT-3 and GPT-4 tokenize this string. GPT-4 will do
    a better job because it seems to tokenize numbers by starting with the first three
    digits every time, resulting in a three-digit number followed by a single-digit
    number. GPT-3 appears inconsistent because it changes the order in which it tokenizes
    numbers, as shown in figure [6.9](#fig__additionTokenProblem).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题在于标准的字节对编码（BPE）算法产生的分词器为数字创建了不一致的标记。例如，“1812”可能会被标记为一个单独的标记，因为成千上万的文档中都有关于1812年战争的引用；分词器可能会将1811和1813分解成更小的数字。为了进一步探究为什么会发生这种情况，考虑初始字符串`3252+3253`以及GPT-3和GPT-4如何分词这个字符串。GPT-4会做得更好，因为它似乎每次都以前三位数字开始分词，结果是一个三位数后面跟着一个一位数。GPT-3看起来不一致，因为它改变了解释数字的顺序，如图[6.9](#fig__additionTokenProblem)所示。
- en: '![figure](../Images/CH06_F09_Boozallen.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F09_Boozallen.png)'
- en: Figure 6.9 LLMs cannot learn to do basic arithmetic unless they tokenize digits
    consistently. In this figure, underlines denote different tokens. The tokenized
    digits might represent the tens, hundreds, or thousands place for any given number.
    GPT-3 (left) is inconsistent in how numbers get tokenized, making adding two numbers
    needlessly complex. GPT-4 is better (but not perfect) at tokenizing numbers in
    a consistent way.
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.9 LLMs如果不能一致地分词数字，就无法学习进行基本的算术。在这张图中，下划线表示不同的标记。分词的数字可能代表任何给定数字的十位、百位或千位。GPT-3（左）在分词数字方面不一致，使得加法变得没有必要地复杂。GPT-4在以一致的方式分词数字方面做得更好（但并不完美）。
- en: Now a significant problem has occurred. The “3” token for GPT-3 occurs two times
    in two different contexts, once in the thousands place (*three-thousand* two hundred
    ...) and once in the tens place (three-thousand two hundred and fifty *three*).
    For GPT-3 to correctly add these numbers, the tokenizer must properly capture
    four different digit locations. In contrast, GPT-4 uses the order for digit representations
    for each number, making it easier to get the correct result.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在出现了一个重大问题。GPT-3 的“3”标记在两个不同的上下文中出现了两次，一次在千位（*三千*二百 ...），一次在十位（三千二百五十 *三*）。为了使
    GPT-3 正确地加这些数字，分词器必须正确地捕获四个不同的数字位置。相比之下，GPT-4 使用每个数字的数字表示顺序，这使得得到正确结果更容易。
- en: People are still experimenting with different ways of changing the tokenizer
    to improve LLMs’ ability to work with numbers. If we are going to tokenize digits
    into subcomponents, the current best approach is to separate each number, like
    3252, into individual digits, like “3, 2, 5, 2” [8]. However, other alternatives
    also exist.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 人们仍在尝试不同的方法来改变分词器，以提高大型语言模型处理数字的能力。如果我们打算将数字分解成子组件，当前最佳的方法是将每个数字，如 3252，分解成单个数字，如“3,
    2, 5, 2” [8]。然而，也存在其他替代方案。
- en: Another interesting approach for representing numbers is called *xVal* [9],
    with the idea of replacing every number with the same token that represents “a
    number.” We could call this special token `NUM`, which will get mapped to a vector
    of numbers by the embedding layer we learned about in chapter [3](../Text/chapter-3.html).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种表示数字的有趣方法是称为 *xVal* [9]，其理念是将每个数字替换为表示“一个数字”的相同标记。我们可以称这个特殊标记为 `NUM`，它将被我们第
    [3](../Text/chapter-3.html) 章中提到的嵌入层映射为一个数字向量。
- en: The clever trick is to include a multiplier with each token, a second number
    multiplied against the embedded vector value. By default, the LLM uses a multiplier
    of 1 for every token. Multiplying anything by 1 does nothing. But for any `NUM`
    token we encounter, it will instead be multiplied by the original number from
    the text! This way, we can represent every possible number that might appear,
    even fractional values, including those that did not appear in the training data.
    Numbers captured in this manner are related in a simple and intuitive way. We
    show this in more detail in figure [6.10](#fig__xVal).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 智巧的做法是给每个标记包含一个乘数，即一个与嵌入向量值相乘的第二个数字。默认情况下，大型语言模型为每个标记使用乘数为 1。将任何东西乘以 1 都不会产生任何效果。但对于我们遇到的任何
    `NUM` 标记，它将被乘以文本中的原始数字！这样，我们可以表示可能出现的每一个可能的数字，包括分数值，甚至包括那些没有出现在训练数据中的数字。以这种方式捕获的数字以简单直观的方式相关联。我们将在图
    [6.10](#fig__xVal) 中更详细地展示这一点。
- en: '![figure](../Images/CH06_F10_Boozallen.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F10_Boozallen.png)'
- en: Figure 6.10 xVal uses a trick to help reduce the number of tokens and make them
    less ambiguous. By modifying how the LLM converts numbers to vectors, a single
    vector represents each number, such as the number 1\. By always using the 1 token
    and multiplying it by the number observed, we avoid many edge cases in number
    token representation, such as numbers that never appeared in the training data.
    This conversion method also makes fractional numbers like 3.14 easier to support.
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.10 xVal 使用一个技巧来帮助减少标记的数量并使它们更不模糊。通过修改大型语言模型将数字转换为向量的方式，单个向量表示每个数字，例如数字 1。通过始终使用
    1 标记并将其乘以观察到的数字，我们避免了数字标记表示中的许多边缘情况，例如那些从未出现在训练数据中的数字。这种转换方法也使得支持分数数字，如 3.14，变得更容易。
- en: Both the consistent digits and the xVal strategy share one important realization.
    We know how to represent math and simple algorithms like grade-school addition
    and multiplication. If we design the LLM to tokenize mathematics in a way that
    is more consistent with how we, as humans, do mathematical tasks, our LLMs get
    better and more consistent mathematical capabilities.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一致数字和 xVal 策略共享一个重要的认识。我们知道如何表示数学和简单的算法，如小学的加法和乘法。如果我们设计大型语言模型以更符合我们作为人类进行数学任务的方式对数学进行分词，我们的语言模型将获得更好的和更一致的数学能力。
- en: 6.2.3 Math LLMs also use tools
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 数学大型语言模型也使用工具
- en: The astute reader may have noticed that most of the tokenization problems related
    to math involve handling digits and not symbolic math. LLMs cannot do essential
    addition or subtraction without changing the tokenizer and keeping typically “bad”
    symbols like `{}<>;^`. Enabling computation by changing the way the tokenizerhandles
    numbers may seem like a minor problem. Still, it is a significant factor for good
    symbolic performance and often insufficient for handling other forms of symbolic
    math. Obtaining the best possible performance on symbolic math relies on external
    tools and playing clever tricks with LLM output.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明的读者可能已经注意到，大多数与数学相关的标记化问题都涉及处理数字而不是符号数学。LLM在改变标记化器并保留通常“不好”的符号（如`{}<>;^`）的情况下，无法进行基本的加法或减法运算。通过改变标记化器处理数字的方式来启用计算可能看起来像是一个小问题。然而，它对于良好的符号性能是一个重要因素，并且通常不足以处理其他形式的符号数学。在符号数学上获得最佳性能依赖于外部工具和巧妙地使用LLM输出。
- en: If you ever had the TI-89 calculator that could solve derivatives for you, you
    know that computers can automate calculations without LLMs. Functionally, computer
    algebra systems (CAS) can provide this functionality. A CAS implements algorithms
    to perform some (but not all) mathematical steps. Calculating derivatives is one
    of them, so having an LLM use a CAS, like Sympy, helps ensure the LLM always performs
    specific steps correctly. However, the ability to integrate a CAS like Sympy into
    an LLM does not guarantee the entire sequence of steps will be performed correctly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经使用过能够为你求解导数的TI-89计算器，你就知道计算机可以在没有LLM的情况下自动化计算。从功能上讲，计算机代数系统（CAS）可以提供这种功能。CAS实现算法来执行一些（但不是所有）数学步骤。计算导数是其中之一，因此让LLM使用CAS，如Sympy，有助于确保LLM始终正确执行特定步骤。然而，将Sympy这样的CAS集成到LLM中并不能保证整个步骤序列都会被正确执行。
- en: To validate correctness, math LLMs have begun to use a programming language
    called *Lean*. In Lean, the program is a kind of mathematical proof, and the program
    will not compile if there is an error in the proof. It effectively makes incorrect
    proof steps one type of syntax error that can then be detected. Once detected,
    as we have shown in other examples, the output can be regenerated by the LLM until
    the proof, output as a Lean program, compiles successfully, just like we show
    in section [6.1.2](#sec__code_validation).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证正确性，数学LLM已经开始使用一种名为*Lean*的编程语言。在Lean中，程序是一种数学证明，如果证明中存在错误，程序将无法编译。它有效地将错误的证明步骤视为一种语法错误，然后可以被检测到。一旦检测到，正如我们在其他示例中所示，LLM可以重新生成输出，直到证明，以Lean程序的形式输出，编译成功，就像我们在第[6.1.2](#sec__code_validation)节中展示的那样。
- en: Using Lean can guarantee that a returned proof from an LLM is 100% correct,
    but there is no guarantee that the LLM can find the proof. Notably, there may
    also be cases where the LLM might be able to solve the problem correctly but might
    not be able to express the solved problem using Lean. We diagram the logic behind
    this problem in figure [6.11](#fig__leanHard), and it boils down to the fact that
    the effectiveness of tool use in LLMs depends on the variety of examples of the
    tool’s use in training data. Since Lean is relatively new and niche, there are
    few examples of fine-tuning an LLM to use Lean effectively. People like you and
    me will need to generate those examples to produce suitable training data to teach
    an LLM how to use Lean.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Lean可以保证从LLM返回的证明是100%正确的，但并不能保证LLM能够找到这个证明。值得注意的是，也可能存在LLM能够正确解决问题但可能无法使用Lean表达所解决问题的情况。我们在图[6.11](#fig__leanHard)中展示了这个问题背后的逻辑，归结为LLM中工具使用效果取决于训练数据中工具使用示例的多样性。由于Lean相对较新且专业，很少有调整LLM以有效使用Lean的示例。像我们这样的人需要生成这些示例，以产生合适的训练数据，教会LLM如何使用Lean。
- en: '![figure](../Images/CH06_F11_Boozallen.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F11_Boozallen.png)'
- en: Figure 6.11 Given some mathematical goal, getting an LLM to use Lean (right
    path) might not result in a verifiably correct proof because it may not be effective
    at using Lean as a tool. Having the LLM produce a normal proof (left path) may
    yield a correct proof, but not a way for us to verify that it is (in)correct.
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.11 对于某个数学目标，让一个大型语言模型（LLM）使用Lean（正确路径）可能不会导致一个可验证的正确证明，因为它可能不擅长将Lean作为工具使用。让LLM生成一个普通证明（左侧路径）可能得到一个正确的证明，但不是一种让我们验证其正确性的方法。
- en: So what can you do if the LLM cannot provide verifiable proof that its math
    is correct? A trick used today is to run the LLM multiple times. Because the next
    token is selected randomly, you can potentially get a different result with a
    different answer each time you run the LLM. Whichever answer appears most frequently
    is most likely correct. This process does not guarantee the proof is correct,
    but it helps.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果LLM无法提供其数学正确的可验证证据，你该怎么办？今天使用的一个技巧是多次运行LLM。因为下一个标记是随机选择的，所以每次运行LLM时，你可能会得到不同的结果，每次的结果都不同。出现频率最高的答案最可能是正确的。这个过程不能保证证明是正确的，但它有帮助。
- en: 6.3 Transformers and computer vision
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 变压器与计算机视觉
- en: The process of translating code and math to tokens is fairly intuitive. Code
    is fundamentally text used to tell computers what to do in a highly pedantic way.
    Math is difficult to convert into tokens, but we have discussed how it is possible.
    Computer vision is a different story, where the data involved is images or videos
    represented using pixels. The idea of tokens for images seems confusing. How on
    earth could we possibly convert an image into tokens? Images typically contain
    lots of detail, and you cannot just combine a bunch of small images into one coherent
    image like you do when you string words together to form a sentence. Nevertheless,
    we can apply transformers to images if we think about tokenization as a process
    to convert any input into a sequence of numbers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码和数学转换为标记的过程相当直观。代码是基本文本，用于以高度严谨的方式告诉计算机做什么。数学转换为标记比较困难，但我们已经讨论了它是如何可能的。计算机视觉是另一回事，其中涉及的数据是使用像素表示的图像或视频。图像标记的概念似乎很令人困惑。我们怎么可能将图像转换为标记呢？图像通常包含大量细节，你不能像将单词连成句子那样将许多小图像组合成一个连贯的图像。尽管如此，如果我们将标记化视为将任何输入转换为数字序列的过程，我们仍然可以将变压器应用于图像。
- en: Note There was an approach to representing images as a combination of tiny images
    called *code books*. Code books can be useful, but not the same in the spirit
    of our discussion. Consider this a keyword nugget to explore if you want to learn
    about some older computer vision techniques.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有一种将图像表示为称为“代码簿”的微小图像组合的方法。代码簿可能很有用，但与我们的讨论精神不同。如果你想了解一些较老的计算机视觉技术，请将此视为一个关键词要点。
- en: While high-quality image recognition algorithms and image generators existed
    for many years before transformers, transformers have rapidly become one of the
    premier ways to work with images in machine learning. Both vision transformer
    (ViT) architectures that strictly use transformers, as well as mixed architecture
    models such as VQGAN and U-Net transformer that mix transformers with other types
    of data structures, have seen great success in both interpreting image-based data
    and producing amazing computer-generated images from text descriptions. It may
    seem counterintuitive that transformers perform so well in images because images
    do not look like discrete sequences of symbols like natural language, code, or
    amino acid sequences do. Still, transformers fulfill a critical role in computer
    vision by bringing global cohesion to models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在变压器出现之前，高质量的图像识别算法和图像生成器已经存在了很多年。但是，变压器迅速成为机器学习中处理图像的顶级方法之一。严格使用变压器的视觉变压器（ViT）架构，以及混合架构模型，如VQGAN和U-Net变压器，这些模型将变压器与其他类型的数据结构混合，在解释基于图像的数据和从文本描述生成惊人的计算机生成图像方面都取得了巨大成功。变压器在图像上表现如此出色似乎有些反直觉，因为图像不像自然语言、代码或氨基酸序列那样看起来像离散的符号序列。然而，变压器通过为模型带来全局凝聚力，在计算机视觉中发挥着关键作用。
- en: 6.3.1 Converting images to patches and back
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 将图像转换为补丁并反向转换
- en: Conceptually, we will replace the tokenizer and embedding process with a new
    process that outputs a sequence of vectors similar to the embedding layers we
    discussed in section [3.1.1](../Text/chapter-3.html#sec__transformer_layers).
    The prevailing approach to creating a sequence representing an image is to divide
    the image into a set of *patches*. As a result, we will replace our tokenizer
    with a *patch extractor* that returns a sequence of vectors. The output of an
    LLM uses an unembedding layer to convert vectors back into tokens. Since we have
    no tokens, we need a *patch combiner* to take the outputs of a transformer and
    merge them into one coherent image. We show this process in figure [6.12](#fig__tokensToPatches).
    Please pay special attention to the fact that the central portion of the diagram
    remains the same as it was for text-based LLMs. We reuse the same transformer
    layers and learning algorithm (gradient descent) between text and images.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，我们将用一个新的过程来替换分词器和嵌入过程，该过程输出一系列与我们在第 [3.1.1](../Text/chapter-3.html#sec__transformer_layers)
    节中讨论的嵌入层类似的向量序列。创建表示图像的序列的常用方法是将其划分为一组 *补丁*。因此，我们将用 *补丁提取器* 来替换我们的分词器，该提取器返回一系列向量。LLM
    的输出使用反嵌入层将向量转换回标记。由于我们没有标记，我们需要一个 *补丁组合器* 来将转换器的输出合并成一个连贯的图像。我们在图 [6.12](#fig__tokensToPatches)
    中展示了这个过程。请特别注意，图中的中心部分与基于文本的 LLM 的情况相同。我们在文本和图像之间重用了相同的转换器层和学习算法（梯度下降）。
- en: '![figure](../Images/CH06_F12_Boozallen.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F12_Boozallen.png)'
- en: Figure 6.12 On the left, this simplified diagram shows how text input is tokenized
    and embedded before going to the transformer. An unembedding layer then converts
    the transformer output into the desired text representation. The input and output
    will be images when performing a computer vision task. The transformer stays the
    same, but then we modify the method for breaking the image into a sequence of
    vectors to perform patch extraction instead of tokenization. The LLM produces
    image output using a patch combiner, analogous to the unembedding layer for text
    LLMs.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.12 在左侧，这个简化的图展示了文本输入在进入转换器之前是如何被分词和嵌入的。然后，反嵌入层将转换器的输出转换为所需的文本表示。当执行计算机视觉任务时，输入和输出将是图像。转换器保持不变，但我们修改了将图像分割成向量序列的方法，以执行补丁提取而不是分词。LLM
    使用补丁组合器产生图像输出，类似于文本 LLM 的反嵌入层。
- en: Since everything except the input vector sequence generation and output steps
    remains the same, we can focus on how the conversion of images to and from vectors
    works. It will be helpful to focus on the input side first.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于除了输入向量序列生成和输出步骤之外的所有内容都保持不变，我们可以专注于图像到向量以及从向量到图像的转换是如何工作的。首先关注输入端将是有帮助的。
- en: As the name *patch* implies, the patch extractor breaks up each image into asequence
    of smaller images. It is common to pick a fixed size for the patch, likea square
    of ![equation image](../Images/eq-chapter-6-82-1.png) pixels. We want a fixed
    size so that it is easy to feed into a neural network, which always processes
    data of a fixed size, and small so that they represent just a piece of the entire
    image. Breaking an image into patches is similar to breaking text into a collection
    of tokens. Each individual token isn’t informative, but when combined with other
    tokens, it makes a coherent sentence.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *补丁* 的名字所暗示的，补丁提取器将每个图像分割成一系列较小的图像。通常会选择一个固定大小的补丁，比如一个 ![equation image](../Images/eq-chapter-6-82-1.png)
    像素的正方形。我们希望有一个固定的大小，这样就可以轻松地将其输入到神经网络中，神经网络总是处理固定大小的数据，并且大小要小，以便它们只代表整个图像的一部分。将图像分割成补丁类似于将文本分割成标记集合。每个单独的标记并不具有信息量，但与其他标记结合时，它就构成了一个连贯的句子。
- en: Once an image is broken into patches, each pixel in that patch is converted
    to three numbers representing the amount of red, green, and blue (RGB) present
    in each pixel. An initial vector is created by combining each pixel’s RGB values
    into a single long vector. So for our square of ![equation image](../Images/eq-chapter-6-83-1.png)
    pixels with three color values for each pixel, we will have a vector that is 768
    values in length (16 height, 16 width, and an RGB value for each pixel). Then,
    a small neural network that might have only one or two layers processes each vector
    separately to make the final outputs. This neural network implements a very light
    feature-extraction process that does not require significant memory or computation
    resources. This design is common in computer vision because the first layer usually
    learns simple patterns like “dark inside, light outside” and does not need a transformer
    layer’s greater expense or power to learn the basic features of an image patch.
    This whole process is summarized in figure [6.13](#fig__patchExtractor).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图像被分割成补丁，该补丁中的每个像素都被转换成代表每个像素中存在的红色、绿色和蓝色（RGB）数量的三个数字。通过将每个像素的RGB值组合成一个单一的长时间向量，创建了一个初始向量。因此，对于我们的![equation
    image](../Images/eq-chapter-6-83-1.png)像素的正方形，每个像素有三个颜色值，我们将有一个长度为768的向量（16高度，16宽度，以及每个像素的RGB值）。然后，一个可能只有一到两层的小型神经网络分别处理每个向量，以生成最终的输出。这个神经网络实现了一个非常轻量级的特征提取过程，不需要大量的内存或计算资源。这种设计在计算机视觉中很常见，因为第一层通常学习简单的模式，如“内部黑暗，外部明亮”，并且不需要变压器层的高成本或高能耗来学习图像补片的基本特征。整个过程总结在图[6.13](#fig__patchExtractor)中。
- en: '![figure](../Images/CH06_F13_Boozallen.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F13_Boozallen.png)'
- en: Figure 6.13 Extracting patches is a straightforward process. The patch extractor
    breaks up an image into square tiles called patches. Images consist of pixel values
    that are already numbers, so we convert each patch into a vector of numbers. Then,
    we use a small neural network as a preprocessor before passing the vectors to
    the full transformer-based neural network.
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.13 提取补丁是一个简单的过程。补丁提取器将图像分割成称为补丁的正方形瓷砖。图像由像素值组成，这些像素值已经是数字，因此我们将每个补丁转换成一个数字向量。然后，我们在将向量传递给基于完整变压器的神经网络之前，使用一个小型神经网络作为预处理程序。
- en: There are many possible ways to design the small neural network used in the
    patch extractor, but all generally work equally well. One option is to use what
    is called a *convolutional neural network* (CNN), which is a type of neural network
    that understands that pixels near each other are related to each other. Others
    have used just the same kind of linear layer that is a component of a transformer
    layer. In this case, the overall model that includes the small neural network
    and a series of transformers is often called a *vision transformer*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在补丁提取器中使用的小型神经网络的设计方法有很多种，但所有方法通常都同样有效。一个选择是使用所谓的**卷积神经网络**（CNN），这是一种理解相邻像素之间相互关联的神经网络类型。其他人则使用了与变压器层组件相同的线性层。在这种情况下，包括小型神经网络和一系列变压器在内的整体模型通常被称为**视觉变压器**。
- en: The design of the small network is a minor detail but worth mentioning because
    its existence is relevant to the patch combiner that produces the final output.
    It does not matter whether you pick a CNN or a linear layer for the architecture
    of the small neural network, but it is essential to ensure the output’s shape
    matches the input’s shape. For example, if you have ![equation image](../Images/eq-chapter-6-86-1.png)
    patches, you can use the small network to force the output to have ![equation
    image](../Images/eq-chapter-6-86-2.png) values, regardless of the size of the
    transformer layer itself. To produce image output, you reverse the patch extraction
    process to convert the vectors into patches and then combine the patches into
    an image, as shown in figure [6.14](#fig__patchCombiner).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 小型网络的设计是一个细节，但值得提及，因为它的存在与产生最终输出的补丁合并器相关。无论你选择CNN还是线性层作为小型神经网络的架构，但确保输出的形状与输入的形状匹配是至关重要的。例如，如果你有![equation
    image](../Images/eq-chapter-6-86-1.png)个补丁，你可以使用小型网络强制输出![equation image](../Images/eq-chapter-6-86-2.png)个值，无论变压器层的本身大小如何。为了产生图像输出，你需要反转补丁提取过程，将向量转换为补丁，然后将补丁组合成图像，如图[6.14](#fig__patchCombiner)所示。
- en: We have thus successfully replaced the input tokenization and the output embedding
    with new image-centric layers. In many ways, this is much nicer than tokenization.
    There is no need to build/keep track of a vocabulary, no sampling process, etc.
    This is a crucial insight into the general applicability of transformers as the
    general-purpose core of an LLM. If you can find a lot of data and a reasonable
    method of converting that data into a sequence of vectors, you can use transformers
    to solve certain classes of input and output problems.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经成功地将输入分词和输出嵌入替换为新的以图像为中心的层。在许多方面，这比分词更优雅。无需构建/跟踪词汇表，无需采样过程等。这是对变换器作为LLM通用核心的通用适用性的关键洞察。如果你能找到大量数据，并且有合理的方法将数据转换为向量序列，你可以使用变换器来解决某些类别的输入和输出问题。
- en: '![figure](../Images/CH06_F14_Boozallen.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F14_Boozallen.png)'
- en: Figure 6.14 Compared to figure [6.13](#fig__patchExtractor), the arrows here
    go in the opposite direction. The purpose is to emphasize that the patch combiner
    and extractor do the same thing but operate in different directions. The neural
    network is more important in this stage as a way to force the transformer’s output
    to have the same shape as the original patches because we can control the output
    size of any neural network.
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.14 与图[6.13](#fig__patchExtractor)相比，这里的箭头方向相反。其目的是强调补丁组合器和提取器做的是相同的事情，但操作方向不同。在这个阶段，神经网络更为重要，因为它可以迫使变换器的输出与原始补丁具有相同的形状，因为我们能够控制任何神经网络的输出大小。
- en: 6.3.2 Multimodal models using images and text
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 使用图像和文本的多模态模型
- en: The ability to change the input and output of an LLM to arrive at a vision transformer
    means that we can take an image as input and produce an image as output. It demonstrates
    how a transformer can produce input of different modalities, but we have only
    discussed cases where the input and output are the same modality. We either have
    text as input and text as output or images as input and images as output. However,
    deep learning is flexible! There is nothing that forces us to use the same modality
    as both input and output or even restrict input and output to be a single modality.
    You can combine text as input with image as output, images as input and text as
    output, text and images as input and audio as output, or any other data modality
    combinations you might think of. Figure [6.15](#fig__mixAndMatch) shows how image
    and text give us four total ways we might combine them to handle different kinds
    of data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLM的输入和输出更改为视觉变换器的能力意味着我们可以以图像为输入，生成图像为输出。这展示了变换器如何产生不同模态的输入，但我们只讨论了输入和输出是同一模态的情况。我们要么以文本为输入和输出，要么以图像为输入和输出。然而，深度学习是灵活的！没有任何东西强迫我们使用相同的模态作为输入和输出，甚至将输入和输出限制为单一模态。你可以将文本作为输入与图像作为输出相结合，图像作为输入与文本作为输出相结合，文本和图像作为输入与音频作为输出相结合，或者任何其他你可能想到的数据模态组合。图[6.15](#fig__mixAndMatch)展示了图像和文本如何给我们提供四种组合方式，我们可以通过这些方式处理不同类型的数据。
- en: By creating a model that uses images as input and text as output, we create
    an *image captioning model*. We can train this model to generate text describing
    the input image’s content. Models such as these help make images more discoverable
    and aid visually impaired users.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建一个以图像为输入、文本为输出的模型，我们构建了一个*图像描述模型*。我们可以训练这个模型来生成描述输入图像内容的文本。这类模型有助于使图像更易于发现，并帮助视觉障碍用户。
- en: 'By creating a model that uses text as the input and an image as the output,
    we create an *image generation model*. You can describe a desired image using
    words, and the model can create a reasonable image based on your input. Famous
    products like MidJourney are models of this flavor. Though their implementation
    involves more than just a vision transformer, the high-level idea is the same:
    by pairing a text-based input with image-based output and a lot of data, we can
    create new multimodal capabilities that span different data types.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建一个以文本为输入、图像为输出的模型，我们构建了一个*图像生成模型*。你可以用文字描述一个期望的图像，模型可以根据你的输入创建一个合理的图像。MidJourney等著名产品就是这类模型的例子。尽管它们的实现涉及不仅仅是视觉变换器，但高级思想是相同的：通过将基于文本的输入与基于图像的输出以及大量数据相结合，我们可以创建跨越不同数据类型的新多模态能力。
- en: '![figure](../Images/CH06_F15_Boozallen.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F15_Boozallen.png)'
- en: Figure 6.15 Four combinations showing different types of model input and output.
    The example at the furthest right represents a normal text-based LLM we have already
    learned about. To the left, we show possibilities like an image-generating model
    that takes text as input (“Draw me a picture of a stop sign in a flood zone”)
    or an image captioning model that generates text that describes an image input
    (“This picture shows a stop sign surrounded by murky water”).
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.15 展示了四种不同类型的模型输入和输出的组合。最右边的一个例子代表了我们已经了解的正常基于文本的LLM。向左，我们展示了像以文本作为输入生成图像的模型（“画一个洪水区域的停车标志的图片”）或生成描述图像输入文本的图像标题模型（“这张图片显示了一个被浑浊的水包围的停车标志”）这样的可能性。
- en: 6.3.3 Applicability of prior lessons
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 前期课程的应用性
- en: Other lessons learned throughout this book remain relevant to these vision transformer
    and multimodal models. Ultimately, they learn to do what they are trained for,
    and when you try to bend them in ways beyond what is found in the training data,
    you may get an unusual result. As an example, we might tell an image generation
    model “Draw anything but an adorable cat,” and you will probably end up with a
    cat as shown in figure [6.16](#fig__adorableCat)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中学习到的其他经验教训仍然适用于这些视觉转换器和多模态模型。最终，它们学会做它们被训练去做的事情，当你试图以训练数据中找不到的方式弯曲它们时，你可能会得到一个不寻常的结果。例如，我们可能会告诉一个图像生成模型“画任何东西，但不是可爱的猫”，你可能会得到如图[6.16](#fig__adorableCat)所示的猫。
- en: These models are (currently) trained with pairs of images and pieces of text
    describing the image. Thus, they learn a strong correlation to produce visualizations
    of anything in the input sentence. For example, the model wants to produce a cat
    since the word *cat* is in the input sentence. More sophisticated abstract drawing
    requests like “Draw anything but” do not appear in such datasets, and so the model
    is not trained to handle such a request.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型（目前）是通过图像和描述图像的文本片段的成对进行训练的。因此，它们学会了对输入句子中的任何东西产生视觉化的强烈相关性。例如，模型想要生成猫，因为输入句子中有“猫”这个词。更复杂的抽象绘图请求，如“画任何东西但”，没有出现在这样的数据集中，因此模型没有被训练去处理这样的请求。
- en: Similarly, as LLMs like ChatGPT have developed prompting as a strategy for devising
    inputs that produce desired outputs, prompting has also been developed for image
    captioning models. It is not uncommon to include unusual information like “Unreal3D,”
    the name of software used to generate 3D imagery for computer games to produce
    output with a particular style and quality. Words like *high resolution* and even
    the names of artists, alive and dead, are used to try to influence the models
    into producing particular styles.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，随着像ChatGPT这样的LLM将提示作为制定产生期望输出的输入的策略，提示也被开发用于图像标题模型。包含像“Unreal3D”这样的不寻常信息，这是用于为电脑游戏生成3D图像的软件名称，以产生具有特定风格和质量输出的情况并不少见。像“高分辨率”这样的词甚至艺术家的名字，无论是生前的还是已故的，都被用来试图影响模型产生特定的风格。
- en: '![figure](../Images/CH06_F16_Boozallen.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F16_Boozallen.jpg)'
- en: Figure 6.16 This was generated with an old version of Stable Diffusion, a popular
    image generation model. Despite telling the model “Do not draw a cat,” the model
    was trained to generate content. The request is outside what the model was incentivized
    to learn, so it cannot handle it. This is similar to the problems with LLMs regurgitating
    close-but-wrong output because the model saw similar data during training.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.16 这是由一个老版本的Stable Diffusion生成的，这是一个流行的图像生成模型。尽管告诉模型“不要画猫”，但模型被训练生成内容。请求超出了模型被激励去学习的内容，因此它无法处理。这与LLM在训练期间看到类似数据时重复输出错误但接近的结果的问题类似。
- en: Summary
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs benefit when they can use external tools. For example, a code LLM can use
    syntax checkers and compilers to detect erroneous code generation. When the LLM
    finds an error, the output is regenerated, minimizing the risk of giving the user
    unhelpful or broken code.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当LLM能够使用外部工具时，它们会受益。例如，一个代码LLM可以使用语法检查器和编译器来检测错误代码生成。当LLM发现错误时，输出会被重新生成，从而最小化向用户提供无帮助或损坏的代码的风险。
- en: Tokenizers must be modified to support math by keeping unusual symbols used
    to express formatted math and changing digit representations. We can improve math
    LLMs further by giving them tools like computer algebra systems to detect and
    avoid errors.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记化器必须修改以支持数学，通过保留用于表达格式化数学的不寻常符号并更改数字表示。我们可以通过给他们提供像计算机代数系统这样的工具来检测和避免错误来进一步提高数学LLM。
- en: Transformers can be applied to images by breaking up an image into patches,
    where each patch becomes a vector and makes a sequence of inputs for the transformer
    to process. Patches are conceptually similar to tokens for text LLMs.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer可以通过将图像分割成块（patches）来应用于图像，其中每个块变成一个向量，并为Transformer处理生成一系列输入。从概念上讲，块与文本LLM（大型语言模型）的标记相似。
- en: Transformers can use different data modalities for input and output, allowing
    the creation of multimodal models like those used in image captioning and image
    generation.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer可以使用不同的数据模态作为输入和输出，这允许创建多模态模型，如用于图像标题和图像生成的模型。
