- en: Chapter 16\. Productionizing Machine Learning Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章。生产机器学习系统
- en: Building and training a model is one thing; deploying your model in a production
    system is a different and often overlooked story. Running code in a Python notebook
    is nice, but not a great way to serve web clients. In this chapter we’ll look
    at how to get up and running for real.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 构建和训练模型是一回事；在生产系统中部署您的模型是另一回事，通常被忽视。在Python笔记本中运行代码很好，但不是为web客户端提供服务的好方法。在本章中，我们将看看如何真正开始运行。
- en: We’ll start with embeddings. Embeddings have played a role in many of the recipes
    in this book. In [Chapter 3](ch03.html#word_embeddings), we looked at the interesting
    things we can do with word embeddings, like finding similar words by looking at
    their nearest neighbors or finding analogues by adding and subtracting embedding
    vectors. In [Chapter 4](ch04.html#movie_recommender), we used embeddings of Wikipedia
    articles to build a simple movie recommender system. In [Chapter 10](ch10.html#image_search),
    we saw how we can treat the output of the final layer of a pretrained image classification
    network as embeddings for the input image and use this to build a reverse image
    search service.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从嵌入开始。嵌入在本书的许多食谱中发挥了作用。在[第3章](ch03.html#word_embeddings)中，我们看到了我们可以通过查看最近邻来找到相似单词的有趣事情，或者通过添加和减去嵌入向量来找到类比。在[第4章](ch04.html#movie_recommender)中，我们使用维基百科文章的嵌入来构建一个简单的电影推荐系统。在[第10章](ch10.html#image_search)中，我们看到了我们可以将预训练图像分类网络的最终层的输出视为输入图像的嵌入，并使用此来构建反向图像搜索服务。
- en: Just as with these examples, we find that real-world cases often end with embeddings
    for certain entities that we then want to query from a production-quality application.
    In other words, we have a set of images, texts, or words and an algorithm that
    for each produces a vector in a high-dimensional space. For a concrete application,
    we want to be able to query this space.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 就像这些示例一样，我们发现真实世界的案例通常以某些实体的嵌入结束，然后我们希望从生产质量的应用程序中查询这些实体。换句话说，我们有一组图像、文本或单词，以及一个算法，为每个实体在高维空间中生成一个向量。对于一个具体的应用程序，我们希望能够查询这个空间。
- en: 'We’ll start with a simple approach: we’ll build a nearest neighbor model and
    save it to disk, so we can load it when we need it. We’ll then look at using Postgres
    for the same purpose.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的方法开始：我们将构建一个最近邻模型并将其保存到磁盘，以便在需要时加载。然后我们将看看如何使用Postgres达到相同的目的。
- en: We’ll also explore using microservices as a way to expose machine learning models
    using Flask as a web server and Keras’s ability to save and load models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨使用微服务作为一种使用Flask作为web服务器和Keras的保存和加载模型功能来暴露机器学习模型的方法。
- en: 'The following notebooks are available for this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下笔记本可用于本章：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 16.1 Using Scikit-Learn’s Nearest Neighbors for Embeddings
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.1 使用Scikit-Learn的最近邻算法进行嵌入
- en: Problem
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you quickly serve up the closest matches from an embedding model?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如何快速提供嵌入模型的最接近匹配项？
- en: Solution
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use scikit-learn’s nearest neighbor’s algorithm and save the model into a file.
    We’ll continue the code from [Chapter 4](ch04.html#movie_recommender), where we
    created a movie prediction model. After we’ve run everything, we normalize the
    values and fit a nearest neighbor model:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn的最近邻算法并将模型保存到文件中。我们将继续从[第4章](ch04.html#movie_recommender)的代码开始，我们在那里创建了一个电影预测模型。在我们运行完所有内容后，我们将归一化值并拟合一个最近邻模型：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can then later load the model again with:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后稍后可以再次加载模型：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Discussion
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The simplest way to productionize a machine learning model is to save it to
    disk after the training is done and then to load it up when it is needed. All
    major machine learning frameworks support this, including the ones we’ve used
    throughout this book, Keras and scikit-learn.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 生产机器学习模型的最简单方法是在训练完成后将其保存到磁盘，然后在需要时加载。所有主要的机器学习框架都支持这一点，包括我们在本书中使用的Keras和scikit-learn。
- en: This solution is great if you are in control of memory management. In a production
    web server this is often not the case, however, and when you have to load a large
    model into memory when a web request comes in, latency obviously suffers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您控制内存管理，这个解决方案非常好。然而，在生产web服务器中，通常情况并非如此，当web请求到来时，如果必须将大型模型加载到内存中，延迟显然会受到影响。
- en: 16.2 Use Postgres to Store Embeddings
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.2 使用Postgres存储嵌入
- en: Problem
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’d like to use Postgres to store embeddings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您想使用Postgres存储嵌入。
- en: Solution
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the Postgres `Cube` extension.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Postgres的`Cube`扩展。
- en: 'The `Cube` extension allows for the handling of high-dimensional data, but
    it needs to be enabled first:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cube`扩展允许处理高维数据，但需要先启用它：'
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once that is done, we can create a table and corresponding index. To make it
    also possible to search on movie names, we’ll create a text index on the `movie_name`
    field, too:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以创建一个表和相应的索引。为了也能够在`movie_name`字段上进行搜索，我们还将在`movie_name`字段上创建一个文本索引：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Discussion
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Postgres is a free database that is remarkably powerful, not least because of
    the large number of extensions that are available. One of those modules is the
    `cube` module. As the name suggests, it was originally meant to make 3-dimensional
    coordinates available as a primitive, but it has since been extended to index
    arrays up to 100 dimensions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Postgres是一个免费的数据库，非常强大，其中一个原因是有大量可用的扩展。其中一个模块是`cube`模块。顾名思义，它最初是为了将三维坐标作为原始数据可用，但后来已扩展到可以索引高达100维的数组。
- en: Postgres has many extensions that are well worth exploring for anybody handling
    sizeable amounts of data. In particular, the ability to store less-structured
    data in the form of arrays and JSON documents inside of classical SQL tables comes
    in handy when prototyping.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Postgres有许多扩展，值得探索，特别是对于处理大量数据的任何人。特别是，在经典SQL表中以数组和JSON文档的形式存储较少结构化的数据的能力在原型设计时非常方便。
- en: 16.3 Populating and Querying Embeddings Stored in Postgres
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.3 填充和查询存储在Postgres中的嵌入
- en: Problem
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Can you store our model and query results in Postgres?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您能够在Postgres中存储我们的模型和查询结果吗？
- en: Solution
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use `psycopg2` to connect to Postgres from Python.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`psycopg2`从Python连接到Postgres。
- en: 'Given a username/password/database/host combination we can easily connect to
    Postgres using Python:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过给定的用户名/密码/数据库/主机组合，我们可以轻松地使用Python连接到Postgres：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Inserting our previously built model works like any other SQL operation in
    Python, except that we need to cast our `numpy` array to a Python list:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 插入我们之前构建的模型与Python中的任何其他SQL操作一样，只是我们需要将我们的`numpy`数组转换为Python列表：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once this is done, we can query the values. In this case we take (part of)
    a title of a movie, find the best match for that movie, and return the most similar
    movies:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以查询这些值。在这种情况下，我们取（部分）电影标题，找到该电影的最佳匹配，并返回最相似的电影：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Discussion
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Storing an embedding model in a Postgres database allows us to query it directly,
    without having to load the model up on every request, and is therefore a good
    solution when we want to use such a model from a web server—especially when our
    web setup was Postgres-based to begin with, of course.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将嵌入模型存储在Postgres数据库中允许我们直接查询它，而无需在每个请求中加载模型，因此当我们想要从Web服务器使用这样的模型时，这是一个很好的解决方案——特别是当我们的Web设置一开始就是基于Postgres的时候。
- en: Running a model or the results of a model on the database server that is powering
    your website has the added advantage that you can seamlessly mix ranking components.
    We could easily extend the code of this recipe to include the Rotten Tomatoes
    ratings in our movies table, from which point on we could use this information
    to help sort the returned movies. However, if the ratings and similarity distance
    come from a different source, we would either have to do an in-memory join by
    hand or return incomplete results.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持您网站的数据库服务器上运行模型或模型的结果具有额外的优势，您可以无缝地混合排名组件。我们可以轻松地扩展这个示例的代码，将新鲜度评分包含在我们的电影表中，从那时起，我们可以使用这些信息来帮助对返回的电影进行排序。但是，如果评分和相似性距离来自不同的来源，我们要么必须手动进行内存连接，要么返回不完整的结果。
- en: 16.4 Storing High-Dimensional Models in Postgres
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.4 在Postgres中存储高维模型
- en: Problem
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you store a model with more than 100 dimensions in Postgres?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在Postgres中存储具有超过100个维度的模型？
- en: Solution
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a dimension reduction technique.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用降维技术。
- en: 'Let’s say we wanted to load Google’s pretrained Word2vec model that we used
    in [Chapter 3](ch03.html#word_embeddings) into Postgres. Since the Postgres `cube`
    extension (see [Recipe 16.2](#use_postgres_to_store_embeddings)) limits the number
    of dimensions it will index to 100, we need to do something to make this fit.
    Reducing the dimensionality using singular value decomposition (SVD)—a technique
    we met in [Recipe 10.4](ch10.html#exploring_local_neighborhoods_in_embeddings)—is
    a good option. Let’s load up the Word2vec model as before:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要加载谷歌预训练的Word2vec模型，我们在[第3章](ch03.html#word_embeddings)中使用了这个模型，加载到Postgres中。由于Postgres的`cube`扩展（参见[Recipe
    16.2](#use_postgres_to_store_embeddings)）限制了它将索引的维度数量为100，我们需要做一些处理以使其适应。使用奇异值分解（SVD）来降低维度——这是我们在[Recipe
    10.4](ch10.html#exploring_local_neighborhoods_in_embeddings)中遇到的一种技术——是一个不错的选择。让我们像以前一样加载Word2vec模型：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The normalized vectors per word are stored in the `syn0norm` property, so we
    can run the SVD over that. This does take a little while:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词的归一化向量存储在`syn0norm`属性中，因此我们可以在其上运行SVD。这需要一点时间：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We need to renormalize the vectors:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要重新归一化向量：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we can look at the similarity:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看相似性：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The results still look reasonable, but they are not exactly the same. The last
    entry, martini, is somewhat unexpected in a list of caffeinated pick-me-ups.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来仍然合理，但并不完全相同。最后一个条目，马提尼，出现在一系列含有咖啡因提神饮料的列表中有些出乎意料。
- en: Discussion
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'The Postgres `cube` extension is great, but comes with the caveat that it only
    works for vectors that have 100 or fewer elements. The documentation helpfully
    explains this limitation with: “To make it harder for people to break things,
    there is a limit of 100 on the number of dimensions of cubes.” One way around
    this restriction is to recompile Postgres, but that’s only an option if you directly
    control your setup. Also, it requires you to keep doing this as new versions of
    the database come out.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Postgres的`cube`扩展很棒，但带有一个警告，它仅适用于具有100个或更少元素的向量。文档有用地解释了这个限制：“为了让人们更难破坏事物，立方体的维度数量限制为100。”绕过这个限制的一种方法是重新编译Postgres，但这只是一个选项，如果您直接控制您的设置。此外，随着数据库的新版本发布，您需要继续这样做。
- en: Reducing the dimensionality before inserting our vectors into the database can
    easily be done using the `TruncatedSVD` class. In this recipe we used the entire
    set of words from the Word2vec dataset, which led to the loss of some precision.
    If we not only reduce the dimensionality of the output but also cut down the number
    of terms, we can do better. SVD can then find the most important dimensions for
    the data that we provide, rather than for all the data. This can even help by
    generalizing a bit and papering over a lack of data in our original input.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在将我们的向量插入数据库之前降低维度可以很容易地使用`TruncatedSVD`类来完成。在这个示例中，我们使用了Word2vec数据集中的整个单词集，这导致了一些精度的损失。如果我们不仅降低输出的维度，还减少术语的数量，我们可以做得更好。然后SVD可以找到我们提供的数据中最重要的维度，而不是所有数据。这甚至可以通过泛化一点并掩盖原始输入数据中的数据不足来帮助。
- en: 16.5 Writing Microservices in Python
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.5 使用Python编写微服务
- en: Problem
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’d like to write and deploy a simple Python microservice.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您想编写并部署一个简单的Python微服务。
- en: Solution
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Build a minimal web app using Flask, returning a JSON document based on a REST
    request.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flask构建一个最小的Web应用程序，根据REST请求返回一个JSON文档。
- en: 'First we need a Flask web server:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们需要一个Flask Web服务器：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We then define the service we want to offer. As an example, we’ll take in an
    image and return the size of the image. We expect the image to be part of a `POST`
    request. If we don’t get a `POST` request, we’ll return a simple HTML form so
    we can test the service without a client. The `@app.route` decoration specifies
    that the `return_size` handles any requests at the root, supporting both `GET`
    and `POST`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义我们想要提供的服务。例如，我们将接收一张图片并返回图片的大小。我们期望图片是`POST`请求的一部分。如果我们没有收到`POST`请求，我们将返回一个简单的HTML表单，这样我们就可以在没有客户端的情况下测试服务。`@app.route`装饰指定`return_size`处理根目录的任何请求，支持`GET`和`POST`：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now all we have to do is run the server at a port:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需要在一个端口上运行服务器：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Discussion
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: REST was originally meant as a full-blown resource management framework that
    assigns URLs to all resources in a system and then lets clients interact with
    the whole spectrum of HTTP verbs, from `PUT` to `DELETE`. Like many APIs out there,
    we forego all that in this example and just have a `GET` method defined on one
    handler that triggers our API and returns a JSON document.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: REST最初是作为一个完整的资源管理框架，为系统中的所有资源分配URL，并让客户端与HTTP谓词的整个范围进行交互，从`PUT`到`DELETE`。就像许多API一样，在这个示例中我们放弃了所有这些，只在一个处理程序上定义了一个`GET`方法，触发我们的API并返回一个JSON文档。
- en: The service we developed here is of course rather trivial; having a microservice
    for just getting the size of an image is probably taking the concept a little
    too far. In the next recipe we’ll explore how we can use this approach to serve
    up the results of a previously developed machine learning model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里开发的服务当然相当琐碎；仅仅为了获取图像大小而拥有一个微服务可能有点过头了。在下一个示例中，我们将探讨如何使用这种方法来提供先前开发的机器学习模型的结果。
- en: 16.6 Deploying a Keras Model Using a Microservice
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.6 使用微服务部署Keras模型
- en: Problem
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to deploy a Keras model as a standalone service.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要将Keras模型部署为独立服务。
- en: Solution
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Expand your Flask server to forward requests to a pretrained Keras model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的Flask服务器扩展到转发请求到预训练的Keras模型。
- en: This recipe builds on the recipes in [Chapter 10](ch10.html#image_search), where
    we downloaded thousands of images from Wikipedia and fed them into a pretrained
    image recognition network, getting back a 2,048-dimensional vector describing
    each image. We’ll fit a nearest neighbor model on these vectors so that we can
    quickly find the most similar image, given a vector.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例是在[第10章](ch10.html#image_search)中的示例构建的，我们从维基百科下载了成千上万张图片，并将它们馈送到一个预训练的图像识别网络中，得到一个描述每张图片的2,048维向量。我们将在这些向量上拟合一个最近邻模型，以便我们可以快速找到最相似的图像，给定一个向量。
- en: 'The first step is to load the pickled image names and nearest neighbor model
    and instantiate the pretrained model for image recognition:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是加载腌制的图像名称和最近邻模型，并实例化用于图像识别的预训练模型：
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can now modify how we handle the incoming image by changing the bit of code
    after `if file:`. We’ll resize the image to the target size of the model, normalize
    the data, run the prediction, and find the nearest neighbors:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过更改`if file:`后的代码部分来修改如何处理传入的图像。我们将调整图像大小到模型的目标大小，规范化数据，运行预测，并找到最近的邻居：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Feed it an image of a cat, and you should see a large number of cats sampled
    from the Wikipedia images—with one photo of kids playing with a home computer
    thrown in.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 给它一张猫的图片，你应该看到从维基百科图片中抽样出大量猫的图片——还有一张孩子们玩家用电脑的照片。
- en: Discussion
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: By loading the model on startup and then feeding in the images as they come
    in, we can cut down on the latency that we would get if we followed the approach
    of the first recipe in this section. We’re effectively chaining two models here,
    the pretrained image recognition network and the nearest neighbor classifier,
    and exporting the combination as one service.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在启动时加载模型，然后在图像进入时馈送图像，我们可以减少我们遵循本节第一个示例方法所获得的延迟。我们在这里有效地链接了两个模型，预训练的图像识别网络和最近邻分类器，并将组合导出为一个服务。
- en: 16.7 Calling a Microservice from a Web Framework
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.7 从Web框架调用微服务
- en: Problem
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to call a microservice from Django.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要从Django调用一个微服务。
- en: Solution
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use `requests` to call the microservice while handling the Django request.
    We can do this along the lines of the following example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`requests`调用微服务同时处理Django请求。我们可以按照以下示例的方式进行：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Discussion
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The code here is from a Django request handler, but things should look really
    similar in other web frameworks, even ones based on a different language than
    Python.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的代码来自Django请求处理程序，但在其他Web框架中看起来应该非常相似，即使是基于Python以外的语言的框架。
- en: The key thing here is that we separate the session management of the web framework
    from the session management of our microservice. This way we know that at any
    given time there is exactly one instance of our model, which makes latency and
    memory use predictable.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键是我们将Web框架的会话管理与微服务的会话管理分开。这样我们就知道在任何给定时间只有一个模型实例，这使得延迟和内存使用可预测。
- en: '`Requests` is a straightforward module for making `HTTP` calls. It doesn’t
    support making async calls, though. In the code for this recipe that isn’t important,
    but if we need to call multiple services, we’d want to do that in parallel. There
    are a number of options for this, but they all fall into the pattern where we
    fire off calls to the backends at the beginning of our request, do the processing
    we need to, and then, when we need the results, wait on the outstanding requests.
    This is a good setup for building high-performance systems using Python.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`Requests`是一个用于发起`HTTP`调用的简单模块。尽管它不支持发起异步调用。在这个示例的代码中，这并不重要，但如果我们需要调用多个服务，我们希望能够并行进行。有许多选项可供选择，但它们都遵循一种模式，即在请求开始时向后端发起调用，进行我们需要的处理，然后在需要结果时等待未完成的请求。这是使用Python构建高性能系统的良好设置。'
- en: 16.8 TensorFlow seq2seq models
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.8 TensorFlow seq2seq模型
- en: Problem
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you productionize a seq2seq chat model?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将seq2seq聊天模型投入生产？
- en: Solution
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Run a TensorFlow session with an output-capturing hook.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 运行一个带有输出捕获钩子的TensorFlow会话。
- en: The `seq2seq` model that Google published is a very nice way to quickly develop
    sequence-to-sequence models, but out of the box the inference phase can only be
    run using `stdin` and `stdout`. It’s entirely possible to call out from our microservice
    this way, but that means we’ll incur the latency cost of loading the model up
    on every call.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Google发布的`seq2seq`模型是一种非常好的快速开发序列到序列模型的方式，但默认情况下推断阶段只能使用`stdin`和`stdout`运行。通过这种方式从我们的微服务调用是完全可能的，但这意味着我们将在每次调用时承担加载模型的延迟成本。
- en: 'A better way is to instantiate the model manually and capture the output using
    a hook. The first step is to reinstate the model from the checkpoint directory.
    We need to load both the model and the model configuration. The model feeds in
    the `source_tokens` (i.e., the chat prompt) and we’ll use a batch size of 1, since
    we’ll do this in an interactive fashion:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是手动实例化模型并使用钩子捕获输出。第一步是从检查点目录中恢复模型。我们需要加载模型和模型配置。模型将输入`source_tokens`（即聊天提示），我们将使用批量大小为1，因为我们将以交互方式进行：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The next step is to set up the TensorFlow session that allows us to feed data
    into the model. This is all fairly boilerplate stuff (and should make us appreciate
    frameworks like Keras even more):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是设置允许我们将数据输入模型的TensorFlow会话。这都是相当标准的内容（应该让我们更加欣赏像Keras这样的框架）：
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We’ve now configured a TensorFlow session with a hook to `DecodeOnce`, which
    is a class that implements the basic functionality of the inference task but then,
    when it is done, calls the provided `callback` function to return the actual results.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在配置了一个带有`DecodeOnce`钩子的TensorFlow会话，它是一个实现推断任务基本功能的类，但在完成后调用提供的`callback`函数返回实际结果。
- en: 'In the code for *seq2seq_server.py* we can then use this to handle an HTTP
    request as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在*seq2seq_server.py*的代码中，我们可以使用这个来处理HTTP请求，如下所示：
- en: '[PRE22]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This will let us handle seq2seq calls from a simple web server.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这将让我们处理来自简单Web服务器的seq2seq调用。
- en: Discussion
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The way we feed data into the seq2seq TensorFlow model in this recipe is not
    very pretty, but it is effective and in terms of performance a much better option
    than using `stdin` and `stdout`. Hopefully an upcoming version of this library
    will provide us with a nicer way to use these models in production, but for now
    this will have to do.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将数据输入到seq2seq TensorFlow模型的方式并不是很漂亮，但它是有效的，并且在性能方面比使用`stdin`和`stdout`要好得多。希望这个库的即将推出的版本将为我们提供一个更好的方式来在生产中使用这些模型，但目前这样做就可以了。
- en: 16.9 Running Deep Learning Models in the Browser
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.9在浏览器中运行深度学习模型
- en: Problem
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you run a deep learning web app without a server?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在没有服务器的情况下运行深度学习Web应用程序？
- en: Solution
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Keras.js to run the model in the browser.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras.js在浏览器中运行模型。
- en: Running a deep learning model in the browser sounds crazy. Deep learning needs
    lots of processing power, and we all know that JavaScript is slow. But it turns
    out that you can run models in the browser at a decent speed with GPU acceleration.
    [Keras.js](https://transcranial.github.io/keras-js/#) has a tool to convert Keras
    models to something that the JavaScript runtime can work with, and it uses WebGL
    to get the GPU to help with this. It’s an amazing bit of engineering and it comes
    with some impressive demos. Let’s try this on one of our own models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中运行深度学习模型听起来很疯狂。深度学习需要大量的处理能力，我们都知道JavaScript很慢。但事实证明，您可以在浏览器中以相当快的速度运行模型，并使用GPU加速。[Keras.js](https://transcranial.github.io/keras-js/#)有一个工具，可以将Keras模型转换为JavaScript运行时可以处理的内容，并使用WebGL来让GPU帮助处理。这是一个令人惊叹的工程成就，并且配备了一些令人印象深刻的演示。让我们尝试在我们自己的模型上尝试一下。
- en: 'The notebook `16.1 Simple Text Generation` is taken from the Keras example
    directory and trains a simple text generation model based on the writings of Nietzsche.
    After training we save the model with:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`16.1简单文本生成`取自Keras示例目录，并基于尼采的著作训练了一个简单的文本生成模型。训练后，我们保存模型：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we need to convert the Keras model to the Keras.js format. First get the
    conversion code using:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将Keras模型转换为Keras.js格式。首先获取转换代码：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now open a shell and, in the directory where you saved the model, execute:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打开一个shell，在保存模型的目录中执行：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This should give you a *nietzsche.bin* file.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给您一个*nietzsche.bin*文件。
- en: The next step is to use this file from a web page.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从网页中使用这个文件。
- en: 'We’ll do this in the file *nietzsche.html*, which you’ll find in the *keras_js*
    directory of the *deep_learning_cookbook* repository. Let’s take a look. It starts
    with code to load the Keras.js library and the variables we saved from Python:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*nietzsche.html*文件中执行此操作，您将在*deep_learning_cookbook*存储库的*keras_js*目录中找到它。让我们来看看。它以加载Keras.js库和我们从Python保存的变量的代码开始：
- en: '[PRE26]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'At the bottom we have a very simple bit of HTML that lets the user enter some
    text and then press a button to run the model to extend the text in a Nietzschean
    way:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在底部，我们有一个非常简单的HTML代码，让用户输入一些文本，然后按下按钮以尼采式的方式扩展文本：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now let’s load the model and, when it’s done, enable the currently disabled
    button `buttonGo`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们加载模型，加载完成后，启用当前禁用的按钮`buttonGo`：
- en: '[PRE28]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In `runModel` we first need to one-hot encode the text data using the `char_indices`
    we imported before:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在`runModel`中，我们首先需要使用之前导入的`char_indices`对文本数据进行独热编码：
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can now run the model with:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行模型：
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `outputData` variable will contain a probability distribution for each
    of the characters in our vocabulary. The easiest way to make sense of that is
    to pick just the character with the highest probability:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`outputData`变量将包含我们词汇表中每个字符的概率分布。理解这一点最简单的方法是选择具有最高概率的字符：'
- en: '[PRE31]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now we just add that character to what we had so far and do the same thing
    again:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需将该字符添加到我们到目前为止的内容中，并再次执行相同的操作：
- en: '[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Discussion
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Being able to run models straight in the browser creates entirely new possibilities
    for productionalizing. It means you don’t need a server to do the actual calculations,
    and with WebGL you even get GPU acceleration for free. Check out the fun demos
    at [*https://transcranial.github.io/keras-js*](https://transcranial.github.io/keras-js).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 能够直接在浏览器中运行模型为生产提供了全新的可能性。这意味着您不需要服务器来执行实际计算，并且使用WebGL，您甚至可以免费获得GPU加速。查看[*https://transcranial.github.io/keras-js*](https://transcranial.github.io/keras-js)上的有趣演示。
- en: There are limitations to this approach. To use the GPU, Keras.js uses WebGL
    2.0\. Unfortunately, not all browsers support this at the moment. Moreover, tensors
    are encoded as WebGL textures, which are limited in size. The actual limit depends
    on your browser and hardware. You can of course fall back to CPU only, but that
    means running in pure JavaScript.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法存在一些限制。为了使用GPU，Keras.js使用WebGL 2.0。不幸的是，目前并非所有浏览器都支持这一点。此外，张量被编码为WebGL纹理，其大小受限。实际限制取决于您的浏览器和硬件。当然，您可以退回到仅使用CPU，但这意味着在纯JavaScript中运行。
- en: A second limitation is the size of the models. Production-quality models often
    have sizes of tens of megabytes, which isn’t a problem at all when they are loaded
    up once on the server but might create issues when they need to be sent to a client.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个限制是模型的大小。生产质量的模型通常有几十兆字节的大小，当它们一次加载到服务器上时，这一点根本不是问题，但当它们需要发送到客户端时可能会出现问题。
- en: Note
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The *encoder.py* script has a flag called `--quantize` that will encode the
    weights of the model as 8-bit integers. This reduces the size of the model by
    75%, but it means the weights will be less precise, which might hurt prediction
    accuracy.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*encoder.py*脚本有一个名为`--quantize`的标志，它将模型的权重编码为8位整数。这将减少模型大小75%，但意味着权重将不太精确，这可能会影响预测准确性。'
- en: 16.10 Running a Keras Model Using TensorFlow Serving
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.10 使用TensorFlow Serving运行Keras模型
- en: Problem
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you run a Keras model using Google’s state-of-the art server?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用谷歌最先进的服务器运行Keras模型？
- en: Solution
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Convert the model and invoke the TensorFlow Serving toolkit to write out the
    model spec so you can run it using TensorFlow Serving.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 转换模型并调用TensorFlow Serving工具包以写出模型规范，以便您可以使用TensorFlow Serving运行它。
- en: TensorFlow Serving is part of the TensorFlow platform; according to Google it’s
    a flexible, high-performance serving system for machine learning models, designed
    for production environments.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving是TensorFlow平台的一部分；根据谷歌的说法，它是一个灵活、高性能的机器学习模型服务系统，专为生产环境设计。
- en: Writing out a TensorFlow model in a way that TensorFlow Serving will work with
    is somewhat involved. Keras models need even more massaging in order for this
    to work. In principle, any model can be used as long as the model has only one
    input and only one output—a restriction that comes with TensorFlow Serving. Another
    is that TensorFlow Serving only supports Python 2.7.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个TensorFlow模型写成TensorFlow Serving可以使用的方式有些复杂。为了使其正常工作，Keras模型需要更多的调整。原则上，只要模型只有一个输入和一个输出，就可以使用任何模型——这是TensorFlow
    Serving的限制之一。另一个限制是TensorFlow Serving只支持Python 2.7。
- en: 'The first thing to do is recreate the model as a testing-only model. Models
    behave differently during training and testing. For example, the `Dropout` layer
    only randomly drops neurons while training—during testing everything is used.
    Keras hides this from the user, passing the learning phase in as an extra variable.
    If you see errors stating that something is missing from your input, this is probably
    it. We’ll set the learning phase to `0` (false) and extract the config and the
    weights from our character CNN model:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是将模型重新创建为仅用于测试的模型。模型在训练和测试期间的行为不同。例如，`Dropout`层在训练时只会随机丢弃神经元，而在测试时会使用所有神经元。Keras会将这些隐藏在用户之外，将学习阶段作为额外变量传递。如果您看到错误提示缺少输入的某些内容，这可能是原因。我们将学习阶段设置为`0`（false），并从我们的字符CNN模型中提取配置和权重：
- en: '[PRE33]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'At this point it might be useful to run a prediction on the model so we can
    later see that it still works:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此时可能有必要对模型进行预测，以便稍后查看它仍然有效：
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can then rebuild the model with:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以重新构建模型：
- en: '[PRE36]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In order for the model to run, we need to provide TensorFlow Serving with the
    input and output spec:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型运行，我们需要为TensorFlow Serving提供输入和输出规范：
- en: '[PRE37]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can then construct the `builder` object to define our handler and write
    out the definition:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以构建`builder`对象来定义我们的处理程序并写出定义：
- en: '[PRE38]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now we run the server with:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们运行服务器：
- en: '[PRE39]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: You can either get the binaries directly from Google or build them from source—see
    [the installation instructions](https://www.tensorflow.org/serving/setup) for
    details.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接从谷歌获取二进制文件，也可以从源代码构建——有关详细信息，请参阅[安装说明](https://www.tensorflow.org/serving/setup)。
- en: 'Let’s see if we can call the model from Python. We’ll instantiate a prediction
    request and use `grpc` to make a call:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们是否可以从Python调用模型。我们将实例化一个预测请求并使用`grpc`进行调用：
- en: '[PRE40]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Get the actual predicted emojis:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 获取实际预测的表情符号：
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Discussion
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: TensorFlow Serving is the way to productionize models blessed by Google but
    using it with a Keras model is somewhat involved compared to bringing up a custom
    Flask server and handling the input and output ourselves.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving是谷歌认可的将模型投入生产的方式，但与启动自定义Flask服务器并自行处理输入和输出相比，使用它与Keras模型有些复杂。
- en: It does have advantages, though. For one thing, since is not custom, these servers
    all behave the same. Furthermore, it is an industrial-strength server that supports
    versioning and can load models straight from a number of cloud providers.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有优势。首先，由于不是自定义的，这些服务器都表现一致。此外，它是一个工业强度的服务器，支持版本控制，并可以直接从多个云提供商加载模型。
- en: 16.11 Using a Keras Model from iOS
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.11 在iOS上使用Keras模型
- en: Problem
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’d like to use a model trained on the desktop from a mobile app on iOS.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望在iOS上的移动应用程序中使用在桌面上训练的模型。
- en: Solution
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use CoreML to convert your model and talk to it directly from Swift.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CoreML将您的模型转换并直接从Swift与之通信。
- en: Note
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This recipe describes how to build an app for iOS, so you’ll need a Mac with
    Xcode installed to run the example. Moreover, since the example uses the camera
    for detection, you’ll also need an iOS device with a camera to try it out.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了如何为iOS构建应用程序，因此您需要安装了Xcode的Mac来运行示例。此外，由于示例使用相机进行检测，因此您还需要具有相机的iOS设备来尝试它。
- en: 'The first thing to do is to convert the model. Unfortunately Apple’s code only
    supports Python 2.7 and also seems to lag a bit when it comes to supporting the
    latest versions of `tensorflow` and `keras`, so we’ll set specific versions. Open
    a shell to set up Python 2.7 with the right requirements and type in:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是转换模型。不幸的是，苹果的代码只支持Python 2.7，并且在支持最新版本的`tensorflow`和`keras`方面似乎有点滞后，所以我们将设置特定版本。打开一个shell来设置Python
    2.7并输入正确的要求，然后输入：
- en: '[PRE42]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then start Python and enter:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后启动Python并输入：
- en: '[PRE43]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Load the previously saved model and the labels:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 加载先前保存的模型和标签：
- en: '[PRE44]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then convert the model:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后转换模型：
- en: '[PRE45]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Tip
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You could also skip this and work with the *.mlmodel* file in the *zoo* directory.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以跳过这一步，在*zoo*目录中使用*.mlmodel*文件进行工作。
- en: Now start Xcode, create a new project, and drag the *PetRecognizer.mlmodel*
    file to the project. Xcode automatically imports the model and makes it callable.
    Let’s recognize some pets!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始Xcode，创建一个新项目，并将*PetRecognizer.mlmodel*文件拖到项目中。Xcode会自动导入模型并使其可调用。让我们识别一些宠物！
- en: Apple has an example project [on its website](https://apple.co/2HPUHOW) that
    uses a standard image recognition network. Download this project, unzip it, and
    then open it with Xcode.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果在其网站上有一个示例项目[链接](https://apple.co/2HPUHOW)，使用了标准的图像识别网络。下载这个项目，解压缩它，然后用Xcode打开它。
- en: In the project overview, you should see a file called *MobileNet.mlmodel*. Delete
    that and then drag the *PetRecognizer.mlmodel* file to where *MobileNet.mlmodel*
    used to be. Now open *ImageClassificationViewController.swift* and replace any
    occurences of `MobileNet` with `PetRecognizer`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目概述中，您应该看到一个名为*MobileNet.mlmodel*的文件。删除它，然后将*PetRecognizer.mlmodel*文件拖到原来*MobileNet.mlmodel*的位置。现在打开*ImageClassificationViewController.swift*，并将任何`MobileNet`的出现替换为`PetRecognizer`。
- en: You should now be able to run the app as before, but with the new model and
    output classes.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您应该能够像以前一样运行该应用程序，但使用新模型和输出类。
- en: Discussion
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Using a Keras model from an iOS app is surprisingly simple, at least if we stick
    to the examples that Apple’s SDK ships with. The technology is quite recent though,
    and there are not a lot of working examples out there that are radically different
    from Apple’s examples. Moreover, CoreML only works on Apple operating systems,
    and then only on iOS 11 or higher or macOS 10.13 or higher.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在iOS应用程序中使用Keras模型非常简单，至少如果我们坚持使用苹果SDK提供的示例的话。尽管这项技术相当新，但在那里没有太多与苹果示例大不相同的可用示例。此外，CoreML仅适用于苹果操作系统，仅适用于iOS
    11或更高版本或macOS 10.13或更高版本。
