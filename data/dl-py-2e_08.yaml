- en: 8 Introduction to deep learning for computer vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 计算机视觉深度学习简介
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding convolutional neural networks (convnets)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解卷积神经网络（卷积网络）
- en: Using data augmentation to mitigate overfitting
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据增强来减轻过拟合
- en: Using a pretrained convnet to do feature extraction
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的卷积网络进行特征提取
- en: Fine-tuning a pretrained convnet
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对预训练的卷积网络进行微调
- en: Computer vision is the earliest and biggest success story of deep learning.
    Every day, you’re interacting with deep vision models—via Google Photos, Google
    image search, YouTube, video filters in camera apps, OCR software, and many more.
    These models are also at the heart of cutting-edge research in autonomous driving,
    robotics, AI-assisted medical diagnosis, autonomous retail checkout systems, and
    even autonomous farming.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是深度学习最早也是最大的成功故事。每天，你都在与深度视觉模型互动——通过Google照片、Google图像搜索、YouTube、相机应用中的视频滤镜、OCR软件等等。这些模型也是自动驾驶、机器人、AI辅助医学诊断、自动零售结账系统甚至自动农业等尖端研究的核心。
- en: Computer vision is the problem domain that led to the initial rise of deep learning
    between 2011 and 2015\. A type of deep learning model called *convolutional neural
    networks* started getting remarkably good results on image classification competitions
    around that time, first with Dan Ciresan winning two niche competitions (the ICDAR
    2011 Chinese character recognition competition and the IJCNN 2011 German traffic
    signs recognition competition), and then more notably in fall 2012 with Hinton’s
    group winning the high-profile ImageNet large-scale visual recognition challenge.
    Many more promising results quickly started bubbling up in other computer vision
    tasks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是在2011年至2015年间导致深度学习初次崛起的问题领域。一种称为*卷积神经网络*的深度学习模型开始在那个时候在图像分类竞赛中取得非常好的结果，首先是Dan
    Ciresan在两个小众竞赛中获胜（2011年ICDAR汉字识别竞赛和2011年IJCNN德国交通标志识别竞赛），然后更引人注目的是2012年秋季Hinton的团队赢得了备受关注的ImageNet大规模视觉识别挑战赛。在其他计算机视觉任务中，很快也涌现出更多有希望的结果。
- en: Interestingly, these early successes weren’t quite enough to make deep learning
    mainstream at the time—it took a few years. The computer vision research community
    had spent many years investing in methods other than neural networks, and it wasn’t
    quite ready to give up on them just because there was a new kid on the block.
    In 2013 and 2014, deep learning still faced intense skepticism from many senior
    computer vision researchers. It was only in 2016 that it finally became dominant.
    I remember exhorting an ex-professor of mine, in February 2014, to pivot to deep
    learning. “It’s the next big thing!” I would say. “Well, maybe it’s just a fad,”
    he replied. By 2016, his entire lab was doing deep learning. There’s no stopping
    an idea whose time has come.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这些早期的成功并不足以使深度学习在当时成为主流——这花了几年的时间。计算机视觉研究社区花了很多年投资于除神经网络之外的方法，他们并不准备放弃这些方法，只因为有了一个新的玩家。在2013年和2014年，深度学习仍然面临着许多资深计算机视觉研究人员的强烈怀疑。直到2016年，它才最终占据主导地位。我记得在2014年2月，我曾劝告我的一位前教授转向深度学习。“这是下一个大事！”我会说。“嗯，也许只是一时的热潮，”他回答。到了2016年，他的整个实验室都在做深度学习。一个时机已经成熟的想法是无法阻挡的。
- en: This chapter introduces convolutional neural networks, also known as *convnets*,
    the type of deep learning model that is now used almost universally in computer
    vision applications. You’ll learn to apply convnets to image-classification problems—in
    particular those involving small training datasets, which are the most common
    use case if you aren’t a large tech company.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了卷积神经网络，也被称为*卷积网络*，这种深度学习模型现在几乎在计算机视觉应用中被普遍使用。你将学会将卷积网络应用于图像分类问题，特别是涉及小训练数据集的问题，如果你不是一个大型科技公司，这是最常见的用例。
- en: 8.1 Introduction to convnets
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 卷积网络简介
- en: We’re about to dive into the theory of what convnets are and why they have been
    so successful at computer vision tasks. But first, let’s take a practical look
    at a simple convnet example that classifies MNIST digits, a task we performed
    in chapter 2 using a densely connected network (our test accuracy then was 97.8%).
    Even though the convnet will be basic, its accuracy will blow our densely connected
    model from chapter 2 out of the water.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要深入探讨卷积网络是什么以及为什么它们在计算机视觉任务中取得如此成功的理论。但首先，让我们从一个简单的卷积网络示例开始，该示例对MNIST数字进行分类，这是我们在第2章中使用全连接网络执行的任务（当时我们的测试准确率为97.8%）。即使卷积网络很基础，它的准确率也会远远超过我们在第2章中使用的全连接模型。
- en: The following listing shows what a basic convnet looks like. It’s a stack of
    `Conv2D` and `MaxPooling2D` layers. You’ll see in a minute exactly what they do.
    We’ll build the model using the Functional API, which we introduced in the previous
    chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了基本卷积网络的外观。它是一堆`Conv2D`和`MaxPooling2D`层。你马上就会看到它们的作用。我们将使用我们在上一章中介绍的函数式API构建模型。
- en: Listing 8.1 Instantiating a small convnet
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.1 实例化一个小型卷积网络
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Importantly, a convnet takes as input tensors of shape `(image_height,` `image_width,`
    `image_channels)`, not including the batch dimension. In this case, we’ll configure
    the convnet to process inputs of size `(28,` `28,` `1)`, which is the format of
    MNIST images.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，卷积神经网络以形状为`(image_height,` `image_width,` `image_channels)`的张量作为输入，不包括批处理维度。在这种情况下，我们将配置卷积网络以处理大小为`(28,`
    `28,` `1)`的输入，这是MNIST图像的格式。
- en: Let’s display the architecture of our convnet.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示一下我们卷积网络的架构。
- en: Listing 8.2 Displaying the model’s summary
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.2 显示模型的摘要
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can see that the output of every `Conv2D` and `MaxPooling2D` layer is a
    rank-3 tensor of shape `(height,` `width,` `channels)`. The width and height dimensions
    tend to shrink as you go deeper in the model. The number of channels is controlled
    by the first argument passed to the `Conv2D` layers (32, 64, or 128).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到每个`Conv2D`和`MaxPooling2D`层的输出是形状为`(height,` `width,` `channels)`的三维张量。随着模型深入，宽度和高度维度会逐渐缩小。通道的数量由传递给`Conv2D`层的第一个参数控制（32、64或128）。
- en: 'After the last `Conv2D` layer, we end up with an output of shape `(3,` `3,`
    `128)`—a 3 × 3 feature map of 128 channels. The next step is to feed this output
    into a densely connected classifier like those you’re already familiar with: a
    stack of `Dense` layers. These classifiers process vectors, which are 1D, whereas
    the current output is a rank-3 tensor. To bridge the gap, we flatten the 3D outputs
    to 1D with a `Flatten` layer before adding the `Dense` layers.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个`Conv2D`层之后，我们得到了一个形状为`(3, 3, 128)`的输出——一个3×3的128通道特征图。下一步是将这个输出馈送到一个类似你已经熟悉的密集连接分类器的地方：一堆`Dense`层。这些分类器处理向量，这些向量是1D的，而当前的输出是一个秩为3的张量。为了弥合这个差距，我们使用`Flatten`层将3D输出展平为1D，然后再添加`Dense`层。
- en: Finally, we do 10-way classification, so our last layer has 10 outputs and a
    softmax activation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们进行10路分类，所以我们的最后一层有10个输出和softmax激活。
- en: Now, let’s train the convnet on the MNIST digits. We’ll reuse a lot of the code
    from the MNIST example in chapter 2\. Because we’re doing 10-way classification
    with a softmax output, we’ll use the categorical crossentropy loss, and because
    our labels are integers, we’ll use the sparse version, `sparse_categorical_crossentropy`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在MNIST数字上训练卷积神经网络。我们将重用第2章MNIST示例中的许多代码。因为我们要进行10路分类，并且输出是softmax，所以我们将使用分类交叉熵损失，因为我们的标签是整数，所以我们将使用稀疏版本，`sparse_categorical_crossentropy`。
- en: Listing 8.3 Training the convnet on MNIST images
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.3 在MNIST图像上训练卷积神经网络
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s evaluate the model on the test data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在测试数据上评估模型。
- en: Listing 8.4 Evaluating the convnet
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.4 评估卷积神经网络
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Whereas the densely connected model from chapter 2 had a test accuracy of 97.8%,
    the basic convnet has a test accuracy of 99.1%: we decreased the error rate by
    about 60% (relative). Not bad!'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之前第2章的密集连接模型的测试准确率为97.8%，基本卷积神经网络的测试准确率为99.1%：我们将错误率降低了约60%（相对）。不错！
- en: But why does this simple convnet work so well, compared to a densely connected
    model? To answer this, let’s dive into what the `Conv2D` and `MaxPooling2D` layers
    do.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为什么这个简单的卷积神经网络效果如此出色，相比之下要好于密集连接模型？为了回答这个问题，让我们深入了解`Conv2D`和`MaxPooling2D`层的作用。
- en: 8.1.1 The convolution operation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 卷积操作
- en: 'The fundamental difference between a densely connected layer and a convolution
    layer is this: `Dense` layers learn global patterns in their input feature space
    (for example, for a MNIST digit, patterns involving all pixels), whereas convolution
    layers learn local patterns—in the case of images, patterns found in small 2D
    windows of the inputs (see figure 8.1). In the previous example, these windows
    were all 3 × 3.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 密集连接层和卷积层之间的根本区别在于：`Dense`层在其输入特征空间中学习全局模式（例如，对于MNIST数字，涉及所有像素的模式），而卷积层学习局部模式——在图像的情况下，是在输入的小2D窗口中找到的模式（见图8.1）。在前面的例子中，这些窗口都是3×3的。
- en: '![](../Images/08-01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-01.png)'
- en: Figure 8.1 Images can be broken into local patterns such as edges, textures,
    and so on.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 图像可以被分解为局部模式，如边缘、纹理等。
- en: 'This key characteristic gives convnets two interesting properties:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关键特征赋予了卷积神经网络两个有趣的特性：
- en: '*The patterns they learn are translation-invariant*. After learning a certain
    pattern in the lower-right corner of a picture, a convnet can recognize it anywhere:
    for example, in the upper-left corner. A densely connected model would have to
    learn the pattern anew if it appeared at a new location. This makes convnets data-efficient
    when processing images (because the *visual world is fundamentally translation-invariant*):
    they need fewer training samples to learn representations that have generalization
    power.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它们学习的模式是平移不变的*。在学习了图片右下角的某个模式后，卷积神经网络可以在任何地方识别它：例如，在左上角。密集连接模型如果出现在新位置，就必须重新学习这个模式。这使得卷积神经网络在处理图像时具有数据效率（因为*视觉世界在根本上是平移不变的*）：它们需要更少的训练样本来学习具有泛化能力的表示。'
- en: '*They can learn spatial hierarchies of patterns*. A first convolution layer
    will learn small local patterns such as edges, a second convolution layer will
    learn larger patterns made of the features of the first layers, and so on (see
    figure 8.2). This allows convnets to efficiently learn increasingly complex and
    abstract visual concepts, because *the visual world is fundamentally spatially
    hierarchical*.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它们可以学习空间模��的层次结构*。第一个卷积层将学习小的局部模式，如边缘，第二个卷积层将学习由第一层特征组成的更大模式，依此类推（见图8.2）。这使得卷积神经网络能够高效地学习越来越复杂和抽象的视觉概念，因为*视觉世界在根本上是空间层次结构的*。'
- en: '![](../Images/08-02.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-02.png)'
- en: 'Figure 8.2 The visual world forms a spatial hierarchy of visual modules: elementary
    lines or textures combine into simple objects such as eyes or ears, which combine
    into high-level concepts such as “cat.”'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 视觉世界形成了视觉模块的空间层次结构：基本线条或纹理组合成简单的对象，如眼睛或耳朵，这些对象组合成高级概念，如“猫”。
- en: 'Convolutions operate over rank-3 tensors called *feature maps*, with two spatial
    axes (*height* and *width*) as well as a *depth* axis (also called the *channels*
    axis). For an RGB image, the dimension of the depth axis is 3, because the image
    has three color channels: red, green, and blue. For a black-and-white picture,
    like the MNIST digits, the depth is 1 (levels of gray). The convolution operation
    extracts patches from its input feature map and applies the same transformation
    to all of these patches, producing an *output feature map*. This output feature
    map is still a rank-3 tensor: it has a width and a height. Its depth can be arbitrary,
    because the output depth is a parameter of the layer, and the different channels
    in that depth axis no longer stand for specific colors as in RGB input; rather,
    they stand for *filters*. Filters encode specific aspects of the input data: at
    a high level, a single filter could encode the concept “presence of a face in
    the input,” for instance.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积在称为*特征图*的秩为3的张量上运行，具有两个空间轴（*高度*和*宽度*）以及一个*深度*轴（也称为*通道*轴）。对于RGB图像，深度轴的维度为3，因为图像具有三个颜色通道：红色、绿色和蓝色。对于像MNIST数字这样的黑白图片，深度为1（灰度级）。卷积操作从其输入特征图中提取补丁，并对所有这些补丁应用相同的变换，生成一个*输出特征图*。这个输出特征图仍然是一个秩为3的张量：它有一个宽度和一个高度。它的深度可以是任意的，因为输出深度是层的一个参数，而该深度轴中的不同通道不再代表RGB输入中的特定颜色；相反，它们代表*滤波器*。滤波器编码输入数据的特定方面：在高层次上，单个滤波器可以编码“输入中存在面孔”的概念，例如。
- en: 'In the MNIST example, the first convolution layer takes a feature map of size
    `(28,` `28,` `1)` and outputs a feature map of size `(26,` `26,` `32)`: it computes
    32 filters over its input. Each of these 32 output channels contains a 26 × 26
    grid of values, which is a *response map* of the filter over the input, indicating
    the response of that filter pattern at different locations in the input (see figure
    8.3).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MNIST 示例中，第一个卷积层接收大小为`(28,` `28,` `1)`的特征图，并输出大小为`(26,` `26,` `32)`的特征图：它在输入上计算32个滤波器。这32个输出通道中的每一个包含一个26×26的值网格，这是滤波器在输入上的*响应图*，指示了该滤波器模式在输入的不同位置的响应（见图8.3）。
- en: '![](../Images/08-03.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-03.png)'
- en: 'Figure 8.3 The concept of a response map: a 2D map of the presence of a pattern
    at different locations in an input'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 响应图的概念：在输入的不同位置显示模式存在的2D地图
- en: 'That is what the term *feature map* means: every dimension in the depth axis
    is a *feature* (or filter), and the rank-2 tensor `output[:,` `:,` `n]` is the
    2D spatial *map* of the response of this filter over the input.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是术语*特征图*的含义：深度轴中的每个维度都是一个*特征*（或滤波器），而张量`output[:,` `:,` `n]`是该滤波器在输入上的2D空间*响应图*。
- en: 'Convolutions are defined by two key parameters:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积由两个关键参数定义：
- en: '*Size of the patches extracted from the inputs*—These are typically 3 × 3 or
    5 × 5\. In the example, they were 3 × 3, which is a common choice.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从输入中提取的补丁的大小*—通常为3×3或5×5。在示例中，它们是3×3，这是一个常见选择。'
- en: '*Depth of the output feature map*—This is the number of filters computed by
    the convolution. The example started with a depth of 32 and ended with a depth
    of 64.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出特征图的深度*—这是卷积计算的滤波器数量。示例从深度为32开始，最终深度为64。'
- en: 'In Keras `Conv2D` layers, these parameters are the first arguments passed to
    the layer: `Conv2D(output_depth,` `(window_height,` `window_width))`.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 的`Conv2D`层中，这些参数是传递给层的第一个参数：`Conv2D(output_depth,` `(window_height,`
    `window_width))`。
- en: A convolution works by *sliding* these windows of size 3 × 3 or 5 × 5 over the
    3D input feature map, stopping at every possible location, and extracting the
    3D patch of surrounding features (shape `(window_height,` `window_width,` `input_depth)`).
    Each such 3D patch is then transformed into a 1D vector of shape `(output_depth,)`,
    which is done via a tensor product with a learned weight matrix, called the *convolution
    kernel*—the same kernel is reused across every patch. All of these vectors (one
    per patch) are then spatially reassembled into a 3D output map of shape `(height,`
    `width,` `output_ depth)`. Every spatial location in the output feature map corresponds
    to the same location in the input feature map (for example, the lower-right corner
    of the output contains information about the lower-right corner of the input).
    For instance, with 3 × 3 windows, the vector `output[i,` `j,` `:]` comes from
    the 3D patch `input[i-1:i+1,` `j-1:j+1,` `:]`. The full process is detailed in
    figure 8.4.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积通过在3D输入特征图上*滑动*大小为3×3或5×5的窗口，在每个可能的位置停止，并提取周围特征的3D补丁（形状为`(window_height,`
    `window_width,` `input_depth)`）。然后，每个这样的3D补丁通过与一个学习的权重矩阵进行张量积转换为形状为`(output_depth,)`的1D向量，称为*卷积核*—相同的核在每个补丁上重复使用。所有这些向量（每个补丁一个）然后在空间上重新组装成形状为`(height,`
    `width,` `output_ depth)`的3D输出图。输出特征图中的每个空间位置对应于输入特征图中的相同位置（例如，输出的右下角包含有关输入右下角的信息）。例如，对于3×3窗口，向量`output[i,`
    `j,` `:]`来自3D补丁`input[i-1:i+1,` `j-1:j+1,` `:]`。整个过程在图8.4中有详细说明。
- en: '![](../Images/08-04.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-04.png)'
- en: Figure 8.4 How convolution works
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 卷积的工作原理
- en: 'Note that the output width and height may differ from the input width and height
    for two reasons:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出宽度和高度可能与输入宽度和高度不同，原因有两个：
- en: Border effects, which can be countered by padding the input feature map
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边界效应，可以通过填充输入特征图来抵消
- en: The use of *strides*, which I’ll define in a second
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步幅*的使用，我将在下一节中定义'
- en: Let’s take a deeper look at these notions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解这些概念。
- en: Understanding border effects and padding
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 理解边界效应和填充
- en: 'Consider a 5 × 5 feature map (25 tiles total). There are only 9 tiles around
    which you can center a 3 × 3 window, forming a 3 × 3 grid (see figure 8.5). Hence,
    the output feature map will be 3 × 3\. It shrinks a little: by exactly two tiles
    alongside each dimension, in this case. You can see this border effect in action
    in the earlier example: you start with 28 × 28 inputs, which become 26 × 26 after
    the first convolution layer.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个5×5的特征图（总共25个瓦片）。只有9个瓦片周围可以放置一个3×3窗口的中心，形成一个3×3的网格（见图8.5）。因此，输出特征图将是3×3。它会略微缩小：在每个维度上正好减少两个瓦片，本例中是这样。您可以在之前的示例中看到这种边界效应：您从28×28的输入开始，经过第一层卷积后变为26×26。
- en: '![](../Images/08-05.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-05.png)'
- en: Figure 8.5 Valid locations of 3 × 3 patches in a 5 × 5 input feature map
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 5×5输入特征图中3×3补丁的有效位置
- en: If you want to get an output feature map with the same spatial dimensions as
    the input, you can use *padding*. Padding consists of adding an appropriate number
    of rows and columns on each side of the input feature map so as to make it possible
    to fit center convolution windows around every input tile. For a 3 × 3 window,
    you add one column on the right, one column on the left, one row at the top, and
    one row at the bottom. For a 5 × 5 window, you add two rows (see figure 8.6).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想获得与输入相同空间维度的输出特征图，可以使用*填充*。填充包括在输入特征图的每一侧添加适当数量的行和列，以便使每个输入瓦片周围都能放置中心卷积窗口。对于3×3窗口，您在右侧添加一列，在左侧添加一列，在顶部添加一行，在底部添加一行。对于5×5窗口，您添加两行（见图8.6）。
- en: '![](../Images/08-06.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-06.png)'
- en: Figure 8.6 Padding a 5 × 5 input in order to be able to extract 25 3 × 3 patches
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 对5×5输入进行填充以便提取25个3×3补丁
- en: 'In `Conv2D` layers, padding is configurable via the `padding` argument, which
    takes two values: `"valid"`, which means no padding (only valid window locations
    will be used), and `"same"`, which means “pad in such a way as to have an output
    with the same width and height as the input.” The `padding` argument defaults
    to `"valid"`.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Conv2D`层中，填充可以通过`padding`参数进行配置，该参数接受两个值："valid"表示无填充（只使用有效的窗口位置），"same"表示“填充以使输出具有与输入相同的宽度和高度”。`padding`参数默认为"valid"。
- en: Understanding convolution strides
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 理解卷积步幅
- en: 'The other factor that can influence output size is the notion of *strides*.
    Our description of convolution so far has assumed that the center tiles of the
    convolution windows are all contiguous. But the distance between two successive
    windows is a parameter of the convolution, called its *stride*, which defaults
    to 1\. It’s possible to have *strided convolutions*: convolutions with a stride
    higher than 1\. In figure 8.7, you can see the patches extracted by a 3 × 3 convolution
    with stride 2 over a 5 × 5 input (without padding).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 影响输出大小的另一个因素是*步幅*的概念。到目前为止，我们对卷积的描述假定卷积窗口的中心瓦片都是连续的。但是两个连续窗口之间的距离是卷积的一个参数，称为*步幅*，默认为1。可以进行*步幅卷积*：步幅大于1的卷积。在图8.7中，您可以看到在5×5输入（无填充）上使用步幅2进行3×3卷积提取的补丁。
- en: '![](../Images/08-07.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-07.png)'
- en: Figure 8.7 3 × 3 convolution patches with 2 × 2 strides
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 2×2步幅下的3×3卷积补丁
- en: Using stride 2 means the width and height of the feature map are downsampled
    by a factor of 2 (in addition to any changes induced by border effects). Strided
    convolutions are rarely used in classification models, but they come in handy
    for some types of models, as you will see in the next chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用步幅2意味着特征图的宽度和高度会被下采样2倍（除了边界效应引起的任何变化）。步幅卷积在分类模型中很少使用，但对于某些类型的模型非常有用，您将在下一章中看到。
- en: In classification models, instead of strides, we tend to use the *max-pooling*
    operation to downsample feature maps, which you saw in action in our first convnet
    example. Let’s look at it in more depth.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类模型中，我们倾向于使用*最大池化*操作来对特征图进行下采样，您在我们的第一个卷积神经网络示例中看到了它的作用。让我们更深入地看一下。
- en: 8.1.2 The max-pooling operation
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 最大池化操作
- en: 'In the convnet example, you may have noticed that the size of the feature maps
    is halved after every `MaxPooling2D` layer. For instance, before the first `MaxPooling2D`
    layers, the feature map is 26 × 26, but the max-pooling operation halves it to
    13 × 13\. That’s the role of max pooling: to aggressively downsample feature maps,
    much like strided convolutions.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络示例中，您可能已经注意到在每个`MaxPooling2D`层之后特征图的大小减半。例如，在第一个`MaxPooling2D`层之前，特征图为26×26，但最大池化操作将其减半为13×13。这就是最大池化的作用：大幅度地对特征图进行下采样，类似于步幅卷积。
- en: Max pooling consists of extracting windows from the input feature maps and outputting
    the max value of each channel. It’s conceptually similar to convolution, except
    that instead of transforming local patches via a learned linear transformation
    (the convolution kernel), they’re transformed via a hardcoded `max` tensor operation.
    A big difference from convolution is that max pooling is usually done with 2 ×
    2 windows and stride 2, in order to downsample the feature maps by a factor of
    2\. On the other hand, convolution is typically done with 3 × 3 windows and no
    stride (stride 1).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化包括从输入特征图中提取窗口并输出每个通道的最大值。它在概念上类似于卷积，不同之处在于，最大池化不是通过学习的线性变换（卷积核）来转换局部补丁，而是通过硬编码的`max`张量操作来转换。与卷积的一个重要区别是，最大池化通常使用2×2窗口和步幅2进行，以便将特征图下采样2倍。另一方面，卷积通常使用3×3窗口和无步幅（步幅1）。
- en: Why downsample feature maps this way? Why not remove the max-pooling layers
    and keep fairly large feature maps all the way up? Let’s look at this option.
    Our model would then look like the following listing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要以这种方式对特征图进行下采样？为什么不删除最大池化层，一直保持相当大的特征图？让我们看看这个选项。我们的模型将如下所示。
- en: Listing 8.5 An incorrectly structured convnet missing its max-pooling layers
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5 一个结构不正确的卷积神经网络，缺少最大池化层
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here’s a summary of the model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是模型的摘要：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'What’s wrong with this setup? Two things:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置有什么问题？有两个问题：
- en: It isn’t conducive to learning a spatial hierarchy of features. The 3 × 3 windows
    in the third layer will only contain information coming from 7 × 7 windows in
    the initial input. The high-level patterns learned by the convnet will still be
    very small with regard to the initial input, which may not be enough to learn
    to classify digits (try recognizing a digit by only looking at it through windows
    that are 7 × 7 pixels!). We need the features from the last convolution layer
    to contain information about the totality of the input.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这不利于学习特征的空间层次结构。第三层中的3×3窗口只包含来自初始输入的7×7窗口的信息。卷积网络学到的高级模式仍然相对于初始输入非常小，这可能不足以学会分类数字（尝试仅通过查看7×7像素窗口来识别数字！）。我们需要最后一个卷积层的特征包含关于整个输入的信息。
- en: The final feature map has 22 × 22 × 128 = 61,952 total coefficients per sample.
    This is huge. When you flatten it to stick a `Dense` layer of size 10 on top,
    that layer would have over half a million parameters. This is far too large for
    such a small model and would result in intense overfitting.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终的特征图每个样本有22×22×128 = 61,952个总系数。这是巨大的。当你将其展平以在顶部放置一个`Dense`层大小为10时，该层将有超过一百万个参数。对于这样一个小模型来说，这太大了，会导致严重的过拟合。
- en: In short, the reason to use downsampling is to reduce the number of feature-map
    coefficients to process, as well as to induce spatial-filter hierarchies by making
    successive convolution layers look at increasingly large windows (in terms of
    the fraction of the original input they cover).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，使用降采样的原因是减少要处理的特征图系数的数量，并通��使连续的卷积层查看越来越大的窗口（就覆盖原始输入的部分而言）来引入空间滤波器层次结构。
- en: Note that max pooling isn’t the only way you can achieve such downsampling.
    As you already know, you can also use strides in the prior convolution layer.
    And you can use average pooling instead of max pooling, where each local input
    patch is transformed by taking the average value of each channel over the patch,
    rather than the max. But max pooling tends to work better than these alternative
    solutions. The reason is that features tend to encode the spatial presence of
    some pattern or concept over the different tiles of the feature map (hence the
    term *feature map*), and it’s more informative to look at the *maximal presence*
    of different features than at their *average presence*. The most reasonable subsampling
    strategy is to first produce dense maps of features (via unstrided convolutions)
    and then look at the maximal activation of the features over small patches, rather
    than looking at sparser windows of the inputs (via strided convolutions) or averaging
    input patches, which could cause you to miss or dilute feature-presence information.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最大池化并不是唯一实现这种降采样的方法。正如你已经知道的，你也可以在先前的卷积层中使用步幅。你也可以使用平均池化代替最大池化，其中每个局部输入块通过取该块上每个通道的平均值来进行转换，而不是最大值。但是最大池化往往比这些替代方案效果更好。原因在于特征往往编码了特定模式或概念在特征图的不同块上的空间存在（因此术语*特征图*），查看不同特征的*最大存在*比查看它们的*平均存在*更具信息量。最合理的子采样策略是首先通过非步幅卷积生成密集特征图，然后查看特征在小块上的最大激活，而不是查看输入的稀疏窗口（通过步幅卷积）或平均输入块，这可能导致您错过或稀释特征存在信息。
- en: At this point, you should understand the basics of convnets—feature maps, convolution,
    and max pooling—and you should know how to build a small convnet to solve a toy
    problem such as MNIST digits classification. Now let’s move on to more useful,
    practical applications.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你应该了解卷积网络的基础知识——特征图、卷积和最大池化，并且应该知道如何构建一个小型卷积网络来解决诸如MNIST数字分类之类的玩具问题。现在让我们转向更有用、实际的应用。
- en: 8.2 Training a convnet from scratch on a small dataset
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 在小数据集上从头开始训练卷积网络
- en: Having to train an image-classification model using very little data is a common
    situation, which you’ll likely encounter in practice if you ever do computer vision
    in a professional context. A “few” samples can mean anywhere from a few hundred
    to a few tens of thousands of images. As a practical example, we’ll focus on classifying
    images as dogs or cats in a dataset containing 5,000 pictures of cats and dogs
    (2,500 cats, 2,500 dogs). We’ll use 2,000 pictures for training, 1,000 for validation,
    and 2,000 for testing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 不得不使用非常少的数据训练图像分类模型是一种常见情况，在实践中，如果你在专业环境中进行计算机视觉，你可能会遇到这种情况。少量样本可以是从几百到几万张图像。作为一个实际例子，我们将专注于在一个包含5,000张猫和狗图片的数据集中对图像进行分类（2,500只猫，2,500只狗）。我们将使用2,000张图片进行训练，1,000张用于验证，2,000张用于测试。
- en: 'In this section, we’ll review one basic strategy to tackle this problem: training
    a new model from scratch using what little data you have. We’ll start by naively
    training a small convnet on the 2,000 training samples, without any regularization,
    to set a baseline for what can be achieved. This will get us to a classification
    accuracy of about 70%. At that point, the main issue will be overfitting. Then
    we’ll introduce *data augmentation*, a powerful technique for mitigating overfitting
    in computer vision. By using data augmentation, we’ll improve the model to reach
    an accuracy of 80–85%.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾一种基本策略来解决这个问题：使用你拥有的少量数据从头开始训练一个新模型。我们将从头开始训练一个小型卷积网络，使用2,000个训练样本，没有任何正则化，来建立一个可以实现的基准。这将使我们达到约70%的分类准确率。在那时，主要问题将是过拟合。然后我们将介绍*数据增强*，这是一种在计算机视觉中减轻过拟合的强大技术。通过使用数据增强，我们将改进模型，使准确率达到80-85%。
- en: 'In the next section, we’ll review two more essential techniques for applying
    deep learning to small datasets: *feature extraction with a pretrained model*
    (which will get us to an accuracy of 97.5%) and *fine-tuning a pretrained model*
    (which will get us to a final accuracy of 98.5%). Together, these three strategies—training
    a small model from scratch, doing feature extraction using a pretrained model,
    and fine-tuning a pretrained model—will constitute your future toolbox for tackling
    the problem of performing image classification with small datasets.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾将深度学习应用于小数据集的另外两种基本技术：*使用预训练模型进行特征提取*（这将使我们达到 97.5% 的准确率）和*微调预训练模型*（这将使我们达到最终准确率
    98.5%）。这三种策略——从头开始训练一个小模型、使用预训练模型进行特征提取以及微调预训练模型——将构成您未来解决使用小数据集进行图像分类问题的工具箱。
- en: 8.2.1 The relevance of deep learning for small-data problems
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 深度学习在小数据问题上的相关性
- en: What qualifies as “enough samples” to train a model is relative—relative to
    the size and depth of the model you’re trying to train, for starters. It isn’t
    possible to train a convnet to solve a complex problem with just a few tens of
    samples, but a few hundred can potentially suffice if the model is small and well
    regularized and the task is simple. Because convnets learn local, translation-invariant
    features, they’re highly data-efficient on perceptual problems. Training a convnet
    from scratch on a very small image dataset will yield reasonable results despite
    a relative lack of data, without the need for any custom feature engineering.
    You’ll see this in action in this section.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 什么样的“足够样本”可以用来训练模型是相对的——相对于您尝试训练的模型的大小和深度。仅凭几十个样本无法训练卷积网络解决复杂问题，但如果模型小且经过良好的正则化，任务简单，那么几百个样本可能就足够了。因为卷积网络学习局部、平移不变的特征，它们在感知问题上具有高效的数据利用率。在非常小的图像数据集上从头开始训练卷积网络将产生合理的结果，尽管数据相对较少，无需进行任何自定义特征工程。您将在本节中看到这一点。
- en: 'What’s more, deep learning models are by nature highly repurposable: you can
    take, say, an image-classification or speech-to-text model trained on a large-scale
    dataset and reuse it on a significantly different problem with only minor changes.
    Specifically, in the case of computer vision, many pretrained models (usually
    trained on the ImageNet dataset) are now publicly available for download and can
    be used to bootstrap powerful vision models out of very little data. This is one
    of the greatest strengths of deep learning: feature reuse. You’ll explore this
    in the next section.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习模型天生具有高度的可重用性：您可以拿一个在大规模数据集上训练的图像分类或语音转文本模型，仅进行轻微更改就可以在完全不同的问题上重用它。具体来说，在计算机视觉领域，现在有许多预训练模型（通常在
    ImageNet 数据集上训练）可以公开下载，并且可以用来从很少的数据中启动强大的视觉模型。这是深度学习的最大优势之一：特征重用。您将在下一节中探索这一点。
- en: Let’s start by getting our hands on the data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始获取数据。
- en: 8.2.2 Downloading the data
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 下载数据
- en: The Dogs vs. Cats dataset that we will use isn’t packaged with Keras. It was
    made available by Kaggle as part of a computer vision competition in late 2013,
    back when convnets weren’t mainstream. You can download the original dataset from
    [www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data)
    (you’ll need to create a Kaggle account if you don’t already have one—don’t worry,
    the process is painless). You can also use the Kaggle API to download the dataset
    in Colab (see the “Downloading a Kaggle dataset in Google Colaboratory” sidebar).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的 Dogs vs. Cats 数据集不随 Keras 打包。它是由 Kaggle 在 2013 年底作为计算机视觉竞赛的一部分提供的，当时卷积网络还不是主流。您可以从
    [www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data)
    下载原始数据集（如果您还没有 Kaggle 帐户，需要创建一个—不用担心，这个过程很简单）。您���可以使用 Kaggle API 在 Colab 中下载数据集（请参阅“在
    Google Colaboratory 中下载 Kaggle 数据集”侧边栏）。
- en: Downloading a Kaggle dataset in Google Colaboratory
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colaboratory 中下载 Kaggle 数据集
- en: 'Kaggle makes available an easy-to-use API to programmatically download Kaggle-hosted
    datasets. You can use it to download the Dogs vs. Cats dataset to a Colab notebook,
    for instance. This API is available as the `kaggle` package, which is preinstalled
    on Colab. Downloading this dataset is as easy as running the following command
    in a Colab cell:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 提供了一个易于使用的 API，用于以编程方式下载托管在 Kaggle 上的数据集。例如，您可以使用它将 Dogs vs. Cats 数据集下载到
    Colab 笔记本中。这个 API 可以作为 `kaggle` 包使用，在 Colab 上预先安装。在 Colab 单元格中运行以下命令就可以轻松下载这个数据集：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: However, access to the API is restricted to Kaggle users, so in order to run
    the preceding command, you first need to authenticate yourself. The `kaggle` package
    will look for your login credentials in a JSON file located at ~/.kaggle/kaggle.json.
    Let’s create this file.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，API 的访问权限仅限于 Kaggle 用户，因此为了运行上述命令，您首先需要进行身份验证。`kaggle` 包将在位于 ~/.kaggle/kaggle.json
    的 JSON 文件中查找您的登录凭据。让我们创建这个文件。
- en: First, you need to create a Kaggle API key and download it to your local machine.
    Just navigate to the Kaggle website in a web browser, log in, and go to the My
    Account page. In your account settings, you’ll find an API section. Clicking the
    Create New API Token button will generate a kaggle.json key file and will download
    it to your machine.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要创建一个 Kaggle API 密钥并将其下载到本地计算机。只需在 Web 浏览器中导航到 Kaggle 网站，登录，然后转到“我的帐户”页面。在您的帐户设置中，您会找到一个
    API 部分。点击“创建新的 API 令牌”按钮将生成一个 kaggle.json 密钥文件，并将其下载到您的计算机。
- en: 'Second, go to your Colab notebook, and upload the API’s key JSON file to your
    Colab session by running the following code in a notebook cell:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，转到您的 Colab 笔记本，并通过在笔记本单元格中运行以下代码将 API 密钥 JSON 文件上传到您的 Colab 会话：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When you run this cell, you will see a Choose Files button appear. Click it
    and select the kaggle.json file you just downloaded. This uploads the file to
    the local Colab runtime.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行此单元格时，您将看到一个“选择文件”按钮出现。点击它并选择您刚下载的 kaggle.json 文件。这将上传文件到本地的 Colab 运行时。
- en: 'Finally, create a ~/.kaggle folder (`mkdir` `~/.kaggle`), and copy the key
    file to it (`cp` `kaggle.json` `~/.kaggle/`). As a security best practice, you
    should also make sure that the file is only readable by the current user, yourself
    (`chmod` `600`):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，创建一个 `~/.kaggle` 文件夹（`mkdir ~/.kaggle`），并将密钥文件复制到其中（`cp kaggle.json ~/.kaggle/`）。作为安全最佳实践，您还应确保该文件仅可由当前用户，即您自己（`chmod
    600`）读取：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can now download the data we’re about to use:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以下载我们即将使用的数据：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first time you try to download the data, you may get a “403 Forbidden” error.
    That’s because you need to accept the terms associated with the dataset before
    you download it—you’ll have to go to [www.kaggle.com/c/dogs-vs-cats/rules](https://www.kaggle.com/c/dogs-vs-cats/rules)
    (while logged into your Kaggle account) and click the I Understand and Accept
    button. You only need to do this once.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次尝试下载数据时，您可能会收到“403 Forbidden”错误。这是因为您需要在下载数据之前接受与数据集相关的条款 - 您需要转到 [www.kaggle.com/c/dogs-vs-cats/rules](https://www.kaggle.com/c/dogs-vs-cats/rules)（登录到您的
    Kaggle 帐户）并点击“我理解并接受”按钮。您只需要执行一次此操作。
- en: 'Finally, the training data is a compressed file named train.zip. Make sure
    you uncompress it (`unzip`) silently (`-qq`):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，训练数据是一个名为 `train.zip` 的压缩文件。确保您安静地解压缩它（`unzip -qq`）：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The pictures in our dataset are medium-resolution color JPEGs. Figure 8.8 shows
    some examples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的图片是中等分辨率的彩色 JPEG 图片。图 8.8 展示了一些示例。
- en: '![](../Images/08-08.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-08.png)'
- en: 'Figure 8.8 Samples from the Dogs vs. Cats dataset. Sizes weren’t modified:
    the samples come in different sizes, colors, backgrounds, etc.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 显示了来自狗与猫数据集的样本。大小没有被修改：样本具有不同的大小、颜色、背景等。
- en: Unsurprisingly, the original dogs-versus-cats Kaggle competition, all the way
    back in 2013, was won by entrants who used convnets. The best entries achieved
    up to 95% accuracy. In this example, we will get fairly close to this accuracy
    (in the next section), even though we will train our models on less than 10% of
    the data that was available to the competitors.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，最早的狗与猫 Kaggle 竞赛，即 2013 年，是由使用卷积网络的参赛者赢得的。最好的参赛作品达到了高达 95% 的准确率。在这个示例中，我们将接近这个准确率（在下一节中），即使我们将在可用于参赛者的数据的不到
    10% 上训练我们的模型。
- en: 'This dataset contains 25,000 images of dogs and cats (12,500 from each class)
    and is 543 MB (compressed). After downloading and uncompressing the data, we’ll
    create a new dataset containing three subsets: a training set with 1,000 samples
    of each class, a validation set with 500 samples of each class, and a test set
    with 1,000 samples of each class. Why do this? Because many of the image datasets
    you’ll encounter in your career only contain a few thousand samples, not tens
    of thousands. Having more data available would make the problem easier, so it’s
    good practice to learn with a small dataset.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含 25,000 张狗和猫的图片（每类 12,500 张）并且大小为 543 MB（压缩）。在下载和解压缩数据后，我们将创建一个新数据集，其中包含三个子集：一个包含每个类别
    1,000 个样本的训练集，一个包含每个类别 500 个样本的验证集，以及一个包含每个类别 1,000 个样本的测试集。为什么这样做？因为您在职业生涯中遇到的许多图像数据集只包含几千个样本，而不是数万个。有更多的数据可用会使问题变得更容易，因此最好的做法是使用一个小数据集进行学习。
- en: 'The subsampled dataset we will work with will have the following directory
    structure:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的子采样数据集将具有以下目录结构：
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Contains 1,000 cat images
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 包含 1,000 张猫的图片
- en: ❷ Contains 1,000 dog images
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 包含 1,000 张狗的图片
- en: ❸ Contains 500 cat images
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 包含 500 张猫的图片
- en: ❹ Contains 500 dog images
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 包含 500 张狗的图片
- en: ❺ Contains 1,000 cat images
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 包含 1,000 张猫的图片
- en: ❻ Contains 1,000 dog images
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 包含 1,000 张狗的图片
- en: Let’s make it happen in a couple calls to `shutil`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过几次调用 `shutil` 来实现。
- en: Listing 8.6 Copying images to training, validation, and test directories
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.6 将图片复制到训练、验证和测试目录
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Path to the directory where the original dataset was uncompressed
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 包含原始数据集解压缩后的目录路径
- en: ❷ Directory where we will store our smaller dataset
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们将存储我们较小数据集的目录
- en: ❸ Utility function to copy cat (and dog) images from index start_index to index
    end_index to the subdirectory new_base_dir/{subset_name}/cat (and /dog). The "subset_name"
    will be either "train", "validation", or "test".
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 复制猫（和狗）图片的实用函数，从索引 `start_index` 到索引 `end_index` 复制到子目录 `new_base_dir/{subset_name}/cat`（和/dog）。"subset_name"
    将是 "train"、"validation" 或 "test" 中的一个。
- en: ❹ Create the training subset with the first 1,000 images of each category.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建训练子集，包括每个类别的前 1,000 张图片。
- en: ❺ Create the validation subset with the next 500 images of each category.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建验证子集，包括每个类别的接下来的 500 张图片。
- en: ❻ Create the test subset with the next 1,000 images of each category.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 创建测试子集，包括每个类别的接下来的 1,000 张图片。
- en: 'We now have 2,000 training images, 1,000 validation images, and 2,000 test
    images. Each split contains the same number of samples from each class: this is
    a balanced binary-classification problem, which means classification accuracy
    will be an appropriate measure of success.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有 2,000 张训练图片，1,000 张验证图片和 2,000 张测试图片。每个拆分包含每个类别相同数量的样本：这是一个平衡的二元分类问题，这意味着分类准确率将是一个适当的成功衡量标准。
- en: 8.2.3 Building the model
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 构建模型
- en: 'We will reuse the same general model structure you saw in the first example:
    the convnet will be a stack of alternated `Conv2D` (with `relu` activation) and
    `MaxPooling2D` layers.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用你在第一个示例中看到的相同的通用模型结构：卷积网络将是交替的 `Conv2D`（带有 `relu` 激活）和 `MaxPooling2D` 层的堆叠。
- en: 'But because we’re dealing with bigger images and a more complex problem, we’ll
    make our model larger, accordingly: it will have two more `Conv2D` and `MaxPooling2D`
    stages. This serves both to augment the capacity of the model and to further reduce
    the size of the feature maps so they aren’t overly large when we reach the `Flatten`
    layer. Here, because we start from inputs of size 180 pixels × 180 pixels (a somewhat
    arbitrary choice), we end up with feature maps of size 7 × 7 just before the `Flatten`
    layer.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于我们处理的是更大的图片和更复杂的问题，我们将相应地使我们的模型更大：它将有两个额外的 `Conv2D` 和 `MaxPooling2D` 阶段。这既增加了模型的容量，也进一步减小了特征图的大小，以便在达到
    `Flatten` 层时它们不会过大。在这里，因为我们从大小为 180 像素 × 180 像素的输入开始（这是一个有点随意的选择），我们最终得到了在 `Flatten`
    层之前大小为 7 × 7 的特征图。
- en: Note The depth of the feature maps progressively increases in the model (from
    32 to 256), whereas the size of the feature maps decreases (from 180 × 180 to
    7 × 7). This is a pattern you’ll see in almost all convnets.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：特征图的深度在模型中逐渐增加（从 32 增加到 256），而特征图的大小在减小（从 180 × 180 减小到 7 × 7）。这是您几乎在所有卷积网络中看到的模式。
- en: Because we’re looking at a binary-classification problem, we’ll end the model
    with a single unit (a `Dense` layer of size 1) and a `sigmoid` activation. This
    unit will encode the probability that the model is looking at one class or the
    other.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们正在处理一个二分类问题，所以我们将模型以一个单元（大小为 1 的 `Dense` 层）和一个 `sigmoid` 激活结束。这个单元将编码模型正在查看的是一个类还是另一个类的概率。
- en: 'One last small difference: we will start the model with a `Rescaling` layer,
    which will rescale image inputs (whose values are originally in the [0, 255] range)
    to the [0, 1] range.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个小差异：我们将使用一个 `Rescaling` 层开始模型，它将重新缩放图像输入（其值最初在 [0, 255] 范围内）到 [0, 1] 范围内。
- en: Listing 8.7 Instantiating a small convnet for dogs vs. cats classification
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.7 实例化一个用于狗与猫分类的小型卷积网络
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ The model expects RGB images of size 180 × 180.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 模型期望尺寸为 180 × 180 的 RGB 图像。
- en: ❷ Rescale inputs to the [0, 1] range by dividing them by 255.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将输入重新缩放到 [0, 1] 范围，通过将它们除以 255。
- en: 'Let’s look at how the dimensions of the feature maps change with every successive
    layer:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看随着每一层的连续变化，特征图的维度如何改变：
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For the compilation step, we’ll go with the `RMSprop` optimizer, as usual. Because
    we ended the model with a single sigmoid unit, we’ll use binary crossentropy as
    the loss (as a reminder, check out table 6.1 in chapter 6 for a cheat sheet on
    which loss function to use in various situations).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编译步骤，我们将继续使用 `RMSprop` 优化器。因为我们最后一层是一个单一的 sigmoid 单元，所以我们将使用二元交叉熵作为损失函数（作为提醒，请查看第
    6 章中表 6.1，了解在各种情况下使用哪种损失函数的速查表）。
- en: Listing 8.8 Configuring the model for training
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.8 配置用于训练的模型
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 8.2.4 Data preprocessing
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.4 数据预处理
- en: 'As you know by now, data should be formatted into appropriately preprocessed
    floating-point tensors before being fed into the model. Currently, the data sits
    on a drive as JPEG files, so the steps for getting it into the model are roughly
    as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你现在所知，数据在被馈送到模型之前应该被格式化为适当预处理的浮点张量。目前，数据以 JPEG 文件的形式存储在驱动器上，因此将其传递到模型的步骤大致如下：
- en: Read the picture files.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取图片文件。
- en: Decode the JPEG content to RGB grids of pixels.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 JPEG 内容解码为 RGB 像素网格。
- en: Convert these into floating-point tensors.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将它们转换为浮点张量。
- en: Resize them to a shared size (we’ll use 180 × 180).
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整它们到共享大小（我们将使用 180 × 180）。
- en: Pack them into batches (we’ll use batches of 32 images).
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将它们打包成批次（我们将使用 32 张图像的批次）。
- en: It may seem a bit daunting, but fortunately Keras has utilities to take care
    of these steps automatically. In particular, Keras features the utility function
    `image_dataset_from_ directory()`, which lets you quickly set up a data pipeline
    that can automatically turn image files on disk into batches of preprocessed tensors.
    This is what we’ll use here.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有点令人生畏，但幸运的是，Keras 有工具可以自动处理这些步骤。特别是，Keras 提供了实用函数 `image_dataset_from_directory()`，它可以让您快速设置一个数据管道，可以自动将磁盘上的图像文件转换为预处理张量的批次。这就是我们将在这里使用的方法。
- en: Calling `image_dataset_from_directory(directory)` will first list the subdirectories
    of `directory` and assume each one contains images from one of our classes. It
    will then index the image files in each subdirectory. Finally, it will create
    and return a `tf.data.Dataset` object configured to read these files, shuffle
    them, decode them to tensors, resize them to a shared size, and pack them into
    batches.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `image_dataset_from_directory(directory)` 首先会列出 `directory` 的子目录，并假定每个子目录包含一��类别的图像。然后，它将索引每个子目录中的图像文件。最后，它将创建并返回一个配置为读取这些文件、对其进行洗牌、解码为张量、调整大小为共享大小并打包成批次的
    `tf.data.Dataset` 对象。
- en: Listing 8.9 Using `image_dataset_from_directory` to read images
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.9 使用 `image_dataset_from_directory` 读取图像
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Understanding TensorFlow `Dataset` objects
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 TensorFlow `Dataset` 对象
- en: TensorFlow makes available the `tf.data` API to create efficient input pipelines
    for machine learning models. Its core class is `tf.data.Dataset`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了 `tf.data` API 来为机器学习模型创建高效的输入管道。其核心类是 `tf.data.Dataset`。
- en: 'A `Dataset` object is an iterator: you can use it in a `for` loop. It will
    typically return batches of input data and labels. You can pass a `Dataset` object
    directly to the `fit()` method of a Keras model.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset` 对象是一个迭代器：你可以在 `for` 循环中使用它。它通常会返回输入数据和标签的批次。你可以直接将 `Dataset` 对象传递给
    Keras 模型的 `fit()` 方法。'
- en: The `Dataset` class handles many key features that would otherwise be cumbersome
    to implement yourself—in particular, asynchronous data prefetching (preprocessing
    the next batch of data while the previous one is being handled by the model, which
    keeps execution flowing without interruptions).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset` 类处理许多关键功能，否则实现起来会很麻烦，特别是异步数据预取（在模型处理上一个批次数据的同时预处理下一个批次数据，从而保持执行流畅而没有中断）。'
- en: 'The `Dataset` class also exposes a functional-style API for modifying datasets.
    Here’s a quick example: let’s create a `Dataset` instance from a NumPy array of
    random numbers. We’ll consider 1,000 samples, where each sample is a vector of
    size 16:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset` 类还提供了一个用于修改数据集的函数式 API。这里有一个快速示例：让我们从一个大小为 16 的随机数 NumPy 数组创建一个 `Dataset`
    实例。我们将考虑 1,000 个样本，每个样本是一个大小为 16 的向量：'
- en: '[PRE17]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The from_tensor_slices() class method can be used to create a Dataset from
    a NumPy array, or a tuple or dict of NumPy arrays.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 `from_tensor_slices()` 类方法可以从 NumPy 数组、元组或字典中创建一个 Dataset。
- en: 'At first, our dataset just yields single samples:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我们的数据集只产生单个样本：
- en: '[PRE18]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can use the `.batch()` method to batch the data:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `.batch()` 方法对数据进行分批处理：
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: More broadly, we have access to a range of useful dataset methods, such as
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，我们可以访问一系列有用的数据集方法，例如
- en: '`.shuffle(buffer_size)`—Shuffles elements within a buffer'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.shuffle(buffer_size)`—在缓冲区内对元素进行洗牌'
- en: '`.prefetch(buffer_size)`—Prefetches a buffer of elements in GPU memory to achieve
    better device utilization.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.prefetch(buffer_size)`—预取 GPU 内存中的一组元素，以实现更好的设备利用率。'
- en: '`.map(callable)`—Applies an arbitrary transformation to each element of the
    dataset (the function `callable`, which expects to take as input a single element
    yielded by the dataset).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.map(callable)`—对数据集的每个元素应用任意转换（函数`callable`，期望接受数据集产生的单个元素作为输入）。'
- en: 'The `.map()` method, in particular, is one that you will use often. Here’s
    an example. We’ll use it to reshape the elements in our toy dataset from shape
    `(16,)` to shape `(4,` `4)`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`.map()`方法特别常用。这里有一个例子。我们将用它将我们的玩具数据集中的元素从形状`(16,)`改变为形状`(4,` `4)`：'
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You’re about to see more `map()` action in this chapter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将看到更多`map()`的应用。
- en: 'Let’s look at the output of one of these `Dataset` objects: it yields batches
    of 180 × 180 RGB images (shape `(32,` `180,` `180,` `3)`) and integer labels (shape
    `(32,)`). There are 32 samples in each batch (the batch size).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看其中一个`Dataset`对象的输出：它产生大小为`(32,` `180,` `180,` `3)`的 RGB 图像批次和整数标签（形状为`(32,)`）。每个批次中有
    32 个样本（批次大小）。
- en: Listing 8.10 Displaying the shapes of the data and labels yielded by the `Dataset`
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.10 显示`Dataset`产生的数据和标签的形状
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let’s fit the model on our dataset. We’ll use the `validation_data` argument
    in `fit()` to monitor validation metrics on a separate `Dataset` object.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的数据集上拟合模型。我们将使用`fit()`中的`validation_data`参数来监视单独的`Dataset`对象上的验证指标。
- en: 'Note that we’ll also use a `ModelCheckpoint` callback to save the model after
    each epoch. We’ll configure it with the path specifying where to save the file,
    as well as the arguments `save_best_only=True` and `monitor="val_loss"`: they
    tell the callback to only save a new file (overwriting any previous one) when
    the current value of the `val_loss` metric is lower than at any previous time
    during training. This guarantees that your saved file will always contain the
    state of the model corresponding to its best-performing training epoch, in terms
    of its performance on the validation data. As a result, we won’t have to retrain
    a new model for a lower number of epochs if we start overfitting: we can just
    reload our saved file.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还将使用`ModelCheckpoint`回调来在每个周期后保存模型。我们将配置它的路径，指定保存文件的位置，以及参数`save_best_only=True`和`monitor="val_loss"`：它们告诉回调只在当前`val_loss`指标的值低于训练过程中任何先前时间的值时保存新文��（覆盖任何先前的文件）。这确保了你保存的文件始终包含模型对验证数据表现最佳的训练周期状态。因此，如果开始过拟合，我们不必重新训练一个更少周期的模型：我们只需重新加载保存的文件。
- en: Listing 8.11 Fitting the model using a `Dataset`
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.11 使用`Dataset`拟合模型
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let’s plot the loss and accuracy of the model over the training and validation
    data during training (see figure 8.9).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制模型在训练和验证数据上的损失和准确率随训练过程的变化（见图 8.9）。
- en: Listing 8.12 Displaying curves of loss and accuracy during training
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.12 显示训练过程中损失和准确率的曲线
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/08-09.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-09.png)'
- en: Figure 8.9 Training and validation metrics for a simple convnet
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 简单卷积网络的训练和验证指标
- en: These plots are characteristic of overfitting. The training accuracy increases
    linearly over time, until it reaches nearly 100%, whereas the validation accuracy
    peaks at 75%. The validation loss reaches its minimum after only ten epochs and
    then stalls, whereas the training loss keeps decreasing linearly as training proceeds.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表是过拟合的特征。训练准确率随时间线性增加，直到接近100%，而验证准确率在75%时达到峰值。验证损失在仅十个周期后达到最小值，然后停滞，而训练损失随着训练的进行线性减少。
- en: Let’s check the test accuracy. We’ll reload the model from its saved file to
    evaluate it as it was before it started overfitting.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查测试准确率。我们将从保存的文件重新加载模型以评估它在过拟合之前的状态。
- en: Listing 8.13 Evaluating the model on the test set
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.13 在测试集上评估模型
- en: '[PRE24]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We get a test accuracy of 69.5%. (Due to the randomness of neural network initializations,
    you may get numbers within one percentage point of that.)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了 69.5% 的测试准确率。（由于神经网络初始化的随机性，你可能得到与此相差不到一个百分点的数字。）
- en: 'Because we have relatively few training samples (2,000), overfitting will be
    our number one concern. You already know about a number of techniques that can
    help mitigate overfitting, such as dropout and weight decay (L2 regularization).
    We’re now going to work with a new one, specific to computer vision and used almost
    universally when processing images with deep learning models: *data augmentation*.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有相对较少的训练样本（2,000），过拟合将是我们关注的首要问题。你已经了解到一些可以帮助减轻过拟合的技术，如 dropout 和权重衰减（L2
    正则化）。现在我们将使用一个新的技术，特定于计算机视觉，并在使用深度学习模型处理图像时几乎普遍使用：*数据增强*。
- en: 8.2.5 Using data augmentation
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.5 使用数据增强
- en: 'Overfitting is caused by having too few samples to learn from, rendering you
    unable to train a model that can generalize to new data. Given infinite data,
    your model would be exposed to every possible aspect of the data distribution
    at hand: you would never overfit. Data augmentation takes the approach of generating
    more training data from existing training samples by *augmenting* the samples
    via a number of random transformations that yield believable-looking images. The
    goal is that, at training time, your model will never see the exact same picture
    twice. This helps expose the model to more aspects of the data so it can generalize
    better.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是由于样本量太少，导致无法训练出能够泛化到新数据的模型。如果有无限的数据，你的模型将暴露于手头数据分布的每一个可能方面：你永远不会过拟合。数据增强采取生成更多训练数据的方法，通过一些随机转换*增强*样本，生成看起来可信的图像。目标是，在训练时，你的模型永远不会看到完全相同的图片。这有助于让模型接触数据的更多方面，从而更好地泛化。
- en: 'In Keras, this can be done by adding a number of *data augmentation layers*
    at the start of your model. Let’s get started with an example: the following Sequential
    model chains several random image transformations. In our model, we’d include
    it right before the `Rescaling` layer.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，可以通过在模型开头添加一些*数据增强层*来实现。让我们通过一个示例开始：下面的 Sequential 模型链接了几个随机图像转换。在我们的模型中，我们会在`Rescaling`层之前包含它。
- en: Listing 8.14 Define a data augmentation stage to add to an image model
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.14 定义要添加到图像模型中的数据增强阶段
- en: '[PRE25]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'These are just a few of the layers available (for more, see the Keras documentation).
    Let’s quickly go over this code:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是一些可用的层（更多内容，请参阅Keras文档）。让我们快速浏览一下这段代码：
- en: '`RandomFlip("horizontal")`—Applies horizontal flipping to a random 50% of the
    images that go through it'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomFlip("horizontal")`—对通过它的随机50%图像应用水平翻转'
- en: '`RandomRotation(0.1)`—Rotates the input images by a random value in the range
    [–10%, +10%] (these are fractions of a full circle—in degrees, the range would
    be [–36 degrees, +36 degrees])'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomRotation(0.1)`—将输入图像旋转一个在范围[–10%，+10%]内的随机值（这些是完整圆的分数—以度为单位，范围将是[–36度，+36度]）'
- en: '`RandomZoom(0.2)`—Zooms in or out of the image by a random factor in the range
    [-20%, +20%]'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomZoom(0.2)`—通过在范围[-20%，+20%]内的随机因子放大或缩小图像'
- en: Let’s look at the augmented images (see figure 8.10).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下增强后的图像（参见图8.10）。
- en: Listing 8.15 Displaying some randomly augmented training images
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.15 显示一些随机增强的训练图像
- en: '[PRE26]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ We can use take(N) to only sample N batches from the dataset. This is equivalent
    to inserting a break in the loop after the N th batch.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们可以使用take(N)仅从数据集中取样N批次。这相当于在第N批次后的循环中插入一个中断。
- en: ❷ Apply the augmentation stage to the batch of images.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将增强阶段应用于图像批次。
- en: ❸ Display the first image in the output batch. For each of the nine iterations,
    this is a different augmentation of the same image.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 显示输出批次中的第一张图像。对于九次迭代中的每一次，这是同一图像的不同增强。
- en: '![](../Images/08-10.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-10.png)'
- en: Figure 8.10 Generating variations of a very good boy via random data augmentation
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 通过随机数据增强生成一个非常好的男孩的变化
- en: If we train a new model using this data-augmentation configuration, the model
    will never see the same input twice. But the inputs it sees are still heavily
    intercorrelated because they come from a small number of original images—we can’t
    produce new information; we can only remix existing information. As such, this
    may not be enough to completely get rid of overfitting. To further fight overfitting,
    we’ll also add a `Dropout` layer to our model right before the densely connected
    classifier.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用这个数据增强配置训练一个新模型，那么模型将永远不会看到相同的输入两次。但是它看到的输入仍然高度相关，因为它们来自少量原始图像—我们无法产生新信息；我们只能重新混合现有信息。因此，这可能不足以完全消除过拟合。为了进一步对抗过拟合，我们还将在密集连接分类器之前向我们的模型添加一个`Dropout`层。
- en: 'One last thing you should know about random image augmentation layers: just
    like `Dropout`, they’re inactive during inference (when we call `predict()` or
    `evaluate()`). During evaluation, our model will behave just the same as when
    it did not include data augmentation and dropout.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 关于随机图像增强层，还有一件事你应该知道：就像`Dropout`一样，在推断时（当我们调用`predict()`或`evaluate()`时），它们是不活动的。在评估期间，我们的模型的行为将与不包括数据增强和dropout时完全相同。
- en: Listing 8.16 Defining a new convnet that includes image augmentation and dropout
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.16 定义一个包含图像增强和dropout的新卷积神经网络
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Let’s train the model using data augmentation and dropout. Because we expect
    overfitting to occur much later during training, we will train for three times
    as many epochs—one hundred.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用数据增强和dropout来训练模型。因为我们预计过拟合会在训练期间发生得更晚，所以我们将训练三倍的时期—一百个时期。
- en: Listing 8.17 Training the regularized convnet
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.17 训练正则化的卷积神经网络
- en: '[PRE28]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s plot the results again: see figure 8.11\. Thanks to data augmentation
    and dropout, we start overfitting much later, around epochs 60–70 (compared to
    epoch 10 for the original model). The validation accuracy ends up consistently
    in the 80–85% range—a big improvement over our first try.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次绘制结果：参见图8.11。由于数据增强和dropout，我们开始过拟合的时间要晚得多，大约在60-70个时期（与原始模型的10个时期相比）。验证准确性最终稳定在80-85%的范围内—相比我们的第一次尝试，这是一个很大的改进。
- en: '![](../Images/08-11.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-11.png)'
- en: Figure 8.11 Training and validation metrics with data augmentation
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 使用数据增强的训练和验证指标
- en: Let’s check the test accuracy.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查测试准确性。
- en: Listing 8.18 Evaluating the model on the test set
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.18 在测试集上评估模型
- en: '[PRE29]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We get a test accuracy of 83.5%. It’s starting to look good! If you’re using
    Colab, make sure you download the saved file (convnet_from_scratch_with_augmentation.keras),
    as we will use it for some experiments in the next chapter.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了83.5%的测试准确性。看起来不错！如果你在使用Colab，请确保下载保存的文件（convnet_from_scratch_with_augmentation.keras），因为我们将在下一章中用它进行一些实验。
- en: By further tuning the model’s configuration (such as the number of filters per
    convolution layer, or the number of layers in the model), we might be able to
    get an even better accuracy, likely up to 90%. But it would prove difficult to
    go any higher just by training our own convnet from scratch, because we have so
    little data to work with. As a next step to improve our accuracy on this problem,
    we’ll have to use a pretrained model, which is the focus of the next two sections.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通过进一步调整模型的配置（例如每个卷积层的滤波器数量，或模型中的层数），我们可能能够获得更高的准确性，可能高达90%。但是，仅通过从头开始训练我们自己的卷积神经网络，要想获得更高的准确性将会变得困难，因为我们的数据量太少。为了提高这个问题上的准确性，我们将不得不使用一个预训练模型，这是接下来两节的重点。
- en: 8.3 Leveraging a pretrained model
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 利用预训练模型
- en: A common and highly effective approach to deep learning on small image datasets
    is to use a pretrained model. A *pretrained model* is a model that was previously
    trained on a large dataset, typically on a large-scale image-classification task.
    If this original dataset is large enough and general enough, the spatial hierarchy
    of features learned by the pretrained model can effectively act as a generic model
    of the visual world, and hence, its features can prove useful for many different
    computer vision problems, even though these new problems may involve completely
    different classes than those of the original task. For instance, you might train
    a model on ImageNet (where classes are mostly animals and everyday objects) and
    then repurpose this trained model for something as remote as identifying furniture
    items in images. Such portability of learned features across different problems
    is a key advantage of deep learning compared to many older, shallow learning approaches,
    and it makes deep learning very effective for small-data problems.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见且高效的小图像数据集深度学习方法是使用预训练模型。*预训练模型*是先前在大型数据集上训练过的模型，通常是在大规模图像分类任务上。如果原始数据集足够大且足够通用，那么预训练模型学习到的空间特征层次结构可以有效地充当视觉世界的通用模型，因此，其特征对许多不同的计算机视觉问题都可能有用，即使这些新问题可能涉及与原始任务完全不同的类别。例如，您可以在ImageNet上训练一个模型（其中类别主要是动物和日常物品），然后将这个训练好的模型重新用于识别图像中的家具物品等远程任务。与许多较旧的、浅层学习方法相比，学习到的特征在不同问题之间的可移植性是深度学习的一个关键优势，这使得深度学习在小数据问题上非常有效。
- en: In this case, let’s consider a large convnet trained on the ImageNet dataset
    (1.4 million labeled images and 1,000 different classes). ImageNet contains many
    animal classes, including different species of cats and dogs, and you can thus
    expect it to perform well on the dogs-versus-cats classification problem.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，让我们考虑一个在ImageNet数据集上训练的大型卷积网络（140万标记图像和1000个不同类别）。ImageNet包含许多动物类别，包括不同品种的猫和狗，因此您可以期望它在狗与猫的分类问题上表现良好。
- en: We’ll use the VGG16 architecture, developed by Karen Simonyan and Andrew Zisserman
    in 2014.[¹](../Text/08.htm#pgfId-1018503) Although it’s an older model, far from
    the current state of the art and somewhat heavier than many other recent models,
    I chose it because its architecture is similar to what you’re already familiar
    with, and it’s easy to understand without introducing any new concepts. This may
    be your first encounter with one of these cutesy model names—VGG, ResNet, Inception,
    Xception, and so on; you’ll get used to them because they will come up frequently
    if you keep doing deep learning for computer vision.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用VGG16架构，这是由Karen Simonyan和Andrew Zisserman于2014年开发的。虽然这是一个较老的模型，远非当前技术水平，并且比许多其他最新模型要重，但我选择它是因为其架构类似于您已经熟悉的内容，并且没有引入任何新概念。这可能是您第一次遇到这些可爱的模型名称之一——VGG、ResNet、Inception、Xception等；如果您继续进行计算机视觉的深度学习，您将经常遇到它们。
- en: 'There are two ways to use a pretrained model: *feature extraction* and *fine-tuning*.
    We’ll cover both of them. Let’s start with feature extraction.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型有两种方法：*特征提取*和*微调*。我们将涵盖这两种方法。让我们从特征提取开始。
- en: 8.3.1 Feature extraction with a pretrained model
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 使用预训练模型进行特征提取
- en: Feature extraction consists of using the representations learned by a previously
    trained model to extract interesting features from new samples. These features
    are then run through a new classifier, which is trained from scratch.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取包括使用先前训练模型学习到的表示来从新样本中提取有趣的特征。然后，这些特征通过一个新的分类器，该分类器是从头开始训练的。
- en: 'As you saw previously, convnets used for image classification comprise two
    parts: they start with a series of pooling and convolution layers, and they end
    with a densely connected classifier. The first part is called the *convolutional
    base* of the model. In the case of convnets, feature extraction consists of taking
    the convolutional base of a previously trained network, running the new data through
    it, and training a new classifier on top of the output (see figure 8.12).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您之前看到的，用于图像分类的卷积网络由两部分组成：它们从一系列池化和卷积层开始，然后以一个密集连接的分类器结束。第一部分被称为模型的*卷积基础*。在卷积网络的情况下，特征提取包括获取先前训练网络的卷积基础，将新数据通过它运行，并在输出之上训练一个新的分类器（参见图8.12）。
- en: '![](../Images/08-12.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-12.png)'
- en: Figure 8.12 Swapping classifiers while keeping the same convolutional base
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 在保持相同卷积基础的情况下交换分类器
- en: 'Why only reuse the convolutional base? Could we reuse the densely connected
    classifier as well? In general, doing so should be avoided. The reason is that
    the representations learned by the convolutional base are likely to be more generic
    and, therefore, more reusable: the feature maps of a convnet are presence maps
    of generic concepts over a picture, which are likely to be useful regardless of
    the computer vision problem at hand. But the representations learned by the classifier
    will necessarily be specific to the set of classes on which the model was trained—they
    will only contain information about the presence probability of this or that class
    in the entire picture. Additionally, representations found in densely connected
    layers no longer contain any information about where objects are located in the
    input image; these layers get rid of the notion of space, whereas the object location
    is still described by convolutional feature maps. For problems where object location
    matters, densely connected features are largely useless.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么只重用卷积基？我们能否也重用密集连接的分类器？一般来说，应该避免这样做。原因是卷积基学习到的表示可能更通用，因此更具重用性：卷积网络的特征图是图片上通用概念的存在图，这些概念可能无论面临什么计算机视觉问题都有用。但分类器学习到的表示必然是特定于模型训练的类集合的——它们只包含关于整个图片中这个或那个类别存在概率的信息。此外，密集连接层中的表示不再包含有关对象在输入图像中位置的信息；这些层摆脱了空间的概念，而对象位置仍然由卷积特征图描述。对于需要考虑对象位置的问题，密集连接特征基本上是无用的。
- en: Note that the level of generality (and therefore reusability) of the representations
    extracted by specific convolution layers depends on the depth of the layer in
    the model. Layers that come earlier in the model extract local, highly generic
    feature maps (such as visual edges, colors, and textures), whereas layers that
    are higher up extract more-abstract concepts (such as “cat ear” or “dog eye”).
    So if your new dataset differs a lot from the dataset on which the original model
    was trained, you may be better off using only the first few layers of the model
    to do feature extraction, rather than using the entire convolutional base.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，特定卷积层提取的表示的泛化程度（因此可重用性）取决于模型中该层的深度。模型中较早的层提取局部、高度通用的特征图（如视觉边缘、颜色和纹理），而较高层提取更抽象的概念（如“猫耳”或“狗眼”）。因此，如果您的新数据集与原始模型训练的数据集差异很大，您可能最好只使用模型的前几层进行特征提取，而不是使用整个卷积基。
- en: In this case, because the ImageNet class set contains multiple dog and cat classes,
    it’s likely to be beneficial to reuse the information contained in the densely
    connected layers of the original model. But we’ll choose not to, in order to cover
    the more general case where the class set of the new problem doesn’t overlap the
    class set of the original model. Let’s put this into practice by using the convolutional
    base of the VGG16 network, trained on ImageNet, to extract interesting features
    from cat and dog images, and then train a dogs-versus-cats classifier on top of
    these features.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，因为 ImageNet 类别集包含多个狗和猫类别，重用原始模型的密集连接层中包含的信息可能是有益的。但我们选择不这样做，以涵盖新问题的类别集与原始模型的类别集不重叠的更一般情况。让我们通过使用在
    ImageNet 上训练的 VGG16 网络的卷积基从猫和狗图片中提取有趣的特征，然后在这些特征之上训练一个狗与猫的分类器来实践这一点。
- en: 'The VGG16 model, among others, comes prepackaged with Keras. You can import
    it from the `keras.applications` module. Many other image-classification models
    (all pretrained on the ImageNet dataset) are available as part of `keras.applications`:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16 模型，以及其他模型，已经预先打包在 Keras 中。您可以从 `keras.applications` 模块导入它。许多其他图像分类模型（都在
    ImageNet 数据集上预训练）都作为 `keras.applications` 的一部分可用：
- en: Xception
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xception
- en: ResNet
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet
- en: MobileNet
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MobileNet
- en: EfficientNet
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EfficientNet
- en: DenseNet
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet
- en: etc.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: Let’s instantiate the VGG16 model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实例化 VGG16 模型。
- en: Listing 8.19 Instantiating the VGG16 convolutional base
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.19 实例化 VGG16 卷积基
- en: '[PRE30]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We pass three arguments to the constructor:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向构造函数传递三个参数：
- en: '`weights` specifies the weight checkpoint from which to initialize the model.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weights` 指定了初始化模型的权重检查点。'
- en: '`include_top` refers to including (or not) the densely connected classifier
    on top of the network. By default, this densely connected classifier corresponds
    to the 1,000 classes from ImageNet. Because we intend to use our own densely connected
    classifier (with only two classes: `cat` and `dog`), we don’t need to include
    it.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_top` 指的是是否包括（或不包括）网络顶部的密集连接分类器。默认情况下，这个密集连接分类器对应于 ImageNet 的 1,000
    个类。因为我们打算使用我们自己的密集连接分类器（只有两个类：`cat` 和 `dog`），所以我们不需要包含它。'
- en: '`input_shape` is the shape of the image tensors that we’ll feed to the network.
    This argument is purely optional: if we don’t pass it, the network will be able
    to process inputs of any size. Here we pass it so that we can visualize (in the
    following summary) how the size of the feature maps shrinks with each new convolution
    and pooling layer.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_shape` 是我们将馈送到网络的图像张量的形状。这个参数是完全可选的：如果我们不传递它，网络将能够处理任何大小的输入。在这里，我们传递它，以便我们可以可视化（在下面的摘要中）随着每个新的卷积和池化层特征图的大小如何缩小。'
- en: 'Here’s the detail of the architecture of the VGG16 convolutional base. It’s
    similar to the simple convnets you’re already familiar with:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 VGG16 卷积基架构的详细信息。它类似于您已经熟悉的简单卷积网络：
- en: '[PRE31]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The final feature map has shape `(5,` `5,` `512)`. That’s the feature map on
    top of which we’ll stick a densely connected classifier.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的特征图形状为`(5, 5, 512)`。这是我们将在其上放置一个密集连接分类器的特征图。
- en: 'At this point, there are two ways we could proceed:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以有两种方式继续：
- en: Run the convolutional base over our dataset, record its output to a NumPy array
    on disk, and then use this data as input to a standalone, densely connected classifier
    similar to those you saw in chapter 4 of this book. This solution is fast and
    cheap to run, because it only requires running the convolutional base once for
    every input image, and the convolutional base is by far the most expensive part
    of the pipeline. But for the same reason, this technique won’t allow us to use
    data augmentation.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行卷积基在我们的数据集上，将其输出记录到磁盘上的NumPy数组中，然后使用这些数据作为输入到一个独立的、与本书第4章中看到的类似的密集连接分类器。这种解决方案运行快速且成本低，因为它只需要为每个输入图像运行一次卷积基，而卷积基是整个流程中最昂贵的部分。但出于同样的原因，这种技术不允许我们使用数据增强。
- en: Extend the model we have (`conv_base`) by adding `Dense` layers on top, and
    run the whole thing from end to end on the input data. This will allow us to use
    data augmentation, because every input image goes through the convolutional base
    every time it’s seen by the model. But for the same reason, this technique is
    far more expensive than the first.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在`conv_base`顶部添加`Dense`层来扩展我们的模型，并在输入数据上端对端地运行整个模型。这将允许我们使用数据增强，因为每个输入图像在模型看到时都会经过卷积基。但出于同样的原因，这种技术比第一种要昂贵得多。
- en: 'We’ll cover both techniques. Let’s walk through the code required to set up
    the first one: recording the output of `conv_base` on our data and using these
    outputs as inputs to a new model.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖这两种技术。让我们逐步了解设置第一种技术所需的代码：记录`conv_base`在我们的数据上的输出，并使用这些输出作为新模型的输入。
- en: Fast feature extraction without data augmentation
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 无数据增强的快速特征提取
- en: We’ll start by extracting features as NumPy arrays by calling the `predict()`
    method of the `conv_base` model on our training, validation, and testing datasets.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在训练、验证和测试数据集上调用`conv_base`模型的`predict()`方法来提取特征作为NumPy数组。
- en: Let’s iterate over our datasets to extract the VGG16 features.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们迭代我们的数据集以提取VGG16特征。
- en: Listing 8.20 Extracting the VGG16 features and corresponding labels
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.20 提取VGG16特征和相应标签
- en: '[PRE32]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Importantly, `predict()` only expects images, not labels, but our current dataset
    yields batches that contain both images and their labels. Moreover, the `VGG16`
    model expects inputs that are preprocessed with the function `keras.applications.vgg16.preprocess_input`,
    which scales pixel values to an appropriate range.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，`predict()`只期望图像，而不是标签，但我们当前的数据集产生的批次包含图像和它们的标签。此外，`VGG16`模型期望使用`keras.applications.vgg16.preprocess_input`函数预处理输入，该函数将像素值缩放到适当的范围。
- en: 'The extracted features are currently of shape `(samples,` `5,` `5,` `512)`:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的特征目前的形状为`(samples,` `5,` `5,` `512)`：
- en: '[PRE33]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: At this point, we can define our densely connected classifier (note the use
    of dropout for regularization) and train it on the data and labels that we just
    recorded.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以定义我们的密集连接分类器（注意使用了dropout进行正则化），并在我们刚刚记录的数据和标签上对其进行训练。
- en: Listing 8.21 Defining and training the densely connected classifier
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.21 定义和训练密集连接分类器
- en: '[PRE34]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Note the use of the Flatten layer before passing the features to a Dense layer.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 注意在将特征传递给密集层之前使用了Flatten层。
- en: Training is very fast because we only have to deal with two `Dense` layers—an
    epoch takes less than one second even on CPU.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 训练非常快，因为我们只需要处理两个`Dense`层——即使在CPU上，一个时代也不到一秒。
- en: Let’s look at the loss and accuracy curves during training (see figure 8.13).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在训练过程中查看损失和准确率曲线（见图8.13）。
- en: '![](../Images/08-13.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-13.png)'
- en: Figure 8.13 Training and validation metrics for plain feature extraction
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 普通特征提取的训练和验证指标
- en: Listing 8.22 Plotting the results
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.22 绘制结果
- en: '[PRE35]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We reach a validation accuracy of about 97%—much better than we achieved in
    the previous section with the small model trained from scratch. This is a bit
    of an unfair comparison, however, because ImageNet contains many dog and cat instances,
    which means that our pretrained model already has the exact knowledge required
    for the task at hand. This won’t always be the case when you use pretrained features.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们达到了约97%的验证准确率——比我们在前一节使用从头开始训练的小模型取得的结果要好得多。然而，这有点不公平的比较，因为ImageNet包含许多狗和猫实例，这意味着我们预训练的模型已经具有了完成任务所需的确切知识。当您使用预训练特征时，情况并不总是如此。
- en: However, the plots also indicate that we’re overfitting almost from the start—despite
    using dropout with a fairly large rate. That’s because this technique doesn’t
    use data augmentation, which is essential for preventing overfitting with small
    image datasets.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，图表也表明我们几乎从一开始就过拟合了——尽管使用了相当大的dropout率。这是因为这种技术没有使用数据增强，而数据增强对于防止小图像数据集过拟合是至关重要的。
- en: Feature extraction together with data augmentation
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 结合数据增强的特征提取
- en: 'Now let’s review the second technique I mentioned for doing feature extraction,
    which is much slower and more expensive, but which allows us to use data augmentation
    during training: creating a model that chains the `conv_base` with a new dense
    classifier, and training it end to end on the inputs.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回顾一下我提到的第二种特征提取技术，这种技术速度较慢，成本较高，但允许我们在训练过程中使用数据增强：创建一个将`conv_base`与新的密集分类器连接起来的模型，并在输入上端对端地进行训练。
- en: In order to do this, we will first *freeze the convolutional base*. *Freezing*
    a layer or set of layers means preventing their weights from being updated during
    training. If we don’t do this, the representations that were previously learned
    by the convolutional base will be modified during training. Because the `Dense`
    layers on top are randomly initialized, very large weight updates would be propagated
    through the network, effectively destroying the representations previously learned.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们首先要*冻结卷积基*。*冻结*一层或一组层意味着在训练过程中阻止它们的权重被更新。如果我们不这样做，卷积基先前学到的表示将在训练过程中被修改。因为顶部的`Dense`层是随机初始化的，非常大的权重更新会通过网络传播，有效地破坏先前学到的表示。
- en: In Keras, we freeze a layer or model by setting its `trainable` attribute to
    `False`.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，通过将其`trainable`属性设置为`False`来冻结一个层或模型。
- en: Listing 8.23 Instantiating and freezing the VGG16 convolutional base
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.23 实例化和冻结VGG16卷积基
- en: '[PRE36]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Setting `trainable` to `False` empties the list of trainable weights of the
    layer or model.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 将`trainable`设置为`False`会清空层或模型的可训练权重列表。
- en: Listing 8.24 Printing the list of trainable weights before and after freezing
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.24 在冻结前后打印可训练权重列表
- en: '[PRE37]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now we can create a new model that chains together
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个新模型，将
- en: A data augmentation stage
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个数据增强阶段
- en: Our frozen convolutional base
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们冻结的卷积基础
- en: A dense classifier
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个密集分类器
- en: Listing 8.25 Adding a data augmentation stage and a classifier to the convolutional
    base
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.25 向卷积基添加数据增强阶段和分类器
- en: '[PRE38]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Apply data augmentation.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 应用数据增强。
- en: ❷ Apply input value scaling.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 应用输入值缩放。
- en: 'With this setup, only the weights from the two `Dense` layers that we added
    will be trained. That’s a total of four weight tensors: two per layer (the main
    weight matrix and the bias vector). Note that in order for these changes to take
    effect, you must first compile the model. If you ever modify weight trainability
    after compilation, you should then recompile the model, or these changes will
    be ignored.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种设置，只有我们添加的两个`Dense`层的权重将被训练。总共有四个权重张量：每层两个（主要权重矩阵和偏置向量）。请注意，为了使这些更改生效，您必须首先编译模型。如果在编译后修改权重的可训练性，那么您应该重新编译模型，否则这些更改将被忽略。
- en: Let’s train our model. Thanks to data augmentation, it will take much longer
    for the model to start overfitting, so we can train for more epochs—let’s do 50.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练我们的模型。由于数据增强，模型开始过拟合的时间会更长，所以我们可以训练更多的epochs——让我们做50个。
- en: Note This technique is expensive enough that you should only attempt it if you
    have access to a GPU (such as the free GPU available in Colab)—it’s intractable
    on CPU. If you can’t run your code on GPU, then the previous technique is the
    way to go.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 这种技术足够昂贵，只有在您可以访问GPU（例如Colab中提供的免费GPU）时才应尝试——在CPU上无法实现。如果无法在GPU上运行代码，则应采用前一种技术。
- en: '[PRE39]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Let’s plot the results again (see figure 8.14). As you can see, we reach a validation
    accuracy of over 98%. This is a strong improvement over the previous model.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次绘制结果（参见图8.14）。正如您所看到的，我们达到了超过98%的验证准确率。这是对先前模型的一个强大改进。
- en: '![](../Images/08-14.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-14.png)'
- en: Figure 8.14 Training and validation metrics for feature extraction with data
    augmentation
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 使用数据增强进行特征提取的训练和验证指标
- en: Let’s check the test accuracy.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查测试准确率。
- en: Listing 8.26 Evaluating the model on the test set
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.26 在测试集上评估模型
- en: '[PRE40]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We get a test accuracy of 97.5%. This is only a modest improvement compared
    to the previous test accuracy, which is a bit disappointing given the strong results
    on the validation data. A model’s accuracy always depends on the set of samples
    you evaluate it on! Some sample sets may be more difficult than others, and strong
    results on one set won’t necessarily fully translate to all other sets.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了97.5%的测试准确率。与先前的测试准确率相比，这只是一个适度的改进，考虑到验证数据的强大结果，有点令人失望。模型的准确性始终取决于您评估的样本集！某些样本集可能比其他样本集更难，对一个样本集的强大结果不一定会完全转化为所有其他样本集。
- en: 8.3.2 Fine-tuning a pretrained model
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 微调预训练模型
- en: Another widely used technique for model reuse, complementary to feature extraction,
    is *fine-tuning* (see figure 8.15). Fine-tuning consists of unfreezing a few of
    the top layers of a frozen model base used for feature extraction, and jointly
    training both the newly added part of the model (in this case, the fully connected
    classifier) and these top layers. This is called *fine-tuning* because it slightly
    adjusts the more abstract representations of the model being reused in order to
    make them more relevant for the problem at hand.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 用于模型重用的另一种广泛使用的技术，与特征提取相辅相成，即*微调*（参见图8.15）。微调包括解冻用于特征提取的冻结模型基础的顶部几层，并同时训练模型的这部分新添加部分（在本例中是全连接分类器）和这些顶部层。这被称为*微调*，因为它略微调整了被重用模型的更抽象的表示，以使它们对手头的问题更相关。
- en: '![](../Images/08-15.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-15.png)'
- en: Figure 8.15 Fine-tuning the last convolutional block of the VGG16 network
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 微调VGG16网络的最后一个卷积块
- en: 'I stated earlier that it’s necessary to freeze the convolution base of VGG16
    in order to be able to train a randomly initialized classifier on top. For the
    same reason, it’s only possible to fine-tune the top layers of the convolutional
    base once the classifier on top has already been trained. If the classifier isn’t
    already trained, the error signal propagating through the network during training
    will be too large, and the representations previously learned by the layers being
    fine-tuned will be destroyed. Thus the steps for fine-tuning a network are as
    follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前说过，为了能够在顶部训练一个随机初始化的分类器，需要冻结VGG16的卷积基。出于同样的原因，只有在顶部的分类器已经训练好后，才能微调卷积基的顶层。如果分类器尚未训练好，那么在训练过程中通过网络传播的误差信号将会太大，并且之前由微调层学到的表示将被破坏。因此，微调网络的步骤如下：
- en: Add our custom network on top of an already-trained base network.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在已经训练好的基础网络上添加我们的自定义网络。
- en: Freeze the base network.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结基础网络。
- en: Train the part we added.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练我们��加的部分。
- en: Unfreeze some layers in the base network. (Note that you should not unfreeze
    “batch normalization” layers, which are not relevant here since there are no such
    layers in VGG16\. Batch normalization and its impact on fine-tuning is explained
    in the next chapter.)
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解冻基础网络中的一些层。（请注意，不应解冻“批量归一化”层，在这里不相关，因为VGG16中没有这样的层。有关批量归一化及其对微调的影响，将在下一章中解释。）
- en: Jointly train both these layers and the part we added.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时训练这两个层和我们添加的部分。
- en: 'You already completed the first three steps when doing feature extraction.
    Let’s proceed with step 4: we’ll unfreeze our `conv_base` and then freeze individual
    layers inside it.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行特征提取时，您已经完成了前三个步骤。让我们继续进行第四步：我们将解冻我们的`conv_base`，然后冻结其中的各个层。
- en: 'As a reminder, this is what our convolutional base looks like:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，这是我们的卷积基的样子：
- en: '[PRE41]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We’ll fine-tune the last three convolutional layers, which means all layers
    up to `block4_ pool` should be frozen, and the layers `block5_conv1`, `block5_conv2`,
    and `block5_conv3` should be trainable.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将微调最后三个卷积层，这意味着所有层直到`block4_pool`应该被冻结，而层`block5_conv1`、`block5_conv2`和`block5_conv3`应该是可训练的。
- en: 'Why not fine-tune more layers? Why not fine-tune the entire convolutional base?
    You could. But you need to consider the following:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不微调更多层？为什么不微调整个卷积基？你可以。但你需要考虑以下几点：
- en: Earlier layers in the convolutional base encode more generic, reusable features,
    whereas layers higher up encode more specialized features. It’s more useful to
    fine-tune the more specialized features, because these are the ones that need
    to be repurposed on your new problem. There would be fast-decreasing returns in
    fine-tuning lower layers.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较早的卷积基层编码更通用、可重复使用的特征，而较高层编码更专业化的特征。对更专业化的特征进行微调更有用，因为这些特征需要在新问题上重新利用。微调较低层会有快速减少的回报。
- en: The more parameters you’re training, the more you’re at risk of overfitting.
    The convolutional base has 15 million parameters, so it would be risky to attempt
    to train it on your small dataset.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您训练的参数越多，过拟合的风险就越大。卷积基有 1500 万个参数，因此在您的小数据集上尝试训练它是有风险的。
- en: Thus, in this situation, it’s a good strategy to fine-tune only the top two
    or three layers in the convolutional base. Let’s set this up, starting from where
    we left off in the previous example.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，只微调卷积基的前两三层是一个好策略。让我们从前一个示例中结束的地方开始设置这个。
- en: Listing 8.27 Freezing all layers until the fourth from the last
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.27 冻结直到倒数第四层的所有层
- en: '[PRE42]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now we can begin fine-tuning the model. We’ll do this with the RMSprop optimizer,
    using a very low learning rate. The reason for using a low learning rate is that
    we want to limit the magnitude of the modifications we make to the representations
    of the three layers we’re fine-tuning. Updates that are too large may harm these
    representations.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始微调模型了。我们将使用 RMSprop 优化器，使用非常低的学习率。使用低学习率的原因是我们希望限制对我们正在微调的三层表示所做修改的幅度。更新过大可能会损害这些表示。
- en: Listing 8.28 Fine-tuning the model
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.28 微调模型
- en: '[PRE43]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can finally evaluate this model on the test data:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 最终我们可以在测试数据上评估这个模型：
- en: '[PRE44]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Here, we get a test accuracy of 98.5% (again, your own results may be within
    one percentage point). In the original Kaggle competition around this dataset,
    this would have been one of the top results. It’s not quite a fair comparison,
    however, since we used pretrained features that already contained prior knowledge
    about cats and dogs, which competitors couldn’t use at the time.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们获得了 98.5% 的测试准确率（再次强调，您自己的结果可能在一个百分点内）。在围绕这个数据集的原始 Kaggle 竞赛中，这将是顶尖结果之一。然而，这并不是一个公平的比较，因为我们使用了预训练特征，这些特征已经包含了关于猫和狗的先前知识，而竞争对手当时无法使用。
- en: On the positive side, by leveraging modern deep learning techniques, we managed
    to reach this result using only a small fraction of the training data that was
    available for the competition (about 10%). There is a huge difference between
    being able to train on 20,000 samples compared to 2,000 samples!
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 积极的一面是，通过利用现代深度学习技术，我们成功地仅使用了比比赛可用的训练数据的一小部分（约 10%）就达到了这个结果。在能够训练 20,000 个样本和
    2,000 个样本之间存在巨大差异！
- en: Now you have a solid set of tools for dealing with image-classification problems—in
    particular, with small datasets.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经掌握了一套处理图像分类问题的工具，特别是处理小数据集。
- en: Summary
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Convnets are the best type of machine learning models for computer vision tasks.
    It’s possible to train one from scratch even on a very small dataset, with decent
    results.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络是计算机视觉任务中最好的机器学习模型类型。即使在一个非常小的数据集上，也可以从头开始训练一个，并取得不错的结果。
- en: Convnets work by learning a hierarchy of modular patterns and concepts to represent
    the visual world.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络通过学习一系列模块化的模式和概念来表示视觉世界。
- en: On a small dataset, overfitting will be the main issue. Data augmentation is
    a powerful way to fight overfitting when you’re working with image data.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个小数据集上，过拟合将是主要问题。数据增强是处理图像数据时对抗过拟合的强大方式。
- en: It’s easy to reuse an existing convnet on a new dataset via feature extraction.
    This is a valuable technique for working with small image datasets.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征提取，可以很容易地在新数据集上重用现有的卷积神经网络。这是处理小图像数据集的有价值的技术。
- en: As a complement to feature extraction, you can use fine-tuning, which adapts
    to a new problem some of the representations previously learned by an existing
    model. This pushes performance a bit further.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为特征提取的补充，您可以使用微调，这会使现有模型先前学习的一些表示适应新问题。这会稍微提高性能。
- en: '* * *'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[¹](../Text/08.htm#Id-1018503) Karen Simonyan and Andrew Zisserman, “Very Deep
    Convolutional Networks for Large-Scale Image Recognition,” arXiv (2014), [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556).'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[¹](../Text/08.htm#Id-1018503) Karen Simonyan 和 Andrew Zisserman，“Very Deep
    Convolutional Networks for Large-Scale Image Recognition”，arXiv（2014），[https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)。'
