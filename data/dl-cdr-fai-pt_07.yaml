- en: 'Chapter 4\. Under the Hood: Training a Digit Classifier'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。底层：训练数字分类器
- en: Having seen what it looks like to train a variety of models in [Chapter 2](ch02.xhtml#chapter_production),
    let’s now look under the hood and see exactly what is going on. We’ll start by
    using computer vision to introduce fundamental tools and concepts for deep learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.xhtml#chapter_production)中看到训练各种模型的样子后，现在让我们深入了解并看看究竟发生了什么。我们将使用计算机视觉来介绍深度学习的基本工具和概念。
- en: To be exact, we’ll discuss the roles of arrays and tensors and of broadcasting,
    a powerful technique for using them expressively. We’ll explain stochastic gradient
    descent (SGD), the mechanism for learning by updating weights automatically. We’ll
    discuss the choice of a loss function for our basic classification task, and the
    role of mini-batches. We’ll also describe the math that a basic neural network
    is doing. Finally, we’ll put all these pieces together.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 确切地说，我们将讨论数组和张量的作用以及广播的作用，这是一种使用它们表达性地的强大技术。我们将解释随机梯度下降（SGD），这是通过自动更新权重学习的机制。我们将讨论基本分类任务的损失函数的选择，以及小批量的作用。我们还将描述基本神经网络正在执行的数学。最后，我们将把所有这些部分组合起来。
- en: In future chapters, we’ll do deep dives into other applications as well, and
    see how these concepts and tools generalize. But this chapter is about laying
    foundation stones. To be frank, that also makes this one of the hardest chapters,
    because of how these concepts all depend on each other. Like an arch, all the
    stones need to be in place for the structure to stay up. Also like an arch, once
    that happens, it’s a powerful structure that can support other things. But it
    requires some patience to assemble.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的章节中，我们还将深入研究其他应用，并看看这些概念和工具如何泛化。但本章是关于奠定基础的。坦率地说，这也使得这是最困难的章节之一，因为这些概念彼此相互依赖。就像一个拱门，所有的石头都需要放在正确的位置才能支撑结构。也像一个拱门，一旦发生这种情况，它就是一个强大的结构，可以支撑其他事物。但是需要一些耐心来组装。
- en: Let’s begin. The first step is to consider how images are represented in a computer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。第一步是考虑图像在计算机中是如何表示的。
- en: 'Pixels: The Foundations of Computer Vision'
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 像素：计算机视觉的基础
- en: To understand what happens in a computer vision model, we first have to understand
    how computers handle images. We’ll use one of the most famous datasets in computer
    vision, [MNIST](https://oreil.ly/g3RDg), for our experiments. MNIST contains images
    of handwritten digits, collected by the National Institute of Standards and Technology
    and collated into a machine learning dataset by Yann Lecun and his colleagues.
    Lecun used MNIST in 1998 in [LeNet-5](https://oreil.ly/LCNEx), the first computer
    system to demonstrate practically useful recognition of handwritten digit sequences.
    This was one of the most important breakthroughs in the history of AI.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解计算机视觉模型中发生的事情，我们首先必须了解计算机如何处理图像。我们将使用计算机视觉中最著名的数据集之一MNIST进行实验。MNIST包含由国家标准与技术研究所收集的手写数字图像，并由Yann
    Lecun及其同事整理成一个机器学习数据集。Lecun在1998年使用MNIST在LeNet-5中，这是第一个演示实用手写数字序列识别的计算机系统。这是人工智能历史上最重要的突破之一。
- en: 'For this initial tutorial, we are just going to try to create a model that
    can classify any image as a 3 or a 7\. So let’s download a sample of MNIST that
    contains images of just these digits:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个初始教程，我们只是尝试创建一个模型，可以将任何图像分类为3或7。所以让我们下载一个包含这些数字图像的MNIST样本：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can see what’s in this directory by using `ls`, a method added by fastai.
    This method returns an object of a special fastai class called `L`, which has
    all the same functionality of Python’s built-in `list`, plus a lot more. One of
    its handy features is that, when printed, it displays the count of items before
    listing the items themselves (if there are more than 10 items, it shows just the
    first few):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`ls`来查看此目录中的内容，这是fastai添加的一个方法。这个方法返回一个特殊的fastai类`L`的对象，它具有Python内置`list`的所有功能，还有更多功能。其中一个方便的功能是，在打印时，它会显示项目的计数，然后列出项目本身（如果项目超过10个，它只显示前几个）：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The MNIST dataset follows a common layout for machine learning datasets: separate
    folders for the training set and the validation (and/or test) set. Let’s see what’s
    inside the training set:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集遵循机器学习数据集的常见布局：训练集和验证（和/或测试）集分开存放。让我们看看训练集中的内容：
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There’s a folder of 3s, and a folder of 7s. In machine learning parlance, we
    say that “3” and “7” are the *labels* (or targets) in this dataset. Let’s take
    a look in one of these folders (using `sorted` to ensure we all get the same order
    of files):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个包含3的文件夹，和一个包含7的文件夹。在机器学习术语中，我们说“3”和“7”是这个数据集中的*标签*（或目标）。让我们看看其中一个文件夹中的内容（使用`sorted`确保我们都得到相同的文件顺序）：
- en: '[PRE5]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As we might expect, it’s full of image files. Let’s take a look at one now.
    Here’s an image of a handwritten number 3, taken from the famous MNIST dataset
    of handwritten numbers:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预期的那样，它充满了图像文件。让我们现在看一个。这是一个手写数字3的图像，来自著名的手写数字MNIST数据集：
- en: '[PRE7]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](Images/dlcf_04in01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in01.png)'
- en: Here we are using the `Image` class from the *Python Imaging Library* (PIL),
    which is the most widely used Python package for opening, manipulating, and viewing
    images. Jupyter knows about PIL images, so it displays the image for us automatically.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用*Python Imaging Library*（PIL）中的`Image`类，这是最广泛使用的Python包，用于打开、操作和查看图像。Jupyter知道PIL图像，所以它会自动为我们显示图像。
- en: 'In a computer, everything is represented as a number. To view the numbers that
    make up this image, we have to convert it to a *NumPy array* or a *PyTorch tensor*.
    For instance, here’s what a section of the image looks like converted to a NumPy
    array:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机中，一切都以数字表示。要查看构成这幅图像的数字，我们必须将其转换为*NumPy数组*或*PyTorch张量*。例如，这是转换为NumPy数组后图像的一部分的样子：
- en: '[PRE8]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `4:10` indicates we requested the rows from index 4 (inclusive) to 10 (noninclusive),
    and the same for the columns. NumPy indexes from top to bottom and from left to
    right, so this section is located near the top-left corner of the image. Here’s
    the same thing as a PyTorch tensor:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`4:10`表示我们请求从索引4（包括）到10（不包括）的行，列也是一样。NumPy从上到下，从左到右索引，因此此部分位于图像的左上角附近。这里是一个PyTorch张量：'
- en: '[PRE10]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can slice the array to pick just the part with the top of the digit in it,
    and then use a Pandas DataFrame to color-code the values using a gradient, which
    shows us clearly how the image is created from the pixel values:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以切片数组，只选择包含数字顶部部分的部分，然后使用Pandas DataFrame使用渐变对值进行着色，这清楚地显示了图像是如何由像素值创建的：
- en: '[PRE12]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](Images/dlcf_04in02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in02.png)'
- en: You can see that the background white pixels are stored as the number 0, black
    is the number 255, and shades of gray are between the two. The entire image contains
    28 pixels across and 28 pixels down, for a total of 768 pixels. (This is much
    smaller than an image that you would get from a phone camera, which has millions
    of pixels, but is a convenient size for our initial learning and experiments.
    We will build up to bigger, full-color images soon.)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，背景白色像素存储为数字0，黑色为数字255，灰色在两者之间。整个图像横向包含28个像素，纵向包含28个像素，总共768个像素。（这比你从手机相机得到的图像要小得多，手机相机有数百万像素，但对于我们的初始学习和实验来说，这是一个方便的大小。我们将很快构建更大的全彩图像。）
- en: 'So, now you’ve seen what an image looks like to a computer, let’s recall our
    goal: create a model that can recognize 3s and 7s. How might you go about getting
    a computer to do that?'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在你已经看到了计算机对图像的看法，让我们回顾一下我们的目标：创建一个能够识别3和7的模型。你会如何让计算机做到这一点呢？
- en: Stop and Think!
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停下来思考！
- en: Before you read on, take a moment to think about how a computer might be able
    to recognize these two digits. What kinds of features might it be able to look
    at? How might it be able to identify these features? How could it combine them?
    Learning works best when you try to solve problems yourself, rather than just
    reading somebody else’s answers; so step away from this book for a few minutes,
    grab a piece of paper and pen, and jot some ideas down.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续阅读之前，花点时间考虑一下计算机可能如何识别这两个数字。它可能能够看到什么样的特征？它可能如何识别这些特征？它如何将它们结合起来？学习最好的方式是尝试自己解决问题，而不仅仅是阅读别人的答案；所以离开这本书几分钟，拿一张纸和笔，写下一些想法。
- en: 'First Try: Pixel Similarity'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一次尝试：像素相似度
- en: 'So, here is a first idea: how about we find the average pixel value for every
    pixel of the 3s, then do the same for the 7s. This will give us two group averages,
    defining what we might call the “ideal” 3 and 7\. Then, to classify an image as
    one digit or the other, we see which of these two ideal digits the image is most
    similar to. This certainly seems like it should be better than nothing, so it
    will make a good baseline.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这是一个第一个想法：我们可以找到每个3的像素的平均值，然后对7做同样的操作。这将给我们两组平均值，定义了我们可能称之为“理想”3和7。然后，为了将图像分类为一个数字或另一个数字，我们看看这两个理想数字中图像与哪个更相似。这肯定似乎比没有好，所以这将成为一个很好的基线。
- en: 'Jargon: Baseline'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：基线
- en: 'A simple model that you are confident should perform reasonably well. It should
    be simple to implement and easy to test, so that you can then test each of your
    improved ideas and make sure they are always better than your baseline. Without
    starting with a sensible baseline, it is difficult to know whether your super-fancy
    models are any good. One good approach to creating a baseline is doing what we
    have done here: think of a simple, easy-to-implement model. Another good approach
    is to search around to find other people who have solved problems similar to yours,
    and download and run their code on your dataset. Ideally, try both of these!'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的模型，你有信心应该表现得相当不错。它应该简单实现和易于测试，这样你就可以测试每个改进的想法，并确保它们始终优于基线。如果没有以合理的基线开始，很难知道你的超级花哨的模型是否好用。创建基线的一个好方法是做我们在这里做的事情：考虑一个简单、易于实现的模型。另一个好方法是四处寻找解决类似问题的其他人，并在你的数据集上下载并运行他们的代码。最好两者都尝试一下！
- en: Step 1 for our simple model is to get the average of pixel values for each of
    our two groups. In the process of doing this, we will learn a lot of neat Python
    numeric programming tricks!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单模型的第一步是获取我们两组像素值的平均值。在这个过程中，我们将学习很多有趣的Python数值编程技巧！
- en: Let’s create a tensor containing all of our 3s stacked together. We already
    know how to create a tensor containing a single image. To create a tensor containing
    all the images in a directory, we will first use a Python list comprehension to
    create a plain list of the single image tensors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个包含所有3的张量堆叠在一起。我们已经知道如何创建包含单个图像的张量。要创建一个包含目录中所有图像的张量，我们将首先使用Python列表推导来创建一个单个图像张量的普通列表。
- en: 'We will use Jupyter to do some little checks of our work along the way—in this
    case, making sure that the number of returned items seems reasonable:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Jupyter在途中做一些小的检查——在这种情况下，确保返回的项目数量看起来合理：
- en: '[PRE13]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: List Comprehensions
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列表推导
- en: 'List and dictionary comprehensions are a wonderful feature of Python. Many
    Python programmers use them every day, including the authors of this book—they
    are part of “idiomatic Python.” But programmers coming from other languages may
    have never seen them before. A lot of great tutorials are just a web search away,
    so we won’t spend a long time discussing them now. Here is a quick explanation
    and example to get you started. A list comprehension looks like this: `new_list
    = [f(o) for o in a_list if o>0]`. This will return every element of `a_list` that
    is greater than 0, after passing it to the function `f`. There are three parts
    here: the collection you are iterating over (`a_list`), an optional filter (`if
    o>0`), and something to do to each element (`f(o)`). It’s not only shorter to
    write, but also way faster than the alternative ways of creating the same list
    with a loop.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表和字典推导是Python的一个很棒的特性。许多Python程序员每天都在使用它们，包括本书的作者们——它们是“Python的成语”。但是来自其他语言的程序员可能以前从未见过它们。许多很棒的教程只需一次网络搜索，所以我们现在不会花很长时间讨论它们。这里有一个快速的解释和示例，让您开始。列表推导看起来像这样：`new_list
    = [f(o) for o in a_list if o>0]`。这将返回`a_list`中大于0的每个元素，在将其传递给函数`f`之后。这里有三个部分：您正在迭代的集合（`a_list`），一个可选的过滤器（`if
    o>0`），以及对每个元素执行的操作（`f(o)`）。不仅写起来更短，而且比用循环创建相同列表的替代方法更快。
- en: 'We’ll also check that one of the images looks OK. Since we now have tensors
    (which Jupyter by default will print as values), rather than PIL images (which
    Jupyter by default will display images), we need to use fastai’s `show_image`
    function to display it:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将检查其中一张图像是否正常。由于我们现在有张量（Jupyter默认会将其打印为值），而不是PIL图像（Jupyter默认会显示图像），我们需要使用fastai的`show_image`函数来显示它：
- en: '[PRE15]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](Images/dlcf_04in03.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in03.png)'
- en: For every pixel position, we want to compute the average over all the images
    of the intensity of that pixel. To do this, we first combine all the images in
    this list into a single three-dimensional tensor. The most common way to describe
    such a tensor is to call it a *rank-3 tensor*. We often need to stack up individual
    tensors in a collection into a single tensor. Unsurprisingly, PyTorch comes with
    a function called `stack` that we can use for this purpose.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个像素位置，我们想要计算该像素的强度在所有图像上的平均值。为了做到这一点，我们首先将此列表中的所有图像组合成一个三维张量。描述这样的张量最常见的方式是称之为*rank-3张量*。我们经常需要将集合中的单个张量堆叠成一个张量。不出所料，PyTorch带有一个名为`stack`的函数，我们可以用它来实现这个目的。
- en: Some operations in PyTorch, such as taking a mean, require us to *cast* our
    integer types to float types. Since we’ll be needing this later, we’ll also cast
    our stacked tensor to `float` now. Casting in PyTorch is as simple as writing
    the name of the type you wish to cast to, and treating it as a method.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch中的一些操作，如取平均值，需要我们将整数类型转换为浮点类型。由于我们稍后会需要这个，我们现在也将我们的堆叠张量转换为`float`。在PyTorch中进行转换就像写下您希望转换为的类型名称，并将其视为方法一样简单。
- en: 'Generally, when images are floats, the pixel values are expected to be between
    0 and 1, so we will also divide by 255 here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当图像是浮点数时，像素值应该在0到1之间，所以我们也会在这里除以255：
- en: '[PRE16]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Perhaps the most important attribute of a tensor is its *shape*. This tells
    you the length of each axis. In this case, we can see that we have 6,131 images,
    each of size 28×28 pixels. There is nothing specifically about this tensor that
    says that the first axis is the number of images, the second is the height, and
    the third is the width—the semantics of a tensor are entirely up to us, and how
    we construct it. As far as PyTorch is concerned, it is just a bunch of numbers
    in memory.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 张量最重要的属性也许是其*形状*。这告诉您每个轴的长度。在这种情况下，我们可以看到我们有6,131张图像，每张图像大小为28×28像素。关于这个张量没有特别的地方表明第一个轴是图像的数量，第二个是高度，第三个是宽度——张量的语义完全取决于我们以及我们如何构建它。就PyTorch而言，它只是内存中的一堆数字。
- en: 'The *length* of a tensor’s shape is its rank:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 张量形状的*长度*是其秩：
- en: '[PRE18]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It is really important for you to commit to memory and practice these bits
    of tensor jargon: *rank* is the number of axes or dimensions in a tensor; *shape*
    is the size of each axis of a tensor.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您来说，将张量术语的这些部分记忆并加以实践非常重要：*秩*是张量中轴或维度的数量；*形状*是张量每个轴的大小。
- en: Alexis Says
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Alexis说
- en: Watch out because the term “dimension” is sometimes used in two ways. Consider
    that we live in “three-dimensional space,” where a physical position can be described
    by a vector `v`, of length 3\. But according to PyTorch, the attribute `v.ndim`
    (which sure looks like the “number of dimensions” of `v`) equals one, not three!
    Why? Because `v` is a vector, which is a tensor of rank one, meaning that it has
    only one *axis* (even if that axis has a length of three). In other words, sometimes
    dimension is used for the size of an axis (“space is three-dimensional”), while
    other times it is used for the rank, or the number of axes (“a matrix has two
    dimensions”). When confused, I find it helpful to translate all statements into
    terms of rank, axis, and length, which are unambiguous terms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要小心，因为术语“维度”有时以两种方式使用。考虑我们生活在“三维空间”中，其中物理位置可以用长度为3的向量`v`描述。但根据PyTorch，属性`v.ndim`（看起来确实像`v`的“维度数量”）等于一，而不是三！为什么？因为`v`是一个向量，它是一个秩为一的张量，这意味着它只有一个*轴*（即使该轴的长度为三）。换句话说，有时维度用于描述轴的大小（“空间是三维的”），而其他时候用于描述秩或轴的数量（“矩阵有两个维度”）。当感到困惑时，我发现将所有陈述转换为秩、轴和长度这些明确的术语是有帮助的。
- en: 'We can also get a tensor’s rank directly with `ndim`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以直接使用`ndim`来获取张量的秩：
- en: '[PRE20]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Finally, we can compute what the ideal 3 looks like. We calculate the mean of
    all the image tensors by taking the mean along dimension 0 of our stacked, rank-3
    tensor. This is the dimension that indexes over all the images.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以计算理想的3是什么样子的。我们通过沿着我们堆叠的rank-3张量的维度0取平均值来计算所有图像张量的平均值。这是索引所有图像的维度。
- en: 'In other words, for every pixel position, this will compute the average of
    that pixel over all images. The result will be one value for every pixel position,
    or a single image. Here it is:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，对于每个像素位置，这将计算所有图像中该像素的平均值。结果将是每个像素位置的一个值，或者一个单独的图像。这就是它：
- en: '[PRE22]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](Images/dlcf_04in04.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in04.png)'
- en: According to this dataset, this is the ideal number 3! (You may not like it,
    but this is what peak number 3 performance looks like.) You can see how it’s very
    dark where all the images agree it should be dark, but it becomes wispy and blurry
    where the images disagree.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个数据集，这是理想的数字3！（您可能不喜欢，但这就是顶级数字3表现的样子。）您可以看到在所有图像都认为应该是暗的地方非常暗，但在图像不一致的地方变得模糊。
- en: 'Let’s do the same thing for the 7s, but put all the steps together at once
    to save time:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对7做同样的事情，但一次将所有步骤放在一起以节省时间：
- en: '[PRE23]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](Images/dlcf_04in05.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in05.png)'
- en: Let’s now pick an arbitrary 3 and measure its *distance* from our “ideal digits.”
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们选择一个任意的3，并测量它与我们的“理想数字”的*距离*。
- en: Stop and Think!
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停下来思考一下！
- en: How would you calculate how similar a particular image is to each of our ideal
    digits? Remember to step away from this book and jot down some ideas before you
    move on! Research shows that recall and understanding improve dramatically when
    you are engaged with the learning process by solving problems, experimenting,
    and trying new ideas yourself.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您如何计算特定图像与我们的每个理想数字之间的相似程度？在继续前进之前，请记得远离这本书，记录一些想法！研究表明，通过解决问题、实验和尝试新想法，您参与学习过程时，召回和理解会显著提高。
- en: 'Here’s a sample 3:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例3：
- en: '[PRE24]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](Images/dlcf_04in06.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in06.png)'
- en: How can we determine its distance from our ideal 3? We can’t just add up the
    differences between the pixels of this image and the ideal digit. Some differences
    will be positive, while others will be negative, and these differences will cancel
    out, resulting in a situation where an image that is too dark in some places and
    too light in others might be shown as having zero total differences from the ideal.
    That would be misleading!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何确定它与我们理想的3之间的距离？我们不能简单地将此图像的像素之间的差异相加，并与理想数字进行比较。一些差异将是正的，而另一些将是负的，这些差异将相互抵消，导致一种情况，即在某些地方太暗而在其他地方太亮的图像可能被显示为与理想的总差异为零。那将是误导性的！
- en: 'To avoid this, data scientists use two main ways to measure distance in this
    context:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，数据科学家在这种情况下使用两种主要方法来测量距离：
- en: Take the mean of the *absolute value* of differences (absolute value is the
    function that replaces negative values with positive values). This is called the
    *mean absolute difference* or *L1 norm*.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取差值的*绝对值*的平均值（绝对值是将负值替换为正值的函数）。这被称为*平均绝对差*或*L1范数*。
- en: Take the mean of the *square* of differences (which makes everything positive)
    and then take the *square root* (which undoes the squaring). This is called the
    *root mean squared error* (RMSE) or *L2 norm*.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取差值的*平方*的平均值（使所有值变为正数），然后取*平方根*（撤销平方）。这被称为*均方根误差*（RMSE）或*L2范数*。
- en: It’s OK to Have Forgotten Your Math
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 忘记数学是可以的
- en: In this book, we generally assume that you have completed high school math,
    and remember at least some of it—but everybody forgets some things! It all depends
    on what you happen to have had reason to practice in the meantime. Perhaps you
    have forgotten what a *square root* is, or exactly how they work. No problem!
    Anytime you come across a math concept that is not explained fully in this book,
    don’t just keep moving on; instead, stop and look it up. Make sure you understand
    the basic idea, how it works, and why we might be using it. One of the best places
    to refresh your understanding is Khan Academy. For instance, Khan Academy has
    a great [introduction to square roots](https://oreil.ly/T7mxH).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们通常假设您已经完成了高中数学，并且至少记得一些内容 - 但每个人都会忘记一些东西！这完全取决于您在此期间有理由练习的内容。也许您已经忘记了*平方根*是什么，或者它们究竟是如何工作的。没问题！每当您遇到本书中没有完全解释的数学概念时，不要只是继续前进；相反，停下来查一下。确保您理解基本概念，它是如何工作的，以及为什么我们可能会使用它。刷新您理解的最佳地方之一是
    Khan Academy。例如，Khan Academy有一个很棒的[平方根介绍](https://oreil.ly/T7mxH)。
- en: 'Let’s try both of these now:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试这两种方法：
- en: '[PRE25]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In both cases, the distance between our 3 and the “ideal” 3 is less than the
    distance to the ideal 7, so our simple model will give the right prediction in
    this case.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们的3与“理想”的3之间的距离小于与理想的7之间的距离，因此在这种情况下，我们简单的模型将给出正确的预测。
- en: 'PyTorch already provides both of these as *loss functions*. You’ll find these
    inside `torch.nn.functional`, which the PyTorch team recommends importing as `F`
    (and is available by default under that name in fastai):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch已经提供了这两种作为*损失函数*。您会在`torch.nn.functional`中找到这些，PyTorch团队建议将其导入为`F`（并且默认情况下以这个名称在fastai中可用）：
- en: '[PRE29]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, `MSE` stands for *mean squared error*, and `l1` refers to the standard
    mathematical jargon for *mean absolute value* (in math it’s called the *L1 norm*).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`MSE`代表*均方误差*，`l1`是标准数学术语*平均绝对值*的缩写（在数学中称为*L1范数*）。
- en: Sylvain Says
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sylvain说
- en: Intuitively, the difference between L1 norm and mean squared error (MSE) is
    that the latter will penalize bigger mistakes more heavily than the former (and
    be more lenient with small mistakes).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，L1范数和均方误差（MSE）之间的区别在于，后者会比前者更严厉地惩罚更大的错误（并对小错误更宽容）。
- en: Jeremy Says
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 杰里米说
- en: 'When I first came across this L1 thingie, I looked it up to see what on earth
    it meant. I found on Google that it is a *vector norm* using *absolute value*,
    so I looked up “vector norm” and started reading: *Given a vector space V over
    a field F of the real or complex numbers, a norm on V is a nonnegative-valued
    any function p: V → \[0,+∞) with the following properties: For all a ∈ F and all
    u, v ∈ V, p(u + v) ≤ p(u) + p(v)…*Then I stopped reading. “Ugh, I’ll never understand
    math!” I thought, for the thousandth time. Since then, I’ve learned that every
    time these complex mathy bits of jargon come up in practice, it turns out I can
    replace them with a tiny bit of code! Like, the *L1 loss* is just equal to `(a-b).abs().mean()`,
    where `a` and `b` are tensors. I guess mathy folks just think differently than
    I do…I’ll make sure in this book that every time some mathy jargon comes up, I’ll
    give you the little bit of code it’s equal to as well, and explain in common-sense
    terms what’s going on.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '当我第一次遇到这个 L1 的东西时，我查了一下看它到底是什么意思。我在谷歌上发现它是使用“绝对值”作为“向量范数”，所以我查了“向量范数”并开始阅读：“给定一个实数或复数域
    F 上的向量空间 V，V 上的范数是一个非负值的任意函数 p: V → \[0,+∞)，具有以下属性：对于所有的 a ∈ F 和所有的 u, v ∈ V，p(u
    + v) ≤ p(u) + p(v)…”然后我停止阅读。“唉，我永远也理解不了数学！”我想，这已经是第一千次了。从那时起，我学到了每当实践中出现这些复杂的数学术语时，我可以用一点点代码来替换它们！比如，*L1
    损失* 只等于 `(a-b).abs().mean()`，其中 `a` 和 `b` 是张量。我猜数学家们只是和我想法不同…我会确保在本书中，每当出现一些数学术语时，我会给你相应的代码片段，并用通俗的语言解释发生了什么。'
- en: We just completed various mathematical operations on PyTorch tensors. If you’ve
    done numeric programming in PyTorch before, you may recognize these as being similar
    to NumPy arrays. Let’s have a look at those two important data structures.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚在 PyTorch 张量上完成了各种数学运算。如果你之前在 PyTorch 中进行过数值编程，你可能会发现这些与 NumPy 数组相似。让我们来看看这两个重要的数据结构。
- en: NumPy Arrays and PyTorch Tensors
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NumPy 数组和 PyTorch 张量
- en: '[NumPy](https://numpy.org) is the most widely used library for scientific and
    numeric programming in Python. It provides similar functionality and a similar
    API to that provided by PyTorch; however, it does not support using the GPU or
    calculating gradients, which are both critical for deep learning. Therefore, in
    this book, we will generally use PyTorch tensors instead of NumPy arrays, where
    possible.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[NumPy](https://numpy.org) 是 Python 中用于科学和数值编程最广泛使用的库。它提供了类似的功能和类似的 API，与 PyTorch
    提供的功能相似；然而，它不支持使用 GPU 或计算梯度，这两者对于深度学习都是至关重要的。因此，在本书中，我们通常会在可能的情况下使用 PyTorch 张量而不是
    NumPy 数组。'
- en: '(Note that fastai adds some features to NumPy and PyTorch to make them a bit
    more similar to each other. If any code in this book doesn’t work on your computer,
    it’s possible that you forgot to include a line like this at the start of your
    notebook: `from` `fastai.vision.all import *`.)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: （请注意，fastai 在 NumPy 和 PyTorch 中添加了一些功能，使它们更加相似。如果本书中的任何代码在您的计算机上无法运行，可能是因为您忘记在笔记本的开头包含类似这样的一行代码：`from
    fastai.vision.all import *`。）
- en: But what are arrays and tensors, and why should you care?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但是数组和张量是什么，为什么你应该关心呢？
- en: Python is slow compared to many languages. Anything fast in Python, NumPy, or
    PyTorch is likely to be a wrapper for a compiled object written (and optimized)
    in another language—specifically, C. In fact, *NumPy arrays and PyTorch tensors
    can finish computations many thousands of times faster than using pure Python*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Python 相对于许多语言来说速度较慢。在 Python、NumPy 或 PyTorch 中快速的任何东西，很可能是另一种语言（特别是 C）编写（并优化）的编译对象的包装器。事实上，*NumPy
    数组和 PyTorch 张量可以比纯 Python 快几千倍完成计算*。
- en: A NumPy array is a multidimensional table of data, with all items of the same
    type. Since that can be any type at all, they can even be arrays of arrays, with
    the innermost arrays potentially being different sizes—this is called a *jagged
    array*. By “multidimensional table,” we mean, for instance, a list (dimension
    of one), a table or matrix (dimension of two), a table of tables or cube (dimension
    of three), and so forth. If the items are all of simple type such as integer or
    float, NumPy will store them as a compact C data structure in memory. This is
    where NumPy shines. NumPy has a wide variety of operators and methods that can
    run computations on these compact structures at the same speed as optimized C,
    because they are written in optimized C.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 数组是一个多维数据表，所有项都是相同类型的。由于可以是任何类型，它们甚至可以是数组的数组，内部数组可能是不同大小的 - 这被称为 *不规则数组*。通过“多维数据表”，我们指的是，例如，一个列表（一维）、一个表或矩阵（二维）、一个表的表或立方体（三维），等等。如果所有项都是简单类型，如整数或浮点数，NumPy
    将它们存储为紧凑的 C 数据结构在内存中。这就是 NumPy 的优势所在。NumPy 有各种运算符和方法，可以在这些紧凑结构上以优化的 C 速度运行计算，因为它们是用优化的
    C 编写的。
- en: A PyTorch tensor is nearly the same thing as a NumPy array, but with an additional
    restriction that unlocks additional capabilities. It’s the same in that it, too,
    is a multidimensional table of data, with all items of the same type. However,
    the restriction is that a tensor cannot use just any old type—it has to use a
    single basic numeric type for all components. As a result, a tensor is not as
    flexible as a genuine array of arrays. For example, a PyTorch tensor cannot be
    jagged. It is always a regularly shaped multidimensional rectangular structure.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 张量几乎与 NumPy 数组相同，但有一个额外的限制，可以解锁额外的功能。它与 NumPy 数组相同，也是一个多维数据表，所有项都是相同类型的。然而，限制是张量不能使用任何旧类型
    - 它必须对所有组件使用单一基本数值类型。因此，张量不像真正的数组数组那样灵活。例如，PyTorch 张量不能是不规则的。它始终是一个形状规则的多维矩形结构。
- en: The vast majority of methods and operators supported by NumPy on these structures
    are also supported by PyTorch, but PyTorch tensors have additional capabilities.
    One major capability is that these structures can live on the GPU, in which case
    their computation will be optimized for the GPU and can run much faster (given
    lots of values to work on). In addition, PyTorch can automatically calculate derivatives
    of these operations, including combinations of operations. As you’ll see, it would
    be impossible to do deep learning in practice without this capability.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy在这些结构上支持的绝大多数方法和运算符在PyTorch上也支持，但PyTorch张量具有额外的功能。一个主要功能是这些结构可以存在于GPU上，这样它们的计算将被优化为GPU，并且可以运行得更快（给定大量值进行处理）。此外，PyTorch可以自动计算这些操作的导数，包括操作的组合。正如你将看到的，没有这种能力，实际上是不可能进行深度学习的。
- en: Sylvain Says
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sylvain说
- en: 'If you don’t know what C is, don’t worry: you won’t need it at all. In a nutshell,
    it’s a low-level (low-level means more similar to the language that computers
    use internally) language that is very fast compared to Python. To take advantage
    of its speed while programming in Python, try to avoid as much as possible writing
    loops, and replace them by commands that work directly on arrays or tensors.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道C是什么，不用担心：你根本不需要它。简而言之，它是一种低级语言（低级意味着更类似于计算机内部使用的语言），与Python相比非常快。为了在Python中利用其速度，尽量避免编写循环，用直接作用于数组或张量的命令替换它们。
- en: Perhaps the most important new coding skill for a Python programmer to learn
    is how to effectively use the array/tensor APIs. We will be showing lots more
    tricks later in this book, but here’s a summary of the key things you need to
    know for now.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 也许对于Python程序员来说，学习如何有效地使用数组/张量API是最重要的新编码技能。我们将在本书的后面展示更多技巧，但现在这里是你需要知道的关键事项的摘要。
- en: 'To create an array or tensor, pass a list (or list of lists, or list of lists
    of lists, etc.) to `array` or `tensor`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个数组或张量，将列表（或列表的列表，或列表的列表的列表等）传递给`array`或`tensor`：
- en: '[PRE31]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: All the operations that follow are shown on tensors, but the syntax and results
    for NumPy arrays are identical.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下所有操作都是在张量上展示的，但NumPy数组的语法和结果是相同的。
- en: 'You can select a row (note that, like lists in Python, tensors are 0-indexed,
    so 1 refers to the second row/column):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择一行（请注意，与Python中的列表一样，张量是从0开始索引的，所以1指的是第二行/列）：
- en: '[PRE36]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Or a column, by using `:` to indicate *all of the first axis* (we sometimes
    refer to the dimensions of tensors/arrays as *axes*):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 或者通过使用`:`来指示*所有第一个轴*（我们有时将张量/数组的维度称为*轴*）选择一列。
- en: '[PRE38]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You can combine these with Python slice syntax (`[*start*:*end*]`, with *`end`*
    being excluded) to select part of a row or column:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以结合Python切片语法（`[*start*:*end*]`，其中*`end`*被排除）来选择一行或一列的一部分：
- en: '[PRE40]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'And you can use the standard operators, such as `+`, `-`, `*`, and `/`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用标准运算符，如`+`、`-`、`*`和`/`：
- en: '[PRE42]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Tensors have a type:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 张量有一个类型：
- en: '[PRE44]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'And will automatically change that type as needed; for example, from `int`
    to `float`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 并且会根据需要自动更改该类型；例如，从`int`到`float`：
- en: '[PRE46]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: So, is our baseline model any good? To quantify this, we must define a metric.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们的基准模型好吗？为了量化这一点，我们必须定义一个度量。
- en: Computing Metrics Using Broadcasting
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用广播计算度量
- en: Recall that a *metric* is a number that is calculated based on the predictions
    of our model and the correct labels in our dataset, in order to tell us how good
    our model is. For instance, we could use either of the functions we saw in the
    previous section, mean squared error or mean absolute error, and take the average
    of them over the whole dataset. However, neither of these are numbers that are
    very understandable to most people; in practice, we normally use *accuracy* as
    the metric for classification models.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下*度量*是基于我们模型的预测和数据集中正确标签计算出来的一个数字，以告诉我们我们的模型有多好。例如，我们可以使用我们在上一节中看到的两个函数之一，均方误差或平均绝对误差，并计算整个数据集上它们的平均值。然而，这两个数字对大多数人来说并不是很容易理解；实际上，我们通常使用*准确度*作为分类模型的度量。
- en: As we’ve discussed, we want to calculate our metric over a *validation set*.
    This is so that we don’t inadvertently overfit—that is, train a model to work
    well only on our training data. This is not really a risk with the pixel similarity
    model we’re using here as a first try, since it has no trained components, but
    we’ll use a validation set anyway to follow normal practices and to be ready for
    our second try later.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论过的，我们想要在*验证集*上计算我们的度量。这样我们就不会无意中过拟合——也就是说，训练一个模型只在我们的训练数据上表现良好。这对于我们在这里作为第一次尝试使用的像素相似度模型来说并不是真正的风险，因为它没有经过训练的组件，但我们仍然会使用一个验证集来遵循正常的实践，并为我们稍后的第二次尝试做好准备。
- en: To get a validation set, we need to remove some of the data from training entirely,
    so it is not seen by the model at all. As it turns out, the creators of the MNIST
    dataset have already done this for us. Do you remember how there was a whole separate
    directory called *valid*? That’s what this directory is for!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得一个验证集，我们需要完全从训练数据中删除一些数据，这样模型根本就看不到它。事实证明，MNIST数据集的创建者已经为我们做了这个。你还记得*valid*这个整个独立的目录吗？这个目录就是为此而设立的！
- en: 'So to start, let’s create tensors for our 3s and 7s from that directory. These
    are the tensors we will use to calculate a metric measuring the quality of our
    first-try model, which measures distance from an ideal image:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们从那个目录中为我们的3和7创建张量。这些是我们将用来计算度量的张量，用来衡量我们第一次尝试模型的质量，这个度量衡量了与理想图像的距离：
- en: '[PRE48]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: It’s good to get in the habit of checking shapes as you go. Here we see two
    tensors, one representing the 3s validation set of 1,010 images of size 28×28,
    and one representing the 7s validation set of 1,028 images of size 28×28.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行操作时检查形状是一个好习惯。在这里我们看到两个张量，一个代表了1,010张大小为28×28的3的验证集，另一个代表了1,028张大小为28×28的7的验证集。
- en: We ultimately want to write a function, `is_3`, that will decide whether an
    arbitrary image is a 3 or a 7\. It will do this by deciding which of our two “ideal
    digits” that arbitrary image is closer to. For that we need to define a notion
    of *distance*—that is, a function that calculates the distance between two images.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终想要编写一个函数`is_3`，它将决定任意图像是3还是7。它将通过确定任意图像更接近我们的两个“理想数字”中的哪一个来实现这一点。为此，我们需要定义*距离*的概念——即，计算两个图像之间距离的函数。
- en: 'We can write a simple function that calculates the mean absolute error using
    an expression very similar to the one we wrote in the last section:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编写一个简单的函数，使用与我们在上一节中编写的表达式非常相似的表达式来计算平均绝对误差：
- en: '[PRE50]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This is the same value we previously calculated for the distance between these
    two images, the ideal 3 `mean_3` and the arbitrary sample 3 `a_3`, which are both
    single-image tensors with a shape of `[28,28]`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们先前为这两个图像之间的距离计算的相同值，理想数字3 `mean_3`和任意样本3 `a_3`，它们都是形状为`[28,28]`的单个图像张量。
- en: But to calculate a metric for overall accuracy, we will need to calculate the
    distance to the ideal 3 for *every* image in the validation set. How do we do
    that calculation? We could write a loop over all of the single-image tensors that
    are stacked within our validation set tensor, `valid_3_tens`, which has a shape
    of `[1010,28,28]` representing 1,010 images. But there is a better way.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 但是要计算整体准确度的指标，我们需要计算验证集中*每张*图像到理想数字3的距离。我们如何进行这种计算？我们可以编写一个循环，遍历验证集张量`valid_3_tens`中堆叠的所有单图像张量，其形状为`[1010,28,28]`，表示1,010张图像。但是有一种更好的方法。
- en: 'Something interesting happens when we take this exact same distance function,
    designed for comparing two single images, but pass in as an argument `valid_3_tens`,
    the tensor that represents the 3s validation set:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用相同的距离函数，设计用于比较两个单个图像，但将表示3的验证集张量`valid_3_tens`作为参数传入时，会发生一些有趣的事情：
- en: '[PRE52]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Instead of complaining about shapes not matching, it returned the distance for
    every single image as a vector (i.e., a rank-1 tensor) of length 1,010 (the number
    of 3s in our validation set). How did that happen?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 它没有抱怨形状不匹配，而是为每个单个图像返回了一个距离（即，长度为1,010的秩-1张量）。这是如何发生的？
- en: 'Take another look at our function `mnist_distance`, and you’ll see we have
    there the subtraction `(a-b)`. The magic trick is that PyTorch, when it tries
    to perform a simple subtraction operation between two tensors of different ranks,
    will use *broadcasting*: it will automatically expand the tensor with the smaller
    rank to have the same size as the one with the larger rank. Broadcasting is an
    important capability that makes tensor code much easier to write.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 再看看我们的函数`mnist_distance`，您会看到我们在那里有减法`(a-b)`。魔术技巧在于PyTorch在尝试在不同秩的两个张量之间执行简单的减法操作时，将使用*广播*：它将自动扩展秩较小的张量，使其大小与秩较大的张量相同。广播是一种重要的功能，使张量代码更容易编写。
- en: 'After broadcasting so the two argument tensors have the same rank, PyTorch
    applies its usual logic for two tensors of the same rank: it performs the operation
    on each corresponding element of the two tensors, and returns the tensor result.
    For instance:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在广播后，使两个参数张量具有相同的秩后，PyTorch对于秩相同的两个张量应用其通常的逻辑：它对两个张量的每个对应元素执行操作，并返回张量结果。例如：
- en: '[PRE54]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'So in this case, PyTorch treats `mean3`, a rank-2 tensor representing a single
    image, as if it were 1,010 copies of the same image, and then subtracts each of
    those copies from each 3 in our validation set. What shape would you expect this
    tensor to have? Try to figure it out yourself before you look at the answer here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，PyTorch将`mean3`视为一个表示单个图像的秩-2张量，就好像它是1,010个相同图像的副本，然后从我们的验证集中的每个3中减去每个副本。您期望这个张量的形状是什么？在查看这里的答案之前，请尝试自己想出来：
- en: '[PRE56]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We are calculating the difference between our ideal 3 and each of the 1,010
    3s in the validation set, for each of 28×28 images, resulting in the shape `[1010,28,28]`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在计算我们的理想数字3与验证集中的每个1,010个3之间的差异，对于每个28×28图像，结果形状为`[1010,28,28]`。
- en: 'There are a couple of important points about how broadcasting is implemented,
    which make it valuable not just for expressivity but also for performance:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 有关广播实现的一些重要要点，使其不仅对于表达性有价值，而且对于性能也有价值：
- en: PyTorch doesn’t *actually* copy `mean3` 1,010 times. It *pretends* it were a
    tensor of that shape, but doesn’t allocate any additional memory.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch实际上并没有将`mean3`复制1,010次。它*假装*它是一个具有该形状的张量，但不分配任何额外内存。
- en: It does the whole calculation in C (or, if you’re using a GPU, in CUDA, the
    equivalent of C on the GPU), tens of thousands of times faster than pure Python
    (up to millions of times faster on a GPU!).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在C中完成整个计算（或者，如果您使用GPU，则在CUDA中，相当于GPU上的C），比纯Python快数万倍（在GPU上甚至快数百万倍！）。
- en: This is true of all broadcasting and elementwise operations and functions done
    in PyTorch. *It’s the most important technique for you to know to create efficient
    PyTorch code.*
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这适用于PyTorch中所有广播和逐元素操作和函数。*这是您要了解的最重要的技术，以创建高效的PyTorch代码。*
- en: Next in `mnist_distance` we see `abs`. You might be able to guess now what this
    does when applied to a tensor. It applies the method to each individual element
    in the tensor, and returns a tensor of the results (that is, it applies the method
    *elementwise*). So in this case, we’ll get back 1,010 absolute values.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来在`mnist_distance`中我们看到`abs`。现在您可能能猜到将其应用于张量时会发生什么。它将方法应用于张量中的每个单独元素，并返回结果的张量（即，它逐元素应用方法）。因此，在这种情况下，我们将得到1,010个绝对值。
- en: Finally, our function calls `mean((-1,-2))`. The tuple `(-1,-2)` represents
    a range of axes. In Python, `-1` refers to the last element, and `-2` refers to
    the second-to-last. So in this case, this tells PyTorch that we want to take the
    mean ranging over the values indexed by the last two axes of the tensor. The last
    two axes are the horizontal and vertical dimensions of an image. After taking
    the mean over the last two axes, we are left with just the first tensor axis,
    which indexes over our images, which is why our final size was `(1010)`. In other
    words, for every image, we averaged the intensity of all the pixels in that image.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的函数调用`mean((-1,-2))`。元组`(-1,-2)`表示一系列轴。在Python中，`-1`指的是最后一个元素，`-2`指的是倒数第二个元素。因此，在这种情况下，这告诉PyTorch我们要对张量的最后两个轴的值进行平均。最后两个轴是图像的水平和垂直维度。在对最后两个轴进行平均后，我们只剩下第一个张量轴，它索引我们的图像，这就是为什么我们的最终大小是`(1010)`。换句话说，对于每个图像，我们对该图像中所有像素的强度进行了平均。
- en: We’ll be learning lots more about broadcasting throughout this book, especially
    in [Chapter 17](ch17.xhtml#chapter_foundations), and will be practicing it regularly
    too.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将学习更多关于广播的知识，特别是在[第17章](ch17.xhtml#chapter_foundations)中，并且也会经常进行实践。
- en: 'We can use `mnist_distance` to figure out whether an image is a 3 by using
    the following logic: if the distance between the digit in question and the ideal
    3 is less than the distance to the ideal 7, then it’s a 3\. This function will
    automatically do broadcasting and be applied elementwise, just like all PyTorch
    functions and operators:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`mnist_distance`来确定一幅图像是否为3，方法是使用以下逻辑：如果问题中的数字与理想的3之间的距离小于到理想的7的距离，则它是一个3。这个函数将自动进行广播，并逐个应用，就像所有PyTorch函数和运算符一样：
- en: '[PRE58]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Let’s test it on our example case:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的示例案例上测试一下：
- en: '[PRE59]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Note that when we convert the Boolean response to a float, we get `1.0` for
    `True` and `0.0` for `False`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们将布尔响应转换为浮点数时，`True`会得到`1.0`，`False`会得到`0.0`。
- en: 'Thanks to broadcasting, we can also test it on the full validation set of 3s:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 由于广播，我们还可以在所有3的完整验证集上进行测试：
- en: '[PRE61]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now we can calculate the accuracy for each of the 3s and 7s, by taking the
    average of that function for all 3s and its inverse for all 7s:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算每个3和7的准确率，方法是对所有3的函数取平均值，对所有7的函数取其倒数的平均值：
- en: '[PRE63]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'This looks like a pretty good start! We’re getting over 90% accuracy on both
    3s and 7s, and we’ve seen how to define a metric conveniently using broadcasting.
    But let’s be honest: 3s and 7s are very different-looking digits. And we’re classifying
    only 2 out of the 10 possible digits so far. So we’re going to need to do better!'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来是一个相当不错的开始！我们在3和7上都获得了超过90%的准确率，我们已经看到了如何使用广播方便地定义度量。但让我们诚实一点：3和7是非常不同的数字。到目前为止，我们只对10个可能的数字中的2个进行分类。所以我们需要做得更好！
- en: To do better, perhaps it is time to try a system that does some real learning—one
    that can automatically modify itself to improve its performance. In other words,
    it’s time to talk about the training process and SGD.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做得更好，也许现在是时候尝试一个真正学习的系统了，一个可以自动修改自身以提高性能的系统。换句话说，现在是时候谈论训练过程和SGD了。
- en: Stochastic Gradient Descent
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: Do you remember the way that Arthur Samuel described machine learning, which
    we quoted in [Chapter 1](ch01.xhtml#chapter_intro)?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得Arthur Samuel在[第1章](ch01.xhtml#chapter_intro)中描述机器学习的方式吗？
- en: Suppose we arrange for some automatic means of testing the effectiveness of
    any current weight assignment in terms of actual performance and provide a mechanism
    for altering the weight assignment so as to maximize the performance. We need
    not go into the details of such a procedure to see that it could be made entirely
    automatic and to see that a machine so programmed would “learn” from its experience.
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 假设我们安排一些自动手段来测试任何当前权重分配的有效性，以实际性能为基础，并提供一种机制来改变权重分配以最大化性能。我们不需要详细了解这种程序的细节，就可以看到它可以完全自动化，并且可以看到一个这样编程的机器会从中学习。
- en: As we discussed, this is the key to allowing us to have a model that can get
    better and better—that can learn. But our pixel similarity approach does not really
    do this. We do not have any kind of weight assignment, or any way of improving
    based on testing the effectiveness of a weight assignment. In other words, we
    can’t really improve our pixel similarity approach by modifying a set of parameters.
    To take advantage of the power of deep learning, we will first have to represent
    our task in the way that Samuel described it.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论过的，这是让我们拥有一个可以变得越来越好的模型的关键，可以学习。但我们的像素相似性方法实际上并没有做到这一点。我们没有任何权重分配，也没有任何根据测试权重分配的有效性来改进的方法。换句话说，我们无法通过修改一组参数来改进我们的像素相似性方法。为了充分利用深度学习的力量，我们首先必须按照Samuel描述的方式来表示我们的任务。
- en: 'Instead of trying to find the similarity between an image and an “ideal image,”
    we could instead look at each individual pixel and come up with a set of weights
    for each, such that the highest weights are associated with those pixels most
    likely to be black for a particular category. For instance, pixels toward the
    bottom right are not very likely to be activated for a 7, so they should have
    a low weight for a 7, but they are likely to be activated for an 8, so they should
    have a high weight for an 8\. This can be represented as a function and set of
    weight values for each possible category—for instance, the probability of being
    the number 8:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与其尝试找到图像与“理想图像”之间的相似性，我们可以查看每个单独的像素，并为每个像素提出一组权重，使得最高的权重与最有可能为特定类别的黑色像素相关联。例如，向右下方的像素不太可能被激活为7，因此它们对于7的权重应该很低，但它们很可能被激活为8，因此它们对于8的权重应该很高。这可以表示为一个函数和每个可能类别的一组权重值，例如，成为数字8的概率：
- en: '[PRE65]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Here we are assuming that `X` is the image, represented as a vector—in other
    words, with all of the rows stacked up end to end into a single long line. And
    we are assuming that the weights are a vector `W`. If we have this function, we
    just need some way to update the weights to make them a little bit better. With
    such an approach, we can repeat that step a number of times, making the weights
    better and better, until they are as good as we can make them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设`X`是图像，表示为一个向量—换句话说，所有行都堆叠在一起形成一个长长的单行。我们假设权重是一个向量`W`。如果我们有了这个函数，我们只需要一种方法来更新权重，使它们变得更好一点。通过这种方法，我们可以重复这个步骤多次，使权重变得越来越好，直到我们能够使它们尽可能好。
- en: We want to find the specific values for the vector `W` that cause the result
    of our function to be high for those images that are 8s, and low for those images
    that are not. Searching for the best vector `W` is a way to search for the best
    function for recognizing 8s. (Because we are not yet using a deep neural network,
    we are limited by what our function can do—we are going to fix that constraint
    later in this chapter.)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望找到导致我们的函数对于那些是8的图像结果高，对于那些不是的图像结果低的向量`W`的特定值。搜索最佳向量`W`是搜索最佳函数以识别8的一种方式。（因为我们还没有使用深度神经网络，我们受到我们的函数能力的限制，我们将在本章后面解决这个约束。）
- en: 'To be more specific, here are the steps required to turn this function into
    a machine learning classifier:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，以下是将这个函数转化为机器学习分类器所需的步骤：
- en: '*Initialize* the weights.'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*初始化*权重。'
- en: For each image, use these weights to *predict* whether it appears to be a 3
    or a 7.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个图像，使用这些权重来*预测*它是3还是7。
- en: Based on these predictions, calculate how good the model is (its *loss*).
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于这些预测，计算模型有多好（它的*损失*）。
- en: Calculate the *gradient*, which measures for each weight how changing that weight
    would change the loss.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算*梯度*，它衡量了每个权重的变化如何改变损失。
- en: '*Step* (that is, change) all the weights based on that calculation.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据这个计算，*改变*（即，改变）所有权重。
- en: Go back to step 2 and *repeat* the process.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到步骤2并*重复*这个过程。
- en: Iterate until you decide to *stop* the training process (for instance, because
    the model is good enough or you don’t want to wait any longer).
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代直到你决定*停止*训练过程（例如，因为模型已经足够好或者你不想再等待了）。
- en: These seven steps, illustrated in [Figure 4-1](#gradient_descent), are the key
    to the training of all deep learning models. That deep learning turns out to rely
    entirely on these steps is extremely surprising and counterintuitive. It’s amazing
    that this process can solve such complex problems. But, as you’ll see, it really
    does!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这七个步骤，如[图4-1](#gradient_descent)所示，是所有深度学习模型训练的关键。深度学习完全依赖于这些步骤，这是非常令人惊讶和反直觉的。令人惊奇的是，这个过程可以解决如此复杂的问题。但是，正如你将看到的，它确实可以！
- en: '![Graph showing the steps for Gradient Descent](Images/dlcf_0401.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![显示梯度下降步骤的图表](Images/dlcf_0401.png)'
- en: Figure 4-1\. The gradient descent process
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1. 梯度下降过程
- en: 'There are many ways to do each of these seven steps, and we will be learning
    about them throughout the rest of this book. These are the details that make a
    big difference for deep learning practitioners, but it turns out that the general
    approach to each one follows some basic principles. Here are a few guidelines:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤都有许多方法，我们将在本书的其余部分学习它们。这些细节对于深度学习从业者来说非常重要，但事实证明，对于每个步骤的一般方法都遵循一些基本原则。以下是一些建议：
- en: Initialize
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化
- en: We initialize the parameters to random values. This may sound surprising. There
    are certainly other choices we could make, such as initializing them to the percentage
    of times that pixel is activated for that category—but since we already know that
    we have a routine to improve these weights, it turns out that just starting with
    random weights works perfectly well.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将参数初始化为随机值。这可能听起来令人惊讶。我们当然可以做其他选择，比如将它们初始化为该类别激活该像素的百分比—但由于我们已经知道我们有一种方法来改进这些权重，结果证明只是从随机权重开始就可以完全正常运行。
- en: Loss
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 损失
- en: This is what Samuel referred to when he spoke of *testing the effectiveness
    of any current weight assignment in terms of actual performance*. We need a function
    that will return a number that is small if the performance of the model is good
    (the standard approach is to treat a small loss as good and a large loss as bad,
    although this is just a convention).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Samuel所说的*根据实际表现测试任何当前权重分配的有效性*。我们需要一个函数，如果模型的表现好，它将返回一个小的数字（标准方法是将小的损失视为好的，大的损失视为坏的，尽管这只是一种约定）。
- en: Step
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤
- en: 'A simple way to figure out whether a weight should be increased a bit or decreased
    a bit would be just to try it: increase the weight by a small amount, and see
    if the loss goes up or down. Once you find the correct direction, you could then
    change that amount by a bit more, or a bit less, until you find an amount that
    works well. However, this is slow! As we will see, the magic of calculus allows
    us to directly figure out in which direction, and by roughly how much, to change
    each weight, without having to try all these small changes. The way to do this
    is by calculating *gradients*. This is just a performance optimization; we would
    get exactly the same results by using the slower manual process as well.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的方法来判断一个权重是否应该增加一点或减少一点就是尝试一下：增加一点权重，看看损失是增加还是减少。一旦找到正确的方向，你可以再多改变一点或少改变一点，直到找到一个效果好的量。然而，这很慢！正如我们将看到的，微积分的魔力使我们能够直接找出每个权重应该朝哪个方向改变，大概改变多少，而不必尝试所有这些小的改变。这样做的方法是通过计算*梯度*。这只是一种性能优化；我们也可以通过使用更慢的手动过程得到完全相同的结果。
- en: Stop
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 停止
- en: Once we’ve decided how many epochs to train the model for (a few suggestions
    for this were given in the earlier list), we apply that decision. For our digit
    classifier, we would keep training until the accuracy of the model started getting
    worse, or we ran out of time.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们决定要为模型训练多少个周期（之前的列表中给出了一些建议），我们就会应用这个决定。对于我们的数字分类器，我们会继续训练，直到模型的准确率开始变差，或者我们用完时间为止。
- en: 'Before applying these steps to our image classification problem, let’s illustrate
    what they look like in a simpler case. First we will define a very simple function,
    the quadratic—let’s pretend that this is our loss function, and `x` is a weight
    parameter of the function:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在将这些步骤应用于我们的图像分类问题之前，让我们在一个更简单的情况下看看它们是什么样子。首先我们将定义一个非常简单的函数，二次函数—假设这是我们的损失函数，`x`是函数的权重参数：
- en: '[PRE66]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Here is a graph of that function:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该函数的图表：
- en: '[PRE67]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![](Images/dlcf_04in07.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in07.png)'
- en: 'The sequence of steps we described earlier starts by picking a random value
    for a parameter, and calculating the value of the loss:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前描述的步骤序列从选择参数的随机值开始，并计算损失的值：
- en: '[PRE68]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![](Images/dlcf_04in08.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in08.png)'
- en: 'Now we look to see what would happen if we increased or decreased our parameter
    by a little bit—the *adjustment*. This is simply the slope at a particular point:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如果我们稍微增加或减少参数会发生什么—*调整*。这只是特定点的斜率：
- en: '![A graph showing the squared function with the slope at one point](Images/dlcf_04in09.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![显示在某一点的斜率的平方函数的图表](Images/dlcf_04in09.png)'
- en: 'We can change our weight by a little in the direction of the slope, calculate
    our loss and adjustment again, and repeat this a few times. Eventually, we will
    get to the lowest point on our curve:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以稍微改变我们的权重朝着斜坡的方向，计算我们的损失和调整，然后再重复几次。最终，我们将到达曲线上的最低点：
- en: '![An illustration of gradient descent](Images/dlcf_04in10.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降的示意图](Images/dlcf_04in10.png)'
- en: This basic idea goes all the way back to Isaac Newton, who pointed out that
    we can optimize arbitrary functions in this way. Regardless of how complicated
    our functions become, this basic approach of gradient descent will not significantly
    change. The only minor changes we will see later in this book are some handy ways
    we can make it faster, by finding better steps.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本思想最早可以追溯到艾萨克·牛顿，他指出我们可以以这种方式优化任意函数。无论我们的函数变得多么复杂，梯度下降的这种基本方法不会有太大变化。我们在本书后面看到的唯一微小变化是一些方便的方法，可以让我们更快地找到更好的步骤。
- en: Calculating Gradients
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算梯度
- en: The one magic step is the bit where we calculate the gradients. As we mentioned,
    we use calculus as a performance optimization; it allows us to more quickly calculate
    whether our loss will go up or down when we adjust our parameters up or down.
    In other words, the gradients will tell us how much we have to change each weight
    to make our model better.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的魔法步骤是计算梯度的部分。正如我们提到的，我们使用微积分作为性能优化；它让我们更快地计算当我们调整参数时我们的损失会上升还是下降。换句话说，梯度将告诉我们我们需要改变每个权重多少才能使我们的模型更好。
- en: You may remember from your high school calculus class that the *derivative*
    of a function tells you how much a change in its parameters will change its result.
    If not, don’t worry; lots of us forget calculus once high school is behind us!
    But you will need some intuitive understanding of what a derivative is before
    you continue, so if this is all very fuzzy in your head, head over to Khan Academy
    and complete the [lessons on basic derivatives](https://oreil.ly/nyd0R). You won’t
    have to know how to calculate them yourself; you just have to know what a derivative
    is.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得高中微积分课上的*导数*告诉您函数参数的变化会如何改变其结果。如果不记得，不用担心；我们很多人高中毕业后就忘了微积分！但在继续之前，您需要对导数有一些直观的理解，所以如果您对此一头雾水，可以前往
    Khan Academy 完成[基本导数课程](https://oreil.ly/nyd0R)。您不必自己计算导数；您只需要知道导数是什么。
- en: 'The key point about a derivative is this: for any function, such as the quadratic
    function we saw in the previous section, we can calculate its derivative. The
    derivative is another function. It calculates the change, rather than the value.
    For instance, the derivative of the quadratic function at the value 3 tells us
    how rapidly the function changes at the value 3\. More specifically, you may recall
    that gradient is defined as *rise/run*; that is, the change in the value of the
    function, divided by the change in the value of the parameter. When we know how
    our function will change, we know what we need to do to make it smaller. This
    is the key to machine learning: having a way to change the parameters of a function
    to make it smaller. Calculus provides us with a computational shortcut, the derivative,
    which lets us directly calculate the gradients of our functions.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 导数的关键点在于：对于任何函数，比如我们在前一节中看到的二次函数，我们可以计算它的导数。导数是另一个函数。它计算的是变化，而不是值。例如，在值为3时，二次函数的导数告诉我们函数在值为3时的变化速度。更具体地说，您可能还记得梯度被定义为*上升/水平移动*；也就是说，函数值的变化除以参数值的变化。当我们知道我们的函数将如何变化时，我们就知道我们需要做什么来使它变小。这是机器学习的关键：有一种方法来改变函数的参数使其变小。微积分为我们提供了一个计算的捷径，即导数，它让我们直接计算我们函数的梯度。
- en: One important thing to be aware of is that our function has lots of weights
    that we need to adjust, so when we calculate the derivative, we won’t get back
    one number, but lots of them—a gradient for every weight. But there is nothing
    mathematically tricky here; you can calculate the derivative with respect to one
    weight and treat all the other ones as constant, and then repeat that for each
    other weight. This is how all of the gradients are calculated, for every weight.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的事情要注意的是我们的函数有很多需要调整的权重，所以当我们计算导数时，我们不会得到一个数字，而是很多个—每个权重都有一个梯度。但在这里没有数学上的技巧；您可以计算相对于一个权重的导数，将其他所有权重视为常数，然后对每个其他权重重复这个过程。这就是计算所有梯度的方法，对于每个权重。
- en: We mentioned just now that you won’t have to calculate any gradients yourself.
    How can that be? Amazingly enough, PyTorch is able to automatically compute the
    derivative of nearly any function! What’s more, it does it very fast. Most of
    the time, it will be at least as fast as any derivative function that you can
    create by hand. Let’s see an example.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 刚才我们提到您不必自己计算任何梯度。这怎么可能？令人惊讶的是，PyTorch能够自动计算几乎任何函数的导数！而且，它计算得非常快。大多数情况下，它至少与您手动创建的任何导数函数一样快。让我们看一个例子。
- en: 'First, let’s pick a tensor value at which we want gradients:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们选择一个张量数值，我们想要梯度：
- en: '[PRE69]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Notice the special method `requires_grad_`? That’s the magical incantation we
    use to tell PyTorch that we want to calculate gradients with respect to that variable
    at that value. It is essentially tagging the variable, so PyTorch will remember
    to keep track of how to compute gradients of the other direct calculations on
    it that you will ask for.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 注意特殊方法`requires_grad_`？这是我们告诉PyTorch我们想要计算梯度的神奇咒语。这实质上是给变量打上标记，这样PyTorch就会记住如何计算您要求的其他直接计算的梯度。
- en: Alexis Says
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Alexis说
- en: This API might throw you off if you’re coming from math or physics. In those
    contexts, the “gradient” of a function is just another function (i.e., its derivative),
    so you might expect gradient-related APIs to give you a new function. But in deep
    learning, “gradient” usually means the *value* of a function’s derivative at a
    particular argument value. The PyTorch API also puts the focus on the argument,
    not the function you’re actually computing the gradients of. It may feel backward
    at first, but it’s just a different perspective.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您来自数学或物理学，这个API可能会让您困惑。在这些背景下，函数的“梯度”只是另一个函数（即，它的导数），因此您可能期望与梯度相关的API提供给您一个新函数。但在深度学习中，“梯度”通常意味着函数的导数在特定参数值处的*值*。PyTorch
    API也将重点放在参数上，而不是您实际计算梯度的函数。起初可能感觉有些反常，但这只是一个不同的视角。
- en: 'Now we calculate our function with that value. Notice how PyTorch prints not
    just the value calculated, but also a note that it has a gradient function it’ll
    be using to calculate our gradients when needed:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们用这个值计算我们的函数。注意PyTorch打印的不仅是计算的值，还有一个提示，它有一个梯度函数将在需要时用来计算我们的梯度：
- en: '[PRE70]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Finally, we tell PyTorch to calculate the gradients for us:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们告诉PyTorch为我们计算梯度：
- en: '[PRE72]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The “backward” here refers to *backpropagation*, which is the name given to
    the process of calculating the derivative of each layer. We’ll see how this is
    done exactly in [Chapter 17](ch17.xhtml#chapter_foundations), when we calculate
    the gradients of a deep neural net from scratch. This is called the *backward
    pass* of the network, as opposed to the *forward pass*, which is where the activations
    are calculated. Life would probably be easier if `backward` was just called `calculate_grad`,
    but deep learning folks really do like to add jargon everywhere they can!
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的“backward”指的是*反向传播*，这是计算每一层导数的过程的名称。我们将在[第17章](ch17.xhtml#chapter_foundations)中看到这是如何精确完成的，当我们从头开始计算深度神经网络的梯度时。这被称为网络的*反向传播*，与*前向传播*相对，前者是计算激活的地方。如果`backward`只是被称为`calculate_grad`，生活可能会更容易，但深度学习的人确实喜欢在任何地方添加行话！
- en: 'We can now view the gradients by checking the `grad` attribute of our tensor:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过检查我们张量的`grad`属性来查看梯度：
- en: '[PRE73]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: If you remember your high school calculus rules, the derivative of `x**2` is
    `2*x`, and we have `x=3`, so the gradients should be `2*3=6`, which is what PyTorch
    calculated for us!
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您记得高中微积分规则，`x**2`的导数是`2*x`，我们有`x=3`，所以梯度应该是`2*3=6`，这就是PyTorch为我们计算的结果！
- en: 'Now we’ll repeat the preceding steps, but with a vector argument for our function:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将重复前面的步骤，但使用一个向量参数来计算我们的函数：
- en: '[PRE75]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'And we’ll add `sum` to our function so it can take a vector (i.e., a rank-1
    tensor) and return a scalar (i.e., a rank-0 tensor):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 并且我们将`sum`添加到我们的函数中，以便它可以接受一个向量（即，一个秩为1的张量）并返回一个标量（即，一个秩为0的张量）：
- en: '[PRE77]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Our gradients are `2*xt`, as we’d expect!
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的梯度是`2*xt`，正如我们所期望的！
- en: '[PRE79]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The gradients tell us only the slope of our function; they don’t tell us exactly
    how far to adjust the parameters. But they do give us some idea of how far: if
    the slope is very large, that may suggest that we have more adjustments to do,
    whereas if the slope is very small, that may suggest that we are close to the
    optimal value.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度告诉我们函数的斜率；它们并不告诉我们要调整参数多远。但它们确实给了我们一些想法：如果斜率非常大，那可能意味着我们需要更多的调整，而如果斜率非常小，那可能意味着我们接近最优值。
- en: Stepping with a Learning Rate
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用学习率进行步进
- en: 'Deciding how to change our parameters based on the values of the gradients
    is an important part of the deep learning process. Nearly all approaches start
    with the basic idea of multiplying the gradient by some small number, called the
    *learning rate* (LR). The learning rate is often a number between 0.001 and 0.1,
    although it could be anything. Often people select a learning rate just by trying
    a few, and finding which results in the best model after training (we’ll show
    you a better approach later in this book, called the *learning rate finder*).
    Once you’ve picked a learning rate, you can adjust your parameters using this
    simple function:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 根据梯度值来决定如何改变我们的参数是深度学习过程中的一个重要部分。几乎所有方法都从一个基本思想开始，即将梯度乘以一些小数字，称为*学习率*（LR）。学习率通常是0.001到0.1之间的数字，尽管它可以是任何值。通常人们通过尝试几个学习率来选择一个，并找出哪个在训练后产生最佳模型的结果（我们将在本书后面展示一个更好的方法，称为*学习率查找器*）。一旦选择了学习率，您可以使用这个简单函数调整参数：
- en: '[PRE81]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: This is known as *stepping* your parameters, using an *optimization step*.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为*调整*您的参数，使用*优化步骤*。
- en: If you pick a learning rate that’s too low, it can mean having to do a lot of
    steps. [Figure 4-2](#descent_small) illustrates that.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择的学习率太低，可能意味着需要执行很多步骤。[图4-2](#descent_small)说明了这一点。
- en: '![An illustration of gradient descent with a LR too low](Images/dlcf_0402.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降示例，学习率过低](Images/dlcf_0402.png)'
- en: Figure 4-2\. Gradient descent with low LR
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2。学习率过低的梯度下降
- en: But picking a learning rate that’s too high is even worse—it can result in the
    loss getting *worse*, as we see in [Figure 4-3](#descent_div)!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 但选择一个学习率太高的学习率更糟糕——它可能导致损失变得*更糟*，正如我们在[图4-3](#descent_div)中看到的！
- en: '![An illustration of gradient descent with a LR too high](Images/dlcf_0403.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![学习率过高的梯度下降示例](Images/dlcf_0403.png)'
- en: Figure 4-3\. Gradient descent with high LR
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3\. 学习率过高的梯度下降
- en: If the learning rate is too high, it may also “bounce” around, rather than diverging;
    [Figure 4-4](#descent_bouncy) shows how this results in taking many steps to train
    successfully.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习率太高，它也可能会“弹跳”而不是发散；[图4-4](#descent_bouncy)显示了这样做需要许多步骤才能成功训练。
- en: '![An illustation of gradient descent with a bouncy LR](Images/dlcf_0404.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![带有弹跳学习率的梯度下降示例](Images/dlcf_0404.png)'
- en: Figure 4-4\. Gradient descent with bouncy LR
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4\. 带有弹跳学习率的梯度下降
- en: Now let’s apply all of this in an end-to-end example.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在一个端到端的示例中应用所有这些。
- en: An End-to-End SGD Example
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个端到端的SGD示例
- en: We’ve seen how to use gradients to minimize our loss. Now it’s time to look
    at an SGD example and see how finding a minimum can be used to train a model to
    fit data better.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何使用梯度来最小化我们的损失。现在是时候看一个SGD示例，并看看如何找到最小值来训练模型以更好地拟合数据。
- en: 'Let’s start with a simple, synthetic example model. Imagine you were measuring
    the speed of a roller coaster as it went over the top of a hump. It would start
    fast, and then get slower as it went up the hill; it would be slowest at the top,
    and it would then speed up again as it went downhill. You want to build a model
    of how the speed changes over time. If you were measuring the speed manually every
    second for 20 seconds, it might look something like this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的合成示例模型开始。想象一下，您正在测量过山车通过顶峰时的速度。它会开始快速，然后随着上坡而变慢；在顶部最慢，然后在下坡时再次加速。您想建立一个关于速度随时间变化的模型。如果您每秒手动测量速度20秒，它可能看起来像这样：
- en: '[PRE82]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '![](Images/dlcf_04in11.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in11.png)'
- en: 'We’ve added a bit of random noise, since measuring things manually isn’t precise.
    This means it’s not that easy to answer the question: what was the roller coaster’s
    speed? Using SGD, we can try to find a function that matches our observations.
    We can’t consider every possible function, so let’s use a guess that it will be
    quadratic; i.e., a function of the form `a*(time**2)+(b*time)+c`.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了一些随机噪声，因为手动测量不够精确。这意味着很难回答问题：过山车的速度是多少？使用SGD，我们可以尝试找到一个与我们的观察相匹配的函数。我们无法考虑每种可能的函数，所以让我们猜测它将是二次的；即，一个形式为`a*(time**2)+(b*time)+c`的函数。
- en: 'We want to distinguish clearly between the function’s input (the time when
    we are measuring the coaster’s speed) and its parameters (the values that define
    *which* quadratic we’re trying). So let’s collect the parameters in one argument
    and thus separate the input, `t`, and the parameters, `params`, in the function’s
    signature:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望清楚地区分函数的输入（我们测量过山车速度的时间）和其参数（定义*我们正在尝试的*二次函数的值）。因此，让我们将参数收集在一个参数中，从而在函数的签名中分离输入`t`和参数`params`：
- en: '[PRE85]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: In other words, we’ve restricted the problem of finding the best imaginable
    function that fits the data to finding the best *quadratic* function. This greatly
    simplifies the problem, since every quadratic function is fully defined by the
    three parameters `a`, `b`, and `c`. Thus, to find the best quadratic function,
    we need to find only the best values for `a`, `b`, and `c`.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们已经将找到最佳拟合数据的最佳函数的问题限制为找到最佳*二次*函数。这极大地简化了问题，因为每个二次函数都由三个参数`a`、`b`和`c`完全定义。因此，要找到最佳二次函数，我们只需要找到最佳的`a`、`b`和`c`的值。
- en: If we can solve this problem for the three parameters of a quadratic function,
    we’ll be able to apply the same approach for other, more complex functions with
    more parameters—such as a neural net. Let’s find the parameters for `f` first,
    and then we’ll come back and do the same thing for the MNIST dataset with a neural
    net.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以解决二次函数的三个参数的问题，我们就能够对其他具有更多参数的更复杂函数应用相同的方法——比如神经网络。让我们先找到`f`的参数，然后我们将回来对MNIST数据集使用神经网络做同样的事情。
- en: 'We need to define first what we mean by “best.” We define this precisely by
    choosing a *loss function*, which will return a value based on a prediction and
    a target, where lower values of the function correspond to “better” predictions.
    For continuous data, it’s common to use *mean squared error*:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义“最佳”是什么意思。我们通过选择一个*损失函数*来精确定义这一点，该函数将根据预测和目标返回一个值，其中函数的较低值对应于“更好”的预测。对于连续数据，通常使用*均方误差*：
- en: '[PRE86]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Now, let’s work through our seven-step process.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们按照我们的七步流程进行工作。
- en: 'Step 1: Initialize the parameters'
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一步：初始化参数
- en: 'First, we initialize the parameters to random values and tell PyTorch that
    we want to track their gradients using `requires_grad_`:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将参数初始化为随机值，并告诉PyTorch我们要使用`requires_grad_`跟踪它们的梯度：
- en: '[PRE87]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Step 2: Calculate the predictions'
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二步：计算预测
- en: 'Next, we calculate the predictions:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算预测：
- en: '[PRE88]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Let’s create a little function to see how close our predictions are to our
    targets, and take a look:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个小函数来查看我们的预测与目标的接近程度，并看一看：
- en: '[PRE89]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '![](Images/dlcf_04in12.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in12.png)'
- en: This doesn’t look very close—our random parameters suggest that the roller coaster
    will end up going backward, since we have negative speeds!
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来并不接近——我们的随机参数表明过山车最终会倒退，因为我们有负速度！
- en: 'Step 3: Calculate the loss'
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三步：计算损失
- en: 'We calculate the loss as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算损失如下：
- en: '[PRE91]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Our goal is now to improve this. To do that, we’ll need to know the gradients.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标现在是改进这一点。为了做到这一点，我们需要知道梯度。
- en: 'Step 4: Calculate the gradients'
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第四步：计算梯度
- en: 'The next step is to calculate the gradients, or an approximation of how the
    parameters need to change:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是计算梯度，或者近似参数需要如何改变：
- en: '[PRE93]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'We can use these gradients to improve our parameters. We’ll need to pick a
    learning rate (we’ll discuss how to do that in practice in the next chapter; for
    now, we’ll just use 1e-5 or 0.00001):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这些梯度来改进我们的参数。我们需要选择一个学习率（我们将在下一章中讨论如何在实践中做到这一点；现在，我们将使用1e-5或0.00001）：
- en: '[PRE97]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Step 5: Step the weights'
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第5步：调整权重
- en: 'Now we need to update the parameters based on the gradients we just calculated:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要根据刚刚计算的梯度更新参数：
- en: '[PRE99]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Alexis Says
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Alexis说
- en: Understanding this bit depends on remembering recent history. To calculate the
    gradients, we call `backward` on the `loss`. But this `loss` was itself calculated
    by `mse`, which in turn took `preds` as an input, which was calculated using `f`
    taking as an input `params`, which was the object on which we originally called
    `required_grads_`—which is the original call that now allows us to call `backward`
    on `loss`. This chain of function calls represents the mathematical composition
    of functions, which enables PyTorch to use calculus’s chain rule under the hood
    to calculate these gradients.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这一点取决于记住最近的历史。为了计算梯度，我们在`loss`上调用`backward`。但是这个`loss`本身是通过`mse`计算的，而`mse`又以`preds`作为输入，`preds`是使用`f`计算的，`f`以`params`作为输入，`params`是我们最初调用`required_grads_`的对象，这是最初的调用，现在允许我们在`loss`上调用`backward`。这一系列函数调用代表了函数的数学组合，使得PyTorch能够在幕后使用微积分的链式法则来计算这些梯度。
- en: 'Let’s see if the loss has improved:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看损失是否有所改善：
- en: '[PRE100]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'And take a look at the plot:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 再看一下图表：
- en: '[PRE102]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '![](Images/dlcf_04in13.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in13.png)'
- en: 'We need to repeat this a few times, so we’ll create a function to apply one
    step:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要重复这个过程几次，所以我们将创建一个应用一步的函数：
- en: '[PRE103]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Step 6: Repeat the process'
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第6步：重复这个过程
- en: 'Now we iterate. By looping and performing many improvements, we hope to reach
    a good result:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进行迭代。通过循环和进行许多改进，我们希望达到一个好的结果：
- en: '[PRE104]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'The loss is going down, just as we hoped! But looking only at these loss numbers
    disguises the fact that each iteration represents an entirely different quadratic
    function being tried, on the way to finding the best possible quadratic function.
    We can see this process visually if, instead of printing out the loss function,
    we plot the function at every step. Then we can see how the shape is approaching
    the best possible quadratic function for our data:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 损失正在下降，正如我们所希望的！但仅仅看这些损失数字掩盖了一个事实，即每次迭代代表尝试一个完全不同的二次函数，以找到最佳可能的二次函数。如果我们不打印出损失函数，而是在每一步绘制函数，我们可以看到形状是如何接近我们的数据的最佳可能的二次函数：
- en: '[PRE106]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '![](Images/dlcf_04in14.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in14.png)'
- en: 'Step 7: Stop'
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第7步：停止
- en: We just decided to stop after 10 epochs arbitrarily. In practice, we would watch
    the training and validation losses and our metrics to decide when to stop, as
    we’ve discussed.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚决定在任意选择的10个epochs后停止。在实践中，我们会观察训练和验证损失以及我们的指标，以决定何时停止，正如我们所讨论的那样。
- en: Summarizing Gradient Descent
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结梯度下降
- en: Now that you’ve seen what happens in each step, let’s take another look at our
    graphical representation of the gradient descent process ([Figure 4-5](#gradient_descent_process))
    and do a quick recap.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到每个步骤中发生的事情，让我们再次看一下我们的梯度下降过程的图形表示（[图4-5](#gradient_descent_process)）并进行一个快速回顾。
- en: '![Graph showing the steps for Gradient Descent](Images/dlcf_0405.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![显示梯度下降步骤的图表](Images/dlcf_0405.png)'
- en: Figure 4-5\. The gradient descent process
  id: totrans-334
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5\. 梯度下降过程
- en: At the beginning, the weights of our model can be random (training *from scratch*)
    or come from a pretrained model (*transfer learning*). In the first case, the
    output we will get from our inputs won’t have anything to do with what we want,
    and even in the second case, it’s likely the pretrained model won’t be very good
    at the specific task we are targeting. So the model will need to *learn* better
    weights.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时，我们模型的权重可以是随机的（从头开始训练）或来自预训练模型（迁移学习）。在第一种情况下，我们从输入得到的输出与我们想要的完全无关，即使在第二种情况下，预训练模型也可能不太擅长我们所针对的特定任务。因此，模型需要学习更好的权重。
- en: We begin by comparing the outputs the model gives us with our targets (we have
    labeled data, so we know what result the model should give) using a *loss function*,
    which returns a number that we want to make as low as possible by improving our
    weights. To do this, we take a few data items (such as images) from the training
    set and feed them to our model. We compare the corresponding targets using our
    loss function, and the score we get tells us how wrong our predictions were. We
    then change the weights a little bit to make it slightly better.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将模型给出的输出与我们的目标进行比较（我们有标记数据，所以我们知道模型应该给出什么结果），使用一个*损失函数*，它返回一个数字，我们希望通过改进我们的权重使其尽可能低。为了做到这一点，我们从训练集中取出一些数据项（如图像）并将它们馈送给我们的模型。我们使用我们的损失函数比较相应的目标，我们得到的分数告诉我们我们的预测有多么错误。然后我们稍微改变权重使其稍微更好。
- en: To find how to change the weights to make the loss a bit better, we use calculus
    to calculate the *gradients*. (Actually, we let PyTorch do it for us!) Let’s consider
    an analogy. Imagine you are lost in the mountains with your car parked at the
    lowest point. To find your way back to it, you might wander in a random direction,
    but that probably wouldn’t help much. Since you know your vehicle is at the lowest
    point, you would be better off going downhill. By always taking a step in the
    direction of the steepest downward slope, you should eventually arrive at your
    destination. We use the magnitude of the gradient (i.e., the steepness of the
    slope) to tell us how big a step to take; specifically, we multiply the gradient
    by a number we choose called the *learning rate* to decide on the step size. We
    then *iterate* until we have reached the lowest point, which will be our parking
    lot; then we can *stop*.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出如何改变权重使损失稍微变好，我们使用微积分来计算*梯度*。（实际上，我们让PyTorch为我们做这个！）让我们考虑一个类比。想象一下你在山上迷路了，你的车停在最低点。为了找到回去的路，你可能会朝着随机方向走，但那可能不会有太大帮助。由于你知道你的车在最低点，你最好是往下走。通过始终朝着最陡峭的下坡方向迈出一步，你最终应该到达目的地。我们使用梯度的大小（即坡度的陡峭程度）来告诉我们应该迈多大一步；具体来说，我们将梯度乘以我们选择的一个称为*学习率*的数字来决定步长。然后我们*迭代*直到达到最低点，那将是我们的停车场；然后我们可以*停止*。
- en: All of what we just saw can be transposed directly to the MNIST dataset, except
    for the loss function. Let’s now see how we can define a good training objective.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到的所有内容都可以直接转换到MNIST数据集，除了损失函数。现在让我们看看如何定义一个好的训练目标。
- en: The MNIST Loss Function
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST损失函数
- en: 'We already have our `x`s—that is, our independent variables, the images themselves.
    We’ll concatenate them all into a single tensor, and also change them from a list
    of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor). We can do
    this using `view`, which is a PyTorch method that changes the shape of a tensor
    without changing its contents. `-1` is a special parameter to `view` that means
    “make this axis as big as necessary to fit all the data”:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了我们的`x`—也就是我们的自变量，图像本身。我们将它们全部连接成一个单一的张量，并且还将它们从矩阵列表（一个秩为3的张量）转换为向量列表（一个秩为2的张量）。我们可以使用`view`来做到这一点，`view`是一个PyTorch方法，可以改变张量的形状而不改变其内容。`-1`是`view`的一个特殊参数，意思是“使这个轴尽可能大以适应所有数据”：
- en: '[PRE107]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'We need a label for each image. We’ll use `1` for 3s and `0` for 7s:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为每张图片标记。我们将使用`1`表示3，`0`表示7：
- en: '[PRE108]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'A `Dataset` in PyTorch is required to return a tuple of `(x,y)` when indexed.
    Python provides a `zip` function that, when combined with `list`, provides a simple
    way to get this functionality:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，当索引时，`Dataset`需要返回一个`(x,y)`元组。Python提供了一个`zip`函数，当与`list`结合使用时，可以简单地实现这个功能：
- en: '[PRE110]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Now we need an (initially random) weight for every pixel (this is the *initialize*
    step in our seven-step process):'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要为每个像素（最初是随机的）分配一个权重（这是我们七步过程中的*初始化*步骤）：
- en: '[PRE113]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'The function `weights*pixels` won’t be flexible enough—it is always equal to
    0 when the pixels are equal to 0 (i.e., its *intercept* is 0). You might remember
    from high school math that the formula for a line is `y=w*x+b`; we still need
    the `b`. We’ll initialize it to a random number too:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`weights*pixels`不够灵活—当像素等于0时，它总是等于0（即其*截距*为0）。你可能还记得高中数学中线的公式是`y=w*x+b`；我们仍然需要`b`。我们也会将其初始化为一个随机数：
- en: '[PRE115]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: In neural networks, the `w` in the equation `y=w*x+b` is called the *weights*,
    and the `b` is called the *bias*. Together, the weights and bias make up the *parameters*.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，方程`y=w*x+b`中的`w`被称为*权重*，`b`被称为*偏置*。权重和偏置一起构成*参数*。
- en: 'Jargon: Parameters'
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：参数
- en: The *weights* and *biases* of a model. The weights are the `w` in the equation
    `w*x+b`, and the biases are the `b` in that equation.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的*权重*和*偏置*。权重是方程`w*x+b`中的`w`，偏置是该方程中的`b`。
- en: 'We can now calculate a prediction for one image:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以为一张图片计算一个预测：
- en: '[PRE116]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: While we could use a Python `for` loop to calculate the prediction for each
    image, that would be very slow. Because Python loops don’t run on the GPU, and
    because Python is a slow language for loops in general, we need to represent as
    much of the computation in a model as possible using higher-level functions.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以使用Python的`for`循环来计算每张图片的预测，但那将非常慢。因为Python循环不在GPU上运行，而且因为Python在一般情况下循环速度较慢，我们需要尽可能多地使用高级函数来表示模型中的计算。
- en: In this case, there’s an extremely convenient mathematical operation that calculates
    `w*x` for every row of a matrix—it’s called *matrix multiplication*. [Figure 4-6](#matmul)
    shows what matrix multiplication looks like.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，有一个非常方便的数学运算可以为矩阵的每一行计算`w*x`—它被称为*矩阵乘法*。[图4-6](#matmul)展示了矩阵乘法的样子。
- en: '![Matrix multiplication](Images/dlcf_0406.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![矩阵乘法](Images/dlcf_0406.png)'
- en: Figure 4-6\. Matrix multiplication
  id: totrans-363
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6\. 矩阵乘法
- en: This image shows two matrices, `A` and `B`, being multiplied together. Each
    item of the result, which we’ll call `AB`, contains each item of its corresponding
    row of `A` multiplied by each item of its corresponding column of `B`, added together.
    For instance, row 1, column 2 (the yellow dot with a red border) is calculated
    as <math alttext="a Subscript 1 comma 1 Baseline asterisk b Subscript 1 comma
    2 plus a Subscript 1 comma 2 Baseline asterisk b Subscript 2 comma 2"><mrow><msub><mi>a</mi>
    <mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub> <mo>*</mo> <msub><mi>b</mi>
    <mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub> <mo>+</mo> <msub><mi>a</mi>
    <mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub> <mo>*</mo> <msub><mi>b</mi>
    <mrow><mn>2</mn><mo>,</mo><mn>2</mn></mrow></msub></mrow></math> . If you need
    a refresher on matrix multiplication, we suggest you take a look at the [“Intro
    to Matrix Multiplication”](https://oreil.ly/w0XKS) on Khan Academy, since this
    is the most important mathematical operation in deep learning.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅图展示了两个矩阵`A`和`B`相乘。结果的每个项目，我们称之为`AB`，包含了`A`的对应行的每个项目与`B`的对应列的每个项目相乘后相加。例如，第1行第2列（带有红色边框的黄色点）计算为<math
    alttext="a下标1，1乘以b下标1，2加上a下标1，2乘以b下标2，2">。如果您需要复习矩阵乘法，我们建议您查看Khan Academy的“矩阵乘法简介”，因为这是深度学习中最重要的数学运算。
- en: 'In Python, matrix multiplication is represented with the `@` operator. Let’s
    try it:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，矩阵乘法用`@`运算符表示。让我们试一试：
- en: '[PRE118]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: The first element is the same as we calculated before, as we’d expect. This
    equation, `batch @ weights + bias`, is one of the two fundamental equations of
    any neural network (the other one is the *activation function*, which we’ll see
    in a moment).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个元素与我们之前计算的相同，正如我们所期望的。这个方程`batch @ weights + bias`是任何神经网络的两个基本方程之一（另一个是*激活函数*，我们马上会看到）。
- en: 'Let’s check our accuracy. To decide if an output represents a 3 or a 7, we
    can just check whether it’s greater than 0, so our accuracy for each item can
    be calculated (using broadcasting, so no loops!) as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查我们的准确性。为了确定输出代表3还是7，我们只需检查它是否大于0，因此我们可以计算每个项目的准确性（使用广播，因此没有循环！）如下：
- en: '[PRE120]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'Now let’s see what the change in accuracy is for a small change in one of the
    weights:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一个权重的微小变化对准确性的影响是什么：
- en: '[PRE124]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: As we’ve seen, we need gradients in order to improve our model using SGD, and
    in order to calculate gradients we need a *loss function* that represents how
    good our model is. That is because the gradients are a measure of how that loss
    function changes with small tweaks to the weights.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们需要梯度来通过SGD改进我们的模型，为了计算梯度，我们需要一个*损失函数*，它代表了我们的模型有多好。这是因为梯度是损失函数如何随着对权重的微小调整而变化的度量。
- en: So, we need to choose a loss function. The obvious approach would be to use
    accuracy, which is our metric, as our loss function as well. In this case, we
    would calculate our prediction for each image, collect these values to calculate
    an overall accuracy, and then calculate the gradients of each weight with respect
    to that overall accuracy.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要选择一个损失函数。显而易见的方法是使用准确性作为我们的度量标准，也作为我们的损失函数。在这种情况下，我们将为每个图像计算我们的预测，收集这些值以计算总体准确性，然后计算每个权重相对于总体准确性的梯度。
- en: 'Unfortunately, we have a significant technical problem here. The gradient of
    a function is its *slope*, or its steepness, which can be defined as *rise over
    run*—that is, how much the value of the function goes up or down, divided by how
    much we changed the input. We can write this mathematically as:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们在这里有一个重要的技术问题。函数的梯度是其*斜率*，或者是其陡峭程度，可以定义为*上升与下降*——也就是说，函数值上升或下降的幅度，除以我们改变输入的幅度。我们可以用数学方式写成：
- en: '[PRE127]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: This gives a good approximation of the gradient when `x_new` is very similar
    to `x_old`, meaning that their difference is very small. But accuracy changes
    at all only when a prediction changes from a 3 to a 7, or vice versa. The problem
    is that a small change in weights from `x_old` to `x_new` isn’t likely to cause
    any prediction to change, so `(y_new – y_old)` will almost always be 0. In other
    words, the gradient is 0 almost everywhere.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 当`x_new`非常类似于`x_old`时，这给出了梯度的良好近似，这意味着它们的差异非常小。但是，只有当预测从3变为7，或者反之时，准确性才会发生变化。问题在于，从`x_old`到`x_new`的权重的微小变化不太可能导致任何预测发生变化，因此`(y_new
    - y_old)`几乎总是为0。换句话说，梯度几乎在任何地方都为0。
- en: A very small change in the value of a weight will often not change the accuracy
    at all. This means it is not useful to use accuracy as a loss function—if we do,
    most of the time our gradients will be 0, and the model will not be able to learn
    from that number.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 权重值的微小变化通常不会改变准确性。这意味着使用准确性作为损失函数是没有用的——如果我们这样做，大多数时候我们的梯度将为0，模型将无法从该数字中学习。
- en: Sylvain Says
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sylvain说
- en: In mathematical terms, accuracy is a function that is constant almost everywhere
    (except at the threshold, 0.5), so its derivative is nil almost everywhere (and
    infinity at the threshold). This then gives gradients that are 0 or infinite,
    which are useless for updating the model.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学术语中，准确性是一个几乎在任何地方都是常数的函数（除了阈值0.5），因此它的导数几乎在任何地方都是零（在阈值处为无穷大）。这将导致梯度为0或无穷大，这对于更新模型是没有用的。
- en: Instead, we need a loss function that, when our weights result in slightly better
    predictions, gives us a slightly better loss. So what does a “slightly better
    prediction” look like, exactly? Well, in this case, it means that if the correct
    answer is a 3, the score is a little higher, or if the correct answer is a 7,
    the score is a little lower.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们需要一个损失函数，当我们的权重导致稍微更好的预测时，给出稍微更好的损失。那么，“稍微更好的预测”具体是什么样呢？在这种情况下，这意味着如果正确答案是3，则分数稍高，或者如果正确答案是7，则分数稍低。
- en: Let’s write such a function now. What form does it take?
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写这样一个函数。它是什么形式？
- en: The loss function receives not the images themselves, but the predictions from
    the model. So let’s make one argument, `prds`, of values between 0 and 1, where
    each value is the prediction that an image is a 3\. It is a vector (i.e., a rank-1
    tensor) indexed over the images.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数接收的不是图像本身，而是模型的预测。因此，让我们做一个参数`prds`，值在0和1之间，其中每个值是图像是3的预测。它是一个矢量（即，一个秩-1张量），索引在图像上。
- en: The purpose of the loss function is to measure the difference between predicted
    values and the true values—that is, the targets (aka labels). Let’s therefore
    make another argument, `trgts`, with values of 0 or 1 that tells whether an image
    actually is a 3 or not. It is also a vector (i.e., another rank-1 tensor) indexed
    over the images.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的目的是衡量预测值与真实值之间的差异，即目标（又称标签）。因此，让我们再做一个参数`trgts`，其值为0或1，告诉图像实际上是3还是不是3。它也是一个矢量（即，另一个秩-1张量），索引在图像上。
- en: 'For instance, suppose we had three images that we knew were a 3, a 7, and a
    3\. And suppose our model predicted with high confidence (`0.9`) that the first
    was a 3, with slight confidence (`0.4`) that the second was a 7, and with fair
    confidence (`0.2`), but incorrectly, that the last was a 7\. This would mean our
    loss function would receive these values as its inputs:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有三幅图像，我们知道其中一幅是3，一幅是7，一幅是3。假设我们的模型以高置信度（`0.9`）预测第一幅是3，以轻微置信度（`0.4`）预测第二幅是7，以公平置信度（`0.2`），但是错误地预测最后一幅是7。这意味着我们的损失函数将接收这些值作为其输入：
- en: '[PRE128]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Here’s a first try at a loss function that measures the distance between `predictions`
    and `targets`:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个测量`predictions`和`targets`之间距离的损失函数的第一次尝试：
- en: '[PRE129]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: We’re using a new function, `torch.where(a,b,c)`. This is the same as running
    the list comprehension `[b[i] if a[i] else c[i] for i in range(len(a))]`, except
    it works on tensors, at C/CUDA speed. In plain English, this function will measure
    how distant each prediction is from 1 if it should be 1, and how distant it is
    from 0 if it should be 0, and then it will take the mean of all those distances.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用一个新函数，`torch.where(a,b,c)`。这与运行列表推导`[b[i] if a[i] else c[i] for i in range(len(a))]`相同，只是它在张量上运行，以C/CUDA速度运行。简单来说，这个函数将衡量每个预测离1有多远，如果应该是1的话，以及它离0有多远，如果应该是0的话，然后它将取所有这些距离的平均值。
- en: Read the Docs
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阅读文档
- en: It’s important to learn about PyTorch functions like this, because looping over
    tensors in Python performs at Python speed, not C/CUDA speed! Try running `help(torch.where)`
    now to read the docs for this function, or, better still, look it up on the PyTorch
    documentation site.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 学习PyTorch这样的函数很重要，因为在Python中循环张量的速度是Python速度，而不是C/CUDA速度！现在尝试运行`help(torch.where)`来阅读此函数的文档，或者更好的是，在PyTorch文档站点上查找。
- en: 'Let’s try it on our `prds` and `trgts`:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的`prds`和`trgts`上尝试一下：
- en: '[PRE130]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'You can see that this function returns a lower number when predictions are
    more accurate, when accurate predictions are more confident (higher absolute values),
    and when inaccurate predictions are less confident. In PyTorch, we always assume
    that a lower value of a loss function is better. Since we need a scalar for the
    final loss, `mnist_loss` takes the mean of the previous tensor:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，当预测更准确时，当准确预测更自信时（绝对值更高），以及当不准确预测更不自信时，此函数返回较低的数字。在PyTorch中，我们始终假设损失函数的较低值更好。由于我们需要一个标量作为最终损失，`mnist_loss`取前一个张量的平均值：
- en: '[PRE132]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'For instance, if we change our prediction for the one “false” target from `0.2`
    to `0.8`, the loss will go down, indicating that this is a better prediction:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们将对一个“错误”目标的预测从`0.2`更改为`0.8`，损失将减少，表明这是一个更好的预测：
- en: '[PRE134]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: One problem with `mnist_loss` as currently defined is that it assumes that predictions
    are always between 0 and 1\. We need to ensure, then, that this is actually the
    case! As it happens, there is a function that does exactly that—let’s take a look.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '`mnist_loss`当前定义的一个问题是它假设预测总是在0和1之间。因此，我们需要确保这实际上是这种情况！恰好有一个函数可以做到这一点，让我们来看看。'
- en: Sigmoid
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigmoid
- en: 'The `sigmoid` function always outputs a number between 0 and 1\. It’s defined
    as follows:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid`函数总是输出一个介于0和1之间的数字。它的定义如下：'
- en: '[PRE136]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'PyTorch defines an accelerated version for us, so we don’t really need our
    own. This is an important function in deep learning, since we often want to ensure
    that values are between 0 and 1\. This is what it looks like:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch为我们定义了一个加速版本，所以我们不需要自己的。这是深度学习中一个重要的函数，因为我们经常希望确保数值在0和1之间。它看起来是这样的：
- en: '[PRE137]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '![](Images/dlcf_04in15.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in15.png)'
- en: As you can see, it takes any input value, positive or negative, and smooshes
    it into an output value between 0 and 1\. It’s also a smooth curve that only goes
    up, which makes it easier for SGD to find meaningful gradients.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，它接受任何输入值，正数或负数，并将其压缩为0和1之间的输出值。它还是一个只上升的平滑曲线，这使得SGD更容易找到有意义的梯度。
- en: 'Let’s update `mnist_loss` to first apply `sigmoid` to the inputs:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新`mnist_loss`，首先对输入应用`sigmoid`：
- en: '[PRE138]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: Now we can be confident our loss function will work, even if the predictions
    are not between 0 and 1\. All that is required is that a higher prediction corresponds
    to higher confidence.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以确信我们的损失函数将起作用，即使预测不在0和1之间。唯一需要的是更高的预测对应更高的置信度。
- en: Having defined a loss function, now is a good moment to recapitulate why we
    did this. After all, we already had a metric, which was overall accuracy. So why
    did we define a loss?
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了一个损失函数，现在是一个好时机回顾为什么这样做。毕竟，我们已经有了一个度量标准，即整体准确率。那么为什么我们定义了一个损失？
- en: The key difference is that the metric is to drive human understanding and the
    loss is to drive automated learning. To drive automated learning, the loss must
    be a function that has a meaningful derivative. It can’t have big flat sections
    and large jumps, but instead must be reasonably smooth. This is why we designed
    a loss function that would respond to small changes in confidence level. This
    requirement means that sometimes it does not really reflect exactly what we are
    trying to achieve, but is rather a compromise between our real goal and a function
    that can be optimized using its gradient. The loss function is calculated for
    each item in our dataset, and then at the end of an epoch, the loss values are
    all averaged and the overall mean is reported for the epoch.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 关键区别在于指标用于驱动人类理解，而损失用于驱动自动学习。为了驱动自动学习，损失必须是一个具有有意义导数的函数。它不能有大的平坦部分和大的跳跃，而必须是相当平滑的。这就是为什么我们设计了一个损失函数，可以对置信水平的小变化做出响应。这个要求意味着有时它实际上并不完全反映我们试图实现的目标，而是我们真正目标和一个可以使用其梯度进行优化的函数之间的妥协。损失函数是针对数据集中的每个项目计算的，然后在时代结束时，所有损失值都被平均，整体均值被报告为时代。
- en: Metrics, on the other hand, are the numbers that we care about. These are the
    values that are printed at the end of each epoch that tell us how our model is
    doing. It is important that we learn to focus on these metrics, rather than the
    loss, when judging the performance of a model.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，指标是我们关心的数字。这些是在每个时代结束时打印的值，告诉我们我们的模型表现如何。重要的是，我们学会关注这些指标，而不是损失，来评估模型的性能。
- en: SGD and Mini-Batches
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SGD和小批次
- en: Now that we have a loss function suitable for driving SGD, we can consider some
    of the details involved in the next phase of the learning process, which is to
    change or update the weights based on the gradients. This is called an *optimization
    step*.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个适合驱动SGD的损失函数，我们可以考虑学习过程的下一阶段涉及的一些细节，即根据梯度改变或更新权重。这被称为*优化步骤*。
- en: To take an optimization step, we need to calculate the loss over one or more
    data items. How many should we use? We could calculate it for the whole dataset
    and take the average, or we could calculate it for a single data item. But neither
    of these is ideal. Calculating it for the whole dataset would take a long time.
    Calculating it for a single item would not use much information, so it would result
    in an imprecise and unstable gradient. You’d be going to the trouble of updating
    the weights, but taking into account only how that would improve the model’s performance
    on that single item.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行优化步骤，我们需要计算一个或多个数据项的损失。我们应该使用多少？我们可以为整个数据集计算并取平均值，或者可以为单个数据项计算。但这两种方法都不理想。为整个数据集计算将需要很长时间。为单个数据项计算将不会使用太多信息，因此会导致不精确和不稳定的梯度。您将费力更新权重，但只考虑这将如何改善模型在该单个数据项上的性能。
- en: 'So instead we compromise: we calculate the average loss for a few data items
    at a time. This is called a *mini-batch*. The number of data items in the mini-batch
    is called the *batch size*. A larger batch size means that you will get a more
    accurate and stable estimate of your dataset’s gradients from the loss function,
    but it will take longer, and you will process fewer mini-batches per epoch. Choosing
    a good batch size is one of the decisions you need to make as a deep learning
    practitioner to train your model quickly and accurately. We will talk about how
    to make this choice throughout this book.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们做出妥协：我们一次计算几个数据项的平均损失。这被称为*小批次*。小批次中的数据项数量称为*批次大小*。较大的批次大小意味着您将从损失函数中获得更准确和稳定的数据集梯度估计，但这将需要更长时间，并且您将在每个时代处理较少的小批次。选择一个好的批次大小是您作为深度学习从业者需要做出的决定之一，以便快速准确地训练您的模型。我们将在本书中讨论如何做出这个选择。
- en: Another good reason for using mini-batches rather than calculating the gradient
    on individual data items is that, in practice, we nearly always do our training
    on an accelerator such as a GPU. These accelerators perform well only if they
    have lots of work to do at a time, so it’s helpful if we can give them lots of
    data items to work on. Using mini-batches is one of the best ways to do this.
    However, if you give them too much data to work on at once, they run out of memory—making
    GPUs happy is also tricky!
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小批次而不是在单个数据项上计算梯度的另一个很好的理由是，实际上，我们几乎总是在加速器上进行训练，例如GPU。这些加速器只有在一次有很多工作要做时才能表现良好，因此如果我们可以给它们很多数据项来处理，这将是有帮助的。使用小批次是实现这一目标的最佳方法之一。但是，如果您一次给它们太多数据来处理，它们会耗尽内存——让GPU保持愉快也是棘手的！
- en: As you saw in our discussion of data augmentation in [Chapter 2](ch02.xhtml#chapter_production),
    we get better generalization if we can vary things during training. One simple
    and effective thing we can vary is what data items we put in each mini-batch.
    Rather than simply enumerating our dataset in order for every epoch, instead what
    we normally do is randomly shuffle it on every epoch, before we create mini-batches.
    PyTorch and fastai provide a class that will do the shuffling and mini-batch collation
    for you, called `DataLoader`.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第2章](ch02.xhtml#chapter_production)中关于数据增强的讨论中所看到的，如果我们在训练过程中可以改变一些东西，我们会获得更好的泛化能力。我们可以改变的一个简单而有效的事情是将哪些数据项放入每个小批次。我们通常不是简单地按顺序枚举我们的数据集，而是在每个时代之前随机洗牌，然后创建小批次。PyTorch和fastai提供了一个类，可以为您执行洗牌和小批次整理，称为`DataLoader`。
- en: 'A `DataLoader` can take any Python collection and turn it into an iterator
    over many batches, like so:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataLoader`可以将任何Python集合转换为一个迭代器，用于生成多个批次，就像这样：'
- en: '[PRE139]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'For training a model, we don’t just want any Python collection, but a collection
    containing independent and dependent variables (the inputs and targets of the
    model). A collection that contains tuples of independent and dependent variables
    is known in PyTorch as a `Dataset`. Here’s an example of an extremely simple `Dataset`:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练模型，我们不只是想要任何Python集合，而是一个包含独立和相关变量（模型的输入和目标）的集合。包含独立和相关变量元组的集合在PyTorch中被称为`Dataset`。这是一个极其简单的`Dataset`的示例：
- en: '[PRE141]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'When we pass a `Dataset` to a `DataLoader` we will get back many batches that
    are themselves tuples of tensors representing batches of independent and dependent
    variables:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将`Dataset`传递给`DataLoader`时，我们将得到许多批次，它们本身是表示独立和相关变量批次的张量元组：
- en: '[PRE143]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: We are now ready to write our first training loop for a model using SGD!
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备为使用SGD的模型编写我们的第一个训练循环！
- en: Putting It All Together
  id: totrans-436
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 把所有东西放在一起
- en: 'It’s time to implement the process we saw in [Figure 4-1](#gradient_descent).
    In code, our process will be implemented something like this for each epoch:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候实现我们在[图4-1](#gradient_descent)中看到的过程了。在代码中，我们的过程将为每个时期实现类似于这样的东西：
- en: '[PRE145]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'First, let’s reinitialize our parameters:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们重新初始化我们的参数：
- en: '[PRE146]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'A `DataLoader` can be created from a `Dataset`:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataLoader`可以从`Dataset`创建：'
- en: '[PRE147]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'We’ll do the same for the validation set:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对验证集执行相同的操作：
- en: '[PRE149]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: 'Let’s create a mini-batch of size 4 for testing:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个大小为4的小批量进行测试：
- en: '[PRE150]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: 'Now we can calculate the gradients:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算梯度了：
- en: '[PRE156]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: 'Let’s put that all in a function:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把所有这些放在一个函数中：
- en: '[PRE158]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: 'And test it:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 并测试它：
- en: '[PRE159]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'But look what happens if we call it twice:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 但是看看如果我们调用两次会发生什么：
- en: '[PRE161]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: 'The gradients have changed! The reason for this is that `loss.backward` *adds*
    the gradients of `loss` to any gradients that are currently stored. So, we have
    to set the current gradients to 0 first:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度已经改变了！这是因为`loss.backward` *添加*了`loss`的梯度到当前存储的任何梯度中。因此，我们首先必须将当前梯度设置为0：
- en: '[PRE163]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: In-Place Operations
  id: totrans-466
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原地操作
- en: Methods in PyTorch whose names end in an underscore modify their objects *in
    place*. For instance, `bias.zero_` sets all elements of the tensor `bias` to 0.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch中以下划线结尾的方法会*原地*修改它们的对象。例如，`bias.zero_`会将张量`bias`的所有元素设置为0。
- en: 'Our only remaining step is to update the weights and biases based on the gradient
    and learning rate. When we do so, we have to tell PyTorch not to take the gradient
    of this step too—otherwise, things will get confusing when we try to compute the
    derivative at the next batch! If we assign to the `data` attribute of a tensor,
    PyTorch will not take the gradient of that step. Here’s our basic training loop
    for an epoch:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 我们唯一剩下的步骤是根据梯度和学习率更新权重和偏差。当我们这样做时，我们必须告诉PyTorch不要对这一步骤进行梯度计算，否则当我们尝试在下一个批次计算导数时会变得混乱！如果我们将张量的`data`属性赋值，PyTorch将不会对该步骤进行梯度计算。这是我们用于一个时期的基本训练循环：
- en: '[PRE164]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'We also want to check how we’re doing, by looking at the accuracy of the validation
    set. To decide if an output represents a 3 or a 7, we can just check whether it’s
    greater than 0\. So our accuracy for each item can be calculated (using broadcasting,
    so no loops!) as follows:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还想通过查看验证集的准确性来检查我们的表现。要决定输出是否代表3或7，我们只需检查它是否大于0。因此，我们可以计算每个项目的准确性（使用广播，所以没有循环！）如下：
- en: '[PRE165]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'That gives us this function to calculate our validation accuracy:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们计算验证准确性的这个函数：
- en: '[PRE167]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: 'We can check it works:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查它是否有效：
- en: '[PRE168]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: 'And then put the batches together:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 然后把批次放在一起：
- en: '[PRE170]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: 'That’s our starting point. Let’s train for one epoch and see if the accuracy
    improves:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的起点。让我们训练一个时期，看看准确性是否提高：
- en: '[PRE173]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: 'Then do a few more:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 然后再做几次：
- en: '[PRE175]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: Looking good! We’re already about at the same accuracy as our “pixel similarity”
    approach, and we’ve created a general-purpose foundation we can build on. Our
    next step will be to create an object that will handle the SGD step for us. In
    PyTorch, it’s called an *optimizer*.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！我们的准确性已经接近“像素相似性”方法的准确性，我们已经创建了一个通用的基础可以构建。我们的下一步将是创建一个将处理SGD步骤的对象。在PyTorch中，它被称为*优化器*。
- en: Creating an Optimizer
  id: totrans-489
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个优化器
- en: Because this is such a general foundation, PyTorch provides some useful classes
    to make it easier to implement. The first thing we can do is replace our `linear`
    function with PyTorch’s `nn.Linear` module. A *module* is an object of a class
    that inherits from the PyTorch `nn.Module` class. Objects of this class behave
    identically to standard Python functions, in that you can call them using parentheses,
    and they will return the activations of a model.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个如此通用的基础，PyTorch提供了一些有用的类来使实现更容易。我们可以做的第一件事是用PyTorch的`nn.Linear`模块替换我们的`linear`函数。*模块*是从PyTorch
    `nn.Module`类继承的类的对象。这个类的对象的行为与标准Python函数完全相同，您可以使用括号调用它们，它们将返回模型的激活。
- en: '`nn.Linear` does the same thing as our `init_params` and `linear` together.
    It contains both the *weights* and *biases* in a single class. Here’s how we replicate
    our model from the previous section:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Linear`做的事情与我们的`init_params`和`linear`一样。它包含了*权重*和*偏差*在一个单独的类中。这是我们如何复制上一节中的模型：'
- en: '[PRE177]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: 'Every PyTorch module knows what parameters it has that can be trained; they
    are available through the `parameters` method:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 每个PyTorch模块都知道它有哪些可以训练的参数；它们可以通过`parameters`方法获得：
- en: '[PRE178]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: 'We can use this information to create an optimizer:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些信息创建一个优化器：
- en: '[PRE180]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: 'We can create our optimizer by passing in the model’s parameters:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过传入模型的参数来创建优化器：
- en: '[PRE181]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: 'Our training loop can now be simplified:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练循环现在可以简化：
- en: '[PRE182]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'Our validation function doesn’t need to change at all:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的验证函数不需要任何更改：
- en: '[PRE183]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '[PRE184]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: 'Let’s put our little training loop in a function, to make things simpler:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把我们的小训练循环放在一个函数中，让事情变得更简单：
- en: '[PRE185]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: 'The results are the same as in the previous section:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与上一节相同：
- en: '[PRE186]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: 'fastai provides the `SGD` class that, by default, does the same thing as our
    `BasicOptim`:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: fastai提供了`SGD`类，默认情况下与我们的`BasicOptim`做相同的事情：
- en: '[PRE188]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: 'fastai also provides `Learner.fit`, which we can use instead of `train_model`.
    To create a `Learner`, we first need to create a `DataLoaders`, by passing in
    our training and validation `DataLoader`s:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: fastai还提供了`Learner.fit`，我们可以使用它来代替`train_model`。要创建一个`Learner`，我们首先需要创建一个`DataLoaders`，通过传入我们的训练和验证`DataLoader`：
- en: '[PRE190]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: 'To create a `Learner` without using an application (such as `cnn_learner`),
    we need to pass in all the elements that we’ve created in this chapter: the `DataLoaders`,
    the model, the optimization function (which will be passed the parameters), the
    loss function, and optionally any metrics to print:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个`Learner`而不使用应用程序（如`cnn_learner`），我们需要传入本章中创建的所有元素：`DataLoaders`，模型，优化函数（将传递参数），损失函数，以及可选的任何要打印的指标：
- en: '[PRE191]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: 'Now we can call `fit`:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以调用`fit`：
- en: '[PRE192]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '| epoch | train_loss | valid_loss | batch_accuracy | time |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | batch_accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.636857 | 0.503549 | 0.495584 | 00:00 |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.636857 | 0.503549 | 0.495584 | 00:00 |'
- en: '| 1 | 0.545725 | 0.170281 | 0.866045 | 00:00 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.545725 | 0.170281 | 0.866045 | 00:00 |'
- en: '| 2 | 0.199223 | 0.184893 | 0.831207 | 00:00 |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.199223 | 0.184893 | 0.831207 | 00:00 |'
- en: '| 3 | 0.086580 | 0.107836 | 0.911187 | 00:00 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.086580 | 0.107836 | 0.911187 | 00:00 |'
- en: '| 4 | 0.045185 | 0.078481 | 0.932777 | 00:00 |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.045185 | 0.078481 | 0.932777 | 00:00 |'
- en: '| 5 | 0.029108 | 0.062792 | 0.946516 | 00:00 |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.029108 | 0.062792 | 0.946516 | 00:00 |'
- en: '| 6 | 0.022560 | 0.053017 | 0.955348 | 00:00 |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.022560 | 0.053017 | 0.955348 | 00:00 |'
- en: '| 7 | 0.019687 | 0.046500 | 0.962218 | 00:00 |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.019687 | 0.046500 | 0.962218 | 00:00 |'
- en: '| 8 | 0.018252 | 0.041929 | 0.965162 | 00:00 |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.018252 | 0.041929 | 0.965162 | 00:00 |'
- en: '| 9 | 0.017402 | 0.038573 | 0.967615 | 00:00 |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.017402 | 0.038573 | 0.967615 | 00:00 |'
- en: As you can see, there’s nothing magic about the PyTorch and fastai classes.
    They are just convenient prepackaged pieces that make your life a bit easier!
    (They also provide a lot of extra functionality we’ll be using in future chapters.)
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，PyTorch和fastai类并没有什么神奇之处。它们只是方便的预打包部件，使您的生活变得更轻松！（它们还提供了许多我们将在未来章节中使用的额外功能。）
- en: With these classes, we can now replace our linear model with a neural network.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些类，我们现在可以用神经网络替换我们的线性模型。
- en: Adding a Nonlinearity
  id: totrans-533
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加非线性
- en: 'So far, we have a general procedure for optimizing the parameters of a function,
    and we have tried it out on a boring function: a simple linear classifier. A linear
    classifier is constrained in terms of what it can do. To make it a bit more complex
    (and able to handle more tasks), we need to add something nonlinear (i.e., different
    from ax+b) between two linear classifiers—this is what gives us a neural network.'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经有了一个优化函数的一般过程，并且我们已经在一个无聊的函数上尝试了它：一个简单的线性分类器。线性分类器在能做什么方面受到限制。为了使其更复杂一些（并且能够处理更多任务），我们需要在两个线性分类器之间添加一些非线性（即与ax+b不同的东西）——这就是给我们神经网络的东西。
- en: 'Here is the entire definition of a basic neural network:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基本神经网络的完整定义：
- en: '[PRE193]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: That’s it! All we have in `simple_net` is two linear classifiers with a `max`
    function between them.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！在`simple_net`中，我们只有两个线性分类器，它们之间有一个`max`函数。
- en: 'Here, `w1` and `w2` are weight tensors, and `b1` and `b2` are bias tensors;
    that is, parameters that are initially randomly initialized, just as we did in
    the previous section:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`w1`和`w2`是权重张量，`b1`和`b2`是偏置张量；也就是说，这些参数最初是随机初始化的，就像我们在上一节中所做的一样：
- en: '[PRE194]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: The key point is that `w1` has 30 output activations (which means that `w2`
    must have 30 input activations, so they match). That means that the first layer
    can construct 30 different features, each representing a different mix of pixels.
    You can change that `30` to anything you like, to make the model more or less
    complex.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点是`w1`有30个输出激活（这意味着`w2`必须有30个输入激活，以便匹配）。这意味着第一层可以构建30个不同的特征，每个特征代表不同的像素混合。您可以将`30`更改为任何您喜欢的数字，以使模型更复杂或更简单。
- en: 'That little function `res.max(tensor(0.0))` is called a *rectified linear unit*,
    also known as *ReLU*. We think we can all agree that *rectified linear unit* sounds
    pretty fancy and complicated…But actually, there’s nothing more to it than `res.max(tensor(0.0))`—in
    other words, replace every negative number with a zero. This tiny function is
    also available in PyTorch as `F.relu`:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 那个小函数`res.max(tensor(0.0))`被称为*修正线性单元*，也被称为*ReLU*。我们认为我们都可以同意*修正线性单元*听起来相当花哨和复杂...但实际上，它不过是`res.max(tensor(0.0))`——换句话说，用零替换每个负数。这个微小的函数在PyTorch中也可以作为`F.relu`使用：
- en: '[PRE195]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '![](Images/dlcf_04in16.png)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in16.png)'
- en: Jeremy Says
  id: totrans-544
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jeremy说
- en: There is an enormous amount of jargon in deep learning, including terms like
    *rectified linear unit*. The vast majority of this jargon is no more complicated
    than can be implemented in a short line of code, as we saw in this example. The
    reality is that for academics to get their papers published, they need to make
    them sound as impressive and sophisticated as possible. One way that they do that
    is to introduce jargon. Unfortunately, this results in the field becoming far
    more intimidating and difficult to get into than it should be. You do have to
    learn the jargon, because otherwise papers and tutorials are not going to mean
    much to you. But that doesn’t mean you have to find the jargon intimidating. Just
    remember, when you come across a word or phrase that you haven’t seen before,
    it will almost certainly turn out to be referring to a very simple concept.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中有大量行话，包括*修正线性单元*等术语。绝大多数这些行话并不比我们在这个例子中看到的一行代码更复杂。事实是，学术界为了发表论文，他们需要让论文听起来尽可能令人印象深刻和复杂。他们通过引入行话来实现这一点。不幸的是，这导致该领域变得比应该更加令人生畏和难以进入。您确实需要学习这些行话，因为否则论文和教程对您来说将毫无意义。但这并不意味着您必须觉得这些行话令人生畏。只需记住，当您遇到以前未见过的单词或短语时，它几乎肯定是指一个非常简单的概念。
- en: The basic idea is that by using more linear layers, we can have our model do
    more computation, and therefore model more complex functions. But there’s no point
    in just putting one linear layout directly after another one, because when we
    multiply things together and then add them up multiple times, that could be replaced
    by multiplying different things together and adding them up just once! That is
    to say, a series of any number of linear layers in a row can be replaced with
    a single linear layer with a different set of parameters.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是通过使用更多的线性层，我们的模型可以进行更多的计算，从而模拟更复杂的函数。但是，直接将一个线性布局放在另一个线性布局之后是没有意义的，因为当我们将事物相乘然后多次相加时，可以用不同的事物相乘然后只相加一次来替代！也就是说，一系列任意数量的线性层可以被替换为具有不同参数集的单个线性层。
- en: But if we put a nonlinear function between them, such as `max`, this is no longer
    true. Now each linear layer is somewhat decoupled from the other ones and can
    do its own useful work. The `max` function is particularly interesting, because
    it operates as a simple `if` statement.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们在它们之间放置一个非线性函数，比如`max`，这就不再成立了。现在每个线性层都有点解耦，可以做自己有用的工作。`max`函数特别有趣，因为它作为一个简单的`if`语句运行。
- en: Sylvain Says
  id: totrans-548
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sylvain说
- en: Mathematically, we say the composition of two linear functions is another linear
    function. So, we can stack as many linear classifiers as we want on top of each
    other, and without nonlinear functions between them, it will just be the same
    as one linear classifier.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，我们说两个线性函数的组合是另一个线性函数。因此，我们可以堆叠任意多个线性分类器在一起，而它们之间没有非线性函数，这将与一个线性分类器相同。
- en: Amazingly enough, it can be mathematically proven that this little function
    can solve any computable problem to an arbitrarily high level of accuracy, if
    you can find the right parameters for `w1` and `w2` and if you make these matrices
    big enough. For any arbitrarily wiggly function, we can approximate it as a bunch
    of lines joined together; to make it closer to the wiggly function, we just have
    to use shorter lines. This is known as the *universal approximation theorem*.
    The three lines of code that we have here are known as *layers*. The first and
    third are known as *linear layers*, and the second line of code is known variously
    as a *nonlinearity*, or *activation function*.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，可以数学证明这个小函数可以解决任何可计算问题，只要你能找到`w1`和`w2`的正确参数，并且使这些矩阵足够大。对于任何任意波动的函数，我们可以将其近似为一堆连接在一起的线条；为了使其更接近波动函数，我们只需使用更短的线条。这被称为*通用逼近定理*。我们这里的三行代码被称为*层*。第一和第三行被称为*线性层*，第二行代码被称为*非线性*或*激活函数*。
- en: 'Just as in the previous section, we can replace this code with something a
    bit simpler by taking advantage of PyTorch:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在前一节中一样，我们可以利用PyTorch简化这段代码：
- en: '[PRE196]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '`nn.Sequential` creates a module that will call each of the listed layers or
    functions in turn.'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Sequential`创建一个模块，依次调用列出的每个层或函数。'
- en: '`nn.ReLU` is a PyTorch module that does exactly the same thing as the `F.relu`
    function. Most functions that can appear in a model also have identical forms
    that are modules. Generally, it’s just a case of replacing `F` with `nn` and changing
    the capitalization. When using `nn.Sequential`, PyTorch requires us to use the
    module version. Since modules are classes, we have to instantiate them, which
    is why you see `nn.ReLU` in this example.'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.ReLU`是一个PyTorch模块，与`F.relu`函数完全相同。大多数可以出现在模型中的函数也有相同的模块形式。通常，只需将`F`替换为`nn`并更改大小写。在使用`nn.Sequential`时，PyTorch要求我们使用模块版本。由于模块是类，我们必须实例化它们，这就是为什么在这个例子中看到`nn.ReLU`。'
- en: 'Because `nn.Sequential` is a module, we can get its parameters, which will
    return a list of all the parameters of all the modules it contains. Let’s try
    it out! As this is a deeper model, we’ll use a lower learning rate and a few more
    epochs:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`nn.Sequential`是一个模块，我们可以获取它的参数，它将返回它包含的所有模块的所有参数的列表。让我们试一试！由于这是一个更深层的模型，我们将使用更低的学习率和更多的周期：
- en: '[PRE197]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: 'We’re not showing the 40 lines of output here to save room; the training process
    is recorded in `learn.recorder`, with the table of output stored in the `values`
    attribute, so we can plot the accuracy over training:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里不展示40行输出，以节省空间；训练过程记录在`learn.recorder`中，输出表存储在`values`属性中，因此我们可以绘制训练过程中的准确性：
- en: '[PRE199]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '![](Images/dlcf_04in17.png)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_04in17.png)'
- en: 'And we can view the final accuracy:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看最终的准确性：
- en: '[PRE200]'
  id: totrans-562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: 'At this point, we have something that is rather magical:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们有一些非常神奇的东西：
- en: A function that can solve any problem to any level of accuracy (the neural network)
    given the correct set of parameters
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定正确的参数集，可以解决任何问题到任何精度的函数（神经网络）
- en: A way to find the best set of parameters for any function (stochastic gradient
    descent)
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到任何函数的最佳参数集的方法（随机梯度下降）
- en: 'This is why deep learning can do such fantastic things. Believing that this
    combination of simple techniques can really solve any problem is one of the biggest
    steps that we find many students have to take. It seems too good to be true—surely
    things should be more difficult and complicated than this? Our recommendation:
    try it out! We just tried it on the MNIST dataset, and you’ve seen the results.
    And since we are doing everything from scratch ourselves (except for calculating
    the gradients), you know that there is no special magic hiding behind the scenes.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么深度学习可以做出如此奇妙的事情。相信这些简单技术的组合确实可以解决任何问题是我们发现许多学生必须迈出的最大步骤之一。这似乎太好了，以至于难以置信——事情肯定应该比这更困难和复杂吧？我们的建议是：试一试！我们刚刚在MNIST数据集上尝试了一下，你已经看到了结果。由于我们自己从头开始做所有事情（除了计算梯度），所以你知道背后没有隐藏任何特殊的魔法。
- en: Going Deeper
  id: totrans-568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更深入地探讨
- en: There is no need to stop at just two linear layers. We can add as many as we
    want, as long as we add a nonlinearity between each pair of linear layers. As
    you will learn, however, the deeper the model gets, the harder it is to optimize
    the parameters in practice. Later in this book, you will learn about some simple
    but brilliantly effective techniques for training deeper models.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必止步于只有两个线性层。我们可以添加任意数量的线性层，只要在每对线性层之间添加一个非线性。然而，正如您将了解的那样，模型变得越深，实际中优化参数就越困难。在本书的后面，您将学习一些简单但非常有效的训练更深层模型的技巧。
- en: We already know that a single nonlinearity with two linear layers is enough
    to approximate any function. So why would we use deeper models? The reason is
    performance. With a deeper model (one with more layers), we do not need to use
    as many parameters; it turns out that we can use smaller matrices, with more layers,
    and get better results than we would get with larger matrices and few layers.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，一个带有两个线性层的单个非线性足以逼近任何函数。那么为什么要使用更深的模型呢？原因是性能。通过更深的模型（具有更多层），我们不需要使用太多参数；事实证明，我们可以使用更小的矩阵，更多的层，获得比使用更大的矩阵和少量层获得更好的结果。
- en: That means that we can train the model more quickly, and it will take up less
    memory. In the 1990s, researchers were so focused on the universal approximation
    theorem that few were experimenting with more than one nonlinearity. This theoretical
    but not practical foundation held back the field for years. Some researchers,
    however, did experiment with deep models, and eventually were able to show that
    these models could perform much better in practice. Eventually, theoretical results
    were developed that showed why this happens. Today, it is extremely unusual to
    find anybody using a neural network with just one nonlinearity.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以更快地训练模型，并且它将占用更少的内存。在1990年代，研究人员如此专注于通用逼近定理，以至于很少有人尝试超过一个非线性。这种理论但不实际的基础阻碍了该领域多年。然而，一些研究人员确实尝试了深度模型，并最终能够证明这些模型在实践中表现得更好。最终，出现了理论结果，解释了为什么会发生这种情况。今天，几乎不可能找到任何人只使用一个非线性的神经网络。
- en: 'Here is what happens when we train an 18-layer model using the same approach
    we saw in [Chapter 1](ch01.xhtml#chapter_intro):'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用与我们在[第1章](ch01.xhtml#chapter_intro)中看到的相同方法训练一个18层模型时会发生什么：
- en: '[PRE202]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '|时代|训练损失|验证损失|准确性|时间|'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.082089 | 0.009578 | 0.997056 | 00:11 |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.082089 | 0.009578 | 0.997056 | 00:11 |'
- en: Nearly 100% accuracy! That’s a big difference compared to our simple neural
    net. But as you’ll learn in the remainder of this book, there are just a few little
    tricks you need to use to get such great results from scratch yourself. You already
    know the key foundational pieces. (Of course, even when you know all the tricks,
    you’ll nearly always want to work with the prebuilt classes provided by PyTorch
    and fastai, because they save you from having to think about all the little details
    yourself.)
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 近乎100%的准确性！这与我们简单的神经网络相比有很大的差异。但是在本书的剩余部分中，您将学习到一些小技巧，可以让您自己从头开始获得如此出色的结果。您已经了解了关键的基础知识。
    （当然，即使您知道所有技巧，您几乎总是希望使用PyTorch和fastai提供的预构建类，因为它们可以帮助您省去自己考虑所有细节的麻烦。）
- en: Jargon Recap
  id: totrans-578
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语回顾
- en: 'Congratulations: you now know how to create and train a deep neural network
    from scratch! We’ve gone through quite a few steps to get to this point, but you
    might be surprised at how simple it really is.'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜：您现在知道如何从头开始创建和训练深度神经网络了！我们经历了很多步骤才达到这一点，但您可能会惊讶于它实际上是多么简单。
- en: Now that we are at this point, it is a good opportunity to define, and review,
    some jargon and key concepts.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经到了这一点，现在是一个很好的机会来定义和回顾一些术语和关键概念。
- en: 'A neural network contains a lot of numbers, but they are only of two types:
    numbers that are calculated, and the parameters that these numbers are calculated
    from. This gives us the two most important pieces of jargon to learn:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络包含很多数字，但它们只有两种类型：计算的数字和这些数字计算出的参数。这给我们学习最重要的两个术语：
- en: Activations
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 激活
- en: Numbers that are calculated (both by linear and nonlinear layers)
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 计算的数字（线性和非线性层）
- en: Parameters
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: Numbers that are randomly initialized, and optimized (that is, the numbers that
    define the model)
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 随机初始化并优化的数字（即定义模型的数字）
- en: We will often talk in this book about activations and parameters. Remember that
    they have specific meanings. They are numbers. They are not abstract concepts,
    but they are actual specific numbers that are in your model. Part of becoming
    a good deep learning practitioner is getting used to the idea of looking at your
    activations and parameters, and plotting them and testing whether they are behaving
    correctly.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们经常谈论激活和参数。请记住它们具有特定的含义。它们是数字。它们不是抽象概念，而是实际存在于您的模型中的具体数字。成为一名优秀的深度学习从业者的一部分是习惯于查看您的激活和参数，并绘制它们以及测试它们是否正确运行的想法。
- en: 'Our activations and parameters are all contained in *tensors*. These are simply
    regularly shaped arrays—for example, a matrix. Matrices have rows and columns;
    we call these the *axes* or *dimensions*. The number of dimensions of a tensor
    is its *rank*. There are some special tensors:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的激活和参数都包含在 *张量* 中。这些只是正规形状的数组—例如，一个矩阵。矩阵有行和列；我们称这些为 *轴* 或 *维度*。张量的维度数是它的 *等级*。有一些特殊的张量：
- en: 'Rank-0: scalar'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等级-0：标量
- en: 'Rank-1: vector'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等级-1：向量
- en: 'Rank-2: matrix'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等级-2：矩阵
- en: A neural network contains a number of layers. Each layer is either *linear*
    or *nonlinear*. We generally alternate between these two kinds of layers in a
    neural network. Sometimes people refer to both a linear layer and its subsequent
    nonlinearity together as a single layer. Yes, this is confusing. Sometimes a nonlinearity
    is referred to as an *activation function*.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络包含多个层。每一层都是*线性*或*非线性*的。我们通常在神经网络中交替使用这两种类型的层。有时人们将线性层及其后续的非线性一起称为一个单独的层。是的，这很令人困惑。有时非线性被称为*激活函数*。
- en: '[Table 4-1](#dljargon1) summarizes the key concepts related to SGD.'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4-1](#dljargon1)总结了与SGD相关的关键概念。'
- en: Table 4-1\. Deep learning vocabulary
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1\. 深度学习词汇表
- en: '| Term | Meaning |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| 术语 | 意义 |'
- en: '| --- | --- |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ReLU | Function that returns 0 for negative numbers and doesn’t change positive
    numbers. |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| ReLU | 对负数返回0且不改变正数的函数。 |'
- en: '| Mini-batch | A small group of inputs and labels gathered together in two
    arrays. A gradient descent step is updated on this batch (rather than a whole
    epoch). |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| 小批量 | 一小组输入和标签，聚集在两个数组中。在这个批次上更新梯度下降步骤（而不是整个epoch）。 |'
- en: '| Forward pass | Applying the model to some input and computing the predictions.
    |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| 前向传播 | 将模型应用于某些输入并计算预测。 |'
- en: '| Loss | A value that represents how well (or badly) our model is doing. |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| 损失 | 代表我们的模型表现如何（好或坏）的值。 |'
- en: '| Gradient | The derivative of the loss with respect to some parameter of the
    model. |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| 梯度 | 损失相对于模型某个参数的导数。 |'
- en: '| Backward pass | Computing the gradients of the loss with respect to all model
    parameters. |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| 反向传播 | 计算损失相对于所有模型参数的梯度。 |'
- en: '| Gradient descent | Taking a step in the direction opposite to the gradients
    to make the model parameters a little bit better. |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| 梯度下降 | 沿着梯度相反方向迈出一步，使模型参数稍微变得更好。 |'
- en: '| Learning rate | The size of the step we take when applying SGD to update
    the parameters of the model. |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 当应用SGD更新模型参数时我们所采取的步骤的大小。 |'
- en: '*Choose Your Own Adventure* Reminder'
  id: totrans-604
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*选择你的冒险* 提醒'
- en: Did you choose to skip over Chapters [2](ch02.xhtml#chapter_production) and
    [3](ch03.xhtml#chapter_ethics), in your excitement to peek under the hood? Well,
    here’s your reminder to head back to [Chapter 2](ch02.xhtml#chapter_production)
    now, because you’ll be needing to know that stuff soon!
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 在你兴奋地想要窥探内部机制时，你选择跳过第[2](ch02.xhtml#chapter_production)和第[3](ch03.xhtml#chapter_ethics)章节了吗？好吧，这里提醒你现在回到[第2章](ch02.xhtml#chapter_production)，因为你很快就会需要了解那些内容！
- en: Questionnaire
  id: totrans-606
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问卷调查
- en: How is a grayscale image represented on a computer? How about a color image?
  id: totrans-607
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 灰度图像在计算机上是如何表示的？彩色图像呢？
- en: How are the files and folders in the `MNIST_SAMPLE` dataset structured? Why?
  id: totrans-608
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MNIST_SAMPLE`数据集中的文件和文件夹是如何结构化的？为什么？'
- en: Explain how the “pixel similarity” approach to classifying digits works.
  id: totrans-609
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释“像素相似性”方法如何工作以对数字进行分类。
- en: What is a list comprehension? Create one now that selects odd numbers from a
    list and doubles them.
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是列表推导？现在创建一个从列表中选择奇数并将其加倍的列表推导。
- en: What is a rank-3 tensor?
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是秩-3张量？
- en: What is the difference between tensor rank and shape? How do you get the rank
    from the shape?
  id: totrans-612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 张量秩和形状之间有什么区别？如何从形状中获取秩？
- en: What are RMSE and L1 norm?
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RMSE和L1范数是什么？
- en: How can you apply a calculation on thousands of numbers at once, many thousands
    of times faster than a Python loop?
  id: totrans-614
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何才能比Python循环快几千倍地一次性对数千个数字进行计算？
- en: Create a 3×3 tensor or array containing the numbers from 1 to 9\. Double it.
    Select the bottom-right four numbers.
  id: totrans-615
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含从1到9的数字的3×3张量或数组。将其加倍。选择右下角的四个数字。
- en: What is broadcasting?
  id: totrans-616
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 广播是什么？
- en: Are metrics generally calculated using the training set or the validation set?
    Why?
  id: totrans-617
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 度量通常是使用训练集还是验证集计算的？为什么？
- en: What is SGD?
  id: totrans-618
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SGD是什么？
- en: Why does SGD use mini-batches?
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么SGD使用小批量？
- en: What are the seven steps in SGD for machine learning?
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SGD在机器学习中有哪七个步骤？
- en: How do we initialize the weights in a model?
  id: totrans-621
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何初始化模型中的权重？
- en: What is loss?
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是损失？
- en: Why can’t we always use a high learning rate?
  id: totrans-623
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们不能总是使用高学习率？
- en: What is a gradient?
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是梯度？
- en: Do you need to know how to calculate gradients yourself?
  id: totrans-625
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要知道如何自己计算梯度吗？
- en: Why can’t we use accuracy as a loss function?
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们不能将准确率作为损失函数使用？
- en: Draw the sigmoid function. What is special about its shape?
  id: totrans-627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制Sigmoid函数。它的形状有什么特别之处？
- en: What is the difference between a loss function and a metric?
  id: totrans-628
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数和度量之间有什么区别？
- en: What is the function to calculate new weights using a learning rate?
  id: totrans-629
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用学习率计算新权重的函数是什么？
- en: What does the `DataLoader` class do?
  id: totrans-630
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataLoader`类是做什么的？'
- en: Write pseudocode showing the basic steps taken in each epoch for SGD.
  id: totrans-631
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写伪代码，显示每个epoch中SGD所采取的基本步骤。
- en: Create a function that, if passed two arguments `[1,2,3,4]` and `'abcd'`, returns
    `[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]`. What is special about that output
    data structure?
  id: totrans-632
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，如果传递两个参数`[1,2,3,4]`和`'abcd'`，则返回`[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]`。该输出数据结构有什么特别之处？
- en: What does `view` do in PyTorch?
  id: totrans-633
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyTorch中的`view`是做什么的？
- en: What are the bias parameters in a neural network? Why do we need them?
  id: totrans-634
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络中的偏差参数是什么？我们为什么需要它们？
- en: What does the `@` operator do in Python?
  id: totrans-635
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Python中的`@`运算符是做什么的？
- en: What does the `backward` method do?
  id: totrans-636
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`backward`方法是做什么的？'
- en: Why do we have to zero the gradients?
  id: totrans-637
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们必须将梯度清零？
- en: What information do we have to pass to `Learner`?
  id: totrans-638
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要向`Learner`传递什么信息？
- en: Show Python or pseudocode for the basic steps of a training loop.
  id: totrans-639
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展示训练循环的基本步骤的Python或伪代码。
- en: What is ReLU? Draw a plot of it for values from `-2` to `+2`.
  id: totrans-640
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ReLU是什么？为值从`-2`到`+2`绘制一个图。
- en: What is an activation function?
  id: totrans-641
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是激活函数？
- en: What’s the difference between `F.relu` and `nn.ReLU`?
  id: totrans-642
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`F.relu`和`nn.ReLU`之间有什么区别？'
- en: The universal approximation theorem shows that any function can be approximated
    as closely as needed using just one nonlinearity. So why do we normally use more?
  id: totrans-643
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通用逼近定理表明，任何函数都可以使用一个非线性逼近得到所需的精度。那么为什么我们通常使用更多的非线性函数？
- en: Further Research
  id: totrans-644
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究
- en: Create your own implementation of `Learner` from scratch, based on the training
    loop shown in this chapter.
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始创建自己的`Learner`实现，基于本章展示的训练循环。
- en: Complete all the steps in this chapter using the full MNIST datasets (for all
    digits, not just 3s and 7s). This is a significant project and will take you quite
    a bit of time to complete! You’ll need to do some of your own research to figure
    out how to overcome obstacles you’ll meet on the way.
  id: totrans-646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用完整的MNIST数据集完成本章的所有步骤（不仅仅是3和7）。这是一个重要的项目，需要花费相当多的时间来完成！您需要进行一些研究，以找出如何克服在途中遇到的障碍。
