- en: 'Chapter 6\. Maximizing Speed and Performance of TensorFlow: A Handy Checklist'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。最大化TensorFlow的速度和性能：一个便捷清单
- en: Life is all about making do with what we have, and optimization is the name
    of the game.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 生活就是要用手头的资源做到最好，优化就是游戏的名字。
- en: It’s not about having everything—it’s about using your resources wisely. Maybe
    we really want to buy that Ferrari, but our budget allows for a Toyota. You know
    what, though? With the right kinds of performance tuning, we can make that bad
    boy race at NASCAR!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 关键不在于拥有一切，而在于明智地利用你的资源。也许我们真的想买那辆法拉利，但我们的预算只够买一辆丰田。不过你知道吗？通过正确的性能调优，我们可以让那家伙在纳斯卡比赛中飞驰！
- en: Let’s look at this in terms of the deep learning world. Google, with its engineering
    might and TPU pods capable of boiling the ocean, set a speed record by training
    ImageNet in just about 30 minutes! And yet, just a few months later, a ragtag
    team of three researchers (Andrew Shaw, Yaroslav Bulatov, and Jeremy Howard),
    with $40 in their pockets using a public cloud, were able to train ImageNet in
    only 18 minutes!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从深度学习世界来看这个问题。谷歌凭借其工程实力和能够煮沸海洋的TPU机架，在约30分钟内训练ImageNet创下了速度纪录！然而，仅仅几个月后，三名研究人员（Andrew
    Shaw、Yaroslav Bulatov和Jeremy Howard）带着口袋里的40美元，在公共云上只用了18分钟就训练完了ImageNet！
- en: The lesson we can draw from these examples is that the amount of resources that
    you have is not nearly as important as using them to their maximum potential.
    It’s all about doing more with less. In that spirit, this chapter is meant to
    serve as a handy checklist of potential performance optimizations that we can
    make when building all stages of the deep learning pipelines, and will be useful
    throughout the book. Specifically, we will discuss optimizations related to data
    preparation, data reading, data augmentation, training, and finally inference.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这些例子中得出的教训是，你拥有的资源量并不像你充分利用它们那样重要。关键在于用最大潜力来充分利用资源。这一章旨在作为一个潜在性能优化的便捷清单，我们在构建深度学习流水线的所有阶段都可以使用，并且在整本书中都会很有用。具体来说，我们将讨论与数据准备、数据读取、数据增强、训练以及最终推断相关的优化。
- en: And the story starts and ends with two words...
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 而这个故事始于两个词，也终于两个词...
- en: GPU Starvation
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU饥饿
- en: A commonly asked question by AI practitioners is, “Why is my training so slow?”
    The answer more often than not is GPU starvation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能从业者经常问的一个问题是，“为什么我的训练速度这么慢？”答案往往是GPU饥饿。
- en: GPUs are the lifelines of deep learning. They can also be the most expensive
    component in a computer system. In light of that, we want to fully utilize them.
    This means that a GPU should not need to wait for data to be available from other
    components for processing. Rather, when the GPU is ready to process, the preprocessed
    data should already be available at its doorstep and ready to go. Yet, the reality
    is that the CPU, memory, and storage are frequently the performance bottlenecks,
    resulting in suboptimal utilization of the GPU. In other words, we want the GPU
    to be the bottleneck, not the other way round.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: GPU是深度学习的生命线。它们也可能是计算机系统中最昂贵的组件。鉴于此，我们希望充分利用它们。这意味着GPU不应该等待来自其他组件的数据以进行处理。相反，当GPU准备好处理时，预处理的数据应该已经准备就绪并等待使用。然而，现实是CPU、内存和存储通常是性能瓶颈，导致GPU的利用率不佳。换句话说，我们希望GPU成为瓶颈，而不是反过来。
- en: Buying expensive GPUs for thousands of dollars can be worthwhile, but only if
    the GPU is the bottleneck to begin with. Otherwise, we might as well burn the
    cash.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为数千美元购买昂贵的GPU可能是值得的，但前提是GPU本身就是瓶颈。否则，我们可能还不如把钱烧了。
- en: To illustrate this better, consider [Figure 6-1](part0008.html#gpu_starvationcomma_while_waiting_for_cp).
    In a deep learning pipeline, the CPU and GPU work in collaboration, passing data
    to each other. The CPU reads the data, performs preprocessing steps including
    augmentations, and then passes it on to the GPU for training. Their collaboration
    is like a relay race, except one of the relay runners is an Olympic athlete, waiting
    for a high school track runner to pass the baton. The more time the GPU stays
    idle, the more wasted resources.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地说明这一点，考虑[图6-1](part0008.html#gpu_starvationcomma_while_waiting_for_cp)。在深度学习流水线中，CPU和GPU协作工作，彼此传递数据。CPU读取数据，执行包括增强在内的预处理步骤，然后将其传递给GPU进行训练。它们的合作就像接力比赛，只不过其中一个接力选手是奥运会运动员，等待着一个高中田径选手传递接力棒。GPU空闲的时间越长，资源浪费就越多。
- en: '![GPU starvation, while waiting for CPU to finish preparing the data](../images/00112.jpeg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![GPU饥饿，等待CPU完成数据准备](../images/00112.jpeg)'
- en: Figure 6-1\. GPU starvation, while waiting for CPU to finish preparing the data
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. GPU饥饿，等待CPU完成数据准备
- en: A large portion of this chapter is devoted to reducing the idle time of the
    GPU and the CPU.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的大部分内容致力于减少GPU和CPU的空闲时间。
- en: 'A logical question to ask is: how do we know whether the GPU is starving? Two
    handy tools can help us answer this question:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个合理的问题是：我们如何知道GPU是否饥饿？两个方便的工具可以帮助我们回答这个问题：
- en: '`nvidia-smi`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvidia-smi`'
- en: This command shows GPU statistics including utilization.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令显示GPU的统计信息，包括利用率。
- en: TensorFlow Profiler + TensorBoard
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Profiler + TensorBoard
- en: This visualizes program execution interactively in a timeline within TensorBoard.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这在TensorBoard中以时间线的形式交互式地可视化程序执行。
- en: nvidia-smi
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: nvidia-smi
- en: Short for NVIDIA System Management Interface program, `nvidia-smi` provides
    detailed statistics about our precious GPUs, including memory, utilization, temperature,
    power wattage, and more. It’s a geek’s dream come true.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvidia-smi`的全称是NVIDIA系统管理接口程序，提供了关于我们珍贵GPU的详细统计信息，包括内存、利用率、温度、功耗等。这对于极客来说是一个梦想成真。'
- en: 'Let’s take it for a test drive:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来试一试：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Figure 6-2](part0008.html#terminal_output_of_nvidia-smi_highlighti) shows
    the result.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-2](part0008.html#terminal_output_of_nvidia-smi_highlighti)展示了结果。'
- en: '![Terminal output of nvidia-smi highlighting the GPU utilization](../images/00217.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![nvidia-smi的终端输出，突出显示GPU利用率](../images/00217.jpeg)'
- en: Figure 6-2\. Terminal output of `nvidia-smi` highlighting the GPU utilization
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2。`nvidia-smi`的终端输出，突出显示GPU利用率
- en: 'While training a network, the key figure we are interested in is the GPU utilization,
    defined in the documentation as the percent of time over the past second during
    which *one or more* kernels was executing on the GPU. Fifty-one percent is frankly
    not that great. But this is utilization at the moment in time when `nvidia-smi`
    is called. How do we continuously monitor these numbers? To better understand
    the GPU usage, we can refresh the utilization metrics every half a second with
    the `watch` command (it’s worth memorizing this command):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练网络时，我们感兴趣的关键数字是GPU利用率，文档中定义为过去一秒钟内GPU上执行*一个或多个*内核的时间百分比。51%实际上并不那么好。但这是在调用`nvidia-smi`时的瞬间利用率。我们如何持续监控这些数字？为了更好地了解GPU使用情况，我们可以使用`watch`命令每半秒刷新一次利用率指标（值得记住这个命令）：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although GPU utilization is a good proxy for measuring the efficiency of our
    pipeline, it does not alone measure how well we’re using the GPU, because the
    work could still be using a small fraction of the GPU’s resources.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPU利用率是衡量我们流水线效率的一个很好的代理，但它本身并不能衡量我们如何充分利用GPU，因为工作仍可能只使用GPU资源的一小部分。
- en: 'Because staring at the terminal screen with the number jumping around is not
    the most optimal way to analyze, we can instead poll the GPU utilization every
    second and dump that into a file. Run this for about 30 seconds while any GPU-related
    process is running on our system and stop it by pressing Ctrl+C:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因为盯着终端屏幕看数字跳动并不是分析的最佳方式，我们可以每秒轮询一次GPU利用率并将其转储到文件中。在我们的系统上运行任何与GPU相关的进程时运行大约30秒，然后通过按Ctrl+C停止它：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, calculate the median GPU utilization from the file generated:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从生成的文件中计算中位数GPU利用率：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Tip
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Datamash is a handy command-line tool that performs basic numeric, textual,
    and statistical operations on textual data files. You can find instructions to
    install it at [*https://www.gnu.org/software/datamash/*](https://www.gnu.org/software/datamash/).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Datamash是一个方便的命令行工具，可对文本数据文件执行基本的数字、文本和统计操作。您可以在[*https://www.gnu.org/software/datamash/*](https://www.gnu.org/software/datamash/)找到安装说明。
- en: '`nvidia-smi` is the most convenient way to check our GPU utilization on the
    command line. Could we get a deeper analysis? It turns out, for advanced users,
    TensorFlow provides a powerful set of tools.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvidia-smi`是在命令行上检查GPU利用率的最便捷方式。我们能否进行更深入的分析？原来，对于高级用户，TensorFlow提供了一套强大的工具。'
- en: TensorFlow Profiler + TensorBoard
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow分析器 + TensorBoard
- en: TensorFlow ships with `tfprof` ([Figure 6-3](part0008.html#profilerapostrophes_timeline_in_tensorbo)),
    the TensorFlow profiler to help analyze and understand the training process at
    a much deeper level, such as generating a detailed model analysis report for each
    operation in our model. But the command line can be a bit daunting to navigate.
    Luckily, TensorBoard, a suite of browser-based visualization tools for TensorFlow,
    includes a plugin for the profiler that lets us interactively debug the network
    with a few mouse clicks. This includes Trace Viewer, a feature that shows events
    in a timeline. It helps investigate how resources are being used precisely at
    a given period of time and spot inefficiencies.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow附带了`tfprof`（[图6-3](part0008.html#profilerapostrophes_timeline_in_tensorbo)），TensorFlow分析器，帮助分析和理解训练过程，例如为模型中的每个操作生成详细的模型分析报告。但是命令行可能有点难以导航。幸运的是，TensorBoard是一个基于浏览器的TensorFlow可视化工具套件，包括一个用于分析器的插件，让我们可以通过几次鼠标点击与网络进行交互式调试。其中包括Trace
    Viewer，一个显示时间轴上事件的功能。它有助于调查资源在特定时间段内的精确使用情况并发现效率低下的地方。
- en: Note
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As of this writing, TensorBoard is fully supported only in Google Chrome and
    might not show the profile view in other browsers, like Firefox.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前为止，TensorBoard仅在Google Chrome中得到完全支持，可能不会在其他浏览器（如Firefox）中显示分析视图。
- en: '![Profiler’s timeline in TensorBoard shows an idle GPU while the CPU is processing
    as well as CPU idling while the GPU is processing](../images/00276.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![TensorBoard中分析器的时间轴显示GPU处于空闲状态，而CPU正在处理，以及CPU处于空闲状态而GPU正在处理](../images/00276.jpeg)'
- en: Figure 6-3\. Profiler’s timeline in TensorBoard shows an idle GPU while the
    CPU is processing as well as CPU idling while the GPU is processing
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3。TensorBoard中分析器的时间轴显示GPU处于空闲状态，而CPU正在处理，以及CPU处于空闲状态而GPU正在处理
- en: 'TensorBoard, by default, has the profiler enabled. Activating TensorBoard involves
    a simple callback function:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，TensorBoard启用了分析器。激活TensorBoard涉及一个简单的回调函数：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When initializing the callback, unless `profile_batch` is explicitly specified,
    it profiles the second batch. Why the second batch? Because the first batch is
    usually slower than the rest due to some initialization overhead.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化回调时，除非显式指定`profile_batch`，否则会对第二批进行分析。为什么是第二批？因为第一批通常比其余批次慢，这是由于一些初始化开销。
- en: Note
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It bears reiterating that profiling using TensorBoard is best suited for power
    users of TensorFlow. If you are just starting out, you are better off using `nvidia-smi`.
    (Although `nvidia-smi` is a far more capable than just providing GPU utilization
    info, which is typically how most practitioners use it.) For users wanting even
    deeper access to their hardware utilization metrics, NVIDIA Nsight is a great
    tool.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 需要重申的是，使用TensorBoard进行分析最适合TensorFlow的高级用户。如果您刚开始使用，最好使用`nvidia-smi`。（尽管`nvidia-smi`不仅提供GPU利用率信息，而且通常是大多数实践者使用的方式。）对于希望更深入了解硬件利用率指标的用户，NVIDIA
    Nsight是一个很好的工具。
- en: Alright. With these tools at our disposal, we know that our program needs some
    tuning and has room for efficiency improvements. We look at those areas one by
    one in the next few sections.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好了。有了这些工具，我们知道我们的程序需要一些调整，并有提高效率的空间。我们将在接下来的几节中逐个查看这些领域。
- en: How to Use This Checklist
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用此检查表
- en: In business, an oft-quoted piece of advice is “You can’t improve what you can’t
    measure.” This applies to deep learning pipelines, as well. Tuning performance
    is like a science experiment. You set up a baseline run, tune a knob, measure
    the effect, and iterate in the direction of improvement. The items on the following
    checklist are our knobs—some are quick and easy, whereas others are more involved.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业中，一个经常引用的建议是“无法衡量的东西无法改进”。这也适用于深度学习流水线。调整性能就像进行科学实验。您设置一个基准运行，调整一个旋钮，测量效果，并朝着改进的方向迭代。以下清单上的项目是我们的旋钮——有些快速简单，而其他一些更复杂。
- en: 'To use this checklist effectively, do the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要有效使用此清单，请执行以下操作：
- en: Isolate the part of the pipeline that you want to improve.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隔离要改进的流水线部分。
- en: Find a relevant point on the checklist.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在清单上找到相关的要点。
- en: Implement, experiment, and observe if runtime is reduced. If not reduced, ignore
    change.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实施、实验，并观察运行时间是否减少。如果没有减少，则忽略更改。
- en: Repeat steps 1 through 3 until the checklist is exhausted.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1至3，直到清单耗尽。
- en: Some of the improvements might be minute, some more drastic. But the cumulative
    effect of all these changes should hopefully result in faster, more efficient
    execution and best of all, more bang for the buck for your hardware. Let’s look
    at each area of the deep learning pipeline step by step, including data preparation,
    data reading, data augmentation, training, and, finally, inference.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一些改进可能微小，一些可能更为重大。但所有这些变化的累积效应希望能够实现更快、更高效的执行，最重要的是，为您的硬件带来更大的回报。让我们逐步查看深度学习流水线的每个领域，包括数据准备、数据读取、数据增强、训练，最后是推理。
- en: Performance Checklist
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能清单
- en: Data Preparation
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: '[“Store as TFRecords”](part0008.html#7K4PE-13fa565533764549a6f0ab7f11eed62b)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“存储为TFRecords”](part0008.html#7K4PE-13fa565533764549a6f0ab7f11eed62b)'
- en: '[“Reduce Size of Input Data”](part0008.html#reduce_size_of_input_data)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“减少输入数据的大小”](part0008.html#reduce_size_of_input_data)'
- en: '[“Use TensorFlow Datasets”](part0008.html#use_tensorflow_datasets)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用TensorFlow数据集”](part0008.html#use_tensorflow_datasets)'
- en: Data Reading
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据读取
- en: '[“Use tf.data”](part0008.html#use_tfdotdata)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用tf.data”](part0008.html#use_tfdotdata)'
- en: '[“Prefetch Data”](part0008.html#prefetch_data)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“预取数据”](part0008.html#prefetch_data)'
- en: '[“Parallelize CPU Processing”](part0008.html#parallelize_cpu_processing)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“并行化CPU处理”](part0008.html#parallelize_cpu_processing)'
- en: '[“Parallelize I/O and Processing”](part0008.html#parallelize_isoliduso_and_processing)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“并行化I/O和处理”](part0008.html#parallelize_isoliduso_and_processing)'
- en: '[“Enable Nondeterministic Ordering”](part0008.html#enable_nondeterministic_ordering)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“启用非确定性排序”](part0008.html#enable_nondeterministic_ordering)'
- en: '[“Cache Data”](part0008.html#cache_data)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“缓存数据”](part0008.html#cache_data)'
- en: '[“Turn on Experimental Optimizations”](part0008.html#7K5GA-13fa565533764549a6f0ab7f11eed62b)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“打开实验性优化”](part0008.html#7K5GA-13fa565533764549a6f0ab7f11eed62b)'
- en: '[“Autotune Parameter Values”](part0008.html#7K5KJ-13fa565533764549a6f0ab7f11eed62b)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“自动调整参数值”](part0008.html#7K5KJ-13fa565533764549a6f0ab7f11eed62b)'
- en: Data Augmentation
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: '[“Use GPU for Augmentation”](part0008.html#use_gpu_for_augmentation)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用GPU进行增强”](part0008.html#use_gpu_for_augmentation)'
- en: Training
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: '[“Use Automatic Mixed Precision”](part0008.html#use_automatic_mixed_precision)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用自动混合精度”](part0008.html#use_automatic_mixed_precision)'
- en: '[“Use Larger Batch Size”](part0008.html#use_larger_batch_size)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用更大的批量大小”](part0008.html#use_larger_batch_size)'
- en: '[“Use Multiples of Eight”](part0008.html#use_multiples_of_eight)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用八的倍数”](part0008.html#use_multiples_of_eight)'
- en: '[“Find the Optimal Learning Rate”](part0008.html#find_the_optimal_learning_rate)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“找到最佳学习率”](part0008.html#find_the_optimal_learning_rate)'
- en: '[“Use tf.function”](part0008.html#use_tfdotfunction)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用tf.function”](part0008.html#use_tfdotfunction)'
- en: '[“Overtrain, and Then Generalize”](part0008.html#overtraincomma_and_then_generalize)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“过度训练，然后泛化”](part0008.html#overtraincomma_and_then_generalize)'
- en: '[“Use progressive sampling”](part0008.html#use_progressive_sampling)'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用渐进采样”](part0008.html#use_progressive_sampling)'
- en: '[“Use progressive augmentation”](part0008.html#use_progressive_augmentation)'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用渐进增强”](part0008.html#use_progressive_augmentation)'
- en: '[“Use progressive resizing”](part0008.html#use_progressive_resizing)'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用渐进调整大小”](part0008.html#use_progressive_resizing)'
- en: '[“Install an Optimized Stack for the Hardware”](part0008.html#install_an_optimized_stack_for_the_hardw)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“为硬件安装优化堆栈”](part0008.html#install_an_optimized_stack_for_the_hardw)'
- en: '[“Optimize the Number of Parallel CPU Threads”](part0008.html#optimize_the_number_of_parallel_cpu_thre)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“优化并行CPU线程数量”](part0008.html#optimize_the_number_of_parallel_cpu_thre)'
- en: '[“Use Better Hardware”](part0008.html#use_better_hardware)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用更好的硬件”](part0008.html#use_better_hardware)'
- en: '[“Distribute Training”](part0008.html#distribute_training)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“分布式训练”](part0008.html#distribute_training)'
- en: '[“Examine Industry Benchmarks”](part0008.html#7K6CP-13fa565533764549a6f0ab7f11eed62b)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“检查行业基准”](part0008.html#7K6CP-13fa565533764549a6f0ab7f11eed62b)'
- en: Inference
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: '[“Use an Efficient Model”](part0008.html#use_an_efficient_model)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用高效模型”](part0008.html#use_an_efficient_model)'
- en: '[“Quantize the Model”](part0008.html#7K6EM-13fa565533764549a6f0ab7f11eed62b)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“量化模型”](part0008.html#7K6EM-13fa565533764549a6f0ab7f11eed62b)'
- en: '[“Prune the Model”](part0008.html#prune_the_model)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“修剪模型”](part0008.html#prune_the_model)'
- en: '[“Use Fused Operations”](part0008.html#use_fused_operations)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用融合操作”](part0008.html#use_fused_operations)'
- en: '[“Enable GPU Persistence”](part0008.html#enable_gpu_persistence)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“启用GPU持久性”](part0008.html#enable_gpu_persistence)'
- en: Note
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A printable version of this checklist is available at [http://PracticalDeepLearning.ai](http://PracticalDeepLearning.ai).
    Feel free to use it as a reference next time you train or deploy a model. Or even
    better, spread the cheer by sharing with your friends, colleagues, and more importantly,
    your manager.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此清单的可打印版本可在[http://PracticalDeepLearning.ai](http://PracticalDeepLearning.ai)上找到。下次训练或部署模型时，可以将其用作参考。或者更好的是，通过与朋友、同事以及更重要的是您的经理分享，传播快乐。
- en: Data Preparation
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: There are a few optimizations that we can make even before we do any kind of
    training, and they have to do with how we prepare our data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行任何训练之前，我们可以进行一些优化，这些优化与我们如何准备数据有关。
- en: Store as TFRecords
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储为TFRecords
- en: Image datasets typically consist of thousands of tiny files, each file measuring
    a few kilobytes. And our training pipeline must read each file individually. Doing
    this thousands of times has significant overhead, causing a slowdown of the training
    process. That problem is even more severe in the case of spinning hard drives,
    for which the magnetic head needs to seek to the beginning of each file. This
    problem is further exacerbated when the files are stored on a remote storage service
    like the cloud. And there lies our first hurdle!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据集通常由成千上万个小文件组成，每个文件大小几千字节。我们的训练管道必须逐个读取每个文件。这样做成千上万次会产生显著的开销，导致训练过程变慢。在使用旋转硬盘时，这个问题更加严重，因为磁头需要寻找每个文件的开头。当文件存储在像云这样的远程存储服务上时，这个问题会进一步恶化。这就是我们的第一个障碍所在！
- en: 'To speed up the reads, one idea is to combine thousands of files into a handful
    of larger files. And that’s exactly what TFRecord does. It stores data in efficient
    Protocol Buffer (protobuf) objects, making them quicker to read. Let’s see how
    to create TFRecord files:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快读取速度，一个想法是将成千上万个文件合并成少数几个较大的文件。这正是TFRecord所做的。它将数据存储在高效的Protocol Buffer（protobuf）对象中，使其更快速读取。让我们看看如何创建TFRecord文件：
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let’s take a look at reading these TFRecord files:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何读取这些TFRecord文件：
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: So, why not join all of the data in a single file, like say for ImageNet? Although
    reading thousands of tiny files harms performance due to the overhead involved,
    reading gigantic files is an equally bad idea. They reduce our ability to make
    parallel reads and parallel network calls. The sweet spot to shard (divide) a
    large dataset in TFRecord files lies at around 100 MB.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么不将所有数据合并到一个文件中，比如说ImageNet？尽管读取成千上万个小文件会因涉及的开销而影响性能，但读取巨大文件同样不是一个好主意。它们降低了我们进行并行读取和并行网络调用的能力。将大型数据集分片（划分）为TFRecord文件的甜蜜点在大约100
    MB左右。
- en: Reduce Size of Input Data
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减小输入数据的大小
- en: 'Image datasets with large images need to be resized before passing through
    to the GPU. This means the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 具有大图像的图像数据集在传递到GPU之前需要调整大小。这意味着以下内容：
- en: Repeated CPU cycles at every iteration
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次迭代中重复使用CPU周期
- en: Repeated I/O bandwidth being consumed at a larger rate than needed in our data
    pipeline
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的数据管道中消耗的I/O带宽以比所需更大的速率被重复使用
- en: One good strategy to save compute cycles is to perform common preprocessing
    steps once on the entire dataset (like resizing) and then saving the results in
    TFRecord files for all future runs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 节省计算周期的一个好策略是在整个数据集上执行常见的预处理步骤一次（如调整大小），然后将结果保存在TFRecord文件中，以供所有未来运行使用。
- en: Use TensorFlow Datasets
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow数据集
- en: For commonly used public datasets, from MNIST (11 MB) to CIFAR-100 (160 MB)
    all the way to MS COCO (38 GB) and Google Open Images (565 GB), it’s quite an
    effort to download the data (often spread across multiple zipped files). Imagine
    your frustration if after downloading 95% of the file slowly, the connection becomes
    spotty and breaks. This is not unusual because these files are typically hosted
    on university servers, or are downloaded from various sources like Flickr (as
    is the case with ImageNet 2012, which gives us the URLs from which to download
    150 GB-plus of images). A broken connection might mean having to start all over
    again.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于常用的公共数据集，从MNIST（11 MB）到CIFAR-100（160 MB），再到MS COCO（38 GB）和Google Open Images（565
    GB），下载数据是相当费力的（通常分布在多个压缩文件中）。想象一下，如果在慢慢下载文件的过程中，下载了95%，然后连接变得不稳定并中断，你会感到多么沮丧。这并不罕见，因为这些文件通常托管在大学服务器上，或者从各种来源（如Flickr）下载（就像ImageNet
    2012一样，它提供了下载150 GB以上图像的URL）。连接中断可能意味着需要重新开始。
- en: If you think that was tedious, the real challenge actually begins only after
    you successfully download the data. For every new dataset, we now need to hunt
    through the documentation to determine how the data is formatted and organized,
    so we can begin reading and processing appropriately. Then, we need to split the
    data into training, validation, and test sets (preferably converting to TFRecords).
    And when the data is so large as to not fit in memory, we will need to do some
    manual jiu-jitsu to read it and feed it efficiently to the training pipeline.
    We never said it was easy.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为这很繁琐，那么真正的挑战实际上只有在成功下载数据之后才开始。对于每个新数据集，我们现在需要查阅文档，确定数据的格式和组织方式，以便适当地开始读取和处理。然后，我们需要将数据分割为训练、验证和测试集（最好转换为TFRecords）。当数据太大而无法放入内存时，我们需要做一些手动操作来高效地读取并将其有效地提供给训练管道。我们从未说过这很容易。
- en: Alternately, we could skip all the pain by consuming the high-performance, ready-to-use
    TensorFlow Datasets package. With several famous datasets available, it downloads,
    splits, and feeds our training pipeline using best practices in a few lines.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过使用高性能、即用即用的TensorFlow数据集包来避免所有痛苦。有几个著名的数据集可用，它会下载、拆分并使用最佳实践来喂养我们的训练管道，只需几行代码。
- en: Let’s look at which datasets are available.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看有哪些数据集可用。
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'There are more than 100 datasets as of this writing, and that number is steadily
    increasing. Now, let’s download, extract, and make an efficient pipeline using
    the training set of CIFAR-10:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，已有100多个数据集，这个数字还在稳步增长。现在，让我们下载、提取并使用CIFAR-10的训练集创建一个高效的管道：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That’s it! The first time we execute the code, it will download and cache the
    dataset on our machine. For every future run, it will skip the network download
    and directly read from the cache.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！第一次执行代码时，它将在我们的机器上下载并缓存数据集。对于以后的每次运行，它将跳过网络下载，直接从缓存中读取。
- en: Data Reading
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据读取
- en: Now that the data is prepared, let’s look for opportunities to maximize the
    throughput of the data reading pipeline.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据准备好了，让我们寻找最大化数据读取管道吞吐量的机会。
- en: Use tf.data
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用tf.data
- en: We could choose to manually read every file from our dataset with Python’s built-in
    I/O library. We could simply call `open` for each file and we’d be good to go,
    right? The main downside in this approach is that our GPU would be bottlenecked
    by our file reads. Every time we read a file, the GPU needs to wait. Every time
    the GPU starts processing its input, we wait before we read the next file from
    disk. Seems rather wasteful, doesn’t it?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择使用Python的内置I/O库手动读取数据集中的每个文件。我们只需为每个文件调用`open`，然后就可以开始了，对吧？这种方法的主要缺点是我们的GPU将受到文件读取的限制。每次读取一个文件时，GPU都需要等待。每次GPU开始处理其输入时，我们都需要等待下一个文件从磁盘中读取。看起来相当浪费，不是吗？
- en: 'If there’s only one thing you can take away from this chapter, let it be this:
    `tf.data` is the way to go for building a high-performance training pipeline.
    In the next few sections, we explore several aspects of `tf.data` that you can
    exploit to improve training speed.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只能从本章中学到一件事，那就是：`tf.data`是构建高性能训练管道的方法。在接下来的几节中，我们将探讨几个可以利用来提高训练速度的`tf.data`方面。
- en: 'Let’s set up a base pipeline for reading data:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为读取数据设置一个基本管道：
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Prefetch Data
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预取数据
- en: In the pipeline we discussed earlier, the GPU waits for the CPU to generate
    data, and then the CPU waits for the GPU to finish computation before generating
    data for the next cycle. This circular dependency causes idle time for both CPU
    and GPU, which is inefficient.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前讨论的管道中，GPU等待CPU生成数据，然后CPU等待GPU完成计算，然后再生成下一个周期的数据。这种循环依赖会导致CPU和GPU的空闲时间，这是低效的。
- en: The `prefetch` function helps us here by delinking the production of the data
    (by the CPU) from the consumption of the data (by the GPU). Using a background
    thread, it allows data to be passed *asynchronously* into an intermediate buffer,
    where it is readily available for a GPU to consume. The CPU now carries on with
    the next computation instead of waiting for the GPU. Similarly, as soon as the
    GPU is finished with its previous computation, and there’s data readily available
    in the buffer, it starts processing.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`prefetch`函数通过将数据的生成（由CPU）与数据的消耗（由GPU）分离，帮助我们。使用一个后台线程，它允许数据被*异步*传递到一个中间缓冲区，其中数据可以立即供GPU消耗。CPU现在继续进行下一个计算，而不是等待GPU。同样，一旦GPU完成了其先前的计算，并且缓冲区中有数据可用，它就开始处理。'
- en: 'To use it, we can simply call `prefetch` on our dataset at the very end of
    our pipeline along with a `buffer_size` parameter (which is the maximum amount
    of data that can be stored). Usually `buffer_size` is a small number; `1` is good
    enough in many cases:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用它，我们只需在管道的最后调用`prefetch`，并附加一个`buffer_size`参数（即可以存储的最大数据量）。通常`buffer_size`是一个小数字；在许多情况下，`1`就足够了：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In just a few pages, we show you how to find an optimal value for this parameter.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在短短几页中，我们将向您展示如何找到这个参数的最佳值。
- en: In summary, if there’s an opportunity to overlap CPU and GPU computations, `prefetch`
    will automatically exploit it.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，如果有机会重叠CPU和GPU计算，`prefetch`将自动利用它。
- en: Parallelize CPU Processing
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行化CPU处理
- en: 'It would be a waste to have a CPU with multiple cores but doing all of our
    processing on only one of them. Why not take advantage of the rest? This is exactly
    where the `num_parallel_calls` argument in the `map` function comes in handy:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有多个核心的CPU，但只使用其中一个核心进行所有处理，那将是一种浪费。为什么不利用其他核心呢？这正是`map`函数中的`num_parallel_calls`参数派上用场的地方：
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This starts multiple threads to parallelize processing of the `map()` function.
    Assuming that there is no heavy application running in the background, we will
    want to set `num_parallel_calls` to the number of CPU cores on our system. Anything
    more will potentially degrade the performance due to the overhead of context switching.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动多个线程来并行处理`map()`函数。假设后台没有运行重型应用程序，我们将希望将`num_parallel_calls`设置为系统上的CPU核心数。任何更多的设置可能会由于上下文切换的开销而降低性能。
- en: Parallelize I/O and Processing
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行化I/O和处理
- en: Reading files from disk or worse, over a network, is a huge cause of bottlenecks.
    We might possess the best CPU and GPU in the world, but if we don’t optimize our
    file reads, it would all be for naught. One solution that addresses this problem
    is to parallelize both I/O and subsequent processing (also known as *interleaving*).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从磁盘或更糟的是从网络中读取文件是瓶颈的主要原因。我们可能拥有世界上最好的CPU和GPU，但如果我们不优化文件读取，那一切都将是徒劳的。解决这个问题的一个方法是并行化I/O和后续处理（也称为*交错处理）。
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this command, two things are happening:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令中，发生了两件事：
- en: The input data is acquired in parallel (by default equal to the number of cores
    on the system).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据是并行获取的（默认情况下等于系统上的核心数）。
- en: On the acquired data, setting the `num_parallel_calls` parameter allows the
    `map_func` function to execute on multiple parallel threads and read from the
    incoming data asynchronously.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在获取的数据上，设置`num_parallel_calls`参数允许`map_func`函数在多个并行线程上执行，并异步从传入的数据中读取。
- en: If `num_parallel_calls` was not specified, even if the data were read in parallel,
    `map_func` would run synchronously on a single thread. As long as `map_func` runs
    faster than the rate at which the input data is coming in, there will not be a
    problem. We definitely want to set `num_parallel_calls` higher if `map_func` becomes
    a bottleneck.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有指定`num_parallel_calls`，即使数据是并行读取的，`map_func`也会在单个线程上同步运行。只要`map_func`的运行速度快于输入数据到达的速度，就不会有问题。如果`map_func`成为瓶颈，我们肯定希望将`num_parallel_calls`设置得更高。
- en: Enable Nondeterministic Ordering
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用非确定性排序
- en: 'For many datasets, the reading order is not important. After all, we might
    be randomizing their ordering anyway. By default, when reading files in parallel,
    `tf.data` still attempts to produce their outputs in a *fixed round-robin order*.
    The disadvantage is that we might encounter a “straggler” along the way (i.e.,
    an operation that takes a lot longer than others, such as a slow file read, and
    holds up all other operations). It’s like a grocery store line where the person
    in front of us insists on using cash with the exact change, whereas everyone else
    uses a credit card. So instead of blocking all the subsequent operations that
    are ready to give output, we skip over the stragglers until they are done with
    their processing. This breaks the ordering while reducing wasted cycles waiting
    for the handful of slower operations:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多数据集，读取顺序并不重要。毕竟，我们可能会随机排列它们的顺序。默认情况下，在并行读取文件时，`tf.data`仍然尝试以*固定的轮流顺序*产生它们的输出。缺点是我们可能会在途中遇到“拖延者”（即，一个操作比其他操作花费更长时间，例如慢速文件读取，并阻止所有其他操作）。这就像在杂货店排队时，我们前面的人坚持使用现金并找零，而其他人都使用信用卡。因此，我们跳过拖延者，直到他们完成处理，而不是阻塞所有准备输出的后续操作。这打破了顺序，同时减少了等待少数较慢操作的浪费周期：
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Cache Data
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存数据
- en: 'The `Dataset.cache()` function allows us to make a copy of data either in memory
    or as a file on disk. There are two reasons why you might want to cache a dataset:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset.cache()`函数允许我们将数据复制到内存或磁盘文件中。有两个原因可能需要缓存数据集：'
- en: To avoid repeatedly reading from disk after the first epoch. This is obviously
    effective only when the cache is in memory and can fit in the available RAM.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免在第一个epoch之后重复从磁盘读取。这显然只在缓存在内存中且可以适应可用RAM时有效。
- en: To avoid having to repeatedly perform expensive CPU operations on data (e.g.,
    resizing large images to a smaller size).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免重复执行昂贵的CPU操作（例如，将大图像调整为较小尺寸）。
- en: Tip
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Cache is best used for data that is not going to change. It is recommended to
    place `cache()` before any random augmentations and shuffling; otherwise, caching
    at the end will result in exactly the same data and order in every run.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存最适用于不会更改的数据。建议在任何随机增强和洗牌之前放置`cache()`；否则，在最后进行缓存将导致每次运行中数据和顺序完全相同。
- en: 'Depending on our scenario, we can use one of the two following lines:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的情况，我们可以使用以下两行中的一行：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It’s worth noting that in-memory cache is volatile and hence only shows performance
    improvements in the second epoch of every run. On the other hand, file-based cache
    will make every run faster (beyond the very first epoch of the first run).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，内存中的缓存是易失性的，因此只在每次运行的第二个epoch中显示性能改进。另一方面，基于文件的缓存将使每次运行更快（超出第一次运行的第一个epoch）。
- en: Tip
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In the [“Reduce Size of Input Data”](part0008.html#reduce_size_of_input_data),
    we mentioned preprocessing the data and saving it as TFRecord files as input to
    future data pipelines. Using the **`cache()`** function directly after the preprocessing
    step in your pipeline would give a similar performance with a single word change
    in code.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“减少输入数据的大小”](part0008.html#reduce_size_of_input_data)中，我们提到预处理数据并将其保存为TFRecord文件作为未来数据流水线的输入。在流水线中的预处理步骤之后直接使用**`cache()`**函数，只需在代码中进行一个单词的更改，就可以获得类似的性能。
- en: Turn on Experimental Optimizations
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打开实验性优化
- en: TensorFlow has many built-in optimizations, often initially experimental and
    turned off by default. Depending on your use case, you might want to turn on some
    of them to squeeze out just a little more performance from your pipeline. Many
    of these optimizations are detailed in the documentation for `tf.data.experimental.OptimizationOptions`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow有许多内置的优化，通常最初是实验性的，并默认关闭。根据您的用例，您可能希望打开其中一些以从流水线中挤出更多性能。这些优化中的许多细节在`tf.data.experimental.OptimizationOptions`的文档中有详细说明。
- en: Note
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Here’s a quick refresher on filter and map operations:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关于过滤和映射操作的快速复习：
- en: Filter
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤
- en: A filter operation goes through a list element by element and grabs those that
    match a given condition. The condition is supplied as a lambda operation that
    returns a boolean value.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤操作逐个元素遍历列表，并抓取符合给定条件的元素。条件以lambda操作的形式提供，返回布尔值。
- en: Map
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 映射
- en: A map operation simply takes in an element, performs a computation, and returns
    an output. For example, resizing an image.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 映射操作只是接受一个元素，执行计算，并返回一个输出。例如，调整图像大小。
- en: Let’s look at a few experimental optimizations that are available to us, including
    examples of two consecutive operations that could benefit from being fused together
    as one single operation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些可用的实验性优化，包括两个连续操作的示例，这些操作合并在一起作为一个单一操作可能会受益。
- en: Filter fusion
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤融合
- en: 'Sometimes, we might want to filter based on multiple attributes. Maybe we want
    to use only images that have both a dog and a cat. Or, in a census dataset, only
    look at families above a certain income threshold who also live within a certain
    distance to the city center. `filter_fusion` can help speed up such scenarios.
    Consider the following example:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可能想根据多个属性进行过滤。也许我们只想使用同时有狗和猫的图像。或者在人口普查数据集中，只查看收入超过一定门槛且距离市中心一定距离的家庭。`filter_fusion`可以帮助加快这种情况的速度。考虑以下示例：
- en: '[PRE16]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first filter performs a full pass over the entire dataset and returns elements
    that are less than 1,000\. On this output, the second filter does another pass
    to further remove elements not divisible by three. Instead of doing two passes
    over many of the same elements, we could instead combine both the filter operations
    into one pass using an `AND` operation. That is precisely what the `filter_fusion`
    option enables—combining multiple filter operations into one pass. By default,
    it is turned off. You can enable it by using the following statement:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个过滤器对整个数据集执行完整传递，并返回小于1,000的元素。在此输出上，第二个过滤器执行另一个传递以进一步删除不能被三整除的元素。我们可以将两个过滤操作合并为一个传递，而不是对许多相同元素执行两次传递，方法是使用`AND`操作。这正是`filter_fusion`选项所能实现的——将多个过滤操作合并为一个传递。默认情况下，它是关闭的。您可以使用以下语句启用它：
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Map and filter fusion
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 映射和过滤融合
- en: 'Consider the following example:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例：
- en: '[PRE18]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this example, the `map` function does a full pass on the entire dataset
    to calculate the square of every element. Then, the `filter` function discards
    the odd elements. Rather than doing two passes (more so in this particularly wasteful
    example), we could simply fuse the map and filter operations together by turning
    on the `map_and_filter_fusion` option so that they operate as a single unit:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，`map`函数对整个数据集执行完整传递，计算每个元素的平方。然后，`filter`函数丢弃奇数元素。与执行两次传递（尤其是在这个特别浪费的示例中）相比，我们可以通过打开`map_and_filter_fusion`选项将map和filter操作合并在一起，使它们作为一个单元操作：
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Map fusion
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 映射融合
- en: 'Similar to the aforementioned two examples, fusing two or more map operations
    prevents multiple passes from being performed on the same data and instead combines
    them in a single pass:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面两个示例类似，合并两个或多个映射操作可以防止对相同数据执行多次传递，而是将它们合并为单次传递：
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Autotune Parameter Values
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动调整参数值
- en: You might have noticed that many of the code examples in this section have hardcoded
    values for some of the parameters. For the combination of the problem and hardware
    at hand, you can tune them for maximum efficiency. How to tune them? One obvious
    way is to manually tweak the parameters one at a time and isolate and observe
    the impact of each of them on the overall performance until we get the precise
    parameter set. But the number of knobs to tune quickly gets out of hand due to
    the combinatorial explosion. If this wasn’t enough, our finely tuned script wouldn’t
    necessarily be as efficient on another machine due to differences in hardware
    such as the number of CPU cores, GPU availability, and so on. And even on the
    same system, depending on resource usage by other programs, these knobs might
    need to be adjusted over different runs.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，本节中的许多代码示例对一些参数具有硬编码值。针对手头问题和硬件的组合，您可以调整它们以实现最大效率。如何调整它们？一个明显的方法是逐个手动调整参数并隔离并观察每个参数对整体性能的影响，直到我们获得精确的参数集。但由于组合爆炸，要调整的旋钮数量很快就会失控。如果这还不够，我们精心调整的脚本在另一台机器上不一定会像在另一台机器上那样高效，因为硬件的差异，如CPU核心数量、GPU可用性等。甚至在同一系统上，根据其他程序的资源使用情况，这些旋钮可能需要在不同运行中进行调整。
- en: 'How do we solve this? We do the opposite of manual tuning: autotuning. Using
    hill-climbing optimization algorithms (which are a type of heuristic-driven search
    algorithms), this option automatically finds the ideal parameter combination for
    many of the `tf.data` function parameters. Simply use `tf.data.experimental.AUTOTUNE`
    instead of manually assigning numbers. It’s the one parameter to rule them all.
    Consider the following example:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解决这个问题？我们做与手动调整相反的事情：自动调整。使用爬山优化算法（这是一种启发式搜索算法），此选项会自动找到许多`tf.data`函数参数的理想参数组合。只需使用`tf.data.experimental.AUTOTUNE`而不是手动分配数字。这是一个参数来统治它们所有。考虑以下示例：
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Isn’t that an elegant solution? We can do that for several other function calls
    in the `tf.data` pipeline. The following is an example of combining together several
    optimizations from the section “Data Reading” to make a high-performance data
    pipeline:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个优雅的解决方案吗？我们可以对`tf.data`管道中的几个其他函数调用执行相同的操作。以下是一个示例，结合了“数据读取”部分中的几个优化，以创建高性能数据管道：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Data Augmentation
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: Sometimes, we might not have sufficient data to run our training pipeline. Even
    if we did, we might still want to manipulate the images to improve the robustness
    of our model—with the help of data augmentation. Let’s see whether we can make
    this step any faster.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可能没有足够的数据来运行我们的训练管道。即使有，我们可能仍然希望操纵图像以提高模型的鲁棒性—借助数据增强。让我们看看是否可以使这一步更快。
- en: Use GPU for Augmentation
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GPU进行增强
- en: Data preprocessing pipelines can be elaborate enough that you could write an
    entire book about them. Image transformation operations such as resizing, cropping,
    color transformations, blurring, and so on are commonly performed on the data
    immediately after it’s read from disk into memory. Given that these are all matrix
    transformation operations, they might do well on a GPU.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理管道可能足够复杂，以至于你可以写一本关于它们的整本书。图像转换操作，如调整大小、裁剪、颜色转换、模糊等通常在数据从磁盘读入内存后立即执行。鉴于这些都是矩阵转换操作，它们可能在GPU上表现良好。
- en: OpenCV, Pillow, and the built-in Keras augmentation functionality are the most
    commonly used libraries in computer vision for working on images. There’s one
    major limitation here, though. Their image processing is primarily CPU based (although
    you can compile OpenCV to work with CUDA), which means that the pipeline might
    not be fully utilizing the underlying hardware to its true potential.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV、Pillow和内置的Keras增强功能是计算机视觉中最常用的用于处理图像的库。然而，这里有一个主要限制。它们的图像处理主要基于CPU（尽管你可以编译OpenCV以与CUDA一起使用），这意味着管道可能没有充分利用底层硬件的真正潜力。
- en: Note
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As of August 2019, there are efforts underway to convert Keras image augmentation
    to be GPU accelerated, as well.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2019年8月，正在努力将Keras图像增强转换为GPU加速。
- en: There are a few different GPU-bound options that we can explore.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以探索一些不同的受GPU限制的选项。
- en: tf.image built-in augmentations
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tf.image内置增强
- en: '`tf.image` provides some handy augmentation functions that we can seamlessly
    plug into a `tf.data` pipeline. Some of the methods include image flipping, color
    augmentations (hue, saturation, brightness, contrast), zooming, and rotation.
    Consider the following example, which changes the hue of an image:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.image`提供了一些方便的增强函数，我们可以轻松地将其插入到`tf.data`流水线中。其中一些方法包括图像翻转、颜色增强（色调、饱和度、亮度、对比度）、缩放和旋转。考虑以下示例，改变图像的色调：'
- en: '[PRE23]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The downside to relying on `tf.image` is that the functionality is much more
    limited compared to OpenCV, Pillow, and even Keras. For example, the built-in
    function for image rotation in `tf.image` only supports rotating images by 90
    degrees counter-clockwise. If we need to be able to rotate by an arbitrary amount,
    such as 10 degrees, we’d need to manually build that functionality. Keras, on
    the other hand, provides that functionality out of the box.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖`tf.image`的缺点是，与OpenCV、Pillow甚至Keras相比，功能要受限得多。例如，在`tf.image`中，用于图像旋转的内置函数只支持逆时针旋转90度的图像。如果我们需要能够按任意角度旋转，比如10度，我们就需要手动构建这个功能。另一方面，Keras提供了这个功能。
- en: As another alternative to the `tf.data` pipeline, the NVIDIA Data Loading Library
    (DALI) offers a fast data loading and preprocessing pipeline accelerated by GPU
    processing. As shown in [Figure 6-4](part0008.html#the_nvidia_dali_pipeline),
    DALI implements several common steps including resizing an image and augmenting
    an image in the GPU, immediately before the training. DALI works with multiple
    deep learning frameworks including TensorFlow, PyTorch, MXNet, and others, offering
    portability of the preprocessing pipelines.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 作为`tf.data`流水线的另一种选择，NVIDIA数据加载库（DALI）提供了一个由GPU处理加速的快速数据加载和预处理流水线。如[图6-4](part0008.html#the_nvidia_dali_pipeline)所示，DALI实现了包括在训练之前在GPU中调整图像大小和增强图像等几个常见步骤。DALI与多个深度学习框架一起工作，包括TensorFlow、PyTorch、MXNet等，提供了预处理流水线的可移植性。
- en: NVIDIA DALI
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NVIDIA DALI
- en: '![The NVIDIA DALI pipeline](../images/00256.jpeg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![NVIDIA DALI流水线](../images/00256.jpeg)'
- en: Figure 6-4\. The NVIDIA DALI pipeline
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4\. NVIDIA DALI流水线
- en: Additionally, even JPEG decoding (a relatively heavy task) can partially make
    use of the GPU, giving it an additional boost. This is done using nvJPEG, a GPU-accelerated
    library for JPEG decoding. For multi-GPU tasks, this scales near linearly as the
    number of GPUs increases.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使JPEG解码（一个相对繁重的任务）也可以部分利用GPU，从而获得额外的提升。这是通过使用nvJPEG实现的，nvJPEG是一个用于JPEG解码的GPU加速库。对于多GPU任务，随着GPU数量的增加，性能几乎呈线性增长。
- en: NVIDIAs efforts culminated in a record-breaking MLPerf entry (which benchmarks
    machine learning hardware, software, and services), training a ResNet-50 model
    in 80 seconds.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA的努力在MLPerf中取得了创纪录的成绩（对机器学习硬件、软件和服务进行基准测试），在80秒内训练了一个ResNet-50模型。
- en: Training
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: For those beginning their performance optimization journey, the quickest wins
    come from improving the data pipelines, which is relatively easy. For a training
    pipeline that is already being fed data fast, let’s investigate optimizations
    for our actual training step.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些刚开始进行性能优化的人来说，最快的收获来自于改进数据流水线，这相对容易。对于已经快速提供数据的训练流水线，让我们来研究一下实际训练步骤的优化。
- en: Use Automatic Mixed Precision
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自动混合精度
- en: “*One line to make your training two to three times faster!*”
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: “*一行代码让你的训练速度提高两到三倍！*”
- en: 'Weights in deep learning models are typically stored in single-precision; that
    is, 32-bit floating point, or as it’s more commonly referenced: FP32\. Putting
    these models in memory-constrained devices such as mobile phones can be challenging
    to accommodate. A simple trick to make models smaller is to convert them from
    single-precision (FP32) to half-precision (FP16). Sure, the representative power
    of these weights goes down, but as we demonstrate later in this chapter ([“Quantize
    the Model”](part0008.html#7K6EM-13fa565533764549a6f0ab7f11eed62b)), neural networks
    are resilient to small changes, much like they are resilient to noise in images.
    Hence, we get the benefits of a more efficient model without sacrificing much
    accuracy. In fact, we can even reduce the representation to 8-bit integers (INT8)
    without a significant loss in accuracy, as we will see in some upcoming chapters.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型中的权重通常以单精度存储，即32位浮点，或者更常见的称为FP32。将这些模型放在内存受限的设备上，如手机，可能会很具挑战性。使模型变小的一个简单技巧是将其从单精度（FP32）转换为半精度（FP16）。当然，这些权重的代表能力会下降，但正如我们在本章后面展示的（“量化模型”）中所示，神经网络对于小的变化是有弹性的，就像它们对图像中的噪声有弹性一样。因此，我们可以获得更高效的模型，而几乎不损失准确性。事实上，我们甚至可以将表示减少到8位整数（INT8），而不会显著损失准确性，正如我们将在接下来的一些章节中看到的。
- en: So, if we can use reduced-precision representation during inference, could we
    do the same during training, as well? Going from 32-bit to 16-bit representation
    would effectively mean double the memory bandwidth available, double the model
    size, or double the batch size can be accommodated. Unfortunately, it turns out
    that using FP16 naïvely *during training* can potentially lead to a significant
    loss in model accuracy and might not even converge to an optimal solution. This
    happens because of FP16’s limited range for representing numbers. Due to a lack
    of adequate precision, any updates to the model during training, if sufficiently
    small, will cause an update to not even register. Imagine adding 0.00006 to a
    weight value of 1.1\. With FP32, the weight would be correctly updated to 1.10006\.
    With FP16, however, the weight would remain 1.1\. Conversely, any activations
    from layers such as Rectified Linear Unit (ReLU) could be high enough for FP16
    to overflow and hit infinity (`NaN` in Python).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们可以在推断期间使用降低精度表示，那么在训练期间也可以这样做吗？从32位到16位表示实际上意味着可以提供双倍的内存带宽，双倍的模型大小，或者可以容纳双倍的批量大小。不幸的是，结果表明在训练期间天真地使用FP16可能会导致模型准确性显著下降，甚至可能无法收敛到最佳解决方案。这是因为FP16对于表示数字的范围有限。由于精度不足，如果训练期间对模型的任何更新足够小，将导致更新甚至不被注册。想象一下将0.00006添加到权重值为1.1的情况。使用FP32，权重将正确更新为1.10006。然而，使用FP16，权重将保持为1.1。相反，诸如修正线性单元（ReLU）之类的层的任何激活可能足够高，以至于FP16会溢出并达到无穷大（Python中的`NaN`）。
- en: The easy answer to these challenges is to use automatic mixed-precision training.
    In this method, we store the model in FP32 as a master copy and perform the forward/backward
    passes of training in FP16\. After each training step is performed, the final
    update from that step is then scaled back up to FP32 before it is applied to the
    master copy. This helps avoid the pitfalls of FP16 arithmetic and results in a
    lower memory footprint, and faster training (experiments have shown increases
    in speed by two to three times), while achieving similar accuracy levels as training
    solely in FP32\. It is noteworthy that newer GPU architectures like the NVIDIA
    Volta and Turing especially optimize FP16 operations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 应对这些挑战的简单答案是使用自动混合精度训练。在这种方法中，我们将模型存储为FP32作为主副本，并在FP16中执行训练的前向/后向传递。在执行每个训练步骤后，该步骤的最终更新然后被缩放回FP32，然后应用于主副本。这有助于避免FP16算术的缺陷，并导致更低的内存占用和更快的训练（实验证明速度增加了两到三倍），同时实现与仅在FP32中训练相似的准确性水平。值得注意的是，像NVIDIA
    Volta和Turing这样的新型GPU架构特别优化FP16操作。
- en: 'To enable mixed precision during training, we simply need to add the following
    line to the beginning of our Python script:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 要在训练期间启用混合精度，我们只需在Python脚本的开头添加以下行：
- en: '[PRE24]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Use Larger Batch Size
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用更大的批量大小
- en: 'Instead of using the entire dataset for training in one batch, we train with
    several minibatches of data. This is done for two reasons:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 与在一个批次中使用整个数据集进行训练不同，我们使用几个数据的小批量进行训练。这是出于两个原因：
- en: Our full data (single batch) might not fit in the GPU RAM.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的完整数据（单批次）可能无法适应GPU内存。
- en: We can achieve similar training accuracy by feeding many smaller batches, just
    as you would by feeding fewer larger batches.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提供许多较小的批次，我们可以实现类似的训练准确性，就像通过提供较少的较大批次一样。
- en: Having smaller minibatches might not fully utilize the available GPU memory,
    so it’s vital to experiment with this parameter, see its effect on the GPU utilization
    (using the `nvidia-smi` command), and choose the batch size that maximizes the
    utilization. Consumer GPUs like the NVIDIA 2080 Ti ship with 11 GB of GPU memory,
    which is plenty for efficient models like MobileNet family.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较小的小批量可能无法充分利用可用的GPU内存，因此对于这个参数进行实验，查看其对GPU利用率的影响（使用`nvidia-smi`命令），并选择最大化利用率的批量大小是至关重要的。像NVIDIA
    2080 Ti这样的消费级GPU配备了11 GB的GPU内存，对于像MobileNet家族这样的高效模型来说是足够的。
- en: For example on hardware with the 2080 Ti graphics card, using 224 x 224 resolution
    images and MobileNetV2 model, the GPU can accommodate a batch size up to 864\.
    [Figure 6-5](part0008.html#effect_of_varying_batch_size_on_time_per) shows the
    effect of varying batch sizes from 4 to 864, on both the GPU utilization (solid
    line) as well as the time per epoch (dashed line). As we can see in the figure,
    the higher the batch size, the higher the GPU utilization, leading to a shorter
    training time per epoch.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在具有2080 Ti显卡的硬件上，使用224 x 224分辨率图像和MobileNetV2模型，GPU可以容纳高达864的批量大小。[图6-5](part0008.html#effect_of_varying_batch_size_on_time_per)显示了从4到864不同批量大小对GPU利用率（实线）以及每个epoch的时间（虚线）的影响。正如我们在图中看到的那样，批量大小越大，GPU利用率越高，导致每个epoch的训练时间更短。
- en: Even at our max batch size of 864 (before running out of memory allocation),
    the GPU utilization does not cross 85%. This means that the GPU was fast enough
    to handle the computations of our otherwise very efficient data pipeline. Replacing
    MobileNetV2 with a heavier ResNet-50 model immediately increased GPU to 95%.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在我们的最大批量大小为864（在内存分配耗尽之前），GPU利用率也不会超过85%。这意味着GPU足够快，可以处理我们非常高效的数据管道的计算。将MobileNetV2替换为更重的ResNet-50模型立即将GPU利用率提高到95%。
- en: '![Effect of varying batch size on time per epoch (seconds) as well as on percentage
    GPU utilization (Log scales have been used for both X- and Y-axes.)](../images/00065.jpeg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![不同批量大小对每个epoch的时间（秒）以及GPU利用率的影响（X轴和Y轴均采用对数刻度）](../images/00065.jpeg)'
- en: Figure 6-5\. Effect of varying batch size on time per epoch (seconds) as well
    as on percentage GPU utilization (Log scales have been used for both X- and Y-axes.)
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5。不同批量大小对每个epoch的时间（秒）以及GPU利用率的影响（X轴和Y轴均采用对数刻度）。
- en: Tip
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Even though we showcased batch sizes up to a few hundreds, large industrial
    training loads distributed across multiple nodes often use much larger batch sizes
    with the help of a technique called Layer-wise Adaptive Rate Scaling (LARS). For
    example, Fujitsu Research trained a ResNet-50 network to 75% Top-1 accuracy on
    ImageNet in a mere 75 seconds. Their ammunition? 2048 Tesla V100 GPUs and a whopping
    batch size of 81,920!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们展示了高达几百的批量大小，但大型工业训练负载通常使用更大的批量大小，借助一种称为层自适应速率缩放（LARS）的技术。例如，富士通研究在短短75秒内使用2048个Tesla
    V100 GPU和庞大的批量大小81920在ImageNet上训练了一个ResNet-50网络，使其Top-1准确率达到75%！
- en: Use Multiples of Eight
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用8的倍数
- en: 'Most of the computations in deep learning are in the form of “matrix multiply
    and add.” Although it’s an expensive operation, specialized hardware has increasingly
    been built in the past few years to optimize for its performance. Examples include
    Google’s TPUs and NVIDIA’s Tensor Cores (which can be found in the Turing and
    Volta architectures). Turing GPUs provide both Tensor Cores (for FP16 and INT8
    operations) as well as CUDA cores (for FP32 operations), with the Tensor Cores
    delivering significantly higher throughput. Due to their specialized nature, Tensor
    Cores require that certain parameters within the data supplied to them be divisible
    by eight. Here are just three such parameters:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的大多数计算都是“矩阵乘法和加法”的形式。尽管这是一项昂贵的操作，但在过去几年中已经建立了专门的硬件来优化其性能。例如，谷歌的TPU和英伟达的张量核心（可以在图灵和伏尔塔架构中找到）。图灵GPU提供张量核心（用于FP16和INT8操作）以及CUDA核心（用于FP32操作），张量核心提供了显著更高的吞吐量。由于它们的专门性质，张量核心要求提供给它们的数据中的某些参数必须是8的倍数。以下是三个这样的参数：
- en: The number of channels in a convolutional filter
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积滤波器中的通道数
- en: The number of neurons in a fully connected layer and the inputs to this layer
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接层中的神经元数量和该层的输入
- en: The size of minibatches
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小批量的大小
- en: If these parameters are not divisible by eight, the GPU CUDA cores will be used
    as the fallback accelerator instead. In an [experiment](https://oreil.ly/KoEkM)
    reported by NVIDIA, simply changing the batch size from 4,095 to 4,096 resulted
    in an increase in throughput of five times. Keep in mind that using multiples
    of eight (or 16 in the case of INT8 operations), in addition to using automatic
    mixed precision, is the bare minimum requirement to activate the Tensor Cores.
    For higher efficiency, the recommended values are in fact multiples of 64 or 256\.
    Similarly, Google recommends multiples of 128 when using TPUs for maximum efficiency.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些参数不能被8整除，GPU CUDA核心将被用作备用加速器。根据英伟达报告的一个[实验](https://oreil.ly/KoEkM)，将批量大小从4,095更改为4,096导致吞吐量增加了五倍。请记住，使用8的倍数（或在INT8操作中使用16的倍数），以及使用自动混合精度，是激活张量核心的最低要求。为了更高的效率，推荐值实际上是64或256的倍数。同样，谷歌建议在使用TPU时使用128的倍数以获得最大效率。
- en: Find the Optimal Learning Rate
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 找到最佳学习率
- en: One hyperparameter that greatly affects our speed of convergence (and accuracy)
    is the learning rate. The ideal result of training is the global minimum; that
    is, the point of least loss. Too high a learning rate can cause our model to overshoot
    the global minimum (like a wildly swinging pendulum) and potentially never converge.
    Too low a learning rate can cause convergence to take too long because the learning
    algorithm will take very small steps toward the minimum. Finding the right initial
    learning rate can make a world of difference.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一个极大影响我们收敛速度（和准确性）的超参数是学习率。训练的理想结果是全局最小值；也就是说，最小损失点。学习率过高可能会导致我们的模型超过全局最小值（就像一个疯狂摆动的钟摆），可能永远不会收敛。学习率过低可能会导致收敛时间过长，因为学习算法将朝着最小值迈出非常小的步骤。找到正确的初始学习率可以产生巨大的差异。
- en: 'The naive way to find the ideal initial learning rate is to try a few different
    learning rates (such as 0.00001, 0.0001, 0.001, 0.01, 0.1) and find one that starts
    converging quicker than others. Or, even better, perform grid search over a range
    of values. This approach has two problems: 1) depending on the granularity, it
    might find a decent value, but it might not be the most optimal value; and 2)
    we need to train multiple times, which can be time consuming.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 找到理想的初始学习率的朴素方法是尝试几种不同的学习率（例如0.00001、0.0001、0.001、0.01、0.1）并找到一个比其他更快收敛的学习率。或者，更好的是，在一系列值上执行网格搜索。这种方法有两个问题：1）根据粒度的不同，可能会找到一个不错的值，但可能不是最优值；2）我们需要多次训练，这可能会耗费时间。
- en: 'In Leslie N. Smith’s 2015 paper, “Cyclical Learning Rates for Training Neural
    Networks,” he describes a much better strategy to find this optimal learning rate.
    In summary:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在Leslie N. Smith的2015年论文“用于训练神经网络的循环学习率”中，他描述了一种更好的策略来找到这个最佳学习率。总结如下：
- en: Start with a really low learning rate and gradually increase it until reaching
    a prespecified maximum value.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个非常低的学习率开始，逐渐增加直到达到预定的最大值。
- en: At each learning rate, observe the loss—first it will be stagnant, then it will
    begin going down and then eventually go back up.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个学习率下观察损失——首先它会停滞，然后开始下降，最后会再次上升。
- en: Calculate the rate of decrease of loss (first derivative) at each learning rate.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个学习率下损失的减少率（一阶导数）。
- en: Select the point with the highest rate of decrease of loss.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择具有最高损失减少率的点。
- en: 'It sounds like a lot of steps, but thankfully we don’t need to write code for
    it. The [keras_lr_finder](https://oreil.ly/il_BI) library by Pavel Surmenok gives
    us a handy function to find it:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来好像有很多步骤，但幸运的是我们不需要为此编写代码。Pavel Surmenok的[keras_lr_finder](https://oreil.ly/il_BI)库为我们提供了一个方便的函数来找到它：
- en: '[PRE25]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Figure 6-6](part0008.html#a_graph_showing_the_change_in_loss_as_th) shows
    the plot of loss versus learning rate. It becomes evident that a learning rate
    of 10^(–4) or 10^(–3) might be too low (owing to barely any drop in loss), and
    similarly, above 1 might be too high (because of the rapid increase in loss).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-6](part0008.html#a_graph_showing_the_change_in_loss_as_th)显示了损失与学习率之间的关系图。很明显，学习率为10^（-4）或10^（-3）可能太低（因为损失几乎没有下降），同样，大于1可能太高（因为损失急剧增加）。'
- en: '![A graph showing the change in loss as the learning rate is increased](../images/00175.jpeg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![显示损失随学习率增加而变化的图表](../images/00175.jpeg)'
- en: Figure 6-6\. A graph showing the change in loss as the learning rate is increased
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6。显示损失随学习率增加而变化的图表
- en: 'What we are most interested in is the point of the greatest decrease in loss.
    After all, we want to minimize the time we spend in getting to the least loss
    during training. In [Figure 6-7](part0008.html#a_graph_showing_the_rate_of_change_in_lo),
    we plot the *rate of change* of loss—the derivative of the loss with regard to
    the learning rate:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最感兴趣的是损失减少最多的点。毕竟，我们希望尽量减少在训练过程中达到最小损失所花费的时间。在[图6-7](part0008.html#a_graph_showing_the_rate_of_change_in_lo)中，我们绘制了损失的*变化速率*
    - 损失相对于学习率的导数：
- en: '[PRE26]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![A graph showing the rate of change in loss as the learning rate is increased](../images/00314.jpeg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![显示损失随学习率增加而变化的图表](../images/00314.jpeg)'
- en: Figure 6-7\. A graph showing the rate of change in loss as the learning rate
    is increased
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7。显示损失随学习率增加而变化的图表
- en: These figures show that values around 0.1 would lead to the fastest decrease
    in loss, and hence we would choose it as our optimal learning rate.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表显示，值在0.1左右会导致损失最快下降，因此我们会选择它作为我们的最佳学习率。
- en: Use tf.function
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用tf.function
- en: Eager execution mode, which is turned on by default in TensorFlow 2.0, allows
    users to execute code line by line and immediately see the results. This is immensely
    helpful in development and debugging. This is in contrast to TensorFlow 1.x, for
    which the user had to build all operations as a graph and then execute them in
    one go to see the results. This made debugging a nightmare!
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下在TensorFlow 2.0中启用的急切执行模式允许用户逐行执行代码并立即看到结果。这在开发和调试中非常有帮助。这与TensorFlow 1.x形成对比，对于TensorFlow
    1.x，用户必须将所有操作构建为图形，然后一次执行它们以查看结果。这使得调试成为一场噩梦！
- en: Does the added flexibility from eager execution come at a cost? Yes, a tiny
    one, typically in the order of microseconds, which can essentially be ignored
    for large compute-intensive operations, like training ResNet-50\. But where there
    are many small operations, eager execution can have a sizable impact.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 急切执行的灵活性是否会带来成本？是的，一个微小的成本，通常在微秒级别，对于像训练ResNet-50这样的大型计算密集型操作基本可以忽略不计。但是在有许多小操作的情况下，急切执行可能会产生较大的影响。
- en: 'We can overcome this by two approaches:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两种方法克服这个问题：
- en: Disabling eager execution
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用急切执行
- en: For TensorFlow 1.x, not enabling eager execution will let the system optimize
    the program flow as a graph and run it faster.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 对于TensorFlow 1.x，不启用急切执行将让系统优化程序流程作为图形并运行得更快。
- en: Use `tf.function`
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `tf.function`
- en: 'In TensorFlow 2.x, you cannot disable eager execution (there is a compatibility
    API, but we shouldn’t be using that for anything other than migration from TensorFlow
    1.x). Instead, any function that could benefit from a speedup by executing in
    graph mode can simply be annotated with `@tf.function`. It’s worth noting that
    any function that is called within an annotated function will also run in graph
    mode. This gives us the advantage of speedup from graph-based execution without
    sacrificing the debugging capabilities of eager execution. Typically, the best
    speedup is observed on short computationally intensive tasks:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow 2.x中，您无法禁用急切执行（有一个兼容性API，但我们不应该将其用于除了从TensorFlow 1.x迁移之外的任何其他用途）。相反，任何可以通过在图模式下执行来加速的函数可以简单地用`@tf.function`进行注释。值得注意的是，在注释函数内调用的任何函数也将在图模式下运行。这使我们能够从基于图形的执行中获得加速的优势，而不会牺牲急切执行的调试能力。通常，最佳加速是在短时间的计算密集型任务中观察到的：
- en: '[PRE27]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As we can see in our contrived example, simply attributing a function with `@tf.function`
    has given us a speedup of 10 times, from 7.2 seconds to 0.7 seconds.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在编造的例子中所看到的，简单地使用`@tf.function`给我们带来了10倍的加速，从7.2秒到0.7秒。
- en: Overtrain, and Then Generalize
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过度训练，然后泛化
- en: In machine learning, overtraining on a dataset is considered to be harmful.
    However, we will demonstrate that we can use overtraining in a controlled fashion
    to our advantage to make training faster.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，对数据集进行过度训练被认为是有害的。然而，我们将演示我们可以以受控的方式利用过度训练来使训练更快。
- en: As the saying goes, “The perfect is the enemy of the good.” We don’t want our
    network to be perfect right off the bat. In fact, we wouldn’t even want it to
    be any good initially. What we really want instead is for it to be learning *something*
    quickly, even if imperfectly. Because then we have a good baseline that we can
    fine tune to its highest potential. And experiments have shown that we can get
    to the end of the journey faster than training conventionally.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 俗话说，“完美是好的敌人。”我们不希望我们的网络一开始就完美。事实上，我们甚至不希望它一开始就很好。我们真正想要的是它能够快速学习*一些东西*，即使不完美。因为这样我们就有了一个良好的基线，可以将其调整到最高潜力。实验证明，我们可以比传统训练更快地到达目的地。
- en: Note
  id: totrans-263
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To further clarify the idea of overtraining and then generalizing, let’s look
    at an imperfect analogy of language learning. Suppose that you want to learn French.
    One way is to throw a book of vocabulary and grammar at you and expect you to
    memorize everything. Sure, you might go through the book every day and maybe in
    a few years, you might be able to speak some French. But this would not be the
    optimal way to learn.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步澄清过度训练然后泛化的概念，让我们看一个关于语言学习的不完美类比。假设您想学习法语。一种方法是向您抛出一本词汇和语法书，希望您能记住所有内容。当然，您可能每天都会翻阅这本书，也许几年后，您可能能说一些法语。但这不是学习的最佳方式。
- en: Alternatively, we could look at how language learning programs approach this
    process. These programs introduce you to only a small set of words and grammatical
    rules initially. After you have learned them, you will be able to speak some broken
    French. Maybe you could ask for a cup of coffee at a restaurant or ask for directions
    at a bus stop. At this point, you will be introduced constantly to a larger set
    of words and rules, and this will help you to improve over time.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以看看语言学习程序如何处理这个过程。这些程序最初只向您介绍一小部分单词和语法规则。学会它们后，您将能够说一些断断续续的法语。也许您可以在餐厅要一杯咖啡，或者在公交车站问路。此时，您将不断地接触到更多的单词和规则，这将帮助您随着时间的推移不断提高。
- en: This process is similar to how our model would learn gradually with more and
    more data.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程类似于我们的模型如何逐渐学习更多数据。
- en: How do we force a network to learn quickly and imperfectly? Make it overtrain
    on our data. The following three strategies can help.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何迫使网络快速而不完美地学习？让它在我们的数据上过度训练。以下三种策略可以帮助。
- en: Use progressive sampling
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用渐进采样
- en: 'One approach to overtrain and then generalize is to progressively show more
    and more of the original training set to the model. Here’s a simple implementation:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 一种过度训练然后泛化的方法是逐渐向模型展示越来越多原始训练集的内容。以下是一个简单的实现：
- en: Take a sample of the dataset (say, roughly 10%).
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据集中取样（比如大约 10%）。
- en: Train the network until it converges; in other words, until it begins to perform
    well on the training set.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练网络直到收敛；换句话说，直到在训练集上表现良好。
- en: Train on a larger sample (or even the entire training set).
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更大的样本上进行训练（甚至整个训练集）。
- en: By repeatedly showing a smaller sample of the dataset, the network will learn
    features much more quickly, but only related to the sample shown. Hence, it would
    tend to overtrain, usually performing better on the training set compared to the
    test set. When that happens, exposing the training process to the entire dataset
    will tend to generalize its learning, and eventually the test set performance
    would increase.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反复显示数据集的较小样本，网络将更快地学习特征，但只与显示的样本相关。因此，它往往会过度训练，通常在训练集上表现比测试集更好。当发生这种情况时，将训练过程暴露给整个数据集将有助于泛化学习，最终测试集的性能会提高。
- en: Use progressive augmentation
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用渐进增强
- en: Another approach is to train on the entire dataset with little to no data augmentation
    at first, and then progressively increase the degree of augmentation.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是首先在整个数据集上进行训练，几乎没有数据增强，然后逐渐增加增强程度。
- en: By showing the unaugmented images repeatedly, the network would learn patterns
    faster, and by progressively increasing the degree of augmentation, it would become
    more robust.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反复显示未增强的图像，网络会更快地学习模式，逐渐增加增强程度，使其更加稳健。
- en: Use progressive resizing
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用渐进调整大小
- en: Another approach, made famous by Jeremy Howard from fast.ai (which offers free
    courses on AI), is progressive resizing. The key idea behind this approach is
    to train first on images scaled down to smaller pixel size, and then progressively
    fine tune on larger and larger sizes until the original image size is reached.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法，由 fast.ai 的 Jeremy Howard 提出（该网站提供免费的人工智能课程），是渐进式调整大小。这种方法背后的关键思想是首先在缩小像素尺寸的图像上进行训练，然后逐渐在越来越大的尺寸上进行微调，直到达到原始图像尺寸。
- en: Images resized by half along both the width and height have a 75% reduction
    in pixels, and theoretically could lead to an increase in training speed of four
    times over the original images. Similarly, resizing to a quarter of the original
    height and width can in the best case lead to 16-times reduction (at a lower accuracy).
    Smaller images have fewer details visible, forcing the network to instead learn
    higher-level features including broad shapes and colors. Then, training with larger
    images will help the network learn the finer details, progressively increasing
    the test accuracy, as well. Just like a child is taught the high-level concepts
    first and then progressively exposed to more details in later years, the same
    concept is applied here to CNNs.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 宽度和高度都减半的图像像素减少了 75%，理论上可以使训练速度比原始图像提高四倍。类似地，将原始高度和宽度缩小到四分之一在最好的情况下可以导致 16 倍的减少（精度较低）。较小的图像显示的细节较少，迫使网络学习更高级的特征，包括广泛的形状和颜色。然后，使用较大的图像进行训练将有助于网络学习更精细的细节，逐渐提高测试精度。就像一个孩子首先学习高级概念，然后逐渐在后来的岁月中暴露于更多细节一样，这个概念也适用于
    CNN。
- en: Tip
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can experiment with a combination of any of these methods or even build
    your own creative methods such as training on a subset of classes and then generalizing
    to all the classes later.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试结合任何这些方法，甚至构建自己的创造性方法，比如首先在一部分类别上进行训练，然后逐渐推广到所有类别。
- en: Install an Optimized Stack for the Hardware
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为硬件安装优化的堆栈。
- en: Hosted binaries for open source packages are usually built to run on a variety
    of hardware and software configurations. These packages try to appeal to the least
    common denominator. When we do `pip install` on a package, we end up downloading
    and installing this general-purpose, works-for-everyone binary. This convenience
    comes at the expense of not being able to take advantage of the specific features
    offered by a particular hardware stack. This issue is one of the big reasons to
    avoid installing prebuilt binaries and instead opt for building packages from
    source.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 托管的开源软件包二进制文件通常是为了在各种硬件和软件配置上运行而构建的。这些软件包试图迎合最普遍的需求。当我们在软件包上执行 `pip install`
    时，我们最终会下载并安装这个通用的、适用于所有人的二进制文件。这种便利是以无法利用特定硬件堆栈提供的特定功能为代价的。这个问题是避免安装预构建二进制文件的一个重要原因，而是选择从源代码构建软件包。
- en: As an example, Google has a single TensorFlow package on `pip` that can run
    on an old Sandy Bridge (second-generation Core i3) laptop as well as a powerful
    16-core Intel Xeon server. Although convenient, the downside of this is that this
    package does not take advantage of the highly powerful hardware of the Xeon server.
    Hence, for CPU-based training and inference, Google recommends compiling TensorFlow
    from source to best optimize for the hardware at hand.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Google在`pip`上有一个单独的TensorFlow包，可以在旧的Sandy Bridge（第二代Core i3）笔记本电脑上运行，也可以在强大的16核Intel
    Xeon服务器上运行。尽管方便，但这种方法的缺点是该软件包无法充分利用Xeon服务器的强大硬件。因此，对于基于CPU的训练和推断，Google建议从源代码编译TensorFlow以最佳优化手头的硬件。
- en: 'One way to do this manually is by setting the configuration flags for the hardware
    before building the source code. For example, to enable support for AVX2 and SSE
    4.2 instruction sets, we can simply execute the following build command (note
    the extra `m` character ahead of each instruction set in the command):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 手动执行此操作的一种方法是在构建源代码之前设置硬件的配置标志。例如，要启用AVX2和SSE 4.2指令集的支持，我们可以简单地执行以下构建命令（请注意命令中每个指令集前面的额外`m`字符）：
- en: '[PRE29]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'How do you check which CPU features are available? Use the following command
    (Linux only):'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如何检查可用的CPU特性？使用以下命令（仅限Linux）：
- en: '[PRE30]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Building TensorFlow from source with the appropriate instruction set specified
    as build flags should result in a substantial increase in speed. The downside
    here is that building from source can take quite some time, at least a couple
    of hours. Alternatively, we can use Anaconda to download and install a highly
    optimized variant of TensorFlow, built by Intel on top of their Math Kernel Library
    for Deep Neural Networks (MKL-DNN). The installation process is pretty straightforward.
    First, we install the [Anaconda](https://anaconda.com) package manager. Then,
    we run the following command:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用适当的指令集指定构建标志从源代码构建TensorFlow应该会显著提高速度。这里的缺点是从源代码构建可能需要相当长的时间，至少几个小时。或者，我们可以使用Anaconda下载和安装由英特尔在他们的深度神经网络数学核心库（MKL-DNN）之上构建的高度优化的TensorFlow变体。安装过程非常简单。首先，我们安装[Anaconda](https://anaconda.com)包管理器。然后，我们运行以下命令：
- en: '[PRE31]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: On Xeon CPUs, MKL-DNN often provides upward of two-times speedup in inference.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在Xeon CPU上，MKL-DNN通常可以提供推理速度提升两倍以上。
- en: How about optimization for GPUs? Because NVIDIA abstracts away the differences
    between the various GPU internals with the CUDA library, there is usually no need
    to build from source. Instead, we could simply install a GPU variant of TensorFlow
    from `pip` (`tensorflow-gpu` package). We recommend the [Lambda Stack](https://oreil.ly/4AUxp)
    one-liner installer for convenience (along with NVIDIA drivers, CUDA, and cuDNN).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 关于GPU的优化怎么样？因为NVIDIA使用CUDA库来抽象化各种GPU内部的差异，通常不需要从源代码构建。相反，我们可以简单地从`pip`（`tensorflow-gpu`包）安装GPU变体的TensorFlow。我们推荐使用[Lambda
    Stack](https://oreil.ly/4AUxp)一键安装程序以方便安装（还包括NVIDIA驱动程序、CUDA和cuDNN）。
- en: For training and inference on the cloud, AWS, Microsoft Azure, and GCP all provide
    GPU machine images of TensorFlow optimized for their hardware. It’s quick to spin
    up multiple instances and get started. Additionally, NVIDIA offers GPU-accelerated
    containers for on-premises and cloud setups.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 对于云端的训练和推断，AWS、微软Azure和GCP都提供了针对其硬件优化的TensorFlow GPU机器映像。快速启动多个实例并开始使用。此外，NVIDIA还提供了用于本地和云端设置的GPU加速容器。
- en: Optimize the Number of Parallel CPU Threads
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化并行CPU线程的数量
- en: 'Compare the following two examples:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 比较以下两个例子：
- en: '[PRE32]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'There are a couple of areas in these examples where we can exploit inherent
    parallelism:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些例子中有几个领域可以利用内在的并行性：
- en: Between operations
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作之间
- en: In example 1, the calculation of Y does not depend on the calculation of X.
    This is because there is no shared data between those two operations, and thus
    both of them can execute in parallel on two separate threads.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在例子1中，Y的计算不依赖于X的计算。这是因为这两个操作之间没有共享数据，因此它们都可以在两个单独的线程上并行执行。
- en: In contrast, in example 2, the calculation of Y depends on the outcome of the
    first operation (X), and so the second statement cannot execute until the first
    statement completes execution.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在例子2中，Y的计算取决于第一个操作（X）的结果，因此第二个语句在第一个语句完成执行之前无法执行。
- en: 'The configuration for the maximum number of threads that can be used for interoperation
    parallelism is set using the following statement:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 用以下语句设置可用于操作间并行性的最大线程数配置：
- en: '[PRE33]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The recommended number of threads is equal to the number of CPUs (sockets) on
    the machine. This value can be obtained by using the `lscpu` command (Linux only).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的线程数等于机器上的CPU（插槽）数量。可以使用`lscpu`命令（仅限Linux）获取此值。
- en: Per-operation level
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 每个操作级别
- en: We can also exploit the parallelism within a single operation. Operations such
    as matrix multiplications are inherently parallelizable.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以利用单个操作内的并行性。诸如矩阵乘法之类的操作本质上是可并行化的。
- en: '[Figure 6-8](part0008.html#a_matrix_multiplication_for_a_x_b_operat) demonstrates
    a simple matrix multiplication operation. It’s clear that the overall product
    can be split into four independent calculations. After all, the product between
    one row of a matrix and one column of another matrix does not depend on the calculations
    for the other rows and columns. Each of those splits could potentially get its
    own thread and all four of them could execute at the same time.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-8](part0008.html#a_matrix_multiplication_for_a_x_b_operat)展示了一个简单的矩阵乘法操作。很明显，整体乘积可以分为四个独立的计算。毕竟，一个矩阵的一行与另一个矩阵的一列的乘积不依赖于其他行和列的计算。每个这样的分割都可能获得自己的线程，所有这四个线程可以同时执行。'
- en: '![A matrix multiplication for A x B operation with one of the multiplications
    highlighted](../images/00088.jpeg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![一个矩阵乘法的A x B操作，其中一个乘法被突出显示](../images/00088.jpeg)'
- en: Figure 6-8\. A matrix multiplication for A x B operation with one of the multiplications
    highlighted
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8\. A matrix multiplication for A x B operation with one of the multiplications
    highlighted
- en: 'The configuration for the number of threads that can be used for intraoperation
    parallelism is set using the following statement:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 用以下语句设置用于操作内并行性的线程数配置：
- en: '[PRE34]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The recommended number of threads is equal to the number of cores per CPU. You
    can obtain this value by using the `lscpu` command on Linux.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的线程数等于每个CPU的核心数。你可以使用Linux上的`lscpu`命令获取这个值。
- en: Use Better Hardware
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用更好的硬件
- en: If you have already maximized performance optimizations and still need faster
    training, you might be ready for some new hardware. Replacing spinning hard drives
    with SSDs can go a long way, as can adding one or more better GPUs. And let’s
    not forget, sometimes the CPU can be the culprit.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经最大化了性能优化，但仍需要更快的训练速度，那么你可能已经准备好购买一些新的硬件。用固态硬盘替换旋转硬盘可以走很远，添加一个或多个更好的GPU也可以。还有，有时CPU可能是罪魁祸首。
- en: 'In fact, you might not need to spend much money: public clouds like AWS, Azure,
    and GCP all provide the ability to rent powerful configurations for a few dollars
    per hour. Best of all, they come with optimized TensorFlow stacks preinstalled.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你可能不需要花太多钱：像AWS、Azure和GCP这样的公共云都提供了租用强大配置的能力，每小时只需几美元。最重要的是，它们都预装了优化的TensorFlow堆栈。
- en: Of course, if you have the cash to spend or have a rather generous expense account,
    you could just skip this entire chapter and buy the 2-petaFLOPS NVIDIA DGX-2\.
    Weighing in at 163 kgs (360 pounds), its 16 V100 GPUs (with a total of 81,920
    CUDA cores) consume 10 kW of power—the equivalent of seven large window air conditioners.
    And all it costs is $400,000!
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你有足够的资金或者有一个相当慷慨的费用账户，你可以直接跳过整个章节，购买2-petaFLOPS的NVIDIA DGX-2。它重达163公斤（360磅），其16个V100
    GPU（共81920个CUDA核心）消耗10千瓦的功率——相当于七台大型窗式空调。而且它的售价只有40万美元！
- en: '![The $400,000 NVIDIA DGX-2 deep learning system](../images/00303.jpeg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![价值40万美元的NVIDIA DGX-2深度学习系统](../images/00303.jpeg)'
- en: Figure 6-9\. The $400,000 NVIDIA DGX-2 deep learning system
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-9. 价值40万美元的NVIDIA DGX-2深度学习系统
- en: Distribute Training
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分发训练
- en: “*Two lines to scale training horizontally!*”
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: “*两行代码实现训练水平扩展！*”
- en: On a single machine with a single GPU, there’s only so far that we can go. Even
    the beefiest GPUs have an upper limit in compute power. Vertical scaling can take
    us only so far. Instead, we look to scale horizontally—distribute computation
    across processors. We can do this across multiple GPUs, TPUs, or even multiple
    machines. In fact, that is exactly what researchers at Google Brain did back in
    2012, using 16,000 processors to run a neural network built to look at cats on
    YouTube.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在单台只有一个GPU的机器上，我们只能走得这么远。即使是最强大的GPU在计算能力上也有上限。垂直扩展只能带我们走到这么远。相反，我们寻求水平扩展——在处理器之间分发计算。我们可以在多个GPU、TPU甚至多台机器之间进行这样的操作。事实上，这正是Google
    Brain的研究人员在2012年所做的，他们使用了16000个处理器来运行一个用于观看YouTube上猫的神经网络。
- en: 'In the dark days of the early 2010s, training on ImageNet used to take anywhere
    from several weeks to months. Multiple GPUs would speed things up, but few people
    had the technical know-how to configure such a setup. It was practically out of
    reach for beginners. Luckily, we live in the day of TensorFlow 2.0, in which setting
    up distributed training is a matter of introducing two lines of code:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在2010年代初的黑暗时期，对ImageNet的训练通常需要几周甚至几个月的时间。多个GPU可以加快速度，但很少有人有技术知识来配置这样的设置。对于初学者来说，这几乎是不可能的。幸运的是，我们生活在TensorFlow
    2.0的时代，设置分布式训练只需要引入两行代码：
- en: '[PRE35]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Training speed increases nearly proportionally (90–95%) in relation to the number
    of GPUs added. As an example, if we added four GPUs (of similar compute power),
    we would notice an increase of >3.6 times speedup ideally.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 训练速度几乎与添加的GPU数量成比例增加（90-95%）。例如，如果我们添加了四个GPU（具有相似的计算能力），理想情况下我们会注意到速度提高了超过3.6倍。
- en: Still, a single system can only support a limited number of GPUs. How about
    multiple nodes, each with multiple GPUs? Similar to `MirroredStrategy`, we can
    use `Multi``Worker``MirroredStrategy`. This is quite useful when building a cluster
    on the cloud. [Table 6-1](part0008.html#recommended_distribution_strategies) presents
    a couple of distribution strategies for different use cases.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单个系统只能支持有限数量的GPU。那么多个节点，每个节点都有多个GPU呢？类似于`MirroredStrategy`，我们可以使用`MultiWorkerMirroredStrategy`。在构建云上集群时，这非常有用。[表6-1](part0008.html#recommended_distribution_strategies)展示了不同用例的几种分发策略。
- en: Table 6-1\. Recommended distribution strategies
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-1. 推荐的分发策略
- en: '| **Strategy** | **Use case** |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| **策略** | **用例** |'
- en: '| --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `MirroredStrategy` | Single node with two or more GPUs |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| `MirroredStrategy` | 单个节点有两个或更多GPU |'
- en: '| `MultiWorkerMirroredStrategy` | Multiple nodes with one or more GPUs each
    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| `MultiWorkerMirroredStrategy` | 每个节点有一个或多个GPU |'
- en: To get the cluster nodes to communicate with one another for `MultiWorkerMirroredStrategy`,
    we need to configure the `TF_CONFIG` environment variable on every single host.
    This requires setting up a JSON object that contains the IP addresses and ports
    of all other hosts in the cluster. Manually managing this can be error prone,
    and this is where orchestration frameworks like Kubernetes really shine.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让集群节点之间能够相互通信以使用`MultiWorkerMirroredStrategy`，我们需要在每个主机上配置`TF_CONFIG`环境变量。这需要设置一个包含集群中所有其他主机的IP地址和端口的JSON对象。手动管理这个过程可能会出错，这就是Kubernetes等编排框架真正发挥作用的地方。
- en: Note
  id: totrans-331
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The open source Horovod library from Uber is another high-performance and easy-to-use
    distribution framework. Many of the record benchmark performances seen in the
    next section require distributed training on several nodes, and Horovod’s performance
    helped them get the edge. It is worth noting that the majority of the industry
    uses Horovod particularly because distributed training on earlier versions of
    TensorFlow was a much more involved process. Additionally, Horovod works with
    all major deep learning libraries with minimal amount of code change or expertise.
    Often configured through the command line, running a distributed program on four
    nodes, each with four GPUs, can be done in a single command line:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Uber的开源Horovod库是另一个高性能且易于使用的分布框架。在下一节中看到的许多记录基准性能需要在多个节点上进行分布式训练，而Horovod的性能帮助它们获得了优势。值得注意的是，大多数行业使用Horovod，特别是因为在早期版本的TensorFlow上进行分布式训练是一个更加复杂的过程。此外，Horovod可以与所有主要的深度学习库一起工作，只需进行最少量的代码更改或专业知识。通常通过命令行配置，可以通过单个命令行在四个节点上运行一个分布式程序，每个节点有四个GPU：
- en: '[PRE36]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Examine Industry Benchmarks
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查行业基准测试
- en: Three things were universally popular in the 1980s—long hair, the Walkman, and
    database benchmarks. Much like the current hype of deep learning, database software
    was similarly going through a phase of making bold promises, some of which were
    marketing hype. To put these companies to the test, a few benchmarks were introduced,
    more famously among them was the Transaction Processing Council (TPC) benchmark.
    When someone needed to buy database software, they could rely on this public benchmark
    to decide where to spend their company’s budget. This competition fueled rapid
    innovation, increasing speed and performance per dollar, moving the industry ahead
    faster than anticipated.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 上世纪80年代有三样东西普遍受欢迎——长发、随身听和数据库基准测试。就像当前深度学习的热潮一样，数据库软件也在经历着一段大胆承诺的阶段，其中一些是营销炒作。为了对这些公司进行测试，引入了一些基准测试，其中最著名的是事务处理委员会（TPC）基准测试。当有人需要购买数据库软件时，他们可以依靠这个公共基准测试来决定在哪里花费公司的预算。这种竞争推动了快速创新，提高了每美元的速度和性能，使行业比预期更快地前进。
- en: Inspired by TPC and other benchmarks, a few system benchmarks were created to
    standardize performance reporting in machine learning.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 受TPC和其他基准测试的启发，一些系统基准测试被创建出来，以标准化机器学习性能报告。
- en: DAWNBench
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: DAWNBench
- en: Stanford’s DAWNBench benchmarks time and cost to get a model to 93% Top-5 accuracy
    on ImageNet. Additionally, it also does a time and cost leaderboard on inference
    time. It’s worth appreciating the rapid pace of performance improvement for training
    such a massive network. When DAWNBench originally started in September 2017, the
    reference entry trained in 13 days at a cost of $2,323.39\. In just one and a
    half years since then, although the cheapest training costs as low as $12, the
    fastest training time is 2 minutes 43 seconds. Best of all, most entries contain
    the training source code and optimizations that can be studied and replicated
    by us. This gives further guidance on the effects of hyperparameters and how we
    can use the cloud for cheap and fast training without breaking the bank.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福的DAWNBench基准测试了在ImageNet上将模型训练到93% Top-5准确率所需的时间和成本。此外，它还对推理时间进行了时间和成本排行榜。值得赞赏的是，训练如此庞大的网络的性能改进速度之快。当DAWNBench最初于2017年9月开始时，参考条目在13天内以2323.39美元的成本进行了训练。从那时起仅仅一年半的时间里，尽管最便宜的训练成本低至12美元，最快的训练时间是2分钟43秒。最重要的是，大多数条目包含了训练源代码和优化，我们可以通过研究和复制来进一步指导。这进一步说明了超参数的影响以及我们如何利用云进行廉价和快速的训练而不会让银行破产。
- en: Table 6-2\. Entries on DAWNBench as of August 2019, sorted by the least cost
    for training a model to 93% Top-5 accuracy
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-2。截至2019年8月的DAWNBench条目，按将模型训练到93% Top-5准确率的最低成本排序
- en: '| **Cost (USD)** | **Training time** | **Model** | **Hardware** | **Framework**
    |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| **成本（美元）** | **训练时间** | **模型** | **硬件** | **框架** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| $12.60 | 2:44:31 | ResNet-50Google Cloud TPU | GCP n1-standard-2, Cloud TPU
    | TensorFlow 1.11 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| $12.60 | 2:44:31 | ResNet-50Google Cloud TPU | GCP n1-standard-2, Cloud TPU
    | TensorFlow 1.11 |'
- en: '| $20.89 | 1:42:23 | ResNet-50Setu Chokshi(MS AI MVP) | Azure ND40s_v2 | PyTorch
    1.0 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| $20.89 | 1:42:23 | ResNet-50Setu Chokshi(MS AI MVP) | Azure ND40s_v2 | PyTorch
    1.0 |'
- en: '| $42.66 | 1:44:34 | ResNet-50 v1GE Healthcare(Min Zhang) | 8*V100 (single
    p3.16x large) | TensorFlow 1.11 + Horovod |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| $42.66 | 1:44:34 | ResNet-50 v1GE Healthcare(Min Zhang) | 8*V100（单个p3.16x大）
    | TensorFlow 1.11 + Horovod |'
- en: '| $48.48 | 0:29:43 | ResNet-50Andrew Shaw, Yaroslav Bulatov, Jeremy Howard
    | 32 * V100(4x - AWS p3.16x large) | Ncluster + PyTorch 0.5 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| $48.48 | 0:29:43 | ResNet-50Andrew Shaw, Yaroslav Bulatov, Jeremy Howard
    | 32 * V100（4x - AWS p3.16x大） | Ncluster + PyTorch 0.5 |'
- en: MLPerf
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: MLPerf
- en: 'Similar to DAWNBench, MLPerf is aimed at repeatable and fair testing of AI
    system performance. Although newer than DAWNBench, this is an industry consortium
    with much wider support, especially on the hardware side. It runs challenges for
    both training and inference in two divisions: open and closed. The closed division
    trains the same model with the same optimizers, so the raw hardware performance
    can be compared apples-to-apples. The open division, on the other hand, allows
    using faster models and optimizers to allow for more rapid progress. Compared
    to the more cost-effective entries in DAWNBench in [Table 6-2](part0008.html#entries_on_dawnbench_as_of_august_2019co),
    the top performers on MLPerf as shown in [Table 6-3](part0008.html#key_closed-division_entries_on_dawnbench)
    might be a bit out of reach for most of us. The top-performing NVIDIA DGX SuperPod,
    composed of 96 DGX-2H with a total of 1,536 V100 GPUs, costs in the $35 to $40
    million range. Even though 1,024 Google TPUs might themselves cost in the several
    millions, they are each available to rent on the cloud at $8/hour on-demand pricing
    (as of August 2019), resulting in a net cost of under $275 for the less-than two
    minutes of training time.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 与DAWNBench类似，MLPerf旨在对人工智能系统性能进行可重复和公平的测试。虽然比DAWNBench更新，但这是一个行业联盟，在硬件方面得到了更广泛的支持。它在两个分区中进行训练和推断的挑战：开放和闭合。闭合分区使用相同的模型和优化器进行训练，因此可以将原始硬件性能进行苹果对苹果的比较。另一方面，开放分区允许使用更快的模型和优化器，以实现更快的进展。与DAWNBench中更具成本效益的条目相比，在MLPerf中表现最佳的参与者可能对我们大多数人来说有些难以企及。性能最佳的NVIDIA
    DGX SuperPod，由96个DGX-2H组成，共计1,536个V100 GPU，价格在3500万至4000万美元的范围内。尽管1024个Google
    TPU本身可能价值数百万美元，但它们每个都可以按需在云上租用，价格为每小时8美元（截至2019年8月），导致不到275美元的净成本用于不到两分钟的训练时间。
- en: Table 6-3\. Key closed-division entries on DAWNBench as of August 2019, showing
    training time for a ResNet-50 model to get to 75.9% Top-1 accuracy
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-3. 截至2019年8月DAWNBench上的关键闭式分区条目，显示ResNet-50模型达到75.9% Top-1准确率的训练时间
- en: '| **Time (minutes)** | **Submitter** | **Hardware** | **Accelerator** | **#
    of accelerators** |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| **时间（分钟）** | **提交者** | **硬件** | **加速器** | **加速器数量** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1.28 | Google | TPUv3 | TPUv3 | 1,024 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 1.28 | Google | TPUv3 | TPUv3 | 1,024 |'
- en: '| 1.33 | NVIDIA | 96x DGX-2H | Tesla V100 | 1,536 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 1.33 | NVIDIA | 96x DGX-2H | Tesla V100 | 1,536 |'
- en: '| 8,831.3 | Reference | Pascal P100 | Pascal P100 | 1 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 8,831.3 | 参考 | Pascal P100 | Pascal P100 | 1 |'
- en: 'Although both the aforementioned benchmarks highlight training as well as inference
    (usually on more powerful devices), there are other inference-specific competitions
    on low-power devices, with the aim to maximize accuracy and speed while reducing
    power consumption. Held at annual conferences, here are some of these competitions:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述两个基准测试都强调训练和推断（通常在更强大的设备上），但还有其他针对低功耗设备的推断特定竞赛，旨在最大化准确性和速度，同时降低功耗。在年度会议上举行，以下是其中一些比赛：
- en: 'LPIRC: Low-Power Image Recognition Challenge'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LPIRC：低功耗图像识别挑战
- en: 'EDLDC: Embedded Deep Learning Design Contest'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EDLDC：嵌入式深度学习设计竞赛
- en: System Design Contest at Design Automation Conference (DAC)
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计自动化会议（DAC）的系统设计比赛
- en: Inference
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推断
- en: Training our model is only half the game. We eventually need to serve the predictions
    to our users. The following points guide you to making your serving side more
    performant.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 训练我们的模型只是游戏的一半。我们最终需要向用户提供预测。以下几点指导您使服务端更具性能。
- en: Use an Efficient Model
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用高效的模型
- en: Deep learning competitions have traditionally been a race to come up with the
    highest accuracy model, get on top of the leaderboard, and get the bragging rights.
    But practitioners live in a different world—the world of serving their users quickly
    and efficiently. With devices like smartphones, edge devices, and servers with
    thousands of calls per second, being efficient on all fronts (model size and computation)
    is critically needed. After all, many machines would not be capable of serving
    a half gigabyte VGG-16 model, which happens to need 30 billion operations to execute,
    for not even that high of accuracy. Among the wide variety of pretrained architectures
    available, some are on the higher end of accuracy but large and resource intensive,
    whereas others provide modest accuracy but are much lighter. Our goal is to pick
    the architecture that can deliver the highest accuracy for the available computational
    power and memory budget of our inference device. In [Figure 6-10](part0008.html#comparing_different_models_for_sizecomma),
    we want to pick models in the upper-left zone.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习竞赛传统上是为了提出最高准确性模型，登上排行榜并获得炫耀权。但从业者生活在一个不同的世界——为用户快速高效地提供服务的世界。在智能手机、边缘设备和每秒数千次调用的服务器等设备上，全面高效（模型大小和计算）至关重要。毕竟，许多设备可能无法提供半个千兆字节的VGG-16模型，该模型需要执行300亿次操作，甚至没有那么高的准确性。在众多预训练架构中，有些准确性较高但较大且资源密集，而其他一些提供适度准确性但更轻。我们的目标是选择可以在推断设备的可用计算能力和内存预算下提供最高准确性的架构。在[图6-10](part0008.html#comparing_different_models_for_sizecomma)中，我们希望选择位于左上角区域的模型。
- en: '![Comparing different models for size, accuracy, and operations per second
    (adapted from “An Analysis of Deep Neural Network Models for Practical Applications”
    by Alfredo Canziani, Adam Paszke and Eugenio Culurciello)](../images/00008.jpeg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![比较不同模型的大小、准确性和每秒操作次数（改编自Alfredo Canziani、Adam Paszke和Eugenio Culurciello的“深度神经网络模型在实际应用中的分析”）](../images/00008.jpeg)'
- en: Figure 6-10\. Comparing different models for size, accuracy, and operations
    per second (adapted from “An Analysis of Deep Neural Network Models for Practical
    Applications” by Alfredo Canziani, Adam Paszke, and Eugenio Culurciello)
  id: totrans-363
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-10. 比较不同模型的大小、准确性和每秒操作次数（改编自Alfredo Canziani、Adam Paszke和Eugenio Culurciello的“深度神经网络模型在实际应用中的分析”）
- en: Usually, the approximately 15 MB MobileNet family is the go-to model for efficient
    smartphone runtimes, with more recent versions like MobileNetV2 and MobileNetV3
    being better than their predecessors. Additionally, by varying the hyperparameters
    of the MobileNet models like depth multiplier, the number of computations can
    be further reduced, making it ideal for real-time applications. Since 2017, the
    task of generating the most optimal architecture to maximize accuracy has also
    been automated with NAS. It has helped discover new (rather obfuscated looking)
    architectures that have broken the ImageNet accuracy metric multiple times. For
    example, FixResNeXt (based on PNASNet architecture at 829 MB) reaches a whopping
    86.4% Top-1 accuracy on ImageNet. So, it was natural for the research community
    to ask whether NAS helps find architecture that’s tuned for mobile, maximizing
    accuracy while minimizing computations. The answer is a resounding yes—resulting
    in faster and better models, optimized for the hardware at hand. As an example,
    MixNet (July 2019) outperforms many state-of-the-art models. Note how we went
    from billions of floating-point operations to millions ([Figure 6-10](part0008.html#comparing_different_models_for_sizecomma)
    and [Figure 6-11](part0008.html#comparison_of_several_mobile-friendly_mo)).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，约15 MB的MobileNet系列是高效智能手机运行时的首选模型，更近期的版本如MobileNetV2和MobileNetV3比它们的前身更好。此外，通过改变MobileNet模型的超参数，如深度乘数，可以进一步减少计算量，使其成为实时应用的理想选择。自2017年以来，生成最优架构以最大化准确性的任务也已经通过NAS自动化。它帮助发现了多次打破ImageNet准确性指标的新（看起来相当晦涩的）架构。例如，基于829
    MB的PNASNet架构的FixResNeXt在ImageNet上达到了惊人的86.4%的Top-1准确性。因此，研究界自然会问NAS是否有助于找到针对移动设备调整的架构，最大化准确性同时最小化计算量。答案是肯定的——导致更快更好的模型，优化了手头的硬件。例如，MixNet（2019年7月）胜过许多最先进的模型。请注意，我们从数十亿的浮点运算转变为数百万次（[图6-10](part0008.html#comparing_different_models_for_sizecomma)和[图6-11](part0008.html#comparison_of_several_mobile-friendly_mo)）。
- en: '![Comparison of several mobile-friendly models in the paper “MixNet: Mixed
    Depthwise Convolution Kernels” by Mingxing Tan and Quoc V. Le](../images/00172.jpeg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![由Mingxing Tan和Quoc V. Le撰写的论文“MixNet:混合深度卷积核”中的几个移动友好模型的比较](../images/00172.jpeg)'
- en: 'Figure 6-11\. Comparison of several mobile-friendly models in the paper “MixNet:
    Mixed Depthwise Convolution Kernels” by Mingxing Tan and Quoc V. Le'
  id: totrans-366
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11。由Mingxing Tan和Quoc V. Le撰写的论文“MixNet:混合深度卷积核”中的几个移动友好模型的比较
- en: As practitioners, where can we find current state-of-the-art models? *PapersWithCode.com/SOTA*
    showcases leaderboards on several AI problems, comparing paper results over time,
    along with the model code. Of particular interest would be the models with a low
    number of parameters that achieve high accuracies. For example, EfficientNet gets
    an amazing Top-1 84.4% accuracy with 66 million parameters, so it could be an
    ideal candidate for running on servers. Additionally, the ImageNet test metrics
    are on 1,000 classes, whereas our case might just require classification on a
    few classes. For those cases, a much smaller model would suffice. Models listed
    in Keras Application (*tf.keras.applications*), TensorFlow Hub, and TensorFlow
    Models usually carry many variations (input image sizes, depth multipliers, quantizations,
    etc.).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 作为从业者，我们在哪里可以找到当前最先进的模型？*PapersWithCode.com/SOTA*展示了几个AI问题的排行榜，随着时间的推移比较了论文结果，以及模型代码。特别感兴趣的是那些参数数量少但准确率高的模型。例如，EfficientNet以6600万参数获得了惊人的Top-1
    84.4%的准确率，因此它可能是在服务器上运行的理想选择。此外，ImageNet测试指标是在1,000个类别上，而我们的情况可能只需要对少数类别进行分类。对于这些情况，一个更小的模型就足够了。列在Keras
    Application（*tf.keras.applications*）、TensorFlow Hub和TensorFlow Models中的模型通常有许多变体（输入图像尺寸、深度乘数、量化等）。
- en: Tip
  id: totrans-368
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Shortly after Google AI researchers publish a paper, they release the model
    used in the paper on the [TensorFlow Models](https://oreil.ly/Piq40) repository.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌AI研究人员发表论文后不久，他们会在[TensorFlow Models](https://oreil.ly/Piq40)存储库上发布论文中使用的模型。
- en: Quantize the Model
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化模型
- en: “*Represent 32-bit weights to 8-bit integer, get 2x faster, 4x smaller models*”
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: “*将32位权重表示为8位整数，获得2倍更快，4倍更小的模型*”
- en: Neural networks are driven primarily by matrix–matrix multiplications. The arithmetic
    involved tends to be rather forgiving in that small deviations in values do not
    cause a significant swing in output. This makes neural networks fairly robust
    to noise. After all, we want to be able to recognize an apple in a picture, even
    in less-than-perfect lighting. When we quantize, we essentially take advantage
    of this “forgiving” nature of neural networks.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络主要由矩阵-矩阵乘法驱动。所涉及的算术通常相当宽容，即数值上的小偏差不会导致输出的显著波动。这使得神经网络对噪声相当稳健。毕竟，我们希望能够在图片中识别出一个苹果，即使在不太完美的光线下也能做到。当我们进行量化时，实质上是利用了神经网络的这种“宽容”特性。
- en: 'Before we look at the different quantization techniques, let’s first try to
    build an intuition for it. To illustrate quantized representations with a simple
    example, we’ll convert 32-bit floating-point weights to INT8 (8-bit integer) using
    *linear quantization*. Obviously, FP32 represents 2^(32) values (hence, 4 bytes
    to store), whereas INT8 represents 2⁸ = 256 values (1 byte). To quantize:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看不同的量化技术之前，让我们先试着建立对它的直觉。为了用一个简单的例子说明量化表示，我们将把32位浮点权重转换为INT8（8位整数）使用*线性量化*。显然，FP32表示2^(32)个值（因此需要4个字节来存储），而INT8表示2⁸
    = 256个值（1个字节）。进行量化：
- en: Find the minimum and maximum values represented by FP32 weights in the neural
    network.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出神经网络中FP32权重所代表的最小值和最大值。
- en: Divide this range into 256 intervals, each corresponding to an INT8 value.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此范围分为256个间隔，每个间隔对应一个INT8值。
- en: Calculate a scaling factor that converts an INT8 (integer) back to a FP32\.
    For example, if our original range is from 0 to 1, and INT8 numbers are 0 to 255,
    the scaling factor will be 1/256.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算一个将INT8（整数）转换回FP32的缩放因子。例如，如果我们的原始范围是从0到1，而INT8数字是0到255，则缩放因子将为1/256。
- en: Replace FP32 numbers in each interval with the INT8 value. Additionally, store
    the scaling factor for the inference stage where we convert INT8 values back to
    FP32 values. This scaling factor only needs to be stored once for the entire group
    of quantized values.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个区间中用INT8值替换FP32数字。此外，在推理阶段存储缩放因子，用于将INT8值转换回FP32值。这个缩放因子只需要为整个量化值组存储一次。
- en: During inference calculations, multiply the INT8 values by the scaling factor
    to convert it back to a floating-point representation. [Figure 6-12](part0008.html#quantizing_from_a_0_to_1_32-bit_floating)
    illustrates an example of linear quantization for the interval [0, 1].
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在推理计算期间，将INT8值乘以缩放因子以将其转换回浮点表示。[图6-12](part0008.html#quantizing_from_a_0_to_1_32-bit_floating)展示了线性量化的一个例子，区间为[0,
    1]。
- en: '![Quantizing from a 0 to 1 32-bit floating-point range down to 8-bit integer
    range for reduced storage space](../images/00254.jpeg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![将从0到1的32位浮点范围量化为8位整数范围，以减少存储空间](../images/00254.jpeg)'
- en: Figure 6-12\. Quantizing from a 0 to 1 32-bit floating-point range down to an
    8-bit integer range for reduced storage space
  id: totrans-380
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12。将从0到1的32位浮点范围量化为8位整数范围，以减少存储空间
- en: There are a few different ways to quantize our models, the simplest one being
    reducing the bit representation of the weights from 32-bit to 16-bit or lower.
    As might be evident, converting 32-bit to 16-bit means half the memory size is
    needed to store a model. Similarly, converting to 8-bit would require a quarter
    of the size. So why not convert it to 1-bit and save 32x the size? Well, although
    the models are forgiving up to a certain extent, with each reduction, we will
    notice a drop in accuracy. This reduction in accuracy grows exponentially beyond
    a certain threshold (especially below 8 bits). To go below and still have a useful
    working model (like a 1-bit representation), we’d need to follow a special conversion
    process to convert them to binarized neural networks. XNOR.ai, a deep learning
    startup, has famously been able to bring this technique to production. The Microsoft
    Embedded Learning Library (ELL) similarly provides such tools, which have a lot
    of value for edge devices like the Raspberry Pi.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的方法可以量化我们的模型，最简单的方法是将权重的位表示从32位减少到16位或更低。显而易见，将32位转换为16位意味着需要一半的内存大小来存储模型。同样，转换为8位将需要四分之一的大小。那么为什么不将其转换为1位并节省32倍的大小呢？嗯，尽管模型在一定程度上是宽容的，但随着每次减少，我们会注意到精度下降。在某个阈值之下，精度的降低呈指数增长（特别是在8位以下）。要在下面并仍然拥有一个有用的工作模型（如1位表示），我们需要遵循一个特殊的转换过程将它们转换为二值化神经网络。深度学习初创公司XNOR.ai已经成功将这种技术引入生产。微软嵌入式学习库（ELL）同样提供了这样的工具，对于像树莓派这样的边缘设备具有很大的价值。
- en: 'There are numerous benefits to quantization:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 量化有许多好处：
- en: Improved memory usage
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 改进的内存使用
- en: By quantizing to 8-bit integer representation (INT8), we typically get a 75%
    reduction in model size. This makes it more convenient to store and load the model
    in memory.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将模型量化为8位整数表示（INT8），我们通常可以减少75%的模型大小。这使得在内存中存储和加载模型更加方便。
- en: Improved performance
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 性能改善
- en: Integer operations are faster than floating-point operations. Additionally,
    the savings in memory usage reduces the likelihood of having to unload the model
    from RAM during execution, which also has the added benefit of decreased power
    consumption.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 整数运算比浮点运算更快。此外，内存使用的节省减少了在执行期间从RAM卸载模型的可能性，这也附带减少了功耗消耗的好处。
- en: Portability
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 可移植性
- en: Edge devices such as Internet of Things devices might not support floating-point
    arithmetic, so it would be untenable to keep the model as a floating-point in
    such situations.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘设备，如物联网设备，可能不支持浮点运算，因此在这种情况下将模型保持为浮点是不可行的。
- en: Most inference frameworks provide a way to quantize, including Core ML Tools
    from Apple, TensorRT from NVIDIA (for servers), and TensorFlow Lite, as well as
    the TensorFlow Model Optimization Toolkit from Google. With TensorFlow Lite, models
    can be quantized after training during conversion (called post-training quantization).
    To minimize accuracy losses even further, we can use the TensorFlow Model Optimization
    Toolkit during training. This process is called *quantization-aware training*.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数推理框架提供了量化的方法，包括苹果的Core ML工具，NVIDIA的TensorRT（用于服务器），以及谷歌的TensorFlow Lite，以及谷歌的TensorFlow模型优化工具包。使用TensorFlow
    Lite，模型可以在训练后转换期间进行量化（称为后训练量化）。为了进一步减少精度损失，我们可以在训练期间使用TensorFlow模型优化工具包。这个过程称为*量化感知训练*。
- en: It would be useful to measure the benefit provided by quantization. Metrics
    from the [TensorFlow Lite Model optimization](https://oreil.ly/me4-I) benchmarks
    (shown in [Table 6-4](part0008.html#effects_of_different_quantization_strate))
    give us a hint, comparing 1) unquantized, 2) post-training quantized, and 3) quantization-aware
    trained models. The performance was measured on a Google Pixel 2 device.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量量化带来的好处是很有用的。来自[TensorFlow Lite模型优化](https://oreil.ly/me4-I)基准测试的指标（在[表6-4](part0008.html#effects_of_different_quantization_strate)中显示）给了我们一个提示，比较了1）未量化，2）后训练量化，和3）量化感知训练模型。性能是在Google
    Pixel 2设备上测量的。
- en: 'Table 6-4\. Effects of different quantization strategies (8-bit) on models
    (source: TensorFlow Lite model optimization documentation)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-4。不同量化策略（8位）对模型的影响（来源：TensorFlow Lite模型优化文档）
- en: '| **Model** | **MobileNet** | **MobileNetV2** | **InceptionV3** |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **MobileNet** | **MobileNetV2** | **InceptionV3** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Top-1 accuracy** | **Original** | 0.709 | 0.719 | 0.78 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| **Top-1准确率** | **原始** | 0.709 | 0.719 | 0.78 |'
- en: '| **Post-training quantized** | 0.657 | 0.637 | 0.772 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| **后训练量化** | 0.657 | 0.637 | 0.772 |'
- en: '| **Quantization-aware training** | 0.7 | 0.709 | 0.775 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| **量化感知训练** | 0.7 | 0.709 | 0.775 |'
- en: '| **Latency (ms)** | **Original** | 124 | 89 | 1130 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| **延迟（毫秒）** | **原始** | 124 | 89 | 1130 |'
- en: '| **Post-training quantized** | 112 | 98 | 845 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| **后训练量化** | 112 | 98 | 845 |'
- en: '| **Quantization-aware training** | 64 | 54 | 543 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| **量化感知训练** | 64 | 54 | 543 |'
- en: '| **Size (MB)** | **Original** | 16.9 | 14 | 95.7 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| **尺寸（MB）** | **原始** | 16.9 | 14 | 95.7 |'
- en: '| **Optimized** | 4.3 | 3.6 | 23.9 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| **优化** | 4.3 | 3.6 | 23.9 |'
- en: So, what do these numbers indicate? After quantization using TensorFlow Lite
    to INT8, we see roughly a four-times reduction in size, approximately two-times
    speedup in run time, and less than 1% change in accuracy. Not bad!
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些数字表示什么？在使用TensorFlow Lite进行INT8量化后，我们看到尺寸大约减小了四倍，运行时间大约加快了两倍，准确性变化小于1%。不错！
- en: More extreme form of quantization, like 1-bit binarized neural networks (like
    XNOR-Net), claim a whopping 58-times speedup with roughly 32-times smaller size
    when tested on AlexNet, with a 22% loss in accuracy.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 更极端的量化形式，如1位二值化神经网络（如XNOR-Net），在AlexNet上测试时声称速度提高了58倍，尺寸大约减小了32倍，准确性损失了22%。
- en: Prune the Model
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修剪模型
- en: Pick a number. Multiply it by 0\. What do we get? Zero. Multiply your pick again
    by a small value neighboring 0, like 10–⁶, and we’ll still get an insignificant
    value. If we replace such tiny weights (→ 0) in a model with 0 itself, it should
    have little effect on the model’s predictions. This is called *magnitude-based
    weight pruning,* or simply pruning, and is a form of *model compression*. Logically,
    putting a weight of 0 between two nodes in a fully connected layer is equivalent
    to deleting the edge between them. This makes a model with dense connections sparser.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 选一个数字。将它乘以0。我们得到什么？零。再将你选择的数字乘以一个接近0的小值，比如10^-6，我们仍然会得到一个微不足道的值。如果我们用0替换这样微小的权重（→
    0）在一个模型中，它对模型的预测应该几乎没有影响。这被称为*基于幅度的权重修剪*，或简称修剪，是一种*模型压缩*形式。从逻辑上讲，在全连接层中在两个节点之间放置一个权重为0等同于删除它们之间的边。这使得具有密集连接的模型更加稀疏。
- en: As it happens, a large chunk of the weights in a model are close to 0\. Pruning
    the model will result in many of those weights being set to 0\. This happens with
    little impact to accuracy. Although this does not save any space by itself, it
    introduces a ton of redundancy that can be exploited when it comes time to save
    the model to disk in a compressed format such as ZIP. (It is worth noting that
    compression algorithms thrive on repeating patterns. The more the repetition,
    the higher the compressibility.) The end result is that our model can often be
    compressed by four times. Of course, when we finally need to use the model, it
    would need to be uncompressed before loading in memory for inference.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，模型中大部分权重接近0。修剪模型将导致许多这些权重被设置为0。这对准确性几乎没有影响。虽然这本身并不节省任何空间，但在将模型保存到像ZIP这样的压缩格式的磁盘时，它引入了大量冗余，可以被利用。
    （值得注意的是，压缩算法擅长重复模式。重复次数越多，可压缩性就越高。）最终结果是我们的模型通常可以压缩四倍。当我们最终需要使用模型时，需要在加载到内存进行推断之前对其进行解压缩。
- en: The TensorFlow team observed the accuracy loss shown in [Table 6-5](part0008.html#model_accuracy_loss_versus_pruning_perce)
    while pruning the models. As expected, more efficient models like MobileNet observe
    higher (though still small) accuracy loss when compared with comparatively bigger
    models like InceptionV3.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow团队在修剪模型时观察到了[表6-5](part0008.html#model_accuracy_loss_versus_pruning_perce)中显示的准确性损失。如预期的那样，像MobileNet这样更高效的模型与相对较大的模型（如InceptionV3）相比，观察到更高（尽管仍然很小）的准确性损失。
- en: Table 6-5\. Model accuracy loss versus pruning percentage
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-5。模型准确性损失与修剪百分比
- en: '| **Model** | **Sparsity** | **Accuracy loss against original accuracy** |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **稀疏度** | **相对于原始准确性的准确性损失** |'
- en: '| --- | --- | --- |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| InceptionV3 | 50% | 0.1% |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| InceptionV3 | 50% | 0.1% |'
- en: '| InceptionV3 | 75% | 2.5% |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| InceptionV3 | 75% | 2.5% |'
- en: '| InceptionV3 | 87.5% | 4.5% |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| InceptionV3 | 87.5% | 4.5% |'
- en: '| MobileNet | 50% | 2% |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet | 50% | 2% |'
- en: Keras provides APIs to prune our model. This process can be done iteratively
    during training. Train a model normally or pick a pretrained model. Then, periodically
    prune the model and continue training. Having enough epochs between the periodic
    prunes allows the model to recover from any damage due to introducing so much
    sparsity. The amount of sparsity and number of epochs between prunes can be treated
    as hyperparameters to be tuned.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 'Keras提供了API来修剪我们的模型。这个过程可以在训练过程中进行迭代。正常训练一个模型或选择一个预训练模型。然后，定期修剪模型并继续训练。在定期修剪之间有足够的时代允许模型从引入如此多的稀疏性造成的任何损害中恢复过来。稀疏度和修剪之间的时代数量可以被视为要调整的超参数。 '
- en: Another way of implementing this is by using [Tencent’s PocketFlow](https://oreil.ly/JJms2)
    tool, a one-line command that provides several other pruning strategies implemented
    in recent research papers.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种实现这一点的方法是使用[Tencent的PocketFlow](https://oreil.ly/JJms2)工具，这是一个一行命令，提供了最近研究论文中实现的几种其他修剪策略。
- en: Use Fused Operations
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用融合操作
- en: In any serious CNN, the convolutional layer and batch normalization layer frequently
    appear together. They are kind of the Laurel and Hardy of CNN layers. Fundamentally,
    they are both linear operations. Basic linear algebra tells us that combining
    two or more linear operations will also result in a linear operation. By combining
    convolutional and batch normalization layers, we not only reduce the number of
    computations, but also decrease the amount of time spent in data transfer, both
    between main memory and GPU, and main memory and CPU registers/cache. Making them
    one operation prevents an extra roundtrip. Luckily, for inference purposes, most
    inference frameworks either automatically do this fusing step or provide model
    converters (like TensorFlow Lite) to make this optimization while converting the
    model to the inference format.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何严肃的CNN中，卷积层和批量归一化层经常一起出现。它们有点像CNN层的劳雷尔和哈迪。从根本上讲，它们都是线性操作。基本线性代数告诉我们，组合两个或更多线性操作也将导致一个线性操作。通过组合卷积和批量归一化层，我们不仅减少了计算量，还减少了在数据传输中花费的时间，包括主存储器和GPU之间，以及主存储器和CPU寄存器/缓存之间。将它们合并为一个操作可以防止额外的往返。幸运的是，对于推断目的，大多数推断框架要么自动执行这个融合步骤，要么提供模型转换器（如TensorFlow
    Lite）在将模型转换为推断格式时进行这种优化。
- en: Enable GPU Persistence
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用GPU持久性
- en: 'Loading and initializing the GPU drivers take time. You might have noticed
    a delay every time a training or inference job was initiated. For frequent, short
    jobs, the overhead can become relatively expensive quickly. Imagine an image classification
    program for which the classification takes 10 seconds, 9.9 of which were spent
    in loading the driver. What we need is for the GPU driver to stay preinitialized
    in the background, and be ready for whenever our training jobs start. And that’s
    where the NVIDIA GPU Persistence Daemon comes to the rescue:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 加载和初始化GPU驱动程序需要时间。您可能已经注意到，每次启动训练或推理作业时都会有延迟。对于频繁且短暂的作业来说，开销可能会迅速变得相对昂贵。想象一下一个图像分类程序，分类需要10秒，其中有9.9秒用于加载驱动程序。我们需要的是GPU驱动程序在后台保持预初始化，并在我们的训练作业启动时随时准备好。这就是NVIDIA
    GPU Persistence Daemon发挥作用的地方：
- en: '[PRE37]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Our GPUs will use a bit more wattage during idle time, but they will be ready
    and available the next time a program is launched.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的GPU在空闲时间会消耗更多的瓦特，但它们将在下次启动程序时准备好并可用。
- en: Summary
  id: totrans-423
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored different avenues for improving the speed and performance
    of our deep learning pipeline, from storing and reading the data to inference.
    A slow data pipeline often leads to a GPU starving for data, resulting in idle
    cycles. With several of the simple optimizations we discussed, our hardware can
    be put to its maximum efficiency. The handy checklist can serve as a ready reference.
    Feel free to make a copy for your desk (or your refrigerator). With these learnings,
    we hope to see your entry among the top performers of the MLPerf benchmark list.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了改进深度学习流程速度和性能的不同途径，从存储和读取数据到推理。慢速数据流通常导致GPU缺乏数据，导致空闲周期。通过我们讨论的几种简单优化方法，我们的硬件可以发挥最大效率。这个方便的清单可以作为一个方便的参考。请随意为您的桌子（或冰箱）制作一份副本。通过这些学习，我们希望看到您的名字出现在MLPerf基准测试列表的前列。
