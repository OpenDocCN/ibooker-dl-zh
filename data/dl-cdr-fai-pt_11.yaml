- en: Chapter 8\. Collaborative Filtering Deep Dive
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. 协同过滤深入探讨
- en: 'One common problem to solve is having a number of users and a number of products,
    and you want to recommend which products are most likely to be useful for which
    users. Many variations exist: for example, recommending movies (such as on Netflix),
    figuring out what to highlight for a user on a home page, deciding what stories
    to show in a social media feed, and so forth. A general solution to this problem,
    called *collaborative filtering*, works like this: look at which products the
    current user has used or liked, find other users who have used or liked similar
    products, and then recommend other products that those users have used or liked.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 解决的一个常见问题是有一定数量的用户和产品，您想推荐哪些产品最有可能对哪些用户有用。存在许多变体：例如，推荐电影（如Netflix上），确定在主页上为用户突出显示什么，决定在社交媒体动态中显示什么故事等。解决这个问题的一般方法称为*协同过滤*，工作原理如下：查看当前用户使用或喜欢的产品，找到其他使用或喜欢类似产品的用户，然后推荐那些用户使用或喜欢的其他产品。
- en: For example, on Netflix, you may have watched lots of movies that are science
    fiction, full of action, and were made in the 1970s. Netflix may not know these
    particular properties of the films you have watched, but it will be able to see
    that other people who have watched the same movies that you watched also tended
    to watch other movies that are science fiction, full of action, and were made
    in the 1970s. In other words, to use this approach, we don’t necessarily need
    to know anything about the movies except who likes to watch them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在Netflix上，您可能观看了很多科幻、充满动作并且是上世纪70年代制作的电影。Netflix可能不知道您观看的这些电影的特定属性，但它将能够看到观看了与您观看相同电影的其他人也倾向于观看其他科幻、充满动作并且是上世纪70年代制作的电影。换句话说，要使用这种方法，我们不一定需要了解电影的任何信息，只需要知道谁喜欢观看它们。
- en: There is a more general class of problems that this approach can solve, not
    necessarily involving users and products. Indeed, for collaborative filtering,
    we more commonly refer to *items*, rather than *products*. Items could be links
    that people click, diagnoses that are selected for patients, and so forth.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以解决更一般的一类问题，不一定涉及用户和产品。实际上，在协同过滤中，我们更常用*项目*这个术语，而不是*产品*。项目可以是人们点击的链接、为患者选择的诊断等。
- en: The key foundational idea is that of *latent factors*. In the Netflix example,
    we started with the assumption that you like old, action-packed sci-fi movies.
    But you never told Netflix that you like these kinds of movies. And Netflix never
    needed to add columns to its movies table saying which movies are of these types.
    Still, there must be some underlying concept of sci-fi, action, and movie age,
    and these concepts must be relevant for at least some people’s movie-watching
    decisions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的基础概念是*潜在因素*。在Netflix的例子中，我们假设您喜欢老式、充满动作的科幻电影。但您从未告诉Netflix您喜欢这类电影。Netflix也不需要在其电影表中添加列，说明哪些电影属于这些类型。尽管如此，必须存在一些关于科幻、动作和电影年龄的潜在概念，这些概念对于至少一些人的电影观看决策是相关的。
- en: For this chapter, we are going to work on this movie recommendation problem.
    We’ll start by getting some data suitable for a collaborative filtering model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将解决这个电影推荐问题。我们将从获取适合协同过滤模型的一些数据开始。
- en: A First Look at the Data
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据初探
- en: We do not have access to Netflix’s entire dataset of movie watching history,
    but there is a great dataset that we can use, called [MovieLens](https://oreil.ly/gP3Q5).
    This dataset contains tens of millions of movie rankings (a combination of a movie
    ID, a user ID, and a numeric rating), although we will just use a subset of 100,000
    of them for our example. If you’re interested, it would be a great learning project
    to try to replicate this approach on the full 25-million recommendation dataset,
    which you can get from their website.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法访问Netflix的完整电影观看历史数据集，但有一个很好的数据集可供我们使用，称为[MovieLens](https://oreil.ly/gP3Q5)。该数据集包含数千万部电影排名（电影ID、用户ID和数字评分的组合），尽管我们只会使用其中的10万部作为示例。如果您感兴趣，可以尝试在完整的2500万推荐数据集上复制这种方法，您可以从他们的网站上获取。
- en: 'The dataset is available through the usual fastai function:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集可通过通常的fastai函数获得：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'According to the *README*, the main table is in the file *u.data*. It is tab-separated
    and the columns are, respectively, user, movie, rating, and timestamp. Since those
    names are not encoded, we need to indicate them when reading the file with Pandas.
    Here is a way to open this table and take a look:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 根据*README*，主表位于文件*u.data*中。它是以制表符分隔的，列分别是用户、电影、评分和时间戳。由于这些名称没有编码，我们需要在使用Pandas读取文件时指定它们。以下是打开此表并查看的方法：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|  | user | movie | rating | timestamp |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '|  | 用户 | 电影 | 评分 | 时间戳 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 196 | 242 | 3 | 881250949 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 196 | 242 | 3 | 881250949 |'
- en: '| 1 | 186 | 302 | 3 | 891717742 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 186 | 302 | 3 | 891717742 |'
- en: '| 2 | 22 | 377 | 1 | 878887116 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 22 | 377 | 1 | 878887116 |'
- en: '| 3 | 244 | 51 | 2 | 880606923 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 244 | 51 | 2 | 880606923 |'
- en: '| 4 | 166 | 346 | 1 | 886397596 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 166 | 346 | 1 | 886397596 |'
- en: Although this has all the information we need, it is not a particularly helpful
    way for humans to look at this data. [Figure 8-1](#movie_xtab) shows the same
    data cross-tabulated into a human-friendly table.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这包含了我们需要的所有信息，但这并不是人类查看这些数据的特别有用的方式。[图8-1](#movie_xtab)将相同数据交叉制表成了一个人类友好的表格。
- en: '![Crosstab of movies and users](Images/dlcf_0801.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![电影和用户的交叉表](Images/dlcf_0801.png)'
- en: Figure 8-1\. Crosstab of movies and users
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1. 电影和用户的交叉表
- en: We have selected just a few of the most popular movies, and users who watch
    the most movies, for this crosstab example. The empty cells in this table are
    the things that we would like our model to learn to fill in. Those are the places
    where a user has not reviewed the movie yet, presumably because they have not
    watched it. For each user, we would like to figure out which of those movies they
    might be most likely to enjoy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只选择了一些最受欢迎的电影和观看电影最多的用户，作为这个交叉表示例。这个表格中的空单元格是我们希望我们的模型学会填充的内容。这些是用户尚未评论电影的地方，可能是因为他们还没有观看。对于每个用户，我们希望找出他们最有可能喜欢哪些电影。
- en: 'If we knew for each user to what degree they liked each important category
    that a movie might fall into, such as genre, age, preferred directors and actors,
    and so forth, and we knew the same information about each movie, then a simple
    way to fill in this table would be to multiply this information together for each
    movie and use a combination. For instance, assuming these factors range between
    –1 and +1, with positive numbers indicating stronger matches and negative numbers
    weaker ones, and the categories are science-fiction, action, and old movies, then
    we could represent the movie *The Last Skywalker* as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道每个用户对电影可能属于的每个重要类别的喜好程度，比如流派、年龄、喜欢的导演和演员等，以及我们对每部电影的相同信息，那么填写这个表格的一个简单方法是将这些信息相乘，然后使用组合。例如，假设这些因子的范围在-1到+1之间，正数表示更强的匹配，负数表示更弱的匹配，类别是科幻、动作和老电影，那么我们可以表示电影《最后的绝地武士》如下：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, for instance, we are scoring *very science-fiction* as 0.98, and *very
    not old* as –0.9\. We could represent a user who likes modern sci-fi action movies
    as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，例如，我们将*非常科幻*评分为0.98，*非常不老*评分为-0.9。我们可以表示喜欢现代科幻动作电影的用户如下：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now calculate the match between this combination:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算这种组合之间的匹配：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When we multiply two vectors together and add up the results, this is known
    as the *dot product*. It is used a lot in machine learning and forms the basis
    of matrix multiplication. We will be looking a lot more at matrix multiplication
    and dot products in [Chapter 17](ch17.xhtml#chapter_foundations).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将两个向量相乘并将结果相加时，这被称为*点积*。它在机器学习中被广泛使用，并构成了矩阵乘法的基础。我们将在[第17章](ch17.xhtml#chapter_foundations)中更多地研究矩阵乘法和点积。
- en: 'Jargon: Dot Product'
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：点积
- en: The mathematical operation of multiplying the elements of two vectors together,
    and then summing up the result.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个向量的元素相乘，然后将结果相加的数学运算。
- en: 'On the other hand, we might represent the movie *Casablanca* as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们可以表示电影《卡萨布兰卡》如下：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The match between this combination is shown here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种组合之间的匹配如下所示：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Since we don’t know what the latent factors are, and we don’t know how to score
    them for each user and movie, we should learn them.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不知道潜在因子是什么，也不知道如何为每个用户和电影评分，我们应该学习它们。
- en: Learning the Latent Factors
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习潜在因子
- en: There is surprisingly little difference between specifying the structure of
    a model, as we did in the preceding section, and learning one, since we can just
    use our general gradient descent approach.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定模型的结构和学习模型之间，实际上几乎没有什么区别，因为我们可以使用我们的一般梯度下降方法。
- en: Step 1 of this approach is to randomly initialize some parameters. These parameters
    will be a set of latent factors for each user and movie. We will have to decide
    how many to use. We will discuss how to select this shortly, but for illustrative
    purposes, let’s use 5 for now. Because each user will have a set of these factors,
    and each movie will have a set of these factors, we can show these randomly initialized
    values right next to the users and movies in our crosstab, and we can then fill
    in the dot products for each of these combinations in the middle. For example,
    [Figure 8-2](#xtab_latent) shows what it looks like in Microsoft Excel, with the
    top-left cell formula displayed as an example.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的第一步是随机初始化一些参数。这些参数将是每个用户和电影的一组潜在因子。我们将不得不决定要使用多少个。我们将很快讨论如何选择这些，但为了说明，让我们现在使用5个。因为每个用户将有一组这些因子，每部电影也将有一组这些因子，我们可以在交叉表中的用户和电影旁边显示这些随机初始化的值，然后我们可以填写这些组合的点积。例如，[图8-2](#xtab_latent)显示了在Microsoft
    Excel中的样子，顶部左侧的单元格公式显示为示例。
- en: Step 2 of this approach is to calculate our predictions. As we’ve discussed,
    we can do this by simply taking the dot product of each movie with each user.
    If, for instance, the first latent user factor represents how much the user likes
    action movies and the first latent movie factor represents whether the movie has
    a lot of action or not, the product of those will be particularly high if either
    the user likes action movies and the movie has a lot of action in it, or the user
    doesn’t like action movies and the movie doesn’t have any action in it. On the
    other hand, if we have a mismatch (a user loves action movies but the movie isn’t
    an action film, or the user doesn’t like action movies and it is one), the product
    will be very low.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的第二步是计算我们的预测。正如我们讨论过的，我们可以通过简单地将每部电影与每个用户进行点积来实现这一点。例如，如果第一个潜在用户因子代表用户喜欢动作电影的程度，第一个潜在电影因子代表电影是否有很多动作，那么如果用户喜欢动作电影并且电影中有很多动作，或者用户不喜欢动作电影并且电影中没有任何动作，这两者的乘积将特别高。另一方面，如果存在不匹配（用户喜欢动作电影但电影不是动作片，或者用户不喜欢动作电影但电影是动作片），乘积将非常低。
- en: '![Latent factors with crosstab](Images/dlcf_0802.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![交叉表中的潜在因子](Images/dlcf_0802.png)'
- en: Figure 8-2\. Latent factors with crosstab
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2. 交叉表中的潜在因子
- en: Step 3 is to calculate our loss. We can use any loss function that we wish;
    let’s pick mean squared error for now, since that is one reasonable way to represent
    the accuracy of a prediction.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是计算我们的损失。我们可以使用任何损失函数，让我们现在选择均方误差，因为这是一种合理的表示预测准确性的方法。
- en: That’s all we need. With this in place, we can optimize our parameters (the
    latent factors) using stochastic gradient descent, such as to minimize the loss.
    At each step, the stochastic gradient descent optimizer will calculate the match
    between each movie and each user using the dot product, and will compare it to
    the actual rating that each user gave to each movie. It will then calculate the
    derivative of this value and step the weights by multiplying this by the learning
    rate. After doing this lots of times, the loss will get better and better, and
    the recommendations will also get better and better.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要的全部内容。有了这个，我们可以使用随机梯度下降来优化我们的参数（潜在因素），以最小化损失。在每一步中，随机梯度下降优化器将使用点积计算每部电影与每个用户之间的匹配，并将其与每个用户给出的每部电影的实际评分进行比较。然后它将计算这个值的导数，并通过学习率乘以这个值来调整权重。经过多次这样的操作，损失会变得越来越好，推荐也会变得越来越好。
- en: To use the usual `Learner.fit` function, we will need to get our data into a
    `DataLoaders`, so let’s focus on that now.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用通常的`Learner.fit`函数，我们需要将我们的数据放入`DataLoaders`中，所以让我们现在专注于这一点。
- en: Creating the DataLoaders
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建DataLoaders
- en: 'When showing the data, we would rather see movie titles than their IDs. The
    table `u.item` contains the correspondence of IDs to titles:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示数据时，我们宁愿看到电影标题而不是它们的ID。表`u.item`包含ID与标题的对应关系：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|  | movie | title |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | 电影 | 标题 |'
- en: '| --- | --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 1 | Toy Story (1995) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 玩具总动员（1995） |'
- en: '| 1 | 2 | GoldenEye (1995) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 黄金眼（1995） |'
- en: '| 2 | 3 | Four Rooms (1995) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3 | 四个房间（1995） |'
- en: '| 3 | 4 | Get Shorty (1995) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4 | 短小（1995） |'
- en: '| 4 | 5 | Copycat (1995) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 5 | 复制猫（1995） |'
- en: 'We can merge this with our `ratings` table to get the user ratings by title:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个表与我们的`ratings`表合并，以获得按标题分类的用户评分：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|  | user | movie | rating | timestamp | title |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | 用户 | 电影 | 评分 | 时间戳 | 标题 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 196 | 242 | 3 | 881250949 | Kolya (1996) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 196 | 242 | 3 | 881250949 | 科洛亚（1996） |'
- en: '| 1 | 63 | 242 | 3 | 875747190 | Kolya (1996) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 63 | 242 | 3 | 875747190 | 科洛亚（1996） |'
- en: '| 2 | 226 | 242 | 5 | 883888671 | Kolya (1996) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 226 | 242 | 5 | 883888671 | 科洛亚（1996） |'
- en: '| 3 | 154 | 242 | 3 | 879138235 | Kolya (1996) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 154 | 242 | 3 | 879138235 | 科洛亚（1996） |'
- en: '| 4 | 306 | 242 | 5 | 876503793 | Kolya (1996) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 306 | 242 | 5 | 876503793 | 科洛亚（1996） |'
- en: 'We can then build a `DataLoaders` object from this table. By default, it takes
    the first column for the user, the second column for the item (here our movies),
    and the third column for the ratings. We need to change the value of `item_name`
    in our case to use the titles instead of the IDs:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以从这个表构建一个`DataLoaders`对象。默认情况下，它将使用第一列作为用户，第二列作为项目（这里是我们的电影），第三列作为评分。在我们的情况下，我们需要更改`item_name`的值，以使用标题而不是ID：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|  | user | title | rating |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | 用户 | 标题 | 评分 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 207 | Four Weddings and a Funeral (1994) | 3 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 207 | 四个婚礼和一个葬礼（1994） | 3 |'
- en: '| 1 | 565 | Remains of the Day, The (1993) | 5 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 565 | 日残余（1993） | 5 |'
- en: '| 2 | 506 | Kids (1995) | 1 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 506 | 小孩（1995） | 1 |'
- en: '| 3 | 845 | Chasing Amy (1997) | 3 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 845 | 追求艾米（1997） | 3 |'
- en: '| 4 | 798 | Being Human (1993) | 2 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 798 | 人类（1993） | 2 |'
- en: '| 5 | 500 | Down by Law (1986) | 4 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 500 | 低俗法则（1986） | 4 |'
- en: '| 6 | 409 | Much Ado About Nothing (1993) | 3 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 409 | 无事生非（1993） | 3 |'
- en: '| 7 | 721 | Braveheart (1995) | 5 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 721 | 勇敢的心（1995） | 5 |'
- en: '| 8 | 316 | Psycho (1960) | 2 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 316 | 精神病患者（1960） | 2 |'
- en: '| 9 | 883 | Judgment Night (1993) | 5 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 883 | 判决之夜（1993） | 5 |'
- en: 'To represent collaborative filtering in PyTorch, we can’t just use the crosstab
    representation directly, especially if we want it to fit into our deep learning
    framework. We can represent our movie and user latent factor tables as simple
    matrices:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在PyTorch中表示协同过滤，我们不能直接使用交叉表表示，特别是如果我们希望它适应我们的深度学习框架。我们可以将我们的电影和用户潜在因素表表示为简单的矩阵：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To calculate the result for a particular movie and user combination, we have
    to look up the index of the movie in our movie latent factor matrix, and the index
    of the user in our user latent factor matrix; then we can do our dot product between
    the two latent factor vectors. But *look up in an index* is not an operation our
    deep learning models know how to do. They know how to do matrix products and activation
    functions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算特定电影和用户组合的结果，我们必须查找电影在我们的电影潜在因素矩阵中的索引，以及用户在我们的用户潜在因素矩阵中的索引；然后我们可以在两个潜在因素向量之间进行点积。但*查找索引*不是我们的深度学习模型知道如何执行的操作。它们知道如何执行矩阵乘积和激活函数。
- en: 'Fortunately, it turns out that we can represent *look up in an index* as a
    matrix product. The trick is to replace our indices with one-hot-encoded vectors.
    Here is an example of what happens if we multiply a vector by a one-hot-encoded
    vector representing the index 3:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以将*查找索引*表示为矩阵乘积。技巧是用单热编码向量替换我们的索引。这是一个例子，展示了如果我们将一个向量乘以一个表示索引3的单热编码向量会发生什么：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It gives us the same vector as the one at index 3 in the matrix:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 它给我们的结果与矩阵中索引3处的向量相同：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If we do that for a few indices at once, we will have a matrix of one-hot-encoded
    vectors, and that operation will be a matrix multiplication! This would be a perfectly
    acceptable way to build models using this kind of architecture, except that it
    would use a lot more memory and time than necessary. We know that there is no
    real underlying reason to store the one-hot-encoded vector, or to search through
    it to find the occurrence of the number 1—we should just be able to index into
    an array directly with an integer. Therefore, most deep learning libraries, including
    PyTorch, include a special layer that does just this; it indexes into a vector
    using an integer, but has its derivative calculated in such a way that it is identical
    to what it would have been if it had done a matrix multiplication with a one-hot-encoded
    vector. This is called an *embedding*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们一次为几个索引这样做，我们将得到一个独热编码向量的矩阵，这个操作将是一个矩阵乘法！这将是使用这种架构构建模型的一种完全可接受的方式，只是它会比必要的使用更多的内存和时间。我们知道没有真正的基础原因来存储独热编码向量，或者通过搜索找到数字1的出现
    - 我们应该能够直接使用整数索引到数组中。因此，大多数深度学习库，包括PyTorch，都包括一个特殊的层，它就是这样做的；它使用整数索引到一个向量中，但其导数的计算方式使其与使用独热编码向量进行矩阵乘法时完全相同。这被称为*嵌入*。
- en: 'Jargon: Embedding'
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：嵌入
- en: Multiplying by a one-hot-encoded matrix, using the computational shortcut that
    it can be implemented by simply indexing directly. This is quite a fancy word
    for a very simple concept. The thing that you multiply the one-hot-encoded matrix
    by (or, using the computational shortcut, index into directly) is called the *embedding
    matrix*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个独热编码矩阵相乘，使用计算快捷方式，可以通过直接索引来实现。这是一个非常简单概念的相当花哨的词。您将独热编码矩阵相乘的东西（或者使用计算快捷方式，直接索引）称为*嵌入矩阵*。
- en: 'In computer vision, we have a very easy way to get all the information of a
    pixel through its RGB values: each pixel in a colored image is represented by
    three numbers. Those three numbers give us the redness, the greenness, and the
    blueness, which is enough to get our model to work afterward.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，我们有一种非常简单的方法通过其RGB值获取像素的所有信息：彩色图像中的每个像素由三个数字表示。这三个数字给我们红色、绿色和蓝色，这足以让我们的模型在之后工作。
- en: 'For the problem at hand, we don’t have the same easy way to characterize a
    user or a movie. There are probably relations with genres: if a given user likes
    romance, they are likely to give higher scores to romance movies. Other factors
    might be whether the movie is more action-oriented versus heavy on dialogue, or
    the presence of a specific actor whom a user might particularly like.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于手头的问题，我们没有同样简单的方法来描述用户或电影。可能与流派有关：如果给定用户喜欢爱情片，他们可能会给爱情片更高的评分。其他因素可能是电影是更注重动作还是对话，或者是否有一个特定的演员，用户可能特别喜欢。
- en: How do we determine numbers to characterize those? The answer is, we don’t.
    We will let our model *learn* them. By analyzing the existing relations between
    users and movies, our model can figure out itself the features that seem important
    or not.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何确定用来描述这些数字的数字？答案是，我们不确定。我们将让我们的模型*学习*它们。通过分析用户和电影之间的现有关系，我们的模型可以自己找出看起来重要或不重要的特征。
- en: This is what embeddings are. We will attribute to each of our users and each
    of our movies a random vector of a certain length (here, `n_factors=5`), and we
    will make those learnable parameters. That means that at each step, when we compute
    the loss by comparing our predictions to our targets, we will compute the gradients
    of the loss with respect to those embedding vectors and update them with the rules
    of SGD (or another optimizer).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是嵌入。我们将为我们的每个用户和每个电影分配一个特定长度的随机向量（这里，`n_factors=5`），并将使它们成为可学习的参数。这意味着在每一步，当我们通过比较我们的预测和目标来计算损失时，我们将计算损失相对于这些嵌入向量的梯度，并根据SGD（或其他优化器）的规则更新它们。
- en: At the beginning, those numbers don’t mean anything since we have chosen them
    randomly, but by the end of training, they will. By learning on existing data
    about the relations between users and movies, without having any other information,
    we will see that they still get some important features, and can isolate blockbusters
    from independent films, action movies from romance, and so on.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，这些数字没有任何意义，因为我们是随机选择的，但在训练结束时，它们将有意义。通过学习关于用户和电影之间关系的现有数据，没有任何其他信息，我们将看到它们仍然获得一些重要特征，并且可以将大片与独立电影、动作片与爱情片等区分开来。
- en: We are now in a position to create our whole model from scratch.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有能力从头开始创建我们的整个模型。
- en: Collaborative Filtering from Scratch
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始协同过滤
- en: Before we can write a model in PyTorch, we first need to learn the basics of
    object-oriented programming and Python. If you haven’t done any object-oriented
    programming before, we will give you a quick introduction here, but we would recommend
    looking up a tutorial and getting some practice before moving on.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以用PyTorch编写模型之前，我们首先需要学习面向对象编程和Python的基础知识。如果您以前没有进行过面向对象编程，我们将在这里为您进行快速介绍，但我们建议您在继续之前查阅教程并进行一些练习。
- en: 'The key idea in object-oriented programming is the *class*. We have been using
    classes throughout this book, such as `DataLoader`, `String`, and `Learner`. Python
    also makes it easy for us to create new classes. Here is an example of a simple
    class:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 面向对象编程中的关键思想是*类*。我们在本书中一直在使用类，比如`DataLoader`、`String`和`Learner`。Python还让我们很容易地创建新类。这是一个简单类的示例：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The most important piece of this is the special method called `__init__` (pronounced
    *dunder init*). In Python, any method surrounded in double underscores like this
    is considered special. It indicates that some extra behavior is associated with
    this method name. In the case of `__init__`, this is the method Python will call
    when your new object is created. So, this is where you can set up any state that
    needs to be initialized upon object creation. Any parameters included when the
    user constructs an instance of your class will be passed to the `__init__` method
    as parameters. Note that the first parameter to any method defined inside a class
    is `self`, so you can use this to set and get any attributes that you will need:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这其中最重要的部分是一个特殊的方法叫做`__init__`（发音为*dunder init*）。在Python中，任何像这样用双下划线包围的方法都被认为是特殊的。它表示与这个方法名称相关联一些额外的行为。对于`__init__`，这是Python在创建新对象时将调用的方法。因此，这是你可以在对象创建时设置任何需要初始化的状态的地方。当用户构造类的实例时包含的任何参数都将作为参数传递给`__init__`方法。请注意，在类内定义的任何方法的第一个参数是`self`，因此你可以使用它来设置和获取任何你需要的属性：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Also note that creating a new PyTorch module requires inheriting from `Module`.
    *Inheritance* is an important object-oriented concept that we will not discuss
    in detail here—in short, it means that we can add additional behavior to an existing
    class. PyTorch already provides a `Module` class, which provides some basic foundations
    that we want to build on. So, we add the name of this *superclass* after the name
    of the class that we are defining, as shown in the following examples.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，创建一个新的PyTorch模块需要继承自`Module`。*继承*是一个重要的面向对象的概念，在这里我们不会详细讨论——简而言之，它意味着我们可以向现有类添加额外的行为。PyTorch已经提供了一个`Module`类，它提供了一些我们想要构建的基本基础。因此，我们在定义类的名称后面添加这个*超类*的名称，如下面的示例所示。
- en: 'The final thing that you need to know to create a new PyTorch module is that
    when your module is called, PyTorch will call a method in your class called `forward`,
    and will pass along to that any parameters that are included in the call. Here
    is the class defining our dot product model:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要知道创建一个新的PyTorch模块的最后一件事是，当调用你的模块时，PyTorch将调用你的类中的一个名为`forward`的方法，并将包含在调用中的任何参数传递给它。这是定义我们的点积模型的类：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you haven’t seen object-oriented programming before, don’t worry; you won’t
    need to use it much in this book. We are just mentioning this approach here because
    most online tutorials and documentation will use the object-oriented syntax.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以前没有见过面向对象的编程，不用担心；在这本书中你不需要经常使用它。我们在这里提到这种方法只是因为大多数在线教程和文档将使用面向对象的语法。
- en: 'Note that the input of the model is a tensor of shape `batch_size x 2`, where
    the first column (`x[:, 0]`) contains the user IDs, and the second column (`x[:,
    1]`) contains the movie IDs. As explained before, we use the *embedding* layers
    to represent our matrices of user and movie latent factors:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型的输入是一个形状为`batch_size x 2`的张量，其中第一列（`x[:, 0]`）包含用户ID，第二列（`x[:, 1]`）包含电影ID。如前所述，我们使用*嵌入*层来表示我们的用户和电影潜在因子的矩阵：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now that we have defined our architecture and created our parameter matrices,
    we need to create a `Learner` to optimize our model. In the past, we have used
    special functions, such as `cnn_learner`, which set up everything for us for a
    particular application. Since we are doing things from scratch here, we will use
    the plain `Learner` class:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的架构并创建了参数矩阵，我们需要创建一个`Learner`来优化我们的模型。在过去，我们使用了特殊函数，比如`cnn_learner`，为特定应用程序为我们设置了一切。由于我们在这里从头开始做事情，我们将使用普通的`Learner`类：
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We are now ready to fit our model:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备拟合我们的模型：
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '| epoch | train_loss | valid_loss | time |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | time |'
- en: '| --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 1.326261 | 1.295701 | 00:12 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.326261 | 1.295701 | 00:12 |'
- en: '| 1 | 1.091352 | 1.091475 | 00:11 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.091352 | 1.091475 | 00:11 |'
- en: '| 2 | 0.961574 | 0.977690 | 00:11 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.961574 | 0.977690 | 00:11 |'
- en: '| 3 | 0.829995 | 0.893122 | 00:11 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.829995 | 0.893122 | 00:11 |'
- en: '| 4 | 0.781661 | 0.876511 | 00:12 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.781661 | 0.876511 | 00:12 |'
- en: 'The first thing we can do to make this model a little bit better is to force
    those predictions to be between 0 and 5\. For this, we just need to use `sigmoid_range`,
    as in [Chapter 6](ch06.xhtml#chapter_multicat). One thing we discovered empirically
    is that it’s better to have the range go a little bit over 5, so we use `(0, 5.5)`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的第一件事是让这个模型更好一点，强制这些预测值在0到5之间。为此，我们只需要使用`sigmoid_range`，就像[第6章](ch06.xhtml#chapter_multicat)中那样。我们经验性地发现，最好让范围略微超过5，所以我们使用`(0,
    5.5)`：
- en: '[PRE25]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '| epoch | train_loss | valid_loss | time |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | time |'
- en: '| --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0.976380 | 1.001455 | 00:12 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.976380 | 1.001455 | 00:12 |'
- en: '| 1 | 0.875964 | 0.919960 | 00:12 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.875964 | 0.919960 | 00:12 |'
- en: '| 2 | 0.685377 | 0.870664 | 00:12 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.685377 | 0.870664 | 00:12 |'
- en: '| 3 | 0.483701 | 0.874071 | 00:12 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.483701 | 0.874071 | 00:12 |'
- en: '| 4 | 0.385249 | 0.878055 | 00:12 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.385249 | 0.878055 | 00:12 |'
- en: This is a reasonable start, but we can do better. One obvious missing piece
    is that some users are just more positive or negative in their recommendations
    than others, and some movies are just plain better or worse than others. But in
    our dot product representation, we do not have any way to encode either of these
    things. If all you can say about a movie is, for instance, that it is very sci-fi,
    very action-oriented, and very not old, then you don’t really have any way to
    say whether most people like it.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个合理的开始，但我们可以做得更好。一个明显缺失的部分是，有些用户在推荐中只是更积极或更消极，有些电影只是比其他电影更好或更差。但在我们的点积表示中，我们没有任何方法来编码这两件事。如果你只能说一部电影，例如，它非常科幻，非常动作导向，非常不老旧，那么你实际上没有办法说大多数人是否喜欢它。
- en: 'That’s because at this point we have only weights; we do not have biases. If
    we have a single number for each user that we can add to our scores, and ditto
    for each movie, that will handle this missing piece very nicely. So first of all,
    let’s adjust our model architecture:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为在这一点上我们只有权重；我们没有偏差。如果我们为每个用户有一个可以添加到我们的分数中的单个数字，对于每部电影也是如此，那么这将非常好地处理这个缺失的部分。因此，首先让我们调整我们的模型架构：
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s try training this and see how it goes:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试训练这个模型，看看效果如何：
- en: '[PRE28]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '| epoch | train_loss | valid_loss | time |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | time |'
- en: '| --- | --- | --- | --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0.929161 | 0.936303 | 00:13 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.929161 | 0.936303 | 00:13 |'
- en: '| 1 | 0.820444 | 0.861306 | 00:13 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.820444 | 0.861306 | 00:13 |'
- en: '| 2 | 0.621612 | 0.865306 | 00:14 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.621612 | 0.865306 | 00:14 |'
- en: '| 3 | 0.404648 | 0.886448 | 00:13 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.404648 | 0.886448 | 00:13 |'
- en: '| 4 | 0.292948 | 0.892580 | 00:13 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.292948 | 0.892580 | 00:13 |'
- en: Instead of being better, it ends up being worse (at least at the end of training).
    Why is that? If we look at both trainings carefully, we can see the validation
    loss stopped improving in the middle and started to get worse. As we’ve seen,
    this is a clear indication of overfitting. In this case, there is no way to use
    data augmentation, so we will have to use another regularization technique. One
    approach that can be helpful is *weight decay*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，结果并不比之前更好（至少在训练结束时）。为什么呢？如果我们仔细观察这两次训练，我们会发现验证损失在中间停止改善并开始变差。正如我们所见，这是过拟合的明显迹象。在这种情况下，没有办法使用数据增强，所以我们将不得不使用另一种正则化技术。一个有帮助的方法是*权重衰减*。
- en: Weight Decay
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Weight Decay
- en: Weight decay, or *L2 regularization*, consists of adding to your loss function
    the sum of all the weights squared. Why do that? Because when we compute the gradients,
    it will add a contribution to them that will encourage the weights to be as small
    as possible.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减，或*L2正则化*，包括将所有权重的平方和添加到损失函数中。为什么这样做？因为当我们计算梯度时，它会为梯度增加一个贡献，鼓励权重尽可能小。
- en: 'Why would it prevent overfitting? The idea is that the larger the coefficients
    are, the sharper canyons we will have in the loss function. If we take the basic
    example of a parabola, `y = a * (x**2)`, the larger `a` is, the more *narrow*
    the parabola is:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么它可以防止过拟合？这个想法是，系数越大，损失函数中的峡谷就会越尖锐。如果我们以抛物线的基本例子`y = a * (x**2)`为例，`a`越大，抛物线就越*狭窄*：
- en: '![Parabolas with various a values](Images/dlcf_0803.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![不同a值的抛物线](Images/dlcf_0803.png)'
- en: So, letting our model learn high parameters might cause it to fit all the data
    points in the training set with an overcomplex function that has very sharp changes,
    which will lead to overfitting.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们的模型学习高参数可能导致它用一个过于复杂、具有非常尖锐变化的函数拟合训练集中的所有数据点，这将导致过拟合。
- en: 'Limiting our weights from growing too much is going to hinder the training
    of the model, but it will yield a state where it generalizes better. Going back
    to the theory briefly, weight decay (or just `wd`) is a parameter that controls
    that sum of squares we add to our loss (assuming `parameters` is a tensor of all
    parameters):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 限制我们的权重过大会阻碍模型的训练，但会产生一个更好泛化的状态。回顾一下理论，权重衰减（或`wd`）是一个控制我们在损失中添加的平方和的参数（假设`parameters`是所有参数的张量）：
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In practice, though, it would be very inefficient (and maybe numerically unstable)
    to compute that big sum and add it to the loss. If you remember a little bit of
    high school math, you might recall that the derivative of `p**2` with respect
    to `p` is `2*p`, so adding that big sum to our loss is exactly the same as doing
    this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，计算那个大和并将其添加到损失中将非常低效（也许在数值上不稳定）。如果你还记得一点高中数学，你可能会记得`p**2`关于`p`的导数是`2*p`，所以将那个大和添加到我们的损失中，实际上等同于这样做：
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In practice, since `wd` is a parameter that we choose, we can make it twice
    as big, so we don’t even need the `*2` in this equation. To use weight decay in
    fastai, pass `wd` in your call to `fit` or `fit_one_cycle` (it can be passed on
    both):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，由于`wd`是我们选择的一个参数，我们可以使它变为两倍大，所以在这个方程中我们甚至不需要`*2`。要在fastai中使用权重衰减，在调用`fit`或`fit_one_cycle`时传递`wd`即可（可以同时传递）：
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '| epoch | train_loss | valid_loss | time |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | time |'
- en: '| --- | --- | --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0.972090 | 0.962366 | 00:13 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.972090 | 0.962366 | 00:13 |'
- en: '| 1 | 0.875591 | 0.885106 | 00:13 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.875591 | 0.885106 | 00:13 |'
- en: '| 2 | 0.723798 | 0.839880 | 00:13 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.723798 | 0.839880 | 00:13 |'
- en: '| 3 | 0.586002 | 0.823225 | 00:13 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.586002 | 0.823225 | 00:13 |'
- en: '| 4 | 0.490980 | 0.823060 | 00:13 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.490980 | 0.823060 | 00:13 |'
- en: Much better!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 好多了！
- en: Creating Our Own Embedding Module
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建我们自己的嵌入模块
- en: 'So far, we’ve used `Embedding` without thinking about how it really works.
    Let’s re-create `DotProductBias` *without* using this class. We’ll need a randomly
    initialized weight matrix for each of the embeddings. We have to be careful, however.
    Recall from [Chapter 4](ch04.xhtml#chapter_mnist_basics) that optimizers require
    that they can get all the parameters of a module from the module’s `parameters`
    method. However, this does not happen fully automatically. If we just add a tensor
    as an attribute to a `Module`, it will not be included in `parameters`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用`Embedding`而没有考虑它是如何工作的。让我们重新创建`DotProductBias`，*不*使用这个类。我们需要为每个嵌入初始化一个随机权重矩阵。然而，我们必须小心。回想一下[第四章](ch04.xhtml#chapter_mnist_basics)中提到的，优化器要求能够从模块的`parameters`方法中获取模块的所有参数。然而，这并不是完全自动发生的。如果我们只是将一个张量作为`Module`的属性添加，它不会包含在`parameters`中：
- en: '[PRE32]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To tell `Module` that we want to treat a tensor as a parameter, we have to
    wrap it in the `nn.Parameter` class. This class doesn’t add any functionality
    (other than automatically calling `requires_grad_` for us). It’s used only as
    a “marker” to show what to include in `parameters`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 要告诉`Module`我们希望将一个张量视为参数，我们必须将其包装在`nn.Parameter`类中。这个类不添加任何功能（除了自动为我们调用`requires_grad_`）。它只用作一个“标记”，以显示要包含在`parameters`中的内容：
- en: '[PRE34]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'All PyTorch modules use `nn.Parameter` for any trainable parameters, which
    is why we haven’t needed to explicitly use this wrapper until now:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 所有PyTorch模块都使用`nn.Parameter`来表示任何可训练参数，这就是为什么我们直到现在都不需要显式使用这个包装器：
- en: '[PRE36]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can create a tensor as a parameter, with random initialization, like so:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个张量作为参数，进行随机初始化，如下所示：
- en: '[PRE40]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let’s use this to create `DotProductBias` again, but without `Embedding`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次使用这个来创建`DotProductBias`，但不使用`Embedding`：
- en: '[PRE41]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then let’s train it again to check we get around the same results we saw in
    the previous section:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们再次训练它，以检查我们是否得到了与前一节中看到的大致相同的结果：
- en: '[PRE42]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '| epoch | train_loss | valid_loss | time |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | time |'
- en: '| --- | --- | --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0.962146 | 0.936952 | 00:14 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.962146 | 0.936952 | 00:14 |'
- en: '| 1 | 0.858084 | 0.884951 | 00:14 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.858084 | 0.884951 | 00:14 |'
- en: '| 2 | 0.740883 | 0.838549 | 00:14 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.740883 | 0.838549 | 00:14 |'
- en: '| 3 | 0.592497 | 0.823599 | 00:14 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.592497 | 0.823599 | 00:14 |'
- en: '| 4 | 0.473570 | 0.824263 | 00:14 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.473570 | 0.824263 | 00:14 |'
- en: Now, let’s take a look at what our model has learned.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的模型学到了什么。
- en: Interpreting Embeddings and Biases
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释嵌入和偏差
- en: 'Our model is already useful, in that it can provide us with movie recommendations
    for our users—but it is also interesting to see what parameters it has discovered.
    The easiest to interpret are the biases. Here are the movies with the lowest values
    in the bias vector:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型已经很有用，因为它可以为我们的用户提供电影推荐，但看到它发现了什么参数也很有趣。最容易解释的是偏差。以下是偏差向量中值最低的电影：
- en: '[PRE43]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Think about what this means. What it’s saying is that for each of these movies,
    even when a user is very well matched to its latent factors (which, as we will
    see in a moment, tend to represent things like level of action, age of movie,
    and so forth), they still generally don’t like it. We could have simply sorted
    the movies directly by their average rating, but looking at the learned bias tells
    us something much more interesting. It tells us not just whether a movie is of
    a kind that people tend not to enjoy watching, but that people tend to not like
    watching it even if it is of a kind that they would otherwise enjoy! By the same
    token, here are the movies with the highest bias:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 想想这意味着什么。它表明对于这些电影中的每一部，即使用户与其潜在因素非常匹配（稍后我们将看到，这些因素往往代表动作水平、电影年龄等等），他们通常仍然不喜欢它。我们本可以简单地按照电影的平均评分对其进行排序，但查看学到的偏差告诉我们更有趣的事情。它告诉我们不仅仅是电影是人们不喜欢观看的类型，而且即使是他们本来会喜欢的类型，人们也倾向于不喜欢观看！同样地，以下是偏差最高的电影：
- en: '[PRE45]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: So, for instance, even if you don’t normally enjoy detective movies, you might
    enjoy *LA Confidential*!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，例如，即使您通常不喜欢侦探电影，您可能会喜欢*LA机密*！
- en: It is not quite so easy to directly interpret the embedding matrices. There
    are just too many factors for a human to look at. But there is a technique that
    can pull out the most important underlying *directions* in such a matrix, called
    *principal component analysis* (PCA). We will not be going into this in detail
    in this book, because it is not particularly important for you to understand to
    be a deep learning practitioner, but if you are interested, we suggest you check
    out the fast.ai course [Computational Linear Algebra for Coders](https://oreil.ly/NLj2R).
    [Figure 8-3](#img_pca_movie) shows what our movies look like based on two of the
    strongest PCA components.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 直接解释嵌入矩阵并不那么容易。对于人类来说，因素太多了。但有一种技术可以提取出这种矩阵中最重要的基础*方向*，称为*主成分分析*（PCA）。我们不会在本书中详细讨论这个，因为您要成为深度学习从业者并不特别重要，但如果您感兴趣，我们建议您查看fast.ai课程[面向程序员的计算线性代数](https://oreil.ly/NLj2R)。[图8-3](#img_pca_movie)显示了基于两个最强的PCA组件的电影的外观。
- en: '![Representation of movies based on two strongest PCA components](Images/dlcf_0804.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![基于两个最强的PCA组件的电影表示](Images/dlcf_0804.png)'
- en: Figure 8-3\. Representation of movies based on two strongest PCA components
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3. 基于两个最强的PCA组件的电影表示
- en: We can see here that the model seems to have discovered a concept of *classic*
    versus *pop culture* movies, or perhaps it is *critically acclaimed* that is represented
    here.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型似乎已经发现了*经典*与*流行文化*电影的概念，或者这里代表的是*广受好评*。
- en: Jeremy Says
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 杰里米说
- en: No matter how many models I train, I never stop getting moved and surprised
    by how these randomly initialized bunches of numbers, trained with such simple
    mechanics, manage to discover things about my data all by themselves. It almost
    seems like cheating that I can create code that does useful things without ever
    actually telling it how to do those things!
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我训练多少模型，我永远不会停止被这些随机初始化的数字组合所感动和惊讶，这些数字通过如此简单的机制训练，竟然能够自己发现关于我的数据的东西。我几乎觉得可以欺骗，我可以创建一个能够做有用事情的代码，而从未真正告诉它如何做这些事情！
- en: We defined our model from scratch to teach you what is inside, but you can directly
    use the fastai library to build it. We’ll look at how to do that next.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从头开始定义了我们的模型，以教给您内部情况，但您可以直接使用fastai库来构建它。我们将在下一节看看如何做到这一点。
- en: Using fastai.collab
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用fastai.collab
- en: 'We can create and train a collaborative filtering model using the exact structure
    shown earlier by using fastai’s `collab_learner`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用fastai的`collab_learner`使用先前显示的确切结构创建和训练协同过滤模型：
- en: '[PRE47]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '| epoch | train_loss | valid_loss | time |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | time |'
- en: '| --- | --- | --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0.931751 | 0.953806 | 00:13 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.931751 | 0.953806 | 00:13 |'
- en: '| 1 | 0.851826 | 0.878119 | 00:13 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.851826 | 0.878119 | 00:13 |'
- en: '| 2 | 0.715254 | 0.834711 | 00:13 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.715254 | 0.834711 | 00:13 |'
- en: '| 3 | 0.583173 | 0.821470 | 00:13 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.583173 | 0.821470 | 00:13 |'
- en: '| 4 | 0.496625 | 0.821688 | 00:13 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.496625 | 0.821688 | 00:13 |'
- en: 'The names of the layers can be seen by printing the model:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通过打印模型可以看到层的名称：
- en: '[PRE49]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We can use these to replicate any of the analyses we did in the previous section—for
    instance:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些来复制我们在上一节中所做的任何分析，例如：
- en: '[PRE51]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Another interesting thing we can do with these learned embeddings is to look
    at *distance*.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些学到的嵌入来查看*距离*。
- en: Embedding Distance
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入距离
- en: 'On a two-dimensional map, we can calculate the distance between two coordinates
    by using the formula of Pythagoras: <math alttext="StartRoot x squared plus y
    squared EndRoot"><msqrt><mrow><msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <msup><mi>y</mi>
    <mn>2</mn></msup></mrow></msqrt></math> (assuming that *x* and *y* are the distances
    between the coordinates on each axis). For a 50-dimensional embedding, we can
    do exactly the same thing, except that we add up the squares of all 50 of the
    coordinate distances.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维地图上，我们可以通过使用毕达哥拉斯定理的公式来计算两个坐标之间的距离：<math alttext="StartRoot x squared plus
    y squared EndRoot"><msqrt><mrow><msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo>
    <msup><mi>y</mi> <mn>2</mn></msup></mrow></msqrt></math>（假设*x*和*y*是每个轴上坐标之间的距离）。对于一个50维的嵌入，我们可以做完全相同的事情，只是将所有50个坐标距离的平方相加。
- en: 'If there were two movies that were nearly identical, their embedding vectors
    would also have to be nearly identical, because the users who would like them
    would be nearly exactly the same. There is a more general idea here: movie similarity
    can be defined by the similarity of users who like those movies. And that directly
    means that the distance between two movies’ embedding vectors can define that
    similarity. We can use this to find the most similar movie to *Silence of the
    Lambs*:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有两部几乎相同的电影，它们的嵌入向量也必须几乎相同，因为喜欢它们的用户几乎完全相同。这里有一个更一般的想法：电影的相似性可以由喜欢这些电影的用户的相似性来定义。这直接意味着两部电影的嵌入向量之间的距离可以定义这种相似性。我们可以利用这一点找到与“沉默的羔羊”最相似的电影：
- en: '[PRE53]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Now that we have successfully trained a model, let’s see how to deal with the
    situation of having no data for a user. How can we make recommendations to new
    users?
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功训练了一个模型，让我们看看如何处理没有用户数据的情况。我们如何向新用户推荐？
- en: Bootstrapping a Collaborative Filtering Model
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引导协同过滤模型
- en: The biggest challenge with using collaborative filtering models in practice
    is the *bootstrapping problem*. The most extreme version of this problem is having
    no users, and therefore no history to learn from. What products do you recommend
    to your very first user?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中使用协同过滤模型的最大挑战是“引导问题”。这个问题的最极端版本是没有用户，因此没有历史可供学习。您向您的第一个用户推荐什么产品？
- en: 'But even if you are a well-established company with a long history of user
    transactions, you still have the question: what do you do when a new user signs
    up? And indeed, what do you do when you add a new product to your portfolio? There
    is no magic solution to this problem, and really the solutions that we suggest
    are just variations of *use your common sense*. You could assign new users the
    mean of all of the embedding vectors of your other users, but this has the problem
    that that particular combination of latent factors may be not at all common (for
    instance, the average for the science-fiction factor may be high, and the average
    for the action factor may be low, but it is not that common to find people who
    like science-fiction without action). It would probably be better to pick a particular
    user to represent *average taste*.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使您是一家历史悠久的公司，拥有长期的用户交易记录，您仍然会面临一个问题：当新用户注册时，您该怎么办？实际上，当您向您的产品组合添加新产品时，您该怎么办？这个问题没有魔法解决方案，而我们建议的解决方案实际上只是“运用常识”的变体。您可以将新用户分配为其他用户所有嵌入向量的平均值，但这会带来一个问题，即该潜在因素的特定组合可能并不常见（例如，科幻因素的平均值可能很高，而动作因素的平均值可能很低，但很少有人喜欢科幻而不喜欢动作）。最好选择一个特定用户来代表“平均品味”。
- en: Better still is to use a tabular model based on user metadata to construct your
    initial embedding vector. When a user signs up, think about what questions you
    could ask to help you understand their tastes. Then you can create a model in
    which the dependent variable is a user’s embedding vector, and the independent
    variables are the results of the questions that you ask them, along with their
    signup metadata. We will see in the next section how to create these kinds of
    tabular models. (You may have noticed that when you sign up for services such
    as Pandora and Netflix, they tend to ask you a few questions about what genres
    of movie or music you like; this is how they come up with your initial collaborative
    filtering recommendations.)
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是使用基于用户元数据的表格模型来构建您的初始嵌入向量。当用户注册时，考虑一下您可以询问哪些问题来帮助您了解他们的口味。然后，您可以创建一个模型，其中因变量是用户的嵌入向量，而自变量是您问他们的问题的结果，以及他们的注册元数据。我们将在下一节中看到如何创建这些类型的表格模型。（您可能已经注意到，当您注册Pandora和Netflix等服务时，它们往往会问您一些关于您喜欢的电影或音乐类型的问题；这就是它们如何提出您的初始协同过滤推荐的方式。）
- en: One thing to be careful of is that a small number of extremely enthusiastic
    users may end up effectively setting the recommendations for your whole user base.
    This is a very common problem, for instance, in movie recommendation systems.
    People who watch anime tend to watch a whole lot of it, and don’t watch very much
    else, and spend a lot of time putting their ratings on websites. As a result,
    anime tends to be heavily overrepresented in a lot of *best ever movies* lists.
    In this particular case, it can be fairly obvious that you have a problem of representation
    bias, but if the bias is occurring in the latent factors, it may not be obvious
    at all.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，一小部分非常热情的用户可能最终会有效地为整个用户群设置推荐。这是一个非常常见的问题，例如，在电影推荐系统中。看动漫的人往往会看很多动漫，而且不怎么看其他东西，花很多时间在网站上评分。因此，动漫往往在许多“有史以来最佳电影”列表中被过度代表。在这种特殊情况下，很明显您有一个代表性偏见的问题，但如果偏见发生在潜在因素中，可能一点也不明显。
- en: Such a problem can change the entire makeup of your user base, and the behavior
    of your system. This is particularly true because of positive feedback loops.
    If a small number of your users tend to set the direction of your recommendation
    system, they are naturally going to end up attracting more people like them to
    your system. And that will, of course, amplify the original representation bias.
    This type of bias is a natural tendency to be amplified exponentially. You may
    have seen examples of company executives expressing surprise at how their online
    platforms rapidly deteriorated in such a way that they expressed values at odds
    with the values of the founders. In the presence of these kinds of feedback loops,
    it is easy to see how such a divergence can happen both quickly and in a way that
    is hidden until it is too late.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的问题可能会改变您的用户群体的整体构成，以及您系统的行为。这特别是由于正反馈循环。如果您的一小部分用户倾向于设定您的推荐系统的方向，他们自然会吸引更多类似他们的人来到您的系统。这当然会放大原始的表征偏见。这种偏见是一种被指数级放大的自然倾向。您可能已经看到一些公司高管对他们的在线平台如何迅速恶化表示惊讶，以至于表达了与创始人价值观不符的价值观。在存在这种类型的反馈循环的情况下，很容易看到这种分歧如何迅速发生，以及以一种隐藏的方式，直到为时已晚。
- en: In a self-reinforcing system like this, we should probably expect these kinds
    of feedback loops to be the norm, not the exception. Therefore, you should assume
    that you will see them, plan for that, and identify up front how you will deal
    with these issues. Try to think about all of the ways in which feedback loops
    may be represented in your system, and how you might be able to identify them
    in your data. In the end, this is coming back to our original advice about how
    to avoid disaster when rolling out any kind of machine learning system. It’s all
    about ensuring that there are humans in the loop; that there is careful monitoring,
    and a gradual and thoughtful rollout.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样一个自我强化的系统中，我们可能应该预期这些反馈循环是常态，而不是例外。因此，您应该假设您会看到它们，为此做好计划，并提前确定如何处理这些问题。尝试考虑反馈循环可能在您的系统中表示的所有方式，以及您如何能够在数据中识别它们。最终，这又回到了我们关于如何在推出任何类型的机器学习系统时避免灾难的最初建议。这一切都是为了确保有人参与其中；有仔细的监控，以及一个渐进和周到的推出。
- en: Our dot product model works quite well, and it is the basis of many successful
    real-world recommendation systems. This approach to collaborative filtering is
    known as *probabilistic matrix factorization* (PMF). Another approach, which generally
    works similarly well given the same data, is deep learning.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的点积模型效果相当不错，并且是许多成功的现实世界推荐系统的基础。这种协同过滤方法被称为*概率矩阵分解*（PMF）。另一种方法，通常在给定相同数据时效果类似，是深度学习。
- en: Deep Learning for Collaborative Filtering
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协同过滤的深度学习
- en: To turn our architecture into a deep learning model, the first step is to take
    the results of the embedding lookup and concatenate those activations together.
    This gives us a matrix that we can then pass through linear layers and nonlinearities
    in the usual way.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的架构转换为深度学习模型的第一步是获取嵌入查找的结果并将这些激活连接在一起。这给我们一个矩阵，然后我们可以按照通常的方式通过线性层和非线性传递它们。
- en: 'Since we’ll be concatenating the embedding matrices, rather than taking their
    dot product, the two embedding matrices can have different sizes (different numbers
    of latent factors). fastai has a function `get_emb_sz` that returns recommended
    sizes for embedding matrices for your data, based on a heuristic that fast.ai
    has found tends to work well in practice:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将连接嵌入矩阵，而不是取它们的点积，所以两个嵌入矩阵可以具有不同的大小（不同数量的潜在因素）。fastai有一个函数`get_emb_sz`，根据fast.ai发现在实践中往往效果良好的启发式方法，返回推荐的嵌入矩阵大小：
- en: '[PRE55]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let’s implement this class:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现这个类：
- en: '[PRE57]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'And use it to create a model:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用它创建一个模型：
- en: '[PRE58]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '`CollabNN` creates our `Embedding` layers in the same way as previous classes
    in this chapter, except that we now use the `embs` sizes. `self.layers` is identical
    to the mini-neural net we created in [Chapter 4](ch04.xhtml#chapter_mnist_basics)
    for MNIST. Then, in `forward`, we apply the embeddings, concatenate the results,
    and pass this through the mini-neural net. Finally, we apply `sigmoid_range` as
    we have in previous models.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`CollabNN`以与本章中先前类似的方式创建我们的`Embedding`层，只是现在我们使用`embs`大小。`self.layers`与我们在[第4章](ch04.xhtml#chapter_mnist_basics)为MNIST创建的迷你神经网络是相同的。然后，在`forward`中，我们应用嵌入，连接结果，并通过迷你神经网络传递。最后，我们像以前的模型一样应用`sigmoid_range`。'
- en: 'Let’s see if it trains:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是否训练：
- en: '[PRE59]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '| epoch | train_loss | valid_loss | time |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | time |'
- en: '| --- | --- | --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0.940104 | 0.959786 | 00:15 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.940104 | 0.959786 | 00:15 |'
- en: '| 1 | 0.893943 | 0.905222 | 00:14 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.893943 | 0.905222 | 00:14 |'
- en: '| 2 | 0.865591 | 0.875238 | 00:14 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.865591 | 0.875238 | 00:14 |'
- en: '| 3 | 0.800177 | 0.867468 | 00:14 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.800177 | 0.867468 | 00:14 |'
- en: '| 4 | 0.760255 | 0.867455 | 00:14 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.760255 | 0.867455 | 00:14 |'
- en: 'fastai provides this model in `fastai.collab` if you pass `use_nn=True` in
    your call to `collab_learner` (including calling `get_emb_sz` for you), and it
    lets you easily create more layers. For instance, here we’re creating two hidden
    layers, of size 100 and 50, respectively:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在调用`collab_learner`时传递`use_nn=True`（包括为您调用`get_emb_sz`），fastai在`fastai.collab`中提供了这个模型，并且让您轻松创建更多层。例如，在这里我们创建了两个隐藏层，分别为大小100和50：
- en: '[PRE60]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '| epoch | train_loss | valid_loss | time |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | time |'
- en: '| --- | --- | --- | --- |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 1.002747 | 0.972392 | 00:16 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.002747 | 0.972392 | 00:16 |'
- en: '| 1 | 0.926903 | 0.922348 | 00:16 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.926903 | 0.922348 | 00:16 |'
- en: '| 2 | 0.877160 | 0.893401 | 00:16 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.877160 | 0.893401 | 00:16 |'
- en: '| 3 | 0.838334 | 0.865040 | 00:16 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.838334 | 0.865040 | 00:16 |'
- en: '| 4 | 0.781666 | 0.864936 | 00:16 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.781666 | 0.864936 | 00:16 |'
- en: '`learn.model` is an object of type `EmbeddingNN`. Let’s take a look at fastai’s
    code for this class:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`learn.model`是`EmbeddingNN`类型的对象。让我们看一下fastai对这个类的代码：'
- en: '[PRE61]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Wow, that’s not a lot of code! This class *inherits* from `TabularModel`, which
    is where it gets all its functionality from. In `__init__`, it calls the same
    method in `TabularModel`, passing `n_cont=0` and `out_sz=1`; other than that,
    it passes along only whatever arguments it received.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这不是很多代码！这个类*继承*自“TabularModel”，这是它获取所有功能的地方。在“__init__”中，它调用“TabularModel”中的相同方法，传递“n_cont=0”和“out_sz=1”；除此之外，它只传递它收到的任何参数。
- en: 'Although the results of `EmbeddingNN` are a bit worse than the dot product
    approach (which shows the power of carefully constructing an architecture for
    a domain), it does allow us to do something very important: we can now directly
    incorporate other user and movie information, date and time information, or any
    other information that may be relevant to the recommendation. That’s exactly what
    `TabularModel` does. In fact, we’ve now seen that `EmbeddingNN` is just a `TabularModel`,
    with `n_cont=0` and `out_sz=1`. So, we’d better spend some time learning about
    `TabularModel`, and how to use it to get great results! We’ll do that in the next
    chapter.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管“EmbeddingNN”的结果比点积方法稍差一些（这显示了为领域精心构建架构的力量），但它确实允许我们做一件非常重要的事情：我们现在可以直接将其他用户和电影信息、日期和时间信息或任何可能与推荐相关的信息纳入考虑。这正是“TabularModel”所做的。事实上，我们现在已经看到，“EmbeddingNN”只是一个“TabularModel”，其中“n_cont=0”和“out_sz=1”。因此，我们最好花一些时间了解“TabularModel”，以及如何使用它获得出色的结果！我们将在下一章中做到这一点。
- en: Conclusion
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: For our first non–computer vision application, we looked at recommendation systems
    and saw how gradient descent can learn intrinsic factors or biases about items
    from a history of ratings. Those can then give us information about the data.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的第一个非计算机视觉应用，我们研究了推荐系统，并看到梯度下降如何从评分历史中学习有关项目的内在因素或偏差。然后，这些因素可以为我们提供有关数据的信息。
- en: We also built our first model in PyTorch. We will do a lot more of this in the
    next section of the book, but first, let’s finish our dive into the other general
    applications of deep learning, continuing with tabular data.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在PyTorch中构建了我们的第一个模型。在书的下一部分中，我们将做更多这样的工作，但首先，让我们完成对深度学习的其他一般应用的探讨，继续处理表格数据。
- en: Questionnaire
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问卷
- en: What problem does collaborative filtering solve?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 协同过滤解决了什么问题？
- en: How does it solve it?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它是如何解决的？
- en: Why might a collaborative filtering predictive model fail to be a very useful
    recommendation system?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么协同过滤预测模型可能无法成为非常有用的推荐系统？
- en: What does a crosstab representation of collaborative filtering data look like?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 协同过滤数据的交叉表表示是什么样的？
- en: Write the code to create a crosstab representation of the MovieLens data (you
    might need to do some web searching!).
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码创建MovieLens数据的交叉表表示（您可能需要进行一些网络搜索！）。
- en: What is a latent factor? Why is it “latent”?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是潜在因素？为什么它是“潜在”的？
- en: What is a dot product? Calculate a dot product manually using pure Python with
    lists.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是点积？使用纯Python和列表手动计算点积。
- en: What does `pandas.DataFrame.merge` do?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “pandas.DataFrame.merge”是做什么的？
- en: What is an embedding matrix?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是嵌入矩阵？
- en: What is the relationship between an embedding and a matrix of one-hot-encoded
    vectors?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入和一个独热编码向量矩阵之间的关系是什么？
- en: Why do we need `Embedding` if we could use one-hot-encoded vectors for the same
    thing?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们可以使用独热编码向量来做同样的事情，为什么我们需要“Embedding”？
- en: What does an embedding contain before we start training (assuming we’re not
    using a pretrained model)?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们开始训练之前，嵌入包含什么内容（假设我们没有使用预训练模型）？
- en: Create a class (without peeking, if possible!) and use it.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个类（尽量不要偷看！）并使用它。
- en: What does `x[:,0]` return?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “x[:,0]”返回什么？
- en: Rewrite the `DotProduct` class (without peeking, if possible!) and train a model
    with it.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重写“DotProduct”类（尽量不要偷看！）并用它训练模型。
- en: What is a good loss function to use for MovieLens? Why?
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在MovieLens中使用什么样的损失函数是好的？为什么？
- en: What would happen if we used cross-entropy loss with MovieLens? How would we
    need to change the model?
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们在MovieLens中使用交叉熵损失会发生什么？我们需要如何更改模型？
- en: What is the use of bias in a dot product model?
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点积模型中偏差的用途是什么？
- en: What is another name for weight decay?
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重衰减的另一个名称是什么？
- en: Write the equation for weight decay (without peeking!).
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出权重衰减的方程（不要偷看！）。
- en: Write the equation for the gradient of weight decay. Why does it help reduce
    weights?
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出权重衰减的梯度方程。为什么它有助于减少权重？
- en: Why does reducing weights lead to better generalization?
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么减少权重会导致更好的泛化？
- en: What does `argsort` do in PyTorch?
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyTorch中的“argsort”是做什么的？
- en: Does sorting the movie biases give the same result as averaging overall movie
    ratings by movie? Why/why not?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对电影偏差进行排序是否会得到与按电影平均评分相同的结果？为什么/为什么不？
- en: How do you print the names and details of the layers in a model?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何打印模型中层的名称和详细信息？
- en: What is the “bootstrapping problem” in collaborative filtering?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 协同过滤中的“自举问题”是什么？
- en: How could you deal with the bootstrapping problem for new users? For new movies?
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何处理新用户的自举问题？对于新电影呢？
- en: How can feedback loops impact collaborative filtering systems?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反馈循环如何影响协同过滤系统？
- en: When using a neural network in collaborative filtering, why can we have different
    numbers of factors for movies and users?
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在协同过滤中使用神经网络时，为什么我们可以为电影和用户使用不同数量的因素？
- en: Why is there an `nn.Sequential` in the `CollabNN` model?
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在“CollabNN”模型中有一个“nn.Sequential”？
- en: What kind of model should we use if we want to add metadata about users and
    items, or information such as date and time, to a collaborative filtering model?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们想要向协同过滤模型添加有关用户和项目的元数据，或者有关日期和时间等信息，应该使用什么样的模型？
- en: Further Research
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究
- en: 'Take a look at all the differences between the `Embedding` version of `DotProductBias`
    and the `create_params` version, and try to understand why each of those changes
    is required. If you’re not sure, try reverting each change to see what happens.
    (NB: even the type of brackets used in `forward` has changed!)'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看“Embedding”版本的“DotProductBias”和“create_params”版本之间的所有差异，并尝试理解为什么需要进行每一项更改。如果不确定，尝试撤销每个更改以查看发生了什么。（注意：甚至在“forward”中使用的括号类型也已更改！）
- en: Find three other areas where collaborative filtering is being used, and identify
    the pros and cons of this approach in those areas.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到另外三个协同过滤正在使用的领域，并在这些领域中确定这种方法的优缺点。
- en: Complete this notebook using the full MovieLens dataset, and compare your results
    to online benchmarks. See if you can improve your accuracy. Look on the book’s
    website and the fast.ai forums for ideas. Note that there are more columns in
    the full dataset—see if you can use those too (the next chapter might give you
    ideas).
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用完整的MovieLens数据集完成这个笔记本，并将结果与在线基准进行比较。看看你能否提高准确性。在书的网站和fast.ai论坛上寻找想法。请注意，完整数据集中有更多列，看看你是否也可以使用这些列（下一章可能会给你一些想法）。
- en: Create a model for MovieLens that works with cross-entropy loss, and compare
    it to the model in this chapter.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为MovieLens创建一个使用交叉熵损失的模型，并将其与本章中的模型进行比较。
