- en: Capitolo 3\. Integrazione dell'IA e servizio del modello
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。人工智能集成与模型服务
- en: 'Questo lavoro è stato tradotto utilizzando l''AI. Siamo lieti di ricevere il
    tuo feedback e i tuoi commenti: [translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作是用AI翻译的。我们很高兴收到您的反馈和评论：[translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)
- en: In questo capitolo imparerai i meccanismi dei vari modelli GenAI e come servirli
    in un'applicazione FastAPI. Inoltre, utilizzando il [pacchetto Streamlit UI](https://oreil.ly/9BXmn),
    creerai un semplice client browser per interagire con gli endpoint che servono
    i modelli. Esploreremo le diverse strategie di servizio dei modelli, come precaricare
    i modelli per renderli più efficienti e come utilizzare le funzioni FastAPI per
    ilmonitoraggio dei servizi.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习各种GenAI模型的机制以及如何在FastAPI应用程序中提供服务。此外，使用[Streamlit UI包](https://oreil.ly/9BXmn)，你将创建一个简单的浏览器客户端来与提供模型的端点进行交互。我们将探讨不同的模型服务策略，例如如何预加载模型以提高效率，以及如何使用FastAPI的功能进行服务监控。
- en: Per consolidare quanto appreso in questo capitolo, costruiremo progressivamente
    un servizio FastAPI utilizzando modelli GenAI open source che generano testo,
    immagini, audio e geometrie 3D, il tutto partendo da zero. Nei capitoli successivi,
    costruirai la funzionalità di analisi dei documenti e dei contenuti web per il
    tuo servizio GenAI in modo da poter dialogare con loro utilizzando un modello
    linguistico.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固本章所学内容，我们将逐步构建一个FastAPI服务，使用开源的GenAI模型生成文本、图像、音频和3D几何形状，一切从头开始。在接下来的章节中，你将构建你的GenAI服务的文档和内容分析功能，以便能够使用语言模型与他们进行对话。
- en: Nota
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Nel capitolo precedente hai visto come configurare un nuovo progetto FastAPI
    in Python. Assicurati di avere pronta un'installazione fresca prima di leggere
    il resto di questo capitolo. In alternativa, puoi clonare o scaricare il [repository
    GitHub](https://github.com/Ali-Parandeh/building-generative-ai-services) del libro.
    Poi, una volta clonato, passa al ramo `ch03-start`, pronto per i passi da seguire.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你看到了如何配置Python中的新FastAPI项目。在阅读本章的其余部分之前，请确保你有一个新的安装。或者，你可以克隆或下载本书的[GitHub仓库](https://github.com/Ali-Parandeh/building-generative-ai-services)。一旦克隆，切换到`ch03-start`分支，准备执行后续步骤。
- en: Alla fine di questo capitolo, avrai un servizio FastAPI che serve vari modelli
    GenAI open source che potrai testare all'interno dell'interfaccia utente Streamlit.
    Inoltre, il tuo servizio sarà in grado di registrare i dati di utilizzo su disco
    utilizzando il middleware.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到了这一章的结尾，你将拥有一个FastAPI服务，它可以服务于各种开源的GenAI模型，你可以在Streamlit用户界面内对其进行测试。此外，你的服务将能够使用中间件在磁盘上记录使用数据。
- en: Modelli generativi al servizio del cliente
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为客户服务的*生成性*模型
- en: Prima di utilizzare modelli generativi pre-addestrati nella tua applicazione,
    vale la pena di imparare come questi modelli vengono addestrati e generano dati.
    Grazie a questa conoscenza, puoi personalizzare gli interni della tua applicazione
    per migliorare i risultati che fornisci all'utente.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的应用程序中使用预训练的生成性模型之前，了解这些模型是如何训练和生成数据的很有价值。有了这些知识，你可以根据需要定制应用程序的内部结构，以改善提供给用户的结果。
- en: 'In questo capitolo ti mostrerò come servire i modelli in diverse modalità,
    tra cui:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将向你展示如何以不同的模式提供服务，包括：
- en: Modelli*linguistici* basati sull'architettura della rete neurale trasformatrice
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于转换器神经网络架构的*语言*模型
- en: Modelli*audio* nei servizi text-to-speech e text-to-audio basati sull'architettura
    a trasformatori aggressivi
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在text-to-speech和text-to-audio服务中基于激进变换架构的*音频*模型
- en: Modelli di*visione* per servizi text-to-image e text-to-video basati sulla Diffusione
    Stabile e sulle architetture di trasformazione della visione
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于稳定扩散和视觉变换架构的*视觉*模型，用于text-to-image和text-to-video服务
- en: Modelli*3D* per servizi text-to-3D basati sull'architettura del codificatore
    di funzioni implicite condizionali e del decodificatore di diffusione
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于条件隐式函数编码器和解码器架构的text-to-3D服务的*3D*模型
- en: Questo elenco non è esaustivo e copre una manciata di modelli GenAI. Per esplorare
    altri modelli, visita il [repository dei modelli Hugging Face](https://oreil.ly/-4wlQ).^([1](ch03.html#id630))
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表并不全面，只涵盖了一小部分GenAI模型。要探索其他模型，请访问[Hugging Face模型仓库](https://oreil.ly/-4wlQ)^([1](ch03.html#id630))
- en: Modelli linguistici
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型
- en: In questa sezione parliamo dei modelli linguistici, compresi i trasformatori
    e le reti neurali ricorrenti (RNN).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论语言模型，包括转换器和循环神经网络（RNN）。
- en: Trasformatori contro reti neurali ricorrenti
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换器与循环神经网络（RNN）
- en: Il mondo dell'Intelligenza Artificiale è stato scosso dalla pubblicazione dell'importante
    articolo "Attention Is All You Need".^([2](ch03.html#id631)) In questo articolo,
    gli autori proponevano un approccio completamente diverso all'elaborazione del
    linguaggio naturale (NLP) e alla modellazione delle sequenze che si differenziava
    dalle architetture RNN esistenti.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）领域因重要文章“Attention Is All You Need”的发表而受到震动。[2](ch03.html#id631)在这篇文章中，作者提出了一种完全不同的自然语言处理（NLP）和序列建模方法，与现有的循环神经网络（RNN）架构不同。
- en: La[Figura 3-1](#transformer_architecture) mostra una versione semplificata dell'architettura
    del trasformatore proposta nell'articolo originale.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-1](#transformer_architecture)展示了文章原始提案中提出的转换器架构的简化版本。'
- en: '![bgai 0301](assets/bgai_0301.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0301](assets/bgai_0301.png)'
- en: Figura 3-1\. Architettura del trasformatore
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\. 转换器架构
- en: Storicamente, le attività di generazione del testo hanno sfruttato i modelli
    RNN per apprendere modelli in dati sequenziali come il testo libero.Per elaborare
    il testo, questi modelli lo suddividono in piccoli pezzi, come una parola o un
    carattere, chiamati *token*, che possono essere elaborati in sequenza.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，文本生成活动利用循环神经网络（RNN）模型来学习序列数据（如自由文本）中的模型。为了处理文本，这些模型将其分割成小块，如单词或字符，称为*标记（token*），这些标记可以按顺序进行处理。
- en: Le RNN mantengono un archivio di memoria chiamato *vettore di stato*, che trasporta
    le informazioni da un token all'altro per tutta la sequenza di testo, fino alla
    fine. Questo significa che quando si arriva alla fine della sequenza di testo,
    l'impatto dei primi token sul vettore di stato è molto minore rispetto ai token
    più recenti.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）维护一个称为*状态向量（vector of state*）的记忆档案，它将信息从一个标记传递到另一个标记，贯穿整个文本序列，直到序列的末尾。这意味着当到达文本序列的末尾时，最初标记对状态向量的影响远小于较近的标记。
- en: Idealmente, ogni token dovrebbe avere la stessa importanza degli altri token
    in un testo. Tuttavia, poiché le RNN possono prevedere l'elemento successivo in
    una sequenza solo osservando gli elementi che l'hanno preceduto, non riescono
    a cogliere le dipendenze a lungo raggio e a modellare i modelli in grandi porzioni
    di testo. Di conseguenza, non riescono a ricordare o a comprendere le informazioni
    essenziali o il contesto in documenti di grandi dimensioni.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，每个标记（token）在文本中应该具有与其他标记相同的重要性。然而，由于循环神经网络（RNN）只能通过观察前面的元素来预测序列中的下一个元素，因此它们无法捕捉到长距离依赖关系，也无法在大量文本中建模模型。因此，它们无法记住或理解大型文档中的关键信息或上下文。
- en: Con l'invenzione dei trasformatori, la modellazione ricorrente o convoluzionale
    poteva essere sostituita da un approccio più efficiente.Poiché i trasformatori
    non mantengono una memoria di stato nascosta e sfruttano una nuova capacità definita
    *auto-attenzione*, sono in grado di modellare le relazioni tra le parole, indipendentemente
    dalla distanza tra loro in una frase. Questa componente di auto-attenzione consente
    al modello di "porre l'attenzione" sulle parole contestualmente rilevanti all'interno
    di una frase.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 随着转换器的发明，循环或卷积建模可以被更有效的方法所取代。由于转换器不维护隐藏状态的记忆，并利用一种称为*自我注意力（auto-attention*）的新能力，因此它们能够建模单词之间的关系，而不管它们在句子中的距离有多远。这个自我注意力组件允许模型“关注”句子中上下文中相关的单词。
- en: Mentre le RNN modellano le relazioni tra parole vicine in una frase, i trasformatori
    mappano le relazioni a coppie tra ogni parola del testo.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当循环神经网络（RNN）建模句子中相邻单词之间的关系时，转换器将文本中每个单词之间的成对关系映射到关系上。
- en: La[Figura 3-2](#rnn_vs_transformer) mostra come le RNN elaborano le frasi rispetto
    ai trasformatori.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-2](#rnn_vs_transformer)展示了循环神经网络（RNN）与转换器在处理句子方面的差异。'
- en: '![bgai 0302](assets/bgai_0302.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0302](assets/bgai_0302.png)'
- en: Figura 3-2\. RNN rispetto ai trasformatori nell'elaborazione delle frasi
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. RNN与转换器在句子处理中的比较
- en: Ciò che alimenta il sistema di auto-attenzione sono blocchi specializzati chiamati
    *teste di attenzione* che catturano gli schemi di coppia tra le parole come *mappe
    di attenzione*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自我注意力系统由称为*注意力头（heads of attention*）的特殊块提供动力，这些块捕获单词之间的成对模式，如*注意力图（attention
    maps*）。
- en: La [Figura 3-3](#head_attention_map) visualizza la mappa di attenzione di una
    testa di attenzione.^([3](ch03.html#id635)) Le connessioni possono essere bidirezionali
    e lo spessore rappresenta la forza della relazione tra le parole della frase.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-3](#head_attention_map)展示了注意力头（即注意力单元）的注意力图。[3](ch03.html#id635)）连接可以是双向的，而厚度则代表句子中词语之间关系的强度。'
- en: '![bgai 0303](assets/bgai_0303.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0303](assets/bgai_0303.png)'
- en: Figura 3-3\. Vista di una mappa di attenzione all'interno di una testa di attenzione
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3. 注意力头内部的注意力图
- en: Un modello trasformatore contiene diverse teste di attenzione distribuite tra
    gli strati della rete neurale. Ogni testa calcola la propria mappa di attenzione
    in modo indipendente per catturare le relazioni tra le parole concentrandosi su
    determinati schemi negli input. Utilizzando più teste di attenzione, il modello
    può analizzare simultaneamente gli input da diverse angolazioni e contesti per
    comprendere schemi complessi e dipendenze all'interno dei dati.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个变压器模型包含多个注意力头，这些头分布在网络神经层的不同层中。每个注意力头独立计算自己的注意力图，以捕捉输入中特定模式下的词语关系。通过使用多个注意力头，模型可以从不同的角度和上下文中同时分析输入，以理解数据中的复杂模式和依赖关系。
- en: La[Figura 3-4](#model_attention_map) mostra le mappe di attenzione per ogni
    testa (cioè un insieme indipendente di pesi di attenzione) all'interno di ogni
    strato del modello.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-4](#model_attention_map)显示了模型每个层中每个注意力头（即一组独立的注意力权重）的注意力图。'
- en: '![bgai 0304](assets/bgai_0304.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0304](assets/bgai_0304.png)'
- en: Figura 3-4\. Vista delle mappe di attenzione all'interno del modello
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4. 模型内部的注意力图
- en: Le RNN richiedono inoltre una grande potenza di calcolo per l'addestramento,
    in quanto il processo di addestramento non può essere parallelizzato su più GPU
    a causa della natura sequenziale dei loro algoritmi di addestramento. I trasformatori,
    invece, elaborano le parole in modo non sequenziale, quindi possono eseguire i
    meccanismi di attenzione in parallelo sulle GPU.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: RNN还需要大量的计算能力来训练，因为其训练过程不能在多个GPU上并行化，因为其训练算法具有序列性质。相反，变压器以非序列方式处理词语，因此可以在GPU上并行执行注意力机制。
- en: L'efficienza dell'architettura a trasformatori significa che questi modelli
    sono più scalabili se ci sono più dati, potenza di calcolo e memoria. È possibile
    costruire modelli linguistici con un corpus che abbraccia le biblioteche di libri
    prodotti dall'umanità. Tutto ciò che serve è una grande potenza di calcolo e dati
    per addestrare un LLM. E questo è esattamente ciò che ha fatto OpenAI, l'azienda
    dietro la famosa applicazione ChatGPT che era alimentata da diversi LLM proprietari,
    tra cui GPT-4o.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构的效率意味着当有更多数据、计算能力和内存时，这些模型更具可扩展性。可以使用涵盖人类生产的书籍库的语料库构建语言模型。这一切所需的就是强大的计算能力和数据来训练一个大型语言模型（LLM）。这正是OpenAI所做的事情，它是著名应用ChatGPT背后的公司，该应用由多个自有的LLM提供支持，包括GPT-4o。
- en: Al momento in cui scriviamo, i dettagli dell'implementazione dei LLMs di OpenAI
    rimangono un segreto commerciale. Sebbene molti ricercatori abbiano una conoscenza
    generale dei metodi di OpenAI, non è detto che abbiano le risorse per replicarli.
    Tuttavia, da allora sono state rilasciate diverse alternative open source per
    la ricerca e l'uso commerciale, tra cui Llama (Facebook), Gemma (Google), Mistral
    e Falcon, solo per citarne alcune.^([4](ch03.html#id637)) Al momento in cui scriviamo,
    le dimensioni dei modelli variano da 0,05B a 480B parametri (cioè pesi e distorsioni
    del modello) per adattarsi alle tue esigenze.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 到我们撰写本文时，OpenAI的LLM实现细节仍被视为商业机密。尽管许多研究人员对OpenAI的方法有一般了解，但并不意味着他们有资源来复制这些方法。然而，从那时起，已经发布了多个开源替代方案，用于研究和商业用途，包括Llama（Facebook）、Gemma（Google）、Mistral和Falcon等，仅举几个例子。[4](ch03.html#id637)）到我们撰写本文时，模型的规模从0.05B到480B个参数（即模型的权重和偏差）不等，以适应您的需求。
- en: Il servizio di LLM rimane una sfida a causa degli elevati requisiti di memoria,
    che raddoppiano se devi addestrarli e metterli a punto sul tuo set di dati. Questo
    perché il processo di addestramento richiede la memorizzazione nella cache e il
    riutilizzo dei parametri del modello tra i vari lotti di addestramento. Di conseguenza,
    la maggior parte delle organizzazioni può affidarsi a modelli leggeri (fino a
    3B) o alle API di fornitori di LLMs come OpenAI, Anthropic, Cohere, Mistral, ecc.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 服务仍然是一个挑战，因为对内存的高要求，如果需要在你的数据集上训练和调整，这些要求会加倍。这是因为训练过程需要在多个训练批次之间缓存和重用模型参数。因此，大多数组织可以依赖轻量级模型（高达
    3B）或 OpenAI、Anthropic、Cohere、Mistral 等LLM提供商的API。
- en: Con l'aumento della popolarità degli LLMs, diventa ancora più importante capire
    come vengono addestrati e come elaborano i dati, quindi parliamo dei meccanismi
    sottostanti.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLM 的普及，了解它们如何训练和如何处理数据变得尤为重要，因此我们来谈谈背后的机制。
- en: Tokenizzazione e incorporazione
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词和嵌入
- en: Le reti neurali non possono elaborare direttamente le parole perché sono grandi
    modelli statistici che funzionano con i numeri. Per colmare il divario tra linguaggio
    e numeri, è necessario ricorrere alla *tokenizzazione*. Con la tokenizzazione,
    si scompone il testo in pezzi più piccoli che un modello può elaborare.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络不能直接处理单词，因为它们是处理数字的大型统计模型。为了弥合语言和数字之间的差距，需要依赖 *分词*。通过分词，将文本分解成模型可以处理的更小的片段。
- en: Qualsiasi testo deve essere innanzitutto suddiviso in un elenco di *token* che
    rappresentano parole, sillabe, simboli e punteggiature. Questi token vengono poi
    mappati in numeri unici in modo da poter modellare numericamente i modelli.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 任何文本都必须首先分为代表单词、音节、符号和标点的 *标记* 列表。然后，这些标记被映射为唯一的数字，以便可以数值化地建模。
- en: Fornendo un vettore di token in ingresso a un trasformatore addestrato, la rete
    può prevedere il token successivo migliore per generare il testo, una parola alla
    volta.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 向训练好的转换器提供输入的标记向量，网络可以预测生成文本的最佳下一个标记，一次一个单词。
- en: La[Figura 3-5](#openai_tokenizer) mostra come il tokenizer di OpenAI converte
    il testo in una sequenza di token, assegnando a ciascuno di essi degli identificatori
    unici.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-5](#openai_tokenizer) 展示了 OpenAI 分词器如何将文本转换为一系列标记，并为每个标记分配唯一的标识符。'
- en: '![bgai 0305](assets/bgai_0305.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0305](assets/bgai_0305.png)'
- en: 'Figura 3-5\. Tokenizer di OpenAI (Fonte: [OpenAI](https://oreil.ly/S-a9M))'
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-5\. OpenAI 分词器（来源：[OpenAI](https://oreil.ly/S-a9M))
- en: Quindi, cosa si può fare dopo aver tokenizzato un testo? Questi token devono
    essere elaborati ulteriormente prima che un modello linguistico possa elaborarli.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在分词文本之后可以做什么？这些标记在语言模型可以处理之前需要进一步处理。
- en: Dopo la tokenizzazione, è necessario utilizzare un *embedder*^([5](ch03.html#id647))
    per convertire questi token in vettori densi di numeri reali chiamati *embeddings*,
    che catturano le informazioni semantiche (cioè il significato di ogni token) in
    uno spazio vettoriale continuo. La[Figura 3-6](#embeddings) mostra questi embeddings.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 分词之后，需要使用一个 *嵌入器*^([5](ch03.html#id647)) 将这些标记转换为实数密集向量，称为 *嵌入*，这些嵌入在连续向量空间中捕获语义信息（即每个标记的含义）。[图
    3-6](#embeddings) 展示了这些嵌入。
- en: '![bgai 0306](assets/bgai_0306.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0306](assets/bgai_0306.png)'
- en: Figura 3-6\. Assegnazione di un vettore di incorporamento di dimensionen a ciascun
    token durante il processo diincorporamento
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-6\. 在嵌入过程中为每个标记分配一个嵌入向量
- en: Suggerimento
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 建议
- en: Questi vettori di incorporamento utilizzano piccoli *numeri in virgola mobile*
    (non numeri interi) per catturare le relazioni sfumate tra i token con maggiore
    flessibilità e precisione. Inoltre, tendono a essere *distribuiti normalmente*,
    quindi l'addestramento e l'inferenza del modello linguistico possono essere più
    stabili e coerenti.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入向量使用小的 *浮点数*（非整数）来捕捉标记之间微妙的关系，具有更大的灵活性和精确度。此外，它们通常呈正态分布，因此语言模型的训练和推理可以更加稳定和一致。
- en: Dopo il processo di embedding, a ogni token viene assegnato un vettore di embedding
    composto da *n* numeri. Ogni numero del vettore di embedding si concentra su una
    dimensione che rappresenta un aspetto specifico del significato del token.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入过程之后，每个标记都被分配了一个由 *n* 个数字组成的嵌入向量。嵌入向量中的每个数字都专注于表示标记含义的一个特定维度。
- en: Trasformatori di formazione
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练转换器
- en: Una volta ottenuta una serie di vettori di incorporamento, puoi addestrare un
    modello sui tuoi documenti per aggiornare i valori all'interno di ciascun incorporamento.
    Durante l'addestramento del modello, l'algoritmo di addestramento aggiorna i parametri
    degli strati di incorporamento in modo che i vettori di incorporamento descrivano
    il significato di ciascun token il più possibile all'interno del testo in ingresso.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得一系列嵌入向量，你可以在你的文档上训练一个模型来更新每个嵌入中的值。在模型训练过程中，训练算法会更新嵌入层的参数，使得嵌入向量尽可能描述输入文本中每个标记的意义。
- en: Capire come funzionano i vettori di incorporamento può essere difficile, quindi
    proviamo un approccio di visualizzazione.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 理解嵌入向量的工作原理可能很困难，因此我们尝试一种可视化方法。
- en: Immagina di utilizzare vettori di incorporamento bidimensionali, cioè contenenti
    solo due numeri. Se tracci questi vettori, prima e dopo l'addestramento del modello,
    osserverai dei grafici simili a quelli della [Figura 3-7](#untrained_to_trained_transformer).
    I vettori di incorporamento di token, o parole, con significati simili saranno
    più vicini tra loro.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你使用的是二维嵌入向量，即只包含两个数字的向量。如果你在模型训练前后追踪这些向量，你会观察到类似于[图3-7](#untrained_to_trained_transformer)中的图表。具有相似意义的标记或单词向量将彼此更接近。
- en: '![bgai 0307](assets/bgai_0307.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0307](assets/bgai_0307.png)'
- en: Figura 3-7\. Spazio latente di addestramento della rete di trasformatori utilizzando
    i vettori di incorporazione
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7. 使用嵌入向量训练的变换器网络潜在空间
- en: Per determinare la somiglianza tra due parole, puoi calcolare l'angolo tra i
    vettori utilizzando un calcolo noto come *somiglianza del coseno*. Angoli più
    piccoli implicano una maggiore somiglianza, che rappresenta un contesto e un significato
    simili. Dopo l'addestramento, il calcolo della somiglianza del coseno di due vettori
    di incorporamento con significati simili convaliderà che questi vettori sono vicini
    tra loro.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定两个单词之间的相似度，你可以使用一种称为*余弦相似度*的计算方法。较小的角度意味着更大的相似度，这表示语境和意义相似。在训练后，计算具有相似意义的嵌入向量之间的余弦相似度将验证这些向量彼此之间是接近的。
- en: La[Figura 3-8](#embedding_vectors) illustra l'intero processo di tokenizzazione,
    incorporazione e formazione.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-8](#embedding_vectors)展示了整个分词、嵌入和形成的全过程。'
- en: '![bgai 0308](assets/bgai_0308.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0308](assets/bgai_0308.png)'
- en: Figura 3-8\. Elaborazione di dati sequenziali come un testo in un vettore di
    token e di incorporazioni di token
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8. 将序列数据（如文本）作为标记向量和标记嵌入进行加工
- en: Una volta ottenuto un livello di incorporazione addestrato, puoi utilizzarlo
    per incorporare qualsiasi nuovo testo in ingresso nel modello di trasformatore
    mostrato nella [Figura 3-1](#transformer_architecture).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得训练好的嵌入层，你可以用它来将任何新的输入文本嵌入到[图3-1](#transformer_architecture)中所示的变换器模型中。
- en: Codifica posizionale
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置编码
- en: Un'ultima fase prima di inoltrare i vettori di incorporamento agli strati di
    attenzione della rete di trasformazione consiste nell'implementare la *codifica
    posizionale*. Il processo di codifica posizionale produce i vettori di incorporamento
    posizionale che vengono poi sommati ai vettori di incorporamento dei token.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在将嵌入向量传递到变换器网络的注意力层之前，最后一个阶段是实施*位置编码*。位置编码过程产生位置嵌入向量，然后这些向量被加到标记嵌入向量上。
- en: Poiché i trasformatori elaborano le parole simultaneamente anziché in sequenza,
    sono necessari embedding posizionali per registrare l'ordine delle parole e il
    contesto all'interno dei dati sequenziali, come le frasi. I vettori embedding
    risultanti catturano sia il significato che le informazioni posizionali delle
    parole nelle frasi prima di passarle ai meccanismi di attenzione del trasformatore.
    Questo processo assicura che le teste di attenzione abbiano tutte le informazioni
    necessarie per apprendere efficacemente i modelli.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于变换器是同时而不是按顺序处理单词的，因此需要位置嵌入来记录序列数据（如句子）中的单词顺序和上下文。结果向量嵌入捕捉了句子中单词的意义和位置信息，在传递给变换器的注意力机制之前。这个过程确保注意力头拥有所有必要的信息来有效地学习模型。
- en: La[Figura 3-9](#positional_encoding) mostra il processo di codifica posizionale
    in cui le incorporazioni posizionali vengono sommate alle incorporazioni dei token.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-9](#positional_encoding)展示了位置编码过程，其中位置嵌入被加到标记嵌入上。'
- en: '![bgai 0309](assets/bgai_0309.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0309](assets/bgai_0309.png)'
- en: Figura 3-9\. Codifica posizionale
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-9\. 位置编码
- en: Previsione autoregressiva
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自回归预测
- en: Il trasformatore è un modello autoregressivo (cioè sequenziale), in quanto le
    previsioni future si basano sui valori passati, come mostrato nella [Figura 3-10](#autoregressive_prediction3).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器是一个自回归模型（即序列模型），因为未来的预测基于过去的价值，如图 3-10 所示。[#autoregressive_prediction3](#autoregressive_prediction3)。
- en: '![bgai 0310](assets/bgai_0310.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0310](assets/bgai_0310.png)'
- en: Figura 3-10\. Previsione autoregressiva
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-10\. 自回归预测
- en: Il modello riceve dei token in ingresso che vengono poi incorporati e passati
    attraverso la rete per fare la migliore previsione del token successivo. Questo
    processo si ripete fino a quando non viene generato un token `<stop>` o di fine
    frase `<eos>`.^([6](ch03.html#id658))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型接收输入标记，然后将其嵌入并通过网络进行预测以生成下一个标记的最佳预测。这个过程会一直重复，直到生成一个`<stop>`或句子结束标记`<eos>`。^([6](ch03.html#id658))
- en: Tuttavia, c'è un limite al numero di token che il modello può immagazzinare
    nella sua memoria per generare il token successivo.Questo limite di token è indicato
    come la *finestra di contesto* del modello, un fattore importante da considerare
    durante la fase di selezione del modello per i tuoi servizi GenAI.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型在其内存中可以存储的标记数量有一个上限，用于生成下一个标记。这个标记上限被称为模型的*上下文窗口*，在选择用于你的GenAI服务的模型时是一个重要的考虑因素。
- en: Se il limite della finestra di contesto viene raggiunto, il modello scarta semplicemente
    i token utilizzati più di recente, il che significa che può *dimenticare* le frasi
    utilizzate più di recente nei documenti o nei messaggi di una conversazione.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果达到上下文窗口的极限，模型会简单地丢弃最近使用的标记，这意味着它可能会*忘记*在文档或对话消息中最近使用的句子。
- en: Nota
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Al momento in cui scriviamo, il contesto del modello OpenAI `gpt-4o-mini` meno
    costoso è di circa 128.000 tokens, equivalenti a più di 300 pagine di testo.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们撰写本文时，成本较低的OpenAI `gpt-4o-mini`模型的上下文约为128,000个标记，相当于超过300页的文本。
- en: La finestra di contesto più grande a marzo 2025 appartiene a [Magic.Dev LTM-2-mini](https://oreil.ly/10Mj1)
    con 100 milioni di gettoni. Ciò equivale a ~10 milioni di righe di codice di ~750
    romanzi.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 2025年3月最大的上下文窗口属于[Magic.Dev LTM-2-mini](https://oreil.ly/10Mj1)，拥有1亿个标记。这相当于约750部小说的约1000万行代码。
- en: La finestra di contesto di altri modelli si aggira intorno alle centinaia di
    migliaia di gettoni.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其他模型的上下文窗口大约在数十万个标记左右。
- en: Finestre brevi comportano la perdita di informazioni, la difficoltà di mantenere
    le conversazioni e una minore coerenza con la query dell'utente.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 短窗口会导致信息丢失，难以维持对话，以及与用户查询的连贯性降低。
- en: D'altro canto, le finestre di contesto più lunghe hanno requisiti di memoria
    più elevati e possono causare problemi di prestazioni o rallentamenti dei servizi
    quando si scalano a migliaia di utenti contemporanei che utilizzano il tuo servizio.
    Inoltre, dovrai considerare i costi di affidarti a modelli con finestre di contesto
    più ampie, in quanto tendono ad essere più costosi a causa dei maggiori requisiti
    di calcolo e di memoria. La scelta corretta dipenderà dal tuo budget e dalle esigenze
    degli utenti nel tuo caso d'uso.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，较长的上下文窗口需要更高的内存要求，并且当有成千上万的用户同时使用你的服务时，可能会引起性能问题或服务延迟。此外，你还需要考虑使用具有较宽上下文窗口的模型的成本，因为它们通常更昂贵，因为它们对计算和内存的要求更高。正确的选择将取决于你的预算和你的用例中用户的需求。
- en: Integrare un modello linguistico nella tua applicazione
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在你的应用程序中集成语言模型
- en: Puoi scaricare e utilizzare un modello linguistico all'interno della tua applicazione
    con poche righe di codice.Nell'[Esempio 3-1](#language_model_usage_example), scaricherai
    un modello TinyLlama con 1,1 miliardi di parametri e preaddestrato su 3 trilioni
    di token.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用几行代码在你的应用程序中下载和使用语言模型。在[Nel Esempio 3-1](#language_model_usage_example)中，你将下载一个拥有1.1亿参数、在3000亿标记上预训练的TinyLlama模型。
- en: '[PRE0] # models.py  import torch from transformers import Pipeline, pipeline  prompt
    = "How to set up a FastAPI project?" system_prompt = """ Your name is FastAPI
    bot and you are a helpful chatbot responsible for teaching FastAPI to your users.
    Always respond in markdown. """  device = torch.device("cuda" if torch.cuda.is_available()
    else "cpu") ![1](assets/1.png)  def load_text_model():     pipe = pipeline(         "text-generation",         model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    ![2](assets/2.png)         torch_dtype=torch.bfloat16,         device=device ![3](assets/3.png)     )     return
    pipe   def generate_text(pipe: Pipeline, prompt: str, temperature: float = 0.7)
    -> str:     messages = [         {"role": "system", "content": system_prompt},         {"role":
    "user", "content": prompt},     ] ![4](assets/4.png)     prompt = pipe.tokenizer.apply_chat_template(         messages,
    tokenize=False, add_generation_prompt=True     ) ![5](assets/5.png)     predictions
    = pipe(         prompt,         temperature=temperature,         max_new_tokens=256,         do_sample=True,         top_k=50,         top_p=0.95,     )
    ![6](assets/6.png)     output = predictions[0]["generated_text"].split("</s>\n<|assistant|>\n")[-1]
    ![7](assets/7.png)     return output [PRE1] # main.py  from fastapi import FastAPI
    from models import load_text_model, generate_text  app = FastAPI()  @app.get("/generate/text")
    ![1](assets/1.png) def serve_language_model_controller(prompt: str) -> str: ![2](assets/2.png)     pipe
    = load_text_model() ![3](assets/3.png)     output = generate_text(pipe, prompt)
    ![4](assets/4.png)     return output ![5](assets/5.png) [PRE2] http://localhost:8000/generate/text?prompt="What
    is FastAPI?" [PRE3]`  [PRE4] $ pip install streamlit [PRE5]` L[''esempio 3-3](#streamlit_chat_ui)
    mostra come sviluppare una semplice interfaccia utente per connettersi al servizio.    #####
    Esempio 3-3\. L''interfaccia utente della chat di Streamlit che utilizza l''endpoint
    FastAPI /`generate`    [PRE6]    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO3-1)      Aggiungi
    un titolo alla tua applicazione che sarà reso all''interfaccia utente.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO3-2)      Inizializza
    la chat e tiene traccia della cronologia della chat.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO3-3)      Visualizza
    i messaggi della cronologia delle chat al riavvio dell''applicazione.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO3-4)      Attendi
    che l''utente invii un prompt tramite il campo di inserimento della chat.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO3-5)      Aggiungi
    i messaggi dell''utente o dell''assistente alla cronologia delle chat.      [![6](assets/6.png)](#co_ai_integration_and_model_serving_CO3-6)      Visualizza
    il messaggio dell''utente nel contenitore dei messaggi della chat.      [![7](assets/7.png)](#co_ai_integration_and_model_serving_CO3-7)      Invia
    una richiesta `GET` con il prompt come parametro di query al tuo endpoint FastAPI
    per generare una risposta da TinyLlama.      [![8](assets/8.png)](#co_ai_integration_and_model_serving_CO3-8)      Convalida
    che la risposta sia OK.      [![9](assets/9.png)](#co_ai_integration_and_model_serving_CO3-9)      Visualizza
    il messaggio dell''assistente nel contenitore dei messaggi della chat.      Ora
    puoi avviare l''applicazione client Streamlit:^([11](ch03.html#id674))    [PRE7]   `Ora
    dovresti essere in grado di interagire con TinyLlama all''interno di Streamlit,
    come mostrato nella [Figura 3-12](#streamlit_ui_text_results). Tutto questo è
    stato possibile con alcuni brevi script Python.  ![bgai 0312](assets/bgai_0312.png)  ######
    Figura 3-12\. Client Streamlit    La[Figura 3-13](#tiny_llama_fastapi_architecture)
    mostra l''architettura generale del sistema della soluzione che abbiamo sviluppato
    finora.  ![bgai 0313](assets/bgai_0313.png)  ###### Figura 3-13\. Architettura
    del sistema di servizi FastAPI    ###### Avvertenze    Sebbene la soluzione dell''[Esempio
    3-3](#streamlit_chat_ui) sia ottima per la prototipazione e il test dei modelli,
    non è adatta per i carichi di lavoro di produzione in cui diversi utenti hanno
    bisogno di accedere simultaneamente al modello. Questo perché con la configurazione
    attuale, il modello viene caricato e scaricato in memoria ogni volta che viene
    elaborata una richiesta. Dover caricare e scaricare un modello di grandi dimensioni
    da e verso la memoria è lento eblocca l''I/O.    Il servizio TinyLlama che hai
    appena costruito utilizza un trasformatore *decodificatore*, ottimizzato per i
    casi d''uso delle conversazioni e delle chat. Tuttavia, il [documento originale
    sui trasformatori](https://oreil.ly/RqztC) introduceva un''architettura che consisteva
    sia in un encoder che in un decoder.    A questo punto dovresti essere più sicuro
    di come funzionano i modelli linguistici e di come confezionarli in un server
    web FastAPI.    I modelli linguistici rappresentano solo una parte di tutti i
    modelli generativi. Le prossime sezioni amplieranno le tue conoscenze per includere
    il funzionamento e il servizio dei modelli che generano audio, immagini e video.    Possiamo
    iniziare a lavorare con i modelli audio.` [PRE8]``  [PRE9][PRE10]``py[PRE11]``
    ## Modelli audio    Nei servizi GenAI, i modelli audio sono importanti per la
    creazione di suoni interattivi e realistici. A differenza dei modelli di testo
    con cui hai familiarità, che si concentrano sull''elaborazione e la generazione
    del testo, i modelli audio possono gestire i segnali audio. Con essi puoisintetizzare
    il parlato, generare musica e persino creare effetti sonori per applicazioni come
    gli assistentivirtuali, il doppiaggio automatico, lo sviluppo di giochi e gliambienti
    audio immersivi.    Uno dei modelli text-to-speech e text-to-audio più efficaci
    è il modello Bark creato da Suno AI. Questo modello, basato su trasformatori,
    è in grado di generare un parlato e un audio multilingue realistico che include
    musica, rumore di fondo ed effetti sonori.    Il modello Bark è composto da quattro
    modelli concatenati tra loro come una pipeline per sintetizzare forme d''onda
    audio a partire da prompt testuali, come mostrato nella [Figura 3-15](#bark_pipeline).  ![bgai
    0315](assets/bgai_0315.png)  ###### Figura 3-15\. Pipeline di sintesi della corteccia    1\.
    Modello semantico del testo      Un modello trasformatore autoregressivo causale
    (sequenziale) accetta un testo di input tokenizzato e ne cattura il significato
    tramite tokens semantici. I modelli autoregressivi predicono i valori futuri di
    una sequenza riutilizzando i propri output precedenti.      2\. Modello acustico
    grossolano      Un trasformatore autoregressivo causale riceve le uscite del modello
    semantico e genera le caratteristiche audio iniziali, che mancano di dettagli
    più fini. Ogni previsione si basa sulle informazioni passate e presenti della
    sequenza di token semantici.      3\. Modello di acustica fine      Un trasformatore
    autocodificatore non causale perfeziona la rappresentazione audio generando le
    caratteristiche audio rimanenti. Poiché il modello acustico grossolano ha generato
    l''intera sequenza audio, il modello fine non deve essere casuale.      4\. Modello
    di codec audio Encodec      Il modello decodifica l''array audio in uscita da
    tutti i codici audio generati in precedenza.      Bark sintetizza la forma d''onda
    audio decodificando le caratteristiche audio raffinate nell''uscita audio finale
    sotto forma di parole, musica o semplici effetti audio.    L[''esempio 3-4](#small_bark)
    mostra come utilizzare il modello del piccolo Bark.    ##### Esempio 3-4\. Scarica
    e carica il modello della piccola corteccia dal repository di Hugging Face    [PRE12]    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO4-1)      Specifica
    le opzioni di preselezione vocale supportate utilizzando un tipo di `Literal`.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO4-2)      Scarica
    il piccolo processore Bark, che prepara il prompt di testo per il modello principale.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO4-3)      Scarica
    il modello Bark, che verrà utilizzato per generare l''audio in uscita. Entrambi
    gli oggetti saranno necessari per la generazione dell''audio in seguito.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO4-4)      Preelabora
    il prompt di testo con un embedding della voce dell''altoparlante e restituisce
    un array di tensori PyTorch di input tokenizzati utilizzando `return_tensors="pt"`.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO4-5)      Genera
    un array audio che contiene i valori di ampiezza del segnale audio sintetizzato
    nel tempo.      [![6](assets/6.png)](#co_ai_integration_and_model_serving_CO4-6)      Ottieni
    la frequenza di campionamento dalle configurazioni di generazione del modello,
    che possono essere utilizzate per produrre l''audio.      Quando generi l''audio
    utilizzando un modello, l''output è una sequenza di numeri a virgola mobile che
    rappresentano l''*ampiezza* (o la forza) del segnale audio in ogni momento.    Per
    riprodurre l''audio, è necessario convertirlo in un formato digitale che possa
    essere inviato agli altoparlanti, il che comporta il campionamento del segnale
    audio a una frequenza fissa e la quantizzazione dei valori di ampiezza a un numero
    fisso di bit. La libreria `soundfile` può aiutarti a generare il file audio utilizzando
    una *frequenza di campionamento*. Più alta è la frequenza di campionamento, più
    campioni vengono prelevati, migliorando la qualità dell''audio ma aumentando anche
    le dimensioni del file.    Puoi installare la libreria audio `soundfile` per scrivere
    file audio utilizzando `pip`:    [PRE13]   [PRE14] # utils.py  from io import
    BytesIO import soundfile import numpy as np  def audio_array_to_buffer(audio_array:
    np.array, sample_rate: int) -> BytesIO:     buffer = BytesIO()     soundfile.write(buffer,
    audio_array, sample_rate, format="wav") ![1](assets/1.png)     buffer.seek(0)     return
    buffer ![2](assets/2.png)  # main.py  from fastapi import FastAPI, status from
    fastapi.responses import StreamingResponse  from models import load_audio_model,
    generate_audio from schemas import VoicePresets from utils import audio_array_to_buffer  @app.get(     "/generate/audio",     responses={status.HTTP_200_OK:
    {"content": {"audio/wav": {}}}},     response_class=StreamingResponse, ) ![3](assets/3.png)
    def serve_text_to_audio_model_controller(     prompt: str,     preset: VoicePresets
    = "v2/en_speaker_1", ):     processor, model = load_audio_model()     output,
    sample_rate = generate_audio(processor, model, prompt, preset)     return StreamingResponse(         audio_array_to_buffer(output,
    sample_rate), media_type="audio/wav"     ) ![4](assets/4.png) [PRE15] # client.py  for
    message in st.session_state.messages:     with st.chat_message(message["role"]):         content
    = message["content"]         if isinstance(content, bytes):             st.audio(content)         else:             st.markdown(content)   if
    prompt := st.chat_input("Write your prompt in this input field"):     response
    = requests.get(         f"http://localhost:8000/generate/audio", params={"prompt":
    prompt}     )     response.raise_for_status()     with st.chat_message("assistant"):         st.text("Here
    is your generated audio")         st.audio(response.content) ![1](assets/1.png)
    [PRE16]`  [PRE17]`` ## Modelli di visione    Utilizzando i modelli di visione,
    puoi generare, migliorare e comprendere le informazioni visive contenute nei prompt.    Poiché
    questi modelli sono in grado di produrre risultati molto realistici più velocemente
    di qualsiasi essere umano e di comprendere e manipolare i contenuti visivi esistenti,
    sono estremamente utili per applicazioni come i generatori e gli editor di immagini,
    il rilevamento di oggetti, la classificazione e la didascalia delle immagini e
    la realtà aumentata.    Una delle architetture più popolari utilizzate per addestrare
    i modelli di immagini si chiama *Diffusione Stabile* (SD).    I modelli SD vengono
    addestrati per codificare le immagini in ingresso in uno spazio latente. Questo
    spazio latente è la rappresentazione matematica dei modelli nei dati di addestramento
    che il modello ha appreso. Se provi a visualizzare un''immagine codificata, tutto
    ciò che vedresti è un''immagine di rumore bianco, simile ai punti bianchi e neri
    che vedi sullo schermo della tua TV quando perde il segnale.    La[Figura 3-17](#stable_diffusion)
    mostra l''intero processo di addestramento e inferenza e visualizza come le immagini
    vengono codificate e decodificate attraverso i processi di diffusione in avanti
    e inversa. Un codificatore di testo che utilizza testo, immagini e mappe semantiche
    aiuta a controllare l''output attraverso la diffusione inversa.  ![bgai 0317](assets/bgai_0317.png)  ######
    Figura 3-17\. Formazione e inferenza della diffusione stabile    Ciò che rende
    questi modelli magici è la loro capacità di decodificare le immagini rumorose
    in immagini di input originali. In effetti, i modelli SD imparano anche a rimuovere
    il rumore bianco da un''immagine codificata per riprodurre l''immagine originale.
    Il modello esegue questo processo di denoising in diverse iterazioni.    Tuttavia,
    non vuoi ricreare immagini già esistenti, ma vuoi che il modello crei nuove immagini
    mai viste prima. Ma come può un modello SD raggiungere questo obiettivo? La risposta
    sta nello spazio latente in cui risiedono le immagini rumorose codificate. Puoi
    modificare il rumore di queste immagini in modo che quando il modello le denobilita
    e le decodifica, ottieni un''immagine completamente nuova che il modello non ha
    mai visto prima.    La soluzione consiste nel codificare anche le descrizioni
    dell''immagine accanto all''immagine stessa. I modelli nello spazio latente vengono
    quindi mappati in descrizioni testuali dell''immagine che si vede in ogni immagine
    di input. A questo punto, si utilizzano prompt testuali per campionare lo spazio
    latente rumoroso in modo che l''immagine di output prodotta dopo il processo di
    denoising sia quella desiderata.    In questo modo i modelli SD possono generare
    nuove immagini che non hanno mai visto prima nei loro dati di formazione. In sostanza,
    questi modelli navigano in uno spazio latente che contiene rappresentazioni codificate
    di vari modelli e significati.^([12](ch03.html#id686)) Il modello affina iterativamente
    questo rumore attraverso un processo di denoising per produrre un''immagine nuova
    non presente nel set di dati di addestramento.    Per scaricare un modello SD,
    è necessario che sia installata la libreria Hugging Face `diffusers`:    [PRE18]   [PRE19]
    # models.py  import torch from diffusers import DiffusionPipeline, StableDiffusionInpaintPipelineLegacy
    from PIL import Image  device = torch.device("cuda" if torch.cuda.is_available()
    else "cpu")  def load_image_model() -> StableDiffusionInpaintPipelineLegacy:     pipe
    = DiffusionPipeline.from_pretrained(         "segmind/tiny-sd", torch_dtype=torch.float32,         device=device     )
    ![1](assets/1.png)     return pipe  def generate_image(     pipe: StableDiffusionInpaintPipelineLegacy,
    prompt: str ) -> Image.Image:     output = pipe(prompt, num_inference_steps=10).images[0]
    ![2](assets/2.png) ![3](assets/3.png)     return output ![4](assets/4.png) [PRE20]
    # utils.py  from typing import Literal from PIL import Image from io import BytesIO  def
    img_to_bytes(     image: Image.Image, img_format: Literal["PNG", "JPEG"] = "PNG"
    ) -> bytes:     buffer = BytesIO()     image.save(buffer, format=img_format)     return
    buffer.getvalue() ![1](assets/1.png)  # main.py  from fastapi import FastAPI,
    Response, status from models import load_image_model, generate_image from utils
    import img_to_bytes  ...  @app.get("/generate/image",          responses={status.HTTP_200_OK:
    {"content": {"image/png": {}}}}, ![2](assets/2.png)          response_class=Response)
    ![3](assets/3.png) def serve_text_to_image_model_controller(prompt: str):     pipe
    = load_image_model()     output = generate_image(pipe, prompt) ![4](assets/4.png)     return
    Response(content=img_to_bytes(output), media_type="image/png") ![5](assets/5.png)
    [PRE21] # client.py  ...  for message in st.session_state.messages:     with st.chat_message(message["role"]):         st.image(message["content"])
    ![1](assets/1.png) ...  if prompt := st.chat_input("Write your prompt in this
    input field"):     ...     response = requests.get(         f"http://localhost:8000/generate/image",
    params={"prompt": prompt}     ) ![2](assets/2.png)     response.raise_for_status()     with
    st.chat_message("assistant"):         st.text("Here is your generated image")         st.image(response.content)      ...
    [PRE22]`  [PRE23]` ## Modelli video    I modelli video sono tra i modelli generativi
    più avidi di risorse e spesso richiedono una GPU per produrre un breve frammento
    di buona qualità. Questi modelli devono generare diverse decine di fotogrammi
    per produrre un singolo secondo di video, anche senza alcun contenuto audio.    Stability
    AI ha rilasciato diversi modelli video open source basati sull''architettura SD
    di Hugging Face. Lavoreremo con la versione compressa del loro modello immagine-video
    per un servizio di animazione delle immagini più veloce.    Per iniziare, facciamo
    funzionare un piccolo modello immagine-video usando l''[Esempio 3-10](#video_model_loading).    ######
    Nota    Per eseguire l''[Esempio 3-10](#video_model_loading), potresti aver bisogno
    di accedere a una GPU NVIDIA compatibile con CUDA.    Inoltre, per l''uso commerciale
    del modello `stable-video-diffusion-img2vid`, fai riferimento alla sua [scheda
    modello](https://oreil.ly/DM-0p).    ##### Esempio 3-10\. Scarica e carica il
    modello *img2vid* dell''IA Stability dal repository Hugging Face.    [PRE24]    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO10-1)      Ridimensiona
    l''immagine in ingresso a una dimensione standard prevista dal modello di input.
    Il ridimensionamento protegge anche da input di grandi dimensioni.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO10-2)      Crea
    un generatore di tensori casuali con il seme impostato a 42 per generare fotogrammi
    video riproducibili.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO10-3)      Esegui
    la pipeline di generazione dei fotogrammi per produrre tutti i fotogrammi video
    in una volta sola.Afferra il primo gruppo di fotogrammi generati. Questo passaggio
    richiede una notevole quantità di memoria video.`num_frames` specifica il numero
    di fotogrammi da generare, mentre`decode_chunk_size` specifica quanti fotogrammi
    generare in una volta sola.      Con le funzioni di caricamento del modello già
    pronte, puoi ora creare l''endpoint per il servizio video.    Tuttavia, prima
    di procedere con la dichiarazione del route handler, hai bisogno di una funzione
    di utilità che elabori gli output del modello video dai fotogrammi in un video
    in streaming utilizzando un buffer I/O.    Per esportare una sequenza di fotogrammi
    in video, devi codificarli in un contenitore video utilizzando una libreria video
    come `av`, che implementa i binding di Python alla popolare libreria di elaborazione
    video`ffmpeg`.    Puoi installare la libreria `av` tramite:    [PRE25]   [PRE26]
    # utils.py  from io import BytesIO from PIL import Image import av  def export_to_video_buffer(images:
    list[Image.Image]) -> BytesIO:     buffer = BytesIO()     output = av.open(buffer,
    "w", format="mp4") ![1](assets/1.png)     stream = output.add_stream("h264", 30)
    ![2](assets/2.png)     stream.width = images[0].width     stream.height = images[0].height     stream.pix_fmt
    = "yuv444p" ![3](assets/3.png)     stream.options = {"crf": "17"} ![4](assets/4.png)     for
    image in images:         frame = av.VideoFrame.from_image(image)         packet
    = stream.encode(frame)   ![5](assets/5.png)         output.mux(packet) ![6](assets/6.png)     packet
    = stream.encode(None)     output.mux(packet)     return buffer ![7](assets/7.png)
    [PRE27] $ pip install python-multipart [PRE28]`Una volta installato, puoi configurare
    il nuovo endpoint utilizzando l''[Esempio 3-12](#video_endpoint).    ##### Esempio
    3-12\. Servire i video generati dal modello immagine-video    [PRE29]    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO12-1)      Utilizza
    l''oggetto `File` per specificare `image` come caricamento del file del modulo.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO12-2)      Crea
    un oggetto Pillow `Image` passando i byte dell''immagine trasferiti al servizio.
    La pipeline del modello si aspetta un formato di immagine Pillow come input.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO12-3)      Esporta
    i fotogrammi generati come video MP4 e trasmettili al client utilizzando un buffer
    video iterabile.      Con l''endpoint video configurato, ora puoi caricare le
    immagini sul tuo servizio FastAPI per animarle come video.    Ci sono altri modelli
    video disponibili sull''hub che ti permettono di generare GIF e animazioni. Per
    fare ulteriore pratica, puoi provare a creare un servizio GenAI con questi modelli.Mentre
    i modelli video open source possono produrre video di ampia qualità, l''annuncio
    di OpenAI di un nuovo modello di visione di grandi dimensioni (LVM) chiamato Sora
    ha scosso il settore della generazione di video.    ### OpenAI Sora    I modelli
    text-to-video sono limitati nelle loro capacità di generazione: a parte l''immensa
    potenza di calcolo necessaria per generare sequenzialmente fotogrammi video coerenti,
    l''addestramento di questi modelli può essere impegnativo a causa di:    *   *Mantenere
    la coerenza temporale e spaziale tra i fotogrammi* per ottenere risultati video
    realistici e non distorti.           *   *Mancanza di dati di formazione* con
    didascalie e metadati di alta qualità, necessari per addestrare i modelli video.           *   Le*sfide
    legate alle didascalie* sono molteplici: la didascalia del contenuto dei video,
    chiara e descrittiva, richiede molto tempo e va oltre la stesura di brevi brani
    di testo. Le didascalie devono descrivere la narrazione e le scene di ogni sequenza
    affinché il modello possa apprendere e mappare i ricchi schemi contenuti nel video
    nel testo.              Per questi motivi, non c''è stata una svolta nei modelli
    di generazione video fino all''annuncio del modello Sora di OpenAI.    Sora è
    un modello di trasformatore generalista per la diffusione della visione di grandi
    dimensioni in grado di generare video e immagini con durate, rapporti d''aspetto
    e risoluzioni diverse, fino a un minuto intero di video ad alta definizione. La
    sua architettura si basa sui trasformatori comunemente utilizzati negli LLMs e
    sul processo di diffusione. Mentre gli LLMs utilizzano token di testo, Sora utilizza
    patch visive.    ###### Suggerimento    Il modello Sora combina elementi e principi
    delle architetture transformer e SD, mentre nell''[Esempio 3-10](#video_model_loading)
    hai utilizzato il modello SD di Stability AI per generare i video.    Quindi cosa
    rende Sora diverso?    I trasformatori hanno dimostrato una notevole scalabilità
    nei modelli linguistici, nella visione computerizzata e nella generazione di immagini,
    quindi era logico che l''architettura di Sora si basasse sui trasformatori per
    gestire input diversi come testo, immagini o fotogrammi video. Inoltre, poiché
    i trasformatori sono in grado di comprendere schemi complessi e dipendenze a lungo
    raggio nei dati sequenziali, Sora, in quanto trasformatore di visione, può anche
    catturare relazioni temporali e spaziali a grana fine tra i fotogrammi video per
    generare fotogrammi coerenti con transizioni fluide tra di essi (cioè, che mostrano
    coerenza temporale).    Inoltre, Sora prende in prestito le capacità dei modelli
    SD per generare fotogrammi video di alta qualità e visivamente coerenti, con controlli
    precisi grazie al processo iterativo di riduzione del rumore. L''utilizzo del
    processo di diffusione consente a Sora di generare immagini con dettagli fini
    e proprietà desiderabili.    Combinando il ragionamento sequenziale dei trasformatori
    con il perfezionamento iterativo di SD, Sora è in grado di generare video ad alta
    risoluzione, coerenti e fluidi da input multimodali come testo e immagini che
    contengono concetti astratti.    L''architettura di rete di Sora è stata progettata
    anche per ridurre la dimensionalità attraverso una retea forma di U in cui i dati
    visivi altamente dimensionali vengono compressi e codificati in uno spazio latente
    rumoroso. Sora può quindi generare patch dallo spazio latente attraverso il processo
    di diffusione del denoising.    Il processo di diffusione è simile a quello dei
    modelli SD basati sulle immagini. Invece di avere unarete U 2D normalmente utilizzata
    per le immagini, OpenAI ha addestrato una rete U 3D in cui la terza dimensione
    è una sequenza di fotogrammi nel tempo (un video), come mostra la [Figura 3-20](#images_to_videos).  ![bgai
    0320](assets/bgai_0320.png)  ###### Figura 3-20\. Una sequenza di immagini forma
    un video    OpenAI ha dimostrato che comprimendo i video in patch, come mostrato
    nella [Figura 3-21](#videos_to_patches), il modello può raggiungere la scalabilità
    dell''apprendimento di rappresentazioni ad alta dimensionalità durante l''addestramento
    su diversi tipi di video e immagini che variano per risoluzione, durata e rapporto
    di aspetto.  ![bgai 0321](assets/bgai_0321.png)  ###### Figura 3-21\. Compressione
    video in patch spazio-temporali    Tramite il processo di diffusione, Sora sminuzza
    le patch rumorose in ingresso per generare video e immagini pulite in qualsiasi
    rapporto di aspetto, dimensione e risoluzione per i dispositivi direttamente nelle
    loro dimensioni native.    Mentre un trasformatore di testo predice il token successivo
    in una sequenza di testo, il trasformatore di visione di Sora predice la patch
    successiva per generare un''immagine o un video, come mostrato nella [Figura 3-22](#vision_transformer_sequence).  ![bgai
    0322](assets/bgai_0322.png)  ###### Figura 3-22\. Previsione dei token da parte
    del trasformatore di visione    Grazie all''addestramento su vari set di dati,
    OpenAI ha superato le sfide precedentemente menzionate per l''addestramento dei
    modelli di visione, come la mancanza di didascalie di qualità, l''elevata dimensionalità
    dei dati video e così via, per citarne alcune.    Ciò che affascina di Sora e
    potenzialmente di altri LVM sono le capacità emergenti che mostrano:    Coerenza
    3D      Gli oggetti nelle scene generate rimangono coerenti e si adattano alla
    prospettiva anche quando la telecamera si muove e ruota intorno alla scena.      Permanenza
    dell''oggetto e coerenza ad ampio raggio      Gli oggetti e le persone che vengono
    occlusi o che escono da un''inquadratura in una determinata posizione persisteranno
    quando riappariranno nel campo visivo. In alcuni casi, il modello ricorda effettivamente
    come mantenerli coerenti nell''ambiente. Si tratta anche della *coerenza temporale*,
    che la maggior parte dei modelli video non riesce a gestire.      Interazione
    mondiale      Le azioni simulate nei video generati influenzano realisticamente
    l''ambiente. Ad esempio, Sora capisce che l''azione di mangiare un hamburger dovrebbe
    lasciare il segno di un morso su di esso.      Simulazione di ambienti      Sora
    può anche simulare mondi reali o fittizi, come nei giochi, rispettando le regole
    delle interazioni in quegli ambienti, come nel caso di un personaggio in un livello
    di *Minecraft*. In altre parole, Sora ha imparato a essere un motore fisico guidato
    dai dati.      La[Figura 3-23](#sora_emerging_capabilities) illustra queste funzionalità.  ![bgai
    0323](assets/bgai_0323.png)  ###### Figura 3-23\. Capacità emergenti di Sora    Al
    momento della stesura di questo articolo, Sora non è ancora stato rilasciato come
    API, ma sono già nate delle alternative open source. Un promettente modello di
    visione di grandi dimensioni chiamato "Latte" ti permette di mettere a punto l''LVM
    sui tuoi dati visivi.    ###### Attenzione    Al momento in cui scriviamo non
    è ancora possibile commercializzare alcuni modelli open source, tra cui Latte.
    Controlla sempre la scheda del modello e la licenza per assicurarti che l''uso
    commerciale sia consentito.    La combinazione di trasformatori e diffusori per
    creare LVM è un''area di ricerca promettente per la generazione di output complessi
    come i video. Tuttavia, immagino che lo stesso processo possa essere applicato
    per generare altri tipi di dati ad alta dimensionalità che possono essere rappresentati
    come array multidimensionali .    Ora dovresti sentirti più a tuo agio nella creazione
    di servizi con modelli di testo, audio, visione e video. Poi, diamo un''occhiata
    a un''altra serie di modelli in grado di generare dati complessi come le geometrie
    3D costruendo un servizio di generatore di asset 3D.[PRE30]``  [PRE31] [PRE32]`py
    [PRE33]py`` [PRE34]py[PRE35][PRE36][PRE37] [PRE38]py` # Strategie per servire
    i modelli di intelligenza artificiale generativa    Ora dovresti sentirti più
    sicuro nel creare i tuoi endpoint che servono una varietà di modelli dal repository
    di modelli di Hugging Face. Abbiamo toccato alcuni modelli diversi, tra cui quelli
    che generano testo, immagini, video, audio e forme 3D.    I modelli utilizzati
    erano piccoli, quindi potevano essere caricati e utilizzati su una CPU con risultati
    ragionevoli.Tuttavia, in uno scenario di produzione, potresti voler utilizzare
    modelli più grandi per produrre risultati di qualità superiore che potrebbero
    essere eseguiti solo su GPU e richiedere una quantità significativa di memoria
    video ad accesso casuale (VRAM).    Oltre a sfruttare le GPU, dovrai scegliere
    una strategia di model-serving tra diverse opzioni:    Sii agnostico rispetto
    al modello      Carica i modelli e genera gli output a ogni richiesta (utile per
    lo scambio di modelli).      Essere efficiente dal punto di vista del calcolo      Utilizza
    la FastAPI lifespan per precaricare i modelli che possono essere riutilizzati
    per ogni richiesta.      Sii snello      Servire i modelli all''esterno senza
    framework o lavorare con API di modelli di terze parti e interagire con esse tramite
    FastAPI.      Vediamo nel dettaglio ogni strategia.    ## Sii agnostico: scambia
    i modelli ad ogni richiesta    Negli esempi di codice precedenti, hai definito
    le funzioni di caricamento e generazione del modello e poi le hai utilizzate nei
    controller dei route handler. Utilizzando questa strategia di servizio, FastAPI
    carica un modello nella RAM (o nella VRAM se si utilizza una GPU) ed esegue un
    processo di generazione. Una volta che FastAPI restituisce i risultati, il modello
    viene scaricato dalla RAM. Il processo si ripete per la richiesta successiva.    Quando
    il modello viene scaricato dopo l''uso, la memoria viene liberata per essere utilizzata
    da un altro processo o modello. Con questo approccio, puoi scambiare dinamicamente
    vari modelli in un''unica richiesta se il tempo di elaborazione non è un problema.
    Ciò significa che altre richieste concorrenti devono aspettare prima che il server
    risponda.    Quando serve le richieste, FastAPI mette in coda le richieste in
    arrivo e le elabora in un ordine "first in first out" (FIFO). Questo comportamento
    comporta lunghi tempi di attesa perché un modello deve essere caricato e scaricato
    ogni volta. Nella maggior parte dei casi, questa strategia non è consigliata,
    ma se hai bisogno di scambiare tra più modelli di grandi dimensioni e non hai
    sufficiente RAM, allora puoi adottarla per la prototipazione. Tuttavia, negli
    scenari di produzione, non dovresti mai usare questa strategia per ovvie ragioni:
    i tuoi utenti vorranno evitare i lunghi tempi di attesa.    [La Figura 3-28](#model_loading_on_request)
    mostra questo modello di strategia di servizio.  ![bgai 0329](assets/bgai_0329.png)  ######
    Figura 3-28\. Caricamento e utilizzo dei modelli ad ogni richiesta    Se hai bisogno
    di usare modelli diversi per ogni richiesta e hai una memoria limitata, questo
    metodo può funzionare bene per fare delle prove veloci su un computer meno potente
    con pochi utenti. Il compromesso è un tempo di elaborazione significativamente
    più lento a causa delloscambio deimodelli.Tuttavia, negli scenari di produzione,
    è meglio avere una RAM più grande e usare la strategia di precaricamento dei modelli
    con la durata dell''applicazione FastAPI.    ## Efficienza di calcolo: Precaricare
    i modelli con la durata di vita di FastAPI    La strategia più efficiente dal
    punto di vista dei calcoli per caricare i modelli in FastAPI è quella di utilizzare
    la durata di vita dell''applicazione. Con questo approccio, carichi i modelli
    all''avvio dell''applicazione e li scarichi allo spegnimento. Durante lo spegnimento,
    puoi anche eseguire tutte le operazioni di pulizia necessarie, come la pulizia
    del filesystem o la registrazione.    Il vantaggio principale di questa strategia
    rispetto alla prima è che eviti di ricaricare i modelli pesanti a ogni richiesta.
    Puoi caricare un modello pesante una volta sola e poi eseguire le generazioni
    a ogni richiesta che arriva utilizzando un modello precaricato. Di conseguenza,
    risparmierai diversi minuti di tempo di elaborazione in cambio di una parte significativa
    della tua RAM (o VRAM se usi la GPU). Tuttavia, l''esperienza dell''utente dell''applicazione
    migliorerà notevolmente grazie ai tempi di risposta più brevi.    La[Figura 3-29](#model_loading_lifespan)
    mostra la strategia di model-serving che utilizza l''application lifespan.  ![bgai
    0330](assets/bgai_0330.png)  ###### Figura 3-29\. Utilizzo dell''applicazione
    FastAPI per precaricare i modelli    Puoi implementare il precaricamento del modello
    utilizzando l''applicazione lifespan, come mostrato nell''[Esempio 3-16](#model_preloading_lifespan).    #####
    Esempio 3-16\. Precaricamento del modello con l''applicazione lifespan    [PRE39]py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO16-1)      Inizializza
    un dizionario mutabile vuoto nell''ambito dell''applicazione *globale* per contenere
    uno o più modelli.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO16-2)      Utilizza
    il decoratore `asynccontextmanager` per gestire gli eventi di avvio e chiusura
    come parte di un gestore di contesto asincrono:    *   Il gestore del contesto
    eseguirà il codice prima e dopo la parola chiave `yield`.           *   La parola
    chiave `yield` nella funzione decorata `lifespan` separa le fasi di avvio e di
    arresto.           *   Il codice precedente alla parola chiave `yield` viene eseguito
    all''avvio dell''applicazione prima che vengano gestite le richieste.           *   Quando
    vuoi terminare l''applicazione, FastAPI eseguirà il codice dopo la parola chiave
    `yield` come parte della fase di arresto.                [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO16-3)      Precarica
    il modello all''avvio sul dizionario `models`.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO16-4)      Inizia
    a gestire le richieste perché la fase di avvio è terminata.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO16-5)      Cancella
    il modello all''arresto dell''applicazione.      [![6](assets/6.png)](#co_ai_integration_and_model_serving_CO16-6)      Crea
    il server FastAPI e passagli la funzione lifespan da utilizzare.      [![7](assets/7.png)](#co_ai_integration_and_model_serving_CO16-7)      Passa
    l''istanza del modello globale precaricato alla funzione di generazione.      Se
    avvii l''applicazione ora, dovresti vedere immediatamente il caricamento delle
    pipeline del modello nella memoria. Prima di applicare queste modifiche, le pipeline
    del modello venivano caricate solo quando si effettuava la prima richiesta.    ######
    Avvertenze    Puoi precaricare più di un modello in memoria usando la strategia
    lifespan model-serving, ma questo non è pratico con modelli GenAI di grandi dimensioni.
    I modelli generativi possono essere affamati di risorse e nella maggior parte
    dei casi avrai bisogno di GPU per accelerare il processo di generazione. Le GPU
    consumer più potenti vengono fornite con soli 24 GB di VRAM. Alcuni modelli richiedono
    18 GB di memoria per eseguire l''inferenza, quindi cerca di distribuire i modelli
    su istanze dell''applicazione e GPU separate.    ## Essere snelli: servire i modelli
    all''esterno    Un''altra strategia per servire i modelli GenAI è quella di pacchettizzarli
    come servizi esterni tramite altri strumenti. Puoi quindi utilizzare la tua applicazione
    FastAPI come livello logico tra il tuo client e il server esterno del modello.
    In questo livello logico puoi gestire il coordinamento tra i modelli, la comunicazione
    con le API, la gestione degli utenti, le misure di sicurezza, le attività di monitoraggio,
    il filtraggio dei contenuti, il miglioramento dei prompt o qualsiasi altra logica
    necessaria.    ### Fornitori di Cloud    I fornitori di Cloud stanno innovando
    costantemente le soluzioni serverless e di calcolo dedicate che puoi utilizzare
    per servire i tuoi modelli all''esterno. Ad esempio, Azure Machine Learning Studio
    fornisce ora uno strumento PromptFlow che puoi utilizzare per distribuire e personalizzare
    modelli OpenAI o di linguaggi open source. Al momento della distribuzione, riceverai
    un endpoint del modello eseguito sul tuo calcolo Azure pronto per essere utilizzato.
    Tuttavia, l''utilizzo di PromptFlow o di strumenti simili richiede una curva di
    apprendimento ripida, in quanto possono richiedere dipendenze particolari e passaggi
    non tradizionali da seguire.    ### BentoML    Un altro grande concorrente per
    il servizio di modelli esterni a FastAPI è BentoML, chesi ispira a FastAPI ma
    implementa una strategia di servizio diversa, costruita appositamente per i modelli
    AI.    Un enorme miglioramento rispetto a FastAPI per la gestione delle richieste
    di modelli concorrenti è la capacitàdi BentoML di eseguire richieste diverse su
    processi worker diversi. Può parallelizzare le richieste delimitate dalla CPU
    senza dover gestire direttamente il multiprocessing di Python. Inoltre, BentoML
    può anche eseguire inferenze di modelli in batch, in modo che il processo di generazione
    per più utenti possa essere eseguito con un''unica chiamata al modello.    Ho
    trattato BentoML in dettaglio nel [Capitolo 2](ch02.html#ch02).    ###### Suggerimento    Per
    eseguire BentoML, dovrai prima installare alcune dipendenze:    [PRE40]py   [PRE41]
    [PRE42] # main.py  from fastapi import FastAPI from openai import OpenAI  app
    = FastAPI() openai_client = OpenAI() system_prompt = "You are a helpful assistant."  @app.get("/generate/openai/text")
    def serve_openai_language_model_controller(prompt: str) -> str | None:     response
    = openai_client.chat.completions.create( ![1](assets/1.png)         model="gpt-4o",         messages=[             {"role":
    "system", "content": f"{system_prompt}"},             {"role": "user", "content":
    prompt},         ],     )     return response.choices[0].message.content [PRE43]`
    [PRE44][PRE45]`` [PRE46] # main.py  import csv import time from datetime import
    datetime, timezone from uuid import uuid4 from typing import Awaitable, Callable
    from fastapi import FastAPI, Request, Response  # preload model with a lifespan
    ...  app = FastAPI(lifespan=lifespan)  csv_header = [     "Request ID", "Datetime",
    "Endpoint Triggered", "Client IP Address",     "Response Time", "Status Code",
    "Successful" ]   @app.middleware("http") ![1](assets/1.png) async def monitor_service(     req:
    Request, call_next: Callable[[Request], Awaitable[Response]] ) -> Response: ![2](assets/2.png)     request_id
    = uuid4().hex ![3](assets/3.png)     request_datetime = datetime.now(timezone.utc).isoformat()     start_time
    = time.perf_counter()     response: Response = await call_next(req)     response_time
    = round(time.perf_counter() - start_time, 4) ![4](assets/4.png)     response.headers["X-Response-Time"]
    = str(response_time)     response.headers["X-API-Request-ID"] = request_id ![5](assets/5.png)     with
    open("usage.csv", "a", newline="") as file:         writer = csv.writer(file)         if
    file.tell() == 0:             writer.writerow(csv_header)         writer.writerow(
    ![6](assets/6.png)             [                 request_id,                 request_datetime,                 req.url,                 req.client.host,                 response_time,                 response.status_code,                 response.status_code
    < 400,             ]         )     return response   # Usage Log Example  """"
    Request ID: 3d15d3d9b7124cc9be7eb690fc4c9bd5 Datetime: 2024-03-07T16:41:58.895091
    Endpoint triggered: http://localhost:8000/generate/text Client IP Address: 127.0.0.1
    Processing time: 26.7210 seconds Status Code: 200 Successful: True """  # model-serving
    handlers ... [PRE47]` [PRE48][PRE49][PRE50][PRE51]``````'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0] # models.py  import torch from transformers import Pipeline, pipeline  prompt
    = "How to set up a FastAPI project?" system_prompt = """ Your name is FastAPI
    bot and you are a helpful chatbot responsible for teaching FastAPI to your users.
    Always respond in markdown. """  device = torch.device("cuda" if torch.cuda.is_available()
    else "cpu") ![1](assets/1.png)  def load_text_model():     pipe = pipeline(         "text-generation",         model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    ![2](assets/2.png)         torch_dtype=torch.bfloat16,         device=device ![3](assets/3.png)     )     return
    pipe   def generate_text(pipe: Pipeline, prompt: str, temperature: float = 0.7)
    -> str:     messages = [         {"role": "system", "content": system_prompt},         {"role":
    "user", "content": prompt},     ] ![4](assets/4.png)     prompt = pipe.tokenizer.apply_chat_template(         messages,
    tokenize=False, add_generation_prompt=True     ) ![5](assets/5.png)     predictions
    = pipe(         prompt,         temperature=temperature,         max_new_tokens=256,         do_sample=True,         top_k=50,         top_p=0.95,     )
    ![6](assets/6.png)     output = predictions[0]["generated_text"].split("</s>\n<|assistant|>\n")[-1]
    ![7](assets/7.png)     return output [PRE1] # main.py  from fastapi import FastAPI
    from models import load_text_model, generate_text  app = FastAPI()  @app.get("/generate/text")
    ![1](assets/1.png) def serve_language_model_controller(prompt: str) -> str: ![2](assets/2.png)     pipe
    = load_text_model() ![3](assets/3.png)     output = generate_text(pipe, prompt)
    ![4](assets/4.png)     return output ![5](assets/5.png) [PRE2] http://localhost:8000/generate/text?prompt="What
    is FastAPI?" [PRE3]`  [PRE4] $ pip install streamlit [PRE5]` L[''esempio 3-3](#streamlit_chat_ui)
    mostra come sviluppare una semplice interfaccia utente per connettersi al servizio.    #####
    Esempio 3-3\. L''interfaccia utente della chat di Streamlit che utilizza l''endpoint
    FastAPI /`generate`    [PRE6]    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO3-1)      Aggiungi
    un titolo alla tua applicazione che sarà reso all''interfaccia utente.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO3-2)      Inizializza
    la chat e tiene traccia della cronologia della chat.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO3-3)      Visualizza
    i messaggi della cronologia delle chat al riavvio dell''applicazione.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO3-4)      Attendi
    che l''utente invii un prompt tramite il campo di inserimento della chat.      [![5](assets/4.png)](#co_ai_integration_and_model_serving_CO3-5)      Aggiungi
    i messaggi dell''utente o dell''assistente alla cronologia delle chat.      [![6](assets/4.png)](#co_ai_integration_and_model_serving_CO3-6)      Visualizza
    il messaggio dell''utente nel contenitore dei messaggi della chat.      [![7](assets/4.png)](#co_ai_integration_and_model_serving_CO3-7)      Invia
    una richiesta `GET` con il prompt come parametro di query al tuo endpoint FastAPI
    per generare una risposta da TinyLlama.      [![8](assets/4.png)](#co_ai_integration_and_model_serving_CO3-8)      Convalida
    che la risposta sia OK.      [![9](assets/4.png)](#co_ai_integration_and_model_serving_CO3-9)      Visualizza
    il messaggio dell''assistente nel contenitore dei messaggi della chat.      Ora
    puoi avviare l''applicazione client Streamlit:^([11](ch03.html#id674))    [PRE7]   `Ora
    dovresti essere in grado di interagire con TinyLlama all''interno di Streamlit,
    come mostrato nella [Figura 3-12](#streamlit_ui_text_results'
