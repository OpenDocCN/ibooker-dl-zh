- en: Chapter 5\. Model Domain Adaptation for LLM-Based Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章：基于LLM应用的模型领域自适应
- en: In the previous chapter, we discussed different architectures for model deployment.
    In this chapter, we will talk about how to do domain adaptation for your models.
    Practitioners frequently refer to *domain adaptation* as “fine-tuning,” but fine-tuning
    is actually just one of many ways to make a model work well in your domain.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了模型部署的不同架构。在这一章中，我们将讨论如何对模型进行领域自适应。实践者通常将“领域自适应”称为“微调”，但实际上，微调只是使模型在您的领域内良好工作的一种方法。
- en: In this chapter, we will look at several model adaptation methods, including
    prompt engineering, fine-tuning, and retrieval augmented generation (RAG).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将探讨几种模型自适应方法，包括提示工程、微调和检索增强生成（RAG）。
- en: We will also look at how to optimize LLMs to run them in resource-constrained
    environments that require model compression. Finally, we will discuss best practices
    and scaling laws to show you how to determine how much data your LLMs need to
    run effectively.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨如何优化LLM以在资源受限的环境中运行，这些环境需要模型压缩。最后，我们将讨论最佳实践和扩展定律，以向您展示如何确定您的LLM需要多少数据才能有效运行。
- en: Training LLMs from Scratch
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始训练LLM
- en: Training LLMs from scratch can be simple or resource intensive, depending on
    your application. For most applications, it makes sense to use an existing open
    source LLM or proprietary LLM. On the other hand, there’s no better way to learn
    how an LLM works than to train one from scratch.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始训练LLM可以是简单的或资源密集型的，这取决于您的应用。对于大多数应用来说，使用现有的开源LLM或专有LLM是有意义的。另一方面，没有比从头开始训练一个LLM更好的方式来学习LLM是如何工作的。
- en: Training an LLM from scratch is a complex, resource-intensive task requiring
    a comprehensive pipeline that necessitates data preparation, model architecture
    selection, training configuration, and monitoring. Let’s walk through a structured
    approach to training an LLM from scratch.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始训练一个大型语言模型（LLM）是一个复杂且资源密集型的任务，需要一套全面的流程，包括数据准备、模型架构选择、训练配置和监控。让我们通过一种结构化的方法来了解如何从零开始训练一个LLM。
- en: 'Step 1: Pick a Task'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步：选择任务
- en: Determine why you’re building this model, the domain it will serve, and the
    tasks it will perform (such as text generation, summarization, or code generation).
    Decide on success criteria, such as perplexity, accuracy, or other domain-specific
    evaluation metrics.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 确定您为什么要构建这个模型，它将服务的领域以及它将执行的任务（如文本生成、摘要或代码生成）。确定成功标准，如困惑度、准确度或其他特定领域的评估指标。
- en: 'Step 2: Prepare the Data'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：准备数据
- en: 'Before you feed data into a model, the model preprocessing step makes sure
    that the input is in a form the model can handle effectively. This involves tokenizing
    text, removing noise, normalizing formats, and sometimes simplifying complex structures
    into components the model can understand more easily. Preprocessing can also include
    feature selection, which is about picking the most relevant data so the model’s
    “focus” is on what really matters. This step includes:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据输入模型之前，模型预处理步骤确保输入以模型可以有效地处理的形式存在。这包括对文本进行标记化、去除噪声、格式化归一化和有时将复杂结构简化为模型更容易理解的组件。预处理还可以包括特征选择，这是关于选择最相关的数据，以便模型的“焦点”真正关注重要的事情。这一步骤包括：
- en: Collecting large-scale text data
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 收集大规模文本数据
- en: High-quality sources include books, articles, websites, research papers, code
    repositories, and domain-specific texts if the model is specialized (such as for
    use in the legal or medical fields).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型是专业化的（例如用于法律或医疗领域），高质量的数据源包括书籍、文章、网站、研究论文、代码仓库和特定领域的文本。
- en: Cleaning the data
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理
- en: This includes removing non-useful elements (like advertisements or formatting
    artifacts) and handling misspellings. Use libraries like Hugging Face for this
    task.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括移除非有用元素（如广告或格式化碎片）和处理拼写错误。可以使用Hugging Face库来完成这项任务。
- en: Tokenizing the data
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标记化
- en: You can do this using subword tokenization methods like byte-pair encoding (BPE)
    or SentencePiece, as is done in models like BERT and GPT-3\. You can also use
    Hugging Face’s `AutoTokenizer` for this task. Tokenization is essential for handling
    large vocabularies and avoiding the need for excessive parameters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用类似于字节对编码（BPE）或SentencePiece的子词标记化方法来完成这项任务，就像BERT和GPT-3等模型中所做的那样。您还可以使用Hugging
    Face的`AutoTokenizer`来完成这项任务。标记化对于处理大型词汇表和避免需要过多参数至关重要。
- en: 'Step 3: Decide on the Model Architecture'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步：确定模型架构
- en: Choose a model size appropriate for your data, resources, and goals. Model configurations
    range from smaller models (hundreds of millions of parameters) to full-scale LLMs
    (billions or even trillions of parameters). As discussed in [Chapter 1](ch01.html#ch01_introduction_to_large_language_models_1748895465615150),
    adapt the base architecture to fit your specific needs, whether that’s changing
    the number of layers, changing the attention mechanism, or adding specialized
    components (such as retrieval-augmented mechanisms if focusing on knowledge-intensive
    tasks). Three general types of architecture are shown in [Figure 5-1](#ch05_figure_1_1748896666799655).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适合您的数据、资源和目标模型大小的模型。模型配置范围从小型模型（数亿参数）到全规模LLM（数十亿甚至数万亿参数）。如[第1章](ch01.html#ch01_introduction_to_large_language_models_1748895465615150)中所述，根据您的具体需求调整基本架构，无论是更改层数、更改注意力机制还是添加专用组件（如专注于知识密集型任务的检索增强机制）。[图5-1](#ch05_figure_1_1748896666799655)展示了三种一般类型的架构。
- en: '![](assets/llmo_0501.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_0501.png)'
- en: 'Figure 5-1\. Three types of LLM architectures (source: [Abhinav Kimothi](https://oreil.ly/bR9E1))'
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 三种类型的LLM架构（来源：[Abhinav Kimothi](https://oreil.ly/bR9E1))
- en: 'Step 4: Set Up Your Training Infrastructure'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步：设置您的训练基础设施
- en: Training a large model typically requires distributed training across multiple
    GPUs or TPUs, ideally with high memory (16 GB+) and fast interconnects (like NVLink).
    Frameworks like PyTorch’s Distributed Data Parallel (DDP) or TensorFlow’s `MultiWorkerMirroredStrategy`
    can come in handy. Those coming from an MLOps background may already know of libraries
    like DeepSpeed and Megatron-LM that are designed to optimize memory and computation
    for large-scale model training.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型模型通常需要在多个GPU或TPU上分布式训练，理想情况下具有高内存（16 GB+）和快速互连（如NVLink）。PyTorch的分布式数据并行（DDP）或TensorFlow的`MultiWorkerMirroredStrategy`等框架可能会有所帮助。来自MLOps背景的人可能已经熟悉DeepSpeed和Megatron-LM等库，这些库旨在优化大规模模型训练的内存和计算。
- en: Although there are plenty of optimizers for training ML models, including stochastic
    gradient descent (SGD) and Autograd, we suggest selecting an optimizer suitable
    for large models, such as Adam or AdamW, and use mixed-precision training (for
    instance, FP16) to reduce memory usage and accelerate training.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有很多用于训练机器学习模型的优化器，包括随机梯度下降（SGD）和Autograd，但我们建议选择适合大型模型的优化器，例如Adam或AdamW，并使用混合精度训练（例如，FP16）以减少内存使用并加速训练。
- en: 'Step 5: Implement Training'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步：实施训练
- en: Train the model on the task. While implementing the training, there are a few
    things to think of. What will your hyperparameters be? What will be your seed
    value? You can view one of the simplest implementations for training LLMs from
    scratch in this [one-hour video by Andrej Karpathy](https://oreil.ly/PfnyZ) (see
    [Example 5-1](#ch05_example_1_1748896666808696)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在任务上训练模型。在实施训练时，有一些事情需要考虑。你的超参数将是什么？你的种子值是什么？您可以在安德烈·卡帕西（Andrej Karpathy）的[这个一小时视频](https://oreil.ly/PfnyZ)中查看从头开始训练LLM的最简单实现（见[示例5-1](#ch05_example_1_1748896666808696)）。
- en: Example 5-1\. Implementation for training an LLM from scratch by Andrej Karpathy
    (used with permission)
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-1\. 安德烈·卡帕西（Andrej Karpathy）从头开始训练LLM的实现（经许可使用）
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that you know how a simple LLM works, let’s look into how to combine different
    model architectures.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了简单LLM的工作原理，让我们来看看如何结合不同的模型架构。
- en: Model Ensembling Approaches
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型集成方法
- en: '*Ensembling* refers to combining multiple models to get better results than
    any single model can provide on its own. Thus, each model contributes something
    unique, balancing one another’s weaknesses and complementing their strengths.
    Ensembling LLMs is a powerful approach for boosting performance, enhancing robustness,
    and increasing the interpretability of language models. While ensembling has traditionally
    been more common in smaller ML models, such as random forests or smaller-scale
    NLP models, it’s becoming increasingly relevant for LLMs due to their specialized
    behavior and varied responses.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*集成*指的是将多个模型结合起来，以获得比单个模型单独提供的结果更好的效果。因此，每个模型都贡献了一些独特的东西，平衡彼此的弱点，并补充彼此的优势。集成大型语言模型（LLM）是提升性能、增强鲁棒性和提高语言模型可解释性的强大方法。虽然集成在较小的机器学习模型中（如随机森林或较小规模的NLP模型）更为常见，但由于LLM具有特定的行为和多样化的响应，它正变得越来越相关。'
- en: There are, of course, trade-offs with ensembling LLMs. One of the main challenges
    is computational cost—running multiple large models in parallel can be resource
    intensive. Memory usage and inference time increase significantly, which can sometimes
    be prohibitive for real-time, low-latency applications.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，集成LLM存在权衡。其中一个主要挑战是计算成本——并行运行多个大型模型可能会消耗大量资源。内存使用量和推理时间显著增加，有时可能会对实时、低延迟应用产生阻碍。
- en: Another challenge is complexity in deployment. Deploying an ensemble of LLMs
    requires orchestrating several models, managing dependencies, and possibly integrating
    ensemble-specific logic. Model ensembling, however, can often be optimized by
    using quantized versions of models, caching predictions, or limiting the ensemble
    to run only when certain criteria are met (for example, if a model’s confidence
    is low). These techniques will be discussed in [Chapter 9](ch09.html#ch09_scaling_hardware_infrastructure_and_resource_ma_1748896826216961).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是部署的复杂性。部署一组LLM（大型语言模型）需要协调多个模型，管理依赖关系，并可能集成特定于集成的逻辑。然而，通过使用模型的量化版本、缓存预测或限制集成仅在满足某些条件时运行（例如，如果模型的置信度低），模型集成通常可以优化。这些技术将在[第9章](ch09.html#ch09_scaling_hardware_infrastructure_and_resource_ma_1748896826216961)中讨论。
- en: However, for most people and companies, model domain adaptation is one of the
    most common and cost-effective ways to improve LLM accuracy. Let’s look into a
    few ways to ensemble LLMs effectively, along with some code examples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于大多数人和企业来说，模型领域自适应是提高LLM准确率最常见且成本效益最高的方法之一。让我们探讨一些有效集成LLM的方法，以及一些代码示例。
- en: Model Averaging and Blending
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型平均和混合
- en: One straightforward method is to average the predictions of multiple models.
    This is useful when working with models that have different strengths, such as
    one model that excels at generating fact-based text and another that is more creative.
    When we average their responses or blend their outputs, we get a more balanced
    response. This can be as simple as computing the probability distribution across
    models and averaging them. In practice, it can also look like generating the softmax
    probability distributions for each model and averaging them for final predictions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直接的方法是平均多个模型的预测。当与具有不同优势的模型一起工作时很有用，例如一个擅长生成基于事实的文本的模型，另一个则更具创造力。当我们平均它们的响应或混合它们的输出时，我们得到一个更平衡的响应。这可以简单到计算模型之间的概率分布并平均它们。在实践中，它也可以表现为为每个模型生成softmax概率分布，并平均它们以进行最终预测。
- en: The code in [Example 5-2](#ch05_example_2_1748896666808730) simply iterates
    over each model, sums up their outputs, and divides by the number of models to
    get the average prediction.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例5-2](#ch05_example_2_1748896666808730)中的代码简单地遍历每个模型，汇总它们的输出，然后除以模型的数量以获得平均预测。'
- en: Example 5-2\. Averaging the predictions of multiple models
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-2. 多个模型的预测平均
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `models` is a list of model instances, and `input_text` is the text prompt.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`models`是一个模型实例的列表，而`input_text`是文本提示。
- en: Weighted Ensembling
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加权集成
- en: Sometimes it’s beneficial to give more weight to certain models based on their
    accuracy or performance on specific tasks. For instance, if Model A is known to
    perform better on summarization tasks, it can be given a higher weight than Model
    B in the ensemble. *Weighted ensembling* allows us to incorporate domain expertise
    or empirical model evaluations directly into the ensemble. In [Example 5-3](#ch05_example_3_1748896666808750),
    `weights` is a list with the same length as `models`, containing the weight for
    each respective model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有时根据某些模型的准确性或特定任务上的表现给予更多权重是有益的。例如，如果模型A在摘要任务上表现更好，那么在集成中它可以比模型B获得更高的权重。*加权集成*允许我们将领域专业知识或经验模型评估直接纳入集成。在[示例5-3](#ch05_example_3_1748896666808750)中，`weights`是一个与`models`长度相同的列表，包含每个相应模型的权重。
- en: Example 5-3\. Weighted ensembling
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-3. 加权集成
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output is a weighted combination that can be adjusted depending on the desired
    emphasis for each model in the ensemble.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个加权组合，可以根据对集成中每个模型的期望强调程度进行调整。
- en: Stacked Ensembling (Two-Stage Model)
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠集成（两阶段模型）
- en: In a *stacked ensembling* approach, the outputs from multiple models are fed
    into a secondary model (often a smaller, simpler model) that learns to combine
    their outputs effectively. This metamodel learns patterns in the output spaces
    of the LLMs. This can be particularly useful for complex tasks like summarization
    or translation, where different models might capture different nuances of the
    input.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在堆叠集成方法中，多个模型的输出被输入到一个二级模型（通常是较小的、更简单的模型）中，该模型学习有效地组合它们的输出。这个元模型学习 LLMs 输出空间中的模式。这可以特别适用于复杂任务，如摘要或翻译，其中不同的模型可能捕捉到输入的不同细微差别。
- en: The method in [Example 5-4](#ex-5-4) uses an SKlearn model as a metamodel, which
    is trained on the outputs of the LLMs. It requires a training phase as it learns
    to make sense of each LLM’s predictions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-4](#ex-5-4) 中的方法使用一个 SKlearn 模型作为元模型，该模型在 LLMs 的输出上训练。它需要一个训练阶段，因为它学习理解每个
    LLM 的预测。'
- en: Example 5-4\. Stacked ensembling
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-4\. 堆叠集成
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, what if you don’t want to combine different models but simply different
    model architectures? Well, with LLMs you can do that too, since every model architecture
    has its own strengths.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你不想结合不同的模型，而只是不同的模型架构呢？嗯，使用大型语言模型（LLMs）你也可以做到这一点，因为每个模型架构都有其自身的优势。
- en: Diverse Ensembles for Robustness
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于鲁棒性的多样化集成
- en: Using diverse models—like a mixture of encoder–decoder architectures and transformer-based
    language models—can be very effective for handling edge cases or generating more
    comprehensive answers. This diversity brings complementary strengths to the models
    and tends to be more resistant to errors in any single model. For example, if
    one model is prone to hallucinations (a known issue in some LLMs), the other models
    can serve as a balancing force, correcting or limiting this effect.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多样化的模型——如编码器-解码器架构和基于转换器的语言模型的混合——在处理边缘情况或生成更全面的答案时可能非常有效。这种多样性为模型带来了互补的优势，并且通常对单个模型中的错误更具抵抗力。例如，如果一个模型容易产生幻觉（某些大型语言模型中已知的问题），其他模型可以作为平衡力量，纠正或限制这种效应。
- en: Diversity in ensembling also opens doors for specialized responses using models
    that focus on different aspects of language, like factuality or creativity. For
    example, ensembling a smaller factual model with a generative transformer-based
    model can yield an LLM that provides both creativity and accurate information.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 集成中的多样性也为使用专注于语言不同方面的模型提供了专门响应的机会，例如事实性或创造性。例如，将较小的基于事实的模型与基于生成转换器的模型集成可以产生一个既具有创造性又提供准确信息的
    LLM。
- en: Multi-Step Decoding and Voting Mechanisms
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多步解码和投票机制
- en: A unique way to generate text in high-latency, high-accuracy applications is
    to use voting mechanisms, where models vote on the next token or phrase. Voting
    schemes like majority, weighted, or ranked voting help ensure that common tokens
    across models have a higher chance of being selected, while outlier tokens are
    filtered out. This process can significantly improve the coherence and consistency
    of generated text, especially for complex prompts or tasks requiring precise language.
    [Example 5-5](#ch05_example_4_1748896666808768) provides code for a majority vote.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在高延迟、高精度应用中生成文本的独特方法之一是使用投票机制，其中模型对下一个标记或短语进行投票。如多数投票、加权投票或排名投票等投票方案有助于确保模型之间常见的标记有更高的被选中机会，而异常标记则被过滤掉。这个过程可以显著提高生成文本的连贯性和一致性，尤其是在需要精确语言的复杂提示或任务中。[示例
    5-5](#ch05_example_4_1748896666808768) 提供了多数投票的代码。
- en: Example 5-5\. Majority voting
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-5\. 多数投票
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, `voting_ensemble` uses a majority vote to select the most common output
    from each model. If there’s a tie, additional logic can be added to consider weighted
    voting or random selection among the tied options.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`voting_ensemble` 使用多数投票来选择每个模型中最常见的输出。如果有平局，可以添加额外的逻辑来考虑加权投票或随机选择平局选项。
- en: Composability
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可组合性
- en: In ensembling, another popular technique is composability. *Composability* is
    the ability to mix and match models or parts of models flexibly. Some ensemble
    methods might combine outputs from multiple models by averaging them, while others
    might chain models so that the output of one becomes the input for another. This
    setup allows you avoid using a massive, all-encompassing model for complex tasks
    by using smaller, specialized models instead.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成学习中，另一种流行的技术是可组合性。*可组合性*是指灵活地混合和匹配模型或模型部分的能力。一些集成方法可能通过平均多个模型的输出来组合它们，而其他方法可能将模型串联起来，使得一个模型的输出成为另一个模型的输入。这种设置允许你通过使用较小的、专门的模型来避免使用庞大的、包罗万象的模型来完成复杂任务。
- en: For example, suppose we have a summarization model, a translation model, and
    a sentiment-analysis model. Instead of retraining a single, monolithic model that
    can handle all three tasks, we can compose these individual models in a pipeline
    where each one processes the output of the previous one. This modular approach
    allows for adaptability and maintenance, as each model can be fine-tuned independently,
    reducing the overall computational cost and development time (see [Example 5-6](#ch05_example_5_1748896666808785)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个摘要模型、一个翻译模型和一个情感分析模型。我们不必重新训练一个能够处理所有三个任务的单一、庞大的模型，而是可以将这些单个模型以管道的形式组合起来，其中每个模型处理前一个模型的输出。这种模块化方法允许适应性和维护性，因为每个模型都可以独立地进行微调，从而降低整体计算成本和开发时间（参见[示例
    5-6](#ch05_example_5_1748896666808785)）。
- en: Example 5-6\. Composing a model
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-6\. 模型组合
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, `translate_model`, `summarize_model`, and `sentiment_model` can be individually
    updated or replaced, which is especially beneficial if one of the models becomes
    outdated or needs retuning.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`translate_model`、`summarize_model`和`sentiment_model`可以单独更新或替换，如果其中一个模型过时或需要重新调整，这尤其有益。
- en: There are many benefits to composability. For instance, it provides modularity,
    since different models can be plugged in as needed, enhancing flexibility. Composability
    allows for easy extension by adding or swapping individual models, making these
    models scalable. It is also efficient, since you can optimize individual components
    without affecting the rest of the pipeline.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可组合性有许多好处。例如，它提供了模块化，因为不同的模型可以根据需要插入，增强了灵活性。可组合性允许通过添加或交换单个模型轻松扩展，使这些模型可扩展。它也很高效，因为你可以优化单个组件而不影响管道的其余部分。
- en: However, composability doesn’t come without challenges. First, errors in one
    component can propagate downstream, potentially compounding inaccuracies. Secondly,
    each stage adds processing time, which may affect real-time applications. And
    finally, ensuring coherent responses across multiple models requires careful coordination
    and tuning.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可组合性并非没有挑战。首先，一个组件中的错误可能会向下传播，可能加剧不准确。其次，每个阶段都会增加处理时间，这可能会影响实时应用。最后，确保多个模型之间响应的一致性需要仔细的协调和调整。
- en: Soft Actor–Critic
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软演员-评论家
- en: This is where the *soft actor–critic* (SAC) technique comes in. SAC can be advantageous
    for LLMs where the goal is not just to maximize accuracy but to achieve a balance
    between different qualitative aspects, such as creativity and coherence. SAC is
    a reinforcement learning technique that helps the ensemble balance exploration
    with exploitation. One of the unique features of SAC is its use of “soft” reward
    maximization, introducing entropy regularization. It promotes exploratory actions,
    encouraging the model to try different responses rather than always choosing the
    most predictable one, an approach that can lead to more natural and varied responses
    in language tasks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*软演员-评论家*（SAC）技术发挥作用的地方。SAC对于LLMs来说可能是有利的，其目标不仅仅是最大化准确性，而是要在不同的定性方面（如创造性和连贯性）之间取得平衡。SAC是一种强化学习技术，有助于集成平衡探索与利用。SAC的一个独特特征是使用“软”奖励最大化，引入熵正则化。它促进探索性动作，鼓励模型尝试不同的响应，而不是总是选择最可预测的一个，这种方法可以在语言任务中导致更自然和多样化的响应。
- en: When we ensemble LLMs, SAC can fine-tune how the models interact, making them
    more adaptable to new information or tasks without overcommitting to one approach.
    It’s particularly useful for dynamic environments where responses need to adapt
    based on user feedback or other shifting factors as well as for developing generalized
    models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们集成LLMs时，SAC可以微调模型之间的交互方式，使它们能够适应新的信息或任务，而不会过度承诺于一种方法。这对于需要根据用户反馈或其他变化因素调整响应的动态环境特别有用，以及对于开发通用模型。
- en: In LLMs, you can use SAC to adjust model outputs to maximize rewards associated
    with desirable behaviors. For example, in a customer service chatbot, rewards
    might be based on user satisfaction, response brevity, and politeness. SAC allows
    the LLM to learn a policy that maximizes these rewards through trial and error,
    iteratively improving its responses based on feedback.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLMs）中，你可以使用SAC来调整模型输出，以最大化与期望行为相关的奖励。例如，在客户服务聊天机器人中，奖励可能基于用户满意度、响应简短和礼貌。SAC允许LLM通过试错学习一个策略，以最大化这些奖励，并通过反馈迭代地改进其响应。
- en: SAC operates with two core components. The *actor network* proposes actions
    (in LLMs, possible responses or actions in dialogue), while *critic networks*
    evaluate the value of the proposed actions, considering both immediate and future
    rewards.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: SAC通过两个核心组件运行。*演员网络*提出动作（在LLMs中，可能是可能的响应或对话中的动作），而*评论家网络*评估所提出动作的价值，考虑即时的和未来的奖励。
- en: Implementing SAC for LLMs involves defining a reward function tailored to the
    task, setting up the actor and critic networks, and training the policy over several
    episodes, as shown in [Example 5-7](#ch05_example_6_1748896666808799).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 实施针对大型语言模型（LLMs）的SAC涉及定义针对任务的奖励函数，设置演员网络和评论家网络，并在多个回合中训练策略，如[示例 5-7](#ch05_example_6_1748896666808799)所示。
- en: Example 5-7\. SAC implementation
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-7\. SAC实现
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: SAC comes with its own benefits. For instance, its entropy-based exploration
    ensures that responses remain varied, which is ideal for creative language tasks.
    SAC can also adapt responses over time based on live feedback, improving adaptability
    in real-world applications like chatbots. In addition, it allows for custom reward
    functions to tune behavior, making it suitable for multiobjective tasks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: SAC有其自身的优势。例如，其基于熵的探索确保了响应的多样性，这对于创意语言任务来说非常理想。SAC还可以根据实时反馈调整响应，从而提高在聊天机器人等现实世界应用中的适应性。此外，它允许自定义奖励函数来调整行为，使其适用于多目标任务。
- en: One challenge of SAC is that reward functions must be carefully crafted for
    each task, balancing multiple objectives, which is a hard task. Second, as with
    many reinforcement learning algorithms, training such models can be sensitive
    to hyperparameters, requiring significant tuning. And most importantly, SAC can
    be computationally intensive, especially for large models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: SAC的一个挑战是，必须为每个任务精心设计奖励函数，平衡多个目标，这是一个艰巨的任务。其次，与许多强化学习算法一样，训练此类模型可能对超参数敏感，需要大量调整。最重要的是，SAC可能计算密集，特别是对于大型模型。
- en: Model Domain Adaptation
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型域适应
- en: While LLMs are generally powerful, they often lack specific knowledge or contextual
    nuances for specialized domains. For example, ChatGPT may understand everyday
    medical terms, but it might not accurately interpret complex medical jargon or
    nuanced legal phrases without additional tuning. By fine-tuning an LLM on domain-specific
    data, you can enhance its ability to understand and produce domain-relevant content,
    improving both accuracy and coherence in that context.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LLMs通常功能强大，但它们往往缺乏特定领域或语境的细微差别。例如，ChatGPT可能理解日常医疗术语，但如果没有额外的调整，它可能无法准确解释复杂的医学术语或细微的法律短语。通过在特定领域数据上微调LLM，您可以增强其理解和生成领域相关内容的能力，从而提高该语境下的准确性和连贯性。
- en: '*Model adaptation* means refining a pretrained model to make it perform better
    on specific tasks or respond to unique contexts. This approach is particularly
    useful for LLMs that have been pretrained on diverse but general-purpose data.
    When applied to specialized areas like legal, medical, or scientific text, domain
    adaptation helps these models understand and generate more accurate, relevant,
    and context-sensitive responses.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型适应*意味着细化预训练模型，使其在特定任务上表现更好或对独特语境做出响应。这种方法对于在多样化但通用数据上预训练的LLMs尤其有用。当应用于法律、医学或科学文本等特定领域时，域适应有助于这些模型理解和生成更准确、相关和上下文敏感的响应。'
- en: Domain adaptation methods vary in complexity, from simple fine-tuning on domain-specific
    datasets to more advanced techniques that adapt models to the specialized vocabulary,
    terminology, and stylistic nuances of a particular field.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 域适应方法在复杂性上有所不同，从对特定领域数据集的简单微调到更高级的技术，这些技术将模型适应到特定领域的专业词汇、术语和风格细微差别。
- en: 'Overall, there are three core benefits to model domain adaptation:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，模型域适应有三个核心好处：
- en: Improvement of LLMs’ performance on tasks in underrepresented domains, such
    as medical texts or legal documents.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高LLMs在代表性不足的领域（如医学文本或法律文件）上的任务性能。
- en: Reduction of the need to collect and label large amounts of data for each new
    domain. This can be especially useful for domains where data is scarce or expensive
    to collect.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少了为每个新域收集和标注大量数据的需求。这在数据稀缺或收集成本高昂的领域尤其有用。
- en: The ability to make LLMs more accessible to a wider range of users, even if
    the user does not have expertise in that specific domain.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使LLMs更容易被更广泛的用户使用，即使用户在该特定领域没有专业知识。
- en: Let’s look at an example to clarify further. Say your target domain has unique
    vocabulary, such as chemical compounds or legal citations. You can update the
    tokenizer and embedding layers to include domain-specific tokens to improve the
    model’s performance, as shown in [Example 5-8](#ch05_example_7_1748896666808815).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来进一步阐明。假设你的目标领域有独特的词汇，例如化学化合物或法律引文。你可以更新标记化和嵌入层，包括特定领域的标记，以提高模型的表现，如[示例
    5-8](#ch05_example_7_1748896666808815)所示。
- en: Example 5-8\. Adding domain-specific tokens
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-8\. 添加特定领域的标记
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These custom tokenizers can identify unique entities (like chemical formulas
    or legal citations) as atomic tokens, ensuring that the model recognizes them
    as distinct units rather than breaking them down into subwords. Embedding these
    domain-specific tokens helps the model better grasp domain-relevant information
    and retain consistency across complex terminologies.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些自定义标记化器可以识别独特的实体（如化学公式或法律引文）作为原子标记，确保模型将它们识别为独立的单元，而不是将它们分解为子词。嵌入这些特定领域的标记有助于模型更好地掌握领域相关的信息，并在复杂的术语中保持一致性。
- en: 'There are three techniques for model domain adaptation: prompt engineering,
    RAG, and fine-tuning. Strictly speaking, RAG is a form of dynamic prompt engineering
    where developers use a retrieval system to add content to an existing prompt,
    but RAG systems are used so often that it’s worth discussing them separately.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种模型领域适应的技术：提示工程、RAG和微调。严格来说，RAG是一种动态提示工程的形式，其中开发人员使用检索系统向现有提示添加内容，但RAG系统使用得如此频繁，因此值得单独讨论。
- en: One critical difference with fine-tuning is that you must have access to the
    model’s weights, information that is usually not available with cloud-based, proprietary
    LLMs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与微调相比，一个关键的区别是，你必须能够访问模型的权重，这种信息通常在基于云的专有LLMs中不可用。
- en: Prompt Engineering
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程
- en: In *prompt engineering*, we customize the prompts or questions we give the model
    to get more accurate or insightful responses. The way a prompt is structured has
    a massive impact on how well a model understands the task at hand and, ultimately,
    how well it performs. Given LLMs’ versatility, prompt engineering has become an
    important skill for getting the most out of these models across different domains
    and tasks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在*提示工程*中，我们定制提供给模型的提示或问题，以获得更准确或更有洞察力的回答。提示的结构方式对模型理解手头的任务以及最终表现有巨大影响。鉴于LLMs的通用性，提示工程已成为在不同领域和任务中充分利用这些模型的重要技能。
- en: The key is to understand how different prompt structures lead to different model
    behaviors. There are various strategies—ranging from simple one-shot prompting
    to more complex techniques like chain-of-thought prompting—that can significantly
    improve the effectiveness of LLMs. Let’s look into some common techniques.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于理解不同的提示结构如何导致不同的模型行为。有各种策略——从简单的单次提示到更复杂的技巧，如思维链提示——这些策略可以显著提高大型语言模型（LLMs）的有效性。让我们来看看一些常见的技巧。
- en: One-Shot Prompting
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单次提示
- en: '*One-shot prompting* refers to providing the model with a single example of
    a prompt and the kind of output you’re expecting. This is a relatively simple
    approach. The idea is to show the model what kind of answer or action you want
    by giving a clear and concise example. One-shot prompting works best when the
    task is simple and well-defined and doesn’t require the model to infer many patterns.
    If you’re asking the model to translate text, a one-shot prompt might look like
    this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*单次提示*指的是向模型提供一个提示示例以及你期望的输出类型。这是一个相对简单的方法。想法是通过提供一个清晰简洁的示例来展示你想要的答案或行为类型。单次提示在任务简单且定义明确，且不需要模型推断许多模式时效果最佳。如果你要求模型翻译文本，一个单次提示可能看起来像这样：'
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After showing the example, you can then ask the model to translate a new sentence:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 展示示例后，你可以要求模型翻译一个新的句子：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For more complex tasks, one-shot prompting may not provide enough context for
    the model to generate reliable or meaningful results.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的任务，单次提示可能无法为模型提供足够的信息来生成可靠或有意义的成果。
- en: Few-Shot Prompting
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 少样本提示
- en: '*Few-shot prompting* takes things a step further by providing the model with
    multiple examples of the desired output. This method is useful when the task involves
    identifying patterns or when the model needs more context to perform well. These
    examples give the model a better understanding of what the output should look
    like, and it can then apply those patterns to unseen examples.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*少样本提示*通过为模型提供多个期望输出的示例来更进一步。当任务涉及识别模式或模型需要更多上下文才能表现良好时，这种方法很有用。这些示例使模型更好地理解输出应该是什么样子，然后它可以应用这些模式到未见过的示例中。'
- en: Few-shot prompting is particularly useful when the task involves generating
    specific types of responses, such as text generation in a particular style or
    format. The more examples you give, the better the model becomes at identifying
    the task’s underlying structure.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本提示在涉及生成特定类型响应的任务中特别有用，例如以特定风格或格式生成文本。你提供的示例越多，模型在识别任务潜在结构方面的能力就越强。
- en: 'For example, imagine you’re asking the model to generate math word problems,
    and you give it a few examples to show how to generate them from given data:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象你要求模型生成数学文字问题，并给它几个示例来展示如何从给定数据中生成它们：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, you can ask the model to generate a new problem:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以要求模型生成一个新的问题：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With few-shot examples, the model is more likely to generate a relevant word
    problem that matches the style and logic of the examples provided. Few-shot prompting
    is highly effective in tasks like text summarization, translation, and question
    generation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用少样本示例，模型更有可能生成与提供的示例风格和逻辑相匹配的相关文字问题。少样本提示在文本摘要、翻译和问题生成等任务中非常有效。
- en: Chain-of-Thought Prompting
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维链提示
- en: '*Chain-of-thought prompting* encourages the model to break down its reasoning
    process step-by-step, making the reasoning process more explicit and understandable,
    rather than just providing the final answer. This approach is particularly valuable
    for complex tasks that require logical reasoning, multiple steps, or problem-solving,
    such as mathematical reasoning, decision-making, or any situation where intermediate
    steps are important. It helps models avoid making incorrect or oversimplified
    assumptions by encouraging them to evaluate different aspects of the task before
    reaching a conclusion.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*思维链提示*鼓励模型逐步分解其推理过程，使推理过程更加明确和易于理解，而不是仅仅提供最终答案。这种方法对于需要逻辑推理、多个步骤或解决问题的复杂任务特别有价值，例如数学推理、决策或任何中间步骤重要的情境。它通过鼓励模型在得出结论之前评估任务的各个方面，帮助模型避免做出错误或过于简化的假设。'
- en: 'Let’s say you’re asking the model to solve a math problem that involves multiple
    steps. Using chain-of-thought prompting, you would encourage the model to reason
    through the problem rather than simply provide an answer:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你要求模型解决一个涉及多个步骤的数学问题。使用思维链提示，你会鼓励模型通过问题进行推理，而不是简单地提供一个答案：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, you can ask the model to solve a new problem:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以要求模型解决一个新的问题：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Chain-of-thought prompting helps the model demonstrate its reasoning and ensures
    that it isn’t skipping over crucial details.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链提示帮助模型展示其推理，并确保它没有跳过关键细节。
- en: One powerful strategy is to combine these different types of prompting to leverage
    their individual strengths. For example, you might start with few-shot prompting
    to give the model some context and examples, then switch to chain-of-thought prompting
    to guide it through the reasoning process. This hybrid approach can be highly
    effective for more intricate tasks that require both pattern recognition and logical
    reasoning.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一种强大的策略是将这些不同类型的提示结合起来，以利用它们的各自优势。例如，你可能从少样本提示开始，为模型提供一些上下文和示例，然后切换到思维链提示，以引导其通过推理过程。这种混合方法对于需要模式识别和逻辑推理的更复杂任务非常有效。
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This combined approach would help the model generate a detailed and coherent
    description by first learning from a few examples and then reasoning through the
    unique features of the object.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结合方法将有助于模型首先从几个示例中学习，然后通过分析对象的独特特征来进行推理，从而生成详细且连贯的描述。
- en: Retrieval-Augmented Generation
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索增强生成
- en: '*Retrieval-augmented generation* (RAG) is one of the most powerful techniques
    for combining pretrained language models with external knowledge sources. It uses
    retrieval-based methods to improve the generative model’s ability to handle complex
    queries or provide more accurate, fact-based responses. RAG models combine the
    power of information retrieval with text generation, making them especially useful
    for tasks where knowledge from large external corpora or databases is required.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*检索增强生成*（RAG）是将预训练语言模型与外部知识源相结合的最强大技术之一。它使用基于检索的方法来提高生成模型处理复杂查询或提供更准确、基于事实的响应的能力。RAG
    模型结合了信息检索和文本生成的力量，使它们在需要从大型外部语料库或数据库中获取知识的任务中特别有用。'
- en: RAG works by retrieving relevant documents or pieces of information from a knowledge
    base or search engine, which are then used to inform the generation process. This
    method enables the model to reference real-world data, producing responses that
    are not limited by the model’s preexisting knowledge.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 通过从知识库或搜索引擎检索相关文档或信息片段来工作，这些信息随后被用于告知生成过程。这种方法使模型能够参考现实世界的数据，生成不受模型现有知识限制的响应。
- en: In a typical RAG model, the input query goes through two main stages. First,
    in the *retrieval* stage, a retrieval system fetches relevant documents or text
    snippets from a knowledge base, search engine, or database. Then, in the *generation*
    stage, the LLM generates output based on the input query and the retrieved text
    snippets.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的 RAG 模型中，输入查询经过两个主要阶段。首先，在 *检索* 阶段，检索系统从知识库、搜索引擎或数据库中检索相关文档或文本片段。然后，在 *生成*
    阶段，LLM 根据输入查询和检索到的文本片段生成输出。
- en: RAG allows the model to effectively handle complex questions, fact-check its
    responses, and dynamically reference a broad range of external information. For
    example, a question-answering system built with a RAG model could provide more
    accurate answers by first retrieving relevant documents or Wikipedia entries and
    then generating a response based on those documents.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 允许模型有效地处理复杂问题，验证其回答的事实性，并动态地引用广泛的外部信息。例如，使用 RAG 模型构建的问答系统可以通过首先检索相关文档或维基百科条目，然后基于这些文档生成响应，从而提供更准确的答案。
- en: The code in [Example 5-9](#ch05_example_8_1748896666808830) demonstrates how
    you can implement a simple RAG-based model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-9](#ch05_example_8_1748896666808830) 中的代码演示了如何实现一个基于 RAG 的简单模型。'
- en: Example 5-9\. RAG implementation
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-9\. RAG 实现
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: RAG can be particularly useful in scenarios like open-domain question answering,
    where the model may need to access and reference up-to-date or highly specific
    information.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 在诸如开放域问答等场景中特别有用，在这些场景中，模型可能需要访问和引用最新或高度具体的信息。
- en: Semantic Kernel
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义内核
- en: '[Semantic Kernel](https://oreil.ly/ljuli) is a framework designed to simplify
    integrating LLMs into applications that require dynamic knowledge, reasoning,
    and state tracking. It’s particularly useful when you want to build complex, modular
    AI systems that can interact with external APIs, knowledge bases, or decision-making
    processes. Semantic Kernel focuses on building more flexible AI systems that can
    handle a variety of tasks beyond just generating text. It allows for modularity,
    enabling developers to easily combine different components—such as embeddings,
    prompt templates, and custom functions—in a cohesive manner.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[语义内核](https://oreil.ly/ljuli) 是一个旨在简化将 LLM 集成到需要动态知识、推理和状态跟踪的应用程序的框架。当你想要构建可以与外部
    API、知识库或决策过程交互的复杂、模块化 AI 系统时，它尤其有用。语义内核专注于构建更灵活的 AI 系统，能够处理超出仅生成文本的各种任务。它允许模块化，使开发者能够以统一的方式轻松组合不同的组件——例如嵌入、提示模板和自定义函数。'
- en: It supports asynchronous processing, which is useful for managing long-running
    tasks or interacting with external services, and can be used in conjunction with
    models that require reasoning through complex steps, like those using chain-of-thought
    prompting. The framework supports maintaining and retrieving *semantic memory*,
    which helps the model to remember past interactions or previously retrieved information
    to generate more consistent results. Finally, Semantic Kernel can integrate external
    functions and APIs, making it easy to combine model inference with real-world
    data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 它支持异步处理，这对于管理长时间运行的任务或与外部服务交互非常有用，并且可以与需要通过复杂步骤进行推理的模型（如使用思维链提示的模型）结合使用。该框架支持维护和检索*语义记忆*，这有助于模型记住过去的交互或之前检索到的信息，以生成更一致的结果。最后，语义内核可以集成外部函数和API，使得将模型推理与实际数据相结合变得容易。
- en: 'As [Example 5-10](#ch05_example_9_1748896666808845) shows, you can use Semantic
    Kernel to build a modular assistant that can:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如[示例 5-10](#ch05_example_9_1748896666808845)所示，您可以使用语义内核构建一个模块化助手，该助手可以：
- en: Retrieve historical information from a knowledge base
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从知识库中检索历史信息
- en: Use an external API to fetch live data (such as stock prices)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外部API获取实时数据（例如股票价格）
- en: Process natural language instructions
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理自然语言指令
- en: Perform complex reasoning tasks by chaining multiple AI functions together
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过链接多个AI函数执行复杂推理任务
- en: Example 5-10\. Semantic Kernel
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-10\. 语义内核
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can also integrate external functions (like fetching data from Azure APIs)
    to further enrich the model’s responses and maintain state across interactions.
    This makes Semantic Kernel an excellent choice for creating sophisticated AI-driven
    applications.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以集成外部函数（如从Azure API获取数据）以进一步丰富模型的响应，并在交互中保持状态。这使得语义内核成为创建复杂AI驱动应用程序的绝佳选择。
- en: While RAG enhances generative models by integrating external knowledge sources
    for fact-based responses, Semantic Kernel provides a flexible framework for building
    modular AI systems with advanced reasoning and stateful interactions. If you want
    to make behavioral changes in a model, however, you should use *fine-tuning.*
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 RAG 通过集成外部知识源以提供基于事实的响应来增强生成模型，但语义内核提供了一个灵活的框架，用于构建具有高级推理和状态交互的模块化AI系统。然而，如果您想在模型中做出行为改变，则应使用*微调*。
- en: Fine-Tuning
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调
- en: Compared to training from scratch, which requires massive amounts of data and
    compute, *fine-tuning* allows you to adapt an already-trained model to new tasks
    with fewer resources. By modifying the model’s parameters based on specific data
    or behaviors, fine-tuning makes LLMs more effective for specialized applications,
    whether for handling a particular industry’s terminology or modulating the model’s
    style and tone. Note that fine-tuning changes a model’s weights, and this means
    you must have access to it, either directly through a model checkpoint or indirectly,
    like OpenAI provides through its fine-tuning APIs.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与从头开始训练相比，从头开始训练需要大量的数据和计算资源，*微调*允许您使用更少的资源将已训练的模型适应新任务。通过根据特定数据或行为修改模型的参数，微调使得大型语言模型（LLMs）在特定应用中更加有效，无论是处理特定行业的术语还是调节模型的风格和语气。请注意，微调会改变模型的权重，这意味着您必须能够访问它，无论是直接通过模型检查点还是间接地，例如OpenAI通过其微调API提供的方式。
- en: Fine-tuning offers a range of strategies to adapt pretrained models to specialized
    tasks, improve their efficiency, and ensure they align with user expectations.
    Techniques like adaptive fine-tuning, adapters, and parameter-efficient methods
    help tailor LLMs to specific domains, all while reducing resource requirements.
    Fine-tuning isn’t just about improving task accuracy; it also focuses on adjusting
    model behavior, ensuring that outputs are ethically sound, efficient, and user-friendly.
    Whether you’re working with a complex domain-specific model or a general-purpose
    assistant, fine-tuning makes your models smarter, more efficient, and more aligned
    with your needs.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 微调提供了一系列策略，以适应预训练模型以特定任务，提高其效率，并确保它们符合用户期望。如自适应微调、适配器和参数高效方法等技术有助于将LLMs定制到特定领域，同时减少资源需求。微调不仅仅是提高任务准确性；它还侧重于调整模型行为，确保输出在道德上是合理的、高效的且用户友好的。无论您是在处理复杂的特定领域模型还是通用助手，微调都能使您的模型更智能、更高效，并与您的需求更一致。
- en: In this section, we’ll dive into several key strategies for fine-tuning LLMs,
    from adaptive fine-tuning to techniques like prefix tuning and parameter-efficient
    fine-tuning (PEFT), each serving different needs while maintaining efficiency.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入了解几个关键的微调LLM策略，从自适应微调到前缀微调和技术如参数高效微调（PEFT），它们在满足不同需求的同时保持效率。
- en: Adaptive Fine-Tuning
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应微调
- en: '*Adaptive fine-tuning* is the process of updating a model’s parameters so it
    can better handle a specific dataset or task. It involves training the model on
    new data that is more closely aligned with the task at hand. For example, if you
    have an LLM that has been pretrained on general web text, adaptive fine-tuning
    can help it specialize in a particular area like medical texts, legal jargon,
    or customer service interactions. The goal of adaptive fine-tuning is to adjust
    the model’s weights in a way that enables it to capture more domain-specific knowledge
    without forgetting the general understanding it already possesses.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*自适应微调* 是一个更新模型参数的过程，以便它能更好地处理特定的数据集或任务。这涉及到在更接近当前任务的新数据上训练模型。例如，如果你有一个在通用网络文本上预训练的LLM，自适应微调可以帮助它专门化于特定领域，如医学文本、法律术语或客户服务互动。自适应微调的目标是以一种方式调整模型的权重，使其能够捕获更多领域特定的知识，同时不会忘记它已经拥有的通用理解。'
- en: 'Suppose you’re fine-tuning a model for medical question answering. Your base
    model might be trained on a diverse dataset, but for the fine-tuning dataset,
    you’ll use a collection of medical-related texts—such as research papers, clinical
    notes, and FAQs. Consider the following prompt:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在微调一个用于医学问答的模型。你的基础模型可能在多样化的数据集上进行了训练，但对于微调数据集，你将使用一组与医学相关的文本——例如研究论文、临床笔记和常见问题解答。考虑以下提示：
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Adapters (Single, Parallel, and Scaled Parallel)
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适配器（单个、并行和缩放并行）
- en: '*Adapters* are a powerful method for efficient fine-tuning. Instead of retraining
    the entire model, adapters introduce small, task-specific modules that are trained
    while leaving the original model’s parameters frozen. This approach makes fine-tuning
    much more computationally efficient since only a small part of the model is modified.
    Adapters are particularly useful when you need to apply fine-tuning across multiple
    tasks, as they allow the model to maintain its general capabilities while adapting
    to specific contexts. Methods for using adapters include:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*适配器* 是一种高效的微调方法。与重新训练整个模型不同，适配器引入了小型、任务特定的模块，在训练时保持原始模型参数冻结。这种方法使得微调在计算上更加高效，因为只有模型的一小部分被修改。适配器在需要将微调应用于多个任务时特别有用，因为它们允许模型保持其通用能力，同时适应特定上下文。使用适配器的方法包括：'
- en: Single adapter
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 单个适配器
- en: A single task-specific adapter is added to the model, allowing it to focus on
    one task. The rest of the model stays unchanged.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 向模型添加一个单一的任务特定适配器，使其能够专注于一个任务。模型的其余部分保持不变。
- en: Parallel adapters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 并行适配器
- en: Multiple adapters can be trained in parallel for different tasks. Each adapter
    handles its task, and the original model’s weights remain frozen.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 可以并行训练多个适配器以处理不同的任务。每个适配器处理其任务，而原始模型的权重保持冻结。
- en: Scaled parallel adapters
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放并行适配器
- en: For more complex use cases, multiple adapters can be trained at different scales,
    allowing the model to handle more complex tasks and achieve higher performance
    without overburdening its architecture.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的使用案例，可以在不同规模上训练多个适配器，使模型能够处理更复杂的任务并实现更高的性能，而不会对其架构造成过重负担。
- en: 'Say you’re applying the model to two tasks: text summarization and sentiment
    analysis. You could introduce two parallel adapters, one fine-tuned for summarization
    and the other for sentiment analysis. The model would utilize the appropriate
    adapter for each task while still benefiting from its general knowledge.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你将模型应用于两个任务：文本摘要和情感分析。你可以引入两个并行适配器，一个用于微调摘要，另一个用于情感分析。模型将利用每个任务的适当适配器，同时仍然受益于其通用知识。
- en: Behavioral Fine-Tuning
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行为微调
- en: '*Behavioral fine-tuning* focuses on adjusting the model’s behavior to match
    specific expectations, such as producing more ethical, polite, or user-friendly
    outputs. In many real-world applications, it’s crucial that language models align
    with human values, especially when interacting with sensitive topics or making
    decisions that affect users. Through fine-tuning on data that reflects the desired
    behavior, the model can learn to produce responses that better adhere to a code
    of conduct or ethical guidelines. This is particularly useful in customer-service
    chatbots, healthcare assistants, and other models that interact directly with
    users.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*行为微调*专注于调整模型的行为以符合特定期望，例如产生更多道德、礼貌或用户友好的输出。在许多实际应用中，语言模型与人类价值观保持一致至关重要，尤其是在处理敏感话题或影响用户决策时。通过在反映所需行为的数据上进行微调，模型可以学习产生更符合行为准则或道德指南的响应。这在客户服务聊天机器人、医疗助手和其他直接与用户交互的模型中特别有用。'
- en: 'For a chatbot that provides mental health advice, you could fine-tune the model
    using datasets that emphasize empathetic responses, ensuring that the model’s
    replies are both helpful and compassionate. Consider the following prompt and
    output:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提供心理健康建议的聊天机器人，你可以使用强调同理心响应的数据集来微调模型，确保模型的回复既有帮助又富有同情心。考虑以下提示和输出：
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Behavioral fine-tuning is vital in ensuring that models don’t just deliver accurate
    responses but also reflect the right tone and ethics.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 行为微调对于确保模型不仅提供准确的响应，而且反映正确的语气和道德至关重要。
- en: Prefix Tuning
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前缀调整
- en: '*Prefix tuning* is a technique for fine-tuning a model’s behavior for specific
    tasks without drastically changing its structure or altering its core weights.
    Instead of modifying the entire model, prefix tuning adjusts only a small, tunable
    part of the model: the *prefix*, a small input sequence that is prepended to the
    input data. The model uses the prefix to adapt its outputs to a specific task.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*前缀调整*是一种微调模型行为以适应特定任务的技术，而不会大幅改变其结构或改变其核心权重。不是修改整个模型，前缀调整仅调整模型的一小部分可调整部分：*前缀*，这是一个小输入序列，被添加到输入数据之前。模型使用前缀来调整其输出以适应特定任务。'
- en: 'This method is highly efficient because it requires fewer resources than traditional
    fine-tuning and allows for specialized adaptations without having to retrain the
    entire model. If you are fine-tuning a model to generate poetry, the prefix might
    include a sequence that sets the tone or style of the poem, while the model generates
    the rest of the content accordingly:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法非常高效，因为它比传统微调需要的资源更少，并且可以在不重新训练整个模型的情况下进行专门的调整。如果你正在微调一个用于生成诗歌的模型，前缀可能包括设置诗歌基调或风格的序列，而模型则相应地生成其余内容：
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, only the prefix is adjusted to favor the Shakespearean style, but the
    rest of the model remains unchanged.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，仅调整前缀以偏向莎士比亚风格，但模型的其余部分保持不变。
- en: Parameter-Efficient Fine-Tuning
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数高效微调
- en: '*Parameter-efficient fine-tuning* (PEFT) is a technique designed to fine-tune
    large models using minimal resources. Traditional fine-tuning involves modifying
    the entire model’s parameters, which can be both time-consuming and costly. PEFT
    techniques, like *low-rank adaptation* (LoRA) and quantized LoRA (qLoRA), focus
    on modifying only a small, low-ranked portion of the model’s weights, saving on
    memory and compute resources while maintaining model performance. They are particularly
    useful when working with very large models, such as GPT-3 or GPT-4, where full
    fine-tuning would be prohibitively expensive. LoRA introduces a low-rank approximation
    for the weight updates, reducing the number of parameters that need to be fine-tuned.
    This makes the process more efficient without sacrificing accuracy. And qLoRA
    builds on LoRA by incorporating quantization to reduce storage requirements even
    further, making it ideal for large-scale deployments.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*参数高效微调*（PEFT）是一种旨在使用最少资源微调大型模型的技术。传统的微调涉及修改整个模型的参数，这既耗时又昂贵。PEFT技术，如*低秩调整*（LoRA）和量化LoRA（qLoRA），专注于仅修改模型权重的一小部分，低秩部分，从而节省内存和计算资源，同时保持模型性能。它们在处理非常大的模型时特别有用，例如GPT-3或GPT-4，因为完全微调会非常昂贵。LoRA引入了对权重更新的低秩近似，减少了需要微调的参数数量。这使得过程更高效，同时不牺牲准确性。qLoRA通过引入量化进一步减少存储需求，使其非常适合大规模部署。'
- en: For an LLM deployed in a resource-constrained environment, you could apply LoRA
    to adjust just the weights that handle specific tasks, such as summarization or
    question answering. This allows for quicker updates and lowers computational costs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于部署在资源受限环境中的LLM，你可以应用LoRA（低秩自适应）来调整仅处理特定任务（如摘要或问答）的权重。这允许更快地更新并降低计算成本。
- en: Instruction Tuning and Reinforcement Learning from Human Feedback
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指令调整与人类反馈强化学习
- en: '*Instruction tuning* involves fine-tuning a model so that it follows explicit
    instructions in a more precise and reliable way. This can be particularly useful
    when you need the model to consistently perform specific tasks based on user instructions
    or prompts.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*指令调整*涉及微调模型，使其以更精确和可靠的方式遵循明确的指令。这在需要模型根据用户指令或提示持续执行特定任务时特别有用。'
- en: With *reinforcement learning from human feedback* (RLHF), the model receives
    feedback from humans on its outputs, allowing it to improve over time. This feedback
    loop helps the model better align with user expectations and improve the relevance,
    coherence, and overall quality of its responses.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*人类反馈强化学习*（RLHF），模型从人类那里获得对其输出的反馈，使其随着时间的推移不断改进。这个反馈循环有助于模型更好地与用户期望保持一致，并提高其响应的相关性、连贯性和整体质量。
- en: RLHF is often used to fine-tune models for conversational agents or other interactive
    systems, ensuring that the model’s responses are not only accurate but also helpful
    and appropriate to the context.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF通常用于微调用于对话代理或其他交互式系统的模型，确保模型的响应不仅准确，而且有帮助且适合上下文。
- en: For a virtual assistant, you might first fine-tune the model using instruction
    tuning to ensure it answers questions directly. Then, using RLHF, you would gather
    feedback from users on the helpfulness of responses and adjust the model to improve
    its conversational behavior.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个虚拟助手，你可能首先使用指令调整微调模型以确保它直接回答问题。然后，使用RLHF，你会收集用户对响应有用性的反馈，并调整模型以改善其对话行为。
- en: 'An instruction-tuning prompt might be:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一个指令调整提示可能是：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'An RLHF prompt might be:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RLHF提示可能是：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: With RLHF, the model can continue to learn and improve, aligning its behavior
    with real-world user needs.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通过RLHF，模型可以继续学习和改进，使其行为与真实世界的用户需求保持一致。
- en: Choosing Between Fine-Tuning and Prompt Engineering
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在微调和提示工程之间进行选择
- en: If you don’t have access to a way to modify the model’s weights and you want
    to adapt your model for a specific domain, you will have to use prompt engineering.
    But when both choices are available, which one should you use?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有修改模型权重的途径，并且想要为特定领域调整你的模型，你必须使用提示工程。但是，当两种选择都可用时，你应该选择哪一个？
- en: The first thing to consider is the cost, as fine-tuning is expensive in terms
    of computational costs. You can usually get to a better prompt with a few hours
    of experimentation, but running a fine-tuning experiment can cost thousands of
    dollars. A recent price list from OpenAI (as of this writing) lists the fine-tuning
    cost for its latest GPT-4o model at $25,000 per million tokens.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要考虑的是成本，因为微调在计算成本上很昂贵。你通常可以通过几小时的实验得到更好的提示，但运行微调实验可能需要数千美元。OpenAI最近的价格列表（截至本文撰写时）显示，其最新的GPT-4o模型的微调成本为每百万个令牌25,000美元。
- en: While fine-tuning charges the costs up front, prompt engineering is more like
    a mortgage. Developing a larger prompt through prompt engineering will increase
    your costs for every request, whereas inference costs the same whether the model
    is fine-tuned or not. One additional thing to consider is that there is a lot
    of change in the LLM space these days, so the time horizons to recoup costs are
    likely to be short. If fine-tuning and prompt engineering have the same performance
    and costs over a 10-year horizon, but the model you’re using will have a 2-year
    life span, it’s not cost-effective to prepay for 10 years of something that you’re
    only going to use for 2 years. Prompt engineering in this case would be a better
    choice in terms of cost.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的微调会提前增加成本，而提示工程则更像是一种按揭。通过提示工程开发更大的提示将增加每次请求的成本，而推理成本则不因模型是否经过微调而改变。还有一个需要考虑的因素是，目前LLM（大型语言模型）领域变化很大，因此回收成本的时长远可能较短。如果微调和提示工程在10年的预期内具有相同的表现和成本，但你所使用的模型寿命仅为2年，那么提前支付10年的费用来使用仅2年就足够的东西是不划算的。在这种情况下，提示工程在成本方面会是一个更好的选择。
- en: Even if you have access to the model weights, don’t need to worry about costs,
    and only want to focus on performance, it helps to know that fine-tuning and prompt
    engineering solve different problems. Prompt engineering changes what the model
    *knows about*, giving it more context. RAG does what prompt engineering does,
    but on a much larger scale, using a system to generate prompts dynamically based
    on inputs. On the other hand, fine-tuning changes how the model *behaves*. The
    quadrant diagram in [Figure 5-2](#ch05_figure_2_1748896666799695) illustrates
    this.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你可以访问模型权重，不需要担心成本，只想专注于性能，了解微调和提示工程解决不同问题也是有帮助的。提示工程改变模型*知道的内容*，给它更多的上下文。RAG做的是提示工程做的事情，但规模更大，使用系统根据输入动态生成提示。另一方面，微调改变模型*的行为方式*。图5-2中的象限图说明了这一点。
- en: For example, let’s assume the model you’re using has all the knowledge it needs,
    but you want it to generate answers using a specific XML format instead of the
    usual chat outputs because your output will be consumed by another system. In
    this case, fine-tuning the model will yield much better performance than giving
    it lots of examples of how you want the output to look through prompt engineering.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你使用的模型已经拥有了所需的所有知识，但你希望它使用特定的XML格式来生成答案，而不是通常的聊天输出，因为你的输出将被另一个系统消费。在这种情况下，微调模型将比通过提示工程给出大量你希望输出看起来如何的示例获得更好的性能。
- en: '![](assets/llmo_0502.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/llmo_0502.png)'
- en: Figure 5-2\. LLM and context optimization
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. LLM和上下文优化
- en: 'It’s worthwhile to point out an unexpected consequence of changing how a model
    behaves through fine-tuning: it can cause the model to stop doing things it did
    before. Let’s say you are using a model like GPT-3.5-turbo to generate blog posts
    about your product, and it’s doing a good job, but the posts are not very technical.
    An AI engineer suggests fine-tuning the model using the messages in the “product
    chat” channel inside your company, where people discuss technical features of
    the product. After fine-tuning the model, you ask it to “generate a 500-word blog
    post about feature *X* of our product,” something that it would do reasonably
    well before, just without much technical depth. Now it answers: “I’m too busy.”
    Fine-tuning changed how the model behaves. In this case, a RAG solution that searches
    the product chat data and creates a prompt describing feature *X* to the old model
    would work a lot better.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 值得指出的是，通过微调改变模型行为的一个意想不到的后果：它可能导致模型停止之前做的事情。比如说，你正在使用GPT-3.5-turbo这样的模型来生成关于你产品的博客文章，它做得很好，但这些文章不太技术化。一位AI工程师建议使用公司内部“产品聊天”频道中的消息来微调模型，那里的人们讨论产品的技术特性。微调模型后，你要求它“生成一篇关于我们产品特性*X*的500字博客文章”，这是它之前可以做得相当好的事情，只是没有太多的技术深度。现在它的回答是：“我太忙了。”微调改变了模型的行为。在这种情况下，一个RAG解决方案，通过搜索产品聊天数据并创建一个描述特性*X*的提示给旧模型，会工作得更好。
- en: Mixture of Experts
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专家混合
- en: Adaptive fine-tuning and PEFT methods optimize the adaptation of large language
    models by selectively updating parameters or leveraging instruction tuning. A
    more recent technique, *mixture of experts* (MoE), approaches optimization from
    a different angle, that of architectural modularity and conditional computation.
    Unlike fine-tuning, which changes the model’s parameters after training, MoE changes
    the model’s structure itself by leveraging many “experts,” which are smaller specialized
    subnetworks inside one big model (see [Figure 5-3](#ch05_figure_3_1748896666799717)).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应微调和PEFT方法通过选择性地更新参数或利用指令调整来优化大型语言模型的适应性。一种更近期的技术，*专家混合*（MoE），从不同的角度接近优化，即架构模块化和条件计算。与微调不同，微调在训练后改变模型的参数，而MoE通过利用许多“专家”来改变模型的结构本身，这些“专家”是大型模型内部较小的专用子网络（参见图5-3）。
- en: Instead of using the whole model to make inferences, a gating system selects
    and activates only a few of these experts, based on the input. This means that
    the model uses only part of its capacity to answer each query. The benefit? You
    can build a huge model with trillions of parameters but keep the computation cost
    low, because only a small piece of it runs each time.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用整个模型进行推理不同，一个门控系统根据输入选择并激活这些专家中的少数几个。这意味着模型只使用其部分能力来回答每个查询。好处是？你可以构建一个具有万亿参数的巨大模型，但保持计算成本很低，因为每次只有其中一小部分在运行。
- en: This is different from adaptive fine-tuning or parameter-efficient tuning in
    which you update the model to better handle new tasks. MoE lets the model specialize
    inside itself, which means that some experts get better at certain types of tasks
    or data, while others focus elsewhere. The model learns to route inputs dynamically,
    making it flexible across many domains without retraining the whole thing every
    time.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这与自适应微调或参数高效微调不同，后者是更新模型以更好地处理新任务。MoE让模型在其内部进行专业化，这意味着一些专家在特定类型的任务或数据上变得更好，而其他专家则专注于其他方面。模型学习动态路由输入，使其能够在许多领域灵活运用，而无需每次都重新训练整个模型。
- en: That said, MoEs aren’t perfect. If the gating system doesn’t spread the work
    evenly, only a few experts do most of the work, wasting the rest of the model
    and reducing efficiency. Also, training these models is more complex and requires
    special software and hardware support to get the speed and cost benefits.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，MoEs并不完美。如果门控系统没有均匀分配工作，只有少数专家做大部分工作，浪费了模型的其余部分并降低了效率。此外，训练这些模型更复杂，需要特殊的软件和硬件支持以获得速度和成本效益。
- en: '![](assets/llmo_0503.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_0503.png)'
- en: 'Figure 5-3\. A visualization of how MoE works (source: [“Mixture of Experts”](https://oreil.ly/r4ys1))'
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3\. MoE工作原理的可视化（来源：[“混合专家”](https://oreil.ly/r4ys1)）
- en: MoE models like GShard, DeepSeek, and others change how LLMs handle scale by
    splitting the model into many smaller expert subnetworks and selectively activating
    only a few for each input token. The key to this capability is the gating network,
    a small module that uses the hidden state of each token to produce a score for
    every expert. In GShard, these scores go through a softmax function, which converts
    a vector of raw prediction scores into a probability distribution over all experts.
    The top two experts per token are selected, and the token’s representation is
    sent to both experts, weighted by their gating probabilities. This routing of
    work to two experts can improve the model’s expressiveness but increases communication
    overhead, since tokens must be sent to multiple experts on different devices.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GShard、DeepSeek和其他MoE模型通过将模型分成许多更小的专家子网络并针对每个输入标记选择性地激活其中的一些来改变LLMs处理规模的方式。这种能力的关键是门控网络，这是一个小型模块，它使用每个标记的隐藏状态为每个专家生成一个分数。在GShard中，这些分数通过softmax函数进行处理，将原始预测分数向量转换为所有专家的概率分布。每个标记选择前两个专家，并将标记的表示发送给这两个专家，根据它们的门控概率进行加权。将工作分配给两个专家可以提高模型的表达能力，但会增加通信开销，因为标记必须发送到不同设备上的多个专家。
- en: Switch Transformer, on the other hand, simplifies this by using hard routing.
    *Hard routing* requires the gating to pick the single expert with the highest
    score for each token. This means each token activates only one expert, reducing
    cross-device communication and memory usage. The gating output is a one-shot vector
    indicating which expert is responsible, and this top-one routing cuts down the
    data that needs to move between accelerators.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Switch Transformer通过使用硬路由来简化这个过程。*硬路由*需要门控机制选择每个标记的最高得分专家。这意味着每个标记只激活一个专家，从而减少跨设备通信和内存使用。门控输出是一个一次性向量，指示哪个专家负责，这种顶级路由减少了需要在加速器之间移动的数据量。
- en: One common challenge for all MoE models is *load balancing*. Without constraints,
    the gating tends to funnel most tokens to a small set of popular experts, causing
    some experts to be overloaded while others remain idle. This expert collapse wastes
    capacity and slows training convergence. To fix this, training adds a load-balancing
    loss term to the main objective. This loss term measures how tokens are distributed
    across experts by first calculating the fraction of tokens assigned and the gating
    probability mass for each expert. It then computes the coefficient of variation
    across these values and penalizes uneven distributions, forcing the gating network
    to spread tokens more uniformly. This keeps all experts busy and fully utilizes
    the model’s parameter budget.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 所有MoE模型都面临的一个共同挑战是*负载均衡*。如果没有约束，门控机制往往会将大多数标记引导到一小部分流行专家，导致一些专家过载而其他专家闲置。这种专家崩溃浪费了容量并减慢了训练收敛。为了解决这个问题，训练过程中在主要目标中添加了一个负载均衡损失项。这个损失项通过首先计算每个专家分配的标记分数和门控概率质量来衡量标记在专家之间的分布情况。然后，它计算这些值的变异系数并惩罚不均匀分布，迫使门控网络更均匀地分配标记。这使所有专家都保持忙碌并充分利用模型的参数预算。
- en: Each expert has a fixed capacity in that it can process only a limited number
    of tokens per batch to fit within memory constraints. If too many tokens are routed
    to an expert, excess tokens are either dropped or rerouted. Although such token
    dropping can prevent memory overflow, it can also cause some input data to be
    ignored during training—a trade-off that needs careful tuning.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 每个专家都有一个固定的容量，即它只能处理每个批次中有限数量的标记，以适应内存限制。如果将过多的标记路由到专家，则多余的标记将被丢弃或重新路由。尽管这种标记丢弃可以防止内存溢出，但它也可能导致在训练期间忽略一些输入数据——这是一个需要仔细调整的权衡。
- en: During *backpropagation*, gradients flow only through the experts that were
    activated for each token. Experts not involved in processing a given token receive
    no gradient updates. Thus, computation and memory use are less than they would
    otherwise be, given the model’s huge parameter count. This sparsity in gradient
    flow is one reason MoEs can scale efficiently.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在*反向传播*过程中，梯度仅通过为每个标记激活的专家流动。未参与处理给定标记的专家不会收到梯度更新。因此，计算和内存使用量比模型巨大的参数数量所应有的要少。这种梯度流的稀疏性是MoE能够高效扩展的一个原因。
- en: Training MoEs is tricky, and the process can be unstable. Researchers use several
    techniques to enhance stability. For example, gating weights are carefully initialized
    to avoid extreme outputs early on, dropout can be applied to gating outputs to
    prevent the gating network from becoming overconfident based on a few experts,
    and gradient clipping can be used to keep updates ​stable. Load balancing losses
    not only improves utilization but also helps to stabilize routing decisions during
    training.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 训练MoE很棘手，过程可能不稳定。研究人员使用多种技术来提高稳定性。例如，门控权重被仔细初始化以避免早期产生极端输出，可以通过门控输出应用dropout来防止门控网络基于少数专家而变得过于自信，并且可以使用梯度裁剪来保持更新稳定。负载平衡损失不仅提高了利用率，还有助于在训练期间稳定路由决策。
- en: Overall, MoE is another way to make huge models more scalable and adaptable.
    That said, it often complements rather than replaces fine-tuning methods.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，MoE（多专家模型）是使大型模型更具可扩展性和适应性的一种另一种方法。然而，它通常与微调方法相辅相成，而不是取代它们。
- en: Model Optimization for Resource-Constrained Devices
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源受限设备的模型优化
- en: Optimizing a model for resource-limited devices ensures that it runs smoothly
    on low-power hardware like mobile devices or edge devices.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为资源有限的设备优化模型确保它在低功耗硬件（如移动设备或边缘设备）上运行顺畅。
- en: '*Compression techniques* help reduce the computational and memory footprint
    of these models while maintaining their performance. This is particularly important
    for deploying LLMs on edge devices or optimizing their runtime in cloud environments.
    There are several techniques to compress LLMs. Let’s look into them one by one:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*压缩技术*有助于减少这些模型的计算和内存占用，同时保持其性能。这对于在边缘设备上部署LLM或优化云环境中的运行时尤其重要。有几种技术可以压缩LLM。让我们逐一探讨：'
- en: Prompt caching
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 提示缓存
- en: Prompt caching involves storing previously computed responses for frequently
    occurring prompts. Instead of rerunning the entire model, the cached results are
    quickly retrieved and returned. This is particularly useful for scenarios where
    the same or similar prompts are repeatedly queried, such as with customer support
    chatbots or knowledge retrieval systems, and for accelerating inference by avoiding
    redundant computations or reducing costs in high-traffic systems.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 提示缓存涉及存储频繁出现的提示的先前计算出的响应。而不是重新运行整个模型，缓存的结果被快速检索并返回。这在需要重复查询相同或类似提示的场景中特别有用，例如客户支持聊天机器人或知识检索系统，并且通过避免冗余计算或减少高流量系统中的成本来加速推理。
- en: Key–value caching
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 键值缓存
- en: Key–value (KV) caching optimizes transformer-based LLMs by caching attention-related
    computations, especially in scenarios involving sequential or streaming input
    data. By storing the key (K) and value (V) tensors computed during forward passes,
    subsequent tokens can reuse these precomputed tensors, reducing redundancy in
    computation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 键值（KV）缓存通过缓存注意力相关的计算来优化基于transformer的LLM，特别是在涉及顺序或流输入数据的情况下。通过存储正向传递期间计算出的键（K）和值（V）张量，后续标记可以重用这些预先计算的张量，减少计算中的冗余。
- en: KV caching comes in handy when speeding up autoregressive generation (as with
    GPT-style models) or improving latency for generating long texts.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 当加速自回归生成（如GPT风格模型）或提高生成长文本的延迟时，KV缓存非常有用。
- en: Quantization
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 量化
- en: Quantization reduces the precision of the model’s weights, such as from 32-bit
    floating point to 8-bit or even 4-bit. This drastically reduces memory requirements
    and speeds up inference without a substantial loss in model accuracy. There are
    different types of quantization techniques. In  *static quantization*, weights
    and activations are quantized before runtime. In  *dynamic quantization*, activations
    are quantized on the fly during runtime. Finally, with *quantization-aware training*
    (QAT), the model is trained to account for quantization effects, leading to better
    accuracy after quantization.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 量化降低模型权重的精度，例如从 32 位浮点数到 8 位甚至 4 位。这极大地减少了内存需求并加快了推理速度，同时不会在模型精度上造成重大损失。存在不同类型的量化技术。在*静态量化*中，权重和激活在运行前进行量化。在*动态量化*中，激活在运行时即时量化。最后，通过*量化感知训练*（QAT），模型被训练以考虑量化效应，从而在量化后获得更好的精度。
- en: Pruning
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝
- en: Pruning removes less significant weights, neurons, or entire layers from the
    model, reducing its size and computational complexity. *Structured pruning* removes
    components systematically (like neurons in a layer), while *unstructured pruning*
    removes individual weights based on importance.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝从模型中移除不太重要的权重、神经元或整个层，从而减小其大小和计算复杂性。*结构化剪枝*系统地移除组件（如层中的神经元），而*非结构化剪枝*根据重要性移除单个权重。
- en: Distillation
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏
- en: Model distillation trains a smaller “student” model to replicate the behavior
    of a larger “teacher” model. The student model learns by mimicking the teacher’s
    outputs, including logits or intermediate ​layer representations.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 模型蒸馏训练一个较小的“学生”模型来复制较大的“教师”模型的行为。学生模型通过模仿教师的输出进行学习，包括 logits 或中间层表示。
- en: Lessons for Effective LLM Development
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有效大型语言模型开发的教训
- en: Training LLMs is a complex process requiring precise strategies to balance efficiency,
    cost, and performance. Best practices in this space keep evolving. This section
    looks at some optimizations we haven’t covered yet, including scaling laws, model
    size versus data trade-offs, learning-rate optimizations, pitfalls like overtraining,
    and innovative techniques like speculative sampling.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型语言模型是一个复杂的过程，需要精确的策略来平衡效率、成本和性能。这个领域的最佳实践不断演变。本节探讨了尚未涵盖的一些优化方法，包括规模定律、模型大小与数据权衡、学习率优化、过度训练等陷阱以及像投机采样这样的创新技术。
- en: Scaling Law
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规模定律
- en: Scaling laws ([Example 5-11](#ch05_example_10_1748896666808859)) describe how
    model performance improves with increases in data, model size, or compute. Research
    has shown that performance gains often follow a predictable curve, with diminishing
    returns beyond certain thresholds. The balance lies in optimizing the interplay
    between model size and training data. Doubling both the model size and the training
    dataset typically results in better performance than doubling only one. It’s also
    important to know that models can become undertrained or overparameterized if
    the data isn’t scaled appropriately.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 规模定律([示例 5-11](#ch05_example_10_1748896666808859))描述了模型性能如何随着数据、模型大小或计算的增大而提高。研究表明，性能提升通常遵循可预测的曲线，超过一定阈值后收益递减。平衡点在于优化模型大小和训练数据之间的相互作用。通常，同时加倍模型大小和训练数据集比仅加倍其中之一能带来更好的性能。还重要的是要知道，如果数据未适当缩放，模型可能会变得欠训练或过参数化。
- en: Example 5-11\. Scaling law
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-11\. 规模定律
- en: '[PRE24]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Chinchilla Models
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 灵猫模型
- en: Chinchilla models ([Example 5-12](#ch05_example_11_1748896666808874)) challenge
    the paradigm of building increasingly larger models. Instead, they prioritize
    training on more data while keeping the model size fixed. This approach achieves
    comparable or even better performance at lower costs. For a fixed compute budget,
    smaller models trained on larger datasets outperform larger models trained on
    limited data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 灵猫模型([示例 5-12](#ch05_example_11_1748896666808874))挑战了构建越来越大的模型的传统模式。相反，它们优先考虑在更多数据上训练，同时保持模型大小不变。这种方法在较低的成本下实现了可比较甚至更好的性能。对于固定的计算预算，在大型数据集上训练的较小模型优于在有限数据上训练的大型模型。
- en: Example 5-12\. Chinchilla model
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-12\. 灵猫模型
- en: '[PRE25]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Learning-Rate Optimization
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率优化
- en: Choosing the right learning rate is critical for effective training. An optimal
    learning rate allows models to converge faster and avoid pitfalls like vanishing
    gradients or oscillations. Gradually increase the learning rate at the start of
    training to stabilize convergence. Then, smoothly reduce the learning rate over
    time for better final convergence. To do this, try running the code in [Example 5-13](#ch05_example_12_1748896666808889).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的学习率对于有效的训练至关重要。一个最优的学习率可以使模型更快收敛并避免诸如梯度消失或振荡等陷阱。在训练开始时逐渐增加学习率以稳定收敛。然后，随着时间的推移平滑地降低学习率以获得更好的最终收敛。为此，尝试运行[示例
    5-13](#ch05_example_12_1748896666808889)中的代码。
- en: Example 5-13\. Optimizing the learning rate
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-13\. 优化学习率
- en: '[PRE26]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*Overtraining* occurs when a model becomes too well adapted to the training
    dataset and thus overly specialized, leading to poor generalization on unseen
    data. If you find that validation loss increases while training loss decreases,
    or if your model’s predictions on test data are overly confident but incorrect,
    the model may be overtrained. Early stopping and other regularization techniques
    help mitigate this. *Regularization* involves adding a penalty term to the model’s
    loss function to discourage it from learning overly complex relationships with
    the training data. With *early stopping* ([Example 5-14](#ch05_example_13_1748896666808902)),
    a performance metric (like accuracy or loss) is monitored on a validation set,
    and the training is halted when this metric plateaus or deteriorates.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*过拟合*发生在模型对训练数据集过度适应，因此过度专业化的情况下，导致在未见过的数据上泛化能力差。如果你发现验证损失增加而训练损失减少，或者如果你的模型在测试数据上的预测过于自信但错误，那么模型可能已经过拟合。提前停止和其他正则化技术有助于减轻这种情况。*正则化*涉及向模型的损失函数添加惩罚项，以阻止其从训练数据中学习过于复杂的关系。在*提前停止*([示例
    5-14](#ch05_example_13_1748896666808902))中，一个性能指标（如准确率或损失）在验证集上被监控，当这个指标停滞或恶化时，训练就会停止。'
- en: Example 5-14\. Early stopping
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-14\. 提前停止
- en: '[PRE27]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Speculative Sampling
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推测采样
- en: '*Speculative sampling* is a method to speed up autoregressive decoding during
    inference. It involves using a smaller, faster model to predict multiple token
    candidates, which are then verified by the larger model. This can be really useful
    for applications requiring low-latency generation, like real-time conversational
    agents.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*推测采样*是一种在推理过程中加速自回归解码的方法。它涉及使用一个更小、更快的模型来预测多个标记候选者，然后由更大的模型进行验证。这对于需要低延迟生成的应用，如实时对话代理，非常有用。'
- en: Understanding different training strategies and pitfalls is important for optimizing
    LLMs. Techniques like scaling laws and Chinchilla models guide compute-efficient
    training, while learning-rate optimization and speculative sampling improve both
    training and inference dynamics. Also, avoiding overtraining ensures that models
    generalize well to real-world data. Incorporating these lessons will lead to more
    robust and cost-effective LLMs.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 理解不同的训练策略和陷阱对于优化大型语言模型（LLMs）至关重要。像缩放定律和Chinchilla模型这样的技术指导着计算高效的训练，而学习率优化和推测采样则改善了训练和推理动态。此外，避免过拟合确保模型能够很好地泛化到真实世界数据。将这些经验教训融入其中将导致更稳健且成本效益更高的LLMs。
- en: Conclusion
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this chapter, you learned about critical aspects of optimizing the deployment
    of LLMs. From understanding the methods of domain adaptation like prompt engineering,
    fine-tuning, and retrieval-augmented generation (RAG) to exploring efficient model
    deployment strategies, the chapter covered the foundational knowledge you need
    to adapt LLMs for specific tasks and resource constraints. Each method has unique
    strengths, allowing developers to align the model’s behavior, knowledge, or outputs
    with organizational needs and technical limitations. We know historically that
    there will be naming and renaming of a lot terms and techniques in AI/ML. By the
    time this book comes out, you may hear terms like context engineering, the fundamentals
    of which we have already covered in this book. Regardless, the term you use doesn’t
    matter for engineering LLMs as long as you build for the key goals of LLMOps systems:
    reliability, scalability, robustness, and security.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了优化 LLM 部署的关键方面。从理解领域自适应的方法，如提示工程、微调和检索增强生成（RAG），到探索高效的模型部署策略，本章涵盖了适应特定任务和资源限制所需的坚实基础知识。每种方法都有其独特的优势，允许开发者将模型的行为、知识或输出与组织需求和技术限制相一致。我们知道在
    AI/ML 历史上，许多术语和技术都会经历命名和重命名。到这本书出版时，你可能会听到像上下文工程这样的术语，其基本原理我们已经在本书中进行了介绍。无论如何，你使用的术语并不重要，只要你在构建
    LLMOps 系统的关键目标：可靠性、可扩展性、鲁棒性和安全性时，为这些目标而努力即可。
- en: The chapter also examined how to optimize LLMs for resource-constrained environments
    through techniques such as quantization, pruning, and distillation, with an emphasis
    on the importance of balancing computational cost with performance.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还探讨了如何通过量化、剪枝和蒸馏等技术优化资源受限环境中的大型语言模型（LLM），并强调了在计算成本与性能之间平衡的重要性。
- en: References
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Karpathy, Andrej. [“Let’s Build GPT: From Scratch, in Code, Spelled Out”](https://oreil.ly/PfnyZ),
    YouTube, January 17, 2023.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Karpathy, Andrej. [“从零开始，用代码详细解析构建 GPT”](https://oreil.ly/PfnyZ), YouTube, 2023年1月17日。
- en: Kimothi, Abhinav. [“3 LLM Architectures”](https://oreil.ly/A-C1L), *Medium*,
    July 24, 2023.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Kimothi, Abhinav. [“3 种 LLM 架构”](https://oreil.ly/A-C1L), *Medium*, 2023年7月24日。
- en: Microsoft. [“Introduction to Semantic Kernel”](https://oreil.ly/xrjkE), June
    24, 2024.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft. [“语义内核简介”](https://oreil.ly/xrjkE), 2024年6月24日。
- en: 'Wang, Zian (Andy). [“Mixture of Experts: How an Ensemble of AI Models Decide
    As One”](https://oreil.ly/stFQa), Deepgram, June 27, 2024.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Wang, Zian (Andy). [“专家混合：如何使一组 AI 模型协同工作”](https://oreil.ly/stFQa), Deepgram,
    2024年6月27日。
