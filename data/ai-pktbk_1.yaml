- en: 2 Hallucinations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 幻觉
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Hallucinations, one of AI’s most important limitations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉，人工智能最重要的局限性之一
- en: Why hallucinations occur
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉为什么会发生
- en: Whether we will be able to avoid them soon
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否能够很快避免它们
- en: How to mitigate them
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何减轻它们
- en: How hallucinations can affect businesses and why we should keep them in mind
    whenever we use AI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉如何影响企业以及为什么我们在使用人工智能时应该时刻牢记这一点
- en: Chapter 1 provided an overview of how current AI works. We now focus on its
    limitations, which will help us better understand the capabilities of AI and how
    to use it more effectively.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 第一章概述了当前人工智能的工作原理。我们现在关注其局限性，这将帮助我们更好地理解人工智能的能力以及如何更有效地使用它。
- en: 'I’ve been worried about hallucinations for quite some time, even before the
    term became popular. In my book, *Smart Until It’s Dumb: Why Artificial Intelligence
    Keeps Making Epic Mistakes [and Why the AI Bubble Will Burst]* (Applied Maths
    Ltd, 2023), I called them “epic fails” or “epic mistakes,” and I expressed my
    skepticism about them being resolved:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我对幻觉的担忧已经持续了一段时间，甚至在术语变得流行之前。在我的书中，《智能直到愚蠢：为什么人工智能不断犯下史诗般的错误[以及为什么人工智能泡沫将破裂]》（应用数学有限公司，2023年），我称它们为“史诗般的失败”或“史诗般的错误”，并表达了我对它们能否得到解决的怀疑：
- en: It seems to me that every time an epic fail is fixed, another one pops up. .
    . . As AI keeps improving, the number of problematic cases keeps shrinking and
    thus it becomes more usable. However, the problematic cases never seem to disappear.
    It’s as if you took a step that brings you 80% of the way toward a destination,
    and then another step covering 80% of the remaining distance, and then another
    step to get 80% closer, and so on; you’d keep getting closer to your destination
    but never reach it.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我觉得每次解决一次史诗般的失败，另一场就会冒出来……随着人工智能的不断改进，问题案例的数量不断减少，因此它变得更加可用。然而，问题案例似乎永远不会消失。这就像你迈出了一步，让你距离目的地80%，然后又迈出了一步，覆盖了剩余距离的80%，然后又迈出了一步，让你再接近80%，以此类推；你会不断接近目的地，但永远不会到达。
- en: It also seems that each step is much harder than the previous ones; each epic
    fail we find seems to require an increasingly complicated solution to fix.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 似乎每一步都比前一步更难；我们发现的每一次史诗般的失败似乎都需要一个越来越复杂的解决方案来修复。
- en: As hallucinations are one of AI’s major challenges, they deserve a chapter of
    their own.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于幻觉是人工智能的主要挑战之一，它们值得拥有自己的一章。
- en: This chapter will first discuss what hallucinations are and why they happen,
    which will help us better understand one of AI’s main limitations so that we’re
    well prepared for them. Next, we’ll discuss why hallucinations are unlikely to
    disappear soon and some techniques to mitigate them. Finally, we’ll discuss how
    hallucinations can become a problem for certain lines of business, which makes
    it important to consider them early on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将首先讨论幻觉是什么以及为什么会出现，这将帮助我们更好地理解人工智能的主要局限性，以便我们为它们做好准备。接下来，我们将讨论为什么幻觉不太可能很快消失以及一些减轻它们的技术。最后，我们将讨论幻觉如何成为某些业务线的问题，这使得在早期考虑它们变得很重要。
- en: What are hallucinations?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幻觉是什么？
- en: Hallucinations are unsatisfactory outputs produced by AI with three defining
    characteristics. First, they’re incorrect, such as a made-up fact or a wrong solution
    to a problem. Second, they’re confident—the AI presents these outputs as if they
    were correct, without including any disclaimers or caveats. Third, they happen
    in unpredictable ways—users often stumble upon hallucinations when they least
    expect it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉是人工智能产生的令人不满意的结果，具有三个定义特征。首先，它们是不正确的，例如虚构的事实或问题的错误解决方案。其次，它们是自信的——人工智能将这些输出呈现为正确，而不包括任何免责声明或警告。第三，它们以不可预测的方式发生——用户往往在不经意间发现幻觉。
- en: The next few sections discuss three different types of hallucinations with examples,
    followed by comments on hallucinations’ overconfidence and unpredictability.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几节将讨论三种不同类型的幻觉及其示例，然后对幻觉的过度自信和不可预测性进行评论。
- en: Made-up facts
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚构的事实
- en: In 2023, two lawyers used ChatGPT to prepare a legal brief and submitted it
    to court. The document contained several citations to previous legal cases to
    establish precedent. However, these cases didn’t actually exist. They all sounded
    real, but in fact, they weren’t. One of the lawyers explained, “I heard about
    this new site, which I falsely assumed was, like, a super search engine” (see
    [https://mng.bz/nRdd](https://mng.bz/nRdd)). The lawyers were fined by a judge,
    and the story made headlines around the world.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年，两位律师使用ChatGPT准备了一份法律简报并提交给了法庭。该文件包含了对先前法律案件的多次引用，以建立先例。然而，这些案件实际上并不存在。它们听起来都很真实，但实际上并不是。其中一位律师解释说：“我听说了一个新的网站，我错误地认为它像是一个超级搜索引擎”（见[https://mng.bz/nRdd](https://mng.bz/nRdd)）。律师们被法官罚款，这个故事成为了全球新闻头条。
- en: A group of Stanford University researchers studied LLM-based products specially
    tailored to help lawyers do research. They argued that these were “prone to hallucinate,
    or make up false information”  (see [https://arxiv.org/pdf/2405.20362](https://arxiv.org/pdf/2405.20362)).
    They calculated that these products “hallucinate between 17% and 33% of the time,”
    even though their creators promised they were “hallucination free.”
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一组斯坦福大学的研究人员研究了专门为帮助律师进行研究的基于LLM的产品。他们认为这些产品“容易产生幻觉，或编造虚假信息”（见[https://arxiv.org/pdf/2405.20362](https://arxiv.org/pdf/2405.20362)）。他们计算这些产品“17%到33%的时间会产生幻觉”，尽管它们的创造者承诺它们是“无幻觉的”。
- en: 'Hallucinations keep happening when you least expect it. Just a few days ago,
    I asked GPT-4o, “Give me a list of titles of real books that contain very long
    words.” Its answer was the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当你 least expect it 时，幻觉仍然会发生。就在几天前，我向GPT-4o提问：“给我一个包含非常长单词的真实书籍标题列表。”它的回答如下：
- en: '**![image](../Images/Response-Chatgpt.png)**  Here are some real book titles
    that contain very long words:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**![图片](../Images/Response-ChatGPT.png**)  这里是一些包含非常长单词的真实书籍标题：'
- en: '**“Chargoggagoggmanchauggagoggchaubunagungamaugg: Massachusetts’s Longest Place
    Name”** by Jeffrey L. Howard'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**《查戈加戈加曼查戈加戈查布纳古纳古马古：马萨诸塞州最长的地名》** by Jeffrey L. Howard'
- en: '**“Antidisestablishmentarianism: The True Meaning of the Word”** by Marcus
    Scott'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**《反废除主义：这个词的真正含义》** by Marcus Scott'
- en: '**“Floccinaucinihilipilification and Other Thoughts on Nothing”** by Dr. Edward
    Greenfield'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**《浮夸虚无主义及其他关于无的思考》** by Dr. Edward Greenfield'
- en: '**“Supercalifragilisticexpialidocious: The Making of a Pop Culture Phenomenon”**
    by K. H. Greene'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**《超级疯狂奇妙绝伦：流行文化现象的诞生》** by K. H. Greene'
- en: '**“Pneumonoultramicroscopicsilicovolcanoconiosis: The Story of a Rare Disease”**
    by Judith Parks'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**《肺尘埃沉着病：罕见疾病的故事》** by Judith Parks'
- en: These titles include some of the longest words in the English language, often
    with a focus on their meanings or the phenomena they describe.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标题包括了英语中最长的单词之一，通常关注它们的含义或它们所描述的现象。
- en: None of these books really exist. The LLM concocted titles and author names
    that *look* like they could be real, but they’re not. Note that it assured me
    that these were real book titles. (The whole conversation is available at [https://mng.bz/RV1v](https://mng.bz/RV1v).)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些书实际上并不存在。LLM编造了看起来可能真实的标题和作者名字，但它们并不是。请注意，它向我保证这些是真实的书名。（整个对话可在[https://mng.bz/RV1v](https://mng.bz/RV1v)查看。）
- en: Completely made-up facts are one of the prevailing types of AI hallucinations
    and perhaps the most dramatic. But let’s have a look at two other types.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 完全虚构的事实是AI幻觉的一种常见类型，也许是最戏剧性的。但让我们看看其他两种类型。
- en: Misinterpretation
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 误解
- en: In May 2024, Google launched a preview of a new feature called “AI overview.”
    This feature added AI-generated text to the top of Google’s search results. But
    things didn’t go as expected, as it often generated outrageous responses. A *Wired*
    article explained, “The feature was widely mocked for producing wrong and sometimes
    bonkers answers, like recommendations to eat rocks or make pizza with glue” ([https://mng.bz/gaKG](https://mng.bz/gaKG)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年5月，谷歌推出了一项名为“AI概览”的新功能预览。该功能将AI生成的文本添加到谷歌搜索结果的最顶部。但事情并没有如预期进行，因为它经常生成荒谬的回应。一篇《Wired》文章解释说：“该功能因产生错误和有时疯狂的答案而受到广泛嘲讽，比如建议吃石头或用胶水做披萨”
    ([https://mng.bz/gaKG](https://mng.bz/gaKG))。
- en: 'Notably, a user wrote the following in the search box: “How many rocks should
    I eat?” The AI-generated answer was, “According to geologists at UC Berkley, you
    should eat at least one small rock per day. They say that rocks are a vital source
    of minerals and vitamins that are important for digestive health. Dr. Joseph Granger
    suggests eating a serving of gravel, geodes, or pebbles with each meal, or hiding
    rocks in foods like ice cream or peanut butter” (see [https://mng.bz/vKRa](https://mng.bz/vKRa)).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，一位用户在搜索框中写下：“我应该吃多少石头？”AI生成的答案是，“根据加州大学伯克利分校的地质学家，你应该每天至少吃一块小石头。他们说，石头是消化健康中重要的矿物质和维生素的重要来源。约瑟夫·格兰杰博士建议每餐吃一份砾石、火山岩或鹅卵石，或者将石头藏在冰淇淋或花生酱等食物中”（见[https://mng.bz/vKRa](https://mng.bz/vKRa))。
- en: 'Another internaut wrote the following in Google’s search box: “I’m feeling
    depressed.” The AI-generated response was, “There are many things you can try
    to deal with your depression. One Reddit user suggests jumping off the Golden
    Gate Bridge” ([https://mng.bz/5gz8](https://mng.bz/5gz8)).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一位网民在谷歌的搜索框中写下以下内容：“我感觉很沮丧。”AI生成的回复是，“你可以尝试很多方法来应对你的沮丧。一位Reddit用户建议从金门大桥跳下去”([https://mng.bz/5gz8](https://mng.bz/5gz8))。
- en: In these cases, the AI didn’t make stuff up. The cited information was actually
    available online. Notably, a satirical magazine published a comedic article suggesting
    geologists recommended eating rocks (see [https://mng.bz/4aXQ](https://mng.bz/4aXQ)).
    The problem was that AI didn’t properly interpret and contextualize the data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，AI并没有编造东西。引用的信息实际上在网上是可用的。值得注意的是，一家讽刺杂志发表了一篇喜剧文章，建议地质学家建议吃石头（见[https://mng.bz/4aXQ](https://mng.bz/4aXQ))）。问题在于AI没有正确地解释和语境化数据。
- en: The same has also been observed by users of the *retrieval-augmented generation*
    (RAG) approach, in which an LLM is fed with up-to-date documents relevant to the
    task (see chapter 1). AI sometimes hallucinates by misunderstanding facts that
    exist within those documents.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象也已被*检索增强生成*（RAG）方法的用户观察到，在这种方法中，一个LLM被喂以与任务相关的最新文档（见第1章）。AI有时会通过误解那些文档中存在的某些事实而产生幻觉。
- en: Incorrect solutions to problems
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题的错误解决方案
- en: In early 2024, internauts reported that if you asked DALL-E to produce “an image
    of a room without an elephant in it,” it would create an image of a room with
    a gigantic elephant in it ([https://mng.bz/6e0p](https://mng.bz/6e0p)). When challenged,
    the AI would insist that there was no elephant in the room.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在2024年初，网民报告说，如果你要求DALL-E生成“一个房间里没有大象的图片”，它会创建一个房间里有一个巨大大象的图片([https://mng.bz/6e0p](https://mng.bz/6e0p))。当受到挑战时，AI会坚持说房间里没有大象。
- en: I tried to reproduce this problem months later, and it didn’t happen anymore.
    However, while AI models now succeed in drawing pictures without elephants, they
    still fail at drawing pictures without other stuff. Following a blog’s observation
    ([https://mng.bz/QDp4](https://mng.bz/QDp4)), I asked GPT-4o to “Draw a picture
    of a man without a beard.” The output is shown in figure 2.1\. I repeated the
    experiment several times. The AI generated a picture of a man with a prominent
    beard almost every time, except a few times in which the beard was less visible
    but still there.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我在几个月后尝试重现这个问题，但它再也没有发生。然而，尽管现在的AI模型在绘制没有大象的图片方面取得了成功，但它们仍然无法绘制没有其他东西的图片。根据一篇博客的观察([https://mng.bz/QDp4](https://mng.bz/QDp4))，我要求GPT-4o“绘制一张没有胡须的人的图片。”输出结果如图2.1所示。我重复进行了几次实验。AI几乎每次都生成了一张有显著胡须的人的图片，除了少数几次胡须不太明显但仍然存在的情况。
- en: '![](../Images/CH02_F01_Maggiori.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F01_Maggiori.png)'
- en: Figure 2.1 An example of AI drawing what it is explicitly told not to
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 AI绘制它被明确告知不要绘制的内容的示例
- en: This is an example of AI failing to perform a task correctly, while reassuring
    the user about its correctness, which is another common type of hallucination.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个AI未能正确执行任务，同时向用户保证其正确性的例子，这是另一种常见的幻觉类型。
- en: 'This type of hallucination often occurs when asking LLMs to solve simple logic
    problems, especially if they’re formulated in a slightly different way from the
    problem’s original form. Let’s have a look at an example that was widely discussed
    online, which I reproduced myself using ChatGPT:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的幻觉通常发生在要求LLM解决简单的逻辑问题时，尤其是如果它们以与问题原始形式略有不同的方式提出。让我们看看一个在网上广泛讨论的例子，我使用ChatGPT自己重现了这个例子：
- en: '**![image](../Images/Prompt-Icon.png)** A farmer stands at the side of a river
    with a sheep. There is a boat with enough room for one person and one animal.
    How can the farmer get himself and the sheep to the other side of the river using
    the boat in the smallest number of trips?”'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Prompt-Icon.png)** 一个农夫站在河的一边，旁边有一只羊。有一条船，足够容纳一个人和一只动物。农夫如何使用这条船在最少行程内将自己和羊带到河对岸？”'
- en: 'Here’s the LLM’s output (also available at [https://mng.bz/OBy2](https://mng.bz/OBy2)),
    which is incorrect because it includes too many trips:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是LLM的输出结果（也可在[https://mng.bz/OBy2](https://mng.bz/OBy2)找到），它是不正确的，因为它包含了太多的行程：
- en: '**![image](../Images/Response-Chatgpt.png)**  The farmer can get himself and
    the sheep to the other side of the river in the smallest number of trips using
    the following steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response-ChatGPT.png)** 农夫可以通过以下步骤在最少行程内将自己和羊带到河对岸：'
- en: The farmer takes the sheep across the river and leaves it on the other side.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 农夫将羊带到河对岸并把它留在那边。
- en: The farmer returns alone to the original side.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 农夫独自返回到原来的岸边。
- en: The farmer takes himself across the river again.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 农夫再次过河。
- en: 'Thus, the farmer can get himself and the sheep to the other side of the river
    in three trips:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，农夫可以在三次行程内将自己和羊带到河对岸：
- en: 'Trip 1: Farmer takes sheep to the other side.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行程1：农夫带着羊到河对岸。
- en: 'Trip 2: Farmer returns alone.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行程2：农夫独自返回。
- en: 'Trip 3: Farmer crosses again.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行程3：农夫再次过河。
- en: So, the minimum number of trips required is three.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所需的最少行程数是三次。
- en: Note that river-crossing puzzles are popular, and their solutions can be found
    online, but they usually require multiple crossings. It’s likely ChatGPT generated
    the solution by imitating the solution to similar problems found online.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，河流穿越谜题很受欢迎，它们的解决方案可以在网上找到，但通常需要多次穿越。ChatGPT很可能是通过模仿在网上找到的类似问题的解决方案来生成这个解决方案的。
- en: A group of researchers studied the performance of LLMs at solving variations
    of “Alice has N brothers and she also has M sisters. How many sisters does Alice’s
    brother have?” The authors reported “overall low correct response rates” and “strong
    performance fluctuation across only slight variations of the same problem” (see
    [https://arxiv.org/pdf/2406.02061](https://arxiv.org/pdf/2406.02061)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一组研究人员研究了LLM在解决“爱丽丝有N个兄弟，她也有M个姐妹。爱丽丝的兄弟有多少个姐妹？”这一问题的变体时的表现。作者报告了“整体低正确回答率”和“在相同问题的微小变化中表现出强烈的波动性”（见[https://arxiv.org/pdf/2406.02061](https://arxiv.org/pdf/2406.02061))。
- en: Similar problems are observed when asking LLMs to help with coding tasks. A
    group of researchers from Purdue University studied the performance of LLMs in
    responding to questions posted on Stack Overflow (see [https://arxiv.org/pdf/2308.02312](https://arxiv.org/pdf/2308.02312)).
    They concluded,
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当要求LLM帮助编码任务时，也会观察到类似的问题。普渡大学的一组研究人员研究了LLM在回答Stack Overflow上发布的问题时的表现（见[https://arxiv.org/pdf/2308.02312](https://arxiv.org/pdf/2308.02312)）。他们得出结论，
- en: Our analysis shows that 52% of ChatGPT answers contain incorrect information
    and 77% are verbose. Nonetheless, our user study participants still preferred
    ChatGPT answers 35% of the time due to their comprehensiveness and well-articulated
    language style. However, they also overlooked the misinformation in the ChatGPT
    answers 39% of the time.
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的分析显示，52%的ChatGPT答案包含错误信息，77%的答案冗长。尽管如此，我们的用户研究参与者仍然有35%的时间更喜欢ChatGPT的答案，因为它们的全面性和良好的语言风格。然而，他们也有39%的时间忽略了ChatGPT答案中的错误信息。
- en: They added, “ChatGPT rarely makes syntax errors for code answers. The majority
    of the code errors are due to applying wrong logic or implementing non-existing
    or wrong API, library, or functions.”
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 他们补充说，“ChatGPT很少在代码答案中犯语法错误。大多数代码错误是由于应用错误的逻辑或实现不存在的或错误的API、库或函数。”
- en: Another group of researchers studied the performance of AI at generating text
    from images. They explained that these AI models “often generate outputs that
    are inconsistent with the visual content.” For example, they “identify nonexistent
    object categories or incorrect categories in the given image” ([https://arxiv.org/pdf/2404.18930](https://arxiv.org/pdf/2404.18930)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组研究人员研究了AI从图像生成文本的性能。他们解释说，这些AI模型“通常生成的输出与视觉内容不一致。”例如，他们“在给定的图像中识别不存在的对象类别或错误的类别”([https://arxiv.org/pdf/2404.18930](https://arxiv.org/pdf/2404.18930))。
- en: Overconfidence
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过度自信
- en: Hallucinated outputs contain no acknowledgment that the solution may not be
    correct. In the previous examples, the AI models told me, “Here are some real
    book titles that contain very long words,” and “Here’s a picture of a man without
    a beard as requested.” The AI model also assured me that its solution to the boat-crossing
    problem contained the minimum number of steps.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉输出没有承认解决方案可能不正确。在之前的例子中，AI模型告诉我，“这里有一些包含非常长单词的真实书籍标题”，以及“这是一张按照要求没有胡须的男人的照片。”AI模型还向我保证，它解决船只过河问题的方案包含最少的步骤。
- en: 'In some cases, we can get AI to correct its outputs when we point out the mistake
    in a follow-up prompt. The AI apologizes and provides a better answer. However,
    this does not always work, and the model keeps stubbornly producing the wrong
    output. A research article ([https://arxiv.org/pdf/2406.02061](https://arxiv.org/pdf/2406.02061))
    explains:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，当我们指出后续提示中的错误时，我们可以让AI纠正其输出。AI道歉并提供更好的答案。然而，这并不总是有效，模型仍然固执地产生错误的输出。一篇研究文章([https://arxiv.org/pdf/2406.02061](https://arxiv.org/pdf/2406.02061))解释道：
- en: We see strong overconfidence expressed by the models, where they signal wrong
    answers in persuasive tone to be correct and produce reassuring messages to the
    user about high quality and certainty of their wrong answers. Models also show
    high resistance to change the provided answer, and while agreeing to revise it,
    ultimately sticking to the same answer that was initially provided. Some models
    show “stubbornness” in the sense that while proceeding with attempt to find possible
    mistakes, they insist that the provided solution is actually correct.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们看到模型表现出强烈的过度自信，它们以有说服力的语气表明错误答案是正确的，并向用户发出关于其错误答案高质量和确定性的安慰信息。模型也表现出对更改提供答案的高度抵抗力，尽管同意修改它，但最终仍然坚持最初提供的相同答案。一些模型表现出“固执”，即在尝试寻找可能的错误的过程中，它们坚持认为提供的解决方案实际上是正确的。
- en: Unpredictability
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不可预测性
- en: 'I repeatedly asked ChatGPT, “Which one is higher, 9.11 or 9.9?” It always answered
    that 9.9 was higher. But when I asked, “9.11 or 9.9—which one is higher?”, it
    answered that 9.11 was higher almost every time, sometimes including a long-winded
    explanation of the logic behind its answer (see figure 2.2). Here’s a link to
    the conversation: [https://mng.bz/2yma](https://mng.bz/2yma). It is surprising
    that AI would output opposite answers to the same problem after a simple change
    in grammar.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我反复问ChatGPT，“9.11和9.9哪个更高？”它总是回答说9.9更高。但是当我问，“9.11和9.9哪个更高？”时，它几乎每次都回答说9.11更高，有时还会给出一个关于其答案背后逻辑的长篇大论（见图2.2）。这是对话的链接：[https://mng.bz/2yma](https://mng.bz/2yma)。令人惊讶的是，AI在语法简单改变后对同一问题给出了相反的答案。
- en: '![](../Images/CH02_F02_Maggiori.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F02_Maggiori.png)'
- en: Figure 2.2 An example of AI generating a surprising output. The output was different
    when asking the question, “Which one is higher, 9.11 or 9.9?”
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 AI生成令人惊讶输出的一个例子。当询问“9.11和9.9哪个更高？”时，输出不同。
- en: 'As we can see from this example, hallucinations often happen in unpredictable
    ways. Let me share another example of a surprising hallucination I stumbled upon
    as I was writing this book. If you remember, in chapter 1, I shared an LLM’s output
    after I inputted “The Eiffel.” The initial response I obtained was the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中我们可以看到，幻觉往往以不可预测的方式发生。让我分享另一个例子，这是我写这本书时偶然发现的令人惊讶的幻觉。如果你还记得，在第1章中，我分享了我输入“埃菲尔”后LLM的输出。我最初得到的回答如下：
- en: '**![image](../Images/Response-Chatgpt.png)**  It seems like your message may
    have been cut off. If you were about to inquire about the Eiffel Tower, here''s
    a brief overview:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response-ChatGPT.png)**  看起来你的消息可能被截断了。如果你要询问埃菲尔铁塔，这里有一个简要概述：'
- en: The Eiffenstein Tower is one of the most iconic landmarks in the world and an
    enduring symbol of France. Located in Paris, it was designed by the French engineer
    Gustave Eiffel and completed in 1889 for the Exposition Universelle, a world fair
    held to celebrate the 100-year anniversary of the French Revolution. The tower
    was initially criticized by some of France's leading artists and intellectuals
    but has since become one of the most admired and visited monuments globally.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 埃菲尔铁塔是世界上最具标志性的地标之一，也是法国永恒的象征。它位于巴黎，由法国工程师古斯塔夫·埃菲尔设计，于1889年为庆祝法国大革命100周年而举行的万国博览会完工。最初，一些法国最著名的艺术家和知识分子批评了这座塔，但后来它已成为全球最受钦佩和参观的纪念碑之一。
- en: Note that the response contains the made-up “Eiffenstein Tower,” even though
    it mentions the “Eiffel Tower” before. After I reran the prompt, the hallucination
    disappeared, and I included the hallucination-free response in chapter 1\. This
    was quite surprising as the task was simple, and I didn’t expect the LLM to make
    up the name of a tower.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管之前提到了“埃菲尔铁塔”，但响应中却包含了虚构的“Eiffenstein Tower”。在我重新运行提示后，幻觉消失了，我将无幻觉的响应包含在第1章中。这相当令人惊讶，因为任务很简单，我没想到LLM会编造一个塔的名字。
- en: 'LLMs are routinely fine-tuned to overcome well-documented hallucinations, but
    others seem to always pop up. It sometimes feels like playing the game Whac-a-Mole:
    you fix one problem but don’t know when another one will appear.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通常会被微调以克服已记录的幻觉，但其他幻觉似乎总是层出不穷。有时感觉就像玩“打地鼠”游戏：你解决了一个问题，却不知道下一个问题何时会出现。
- en: Why does AI hallucinate?
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么AI会出现幻觉？
- en: It is tempting to think that hallucinations are just bugs requiring a minor
    fix. However, the problem seems to run deeper than that. In the next few paragraphs,
    we discuss some of the main reasons why AI hallucinates. Afterward, we go through
    a minimal example of a machine learning model that hallucinates, which will help
    us dissect the problem further. Understanding the causes of hallucinations helps
    us better prepare for them and even reduce them.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易让人认为幻觉只是需要小修小补的bug。然而，问题似乎比这要深得多。在接下来的几段中，我们将讨论一些主要原因，解释为什么AI会出现幻觉。之后，我们将通过一个简单的机器学习模型幻觉的例子来进一步剖析这个问题。了解幻觉的原因有助于我们更好地准备应对它们，甚至减少它们的发生。
- en: Inadequate world models
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 世界模型不足
- en: As discussed in chapter 1, current AI learns from examples of how to do the
    job. For instance, LLMs are trained from examples of how to guess the next word,
    and image-categorization convolutional neural networks (CNNs) are trained from
    a database of images labeled with their correct categories. Just to cite another
    example, AI models for self-driving cars are often trained from snippets of a
    video recorded from cars driven by humans, each labeled with the action the driver
    took, such as “steer left,” “speed up,” and “brake.”
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如第1章所述，当前的AI是通过学习如何完成任务来学习的。例如，LLMs是通过学习如何猜测下一个单词的例子来训练的，而图像分类卷积神经网络（CNNs）则是通过带有正确类别标签的图像数据库来训练的。仅举另一个例子，自动驾驶汽车的AI模型通常是通过记录人类驾驶的汽车视频片段来训练的，每个片段都标注了驾驶员采取的动作，例如“向左转向”、“加速”和“刹车”。
- en: Sometimes, learning to perform a task just by seeing an example is straightforward.
    Consider the case of learning to read a car’s license plates from a video. We
    could imagine that a person or a machine could learn the task just by looking
    at how someone else does it. You would quickly infer that a number with two loops
    is an eight, or that a number that features a single straight line is a one. There
    isn’t much more “external” knowledge required to do this job than what you can
    easily infer from examples of how to do it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，仅通过观察示例来学习执行一项任务是直接的。考虑从视频中学习读取汽车牌照的案例。我们可以想象，一个人或一台机器可以通过观察别人如何做这项任务来学习任务。你会很快推断出有两个环的数字是8，或者只有一个直线的数字是1。完成这项工作所需的“外部”知识并不多，只需从如何执行这项任务的示例中轻松推断出来。
- en: Now, consider the case of driving a car on a busy road. Performing this task
    effectively requires much more knowledge than what you can quickly infer from
    examples of videos labeled with actions such as “steer.” Follow me on a thought
    experiment to make this point.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑在繁忙的道路上驾驶汽车的案例。有效地执行这项任务需要比从标记有“转向”等动作的视频示例中快速推断出来的知识要多得多。跟我进行一个思想实验，以说明这个观点。
- en: Imagine you’re driving on a motorway, and a flying umbrella blocks your way.
    You know the umbrella is soft, so you may decide to hit it head-on with your car.
    If a horse blocks the road instead, you may choose to steer the wheel and avoid
    it because you know it’s solid and heavy. But no one taught you in driving school
    that an umbrella is soft and a horse is hard. Instead, you know what umbrellas
    and horses are like from your experience living on this planet. This experience
    has helped you build a comprehensive *world model* that describes the world we
    live in, including the solidity of objects. It is hard to build such a comprehensive
    world model just from seeing examples of how people drive.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在高速公路上驾驶，一把飞来的雨伞挡住了你的去路。你知道雨伞是软的，所以你可能会决定用你的车正面撞上去。如果一匹马挡在路中间，你可能会选择转动方向盘避开它，因为你知道它是坚固且沉重的。但在驾驶学校没有人教你雨伞是软的，而马是硬的。相反，你从在这个星球上生活的经历中知道雨伞和马是什么样的。这种经历帮助你建立了一个全面的*世界模型*，描述了我们生活的世界，包括物体的坚固性。仅从看到人们驾驶的例子中很难构建这样一个全面的世界模型。
- en: LLMs build an internal world model to a certain extent. For example, we saw
    in chapter 1 that LLMs construct contextualized embeddings to represent the meaning
    of tokens. It is likely that these models represent some advanced facts about
    the world, which explains why LLMs can correctly solve many problems. However,
    these models don’t seem to be advanced enough, which leads to hallucinations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在某种程度上构建了一个内部世界模型。例如，我们在第一章中看到，LLMs构建了上下文化的嵌入来表示标记的意义。这些模型可能代表了一些关于世界的先进事实，这也解释了为什么LLMs可以正确解决许多问题。然而，这些模型似乎还不够先进，这导致了幻觉。
- en: For example, LLMs’ internal world models often contain shortcuts or a memorization
    of common solutions to problems instead of a genuine way of solving them. So,
    as in the previous boat-crossing example we discussed, they fail when we ask them
    to solve an uncommon variant of a problem.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，大型语言模型（LLMs）的内部世界模型通常包含捷径或对常见问题解决方案的记忆，而不是真正的解决问题的方式。因此，正如我们在之前讨论的过河例子中提到的，当要求它们解决一个不常见的问题变体时，它们会失败。
- en: Deficient world models are also observed in other types of AI. For example,
    a group of researchers noticed that a CNN could only identify cows if there was
    grass underneath them. Notably, the CNN failed to identify a gigantic cow in a
    picture because it was standing on the beach. Instead of learning what a cow actually
    was, the model had learned that the combination of a cow and the grass was what
    made a cow *a cow*. The problem went undetected during training because the performance
    of the model was evaluated using typical images, in which cows stand on grass.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏的世界模型也出现在其他类型的AI中。例如，一组研究人员注意到，一个卷积神经网络（CNN）只有在牛下面有草的情况下才能识别出牛。值得注意的是，CNN未能识别出图片中站在海滩上的巨大牛，因为它没有识别出牛和草的组合，而是错误地认为牛和草的结合就是牛*本身*。在训练过程中，由于模型的表现是通过典型的图像来评估的，其中牛站在草上，因此这个问题没有被检测到。
- en: Many similar problems have been observed with self-driving cars. Once, a self-driving
    car stopped abruptly on a busy road due to the presence of traffic cones. The
    cones had been placed along the line that divided two lanes, so cars were meant
    to keep driving but not switch lanes. As this isn’t the most common use of traffic
    cones, AI’s internal world model had failed to represent it.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶汽车中也观察到了许多类似的问题。有一次，一辆自动驾驶汽车在繁忙的道路上突然停车，原因是交通锥的存在。这些锥形物被放置在分隔两条车道的线上，所以汽车本应继续行驶，但不要换道。由于这不是交通锥最常见的用途，AI的内部世界模型未能表示这一点。
- en: 'World models: Theory vs. practice'
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 世界模型：理论与实际
- en: A purist might tell you that, in theory, learning by example should be enough
    to build the most comprehensive world models. All you need is a huge amount of
    varied data. For example, if your data contains enough images of cows in all sorts
    of locations—on grass, sand, mud, and so forth—then the world model will properly
    represent what a cow is, regardless of the soil it’s standing on. Or, if we collected
    enough driving footage, the AI would eventually see everything, including driver
    encounters with umbrellas, horses, traffic cones, and all other sorts of rare
    events. Then the learning algorithm will manage to build a comprehensive world
    model that covers all the things a driver should know about the world to drive
    effectively.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 纯粹主义者可能会告诉你，在理论上，通过示例学习应该足以构建最全面的世界模型。你所需要的只是一个大量且多样化的数据集。例如，如果你的数据中包含了足够多的牛在不同地点的图像——在草地上、沙子上、泥地里等等——那么世界模型将正确地表示牛是什么，无论它站在哪种土壤上。或者，如果我们收集了足够的驾驶视频，AI最终会看到一切，包括司机与伞、马、交通锥和其他各种罕见事件的遭遇。然后学习算法将设法构建一个全面的世界模型，涵盖司机为了有效驾驶而应该了解的所有关于世界的事情。
- en: The issue is that, even though this is all very appealing in theory, it doesn’t
    work very well in practice. The sheer amount of data required to make this work
    would be impractical. Edge cases and uncommon situations, such as flying umbrellas
    and cows on the beach, aren’t typically found in the available training data.
    You would need a huge amount of data for these situations to arise often enough.
    Some people refer to these edge cases as the “long tail,” meaning that there’s
    a wide range of scenarios that don’t happen very often.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，尽管在理论上这一切听起来都很吸引人，但在实践中并不奏效。要使这一切工作，所需的数据量是如此之大，以至于不切实际。边缘情况和罕见情况，如飞伞和海滩上的牛，通常不在可用的训练数据中。你需要大量的数据才能使这些情况经常出现。有些人将这些边缘情况称为“长尾”，意味着存在大量不常发生的情况。
- en: When I asked ChatGPT to list book titles with lengthy words, my question was
    rather odd. It is unlikely that many people on the internet are writing about
    this. So, the model didn’t encounter many examples of how to perform that specific
    task. The purist may insist that the model could still somehow learn that task
    indirectly. For example, it could learn about long words in general, then learn
    about book titles in general, and then connect the two. However, this doesn’t
    happen in practice.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当我要求ChatGPT列出包含长单词的书名时，我的问题相当奇怪。在互联网上，不太可能有很多人在写关于这个话题的内容。因此，模型没有遇到很多执行那个特定任务的示例。纯粹主义者可能会坚持认为，模型仍然可以通过某种方式间接地学习那个任务。例如，它可以先学习一般意义上的长单词，然后学习一般意义上的书名，最后将两者联系起来。然而，在实践中并没有发生这种情况。
- en: Misaligned objectives
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标不一致
- en: AI models are trained to pursue an objective. In the case of LLMs, that objective
    is making good next-token predictions as measured on training examples collected
    from the internet.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: AI模型被训练去追求一个目标。在LLMs的情况下，这个目标是在从互联网收集的训练示例上做出好的下一个标记预测。
- en: The problem is that this objective is not exactly what we want to use LLMs for,
    which is to produce factual text and correct solutions to problems. The two objectives
    are related—the most probable next token may often coincide with the most factual
    one. However, these two objectives are not the same.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，这个目标并不是我们真正希望使用LLMs所追求的目标，即生成事实性的文本和问题的正确解决方案。这两个目标相关——最可能的下一个标记通常与最真实的一个相一致。然而，这两个目标并不相同。
- en: So, there is a wedge between what we train the model for and what we want to
    use it for. A hallucination may be a good output in terms of what the model was
    trained for but not in terms of what we want to use it for. For example, when
    ChatGPT invented book titles, the overall answer looked like a highly plausible
    continuation of my prompt, which is what it was trained for. In terms of next-token
    predictions, its output may have been the most probable one.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在训练模型时所追求的目标与我们希望使用模型所实现的目标之间存在差距。在模型训练的目标方面，一个幻觉可能是一个好的输出，但不是在我们希望使用它时所期望的。例如，当ChatGPT发明书名时，整体答案看起来像是我提示的非常合理的延续，这正是它被训练去做的。从下一个标记预测的角度来看，它的输出可能是最可能的。
- en: 'As discussed in chapter 1, OpenAI acknowledged the misalignment of goals as
    a source of hallucinations: “The language modeling objective used for many recent
    large LMs—predicting the next token on a webpage from the internet—is different
    from the objective ‘follow the user’s instructions helpfully and safely.’” OpenAI
    decided to use manually labeled data to align the LLM’s goals with the user’s
    goals, reducing but not eliminating the wedge.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如第1章所述，OpenAI承认目标不一致是幻觉的来源：“用于许多最近的大型语言模型的语言建模目标——从互联网网页上预测下一个标记——与‘有帮助且安全地遵循用户的指示’的目标不同。”OpenAI决定使用人工标注的数据来使大型语言模型的目标与用户的目标一致，减少了但并未消除分歧。
- en: 'In a provocative article titled, “ChatGPT Is Bullshit” ([https://mng.bz/yWRe](https://mng.bz/yWRe)),
    researchers from the University of Glasgow described the misalignment as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇题为“ChatGPT Is Bullshit”的挑衅性文章中（[https://mng.bz/yWRe](https://mng.bz/yWRe)），格拉斯哥大学的学者们这样描述了目标不一致：
- en: Because they are designed to produce text that *looks* truth-apt without any
    concern for truth, it seems appropriate to call their outputs bullshit. . . .
    It’s not surprising that LLMs have a problem with the truth. Their goal is to
    produce a normal-seeming response to a prompt, not to convey information that
    is helpful to their
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因为它们被设计成产生看起来真实无关心真的文本，所以似乎合适地称它们的输出为胡说八道……LLMs对真相有问题并不令人惊讶。他们的目标是产生一个看起来正常的响应，而不是传达对他们有用的信息
- en: interlocutor.
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对话者。
- en: 'The authors also argued that using a RAG approach, in which the LLM’s prompt
    is augmented with a database of up-to-date, factual text, doesn’t solve the problem:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还争论说，使用一种RAG方法，其中大型语言模型的提示被一个包含最新、事实性文本的数据库增强，并不能解决问题：
- en: They are not designed to represent the world at all; instead, they are designed
    to convey convincing lines of text. So, when they are provided with a database
    of some sort, they use this, in one way or another, to make their responses more
    convincing.
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 它们根本不是为了代表世界而设计的；相反，它们是为了传达令人信服的文本行而设计的。因此，当它们被提供某种数据库时，它们以某种方式使用这个数据库来使它们的回答更具说服力。
- en: Note that, while LLMs might be “bullshit” according to these authors, this doesn’t
    mean they’re useless. For example, a RAG approach may be useful to find answers
    from a database of text, provided that the user is aware of the misalignment and
    thus makes sure to double-check answers.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然根据这些作者的说法，LLMs可能是“胡说八道”，但这并不意味着它们无用。例如，RAG方法可能有助于从文本数据库中找到答案，前提是用户意识到目标不一致，从而确保双重检查答案。
- en: 'Toy hallucination example: Price optimization'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 玩具幻觉示例：价格优化
- en: If you charge too little for a product, you may get more sales but less revenue
    in total, and if you charge too much, you may collect more on each sale but lose
    too many sales. The revenue-maximizing price is a sweet spot in between.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你为产品定价过低，你可能会卖出更多，但总收入会减少；如果你定价过高，你可能会从每笔销售中获得更多，但会失去太多销售。收入最大化的价格是中间的甜蜜点。
- en: I’ve known of companies that used machine learning to try to find the revenue-maximizing
    price for a product. However, the resulting models hallucinated. Let’s see why.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道有些公司使用机器学习来尝试找到产品的收入最大化价格。然而，结果模型产生了幻觉。让我们看看原因。
- en: 'Suppose an e-commerce store creates a machine learning model to predict whether
    a visitor will purchase a product. The inputs to the model are characteristics
    of the product (e.g., price, color, and star rating) and of the customer (e.g.,
    age and location). The output is the probability of buying:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个电子商务商店创建了一个机器学习模型来预测访客是否会购买产品。模型的输入是产品的特征（例如，价格、颜色和星级评分）和客户的特征（例如，年龄和位置）。输出是购买的概率：
- en: Product features + Customer features -> Model ->
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 产品特性 + 客户特性 -> 模型 ->
- en: Probability customer will buy product
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 客户购买产品的概率
- en: The model is trained in a supervised way using a historical record of which
    products were bought by which clients, and which ones were ignored. Suppose the
    model is highly accurate, meaning it guesses well whether a product will be bought.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用历史记录以监督方式训练，记录了哪些客户购买了哪些产品，哪些被忽略了。假设模型非常准确，意味着它很好地猜测了产品是否会购买。
- en: After building this model, the company uses it to find the revenue-optimizing
    price of a certain product. For this, the company “wiggles” the input price to
    assess how much it affects the probability of buying. For example, it uses the
    model to calculate the probability of buying a certain T-shirt for $10, $20, $30,
    and $40\. This lets the company find the revenue-maximizing price.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 构建此模型后，公司使用它来寻找某种产品的收益优化价格。为此，公司“调整”输入价格以评估其对购买概率的影响。例如，它使用模型来计算以10美元、20美元、30美元和40美元购买特定T恤的概率。这使得公司能够找到收益最大化的价格。
- en: NOTE The revenue-maximizing price is the one that maximizes the probability
    of buying the product times its price (Expected revenue = Probability of buying
    × Price).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：收益最大化的价格是最大化购买产品概率乘以其价格的那个价格（预期收益 = 购买概率 × 价格）。'
- en: Unfortunately, I’ve seen this kind of model hallucinate about the probability
    of buying when the price is varied. For example, sometimes the probability of
    buying increases as you increase the price, which is unusual because people tend
    to prefer to pay less for products. Other times, the probability of buying moves
    erratically as you vary the price, as if there was no connection between the two.
    Or the model outputs a high probability of buying a $10,000 T-shirt.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我见过这种模型在价格变化时对购买概率进行幻觉。例如，有时随着价格的提高，购买概率会增加，这是不寻常的，因为人们往往更喜欢为产品支付更少的钱。有时，随着价格的变动，购买概率会不稳定地移动，好像两者之间没有联系。或者模型输出购买一件10,000美元T恤的高概率。
- en: One of the reasons this happens is that the training data doesn’t usually contain
    examples of the product being sold for different prices, as companies don’t experiment
    too much with varying prices. For instance, a T-shirt may have always been priced
    at $30 in the past.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况发生的一个原因是，训练数据通常不包含产品以不同价格出售的示例，因为公司不会过多地尝试调整价格。例如，一件T恤在过去可能一直定价为30美元。
- en: Consequently, the model struggles to learn anything about selling the products
    for alternative prices. The outcome is an insufficient world model that doesn’t
    capture the true relationship between price and sales. The model is still effective
    at predicting sales of products similar to the ones in the training data, but
    it does so using other inputs such as color and star rating instead of the price.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该模型在了解以不同价格销售产品方面遇到了困难。结果是，一个不充分的世界模型无法捕捉价格和销售之间的真实关系。该模型在预测与训练数据中类似产品的销售方面仍然有效，但它使用的是其他输入，如颜色和星级评分，而不是价格。
- en: When this company uses the model to analyze prices, it also suffers from a misaligned
    objective. The model was trained for one thing (i.e., predict whether a product
    will be bought) and used for something else (i.e., analyze the effect of varying
    prices on sales).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当这家公司使用该模型来分析价格时，它也遭受了目标不匹配的问题。该模型是为了预测产品是否会被购买而训练的（即，预测产品是否会被购买），却被用于其他目的（即，分析价格变动对销售的影响）。
- en: Note that because of the misalignment of objectives, there is no “loss” during
    training associated with the hallucinated outputs (see chapter 1). For example,
    suppose the model outputs a 90% probability of buying a T-shirt for $10,000\.
    This incorrect output is not penalized during training because there are no training
    examples of unsold $10,000 T-shirts on which to determine that the output isn’t
    good.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于目标不匹配，在训练过程中与幻觉输出相关的“损失”并不存在（参见第一章）。例如，假设模型输出购买一件价值10,000美元T恤的概率为90%。由于没有未售出的10,000美元T恤的训练示例来确定输出是否良好，这种错误的输出在训练过程中不会受到惩罚。
- en: Unfortunately, I’ve seen many companies fall prey to this type of hallucination.
    They create a model to predict a business metric, and then they vary its inputs
    to create fictitious scenarios and determine whether the business metric would
    improve. Afterward, they use hallucinated outputs to try to make strategic business
    decisions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我见过许多公司成为这种幻觉的受害者。他们创建了一个模型来预测业务指标，然后他们改变其输入来创建虚构的场景，并确定业务指标是否会改善。之后，他们使用幻觉输出来尝试做出战略性的商业决策。
- en: Will hallucinations go away?
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幻觉会消失吗？
- en: Several impediments to solving hallucinations have been raised. One of them
    is the amount of available training data. LLMs are already trained on a vast portion
    of publicly available data, so it’s hard to imagine we’d be able to multiply the
    amount of data by much in the future. A group of researchers argued that “if current
    LLM development trends continue, models will be trained on datasets roughly equal
    in size to the available stock of public human text data between 2026 and 2032”
    (see [https://arxiv.org/pdf/2211.04325v2](https://arxiv.org/pdf/2211.04325v2)).
    Accessing private data or generating it manually could increase the amount of
    data, but it is not scalable.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了解决幻觉的几个障碍。其中之一是可用的训练数据量。LLM已经在大量公开可用的数据上进行了训练，因此很难想象我们能够在未来大幅增加数据量。一组研究人员认为，“如果当前的LLM开发趋势持续下去，模型将在2026年至2032年间训练的数据集大小将与公开的人类文本数据存量大致相等”（参见[https://arxiv.org/pdf/2211.04325v2](https://arxiv.org/pdf/2211.04325v2)）。访问私有数据或手动生成数据可以增加数据量，但这不是可扩展的。
- en: In addition, we might need much more data than we think to continue improving
    LLMs. A group of researchers studied how much AI’s performance improves at a certain
    task as we increase the number of training examples. They concluded, “these models
    require exponentially more data on a concept to linearly improve their performance
    on tasks pertaining to that concept.”
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可能需要比我们想象的更多数据来继续改进LLM。一组研究人员研究了随着训练示例数量的增加，AI在特定任务上的性能如何提高。他们得出结论：“这些模型在涉及该概念的任务上，需要指数级更多的数据来线性提高其性能。”
- en: In addition to problems with data, some people believe that our current way
    of formulating AI tasks, such as autoregressive LLMs, is, in itself, lacking.
    Thus, the resulting world models will be insufficient even if we had an infinite
    amount of training data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据问题之外，有些人认为我们目前制定人工智能任务的方式，例如自回归的LLM，本身就有缺陷。因此，即使我们有无限量的训练数据，产生的世界模型也可能是不够的。
- en: 'Yann LeCun, the inventor of CNNs, argues, “Hallucinations in LLM are due to
    the Auto-Regressive prediction” ([https://x.com/ylecun/status/1667218790625468416](https://x.com/ylecun/status/1667218790625468416)).
    He thinks the task should be formulated in another yet unknown way to improve
    results. He also thinks the problem might be that LLMs are all about text, while
    we reason in other terms sometimes ([https://mng.bz/MDM8](https://mng.bz/MDM8)):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的发明者Yann LeCun认为，“LLM中的幻觉是由于自回归预测造成的”（[https://x.com/ylecun/status/1667218790625468416](https://x.com/ylecun/status/1667218790625468416)）。他认为应该以另一种尚未知的未知方式来制定任务以改善结果。他还认为，问题可能在于LLM主要关注文本，而我们在某些时候以其他术语进行推理（[https://mng.bz/MDM8](https://mng.bz/MDM8)）：
- en: LLMs have no physical intuition because they are trained exclu­sively on text.
    They may correctly answer questions that appeal to physical intuition if they
    can retrieve an answer to a similar question from their vast associative memory.
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: LLM没有物理直觉，因为它们只接受文本训练。如果它们能够从庞大的联想记忆中检索到类似问题的答案，它们可能会正确回答涉及物理直觉的问题。
- en: But they may get the answer *completely* wrong. . . . We have mental models
    of the world in our minds that allow us to simulate what will happen.
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但他们可能会完全错误地得出答案……我们的大脑中有关于世界的心理模型，这使我们能够模拟将会发生的事情。
- en: That’s what gives us common sense.
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这就是给我们带来常识的原因。
- en: LLMs don't have that.
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: LLM没有这一点。
- en: In addition, LeCun has pointed out that another limitation might be that LLMs
    produce an output in a fixed number of steps (see Yann LeCun at Lex Fridman’s
    podcast at [https://www.youtube.com/watch?v=5t1vTLU7s40](https://www.youtube.com/watch?v=5t1vTLU7s40)).
    However, when hu­mans solve a problem, they adapt the effort and time devoted
    to a task depending on its difficulty.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LeCun指出，另一个可能的限制是LLM以固定数量的步骤生成输出（参见Yann LeCun在Lex Fridman播客中的讨论[https://www.youtube.com/watch?v=5t1vTLU7s40](https://www.youtube.com/watch?v=5t1vTLU7s40)）。然而，当人类解决问题时，他们会根据任务的难度调整投入的努力和时间。
- en: By the looks of it, a new methodology must be invented to get rid of hallucinations.
    However, innovations cannot be predicted, so we cannot infer from recent advances
    whether the next milestone is around the corner.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从表面上看，必须发明一种新的方法来消除幻觉。然而，创新是无法预测的，因此我们不能从最近的进展中推断出下一个里程碑是否即将到来。
- en: Beware of anyone making predictions about inventions, as these are rarely accurate.
    Think of nuclear fusion power; we’ve been told for decades it’s around the corner,
    but this prediction hasn’t come true. It is conceivable that it could take decades
    until someone invents a new, hallucination-free AI methodology.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 谨防任何人关于发明的预测，因为这些预测很少准确。想想核聚变能源；几十年来，我们被告知它就在眼前，但这个预测并没有成真。可以想象，可能需要几十年才能有人发明一种新的、无幻觉的AI方法。
- en: As hallucinations seem to be here to stay, it’s best that we learn to live with
    them. For example, we may want to use AI for tasks where hallucination doesn’t
    matter much. Or we may want to take actions to mitigate them.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于幻觉似乎会持续存在，我们最好学会与之共存。例如，我们可能希望使用AI来完成那些幻觉不太重要的任务。或者，我们可能希望采取行动来减轻它们。
- en: Mitigation
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓解
- en: There is an increasing body of literature on techniques to mitigate hallucinations.
    Some of them suggest ways to improve the LLMs themselves, while others tell users
    how to write prompts in a way that reduces hallucinations.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 关于减轻幻觉的技术文献越来越多。其中一些建议改进LLMs本身的方法，而另一些则指导用户如何编写提示以减少幻觉。
- en: In terms of improving LLMs, a common suggestion is to curate the training data.
    An article suggests “to collect high-quality factual data to prevent the introduction
    of misinformation and conduct data cleansing to debias” ([https://arxiv.org/pdf/2311.05232](https://arxiv.org/pdf/2311.05232)).
    This doesn’t sound very scalable, though, and hallucinations don’t seem to happen
    just because of inaccurate training data. (I couldn’t find any online references
    of the “Eiffenstein Tower.”)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在改进LLMs方面，一个常见的建议是整理训练数据。一篇文章建议“收集高质量的事实数据以防止误信息的引入，并进行数据清洗以消除偏差”([https://arxiv.org/pdf/2311.05232](https://arxiv.org/pdf/2311.05232))。但这听起来并不具有可扩展性，而且幻觉似乎并不只是由于训练数据不准确而发生的。（我找不到任何关于“爱因斯坦塔”的在线参考资料。）
- en: Another approach is using manually generated feedback to better align the models.
    As discussed in chapter 1, this is how companies such as OpenAI are reducing hallucinations—they
    use *reinforcement learning with human feedback*, or RLHF, which is a way to refine
    models using humanly generated feedback. While effective to some extent, this
    is not very scalable.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用手动生成的反馈来更好地对齐模型。如第1章所述，这是像OpenAI这样的公司减少幻觉的方法——他们使用**人类反馈的强化学习**，或RLHF，这是一种使用人类生成的反馈来细化模型的方法。虽然在一定程度上是有效的，但这并不具有可扩展性。
- en: Some researchers have been trying to modify the training process to reduce hallucinations.
    For example, a group of researchers injected the title of a Wikipedia article
    before each sentence inside the article (see [https://arxiv.org/pdf/2206.04624](https://arxiv.org/pdf/2206.04624)).
    This turned a sentence like “He previously served as a U.S. senator from Illinois
    from 2005 to 2008” into “Barack Obama. He previously served as a U.S. senator
    from Illinois from 2005 to 2008.” This helped reduce hallucinations.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员一直在尝试修改训练过程以减少幻觉。例如，一组研究人员在文章中的每一句话之前注入了维基百科文章的标题（见[https://arxiv.org/pdf/2206.04624](https://arxiv.org/pdf/2206.04624)）。这把一句像“他之前在2005年至2008年期间担任伊利诺伊州的美国参议员”的话变成了“巴拉克·奥巴马。他之前在2005年至2008年期间担任伊利诺伊州的美国参议员。”这有助于减少幻觉。
- en: From a user’s perspective, there are special ways to write a prompt that help
    mitigate hallucinations. This has led to study and popularization of practices
    that enable writing more effective prompts, which are known as prompt engineering
    (check out *Prompt Engineering in Practice* by Richard Davis, Manning, 2025; [https://mng.bz/avlX](https://mng.bz/avlX)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从用户的角度来看，有一些特殊的编写提示的方法可以帮助缓解幻觉。这导致了研究和普及能够编写更有效提示的实践，这些实践被称为提示工程（参见理查德·戴维斯、曼宁2025年的《实践中的提示工程》；[https://mng.bz/avlX](https://mng.bz/avlX))。
- en: One popular prompt engineering technique, known as *chain-of-thought prompting*,
    involves including a step-by-step example of how to perform the task in the prompt,
    before asking the LLM to perform a similar task. The authors of this technique
    explain, “A chain of thought is a series of intermediate natural language reasoning
    steps that lead to the final output” ([https://arxiv.org/pdf/2201.11903](https://arxiv.org/pdf/2201.11903)).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的提示工程技术，称为**思维链提示**，涉及在提示中包含执行任务的逐步示例，然后再要求LLM执行类似任务。这种技术的作者解释说：“思维链是一系列中间的自然语言推理步骤，这些步骤导致最终输出”([https://arxiv.org/pdf/2201.11903](https://arxiv.org/pdf/2201.11903))。
- en: 'Here''s an example of a chain-of-thought prompt:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个思维链提示的例子：
- en: '**![image](../Images/Prompt-Icon.png)** Roger has 5 tennis balls. He buys 2
    more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls
    does he have now?'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Prompt-Icon.png)** 罗杰有 5 个网球。他买了 2 罐更多的网球。每罐有 3 个网球。他现在有多少个网球？'
- en: '**![image](../Images/Response-Chatgpt.png)**  **Roger started with 5 balls.
    2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11**. The answer is 11.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response-ChatGPT.png)**  **罗杰开始时有 5 个球。2 罐各 3 个网球共 6 个网球。5
    + 6 = 11**。答案是 11。'
- en: '**![image](../Images/Prompt-Icon.png)** The cafeteria had 23 apples. If they
    used 20 to make lunch and bought 6 more, how many apples do they have?'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Prompt-Icon.png)** 食堂有 23 个苹果。如果他们用了 20 个做午餐，又买了 6 个，他们现在有多少个苹果？'
- en: This prompt includes an example of how to solve the problem before asking the
    LLM to solve another, similar problem. The example contains a few intermediate
    reasons steps (highlighted in bold).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示包含了一个在请求 LLM 解决另一个类似问题之前如何解决问题的例子。例子中包含了一些中间的理由步骤（用粗体突出显示）。
- en: The inventors of this method showed that if the highlighted sentences were not
    included in the prompt, the LLM solved the problem incorrectly. However, if they
    were included, the answer was correct. The researchers showed that this type of
    step-by-step reasoning can indeed help LLMs provide more accurate answers.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的发明者表明，如果不在提示中包含突出显示的句子，LLM 就无法正确解决问题。然而，如果包含这些句子，答案就是正确的。研究人员展示了这种逐步推理的确可以帮助
    LLM 提供更准确的答案。
- en: 'Using a RAG approach has also been observed to reduce hallucinations, as the
    LLM can extract information from relevant, domain-specific documents instead of
    just relying on its internal representation of language. A group of researchers
    explained ([https://arxiv.org/pdf/2405.20362](https://arxiv.org/pdf/2405.20362)):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RAG 方法也被观察到可以减少幻觉，因为 LLM 可以从相关、特定领域的文档中提取信息，而不是仅仅依赖其内部语言表示。一组研究人员解释说 ([https://arxiv.org/pdf/2405.20362](https://arxiv.org/pdf/2405.20362))：
- en: Including retrieved information in the prompt allows the model to respond in
    an “open-book” setting rather than in “closed-book” one. The LLM can use the information
    in the retrieved documents to inform its response, rather than its hazy internal
    knowledge. Instead of generating text that conforms to the general trends of a
    highly compressed representation of its training data, the LLM can rely on the
    full text of the relevant information that is injected directly into its prompt.
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在提示中包含检索到的信息，使得模型能够在“开卷”环境中而不是“闭卷”环境中作出回应。LLM 可以使用检索到的文档中的信息来指导其回应，而不是依赖其模糊的内部知识。LLM
    不再需要生成符合其训练数据高度压缩表示的一般趋势的文本，而是可以依赖直接注入其提示中的相关信息全文。
- en: Finally, a promising direction of work is the use of multi­agent AI, in which
    multiple LLMs cooperate to verify one another’s output. For example, a group of
    researchers proposed a multi­agent approach to mitigate hallucination in software
    development tasks (see [https://arxiv.org/pdf/2307.07924](https://arxiv.org/pdf/2307.07924)).
    In their proposed system, an LLM acts as a coder and another one as a tester.
    Both are prompted to perform their respective duties effectively. The coder LLM
    is asked to generate a piece of code, then the tester LLM is asked to evaluate
    the code and point out problems, then the coder LLM is asked to refine its code
    based on this feedback, and so on. The authors call this “communicative dehallucination.”
    Sometimes this approach improves results as the tester LLM correctly identifies
    errors. Other times, however, the tester fails to identify mistakes or generates
    incorrect tests.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个有希望的研究方向是使用多智能体 AI，其中多个 LLM 协同以验证彼此的输出。例如，一组研究人员提出了一种多智能体方法来减轻软件开发任务中的幻觉（参见
    [https://arxiv.org/pdf/2307.07924](https://arxiv.org/pdf/2307.07924)）。在他们提出的系统中，一个
    LLM 扮演程序员角色，另一个扮演测试员。两者都被提示有效地执行各自的职责。程序员 LLM 被要求生成一段代码，然后测试员 LLM 被要求评估代码并指出问题，然后程序员
    LLM 根据这些反馈来改进其代码，依此类推。作者称之为“沟通去幻觉”。有时这种方法可以改善结果，因为测试员 LLM 正确地识别了错误。然而，有时测试员未能识别错误或生成了错误的测试。
- en: In addition to trying to mitigate hallucinations, some people have been studying
    ways of detecting them. One promising way is to analyze the probabilities outputted
    by the LLM. If you recall, LLMs output a probability value for each possible next
    token, and the next token is sampled using those probabilities. Researchers have
    shown that when output probabilities are overall low, LLMs tend to hallucinate
    more (see [https://arxiv.org/pdf/2307.03987](https://arxiv.org/pdf/2307.03987)).
    This shows that an LLM’s lack of confidence about its output is correlated with
    hallucinations. Thus, the user can detect low-probability outputs and validate
    them.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 除了试图减轻幻觉之外，一些人一直在研究检测幻觉的方法。一种有希望的方法是分析LLM输出的概率。如果你还记得，LLM为每个可能的下一个标记输出一个概率值，并且下一个标记是使用这些概率进行采样的。研究人员已经表明，当输出概率总体较低时，LLM倾向于产生更多的幻觉（见[https://arxiv.org/pdf/2307.03987](https://arxiv.org/pdf/2307.03987)）。这表明LLM对其输出的不自信与幻觉相关。因此，用户可以检测到低概率输出并验证它们。
- en: Hallucinations can kill a product
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幻觉会毁掉一个产品
- en: The presence of hallucinations can sometimes harm the success of certain sensitive
    products. For example, the customer-­service chatbot of a major airline provided
    hallucinated information to a passenger on how to obtain a refund. The airline
    refused to proceed with the refund citing that the actual conditions were different
    from what the chatbot had indicated. A court ordered the company to honor the
    refund anyway, saying that the airline “does not explain why customers should
    have to double-check information found in one part of its website on another part
    of its website.” The story made headlines, and the airline disabled the chatbot
    soon after (see [https://mng.bz/galG](https://mng.bz/galG)).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉的存在有时会损害某些敏感产品的成功率。例如，一家大型航空公司的客户服务聊天机器人向一位乘客提供了关于如何获得退款的不实信息。航空公司拒绝退款，理由是实际条件与聊天机器人所指示的不同。法院命令该公司无论如何都要履行退款，并表示航空公司“没有解释为什么客户需要在其网站的一个部分找到的信息在另一个部分再次进行核实。”这个故事成为了头条新闻，航空公司随后很快就关闭了该聊天机器人（见[https://mng.bz/galG](https://mng.bz/galG)）。
- en: The industry of self-driving cars has perhaps been the greatest casualty of
    AI’s hallucinations. Once a booming industry, now it is flailing, and its future
    is uncertain. One of the main reasons is that self-driving cars keep making surprisingly
    bad decisions due to hallucinations, especially in uncommon situations that aren’t
    present in the training data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车行业可能是AI幻觉的最大受害者。曾经是一个繁荣的行业，现在却陷入困境，其未来充满不确定性。其中一个主要原因是自动驾驶汽车由于幻觉而不断做出令人惊讶的糟糕决定，尤其是在训练数据中不常见的情况。
- en: For example, in October 2023, a self-driving car hit a pedestrian in California
    right after she’d been hit by another car. The pedestrian was visible in the camera’s
    sensors, yet the AI didn’t classify her correctly. An engineering firm explained,
    “The pedestrian’s feet and lower legs were visible in the wide-angle left side
    camera from the time of impact to the final stop, but, despite briefly detecting
    the legs, neither the pedestrian nor her legs were classified or tracked by the
    vehicle” (see [https://mng.bz/eyAq](https://mng.bz/eyAq)). Instead of stopping,
    the self-driving car continued driving, dragging the pedestrian 20 feet.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，2023年10月，一辆自动驾驶汽车在加利福尼亚州发生碰撞后不久，又撞到了一名行人。行人在摄像头的传感器中是可见的，但AI没有正确分类她。一家工程公司解释说：“从碰撞时刻到最终停止，行人的脚和下半身都出现在广角左侧摄像头中，尽管短暂地检测到了腿，但车辆既没有将行人也没有将她的腿分类或跟踪”（见[https://mng.bz/eyAq](https://mng.bz/eyAq)）。而不是停车，自动驾驶汽车继续行驶，将行人拖行了20英尺。
- en: The car in question had been manufactured by Cruise, one of the foremost self-driving
    car companies and a subsidiary of General Motors. After the incident, Cruise had
    its license to operate in California revoked, and the company decided to recall
    all its vehicles in the United States.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这辆有问题的汽车是由Cruise制造的，Cruise是领先的自动驾驶汽车公司之一，也是通用汽车的子公司。事故发生后，Cruise在加利福尼亚州的运营许可证被吊销，该公司决定召回其在美国的所有车辆。
- en: A month after the incident, it was revealed that Cruise cars weren’t actually
    driving themselves as much as it appeared. Instead, humans had to remotely intervene
    every 2.5 to 5 miles to assist the vehicles (see [https://mng.bz/pKlw](https://mng.bz/pKlw)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 事故发生一个月后，人们发现Cruise汽车实际上并没有像看起来那样自动驾驶。相反，人类每2.5到5英里就需要远程干预以协助车辆（见[https://mng.bz/pKlw](https://mng.bz/pKlw)）。
- en: A few months later, Waymo, which is Google’s self-driving car initiative, was
    involved in a similar scandal. A Waymo car hit a truck that was being towed in
    an unusual way. A few minutes later, another Waymo car hit the same truck. Waymo
    explained ([https://mng.bz/OBga](https://mng.bz/OBga)),
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 几个月后，谷歌的自动驾驶汽车项目Waymo卷入了一场类似的丑闻。一辆Waymo汽车撞上了一辆以不寻常方式拖曳的卡车。几分钟后，另一辆Waymo汽车撞上了同一辆卡车。Waymo解释说([https://mng.bz/OBga](https://mng.bz/OBga))，
- en: A Waymo vehicle made contact with a backwards-facing pickup truck being improperly
    towed ahead of the Waymo vehicle such that the pickup truck was persistently angled
    across a center turn lane and a traffic lane . . . and a few minutes later another
    Waymo vehicle made contact with the same pickup truck while it was being towed
    in the same manner. . . . We determined that due to the persistent orientation
    mismatch of the towed pickup truck and tow truck combination, the Waymo autonomous
    vehicle incorrectly predicted the future motion of the towed vehicle.
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一辆Waymo车辆与一辆被错误拖曳的逆向行驶的皮卡接触，使得皮卡持续地以一个角度横跨中心转弯车道和车道……几分钟后，另一辆Waymo车辆以同样的方式与同一辆皮卡接触。……我们确定，由于被拖曳的皮卡和拖车组合的持续方向不匹配，Waymo自动驾驶车辆错误地预测了被拖曳车辆的未来运动。
- en: As we can see from Waymo’s explanation, the manufacturers attribute the problem
    to the truck being towed in an unusual way, which made AI not recognize the truck
    as such. This is an example of AI not coping with an edge case.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从Waymo的解释中我们可以看出，制造商将问题归咎于以不寻常的方式拖曳的卡车，这使得AI无法识别卡车。这是一个AI无法处理边缘情况的例子。
- en: As is often the case with hallucinations, Waymo engineers took action to patch
    this specific problem with ad hoc actions. Waymo explained, “After developing,
    rigorously testing, and validating a fix, on December 20, 2023, we began deploying
    a software update to our fleet to address this issue.” But what about other unusual
    problems Waymo cars haven’t been specifically patched to deal with? What if a
    truck is painted with an unusual color or a pedestrian is wearing an unusual wig?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 正如幻觉通常发生的情况一样，Waymo工程师采取了临时措施来修复这个具体问题。Waymo解释说：“在开发、严格测试和验证了一个修复方案后，2023年12月20日，我们开始部署软件更新到我们的车队中，以解决这个问题。”但是，对于Waymo汽车尚未具体修补以应对的其他不寻常问题呢？如果一辆卡车被涂上了不寻常的颜色，或者一个行人戴了一个不寻常的假发呢？
- en: Applying patch after patch doesn’t seem to be working well for the industry,
    as problems persist, and some companies are giving up. A Bloomberg article declared,
    “Even after $100 billion, self-driving cars are going nowhere” ([https://mng.bz/YDja](https://mng.bz/YDja)).
    Uber, Lyft, Ford, and Volkswagen have all abandoned their self-driving initiatives.
    The remaining contenders, Cruise and Waymo being among the most important ones,
    keep moving their goalposts. Unless we discover a new AI methodology that doesn’t
    hallucinate, they’ll probably have to keep moving them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 应用补丁似乎对行业不起作用，因为问题持续存在，一些公司正在放弃。一篇彭博社的文章宣称，“即使投入了1000亿美元，自动驾驶汽车仍然毫无进展”([https://mng.bz/YDja](https://mng.bz/YDja))。Uber、Lyft、福特和大众都放弃了他们的自动驾驶计划。剩下的竞争者，包括最重要的Cruise和Waymo，仍在不断调整他们的目标。除非我们发现一种新的AI方法，不会产生幻觉，否则他们可能不得不继续调整。
- en: Living with hallucinations
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与幻觉共存
- en: Because hallucinations might remain part of AI for quite some time, it’s best
    to learn how to live with them. We should keep them in mind from the very start
    when we use AI or build an AI-related product. In chapter 4, we’ll discuss that
    there are many AI applications in which hallucinations aren’t a big problem, so
    we have the highest chances of building a successful AI product. In other cases,
    in which hallucinations matter, we should assess their effects and think of mitigation
    and detection strategies early on.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于幻觉可能长期存在于AI中，我们最好学会如何与它们共存。在使用AI或构建与AI相关的产品时，我们应该从一开始就牢记这一点。在第4章中，我们将讨论许多AI应用中幻觉并不是大问题，因此我们有机会构建一个成功的AI产品。在其他情况下，如果幻觉很重要，我们应该评估它们的影响，并尽早考虑缓解和检测策略。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Hallucinations are confidently wrong outputs generated by AI.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉是AI自信地生成的错误输出。
- en: Common types of hallucinations are made-up facts, misinterpreted information,
    and incorrect solutions to problems.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的幻觉类型包括虚构的事实、误解的信息和对问题的错误解决方案。
- en: One cause of hallucinations is that AI’s internal world model is insufficient
    to describe how our world operates.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉的一个原因是AI的内部世界模型不足以描述我们世界的运作方式。
- en: Another cause is that AI models are often trained to do one thing and used for
    something else—they’re misaligned with our goals.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个原因是，AI模型通常被训练来做某件事，却被用于其他目的——它们与我们目标不一致。
- en: Hallucinations are not going away anytime soon because this would require modifying
    prevailing machine learning methods in a yet-unknown way.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉在不久的将来不会消失，因为这需要以尚未知晓的方式修改现有的机器学习方法。
- en: Hallucinations are sometimes unacceptable or unsafe for users, which can deeply
    hurt a product’s chances of success.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉有时对用户来说可能不可接受或不安全，这可能会严重损害产品的成功机会。
- en: Hallucinations can be mitigated by using prompt engineering techniques, and
    they can be detected sometimes.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用提示工程技术，幻觉可以得到缓解，有时也可以被检测到。
- en: We must keep hallucinations in mind throughout the life cycle of an AI-related
    product.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须在整个AI相关产品的生命周期中牢记幻觉的存在。
