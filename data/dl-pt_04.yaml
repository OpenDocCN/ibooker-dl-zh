- en: 4 Real-world data representation using tensors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 使用张量表示真实世界数据
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Representing real-world data as PyTorch tensors
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将现实世界的数据表示为 PyTorch 张量
- en: Working with a range of data types
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理各种数据类型
- en: Loading data from a file
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文件加载数据
- en: Converting data to tensors
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据转换为张量
- en: Shaping tensors so they can be used as inputs for neural network models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 塑造张量，使其可以作为神经网络模型的输入
- en: In the previous chapter, we learned that tensors are the building blocks for
    data in PyTorch. Neural networks take tensors as input and produce tensors as
    outputs. In fact, all operations within a neural network and during optimization
    are operations between tensors, and all parameters (for example, weights and biases)
    in a neural network are tensors. Having a good sense of how to perform operations
    on tensors and index them effectively is central to using tools like PyTorch successfully.
    Now that you know the basics of tensors, your dexterity with them will grow as
    you make your way through the book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解到张量是 PyTorch 中数据的构建块。神经网络将张量作为输入，并产生张量作为输出。事实上，神经网络内部的所有操作以及优化过程中的所有操作都是张量之间的操作，神经网络中的所有参数（例如权重和偏置）都是张量。对于成功使用
    PyTorch 这样的工具，对张量执行操作并有效地对其进行索引的能力至关重要。现在您已经了解了张量的基础知识，随着您在本书中的学习过程中，您对张量的灵活性将会增长。
- en: 'Here’s a question that we can already address: how do we take a piece of data,
    a video, or a line of text, and represent it with a tensor in a way that is appropriate
    for training a deep learning model? This is what we’ll learn in this chapter.
    We’ll cover different types of data with a focus on the types relevant to this
    book and show how to represent that data as tensors. Then we’ll learn how to load
    the data from the most common on-disk formats and get a feel for those data types’
    structure so we can see how to prepare them for training a neural network. Often,
    our raw data won’t be perfectly formed for the problem we’d like to solve, so
    we’ll have a chance to practice our tensor-manipulation skills with a few more
    interesting tensor operations.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以回答一个问题：我们如何将一段数据、一个视频或一行文本表示为张量，以便适合训练深度学习模型？这就是我们将在本章学习的内容。我们将重点介绍与本书相关的数据类型，并展示如何将这些数据表示为张量。然后，我们将学习如何从最常见的磁盘格式加载数据，并了解这些数据类型的结构，以便了解如何准备它们用于训练神经网络。通常，我们的原始数据不会完全符合我们想要解决的问题，因此我们将有机会通过一些更有趣的张量操作来练习我们的张量操作技能。
- en: Each section in this chapter will describe a data type, and each will come with
    its own dataset. While we’ve structured the chapter so that each data type builds
    on the previous one, feel free to skip around a bit if you’re so inclined.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的每个部分将描述一种数据类型，并且每种数据类型都将配有自己的数据集。虽然我们已经将本章结构化，使得每种数据类型都建立在前一种数据类型的基础上，但如果你愿意，可以随意跳跃一下。
- en: We’ll be using a lot of image and volumetric data through the rest of the book,
    since those are common data types and they reproduce well in book format. We’ll
    also cover tabular data, time series, and text, as those will also be of interest
    to a number of our readers. Since a picture is worth a thousand words, we’ll start
    with image data. We’ll then demonstrate working with a three-dimensional array
    using medical data that represents patient anatomy as a volume. Next, we’ll work
    with tabular data about wines, just like what we’d find in a spreadsheet. After
    that, we’ll move to *ordered* tabular data, with a time-series dataset from a
    bike-sharing program. Finally, we’ll dip our toes into text data from Jane Austen.
    Text data retains its ordered aspect but introduces the problem of representing
    words as arrays of numbers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分中，我们将使用大量图像和体积数据，因为这些是常见的数据类型，并且��书籍格式中可以很好地再现。我们还将涵盖表格数据、时间序列和文本，因为这些也将引起许多读者的兴趣。因为一图胜千言，我们将从图像数据开始。然后，我们将演示使用代表患者解剖结构的医学数据的三维数组。接下来，我们将处理关于葡萄酒的表格数据，就像我们在电子表格中找到的那样。之后，我们将转向*有序*表格数据，使用来自自行车共享计划的时间序列数据集。最后，我们将涉足简·奥斯汀的文本数据。文本数据保留了其有序性，但引入了将单词表示为数字数组的问题。
- en: 'In every section, we will stop where a deep learning researcher would start:
    right before feeding the data to a model. We encourage you to keep these datasets;
    they will constitute excellent material for when we start learning how to train
    neural network models in the next chapter.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个部分中，我们将在深度学习研究人员开始的地方停下来：就在将数据馈送给模型之前。我们鼓励您保留这些数据集；它们将成为我们在下一章开始学习如何训练神经网络模型时的优秀材料。
- en: 4.1 Working with images
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 处理图像
- en: The introduction of convolutional neural networks revolutionized computer vision
    (see [http://mng.bz/zjMa](http://mng.bz/zjMa)), and image-based systems have since
    acquired a whole new set of capabilities. Problems that required complex pipelines
    of highly tuned algorithmic building blocks are now solvable at unprecedented
    levels of performance by training end-to-end networks using paired input-and-desired-output
    examples. In order to participate in this revolution, we need to be able to load
    an image from common image formats and then transform the data into a tensor representation
    that has the various parts of the image arranged in the way PyTorch expects.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络的引入彻底改变了计算机视觉（参见 [http://mng.bz/zjMa](http://mng.bz/zjMa)），基于图像的系统随后获得了全新的能力。以前需要高度调整的算法构建块的复杂流水线现在可以通过使用成对的输入和期望输出示例训练端到端网络以前所未有的性能水平解决。为了参与这场革命，我们需要能够从常见的图像格式中加载图像，然后将数据转换为
    PyTorch 期望的方式排列图像各部分的张量表示。
- en: An image is represented as a collection of scalars arranged in a regular grid
    with a height and a width (in pixels). We might have a single scalar per grid
    point (the pixel), which would be represented as a grayscale image; or multiple
    scalars per grid point, which would typically represent different colors, as we
    saw in the previous chapter, or different *features* like depth from a depth camera.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图像被表示为一个规则网格中排列的标量集合，具有高度和宽度（以像素为单位）。 我们可能在每个网格点（像素）上有一个单一的标量，这将被表示为灰度图像；或者在每个网格点上有多个标量，这通常代表不同的颜色，就像我们在上一章中看到的那样，或者不同的
    *特征*，比如来自深度相机的深度。
- en: Scalars representing values at individual pixels are often encoded using 8-bit
    integers, as in consumer cameras. In medical, scientific, and industrial applications,
    it is not unusual to find higher numerical precision, such as 12-bit or 16-bit.
    This allows a wider range or increased sensitivity in cases where the pixel encodes
    information about a physical property, like bone density, temperature, or depth.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表示单个像素值的标量通常使用 8 位整数进行编码，如消费级相机。 在医疗、科学和工业应用中，发现更高的数值精度，如 12 位或 16 位，是很常见的。
    这允许在像骨密度、温度或深度等物理属性的像素编码信息的情况下拥有更广泛的范围或增加灵敏度。
- en: 4.1.1 Adding color channels
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 添加颜色通道
- en: We mentioned colors earlier. There are several ways to encode colors into numbers.[¹](#pgfId-1012659)
    The most common is RGB, where a color is defined by three numbers representing
    the intensity of red, green, and blue. We can think of a color channel as a grayscale
    intensity map of only the color in question, similar to what you’d see if you
    looked at the scene in question using a pair of pure red sunglasses. Figure 4.1
    shows a rainbow, where each of the RGB channels captures a certain portion of
    the spectrum (the figure is simplified, in that it elides things like the orange
    and yellow bands being represented as a combination of red and green).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过颜色。 有几种将颜色编码为数字的方法。 最常见的是 RGB，其中颜色由表示红色、绿色和蓝色强度的三个数字定义。 我们可以将颜色通道看作是仅包含所讨论颜色的灰度强度图，类似于如果您戴上一副纯红色太阳镜看到的场景。
    图 4.1 展示了一个彩虹，其中每个 RGB 通道捕获光谱的某个部分（该图简化了，省略了将橙色和黄色带表示为红色和绿色组合的内容）。
- en: '![](../Images/CH04_F01_Stevens2_GS.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F01_Stevens2_GS.png)'
- en: Figure 4.1 A rainbow, broken into red, green, and blue channels
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 彩虹，分为红色、绿色和蓝色通道
- en: The red band of the rainbow is brightest in the red channel of the image, while
    the blue channel has both the blue band of the rainbow and the sky as high-intensity.
    Note also that the white clouds are high-intensity in all three channels.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 彩虹的红色带在图像的红色通道中最亮，而蓝色通道既有彩虹的蓝色带又有天空作为高强度。 还要注意，白云在所有三个通道中都是高强度的。
- en: 4.1.2 Loading an image file
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 加载图像文件
- en: Images come in several different file formats, but luckily there are plenty
    of ways to load images in Python. Let’s start by loading a PNG image using the
    `imageio` module (code/p1ch4/1_image_dog.ipynb).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图像有多种不同的文件格式，但幸运的是在 Python 中有很多加载图像的方法。 让我们从使用 `imageio` 模块加载 PNG 图像开始（code/p1ch4/1_image_dog.ipynb）。
- en: Listing 4.1 code/p1ch4/1_image_dog.ipynb
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.1 code/p1ch4/1_image_dog.ipynb
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Note* We’ll use `imageio` throughout the chapter because it handles different
    data types with a uniform API. For many purposes, using TorchVision is a great
    default choice to deal with image and video data. We go with `imageio` here for
    somewhat lighter exploration.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* 我们将在整个章节中使用 `imageio`，因为它使用统一的 API 处理不同的数据类型。 对于许多目的，使用 TorchVision 处理图像和视频数据是一个很好的默认选择。
    我们在这里选择 `imageio` 进行稍微轻松的探索。'
- en: 'At this point, `img` is a NumPy array-like object with three dimensions: two
    spatial dimensions, width and height; and a third dimension corresponding to the
    red, green, and blue channels. Any library that outputs a NumPy array will suffice
    to obtain a PyTorch tensor. The only thing to watch out for is the layout of the
    dimensions. PyTorch modules dealing with image data require tensors to be laid
    out as *C* × *H* × *W* : channels, height, and width, respectively.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`img` 是一个类似于 NumPy 数组的对象，具有三个维度：两个空间维度，宽度和高度；以及第三个维度对应于红色、绿色和蓝色通道。 任何输出 NumPy
    数组的库都足以获得 PyTorch 张量。 唯一需要注意的是维度的布局。 处理图像数据的 PyTorch 模块要求张量按照 *C* × *H* × *W*
    的方式布局：通道、高度和宽度。
- en: 4.1.3 Changing the layout
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 更改布局
- en: 'We can use the tensor’s `permute` method with the old dimensions for each new
    dimension to get to an appropriate layout. Given an input tensor *H* × *W* × *C*
    as obtained previously, we get a proper layout by having channel 2 first and then
    channels 0 and 1:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用张量的 `permute` 方法，使用旧维度替换每个新维度，以获得适当的布局。 给定一个先前获得的输入张量 *H* × *W* × *C*，通过首先将通道
    2 放在前面，然后是通道 0 和 1，我们得到一个正确的布局：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We’ve seen this previously, but note that this operation does not make a copy
    of the tensor data. Instead, `out` uses the same underlying storage as `img` and
    only plays with the size and stride information at the tensor level. This is convenient
    because the operation is very cheap; but just as a heads-up: changing a pixel
    in `img` will lead to a change in `out`.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到过这个，但请注意，此操作不会复制张量数据。 相反，`out` 使用与 `img` 相同的底层存储，并且仅在张量级别上处理大小和步幅信息。 这很方便，因为该操作非常便宜；
    但是需要注意的是：更改 `img` 中的像素将导致 `out` 中的更改。
- en: Note also that other deep learning frameworks use different layouts. For instance,
    originally TensorFlow kept the channel dimension last, resulting in an *H* × *W*
    × *C* layout (it now supports multiple layouts). This strategy has pros and cons
    from a low-level performance standpoint, but for our concerns, it doesn’t make
    a difference as long as we reshape our tensors properly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，其他深度学习框架使用不同的布局。 例如，最初 TensorFlow 将通道维度保留在最后，导致 *H* × *W* × *C* 的布局（现在支持多种布局）。
    从低级性能的角度来看，这种策略有利有弊，但就我们的问题而言，只要我们正确地重塑张量，就不会有任何区别。
- en: So far, we have described a single image. Following the same strategy we’ve
    used for earlier data types, to create a dataset of multiple images to use as
    an input for our neural networks, we store the images in a batch along the first
    dimension to obtain an *N* × *C* × *H* × *W* tensor.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们描述了单个图像。按照我们之前用于其他数据类型的相同策略，为了创建一个包含多个图像的数据集，以用作神经网络的输入，我们将图像存储在一个批次中，沿着第一个维度获得一个*N*
    × *C* × *H* × *W*张量。
- en: 'As a slightly more efficient alternative to using `stack` to build up the tensor,
    we can preallocate a tensor of appropriate size and fill it with images loaded
    from a directory, like so:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为使用`stack`构建张量的略微更高效的替代方法，我们可以预先分配一个适当大小的张量，并用从目录加载的图像填充它，如下所示：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This indicates that our batch will consist of three RGB images 256 pixels in
    height and 256 pixels in width. Notice the type of the tensor: we’re expecting
    each color to be represented as an 8-bit integer, as in most photographic formats
    from standard consumer cameras. We can now load all PNG images from an input directory
    and store them in the tensor:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们的批次将由三个RGB图像组成，高度为256像素，宽度为256像素。注意张量的类型：我们期望每种颜色都表示为8位整数，就像大多数标准消费级相机的照片格式一样。现在我们可以从输入目录加载所有PNG图像并将它们存储在张量中：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Here we keep only the first three channels. Sometimes images also have an
    alpha channel indicating transparency, but our network only wants RGB input.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这里我们仅保留前三个通道。有时图像还具有表示透明度的alpha通道，但我们的网络只需要RGB输入。
- en: 4.1.4 Normalizing the data
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.4 数据归一化
- en: We mentioned earlier that neural networks usually work with floating-point tensors
    as their input. Neural networks exhibit the best training performance when the
    input data ranges roughly from 0 to 1, or from -1 to 1 (this is an effect of how
    their building blocks are defined).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到神经网络通常使用浮点张量作为它们的输入。当输入数据的范围大致从0到1，或从-1到1时，神经网络表现出最佳的训练性能（这是由它们的构建块定义方式所决定的效果）。
- en: 'So a typical thing we’ll want to do is cast a tensor to floating-point and
    normalize the values of the pixels. Casting to floating-point is easy, but normalization
    is trickier, as it depends on what range of the input we decide should lie between
    0 and 1 (or -1 and 1). One possibility is to just divide the values of the pixels
    by 255 (the maximum representable number in 8-bit unsigned):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们通常需要做的一件事是将张量转换为浮点数并对像素的值进行归一化。将其转换为浮点数很容易，但归一化则更加棘手，因为它取决于我们决定将输入的哪个范围置于0和1之间（或-1和1之间）。一种可能性是仅通过255（8位无符号数中的最大可表示数）来除以像素的值：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Another possibility is to compute the mean and standard deviation of the input
    data and scale it so that the output has zero mean and unit standard deviation
    across each channel:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能性是计算输入数据的均值和标准差，并对其进行缩放，使输出在每个通道上具有零均值和单位标准差：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Note* Here, we normalize just a single batch of images because we do not know
    yet how to operate on an entire dataset. In working with images, it is good practice
    to compute the mean and standard deviation on all the training data in advance
    and then subtract and divide by these fixed, precomputed quantities. We saw this
    in the preprocessing for the image classifier in section 2.1.4.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* 这里，我们仅对一批图像进行归一化，因为我们还不知道如何操作整个数据集。在处理图像时，最好提前计算所有训练数据的均值和标准差，然后减去并除以这些固定的、预先计算的量。我们在第2.1.4节中的图像分类器的预处理中看到了这一点。'
- en: We can perform several other operations on inputs, such as geometric transformations
    like rotations, scaling, and cropping. These may help with training or may be
    required to make an arbitrary input conform to the input requirements of a network,
    like the size of the image. We will stumble on quite a few of these strategies
    in section 12.6\. For now, just remember that you have image-manipulation options
    available.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对输入执行几种其他操作，如几何变换（旋转、缩放和裁剪）。这些操作可能有助于训练，或者可能需要使任意输入符合网络的输入要求，比如图像的大小。我们将在第12.6节中遇到许多这些策略。现在，只需记住你有可用的图像处理选项即可。
- en: '4.2 3D images: Volumetric data'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 3D 图像：体积数据
- en: We’ve learned how to load and represent 2D images, like the ones we take with
    a camera. In some contexts, such as medical imaging applications involving, say,
    CT (computed tomography) scans, we typically deal with sequences of images stacked
    along the head-to-foot axis, each corresponding to a slice across the human body.
    In CT scans, the intensity represents the density of the different parts of the
    body--lungs, fat, water, muscle, and bone, in order of increasing density--mapped
    from dark to bright when the CT scan is displayed on a clinical workstation. The
    density at each point is computed from the amount of X-rays reaching a detector
    after crossing through the body, with some complex math to deconvolve the raw
    sensor data into the full volume.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何加载和表示2D图像，就像我们用相机拍摄的那些图像一样。在某些情境下，比如涉及CT（计算机断层扫描）扫描的医学成像应用中，我们通常处理沿着头到脚轴堆叠的图像序列，每个图像对应于人体的一个切片。在CT扫描中，强度代表了身体不同部位的密度--肺部、脂肪、水、肌肉和骨骼，按照密度递增的顺序--在临床工作站上显示CT扫描时，从暗到亮进行映射。每个点的密度是根据穿过身体后到达探测器的X射线量计算的，通过一些复杂的数学将原始传感器数据解卷积为完整体积。
- en: CTs have only a single intensity channel, similar to a grayscale image. This
    means that often, the channel dimension is left out in native data formats; so,
    similar to the last section, the raw data typically has three dimensions. By stacking
    individual 2D slices into a 3D tensor, we can build volumetric data representing
    the 3D anatomy of a subject. Unlike what we saw in figure 4.1, the extra dimension
    in figure 4.2 represents an offset in physical space, rather than a particular
    band of the visible spectrum.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: CT（计算机断层扫描）只有一个强度通道，类似于灰度图像。这意味着通常情况下，原生数据格式中会省略通道维度；因此，类似于上一节，原始数据通常具有三个维度。通过将单个2D切片堆叠成3D张量，我们可以构建代表主体的3D解剖结构的体积数据。与我们在图4.1中看到的情况不同，图4.2中的额外维度代表的是物理空间中的偏移，而不是可见光谱中的特定波段。
- en: '![](../Images/CH04_F02_Stevens2_GS.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F02_Stevens2_GS.png)'
- en: Figure 4.2 Slices of a CT scan, from the top of the head to the jawline
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 从头部到下颌的CT扫描切片
- en: Part 2 of this book will be devoted to tackling a medical imaging problem in
    the real world, so we won’t go into the details of medical-imaging data formats.
    For now, it suffices to say that there’s no fundamental difference between a tensor
    storing volumetric data versus image data. We just have an extra dimension, *depth*,
    after the *channel* dimension, leading to a 5D tensor of shape *N* × *C* × *D*
    × *H* × *W*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的第二部分将致力于解决现实世界中的医学成像问题，因此我们不会深入讨论医学成像数据格式的细节。目前，可以说存储体积数据与图像数据的张量之间没有根本区别。我们只是在*通道*维度之后多了一个维度，*深度*，导致了一个形状为*N*
    × *C* × *D* × *H* × *W*的5D张量。
- en: 4.2.1 Loading a specialized format
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 加载专用格式
- en: Let’s load a sample CT scan using the `volread` function in the `imageio` module,
    which takes a directory as an argument and assembles all Digital Imaging and Communications
    in Medicine (DICOM) files[²](#pgfId-1013706) in a series in a NumPy 3D array (code/p1ch4/
    2_volumetric_ct.ipynb).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`imageio`模块中的`volread`函数加载一个样本CT扫描，该函数以一个目录作为参数，并将所有数字影像与通信医学（DICOM）文件[²](#pgfId-1013706)组装成一个NumPy
    3D数组（code/p1ch4/ 2_volumetric_ct.ipynb）。
- en: Listing 4.2 code/p1ch4/2_volumetric_ct.ipynb
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单4.2 code/p1ch4/2_volumetric_ct.ipynb
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As was true in section 4.1.3, the layout is different from what PyTorch expects,
    due to having no channel information. So we’ll have to make room for the `channel`
    dimension using `unsqueeze`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在第4.1.3节中所述，布局与PyTorch期望的不同，因为没有通道信息。因此，我们将使用`unsqueeze`为`channel`维度腾出空间：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: At this point we could assemble a 5D dataset by stacking multiple volumes along
    the `batch` direction, just as we did in the previous section. We’ll see a lot
    more CT data in part 2\.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以通过沿着`batch`方向堆叠多个体积来组装一个5D数据集，就像我们在上一节中所做的那样。在第二部分中我们将看到更多的CT数据。
- en: 4.3 Representing tabular data
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 表格数据的表示
- en: The simplest form of data we’ll encounter on a machine learning job is sitting
    in a spreadsheet, CSV file, or database. Whatever the medium, it’s a table containing
    one row per sample (or record), where columns contain one piece of information
    about our sample.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习工作中我们会遇到的最简单形式的数据位于电子表格、CSV文件或数据库中。无论媒介如何，它都是一个包含每个样本（或记录）一行的表格，其中列包含关于我们样本的一条信息。
- en: 'At first we are going to assume there’s no meaning to the order in which samples
    appear in the table: such a table is a collection of independent samples, unlike
    a time series, for instance, in which samples are related by a time dimension.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我们假设表格中样本出现���顺序没有意义：这样的表格是独立样本的集合，不像时间序列那样，其中样本由时间维度相关联。
- en: 'Columns may contain numerical values, like temperatures at specific locations;
    or labels, like a string expressing an attribute of the sample, like “blue.” Therefore,
    tabular data is typically not homogeneous: different columns don’t have the same
    type. We might have a column showing the weight of apples and another encoding
    their color in a label.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 列可能包含数值，例如特定位置的温度；或标签，例如表示样本属性的字符串，如“蓝色”。因此，表格数据通常不是同质的：不同列的类型不同。我们可能有一列显示苹果的重量，另一列用标签编码它们的颜色。
- en: PyTorch tensors, on the other hand, are homogeneous. Information in PyTorch
    is typically encoded as a number, typically floating-point (though integer types
    and Boolean are supported as well). This numeric encoding is deliberate, since
    neural networks are mathematical entities that take real numbers as inputs and
    produce real numbers as output through successive application of matrix multiplications
    and nonlinear functions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，PyTorch张量是同质的。PyTorch中的信息通常被编码为一个数字，通常是浮点数（尽管也支持整数类型和布尔类型）。这种数字编码是有意的，因为神经网络是数学实体，通过矩阵乘法和非线性函数的连续应用将实数作为输入并产生实数作为输出。
- en: 4.3.1 Using a real-world dataset
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 使用真实世界数据集
- en: 'Our first job as deep learning practitioners is to encode heterogeneous, real-world
    data into a tensor of floating-point numbers, ready for consumption by a neural
    network. A large number of tabular datasets are freely available on the internet;
    see, for instance, [https://github.com/caesar0301/awesome-public-datasets](https://github.com/caesar0301/awesome-public-datasets).
    Let’s start with something fun: wine! The Wine Quality dataset is a freely available
    table containing chemical characterizations of samples of *vinho verde*, a wine
    from north Portugal, together with a sensory quality score. The dataset for white
    wines can be downloaded here: [http://mng.bz/90Ol](http://mng.bz/90Ol). For convenience,
    we also created a copy of the dataset on the Deep Learning with PyTorch Git repository,
    under data/p1ch4/tabular-wine.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作为深度学习从业者的第一项工作是将异构的现实世界数据编码为浮点数张量，以便神经网络消费。互联网上有大量的表格数据集可供免费使用；例如，可以查看[https://github.com/caesar0301/awesome-public-datasets](https://github.com/caesar0301/awesome-public-datasets)。让我们从一些有趣的东西开始：葡萄酒！葡萄酒质量数据集是一个包含葡萄牙北部*绿葡萄酒*样本的化学特征和感官质量评分的免费表格。白葡萄酒数据集可以在这里下载：[http://mng.bz/90Ol](http://mng.bz/90Ol)。为了方便起见，我们还在Deep
    Learning with PyTorch Git存储库中的data/p1ch4/tabular-wine目录下创建了数据集的副本。
- en: 'The file contains a comma-separated collection of values organized in 12 columns
    preceded by a header line containing the column names. The first 11 columns contain
    values of chemical variables, and the last column contains the sensory quality
    score from 0 (very bad) to 10 (excellent). These are the column names in the order
    they appear in the dataset:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件包含一个逗号分隔的值集合，由一个包含列名的标题行引导。前11列包含化学变量的值，最后一列包含从0（非常糟糕）到10（优秀）的感官质量评分。这些是数据集中按照它们出现的顺序的列名：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A possible machine learning task on this dataset is predicting the quality score
    from chemical characterization alone. Don’t worry, though; machine learning is
    not going to kill wine tasting anytime soon. We have to get the training data
    from somewhere! As we can see in figure 4.3, we’re hoping to find a relationship
    between one of the chemical columns in our data and the quality column. Here,
    we’re expecting to see quality increase as sulfur decreases.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集上的一个可能的机器学习任务是仅通过化学特征预测质量评分。不过，不用担心；机器学习不会很快消灭品酒。我们必须从某处获取训练数据！正如我们在图4.3中看到的，我们希望在我们的数据中的化学列和质量列之间找到一个关系。在这里，我们期望看到随着硫的减少，质量会提高。
- en: '![](../Images/CH04_F03_Stevens2_GS.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F03_Stevens2_GS.png)'
- en: Figure 4.3 The (we hope) relationship between sulfur and quality in wine
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 我们（希望）在葡萄酒中硫和质量之间的关系
- en: 4.3.2 Loading a wine data tensor
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 加载葡萄酒数据张量
- en: Before we can get to that, however, we need to be able to examine the data in
    a more usable way than opening the file in a text editor. Let’s see how we can
    load the data using Python and then turn it into a PyTorch tensor. Python offers
    several options for quickly loading a CSV file. Three popular options are
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在进行这之前，我们需要以一种比在文本编辑器中打开文件更可用的方式来检查数据。让我们看看如何使用Python加载数据，然后将其转换为PyTorch张量。Python提供了几种快速加载CSV文件的选项。三种流行的选项是
- en: The `csv` module that ships with Python
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python自带的`csv`模块
- en: NumPy
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Pandas
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas
- en: The third option is the most time- and memory-efficient. However, we’ll avoid
    introducing an additional library in our learning trajectory just because we need
    to load a file. Since we already introduced NumPy in the previous section, and
    PyTorch has excellent NumPy interoperability, we’ll go with that. Let’s load our
    file and turn the resulting NumPy array into a PyTorch tensor (code/p1ch4/3_tabular_wine.ipynb).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个选项是最节省时间和内存的。然而，我们将避免在我们的学习轨迹中引入额外的库，只是因为我们需要加载一个文件。由于我们在上一节中已经介绍了NumPy，并且PyTorch与NumPy有很好的互操作性，我们将选择这个。让我们加载我们的文件，并将生成的NumPy数组转换为PyTorch张量（code/p1ch4/3_tabular_wine.ipynb）。
- en: Listing 4.3 code/p1ch4/3_tabular_wine.ipynb
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单4.3 code/p1ch4/3_tabular_wine.ipynb
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here we just prescribe what the type of the 2D array should be (32-bit floating-point),
    the delimiter used to separate values in each row, and the fact that the first
    line should not be read since it contains the column names. Let’s check that all
    the data has been read
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只规定2D数组的类型应该是32位浮点数，用于分隔每行值的分隔符，以及不应读取第一行，因为它包含列名。让我们检查所有数据是否都已读取
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'and proceed to convert the NumPy array to a PyTorch tensor:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将NumPy数组转换为PyTorch张量：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: At this point, we have a floating-point `torch.Tensor` containing all the columns,
    including the last, which refers to the quality score. [³](#pgfId-1015644)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们有一个包含所有列的浮点`torch.Tensor`，包括最后一列，指的是质量评分。[³](#pgfId-1015644)
- en: Continuous, ordinal, and categorical values
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 连续值、有序值和分类值
- en: 'We should be aware of three different kinds of numerical values as we attempt
    to make sense of our data.3 The first kind is continuous values. These are the
    most intuitive when represented as numbers. They are strictly ordered, and a difference
    between various values has a strict meaning. Stating that package A is 2 kilograms
    heavier than package B, or that package B came from 100 miles farther away than
    A has a fixed meaning, regardless of whether package A is 3 kilograms or 10, or
    if B came from 200 miles away or 2,000\. If you’re counting or measuring something
    with units, it’s probably a continuous value. The literature actually divides
    continuous values further: in the previous examples, it makes sense to say something
    is twice as heavy or three times farther away, so those values are said to be
    on a ratio scale. The time of day, on the other hand, does have the notion of
    difference, but it is not reasonable to claim that 6:00 is twice as late as 3:00;
    so time of day only offers an interval scale.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们试图理解数据时，我们应该意识到三种不同类型的数值。第一种是连续值。当以数字表示时，这些值是最直观的。它们是严格有序的，各个值之间的差异具有严格的含义。声明A包比B包重2千克，或者B包比A包远100英里，无论A包是3千克还是10千克，或者B包来自200英里还是2,000英里，都有固定的含义。如果你在计数或测量带单位的东西，那么它很可能是一个连续值。文献实际上进一步将连续值分为不同类型：在前面的例子中，说某物体重两倍或距离远三倍是有意义的，因此这些值被称为比例尺。另一方面，一天中的时间具有差异的概念，但声称6:00比3:00晚两倍是不合理的；因此时间只提供一个区间尺度。
- en: Next we have ordinal values. The strict ordering we have with continuous values
    remains, but the fixed relationship between values no longer applies. A good example
    of this is ordering a small, medium, or large drink, with small mapped to the
    value 1, medium 2, and large 3\. The large drink is bigger than the medium, in
    the same way that 3 is bigger than 2, but it doesn’t tell us anything about how
    much bigger. If we were to convert our 1, 2, and 3 to the actual volumes (say,
    8, 12, and 24 fluid ounces), then they would switch to being interval values.
    It’s important to remember that we can’t “do math” on the values outside of ordering
    them; trying to average large = 3 and small = 1 does not result in a medium drink!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是有序值。我们在连续值中具有的严格排序仍然存在，但值之间的固定关系不再适用。一个很好的例子是将小、中、大饮料排序，其中小映射到值1，中2，大3。大饮料比中饮料大，就像3比2大一样，但这并不告诉我们有多大差异。如果我们将我们的1、2和3转换为实际容量（比如8、12和24液体盎司），那么它们将转变为区间值。重要的是要记住，我们不能在值上“做数学运算”以外的排序它们；尝试平均大=3和小=1并不会得到中等饮料！
- en: Finally, categorical values have neither ordering nor numerical meaning to their
    values. These are often just enumerations of possibilities assigned arbitrary
    numbers. Assigning water to 1, coffee to 2, soda to 3, and milk to 4 is a good
    example. There’s no real logic to placing water first and milk last; they simply
    need distinct values to differentiate them. We could assign coffee to 10 and milk
    to -3, and there would be no significant change (though assigning values in the
    range 0..*N* - 1 will have advantages for one-hot encoding and the embeddings
    we’ll discuss in section 4.5.4.) Because the numerical values bear no meaning,
    they are said to be on a nominal scale.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，分类值既没有顺序也没有数值含义。这些通常只是分配了任意数字的可能性枚举。将水分配给1，咖啡分配给2，苏打分配给3，牛奶分配给4就是一个很好的例子。将水放在第一位，牛奶放在最后一位并没有真正的逻辑；它们只是需要不同的值来区分它们。我们可以将咖啡分配给10，牛奶分配给-3，这样也不会有显著变化（尽管在范围0..*N*
    - 1内分配值将对独热编码和我们将在第4.5.4节讨论的嵌入有优势）。因为数值值没有含义，它们被称为名义尺度。
- en: 4.3.3 Representing scores
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 表示分数
- en: 'We could treat the score as a continuous variable, keep it as a real number,
    and perform a regression task, or treat it as a label and try to guess the label
    from the chemical analysis in a classification task. In both approaches, we will
    typically remove the score from the tensor of input data and keep it in a separate
    tensor, so that we can use the score as the ground truth without it being input
    to our model:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将分数视为连续变量，保留为实数，并执行回归任务，或将其视为标签并尝试从化学分析中猜测标签以进行分类任务。在这两种方法中，我们通常会从输入数据张量中删除分数，并将其保留在单独的张量中，以便我们可以将分数用作地面实况，而不将其作为模型的输入：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Selects all rows and all columns except the last
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 选择所有行和除最后一列之外的所有列
- en: ❷ Selects all rows and the last column
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择所有行和最后一列
- en: 'If we want to transform the `target` tensor in a tensor of labels, we have
    two options, depending on the strategy or what we use the categorical data for.
    One is simply to treat labels as an integer vector of scores:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要将`target`张量转换为标签张量，我们有两种选择，取决于策略或我们如何使用分类数据。一种方法是简单地将标签视为整数分数的向量：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If targets were string labels, like *wine color*, assigning an integer number
    to each string would let us follow the same approach.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标是字符串标签，比如*葡萄酒颜色*，为每个字符串分配一个整数编号将让我们遵循相同的方法。
- en: 4.3.4 One-hot encoding
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.4 独热编码
- en: 'The other approach is to build a *one-hot* encoding of the scores: that is,
    encode each of the 10 scores in a vector of 10 elements, with all elements set
    to 0 but one, at a different index for each score. This way, a score of 1 could
    be mapped onto the vector `(1,0,0,0,0,0,0,0,0,0)`, a score of 5 onto `(0,0,0,0,1,0,0,0,0,0)`,
    and so on. Note that the fact that the score corresponds to the index of the nonzero
    element is purely incidental: we could shuffle the assignment, and nothing would
    change from a classification standpoint.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是构建分数的*独热编码*：即，将10个分数中的每一个编码为一个具有10个元素的向量，其中所有元素均设置为0，但一个元素在每个分数的不同索引上设置为1。
    这样，分数1可以映射到向量`(1,0,0,0,0,0,0,0,0,0)`，分数5可以映射到`(0,0,0,0,1,0,0,0,0,0)`，依此类推。请注意，分数对应于非零元素的索引纯属偶然：我们可以重新排列分配，从分类的角度来看，没有任何变化。
- en: 'There’s a marked difference between the two approaches. Keeping wine quality
    scores in an integer vector of scores induces an ordering on the scores--which
    might be totally appropriate in this case, since a score of 1 is lower than a
    score of 4\. It also induces some sort of distance between scores: that is, the
    distance between 1 and 3 is the same as the distance between 2 and 4\. If this
    holds for our quantity, then great. If, on the other hand, scores are purely discrete,
    like grape variety, one-hot encoding will be a much better fit, as there’s no
    implied ordering or distance. One-hot encoding is also appropriate for quantitative
    scores when fractional values in between integer scores, like 2.4, make no sense
    for the application--for when the score is either *this* or *that*.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法之间有明显的区别。将葡萄酒质量分数保留在整数分数向量中会对分数产生排序--这在这种情况下可能是完全合适的，因为1分比4分低。它还会在分数之间引入某种距离：也就是说，1和3之间的距离与2和4之间的距离相同。如果这对我们的数量成立，那就太好了。另一方面，如果分数完全是离散的，比如葡萄品种，独热编码将更适合，因为没有暗示的排序或距离。当分数是连续变量时，独热编码也适用，例如在整数分数之间没有意义的情况下，比如2.4，对于应用程序来说要么是*这个*要么是*那个*。
- en: 'We can achieve one-hot encoding using the `scatter_` method, which fills the
    tensor with values from a source tensor along the indices provided as arguments:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`scatter_`方法实现独热编码，该方法将源张量中的值沿提供的索引填充到张量中：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s see what `scatter_` does. First, we notice that its name ends with an
    underscore. As you learned in the previous chapter, this is a convention in PyTorch
    that indicates the method will not return a new tensor, but will instead modify
    the tensor in place. The arguments for `scatter_` are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`scatter_`做了什么。首先，我们注意到它的名称以下划线结尾。正如您在上一章中学到的，这是PyTorch中的一种约定，表示该方法不会返回新张量，而是会直接修改���量。`scatter_`的参数如下：
- en: The dimension along which the following two arguments are specified
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定以下两个参数的维度
- en: A column tensor indicating the indices of the elements to scatter
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指示要散布元素索引的列张量
- en: A tensor containing the elements to scatter or a single scalar to scatter (1,
    in this case)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含要散布的元素或要散布的单个标量的张量（在本例中为1）
- en: In other words, the previous invocation reads, “For each row, take the index
    of the target label (which coincides with the score in our case) and use it as
    the column index to set the value 1.0.” The end result is a tensor encoding categorical
    information.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，前面的调用读取，“对于每一行，取目标标签的索引（在我们的情况下与分数相符）并将其用作列索引设置值为1.0。” 最终结果是一个编码分类信息的张量。
- en: 'The second argument of `scatter_`, the index tensor, is required to have the
    same number of dimensions as the tensor we scatter into. Since `target_onehot`
    has two dimensions (4,898 × 10), we need to add an extra dummy dimension to `target`
    using `unsqueeze`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`scatter_`的第二个参数，索引张量，需要与我们要散布到的张量具有相同数量的维度。由于`target_onehot`有两个维度（4,898 ×
    10），我们需要使用`unsqueeze`添加一个额外的虚拟维度到`target`中：'
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The call to `unsqueeze` adds a *singleton* dimension, from a 1D tensor of 4,898
    elements to a 2D tensor of size (4,898 × 1), without changing its contents--no
    extra elements are added; we just decided to use an extra index to access the
    elements. That is, we access the first element of `target` as `target[0]` and
    the first element of its unsqueezed counterpart as `target_unsqueezed[0,0]`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`unsqueeze`函数会添加一个*单例*维度，将一个包含 4,898 个元素的 1D 张量转换为一个大小为 (4,898 × 1) 的 2D 张量，而不改变其内容--不会添加额外的元素；我们只是决定使用额外的索引来访问元素。也就是说，我们可以通过`target[0]`访问`target`的第一个元素，通过`target_unsqueezed[0,0]`访问其未挤压的对应元素。
- en: PyTorch allows us to use class indices directly as targets while training neural
    networks. However, if we wanted to use the score as a categorical input to the
    network, we would have to transform it to a one-hot-encoded tensor.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 允许我们在训练神经网络时直接使用类索引作为目标。但是，如果我们想将分数用作网络的分类输入，我们将不得不将其转换为一个独热编码张量。
- en: 4.3.5 When to categorize
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.5 何时进行分类
- en: Now we have seen ways to deal with both continuous and categorical data. You
    may wonder what the deal is with the ordinal case discussed in the earlier sidebar.
    There is no general recipe for it; most commonly, such data is either treated
    as categorical (losing the ordering part, and hoping that maybe our model will
    pick it up during training if we only have a few categories) or continuous (introducing
    an arbitrary notion of distance). We will do the latter for the weather situation
    in figure 4.5\. We summarize our data mapping in a small flow chart in figure
    4.4.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何处理连续和分类数据。您可能想知道早期边栏中讨论的有序情况是什么情况。对于这种情况，没有通用的处理方法；最常见的做法是将这些数据视为分类数据（失去排序部分，并希望也许我们的模型在训练过程中会捕捉到它，如果我们只有少数类别）或连续数据（引入一个任意的距离概念）。我们将在图
    4.5 中的天气情况中采取后者。我们在图 4.4 中的一个小流程图中总结了我们的数据映射。
- en: '![](../Images/CH04_F04_Stevens2_GS.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F04_Stevens2_GS.png)'
- en: Figure 4.4 How to treat columns with continuous, ordinal, and categorical data
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 如何处理连续、有序和分类数据的列
- en: 'Let’s go back to our `data` tensor, containing the 11 variables associated
    with the chemical analysis. We can use the functions in the PyTorch Tensor API
    to manipulate our data in tensor form. Let’s first obtain the mean and standard
    deviations for each column:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到包含与化学分析相关的 11 个变量的`data`张量。我们可以使用 PyTorch 张量 API 中的函数以张量形式操作我们的数据。让我们首先获取每列的均值和标准差：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In this case, `dim=0` indicates that the reduction is performed along dimension
    0\. At this point, we can normalize the data by subtracting the mean and dividing
    by the standard deviation, which helps with the learning process (we’ll discuss
    this in more detail in chapter 5, in section 5.4.4):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`dim=0`表示沿着维度 0 执行缩减。此时，我们可以通过减去均值并除以标准差来对数据进行归一化，这有助于学习过程（我们将在第 5 章的
    5.4.4 节中更详细地讨论这一点）：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 4.3.6 Finding thresholds
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.6 寻找阈值
- en: 'Next, let’s start to look at the data with an eye to seeing if there is an
    easy way to tell good and bad wines apart at a glance. First, we’re going to determine
    which rows in `target` correspond to a score less than or equal to 3:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们开始查看数据，看看是否有一种简单的方法可以一眼看出好酒和坏酒的区别。首先，我们将确定`target`中对应于得分小于或等于 3 的行：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ PyTorch also provides comparison functions, here torch.le(target, 3), but
    using operators seems to be a good standard.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ PyTorch 还提供比较函数，例如 torch.le(target, 3)，但使用运算符似乎是一个很好的标准。
- en: 'Note that only 20 of the `bad_indexes` entries are set to `True`! By using
    a feature in PyTorch called *advanced indexing*, we can use a tensor with data
    type `torch.bool` to index the `data` tensor. This will essentially filter `data`
    to be only items (or rows) corresponding to `True` in the indexing tensor. The
    `bad_indexes` tensor has the same shape as `target`, with values of `False` or
    `True` depending on the outcome of the comparison between our threshold and each
    element in the original `target` tensor:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`bad_indexes`中只有 20 个条目被设置为`True`！通过使用 PyTorch 中称为*高级索引*的功能，我们可以使用数据类型为`torch.bool`的张量来索引`data`张量。这将基本上将`data`过滤为仅包含索引张量中为`True`的项目（或行）的项。`bad_indexes`张量与`target`具有相同的形状，其值为`False`或`True`，取决于我们的阈值与原始`target`张量中每个元素之间的比较结果：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note that the new `bad_data` tensor has 20 rows, the same as the number of
    rows with `True` in the `bad_indexes` tensor. It retains all 11 columns. Now we
    can start to get information about wines grouped into good, middling, and bad
    categories. Let’s take the `.mean()` of each column:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，新的`bad_data`张量有 20 行，与`bad_indexes`张量中为`True`的行数相同。它保留了所有 11 列。现在我们可以开始获取关于被分为好、中等和差类别的葡萄酒的信息。让我们对每列进行`.mean()`操作：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ For Boolean NumPy arrays and PyTorch tensors, the & operator does a logical
    “and” operation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对于布尔 NumPy 数组和 PyTorch 张量，& 运算符执行逻辑“与”操作。
- en: 'It looks like we’re on to something here: at first glance, the bad wines seem
    to have higher total sulfur dioxide, among other differences. We could use a threshold
    on total sulfur dioxide as a crude criterion for discriminating good wines from
    bad ones. Let’s get the indexes where the total sulfur dioxide column is below
    the midpoint we calculated earlier, like so:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们有所发现：乍一看，坏酒似乎具有更高的总二氧化硫含量，等等其他差异。我们可以使用总二氧化硫的阈值作为区分好酒和坏酒的粗略标准。让我们获取总二氧化硫列低于我们之前计算的中点的索引，如下所示：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This means our threshold implies that just over half of all the wines are going
    to be high quality. Next, we’ll need to get the indexes of the actually good wines:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的阈值意味着超过一半的葡萄酒将是高质量的。接下来，我们需要获取实际好葡萄酒的索引：
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Since there are about 500 more actually good wines than our threshold predicted,
    we already have hard evidence that it’s not perfect. Now we need to see how well
    our predictions line up with the actual rankings. We will perform a logical “and”
    between our prediction indexes and the actual good indexes (remember that each
    is just an array of zeros and ones) and use that intersection of wines-in-agreement
    to determine how well we did:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实际好酒比我们的阈值预测多约500瓶，我们已经有了不完美的确凿证据。现在我们需要看看我们的预测与实际排名的匹配程度。我们将在我们的预测索引和实际好酒索引之间执行逻辑“与”（记住每个都只是一个由零和一组成的数组），并使用这些一致的酒来确定我们的表现如何：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We got around 2,000 wines right! Since we predicted 2,700 wines, this gives
    us a 74% chance that if we predict a wine to be high quality, it actually is.
    Unfortunately, there are 3,200 good wines, and we only identified 61% of them.
    Well, we got what we signed up for; that’s barely better than random! Of course,
    this is all very naive: we know for sure that multiple variables contribute to
    wine quality, and the relationships between the values of these variables and
    the outcome (which could be the actual score, rather than a binarized version
    of it) is likely more complicated than a simple threshold on a single value.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大约有2,000瓶酒是正确的！由于我们预测了2,700瓶酒，这给了我们74%的机会，如果我们预测一瓶酒是高质量的，那它实际上就是。不幸的是，有3,200瓶好酒，我们只识别了其中的61%。嗯，我们得到了我们签约的东西；这几乎比随机好不了多少！当然，这一切都很天真：我们确切地知道多个变量影响葡萄酒的质量，这些变量的值与结果之间的关系（可能是实际分数，而不是其二值化版本）可能比单个值的简单阈值更复杂。
- en: Indeed, a simple neural network would overcome all of these limitations, as
    would a lot of other basic machine learning methods. We’ll have the tools to tackle
    this problem after the next two chapters, once we have learned how to build our
    first neural network from scratch. We will also revisit how to better grade our
    results in chapter 12\. Let’s move on to other data types for now.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，一个简单的神经网络将克服所有这些限制，许多其他基本的机器学习方法也将克服这些限制。在接下来的两章中，一旦我们学会如何从头开始构建我们的第一个神经网络，我们将有解决这个问题的工具。我们还将在第12章重新审视如何更好地评估我们的结果。现在让我们继续探讨其他数据类型。
- en: 4.4 Working with time series
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 处理时间序列
- en: In the previous section, we covered how to represent data organized in a flat
    table. As we noted, every row in the table was independent from the others; their
    order did not matter. Or, equivalently, there was no column that encoded information
    about what rows came earlier and what came later.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们讨论了如何表示组织在平面表中的数据。正如我们所指出的，表中的每一行都是独立的；它们的顺序并不重要。或者等效地，没有列编码关于哪些行先出现和哪些行后出现的信息。
- en: 'Going back to the wine dataset, we could have had a “year” column that allowed
    us to look at how wine quality evolved year after year. Unfortunately, we don’t
    have such data at hand, but we’re working hard on manually collecting the data
    samples, bottle by bottle. (Stuff for our second edition.) In the meantime, we’ll
    switch to another interesting dataset: data from a Washington, D.C., bike-sharing
    system reporting the hourly count of rental bikes in 2011-2012 in the Capital
    Bikeshare system, along with weather and seasonal information (available here:
    [http://mng.bz/jgOx](http://mng.bz/jgOx)). Our goal will be to take a flat, 2D
    dataset and transform it into a 3D one, as shown in figure 4.5.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 回到葡萄酒数据集，我们本可以有一个“年份”列，让我们看看葡萄酒质量是如何逐年演变的。不幸的是，我们手头没有这样的数据，但我们正在努力手动收集数据样本，一瓶一瓶地。在此期间，我们将转向另一个有趣的数据集：来自华盛顿特区自行车共享系统的数据，报告2011-2012年Capital
    Bikeshare系统中每小时租赁自行车的数量，以及天气和季节信息（可在此处找到：[http://mng.bz/jgOx](http://mng.bz/jgOx)）。我们的目标是将一个平面的二维数据集转换为一个三维数据集，如图4.5所示。
- en: '![](../Images/CH04_F05_Stevens2_GS.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F05_Stevens2_GS.png)'
- en: Figure 4.5 Transforming a 1D, multichannel dataset into a 2D, multichannel dataset
    by separating the date and hour of each sample into separate axes
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 将一维多通道数据集转换为二维多通道数据集，通过将每个样本的日期和小时分开到不同的轴上
- en: 4.4.1 Adding a time dimension
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 添加时间维度
- en: In the source data, each row is a separate hour of data (figure 4.5 shows a
    transposed version of this to better fit on the printed page). We want to change
    the row-per-hour organization so that we have one axis that increases at a rate
    of one day per index increment, and another axis that represents the hour of the
    day (independent of the date). The third axis will be our different columns of
    data (weather, temperature, and so on).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在源数据中，每一行是一个单独的小时数据（图4.5显示了这个的转置版本，以更好地适应打印页面）。我们希望改变每小时一行的组织方式，这样我们就有一个轴，它以每个索引增加一天的速度增加，另一个轴代表一天中的小时（与日期无关）。第三个轴将是我们的不同数据列（天气、温度等）。
- en: Let’s load the data (code/p1ch4/4_time_series_bikes.ipynb).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载数据（code/p1ch4/4_time_series_bikes.ipynb）。
- en: Listing 4.4 code/p1ch4/4_time_series_bikes.ipynb
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 4.4 code/p1ch4/4_time_series_bikes.ipynb
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Converts date strings to numbers corresponding to the day of the month in
    column 1
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将日期字符串转换为对应于第1列中的日期的数字
- en: 'For every hour, the dataset reports the following variables:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个小时，数据集报告以下变量：
- en: 'Index of record: `instant`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录索引：`instant`
- en: 'Day of month: `day`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份的日期：`day`
- en: 'Season: `season` (`1`: spring, `2`: summer, `3`: fall, `4`: winter)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 季节：`season`（`1`：春季，`2`：夏季，`3`：秋季，`4`：冬季）
- en: 'Year: `yr` (`0`: 2011, `1`: 2012)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年份：`yr`（`0`：2011，`1`：2012）
- en: 'Month: `mnth` (`1` to `12`)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份：`mnth`（`1`到`12`）
- en: 'Hour: `hr` (`0` to `23`)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小时：`hr`（`0`到`23`）
- en: 'Holiday status: `holiday`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节假日状态：`holiday`
- en: 'Day of the week: `weekday`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一周的第几天：`weekday`
- en: 'Working day status: `workingday`'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作日状态：`workingday`
- en: 'Weather situation: `weathersit` (`1`: clear, `2`:mist, `3`: light rain/snow,
    `4`: heavy rain/snow)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气情况：`weathersit`（`1`：晴朗，`2`：薄雾，`3`：小雨/小雪，`4`：大雨/大雪）
- en: 'Temperature in °C: `temp`'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄氏度温度：`temp`
- en: 'Perceived temperature in °C: `atemp`'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄氏度感知温度：`atemp`
- en: 'Humidity: `hum`'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 湿度：`hum`
- en: 'Wind speed: `windspeed`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风速：`windspeed`
- en: 'Number of casual users: `casual`'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 休闲用户数量：`casual`
- en: 'Number of registered users: `registered`'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册用户数量：`registered`
- en: 'Count of rental bikes: `cnt`'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 租赁自行车数量：`cnt`
- en: 'In a time series dataset such as this one, rows represent successive time-points:
    there is a dimension along which they are ordered. Sure, we could treat each row
    as independent and try to predict the number of circulating bikes based on, say,
    a particular time of day regardless of what happened earlier. However, the existence
    of an ordering gives us the opportunity to exploit causal relationships across
    time. For instance, it allows us to predict bike rides at one time based on the
    fact that it was raining at an earlier time. For the time being, we’re going to
    focus on learning how to turn our bike-sharing dataset into something that our
    neural network will be able to ingest in fixed-size chunks.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的时间序列数据集中，行代表连续的时间点：有一个维度沿着它们被排序。当然，我们可以将每一行视为独立的，并尝试根据一天中的特定时间来预测循环自行车的数量，而不考虑之前发生了什么。然而，存在排序给了我们利用时间上的因果关系的机会。例如，它允许我们根据较早时间下雨的事实来预测某个时间的骑车次数。目前，我们将专注于学习如何将我们的共享单车数据集转换为我们的神经网络能够以固定大小的块摄入的内容。
- en: 'This neural network model will need to see a number of sequences of values
    for each different quantity, such as ride count, time of day, temperature, and
    weather conditions: *N* parallel sequences of size *C*. *C* stands for *channel*,
    in neural network parlance, and is the same as *column* for 1D data like we have
    here. The *N* dimension represents the time axis, here one entry per hour.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络模型将需要看到每个不同数量的值的一些序列，比如骑行次数、时间、温度和天气条件：*N*个大小为*C*的并行序列。*C*代表神经网络术语中的*通道*，对于我们这里的1D数据来说，它与*列*是相同的。*N*维度代表时间轴，这里每小时一个条目。
- en: 4.4.2 Shaping the data by time period
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 按时间段塑造数据
- en: 'We might want to break up the two-year dataset into wider observation periods,
    like days. This way we’ll have *N* (for *number of samples*) collections of *C*
    sequences of length *L*. In other words, our time series dataset would be a tensor
    of dimension 3 and shape *N* × *C* × *L*. The *C* would remain our 17 channels,
    while *L* would be 24: 1 per hour of the day. There’s no particular reason why
    we *must* use chunks of 24 hours, though the general daily rhythm is likely to
    give us patterns we can exploit for predictions. We could also use 7 × 24 = 168
    hour blocks to chunk by week instead, if we desired. All of this depends, naturally,
    on our dataset having the right size--the number of rows must be a multiple of
    24 or 168\. Also, for this to make sense, we cannot have gaps in the time series.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能希望将这两年的数据集分成更宽的观测周期，比如天。这样我们将有*N*（用于*样本数量*）个长度为*L*的*C*序列集合。换句话说，我们的时间序列数据集将是一个三维张量，形状为*N*
    × *C* × *L*。*C*仍然是我们的17个通道，而*L*将是24：每天的每小时一个。虽然我们*必须*使用24小时的块没有特别的原因，但一般的日常节奏可能会给我们可以用于预测的模式。如果需要，我们也可以使用7
    × 24 = 168小时块按周划分。所有这些当然取决于我们的数据集具有正确的大小--行数必须是24或168的倍数。此外，为了使这有意义，我们的时间序列不能有间断。
- en: Let’s go back to our bike-sharing dataset. The first column is the index (the
    global ordering of the data), the second is the date, and the sixth is the time
    of day. We have everything we need to create a dataset of daily sequences of ride
    counts and other exogenous variables. Our dataset is already sorted, but if it
    were not, we could use `torch.sort` on it to order it appropriately.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的共享单车数据集。第一列是索引（数据的全局排序），第二列是日期，第六列是一天中的时间。我们有一切需要创建每日骑行次数和其他外生变量序列的数据集。我们的数据集已经排序，但如果没有，我们可以使用`torch.sort`对其进行适当排序。
- en: '*Note* The version of the file we’re using, hour-fixed.csv, has had some processing
    done to include rows missing from the original dataset. We presume that the missing
    hours had zero bike active (they were typically in the early morning hours).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意*我们使用的文件版本hour-fixed.csv已经经过一些处理，包括在原始数据集中包含缺失的行。我们假设缺失的小时没有活跃的自行车（它们通常在清晨的小时内）。'
- en: 'All we have to do to obtain our daily hours dataset is view the same tensor
    in batches of 24 hours. Let’s take a look at the shape and strides of our `bikes`
    tensor:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得我们的每日小时数据集，我们只需将相同的张量按照24小时的批次查看。让我们看一下我们的`bikes`张量的形状和步幅：
- en: '[PRE25]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'That’s 17,520 hours, 17 columns. Now let’s reshape the data to have 3 axes--day,
    hour, and then our 17 columns:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这是17,520小时，17列。现在让我们重新塑造数据，使其具有3个轴--天、小时，然后我们的17列：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'What happened here? First, `bikes.shape[1]` is 17, the number of columns in
    the `bikes` tensor. But the real crux of this code is the call to `view`, which
    is really important: it changes the way the tensor looks at the same data as contained
    in storage.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么？首先，`bikes.shape[1]`是17，即`bikes`张量中的列数。但这段代码的关键在于对`view`的调用，这非常重要：它改变了张量查看相同数据的方式，而数据实际上是包含在存储中的。
- en: As you learned in the previous chapter, calling `view` on a tensor returns a
    new tensor that changes the number of dimensions and the striding information,
    without changing the storage. This means we can rearrange our tensor at basically
    zero cost, because no data will be copied. Our call to `view` requires us to provide
    the new shape for the returned tensor. We use `-1` as a placeholder for “however
    many indexes are left, given the other dimensions and the original number of elements.”
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在上一章中学到的，对张量调用`view`会返回一个新的张量，它会改变维度和步幅信息，但不会改变存储。这意味着我们可以在基本上零成本地重新排列我们的张量，因为不会复制任何数据。我们调用`view`需要为返回的张量提供新的形状。我们使用`-1`作为“剩下的索引数量，考虑到其他维度和原始元素数量”的占位符。
- en: Remember also from the previous chapter that storage is a contiguous, linear
    container for numbers (floating-point, in this case). Our `bikes` tensor will
    have each row stored one after the other in its corresponding storage. This is
    confirmed by the output from the call to `bikes.stride()` earlier.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 还要记住上一章中提到的存储是一个连续的、线性的数字容器（在本例中是浮点数）。我们的`bikes`张量将每一行按顺序存储在其相应的存储中。这是通过之前对`bikes.stride()`的调用输出来确认的。
- en: For `daily_bikes`, the stride is telling us that advancing by 1 along the hour
    dimension (the second dimension) requires us to advance by 17 places in the storage
    (or one set of columns); whereas advancing along the day dimension (the first
    dimension) requires us to advance by a number of elements equal to the length
    of a row in the storage times 24 (here, 408, which is 17 × 24).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`daily_bikes`，步幅告诉我们，沿着小时维度（第二维）前进1需要我们在存储中前进17个位置（或者一组列）；而沿着天维度（第一维）前进需要我们前进的元素数量等于存储中一行的长度乘以24（这里是408，即17×24）。
- en: 'We see that the rightmost dimension is the number of columns in the original
    dataset. Then, in the middle dimension, we have time, split into chunks of 24
    sequential hours. In other words, we now have *N* sequences of *L* hours in a
    day, for *C* channels. To get to our desired *N* × *C* × *L* ordering, we need
    to transpose the tensor:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到最右边的维度是原始数据集中的列数。然后，在中间维度，我们有时间，分成24个连续小时的块。换句话说，我们现在有一天中*L*小时的*N*序列，对应*C*个通道。为了得到我们期望的*N*×*C*×*L*顺序，我们需要转置张量：
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now let’s apply some of the techniques we learned earlier to this dataset.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将之前学到的一些技巧应用到这个数据集上。
- en: 4.4.3 Ready for training
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 准备训练
- en: 'The “weather situation” variable is ordinal. It has four levels: `1` for good
    weather, and `4` for, er, really bad. We could treat this variable as categorical,
    with levels interpreted as labels, or as a continuous variable. If we decided
    to go with categorical, we would turn the variable into a one-hot-encoded vector
    and concatenate the columns with the dataset.[⁴](#pgfId-1018727)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: “天气情况”变量是有序的。它有四个级别：`1`表示好天气，`4`表示，嗯，非常糟糕。我们可以将这个变量视为分类变量，其中级别被解释为标签，或者作为连续变量。如果我们决定采用分类方式，我们将把变量转换为一个独热编码向量，并将列与数据集连接起来。[⁴](#pgfId-1018727)
- en: 'In order to make it easier to render our data, we’re going to limit ourselves
    to the first day for a moment. We initialize a zero-filled matrix with a number
    of rows equal to the number of hours in the day and number of columns equal to
    the number of weather levels:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易呈现我们的数据，我们暂时限制在第一天。我们初始化一个以一天中小时数为行数，天气级别数为列数的零填充矩阵：
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then we scatter ones into our matrix according to the corresponding level at
    each row. Remember the use of `unsqueeze` to add a singleton dimension as we did
    in the previous sections:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们根据每行对应级别向我们的矩阵中散布1。记住在前几节中使用`unsqueeze`添加一个单例维度：
- en: '[PRE29]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Decreases the values by 1 because weather situation ranges from 1 to 4, while
    indices are 0-based
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将值减1是因为天气情况范围从1到4，而索引是从0开始的
- en: Our day started with weather “1” and ended with “2,” so that seems right.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的一天从天气“1”开始，以“2”结束，所以这看起来是正确的。
- en: 'Last, we concatenate our matrix to our original dataset using the `cat` function.
    Let’s look at the first of our results:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`cat`函数将我们的矩阵与原始数据集连接起来。让我们看看我们的第一个结果：
- en: '[PRE30]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here we prescribed our original `bikes` dataset and our one-hot-encoded “weather
    situation” matrix to be concatenated along the *column* dimension (that is, 1).
    In other words, the columns of the two datasets are stacked together; or, equivalently,
    the new one-hot-encoded columns are appended to the original dataset. For `cat`
    to succeed, it is required that the tensors have the same size along the other
    dimensions--the *row* dimension, in this case. Note that our new last four columns
    are `1, 0, 0, 0`, exactly as we would expect with a weather value of 1\. We could
    have done the same with the reshaped `daily_bikes` tensor. Remember that it is
    shaped (*B*, *C*, *L*), where *L* = 24\. We first create the zero tensor, with
    the same *B* and *L*, but with the number of additional columns as *C* :'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里��我们规定我们的原始`bikes`数据集和我们的独热编码的“天气情况”矩阵沿着*列*维度（即1）进行连接。换句话说，两个数据集的列被堆叠在一起；或者等效地，新的独热编码列被附加到原始数据集。为了使`cat`成功，需要确保张量在其他维度（在这种情况下是*行*维度）上具有相同的大小。请注意，我们新的最后四列是`1,
    0, 0, 0`，正如我们期望的天气值为1时一样。我们也可以对重塑后的`daily_bikes`张量执行相同操作。记住它的形状是(*B*, *C*, *L*)，其中*L*
    = 24。我们首先创建一个零张量，具有相同的*B*和*L*，但具有*C*个额外列：
- en: '[PRE31]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then we scatter the one-hot encoding into the tensor in the *C* dimension.
    Since this operation is performed in place, only the content of the tensor will
    change:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将独热编码散布到张量的*C*维度中。由于这个操作是原地执行的，因此只有张量的内容会改变：
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'And we concatenate along the *C* dimension:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 并且我们沿着*C*维度进行连接：
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We mentioned earlier that this is not the only way to treat our “weather situation”
    variable. Indeed, its labels have an ordinal relationship, so we could pretend
    they are special values of a continuous variable. We could just transform the
    variable so that it runs from 0.0 to 1.0:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到这不是处理“天气情况”变量的唯一方式。实际上，它的标签具有有序关系，因此我们可以假设它们是连续变量的特殊值。我们可以将变量转换为从0.0到1.0的范围：
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As we mentioned in the previous section, rescaling variables to the [0.0, 1.0]
    interval or the [-1.0, 1.0] interval is something we’ll want to do for all quantitative
    variables, like `temperature` (column 10 in our dataset). We’ll see why later;
    for now, let’s just say that this is beneficial to the training process.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中提到的，将变量重新缩放到[0.0, 1.0]区间或[-1.0, 1.0]区间是我们希望对所有定量变量进行的操作，比如`temperature`（我们数据集中的第10列）。稍后我们会看到为什么要这样做；现在，我们只需说这对训练过程有益。
- en: There are multiple possibilities for rescaling variables. We can either map
    their range to [0.0, 1.0]
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对变量重新缩放有多种可能性。我们可以将它们的范围映射到[0.0, 1.0]
- en: '[PRE35]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'or subtract the mean and divide by the standard deviation:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 或者减去均值并除以标准差：
- en: '[PRE36]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the latter case, our variable will have 0 mean and unitary standard deviation.
    If our variable were drawn from a Gaussian distribution, 68% of the samples would
    sit in the [-1.0, 1.0] interval.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在后一种情况下，我们的变量将具有0均值和单位标准差。如果我们的变量是从高斯分布中抽取的，那么68%的样本将位于[-1.0, 1.0]区间内。
- en: 'Great: we’ve built another nice dataset, and we’ve seen how to deal with time
    series data. For this tour d’horizon, it’s important only that we got an idea
    of how a time series is laid out and how we can wrangle the data in a form that
    a network will digest.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了：我们建立了另一个不错的数据集，并且看到了如何处理时间序列数据。对于这次的概览，重要的是我们对时间序列的布局有了一个概念，以及我们如何将数据整理成网络可以处理的形式。
- en: Other kinds of data look like a time series, in that there is a strict ordering.
    Top two on the list? Text and audio. We’ll take a look at text next, and the “Conclusion”
    section has links to additional examples for audio.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的数据看起来像时间序列，因为有严格的顺序。排在前两位的是什么？文本和音频。接下来我们将看一下文本，而“结论”部分有关于音频的附加示例的链接。
- en: 4.5 Representing text
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 表示文本
- en: Deep learning has taken the field of natural language processing (NLP) by storm,
    particularly using models that repeatedly consume a combination of new input and
    previous model output. These models are called *recurrent neural networks* (RNNs),
    and they have been applied with great success to text categorization, text generation,
    and automated translation systems. More recently, a class of networks called *transformers*
    with a more flexible way to incorporate past information has made a big splash.
    Previous NLP workloads were characterized by sophisticated multistage pipelines
    that included rules encoding the grammar of a language.[⁵](#pgfId-1020191) Now,
    state-of-the-art work trains networks end to end on large corpora starting from
    scratch, letting those rules emerge from the data. For the last several years,
    the most-used automated translation systems available as services on the internet
    have been based on deep learning.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经席卷了自然语言处理（NLP）领域，特别是使用重复消耗新输入和先前模型输出组合的模型。这些模型被称为*循环神经网络*（RNNs），它们已经成功应用于文本分类、文本生成和自动翻译系统。最近，一类名为*transformers*的网络以更灵活的方式整合过去信息引起了轰动。以前的NLP工作负载以包含编码语言语法规则的规则的复杂多阶段管道为特征。现在，最先进的工作是从头开始在大型语料库上端对端训练网络，让这些规则从数据中出现。在过去几年里，互联网上最常用的自动翻译系统基于深度学习。
- en: 'Our goal in this section is to turn text into something a neural network can
    process: a tensor of numbers, just like our previous cases. If we can do that
    and later choose the right architecture for our text-processing job, we’ll be
    in the position of doing NLP with PyTorch. We see right away how powerful this
    all is: we can achieve state-of-the-art performance on a number of tasks in different
    domains *with the same PyTorch tools*; we just need to cast our problem in the
    right form. The first part of this job is reshaping the data.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一部分的目标是将文本转换为神经网络可以处理的东西：一个数字张量，就像我们之前的情况一样。如果我们能够做到这一点，并且稍后选择适合我们文本处理工作的正确架构，我们就可以使用PyTorch进行自然语言处理。我们立即看到这一切是多么强大：我们可以使用相同的PyTorch工具在不同领域的许多任务上实现最先进的性能；我们只需要将问题表述得当。这项工作的第一部分是重新塑造数据。
- en: 4.5.1 Converting text to numbers
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.1 将文本转换为数字
- en: 'There are two particularly intuitive levels at which networks operate on text:
    at the character level, by processing one character at a time, and at the word
    level, where individual words are the finest-grained entities to be seen by the
    network. The technique with which we encode text information into tensor form
    is the same whether we operate at the character level or the word level. And it’s
    not magic, either. We stumbled upon it earlier: one-hot encoding.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 网络在文本上操作有两个特别直观的层次：在字符级别上，逐个处理字符，以及在单词级别上，其中单词是网络所看到的最细粒度的实体。我们将文本信息编码为张量形式的技术，无论我们是在字符级别还是单词级别操作，都是相同的。而且这并不是魔法。我们之前就偶然发现了它：独热编码。
- en: 'Let’s start with a character-level example. First, let’s get some text to process.
    An amazing resource here is Project Gutenberg ([www.gutenberg.org](http://www.gutenberg.org/)),
    a volunteer effort to digitize and archive cultural work and make it available
    for free in open formats, including plain text files. If we’re aiming at larger-scale
    corpora, the Wikipedia corpus stands out: it’s the complete collection of Wikipedia
    articles, containing 1.9 billion words and more than 4.4 million articles. Several
    other corpora can be found at the English Corpora website ([www.english-corpora.org](https://www.english-corpora.org/)).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从字符级别的示例开始。首先，让我们获取一些要处理的文本。这里一个了不起的资源是古腾堡计划（[www.gutenberg.org](http://www.gutenberg.org/)）��这是一个志愿者努力，将文化作品数字化并以开放格式免费提供，包括纯文本文件。如果我们的目标是更大规模的语料库，维基百科语料库是一个突出的选择：它是维基百科文章的完整集合，包含19亿字和440多万篇文章。在英语语料库网站（[www.english-corpora.org](https://www.english-corpora.org/)）可以找到其他语料库。
- en: 'Let’s load Jane Austen’s *Pride and Prejudice* from the Project Gutenberg website:
    [www.gutenberg.org/files/1342/1342-0.txt](http://www.gutenberg.org/files/1342/1342-0.txt).
    We’ll just save the file and read it in (code/p1ch4/5_text_jane_austen.ipynb).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从古腾堡计划网站加载简·奥斯汀的《傲慢与偏见》：[www.gutenberg.org/files/1342/1342-0.txt](http://www.gutenberg.org/files/1342/1342-0.txt)。我们只需保存文件并读取它（code/p1ch4/5_text_jane_austen.ipynb）。
- en: Listing 4.5 code/p1ch4/5_text_jane_austen.ipynb
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 4.5 code/p1ch4/5_text_jane_austen.ipynb
- en: '[PRE37]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 4.5.2 One-hot-encoding characters
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.2 独热编码字符
- en: 'There’s one more detail we need to take care of before we proceed: encoding.
    This is a pretty vast subject, and we will just touch on it. Every written character
    is represented by a code: a sequence of bits of appropriate length so that each
    character can be uniquely identified. The simplest such encoding is ASCII (American
    Standard Code for Information Interchange), which dates back to the 1960s. ASCII
    encodes 128 characters using 128 integers. For instance, the letter *a* corresponds
    to binary 1100001 or decimal 97, the letter *b* to binary 1100010 or decimal 98,
    and so on. The encoding fits 8 bits, which was a big bonus in 1965.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，还有一个细节需要注意：编码。这是一个非常广泛的主题，我们只会简单提及。每个书面字符都由一个代码表示：一个适当长度的比特序列，以便每个字符都可以被唯一识别。最简单的编码是ASCII（美国信息交换标准代码），可以追溯到1960年代。ASCII使用128个整数对128个字符进行编码。例如，字母*a*对应于二进制1100001或十进制97，字母*b*对应于二进制1100010或十进制98，依此类推。这种编码适合8位，这在1965年是一个很大的优势。
- en: '*Note* 128 characters are clearly not enough to account for all the glyphs,
    accents, ligatures, and so on that are needed to properly represent written text
    in languages other than English. To this end, a number of encodings have been
    developed that use a larger number of bits as code for a wider range of characters.
    That wider range of characters was standardized as Unicode, which maps all known
    characters to numbers, with the representation in bits of those numbers provided
    by a specific encoding. Popular encodings are UTF-8, UTF-16, and UTF-32, in which
    the numbers are a sequence of 8-, 16-, or 32-bit integers, respectively. Strings
    in Python 3.x are Unicode strings.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* 128个字符显然不足以涵盖所有需要正确表示非英语语言中的书写文本所需的字形、重音、连字等。为此，已经开发了许多使用更多比特作为代码以涵盖更广字符范围的编码。这更广范围的字符被标准化为Unicode，将所有已知字符映射到数字，这些数字的位表示由特定编码提供。流行的编码包括UTF-8、UTF-16和UTF-32，其中数字分别是8位、16位或32位整数序列。Python
    3.x中的字符串是Unicode字符串。'
- en: We are going to one-hot encode our characters. It is instrumental to limit the
    one-hot encoding to a character set that is useful for the text being analyzed.
    In our case, since we loaded text in English, it is safe to use ASCII and deal
    with a small encoding. We could also make all of the characters lowercase, to
    reduce the number of different characters in our encoding. Similarly, we could
    screen out punctuation, numbers, or other characters that aren’t relevant to our
    expected kinds of text. This may or may not make a practical difference to a neural
    network, depending on the task at hand.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对字符进行独热编码。将独热编码限制在对所分析文本有用的字符集上是非常重要的。在我们的情况下，由于我们加载的是英文文本，使用ASCII并处理一个小编码是安全的。我们还可以将所有字符转换为小写，以减少编码中不同字符的数量。同样，我们可以筛选掉标点、数字或其他与我们期望的文本类型无关的字符。这可能对神经网络有实际影响，具体取决于手头的任务。
- en: At this point, we need to parse through the characters in the text and provide
    a one-hot encoding for each of them. Each character will be represented by a vector
    of length equal to the number of different characters in the encoding. This vector
    will contain all zeros except a one at the index corresponding to the location
    of the character in the encoding.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们需要遍历文本中的字符，并为每个字符提供一个独热编码。每个字符将由一个长度等于编码中不同字符数的向量表示。这个向量将包含除了在编码中字符位置对应的索引处的一个之外的所有零。
- en: 'We first split our text into a list of lines and pick an arbitrary line to
    focus on:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将文本拆分为一系列行，并选择一个任意的行进行关注：
- en: '[PRE38]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let’s create a tensor that can hold the total number of one-hot-encoded characters
    for the whole line:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个能够容纳整行所有独热编码字符总数的张量：
- en: '[PRE39]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ 128 hardcoded due to the limits of ASCII
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于ASCII的限制，128个硬编码
- en: 'Note that `letter_t` holds a one-hot-encoded character per row. Now we just
    have to set a one on each row in the correct position so that each row represents
    the correct character. The index where the one has to be set corresponds to the
    index of the character in the encoding:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`letter_t`每行保存一个独热编码字符。现在我们只需在正确位置的每行设置一个1，以便每行表示正确的字符。其中1应设置的索引对应于编码中字符的索引：
- en: '[PRE40]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ The text uses directional double quotes, which are not valid ASCII, so we
    screen them out here.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 文本使用方向性双引号，这不是有效的ASCII，因此我们在这里将其筛选掉。
- en: 4.5.3 One-hot encoding whole words
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.3 对整个单词进行独热编码
- en: We have one-hot encoded our sentence into a representation that a neural network
    could digest. Word-level encoding can be done the same way by establishing a vocabulary
    and one-hot encoding sentences--sequences of words--along the rows of our tensor.
    Since a vocabulary has many words, this will produce very wide encoded vectors,
    which may not be practical. We will see in the next section that there is a more
    efficient way to represent text at the word level, using *embeddings*. For now,
    let’s stick with one-hot encodings and see what happens.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已��将我们的句子进行了独热编码，以便神经网络可以理解。单词级别的编码可以通过建立词汇表并对句子--单词序列--进行独热编码来完成。由于词汇表有很多单词，这将产生非常宽的编码向量，这可能不太实用。我们将在下一节看到，在单词级别表示文本有一种更有效的方法，即使用*嵌入*。现在，让我们继续使用独热编码，看看会发生什么。
- en: 'We’ll define `clean_words`, which takes text and returns it in lowercase and
    stripped of punctuation. When we call it on our “Impossible, Mr. Bennet” `line`,
    we get the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义`clean_words`，它接受文本并以小写形式返回，并去除标点。当我们在我们的“不可能，本内特先生”`line`上调用它时，我们得到以下结果：
- en: '[PRE41]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, let’s build a mapping of words to indexes in our encoding:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们构建一个单词到编码索引的映射：
- en: '[PRE42]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Note that `word2index_dict` is now a dictionary with words as keys and an integer
    as a value. We will use it to efficiently find the index of a word as we one-hot
    encode it. Let’s now focus on our sentence: we break it up into words and one-hot
    encode it--that is, we populate a tensor with one one-hot-encoded vector per word.
    We create an empty vector and assign the one-hot-encoded values of the word in
    the sentence:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`word2index_dict`现在是一个以单词为键、整数为值的字典。我们将使用它来高效地找到一个单词的索引，因为我们对其进行独热编码。现在让我们专注于我们的句子：我们将其分解为单词，并对其进行独热编码--也就是说，我们为每个单词填充一个独热编码向量的张量。我们创建一个空向量，并为句子中的单词分配独热编码值：
- en: '[PRE43]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: At this point, `tensor` represents one sentence of length 11 in an encoding
    space of size 7,261, the number of words in our dictionary. Figure 4.6 compares
    the gist of our two options for splitting text (and using the embeddings we’ll
    look at in the next section).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`tensor`在大小为7,261的编码空间中表示了一个长度为11的句子，这是我们字典中的单词数。图4.6比较了我们拆分文本的两种选项的要点（以及我们将在下一节中看到的嵌入的使用）。
- en: 'The choice between character-level and word-level encoding leaves us to make
    a trade-off. In many languages, there are significantly fewer characters than
    words: representing characters has us representing just a few classes, while representing
    words requires us to represent a very large number of classes and, in any practical
    application, deal with words that are not in the dictionary. On the other hand,
    words convey much more meaning than individual characters, so a representation
    of words is considerably more informative by itself. Given the stark contrast
    between these two options, it is perhaps unsurprising that intermediate ways have
    been sought, found, and applied with great success: for example, the *byte pair
    encoding* method[⁶](#pgfId-1021205) starts with a dictionary of individual letters
    but then iteratively adds the most frequently observed pairs to the dictionary
    until it reaches a prescribed dictionary size. Our example sentence might then
    be split into tokens like this:[⁷](#pgfId-1021220)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 字符级别和单词级别编码之间的选择让我们需要做出权衡。在许多语言中，字符比单词要少得多：表示字符让我们只表示几个类别，而表示单词则要求我们表示非常多的类别，并且在任何实际应用中，要处理字典中不存在的单词。另一方面，单词传达的意义比单个字符要多得多，因此单词的表示本身就更具信息量。鉴于这两种选择之间的鲜明对比，或许并不奇怪中间方法已经被寻找、发现并成功应用：例如，*字节对编码*方法[⁶](#pgfId-1021205)从一个包含单个字母的字典开始，然后迭代地将最常见的对添加到字典中，直到达到规定的字典���小。我们的示例句子可能会被分割成这样的标记:[⁷](#pgfId-1021220)
- en: '[PRE44]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](../Images/CH04_F06_Stevens2_GS.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F06_Stevens2_GS.png)'
- en: Figure 4.6 Three ways to encode a word
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 编码单词的三种方式
- en: For most things, our mapping is just splitting by words. But the rarer parts--the
    capitalized *Impossible* and the name Bennet--are composed of subunits.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数内容，我们的映射只是按单词拆分。但是，较少见的部分--大写的*Impossible*和名字Bennet--由子单元组成。
- en: 4.5.4 Text embeddings
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.4 文本嵌入
- en: One-hot encoding is a very useful technique for representing categorical data
    in tensors. However, as we have anticipated, one-hot encoding starts to break
    down when the number of items to encode is effectively unbound, as with words
    in a corpus. In just one book, we had over 7,000 items!
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码是一种在张量中表示分类数据的非常有用的技术。然而，正如我们预料的那样，当要编码的项目数量实际上是无限的时，独热编码开始失效，就像语料库中的单词一样。在仅仅一本书中，我们就有超过7,000个项目！
- en: We certainly could do some work to deduplicate words, condense alternate spellings,
    collapse past and future tenses into a single token, and that kind of thing. Still,
    a general-purpose English-language encoding would be *huge*. Even worse, every
    time we encountered a new word, we would have to add a new column to the vector,
    which would mean adding a new set of weights to the model to account for that
    new vocabulary entry--which would be painful from a training perspective.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然可以做一些工作来去重单词，压缩替代拼写，将过去和未来时态合并为一个标记，等等。但是，一个通用的英语编码将会*非常庞大*。更糟糕的是，每当我们遇到一个新单词时，我们都需要向向量中添加一个新列，这意味着需要向模型添加一组新的权重来解释这个新的词汇条目--这将从训练的角度来看是痛苦的。
- en: How can we compress our encoding down to a more manageable size and put a cap
    on the size growth? Well, instead of vectors of many zeros and a single one, we
    can use vectors of floating-point numbers. A vector of, say, 100 floating-point
    numbers can indeed represent a large number of words. The trick is to find an
    effective way to map individual words into this 100-dimensional space in a way
    that facilitates downstream learning. This is called an *embedding*.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将我们的编码压缩到一个更易管理的大小，并限制大小增长？嗯，与其使用许多零和一个单一的向量，我们可以使用浮点数向量。比如，一个包含100个浮点数的向量确实可以表示大量的单词。关键是要找到一种有效的方法，以便将单个单词映射到这个100维空间，从而促进下游学习。这被称为*嵌入*。
- en: In principle, we could simply iterate over our vocabulary and generate a set
    of 100 random floating-point numbers for each word. This would work, in that we
    could cram a very large vocabulary into just 100 numbers, but it would forgo any
    concept of distance between words based on meaning or context. A model using this
    word embedding would have to deal with very little structure in its input vectors.
    An ideal solution would be to generate the embedding in such a way that words
    used in similar contexts mapped to nearby regions of the embedding.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，我们可以简单地遍历我们的词汇表，并为每个单词生成一组100个随机浮点数。这样做是可以的，因为我们可以将一个非常庞大的词汇表压缩到只有100个数字，但它将放弃基于含义或上下文的单词之间距离的概念。使用这种单词嵌入的模型将不得不处理其输入向量中的非常少的结构。一个理想的解决方案是以这样一种方式生成嵌入，使得在相似上下文中使用的单词映射到嵌入的附近区域。
- en: Well, if we were to design a solution to this problem by hand, we might decide
    to build our embedding space by choosing to map basic nouns and adjectives along
    the axes. We can generate a 2D space where axes map to nouns--*fruit* (0.0-0.33),
    *flower* (0.33-0.66), and *dog* (0.66-1.0)--and adjectives--*red* (0.0-0.2), *orange*
    (0.2-0.4), *yellow* (0.4-0.6), *white* (0.6-0.8), and *brown* (0.8-1.0). Our goal
    is to take actual fruit, flowers, and dogs and lay them out in the embedding.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，如果我们要手动设计一个解决这个问题的解决方案，我们可能会决定通过选择将基本名词和形容词映射到轴上来构建我们的嵌入空间。我们可以生成一个 2D 空间，其中轴映射到名词--*水果*（0.0-0.33）、*花朵*（0.33-0.66）和*狗*（0.66-1.0）--和形容词--*红色*（0.0-0.2）、*橙色*（0.2-0.4）、*黄色*（0.4-0.6）、*白色*（0.6-0.8）和*棕色*（0.8-1.0）。我们的目标是将实际的水果、花朵和狗放在嵌入中。
- en: As we start embedding words, we can map *apple* to a number in the *fruit* and
    *red* quadrant. Likewise, we can easily map *tangerine*, *lemon*, *lychee*, and
    *kiwi* (to round out our list of colorful fruits). Then we can start on flowers,
    and assign *rose*, *poppy*, *daffodil*, *lily*, and ... Hmm. Not many brown flowers
    out there. Well, *sunflower* can get *flower*, *yellow*, and *brown*, and then
    *daisy* can get *flower*, *white*, and *yellow*. Perhaps we should update *kiwi*
    to map close to *fruit*, *brown*, and *green*.[⁸](#pgfId-1021707) For dogs and
    color, we can embed *redbone* near *red*; uh, *fox* perhaps for *orange*; *golden
    retriever* for *yellow*, *poodle* for *white*, and ... most kinds of dogs are
    *brown*.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始嵌入词时，我们可以将*苹果*映射到*水果*和*红色*象限中的一个数字。同样，我们可以轻松地将*橘子*、*柠檬*、*荔枝*和*猕猴桃*（补充我们的五彩水果列表）进行映射。然后我们可以开始处理花朵，并将*玫瑰*、*罂粟花*、*水仙花*、*百合花*等映射...嗯。不太多棕色的花朵。好吧，*向日葵*可以被映射到*花朵*、*黄色*和*棕色*，然后*雏菊*可以被映射到*花朵*、*白色*和*黄色*。也许我们应该将*猕猴桃*更新为接近*水果*、*棕色*和*绿色*的映射。对于狗和颜色，我们可以将*红骨*映射到*红色*附近；嗯，也许*狐狸*可以代表*橙色*；*金毛寻回犬*代表*黄色*，*贵宾犬*代表*白色*，以及...大多数狗都是*棕色*。
- en: Now our embeddings look like figure 4.7\. While doing this manually isn’t really
    feasible for a large corpus, note that although we had an embedding size of 2,
    we described 15 different words *besides the base 8* and could probably cram in
    quite a few more if we took the time to be creative about it.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的嵌入看起来像图 4.7。虽然对于大型语料库来说手动操作并不可行，但请注意，尽管我们的嵌入大小为 2，除了基本的 8 个词之外，我们描述了 15
    个不同的词，并且如果我们花时间进行创造性思考，可能还能塞进更多词。
- en: '![](../Images/CH04_F07_Stevens2_GS.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F07_Stevens2_GS.png)'
- en: Figure 4.7 Our manual word embeddings
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 我们的手动词嵌入
- en: 'As you’ve probably already guessed, this kind of work can be automated. By
    processing a large corpus of organic text, embeddings similar to the one we just
    discussed can be generated. The main differences are that there are 100 to 1,000
    elements in the embedding vector and that axes do not map directly to concepts:
    rather, conceptually similar words map in neighboring regions of an embedding
    space whose axes are arbitrary floating-point dimensions.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的那样，这种工作可以自动化。通过处理大量的有机文本语料库，类似我们刚刚讨论的嵌入可以生成。主要区别在于嵌入向量中有 100 到 1,000
    个元素，并且轴不直接映射到概念：相似概念的词在嵌入空间的相邻区域中映射，其轴是任意的浮点维度。
- en: While the exact algorithms[⁹](#pgfId-1022022) used are a bit out of scope for
    what we’re wanting to focus on here, we’d just like to mention that embeddings
    are often generated using neural networks, trying to predict a word from nearby
    words (the context) in a sentence. In this case, we could start from one-hot-encoded
    words and use a (usually rather shallow) neural network to generate the embedding.
    Once the embedding was available, we could use it for downstream tasks.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然具体的算法[⁹](#pgfId-1022022)有点超出了我们想要关注的范围，但我们想提一下，嵌入通常是使用神经网络生成的，试图从句子中附近的词（上下文）中预测一个词。在这种情况下，我们可以从单热编码的词开始，并使用一个（通常相当浅的）神经网络生成嵌入。一旦嵌入可用，我们就可以将其用于下游任务。
- en: One interesting aspect of the resulting embeddings is that similar words end
    up not only clustered together, but also having consistent spatial relationships
    with other words. For example, if we were to take the embedding vector for *apple*
    and begin to add and subtract the vectors for other words, we could begin to perform
    analogies like *apple* - *red* - *sweet* + *yellow* + *sour* and end up with a
    vector very similar to the one for *lemon*.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 结果嵌入的一个有趣方面是相似的词不仅聚集在一起，而且与其他词有一致的空间关系。例如，如果我们取*苹果*的嵌入向量，并开始加减其他词的向量，我们可以开始执行类似*苹果*-*红色*-*甜*+*黄色*+*酸*的类比，最终得到一个与*柠檬*的向量非常相似的向量。
- en: 'More contemporary embedding models--with BERT and GPT-2 making headlines even
    in mainstream media--are much more elaborate and are context sensitive: that is,
    the mapping of a word in the vocabulary to a vector is not fixed but depends on
    the surrounding sentence. Yet they are often used just like the simpler *classic*
    embeddings we’ve touched on here.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 更现代的嵌入模型--BERT 和 GPT-2 甚至在主流媒体中都引起轰动--更加复杂且具有上下文敏感性：也就是说，词汇表中的一个词到向量的映射不是固定的，而是取决于周围的句子。然而，它们通常像我们在这里提到的更简单的*经典*嵌入一样使用。
- en: 4.5.5 Text embeddings as a blueprint
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.5 文本嵌入作为蓝图
- en: Embeddings are an essential tool for when a large number of entries in the vocabulary
    have to be represented by numeric vectors. But we won’t be using text and text
    embeddings in this book, so you might wonder why we introduce them here. We believe
    that how text is represented and processed can also be seen as an example for
    dealing with categorical data in general. Embeddings are useful wherever one-hot
    encoding becomes cumbersome. Indeed, in the form described previously, they are
    an efficient way of representing one-hot encoding immediately followed by multiplication
    with the matrix containing the embedding vectors as rows.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是一种必不可少的工具，当词汇表中有大量条目需要用数字向量表示时。但在本书中我们不会使用文本和文本嵌入，所以您可能会想知道为什么我们在这里介绍它们。我们认为文本如何表示和处理也可以看作是处理分类数据的一个示例。嵌入在独热编码变得繁琐的地方非常有用。事实上，在先前描述的形式中，它们是一种表示独热编码并立即乘以包含嵌入向量的矩阵的有效方式。
- en: In non-text applications, we usually do not have the ability to construct the
    embeddings beforehand, but we will start with the random numbers we eschewed earlier
    and consider improving them part of our learning problem. This is a standard technique--so
    much so that embeddings are a prominent alternative to one-hot encodings for any
    categorical data. On the flip side, even when we deal with text, improving the
    prelearned embeddings while solving the problem at hand has become a common practice.[^(10)](#pgfId-1022178)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在非文本应用中，我们通常没有能力事先构建嵌入，但我们将从之前避开的随机数开始，并考虑将其改进作为我们学习问题的一部分。这是一种标准技术--以至于嵌入是任何分类数据的独热编码的一个突出替代方案。另一方面，即使我们处理文本，改进预先学习的嵌入在解决手头问题时已经成为一种常见做法。[^(10)](#pgfId-1022178)
- en: When we are interested in co-occurrences of observations, the word embeddings
    we saw earlier can serve as a blueprint, too. For example, recommender systems--customers
    who liked our book also bought ...--use the items the customer already interacted
    with as the context for predicting what else will spark interest. Similarly, processing
    text is perhaps the most common, well-explored task dealing with sequences; so,
    for example, when working on tasks with time series, we might look for inspiration
    in what is done in natural language processing.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对观察结果的共现感兴趣时，我们之前看到的词嵌入也可以作为一个蓝图。例如，推荐系统--喜欢我们的书的客户也购买了...--使用客户已经互动过的项目作为预测其他可能引起兴趣的上下文。同样，处理文本可能是最常见、最深入研究序列的任务；因此，例如，在处理时间序列任务时，我们可能会从自然语言处理中所做的工作中寻找灵感。
- en: 4.6 Conclusion
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 结论
- en: We’ve covered a lot of ground in this chapter. We learned to load the most common
    types of data and shape them for consumption by a neural network. Of course, there
    are more data formats in the wild than we could hope to describe in a single volume.
    Some, like medical histories, are too complex to cover here. Others, like audio
    and video, were deemed less crucial for the path of this book. If you’re interested,
    however, we provide short examples of audio and video tensor creation in bonus
    Jupyter Notebooks provided on the book’s website ([www.manning.com/books/deep-learning-with-pytorch](https://www.manning.com/books/deep-learning-with-pytorch))
    and in our code repository ([https://github.com/deep-learning-with-pytorch/dlwpt-code/
    tree/master/p1ch4](https://github.com/deep-learning-with-pytorch/dlwpt-code/tree/master/p1ch4)).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了很多内容。我们学会了加载最常见的数据类型并将其塑造为神经网络可以消费的形式。当然，现实中的数据格式比我们在一本书中描述的要多得多。有些，如医疗史，太复杂了，无法在此处涵盖。其他一些，如音频和视频，被认为对本书的路径不那么关键。然而，如果您感兴趣，我们在书的网站（[www.manning.com/books/deep-learning-with-pytorch](https://www.manning.com/books/deep-learning-with-pytorch)）和我们的代码库（[https://github.com/deep-learning-with-pytorch/dlwpt-code/
    tree/master/p1ch4](https://github.com/deep-learning-with-pytorch/dlwpt-code/tree/master/p1ch4)）提供了音频和视频张量创建的简短示例。
- en: 'Now that we’re familiar with tensors and how to store data in them, we can
    move on to the next step towards the goal of the book: teaching you to train deep
    neural networks! The next chapter covers the mechanics of learning for simple
    linear models.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们熟悉了张量以及如何在其中存储数据，我们可以继续迈向本书目标的下一步：教会你训练深度神经网络！下一章将涵盖简单线性模型的学习机制。
- en: 4.7 Exercises
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.7 练习
- en: Take several pictures of red, blue, and green items with your phone or other
    digital camera (or download some from the internet, if a camera isn’t available).
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用手机或其他数码相机拍摄几张红色、蓝色和绿色物品的照片（如果没有相机，则可以从互联网上下载一些）。
- en: Load each image, and convert it to a tensor.
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载每个图像，并将其转换为张量。
- en: For each image tensor, use the `.mean()` method to get a sense of how bright
    the image is.
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个图像张量，使用`.mean()`方法来了解图像的亮度。
- en: Take the mean of each channel of your images. Can you identify the red, green,
    and blue items from only the channel averages?
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取图像每个通道的平均值。您能仅通过通道平均值识别红色、绿色和蓝色物品吗？
- en: Select a relatively large file containing Python source code.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个包含Python源代码的相对较大的文件。
- en: Build an index of all the words in the source file (feel free to make your tokenization
    as simple or as complex as you like; we suggest starting with replacing `r"[^a-zA-Z0-9_]+"`
    with spaces).
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建源文件中所有单词的索引（随意将您的标记化设计得简单或复杂；我们建议从用空格替换`r"[^a-zA-Z0-9_]+"`开始）。
- en: Compare your index with the one we made for *Pride and Prejudice*. Which is
    larger?
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的索引与我们为*傲慢与偏见*制作的索引进行比较。哪个更大？
- en: Create the one-hot encoding for the source code file.
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为源代码文件创建独热编码。
- en: What information is lost with this encoding? How does that information compare
    to what’s lost in the *Pride and Prejudice* encoding?
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这种编码会丢失哪些信息？这些信息丢失与*傲慢与偏见*编码中丢失的信息相比如何？
- en: 4.8 Summary
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8 总结
- en: Neural networks require data to be represented as multidimensional numerical
    tensors, often 32-bit floating-point.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络要求数据表示为多维数值张量，通常是32位浮点数。
- en: In general, PyTorch expects data to be laid out along specific dimensions according
    to the model architecture--for example, convolutional versus recurrent. We can
    reshape data effectively with the PyTorch tensor API.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般来说，PyTorch 期望数据根据模型架构沿特定维度布局--例如，卷积与循环。我们可以使用 PyTorch 张量 API 有效地重塑数据。
- en: Thanks to how the PyTorch libraries interact with the Python standard library
    and surrounding ecosystem, loading the most common types of data and converting
    them to PyTorch tensors is convenient.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 PyTorch 库与 Python 标准库及周围生态系统的互动方式，加载最常见类型的数据并将其转换为 PyTorch 张量非常方便。
- en: Images can have one or many channels. The most common are the red-green-blue
    channels of typical digital photos.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像可以有一个或多个通道。最常见的是典型数字照片的红绿蓝通道。
- en: Many images have a per-channel bit depth of 8, though 12 and 16 bits per channel
    are not uncommon. These bit depths can all be stored in a 32-bit floating-point
    number without loss of precision.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多图像的每个通道的位深度为 8，尽管每个通道的 12 和 16 位并不罕见。这些位深度都可以存储在 32 位浮点数中而不会丢失精度。
- en: Single-channel data formats sometimes omit an explicit channel dimension.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单通道数据格式有时会省略显式通道维度。
- en: Volumetric data is similar to 2D image data, with the exception of adding a
    third dimension (depth).
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体积数据类似于 2D 图像数据，唯一的区别是添加了第三个维度（深度）。
- en: Converting spreadsheets to tensors can be very straightforward. Categorical-
    and ordinal-valued columns should be handled differently from interval-valued
    columns.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将电子表格转换为张量可能非常简单。分类和有序值列应与间隔值列处理方式不同。
- en: Text or categorical data can be encoded to a one-hot representation through
    the use of dictionaries. Very often, embeddings give good, efficient representations.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本或分类数据可以通过使用字典编码为一热表示。很多时候，嵌入提供了良好且高效的表示。
- en: '* * *'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '^(1.)This is something of an understatement: [https://en.wikipedia.org/wiki/Color_model](https://en.wikipedia.org/wiki/Color_model).'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点轻描淡写：[https://en.wikipedia.org/wiki/Color_model](https://en.wikipedia.org/wiki/Color_model)。
- en: '^(2.)From the Cancer Imaging Archive’s CPTAC-LSCC collection: [http://mng.bz/K21K](http://mng.bz/K21K).'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 来自癌症影像存档的 CPTAC-LSCC 集合：[http://mng.bz/K21K](http://mng.bz/K21K)。
- en: ^(3.)As a starting point for a more in-depth discussion, refer to [https://en.wikipedia.org/wiki/Level_of_measurement](https://en.wikipedia.org/wiki/Level_of_measurement).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 作为更深入讨论的起点，请参考 [https://en.wikipedia.org/wiki/Level_of_measurement](https://en.wikipedia.org/wiki/Level_of_measurement)。
- en: ^(4.)This could also be a case where it is useful to go beyond the main path.
    Speculatively, we could also try to reflect *like categorical, but with order*
    more directly by generalizing one-hot encodings to mapping the *i*th of our four
    categories here to a vector that has ones in the positions 0...*i* and zeros beyond
    that. Or--similar to the embeddings we discussed in section 4.5.4--we could take
    partial sums of embeddings, in which case it might make sense to make those positive.
    As with many things we encounter in practical work, this could be a place where
    *trying what works for others* and then experimenting in a systematic fashion
    is a good idea.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可能是一个超越主要路径的情况。可以尝试将一热编码推广到将我们这里的四个类别中的第*i*个映射到一个向量，该向量在位置 0...*i* 有一个，其他位置为零。或者--类似于我们在第
    4.5.4 节讨论的嵌入--我们可以取嵌入的部分和，这种情况下可能有意义将其设为正值。与我们在实际工作中遇到的许多事物一样，这可能是一个*尝试他人有效方法*然后以系统化方式进行实验的好地方。
- en: '^(5.)Nadkarni et al., “Natural language processing: an introduction,” JAMIA,
    [http://mng.bz/8pJP](http://mng.bz/8pJP). See also [https://en.wikipedia.org/wiki/Natural-language_processing](https://en.wikipedia.org/wiki/Natural-language_processing).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Nadkarni 等人，“自然语言处理：简介”，JAMIA，[http://mng.bz/8pJP](http://mng.bz/8pJP)。另请参阅
    [https://en.wikipedia.org/wiki/Natural-language_processing](https://en.wikipedia.org/wiki/Natural-language_processing)。
- en: ^(6.)Most commonly implemented by the subword-nmt and SentencePiece libraries.
    The conceptual drawback is that the representation of a sequence of characters
    is no longer unique.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 最常由 subword-nmt 和 SentencePiece 库实现。概念上的缺点是字符序列的表示不再是唯一的。
- en: ^(7.)This is from a SentencePiece tokenizer trained on a machine translation
    dataset.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从一个在机器翻译数据集上训练的 SentencePiece 分词器。
- en: ^(8.)Actually, with our 1D view of color, this is not possible, as *sunflower*’s
    *yellow* and *brown* will average to *white* --but you get the idea, and it does
    work better in higher dimensions.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，通过我们对颜色的一维观点，这是不可能的，因为*向日葵*的*黄色*和*棕色*会平均为*白色*--但你明白我的意思，而且在更高维度下效果更好。
- en: '^(9.)One example is word2vec: [https://code.google.com/archive/p/word2vec](https://code.google.com/archive/p/word2vec).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '一个例子是 word2vec: [https://code.google.com/archive/p/word2vec](https://code.google.com/archive/p/word2vec)。'
- en: ^(10.)This goes by the name *fine-tuning*.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为*微调*。
