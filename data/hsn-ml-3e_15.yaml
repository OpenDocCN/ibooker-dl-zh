- en: Chapter 13\. Loading and Preprocessing Data with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 13 章。使用 TensorFlow 加载和预处理数据
- en: In [Chapter 2](ch02.html#project_chapter), you saw that loading and preprocessing
    data is an important part of any machine learning project. You used Pandas to
    load and explore the (modified) California housing dataset—which was stored in
    a CSV file—and you applied Scikit-Learn’s transformers for preprocessing. These
    tools are quite convenient, and you will probably be using them often, especially
    when exploring and experimenting with data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 2 章](ch02.html#project_chapter)中，您看到加载和预处理数据是任何机器学习项目的重要部分。您使用 Pandas 加载和探索（修改后的）加利福尼亚房屋数据集——该数据集存储在
    CSV 文件中——并应用 Scikit-Learn 的转换器进行预处理。这些工具非常方便，您可能会经常使用它们，特别是在探索和实验数据时。
- en: However, when training TensorFlow models on large datasets, you may prefer to
    use TensorFlow’s own data loading and preprocessing API, called *tf.data*. It
    is capable of loading and preprocessing data extremely efficiently, reading from
    multiple files in parallel using multithreading and queuing, shuffling and batching
    samples, and more. Plus, it can do all of this on the fly—it loads and preprocesses
    the next batch of data across multiple CPU cores, while your GPUs or TPUs are
    busy training the current batch of data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大型数据集上训练 TensorFlow 模型时，您可能更喜欢使用 TensorFlow 自己的数据加载和预处理 API，称为*tf.data*。它能够非常高效地加载和预处理数据，使用多线程和排队从多个文件中并行读取数据，对样本进行洗牌和分批处理等。此外，它可以实时执行所有这些操作——在
    GPU 或 TPU 正在训练当前批次数据时，它会在多个 CPU 核心上加载和预处理下一批数据。
- en: The tf.data API lets you handle datasets that don’t fit in memory, and it allows
    you to make full use of your hardware resources, thereby speeding up training.
    Off the shelf, the tf.data API can read from text files (such as CSV files), binary
    files with fixed-size records, and binary files that use TensorFlow’s TFRecord
    format, which supports records of varying sizes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data API 允许您处理无法放入内存的数据集，并充分利用硬件资源，从而加快训练速度。tf.data API 可以直接从文本文件（如 CSV 文件）、具有固定大小记录的二进制文件以及使用
    TensorFlow 的 TFRecord 格式的二进制文件中读取数据。
- en: TFRecord is a flexible and efficient binary format usually containing protocol
    buffers (an open source binary format). The tf.data API also has support for reading
    from SQL databases. Moreover, many open source extensions are available to read
    from all sorts of data sources, such as Google’s BigQuery service (see [*https://tensorflow.org/io*](https://tensorflow.org/io)).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: TFRecord 是一种灵活高效的二进制格式，通常包含协议缓冲区（一种开源二进制格式）。tf.data API 还支持从 SQL 数据库中读取数据。此外，许多开源扩展可用于从各种数据源中读取数据，例如
    Google 的 BigQuery 服务（请参阅[*https://tensorflow.org/io*](https://tensorflow.org/io)）。
- en: 'Keras also comes with powerful yet easy-to-use preprocessing layers that can
    be embedded in your models: this way, when you deploy a model to production, it
    will be able to ingest raw data directly, without you having to add any additional
    preprocessing code. This eliminates the risk of mismatch between the preprocessing
    code used during training and the preprocessing code used in production, which
    would likely cause *training/serving skew*. And if you deploy your model in multiple
    apps coded in different programming languages, you won’t have to reimplement the
    same preprocessing code multiple times, which also reduces the risk of mismatch.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 还提供了强大而易于使用的预处理层，可以嵌入到您的模型中：这样，当您将模型部署到生产环境时，它将能够直接摄取原始数据，而无需您添加任何额外的预处理代码。这消除了训练期间使用的预处理代码与生产中使用的预处理代码之间不匹配的风险，这可能会导致*训练/服务偏差*。如果您将模型部署在使用不同编程语言编写的多个应用程序中，您不必多次重新实现相同的预处理代码，这也减少了不匹配的风险。
- en: As you will see, both APIs can be used jointly—for example, to benefit from
    the efficient data loading offered by tf.data and the convenience of the Keras
    preprocessing layers.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将看到的，这两个 API 可以联合使用——例如，从 tf.data 提供的高效数据加载和 Keras 预处理层的便利性中受益。
- en: In this chapter, we will first cover the tf.data API and the TFRecord format.
    Then we will explore the Keras preprocessing layers and how to use them with the
    tf.data API. Lastly, we will take a quick look at a few related libraries that
    you may find useful for loading and preprocessing data, such as TensorFlow Datasets
    and TensorFlow Hub. So, let’s get started!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先介绍 tf.data API 和 TFRecord 格式。然后我们将探索 Keras 预处理层以及如何将它们与 tf.data API
    一起使用。最后，我们将快速查看一些相关的库，您可能会发现它们在加载和预处理数据时很有用，例如 TensorFlow Datasets 和 TensorFlow
    Hub。所以，让我们开始吧！
- en: The tf.data API
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tf.data API
- en: 'The whole tf.data API revolves around the concept of a `tf.data.Dataset`: this
    represents a sequence of data items. Usually you will use datasets that gradually
    read data from disk, but for simplicity let’s create a dataset from a simple data
    tensor using `tf.data.Dataset.from_tensor_slices()`:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 整个 tf.data API 围绕着 `tf.data.Dataset` 的概念展开：这代表了一系列数据项。通常，您会使用逐渐从磁盘读取数据的数据集，但为了简单起见，让我们使用
    `tf.data.Dataset.from_tensor_slices()` 从一个简单的数据张量创建数据集：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `from_tensor_slices()` function takes a tensor and creates a `tf.data.Dataset`
    whose elements are all the slices of `X` along the first dimension, so this dataset
    contains 10 items: tensors 0, 1, 2, …​, 9\. In this case we would have obtained
    the same dataset if we had used `tf.data.Dataset.range(10)` (except the elements
    would be 64-bit integers instead of 32-bit integers).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`from_tensor_slices()` 函数接受一个张量，并创建一个 `tf.data.Dataset`，其中的元素是沿着第一维度的所有 `X`
    的切片，因此这个数据集包含 10 个项目：张量 0、1、2、…​、9。在这种情况下，如果我们使用 `tf.data.Dataset.range(10)`，我们将获得相同的数据集（除了元素将是
    64 位整数而不是 32 位整数）。'
- en: 'You can simply iterate over a dataset’s items like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以简单地迭代数据集的项目，如下所示：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The tf.data API is a streaming API: you can very efficiently iterate through
    a dataset’s items, but the API is not designed for indexing or slicing.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data API 是一个流式 API：您可以非常高效地迭代数据集的项目，但该 API 不适用于索引或切片。
- en: 'A dataset may also contain tuples of tensors, or dictionaries of name/tensor
    pairs, or even nested tuples and dictionaries of tensors. When slicing a tuple,
    a dictionary, or a nested structure, the dataset will only slice the tensors it
    contains, while preserving the tuple/dictionary structure. For example:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集还可以包含张量的元组，或名称/张量对的字典，甚至是张量的嵌套元组和字典。在对元组、字典或嵌套结构进行切片时，数据集将仅切片它包含的张量，同时保留元组/字典结构。例如：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Chaining Transformations
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链接转换
- en: 'Once you have a dataset, you can apply all sorts of transformations to it by
    calling its transformation methods. Each method returns a new dataset, so you
    can chain transformations like this (this chain is illustrated in [Figure 13-1](#chaining_transformations_diagram)):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了数据集，您可以通过调用其转换方法对其应用各种转换。每个方法都会返回一个新的数据集，因此您可以像这样链接转换（此链在[图13-1](#chaining_transformations_diagram)中有示例）：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, we first call the `repeat()` method on the original dataset,
    and it returns a new dataset that repeats the items of the original dataset three
    times. Of course, this will not copy all the data in memory three times! If you
    call this method with no arguments, the new dataset will repeat the source dataset
    forever, so the code that iterates over the dataset will have to decide when to
    stop.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们首先在原始数据集上调用`repeat()`方法，它返回一个将原始数据集的项目重复三次的新数据集。当然，这不会将所有数据在内存中复制三次！如果您调用此方法而没有参数，新数据集将永远重复源数据集，因此迭代数据集的代码将不得不决定何时停止。
- en: Then we call the `batch()` method on this new dataset, and again this creates
    a new dataset. This one will group the items of the previous dataset in batches
    of seven items.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在这个新数据集上调用`batch()`方法，再次创建一个新数据集。这个新数据集将把前一个数据集的项目分组成七个项目一组的批次。
- en: '![mls3 1301](assets/mls3_1301.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1301](assets/mls3_1301.png)'
- en: Figure 13-1\. Chaining dataset transformations
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-1\. 链接数据集转换
- en: Finally, we iterate over the items of this final dataset. The `batch()` method
    had to output a final batch of size two instead of seven, but you can call `batch()`
    with `drop_remainder=True` if you want it to drop this final batch, such that
    all batches have the exact same size.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们迭代这个最终数据集的项目。`batch()`方法必须输出一个大小为两而不是七的最终批次，但是如果您希望删除这个最终批次，使所有批次具有完全相同的大小，可以调用`batch()`并使用`drop_remainder=True`。
- en: Warning
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The dataset methods do *not* modify datasets—they create new ones. So make sure
    to keep a reference to these new datasets (e.g., with `dataset = ...`), or else
    nothing will happen.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集方法*不会*修改数据集，它们会创建新的数据集。因此，请确保保留对这些新数据集的引用（例如，使用`dataset = ...`），否则什么也不会发生。
- en: 'You can also transform the items by calling the `map()` method. For example,
    this creates a new dataset with all batches multiplied by two:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过调用`map()`方法来转换项目。例如，这将创建一个所有批次乘以二的新数据集：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This `map()` method is the one you will call to apply any preprocessing to your
    data. Sometimes this will include computations that can be quite intensive, such
    as reshaping or rotating an image, so you will usually want to spawn multiple
    threads to speed things up. This can be done by setting the `num_parallel_calls`
    argument to the number of threads to run, or to `tf.data.AUTOTUNE`. Note that
    the function you pass to the `map()` method must be convertible to a TF function
    (see [Chapter 12](ch12.html#tensorflow_chapter)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`map()`方法是您将调用的方法，用于对数据进行任何预处理。有时这将包括一些可能相当密集的计算，比如重塑或旋转图像，因此您通常会希望启动多个线程以加快速度。这可以通过将`num_parallel_calls`参数设置为要运行的线程数，或者设置为`tf.data.AUTOTUNE`来完成。请注意，您传递给`map()`方法的函数必须可以转换为TF函数（请参阅[第12章](ch12.html#tensorflow_chapter)）。
- en: 'It is also possible to simply filter the dataset using the `filter()` method.
    For example, this code creates a dataset that only contains the batchs whose sum
    is greater than 50:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用`filter()`方法简单地过滤数据集。例如，此代码创建一个仅包含总和大于50的批次的数据集：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You will often want to look at just a few items from a dataset. You can use
    the `take()` method for that:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您经常会想查看数据集中的一些项目。您可以使用`take()`方法来实现：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Shuffling the Data
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据洗牌
- en: 'As we discussed in [Chapter 4](ch04.html#linear_models_chapter), gradient descent
    works best when the instances in the training set are independent and identically
    distributed (IID). A simple way to ensure this is to shuffle the instances, using
    the `shuffle()` method. It will create a new dataset that will start by filling
    up a buffer with the first items of the source dataset. Then, whenever it is asked
    for an item, it will pull one out randomly from the buffer and replace it with
    a fresh one from the source dataset, until it has iterated entirely through the
    source dataset. At this point it will continue to pull out items randomly from
    the buffer until it is empty. You must specify the buffer size, and it is important
    to make it large enough, or else shuffling will not be very effective.⁠^([1](ch13.html#idm45720190533488))
    Just don’t exceed the amount of RAM you have, though even if you have plenty of
    it, there’s no need to go beyond the dataset’s size. You can provide a random
    seed if you want the same random order every time you run your program. For example,
    the following code creates and displays a dataset containing the integers 0 to
    9, repeated twice, shuffled using a buffer of size 4 and a random seed of 42,
    and batched with a batch size of 7:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](ch04.html#linear_models_chapter)中讨论的，梯度下降在训练集中的实例是独立且同分布（IID）时效果最好。确保这一点的一个简单方法是对实例进行洗牌，使用`shuffle()`方法。它将创建一个新数据集，首先用源数据集的前几个项目填充缓冲区。然后，每当需要一个项目时，它将从缓冲区随机取出一个项目，并用源数据集中的新项目替换它，直到完全迭代源数据集。在这一点上，它将继续从缓冲区随机取出项目，直到缓冲区为空。您必须指定缓冲区大小，并且很重要的是要足够大，否则洗牌效果不会很好。⁠^([1](ch13.html#idm45720190533488))
    只是不要超出您拥有的RAM量，尽管即使您有很多RAM，也没有必要超出数据集的大小。如果您希望每次运行程序时都获得相同的随机顺序，可以提供一个随机种子。例如，以下代码创建并显示一个包含0到9的整数，重复两次，使用大小为4的缓冲区和随机种子42进行洗牌，并使用批次大小为7进行批处理的数据集：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tip
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you call `repeat()` on a shuffled dataset, by default it will generate a
    new order at every iteration. This is generally a good idea, but if you prefer
    to reuse the same order at each iteration (e.g., for tests or debugging), you
    can set `reshuffle_each_​itera⁠tion=False` when calling `shuffle()`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在打乱的数据集上调用`repeat()`，默认情况下它将在每次迭代时生成一个新的顺序。这通常是个好主意，但是如果您希望在每次迭代中重复使用相同的顺序（例如，用于测试或调试），可以在调用`shuffle()`时设置`reshuffle_each_​itera⁠tion=False`。
- en: 'For a large dataset that does not fit in memory, this simple shuffling-buffer
    approach may not be sufficient, since the buffer will be small compared to the
    dataset. One solution is to shuffle the source data itself (for example, on Linux
    you can shuffle text files using the `shuf` command). This will definitely improve
    shuffling a lot! Even if the source data is shuffled, you will usually want to
    shuffle it some more, or else the same order will be repeated at each epoch, and
    the model may end up being biased (e.g., due to some spurious patterns present
    by chance in the source data’s order). To shuffle the instances some more, a common
    approach is to split the source data into multiple files, then read them in a
    random order during training. However, instances located in the same file will
    still end up close to each other. To avoid this you can pick multiple files randomly
    and read them simultaneously, interleaving their records. Then on top of that
    you can add a shuffling buffer using the `shuffle()` method. If this sounds like
    a lot of work, don’t worry: the tf.data API makes all this possible in just a
    few lines of code. Let’s go over how you can do this.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个无法放入内存的大型数据集，这种简单的打乱缓冲区方法可能不够，因为缓冲区相对于数据集来说很小。一个解决方案是对源数据本身进行打乱（例如，在Linux上可以使用`shuf`命令对文本文件进行打乱）。这将显著改善打乱效果！即使源数据已经被打乱，通常也会希望再次打乱，否则每个时期将重复相同的顺序，模型可能会出现偏差（例如，由于源数据顺序中偶然存在的一些虚假模式）。为了进一步打乱实例，一个常见的方法是将源数据拆分为多个文件，然后在训练过程中以随机顺序读取它们。然而，位于同一文件中的实例仍然会相互靠近。为了避免这种情况，您可以随机选择多个文件并同时读取它们，交错它们的记录。然后在此基础上使用`shuffle()`方法添加一个打乱缓冲区。如果这听起来很费力，不用担心：tf.data
    API可以在几行代码中实现所有这些。让我们看看您可以如何做到这一点。
- en: Interleaving Lines from Multiple Files
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从多个文件中交错行
- en: 'First, suppose you’ve loaded the California housing dataset, shuffled it (unless
    it was already shuffled), and split it into a training set, a validation set,
    and a test set. Then you split each set into many CSV files that each look like
    this (each row contains eight input features plus the target median house value):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，假设您已经加载了加利福尼亚房屋数据集，对其进行了打乱（除非已经打乱），并将其分为训练集、验证集和测试集。然后将每个集合分成许多CSV文件，每个文件看起来像这样（每行包含八个输入特征加上目标中位房价）：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s also suppose `train_filepaths` contains the list of training filepaths
    (and you also have `valid_filepaths` and `test_filepaths`):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`train_filepaths`包含训练文件路径列表（您还有`valid_filepaths`和`test_filepaths`）：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Alternatively, you could use file patterns; for example, `train_filepaths =`
    `"datasets/housing/my_train_*.csv"`. Now let’s create a dataset containing only
    these filepaths:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用文件模式；例如，`train_filepaths =` `"datasets/housing/my_train_*.csv"`。现在让我们创建一个仅包含这些文件路径的数据集：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: By default, the `list_files()` function returns a dataset that shuffles the
    filepaths. In general this is a good thing, but you can set `shuffle=False` if
    you do not want that for some reason.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`list_files()`函数返回一个打乱文件路径的数据集。一般来说这是件好事，但是如果出于某种原因不想要这样，可以设置`shuffle=False`。
- en: 'Next, you can call the `interleave()` method to read from five files at a time
    and interleave their lines. You can also skip the first line of each file—which
    is the header row—using the `skip()` method):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以调用`interleave()`方法一次从五个文件中读取并交错它们的行。您还可以使用`skip()`方法跳过每个文件的第一行（即标题行）：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `interleave()` method will create a dataset that will pull five filepaths
    from the `filepath_dataset`, and for each one it will call the function you gave
    it (a lambda in this example) to create a new dataset (in this case a `TextLineDataset`).
    To be clear, at this stage there will be seven datasets in all: the filepath dataset,
    the interleave dataset, and the five `TextLineDataset`s created internally by
    the interleave dataset. When you iterate over the interleave dataset, it will
    cycle through these five `TextLineDataset`s, reading one line at a time from each
    until all datasets are out of items. Then it will fetch the next five filepaths
    from the `filepath_dataset` and interleave them the same way, and so on until
    it runs out of filepaths. For interleaving to work best, it is preferable to have
    files of identical length; otherwise the end of the longest file will not be interleaved.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`interleave()`方法将创建一个数据集，从`filepath_dataset`中提取五个文件路径，对于每个文件路径，它将调用您提供的函数（在本例中是lambda函数）来创建一个新的数据集（在本例中是`TextLineDataset`）。清楚地说，在这个阶段总共会有七个数据集：文件路径数据集、交错数据集以及交错数据集内部创建的五个`TextLineDataset`。当您迭代交错数据集时，它将循环遍历这五个`TextLineDataset`，从每个数据集中逐行读取，直到所有数据集都用完。然后它将从`filepath_dataset`中获取下一个五个文件路径，并以相同的方式交错它们，依此类推，直到文件路径用完。为了使交错效果最佳，最好拥有相同长度的文件；否则最长文件的末尾将不会被交错。'
- en: 'By default, `interleave()` does not use parallelism; it just reads one line
    at a time from each file, sequentially. If you want it to actually read files
    in parallel, you can set the `interleave()` method’s `num_parallel_calls` argument
    to the number of threads you want (recall that the `map()` method also has this
    argument). You can even set it to `tf.data.AUTOTUNE` to make TensorFlow choose
    the right number of threads dynamically based on the available CPU. Let’s look
    at what the dataset contains now:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`interleave()`不使用并行处理；它只是顺序地从每个文件中一次读取一行。如果您希望实际并行读取文件，可以将`interleave()`方法的`num_parallel_calls`参数设置为您想要的线程数（请记住，`map()`方法也有这个参数）。甚至可以将其设置为`tf.data.AUTOTUNE`，让TensorFlow根据可用的CPU动态选择正确的线程数。现在让我们看看数据集现在包含什么：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These are the first rows (ignoring the header row) of five CSV files, chosen
    randomly. Looks good!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是随机选择的五个 CSV 文件的第一行（忽略标题行）。看起来不错！
- en: Note
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'It’s possible to pass a list of filepaths to the `TextLineDataset` constructor:
    it will go through each file in order, line by line. If you also set the `num_parallel_reads`
    argument to a number greater than one, then the dataset will read that number
    of files in parallel and interleave their lines (without having to call the `interleave()`
    method). However, it will *not* shuffle the files, nor will it skip the header
    lines.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将文件路径列表传递给 `TextLineDataset` 构造函数：它将按顺序遍历每个文件的每一行。如果还将 `num_parallel_reads`
    参数设置为大于一的数字，那么数据集将并行读取该数量的文件，并交错它们的行（无需调用 `interleave()` 方法）。但是，它不会对文件进行洗牌，也不会跳过标题行。
- en: Preprocessing the Data
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Now that we have a housing dataset that returns each instance as a tensor containing
    a byte string, we need to do a bit of preprocessing, including parsing the strings
    and scaling the data. Let’s implement a couple custom functions that will perform
    this preprocessing:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个返回每个实例的住房数据集，其中包含一个字节字符串的张量，我们需要进行一些预处理，包括解析字符串和缩放数据。让我们实现一些自定义函数来执行这些预处理：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s walk through this code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步解释这段代码：
- en: First, the code assumes that we have precomputed the mean and standard deviation
    of each feature in the training set. `X_mean` and `X_std` are just 1D tensors
    (or NumPy arrays) containing eight floats, one per input feature. This can be
    done using a Scikit-Learn `StandardScaler` on a large enough random sample of
    the dataset. Later in this chapter, we will use a Keras preprocessing layer instead.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，代码假设我们已经预先计算了训练集中每个特征的均值和标准差。`X_mean` 和 `X_std` 只是包含八个浮点数的 1D 张量（或 NumPy
    数组），每个输入特征一个。可以使用 Scikit-Learn 的 `StandardScaler` 在数据集的足够大的随机样本上完成这个操作。在本章的后面，我们将使用
    Keras 预处理层来代替。
- en: 'The `parse_csv_line()` function takes one CSV line and parses it. To help with
    that, it uses the `tf.io.decode_csv()` function, which takes two arguments: the
    first is the line to parse, and the second is an array containing the default
    value for each column in the CSV file. This array (`defs`) tells TensorFlow not
    only the default value for each column, but also the number of columns and their
    types. In this example, we tell it that all feature columns are floats and that
    missing values should default to zero, but we provide an empty array of type `tf.float32`
    as the default value for the last column (the target): the array tells TensorFlow
    that this column contains floats, but that there is no default value, so it will
    raise an exception if it encounters a missing value.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse_csv_line()` 函数接受一个 CSV 行并对其进行解析。为了帮助实现这一点，它使用 `tf.io.decode_csv()` 函数，该函数接受两个参数：第一个是要解析的行，第二个是包含
    CSV 文件中每列的默认值的数组。这个数组（`defs`）告诉 TensorFlow 不仅每列的默认值是什么，还告诉它列的数量和类型。在这个例子中，我们告诉它所有特征列都是浮点数，缺失值应默认为零，但我们为最后一列（目标）提供了一个空的
    `tf.float32` 类型的默认值数组：该数组告诉 TensorFlow 这一列包含浮点数，但没有默认值，因此如果遇到缺失值，它将引发异常。'
- en: 'The `tf.io.decode_csv()` function returns a list of scalar tensors (one per
    column), but we need to return a 1D tensor array. So we call `tf.stack()` on all
    tensors except for the last one (the target): this will stack these tensors into
    a 1D array. We then do the same for the target value: this makes it a 1D tensor
    array with a single value, rather than a scalar tensor. The `tf.io.decode_csv()`
    function is done, so it returns the input features and the target.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.io.decode_csv()` 函数返回一个标量张量列表（每列一个），但我们需要返回一个 1D 张量数组。因此，我们对除最后一个（目标）之外的所有张量调用
    `tf.stack()`：这将这些张量堆叠成一个 1D 数组。然后我们对目标值做同样的操作：这将使其成为一个包含单个值的 1D 张量数组，而不是标量张量。`tf.io.decode_csv()`
    函数完成后，它将返回输入特征和目标。'
- en: Finally, the custom `preprocess()` function just calls the `parse_csv_line()`
    function, scales the input features by subtracting the feature means and then
    dividing by the feature standard deviations, and returns a tuple containing the
    scaled features and the target.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，自定义的 `preprocess()` 函数只调用 `parse_csv_line()` 函数，通过减去特征均值然后除以特征标准差来缩放输入特征，并返回一个包含缩放特征和目标的元组。
- en: 'Let’s test this preprocessing function:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试这个预处理函数：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Looks good! The `preprocess()` function can convert an instance from a byte
    string to a nice scaled tensor, with its corresponding label. We can now use the
    dataset’s `map()` method to apply the `preprocess()` function to each sample in
    the dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！`preprocess()` 函数可以将一个实例从字节字符串转换为一个漂亮的缩放张量，带有相应的标签。我们现在可以使用数据集的 `map()`
    方法将 `preprocess()` 函数应用于数据集中的每个样本。
- en: Putting Everything Together
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容放在一起
- en: 'To make the code more reusable, let’s put together everything we have discussed
    so far into another helper function; it will create and return a dataset that
    will efficiently load California housing data from multiple CSV files, preprocess
    it, shuffle it, and batch it (see [Figure 13-2](#input_pipeline_diagram)):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使代码更具重用性，让我们将迄今为止讨论的所有内容放在另一个辅助函数中；它将创建并返回一个数据集，该数据集将高效地从多个 CSV 文件中加载加利福尼亚房屋数据，对其进行预处理、洗牌和分批处理（参见[图
    13-2](#input_pipeline_diagram)）：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that we use the `prefetch()` method on the very last line. This is important
    for performance, as you will see now.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在最后一行使用了 `prefetch()` 方法。这对性能很重要，你现在会看到。
- en: '![mls3 1302](assets/mls3_1302.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1302](assets/mls3_1302.png)'
- en: Figure 13-2\. Loading and preprocessing data from multiple CSV files
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-2\. 从多个 CSV 文件加载和预处理数据
- en: Prefetching
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预取
- en: By calling `prefetch(1)` at the end of the custom `csv_reader_dataset()` function,
    we are creating a dataset that will do its best to always be one batch ahead.⁠^([2](ch13.html#idm45720189926208))
    In other words, while our training algorithm is working on one batch, the dataset
    will already be working in parallel on getting the next batch ready (e.g., reading
    the data from disk and preprocessing it). This can improve performance dramatically,
    as is illustrated in [Figure 13-3](#prefetching_diagram).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在自定义`csv_reader_dataset()`函数末尾调用`prefetch(1)`，我们正在创建一个数据集，该数据集将尽力始终领先一个批次。换句话说，当我们的训练算法在处理一个批次时，数据集将已经在并行工作，准备好获取下一个批次（例如，从磁盘读取数据并对其进行预处理）。这可以显著提高性能，如[图13-3](#prefetching_diagram)所示。
- en: 'If we also ensure that loading and preprocessing are multithreaded (by setting
    `num_parallel_calls` when calling `interleave()` and `map()`), we can exploit
    multiple CPU cores and hopefully make preparing one batch of data shorter than
    running a training step on the GPU: this way the GPU will be almost 100% utilized
    (except for the data transfer time from the CPU to the GPU⁠^([3](ch13.html#idm45720189876688))),
    and training will run much faster.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们还确保加载和预处理是多线程的（通过在调用`interleave()`和`map()`时设置`num_parallel_calls`），我们可以利用多个CPU核心，希望准备一个数据批次的时间比在GPU上运行训练步骤要短：这样GPU将几乎100%利用（除了从CPU到GPU的数据传输时间）[3]，训练将运行得更快。
- en: '![mls3 1303](assets/mls3_1303.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1303](assets/mls3_1303.png)'
- en: 'Figure 13-3\. With prefetching, the CPU and the GPU work in parallel: as the
    GPU works on one batch, the CPU works on the next'
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-3。通过预取，CPU和GPU并行工作：当GPU处理一个批次时，CPU处理下一个批次
- en: Tip
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you plan to purchase a GPU card, its processing power and its memory size
    are of course very important (in particular, a large amount of RAM is crucial
    for large computer vision or natural language processing models). Just as important
    for good performance is the GPU’s *memory bandwidth*; this is the number of gigabytes
    of data it can get into or out of its RAM per second.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划购买GPU卡，其处理能力和内存大小当然非常重要（特别是对于大型计算机视觉或自然语言处理模型，大量的RAM至关重要）。对于良好性能同样重要的是GPU的*内存带宽*；这是它每秒可以将多少千兆字节的数据进出其RAM。
- en: If the dataset is small enough to fit in memory, you can significantly speed
    up training by using the dataset’s `cache()` method to cache its content to RAM.
    You should generally do this after loading and preprocessing the data, but before
    shuffling, repeating, batching, and prefetching. This way, each instance will
    only be read and preprocessed once (instead of once per epoch), but the data will
    still be shuffled differently at each epoch, and the next batch will still be
    prepared in advance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集足够小，可以放入内存，您可以通过使用数据集的`cache()`方法将其内容缓存到RAM来显着加快训练速度。通常应在加载和预处理数据之后，但在洗牌、重复、批处理和预取之前执行此操作。这样，每个实例只会被读取和预处理一次（而不是每个时期一次），但数据仍然会在每个时期以不同的方式洗牌，下一批数据仍然会提前准备好。
- en: You have now learned how to build efficient input pipelines to load and preprocess
    data from multiple text files. We have discussed the most common dataset methods,
    but there are a few more you may want to look at, such as `concatenate()`, `zip()`,
    `window()`, `reduce()`, `shard()`, `flat_map()`, `apply()`, `unbatch()`, and `padded_batch()`.
    There are also a few more class methods, such as `from_generator()` and `from_​ten⁠sors()`,
    which create a new dataset from a Python generator or a list of tensors, respectively.
    Please check the API documentation for more details. Also note that there are
    experimental features available in `tf.data.experimental`, many of which will
    likely make it to the core API in future releases (e.g., check out the `CsvDataset`
    class, as well as the `make_csv_dataset()` method, which takes care of inferring
    the type of each column).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经学会了如何构建高效的输入管道，从多个文本文件加载和预处理数据。我们已经讨论了最常见的数据集方法，但还有一些您可能想看看的方法，例如`concatenate()`、`zip()`、`window()`、`reduce()`、`shard()`、`flat_map()`、`apply()`、`unbatch()`和`padded_batch()`。还有一些更多的类方法，例如`from_generator()`和`from_tensors()`，它们分别从Python生成器或张量列表创建新数据集。请查看API文档以获取更多详细信息。还请注意，`tf.data.experimental`中提供了一些实验性功能，其中许多功能可能会在未来的版本中成为核心API的一部分（例如，请查看`CsvDataset`类，以及`make_csv_dataset()`方法，该方法负责推断每列的类型）。
- en: Using the Dataset with Keras
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用数据集与Keras
- en: 'Now we can use the custom `csv_reader_dataset()` function we wrote earlier
    to create a dataset for the training set, and for the validation set and the test
    set. The training set will be shuffled at each epoch (note that the validation
    set and the test set will also be shuffled, even though we don’t really need that):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用我们之前编写的自定义`csv_reader_dataset()`函数为训练集、验证集和测试集创建数据集。训练集将在每个时期进行洗牌（请注意，验证集和测试集也将进行洗牌，尽管我们实际上并不需要）：
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now you can simply build and train a Keras model using these datasets. When
    you call the model’s `fit()` method, you pass `train_set` instead of `X_train,
    y_train`, and pass `validation_data=valid_set` instead of `validation_data=(X_valid,
    y_valid)`. The `fit()` method will take care of repeating the training dataset
    once per epoch, using a different random order at each epoch:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以简单地使用这些数据集构建和训练Keras模型。当您调用模型的`fit()`方法时，您传递`train_set`而不是`X_train, y_train`，并传递`validation_data=valid_set`而不是`validation_data=(X_valid,
    y_valid)`。`fit()`方法将负责每个时期重复训练数据集，每个时期使用不同的随机顺序：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Similarly, you can pass a dataset to the `evaluate()` and `predict()` methods:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以将数据集传递给`evaluate()`和`predict()`方法：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Unlike the other sets, the `new_set` will usually not contain labels. If it
    does, as is the case here, Keras will ignore them. Note that in all these cases,
    you can still use NumPy arrays instead of datasets if you prefer (but of course
    they need to have been loaded and preprocessed first).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他数据集不同，`new_set`通常不包含标签。如果包含标签，就像这里一样，Keras会忽略它们。请注意，在所有这些情况下，您仍然可以使用NumPy数组而不是数据集（但当然它们需要先加载和预处理）。
- en: 'If you want to build your own custom training loop (as discussed in [Chapter 12](ch12.html#tensorflow_chapter)),
    you can just iterate over the training set, very naturally:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想构建自己的自定义训练循环（如[第12章](ch12.html#tensorflow_chapter)中讨论的），您可以很自然地遍历训练集：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In fact, it is even possible to create a TF function (see [Chapter 12](ch12.html#tensorflow_chapter))
    that trains the model for a whole epoch. This can really speed up training:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，甚至可以创建一个TF函数（参见[第12章](ch12.html#tensorflow_chapter)），用于整个时期训练模型。这可以真正加快训练速度：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In Keras, the `steps_per_execution` argument of the `compile()` method lets
    you define the number of batches that the `fit()` method will process during each
    call to the `tf.function` it uses for training. The default is just 1, so if you
    set it to 50 you will often see a significant performance improvement. However,
    the `on_batch_*()` methods of Keras callbacks will only be called every 50 batches.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，`compile()`方法的`steps_per_execution`参数允许您定义`fit()`方法在每次调用用于训练的`tf.function`时将处理的批次数。默认值只是1，因此如果将其设置为50，您通常会看到显着的性能改进。但是，Keras回调的`on_batch_*()`方法只会在每50批次时调用一次。
- en: Congratulations, you now know how to build powerful input pipelines using the
    tf.data API! However, so far we’ve been using CSV files, which are common, simple,
    and convenient but not really efficient, and do not support large or complex data
    structures (such as images or audio) very well. So, let’s see how to use TFRecords
    instead.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，您现在知道如何使用tf.data API构建强大的输入管道！然而，到目前为止，我们一直在使用常见、简单和方便但不是真正高效的CSV文件，并且不太支持大型或复杂的数据结构（如图像或音频）。因此，让我们看看如何改用TFRecords。
- en: Tip
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you are happy with CSV files (or whatever other format you are using), you
    do not *have* to use TFRecords. As the saying goes, if it ain’t broke, don’t fix
    it! TFRecords are useful when the bottleneck during training is loading and parsing
    the data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对CSV文件（或者您正在使用的其他格式）感到满意，您不一定*必须*使用TFRecords。俗话说，如果它没有坏，就不要修理！当训练过程中的瓶颈是加载和解析数据时，TFRecords非常有用。
- en: The TFRecord Format
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TFRecord格式
- en: 'The TFRecord format is TensorFlow’s preferred format for storing large amounts
    of data and reading it efficiently. It is a very simple binary format that just
    contains a sequence of binary records of varying sizes (each record is comprised
    of a length, a CRC checksum to check that the length was not corrupted, then the
    actual data, and finally a CRC checksum for the data). You can easily create a
    TFRecord file using the `tf.io.TFRecordWriter` class:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: TFRecord格式是TensorFlow存储大量数据并高效读取的首选格式。它是一个非常简单的二进制格式，只包含一系列大小不同的二进制记录（每个记录由长度、用于检查长度是否损坏的CRC校验和、实际数据，最后是数据的CRC校验和组成）。您可以使用`tf.io.TFRecordWriter`类轻松创建TFRecord文件：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And you can then use a `tf.data.TFRecordDataset` to read one or more TFRecord
    files:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用`tf.data.TFRecordDataset`来读取一个或多个TFRecord文件：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Tip
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: By default, a `TFRecordDataset` will read files one by one, but you can make
    it read multiple files in parallel and interleave their records by passing the
    constructor a list of filepaths and setting `num_parallel_reads` to a number greater
    than one. Alternatively, you could obtain the same result by using `list_files()`
    and `interleave()` as we did earlier to read multiple CSV files.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`TFRecordDataset`将逐个读取文件，但您可以使其并行读取多个文件，并通过传递文件路径列表给构造函数并将`num_parallel_reads`设置为大于1的数字来交错它们的记录。或者，您可以通过使用`list_files()`和`interleave()`来获得与我们之前读取多个CSV文件相同的结果。
- en: Compressed TFRecord Files
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩的TFRecord文件
- en: 'It can sometimes be useful to compress your TFRecord files, especially if they
    need to be loaded via a network connection. You can create a compressed TFRecord
    file by setting the `options` argument:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有时将TFRecord文件压缩可能很有用，特别是如果它们需要通过网络连接加载。您可以通过设置`options`参数创建一个压缩的TFRecord文件：
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When reading a compressed TFRecord file, you need to specify the compression
    type:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取压缩的TFRecord文件时，您需要指定压缩类型：
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: A Brief Introduction to Protocol Buffers
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协议缓冲区简介
- en: 'Even though each record can use any binary format you want, TFRecord files
    usually contain serialized protocol buffers (also called *protobufs*). This is
    a portable, extensible, and efficient binary format developed at Google back in
    2001 and made open source in 2008; protobufs are now widely used, in particular
    in [gRPC](https://grpc.io), Google’s remote procedure call system. They are defined
    using a simple language that looks like this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个记录可以使用您想要的任何二进制格式，但TFRecord文件通常包含序列化的协议缓冲区（也称为*protobufs*）。这是一个在2001年由谷歌开发的便携式、可扩展和高效的二进制格式，并于2008年开源；protobufs现在被广泛使用，特别是在[grpc](https://grpc.io)中，谷歌的远程过程调用系统。它们使用一个看起来像这样的简单语言进行定义：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This protobuf definition says we are using version 3 of the protobuf format,
    and it specifies that each `Person` object⁠^([4](ch13.html#idm45720189164144))
    may (optionally) have a `name` of type string, an `id` of type int32, and zero
    or more `email` fields, each of type string. The numbers `1`, `2`, and `3` are
    the field identifiers: they will be used in each record’s binary representation.
    Once you have a definition in a *.proto* file, you can compile it. This requires
    `protoc`, the protobuf compiler, to generate access classes in Python (or some
    other language). Note that the protobuf definitions you will generally use in
    TensorFlow have already been compiled for you, and their Python classes are part
    of the TensorFlow library, so you will not need to use `protoc`. All you need
    to know is how to *use* protobuf access classes in Python. To illustrate the basics,
    let’s look at a simple example that uses the access classes generated for the
    `Person` protobuf (the code is explained in the comments):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个protobuf定义表示我们正在使用protobuf格式的第3版，并且指定每个`Person`对象（可选）可能具有一个字符串类型的`name`、一个int32类型的`id`，以及零个或多个字符串类型的`email`字段。数字`1`、`2`和`3`是字段标识符：它们将在每个记录的二进制表示中使用。一旦你在*.proto*文件中有了一个定义，你就可以编译它。这需要使用protobuf编译器`protoc`在Python（或其他语言）中生成访问类。请注意，你通常在TensorFlow中使用的protobuf定义已经为你编译好了，并且它们的Python类是TensorFlow库的一部分，因此你不需要使用`protoc`。你只需要知道如何在Python中*使用*protobuf访问类。为了说明基础知识，让我们看一个简单的示例，使用为`Person`protobuf生成的访问类（代码在注释中有解释）：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In short, we import the `Person` class generated by `protoc`, we create an instance
    and play with it, visualizing it and reading and writing some fields, then we
    serialize it using the `SerializeToString()` method. This is the binary data that
    is ready to be saved or transmitted over the network. When reading or receiving
    this binary data, we can parse it using the `ParseFromString()` method, and we
    get a copy of the object that was serialized.⁠^([5](ch13.html#idm45720189071424))
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们导入由`protoc`生成的`Person`类，创建一个实例并对其进行操作，可视化它并读取和写入一些字段，然后使用`SerializeToString()`方法对其进行序列化。这是准备保存或通过网络传输的二进制数据。当读取或接收这些二进制数据时，我们可以使用`ParseFromString()`方法进行解析，并获得被序列化的对象的副本。
- en: 'You could save the serialized `Person` object to a TFRecord file, then load
    and parse it: everything would work fine. However, `ParseFromString()` is not
    a TensorFlow operation, so you couldn’t use it in a preprocessing function in
    a tf.data pipeline (except by wrapping it in a `tf.py_function()` operation, which
    would make the code slower and less portable, as you saw in [Chapter 12](ch12.html#tensorflow_chapter)).
    However, you could use the `tf.io.decode_proto()` function, which can parse any
    protobuf you want, provided you give it the protobuf definition (see the notebook
    for an example). That said, in practice you will generally want to use instead
    the predefined protobufs for which TensorFlow provides dedicated parsing operations.
    Let’s look at these predefined protobufs now.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将序列化的`Person`对象保存到TFRecord文件中，然后加载和解析它：一切都会正常工作。然而，`ParseFromString()`不是一个TensorFlow操作，所以你不能在tf.data管道中的预处理函数中使用它（除非将其包装在`tf.py_function()`操作中，这会使代码变慢且不太可移植，正如你在[第12章](ch12.html#tensorflow_chapter)中看到的）。然而，你可以使用`tf.io.decode_proto()`函数，它可以解析任何你想要的protobuf，只要你提供protobuf定义（请参考笔记本中的示例）。也就是说，在实践中，你通常会希望使用TensorFlow提供的专用解析操作的预定义protobuf。现在让我们来看看这些预定义的protobuf。
- en: TensorFlow Protobufs
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Protobufs
- en: 'The main protobuf typically used in a TFRecord file is the `Example` protobuf,
    which represents one instance in a dataset. It contains a list of named features,
    where each feature can either be a list of byte strings, a list of floats, or
    a list of integers. Here is the protobuf definition (from TensorFlow’s source
    code):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: TFRecord文件中通常使用的主要protobuf是`Example`protobuf，它表示数据集中的一个实例。它包含一个命名特征列表，其中每个特征可以是一个字节字符串列表、一个浮点数列表或一个整数列表。以下是protobuf定义（来自TensorFlow源代码）：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The definitions of `BytesList`, `FloatList`, and `Int64List` are straightforward
    enough. Note that `[packed = true]` is used for repeated numerical fields, for
    a more efficient encoding. A `Feature` contains either a `BytesList`, a `FloatList`,
    or an `Int64List`. A `Features` (with an `s`) contains a dictionary that maps
    a feature name to the corresponding feature value. And finally, an `Example` contains
    only a `Features` object.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`BytesList`、`FloatList`和`Int64List`的定义足够简单明了。请注意，对于重复的数值字段，使用`[packed = true]`进行更有效的编码。`Feature`包含一个`BytesList`、一个`FloatList`或一个`Int64List`。一个`Features`（带有`s`）包含一个将特征名称映射到相应特征值的字典。最后，一个`Example`只包含一个`Features`对象。'
- en: Note
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why was `Example` even defined, since it contains no more than a `Features`
    object? Well, TensorFlow’s developers may one day decide to add more fields to
    it. As long as the new `Example` definition still contains the `features` field,
    with the same ID, it will be backward compatible. This extensibility is one of
    the great features of protobufs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会定义`Example`，因为它只包含一个`Features`对象？嗯，TensorFlow的开发人员可能有一天决定向其中添加更多字段。只要新的`Example`定义仍然包含相同ID的`features`字段，它就是向后兼容的。这种可扩展性是protobuf的一个伟大特性。
- en: 'Here is how you could create a `tf.train.Example` representing the same person
    as earlier:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你如何创建一个代表同一个人的`tf.train.Example`：
- en: '[PRE29]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The code is a bit verbose and repetitive, but you could easily wrap it inside
    a small helper function. Now that we have an `Example` protobuf, we can serialize
    it by calling its `SerializeToString()` method, then write the resulting data
    to a TFRecord file. Let’s write it five times to pretend we have several contacts:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码有点冗长和重复，但你可以很容易地将其包装在一个小的辅助函数中。现在我们有了一个`Example` protobuf，我们可以通过调用其`SerializeToString()`方法将其序列化，然后将生成的数据写入TFRecord文件。让我们假装写入五次，以假装我们有几个联系人：
- en: '[PRE30]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Normally you would write much more than five `Example`s! Typically, you would
    create a conversion script that reads from your current format (say, CSV files),
    creates an `Example` protobuf for each instance, serializes them, and saves them
    to several TFRecord files, ideally shuffling them in the process. This requires
    a bit of work, so once again make sure it is really necessary (perhaps your pipeline
    works fine with CSV files).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您会写比五个`Example`更多的内容！通常情况下，您会创建一个转换脚本，从当前格式（比如CSV文件）读取数据，为每个实例创建一个`Example`
    protobuf，将它们序列化，并保存到几个TFRecord文件中，最好在此过程中对它们进行洗牌。这需要一些工作，所以再次确保这确实是必要的（也许您的流水线使用CSV文件运行良好）。
- en: Now that we have a nice TFRecord file containing several serialized `Example`s,
    let’s try to load it.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含多个序列化`Example`的漂亮TFRecord文件，让我们尝试加载它。
- en: Loading and Parsing Examples
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和解析示例
- en: 'To load the serialized `Example` protobufs, we will use a `tf.data.TFRecordDataset`
    once again, and we will parse each `Example` using `tf.io.parse_single_example()`.
    It requires at least two arguments: a string scalar tensor containing the serialized
    data, and a description of each feature. The description is a dictionary that
    maps each feature name to either a `tf.io.FixedLenFeature` descriptor indicating
    the feature’s shape, type, and default value, or a `tf.io.VarLenFeature` descriptor
    indicating only the type if the length of the feature’s list may vary (such as
    for the `"emails"` feature).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载序列化的`Example` protobufs，我们将再次使用`tf.data.TFRecordDataset`，并使用`tf.io.parse_single_example()`解析每个`Example`。它至少需要两个参数：包含序列化数据的字符串标量张量，以及每个特征的描述。描述是一个字典，将每个特征名称映射到`tf.io.FixedLenFeature`描述符，指示特征的形状、类型和默认值，或者`tf.io.VarLenFeature`描述符，仅指示特征列表的长度可能变化的类型（例如`"emails"`特征）。
- en: 'The following code defines a description dictionary, then creates a `TFRecordDataset`
    and applies a custom preprocessing function to it to parse each serialized `Example`
    protobuf that this dataset contains:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了一个描述字典，然后创建了一个`TFRecordDataset`，并对其应用了一个自定义预处理函数，以解析该数据集包含的每个序列化`Example`
    protobuf：
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The fixed-length features are parsed as regular tensors, but the variable-length
    features are parsed as sparse tensors. You can convert a sparse tensor to a dense
    tensor using `tf.sparse.to_dense()`, but in this case it is simpler to just access
    its values:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 固定长度的特征被解析为常规张量，但变长特征被解析为稀疏张量。您可以使用`tf.sparse.to_dense()`将稀疏张量转换为密集张量，但在这种情况下，更简单的方法是直接访问其值：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Instead of parsing examples one by one using `tf.io.parse_single_example()`,
    you may want to parse them batch by batch using `tf.io.parse_example()`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`tf.io.parse_example()`批量解析示例，而不是使用`tf.io.parse_single_example()`逐个解析它们：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Lastly, a `BytesList` can contain any binary data you want, including any serialized
    object. For example, you can use `tf.io.encode_jpeg()` to encode an image using
    the JPEG format and put this binary data in a `BytesList`. Later, when your code
    reads the TFRecord, it will start by parsing the `Example`, then it will need
    to call `tf.io.decode_jpeg()` to parse the data and get the original image (or
    you can use `tf.io.decode_image()`, which can decode any BMP, GIF, JPEG, or PNG
    image). You can also store any tensor you want in a `BytesList` by serializing
    the tensor using `tf.io.serialize_tensor()` then putting the resulting byte string
    in a `BytesList` feature. Later, when you parse the TFRecord, you can parse this
    data using `tf.io.parse_tensor()`. See this chapter’s notebook at [*https://homl.info/colab3*](https://homl.info/colab3)
    for examples of storing images and tensors in a TFRecord file.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`BytesList`可以包含您想要的任何二进制数据，包括任何序列化对象。例如，您可以使用`tf.io.encode_jpeg()`使用JPEG格式对图像进行编码，并将这些二进制数据放入`BytesList`中。稍后，当您的代码读取TFRecord时，它将从解析`Example`开始，然后需要调用`tf.io.decode_jpeg()`来解析数据并获取原始图像（或者您可以使用`tf.io.decode_image()`，它可以解码任何BMP、GIF、JPEG或PNG图像）。您还可以通过使用`tf.io.serialize_tensor()`对张量进行序列化，然后将生成的字节字符串放入`BytesList`特征中，将任何您想要的张量存储在`BytesList`中。稍后，当您解析TFRecord时，您可以使用`tf.io.parse_tensor()`解析这些数据。请参阅本章的笔记本[*https://homl.info/colab3*](https://homl.info/colab3)
    ，了解在TFRecord文件中存储图像和张量的示例。
- en: As you can see, the `Example` protobuf is quite flexible, so it will probably
    be sufficient for most use cases. However, it may be a bit cumbersome to use when
    you are dealing with lists of lists. For example, suppose you want to classify
    text documents. Each document may be represented as a list of sentences, where
    each sentence is represented as a list of words. And perhaps each document also
    has a list of comments, where each comment is represented as a list of words.
    There may be some contextual data too, such as the document’s author, title, and
    publication date. TensorFlow’s `SequenceExample` protobuf is designed for such
    use cases.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，`Example` protobuf非常灵活，因此对于大多数用例来说可能已经足够了。但是，当您处理列表列表时，可能会有些繁琐。例如，假设您想对文本文档进行分类。每个文档可以表示为一个句子列表，其中每个句子表示为一个单词列表。也许每个文档还有一个评论列表，其中每个评论表示为一个单词列表。还可能有一些上下文数据，比如文档的作者、标题和发布日期。TensorFlow的`SequenceExample`
    protobuf就是为这种用例而设计的。
- en: Handling Lists of Lists Using the SequenceExample Protobuf
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SequenceExample Protobuf处理列表列表
- en: 'Here is the definition of the `SequenceExample` protobuf:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`SequenceExample` protobuf的定义：
- en: '[PRE34]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'A `SequenceExample` contains a `Features` object for the contextual data and
    a `FeatureLists` object that contains one or more named `FeatureList` objects
    (e.g., a `FeatureList` named `"content"` and another named `"comments"`). Each
    `FeatureList` contains a list of `Feature` objects, each of which may be a list
    of byte strings, a list of 64-bit integers, or a list of floats (in this example,
    each `Feature` would represent a sentence or a comment, perhaps in the form of
    a list of word identifiers). Building a `SequenceExample`, serializing it, and
    parsing it is similar to building, serializing, and parsing an `Example`, but
    you must use `tf.io.parse_single_sequence_example()` to parse a single `SequenceExample`
    or `tf.io.parse_sequence_example()` to parse a batch. Both functions return a
    tuple containing the context features (as a dictionary) and the feature lists
    (also as a dictionary). If the feature lists contain sequences of varying sizes
    (as in the preceding example), you may want to convert them to ragged tensors
    using `tf.RaggedTensor.from_sparse()` (see the notebook for the full code):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`SequenceExample`包含一个`Features`对象用于上下文数据和一个包含一个或多个命名`FeatureList`对象（例如，一个名为`"content"`的`FeatureList`和另一个名为`"comments"`的`FeatureList`）的`FeatureLists`对象。每个`FeatureList`包含一个`Feature`对象列表，每个`Feature`对象可能是字节字符串列表、64位整数列表或浮点数列表（在此示例中，每个`Feature`可能代表一个句子或评论，可能以单词标识符列表的形式）。构建`SequenceExample`、序列化它并解析它类似于构建、序列化和解析`Example`，但您必须使用`tf.io.parse_single_sequence_example()`来解析单个`SequenceExample`或`tf.io.parse_sequence_example()`来解析批处理。这两个函数返回一个包含上下文特征（作为字典）和特征列表（也作为字典）的元组。如果特征列表包含不同大小的序列（如前面的示例），您可能希望使用`tf.RaggedTensor.from_sparse()`将它们转换为不规则张量（请参阅完整代码的笔记本）：'
- en: '[PRE35]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now that you know how to efficiently store, load, parse, and preprocess the
    data using the tf.data API, TFRecords, and protobufs, it’s time to turn our attention
    to the Keras preprocessing layers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经知道如何使用tf.data API、TFRecords和protobufs高效存储、加载、解析和预处理数据，是时候将注意力转向Keras预处理层了。
- en: Keras Preprocessing Layers
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras预处理层
- en: 'Preparing your data for a neural network typically requires normalizing the
    numerical features, encoding the categorical features and text, cropping and resizing
    images, and more. There are several options for this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为神经网络准备数据通常需要对数值特征进行归一化、对分类特征和文本进行编码、裁剪和调整图像等。有几种选项：
- en: The preprocessing can be done ahead of time when preparing your training data
    files, using any tools you like, such as NumPy, Pandas, or Scikit-Learn. You will
    need to apply the exact same preprocessing steps in production, to ensure your
    production model receives preprocessed inputs similar to the ones it was trained
    on.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理可以提前在准备训练数据文件时完成，使用您喜欢的任何工具，如NumPy、Pandas或Scikit-Learn。您需要在生产中应用完全相同的预处理步骤，以确保您的生产模型接收到与训练时相似的预处理输入。
- en: Alternatively, you can preprocess your data on the fly while loading it with
    tf.data, by applying a preprocessing function to every element of a dataset using
    that dataset’s `map()` method, as we did earlier in this chapter. Again, you will
    need to apply the same preprocessing steps in production.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，您可以在加载数据时使用tf.data进行即时预处理，通过使用该数据集的`map()`方法对数据集的每个元素应用预处理函数，就像本章前面所做的那样。同样，您需要在生产中应用相同的预处理步骤。
- en: One last approach is to include preprocessing layers directly inside your model
    so it can preprocess all the input data on the fly during training, then use the
    same preprocessing layers in production. The rest of this chapter will look at
    this last approach.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一种方法是直接在模型内部包含预处理层，这样它可以在训练期间即时预处理所有输入数据，然后在生产中使用相同的预处理层。本章的其余部分将讨论这种最后一种方法。
- en: 'Keras offers many preprocessing layers that you can include in your models:
    they can be applied to numerical features, categorical features, images, and text.
    We’ll go over the numerical and categorical features in the next sections, as
    well as basic text preprocessing, and we will cover image preprocessing in [Chapter 14](ch14.html#cnn_chapter)
    and more advanced text preprocessing in [Chapter 16](ch16.html#nlp_chapter).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了许多预处理层，您可以将其包含在模型中：它们可以应用于数值特征、分类特征、图像和文本。我们将在接下来的部分中讨论数值和分类特征，以及基本文本预处理，我们将在[第14章](ch14.html#cnn_chapter)中涵盖图像预处理，以及在[第16章](ch16.html#nlp_chapter)中涵盖更高级的文本预处理。
- en: The Normalization Layer
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归一化层
- en: 'As we saw in [Chapter 10](ch10.html#ann_chapter), Keras provides a `Normalization`
    layer that we can use to standardize the input features. We can either specify
    the mean and variance of each feature when creating the layer or—more simply—pass
    the training set to the layer’s `adapt()` method before fitting the model, so
    the layer can measure the feature means and variances on its own before training:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第10章](ch10.html#ann_chapter)中看到的，Keras提供了一个`Normalization`层，我们可以用来标准化输入特征。我们可以在创建层时指定每个特征的均值和方差，或者更简单地在拟合模型之前将训练集传递给该层的`adapt()`方法，以便该层可以在训练之前自行测量特征的均值和方差：
- en: '[PRE36]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Tip
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'The data sample passed to the `adapt()` method must be large enough to be representative
    of your dataset, but it does not have to be the full training set: for the `Normalization`
    layer, a few hundred instances randomly sampled from the training set will generally
    be sufficient to get a good estimate of the feature means and variances.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给`adapt()`方法的数据样本必须足够大，以代表您的数据集，但不必是完整的训练集：对于`Normalization`层，从训练集中随机抽取的几百个实例通常足以获得特征均值和方差的良好估计。
- en: 'Since we included the `Normalization` layer inside the model, we can now deploy
    this model to production without having to worry about normalization again: the
    model will just handle it (see [Figure 13-4](#preprocessing_in_model_diagram)).
    Fantastic! This approach completely eliminates the risk of preprocessing mismatch,
    which happens when people try to maintain different preprocessing code for training
    and production but update one and forget to update the other. The production model
    then ends up receiving data preprocessed in a way it doesn’t expect. If they’re
    lucky, they get a clear bug. If not, the model’s accuracy just silently degrades.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在模型中包含了`Normalization`层，现在我们可以将这个模型部署到生产环境中，而不必再担心归一化的问题：模型会自动处理（参见[图13-4](#preprocessing_in_model_diagram)）。太棒了！这种方法完全消除了预处理不匹配的风险，当人们尝试为训练和生产维护不同的预处理代码，但更新其中一个并忘记更新另一个时，就会发生这种情况。生产模型最终会接收到以其不期望的方式预处理的数据。如果他们幸运的话，会得到一个明显的错误。如果不幸的话，模型的准确性会悄悄下降。
- en: '![mls3 1304](assets/mls3_1304.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1304](assets/mls3_1304.png)'
- en: Figure 13-4\. Including preprocessing layers inside a model
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-4。在模型中包含预处理层
- en: 'Including the preprocessing layer directly in the model is nice and straightforward,
    but it will slow down training (only very slightly in the case of the `Normalization`
    layer): indeed, since preprocessing is performed on the fly during training, it
    happens once per epoch. We can do better by normalizing the whole training set
    just once before training. To do this, we can use the `Normalization` layer in
    a standalone fashion (much like a Scikit-Learn `StandardScaler`):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 直接在模型中包含预处理层很简单明了，但会减慢训练速度（在`Normalization`层的情况下只会稍微减慢）：实际上，由于预处理是在训练过程中实时进行的，每个时期只会发生一次。我们可以通过在训练之前仅对整个训练集进行一次归一化来做得更好。为此，我们可以像使用Scikit-Learn的`StandardScaler`一样单独使用`Normalization`层：
- en: '[PRE37]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we can train a model on the scaled data, this time without a `Normalization`
    layer:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在经过缩放的数据上训练模型，这次不需要`Normalization`层：
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Good! This should speed up training a bit. But now the model won’t preprocess
    its inputs when we deploy it to production. To fix this, we just need to create
    a new model that wraps both the adapted `Normalization` layer and the model we
    just trained. We can then deploy this final model to production, and it will take
    care of both preprocessing its inputs and making predictions (see [Figure 13-5](#optimized_preprocessing_in_model_diagram)):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！这应该会加快训练速度。但是现在当我们将模型部署到生产环境时，模型不会对其输入进行预处理。为了解决这个问题，我们只需要创建一个新模型，将适应的`Normalization`层和刚刚训练的模型包装在一起。然后我们可以将这个最终模型部署到生产环境中，它将负责对其输入进行预处理和进行预测（参见[图13-5](#optimized_preprocessing_in_model_diagram)）：
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![mls3 1305](assets/mls3_1305.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1305](assets/mls3_1305.png)'
- en: Figure 13-5\. Preprocessing the data just once before training using preprocessing
    layers, then deploying these layers inside the final model
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-5。在训练之前仅对数据进行一次预处理，然后将这些层部署到最终模型中
- en: 'Now we have the best of both worlds: training is fast because we only preprocess
    the data once before training begins, and the final model can preprocess its inputs
    on the fly without any risk of preprocessing mismatch.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了最佳的两种方式：训练很快，因为我们只在训练开始前对数据进行一次预处理，而最终模型可以在运行时对其输入进行预处理，而不会有任何预处理不匹配的风险。
- en: 'Moreover, the Keras preprocessing layers play nicely with the tf.data API.
    For example, it’s possible to pass a `tf.data.Dataset` to a preprocessing layer’s
    `adapt()` method. It’s also possible to apply a Keras preprocessing layer to a
    `tf.data.Dataset` using the dataset’s `map()` method. For example, here’s how
    you could apply an adapted `Normalization` layer to the input features of each
    batch in a dataset:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Keras预处理层与tf.data API很好地配合。例如，可以将`tf.data.Dataset`传递给预处理层的`adapt()`方法。还可以使用数据集的`map()`方法将Keras预处理层应用于`tf.data.Dataset`。例如，以下是如何将适应的`Normalization`层应用于数据集中每个批次的输入特征的方法：
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Lastly, if you ever need more features than the Keras preprocessing layers
    provide, you can always write your own Keras layer, just like we discussed in
    [Chapter 12](ch12.html#tensorflow_chapter). For example, if the `Normalization`
    layer didn’t exist, you could get a similar result using the following custom
    layer:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您需要比Keras预处理层提供的更多特性，您可以随时编写自己的Keras层，就像我们在[第12章](ch12.html#tensorflow_chapter)中讨论的那样。例如，如果`Normalization`层不存在，您可以使用以下自定义层获得类似的结果：
- en: '[PRE41]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, let’s look at another Keras preprocessing layer for numerical features:
    the `Discretization` layer.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看另一个用于数值特征的Keras预处理层：`Discretization`层。
- en: The Discretization Layer
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Discretization层
- en: 'The `Discretization` layer’s goal is to transform a numerical feature into
    a categorical feature by mapping value ranges (called bins) to categories. This
    is sometimes useful for features with multimodal distributions, or with features
    that have a highly non-linear relationship with the target. For example, the following
    code maps a numerical `age` feature to three categories, less than 18, 18 to 50
    (not included), and 50 or over:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`Discretization`层的目标是通过将值范围（称为箱）映射到类别，将数值特征转换为分类特征。这对于具有多峰分布的特征或与目标具有高度非线性关系的特征有时是有用的。例如，以下代码将数值`age`特征映射到三个类别，小于18岁，18到50岁（不包括），50岁或以上：'
- en: '[PRE42]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In this example, we provided the desired bin boundaries. If you prefer, you
    can instead provide the number of bins you want, then call the layer’s `adapt()`
    method to let it find the appropriate bin boundaries based on the value percentiles.
    For example, if we set `num_bins=3`, then the bin boundaries will be located at
    the values just below the 33rd and 66th percentiles (in this example, at the values
    10 and 37):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们提供了期望的分箱边界。如果你愿意，你可以提供你想要的箱数，然后调用层的`adapt()`方法，让它根据值的百分位数找到合适的箱边界。例如，如果我们设置`num_bins=3`，那么箱边界将位于第33和第66百分位数之下的值（在这个例子中，值为10和37）：
- en: '[PRE43]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Category identifiers such as these should generally not be passed directly to
    a neural network, as their values cannot be meaningfully compared. Instead, they
    should be encoded, for example using one-hot encoding. Let’s look at how to do
    this now.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通常不应将诸如此类的类别标识符直接传递给神经网络，因为它们的值无法有意义地进行比较。相反，它们应该被编码，例如使用独热编码。现在让我们看看如何做到这一点。
- en: The CategoryEncoding Layer
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CategoryEncoding层
- en: 'When there are only a few categories (e.g., less than a dozen or two), then
    one-hot encoding is often a good option (as discussed in [Chapter 2](ch02.html#project_chapter)).
    To do this, Keras provides the `CategoryEncoding` layer. For example, let’s one-hot
    encode the `age_​cate⁠gories` feature we just created:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当只有少量类别（例如，少于十几个或二十个）时，独热编码通常是一个不错的选择（如[第2章](ch02.html#project_chapter)中讨论的）。为此，Keras提供了`CategoryEncoding`层。例如，让我们对刚刚创建的`age_categories`特征进行独热编码：
- en: '[PRE44]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'If you try to encode more than one categorical feature at a time (which only
    makes sense if they all use the same categories), the `CategoryEncoding` class
    will perform *multi-hot encoding* by default: the output tensor will contain a
    1 for each category present in *any* input feature. For example:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尝试一次对多个分类特征进行编码（只有当它们都使用相同的类别时才有意义），`CategoryEncoding`类将默认执行*多热编码*：输出张量将包含每个输入特征中存在的每个类别的1。例如：
- en: '[PRE45]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: If you believe it’s useful to know how many times each category occurred, you
    can set `output_mode="count"` when creating the `CategoryEncoding` layer, in which
    case the output tensor will contain the number of occurrences of each category.
    In the preceding example, the output would be the same except for the second row,
    which would become `[0., 0., 2.]`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您认为知道每个类别出现的次数是有用的，可以在创建`CategoryEncoding`层时设置`output_mode="count"`，在这种情况下，输出张量将包含每个类别的出现次数。在前面的示例中，输出将与之前相同，只是第二行将变为`[0.,
    0., 2.]`。
- en: 'Note that both multi-hot encoding and count encoding lose information, since
    it’s not possible to know which feature each active category came from. For example,
    both `[0, 1]` and `[1, 0]` are encoded as `[1., 1., 0.]`. If you want to avoid
    this, then you need to one-hot encode each feature separately and concatenate
    the outputs. This way, `[0, 1]` would get encoded as `[1., 0., 0., 0., 1., 0.]`
    and `[1, 0]` would get encoded as `[0., 1., 0., 1., 0., 0.]`. You can get the
    same result by tweaking the category identifiers so they don’t overlap. For example:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，多热编码和计数编码都会丢失信息，因为无法知道每个活动类别来自哪个特征。例如，`[0, 1]`和`[1, 0]`都被编码为`[1., 1., 0.]`。如果要避免这种情况，那么您需要分别对每个特征进行独热编码，然后连接输出。这样，`[0,
    1]`将被编码为`[1., 0., 0., 0., 1., 0.]`，`[1, 0]`将被编码为`[0., 1., 0., 1., 0., 0.]`。您可以通过调整类别标识符来获得相同的结果，以便它们不重叠。例如：
- en: '[PRE46]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In this output, the first three columns correspond to the first feature, and
    the last three correspond to the second feature. This allows the model to distinguish
    the two features. However, it also increases the number of features fed to the
    model, and thereby requires more model parameters. It’s hard to know in advance
    whether a single multi-hot encoding or a per-feature one-hot encoding will work
    best: it depends on the task, and you may need to test both options.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在此输出中，前三列对应于第一个特征，最后三列对应于第二个特征。这使模型能够区分这两个特征。但是，这也增加了馈送到模型的特征数量，因此需要更多的模型参数。很难事先知道单个多热编码还是每个特征的独热编码哪个效果最好：这取决于任务，您可能需要测试两种选项。
- en: Now you can encode categorical integer features using one-hot or multi-hot encoding.
    But what about categorical text features? For this, you can use the `StringLookup`
    layer.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以使用独热编码或多热编码对分类整数特征进行编码。但是对于分类文本特征呢？为此，您可以使用`StringLookup`层。
- en: The StringLookup Layer
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StringLookup层
- en: 'Let’s use the Keras `StringLookup` layer to one-hot encode a `cities` feature:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Keras的`StringLookup`层对`cities`特征进行独热编码：
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We first create a `StringLookup` layer, then we adapt it to the data: it finds
    that there are three distinct categories. Then we use the layer to encode a few
    cities. They are encoded as integers by default. Unknown categories get mapped
    to 0, as is the case for “Montreal” in this example. The known categories are
    numbered starting at 1, from the most frequent category to the least frequent.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个`StringLookup`层，然后将其适应到数据：它发现有三个不同的类别。然后我们使用该层对一些城市进行编码。默认情况下，它们被编码为整数。未知类别被映射为0，就像在这个例子中的“Montreal”一样。已知类别从最常见的类别开始编号，从最常见到最不常见。
- en: 'Conveniently, if you set `output_mode="one_hot"` when creating the `StringLookup`
    layer, it will output a one-hot vector for each category, instead of an integer:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的是，当创建`StringLookup`层时设置`output_mode="one_hot"`，它将为每个类别输出一个独热向量，而不是一个整数：
- en: '[PRE48]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Tip
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Keras also includes an `IntegerLookup` layer that acts much like the `StringLookup`
    layer but takes integers as input, rather than strings.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Keras还包括一个`IntegerLookup`层，其功能类似于`StringLookup`层，但输入为整数，而不是字符串。
- en: 'If the training set is very large, it may be convenient to adapt the layer
    to just a random subset of the training set. In this case, the layer’s `adapt()`
    method may miss some of the rarer categories. By default, it would then map them
    all to category 0, making them indistinguishable by the model. To reduce this
    risk (while still adapting the layer only on a subset of the training set), you
    can set `num_oov_indices` to an integer greater than 1\. This is the number of
    out-of-vocabulary (OOV) buckets to use: each unknown category will get mapped
    pseudorandomly to one of the OOV buckets, using a hash function modulo the number
    of OOV buckets. This will allow the model to distinguish at least some of the
    rare categories. For example:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练集非常大，可能会方便地将层适应于训练集的随机子集。在这种情况下，层的`adapt()`方法可能会错过一些较少见的类别。默认情况下，它会将它们全部映射到类别0，使它们在模型中无法区分。为了减少这种风险（同时仅在训练集的子集上调整层），您可以将`num_oov_indices`设置为大于1的整数。这是要使用的未知词汇（OOV）桶的数量：每个未知类别将使用哈希函数对OOV桶的数量取模，伪随机地映射到其中一个OOV桶。这将使模型能够区分至少一些罕见的类别。例如：
- en: '[PRE49]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Since there are five OOV buckets, the first known category’s ID is now 5 (`"Paris"`).
    But `"Foo"`, `"Bar"`, and `"Baz"` are unknown, so they each get mapped to one
    of the OOV buckets. `"Bar"` gets its own dedicated bucket (with ID 3), but sadly
    `"Foo"` and `"Baz"` happen to be mapped to the same bucket (with ID 4), so they
    remain indistinguishable by the model. This is called a *hashing collision*. The
    only way to reduce the risk of collision is to increase the number of OOV buckets.
    However, this will also increase the total number of categories, which will require
    more RAM and extra model parameters once the categories are one-hot encoded. So,
    don’t increase that number too much.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有五个OOV桶，第一个已知类别的ID现在是5（“巴黎”）。但是，“Foo”、“Bar”和“Baz”是未知的，因此它们各自被映射到OOV桶中的一个。
    “Bar”有自己的专用桶（ID为3），但不幸的是，“Foo”和“Baz”被映射到相同的桶中（ID为4），因此它们在模型中保持不可区分。这被称为*哈希碰撞*。减少碰撞风险的唯一方法是增加OOV桶的数量。但是，这也会增加总类别数，这将需要更多的RAM和额外的模型参数，一旦类别被独热编码。因此，不要将该数字增加得太多。
- en: 'This idea of mapping categories pseudorandomly to buckets is called the *hashing
    trick*. Keras provides a dedicated layer which does just that: the `Hashing` layer.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 将类别伪随机映射到桶中的这种想法称为*哈希技巧*。Keras提供了一个专用的层，就是`Hashing`层。
- en: The Hashing Layer
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哈希层
- en: 'For each category, the Keras `Hashing` layer computes a hash, modulo the number
    of buckets (or “bins”). The mapping is entirely pseudorandom, but stable across
    runs and platforms (i.e., the same category will always be mapped to the same
    integer, as long as the number of bins is unchanged). For example, let’s use the
    `Hashing` layer to encode a few cities:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个类别，Keras的`Hashing`层计算一个哈希值，取模于桶（或“bin”）的数量。映射完全是伪随机的，但在运行和平台之间是稳定的（即，只要桶的数量不变，相同的类别将始终被映射到相同的整数）。例如，让我们使用`Hashing`层来编码一些城市：
- en: '[PRE50]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The benefit of this layer is that it does not need to be adapted at all, which
    may sometimes be useful, especially in an out-of-core setting (when the dataset
    is too large to fit in memory). However, we once again get a hashing collision:
    “Tokyo” and “Montreal” are mapped to the same ID, making them indistinguishable
    by the model. So, it’s usually preferable to stick to the `StringLookup` layer.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层的好处是它根本不需要适应，这有时可能很有用，特别是在核外设置中（当数据集太大而无法放入内存时）。然而，我们再次遇到了哈希碰撞：“东京”和“蒙特利尔”被映射到相同的ID，使它们在模型中无法区分。因此，通常最好坚持使用`StringLookup`层。
- en: 'Let’s now look at another way to encode categories: trainable embeddings.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看另一种编码类别的方法：可训练的嵌入。
- en: Encoding Categorical Features Using Embeddings
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用嵌入编码分类特征
- en: An embedding is a dense representation of some higher-dimensional data, such
    as a category, or a word in a vocabulary. If there are 50,000 possible categories,
    then one-hot encoding would produce a 50,000-dimensional sparse vector (i.e.,
    containing mostly zeros). In contrast, an embedding would be a comparatively small
    dense vector; for example, with just 100 dimensions.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是一种高维数据（例如类别或词汇中的单词）的密集表示。如果有50,000个可能的类别，那么独热编码将产生一个50,000维的稀疏向量（即，大部分为零）。相比之下，嵌入将是一个相对较小的密集向量；例如，只有100个维度。
- en: In deep learning, embeddings are usually initialized randomly, and they are
    then trained by gradient descent, along with the other model parameters. For example,
    the `"NEAR BAY"` category in the California housing dataset could be represented
    initially by a random vector such as `[0.131, 0.890]`, while the `"NEAR OCEAN"`
    category might be represented by another random vector such as `[0.631, 0.791]`.
    In this example, we use 2D embeddings, but the number of dimensions is a hyperparameter
    you can tweak.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，嵌入通常是随机初始化的，然后通过梯度下降与其他模型参数一起训练。例如，在加利福尼亚住房数据集中，“NEAR BAY”类别最初可以由一个随机向量表示，例如`[0.131,
    0.890]`，而“NEAR OCEAN”类别可能由另一个随机向量表示，例如`[0.631, 0.791]`。在这个例子中，我们使用了2D嵌入，但维度的数量是一个可以调整的超参数。
- en: Since these embeddings are trainable, they will gradually improve during training;
    and as they represent fairly similar categories in this case, gradient descent
    will certainly end up pushing them closer together, while it will tend to move
    them away from the `"INLAND"` category’s embedding (see [Figure 13-6](#embedding_diagram)).
    Indeed, the better the representation, the easier it will be for the neural network
    to make accurate predictions, so training tends to make embeddings useful representations
    of the categories. This is called *representation learning* (you will see other
    types of representation learning in [Chapter 17](ch17.html#autoencoders_chapter)).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些嵌入是可训练的，它们在训练过程中会逐渐改进；由于它们在这种情况下代表的是相当相似的类别，梯度下降肯定会使它们彼此更接近，同时也会使它们远离“INLAND”类别的嵌入（参见[图13-6](#embedding_diagram)）。实际上，表示得越好，神经网络就越容易做出准确的预测，因此训练倾向于使嵌入成为类别的有用表示。这被称为*表示学习*（您将在[第17章](ch17.html#autoencoders_chapter)中看到其他类型的表示学习）。
- en: '![mls3 1306](assets/mls3_1306.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1306](assets/mls3_1306.png)'
- en: Figure 13-6\. Embeddings will gradually improve during training
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-6。嵌入将在训练过程中逐渐改进
- en: 'Keras provides an `Embedding` layer, which wraps an *embedding matrix*: this
    matrix has one row per category and one column per embedding dimension. By default,
    it is initialized randomly. To convert a category ID to an embedding, the `Embedding`
    layer just looks up and returns the row that corresponds to that category. That’s
    all there is to it! For example, let’s initialize an `Embedding` layer with five
    rows and 2D embeddings, and use it to encode some categories:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了一个`Embedding`层，它包装了一个*嵌入矩阵*：这个矩阵每行对应一个类别，每列对应一个嵌入维度。默认情况下，它是随机初始化的。要将类别ID转换为嵌入，`Embedding`层只需查找并返回对应于该类别的行。就是这样！例如，让我们用五行和2D嵌入初始化一个`Embedding`层，并用它来编码一些类别：
- en: '[PRE51]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As you can see, category 2 gets encoded (twice) as the 2D vector `[-0.04663396,
    0.01846724]`, while category 4 gets encoded as `[-0.02736737, -0.02768031]`. Since
    the layer is not trained yet, these encodings are just random.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，类别2被编码（两次）为2D向量`[-0.04663396, 0.01846724]`，而类别4被编码为`[-0.02736737, -0.02768031]`。由于该层尚未训练，这些编码只是随机的。
- en: Warning
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: An `Embedding` layer is initialized randomly, so it does not make sense to use
    it outside of a model as a standalone preprocessing layer unless you initialize
    it with pretrained weights.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`Embedding`层是随机初始化的，因此除非使用预训练权重初始化，否则在模型之外作为独立的预处理层使用它是没有意义的。'
- en: 'If you want to embed a categorical text attribute, you can simply chain a `StringLookup`
    layer and an `Embedding` layer, like this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要嵌入一个分类文本属性，您可以简单地将`StringLookup`层和`Embedding`层连接起来，就像这样：
- en: '[PRE52]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Note that the number of rows in the embedding matrix needs to be equal to the
    vocabulary size: that’s the total number of categories, including the known categories
    plus the OOV buckets (just one by default). The `vocabulary_size()` method of
    the `StringLookup` class conveniently returns this number.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，嵌入矩阵中的行数需要等于词汇量的大小：这是总类别数，包括已知类别和OOV桶（默认只有一个）。`StringLookup`类的`vocabulary_size()`方法方便地返回这个数字。
- en: Tip
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In this example we used 2D embeddings, but as a rule of thumb embeddings typically
    have 10 to 300 dimensions, depending on the task, the vocabulary size, and the
    size of your training set. You will have to tune this hyperparameter.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了2D嵌入，但一般来说，嵌入通常有10到300个维度，取决于任务、词汇量和训练集的大小。您将需要调整这个超参数。
- en: 'Putting everything together, we can now create a Keras model that can process
    a categorical text feature along with regular numerical features and learn an
    embedding for each category (as well as for each OOV bucket):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容放在一起，现在我们可以创建一个Keras模型，可以处理分类文本特征以及常规数值特征，并为每个类别（以及每个OOV桶）学习一个嵌入：
- en: '[PRE53]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This model takes two inputs: `num_input`, which contains eight numerical features
    per instance, plus `cat_input`, which contains a single categorical text input
    per instance. The model uses the `lookup_and_embed` model we created earlier to
    encode each ocean-proximity category as the corresponding trainable embedding.
    Next, it concatenates the numerical inputs and the embeddings using the `concatenate()`
    function to produce the complete encoded inputs, which are ready to be fed to
    a neural network. We could add any kind of neural network at this point, but for
    simplicity we just add a single dense output layer, and then we create the Keras
    `Model` with the inputs and output we’ve just defined. Next we compile the model
    and train it, passing both the numerical and categorical inputs.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型有两个输入：`num_input`，每个实例包含八个数值特征，以及`cat_input`，每个实例包含一个分类文本输入。该模型使用我们之前创建的`lookup_and_embed`模型来将每个海洋接近类别编码为相应的可训练嵌入。接下来，它使用`concatenate()`函数将数值输入和嵌入连接起来，生成完整的编码输入，准备输入神经网络。在这一点上，我们可以添加任何类型的神经网络，但为了简单起见，我们只添加一个单一的密集输出层，然后我们创建Keras`Model`，使用我们刚刚定义的输入和输出。接下来，我们编译模型并训练它，传递数值和分类输入。
- en: 'As you saw in [Chapter 10](ch10.html#ann_chapter), since the `Input` layers
    are named `"num"` and `"cat"`, we could also have passed the training data to
    the `fit()` method using a dictionary instead of a tuple: `{"num": X_train_num,
    "cat": X_train_cat}`. Alternatively, we could have passed a `tf.data.Dataset`
    containing batches, each represented as `((X_batch_num, X_batch_cat), y_batch)`
    or as `({"num": X_batch_num, "cat": X_batch_cat}, y_batch)`. And of course the
    same goes for the validation data.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '正如您在[第10章](ch10.html#ann_chapter)中看到的，由于`Input`层的名称是`"num"`和`"cat"`，我们也可以将训练数据传递给`fit()`方法，使用字典而不是元组：`{"num":
    X_train_num, "cat": X_train_cat}`。或者，我们可以传递一个包含批次的`tf.data.Dataset`，每个批次表示为`((X_batch_num,
    X_batch_cat), y_batch)`或者`({"num": X_batch_num, "cat": X_batch_cat}, y_batch)`。当然，验证数据也是一样的。'
- en: Note
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: One-hot encoding followed by a `Dense` layer (with no activation function and
    no biases) is equivalent to an `Embedding` layer. However, the `Embedding` layer
    uses way fewer computations as it avoids many multiplications by zero—the performance
    difference becomes clear when the size of the embedding matrix grows. The `Dense`
    layer’s weight matrix plays the role of the embedding matrix. For example, using
    one-hot vectors of size 20 and a `Dense` layer with 10 units is equivalent to
    using an `Embedding` layer with `input_dim=20` and `output_dim=10`. As a result,
    it would be wasteful to use more embedding dimensions than the number of units
    in the layer that follows the `Embedding` layer.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 先进行独热编码，然后通过一个没有激活函数和偏置的`Dense`层等同于一个`Embedding`层。然而，`Embedding`层使用的计算量要少得多，因为它避免了许多零乘法——当嵌入矩阵的大小增长时，性能差异变得明显。`Dense`层的权重矩阵起到了嵌入矩阵的作用。例如，使用大小为20的独热向量和一个具有10个单元的`Dense`层等同于使用一个`input_dim=20`和`output_dim=10`的`Embedding`层。因此，在`Embedding`层后面的层中使用的嵌入维度不应该超过单元数。
- en: OK, now that you have learned how to encode categorical features, it’s time
    to turn our attention to text preprocessing.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在您已经学会了如何对分类特征进行编码，是时候将注意力转向文本预处理了。
- en: Text Preprocessing
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本预处理
- en: 'Keras provides a `TextVectorization` layer for basic text preprocessing. Much
    like the `StringLookup` layer, you must either pass it a vocabulary upon creation,
    or let it learn the vocabulary from some training data using the `adapt()` method.
    Let’s look at an example:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Keras为基本文本预处理提供了一个`TextVectorization`层。与`StringLookup`层类似，您必须在创建时传递一个词汇表，或者使用`adapt()`方法从一些训练数据中学习词汇表。让我们看一个例子：
- en: '[PRE54]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The two sentences “Be good!” and “Question: be or be?” were encoded as `[2,
    1, 0, 0]` and `[6, 2, 1, 2]`, respectively. The vocabulary was learned from the
    four sentences in the training data: “be” = 2, “to” = 3, etc. To construct the
    vocabulary, the `adapt()` method first converted the training sentences to lowercase
    and removed punctuation, which is why “Be”, “be”, and “be?” are all encoded as
    “be” = 2\. Next, the sentences were split on whitespace, and the resulting words
    were sorted by descending frequency, producing the final vocabulary. When encoding
    sentences, unknown words get encoded as 1s. Lastly, since the first sentence is
    shorter than the second, it was padded with 0s.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '两个句子“Be good!”和“Question: be or be?”分别被编码为`[2, 1, 0, 0]`和`[6, 2, 1, 2]`。词汇表是从训练数据中的四个句子中学习的：“be”
    = 2，“to” = 3，等等。为构建词汇表，`adapt()`方法首先将训练句子转换为小写并去除标点，这就是为什么“Be”、“be”和“be?”都被编码为“be”
    = 2。接下来，句子被按空格拆分，生成的单词按降序频率排序，产生最终的词汇表。在编码句子时，未知单词被编码为1。最后，由于第一个句子比第二个句子短，因此用0进行了填充。'
- en: Tip
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The `TextVectorization` layer has many options. For example, you can preserve
    the case and punctuation if you want, by setting `standardize=None`, or you can
    pass any standardization function you please as the `standardize` argument. You
    can prevent splitting by setting `split=None`, or you can pass your own splitting
    function instead. You can set the `output_sequence_length` argument to ensure
    that the output sequences all get cropped or padded to the desired length, or
    you can set `ragged=True` to get a ragged tensor instead of a regular tensor.
    Please check out the documentation for more options.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextVectorization`层有许多选项。例如，您可以通过设置`standardize=None`来保留大小写和标点，或者您可以将任何标准化函数作为`standardize`参数传递。您可以通过设置`split=None`来防止拆分，或者您可以传递自己的拆分函数。您可以设置`output_sequence_length`参数以确保输出序列都被裁剪或填充到所需的长度，或者您可以设置`ragged=True`以获得一个不规则张量而不是常规张量。请查看文档以获取更多选项。'
- en: 'The word IDs must be encoded, typically using an `Embedding` layer: we will
    do this in [Chapter 16](ch16.html#nlp_chapter). Alternatively, you can set the
    `TextVectorization` layer’s `output_mode` argument to `"multi_hot"` or `"count"`
    to get the corresponding encodings. However, simply counting words is usually
    not ideal: words like “to” and “the” are so frequent that they hardly matter at
    all, whereas, rarer words such as “basketball” are much more informative. So,
    rather than setting `output_mode` to `"multi_hot"` or `"count"`, it is usually
    preferable to set it to `"tf_idf"`, which stands for *term-frequency* × *inverse-document-frequency*
    (TF-IDF). This is similar to the count encoding, but words that occur frequently
    in the training data are downweighted, and conversely, rare words are upweighted.
    For example:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 单词ID必须进行编码，通常使用`Embedding`层：我们将在[第16章](ch16.html#nlp_chapter)中进行这样做。或者，您可以将`TextVectorization`层的`output_mode`参数设置为`"multi_hot"`或`"count"`以获得相应的编码。然而，简单地计算单词通常不是理想的：像“to”和“the”这样的单词非常频繁，几乎没有影响，而“basketball”等更稀有的单词则更具信息量。因此，通常最好将`output_mode`设置为`"tf_idf"`，它代表*词频*
    × *逆文档频率*（TF-IDF）。这类似于计数编码，但在训练数据中频繁出现的单词被降权，反之，稀有单词被升权。例如：
- en: '[PRE55]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'There are many TF-IDF variants, but the way the `TextVectorization` layer implements
    it is by multiplying each word count by a weight equal to log(1 + *d* / (*f* +
    1)), where *d* is the total number of sentences (a.k.a., documents) in the training
    data and *f* counts how many of these training sentences contain the given word.
    For example, in this case there are *d* = 4 sentences in the training data, and
    the word “be” appears in *f* = 3 of these. Since the word “be” occurs twice in
    the sentence “Question: be or be?”, it gets encoded as 2 × log(1 + 4 / (1 + 3))
    ≈ 1.3862944\. The word “question” only appears once, but since it is a less common
    word, its encoding is almost as high: 1 × log(1 + 4 / (1 + 1)) ≈ 1.0986123\. Note
    that the average weight is used for unknown words.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'TF-IDF的变体有很多种，但`TextVectorization`层实现的方式是将每个单词的计数乘以一个权重，该权重等于log(1 + *d* /
    (*f* + 1))，其中*d*是训练数据中的句子总数（也称为文档），*f*表示这些训练句子中包含给定单词的数量。例如，在这种情况下，训练数据中有*d* =
    4个句子，单词“be”出现在*f* = 3个句子中。由于单词“be”在句子“Question: be or be?”中出现了两次，它被编码为2 × log(1
    + 4 / (1 + 3)) ≈ 1.3862944。单词“question”只出现一次，但由于它是一个不太常见的单词，它的编码几乎一样高：1 × log(1
    + 4 / (1 + 1)) ≈ 1.0986123。请注意，对于未知单词，使用平均权重。'
- en: 'This approach to text encoding is straightforward to use and it can give fairly
    good results for basic natural language processing tasks, but it has several important
    limitations: it only works with languages that separate words with spaces, it
    doesn’t distinguish between homonyms (e.g., “to bear” versus “teddy bear”), it
    gives no hint to your model that words like “evolution” and “evolutionary” are
    related, etc. And if you use multi-hot, count, or TF-IDF encoding, then the order
    of the words is lost. So what are the other options?'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这种文本编码方法易于使用，并且对于基本的自然语言处理任务可以得到相当不错的结果，但它有几个重要的局限性：它只适用于用空格分隔单词的语言，它不区分同音异义词（例如“to
    bear”与“teddy bear”），它不提示您的模型单词“evolution”和“evolutionary”之间的关系等。如果使用多热编码、计数或TF-IDF编码，则单词的顺序会丢失。那么还有哪些其他选项呢？
- en: One option is to use the [TensorFlow Text library](https://tensorflow.org/text),
    which provides more advanced text preprocessing features than the `TextVectorization`
    layer. For example, it includes several subword tokenizers capable of splitting
    the text into tokens smaller than words, which makes it possible for the model
    to more easily detect that “evolution” and “evolutionary” have something in common
    (more on subword tokenization in [Chapter 16](ch16.html#nlp_chapter)).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是使用[TensorFlow Text库](https://tensorflow.org/text)，它提供比`TextVectorization`层更高级的文本预处理功能。例如，它包括几种子词标记器，能够将文本分割成比单词更小的标记，这使得模型更容易检测到“evolution”和“evolutionary”之间有一些共同之处（有关子词标记化的更多信息，请参阅[第16章](ch16.html#nlp_chapter)）。
- en: Yet another option is to use pretrained language model components. Let’s look
    at this now.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是使用预训练的语言模型组件。现在让我们来看看这个。
- en: Using Pretrained Language Model Components
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练语言模型组件
- en: The [TensorFlow Hub library](https://tensorflow.org/hub) makes it easy to reuse
    pretrained model components in your own models, for text, image, audio, and more.
    These model components are called *modules*. Simply browse the [TF Hub repository](https://tfhub.dev),
    find the one you need, and copy the code example into your project, and the module
    will be automatically downloaded and bundled into a Keras layer that you can directly
    include in your model. Modules typically contain both preprocessing code and pretrained
    weights, and they generally require no extra training (but of course, the rest
    of your model will certainly require training).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[TensorFlow Hub库](https://tensorflow.org/hub)使得在您自己的模型中重用预训练模型组件变得容易，用于文本、图像、音频等。这些模型组件称为*模块*。只需浏览[TF
    Hub存储库](https://tfhub.dev)，找到您需要的模块，将代码示例复制到您的项目中，模块将自动下载并捆绑到一个Keras层中，您可以直接包含在您的模型中。模块通常包含预处理代码和预训练权重，并且通常不需要额外的训练（但当然，您的模型的其余部分肯定需要训练）。'
- en: For example, some powerful pretrained language models are available. The most
    powerful are quite large (several gigabytes), so for a quick example let’s use
    the `nnlm-en-dim50` module, version 2, which is a fairly basic module that takes
    raw text as input and outputs 50-dimensional sentence embeddings. We’ll import
    TensorFlow Hub and use it to load the module, then use that module to encode two
    sentences to vectors:^([8](ch13.html#idm45720186424416))
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一些强大的预训练语言模型是可用的。最强大的模型非常庞大（几个千兆字节），因此为了快速示例，让我们使用`nnlm-en-dim50`模块，版本2，这是一个相当基本的模块，它将原始文本作为输入并输出50维句子嵌入。我们将导入TensorFlow
    Hub并使用它来加载模块，然后使用该模块将两个句子编码为向量：
- en: '[PRE56]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The `hub.KerasLayer` layer downloads the module from the given URL. This particular
    module is a *sentence encoder*: it takes strings as input and encodes each one
    as a single vector (in this case, a 50-dimensional vector). Internally, it parses
    the string (splitting words on spaces) and embeds each word using an embedding
    matrix that was pretrained on a huge corpus: the Google News 7B corpus (seven
    billion words long!). Then it computes the mean of all the word embeddings, and
    the result is the sentence embedding.⁠^([9](ch13.html#idm45720186382240))'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`hub.KerasLayer`层从给定的URL下载模块。这个特定的模块是一个*句子编码器*：它将字符串作为输入，并将每个字符串编码为单个向量（在本例中是一个50维向量）。在内部，它解析字符串（在空格上拆分单词）并使用在一个巨大的语料库上预训练的嵌入矩阵嵌入每个单词：Google
    News 7B语料库（七十亿字长！）。然后计算所有单词嵌入的平均值，结果就是句子嵌入。'
- en: You just need to include this `hub_layer` in your model, and you’re ready to
    go. Note that this particular language model was trained on the English language,
    but many other languages are available, as well as multilingual models.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 您只需要在您的模型中包含这个`hub_layer`，然后就可以开始了。请注意，这个特定的语言模型是在英语上训练的，但许多其他语言也可用，以及多语言模型。
- en: Last but not least, the excellent open source [Transformers library by Hugging
    Face](https://huggingface.co/docs/transformers) also makes it easy to include
    powerful language model components inside your own models. You can browse the
    [Hugging Face Hub](https://huggingface.co/models), choose the model you want,
    and use the provided code examples to get started. It used to contain only language
    models, but it has now expanded to include image models and more.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由Hugging Face提供的优秀开源[Transformers库](https://huggingface.co/docs/transformers)也使得在您自己的模型中包含强大的语言模型组件变得容易。您可以浏览[Hugging
    Face Hub](https://huggingface.co/models)，选择您想要的模型，并使用提供的代码示例开始。它以前只包含语言模型，但现在已扩展到包括图像模型等。
- en: We will come back to natural language processing in more depth in [Chapter 16](ch16.html#nlp_chapter).
    Let’s now look at Keras’s image preprocessing layers.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第16章](ch16.html#nlp_chapter)中更深入地讨论自然语言处理。现在让我们看一下Keras的图像预处理层。
- en: Image Preprocessing Layers
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像预处理层
- en: 'The Keras preprocessing API includes three image preprocessing layers:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Keras预处理API包括三个图像预处理层：
- en: '`tf.keras.layers.Resizing` resizes the input images to the desired size. For
    example, `Resizing(height=100, width=200)` resizes each image to 100 × 200, possibly
    distorting the image. If you set `crop_to_aspect_ratio=True`, then the image will
    be cropped to the target image ratio, to avoid distortion.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.keras.layers.Resizing`将输入图像调整为所需大小。例如，`Resizing(height=100, width=200)`将每个图像调整为100×200，可能会扭曲图像。如果设置`crop_to_aspect_ratio=True`，则图像将被裁剪到目标图像比例，以避免扭曲。'
- en: '`tf.keras.layers.Rescaling` rescales the pixel values. For example, `Rescal⁠ing​(scale=2/255,
    offset=-1)` scales the values from 0 → 255 to –1 → 1.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.keras.layers.Rescaling`重新缩放像素值。例如，`Rescaling(scale=2/255, offset=-1)`将值从0
    → 255缩放到-1 → 1。'
- en: '`tf.keras.layers.CenterCrop` crops the image, keeping only a center patch of
    the desired height and width.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.keras.layers.CenterCrop`裁剪图像，保留所需高度和宽度的中心区域。'
- en: 'For example, let’s load a couple of sample images and center-crop them. For
    this, we will use Scikit-Learn’s `load_sample_images()` function; this loads two
    color images, one of a Chinese temple and the other of a flower (this requires
    the Pillow library, which should already be installed if you are using Colab or
    if you followed the installation instructions):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们加载一些示例图像并对它们进行中心裁剪。为此，我们将使用Scikit-Learn的`load_sample_images()`函数；这将加载两个彩色图像，一个是中国寺庙的图像，另一个是花朵的图像（这需要Pillow库，如果您正在使用Colab或者按照安装说明进行操作，应该已经安装）：
- en: '[PRE57]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Keras also includes several layers for data augmentation, such as `RandomCrop`,
    `RandomFlip`, `RandomTranslation`, `RandomRotation`, `RandomZoom`, `RandomHeight`,
    `RandomWidth`, and `RandomContrast`. These layers are only active during training,
    and they randomly apply some transformation to the input images (their names are
    self-explanatory). Data augmentation will artificially increase the size of the
    training set, which often leads to improved performance, as long as the transformed
    images look like realistic (nonaugmented) images. We’ll cover image processing
    more closely in the next chapter.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Keras还包括几个用于数据增强的层，如`RandomCrop`、`RandomFlip`、`RandomTranslation`、`RandomRotation`、`RandomZoom`、`RandomHeight`、`RandomWidth`和`RandomContrast`。这些层仅在训练期间激活，并随机对输入图像应用一些转换（它们的名称是不言自明的）。数据增强将人为增加训练集的大小，通常会导致性能提升，只要转换后的图像看起来像真实的（非增强的）图像。我们将在下一章更详细地介绍图像处理。
- en: Note
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Under the hood, the Keras preprocessing layers are based on TensorFlow’s low-level
    API. For example, the `Normalization` layer uses `tf.nn.moments()` to compute
    both the mean and variance, the `Discretization` layer uses `tf.raw_ops.Bucketize()`,
    `CategoricalEncoding` uses `tf.math.bincount()`, `IntegerLookup` and `StringLookup`
    use the `tf.lookup` package, `Hashing` and `TextVectorization` use several ops
    from the `tf.strings` package, `Embedding` uses `tf.nn.embedding_lookup()`, and
    the image preprocessing layers use the ops from the `tf.image` package. If the
    Keras preprocessing API isn’t sufficient for your needs, you may occasionally
    need to use TensorFlow’s low-level API directly.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，Keras预处理层基于TensorFlow的低级API。例如，`Normalization`层使用`tf.nn.moments()`来计算均值和方差，`Discretization`层使用`tf.raw_ops.Bucketize()`，`CategoricalEncoding`使用`tf.math.bincount()`，`IntegerLookup`和`StringLookup`使用`tf.lookup`包，`Hashing`和`TextVectorization`使用`tf.strings`包中的几个操作，`Embedding`使用`tf.nn.embedding_lookup()`，图像预处理层使用`tf.image`包中的操作。如果Keras预处理API不满足您的需求，您可能偶尔需要直接使用TensorFlow的低级API。
- en: Now let’s look at another way to load data easily and efficiently in TensorFlow.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看在TensorFlow中另一种轻松高效地加载数据的方法。
- en: The TensorFlow Datasets Project
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow数据集项目
- en: The [TensorFlow Datasets (TFDS)](https://tensorflow.org/datasets) project makes
    it very easy to load common datasets, from small ones like MNIST or Fashion MNIST
    to huge datasets like ImageNet (you will need quite a bit of disk space!). The
    list includes image datasets, text datasets (including translation datasets),
    audio and video datasets, time series, and much more. You can visit [*https://homl.info/tfds*](https://homl.info/tfds)
    to view the full list, along with a description of each dataset. You can also
    check out [Know Your Data](https://knowyourdata.withgoogle.com), which is a tool
    to explore and understand many of the datasets provided by TFDS.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[TensorFlow数据集（TFDS）](https://tensorflow.org/datasets)项目使加载常见数据集变得非常容易，从小型数据集如MNIST或Fashion
    MNIST到像ImageNet这样的大型数据集（您将需要相当大的磁盘空间！）。列表包括图像数据集、文本数据集（包括翻译数据集）、音频和视频数据集、时间序列等等。您可以访问[*https://homl.info/tfds*](https://homl.info/tfds)查看完整列表，以及每个数据集的描述。您还可以查看[了解您的数据](https://knowyourdata.withgoogle.com)，这是一个用于探索和理解TFDS提供的许多数据集的工具。'
- en: 'TFDS is not bundled with TensorFlow, but if you are running on Colab or if
    you followed the installation instructions at [*https://homl.info/install*](https://homl.info/install),
    then it’s already installed. You can then import `tensorflow_datasets`, usually
    as `tfds`, then call the `tfds.load()` function, which will download the data
    you want (unless it was already downloaded earlier) and return the data as a dictionary
    of datasets (typically one for training and one for testing, but this depends
    on the dataset you choose). For example, let’s download MNIST:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: TFDS并未与TensorFlow捆绑在一起，但如果您在Colab上运行或者按照[*https://homl.info/install*](https://homl.info/install)的安装说明进行安装，那么它已经安装好了。然后您可以导入`tensorflow_datasets`，通常为`tfds`，然后调用`tfds.load()`函数，它将下载您想要的数据（除非之前已经下载过），并将数据作为数据集字典返回（通常一个用于训练，一个用于测试，但这取决于您选择的数据集）。例如，让我们下载MNIST：
- en: '[PRE58]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'You can then apply any transformation you want (typically shuffling, batching,
    and prefetching), and you’re ready to train your model. Here is a simple example:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您可以应用任何您想要的转换（通常是洗牌、批处理和预取），然后准备训练您的模型。这里是一个简单的示例：
- en: '[PRE59]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Tip
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'The `load()` function can shuffle the files it downloads: just set `shuffle_files=True`.
    However, this may be insufficient, so it’s best to shuffle the training data some
    more.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`load()`函数可以对其下载的文件进行洗牌：只需设置`shuffle_files=True`。但是这可能不够，最好对训练数据进行更多的洗牌。'
- en: 'Note that each item in the dataset is a dictionary containing both the features
    and the labels. But Keras expects each item to be a tuple containing two elements
    (again, the features and the labels). You could transform the dataset using the
    `map()` method, like this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据集中的每个项目都是一个包含特征和标签的字典。但是Keras期望每个项目是一个包含两个元素的元组（再次，特征和标签）。您可以使用`map()`方法转换数据集，就像这样：
- en: '[PRE60]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: But it’s simpler to ask the `load()` function to do this for you by setting
    `as_supervised=True` (obviously this works only for labeled datasets).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 但是通过设置`as_supervised=True`，让`load()`函数为您执行此操作会更简单（显然，这仅适用于带标签的数据集）。
- en: 'Lastly, TFDS provides a convenient way to split the data using the `split`
    argument. For example, if you want to use the first 90% of the training set for
    training, the remaining 10% for validation, and the whole test set for testing,
    then you can set `split=["train[:90%]", "train[90%:]", "test"]`. The `load()`
    function will return all three sets. Here is a complete example, loading and splitting
    the MNIST dataset using TFDS, then using these sets to train and evaluate a simple
    Keras model:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，TFDS提供了一种方便的方法来使用`split`参数拆分数据。例如，如果您想要使用训练集的前90%进行训练，剩余的10%进行验证，整个测试集进行测试，那么您可以设置`split=["train[:90%]",
    "train[90%:]", "test"]`。`load()`函数将返回所有三个集合。这里是一个完整的示例，使用TFDS加载和拆分MNIST数据集，然后使用这些集合来训练和评估一个简单的Keras模型：
- en: '[PRE61]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Congratulations, you’ve reached the end of this quite technical chapter! You
    may feel that it is a bit far from the abstract beauty of neural networks, but
    the fact is deep learning often involves large amounts of data, and knowing how
    to load, parse, and preprocess it efficiently is a crucial skill to have. In the
    next chapter, we will look at convolutional neural networks, which are among the
    most successful neural net architectures for image processing and many other applications.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，您已经到达了这个相当技术性的章节的结尾！您可能会觉得它与神经网络的抽象美有些远，但事实是深度学习通常涉及大量数据，知道如何高效加载、解析和预处理数据是一项至关重要的技能。在下一章中，我们将看一下卷积神经网络，这是图像处理和许多其他应用中最成功的神经网络架构之一。
- en: Exercises
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Why would you want to use the tf.data API?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么要使用tf.data API？
- en: What are the benefits of splitting a large dataset into multiple files?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将大型数据集拆分为多个文件的好处是什么？
- en: During training, how can you tell that your input pipeline is the bottleneck?
    What can you do to fix it?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中，如何判断您的输入管道是瓶颈？您可以做些什么来解决它？
- en: Can you save any binary data to a TFRecord file, or only serialized protocol
    buffers?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以将任何二进制数据保存到TFRecord文件中吗，还是只能序列化协议缓冲区？
- en: Why would you go through the hassle of converting all your data to the `Example`
    protobuf format? Why not use your own protobuf definition?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么要费心将所有数据转换为`Example`协议缓冲区格式？为什么不使用自己的协议缓冲区定义？
- en: When using TFRecords, when would you want to activate compression? Why not do
    it systematically?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用TFRecords时，何时应该激活压缩？为什么不系统地这样做？
- en: Data can be preprocessed directly when writing the data files, or within the
    tf.data pipeline, or in preprocessing layers within your model. Can you list a
    few pros and cons of each option?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据可以在编写数据文件时直接进行预处理，或者在tf.data管道中进行，或者在模型内的预处理层中进行。您能列出每个选项的一些优缺点吗？
- en: Name a few common ways you can encode categorical integer features. What about
    text?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列举一些常见的编码分类整数特征的方法。文本呢？
- en: 'Load the Fashion MNIST dataset (introduced in [Chapter 10](ch10.html#ann_chapter));
    split it into a training set, a validation set, and a test set; shuffle the training
    set; and save each dataset to multiple TFRecord files. Each record should be a
    serialized `Example` protobuf with two features: the serialized image (use `tf.io.serialize_tensor()`
    to serialize each image), and the label.⁠^([10](ch13.html#idm45720185796112))
    Then use tf.data to create an efficient dataset for each set. Finally, use a Keras
    model to train these datasets, including a preprocessing layer to standardize
    each input feature. Try to make the input pipeline as efficient as possible, using
    TensorBoard to visualize profiling data.'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载时尚MNIST数据集（在[第10章](ch10.html#ann_chapter)中介绍）；将其分为训练集、验证集和测试集；对训练集进行洗牌；并将每个数据集保存到多个TFRecord文件中。每个记录应该是一个序列化的`Example`协议缓冲区，具有两个特征：序列化图像（使用`tf.io.serialize_tensor()`来序列化每个图像），和标签。然后使用tf.data为每个集创建一个高效的数据集。最后，使用Keras模型来训练这些数据集，包括一个预处理层来标准化每个输入特征。尝试使输入管道尽可能高效，使用TensorBoard来可视化分析数据。
- en: 'In this exercise you will download a dataset, split it, create a `tf.data.Dataset`
    to load it and preprocess it efficiently, then build and train a binary classification
    model containing an `Embedding` layer:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个练习中，您将下载一个数据集，将其拆分，创建一个`tf.data.Dataset`来高效加载和预处理数据，然后构建和训练一个包含`Embedding`层的二元分类模型：
- en: Download the [Large Movie Review Dataset](https://homl.info/imdb), which contains
    50,000 movie reviews from the [Internet Movie Database (IMDb)](https://imdb.com).
    The data is organized in two directories, *train* and *test*, each containing
    a *pos* subdirectory with 12,500 positive reviews and a *neg* subdirectory with
    12,500 negative reviews. Each review is stored in a separate text file. There
    are other files and folders (including preprocessed bag-of-words versions), but
    we will ignore them in this exercise.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载[大型电影评论数据集](https://homl.info/imdb)，其中包含来自[互联网电影数据库（IMDb）](https://imdb.com)的50,000条电影评论。数据组织在两个目录中，*train*和*test*，每个目录包含一个*pos*子目录，其中包含12,500条正面评论，以及一个*neg*子目录，其中包含12,500条负面评论。每个评论存储在单独的文本文件中。还有其他文件和文件夹（包括预处理的词袋版本），但在这个练习中我们将忽略它们。
- en: Split the test set into a validation set (15,000) and a test set (10,000).
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将测试集分为验证集（15,000）和测试集（10,000）。
- en: Use tf.data to create an efficient dataset for each set.
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用tf.data为每个集创建一个高效的数据集。
- en: Create a binary classification model, using a `TextVectorization` layer to preprocess
    each review.
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个二元分类模型，使用`TextVectorization`层来预处理每个评论。
- en: Add an `Embedding` layer and compute the mean embedding for each review, multiplied
    by the square root of the number of words (see [Chapter 16](ch16.html#nlp_chapter)).
    This rescaled mean embedding can then be passed to the rest of your model.
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个`Embedding`层，并计算每个评论的平均嵌入，乘以单词数量的平方根（参见[第16章](ch16.html#nlp_chapter)）。然后将这个重新缩放的平均嵌入传递给您模型的其余部分。
- en: Train the model and see what accuracy you get. Try to optimize your pipelines
    to make training as fast as possible.
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型并查看您获得的准确性。尝试优化您的管道，使训练尽可能快。
- en: 'Use TFDS to load the same dataset more easily: `tfds.load("imdb_reviews")`.'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TFDS更轻松地加载相同的数据集：`tfds.load("imdb_reviews")`。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在本章笔记本的末尾找到，网址为[*https://homl.info/colab3*](https://homl.info/colab3)。
- en: '^([1](ch13.html#idm45720190533488-marker)) Imagine a sorted deck of cards on
    your left: suppose you just take the top three cards and shuffle them, then pick
    one randomly and put it to your right, keeping the other two in your hands. Take
    another card on your left, shuffle the three cards in your hands and pick one
    of them randomly, and put it on your right. When you are done going through all
    the cards like this, you will have a deck of cards on your right: do you think
    it will be perfectly shuffled?'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch13.html#idm45720190533488-marker)) 想象一副排好序的扑克牌在您的左边：假设您只拿出前三张牌并洗牌，然后随机选取一张放在右边，将另外两张留在手中。再从左边拿一张牌，在手中的三张牌中洗牌，随机选取一张放在右边。当您像这样处理完所有的牌后，您的右边将有一副扑克牌：您认为它会被完美洗牌吗？
- en: ^([2](ch13.html#idm45720189926208-marker)) In general, just prefetching one
    batch is fine, but in some cases you may need to prefetch a few more. Alternatively,
    you can let TensorFlow decide automatically by passing `tf.data.AUTOTUNE` to `prefetch()`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch13.html#idm45720189926208-marker)) 一般来说，只预取一个批次就可以了，但在某些情况下，您可能需要预取更多。或者，您可以通过将`tf.data.AUTOTUNE`传递给`prefetch()`，让TensorFlow自动决定。
- en: '^([3](ch13.html#idm45720189876688-marker)) But check out the experimental `tf.data.experimental.prefetch_to_device()`
    function, which can prefetch data directly to the GPU. Any TensorFlow function
    or class with `experimental` in its name may change without warning in future
    versions. If an experimental function fails, try removing the word `experimental`:
    it may have been moved to the core API. If not, then please check the notebook,
    as I will ensure it contains up-to-date code.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch13.html#idm45720189876688-marker)) 但是请查看实验性的`tf.data.experimental.prefetch_to_device()`函数，它可以直接将数据预取到GPU。任何带有`experimental`的TensorFlow函数或类的名称可能会在未来版本中发生更改而没有警告。如果实验性函数失败，请尝试删除`experimental`一词：它可能已经移至核心API。如果没有，请查看笔记本，我会确保其中包含最新的代码。
- en: ^([4](ch13.html#idm45720189164144-marker)) Since protobuf objects are meant
    to be serialized and transmitted, they are called *messages*.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch13.html#idm45720189164144-marker)) 由于protobuf对象旨在被序列化和传输，它们被称为*消息*。
- en: ^([5](ch13.html#idm45720189071424-marker)) This chapter contains the bare minimum
    you need to know about protobufs to use TFRecords. To learn more about protobufs,
    please visit [*https://homl.info/protobuf*](https://homl.info/protobuf).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch13.html#idm45720189071424-marker)) 本章包含了您使用TFRecords所需了解的最基本知识。要了解更多关于protobufs的信息，请访问[*https://homl.info/protobuf*](https://homl.info/protobuf)。
- en: '^([6](ch13.html#idm45720187048176-marker)) Tomáš Mikolov et al., “Distributed
    Representations of Words and Phrases and Their Compositionality”, *Proceedings
    of the 26th International Conference on Neural Information Processing Systems*
    2 (2013): 3111–3119.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch13.html#idm45720187048176-marker)) Tomáš Mikolov等人，“单词和短语的分布式表示及其组合性”，*第26届国际神经信息处理系统会议论文集*
    2（2013）：3111–3119。
- en: '^([7](ch13.html#idm45720187037120-marker)) Malvina Nissim et al., “Fair Is
    Better Than Sensational: Man Is to Doctor as Woman Is to Doctor”, arXiv preprint
    arXiv:1905.09866 (2019).'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch13.html#idm45720187037120-marker)) Malvina Nissim等人，“公平比耸人听闻更好：男人对医生，女人对医生”，arXiv预印本arXiv:1905.09866（2019）。
- en: ^([8](ch13.html#idm45720186424416-marker)) TensorFlow Hub is not bundled with
    TensorFlow, but if you are running on Colab or if you followed the installation
    instructions at [*https://homl.info/install*](https://homl.info/install), then
    it’s already installed.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch13.html#idm45720186424416-marker)) TensorFlow Hub没有与TensorFlow捆绑在一起，但如果您在Colab上运行或者按照[*https://homl.info/install*](https://homl.info/install)的安装说明进行安装，那么它已经安装好了。
- en: ^([9](ch13.html#idm45720186382240-marker)) To be precise, the sentence embedding
    is equal to the mean word embedding multiplied by the square root of the number
    of words in the sentence. This compensates for the fact that the mean of *n* random
    vectors gets shorter as *n* grows.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch13.html#idm45720186382240-marker)) 要精确，句子嵌入等于句子中单词嵌入的平均值乘以句子中单词数的平方根。这是为了弥补随着*n*增长，*n*个随机向量的平均值会变短的事实。
- en: ^([10](ch13.html#idm45720185796112-marker)) For large images, you could use
    `tf.io.encode_jpeg()` instead. This would save a lot of space, but it would lose
    a bit of image quality.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch13.html#idm45720185796112-marker)) 对于大图像，您可以使用`tf.io.encode_jpeg()`。这将节省大量空间，但会损失一些图像质量。
