- en: 7 Deep transfer learning for NLP with the transformer and GPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 深度迁移学习与转换器和 GPT 的自然语言处理
- en: 'This chapter covers:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖：
- en: Understanding the basics of the transformer neural network architecture
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解转换器神经网络架构的基础知识
- en: Using the Generative Pretrained Transformer (GPT) to generate text
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成预训练转换器（GPT）生成文本
- en: 'In this chapter and the following chapter, we cover some representative deep
    transfer learning modeling architectures for NLP that rely on a recently popularized
    neural architecture—*the transformer* [¹](#pgfId-1103589)—for key functions. This
    is arguably the most important architecture for natural language processing (NLP)
    today. Specifically, we will be looking at modeling frameworks such as GPT,[²](#pgfId-1103593)
    Bidirectional Encoder Representations from Transformers (BERT),[³](#pgfId-1103597)
    and multilingual BERT (mBERT).[⁴](#pgfId-1103602) These methods employ neural
    networks with even more parameters than the deep convolutional and recurrent neural
    network models that we looked at in the previous two chapters. Despite the larger
    size, these frameworks have exploded in popularity because they scale comparatively
    more effectively on parallel computing architecture. This enables even larger
    and more sophisticated models to be developed in practice. To make the content
    more digestible, we have split the coverage of these models into two chapters/parts:
    we cover the transformer and GPT neural network architectures in this chapter,
    whereas in the next chapter, we focus on BERT and mBERT.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和接下来的一章中，我们涵盖了一些依赖于最近流行的神经架构——*转换器*[¹](#pgfId-1103589)——进行关键功能的自然语言处理（NLP）的代表性深度迁移学习建模架构。这可以说是当今自然语言处理（NLP）中最重要的架构。具体来说，我们将研究诸如
    GPT,[²](#pgfId-1103593) 双向编码器表示来自转换器（BERT）[³](#pgfId-1103597) 和多语言 BERT（mBERT）[⁴](#pgfId-1103602)
    等建模框架。这些方法使用的神经网络比我们在前两章中看到的深度卷积和循环神经网络模型具有更多的参数。尽管体积更大，但这些框架因在并行计算架构上相对更有效地扩展而变得越来越受欢迎。这使得在实践中可以开发出更大更复杂的模型。为了使内容更易理解，我们将这些模型的覆盖范围分为两个章节/部分：本章我们涵盖了转换器和
    GPT 神经网络架构，而在下一章中，我们将专注于 BERT 和 mBERT。
- en: Until the arrival of the transformer, the dominant NLP models relied on recurrent
    and convolutional components, as we saw in the previous two chapters. Additionally,
    the best *sequence modeling* and *transduction* problems, such as machine translation,
    relied on an encoder-decoder architecture with an *attention mechanism* to detect
    which parts of the input influence each part of the output. The transformer aims
    to replace the recurrent and convolutional components entirely with attention.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换器到来之前，主导的 NLP 模型依赖于循环和卷积组件，就像我们在前两章中看到的一样。此外，最好的*序列建模*和*转导*问题，例如机器翻译，依赖于具有*注意机制*的编码器-解码器架构，以检测输入的哪些部分影响输出的每个部分。转换器的目标是完全用注意力替换循环和卷积组件。
- en: The goal of this and the following chapters is to provide you with a working
    understanding of this important class of models and to help you develop a good
    sense about where some of its beneficial properties come from. We introduce an
    important library—aptly named *transformers*—that makes the analysis, training,
    and application of these types of models in NLP particularly user-friendly. Additionally,
    we use the *tensor2tensor* TensorFlow package to help visualize attention functionality.
    The presentation of each transformer-based model architecture—GPT, BERT, and mBERT—is
    followed by representative code applying it to a relevant task.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和接下来的章节的目标是为您提供对这一重要模型类的工作理解，并帮助您建立起关于其一些有益属性来自何处的良好认识。我们引入了一个重要的库——名为*transformers*——使得在
    NLP 中分析、训练和应用这些类型的模型特别易于使用。此外，我们使用*tensor2tensor* TensorFlow 包来帮助可视化注意力功能。每个基于转换器的模型架构——GPT、BERT
    和 mBERT——的介绍都后跟应用它们于相关任务的代表性代码。
- en: 'GPT, which was developed by OpenAI,[⁵](#pgfId-1103611) is a transformer-based
    model that is trained with a *causal modeling objective* : to predict the next
    word in a sequence. It is also particularly suited for text generation. We show
    how to employ pretrained GPT weights for this purpose with the transformers library.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: GPT，由 OpenAI 开发，[⁵](#pgfId-1103611) 是一个基于转换器的模型，它以*因果建模目标*训练：预测序列中的下一个单词。它也特别适用于文本生成。我们展示了如何使用预训练的
    GPT 权重来实现这一目的，使用 transformers 库。
- en: 'BERT is a transformer-based model that we encountered briefly in chapter 3\.
    It was trained with the *masked modeling objective* : to fill in the blanks. Additionally,
    it was trained with the *next sentence prediction* task: to determine whether
    a given sentence is a plausible subsequent sentence after a target sentence. Although
    not suited for text generation, this model performs well on other general language
    tasks such as classification and question answering. We have already explored
    classification in some detail, so we will use the question-answering task to explore
    this model architecture in more detail than we did in chapter 3.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是一个基于 transformer 的模型，在第 3 章我们简要介绍过它。它是用*掩码建模目标*进行训练的：填补空白。此外，它还通过*下一个句子预测*任务进行了训练：确定给定句子是否是目标句子后的一个合理的后续句子。虽然不适用于文本生成，但这个模型在其他一般语言任务上表现良好，如分类和问答。我们已经比较详细地探讨了分类问题，因此我们将使用问答任务来更详细地探索这个模型架构，而不像第
    3 章中那样简略。
- en: mBERT, which stands for Multilingual BERT, is effectively BERT pretrained on
    over 100 languages simultaneously. Naturally, this model is particularly well-suited
    for cross-lingual transfer learning. We will show how the multilingual pretrained
    checkpoint can facilitate creating BERT embeddings for languages that were not
    even originally included in the multilingual training corpus. Both BERT and mBERT
    were created at Google.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: mBERT，即多语言 BERT，实际上是同时在 100 多种语言上预训练的 BERT。自然地，这个模型特别适用于跨语言迁移学习。我们将展示多语言预训练检查点如何促进为甚至在最初的多语言训练语料库中未包含的语言创建
    BERT 嵌入。BERT 和 mBERT 都是由 Google 创建的。
- en: We begin the chapter with a review of fundamental architectural components and
    visualize them in some detail with the tensor2tensor package. We follow that up
    with a section overviewing the GPT architecture, with text generation as a representative
    application of pretrained weights. The first section of chapter 8 then covers
    BERT, which we apply to the very important question-answering application as a
    representative example in a stand-alone section. Chapter 8 concludes with an experiment
    showing the transfer of pretrained knowledge from mBERT pretrained weights to
    a BERT embedding for a new language. This new language was not initially included
    in the multilingual corpus used to generate the pretrained mBERT weights. We use
    the Ghanaian language Twi as the illustrative language in this case. This example
    also provides an opportunity to further explore fine-tuning pretrained BERT weights
    on a new corpus. Note that Twi is an example of a *low-resource language*—one
    for which high-quality training data is scarce, if available at all.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章开始时回顾了基本的架构组件，并通过 tensor2tensor 软件包详细展示了它们。接着，我们介绍了 GPT 架构的概述部分，以文本生成作为预训练权重的代表应用。第
    8 章的第一部分涵盖了 BERT，我们将其应用于非常重要的问答应用作为一个独立部分的代表示例。第 8 章以一项实验结束，展示了从 mBERT 预训练权重转移到新语言的
    BERT 嵌入的知识传递。这种新语言最初并不包含在用于生成预训练 mBERT 权重的多语言语料库中。在这种情况下，我们以加纳语 Twi 作为示例语言。这个例子也提供了进一步探索在新语料库上微调预训练
    BERT 权重的机会。请注意，Twi 是*低资源语言*的一个示例——高质量的训练数据很少，如果有的话。
- en: 7.1 The transformer
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 transformer
- en: In this section, we look closer at the fundamental transformer architecture
    behind the neural model family covered by this chapter. This architecture was
    developed at Google[⁶](#pgfId-1103626) and was motivated by the observation that
    the best-performing translation models up to that point employed convolutional
    and recurrent components in conjunction with a mechanism called *attention*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们更仔细地观察了本章所涵盖的神经模型系列背后的基本 transformer 架构。这个架构是在 Google[⁶](#pgfId-1103626)
    开发的，并受到了这样一个观察的启发，即到目前为止表现最佳的翻译模型使用了卷积和循环组件，并与一个叫做*注意力*的机制结合使用。
- en: More specifically, such models employ an encoder-decoder architecture, where
    the encoder converts the input text into some intermediate numerical vector representation,
    typically called the *context vector*, and a decoder that converts this vector
    into output text. Attention allows for better performance in these models by modeling
    dependencies between parts of the output and the input. Typically, attention had
    been coupled with recurrent components. Because such components are inherently
    sequential—the internal hidden state at any given position `t` depends on the
    hidden state at the previous position `t-1`—parallelization of the processing
    of a long input sequence is not an option. Parallelization across such input sequences,
    on the other hand, quickly runs into GPU memory limitations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，这些模型采用编码器-解码器架构，其中编码器将输入文本转换为一些中间数值向量表示，通常称为上下文向量，并且解码器将该向量转换为输出文本。通过对输出和输入之间的依赖关系进行建模，注意力允许这些模型实现更好的性能。通常情况下，注意力被与循环组件耦合在一起。因为这些组件本质上是顺序的--给定任何位置`t`的内部隐藏状态都取决于前一位置`t-1`的隐藏状态--对于处理长的输入序列的并行处理不是一个选择。另一方面，跨这样的输入序列进行并行化处理很快就会遇到GPU内存限制。
- en: The transformer discards recurrence and replaces all functionality with attention.
    More specifically, it uses a flavor of attention called *self-attention*. Self-attention
    is essentially attention as previously described but applied to the same sequence
    as both input and output. This allows it to learn the dependencies between every
    part of the sequence and every other part of the same sequence. Figure 7.3 will
    revisit and illustrate this idea in more detail, so don’t worry if you can’t visualize
    it fully yet. These models have better parallelizability versus the aforementioned
    recurrent models. Looking ahead, we address exactly the reason for this in section
    7.1.2, where we use the example sentence, “He didn’t want to talk about cells
    on the cell phone because he considered it boring,” to study how various aspects
    of the infrastructure work.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器舍弃了循环并用注意力替换所有功能。更具体地说，它使用了一种称为自我注意的注意味道。自我注意实质上是之前描述过但应用于相同序列的输入和输出的注意。这使得它能够学习到序列的每个部分与同一序列的每个其他部分之间的依赖关系。图7.3将重新访问并详细说明这个想法，所以如果您还无法完全可视化它，请不要担心。与前面提到的循环模型相比，这些模型具有更好的并行性。展望未来，在7.1.2节中，我们将使用例如“他不想在手机上谈论细胞，因为他觉得这很无聊”的例句来研究基础设施的各个方面是如何工作的。
- en: Now that we understand the basics of the motivation behind this architecture,
    let’s take a look at a simplified bird’s-eye-view representation of the various
    building blocks, shown in figure 7.1.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了这种架构背后的基本动机，让我们看一下各种构建块的简化鸟瞰图表示，如图7.1所示。
- en: '![07_01](../Images/07_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![07_01](../Images/07_01.png)'
- en: Figure 7.1 A high-level representation of the transformer architecture, showing
    stacked encoders, decoders, input/output embeddings, and positional encodings
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：转换器架构的高级表示，显示堆叠的编码器、解码器、输入/输出嵌入和位置编码
- en: We see from the figure that identical encoders are stacked on the encoding,
    or left, side of the architecture. The number of stacked encoders is a tunable
    hyperparameter, with the original paper working with six. Similarly, on the decoding,
    or right, side of the architecture, six identical decoders are stacked. We also
    see that both the input and output are converted into vectors using an embedding
    algorithm of choice. This could be a word embedding algorithm such as word2vec,
    or even a CNN applied to one-hot encoded character vectors very similar to those
    we encountered in the previous chapter. Additionally, we encode the sequential
    nature of the inputs and outputs using *positional encodings*. These allow us
    to discard recurrent components while maintaining sequential awareness.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从图中可以看到，在架构的编码或左侧上堆叠了相同的编码器。堆叠编码器的数量是一个可调的超参数，原始论文中使用了六个。同样，在解码或右侧上，堆叠了六个相同的解码器。我们还看到，使用所选的嵌入算法将输入和输出转换为向量。这可以是诸如word2vec的词嵌入算法，甚至可以是应用于使用one-hot编码的字符向量的类似于我们在前一章中遇到的那些卷积神经网络。此外，我们使用位置编码来编码输入和输出的顺序性。这使我们可以舍弃循环组件，同时保持顺序感知性。
- en: Each encoder can be roughly decomposed into a self-attention layer followed
    by a feedforward neural network, as illustrated in figure 7.2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器都可以粗略地分解为一个自注意层，紧随其后是一个前馈神经网络，如图7.2所示。
- en: '![07_02](../Images/07_02.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![07_02](../Images/07_02.png)'
- en: Figure 7.2 A simplified decomposition of the encoder and decoder into self-attention,
    encoder-decoder attention, and feedforward neural networks
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 编码器和解码器的简化分解，包括自注意力、编码器-解码器注意力和前馈神经网络。
- en: As can be seen in the figure, each decoder can be similarly decomposed with
    the addition of an encoder-decoder attention layer between the self-attention
    layer and the feedforward neural network. Note that in the self-attention of the
    decoder, future tokens are “masked” when computing attention for that token—we
    will return to this at a more appropriate time. Whereas the self-attention learns
    the dependencies of every part of its input sequence and every other part of the
    same sequence, encoder-decoder attention learns similar dependencies between the
    inputs to the encoder and decoder. This process is similar to the way attention
    was initially used in the sequence-to-sequence recurrent translation models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，每个解码器都可以类似地分解，增加了一个在自注意力层和前馈神经网络之间的编码器-解码器注意力层。需要注意的是，在解码器的自注意力中，在计算该标记的注意力时，“未来标记”会被“屏蔽”--我们将在更合适的时间回到这个问题。而自注意力学习其输入序列的每个部分与同一序列的每个其他部分之间的依赖关系，编码器-解码器注意力学习编码器和解码器输入之间的类似依赖关系。这个过程类似于注意力最初被用于序列到序列的循环翻译模型的方式。
- en: The self-attention layer in figure 7.2 can further be refined into *multihead
    attention*—a multidimensional analog of self-attention that leads to improved
    performance. We analyze self-attention in further detail in the following subsections
    and build on the insights gained to cover multihead attention. The *bertviz* package[⁷](#pgfId-1103652)
    is used for visualization purposes to provide further insights. We later close
    the chapter by loading a representative transformer translation model with the
    transformers library and using it to quickly translate a couple of English sentences
    into the low-resource Ghanaian language, Twi.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2中的自注意力层可以进一步细化为*多头注意力* -- 自注意力的多维模拟，可以带来更好的性能。我们将在接下来详细分析自注意力，并借此来介绍多头注意力。*bertviz*包[⁷](#pgfId-1103652)用于可视化目的，以提供进一步的见解。后来我们关闭这一章，通过transformers库加载一个代表性的transformer翻译模型，并使用它快速将几个英文句子翻译成低资源的加纳语Twi。
- en: 7.1.1 An introduction to the transformers library and attention visualization
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 对transformers库和注意力可视化的介绍
- en: 'Before we discuss in detail how various components of multihead attention works,
    let’s visualize it for the example sentence, “He didn’t want to talk about cells
    on the cell phone because he considered it boring.” This exercise also allows
    us to introduce the transformers Python library from Hugging Face. The first step
    to doing this is to obtain the required dependencies using the following commands:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们详细讨论多头注意力各组件是如何工作之前，让我们以例句“他不想谈论手机上的细胞，因为他觉得这很无聊”进行可视化。这个练习也让我们介绍了Hugging
    Face的transformers Python库。进行这个过程的第一步是使用以下命令获取必要的依赖项：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note Recall from the previous chapters that the exclamation sign (!) is required
    only when executing in a Jupyter environment, such as the Kaggle environment we
    recommend for these exercises. When executing via a terminal, it should be dropped.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：回想一下前面的章节，感叹号(!)只在Jupyter环境中执行时需要，比如我们推荐的Kaggle环境中。在通过终端执行时，它应该被去掉。
- en: The tensor2tensor package contains the original authors’ implementation of the
    transformers architecture, together with some visualization utilities. The bertviz
    library is an extension of these visualization utilities to a large set of the
    models within the transformers library. Note that it requires JavaScript to be
    activated to render its visualization (we show you how to do that in the companion
    Kaggle notebook).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: tensor2tensor包含了transformers架构的原始作者实现，以及一些可视化工具。bertviz库是这些可视化工具对transformers库中大量模型的扩展。注意，要渲染可视化内容需要激活JavaScript（我们会在相关Kaggle笔记本中告诉你如何做）。
- en: 'The transformers library can be installed with the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: transformers库可以通过以下方式安装：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that it is already installed on new notebooks on Kaggle.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，它已经安装在Kaggle上的新笔记本中。
- en: For our visualization purposes, we look at the self-attention of a BERT encoder.
    It is arguably the most popular flavor of the transformers-based architecture
    and similar to the encoder in the encoder-decoder architecture of the original
    architecture in figure 7.1\. We will visualize BERT architecture explicitly in
    figure 8.1 of section 8.1\. For now, all you need to note is that the BERT encoder
    is identical to that of the transformer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们的可视化目的，我们看看了BERT编码器的自注意力。这可以说是基于transformer架构最流行的一种变体，类似于原始架构图7.1中编码器-解码器架构中的编码器。我们将在第8.1节的图8.1中明确可视化BERT体系结构。现在，您需要注意的是BERT编码器与transformer的编码器完全相同。
- en: 'For any pretrained model that you want to load in the transformers library,
    you need to load a tokenizer as well as the model using the following commands:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您想在transformers库中加载的任何预训练模型，需要使用以下命令加载标记器以及模型：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The transformers BERT tokenizer and model
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ transformers BERT标记器和模型
- en: ❷ Loads the uncased BERT model, making sure to output attention
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 加载不区分大小写的BERT模型，确保输出注意力
- en: ❸ Loads the uncased BERT tokenizer
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 加载不区分大小写的BERT标记器
- en: Note that the uncased BERT checkpoint we are using here is the same as what
    we used in chapter 3 (listing 3.7) when we first encountered the BERT model through
    TensorFlow Hub.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里使用的不区分大小写的BERT检查点与我们在第3章（清单3.7）中使用的相同，即当我们通过TensorFlow Hub首次遇到BERT模型时。
- en: 'You can tokenize our running example sentence, encode each token as its index
    in the vocabulary, and display the outcome using the following code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以对我们正在运行的示例句子进行标记化，将每个令牌编码为其在词汇表中的索引，并使用以下代码显示结果：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Changing return_tensors to "pt" will return PyTorch tensors.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将return_tensors更改为“pt”将返回PyTorch张量。
- en: 'This yields the following output:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生以下输出：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We could have easily returned a PyTorch tensor simply by setting `return_tensors=''pt''`.
    To see which tokens these indices correspond to, we can execute the following
    code on the `inputs` variable:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在`inputs`变量上执行以下代码轻松地返回一个PyTorch张量，只需设置`return_tensors='pt'`。要查看这些索引对应的标记，可以执行以下代码：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Extracts a sample of batch index 0 from the inputs list of lists
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从输入列表的列表中提取批次索引0的示例
- en: 'This produces the following output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生以下输出：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We notice immediately that the “special tokens” we requested via the `add_special
    _tokens` argument when encoding the `inputs` variable refers to the `'[CLS]'`
    and `'[SEP]'` tokens in this case. The former indicates the beginning of a sentence/
    sequence, whereas the latter indicates the separation point between multiple sequences
    or the end of a sequence (as in this case). Note that these are BERT-dependent,
    and you should check the documentation of each new architecture you try to see
    which special tokens it uses. The other thing we notice from this tokenization
    exercise is that the tokenization is *subword*—notice how `didnt` was split into
    `didn` and `##t`, even without the apostrophe (’), which we deliberately omitted.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即注意到，通过编码`inputs`变量时通过`add_special_tokens`参数请求的“特殊令牌”指的是此案例中的`'[CLS]'`和`'[SEP]'`令牌。前者表示句子/序列的开头，而后者表示多个序列的分隔点或序列的结束（如在此案例中）。请注意，这些是BERT相关的，您应该检查您尝试的每种新架构的文档以查看它使用的特殊令牌。我们从这次标记化练习中注意到的另一件事是分词是*次词*—请注意`didn`如何被分成`didn`和`##t`，即使没有撇号（’），我们刻意省略掉了。
- en: 'Let’s proceed to visualizing the self-attention layer of the BERT model we
    have loaded by defining the following function:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续通过定义以下函数来可视化我们加载的BERT模型的自注意力层：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ The bertviz attention head visualization method
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ bertviz注意力头可视化方法
- en: ❷ Function for displaying the multiheaded attention
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 功能用于显示多头注意力
- en: ❸ Be sure to use PyTorch with bertviz.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 一定要在bertviz中使用PyTorch。
- en: ❹ Gets the attention layer
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取注意力层
- en: ❺ Calls the internal bertviz method to display self-attention
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 调用内部bertviz方法来显示自注意力
- en: ❻ Calls our function to render visualization
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 调用我们的函数来渲染可视化
- en: Figure 7.3 shows the resulting self-attention visualization of the final BERT
    layer of our example sentence. You should play with the visualization and scroll
    through the visualizations of the various words for the various layers. Note that
    not all the attention visualizations may be as easy to interpret as this example,
    and it may take some practice to build intuition for it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3显示了我们示例句子的最终BERT层的自注意力可视化的结果。您应该使用可视化并滚动浏览各层各个词的可视化。注意，并非所有注意力可视化都像这个示例那样容易解释，这可能需要一些练习来建立直觉。
- en: '![07_03](../Images/07_03.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![07_03](../Images/07_03.png)'
- en: Figure 7.3 Self-attention visualization in the final encoding layer of the pretrained
    uncased BERT model for our example sentence. It reveals that “cells” is associated
    with “it” and “boring.” Note that this is a multihead view, with the shadings
    in each single column representing each head. Multiheaded attention is addressed
    in detail in section 7.1.2.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 我们示例句子的预训练非大小写 BERT 模型的最终编码层中的自注意可视化。它显示“细胞”与“它”和“无聊”相关联。请注意，这是一个多头视图，每个单列中的阴影代表一个头。多头注意力在第
    7.1.2 节中详细讨论。
- en: That was it! Now that we have a sense for what self-attention does, having visualized
    it in figure 7.3, let’s get into the mathematical details of how it works. We
    first start with self-attention in the next subsection and then extend our knowledge
    to the full multiheaded context afterward.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！现在我们对自注意力的作用有了一定的了解，通过在图 7.3 中进行可视化，让我们进入它的数学细节。我们首先从下一小节中的自注意力开始，然后在之后将我们的知识扩展到完整的多头上下文中。
- en: 7.1.2 Self-attention
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 自注意力
- en: Consider again the example sentence, “He didn’t want to talk about cells on
    the cell phone because he considered it boring.” Suppose we wanted to figure out
    which noun the adjective “boring” was describing. Being able to answer a question
    like this is an important ability a machine needs to have to understand context.
    We know it refers to “it,” which refers to “cells,” naturally. This was confirmed
    by our visualization in figure 7.3\. A machine needs to be taught this sense of
    context. Self-attention is the method that accomplishes this in the transformer.
    As every token in the input is processed, self-attention looks at all other tokens
    to detect possible dependencies. Recall that we achieved this same function with
    bidirectional LSTMs in the previous chapter.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑例句，“他不想谈论手机上的细胞，因为他认为这很无聊。”假设我们想弄清楚形容词“boring”描述的是哪个名词。能够回答这样的问题是机器需要具备的理解上下文的重要能力。我们知道它指的是“它”，而“它”指的是“细胞”，很自然。这在我们在图
    7.3 中的可视化中得到了证实。机器需要被教会这种上下文意识。自注意力是在transformers中实现这一点的方法。当输入中的每个标记被处理时，自注意力会查看所有其他标记以检测可能的依赖关系。回想一下，在上一章中我们通过双向
    LSTM 实现了相同的功能。
- en: So how does self-attention actually work to accomplish this? We visualize the
    essential ideas of this in figure 7.4\. In the figure, we are computing the self-attention
    weight for the word “boring.” Before delving into further detail, please note
    that once the various query, key, and value vectors for the various words are
    obtained, they can be processed independently.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 那么自注意力是如何实际工作以实现这一目标的呢？我们在图 7.4 中可视化了这个关键思想。在图中，我们正在计算单词“boring”的自注意力权重。在进一步详细说明之前，请注意一旦获取了各个单词的各种查询、键和值向量，它们就可以被独立处理。
- en: '![07_04](../Images/07_04.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![07_04](../Images/07_04.png)'
- en: Figure 7.4 A visualization of the calculation of the self-attention weight of
    the word boring in our example sentence. Observe that the computations of these
    weights for different words can be carried out independently once key, value,
    and query vectors have been created. This is the root of the increased parallelizability
    of transformers over recurrent models. The attention coefficients are what is
    visualized as intensity of shading in any given column of the multihead attention
    in figure 7.3.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 我们示例句子中单词“boring”的自注意力权重计算的可视化。请注意，一旦创建了键、值和查询向量，可以独立地计算这些单词的不同权重的计算。这是transformers在循环模型之上增加的可并行性的根源。注意系数是图
    7.3 中多头注意力中任何给定列的阴影强度的可视化。
- en: Each word is associated with a *query* vector (q), a *key* vector (k), and a
    *value* vector (v). These are obtained by multiplying the input embedding vectors
    by three matrices that are learned during training. These matrices are fixed across
    all input tokens. As shown in the figure, the query vector for the current word
    boring is used in a dot product with each word’s key vector. The results are scaled
    by a fixed constant—the square root of the dimension of the key and value vectors—and
    fed to a softmax. The output vector yields the attention coefficients that indicate
    the strength of the relationship between the current token “boring” and every
    other token in the sequence. Observe that the entries of this vector indicate
    the strength of the shadings in any given single column of the multihead attention
    we visualized in figure 7.3\. We duplicate figure 7.3 next for your convenience,
    so you can inspect the variability in shadings between the various lines.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词都与一个*查询*向量（q）、一个*键*向量（k）和一个*值*向量（v）相关联。这些向量是通过将输入嵌入向量与在训练过程中学习到的三个矩阵相乘得到的。这些矩阵在所有输入标记中都是固定的。如图所示，当前单词
    "boring" 的查询向量与每个单词的键向量进行点积。结果被一个固定常数——键和值向量维度的平方根——进行缩放，并输入到一个 softmax 函数中。输出向量产生的注意力系数表示当前标记
    "boring" 与序列中每个其他标记之间关系的强度。请注意，该向量的条目表示我们在图 7.3 中可视化的多头注意力中任何给定单列中阴影的强度。接下来，为了方便起见，我们重复了图
    7.3，这样您就可以检查不同行之间阴影变化的可变性。
- en: '![07_04_07_03_duplicated](../Images/07_04_07_03_duplicated.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![07_04_07_03_duplicated](../Images/07_04_07_03_duplicated.png)'
- en: Figure 7.3 (Duplicated) Self-attention visualization in the final encoding layer
    of the pretrained uncased BERT model for our example sentence. It reveals that
    “cells” is associated with “it” and “boring.” Note that this is a multihead view,
    with the shadings in each single column representing each head.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3（重复）预训练的不分大小写 BERT 模型在我们示例句子的最终编码层中的自注意可视化。它显示了 "cells" 与 "it" 和 "boring"
    相关联。请注意，这是一个多头视图，每个单列中的阴影代表一个头。
- en: We are now in a good position to understand why transformers are more parallelizable
    than recurrent models. Recall from our presentation that the computations of self-attention
    weights for different words can be carried out independently, once the key, value,
    and query vectors have been created. This means that for long input sequences,
    one can parallelize these computations. Recall that recurrent models are inherently
    sequential—the internal hidden state at any given position `t` depends on the
    hidden state at the previous position `t-1`. This means that parallelization of
    the processing of a long input sequence is not possible in recurrent models because
    the steps have to be executed one after the other. Parallelization across such
    input sequences, on the other hand, quickly runs into GPU memory limitations.
    An additional advantage of transformers over recurrent model is the increased
    interpretability afforded by attention visualizations, such as the one in figure
    7.3.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了足够的条件来理解为什么transformers比循环模型更具并行性。回想一下我们的介绍，不同单词的自注意力权重的计算可以在创建键、值和查询向量后独立进行。这意味着对于长输入序列，可以并行化这些计算。回想一下，循环模型本质上是顺序的——任何给定位置
    `t` 处的内部隐藏状态取决于前一个位置 `t-1` 处的隐藏状态。这意味着无法在循环模型中并行处理长输入序列，因为步骤必须依次执行。另一方面，对于这样的输入序列，跨序列的并行化很快就会遇到
    GPU 内存限制。transformers模型比循环模型的另一个优势是由注意力可视化提供的增加的可解释性，比如图 7.3 中的可视化。
- en: Note that the computation of the weight for every token in the sequence can
    be carried out independently, although some dependence between computations exists
    through the key and value vectors. This means that we can vectorize the overall
    computation using matrices as shown in figure 7.5\. The matrices Q, K, and V in
    that equation are simply the matrices made up of query, key and value vectors
    stacked together as matrices.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，可以独立地计算序列中每个标记的权重，尽管通过键和值向量存在一些计算之间的依赖关系。这意味着我们可以使用矩阵对整体计算进行向量化，如图 7.5 所示。在该方程中，矩阵
    Q、K 和 V 简单地是由查询、键和值向量堆叠在一起形成的矩阵。
- en: '![07_05](../Images/07_05.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![07_05](../Images/07_05.png)'
- en: Figure 7.5 Vectorized self-attention calculation for the whole input sequence
    using matrices
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 使用矩阵对整个输入序列进行向量化的自注意力计算
- en: What exactly is the deal with multihead attention? Now that we have presented
    self-attention, we are at a good point to address this. We have implicitly been
    presenting multihead attention as a generalization of self-attention from a single
    column, in the sense of the shadings in figure 7.3, to multiple columns. Let’s
    think about what we were doing when we very looking for the noun which “boring”
    refers to. Technically, we were looking for a noun-adjective relationship. Assume
    we had one self-attention mechanism that tracked that kind of relationship. What
    if we also needed to track subject-verb relationships? What about all other possible
    kinds of relationships? Multihead attention essentially addresses that by providing
    multiple representation dimensions, not just one.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 到底多头注意力有什么作用？既然我们已经介绍了自注意力，那么现在是一个很好的时机来解决这个问题。从单列的角度，我们已经将多头注意力隐式地作为自注意力的一般化呈现，如图7.3中的阴影部分，变为了多列。让我们思考一下，当我们寻找与“无聊”相关的名词时，我们具体做了什么。从技术上讲，我们是在寻找名词-形容词的关系。假设我们有一个跟踪这类关系的自注意力机制。如果我们还需要跟踪主-谓关系呢？还有其他可能的关系呢？多头注意力通过提供多个表示维度来解决这个问题，而不仅仅是一个。
- en: 7.1.3 Residual connections, encoder-decoder attention, and positional encoding
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 残差连接、编码器-解码器注意力和位置编码
- en: The transformer is a complex architecture with many features that we will not
    cover in as much detail as self-attention. Mastery of these details is not critical
    for you to begin applying transformers to your own problems. Therefore, we only
    briefly summarize them here and encourage you to delve into the original source
    material to deepen your knowledge over time as you gain more experience and intuition.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是一种复杂的架构，具有许多特性，我们将不像自注意力那样详细介绍。精通这些细节对于您开始将Transformer应用于自己的问题并不是至关重要的。因此，我们在这里只是简要总结它们，并鼓励您随着获得更多经验和直觉的时间不断深入学习原始资源材料。
- en: As a first such feature, we note that the simplified encoder representation
    in figure 7.2 does not show an additional residual connection between each self-attention
    layer in the encoder and a normalization layer that follows it. This is illustrated
    in figure 7.6.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一个这样的特性，我们注意到图7.2中简化的编码器表示中没有显示编码器中每个自注意层和接下来的规范化层之间的附加残差连接。这在图7.6中有所说明。
- en: '![07_06](../Images/07_06.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![07_06](../Images/07_06.png)'
- en: Figure 7.6 A more detailed and accurate breakdown of each transformer encoder,
    now incorporating residual connections and normalization layers
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 更详细和准确地拆分每个Transformer编码器，现在包括残差连接和规范化层
- en: As shown in the figure, each feedforward layer has a residual connection and
    a normalization layer after it. Analogous statements are true for the decoder.
    These residual connections allow gradients to skip the nonlinear activation functions
    within the layers, alleviating the problem of vanishing and/or exploding gradients.
    Put simply, normalization ensures that the scale of input features to all layers
    are roughly the same.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，每个前馈层在其后都有一个残差连接和一个规范化层。类似的说明也适用于解码器。这些残差连接使得梯度能够跳过层内的非线性激活函数，缓解了梯度消失和/或梯度爆炸的问题。简单地说，规范化确保所有层的输入特征的尺度大致相同。
- en: On the decoder side, recall from figure 7.2 the existence of the encoder-decoder
    attention layer, which we have not yet addressed. We duplicate figure 7.2 next
    and highlight this layer for your convenience.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码器端，回顾图7.2中编码器-解码器注意力层的存在，这一点我们还没有讨论到。接下来，我们复制图7.2并突出显示该层以方便您查看。
- en: '![07_06_007_02_duplicated](../Images/07_06_007_02_duplicated.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![07_06_007_02_duplicated](../Images/07_06_007_02_duplicated.png)'
- en: Figure 7.2 (Duplicated, encoder-decoder attention highlighted) Simplified decomposition
    of the encoder and decoder into self-attention, encoder-decoder attention, and
    feedforward neural networks
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2（复制，突出显示编码器-解码器注意力）将编码器和解码器简化为自注意、编码器-解码器注意力和前馈神经网络的分解形式
- en: It works analogously to the self-attention layer as described. The important
    distinction is that the input vectors to each decoder that represent keys and
    values come from the top of the encoder stack, whereas the query vectors come
    from the layer immediately below it. If you go through figure 7.4 again with this
    updated information in mind, you should find it obvious that the effect of this
    change is to compute attention between every output token and every input token,
    rather than between all tokens in the input sequence as was the case with the
    self-attention layer. We duplicate figure 7.4 next—adjusting it slightly for the
    encoder-decoder attention case—so you can convince yourself.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作方式类似于所描述的自我关注层。重要的区别在于，表示键和值的每个解码器的输入向量来自编码器堆栈的顶部，而查询向量来自直接位于其下面的层。如果您再次查看图7.4，并记住这个更新后的信息，您应该会发现这种变化的效果是计算每个输出标记和每个输入标记之间的注意力，而不是像自我关注层的情况那样在输入序列的所有标记之间计算。接下来我们将复制图7.4——稍作调整以适用于编码器-解码器注意力——让您自己看一看。
- en: '![07_06_007_04_duplicated](../Images/07_06_007_04_duplicated.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![07_06_007_04_duplicated](../Images/07_06_007_04_duplicated.png)'
- en: Figure 7.4 (Duplicated, slightly adjusted for encoder-decoder attention calculation)
    A visualization of the calculation of the encoder-decoder attention weight between
    the word boring in our example sentence and the output at position n. Observe
    that the computations of these weights for different words can be carried out
    independently once key, value, and query vectors have been created. This is the
    root of the increased parallelizability of transformers over recurrent models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4（重复，稍作调整以计算编码器-解码器注意力）显示了我们的例句中单词“boring”和位置n处输出之间的编码器-解码器注意力权重计算的可视化。注意，一旦创建了键、值和查询向量，就可以独立地计算不同单词的这些权重。这是转换器相对于递归模型具有更高并行性的根源。
- en: On both the encoder and decoder sides, recall from figure 7.1 the existence
    of the positional encoding, which we now address. Because we are dealing with
    sequences, it is important to model and retain the relative positions of each
    token in each sequence. Our description of the transformer operation so far has
    not touched on “positional encoding” and has been agnostic to the order in which
    the tokens are consumed by each self-attention layer. The positional embeddings
    address this, by adding to each token input embedding a vector of equal size that
    is a special function of the token’s position in the sequence. The authors used
    sine and cosine functions of frequencies that are position-dependent to generate
    these positional embeddings.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从图7.1中回顾一下，在编码器和解码器两侧都存在位置编码，我们现在对其进行解释。由于我们处理的是序列，因此对于每个序列中的每个标记建模和保留相对位置非常重要。到目前为止，我们对转换器操作的描述没有涉及“位置编码”，并且对输入的标记按顺序使用的顺序未定义。位置编码通过将等大小的向量添加到每个标记输入嵌入中来解决此问题，这些向量是该标记在序列中位置的特殊函数。作者使用了位置相关的正弦和余弦函数来生成这些位置嵌入。
- en: This brings us to the end of the transformers architecture exposition. To make
    things concrete, we conclude this section by translating a couple of English sentences
    to a low-resource language using a pretrained encoder-decoder model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对转换器架构的阐述。为了让事情具体化，我们在本节中通过使用预训练的编码器-解码器模型将几个英语句子翻译为低资源语言来进行结论。
- en: 7.1.4 Application of pretrained encoder-decoder to translation
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4 预训练编码器-解码器在翻译中的应用
- en: The goal of this subsection is to expose you to a large set of translation models
    available at your fingertips within the transformers library. The Language Technology
    Research Group at the University of Helsinki[⁸](#pgfId-1103800) has made more
    than 1,000 pretrained models available. At the time of writing, these are the
    only available open source models for many low-resource languages. We use the
    popular Ghanaian language Twi as an example here. It was trained on the JW300
    corpus,[⁹](#pgfId-1103804) which contains the only existing parallel translated
    datasets for many low-resource languages.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节的目标是让您了解转换器库中提供的大量翻译模型。赫尔辛基大学语言技术研究组[⁸](#pgfId-1103800)提供了1000多个预训练模型。在撰写本文时，这些模型是许多低资源语言仅有的可用开源模型。在这里，我们以流行的加纳语Twi为例。它是在JW300语料库[⁹](#pgfId-1103804)上进行训练的，该语料库包含许多低资源语言的唯一现有平行翻译数据集。
- en: Unfortunately, JW300 is extremely biased data, being religious text translated
    by the Jehovah’s Witnesses organization. However, our investigation revealed that
    the models are of decent quality as an initial baseline for further transfer learning
    and refinement. We do not explicitly refine the baseline model on better data
    here, due to data-collection challenges and the lack of existing appropriate datasets.
    However, we hope that taken together with the penultimate section of the next
    chapter—where we fine-tune a multilingual BERT model on monolingual Twi data—you
    will gain a powerful set of tools for further cross-lingual transfer learning
    research.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，JW300 是极具偏见的数据，是由耶和华见证人组织翻译的宗教文本。然而，我们的调查发现，这些模型作为进一步迁移学习和精炼的初始基线是相当不错的。我们在这里没有明确地在更好的数据上对基线模型进行改进，原因是数据收集的挑战和缺乏现有的合适数据集。然而，我们希望与下一章的倒数第二节一起来看——在那里我们将在单语特威语数据上对多语言
    BERT 模型进行微调——您将获得一套强大的工具，用于进一步的跨语言迁移学习研究。
- en: 'Without further ado, let’s load the pretrained English-to-Twi translation model
    and tokenizer using the following code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 不多说了，让我们使用以下代码加载预训练的英语到特威语的翻译模型和分词器：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `MarianMTModel` class is a port of encoder-decoder transformer architecture
    from the C++ library MarianNMT.[^(10)](#pgfId-1103816) Note that you can change
    the source and target languages by simply changing the language codes `en` and
    `tw` to representative codes, if made available by the research group. For instance,
    loading a French-to-English model would change the input configuration string
    to `Helsinki-NLP/opus-mt-fr-en`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`MarianMTModel` 类是从 C++ 库 MarianNMT 移植而来的编码器-解码器transformers架构。[^10](#pgfId-1103816)
    请注意，如果研究小组提供了相应的代码，你可以通过简单地更改语言代码 `en` 和 `tw` 来更改源语言和目标语言。例如，加载一个法语到英语的模型将会改变输入配置字符串为
    `Helsinki-NLP/opus-mt-fr-en`。'
- en: 'If we were chatting with a friend in Ghana online and wanted to know how to
    write “My name is Paul” by way of introduction, we could use the following code
    to compute and display the translation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在网上与加纳的朋友聊天，并想知道如何用介绍的方式写“我的名字是保罗”，我们可以使用以下代码计算并显示翻译：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Inputs the English sentence to be translated
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入要翻译的英语句子
- en: ❷ Encodes to the input token Ids
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将输入编码为标记 ID
- en: ❸ Generates the output token Ids
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成输出的标记 ID
- en: ❹ Decodes the output token IDs to actual output tokens
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将输出的标记 ID 解码为实际输出标记
- en: ❺ Displays the translation
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 显示翻译
- en: 'The resulting output from running the code is shown next:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后得到的输出结果如下所示：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first thing we immediately notice is the presence of a special token `<pad>`
    in the output that we have not seen before, as well as underscores before each
    word. This is different from the output the BERT tokenizer produced in section
    7.1.1\. The technical reason is that BERT uses a tokenizer called WordPiece, whereas
    our encoder-decoder model here uses SentencePiece. Although we do not get into
    the detailed differences between these tokenizer types here, we do use this opportunity
    to warn you once again to review documentation about any new tokenizer you try.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即注意到的第一件事是输出中存在一个我们以前没有见过的特殊标记 `<pad>`，以及每个单词前面的下划线。这与第 7.1.1 节中 BERT 分词器产生的输出不同。技术原因是
    BERT 使用了一个称为 WordPiece 的分词器，而我们这里的编码器-解码器模型使用了 SentencePiece。虽然我们在这里没有详细讨论这些分词器类型之间的差异，但我们利用这个机会再次警告您，务必查阅有关您尝试的任何新分词器的文档。
- en: The translation “Me din de Paul” happens to be exactly right. Great! That wasn’t
    too hard, was it? However, repeating the exercise for the input sentence “How
    are things?” yields the translation “*Ɔ*kwan b*ɛ*n so na nne*ɛ*ma te saa?” which
    back-translates literally into “In which way are things like this?” We can see
    that while the semantics of this translation appear close, the translation is
    wrong. The semantic similarity, however, is a sign that this model is a good baseline
    that could be improved further via transfer learning, if good parallel English-Twi
    data were available. Moreover, rephrasing the input sentence to “How are you?”
    yields the correct translation “Wo ho te d*ɛ*n?” from this model. Overall, this
    result is very encouraging, and we hope that some readers are inspired to work
    to extend these baseline models to some excellent open source transformer models
    for some previously unaddressed low-resource languages of choice.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译“Me din de Paul”恰好是完全正确的。太好了！这并不太难，是吗？然而，对于输入句子“How are things?”的重复练习却得到了翻译“*Ɔ*kwan
    b*ɛ*n so na nne*ɛ*ma te saa?”，它直译成“事情是什么样的？”我们可以看到，虽然这个翻译的语义看起来很接近，但翻译是错误的。然而，语义相似性表明，该模型是一个很好的基线，如果有好的平行英文-特威语数据可用，可以通过迁移学习进一步改进。此外，将输入句子改写为“How
    are you?”则从这个模型得到了正确的翻译“Wo ho te d*ɛ*n?”。总的来说，这个结果是非常令人鼓舞的，我们希望一些读者受到启发，致力于将这些基线模型扩展到一些以前未解决的低资源语言的优秀开源转换器模型。
- en: We look at the Generative Pretrained Transformer (GPT) next, a transformer-based
    model preferred for text generation that has become quite famous in the NLP community.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下生成式预训练转换器（GPT），这是一种基于转换器的模型，用于文本生成，在自然语言处理（NLP）社区中变得非常有名。
- en: 7.2 The Generative Pretrained Transformer
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 生成式预训练转换器
- en: The Generative Pretrained Transformer[^(11)](#pgfId-1103851) (GPT) was developed
    by OpenAI and was among the earliest models to apply the transformer architecture
    to the semisupervised learning scenario discussed in this book. By this, we mean,
    of course, the unsupervised (or self-supervised) pretraining of a language-understanding
    model on a large corpus of text data, followed by supervised fine-tuning on the
    final target data of interest. The authors found the performance on four types
    of language-understanding tasks to be significantly improved. These tasks included
    natural language inference, question answering, semantic similarity, and text
    classification. Notably, performance on the General Language-Understanding Evaluation
    (GLUE) benchmark, which includes these along with other difficult and diverse
    tasks, was improved by more than 5 percentage points.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式预训练转换器（Generative Pretrained Transformer）[^11]（GPT）是由OpenAI开发的，并且是最早将转换器架构应用于本书讨论的半监督学习场景的模型之一。通过这个，我们指的当然是在大量文本数据上无监督（或自监督）预训练语言理解模型，然后在最终感兴趣的目标数据上进行监督微调。作者发现在四种类型的语言理解任务上的性能得到了显著提升。这些任务包括自然语言推理、问答、语义相似度和文本分类。值得注意的是，在通用语言理解评估（GLUE）基准上的表现，该基准包括这些以及其他困难和多样化的任务，提高了超过5个百分点。
- en: The GPT model has undergone several iterations—GPT, GPT-2, and, very recently,
    GPT-3\. Indeed, at the time of writing, GPT-3 happens to be one of the largest
    known pretrained language models, with 175 billion parameters. Its predecessor,
    the GPT-2, stands at 1.5 billion parameters and was also considered the largest
    at the time it was released, just a year prior. Before the release of GPT-3 in
    June 2020, the largest model was Microsoft’s Turing-NLG, which stands at 17 billion
    parameters and was released in February 2020\. The sheer speed of progress on
    some of these metrics has been mind-blowing, and these records are likely to be
    made obsolete very soon. In fact, when GPT-2 was initially disclosed, the authors
    felt that not fully open sourcing the technology was the right thing to do, given
    the potential for abuse by malicious actors.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型已经经历了几次迭代——GPT、GPT-2，以及最近的GPT-3。事实上，在撰写本文时，GPT-3恰好是已知的最大的预训练语言模型之一，具有1750亿个参数。它的前身GPT-2具有15亿个参数，在其发布的前一年也被认为是最大的。在2020年6月发布GPT-3之前，最大的模型是微软的图灵-NLG，该模型具有170亿个参数，并于2020年2月发布。在某些指标上的进展速度之快令人难以置信，并且这些记录很可能很快就会过时。事实上，当最初披露GPT-2时，作者认为不完全开源技术是正确的做法，考虑到可能会被恶意行为者滥用的潜力。
- en: Although at the time of its initial release, GPT became the state of the art
    for most of the aforementioned tasks, it has generally come to be preferred as
    a text-generation model. Unlike BERT and its derivatives, which have come to dominate
    most other tasks, GPT was trained with the causal modeling objective (CLM) where
    the next token is predicted, as opposed to BERT’s masked language modeling (MLM)
    fill-in-the-blanks-type prediction objective, which we will cover in more detail
    in the next chapter.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在最初发布时，GPT成为了大多数上述任务的最先进技术，但它通常更受青睐作为一种文本生成模型。与BERT及其衍生模型不同，后者已经主导了大多数其他任务，GPT是以因果建模目标（CLM）进行训练的，其中预测下一个标记，而不是BERT的掩码语言建模（MLM）填空类型的预测目标，我们将在下一章更详细地介绍。
- en: In the next subsection, we briefly describe the key aspects of the GPT architecture.
    We follow that with an introduction to the *pipelines* API concept in the transformers
    library that is used to minimally execute pretrained models for the most common
    tasks. We do this in the context of applying GPT to the task it has come to excel
    at—text generation. Like the previous section on the encoder-decoder transformer
    and translation, we do not explicitly refine the pretrained GPT model on more-specific
    target data here. However, taken together with the final section of the next chapter—where
    we fine-tune a multilingual BERT model on monolingual Twi data—you will gain a
    powerful set of tools for further text-generation transfer learning research.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们简要描述了GPT架构的关键方面。接着介绍了transformers库中用于最常见任务的预训练模型的最小执行的*pipelines* API概念。我们将此概念应用于GPT在其擅长的任务——文本生成方面。与前一节关于编码器-解码器transformers和翻译的内容一样，我们在此处不会明确地在更特定的目标数据上对预训练的GPT模型进行改进。然而，结合下一章的最后一节——我们在单语Twi数据上对多语言BERT模型进行微调——您将获得一套用于进一步文本生成迁移学习研究的强大工具。
- en: 7.2.1 Architecture overview
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 架构概述
- en: You may recall from section 7.1.1, where we visualized BERT self-attention,
    that BERT is essentially a stacked set of encoders of the original encoder-decoder
    transformer architecture. GPT is essentially the converse of that, in the sense
    that it stacks the decoders instead. Recall from figure 7.2 that besides the encoder-decoder
    attention, the other distinguishing feature of the transformer decoder is that
    its self-attention layer is “masked,” that is, future tokens are “masked” when
    computing attention for any given token. We duplicate figure 7.2 for your convenience,
    highlighting this masked layer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得7.1.1节中我们可视化BERT自注意力的情况，BERT本质上是原始编码器-解码器transformers架构的一组叠加编码器。从这个意义上讲，GPT本质上是它的反义词，它将解码器堆叠起来。从图7.2中可以看出，除了编码器-解码器注意力之外，transformers解码器的另一个显著特征是其自注意力层是“掩码的”，即在计算给定标记的注意力时，“未来标记”被“掩码”了。我们复制图7.2供您参考，突出显示此掩码层。
- en: '![07_06_07_02_duplicated2](../Images/07_06_07_02_duplicated2.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![07_06_07_02_duplicated2](../Images/07_06_07_02_duplicated2.png)'
- en: Figure 7.2 (Duplicated, masked layer highlighted) The simplified decomposition
    of the encoder and decoder into self-attention, encoder-decoder attention, and
    feedforward neural networks
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2（重复，突出显示掩码层）将编码器和解码器简化为自注意力、编码器-解码器注意力和前馈神经网络的分解形式
- en: In the attention calculation that we went through in figure 7.3, this just means
    including only the tokens in “he didnt want to talk about cells” in the calculation
    and ignoring the rest. We duplicate figure 7.3 next, slightly modified so you
    can clearly see the future tokens being masked.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在图7.3中经历的注意力计算中，这只意味着在计算中只包括“他不想谈论细胞”中的标记，并忽略其余的标记。我们稍后复制图7.3，稍作修改，以便您清楚地看到未来标记被掩盖的情况。
- en: '![07_06_07_03_duplicated2](../Images/07_06_07_03_duplicated2.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![07_06_07_03_duplicated2](../Images/07_06_07_03_duplicated2.png)'
- en: Figure 7.3 (Duplicated again, modified for masked self-attention) A masked self-attention
    visualization for our example sentence, showing future tokens being masked for
    causality.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3（再次重复，为掩码自注意力修改）我们示例句子的掩码自注意力可视化，显示了因果关系中未来标记的被掩盖情况。
- en: This introduces a sense of causality into the system and suitability for text
    generation, or predicting the next token. Because there is no encoder, the encoder-decoder
    attention is also dropped. Taking these factors into account, we show the GPT
    architecture in figure 7.7.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这为系统引入了因果关系的感觉，并适用于文本生成，或预测下一个标记。由于没有编码器，编码器-解码器注意力也被删除了。考虑到这些因素，我们在图7.7中展示了GPT的架构。
- en: Note in figure 7.7 that the same output can be used for both text prediction/
    generation and classification for some other task. Indeed, the authors devised
    an input transformation scheme where multiple tasks could be handled by the same
    architecture without any architectural changes. For instance, consider the task
    of *textual entailment*, which roughly corresponds to determining if one *premise*
    statement implies another *hypothesis* statement. The input transformation concatenates
    the premise and hypothesis statements, separated by a special delimiter token,
    and feeds the resulting single contiguous string to the same unmodified architecture
    to classify whether or not entailment exists. On the other hand, consider the
    important question-answering application. Here, given some context document, a
    question, and a set of possible answers, the task is to determine which answer
    is the best potential answer to the question. Here, the input transformation is
    to concatenate the context, question, and each possible answer before passing
    each resulting contiguous string through the same model and performing a softmax
    over the corresponding outputs to determine the best answer. A similar input transformation
    was devised for the sentence similarity task as well.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意图 7.7 中，同样的输出可以用于一些其他任务的文本预测/生成和分类。事实上，作者设计了一个输入转换方案，使得多个任务可以通过相同的架构处理，而不需要任何架构更改。例如，考虑到
    *文本蕴涵* 任务，它大致对应于确定一个 *前提* 陈述是否暗示另一个 *假设* 陈述。输入转换会将前提和假设陈述连接起来，用一个特殊的分隔符标记分隔，然后将结果的单一连续字符串馈送到相同的未修改架构，以分类是否存在蕴涵。另一方面，考虑到重要的问答应用。在这里，给定一些上下文文档、一个问题和一组可能的答案，任务是确定哪个答案是问题的最佳潜在答案。在这里，输入转换是将上下文、问题和每个可能的答案连接在一起，然后通过相同的模型将每个结果的连续字符串传递，并对相应的输出执行
    softmax，以确定最佳答案。类似的输入转换也适用于句子相似性任务。
- en: '![07_07](../Images/07_07.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![07_07](../Images/07_07.png)'
- en: Figure 7.7 A high-level representation of the GPT architecture, showing stacked
    decoders, input embeddings, and positional encodings. The output from the top
    can be used for both text prediction/generation and classification.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 GPT 架构的高级表示，显示了堆叠的解码器、输入嵌入和位置编码。顶部的输出可以用于文本预测/生成和分类。
- en: Having briefly introduced the architecture of GPT, let’s use a pretrained version
    of it for some fun coding experiments. We first use it to generate some open-ended
    text given a prompt. We then also use a modification of GPT built at Microsoft—DialoGPT[^(12)](#pgfId-1103891)—to
    perform multiturn conversations with a chatbot in the next subsection.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 简要介绍了 GPT 的架构之后，让我们使用它的一个预训练版本进行一些有趣的编码实验。我们首先使用它生成一些开放式文本，给定一个提示。然后，在下一小节中，我们还将使用由微软构建的
    GPT 的修改版本——DialoGPT[^(12)](#pgfId-1103891)，来执行与聊天机器人的多轮对话。
- en: 7.2.2 Transformers pipelines introduction and application to text generation
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 转换器流水线介绍及应用于文本生成
- en: The first thing we will do in this subsection is generate some open-ended text
    using GPT. We will also use this opportunity to introduce pipelines—an API exposing
    the pretrained models in the transformers library for inference that is even simpler
    than what we did for translation in section 7.1.4\. The stated goal of the transformers
    authors is for this API to abstract away complex code for some frequently used
    tasks, including named entity recognition, masked language modeling, sentiment
    analysis, and question answering. Suitably for our purposes in this subsection,
    text generation is also an option.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将首先使用 GPT 生成一些开放式文本。我们还将利用这个机会介绍管道——一个 API，用于推断中暴露预训练模型在 transformers
    库中的 API，甚至比我们在第 7.1.4 节中进行的翻译更简单。transformers 作者的声明目标是，这个 API 可以摒弃一些常用任务的复杂代码，包括命名实体识别、遮蔽语言建模、情感分析和问答。适合我们在本小节中的目的，文本生成也是一种选择。
- en: 'Let’s start by initializing the transformers pipeline to the GPT-2 model via
    the following two lines of code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下两行代码初始化转换器管道到 GPT-2 模型：
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'By way of reminder, GPT in its original form is well suited for open-ended
    text generation, such as creative writing of sections of text to complement previous
    text. Let us see what the model generates when primed by “Somewhere over the rainbow...,”
    up to a maximum of 100 tokens, via the following command:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，GPT 最初的形式非常适合于开放式文本生成，比如创造性地写出一些文本段落来补充之前的文本。让我们看看当模型以“在彩虹的那边……”为提示，生成最多
    100 个标记时，模型生成了什么，通过以下命令：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This generates the following text:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下文本：
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This seems very semantically correct, even if the message is a bit incoherent.
    You could imagine a creative writer using this to generate ideas to get around
    writer’s block! Now, let’s see if we can prime the model with something less “creative,”
    something more technical, to see how it will do. Let’s prime the model with the
    text “Transfer learning is a field of study” via the following code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 即使消息有些不连贯，这似乎在语义上非常正确。您可以想象一位创意作家使用它来生成想法以克服写作困境！现在，让我们看看是否可以用一些不太“创意”的东西来启动模型，一些更技术性的东西，以查看它的表现。让我们通过以下代码将模型启动文本设置为“迁移学习是一门研究领域”：
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This produces the following output:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Again, we can see this text is pretty good in terms of semantic coherence, grammatic
    structure, spelling, punctuation, and so on—indeed, eerily good. However, as it
    continues, it becomes arguably factually incorrect. We can all agree that transfer
    learning requires a thorough grounding in mathematics for true understanding and
    even argue that it has been around for centuries—via us, the humans! However,
    it is not a field of physics, even if it might be somewhat similar in terms of
    the kinds of skills required to master it. We can see that the model’s output
    becomes less plausible the longer it is allowed to speak.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以看到，从语义连贯性、语法结构、拼写、标点等方面来看，这段文字非常好——实际上，甚至有点诡异。然而，随着它的继续，它变得可能事实上不正确。我们都可以同意，要真正理解迁移学习，需要对数学有扎实的基础，甚至可以说它已经存在了几个世纪——通过我们，人类！然而，它不是物理学的一个领域，即使在需要掌握它的技能方面可能有些类似。我们可以看到，模型的输出允许它说话的时间越长，就越不可信。
- en: Please be sure to experiment some more to get a sense of the model’s strengths
    and weaknesses. For instance, you could try prompting the model with our example
    sentence, “He didn’t want to talk about cells on the cell phone because he considered
    it boring.” We found this a plausible application in the creative writing space
    and perhaps in technical writing space with `max_length` set to a smaller number.
    It is already a plausible aid for many authors out there. One can only imagine
    what GPT-3, which has not yet been fully released to the public at the time of
    writing, will be able to do. The future is indeed very exciting.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必进行更多实验，以了解模型的优缺点。例如，您可以尝试使用我们的示例句子提示模型，“他不想在手机上谈论细胞，因为他认为这很无聊。”我们发现这在创意写作空间和技术写作空间中都是一个可信的应用，`max_length`设置为较小的数值。对许多作者来说，它已经是一个可信的辅助工具。在撰写本文时，我们只能想象GPT-3能够做到什么。未来确实非常令人兴奋。
- en: Having played around with text generation, let us see if we can use it somehow
    to create a chatbot.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 玩弄文本生成后，让我们看看是否可以以某种方式使用它来创建聊天机器人。
- en: 7.2.3 Application to chatbots
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 聊天机器人的应用
- en: It seems intuitively that one should be able to adopt GPT without major modification
    to this application. Luckily for us, the folks at Microsoft already did this via
    the model DialoGPT, which was also recently included in the transformers library.
    Its architecture is the same as GPT’s, with the addition of special tokens to
    indicate the end of a participant’s turn in a conversation. After seeing such
    a token, we can add the new contribution of the participant to the priming context
    text and iteratively repeat the process via direct application of GPT to generate
    a response from our chatbot. Naturally, the pretrained GPT model was fine-tuned
    on conversational text to make sure the response would be appropriate. The authors
    used Reddit threads for the fine-tuning.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉上应该能够无需对此应用进行重大修改即可采用GPT。幸运的是，微软的人员已经通过模型DialoGPT完成了这一点，该模型最近也被包含在transformers库中。它的架构与GPT相同，只是增加了特殊标记，以指示对话中参与者的回合结束。在看到这样的标记后，我们可以将参与者的新贡献添加到启动上下文文本中，并通过直接应用GPT来生成聊天机器人的响应，迭代重复这个过程。自然地，预训练的GPT模型在会话文本上进行了微调，以确保响应是适当的。作者们使用Reddit主题进行了微调。
- en: Let’s go ahead and build a chatbot! We will not use pipelines in this case,
    because this model isn’t yet exposed through that API at the time of writing.
    This allows us to juxtapose the different methods of calling these models for
    inference, which is a useful exercise for you to go through.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续构建一个聊天机器人吧！在这种情况下，我们不会使用管道，因为在撰写本文时，该模型尚未通过该API公开。这使我们能够对比调用这些模型进行推理的不同方法，这对你来说是一个有用的练习。
- en: 'The first thing to do is load the pretrained model and tokenizer via the following
    commands:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的事情是通过以下命令加载预训练模型和分词器：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Note that the DialoGPT model uses the GPT-2 class.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 请注意，DialoGPT 模型使用 GPT-2 类。
- en: ❷ We use Torch here, rather than TensorFlow, because that is the default platform
    choice in transformers documentation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们在这里使用 Torch 而不是 TensorFlow，因为 transformers 文档中默认选择的是 Torch 平台。
- en: A few things are worth highlighting at this stage. First, note that we are using
    the GPT-2 model classes, which is consistent with our earlier discussion that
    described DialoGPT as a direct application of that architecture. Additionally,
    note that we could have used the classes `AutoModelWithLMHead` and `AutoTokenizer`
    interchangeably with these GPT-specific model classes. These utility classes detect
    the best classes to load the model that is specified by their input strings—in
    this case, they would detect that the best classes to use were `GPT2LMHeadModel`
    and `GPT2Tokenizer`. You are likely to run across these utility classes as you
    peruse the transformers library documentation, and it is good to be aware of their
    existence because they can make your code more general. Finally, note that the
    “LMHead” version of the GPT is used here. This means that the output from the
    vanilla GPT is passed through one linear layer and one normalization layer, followed
    by a transformation into a vector of probabilities of a dimension equal to the
    size of the vocabulary. The maximum value corresponds to the next most likely
    token if the model is correctly trained.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此处值得强调几点。首先，请注意我们使用的是 GPT-2 模型类，这与我们先前讨论的 DialoGPT 作为该架构的直接应用是一致的。另外，请注意我们可以与这些
    GPT 特定的模型类交换使用`AutoModelWithLMHead`和`AutoTokenizer`类。这些实用程序类会检测用于加载指定模型的最佳类别，例如，在这种情况下，它们将检测到最佳要使用的类别为`GPT2LMHeadModel`和`GPT2Tokenizer`。浏览
    transformers 库文档时，你可能会遇到这些实用程序类，了解它们的存在对你的代码更一般化是有好处的。最后请注意，这里使用的是 GPT 的“LMHead”版本。这意味着从普通
    GPT 得到的输出将通过一个线性层和一个归一化层，然后转换成一个维度等于词汇表大小的概率向量。最大值对应于模型正确训练的情况下下一个最有可能的令牌。
- en: The code for conversing with our loaded, pretrained DialoGPT model is shown
    in listing 7.1\. We first specify a maximum number of responses of five. We then
    encode the conversational contribution of the user at each turn, append the contribution
    to the chat history, and feed that to the loaded pretrained DialoGPT model for
    generating the next response.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们加载的预训练 DialoGPT 模型进行对话的代码如列表 7.1 所示。我们首先指定最多五个回应的最大数量。然后，我们编码用户在每个轮次的对话，将对话添加到聊天历史记录中，并将其传递给加载的预训练
    DialoGPT 模型，以生成下一个响应。
- en: Listing 7.1 Conversing with the pretrained DialoGPT model for up to five chatbot
    responses
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 与预训练 DialoGPT 模型进行对话，最多五个聊天机器人响应
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Chats for five lines
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 五行聊天
- en: ❷ Encodes new user input, adds an end-of-sentence token, and returns Tensor
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编码新用户输入，添加一个句子结束标记，并返回张量
- en: ❸ Adds new input to the chat history
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将新输入添加到聊天历史记录中
- en: ❹ Generates a response of up to max_length tokens with chat history as context
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用聊天历史记录作为上下文生成最多 max_length 令牌的响应
- en: ❺ Displays the response
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 显示响应
- en: 'One could easily play with this bot all day! We had a lot of fun asking it
    various questions and prompting it in various ways. We include one such exchange
    here for your amusement:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人可能整天都和这个机器人玩耍！我们很开心地向它提问各种问题，并以各种方式提示它。我们在这里包括了其中一个交流以供您娱乐：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'It’s quite plausible that the entity at the other end of this short conversation
    is a human, isn’t it? Does that mean it passes the Turing test? Not quite, as
    the following exchange illustrates:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来这短暂对话的另一端很可能是一个人，不是吗？那是否意味着它通过了图灵测试？并非如此，下面的交流说明了这一点：
- en: '[PRE19]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As you increase the number of allowable conversational turns, you will find
    the bot getting stuck in repeated responses that are off-topic. This is analogous
    to the GPT open-ended text generation becoming more nonsensical as the length
    of generated text increases. One simple way to improve this is to keep a fixed
    local context size, where the model is prompted with conversation history only
    within that context. Of course, this means the conversation will not always take
    the context of the entire conversation into account—a trade-off that has to be
    explored experimentally for any given application.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当你增加允许的对话轮次数量时，你会发现机器人会陷入重复的与话题无关的回复中。这类似于 GPT 开放式文本生成随着生成文本长度的增加变得更加荒谬。改善这一点的一个简单方法是保持固定的局部上下文大小，其中模型只受到该上下文内的对话历史的提示。当然，这意味着对话不总是考虑整个对话的上下文——这是必须对任何给定应用进行实验探索的一个权衡。
- en: 'It is exciting to ponder how well GPT-3 will do on some of these problems,
    isn’t it? In the final chapter of this book, we will briefly discuss GPT-3 in
    more detail and introduce a recent smaller but worthy open source alternative
    to it: GPT-Neo from EleutherAI. It is already available in the transformers library
    and can be used directly by setting the `model` string to one of the model names
    provided by EleutherAI.[^(13)](#pgfId-1103987) We also include a companion notebook
    showing it in action for the exercise we performed in this chapter. Upon inspection,
    you should find its performance better but naturally at a significantly higher
    cost (the weights of the largest model are more than 10 GB in size!).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下 GPT-3 在这些问题上的表现会有多好，是不是令人兴奋？在本书的最后一章中，我们将简要讨论更多关于 GPT-3 的细节，并介绍一个最近推出的更小但同样值得关注的开源替代品：EleutherAI
    的 GPT-Neo。它已经可以在 transformers 库中使用，并且可以通过将 `model` 字符串设置为 EleutherAI 提供的模型名称之一来直接使用。[^13]我们还附上了一个伴随笔记本，在其中展示了它在本章练习中的应用。经过检查，你应该会发现它的性能更好，但自然也会有显著更高的成本（最大模型的权重超过
    10 GB！）。
- en: We take a look at arguably the most important member of the transformer family—BERT—in
    the next chapter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论transformers家族中可能最重要的成员——BERT。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: The transformer architecture uses self-attention to build bidirectional context
    for understanding text. This has enabled it to become a dominant language model
    in NLP recently.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformers架构使用自注意力机制来构建文本的双向上下文。这使得它成为了近期在自然语言处理中占主导地位的语言模型。
- en: The transformer allows tokens in a sequence to be processed independently of
    each other. This achieves greater parallelizability than bi-LSTMs, which process
    tokens in order.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformers允许对序列中的令牌进行独立处理。这比处理顺序的双向LSTM实现了更大的并行性。
- en: The transformer is a good choice for translation applications.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformers是翻译应用的不错选择。
- en: The Generative Pretrained Transformer uses the causal modeling objective during
    training. This makes it the model of choice for text generation, such as chatbot
    applications.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，生成预训练transformers使用因果建模目标。这使得它成为文本生成的首选模型，例如聊天机器人应用。
- en: 1. A. Vaswani et al., “Attention Is All You Need,” NeurIPS (2017).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 1. A. Vaswani等人，“Attention Is All You Need”，NeurIPS（2017）。
- en: 2. A. Radford et al., “Improving Language Understanding by Generative Pre-Training,”
    arXiv (2018).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 2. A. Radford等人，“通过生成预训练来改善语言理解”，arXiv（2018）。
- en: '3. M. E. Peters et al., “BERT: Pre-Training of Deep Bidirectional Transformers
    for Language Understanding,” Proc. of NAACL-HLT (2019).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 3. M. E. Peters等人，“BERT：用于语言理解的深度双向transformers的预训练”，NAACL-HLT（2019）。
- en: 4. [https://github.com/google-research/bert/blob/master/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 4. [https://github.com/google-research/bert/blob/master/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md)
- en: 5. A. Radfordet al., “Improving Language Understanding by Generative Pre-Training,”
    arXiv (2018).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 5. A. Radford等人，“通过生成预训练来改善语言理解”，arXiv（2018）。
- en: 6. A. Vaswaniet al., “Attention Is All You Need,” NeurIPS (2017).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 6. A. Vaswani等人，“Attention Is All You Need”，NeurIPS（2017）。
- en: 7. [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 7. [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)
- en: 8. [https://huggingface.co/Helsinki-NLP](https://huggingface.co/Helsinki-NLP)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 8. [https://huggingface.co/Helsinki-NLP](https://huggingface.co/Helsinki-NLP)
- en: 9. [http://opus.nlpl.eu/JW300.php](http://opus.nlpl.eu/JW300.php)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 9. [http://opus.nlpl.eu/JW300.php](http://opus.nlpl.eu/JW300.php)
- en: 10. [https://marian-nmt.github.io/](https://marian-nmt.github.io/)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 10. [https://marian-nmt.github.io/](https://marian-nmt.github.io/)
- en: 11. A. Radford et al., “Improving Language Understanding by Generative Pre-Training,”
    *arXiv* (2018).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 11. A. Radford等人，“通过生成式预训练提高语言理解能力”，*arXiv*（2018）。
- en: '12. Y. Zhang et al., “DialoGPT: Large-Scale Generative Pretraining for Conversational
    Response Generation,” *arXiv* (2019).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 12. Y. Zhang等人，“DialoGPT：面向对话回应生成的大规模生成式预训练”，*arXiv*（2019）。
- en: 13. [https://huggingface.co/EleutherAI](https://huggingface.co/EleutherAI)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 13. [https://huggingface.co/EleutherAI](https://huggingface.co/EleutherAI)
