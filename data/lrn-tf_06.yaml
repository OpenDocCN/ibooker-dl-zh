- en: 'Chapter 6\. Text II: Word Vectors, Advanced RNN, and Embedding Visualization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。文本II：词向量、高级RNN和嵌入可视化
- en: In this chapter, we go deeper into important topics discussed in [Chapter 5](ch05.html#text_i)
    regarding working with text sequences. We first show how to train word vectors
    by using an unsupervised method known as *word2vec*, and how to visualize embeddings
    interactively with TensorBoard. We then use pretrained word vectors, trained on
    massive amounts of public data, in a supervised text-classification task, and
    also introduce more-advanced RNN components that are frequently used in state-of-the-art
    systems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了在[第5章](ch05.html#text_i)中讨论的与文本序列处理相关的重要主题。我们首先展示了如何使用一种称为*word2vec*的无监督方法训练词向量，以及如何使用TensorBoard交互地可视化嵌入。然后我们在监督文本分类任务中使用预训练的词向量，在大量公共数据上进行训练，并介绍了在最先进系统中经常使用的更高级的RNN组件。
- en: Introduction to Word Embeddings
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入简介
- en: In [Chapter 5](ch05.html#text_i) we introduced RNN models and working with text
    sequences in TensorFlow. As part of the supervised model training, we also trained
    word vectors—mapping from word IDs to lower-dimensional continuous vectors. The
    reasoning for this was to enable a scalable representation that can be fed into
    an RNN layer. But there are deeper reasons for the use of word vectors, which
    we discuss next.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#text_i)中，我们介绍了RNN模型和在TensorFlow中处理文本序列。作为监督模型训练的一部分，我们还训练了词向量，将单词ID映射到低维连续向量。这样做的原因是为了实现可扩展的表示，可以输入到RNN层中。但是使用词向量还有更深层次的原因，我们接下来会讨论。
- en: 'Consider the sentence appearing in [Figure 6-1](#generating_skip_grams_from_text):
    “Our company provides smart agriculture solutions for farms, with advanced AI,
    deep-learning.” This sentence may be taken from, say, a tweet promoting a company. As
    data scientists or engineers, we now may wish to process it as part of an advanced
    machine intelligence system, that sifts through tweets and automatically detects
    informative content (e.g., public sentiment).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑出现在[图6-1](#generating_skip_grams_from_text)中的句子：“我们公司为农场提供智能农业解决方案，具有先进的人工智能、深度学习。”这句话可能来自于一条推广公司的推文。作为数据科学家或工程师，我们现在可能希望将其作为先进机器智能系统的一部分进行处理，该系统可以筛选推文并自动检测信息内容（例如公众情绪）。
- en: In one of the major traditional natural language processing (NLP) approaches
    to text processing, each of the words in this sentence would be represented with
    N ID—say, an integer. So, as we posited in the previous chapter, the word “agriculture”
    might be mapped to the integer 3452, the word “farm” to 12, “AI” to 150, and “deep-learning”
    to 0.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的自然语言处理（NLP）方法中，每个单词都会用N个ID来表示，比如一个整数。因此，正如我们在上一章中提出的，单词“agriculture”可能被映射到整数3452，单词“farm”到12，“AI”到150，“deep-learning”到0。
- en: While this representation has led to excellent results in practice in some basic
    NLP tasks and is still often used in many cases (such as in bag-of-words text
    classification), it has some major inherent problems. First, by using this type
    of atomic representation, we lose all meaning encoded within the word, and crucially,
    we thus lose information on the semantic proximity between words. In our example,
    we of course know that “agriculture” and “farm” are strongly related, and so are
    “AI” and “deep-learning,” while deep learning and farms don’t usually have much
    to do with one another. This is not reflected by their arbitrary integer IDs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种表示在一些基本的NLP任务中取得了出色的结果，并且在许多情况下仍然经常使用（例如在基于词袋的文本分类中），但它存在一些主要的固有问题。首先，通过使用这种原子表示，我们丢失了单词内部编码的所有含义，关键的是，我们因此丢失了单词之间语义接近的信息。在我们的例子中，我们当然知道“agriculture”和“farm”之间有很强的关联，“AI”和“deep-learning”也是如此，而深度学习和农场通常没有太多关联。这并没有反映在它们的任意整数ID中。
- en: Another important issue with this way of looking at data stems from the size
    of typical vocabularies, which can easily reach huge numbers. This means that
    naively, we could need to keep millions of such word identifiers, leading to great
    data sparsity and in turn, making learning harder and more expensive.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这种查看数据的方式的另一个重要问题源于典型词汇量的规模，很容易达到庞大的数字。这意味着天真地说，我们可能需要保留数百万个这样的单词标识符，导致数据稀疏性增加，进而使学习变得更加困难和昂贵。
- en: With images, such as in the MNIST data we used in the first section of [Chapter 5](ch05.html#text_i),
    this is not quite the case. While images can be high-dimensional, their natural
    representation in terms of pixel values already encodes some semantic meaning,
    and this representation is dense.  In practice, RNN models like the one we saw
    in Chapter [5](ch05.html#text_i) require dense vector representations to work
    well.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像，比如我们在[第5章](ch05.html#text_i)第一节中使用的MNIST数据，情况并非如此。虽然图像可以是高维的，但它们在像素值方面的自然表示已经编码了一些语义含义，并且这种表示是密集的。实际上，像我们在第5章中看到的RNN模型需要密集的向量表示才能很好地工作。
- en: We would like, therefore, to use dense vector representations of words, which
    carry semantic meaning. But how do we obtain them?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望使用携带语义含义的单词的密集向量表示。但是我们如何获得它们呢？
- en: In [Chapter 5](ch05.html#text_i) we trained supervised word vectors to solve
    a specific task, using labeled data. But it is often expensive for individuals
    and organizations to obtain labeled data, in terms of the resources, time, and
    effort involved in manually tagging texts or somehow acquiring enough labeled
    instances. Obtaining huge amounts of unlabeled data, however, is often a much
    less daunting endeavor. We thus would like a way to use this data to train word
    representations, in an unsupervised fashion.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#text_i)中，我们训练了监督词向量来解决特定任务，使用了标记数据。但是对于个人和组织来说，获取标记数据往往是昂贵的，需要耗费大量资源、时间和精力来手动标记文本或以某种方式获取足够的标记实例。然而，获取大量未标记数据往往是一个不那么艰巨的任务。因此，我们希望找到一种方法来利用这些数据以无监督的方式训练词表示。
- en: 'There are actually many ways to do unsupervised training of word embeddings,
    including both more traditional approaches to NLP that can still work very well
    and newer methods, many of which use neural networks. Whether old or new, these
    all rely at their core on the *distributional hypothesis*, which is most easily
    explained by a well-known quote by linguist John Firth: “You shall know a word
    by the company it keeps.”  In other words, words that tend to appear in similar
    contexts tend to have similar semantic meanings.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上有许多方法可以进行无监督训练词嵌入，包括传统的 NLP 方法和许多使用神经网络的新方法，这些方法无论新旧，都在核心上依赖于*分布假设*，这最容易通过语言学家约翰·弗斯的一句著名引言来解释：“你可以通过它的伙伴来认识一个词。”
    换句话说，倾向于出现在相似上下文中的词往往具有相似的语义含义。
- en: In this book, we focus on powerful word embedding methods based on neural networks.
    In [Chapter 5](ch05.html#text_i) we saw how to train them as part of a downstream
    text-classification task. We now show how to train word vectors in an unsupervised
    manner, and then how to use pretrained vectors that were trained using huge amounts
    of text from the web.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们专注于基于神经网络的强大词嵌入方法。在[第5章](ch05.html#text_i)中，我们看到了如何将它们作为下游文本分类任务的一部分进行训练。我们现在展示如何以无监督的方式训练词向量，然后如何使用在网络上大量文本训练的预训练向量。
- en: Word2vec
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2vec
- en: Word2vec is a very well-known unsupervised word embedding approach. It is actually
    more like a family of algorithms, all based in some way on exploiting the context
    in which words appear to learn their representation (in the spirit of the distributional
    hypothesis). We focus on the most popular word2vec implementation, which trains
    a model that, given an input word, predicts the word’s context by using something
    known as *skip-grams*. This is actually rather simple, as the following example
    will demonstrate.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 是一个非常著名的无监督词嵌入方法。实际上，它更像是一系列算法，所有这些算法都以某种方式利用单词出现的上下文来学习它们的表示（符合分布假设的精神）。我们专注于最流行的
    word2vec 实现，它训练一个模型，给定一个输入词，通过使用所谓的*跳字*来预测单词的上下文。这实际上相当简单，下面的例子将会演示。
- en: 'Consider, again, our example sentence: “Our company provides smart agriculture
    solutions for farms, with advanced AI, deep-learning.” We define (for simplicity) the
    context of a word as its immediate neighbors (“the company it keeps”)—i.e., the
    word to its left and the word to its right. So, the context of “company” is [our,
    provides], the context of “AI” is [advanced, deep-learning], and so on (see [Figure 6-1](#generating_skip_grams_from_text)).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑我们的例句：“Our company provides smart agriculture solutions for farms, with
    advanced AI, deep-learning.” 我们（为了简单起见）将一个词的上下文定义为它的直接邻居（“它的伙伴”）——即，它左边的词和右边的词。因此，“company”的上下文是[our,
    provides]，“AI”的上下文是[advanced, deep-learning]，等等（参见[图6-1](#generating_skip_grams_from_text)）。
- en: '![](assets/letf_0601.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0601.png)'
- en: Figure 6-1\. Generating skip-grams from text.
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1。从文本生成跳字。
- en: In the skip-gram word2vec model, we train a model to predict context based on
    an input word. All that means in this case is that we generate training instance
    and label pairs such as (our, company), (provides, company), (advanced, AI), (deep-learning,
    AI), etc.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳字 word2vec 模型中，我们训练一个模型来根据输入词预测上下文。在这种情况下，这意味着我们生成训练实例和标签对，如（our，company），（provides，company），（advanced，AI），（deep-learning，AI），等等。
- en: In addition to these pairs we extract from the data, we also sample “fake” pairs—that
    is, for a given input word (such as “AI”), we also sample random noise words as
    context (such as “monkeys”), in a process known as *negative sampling*. We use
    the true pairs combined with noise pairs to build our training instances and labels,
    which we use to train a binary classifier that learns to distinguish between them.
    The trainable parameters in this classifier are the vector representations—word
    embeddings. We tune these vectors to yield a classifier able to tell the difference
    between true contexts of a word and randomly sampled ones, in a binary classification
    setting.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了从数据中提取的这些对，我们还抽取“假”对——也就是，对于给定的输入词（如“AI”），我们还会抽取随机噪声词作为上下文（如“monkeys”），这个过程被称为*负采样*。我们使用真实对和噪声对结合起来构建我们的训练实例和标签，用于训练一个学习区分它们之间差异的二元分类器。这个分类器中的可训练参数是向量表示——词嵌入。我们调整这些向量以产生一个能够在二元分类设置中区分一个词的真实上下文和随机抽样的上下文的分类器。
- en: TensorFlow enables many ways to implement the word2vec model, with increasing
    levels of sophistication and optimization, using multithreading and higher-level
    abstractions for optimized and shorter code. We present here a fundamental approach,
    which will introduce you to the core ideas and operations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了许多实现 word2vec 模型的方式，随着复杂性和优化水平的增加，使用多线程和更高级的抽象来优化和缩短代码。我们在这里介绍了一种基本方法，它将向您介绍核心思想和操作。
- en: Let’s dive straight into implementing the core ideas in TensorFlow code.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接开始在 TensorFlow 代码中实现核心思想。
- en: Skip-Grams
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跳字
- en: 'We begin by preparing our data and extracting skip-grams. As in [Chapter 5](ch05.html#text_i),
    our data comprises two classes of very short “sentences,” one composed of odd
    digits and the other of even digits (with numbers written in English). We make
    sentences equally sized here, for simplicity, but this doesn’t really matter for
    word2vec training. Let’s start by setting some parameters and creating sentences:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先准备我们的数据并提取跳字。就像在[第5章](ch05.html#text_i)中一样，我们的数据包括两类非常短的“句子”，一类由奇数组成，另一类由偶数组成（用英文写的数字）。为了简单起见，我们在这里让句子大小相等，但这对于
    word2vec 训练并不重要。让我们开始设置一些参数并创建句子：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s take a look at our sentences:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们的句子：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, as in [Chapter 5](ch05.html#text_i), we map words to indices by creating
    a dictionary with words as keys and indices as values, and create the inverse
    map:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，就像在[第5章](ch05.html#text_i)中一样，我们通过创建一个以单词为键、索引为值的字典，将单词映射到索引，并创建反向映射：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To prepare the data for word2vec, let’s create skip-grams:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备 word2vec 的数据，让我们创建跳字模型：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Each skip-gram pair is composed of target and context word indices (given by
    the `word2index_map` dictionary, and not in correspondence to the actual digit
    each word represents). Let’s take a look:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每个skip-gram对由目标和上下文单词索引组成（由`word2index_map`字典给出，并不对应于每个单词表示的实际数字）。让我们来看一下：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can generate batches of sequences of word indices, and check out the original
    sentences with the inverse dictionary we created earlier:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以生成单词索引序列的批次，并使用我们之前创建的逆字典查看原始句子：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we create our input and label placeholders:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建我们的输入和标签占位符：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Embeddings in TensorFlow
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow中的嵌入
- en: 'In [Chapter 5](ch05.html#text_i), we used the built-in `tf.nn.embedding_lookup()`
    function as part of our supervised RNN. The same functionality is used here. Here
    too, word embeddings can be viewed as lookup tables that map words to vector values,
    which are optimized as part of the training process to minimize a loss function.
    As we shall see in the next section, unlike in [Chapter 5](ch05.html#text_i),
    here we use a loss function accounting for the unsupervised nature of the task,
    but the embedding lookup, which efficiently retrieves the vectors for each word
    in a given sequence of word indices, remains the same:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#text_i)中，我们在监督RNN的一部分中使用了内置的`tf.nn.embedding_lookup()`函数。这里也使用了相同的功能。在这里，单词嵌入可以被视为查找表，将单词映射到向量值，这些向量值在训练过程中被优化以最小化损失函数。正如我们将在下一节中看到的，与[第5章](ch05.html#text_i)不同的是，这里我们使用了一个考虑到任务非监督性质的损失函数，但是嵌入查找仍然是相同的，它有效地检索给定单词索引序列中每个单词的向量：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The Noise-Contrastive Estimation (NCE) Loss Function
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 噪声对比估计（NCE）损失函数
- en: 'In our introduction to skip-grams, we mentioned we create two types of context–target
    pairs of words: real ones that appear in the text, and “fake” noisy pairs that
    are generated by inserting random context words. Our goal is to learn to distinguish
    between the two, helping us learn a good word representation. We could draw random
    noisy context pairs ourselves, but luckily TensorFlow comes with a useful loss
    function designed especially for our task. `tf.nn.nce_loss()` automatically draws
    negative (“noise”) samples when we evaluate the loss (run it in a session):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍skip-grams时，我们提到我们创建了两种类型的上下文-目标单词对：在文本中出现的真实单词和通过插入随机上下文单词生成的“虚假”嘈杂对。我们的目标是学会区分这两者，帮助我们学习一个良好的单词表示。我们可以自己绘制随机嘈杂的上下文对，但幸运的是，TensorFlow带有一个专门为我们的任务设计的有用的损失函数。当我们评估损失时（在会话中运行），`tf.nn.nce_loss()`会自动绘制负（“噪声”）样本：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We don’t go into the mathematical details of this loss function, but it is sufficient
    to think of it as a sort of efficient approximation to the ordinary softmax function
    used in classification tasks, as introduced in previous chapters. We tune our
    embedding vectors to optimize this loss function. For more details about it, see
    the official TensorFlow [documentation](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss)
    and references within.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入讨论这个损失函数的数学细节，但可以将其视为一种对用于分类任务中的普通softmax函数的有效近似，正如在之前的章节中介绍的那样。我们调整我们的嵌入向量以优化这个损失函数。有关更多详细信息，请参阅官方TensorFlow[文档](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss)和其中的参考资料。
- en: 'We’re now ready to train. In addition to obtaining our word embeddings in TensorFlow,
    we next introduce two useful capabilities: adjustment of the optimization learning
    rate, and interactive visualization of embeddings.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好进行训练了。除了在TensorFlow中获取我们的单词嵌入之外，我们接下来介绍两个有用的功能：调整优化学习率和交互式可视化嵌入。
- en: Learning Rate Decay
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率衰减
- en: As discussed in previous chapters, gradient-descent optimization adjusts weights
    by making small steps in the direction that minimizes our loss function. The `learning_rate`
    hyperparameter controls just how aggressive these steps are. During gradient-descent
    training of a model, it is common practice to gradually make these steps smaller
    and smaller, so that we allow our optimization process to “settle down” as it
    approaches good points in the parameter space. This small addition to our training
    process can actually often lead to significant boosts in performance, and is a
    good practice to keep in mind in general.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在之前的章节中讨论的那样，梯度下降优化通过朝着最小化损失函数的方向进行小步调整权重。`learning_rate`超参数控制这些步骤的侵略性。在模型的梯度下降训练过程中，通常会逐渐减小这些步骤的大小，以便让我们的优化过程在接近参数空间中的良好点时“安定下来”。这个小改动实际上经常会显著提升性能，并且是一个一般性的良好实践。
- en: '`tf.train.exponential_decay()` applies exponential decay to the learning rate,
    with the exact form of decay controlled by a few hyperparameters, as seen in the
    following code (for exact details, see the official TensorFlow documentation at
    [http://bit.ly/2tluxP1](http://bit.ly/2tluxP1)). Here, just as an example, we
    decay every 1,000 steps, and the decayed learning rate follows a staircase function—a
    piecewise constant function that resembles a staircase, as its name implies:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.train.exponential_decay()`对学习率应用指数衰减，衰减的确切形式由一些超参数控制，如下面的代码所示（有关详细信息，请参阅官方TensorFlow文档[http://bit.ly/2tluxP1](http://bit.ly/2tluxP1)）。在这里，仅作为示例，我们每1,000步衰减一次，衰减的学习率遵循阶梯函数——一种类似于楼梯的分段常数函数，正如其名称所暗示的那样：'
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Training and Visualizing with TensorBoard
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorBoard进行训练和可视化
- en: We train our graph within a session as usual, adding some lines of code enabling
    cool interactive visualization in TensorBoard, a new tool for visualizing embeddings
    of high-dimensional data—typically images or word vectors—introduced for TensorFlow
    in late 2016.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像往常一样在会话中训练我们的图，添加了一些代码行，使TensorBoard中的交互式可视化更加酷炫，这是一种用于可视化高维数据（通常是图像或单词向量）的新工具，于2016年底引入TensorFlow。
- en: First, we create a TSV (tab-separated values) metadata file. This file connects
    embedding vectors with associated labels or images we may have for them. In our
    case, each embedding vector has a label that is just the word it stands for.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个TSV（制表符分隔值）元数据文件。该文件将嵌入向量与我们可能拥有的相关标签或图像连接起来。在我们的情况下，每个嵌入向量都有一个标签，就是它代表的单词。
- en: We then point TensorBoard to our embedding variables (in this case, only one),
    and link them to the metadata file.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将TensorBoard指向我们的嵌入变量（在这种情况下，只有一个），并将它们链接到元数据文件。
- en: 'Finally, after completing optimization but before closing the session, we normalize
    the word embedding vectors to unit length, a standard post-processing step:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在完成优化但在关闭会话之前，我们将词嵌入向量归一化为单位长度，这是一个标准的后处理步骤：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Checking Out Our Embeddings
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查我们的嵌入
- en: 'Let’s take a quick look at the word vectors we got. We select one word (*one*)
    and sort all the other word vectors by how close they are to it, in descending
    order:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下我们得到的词向量。我们选择一个单词（*one*）并按照它们与其接近程度的顺序对所有其他词向量进行排序，降序排列：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now let’s take a look at the word distances from the *one* vector:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看与*one*向量的词距离：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We see that the word vectors representing odd numbers are similar (in terms
    of the dot product) to *one*, while those representing even numbers are not similar
    to it (and have a negative dot product with the *one* vector). We learned embedded
    vectors that allow us to distinguish between even and odd numbers—their respective
    vectors are far apart, and thus capture the context in which each word (odd or
    even digit) appeared.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，代表奇数的词向量与*one*相似（在点积方面），而代表偶数的词向量与之不相似（并且与*one*向量的点积为负）。我们学习了嵌入向量，使我们能够区分偶数和奇数——它们各自的向量相距甚远，因此捕捉了每个单词（奇数或偶数数字）出现的上下文。
- en: Now, in TensorBoard, go to the Embeddings tab. This is a three-dimensional interactive
    visualization panel, where we can move around the space of our embedded vectors
    and explore different “angles,” zoom in, and more (see Figures [6-2](#fig0602)
    and [6-3](#fig0603)). This enables us to understand our data and interpret the
    model in a visually comfortable manner. We can see, for instance, that the odd
    and even numbers occupy different areas in feature space.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在TensorBoard中，转到嵌入选项卡。这是一个三维交互式可视化面板，我们可以在嵌入向量空间中移动并探索不同的“角度”，放大等（参见图[6-2](#fig0602)和[6-3](#fig0603)）。这使我们能够以视觉舒适的方式理解我们的数据并解释模型。我们可以看到，奇数和偶数在特征空间中占据不同的区域。
- en: '![](assets/letf_0602.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: ！[](assets/letf_0602.png)
- en: Figure 6-2\. Interactive visualization of word embeddings.
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2. 词嵌入的交互式可视化。
- en: '![](assets/letf_0603.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: ！[](assets/letf_0603.png)
- en: Figure 6-3\. We can explore our word vectors from different angles (especially
    useful in high-dimensional problems with large vocabularies).
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3. 我们可以从不同角度探索我们的词向量（在具有大词汇量的高维问题中特别有用）。
- en: Of course, this type of visualization really shines when we have a great number
    of embedded vectors, such as in real text classification tasks with larger vocabularies,
    as we will see in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications),
    for example, or in the Embedding Projector [TensorFlow demo](http://projector.tensorflow.org/). Here,
    we just give you a taste of how to interactively explore your data and deep learning
    models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有大量嵌入向量时，例如在具有更大词汇量的实际文本分类任务中，这种类型的可视化效果非常出色，例如在[第7章](ch07.html#tensorflow_abstractions_and_simplifications)中，或在嵌入投影仪[TensorFlow演示](http://projector.tensorflow.org/)中。在这里，我们只是让您尝试如何交互地探索您的数据和深度学习模型。
- en: Pretrained Embeddings, Advanced RNN
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练嵌入，高级RNN
- en: As we discussed earlier, word embeddings are a powerful component in deep learning
    models for text. A popular approach seen in many applications is to first train
    word vectors with methods such as word2vec on massive amounts of (unlabeled) text,
    and then use these vectors in a downstream task such as supervised document classification.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，词嵌入是深度学习模型中文本的强大组件。在许多应用中看到的一种流行方法是首先使用诸如word2vec之类的方法在大量（未标记的）文本上训练词向量，然后在监督文档分类等下游任务中使用这些向量。
- en: In the previous section, we trained unsupervised word vectors from scratch.
    This approach typically requires very large corpora, such as Wikipedia entries
    or web pages. In practice, we often use pretrained word embeddings, trained on
    such huge corpora and available online, in much the same manner as the pretrained
    models presented in previous chapters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们从头开始训练了无监督的词向量。这种方法通常需要非常大的语料库，例如维基百科条目或网页。在实践中，我们经常使用预训练的词嵌入，这些词嵌入在如前几章中介绍的预训练模型中以类似的方式训练，并且可在线获取。
- en: 'In this section, we show how to use pretrained word embeddings in TensorFlow
    in a simplified text-classification task. To make things more interesting, we
    also take this opportunity to introduce some more useful and powerful components
    that are frequently used in modern deep learning applications for natural language
    understanding: the bidirectional RNN layers and the gated recurrent unit (GRU)
    cell.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何在TensorFlow中使用预训练的词嵌入进行简化的文本分类任务。为了使事情更有趣，我们还借此机会介绍了一些在现代深度学习应用中经常使用的更有用和更强大的组件，用于自然语言理解：双向RNN层和门控循环单元（GRU）单元。
- en: We will expand and adapt our text-classification example from [Chapter 5](ch05.html#text_i),
    focusing only on the parts that have changed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将扩展和调整我们从[第5章](ch05.html#text_i)中的文本分类示例，只关注已更改的部分。
- en: Pretrained Word Embeddings
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练词嵌入
- en: Here, we show how to take word vectors trained based on web data and incorporate
    them into a (contrived) text-classification task. The embedding method is known
    as *GloVe*, and while we don’t go into the details here, the overall idea is similar
    to that of word2vec—learning representations of words by the context in which
    they appear. Information on the method and its authors, and the pretrained vectors,
    is available on the project’s [website](http://nlp.stanford.edu/projects/glove/).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了如何将基于网络数据训练的词向量并入（虚构的）文本分类任务中。嵌入方法被称为*GloVe*，虽然我们在这里不深入讨论细节，但总体思想与word2vec类似——通过单词出现的上下文学习单词的表示。关于该方法及其作者以及预训练向量的信息可在项目的[网站](http://nlp.stanford.edu/projects/glove/)上找到。
- en: We download the Common Crawl vectors (840B tokens), and proceed to our example.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下载了Common Crawl向量（840B个标记），然后进行我们的示例。
- en: 'We first set the path to the downloaded word vectors and some other parameters,
    as in [Chapter 5](ch05.html#text_i):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先设置下载的单词向量的路径和一些其他参数，就像[第5章](ch05.html#text_i)中所示的那样：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We then create the contrived, simple simulated data, also as in [Chapter 5](ch05.html#text_i)
    (see details there):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建人为的、简单的模拟数据，也如同[第5章](ch05.html#text_i)中所示（详细信息请参见那里）：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we create the word index map:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建单词索引映射：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s refresh our memory of its content—just a map from word to an (arbitrary)
    index:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下它的内容——只是一个从单词到（任意）索引的映射：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we are ready to get word vectors. There are 2.2 million words in the vocabulary
    of the pretrained GloVe embeddings we downloaded, and in our toy example we have
    only 9\. So, we take the GloVe vectors only for words that appear in our own tiny
    vocabulary:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备获取单词向量。我们下载的预训练GloVe嵌入中有220万个单词，而在我们的玩具示例中只有9个。因此，我们只取出出现在我们自己微小词汇表中的单词的GloVe向量：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We go over the GloVe file line by line, take the word vectors we need, and normalize
    them. Once we have extracted the nine words we need, we stop the process and exit
    the loop. The output of our function is a dictionary, mapping from each word to
    its vector.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们逐行查看GloVe文件，获取我们需要的单词向量，并对它们进行归一化。一旦我们提取出需要的九个单词，我们就停止这个过程并退出循环。我们函数的输出是一个字典，将每个单词映射到它的向量。
- en: 'The next step is to place these vectors in a matrix, which is the required
    format for TensorFlow. In this matrix, each row index should correspond to the
    word index:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将这些向量放入矩阵中，这是TensorFlow所需的格式。在这个矩阵中，每行索引应该对应单词索引：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that for the `PAD_TOKEN` word, we set the corresponding vector to 0\. As
    we saw in [Chapter 5](ch05.html#text_i), we ignore padded tokens in our call to
    `dynamic_rnn()` by telling it the original sequence length.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于`PAD_TOKEN`单词，我们将相应的向量设置为0。正如我们在[第5章](ch05.html#text_i)中看到的，我们在调用`dynamic_rnn()`时会忽略填充的标记，告诉它原始序列长度。
- en: 'We now create our training and test data:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建我们的训练和测试数据：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And we create our input placeholders:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建我们的输入占位符：
- en: '[PRE20]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that we created an `embedding_placeholder`, to which we feed the word
    vectors:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们创建了一个`embedding_placeholder`，我们向其中提供单词向量：
- en: '[PRE21]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our embeddings are initialized with the content of `embedding_placeholder`,
    using the `assign()` function to assign initial values to the `embeddings` variable.
    We set `trainable=True` to tell TensorFlow we want to update the values of the
    word vectors, by optimizing them for the task at hand. However, it is often useful
    to set `trainable=False` and not update these values; for example, when we do
    not have much labeled data or have reason to believe the word vectors are already
    “good” at capturing the patterns we are after.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的嵌入是用`embedding_placeholder`的内容初始化的，使用`assign()`函数将初始值分配给`embeddings`变量。我们设置`trainable=True`告诉TensorFlow我们希望更新单词向量的值，通过优化它们以适应当前任务。然而，通常有用的是将`trainable=False`，不更新这些值；例如，当我们没有太多标记数据或有理由相信单词向量已经“很好”地捕捉到我们想要的模式时。
- en: There is one more step missing to fully incorporate the word vectors into the
    training—feeding `embedding_placeholder` with `embedding_matrix`. We will get
    to that soon, but for now we continue the graph building and introduce bidirectional
    RNN layers and GRU cells.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个步骤缺失，以完全将单词向量纳入训练中——用`embedding_matrix`喂`embedding_placeholder`。我们很快就会做到这一点，但现在我们继续构建图并引入双向RNN层和GRU单元。
- en: Bidirectional RNN and GRU Cells
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双向RNN和GRU单元
- en: 'Bidirectional RNN layers are a simple extension of the RNN layers we saw in
    [Chapter 5](ch05.html#text_i). All they consist of, in their basic form, is two
    ordinary RNN layers: one layer that reads the sequence from left to right, and
    another that reads from right to left. Each yields a hidden representation, the
    left-to-right vector <math><mover accent="true"><mi>h</mi> <mo>→</mo></mover></math>
    , and the right-to-left vector  <math alttext="ModifyingAbove h With left-arrow"><mover
    accent="true"><mi>h</mi> <mo>←</mo></mover></math> . These are then concatenated
    into one vector. The major advantage of this representation is its ability to
    capture the context of words from both directions, which enables richer understanding
    of natural language and the underlying semantics in text. In practice, in complex
    tasks, it often leads to improved accuracy. For example, in part-of-speech (POS)
    tagging, we want to output a predicted tag for each word in a sentence (such as
    “noun,” “adjective,” etc.). In order to predict a POS tag for a given word, it
    is useful to have information on its surrounding words, from both directions.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 双向RNN层是我们在[第5章](ch05.html#text_i)中看到的RNN层的一个简单扩展。它们的基本形式只包括两个普通的RNN层：一个从左到右读取序列的层，另一个从右到左读取。每个都产生一个隐藏表示，左到右向量
    <math><mover accent="true"><mi>h</mi> <mo>→</mo></mover></math> ，和右到左向量 <math
    alttext="ModifyingAbove h With left-arrow"><mover accent="true"><mi>h</mi> <mo>←</mo></mover></math>
    。然后将它们连接成一个向量。这种表示的主要优势在于它能够捕捉单词的上下文，从两个方向，这使得对自然语言和文本中的基础语义有更丰富的理解。在实践中，在复杂任务中，它通常会导致更高的准确性。例如，在词性标注中，我们希望为句子中的每个单词输出一个预测的标签（如“名词”，“形容词”等）。为了预测给定单词的词性标签，有必要获取其周围单词的信息，从两个方向。
- en: Gated recurrent unit (GRU) cells are a simplification of sorts of LSTM cells.
    They also have a memory mechanism, but with considerably fewer parameters than
    LSTM. They are often used when there is less available data, and are faster to
    compute. We do not go into the mathematical details here, as they are not important
    for our purposes; there are many good online resources explaining GRU and how
    it is different from LSTM.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 门控循环单元（GRU）单元是LSTM单元的一种简化。它们也有记忆机制，但参数比LSTM少得多。当可用数据较少时，它们经常被使用，并且计算速度更快。我们在这里不详细介绍数学细节，因为对于我们的目的来说并不重要；有许多在线资源解释GRU以及它与LSTM的区别。
- en: 'TensorFlow comes equipped with `tf.nn.bidirectional_dynamic_rnn()`, which is
    an extension of `dynamic_rnn()` for bidirectional layers. It takes `cell_fw` and
    `cell_bw` RNN cells, which are the left-to-right and right-to-left vectors, respectively.
    Here we use `GRUCell()` for our forward and backward representations and add dropout
    for regularization, using the built-in `DropoutWrapper()`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow配备了`tf.nn.bidirectional_dynamic_rnn()`，这是`dynamic_rnn()`的扩展，用于双向层。它接受`cell_fw`和`cell_bw`
    RNN单元，分别是从左到右和从右到左的向量。在这里，我们使用`GRUCell()`作为我们的前向和后向表示，并添加了用于正则化的dropout，使用内置的`DropoutWrapper()`：
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We concatenate the forward and backward state vectors by using `tf.concat()`
    along the suitable axis, and then add a linear layer followed by softmax as in
    [Chapter 5](ch05.html#text_i):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过沿着适当的轴使用`tf.concat()`来连接前向和后向状态向量，然后添加一个线性层，后跟softmax，就像[第5章](ch05.html#text_i)中一样：
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We are now ready to train. We initialize the `embedding_placeholder` by feeding
    it our `embedding_matrix`. It’s important to note that we do so after calling
    `tf.global_variables_initializer()`—doing this in the reverse order would overrun
    the pre-trained vectors with a default initializer:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备开始训练。我们通过将我们的`embedding_matrix`传递给`embedding_placeholder`来初始化它。重要的是要注意，我们在调用`tf.global_variables_initializer()`之后这样做——以相反的顺序进行会用默认初始化器覆盖预训练向量：
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Summary
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we extended our knowledge regarding working with text sequences,
    adding some important tools to our TensorFlow toolbox. We saw a basic implementation
    of word2vec, learning the core concepts and ideas, and used TensorBoard for 3D
    interactive visualization of embeddings. We then incorporated publicly available
    GloVe word vectors, and RNN components that allow for richer and more efficient
    models. In the next chapter, we will see how to use abstraction libraries, including
    for classification tasks on real text data with LSTM networks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们扩展了关于处理文本序列的知识，为我们的TensorFlow工具箱添加了一些重要工具。我们看到了word2vec的基本实现，学习了核心概念和思想，并使用TensorBoard对嵌入进行了3D交互式可视化。然后我们加入了公开可用的GloVe词向量，以及允许更丰富和更高效模型的RNN组件。在下一章中，我们将看到如何使用抽象库，包括在真实文本数据上使用LSTM网络进行分类任务。
