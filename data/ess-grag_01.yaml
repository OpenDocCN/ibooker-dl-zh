- en: 2 Vector similarity search and hybrid search
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 向量相似度搜索和混合搜索
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introduction to embeddings, embedding models, vector space, and vector similarity
    search
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入、嵌入模型、向量空间和向量相似度搜索简介
- en: How vector similarity fits in RAG applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量相似度在RAG应用中的位置
- en: A practical walkthrough of a RAG application using vector similarity search
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用向量相似度搜索的RAG应用的实用操作指南
- en: Adding full-text search to the RAG application to see how enabling a hybrid
    search approach can improve results
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向RAG应用中添加全文搜索以查看启用混合搜索方法如何提高结果
- en: Creating a knowledge graph can be an iterative process where you start with
    unstructured data and then add structure to it. This is often the case when you
    have a lot of unstructured data and you want to start using it to answer questions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 创建知识图谱可能是一个迭代过程，你从非结构化数据开始，然后向其添加结构。当你拥有大量非结构化数据并希望开始使用它来回答问题时，这通常是这种情况。
- en: This chapter will look at how we can use RAG to answer questions using unstructured
    data. We’ll look at how to use vector similarity search and hybrid search to find
    relevant information and how to use that information to generate an answer. In
    later chapters, we’ll look at what techniques we can use to improve the retriever
    and generator to get better results when there’s some structure to the data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨如何使用RAG通过非结构化数据来回答问题。我们将探讨如何使用向量相似度搜索和混合搜索来找到相关信息，以及如何使用这些信息来生成答案。在后续章节中，我们将探讨我们可以使用哪些技术来改进检索器和生成器，以便在数据具有一些结构时获得更好的结果。
- en: In data science and machine learning, embedding models and vector similarity
    search are important tools for handling complex data. This chapter looks at how
    these technologies turn complicated data, like text and images, into uniform formats
    called embeddings.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学和机器学习中，嵌入模型和向量相似度搜索是处理复杂数据的重要工具。本章将探讨这些技术如何将复杂的文本和图像等数据转换为称为嵌入的统一格式。
- en: 'In this chapter, we will cover the basics of embedding models and vector similarity
    search, explaining why they are useful, how they are used, and the challenges
    they help solve in RAG applications. To follow along, you’ll need access to a
    running, blank Neo4j instance. This can be a local installation or a cloud-hosted
    instance; just make sure it’s empty. You can follow the implementation directly
    in the accompanying Jupyter notebook available here: [https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch02.ipynb](https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch02.ipynb).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍嵌入模型和向量相似度搜索的基础知识，解释为什么它们是有用的，如何使用它们，以及它们在RAG应用中帮助解决的挑战。为了跟上进度，你需要访问一个正在运行的、空白的Neo4j实例。这可以是一个本地安装或云托管实例；只需确保它是空的。你可以直接在附带的Jupyter笔记本中跟随实现，笔记本地址为：[https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch02.ipynb](https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch02.ipynb)。
- en: 2.1 Components of a RAG architecture
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 RAG架构的组件
- en: 'In a RAG application, there are two main components: a *retriever* and a *generator*.
    The retriever finds relevant information, and the generator uses that information
    to create a response. Vector similarity search is used in the retriever to find
    relevant information; this is explained in more detail later. Let’s dig into both
    these components.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG应用中，有两个主要组件：一个*检索器*和一个*生成器*。检索器找到相关信息，生成器使用这些信息来创建响应。向量相似度搜索在检索器中用于找到相关信息；这将在稍后进行更详细的解释。让我们深入了解这两个组件。
- en: 2.1.1 The retriever
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 检索器
- en: The retriever is the first component of a RAG application. Its purpose is to
    find relevant information and pass that information to the generator. How the
    retriever finds the relevant information is not implied in the RAG framework,
    but the most common way is to use vector similarity search. Let’s look at what’s
    needed to prepare data for the retriever to be successful using vector similarity
    search.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器是RAG应用的第一个组件。它的目的是找到相关信息并将这些信息传递给生成器。检索器如何找到相关信息并未在RAG框架中暗示，但最常见的方式是使用向量相似度搜索。让我们看看为检索器准备数据以成功使用向量相似度搜索需要哪些内容。
- en: Vector index
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量索引
- en: While a vector index isn’t strictly required for vector similarity search, it’s
    highly recommended. A vector index is a data structure (like a map) that stores
    vectors in a way that makes it easy to search for similar vectors. When using
    a vector index, the retriever method is often referred to as an *approximate nearest
    neighbor search*. This is because the vector index doesn’t find the exact nearest
    neighbors, but it finds vectors that are very close to the nearest neighbor. This
    is a tradeoff between speed and accuracy. The vector index is much faster than
    a brute-force search, but it’s not as accurate.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然向量索引对于向量相似度搜索不是严格必需的，但它被高度推荐。向量索引是一种数据结构（如映射），以使其易于搜索相似向量的方式存储向量。当使用向量索引时，检索方法通常被称为*近似最近邻搜索*。这是因为向量索引不找到确切的最近邻，而是找到非常接近最近邻的向量。这是速度和精度之间的权衡。向量索引比暴力搜索快得多，但精度不如暴力搜索。
- en: Vector similarity search function
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量相似度搜索功能
- en: A *vector similarity search* function is a function that takes a vector as input
    and returns a list of similar vectors. This function might use a vector index
    to find similar vectors, or it might use some other (brute-force) method. The
    important thing is that it returns a list of similar vectors.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*向量相似度搜索*功能是一个函数，它接受一个向量作为输入，并返回一个相似向量的列表。这个函数可能使用向量索引来找到相似向量，也可能使用其他（暴力）方法。重要的是它返回一个相似向量的列表。'
- en: The two most common vector similarity search functions are cosine similarity
    and Euclidean distance. *Euclidean distance* represents the content and intensity
    of the text, which is not as important in most cases covered in this book. *Cosine
    similarity* is a measure of the angle between two vectors. In our text-embedding
    case, this angle represents how similar two texts are in their meaning. The cosine
    similarity function takes two vectors as input and returns a number between 0
    and 1; 0 means the vectors are completely different, and 1 means they are identical.
    Cosine similarity is considered the best fit for text chatbots, and it’s the one
    we’ll use in this book.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 两种最常见的向量相似度搜索功能是余弦相似度和欧几里得距离。*欧几里得距离*代表文本的内容和强度，在本书的大部分情况下并不重要。*余弦相似度*是两个向量之间角度的度量。在我们的文本嵌入案例中，这个角度代表了两篇文本在意义上相似的程度。余弦相似度函数接受两个向量作为输入，并返回一个介于0和1之间的数字；0表示向量完全不同，1表示它们完全相同。余弦相似度被认为是文本聊天机器人的最佳匹配，也是本书中我们将使用的方法。
- en: Embedding model
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌入模型
- en: The result from a semantic classification of text is called an *embedding*.
    Any text you want to match using vector similarity search must be converted into
    an embedding. This is done using an embedding model, and it’s important that the
    embedding model stays the same throughout the RAG application. If you want to
    change the embedding model, you must repopulate the vector index.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 文本语义分类的结果被称为*嵌入*。任何你想通过向量相似度搜索匹配的文本都必须转换为嵌入。这是通过嵌入模型完成的，并且在整个RAG应用中保持嵌入模型不变非常重要。如果你想更改嵌入模型，你必须重新填充向量索引。
- en: Embeddings are lists of numbers, and the length of the list is called the embedding
    dimension. The embedding dimension is important because it determines how much
    information the embedding can hold. The higher the embedding dimension, the more
    computationally expensive it is to work with the embedding, both when generating
    the embedding as well as when performing vector similarity search.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是数字列表，列表的长度称为嵌入维度。嵌入维度很重要，因为它决定了嵌入可以包含多少信息。嵌入维度越高，处理嵌入的计算成本就越高，无论是生成嵌入还是执行向量相似度搜索。
- en: An *embedding* is a way to represent complex data as a set of numbers in a simpler,
    lower-dimensional space. Think of it as translating data into a format that a
    computer can easily understand and work with.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌入*是将复杂数据表示为更低维空间中一组数字的方法。把它想象成将数据转换成计算机可以轻松理解和处理的形式。'
- en: '*Embedding models* provide a uniform way to represent different types of data.
    Input to an embedding model can be any complex data, and the output is a vector.
    For instance, in dealing with text, an embedding model will take words or sentences
    and turn them into vectors, which are lists of numbers. The model is trained to
    ensure that these number lists capture essential aspects of the original words,
    such as their meaning or context.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌入模型*提供了一种统一的方式来表示不同类型的数据。嵌入模型的输入可以是任何复杂的数据，输出是一个向量。例如，在处理文本时，嵌入模型将单词或句子转换为向量，这些向量是数字列表。该模型经过训练以确保这些数字列表捕捉到原始单词的基本方面，如它们的含义或上下文。'
- en: Text chunking
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本分块
- en: '*Text chunking* is the process of splitting up text into smaller pieces. This
    is done to improve the accuracy of the retriever. The presence of smaller pieces
    of text means that the embedding is narrower and more specific; thus the retriever
    will find more relevant information when searching.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*文本分块*是将文本拆分成更小片段的过程。这样做是为了提高检索器的准确性。存在更小的文本片段意味着嵌入更窄且更具体；因此，检索器在搜索时将找到更多相关信息。'
- en: 'Text chunking is very important and not easy to get right. You need to think
    about how to split up the text: Should it be sentences, paragraphs, semantic meaning,
    or something else? Should you use a sliding window, or should you use a fixed
    size? How big should the chunks be?'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分块非常重要，而且不容易做对。你需要考虑如何拆分文本：应该是句子、段落、语义意义，还是其他什么？你应该使用滑动窗口，还是使用固定大小？块的大小应该是多少？
- en: There are no right answers to these questions, and it depends on the use case,
    data, and domain. But it’s important to think about these questions and try different
    approaches to find the best solution for your use case.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些问题，没有正确答案，这取决于用例、数据和领域。但思考这些问题并尝试不同的方法来找到最适合你用例的最佳解决方案是很重要的。
- en: Retriever pipeline
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检索器管道
- en: Once all pieces are in place, the retriever pipeline is quite simple. It takes
    a query as input, converts it into an embedding using the embedding model, and
    then uses the vector similarity search function to find similar embeddings. In
    the naive case, the retriever pipeline then just returns the source chunks, which
    then are passed to the generator. But in most cases, the retriever pipeline needs
    to do some postprocessing to find the best chunks to pass to the generator. We’ll
    get to more advanced strategies in the next chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有部件都到位，检索器管道相当简单。它将查询作为输入，使用嵌入模型将其转换为嵌入，然后使用向量相似度搜索功能来找到相似的嵌入。在简单情况下，检索器管道随后只返回源块，然后这些块被传递给生成器。但在大多数情况下，检索器管道需要进行一些后处理以找到传递给生成器的最佳块。我们将在下一章中介绍更高级的策略。
- en: 2.1.2 The generator
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 生成器
- en: The *generator* is the second component of a RAG application. It uses the information
    found by the retriever to generate a response. The generator is often an LLM,
    but one benefit of RAG over finetuning or relying on a model’s base knowledge
    is that the models don’t need to be as large. This is because the retriever finds
    relevant information, so the generator doesn’t need to know everything. It does
    need to know how to use the information found by the retriever to create a response.
    This is a much smaller task than knowing everything.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成器*是RAG应用中的第二个组件。它使用检索器找到的信息来生成响应。生成器通常是一个LLM，但RAG相对于微调或依赖模型的基础知识的一个好处是模型不需要那么大。这是因为检索器找到了相关信息，所以生成器不需要知道一切。它确实需要知道如何使用检索器找到的信息来创建响应。这是一个比知道一切小得多的任务。'
- en: So we’re using the language model for its ability to generate text, not for
    its knowledge. This means we can use smaller language models, which are faster
    and cheaper to run. It also means that we can trust that the language model will
    base its response on the information found by the retriever and therefore make
    fewer things up and hallucinate less.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用语言模型的能力来生成文本，而不是它的知识。这意味着我们可以使用更小的语言模型，它们运行更快且成本更低。这也意味着我们可以相信语言模型将基于检索器找到的信息来生成响应，因此会少创造一些东西，幻觉也更少。
- en: 2.2 RAG using vector similarity search
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 使用向量相似度搜索的RAG
- en: There are a few pieces needed to implement a RAG application using vector similarity
    search. We’ll go through each of them in this chapter. The goal is to show how
    to implement a RAG application using vector similarity search and how to use the
    information found by the retriever to generate a response. Figure 2.1 illustrates
    the data flow for the finished RAG application.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 实现使用向量相似度搜索的RAG应用程序需要一些组件。我们将在本章中逐一介绍它们。目标是展示如何使用向量相似度搜索实现RAG应用程序，以及如何使用检索器找到的信息来生成响应。图2.1说明了完成RAG应用程序的数据流。
- en: '![figure](../Images/2-1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-1.png)'
- en: Figure 2.1 The data flow for this RAG application using vector similarity search
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 使用向量相似度搜索的此RAG应用程序的数据流
- en: 'We need to separate the application into two stages:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将应用程序分为两个阶段：
- en: Data setup
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据设置
- en: Query time
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询时间
- en: We’ll start by looking at the data setup, and then we’ll look at what the application
    will do at query time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先查看数据设置，然后我们将查看在查询时间应用程序将执行的操作。
- en: 2.2.1 Application data setup
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 应用程序数据设置
- en: From earlier sections, we know that we need to process the data a bit to be
    able to place it in the embedding model vector space to perform vector similarity
    search at run time. The pieces needed are
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的章节中，我们知道我们需要对数据进行一些处理，以便能够在运行时将其放置在嵌入模型向量空间中执行向量相似度搜索。所需的部分包括
- en: A text corpus
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本语料库
- en: Text-chunking function
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分块函数
- en: Embedding model
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入模型
- en: Database with vector similarity search ability
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有向量相似度搜索能力的数据库
- en: We will go through these pieces one by one and show how they contribute to the
    application data setup.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐一介绍这些组件，并展示它们如何有助于应用程序数据设置。
- en: The data will be stored in text chunks in a database, and the vector index will
    be populated with the embeddings of the text chunks. Later, at run time, when
    a user asks a question, the question will be embedded using the same embedding
    model as the text chunks, and then the vector index will be used to find similar
    text chunks. Figure 2.2 shows the data flow for the application data setup.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据将存储在数据库中的文本块中，向量索引将填充文本块的嵌入。稍后，在运行时，当用户提出一个问题，该问题将使用与文本块相同的嵌入模型进行嵌入，然后使用向量索引来找到相似文本块。图2.2显示了应用程序数据设置的数据流。
- en: '![figure](../Images/2-2.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-2.png)'
- en: Figure 2.2 The pieces in the pipeline for the application data setup
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 应用程序数据设置管道中的组件
- en: 2.2.2 The text corpus
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 文本语料库
- en: The text we will be using in this example is a paper titled “Einstein’s Patents
    and Inventions” (Caudhuri, 2017). Even though LLMs are well aware of Albert Einstein,
    we show that RAG works by asking very specific questions and comparing them with
    the answers we get from the paper versus answers we get from an LLM.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中我们将使用的文本是一篇题为“爱因斯坦的专利和发明”（Caudhuri，2017）的论文。尽管LLM对阿尔伯特·爱因斯坦非常了解，但我们通过提出非常具体的问题，并将它们与我们从论文中得到的答案与从LLM得到的答案进行比较，来展示RAG是如何工作的。
- en: 2.2.3 Text chunking
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 文本分块
- en: With an LLM having a large enough context window, we can use the whole paper
    as a single chunk. But to get better results, we’ll split the paper into smaller
    chunks and use every few hundred characters as a chunk. The chunk size that yields
    the best results varies on a case-by-case basis, so make sure to experiment with
    different chunk sizes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个LLM拥有足够大的上下文窗口，我们可以将整篇论文作为一个单独的块。但为了获得更好的结果，我们将论文分成更小的块，并使用每几百个字符作为一个块。最佳的块大小因情况而异，所以请确保对不同块大小进行实验。
- en: In this case, we also want to have some overlap between the chunks. This is
    because we want to be able to find answers that span multiple chunks. So we’ll
    use a sliding window with a size of 500 characters and an overlap of 40 characters.
    This will make the index a bit bigger, but it will also make the retriever more
    accurate.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们还想在块之间有一些重叠。这是因为我们希望能够找到跨越多个块的答案。所以我们将使用一个大小为500个字符、重叠为40个字符的滑动窗口。这将使索引稍微大一些，但也会使检索器更准确。
- en: To help the embedding model better classify the semantics of each chunk, we
    will only chunk at spaces, so we don’t have broken words at the start and end
    of each chunk. This function takes a text, chunk size (number of characters),
    overlap (number of characters), and an optional argument whether to split on any
    character or on whitespaces only and returns a list of chunks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助嵌入模型更好地分类每个块的语义，我们将在空格处进行分块，这样每个块的开始和结束处就不会有断词。这个函数接受一个文本、块大小（字符数）、重叠（字符数），以及一个可选参数，是否在任意字符或仅空白处分割，并返回一个块列表。
- en: Listing 2.1 The text-chunking function
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.1 文本分割函数
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Defines the function to chunk text'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义分割文本的功能'
- en: '#2 Calls the function and get chunks back'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 调用函数并获取块'
- en: '#3 Prints the length of the chunks list. The majority of the function is just
    to make sure that we don’t split individual words but only split on spaces.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 打印块列表的长度。函数的大部分只是为了确保我们不分割单个单词，而只是在空格上分割。'
- en: 2.2.4 Embedding model
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 嵌入模型
- en: When choosing an embedding model, it’s important to think about what kind of
    data you want to match. In this case, we want to match text, so we’ll use a text-embedding
    model. Throughout this book, we will use both embedding models and LLMs from OpenAI,
    but there are many alternatives out there. `all-MiniLM-L12-v2` via Sentence Transformers
    ([https://mng.bz/nZZ2](https://mng.bz/nZZ2)) from Hugging Face is a great alternative
    to OpenAI’s embedding models, and it’s very easy to use and can run on your local
    CPU.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择嵌入模型时，重要的是要考虑你想要匹配哪种类型的数据。在这种情况下，我们想要匹配文本，所以我们将使用文本嵌入模型。在这本书的整个过程中，我们将使用来自OpenAI的嵌入模型和LLMs，但外面有许多替代方案。来自Hugging
    Face的`all-MiniLM-L12-v2`通过Sentence Transformers（[https://mng.bz/nZZ2](https://mng.bz/nZZ2)）是OpenAI嵌入模型的一个很好的替代品，它非常易于使用，并且可以在你的本地CPU上运行。
- en: Once we have decided on a embedding model, we need to make sure that we use
    the same model throughout the RAG application. This is because the vector index
    is populated with vectors from the embedding model, so if we change the embedding
    model, we need to repopulate the vector index. To embed the chunks using OpenAI’s
    embedding models, we’ll use the following code.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们决定了一个嵌入模型，我们需要确保在整个RAG应用程序中使用相同的模型。这是因为向量索引是由嵌入模型中的向量填充的，所以如果我们更改嵌入模型，我们需要重新填充向量索引。要使用OpenAI的嵌入模型嵌入块，我们将使用以下代码。
- en: Listing 2.2 Embedding chunks
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.2 嵌入块
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Defines the function to embed chunks'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义嵌入块的功能'
- en: '#2 Calls the function and get embeddings back'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 调用函数并获取嵌入向量'
- en: '#3 Prints the length of the embeddings list'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 打印嵌入向量列表的长度'
- en: '#4 Prints the length of the first embedding'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 打印第一个嵌入向量的长度'
- en: 2.2.5 Database with vector similarity search function
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.5 带有向量相似度搜索功能的数据库
- en: Now that we have the embeddings, we need to store them so we can perform a similarity
    search later. In this book, we will use Neo4j as our database, since it has a
    built-in vector index and it’s easy to use; later in the book we will use Neo4j
    for its graph capabilities.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了嵌入向量，我们需要将它们存储起来，以便稍后进行相似度搜索。在这本书中，我们将使用Neo4j作为我们的数据库，因为它具有内置的向量索引，并且易于使用；在本书的后面部分，我们将使用Neo4j的图功能。
- en: 'The data model we’ll use at this stage is quite simple. We’ll have a single
    node type `Chunk` with two properties: `text` and `embedding`. The `text` property
    will hold the text of the chunk, and the `embedding` property will hold the embedding
    of the chunk.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段我们将使用的模型相当简单。我们将有一个单一的节点类型`Chunk`，具有两个属性：`text`和`embedding`。`text`属性将保存块中的文本，而`embedding`属性将保存块的嵌入。
- en: '![figure](../Images/2-3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-3.png)'
- en: Figure 2.3 The data model
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3 数据模型
- en: Figure 2.3 shows the simplistic data model that will be used to demonstrate
    how to implement a RAG application using vector similarity search.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3显示了将用于演示如何使用向量相似度搜索实现RAG应用程序的简单数据模型。
- en: First, let’s create a vector index. One thing to keep in mind is that when we
    create the vector index, we need to define the number of dimensions the vectors
    will have. If you at any point in the future change the embedding model that outputs
    a different number of dimensions, you need to recreate the vector index.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个向量索引。需要记住的一点是，当我们创建向量索引时，我们需要定义向量将具有的维度数。如果你在未来任何时候更改输出不同维度数的嵌入模型，你需要重新创建向量索引。
- en: As we saw in the code listing 2.2, the embedding model we used outputs vectors
    with 1,536 dimensions, so we’ll use that as the number of dimensions when we create
    the vector index.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在代码列表2.2中看到的，我们使用的嵌入模型输出的是1,536维度的向量，因此当我们创建向量索引时，我们将使用这个维度数。
- en: Listing 2.3 Creating a vector index in Neo4j
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.3 在Neo4j中创建向量索引
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will name the vector index `pdf` and it will be used to index nodes of type
    `Chunk` on the property `embedding` using the cosine similarity search function.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将命名向量索引为`pdf`，它将用于在`embedding`属性上索引类型为`Chunk`的节点，使用余弦相似度搜索功能。
- en: Now that we have a vector index, we can populate it with the embeddings. We
    will do this using Cypher, where we first create a node for each chunk and then
    set the `text` and `embedding` properties on the node using a Cypher loop. We’re
    also storing an index on each `:Chunk` node, so we can easily find the chunk later.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了向量索引，我们可以用嵌入来填充它。我们将使用Cypher来完成这项工作，首先为每个块创建一个节点，然后在节点上设置`text`和`embedding`属性，使用Cypher循环。我们还在每个`:Chunk`节点上存储一个索引，这样我们就可以轻松地稍后找到块。
- en: Listing 2.4 Storing chunks and populating the vector index in Neo4j
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.4 在Neo4j中存储块和填充向量索引
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To check what’s in the database, we can run this Cypher query to get the `:Chunk`
    node with index 0\.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查数据库中有什么，我们可以运行这个Cypher查询来获取索引为0的`:Chunk`节点。
- en: Listing 2.5 Getting data from a chunk node in Neo4j
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.5 从Neo4j中的块节点获取数据
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 2.2.6 Performing vector search
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.6 执行向量搜索
- en: Now that we have the vector index populated with the embeddings, we can perform
    a vector similarity search. First, we need to embed the question that we want
    to answer. We’ll use the same embedding model as we used for the chunks, and we’ll
    use the same function as we used to embed the chunks.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将向量索引填充了嵌入，我们可以执行向量相似度搜索。首先，我们需要嵌入我们想要回答的问题。我们将使用与块相同的嵌入模型，并且我们将使用与嵌入块相同的函数。
- en: Listing 2.6 Embedding user question
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.6 嵌入用户问题
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have the question embedded, we can perform a vector similarity search
    using Cypher.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将问题嵌入，我们可以使用Cypher执行向量相似度搜索。
- en: Listing 2.7 Performing vector search in Neo4j
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.7 在Neo4j中执行向量搜索
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The query returns the top two most similar chunks, and we can print the results
    to see what we got back. The code will print the following text chunks and their
    similarity scores.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 查询返回最相似的两个块，我们可以打印结果以查看我们得到了什么。代码将打印以下文本块及其相似度得分。
- en: Listing 2.8 Printing results
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.8 打印结果
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From the print, we can see the matched chunks, their similarity score, and their
    index. The next step is to use the chunks to generate an answer using an LLM.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从打印结果中，我们可以看到匹配的块、它们的相似度得分和它们的索引。下一步是使用这些块通过LLM生成一个答案。
- en: 2.2.7 Generating an answer using an LLM
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.7 使用LLM生成答案
- en: When communicating with an LLM, we have the ability to pass in what’s called
    a “system message,” where we can pass in instructions for the LLM to follow. We
    also pass in a “user message,” which holds the original question and, in our case,
    the answer to the question.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当与LLM通信时，我们有传递所谓的“系统消息”的能力，其中我们可以传递给LLM的指令。我们还传递一个“用户消息”，它包含原始问题，在我们的情况下，是问题的答案。
- en: In the user message, we pass in the chunks that we want the LLM to use to generate
    the answer. We do this by passing in the `text` property of the similar chunks
    we found in the similar search in listing 2.8\.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户消息中，我们传递给LLM想要使用的块，我们通过传递在列表2.8中相似搜索中找到的相似块的`text`属性来完成。
- en: Listing 2.9 The LLM context
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.9 LLM上下文
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s now use the LLM to generate an answer.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在使用LLM来生成一个答案。
- en: Listing 2.10 Generating an answer using an LLM
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.10 使用LLM生成答案
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This will stream the result from the LLM as it’s generated, and we can see the
    result as it’s generated.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这将流式传输LLM生成的结果，我们可以看到生成时的结果。
- en: Listing 2.11 Answer from LLM
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.11 LLM的答案
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Wow, look at that! The LLM was able to generate an answer based on the information
    found by the retriever.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，看看这个！LLM能够根据检索器找到的信息生成一个答案。
- en: 2.3 Adding full-text search to the RAG application to enable hybrid search
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 向RAG应用添加全文搜索以实现混合搜索
- en: In the previous section, we saw how to implement a RAG application using vector
    similarity search. While pure vector similarity search can take you a long way
    and is a great improvement over plain full-text search, it’s often not enough
    to produce high enough quality, accuracy, and performance for production use cases.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了如何使用向量相似度搜索来实现一个RAG应用。虽然纯向量相似度搜索可以带你走很长的路，并且比普通的全文搜索有巨大的改进，但它通常不足以产生足够高质量、准确性和性能，以满足生产用例。
- en: In this section, we’ll look at how to improve the retriever to get better results.
    We’ll consider how to add full-text search to the RAG application to enable hybrid
    search.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何改进检索器以获得更好的结果。我们将考虑如何向RAG应用添加全文搜索以实现混合搜索。
- en: 2.3.1 Full-text search index
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 全文搜索索引
- en: '*Full-text search*, a text search method in databases, has existed for a long
    time. It searches for matches in the data via keywords and not by similarity in
    a vector space. To find a match in a full-text search, the search term must be
    an exact match to a word in the data.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*全文搜索*，数据库中的一种文本搜索方法，已经存在很长时间了。它通过关键词在数据中搜索匹配项，而不是在向量空间中的相似度。要在全文搜索中找到匹配项，搜索词必须与数据中的单词完全匹配。'
- en: To enable hybrid search, we need to add a full-text search index to the database.
    Most databases have some kind of full-text search index, and in this book we’ll
    use Neo4j’s full-text search index.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用混合搜索，我们需要向数据库中添加一个全文搜索索引。大多数数据库都有某种形式的全文搜索索引，在这本书中我们将使用 Neo4j 的全文搜索索引。
- en: Listing 2.12 Creating a full-text index in Neo4j
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.12 在 Neo4j 中创建全文索引
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here we create a full-text index named `PdfChunkFulltext` on the `text` property
    of the `:Chunk` nodes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们在 `:Chunk` 节点的 `text` 属性上创建一个名为 `PdfChunkFulltext` 的全文索引。
- en: 2.3.2 Performing hybrid search
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 执行混合搜索
- en: The idea with the hybrid search is that we perform a vector similarity search
    and a full-text search and then combine the results. To be able to compare the
    scores for the two different matches, we need to normalize the scores. We do this
    by dividing the scores by the highest score for each search.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 混合搜索的想法是我们执行向量相似度搜索和全文搜索，然后合并结果。为了能够比较两种不同匹配的分数，我们需要对分数进行归一化。我们通过将分数除以每个搜索的最高分数来实现这一点。
- en: Listing 2.13 Performing hybrid search in Neo4j
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.13 在 Neo4j 中执行混合搜索
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We write a union Cypher query where we first perform a vector similarity search
    and then a full-text search. We then deduplicate the results and return the top
    `k` results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写了一个联合 Cypher 查询，首先执行向量相似度搜索，然后执行全文搜索。然后我们去重结果并返回前 `k` 个结果。
- en: Listing 2.14 Calling hybrid search in Neo4j
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.14 在 Neo4j 中调用混合搜索
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Listing 2.15 Answer from hybrid search
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.15 混合搜索的答案
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here we can see that the top result got a score of 1.0 because of the normalization.
    This means that the top result is the same as the top result from the vector similarity
    search. But we can also see that the second result is different. This is because
    the full-text search found a better match than the vector similarity search.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到由于归一化，最上面的结果得到了 1.0 的分数。这意味着最上面的结果与向量相似度搜索的最上面的结果相同。但我们也可以看到第二个结果不同。这是因为全文搜索找到了比向量相似度搜索更好的匹配。
- en: 2.4 Concluding thoughts
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 总结思考
- en: In this chapter, we looked at what vector similarity search is, what components
    it consists of, and how it fits into RAG applications. We then added full-text
    search to improve the performance of the retriever.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了向量相似度搜索是什么，它由哪些组件组成，以及它如何适应 RAG 应用。然后我们添加全文搜索来提高检索器的性能。
- en: By using both vector similarity search and full-text search, we can get better
    results than by using only one of them. While this approach might work well in
    certain situations, its quality, accuracy, and performance when using hybrid search
    is still quite limited since we’re using unstructured data to retrieve information.
    References in the text are not always captured, and the surrounding context is
    not always enough to understand the meaning of the text for the LLMs to generate
    good answers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用向量相似度搜索和全文搜索，我们可以比只使用其中之一获得更好的结果。虽然这种方法在某些情况下可能效果很好，但由于我们使用非结构化数据来检索信息，所以使用混合搜索的质量、准确性和性能仍然相当有限。文本中的引用并不总是被捕获，周围的环境也不总是足以让
    LLMs 理解文本的意义以生成好的答案。
- en: In the next chapter, we’ll look at how to improve the retriever to get better
    results.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何改进检索器以获得更好的结果。
- en: Summary
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A RAG application consists of a retriever and a generator. The retriever finds
    relevant information, and the generator uses that information to create a response.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG 应用由一个检索器和生成器组成。检索器找到相关信息，生成器使用这些信息来创建响应。
- en: Text embeddings capture the meaning of text in a vector space, which allows
    us to use vector similarity search to find similar text.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本嵌入在向量空间中捕捉文本的意义，这使得我们可以使用向量相似度搜索来找到相似文本。
- en: By adding full-text search to the RAG application, we can enable hybrid search
    to improve the performance of the retriever.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过向 RAG 应用添加全文搜索，我们可以启用混合搜索来提高检索器的性能。
- en: Vector similarity search and hybrid search can work well in certain situations,
    but their quality, accuracy, and performance are still quite limited as the data
    complexity grows.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量相似度搜索和混合搜索在特定情况下可以很好地工作，但随着数据复杂性的增加，它们的质量、准确性和性能仍然相当有限。
