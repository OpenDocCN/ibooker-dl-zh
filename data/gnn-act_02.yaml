- en: 2 Graph embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 图嵌入
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Exploring graph embeddings and their importance
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索图嵌入及其重要性
- en: Creating node embeddings using non-GNN and GNN methods
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用非GNN和GNN方法创建节点嵌入
- en: Comparing node embeddings on a semi-supervised problem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在半监督问题上比较节点嵌入
- en: Taking a deeper dive into embedding methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入探讨嵌入方法
- en: Graph embeddings are essential tools in graph-based machine learning. They transform
    the intricate structure of graphs—be it the entire graph, individual nodes, or
    edges—into a more manageable, lower-dimensional space. We do this to compress
    a complex dataset into a form that’s easier to work with, without losing its inherent
    patterns and relationships, the information to which we’ll apply a graph neural
    network (GNN) or other machine learning method.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图嵌入是图机器学习中的基本工具。它们将图复杂的结构——无论是整个图、单个节点还是边——转换成一个更易于管理的、低维度的空间。我们这样做是为了将复杂的数据集压缩成更容易处理的形式，同时不丢失其固有的模式和关系，这些信息将应用于图神经网络（GNN）或其他机器学习方法。
- en: Graphs, as we’ve learned, encapsulate relationships and interactions within
    networks, whether they’re social networks, biological networks, or any system
    where entities are interconnected. Embeddings capture these real-life relationships
    in a compact form, facilitating tasks such as visualization, clustering, or predictive
    modeling.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所学的，图封装了网络中的关系和交互，无论是社交网络、生物网络还是任何实体相互连接的系统。嵌入以紧凑的形式捕捉这些现实生活中的关系，促进了可视化、聚类或预测建模等任务。
- en: 'There are numerous strategies to derive these embeddings, each with its unique
    approach and application: from classical graph algorithms that use the network’s
    topology, to linear algebra techniques that decompose matrices representing the
    graph, and more advanced methods such as GNNs [1]. GNNs stand out because they
    can integrate the embedding process directly into the learning algorithm itself.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多策略可以推导这些嵌入，每种都有其独特的方法和应用：从使用网络拓扑的经典图算法，到分解表示图的矩阵的线性代数技术，以及更高级的方法如GNN [1]。GNN之所以突出，是因为它们可以将嵌入过程直接集成到学习算法本身。
- en: In traditional machine learning workflows, embeddings are generated as a separate
    step, serving as a dimensionality-reduction technique in tasks such as regression
    or classification. However, GNNs merge embedding generation with the model’s learning
    process. As the network processes inputs through its layers, the embeddings are
    refined and updated, making the learning phase and the embedding phase inseparable.
    This means that GNNs learn the most informative representation of the graph data
    during training time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的机器学习工作流程中，嵌入作为单独的步骤生成，在回归或分类等任务中作为降维技术。然而，GNN将嵌入生成与模型的训练过程合并。随着网络通过其层处理输入，嵌入被精炼和更新，使得学习阶段和嵌入阶段不可分割。这意味着GNN在训练时间学习图数据的最大信息表示。
- en: Using graph embeddings can significantly enhance your data science and machine
    learning projects, especially when dealing with complex networked data. By capturing
    the essence of the graph in a lower-dimensional space, embeddings make it feasible
    to apply a variety of other machine learning techniques to graph data, opening
    up a world of possibilities for analysis and model building.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用图嵌入可以显著提升你的数据科学和机器学习项目，尤其是在处理复杂网络数据时。通过在低维空间中捕捉图的本质，嵌入使得将各种其他机器学习技术应用于图数据成为可能，为分析和模型构建开辟了一个广阔的可能性世界。
- en: In this chapter, we begin with an introduction to graph embeddings and a case
    study on a graph of political book purchases. We start with Node2Vec (N2V) to
    establish a baseline with a non-GNN approach, guiding you through its practical
    application. In section 2.2, we shift to GNNs, offering a hands-on introduction
    to GNN-based embeddings, including setup, preprocessing, and visualization. Section
    2.3 provides a comparative analysis of N2V and GNN embeddings, highlighting their
    applications. The chapter then rounds off with a discussion of the theoretical
    aspects of these embedding methods, with a special focus on the principles behind
    N2V and the message-passing mechanism in GNNs. The process we take in this chapter
    is illustrated in figure 2.1.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先介绍图嵌入以及关于政治书籍购买图的案例研究。我们首先使用Node2Vec（N2V）来建立一个非GNN方法的基线，引导您了解其实际应用。在第2.2节中，我们转向GNN，提供基于GNN的嵌入的动手介绍，包括设置、预处理和可视化。第2.3节提供了N2V和GNN嵌入的比较分析，突出了它们的应用。本章随后通过讨论这些嵌入方法的理论方面结束，特别关注N2V背后的原理和GNN中的消息传递机制。本章所采用的过程在图2.1中得到了说明。
- en: '![figure](../Images/2-1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-1.png)'
- en: Figure 2.1 Summary of process and objectives in chapter 2
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 第2章过程和目标总结
- en: Note  Code from this chapter can be found in notebook form at the GitHub repository
    ([https://mng.bz/qxnE](https://mng.bz/qxnE)). Colab links and data from this chapter
    can be accessed in the same location.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章的代码以笔记本形式存储在GitHub仓库中（[https://mng.bz/qxnE](https://mng.bz/qxnE)）。本章的Colab链接和数据可以在同一位置访问。
- en: 2.1 Creating embeddings with Node2Vec
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 使用Node2Vec创建嵌入
- en: Understanding the relationships within a network is a core task in many fields,
    from social network analysis to biology and recommendation systems. In this section,
    we’ll explore how to create node embeddings using *Node2Vec (N2V)*, a technique
    inspired by Word2Vec from natural language processing (NLP) [2]. N2V captures
    the context of nodes within a graph by simulating random walks, allowing us to
    understand the neighborhood relationships between nodes in a low-dimensional space.
    This approach is effective for identifying patterns, clustering similar nodes,
    and preparing data for machine learning tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多领域，理解网络中的关系是一个核心任务，从社交网络分析到生物学和推荐系统。在本节中，我们将探讨如何使用受自然语言处理（NLP）中的Word2Vec启发的技术*Node2Vec（N2V）*来创建节点嵌入。N2V通过模拟随机游走来捕捉图中节点的上下文，使我们能够在低维空间中理解节点之间的邻域关系。这种方法对于识别模式、聚类相似节点以及为机器学习任务准备数据是有效的。
- en: 'To make this process accessible, we’ll use the `Node2Vec` Python library, which
    is beginner-friendly, although it may be slower on larger graphs. N2V helps create
    embeddings that capture the structural relationships between nodes, which we can
    then visualize to uncover insights about the graph’s structure. Our workflow involves
    several steps:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个过程易于理解，我们将使用`Node2Vec` Python库，它对初学者友好，尽管在大型图上可能较慢。N2V有助于创建捕获节点之间结构关系的嵌入，然后我们可以将其可视化，以揭示关于图结构的见解。我们的工作流程涉及几个步骤：
- en: '*Load data and set N2V parameters.* We start by loading our graph data and
    initializing N2V with specific parameters to control the random walks, such as
    walk length and the number of walks per node.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*加载数据并设置N2V参数。* 我们首先加载数据，并使用特定的参数初始化N2V来控制随机游走，例如游走长度和每个节点的游走次数。'
- en: '*Create embeddings.* N2V generates node embeddings by performing random walks
    on the graph, effectively summarizing each node’s local neighborhood into a vector
    format.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*创建嵌入。* N2V通过在图上执行随机游走来生成节点嵌入，有效地将每个节点的局部邻域总结为向量格式。'
- en: '*Transform embeddings.* The resulting embeddings are saved and then transformed
    into a format suitable for visualization.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*转换嵌入。* 得到的嵌入被保存，然后转换为适合可视化的格式。'
- en: '*Visualize embeddings in two dimensions.* We use UMAP, a dimensionality reduction
    technique, to project these embeddings into two dimensions, making it easier to
    visualize and interpret the results.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在二维中可视化嵌入。* 我们使用UMAP，一种降维技术，将这些嵌入投影到二维空间，使其更容易可视化和解释结果。'
- en: Our data is the Political Books dataset, which comprises books (nodes) connected
    by frequent co-purchases on Amazon.com during the 2004 US election period (edges)
    [3]. Using this dataset provides a compelling example of how N2V can reveal underlying
    patterns in co-purchasing behavior, potentially reflecting broader ideological
    groupings among book buyers [4]. Table 2.1 provides key information about the
    Political Books graph.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据是政论书籍数据集，它由2004年美国选举期间在Amazon.com上频繁共同购买的书籍（边）连接而成[3]。使用这个数据集提供了一个令人信服的例子，说明N2V如何揭示共同购买行为中的潜在模式，可能反映了书籍购买者中更广泛的意识形态分组[4]。表2.1提供了政论书籍图的关键信息。
- en: Table 2.1 Overview of the Political Books dataset
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.1 政论书籍数据集概览
- en: '| Books in the political genre co-purchased on Amazon.com |  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 在Amazon.com上共同购买的政论书籍 |  |'
- en: '| --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Number of nodes (books)  | 105  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 节点数量（书籍） | 105 |'
- en: '| Left-leaning nodes  | 41.0%  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 左倾节点 | 41.0% |'
- en: '| Right-leaning nodes  | 46.7%  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 右倾节点 | 46.7% |'
- en: '| Neutral nodes  | 12.4%  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 中立节点 | 12.4% |'
- en: '| Number of edges Edges represent the prevalence of a co-purchase between two
    books.  | 441  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 边的数量 边代表两本书之间共同购买的发生频率。 | 441 |'
- en: 'The Political Books dataset contains the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 政论书籍数据集包含以下内容：
- en: '*Nodes* —Represent books about US politics sold by [Amazon.com](http://Amazon.com).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点* — 代表由[Amazon.com](http://Amazon.com)销售的关于美国政治的书籍。'
- en: '*Edges* —Indicate frequent co-purchasing by the same buyers, as suggested by
    Amazon’s “customers who bought this book also bought these other books” feature.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*边* — 指示相同买家频繁共同购买，如亚马逊的“购买此书的顾客也购买了这些其他书籍”功能所示。'
- en: In figure 2.2, books are shaded based on their political alignment—darker shade
    for liberal, lighter shade for conservative, and striped for neutral. The categories
    were assigned by Mark Newman through a qualitative analysis of book descriptions
    and reviews posted on Amazon.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.2中，书籍根据其政治立场进行着色 — 深色代表自由主义，浅色代表保守主义，条纹代表中立。这些类别是通过Mark Newman对亚马逊上发布的书籍描述和评论的定性分析来分配的。
- en: '![figure](../Images/2-2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-2.png)'
- en: Figure 2.2 Graph visualization of the Political Books dataset. Right-leaning
    books (nodes) are in a lighter shade and are clustered in the top half of the
    figure, left-leaning are darker shaded circles and clustered in the lower half
    of the figure, and neutral political stance are dark squares and appear in the
    middle. When two nodes are connected, it indicates that they have been purchased
    together frequently on Amazon.com.
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 政论书籍数据集的图形可视化。右倾书籍（节点）以浅色显示，并聚集在图的上半部分，左倾书籍以深色阴影显示，并聚集在图的下半部分，中立的政治立场以深色方块显示，并出现在中间。当两个节点相连时，表示它们在Amazon.com上经常一起购买。
- en: This dataset, compiled by Valdis Krebs and available through the GNN in Action
    repository ([https://mng.bz/qxnE](https://mng.bz/qxnE)) or the Carnegie Mellon
    University website ([https://mng.bz/mG8M](https://mng.bz/mG8M)), contains 105
    books (nodes) and 441 edges (co-purchases). If you want to learn more about the
    background of this dataset, Krebs has written an article with this information
    [4].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集由Valdis Krebs编制，可通过GNN in Action仓库（[https://mng.bz/qxnE](https://mng.bz/qxnE)）或卡内基梅隆大学网站（[https://mng.bz/mG8M](https://mng.bz/mG8M)）获取，包含105本书（节点）和441条边（共同购买）。如果您想了解更多关于这个数据集的背景信息，Krebs已经撰写了一篇文章包含这些信息[4]。
- en: Using N2V, we aim to explore the structure of this collection of books, uncovering
    insights based on political leanings and the potential associations between different
    book categories. By visualizing the embeddings created by N2V, we can gain a better
    understanding of how books are grouped and which ones might share a common audience,
    providing valuable insights into consumer behavior during a politically charged
    period.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用N2V，我们旨在探索这本书集合的结构，基于政治倾向和不同书籍类别之间可能存在的关联来揭示洞察。通过可视化N2V创建的嵌入，我们可以更好地理解书籍是如何分组的，哪些书籍可能拥有共同的受众，为政治敏感时期的消费者行为提供有价值的见解。
- en: From the visualization, note that the data is already clustered in a logical
    way. This is thanks to the *Kamada-Kawai algorithm* graph algorithm, which exploits
    the topological data only without the metadata and is useful for visualizing the
    graph. This graph visualization technique positions nodes in a way that reflects
    their connections, aiming for an arrangement where closely connected nodes are
    near each other but less connected nodes are farther apart. It achieves this by
    treating the nodes like points connected by springs, iteratively adjusting their
    positions until the “tension” in the springs is minimized. This results in a layout
    that naturally reveals clusters and relationships within the graph based purely
    on its structure.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从可视化中可以看出，数据已经按照逻辑方式进行了聚类。这得益于*Kamada-Kawai算法*这一图算法，它仅利用拓扑数据而不使用元数据，因此对图形可视化很有用。这种图形可视化技术将节点定位在反映它们连接的位置，旨在使紧密连接的节点彼此靠近，而较少连接的节点则相隔较远。它是通过将节点视为由弹簧连接的点，迭代调整其位置，直到弹簧中的“张力”最小化来实现的。这导致布局自然地揭示了基于图形结构的集群和关系。
- en: For the Political Books dataset, the Kamada-Kawai algorithm helps us visualize
    books (nodes) based on how often they are co-purchased on Amazon, without using
    any external information such as political alignment or book titles. This gives
    us an initial view of how books are grouped together by buying behavior. In the
    next steps, we’ll use methods such as N2V to create embeddings that capture more
    detailed patterns and further distinguish different book groups.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于政治书籍数据集，Kamada-Kawai算法帮助我们根据在亚马逊上共同购买的频率来可视化书籍（节点），而不使用任何外部信息，如政治立场或书籍标题。这使我们能够看到书籍是如何根据购买行为分组在一起的。在接下来的步骤中，我们将使用N2V等方法创建嵌入，以捕获更详细的模式和进一步区分不同的书籍组。
- en: 2.1.1 Loading data, setting parameters, and creating embeddings
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 加载数据、设置参数和创建嵌入
- en: We use the `Node2Vec` and `NetworkX` libraries for our first hands-on encounter
    with graph embeddings. After installing these packages using pip, we load our
    dataset’s graph data, which is stored in .gml format (Graph Modeling Language,
    GML), using the `NetworkX` library and generate the embeddings with the `Node2Vec`
    library.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Node2Vec`和`NetworkX`库来体验我们的第一次图嵌入实践。通过使用pip安装这些包后，我们使用`NetworkX`库加载我们的数据集的图形数据，该数据存储在.gml格式（图形建模语言，GML）中，并使用`Node2Vec`库生成嵌入。
- en: GML is a simple, human-readable plain text file format used to represent graph
    structures. It stores information about nodes, edges, and their attributes in
    a structured way, making it easy to read and write graph data. For instance, a
    .gml file might contain a list of nodes (e.g., books in our dataset) and edges
    (connections representing co-purchases) along with additional properties such
    as labels or weights. This format is widely used for exchanging graph data between
    different software and tools. By loading the .gml file with `NetworkX`, we can
    easily manipulate and analyze the graph in Python.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GML是一种简单、可读的纯文本文件格式，用于表示图结构。它以结构化的方式存储有关节点、边及其属性的信息，使其易于读取和写入图数据。例如，一个.gml文件可能包含节点列表（例如，我们数据集中的书籍）和边（表示共同购买的联系），以及额外的属性，如标签或权重。这种格式在交换不同软件和工具之间的图数据方面得到了广泛应用。通过使用`NetworkX`加载.gml文件，我们可以轻松地在Python中操作和分析图形。
- en: 'In the `Node2Vec` library’s `Node2Vec` function, we can use the following parameters
    to specify the calculations done and the properties of the output embedding:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Node2Vec`库的`Node2Vec`函数中，我们可以使用以下参数来指定进行的计算和输出嵌入的性质：
- en: '*Size of the embedding (*`dimensions`*)* —Think of this as how detailed each
    node’s profile is, as in how many different traits you’re noting down. The standard
    detail level is 128 traits, but you can tweak this based on how complex you want
    each node’s profile to be.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*嵌入的大小（*`dimensions`*）* — 将其视为每个节点配置文件的详细程度，就像记录了多少种不同的特征一样。标准的详细程度是128个特征，但你可以根据你希望每个节点的配置文件有多复杂来调整这个值。'
- en: '*Length of each walk (*`Walk` `Length`*)* —This is about how far each random
    walk through your graph goes, with 80 steps being the usual journey. If you want
    to see more of the neighborhood around a node, increase this number.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每次遍历的长度（*`Walk Length`*）* — 这是指每次随机遍历你的图形有多远，通常的旅程是80步。如果你想看到节点周围更多的邻域，增加这个数字。'
- en: '*Number of walks per node (*`Num` `Walks`*)* —This tells us how many times
    we’ll take a walk starting from each node. Starting with 10 walks gives a good
    overview, but if you want a fuller picture of a node’s surroundings, consider
    going on more walks.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每个节点的行走次数（`Num` `Walks`*）—这告诉我们我们将从每个节点开始行走多少次。从10次行走开始可以提供一个良好的概述，但如果你想要更全面地了解节点的周围环境，可以考虑进行更多次行走。'
- en: '*Backtracking control (Return Parameter,* `p`*)* —This setting helps decide
    if our walk should circle back to where it’s been. Setting it at 1 keeps things
    balanced, but adjusting it can make your walks more or less exploratory.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回溯控制（返回参数，`p`*）—这个设置有助于决定我们的行走是否应该回到它曾经去过的地方。将其设置为1可以保持平衡，但调整它可以使你的行走更具或更少的探索性。'
- en: '*Exploration Depth (In-Out Parameter,* `q`*)* —This one’s about choosing between
    taking in the broader neighborhood scene (e.g., a breadth-first search with `q`
    greater than 1) or diving deep into specific paths (e.g., a depth-first search
    with `q` less than 1), with 1 being a mix of both.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*探索深度（输入-输出参数，`q`*）—这一部分是关于选择是接受更广泛的邻域场景（例如，当`q`大于1时的广度优先搜索）还是深入特定的路径（例如，当`q`小于1时的深度优先搜索），其中1是两者的混合。'
- en: Adjust these settings based on what you’re looking to understand about your
    nodes and their connections. Want more depth? Tweak the exploration depth. Looking
    for broader context? Adjust the walk length and the number of walks. In addition,
    keep in mind that the size of your embeddings should match the level of detail
    you need. In general, it’s a good idea to try different combinations of these
    parameters to see the effect on the embeddings.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你想了解的节点及其连接的情况调整这些设置。想要更深入的了解？调整探索深度。想要更广泛的上下文？调整行走长度和行走次数。此外，请注意，你的嵌入大小应该与所需的细节水平相匹配。一般来说，尝试这些参数的不同组合以查看对嵌入的影响是一个好主意。
- en: For this exercise, we’ll use the first four parameters. Deeper details on these
    parameters are found in section 2.4.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们将使用前四个参数。关于这些参数的更详细信息可以在第2.4节中找到。
- en: 'The code in listing 2.1 begins by loading the graph into a variable called
    `books_ graph`, using the `read_gml` method from the `NetworkX` library. Next,
    a N2V `model` is initialized with the loaded graph. This model is set up with
    specific parameters: it will create 64-dimensional embeddings for each node, use
    walks of 30 steps long, perform 200 walks starting from each node to gather context,
    and run these operations in parallel across four workers to speed up the process.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1中的代码首先使用`NetworkX`库的`read_gml`方法将图加载到名为`books_graph`的变量中。接下来，使用加载的图初始化一个N2V
    `模型`。此模型设置了特定的参数：它将为每个节点创建64维的嵌入，使用30步长的行走，从每个节点开始执行200次行走以收集上下文，并在四个工作器上并行运行这些操作以加快处理速度。
- en: The N2V model is then trained with additional parameters defined in the `fit`
    method. This involves setting a context window size of 10 nodes around each target
    node to learn the embeddings, considering all nodes at least once (`min_count=1`),
    and processing four words (nodes, in this context) each time during training.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用`fit`方法中定义的附加参数训练N2V模型。这包括设置每个目标节点周围10个节点的上下文窗口大小以学习嵌入，考虑所有节点至少一次（`min_count=1`），并在训练过程中每次处理四个单词（在这个上下文中是节点）。
- en: Once trained, we access the node embeddings using the `model`’s `wv` method
    (reflecting its NLP heritage, wv stands for word vectors). For our downstream
    tasks, we map each node to its embedding using a dictionary comprehension.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们使用`model`的`wv`方法（反映其NLP传承，wv代表词向量）访问节点嵌入。对于我们的下游任务，我们使用字典推导式将每个节点映射到其嵌入。
- en: Listing 2.1 Generating N2V embeddings
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.1 生成N2V嵌入
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Loads the graph data from a GML file into a NetworkX graph object'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从GML文件中加载图数据到NetworkX图对象'
- en: '#2 Initializes the N2V model with specified parameters for the input graph'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用输入图的指定参数初始化N2V模型'
- en: '#3 Trains the N2V model'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 训练N2V模型'
- en: '#4 Extracts and stores the node embeddings generated by the N2V model in a
    dictionary'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 从N2V模型中提取并存储生成的节点嵌入到字典中'
- en: 2.1.2 Demystifying embeddings
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 揭秘嵌入
- en: Let’s explore what these embeddings are and why they are valuable. An *embedding*
    is a dense numerical vector that represents the identity of a node, edge, or graph
    in a way that captures essential information about its structure and relationships.
    In our context, an embedding created by N2V captures a node’s position and neighborhood
    within the graph using topological information. This means it summarizes how the
    node is connected to others, effectively capturing its role and importance in
    the network. Later, when we use GNNs to create embeddings, they will also encapsulate
    the node’s features, providing an even richer representation that includes both
    structure and attributes. We get deeper into theoretical aspects of embeddings
    in section 2.4\.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索这些嵌入是什么以及为什么它们有价值。*嵌入*是一个密集的数值向量，以捕捉节点、边或图的结构和关系的方式表示其身份。在我们的上下文中，由N2V创建的嵌入使用拓扑信息捕捉节点在图中的位置和邻域。这意味着它总结了节点如何与其他节点连接，有效地捕捉了其在网络中的角色和重要性。稍后，当我们使用GNN创建嵌入时，它们也将封装节点的特征，提供包含结构和属性的双重丰富表示。我们将在第2.4节中深入了解嵌入的理论方面。
- en: These embeddings are powerful because they transform complex, high-dimensional
    graph data into a fixed-size vector format that can be easily used in various
    analyses and machine learning tasks. For example, they allow us to perform exploratory
    data analysis by revealing patterns, clusters, and relationships within the graph.
    Beyond this, embeddings can be directly used as features in machine learning models,
    where each dimension of the vector represents a distinct feature. This is particularly
    useful in applications where understanding the structure and connections between
    data points, such as in social networks or recommendation systems, can significantly
    improve model performance.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入非常强大，因为它们将复杂的高维图数据转换成固定大小的向量格式，可以轻松用于各种分析和机器学习任务。例如，它们允许我们通过揭示图中的模式、集群和关系来执行探索性数据分析。除此之外，嵌入可以直接用作机器学习模型中的特征，其中向量的每个维度代表一个独特的特征。这在理解数据点之间的结构和连接的应用中特别有用，例如在社会网络或推荐系统中，这可以显著提高模型性能。
- en: To illustrate, consider the node representing the book *Losing Bin Laden* in
    our Political Books dataset. Using the command `model.wv['Losing` `Bin` `Laden']`,
    we retrieve its dense vector embedding. This vector, shown in figure 2.3, captures
    various aspects of the book’s role within the network of co-purchased books, providing
    a compact, informative representation that can be used for further analysis or
    as input to other models.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，考虑一下我们政治书籍数据集中代表书籍《丢失的本·拉登》的节点。使用命令`model.wv['Losing` `Bin` `Laden']`，我们可以检索其密集向量嵌入。这个向量如图2.3所示，捕捉了书籍在共同购买书籍网络中的各种角色，提供了一个紧凑、信息丰富的表示，可用于进一步分析或作为其他模型的输入。
- en: '![figure](../Images/2-3.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-3.png)'
- en: Figure 2.3 Extracting the embedding for the node associated with the political
    book Losing Bin Laden. The output is a dense vector represented as a Python list.
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3 提取与政治书籍《丢失的本·拉登》相关的节点嵌入。输出是一个以Python列表表示的密集向量。
- en: These embeddings can be used for exploratory data analysis to see the patterns
    and relationships in a graph. However, their usage extends further. One common
    application is to use these vectors as features in a machine learning problem
    that uses tabular data. In that case, each element in our embedding array will
    become a distinct feature column in the tabular data. This can add a rich representation
    of each node to complement other attributes in model training. In the next section,
    we’ll look at how to visualize these embeddings to gain deeper insights into the
    patterns and relationships they represent.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入可用于探索性数据分析，以查看图中的模式和关系。然而，它们的用途更广泛。一个常见应用是将这些向量用作使用表格数据的机器学习问题中的特征。在这种情况下，我们嵌入数组中的每个元素将成为表格数据中一个独特的特征列。这可以为模型训练中的其他属性添加丰富的表示。在下一节中，我们将探讨如何可视化这些嵌入，以深入了解它们所代表的模式和关系。
- en: 2.1.3 Transforming and visualizing the embeddings
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 转换和可视化嵌入
- en: Visualization methods such as Uniform Manifold Approximation and Projection
    (UMAP) are powerful tools for reducing high-dimensional datasets into lower-dimensional
    space [5]. UMAP is particularly effective for identifying inherent clusters and
    visualizing complex structures that are difficult to perceive in high-dimensional
    data. Compared to other methods, such as t-SNE, UMAP excels in preserving both
    local and global structures, making it ideal for revealing patterns and relationships
    across different scales in the data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如统一流形近似和投影（UMAP）之类的可视化方法是将高维数据集降低到低维空间的有力工具[5]。UMAP特别有效于识别内在集群和可视化在高度数据中难以感知的复杂结构。与其他方法，如t-SNE相比，UMAP在保留局部和全局结构方面表现出色，使其成为揭示数据中不同尺度上的模式和关系的理想选择。
- en: While N2V generates embeddings by capturing the network structure of our data,
    UMAP takes these high-dimensional embeddings and maps them onto a lower-dimensional
    space (typically two or three dimensions). This mapping aims to keep similar nodes
    close together while also preserving broader structural relationships, providing
    a more comprehensive visualization of the graph’s topology. After obtaining our
    N2V embeddings and converting them into a numerical array, we initialize the UMAP
    model with two components to project our data onto a 2D plane. By carefully selecting
    parameters such as the number of neighbors and minimum distance, UMAP can balance
    between revealing fine-grained local relationships and maintaining global distances
    between clusters.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当N2V通过捕获我们的数据网络结构生成嵌入时，UMAP将这些高维嵌入映射到低维空间（通常是两到三个维度）。这种映射旨在使相似的节点彼此靠近，同时保留更广泛的结构关系，从而提供对图拓扑的更全面可视化。在获得我们的N2V嵌入并将它们转换为数值数组后，我们使用两个组件初始化UMAP模型，以将我们的数据投影到二维平面上。通过仔细选择参数，如邻居数量和最小距离，UMAP可以在揭示细粒度局部关系和保持集群之间的全局距离之间取得平衡。
- en: By using UMAP, we gain a more accurate and interpretable visualization of our
    graph embeddings as shown in the following listing, allowing us to explore and
    analyze patterns, clusters, and structures more effectively than with traditional
    methods such as t-SNE.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用UMAP，我们获得了更准确和可解释的图嵌入可视化，如下所示列表，使我们能够比使用t-SNE等传统方法更有效地探索和分析模式、集群和结构。
- en: Listing 2.2 Visualizing the embeddings using UMAP
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.2使用UMAP可视化嵌入
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Transforms the embeddings into a list of vectors for UMAP'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将嵌入转换为UMAP的向量列表'
- en: '#2 Initializes and fits UMAP'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 初始化和拟合UMAP'
- en: '#3 Plots the nodes with UMAP embeddings and color by their value'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用UMAP嵌入绘制节点并按其值着色'
- en: The resultant figure 2.4 encapsulates the political book graph’s embeddings
    as distilled by N2V and subsequently visualized through UMAP. The nodes appear
    in different shades according to their political alignment. The visualization
    unfolds a discernible structure, with potential clusters that correspond to the
    various political leanings.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图2.4封装了由N2V提炼并由UMAP可视化的政治书籍图嵌入。节点根据其政治立场以不同的阴影出现。可视化展开了一个可识别的结构，其中潜在的集群对应于各种政治倾向。
- en: '![figure](../Images/2-4.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-4.png)'
- en: Figure 2.4 Embeddings of the Political Books dataset graph generated by N2V
    and visualized using UMAP. Shape and shading variations distinguish the three
    political classes.
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4展示了由N2V生成并通过UMAP可视化的政治书籍数据集图嵌入。形状和阴影的变化区分了三个政治类别。
- en: You might wonder why we don’t just reduce the dimensions of the N2V embeddings
    from `64` to `2` and visualize them directly, bypassing UMAP altogether? In listing
    2.3, we show this approach, applying a 2D N2V transformation directly to our `books_graph`
    object. (For more technical detail and theory of these methods, see section 2.4.)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么我们不直接将N2V嵌入的维度从`64`降低到`2`并直接可视化它们，完全绕过UMAP？在列表2.3中，我们展示了这种方法，直接将2D
    N2V转换应用于我们的`books_graph`对象。（有关这些方法的更多技术细节和理论，请参阅第2.4节。）
- en: The `dimensions` parameter is set to `2`, aiming for a direct 2D representation
    suitable for immediate visualization without further dimensionality reduction.
    The other parameters are kept the same.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`dimensions`参数设置为`2`，旨在实现直接的二维表示，适合立即可视化，无需进一步降维。其他参数保持不变。'
- en: Once the model is fitted with the specified window and word batch settings,
    we extract the 2D embeddings and store them in a dictionary keyed by the string
    representation of each node. This enables a direct mapping from the node to its
    embedding vector.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型根据指定的窗口和词批次设置拟合，我们就提取2D嵌入并将它们存储在一个字典中，该字典以每个节点的字符串表示为键。这使得可以直接从节点映射到其嵌入向量。
- en: The extracted 2D points are compiled into a NumPy array and plotted. We use
    the standard `Matplotlib` library to create a scatterplot of these points using
    the prepared color scheme to represent the political leaning of each node visually.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的2D点被编译成一个NumPy数组并绘制。我们使用标准的`Matplotlib`库创建这些点的散点图，并使用准备好的颜色方案来直观地表示每个节点的政治倾向。
- en: Listing 2.3 Visualizing 2D N2V embeddings without t-SNE
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.3 无t-SNE可视化2D N2V嵌入
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Initializes N2V with 2D embeddings for visualization'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化N2V，使用2D嵌入进行可视化'
- en: '#2 Trains N2V model with specified window and walks settings'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用指定的窗口和行走设置训练N2V模型'
- en: '#3 Maps nodes to their 2D embeddings'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将节点映射到它们的2D嵌入'
- en: '#4 Forms an array of 2D points for each node’s embedding'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 为每个节点的嵌入形成2D点数组'
- en: '#5 Plots the 2D embeddings with specified node colors'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 使用指定的节点颜色绘制2D嵌入'
- en: The outcome shows how the books are separated by political leanings, similar
    to the UMAP result, but where the books are more bunched together (see figure
    2.5). The two embeddings are then shown in figure 2.6.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示了书籍如何根据政治倾向被分开，类似于UMAP的结果，但书籍更加密集地聚集在一起（见图2.5）。然后，这两个嵌入在图2.6中展示。
- en: '![figure](../Images/2-5.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-5.png)'
- en: Figure 2.5 Embeddings of the Political Books dataset graph generated and visualized
    by N2V for two dimensions. Shape and shading variations distinguish the three
    political classes. Here, we see a similar clustering by political leaning as earlier
    in figure 2.4 but more bunched together.
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 由N2V生成并可视化的政治书籍数据集图嵌入，用于两个维度。形状和阴影变化区分了三个政治类别。在这里，我们看到与图2.4中类似的按政治倾向的聚类，但更加密集。
- en: '![figure](../Images/2-6.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-6.png)'
- en: Figure 2.6 Comparison of embeddings generated by N2V and t-SNE and a direct
    visualization of 2D Node2Vec
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 N2V和t-SNE生成的嵌入比较以及2D Node2Vec的直接可视化
- en: It’s clear that both methods know to separate the books into groups based on
    political leanings. N2V is less expressive in how it separates the books, bunching
    them together across the two dimensions. Meanwhile, UMAP is better for spreading
    out the books in two dimensions. The relevant benefit or information contained
    within these dimensions depends on the task at hand.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这两种方法都知道根据政治倾向将书籍分成组。N2V在分离书籍方面表达性较弱，将它们在两个维度上聚集在一起。同时，UMAP更适合在两个维度上分散书籍。这些维度中包含的相关好处或信息取决于手头的任务。
- en: '2.1.4 Beyond visualization: Applications and considerations of N2V embeddings'
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.4 超越可视化：N2V嵌入的应用和考虑
- en: While visualizing N2V embeddings offers intuitive insights into the dataset’s
    structure, their usage extends far beyond graphical representation. N2V is an
    embedding method designed specifically for graphs; it captures both the local
    and global structural properties of nodes by simulating random walks through the
    graph. This process allows N2V to create dense, numerical vectors that summarize
    the position and context of each node within the overall network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可视化N2V嵌入可以为数据集的结构提供直观的见解，但其用途远不止图形表示。N2V是一种专门为图设计的嵌入方法；它通过在图中模拟随机游走来捕捉节点的局部和全局结构属性。这个过程允许N2V创建密集的数值向量，这些向量总结了每个节点在整个网络中的位置和上下文。
- en: These embeddings can then serve as feature-rich inputs for a variety of machine
    learning tasks, such as classification, recommendation, or clustering. For example,
    in our Political Books dataset, embeddings could help predict a book’s political
    leaning based on its co-purchase patterns or could recommend books to users with
    similar political interests. They might even be used to forecast future sales
    based on the content of a book.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入可以作为丰富特征输入用于各种机器学习任务，如分类、推荐或聚类。例如，在我们的政治书籍数据集中，嵌入可以帮助根据书籍的共购买模式预测书籍的政治倾向，或者向具有相似政治兴趣的用户推荐书籍。它们甚至可以用来根据书籍的内容预测未来的销售。
- en: However, it’s important to understand the nature of N2V’s learning approach,
    which is *transductive*. Transductive learning is designed to work only with the
    specific dataset it was trained on and can’t generalize to new, unseen nodes without
    retraining the model. This characteristic makes N2V highly effective for static
    datasets where all nodes and edges are known up front but less suitable for dynamic
    settings where new data points or connections frequently appear. Essentially,
    N2V focuses on extracting detailed patterns and relationships from the existing
    graph rather than developing a model that can easily adapt to new data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，了解N2V学习方法的本质，即它是**归纳的**，是很重要的。归纳学习旨在仅与训练数据集一起工作，并且不能在没有重新训练模型的情况下泛化到新的、未见的节点。这一特性使得N2V在所有节点和边都事先已知的情况下非常有效，但在新数据点或连接频繁出现的动态环境中则不太适用。本质上，N2V专注于从现有图中提取详细模式和关系，而不是开发能够轻松适应新数据的模型。
- en: While this transductive nature has its limitations, it also offers significant
    advantages. Because N2V uses the full structure of the graph during training,
    it can capture intricate relationships and dependencies that might be missed by
    more generalized methods. This makes N2V particularly powerful for tasks where
    the complete, fixed structure of the data is known and stable. However, to apply
    N2V effectively, it’s crucial to ensure that the graph data is represented in
    a way that captures all relevant features. In some cases, additional edges or
    nodes may need to be added to the graph to fully represent the underlying relationships.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种归纳性质有其局限性，但它也提供了显著的优势。因为N2V在训练过程中使用了图的完整结构，它可以捕捉到可能被更通用方法遗漏的复杂关系和依赖。这使得N2V在数据完整、固定结构已知且稳定的情况下特别强大。然而，为了有效地应用N2V，至关重要的是要确保图数据以能够捕捉所有相关特征的方式表示。在某些情况下，可能需要向图中添加额外的边或节点，以完全表示潜在的关系。
- en: For those interested in a deeper understanding of transductive models and how
    N2V’s approach compares to other methods, further details are provided in section
    2.4.2\. That section will explore the tradeoffs between transductive and inductive
    learning in greater depth [6, 7], helping you understand when each approach is
    most appropriate.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对归纳模型有更深入理解以及N2V方法与其他方法比较感兴趣的人来说，2.4.2节提供了更多细节。该节将进一步探讨归纳学习和归纳学习之间的权衡[6,
    7]，帮助你理解何时采用每种方法最为合适。
- en: While N2V is effective for generating embeddings that capture the structure
    of a fixed graph, real-world data often demands a more flexible and generalizable
    approach. This need brings us to our first GNN architecture for creating node
    embeddings. Unlike N2V, which is a transductive method limited to the specific
    nodes and edges in the training data, GNNs can learn in an *inductive* manner.
    This means GNNs are capable of generalizing to new, unseen nodes or edges without
    requiring retraining on the entire graph.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然N2V在生成能够捕捉固定图结构的嵌入方面是有效的，但现实世界的数据通常需要更灵活和可泛化的方法。这种需求将我们引向了第一个用于创建节点嵌入的GNN架构。与仅限于训练数据中特定节点和边的归纳方法N2V不同，GNNs可以以归纳的方式学习。这意味着GNNs能够泛化到新的、未见的节点或边，而无需在整个图上重新训练。
- en: GNNs achieve this by not only understanding the network’s complex structure
    but also by incorporating node features and relationships into the learning process.
    This approach allows GNNs to adapt dynamically to changes in the graph, making
    them well-suited for applications where the data is continually evolving. The
    shift from N2V to GNNs represents a key transition from focusing on deep analysis
    within a static dataset to a broader applicability across diverse, evolving networks.
    This adaptability sets the stage for a wider range of graph-based machine learning
    applications that require flexibility and scalability. In the next section, we’ll
    explore how GNNs go beyond the capabilities of N2V and other transductive methods,
    allowing for more versatile and powerful models that can handle the dynamic nature
    of real-world data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs通过不仅理解网络的复杂结构，还将节点特征和关系纳入学习过程来实现这一点。这种方法允许GNNs动态地适应图中的变化，使它们非常适合数据持续演变的应用。从N2V到GNNs的转变代表了从关注静态数据集内的深度分析到更广泛适用于各种演变网络的适用性的关键过渡。这种适应性为需要灵活性和可扩展性的更广泛的基于图机器学习应用奠定了基础。在下一节中，我们将探讨GNNs如何超越N2V和其他归纳方法的能力，允许构建更灵活、更强大的模型，以处理现实世界数据的动态特性。
- en: 2.2 Creating embeddings with a GNN
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 使用GNN创建嵌入
- en: While N2V provides a powerful method for generating embeddings by capturing
    the local and global structure of a graph, it’s fundamentally a transductive approach,
    meaning it can’t easily generalize to unseen nodes or edges without retraining.
    Although there have been extensions to N2V that enable it to work in inductive
    settings, GNNs are inherently designed for inductive learning. This means they
    can learn general patterns from the graph data that allow them to make predictions
    or to generate embeddings for new nodes or edges without needing to retrain the
    entire model. This gives GNNs a significant edge in scenarios where flexibility
    and adaptability are crucial.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管N2V提供了一种通过捕捉图的本地和全局结构来生成嵌入的强大方法，但它的本质上是归纳方法，这意味着它不能轻易地泛化到未见过的节点或边，除非重新训练。尽管N2V的扩展使其能够在归纳设置中工作，但GNNs本质上是为归纳学习设计的。这意味着它们可以从图数据中学习一般模式，从而允许它们做出预测或为新节点或边生成嵌入，而无需重新训练整个模型。这使GNNs在灵活性和适应性至关重要的场景中具有显著优势。
- en: GNNs not only incorporate the structural information of the graph, like N2V,
    but they also use node features to create richer representations. This dual capacity
    allows GNNs to learn both the complex relationships within the graph and the specific
    characteristics of individual nodes, enabling them to excel in tasks where both
    types of information are important.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs不仅结合了图的结构信息，如N2V，而且它们还使用节点特征来创建更丰富的表示。这种双重能力使得GNNs能够学习图内复杂的相互关系以及单个节点的特定特征，使它们在需要这两种类型信息的重要任务中表现出色。
- en: That said, while GNNs have demonstrated impressive performance across many applications,
    they don’t universally outperform methods such as N2V in all cases. For instance,
    N2V and other random walk-based methods can sometimes perform better in scenarios
    where labeled data is scarce or noisy, thanks to their ability to work with just
    the graph structure without needing additional node features.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GNNs在许多应用中展示了令人印象深刻的性能，但它们并不在所有情况下都优于N2V等方法。例如，N2V和其他基于随机游走的方法有时在标记数据稀缺或噪声的情景中表现更好，这得益于它们仅通过图结构就能工作，而不需要额外的节点特征。
- en: 2.2.1 Constructing the embeddings
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 构建嵌入
- en: Unlike N2V, GNNs learn graph representations and perform tasks such as node
    classification or link prediction simultaneously during training. Information
    from the entire graph is processed through successive GNN layers, each refining
    the node embeddings without requiring a separate step for their creation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与N2V不同，GNNs在训练期间同时学习图表示和执行节点分类或链接预测等任务。整个图的信息通过连续的GNN层进行处理，每一层都细化节点嵌入，而不需要为它们的创建单独的步骤。
- en: To demonstrate how a GNN extracts features from graph data, we’ll perform a
    straightforward pass-through using an untrained model to generate preliminary
    embeddings. Even without the optimization typically involved in training, this
    approach will show how GNNs use message passing (explored further in section 2.4.4)
    to update embeddings, capturing both the graph’s structure and its node features.
    When optimization is added, these embeddings become tailored to specific tasks
    such as node classification or link prediction.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示GNN如何从图数据中提取特征，我们将使用一个未训练的模型进行简单的传递，以生成初步嵌入。即使没有通常涉及训练的优化，这种方法也将展示GNNs如何使用消息传递（在2.4.4节中进一步探讨）来更新嵌入，捕捉图的结构和节点特征。当添加优化时，这些嵌入将针对特定任务进行调整，如节点分类或链接预测。
- en: Defining our GNN architecture
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义我们的GNN架构
- en: We initiate our process by defining a simple GCN architecture, as shown in listing
    2.4\. Our `SimpleGNN` class inherits from `torch.nn.Module` and is composed of
    two `GCNConv` layers, which are the building blocks of our GNN. This architecture
    is shown in figure 2.7, consisting of the first layer, a message passing layer
    (`self.conv1`), an activation (`torch.relu`), a dropout layer (`torch.dropout`),
    and a second message passing layer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过定义一个简单的GCN架构来启动我们的过程，如列表2.4所示。我们的`SimpleGNN`类继承自`torch.nn.Module`，由两个`GCNConv`层组成，这是我们的GNN的构建块。这种架构如图2.7所示，包括第一层、一个消息传递层(`self.conv1`)、一个激活(`torch.relu`)、一个dropout层(`torch.dropout`)和第二个消息传递层。
- en: Listing 2.4 Our `SimpleGNN` class
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.4 我们的`SimpleGNN`类
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Initializes the GNN class with input and hidden layer sizes'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化GNN类，包括输入和隐藏层大小'
- en: '#2 First GCN layer from input features to hidden channels'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 从输入特征到隐藏通道的第一个GCN层'
- en: '#3 Second GCN layer within the hidden space'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 隐藏空间中的第二个GCN层'
- en: '#4 Forward pass function defines data flow'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 前向传递函数定义数据流'
- en: '#5 First GCN layer processing'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 第一个GCN层处理'
- en: '#6 Activation function for nonlinearity'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 非线性激活函数'
- en: '#7 Dropout for regularization during training'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 训练过程中的Dropout正则化'
- en: '#8 Second GCN layer processing'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 第二个GCN层处理'
- en: '#9 Returns the final node embeddings'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 返回最终的节点嵌入'
- en: '![figure](../Images/2-7.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-7.png)'
- en: Figure 2.7 Architecture diagram of the `SimpleGNN` model
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7 `SimpleGNN`模型架构图
- en: Let’s talk about the architecture aspects specific to GNNs. The activation and
    dropout are common in many deep learning scenarios. The GNN layers, however, are
    different from conventional deep learning layers in a fundamental way. The core
    principle that allows GNNs to learn from graph data is message passing. For each
    GNN layer, in addition to updating the layer’s weights, a “message” is gathered
    from every node or edge neighborhood and used to update an embedding. Essentially,
    each node sends messages to its neighbors and simultaneously receives messages
    from them. For every node, its new embedding is computed by combining its own
    features with the aggregated messages from its neighbors, through a combination
    of nonlinear transformations.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈GNNs特有的架构方面。激活和Dropout在许多深度学习场景中都很常见。然而，GNN层在本质上与传统的深度学习层不同。允许GNNs从图数据中学习的核心原理是消息传递。对于每个GNN层，除了更新层的权重外，还会从每个节点或边邻域收集一个“消息”，并用于更新一个嵌入。本质上，每个节点向其邻居发送消息，并同时从他们那里接收消息。对于每个节点，其新的嵌入是通过结合自己的特征和从邻居那里聚合的消息，通过非线性变换的组合来计算的。
- en: In this example, we’re going to be using a graph convolutional network (GCN)
    to act as our message-passing GNN layers. We describe GCNs in much more detail
    in chapter 3\. For now, you just need to know that GCNs act as message-passing
    layers that are critical in constructing embeddings.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用图卷积网络（GCN）作为我们的消息传递GNN层。我们在第3章中对GCNs进行了更详细的描述。现在，你只需要知道GCNs作为消息传递层，对于构建嵌入至关重要。
- en: Data preparation
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据准备
- en: Next, we prepare our data. We’ll start with the same graph from the previous
    section, `books_gml`, in its `NetworkX` form. We have to convert this `NetworkX`
    object into a tensor form that is suitable to use with PyTorch operations. Because
    PyTorch Geometric (PyG) has many functions that convert graph objects, we can
    do this quite simply with `data` `=` `from_NetworkX(gml_graph)`. Method `from_NetworkX`
    specifically translates the edge lists and node/edge attributes into PyTorch tensors.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们准备我们的数据。我们将从上一节中的相同图表开始，即`books_gml`，以它的`NetworkX`形式。我们必须将这个`NetworkX`对象转换成适合与PyTorch操作使用的张量形式。因为PyTorch
    Geometric (PyG)有许多将图对象转换的功能，我们可以通过`data = from_NetworkX(gml_graph)`这一步相当简单地完成。`from_NetworkX`方法特别将边列表和节点/边属性转换成PyTorch张量。
- en: For GNNs, generating node embeddings requires initializing node features. In
    our case, we don’t have any predefined node features. When no node features are
    available or they aren’t informative, it’s common practice to initialize the node
    features randomly. A more effective approach is to use *Xavier initialization*,
    which sets the initial node features with values drawn from a distribution that
    keeps the variety of activations consistent across layers. This technique ensures
    that the model starts with a balanced representation, preventing problems such
    as vanishing or exploding gradients.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GNNs，生成节点嵌入需要初始化节点特征。在我们的例子中，我们没有任何预定义的节点特征。当没有节点特征可用或它们不具信息性时，通常的做法是随机初始化节点特征。一种更有效的方法是使用*Xavier初始化*，该方法通过从保持激活多样性一致性的分布中抽取值来设置初始节点特征。这项技术确保模型从平衡的表示开始，防止梯度消失或爆炸等问题。
- en: 'By initializing `data.x` with Xavier initialization, we provide the GNN with
    a starting point that allows it to learn meaningful node embeddings from noninformative
    features. During training, the network adjusts these initial values to minimize
    the loss function. When the loss function is aligned with a specific target, such
    as node prediction, the embeddings learned from the initial random features will
    become tailored to the task at hand, resulting in more effective representations.
    We randomize the node features using the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Xavier 初始化 `data.x`，我们为 GNN 提供了一个起点，使其能够从非信息性特征中学习有意义的节点嵌入。在训练过程中，网络调整这些初始值以最小化损失函数。当损失函数与特定目标（如节点预测）对齐时，从初始随机特征学习到的嵌入将针对当前任务进行调整，从而产生更有效的表示。我们使用以下方法随机化节点特征：
- en: '[PRE4]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We could have also used the embeddings from the N2V exercise to use as node
    features. Recall the `node_embeddings` object from section 2.1.3:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 N2V 练习中的嵌入作为节点特征。回想一下 2.1.3 节中的 `node_embeddings` 对象：
- en: '[PRE5]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'From this, we can convert the node embedding to a PyTorch tensor object and
    assign it to the node feature object, `data.x`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果中，我们可以将节点嵌入转换为 PyTorch 张量对象，并将其分配给节点特征对象，`data.x`：
- en: '[PRE6]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Passing the graph through the GNN
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将图通过 GNN 传递
- en: With the structure of our GNN model defined and our graph data formatted for
    PyG, we proceed to the embedding generation step. We initialize our model, `SimpleGNN`,
    specifying the number of features for each node and the size of the hidden channels
    within the network.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 GNN 模型结构定义和图数据格式化为 PyG 后，我们进入嵌入生成步骤。我们初始化我们的模型，`SimpleGNN`，指定每个节点的特征数量和网络中隐藏通道的大小。
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we specify 64 hidden channels because we want to compare the resulting
    embeddings to the ones we produced using the `node2vec` method, which had 64 dimensions.
    Because the second GNN layer is the last layer, the output will be a 64-element
    vector.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定了 64 个隐藏通道，因为我们想将生成的嵌入与使用 `node2vec` 方法生成的嵌入进行比较，后者有 64 个维度。由于第二个 GNN
    层是最后一层，输出将是一个 64 元素的向量。
- en: Once initialized, we switch the model to evaluation mode using `model.eval()`.
    This mode is used during inference or validation phases when we want to make predictions
    or assess model performance without modifying the model’s parameters. Specifically,
    `model.eval()` turns off certain behaviors specific to training, such as *dropout*,
    which randomly deactivates some neurons to prevent overfitting, and *batch normaliza**tion*,
    which normalizes inputs across a mini-batch. By disabling these features, the
    model provides consistent and deterministic outputs, ensuring that the evaluation
    accurately reflects its true performance on unseen data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化完成后，我们使用 `model.eval()` 将模型切换到评估模式。在推理或验证阶段，当我们想要做出预测或评估模型性能而不修改模型参数时，使用此模式。具体来说，`model.eval()`
    关闭了特定于训练的某些行为，例如 *dropout*，它随机停用一些神经元以防止过拟合，以及 *批量归一化*，它对 mini-batch 中的输入进行归一化。通过禁用这些功能，模型提供一致且确定性的输出，确保评估能够准确反映其在未见数据上的真实性能。
- en: It’s important to disable gradient computations because they’re not necessary
    for the forward pass and embedding generation. So, we employ `torch.no_grad()`,
    which ensures that the computational graph that records operations for backpropagation
    isn’t constructed, preventing us from accidentally changing performance.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用梯度计算很重要，因为在正向传递和嵌入生成过程中它们不是必需的。因此，我们使用 `torch.no_grad()`，这确保了记录反向传播操作的计算图不会被构建，从而防止我们意外地改变性能。
- en: 'Next, we pass our node-feature matrix (`data.x)` and the edge index (`data.edge_
    index`) through the model. The result is `gnn_embeddings`, a tensor where each
    row corresponds to the embedding of a node in our graph—a numerical representation
    learned by our GNN, ready for downstream tasks such as visualization or classification:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将节点特征矩阵（`data.x`）和边索引（`data.edge_index`）通过模型传递。结果是 `gnn_embeddings`，一个张量，其中每一行对应于我们图中节点的嵌入——这是我们的
    GNN 学习到的数值表示，为下游任务（如可视化或分类）做好准备：
- en: '[PRE8]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After producing these embeddings, we use UMAP to visualize them, as we did
    in section 2.1.3\. Since we’ve been working with PyTorch tensor data types running
    on a GPU, we need to convert our embeddings to a NumPy array data type to use
    analysis methods outside of PyTorch, which are done on a CPU:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成这些嵌入后，我们使用 UMAP 来可视化它们，就像我们在 2.1.3 节中做的那样。由于我们一直在使用在 GPU 上运行的 PyTorch 张量数据类型，我们需要将我们的嵌入转换为
    NumPy 数组数据类型，以便使用 PyTorch 之外的 CPU 上的分析方法：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With this conversion, we can produce the UMAP calculations and visualization
    following the process we used in the N2V case. The resulting scatterplot (figure
    2.8) is a first glimpse at the clusters within our graph. We add different shadings
    based on each node’s label (left-, right-, or neutral-leaning) to see that similar
    leaning books are fairly well grouped, given that these embeddings were constructed
    from topology alone.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种转换，我们可以按照在N2V案例中使用的流程生成UMAP计算和可视化。得到的散点图（图2.8）是我们图中集群的第一印象。我们根据每个节点的标签（左倾、右倾或中立）添加不同的阴影，以显示相似倾向的书籍被相当好地分组，因为这些嵌入仅从拓扑结构构建。
- en: Next, let’s discuss both how GNN embeddings are used and how they differ from
    those produced with N2V.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论GNN嵌入的使用方式以及它们与N2V生成的嵌入的不同之处。
- en: '![figure](../Images/2-8.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-8.png)'
- en: Figure 2.8 Visualization of embeddings generated from passing a graph through
    a GNN
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8 通过GNN传递图生成的嵌入可视化
- en: 2.2.2 GNN vs. N2V embeddings
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 GNN与N2V嵌入的比较
- en: Throughout this book, we predominantly use GNNs to generate embeddings because
    this embedding process is intrinsic to a GNN’s architecture. While embeddings
    play a pivotal role in the methodologies and applications we explore in the rest
    of the book, their presence is often subtle and not always highlighted. This approach
    allows us to focus on the broader concepts and applications of GNN-based machine
    learning without getting slowed down by the technicalities. Nonetheless, it’s
    important to acknowledge that the underlying power and adaptability of embeddings
    are central to the advanced techniques and insights we get into throughout the
    text.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们主要使用GNN来生成嵌入，因为嵌入过程是GNN架构固有的。虽然嵌入在我们探索的方法和应用的其余部分中扮演着关键角色，但它们的呈现往往是微妙的，并不总是被强调。这种方法使我们能够专注于GNN机器学习的更广泛概念和应用，而不会因为技术细节而减慢速度。尽管如此，承认嵌入的潜在力量和适应性是我们全文中获得的先进技术和洞察力的核心。
- en: GNN-produced node embeddings are particularly powerful because they enable us
    to tackle a broad range of graph-related tasks by using their inductive nature.
    Inductive learning allows these embeddings to generalize to new, unseen nodes
    or even entirely new graphs without needing to retrain the model. In contrast,
    N2V embeddings are limited to the specific graphs they were trained on and can’t
    easily adapt to new data. Let’s reiterate the key ways in which GNN embeddings
    differ from other embedding methods, such as N2V [1, 3].
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: GNN生成的节点嵌入特别强大，因为它们能够通过其归纳性质，使我们能够通过使用它们来解决广泛的图相关任务。归纳学习使得这些嵌入能够推广到新的、未见的节点，甚至完全新的图，而无需重新训练模型。相比之下，N2V嵌入仅限于它们被训练的特定图，并且难以适应新数据。让我们重申GNN嵌入与其他嵌入方法（如N2V
    [1, 3]）不同的关键方式。
- en: Adaptability to new graphs
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 适应新图的能力
- en: One of the critical features of GNN embeddings is their adaptability. Because
    GNNs learn a function that maps node features to embeddings, this function can
    be applied to nodes in new graphs without needing to be retrained, provided the
    nodes have similar feature spaces. This inductive capability is particularly valuable
    in dynamic environments where the graph may evolve over time or in applications
    where the model needs to be applied to different but structurally similar graphs.
    N2V, on the other hand, needs to be reapplied for each new graph or set of nodes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: GNN嵌入的一个关键特性是其适应性。因为GNN学习一个将节点特征映射到嵌入的函数，所以这个函数可以应用于新图中的节点，而无需重新训练，前提是节点具有相似的特征空间。这种归纳能力在图可能随时间演变或模型需要应用于不同但结构相似的图的应用中特别有价值。另一方面，N2V需要为每个新的图或节点集重新应用。
- en: Enhanced feature integration
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 增强的特征集成
- en: GNNs inherently consider node features during the embedding process, allowing
    for a complex and nuanced representation of each node. This integration of node
    features, alongside the structural information, offers a more comprehensive view
    compared to N2V and other methods that focus on a graph’s topology. This capability
    makes GNN embeddings particularly suited for tasks where node features contain
    significant additional information.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs（图神经网络）在嵌入过程中天生考虑节点特征，这使得每个节点都能有复杂和细腻的表现。这种节点特征与结构信息的结合，比N2V和其他仅关注图拓扑的方法提供了更全面的视角。这种能力使得GNN嵌入特别适合于节点特征包含大量额外信息的任务。
- en: Task-specific optimization
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务特定优化
- en: GNN embeddings are trained alongside specific tasks, such as node classification,
    link prediction, or even graph classification. Through end-to-end training, the
    GNN model learns to optimize the embeddings for the task at hand, leading to potentially
    higher performance and efficiency compared to using pre-generated embeddings such
    as those from N2V.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: GNN嵌入与特定任务（如节点分类、链接预测甚至图分类）一起训练。通过端到端训练，GNN模型学会优化嵌入以适应当前任务，与使用如N2V生成的预生成嵌入相比，可能带来更高的性能和效率。
- en: That said, while GNN embeddings offer clear advantages in terms of adaptability
    and applicability to new data, N2V embeddings have their strengths, particularly
    in capturing nuanced patterns within a specific graph’s structure. In practice,
    the choice between GNN and N2V embeddings may depend on the specific requirements
    of the task, the nature of the graph data, and the constraints of the computational
    environment.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，虽然GNN嵌入在适应性和对新数据的适用性方面具有明显优势，但N2V嵌入也有其优势，尤其是在捕捉特定图结构中的细微模式方面。在实践中，GNN和N2V嵌入之间的选择可能取决于任务的特定要求、图数据的性质以及计算环境的限制。
- en: For tasks where the graph structure is static and well-defined, N2V might provide
    a simpler and computationally efficient solution. Conversely, for dynamic graphs,
    large-scale applications, or scenarios requiring the incorporation of node features,
    GNNs will often be the more robust and versatile choice. Additionally, when the
    task itself is not well-defined and the work is exploratory, N2V is likely faster
    and easier to use.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些图结构静态且定义明确的任务，N2V可能提供了一种更简单且计算效率更高的解决方案。相反，对于动态图、大规模应用或需要结合节点特征的场景，GNN通常会是更稳健和通用的选择。此外，当任务本身定义不明确且工作具有探索性时，N2V可能更快且更容易使用。
- en: We’ve now successfully built our first GNN embedding. This is the key first
    step for all GNN models, and everything from this point will build on it. In the
    next section, we give an example of some of these next steps and show how to use
    embeddings to solve a machine learning problem.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经成功构建了第一个GNN嵌入。这是所有GNN模型的关键第一步，从这一点开始，所有后续步骤都将基于它。在下一节中，我们将给出一些后续步骤的示例，并展示如何使用嵌入来解决机器学习问题。
- en: 2.3 Using node embeddings
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 使用节点嵌入
- en: Semi-supervised learning, which involves a combination of labeled and unlabeled
    data, provides a valuable opportunity to compare different embedding techniques.
    In this chapter, we’ll explore how GNN and N2V embeddings can be used to predict
    labels when the majority of the data lacks labels.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习，涉及标记数据和未标记数据的组合，为比较不同的嵌入技术提供了宝贵的机会。在本章中，我们将探讨如何使用GNN和N2V嵌入来预测标签，当大多数数据缺乏标签时。
- en: Our task involves the Political Books dataset (`books_graph`), where nodes represent
    political books and edges indicate co-purchase relationships. To make the process
    clearer, let’s review the steps taken so far and outline our next steps, as illustrated
    in figure 2.9.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务涉及政治书籍数据集（`books_graph`），其中节点代表政治书籍，边表示共同购买关系。为了使过程更清晰，让我们回顾迄今为止采取的步骤，并概述我们的下一步计划，如图2.9所示。
- en: '![figure](../Images/2-9.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-9.png)'
- en: Figure 2.9 Overview of steps taken in chapter 2
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9 第2章采取的步骤概述
- en: We began with the `books_graph` dataset in graph format and performed light
    preprocessing to prepare the data for embedding. For N2V, this involved converting
    the dataset from a .gml file to a `NetworkX` format. For the GNN-based embeddings,
    we converted the `NetworkX` graph into a PyTorch tensor and initialized the node
    features using Xavier initialization to ensure balanced variability across layers.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`books_graph`数据集开始，以图格式进行轻量级预处理，为嵌入准备数据。对于N2V，这包括将数据集从.gml文件转换为`NetworkX`格式。对于基于GNN的嵌入，我们将`NetworkX`图转换为PyTorch张量，并使用Xavier初始化来初始化节点特征，以确保层间变异性平衡。
- en: After preparing the data, we generated embeddings using both N2V and GCNs. Now,
    in this section, we’ll apply these embeddings to a semi-supervised classification
    problem. This involves further processing to define the classification task, where
    only 20% of the book labels are retained, simulating a realistic scenario with
    sparse labeled data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备数据后，我们使用N2V和GCN生成了嵌入。现在，在本节中，我们将将这些嵌入应用于半监督分类问题。这涉及到进一步处理以定义分类任务，其中仅保留了20%的书籍标签，模拟了一个具有稀疏标记数据的现实场景。
- en: 'We’ll use the two sets of embeddings (N2V and GCN) with two different classifiers:
    a random forest classifier (to use the embeddings as tabular features) and a GCN
    classifier (to use the graph structure and node features). The goal is to predict
    the political orientation of the books, with the remaining 80% of the labels inferred
    based on the given embeddings.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用两组嵌入（N2V和GCN）和两种不同的分类器：一个随机森林分类器（将嵌入用作表格特征）和一个GCN分类器（使用图结构和节点特征）。目标是预测书籍的政治倾向，其余80%的标签基于给定的嵌入进行推断。
- en: 2.3.1 Data preprocessing
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 数据预处理
- en: To start, we do a little more preprocessing to our `books_gml` dataset (see
    listing 2.5). We must format the labels in a suitable way for the learning process.
    Because all the nodes are labeled, we also have to set up the semi-supervised
    problem by randomly selecting the nodes from which we hide the labels.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们对`books_gml`数据集进行一些额外的预处理（见列表2.5）。我们必须以适合学习过程的方式格式化标签。因为所有节点都有标签，我们还必须通过随机选择节点来设置半监督问题，从这些节点中隐藏标签。
- en: Nodes associated with attribute `'c'` are classified as `'right'`, while those
    with `'l'` are classified as `'left'`. Nodes that don’t fit these criteria, including
    those with neutral or unspecified attributes, are categorized as `'neutral'`.
    These classifications are then placed into a NumPy array, `labels`, for optimized
    computational handling.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与属性`'c'`关联的节点被分类为`'right'`，而与`'l'`关联的节点被分类为`'left'`。不符合这些标准的节点，包括具有中性或未指定属性的节点，被归类为`'neutral'`。然后，将这些分类放入一个NumPy数组`labels`中，以便于优化的计算处理。
- en: Then, an array, `indices`, is created, representing the positional indexes of
    all nodes within the dataset. A subset of these indices, corresponding to 20%
    of the total node count, is designated as our labeled data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，创建了一个名为`indices`的数组，表示数据集中所有节点的位置索引。这些索引的一个子集，对应于总节点数的20%，被指定为我们标记的数据。
- en: To manage the labeled and unlabeled data, Boolean masks, `labelled_mask` and
    `unlabelled_mask`, are initialized and populated. The `labelled_mask` is set to
    `True` for indices selected as labeled; these are the ground truth labels for
    corresponding nodes. Similarly, `unlabelled_mask` is set to `False`. These masks
    segment the dataset for training and evaluation, ensuring that algorithms are
    correctly trained and validated on the correct subsets of data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了管理标记和未标记的数据，初始化并填充了布尔掩码`labelled_mask`和`unlabelled_mask`。`labelled_mask`对于选为标记的索引设置为`True`；这些是相应节点的真实标签。同样，`unlabelled_mask`设置为`False`。这些掩码将数据集分割为训练和评估部分，确保算法在正确的数据子集上正确训练和验证。
- en: Listing 2.5 Preprocessing for semi-supervised problem
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.5 半监督问题的预处理
- en: '[PRE10]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Extracts labels and handles neutral values'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 提取标签并处理中性值'
- en: '#2 Random seed for reproducibility'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 用于可重复性的随机种子'
- en: '#3 Indices of all nodes'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 所有节点的索引'
- en: '#4 20% of data to keep as labeled'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 保留20%的数据作为标记数据'
- en: '#5 Selects a subset of indices to remain labeled'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 选择保留标记的索引子集'
- en: '#6 Initializes masks for labeled and unlabeled data'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 初始化标签数据和未标记数据的掩码'
- en: '#7 Updates masks'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 更新掩码'
- en: '#8 Uses masks to split the dataset'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 使用掩码分割数据集'
- en: '#9 Transformed labels to numerical form'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 将标签转换为数值形式'
- en: Now we transform the data for model training, as shown in listing 2.6\. For
    the GNN-derived embeddings, `X_train_gnn` and `y_train_gnn` are assigned arrays
    of embeddings and corresponding numeric labels filtered by a `labelled_mask`.
    This mask is a Boolean array indicating which nodes in the graph are part of the
    labeled subset, ensuring that only data points with known labels are included
    in the training set.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对模型训练数据进行转换，如列表2.6所示。对于由GNN生成的嵌入，`X_train_gnn`和`y_train_gnn`被分配了嵌入数组和相应的数字标签，这些标签通过一个`labelled_mask`进行过滤。这个掩码是一个布尔数组，指示图中哪些节点是标签子集的一部分，确保只有具有已知标签的数据点包含在训练集中。
- en: For N2V embeddings, a similar approach is adopted with an added preprocessing
    step to align the embeddings with their corresponding labels. The embeddings for
    each node are aggregated into NumPy array `X_n2v` in the same order as the nodes
    appear in the `books_graph`. This ensures consistency between the embeddings and
    their labels, a crucial step for supervised learning tasks. Subsequently, `X_train_n2v`
    and `y_train_n2v` are populated with N2V embeddings and labels, again applying
    the `labelled_mask` to filter for the labeled data points.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于N2V嵌入，采用类似的方法，并添加预处理步骤以对齐嵌入与其对应的标签。每个节点的嵌入按`books_graph`中节点出现的顺序聚合到NumPy数组`X_n2v`中。这确保了嵌入与其标签之间的一致性，对于监督学习任务是一个关键步骤。随后，`X_train_n2v`和`y_train_n2v`被填充了N2V嵌入和标签，再次应用`labelled_mask`来过滤标记数据点。
- en: 'Listing 2.6 Preprocessing: constructing training data'
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.6 预处理：构建训练数据
- en: '[PRE11]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 For GNN embeddings'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用于GNN嵌入'
- en: '#2 For N2V embeddings'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 用于N2V嵌入'
- en: '#3 Ensures N2V embeddings are in the same order as labels'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 确保N2V嵌入与标签顺序相同'
- en: The extra alignment step for the N2V embeddings isn’t necessary for the GNN
    embeddings because GNN models inherently maintain the order of nodes as they process
    the entire graph in a structured manner. As a result, the output embeddings from
    a GNN are naturally ordered in correspondence with the input graph’s node order.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于N2V嵌入，不需要额外的对齐步骤，因为GNN模型在以结构化方式处理整个图时，内在地维护了节点的顺序。因此，GNN的输出嵌入自然地按照输入图的节点顺序排序。
- en: In contrast, N2V generates embeddings through independent random walks starting
    from each node, and the order of the resulting embeddings doesn’t necessarily
    match the order of nodes in the original graph data structure. Therefore, an explicit
    alignment step is required to ensure that each N2V embedding is correctly associated
    with its corresponding label, as extracted from the graph. This step is critical
    for supervised learning tasks where the correct matching of features (embeddings)
    to labels is essential for model training and evaluation. For this task, we use
    the attribute `index_to_key`, which contains the identifiers of the nodes in the
    order they are processed and stored within the model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，N2V通过从每个节点开始的独立随机游走来生成嵌入，生成的嵌入的顺序不一定与原始图数据结构中节点的顺序相匹配。因此，需要一个显式的对齐步骤来确保每个N2V嵌入与其从图中提取的对应标签正确关联。这一步骤对于监督学习任务至关重要，在这些任务中，将特征（嵌入）与标签的正确匹配对于模型训练和评估至关重要。对于这个任务，我们使用属性`index_to_key`，它包含节点标识符，按照它们在模型中处理和存储的顺序排列。
- en: 2.3.2 Random forest classification
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 随机森林分类
- en: With our data prepped, we use GNN and N2V embeddings from sections 2.1 and 2.2
    as input features for a `RandomForestClassifier`, as shown in listing 2.7\.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据准备就绪后，我们使用第2.1节和第2.2节中的GNN和N2V嵌入作为`RandomForestClassifier`的输入特征，如列表2.7所示。
- en: 'Listing 2.7 Preprocessing: constructing training data'
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.7 预处理：构建训练数据
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Classifier for GNN embeddings'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 GNN嵌入的分类器'
- en: '#2 Classifier for N2V embeddings'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 N2V嵌入的分类器'
- en: This approach allows us to directly compare the embeddings’ predictive power,
    where we compare results in table 2.2.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使我们能够直接比较嵌入的预测能力，如表2.2所示。
- en: Table 2.2 Classification performance
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.2 分类性能
- en: '| Embedding Type | Accuracy | F1 Score |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入类型 | 准确率 | F1分数 |'
- en: '| --- | --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GNN  | 83.33%  | 82.01%  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| GNN  | 83.33%  | 82.01%  |'
- en: '| N2V  | 84.52%  | 80.72%  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| N2V  | 84.52%  | 80.72%  |'
- en: 'For this basic classification exercise, we’ll evaluate the performance of our
    models using two fundamental metrics:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个基本的分类练习，我们将使用两个基本指标来评估我们模型的性能：
- en: '*Accuracy —*This metric measures the proportion of correct predictions made
    by the model out of all predictions. It provides a straightforward assessment
    of how often the classifier correctly identifies the political orientation of
    the books. For instance, an accuracy of 84.52% means that the model correctly
    predicted the orientation of the books approximately 85 times out of 100\.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*准确率*—这个指标衡量模型在所有预测中正确预测的比例。它提供了一个直接的评估，即分类器正确识别书籍政治倾向的频率。例如，准确率为84.52%意味着模型在大约100次中有85次正确预测了书籍的倾向。'
- en: '*F1 score*—This is a more nuanced metric that balances precision and recall,
    which is particularly useful in cases where the data is imbalanced—meaning the
    classes aren’t equally represented. It provides a harmonic mean of precision (the
    number of true positive predictions divided by the total number of positive predictions)
    and recall (the number of true positive predictions divided by the total number
    of actual positives). A higher F1 score indicates a model’s robust performance
    in correctly identifying both the presence and absence of the different classes,
    minimizing both false positives and false negatives.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F1分数*—这是一个更细微的指标，它平衡了精确度和召回率，在数据不平衡的情况下尤其有用——这意味着类别并不均匀地表示。它提供了精确度（真实正预测数除以总正预测数）和召回率（真实正预测数除以实际正总数）的调和平均值。更高的F1分数表明模型在正确识别不同类别的存在和不存在方面表现出稳健的性能，同时最小化错误正例和错误负例。'
- en: 'The performance metrics reveal that N2V embeddings yield a slightly higher
    accuracy of 84.52% when used within a `RandomForestClassifier`, compared to 83.33%
    for GNN embeddings. However, GNN embeddings achieve a marginally better F1 score
    of 82.01%, compared to 80.72% for N2V embeddings. This nuanced difference underscores
    potential tradeoffs between the two embedding types: while N2V provides slightly
    better overall prediction accuracy, GNN embeddings may offer a more balanced performance
    across both the majority and minority classes.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标显示，当在`RandomForestClassifier`中使用时，N2V嵌入产生了略高的准确率84.52%，而GNN嵌入为83.33%。然而，GNN嵌入实现了略好的F1分数82.01%，而N2V嵌入为80.72%。这种细微的差异强调了两种嵌入类型之间的潜在权衡：虽然N2V提供了略好的整体预测准确率，但GNN嵌入可能在多数和少数类别上提供更平衡的性能。
- en: In general, the inductive nature of GNNs presents a robust framework for learning
    node representations for graphs of many different sizes. Even on smaller graphs,
    GNNs can effectively learn the underlying patterns and interactions between nodes,
    as evidenced by the higher F1 score, indicating a better balance between precision
    and recall in classification tasks.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，GNN的归纳性质为学习不同大小图的网络节点表示提供了一个稳健的框架。即使在较小的图上，GNN也能有效地学习节点之间的底层模式和交互，这从更高的F1分数中可以看出，这表明在分类任务中精确度和召回率之间的平衡更好。
- en: In this context, the choice between GNN and N2V embeddings might also hinge
    on the specific goals of the analysis and the performance metrics of greatest
    interest. If the priority is achieving the highest possible accuracy and the dataset
    is unlikely to expand significantly, N2V could be the more suitable option. Conversely,
    if the task values a balance between precision and recall and there’s potential
    for applying the learned model to similar but new graphs, GNNs offer valuable
    flexibility and robustness, even for smaller datasets. Having used the N2V and
    GNN embeddings as inputs to a random forest model, let’s next study what happens
    when we use them as inputs to a full end-to-end GNN model.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，GNN和N2V嵌入的选择也可能取决于分析的具体目标和最感兴趣的性能指标。如果优先考虑实现尽可能高的准确率，并且数据集不太可能显著扩展，那么N2V可能是更合适的选择。相反，如果任务重视精确度和召回率之间的平衡，并且有将学习到的模型应用于类似但新的图的可能性，那么GNN提供了宝贵的灵活性和稳健性，即使对于较小的数据集也是如此。在将N2V和GNN嵌入用作随机森林模型的输入之后，接下来让我们研究当我们将它们用作全端到端GNN模型的输入时会发生什么。
- en: 2.3.3 Embeddings in an end-to-end model
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 全端到端模型中的嵌入
- en: In the previous section, we used GNN and N2V embeddings as static inputs to
    a traditional machine learning model, namely a random forest classifier. Here,
    we use an end-to-end GNN model applied to the same problem of label prediction.
    By *end-to-end*, we mean that the embeddings will be generated while we also predict
    labels. This means that the embeddings here won’t be static because, as the GNN
    learns, it will update the node embeddings.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了GNN和N2V嵌入作为静态输入到传统的机器学习模型中，即随机森林分类器。在这里，我们使用一个端到端的GNN模型来解决相同的标签预测问题。通过“端到端”，我们指的是在预测标签的同时生成嵌入。这意味着这里的嵌入不会是静态的，因为随着GNN的学习，它将更新节点嵌入。
- en: 'To build this model, we’ll use the same tools as before—the `books_gml` dataset,
    and the `SimpleGNN` architecture. We’ll change the GNN slightly, by adding a log
    `softmax` activation at the end, to facilitate the output for a three-label classification
    problem. We’ll also slightly modify the output of our `SimpleGNN` class, allowing
    us to observe the embeddings as well as the predictive output. Our process includes
    the following:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建这个模型，我们将使用之前相同的工具——`books_gml` 数据集和 `SimpleGNN` 架构。我们将稍微修改 GNN，通过在末尾添加 log
    `softmax` 激活，以方便对三标签分类问题的输出。我们还将稍微修改 `SimpleGNN` 类的输出，使我们能够观察到嵌入以及预测输出。我们的过程包括以下内容：
- en: Data prep
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Model/architecture modification
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型/架构修改
- en: Establish the training loop
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立训练循环
- en: Study performance
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究性能
- en: Study embeddings pre-training and post-training
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究嵌入预训练和训练后
- en: Data prep
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Assuming we use the `books_gml` data set, the process to transform it for use
    within the PyG framework remains the same. We’ll train two versions of the data:
    one with node features initialized randomly, and one with node features using
    the N2V embeddings.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用 `books_gml` 数据集，将其转换为 PyG 框架内使用的进程保持不变。我们将训练两种版本的数据：一种使用随机初始化的节点特征，另一种使用
    N2V 嵌入的节点特征。
- en: Model modification
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型修改
- en: We use the same `SimpleGNN` class with modifications. First, in this enhanced
    version of the `SimpleGNN` class, we extend its functionality to provide a predictive
    output for each node. This is achieved by applying a log `softmax` activation
    to the embeddings produced by the second GCN layer. The log `softmax` output provides
    a normalized log probability distribution over the potential classes for each
    node for the classification task.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用相同的 `SimpleGNN` 类及其修改。首先，在这个 `SimpleGNN` 类的增强版本中，我们扩展了其功能，为每个节点提供预测输出。这是通过将第二个
    GCN 层产生的嵌入应用 log `softmax` 激活来实现的。log `softmax` 输出为每个节点的潜在类别提供了一个归一化的对数概率分布，用于分类任务。
- en: 'Second, we introduce dual outputs. The method returns two values: the raw embeddings
    from the `conv2` layer, which capture the node representations, and the log `softmax`
    of these embeddings. For us to observe both the embedding and the predictions,
    we have the `forward` method return both. In addition to this two-layer model,
    we added two layers to this architecture to have a four-layer model for comparison,
    as shown in listing 2.8.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们引入双重输出。该方法返回两个值：来自 `conv2` 层的原始嵌入，它捕获节点表示，以及这些嵌入的 log `softmax`。为了观察嵌入和预测，我们将
    `forward` 方法返回两者。除了这个双层模型之外，我们还添加了两个层到这个架构中，以形成一个四层模型进行比较，如列表 2.8 所示。
- en: 'Listing 2.8 Preprocessing: Constructing training data'
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.8 预处理：构建训练数据
- en: '[PRE13]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Predicts classes by passing the final conv layer through a log softmax'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 通过将最终卷积层通过 log softmax 预测类别'
- en: '#2 The class returns both the last embedding and the prediction.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 该类返回最后一个嵌入和预测。'
- en: Establish the training loop
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 建立训练循环
- en: 'We program the training loop for the GNN model in a semi-supervised learning
    context, as shown in listing 2.9\. This loop iterates over a specified number
    of epochs, where an epoch represents a complete pass through the entire training
    dataset. Within each epoch, the model’s parameters are updated to minimize a loss
    function, which quantifies the difference between the predicted outputs and the
    actual labels for the nodes in the training set. For those familiar with programming
    deep learning training loops, this should be very familiar. For those who could
    do with a quick reminder, the following describes some of the key steps in initializing
    and running the training loop:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在半监督学习环境中为 GNN 模型编写训练循环，如列表 2.9 所示。这个循环遍历指定数量的 epoch，其中 epoch 代表整个训练数据集的一次完整遍历。在每个
    epoch 中，模型的参数被更新以最小化损失函数，该函数量化了预测输出与训练集中节点实际标签之间的差异。对于那些熟悉深度学习训练循环编程的人来说，这应该非常熟悉。对于那些需要快速提醒的人来说，以下描述了初始化和运行训练循环的一些关键步骤：
- en: '*Optimizer initialization*—The optimizer is initialized with a specific learning
    rate when it’s created. For example, here we use the Adam optimizer, with an initial
    learning rate of 0.01\.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器初始化*—当创建优化器时，它会使用一个特定的学习率进行初始化。例如，在这里我们使用 Adam 优化器，初始学习率为 0.01。'
- en: '*Zeroing the gradients*—`optimizer.zero_grad()` ensures that the gradients
    are reset before each update, preventing them from accumulating across epochs.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*归零梯度*—`optimizer.zero_grad()` 确保在每次更新之前重置梯度，防止它们在 epoch 之间累积。'
- en: '*Model forward pass*—The model processes the node features (`data.x`) and the
    graph structure (`data.edge_index`) to produce output predictions. In semi-supervised
    settings, not all nodes have labels, so the model’s output includes predictions
    for both labeled and unlabeled nodes.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型前向传播*—模型处理节点特征（`data.x`）和图结构（`data.edge_index`）以产生输出预测。在半监督设置中，并非所有节点都有标签，因此模型的输出包括对标记节点和无标记节点的预测。'
- en: '*Applying the training mask*—`out_masked` `=` `out[data.train_mask]` applies
    a mask to the model’s output to select only the predictions corresponding to labeled
    nodes. This is crucial in semi-supervised learning, where only a subset of nodes
    has known labels.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*应用训练掩码*—`out_masked` `=` `out[data.train_mask]` 对模型的输出应用掩码，以仅选择对应于标记节点的预测。这在半监督学习中至关重要，因为在半监督学习中，只有一部分节点具有已知的标签。'
- en: '*Loss computation and backpropagation*—Loss function `loss_fn` compares the
    selected predictions (`out_masked`) with the true labels of the labeled nodes
    (`train_labels`). The `loss.backward()` call computes the gradient of the loss
    function with respect to the model parameters, which is then used to update these
    parameters via `optimizer.step()`.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失计算和反向传播*—损失函数 `loss_fn` 将选定的预测（`out_masked`）与标记节点的真实标签（`train_labels`）进行比较。`loss.backward()`
    调用计算损失函数相对于模型参数的梯度，然后通过 `optimizer.step()` 更新这些参数。'
- en: '*Logging*—The training loop prints the loss at regular intervals (every 10
    epochs in this case) to monitor the training progress.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*日志记录*—训练循环在固定间隔（在这种情况下为每 10 轮）打印损失，以监控训练进度。'
- en: Listing 2.9 Training loop
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.9 训练循环
- en: '[PRE14]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Number of epochs'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 训练轮数'
- en: '#2 Passes both node features and edge_index to the model'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将节点特征和 edge_index 传递给模型'
- en: '#3 Applies the training mask to select only the outputs for the labeled nodes'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 应用训练掩码以仅选择标记节点的输出'
- en: '#4 Computes the loss using only the labeled nodes'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 仅使用标记节点计算损失'
- en: '#5 Prints the loss every 10 epochs'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 每 10 轮打印一次损失'
- en: This process iteratively refines the model’s parameters to improve its predictions
    on the labeled portion of the dataset, with the goal of learning a model that
    can generalize well to the unlabeled nodes and potentially to new, unseen data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程迭代地细化模型的参数，以提高其在数据集标记部分上的预测能力，目标是学习一个能够很好地泛化到无标记节点，甚至可能到新的、未见过的数据的模型。
- en: 'GNN results: Randomized vs. N2V node features'
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GNN 结果：随机化与 N2V 节点特征
- en: Let’s compare the classification task comparing the GNN performance from the
    randomized node features versus the N2V node features, as shown in table 2.3.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较分类任务，比较随机节点特征与 N2V 节点特征的 GNN 性能，如表 2.3 所示。
- en: '**Table 2.3 Classification performance of the GNN model where the input graph
    uses different node features**'
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**表 2.3 使用不同节点特征的 GNN 模型的分类性能**'
- en: '| Model | GNN Accuracy | GNN F1 Score |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | GNN 准确率 | GNN F1 分数 |'
- en: '| --- | --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Two-layer, randomized features  | 82.27%  | 82.14%  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 双层，随机化特征 | 82.27% | 82.14% |'
- en: '| Two-layer, N2V features  | 87.79%  | 88.10%  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 双层，N2V 特征 | 87.79% | 88.10% |'
- en: '| Four-layer, randomized features  | 86.58%  | 86.90%  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 四层，随机化特征 | 86.58% | 86.90% |'
- en: '| Four-layer, N2V features  | 88.99%  | 89.29%  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 四层，N2V 特征 | 88.99% | 89.29% |'
- en: The table summarizes the performance of different GNN models based on their
    accuracy and F1 score. It highlights that GNNs using N2V features consistently
    outperform those using randomized features across all model configurations. Specifically,
    the four-layer GNN with N2V features achieves the highest accuracy and F1 score,
    indicating the effectiveness of incorporating meaningful node representations
    derived from N2V embeddings. If we had more information about specific features
    for the node, as we do in chapter 3, the GNN embeddings may further improve accuracy
    for the GNN model.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 表格总结了不同 GNN 模型的性能，基于它们的准确率和 F1 分数。它突出了使用 N2V 特征的 GNN 在所有模型配置中一致优于使用随机化特征。具体来说，具有
    N2V 特征的四层 GNN 实现了最高的准确率和 F1 分数，这表明从 N2V 嵌入中提取的有意义节点表示的有效性。如果我们对节点具有更多具体信息，就像我们在第
    3 章中所做的那样，GNN 嵌入可能会进一步提高 GNN 模型的准确率。
- en: 'Results: GNN vs. random forest'
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果：GNN 与随机森林
- en: We now compare the performance of the GNN model from this section with the random
    forest model from the previous section (see table 2.4).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在比较本节中 GNN 模型的性能与上一节中随机森林模型的性能（见表 2.4）。
- en: '**Table 2.4 Comparison of classification performance between the GNN model
    and the random forest model**'
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**表 2.4 GNN 模型和随机森林模型分类性能的比较**'
- en: '| Model | Data Input | Accuracy | F1 Score |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 数据输入 | 准确率 | F1分数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Random forest  | Embedding from GNN  | 83.33%  | 82.01%  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | GNN嵌入 | 83.33% | 82.01% |'
- en: '| Random forest  | Embedding from N2V  | 84.52%  | 80.72%  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | N2V嵌入 | 84.52% | 80.72% |'
- en: '| Two-layer simple GNN  | Graph with randomized node features  | 82.27%  |
    82.14%  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 两层简单GNN | 具有随机节点特征的图 | 82.27% | 82.14% |'
- en: '| Two-layer simple GNN  | Graph with n2v embeddings as node features  | 87.79%  |
    88.10%  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 两层简单GNN | 使用n2v嵌入作为节点特征的图 | 87.79% | 88.10% |'
- en: '| Four-layer simple GNN  | Graph with randomized node features  | 86.58%  |
    86.90%  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 四层简单GNN | 随机节点特征的图 | 86.58% | 86.90% |'
- en: '| Four-layer simple GNN  | Graph with n2v embeddings as node features  | 88.99%  |
    89.29%  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 四层简单GNN | 使用n2v嵌入作为节点特征的图 | 88.99% | 89.29% |'
- en: Figure 2.10 visualizes the results from table 2.4\. Overall, the GNN models
    outperformed the random forest models.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10可视化表2.4的结果。总体而言，GNN模型优于随机森林模型。
- en: '![figure](../Images/2-10.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-10.png)'
- en: 'Figure 2.10 Chart comparing the classification performance of the random forest
    with the GNN. Only one GNN model is outperformed by the random forest: The two-layer
    model trained on graph data with randomized node features is outperformed in terms
    of accuracy.'
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10 比较随机森林与GNN分类性能的图表。只有一个GNN模型被随机森林超越：在具有随机节点特征的图数据上训练的两层模型在准确率方面被超越。
- en: When comparing the performance of the GNN models with the random forest models,
    we can make several observations. Random forest, when trained on embeddings derived
    from GNN pass-through or N2V embeddings, achieves comparable accuracy to the two-layer
    simple GNN model. However, when considering the F1 score, both GNN models outperform
    random forest. Notably, the four-layer simple GNN model, especially when using
    N2V embeddings as features, exhibits significantly better performance than the
    random forest model, showcasing higher accuracy and F1 score.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较GNN模型与随机森林模型的性能时，我们可以得出几个观察结果。当在由GNN透传或N2V嵌入得到的嵌入上进行训练时，随机森林达到与两层简单GNN模型相当的准确率。然而，在考虑F1分数时，两个GNN模型都优于随机森林。值得注意的是，四层简单GNN模型，尤其是在使用N2V嵌入作为特征时，比随机森林模型表现出显著更好的性能，展示了更高的准确率和F1分数。
- en: This indicates that while random forest may outperform simpler GNN architectures
    such as the two-layer model in terms of accuracy, the more complex GNN architectures
    demonstrate superior performance in terms of F1 score, especially when using sophisticated
    node embeddings such as N2V. Therefore, the choice between random forest and GNN
    should consider both accuracy and F1 score, as well as the complexity of the model
    architecture and the nature of the input features, to achieve optimal performance
    for the given task and dataset.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，虽然随机森林在准确率方面可能优于简单的GNN架构，如两层模型，但在F1分数方面，更复杂的GNN架构表现出更优越的性能，尤其是在使用N2V等复杂的节点嵌入时。因此，在随机森林和GNN之间进行选择时，应考虑准确率、F1分数以及模型架构的复杂性和输入特征的性质，以实现给定任务和数据集的最佳性能。
- en: It’s important to note that this short example didn’t extensively fine-tune
    either the GNN or the random forest models. Further optimization of both types
    of models could potentially lead to significant improvements in their performance.
    Fine-tuning hyperparameters, adjusting model architectures, and optimizing training
    processes could all contribute to enhancing the accuracy and F1 score of both
    GNNs and random forest classifiers. Therefore, while the results presented here
    provide starting insights into the performance on a small graph dataset, we suggest
    you try out the models and experiment with performance.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这个简短的例子并没有对GNN或随机森林模型进行广泛的微调。对这两种类型的模型进行进一步的优化可能会显著提高它们的性能。微调超参数、调整模型架构和优化训练过程都可以有助于提高GNN和随机森林分类器的准确率和F1分数。因此，虽然这里呈现的结果为小图数据集上的性能提供了初步的见解，我们建议您尝试这些模型并实验其性能。
- en: 2.4 Under the Hood
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 内部结构
- en: This section gets deeper into the theoretical foundations of graph representations
    and embeddings, particularly in the context of GNNs. It emphasizes the importance
    of embeddings in transforming complex graph data into lower-dimensional, manageable
    forms that retain essential information.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 本节深入探讨了图表示和嵌入的理论基础，特别是在GNN的背景下。它强调了嵌入在将复杂的图数据转换为低维、可管理的形式并保留关键信息方面的重要性。
- en: 'We distinguish between two primary types of learning: transductive and inductive.
    Transductive methods, such as N2V, optimize embeddings specifically for the training
    data, making them effective within a known dataset but less adaptable to new data.
    In contrast, inductive methods, as exemplified by GNNs, enable generalization
    to new, unseen data by integrating both graph structure and node features during
    training. This section also examines the mechanisms behind N2V (random walk) and
    GNNs (message passing).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们区分两种主要的学习类型：归纳和演绎。归纳方法，如N2V，针对训练数据优化嵌入，使其在已知数据集中有效，但不太适应新数据。相比之下，归纳方法，如GNNs，通过在训练过程中整合图结构和节点特征，使模型能够泛化到新的、未见过的数据。本节还探讨了N2V（随机游走）和GNNs（消息传递）背后的机制。
- en: 2.4.1 Representations and embeddings
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 表示和嵌入
- en: Understanding graph representations and the role of embeddings is crucial for
    effectively applying GNNs in machine learning. Representations convert complex
    graph data into simpler, manageable forms without losing essential information,
    facilitating analysis and interpretation of the underlying structures within graphs.
    In the context of GNNs, representations enable the processing of graph data in
    a way that is compatible with machine learning algorithms, ensuring that the rich
    and complex structures of graphs are preserved.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 理解图表示和嵌入的作用对于在机器学习中有效地应用GNNs至关重要。表示将复杂的图数据转换为更简单、更易管理的形式，同时不丢失关键信息，从而促进对图内部结构的分析和解释。在GNNs的背景下，表示使图数据以与机器学习算法兼容的方式处理，确保图的丰富和复杂结构得到保留。
- en: Traditional methods such as adjacency matrices and edge lists provide a foundational
    way to represent graph structures, but they often fall short in capturing richer
    information, such as node features or subtle topological details. This limitation
    is where graph embeddings come into play. A graph embedding is a low-dimensional
    vector representation of a graph, node, or edge that retains essential structural
    and relational information. Much like reducing a high-resolution image to a compact
    feature vector, embeddings condense the graph’s complexity while preserving its
    distinguishing characteristics.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的如邻接矩阵和边列表的方法为表示图结构提供了一种基础方式，但它们往往不足以捕捉更丰富的信息，如节点特征或微妙的拓扑细节。这种局限性正是图嵌入发挥作用的地方。图嵌入是图、节点或边的低维向量表示，它保留了基本的结构和关系信息。就像将高分辨率图像降低到紧凑的特征向量一样，嵌入将图的复杂性压缩，同时保留其独特的特征。
- en: Embeddings simplify data handling and open new possibilities for machine learning
    applications. They enable visualization of complex graphs in two or three dimensions,
    allowing us to explore their inherent structures and relationships more intuitively.
    Furthermore, embeddings serve as versatile inputs for various downstream tasks,
    such as node classification and link prediction, as demonstrated in earlier sections
    of this chapter. By providing a bridge between raw graph data and machine learning
    models, embeddings are key to unlocking the full potential of GNNs.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入简化了数据处理，并为机器学习应用开辟了新的可能性。它们使复杂图在二维或三维中可视化成为可能，使我们能够更直观地探索其固有的结构和关系。此外，嵌入作为各种下游任务的通用输入，如节点分类和链接预测，正如本章前面的部分所展示的。通过在原始图数据和机器学习模型之间架起桥梁，嵌入是解锁GNNs全部潜力的关键。
- en: The significance of node similarity and context
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 节点相似性和上下文的重要性
- en: An important use of graph embeddings is to encapsulate the notion of similarity
    and context within the graph. In a spatial context, proximity (or similarity)
    often translates to a measurable distance or angle between points.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图嵌入的一个重要用途是在图中封装相似性和上下文的概念。在空间上下文中，邻近性（或相似性）通常转化为点之间的可测量距离或角度。
- en: For graphs, however, these concepts are redefined in terms of connections and
    paths. The similarity between nodes can be interpreted through their connectivity,
    that is, how many “hops” or steps it takes to move from one node to another, or
    the likelihood of traversing from one node to another during random walks on the
    graph (figure 2.11).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图而言，这些概念在连接和路径的术语下被重新定义。节点之间的相似性可以通过它们的连通性来解释，即从一个节点移动到另一个节点需要多少“跳数”或步骤，或者是在图上随机游走时从一个节点穿越到另一个节点的可能性（图2.11）。
- en: '![figure](../Images/2-11.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-11.png)'
- en: 'Figure 2.11 Comparison of similarity concepts: using distance on a plane (left)
    and using steps along a graph (right)'
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11相似性概念的比较：在平面上使用距离（左）和使用图上的步数（右）
- en: 'Another way to think about proximity is in terms of probability: Given two
    nodes (node A and node B), what is the chance that I will encounter node B if
    I start to hop from node A? In figure 2.12, if the number of hops is 1, the probability
    is 0, as there is no way to reach node B from node A in one hop. However, if the
    number of hops is 2, then we need to first count to see how many different possible
    routes there are. Let’s also assume that no node can be encountered twice in a
    traversal and that each direction is equally likely. With these assumptions, there
    are three unique routes of 2 hops starting from node A. Of those, only one leads
    to node B. Thus, the probability is one out of three, or 33%. This probabilistic
    approach to measuring proximity between nodes offers a nuanced understanding of
    the graph’s topology, meaning that graph structures can be encoded within a probability
    space.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考邻近性的方式是使用概率：给定两个节点（节点A和节点B），如果我从节点A开始跳跃，遇到节点B的概率是多少？在图2.12中，如果跳跃次数为1，概率为0，因为从节点A到节点B不可能在一跳内到达。然而，如果跳跃次数为2，那么我们需要先计算有多少不同的可能路径。假设在遍历中不会遇到任何节点两次，并且每个方向的可能性相同。在这些假设下，从节点A开始有三个独特的2跳路径。其中，只有一条通向节点B。因此，概率是三分之一，或者说33%。这种基于概率的节点间邻近性测量方法提供了对图拓扑的细微理解，这意味着图结构可以编码在概率空间内。
- en: '![figure](../Images/2-12.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-12.png)'
- en: 'Figure 2.12 Illustrating the notion of proximity computed in terms of probability:
    given a walk from node A, the probability of encountering node B is a measure
    of proximity.'
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.12展示了基于概率计算的邻近性概念：给定从节点A的行走路径，遇到节点B的概率是邻近性的度量。
- en: The ideas explained here are relevant as we approach the topic of inductive
    and transductive methods as applied to graph embeddings. Both of these methodologies
    use the notion of node proximity, although in distinct ways, to generate embeddings
    that capture the essence of node relationships and graph structure. Inductive
    methods excel in generalizing to accommodate new, unseen data, enabling models
    to adapt and learn beyond their initial training set. Conversely, transductive
    methods specialize in optimizing embeddings specifically for the training data
    itself, making them highly effective within their learned context but less flexible
    when introduced to new data.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这里解释的思想与我们接近图嵌入的归纳和转导方法相关。这两种方法都使用节点邻近性的概念，尽管方式不同，以生成能够捕捉节点关系和图结构的嵌入。归纳方法擅长推广以适应新的、未见过的数据，使模型能够适应并学习超出其初始训练集。相反，转导方法专门优化嵌入以适应训练数据本身，使其在其学习环境中非常有效，但引入新数据时灵活性较低。
- en: 2.4.2 Transductive and inductive methods
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 转导和归纳方法
- en: 'The way an embedding is created determines the scope of its subsequent usage.
    Here we examine embedding methods that can be broadly classified as transductive
    and inductive. Transductive embedding methods learn representations for a fixed
    set of nodes in a single, static graph:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入创建的方式决定了其后续使用的范围。在这里，我们研究可以广泛归类为转导和归纳的嵌入方法。转导嵌入方法学习单个静态图中固定节点集的表示：
- en: These methods directly optimize individual embeddings for each node.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法直接优化每个节点的单个嵌入。
- en: The entire graph structure must be available during training.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个图结构必须在训练期间可用。
- en: These methods can’t naturally generalize to unseen nodes or graphs.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法无法自然地推广到未见过的节点或图。
- en: Adding new nodes requires retraining the entire model.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加新节点需要重新训练整个模型。
- en: Examples include DeepWalk [8], N2V, and matrix factorization approaches.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，包括DeepWalk [8]、N2V和矩阵分解方法。
- en: Transductive methods allow us to reduce the scope of the prediction problem.
    For transduction, we’re only concerned with the data we’re presented with.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转导方法使我们能够缩小预测问题的范围。对于转导，我们只关心我们呈现的数据。
- en: These methods are computationally costly for large amounts of data.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大量数据，这些方法在计算上成本较高。
- en: 'Inductive embedding methods learn a function to generate embeddings, allowing
    generalization to unseen nodes and even entirely new graphs:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳嵌入方法学习一个生成嵌入的函数，允许推广到未见过的节点甚至全新的图：
- en: These methods learn to aggregate and transform node features and local graph
    structure.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法学习聚合和转换节点特征和局部图结构。
- en: These methods can generate embeddings for previously unseen nodes without retraining.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法可以在不重新训练的情况下为以前未见过的节点生成嵌入。
- en: Node attributes or structural features are often used.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点属性或结构特征通常被使用。
- en: These methods are more flexible and scalable for dynamic or expanding graphs.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法对于动态或扩展图更加灵活和可扩展。
- en: Examples include GraphSAGE, GCNs, and graph attention networks (GATs).
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例包括GraphSAGE、GCNs和图注意力网络（GATs）。
- en: 'Let’s illustrate this with two examples:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用两个示例来说明这一点：
- en: '*Example 1: Email spam detection*—An inductive model for email spam detection
    is trained on a dataset of labeled emails (spam or not spam) and learns to generalize
    from the training data. Once trained, the model can classify new incoming emails
    as spam or not spam without needing to retrain.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*示例1：电子邮件垃圾邮件检测*—一个用于电子邮件垃圾邮件检测的归纳模型在标记的电子邮件数据集（垃圾邮件或非垃圾邮件）上训练，并从训练数据中学习泛化。一旦训练完成，该模型可以对新到达的电子邮件进行分类，判断其为垃圾邮件或非垃圾邮件，而无需重新训练。'
- en: Transductive wouldn’t be better in this example because models would require
    retraining with every new batch of emails, making them computationally expensive
    and impractical for real-time spam detection.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，归纳方法不会更好，因为模型需要为每个新的电子邮件批次重新训练，这使得它们在计算上昂贵且不适用于实时垃圾邮件检测。
- en: '*Example 2: Semi-supervised learning for community detection in social networks*—A
    transductive model uses the entire graph to identify communities within a social
    network. Using a combination of labeled and unlabeled nodes, the model exploits
    the network in a better way: inductive models wouldn’t take full advantage of
    the specific network structure and node interconnections because they only process
    part of the data—the training set. This isn’t enough information for accurate
    community detection.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*示例2：社交网络社区检测的半监督学习*—一个归纳模型使用整个图来识别社交网络中的社区。通过结合标记和无标记的节点，该模型更好地利用了网络：归纳模型不会充分利用特定的网络结构和节点互连，因为它们只处理部分数据——训练集。这不足以进行准确的社区检测。'
- en: Table 2.5 compares the types of graph representation we’ve learned so far, consisting
    of representations generated by both nonembedding methods, and embedding methods.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.5比较了迄今为止我们学到的图表示类型，包括由非嵌入方法和嵌入方法生成的表示。
- en: Table 2.5 Different methods of graph representation
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.5 不同图表示方法
- en: '| Representation | Description | Examples |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 表示 | 描述 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Basic data representations  | • Great for analytical methods that involve
    network traversal • Useful for some node classification algorithms'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '| 基本数据表示 | • 对于涉及网络遍历的分析方法非常有用 • 对于某些节点分类算法有用'
- en: '• Information provided: Node and edge neighbors'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: • 提供的信息：节点和边邻居
- en: '| • Adjacency list • Edge list'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '| • 邻接表 • 边列表'
- en: • Adjacency matrix
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: • 邻接矩阵
- en: '|'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Transductive (shallow) embeddings  | • Useless for data not trained on •
    Difficult to scale'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '| 归纳（浅层）嵌入 | • 对于未在数据上训练的数据无作用 • 难以扩展'
- en: '| • DeepWalk • N2V'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '| • DeepWalk • N2V'
- en: • TransE
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: • TransE
- en: • RESCAL
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: • RESCAL
- en: • Graph factorization
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: • 图分解
- en: • Spectral techniques
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: • 谱技术
- en: '|'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Inductive embeddings  | • Models can be generalized to new and structurally
    different graphs • Represents data as vectors in continuous space'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '| 归纳嵌入 | • 模型可以推广到新的和结构不同的图 • 将数据表示为连续空间中的向量'
- en: • Learns a mapping from data (new and old) to positions within the continuous
    space
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: • 学习从数据（新和旧）到连续空间中位置的映射
- en: '| • GNNs can be used to inductively generate embeddings • Transformers'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '| • GNNs可以用于归纳生成嵌入 • Transformers'
- en: • N2V with feature concatenation
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: • N2V与特征拼接
- en: '|'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Summary of terms related to transductive embedding methods
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与归纳嵌入方法相关的术语总结
- en: Two additional terms related to embedding methods and sometimes used interchangeably
    with it are *shallow methods* and *encoders*. Here, we’ll briefly distinguish
    these terms.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 与嵌入方法相关且有时与其互换使用的两个额外术语是*浅层方法*和*编码器*。在这里，我们将简要区分这些术语。
- en: Transductive methods, explained earlier, are a large class of methods of which
    graph embedding is one application. So, outside of our present context of representation
    learning, the attributes of transductive learning remain the same.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 之前解释过的归纳方法是一大类方法，其中图嵌入是其中一个应用。因此，在我们当前关于表示学习的背景下，归纳学习属性保持不变。
- en: In machine learning, *shallow* is often used to refer to the opposite of deep
    learning models or algorithms. Such models are distinguished from deep learning
    models in that they don’t use multiple processing layers to produce an output
    from input data. In our context of graph/node embeddings, this term also refers
    to methods that aren’t based on deep learning, but more specifically points to
    methods that mimic a simple lookup table, rather than a generalized model produced
    from a supervised learning algorithm.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，*浅层*通常用来指代深度学习模型或算法的相反。这些模型与深度学习模型不同，因为它们不使用多个处理层从输入数据中产生输出。在我们的图/节点嵌入的上下文中，这个术语也指那些不是基于深度学习的方法，但更具体地指向那些模仿简单的查找表，而不是从监督学习算法生成的通用模型。
- en: Any method that reproduces a low dimensional representation of data, an embedding,
    is often known as an *encoder*. This encoder simply matches a given data point
    such as a node (or even an entire graph) to its respective embedding in low dimensional
    space. GNNs can be broadly understood as a class of encoders, similar to Transformers.
    However, there are specific GNN encoders, such as the graph autoencoder (GAE),
    which you’ll meet in chapter 5\.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 任何能够再现数据低维表示（嵌入）的方法通常被称为*编码器*。这个编码器简单地匹配给定的数据点，如节点（甚至整个图）到其在低维空间中的相应嵌入。GNNs可以广泛地理解为编码器的一类，类似于Transformers。然而，有一些特定的GNN编码器，如图自动编码器（GAE），你将在第5章中遇到。
- en: '2.4.3 N2V: Random walks across graphs'
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 N2V：图上的随机游走
- en: Random walk approaches construct embeddings by using random walks across the
    graph. With these, the similarity between two nodes, A and B, is defined as the
    probability that one will encounter node B on a random graph traversal from node
    A (as we described in section 2.4.1). These walks are unrestricted, with no restriction
    preventing a walk from backtracking or encountering the same node multiple times.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 随机游走方法通过在图上使用随机游走来构建嵌入。使用这些方法，两个节点A和B之间的相似性被定义为从节点A到一个随机图遍历中遇到节点B的概率（正如我们在2.4.1节中描述的）。这些游走是不受限制的，没有任何限制阻止游走回溯或多次遇到相同的节点。
- en: For each node, we perform a random walk within its neighborhood. As we perform
    more and more random walks, we begin to notice similarities in the types of nodes
    we encounter. A potential mental model is exploring a city or forest. In a distinct
    neighborhood, for example, as we take the same streets or paths multiple times,
    we begin to notice that houses have a similar style and trees have a similar species.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个节点，我们在其邻域内执行随机游走。随着我们执行越来越多的随机游走，我们开始注意到我们遇到的节点类型的相似性。一个潜在的心理模型是探索一个城市或森林。例如，在一个独特的邻域中，当我们多次走相同的街道或路径时，我们开始注意到房屋有相似的风格，树木有相似的种类。
- en: The result of a random walk method is a vector of nodes visited for each walk,
    with different starting nodes. In the upcoming figure 2.13, we show some examples
    of how we can walk (or search) across a graph.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 随机游走方法的结果是每个游走访问的节点向量，具有不同的起始节点。在即将出现的图2.13中，我们展示了如何在我们可以在图上行走（或搜索）的一些示例。
- en: DeepWalk is one method that creates embeddings by performing several random
    walks of a fixed size for each node and calculating embeddings from each of these.
    Here, any path is equally likely to occur, making the walks unbiased and meaning
    that all nodes connected by an edge are equally likely to be encountered at each
    step. The output for a DeepWalk on the graph in figure 2.13 might be the vector
    [u, s1, s3] or the vector [u, s1, s2, s4, s5]. Each of these vectors contains
    the unique nodes visited in a random walk.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: DeepWalk是一种通过为每个节点执行固定大小的多个随机游走来创建嵌入的方法，并从这些中计算嵌入。在这里，任何路径出现的可能性都是相同的，这使得游走无偏，意味着所有通过边连接的节点在每个步骤中遇到的可能性都是相同的。图2.13上的DeepWalk的输出可能是向量[u,
    s1, s3]或向量[u, s1, s2, s4, s5]。这些向量中的每一个都包含随机游走中访问的唯一节点。
- en: 'N2V improved on DeepWalk by introducing tunable bias in these random walks.
    The idea is to be able to trade off learnings from a node’s close-by neighborhood
    and from further away. N2V captures this in two parameters:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: N2V通过在这些随机游走中引入可调偏置来改进DeepWalk。想法是能够在节点附近邻域的学习和更远处的学习中权衡。N2V通过两个参数来捕捉这一点：
- en: '`p`—Controls the probability that the path walked will return to the previous
    node.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p`—控制路径游走返回到前一个节点的概率。'
- en: '`q`—Controls the probability of whether a depth-first search (DFS, a hopping
    strategy that emphasizes faraway nodes) or breadth-first search (BFS, a strategy
    that emphasizes nearby nodes). DFS and BFS are illustrated in figure 2.13, where
    we demonstrate what happens for four hops.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q`——控制深度优先搜索（DFS，一种强调远端节点的跳跃策略）或广度优先搜索（BFS，一种强调附近节点的策略）的概率。DFS和BFS在图2.13中展示，我们展示了四个跳跃发生的情况。'
- en: To mimic the DeepWalk algorithm, both `p` and `q` would be set to `0` such that
    the search is *unbiased*. So, for figure 2.13, the output for N2V could be [u,
    s1,s2] or [u,s4,s5,s6] depending on whether the walks are BFS or DFS.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟DeepWalk算法，`p`和`q`都会被设置为`0`，这样搜索就是**无偏的**。因此，对于图2.13，N2V的输出可以是[u, s1,s2]或[u,s4,s5,s6]，这取决于行走是BFS还是DFS。
- en: '![figure](../Images/2-13.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-13.png)'
- en: Figure 2.13 Depth-first search (DFS) and breadth first search (BFS) on a graph
    where embeddings are generated based on random walks using these graph-traversal
    strategies. DFS (light arrows) prioritizes going deep down one path, while BFS
    (dark arrows) prioritizes checking all adjacent and nearby paths.
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.13展示了基于随机行走和这些图遍历策略生成的嵌入的深度优先搜索（DFS）和广度优先搜索（BFS）。DFS（浅色箭头）优先深入一条路径，而BFS（深色箭头）优先检查所有相邻和附近的路径。
- en: Once we have the vector of nodes, we create embeddings by using a neural network
    to predict the most likely neighboring node for a given node. Usually, this neural
    network is shallow, with one hidden layer. After training, the hidden layer becomes
    that node’s embedding.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了节点向量，我们通过使用神经网络来预测给定节点的最可能相邻节点来创建嵌入。通常，这个神经网络是浅层的，只有一个隐藏层。训练后，隐藏层就变成了该节点的嵌入。
- en: 2.4.4 Message passing as deep learning
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 消息传递作为深度学习
- en: Deep learning methods in general are composed of building blocks, or layers,
    that take some tensor-based input and then produce an output that is transformed
    as it flows through the various layers. At the end, more transformations and aggregations
    are applied to yield a prediction. However, often the output of the hidden layers
    is directly exploited for other tasks within the model architecture or are used
    as inputs to other models. This is what we saw in our classification problem in
    section 2.3\. We constructed a vector of visited nodes, and these nodes were then
    passed to a deep learning model. The deep learning model learned to predict future
    nodes based on a starting node. But the actual embeddings were contained within
    *the hidden layer* of the network.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法通常由构建块或层组成，这些层接受一些基于张量的输入，并在通过各个层流动时对其进行转换。最后，应用更多的转换和聚合以生成预测。然而，通常隐藏层的输出会直接在模型架构中的其他任务中被利用，或者作为其他模型的输入。这就是我们在第2.3节中分类问题中看到的情况。我们构建了一个访问节点向量，然后这些节点被传递给深度学习模型。深度学习模型学会了根据起始节点预测未来的节点。但实际的嵌入包含在网络中的**隐藏层**。
- en: Tip  For a refresher on deep learning, read *Deep Learning with Python* by François
    Chollet (Manning, 2021).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：为了复习深度学习，请阅读弗朗索瓦·肖莱特（François Chollet）的《Python深度学习》（Manning, 2021）。
- en: We show the classic architecture for a deep feed-forward neural network, specifically
    a multilayer perceptron (MLP), in figure 2.14\. Briefly, the network takes a node
    vector as input, and the hidden layers are trained to produce an output vector
    that achieves some task, such as identifying a node class. The input vector may
    be flattened images, and the output may be a single number reflecting where there
    is a dog or cat in the image. For the N2V example, the input is a vector of starting
    nodes, and the output is the corresponding other node vectors that are visited
    after traversing the graph from the starting node. In the image example, the output
    is the explicit task function, namely to classify images based on whether they
    contain a dog or cat. In N2V, the output is implicit in the graph structure. We
    know the subsequent nodes that are visited, but we’re interested in the way the
    network has encoded the data, that is, how it has built an embedding of the node
    data. This is contained within the hidden layer, where typically we just take
    the last layer.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图2.14中展示了深度前馈神经网络的经典架构，具体来说是一个多层感知器（MLP）。简而言之，网络以节点向量为输入，隐藏层被训练以产生一个输出向量，以完成某些任务，例如识别节点类别。输入向量可能是展平的图像，输出可能是一个数字，反映图像中狗或猫的位置。在N2V示例中，输入是起始节点的向量，输出是在从起始节点遍历图后访问的其他节点向量。在图像示例中，输出是显式的任务函数，即根据图像中是否包含狗或猫来对图像进行分类。在N2V中，输出隐含在图结构中。我们知道后续访问的节点，但我们感兴趣的是网络如何编码数据，即它是如何构建节点数据的嵌入。这包含在隐藏层中，通常我们只取最后一层。
- en: '![figure](../Images/2-14.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-14.png)'
- en: Figure 2.14 Structure of a multilayer perceptron
  id: totrans-353
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.14 多层感知器的结构
- en: For GNNs, the input will be the entire graph structure, and the output will
    be the embeddings. Therefore, the model is explicit in how it constructs the embeddings.
    However, the output isn’t restricted to embeddings. Instead, we can have the output
    as a classification, such as whether the book has a specific political affiliation
    in that node in the graph. The embeddings are again implicit in the hidden layers.
    However, the entire process is wrapped into a single model, so we don’t need to
    extract this data. Instead, we’ve used the embeddings to achieve our goal, such
    as node classification.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GNNs，输入将是整个图结构，输出将是嵌入。因此，模型在构建嵌入的方式上是明确的。然而，输出并不局限于嵌入。相反，我们可以将输出作为分类，例如在图中的该节点是否有特定的政治倾向。嵌入再次隐含在隐藏层中。然而，整个过程被封装在一个单一模型中，因此我们不需要提取这些数据。相反，我们使用了嵌入来实现我们的目标，例如节点分类。
- en: While the architecture of GNNs is very different from feed-forward neural networks,
    there are some parallels. In many of the GNNs we learn about, a graph in tensor
    form is passed into the GNN architecture, and one or more iterations of message
    passing is applied. The message-passing process is shown in figure 2.15.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GNNs的架构与前馈神经网络非常不同，但也有一些相似之处。在我们所学习的许多GNNs中，一个以张量形式表示的图被输入到GNN架构中，并应用一到多个消息传递迭代。消息传递过程如图2.15所示。
- en: '![figure](../Images/2-15.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-15.png)'
- en: Figure 2.15 Elements of our message-passing layer. Each message-passing layer
    consists of an aggregation, a transformation, and an update step.
  id: totrans-357
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.15我们的消息传递层的元素。每个消息传递层由聚合、转换和更新步骤组成。
- en: In chapter 1, we first discussed the idea of message passing. In its simplest
    form, message passing reflects that we’re taking information or data from nodes
    or edges and sending it somewhere else [1]. The messages are the data, and we’re
    passing the messages across the structure of our graph. Each message can contain
    information from either sender or receiver, or often both.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们首先讨论了消息传递的概念。在其最简单形式中，消息传递反映了我们从节点或边获取信息或数据并将其发送到其他地方[1]。消息是数据，我们在图的结构中传递消息。每个消息可以包含发送者或接收者的信息，或者通常是两者都有。
- en: We can now explain further why message passing is so important to GNNs. The
    message passing step updates the information about each node by using the node
    information and nodes neighborhood information (both in terms of nearby node data
    and the edge data connecting them). Message passing is how we construct representations
    about our graph. These are the critical mechanisms that build graph embeddings,
    which inform other tasks such as node classification. There are two important
    aspects to consider when constructing these node (or edge) embeddings.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以进一步解释为什么消息传递对于GNNs如此重要。消息传递步骤通过使用节点信息和节点邻近区域的信息（包括附近节点数据和连接它们的边数据）来更新每个节点的信息。消息传递是我们构建关于我们的图表示的方法。这些是构建图嵌入的关键机制，这些嵌入为其他任务，如节点分类提供了信息。在构建这些节点（或边）嵌入时，有两个重要的方面需要考虑。
- en: First, we need to think about what is inside a message. In our earlier example,
    we had a list of books on political topics. This dataset only had information
    about the books’ co-purchasing connections and their political leaning labels.
    However, if we had additional information such as the book length, author name,
    or even the synopsis, then those node features could be contained within our messages.
    However, it’s important to remember that it could also be edge data, such as when
    another book was bought together, that could also be contained in messages. In
    fact, sometimes messages can contain both node and edge data.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要考虑消息中包含的内容。在我们之前的例子中，我们有一个关于政治主题的书籍列表。这个数据集只包含关于书籍的共购买连接和它们的政治倾向标签。然而，如果我们有额外的信息，比如书籍长度、作者名字，甚至是简介，那么这些节点特征就可以包含在我们的消息中。然而，重要的是要记住，它也可能是边数据，比如当另一本书一起购买时，这也可能包含在消息中。实际上，有时消息可以同时包含节点和边数据。
- en: Second, we need to think about how much local information we want to consider
    when making each embedding. We want to know how much of the neighborhood to sample.
    We already discussed this when we introduced random walk methods. We need to define
    how many hops to take when sampling our graph.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们需要考虑在制作每个嵌入时想要考虑多少本地信息。我们需要知道要采样多少邻近区域的信息。当我们介绍随机游走方法时已经讨论过这一点。我们需要定义在采样我们的图时需要跳过多少步。
- en: Both the data and the number of hops are critical to message passing in GNNs.
    The features, either node or edge data, are the messages, and the number of hops
    is the number of times we pass a message. Both of these are controlled by the
    layers of a GNN. The number of hidden layers is the number of hops that we’ll
    be sending messages. The input to each hidden layer is the data contained in a
    message. This is almost always the case for GNNs but it’s worth noting that it
    isn’t always true. Sometimes, other mechanisms such as attention can determine
    the depth of message-passing samples from the neighborhood. We’ll discuss graph
    attention networks (GATs) in chapter 4\. Until then, understanding that the number
    of layers in a GNN reflects the number of hops undertaken during message passing
    is a good intuition to have.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在GNNs（图神经网络）中，数据和跳数对于消息传递至关重要。特征，无论是节点还是边数据，都是消息，而跳数是我们传递消息的次数。这两者都由GNN的层来控制。隐藏层的数量是我们将发送消息的跳数。每个隐藏层的输入是消息中包含的数据。对于GNNs来说，这几乎总是情况，但值得注意的是，这并不总是正确的。有时，其他机制，如注意力，可以确定从邻近区域的消息传递样本的深度。我们将在第4章讨论图注意力网络（GATs）。在此之前，理解GNN中的层数反映了消息传递过程中进行的跳数是一个很好的直觉。
- en: For a feed-forward network, like the one on the left in figure 2.15, information
    is passed between the nodes of our neural network. In a GNN, this information
    comprises the messages that we send over our graph. For each message-passing step,
    the vertex in our neural network collects information from nodes or edges one
    hop away. So, if we want our node representations to take account of nodes from
    three hops away from each node, we need three hidden message-passing layers. Three
    layers may not seem like very many, but the amount of a graph we cover scales
    exponentially with the number of hops. Intuitively, we can understand this as
    a type of six degrees of separation principle—that all people are only six degrees
    of social separation apart from each other. This would mean that you and I could
    be connected by six short hops across the global combined social network.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前馈网络，如图2.15左侧所示，信息在我们神经网络的节点之间传递。在GNN中，这些信息包括我们在图上发送的消息。对于每个消息传递步骤，我们神经网络中的顶点从一步之遥的节点或边收集信息。因此，如果我们想让节点表示考虑每个节点三步之遥的节点，我们需要三个隐藏的消息传递层。三个层可能看起来并不多，但覆盖的图量会随着步数的增加而指数级增长。直观上，我们可以将其理解为一种六度分隔原理——所有人之间都只有六度的社会距离。这意味着我和你之间可能通过全球综合社交网络中的六个短暂的跳跃而连接。
- en: Different message-passing schemes lead to different flavors of GNNs. So, for
    each GNN we study in this book, we’ll pay close attention to the math and code
    implementation for message passing. One important aspect is how we aggregate messages,
    which we’ll discuss in chapter 3 when we discuss GCNs in depth.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的消息传递方案会导致不同的GNN变体。因此，在这本书中我们研究的每个GNN，我们都会密切关注消息传递的数学和代码实现。一个重要的方面是我们如何聚合消息，我们将在第3章深入讨论GCN时进行讨论。
- en: After message passing, the resulting tensor is passed through feed-forward layers
    that result in a prediction. In the left of figure 2.16, which illustrates a GNN
    model for node prediction, the data flows through message-passing layers, the
    tensor then passes through an additional MLP and activation function to output
    a prediction. For example, we could use our GNN to classify whether employees
    are likely to join a new company or have good recommendations.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在消息传递之后，得到的张量会通过前馈层，从而产生预测。在图2.16的左侧，该图展示了用于节点预测的GNN模型，数据流经消息传递层，然后张量通过一个额外的MLP和激活函数以输出预测。例如，我们可以使用我们的GNN来分类员工是否可能加入一家新公司或获得良好的推荐。
- en: '![figure](../Images/2-16.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-16.png)'
- en: Figure 2.16 A simple GNN architecture diagram (left). A graph is input on the
    left, encountering node-information-passing layers. This is followed by MLP layers.
    After an activation is applied, a prediction is yielded. A GNN architecture (right).
  id: totrans-367
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.16 一个简单的GNN架构图（左侧）。一个图在左侧输入，遇到节点信息传递层。随后是MLP层。应用激活后，产生预测。一个GNN架构（右侧）。
- en: However, as with the feed-forward neural network illustrated previously, we
    can also output just the hidden layers and work directly with that output. For
    GNNs, this output is the graph, node, or edge embeddings.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，就像之前图示的前馈神经网络一样，我们也可以只输出隐藏层，并直接与该输出工作。对于GNN，这个输出是图、节点或边嵌入。
- en: One final note on message passing in GNNs is that in each step in the message-passing
    layer of our GNNs, we’ll be passing information from nodes to another node one
    hop away. Importantly, a neural network then takes the data from the one-hop neighbors
    and applies a nonlinear transformation. This is the beauty of GNNs; we’re applying
    many small neural networks at the level of individual nodes and/or edges to build
    embeddings of the graph features. Therefore, when we say that a message-passing
    layer is like the first layer of a neural network, we’re really saying that it’s
    the first layer of many individual neural networks that are all learning on local
    node data or edge-specific data. In practice, the overall code constructing and
    training is the same as if it were one single transformation, but the intuition
    that we’re applying individual nonlinear transformations will become useful as
    we travel deeper into the workings of complex GNN models.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 关于GNN中的消息传递的最后一点是，在GNN的消息传递层中的每一步，我们将从节点传递信息到另一个跳数远的节点。重要的是，一个神经网络随后从单跳邻居那里获取数据并应用非线性变换。这是GNNs的美丽之处；我们在单个节点和/或边的层面上应用许多小的神经网络来构建图特征的嵌入。因此，当我们说消息传递层就像神经网络的第1层时，我们实际上是在说它是许多单个神经网络的第1层，这些神经网络都在学习局部节点数据或边特定数据。在实践中，整体代码构建和训练与一个单一变换相同，但当我们深入研究复杂GNN模型的工作原理时，应用单个非线性变换的直觉将变得有用。
- en: Summary
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Node and graph embeddings are powerful methods to extract insights from our
    data, and they can serve as inputs/features in our machine learning models. There
    are several independent methods for generating such embeddings. GNNs have embedding
    built into the architecture.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点和图嵌入是强大的方法，可以从我们的数据中提取洞察，并且可以作为我们的机器学习模型中的输入/特征。有几种独立的方法可以生成此类嵌入。GNNs在其架构中内置了嵌入。
- en: Graph embeddings, including node and edge embeddings, serve as foundational
    techniques to transform complex graph data into structured formats suitable for
    machine learning tasks.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图嵌入，包括节点和边嵌入，是将复杂图数据转换为适合机器学习任务的格式的基础技术。
- en: 'We explored two main types of graph embeddings: N2V, a transductive method,
    and GNN-based embeddings, an inductive method, each with distinct characteristics
    and applications.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探索了两种主要的图嵌入类型：N2V，一种归纳方法，以及基于GNN的嵌入，一种归纳方法，每种方法都有其独特的特性和应用。
- en: N2V operates on fixed datasets using random walks to establish node contexts
    and similarities, but it doesn’t generalize to unseen data or graphs.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N2V在固定数据集上操作，使用随机游走来建立节点上下文和相似性，但它不能推广到未见数据或图。
- en: GNNs, on the other hand, are versatile, inductive frameworks that can generate
    embeddings for new, unseen data, making them adaptable across different graph
    structures.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，GNNs是灵活的归纳框架，可以为新的、未见数据生成嵌入，使它们能够适应不同的图结构。
- en: The comparison of embeddings in machine learning tasks, such as semi-supervised
    learning, reveals the importance of choosing the right embedding method based
    on the data size, complexity, and specific problem at hand.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习任务中，如半监督学习，嵌入的比较揭示了根据数据大小、复杂性和具体问题选择正确嵌入方法的重要性。
- en: Despite the effectiveness of random forest classifiers in handling both N2V
    and GNN embeddings for smaller graphs, GNNs demonstrate a unique ability to use
    graph topology and node features, particularly in larger and more complex graphs.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管随机森林分类器在处理较小图的N2V和GNN嵌入方面非常有效，但GNNs展示了使用图拓扑和节点特征的独特能力，尤其是在较大和更复杂的图中。
- en: Embeddings can be used as features in traditional machine learning models and
    in graph data visualization and insight extraction.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入可以被用作传统机器学习模型和图数据可视化和洞察提取中的特征。
