- en: Chapter 19\. Using Generative Models with Hugging Face Diffusers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第19章\. 使用Hugging Face Diffusers进行生成模型
- en: Over the last few chapters, we have been looking at inference on generative
    models and primarily using LLMs (aka text-to-text models) to explore different
    scenarios. However, generative AI isn’t limited just to text-based models, and
    another important innovation is, of course, image generation (aka text-to-image).
    Most image generation models today are based on a process called *diffusion*,
    which inspires the name *diffusers* for the Hugging Face APIs used to create images
    from text prompts. In this chapter, we’ll explore how diffusion models work and
    how to get up and running with your own apps that can generate images from prompts.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们一直在研究生成模型的推理，并主要使用LLMs（即文本到文本模型）来探索不同的场景。然而，生成AI并不仅限于基于文本的模型，另一个重要的创新当然是图像生成（即文本到图像）。今天的大多数图像生成模型都是基于称为*扩散*的过程，这启发了Hugging
    Face API的名称，该API用于从文本提示创建图像。在本章中，我们将探讨扩散模型的工作原理以及如何启动并运行您自己的应用程序，这些应用程序可以从提示生成图像。
- en: What Are Diffusion Models?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是扩散模型？
- en: By now, most of us have seen images that are AI created, and we’ve likely been
    amazed at how quickly they have grown from abstract, rough representations to
    near photoreal representations of what we asked for via a prompt. Because the
    models allow for longer prompts, with more detail, and as their training sets
    have grown, we’ve seen a near endless stream of improvements to what can be done
    with AI image generation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们大多数人已经看到了AI创建的图像，并且我们可能对它们如何从抽象、粗糙的表示迅速发展到我们通过提示请求的几乎逼真的表示感到惊讶。由于模型允许更长的提示，包含更多细节，并且随着训练集的增长，我们看到了几乎无尽的改进，这些改进可以用于AI图像生成。
- en: But how does all of this work? It starts with the idea of diffusion.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这一切是如何工作的呢？它始于扩散的想法。
- en: You can start this process by creating a dataset of images and their associated
    noise. Consider [Figure 19-1](#ch19_figure_1_1748573005759131).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过创建一个包含图像及其相关噪声的数据集来开始这个过程。考虑[图19-1](#ch19_figure_1_1748573005759131)。
- en: '![](assets/aiml_1901.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1901.png)'
- en: Figure 19-1\. Noising an image
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-1\. 对图像进行噪声处理
- en: Then, once you have a set of images you’ve made noisy like this, you can train
    a model that learns how to denoise to get the image back to its original state.
    Consider the noise to be the data and the original image to be the labels. So,
    in the case of [Figure 19-1](#ch19_figure_1_1748573005759131), the noise on the
    right can be the data and the image of the puppy can be the label. At that point,
    you can train a model that, when it sees noise, can figure out how to turn that
    noise into an image. The logical extension is that you can then *generate* noise,
    and the model will figure out how to turn that noise into an image that will look
    a little bit like one of those in your training set.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦您有一组这样的噪声图像，您就可以训练一个模型，该模型学习如何去噪以将图像恢复到其原始状态。将噪声视为数据，将原始图像视为标签。因此，在[图19-1](#ch19_figure_1_1748573005759131)的情况下，右侧的噪声可以视为数据，小狗的图像可以视为标签。在那个阶段，您可以训练一个模型，当它看到噪声时，可以找出如何将噪声转换为图像。逻辑上的扩展是，您然后可以*生成*噪声，模型将找出如何将噪声转换为看起来有点像训练集中那些图像的图像。
- en: But, what if you go back to the step of creating the noisy image and add text
    to it with a very verbose description? Then, your noisy image will have a text
    label (represented in embeddings) attached to it (see [Figure 19-2](#fig-19-2))!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您回到创建噪声图像的步骤，并添加一个非常冗长的描述文本呢？那么，您的噪声图像将附有一个文本标签（以嵌入表示）！（见[图19-2](#fig-19-2)）！
- en: '![](assets/aiml_1902.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1902.png)'
- en: Figure 19-2\. Adding text encodings to the diffusion process
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-2\. 将文本编码添加到扩散过程中
- en: Now, the noisy image has the embeddings describing it attached to it. In simple
    terms, the piece of noise is enhanced by embeddings that describe it, so the process
    of denoising this image back into the original image of the puppy has the extra
    data to guide it in how it denoises. So, again, if you train a model with the
    noise plus embeddings as the data and the original images as the labels, then
    a model can now learn more effectively how to turn noise plus embeddings into
    a picture.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，带有描述其的嵌入的噪声图像已经附加到它上面。简单来说，噪声片段通过描述它的嵌入得到增强，因此将此图像去噪回小狗的原始图像的过程有了额外的数据来指导它如何去噪。因此，再次强调，如果您用噪声加嵌入作为数据，原始图像作为标签来训练模型，那么模型现在可以更有效地学习如何将噪声加嵌入转换为图片。
- en: You probably see where this is going. Once that model is trained, then, in the
    future, if someone gives it a piece of text in a prompt, the text can be encoded
    into embeddings, a set of random noise can be generated, and the model can try
    to figure out how to take that random noise and denoise it, guided by the text,
    into an image. For all intents and purposes, it will create a whole new image
    as a result (see [Figure 19-3](#fig-19-3)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经看到了这个趋势。一旦该模型训练完成，那么在未来，如果有人给它一个提示中的文本片段，文本可以被编码成嵌入，生成一组随机噪声，模型可以尝试在文本的引导下，将随机噪声去噪成图像。从所有目的和意义上讲，它将创建一个全新的图像（见图
    19-3）。
- en: '![](assets/aiml_1903.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1903.png)'
- en: Figure 19-3\. Beginning the process of denoising an image
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 19-3\. 开始去噪图像的过程
- en: Here, we can start with purely random noise and a prompt. The prompt is something
    that likely wasn’t in the training set—there are no known images (other than AI-generated
    ones, of course) of teddy bears eating pizza on the surface of Mars.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以从完全随机的噪声和一个提示开始。提示很可能是训练集中没有的东西——除了人工智能生成的图像之外，没有已知图像（当然，泰迪熊在火星表面吃披萨的图像）。
- en: So a model can then denoise this over multiple steps. As you can imagine, the
    very first step will be random noise, the second step will be where the model
    tries to get the noise to match the prompt, the third step will get it a little
    closer, and so on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型可以在多个步骤中去除噪声。正如你可以想象的那样，第一步将是随机噪声，第二步将是模型尝试使噪声与提示匹配，第三步会使其更接近，依此类推。
- en: This is depicted in [Figure 19-4](#ch19_figure_2_1748573005759167), where you
    can see what the image looks like with the popular *stable diffusion* models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这在[图 19-4](#ch19_figure_2_1748573005759167)中有所描述，你可以看到使用流行的 *稳定扩散* 模型的图像看起来是什么样子。
- en: '![](assets/aiml_1904.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1904.png)'
- en: Figure 19-4\. Gradually denoising an image based on a prompt
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 19-4\. 基于提示逐渐去噪图像
- en: In this case, I used a diffusion model with the prompt from [Figure 19-3](#fig-19-3)
    about teddy bears eating pizza. You’ll see the code for this a little later in
    this chapter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我使用了一个扩散模型，其提示来自[图 19-3](#fig-19-3)中关于泰迪熊吃披萨的内容。你将在本章稍后看到这段代码。
- en: In Step 0, you can see that we just have pure noise. In Step 1, the model has
    already started taking some of the stronger characteristics of the prompt—the
    surface of Mars—and given the image a very red hue. By Step 10, we have teddy
    bears and pizza, and by Step 40, the teddy bears are actually eating the pizza
    and the lighting has changed—presumably for dinnertime!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 0 中，你可以看到我们只有纯噪声。在步骤 1 中，模型已经开始吸收一些提示中的较强特征——火星表面——并给图像赋予非常红的色调。到步骤 10，我们有了泰迪熊和披萨，到步骤
    40，泰迪熊实际上正在吃披萨，光线也发生了变化——可能是晚餐时间了！
- en: The *size* of the image depends on the model. Many earlier models, or those
    designed to run on consumer hardware, will generate smaller images that they will
    then upscale to give the desired output. The images I have shown here were created
    with Stable Diffusion 3.5, which creates 1024 × 1024 images by default.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的 *大小* 取决于模型。许多早期的模型，或者那些设计用于在消费级硬件上运行的模型，将生成较小的图像，然后将其放大以得到所需的输出。我这里展示的图像是用
    Stable Diffusion 3.5 创建的，默认创建 1024 × 1024 的图像。
- en: Note
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While this chapter will focus on diffusion models, using them isn’t the *only*
    way to generate images. There are also *autoregressive models*, which learn the
    mappings between the tokens for the text in the description of the image and the
    tokens that represent the visual contents of the image. With lots of examples
    of these mappings, you can train a model on them. Then, you can give the model
    a piece of text, and it will be able to predict the tokens for that text and reassemble
    them into an image.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章将重点介绍扩散模型，但使用它们并不是生成图像的 *唯一* 方法。还有 *自回归模型*，这些模型学习图像描述中的文本标记与代表图像视觉内容的标记之间的映射。有了这些映射的大量示例，你可以在它们上训练一个模型。然后，你可以给模型一段文本，它将能够预测该文本的标记并将它们重新组装成图像。
- en: Using Hugging Face Diffusers
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Hugging Face Diffusers
- en: Just as Hugging Face offers a transformers library (as we explained in [Chapter 15](ch15.html#ch15_transformers_and_transformers_1748549808974580)),
    it also offers a diffusers library to make it easier for you to use diffusion
    models. Diffusers abstract the complexities of using various models into an easy-to-use
    API.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 Hugging Face 提供了 transformers 库（如我们在第 15 章[第 15 章](ch15.html#ch15_transformers_and_transformers_1748549808974580)中解释的），它还提供了一个
    diffusers 库，以便更容易地使用扩散模型。Diffusers 将使用各种模型的复杂性抽象成一个易于使用的 API。
- en: 'To get started with diffusers, you simply install them like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用diffusers，您只需像这样安装它们：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The diffusers library manages the pipelining of model inference in the same
    way we experienced in earlier chapters with transformers. There are many steps
    involved in getting a model to render an image based on a prompt: encoding the
    prompt, making embeddings, passing the embeddings to the model along with any
    hyperparameters it needs, grabbing the output tensors, and turning them into an
    image. But diffusers encapsulate this for you into a pipeline, and there are a
    number of open source pipelines for many different models.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: diffusers库以我们在早期章节中与transformers一起体验的方式管理模型推理的流水线。根据提示渲染图像涉及许多步骤：编码提示、创建嵌入、将嵌入以及任何所需的超参数传递给模型、获取输出张量并将它们转换为图像。但是diffusers为您封装了这些步骤，并为许多不同的模型提供了开源流水线。
- en: So, for example, in the image of teddy bears on Mars, I used Stable Diffusion
    3.5 Medium, which you can find on the [Hugging Face website](https://oreil.ly/liUY-).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在火星上的泰迪熊图像中，我使用了Stable Diffusion 3.5 Medium，您可以在[Hugging Face网站](https://oreil.ly/liUY-)上找到它。
- en: This model has limited access, so at the top of the Hugging Face page, you’ll
    see a form that you need to fill out to get permission. You’ll also need to configure
    your Hugging Face secret key in Colab (if you’re using Colab), which we demonstrated
    how to do back in [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型访问权限有限，因此您会在Hugging Face页面的顶部看到一个表单，您需要填写以获得权限。如果您正在使用Colab，您还需要配置Hugging
    Face的秘密密钥（如果您正在使用Colab），我们之前在[第14章](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)中演示了如何操作。
- en: 'If you aren’t using Colab, your code will need to be signed in to Hugging Face
    using their API. You can do this with the following code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不使用Colab，您的代码需要通过Hugging Face的API登录。您可以使用以下代码完成此操作：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once you’re signed in (or if you’re using a model that doesn’t require signing
    in), the process of generating an image is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您登录（或者如果您使用的是不需要登录的模型），生成图像的过程如下：
- en: Create a Generator object, which allows you to specify the seed.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个生成器对象，它允许您指定种子。
- en: Create an instance of the appropriate pipeline for the model you require.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为您所需的模型创建适当的流水线实例。
- en: Send that pipeline to the appropriate accelerator.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该流水线发送到适当的加速器。
- en: Generate the image with the pipeline, giving it the appropriate parameters.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用流水线生成图像，并为其提供适当的参数。
- en: Let’s look at this step-by-step.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步来看。
- en: 'First, you specify the generator using `torch.Generator`, where you will specify
    the accelerator for the generator and set the seed. You use the seed value to
    create the initial noise with a level of determinism. If you want to be able to
    *replicate* the images that are generated, despite the noise being random, you
    do so by guiding the noise with the seed. In other words, when the noise is generated
    with a seed value, the *same* noise will be generated subsequent times with the
    same seed. So effectively, the noise will be pseudo-random, as there will be a
    deterministic seed at play. On the other hand, if you don’t specify a seed, you’ll
    get a random value for it. Here’s an example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您使用`torch.Generator`指定生成器，其中您将指定生成器的加速器并设置种子。您使用种子值以确定性水平创建初始噪声。如果您想要能够*复制*生成的图像，尽管噪声是随机的，您可以通过用种子引导噪声来实现。换句话说，当使用种子值生成噪声时，随后的相同种子将生成相同的噪声。因此，噪声将是伪随机的，因为将有一个确定性的种子在起作用。另一方面，如果您没有指定种子，您将得到一个随机值。以下是一个示例：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, you’ll specify the pipeline and instantiate it with a model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将指定流水线并使用模型实例化它：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we’re using Stable Diffusion 3.5, which uses the `StableDiffusion3Pipeline`
    class. The diffusers API is open source, with new pipelines being added all the
    time. You can inspect them on [GitHub](https://oreil.ly/uYGnJ).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用Stable Diffusion 3.5，它使用`StableDiffusion3Pipeline`类。diffusers API是开源的，并且不断添加新的流水线。您可以在[GitHub](https://oreil.ly/uYGnJ)上检查它们。
- en: You can also browse the different models on the [Hugging Face website](https://oreil.ly/R4dKT).
    Often, their landing pages will include source code about which pipeline to use
    and the address of the model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以浏览[Hugging Face网站](https://oreil.ly/R4dKT)上的不同模型。通常，它们的着陆页会包括关于使用哪个流水线的源代码以及模型的地址。
- en: 'Once you have the pipeline, you can use it to create an image by specifying
    the prompt and some other model-dependent parameters that you’ll find in the model
    document. So, for example, for stable diffusion, you’ll specify the number of
    inference steps and the generator that you specified earlier. You should also
    specify *where* you want the pipe to execute—(in this case, it’s `cuda`, as you
    can see in the previous code, which uses the GPU accelerator in Colab):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了这个管道，你可以通过指定提示和一些其他模型相关的参数来创建一个图片，这些参数你可以在模型文档中找到。例如，对于稳定扩散，你需要指定推理步骤的数量以及你之前指定的生成器。你还应该指定*在哪里*你想让管道执行——（在这个例子中，它是`cuda`，如前述代码所示，它使用了Colab中的GPU加速器）：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: I’ve found that the best way to experiment with this is to explore the pipeline’s
    source code and see the parameters that it supports. For example, with the Stable
    Diffusion 3 pipeline, there’s a *negative prompt* that dictates things that you
    do *not* want to see in the image. Often, you can use this to make images better.
    For example, you may have heard that image generators, particularly early ones,
    were very bad at drawing hands. You could use the negative prompt to have the
    image generator avoid this problem by saying “deformed hands” or something similar
    in that prompt.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现，探索这个管道的源代码并查看它支持的参数是实验的最佳方式。例如，在Stable Diffusion 3管道中，有一个*负提示*，它规定了你不希望在图像中看到的事物。通常，你可以使用这个来使图像变得更好。例如，你可能听说过图像生成器，尤其是早期的，在绘制手部时非常糟糕。你可以使用负提示来让图像生成器通过在提示中说“变形的手”或类似的话来避免这个问题。
- en: 'You can also specify things you don’t want to see in the image that are more
    trivial! For example, every instance of the image I drew had the teddy bears eating
    *pepperoni* pizza. I could remove the pepperoni from this image with this code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以指定你不想在图片中看到的一些更简单的事物！例如，我画的每个图像中都有泰迪熊在吃*意大利辣香肠*披萨。我可以使用以下代码从这个图像中移除意大利辣香肠：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The resulting image is shown in [Figure 19-5](#ch19_figure_3_1748573005759194).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图像显示在[图19-5](#ch19_figure_3_1748573005759194)中。
- en: The teddy on the left doesn’t look thrilled about it, but the others seem more
    content!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 左边的泰迪熊看起来对此并不兴奋，但其他的似乎更满意！
- en: In this case, we used text-to-image to create these images—but diffusion models
    have become a little more advanced with add-ons for *image-to-image.* With such
    add-ons, instead of starting with random noise, we can begin with an existing
    image and then perform *inpainting*, in which we can have the model fill in new
    details in an existing image. We’ll explore this next.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了文本到图像来创建这些图像——但是扩散模型通过添加*图像到图像*的功能而变得更加先进。有了这些附加功能，我们不再是随机噪声开始，而是可以从现有的图像开始，然后进行*修复*，在这个过程中，我们可以让模型在现有图像中填充新的细节。我们将在下一部分探讨这一点。
- en: '![](assets/aiml_1905.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片1](assets/aiml_1905.png)'
- en: Figure 19-5\. Teddies that don’t like pepperoni
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-5\. 不喜欢意大利辣香肠的泰迪熊
- en: Image-to-Image with Diffusers
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于扩散器的图像到图像
- en: 'When inspecting the source code for the pipeline, you may have discovered other
    classes in there, such as this one: `StableDiffusion3Img2ImgPipeline.`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当检查管道的源代码时，你可能发现了其中的其他类，例如这个：`StableDiffusion3Img2ImgPipeline.`
- en: 'As its name suggests, this class allows you to start with one image to create
    another. You can initialize it in a way that’s very similar to initializing the
    text-to-image pipeline:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如它的名字所暗示的，这个类别允许你从一个图片开始创建另一个。你可以以非常类似于初始化文本到图片管道的方式初始化它：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, you’ll specify an image to use as the source image:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将指定一个用作源图像的图片：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: I’m starting with an image of a puppy (see [Figure 19-6](#ch19_figure_4_1748573005759214)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我从一张小狗的图片开始（见图19-6[见](#ch19_figure_4_1748573005759214)）。
- en: '![](assets/aiml_1906.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片2](assets/aiml_1906.png)'
- en: Figure 19-6\. Source image of a puppy
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-6\. 小狗的源图像
- en: 'We’ll use this as the initialization image in an image-to-image pipeline with
    the following code. Note that the prompt is specifying a highly detailed photograph
    of a baby *dragon*:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码作为图像到图像管道的初始化图片。注意，提示指定了一张高度详细的婴儿*龙*的照片：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The strength parameter specifies how closely the generated image should follow
    the input image. At 0.0, the model won’t do anything and the output will be the
    input image. At 1.0, it will effectively *ignore* the input image and will just
    act as a text-to-image model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 强度参数指定了生成的图像应该多么紧密地遵循输入图像。在0.0时，模型不会做任何事情，输出将是输入图像。在1.0时，它将有效地*忽略*输入图像，并仅作为文本到图像模型运行。
- en: Under the hood, it does this with the following process.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，它是通过以下过程实现的。
- en: Given that the code specified a strength of 0.7, the model will add noise to
    the image until the image has had 70% of its pixels replaced by noise (and thus
    only 30% of the image is the original values).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码指定了0.7的强度，模型将向图像添加噪声，直到图像有70%的像素被噪声替换（因此只有30%的图像是原始值）。
- en: The model will then run 70 denoising steps (70% of the 100 specified), which
    will give an image like [Figure 19-7](#ch19_figure_5_1748573005759233).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将运行70个去噪步骤（100个指定步骤中的70%），这将得到一个像[图19-7](#ch19_figure_5_1748573005759233)那样的图像。
- en: Typically, if you use strength 0.2 to 0.4, you’ll get style transfer and other
    minor modifications. At 0.5 to 0.7, you’ll have basic composition maintained,
    but major element changes, like puppy to dragon, will be seen. Above 0.8, you’ll
    see almost complete regeneration, but some slight influence from the original
    may be retained.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果你使用0.2到0.4的强度，你会得到风格迁移和其他小修改。在0.5到0.7时，你会保持基本构图，但主要元素的变化，如小狗变龙，将会出现。超过0.8，你会看到几乎完全的再生，但可能会保留一些原始的影响。
- en: '![](assets/aiml_1907.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1907.png)'
- en: Figure 19-7\. Using image-to-image to turn a puppy into a dragon
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-7\. 使用图像到图像将小狗变成龙
- en: You can see that the basic pose has been maintained, but the computer has imagined
    a dragon to replace the puppy as required. There’s also new foreground and background,
    as we didn’t specify anything about them, but they’re pretty close to the originals.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到基本姿势已经保持不变，但计算机想象出一个龙来替换小狗，正如所需的那样。还有新的前景和背景，因为我们没有指定它们，但它们与原始图像非常接近。
- en: As an example of a different strength level, [Figure 19-8](#ch19_figure_6_1748573005759251)
    shows the strength at 0.4\. We can also see that the basic shape of the puppy
    has been maintained, but it has become more dragon-like, with scaly skin and the
    beginnings of claws!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作为不同强度级别的例子，[图19-8](#ch19_figure_6_1748573005759251)显示了0.4的强度。我们还可以看到小狗的基本形状已经保持不变，但它变得更加像龙，有鳞片皮肤和爪子的开始！
- en: '![](assets/aiml_1908.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1908.png)'
- en: Figure 19-8\. Strength level of 0.4 for the puppy to dragon image-to-image
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-8\. 小狗变龙图像到图像的强度级别为0.4
- en: This technique can be very useful in helping you create new images by starting
    from existing ones. I’ve seen it used in scenarios like filmmaking—where one can
    start with existing video that’s filmed in a basic, cheap locale but then enhanced
    with image-to-image frame by frame to get a different outcome. It’s a much cheaper
    way of doing postproduction by adding special effects!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在帮助你从现有图像开始创建新图像时非常有用。我见过它在电影制作等场景中使用——一个人可以从在基本、便宜的地方拍摄的现有视频开始，然后通过逐帧的图像到图像转换来获得不同的结果。这是一种通过添加特效来降低后期制作成本的方法！
- en: Inpainting with Diffusers
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用扩散器进行修复
- en: Another scenario that involves using diffusers that is supported by some models—including
    stable diffusion models—is the idea of *inpainting*, in which you can take an
    image and replace parts of it with AI-generated content. So, for example, consider
    the puppy from [Figure 19-6](#ch19_figure_4_1748573005759214) and how you can
    change the image so the little pooch is on the moon, as in [Figure 19-9](#ch19_figure_7_1748573005759269).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个涉及使用扩散器且一些模型（包括稳定扩散模型）支持的场景是*修复*的概念，其中你可以用AI生成的内容替换图像的一部分。例如，考虑[图19-6](#ch19_figure_4_1748573005759214)中的小狗，以及你如何改变图像，让小狗在月球上，就像[图19-9](#ch19_figure_7_1748573005759269)中那样。
- en: '![](assets/aiml_1909.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1909.png)'
- en: Figure 19-9\. Using inpainting to put our puppy on the moon
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-9\. 使用修复功能将我们的小狗放在月球上
- en: 'You can do this by using a pattern that’s similar to the previous one. First,
    you’ll set up the pipeline for inpainting:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用与之前类似的模式来实现这一点。首先，你需要设置修复的管道：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The parameters to initialize it are the same as earlier. Next, you’ll need
    the generator:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化它的参数与之前相同。接下来，你需要生成器：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, you’ll specify the source image, which in this case is the original image
    of the puppy:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你需要指定源图像，在这个例子中是小狗的原始图像：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The complicated step is the next one, in which you specify the *mask* for the
    image:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的一步是下一步，你需要指定图像的*掩码*：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A *mask* is simply an image that corresponds to the original one, in which pieces
    to be *replaced* are in white and pieces to be *preserved* are in black. [Figure 19-10](#ch19_figure_8_1748573005759284)
    shows the mask image used for the puppy. I like to think of this as similar to
    the green-screen process used in making movies. The white part of the image is
    the screen, and the black is the stuff that’s in front of it! The model will then
    replace the white with whatever you prompt it for.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*遮罩*简单来说就是与原始图像相对应的图像，其中要*替换*的部分是白色的，而要*保留*的部分是黑色的。[图19-10](#ch19_figure_8_1748573005759284)展示了用于小狗的遮罩图像。我喜欢把它想象成电影制作中使用的绿幕过程。图像的白色部分是屏幕，黑色的是屏幕前面的东西！然后模型会用你提示的内容替换白色部分。
- en: '![](assets/aiml_1910.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1910.png)'
- en: Figure 19-10\. Mask for the image
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-10\. 图像的遮罩
- en: There are many ways to create masks. For this one, I used the Acorn 8 tool for
    the Mac. This tool gives you the ability to remove the background and paint it
    all in white, and then, for what’s left, it lets you select the pixels with a
    magic wand and paint them all in black. Every tool does this differently, so be
    sure to check the appropriate documentation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以创建遮罩。对于这个例子，我使用了Mac的Acorn 8工具。这个工具让你能够去除背景并将所有东西都涂成白色，然后，对于剩下的部分，它让你可以用魔法棒选择像素并将它们全部涂成黑色。每个工具的做法都不同，所以请确保查看适当的文档。
- en: 'Once you have the image and the mask, you can easily use the pipeline to have
    the model inpaint the areas that correspond to the white part of the mask. Given
    that the puppy is already present, I didn’t mention it in the prompt, and I just
    used “on the surface of the moon” to get the image in [Figure 19-9](#ch19_figure_7_1748573005759269):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了图像和遮罩，你可以轻松地使用管道让模型填充与遮罩白色部分相对应的区域。鉴于小狗已经存在，我在提示中并未提及它，只是用了“在月球表面”来获取[图19-9](#ch19_figure_7_1748573005759269)中的图像：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The diffusers API, as you can see, gives you a very consistent approach to managing
    image creation, be it directly from a text prompt, starting from a source image,
    or inpainting a particular area.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，diffusers API为你提供了一个非常一致的方法来管理图像创建，无论是直接从文本提示开始，还是从源图像开始，或者填充特定区域。
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you explored how to use generative models for image creation
    by using the Hugging Face diffusers library. You started by looking at the fundamental
    underlying concepts, seeing how the idea of denoising to create new content works.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你通过使用Hugging Face diffusers库探索了如何使用生成模型进行图像创建。你首先研究了基本的概念，了解了通过去噪创建新内容的思想是如何工作的。
- en: 'You also looked into practical code-based implementation of image generation
    by using the diffusers API, and you focused on three main approaches:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你还研究了使用diffusers API通过代码实现图像生成的实际应用，并专注于三种主要方法：
- en: You explored text-to-image by converting text prompts directly into images using
    the Stable Diffusion 3.5 model. You also looked at how you can control this process
    with parameters like the seed value and the number of inference steps.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你通过将文本提示直接转换为图像来探索了文本到图像的转换，使用了Stable Diffusion 3.5模型。你还研究了如何通过参数如种子值和推理步骤数来控制这个过程。
- en: You explored image-to-image by starting with an existing image and transforming
    it by using a prompt. In particular, you saw how the `strength` hyperparameter
    controls the overall transformation
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你通过从现有图像开始并使用提示进行转换来探索了图像到图像的转换。特别是，你看到了`strength`超参数如何控制整体转换
- en: You explored inpainting by preserving parts of the original image by using a
    mask, which allows for targeted modifications while preserving some elements.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你通过使用遮罩保留原始图像的部分来探索了修复，这允许进行有针对性的修改同时保留一些元素。
- en: You also explored hands-on, concrete code examples of each of these approaches,
    which showed you how to do the pipeline setup, generator initialization, and basic
    parameter tuning. You also saw how *negative* prompts can help you get images
    closer to what you really want.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你还探索了每个这些方法的实际、具体的代码示例，这些示例展示了如何进行管道设置、生成器初始化和基本的参数调整。你还看到了如何使用*负面*提示帮助你得到更接近你真正想要的图像。
- en: In the next chapter, you’ll look at LoRA (low-ranking adaptation), which lets
    you fine-tune diffusion models to achieve more controlled and customized images.
    LoRA is a powerful technique that allows for efficient model adaptation by only
    fine-tuning a small number of parameters, thus helping you guide the model toward
    specific styles, subjects, or artistic directions. You’ll explore how to implement
    LoRA with the diffusers library, and you’ll customize these models to create *specialized*
    image generators for your needs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
