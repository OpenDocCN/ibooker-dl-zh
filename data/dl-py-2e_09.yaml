- en: 9 Advanced deep learning for computer vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 高级计算机视觉深度学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: 'The different branches of computer vision: image classification, image segmentation,
    object detection'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉的不同分支：图像分类、图像分割、目标检测
- en: 'Modern convnet architecture patterns: residual connections, batch normalization,
    depthwise separable convolutions'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代卷积神经网络架构模式：残差连接、批量归一化、深度可分离卷积
- en: Techniques for visualizing and interpreting what convnets learn
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化和解释卷积神经网络学习的技术
- en: The previous chapter gave you a first introduction to deep learning for computer
    vision via simple models (stacks of `Conv2D` and `MaxPooling2D` layers) and a
    simple use case (binary image classification). But there’s more to computer vision
    than image classification! This chapter dives deeper into more diverse applications
    and advanced best practices.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章通过简单模型（一堆`Conv2D`和`MaxPooling2D`层）和一个简单的用例（二进制图像分类）为您介绍了计算机视觉的深度学习。但是，计算机视觉不仅仅是图像分类！本章将深入探讨更多不同应用和高级最佳实践。
- en: 9.1 Three essential computer vision tasks
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 三个基本的计算机视觉任务
- en: 'So far, we’ve focused on image classification models: an image goes in, a label
    comes out. “This image likely contains a cat; this other one likely contains a
    dog.” But image classification is only one of several possible applications of
    deep learning in computer vision. In general, there are three essential computer
    vision tasks you need to know about:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们专注于图像分类模型：输入一幅图像，输出一个标签。“这幅图像可能包含一只猫；另一幅可能包含一只狗。”但是图像分类只是深度学习在计算机视觉中的几种可能应用之一。一般来说，有三个您需要了解的基本计算机视觉任务：
- en: '*Image classification*—Where the goal is to assign one or more labels to an
    image. It may be either single-label classification (an image can only be in one
    category, excluding the others), or multi-label classification (tagging all categories
    that an image belongs to, as seen in figure 9.1). For example, when you search
    for a keyword on the Google Photos app, behind the scenes you’re querying a very
    large multilabel classification model—one with over 20,000 different classes,
    trained on millions of images.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像分类*——目标是为图像分配一个或多个标签。它可以是单标签分类（一幅图像只能属于一个类别，排除其他类别），也可以是多标签分类（标记图像所属的所有类别，如图9.1所示）。例如，当您在
    Google Photos 应用上搜索关键字时，背后实际上是在查询一个非常庞大的多标签分类模型——一个包含超过20,000个不同类别的模型，经过数百万图像训练。'
- en: '*Image segmentation*—Where the goal is to “segment” or “partition” an image
    into different areas, with each area usually representing a category (as seen
    in figure 9.1). For instance, when Zoom or Google Meet diplays a custom background
    behind you in a video call, it’s using an image segmentation model to tell your
    face apart from what’s behind it, at pixel precision.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像分割*——目标是将图像“分割”或“划分”为不同区域，每个区域通常代表一个类别（如图9.1所示）。例如，当 Zoom 或 Google Meet
    在视频通话中在您身后显示自定义背景时，它使用图像分割模型来精确区分您的面部和背景。'
- en: '*Object detection*—Where the goal is to draw rectangles (called *bounding boxes*)
    around objects of interest in an image, and associate each rectangle with a class.
    A self-driving car could use an object-detection model to monitor cars, pedestrians,
    and signs in view of its cameras, for instance.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*目标检测*——目标是在图像中绘制矩形（称为*边界框*）围绕感兴趣的对象，并将每个矩形与一个类别关联起来。例如，自动驾驶汽车可以使用目标检测模型监视其摄像头视野中的汽车、行人和标志。'
- en: '![](../Images/09-01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-01.png)'
- en: 'Figure 9.1 The three main computer vision tasks: classification, segmentation,
    detection'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 三个主要的计算机视觉任务：分类、分割、检测
- en: Deep learning for computer vision also encompasses a number of somewhat more
    niche tasks besides these three, such as image similarity scoring (estimating
    how visually similar two images are), keypoint detection (pinpointing attributes
    of interest in an image, such as facial features), pose estimation, 3D mesh estimation,
    and so on. But to start with, image classification, image segmentation, and object
    detection form the foundation that every machine learning engineer should be familiar
    with. Most computer vision applications boil down to one of these three.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的深度学习还涵盖了除这三个任务之外的一些更专业的任务，例如图像相似性评分（估计两幅图像在视觉上的相似程度）、关键点检测（在图像中定位感兴趣的属性，如面部特征）、姿势估计、3D
    网格估计等。但是，开始时，图像分类、图像分割和目标检测构成了每位机器学习工程师都应熟悉的基础。大多数计算机视觉应用都可以归结为这三种任务之一。
- en: You’ve seen image classification in action in the previous chapter. Next, let’s
    dive into image segmentation. It’s a very useful and versatile technique, and
    you can straightforwardly approach it with what you’ve already learned so far.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您已经看到了图像分类的实际应用。接下来，让我们深入了解图像分割。这是一种非常有用且多功能的技术，您可以直接使用到目前为止学到的知识来处理它。
- en: Note that we won’t cover object detection, because it would be too specialized
    and too complicated for an introductory book. However, you can check out the RetinaNet
    example on keras.io, which shows how to build and train an object detection model
    from scratch in Keras in around 450 lines of code ([https://keras.io/examples/vision/retinanet/](https://keras.io/examples/vision/retinanet/)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不会涵盖目标检测，因为这对于入门书籍来说太专业且太复杂。但是，您可以查看 keras.io 上的 RetinaNet 示例，该示例展示了如何在
    Keras 中使用大约450行代码从头构建和训练目标检测模型（[https://keras.io/examples/vision/retinanet/](https://keras.io/examples/vision/retinanet/)）。
- en: 9.2 An image segmentation example
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 图像分割示例
- en: Image segmentation with deep learning is about using a model to assign a class
    to each pixel in an image, thus *segmenting* the image into different zones (such
    as “background” and “foreground,” or “road,” “car,” and “sidewalk”). This general
    category of techniques can be used to power a considerable variety of valuable
    applications in image and video editing, autonomous driving, robotics, medical
    imaging, and so on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习进行图像分割是指使用模型为图像中的每个像素分配一个类别，从而将图像分割为不同区域（如“背景”和“前景”，或“道路”、“汽车”和“人行道”）。这一类技术可以用于图像和视频编辑、自动驾驶、机器人技术、医学成像等各种有价值的应用。
- en: 'There are two different flavors of image segmentation that you should know
    about:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种不同的图像分割类型，你应该了解：
- en: '*Semantic segmentation*, where each pixel is independently classified into
    a semantic category, like “cat.” If there are two cats in the image, the corresponding
    pixels are all mapped to the same generic “cat” category (see figure 9.2).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语义分割*，其中每个像素独立地分类为语义类别，如“猫”。如果图像中有两只猫，相应像素都映射到相同的通用“猫”类别（见图9.2）。'
- en: '*Instance segmentation*, which seeks not only to classify image pixels by category,
    but also to parse out individual object instances. In an image with two cats in
    it, instance segmentation would treat “cat 1” and “cat 2” as two separate classes
    of pixels (see figure 9.2).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实例分割*，不仅试图按类别对图像像素进行分类，还要解析出各个对象实例。在一幅图像中有两只猫，实例分割会将“猫1”和“猫2”视为两个不同的像素类别（见图9.2）。'
- en: '![](../Images/09-02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-02.png)'
- en: Figure 9.2 Semantic segmentation vs. instance segmentation
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 语义分割 vs. 实例分割
- en: 'In this example, we’ll focus on semantic segmentation: we’ll be looking once
    again at images of cats and dogs, and this time we’ll learn how to tell apart
    the main subject and its background.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将专注于语义分割：我们将再次查看猫和狗的图像，并学习如何区分主题和背景。
- en: 'We’ll work with the Oxford-IIIT Pets dataset ([www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)),
    which contains 7,390 pictures of various breeds of cats and dogs, together with
    foreground-background segmentation masks for each picture. A *segmentation mask*
    is the image-segmentation equivalent of a label: it’s an image the same size as
    the input image, with a single color channel where each integer value corresponds
    to the class of the corresponding pixel in the input image. In our case, the pixels
    of our segmentation masks can take one of three integer values:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用牛津-IIIT宠物数据集（[www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)），其中包含7,390张各种品种的猫和狗的图片，以及每张图片的前景-背景分割掩模。*分割掩模*是图像分割中的标签等效物：它是与输入图像大小相同的图像，具有单个颜色通道，其中每个整数值对应于输入图像中相应像素的类别。在我们的情况下，我们的分割掩模像素可以取三个整数值中的一个：
- en: 1 (foreground)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 (前景)
- en: 2 (background)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 (背景)
- en: 3 (contour)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 (轮廓)
- en: 'Let’s start by downloading and uncompressing our dataset, using the `wget`
    and `tar` shell utilities:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始下载并解压我们的数据集，使用`wget`和`tar` shell工具：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The input pictures are stored as JPG files in the images/ folder (such as images/Abyssinian_1.jpg),
    and the corresponding segmentation mask is stored as a PNG file with the same
    name in the annotations/trimaps/ folder (such as annotations/trimaps/ Abyssinian_1.png).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图片以JPG文件的形式存储在images/文件夹中（例如images/Abyssinian_1.jpg），相应的分割掩模以PNG文件的形式存储在annotations/trimaps/文件夹中（例如annotations/trimaps/Abyssinian_1.png）。
- en: 'Let’s prepare the list of input file paths, as well as the list of the corresponding
    mask file paths:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们准备输入文件路径列表，以及相应掩模文件路径列表：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, what does one of these inputs and its mask look like? Let’s take a quick
    look. Here’s a sample image (see figure 9.3):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，其中一个输入及其掩模是什么样子？让我们快速看一下。这是一个示例图像（见图9.3）：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Display input image number 9.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 显示第9个输入图像。
- en: '![](../Images/09-03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-03.png)'
- en: Figure 9.3 An example image
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 一个示例图像
- en: 'And here is its corresponding target (see figure 9.4):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它对应的目标（见图9.4）：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The original labels are 1, 2, and 3\. We subtract 1 so that the labels range
    from 0 to 2, and then we multiply by 127 so that the labels become 0 (black),
    127 (gray), 254 (near-white).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 原始标签为1、2和3。我们减去1，使标签范围从0到2，然后乘以127，使标签变为0（黑色）、127（灰色）、254（接近白色）。
- en: ❷ We use color_mode="grayscale" so that the image we load is treated as having
    a single color channel.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们使用color_mode="grayscale"，以便加载的图像被视为具有单个颜色通道。
- en: '![](../Images/09-04.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-04.png)'
- en: Figure 9.4 The corresponding target mask
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 对应的目标掩模
- en: 'Next, let’s load our inputs and targets into two NumPy arrays, and let’s split
    the arrays into a training and a validation set. Since the dataset is very small,
    we can just load everything into memory:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将输入和目标加载到两个NumPy数组中，并将数组分割为训练集和验证集。由于数据集非常小，我们可以将所有内容加载到内存中：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ We resize everything to 200 × 200.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们将所有内容调整为200 × 200。
- en: ❷ Total number of samples in the data
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 数据中的样本总数
- en: ❸ Shuffle the file paths (they were originally sorted by breed). We use the
    same seed (1337) in both statements to ensure that the input paths and target
    paths stay in the same order.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对文件路径进行洗牌（它们最初是按品种排序的）。我们在两个语句中使用相同的种子（1337），以确保输入路径和目标路径保持相同顺序。
- en: ❹ Subtract 1 so that our labels become 0, 1, and 2.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 减去1，使我们的标签变为0、1和2。
- en: ❺ Load all images in the input_imgs float32 array and their masks in the targets
    uint8 array (same order). The inputs have three channels (RBG values) and the
    targets have a single channel (which contains integer labels).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将所有图像加载到input_imgs的float32数组中，将它们的掩模加载到targets的uint8数组中（顺序相同）。输入有三个通道（RGB值），目标有一个单通道（包含整数标签）。
- en: ❻ Reserve 1,000 samples for validation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 保留1,000个样本用于验证。
- en: ❼ Split the data into a training and a validation set.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将数据分割为训练集和验证集。
- en: 'Now it’s time to define our model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是定义我们的模型的时候了：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Don’t forget to rescale input images to the [0-1] range.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 不要忘记将输入图像重新缩放到[0-1]范围。
- en: ❷ Note how we use padding="same" everywhere to avoid the influence of border
    padding on feature map size.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 请注意我们在所有地方都使用padding="same"，以避免边界填充对特征图大小的影响。
- en: ❸ We end the model with a per-pixel three-way softmax to classify each output
    pixel into one of our three categories.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们以每像素三路softmax结束模型，将每个输出像素分类为我们的三个类别之一。
- en: 'Here’s the output of the `model.summary()` call:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`model.summary()`调用的输出：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The first half of the model closely resembles the kind of convnet you’d use
    for image classification: a stack of `Conv2D` layers, with gradually increasing
    filter sizes. We downsample our images three times by a factor of two each, ending
    up with activations of size `(25,` `25,` `256)`. The purpose of this first half
    is to encode the images into smaller feature maps, where each spatial location
    (or pixel) contains information about a large spatial chunk of the original image.
    You can understand it as a kind of compression.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的前半部分与你用于图像分类的卷积网络非常相似：一堆`Conv2D`层，逐渐增加滤波器大小。我们通过每次减少两倍的因子三次对图像进行下采样，最终得到大小为`(25,`
    `25,` `256)`的激活。这前半部分的目的是将图像编码为较小的特征图，其中每个空间位置（或像素）包含有关原始图像大空间块的信息。你可以将其理解为一种压缩。
- en: 'One important difference between the first half of this model and the classification
    models you’ve seen before is the way we do downsampling: in the classification
    convnets from the last chapter, we used `MaxPooling2D` layers to downsample feature
    maps. Here, we downsample by adding *strides* to every other convolution layer
    (if you don’t remember the details of how convolution strides work, see “Understanding
    convolution strides” in section 8.1.1). We do this because, in the case of image
    segmentation, we care a lot about the *spatial location* of information in the
    image, since we need to produce per-pixel target masks as output of the model.
    When you do 2 × 2 max pooling, you are completely destroying location information
    within each pooling window: you return one scalar value per window, with zero
    knowledge of which of the four locations in the windows the value came from. So
    while max pooling layers perform well for classification tasks, they would hurt
    us quite a bit for a segmentation task. Meanwhile, strided convolutions do a better
    job at downsampling feature maps while retaining location information. Throughout
    this book, you’ll notice that we tend to use strides instead of max pooling in
    any model that cares about feature location, such as the generative models in
    chapter 12.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的前半部分与你之前看到的分类模型之间的一个重要区别是我们进行下采样的方式：在上一章的分类卷积网络中，我们使用`MaxPooling2D`层来对特征图进行下采样。在这里，我们通过向每个卷积层添加*步幅*来进行下采样（如果你不记得卷积步幅的详细信息，请参阅第8.1.1节中的“理解卷积步幅”）。我们这样做是因为在图像分割的情况下，我们非常关心图像中信息的*空间位置*，因为我们需要将每个像素的目标掩模作为模型的输出。当你进行2×2最大池化时，你完全破坏了每个池化窗口内的位置信息：你返回每个窗口一个标量值，对于窗口中的四个位置中的哪一个位置的值来自于零了解。因此，虽然最大池化层在分类任务中表现良好，但对于分割任务，它会对我们造成相当大的伤害。与此同时，步幅卷积在下采样特征图的同时保留位置信息做得更好。在本书中，你会注意到我们倾向于在任何关心特征位置的模型中使用步幅而不是最大池化，比如第12章中的生成模型。
- en: 'The second half of the model is a stack of `Conv2DTranspose` layers. What are
    those? Well, the output of the first half of the model is a feature map of shape
    `(25,` `25,` `256)`, but we want our final output to have the same shape as the
    target masks, `(200,` `200,` `3)`. Therefore, we need to apply a kind of *inverse*
    of the transformations we’ve applied so far—something that will *upsample* the
    feature maps instead of downsampling them. That’s the purpose of the `Conv2DTranspose`
    layer: you can think of it as a kind of convolution layer that *learns to upsample*.
    If you have an input of shape `(100,` `100,` `64)`, and you run it through the
    layer `Conv2D(128,` `3,` `strides=2,` `padding="same")`, you get an output of
    shape `(50,` `50,` `128)`. If you run this output through the layer `Conv2DTranspose(64,`
    `3,` `strides=2,` `padding="same")`, you get back an output of shape `(100,` `100,`
    `64)`, the same as the original. So after compressing our inputs into feature
    maps of shape `(25,` `25,` `256)` via a stack of `Conv2D` layers, we can simply
    apply the corresponding sequence of `Conv2DTranspose` layers to get back to images
    of shape `(200,` `200,` `3)`.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的后半部分是一堆`Conv2DTranspose`层。那些是什么？嗯，模型的前半部分的输出是形状为`(25,` `25,` `256)`的特征图，但我们希望最终输出与目标掩模的形状相同，即`(200,`
    `200,` `3)`。因此，我们需要应用一种*逆*转换，而不是迄今为止应用的转换的一种—一种*上采样*特征图而不是下采样的方法。这就是`Conv2DTranspose`层的目的：你可以将其视为一种学习*上采样*的卷积层。如果你有形状为`(100,`
    `100,` `64)`的输入，并将其通过层`Conv2D(128,` `3,` `strides=2,` `padding="same")`，你将得到形状为`(50,`
    `50,` `128)`的输出。如果你将此输出通过层`Conv2DTranspose(64,` `3,` `strides=2,` `padding="same")`，你将得到形状为`(100,`
    `100,` `64)`的输出，与原始相同。因此，通过一堆`Conv2D`层将我们的输入压缩成形状为`(25,` `25,` `256)`的特征图后，我们只需应用相应的`Conv2DTranspose`层序列即可恢复到形状为`(200,`
    `200,` `3)`的图像。
- en: 'We can now compile and fit our model:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编译和拟合我们的模型：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s display our training and validation loss (see figure 9.5):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示我们的训练和验证损失（见图9.5）：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/09-05.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-05.png)'
- en: Figure 9.5 Displaying training and validation loss curves
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 显示训练和验证损失曲线
- en: 'You can see that we start overfitting midway, around epoch 25\. Let’s reload
    our best performing model according to the validation loss, and demonstrate how
    to use it to predict a segmentation mask (see figure 9.6):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们在中途开始过拟合，大约在第25个时期。让我们重新加载根据验证损失表现最佳的模型，并演示如何使用它来预测分割掩模（见图9.6）：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Utility to display a model’s prediction
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 显示模型预测的实用程序
- en: '![](../Images/09-06.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-06.png)'
- en: Figure 9.6 A test image and its predicted segmentation mask
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 一个测试图像及其预测的分割掩模
- en: There are a couple of small artifacts in our predicted mask, caused by geometric
    shapes in the foreground and background. Nevertheless, our model appears to work
    nicely.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测掩模中有一些小的人为瑕疵，这是由前景和背景中的几何形状引起的。尽管如此，我们的模型似乎运行良好。
- en: 'By this point, throughout chapter 8 and the beginning of chapter 9, you’ve
    learned the basics of how to perform image classification and image segmentation:
    you can already accomplish a lot with what you know. However, the convnets that
    experienced engineers develop to solve real-world problems aren’t quite as simple
    as those we’ve been using in our demonstrations so far. You’re still lacking the
    essential mental models and thought processes that enable experts to make quick
    and accurate decisions about how to put together state-of-the-art models. To bridge
    that gap, you need to learn about *architecture patterns*. Let’s dive in.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在第8章和第9章的开头，你已经学会了如何执行图像分类和图像分割的基础知识：你已经可以用你所知道的知识做很多事情了。然而，有经验的工程师开发的用于解决现实世界问题的卷积神经网络并不像我们迄今在演示中使用的那么简单。你仍然缺乏使专家能够快速准确地决定如何组合最先进模型的基本思维模型和思维过程。为了弥合这一差距，你需要了解*架构模式*。让我们深入探讨。
- en: 9.3 Modern convnet architecture patterns
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 现代卷积神经网络架构模式
- en: 'A model’s “architecture” is the sum of the choices that went into creating
    it: which layers to use, how to configure them, and in what arrangement to connect
    them. These choices define the *hypothesis space* of your model: the space of
    possible functions that gradient descent can search over, parameterized by the
    model’s weights. Like feature engineering, a good hypothesis space encodes *prior
    knowledge* that you have about the problem at hand and its solution. For instance,
    using convolution layers means that you know in advance that the relevant patterns
    present in your input images are translation-invariant. In order to effectively
    learn from data, you need to make assumptions about what you’re looking for.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型的“架构”是创建它所做选择的总和：使用哪些层，如何配置它们，以及如何连接它们。这些选择定义了你的模型的*假设空间*：梯度下降可以搜索的可能函数空间，由模型的权重参数化。像特征工程一样，一个好的假设空间编码了你对手头问题及其解决方案的*先验知识*。例如，使用卷积层意味着你事先知道你的输入图像中存在的相关模式是平移不变的。为了有效地从数据中学习，你需要对你正在寻找的内容做出假设。
- en: Model architecture is often the difference between success and failure. If you
    make inappropriate architecture choices, your model may be stuck with suboptimal
    metrics, and no amount of training data will save it. Inversely, a good model
    architecture will accelerate learning and will enable your model to make efficient
    use of the training data available, reducing the need for large datasets. A good
    model architecture is one that *reduces the size of the search space* or otherwise
    *makes it easier to converge to a good point of the search space*. Just like feature
    engineering and data curation, model architecture is all about *making the problem
    simpler* for gradient descent to solve. And remember that gradient descent is
    a pretty stupid search process, so it needs all the help it can get.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构往往是成功与失败之间的区别。如果你做出不恰当的架构选择，你的模型可能会陷入次优指标，无论训练数据量多大都无法拯救它。相反，一个好的模型架构将加速学习，并使你的模型能够有效利用可用的训练数据，减少对大型数据集的需求。一个好的模型架构是*减少搜索空间的大小*或*使其更容易收敛到搜索空间的良好点*。就像特征工程和数据整理一样，模型架构的目标是*简化问题*，以便梯度下降解决。记住，梯度下降是一个相当愚蠢的搜索过程，所以它需要尽可能多的帮助。
- en: 'Model architecture is more an art than a science. Experienced machine learning
    engineers are able to intuitively cobble together high-performing models on their
    first try, while beginners often struggle to create a model that trains at all.
    The keyword here is *intuitively*: no one can give you a clear explanation of
    what works and what doesn’t. Experts rely on pattern-matching, an ability that
    they acquire through extensive practical experience. You’ll develop your own intuition
    throughout this book. However, it’s not *all* about intuition either—there isn’t
    much in the way of actual science, but as in any engineering discipline, there
    are best practices.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构更像是一门艺术而不是一门科学。有经验的机器学习工程师能够直观地拼凑出高性能模型，而初学者常常难以创建一个能够训练的模型。关键词在于*直觉*：没有人能给你清晰的解释什么有效什么无效。专家依赖于模式匹配，这是他们通过广泛实践经验获得的能力。你将在本书中培养自己的直觉。然而，这也不完全是关于直觉的——实际上并没有太多的科学，但就像任何工程学科一样，有最佳实践。
- en: 'In the following sections, we’ll review a few essential convnet architecture
    best practices: in particular, *residual connections*, *batch normalization*,
    and *separable convolutions*. Once you master how to use them, you will be able
    to build highly effective image models. We will apply them to our cat vs. dog
    classification problem.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将回顾一些关键的卷积神经网络架构最佳实践：特别是*残差连接*、*批量归一化*和*可分离卷积*。一旦你掌握了如何使用它们，你将能够构建高效的图像模型。我们将把它们应用到我们的猫狗分类问题中。
- en: 'Let’s start from the bird’s-eye view: the modularity-hierarchy-reuse (MHR)
    formula for system architecture.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从鸟瞰图开始：系统架构的模块化-层次结构-重用（MHR）公式。
- en: 9.3.1 Modularity, hierarchy, and reuse
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 模块化、层次结构和重用
- en: 'If you want to make a complex system simpler, there’s a universal recipe you
    can apply: just structure your amorphous soup of complexity into *modules*, organize
    the modules into a *hierarchy*, and start *reusing* the same modules in multiple
    places as appropriate (“reuse” is another word for *abstraction* in this context).
    That’s the MHR formula (modularity-hierarchy-reuse), and it underlies system architecture
    across pretty much every domain where the term “architecture” is used. It’s at
    the heart of the organization of any system of meaningful complexity, whether
    it’s a cathedral, your own body, the US Navy, or the Keras codebase (see figure
    9.7).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想让一个复杂系统变得简单，你可以应用一个通用的方法：将你的复杂混乱的系统结构化为*模块*，将模块组织成*层次结构*，并开始在适当的地方*重用*相同的模块（“重用”在这个上下文中是*抽象*的另一个词）。这就是MHR公式（模块化-层次化-重用），它是几乎每个领域中使用“架构”这个术语的系统架构的核心。它是任何有意义的复杂系统的组织核心，无论是大教堂、你自己的身体、美国海军还是Keras代码库（见图9.7）。
- en: '![](../Images/09-07.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-07.png)'
- en: Figure 9.7 Complex systems follow a hierarchical structure and are organized
    into distinct modules, which are reused multiple times (such as your four limbs,
    which are all variants of the same blueprint, or your 20 “fingers”).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 复杂系统遵循层次结构，并组织成不同的模块，这些模块被多次重复使用（比如你的四肢，它们都是同一个蓝图的变体，或者你的20个“手指”）。
- en: 'If you’re a software engineer, you’re already keenly familiar with these principles:
    an effective codebase is one that is modular, hierarchical, and where you don’t
    reimplement the same thing twice, but instead rely on reusable classes and functions.
    If you factor your code by following these principles, you could say you’re doing
    “software architecture.”'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一名软件工程师，你已经对这些原则非常熟悉：一个有效的代码库是模块化、层次化的，你不会重复实现相同的东西，而是依赖可重用的类和函数。如果你按照这些原则来因素化你的代码，你可以说你在做“软件架构”。
- en: 'Deep learning itself is simply the application of this recipe to continuous
    optimization via gradient descent: you take a classic optimization technique (gradient
    descent over a continuous function space), and you structure the search space
    into modules (layers), organized into a deep hierarchy (often just a stack, the
    simplest kind of hierarchy), where you reuse whatever you can (for instance, convolutions
    are all about reusing the same information in different spatial locations).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习本身只是通过梯度下降对连续优化应用这一方法的结果：你采用了经典的优化技术（在连续函数空间上的梯度下降），并将搜索空间结构化为模块（层），组织成深层次的层级结构（通常只是一个堆栈，最简单的层次结构），在其中重复利用任何可以的东西（例如，卷积就是关于在不同空间位置重复使用相同信息）。
- en: Likewise, deep learning model architecture is primarily about making clever
    use of modularity, hierarchy, and reuse. You’ll notice that all popular convnet
    architectures are not only structured into layers, they’re structured into repeated
    groups of layers (called “blocks” or “modules”). For instance, the popular VGG16
    architecture we used in the previous chapter is structured into repeated “conv,
    conv, max pooling” blocks (see figure 9.8).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，深度学习模型架构主要是关于巧妙地利用模块化、层次化和重用。你会注意到所有流行的卷积神经网络架构不仅结构化为层，而且结构化为重复的层组（称为“块”或“模块”）。例如，在上一章中我们使用的流行的VGG16架构结构化为重复的“卷积、卷积、最大池化”块（见图9.8）。
- en: 'Further, most convnets often feature pyramid-like structures (*feature hierarchies*).
    Recall, for example, the progression in the number of convolution filters we used
    in the first convnet we built in the previous chapter: 32, 64, 128\. The number
    of filters grows with layer depth, while the size of the feature maps shrinks
    accordingly. You’ll notice the same pattern in the blocks of the VGG16 model (see
    figure 9.8).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大多数卷积神经网络通常具有金字塔结构（*特征层次结构*）。例如，回想一下我们在上一章中构建的第一个卷积神经网络中使用的卷积滤波器数量的增长：32、64、128。随着层次深度的增加，滤波器的数量也增加，而特征图的大小相应缩小。你会在VGG16模型的块中看到相同的模式（见图9.8）。
- en: '![](../Images/09-08.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-08.png)'
- en: 'Figure 9.8 The VGG16 architecture: note the repeated layer blocks and the pyramid-like
    structure of the feature maps'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 VGG16架构：请注意重复的层块和特征图的金字塔结构
- en: 'Deeper hierarchies are intrinsically good because they encourage feature reuse,
    and therefore abstraction. In general, a deep stack of narrow layers performs
    better than a shallow stack of large layers. However, there’s a limit to how deep
    you can stack layers, due to the problem of *vanishing gradients*. This leads
    us to our first essential model architecture pattern: residual connections.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 更深的层次结构本质上是好的，因为它们鼓励特征的重复使用，从而实现抽象化。一般来说，一堆窄层次的深层比一堆大层次的浅层表现更好。然而，由于“梯度消失”问题，你可以堆叠的层次有限。这引出了我们的第一个基本模型架构模式：残差连接。
- en: On the importance of ablation studies in deep learning research
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度学习研究中消融研究的重要性
- en: Deep learning architectures are often more *evolved* than designed—they were
    developed by repeatedly trying things and selecting what seemed to work. Much
    like in biological systems, if you take any complicated experimental deep learning
    setup, chances are you can remove a few modules (or replace some trained features
    with random ones) with no loss of performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习架构通常比设计更*进化*——它们是通过反复尝试和选择有效的方法开发出来的。就像生物系统一样，如果你拿任何复杂的实验性深度学习设置，很可能你可以删除一些模块（或用随机的特征替换一些训练过的特征），而不会损失性能。
- en: 'This is made worse by the incentives that deep learning researchers face: by
    making a system more complex than necessary, they can make it appear more interesting
    or more novel, and thus increase their chances of getting a paper through the
    peer-review process. If you read lots of deep learning papers, you will notice
    that they’re often optimized for peer review in both style and content in ways
    that actively hurt clarity of explanation and reliability of results. For instance,
    mathematics in deep learning papers is rarely used for clearly formalizing concepts
    or deriving non-obvious results—rather, it gets leveraged as a *signal of seriousness*,
    like an expensive suit on a salesman.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习研究人员面临的激励使情况变得更糟：通过使系统比必要复杂，他们可以使其看起来更有趣或更新颖，从而增加他们通过同行评审过程的机会。如果你阅读了很多深度学习论文，你会注意到它们通常在风格和内容上都被优化以满足同行评审，这些方式实际上会损害解释的清晰度和结果的可靠性。例如，深度学习论文中的数学很少用于清晰地形式化概念或推导非显而易见的结果——相反，它被利用作为*严肃性的信号*，就像推销员身上的昂贵西装一样。
- en: 'The goal of research shouldn’t be merely to publish, but to generate reliable
    knowledge. Crucially, understanding *causality* in your system is the most straightforward
    way to generate reliable knowledge. And there’s a very low-effort way to look
    into causality: *ablation studies*. Ablation studies consist of systematically
    trying to remove parts of a system—making it simpler—to identify where its performance
    actually comes from. If you find that X + Y + Z gives you good results, also try
    X, Y, Z, X + Y, X + Z, and Y + Z, and see what happens.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 研究的目标不应仅仅是发表论文，而是产生可靠的知识。至关重要的是，理解系统中的*因果关系*是生成可靠知识的最直接方式。而且有一种非常低成本的方法来研究因果关系：*消融研究*��消融研究包括系统地尝试去除系统的部分——使其更简单——以确定其性能实际来自何处。如果你发现X
    + Y + Z给你良好的结果，也尝试X、Y、Z、X + Y、X + Z和Y + Z，看看会发生什么。
- en: 'If you become a deep learning researcher, cut through the noise in the research
    process: do ablation studies for your models. Always ask, “Could there be a simpler
    explanation? Is this added complexity really necessary? Why?”'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你成为一名深度学习研究人员，请剔除研究过程中的噪声：为你的模型进行消融研究。始终问自己，“可能有一个更简单的解释吗？这种增加的复杂性真的有必要吗？为什么？”
- en: 9.3.2 Residual connections
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 残差连接
- en: You probably know about the game of Telephone, also called *Chinese whispers*
    in the UK and *téléphone arabe* in France, where an initial message is whispered
    in the ear of a player, who then whispers it in the ear of the next player, and
    so on. The final message ends up bearing little resemblance to its original version.
    It’s a fun metaphor for the cumulative errors that occur in sequential transmission
    over a noisy channel.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能知道电话游戏，也称为英国的*中国耳语*和法国的*阿拉伯电话*，其中一个初始消息被耳语给一个玩家，然后由下一个玩家耳语给下一个玩家，依此类推。最终的消息与其原始版本几乎没有任何相似之处。这是一个有趣的比喻，用于描述在嘈杂信道上的顺序传输中发生的累积错误。
- en: 'As it happens, backpropagation in a sequential deep learning model is pretty
    similar to the game of Telephone. You’ve got a chain of functions, like this one:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在顺序深度学习模型中的反向传播与电话游戏非常相似。你有一系列函数，就像这样：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The name of the game is to adjust the parameters of each function in the chain
    based on the error recorded on the output of `f4` (the loss of the model). To
    adjust `f1`, you’ll need to percolate error information through `f2`, `f3`, and
    `f4`. However, each successive function in the chain introduces some amount of
    noise. If your function chain is too deep, this noise starts overwhelming gradient
    information, and backpropagation stops working. Your model won’t train at all.
    This is the *vanishing gradients* problem.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的名字是根据记录在`f4`输出上的错误来调整链中每个函数的参数。要调整`f1`，你需要通过`f2`、`f3`和`f4`传播错误信息。然而，链中的每个连续函数都会引入一定量的噪声。如果你的函数链太深，这种噪声开始压倒梯度信息，反向传播停止工作。你的模型根本无法训练。这就是*梯度消失*问题。
- en: 'The fix is simple: just force each function in the chain to be nondestructive—to
    retain a noiseless version of the information contained in the previous input.
    The easiest way to implement this is to use a *residual connection*. It’s dead
    easy: just add the input of a layer or block of layers back to its output (see
    figure 9.9). The residual connection acts as an *information shortcut* around
    destructive or noisy blocks (such as blocks that contain `relu` activations or
    dropout layers), enabling error gradient information from early layers to propagate
    noiselessly through a deep network. This technique was introduced in 2015 with
    the ResNet family of models (developed by He et al. at Microsoft).[¹](../Text/09.htm#pgfId-1015871)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 修复很简单：只需强制链中的每个函数都是非破坏性的——保留前一个输入中包含的信息的无噪声版本。实现这一点的最简单方法是使用*残差连接*。这很简单：只需将层或层块的输入添加回其输出（见图9.9）。残差连接充当*信息捷径*，绕过具有破坏性或嘈杂块（例如包含`relu`激活或dropout层的块）的错误梯度信息，使其能够无噪声地通过深度网络传播。这项技术是在2015年由微软的He等人开发的ResNet系列模型中引入的。[¹](../Text/09.htm#pgfId-1015871)
- en: '![](../Images/09-09.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-09.png)'
- en: Figure 9.9 A residual connection around a processing block
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 处理块周围的残差连接
- en: In practice, you’d implement a residual connection as follows.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你可以这样实现一个残差连接。
- en: Listing 9.1 A residual connection in pseudocode
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.1 伪代码中的残差连接
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Some input tensor
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一些输入张量
- en: ❷ Save a pointer to the original input. This is called the residual.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保存原始输入的指针。这被称为残差。
- en: ❸ This computation block can potentially be destructive or noisy, and that’s
    fine.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这个计算块可能会具有破坏性或嘈杂，这没关系。
- en: '❹ Add the original input to the layer’s output: the final output will thus
    always preserve full information about the original input.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将原始输入添加到层的输出中：最终输出将始终保留有关原始输入的完整信息。
- en: Note that adding the input back to the output of a block implies that the output
    should have the same shape as the input. However, this is not the case if your
    block includes convolutional layers with an increased number of filters, or a
    max pooling layer. In such cases, use a 1 × 1 `Conv2D` layer with no activation
    to linearly project the residual to the desired output shape (see listing 9.2).
    You’d typically use `padding= "same"` in the convolution layers in your target
    block so as to avoid spatial downsampling due to padding, and you’d use strides
    in the residual projection to match any downsampling caused by a max pooling layer
    (see listing 9.3).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，将输入添加回块的输出意味着输出应��有与输入相同的形状。但是，如果您的块包括具有增加滤波器数量或最大池化层的卷积层，则情况并非如此。在这种情况下，使用没有激活的1
    × 1 `Conv2D`层线性地将残差投影到所需的输出形状（请参见列表9.2）。您通常会在目标块中的卷积层中使用`padding="same"`，以避免由于填充而导致空间下采样，并且您会在残差投影中使用步幅以匹配由最大池化层引起的任何下采样（请参见列表9.3）。
- en: Listing 9.2 Residual block where the number of filters changes
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 残差块，其中滤波器数量发生变化
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Set aside the residual.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将残差单独放在一边。
- en: '❷ This is the layer around which we create a residual connection: it increases
    the number of output filers from 32 to 64\. Note that we use padding="same" to
    avoid downsampling due to padding.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这是我们创建残差连接的层：它将输出滤波器的数量从32增加到64。请注意，我们使用padding="same"以避免由于填充而导致下采样。
- en: ❸ The residual only had 32 filters, so we use a 1 × 1 Conv2D to project it to
    the correct shape.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 残差只有32个滤波器，因此我们使用1 × 1 Conv2D将其投影到正确的形状。
- en: ❹ Now the block output and the residual have the same shape and can be added.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 现在块输出和残差具有相同的形状，可以相加。
- en: Listing 9.3 Case where the target block includes a max pooling layer
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3 目标块包含最大池化层的情况
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Set aside the residual.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将残差单独放在一边。
- en: '❷ This is the block of two layers around which we create a residual connection:
    it includes a 2 × 2 max pooling layer. Note that we use padding="same" in both
    the convolution layer and the max pooling layer to avoid downsampling due to padding.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这是我们创建残差连接的两层块：它包括一个2 × 2最大池化层。请注意，我们在卷积层和最大池化层中都使用padding="same"以避免由于填充而导致下采样。
- en: ❸ We use strides=2 in the residual projection to match the downsampling created
    by the max pooling layer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们在残差投影中使用strides=2以匹配由最大池化层创建的下采样。
- en: ❹ Now the block output and the residual have the same shape and can be added.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 现在块输出和残差具有相同的形状，可以相加。
- en: 'To make these ideas more concrete, here’s an example of a simple convnet structured
    into a series of blocks, each made of two convolution layers and one optional
    max pooling layer, with a residual connection around each block:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些想法更具体，这里是一个简单卷积网络的示例，结构化为一系列块，每个块由两个卷积层和一个可选的最大池化层组成，并在每个块周围有一个残差连接：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Utility function to apply a convolutional block with a residual connection,
    with an option to add max pooling
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 应用具有残差连接的卷积块的实用函数，可以选择添加最大池化
- en: ❷ If we use max pooling, we add a strided convolution to project the residual
    to the expected shape.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果我们使用最大池化，我们添加一个步幅卷积以将残差投影到预期形状。
- en: ❸ If we don’t use max pooling, we only project the residual if the number of
    channels has changed.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果我们不使用最大池化，只有在通道数量发生变化时才投影残差。
- en: ❹ First block
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 第一个块
- en: ❺ Second block; note the increasing filter count in each block.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 第二个块；请注意每个块中滤波器数量的增加。
- en: ❻ The last block doesn’t need a max pooling layer, since we will apply global
    average pooling right after it.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 最后一个块不需要最大池化层，因为我们将在其后立即应用全局平均池化。
- en: 'This is the model summary we get:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的模型摘要：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With residual connections, you can build networks of arbitrary depth, without
    having to worry about vanishing gradients.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用残差连接，您可以构建任意深度的网络，而无需担心梯度消失。
- en: 'Now let’s move on to the next essential convnet architecture pattern: *batch
    normalization*.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续下一个重要的卷积神经网络架构模式：*批量归一化*。
- en: 9.3.3 Batch normalization
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 批量归一化
- en: '*Normalization* is a broad category of methods that seek to make different
    samples seen by a machine learning model more similar to each other, which helps
    the model learn and generalize well to new data. The most common form of data
    normalization is one you’ve already seen several times in this book: centering
    the data on zero by subtracting the mean from the data, and giving the data a
    unit standard deviation by dividing the data by its standard deviation. In effect,
    this makes the assumption that the data follows a normal (or Gaussian) distribution
    and makes sure this distribution is centered and scaled to unit variance:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*归一化*是一类方法，旨在使机器学习模型看到的不同样本更相似，这有助于模型学习并很好地泛化到新数据。数据归一化的最常见形式是您在本书中已经多次看到的：通过从数据中减去均值使数据以零为中心，并通过将数据除以其标准差使数据具有单位标准差。实际上，这假设数据遵循正态（或高斯）分布，并确保该分布居中并缩放为单位方差：'
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Previous examples in this book normalized data before feeding it into models.
    But data normalization may be of interest after every transformation operated
    by the network: even if the data entering a `Dense` or `Conv2D` network has a
    0 mean and unit variance, there’s no reason to expect a priori that this will
    be the case for the data coming out. Could normalizing intermediate activations
    help?'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的先前示例在将数据馈送到模型之前对数据进行了归一化。但是数据归一化可能在网络操作的每次转换之后感兴趣：即使进入`Dense`或`Conv2D`网络的数据具有0均值和单位方差，也没有理由预期这将是数据输出的情况。归一化中间激活是否有帮助？
- en: Batch normalization does just that. It’s a type of layer (`BatchNormalization`
    in Keras) introduced in 2015 by Ioffe and Szegedy;[²](../Text/09.htm#pgfId-1017279)
    it can adaptively normalize data even as the mean and variance change over time
    during training. During training, it uses the mean and variance of the current
    batch of data to normalize samples, and during inference (when a big enough batch
    of representative data may not be available), it uses an exponential moving average
    of the batch-wise mean and variance of the data seen during training.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化就是这样。这是一种层类型（Keras 中的`BatchNormalization`），由 Ioffe 和 Szegedy 于 2015 年引入；[²](../Text/09.htm#pgfId-1017279)它可以在训练过程中随着均值和方差随时间变化而自适应地归一化数据。在训练过程中，它使用当前数据批次的均值和方差来归一化样本，在推断过程中（当可能没有足够大的代表性数据批次可用时），它使用训练过程中看到的数据的批次均值和方差的指数移动平均值。
- en: 'Although the original paper stated that batch normalization operates by “reducing
    internal covariate shift,” no one really knows for sure why batch normalization
    helps. There are various hypotheses, but no certitudes. You’ll find that this
    is true of many things in deep learning—deep learning is not an exact science,
    but a set of ever-changing, empirically derived engineering best practices, woven
    together by unreliable narratives. You will sometimes feel like the book you have
    in hand tells you *how* to do something but doesn’t quite satisfactorily say *why*
    it works: that’s because we know the how but we don’t know the why. Whenever a
    reliable explanation is available, I make sure to mention it. Batch normalization
    isn’t one of those cases.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管原始论文指出批量归一化通过“减少内部协变量转移”来运作，但没有人确切知道为什么批量归一化有帮助。有各种假设，但没有确定性。你会发现这在深度学习中很常见——深度学习不是一门确切的科学，而是一组不断变化的、经验性的最佳工程实践，由不可靠的叙事编织在一起。有时你会觉得手中的书告诉你*如何*做某事，但并没有完全令人满意地解释*为什么*它有效：这是因为我们知道如何做但不知道为什么。每当有可靠的解释时，我会确保提到。批量归一化不是这种情况之一。
- en: In practice, the main effect of batch normalization appears to be that it helps
    with gradient propagation—much like residual connections—and thus allows for deeper
    networks. Some very deep networks can only be trained if they include multiple
    `BatchNormalization` layers. For instance, batch normalization is used liberally
    in many of the advanced convnet architectures that come packaged with Keras, such
    as ResNet50, EfficientNet, and Xception.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，批量归一化的主要效果似乎是有助于梯度传播——就像残差连接一样——从而允许更深的网络。一些非常深的网络只有包含多个`BatchNormalization`层才能训练。例如，批量归一化在许多与
    Keras 捆绑在一起的高级卷积网络架构中被广泛使用，如 ResNet50、EfficientNet 和 Xception。
- en: 'The `BatchNormalization` layer can be used after any layer—`Dense`, `Conv2D`,
    etc.:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`BatchNormalization` 层可以在任何层之后使用——`Dense`、`Conv2D`等：'
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Because the output of the Conv2D layer gets normalized, the layer doesn’t
    need its own bias vector.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 因为 Conv2D 层的输出被归一化，所以该层不需要自己的偏置向量。
- en: Note Both `Dense` and `Conv2D` involve a *bias vector*, a learned variable whose
    purpose is to make the layer *affine* rather than purely linear. For instance,
    `Conv2D` returns, schematically, `y` `=` `conv(x,` `kernel)` `+` `bias`, and `Dense`
    returns `y` `=` `dot(x,` `kernel)` `+` `bias`. Because the normalization step
    will take care of centering the layer’s output on zero, the bias vector is no
    longer needed when using `BatchNormalization`, and the layer can be created without
    it via the option `use_bias=False`. This makes the layer slightly leaner.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`Dense`和`Conv2D`都涉及*偏置向量*，这是一个学习的变量，其目的是使层*仿射*而不是纯线性。例如，`Conv2D`返回，概略地说，`y`
    `=` `conv(x,` `kernel)` `+` `bias`，而`Dense`返回`y` `=` `dot(x,` `kernel)` `+` `bias`。因为归一化步骤将���理使层的输出以零为中心，所以在使用`BatchNormalization`时不再需要偏置向量，可以通过选项`use_bias=False`创建该层。这使得该层稍微更加精简。
- en: Importantly, I would generally recommend placing the previous layer’s activation
    *after* the batch normalization layer (although this is still a subject of debate).
    So instead of doing what is shown in listing 9.4, you would do what’s shown in
    listing 9.5.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我通常建议将前一层的激活*放在*批量归一化层之后（尽管这仍然是一个争论的话题）。所以，不要像列表 9.4 中所示那样做，而要像列表 9.5 中所示那样做。
- en: Listing 9.4 How not to use batch normalization
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.4 如何不使用批量归一化
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Listing 9.5 How to use batch normalization: the activation comes last'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.5 如何使用批量归一化：激活放在最后
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Note the lack of activation here.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 注意这里缺少激活。
- en: ❷ We place the activation after the BatchNormalization layer.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们将激活放在 BatchNormalization 层之后。
- en: 'The intuitive reason for this approach is that batch normalization will center
    your inputs on zero, while your `relu` activation uses zero as a pivot for keeping
    or dropping activated channels: doing normalization before the activation maximizes
    the utilization of the `relu`. That said, this ordering best practice is not exactly
    critical, so if you do convolution, then activation, and then batch normalization,
    your model will still train, and you won’t necessarily see worse results.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的直观原因是，批量归一化将使你的输入以零为中心，而你的`relu`激活使用零作为保留或丢弃激活通道的中心：在激活之前进行归一化最大化了`relu`的利用。也就是说，这种顺序最佳实践并不是绝对关键的，所以如果你进行卷积，然后激活，然后批量归一化，你的模型仍然会训练，并且不一定会看到更糟糕的结果。
- en: On batch normalization and fine-tuning
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 关于批量归一化和微调
- en: 'Batch normalization has many quirks. One of the main ones relates to fine-tuning:
    when fine-tuning a model that includes `BatchNormalization` layers, I recommend
    leaving these layers frozen (set their `trainable` attribute to `False`). Otherwise
    they will keep updating their internal mean and variance, which can interfere
    with the very small updates applied to the surrounding `Conv2D` layers.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化有许多怪癖。其中一个主要的怪癖与微调有关：在微调包含`BatchNormalization`层的模型时，我建议将这些层保持冻结（将它们的`trainable`属性设置为`False`）。否则，它们将继续更新其内部均值和方差，这可能会干扰周围`Conv2D`层应用的非常小的更新。
- en: 'Now let’s take a look at the last architecture pattern in our series: depthwise
    separable convolutions.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看我们系列中的最后一个架构模式：深度可分离卷积。
- en: 9.3.4 Depthwise separable convolutions
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.4 深度可分离卷积
- en: What if I told you that there’s a layer you can use as a drop-in replacement
    for `Conv2D` that will make your model smaller (fewer trainable weight parameters)
    and leaner (fewer floating-point operations) and cause it to perform a few percentage
    points better on its task? That is precisely what the *depthwise separable convolution*
    layer does (`SeparableConv2D` in Keras). This layer performs a spatial convolution
    on each channel of its input, independently, before mixing output channels via
    a pointwise convolution (a 1 × 1 convolution), as shown in figure 9.10.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我告诉你，有一种层可以作为`Conv2D`的即插即用替代品，可以使你的模型更小（可训练权重参数更少）、更精简（浮点操作更少），并使其在任务上表现更好几个百分点，你会怎么想？这正是*深度可分离卷积*层（Keras中的`SeparableConv2D`）所做的。这个层在每个输入通道上执行空间卷积，然后通过点卷积（1×1卷积）混合输出通道，如图9.10所示。
- en: '![](../Images/09-10.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-10.png)'
- en: 'Figure 9.10 Depthwise separable convolution: a depthwise convolution followed
    by a pointwise convolution'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 深度可分离卷积：深度卷积后跟点卷积
- en: This is equivalent to separating the learning of spatial features and the learning
    of channel-wise features. In much the same way that convolution relies on the
    assumption that the patterns in images are not tied to specific locations, depthwise
    separable convolution relies on the assumption that *spatial locations* in intermediate
    activations are *highly correlated*, but *different channels* are *highly independent*.
    Because this assumption is generally true for the image representations learned
    by deep neural networks, it serves as a useful prior that helps the model make
    more efficient use of its training data. A model with stronger priors about the
    structure of the information it will have to process is a better model—as long
    as the priors are accurate.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于将空间特征的学习与通道特征的学习分开。就像卷积依赖于图像中的模式不与特定位置绑定一样，深度可分离卷积依赖于中间激活中的*空间位置*高度相关，但*不同通道*高度独立。因为这个假设通常对深度神经网络学习到的图像表示是正确的，它作为一个有用的先验，帮助模型更有效地利用其训练数据。一个对其将要处理的信息结构有更强先验的模型是一个更好的模型——只要这些先验是准确的。
- en: Depthwise separable convolution requires significantly fewer parameters and
    involves fewer computations compared to regular convolution, while having comparable
    representational power. It results in smaller models that converge faster and
    are less prone to overfitting. These advantages become especially important when
    you’re training small models from scratch on limited data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积相比常规卷积需要更少的参数，并涉及更少的计算，同时具有可比较的表征能力。它导致更小的模型收敛更快，更不容易过拟合。当你在有限数据上从头开始训练小模型时，这些优势变得尤为���要。
- en: 'When it comes to larger-scale models, depthwise separable convolutions are
    the basis of the Xception architecture, a high-performing convnet that comes packaged
    with Keras. You can read more about the theoretical grounding for depthwise separable
    convolutions and Xception in the paper “Xception: Deep Learning with Depthwise
    Separable Convolutions.”[³](../Text/09.htm#pgfId-1018023)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '当涉及到大规模模型时，深度可分离卷积是Xception架构的基础，这是一个性能优异的卷积神经网络，与Keras捆绑在一起。你可以在论文“Xception:
    使用深度可分离卷积进行深度学习”中了解更多关于深度可分离卷积和Xception的理论基础。[³](../Text/09.htm#pgfId-1018023)'
- en: The co-evolution of hardware, software, and algorithms
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件、软件和算法的共同演进
- en: 'Consider a regular convolution operation with a 3 × 3 window, 64 input channels,
    and 64 output channels. It uses 3*3*64*64 = 36,864 trainable parameters, and when
    you apply it to an image, it runs a number of floating-point operations that is
    proportional to this parameter count. Meanwhile, consider an equivalent depthwise
    separable convolution: it only involves 3*3*64 + 64*64 = 4,672 trainable parameters,
    and proportionally fewer floating-point operations. This efficiency improvement
    only increases as the number of filters or the size of the convolution windows
    gets larger.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有3×3窗口、64个输入通道和64个输出通道的常规卷积操作。它使用了3*3*64*64 = 36,864个可训练参数，当你将其应用于图像时，它运行的浮点操作数量与这个参数数量成比例。同时，考虑一个等效的深度可分离卷积：它只涉及3*3*64
    + 64*64 = 4,672个可训练参数，并且浮点操作数量比例更少。这种效率改进只会随着滤波器数量或卷积窗口大小的增加而增加。
- en: 'As a result, you would expect depthwise separable convolutions to be dramatically
    faster, right? Hold on. This would be true if you were writing simple CUDA or
    C implementations of these algorithms—in fact, you do see a meaningful speedup
    when running on CPU, where the underlying implementation is parallelized C. But
    in practice, you’re probably using a GPU, and what you’re executing on it is far
    from a “simple” CUDA implementation: it’s a *cuDNN kernel*, a piece of code that
    has been extraordinarily optimized, down to each machine instruction. It certainly
    makes sense to spend a lot of effort optimizing this code, since cuDNN convolutions
    on NVIDIA hardware are responsible for many exaFLOPS of computation every day.
    But a side effect of this extreme micro-optimization is that alternative approaches
    have little chance to compete on performance—even approaches that have significant
    intrinsic advantages, like depthwise separable convolutions.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你会期望深度可分离卷积会明显更快，对吧？等一下。如果你正在编写这些算法的简单CUDA或C实现，这是正确的——事实上，在CPU上运行时，你确实会看到有意义的加速，其中底层实现是并行化的C。但实际上，你可能正在使用GPU，并且你在其上执行的远非“简单”的CUDA实现：它是一个*cuDNN内核*，这是一段被极致优化的代码，直到每个机器指令。花费大量精力优化这段代码是有意义的，因为NVIDIA硬件上的cuDNN卷积每天负责许多exaFLOPS的计算。但这种极端微观优化的副作用是，其他方法几乎没有机会在性能上竞争——即使是具有显著内在优势的方法，比如深度可分离卷积。
- en: 'Despite repeated requests to NVIDIA, depthwise separable convolutions have
    not benefited from nearly the same level of software and hardware optimization
    as regular convolutions, and as a result they remain only about as fast as regular
    convolutions, even though they’re using quadratically fewer parameters and floating-point
    operations. Note, though, that using depthwise separable convolutions remains
    a good idea even if it does not result in a speedup: their lower parameter count
    means that you are less at risk of overfitting, and their assumption that channels
    should be uncorrelated leads to faster model convergence and more robust representations.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管多次要求 NVIDIA 进行优化，深度可分离卷积并没有像常规卷积那样受益于几乎相同级别的软件和硬件优化，因此它们仍然只比常规卷积快，即使它们使用的参数和浮点运算量减少了平方倍。不过，需要注意的是，即使深度可分离卷积并没有加速，仍然是一个好主意：它们较低的参数数量意味着你不太容易过拟合，并且它们假设通道应该是不相关的导致模型收敛更快，表示更加稳健。
- en: 'What is a slight inconvenience in this case can become an impassable wall in
    other situations: because the entire hardware and software ecosystem of deep learning
    has been micro-optimized for a very specific set of algorithms (in particular,
    convnets trained via backpropagation), there’s an extremely high cost to steering
    away from the beaten path. If you were to experiment with alternative algorithms,
    such as gradient-free optimization or spiking neural networks, the first few parallel
    C++ or CUDA implementations you’d come up with would be orders of magnitude slower
    than a good old convnet, no matter how clever and efficient your ideas were. Convincing
    other researchers to adopt your method would be a tough sell, even if it were
    just plain better.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下的轻微不便可能在其他情况下变成一道不可逾越的障碍：因为整个深度学习的硬件和软件生态系统都被微调为一组非常特定的算法（特别是通过反向传播训练的卷积网络），所以偏离传统路线的成本极高。如果你尝试使用替代算法，比如无梯度优化或脉冲神经网络，那么你设计的前几个并行
    C++ 或 CUDA 实现将比一个老式的卷积网络慢几个数量级，无论你的想法多么聪明和高效。说服其他研究人员采纳你的方法将是一项艰巨的任务，即使它确实更好。
- en: 'You could say that modern deep learning is the product of a co-evolution process
    between hardware, software, and algorithms: the availability of NVIDIA GPUs and
    CUDA led to the early success of backpropagation-trained convnets, which led NVIDIA
    to optimize its hardware and software for these algorithms, which in turn led
    to consolidation of the research community behind these methods. At this point,
    figuring out a different path would require a multi-year re-engineering of the
    entire ecosystem.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，现代深度学习是硬件、软件和算法之间的共同演化过程的产物：NVIDIA GPU 和 CUDA 的可用性导致了反向传播训练的卷积网络的早期成功，这又促使
    NVIDIA 优化其硬件和软件以适应这些算法，进而导致研究社区围绕这些方法形成共识。在这一点上，找到一条不同的道路将需要对整个生态系统进行多年的重新设计。
- en: '9.3.5 Putting it together: A mini Xception-like model'
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.5 将其整合在一起：一个迷你 Xception 风格的模型
- en: 'As a reminder, here are the convnet architecture principles you’ve learned
    so far:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是迄今为止学到的卷积网络架构原则的提醒：
- en: Your model should be organized into repeated *blocks* of layers, usually made
    of multiple convolution layers and a max pooling layer.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的模型应该组织成重复的*层块*，通常由多个卷积层和一个最大池化层组成。
- en: The number of filters in your layers should increase as the size of the spatial
    feature maps decreases.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的层中的滤波器数量应随着空间特征图的大小减小而增加。
- en: Deep and narrow is better than broad and shallow.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深而窄比宽而浅更好。
- en: Introducing residual connections around blocks of layers helps you train deeper
    networks.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在层块周围引入残差连接有助于训练更深的网络。
- en: It can be beneficial to introduce batch normalization layers after your convolution
    layers.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在卷积层之后引入批量归一化层可能是有益的。
- en: It can be beneficial to replace `Conv2D` layers with `SeparableConv2D` layers,
    which are more parameter-efficient.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `Conv2D` 层替换为 `SeparableConv2D` 层可能是有益的，因为它们更节省参数。
- en: 'Let’s bring these ideas together into a single model. Its architecture will
    resemble a smaller version of Xception, and we’ll apply it to the dogs vs. cats
    task from the last chapter. For data loading and model training, we’ll simply
    reuse the setup we used in section 8.2.5, but we’ll replace the model definition
    with the following convnet:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些想法整合到一个单一模型中。其架构将类似于 Xception 的较小版本，并且我们将应用它到上一章的狗与猫任务中。对于数据加载和模型训练，我们将简单地重用我们在第
    8.2.5 节中使用的设置，但我们将用以下卷积网络替换模型定义：
- en: '[PRE20]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ We use the same data augmentation configuration as before.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们使用与之前相同的数据增强配置。
- en: ❷ Don’t forget input rescaling!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 不要忘记输入重新缩放！
- en: ❸ Note that the assumption that underlies separable convolution, “feature channels
    are largely independent,” does not hold for RGB images! Red, green, and blue color
    channels are actually highly correlated in natural images. As such, the first
    layer in our model is a regular Conv2D layer. We’ll start using SeparableConv2D
    afterwards.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 需要注意的是，支持可分离卷积的假设“特征通道在很大程度上是独立的”在 RGB 图像中并不成立！红色、绿色和蓝色通道在自然图像中实际上高度相关。因此，我们模型中的第一层是一个常规的
    Conv2D 层。之后我们将开始使用 SeparableConv2D。
- en: ❹ We apply a series of convolutional blocks with increasing feature depth. Each
    block consists of two batch-normalized depthwise separable convolution layers
    and a max pooling layer, with a residual connection around the entire block.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们���用一系列具有增加特征深度的卷积块。每个块由两个经过批量归一化的深度可分离卷积层和一个最大池化层组成，并在整个块周围有一个残差连接。
- en: ❺ In the original model, we used a Flatten layer before the Dense layer. Here,
    we go with a GlobalAveragePooling2D layer.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在原始模型中，我们在密集层之前使用了一个 Flatten 层。在这里，我们使用了一个 GlobalAveragePooling2D 层。
- en: ❻ Like in the original model, we add a dropout layer for regularization.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 像原始模型一样，我们为了正则化添加了一个 dropout 层。
- en: This convnet has a trainable parameter count of 721,857, slightly lower than
    the 991,041 trainable parameters of the original model, but still in the same
    ballpark. Figure 9.11 shows its training and validation curves.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这个卷积神经网络的可训练参数数量为721,857，略低于原始模型的991,041个可训练参数，但仍在同一数量级。图9.11显示了其训练和验证曲线。
- en: '![](../Images/09-11.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-11.png)'
- en: Figure 9.11 Training and validation metrics with an Xception-like architecture
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 具有类似Xception架构的训练和验证指标
- en: You’ll find that our new model achieves a test accuracy of 90.8%, compared to
    83.5% for the naive model in the last chapter. As you can see, following architecture
    best practices does have an immediate, sizable impact on model performance!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 您会发现我们的新模型的测试准确率为90.8%，而上一章中的朴素模型为83.5%。正如您所看到的，遵循架构最佳实践确实对模型性能产生了即时且显著的影响！
- en: At this point, if you want to further improve performance, you should start
    systematically tuning the hyperparameters of your architecture—a topic we’ll cover
    in detail in chapter 13\. We haven’t gone through this step here, so the configuration
    of the preceding model is purely based on the best practices we discussed, plus,
    when it comes to gauging model size, a small amount of intuition.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，如果您想进一步提高性能，您应该开始系统地调整架构的超参数 — 这是我们将在第13章中详细讨论的一个主题。我们在这里没有经历这一步骤，因此前述模型的配置纯粹基于我们讨论的最佳实践，再加上在评估模型大小时的一点直觉。
- en: Note that these architecture best practices are relevant to computer vision
    in general, not just image classification. For example, Xception is used as the
    standard convolutional base in DeepLabV3, a popular state-of-the-art image segmentation
    solution.[⁴](../Text/09.htm#pgfId-1018564)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些架构最佳实践适用于计算机视觉的一般情况，不仅仅是图像分类。例如，Xception被用作DeepLabV3中的标准卷积基础，这是一种流行的最先进的图像分割解决方案。
- en: 'This concludes our introduction to essential convnet architecture best practices.
    With these principles in hand, you’ll be able to develop higher-performing models
    across a wide range of computer vision tasks. You’re now well on your way to becoming
    a proficient computer vision practitioner. To further deepen your expertise, there’s
    one last important topic we need to cover: interpreting how a model arrives at
    its predictions.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对基本卷积神经网络架构最佳实践的介绍。有了这些原则，您将能够在各种计算机视觉任务中开发性能更高的模型。您现在已经在成为熟练的计算机视觉从业者的道路上走得很顺利。为了进一步加深您的专业知识，我们需要讨论最后一个重要主题：解释模型如何得出预测。
- en: 9.4 Interpreting what convnets learn
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 解释卷积神经网络学习的内容
- en: 'A fundamental problem when building a computer vision application is that of
    *interpretability*: *why* did your classifier think a particular image contained
    a fridge, when all you can see is a truck? This is especially relevant to use
    cases where deep learning is used to complement human expertise, such as in medical
    imaging use cases. We will end this chapter by getting you familiar with a range
    of different techniques for visualizing what convnets learn and understanding
    the decisions they make.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建计算机视觉应用程序时的一个基本问题是*可解释性*：当您只能看到一辆卡车时，为什么您的分类器认为特定图像包含一个冰箱？这在深度学习用于补充人类专业知识的用例中尤为重要，比如在医学成像用例中。我们将通过让您熟悉一系列不同的技术来结束本章，以便可视化卷积神经网络学习的内容并理解它们所做的决定。
- en: 'It’s often said that deep learning models are “black boxes”: they learn representations
    that are difficult to extract and present in a human-readable form. Although this
    is partially true for certain types of deep learning models, it’s definitely not
    true for convnets. The representations learned by convnets are highly amenable
    to visualization, in large part because they’re *representations of visual concepts*.
    Since 2013, a wide array of techniques has been developed for visualizing and
    interpreting these representations. We won’t survey all of them, but we’ll cover
    three of the most accessible and useful ones:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 人们常说深度学习模型是“黑匣子”：它们学习的表示很难以提取并以人类可读的形式呈现。尽管对于某些类型的深度学习模型来说这在一定程度上是正确的，但对于卷积神经网络来说绝对不是真的。卷积神经网络学��的表示非常适合可视化，这在很大程度上是因为它们是*视觉概念的表示*。自2013年以来，已经开发出了各种技术来可视化和解释这些表示。我们不会对它们进行全面调查，但我们将介绍其中三种最易于访问和有用的方法：
- en: '*Visualizing intermediate convnet outputs (intermediate activations)*—Useful
    for understanding how successive convnet layers transform their input, and for
    getting a first idea of the meaning of individual convnet filters'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可视化中间卷积网络输出（中间激活）* — 有助于理解连续的卷积网络层如何转换其输入，并初步了解单个卷积滤波器的含义'
- en: '*Visualizing convnet filters*—Useful for understanding precisely what visual
    pattern or concept each filter in a convnet is receptive to'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可视化卷积神经网络滤波器* — 有助于准确理解卷积神经网络中每个滤波器对哪种视觉模式或概念具有接受性'
- en: '*Visualizing heatmaps of class activation in an image*—Useful for understanding
    which parts of an image were identified as belonging to a given class, thus allowing
    you to localize objects in images'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可视化图像中类激活的热图* — 有助于理解图像的哪些部分被识别为属于给定类别，从而使您能够在图像中定位对象'
- en: For the first method—activation visualization—we’ll use the small convnet that
    we trained from scratch on the dogs-versus-cats classification problem in section
    8.2\. For the next two methods, we’ll use a pretrained Xception model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一种方法 — 激活可视化 — 我们将使用我们在第8.2节中从头开始在狗与猫分类问题上训练的小型卷积网络。对于接下来的两种方法，我们将使用一个预训练的Xception模型。
- en: 9.4.1 Visualizing intermediate activations
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 可视化中间激活
- en: 'Visualizing intermediate activations consists of displaying the values returned
    by various convolution and pooling layers in a model, given a certain input (the
    output of a layer is often called its *activation*, the output of the activation
    function). This gives a view into how an input is decomposed into the different
    filters learned by the network. We want to visualize feature maps with three dimensions:
    width, height, and depth (channels). Each channel encodes relatively independent
    features, so the proper way to visualize these feature maps is by independently
    plotting the contents of every channel as a 2D image. Let’s start by loading the
    model that you saved in section 8.2:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化中间激活包括显示模型中各种卷积和池化层返回的值，给定某个输入（层的输出通常称为*激活*，激活函数的输出）。这可以让我们看到输入是如何被网络学习的不同滤波器分解的。我们想要可视化具有三个维度的特征图：宽度、高度和深度（通道）。每个通道编码相对独立的特征，因此正确的可视化这些特征图的方式是独立绘制每个通道的内容作为2D图像。让我们从加载你在第8.2节保存的模型开始：
- en: '[PRE21]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Next, we’ll get an input image—a picture of a cat, not part of the images the
    network was trained on.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将获得一张输入图像——一张猫的图片，不是网络训练过的图片的一部分。
- en: Listing 9.6 Preprocessing a single image
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6 对单个图像进行预处理
- en: '[PRE22]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Download a test image.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 下载一个测试图片。
- en: ❷ Open the image file and resize it.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打开图像文件并调整大小。
- en: ❸ Turn the image into a float32 NumPy array of shape (180, 180, 3).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将图像转换为形状为（180, 180, 3）的float32 NumPy数组。
- en: ❹ Add a dimension to transform the array into a “batch” of a single sample.
    Its shape is now (1, 180, 180, 3).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加一个维度，将数组转换为“批量”中的单个样本。现在其形状为（1, 180, 180, 3）。
- en: Let’s display the picture (see figure 9.12).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示这张图片（见图9.12）。
- en: Listing 9.7 Displaying the test picture
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.7 显示测试图片
- en: '[PRE23]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/09-12.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-12.png)'
- en: Figure 9.12 The test cat picture
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 测试猫图片
- en: In order to extract the feature maps we want to look at, we’ll create a Keras
    model that takes batches of images as input, and that outputs the activations
    of all convolution and pooling layers.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取我们想要查看的特征图，我们将创建一个接受图像批量作为输入的Keras���型，并输出所有卷积和池化层的激活。
- en: Listing 9.8 Instantiating a model that returns layer activations
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.8 实例化一个返回层激活的模型
- en: '[PRE24]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Extract the outputs of all Conv2D and MaxPooling2D layers and put them in
    a list.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提取所有Conv2D和MaxPooling2D层的输出，并将它们放入列表中。
- en: ❷ Save the layer names for later.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保存层的名称以备后用。
- en: ❸ Create a model that will return these outputs, given the model input.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个模型，给定模型输入，将返回这些输出。
- en: 'When fed an image input, this model returns the values of the layer activations
    in the original model, as a list. This is the first time you’ve encountered a
    multi-output model in this book in practice since you learned about them in chapter
    7; until now, the models you’ve seen have had exactly one input and one output.
    This one has one input and nine outputs: one output per layer activation.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入一张图像时，这个模型会返回原始模型中层的激活值，作为一个列表。这是你在本书中第一次实际遇到多输出模型，因为你在第7章学习过它们；到目前为止，你看到的模型都只有一个输入和一个输出。这个模型有一个输入和九个输出：每个层激活一个输出。
- en: Listing 9.9 Using the model to compute layer activations
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.9 使用模型计算层激活
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '❶ Return a list of nine NumPy arrays: one array per layer activation.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回一个包含九个NumPy数组的列表：每个数组代表一层的激活。
- en: 'For instance, this is the activation of the first convolution layer for the
    cat image input:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这是原始模型第一卷积层对猫图像输入的激活：
- en: '[PRE26]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It’s a 178 × 178 feature map with 32 channels. Let’s try plotting the fifth
    channel of the activation of the first layer of the original model (see figure
    9.13).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个具有32个通道的178×178特征图。让我们尝试绘制原始模型第一层激活的第五个通道（见图9.13）。
- en: Listing 9.10 Visualizing the fifth channel
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.10 可视化第五个通道
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/09-13.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-13.png)'
- en: Figure 9.13 Fifth channel of the activation of the first layer on the test cat
    picture
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 测试猫图片上第一层激活的第五个通道
- en: This channel appears to encode a diagonal edge detector—but note that your own
    channels may vary, because the specific filters learned by convolution layers
    aren’t deterministic.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这个通道似乎编码了一个对角边缘检测器，但请注意，你自己的通道可能会有所不同，因为卷积层学习的特定滤波器并不是确定性的。
- en: Now, let’s plot a complete visualization of all the activations in the network
    (see figure 9.14). We’ll extract and plot every channel in each of the layer activations,
    and we’ll stack the results in one big grid, with channels stacked side by side.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制网络中所有激活的完整可视化（见图9.14）。我们将提取并绘制每个层激活中的每个通道，并将结果堆叠在一个大网格中，通道并排堆叠。
- en: Listing 9.11 Visualizing every channel in every intermediate activation
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.11 可视化每个中间激活的每个通道
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Iterate over the activations (and the names of the corresponding layers).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 迭代激活（和相应层的名称）。
- en: ❷ The layer activation has shape (1, size, size, n_features).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 层激活的形状为（1, size, size, n_features）。
- en: ❸ Prepare an empty grid for displaying all the channels in this activation.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 准备一个空网格，用于显示该激活中的所有通道。
- en: ❹ This is a single channel (or feature).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 这是一个单通道（或特征）。
- en: ❺ Normalize channel values within the [0, 255] range. All-zero channels are
    kept at zero.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将通道值归一化到[0, 255]范围内。所有零通道保持为零。
- en: ❻ Place the channel matrix in the empty grid we prepared.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将通道矩阵放入我们准备好的空网格中。
- en: ❼ Display the grid for the layer.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 显示该层的网格。
- en: '![](../Images/09-14.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-14.png)'
- en: Figure 9.14 Every channel of every layer activation on the test cat picture
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 测试猫图片上每个层激活的每个通道
- en: 'There are a few things to note here:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几点需要注意：
- en: The first layer acts as a collection of various edge detectors. At that stage,
    the activations retain almost all of the information present in the initial picture.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层充当各种边缘检测器的集合。在这个阶段，激活保留了初始图片中几乎所有的信息。
- en: As you go deeper, the activations become increasingly abstract and less visually
    interpretable. They begin to encode higher-level concepts such as “cat ear” and
    “cat eye.” Deeper presentations carry increasingly less information about the
    visual contents of the image, and increasingly more information related to the
    class of the image.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着深入，激活变得越来越抽象，越来越难以直观解释。它们开始编码更高级别的概念，如“猫耳朵”和“猫眼”。更深层次的表现包含的关于图像视觉内容的信息越来越少，包含的与图像类别相关的信息越来越多。
- en: 'The sparsity of the activations increases with the depth of the layer: in the
    first layer, almost all filters are activated by the input image, but in the following
    layers, more and more filters are blank. This means the pattern encoded by the
    filter isn’t found in the input image.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活的稀疏性随着层的深度增加而增加：在第一层中，几乎所有滤波器都被输入图像激活，但在后续层中，越来越多的滤波器为空白。这意味着滤波器编码的模式在输入图像中找不到。
- en: 'We have just evidenced an important universal characteristic of the representations
    learned by deep neural networks: the features extracted by a layer become increasingly
    abstract with the depth of the layer. The activations of higher layers carry less
    and less information about the specific input being seen, and more and more information
    about the target (in this case, the class of the image: cat or dog). A deep neural
    network effectively acts as an *information distillation pipeline*, with raw data
    going in (in this case, RGB pictures) and being repeatedly transformed so that
    irrelevant information is filtered out (for example, the specific visual appearance
    of the image), and useful information is magnified and refined (for example, the
    class of the image).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚证明了深度神经网络学习的表示的一个重要普遍特征：随着层深度的增加，由层提取的特征变得越来越抽象。更高层的激活包含的关于特定输入的信息越来越少，包含的关于目标的信息越来越多（在本例中，图像的类别：猫或狗）。深度神经网络有效地充当*信息蒸馏管道*，原始数据输入（在本例中是RGB图片），并被反复转换，以便过滤掉不相关的信息（例如，图像的具体视觉外观），并放大和精炼有用的信息（例如，图像的类别）。
- en: 'This is analogous to the way humans and animals perceive the world: after observing
    a scene for a few seconds, a human can remember which abstract objects were present
    in it (bicycle, tree) but can’t remember the specific appearance of these objects.
    In fact, if you tried to draw a generic bicycle from memory, chances are you couldn’t
    get it even remotely right, even though you’ve seen thousands of bicycles in your
    lifetime (see, for example, figure 9.15). Try it right now: this effect is absolutely
    real. Your brain has learned to completely abstract its visual input—to transform
    it into high-level visual concepts while filtering out irrelevant visual details—making
    it tremendously difficult to remember how things around you look.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于人类和动物感知世界的方式：观察一个场景几秒钟后，人类可以记住其中存在的抽象对象（自行车，树），但无法记住这些对象的具体外观。事实上，如果你试图凭记忆画一辆普通的自行车，很可能你无法得到一个近似正确的结果，即使你一生中见过成千上万辆自行车（例如，参见图9.15）。现在就试试吧：这种效应绝对是真实的。你的大脑已经学会完全抽象化其视觉输入——将其转化为高级视觉概念，同时过滤掉不相关的视觉细节——这使得记住你周围事物的外观变得极其困难。
- en: '![](../Images/09-15.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-15.png)'
- en: 'Figure 9.15 Left: attempts to draw a bicycle from memory. Right: what a schematic
    bicycle should look like.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 左：试图凭记忆画一辆自行车。右：原理图自行车的样子。
- en: 9.4.2 Visualizing convnet filters
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 可视化卷积滤波器
- en: 'Another easy way to inspect the filters learned by convnets is to display the
    visual pattern that each filter is meant to respond to. This can be done with
    *gradient ascent in input space*: applying *gradient descent* to the value of
    the input image of a convnet so as to *maximize* the response of a specific filter,
    starting from a blank input image. The resulting input image will be one that
    the chosen filter is maximally responsive to.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 检查卷积网络学习的滤波器的另一种简单方法是显示每个滤波器应该响应的视觉模式。这可以通过*输入空间中的梯度上升*来实现：将*梯度下降*应用于卷积网络的输入图像的值，以*最大化*特定滤波器的响应，从一个空白输入图像开始。生成的输入图像将是所选滤波器最大响应的图像。
- en: 'Let’s try this with the filters of the Xception model, pretrained on ImageNet.
    The process is simple: we’ll build a loss function that maximizes the value of
    a given filter in a given convolution layer, and then we’ll use stochastic gradient
    descent to adjust the values of the input image so as to maximize this activation
    value. This will be our second example of a low-level gradient descent loop leveraging
    the `GradientTape` object (the first one was in chapter 2).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用在ImageNet上预训练的Xception模型的滤波器。这个过程很简单：我们将构建一个损失函数，最大化给定卷积层中给定滤波器的值，然后我们将使用随机梯度下降来调整输入图像的值，以最大化这个激活值。这将是我们利用`GradientTape`对象进行低级梯度下降循环的第二个示例（第一个示例在第2章中）。
- en: First, let’s instantiate the Xception model, loaded with weights pretrained
    on the ImageNet dataset.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实例化加载了在ImageNet数据集上预训练权重的Xception模型。
- en: Listing 9.12 Instantiating the Xception convolutional base
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.12 实例化Xception卷积基础
- en: '[PRE29]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ The classification layers are irrelevant for this use case, so we don’t include
    the top stage of the model.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 分类层对于这个用例是无关紧要的，所以我们不包括模型的顶层。
- en: We’re interested in the convolutional layers of the model—the `Conv2D` and `SeparableConv2D`
    layers. We’ll need to know their names so we can retrieve their outputs. Let’s
    print their names, in order of depth.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型的卷积层感兴趣——`Conv2D`和`SeparableConv2D`层。我们需要知道它们的名称，以便检索它们的输出。让我们按深度顺序打印它们的名称。
- en: Listing 9.13 Printing the names of all convolutional layers in Xception
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.13 打印Xception中所有卷积层的名称
- en: '[PRE30]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You’ll notice that the `SeparableConv2D` layers here are all named something
    like `block6_sepconv1`, `block7_sepconv2`, etc. Xception is structured into blocks,
    each containing several convolutional layers.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这里的`SeparableConv2D`层都被命名为类似`block6_sepconv1`、`block7_sepconv2`等。Xception被结构化为包含几个卷积层的块。
- en: 'Now, let’s create a second model that returns the output of a specific layer—a
    *feature extractor* model. Because our model is a Functional API model, it is
    inspectable: we can query the `output` of one of its layers and reuse it in a
    new model. No need to copy the entire Xception code.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个第二个模型，返回特定层的输出——一个*特征提取器*模型。因为我们的模型是一个功能 API 模型，它是可检查的：我们可以查询其一个层的
    `output` 并在新模型中重用它。无需复制整个 Xception 代码。
- en: Listing 9.14 Creating a feature extractor model
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 第9.14节 创建特征提取器模型
- en: '[PRE31]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ You could replace this with the name of any layer in the Xception convolutional
    base.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 您可以将其替换为 Xception 卷积基中的任何层的名称。
- en: ❷ This is the layer object we’re interested in.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这是我们感兴趣的层对象。
- en: ❸ We use model.input and layer.output to create a model that, given an input
    image, returns the output of our target layer.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们使用 model.input 和 layer.output 来创建一个模型，给定一个输入图像，返回我们目标层的输出。
- en: To use this model, simply call it on some input data (note that Xception requires
    inputs to be preprocessed via the `keras.applications.xception.preprocess_input`
    function).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个模型，只需在一些输入数据上调用它（请注意，Xception 需要通过 `keras.applications.xception.preprocess_input`
    函数对输入进行预处理）。
- en: Listing 9.15 Using the feature extractor
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 第9.15节 使用特征提取器
- en: '[PRE32]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let’s use our feature extractor model to define a function that returns a scalar
    value quantifying how much a given input image “activates” a given filter in the
    layer. This is the “loss function” that we’ll maximize during the gradient ascent
    process:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用我们的特征提取器模型定义一个函数，该函数返回一个标量值，量化给定输入图像在给定层中“激活”给定滤波器的程度。这是我们在梯度上升过程中将最大化的“损失函数”：
- en: '[PRE33]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ The loss function takes an image tensor and the index of the filter we are
    considering (an integer).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 损失函数接受一个图像张量和我们正在考虑的滤波器的索引（一个整数）。
- en: ❷ Note that we avoid border artifacts by only involving non-border pixels in
    the loss; we discard the first two pixels along the sides of the activation.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 请注意，我们通过仅涉及损失中的非边界像素来避免边界伪影；我们丢弃激活边缘两侧的前两个像素。
- en: ❸ Return the mean of the activation values for the filter.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回滤波器激活值的平均值。
- en: The difference between `model.predict(x)` and `model(x)`
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.predict(x)` 和 `model(x)` 的区别'
- en: In the previous chapter, we used `predict(x)` for feature extraction. Here,
    we’re using `model(x)`. What gives?
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用 `predict(x)` 进行特征提取。在这里，我们使用 `model(x)`。这是为什么？
- en: Both `y` `=` `model.predict(x)` and `y` `=` `model(x)` (where `x` is an array
    of input data) mean “run the model on `x` and retrieve the output `y`.” Yet they
    aren’t exactly the same thing.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '`y` `=` `model.predict(x)` 和 `y` `=` `model(x)`（其中 `x` 是输入数据的数组）都表示“在 `x` 上运行模型并检索输出
    `y`”。然而它们并不完全相同。'
- en: '`predict()` loops over the data in batches (in fact, you can specify the batch
    size via `predict(x,` `batch_size=64)`), and it extracts the NumPy value of the
    outputs. It’s schematically equivalent to this:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()` 在批处理中循环数据（实际上，您可以通过 `predict(x,` `batch_size=64)` 指定批处理大小），并提取输出的
    NumPy 值。它在原理上等同于这样：'
- en: '[PRE34]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This means that `predict()` calls can scale to very large arrays. Meanwhile,
    `model(x)` happens in-memory and doesn’t scale. On the other hand, `predict()`
    is not differentiable: you cannot retrieve its gradient if you call it in a `GradientTape`
    scope.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 `predict()` 调用可以扩展到非常大的数组。与此同时，`model(x)` 在内存中进行，不会扩展。另一方面，`predict()`
    不可微分：如果在 `GradientTape` 范围内调用它，则无法检索其梯度。
- en: You should use `model(x)` when you need to retrieve the gradients of the model
    call, and you should use `predict()` if you just need the output value. In other
    words, always use `predict()` unless you’re in the middle of writing a low-level
    gradient descent loop (as we are now).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要检索模型调用的梯度时，应该使用 `model(x)`，如果只需要输出值，则应该使用 `predict()`。换句话说，除非您正在编写低级梯度下降循环（就像我们现在所做的那样），否则始终使用
    `predict()`。
- en: Let’s set up the gradient ascent step function, using the `GradientTape`. Note
    that we’ll use a `@tf.function` decorator to speed it up.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置梯度上升步骤函数，使用 `GradientTape`。请注意，我们将使用 `@tf.function` 装饰器来加快速度。
- en: A non-obvious trick to help the gradient descent process go smoothly is to normalize
    the gradient tensor by dividing it by its L2 norm (the square root of the average
    of the square of the values in the tensor). This ensures that the magnitude of
    the updates done to the input image is always within the same range.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助梯度下降过程顺利进行的一个不明显的技巧是通过将梯度张量除以其 L2 范数（张量中值的平方的平均值的平方根）来对梯度张量进行归一化。这确保了对输入图像的更新的幅度始终在相同范围内。
- en: Listing 9.16 Loss maximization via stochastic gradient ascent
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 第9.16节 通过随机梯度上升最大化损失
- en: '[PRE35]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Explicitly watch the image tensor, since it isn’t a TensorFlow Variable (only
    Variables are automatically watched in a gradient tape).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 明确监视图像张量，因为它不是 TensorFlow 变量（只有变量在梯度磁带中会自动被监视）。
- en: ❷ Compute the loss scalar, indicating how much the current image activates the
    filter.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算损失标量，指示当前图像激活滤波器的程度。
- en: ❸ Compute the gradients of the loss with respect to the image.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算损失相对于图像的梯度。
- en: ❹ Apply the “gradient normalization trick.”
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 应用“梯度归一化技巧”。
- en: ❺ Move the image a little bit in a direction that activates our target filter
    more strongly.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将图像稍微移动到更强烈激活目标滤波器的方向。
- en: ❻ Return the updated image so we can run the step function in a loop.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 返回更新后的图像，以便我们可以在循环中运行步骤函数。
- en: Now we have all the pieces. Let’s put them together into a Python function that
    takes as input a layer name and a filter index, and returns a tensor representing
    the pattern that maximizes the activation of the specified filter.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有的部分。让我们将它们组合成一个Python函数，该函数接受一个层名称和一个滤波器索引作为输入，并返回表示最大化指定滤波器激活的模式的张量。
- en: Listing 9.17 Function to generate filter visualizations
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 第9.17节 生成滤波器可视化的函数
- en: '[PRE36]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Number of gradient ascent steps to apply
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 应用的梯度上升步骤数
- en: ❷ Amplitude of a single step
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 单步幅的振幅
- en: ❸ Initialize an image tensor with random values (the Xception model expects
    input values in the [0, 1] range, so here we pick a range centered on 0.5).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用随机值初始化图像张量（Xception 模型期望输入值在 [0, 1] 范围内，因此这里选择以 0.5 为中心的范围）。
- en: ❹ Repeatedly update the values of the image tensor so as to maximize our loss
    function.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 反复更新图像张量的值，以最大化我们的损失函数。
- en: The resulting image tensor is a floating-point array of shape `(200,` `200,`
    `3)`, with values that may not be integers within `[0,` `255]`. Hence, we need
    to post-process this tensor to turn it into a displayable image. We do so with
    the following straightforward utility function.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图像张量是一个形状为`(200,` `200,` `3)`的浮点数组，其值可能不是在`[0,` `255]`范围内的整数。因此，我们需要对这个张量进行后处理，将其转换为可显示的图像。我们使用以下简单的实用函数来实现。
- en: Listing 9.18 Utility function to convert a tensor into a valid image
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.18 将张量转换为有效图像的实用函数
- en: '[PRE37]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Normalize image values within the [0, 255] range.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将图像值归一化到`[0, 255]`范围内。
- en: ❷ Center crop to avoid border artifacts.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 中心裁剪以避免边缘伪影。
- en: 'Let’s try it (see figure 9.16):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试（见图9.16）：
- en: '[PRE38]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](../Images/09-16.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-16.png)'
- en: Figure 9.16 Pattern that the second channel in layer `block3_sepconv1` responds
    to maximally
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 `block3_sepconv1`层中第二通道响应最大的模式
- en: It seems that filter 0 in layer `block3_sepconv1` is responsive to a horizontal
    lines pattern, somewhat water-like or fur-like.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来`block3_sepconv1`层中的滤波器0对水平线模式有响应，有点类似水或毛皮。
- en: 'Now the fun part: you can start visualizing every filter in the layer, and
    even every filter in every layer in the model.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看有趣的部分：你可以开始可视化每一层中的每一个滤波器，甚至是模型中每一层中的每一个滤波器。
- en: Listing 9.19 Generating a grid of all filter response patterns in a layer
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.19 生成层中所有滤波器响应模式的网格
- en: '[PRE39]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ Generate and save visualizations for the first 64 filters in the layer.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成并保存层中前64个滤波器的可视化。
- en: ❷ Prepare a blank canvas for us to paste filter visualizations on.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 准备一个空白画布，供我们粘贴滤波器可视化。
- en: ❸ Fill the picture with the saved filters.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用保存的滤波器填充图片。
- en: ❹ Save the canvas to disk.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将画布保存到磁盘。
- en: 'These filter visualizations (see figure 9.17) tell you a lot about how convnet
    layers see the world: each layer in a convnet learns a collection of filters such
    that their inputs can be expressed as a combination of the filters. This is similar
    to how the Fourier transform decomposes signals onto a bank of cosine functions.
    The filters in these convnet filter banks get increasingly complex and refined
    as you go deeper in the model:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这些滤波器可视化（见图9.17）告诉你很多关于卷积神经网络层如何看待世界的信息：卷积神经网络中的每一层学习一组滤波器，以便它们的输入可以被表达为滤波器的组合。这类似于傅里叶变换将信号分解为一组余弦函数的方式。随着你在模型中深入，这些卷积神经网络滤波器组中的滤波器变得越来越复杂和精细：
- en: The filters from the first layers in the model encode simple directional edges
    and colors (or colored edges, in some cases).
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型中第一层的滤波器编码简单的方向边缘和颜色（或在某些情况下是彩色边缘）。
- en: The filters from layers a bit further up the stack, such as `block4_sepconv1`,
    encode simple textures made from combinations of edges and colors.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位于堆栈中稍微靠上的层，如`block4_sepconv1`，编码由边缘和颜色组合而成的简单纹理。
- en: 'The filters in higher layers begin to resemble textures found in natural images:
    feathers, eyes, leaves, and so on.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高层的滤波器开始类似于自然图像中发现的纹理：羽毛、眼睛、叶子等。
- en: '![](../Images/09-17.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-17.png)'
- en: Figure 9.17 Some filter patterns for layers `block2_sepconv1`, `block4_sepconv1`,
    and `block8_sepconv1`
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 层`block2_sepconv1`、`block4_sepconv1`和`block8_sepconv1`的一些滤波器模式
- en: 9.4.3 Visualizing heatmaps of class activation
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.3 可视化类激活热图
- en: We’ll introduce one last visualization technique—one that is useful for understanding
    which parts of a given image led a convnet to its final classification decision.
    This is helpful for “debugging” the decision process of a convnet, particularly
    in the case of a classification mistake (a problem domain called *model interpretability*).
    It can also allow you to locate specific objects in an image.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍最后一种可视化技术——这对于理解哪些部分的图像导致卷积神经网络做出最终分类决策是有用的。这对于“调试”卷积神经网络的决策过程特别有帮助，尤其是在分类错误的情况下（一个称为*模型可解释性*的问题领域）。它还可以让你在图像中定位特定的对象。
- en: This general category of techniques is called *class activation map* (CAM) visualization,
    and it consists of producing heatmaps of class activation over input images. A
    class activation heatmap is a 2D grid of scores associated with a specific output
    class, computed for every location in any input image, indicating how important
    each location is with respect to the class under consideration. For instance,
    given an image fed into a dogs-versus-cats convnet, CAM visualization would allow
    you to generate a heatmap for the class “cat,” indicating how cat-like different
    parts of the image are, and also a heatmap for the class “dog,” indicating how
    dog-like parts of the image are.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这类技术的通用类别称为*类激活映射*（CAM）可视化，它包括在输入图像上生成类激活热图。类激活热图是与特定输出类相关联的一组分数的2D网格，针对任何输入图像中的每个位置计算，指示每个位置相对于考虑的类的重要性。例如，给定一个输��到狗与猫卷积神经网络中的图像，CAM可视化将允许你为“猫”类生成一个热图，指示图像的不同部分有多像猫，还可以为“狗”类生成一个热图，指示图像的哪些部分更像狗。
- en: 'The specific implementation we’ll use is the one described in an article titled
    “Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.”[⁵](../Text/09.htm#pgfId-1023006)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将使用的具体实现是一篇名为“Grad-CAM: 基于梯度定位的深度网络的视觉解释”的文章中描述的实现。'
- en: Grad-CAM consists of taking the output feature map of a convolution layer, given
    an input image, and weighing every channel in that feature map by the gradient
    of the class with respect to the channel. Intuitively, one way to understand this
    trick is to imagine that you’re weighting a spatial map of “how intensely the
    input image activates different channels” by “how important each channel is with
    regard to the class,” resulting in a spatial map of “how intensely the input image
    activates the class.”
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Grad-CAM包括获取给定输入图像的卷积层的输出特征图，并通过类别相对于通道的梯度对该特征图中的每个通道进行加权。直观地，理解这个技巧的一种方式是想象你正在通过“输入图像如何激活不同通道”的空间地图来“每个通道对于类别的重要性有多大”，从而产生一个“输入图像如何激活类别”的空间地图。
- en: Let’s demonstrate this technique using the pretrained Xception model.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用预训练的Xception模型演示这种技术。
- en: Listing 9.20 Loading the Xception network with pretrained weights
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.20 加载带有预训练权重的Xception网络
- en: '[PRE40]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Note that we include the densely connected classifier on top; in all previous
    cases, we discarded it.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 请注意，我们在顶部包含了密集连接的分类器；在所有以前的情况下，我们都将其丢弃。
- en: 'Consider the image of two African elephants shown in figure 9.18, possibly
    a mother and her calf, strolling on the savanna. Let’s convert this image into
    something the Xception model can read: the model was trained on images of size
    299 × 299, preprocessed according to a few rules that are packaged in the `keras.applications.xception
    .preprocess_input` utility function. So we need to load the image, resize it to
    299 × 299, convert it to a NumPy `float32` tensor, and apply these preprocessing
    rules.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图9.18中显示的两只非洲大象的图像，可能是母象和幼象，在热带草原上漫步。让我们将这幅图像转换为Xception模型可以读取的内容：该模型是在大小为299×299的图像上训练的，根据`keras.applications.xception
    .preprocess_input`实用程序函数中打包的一些规则进行预处理。因此，我们需要加载图像，将其调整大小为299×299，将其转换为NumPy的`float32`张量，并应用这些预处理规则。
- en: Listing 9.21 Preprocessing an input image for Xception
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.21 为Xception预处理输入图像
- en: '[PRE41]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ Download the image and store it locally under the path img_path.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 下载图像并将其存储在本地路径img_path下。
- en: ❷ Return a Python Imaging Library (PIL) image of size 299 × 299.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回一个大小为299×299的Python Imaging Library（PIL）图像。
- en: ❸ Return a float32 NumPy array of shape (299, 299, 3).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回一个形状为（299，299，3）的float32 NumPy数组。
- en: ❹ Add a dimension to transform the array into a batch of size (1, 299, 299,
    3).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加一个维度，将数组转换为大小为（1，299，299，3）的批处理。
- en: ❺ Preprocess the batch (this does channel-wise color normalization).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 预处理批处理（这样做是按通道进行颜色归一化）。
- en: '![](../Images/09-18.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-18.png)'
- en: Figure 9.18 Test picture of African elephants
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 非洲大象的测试图片
- en: 'You can now run the pretrained network on the image and decode its prediction
    vector back to a human-readable format:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以在图像上运行预训练网络，并将其预测向量解码回人类可读格式：
- en: '[PRE42]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The top three classes predicted for this image are as follows:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 该图像的前三个预测类别如下：
- en: African elephant (with 87% probability)
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非洲大象（概率为87%）
- en: Tusker (with 7% probability)
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 雄象（概率为7%）
- en: Indian elephant (with 2% probability)
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 印度大象（概率为2%）
- en: 'The network has recognized the image as containing an undetermined quantity
    of African elephants. The entry in the prediction vector that was maximally activated
    is the one corresponding to the “African elephant” class, at index 386:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 网络已将图像识别为包含非洲大象数量不确定的图像。预测向量中最大激活的条目对应于“非洲大象”类别，索引为386：
- en: '[PRE43]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: To visualize which parts of the image are the most African-elephant–like, let’s
    set up the Grad-CAM process.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化图像的哪些部分最像非洲大象，让我们设置Grad-CAM过程。
- en: First, we create a model that maps the input image to the activations of the
    last convolutional layer.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个模型，将输入图像映射到最后一个卷积层的激活。
- en: Listing 9.22 Setting up a model that returns the last convolutional output
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.22 设置返回最后一个卷积输出的模型
- en: '[PRE44]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Second, we create a model that maps the activations of the last convolutional
    layer to the final class predictions.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们创建一个模型，将最后一个卷积层的激活映射到最终的类别预测。
- en: Listing 9.23 Reapplying the classifier on top of the last convolutional output
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.23 重新应用最后一个卷积输出的分类器
- en: '[PRE45]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Then we compute the gradient of the top predicted class for our input image
    with respect to the activations of the last convolution layer.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算输入图像的顶部预测类别相对于最后一个卷积层的激活的梯度。
- en: Listing 9.24 Retrieving the gradients of the top predicted class
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.24 检索顶部预测类别的梯度
- en: '[PRE46]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ❶ Compute activations of the last conv layer and make the tape watch it.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算最后一个卷积层的激活并让磁带观察它。
- en: ❷ Retrieve the activation channel corresponding to the top predicted class.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检索与顶部预测类别对应的激活通道。
- en: ❸ This is the gradient of the top predicted class with regard to the output
    feature map of the last convolutional layer.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这是顶部预测类别相对于最后一个卷积层的输出特征图的梯度。
- en: Now we apply pooling and importance weighting to the gradient tensor to obtain
    our heatmap of class activation.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对梯度张量应用池化和重要性加权，以获得我们的类别激活热图。
- en: Listing 9.25 Gradient pooling and channel-importance weighting
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.25 梯度池化和通道重要性加权
- en: '[PRE47]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: ❶ This is a vector where each entry is the mean intensity of the gradient for
    a given channel. It quantifies the importance of each channel with regard to the
    top predicted class.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是一个向量，其中每个条目是给定通道的梯度的平均强度。它量化了每个通道相对于顶部预测类的重要性。
- en: ❷ Multiply each channel in the output of the last convolutional layer by “how
    important this channel is.”
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将最后一个卷积层的输出中的每个通道乘以“这个通道的重要性”。
- en: ❸ The channel-wise mean of the resulting feature map is our heatmap of class
    activation.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 结果特征图的通道均值是我们的类别激活热图。
- en: For visualization purposes, we’ll also normalize the heatmap between 0 and 1\.
    The result is shown in figure 9.19.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化目的，我们还将将热图归一化到0和1之间。结果显示在图9.19中。
- en: Listing 9.26 Heatmap post-processing
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.26 热图后处理
- en: '[PRE48]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](../Images/09-19.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-19.png)'
- en: Figure 9.19 Standalone class activation heatmap
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 独立类别激活热图
- en: Finally, let’s generate an image that superimposes the original image on the
    heatmap we just obtained (see figure 9.20).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们生成一幅将原始图像叠加在我们刚刚获得的热图上的图像（见图9.20）。
- en: Listing 9.27 Superimposing the heatmap on the original picture
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.27 将热图叠加在原始图片上
- en: '[PRE49]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ Load the original image.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载原始图像。
- en: ❷ Rescale the heatmap to the range 0–255.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将热图重新缩放到0-255的范围。
- en: ❸ Use the "jet" colormap to recolorize the heatmap.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用“jet”颜色图重新着色热图。
- en: ❹ Create an image that contains the recolorized heatmap.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建包含重新着色的热图的图像。
- en: ❺ Superimpose the heatmap and the original image, with the heatmap at 40% opacity.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将热图和原始图像叠加，热图透明度为40%。
- en: ❻ Save the superimposed image.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 保存叠加的图像。
- en: '![](../Images/09-20.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-20.png)'
- en: Figure 9.20 African elephant class activation heatmap over the test picture
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 测试图片上的非洲象类激活热图
- en: 'This visualization technique answers two important questions:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可视化技术回答了两个重要问题：
- en: Why did the network think this image contained an African elephant?
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络为什么认为这幅图像包含非洲象？
- en: Where is the African elephant located in the picture?
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非洲象在图片中的位置在哪里？
- en: 'In particular, it’s interesting to note that the ears of the elephant calf
    are strongly activated: this is probably how the network can tell the difference
    between African and Indian elephants.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 特别值得注意的是，小象的耳朵被强烈激活：这可能是网络区分非洲象和印度象的方式。
- en: Summary
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'There are three essential computer vision tasks you can do with deep learning:
    image classification, image segmentation, and object detection.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用深度学习执行三项基本的计算机视觉任务：图像分类、图像分割和目标检测。
- en: Following modern convnet architecture best practices will help you get the most
    out of your models. Some of these best practices include using residual connections,
    batch normalization, and depthwise separable convolutions.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵循现代卷积神经网络架构的最佳实践将帮助您充分利用您的模型。其中一些最佳实践包括使用残差连接、批量归一化和深度可分离卷积。
- en: The representations that convnets learn are easy to inspect—convnets are the
    opposite of black boxes!
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络学习的表示易于检查——卷积神经网络与黑匣子相反！
- en: You can generate visualizations of the filters learned by your convnets, as
    well as heatmaps of class activity.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以生成卷积神经网络学习的滤波器的可视化，以及类活动的热图。
- en: '* * *'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[¹](../Text/09.htm#Id-1015871) Kaiming He et al., “Deep Residual Learning for
    Image Recognition,” Conference on Computer Vision and Pattern Recognition (2015),
    [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385).'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '[¹](../Text/09.htm#Id-1015871) Kaiming He等，“深度残差学习用于图像识别”，计算机视觉与模式识别会议（2015），[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)。'
- en: '[²](../Text/09.htm#Id-1017279) Sergey Ioffe and Christian Szegedy, “Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift,” *Proceedings
    of the 32nd International Conference on Machine Learning* (2015), [https:// arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[²](../Text/09.htm#Id-1017279) Sergey Ioffe和Christian Szegedy，“批量归一化：通过减少内部协变量转移加速深度网络训练”，*第32届国际机器学习会议论文集*（2015），[https://
    arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)。'
- en: '[³](../Text/09.htm#Id-1018023) François Chollet, “Xception: Deep Learning with
    Depthwise Separable Convolutions,” Conference on Computer Vision and Pattern Recognition
    (2017), [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357).'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '[³](../Text/09.htm#Id-1018023) François Chollet，“Xception：使用深度可分离卷积的深度学习”，计算机视觉与模式识别会议（2017），[https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)。'
- en: '[⁴](../Text/09.htm#Id-1018564) Liang-Chieh Chen et al., “Encoder-Decoder with
    Atrous Separable Convolution for Semantic Image Segmentation,” ECCV (2018), [https://arxiv.org/abs/1802.02611](https://arxiv.org/abs/1802.02611).'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[⁴](../Text/09.htm#Id-1018564) Liang-Chieh Chen等，“具有空洞可分离卷积的编码器-解码器用于语义图像分割”，ECCV（2018），[https://arxiv.org/abs/1802.02611](https://arxiv.org/abs/1802.02611)。'
- en: '[⁵](../Text/09.htm#Id-1023006) Ramprasaath R. Selvaraju et al., arXiv (2017),
    [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.0239).'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[⁵](../Text/09.htm#Id-1023006) Ramprasaath R. Selvaraju等，arXiv（2017），[https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.0239)。'
