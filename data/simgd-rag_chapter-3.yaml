- en: '3 Indexing pipeline: Creating a knowledge base for RAG'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 索引管道：为RAG创建知识库
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Data loading
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据加载
- en: Text splitting or chunking
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本拆分或分块
- en: Converting text to embeddings
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本转换为嵌入
- en: Storing embeddings in vector databases
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在向量数据库中存储嵌入
- en: Examples in Python using LangChain
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LangChain 的 Python 示例
- en: In chapter 2, we discussed the main components of retrieval-augmented generation
    (RAG) systems. You may recall that the indexing pipeline creates the knowledge
    base or the non-parametric memory of RAG applications. An indexing pipeline needs
    to be set up before the real-time user interaction with the large language model
    (LLM) can begin.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们讨论了检索增强生成（RAG）系统的主要组件。您可能还记得，索引管道创建了RAG应用的知识库或非参数记忆。在开始与大型语言模型（LLM）的实时用户交互之前，需要设置索引管道。
- en: This chapter elaborates on the four components of the indexing pipeline. We
    begin by discussing data loading, which involves connecting to the source, extracting
    files, and parsing text. At this stage, we introduce a framework called LangChain,
    which has become increasingly popular in the LLM app developer community. Next,
    we elaborate on the need for data splitting or chunking and discuss chunking strategies.
    Embeddings is an important design pattern in the world of AI and ML. We explore
    embeddings in detail and how they are relevant in the RAG context. Finally, we
    look at a new storage technique called vector storage and the databases that facilitate
    it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细阐述了索引管道的四个组件。我们首先讨论数据加载，这涉及到连接到源、提取文件和解析文本。在这个阶段，我们介绍了一个称为LangChain的框架，它在LLM应用开发者社区中越来越受欢迎。接下来，我们详细阐述数据拆分或分块的需求，并讨论分块策略。嵌入是AI和ML世界中的一个重要设计模式。我们详细探讨了嵌入及其在RAG环境中的相关性。最后，我们探讨了一种新的存储技术——向量存储以及促进其发展的数据库。
- en: By the end of this chapter, you should have a solid understanding of how a knowledge
    base, or the non-parametric memory of a RAG application, is created. We also embellish
    this chapter with snippets of Python code, so those of you who are so inclined
    can try a hands-on development of the indexing pipeline.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您应该对如何创建知识库（或 RAG 应用程序的参数外记忆）有一个稳固的理解。我们还用 Python 代码片段丰富了本章内容，以便那些有兴趣的人可以尝试亲手开发索引管道。
- en: By the end of this chapter, you should
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您应该
- en: Know how to extract data from sources.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何从来源中提取数据。
- en: Get a deeper understanding of text-chunking strategies.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解文本分块策略。
- en: Learn what embeddings are and how they are used.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习嵌入是什么以及它们是如何被使用的。
- en: Gain knowledge of vector storage and vector databases.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掌握向量存储和向量数据库的知识。
- en: Have an end-to-end knowledge of setting up the indexing pipeline.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对设置索引管道有端到端的知识。
- en: 3.1 Data loading
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 数据加载
- en: This section focuses on the first stage of the indexing pipeline. You will read
    about data loaders, metadata information, and data transformers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍索引管道的第一阶段。您将了解数据加载器、元数据信息和数据转换器。
- en: The first step toward building a knowledge base (or non-parametric memory) of
    a RAG system is to source data from its original location. This data may be in
    the form of Word documents, PDF files, CSV, HTML, and similar. Furthermore, the
    data may be stored in file, block, or object stores, in data lakes, data warehouses,
    or even in third-party sources that can be accessed via the open internet. This
    process of sourcing data from its original location is called *data loading*.
    Loading documents from a list of sources may turn out to be a complicated process.
    Therefore, it is advisable to document all the sources and the file formats in
    advance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 构建RAG系统（或非参数记忆）的知识库的第一步是从其原始位置获取数据。这些数据可能是Word文档、PDF文件、CSV、HTML等形式。此外，数据可能存储在文件、块或对象存储中，在数据湖、数据仓库中，甚至在可以通过开放互联网访问的第三方来源中。从原始位置获取数据的过程称为*数据加载*。从一系列来源加载文档可能是一个复杂的过程。因此，建议提前记录所有来源和文件格式。
- en: Before going too deep, let’s begin with a simple example. If you recall, in
    chapter 1, we used Wikipedia as a source of information about the 2023 Cricket
    World Cup. At that time, we copied the opening paragraph of the article and pasted
    it in the ChatGPT prompt window. Instead of doing it manually, we will now *connect*
    to Wikipedia and *extract* the data programmatically, using a very popular framework
    called LangChain. The code in this chapter and the book can be run on Python notebooks
    and is available in the GitHub repository of this book ([https://mng.bz/a9DJ](https://mng.bz/a9DJ)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入之前，让我们从一个简单的例子开始。如果你还记得，在第1章中，我们使用维基百科作为关于2023年板球世界杯信息来源。当时，我们复制了文章的开头段落并将其粘贴到ChatGPT提示窗口中。现在，我们将不再手动操作，而是通过一个名为LangChain的非常流行的框架来*连接*到维基百科并*提取*数据。本章和本书中的代码可以在Python笔记本上运行，并在本书的GitHub仓库中提供（[https://mng.bz/a9DJ](https://mng.bz/a9DJ)）。
- en: Note LangChain is an open source framework developed by Harrison Chase and launched
    in October 2022\. It was written in Python and JavaScript and designed for building
    applications using LLMs. Apart from being suitable for RAG, LangChain is also
    suitable for building application use cases such as chatbots, document summarizers,
    synthetic data generation, and more. Over time, LangChain has built integrations
    with LLM providers such as OpenAI, Anthropic, and Hugging Face; a variety of vector
    store providers; cloud storage systems such as AWS, Google, Azure, and SQL and
    NoSQL databases; and APIs for news, weather, and similar. Although LangChain has
    received some criticism, it is still a good starting point for developers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意LangChain是一个由Harrison Chase开发的开源框架，于2022年10月发布。它用Python和JavaScript编写，旨在用于构建使用LLMs的应用程序。除了适合RAG之外，LangChain也适合构建聊天机器人、文档摘要器、合成数据生成等应用场景。随着时间的推移，LangChain已经与LLM提供商（如OpenAI、Anthropic和Hugging
    Face）、各种向量存储提供商、云存储系统（如AWS、Google、Azure）以及新闻、天气等API建立了集成。尽管LangChain受到了一些批评，但它仍然是开发者的一个良好起点。
- en: Installing LangChain
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 安装LangChain
- en: To install LangChain (we’ll use the version 0.3.19 in this chapter) using `pip`,
    run
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`pip`安装LangChain（在本章中我们将使用版本0.3.19），请运行
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `langchain-community` package contains third-party integrations. It is
    automatically installed by LangChain, but in case it does not work, you can also
    install it separately using `pip`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`langchain-community`包包含第三方集成。LangChain会自动安装它，但如果它不起作用，你也可以使用`pip`单独安装它：'
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now that you have installed LangChain, we will use it to connect to Wikipedia
    and extract data from the page about the 2023 Cricket World Cup. For this task,
    we will use the `AsyncHtmlLoader` function from the `document_loaders`library
    in the `langchain-community` package. To run `AsyncHtmlLoader`, we will have to
    install another Python package called bs4:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经安装了LangChain，我们将使用它连接到维基百科并从关于2023年板球世界杯的页面中提取数据。为此任务，我们将使用`langchain-community`包中的`document_loaders`库的`AsyncHtmlLoader`函数。要运行`AsyncHtmlLoader`，我们必须安装另一个名为bs4的Python包：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `data` variable in the code now stores the information from the Wikipedia
    page.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的`data`变量现在存储了来自维基百科页面的信息。
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here is the output (A large section of the text is replaced with periods to
    save space.)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出（为了节省空间，大量文本被替换为句点。）
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The variable `data` is a list of documents that includes two elements: `page_content`
    and `metadata`. `page_content` contains the text sourced from the URL. You will
    notice that the text along with the relevant information also has newline characters
    (`\n`) and other HTML tags; however, `metadata` contains another important data
    aspect.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`data`是一个包含两个元素的文档列表：`page_content`和`metadata`。`page_content`包含从URL获取的文本。你会注意到文本以及相关信息还包括换行符（`\n`）和其他HTML标签；然而，`metadata`包含另一个重要的数据方面。
- en: Metadata is information about the data (e.g., type, origin, and purpose). This
    can include a data summary; the way the data was created; who created it and why;
    when it was created; and the size, quality, and condition of the data. Metadata
    information comes in extremely handy in the retrieval stage. Also, it can be used
    to resolve conflicting information that can arise due to chronology or origin.
    In the previous example, while extracting the data from the URL, Wikipedia has
    already provided the source, title, and language in the metadata information.
    For many data sources, you will have to add metadata.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据是关于数据的信息（例如，类型、来源和目的）。这可以包括数据摘要；数据创建的方式；谁创建了它以及为什么；创建时间；以及数据的大小、质量和状况。在检索阶段，元数据信息非常有用。此外，它还可以用于解决由于时间顺序或来源可能出现的冲突信息。在先前的例子中，在从URL提取数据时，维基百科已经在元数据信息中提供了来源、标题和语言。对于许多数据来源，您可能需要添加元数据。
- en: Often, a *cleaning* of the source data is required. The data in our example
    has a lot of new line characters and HTML tags, which requires a certain level
    of cleanup. We will attempt to clean up the webpage data that we extracted using
    the `Html2Text­Transformer` function from the `document_transformers` library
    in the `langchain-community` package. For `Html2TextTransformer`*,* we will also
    have to install another package called `html2text`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，需要对源数据进行“清理”。我们例子中的数据有很多换行符和HTML标签，这需要一定程度的清理。我们将尝试使用来自`langchain-community`包中的`document_transformers`库的`Html2Text­Transformer`函数清理我们提取的网页数据。对于`Html2TextTransformer`，我们还需要安装另一个名为`html2text`的包。
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the `page_content` is now free of any HTML tags and contains
    only the text from the webpage:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的`page_content`输出已经没有任何HTML标签，只包含网页上的文本：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The text is more coherent now since we have removed the HTML part of the data.
    There can be further cleanup, such as removing special characters and other unnecessary
    information. Data cleaning also removes duplication. Yet another step to include
    in the data-loading stage can be masking of sensitive information such as PII
    (Personally Identifiable Information) or company secrets. In some cases, a fact
    check may also be required.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经移除了数据的HTML部分，文本现在更加连贯。可以进行进一步的清理，例如删除特殊字符和其他不必要的信息。数据清理还会消除重复。在数据加载阶段，还可以包括对敏感信息的掩码，如PII（个人信息标识符）或公司机密。在某些情况下，可能还需要进行事实核查。
- en: The source for our data was Wikipedia (more precisely, a web address pointing
    to a Wikipedia page), and the format was HTML. The source can also be other storage
    locations such as AWS S3, SQL/NoSQL databases, Google Drive, GitHub, even WhatsApp,
    YouTube, and other social media sites. Likewise, the data formats can be .doc,
    .pdf, .csv, .ppt, .eml, and the like. Most of the time, you will be able to use
    frameworks such as LangChain that have integrations for the sources and the formats
    already built in. Sometimes, you may have to build custom connectors and loaders.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据来源是维基百科（更确切地说，是一个指向维基百科页面的网址），格式是HTML。来源也可以是其他存储位置，如AWS S3、SQL/NoSQL数据库、Google
    Drive、GitHub，甚至是WhatsApp、YouTube和其他社交媒体网站。同样，数据格式可以是.doc、.pdf、.csv、.ppt、.eml等。大多数情况下，您将能够使用已经内置了源和格式集成的框架，如LangChain。有时，您可能需要构建自定义连接器和加载器。
- en: Although data loading may seem simple (after all, it’s just connecting to a
    source and extracting data), the nuances of adding metadata, document transformation,
    masking, and similar add complexity to this step. Advanced planning of the sources,
    a review of the formats, and curation of metadata information are advised for
    best results.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据加载可能看起来很简单（毕竟，它只是连接到源并提取数据），但添加元数据、文档转换、掩码和类似的细微差别增加了这一步骤的复杂性。为了获得最佳结果，建议进行数据源的先进规划、格式审查和元数据信息的整理。
- en: 'We have now taken the first step toward building our RAG system. The data-loading
    process can be further broken down into four sub-steps, as shown in figure 3.1:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经迈出了构建我们的RAG系统第一步。数据加载过程可以进一步细分为四个子步骤，如图3.1所示：
- en: '![A diagram of a data conversion process'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![数据转换流程图](../Images/CH03_F01_Kimothi.png)'
- en: AI-generated content may be incorrect.](../Images/CH03_F01_Kimothi.png)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH03_F01_Kimothi.png)
- en: Figure 3.1  Four sub-steps of the data-loading component of the indexing pipeline
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1  索引管道数据加载组件的四个子步骤
- en: Connect to the source of data.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到数据源。
- en: Extract text from the file.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文件中提取文本。
- en: Review and update metadata information.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 审查和更新元数据信息。
- en: Clean or transform the data.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理或转换数据。
- en: We have now obtained data from the source and cleaned it to an extent. This
    Wikipedia page that we have loaded has more than 8,000 words, alone. Imagine the
    number of words if we had multiple documents. For efficient management of information,
    we employ something called data splitting, which will be discussed in the next
    section.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经从源数据中获取了数据，并将其清理到一定程度。我们加载的这个维基百科页面本身就有超过8,000个单词。想象一下，如果我们有多个文档，单词的数量会有多少。为了有效地管理信息，我们采用了一种称为数据拆分的方法，这将在下一节中讨论。
- en: 3.2 Data splitting (chunking)
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 数据拆分（分块）
- en: Breaking down long pieces of text to manageable segments is called *data splitting*
    or *chunking*. This section discusses why chunking is necessary and the different
    chunking strategies. We also use functions from LangChain to illustrate a few
    examples.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 将长文本分解成可管理的片段称为*数据拆分*或*分块*。本节讨论为什么需要分块以及不同的分块策略。我们还使用LangChain的功能来展示几个示例。
- en: 3.2.1 Advantages of chunking
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 分块的优势
- en: 'In the previous section, we loaded the data from a URL (a Wikipedia page) and
    extracted the text. It was a long piece of text of approximately 8,000 words.
    When it comes to overcoming the major limitations of using long pieces of text
    in LLM applications, chunking offers the following three advantages:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们从URL（一个维基百科页面）加载数据并提取了文本。这是一篇大约有8,000个单词的长文本。当涉及到克服在LLM应用中使用长文本的主要限制时，分块提供了以下三个优势：
- en: '*Context window of LLM**s*—Due to the inherent nature of the technology, the
    number of tokens (loosely, words) LLMs can work with at a time is limited. This
    includes both the number of tokens in the prompt (or the input) and in the completion
    (or the output). The limit on the total number of tokens that an LLM can process
    in one go is called “the context window size.” If we pass an input that is longer
    than the context window size, the LLM chooses to ignore all text beyond the size.
    It is thus very important to be careful with the amount of text being passed to
    the LLM. In our example, a text of 50,000 words will not work well with LLMs that
    have a smaller context window. The way to address this problem is to break the
    text down into smaller chunks.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM的上下文窗口*—由于技术的固有性质，LLM一次可以处理的标记（大致相当于单词）数量是有限的。这包括提示（或输入）中的标记数量以及完成（或输出）中的标记数量。一个LLM一次可以处理的标记总数限制被称为“上下文窗口大小”。如果我们传递的输入超过了上下文窗口大小，LLM会选择忽略超过大小的所有文本。因此，非常重要的是要小心处理传递给LLM的文本数量。在我们的例子中，一篇有50,000个单词的文本将无法与上下文窗口较小的LLM很好地工作。解决这个问题的方式是将文本分解成更小的部分。'
- en: '*Lost-in-the-middle proble**m*—Even in those LLMs that have a long context
    window (e.g., Claude 3 by Anthropic has a context window of up to 200,000 tokens),
    a problem with accurately reading the information has been observed. It has been
    noticed that accuracy declines dramatically if the relevant information is somewhere
    in the middle of the prompt. This problem can be addressed by passing only the
    relevant information to the LLM instead of the entire document.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*中间丢失问题*—即使在那些具有较长的上下文窗口的LLM中（例如，Anthropic的Claude 3上下文窗口高达200,000个标记），观察到准确读取信息的问题。已经注意到，如果相关信息位于提示的中间部分，准确性会急剧下降。通过只传递相关信息给LLM而不是整个文档，可以解决这个问题。'
- en: '*Ease of searc**h*—This is not a problem with the LLM per se, but it has been
    observed that large chunks of text are harder to search over. When we use a retriever
    (recall the generation pipeline introduced in chapter 2), it is more efficient
    to search over smaller pieces of text.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索的便利性*—这本身并不是LLM的问题，但观察到大量文本更难进行搜索。当我们使用检索器（回想一下第2章中引入的生成管道）时，搜索较小的文本片段会更有效率。'
- en: DEFINITION Tokens are the fundamental semantic units used in natural language
    processing (NLP) tasks. Tokens can be assumed to be words, but sometimes, a single
    word can be made up of more than one token. OpenAI suggests one token to be made
    of four characters or 0.75 words. Tokens are important as most proprietary LLMs
    are priced based on token usage.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：标记是自然语言处理（NLP）任务中使用的根本语义单元。标记可以假设是单词，但有时，一个单词可以由多个标记组成。OpenAI建议一个标记由四个字符或0.75个单词组成。标记很重要，因为大多数专有LLM的定价是基于标记使用的。
- en: 3.2.2 Chunking process
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 分块过程
- en: 'The chunking process can be divided into three steps, as illustrated in figure
    3.2:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 分块过程可以分为三个步骤，如图3.2所示：
- en: Divide the longer text into compact, meaningful units (e.g., sentences or paragraphs).
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将较长的文本分割成紧凑、有意义的单元（例如，句子或段落）。
- en: Merge the smaller units into larger chunks until a specific size is achieved.
    After that, this chunk is treated as an independent segment of text.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将较小的单元合并成较大的块，直到达到特定的尺寸。之后，这个块被视为一个独立的文本段。
- en: When creating a new chunk, include a part of the previous chunk at the start
    of the new chunk. This overlap is necessary to maintain contextual continuity.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建新的块时，将前一个块的一部分包含在新块的开始处。这种重叠是必要的，以保持上下文的连续性。
- en: This process is also known as “small to big” chunking.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程也被称为“从小到大”的分块。
- en: '![Diagram of a diagram of a large chunking unit'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![大块分块单位的示意图'
- en: AI-generated content may be incorrect.](../Images/CH03_F02_Kimothi.png)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: AI 生成的可能是不正确的。](../Images/CH03_F02_Kimothi.png)
- en: Figure 3.2  Data-chunking process
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.2 数据分块过程
- en: 3.2.3 Chunking methods
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 分块方法
- en: 'While splitting documents into chunks might sound like a simple concept, multiple
    methods can be employed to execute chunking. The following two aspects vary across
    the chunking methodologies:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将文档分割成块听起来像是一个简单的概念，但在执行分块时可以采用多种方法。以下两个方面的差异存在于不同的分块方法中：
- en: The manner of text splitting
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分割的方式
- en: Measuring of the chunk size
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量块的大小
- en: Fixed-Size Chunking
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 固定大小块分块
- en: 'A very common approach is to predetermine the size of the chunk and the amount
    of overlap between the chunks. The following two methods fall under the *fixed-size
    chunking* category:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常常见的方法是预先确定块的大小和块之间的重叠量。以下两种方法属于 *固定大小块分块* 类别：
- en: '*Split by characte**r*—Here, we specify a certain character, such as a newline
    character `\n` or a special character `*`, to determine how the text should be
    split. The text is split into a unit whenever this character is encountered. The
    chunk size is measured in the number of characters. We must choose the chunk size
    or the number of characters we need in each chunk. We can also choose the number
    of characters we need to overlap between two chunks. We will look at an example
    and demonstrate this method using `CharacterTextSplitter` from `langchain.text_splitters`.
    For this, we will take the same document that we loaded and transformed in the
    previous section from Wikipedia and store it in the variable `html_data_transformed.`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*按字符分割*——在这里，我们指定一个特定的字符，例如换行符 `\n` 或特殊字符 `*`，以确定文本应该如何分割。每当遇到这个字符时，文本就会被分割成单元。块的大小以字符数来衡量。我们必须选择块的大小或每个块中需要的字符数。我们还可以选择两个块之间需要重叠的字符数。我们将通过一个示例来查看，并使用来自
    `langchain.text_splitters` 的 `CharacterTextSplitter` 方法演示这种方法。为此，我们将从上一节中加载并转换的维基百科文档存储在变量
    `html_data_transformed` 中。'
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This method created 64 chunks. But what about the overlap? Let’s check two chunks
    at random, say, chunks 4 and 5\. We will compare the last 200 characters
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法创建了 64 个块。但重叠部分如何？让我们随机检查两个块，比如说，块 4 和块 5。我们将比较两个连续块的最后 200 个字符
- en: 'of chunk 4 with the first 200 characters of chunk 5:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将块 4 与块 5 的前 200 个字符合并：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Comparing the two outputs, we can observe that there is an overlap between the
    two consecutive chunks.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 比较两个输出，我们可以观察到两个连续块之间存在重叠。
- en: Splitting by character is a simple and effective way to create chunks. It is
    the first chunking method that anyone should try. However, sometimes, it may not
    be feasible to create chunks within the specified length. This is because the
    sequential occurrence of the character on which the text needs to be split is
    far apart. To address this problem, a recursive approach is employed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 按字符分割是一种简单而有效的方法来创建块。这是任何人都应该尝试的第一个分块方法。然而，有时在指定的长度内创建块可能不可行。这是因为需要分割文本的字符的顺序出现相隔甚远。为了解决这个问题，采用了递归方法。
- en: '*Recursively split by character—*This method is quite like the split by character
    but instead of specifying a single character for splitting, we specify a list
    of characters. The approach initially tries creating chunks based on the first
    character. In case it is not able to create a chunk of the specified size using
    the first character, it then uses the next character to further break down chunks
    to the required size. This method ensures that chunks are largely created within
    the specified size. This method is recommended for generic texts. You can use
    `RecursiveCharacter­TextSplitter` from LangChain to use this method. The only
    difference in `RecursiveCharacterTextSplitter`is that instead of passing a single
    character in the separator parameter `separator=``"``\n``"`, we will need to pass
    a list `separators= [``"``\n\n``"``,``"``\n``"``,` `"``.``"``,` `"` `"``]`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*递归按字符分割—*这种方法与按字符分割类似，但不是指定单个字符进行分割，而是指定一个字符列表。该方法最初尝试根据第一个字符创建块。如果无法使用第一个字符创建指定大小的块，它将使用下一个字符进一步分解块到所需的大小。这种方法确保块主要在指定的大小内创建。这种方法适用于通用文本。你可以使用LangChain中的`RecursiveCharacter­TextSplitter`来使用这种方法。`RecursiveCharacterTextSplitter`的唯一区别在于，我们不需要在分隔符参数`separator=``"``\n``"`中传递单个字符，而是需要传递一个列表`separators=
    [``"``\n\n``"``,``"``\n``"``,` `"``.``"``,` `"` `"``]`。'
- en: Another perspective to consider with fixed-sized chunking is the use of tokens.
    As shown at the beginning of this section, tokens are the fundamental units of
    NLP. They can be understood loosely as a proxy for words. All LLMs process text
    in the form of tokens. So, it would also make sense to use tokens to determine
    the size of the chunks. This method is called the *split by token method*. Here,
    the splitting still happens based on a character, but the size of the chunk and
    the overlap are determined by the number of tokens instead of the number of characters.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在固定大小分块中考虑的另一个角度是使用标记。如本节开头所示，标记是NLP的基本单位。它们可以粗略地理解为单词的代理。所有LLM都以标记的形式处理文本。因此，使用标记来确定块的大小也是有意义的。这种方法被称为*按标记分割方法*。在这里，分割仍然基于字符，但块的大小和重叠由标记的数量而不是字符的数量决定。
- en: NOTE Tokenizers are used to create tokens from a piece of text. Tokens are slightly
    different from words. A phrase such as “I’d like that!” has three words; however,
    in NLP, this text may be parsed as five tokens, that is, “I”, “‘d”, “like”, “that”,
    “!”. Different LLMs use different methods for creating tokens. OpenAI uses a tokenizer
    called tiktoken for GPT3.5 and GPT4 models;Llama2 by Meta uses LLamaTokenizer,
    available in the transformers library by Hugging Face. You can also explore other
    tokenizers on Hugging Face. NLTK and spaCy are some other popular libraries that
    can be used as tokenizers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：标记化器用于从文本片段中创建标记。标记与单词略有不同。例如，“I’d like that!”有三个单词；然而，在NLP中，此文本可能被解析为五个标记，即“
    I”，“‘d”，“like”，“that”，“!”。不同的LLM使用不同的方法来创建标记。OpenAI使用tiktoken标记器为GPT3.5和GPT4模型创建标记；Meta的Llama2使用Hugging
    Face的transformers库中的LLamaTokenizer。你还可以在Hugging Face上探索其他标记化器。NLTK和spaCy是一些其他流行的库，可以用作标记化器。
- en: To use the split by token method, you can use specific methods within the `Recursive­CharacterTextSplitter`  and  `CharacterTextSplitter`
    classes, such as `Recursive­CharacterTextSplitter.from_tiktoken_encoder` `(encoding=``"``cl100k_base``"``,
    chunk_size=100,` `chunk_overlap=10)` for creating chunks of 100 tokens with an
    overlap of 10 tokens using OpenAI’s tiktoken tokenizer or `CharacterTextSplitter.from_``huggingface_tokenizer(tokenizer,`
    `chunk_size=100,` `chunk_overlap=10)` for creating the same sized chunk using
    another tokenizer from Hugging Face.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用按标记分割的方法，你可以在`Recursive­CharacterTextSplitter`和`CharacterTextSplitter`类中使用特定方法，例如使用OpenAI的tiktoken标记器创建具有10个标记重叠的100个标记块，可以使用`Recursive­CharacterTextSplitter.from_tiktoken_encoder`(encoding=``"``cl100k_base``"``,
    chunk_size=100,` `chunk_overlap=10)`，或者使用来自Hugging Face的另一个标记器创建相同大小的块，可以使用`CharacterTextSplitter.from_``huggingface_tokenizer(tokenizer,`
    `chunk_size=100,` `chunk_overlap=10)`。
- en: The limitation of fixed-size chunking is that it doesn’t consider the semantic
    integrity of the text. In other words, the meaning of the text is ignored. It
    works best in scenarios where data is inherently uniform, such as genetic sequences
    and service manuals, or uniformly structured reports such as survey responses.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 固定大小分块的限制在于它没有考虑文本的语义完整性。换句话说，忽略了文本的意义。它在数据本质上均匀的场景中效果最好，例如基因序列和服务手册，或者结构统一的报告，如调查问卷。
- en: Specialized chunking
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 专用分块
- en: Chunking aims to keep meaningful data together. If we are dealing with data
    in the form of HTML, Markdown, JSON, or even computer code, it makes more sense
    to split the data based on the structure rather than a fixed size. Another approach
    to chunking is to consider the format of the extracted and loaded data. A markdown
    file, for example, is organized by headers, a code written in a programming language
    such as Python or Java is organized by classes and functions, and likewise, HTML
    is organized in headers and sections. For such formats, a specialized chunking
    approach can be employed. LangChain offers classes such as `MarkdownHeaderTextSplitter`,
    `HTMLHeader­TextSplitter`, and `RecursiveJsonSplitter`, among others, for these
    formats.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 块分割的目标是将有意义的数据放在一起。如果我们处理的是HTML、Markdown、JSON或甚至计算机代码形式的数据，那么根据结构而不是固定大小来分割数据更有意义。块分割的另一种方法是考虑提取和加载数据的格式。例如，Markdown文件是根据标题组织的，用Python或Java等编程语言编写的代码是根据类和函数组织的，同样，HTML是根据标题和部分组织的。对于这些格式，可以采用专门的块分割方法。LangChain提供了`MarkdownHeaderTextSplitter`、`HTMLHeader-TextSplitter`和`RecursiveJsonSplitter`等类，用于这些格式。
- en: 'Here is a simple example of a code that splits an HTML document using `HTML­SectionSplitter`*.*
    We are using the same Wikipedia article to source the HTML page. We first split
    the input data based on the sections. Sections in HTML are tagged as `<h1>`, `<h2>`,
    `<table>`, and so on. It can be assumed that a well-structured HTML document will
    have similar information. This helps us in creating chunks that have similar information.
    To use the `HTMLSectionSplitter`library, we must install another Python package
    called `lxml`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用`HTML-SectionSplitter`*.* 分割HTML文档的简单代码示例。我们使用相同的维基百科文章作为HTML页面的来源。我们首先根据部分分割输入数据。HTML中的部分被标记为`<h1>`、`<h2>`、`<table>`等。可以假设一个结构良好的HTML文档将包含类似的信息。这有助于我们创建包含类似信息的块。要使用`HTMLSectionSplitter`库，我们必须安装另一个名为`lxml`的Python包：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The advantage of specialized chunking is that chunk sizes are no longer limited
    by a fixed width. This feature helps in preserving the inherent structure of the
    data. Because the size of the chunks changes depending on the structure, this
    method is also sometimes called *adaptive chunking*. Specialized chunking works
    well in structured scenarios such as customer reviews or patient records where
    data can be of different lengths but should ideally be in the same chunk.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 专门块分割的优势在于块的大小不再受固定宽度的限制。这个特性有助于保留数据的固有结构。因为块的大小根据结构而变化，所以这种方法有时也被称为*自适应块分割*。在结构化场景中，如客户评论或患者记录，数据长度可能不同，但理想情况下应在同一个块中，专门块分割效果很好。
- en: 'In the previous example, let’s see how many chunks have been created:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，让我们看看创建了多少个块：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This method has given us 231 chunks from the URL. Chunking methods do not have
    to be exclusive. We can further chunk these 231 chunks using a fixed-size chunking
    method such as `RecursiveCharacterTextSplitter`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法从URL中为我们提供了231个块。块分割方法不必是互斥的。我们可以进一步使用固定大小的块分割方法，如`RecursiveCharacterTextSplitter`，对这些231个块进行分割。
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s look at how many chunks were created by this combination of techniques:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这种技术组合创建了多少个块：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A total of 285 chunks were created by splitting the HTML data from the URL first,
    using a specialized chunking method followed by a fixed size method. This gave
    us more chunks than using the fixed size method alone that we saw in the previous
    section (“split by character” gave us 67 chunks).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通过首先从URL中分割HTML数据，然后使用专门的块分割方法，接着使用固定大小方法，总共创建了285个块。这比仅使用固定大小方法（我们在上一节中看到的是“按字符分割”给我们带来了67个块）要多。
- en: You may be wondering about the advantages of having more chunks and the optimal
    number. Unfortunately, there’s no straightforward answer to that. Having many
    chunks (consequently smaller-sized chunks) means that the information in the chunks
    is precise. This is advantageous when it comes to providing the LLM with accurate
    information. In contrast, by chunking into small sizes, you may lose the overall
    themes, ideas, and coherence of the larger document. The task here is to strike
    a balance. We will discuss more chunking strategies after we take a cursory look
    at a novel method that considers the meaning of the text to perform chunking and
    aims to create chunks that are super-contextual.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道拥有更多分块和最佳数量的优势。不幸的是，对此并没有直接的答案。拥有许多分块（因此分块尺寸更小）意味着分块中的信息是精确的。这在向LLM提供准确信息时是有利的。相比之下，通过将数据分割成小块，你可能会失去更大文档的整体主题、思想和连贯性。这里的任务是找到一个平衡点。在我们简要地审视一种考虑文本意义以进行分块并旨在创建超级上下文分块的方法之后，我们将讨论更多的分块策略。
- en: Semantic chunking
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语义分块
- en: This idea, proposed by Greg Kamradt, questions two aspects of the previous chunking
    methods.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法，由Greg Kamradt提出，质疑了先前分块方法的两个方面。
- en: Why should we have a predefined fixed size of chunks?
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们应该有一个预定义的固定大小的分块？
- en: Why don’t chunking methods take into consideration the actual meaning of content?
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么分块方法没有考虑内容的实际意义？
- en: To address these problems, a method that looks at semantic similarity (or similarity
    in the meaning) between sentences is called semantic chunking. It first creates
    groups of three sentences and then merges groups that are similar in meaning.
    To find out the similarity in meaning, this method uses embeddings. (We will discuss
    embeddings in the next section.) This is still an experimental chunking technique.
    In LangChain, you can use the class `SemanticChunker` from the `langchain_experimental.text_splitter`
    library. See figure 3.3 for examples of different chunking methods.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，一种通过查看句子之间的语义相似度（或意义相似度）的方法被称为语义分块。它首先创建由三个句子组成的小组，然后合并意义相似的小组。为了找出意义的相似度，这种方法使用嵌入（我们将在下一节讨论嵌入）。这仍然是一种实验性的分块技术。在LangChain中，你可以使用来自`langchain_experimental.text_splitter`库的`SemanticChunker`类。见图3.3，以了解不同分块方法的示例。
- en: '![Several different types of diagrams'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![几种不同类型的图表'
- en: AI-generated content may be incorrect.](../Images/CH03_F03_Kimothi.png)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是错误的。](../Images/CH03_F03_Kimothi.png)
- en: Figure 3.3  Chunking methods
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3 分块方法
- en: As the LLM and the generative AI space are evolving fast, chunking methods are
    also becoming more sophisticated. Simple chunking methods predetermine the size
    of the chunks and a split by characters. A slightly more nuanced technique is
    to split the data by tokens. Specialized methods are more suitable for different
    data formats. Experimental techniques such as semantic chunking and agentic chunking
    are spearheading the advancements in the chunking space. Now, let’s consider the
    important question of how to select a chunking method.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM和生成式AI空间的快速发展，分块方法也在变得更加复杂。简单的分块方法预先确定分块的大小和按字符分割。稍微复杂一点的技术是按标记分割数据。专门的方法更适合不同的数据格式。实验技术，如语义分块和代理分块，正在引领分块空间的进步。现在，让我们考虑一个重要的问题：如何选择分块方法。
- en: 3.2.4 Choosing a chunking strategy
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.4 选择分块策略
- en: We have seen that there are many chunking methods available. Which chunking
    method to use (i.e., whether to use a single method or multiple methods) is a
    question that comes up during the creation of the indexing pipeline. There are
    no guidelines or rules to answer this question. However, certain features of the
    application that you’re developing can guide you toward an effective strategy.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到有许多分块方法可用。在创建索引管道的过程中，选择哪种分块方法（即是否使用单一方法或多种方法）是一个经常出现的问题。没有指导方针或规则来回答这个问题。然而，你正在开发的应用程序的一些特性可以指导你走向一个有效的策略。
- en: Nature of the content
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内容的性质
- en: The type of data that you’re dealing with can be a guide for the chunking strategy.
    If your application uses data in a specific format such as code or HTML, a specialized
    chunking method is recommended. Not only that, whether you’re working with long
    documents such as whitepapers and reports or short-form content such as social
    media posts, tweets, and so on, can also guide the chunk size and overlap limits.
    If you’re using a diverse set of information sources, then you might have to use
    different methods for different sources.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在处理的数据类型可以作为数据块化策略的指南。如果你的应用程序使用特定格式的数据，如代码或HTML，建议使用专门的数据块化方法。不仅如此，无论你是在处理长文档，如白皮书和报告，还是短形式内容，如社交媒体帖子、推文等，也可以指导数据块大小和重叠限制。如果你使用的信息来源多样化，那么你可能需要针对不同的来源使用不同的方法。
- en: Expected length and complexity of user query
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用户查询的预期长度和复杂性
- en: The nature of the query that your RAG system is likely to receive also determines
    the chunking strategy. If your system expects a short and straightforward query,
    then the size of your chunks should be different when compared to a long and complex
    query. Matching long queries to short chunks may prove inefficient in certain
    cases. Similarly, short queries matching large chunks may yield partially irrelevant
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你的RAG系统可能收到的查询的性质也决定了数据块化策略。如果你的系统预期一个简短且直接的查询，那么与长且复杂的查询相比，你的块大小应该不同。在某些情况下，将长查询与短块匹配可能效率低下。同样，短查询与大型块匹配可能产生部分不相关的结果。
- en: results.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。
- en: Application and use case requirements
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 应用和用例需求
- en: The nature of the use case you’re addressing may also determine the optimal
    chunking strategy. For a direct question-answering system, shorter chunks are
    likely used for precise results, while for summarization tasks, longer chunks
    may make more sense. If the results of your system need to serve as an input to
    another downstream application, that may also influence the choice of the chunking
    strategy.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在解决的使用案例的性质也可能决定最佳的数据块化策略。对于直接问答系统，较短的块可能用于精确的结果，而对于摘要任务，较长的块可能更有意义。如果你的系统需要的结果作为另一个下游应用程序的输入，这也可能影响数据块化策略的选择。
- en: Embeddings model
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌入模型
- en: We are going to discuss embeddings in the next section. For now, you can make
    a note that certain embeddings models perform better with chunks of specific sizes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中讨论嵌入。目前，你可以记下某些嵌入模型在特定尺寸的数据块上表现更好。
- en: We have discussed chunking at length in this section. From understanding the
    need and advantages of chunking to different chunking methods and the choice of
    chunking strategies, you are now equipped to load data from different sources
    and split them into optimal sizes. Remember, chunking is not an overcomplicated
    task, and most chunking methods will work. You will, however, have to evaluate
    and improve your chunking strategy depending on the results you observe.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经详细讨论了数据块化。从理解数据块化的需求和优势到不同的数据块化方法和数据块化策略的选择，你现在已经准备好从不同的来源加载数据并将它们分割成最佳尺寸。记住，数据块化不是一个过于复杂的任务，大多数数据块化方法都会有效。然而，你将不得不根据观察到的结果评估和改进你的数据块化策略。
- en: Now that data has been split into manageable sizes, we need to store it so that
    it can be fetched later to be used in the generation pipeline. We need to ensure
    that these chunks can be effectively searched over to match the user query. Turns
    out that one data pattern is the most efficient for such tasks. This pattern is
    called “embeddings.” Let’s explore embeddings and their use in RAG systems in
    the next
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经被分割成可管理的尺寸，我们需要存储它，以便稍后可以检索出来用于生成管道。我们需要确保这些数据块可以有效地搜索以匹配用户查询。结果发现，一种数据模式对于此类任务是最有效的。这种模式被称为“嵌入”。让我们在下一节中探讨嵌入及其在RAG系统中的应用。
- en: section.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 部分。
- en: 3.3 Data conversion (embeddings)
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 数据转换（嵌入）
- en: Computers, at their very core, do mathematical calculations. Mathematical calculations
    are done on numbers. Therefore, for a computer to process any kind of nonnumeric
    data such as text or image, it must be first converted into a numerical form.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机在其核心上执行数学计算。数学计算是在数字上进行的。因此，为了使计算机处理任何类型的非数值数据，如文本或图像，它必须首先将其转换为数值形式。
- en: 3.3.1 What are embeddings?
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 嵌入是什么？
- en: '*Embeddings* is a design pattern that is extremely helpful in the fields of
    data science, machine learning, and AI. Embeddings are vector representations
    of data. As a general definition, embeddings are data that has been transformed
    into *n*-dimensional matrixes. The word *embedding* is a vector representation
    of words. I explain embeddings by using three words as an example: *dog, bark,*
    and *fly*.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌入* 是数据科学、机器学习和人工智能领域非常有用的设计模式。嵌入是数据的向量表示。作为一个一般定义，嵌入是将数据转换成 *n*-维矩阵的数据。单词
    *embedding* 是单词的向量表示。我通过使用三个单词作为例子来解释嵌入：*dog, bark,* 和 *fly*。'
- en: NOTE In physics and mathematics, the vector is an object that has a magnitude
    and a direction, like an arrow in space. The length of the arrow is the magnitude
    of the quantity and the direction that the arrow points to is the direction of
    the quantity. Examples of such quantities in physics are velocity, force, acceleration,
    and so forth. In computer science and machine learning, the idea of a vector is
    an abstract representation of data, and the representation is an array or list
    of numbers. These numbers represent the data features or attributes. In NLP, a
    vector can represent a document, a sentence, or even a word. The length of the
    array or list is the number of dimensions in the vector. A 2D vector will have
    two numbers, a 3D vector will have three numbers, and an *n*-dimensional vector
    will have *n* numbers.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：在物理学和数学中，向量是一个具有大小和方向的物体，就像空间中的箭头一样。箭头的长度是数量的量级，箭头所指的方向是数量的方向。物理学中此类数量的例子有速度、力、加速度等等。在计算机科学和机器学习中，向量的概念是数据的抽象表示，这种表示是一个数字数组或列表。这些数字代表数据特征或属性。在自然语言处理（NLP）中，向量可以表示文档、句子，甚至单词。数组或列表的长度是向量中的维度数。一个二维向量将有两个数字，一个三维向量将有三个数字，一个
    *n*-维向量将有 *n* 个数字。
- en: 'Let’s understand embeddings by assigning a number to the three words: Dog =
    1, Bark = 2 and Fly = 6, as shown in figure 3.4\. We chose these numbers because
    the word *dog* is closer to the word *bark* and farther from the word *fly*.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过给三个单词分配一个数字来理解嵌入：Dog = 1，Bark = 2，Fly = 6，如图3.4所示。我们选择这些数字是因为单词 *dog* 更接近于单词
    *bark*，而远离单词 *fly*。
- en: '![A blue line with black text'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![一条带有黑色文字的蓝色线'
- en: AI-generated content may be incorrect.](../Images/CH03_F04_Kimothi.png)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH03_F04_Kimothi.png)
- en: Figure 3.4  Words in a unidimensional vector
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4  单词在一维向量中
- en: Unidimensional vectors are not great representations because we can’t plot unrelated
    words accurately. In our example, we can plot that the words *fly* and *bark*,
    which are verbs, are far from each other, and bark is closer to a dog because
    dogs can bark. But how do we plot words such as *love* or *re**d*? To accurately
    represent all the words, we need to increase the number of dimensions. See figure
    3.5.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一维向量并不是很好的表示方式，因为我们无法准确绘制无关的单词。在我们的例子中，我们可以绘制出单词 *fly* 和 *bark*（都是动词），它们相距很远，而
    *bark* 更接近于狗，因为狗会吠叫。但是，我们如何绘制像 *love* 或 *red* 这样的单词呢？为了准确表示所有单词，我们需要增加维度的数量。见图3.5。
- en: '![A blue line with white text'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![一条带有白色文字的蓝色线'
- en: AI-generated content may be incorrect.](../Images/CH03_F05_Kimothi.png)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH03_F05_Kimothi.png)
- en: Figure 3.5  Words in a 2D vector space
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5 2D向量空间中的单词
- en: The goal of an embedding model is to convert words (or sentences/paragraphs)
    into *n*-dimensional vectors so that the words (or sentences/paragraphs) that
    are like each other in meaning lie close to each other in the vector space. See
    figure 3.6.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型的目的是将单词（或句子/段落）转换为 *n*-维向量，使得在意义上相似的单词（或句子/段落）在向量空间中彼此靠近。见图3.6。
- en: '![A diagram of a model'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个模型图'
- en: AI-generated content may be incorrect.](../Images/CH03_F06_Kimothi.png)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH03_F06_Kimothi.png)
- en: Figure 3.6  The process of embedding transforms data (such as text) into vectors
    and compresses the input information, which results in an embedding space specific
    to the training data.
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6 嵌入过程将数据（如文本）转换为向量并压缩输入信息，从而产生一个针对训练数据的特定嵌入空间。
- en: 'An embeddings model can be trained on a corpus of preprocessed text data using
    an embedding algorithm such as Word2Vec, GloVe, FastText, or BERT:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个嵌入模型可以使用Word2Vec、GloVe、FastText或BERT等嵌入算法在预处理文本数据的语料库上训练：
- en: '*Word2Ve**c*—Word2Vec is a shallow-neural-network-based model for learning
    word embeddings, developed by researchers at Google. It is one of the earliest
    embedding techniques.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Word2Ve**c*—Word2Vec是由Google的研究人员开发的一种基于浅层神经网络的学习词嵌入的模型。它是最早的嵌入技术之一。'
- en: '*GloV**e*—Global Vectors for Word Representations is an unsupervised learning
    technique developed by researchers at Stanford University.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*GloV**e*—全球词表示向量是由斯坦福大学的研究人员开发的无监督学习技术。'
- en: '*FastTex**t*—FastText is an extension of Word2Vec developed by Facebook AI
    Research. It is particularly useful for handling misspellings and rare words.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*FastTex**t*—FastText是Facebook AI Research开发的Word2Vec的扩展。它特别适用于处理拼写错误和罕见单词。'
- en: '*ELM**o*—Embeddings from Language Models was developed by researchers at Allen
    Institute for AI. ELMo embeddings have been shown to improve performance on question
    answering and sentiment analysis tasks.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ELM**o*—由Allen Institute for AI的研究人员开发的Embeddings from Language Models。ELMo嵌入已被证明可以改善问答和情感分析任务的性能。'
- en: '*BERT*—Bidirectional Encoder Representations from Transformers, developed by
    researchers at Google, is a transformers-architecture-based model. It provides
    contextualized word embeddings by considering bidirectional context, achieving
    state-of-the-art performance on various NLP tasks.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BERT*—由Google的研究人员开发的基于Transformer架构的模型。通过考虑双向上下文，它提供了上下文化的词嵌入，在各种NLP任务上实现了最先进的性能。'
- en: Training a custom embeddings model can prove to be beneficial in some use cases
    where the scope is limited. Training an embeddings model that generalizes well
    can be a laborious exercise. Collection and preprocessing text data can be cumbersome.
    The training process can turn out to be computationally expensive too.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些用例中，训练定制的嵌入模型可能是有益的，其中范围有限。训练一个泛化良好的嵌入模型可能是一项费力的练习。收集和预处理文本数据可能很繁琐。训练过程也可能变得计算成本高昂。
- en: 3.3.2 Common pre-trained embeddings models
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 常见的预训练嵌入模型
- en: The good news for anyone building RAG systems is that embeddings once created
    can also generalize across tasks and domains. There are a variety of proprietary
    and open source pre-trained embeddings models available to use. This is also one
    of the reasons why the usage of embeddings has exploded in popularity across machine
    learning applications.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何正在构建RAG系统的人来说，好消息是一旦创建了嵌入，它们也可以跨任务和领域泛化。有各种专有和开源的预训练嵌入模型可供使用。这也是嵌入在机器学习应用中流行度激增的原因之一。
- en: '*Embeddings models by OpenAI*—OpenAI, the company behind ChatGPT and the GPT
    series of LLMs, also provides three embeddings models:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OpenAI的嵌入模型*—ChatGPT和GPT系列LLM背后的公司OpenAI也提供了三个嵌入模型：'
- en: '*text-embedding-ada-002* was released in December 2022\. It has a dimension
    of 1536, meaning that it converts text into a vector of 1536 dimensions.'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*text-embedding-ada-002*于2022年12月发布。它具有1536维的维度，这意味着它将文本转换为1536维的向量。'
- en: '*text-embedding-3-small*is the latest small embedding model of 1536 dimensions
    released in January 2024\. The flexibility it provides over the ada-002 model
    is that users can adjust the size of the dimensions according to their needs.'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*text-embedding-3-small*是2024年1月发布的最新1536维小嵌入模型。它相对于ada-002模型的灵活性在于用户可以根据自己的需求调整维度的尺寸。'
- en: '*text-embedding-3-large* is a large embedding model of 3072 dimensions, released
    together with the text-embedding-3-small model. It is the best performing model
    released by OpenAI yet.'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*text-embedding-3-large*是与text-embedding-3-small模型一起发布的3072维大型嵌入模型。它是OpenAI迄今为止性能最好的模型。'
- en: OpenAI models are closed source and can be accessed using the OpenAI API. They
    are priced based on the number of input tokens for which embeddings are desired.
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: OpenAI模型是闭源的，可以通过OpenAI API访问。它们的价格基于所需的嵌入输入令牌的数量。
- en: '*Gemini Embeddings Model by Googl**e*—*text-embedding-004* (last updated in
    April 2024)is the model offered by Google Gemini. It offers elastic embeddings
    size up to 768 dimensions and can be accessed via the Gemini API.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Google的Gemini嵌入模型*—*text-embedding-004*（最后更新于2024年4月）是Google Gemini提供的模型。它提供弹性嵌入尺寸高达768维，可以通过Gemini
    API访问。'
- en: '*Voyage A**I*—These embeddings models are recommended by Anthropic, the provider
    of the Claude series of LLMs. Voyage offers several embedding models such as'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Voyage A**I*—这些嵌入模型由Claude系列LLM的提供者Anthropic推荐。Voyage提供几种嵌入模型，例如'
- en: '*voyage-large-2-instruct*is a 1024-dimensional embeddings model that has become
    a leader in embeddings models.'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*voyage-large-2-instruct* 是一个成为嵌入模型领导者的1024维嵌入模型。'
- en: '*voyage-law-2*is a 1024-dimension model optimized for legal documents.'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*voyage-law-2* 是一个针对法律文件优化的1024维模型。'
- en: '*voyage-code-2* is a 1536-dimension model optimized for code retrieval.'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*voyage-code-2* 是一个针对代码检索优化的1536维模型。'
- en: '*voyage-large-2* is a 1536-dimension general-purpose model optimized for retrieval.'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*voyage-large-2* 是一个针对检索优化的1536维通用模型。'
- en: Voyage AI offers several free tokens before charging for using the embeddings
    models.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Voyage AI 在收费使用嵌入模型之前提供了一些免费令牌。
- en: '*Mistral AI embedding**s*—Mistral is the company behind LLMs such as Mistral
    and Mixtral. They offer a 1024-dimensional embeddings model known as *mistral-embed*.
    This is an open source embeddings model.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Mistral AI embedding**s*——Mistral 是 Mistral 和 Mixtral 等LLM背后的公司。他们提供了一种名为
    *mistral-embed* 的1024维嵌入模型，这是一个开源嵌入模型。'
- en: '*Cohere embedding**s*—Cohere, the developers of Command, Command R, and Command
    R + LLMs also offer a variety of embeddings models, which can be accessed via
    the Cohere API. Some of these are'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Cohere embedding**s*——Cohere，Command、Command R 和 Command R + LLMs 的开发者，也提供了一系列嵌入模型，这些模型可以通过
    Cohere API 访问。其中一些是'
- en: '*embed-english-v3.0* is a 1024-dimension model that works on embeddings for
    English only.'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*embed-english-v3.0* 是一个仅适用于英语嵌入的1024维模型。'
- en: '*embed-english-light-v3.0* is a lighter version of the embed-english model,
    which has 384 dimensions.'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*embed-english-light-v3.0* 是 embed-english 模型的轻量版本，具有384维。'
- en: '*embed-multilingual-v3.0* offers multilingual support for over 100 languages.'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*embed-multilingual-v3.0* 提供了超过100种语言的跨语言支持。'
- en: These five models are in no way recommendations but just a list of the popular
    embeddings models. Apart from these providers, almost all LLM developers such
    Meta, TII, and LMSYS also offer pre-trained embeddings models. One place to check
    out all the popular embeddings models is the MTEB (Massive Text Embedding Benchmark)
    Leaderboard on Hugging Face ([https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)).
    The MTEB benchmark compares the embeddings models on tasks such as classification,
    retrieval, clustering, and more. You now know what embeddings are, but why are
    they useful? Let’s discuss that next with some examples of use cases.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这五种模型绝非推荐，而仅仅是一份流行的嵌入模型的列表。除了这些提供商之外，几乎所有的LLM开发者，如Meta、TII和LMSYS，也提供预训练的嵌入模型。一个可以查看所有流行嵌入模型的地方是
    Hugging Face 上的 MTEB（大规模文本嵌入基准）排行榜（[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)）。MTEB基准比较了在分类、检索、聚类等任务上的嵌入模型。你现在已经知道了嵌入是什么，但为什么它们有用呢？让我们通过一些用例的例子来讨论这个问题。
- en: 3.3.3 Embeddings use cases
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 嵌入应用案例
- en: The reason why embeddings are so popular is because they help in establishing
    semantic relationships between words, phrases, and documents. In the simplest
    methods of searching or text matching, we use keywords, and if the keywords match,
    we can show the matching documents as results of the search. However, this approach
    fails to consider the semantic relationships or the meanings of the words while
    searching. This challenge is overcome by using embeddings.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入之所以如此受欢迎，是因为它们有助于建立单词、短语和文档之间的语义关系。在搜索或文本匹配的最简单方法中，我们使用关键词，如果关键词匹配，我们就可以将匹配的文档作为搜索结果展示。然而，这种方法在搜索时未能考虑语义关系或词语的含义。通过使用嵌入，我们可以克服这个挑战。
- en: How is similarity calculated
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 相似度是如何计算的
- en: We discussed that embeddings are vector representations of words or sentences.
    Similar pieces of text lie close to each other. Closeness to each other is calculated
    by the distance between the points in the vector space. One of the most common
    measures of similarity is *cosine similarity*. Cosine similarity is calculated
    as the cosine value of the angle between the two vectors. Recall from trigonometry
    that the cosine of parallel lines (i.e., angle = 0o) is 1, and the cosine of a
    right angle (i.e., 90o) is 0\. The cosine of the opposite lines (i.e., angle =
    180o) is −1\. Therefore, the cosine similarity lies between −1 and 1, where unrelated
    terms have a value close to 0, and related terms have a value close to 1\. Terms
    that are opposite in meaning have a value of −1\. See figure 3.7.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了嵌入是单词或句子的向量表示。相似的文本片段彼此靠近。彼此的接近度是通过向量空间中点的距离来计算的。最常用的相似度度量之一是*余弦相似度*。余弦相似度是两个向量之间角度的余弦值。回想一下三角学，平行线的余弦值（即角度
    = 0°）为1，直角的余弦值（即90°）为0。相反方向的余弦值（即角度 = 180°）为-1。因此，余弦相似度介于-1和1之间，其中不相关的术语值接近0，相关的术语值接近1。意义相反的术语具有-1的值。见图3.7。
- en: '![A graph of a trigonometry'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![三角函数的图像'
- en: AI-generated content may be incorrect.](../Images/CH03_F07_Kimothi.png)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH03_F07_Kimothi.png)
- en: Figure 3.7  Cosine similarity of vectors in 2D vector space
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7  2D向量空间中向量的余弦相似度
- en: 'Yet another measure of similarity is the *Euclidean distance* between two vectors.
    Close vectors have a small Euclidean distance. It can be calculated using the
    following formula:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种相似度的度量是两个向量之间的*欧几里得距离*。接近的向量具有较小的欧几里得距离。可以使用以下公式进行计算：
- en: Distance (A, B) = sqrt((A[i]-B[i])²),
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 距离 (A, B) = sqrt((A[i]-B[i])²),
- en: where i is the i-th dimension of the *n*-dimensional vectors
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 i 是 n 维向量的第 i 个维度
- en: Different use cases of embeddings
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌入的不同用例
- en: 'Here are some different use cases of embeddings:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了嵌入的不同用例：
- en: '*Text search*—Searching through the knowledge base for the right document chunk
    is a key component of RAG systems. Embeddings are used to calculate similarity
    between the user query and the stored documents.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本搜索*—在知识库中搜索正确的文档片段是RAG系统的一个关键组成部分。嵌入用于计算用户查询和存储文档之间的相似度。'
- en: '*Clustering*—Categorizing similar data together to find themes and groups in
    the data can result in valuable insights. Embeddings are used to group similar
    pieces of text together to find out, for example, the common themes in customer
    reviews.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类*—将相似数据分类在一起以在数据中找到主题和组可以获得有价值的见解。嵌入用于将相似的文本片段分组在一起，例如，找出客户评论中的共同主题。'
- en: '*Machine learning*—Advanced machine learning techniques can be used for different
    problems such as classification and regression. To convert text data into numerical
    features, embeddings prove to be a valuable technique.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习*—高级机器学习技术可用于解决各种问题，如分类和回归。为了将文本数据转换为数值特征，嵌入证明是一种非常有价值的技巧。'
- en: '*Recommendation engines*—Shorter distances between product features mean greater
    similarity. Using embeddings for product and user features can be used to recommend
    similar products.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*推荐引擎*—产品特征之间的较短距离意味着更大的相似度。使用嵌入进行产品和用户特征可以用于推荐相似产品。'
- en: Since we are focusing on RAG systems, here we examine using embeddings for text
    search— to find the document chunks that are closest to the user’s query. Let’s
    continue with our example of the Wikipedia page on the 2023 Cricket World Cup.
    In the last section, we created 67 chunks using a combination of specialized and
    fixed-width chunking. Now we will see how to create embeddings for each chunk.
    We will see how to use an open source as well as a proprietary embeddings model.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们专注于RAG系统，因此我们在此检查使用嵌入进行文本搜索——找到与用户查询最接近的文档片段。让我们继续我们的2023年板球世界杯维基百科页面的例子。在上一个部分，我们使用专业和固定宽度的组合创建了67个片段。现在我们将看到如何为每个片段创建嵌入。我们将看到如何使用开源以及专有嵌入模型。
- en: 'Here is the code example for creating embeddings using an open source embeddings
    model all-MPnet-base-v2 via Hugging Face:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用Hugging Face通过开源嵌入模型all-MPnet-base-v2创建嵌入的代码示例：
- en: '[PRE13]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This model creates embeddings of dimension 768\. The list `hf_embeddings` is
    made up of 285 lists, each containing 768 numbers for each chunk. Figure 3.8 shows
    the embeddings space of all the chunks.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型创建维度为768的嵌入。`hf_embeddings`列表由285个列表组成，每个列表包含每个片段的768个数字。图3.8显示了所有片段的嵌入空间。
- en: '![A blank lined paper with numbers'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![一张带有数字的空白横线纸'
- en: AI-generated content may be incorrect.](../Images/CH03_F08_Kimothi.png)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是错误的。](../Images/CH03_F08_Kimothi.png)
- en: Figure 3.8  Embeddings created for chunks of Wikipedia page using the all-MiniLM-l6-v2
    model.
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8 使用all-MiniLM-l6-v2模型为维基百科页面块创建的嵌入。
- en: Similarly, we can use a proprietary model such as the text-embedding-3-small
    model, hosted by OpenAI. The only prerequisite is obtaining an API key and setting
    up a billing account with OpenAI.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以使用OpenAI托管的私有模型，如text-embedding-3-small模型。唯一的前提是获取API密钥并在OpenAI上设置账单账户。
- en: '[PRE14]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This text-embedding-3-small model creates embeddings for the same chunks of
    dimension 1536.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: text-embedding-3-small模型为相同维度的块创建嵌入，维度为1536。
- en: There are several embeddings models available, and new ones are being added
    every day. The choice of embeddings can be dictated by certain factors. Let’s
    look at a few factors.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的嵌入模型有很多，每天都有新的模型被添加。嵌入的选择可能由某些因素决定。让我们看看几个因素。
- en: 3.3.4 How to choose embeddings?
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.4 如何选择嵌入？
- en: There are a few major factors that will affect your choice of embeddings.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个主要因素会影响你选择嵌入的决定。
- en: Use case
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用例
- en: 'Your application use case may determine your choice of embeddings. The MTEB
    leaderboard scores each of the embeddings models across seven use cases: classification,
    clustering, pair classification, reranking, retrieval, semantic text similarity,
    and summarization. At the time of writing this book, the `SFR-Embedding-Mistral`
    model developed by Salesforce performs the best for retrieval tasks.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你的应用程序用例可能决定了你选择的嵌入。MTEB排行榜对每个嵌入模型在七个用例中的表现进行了评分：分类、聚类、成对分类、重排序、检索、语义文本相似性和摘要。在撰写本书时，Salesforce开发的`SFR-Embedding-Mistral`模型在检索任务中表现最佳。
- en: Cost
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成本
- en: Cost is another important factor to consider. To create the knowledge base,
    you may have to create embeddings for thousands of documents, thus running into
    millions of tokens.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 成本是另一个需要考虑的重要因素。为了创建知识库，你可能需要为数千份文档创建嵌入，从而产生数百万个标记。
- en: Embeddings are powerful data patterns that are most effective in finding similarities
    between texts. In RAG systems, embeddings play a critical role in search and retrieval
    of data relevant to the user query. Once the embeddings have been created, they
    need to be stored in persistent memory for real-time access. To store embeddings,
    a new kind of database called a *vector database* have become increasingly popular.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是强大的数据模式，在寻找文本之间的相似性方面最为有效。在RAG系统中，嵌入在搜索和检索与用户查询相关的数据中起着关键作用。一旦创建了嵌入，它们需要存储在持久内存中以便实时访问。为了存储嵌入，一种新的数据库类型，称为*向量数据库*，变得越来越流行。
- en: 3.4 Storage (vector databases)
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 存储（向量数据库）
- en: Now we are at the last step of the indexing pipeline. The data has been loaded,
    split, and converted to embeddings. To use this information repeatedly, we need
    to store it in memory so that it can be accessed on demand.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经到达索引管道的最后一步。数据已经被加载、分割并转换为嵌入。为了重复使用这些信息，我们需要将其存储在内存中，以便按需访问。
- en: 3.4.1 What are vector databases?
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 向量数据库是什么？
- en: The evolution of databases can be traced back to the early days of computing.
    Databases are organized collections of data, designed to be easily accessed, managed,
    and updated. Relational databases such as MySQL organize structured data into
    rows and columns. NoSQL databases such as MongoDB specialize in handling unstructured
    and semi-structured data. Graph databases such as Neo4j are optimized for querying
    graph data. In the same manner, vector databases are built to handle high-dimensional
    vectors. These databases specialize in indexing and storing vector embeddings
    for fast semantic search and retrieval.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库的演变可以追溯到计算机的早期阶段。数据库是有组织的集合，旨在易于访问、管理和更新。例如，MySQL这样的关系型数据库将结构化数据组织成行和列。MongoDB这样的NoSQL数据库擅长处理非结构化和半结构化数据。Neo4j这样的图数据库针对查询图数据进行了优化。同样，向量数据库是为了处理高维向量而构建的。这些数据库专门用于索引和存储向量嵌入，以实现快速语义搜索和检索。
- en: Apart from efficiently storing high-dimensional vector data, modern vector databases
    offer traditional features such as scalability, security, multi-tenancy, versioning
    and management, and similar. However, vector databases are unique in offering
    similarity searches based on Euclidean distance or cosine similarity. They also
    employ specialized indexing techniques.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 除了有效地存储高维向量数据外，现代向量数据库还提供传统功能，如可伸缩性、安全性、多租户、版本控制和管理工作，等等。然而，向量数据库的独特之处在于提供基于欧几里得距离或余弦相似度的相似搜索。它们还采用专门的索引技术。
- en: 3.4.2 Types of vector databases
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 向量数据库的类型
- en: Vector databases started as a specialized database offering, but propelled by
    the growth in demand for storing vector data, all major database providers have
    added the vector indexing capability. We can categorize the popular vector databases
    available today into six broad categories.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库最初是一种专门的数据库产品，但受到存储向量数据需求增长的推动，所有主要的数据库提供商都增加了向量索引功能。我们可以将今天可用的流行向量数据库分为六大类。
- en: '*Vector indexe**s*—These are libraries that focus on the core features of indexing
    and search. They do not support data management, query processing, or interfaces.
    They can be considered a bare-bones vector database. Examples of vector indexes
    are Facebook AI Similarity Search (FAISS), Non-Metric Space Library (NMSLIB),
    Approximate Nearest Neighbors Oh Yeah (ANNOY), Scalable Nearest Neighbors (ScaNN),
    and similar.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*向量索引**—*这些库专注于索引和搜索的核心功能。它们不支持数据管理、查询处理或接口。它们可以被视为一个基本的向量数据库。向量索引的例子包括Facebook
    AI Similarity Search (FAISS)、Non-Metric Space Library (NMSLIB)、Approximate Nearest
    Neighbors Oh Yeah (ANNOY)、Scalable Nearest Neighbors (ScaNN)等。'
- en: '*Specialized vector DB**s*—These databases focus on the core feature of high-dimensional
    vector support, indexing, search, and retrieval such as vector indexes, but also
    offer database features such as data management, extensibility, security, scalability,
    non-vector data support, and similar. Examples of specialized vector DBs are Pinecone,
    ChromaDB, Milvus, Qdrant, Weaviate, Vald, LanceDB, Vespa, and Marqo.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*专门的向量数据库**—*这些数据库专注于高维向量支持的核心功能，如索引、搜索和检索（例如向量索引），同时也提供数据库功能，如数据管理、可扩展性、安全性、可伸缩性、非向量数据支持等。专门的向量数据库的例子包括Pinecone、ChromaDB、Milvus、Qdrant、Weaviate、Vald、LanceDB、Vespa和Marqo。'
- en: '*Search platform**s*—Solr, Elastic Search, Open Search, and Apache Lucene are
    traditional text search platforms and engines built for full text search. They
    have now added vector similarity search capabilities to their existing search
    capabilities.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索平台**—*Solr、Elastic Search、Open Search和Apache Lucene是传统的文本搜索平台和引擎，专为全文搜索构建。它们现在已将向量相似搜索能力添加到现有的搜索能力中。'
- en: '*Vector capabilities for SQL database**s*—Azure SQL, Postgres SQL(pgvector),
    SingleStore, and CloudSQL are traditional SQL databases that have now added vector
    data-handling capabilities.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SQL数据库的向量功能**—*Azure SQL、Postgres SQL(pgvector)、SingleStore和CloudSQL是传统的SQL数据库，现在已经增加了向量数据处理能力。'
- en: '*Vector capabilities for NoSQL database**s*—Like SQL DBs, NoSQL DBs such as
    MongoDB have also added vector search capabilities.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NoSQL数据库的向量功能**—*与SQL数据库一样，NoSQL数据库如MongoDB也增加了向量搜索功能。'
- en: '*Graph databases with vector capabilitie**s*—Graph DBs such as Neo4j, have
    also opened new possibilities by adding vector capabilities, .'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*具有向量功能的图数据库**—*例如Neo4j，通过增加向量功能，也开辟了新的可能性。'
- en: 'Using a vector index such as FAISS is supported by LangChain. To use FAISS,
    we first must install the `faiss-cpu` library. We will use the chunks already
    created in section 3.2 and the OpenAI embeddings that we used in section 3.3:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain支持使用向量索引，例如FAISS。要使用FAISS，我们首先必须安装`faiss-cpu`库。我们将使用第3.2节中已创建的块和第3.3节中使用的OpenAI嵌入：
- en: '[PRE15]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With this code, the 285 chunks of data have been converted to vector embeddings,
    and these embeddings are stored in a FAISS vector index. The FAISS vector index
    can also be saved to memory using the `vector_store.save_local(folder_path,index_name)`
    and `FAISS.load_local(folder_path,index_name)` functions. Let’s now take a cursory
    look at how a vector store can be used. We will take the original question that
    we have been asking from the beginning of this book: “Who won the 2023 Cricket
    World Cup?”'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此代码，285个数据块已转换为向量嵌入，这些嵌入存储在FAISS向量索引中。FAISS向量索引也可以使用`vector_store.save_local(folder_path,index_name)`和`FAISS.load_local(folder_path,index_name)`函数保存到内存中。现在让我们简要地看看如何使用向量存储。我们将从本书开始就一直在询问的原始问题：
    “2023年板球世界杯的冠军是谁？”
- en: '[PRE16]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Similarity search orders the chunks in descending order of similarity, meaning
    that the most similar chunks to the query are ranked on top. In the previous example,
    we can observe that the chunk that speaks about the world cup final has been ranked
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 相似性搜索按相似度降序排列块，这意味着与查询最相似的块将排在顶部。在先前的例子中，我们可以观察到关于世界杯决赛的块已经被排在
- en: on top.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部。
- en: FAISS is a stripped-down high-performance vector index that works for many applications.
    ChromaDB is another user-friendly vector DB that has gained popularity. Pinecone
    offers managed services and customization. Milvus claims higher performance on
    similarity search, while Qdrant provides an advanced filtering system. We will
    now discuss some points on how to choose a vector database that works best for
    your requirements.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: FAISS 是一个简化版的高性能向量索引，适用于许多应用。ChromaDB 是另一个用户友好的向量数据库，已经获得了广泛认可。Pinecone 提供托管服务和定制化。Milvus
    在相似性搜索方面声称有更高的性能，而 Qdrant 提供了先进的过滤系统。现在我们将讨论一些关于如何选择最适合您需求的向量数据库的要点。
- en: 3.4.3 Choosing a vector database
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 选择向量数据库
- en: 'All vector databases offer the same basic capabilities, but each one of them
    also claims a differentiated value. Your choice should be influenced by the nuance
    of your use case matching with the value proposition of the database. Here are
    a few things to consider while evaluating and implementing a vector database:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 所有向量数据库都提供相同的基本功能，但每个数据库也声称具有差异化的价值。您的选择应受您用例的细微差别与数据库的价值主张相匹配的影响。在评估和实施向量数据库时，以下是一些需要考虑的事项：
- en: '*Accuracy vs. spee**d*—Certain algorithms are more accurate but slower. A balance
    between search accuracy and query speed must be achieved based on application
    needs. It will become important to evaluate vector DBs on these parameters.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*准确性与速度*—某些算法更准确但速度较慢。必须根据应用需求在搜索准确性和查询速度之间取得平衡。评估向量数据库时，这些参数将变得很重要。'
- en: '*Flexibility vs. performanc**e*—Vector DBs provide customizations to the user.
    While it may help you in tailoring the DB to your specific requirements, more
    customizations can add overhead and slow systems down.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*灵活性与性能*—向量数据库为用户提供定制化。虽然这可能有助于您根据特定需求定制数据库，但更多的定制可能会增加开销并减慢系统速度。'
- en: '*Local vs. cloud storag**e*—Assess tradeoffs between local storage speed and
    access versus cloud storage benefits like security, redundancy, and scalability.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*本地存储与云存储*—评估本地存储速度和访问与云存储优势（如安全性、冗余和可扩展性）之间的权衡。'
- en: '*Direct access vs. AP**I*—Determine if tight integration control via direct
    libraries is required or if ease-of-use abstractions like APIs better suit your
    use case.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*直接访问与API*—确定是否需要通过直接库进行紧密集成控制，或者是否需要像API这样的易于使用的抽象更适合您的用例。'
- en: '*Simplicity vs. advanced feature**s*—Compare advanced algorithm optimizations,
    query features, and indexing versus how much complexity your use case necessitates
    versus needs for simplicity.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*简单性与高级功能*—比较高级算法优化、查询功能和索引与您的用例所需的复杂性和对简单性的需求之间的比较。'
- en: '*Cos**t*—While you may incur regular costs in a fully managed solution, a self-hosted
    one might prove costlier if not managed well.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本*—虽然您可能在完全托管的解决方案中产生常规成本，但如果管理不善，自托管可能更昂贵。'
- en: We have now completed an end-to-end indexing of a document. We continued with
    the same question (“Who won the 2023 Cricket World Cup?”) and the same external
    source—the Wikipedia page of the 2023 Cricket World Cup ([https://mng.bz/yN4J](https://mng.bz/yN4J)).
    In this chapter, we started with the programmatic loading of this Wikipedia page
    extracting the HTML document and then parsing the HTML document to extract. Thereafter,
    we divided the text into small-sized chunks using a specialized and fixed-width
    chunking method. We converted these chunks into embeddings using OpenAI’s text-embedding-003-large
    model. Finally, we stored the embeddings into a FAISS vector index. We also saw
    how using similarity search on this vector index helped us retrieve relevant chunks.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了一个文档的端到端索引。我们继续使用相同的问题（“2023年板球世界杯谁赢了？”）和相同的外部来源——2023年板球世界杯的维基百科页面（[https://mng.bz/yN4J](https://mng.bz/yN4J)）。在本章中，我们从程序化加载这个维基百科页面开始，提取HTML文档，然后解析HTML文档以提取。之后，我们使用一种专业和固定宽度的分块方法将文本分成小块。我们使用
    OpenAI 的 text-embedding-003-large 模型将这些块转换为嵌入。最后，我们将嵌入存储到 FAISS 向量索引中。我们还看到了如何使用这个向量索引上的相似性搜索帮助我们检索相关块。
- en: When several such documents in different formats from different sources are
    indexed using a combination of methods and strategies, we can store all the information
    in the form of vector embeddings creating a non-parametric knowledge base for
    our RAG system.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用多种方法和策略对来自不同来源的不同格式的多个文档进行索引时，我们可以将所有信息以向量嵌入的形式存储，为我们的RAG系统创建一个非参数化知识库。
- en: This concludes our discussion on the indexing pipeline. By now, you must have
    built a solid understanding of the four components of the indexing pipeline and
    should be ready to build a knowledge base for a RAG system.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对索引管道的讨论。到现在为止，您必须已经对索引管道的四个组成部分有了坚实的理解，并准备好为RAG系统构建知识库。
- en: In the next chapter, we will use this knowledge base to generate real-time responses
    to user queries through the generation pipeline.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用这个知识库通过生成管道实时响应用户查询。
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Data loading
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据加载
- en: 'The process of sourcing data from its original location is called *data loading*,
    and it includes the following four steps: connecting to the source, extracting
    and parsing text, reviewing and updating metadata, and cleaning and transforming
    data.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从原始位置获取数据的过程称为*数据加载*，包括以下四个步骤：连接到源、提取和解析文本、审查和更新元数据以及清理和转换数据。
- en: Loading documents from a list of sources may turn out to be a complicated process.
    Make sure to plan for all the sources and loaders in advance.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一系列来源加载数据可能是一个复杂的过程。务必提前规划好所有来源和加载器。
- en: A variety of data loaders from LangChain can be used.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain提供了多种数据加载器可供使用。
- en: Breaking down long pieces of text into manageable sizes is called *data splitting*
    or *chunking*.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将长文本分解成可管理的尺寸称为*数据拆分*或*分块*。
- en: Chunking addresses context window limits of LLMs, mitigates the lost-in-the-middle
    problem for long prompts, and enables easier search and retrieval.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分块解决了LLM的上下文窗口限制问题，减轻了长提示中丢失在中间的问题，并使搜索和检索更容易。
- en: The chunking process involves dividing longer texts into small units, merging
    small units into chunks, and including an overlap between chunks to preserve contextual
    continuity.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分块过程涉及将较长的文本划分为小单元，将小单元合并成块，并在块之间包含重叠以保持上下文连续性。
- en: Chunking can be fixed size, specialized (or adaptive), or semantic. Newer chunking
    methods are constantly being introduced.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分块可以是固定大小、专用（或自适应）或语义的。不断有新的分块方法被引入。
- en: Your choice of the chunking strategy should be based on the nature of the content,
    expected length and complexity of user query, application use case, and the embeddings
    model being used.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您选择的分块策略应基于内容的性质、预期用户查询的长度和复杂性、应用用例以及所使用的嵌入模型。
- en: A chunking strategy can include multiple methods.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分块策略可以包括多种方法。
- en: Data conversion
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据转换
- en: For processing, text needs to be converted into a numerical format.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了处理，文本需要转换为数值格式。
- en: Embeddings are vector representations of data (words, sentences, documents,
    etc.).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入是数据的向量表示（单词、句子、文档等）。
- en: The goal of an embedding algorithm is to position similar data points close
    to each other in a vector space.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入算法的目标是在向量空间中将相似的数据点放置在彼此附近。
- en: Several pre-trained, open source and proprietary, embedding models are available
    for use.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用几种预训练的、开源和专有嵌入模型供使用。
- en: Embeddings models enable similarity search. Embeddings can be used for text
    search, clustering, ML models, and recommendation engines.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入模型使相似度搜索成为可能。嵌入可以用于文本搜索、聚类、机器学习模型和推荐引擎。
- en: The choice of embeddings is largely based on the use case and the cost implications.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入的选择在很大程度上取决于用例和成本影响。
- en: Vector databases are designed to efficiently store and retrieve high-dimensional
    vector data such as embeddings.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量数据库旨在高效地存储和检索高维向量数据，如嵌入。
- en: Vector databases provide similarity searches based on distance metrics such
    as cosine similarity.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量数据库基于距离度量，如余弦相似度，提供相似度搜索。
- en: Apart from the similarity search, vector databases offer traditional services
    such as scalability, security, versioning, and the like.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了相似度搜索之外，向量数据库还提供传统服务，如可扩展性、安全性、版本控制和类似服务。
- en: Vector capabilities can be offered by standalone vector indexes, specialized
    vector databases, or legacy offerings such as search platforms, SQL, and NoSQL
    databases with added vector capabilities.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立的向量索引、专门的向量数据库或具有附加向量能力的传统产品（如搜索平台、SQL和NoSQL数据库）都可以提供向量功能。
- en: Accuracy, speed, flexibility, storage, performance, simplicity, access, and
    cost are some of the factors that can influence the choice of a vector database.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性、速度、灵活性、存储、性能、简单性、访问性和成本是影响选择向量数据库的一些因素。
