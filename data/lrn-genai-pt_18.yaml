- en: 15 Diffusion models and text-to-image Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15种扩散模型和文本到图像Transformer
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How forward diffusion and reverse diffusion work
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向扩散和反向扩散是如何工作的
- en: How to build and train a denoising U-Net model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建和训练去噪U-Net模型
- en: Using the trained U-Net to generate flower images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练好的U-Net生成花卉图像
- en: Concepts behind text-to-image Transformers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到图像Transformer背后的概念
- en: Writing a Python program to generate an image through text with DALL-E 2
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写Python程序通过DALL-E 2使用文本生成图像
- en: In recent years, multimodal large language models (LLMs) have gained significant
    attention for their ability to handle various content formats, such as text, images,
    video, audio, and code. A notable example of this is text-to-image Transformers,
    such as OpenAI’s DALL-E 2, Google’s Imagen, and Stability AI’s Stable Diffusion.
    These models are capable of generating high-quality images based on textual descriptions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，多模态大型语言模型（LLMs）因其处理各种内容格式的能力而受到广泛关注，例如文本、图像、视频、音频和代码。这一领域的显著例子包括OpenAI的DALL-E
    2、Google的Imagen和Stability AI的Stable Diffusion等文本到图像Transformer。这些模型能够根据文本描述生成高质量的图像。
- en: 'These text-to-image models comprise three essential components: a text encoder
    that compresses text into a latent representation, a method to incorporate text
    information into the image generation process, and a diffusion mechanism to gradually
    refine an image to produce realistic output. Understanding the diffusion mechanism
    is particularly crucial for comprehending text-to-image Transformers, as diffusion
    models form the foundation of all leading text-to-image Transformers. For this
    reason, you will start by building and training a diffusion model to generate
    flower images in this chapter. This will provide you with a deep understanding
    of the forward diffusion process, where noise is incrementally added to images
    until they become random noise. Subsequently, you will train a model to reverse
    the diffusion process by gradually removing noise from images until the model
    can generate a new, clean image from random noise, resembling those in the training
    dataset.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文本到图像模型包括三个基本组件：一个文本编码器，它将文本压缩成潜在表示；一种将文本信息融入图像生成过程的方法；以及一种扩散机制，用于逐步细化图像以产生逼真的输出。理解扩散机制对于理解文本到图像Transformer尤其关键，因为扩散模型构成了所有领先文本到图像Transformer的基础。因此，你将首先在本章中构建和训练一个扩散模型来生成花卉图像。这将帮助你深入理解前向扩散过程，其中噪声逐步添加到图像中，直到它们变成随机噪声。随后，你将训练一个模型通过逐步从图像中移除噪声来逆转扩散过程，直到模型可以从随机噪声中生成一个新、干净的图像，类似于训练数据集中的图像。
- en: Diffusion models have become the go-to choice for generating high-resolution
    images. The success of diffusion models lies in their ability to simulate and
    reverse a complex noise addition process, which mimics a deep understanding of
    how images are structured and how to construct them from abstract patterns. This
    method not only ensures high quality but also maintains a balance between diversity
    and accuracy in the generated images.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型已成为生成高分辨率图像的首选选择。扩散模型的成功在于它们能够模拟和逆转复杂的噪声添加过程，这模仿了对图像结构和如何从抽象模式中构建图像的深入理解。这种方法不仅确保了高质量，而且在生成的图像中保持了多样性和准确性的平衡。
- en: 'After that, we’ll explain how a text-to-image Transformer works conceptually.
    We’ll focus on the contrastive language–image pretraining (CLIP) model developed
    by OpenAI, which is designed to comprehend and link visual and textual information.
    CLIP processes two types of inputs: images and text (typically in the form of
    captions or descriptions). These inputs are handled separately through two encoders
    in the model.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将解释文本到图像Transformer在概念上的工作方式。我们将重点关注由OpenAI开发的对比语言-图像预训练（CLIP）模型，该模型旨在理解和关联视觉和文本信息。CLIP处理两种类型的输入：图像和文本（通常是标题或描述的形式）。这些输入通过模型中的两个编码器分别处理。
- en: The image branch of CLIP employs a Vision Transformer (ViT) to encode images
    into a high-dimensional vector space, extracting visual features in the process.
    Meanwhile, the text branch uses a Transformer-based language model to encode textual
    descriptions into the same vector space, capturing semantic features from the
    text. CLIP has been trained on many pairs of matching images and text descriptions
    to closely align the representations of matching pairs in the vector space.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP的图像分支使用视觉Transformer (ViT)将图像编码到高维向量空间中，在这个过程中提取视觉特征。同时，文本分支使用基于Transformer的语言模型将文本描述编码到相同的向量空间中，从文本中捕获语义特征。CLIP已经在许多匹配图像和文本描述的配对上进行了训练，以使向量空间中匹配对的表示紧密对齐。
- en: OpenAI’s text-to-image Transformers, such as DALL-E 2, incorporate CLIP as a
    core component. In this chapter, you’ll learn to obtain an OpenAI API key and
    write a Python program to generate images using DALL-E 2 based on text descriptions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的文本到图像的Transformers，如DALL-E 2，将CLIP作为一个核心组件。在本章中，你将学习如何获取OpenAI API密钥，并编写一个Python程序，根据文本描述使用DALL-E
    2生成图像。
- en: 15.1 Introduction to denoising diffusion models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 去噪扩散模型简介
- en: The concept of diffusion-based models can be illustrated using the following
    example. Consider the goal of generating high-resolution flower images using a
    diffusion-based model. To do that, you first acquire a set of high-quality flower
    images for training. The model is then instructed to incrementally introduce small
    amounts of random noise into these images, a process known as forward diffusion.
    After many steps of adding noise, the training images eventually become random
    noise. The next phase involves training the model to reverse this process, starting
    with pure noise images and progressively reducing the noise until the images are
    indistinguishable from those in the original training set.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下示例可以说明基于扩散的模型的概念。考虑使用基于扩散的模型生成高分辨率花卉图像的目标。为此，你首先需要获取一组高质量的花卉图像用于训练。然后，指导模型逐步将这些图像中引入少量随机噪声，这个过程被称为正向扩散。经过多次添加噪声的步骤后，训练图像最终变成随机噪声。下一阶段涉及训练模型逆转这个过程，从纯噪声图像开始，逐步减少噪声，直到图像与原始训练集中的图像无法区分。
- en: Once trained, the model is given random noise images to work with. It systematically
    eliminates noise from the image over many iterations until it generates a high-resolution
    flower image that resembles those in the training set. This is the underlying
    principle of diffusion-based models.^([1](#footnote-003))
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，模型将得到随机噪声图像进行处理。它通过多次迭代系统地消除图像中的噪声，直到生成一个与训练集中相似的、高分辨率的花卉图像。这就是基于扩散模型的底层原理。[1](#footnote-003)
- en: In this section, you will first explore the mathematical foundations of diffusion-based
    models. Then you will dive into the architecture of U-Nets, the type of model
    used for denoising images and producing high-resolution flower images. Specifically,
    the U-Net employs a scaled dot product attention (SDPA) mechanism, similar to
    what you have seen in Transformer models in chapters 9 to 12\. Finally, you will
    learn the training process of diffusion-based models and the image-generation
    process of the trained model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将首先探索基于扩散的模型的数学基础。然后，你将深入了解U-Nets的架构，这是一种用于去噪图像和生成高分辨率花卉图像的模型类型。具体来说，U-Net采用缩放点积注意力（SDPA）机制，类似于你在第9章到第12章中看到的Transformer模型。最后，你将学习基于扩散的模型的训练过程以及训练模型的图像生成过程。
- en: 15.1.1 The forward diffusion process
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.1 正向扩散过程
- en: Several papers have proposed diffusion-based models with similar underlying
    mechanisms.^([2](#footnote-002)) Let’s use the flower images as a concrete example
    to explain the idea behind denoising diffusion models. Figure 15.1 is a diagram
    of how the forward diffusion process works.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 几篇论文提出了具有类似底层机制的基于扩散的模型。[2](#footnote-002) 让我们以花卉图像作为一个具体的例子来解释去噪扩散模型背后的思想。图15.1展示了正向扩散过程的工作原理。
- en: '![](../../OEBPS/Images/CH15_F01_Liu.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH15_F01_Liu.png)'
- en: Figure 15.1 A diagram of the forward diffusion process. We start with a clean
    image from the training set, *x*[0], and add noise є[0] to it to form a noisy
    image *x*[١] = √(1 – *β*[1])*x*[0] + √(*β*[١])є[0]. We repeat this process for
    1,000 time steps until the image *x*[1000] becomes random noise.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 正向扩散过程的示意图。我们从训练集中的干净图像*x*[0]开始，向其添加噪声є[0]，形成噪声图像*x*[1] = √(1 – *β*[1])*x*[0]
    + √(*β*[1])є[0]。我们重复这个过程1000次，直到图像*x*[1000]变成随机噪声。
- en: 'Assume that flower images, *x*[0] (illustrated in the left image in figure
    15.1), follow a distribution of q(x). In the forward diffusion process, we’ll
    add small amounts of noise to the images in each of the T = 1,000 steps. The noise
    tensor is normally distributed and has the same shape as the flower images: (3,
    64, 64), meaning three color channels, with a height and width of 64 pixels.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设花朵图像 *x*[0]（如图 15.1 左侧图像所示）遵循 q(x) 分布。在正向扩散过程中，我们将向每个 T = 1,000 步中的图像添加少量噪声。噪声张量是正态分布的，其形状与花朵图像相同：(3,
    64, 64)，表示三个颜色通道，高度和宽度为 64 像素。
- en: Time steps in diffusion models
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型中的时间步
- en: In diffusion models, time steps refer to the discrete stages during the process
    of gradually adding noise to data and subsequently reversing this process to generate
    samples. The forward phase of a diffusion model progressively adds noise over
    a series of time steps, transforming data from its original, clean state into
    a noisy distribution. During the reverse phase, the model operates over a similar
    series of time steps but in a reverse order. It systematically removes noise from
    the data to reconstruct the original or generate new, high-fidelity samples. Each
    time step in this reverse process involves predicting the noise that was added
    in the corresponding forward step and subtracting it, thereby gradually denoising
    the data until reaching a clean state.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩散模型中，时间步指的是在逐渐向数据添加噪声并随后逆转此过程以生成样本的过程中所经历的离散阶段。扩散模型的前向阶段在一系列时间步中逐步添加噪声，将数据从其原始、干净的状态转换为噪声分布。在反向阶段，模型在类似的一系列时间步中操作，但顺序相反。它系统地从数据中去除噪声以重建原始数据或生成新的、高保真度的样本。在此反向过程的每个时间步中，都涉及预测在相应的正向步骤中添加的噪声并将其减去，从而逐渐去除数据直到达到干净状态。
- en: 'In time step 1, we add noise є[0] to the image *x*[0], so that we obtain a
    noisy image *x*[1]:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 1 中，我们将噪声 є[0] 添加到图像 *x*[0]，从而获得一个噪声图像 *x*[1]：
- en: '|'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ01.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH15_F01_Liu-EQ01.png)'
- en: '| (15.1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| (15.1) |'
- en: That is, *x*[1] is a weighted sum of *x*[0] and є[0], where *β*[1] measures
    the weight placed on the noise. The value of *β* changes in different time steps—hence
    the subscript in *β*[1]. If we assume *x*[0] and є[0] are independent of each
    other and follow a standard normal distribution (i.e., with mean 0 and variance
    1), the noisy image *x*[1] will also follow a standard normal distribution. This
    is easy to prove since
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 即，*x*[1] 是 *x*[0] 和 є[0] 的加权求和，其中 *β*[1] 衡量了噪声的权重。*β* 的值在不同时间步中会变化——因此 *β*[1]
    中的下标。如果我们假设 *x*[0] 和 є[0] 之间相互独立，并且遵循标准正态分布（即，均值为 0，方差为 1），则噪声图像 *x*[1] 也将遵循标准正态分布。这很容易证明，因为
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH15_F01_Liu-EQ02.png)'
- en: and
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ03.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH15_F01_Liu-EQ03.png)'
- en: We can keep adding noise to the image for the next T–1 time steps so that
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在接下来的 T-1 个时间步中继续向图像添加噪声，以便
- en: '|'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ04.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH15_F01_Liu-EQ04.png)'
- en: '| (15.2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| (15.2) |'
- en: We can use a reparameterization trick and define *α[t]* = 1 − *β[t]* and
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用重新参数化技巧并定义 *α[t]* = 1 − *β[t]* 和
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ05.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH15_F01_Liu-EQ05.png)'
- en: to allow us to sample *x[t]* at any arbitrary time step t, where t can take
    any value in [1, 2, . . ., T−1, T]. Then we have
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以便我们可以在任意时间步 t 对 *x[t]* 进行采样，其中 t 可以取 [1, 2, . . ., T−1, T] 中的任何值。然后我们有
- en: '|'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ06.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH15_F01_Liu-EQ06.png)'
- en: '| (15.3) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| (15.3) |'
- en: Where є is a combination of є[0], є[1], . . ., and є*[t]*[–1], using the fact
    that we can add two normal distributions to obtain a new normal distribution.
    See, for example, the blog of Lilian Weng at [https://mng.bz/Aalg](https://mng.bz/Aalg)
    for proof.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 є 是 є[0]、є[1]、...、є*[t]*[–1] 的组合，利用我们可以将两个正态分布相加以获得一个新的正态分布这一事实。例如，请参阅 Lilian
    Weng 的博客 [https://mng.bz/Aalg](https://mng.bz/Aalg) 以获取证明。
- en: The farther left of figure 15.1 shows a clean flower, *x*[0], from the training
    set. In the first time step, we inject noise є[0] to it to form a noisy image
    *x*[1] (second image in figure 15.1). We repeat this process for 1,000 time steps,
    until the image becomes random noise (the rightmost image).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1 的左侧显示了来自训练集的干净花朵 *x*[0]。在第一步中，我们向其注入噪声 є[0]，以形成噪声图像 *x*[1]（图 15.1 中的第二个图像）。我们重复此过程
    1,000 个时间步，直到图像变成随机噪声（最右侧的图像）。
- en: 15.1.2 Using the U-Net model to denoise images
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.2 使用 U-Net 模型进行图像去噪
- en: Now that you understand the forward diffusion process, let’s discuss the reverse
    diffusion process (i.e., the denoising process). If we can train a model to reverse
    the forward diffusion process, we can feed the model with random noise and ask
    it to produce a noisy flower image. We can then feed the noisy image to the trained
    model again and produce a clearer, though still noisy, image. We can iteratively
    repeat the process for many time steps until we obtain a clean image, indistinguishable
    from images from the training set. The use of multiple inference steps in the
    reverse diffusion process, rather than just a single step, is crucial for gradually
    reconstructing high-quality data from a noisy distribution. It allows for a more
    controlled, stable, and high-quality generation of data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了正向扩散过程，让我们来讨论反向扩散过程（即去噪过程）。如果我们能够训练一个模型来逆转正向扩散过程，我们可以向模型输入随机噪声并要求它生成一个带噪声的花朵图像。然后我们可以将这个带噪声的图像再次输入到训练好的模型中，生成一个更清晰、但仍然带噪声的图像。我们可以迭代重复这个过程多次，直到我们获得一个干净的图像，与训练集中的图像无法区分。在反向扩散过程中使用多个推理步骤，而不是仅仅一个步骤，对于逐渐从噪声分布中重建高质量数据至关重要。它允许更可控、稳定且高质量地生成数据。
- en: To that end, we’ll create a denoising U-Net model. The U-Net architecture, which
    was originally designed for biomedical image segmentation, is characterized by
    its symmetric shape, with a contracting path (encoder) and an expansive path (decoder),
    connected by a bottleneck layer. In the context of denoising, U-Net models are
    adapted to remove noise from images while preserving important details. U-Nets
    outperform simple convolutional networks in denoising tasks due to their efficient
    capturing of local and global features in images.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，我们将创建一个去噪 U-Net 模型。U-Net 架构最初是为生物医学图像分割设计的，其特点是具有对称形状，包括收缩路径（编码器）和扩展路径（解码器），通过瓶颈层连接。在去噪的背景下，U-Net
    模型被调整为从图像中去除噪声，同时保留重要细节。U-Net 在去噪任务中优于简单的卷积网络，因为它们能够有效地捕捉图像中的局部和全局特征。
- en: Figure 15.2 is a diagram of the structure of the denoising U-Net we use in this
    chapter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.2 是本章中使用的去噪 U-Net 结构图。
- en: The model takes a noisy image and the time step the noisy image is in (*x[t]*
    and t in equation 15.3) as input and predicts the noise in the image (i.e., є).
    Since the noisy image is a weighted sum of the original clean image and noise
    (see equation 15.3), knowing the noise allows us to deduce and reconstruct the
    original image.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型以一个带噪声的图像和带噪声图像的时间步长（*x[t]* 和方程 15.3 中的 t）作为输入，并预测图像中的噪声（即 є）。由于带噪声的图像是原始干净图像和噪声的加权总和（参见方程
    15.3），了解噪声使我们能够推断并重建原始图像。
- en: The contracting path (i.e., the encoder; left side of figure 15.2) consists
    of multiple convolutional layers and pooling layers. It progressively downsamples
    the image, extracting and encoding features at different levels of abstraction.
    This part of the network learns to recognize patterns and features that are relevant
    for denoising.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 收缩路径（即编码器；图 15.2 的左侧）由多个卷积层和池化层组成。它逐步下采样图像，在不同抽象级别提取和编码特征。这一部分网络学会识别与去噪相关的模式和特征。
- en: The bottleneck layer (bottom of figure 15.2) connects the encoder and decoder
    paths. It consists of convolutional layers and is responsible for capturing the
    most abstract representations of the image.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 瓶颈层（图 15.2 的底部）连接编码器和解码器路径。它由卷积层组成，负责捕捉图像的最抽象表示。
- en: The expansive path (i.e., the decoder; right side of figure 15.2) consists of
    upsampling layers and convolutional layers. It progressively upsamples the feature
    maps, reconstructing the image while incorporating features from the encoder through
    skip connections. Skip connections (denoted by dashed lines in figure 15.2) are
    crucial in U-Net models, as they allow the model to retain fine-grained details
    from the input image by combining low-level and high-level features. Next, I briefly
    explain how skip connections work.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展路径（即解码器；图 15.2 的右侧）由上采样层和卷积层组成。它逐步上采样特征图，在通过跳跃连接结合编码器特征的同时重建图像。跳跃连接（图 15.2
    中用虚线表示）在 U-Net 模型中至关重要，因为它们允许模型通过结合低级和高级特征来保留输入图像的精细细节。接下来，我将简要解释跳跃连接是如何工作的。
- en: '![](../../OEBPS/Images/CH15_F02_Liu.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH15_F02_Liu.png)'
- en: Figure 15.2 The architecture of the denoising U-Net model. The U-Net architecture
    is characterized by its symmetric shape, with a contracting path (encoder) and
    an expansive path (decoder), connected by a bottleneck layer. The model is designed
    to remove noise from images while preserving important details. The input to the
    model is a noisy image, along with which time step the image is in, and the output
    is the predicted noise in the image.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2 去噪U-Net模型架构。U-Net架构以其对称形状为特征，包括收缩路径（编码器）和扩展路径（解码器），通过瓶颈层连接。该模型旨在去除图像中的噪声，同时保留重要细节。模型的输入是一个噪声图像，以及图像所在的时间步，输出是图像中的预测噪声。
- en: In a U-Net model, skip connections are implemented by concatenating feature
    maps from the encoder path with corresponding feature maps in the decoder path.
    These feature maps are typically of the same spatial dimensions but may have been
    processed differently due to the separate paths they have traversed. During the
    encoding process, the input image is progressively downsampled, and some spatial
    information (such as edges and textures) may be lost. Skip connections help preserve
    this information by directly passing feature maps from the encoder to the decoder,
    bypassing the information bottleneck.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在U-Net模型中，跳过连接是通过将编码路径的特征图与解码路径中的相应特征图连接起来实现的。这些特征图通常具有相同的空间维度，但由于它们经过的路径不同，可能已经以不同的方式处理过。在编码过程中，输入图像逐渐下采样，一些空间信息（如边缘和纹理）可能会丢失。跳过连接通过直接从编码器传递特征图到解码器，绕过信息瓶颈，帮助保留这些信息。
- en: For example, the dashed line at the top of figure 15.2 indicates that the model
    concatenates the *output* from the Conv2D layer in the encoder, which has a shape
    of (128, 64, 64), with the *input* to the Conv2D layer in the decoder, which also
    has a shape of (128, 64, 64). As a result, the final input to the Conv2D layer
    in the decoder has a shape of (256, 64, 64).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图15.2顶部的虚线表示模型将编码器中的Conv2D层的输出（形状为（128，64，64））与解码器中的Conv2D层的输入（形状也为（128，64，64））连接起来。因此，解码器中Conv2D层的最终输入形状为（256，64，64）。
- en: By combining high-level, abstract features from the decoder with low-level,
    detailed features from the encoder, skip connections enable the model to better
    reconstruct fine details in the denoised image. This is particularly important
    in denoising tasks, where retaining subtle image details is crucial.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将解码器中的高级、抽象特征与编码器中的低级、详细特征相结合，跳过连接使模型能够更好地重建去噪图像中的细微细节。这在去噪任务中尤为重要，因为保留细微的图像细节至关重要。
- en: The scaled dot product attention (SDPA) mechanism is implemented in both the
    final block of the contracting path and the final block of the expansive path
    in our denoising U-Net model, accompanied by layer normalization and residual
    connections (as shown in figure 15.2 with the label Attn/Norm/Add). This SDPA
    mechanism is essentially the same as the one we developed in chapter 9; the key
    difference is its application to image pixels rather than text tokens.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的去噪U-Net模型中，缩放点积注意力（SDPA）机制在收缩路径的最后一个块和扩展路径的最后一个块中实现，伴随着层归一化和残差连接（如图15.2中标记为Attn/Norm/Add）。这个SDPA机制本质上与我们第9章中开发的相同；关键区别在于它应用于图像像素而不是文本标记。
- en: The use of skip connections and the model’s size lead to redundant feature extractions
    in our denoising U-Net, ensuring that no important feature is lost during the
    denoising process. However, the large size of the model also complicates the identification
    of relevant features, akin to searching for a needle in a haystack. The attention
    mechanism empowers the model to emphasize significant features while disregarding
    irrelevant ones, thereby enhancing the effectiveness of the learning process.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 跳过连接的使用和模型的大小导致我们的去噪U-Net中存在冗余的特征提取，确保在去噪过程中不会丢失任何重要特征。然而，模型的大尺寸也使得识别相关特征变得复杂，就像在干草堆里找针一样。注意力机制赋予模型强调显著特征并忽略不相关特征的能力，从而增强了学习过程的有效性。
- en: 15.1.3 A blueprint to train the denoising U-Net model
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.3 训练去噪U-Net模型的蓝图
- en: The output of the denoising U-Net is the noise injected into the noisy image.
    The model is trained to minimize the difference between the output (predicted
    noise) and the ground truth (actual noise).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪U-Net的输出是注入到噪声图像中的噪声。该模型经过训练以最小化输出（预测噪声）与真实噪声之间的差异。
- en: The denoising U-Net model uses the U-Net architecture’s ability to capture both
    local and global context, making it effective for removing noise while preserving
    important details such as edges and textures. These models are widely used in
    various applications, including medical image denoising, photographic image restoration,
    and more. Figure 15.3 is a diagram of the training process of our denoising U-Net
    model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪U-Net模型利用U-Net架构捕捉局部和全局上下文的能力，使其在去除噪声的同时保留重要细节（如边缘和纹理）变得有效。这些模型在各种应用中得到了广泛应用，包括医学图像去噪、摄影图像恢复等。图15.3展示了我们去噪U-Net模型的训练过程。
- en: '![](../../OEBPS/Images/CH15_F03_Liu.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH15_F03_Liu.png)'
- en: Figure 15.3 The training process of the denoising U-Net model. We first obtain
    clean flower images as our training set. We add noise to clean flower images and
    present them to the U-Net model. The model predicts the noise in the noisy images.
    We compare the predicted noise with the actual noise injected into the flower
    images and tweak the model weights to minimize the mean absolute error.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3 去噪U-Net模型的训练过程。我们首先获取干净的花卉图像作为我们的训练集。我们向干净的花卉图像添加噪声，并将其呈现给U-Net模型。模型预测噪声图像中的噪声。我们将预测的噪声与注入花卉图像的实际噪声进行比较，并调整模型权重以最小化平均绝对误差。
- en: The first step is to gather a dataset of flower images. We’ll use the Oxford
    102 Flower dataset as our training set. We’ll resize all images to a fixed resolution
    of 64 × 64 pixels and normalize pixel values to the range [–1, 1]. For denoising,
    we need pairs of clean and noisy images. We’ll synthetically add noise to the
    clean flower images to create noisy counterparts (step 2 in figure 15.3) based
    on the formula specified in equation 15.3.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是收集花卉图像数据集。我们将使用牛津102花卉数据集作为我们的训练集。我们将所有图像调整到64 × 64像素的固定分辨率，并将像素值归一化到范围[–1,
    1]。对于去噪，我们需要成对的干净和噪声图像。我们将根据方程15.3中指定的公式，对干净的花卉图像添加噪声，以创建噪声对应图像（图15.3中的步骤2）。
- en: We’ll then build a denoising U-Net model with a structure as outlined in figure
    15.2\. During each epoch of training, we iterate over the dataset in batches.
    We add noise to the flower images and present the noisy images to the U-Net model
    (step 3), along with the time steps the noisy images are in, t. The U-Net model
    predicts the noise in the noisy images (step 4) based on current parameters in
    the model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将根据图15.2中概述的结构构建一个去噪U-Net模型。在训练的每个epoch中，我们以批处理的方式遍历数据集。我们向花卉图像添加噪声，并将噪声图像及其在噪声图像中的时间步长t呈现给U-Net模型（步骤3）。U-Net模型根据模型中的当前参数预测噪声图像中的噪声（步骤4）。
- en: We compare the predicted noise with the actual noise and calculate the L1 loss
    (i.e., mean absolute error) at the pixel level (step 5). L1 loss is usually preferred
    in such situations because it’s less sensitive to outliers compared to the L2
    loss (mean squared error). We then tweak the model parameters to minimize the
    L1 loss (step 6) so that in the next iteration, the model makes better predictions.
    We repeat this process for many iterations until the model parameters converge.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将预测的噪声与实际噪声进行比较，并在像素级别计算L1损失（即平均绝对误差）（步骤5）。在这种情况下，L1损失通常比L2损失（均方误差）对异常值更不敏感，因此更受欢迎。然后，我们调整模型参数以最小化L1损失（步骤6），以便在下一轮迭代中，模型能做出更好的预测。我们将重复此过程多次，直到模型参数收敛。
- en: 15.2 Preparing the training data
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 准备训练数据
- en: We’ll use the Oxford 102 Flower dataset, which is freely available on Hugging
    Face, as our training data. The dataset contains about 8,000 flower images and
    can be downloaded directly by using the *datasets* library you installed earlier.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用牛津102花卉数据集，该数据集可在Hugging Face上免费获取，作为我们的训练数据。该数据集包含大约8,000张花卉图像，您可以使用之前安装的*datasets*库直接下载。
- en: To save space, we’ll place most helper functions and classes in two local modules,
    ch15util.py and unet_util.py. Download these two files from the book’s GitHub
    repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    and place them in the /utils/ folder on your computer. The Python programs in
    this chapter are adapted from Hugging Face’s GitHub repository ([https://github.com/huggingface/diffusers](https://github.com/huggingface/diffusers))
    and Filip Basara’s GitHub repository ([https://github.com/filipbasara0/simple-diffusion](https://github.com/filipbasara0/simple-diffusion)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省空间，我们将大多数辅助函数和类放在两个本地模块中，ch15util.py和unet_util.py。从本书的GitHub仓库（[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)）下载这两个文件，并将它们放在您计算机上的/utils/文件夹中。本章中的Python程序是从Hugging
    Face的GitHub仓库（[https://github.com/huggingface/diffusers](https://github.com/huggingface/diffusers)）和Filip
    Basara的GitHub仓库（[https://github.com/filipbasara0/simple-diffusion](https://github.com/filipbasara0/simple-diffusion)）改编的。
- en: You’ll use Python to download the dataset to your computer. After that, we’ll
    demonstrate the forward diffusion process by gradually adding noise to clean images
    in the training dataset until they become random noise. Finally, you’ll place
    the training data in batches so that we can use them to train the denoising U-Net
    model later in the chapter.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用Python将数据集下载到您的计算机上。之后，我们将通过逐渐向训练数据集中的干净图像添加噪声，直到它们变成随机噪声，来演示前向扩散过程。最后，您将批量放置训练数据，以便我们可以在本章后面使用它们来训练去噪U-Net模型。
- en: 'You’ll use the following Python libraries in this chapter: datasets, einops,
    diffusers, and openai. To install these libraries, execute the following line
    of code in a new cell in your Jupyter Notebook application on your computer:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您在本章中将使用以下Python库：datasets、einops、diffusers和openai。要安装这些库，请在您的计算机上的Jupyter Notebook应用程序的新单元格中执行以下代码行：
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Follow the on-screen instructions to finish the installation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 按照屏幕上的说明完成安装。
- en: 15.2.1 Flower images as the training data
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.1 将花卉图像作为训练数据
- en: The `load_dataset()` method from the *datasets* library you installed earlier
    allows you to directly download the Oxford 102 Flower dataset from Hugging Face.
    We’ll then use the *matplotlib* library to show some flower images in the dataset
    so that we have an idea of what the images in the training dataset look like.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您之前安装的*datasets*库中的`load_dataset()`方法允许您直接从Hugging Face下载牛津102花卉数据集。然后我们将使用*matplotlib*库显示数据集中的某些花卉图像，以便我们了解训练数据集中的图像是什么样的。
- en: Run the lines of code shown in the following listing in a cell in Jupyter Notebook.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook的单元格中运行以下列表中的代码行。
- en: Listing 15.1 Downloading and visualizing flower images
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.1 下载和可视化花卉图像
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Downloads the images from Hugging Face
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从Hugging Face下载图像
- en: ② Plots the first 16 images
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ② 绘制前16张图像
- en: After running the preceding code listing, you’ll see the first 16 flower images
    in the dataset, as displayed in figure 15.4\. These are high-resolution color
    images of various types of flowers. We have standardized the size of each image
    to (3, 64, 64).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码列表后，您将看到数据集中的前16张花卉图像，如图15.4所示。这些是各种类型花卉的高分辨率彩色图像。我们已经将每张图像的大小标准化为（3，64，64）。
- en: '![](../../OEBPS/Images/CH15_F04_Liu.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH15_F04_Liu.png)'
- en: Figure 15.4 The first 16 Images from the Oxford 102 Flower dataset.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4 来自牛津102花卉数据集的前16张图像。
- en: 'We place the dataset in batches of 4 so that we can use them to train the denoising
    U-Net model later. We choose a batch size of 4 to keep the memory size small enough
    to fit on a GPU during training. Adjust the batch size to 2 or even 1 if your
    GPU memory is small:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集分成每批4个，以便我们可以在本章后面使用它们来训练去噪U-Net模型。我们选择批大小为4，以保持内存大小足够小，以便在训练过程中适合GPU。如果您的GPU内存较小，请将批大小调整为2甚至1：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we’ll code in and visualize the forward diffusion process.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编写代码并可视化前向扩散过程。
- en: 15.2.2 Visualizing the forward diffusion process
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.2 可视化前向扩散过程
- en: We have defined a class `DDIMScheduler()` in the local module ch15util.py you
    just downloaded. Take a look at the class in the file; we’ll use it to add noise
    to images. We’ll also use the class to produce clean images later, along with
    the trained denoising U-Net model. The `DDIMScheduler()` class manages the step
    sizes and sequence of denoising steps, enabling deterministic inference that can
    produce high-quality samples through the denoising process.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本地模块 ch15util.py 中定义了一个 `DDIMScheduler()` 类，您刚刚下载。请查看该文件中的类；我们将使用它向图像添加噪声。我们还将使用该类来生成干净的图像，以及训练好的去噪
    U-Net 模型。`DDIMScheduler()` 类管理步长大小和去噪步骤的顺序，通过去噪过程实现确定性推理，从而可以生成高质量的样本。
- en: 'We first select four clean images from the training set and generate noise
    tensors that have the same shape as these images:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从训练集中选择四张干净图像，并生成与这些图像形状相同的噪声张量：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Obtains four clean images
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ① 获取四张干净图像
- en: ② Generates a tensor, noise, which has the same shape as the clean images; each
    value in the noise follows an independent standard normal distribution.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ② 生成一个与干净图像形状相同的张量，噪声；噪声中的每个值都遵循独立的标准正态分布。
- en: The output from the preceding code block is
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码块输出的结果是
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Both the images and the noise tensors have a shape of (4, 3, 64, 64), meaning
    4 images in the batch and 3 color channels per image, and the height and width
    of the images are 64 pixels.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和噪声张量的形状都是 (4, 3, 64, 64)，这意味着批处理中有 4 张图像，每张图像有 3 个颜色通道，图像的高度和宽度为 64 像素。
- en: During the forward diffusion process, there are 999 transitional noisy images
    between the clean images (*x*[0] as we explained in the first section) and random
    noise (*x[T]*). The transitional noisy images are a weighted sum of the clean
    image and the noise. As *t* goes from 0 to 1,000, the weight on the clean image
    gradually decreases, and the weight on the noise gradually increases, as specified
    in equation 15.3.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向扩散过程中，在干净图像 (*x*[0] 如我们在第一部分中解释的) 和随机噪声 (*x[T]*) 之间有 999 个过渡噪声图像。过渡噪声图像是干净图像和噪声的加权总和。随着
    *t* 从 0 到 1,000 的变化，干净图像的权重逐渐降低，噪声的权重逐渐增加，如方程式 15.3 所述。
- en: Next, we generate and visualize some transitional noisy images.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们生成并可视化一些过渡噪声图像。
- en: Listing 15.2 Visualizing the forward diffusion process
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.2 可视化正向扩散过程
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Instantiates the DDIMScheduler() class with 1,000 time steps
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用 1,000 个时间步创建 DDIMScheduler() 类实例
- en: ② Looks at time steps 200, 400, 600, 800, and 1,000
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ② 查看时间步 200、400、600、800 和 1,000
- en: ③ Creates noisy images at these time steps
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在这些时间步创建噪声图像
- en: ④ Concatenates noisy images with clean images
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将噪声图像与干净图像连接
- en: ⑤ Displays all images
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 显示所有图像
- en: 'The `add_noise()` method in the `DDIMScheduler()` class takes three arguments:
    `clean_images`, `noise`, and `timesteps`. It produces a weighted sum of the clean
    image and the noise, which is a noisy image. Further, the weight is a function
    of the time step, t. As the time step, t, moves from 0 to 1,000, the weight on
    the clean image decreases and that on the noise increases. If you run the previous
    code listing, you’ll see an image similar to figure 15.5.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`DDIMScheduler()` 类中的 `add_noise()` 方法接受三个参数：`clean_images`、`noise` 和 `timesteps`。它产生干净图像和噪声的加权总和，即噪声图像。此外，权重是时间步
    t 的函数。随着时间步 t 从 0 到 1,000 的变化，干净图像的权重逐渐降低，噪声的权重逐渐增加。如果您运行前面的代码列表，您将看到类似于图 15.5
    的图像。'
- en: '![](../../OEBPS/Images/CH15_F05_Liu.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH15_F05_Liu.png)'
- en: Figure 15.5 The forward diffusion process. The four images in the first column
    are clean images from the training dataset. We then gradually add noise to these
    images from time step 1 to time step 1,000\. As the time step increases, more
    and more noise is injected into the images. The four images in the second column
    are images after 200 time steps. The third column contains images after 400 time
    steps, and they have more noise than those in the second column. The last column
    contains images after 1,000 time steps, and they are 100% random noise.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.5 正向扩散过程。第一列中的四张图像是训练数据集中的干净图像。然后我们从时间步 1 到时间步 1,000 逐渐向这些图像添加噪声。随着时间步的增加，图像中注入的噪声越来越多。第二列中的四张图像是在
    200 个时间步之后的图像。第三列包含 400 个时间步之后的图像，它们的噪声比第二列中的图像更多。最后一列包含 1,000 个时间步之后的图像，它们是 100%
    的随机噪声。
- en: The first column contains the four clean images without noise. As we move to
    the right, we gradually add more and more noise to the images. The very last column
    contains pure random noise.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列包含没有噪声的四个干净图像。随着向右移动，我们逐渐向图像添加越来越多的噪声。最后一列包含纯随机噪声。
- en: 15.3 Building a denoising U-Net model
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 构建噪声去除U-Net模型
- en: Earlier in this chapter, we discussed the architecture of the denoising U-Net
    model. In this section, I will guide you through implementing it using Python
    and PyTorch.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期部分，我们讨论了噪声去除U-Net模型的架构。在本节中，我将指导您使用Python和PyTorch来实现它。
- en: The U-Net model we are going to construct is quite large, containing over 133
    million parameters, reflecting the complexity of its intended task. It is engineered
    to capture both local and global features within an image through a process of
    downsampling and upsampling the input. The model uses multiple convolutional layers
    interconnected by skip connections, which combine features from various levels
    of the network. This architecture helps maintain spatial information, facilitating
    more effective learning.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要构建的U-Net模型相当大，包含超过1.33亿个参数，反映了其预期任务的复杂性。它通过下采样和上采样输入来捕捉图像中的局部和全局特征。该模型使用多个通过跳跃连接相互连接的卷积层，这些层结合了网络不同级别的特征。这种架构有助于保持空间信息，从而促进更有效的学习。
- en: Given the substantial size of the denoising U-Net model and its redundant feature
    extraction, the SDPA attention mechanism is employed to enable the model to concentrate
    on the most relevant aspects of the input for the task at hand. To compute SDPA
    attention, we will flatten the image and treat its pixels as a sequence. We will
    then use SDPA to learn the dependencies among different pixels in the image in
    a manner akin to how we learned dependencies among different tokens in text in
    chapter 9.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于噪声去除U-Net模型规模庞大且特征提取冗余，我们采用了SDPA注意力机制，使模型能够专注于当前任务中最相关的输入方面。为了计算SDPA注意力，我们将图像展平并将像素视为一个序列。然后我们将使用SDPA以类似于在第9章中学习文本中不同标记之间依赖关系的方式，学习图像中不同像素之间的依赖关系。
- en: 15.3.1 The attention mechanism in the denoising U-Net model
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.1 噪声去除U-Net模型中的注意力机制
- en: To implement the attention mechanism, we have defined an `Attention()` class
    in the local module ch15util.py, as shown in the following code listing.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现注意力机制，我们在本地模块ch15util.py中定义了一个`Attention()`类，如下面的代码列表所示。
- en: Listing 15.3 The attention mechanism in the denoising U-Net model
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.3 噪声去除U-Net模型中的注意力机制
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Passes the input through three linear layers to obtain query, key, and value
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将输入通过三个线性层传递以获得查询、键和值
- en: ② Splits query, key, and value into four heads
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将查询、键和值分成四个头
- en: ③ Calculates attention weights
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算注意力权重
- en: ④ Calculates the attention vector in each head
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算每个头中的注意力向量
- en: ⑤ Concatenates the four attention vectors into one
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将四个注意力向量连接成一个
- en: The output after running the preceding code listing is
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码列表后的输出是
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The attention mechanism used here, SDPA, is the same as the one we utilized
    in chapter 9, where we applied SDPA to a sequence of indices representing tokens
    in text. Here, we apply it to pixels in an image. We treat the flattened pixels
    of an image as a sequence and use SDPA to extract dependencies among different
    areas of the input image, enhancing the efficiency of the denoising process.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的注意力机制SDPA与我们在第9章中使用的相同，当时我们将SDPA应用于表示文本中标记的索引序列。在这里，我们将它应用于图像中的像素。我们将图像的展平像素视为一个序列，并使用SDPA提取输入图像不同区域之间的依赖关系，从而提高去噪过程的效率。
- en: 'Listing 15.3 demonstrates how SDPA operates in our context. To give you a concrete
    example, we have created a hypothetical image, x, with dimensions (1, 128, 64,
    64), indicating one image in the batch, 128 feature channels, and a size of 64
    × 64 pixels in each channel. The input x is then processed through the attention
    layer. Specifically, each feature channel in the image is flattened into a sequence
    of 64 × 64 = 4,096 pixels. This sequence is then passed through three distinct
    neural network layers to produce the query Q, key K, and value V, which are subsequently
    split into four heads. The attention vector in each head is calculated as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.3展示了SDPA在我们上下文中的操作方式。为了给您一个具体的例子，我们创建了一个假设的图像，x，其维度为（1，128，64，64），表示批次中的一个图像，128个特征通道，每个通道的大小为64
    × 64像素。然后输入x通过注意力层进行处理。具体来说，图像中的每个特征通道被展平成一个64 × 64 = 4,096像素的序列。然后这个序列通过三个不同的神经网络层传递，以产生查询Q、键K和值V，随后将它们分成四个头。每个头中的注意力向量计算如下：
- en: '![](../../OEBPS/Images/CH15_F05_Liu-EQ07.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH15_F05_Liu-EQ07.png)'
- en: where *d[k]* represents the dimension of the key vector K. The attention vectors
    from the four heads are concatenated back into a single attention vector.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *d[k]* 代表关键向量 K 的维度。来自四个头的注意力向量被连接回一个单独的注意力向量。
- en: 15.3.2 The denoising U-Net model
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.2 去噪U-Net模型
- en: In the local module unet_util.py you just downloaded, we have defined a `UNet()`
    class to represent the denoising U-Net model. Take a look at the definition in
    the file, and I’ll provide a brief explanation of how it works later. The following
    code listing presents a portion of the `UNet()` class.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在您刚刚下载的本地模块 unet_util.py 中，我们定义了一个 `UNet()` 类来表示去噪U-Net模型。请查看文件中的定义，稍后我会简要解释其工作原理。以下代码列表展示了
    `UNet()` 类的一部分。
- en: Listing 15.4 Defining the `UNet()` class
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.4 定义 `UNet()` 类
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① The model takes a batch of noisy images and the time steps as input.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ① 模型接受一批噪声图像和时间步长作为输入。
- en: ② The embedded time steps are added to the images as inputs in various stages.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将嵌入的时间步长作为输入添加到图像的各个阶段。
- en: ③ Passes the input through the contracting path
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将输入通过收缩路径传递
- en: ④ Passes the input through the bottleneck path
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 通过瓶颈路径传递输入
- en: ⑤ Passes the input through the expansive path, with skip connections
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 通过具有跳跃连接的扩张路径传递输入
- en: ⑥ The output is the predicted noise in the input images.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 输出是输入图像中预测的噪声。
- en: The job of the denoising U-Net is to predict the noise in the input images based
    on the time steps these images are in. As described in equation 15.3, a noisy
    image at any time step t, *x[t]*, can be represented as a weighted sum of the
    clean image, *x*[o], and standard normally distributed random noise, є. The weight
    assigned to the clean image decreases, and the weight assigned to the random noise
    increases as the time step t progresses from 0 to T. Therefore, to deduce the
    noise in noisy images, the denoising U-Net needs to know which time step a noisy
    image is in.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪U-Net的任务是根据这些图像所在的时间步长预测输入图像中的噪声。如方程15.3所述，任何时间步长 t 的噪声图像 *x[t]* 可以表示为干净图像
    *x*[o] 和标准正态分布的随机噪声 є 的加权总和。分配给干净图像的权重随时间步长 t 从 0 到 T 的进展而减少，分配给随机噪声的权重增加。因此，为了推断噪声图像中的噪声，去噪U-Net需要知道噪声图像所在的时间步长。
- en: Time steps are embedded using sine and cosine functions in a manner akin to
    positional encoding in Transformers (discussed in chapters 9 and 10), resulting
    in a 128-value vector. These embeddings are then expanded to match the dimensions
    of the image features at various layers within the model. For instance, in the
    first down block, the time embeddings are broadcasted to a shape of (128, 64,
    64) before being added to the image features, which also have dimensions of (128,
    64, 64).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步长使用类似于Transformer（在第9章和第10章中讨论）中的位置编码的正弦和余弦函数进行嵌入，结果得到一个128值的向量。然后这些嵌入被扩展以匹配模型中各个层级的图像特征维度。例如，在第一个下采样块中，时间嵌入被广播到形状为（128，64，64），然后添加到图像特征中，这些特征也有（128，64，64）的维度。
- en: 'Next, we create a denoising U-Net model by instantiating the `UNet()` class
    in the local module:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过在本地模块中实例化 `UNet()` 类来创建一个去噪U-Net模型：
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output is
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The model has more than 133 million parameters, as you can see from the previous
    output. Given the large number of parameters, the training process in this chapter
    will be time-consuming, requiring approximately 3 to 4 hours of GPU training.
    However, for those who do not have access to GPU training, the trained weights
    are also available on my website. The link to these weights will be provided in
    the following section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 模型有超过1.33亿个参数，您可以从之前的输出中看到。鉴于参数数量庞大，本章中的训练过程将耗时，大约需要3到4小时的GPU训练时间。然而，对于那些无法访问GPU训练的人，训练好的权重也存在于我的网站上。这些权重的链接将在下一节提供。
- en: 15.4 Training and using the denoising U-Net model
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 训练和使用去噪U-Net模型
- en: Now that we have both the training data and the denoising U-Net model, we’re
    ready to train the model using the training data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了训练数据和去噪U-Net模型，我们可以准备使用训练数据来训练模型了。
- en: During each training epoch, we’ll cycle through all the batches in the training
    data. For each image, we’ll randomly select a time step and add noise to the clean
    images in the training data based on this time step value, resulting in a noisy
    image. These noisy images and their corresponding time step values are then fed
    into the denoising U-Net model to predict the noise in each image. We compare
    the predicted noise to the ground truth (the actual noise added to the image)
    and adjust the model parameters to minimize the mean absolute error between the
    predicted and actual noise.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练周期中，我们将遍历训练数据中的所有批次。对于每张图像，我们将随机选择一个时间步长，并根据这个时间步长值在训练数据中的干净图像上添加噪声，从而得到一个噪声图像。然后，我们将这些噪声图像及其对应的时间步长值输入到去噪U-Net模型中，以预测每张图像中的噪声。我们将预测的噪声与真实值（实际添加到图像中的噪声）进行比较，并调整模型参数以最小化预测噪声与实际噪声之间的平均绝对误差。
- en: After training, we’ll use the trained model to generate flower images. We’ll
    perform this generation in 50 inference steps (i.e., we’ll set time step values
    to 980, 960, . . ., 20, and 0). Starting with random noise, we’ll input it into
    the trained model to obtain a noisy image. This noisy image is then fed back into
    the trained model to denoise it. We repeat this process for 50 inference steps,
    resulting in an image that is indistinguishable from the flowers in the training
    set.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们将使用训练好的模型生成花朵图像。我们将通过50次推理步骤（即，我们将时间步长值设置为980、960、……、20和0）来完成这一生成。从随机噪声开始，我们将将其输入到训练好的模型中，以获得一个噪声图像。然后，我们将这个噪声图像反馈到训练好的模型中，以对其进行去噪。我们将重复这个过程50次推理步骤，最终得到一个与训练集中花朵不可区分的图像。
- en: 15.4.1 Training the denoising U-Net model
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.1 训练去噪U-Net模型
- en: Next, we’ll first define the optimizer and the learning rate scheduler for the
    training process.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们首先定义训练过程中的优化器和学习率调度器。
- en: We’ll use the AdamW optimizer, a variant of the Adam optimizer that we have
    been using throughout this book. The AdamW optimizer, first proposed by Ilya Loshchilov
    and Frank Hutter, decouples weight decay (a form of regularization) from the optimization
    steps.^([3](#footnote-001)) Instead of applying weight decay directly to the gradients,
    AdamW applies weight decay directly to the parameters (weights) after the optimization
    step. This modification helps achieve better generalization performance by preventing
    the decay rate from being adapted along with the learning rates. Interested readers
    can learn more about the AdamW optimizer in the original paper by Loshchilov and
    Hutter.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用AdamW优化器，这是我们在整本书中一直在使用的Adam优化器的一个变体。AdamW优化器最初由Ilya Loshchilov和Frank Hutter提出，它将权重衰减（一种正则化形式）从优化步骤中分离出来.^([3](#footnote-001))
    AdamW优化器不是直接将权重衰减应用于梯度，而是在优化步骤之后直接将权重衰减应用于参数（权重）。这种修改有助于通过防止衰减率与学习率一起调整来实现更好的泛化性能。感兴趣的读者可以在Loshchilov和Hutter的原始论文中了解更多关于AdamW优化器的内容。
- en: We will also use a learning rate scheduler from the diffusers library to adjust
    the learning rate during the training process. Initially using a higher learning
    rate can help the model escape local minima, while gradually lowering the learning
    rate in later stages of training can help the model converge more steadily and
    accurately towards a global minimum. The learning rate scheduler is defined as
    shown in the following listing.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用diffusers库中的学习率调度器来调整训练过程中的学习率。最初使用较高的学习率可以帮助模型逃离局部最小值，而在训练的后期阶段逐渐降低学习率可以帮助模型更稳定、更准确地收敛到全局最小值。学习率调度器定义如下所示。
- en: Listing 15.5 Choosing the optimizer and learning rate in training
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.5 在训练中选择优化器和学习率
- en: '[PRE11]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Will train the model for 100 epochs
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将训练模型100个周期
- en: ② Uses the AdamW optimizer
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用AdamW优化器
- en: ③ Uses the learning rate scheduler in the diffusers library to control the learning
    rate
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用diffusers库中的学习率调度器来控制学习率
- en: 'The exact definition of the `get_scheduler()` function is defined on GitHub
    by Hugging Face: [https://mng.bz/ZVo5](https://mng.bz/ZVo5). In the first 300
    training steps (warmup steps), the learning rate increases linearly from 0 to
    0.0001 (the learning rate we set in the AdamW optimizer). After 300 steps, the
    learning rate decreases following the values of the cosine function between 0.0001
    and 0\. We train the model for 100 epochs in the following listing.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_scheduler()` 函数的精确定义由 Hugging Face 在 GitHub 上定义：[https://mng.bz/ZVo5](https://mng.bz/ZVo5)。在前
    300 个训练步骤（预热步骤）中，学习率从 0 线性增加到 0.0001（我们在 AdamW 优化器中设置的学习率）。在 300 步之后，学习率按照 0.0001
    和 0 之间的余弦函数值递减。在下面的列表中，我们训练模型 100 个周期。'
- en: Listing 15.6 Training the denoising U-Net model
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.6 训练去噪 U-Net 模型
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Adds noise to clean images in the training set
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在训练集中向干净图像添加噪声
- en: ② Uses the denoising U-Net to predict noise in noisy images
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用去噪 U-Net 预测噪声图像中的噪声
- en: ③ Compares the predicted noise with the actual noise to calculate the loss
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将预测的噪声与实际噪声进行比较以计算损失
- en: ④ Tweaks model parameters to minimize the mean absolute error
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 调整模型参数以最小化平均绝对误差
- en: During each epoch, we cycle through all batches of clean flower images in the
    training set. We introduce noise to these clean images and feed them to the denoising
    U-Net to predict the noise in these images. We then compare the predicted noise
    to the actual noise and adjust the model parameters to minimize the mean absolute
    error (pixel-wise) between the two.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个周期中，我们遍历训练集中所有干净的花朵图像的批次。我们向这些干净的图像添加噪声，并将它们输入到去噪 U-Net 中以预测这些图像中的噪声。然后我们将预测的噪声与实际噪声进行比较，并调整模型参数以最小化两者之间的平均绝对误差（像素级）。
- en: The training process described here takes several hours with GPU training. After
    training, the trained model weights are saved on your computer. Alternatively,
    you can download the trained weights from my website at [https://mng.bz/RNlD](https://mng.bz/RNlD).
    Unzip the file after downloading.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的训练过程需要几个小时，使用 GPU 训练。训练后，训练好的模型权重将保存在您的计算机上。或者，您可以从我的网站 [https://mng.bz/RNlD](https://mng.bz/RNlD)
    下载训练好的权重。下载后请解压文件。
- en: 15.4.2 Using the trained model to generate flower images
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.2 使用训练好的模型生成花朵图像
- en: To generate flower images, we’ll use 50 inference steps. This means we’ll look
    at 50 equally spaced time steps between t = 0 and t = T, with T = 1,000 in our
    case. Therefore, the 50 inference time steps are t = 980, 960, 940, . . . , 20,
    and 0. We’ll start with pure random noise, which corresponds to the image at t
    = 1000. We use the trained denoising U-Net model to denoise it and create a noisy
    image at t = 980. We then present the noisy image at t = 980 to the trained model
    to denoise it and obtain the noisy image at t = 960. We repeat the process for
    many iterations until we obtain an image at t = 0, which is a clean image. This
    process is implemented through the `generate()` method in the `DDIMScheduler()`
    class within the local module ch15util.py.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成花朵图像，我们将使用 50 个推理步骤。这意味着我们将查看 t = 0 和 t = T 之间等间隔的 50 个时间步，在我们的情况下 T = 1,000。因此，50
    个推理时间步是 t = 980, 960, 940, . . . , 20, 和 0。我们将从纯随机噪声开始，这对应于 t = 1000 的图像。我们使用训练好的去噪
    U-Net 模型对其进行去噪，并在 t = 980 创建一个噪声图像。然后我们将 t = 980 的噪声图像呈现给训练好的模型进行去噪，以获得 t = 960
    的噪声图像。我们重复这个过程多次，直到获得 t = 0 的图像，这是一个干净的图像。这个过程通过在本地模块 ch15util.py 中的 `DDIMScheduler()`
    类的 `generate()` 方法实现。
- en: Listing 15.7 Defining a `generate()` method in the `DDIMScheduler()` class
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.7 在 `DDIMScheduler()` 类中定义 `generate()` 方法
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Uses random noise as the starting point (i.e., image at t = 1,000)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用随机噪声作为起点（即 t = 1,000 的图像）
- en: ② Uses 50 inference time steps (t = 980, 960, 940, . . , 20, 0)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用 50 个推理时间步（t = 980, 960, 940, . . , 20, 0）
- en: ③ Uses the trained denoising U-Net model to predict noise
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用训练好的去噪 U-Net 模型来预测噪声
- en: ④ Creates an image based on predicted noise
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 根据预测的噪声创建图像
- en: ⑤ Saves intermediate images in a list, imgs
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将中间图像保存在列表，imgs 中
- en: In this `generate()` method, we have also created a list, imgs, to store all
    intermediate images at time steps t = 980, 960,. . . , 20, and 0. We’ll use them
    to visualize the denoising process later. The `generate()` method returns a dictionary
    with the generated images and the list, imgs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 `generate()` 方法中，我们同样创建了一个列表，imgs，用于存储所有在时间步 t = 980, 960,. . . , 20, 和
    0 时的中间图像。我们将使用它们来可视化去噪过程。`generate()` 方法返回一个包含生成的图像和列表，imgs 的字典。
- en: Next, we’ll use the previous `generate()` method to create 10 clean images.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用之前的 `generate()` 方法创建 10 张干净的图像。
- en: Listing 15.8 Image generation with the trained denoising U-Net model
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.8 使用训练好的去噪U-Net模型生成图像
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Sets the random seed to 1 so results are reproducible
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ① 设置随机种子为1，以便结果可重复
- en: ② Uses the defined generate() method to create 10 clean images
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用定义的generate()方法创建10张干净的图像
- en: ③ Plots the generated images
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 绘制生成的图像
- en: We set the random seed to 1\. As a result, if you use the trained model from
    my website, you’ll get identical results as shown in figure 15.6\. We use the
    `generate()` method defined earlier to create 10 clean images, using 50 inference
    steps. We then plot the 10 images in a 2 × 5 grid, as shown in figure 15.6.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将随机种子设置为1。因此，如果您使用我网站上的训练模型，您将得到与图15.6中显示的相同的结果。我们使用之前定义的`generate()`方法，使用50次推理步骤创建10张干净的图像。然后我们将这10张图像以2×5的网格形式绘制出来，如图15.6所示。
- en: '![](../../OEBPS/Images/CH15_F06_Liu.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH15_F06_Liu.png)'
- en: Figure 15.6 Flower images created by the trained denoising U-Net model.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.6 由训练好的去噪U-Net模型创建的花朵图像。
- en: As you can see from figure 15.6, the generated flower images look real and resemble
    those in the training dataset.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从图15.6中看到的那样，生成的花朵图像看起来很真实，类似于训练数据集中的图像。
- en: Exercise 15.1
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 练习15.1
- en: Modify code listing 15.8 and change the random seed to 2\. Keep the rest of
    the code the same. Rerun the code listing and see what the generated images look
    like.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 修改代码列表15.8并将随机种子改为2。保持其余代码不变。重新运行代码列表，看看生成的图像是什么样子。
- en: The `generate()` method also returns a list, imgs, which contains all the images
    in the 50 intermediate steps. We’ll use them to visualize the denoising process.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate()`方法还返回一个包含50个中间步骤中所有图像的列表imgs。我们将使用它们来可视化去噪过程。'
- en: Listing 15.9 Visualizing the denoising process
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.9 可视化去噪过程
- en: '[PRE15]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Keeps time steps 800, 600, 400, 200, and 0
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ① 保持时间步800、600、400、200和0
- en: ② Selects 4 sets of flowers out of 10
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从10组花朵中选择4组
- en: ③ Plots the 20 images in a 4 × 5 grid
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 以4×5的网格绘制20张图像
- en: The list, imgs, contains 10 sets of images in all 50 inference steps, t = 980,
    960, . . . , 20, 0. So there are a total of 500 images in the list. We select
    five time steps (t = 800, 600, 400, 200, and 0) for four different flowers (the
    2^(nd), 4^(th), 7^(th), and 10^(th) images in figure 15.6). We then plot the 20
    images in a 4 × 5 grid, as shown in figure 15.7.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的imgs包含所有50个推理步骤中的10组图像，t = 980, 960, ...，20, 0。因此，列表中共有500张图像。我们选择了五个时间步（t
    = 800, 600, 400, 200和0）用于四种不同的花朵（图15.6中的第2、4、7和10张图像）。然后我们将这20张图像以4×5的网格形式绘制出来，如图15.7所示。
- en: '![](../../OEBPS/Images/CH15_F07_Liu.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH15_F07_Liu.png)'
- en: Figure 15.7 How the trained denoising U-Net model gradually converts random
    noise into clean flower images. We feed random noise to the trained model to obtain
    the image at time step 980\. We then feed the noisy image at t = 980 to the model
    to obtain the image at t = 960. We repeat this process 50 inference steps until
    we obtain the image at t = 0. The first column in this figure shows the four flowers
    at t = 800; the second column shows the same four flowers at t = 600 . . . ; the
    last column shows the four flowers at t = 0 (i.e., clean flower images).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7 训练好的去噪U-Net模型如何逐步将随机噪声转换为干净的鲜花图像。我们将随机噪声输入到训练好的模型中，以获得时间步980的图像。然后我们将t
    = 980的噪声图像输入到模型中，以获得t = 960的图像。我们重复这个过程50次推理步骤，直到我们获得t = 0的图像。该图的第一列显示了t = 800时的四个花朵；第二列显示了相同的四个花朵在t
    = 600时...；最后一列显示了t = 0（即干净的鲜花图像）的四个花朵。
- en: The first column in figure 15.7 shows the four flower images at t = 800. They
    are close to random noise. The second column shows the flowers at t = 600, and
    they start to look like flowers. As we move to the right, the images become clearer
    and clearer. The rightmost column shows the four clean flower images at t = 0.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7的第一列显示了t = 800时的四个花朵图像。它们接近随机噪声。第二列显示了t = 600时的花朵，它们开始看起来像花朵。随着我们向右移动，图像变得越来越清晰。最右边的一列显示了t
    = 0时的四个干净花朵图像。
- en: Now that you understand how diffusion models work, we’ll discuss text-to-image
    generation. The image generation process of text-to-image Transformers such as
    DALL-E 2, Imagen, and Stable Diffusion is very much like the reverse diffusion
    process we discussed earlier in the chapter, except that the model takes the text
    embedding as a conditioning signal when generating an image.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了扩散模型的工作原理，我们将讨论文本到图像的生成。文本到图像的Transformers，如DALL-E 2、Imagen和Stable Diffusion的图像生成过程与我们在本章前面讨论的逆向扩散过程非常相似，只是在生成图像时，模型将文本嵌入作为条件信号。
- en: 15.5 Text-to-image Transformers
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.5 文本到图像的Transformers
- en: Text-to-image Transformers such as OpenAI’s DALL-E 2, Google’s Imagen, and Stability
    AI’s Stable Diffusion use diffusion models to generate images from textual descriptions.
    An important component of these text-to-image Transformers is a diffusion model.
    The process of text-to-image generation involves encoding the text input into
    a latent representation, which is then used as a conditioning signal for the diffusion
    model. These Transformers learn to generate lifelike images that correspond to
    the textual description by iteratively denoising a random noise vector, guided
    by the encoded text.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如OpenAI的DALL-E 2、Google的Imagen和Stability AI的Stable Diffusion这样的文本到图像Transformer使用扩散模型从文本描述生成图像。这些文本到图像Transformer的一个重要组成部分是扩散模型。文本到图像生成的过程涉及将文本输入编码成一个潜在表示，然后将其用作扩散模型的条件信号。这些Transformer通过迭代地去除随机噪声向量来学习生成与文本描述相对应的逼真图像，这个过程由编码的文本引导。
- en: The key to all these text-to-image Transformers is a model to understand content
    in different modalities. In this case, the model must understand the text descriptions
    and link them to images and vice versa.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些文本到图像的Transformer的关键在于一个能够理解不同模态内容的模型。在这种情况下，该模型必须理解文本描述并将它们与图像以及反之联系起来。
- en: In this section, we’ll use OpenAI’s CLIP model as an example. CLIP is a key
    component in DALL-E 2\. We’ll discuss how CLIP was trained to understand the connection
    between text descriptions and images. We then use a short Python program to generate
    an image from a text prompt by using OpenAI’s DALL-E 2.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将以OpenAI的CLIP模型为例。CLIP是DALL-E 2的关键组件。我们将讨论CLIP是如何被训练来理解文本描述和图像之间的联系的。然后，我们使用一个简短的Python程序，通过使用OpenAI的DALL-E
    2，从文本提示中生成图像。
- en: '15.5.1 CLIP: A multimodal Transformer'
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.1 CLIP：一个多模态Transformer
- en: In recent years, the intersection of computer vision and natural language processing
    (NLP) has witnessed significant advancements, one of which is the creation of
    the CLIP model by OpenAI. This innovative model is designed to understand and
    interpret images in the context of natural language, a capability that holds immense
    potential for various applications such as image generation and image classification.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，计算机视觉和自然语言处理（NLP）的交叉领域取得了显著进展，其中之一是OpenAI创建的CLIP模型。这个创新模型旨在理解并解释自然语言环境中的图像，这一能力在图像生成和图像分类等众多应用中具有巨大的潜力。
- en: The CLIP model is a multimodal Transformer that bridges the gap between visual
    and textual data. It is trained to understand images by associating them with
    corresponding textual descriptions. Unlike traditional models that require explicit
    labeling of images, CLIP uses a vast dataset of images and their natural language
    descriptions to learn a more generalizable representation of visual concepts.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP模型是一个多模态Transformer，它弥合了视觉数据和文本数据之间的差距。它通过将图像与相应的文本描述关联起来来训练理解图像。与需要显式标记图像的传统模型不同，CLIP使用大量图像及其自然语言描述的数据集来学习视觉概念的更一般化表示。
- en: '![](../../OEBPS/Images/CH15_F08_Liu.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH15_F08_Liu.png)'
- en: Figure 15.8 How OpenAI’s CLIP model is trained. A large-scale training dataset
    of text–image pairs is collected. The text encoder of the model compresses the
    text description into a D-value text embedding. The image encoder converts the
    corresponding image into an image embedding also with D values. During training,
    a batch of N text–image pairs are converted to N text embeddings and N image embeddings.
    CLIP uses a contrastive learning approach to maximize the similarity between paired
    embeddings (the sum of diagonal values in the figure) while minimizing the similarity
    between embeddings from nonmatching text–image pairs (the sum of off-diagonal
    values in the figure).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8 OpenAI的CLIP模型是如何训练的。收集了一个大规模的文本-图像对训练数据集。模型的文本编码器将文本描述压缩成D值文本嵌入。图像编码器将相应的图像转换成具有D值的图像嵌入。在训练过程中，一批N个文本-图像对被转换成N个文本嵌入和N个图像嵌入。CLIP使用对比学习方法来最大化配对嵌入之间的相似性（图中对角线值的总和）同时最小化来自不匹配文本-图像对的嵌入之间的相似性（图中非对角线值的总和）。
- en: The training of the CLIP model, which is illustrated in figure 15.8, begins
    with the collection of a large-scale dataset comprising images and their associated
    textual descriptions. OpenAI utilizes a diverse set of sources, including publicly
    available datasets and web-crawled data, to ensure a wide variety of visual and
    textual content. The dataset is then preprocessed to standardize the images so
    they all have the same shape and to tokenize the text, preparing them for input
    into the model.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP模型的训练，如图15.8所示，始于收集包含图像及其相关文本描述的大规模数据集。OpenAI利用了多样化的来源，包括公开可用的数据集和网页爬取的数据，以确保有广泛的视觉和文本内容。然后对数据集进行预处理，以标准化图像，使它们都具有相同的形状，并对文本进行分词，为输入模型做准备。
- en: CLIP employs a dual-encoder architecture, consisting of an image encoder and
    a text encoder. The image encoder processes the input images while the text encoder
    processes the corresponding textual descriptions. These encoders project the images
    and text into a shared embedding space where they can be compared and aligned.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP采用双编码器架构，包括图像编码器和文本编码器。图像编码器处理输入图像，而文本编码器处理相应的文本描述。这些编码器将图像和文本投影到一个共享的嵌入空间中，在那里它们可以被比较和对齐。
- en: The core of CLIP’s training lies in its contrastive learning approach. For each
    batch of N image–text pairs in the dataset, the model aims to maximize the similarity
    between paired embeddings (measured by the sum of diagonal values in figure 15.8)
    while minimizing the similarity between embeddings from nonmatching text-image
    pairs (the sum of off-diagonal values). Figure 15.9 is a diagram of how text-to-image
    Transformers such as DALL-E 2 generate realistic images based on text prompts.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP训练的核心在于其对比学习方法。对于数据集中每个包含N个图像-文本对的批次，模型旨在最大化配对嵌入之间的相似性（如图15.8中对角线值的总和），同时最小化来自不匹配的文本-图像对嵌入之间的相似性（非对角线值的总和）。图15.9展示了如何基于文本提示生成逼真图像的文本到图像Transformer，如DALL-E
    2。
- en: '![](../../OEBPS/Images/CH15_F09_Liu.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH15_F09_Liu.png)'
- en: Figure 15.9 How text-to-image Transformers such as DALL-E 2 create images based
    on text prompts. The text encoder in the trained text-to-image Transformer first
    converts the text description in the prompt into text embedding. The text embedding
    is fed to the CLIP model to obtain a prior vector that represents the image in
    the latent space. The text embedding and the prior are concatenated into a conditioning
    vector. To generate an image, the U-Net denoiser first takes a random noise vector
    as input to generate a noisy image using the conditioning vector. It then takes
    the noisy image and the conditioning vector as input and generates another image,
    which is less noisy. The process is repeated for many iterations until the final
    output, a clean image, is generated.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9展示了如何基于文本提示创建图像的文本到图像Transformer，如DALL-E 2。在训练好的文本到图像Transformer中，文本编码器首先将提示中的文本描述转换为文本嵌入。文本嵌入被输入到CLIP模型中，以获得一个表示潜在空间中图像的先验向量。文本嵌入和先验被连接成一个条件向量。为了生成图像，U-Net降噪器首先以一个随机噪声向量为输入，使用条件向量生成一个带噪声的图像。然后它以带噪声的图像和条件向量为输入，生成另一个图像，该图像噪声更少。这个过程重复多次，直到最终输出一个干净的图像。
- en: The image generation process of text-to-image Transformers is similar to the
    reverse diffusion process we discussed earlier in the chapter. Let’s take DALL-E
    2, for example, which was proposed by OpenAI researchers in 2022.^([4](#footnote-000))
    The text encoder in the model first converts the text description in the prompt
    into a text embedding. The text embedding is fed to the CLIP model to obtain a
    prior vector that represents the image in the latent space. The text embedding
    and the prior are concatenated into a conditioning vector. In the first iteration,
    we feed a random noise vector to the U-Net denoiser in the model and ask it to
    generate a noisy image based on the conditioning vector. In the second iteration,
    we feed the noisy image from the previous iteration to the U-Net denoiser and
    ask it to generate another noisy image based on the conditioning vector. We repeat
    this process for many iterations, and the final output is a clean image.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像Transformer的图像生成过程与我们本章 earlier 讨论的逆向扩散过程类似。以DALL-E 2为例，它是由OpenAI研究人员在2022年提出的.^([4](#footnote-000))
    模型中的文本编码器首先将提示中的文本描述转换为文本嵌入。文本嵌入被输入到CLIP模型中，以获得一个表示潜在空间中图像的先验向量。文本嵌入和先验被连接成一个条件向量。在第一次迭代中，我们将一个随机噪声向量输入到模型中的U-Net降噪器，并要求它根据条件向量生成一个噪声图像。在第二次迭代中，我们将前一次迭代中的噪声图像输入到U-Net降噪器，并要求它根据条件向量生成另一个噪声图像。我们重复这个过程多次，最终输出是一个干净的图像。
- en: 15.5.2 Text-to-image generation with DALL-E 2
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.2 使用DALL-E 2进行文本到图像生成
- en: Now that you understand how text-to-image Transformers work, let’s write a Python
    program to interact with DALL-E 2 to create an image based on a text prompt.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了文本到图像Transformer的工作原理，让我们编写一个Python程序来与DALL-E 2交互，根据文本提示创建图像。
- en: First, you need to apply for an OpenAI API key. OpenAI offers various pricing
    tiers that vary based on the number of tokens processed and the type of models
    used. Go to [https://chat.openai.com/auth/login](https://chat.openai.com/auth/login)
    and click on the Sign up button to create an account. After that, log in to your
    account, and go to [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)
    to view your API key. Save it in a secure place for later use. We can generate
    an image by using OpenAI’s DALL-E 2.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要申请一个OpenAI API密钥。OpenAI提供各种定价层，这些定价层根据处理的令牌数量和使用的模型类型而有所不同。访问[https://chat.openai.com/auth/login](https://chat.openai.com/auth/login)并点击“注册”按钮创建账户。之后，登录你的账户，并访问[https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)以查看你的API密钥。将其保存在安全的地方以备后用。我们可以使用OpenAI的DALL-E
    2生成图像。
- en: Listing 15.10 Image generation with DALL-E 2
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.10 使用DALL-E 2生成图像
- en: '[PRE16]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① Makes sure you provide your actual OpenAI API key here, in quotes
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ① 确保你在这里提供实际的OpenAI API密钥，并用引号括起来
- en: ② Instantiates the OpenAI() class to create an agent
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化OpenAI()类以创建代理
- en: ③ Uses the images.generate() method to generate image based on the text prompt
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用images.generate()方法根据文本提示生成图像
- en: ④ Prints out the image URL
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 打印出图像URL
- en: You should place the OpenAI API key you obtained earlier in listing 15.10\.
    We create an agent by instantiating the `OpenAI()` class. To generate an image,
    we need to specify the model, a text prompt, and the size of the image. We have
    used “an astronaut in a space suit riding a unicorn” as the prompt, and the code
    listing provides a URL for us to visualize and download the image. The URL expires
    in an hour, and the resulting image is shown in figure 15.10\.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该将之前获得的OpenAI API密钥放置在列表15.10中。我们通过实例化`OpenAI()`类来创建一个代理。要生成图像，我们需要指定模型、文本提示和图像的大小。我们使用了“一个穿着太空服的宇航员骑独角兽”作为提示，代码列表提供了我们可视化和下载图像的URL。该URL在1小时内过期，生成的图像如图15.10所示。
- en: '![](../../OEBPS/Images/CH15_F10_Liu.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH15_F10_Liu.png)'
- en: Figure 15.10 An image generated by DALL-E 2 with the text prompt “an astronaut
    in a space suit riding a unicorn”
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.10 DALL-E 2根据文本提示“一个穿着太空服的宇航员骑独角兽”生成的图像
- en: Run listing 15.10 yourself and see what image DALLE-2 generates for you. Note
    that your result will be different since the output from DALLE-2 (and all LLMs)
    is stochastic rather than deterministic.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 运行列表15.10并查看DALLE-2为你生成的图像。请注意，你的结果将不同，因为DALLE-2（以及所有LLM）的输出是随机的，而不是确定的。
- en: Exercise 15.2
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 练习15.2
- en: Apply for an OpenAI API key. Then modify code listing 15.10 to generate an image
    using the text prompt “a cat in a suit working on a computer.”
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 申请OpenAI API密钥。然后修改代码列表15.10以使用文本提示“穿着西装的猫在电脑上工作”生成图像。
- en: In this chapter, you learned the inner workings of diffusion-based models and
    their significance in text-to-image Transformers, such as OpenAI’s CLIP model.
    You also discovered how to obtain your OpenAI API key and used a brief Python
    script to generate images from text descriptions with DALL-E 2, which incorporates
    CLIP.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了基于扩散的模型的工作原理及其在文本到图像Transformer中的重要性，例如OpenAI的CLIP模型。你还发现了如何获取你的OpenAI
    API密钥，并使用一个简短的Python脚本来生成由DALL-E 2创建的图像，该模型集成了CLIP。
- en: In the next chapter, you will continue to use the OpenAI API key obtained earlier
    to use pretrained LLMs for generating diverse content, including text, audio,
    and images. Additionally, you will integrate the LangChain Python library with
    other APIs, enabling you to create a know-it-all personal assistant.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将继续使用之前获得的OpenAI API密钥，使用预训练的LLM生成各种内容，包括文本、音频和图像。此外，你将集成LangChain Python库与其他API，使你能够创建一个无所不知的个人助理。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In forward diffusion, we gradually add small amounts of random noise to clean
    images until they transform into pure noise. Conversely, in reverse diffusion,
    we begin with random noise and employ a denoising model to progressively eliminate
    noise from the images, transforming the noise back into a clean image.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在正向扩散中，我们逐渐向干净图像添加少量随机噪声，直到它们转变为纯噪声。相反，在反向扩散中，我们从随机噪声开始，并使用降噪模型逐步从图像中消除噪声，将噪声转换回干净图像。
- en: The U-Net architecture, originally designed for biomedical image segmentation,
    has a symmetric shape with a contracting encoder path and an expansive decoder
    path, connected by a bottleneck layer. In denoising, U-Nets are adapted to remove
    noise while preserving details. Skip connections link encoder and decoder feature
    maps of the same spatial dimensions, helping to preserve spatial information like
    edges and textures that may be lost during downsampling in the encoding process.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: U-Net架构最初是为生物医学图像分割设计的，它具有对称的形状，包括一个收缩的编码器路径和一个扩张的解码器路径，两者通过一个瓶颈层相连。在降噪过程中，U-Nets被调整为去除噪声同时保留细节。跳跃连接将相同空间维度的编码器和解码器特征图连接起来，有助于保留在编码过程中可能丢失的边缘和纹理等空间信息。
- en: Incorporating an attention mechanism into a denoising U-Net model enables it
    to concentrate on important features and disregard irrelevant ones. By treating
    image pixels as a sequence, the attention mechanism learns pixel dependencies,
    similar to how it learns token dependencies in NLP. This enhances the model’s
    ability to identify relevant features effectively.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将注意力机制引入降噪U-Net模型，使其能够专注于重要特征并忽略不相关的特征。通过将图像像素视为一个序列，注意力机制学习像素依赖关系，类似于它在自然语言处理中学习标记依赖关系的方式。这增强了模型有效识别相关特征的能力。
- en: Text-to-image Transformers like OpenAI’s DALL-E 2, Google’s Imagen, and Stability
    AI’s Stable Diffusion use diffusion models to create images from textual descriptions.
    They encode the text into a latent representation that conditions the diffusion
    model, which then iteratively denoises a random noise vector, guided by the encoded
    text, to generate lifelike images matching the textual description.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于OpenAI的DALL-E 2、Google的Imagen和Stability AI的Stable Diffusion这样的文本到图像Transformer使用扩散模型从文本描述中创建图像。它们将文本编码为条件扩散模型的潜在表示，然后迭代地去除由编码文本引导的随机噪声向量，以生成符合文本描述的逼真图像。
- en: '* * *'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([1](#footnote-003-backlink))  Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan,
    and Surya Ganguli, 2015, “Deep Unsupervised Learning Using Nonequilibrium Thermodynamics.”
    International Conference on Machine Learning, [http://arxiv.org/abs/1503.03585](http://arxiv.org/abs/1503.03585).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-003-backlink))  Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan,
    and Surya Ganguli, 2015, “Deep Unsupervised Learning Using Nonequilibrium Thermodynamics.”
    International Conference on Machine Learning, [http://arxiv.org/abs/1503.03585](http://arxiv.org/abs/1503.03585).
- en: ^([2](#footnote-002-backlink))  Sohl-Dickstein et al., 2015, “Deep Unsupervised
    Learning Using Nonequilibrium Thermodynamics,” h[ttps://arxiv.org/abs/1503.03585](https://arxiv.org/abs/1503.03585).
    Yang Song and Stefano Ermon, 2019, “Generative Modeling by Estimating Gradients
    of the Data Distribution.” [https://arxiv.org/abs/1907.05600](https://arxiv.org/abs/1907.05600).
    Jonathan Ho, Ajay Jain, and Pieter Abbeel, 2020, “Denoising Diffusion Probabilistic
    Models,” [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](#footnote-002-backlink))  Sohl-Dickstein et al., 2015, “Deep Unsupervised
    Learning Using Nonequilibrium Thermodynamics,” h[ttps://arxiv.org/abs/1503.03585](https://arxiv.org/abs/1503.03585).
    杨松和斯蒂法诺·埃尔蒙，2019, “通过估计数据分布的梯度进行生成建模。” [https://arxiv.org/abs/1907.05600](https://arxiv.org/abs/1907.05600).
    乔纳森·霍，阿贾伊·贾因和皮埃特·阿贝尔，2020, “去噪扩散概率模型，” [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239).
- en: ^([3](#footnote-001-backlink))  Ilya Loshchilov and Frank Hutter, 2017, “Decoupled
    Weight Decay Regularization.” [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](#footnote-001-backlink))  伊利亚·洛希奇洛夫和弗兰克·胡特，2017, “解耦权重衰减正则化。” [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101).
- en: ^([4](#footnote-000-backlink))  Aditya Rames, Prafulla Dhariwal, Alex Nichol,
    Casey Chu, and Mark Chen, 2022, “Hierarchical Text-Conditional Image Generation
    with CLIP Latents.” [https://arxiv.org/abs/2204.06125](https://arxiv.org/abs/2204.06125).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](#footnote-000-backlink))  阿迪亚·拉梅斯，普拉夫拉·达里瓦尔，亚历克斯·尼科尔，凯西·丘和马克·陈，2022, “使用CLIP潜力的分层文本条件图像生成。”
    [https://arxiv.org/abs/2204.06125](https://arxiv.org/abs/2204.06125).
