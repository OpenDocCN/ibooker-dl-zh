- en: 2 Working with Text Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 使用文本数据
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Preparing text for large language model training
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为大型语言模型训练准备文本
- en: Splitting text into word and subword tokens
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本分割成单词和子单词标记
- en: Byte pair encoding as a more advanced way of tokenizing text
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字节对编码作为一种更高级的文本标记化方式
- en: Sampling training examples with a sliding window approach
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用滑动窗口方法对训练样本进行抽样
- en: Converting tokens into vectors that feed into a large language model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标记转换为输入大型语言模型的向量
- en: In the previous chapter, we delved into the general structure of large language
    models (LLMs) and learned that they are pretrained on vast amounts of text. Specifically,
    our focus was on decoder-only LLMs based on the transformer architecture, which
    underlies ChatGPT and other popular GPT-like LLMs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们深入探讨了大型语言模型（LLMs）的一般结构，并了解到它们在大量文本上进行了预训练。具体来说，我们关注的是基于变压器架构的解码器专用LLMs，这是ChatGPT和其他流行的类GPT
    LLMs的基础。
- en: During the pretraining stage, LLMs process text one word at a time. Training
    LLMs with millions to billions of parameters using a next-word prediction task
    yields models with impressive capabilities. These models can then be further finetuned
    to follow general instructions or perform specific target tasks. But before we
    can implement and train LLMs in the upcoming chapters, we need to prepare the
    training dataset, which is the focus of this chapter, as illustrated in figure
    2.1
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练阶段，LLM逐个单词处理文本。利用亿万到数十亿参数的LLM进行下一个词预测任务的训练，可以产生具有令人印象深刻能力的模型。然后可以进一步微调这些模型以遵循一般指示或执行特定目标任务。但是，在接下来的章节中实施和训练LLM之前，我们需要准备训练数据集，这是本章的重点，如图2.1所示
- en: Figure 2.1 A mental model of the three main stages of coding an LLM, pretraining
    the LLM on a general text dataset, and finetuning it on a labeled dataset. This
    chapter will explain and code the data preparation and sampling pipeline that
    provides the LLM with the text data for pretraining.
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 LLM编码的三个主要阶段的心理模型，LLM在一般文本数据集上进行预训练，然后在有标签的数据集上进行微调。本章将解释并编写提供LLM预训练文本数据的数据准备和抽样管道。
- en: '![](images/ch-02__image002.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image002.png)'
- en: In this chapter, you'll learn how to prepare input text for training LLMs. This
    involves splitting text into individual word and subword tokens, which can then
    be encoded into vector representations for the LLM. You'll also learn about advanced
    tokenization schemes like byte pair encoding, which is utilized in popular LLMs
    like GPT. Lastly, we'll implement a sampling and data loading strategy to produce
    the input-output pairs necessary for training LLMs in subsequent chapters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何准备输入文本以进行LLM训练。这涉及将文本拆分为单独的单词和子单词标记，然后将其编码为LLM的向量表示。您还将学习有关高级标记方案，如字节对编码，这在像GPT这样的流行LLM中被使用。最后，我们将实现一种抽样和数据加载策略，以生成后续章节中训练LLM所需的输入-输出对。
- en: 2.1 Understanding word embeddings
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 理解词嵌入
- en: Deep neural network models, including LLMs, cannot process raw text directly.
    Since text is categorical, it isn't compatible with the mathematical operations
    used to implement and train neural networks. Therefore, we need a way to represent
    words as continuous-valued vectors. (Readers unfamiliar with vectors and tensors
    in a computational context can learn more in Appendix A, section A2.2 Understanding
    tensors.)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络模型，包括LLM，无法直接处理原始文本。由于文本是分类的，所以它与用于实现和训练神经网络的数学运算不兼容。因此，我们需要一种将单词表示为连续值向量的方式。（不熟悉计算上下文中向量和张量的读者可以在附录A，A2.2理解张量中了解更多。）
- en: The concept of converting data into a vector format is often referred to as
    *embedding*. Using a specific neural network layer or another pretrained neural
    network model, we can embed different data types, for example, video, audio, and
    text, as illustrated in figure 2.2.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据转换为向量格式的概念通常被称为*嵌入*。使用特定的神经网络层或其他预训练的神经网络模型，我们可以嵌入不同的数据类型，例如视频、音频和文本，如图2.2所示。
- en: Figure 2.2 Deep learning models cannot process data formats like video, audio,
    and text in their raw form. Thus, we use an embedding model to transform this
    raw data into a dense vector representation that deep learning architectures can
    easily understand and process. Specifically, this figure illustrates the process
    of converting raw data into a three-dimensional numerical vector. It's important
    to note that different data formats require distinct embedding models. For example,
    an embedding model designed for text would not be suitable for embedding audio
    or video data.
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 深度学习模型无法直接处理视频、音频和文本等原始格式的数据。因此，我们使用嵌入模型将这些原始数据转换为深度学习架构可以轻松理解和处理的稠密向量表示。具体来说，这张图说明了将原始数据转换为三维数值向量的过程。需要注意的是，不同的数据格式需要不同的嵌入模型。例如，专为文本设计的嵌入模型不适用于嵌入音频或视频数据。
- en: '![](images/ch-02__image004.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image004.png)'
- en: At its core, an embedding is a mapping from discrete objects, such as words,
    images, or even entire documents, to points in a continuous vector space -- the
    primary purpose of embeddings is to convert non-numeric data into a format that
    neural networks can process.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，嵌入是从离散对象（如单词、图像，甚至整个文档）到连续向量空间中的点的映射——嵌入的主要目的是将非数值数据转换为神经网络可以处理的格式。
- en: While word embeddings are the most common form of text embedding, there are
    also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph
    embeddings are popular choices for *retrieval-augmented generation.* Retrieval-augmented
    generation combines generation (like producing text) with retrieval (like searching
    an external knowledge base) to pull relevant information when generating text,
    which is a technique that is beyond the scope of this book. Since our goal is
    to train GPT-like LLMs, which learn to generate text one word at a time, this
    chapter focuses on word embeddings.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然单词嵌入是文本嵌入的最常见形式，但也有针对句子、段落或整个文档的嵌入。句子或段落嵌入是*检索增强生成*的流行选择。检索增强生成结合了生成（如生成文本）和检索（如搜索外部知识库）以在生成文本时提取相关信息的技术，这是本书讨论范围之外的技术。由于我们的目标是训练类似GPT的LLMs，这些模型学习逐词生成文本，因此本章重点介绍了单词嵌入。
- en: There are several algorithms and frameworks that have been developed to generate
    word embeddings. One of the earlier and most popular examples is the *Word2Vec*
    approach. Word2Vec trained neural network architecture to generate word embeddings
    by predicting the context of a word given the target word or vice versa. The main
    idea behind Word2Vec is that words that appear in similar contexts tend to have
    similar meanings. Consequently, when projected into 2-dimensional word embeddings
    for visualization purposes, it can be seen that similar terms cluster together,
    as shown in figure 2.3.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种算法和框架已被开发用于生成单词嵌入。其中一个较早和最流行的示例是*Word2Vec*方法。Word2Vec训练神经网络架构以通过预测给定目标词或反之亦然的单词的上下文来生成单词嵌入。Word2Vec背后的主要思想是在相似上下文中出现的单词往往具有相似的含义。因此，当投影到二维单词嵌入进行可视化时，可以看到相似术语聚集在一起，如图2.3所示。
- en: Figure 2.3 If word embeddings are two-dimensional, we can plot them in a two-dimensional
    scatterplot for visualization purposes as shown here. When using word embedding
    techniques, such as Word2Vec, words corresponding to similar concepts often appear
    close to each other in the embedding space. For instance, different types of birds
    appear closer to each other in the embedding space compared to countries and cities.
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3 如果单词嵌入是二维的，我们可以在二维散点图中绘制它们进行可视化，如此处所示。使用单词嵌入技术（例如Word2Vec），与相似概念对应的单词通常在嵌入空间中彼此靠近。例如，不同类型的鸟类在嵌入空间中彼此比国家和城市更接近。
- en: '![](images/ch-02__image006.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image006.png)'
- en: Word embeddings can have varying dimensions, from one to thousands. As shown
    in figure 2.3, we can choose two-dimensional word embeddings for visualization
    purposes. A higher dimensionality might capture more nuanced relationships but
    at the cost of computational efficiency.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入的维度可以有不同的范围，从一维到数千维不等。如图2.3所示，我们可以选择二维单词嵌入进行可视化。更高的维度可能捕捉到更加微妙的关系，但会牺牲计算效率。
- en: While we can use pretrained models such as Word2Vec to generate embeddings for
    machine learning models, LLMs commonly produce their own embeddings that are part
    of the input layer and are updated during training. The advantage of optimizing
    the embeddings as part of the LLM training instead of using Word2Vec is that the
    embeddings are optimized to the specific task and data at hand. We will implement
    such embedding layers later in this chapter. Furthermore, LLMs can also create
    contextualized output embeddings, as we discuss in chapter 3.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以使用诸如Word2Vec之类的预训练模型为机器学习模型生成嵌入，但LLMs通常产生自己的嵌入，这些嵌入是输入层的一部分，并在训练过程中更新。优化嵌入作为LLM训练的一部分的优势，而不是使用Word2Vec的优势在于，嵌入被优化为特定的任务和手头的数据。我们将在本章后面实现这样的嵌入层。此外，LLMs还可以创建上下文化的输出嵌入，我们将在第3章中讨论。
- en: Unfortunately, high-dimensional embeddings present a challenge for visualization
    because our sensory perception and common graphical representations are inherently
    limited to three dimensions or fewer, which is why figure 2.3 showed two-dimensional
    embeddings in a two-dimensional scatterplot. However, when working with LLMs,
    we typically use embeddings with a much higher dimensionality than shown in figure
    2.3\. For both GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality
    of the model's hidden states) varies based on the specific model variant and size.
    It is a trade-off between performance and efficiency. The smallest GPT-2 (117M
    parameters) and GPT-3 (125 M parameters) models use an embedding size of 768 dimensions
    to provide concrete examples. The largest GPT-3 model (175B parameters) uses an
    embedding size of 12,288 dimensions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，高维度嵌入给可视化提出了挑战，因为我们的感知和常见的图形表示固有地受限于三个或更少维度，这就是为什么图2.3展示了在二维散点图中的二维嵌入。然而，当使用LLMs时，我们通常使用比图2.3中所示的更高维度的嵌入。对于GPT-2和GPT-3，嵌入大小（通常称为模型隐藏状态的维度）根据特定模型变体和大小而变化。这是性能和效率之间的权衡。最小的GPT-2（117M参数）和GPT-3（125M参数）模型使用768维度的嵌入大小来提供具体的例子。最大的GPT-3模型（175B参数）使用12288维的嵌入大小。
- en: The upcoming sections in this chapter will walk through the required steps for
    preparing the embeddings used by an LLM, which include splitting text into words,
    converting words into tokens, and turning tokens into embedding vectors.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的后续部分将介绍准备LLM使用的嵌入所需的步骤，包括将文本分割为单词，将单词转换为标记，并将标记转换为嵌入向量。
- en: 2.2 Tokenizing text
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 文本分词
- en: This section covers how we split input text into individual tokens, a required
    preprocessing step for creating embeddings for an LLM. These tokens are either
    individual words or special characters, including punctuation characters, as shown
    in figure 2.4.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何将输入文本分割为单个标记，这是为了创建LLM嵌入所必需的预处理步骤。这些标记可以是单独的单词或特殊字符，包括标点符号字符，如图2.4所示。
- en: Figure 2.4 A view of the text processing steps covered in this section in the
    context of an LLM. Here, we split an input text into individual tokens, which
    are either words or special characters, such as punctuation characters. In upcoming
    sections, we will convert the text into token IDs and create token embeddings.
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4 在LLM上下文中查看本节涵盖的文本处理步骤。在这里，我们将输入文本分割为单个标记，这些标记可以是单词或特殊字符，如标点符号字符。在即将到来的部分中，我们将把文本转换为标记ID并创建标记嵌入。
- en: '![](images/ch-02__image008.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image008.png)'
- en: 'The text we will tokenize for LLM training is a short story by Edith Wharton
    called *The Verdict*, which has been released into the public domain and is thus
    permitted to be used for LLM training tasks. The text is available on Wikisource
    at [https://en.wikisource.org/wiki/The_Verdict](wiki.html), and you can copy and
    paste it into a text file, which I copied into a text file "`the-verdict.txt"`
    to load using Python''s standard file reading utilities:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于LLM训练的文本是Edith Wharton的短篇小说《**The Verdict**》，该小说已进入公有领域，因此可以用于LLM训练任务。文本可在Wikisource上获得，网址为[https://en.wikisource.org/wiki/The_Verdict](wiki.html)，您可以将其复制粘贴到文本文件中，我将其复制到一个名为"`the-verdict.txt`"的文本文件中，以便使用Python的标准文件读取实用程序加载：
- en: Listing 2.1 Reading in a short story as text sample into Python
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.1 将短篇小说作为文本示例读入Python
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Alternatively, you can find this "`the-verdict.txt"` file in this book's GitHub
    repository at [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/01_main-chapter-code](ch02.html).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以在本书的GitHub存储库中找到此"`the-verdict.txt`"文件，网址为[https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/01_main-chapter-code](ch02.html)。
- en: 'The print command prints the total number of characters followed by the first
    100 characters of this file for illustration purposes:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 打印命令打印出字符的总数，然后是文件的前100个字符，用于说明目的：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our goal is to tokenize this 20,479-character short story into individual words
    and special characters that we can then turn into embeddings for LLM training
    in the upcoming chapters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将这篇短篇小说的20,479个字符标记成单词和特殊字符，然后将其转换为LLM训练的嵌入。
- en: Text sample sizes
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 文本样本大小
- en: Note that it's common to process millions of articles and hundreds of thousands
    of books -- many gigabytes of text -- when working with LLMs. However, for educational
    purposes, it's sufficient to work with smaller text samples like a single book
    to illustrate the main ideas behind the text processing steps and to make it possible
    to run it in reasonable time on consumer hardware.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在处理LLM时，处理数百万篇文章和数十万本书——许多吉字节的文本——是很常见的。但是，出于教育目的，使用小型文本样本，如一本书，就足以说明文本处理步骤背后的主要思想，并且可以在消费类硬件上合理的时间内运行。
- en: How can we best split this text to obtain a list of tokens? For this, we go
    on a small excursion and use Python's regular expression library `re` for illustration
    purposes. (Note that you don't have to learn or memorize any regular expression
    syntax since we will transition to a pre-built tokenizer later in this chapter.)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何最好地分割这段文本以获得标记列表? 为此，我们进行了小小的探索，并使用Python的正则表达式库`re`进行说明。 （请注意，您无需学习或记忆任何正则表达式语法，因为我们将在本章后面过渡到预构建的标记器。）
- en: 'Using some simple example text, we can use the `re.split` command with the
    following syntax to split a text on whitespace characters:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一些简单的示例文本，我们可以使用`re.split`命令及以下语法来在空格字符上拆分文本：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result is a list of individual words, whitespaces, and punctuation characters:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '结果是一系列单词、空格和标点字符:'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the simple tokenization scheme above mostly works for separating the
    example text into individual words, however, some words are still connected to
    punctuation characters that we want to have as separate list entries.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述简单分词方案通常可将示例文本分隔成单词，但是有些单词仍然与我们希望作为单独列表项的标点字符连接在一起。
- en: 'Let''s modify the regular expression splits on whitespaces (`\s`) and commas,
    and periods (`[,.]`):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改在空格（`\s`）和逗号、句号（`[,.]`）上的正则表达式分割：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can see that the words and punctuation characters are now separate list
    entries just as we wanted:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以看到单词和标点字符现在是作为我们想要的分开的列表条目:'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A small remaining issue is that the list still includes whitespace characters.
    Optionally, we can remove these redundant characters safely remove as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小问题是列表仍然包括空白字符。可选地，我们可以安全地按如下方式删除这些多余的字符：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The resulting whitespace-free output looks like as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 去除空格字符后的输出如下：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Removing whitespaces or not
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 是否去除空白
- en: When developing a simple tokenizer, whether we should encode whitespaces as
    separate characters or just remove them depends on our application and its requirements.
    Removing whitespaces reduces the memory and computing requirements. However, keeping
    whitespaces can be useful if we train models that are sensitive to the exact structure
    of the text (for example, Python code, which is sensitive to indentation and spacing).
    Here, we remove whitespaces for simplicity and brevity of the tokenized outputs.
    Later, we will switch to a tokenization scheme that includes whitespaces.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发简单的标记器时，是否将空白字符编码为单独的字符或仅将其删除取决于我们的应用程序和其要求。去除空格减少了内存和计算需求。但是，如果我们训练的模型对文本的精确结构敏感（例如，对缩进和间距敏感的Python代码），保留空格可能会有用。在这里，为了简化标记化输出的简洁性，我们移除空白。稍后，我们将转换为包括空格的标记方案。
- en: 'The tokenization scheme we devised above works well on the simple sample text.
    Let''s modify it a bit further so that it can also handle other types of punctuation,
    such as question marks, quotation marks, and the double-dashes we have seen earlier
    in the first 100 characters of Edith Wharton''s short story, along with additional
    special characters:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面设计的标记方案在简单的示例文本上运行良好。让我们进一步修改它，使其还可以处理其他类型的标点符号，例如问号，引号以及我们在Edith Wharton的短篇小说的前100个字符中先前看到的双破折号，以及其他额外的特殊字符。
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The resulting output is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出如下：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we can see based on the results summarized in figure 2.5, our tokenization
    scheme can now handle the various special characters in the text successfully.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 根据总结在图2.5中的结果，我们的标记方案现在可以成功处理文本中的各种特殊字符。
- en: Figure 2.5 The tokenization scheme we implemented so far splits text into individual
    words and punctuation characters. In the specific example shown in this figure,
    the sample text gets split into 10 individual tokens.
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 我们目前实施的标记化方案将文本分割为单个单词和标点字符。在本图所示的特定示例中，样本文本被分割成10个单独的标记。
- en: '![](images/ch-02__image010.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image010.png)'
- en: 'Now that we got a basic tokenizer working, let''s apply it to Edith Wharton''s
    entire short story:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个基本的标记器工作，让我们将其应用到爱迪丝·沃顿的整个短篇小说中：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The above print statement outputs `4649`, which is the number of tokens in this
    text (without whitespaces).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的打印语句输出了`4649`，这是这段文本（不包括空格）中的标记数。
- en: 'Let''s print the first 30 tokens for a quick visual check:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印前30个标记进行快速的视觉检查：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The resulting output shows that our tokenizer appears to be handling the text
    well since all words and special characters are neatly separated:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出显示，我们的标记器似乎很好地处理了文本，因为所有单词和特殊字符都被很好地分开了：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 2.3 Converting tokens into token IDs
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 将标记转换为标记ID
- en: In the previous section, we tokenized a short story by Edith Wharton into individual
    tokens. In this section, we will convert these tokens from a Python string to
    an integer representation to produce the so-called token IDs. This conversion
    is an intermediate step before converting the token IDs into embedding vectors.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们将爱迪丝·沃顿的短篇小说标记化为单个标记。在本节中，我们将这些标记从Python字符串转换为整数表示，以生成所谓的标记ID。这种转换是将标记ID转换为嵌入向量之前的中间步骤。
- en: To map the previously generated tokens into token IDs, we have to build a so-called
    vocabulary first. This vocabulary defines how we map each unique word and special
    character to a unique integer, as shown in figure 2.6.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要将之前生成的标记映射到标记ID中，我们必须首先构建一个所谓的词汇表。这个词汇表定义了我们如何将每个唯一的单词和特殊字符映射到一个唯一的整数，就像图2.6中所示的那样。
- en: Figure 2.6 We build a vocabulary by tokenizing the entire text in a training
    dataset into individual tokens. These individual tokens are then sorted alphabetically,
    and unique tokens are removed. The unique tokens are then aggregated into a vocabulary
    that defines a mapping from each unique token to a unique integer value. The depicted
    vocabulary is purposefully small for illustration purposes and contains no punctuation
    or special characters for simplicity.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 我们通过对训练数据集中的整个文本进行标记化来构建词汇表，将这些单独的标记按字母顺序排序，并移除唯一的标记。然后将这些唯一标记聚合成一个词汇表，从而定义了从每个唯一标记到唯一整数值的映射。为了说明的目的，所示的词汇表故意较小，并且不包含标点符号或特殊字符。
- en: '![](images/ch-02__image012.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image012.png)'
- en: 'In the previous section, we tokenized Edith Wharton''s short story and assigned
    it to a Python variable called `preprocessed`. Let''s now create a list of all
    unique tokens and sort them alphabetically to determine the vocabulary size:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们标记化了爱迪丝·沃顿的短篇小说，并将其分配给了一个名为`preprocessed`的Python变量。现在让我们创建一个包含所有唯一标记并按字母顺序排列的列表，以确定词汇表的大小：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After determining that the vocabulary size is 1,159 via the above code, we
    create the vocabulary and print its first 50 entries for illustration purposes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上面的代码确定词汇表的大小为1,159后，我们创建词汇表，并打印其前50个条目以作说明：
- en: Listing 2.2 Creating a vocabulary
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.2 创建词汇表
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As we can see, based on the output above, the dictionary contains individual
    tokens associated with unique integer labels. Our next goal is to apply this vocabulary
    to convert new text into token IDs, as illustrated in figure 2.7.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如上面的输出所示，字典包含与唯一整数标签相关联的单独标记。我们的下一个目标是将这个词汇表应用到新文本中，以将其转换为标记ID，就像图2.7中所示的那样。
- en: Figure 2.7 Starting with a new text sample, we tokenize the text and use the
    vocabulary to convert the text tokens into token IDs. The vocabulary is built
    from the entire training set and can be applied to the training set itself and
    any new text samples. The depicted vocabulary contains no punctuation or special
    characters for simplicity.
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7 从新的文本样本开始，我们对文本进行标记化，并使用词汇表将文本标记转换为标记ID。词汇表是从整个训练集构建的，并且可以应用于训练集本身以及任何新的文本样本。为了简单起见，所示的词汇表不包含标点符号或特殊字符。
- en: '![](images/ch-02__image014.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image014.png)'
- en: Later in this book, when we want to convert the outputs of an LLM from numbers
    back into text, we also need a way to turn token IDs into text. For this, we can
    create an inverse version of the vocabulary that maps token IDs back to corresponding
    text tokens.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后面，当我们想要将LLM的输出从数字转换回文本时，我们还需要一种将标记ID转换成文本的方法。为此，我们可以创建词汇表的反向版本，将标记ID映射回相应的文本标记。
- en: Let's implement a complete tokenizer class in Python with an `encode` method
    that splits text into tokens and carries out the string-to-integer mapping to
    produce token IDs via the vocabulary. In addition, we implement a `decode` method
    that carries out the reverse integer-to-string mapping to convert the token IDs
    back into text.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Python中实现一个完整的标记器类，它具有一个`encode`方法，将文本分割成标记，并通过词汇表进行字符串到整数的映射，以产生标记ID。另外，我们实现了一个`decode`方法，进行反向整数到字符串的映射，将标记ID转回文本。
- en: 'The code for this tokenizer implementation is as in listing 2.3:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标记器实现的代码如下所示，如列表2.3所示：
- en: Listing 2.3 Implementing a simple text tokenizer
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.3 实现一个简单的文本标记器
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Using the `SimpleTokenizerV1` Python class above, we can now instantiate new
    tokenizer objects via an existing vocabulary, which we can then use to encode
    and decode text, as illustrated in figure 2.8.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述的`SimpleTokenizerV1` Python类，我们现在可以通过现有词汇表实例化新的标记对象，然后可以用于编码和解码文本，如图2.8所示。
- en: 'Figure 2.8 Tokenizer implementations share two common methods: an encode method
    and a decode method. The encode method takes in the sample text, splits it into
    individual tokens, and converts the tokens into token IDs via the vocabulary.
    The decode method takes in token IDs, converts them back into text tokens, and
    concatenates the text tokens into natural text.'
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8 标记器实现共享两个常见方法：一个是编码方法，一个是解码方法。编码方法接受示例文本，将其拆分为单独的标记，并通过词汇表将标记转换为标记ID。解码方法接受标记ID，将其转换回文本标记，并将文本标记连接成自然文本。
- en: '![](images/ch-02__image016.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image016.png)'
- en: 'Let''s instantiate a new tokenizer object from the `SimpleTokenizerV1` class
    and tokenize a passage from Edith Wharton''s short story to try it out in practice:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`SimpleTokenizerV1`类中实例化一个新的标记对象，并对爱迪丝·沃顿的短篇小说中的段落进行分词，以尝试实践一下：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The code above prints the following token IDs:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码打印了以下标记ID：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, let''s see if we can turn these token IDs back into text using the decode
    method:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看是否可以使用解码方法将这些标记ID还原为文本：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This outputs the following text:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下文本：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Based on the output above, we can see that the decode method successfully converted
    the token IDs back into the original text.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上面的输出，我们可以看到解码方法成功地将标记ID转换回原始文本。
- en: 'So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing
    text based on a snippet from the training set. Let''s now apply it to a new text
    sample that is not contained in the training set:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 目前为止，我们已经实现了一个能够根据训练集中的片段对文本进行标记化和解标记化的标记器。现在让我们将其应用于训练集中不包含的新文本样本：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Executing the code above will result in the following error:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上面的代码将导致以下错误：
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The problem is that the word "Hello" was not used in the *The Verdict* short
    story. Hence, it is not contained in the vocabulary. This highlights the need
    to consider large and diverse training sets to extend the vocabulary when working
    on LLMs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于“Hello”这个词没有在*The Verdict*短篇小说中出现过。因此，它不包含在词汇表中。这突显了在处理LLMs时需要考虑大量和多样的训练集以扩展词汇表的需求。
- en: In the next section, we will test the tokenizer further on text that contains
    unknown words, and we will also discuss additional special tokens that can be
    used to provide further context for an LLM during training.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将进一步测试标记器对包含未知单词的文本的处理，我们还将讨论在训练期间可以使用的额外特殊标记，以提供LLM更多的上下文信息。
- en: 2.4 Adding special context tokens
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 添加特殊上下文标记
- en: In the previous section, we implemented a simple tokenizer and applied it to
    a passage from the training set. In this section, we will modify this tokenizer
    to handle unknown words.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们实现了一个简单的标记器，并将其应用于训练集中的一个段落。在本节中，我们将修改这个标记器来处理未知单词。
- en: We will also discuss the usage and addition of special context tokens that can
    enhance a model's understanding of context or other relevant information in the
    text. These special tokens can include markers for unknown words and document
    boundaries, for example.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将讨论使用和添加特殊上下文标记的用法，这些标记可以增强模型对文本中上下文或其他相关信息的理解。这些特殊标记可以包括未知单词和文档边界的标记，例如。
- en: In particular, we will modify the vocabulary and tokenizer we implemented in
    the previous section, SimpleTokenizerV2, to support two new tokens, `<|unk|>`
    and `<|endoftext|>`, as illustrated in figure 2.8.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将修改上一节中实现的词汇表和标记器SimpleTokenizerV2，以支持两个新的标记`<|unk|>`和`<|endoftext|>`，如图2.8所示。
- en: Figure 2.9 We add special tokens to a vocabulary to deal with certain contexts.
    For instance, we add an <|unk|> token to represent new and unknown words that
    were not part of the training data and thus not part of the existing vocabulary.
    Furthermore, we add an <|endoftext|> token that we can use to separate two unrelated
    text sources.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9 我们向词汇表中添加特殊标记来处理特定上下文。 例如，我们添加一个<|unk|>标记来表示训练数据中没有出现过的新单词，因此不是现有词汇表的一部分。
    此外，我们添加一个<|endoftext|>标记，用于分隔两个无关的文本源。
- en: '![](images/ch-02__image018.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image018.png)'
- en: As shown in figure 2.9, we can modify the tokenizer to use an `<|unk|>` token
    if it encounters a word that is not part of the vocabulary. Furthermore, we add
    a token between unrelated texts. For example, when training GPT-like LLMs on multiple
    independent documents or books, it is common to insert a token before each document
    or book that follows a previous text source, as illustrated in figure 2.10\. This
    helps the LLM understand that, although these text sources are concatenated for
    training, they are, in fact, unrelated.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如图2.9所示，我们可以修改标记器，在遇到不在词汇表中的单词时使用`<|unk|>`标记。 此外，我们在无关的文本之间添加一个标记。 例如，在训练多个独立文档或书籍的GPT-like
    LLM时，通常会在每个文档或书籍之前插入一个标记，用于指示这是前一个文本源的后续文档或书籍，如图2.10所示。 这有助于LLM理解，尽管这些文本源被连接起来进行训练，但实际上它们是无关的。
- en: Figure 2.10 When working with multiple independent text source, we add <|endoftext|>
    tokens between these texts. These <|endoftext|> tokens act as markers, signaling
    the start or end of a particular segment, allowing for more effective processing
    and understanding by the LLM.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10 当处理多个独立的文本源时，我们在这些文本之间添加`<|endoftext|>`标记。 这些`<|endoftext|>`标记充当标记，标志着特定段落的开始或结束，让LLM更有效地处理和理解。
- en: '![](images/ch-02__image020.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image020.png)'
- en: 'Let''s now modify the vocabulary to include these two special tokens, `<unk>`
    and `<|endoftext|>`, by adding these to the list of all unique words that we created
    in the previous section:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们修改词汇表，以包括这两个特殊标记`<unk>`和`<|endoftext|>`，通过将它们添加到我们在上一节中创建的所有唯一单词列表中：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Based on the output of the print statement above, the new vocabulary size is
    1161 (the vocabulary size in the previous section was 1159).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述打印语句的输出，新的词汇表大小为1161（上一节的词汇表大小为1159）。
- en: 'As an additional quick check, let''s print the last 5 entries of the updated
    vocabulary:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外的快速检查，让我们打印更新后词汇表的最后5个条目：
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The code above prints the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码打印如下所示：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Based on the code output above, we can confirm that the two new special tokens
    were indeed successfully incorporated into the vocabulary. Next, we adjust the
    tokenizer from code listing 2.3 accordingly, as shown in listing 2.4:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上面的代码输出，我们可以确认这两个新的特殊标记确实成功地融入到了词汇表中。 接下来，我们根据代码清单2.3调整标记器，如清单2.4所示：
- en: Listing 2.4 A simple text tokenizer that handles unknown words
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单2.4 处理未知词的简单文本标记器
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Compared to the `SimpleTokenizerV1` we implemented in code listing 2.3 in the
    previous section, the new `SimpleTokenizerV2` replaces unknown words by `<|unk|>`
    tokens.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一节代码清单2.3中实现的`SimpleTokenizerV1`相比，新的`SimpleTokenizerV2`将未知单词替换为`<|unk|>`标记。
- en: 'Let''s now try this new tokenizer out in practice. For this, we will use a
    simple text sample that we concatenate from two independent and unrelated sentences:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试实践这种新的标记器。 为此，我们将使用一个简单的文本示例，该文本由两个独立且无关的句子串联而成：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, let''s tokenize the sample text using the `SimpleTokenizerV2`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用`SimpleTokenizerV2`对样本文本进行标记：
- en: '[PRE29]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This prints the following token IDs:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这打印了以下令牌ID：
- en: '[PRE30]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Above, we can see that the list of token IDs contains 1159 for the <|endoftext|>
    separator token as well as two 160 tokens, which are used for unknown words.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面可以看到，令牌ID列表包含1159个<|endoftext|>分隔符令牌，以及两个用于未知单词的160个令牌。
- en: 'Let''s de-tokenize the text for a quick sanity check:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对文本进行反标记，做一个快速的检查：
- en: '[PRE31]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE32]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Based on comparing the de-tokenized text above with the original input text,
    we know that the training dataset, Edith Wharton's short story *The Verdict*,
    did not contain the words "Hello" and "palace."
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述去标记化文本与原始输入文本的比较，我们知道埃迪斯·沃顿（Edith Wharton）的短篇小说*The Verdict*训练数据集中不包含单词“Hello”和“palace”。
- en: 'So far, we have discussed tokenization as an essential step in processing text
    as input to LLMs. Depending on the LLM, some researchers also consider additional
    special tokens such as the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了分词作为将文本处理为LLMs输入的基本步骤。根据LLM，一些研究人员还考虑其他特殊标记，如下所示：
- en: '`[BOS]` (beginning of sequence): This token marks the start of a text. It signifies
    to the LLM where a piece of content begins.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[BOS]`（序列开始）：该标记标志着文本的开始。它向LLM表示内容的开始位置。'
- en: '`[EOS]` (end of sequence): This token is positioned at the end of a text, and
    is especially useful when concatenating multiple unrelated texts, similar to `<|endoftext|>`.
    For instance, when combining two different Wikipedia articles or books, the `[EOS]`
    token indicates where one article ends and the next one begins.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[EOS]`（序列结束）：该标记位于文本末尾，当连接多个不相关的文本时特别有用，类似于`<|endoftext|>`。例如，当合并两篇不同的维基百科文章或书籍时，`[EOS]`标记指示一篇文章的结束和下一篇文章的开始位置。'
- en: '`[PAD]` (padding): When training LLMs with batch sizes larger than one, the
    batch might contain texts of varying lengths. To ensure all texts have the same
    length, the shorter texts are extended or "padded" using the `[PAD]` token, up
    to the length of the longest text in the batch.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[PAD]`（填充）：当使用大于一的批次大小训练LLMs时，批次可能包含不同长度的文本。为确保所有文本具有相同长度，较短的文本将使用`[PAD]`标记进行扩展或“填充”，直到批次中最长文本的长度。'
- en: Note that the tokenizer used for GPT models does not need any of these tokens
    mentioned above but only uses an `<|endoftext|>` token for simplicity. The `<|endoftext|>`
    is analogous to the `[EOS]` token mentioned above. Also, `<|endoftext|>` is used
    for padding as well. However, as we'll explore in subsequent chapters when training
    on batched inputs, we typically use a mask, meaning we don't attend to padded
    tokens. Thus, the specific token chosen for padding becomes inconsequential.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，用于GPT模型的分词器不需要上述提到的任何这些标记，而仅使用`<|endoftext|>`标记简化。`<|endoftext|>`类似于上述的`[EOS]`标记。此外，`<|endoftext|>`也用于填充。然而，在后续章节中，当在批量输入上训练时，我们通常使用掩码，意味着我们不关注填充的标记。因此，所选择的特定填充标记变得不重要。
- en: Moreover, the tokenizer used for GPT models also doesn't use an `<|unk|>` token
    for out-of-vocabulary words. Instead, GPT models use a *byte pair encoding* tokenizer,
    which breaks down words into subword units, which we will discuss in the next
    section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，用于GPT模型的分词器也不使用`<|unk|>`标记来表示词汇表中没有的单词。相反，GPT模型使用字节对编码分词器，将单词拆分为子词单元，我们将在下一节中讨论。
- en: 2.5 Byte pair encoding
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 字节对编码
- en: We implemented a simple tokenization scheme in the previous sections for illustration
    purposes. This section covers a more sophisticated tokenization scheme based on
    a concept called byte pair encoding (BPE). The BPE tokenizer covered in this section
    was used to train LLMs such as GPT-2, GPT-3, and ChatGPT.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几节中实现了一个简单的分词方案，用于说明目的。本节介绍基于称为字节对编码（BPE）的概念的更复杂的分词方案。本节介绍的BPE分词器用于训练LLMs，如GPT-2、GPT-3和ChatGPT。
- en: 'Since implementing BPE can be relatively complicated, we will use an existing
    Python open-source library called *tiktoken* ([https://github.com/openai/tiktoken](openai.html)),
    which implements the BPE algorithm very efficiently based on source code in Rust.
    Similar to other Python libraries, we can install the tiktoken library via Python''s
    `pip` installer from the terminal:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实现BPE可能相对复杂，我们将使用一个名为*tiktoken*（[https://github.com/openai/tiktoken](openai.html)）的现有Python开源库，该库基于Rust中的源代码非常有效地实现了BPE算法。与其他Python库类似，我们可以通过Python的终端上的`pip`安装程序安装tiktoken库：
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The code in this chapter is based on tiktoken 0.5.1\. You can use the following
    code to check the version you currently have installed:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码基于tiktoken 0.5.1。您可以使用以下代码检查当前安装的版本：
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，我们可以如下实例化tiktoken中的BPE分词器：
- en: '[PRE35]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented
    previously via an `encode` method:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器的使用方式类似于我们之前通过`encode`方法实现的SimpleTokenizerV2：
- en: '[PRE36]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The code above prints the following token IDs:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码打印以下标记ID：
- en: '[PRE37]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can then convert the token IDs back into text using the decode method, similar
    to our `SimpleTokenizerV2` earlier:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用解码方法将标记ID转换回文本，类似于我们之前的`SimpleTokenizerV2`：
- en: '[PRE38]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The above code prints the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码打印如下：
- en: '[PRE39]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We can make two noteworthy observations based on the token IDs and decoded text
    above. First, the `<|endoftext|>` token is assigned a relatively large token ID,
    namely, 50256\. In fact, the BPE tokenizer that was used to train models such
    as GPT-2, GPT-3, and ChatGPT has a total vocabulary size of 50,257, with `<|endoftext|>`
    being assigned the largest token ID.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述标记ID和解码文本，我们可以得出两个值得注意的观察结果。首先，`<|endoftext|>`标记被分配了一个相对较大的标记ID，即50256。事实上，用于训练诸如GPT-2、GPT-3和ChatGPT等模型的BPE分词器具有总共50257个词汇，其中`<|endoftext|>`被分配了最大的标记ID。
- en: Second, the BPE tokenizer above encodes and decodes unknown words, such as "someunknownPlace"
    correctly. The BPE tokenizer can handle any unknown word. How does it achieve
    this without using `<|unk|>` tokens?
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，上述的BPE分词器可以正确地对未知单词进行编码和解码，例如"someunknownPlace"。BPE分词器可以处理任何未知单词。它是如何在不使用`<|unk|>`标记的情况下实现这一点的？
- en: The algorithm underlying BPE breaks down words that aren't in its predefined
    vocabulary into smaller subword units or even individual characters, enabling
    it to handle out-of-vocabulary words. So, thanks to the BPE algorithm, if the
    tokenizer encounters an unfamiliar word during tokenization, it can represent
    it as a sequence of subword tokens or characters, as illustrated in figure 2.11.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: BPE算法的基础是将不在其预定义词汇表中的单词分解为更小的子词单元甚至是单个字符，使其能够处理词汇表之外的词汇。因此，多亏了BPE算法，如果分词器在分词过程中遇到陌生的单词，它可以将其表示为一系列子词标记或字符，如图2.11所示。
- en: Figure 2.11 BPE tokenizers break down unknown words into subwords and individual
    characters. This way, a BPE tokenizer can parse any word and doesn't need to replace
    unknown words with special tokens, such as <|unk|>.
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11 BPE分词器将未知单词分解为子词和单个字符。这样，BPE分词器可以解析任何单词，无需用特殊标记（如`<|unk|>`）替换未知单词。
- en: '![](images/ch-02__image022.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image022.png)'
- en: As illustrated in figure 2.11, the ability to break down unknown words into
    individual characters ensures that the tokenizer, and consequently the LLM that
    is trained with it, can process any text, even if it contains words that were
    not present in its training data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 2.11 所示，将未知单词分解为单个字符的能力确保了分词器以及随之训练的LLM可以处理任何文本，即使其中包含了其训练数据中未出现的单词。
- en: Exercise 2.1 Byte pair encoding of unknown words
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2.1 未知单词的字节对编码
- en: Try the BPE tokenizer from the tiktoken library on the unknown words "Akwirw
    ier" and print the individual token IDs. Then, call the decode function on each
    of the resulting integers in this list to reproduce the mapping shown in figure
    2.1\. Lastly, call the decode method on the token IDs to check whether it can
    reconstruct the original input, "Akwirw ier".
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试从tiktoken库中使用BPE分词器对未知单词"Akwirw ier"，并打印各个标记的ID。然后，在此列表中的每个生成的整数上调用解码函数，以重现图2.1中显示的映射。最后，在标记ID上调用解码方法以检查是否可以重建原始输入，即"Akwirw
    ier"。
- en: A detailed discussion and implementation of BPE is out of the scope of this
    book, but in short, it builds its vocabulary by iteratively merging frequent characters
    into subwords and frequent subwords into words. For example, BPE starts with adding
    all individual single characters to its vocabulary ("a", "b", ...). In the next
    stage, it merges character combinations that frequently occur together into subwords.
    For example, "d" and "e" may be merged into the subword "de," which is common
    in many English words like "define", "depend", "made", and "hidden". The merges
    are determined by a frequency cutoff.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不讨论BPE的详细讨论和实现，但简而言之，它通过迭代地将频繁出现的字符合并为子词和频繁出现的子词合并为单词来构建其词汇表。例如，BPE从将所有单个字符添加到其词汇表开始（"a"，"b"，...）。在下一阶段，它将经常一起出现的字符组合成子词。例如，"d"和"e"可能会合并成子词"de"，在许多英文单词中很常见，如"define"，"depend"，"made"和"hidden"。合并是由频率截止确定的。
- en: 2.6 Data sampling with a sliding window
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 滑动窗口数据采样
- en: The previous section covered the tokenization steps and conversion from string
    tokens into integer token IDs in great detail. The next step before we can finally
    create the embeddings for the LLM is to generate the input-target pairs required
    for training an LLM.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节详细介绍了标记化步骤以及将字符串标记转换为整数标记ID之后，我们最终可以为LLM生成所需的输入-目标对，以用于训练LLM。
- en: What do these input-target pairs look like? As we learned in chapter 1, LLMs
    are pretrained by predicting the next word in a text, as depicted in figure 2.12.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这些输入-目标对是什么样子？正如我们在第一章中学到的那样，LLMs 是通过预测文本中的下一个单词来进行预训练的，如图 2.12 所示。
- en: Figure 2.12 Given a text sample, extract input blocks as subsamples that serve
    as input to the LLM, and the LLM's prediction task during training is to predict
    the next word that follows the input block. During training, we mask out all words
    that are past the target. Note that the text shown in this figure would undergo
    tokenization before the LLM can process it; however, this figure omits the tokenization
    step for clarity.
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.12 给定一个文本样本，提取作为 LLM 输入的子样本的输入块，并且在训练期间，LLM 的预测任务是预测跟随输入块的下一个单词。在训练中，我们屏蔽所有超过目标的单词。请注意，在
    LLM 可处理文本之前，此图中显示的文本会进行 tokenization；但为了清晰起见，该图省略了 tokenization 步骤。
- en: '![](images/ch-02__image024.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image024.png)'
- en: In this section we implement a data loader that fetches the input-target pairs
    depicted in figure 2.12 from the training dataset using a sliding window approach.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在此部分中，我们实现了一个数据加载器，使用滑动窗口方法从训练数据集中提取图 2.12 中所示的输入-目标对。
- en: 'To get started, we will first tokenize the whole The Verdict short story we
    worked with earlier using the BPE tokenizer introduced in the previous section:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，我们将使用前面介绍的 BPE tokenizer 对我们之前使用的《裁决》短篇小说进行标记化处理：
- en: '[PRE40]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Executing the code above will return 5145, the total number of tokens in the
    training set, after applying the BPE tokenizer.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码将返回 5145，应用 BPE tokenizer 后训练集中的总标记数。
- en: 'Next, we remove the first 50 tokens from the dataset for demonstration purposesas
    it results in a slightly more interesting text passage in the next steps:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了演示目的，让我们从数据集中删除前 50 个标记，因为这会使接下来的文本段落稍微有趣一些：
- en: '[PRE41]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'One of the easiest and most intuitive ways to create the input-target pairs
    for the next-word prediction task is to create two variables, `x` and `y`, where
    `x` contains the input tokens and `y` contains the targets, which are the inputs
    shifted by 1:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 创建下一个单词预测任务的输入-目标对最简单直观的方法之一是创建两个变量，`x` 和 `y`，其中 `x` 包含输入标记，`y` 包含目标，即将输入向后移动一个位置的输入：
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Running the above code prints the following output:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码会打印以下输出：
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Processing the inputs along with the targets, which are the inputs shifted
    by one position, we can then create the next-word prediction tasks depicted earlier
    in figure 2.12, as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 处理输入以及目标（即向后移动了一个位置的输入），我们可以创建如图 2.12 中所示的下一个单词预测任务：
- en: '[PRE44]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The code above prints the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会打印以下内容：
- en: '[PRE45]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Everything left of the arrow (`---->`) refers to the input an LLM would receive,
    and the token ID on the right side of the arrow represents the target token ID
    that the LLM is supposed to predict.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 形如箭头 (`---->`) 左侧的所有内容指的是 LLM 收到的输入，箭头右侧的标记 ID 表示 LLM 应该预测的目标标记 ID。
- en: 'For illustration purposes, let''s repeat the previous code but convert the
    token IDs into text:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明目的，让我们重复之前的代码但将标记 ID 转换为文本：
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following outputs show how the input and outputs look in text format:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示输入和输出以文本格式的样式：
- en: '[PRE47]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We've now created the input-target pairs that we can turn into use for the LLM
    training in upcoming chapters.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经创建了输入-目标对，可以在接下来的章节中用于 LLM 训练。
- en: 'There''s only one more task before we can turn the tokens into embeddings,
    as we mentioned at the beginning of this chapter: implementing an efficient data
    loader that iterates over the input dataset and returns the inputs and targets
    as PyTorch tensors.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以将标记转换为嵌入之前，还有最后一个任务，正如我们在本章开头所提到的：实现一个高效的数据加载器，迭代输入数据集并返回 PyTorch 张量作为输入和目标。
- en: 'In particular, we are interested in returning two tensors: an input tensor
    containing the text that the LLM sees and a target tensor that includes the targets
    for the LLM to predict, as depicted in figure 2.13.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们有兴趣返回两个张量：一个包含 LLM 看到的文本的输入张量，以及一个包含 LLM 预测目标的目标张量，如图 2.13 所示。
- en: Figure 2.13 To implement efficient data loaders, we collect the inputs in a
    tensor, x, where each row represents one input context. A second tensor, y, contains
    the corresponding prediction targets (next words), which are created by shifting
    the input by one position.
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.13 为了实现高效的数据加载器，我们将输入都收集到一个张量 x 中，其中每一行代表一个输入上下文。第二个张量 y 包含对应的预测目标（下一个单词），它们是通过将输入向后移动一个位置来创建的。
- en: '![](images/ch-02__image026.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image026.png)'
- en: While figure 2.13 shows the tokens in string format for illustration purposes,
    the code implementation will operate on token IDs directly since the encode method
    of the BPE tokenizer performs both tokenization and conversion into token IDs
    as a single step.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图2.13展示了字符串格式的token以进行说明，但代码实现将直接操作token ID，因为BPE标记器的encode方法执行了tokenization和转换为token
    ID为单一步骤。
- en: For the efficient data loader implementation, we will use PyTorch's built-in
    Dataset and DataLoader classes. For additional information and guidance on installing
    PyTorch, please see section A.1.3, Installing PyTorch, in Appendix A.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高效的数据加载器实现，我们将使用PyTorch内置的Dataset和DataLoader类。有关安装PyTorch的更多信息和指导，请参阅附录A的*A.1.3，安装PyTorch*一节。
- en: 'The code for the dataset class is shown in code listing 2.5:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集类的代码如图2.5所示：
- en: Listing 2.5 A dataset for batched inputs and targets
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 一批输入和目标的数据集
- en: '[PRE48]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The `GPTDatasetV1` class in listing 2.5 is based on the PyTorch `Dataset` class
    and defines how individual rows are fetched from the dataset, where each row consists
    of a number of token IDs (based on a `max_length`) assigned to an `input_chunk`
    tensor. The `target_chunk` tensor contains the corresponding targets. I recommend
    reading on to see how the data returned from this dataset looks like when we combine
    the dataset with a PyTorch `DataLoader` -- this will bring additional intuition
    and clarity.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5中的`GPTDatasetV1`类基于PyTorch的`Dataset`类，定义了如何从数据集中获取单独的行，其中每一行都包含一系列基于`max_length`分配给`input_chunk`张量的token
    ID。`target_chunk`张量包含相应的目标。我建议继续阅读，看看当我们将数据集与PyTorch的`DataLoader`结合使用时，这个数据集返回的数据是什么样的——这将带来额外的直觉和清晰度。
- en: If you are new to the structure of PyTorch `Dataset` classes, such as shown
    in listing 2.5, please read section *A.6, Setting up efficient data loaders*,
    in Appendix A, which explains the general structure and usage of PyTorch `Dataset`
    and `DataLoader` classes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对PyTorch的`Dataset`类的结构（如图2.5所示）是新手，请阅读附录A的*A.6，设置高效的数据加载器*一节，其中解释了PyTorch的`Dataset`和`DataLoader`类的一般结构和用法。
- en: 'The following code will use the `GPTDatasetV1` to load the inputs in batches
    via a PyTorch `DataLoader`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将使用`GPTDatasetV1`通过PyTorch的`DataLoader`来批量加载输入：
- en: Listing 2.6 A data loader to generate batches with input-with pairs
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 用于生成带输入对的批次的数据加载器
- en: '[PRE49]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let''s test the `dataloader` with a batch size of 1 for an LLM with a context
    size of 4 to develop an intuition of how the `GPTDatasetV1` class from listing
    2.5 and the `create_dataloader` function from listing 2.6 work together:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试`dataloader`，将一个上下文大小为4的LLM的批量大小设为1，以便理解图2.5的`GPTDatasetV1`类和图2.6的`create_dataloader`函数如何协同工作。
- en: '[PRE50]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Executing the preceding code prints the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码将打印以下内容：
- en: '[PRE51]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The `first_batch` variable contains two tensors: the first tensor stores the
    input token IDs, and the second tensor stores the target token IDs. Since the
    `max_length` is set to 4, each of the two tensors contains 4 token IDs. Note that
    an input size of 4 is relatively small and only chosen for illustration purposes.
    It is common to train LLMs with input sizes of at least 256.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`first_batch`变量包含两个张量：第一个张量存储输入 token ID，第二个张量存储目标 token ID。由于`max_length`设置为4，这两个张量每个都包含4个token
    ID。值得注意的是，输入大小为4相对较小，仅用于说明目的。通常会用至少256的输入大小来训练LLMs。'
- en: 'To illustrate the meaning of `stride=1`, let''s fetch another batch from this
    dataset:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明`stride=1`的含义，让我们从这个数据集中获取另一个批次：
- en: '[PRE52]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The second batch has the following contents:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 第二批的内容如下：
- en: '[PRE53]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: If we compare the first with the second batch, we can see that the second batch's
    token IDs are shifted by one position compared to the first batch (for example,
    the second ID in the first batch's input is 367, which is the first ID of the
    second batch's input). The `stride` setting dictates the number of positions the
    inputs shift across batches, emulating a sliding window approach, as demonstrated
    in Figure 2.14.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较第一批和第二批，我们会发现相对于第一批，第二批的token ID向后移动了一个位置（例如，第一批输入中的第二个ID是367，这是第二批输入中的第一个ID）。`stride`设置规定了输入在批次之间移动的位置数，模拟了一个滑动窗口的方法，如图2.14所示。
- en: Figure 2.14 When creating multiple batches from the input dataset, we slide
    an input window across the text. If the stride is set to 1, we shift the input
    window by 1 position when creating the next batch. If we set the stride equal
    to the input window size, we can prevent overlaps between the batches.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.14 在从输入数据集创建多个批次时，我们在文本上滑动一个输入窗口。如果将步幅设置为1，则在创建下一个批次时，将输入窗口向右移动1个位置。如果我们将步幅设置为等于输入窗口大小，我们可以防止批次之间的重叠。
- en: '![](images/ch-02__image028.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image028.png)'
- en: Exercise 2.2 Data loaders with different strides and context sizes
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习2.2 具有不同步幅和上下文大小的数据加载器
- en: To develop more intuition for how the data loader works, try to run it with
    different settings such as max_length=2 and stride=2 and max_length=8 and stride=2.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解数据加载器的工作原理，请尝试以不同设置运行，如max_length=2和stride=2以及max_length=8和stride=2。
- en: Batch sizes of 1, such as we have sampled from the data loader so far, are useful
    for illustration purposes. If you have previous experience with deep learning,
    you may know that small batch sizes require less memory during training but lead
    to more noisy model updates. Just like in regular deep learning, the batch size
    is a trade-off and hyperparameter to experiment with when training LLMs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们到目前为止从数据加载器中抽样的批次大小为1一样，这对于说明目的非常有用。如果您有深度学习的经验，您可能知道，较小的批次大小在训练期间需要更少的内存，但会导致更多的噪声模型更新。就像在常规深度学习中一样，批次大小是一个需要在训练LLM时进行实验的权衡和超参数。
- en: 'Before we move on to the two final sections of this chapter that are focused
    on creating the embedding vectors from the token IDs, let''s have a brief look
    at how we can use the data loader to sample with a batch size greater than 1:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续本章的最后两个重点部分，这些部分侧重于从标记ID创建嵌入向量之前，让我们简要了解如何使用数据加载器进行批量大小大于1的抽样：
- en: '[PRE54]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This prints the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE55]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Note that we increase the stride to 5, which is the max length + 1\. This is
    to utilize the data set fully (we don't skip a single word) but also avoid any
    overlap between the batches, since more overlap could lead to increased overfitting.
    For instance, if we set the stride equal to the max length, the target ID for
    the last input token ID in each row would become the first input token ID in the
    next row.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将步幅增加到5，这是最大长度+1。这是为了充分利用数据集（我们不跳过任何单词），同时避免批次之间的任何重叠，因为更多的重叠可能导致过拟合增加。例如，如果我们将步幅设置为与最大长度相等，那么每行中最后一个输入标记ID的目标ID将成为下一行中第一个输入标记ID。
- en: In the final two sections of this chapter, we will implement embedding layers
    that convert the token IDs into continuous vector representations, which serve
    as input data format for LLMs.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后两个部分中，我们将实现将标记ID转换为连续向量表示的嵌入层，这将作为LLM的输入数据格式。
- en: 2.7 Creating token embeddings
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 创建标记嵌入
- en: The last step for preparing the input text for LLM training is to convert the
    token IDs into embedding vectors, as illustrated in figure 2.15, which will be
    the focus of these two last remaining sections of this chapter.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为准备LLM训练的输入文本的最后一步是将标记ID转换为嵌入向量，如图2.15所示，这将是本章最后两个剩余部分的重点。
- en: Figure 2.15 Preparing the input text for an LLM involves tokenizing text, converting
    text tokens to token IDs, and converting token IDs into vector embedding vectors.
    In this section, we consider the token IDs created in previous sections to create
    the token embedding vectors.
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.15 准备LLM输入文本涉及对文本进行标记化、将文本标记转换为标记ID和将标记ID转换为向量嵌入向量。在本节中，我们考虑前几节中创建的标记ID以创建标记嵌入向量。
- en: '![](images/ch-02__image030.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image030.png)'
- en: A continuous vector representation, or embedding, is necessary since GPT-like
    LLMs are deep neural networks trained with the backpropagation algorithm. If you
    are unfamiliar with how neural networks are trained with backpropagation, please
    read section A.4, *Automatic differentiation made easy*, in Appendix A.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 连续向量表示，或嵌入，是必要的，因为类似GPT的LLM是使用反向传播算法训练的深度神经网络。如果您不熟悉神经网络如何使用反向传播进行训练，请阅读附录A中的第A.4节，*简化的自动微分*。
- en: 'Let''s illustrate how the token ID to embedding vector conversion works with
    a hands-on example. Suppose we have the following three input tokens with IDs
    5, 1, 3, and 2:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个实际例子说明标记ID到嵌入向量转换是如何工作的。假设我们有以下三个带有ID 5、1、3和2的输入标记：
- en: '[PRE56]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'For the sake of simplicity and illustration purposes, suppose we have a small
    vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary),
    and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288
    dimensions):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见和说明目的，假设我们只有一个小的词汇表，其中只有 6 个单词（而不是 BPE 标记器词汇表中的 50,257 个单词），我们想创建大小为 3
    的嵌入（在 GPT-3 中，嵌入大小为 12,288 维）：
- en: '[PRE57]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Using the `vocab_size` and `output_dim`, we can instantiate an embedding layer
    in PyTorch, setting the random seed to 123 for reproducibility purposes:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `vocab_size` 和 `output_dim`，我们可以在 PyTorch 中实例化一个嵌入层，设置随机种子为 123 以便进行再现性：
- en: '[PRE58]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The print statement in the preceding code example prints the embedding layer''s
    underlying weight matrix:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码示例中的打印语句打印了嵌入层的底层权重矩阵：
- en: '[PRE59]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We can see that the weight matrix of the embedding layer contains small, random
    values. These values are optimized during LLM training as part of the LLM optimization
    itself, as we will see in upcoming chapters. Moreover, we can see that the weight
    matrix has six rows and three columns. There is one row for each of the six possible
    tokens in the vocabulary. And there is one column for each of the three embedding
    dimensions.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到嵌入层的权重矩阵包含了小型的随机值。这些值在 LLM 训练过程中作为 LLM 优化的一部分而被优化，我们将在后续章节中看到。此外，我们可以看到权重矩阵有六行和三列。词汇表中的每个可能的标记都有一行。这三个嵌入维度中的每个维度都有一列。
- en: 'After we instantiated the embedding layer, let''s now apply it to a token ID
    to obtain the embedding vector:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实例化嵌入层之后，现在让我们将其应用到一个标记 ID 上以获取嵌入向量：
- en: '[PRE60]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The returned embedding vector is as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的嵌入向量如下：
- en: '[PRE61]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: If we compare the embedding vector for token ID 3 to the previous embedding
    matrix, we see that it is identical to the 4th row (Python starts with a zero
    index, so it's the row corresponding to index 3). In other words, the embedding
    layer is essentially a look-up operation that retrieves rows from the embedding
    layer's weight matrix via a token ID.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将标记 ID 3 的嵌入向量与先前的嵌入矩阵进行比较，我们会看到它与第四行完全相同（Python 从零索引开始，所以它是与索引 3 对应的行）。换句话说，嵌入层本质上是一个查找操作，它通过标记
    ID 从嵌入层的权重矩阵中检索行。
- en: Embedding layers versus matrix multiplication
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 嵌入层与矩阵乘法
- en: For those who are familiar with one-hot encoding, the embedding layer approach
    above is essentially just a more efficient way of implementing one-hot encoding
    followed by matrix multiplication in a fully connected layer, which is illustrated
    in the supplementary code on GitHub at [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/03_bonus_embedding-vs-matmul](ch02.html).
    Because the embedding layer is just a more efficient implementation equivalent
    to the one-hot encoding and matrix-multiplication approach, it can be seen as
    a neural network layer that can be optimized via backpropagation.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些熟悉独热编码的人来说，上面的嵌入层方法实质上只是实施独热编码加上全连接层中的矩阵乘法更高效的一种方式，这在 GitHub 上的补充代码中进行了说明
    [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/03_bonus_embedding-vs-matmul](ch02.html)。因为嵌入层只是一个更高效的等效实现，等同于独热编码和矩阵乘法方法，它可以看作是一个可以通过反向传播进行优化的神经网络层。
- en: 'Previously, we have seen how to convert a single token ID into a three-dimensional
    embedding vector. Let''s now apply that to all four input IDs we defined earlier
    (`torch.tensor([5, 1, 3, 2])`):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前，我们已经看到如何将单个标记 ID 转换为三维嵌入向量。现在让我们将其应用到我们之前定义的四个输入 ID 上 (`torch.tensor([5,
    1, 3, 2])`)：
- en: '[PRE62]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The print output reveals that this results in a 4x3 matrix:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 打印输出显示，结果是一个 4x3 的矩阵：
- en: '[PRE63]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Each row in this output matrix is obtained via a lookup operation from the embedding
    weight matrix, as illustrated in figure 2.16.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出矩阵中的每一行都是通过从嵌入权重矩阵中进行查找操作得到的，正如图 2.16 所示。
- en: Figure 2.16 Embedding layers perform a look-up operation, retrieving the embedding
    vector corresponding to the token ID from the embedding layer's weight matrix.
    For instance, the embedding vector of the token ID 5 is the sixth row of the embedding
    layer weight matrix (it is the sixth instead of the fifth row because Python starts
    counting at 0).
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.16 嵌入层执行查找操作，从嵌入层的权重矩阵中检索与标记 ID 对应的嵌入向量。例如，标记 ID 5 的嵌入向量是嵌入层权重矩阵的第六行（它是第六行而不是第五行，因为
    Python 从 0 开始计数）。
- en: '![](images/ch-02__image032.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image032.png)'
- en: This section covered how we create embedding vectors from token IDs. The next
    and final section of this chapter will add a small modification to these embedding
    vectors to encode positional information about a token within a text.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何从标记ID创建嵌入向量。本章的下一节也是最后一节，将对这些嵌入向量进行一些小的修改，以编码文本中标记的位置信息。
- en: 2.8 Encoding word positions
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 编码词的位置
- en: In the previous section, we converted the token IDs into a continuous vector
    representation, the so-called token embeddings. In principle, this is a suitable
    input for an LLM. However, a minor shortcoming of LLMs is that their self-attention
    mechanism, which will be covered in detail in chapter 3, doesn't have a notion
    of position or order for the tokens within a sequence.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们将标记ID转换为连续的向量表示，即所谓的标记嵌入。从原则上讲，这对于LLM来说是一个合适的输入。然而，LLM的一个小缺陷是，它们的自我注意机制（将详细介绍于第3章中）对于序列中的标记没有位置或顺序的概念。
- en: The way the previously introduced embedding layer works is that the same token
    ID always gets mapped to the same vector representation, regardless of where the
    token ID is positioned in the input sequence, as illustrated in figure 2.17.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 先前介绍的嵌入层的工作方式是，相同的标记ID始终被映射到相同的向量表示，无论标记ID在输入序列中的位置如何，如图2.17所示。
- en: Figure 2.17 The embedding layer converts a token ID into the same vector representation
    regardless of where it is located in the input sequence. For example, the token
    ID 5, whether it's in the first or third position in the token ID input vector,
    will result in the same embedding vector.
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.17 嵌入层将标记ID转换为相同的向量表示，无论其在输入序列中的位置如何。例如，标记ID 5，无论是在标记ID输入向量的第一个位置还是第三个位置，都会导致相同的嵌入向量。
- en: '![](images/ch-02__image034.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image034.png)'
- en: In principle, the deterministic, position-independent embedding of the token
    ID is good for reproducibility purposes. However, since the self-attention mechanism
    of LLMs itself is also position-agnostic, it is helpful to inject additional position
    information into the LLM.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从原则上讲，标记ID的确定性、位置无关的嵌入对于可重现性目的很好。然而，由于LLM的自我注意机制本身也是位置不可知的，向LLM注入额外的位置信息是有帮助的。
- en: 'To achieve this, there are two broad categories of position-aware embeddings:
    relative *positional embeddings* and absolute positional embeddings.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，位置感知嵌入有两个广泛的类别：相对*位置嵌入*和绝对位置嵌入。
- en: Absolute positional embeddings are directly associated with specific positions
    in a sequence. For each position in the input sequence, a unique embedding is
    added to the token's embedding to convey its exact location. For instance, the
    first token will have a specific positional embedding, the second token another
    distinct embedding, and so on, as illustrated in figure 2.18.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对位置嵌入与序列中的特定位置直接相关联。对于输入序列中的每个位置，都会添加一个唯一的嵌入，以传达其确切位置。例如，第一个标记将具有特定的位置嵌入，第二个标记是另一个不同的嵌入，依此类推，如图2.18所示。
- en: Figure 2.18 Positional embeddings are added to the token embedding vector to
    create the input embeddings for an LLM. The positional vectors have the same dimension
    as the original token embeddings. The token embeddings are shown with value 1
    for simplicity.
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.18 位置嵌入被添加到标记嵌入向量中，用于创建LLM的输入嵌入。位置向量的维度与原始标记嵌入相同。为简单起见，标记嵌入显示为值1。
- en: '![](images/ch-02__image036.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image036.png)'
- en: Instead of focusing on the absolute position of a token, the emphasis of relative
    positional embeddings is on the relative position or distance between tokens.
    This means the model learns the relationships in terms of "how far apart" rather
    than "at which exact position." The advantage here is that the model can generalize
    better to sequences of varying lengths, even if it hasn't seen such lengths during
    training.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 相对位置嵌入不是关注一个标记的绝对位置，而是关注标记之间的相对位置或距离。这意味着模型学习的是关于“有多远”而不是“在哪个确切位置”。这里的优势在于，即使模型在训练期间没有看到这样的长度，它也能更好地概括不同长度的序列。
- en: Both types of positional embeddings aim to augment the capacity of LLMs to understand
    the order and relationships between tokens, ensuring more accurate and context-aware
    predictions. The choice between them often depends on the specific application
    and the nature of the data being processed.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种位置嵌入的目标都是增强LLM理解标记之间的顺序和关系的能力，确保更准确和能够理解上下文的预测。它们之间的选择通常取决于特定的应用和正在处理的数据的性质。
- en: OpenAI's GPT models use absolute positional embeddings that are optimized during
    the training process rather than being fixed or predefined like the positional
    encodings in the original Transformer model. This optimization process is part
    of the model training itself, which we will implement later in this book. For
    now, let's create the initial positional embeddings to create the LLM inputs for
    the upcoming chapters.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT模型使用的是在训练过程中进行优化的绝对位置嵌入，而不是像原始Transformer模型中的位置编码一样是固定或预定义的。这个优化过程是模型训练本身的一部分，我们稍后会在本书中实现。现在，让我们创建初始位置嵌入以创建即将到来的章节的LLM输入。
- en: 'Previously, we focused on very small embedding sizes in this chapter for illustration
    purposes. We now consider more realistic and useful embedding sizes and encode
    the input tokens into a 256-dimensional vector representation. This is smaller
    than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288
    dimensions) but still reasonable for experimentation. Furthermore, we assume that
    the token IDs were created by the BPE tokenizer that we implemented earlier, which
    has a vocabulary size of 50,257:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们之前专注于非常小的嵌入尺寸以进行举例说明。现在我们考虑更现实和有用的嵌入尺寸，并将输入令牌编码为256维向量表示。这比原始的GPT-3模型使用的要小（在GPT-3中，嵌入尺寸是12,288维），但对于实验仍然是合理的。此外，我们假设令牌ID是由我们先前实现的BPE标记器创建的，其词汇量为50,257：
- en: '[PRE64]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Using the `token_embedding_layer` above, if we sample data from the data loader,
    we embed each token in each batch into a 256-dimensional vector. If we have a
    batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上面的`token_embedding_layer`，如果我们从数据加载器中取样数据，我们将每个批次中的每个令牌嵌入为一个256维的向量。如果我们的批次大小为8，每个有四个令牌，结果将是一个8x4x256的张量。
- en: 'Let''s instantiate the data loader from section 2.6, *Data sampling with a
    sliding window*, first:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从第2.6节“使用滑动窗口进行数据抽样”中实例化数据加载器：
- en: '[PRE65]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The preceding code prints the following output:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码打印如下输出：
- en: '[PRE66]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: As we can see, the token ID tensor is 8x4-dimensional, meaning that the data
    batch consists of 8 text samples with 4 tokens each.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，令牌ID张量是8x4维的，这意味着数据批次由8个文本样本组成，每个样本有4个令牌。
- en: 'Let''s now use the embedding layer to embed these token IDs into 256-dimensional
    vectors:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用嵌入层将这些令牌ID嵌入到256维的向量中：
- en: '[PRE67]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The preceding print function call returns the following:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的打印函数调用返回以下内容：
- en: '[PRE68]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: As we can tell based on the 8x4x256-dimensional tensor output, each token ID
    is now embedded as a 256-dimensional vector.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 根据8x4x256维张量的输出，我们可以看出，现在每个令牌ID都嵌入为一个256维的向量。
- en: 'For a GPT model''s absolute embedding approach, we just need to create another
    embedding layer that has the same dimension as the `token_embedding_layer`:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPT模型的绝对嵌入方法，我们只需要创建另一个具有与`token_embedding_layer`相同维度的嵌入层：
- en: '[PRE69]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: As shown in the preceding code example, the input to the pos_embeddings is usually
    a placeholder vector `torch.arange(block_size)`, which contains a sequence of
    numbers 1, 2, ..., up to the maximum input length. The `block_size` is a variable
    that represents the supported input size of the LLM. Here, we choose it similar
    to the maximum length of the input text. In practice, input text can be longer
    than the supported block size, in which case we have to truncate the text. The
    text can also be shorter than the block size, in which case we fill in the remaining
    input with placeholder tokens to match the block size, as we will see in chapter
    3.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码示例所示，pos_embeddings的输入通常是一个占位符向量`torch.arange(block_size)`，其中包含一个数字序列1、2、…、直到最大输入长度。`block_size`是代表LLM的支持输入尺寸的变量。在这里，我们选择它类似于输入文本的最大长度。在实践中，输入文本可能比支持的块大小更长，在这种情况下，我们必须截断文本。文本还可以比块大小短，在这种情况下，我们填充剩余的输入以匹配块大小的占位符令牌，正如我们将在第3章中看到的。
- en: 'The output of the print statement is as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 打印语句的输出如下所示：
- en: '[PRE70]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'As we can see, the positional embedding tensor consists of four 256-dimensional
    vectors. We can now add these directly to the token embeddings, where PyTorch
    will add the 4x256-dimensional `pos_embeddings` tensor to each 4x256-dimensional
    token embedding tensor in each of the 8 batches:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，位置嵌入张量由四个256维向量组成。我们现在可以直接将它们添加到令牌嵌入中，PyTorch将会将4x256维的`pos_embeddings`张量添加到8个批次中每个4x256维的令牌嵌入张量中：
- en: '[PRE71]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The print output is as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 打印输出如下：
- en: '[PRE72]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The `input_embeddings` we created, as summarized in figure 2.19, are the embedded
    input examples that can now be processed by the main LLM modules, which we will
    begin implementing in chapter 3
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的`input_embeddings`，如图2.19所总结的，是嵌入的输入示例，现在可以被主LLM模块处理，我们将在第3章中开始实施它
- en: Figure 2.19 As part of the input processing pipeline, input text is first broken
    up into individual tokens. These tokens are then converted into token IDs using
    a vocabulary. The token IDs are converted into embedding vectors to which positional
    embeddings of a similar size are added, resulting in input embeddings that are
    used as input for the main LLM layers.
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.19 作为输入处理流程的一部分，输入文本首先被分解为单独的标记。然后这些标记使用词汇表转换为标记ID。标记ID转换为嵌入向量，与类似大小的位置嵌入相加，产生用作主LLM层输入的输入嵌入。
- en: '![](images/ch-02__image038.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-02__image038.png)'
- en: 2.9 Summary
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.9 总结
- en: LLMs require textual data to be converted into numerical vectors, known as embeddings
    since they can't process raw text. Embeddings transform discrete data (like words
    or images) into continuous vector spaces, making them compatible with neural network
    operations.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于LLM不能处理原始文本，所以需要将文本数据转换为数字向量，这些向量被称为嵌入。嵌入将离散数据（如文字或图像）转换为连续的向量空间，使其与神经网络操作兼容。
- en: As the first step, raw text is broken into tokens, which can be words or characters.
    Then, the tokens are converted into integer representations, termed token IDs.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为第一步，原始文本被分解为标记，这些标记可以是单词或字符。然后，这些标记被转换为整数表示，称为标记ID。
- en: Special tokens, such as `<|unk|>` and `<|endoftext|>`, can be added to enhance
    the model's understanding and handle various contexts, such as unknown words or
    marking the boundary between unrelated texts.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特殊标记，比如`<|unk|>`和`<|endoftext|>`，可以增强模型的理解并处理各种上下文，比如未知单词或标记无关文本的边界。
- en: The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3 can
    efficiently handle unknown words by breaking them down into subword units or individual
    characters.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于像GPT-2和GPT-3这样的LLM的字节对编码（BPE）分词器可以通过将未知单词分解为子词单元或单个字符来高效地处理未知单词。
- en: We use a sliding window approach on tokenized data to generate input-target
    pairs for LLM training.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在标记化数据上使用滑动窗口方法生成用于LLM训练的输入-目标对。
- en: Embedding layers in PyTorch function as a lookup operation, retrieving vectors
    corresponding to token IDs. The resulting embedding vectors provide continuous
    representations of tokens, which is crucial for training deep learning models
    like LLMs.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch中的嵌入层作为查找操作，检索与标记ID相对应的向量。结果嵌入向量提供了标记的连续表示，这对于训练像LLM这样的深度学习模型至关重要。
- en: 'While token embeddings provide consistent vector representations for each token,
    they lack a sense of the token''s position in a sequence. To rectify this, two
    main types of positional embeddings exist: absolute and relative. OpenAI''s GPT
    models utilize absolute positional embeddings that are added to the token embedding
    vectors and are optimized during the model training.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然标记嵌入为每个标记提供了一致的向量表示，但它缺乏对标记在序列中位置的感知。为了纠正这一点，存在两种主要类型的位置嵌入：绝对和相对。OpenAI的GPT模型利用绝对位置嵌入，这些嵌入被加到标记嵌入向量中，并在模型训练过程中进行优化。
- en: 2.10 References and further reading
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.10 参考资料和进一步阅读
- en: 'Readers who are interested in discussion and comparison of embedding spaces
    with latent spaces and the general notion of vector representations can find more
    information in the first chapter of my book Machine Learning Q and AI:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 对嵌入空间和潜空间以及向量表达的一般概念感兴趣的读者，可以在我写的书《机器学习 Q 和 AI》的第一章中找到更多信息：
- en: '*Machine Learning Q and AI* (2023) by Sebastian Raschka, [https://leanpub.com/machine-learning-q-and-ai](machine-learning-q-and-ai.html)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习 Q 和 AI* (2023) 由Sebastian Raschka著作，[https://leanpub.com/machine-learning-q-and-ai](machine-learning-q-and-ai.html)'
- en: 'The following paper provides more in-depth discussions of how how byte pair
    encoding is used as a tokenization method:'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下论文更深入地讨论了字节对编码作为分词方法的使用：
- en: Neural Machine Translation of Rare Words with Subword Units (2015) by Sennrich
    at al., [https://arxiv.org/abs/1508.07909](abs.html)
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《稀有词的子词单元神经机器翻译》(2015) 由Sennrich等人编写，[https://arxiv.org/abs/1508.07909](abs.html)
- en: 'The code for the byte pair encoding tokenizer used to train GPT-2 was open-sourced
    by OpenAI:'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练GPT-2的字节对编码分词器的代码已被OpenAI开源：
- en: '[https://github.com/openai/gpt-2/blob/master/src/encoder.py](src.html)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/openai/gpt-2/blob/master/src/encoder.py](src.html)'
- en: 'OpenAI provides an interactive web UI to illustrate how the byte pair tokenizer
    in GPT models works:'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 提供了一个交互式 Web UI，以说明 GPT 模型中的字节对分词器的工作原理：
- en: '[https://platform.openai.com/tokenizer](platform.openai.com.html)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://platform.openai.com/tokenizer](platform.openai.com.html)'
- en: 'Readers who are interested in studying alternative tokenization schemes that
    are used by some other popular LLMs can find more information in the SentencePiece
    and WordPiece papers:'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于对研究其他流行 LLMs 使用的替代分词方案感兴趣的读者，可以在 SentencePiece 和 WordPiece 论文中找到更多信息：
- en: 'SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer
    for Neural Text Processing (2018) by Kudo and Richardson, [https://aclanthology.org/D18-2012/](D18-2012.html)'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SentencePiece：一种简单且语言无关的子词分词器和去分词器，用于神经文本处理（2018），作者 Kudo 和 Richardson，[https://aclanthology.org/D18-2012/](D18-2012.html)
- en: Fast WordPiece Tokenization (2020) by Song et al., [https://arxiv.org/abs/2012.15524](abs.html)
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速 WordPiece 分词（2020），作者 Song 等人，[https://arxiv.org/abs/2012.15524](abs.html)
- en: 2.11 Exercise answers
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.11 练习答案
- en: The complete code examples for the exercises answers can be found in the supplementary
    GitHub repository at [https://github.com/rasbt/LLMs-from-scratch](rasbt.html)
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 练习答案的完整代码示例可以在补充的 GitHub 仓库中找到：[https://github.com/rasbt/LLMs-from-scratch](rasbt.html)
- en: Exercise 2.1
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.1
- en: 'You can obtain the individual token IDs by prompting the encoder with one string
    at a time:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过一个字符串逐个提示编码器来获得单个标记 ID：
- en: '[PRE73]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'This prints:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE74]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'You can then use the following code to assemble the original string:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用以下代码来组装原始字符串：
- en: '[PRE75]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'This returns:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回：
- en: '[PRE76]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
