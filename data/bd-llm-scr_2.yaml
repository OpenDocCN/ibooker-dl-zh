- en: 2 Working with Text Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 处理文本数据
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Preparing text for large language model training
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为大型语言模型训练准备文本
- en: Splitting text into word and subword tokens
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本拆分为单词和子词标记
- en: Byte pair encoding as a more advanced way of tokenizing text
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字节对编码作为一种更高级的文本标记方式
- en: Sampling training examples with a sliding window approach
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用滑动窗口方法采样训练示例
- en: Converting tokens into vectors that feed into a large language model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标记转换为输入大型语言模型的向量
- en: In the previous chapter, we delved into the general structure of large language
    models (LLMs) and learned that they are pretrained on vast amounts of text. Specifically,
    our focus was on decoder-only LLMs based on the transformer architecture, which
    underlies the models used in ChatGPT and other popular GPT-like LLMs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们深入探讨了大型语言模型（LLM）的一般结构，并了解到它们是在大量文本上进行预训练的。具体来说，我们关注的是基于变换器架构的仅解码器LLM，这构成了在ChatGPT和其他流行的GPT类LLM中使用的模型。
- en: During the pretraining stage, LLMs process text one word at a time. Training
    LLMs with millions to billions of parameters using a next-word prediction task
    yields models with impressive capabilities. These models can then be further finetuned
    to follow general instructions or perform specific target tasks. But before we
    can implement and train LLMs in the upcoming chapters, we need to prepare the
    training dataset, which is the focus of this chapter, as illustrated in Figure
    2.1
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练阶段，LLM一次处理一个单词。使用下一词预测任务训练具有数百万到数十亿参数的LLM，产生了具有卓越能力的模型。这些模型可以进一步微调，以遵循一般指令或执行特定目标任务。但是在我们能够在接下来的章节中实现和训练LLM之前，我们需要准备训练数据集，这是本章的重点，如图2.1所示。
- en: Figure 2.1 A mental model of the three main stages of coding an LLM, pretraining
    the LLM on a general text dataset, and finetuning it on a labeled dataset. This
    chapter will explain and code the data preparation and sampling pipeline that
    provides the LLM with the text data for pretraining.
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 LLM编码的三个主要阶段的心理模型，包括在一般文本数据集上进行LLM的预训练，以及在标记数据集上进行微调。本章将解释并编码数据准备和采样管道，为LLM提供预训练所需的文本数据。
- en: '![](images/02__image001.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image001.png)'
- en: In this chapter, you'll learn how to prepare input text for training LLMs. This
    involves splitting text into individual word and subword tokens, which can then
    be encoded into vector representations for the LLM. You'll also learn about advanced
    tokenization schemes like byte pair encoding, which is utilized in popular LLMs
    like GPT. Lastly, we'll implement a sampling and data loading strategy to produce
    the input-output pairs necessary for training LLMs in subsequent chapters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何为训练LLM准备输入文本。这涉及将文本拆分为单个单词和子词标记，然后可以将其编码为LLM的向量表示。你还将学习像字节对编码这样的高级标记方案，它在流行的LLM如GPT中被利用。最后，我们将实现一种采样和数据加载策略，以生成后续章节中训练LLM所需的输入输出对。
- en: 2.1 Understanding word embeddings
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 理解词嵌入
- en: Deep neural network models, including LLMs, cannot process raw text directly.
    Since text is categorical, it isn't compatible with the mathematical operations
    used to implement and train neural networks. Therefore, we need a way to represent
    words as continuous-valued vectors. (Readers unfamiliar with vectors and tensors
    in a computational context can learn more in Appendix A, section A2.2 Understanding
    tensors.)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络模型，包括LLM，不能直接处理原始文本。由于文本是分类的，无法与用于实现和训练神经网络的数学运算兼容。因此，我们需要一种将词语表示为连续值向量的方法。（不熟悉计算上下文中的向量和张量的读者可以在附录A，A2.2节“理解张量”中了解更多信息。）
- en: The concept of converting data into a vector format is often referred to as
    *embedding*. Using a specific neural network layer or another pretrained neural
    network model, we can embed different data types, for example, video, audio, and
    text, as illustrated in Figure 2.2.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据转换为向量格式的概念通常被称为*嵌入*。使用特定的神经网络层或其他预训练的神经网络模型，我们可以嵌入不同类型的数据，例如视频、音频和文本，如图2.2所示。
- en: Figure 2.2 Deep learning models cannot process data formats like video, audio,
    and text in their raw form. Thus, we use an embedding model to transform this
    raw data into a dense vector representation that deep learning architectures can
    easily understand and process. Specifically, this figure illustrates the process
    of converting raw data into a three-dimensional numerical vector.
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 深度学习模型无法处理视频、音频和文本等原始数据格式。因此，我们使用嵌入模型将这些原始数据转换为密集的向量表示，以便深度学习架构能够轻松理解和处理。具体来说，这张图说明了将原始数据转换为三维数值向量的过程。
- en: '![](images/02__image003.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image003.png)'
- en: As shown in Figure 2.2, we can process various different data formats via embedding
    models. However, it's important to note that different data formats require distinct
    embedding models. For example, an embedding model designed for text would not
    be suitable for embedding audio or video data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如图2.2所示，我们可以通过嵌入模型处理各种不同的数据格式。然而，重要的是要注意，不同的数据格式需要不同的嵌入模型。例如，专为文本设计的嵌入模型并不适合嵌入音频或视频数据。
- en: At its core, an embedding is a mapping from discrete objects, such as words,
    images, or even entire documents, to points in a continuous vector space -- the
    primary purpose of embeddings is to convert non-numeric data into a format that
    neural networks can process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的核心是将离散对象（如单词、图像或整个文档）映射到连续向量空间中的点——嵌入的主要目的是将非数字数据转换为神经网络可以处理的格式。
- en: While word embeddings are the most common form of text embedding, there are
    also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph
    embeddings are popular choices for *retrieval-augmented generation.* Retrieval-augmented
    generation combines generation (like producing text) with retrieval (like searching
    an external knowledge base) to pull relevant information when generating text,
    which is a technique that is beyond the scope of this book. Since our goal is
    to train GPT-like LLMs, which learn to generate text one word at a time, this
    chapter focuses on word embeddings.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词嵌入是文本嵌入中最常见的形式，但也存在句子、段落或整篇文档的嵌入。句子或段落嵌入是*检索增强生成*的热门选择。检索增强生成结合了生成（如生成文本）与检索（如搜索外部知识库），在生成文本时提取相关信息，这是一种超出本书范围的技术。由于我们的目标是训练像GPT这样的LLM，这些模型学习逐字生成文本，本章重点讨论词嵌入。
- en: There are several algorithms and frameworks that have been developed to generate
    word embeddings. One of the earlier and most popular examples is the *Word2Vec*
    approach. Word2Vec trained neural network architecture to generate word embeddings
    by predicting the context of a word given the target word or vice versa. The main
    idea behind Word2Vec is that words that appear in similar contexts tend to have
    similar meanings. Consequently, when projected into 2-dimensional word embeddings
    for visualization purposes, it can be seen that similar terms cluster together,
    as shown in Figure 2.3.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种算法和框架被开发出来用于生成词嵌入。其中一个较早且最受欢迎的例子是*Word2Vec*方法。Word2Vec训练神经网络架构，通过预测给定目标单词的上下文来生成词嵌入，反之亦然。Word2Vec背后的主要思想是，出现在相似上下文中的单词往往具有相似的含义。因此，当投影到二维词嵌入以便于可视化时，可以看到相似的术语聚集在一起，如图2.3所示。
- en: Figure 2.3 If word embeddings are two-dimensional, we can plot them in a two-dimensional
    scatterplot for visualization purposes as shown here. When using word embedding
    techniques, such as Word2Vec, words corresponding to similar concepts often appear
    close to each other in the embedding space. For instance, different types of birds
    appear closer to each other in the embedding space compared to countries and cities.
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3 如果词嵌入是二维的，我们可以将它们绘制在二维散点图中以便于可视化，如此处所示。在使用词嵌入技术（例如Word2Vec）时，对应于相似概念的单词往往在嵌入空间中彼此靠近。例如，不同种类的鸟在嵌入空间中相较于国家和城市更靠近。
- en: '![](images/02__image005.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image005.png)'
- en: Word embeddings can have varying dimensions, from one to thousands. As shown
    in Figure 2.3, we can choose two-dimensional word embeddings for visualization
    purposes. A higher dimensionality might capture more nuanced relationships but
    at the cost of computational efficiency.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入可以具有不同的维度，从一维到数千维。如图2.3所示，我们可以选择二维词嵌入用于可视化目的。更高的维度可能会捕捉到更细微的关系，但代价是计算效率下降。
- en: While we can use pretrained models such as Word2Vec to generate embeddings for
    machine learning models, LLMs commonly produce their own embeddings that are part
    of the input layer and are updated during training. The advantage of optimizing
    the embeddings as part of the LLM training instead of using Word2Vec is that the
    embeddings are optimized to the specific task and data at hand. We will implement
    such embedding layers later in this chapter. Furthermore, LLMs can also create
    contextualized output embeddings, as we discuss in chapter 3.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以使用预训练模型如 Word2Vec 为机器学习模型生成嵌入，但 LLM 通常生成自己的嵌入，这些嵌入是输入层的一部分，并在训练过程中更新。将嵌入优化为
    LLM 训练的一部分而不是使用 Word2Vec 的优势在于，这些嵌入会针对具体的任务和数据进行优化。我们将在本章稍后实现这样的嵌入层。此外，LLM 还可以创建上下文相关的输出嵌入，正如我们在第三章中讨论的。
- en: Unfortunately, high-dimensional embeddings present a challenge for visualization
    because our sensory perception and common graphical representations are inherently
    limited to three dimensions or fewer, which is why Figure 2.3 showed two-dimensional
    embeddings in a two-dimensional scatterplot. However, when working with LLMs,
    we typically use embeddings with a much higher dimensionality than shown in Figure
    2.3\. For both GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality
    of the model's hidden states) varies based on the specific model variant and size.
    It is a trade-off between performance and efficiency. The smallest GPT-2 models
    (117M and 125M parameters) use an embedding size of 768 dimensions to provide
    concrete examples. The largest GPT-3 model (175B parameters) uses an embedding
    size of 12,288 dimensions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，高维嵌入在可视化方面存在挑战，因为我们的感官感知和常见的图形表示本质上限于三维或更少，这就是图 2.3 显示二维散点图的原因。然而，在处理 LLM
    时，我们通常使用的嵌入维度远高于图 2.3 所示的维度。对于 GPT-2 和 GPT-3，嵌入大小（通常称为模型隐藏状态的维度）会根据具体的模型变体和大小而有所不同。这是性能与效率之间的权衡。最小的
    GPT-2 模型（117M 和 125M 参数）使用 768 维的嵌入大小以提供具体示例。最大的 GPT-3 模型（175B 参数）使用 12,288 维的嵌入大小。
- en: The upcoming sections in this chapter will walk through the required steps for
    preparing the embeddings used by an LLM, which include splitting text into words,
    converting words into tokens, and turning tokens into embedding vectors.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本章接下来的部分将逐步介绍准备 LLM 使用的嵌入所需的步骤，包括将文本分割为单词、将单词转换为标记，以及将标记转换为嵌入向量。
- en: 2.2 Tokenizing text
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 文本标记化
- en: This section covers how we split input text into individual tokens, a required
    preprocessing step for creating embeddings for an LLM. These tokens are either
    individual words or special characters, including punctuation characters, as shown
    in Figure 2.4.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了如何将输入文本分割成单个标记，这是为 LLM 创建嵌入所需的预处理步骤。这些标记可以是单独的单词或特殊字符，包括标点符号，如图 2.4 所示。
- en: Figure 2.4 A view of the text processing steps covered in this section in the
    context of an LLM. Here, we split an input text into individual tokens, which
    are either words or special characters, such as punctuation characters. In upcoming
    sections, we will convert the text into token IDs and create token embeddings.
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.4 在 LLM 背景下展示了本节所涉及的文本处理步骤。在这里，我们将输入文本分割成单个标记，这些标记可以是单词或特殊字符，例如标点符号。在接下来的章节中，我们将把文本转换为标记
    ID 并创建标记嵌入。
- en: '![](images/02__image007.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image007.png)'
- en: 'The text we will tokenize for LLM training is a short story by Edith Wharton
    called *The Verdict*, which has been released into the public domain and is thus
    permitted to be used for LLM training tasks. The text is available on Wikisource
    at [https://en.wikisource.org/wiki/The_Verdict](wiki.html), and you can copy and
    paste it into a text file, which I copied into a text file `"the-verdict.txt"`
    to load using Python''s standard file reading utilities:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为 LLM 训练标记化的文本是一篇由伊迪丝·沃顿创作的短故事，名为 *判决*，该故事已进入公有领域，因此可以用于 LLM 训练任务。文本可以在维基文库找到，链接为
    [https://en.wikisource.org/wiki/The_Verdict](wiki.html)，你可以将其复制并粘贴到文本文件中，我将其复制到文本文件
    `"the-verdict.txt"` 中，以便使用 Python 的标准文件读取工具加载：
- en: Listing 2.1 Reading in a short story as text sample into Python
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.1 将短故事作为文本样本读取到 Python 中
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Alternatively, you can find this "`the-verdict.txt"` file in this book's GitHub
    repository at [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/01_main-chapter-code](ch02.html).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以在本书的 GitHub 仓库中找到这个 "`the-verdict.txt`" 文件，链接为 [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/01_main-chapter-code](ch02.html)。
- en: 'The print command prints the total number of characters followed by the first
    100 characters of this file for illustration purposes:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 打印命令打印该文件的字符总数，接着是前 100 个字符以供说明：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our goal is to tokenize this 20,479-character short story into individual words
    and special characters that we can then turn into embeddings for LLM training
    in the upcoming chapters.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将这个 20,479 字符的短篇故事分词成单独的单词和特殊字符，然后在即将到来的章节中将其转换为 LLM 训练的嵌入。
- en: Text sample sizes
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 文本样本大小
- en: Note that it's common to process millions of articles and hundreds of thousands
    of books -- many gigabytes of text -- when working with LLMs. However, for educational
    purposes, it's sufficient to work with smaller text samples like a single book
    to illustrate the main ideas behind the text processing steps and to make it possible
    to run it in reasonable time on consumer hardware.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在处理 LLM 时，通常会处理数百万篇文章和数十万本书籍——数十亿字节的文本。然而，出于教育目的，使用像单本书这样的小文本样本来说明文本处理步骤背后的主要思想是足够的，并且可以在消费硬件上合理时间内运行。
- en: How can we best split this text to obtain a list of tokens? For this, we go
    on a small excursion and use Python's regular expression library `re` for illustration
    purposes. (Note that you don't have to learn or memorize any regular expression
    syntax since we will transition to a pre-built tokenizer later in this chapter.)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何才能最好地分割这段文本以获得一个令牌列表？为此，我们进行一次小的探险，使用 Python 的正则表达式库 `re` 进行说明。（请注意，您不必学习或记住任何正则表达式语法，因为我们将在本章稍后过渡到一个预构建的分词器。）
- en: 'Using some simple example text, we can use the `re.split` command with the
    following syntax to split a text on whitespace characters:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一些简单的示例文本，我们可以使用 `re.split` 命令，采用以下语法按空白字符分割文本：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result is a list of individual words, whitespaces, and punctuation characters:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个由单独的单词、空格和标点符号组成的列表：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the simple tokenization scheme above mostly works for separating the
    example text into individual words, however, some words are still connected to
    punctuation characters that we want to have as separate list entries. We also
    refrain from making all text lowercase because capitalization helps LLMs distinguish
    between proper nouns and common nouns, understand sentence structure, and learn
    to generate text with proper capitalization.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述简单的分词方案主要用于将示例文本分隔成单个单词，然而，一些单词仍然与我们希望作为单独列表项的标点符号相连。我们还避免将所有文本转换为小写，因为大写有助于
    LLM 区分专有名词和普通名词，理解句子结构，并学习生成适当大写的文本。
- en: 'Let''s modify the regular expression splits on whitespaces (`\s`) and commas,
    and periods (`[,.]`):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改正则表达式，以空格（`\s`）、逗号和句号（`[,.]`）为分隔符：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can see that the words and punctuation characters are now separate list
    entries just as we wanted:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，单词和标点符号现在是分开的列表项，正如我们所希望的那样：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A small remaining issue is that the list still includes whitespace characters.
    Optionally, we can remove these redundant characters safely as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小问题是，列表中仍然包含空白字符。可选地，我们可以安全地去除这些冗余字符，如下所示：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The resulting whitespace-free output looks like as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是没有空格的输出如下所示：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Removing whitespaces or not
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 是否删除空格
- en: When developing a simple tokenizer, whether we should encode whitespaces as
    separate characters or just remove them depends on our application and its requirements.
    Removing whitespaces reduces the memory and computing requirements. However, keeping
    whitespaces can be useful if we train models that are sensitive to the exact structure
    of the text (for example, Python code, which is sensitive to indentation and spacing).
    Here, we remove whitespaces for simplicity and brevity of the tokenized outputs.
    Later, we will switch to a tokenization scheme that includes whitespaces.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发一个简单的分词器时，我们是否应该将空格编码为单独的字符，或者只是将其删除，取决于我们的应用程序及其需求。删除空格可以减少内存和计算需求。然而，如果我们训练的模型对文本的确切结构敏感（例如，Python
    代码对缩进和空格敏感），保留空格会很有用。在这里，我们为了简单和简洁性而删除空格。稍后，我们将切换到一个包含空格的分词方案。
- en: 'The tokenization scheme we devised above works well on the simple sample text.
    Let''s modify it a bit further so that it can also handle other types of punctuation,
    such as question marks, quotation marks, and the double-dashes we have seen earlier
    in the first 100 characters of Edith Wharton''s short story, along with additional
    special characters:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计的标记化方案在简单的样本文本上效果良好。让我们进一步修改它，以便它也能处理其他类型的标点符号，如问号、引号以及我们在艾迪丝·华顿短篇小说前100个字符中看到的双破折号，还有其他特殊字符：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The resulting output is as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出如下：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we can see based on the results summarized in Figure 2.5, our tokenization
    scheme can now handle the various special characters in the text successfully.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从图2.5中总结的结果来看，我们的标记化方案现在能够成功处理文本中的各种特殊字符。
- en: Figure 2.5 The tokenization scheme we implemented so far splits text into individual
    words and punctuation characters. In the specific example shown in this figure,
    the sample text gets split into 10 individual tokens.
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 我们迄今实施的标记化方案将文本拆分为单个单词和标点符号。在此图中显示的具体示例中，样本文本被拆分为10个单独的标记。
- en: '![](images/02__image009.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image009.png)'
- en: 'Now that we got a basic tokenizer working, let''s apply it to Edith Wharton''s
    entire short story:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个基本的标记器，让我们将其应用于艾迪丝·华顿的整个短篇小说：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The above print statement outputs `4649`, which is the number of tokens in this
    text (without whitespaces).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上述打印语句输出`4649`，这是该文本中的标记数量（不包括空格）。
- en: 'Let''s print the first 30 tokens for a quick visual check:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印前30个标记以进行快速视觉检查：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The resulting output shows that our tokenizer appears to be handling the text
    well since all words and special characters are neatly separated:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出显示我们的标记器似乎能够很好地处理文本，因为所有单词和特殊字符都被整齐分开：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 2.3 Converting tokens into token IDs
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 将标记转换为标记ID
- en: In the previous section, we tokenized a short story by Edith Wharton into individual
    tokens. In this section, we will convert these tokens from a Python string to
    an integer representation to produce the so-called token IDs. This conversion
    is an intermediate step before converting the token IDs into embedding vectors.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们将艾迪丝·华顿的一篇短篇小说标记化为单个标记。在本节中，我们将把这些标记从Python字符串转换为整数表示形式，以生成所谓的标记ID。这一转换是将标记ID转换为嵌入向量之前的一个中间步骤。
- en: To map the previously generated tokens into token IDs, we have to build a so-called
    vocabulary first. This vocabulary defines how we map each unique word and special
    character to a unique integer, as shown in Figure 2.6.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将先前生成的标记映射到标记ID，我们必须首先构建一个所谓的词汇表。这个词汇表定义了我们如何将每个唯一单词和特殊字符映射到一个唯一的整数，如图2.6所示。
- en: Figure 2.6 We build a vocabulary by tokenizing the entire text in a training
    dataset into individual tokens. These individual tokens are then sorted alphabetically,
    and duplicate tokens are removed. The unique tokens are then aggregated into a
    vocabulary that defines a mapping from each unique token to a unique integer value.
    The depicted vocabulary is purposefully small for illustration purposes and contains
    no punctuation or special characters for simplicity.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 我们通过将训练数据集中的整个文本标记化为单个标记来构建词汇表。这些单个标记随后按字母顺序排序，重复标记被移除。唯一标记被汇总成一个定义从每个唯一标记到唯一整数值映射的词汇表。所示的词汇表故意较小以作示例，并且为简单起见不包含标点符号或特殊字符。
- en: '![](images/02__image011.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image011.png)'
- en: 'In the previous section, we tokenized Edith Wharton''s short story and assigned
    it to a Python variable called `preprocessed`. Let''s now create a list of all
    unique tokens and sort them alphabetically to determine the vocabulary size:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们将艾迪丝·华顿的短篇小说进行了标记化，并将其分配给一个名为`preprocessed`的Python变量。现在让我们创建一个所有唯一标记的列表，并按字母顺序对其进行排序，以确定词汇量：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After determining that the vocabulary size is 1,159 via the above code, we
    create the vocabulary and print its first 50 entries for illustration purposes:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过上述代码确定词汇量为1,159后，我们创建词汇表并打印其前50个条目以作示例：
- en: Listing 2.2 Creating a vocabulary
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.2 创建词汇表
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As we can see, based on the output above, the dictionary contains individual
    tokens associated with unique integer labels. Our next goal is to apply this vocabulary
    to convert new text into token IDs, as illustrated in Figure 2.7.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的输出可以看出，字典包含与唯一整数标签相关的单独标记。我们的下一个目标是将这个词汇表应用于将新文本转换为标记ID，如图2.7所示。
- en: Figure 2.7 Starting with a new text sample, we tokenize the text and use the
    vocabulary to convert the text tokens into token IDs. The vocabulary is built
    from the entire training set and can be applied to the training set itself and
    any new text samples. The depicted vocabulary contains no punctuation or special
    characters for simplicity.
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.7 从一个新的文本样本开始，我们对文本进行分词，并利用词汇将文本标记转换为标记 ID。词汇是从整个训练集中构建的，可以应用于训练集本身以及任何新的文本样本。为了简单起见，所示词汇不包含标点符号或特殊字符。
- en: '![](images/02__image013.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image013.png)'
- en: Later in this book, when we want to convert the outputs of an LLM from numbers
    back into text, we also need a way to turn token IDs into text. For this, we can
    create an inverse version of the vocabulary that maps token IDs back to corresponding
    text tokens.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书后面，当我们想要将 LLM 的输出从数字转换回文本时，我们还需要一种方法将标记 ID 转换为文本。为此，我们可以创建一个词汇的逆版本，将标记 ID
    映射回对应的文本标记。
- en: Let's implement a complete tokenizer class in Python with an `encode` method
    that splits text into tokens and carries out the string-to-integer mapping to
    produce token IDs via the vocabulary. In addition, we implement a `decode` method
    that carries out the reverse integer-to-string mapping to convert the token IDs
    back into text.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Python 中实现一个完整的分词器类，包含一个 `encode` 方法，该方法将文本分割为标记，并通过词汇进行字符串到整数的映射以生成标记
    ID。此外，我们实现一个 `decode` 方法，进行反向整数到字符串的映射，将标记 ID 转换回文本。
- en: 'The code for this tokenizer implementation is as in listing 2.3:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该分词器实现的代码如清单 2.3 所示：
- en: Listing 2.3 Implementing a simple text tokenizer
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 2.3 实现一个简单的文本分词器
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Using the `SimpleTokenizerV1` Python class above, we can now instantiate new
    tokenizer objects via an existing vocabulary, which we can then use to encode
    and decode text, as illustrated in Figure 2.8.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述的 `SimpleTokenizerV1` Python 类，我们现在可以通过现有的词汇实例化新的分词器对象，然后利用这些对象对文本进行编码和解码，如图
    2.8 所示。
- en: 'Figure 2.8 Tokenizer implementations share two common methods: an encode method
    and a decode method. The encode method takes in the sample text, splits it into
    individual tokens, and converts the tokens into token IDs via the vocabulary.
    The decode method takes in token IDs, converts them back into text tokens, and
    concatenates the text tokens into natural text.'
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.8 分词器实现共享两个公共方法：一个编码方法和一个解码方法。编码方法接收样本文本，将其拆分为单个标记，并通过词汇将标记转换为标记 ID。解码方法接收标记
    ID，将其转换回文本标记，并将文本标记连接成自然文本。
- en: '![](images/02__image015.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image015.png)'
- en: 'Let''s instantiate a new tokenizer object from the `SimpleTokenizerV1` class
    and tokenize a passage from Edith Wharton''s short story to try it out in practice:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 `SimpleTokenizerV1` 类实例化一个新的分词器对象，并对埃迪丝·华顿短篇小说中的一段进行分词，以便实际测试：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The code above prints the following token IDs:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码打印以下标记 ID：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, let''s see if we can turn these token IDs back into text using the decode
    method:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看是否可以使用解码方法将这些标记 ID 转换回文本：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This outputs the following text:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下文本：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Based on the output above, we can see that the decode method successfully converted
    the token IDs back into the original text.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述输出，我们可以看到解码方法成功地将标记 ID 转换回原始文本。
- en: 'So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing
    text based on a snippet from the training set. Let''s now apply it to a new text
    sample that is not contained in the training set:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。我们实现了一个能够根据训练集的片段对文本进行分词和去分词的分词器。现在，让我们将其应用于一个不包含在训练集中的新文本样本：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Executing the code above will result in the following error:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码将导致以下错误：
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The problem is that the word "Hello" was not used in the *The Verdict* short
    story. Hence, it is not contained in the vocabulary. This highlights the need
    to consider large and diverse training sets to extend the vocabulary when working
    on LLMs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，单词 "Hello" 并未出现在 *判决* 短篇小说中。因此，它不包含在词汇中。这凸显了在处理 LLM 时需要考虑大型和多样化的训练集以扩展词汇的必要性。
- en: In the next section, we will test the tokenizer further on text that contains
    unknown words, and we will also discuss additional special tokens that can be
    used to provide further context for an LLM during training.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将进一步测试该分词器，处理包含未知词汇的文本，并讨论可以用于在训练期间为 LLM 提供进一步上下文的额外特殊标记。
- en: 2.4 Adding special context tokens
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 添加特殊上下文标记
- en: In the previous section, we implemented a simple tokenizer and applied it to
    a passage from the training set. In this section, we will modify this tokenizer
    to handle unknown words.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们实现了一个简单的分词器并将其应用于训练集中的一段文本。在这一节中，我们将修改这个分词器以处理未知单词。
- en: We will also discuss the usage and addition of special context tokens that can
    enhance a model's understanding of context or other relevant information in the
    text. These special tokens can include markers for unknown words and document
    boundaries, for example.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将讨论特殊上下文标记的使用和添加，这些标记可以增强模型对上下文或文本中其他相关信息的理解。这些特殊标记可以包括未知单词和文档边界的标记，例如。
- en: In particular, we will modify the vocabulary and tokenizer we implemented in
    the previous section, `SimpleTokenizerV2`, to support two new tokens, `<|unk|>`
    and `<|endoftext|>`, as illustrated in Figure 2.9.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们将修改在上一节中实现的词汇和分词器`SimpleTokenizerV2`，以支持两个新标记`<|unk|>`和`<|endoftext|>`，如图2.9所示。
- en: Figure 2.9 We add special tokens to a vocabulary to deal with certain contexts.
    For instance, we add an <|unk|> token to represent new and unknown words that
    were not part of the training data and thus not part of the existing vocabulary.
    Furthermore, we add an <|endoftext|> token that we can use to separate two unrelated
    text sources.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9 我们向词汇中添加特殊标记以处理某些上下文。例如，我们添加一个`<|unk|>`标记以表示训练数据中不存在的新单词，因此不属于现有词汇。此外，我们添加一个`<|endoftext|>`标记，用于分隔两个无关的文本来源。
- en: '![](images/02__image017.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image017.png)'
- en: As shown in Figure 2.9, we can modify the tokenizer to use an `<|unk|>` token
    if it encounters a word that is not part of the vocabulary. Furthermore, we add
    a token between unrelated texts. For example, when training GPT-like LLMs on multiple
    independent documents or books, it is common to insert a token before each document
    or book that follows a previous text source, as illustrated in Figure 2.10\. This
    helps the LLM understand that, although these text sources are concatenated for
    training, they are, in fact, unrelated.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如图2.9所示，当分词器遇到不属于词汇表的单词时，我们可以修改分词器以使用`<|unk|>`标记。此外，我们在不相关文本之间添加一个标记。例如，在对多个独立文档或书籍进行GPT类LLM训练时，通常会在每个文档或书籍前插入一个标记，以表明它是跟随之前文本来源的，如图2.10所示。这有助于LLM理解，尽管这些文本来源是为了训练而连接在一起的，但实际上是无关的。
- en: Figure 2.10 When working with multiple independent text source, we add <|endoftext|>
    tokens between these texts. These <|endoftext|> tokens act as markers, signaling
    the start or end of a particular segment, allowing for more effective processing
    and understanding by the LLM.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10 在处理多个独立文本来源时，我们在这些文本之间添加`<|endoftext|>`标记。这些`<|endoftext|>`标记作为标记，信号特定段落的开始或结束，使LLM能够更有效地处理和理解。
- en: '![](images/02__image019.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image019.png)'
- en: 'Let''s now modify the vocabulary to include these two special tokens, `<unk>`
    and `<|endoftext|>`, by adding these to the list of all unique words that we created
    in the previous section:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们修改词汇，以包括这两个特殊标记`<unk>`和`<|endoftext|>`，通过将这些标记添加到我们在上一节创建的所有唯一单词列表中：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Based on the output of the print statement above, the new vocabulary size is
    1161 (the vocabulary size in the previous section was 1159).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上面打印语句的输出，新词汇的大小为1161（上一节的词汇大小为1159）。
- en: 'As an additional quick check, let''s print the last 5 entries of the updated
    vocabulary:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外的快速检查，让我们打印更新词汇的最后5个条目：
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The code above prints the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码打印出以下内容：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Based on the code output above, we can confirm that the two new special tokens
    were indeed successfully incorporated into the vocabulary. Next, we adjust the
    tokenizer from code listing 2.3 accordingly, as shown in listing 2.4:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上面代码的输出，我们可以确认这两个新的特殊标记确实成功地纳入了词汇中。接下来，我们相应地调整代码清单2.3中的分词器，如清单2.4所示：
- en: Listing 2.4 A simple text tokenizer that handles unknown words
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.4 一个处理未知单词的简单文本分词器
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Compared to the `SimpleTokenizerV1` we implemented in code listing 2.3 in the
    previous section, the new `SimpleTokenizerV2` replaces unknown words by `<|unk|>`
    tokens.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一节代码清单2.3中实现的`SimpleTokenizerV1`相比，新`SimpleTokenizerV2`用`<|unk|>`标记替换了未知单词。
- en: 'Let''s now try this new tokenizer out in practice. For this, we will use a
    simple text sample that we concatenate from two independent and unrelated sentences:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在实践中尝试这个新的分词器。为此，我们将使用一个简单的文本样本，由两个独立且无关的句子拼接而成：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, let''s tokenize the sample text using the `SimpleTokenizerV2` on the
    vocab we previously created in listing 2.2:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用之前在列表2.2中创建的词汇表，使用`SimpleTokenizerV2`对示例文本进行分词：
- en: '[PRE29]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This prints the following token IDs:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下标记ID：
- en: '[PRE30]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Above, we can see that the list of token IDs contains 1159 for the <|endoftext|>
    separator token as well as two 1160 tokens, which are used for unknown words.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面，我们可以看到标记ID列表包含1159作为`<|endoftext|>`分隔标记以及两个1160标记，用于未知单词。
- en: 'Let''s de-tokenize the text for a quick sanity check:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行去标记化文本的快速合理性检查：
- en: '[PRE31]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE32]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Based on comparing the de-tokenized text above with the original input text,
    we know that the training dataset, Edith Wharton's short story *The Verdict*,
    did not contain the words "Hello" and "palace."
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 基于将上述去标记化文本与原始输入文本进行比较，我们知道训练数据集《伊迪丝·华顿的短篇小说*裁决*》中没有包含“Hello”和“palace”这两个词。
- en: 'So far, we have discussed tokenization as an essential step in processing text
    as input to LLMs. Depending on the LLM, some researchers also consider additional
    special tokens such as the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了分词化作为处理文本以输入LLM的重要步骤。根据不同的LLM，一些研究人员还考虑了其他特殊标记，例如：
- en: '`[BOS]` (beginning of sequence): This token marks the start of a text. It signifies
    to the LLM where a piece of content begins.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[BOS]`（序列开始）：此标记表示文本的开始。它向大型语言模型（LLM）指示内容的开始位置。'
- en: '`[EOS]` (end of sequence): This token is positioned at the end of a text, and
    is especially useful when concatenating multiple unrelated texts, similar to `<|endoftext|>`.
    For instance, when combining two different Wikipedia articles or books, the `[EOS]`
    token indicates where one article ends and the next one begins.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[EOS]`（序列结束）：此标记位于文本的末尾，尤其在连接多个不相关文本时非常有用，类似于`<|endoftext|>`。例如，当组合两篇不同的维基百科文章或书籍时，`[EOS]`标记指示一篇文章结束的地方和下一篇文章开始的地方。'
- en: '`[PAD]` (padding): When training LLMs with batch sizes larger than one, the
    batch might contain texts of varying lengths. To ensure all texts have the same
    length, the shorter texts are extended or "padded" using the `[PAD]` token, up
    to the length of the longest text in the batch.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[PAD]`（填充）：在使用大于一个的批量大小训练LLM时，批量可能包含长度不同的文本。为了确保所有文本具有相同的长度，较短的文本会使用`[PAD]`标记进行扩展或“填充”，直到达到批量中最长文本的长度。'
- en: Note that the tokenizer used for GPT models does not need any of these tokens
    mentioned above but only uses an `<|endoftext|>` token for simplicity. The `<|endoftext|>`
    is analogous to the `[EOS]` token mentioned above. Also, `<|endoftext|>` is used
    for padding as well. However, as we'll explore in subsequent chapters when training
    on batched inputs, we typically use a mask, meaning we don't attend to padded
    tokens. Thus, the specific token chosen for padding becomes inconsequential.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，用于GPT模型的分词器不需要上述提到的任何标记，而只使用一个`<|endoftext|>`标记以简化操作。`<|endoftext|>`与上面提到的`[EOS]`标记类似。同时，`<|endoftext|>`也用于填充。然而，正如我们将在后面的章节中探讨的那样，在批量输入的训练中，我们通常使用掩码，这意味着我们不关注填充标记。因此，选择用于填充的特定标记变得无关紧要。
- en: Moreover, the tokenizer used for GPT models also doesn't use an `<|unk|>` token
    for out-of-vocabulary words. Instead, GPT models use a *byte pair encoding* tokenizer,
    which breaks down words into subword units, which we will discuss in the next
    section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GPT模型使用的分词器也不使用`<|unk|>`标记来处理超出词汇表的单词。相反，GPT模型使用*字节对编码*分词器，将单词拆分为子词单元，关于这一点我们将在下一节讨论。
- en: 2.5 Byte pair encoding
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 字节对编码
- en: We implemented a simple tokenization scheme in the previous sections for illustration
    purposes. This section covers a more sophisticated tokenization scheme based on
    a concept called byte pair encoding (BPE). The BPE tokenizer covered in this section
    was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的部分实施了一个简单的分词方案以作说明。本节涵盖了一种基于字节对编码（BPE）概念的更复杂的分词方案。本节中介绍的BPE分词器用于训练如GPT-2、GPT-3及ChatGPT原始模型等LLM。
- en: 'Since implementing BPE can be relatively complicated, we will use an existing
    Python open-source library called *tiktoken* ([https://github.com/openai/tiktoken](openai.html)),
    which implements the BPE algorithm very efficiently based on source code in Rust.
    Similar to other Python libraries, we can install the tiktoken library via Python''s
    `pip` installer from the terminal:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实现BPE可能相对复杂，我们将使用一个名为*tiktoken*的现有Python开源库（[https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)），它基于Rust源代码高效地实现了BPE算法。与其他Python库类似，我们可以通过Python的`pip`安装程序从终端安装tiktoken库：
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The code in this chapter is based on tiktoken 0.5.1\. You can use the following
    code to check the version you currently have installed:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码基于tiktoken 0.5.1。您可以使用以下代码检查当前安装的版本：
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，我们可以如下实例化来自tiktoken的BPE分词器：
- en: '[PRE35]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented
    previously via an `encode` method:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 该分词器的用法类似于我们之前通过`encode`方法实现的SimpleTokenizerV2：
- en: '[PRE36]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The code above prints the following token IDs:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码打印以下标记ID：
- en: '[PRE37]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can then convert the token IDs back into text using the decode method, similar
    to our `SimpleTokenizerV2` earlier:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用解码方法将标记ID转换回文本，类似于我们之前的`SimpleTokenizerV2`：
- en: '[PRE38]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The above code prints the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码打印以下内容：
- en: '[PRE39]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We can make two noteworthy observations based on the token IDs and decoded text
    above. First, the `<|endoftext|>` token is assigned a relatively large token ID,
    namely, 50256\. In fact, the BPE tokenizer, which was used to train models such
    as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary
    size of 50,257, with `<|endoftext|>` being assigned the largest token ID.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据上述的标记ID和解码文本做出两个值得注意的观察。首先，`<|endoftext|>`标记被分配了相对较大的标记ID，即50256。实际上，用于训练如GPT-2、GPT-3和ChatGPT原始模型的BPE分词器，总词汇量为50,257，其中`<|endoftext|>`被分配了最大的标记ID。
- en: Second, the BPE tokenizer above encodes and decodes unknown words, such as "someunknownPlace"
    correctly. The BPE tokenizer can handle any unknown word. How does it achieve
    this without using `<|unk|>` tokens?
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，上述BPE分词器可以正确编码和解码未知词，如“someunknownPlace”。BPE分词器可以处理任何未知单词。它是如何在不使用`<|unk|>`标记的情况下实现这一点的？
- en: The algorithm underlying BPE breaks down words that aren't in its predefined
    vocabulary into smaller subword units or even individual characters, enabling
    it to handle out-of-vocabulary words. So, thanks to the BPE algorithm, if the
    tokenizer encounters an unfamiliar word during tokenization, it can represent
    it as a sequence of subword tokens or characters, as illustrated in Figure 2.11.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: BPE背后的算法将不在其预定义词汇表中的单词分解为更小的子词单元或甚至单个字符，从而使其能够处理超出词汇范围的单词。因此，得益于BPE算法，如果分词器在分词时遇到不熟悉的单词，它可以将其表示为一系列子词标记或字符，如图2.11所示。
- en: Figure 2.11 BPE tokenizers break down unknown words into subwords and individual
    characters. This way, a BPE tokenizer can parse any word and doesn't need to replace
    unknown words with special tokens, such as <|unk|>.
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11 BPE分词器将未知词分解为子词和单个字符。通过这种方式，BPE分词器可以解析任何单词，而无需用特殊标记（如<|unk|>）替换未知词。
- en: '![](images/02__image021.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image021.png)'
- en: As illustrated in Figure 2.11, the ability to break down unknown words into
    individual characters ensures that the tokenizer, and consequently the LLM that
    is trained with it, can process any text, even if it contains words that were
    not present in its training data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如图2.11所示，将未知词分解为单个字符的能力确保了分词器，以及由其训练的LLM，可以处理任何文本，即使它包含在训练数据中不存在的单词。
- en: Exercise 2.1 Byte pair encoding of unknown words
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习2.1 未知词的字节对编码
- en: Try the BPE tokenizer from the tiktoken library on the unknown words "Akwirw
    ier" and print the individual token IDs. Then, call the decode function on each
    of the resulting integers in this list to reproduce the mapping shown in Figure
    2.11\. Lastly, call the decode method on the token IDs to check whether it can
    reconstruct the original input, "Akwirw ier".
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用tiktoken库的BPE分词器对未知词“Akwirw ier”进行处理，并打印单个标记ID。然后，对这个列表中的每个结果整数调用解码函数，以重现图2.11中显示的映射。最后，调用标记ID的解码方法，以检查是否可以重构原始输入“Akwirw
    ier”。
- en: A detailed discussion and implementation of BPE is out of the scope of this
    book, but in short, it builds its vocabulary by iteratively merging frequent characters
    into subwords and frequent subwords into words. For example, BPE starts with adding
    all individual single characters to its vocabulary ("a", "b", ...). In the next
    stage, it merges character combinations that frequently occur together into subwords.
    For example, "d" and "e" may be merged into the subword "de," which is common
    in many English words like "define", "depend", "made", and "hidden". The merges
    are determined by a frequency cutoff.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 的详细讨论和实现超出了本书的范围，但简而言之，它通过反复合并频繁出现的字符到子词，以及将频繁子词合并为词来构建其词汇。例如，BPE 从将所有单个字符添加到其词汇表开始（“a”、“b”等）。在下一阶段，它将经常一起出现的字符组合合并为子词。例如，“d”和“e”可能合并为子词“de”，这在许多英语单词中很常见，如“define”、“depend”、“made”和“hidden”。合并的决定基于频率阈值。
- en: 2.6 Data sampling with a sliding window
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 数据采样与滑动窗口
- en: The previous section covered the tokenization steps and conversion from string
    tokens into integer token IDs in great detail. The next step before we can finally
    create the embeddings for the LLM is to generate the input-target pairs required
    for training an LLM.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节详细讨论了分词步骤以及从字符串令牌转换为整数令牌 ID 的过程。在我们最终为 LLM 创建嵌入之前，下一步是生成训练 LLM 所需的输入-目标对。
- en: What do these input-target pairs look like? As we learned in chapter 1, LLMs
    are pretrained by predicting the next word in a text, as depicted in figure 2.12.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这些输入-目标对看起来如何？正如我们在第一章中学习的，LLM 通过预测文本中的下一个词进行预训练，如图 2.12 所示。
- en: Figure 2.12 Given a text sample, extract input blocks as subsamples that serve
    as input to the LLM, and the LLM's prediction task during training is to predict
    the next word that follows the input block. During training, we mask out all words
    that are past the target. Note that the text shown in this figure would undergo
    tokenization before the LLM can process it; however, this figure omits the tokenization
    step for clarity.
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.12 给定一个文本样本，提取输入块作为子样本，作为 LLM 的输入，LLM 的训练预测任务是预测跟随输入块的下一个词。在训练过程中，我们会掩盖掉所有超出目标的词。请注意，这个图中显示的文本在
    LLM 处理之前需要进行分词；然而，为了清晰起见，本图省略了分词步骤。
- en: '![](images/02__image023.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image023.png)'
- en: In this section we implement a data loader that fetches the input-target pairs
    depicted in Figure 2.12 from the training dataset using a sliding window approach.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实现一个数据加载器，使用滑动窗口方法从训练数据集中获取图 2.12 中描述的输入-目标对。
- en: 'To get started, we will first tokenize the whole The Verdict short story we
    worked with earlier using the BPE tokenizer introduced in the previous section:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用上一节介绍的 BPE 分词器对整个《裁决》短篇故事进行分词：
- en: '[PRE40]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Executing the code above will return 5145, the total number of tokens in the
    training set, after applying the BPE tokenizer.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码将返回 5145，即应用 BPE 分词器后训练集中 token 的总数。
- en: 'Next, we remove the first 50 tokens from the dataset for demonstration purposes
    as it results in a slightly more interesting text passage in the next steps:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从数据集中移除前 50 个 tokens，以便演示，因为这会导致后续步骤中出现更有趣的文本段落：
- en: '[PRE41]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'One of the easiest and most intuitive ways to create the input-target pairs
    for the next-word prediction task is to create two variables, `x` and `y`, where
    `x` contains the input tokens and `y` contains the targets, which are the inputs
    shifted by 1:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 创建下一个词预测任务的输入-目标对最简单和最直观的方法之一是创建两个变量 `x` 和 `y`，其中 `x` 包含输入 tokens，`y` 包含目标，这些目标是输入右移
    1 后的结果：
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Running the above code prints the following output:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码将输出以下内容：
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Processing the inputs along with the targets, which are the inputs shifted
    by one position, we can then create the next-word prediction tasks depicted earlier
    in figure 2.12, as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 处理输入和目标（目标是输入向右移动一个位置），我们可以创建之前在图 2.12 中描述的下一个词预测任务，具体如下：
- en: '[PRE44]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The code above prints the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出如下：
- en: '[PRE45]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Everything left of the arrow (`---->`) refers to the input an LLM would receive,
    and the token ID on the right side of the arrow represents the target token ID
    that the LLM is supposed to predict.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 箭头左侧的所有内容（`---->`）指的是 LLM 将接收的输入，而箭头右侧的 token ID 表示 LLM 应该预测的目标 token ID。
- en: 'For illustration purposes, let''s repeat the previous code but convert the
    token IDs into text:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明问题，我们将重复前面的代码，但将 token ID 转换为文本：
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following outputs show how the input and outputs look in text format:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示输入和输出在文本格式中的样子：
- en: '[PRE47]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We've now created the input-target pairs that we can turn into use for the LLM
    training in upcoming chapters.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经创建了可以在即将到来的章节中用于LLM训练的输入-目标对。
- en: 'There''s only one more task before we can turn the tokens into embeddings,
    as we mentioned at the beginning of this chapter: implementing an efficient data
    loader that iterates over the input dataset and returns the inputs and targets
    as PyTorch tensors, which can be thought of as multidimensional arrays.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在将tokens转化为嵌入之前，还有一个任务：实现一个高效的数据加载器，遍历输入数据集并将输入和目标作为PyTorch张量返回，这可以被视为多维数组。
- en: 'In particular, we are interested in returning two tensors: an input tensor
    containing the text that the LLM sees and a target tensor that includes the targets
    for the LLM to predict, as depicted in Figure 2.13.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们关注返回两个张量：一个包含LLM看到的文本的输入张量和一个包含LLM预测目标的目标张量，如图2.13所示。
- en: Figure 2.13 To implement efficient data loaders, we collect the inputs in a
    tensor, x, where each row represents one input context. A second tensor, y, contains
    the corresponding prediction targets (next words), which are created by shifting
    the input by one position.
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.13 为了实现高效的数据加载器，我们在张量x中收集输入，每一行代表一个输入上下文。第二个张量y包含相应的预测目标（下一个单词），它们是通过将输入向右移动一个位置创建的。
- en: '![](images/02__image025.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image025.png)'
- en: While Figure 2.13 shows the tokens in string format for illustration purposes,
    the code implementation will operate on token IDs directly since the `encode`
    method of the BPE tokenizer performs both tokenization and conversion into token
    IDs as a single step.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图2.13出于说明目的以字符串格式显示tokens，但代码实现将直接处理token ID，因为BPE分词器的`encode`方法将分词和转换为token
    ID的过程合并为一个步骤。
- en: For the efficient data loader implementation, we will use PyTorch's built-in
    Dataset and DataLoader classes. For additional information and guidance on installing
    PyTorch, please see section A.1.3, Installing PyTorch, in Appendix A.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高效的数据加载器实现，我们将使用PyTorch内置的Dataset和DataLoader类。有关安装PyTorch的更多信息和指导，请参见附录A中的A.1.3节，安装PyTorch。
- en: 'The code for the dataset class is shown in code listing 2.5:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集类的代码在代码清单2.5中显示：
- en: Listing 2.5 A dataset for batched inputs and targets
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单2.5 用于批量输入和目标的数据集
- en: '[PRE48]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The `GPTDatasetV1` class in listing 2.5 is based on the PyTorch `Dataset` class
    and defines how individual rows are fetched from the dataset, where each row consists
    of a number of token IDs (based on a `max_length`) assigned to an `input_chunk`
    tensor. The `target_chunk` tensor contains the corresponding targets. I recommend
    reading on to see how the data returned from this dataset looks like when we combine
    the dataset with a PyTorch `DataLoader` -- this will bring additional intuition
    and clarity.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 清单2.5中的`GPTDatasetV1`类基于PyTorch的`Dataset`类，定义了如何从数据集中提取单独的行，每一行由多个基于`max_length`分配给`input_chunk`张量的token
    ID组成。`target_chunk`张量包含相应的目标。我建议继续阅读，看看当我们将数据集与PyTorch的`DataLoader`结合时，从这个数据集中返回的数据是什么样的——这将带来额外的直观理解和清晰度。
- en: If you are new to the structure of PyTorch `Dataset` classes, such as shown
    in listing 2.5, please read section *A.6, Setting up efficient data loaders*,
    in Appendix A, which explains the general structure and usage of PyTorch `Dataset`
    and `DataLoader` classes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对清单2.5中展示的PyTorch `Dataset`类的结构不熟悉，请阅读附录A中的*A.6，高效数据加载器设置*，其中解释了PyTorch `Dataset`和`DataLoader`类的一般结构和使用方法。
- en: 'The following code will use the `GPTDatasetV1` to load the inputs in batches
    via a PyTorch `DataLoader`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将使用`GPTDatasetV1`通过PyTorch的`DataLoader`批量加载输入：
- en: Listing 2.6 A data loader to generate batches with input-with pairs
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单2.6 生成包含输入-目标对的批次的数据加载器
- en: '[PRE49]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let''s test the `dataloader` with a batch size of 1 for an LLM with a context
    size of 4 to develop an intuition of how the `GPTDatasetV1` class from listing
    2.5 and the `create_dataloader_v1` function from listing 2.6 work together:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用上下文大小为4的LLM测试批量大小为1的`dataloader`，以便对清单2.5中的`GPTDatasetV1`类和清单2.6中的`create_dataloader_v1`函数的协作有一个直观的理解：
- en: '[PRE50]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Executing the preceding code prints the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码将打印以下内容：
- en: '[PRE51]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The `first_batch` variable contains two tensors: the first tensor stores the
    input token IDs, and the second tensor stores the target token IDs. Since the
    `max_length` is set to 4, each of the two tensors contains 4 token IDs. Note that
    an input size of 4 is relatively small and only chosen for illustration purposes.
    It is common to train LLMs with input sizes of at least 256.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`first_batch` 变量包含两个张量：第一个张量存储输入令牌 ID，第二个张量存储目标令牌 ID。由于 `max_length` 设置为 4，因此两个张量各包含
    4 个令牌 ID。请注意，输入大小为 4 相对较小，仅用于说明目的。通常，训练 LLM 时的输入大小至少为 256。'
- en: 'To illustrate the meaning of `stride=1`, let''s fetch another batch from this
    dataset:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 `stride=1` 的含义，让我们从这个数据集中获取另一个批次：
- en: '[PRE52]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The second batch has the following contents:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个批次的内容如下：
- en: '[PRE53]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: If we compare the first with the second batch, we can see that the second batch's
    token IDs are shifted by one position compared to the first batch (for example,
    the second ID in the first batch's input is 367, which is the first ID of the
    second batch's input). The `stride` setting dictates the number of positions the
    inputs shift across batches, emulating a sliding window approach, as demonstrated
    in Figure 2.14.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将第一个批次与第二个批次进行比较，可以看到第二个批次的令牌 ID 相比于第一个批次向前移动了一个位置（例如，第一个批次输入的第二个 ID 是 367，这正是第二个批次输入的第一个
    ID）。`stride` 设置决定了输入在批次间移动的位置数量，模拟了滑动窗口的方法，如图 2.14 所示。
- en: Figure 2.14 When creating multiple batches from the input dataset, we slide
    an input window across the text. If the stride is set to 1, we shift the input
    window by 1 position when creating the next batch. If we set the stride equal
    to the input window size, we can prevent overlaps between the batches.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.14 在从输入数据集中创建多个批次时，我们在文本上滑动输入窗口。如果将步幅设置为 1，则在创建下一个批次时，我们将输入窗口向前移动 1 个位置。如果将步幅设置为输入窗口大小，则可以防止批次之间的重叠。
- en: '![](images/02__image027.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image027.png)'
- en: Exercise 2.2 Data loaders with different strides and context sizes
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2.2 使用不同步幅和上下文大小的数据加载器
- en: To develop more intuition for how the data loader works, try to run it with
    different settings such as max_length=2 and stride=2 and max_length=8 and stride=2.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解数据加载器的工作原理，可以尝试使用不同的设置运行它，例如 max_length=2 和 stride=2，以及 max_length=8
    和 stride=2。
- en: Batch sizes of 1, such as we have sampled from the data loader so far, are useful
    for illustration purposes. If you have previous experience with deep learning,
    you may know that small batch sizes require less memory during training but lead
    to more noisy model updates. Just like in regular deep learning, the batch size
    is a trade-off and hyperparameter to experiment with when training LLMs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 批次大小为 1，例如我们迄今为止从数据加载器中采样的，适用于说明目的。如果您有深度学习的经验，您可能知道小批次大小在训练过程中需要更少的内存，但会导致模型更新更嘈杂。就像在常规深度学习中，批次大小是一个权衡和超参数，需要在训练
    LLM 时进行实验。
- en: 'Before we move on to the two final sections of this chapter that are focused
    on creating the embedding vectors from the token IDs, let''s have a brief look
    at how we can use the data loader to sample with a batch size greater than 1:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续本章最后两个部分之前，专注于从令牌 ID 创建嵌入向量，让我们简单看看如何使用数据加载器进行大于 1 的批次大小采样：
- en: '[PRE54]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This prints the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印以下内容：
- en: '[PRE55]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Note that we increase the stride to 4\. This is to utilize the data set fully
    (we don't skip a single word) but also avoid any overlap between the batches,
    since more overlap could lead to increased overfitting.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将步幅增加到 4。这是为了充分利用数据集（我们不跳过任何一个词），同时避免批次之间的重叠，因为更多的重叠可能导致过拟合加剧。
- en: In the final two sections of this chapter, we will implement embedding layers
    that convert the token IDs into continuous vector representations, which serve
    as input data format for LLMs.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后两个部分中，我们将实现嵌入层，将令牌 ID 转换为连续的向量表示，这些表示作为 LLM 的输入数据格式。
- en: 2.7 Creating token embeddings
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 创建令牌嵌入
- en: The last step for preparing the input text for LLM training is to convert the
    token IDs into embedding vectors, as illustrated in Figure 2.15, which will be
    the focus of these two last remaining sections of this chapter.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 准备 LLM 训练输入文本的最后一步是将令牌 ID 转换为嵌入向量，如图 2.15 所示，这将是本章最后两个部分的重点。
- en: Figure 2.15 Preparing the input text for an LLM involves tokenizing text, converting
    text tokens to token IDs, and converting token IDs into vector embedding vectors.
    In this section, we consider the token IDs created in previous sections to create
    the token embedding vectors.
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.15 为 LLM 准备输入文本涉及对文本进行标记化，将文本标记转换为标记 ID，并将标记 ID 转换为向量嵌入向量。在本节中，我们考虑在前面的部分创建的标记
    ID，以生成标记嵌入向量。
- en: '![](images/02__image029.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image029.png)'
- en: In addition to the processes outlined in Figure 2.15, it is important to note
    that we initialize these embedding weights with random values as a preliminary
    step. This initialization serves as the starting point for the LLM's learning
    process. We will optimize the embedding weights as part of the LLM training in
    chapter 5.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图 2.15 中概述的过程外，重要的是要注意，我们在初步步骤中以随机值初始化这些嵌入权重。这一初始化作为 LLM 学习过程的起点。我们将在第 5 章中优化嵌入权重，作为
    LLM 训练的一部分。
- en: A continuous vector representation, or embedding, is necessary since GPT-like
    LLMs are deep neural networks trained with the backpropagation algorithm. If you
    are unfamiliar with how neural networks are trained with backpropagation, please
    read section A.4, *Automatic differentiation made easy*, in Appendix A.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GPT 类 LLM 是通过反向传播算法训练的深度神经网络，因此需要连续的向量表示或嵌入。如果你对神经网络如何通过反向传播训练不熟悉，请阅读附录 A
    中的 A.4 节，*自动微分简单易懂*。
- en: 'Let''s illustrate how the token ID to embedding vector conversion works with
    a hands-on example. Suppose we have the following four input tokens with IDs 2,
    3, 5, and 1:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际示例来说明标记 ID 到嵌入向量转换的工作原理。假设我们有以下四个输入标记，ID 为 2、3、5 和 1：
- en: '[PRE56]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'For the sake of simplicity and illustration purposes, suppose we have a small
    vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary),
    and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288
    dimensions):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见和说明，假设我们只有 6 个单词的小词汇表（而不是 BPE 标记器词汇表中的 50,257 个单词），并且我们希望创建大小为 3 的嵌入（在
    GPT-3 中，嵌入大小为 12,288 维）：
- en: '[PRE57]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Using the `vocab_size` and `output_dim`, we can instantiate an embedding layer
    in PyTorch, setting the random seed to 123 for reproducibility purposes:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `vocab_size` 和 `output_dim`，我们可以在 PyTorch 中实例化一个嵌入层，将随机种子设置为 123 以确保可重复性：
- en: '[PRE58]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The print statement in the preceding code example prints the embedding layer''s
    underlying weight matrix:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个代码示例中的打印语句打印了嵌入层的底层权重矩阵：
- en: '[PRE59]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We can see that the weight matrix of the embedding layer contains small, random
    values. These values are optimized during LLM training as part of the LLM optimization
    itself, as we will see in upcoming chapters. Moreover, we can see that the weight
    matrix has six rows and three columns. There is one row for each of the six possible
    tokens in the vocabulary. And there is one column for each of the three embedding
    dimensions.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到嵌入层的权重矩阵包含小的随机值。这些值在 LLM 训练过程中被优化，作为 LLM 优化的一部分，正如我们将在后续章节中看到的。此外，我们可以看到权重矩阵有六行三列。每个词汇表中六个可能的标记都有一行，每个嵌入维度都有一列。
- en: 'After we instantiated the embedding layer, let''s now apply it to a token ID
    to obtain the embedding vector:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化嵌入层后，接下来我们将其应用于标记 ID 以获得嵌入向量：
- en: '[PRE60]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The returned embedding vector is as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的嵌入向量如下：
- en: '[PRE61]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: If we compare the embedding vector for token ID 3 to the previous embedding
    matrix, we see that it is identical to the 4th row (Python starts with a zero
    index, so it's the row corresponding to index 3). In other words, the embedding
    layer is essentially a look-up operation that retrieves rows from the embedding
    layer's weight matrix via a token ID.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将标记 ID 3 的嵌入向量与先前的嵌入矩阵进行比较，我们会发现它与第 4 行相同（Python 从零开始索引，所以它对应于索引 3 的行）。换句话说，嵌入层本质上是一个查找操作，通过标记
    ID 从嵌入层的权重矩阵中检索行。
- en: Embedding layers versus matrix multiplication
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 嵌入层与矩阵乘法
- en: For those who are familiar with one-hot encoding, the embedding layer approach
    above is essentially just a more efficient way of implementing one-hot encoding
    followed by matrix multiplication in a fully connected layer, which is illustrated
    in the supplementary code on GitHub at [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/03_bonus_embedding-vs-matmul](ch02.html).
    Because the embedding layer is just a more efficient implementation equivalent
    to the one-hot encoding and matrix-multiplication approach, it can be seen as
    a neural network layer that can be optimized via backpropagation.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉独热编码的人来说，上述嵌入层方法本质上只是实现独热编码并随后在全连接层中进行矩阵乘法的更高效方式，这在 GitHub 上的补充代码中得到了说明，链接为
    [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/03_bonus_embedding-vs-matmul](ch02.html)。由于嵌入层只是与独热编码和矩阵乘法方法等效的更高效实现，因此可以将其视为一个可以通过反向传播进行优化的神经网络层。
- en: 'Previously, we have seen how to convert a single token ID into a three-dimensional
    embedding vector. Let''s now apply that to all four input IDs we defined earlier
    (`torch.tensor([2, 3, 5, 1])`):'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们已经看到如何将单个令牌 ID 转换为三维嵌入向量。现在让我们将这个方法应用于我们之前定义的所有四个输入 ID（`torch.tensor([2,
    3, 5, 1])`）：
- en: '[PRE62]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The print output reveals that this results in a 4x3 matrix:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 打印输出显示这产生了一个 4x3 的矩阵：
- en: '[PRE63]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Each row in this output matrix is obtained via a lookup operation from the embedding
    weight matrix, as illustrated in Figure 2.16.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出矩阵中的每一行是通过查找嵌入权重矩阵获得的，如图 2.16 所示。
- en: Figure 2.16 Embedding layers perform a look-up operation, retrieving the embedding
    vector corresponding to the token ID from the embedding layer's weight matrix.
    For instance, the embedding vector of the token ID 5 is the sixth row of the embedding
    layer weight matrix (it is the sixth instead of the fifth row because Python starts
    counting at 0). For illustration purposes, we assume that the token IDs were produced
    by the small vocabulary we used in section 2.3.
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.16 嵌入层执行查找操作，从嵌入层的权重矩阵中检索与令牌 ID 对应的嵌入向量。例如，令牌 ID 5 的嵌入向量是嵌入层权重矩阵的第六行（因为
    Python 从 0 开始计数，所以是第六行而不是第五行）。为了便于说明，我们假设令牌 ID 是通过我们在第 2.3 节中使用的小词汇表生成的。
- en: '![](images/02__image031.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image031.png)'
- en: This section covered how we create embedding vectors from token IDs. The next
    and final section of this chapter will add a small modification to these embedding
    vectors to encode positional information about a token within a text.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了我们如何从令牌 ID 创建嵌入向量。本章的下一节也是最后一节将对这些嵌入向量进行小幅修改，以编码文本中令牌的位置信息。
- en: 2.8 Encoding word positions
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 编码词位置
- en: In the previous section, we converted the token IDs into a continuous vector
    representation, the so-called token embeddings. In principle, this is a suitable
    input for an LLM. However, a minor shortcoming of LLMs is that their self-attention
    mechanism, which will be covered in detail in chapter 3, doesn't have a notion
    of position or order for the tokens within a sequence.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们将令牌 ID 转换为连续的向量表示，即所谓的令牌嵌入。原则上，这是 LLM 的合适输入。然而，LLM 的一个小缺点是，它们的自注意力机制（将在第
    3 章详细介绍）没有对序列中令牌的位置或顺序的概念。
- en: The way the previously introduced embedding layer works is that the same token
    ID always gets mapped to the same vector representation, regardless of where the
    token ID is positioned in the input sequence, as illustrated in Figure 2.17.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 之前介绍的嵌入层的工作方式是，相同的令牌 ID 总是映射到相同的向量表示，无论该令牌 ID 在输入序列中的位置如何，如图 2.17 所示。
- en: Figure 2.17 The embedding layer converts a token ID into the same vector representation
    regardless of where it is located in the input sequence. For example, the token
    ID 5, whether it's in the first or third position in the token ID input vector,
    will result in the same embedding vector.
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.17 嵌入层将令牌 ID 转换为相同的向量表示，而不管它在输入序列中的位置。例如，令牌 ID 5，无论是在令牌 ID 输入向量的第一还是第三个位置，都会得到相同的嵌入向量。
- en: '![](images/02__image033.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image033.png)'
- en: In principle, the deterministic, position-independent embedding of the token
    ID is good for reproducibility purposes. However, since the self-attention mechanism
    of LLMs itself is also position-agnostic, it is helpful to inject additional position
    information into the LLM.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，确定性且位置无关的令牌 ID 嵌入有利于重现性。然而，由于大型语言模型（LLMs）的自注意力机制本身也是位置无关的，因此将额外的位置信息注入 LLM
    是有帮助的。
- en: 'To achieve this, there are two broad categories of position-aware embeddings:
    relative *positional embeddings* and absolute positional embeddings.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，位置感知嵌入可以分为两大类：相对*位置嵌入*和绝对位置嵌入。
- en: Absolute positional embeddings are directly associated with specific positions
    in a sequence. For each position in the input sequence, a unique embedding is
    added to the token's embedding to convey its exact location. For instance, the
    first token will have a specific positional embedding, the second token another
    distinct embedding, and so on, as illustrated in Figure 2.18.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对位置嵌入与序列中的特定位置直接关联。对于输入序列中的每个位置，都会添加一个唯一的嵌入到token的嵌入中，以传达其确切位置。例如，第一个token将具有特定的位置嵌入，第二个token将有另一个不同的嵌入，依此类推，如图2.18所示。
- en: Figure 2.18 Positional embeddings are added to the token embedding vector to
    create the input embeddings for an LLM. The positional vectors have the same dimension
    as the original token embeddings. The token embeddings are shown with value 1
    for simplicity.
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.18中，位置嵌入被添加到token嵌入向量中，以创建LLM的输入嵌入。位置向量的维度与原始token嵌入相同。为了简单起见，token嵌入的值显示为1。
- en: '![](images/02__image035.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image035.png)'
- en: Instead of focusing on the absolute position of a token, the emphasis of relative
    positional embeddings is on the relative position or distance between tokens.
    This means the model learns the relationships in terms of "how far apart" rather
    than "at which exact position." The advantage here is that the model can generalize
    better to sequences of varying lengths, even if it hasn't seen such lengths during
    training.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 相对位置嵌入的重点不是关注token的绝对位置，而是token之间的相对位置或距离。这意味着模型学习的是“多远”的关系，而不是“在什么确切位置”。这里的优势在于，模型可以更好地泛化到不同长度的序列，即使在训练期间没有见过这样的长度。
- en: Both types of positional embeddings aim to augment the capacity of LLMs to understand
    the order and relationships between tokens, ensuring more accurate and context-aware
    predictions. The choice between them often depends on the specific application
    and the nature of the data being processed.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种类型的位置嵌入旨在增强LLM理解token之间顺序和关系的能力，确保更准确和上下文感知的预测。选择它们通常取决于具体应用和所处理数据的性质。
- en: OpenAI's GPT models use absolute positional embeddings that are optimized during
    the training process rather than being fixed or predefined like the positional
    encodings in the original Transformer model. This optimization process is part
    of the model training itself, which we will implement later in this book. For
    now, let's create the initial positional embeddings to create the LLM inputs for
    the upcoming chapters.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT模型使用的绝对位置嵌入是在训练过程中优化的，而不是像原始Transformer模型中的位置编码那样固定或预定义。这个优化过程是模型训练的一部分，我们将在本书后面实现。现在，让我们创建初始位置嵌入，以为接下来的章节创建LLM输入。
- en: 'Previously, we focused on very small embedding sizes in this chapter for illustration
    purposes. We now consider more realistic and useful embedding sizes and encode
    the input tokens into a 256-dimensional vector representation. This is smaller
    than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288
    dimensions) but still reasonable for experimentation. Furthermore, we assume that
    the token IDs were created by the BPE tokenizer that we implemented earlier, which
    has a vocabulary size of 50,257:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们在本章中集中讨论了非常小的嵌入大小以作示例。我们现在考虑更现实和实用的嵌入大小，并将输入token编码为256维的向量表示。这比原始GPT-3模型使用的尺寸小（在GPT-3中，嵌入大小为12,288维），但仍适合实验。此外，我们假设token
    ID是由我们之前实现的BPE分词器创建的，其词汇量为50,257：
- en: '[PRE64]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Using the `token_embedding_layer` above, if we sample data from the data loader,
    we embed each token in each batch into a 256-dimensional vector. If we have a
    batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述`token_embedding_layer`，如果我们从数据加载器中抽样数据，我们将每个批次中的每个token嵌入到256维的向量中。如果我们有一个批次大小为8，每个批次有四个token，结果将是一个8
    x 4 x 256的张量。
- en: 'Let''s instantiate the data loader from section 2.6, *Data sampling with a
    sliding window*, first:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实例化第2.6节中的数据加载器，*滑动窗口的数据采样*：
- en: '[PRE65]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The preceding code prints the following output:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出如下：
- en: '[PRE66]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: As we can see, the token ID tensor is 8x4-dimensional, meaning that the data
    batch consists of 8 text samples with 4 tokens each.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，token ID 张量是8x4维的，这意味着数据批次由8个文本样本组成，每个样本有4个token。
- en: 'Let''s now use the embedding layer to embed these token IDs into 256-dimensional
    vectors:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用嵌入层将这些令牌ID嵌入到256维向量中：
- en: '[PRE67]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The preceding print function call returns the following:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的打印函数调用返回如下：
- en: '[PRE68]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: As we can tell based on the 8x4x256-dimensional tensor output, each token ID
    is now embedded as a 256-dimensional vector.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 根据8x4x256维张量的输出，我们可以看出，每个令牌ID现在被嵌入为256维向量。
- en: 'For a GPT model''s absolute embedding approach, we just need to create another
    embedding layer that has the same dimension as the `token_embedding_layer`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPT模型的绝对嵌入方法，我们只需创建另一个与`token_embedding_layer`维度相同的嵌入层：
- en: '[PRE69]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: As shown in the preceding code example, the input to the `pos_embeddings` is
    usually a placeholder vector `torch.arange(context_length)`, which contains a
    sequence of numbers 0, 1, ..., up to the maximum input length − 1\. The `context_length`
    is a variable that represents the supported input size of the LLM. Here, we choose
    it similar to the maximum length of the input text. In practice, input text can
    be longer than the supported context length, in which case we have to truncate
    the text.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码示例所示，`pos_embeddings`的输入通常是一个占位符向量`torch.arange(context_length)`，它包含一个从0到最大输入长度−1的数字序列。`context_length`是一个变量，表示LLM支持的输入大小。在这里，我们将其选择为与输入文本的最大长度相似。在实际应用中，输入文本可能会比支持的上下文长度更长，在这种情况下，我们必须截断文本。
- en: 'The output of the print statement is as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 打印语句的输出如下：
- en: '[PRE70]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'As we can see, the positional embedding tensor consists of four 256-dimensional
    vectors. We can now add these directly to the token embeddings, where PyTorch
    will add the 4x256-dimensional `pos_embeddings` tensor to each 4x256-dimensional
    token embedding tensor in each of the 8 batches:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，位置嵌入张量由四个256维向量组成。我们现在可以将这些直接添加到令牌嵌入中，PyTorch会将4x256维的`pos_embeddings`张量添加到每个8个批次中的4x256维的令牌嵌入张量中：
- en: '[PRE71]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The print output is as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 打印输出如下：
- en: '[PRE72]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The `input_embeddings` we created, as summarized in Figure 2.19, are the embedded
    input examples that can now be processed by the main LLM modules, which we will
    begin implementing in chapter 3
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的`input_embeddings`，如图2.19所示，是可以被主LLM模块处理的嵌入输入示例，我们将在第3章开始实现。
- en: Figure 2.19 As part of the input processing pipeline, input text is first broken
    up into individual tokens. These tokens are then converted into token IDs using
    a vocabulary. The token IDs are converted into embedding vectors to which positional
    embeddings of a similar size are added, resulting in input embeddings that are
    used as input for the main LLM layers.
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.19 作为输入处理管道的一部分，输入文本首先被拆分为单个令牌。这些令牌然后通过词汇表转换为令牌ID。令牌ID被转换为嵌入向量，并添加相似大小的位置信息嵌入，最终得到用于主LLM层输入的输入嵌入。
- en: '![](images/02__image037.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image037.png)'
- en: 2.9 Summary
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.9 摘要
- en: LLMs require textual data to be converted into numerical vectors, known as embeddings
    since they can't process raw text. Embeddings transform discrete data (like words
    or images) into continuous vector spaces, making them compatible with neural network
    operations.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM需要将文本数据转换为数值向量，称为嵌入，因为它们无法处理原始文本。嵌入将离散数据（如单词或图像）转换为连续向量空间，使其与神经网络操作兼容。
- en: As the first step, raw text is broken into tokens, which can be words or characters.
    Then, the tokens are converted into integer representations, termed token IDs.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步，原始文本被拆分为令牌，这可以是单词或字符。然后，令牌被转换为整数表示，称为令牌ID。
- en: Special tokens, such as `<|unk|>` and `<|endoftext|>`, can be added to enhance
    the model's understanding and handle various contexts, such as unknown words or
    marking the boundary between unrelated texts.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特殊标记，例如`<|unk|>`和`<|endoftext|>`，可以添加以增强模型的理解并处理各种上下文，例如未知单词或标记无关文本之间的边界。
- en: The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3 can
    efficiently handle unknown words by breaking them down into subword units or individual
    characters.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于像GPT-2和GPT-3这样的LLM的字节对编码（BPE）标记器可以通过将未知单词分解为子词单元或单个字符来有效处理它们。
- en: We use a sliding window approach on tokenized data to generate input-target
    pairs for LLM training.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在标记化数据上使用滑动窗口方法生成LLM训练的输入-目标对。
- en: Embedding layers in PyTorch function as a lookup operation, retrieving vectors
    corresponding to token IDs. The resulting embedding vectors provide continuous
    representations of tokens, which is crucial for training deep learning models
    like LLMs.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch中的嵌入层作为查找操作，检索与标记ID对应的向量。生成的嵌入向量提供了标记的连续表示，这对训练像LLMs这样的深度学习模型至关重要。
- en: 'While token embeddings provide consistent vector representations for each token,
    they lack a sense of the token''s position in a sequence. To rectify this, two
    main types of positional embeddings exist: absolute and relative. OpenAI''s GPT
    models utilize absolute positional embeddings that are added to the token embedding
    vectors and are optimized during the model training.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然标记嵌入为每个标记提供了一致的向量表示，但它们缺乏标记在序列中的位置感。为了解决这个问题，存在两种主要类型的位置嵌入：绝对位置嵌入和相对位置嵌入。OpenAI的GPT模型利用绝对位置嵌入，这些嵌入被添加到标记嵌入向量中，并在模型训练过程中进行优化。
