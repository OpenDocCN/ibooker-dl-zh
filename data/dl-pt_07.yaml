- en: '7 Telling birds from airplanes: Learning from images'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 从图像中识别鸟类和飞机：从图像中学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Building a feed-forward neural network
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建前馈神经网络
- en: Loading data using `Dataset`s and `DataLoader`s
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`Dataset`和`DataLoader`加载数据
- en: Understanding classification loss
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分类损失
- en: The last chapter gave us the opportunity to dive into the inner mechanics of
    learning through gradient descent, and the facilities that PyTorch offers to build
    models and optimize them. We did so using a simple regression model of one input
    and one output, which allowed us to have everything in plain sight but admittedly
    was only borderline exciting.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章让我们有机会深入了解通过梯度下降学习的内部机制，以及PyTorch提供的构建模型和优化模型的工具。我们使用了一个简单的具有一个输入和一个输出的回归模型，这使我们可以一目了然，但诚实地说只是勉强令人兴奋。
- en: In this chapter, we’ll keep moving ahead with building our neural network foundations.
    This time, we’ll turn our attention to images. Image recognition is arguably the
    task that made the world realize the potential of deep learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续构建我们的神经网络基础。这一次，我们将把注意力转向图像。图像识别可以说是让世界意识到深度学习潜力的任务。
- en: We will approach a simple image recognition problem step by step, building from
    a simple neural network like the one we defined in the last chapter. This time,
    instead of a tiny dataset of numbers, we’ll use a more extensive dataset of tiny
    images. Let’s download the dataset first and get to work preparing it for use.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步解决一个简单的图像识别问题，从上一章中定义的简单神经网络开始构建。这一次，我们将使用一个更广泛的小图像数据集，而不是一组数字。让我们首先下载数据集，然后开始准备使用它。
- en: 7.1 A dataset of tiny images
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 一个小图像数据集
- en: There is nothing like an intuitive understanding of a subject, and there is
    nothing to achieve that like working on simple data. One of the most basic datasets
    for image recognition is the handwritten digit-recognition dataset known as MNIST.
    Here we will use another dataset that is similarly simple and a bit more fun.
    It’s called CIFAR-10, and, like its sibling CIFAR-100, it has been a computer
    vision classic for a decade.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么比对一个主题的直观理解更好，也没有什么比处理简单数据更能实现这一点。图像识别中最基本的数据集之一是被称为MNIST的手写数字识别数据集。在这里，我们将使用另一个类似简单且更有趣的数据集。它被称为CIFAR-10，就像它的姐妹CIFAR-100一样，它已经成为计算机视觉领域的经典数据集十年。
- en: 'CIFAR-10 consists of 60,000 tiny 32 × 32 color (RGB) images, labeled with an
    integer corresponding to 1 of 10 classes: airplane (0), automobile (1), bird (2),
    cat (3), deer (4), dog (5), frog (6), horse (7), ship (8), and truck (9).[¹](#pgfId-1011878)
    Nowadays, CIFAR-10 is considered too simple for developing or validating new research,
    but it serves our learning purposes just fine. We will use the `torchvision` module
    to automatically download the dataset and load it as a collection of PyTorch tensors.
    Figure 7.1 gives us a taste of CIFAR-10.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10由60,000个32×32彩色（RGB）图像组成，标记为10个类别中的一个整数：飞机（0）、汽车（1）、鸟（2）、猫（3）、鹿（4）、狗（5）、青蛙（6）、马（7）、船（8）和卡车（9）。如今，CIFAR-10被认为对于开发或验证新研究来说过于简单，但对于我们的学习目的来说完全够用。我们将使用`torchvision`模块自动下载数据集，并将其加载为一组PyTorch张量。图7.1让我们一睹CIFAR-10的风采。
- en: '![](../Images/CH07_F01_Stevens2_GS.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F01_Stevens2_GS.png)'
- en: Figure 7.1 Image samples from all CIFAR-10 classes
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 显示所有CIFAR-10类别的图像样本
- en: 7.1.1 Downloading CIFAR-10
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 下载CIFAR-10
- en: 'As we anticipated, let’s import `torchvision` and use the `datasets` module
    to download the CIFAR-10 data:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，让我们导入`torchvision`并使用`datasets`模块下载CIFAR-10数据：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Instantiates a dataset for the training data; TorchVision downloads the data
    if it is not present
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为训练数据实例化一个数据集；如果数据不存在，TorchVision会下载数据
- en: ❷ With train=False, this gets us a dataset for the validation data, again downloading
    as necessary.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用train=False，这样我们就得到了一个用于验证数据的数据集，如果需要的话会进行下载。
- en: The first argument we provide to the `CIFAR10` function is the location from
    which the data will be downloaded; the second specifies whether we’re interested
    in the training set or the validation set; and the third says whether we allow
    PyTorch to download the data if it is not found in the location specified in the
    first argument.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供给`CIFAR10`函数的第一个参数是数据将被下载的位置；第二个参数指定我们是对训练集感兴趣还是对验证集感兴趣；第三个参数表示我们是否允许PyTorch在指定的位置找不到数据时下载数据。
- en: 'Just like `CIFAR10`, the `datasets` submodule gives us precanned access to
    the most popular computer vision datasets, such as MNIST, Fashion-MNIST, CIFAR-100,
    SVHN, Coco, and Omniglot. In each case, the dataset is returned as a subclass
    of `torch.utils.data.Dataset`. We can see that the method-resolution order of
    our `cifar10` instance includes it as a base class:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`CIFAR10`一样，`datasets`子模块为我们提供了对最流行的计算机视觉数据集的预先访问，如MNIST、Fashion-MNIST、CIFAR-100、SVHN、Coco和Omniglot。在每种情况下，数据集都作为`torch.utils.data.Dataset`的子类返回。我们可以看到我们的`cifar10`实例的方法解析顺序将其作为一个基类：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 7.1.2 The Dataset class
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 Dataset类
- en: 'It’s a good time to discover what being a subclass of `torch.utils.data.Dataset`
    means in practice. Looking at figure 7.2, we see what PyTorch `Dataset` is all
    about. It is an object that is required to implement two methods: `__len__` and
    `__getitem__`. The former should return the number of items in the dataset; the
    latter should return the item, consisting of a sample and its corresponding label
    (an integer index).[²](#pgfId-1012309)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是一个好时机去了解在实践中成为`torch.utils.data.Dataset`子类意味着什么。看一下图7.2，我们就能明白PyTorch的`Dataset`是什么。它是一个需要实现两个方法的对象：`__len__`和`__getitem__`。前者应该返回数据集中的项目数；后者应该返回项目，包括一个样本及其对应的标签（一个整数索引）。
- en: 'In practice, when a Python object is equipped with the `__len__` method, we
    can pass it as an argument to the `len` Python built-in function:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，当一个Python对象配备了`__len__`方法时，我们可以将其作为参数传递给`len`Python内置函数：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/CH07_F02_Stevens2_GS.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F02_Stevens2_GS.png)'
- en: 'Figure 7.2 Concept of a PyTorch `Dataset` object: it doesn’t necessarily hold
    the data, but it provides uniform access to it through `__len__` and `__getitem__`.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 PyTorch `Dataset` 对象的概念：它不一定保存数据，但通过 `__len__` 和 `__getitem__` 提供统一访问。
- en: 'Similarly, since the dataset is equipped with the `__getitem__` method, we
    can use the standard subscript for indexing tuples and lists to access individual
    items. Here, we get a `PIL` (Python Imaging Library, the `PIL` package) image
    with our desired output--an integer with the value `1`, corresponding to “automobile”:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，由于数据集配备了 `__getitem__` 方法，我们可以使用标准的下标索引元组和列表来访问单个项目。在这里，我们得到了一个 `PIL`（Python
    Imaging Library，`PIL` 包）图像，输出我们期望的整数值 `1`，对应于“汽车”：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'So, the sample in the `data.CIFAR10` dataset is an instance of an RGB PIL image.
    We can plot it right away:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`data.CIFAR10` 数据集中的样本是 RGB PIL 图像的一个实例。我们可以立即绘制它：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This produces the output shown in figure 7.3\. It’s a red car![³](#pgfId-1012622)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了图 7.3 中显示的输出。这是一辆红色的汽车！[³](#pgfId-1012622)
- en: '![](../Images/CH07_F03_Stevens2_GS.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F03_Stevens2_GS.png)'
- en: 'Figure 7.3 The 99th image from the CIFAR-10 dataset: an automobile'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 CIFAR-10 数据集中的第 99 张图像：一辆汽车
- en: 7.1.3 Dataset transforms
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 数据集转换
- en: 'That’s all very nice, but we’ll likely need a way to convert the PIL image
    to a PyTorch tensor before we can do anything with it. That’s where `torchvision.transforms`
    comes in. This module defines a set of composable, function-like objects that
    can be passed as an argument to a `torchvision` dataset such as `datasets.CIFAR10(...)`,
    and that perform transformations on the data after it is loaded but before it
    is returned by `__getitem__`. We can see the list of available objects as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都很好，但我们可能需要一种方法在对其进行任何操作之前将 PIL 图像转换为 PyTorch 张量。这就是 `torchvision.transforms`
    的作用。该模块定义了一组可组合的、类似函数的对象，可以作为参数传递给 `torchvision` 数据集，如 `datasets.CIFAR10(...)`，并在加载数据后但在
    `__getitem__` 返回数据之前对数据执行转换。我们可以查看可用对象的列表如下：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Among those transforms, we can spot `ToTensor`, which turns NumPy arrays and
    PIL images to tensors. It also takes care to lay out the dimensions of the output
    tensor as *C* × *H* × *W* (channel, height, width; just as we covered in chapter
    4).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些转换中，我们可以看到 `ToTensor`，它将 NumPy 数组和 PIL 图像转换为张量。它还会确保输出张量的维度布局为 *C* × *H*
    × *W*（通道、高度、宽度；就像我们在第 4 章中介绍的那样）。
- en: 'Let’s try out the `ToTensor` transform. Once instantiated, it can be called
    like a function with the PIL image as the argument, returning a tensor as output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一下 `ToTensor` 转换。一旦实例化，它可以像一个函数一样调用，参数是 PIL 图像，返回一个张量作为输出：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The image has been turned into a 3 × 32 × 32 tensor and therefore a 3-channel
    (RGB) 32 × 32 image. Note that nothing has happened to `label`; it is still an
    integer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图像已经转换为 3 × 32 × 32 张量，因此是一个 3 通道（RGB）32 × 32 图像。请注意 `label` 没有发生任何变化；它仍然是一个整数。
- en: 'As we anticipated, we can pass the transform directly as an argument to `dataset
    .CIFAR10`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，我们可以直接将转换作为参数传递给 `dataset .CIFAR10`：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'At this point, accessing an element of the dataset will return a tensor, rather
    than a PIL image:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，访问数据集的元素将返回一个张量，而不是一个 PIL 图像：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As expected, the shape has the channel as the first dimension, while the scalar
    type is `float32`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，形状的第一个维度是通道，标量类型是 `float32`：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Whereas the values in the original PIL image ranged from 0 to 255 (8 bits per
    channel), the `ToTensor` transform turns the data into a 32-bit floating-point
    per channel, scaling the values down from 0.0 to 1.0\. Let’s verify that:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 PIL 图像中的值范围从 0 到 255（每个通道 8 位），`ToTensor` 转换将数据转换为每个通道的 32 位浮点数，将值从 0.0 缩放到
    1.0。让我们验证一下：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And let’s verify that we’re getting the same image out:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们验证一下我们得到了相同的图像：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Changes the order of the axes from C × H × W to H × W × C
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 改变轴的顺序从 C × H × W 到 H × W × C
- en: As we can see in figure 7.4, we get the same output as before.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在图 7.4 中看到的，我们得到了与之前相同的输出。
- en: '![](../Images/CH07_F04_Stevens2_GS.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F04_Stevens2_GS.png)'
- en: Figure 7.4 We’ve seen this one already.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 我们已经见过这个。
- en: It checks. Note how we have to use `permute` to change the order of the axes
    from C × H × W to H × W × C to match what Matplotlib expects.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 检查通过。请注意，我们必须使用 `permute` 来改变轴的顺序，从 C × H × W 变为 H × W × C，以匹配 Matplotlib 的期望。
- en: 7.1.4 Normalizing data
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4 数据标准化
- en: 'Transforms are really handy because we can chain them using `transforms.Compose`,
    and they can handle normalization and data augmentation transparently, directly
    in the data loader. For instance, it’s good practice to normalize the dataset
    so that each channel has zero mean and unitary standard deviation. We mentioned
    this in chapter 4, but now, after going through chapter 5, we also have an intuition
    for why: by choosing activation functions that are linear around 0 plus or minus
    1 (or 2), keeping the data in the same range means it’s more likely that neurons
    have nonzero gradients and, hence, will learn sooner. Also, normalizing each channel
    so that it has the same distribution will ensure that channel information can
    be mixed and updated through gradient descent using the same learning rate. This
    is just like the situation in section 5.4.4 when we rescaled the weight to be
    of the same magnitude as the bias in our temperature-conversion model.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 转换非常方便，因为我们可以使用 `transforms.Compose` 链接它们，它们可以透明地处理标准化和数据增强，直接在数据加载器中进行。例如，标准化数据集是一个好习惯，使得每个通道具有零均值和单位标准差。我们在第
    4 章中提到过这一点，但现在，在经历了第 5 章之后，我们也对此有了直观的理解：通过选择在 0 加减 1（或 2）附近线性的激活函数，保持数据在相同范围内意味着神经元更有可能具有非零梯度，因此会更快地学习。此外，将每个通道标准化，使其具有相同的分布，将确保通道信息可以通过梯度下降混合和更新，使用相同的学习率。这就像在第
    5.4.4 节中，当我们将权重重新缩放为与温度转换模型中的偏差相同数量级时的情况。
- en: 'In order to make it so that each channel has zero mean and unitary standard
    deviation, we can compute the mean value and the standard deviation of each channel
    across the dataset and apply the following transform: `v_n[c] = (v[c] - mean[c])
    / stdev[c]`. This is what `transforms.Normalize` does. The values of `mean` and
    `stdev` must be computed offline (they are not computed by the transform). Let’s
    compute them for the CIFAR-10 training set.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使每个通道的均值为零，标准差为单位，我们可以计算数据集中每个通道的均值和标准差，并应用以下转换：`v_n[c] = (v[c] - mean[c])
    / stdev[c]`。这就是`transforms.Normalize`所做的。`mean`和`stdev`的值必须离线计算（它们不是由转换计算的）。让我们为CIFAR-10训练集计算它们。
- en: 'Since the CIFAR-10 dataset is small, we’ll be able to manipulate it entirely
    in memory. Let’s stack all the tensors returned by the dataset along an extra
    dimension:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CIFAR-10数据集很小，我们将能够完全在内存中操作它。让我们沿着额外的维度堆叠数据集返回的所有张量：
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we can easily compute the mean per channel:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以轻松地计算每个通道的均值：
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Recall that view(3, -1) keeps the three channels and merges all the remaining
    dimensions into one, figuring out the appropriate size. Here our 3 × 32 × 32 image
    is transformed into a 3 × 1,024 vector, and then the mean is taken over the 1,024
    elements of each channel.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 请记住，view(3, -1)保留了三个通道，并将所有剩余的维度合并成一个，找出适当的大小。这里我们的3 × 32 × 32图像被转换成一个3 ×
    1,024向量，然后对每个通道的1,024个元素取平均值。
- en: 'Computing the standard deviation is similar:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 计算标准差类似：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With these numbers in our hands, we can initialize the `Normalize` transform
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些数据，我们可以初始化`Normalize`转换
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'and concatenate it after the `ToTensor` transform:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 并在`ToTensor`转换后连接它：
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Note that, at this point, plotting an image drawn from the dataset won’t provide
    us with a faithful representation of the actual image:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这一点上，绘制从数据集中绘制的图像不会为我们提供实际图像的忠实表示：
- en: '[PRE17]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The renormalized red car we get is shown in figure 7.5\. This is because normalization
    has shifted the RGB levels outside the 0.0 to 1.0 range and changed the overall
    magnitudes of the channels. All of the data is still there; it’s just that Matplotlib
    renders it as black. We’ll keep this in mind for the future.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的重新归一化的红色汽车如图7.5所示。这是因为归一化已经将RGB级别移出了0.0到1.0的范围，并改变了通道的整体幅度。所有的数据仍然存在；只是Matplotlib将其渲染为黑色。我们将记住这一点以备将来参考。
- en: '![](../Images/CH07_F05_Stevens2_GS.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F05_Stevens2_GS.png)'
- en: Figure 7.5 Our random CIFAR-10 image after normalization
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 归一化后的随机CIFAR-10图像
- en: '![](../Images/CH07_F06_Stevens2_GS.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F06_Stevens2_GS.png)'
- en: 'Figure 7.6 The problem at hand: we’re going to help our friend tell birds from
    airplanes for her blog, by training a neural network to do the job.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 手头的问题：我们将帮助我们的朋友为她的博客区分鸟和飞机，通过训练一个神经网络来完成这项任务。
- en: Still, we have a fancy dataset loaded that contains tens of thousands of images!
    That’s quite convenient, because we were going to need something exactly like
    it.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们加载了一个包含成千上万张图片的花哨数据集！这非常方便，因为我们正需要这样的东西。
- en: 7.2 Distinguishing birds from airplanes
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 区分鸟和飞机
- en: Jane, our friend at the bird-watching club, has set up a fleet of cameras in
    the woods south of the airport. The cameras are supposed to save a shot when something
    enters the frame and upload it to the club’s real-time bird-watching blog. The
    problem is that a lot of planes coming and going from the airport end up triggering
    the camera, so Jane spends a lot of time deleting pictures of airplanes from the
    blog. What she needs is an automated system like that shown in figure 7.6\. Instead
    of manually deleting, she needs a neural network--an AI if we’re into fancy marketing
    speak--to throw away the airplanes right away.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 珍妮，我们在观鸟俱乐部的朋友，在机场南部的树林里设置了一组摄像头。当有东西进入画面时，摄像头应该保存一张照片并上传到俱乐部的实时观鸟博客。问题是，许多从机场进出的飞机最终触发了摄像头，所以珍妮花了很多时间从博客中删除飞机的照片。她需要的是一个像图7.6中所示的自动化系统。她需要一个神经网络--如果我们喜欢花哨的营销说辞，那就是人工智能--来立即丢弃飞机。
- en: No worries! We’ll take care of that, no problem--we just got the perfect dataset
    for it (what a coincidence, right?). We’ll pick out all the birds and airplanes
    from our CIFAR-10 dataset and build a neural network that can tell birds and airplanes
    apart.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心！我们会处理好的，没问题--我们刚好有了完美的数据集（多么巧合啊，对吧？）。我们将从我们的CIFAR-10数据集中挑选出所有的鸟和飞机，并构建一个可以区分鸟和飞机的神经网��。
- en: 7.2.1 Building the dataset
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 构建数据集
- en: 'The first step is to get the data in the right shape. We could create a `Dataset`
    subclass that only includes birds and airplanes. However, the dataset is small,
    and we only need indexing and `len` to work on our dataset. It doesn’t actually
    have to be a subclass of `torch.utils.data.dataset.Dataset`! Well, why not take
    a shortcut and just filter the data in `cifar10` and remap the labels so they
    are contiguous? Here’s how:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将数据整理成正确的形状。我们可以创建一个仅包含鸟和飞机的`Dataset`子类。然而，数据集很小，我们只需要在数据集上进行索引和`len`操作。它实际上不必是`torch.utils.data.dataset.Dataset`的子类！那么，为什么不简单地过滤`cifar10`中的数据并重新映射标签，使它们连续呢？下面是具体操作：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `cifar2` object satisfies the basic requirements for a `Dataset`--that is,
    `__len__` and `__getitem__` are defined--so we’re going to use that. We should
    be aware, however, that this is a clever shortcut and we might wish to implement
    a proper `Dataset` if we hit limitations with it.[⁴](#pgfId-1014113)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`cifar2`对象满足`Dataset`的基本要求--也就是说，`__len__`和`__getitem__`已经定义--所以我们将使用它。然而，我们应该意识到，这是一个聪明的捷径，如果我们在使用中遇到限制，我们可能希望实现一个合适的`Dataset`。[⁴](#pgfId-1014113)'
- en: We have a dataset! Next, we need a model to feed our data to.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有了数据集！接下来，我们需要一个模型来处理我们的数据。
- en: 7.2.2 A fully connected model
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 一个全连接的模型
- en: We learned how to build a neural network in chapter 5\. We know that it’s a
    tensor of features in, a tensor of features out. After all, an image is just a
    set of numbers laid out in a spatial configuration. OK, we don’t know how to handle
    the spatial configuration part just yet, but in theory if we just take the image
    pixels and straighten them into a long 1D vector, we could consider those numbers
    as input features, right? This is what figure 7.7 illustrates.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第5章学习了如何构建一个神经网络。我们知道它是一个特征的张量输入，一个特征的张量输出。毕竟，一幅图像只是以空间配置排列的一组数字。好吧，我们还不知道如何处理空间配置部分，但理论上，如果我们只是取图像像素并将它们展平成一个长的1D向量，我们可以将这些数字视为输入特征，对吧？这就是图7.7所说明的。
- en: '![](../Images/CH07_F07_Stevens2_GS.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F07_Stevens2_GS.png)'
- en: Figure 7.7 Treating our image as a 1D vector of values and training a fully
    connected classifier on it
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 将我们的图像视为一维值向量并在其上训练一个全连接分类器
- en: 'Let’s try that. How many features per sample? Well, 32 × 32 × 3: that is, 3,072
    input features per sample. Starting from the model we built in chapter 5, our
    new model would be an `nn.Linear` with 3,072 input features and some number of
    hidden features, followed by an activation, and then another `nn.Linear` that
    tapers the network down to an appropriate output number of features (2, for this
    use case):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试看。每个样本有多少特征？嗯，32 × 32 × 3：也就是说，每个样本有3072个输入特征。从我们在第5章构建的模型开始，我们的新模型将是一个具有3072个输入特征和一些隐藏特征数量的`nn.Linear`，然后是一个激活函数，然后是另一个将网络缩减到适当的输出特征数量（对于这种用例为2）的`nn.Linear`：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Input features
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入特征
- en: ❷ Hidden layer size
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 隐藏层大小
- en: ❸ Output classes
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输出类别
- en: We somewhat arbitrarily pick 512 hidden features. A neural network needs at
    least one hidden layer (of activations, so two modules) with a nonlinearity in
    between in order to be able to learn arbitrary functions in the way we discussed
    in section 6.3--otherwise, it would just be a linear model. The hidden features
    represent (learned) relations between the inputs encoded through the weight matrix.
    As such, the model might learn to “compare” vector elements 176 and 208, but it
    does not a priori focus on them because it is structurally unaware that these
    are, indeed (row 5, pixel 16) and
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有点随意地选择了512个隐藏特征。神经网络至少需要一个隐藏层（激活层，所以两个模块），中间需要一个非线性激活函数，以便能够学习我们在第6.3节中讨论的任意函数--否则，它将只是一个线性模型。隐藏特征表示（学习的）输入之间通过权重矩阵编码的关系。因此，模型可能会学习“比较”向量元素176和208，但它并不会事先关注它们，因为它在结构上不知道这些实际上是（第5行，第16像素）和
- en: (row 6, pixel 16), and thus adjacent.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (第6行，第16像素)，因此是相邻的。
- en: So we have a model. Next we’ll discuss what our model output should be.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有了一个模型。接下来我们将讨论我们模型的输出应该是什么。
- en: 7.2.3 Output of a classifier
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 分类器的输出
- en: 'In chapter 6, the network produced the predicted temperature (a number with
    a quantitative meaning) as output. We could do something similar here: make our
    network output a single scalar value (so `n_out = 1`), cast the labels to floats
    (0.0 for airplane and 1.0 for bird), and use those as a target for `MSELoss` (the
    average of squared differences in the batch). Doing so, we would cast the problem
    into a regression problem. However, looking more closely, we are now dealing with
    something a bit different in nature.[⁵](#pgfId-1014480)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章中，网络产生了预测的温度（具有定量意义的数字）作为输出。我们可以在这里做类似的事情：使我们的网络输出一个单一的标量值（所以`n_out = 1`），将标签转换为浮点数（飞机为0.0，鸟为1.0），并将其用作`MSELoss`的目标（批次中平方差的平均值）。这样做，我们将问题转化为一个回归问题。然而，更仔细地观察，我们现在处理的是一种性质有点不同的东西。
- en: 'We need to recognize that the output is categorical: it’s either a bird or
    an airplane (or something else if we had all 10 of the original classes). As we
    learned in chapter 4, when we have to represent a categorical variable, we should
    switch to a one-hot-encoding representation of that variable, such as `[1,` `0]`
    for airplane or `[0,` `1]` for bird (the order is arbitrary). This will still
    work if we have 10 classes, as in the full CIFAR-10 dataset; we’ll just have a
    vector of length 10.[⁶](#pgfId-1014553)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要认识到输出是分类的：它要么是飞机，要么是鸟（或者如果我们有所有10个原始类别的话，还可能是其他东西）。正如我们在第4章中学到的，当我们必须表示一个分类变量时，我们应该切换到该变量的一种独热编码表示，比如对于飞机是`[1,`
    `0]`，对于鸟是`[0,` `1]`（顺序是任意的）。如果我们有10个类别，如完整的CIFAR-10数据集，这仍然有效；我们将只有一个长度为10的向量。
- en: 'In the ideal case, the network would output `torch.tensor([1.0, 0.0])` for
    an airplane and `torch.tensor([0.0, 1.0])` for a bird. Practically speaking, since
    our classifier will not be perfect, we can expect the network to output something
    in between. The key realization in this case is that we can interpret our output
    as probabilities: the first entry is the probability of “airplane,” and the second
    is the probability of “bird.”'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，网络将为飞机输出`torch.tensor([1.0, 0.0])`，为鸟输出`torch.tensor([0.0, 1.0])`。实际上，由于我们的分类器不会是完美的，我们可以期望网络输出介于两者之间的值。在这种情况下的关键认识是，我们可以将输出解释为概率：第一个条目是“飞机”的概率，第二个是“鸟”的概率。
- en: 'Casting the problem in terms of probabilities imposes a few extra constraints
    on the outputs of our network:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 将问题转化为概率的形式对我们网络的输出施加了一些额外的约束：
- en: Each element of the output must be in the `[0.0, 1.0]` range (a probability
    of an outcome cannot be less than 0 or greater than 1).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出的每个元素必须在`[0.0, 1.0]`范围内（一个结果的概率不能小于0或大于1）。
- en: The elements of the output must add up to 1.0 (we’re certain that one of the
    two outcomes will occur).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出的元素必须加起来等于1.0（我们确定两个结果中的一个将会发生）。
- en: 'It sounds like a tough constraint to enforce in a differentiable way on a vector
    of numbers. Yet there’s a very smart trick that does exactly that, and it’s differentiable:
    it’s called *softmax*.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来像是在一个数字向量上以可微分的方式强制执行一个严格的约束。然而，有一个非常聪明的技巧正是做到了这一点，并且是可微分的：它被称为*softmax*。
- en: 7.2.4 Representing the output as probabilities
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 将���出表示为概率
- en: Softmax is a function that takes a vector of values and produces another vector
    of the same dimension, where the values satisfy the constraints we just listed
    to represent probabilities. The expression for softmax is shown in figure 7.8.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax是一个函数，它接受一个值向量并产生另一个相同维度的向量，其中值满足我们刚刚列出的表示概率的约束条件。Softmax的表达式如图7.8所示。
- en: '![](../Images/CH07_F08_Stevens2_GS.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F08_Stevens2_GS.png)'
- en: Figure 7.8 Handwritten softmax
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 手写softmax
- en: 'That is, we take the elements of the vector, compute the elementwise exponential,
    and divide each element by the sum of exponentials. In code, it’s something like
    this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们取向量的元素，计算元素的指数，然后将每个元素除以指数的总和。在代码中，就像这样：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s test it on an input vector:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个输入向量上测试一下：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As expected, it satisfies the constraints on probability:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，它满足概率的约束条件：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Softmax is a monotone function, in that lower values in the input will correspond
    to lower values in the output. However, it’s not *scale invariant*, in that the
    ratio between values is not preserved. In fact, the ratio between the first and
    second elements of the input is 0.5, while the ratio between the same elements
    in the output is 0.3678\. This is not a real issue, since the learning process
    will drive the parameters of the model in a way that values have appropriate ratios.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax是一个单调函数，即输入中的较低值将对应于输出中的较低值。然而，它不是*尺度不变*的，即值之间的比率不被保留。事实上，输入的第一个和第二个元素之间的比率为0.5，而输出中相同元素之间的比率为0.3678。这并不是一个真正的问题，因为学习过程将以适当的比率调整模型的参数。
- en: 'The `nn` module makes softmax available as a module. Since, as usual, input
    tensors may have an additional batch 0th dimension, or have dimensions along which
    they encode probabilities and others in which they don’t, `nn.Softmax` requires
    us to specify the dimension along which the softmax function is applied:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn`模块将softmax作为一个模块提供。由于通常输入张量可能具有额外的批次第0维，或者具有编码概率的维度和其他维度，`nn.Softmax`要求我们指定应用softmax函数的维度：'
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this case, we have two input vectors in two rows (just like when we work
    with batches), so we initialize `nn.Softmax` to operate along dimension 1.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有两个输入向量在两行中（就像我们处理批次时一样），因此我们初始化`nn.Softmax`以沿着第1维操作。
- en: 'Excellent! We can now add a softmax at the end of our model, and our network
    will be equipped to produce probabilities:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们现在可以在模型末尾添加一个softmax，这样我们的网络就能够生成概率：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can actually try running the model before even training it. Let’s do it,
    just to see what comes out. We first build a batch of one image, our bird (figure
    7.9):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以在甚至训练模型之前尝试运行模型。让我们试试，看看会得到什么。我们首先构建一个包含一张图片的批次，我们的鸟（图 7.9）：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/CH07_F09_Stevens2_GS.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F09_Stevens2_GS.png)'
- en: Figure 7.9 A random bird from the CIFAR-10 dataset (after normalization)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 CIFAR-10数据集中的一只随机鸟（归一化后）
- en: 'Oh, hello there. In order to call the model, we need to make the input have
    the right dimensions. We recall that our model expects 3,072 features in the input,
    and that `nn` works with data organized into batches along the zeroth dimension.
    So we need to turn our 3 × 32 × 32 image into a 1D tensor and then add an extra
    dimension in the zeroth position. We learned how to do this in chapter 3:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，你好。为了调用模型，我们需要使输入具有正确的维度。我们记得我们的模型期望输入中有3,072个特征，并且`nn`将数据组织成沿着第零维的批次。因此，我们需要将我们的3
    × 32 × 32图像转换为1D张量，然后在第零位置添加一个额外的维度。我们在第3章学习了如何做到这一点：
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we’re ready to invoke our model:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备调用我们的模型：
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'So, we got probabilities! Well, we know we shouldn’t get too excited: the weights
    and biases of our linear layers have not been trained at all. Their elements are
    initialized randomly by PyTorch between -1.0 and 1.0\. Interestingly, we also
    see `grad_fn` for the output, which is the tip of the backward computation graph
    (it will be used as soon as we need to backpropagate).[⁷](#pgfId-1015202)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们得到了概率！好吧，我们知道我们不应该太兴奋：我们的线性层的权重和偏置根本没有经过训练。它们的元素由PyTorch在-1.0和1.0之间随机初始化。有趣的是，我们还看到输出的`grad_fn`，这是反向计算图的顶点（一旦我们需要反��传播时将被使用）。
- en: 'In addition, while we know which output probability is supposed to be which
    (recall our `class_names`), our network has no indication of that. Is the first
    entry “airplane” and the second “bird,” or the other way around? The network can’t
    even tell that at this point. It’s the loss function that associates a meaning
    with these two numbers, after backpropagation. If the labels are provided as index
    0 for “airplane” and index 1 for “bird,” then that’s the order the outputs will
    be induced to take. Thus, after training, we will be able to get the label as
    an index by computing the *argmax* of the output probabilities: that is, the index
    at which we get the maximum probability. Conveniently, when supplied with a dimension,
    `torch.max` returns the maximum element along that dimension as well as the index
    at which that value occurs. In our case, we need to take the max along the probability
    vector (not across batches), therefore, dimension 1:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，虽然我们知道哪个输出概率应该是哪个（回想一下我们的`class_names`），但我们的网络并没有这方面的指示。第一个条目是“飞机”，第二个是“鸟”，还是反过来？在这一点上，网络甚至无法判断。正是损失函数在反向传播后将这两个数字关联起来。如果标签提供为“飞机”索引0和“鸟”索引1，那么输出将被诱导采取这个顺序。因此，在训练后，我们将能够通过计算输出概率的*argmax*来获得标签：也就是说，我们获得最大概率的索引。方便的是，当提供一个维度时，`torch.max`会返回沿着该维度的最大元素以及该值出现的索引。在我们的情况下，我们需要沿着概率向量（而不是跨批次）取最大值，因此是第1维：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It says the image is a bird. Pure luck. But we have adapted our model output
    to the classification task at hand by getting it to output probabilities. We also
    have now run our model against an input image and verified that our plumbing works.
    Time to get training. As in the previous two chapters, we need a loss to minimize
    during training.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 它说这张图片是一只鸟。纯属运气。但我们通过让模型输出概率来适应手头的分类任务，现在我们已经运行了我们的模型对输入图像进行验证，确保我们的管道正常工作。是时候开始训练了。与前两章一样，我们在训练过程中需要最小化的损失。
- en: 7.2.5 A loss for classifying
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.5 用于分类的损失
- en: We just mentioned that the loss is what gives probabilities meaning. In chapters
    5 and 6, we used mean square error (MSE) as our loss. We could still use MSE and
    make our output probabilities converge to `[0.0, 1.0]` and `[1.0, 0.0]`. However,
    thinking about it, we’re not really interested in reproducing these values exactly.
    Looking back at the argmax operation we used to extract the index of the predicted
    class, what we’re really interested in is that the first probability is higher
    than the second for airplanes and vice versa for birds. In other words, we want
    to penalize misclassifications rather than painstakingly penalize everything that
    doesn’t look exactly like a 0.0 or 1.0.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚提到损失是给概率赋予意义的。在第5和第6章中，我们使用均方误差（MSE）作为我们的损失。我们仍然可以使用MSE，并使我们的输出概率收敛到`[0.0,
    1.0]`和`[1.0, 0.0]`。然而，仔细想想，我们并不真正关心精确复制这些值。回顾我们用于提取预测类别索引的argmax操作，我们真正感兴趣的是第一个概率对于飞机而言比第二个更高，对于鸟而言则相反。换句话说，我们希望惩罚错误分类，而不是费力地惩罚一切看起来不完全像0.0或1.0的东西。
- en: 'What we need to maximize in this case is the probability associated with the
    correct class, `out[class_index]`, where `out` is the output of softmax and `class_index`
    is a vector containing 0 for “airplane” and 1 for “bird” for each sample. This
    quantity--that is, the probability associated with the correct class--is referred
    to as the *likelihood* (of our model’s parameters, given the data).[⁸](#pgfId-1015417)
    In other words, we want a loss function that is very high when the likelihood
    is low: so low that the alternatives have a higher probability. Conversely, the
    loss should be low when the likelihood is higher than the alternatives, and we’re
    not really fixated on driving the probability up to 1.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要最大化的是与正确类别相关联的概率，`out[class_index]`，其中`out`是softmax的输出，`class_index`是一个包含0表示“飞机”和1表示“鸟”的向量，对于每个样本。这个数量--即与正确类别相关联的概率--被称为*似然度*（给定数据的模型参数的）。换句话说，我们希望一个损失函数在似然度低时非常高：低到其他选择具有更高的概率。相反，当似然度高于其他选择时，损失应该很低，我们并不真正固执于将概率提高到1。
- en: There’s a loss function that behaves that way, and it’s called *negative log
    likelihood* (NLL). It has the expression `NLL = - sum(log(out_i[c_i]))`, where
    the sum is taken over *N* samples and `c_i` is the correct class for sample *i*.
    Let’s take a look at figure 7.10, which shows the NLL as a function of predicted
    probability.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个表现出这种行为的损失函数，称为*负对数似然*（NLL）。它的表达式为`NLL = - sum(log(out_i[c_i]))`，其中求和是针对*N*个样本，`c_i`是样本*i*的正确类别。让我们看一下图7.10，它显示了NLL作为预测概率的函数。
- en: '![](../Images/CH07_F10_Stevens2_GS.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F10_Stevens2_GS.png)'
- en: Figure 7.10 The NLL loss as a function of the predicted probabilities
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 预测概率的NLL损失函数
- en: The figure shows that when low probabilities are assigned to the data, the NLL
    grows to infinity, whereas it decreases at a rather shallow rate when probabilities
    are greater than 0.5\. Remember that the NLL takes probabilities as input; so,
    as the likelihood grows, the other probabilities will necessarily decrease.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示，当数据被分配低概率时，NLL增长到无穷大，而当概率大于0.5时，它以相对缓慢的速度下降。记住，NLL以概率作为输入；因此，随着可能性增加，其他概率必然会减少。
- en: 'Summing up, our loss for classification can be computed as follows. For each
    sample in the batch:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们的分类损失可以计算如下。对于批次中的每个样本：
- en: Run the forward pass, and obtain the output values from the last (linear) layer.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行���向传播，并从最后（线性）层获取输出值。
- en: Compute their softmax, and obtain probabilities.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算它们的softmax，并获得概率。
- en: Take the predicted probability corresponding to the correct class (the likelihood
    of the parameters). Note that we know what the correct class is because it’s a
    supervised problem--it’s our ground truth.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取与正确类别对应的预测概率（参数的似然度）。请注意，我们知道正确类别是什么，因为这是一个监督问题--这是我们的真实值。
- en: Compute its logarithm, slap a minus sign in front of it, and add it to the loss.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算其对数，加上一个负号，并将其添加到损失中。
- en: 'So, how do we do this in PyTorch? PyTorch has an `nn.NLLLoss` class. However
    (gotcha ahead), as opposed to what you might expect, it does not take probabilities
    but rather takes a tensor of log probabilities as input. It then computes the
    NLL of our model given the batch of data. There’s a good reason behind the input
    convention: taking the logarithm of a probability is tricky when the probability
    gets close to zero. The workaround is to use `nn.LogSoftmax` instead of `nn.Softmax`,
    which takes care to make the calculation numerically stable.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在PyTorch中我们如何做到这一点呢？PyTorch有一个`nn.NLLLoss`类。然而（注意），与您可能期望的相反，它不接受概率，而是接受对数概率的张量作为输入。然后，它计算给定数据批次的我们模型的NLL。这种输入约定背后有一个很好的原因：当概率接近零时，取对数是棘手的。解决方法是使用`nn.LogSoftmax`而不是`nn.Softmax`，后者会确保计算在数值上是稳定的。
- en: 'We can now modify our model to use `nn.LogSoftmax` as the output module:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以修改我们的模型，使用`nn.LogSoftmax`作为输出模块：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then we instantiate our NLL loss:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们实例化我们的NLL损失：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The loss takes the output of `nn.LogSoftmax` for a batch as the first argument
    and a tensor of class indices (zeros and ones, in our case) as the second argument.
    We can now test it with our birdie:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 损失将`nn.LogSoftmax`的输出作为批次的第一个参数，并将类别索引的张量（在我们的情况下是零和一）作为第二个参数。现在我们可以用我们的小鸟来测试它：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Ending our investigation of losses, we can look at how using cross-entropy loss
    improves over MSE. In figure 7.11, we see that the cross-entropy loss has some
    slope when the prediction is off target (in the low-loss corner, the correct class
    is assigned a predicted probability of 99.97%), while the MSE we dismissed at
    the beginning saturates much earlier and--crucially--also for very wrong predictions.
    The underlying reason is that the slope of the MSE is too low to compensate for
    the flatness of the softmax function for wrong predictions. This is why the MSE
    for probabilities is not a good fit for classification work.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 结束我们对损失的研究，我们可以看看使用交叉熵损失如何改善均方误差。在图7.11中，我们看到当预测偏离目标时，交叉熵损失有一些斜率（在低损失角落，正确类别被分配了预测概率为99.97%），而我们在开始时忽略的均方误差更早饱和，关键是对于非常错误的预测也是如此。其根本原因是均方误差的斜率太低，无法弥补错误预测的softmax函数的平坦性。这就是为什么概率的均方误差不适用于分类工作。
- en: '![](../Images/CH07_F11_Stevens2_GS.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F11_Stevens2_GS.png)'
- en: Figure 7.11 The cross entropy (left) and MSE between predicted probabilities
    and the target probability vector (right) as functions of the predicted scores--that
    is, before the (log-) softmax
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 预测概率与目标概率向量之间的交叉熵（左）和均方误差（右）作为预测分数的函数--也就是在（对数）softmax之前
- en: 7.2.6 Training the classifier
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.6 训练分类器
- en: 'All right! We’re ready to bring back the training loop we wrote in chapter
    5 and see how it trains (the process is illustrated in figure 7.12):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 好了！我们准备好重新引入我们在第5章写的训练循环，并看看它是如何训练的（过程如图7.12所示）：
- en: '[PRE32]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Prints the loss for the last image. In the next chapter, we will improve our
    output to give an average over the entire epoch.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打印最后一张图像的损失。在下一章中，我们将改进我们的输���，以便给出整个时代的平均值。
- en: '![](../Images/CH07_F12_Stevens2_GS.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F12_Stevens2_GS.png)'
- en: 'Figure 7.12 Training loops: (A) averaging updates over the whole dataset; (B)
    updating the model at each sample; (C) averaging updates over minibatches'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 训练循环：（A）对整个数据集进行平均更新；（B）在每个样本上更新模型；（C）对小批量进行平均更新
- en: 'Looking more closely, we made a small change to the training loop. In chapter
    5, we had just one loop: over the epochs (recall that an epoch ends when all samples
    in the training set have been evaluated). We figured that evaluating all 10,000
    images in a single batch would be too much, so we decided to have an inner loop
    where we evaluate one sample at a time and backpropagate over that single sample.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 更仔细地看，我们对训练循环进行了一点改变。在第5章，我们只有一个循环：在时代上（回想一下，一个时代在所有训练集中的样本都被评估完时结束）。我们认为在一个批次中评估所有10,000张图像会太多，所以我们决定有一个内部循环，在那里我们一次评估一个样本并在该单个样本上进行反向传播。
- en: 'While in the first case the gradient is accumulated over all samples before
    being applied, in this case we apply changes to parameters based on a very partial
    estimation of the gradient on a single sample. However, what is a good direction
    for reducing the loss based on one sample might not be a good direction for others.
    By shuffling samples at each epoch and estimating the gradient on one or (preferably,
    for stability) a few samples at a time, we are effectively introducing randomness
    in our gradient descent. Remember SGD? It stands for *stochastic gradient descent*,
    and this is what the *S* is about: working on small batches (aka minibatches)
    of shuffled data. It turns out that following gradients estimated over minibatches,
    which are poorer approximations of gradients estimated across the whole dataset,
    helps convergence and prevents the optimization process from getting stuck in
    local minima it encounters along the way. As depicted in figure 7.13, gradients
    from minibatches are randomly off the ideal trajectory, which is part of the reason
    why we want to use a reasonably small learning rate. Shuffling the dataset at
    each epoch helps ensure that the sequence of gradients estimated over minibatches
    is representative of the gradients computed across the full dataset.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，梯度在应用之前被累积在所有样本上，而在这种情况下，我们基于单个样本上梯度的非常部分估计来应用参数的变化。然而，基于一个样本减少损失的好方向可能不适用于其他样本。通过在每个时代对样本进行洗牌并在一次或（最好是为了稳定性）几个样本上估计梯度，我们有效地在梯度下降中引入了随机性。记得随机梯度下降（SGD）吗？这代表*随机梯度下降*，这就是*S*的含义：在洗牌数据的小批量（又称小批量）上工作。事实证明，遵循在小批量上估计的梯度，这些梯度是对整个数据集估计的梯度的较差近似，有助于收敛并防止优化过程在途中遇到的局部最小值中卡住。正如图7.13所示，来自小批量的梯度随机偏离理想轨迹，这也是为什么我们希望使用相当小的学习率的部分原因。在每个时代对数据集进行洗牌有助于确保在小批量上估计的梯度序列代表整个数据集上计算的梯度。
- en: Typically, minibatches are a constant size that we need to set prior to training,
    just like the learning rate. These are called *hyperparameters*, to distinguish
    them from the parameters of a model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，小批量是一个在训练之前需要设置的固定大小，就像学习率一样。这些被称为*超参数*，以区别于模型的参数。
- en: '![](../Images/CH07_F13_Stevens2_GS.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F13_Stevens2_GS.png)'
- en: Figure 7.13 Gradient descent averaged over the whole dataset (light path) versus
    stochastic gradient descent, where the gradient is estimated on randomly picked
    minibatches
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 梯度下降在整个数据集上的平均值（浅色路径）与随机梯度下降，其中梯度是在随机选择的小批量上估计的。
- en: 'In our training code, we chose minibatches of size 1 by picking one item at
    a time from the dataset. The `torch.utils.data` module has a class that helps
    with shuffling and organizing the data in minibatches: `DataLoader`. The job of
    a data loader is to sample minibatches from a dataset, giving us the flexibility
    to choose from different sampling strategies. A very common strategy is uniform
    sampling after shuffling the data at each epoch. Figure 7.14 shows the data loader
    shuffling the indices it gets from the `Dataset`.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练代码中，我们选择了大小为1的小批量，一次从数据集中选择一个项目。`torch.utils.data`模块有一个帮助对数据进行洗牌和组织成小批量的类：`DataLoader`。数据加载器的工作是从数据集中抽样小批量，使我们能够选择不同的抽样策略。一个非常常见的策略是在每个时代洗牌数据后进行均匀抽样。图7.14显示了数据加载器对从`Dataset`获取的索引进行洗牌的过程。
- en: '![](../Images/CH07_F14_Stevens2_GS.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F14_Stevens2_GS.png)'
- en: Figure 7.14 A data loader dispensing minibatches by using a dataset to sample
    individual data items
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 通过使用数据集来采样单个数据项来分发小批量数据的数据加载器
- en: 'Let’s see how this is done. At a minimum, the `DataLoader` constructor takes
    a `Dataset` object as input, along with `batch_size` and a `shuffle` Boolean that
    indicates whether the data needs to be shuffled at the beginning of each epoch:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是如何完成的。至少，`DataLoader`构造函数需要一个`Dataset`对象作为输入，以及`batch_size`和一个布尔值`shuffle`，指示数据是否需要在每个epoch开始时进行洗牌：
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'A `DataLoader` can be iterated over, so we can use it directly in the inner
    loop of our new training code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataLoader`可以被迭代，因此我们可以直接在新训练代码的内部循环中使用它：'
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Due to the shuffling, this now prints the loss for a random batch--clearly
    something we want to improve in chapter 8
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于洗牌，现在这会打印一个随机批次的损失--显然这是我们在第8章想要改进的地方
- en: At each inner iteration, `imgs` is a tensor of size 64 × 3 × 32 × 32--that is,
    a minibatch of 64 (32 × 32) RGB images--while `labels` is a tensor of size 64
    containing label indices.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个内部迭代中，`imgs`是一个大小为64 × 3 × 32 × 32的张��--也就是说，64个（32 × 32）RGB图像的小批量--而`labels`是一个包含标签索引的大小为64的张量。
- en: 'Let’s run our training:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行我们的训练：
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We see that the loss decreases somehow, but we have no idea whether it’s low
    enough. Since our goal here is to correctly assign classes to images, and preferably
    do that on an independent dataset, we can compute the accuracy of our model on
    the validation set in terms of the number of correct classifications over the
    total:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到损失有所下降，但我们不知道是否足够低。由于我们的目标是正确地为图像分配类别，并最好在一个独立的数据集上完成，我们可以计算我们模型在验证集上的准确率，即正确分类的数量占总数的比例：
- en: '[PRE36]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Not a great performance, but quite a lot better than random. In our defense,
    our model was quite a shallow classifier; it’s a miracle that it worked at all.
    It did because our dataset is really simple--a lot of the samples in the two classes
    likely have systematic differences (such as the color of the background) that
    help the model tell birds from airplanes, based on a few pixels.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 不是很好的性能，但比随机好得多。为我们辩护，我们的模型是一个相当浅的分类器；奇迹的是它居然工作了。这是因为我们的数据集非常简单--两类样本中很多样本可能有系统性差异（比如背景颜色），这有助于模型根据少量像素区分鸟类和飞机。
- en: We can certainly add some bling to our model by including more layers, which
    will increase the model’s depth and capacity. One rather arbitrary possibility
    is
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加更多的层来为我们的模型增加一些亮点，这将增加模型的深度和容量。一个相当任意的可能性是
- en: '[PRE37]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here we are trying to taper the number of features more gently toward the output,
    in the hope that intermediate layers will do a better job of squeezing information
    in increasingly shorter intermediate outputs.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们试图将特征数量逐渐缓和到输出，希望中间层能更好地将信息压缩到越来越短的中间输出中。
- en: The combination of `nn.LogSoftmax` and `nn.NLLLoss` is equivalent to using `nn.CrossEntropyLoss`.
    This terminology is a particularity of PyTorch, as the `nn.NLLoss` computes, in
    fact, the cross entropy but with log probability predictions as inputs where `nn.CrossEntropyLoss`
    takes scores (sometimes called *logits*). Technically, `nn.NLLLoss` is the cross
    entropy between the Dirac distribution, putting all mass on the target, and the
    predicted distribution given by the log probability inputs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.LogSoftmax`和`nn.NLLLoss`的组合等效于使用`nn.CrossEntropyLoss`。这个术语是PyTorch的一个特殊之处，因为`nn.NLLoss`实际上计算交叉熵，但输入是对数概率预测，而`nn.CrossEntropyLoss`采用分数（有时称为*对数几率*）。从技术上讲，`nn.NLLLoss`是Dirac分布之间的交叉熵，将所有质量放在目标上，并且由对数概率输入给出的预测分布。'
- en: To add to the confusion, in information theory, up to normalization by sample
    size, this cross entropy can be interpreted as a negative log likelihood of the
    predicted distribution under the target distribution as an outcome. So both losses
    are the negative log likelihood of the model parameters given the data when our
    model predicts the (softmax-applied) probabilities. In this book, we won’t rely
    on these details, but don’t let the PyTorch naming confuse you when you see the
    terms used in the literature.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加混乱，在信息理论中，这个交叉熵可以被解释为预测分布在目标分布下的负对数似然，经过样本大小归一化。因此，这两种损失都是模型参数的负对数似然，给定数据时，我们的模型预测（应用softmax后的）概率。在本书中，我们不会依赖这些细节，但当你在文献中看到这些术语时，不要让PyTorch的命名混淆你。
- en: 'It is quite common to drop the last `nn.LogSoftmax` layer from the network
    and use `nn.CrossEntropyLoss` as a loss. Let us try that:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会从网络中删除最后一个`nn.LogSoftmax`层，并使用`nn.CrossEntropyLoss`作为损失函数。让我们试试：
- en: '[PRE38]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Note that the numbers will be *exactly* the same as with `nn.LogSoftmax` and
    `nn.NLLLoss`. It’s just more convenient to do it all in one pass, with the only
    gotcha being that the output of our model will not be interpretable as probabilities
    (or log probabilities). We’ll need to explicitly pass the output through a softmax
    to obtain those.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数字将与`nn.LogSoftmax`和`nn.NLLLoss`完全相同。只是一次性完成所有操作更方便，唯一需要注意的是，我们模型的输出将无法解释为概率（或对数概率）。我们需要明确通过softmax传递输出以获得这些概率。
- en: Training this model and evaluating the accuracy on the validation set (0.802000)
    lets us appreciate that a larger model bought us an increase in accuracy, but
    not that much. The accuracy on the training set is practically perfect (0.998100).
    What is this telling us? That we are overfitting our model in both cases. Our
    fully connected model is finding a way to discriminate birds and airplanes on
    the training set by memorizing the training set, but performance on the validation
    set is not all that great, even if we choose a larger model.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个模型并在验证集上评估准确率（0.802000）让我们意识到，一个更大的模型带来了准确率的提高，但并不多。训练集上的准确率几乎完美（0.998100）。这告诉我们什么？我们在两种情况下都过度拟合了我们的模型。我们的全连接模型通过记忆训练集来找到区分鸟类和飞机的方法，但在验证集上的表现并不是很好，即使我们选择了一个更大的模型。
- en: 'PyTorch offers a quick way to determine how many parameters a model has through
    the `parameters()` method of `nn.Model` (the same method we use to provide the
    parameters to the optimizer). To find out how many elements are in each tensor
    instance, we can call the `numel` method. Summing those gives us our total count.
    Depending on our use case, counting parameters might require us to check whether
    a parameter has `requires_grad` set to `True`, as well. We might want to differentiate
    the number of *trainable* parameters from the overall model size. Let’s take a
    look at what we have right now:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch通过`nn.Model`的`parameters()`方法（我们用来向优化器提供参数的相同方法）提供了一种快速确定模型有多少参数的方法。要找出每个张量实例中有多少元素，我们可以调用`numel`方法。将它们相加就得到了我们的总数。根据我们的用例，计算参数可能需要我们检查参数是否将`requires_grad`设置为`True`。我们可能希望区分*可训练*参数的数量与整个模型大小。让我们看看我们现在有什么：
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Wow, 3.7 million parameters! Not a small network for such a small input image,
    is it? Even our first network was pretty large:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，370万个参数！对于这么小的输入图像来说，这不是一个小网络，是吗？即使我们的第一个网络也相当庞大：
- en: '[PRE40]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The number of parameters in our first model is roughly half that in our latest
    model. Well, from the list of individual parameter sizes, we start having an idea
    what’s responsible: the first module, which has 1.5 million parameters. In our
    full network, we had 1,024 output features, which led the first linear module
    to have 3 million parameters. This shouldn’t be unexpected: we know that a linear
    layer computes `y = weight * x + bias`, and if `x` has length 3,072 (disregarding
    the batch dimension for simplicity) and `y` must have length 1,024, then the `weight`
    tensor needs to be of size 1,024 × 3,072 and the `bias` size must be 1,024\. And
    1,024 * 3,072 + 1,024 = 3,146,752, as we found earlier. We can verify these quantities
    directly:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一个模型中的参数数量大约是最新模型的一半。嗯，从单个参数大小的列表中，我们开始有了一个想法：第一个模块有150万个参数。在我们的完整网络中，我们有1,024个输出特征，这导致第一个线性模块有3百万个参数。这不应该出乎意料：我们知道线性层计算`y
    = weight * x + bias`，如果`x`的长度为3,072（为简单起见忽略批处理维度），而`y`必须具有长度1,024，则`weight`张量的大小需要为1,024
    × 3,072，`bias`大小必须为1,024。而1,024 * 3,072 + 1,024 = 3,146,752，正如我们之前发现的那样。我们可以直接验证这些数量：
- en: '[PRE41]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: What is this telling us? That our neural network won’t scale very well with
    the number of pixels. What if we had a 1,024 × 1,024 RGB image? That’s 3.1 million
    input values. Even abruptly going to 1,024 hidden features (which is not going
    to work for our classifier), we would have over 3 *billion* parameters. Using
    32-bit floats, we’re already at 12 GB of RAM, and we haven’t even hit the second
    layer, much less computed and stored the gradients. That’s just not going to fit
    on most present-day GPUs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们什么？我们的神经网络随着像素数量的增加不会很好地扩展。如果我们有一个1,024 × 1,024的RGB图像呢？那就是3.1百万个输入值。即使突然转向1,024个隐藏特征（这对我们的分类器不起作用），我们将有超过30亿个参数。使用32位浮点数，我们已经占用了12
    GB的内存，甚至还没有到达第二层，更不用说计算和存储梯度了。这在大多数现代GPU上根本无法容纳。
- en: 7.2.7 The limits of going fully connected
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.7 完全连接的极限
- en: Let’s reason about what using a linear module on a 1D view of our image entails--figure
    7.15 shows what is going on. It’s like taking every single input value--that is,
    every single component in our RGB image--and computing a linear combination of
    it with all the other values for every output feature. On one hand, we are allowing
    for the combination of any pixel with every other pixel in the image being potentially
    relevant for our task. On the other hand, we aren’t utilizing the relative position
    of neighboring or far-away pixels, since we are treating the image as one big
    vector of numbers.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们推理一下在图像的1D视图上使用线性模块意味着什么--图7.15展示了正在发生的事情。这就像是将每个输入值--也就是我们RGB图像中的每个分量--与每个输出特征的所有其他值进行线性组合。一方面，我们允许任何像素与图像中的每个其他像素进行组合，这可能与我们的任务相关。另一方面，我们没有利用相邻或远离像素的相对位置，因为我们将图像视为一个由数字组成的大向量。
- en: '![](../Images/CH07_F15_Stevens2_GS.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F15_Stevens2_GS.png)'
- en: 'Figure 7.15 Using a fully connected module with an input image: every input
    pixel is combined with every other to produce each element in the output.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 使用带有输入图像的全连接模块：每个输入像素与其他每个像素组合以生成输出中的每个元素。
- en: 'An airplane flying in the sky captured in a 32 × 32 image will be very roughly
    similar to a dark, cross-like shape on a blue background. A fully connected network
    as in figure 7.15 would need to learn that when pixel 0,1 is dark, pixel 1,1 is
    also dark, and so on, that’s a good indication of an airplane. This is illustrated
    in the top half of figure 7.16\. However, shift the same airplane by one pixel
    or more as in the bottom half of the figure, and the relationships between pixels
    will have to be relearned from scratch: this time, an airplane is likely when
    pixel 0,2 is dark, pixel 1,2 is dark, and so on. In more technical terms, a fully
    connected network is not *translation invariant*. This means a network that has
    been trained to recognize a Spitfire starting at position 4,4 will not be able
    to recognize the *exact same* Spitfire starting at position 8,8\. We would then
    have to *augment* the dataset--that is, apply random translations to images during
    training--so the network would have a chance to see Spitfires all over the image,
    and we would need to do this for every image in the dataset (for the record, we
    could concatenate a transform from `torchvision.transforms` to do this transparently).
    However, this *data augmentation* strategy comes at a cost: the number of hidden
    features--that is, of parameters--must be large enough to store the information
    about all of these translated replicas.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个 32 × 32 图像中捕捉到的飞机在蓝色背景上将非常粗略地类似于一个黑色的十字形状。如图 7.15 中的全连接网络需要学习，当像素 0,1 是黑色时，像素
    1,1 也是黑色，依此类推，这是飞机的一个很好的指示。这在图 7.16 的上半部分有所说明。然而，将相同的飞机向下移动一个像素或更多像图的下半部分一样，像素之间的关系将不得不从头开始重新学习：这次，当像素
    0,2 是黑色时，像素 1,2 是黑色，依此类推时，飞机很可能存在。更具体地说，全连接网络不是*平移不变*的。这意味着一个经过训练以识别从位置 4,4 开始的斯皮特火机的网络将无法识别*完全相同*的从位置
    8,8 开始的斯皮特火机。然后，我们必须*增广*数据集--也就是在训练过程中对图像应用随机平移--以便网络有机会在整个图像中看到斯皮特火机，我们需要对数据集中的每个图像都这样做（值得一提的是，我们可以连接一个来自`torchvision.transforms`的转换来透明地执行此操作）。然而，这种*数据增广*策略是有代价的：隐藏特征的数量--也就是参数的数量--必须足够大，以存储关于所有这些平移副本的信息。
- en: '![](../Images/CH07_F16_Stevens2_GS.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F16_Stevens2_GS.png)'
- en: Figure 7.16 Translation invariance, or the lack thereof, with fully connected
    layers
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 全连接层中的平移不变性或缺乏平移不变性
- en: So, at the end of this chapter, we have a dataset, a model, and a training loop,
    and our model learns. However, due to a mismatch between our problem and our network
    structure, we end up overfitting our training data, rather than learning the generalized
    features of what we want the model to detect.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章结束时，我们有了一个数据集，一个模型和一个训练循环，我们的模型学习了。然而，由于我们的问题与网络结构之间存在不匹配，我们最终过拟合了训练数据，而不是学习我们希望模型检测到的泛化特征。
- en: We’ve created a model that allows for relating every pixel to every other pixel
    in the image, regardless of their spatial arrangement. We have a reasonable assumption
    that pixels that are closer together are in theory a lot more related, though.
    This means we are training a classifier that is not translation-invariant, so
    we’re forced to use a lot of capacity for learning translated replicas if we want
    to hope to do well on the validation set. There has to be a better way, right?
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了一个模型，允许将图像中的每个像素与其他像素相关联，而不考虑它们的空间排列。我们有一个合理的假设，即更接近的像素在理论上更相关。这意味着我们正在训练一个不具有平移不变性的分类器，因此如果我们希望在验证集上表现良好，我们被迫使用大量容量来学习平移副本。肯定有更好的方法，对吧？
- en: Of course, most such questions in a book like this are rhetorical. The solution
    to our current set of problems is to change our model to use convolutional layers.
    We’ll cover what that means in the next chapter.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，像这样的问题在这本书中大多是修辞性的。解决我们当前一系列问题的方法是改变我们的模型，使用卷积层。我们将在下一章中介绍这意味着什么。
- en: 7.3 Conclusion
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 结论
- en: In this chapter, we have solved a simple classification problem from dataset,
    to model, to minimizing an appropriate loss in a training loop. All of these things
    will be standard tools for your PyTorch toolbelt, and the skills needed to use
    them will be useful throughout your PyTorch tenure.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解决了一个简单的分类问题，从数据集到模型，再到在训练循环中最小化适当的损失。所有这些都将成为你的 PyTorch 工具箱中的标准工具，并且使用它们所需的技能将在你使用
    PyTorch 的整个期间都很有用。
- en: 'We’ve also found a severe shortcoming of our model: we have been treating 2D
    images as 1D data. Also, we do not have a natural way to incorporate the translation
    invariance of our problem. In the next chapter, you’ll learn how to exploit the
    2D nature of image data to get much better results.[⁹](#pgfId-1017597)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还发现了我们模型的一个严重缺陷：我们一直将 2D 图像视为 1D 数据。此外，我们没有一种自然的方法来融入我们问题的平移不变性。在下一章中，您将学习如何利用图像数据的
    2D 特性以获得更好的结果。[⁹](#pgfId-1017597)
- en: We could use what we have learned right away to process data without this translation
    invariance. For example, using it on tabular data or the time-series data we met
    in chapter 4, we can probably do great things already. To some extent, it would
    also be possible to use it on text data that is appropriately represented.[^(10)](#pgfId-1017613)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即利用所学知识处理没有这种平移不变性的数据。例如，在表格数据或我们在第 4 章中遇到的时间序列数据上使用它，我们可能已经可以做出很棒的事情。在一定程度上，也可以将其应用于适当表示的文本数据。[^(10)](#pgfId-1017613)
- en: 7.4 Exercises
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 练习
- en: Use `torchvision` to implement random cropping of the data.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`torchvision`实现数据的随机裁剪。
- en: How are the resulting images different from the uncropped originals?
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果图像与未裁剪的原始图像有何不同？
- en: What happens when you request the same image a second time?
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当第二次请求相同图像时会发生什么？
- en: What is the result of training using randomly cropped images?
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机裁剪图像进行训练的结果是什么？
- en: Switch loss functions (perhaps MSE).
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换损失函数（也许是均方误差）。
- en: Does the training behavior change?
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练行为是否会改变？
- en: Is it possible to reduce the capacity of the network enough that it stops overfitting?
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否可能减少网络的容量，使其停止过拟合？
- en: How does the model perform on the validation set when doing so?
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样做时模型在验证集上的表现如何？
- en: 7.5 Summary
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 总结
- en: Computer vision is one of the most extensive applications of deep learning.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉是深度学习的最广泛应用之一。
- en: Several datasets of annotated images are publicly available; many of them can
    be accessed via `torchvision`.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有许多带有注释的图像数据集可以公开获取；其中许多可以通过`torchvision`访问。
- en: '`Dataset`s and `DataLoader`s provide a simple yet effective abstraction for
    loading and sampling datasets.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataset`和`DataLoader`为加载和采样数据集提供了简单而有效的抽象。'
- en: For a classification task, using the softmax function on the output of a network
    produces values that satisfy the requirements for being interpreted as probabilities.
    The ideal loss function for classification in this case is obtained by using the
    output of softmax as the input of a non-negative log likelihood function. The
    combination of softmax and such loss is called cross entropy in PyTorch.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类任务，在网络输出上使用softmax函数会产生满足概率解释要求的值。在这种情况下，用softmax的输出作为非负对数似然函数的输入得到的损失函数是理想的分类损失函数。在PyTorch中，softmax和这种损失的组合称为交叉熵。
- en: Nothing prevents us from treating images as vectors of pixel values, dealing
    with them using a fully connected network, just like any other numerical data.
    However, doing so makes it much harder to take advantage of the spatial relationships
    in the data.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有什么能阻止我们将图像视为像素值向量，使用全连接网络处理它们，就像处理任何其他数值数据一样。然而，这样做会使利用数据中的空间关系变得���加困难。
- en: Simple models can be created using `nn.Sequential`.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用`nn.Sequential`创建简单模型。
- en: '* * *'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '^(1.)The images were collected and labeled by Krizhevsky, Nair, and Hinton
    of the Canadian Institute For Advanced Research (CIFAR) and were drawn from a
    larger collection of unlabeled 32 × 32 color images: the “80 million tiny images
    dataset” from the Computer Science and Artificial Intelligence Laboratory (CSAIL)
    at the Massachusetts Institute of Technology.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)这些图像是由加拿大高级研究所（CIFAR）的Krizhevsky、Nair和Hinton收集和标记的，并且来自麻省理工学院计算机科学与人工智能实验室（CSAIL）的更大的未标记32×32彩色图像集合：“8000万小图像数据集”。
- en: '^(2.)For some advanced uses, PyTorch also provides `IterableDataset`. This
    can be used in cases like datasets in which random access to the data is prohibitively
    expensive or does not make sense: for example, because data is generated on the
    fly.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.)对于一些高级用途，PyTorch还提供了`IterableDataset`。这可以用于数据集中随机访问数据代价过高或没有意义的情况：例如，因为数据是即时生成的。
- en: ^(3.)It doesn’t translate well to print; you’ll have to take our word for it,
    or check it out in the eBook or the Jupyter Notebook.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ^(3.)这在打印时无法很好地翻译；你必须相信我们的话，或者在电子书或Jupyter Notebook中查看。
- en: ^(4.)Here, we built the new dataset manually and also wanted to remap the classes.
    In some cases, it may be enough to take a subset of the indices of a given dataset.
    This can be accomplished using the `torch.utils .data.Subset` class. Similarly,
    there is `ConcatDataset` to join datasets (of compatible items) into a larger
    one. For iterable datasets, `ChainDataset` gives a larger, iterable dataset.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ^(4.)在这里，我们手动构建了新数据集，并且还想重新映射类别。在某些情况下，仅需要获取给定数据集的索引子集即可。这可以通过`torch.utils.data.Subset`类来实现。类似地，`ConcatDataset`用于将（兼容项的）数据集合并为一个更大的数据集。对于可迭代数据集，`ChainDataset`提供了一个更大的可迭代数据集。
- en: ^(5.)Using distance on the “probability” vectors would already have been much
    better than using `MSELoss` with the class numbers--which, recalling our discussion
    of types of values in the sidebar “Continuous, ordinal, and categorical values”
    from chapter 4, does not make sense for categories and does not work at all in
    practice. Still, `MSELoss` is not very well suited to classification problems.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ^(5.)在“概率”向量上使用距离已经比使用`MSELoss`与类别编号要好得多——回想我们在第4章“连续、有序和分类值”侧边栏中讨论的值类型，对于类别来说，使用`MSELoss`没有意义，在实践中根本不起作用。然而，`MSELoss`并不适用于分类问题。
- en: ^(6.)For the special binary classification case, using two values here is redundant,
    as one is always 1 minus the other. And indeed PyTorch lets us output only a single
    probability using the `nn.Sigmoid` activation at the end of the model to get a
    probability and the binary cross-entropy loss function `nn.BCELoss`. There also
    is an `nn.BCELossWithLogits` merging these two steps.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ^(6.)对于特殊的二元分类情况，在这里使用两个值是多余的，因为一个总是另一个的1减。事实上，PyTorch允许我们仅在模型末尾使用`nn.Sigmoid`激活输出单个概率，并使用二元交叉熵损失函数`nn.BCELoss`。还有一个将这两个步骤合并的`nn.BCELossWithLogits`。
- en: ^(7.)While it is, in principle, possible to say that here the model is uncertain
    (because it assigns 48% and 52% probabilities to the two classes), it will turn
    out that typical training results in highly overconfident models. Bayesian neural
    networks can provide some remedy, but they are beyond the scope of this book.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ^(7.)虽然原则上可以说这里的模型不确定（因为它将48%和52%的概率分配给两个类别），但典型的训练结果是高度自信的模型。贝叶斯神经网络可以提供一些补救措施，但这超出了本书的范围。
- en: ^(8.)For a succinct definition of the terminology, refer to David MacKay’s *Information
    Theory, Inference, and Learning Algorithms* (Cambridge University Press, 2003),
    section 2.3.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ^(8.)要了解术语的简明定义，请参考David MacKay的《信息理论、推断和学习算法》（剑桥大学出版社，2003年），第2.3节。
- en: '^(9.)The same caveat about translation invariance also applies to purely 1D
    data: an audio classifier should likely produce the same output even if the sound
    to be classified starts a tenth of a second earlier or later.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ^(9.)关于平移不变性的同样警告也适用于纯粹的1D数据：音频分类器应该在要分类的声音开始时间提前或延后十分之一秒时产生相同的输出。
- en: ^(10.)*Bag-of-words models*, which just average over word embeddings, can be
    processed with the network design from this chapter. More contemporary models
    take the positions of the words into account and need more advanced models.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ^(10.)*词袋模型*，只是对单词嵌入进行平均处理，可以使用本章的网络设计进行处理。更现代的模型考虑了单词的位置，并需要更高级的模型。
