- en: Chapter 3\. Data Preprocessing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。数据预处理
- en: In this chapter, you’ll learn how to prepare and set up data for training. Some
    of the most common data formats for ML work are tables, images, and text. There
    are commonly practiced techniques associated with each, though how you set up
    your data engineering pipeline will, of course, depend on what your problem statement
    is and what you are trying to predict.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何为训练准备和设置数据。机器学习工作中最常见的数据格式是表格、图像和文本。与每种数据格式相关的常见实践技术，尽管如何设置数据工程流水线当然取决于您的问题陈述是什么以及您试图预测什么。
- en: I’ll look at all three formats in detail, using specific examples to walk you
    through the techniques. All the data can be read directly into your Python runtime
    memory; however, this isn’t the most efficient way to use your compute resources.
    When I discuss text data, I’ll give particular attention to tokenization and dictionaries.
    By the end of this chapter, you’ll have learned how to prepare table, image, and
    text data for training.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我将详细查看所有三种格式，使用具体示例来引导您了解这些技术。所有数据都可以直接读入Python运行时内存；然而，这并不是最有效的使用计算资源的方式。当讨论文本数据时，我将特别关注标记和字典。通过本章结束时，您将学会如何准备表格、图像和文本数据进行训练。
- en: Preparing Tabular Data for Training
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为训练准备表格数据
- en: In a tabular dataset, it is important to identify which columns are considered
    categorical, because you have to encode their value as a class or a binary representation
    of the class (one-hot encoding), rather than a numerical value. Another aspect
    of tabular datasets is the potential for interactions among multiple features.
    This section will also look at the API that TensorFlow provides to make it easier
    to model column interactions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格数据集中，重要的是要确定哪些列被视为分类列，因为您必须将它们的值编码为类或类的二进制表示（独热编码），而不是数值值。表格数据集的另一个方面是多个特征之间的相互作用的潜力。本节还将查看TensorFlow提供的API，以便更容易地建模列之间的交互。
- en: It’s common to encounter tabular datasets as CSV files or simply as structured
    output from a database query. For this example, we’ll start with a dataset that’s
    already in a pandas DataFrame and then learn how to transform it and set it up
    for model training. We’ll use the *Titanic* dataset, an open source, tabular dataset
    that is often used for teaching because of its manageable size and availability.
    This dataset contains attributes for each passenger, such as age, gender, cabin
    grade, and whether or not they survived. We are going to try to predict each passenger’s
    probability of survival based on their attributes or features. Be aware that this
    is a small dataset for teaching and learning purposes only. In reality, your dataset
    will likely be much larger. You may make different decisions and choose different
    default values for some of these input parameters, so don’t take this example
    too literally.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会遇到作为CSV文件的表格数据集，或者仅仅作为数据库查询结果的结构化输出。在这个例子中，我们将从已经在pandas DataFrame中的数据集开始，并学习如何转换它并为模型训练设置它。我们将使用*泰坦尼克*数据集，这是一个开源的表格数据集，通常用于教学，因为其可管理的大小和可用性。该数据集包含每位乘客的属性，如年龄、性别、舱位等级，以及他们是否幸存。我们将尝试根据他们的属性或特征来预测每位乘客的生存概率。请注意，这是一个用于教学和学习目的的小数据集。实际上，您的数据集可能会更大。您可能会对一些输入参数的默认值做出不同的决定，并选择不同的默认值，所以不要对这个例子过于字面理解。
- en: 'Let’s start with loading all the necessary libraries:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载所有必要的库开始：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the data from Google’s public storage:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从谷歌的公共存储中加载数据：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now take a look at `train_file_path`:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看看`train_file_path`：
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This file path points to a CSV file, which we’ll read as a pandas DataFrame:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件路径指向一个CSV文件，我们将其读取为一个pandas DataFrame：
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Figure 3-1](#titanic_dataset_as_a_pandas_dataframe) shows what `titanic_df`
    looks like as a pandas DataFrame.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-1](#titanic_dataset_as_a_pandas_dataframe)展示了`titanic_df`作为pandas DataFrame的样子。'
- en: '![Titanic dataset as a pandas DataFrame](Images/t2pr_0301.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![泰坦尼克数据集作为pandas DataFrame](Images/t2pr_0301.png)'
- en: Figure 3-1\. *Titanic* dataset as a pandas DataFrame
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。*泰坦尼克*数据集作为pandas DataFrame
- en: Marking Columns
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记列
- en: As you can see in [Figure 3-1](#titanic_dataset_as_a_pandas_dataframe), there
    are numeric as well as categorical columns in this data. The target column, or
    the column for prediction, is the “survived” column. You’ll need to mark it as
    the target and mark the rest of the columns as features.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图3-1](#titanic_dataset_as_a_pandas_dataframe)中所见，这些数据中既有数值列，也有分类列。目标列，或者用于预测的列，是“survived”列。您需要将其标记为目标，并将其余列标记为特征。
- en: Tip
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A best practice in TensorFlow is to convert your table into a streaming dataset.
    This practice ensures that the data’s size does not affect memory consumption.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中的最佳实践是将您的表格转换为流式数据集。这种做法确保数据的大小不会影响内存消耗。
- en: 'To do exactly that, TensorFlow provides the function `tf.data.experimental.make_csv_dataset`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，TensorFlow提供了函数`tf.data.experimental.make_csv_dataset`：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding function signature, you specify the file path for which you
    wish to generate a dataset object. The `batch_size` is arbitrarily set to something
    small (3 in this case) for convenience in inspecting the data. We also set `label_name`
    to the “survived” column. For data quality, if a question mark (?) is specified
    in any cell, you want it to be interpreted as “NA” (not applicable). For training,
    set `num_epochs` to iterate over the dataset once. You can ignore any parsing
    errors or empty lines.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数签名中，您指定要生成数据集对象的文件路径。`batch_size`被任意设置为一个较小的值（在本例中为3），以便方便检查数据。我们还将`label_name`设置为“survived”列。对于数据质量，如果任何单元格中指定了问号（?），您希望将其解释为“NA”（不适用）。对于训练，将`num_epochs`设置为对数据集进行一次迭代。您可以忽略任何解析错误或空行。
- en: 'Next, inspect the data:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，检查数据：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It will appear similar to [Figure 3-2](#a_batch_of_the_titanic_dataset).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来类似于[图3-2](#a_batch_of_the_titanic_dataset)。
- en: '![A batch of the Titanic dataset](Images/t2pr_0302.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![一个批次的泰坦尼克数据集](Images/t2pr_0302.png)'
- en: Figure 3-2\. A batch of the *Titanic* dataset
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2. *泰坦尼克*数据集的一个批次
- en: 'Here are the major steps for training a paradigm to consume your training dataset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是训练范式消耗训练数据集的主要步骤：
- en: Designate columns by feature types.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按特征类型指定列。
- en: Decide whether or not to embed or cross columns.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定是否嵌入或交叉列。
- en: Choose the columns of interest, possibly as an experiment.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择感兴趣的列，可能作为一个实验。
- en: Create a “feature layer” for consumption by the training paradigm.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为训练范式创建一个“特征层”以供使用。
- en: Now that you have set up the data as datasets, you can designate each column
    by its feature type, such as numeric or categorical, bucketized (by binning) if
    necessary. You can also embed the column if there are too many unique categories
    and dimension reduction would be helpful.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经将数据设置为数据集，可以根据其特征类型指定每列，例如数字或分类，如果需要的话可以进行分桶。如果唯一类别太多且降维会有帮助，还可以嵌入列。
- en: 'Let’s go ahead with step 1\. There are four numeric columns: `age`, `n_siblings_spouses`,
    `parch`, and `fare`. Five columns are categorical: `sex`, `class`, `deck`, `embark_town`,
    and `alone`. You will create a `feature_columns` list to hold all the feature
    columns once you are done.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行第1步。有四个数字列：`age`、`n_siblings_spouses`、`parch`和`fare`。五列是分类的：`sex`、`class`、`deck`、`embark_town`和`alone`。完成后，您将创建一个`feature_columns`列表来保存所有特征列。
- en: 'Here is how to designate numeric columns based strictly on the actual numeric
    values, without any transformation:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何根据实际数字值严格指定数字列的方法，而不进行任何转换：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that in addition to using `age` as is, you could also bin `age` into a
    bucket, such as by quantile of age distribution. But what are the bin boundaries
    (quantiles)? You can inspect the general statistics of numeric columns in a pandas
    DataFrame:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了使用`age`本身，您还可以将`age`分箱，例如按年龄分布的四分位数。但是分箱边界（四分位数）是什么？您可以检查pandas DataFrame中数字列的一般统计信息：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Figure 3-3](#statistics_for_numeric_columns_in_the_ti) shows the output.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-3](#statistics_for_numeric_columns_in_the_ti)显示了输出。'
- en: '![Statistics for numeric columns in the Titanic dataset](Images/t2pr_0303.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![泰坦尼克号数据集中数字列的统计](Images/t2pr_0303.png)'
- en: Figure 3-3\. Statistics for numeric columns in the *Titanic* dataset
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3。*泰坦尼克号*数据集中的数字列统计
- en: 'Let’s try three bin boundaries for age: 23, 28, and 35\. This means passenger
    age will be grouped into first quantile, second quantile, and third quantile (as
    shown in [Figure 3-3](#statistics_for_numeric_columns_in_the_ti)):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试为年龄设置三个分箱边界：23、28和35。这意味着乘客年龄将被分组为第一四分位数、第二四分位数和第三四分位数（如[图3-3](#statistics_for_numeric_columns_in_the_ti)所示）：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Therefore, in addition to “age,” you have generated another column, “age_bucket.”
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除了“age”之外，您还生成了另一个列“age_bucket”。
- en: 'To understand the nature of each categorical column, it would be helpful to
    know the distinct values in them. You’ll need to encode the vocabulary list with
    the unique entries in each column. For categorical columns, this means you need
    to determine which entries are unique:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解每个分类列的性质，了解其中的不同值将是有帮助的。您需要使用每列中的唯一条目对词汇表进行编码。对于分类列，这意味着您需要确定哪些条目是唯一的：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The result is shown in [Figure 3-4](#unique_values_in_each_categorical_column).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[图3-4](#unique_values_in_each_categorical_column)中。
- en: '![Unique values in each categorical column of the Titanic dataset](Images/t2pr_0304.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![泰坦尼克号数据集中每个分类列中的唯一值](Images/t2pr_0304.png)'
- en: Figure 3-4\. Unique values in each categorical column of the dataset
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4。数据集中每个分类列的唯一值
- en: 'You need to keep track of these unique values in a dictionary format for the
    model to do the mapping and lookup. Therefore, you’ll encode unique categorical
    values in the “sex” column:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要以字典格式跟踪这些唯一值，以便模型进行映射和查找。因此，您将对“sex”列中的唯一分类值进行编码：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'However, if the list is long, it becomes inconvenient to write it out. Instead,
    as you iterate through the categorical columns, you can save each column’s unique
    values in a Python dictionary data structure `h` for future lookup. Then you can
    pass the unique value as a list into these vocabulary lists:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果列表很长，逐个写出会变得不方便。因此，当您遍历分类列时，可以将每列的唯一值保存在Python字典数据结构`h`中以供将来查找。然后，您可以将唯一值作为列表传递到这些词汇表中：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can also embed the “deck” column, since there are eight unique values,
    more than any other categorical column. Reduce its dimension to 3:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以嵌入“deck”列，因为有八个唯一值，比任何其他分类列都多。将其维度减少到3：
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Another way to reduce the dimensions of categorical columns is by using a *hashed
    feature column*. This method calculates a hashed value based on the input data.
    It then designates a hashed bucket for the data. The following code reduces the
    dimension of the “class” column to 4:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 减少分类列维度的另一种方法是使用*哈希特征列*。该方法根据输入数据计算哈希值，然后为数据指定一个哈希桶。以下代码将“class”列的维度减少到4：
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Encoding Column Interactions as Possible Features
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将列交互编码为可能的特征
- en: 'Now comes the most interesting part: you’re going to find interactions between
    different features (this is referred to as *crossing columns*) and encode those
    interactions as possible features. This is also where your intuition and domain
    knowledge can benefit your feature engineering endeavor. For example, a question
    that comes to mind based on the historical background of the *Titanic* disaster
    is this: were women in first-class cabins more likely to survive than women in
    second- or third-class cabins? To rephrase this as a data science question, you’ll
    need to consider interactions between the gender and cabin class of the passengers.
    Then you’ll need to pick a starting dimension size to represent the data variability.
    Let’s say you arbitrarily decide to bin the variability into five dimensions (`hash_bucket_size`):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来到最有趣的部分：您将找到不同特征之间的交互（这被称为*交叉列*），并将这些交互编码为可能的特征。这也是您的直觉和领域知识可以有益于您的特征工程努力的地方。例如，基于*泰坦尼克号*灾难的历史背景，一个问题是：一等舱的女性是否比二等或三等舱的女性更有可能生存？为了将这个问题重新表述为一个数据科学问题，您需要考虑乘客的性别和舱位等级之间的交互。然后，您需要选择一个起始维度大小来表示数据的变化性。假设您任意决定将变化性分成五个维度（`hash_bucket_size`）：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now that you have created all the features, you need to put them together—and
    perhaps experiment to decide which to include in the training process. For that,
    you’ll first create a list to hold all the features you want to use:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经创建了所有特征，需要将它们组合在一起，并可能进行实验以决定在训练过程中包含哪些特征。为此，您首先要创建一个列表来保存您想要使用的所有特征：
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then you’ll append each feature of interest to the list:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将每个感兴趣的特征附加到列表中：
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now create a feature layer:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建一个特征层：
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This layer will serve as the first (input) layer in the model you are about
    to build and train. This is how you’ll provide all the feature engineering frameworks
    for the model’s training process.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层将作为您即将构建和训练的模型的第一（输入）层。这是您为模型的训练过程提供所有特征工程框架的方式。
- en: Creating a Cross-Validation Dataset
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个交叉验证数据集
- en: 'Before you start training, you have to create a small dataset for cross-validation
    purposes. Since there are only two partitions (training and testing) to begin
    with, one way to generate a cross-validation dataset is to simply subdivide one
    of the partitions:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，您需要为交叉验证目的创建一个小数据集。由于一开始只有两个分区（训练和测试），生成一个交叉验证数据集的一种方法是简单地将其中一个分区细分：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, 40% of the original `test_df` partition was randomly reserved as `test_df`,
    and the remaining 60% is now `val_df`. Usually, test datasets are the smallest
    of the three (training, validation, testing), since they are used only for final
    evaluation, and not during model training.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，原始`test_df`分区的40%被随机保留为`test_df`，剩下的60%现在是`val_df`。通常，测试数据集是三个数据集中最小的，因为它们仅用于最终评估，而不用于模型训练。
- en: 'Now that you have taken care of feature engineering and data partitioning,
    there is one last thing to do: stream the data into the training process with
    the dataset. You’ll convert each of the three DataFrames (training, validation,
    and testing) into its own dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经处理了特征工程和数据分区，还有最后一件事要做：使用数据集将数据流入训练过程。您将把三个DataFrame（训练、验证和测试）分别转换为自己的数据集：
- en: '[PRE19]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As shown in the preceding code, first you’ll arbitrarily decide how many samples
    to include in a batch (`batch_size`). Then you need to set aside a label designation
    (`survived`). The `tf.data.Dataset.from_tensor_slices` method takes a tuple as
    an argument. In this tuple, there are two elements: feature columns and the label
    column.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，首先您将任意决定要包含在一个批次中的样本数量（`batch_size`）。然后，您需要设置一个标签指定（`survived`）。`tf.data.Dataset.from_tensor_slices`方法接受一个元组作为参数。在这个元组中，有两个元素：特征列和标签列。
- en: The first element is `dict(train_df)`. This `dict` operation essentially transforms
    the DataFrame into a key-value pair, where each key represents a column name and
    the corresponding value is an array of the values in the column. The other element
    is `labels`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个元素是`dict(train_df)`。这个`dict`操作实质上将DataFrame转换为键值对，其中每个键代表一个列名，相应的值是该列中的值数组。另一个元素是`labels`。
- en: 'Finally, we shuffle and batch the dataset. Since this conversion will be applied
    to all three datasets, it would be convenient to combine these steps into a helper
    function to reduce repetition:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对数据集进行洗牌和分批处理。由于这种转换将应用于所有三个数据集，将这些步骤合并到一个辅助函数中以减少重复会很方便：
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now you can apply this function to both validation and test data:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以将此函数应用于验证和测试数据：
- en: '[PRE21]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Starting the Model Training Process
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始模型训练过程
- en: Now you’re ready to start the model training process. Technically this isn’t
    a part of preprocessing, but running through this short section will allow you
    to see how the work you have done fits into the model training process itself.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经准备好开始模型训练过程。从技术上讲，这并不是预处理的一部分，但通过这个简短的部分，您可以看到您所做的工作如何融入到模型训练过程中。
- en: 'You’ll start by building a model architecture:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您将从构建模型架构开始：
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: For demonstration purposes, you’ll build a simple two-layer, deep-learning perceptron
    model, which is a basic configuration of a feedforward neural network. (For more
    on this, see Aurélien Géron’s [*Neural Networks and Deep Learning*](https://oreil.ly/1wOQj)
    (O’Reilly)). Notice that since this is a multilayer perceptron model, you’ll use
    the sequential API. Inside this API, the first layer is `feature_layer`, which
    represents all the feature engineering logic and derived features, such as age
    bins and crosses, that are used to model the feature interactions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，您将构建一个简单的两层深度学习感知器模型，这是一个前馈神经网络的基本配置。请注意，由于这是一个多层感知器模型，您将使用顺序API。在这个API中，第一层是`feature_layer`，它代表所有特征工程逻辑和派生特征，例如年龄分段和交叉，用于建模特征交互。
- en: 'Compile the model and set up the loss function for binary classification:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 编译模型并为二元分类设置损失函数：
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then you can start the training. You’ll only train it for 10 epochs:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您可以开始训练。您只会训练10个epochs：
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can expect an outcome similar to that pictured in [Figure 3-5](#example_training_outcome_from_survival_p).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以期望得到与[图3-5](#example_training_outcome_from_survival_p)中所示类似的结果。
- en: '![Example training outcome from survival prediction in the Titanic dataset](Images/t2pr_0305.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![泰坦尼克数据集中生存预测的示例训练结果](Images/t2pr_0305.png)'
- en: Figure 3-5\. Example training outcome from survival prediction in the *Titanic*
    dataset
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5\. *泰坦尼克*数据集中生存预测的示例训练结果
- en: Summary
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this section, you saw how to deal with tabular data that consists of multiple
    data types. You also saw that TensorFlow provides a `feature_column` API, which
    enables proper casting of data types, handling of categorical data, and feature
    crossing for potential interactions. This API is very helpful in simplifying data
    and feature engineering tasks.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您看到了如何处理由多种数据类型组成的表格数据。您还看到了TensorFlow提供的`feature_column`API，它可以正确转换数据类型、处理分类数据，并为潜在交互提供特征交叉。这个API在简化数据和特征工程任务方面非常有帮助。
- en: Preparing Image Data for Processing
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为处理图像数据做准备
- en: For images, you need to reshape or resample all the images into the same pixel
    count; this is known as *standardization*. You also need to ensure that all pixel
    values are within the same color range so that they fall within the finite range
    of RGB values of each pixel.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像，您需要将所有图像重塑或重新采样为相同的像素计数；这被称为*标准化*。您还需要确保所有像素值在相同的颜色范围内，以便它们落在每个像素的RGB值的有限范围内。
- en: Image data comes with different file extensions, such as *.jpg*, *.tiff*, and
    *.bmp*. These are not really problematic, as there are APIs in Python and TensorFlow
    that can read and parse images with any file extension. The tricky part about
    image data is capturing its dimensions—height, width, and depth—as measured by
    pixel counts. (If it is a color image encoded with RGB, these appear as three
    separate channels.)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据具有不同的文件扩展名，例如*.jpg*、*.tiff*和*.bmp*。这些并不是真正的问题，因为Python和TensorFlow中有可以读取和解析任何文件扩展名的图像的API。关于图像数据的棘手部分在于捕获其维度——高度、宽度和深度——由像素计数来衡量。（如果是用RGB编码的彩色图像，这些会显示为三个独立的通道。）
- en: If all the images in your dataset (including training, validation, and all the
    images during testing or deployment time) are expected to have the same dimensions
    *and* you are going to build your own model, then processing image data is not
    too much of a problem. However, if you wish to leverage prebuilt models such as
    ResNet or Inception, then you have to conform to their image requirements. As
    an example, ResNet requires each input image to be 224 × 224 × 3 pixels and be
    presented as a NumPy multidimensional array. This means that, in the preprocessing
    routine, you have to resample your images to conform to those dimensions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据集中的所有图像（包括训练、验证以及测试或部署时的所有图像）都预期具有相同的尺寸*并且*您将构建自己的模型，那么处理图像数据并不是太大的问题。然而，如果您希望利用预构建的模型如ResNet或Inception，那么您必须符合它们的图像要求。例如，ResNet要求每个输入图像为224
    × 224 × 3像素，并呈现为NumPy多维数组。这意味着在预处理过程中，您必须重新采样您的图像以符合这些尺寸。
- en: Another situation for resampling arises when you cannot reasonably expect all
    the images, especially during deployment, to have the same size. In this case,
    you need to consider a proper image dimension as you build the model, then set
    up the preprocessing routine to ensure that resampling is done properly.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要重新采样的情况是当您无法合理地期望所有图像，特别是在部署时，具有相同的尺寸。在这种情况下，您需要在构建模型时考虑适当的图像尺寸，然后设置预处理程序以确保重新采样正确进行。
- en: In this section, you are going to use the flower dataset provided by TensorFlow.
    It consists of five types of flowers and diverse image dimensions. This is a convenient
    dataset to use, since all images are already in JPEG format. You are going to
    process this image data to train a model to parse each image and classify it as
    one of the five classes of flowers.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将使用TensorFlow提供的花卉数据集。它包含五种类型的花卉和不同的图像尺寸。这是一个方便的数据集，因为所有图像都已经是JPEG格式。您将处理这些图像数据，训练一个模型来解析每个图像并将其分类为五类花卉之一。
- en: 'As usual, import all necessary libraries:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，导入所有必要的库：
- en: '[PRE25]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now download the flower dataset from the URL:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从以下网址下载花卉数据集：
- en: '[PRE26]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This file is a compressed TAR archive file. Therefore, you need to set `untar=True`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件是一个压缩的TAR存档文件。因此，您需要设置`untar=True`。
- en: When using `tf.keras.utils.get_file`, by default you will find the downloaded
    data in the `~/.keras/datasets` directory.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tf.keras.utils.get_file`时，默认情况下会在`~/.keras/datasets`目录中找到下载的数据。
- en: 'In a Mac or Linux system’s Jupyter Notebook cell, execute:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在Mac或Linux系统的Jupyter Notebook单元格中执行：
- en: '[PRE27]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You will find the flower dataset as shown in [Figure 3-6](#flower_dataset_folders).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您将找到如[图3-6](#flower_dataset_folders)所示的花卉数据集。
- en: '![Flower dataset folders](Images/t2pr_0306.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![花卉数据集文件夹](Images/t2pr_0306.png)'
- en: Figure 3-6\. Flower dataset folders
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6\. 花卉数据集文件夹
- en: 'Now let’s take a look at one of the flower types:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看其中一种花卉：
- en: '[PRE28]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You should see the first nine images, as shown in [Figure 3-7](#ten_example_image_files_in_the_rose_dire).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到前九个图像，如[图3-7](#ten_example_image_files_in_the_rose_dire)所示。
- en: '![Ten example image files in the rose directory](Images/t2pr_0307.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![玫瑰目录中的十个示例图像文件](Images/t2pr_0307.png)'
- en: Figure 3-7\. Nine example image files in the rose directory
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7\. 玫瑰目录中的九个示例图像文件
- en: These images are all different sizes. You can verify this by examining a couple
    of images. Here is a helper function you can leverage to display the image in
    its original size:^([1](ch03.xhtml#idm45772916304408))
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像都是不同的尺寸。您可以通过检查几张图像来验证这一点。以下是一个您可以利用的辅助函数，用于显示原始尺寸的图像：^([1](ch03.xhtml#idm45772916304408))
- en: '[PRE29]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s use it to display an image (as shown in [Figure 3-8](#rose_image_sample_one)):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用它来显示一张图像（如[图3-8](#rose_image_sample_one)所示）：
- en: '[PRE30]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![Rose image sample 1](Images/t2pr_0308.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![玫瑰图像样本1](Images/t2pr_0308.png)'
- en: Figure 3-8\. Rose image sample 1
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8。玫瑰图像示例1
- en: 'Now try a different image (as shown in [Figure 3-9](#rose_image_sample_two)):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试不同的图像（如[图3-9](#rose_image_sample_two)所示）：
- en: '[PRE31]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![Rose image sample 2](Images/t2pr_0309.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![玫瑰图像示例2](Images/t2pr_0309.png)'
- en: Figure 3-9\. Rose image sample 2
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9。玫瑰图像示例2
- en: Clearly, the dimensions and aspect ratios of these images are different.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这些图像的尺寸和长宽比是不同的。
- en: Transforming Images to a Fixed Specification
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将图像转换为固定规格
- en: Now you’re ready to transform these images to a fixed specification. In this
    particular example, you’ll use the ResNet input image spec, which is 224 × 224
    with three color channels (RGB). Also, it is a best practice to use data streaming
    whenever possible. Therefore, your goal here is to transform these color images
    into the shape of 224 × 224 pixels and build a dataset from them for streaming
    into the training paradigm.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好将这些图像转换为固定规格。在这个特定的示例中，您将使用ResNet输入图像规格，即224×224，带有三个颜色通道（RGB）。此外，尽可能使用数据流。因此，您的目标是将这些彩色图像转换为224×224像素的形状，并从中构建一个数据集，以便流式传输到训练范式中。
- en: To accomplish this you’ll use the `ImageDataGenerator` class and the `flow_from_directory`
    method.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，您将使用`ImageDataGenerator`类和`flow_from_directory`方法。
- en: '`ImageDataGenerator` is responsible for creating a generator object, which
    generates streaming data from the directory as specified by `flow_from_directory`.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageDataGenerator`负责创建一个生成器对象，该对象从由`flow_from_directory`指定的目录中生成流式数据。'
- en: 'In general, the coding pattern is:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，编码模式是：
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In both cases, keyword argument options, or `kwargs`, give your code great
    flexibility. (Keyword arguments are frequently seen in Python.) These arguments
    enable you to pass optional parameters to the function. As it turns out, in `ImageDataGenerator`,
    there are two parameters relevant to your needs: `rescale` and `validation_split`.
    The `rescale` parameter is used for normalizing pixel values into a finite range,
    and `validation_split` lets you subdivide a partition of data, such as for cross
    validation.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，关键字参数选项或`kwargs`为您的代码提供了很大的灵活性。（关键字参数在Python中经常见到。）这些参数使您能够将可选参数传递给函数。事实证明，在`ImageDataGenerator`中，有两个与您需求相关的参数：`rescale`和`validation_split`。`rescale`参数用于将像素值归一化为有限范围，`validation_split`允许您将数据的一个分区细分，例如用于交叉验证。
- en: 'In `flow_from_directory`, there are three parameters that are useful for this
    example: `target_size`, `batch_size`, and `interpolation`. The `target_size` parameter
    helps you specify the desired dimension of each image, and `batch_size` is for
    specifying the number of samples in a batch of images. As for `interpolation`,
    remember how you need to interpolate, or resample, each image to a prescribed
    dimension specified with `target_size`? Supported methods for interpolation are
    `nearest`, `bilinear`, and `bicubic`. For this example, first try `bilinear`.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在`flow_from_directory`中，有三个对于本示例有用的参数：`target_size`、`batch_size`和`interpolation`。`target_size`参数帮助您指定每个图像的期望尺寸，`batch_size`用于指定批量图像中的样本数。至于`interpolation`，请记住您需要对每个图像进行插值或重新采样，以达到用`target_size`指定的规定尺寸？插值的支持方法有`nearest`、`bilinear`和`bicubic`。对于本示例，首先尝试`bilinear`。
- en: 'You can define these keyword arguments as follows. Later you’ll pass them into
    their function calls:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将这些关键字参数定义如下。稍后将把它们传递给它们的函数调用：
- en: '[PRE33]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create a generator object:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个生成器对象：
- en: '[PRE34]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now you can specify the source directory from which this generator will stream
    the data. This generator will only stream 20% of the data, and this is designated
    as a validation dataset:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以指定此生成器将从中流式传输数据的源目录。此生成器将仅流式传输20%的数据，并将其指定为验证数据集：
- en: '[PRE35]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You can use the same generator object for training data:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用相同的生成器对象进行训练数据：
- en: '[PRE36]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Inspect the output of the generator:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 检查生成器的输出：
- en: '[PRE37]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The output is represented as NumPy arrays. For a batch of images, the sample
    size is 32, with 224 pixels in height and width and three channels representing
    RGB color space. For the label batch, there are likewise 32 samples. Each row
    is one-hot encoded to represent which of the five classes it belongs to.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 输出表示为NumPy数组。对于一批图像，样本大小为32，高度和宽度为224像素，三个通道表示RGB颜色空间。对于标签批次，同样有32个样本。每行都是独热编码，表示属于五类中的哪一类。
- en: 'Another important thing to do is to retrieve the lookup dictionary of labels.
    During inferencing, the model will output the probability for each of the five
    classes. The only way to decode which class has the highest probability is with
    a prediction lookup dictionary of labels:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的事情是检索标签的查找字典。在推断期间，模型将输出每个五类中的概率。唯一的解码方式是使用标签的预测查找字典来确定哪个类具有最高的概率：
- en: '[PRE38]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'A typical output from our classification model would be a NumPy array similar
    to this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分类模型的典型输出将类似于以下的NumPy数组：
- en: '[PRE39]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The position with the highest probability value is the first element. Map this
    index to the first key in `idx_labels`—in this case, `daisy`. This is how you
    capture the results of the prediction. Save the `idx_labels` dictionary:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最高概率值的位置是第一个元素。将此索引映射到`idx_labels`中的第一个键 - 在本例中为`daisy`。这是您捕获预测结果的方法。保存`idx_labels`字典：
- en: '[PRE40]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This is how to load it back:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何加载它的方法：
- en: '[PRE41]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Training the Model
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Finally, for training you’ll use a model built from a pretrained ResNet feature
    vector. This technique is known as *transfer learning*. TensorFlow Hub hosts many
    pretrained models for free. This is how to access it during your model construction
    process:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于训练，您将使用从预训练的ResNet特征向量构建的模型。这种技术称为*迁移学习*。TensorFlow Hub免费提供许多预训练模型。这是在模型构建过程中访问它的方法：
- en: '[PRE42]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The first layer is `InputLayer`. Remember that the expected input is 224 ×
    224 × 3 pixels. You’ll use the tuple addition trick to append an extra dimension
    to `IMAGE_SIZE`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是`InputLayer`。请记住，预期输入为224×224×3像素。您将使用元组添加技巧将额外的维度附加到`IMAGE_SIZE`：
- en: '[PRE43]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now you have (224, 224, 3), which is a tuple that represents the dimension of
    an image as a NumPy array.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了（224, 224, 3），这是一个表示图像维度的NumPy数组的元组。
- en: The next layer is the pretrained ResNet feature vector referenced by the URL
    to TensorFlow Hub. Let’s use it as is so that we don’t have to retrain it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 下一层是由指向TensorFlow Hub的预训练ResNet特征向量引用的层。让我们直接使用它，这样我们就不必重新训练它。
- en: Next is the `Dense` layer with five nodes of output. Each output is the probability
    of the image belonging to that class. Then you’ll build the model skeleton, with
    `None` as the first dimension. This means the first dimension, which represents
    the sample size of a batch, is not decided until runtime. This is how to handle
    batch input.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是具有五个输出节点的`Dense`层。每个输出是图像属于该类的概率。然后，您将构建模型骨架，第一个维度为`None`。这意味着第一个维度，代表批处理的样本大小，在运行时尚未确定。这是如何处理批输入的方法。
- en: 'Inspect the model summary to make sure it’s what you expected:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 检查模型摘要以确保它符合您的预期：
- en: '[PRE44]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The output is shown in [Figure 3-10](#image_classification_model_summary).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示在[图3-10](#image_classification_model_summary)中。
- en: '![Image classification model summary](Images/t2pr_0310.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图像分类模型摘要](Images/t2pr_0310.png)'
- en: Figure 3-10\. Image classification model summary
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10\. 图像分类模型摘要
- en: 'Compile the model with `optimizers` and the corresponding `losses` function:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`optimizers`和相应的`losses`函数编译模型：
- en: '[PRE45]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'And then train it:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对其进行训练：
- en: '[PRE46]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: You may see output similar to that in [Figure 3-11](#output_from_training_the_image_classific).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会看到类似于[图3-11](#output_from_training_the_image_classific)的输出。
- en: '![Output from training the image classification model](Images/t2pr_0311.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![训练图像分类模型的输出](Images/t2pr_0311.png)'
- en: Figure 3-11\. Output from training the image classification model
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-11\. 训练图像分类模型的输出
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this section, you learned how to process image files. Specifically, it is
    necessary to make sure you have a predetermined image size requirement set before
    you design the model. Once that standard is accepted, the next step is to resample
    images into that size and normalize the pixel’s value into a smaller dynamic range.
    These routines are nearly universal. Also, streaming images into the training
    workflow is the most efficient method and a best practice, especially in cases
    where your working sample size approaches your Python runtime’s memory.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您学习了如何处理图像文件。具体来说，在设计模型之前，有必要确保您已经设置了一个预定的图像大小要求。一旦这个标准被接受，下一步就是将图像重新采样到该大小，并将像素的值归一化为更小的动态范围。这些例程几乎是通用的。此外，将图像流式传输到训练工作流程是最有效的方法和最佳实践，特别是在您的工作样本大小接近Python运行时内存的情况下。
- en: Preparing Text Data for Processing
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为处理文本数据做准备
- en: For text data, each word or character needs to be represented as a numerical
    integer. This process is known as *tokenization*. Further, if the goal is classification,
    then the target needs to be encoded as *classes*. If the goal is something more
    complicated, such as translation, then the target language in the training data
    (such as French in an English-to-French translation) also requires its own tokenization
    process. This is because the target is essentially a long string of text, just
    like the input text. Likewise, you also need to think about whether to tokenize
    the target at the word or character level.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本数据，每个单词或字符都需要表示为一个数字整数。这个过程被称为*标记化*。此外，如果目标是分类，那么目标需要被编码为*类别*。如果目标是更复杂的，比如翻译，那么训练数据中的目标语言（比如英语到法语翻译中的法语）也需要自己的标记化过程。这是因为目标本质上是一个长字符串的文本，就像输入文本一样。同样，您还需要考虑是在单词级别还是字符级别对目标进行标记化。
- en: Text data can be presented in many different formats. From a content organization
    perspective, it may be stored and organized as a table, with one column containing
    the body or string of text and another column containing labels, such as a binary
    sentiment indicator. It may be a free-form file, with lines of different lengths
    and a carriage return at the end of each line. It may be a manuscript in which
    blocks of text are defined by paragraphs or sections.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据可以以许多不同的格式呈现。从内容组织的角度来看，它可以被存储和组织为一个表格，其中一列包含文本的主体或字符串，另一列包含标签，例如二进制情感指示器。它可能是一个自由格式的文件，每行长度不同，每行末尾有一个换行符。它可能是一份手稿，其中文本块由段落或部分定义。
- en: There are many ways to determine the processing techniques and logic to use
    as you set up a natural language processing (NLP) machine learning problem; this
    section will cover some of the most frequently used techniques.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以确定要使用的处理技术和逻辑，当您设置自然语言处理（NLP）机器学习问题时；本节将涵盖一些最常用的技术。
- en: This example will use text from William Shakespeare’s tragedy *Coriolanus,*
    which is a simple public-domain example hosted on Google. You will build a *text
    generation model* that will learn how to write in Shakespeare’s style.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子将使用威廉·莎士比亚的悲剧《科里奥兰纳斯》中的文本，这是一个简单的公共领域示例，托管在谷歌上。您将构建一个*文本生成模型*，该模型将学习如何以莎士比亚的风格写作。
- en: Tokenizing Text
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对文本进行标记化
- en: Text is represented by strings of characters. These characters need to be converted
    to integers for modeling tasks. This example is a raw text string from *Coriolanus*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 文本由字符字符串表示。这些字符需要转换为整数以进行建模任务。这个例子是*科里奥兰纳斯*的原始文本字符串。
- en: 'Let’s import the necessary libraries and download the text file:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入必要的库并下载文本文件：
- en: '[PRE47]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Open it and output a few lines of sample text:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 打开它并输出几行示例文本：
- en: '[PRE48]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Inspect this text by printing the first 400 characters:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通过打印前400个字符来检查这段文本：
- en: '[PRE49]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The output is shown in [Figure 3-12](#sample_of_william_shakespeareapostrophes).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示在[图3-12](#sample_of_william_shakespeareapostrophes)中。
- en: 'To tokenize each character in this file, a simple `set` operation will suffice.
    This operation will create a unique set of characters found in the text string:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对该文件中的每个字符进行标记化，简单的`set`操作就足够了。这个操作将创建在文本字符串中找到的字符的一个唯一集合：
- en: '[PRE50]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: A glimpse of the list `vocabulary` is shown in [Figure 3-13](#part_of_the_vocabulary_list_from_coriola).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-13](#part_of_the_vocabulary_list_from_coriola)展示了`vocabulary`列表的一瞥。'
- en: '![Sample of William Shakespeare’s Coriolanus](Images/t2pr_0312.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![威廉·莎士比亚《科里奥兰纳斯》的样本](Images/t2pr_0312.png)'
- en: Figure 3-12\. Sample of William Shakespeare’s Coriolanus
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-12. 威廉·莎士比亚《科里奥兰纳斯》的示例
- en: '![Part of the vocabulary list from Coriolanus](Images/t2pr_0313.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![《科里奥兰纳斯》词汇列表的一部分](Images/t2pr_0313.png)'
- en: Figure 3-13\. Part of the vocabulary list from Coriolanus
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-13.《科里奥兰纳斯》词汇列表的一部分
- en: 'These tokens include punctuation, as well as both upper- and lowercase characters.
    It is not always necessary to include both upper- and lowercase characters; if
    you don’t want to, you can convert every character to lowercase before performing
    the `set` operation. Since you sorted the token list, you can see that special
    characters are also being tokenized. In some cases, this is not necessary; these
    tokens can be removed manually. The following code will convert all characters
    to lowercase and then perform the `set` operation:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标记包括标点符号，以及大写和小写字符。不一定需要同时包含大写和小写字符；如果不想要，可以在执行`set`操作之前将每个字符转换为小写。由于您对标记列表进行了排序，您可以看到特殊字符也被标记化了。在某些情况下，这是不必要的；这些标记可以手动删除。以下代码将所有字符转换为小写，然后执行`set`操作：
- en: '[PRE51]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You might be wondering if it would be reasonable to tokenize text at the word
    level instead of the character level. After all, the word is the fundamental unit
    of semantic understanding of a text string. Although this reasoning is sound and
    has some logic to it, in reality it creates more work and problems, while not
    really adding value to the training process or accuracy to the model. To illustrate
    why, let’s try to tokenize the text string by words. The first thing to recognize
    is that words are separated by spaces. So you need to split the text string on
    spaces:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道是否将文本标记化为单词级而不是字符级是否合理。毕竟，单词是对文本字符串的语义理解的基本单位。尽管这种推理是合理的并且有一定的逻辑性，但实际上它会增加更多的工作和问题，而并没有真正为训练过程增加价值或为模型的准确性增加价值。为了说明这一点，让我们尝试按单词对文本字符串进行标记化。首先要认识到的是单词是由空格分隔的。因此，您需要在空格上拆分文本字符串：
- en: '[PRE52]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Inspect the list `vocabulary_word`, shown in [Figure 3-14](#sample_of_tokenized_words).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 检查`vocabulary_word`列表，如[图3-14](#sample_of_tokenized_words)所示。
- en: With special characters and carriage returns embedded in each word token, this
    list is nearly unusable. It would require considerable work to clean it up with
    regular expressions or more sophisticated logic. In some cases, punctuation marks
    are attached to words. Further, the list of word tokens is much larger than the
    character-level token list. This makes it much more complicated for the model
    to learn the patterns in the text. For these reasons and the lack of proven benefit,
    it is not a common practice to tokenize text at the word level. If you wish to
    use word-level tokenization, then it is common to perform a word-embedding operation
    to reduce the variability and dimensions of the utterance representations.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个单词标记中嵌入了特殊字符和换行符，这个列表几乎无法使用。需要通过正则表达式或更复杂的逻辑来清理它。在某些情况下，标点符号附加在单词上。此外，单词标记列表比字符级标记列表要大得多。这使得模型更难学习文本中的模式。出于这些原因和缺乏已证明的好处，将文本标记化为单词级并不是一种常见做法。如果您希望使用单词级标记化，则通常会执行单词嵌入操作以减少话语表示的变异性和维度。
- en: '![Sample of tokenized words](Images/t2pr_0314.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![标记化单词示例](Images/t2pr_0314.png)'
- en: Figure 3-14\. Sample of tokenized words
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-14. 标记化单词的示例
- en: Creating a Dictionary and Reverse Dictionary
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建字典和反向字典
- en: Once you have the list of tokens that contains the chosen characters, you’ll
    need to map each token to an integer. This is known as the *dictionary*. Likewise,
    you’ll need to create a *reverse dictionary* that maps the integers back to the
    tokens.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有包含所选字符的标记列表，您将需要将每个标记映射到一个整数。这被称为*字典*。同样，您需要创建一个*反向字典*，将整数映射回标记。
- en: 'Generating an integer is easy with the `enumerate` function. This function
    takes a list as input and returns an integer corresponding to each unique element
    in the list. In this case, the list contains tokens:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`enumerate`函数很容易生成一个整数。这个函数以列表作为输入，并返回与列表中每个唯一元素对应的整数。在这种情况下，列表包含标记：
- en: '[PRE53]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: You can see a sample of this result in [Figure 3-15](#sample_enumerated_output_of_a_token_list).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[图3-15](#sample_enumerated_output_of_a_token_list)中看到这个结果的示例。
- en: '![Sample enumerated output of a token list](Images/t2pr_0315.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![标记列表的示例枚举输出](Images/t2pr_0315.png)'
- en: Figure 3-15\. Sample enumerated output of a token list
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-15. 标记列表的示例枚举输出
- en: 'Next you need to make this into a dictionary. A dictionary is really a collection
    of key-value pairs used as a lookup table: when you give it a key, it returns
    the value corresponding to that key. The notation to build a dictionary, with
    the key being the token and the value being the integer, is:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要将其制作成一个字典。字典实际上是一组键值对，用作查找表：当您给出一个键时，它会返回与该键对应的值。构建字典的表示法，键是标记，值是整数：
- en: '[PRE54]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The output will look like [Figure 3-16](#sample_of_character-to-index_dictionary).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于[图3-16](#sample_of_character-to-index_dictionary)。
- en: 'This dictionary is used to convert text into integers. At inference time, the
    model output is also in the format of integers. Therefore, if you want the output
    as text, then you’ll need a reverse dictionary to map the integers back to characters.
    To do this, simply reverse the order of `i` and `u`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个字典用于将文本转换为整数。在推理时，模型输出也是整数格式。因此，如果希望输出为文本，则需要一个反向字典将整数映射回字符。要做到这一点，只需颠倒`i`和`u`的顺序：
- en: '[PRE55]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![Sample of character-to-index dictionary](Images/t2pr_0316.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![字符到索引字典的示例](Images/t2pr_0316.png)'
- en: Figure 3-16\. Sample of character-to-index dictionary
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-16. 字符到索引字典的示例
- en: Tokenization is the most basic and necessary step in most NLP problems. A text
    generation model will not generate plain text as the output; it generates the
    output in a series of integers. In order for this series of indices to map to
    letters (tokens), you need a lookup table. `index_to_char` is specifically built
    for this purpose. Using `index_to_char`, you can look up each character (token)
    by key, where the key is the index from the model’s output. Without `index_to_char`,
    you will not be able to map model outputs back to a readable, plain-text format.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化是大多数自然语言处理问题中最基本和必要的步骤。文本生成模型不会生成纯文本作为输出；它会生成一系列整数作为输出。为了使这一系列索引映射到字母（标记），您需要一个查找表。`index_to_char`就是专门为此目的构建的。使用`index_to_char`，您可以通过键查找每个字符（标记），其中键是模型输出的索引。没有`index_to_char`，您将无法将模型输出映射回可读的纯文本格式。
- en: Wrapping Up
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you learned how to handle some of the most common data structures:
    tables, images, and text. Tabular datasets (the structured, CSV-style data) are
    very common, are returned from a typical database query, and are frequently used
    as training data. You learned how to deal with columns of different data types
    in such structures, as well as how to model feature interactions by crossing columns
    of interest.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何处理一些最常见的数据结构：表格、图像和文本。表格数据集（结构化的、类似CSV的数据）非常常见，通常从数据库查询返回，并经常用作训练数据。您学会了如何处理这些结构中不同数据类型的列，以及如何通过交叉感兴趣的列来建模特征交互。
- en: For image data, you learned that you need to standardize image size and pixel
    values before using the image collection as a whole to train a model, and that
    you need to keep track of image labels.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像数据，您学会了在使用整个图像集训练模型之前需要标准化图像大小和像素值，以及需要跟踪图像标签。
- en: Text data is by far the most diverse data type, in terms of both format and
    use. Nevertheless, whether the data is for text classification, translation, or
    question-and-answer models, tokenization and dictionary construction processes
    are very common. The methods and approaches described in this chapter are by no
    means exhaustive or comprehensive; rather, they represent “table stakes” when
    dealing with these data types.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据在格式和用途方面是最多样化的数据类型。然而，无论数据是用于文本分类、翻译还是问答模型，标记化和字典构建过程都非常常见。本章描述的方法和方法并不是详尽或全面的；相反，它们代表了处理这些数据类型时的“基本要求”。
- en: ^([1](ch03.xhtml#idm45772916304408-marker)) From an answer on [StackOverflow](https://oreil.ly/1iVv1)
    by user Joe Kington, January 13, 2016, accessed October 23, 2020.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.xhtml#idm45772916304408-marker)) 用户Joe Kington在[StackOverflow](https://oreil.ly/1iVv1)上的回答，2016年1月13日，2020年10月23日访问。
