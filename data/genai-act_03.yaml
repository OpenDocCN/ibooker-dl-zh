- en: '3 Working through an API: Generating text'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 通过API工作：生成文本
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Generative AI models and their categorization based on specific applications
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据特定应用对生成式AI模型及其分类
- en: The process of listing available models, understanding their capabilities, and
    choosing the appropriate ones
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出可用的模型、了解它们的功能以及选择合适的模型的过程
- en: The completion API and chat completion API offered by OpenAI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI提供的完成API和聊天完成API
- en: Advanced options for completion and chat completion APIs that help us steer
    the model and hence control the generation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成API和聊天完成API的高级选项，帮助我们引导模型并因此控制生成
- en: The importance of managing tokens in a conversation for improved user experience
    and cost-effectiveness
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对话中管理令牌以改善用户体验和成本效益的重要性
- en: 'We have seen that large language models (LLMs) provide a powerful suite of
    machine learning tools specifically designed to enhance natural language understanding
    and generation. OpenAI features two notable APIs: the completion and the chat
    completion APIs. These APIs, unique in their dynamic and effective text-generation
    capabilities, resemble human output. In addition, they offer developers exclusive
    opportunities to craft various applications, from chatbots to writing assistants.
    OpenAI was the first to introduce the pattern of completion and chat completion
    APIs, which now embody almost all implementations, especially when companies want
    to build generative-AI-powered tools and products.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，大型语言模型（LLMs）提供了一套强大的机器学习工具，这些工具专门设计用于增强自然语言理解和生成。OpenAI提供了两个显著的API：完成API和聊天完成API。这些API以其动态和有效的文本生成能力而独树一帜，其输出类似于人类。此外，它们还为开发者提供了独特的机遇，可以构建从聊天机器人到写作助手的各种应用。OpenAI是第一个引入完成API和聊天完成API模式的公司，现在这一模式几乎包含了所有实现，尤其是在公司想要构建由生成式AI驱动的工具和产品时。
- en: The completion API by OpenAI is an advanced tool that generates contextually
    appropriate and coherent text to complete user prompts. Conversely, the chat completion
    API was designed to emulate an interaction with a machine learning model, preserving
    the context of a conversation across multiple exchanges, which makes it suitable
    for interactive applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的完成API是一个高级工具，它生成与上下文相关且连贯的文本以完成用户提示。相反，聊天完成API被设计成模拟与机器学习模型的交互，保持对话在多次交流中的上下文，这使得它适合交互式应用。
- en: Chapter 3 establishes the groundwork for scaling enterprises. These APIs can
    significantly accelerate the development of intelligent applications, thereby
    reducing the time to value. We’ll mostly use OpenAI and Azure OpenAI as illustrative
    examples, often interchangeably. The code models remain consistent, and the APIs
    are largely similar. Many enterprises may gravitate toward Azure OpenAI because
    of the control it offers, while others might favor OpenAI. It is important to
    note that we assume here that an Azure OpenAI instance has already been deployed
    as part of your Azure subscription, and we will be referencing it in the context
    of our examples.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章为企业的扩展奠定了基础。这些API可以显著加速智能应用的开发，从而缩短价值实现的时间。我们将主要使用OpenAI和Azure OpenAI作为示例，经常互换使用。代码模型保持一致，API也大体相似。许多企业可能会因为Azure
    OpenAI提供的控制而倾向于它，而其他人可能会更偏好OpenAI。重要的是要注意，我们在这里假设Azure OpenAI实例已经作为您Azure订阅的一部分部署，我们将在示例的上下文中引用它。
- en: This chapter outlines the basics of the completion and the chat completion APIs,
    including how they differ and when to use each. We will see how to implement them
    in an application and how we can steer the model generation and its randomness.
    We’ll also see how to manage tokens, which are key operation considerations when
    deploying to production. These are the fundamental aspects required to build on
    for a mission-critical application. But first, let’s start by understanding the
    different model categories and their advantages.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了完成API和聊天完成API的基本知识，包括它们之间的区别以及何时使用每个API。我们将看到如何在应用中实现它们，以及我们如何引导模型生成及其随机性。我们还将看到如何管理令牌，这在部署到生产环境时是关键的操作考虑因素。这些都是构建关键任务应用所需的基本方面。但在开始之前，让我们先了解不同的模型类别及其优势。
- en: 3.1 Model categories
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 模型类别
- en: Generative AI models can be classified into various categories based on their
    specific applications, and each category includes different types of models. We
    start our discussion by understanding the different classifications of models
    within generative AI. This understanding will help us identify the range of models
    available and choose the most appropriate one for a given situation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其特定的应用，生成式人工智能模型可以划分为各种类别，每个类别都包含不同类型的模型。我们首先通过了解生成式人工智能中模型的分类来开始我们的讨论。这种理解将帮助我们确定可用的模型范围，并选择在特定情况下最合适的一个。
- en: The availability of different types and models may vary, depending on the API
    in use. For example, Azure OpenAI and OpenAI provide different versions of LLMs.
    Some versions might be phased out, some could be limited, and others could be
    exclusive to a certain organization.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型和模型的可用性可能因所使用的 API 而异。例如，Azure OpenAI 和 OpenAI 提供了不同版本的 LLM。一些版本可能会被淘汰，一些可能会受限，而另一些可能仅限于特定组织。
- en: Different models have unique features and capabilities, directly affecting their
    cost and computational requirements. Thus, choosing the right model for each use
    case is critical. In conventional computer science, the idea that bigger is better
    has often been applied to memory, storage, CPUs, or bandwidth. However, in the
    case of LLMs, this principle is not always applicable. OpenAI provides a host
    of models categorized, as shown in table 3.1\. Note that these are the same for
    both OpenAI and Azure OpenAI, as the underlying models are identical.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的模型具有独特的特性和能力，这直接影响了它们的成本和计算需求。因此，为每个用例选择正确的模型至关重要。在传统的计算机科学中，更大的就是更好的这一观念常常应用于内存、存储、CPU
    或带宽。然而，在大型语言模型（LLM）的情况下，这一原则并不总是适用。OpenAI 提供了一系列分类的模型，如表 3.1 所示。请注意，这些模型在 OpenAI
    和 Azure OpenAI 中都是相同的，因为底层模型是相同的。
- en: Table 3.1 OpenAI model categories
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.1 OpenAI 模型类别
- en: '| Model category | Description |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 模型类别 | 描述 |'
- en: '| --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-4  | The newest and most powerful version is a set of multimodal models.
    GPT-4 is trained on a larger dataset with more parameters, making it even more
    capable. It can perform tasks that are out of reach for the previous models. There
    are various models in the GPT-4 family—GPT-4.0, GPT-4 Turbo, and the latest GPT-4o
    (omni), a multimodal model and the most powerful in the family at the time of
    publication.  |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4  | 最新且最强大的版本是一组多模态模型。GPT-4 在更大的数据集和更多参数上进行了训练，使其能力进一步增强。它可以执行之前模型无法完成的任务。GPT-4
    系列中包含各种模型——GPT-4.0、GPT-4 Turbo 和最新的 GPT-4o（全能），这是在发布时该系列中最强大的多模态模型。  |'
- en: '| GPT-3.5  | A set of models that improve on GPT-3 and can understand and generate
    natural language or code. When unsure, these should be the default models for
    most enterprises.  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5  | 一组在 GPT-3 的基础上进行改进的模型，能够理解和生成自然语言或代码。在不确定的情况下，这些应该是大多数企业的默认模型。  |'
- en: '| DALL.E  | A model that can generate images when given a prompt  |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| DALL.E  | 一种在给定提示时能够生成图像的模型  |'
- en: '| Whisper  | A model that is used for speech-to-text, converting audio into
    text  |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Whisper  | 一种用于语音转文本的模型，将音频转换为文本  |'
- en: '| Embeddings  | A set of models to convert text into its numerical form  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Embeddings  | 一组将文本转换为数值形式的模型  |'
- en: '| GPT-3 (Legacy)  | A set of models that can generate and understand natural
    language. These were the original set of models that are now considered legacy.
    In most cases, we would want to start with one of the newer models, 3.5 or 4.0,
    which derive from GPT-3\.  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 (Legacy)  | 一组能够生成和理解自然语言的模型。这些是现在被认为是遗留的原始模型集合。在大多数情况下，我们希望从较新的模型开始，例如
    3.5 或 4.0，这些模型源自 GPT-3。  |'
- en: Each model category contains variations that are further distinguished by certain
    features such as token size. As discussed in the previous chapter, token size
    determines a model’s context window, which defines the amount of input and output
    it can process. For instance, the original GPT-3 models had a maximum token size
    of 2K. GPT-3.5 Turbo, a subset of models within the GPT-3.5 category, has two
    versions—one with a token size of 4K and another with a token size of 16K. These
    are double and quadruple the token size of the original GPT-3 models. Table 3.2
    outlines the more popular models and their capabilities.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型类别都包含一些变体，这些变体通过某些特征（如令牌大小）进一步区分。如前一章所述，令牌大小决定了模型的内容窗口，这定义了它可以处理的输入和输出量。例如，原始
    GPT-3 模型的最大令牌大小为 2K。GPT-3.5 Turbo，GPT-3.5 类别中的模型子集，有两个版本——一个令牌大小为 4K，另一个令牌大小为
    16K。这是原始 GPT-3 模型令牌大小的两倍和四倍。表 3.2 概述了更受欢迎的模型及其功能。
- en: Table 3.2 Model descriptions and capabilities
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.2 模型描述和功能
- en: '| Model | Capabilities |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 功能 |'
- en: '| --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Ada (legacy)  | Simple classification, parsing, and formatting of text. This
    model is part of the GPT-3 legacy.  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Ada (旧版) | 简单的文本分类、解析和格式化。这个模型是 GPT-3 旧版的一部分。|'
- en: '| Babbage (legacy)  | Semantic search ranking, medium complex classification.
    This model is part of the GPT-3 legacy.  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| Babbage (旧版) | 语义搜索排名、中等复杂度的分类。这个模型是 GPT-3 旧版的一部分。|'
- en: '| Curie (legacy)  | Answering questions, highly complex classification. This
    model is part of the GPT-3 legacy.  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Curie (旧版) | 回答问题、高度复杂的分类。这个模型是 GPT-3 旧版的一部分。|'
- en: '| Davinci (legacy)  | Summarization, generating creative content. This model
    is part of the GPT-3 legacy.  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| Davinci (旧版) | 摘要、生成创意内容。这个模型是 GPT-3 旧版的一部分。|'
- en: '| Cushman-Codex (legacy)  | A descendant of the GPT-3 series, trained in natural
    language and billions of lines of code. It is the most capable in Python and proficient
    in over a dozen other programming languages.  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| Cushman-Codex (旧版) | GPT-3 系列的后代，在自然语言和数十亿行代码上进行训练。它在 Python 中最强大，并且精通十多种其他编程语言。|'
- en: '| Davinci-Codex  | A more capable model of Cushman-codex  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Davinci-Codex | Cushman-codex 的更强大模型 |'
- en: '| GPT3.5-Turbo  | The most capable GPT-3.5 model optimized for chat use cases
    is 90% cheaper and more effective than GPT-3 Davinci.  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| GPT3.5-Turbo | 专为聊天用例优化的最强大的 GPT-3.5 模型，比 GPT-3 Davinci 便宜 90% 且更有效。|'
- en: '| GPT-4, GPT-4 Turbo  | More capable than any GPT-3.5 model. It is able to
    do more complex tasks and is optimized for chat models.  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4, GPT-4 Turbo | 比任何 GPT-3.5 模型都更强大。它能够执行更复杂的任务，并针对聊天模型进行了优化。|'
- en: '| GPT-4o  | The latest GPT-4o model is more capable than the GPT-4 and GPT-4
    Turbo, but it is also twice as fast and 50% cheaper.  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 最新 GPT-4o 模型比 GPT-4 和 GPT-4 Turbo 更强大，但速度也快一倍，且便宜 50%。|'
- en: '| text-embedding-ada-002, text-embedding-ada-003  | This new embedding model
    replaces five separate models for text search, similarity, and code search, outperforming
    them at most tasks; furthermore, it is 99.8% cheaper.  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| text-embedding-ada-002, text-embedding-ada-003 | 这个新的嵌入模型取代了五个单独的模型，用于文本搜索、相似度和代码搜索，在大多数任务中表现优于它们；此外，它还便宜了
    99.8%。|'
- en: Note that the mentioned legacy models are still available and work as intended.
    However, the newer models are better, having more mindshare and longer support.
    Most should start with GPT-3.5 Turbo as the default model and use GPT-4 on a case-by-case
    basis. Sometimes, even a smaller, older model, such as the GPT-3 Curie, is good.
    This provides the right balance between the model’s capability, cost, and overall
    performance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，提到的旧版模型仍然可用，并且按预期工作。然而，新模型更好，拥有更多的市场份额和更长的支持。大多数人应该从 GPT-3.5 Turbo 作为默认模型开始，并根据具体情况使用
    GPT-4。有时，甚至是一个较小、较旧的模型，如 GPT-3 Curie，也是好的。这提供了模型能力、成本和整体性能之间的正确平衡。
- en: In the early days of generative AI, all the models were available only to some.
    These will vary by company, region, and in the case of Azure, your subscription
    type, among other things. We have to list the models and their capabilities that
    are available for us to use. However, before listing models, let us see the dependencies
    required to get things working.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式 AI 的早期，所有模型都只对某些人开放。这些将因公司、地区以及 Azure 的情况（例如，您的订阅类型）等因素而有所不同。我们必须列出可用的模型及其功能。然而，在列出模型之前，让我们看看要使一切正常工作所需的依赖项。
- en: 3.1.1 Dependencies
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 依赖项
- en: 'In this section, we call out the run time dependencies and configurations needed
    at a high level. To get things working, we need at least the following items:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了运行时依赖项和所需的高级配置。为了使一切正常工作，我们至少需要以下项目：
- en: '*Development IDE*—We use Visual Studio Code for our examples, but you can use
    anything you are comfortable with.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开发IDE*—我们使用Visual Studio Code作为我们的示例，但您可以使用您感到舒适的任何东西。'
- en: '*Python*—We use v3.11.3 in this book, but you can use any version as long as
    it is v3.7.1 or later. The installation instructions are available at [https://www.python.org/](https://www.python.org/)
    if you need to install Python.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python*—本书中使用的是v3.11.3，但只要它是v3.7.1或更高版本，您可以使用任何版本。如果您需要安装Python，安装说明可在[https://www.python.org/](https://www.python.org/)找到。'
- en: '*OpenAI Python libraries**—*We use Python libraries for most of the code and
    the demos. The OpenAI Python library can be a simple installation in conda, using
    `conda` `install` `-c` `conda-forge` `openai`. If you are using pip, use `pip`
    `install --upgrade openai`. There are also software development kits (SDKs) for
    specific languages if you prefer to use those instead of Python packages.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OpenAI Python库**—*我们大多数代码和演示使用Python库。OpenAI Python库可以通过conda简单安装，使用`conda
    install -c conda-forge openai`。如果您使用pip，请使用`pip install --upgrade openai`。如果您更喜欢使用特定语言的软件开发工具包（SDKs）而不是Python包，也有相应的SDKs。'
- en: '*Azure Subscription or OpenAI API access**—*We use OpenAI’s endpoint and the
    Azure OpenAI (AOAI) endpoint interchangeably; in most cases, either option will
    work. Given the emphasis on enterprises for this book, we tend to lean toward
    using the Azure OpenAI service:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Azure订阅或OpenAI API访问**—*我们交替使用OpenAI的端点和Azure OpenAI (AOAI)端点；在大多数情况下，任选其一即可。鉴于本书对企业的高度重视，我们倾向于使用Azure
    OpenAI服务：'
- en: To use the library with Azure endpoints, we need the `api_key`.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用带有Azure端点的库，我们需要`api_key`。
- en: We also need to set the `api_type`, `api_base`, and `api_version` properties.
    The `api_type` must be set to `azure`, the `api_base` points to the endpoint that
    we deploy, and the corresponding version of the API is specified via `api_version`.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还需要设置`api_type`、`api_base`和`api_version`属性。`api_type`必须设置为`azure`，`api_base`指向我们部署的端点，而API的相应版本通过`api_version`指定。
- en: Azure OpenAI uses '`engine'` as the parameter to specify the model’s name. When
    deploying the model in your Azure subscription, this name needs to be set to your
    chosen name. For example, figure 3.1 is a screenshot of the deployments in one
    subscription. OpenAI, however, uses the parameter `model` to specify the model’s
    name. These model names are standard as they release them. You can find more details
    on Azure OpenAI and OpenAI at [https://mng.bz/yoYd](https://mng.bz/yoYd) and [https://platform.openai.com/docs/](https://platform.openai.com/docs/).
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI使用`'engine'`作为参数来指定模型的名称。当在您的Azure订阅中部署模型时，此名称需要设置为您的选择名称。例如，图3.1是某个订阅中部署的截图。然而，OpenAI使用参数`model`来指定模型的名称。这些模型名称是标准的，因为它们是发布的。您可以在[https://mng.bz/yoYd](https://mng.bz/yoYd)和[https://platform.openai.com/docs/](https://platform.openai.com/docs/)找到更多关于Azure
    OpenAI和OpenAI的详细信息。
- en: Note  The GitHub code repository accompanying the book ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook))
    has the details of the code, including dependencies and instructions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本书配套的GitHub代码库([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook))包含了代码的详细信息，包括依赖项和说明。
- en: Hardcoding the endpoint and key is not an advisable practice. There are multiple
    methods to accomplish this task, one of which includes using environment variables.
    We demonstrate this method in the steps that follow. Other alternatives could
    be fetching them from secret stores or environment files. For the sake of simplicity,
    we will stick to environment variables in this guide. However, you are encouraged
    to adhere to your enterprise’s best practices and recommendations. Setting up
    the environment variables can be achieved through the following commands.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 将端点和密钥硬编码不是一种推荐的做法。有多种方法可以完成这项任务，其中一种包括使用环境变量。我们将在以下步骤中演示这种方法。其他替代方案可以是从密钥库或环境文件中获取它们。为了简单起见，我们将在这份指南中坚持使用环境变量。然而，我们鼓励您遵循您企业的最佳实践和建议。设置环境变量可以通过以下命令实现。
- en: For Windows, these are
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Windows，这些是
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note  You may need to restart your terminal to read the new variables.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可能需要重新启动终端以读取新变量。
- en: On Linux/Mac, we have
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux/Mac上，我们有
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Bash uses
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Bash使用
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note  In this book, we will use conda, an open source package manager, to manage
    our specific runtime versions and dependencies. Technically, using a package manager
    like conda is not mandatory, but it is extremely beneficial for isolating and
    troubleshooting problems and is highly recommended. We won’t delve into the specifics
    of installing conda in this context; for detailed, step-by-step instructions on
    how to install it, please refer to the official documentation at [https://docs.conda.io/](https://docs.conda.io/).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本书中，我们将使用conda，一个开源的包管理器，来管理我们的特定运行时版本和依赖项。技术上，使用像conda这样的包管理器不是强制性的，但它对于隔离和解决问题非常有帮助，并且强烈推荐。在此上下文中，我们不会深入介绍如何安装conda；有关如何安装的详细、分步说明，请参阅官方文档[https://docs.conda.io/](https://docs.conda.io/)。
- en: 'First, let us create a new conda environment and install the required OpenAI
    Python library:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个新的conda环境并安装所需的OpenAI Python库：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have our dependencies installed, let’s connect to the Azure OpenAI
    endpoint and get details of the available models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了依赖项，让我们连接到Azure OpenAI端点并获取可用模型的详细信息。
- en: 3.1.2 Listing models
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 列出模型
- en: As we outlined earlier, each organization may have different models for use.
    We’ll start by understanding what models we have access to; we’ll use the APIs
    to help us set up the basic environment and get it running. Then, I’ll show you
    how to do this using the Azure OpenAI Python SDK and outline the differences when
    using the OpenAI API.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前概述的，每个组织可能都有不同的模型可供使用。我们将首先了解我们有哪些模型可以使用；我们将使用API来帮助我们设置基本环境并使其运行。然后，我将向您展示如何使用Azure
    OpenAI Python SDK来完成这项工作，并概述使用OpenAI API时的差异。
- en: As the next listing shows, we connect to the Azure OpenAI endpoint, get a list
    of all the models available, iterate over those, and print out the details of
    each model to the console.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，我们连接到Azure OpenAI端点，获取所有可用模型的列表，遍历这些模型，并将每个模型的详细信息打印到控制台。
- en: Listing 3.1 Listing Azure OpenAI models available
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1 列出可用的Azure OpenAI模型
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Required for Azure OpenAI endpoints'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 必须用于Azure OpenAI端点'
- en: '#2 This is the environment variable pointing to the endpoint published via
    the Azure portal.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 这是指向通过Azure门户发布的端点的环境变量。'
- en: '#3 Choose the API version we want to use from the multiple options.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从多个选项中选择我们想要使用的API版本。'
- en: '#4 This is the environment variable with the API key.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 这是包含API密钥的环境变量。'
- en: Running this code will present us with a list of available models. The following
    listing shows an example of the models available; the exact list may be different
    for you.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将向我们展示可用模型的列表。以下列表显示了可用模型的示例；您的确切列表可能会有所不同。
- en: Listing 3.2 Listing Azure OpenAI models’ output
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2 列出Azure OpenAI模型的输出
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Each model is characterized by its distinct capabilities, suggesting the use
    cases for which it is tailored—specifically for chat completions, completions
    (which are regular text completions), embeddings, and fine-tuning. For example,
    a chat completion model would be the ideal selection in a situation where conversational
    engagement is required, like a chat-based interaction that requires significant
    dialogue exchange. Conversely, a completion model would be the most suitable for
    text generation. We can view the OpenAI base models with Azure AI Studio in figure
    3.1.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都有其独特的功能，这表明了它针对的使用场景——特别是用于聊天补全、补全（即常规文本补全）、嵌入和微调。例如，在需要大量对话交流的基于聊天的交互中，聊天补全模型将是理想的选择。相反，补全模型将最适合文本生成。我们可以在图3.1中查看Azure
    AI Studio中的OpenAI基础模型。
- en: '![figure](../Images/CH03_F01_Bahree.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F01_Bahree.png)'
- en: Figure 3.1 Base model listed
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1 列出的基础模型
- en: This feature is part of Azure AI Studio, which you can access when logging into
    your Azure subscription and accessing your Azure OpenAI deployment. You can also
    access it directly via the portal at [https://oai.azure.com/portal](https://oai.azure.com/portal).
    Now that we know which model to use, let’s generate some text. We’ll use the completion
    API and a model that supports completions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能是Azure AI Studio的一部分，您可以在登录Azure订阅并访问您的Azure OpenAI部署时访问它。您也可以直接通过门户[https://oai.azure.com/portal](https://oai.azure.com/portal)访问它。现在我们已经知道了要使用哪个模型，让我们生成一些文本。我们将使用补全API和一个支持补全的模型。
- en: 3.2 Completion API
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 补全API
- en: The completion API is a sophisticated tool that generates text to complete prompts
    provided by the user. It forms the backbone of the OpenAI API and offers a simple
    yet robust and flexible API. It is designed to produce text that is coherent and
    contextually fitting for the given prompt.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 完成API是一个复杂的工具，用于生成文本以完成用户提供的提示。它是OpenAI API的核心，提供了一个简单但强大且灵活的API。它旨在生成与给定提示一致且上下文合适的文本。
- en: Many generation examples that are not chat-type constructs use the completion
    API. We must use the completion API to generate text that is not a chat-style
    conversation. Some of the benefits of completion API are
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 许多非聊天型结构的生成示例都使用完成API。我们必须使用完成API来生成非聊天风格的文本。完成API的一些好处包括
- en: '*Contextual understanding*—The completion API can understand the context of
    the prompt and generate relevant text.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文理解*—完成API可以理解提示的上下文并生成相关文本。'
- en: '*Versatility*—It can be used in various applications, from creating content
    to answering questions, which makes it a valuable tool for multiple applications.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多功能性*—它可以用于各种应用，从创建内容到回答问题，使其成为多种应用的宝贵工具。'
- en: '*Multiple language understanding*—The completion API can understand and generate
    content in several languages, which makes it a global resource.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多语言理解*—完成API能够理解和生成多种语言的内容，使其成为全球资源。'
- en: '*Easy implementation*—The completion API is straightforward, which makes it
    accessible to developers of various skill levels.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*易于实现*—完成API简单直接，这使得它对各种技能水平的开发者都易于访问。'
- en: 'The API’s structure is quite simple, as shown in the following snippet. The
    input (prompt) and the output (completion) are in text format. The API response
    is a JSON object from which the generated text can be extracted using the text
    key. This response is called text completion. The completion strives to adhere
    to the instructions and context provided in the prompt and is one of the potential
    outputs:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: API的结构非常简单，如下面的代码片段所示。输入（提示）和输出（完成）都是文本格式。API响应是一个JSON对象，可以使用text键从中提取生成的文本。这个响应被称为文本完成。完成试图遵循提示和上下文中提供的内容，并且是潜在输出之一：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We start with an instruction, which is the prompt that specifies what we aim
    to generate. In our example, the instruction asks the model to generate a few
    bullets outlining why pets are awesome. The completion API has numerous parameters,
    but the most essential ones are detailed in table 3.3\. We discussed many other
    parameters earlier in this chapter and the book (e.g., the prompt, tokens, and
    temperatures). The stop sequences, however, are a new concept. We can employ these
    sequences to make the model cease generating tokens at a certain point, such as
    at the end of a sentence or a list.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个指令开始，即指定我们想要生成的提示。在我们的例子中，指令要求模型生成几个要点，概述为什么宠物很棒。完成API有许多参数，但最重要的参数在表3.3中详细说明。我们在本章和书中讨论了许多其他参数（例如，提示、标记和温度）。然而，停止序列是一个新概念。我们可以使用这些序列使模型在某个特定点停止生成标记，例如在句子的末尾或列表的末尾。
- en: Table 3.3 Completion API
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.3 完成API
- en: '| Parameter | Type | Default value | Description |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 默认值 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `prompt`  | String or array  | `<\&#124;endoftext\&#124;>`  | A string or
    an array of strings is the prompt used to generate these completions.  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `prompt`  | 字符串或数组  | `<\&#124;endoftext\&#124;>`  | 一个字符串或字符串数组是用于生成这些完成的提示。  |'
- en: '| `max_tokens`  | Integer  | 16  | This is the maximum number of tokens to
    generate in the completion, including the prompt. The `max_tokens` must not exceed
    the model’s context length.  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `max_tokens`  | 整数  | 16  | 这是在完成中生成的最大标记数，包括提示。`max_tokens`不能超过模型的上下文长度。  |'
- en: '| `temperature`  | Number (float)  | 1  | This ranges between 0 and 2\. Higher
    values mean the model takes more risks and gets more creative.  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `temperature`  | 数字（浮点数）  | 1  | 这个值介于0和2之间。更高的值意味着模型承担更多风险并更具创造性。  |'
- en: '| `stop`  | String or array  | Null  | This can be up to four sequences where
    the API stops generating further tokens. The returned text will not contain the
    stop sequence.  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `stop`  | 字符串或数组  | Null  | 这可以是多达四个序列，API会在这些序列处停止生成更多标记。返回的文本将不包含停止序列。  |'
- en: '| `n`  | Integer  | 1 (optional)  | This defines the number of completions
    to generate for each prompt. This generates many completions and can quickly consume
    the token limit; we should have a reasonable setting for `max_tokens` and stop
    managing cost.  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `n`  | 整数  | 1 (可选)  | 这定义了为每个提示生成多少个完成。这会生成许多完成，并可能快速消耗令牌限制；我们应该为 `max_tokens`
    设置一个合理的值并停止管理成本。  |'
- en: '| `stream`  | Boolean  | False (optional)  | This is a flag controlling whether
    to stream back partial progress as tokens are generated. If set, the stream is
    terminated by a data `[DONE]`message.  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `stream`  | 布尔值  | False (可选)  | 这是一个标志，用于控制是否在生成标记时流回部分进度。如果设置，流将通过数据 `[DONE]`
    消息终止。  |'
- en: '| `best_of`  | Integer  | 1 (optional)  | This generates `best_of` completions
    server-side and returns the best completion. This parameter cannot be used with
    gpt-35-turbo.  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `best_of`  | 整数  | 1 (可选)  | 这将在服务器端生成 `best_of` 完成并返回最佳完成。此参数不能与 gpt-35-turbo
    一起使用。  |'
- en: '| `top_p`  | Number (float)  | 1 (optional)  | This controls randomness using
    a technique called nucleus sampling, an alternative to the `temperature` setting
    with a value ranging between 0 and 1\.  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `top_p`  | 数值 (浮点数)  | 1 (可选)  | 这通过称为核采样（nucleus sampling）的技术控制随机性，是 `temperature`
    设置的替代方案，其值介于 0 和 1 之间。  |'
- en: '| `logit_bias`  | Map  | Null (optional)  | This defines the likelihood of
    specified tokens appearing in the completion. It uses a mapping of tokens to a
    bias value (–100 of a ban to 100 of exclusive selection).  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `logit_bias`  | 映射  | Null (可选)  | 这定义了指定标记在完成中出现的可能性。它使用标记到偏差值的映射（从禁止的 -100
    到独家选择的 100）。  |'
- en: '| `user`  | String  | Null (optional)  | This parameter is a unique ID representing
    the end-user; it can help debug, monitor, and detect abuse.  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `user`  | 字符串  | Null (可选)  | 此参数是一个代表最终用户的唯一 ID；它可以帮助调试、监控和检测滥用。  |'
- en: '| `logprobs`  | Integer  | Null (optional)  | This is an optional array of
    log probabilities representing the alternate tokens and their likelihood considered
    for completion. This parameter cannot be used with gpt-35-turbo.  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `logprobs`  | 整数  | Null (可选)  | 这是一个可选的日志概率数组，表示考虑用于完成的替代标记及其可能性。此参数不能与
    gpt-35-turbo 一起使用。  |'
- en: '| `suffix`  | String  | Null (optional)  | This parameter can be a string of
    up to 40 characters added as a suffix to the generated text.  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `suffix`  | 字符串  | Null (可选)  | 此参数可以是一个最多 40 个字符的字符串，作为后缀添加到生成的文本中。  |'
- en: '| `echo`  | Boolean  | False (optional)  | This determines whether the prompt
    is included in the completion. This is useful for use cases that need to capture
    the prompts and for debugging purposes. It cannot be used with gpt-35-turbo.  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| `echo`  | 布尔值  | False (可选)  | 这确定是否将提示包含在完成中。这对于需要捕获提示和用于调试目的的情况很有用。它不能与
    gpt-35-turbo 一起使用。  |'
- en: '| `presence_ penalty`  | Number (float)  | 0 (optional)  | This parameter steers
    the model’s tendency and helps outline its behavior to introduce new topics or
    ideas into the generated text. It ranges from 0.0 to 1.0\.  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `presence_ penalty`  | 数值 (浮点数)  | 0 (可选)  | 此参数引导模型的趋势并帮助概述其行为，以便将新主题或想法引入生成的文本。其范围在
    0.0 到 1.0 之间。  |'
- en: '| `frequency_ penalty`  | Number (float)  | 0 (optional)  | This is another
    parameter that helps steer the model and improve the generation results. It controls
    the level of common or uncommon words in the generated text and can be set to
    a value from 0.0 to 1.0\.  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `frequency_ penalty`  | 数值 (浮点数)  | 0 (可选)  | 这是另一个帮助引导模型并改进生成结果的参数。它控制生成文本中常见或不常见单词的水平，可以设置为
    0.0 到 1.0 之间的值。  |'
- en: '| `function_ call`  |  |  | This controls how the model responds to functions
    when function calling is desired. It only works with 0613 or newer versions of
    the OpenAI models.  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `function_ call`  |  |  | 这控制模型在需要函数调用时的响应方式。它仅适用于 OpenAI 模型的 0613 或更高版本。  |'
- en: '| `functions`  |  |  | This is a list of functions that the model may use.  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `functions`  |  |  | 这是一系列模型可能使用的函数。  |'
- en: Note that the table only lists the most used parameters. It helps us understand
    some of the flows and concepts. Some parameters, such as functions, have more
    advanced uses, which will be covered in later chapters on prompt engineering.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，该表仅列出最常用的参数。它帮助我们理解一些流程和概念。一些参数，如函数，有更高级的使用，这些将在后续章节中关于提示工程中介绍。
- en: We stick with the pets theme and use the model to help us suggest names for
    a pet salon business. We ask for three names, and the instructions also outline
    some of the important characteristics to use. These aspects of the instructions
    help us steer the model toward some desired attributes. Please refer to the API
    documentation for a full list of parameters. Let’s call the completion API and
    walk through it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续以宠物为主题，并使用模型帮助我们为宠物沙龙业务建议名称。我们要求提供三个名称，并且说明也概述了一些重要的特性。这些说明的特性帮助我们引导模型向一些期望的属性发展。请参阅API文档以获取参数的完整列表。让我们调用完成API并了解它。
- en: Listing 3.3 Calling the completion API
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.3 调用完成API
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Completion API call for generating text'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用于生成文本的完成API调用'
- en: '#2 Specifies the model to use; note that this name will change based on what
    you set in the deployment'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 指定要使用的模型；请注意，此名称将根据您在部署中设置的值而更改'
- en: '#3 Prompt'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 提示'
- en: '#4 Model configurations'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 模型配置'
- en: '#5 Extracts the generated text from the response'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 从响应中提取生成的文本'
- en: Congratulations! We used the API for our first text generation. Because of the
    nondeterministic nature of AI, especially generative AI, the output you will see
    when running this differs from
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们使用了API进行第一次文本生成。由于AI，特别是生成式AI的非确定性，运行此操作时您将看到的输出与
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The output is as follows.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '**![image](../Images/Prompt.png)**Suggest three names for a new pet salon business.
    The generated name ideas should evoke positive emotions and the following key
    features: professional, friendly, personalized service.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**![图片](../Images/Prompt.png)**为新的宠物沙龙业务建议三个名称。生成的名称想法应唤起积极的情绪，并具有以下关键特性：专业、友好、个性化服务。'
- en: '**![image](../Images/Response.png)**1\. Pawsitively Professional Pet Salon'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**![图片](../Images/Response.png)**1. Pawsitively 专业宠物沙龙'
- en: 2\. Fur & Feathers Friendly Pet Parlor
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 毛皮与羽毛友好宠物沙龙
- en: 3\. Happy Tails Personalized Pet Pampering
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. Happy Tails 定制宠物宠爱
- en: Note  LLMs and most other generative AI models are nondeterministic, meaning
    that identical inputs could give different outputs. Changing the temperature setting
    to zero can make the outputs more deterministic, but a small amount of variability
    may remain.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：LLM和大多数其他生成式AI模型是非确定性的，这意味着相同的输入可能会产生不同的输出。将温度设置更改为零可以使输出更确定，但可能仍会保留一些可变性。
- en: 3.2.1 Expanding completions
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 扩展完成
- en: Let’s see what a complete response from the API looks like and walk through
    that structure. The following listing shows the full response from the API. The
    `choices` field is among the most interesting, given that it has the completion
    text. The choices property is an array, where each item has an `index`, the reason
    the generation finished (`finish_reason`), and the generated text (via the `text`
    property).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看API的完整响应是什么样的，并了解其结构。以下列表显示了API的完整响应。`choices`字段是最有趣的，因为它包含完成文本。选择属性是一个数组，其中每个项目都有一个`index`，表示生成结束的原因（`finish_reason`），以及生成的文本（通过`text`属性）。
- en: Listing 3.4 API response from a completion API
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.4 完成API的API响应
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Array of completion data'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 完成数据数组'
- en: '#2 Response creation datetime stamp'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 响应创建日期时间戳'
- en: '#3 Unique ID of the response'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 响应的唯一ID'
- en: '#4 Model ID used to generate the response'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 生成响应所使用的模型ID'
- en: '#5 Count of tokens used in this request'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 此请求中使用的标记计数'
- en: Table 3.4 shows the remaining properties. The usage property outlines the tokens
    used (`total_tokens`), including the prompt and response tokens. Because we pay
    per token, it is important to structure the prompt for aspects—first, to return
    only what is needed, minimizing token usage, and second, to limit the number of
    tokens generated in the first place.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.4显示了剩余的属性。使用属性概述了使用的标记（`total_tokens`），包括提示和响应标记。因为我们按标记付费，所以对提示进行结构化是很重要的——首先，只返回所需的内容，最小化标记使用，其次，最初限制生成的标记数量。
- en: Table 3.4 Completion response properties
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.4 完成响应属性
- en: '| Property | Description |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 描述 |'
- en: '| --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `choices`  | An array that can contain one or more completions data  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| `choices`  | 一个可以包含一个或多个完成数据的数组 |'
- en: '| `created`  | UNIX date-time stamp when the response was created  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| `created`  | 响应创建时的UNIX日期时间戳 |'
- en: '| `id`  | A unique identifier of the response is useful when we need to track
    responses  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| `id`  | 当我们需要跟踪响应时，响应的唯一标识符很有用 |'
- en: '| `model`  | Represents the model that was used for the generation  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| `model`  | 表示用于生成的模型 |'
- en: '| `object`  | Outlines the data type of the response (e.g., in this case, it
    is a `text_completion`, outlining a completion API)  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| `object`  | 描述响应的数据类型（例如，在本例中，它是`text_completion`，概述了完成API）  |'
- en: '| `usage`  | Counts the number of tokens used by this request  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| `usage`  | 计算此请求使用的标记数量  |'
- en: A property called `logprobs` specifies the number of log probabilities to generate
    for each token in the response. The log probabilities are useful for generating
    more diverse and interesting responses. It returns the log probabilities of the
    top *n* tokens for each token in the response. The log probabilities are returned
    as an array of arrays, where each subarray corresponds to a token in the response
    and contains the log probabilities of the top *n* tokens for that token.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为`logprobs`的属性指定为响应中的每个标记生成多少个对数概率。对数概率对于生成更多样化和有趣的响应很有用。它返回响应中每个标记的前*n*个标记的对数概率。对数概率以数组的形式返回，其中每个子数组对应于响应中的一个标记，并包含该标记的前*n*个标记的对数概率。
- en: 3.2.2 Azure content safety filter
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 Azure内容安全过滤器
- en: Sometimes, the API returns a `null` response, as shown in listing 3.5\. When
    this happens, we should check the value of the `finish_reason` field. If its value
    is set to `content_filter`, the content filtering system that works alongside
    models has been triggered. The `finish_reason` field indicates why the API returned
    the output it did, and every response will include this field. This topic will
    be covered in more detail later in the chapter.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，API返回一个`null`响应，如列表3.5所示。当这种情况发生时，我们应该检查`finish_reason`字段的值。如果其值设置为`content_filter`，则与模型一起工作的内容过滤系统已被触发。`finish_reason`字段指示API为何返回了它所返回的输出，并且每个响应都将包括此字段。这个主题将在本章后面更详细地介绍。
- en: The filtering system uses specific categories to identify and act on potentially
    harmful content as part of both the input prompts and generated completions. The
    application that uses these APIs must handle this situation and retry after the
    appropriate back-off period. The content safety filter and ethical AI will be
    covered in more detail in chapter 13.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤系统使用特定类别来识别和针对输入提示和生成的完成项中的潜在有害内容采取行动。使用这些API的应用程序必须处理这种情况，并在适当的退避期间后重试。内容安全过滤器和道德AI将在第13章中更详细地介绍。
- en: Listing 3.5 Output showing `null` response
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5 显示`null`响应的输出
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 null response'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 无响应'
- en: '#2 Content filter is the reason the response finished.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 内容过滤器是响应结束的原因。'
- en: 3.2.3 Multiple completions
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 多重完成
- en: 'We might want multiple completions for a few reasons. Sometimes, we need to
    generate multiple message choices for the same prompt. At other times, the API
    is throttled for capacity reasons, and we might want to get more from the same
    API call instead of being rate limited. The completions API can return multiple
    responses; this is done by setting the `n` parameter to more than the default
    value of 1\. For example, we can add this parameter to the completion call:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能有多个完成项的几个原因。有时，我们需要为相同的提示生成多个消息选择。在其他时候，API由于容量原因被限制，我们可能希望从相同的API调用中获得更多，而不是受到速率限制。完成API可以返回多个响应；这是通过将`n`参数设置为大于默认值1来完成的。例如，我们可以将此参数添加到完成调用中：
- en: '[PRE11]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: When we run this updated code, we get the response shown in listing 3.6\. The
    property choices are an array, and we have three items, with the index starting
    at a base zero. Each has the generated text for us to use. Depending on the use
    case, this is helpful when picking multiple completions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行此更新后的代码时，我们得到列表3.6中所示的响应。属性选择是一个数组，我们有三项，索引从零开始。每一项都为我们提供了生成的文本。根据使用情况，在挑选多个完成项时这很有帮助。
- en: Listing 3.6 Output showing multiple responses
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.6 显示多个响应的输出
- en: '[PRE12]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Another similar but more powerful parameter is the `best_of` parameter. Like
    the `n` parameter, it generates multiple completions, allowing the option to pick
    the best. The `best_of` is the completion with the highest log probability per
    token. We cannot stream results when using this option. However, it can be combined
    with the `n` parameters, with `best_of` needs greater than `n`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个类似但更强大的参数是`best_of`参数。与`n`参数类似，它生成多个完成项，允许选择最佳项。`best_of`是每个标记具有最高对数概率的完成项。使用此选项时，我们不能流式传输结果。然而，它可以与`n`参数结合使用，其中`best_of`需要大于`n`。
- en: As shown in the following listing, if we set `n` to 5, we get five completions
    as expected; for brevity, we do not show all five of the completions here, but
    note that this call uses 184 tokens.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下列表所示，如果我们将`n`设置为5，我们会得到预期的五个完成项；为了简洁，我们这里不展示所有五个完成项，但请注意，这个调用使用了184个标记。
- en: Listing 3.7 Output showing multiple responses
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.7 显示多个响应的输出
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If we run a similar call using the `best_of` parameter, do not specify the
    `n` parameter:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`best_of`参数运行类似的调用，不要指定`n`参数：
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: When we run this code, we get only one completion, as shown in listing 3.8;
    however, we are using a similar number of tokens as earlier (171 versus 184).
    This is because the service generates five completions on the server side and
    returns the best one. The API uses the log probability per token to pick the best
    option. The higher the log probability, the more confident the model is about
    its prediction.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行此代码时，我们只得到一个完成项，如列表3.8所示；然而，我们使用的标记数量与之前相似（171比184）。这是因为服务在服务器端生成五个完成项，并返回最佳的一个。API使用每个标记的对数概率来选择最佳选项。对数概率越高，模型对其预测的信心就越大。
- en: Listing 3.8 Output generation with `best_of` five completions
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.8 使用`best_of`五个完成项的输出生成
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The one parameter that influences many of the responses is the temperature setting.
    Let’s see how this changes the output.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 影响许多响应的一个参数是温度设置。让我们看看这如何改变输出。
- en: 3.2.4 Controlling randomness
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.4 控制随机性
- en: As discussed in the previous chapter, the `temperature` setting influences the
    randomness of the generated output. A lower temperature produces more repetitive
    and deterministic responses, while a higher temperature produces more innovative
    responses. Fundamentally, there isn’t a right setting—it all comes down to the
    use cases.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，`temperature`设置影响生成输出的随机性。较低的温度会产生更重复和确定性的响应，而较高的温度会产生更多创新的响应。从根本上说，没有正确的设置——一切都取决于用例。
- en: For enterprises, a more creative output would be when there is interest in diverse
    output and creating text for use cases such as content generation for marketing,
    stories, poems, lyrics, jokes, etc. These are things that usually require creativity.
    However, enterprises need more reliable and precise answers for use cases, such
    as document automation for invoice generation, proposals, code generation, etc.
    These settings are applicable per API call, so combining different temperature
    levels in the same workflow is possible.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业来说，更有创意的输出是在对多样化输出感兴趣并创建用于内容生成、故事、诗歌、歌词、笑话等用例的文本时。这些通常需要创造力。然而，企业在需要更可靠和精确的答案的用例，如发票生成、提案、代码生成等时，需要更多。这些设置适用于每个API调用，因此在同一工作流程中结合不同的温度级别是可能的。
- en: As demonstrated in previous examples, we recommend a temperature setting of
    0.8 for creative responses. Conversely, a setting of 0.2 is suggested for more
    predictable responses. Using an example, let us examine how these settings alter
    the output and observe the variations between multiple calls.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，我们建议创意响应的温度设置为0.8。相反，建议将设置调整为0.2以获得更可预测的响应。通过一个例子，让我们看看这些设置如何改变输出，并观察多次调用之间的变化。
- en: 'When the temperature was set to 0.8, we received the following responses from
    three consecutive calls. The output changes as expected, offering suggestions
    like those seen throughout this chapter. It is important to note that we do not
    need to make three separate API calls. We can set the `n` parameter to 3 in a
    single API call to generate multiple responses. Here is what our API call looks
    like:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当温度设置为0.8时，我们从三个连续调用中收到了以下响应。输出如预期的那样改变，提供了本章中看到的建议。重要的是要注意，我们不需要进行三个单独的API调用。我们可以在单个API调用中将`n`参数设置为3以生成多个响应。以下是我们的API调用示例：
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The following listing shows the creative generation for the three responses.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了三种响应的创意生成。
- en: Listing 3.9 Completions output with the temperature at 0.8
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.9 温度设置为0.8时的完成输出
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 First response: get blocked by the content filter'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第一个响应：被内容过滤器阻止'
- en: '#2 Second of three responses'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 第三个响应'
- en: '#3 Final response with very different generated text'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 具有非常不同生成文本的最终响应'
- en: Let’s change the setting to make this more deterministic and run it again. Note
    that the only change in the API call is `temperature=0.2`. The output is predictable
    and deterministic, with very similar text generated between the three responses.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更改设置以使其更加确定，并再次运行。请注意，API调用中唯一的更改是`temperature=0.2`。输出是可预测和确定的，三个回答之间生成的文本非常相似。
- en: Listing 3.10 Completions output with the temperature at 0.2
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.10 温度为0.2时的完成输出
- en: '[PRE18]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#1 One of three responses'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 三个回答中的一个'
- en: '#2 Two of three responses; very similar generated text'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 三个回答中的两个；生成的文本非常相似'
- en: '#3 The final response with very similar generated text'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 最终的回答，生成的文本非常相似'
- en: The temperature value goes up to 2, but it is not recommended to go that high,
    as the model starts hallucinating more and creating nonsensical text. If we want
    more creativity, we usually want it to be at 0.8 and, at most, 1.2\. Let us see
    an example when the temperature is changed to 1.8\. In this example, we did not
    even get the third generation, as we hit the token limit and stopped the generation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 温度值可以高达2，但并不建议设置得那么高，因为模型开始产生更多幻觉并创建无意义的文本。如果我们想要更多的创造力，通常希望它设置为0.8，最多为1.2。让我们看看当温度值变为1.8时的例子。在这个例子中，我们甚至没有完成第三次生成，因为我们达到了标记限制并停止了生成。
- en: Listing 3.11 Completions output with the temperature at 1.8
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.11 温度为1.8时的完成输出
- en: '[PRE19]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 One of three responses with names that aren’t very clear'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 三个回答中的一个，名称不是很清晰'
- en: '#2 Second and third of three responses, with nonsensical names'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 第二和第三个回答，名称不清晰'
- en: 3.2.5 Controlling randomness using top_p
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.5 使用top_p控制随机性
- en: An alternative to the `temperature` parameter for managing randomness is the
    `top_p` parameter. It has the same affect on the generation as the temperature
    parameter, but it uses a different technique called *nucleus sampling*. Essentially,
    nucleus sampling allows only the tokens with a probability equal to or less than
    the value of `top_p` to be considered as part of the generation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 用于管理随机性的`temperature`参数的替代方法是`top_p`参数。它对生成的影响与温度参数相同，但它使用了一种称为*核心采样*的不同技术。本质上，核心采样只允许概率等于或低于`top_p`值的标记被视为生成的一部分。
- en: Nucleus sampling creates texts by picking words from a small group of the most
    likely ones with the highest cumulative probability. The `top_p` value decides
    how small this group is based on the total chance for the words to appear in it.
    The group size can change depending on the next word’s chance. Nucleus sampling
    can help avoid repetition and generate more varied and clearer texts than other
    methods.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 核心采样通过从具有最高累积概率的一小部分最可能出现的单词中挑选单词来创建文本。`top_p`值决定了这个小组的大小，基于单词出现在其中的总概率。小组的大小可以根据下一个单词的概率而变化。核心采样可以帮助避免重复，并生成比其他方法更多样化和清晰的文本。
- en: For example, if we have the `top_p` value set to 0.9, only the tokens that make
    up 90% of the probability distribution will be sampled for the generation of text.
    This allows us to avoid the last 10%, which are often quite random and diverse
    and end up as nonsensical hallucinations.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们把`top_p`值设置为0.9，那么只有组成90%概率分布的标记会被采样用于文本生成。这使我们能够避免最后10%，这部分通常非常随机且多样化，最终导致无意义的幻觉。
- en: A lower value of `top_p` makes the model more consistent and less creative as
    it chooses fewer tokens to generate. Conversely, a higher value makes the generation
    more creative and diverse, as it has a larger set of tokens to operate. The larger
    value also makes it prone to more errors and randomness. The exact value of `top_p`
    depends on the use case; in most cases, the ideal value for `top_p` ranges between
    0.7 and 0.95\. We should change either the temperature attribute or `top_p`, but
    not both. Table 3.5 outlines the relationship between the two.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`top_p`的值较低时，模型在生成时选择较少的标记，因此更加一致且创造力较低。相反，较高的值使生成更加有创造力和多样化，因为它有更大的标记集可供操作。较大的值也使其更容易出现错误和随机性。`top_p`的确切值取决于用例；在大多数情况下，`top_p`的理想值介于0.7和0.95之间。我们应该更改温度属性或`top_p`，但不能同时更改两者。表3.5概述了两者之间的关系。'
- en: Table 3.5 Relationship between temperature and `top_p`
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.5 温度和`top_p`之间的关系
- en: '| Temperature | top_p | Effect |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | top_p | 影响 |'
- en: '| --- | --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Low  | Low  | Generates predictable text that closely follows common language
    patterns  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 低  | 低  | 生成符合常见语言模式的可预测文本 |'
- en: '| Low  | High  | Generates predictable text, but with occasional less common
    words or phrases  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 低  | 高  | 生成的文本可预测，但偶尔会出现不太常见的单词或短语 |'
- en: '| High  | Low  | Generates text that is often coherent but with creative and
    unexpected word usage  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 高  | 低  | 生成的文本通常连贯，但具有创造性和意外的词汇使用 |'
- en: '| High  | High  | Generates highly diverse and unpredictable text with various
    word choices and ideas; has very creative and diverse output, but may contain
    many errors  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 高  | 高  | 生成的文本高度多样且不可预测，具有各种词汇选择和想法；具有非常创造性和多样化的输出，但可能包含许多错误 |'
- en: Let us look at some of the advanced API options for specific scenarios.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些针对特定场景的高级API选项。
- en: 3.3 Advanced completion API options
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 高级完成API选项
- en: Now that we have examined the basic constructs of the completion API and understand
    how they work, we need to consider more advanced aspects of the completion API.
    Many of these might not seem as complex, but they add many more responsibilities
    to the system architecture, complicating overall implementation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经检查了完成API的基本结构并了解了它们的工作原理，我们需要考虑完成API的更高级方面。其中许多可能看起来并不复杂，但它们给系统架构增加了许多更多责任，从而复杂了整体实现。
- en: 3.3.1 Streaming completions
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 流式完成
- en: The completions API allows streaming responses, offering immediate access to
    information as soon as it is ready rather than waiting for a full response. For
    enterprises, streaming can be important in some cases where real-time content
    generation with lower latency is key. This feature can enhance user experiences
    by processing incoming responses promptly.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 完成API允许流式响应，提供在信息准备好后立即访问信息的能力，而不是等待完整响应。对于企业来说，在某些情况下，实时内容生成和较低延迟是关键，流式传输在这种情况下可能很重要。此功能可以通过及时处理传入的响应来增强用户体验。
- en: To enable streaming from the API’s standpoint, modify the `stream` parameter
    to `true`. By default, this optional parameter is set to `false`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要从API的角度启用流式传输，将`stream`参数修改为`true`。默认情况下，此可选参数设置为`false`。
- en: Streaming employs server-sent events (SSE), which require a client-side implementation.
    SSE is a standard protocol allowing servers to continue transmitting data to clients
    after establishing the initial connection. It is a long-term, one-way connection
    from server to client. SSE offers advantages such as low latency, reduced bandwidth
    consumption, and an uncomplicated configuration setup.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 流式传输使用服务器发送事件（SSE），这需要在客户端实现。SSE是一种标准协议，允许服务器在建立初始连接后继续向客户端传输数据。它是从服务器到客户端的长期单向连接。SSE提供诸如低延迟、减少带宽消耗和简单的配置设置等优势。
- en: Listing 3.12 demonstrates how our example can be adjusted to utilize streaming.
    Although the API modification is straightforward, the description and requested
    multiple generations were adjusted (using the `n` property). This allows us to
    generate more text artificially, making it easier to observe the streaming generation.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.12演示了如何调整我们的示例以利用流式传输。尽管API修改很简单，但描述和请求的多个生成（使用`n`属性）已调整。这使得我们可以人为地生成更多文本，从而更容易观察流式生成。
- en: Listing 3.12 Streaming completion
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.12 流式完成
- en: '[PRE20]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 Tweaked the prompt slightly to add descriptions'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 稍微调整了提示以添加描述'
- en: '#2 We need to handle the streaming response on the client side.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们需要在客户端处理流式响应。'
- en: '#3 Enables streaming'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 启用流式传输'
- en: '#4 We need to loop through the array and handle multiple generations.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 我们需要遍历数组并处理多个生成。'
- en: When managing a streaming call, we must pay extra attention to the `finish_reason`
    property. As messages are streamed, each appears as a standard completion, with
    the text representing the newly generated token. In these instances, the `finish_reason`
    remains null. However, the final message differs; its `finish_reason` could be
    either `stop` or `length`, depending on what triggered it.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理流式调用时，我们必须特别注意`finish_reason`属性。随着消息的流式传输，每个消息都表现为标准完成，其中的文本代表新生成的标记。在这些情况下，`finish_reason`保持为null。然而，最后一条消息不同；它的`finish_reason`可以是`stop`或`length`，具体取决于触发它的原因。
- en: Listing 3.13 Streaming finish reason
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.13 流式完成原因
- en: '[PRE21]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '3.3.2 Influencing token probabilities: logit_bias'
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 影响标记概率：logit_bias
- en: The `logit_bias` parameter is one way we can influence output completion. In
    the API, this parameter allows us to manipulate the probability of certain tokens,
    which can be words or phrases, that the model generates in its responses. It is
    called `logit_ bias` because it directly affects the log odds, or logits, that
    the model calculates for each potential token during the generation process. The
    bias values are added to these log-odds before converting them to probabilities,
    altering the final distribution of tokens the model can pick from.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`logit_bias`参数是我们影响输出完成的一种方式。在API中，此参数允许我们操纵模型在响应中生成的某些标记（可以是单词或短语）的概率。它被称为`logit_bias`，因为它直接影响了模型在生成过程中为每个潜在标记计算的log
    odds或logits。偏差值被添加到这些log-odds中，在将它们转换为概率之前，改变模型可以从中选择的标记的最终分布。'
- en: The importance of this feature lies in its ability to steer the model’s output.
    Say we are creating a chatbot and want it to avoid certain words or phrases. We
    can use `logit_bias` to decrease the likelihood of those tokens being chosen by
    the model. In contrast, if there are certain words or phrases we want the model
    to favor, we could use `logit_bias` to increase their likelihood. The range of
    this parameter is from –100 to 100, and it operates on tokens for the word. Setting
    a token to –100 effectively bans it from the generation, whereas setting it to
    100 makes it exclusive.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能的重要性在于其引导模型输出的能力。比如说我们正在创建一个聊天机器人，并希望它避免某些单词或短语。我们可以使用`logit_bias`降低这些标记被模型选择的概率。相反，如果我们希望模型优先考虑某些单词或短语，我们可以使用`logit_bias`增加它们的可能性。此参数的范围是从-100到100，它作用于单词的标记。将标记设置为-100实际上禁止它在生成中出现，而将其设置为100则使其成为独家。
- en: To use `logit_bias`, we provide a dictionary where the keys are the tokens,
    and the val-ues are the biases that need to be applied to those tokens. To get
    the token, we use the `tiktoken` library. Once you have the appropriate token,
    you can assign a positive bias to make it more likely to appear or a negative
    bias to make it less likely, as shown in figure 3.2\. The blocks show the degree
    of probability that different tokens can be at different probabilities of banning
    or exclusive generation. Smaller changes to the tokens’ value increase or decrease
    the probability of these tokens in the generated output.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`logit_bias`，我们提供了一个字典，其中键是标记，值是需要应用于这些标记的偏差。要获取标记，我们使用`tiktoken`库。一旦你有了适当的标记，你可以分配一个正偏差使其更有可能出现，或者分配一个负偏差使其不太可能出现，如图3.2所示。这些块显示了不同标记在不同禁止或独家生成概率下的概率程度。对标记值的较小更改会增加或减少这些标记在生成输出中的概率。
- en: '![figure](../Images/CH03_F02_Bahree.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F02_Bahree.png)'
- en: Figure 3.2 The `logit_bias` parameter
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2 `logit_bias`参数
- en: 'Let’s use an example to see how we can make this work. For our pet salon name,
    we do not want to use the words “purr,” “purrs,” or “meow.” The first thing we
    want to do is create the tokens for these words. We also want to add words with
    a preceding space and capitalize them as spaces. Capital letters are all different
    tokens. So “Meow” and “Meow” (with a space) and “meow” (again with a space) might
    read the same to us, but when it comes to tokens, these words are all different.
    The output shows us the tokens for the corresponding word:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来看看我们如何使这起作用。对于我们的宠物沙龙名称，我们不希望使用“purr”、“purrs”或“meow”这些词。我们首先想做的事情是为这些词创建标记。我们还希望添加带有前导空格的单词，并将它们作为空格大写。大写字母都是不同的标记。所以“Meow”和“Meow”（带有空格）以及“meow”（再次带有空格）对我们来说可能听起来相同，但就标记而言，这些词都是不同的。输出显示了相应单词的标记：
- en: '[PRE22]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that we have the tokens, we can add them to the completion call. Note that
    we assign each token a bias of –100, steering the model away from these words.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了标记，我们可以将它们添加到完成调用中。请注意，我们为每个标记分配了一个-100的偏差，将模型引导远离这些词。
- en: Listing 3.14 `logit_bias` implementation
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.14 `logit_bias`实现
- en: '[PRE23]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#1 Dictionary containing the tokens and the corresponding bias values to steer
    the model on these specific tokens'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 包含标记及其对应偏差值以引导模型在这些特定标记上的字典'
- en: We do not have any words we want to avoid when we run this code.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此代码时，我们没有想要避免的任何单词。
- en: Listing 3.15 Output of `logit_bias` generation
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.15 `logit_bias`生成输出
- en: '[PRE24]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We can do the opposite and positively bias tokens too. Say we want to overemphasize
    and steer the model toward the word “Furry.” We can use the `tiktoken` library
    we saw earlier and find that the tokens for “Furry” are `[37, 16682]`. We can
    update the previous API call with this and, in this case, a positive bias of 5.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采取相反的做法，并对标记进行正偏差。比如说，我们想要过分强调并引导模型向“Furry”这个词靠拢。我们可以使用之前看到的 `tiktoken`
    库，并找到“Furry”的标记是 `[37, 16682]`。我们可以用这个更新之前的 API 调用，在这种情况下，一个正偏差值为 5。
- en: 'Listing 3.16 `logit_bias`: Positive implementation'
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.16 `logit_bias`：正实现
- en: '[PRE25]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: When we run this code, we get the output shown in the following listing. As
    we can see, there is a much stronger emphasis on “Furry” in our generation. The
    completions also take longer, as the model competes with the bias when generating
    certain tokens.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行此代码时，我们得到以下列表中显示的输出。正如我们所见，在我们的生成中，“Furry”的强调程度要强得多。完成也花费了更长的时间，因为模型在生成某些标记时与偏差竞争。
- en: 'Listing 3.17 Output `logit_bias`: Positive implementation'
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.17 输出 `logit_bias`：正实现
- en: '[PRE26]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `logit_bias` feature should be used carefully; it is a powerful tool for
    guiding the model’s output. However, excessive or inappropriate use can lead to
    nonsensical, overly repetitive, or biased output in unexpected ways.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`logit_bias` 功能应谨慎使用；它是一个强大的工具，用于引导模型的输出。然而，过度或不恰当的使用可能导致以意想不到的方式产生无意义、过度重复或带有偏见的输出。'
- en: 3.3.3 Presence and frequency penalties
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 存在性和频率惩罚
- en: We have two additional parameters in the API, called *presence* and *frequency*
    penalties, that help steer the language model’s output by controlling the generation’s
    repetition. These two parameters influence the likelihood of words (technically
    a sequence of tokens) reappearing in a completion. A higher presence penalty encourages
    the model to focus on the prompt and avoid using tokens that already appear there.
    In contrast, a higher frequency penalty discourages the model from repeating itself.
    Let’s take a look at both in a little more detail.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在 API 中，我们还有两个额外的参数，称为 *存在性* 和 *频率* 惩罚，通过控制生成的重复性来帮助引导语言模型的输出。这两个参数影响单词（技术上是一系列标记）在完成中再次出现的可能性。更高的存在性惩罚鼓励模型专注于提示并避免使用已经出现在那里的标记。相比之下，更高的频率惩罚会阻止模型重复自己。让我们更详细地看看这两个参数。
- en: Presence penalty parameter
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 存在性惩罚参数
- en: The presence penalty parameter affects how often the same token appears in the
    output. This is achievable by using the presence penalty as a value subtracted
    from the probability of a token each time it is generated. This means that the
    more a token is used, the less likely it is to be used again. This helps make
    the model use more varied tokens in the generation and explore new topics. The
    value of this parameter can range from 0 to 2.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 存在性惩罚参数影响相同标记在输出中出现的频率。这可以通过在每次生成标记时将其作为从标记概率中减去的值来实现。这意味着一个标记被使用的越多，它再次被使用的可能性就越小。这有助于使模型在生成过程中使用更多样化的标记并探索新主题。此参数的值可以从
    0 到 2。
- en: The default value is 0, meaning the model does not care if a token is repeated.
    A high presence penalty (1.0) makes the model less likely to use the same token
    again, and a higher value makes the model introduce new topics in the output.
    A low presence penalty (0) makes the model stick to the existing topics in the
    text. Each time a token is generated, the parameter value is subtracted from the
    log probability of that token.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值是 0，意味着模型不关心一个标记是否重复。高存在性惩罚（1.0）使得模型不太可能再次使用相同的标记，并且更高的值使得模型在输出中引入新的主题。低存在性惩罚（0）使得模型坚持文本中的现有主题。每次生成一个标记时，参数值都会从该标记的对数概率中减去。
- en: We can improve the quality of the generation by preventing the same text from
    being repeated multiple times, helping control the flow, and making the output
    more engaging. Now let’s look at the frequency penalty parameter.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过防止相同的文本被多次重复来提高生成的质量，帮助控制流程，并使输出更具吸引力。现在让我们看看频率惩罚参数。
- en: Frequency penalty parameter
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 频率惩罚参数
- en: This parameter controls how much the model avoids repeating itself in the output.
    The higher the frequency penalty (1.0), the more the model tries to use different
    words and phrases, which results in a more diverse generation. The lower the frequency
    penalty (0.0), the more the model can repeat the same words and phrases and the
    more predictable the output. This differs from the presence penalty, which encourages
    the model to use new words and phrases. The frequency penalty adds to the log
    probability of a token each time it appears in the output.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数控制模型在输出中避免重复的程度。频率惩罚（1.0）越高，模型尝试使用不同单词和短语的程度就越高，这导致生成更加多样化。频率惩罚（0.0）越低，模型可以重复相同的单词和短语，输出就越可预测。这与存在惩罚不同，存在惩罚鼓励模型使用新单词和短语。频率惩罚会在标记在输出中每次出现时添加到标记的对数概率中。
- en: The best values for both parameters depend on what you want to achieve with
    the output. Usually, choosing values between 0.1 and 1.0 would be best, which
    noticeably affects the output. If you want a stronger effect, you can increase
    the values up to 2.0, but this might reduce the output quality.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个参数的最佳值取决于您想通过输出实现什么。通常，选择介于0.1和1.0之间的值会更好，这会明显影响输出。如果您想要更强的效果，可以将值增加到2.0，但这可能会降低输出质量。
- en: Note that tuning these parameters requires some trial and error to get the desired
    results, as the model’s output is also influenced by many other factors, including
    the prompt you provide and other fine-tuning parameters. Figure 3.3\. shows the
    correlation for both the presence and frequency penalty parameters.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，调整这些参数需要一些尝试和错误才能得到期望的结果，因为模型输出还受到许多其他因素的影响，包括您提供的提示和其他微调参数。图3.3显示了存在和频率惩罚参数的相关性。
- en: '![figure](../Images/CH03_F03_Bahree.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F03_Bahree.png)'
- en: Figure 3.3 Penalty presence parameter
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3 惩罚存在参数
- en: 3.3.4 Log probabilities
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.4 对数概率
- en: When an LLM generates a token, it assigns a probability to the next considered
    token and uses various techniques to pick the token used in the completion from
    these options. The `logprobs` property of the completion API exposes the natural
    logarithm for these probabilities at each step.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个LLM生成一个标记时，它会为下一个考虑的标记分配一个概率，并使用各种技术从这些选项中选择用于完成的标记。完成API的`logprobs`属性暴露了每个步骤这些概率的自然对数。
- en: This is an integer (max value of 5) that shows the alternate tokens considered
    for each token included in the completion. If this value is set to 3, the API
    will return a list of the three most likely tokens for each selected token in
    the generation. Note that the API always returns the `logprobs` of the sampled
    token, so in the response, we might end up with `logprobs + 1` element in the
    array.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个整数（最大值为5），表示在完成过程中考虑的每个标记的备用标记。如果此值设置为3，API将返回生成中每个选定标记最有可能的三个标记列表。请注意，API始终返回采样标记的`logprobs`，因此响应中我们可能会在数组中结束于`logprobs
    + 1`个元素。
- en: Fundamentally, we use this approach to help debug and improve the prompts. If
    the model isn’t generating text we like, we can use this to see what other words
    (technically tokens) the model considered. This allows us to tune some other settings
    to steer the model. Conversely, we can use the same thing to control randomness
    in the model generation and make the output more deterministic. Finally, we can
    also use this to understand how confident the model is. If the probabilities are
    the same for several different words, this means that the model is not certain
    what word comes next.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们使用这种方法来帮助调试和改进提示。如果模型没有生成我们喜欢的文本，我们可以使用这个来查看模型考虑了哪些其他单词（技术上称为标记）。这使我们能够调整一些其他设置来引导模型。相反，我们也可以用同样的方法来控制模型生成中的随机性，使输出更加确定。最后，我们还可以使用这个来了解模型有多自信。如果几个不同单词的概率相同，这意味着模型不确定下一个单词是什么。
- en: 'Say we want to get a name for a white dog; we can call the completion API.
    In this example, we get the name Cotton, which isn’t bad:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想为一个白色的狗取一个名字；我们可以调用完成API。在这个例子中，我们得到了名字Cotton，这还不错：
- en: '[PRE27]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If we want to see what other tokens were considered for the name, we can add
    the `logprobs` properties:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想查看为该名称考虑的其他标记，我们可以添加`logprobs`属性：
- en: '[PRE28]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As seen in the completion output in the following listing, the model considered
    the following tokens: Casper, Coco, and Snow.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下列表中的完成输出所示，模型考虑了以下标记：Casper、Coco和Snow。
- en: Listing 3.18 Output log probabilities
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.18 输出对数概率
- en: '[PRE29]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As a reminder, we should use this property judiciously and only when required.
    Not only does it increase the number of tokens generated and, hence, the cost
    of the API call, but it also takes time and adds time to the API call, thereby
    increasing overall latency.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，我们应该谨慎使用此属性，并且仅在需要时使用。这不仅会增加生成的token数量，从而增加API调用的成本，而且还会消耗时间，增加API调用的耗时，从而增加整体延迟。
- en: Now that we understand the completion API for text generation, let’s see how
    we can use the chat completion API.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了用于文本生成的完成API，接下来让我们看看如何使用聊天完成API。
- en: 3.4 Chat completion API
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 聊天完成API
- en: The chat completion API has been designed to facilitate interactive and dynamic
    conversations. It is an evolution of the completion API, providing users with
    a more conversational and engaging experience. With this API, developers can create
    applications that have a dialogue with users, making it ideal for creating chatbots,
    writing assistants, and more.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天完成API被设计用于促进交互性和动态对话。它是完成API的演变，为用户提供更对话性和吸引人的体验。使用此API，开发者可以创建与用户进行对话的应用程序，非常适合创建聊天机器人、写作助手等。
- en: The key benefits that the chat completion API provides over the completion API
    are
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天完成API相较于完成API的关键优势包括
- en: '*Enhanced interactivity*—The chat completion API allows for a more dynamic
    and interactive conversation with the user, making the user experience more engaging
    and natural.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*增强的交互性*—聊天完成API允许与用户进行更动态和交互式的对话，使用户体验更加吸引人和自然。'
- en: '*Contextual understanding*—The API maintains the context of the conversation,
    ensuring that the responses are relevant and coherent.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文理解*—API维护对话的上下文，确保响应相关且连贯。'
- en: '*Multiturn conversation*—Unlike the completion API, which is more suited for
    single-turn tasks, the multiturn conversation API allows developers to simulate
    conversations with multiple exchanges.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多轮对话*—与更适合单轮任务的完成API不同，多轮对话API允许开发者模拟包含多个交换的对话。'
- en: '*Cost-effective*—Completion API uses GPT-3.5 Turbo or GPT-4 models, which perform
    at a similar capability as text-davinci-003 but at 10% of the price per token,
    making it a more economical choice for developers.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*经济高效*—完成API使用GPT-3.5 Turbo或GPT-4模型，其性能与text-davinci-003相似，但每个token的价格仅为10%，这使得它成为开发者更经济的选择。'
- en: At a high level, using the chat completion API is similar to the completion
    API. The API takes a series of messages as input, forming the basis of the interaction
    with the model. The ordering of the messages is important, as it outlines the
    turn-by-turn interaction.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，使用聊天完成API与完成API类似。API接收一系列消息作为输入，形成与模型交互的基础。消息的顺序很重要，因为它概述了逐个回合的交互。
- en: 'Each message has two properties: role and content. The role parameter has the
    following three options: `system`, `user`, or `assistant`. The content contains
    the message’s text from the role. Table 3.6 outlines the details of each role
    and its purpose.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 每条消息有两个属性：角色和内容。角色参数有以下三个选项：`system`、`user`或`assistant`。内容包含来自角色的消息文本。表3.6概述了每个角色的详细信息及其目的。
- en: Table 3.6 Chat completion API role description
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.6 聊天完成API角色描述
- en: '| Role parameter | Description |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 角色参数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `system`  | The `system` role is typically used to set the assistant’s behavior
    and provide the model with high-level instructions that guide the behavior throughout
    the conversation. This is where we can describe the assistant’s personality and
    tell it what it should and should not answer, as well as how to format responses.
    While there is no token limit, it is included with every API call and is part
    of the overall token limit.  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| `system`  | `system`角色通常用于设置助手的行性行为，并向模型提供在整个对话中引导行为的指导性指令。这包括描述助手的个性，告诉它应该和不应该回答什么，以及如何格式化回复。虽然没有token限制，但它包含在每个API调用中，并作为整体token限制的一部分。  |'
- en: '| `user`  | This represents the user’s input in the conversation; these messages
    contain the instructions or queries from the user that the assistant responds
    to.  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| `user`  | 这代表对话中用户的输入；这些消息包含用户对助手的指令或查询，助手将对此做出回应。  |'
- en: '| `assistant`  | This represents the assistant’s prior messages in the conversation.
    Think of this as the ongoing memory that helps the model and provides the conversation
    context as it proceeds, turn by turn.  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| `assistant`  | 这代表对话中助手的先前消息。将其视为帮助模型和提供对话上下文的持续记忆，逐个回合地提供。  |'
- en: Listing 3.19 shows the chat completion API. As we called out earlier, the order
    of the messages in the array matters, as it represents the flow of the conversation.
    Usually, the conversation starts with a `system` message that sets the assistant’s
    behavior, followed by alternating `user` and `assistant` messages as the conversation
    proceeds turn by turn. The assistant’s replies are generated based on the conversation
    history.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.19 展示了聊天完成 API。正如我们之前提到的，数组中消息的顺序很重要，因为它代表了对话的流程。通常，对话以设置助手行为的 `system`
    消息开始，然后随着对话的进行，交替出现 `user` 和 `assistant` 消息。助手的回复基于对话历史生成。
- en: Listing 3.19 Chat completion API
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.19 聊天完成 API
- en: '[PRE30]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '#1 Chat complete API call'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 聊天完成 API 调用'
- en: '#2 Different models needed (Turbo) compared to completion API'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 与完成 API 相比，需要不同的模型（Turbo）'
- en: '#3 List of messages that form the heart of the API'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 构成 API 核心的消息列表'
- en: '#4 These parameters are the same as for the completion API.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 这些参数与完成 API 相同。'
- en: We need to update the engine parameter to use one of the chat-compatible models.
    As shown earlier in this chapter, not all models support the chat style, and we
    need to pick the models with the `chat_completion` capability (GPT-3.5 Turbo,
    GPT-4, GPT-4 Turbo). All the other parameters are the same as the completion API
    that we covered earlier in this chapter, and we will not get into those details
    again.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要更新引擎参数以使用一个兼容聊天的模型。正如本章前面所展示的，并非所有模型都支持聊天风格，我们需要选择具有 `chat_completion` 功能的模型（GPT-3.5
    Turbo、GPT-4、GPT-4 Turbo）。所有其他参数与本章前面介绍的完成 API 相同，我们不会再次详细介绍这些细节。
- en: 'Note  The following parameters are unavailable with the new GPT-35 Turbo and
    GPT-4 models: `logprobs`, `best_of`, and `echo`. Trying to set any of these parameters
    will throw an exception.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：在新的 GPT-35 Turbo 和 GPT-4 模型中，以下参数不可用：`logprobs`、`best_of` 和 `echo`。尝试设置这些参数之一将引发异常。
- en: The output of the previous example is shown in the next listing. The user started
    with “Hello, World!”, and the system responded, asking how to help us with the
    assistant message. The question about dog details is the next dialogue turn.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个示例的输出显示在下一条列表中。用户首先说“Hello, World!”，然后系统通过助手消息回应，询问如何帮助。关于狗的细节是下一个对话回合。
- en: Listing 3.20 Chat completion API output
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.20 聊天完成 API 输出
- en: '[PRE31]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 3.4.1 System role
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 系统角色
- en: The system role (some also call it the system message) is included at the beginning
    of the message array. This message provides the initial instructions for the model,
    and we can provide various pieces of information in the system role, including
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 系统角色（有些人也称其为系统消息）包含在消息数组的开头。此消息为模型提供初始指令，我们可以在系统角色中提供各种信息，包括
- en: A brief description of the assistant
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对助手的简要描述
- en: Personality traits of the assistant
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 助手的个性特征
- en: Rules and instructions you want the assistant to follow
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您希望助手遵循的规则和指令
- en: Additional information needed for the model (e.g., relevant questions from an
    FAQ)
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型需要的附加信息（例如，来自常见问题解答的相关问题）
- en: We customize the system role and include basic instructions for the use case.
    From an API perspective, even though the system role is optional, it is highly
    recommended that you make this intentional to get the best results. For example,
    if we expand on the previous example of chatting for pets and pet salons, we can
    instruct the model to only reply in rhyme.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们自定义系统角色，并包括用例的基本指令。从 API 的角度来看，尽管系统角色是可选的，但强烈建议您明确指定，以获得最佳结果。例如，如果我们扩展之前的宠物和宠物美容院聊天示例，我们可以指示模型只以押韵的形式回复。
- en: Listing 3.21 Chat completion system message example
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.21 聊天完成系统消息示例
- en: '[PRE32]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '#1 Instructs to answer in rhyme'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 指示以押韵的形式回答'
- en: In the example, we can have a conversation as expected, which can vary topics
    in turns, but all the answers rhyme.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，我们可以进行预期的对话，话题可以轮流变化，但所有答案都是押韵的。
- en: When we want to give the model additional data as context for the conversation,
    this is called grounding the data. If there is a small amount of data, this can
    be part of the `system` role, as shown in the next listing. However, if there
    is a large amount of data, we should use embeddings and retrieve the most relevant
    information using a semantic search (e.g., Azure cognitive search).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要给模型提供额外的数据作为对话的上下文时，这被称为数据接地。如果数据量较少，这可以成为 `system` 角色的一部分，如下一列表所示。然而，如果数据量较大，我们应该使用嵌入并使用语义搜索（例如，Azure
    认知搜索）检索最相关的信息。
- en: Listing 3.22 Grounding system message example
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.22 基础系统消息示例
- en: '[PRE33]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 3.4.2 Finish reason
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 完成原因
- en: Every chat completion API response has a finish reason encoded in the `finish_
    reason` field. Tracking is important in this case, as it helps us understand why
    the API returned the response it did. This can be useful for debugging and improving
    the application. For example, if you receive an incomplete response due to the
    `length` finish reason, you may want to adjust the `max_tokens` parameter to generate
    more complete responses. The possible values for `finish_reason` are
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 每个聊天完成 API 响应都有一个编码在 `finish_reason` 字段中的完成原因。在这种情况下，跟踪很重要，因为它有助于我们了解 API 为什么返回了这样的响应。这有助于调试和改进应用程序。例如，如果你由于
    `length` 完成原因收到不完整的响应，你可能想要调整 `max_tokens` 参数以生成更完整的响应。`finish_reason` 的可能值包括
- en: '`stop`—The API finished generating and either returned a complete message or
    a message terminated by one of the stop sequences provided using the stop parameter.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop`—API 完成生成并返回了一条完整消息或由使用 stop 参数提供的停止序列终止的消息。'
- en: '`length—`The API stopped the model output due to the `max_tokens` parameter
    or token limit.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length—`API 由于 `max_tokens` 参数或标记限制停止了模型输出。'
- en: '`function_call—`The model decided to call a function.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function_call—`模型决定调用一个函数。'
- en: '`content_filter—`Some of the completion was filtered due to harmful content.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`content_filter—`由于有害内容，部分内容被过滤。'
- en: 3.4.3 Chat completion API for nonchat scenarios
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 非聊天场景的聊天完成 API
- en: OpenAI’s chat completion can be used for nonchat scenarios. The API is quite
    similar and designed to be a flexible tool that can be adapted to various use
    cases, not just conversations. In most cases, the recommended path uses the chat
    completion API as if it were the completion API. The main reason is that the newer
    models (Chat 3.5-Turbo and GPT-4) are much more efficient, cheaper, and powerful
    than the earlier models. The completion use cases we have seen, such as analyzing
    and generating text and answering questions from a knowledge base, would all still
    work with the chat completion API.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的聊天完成可用于非聊天场景。该 API 非常相似，设计成一个灵活的工具，可以适应各种用例，而不仅仅是对话。在大多数情况下，推荐的方法是使用聊天完成
    API，就像它是完成 API 一样。主要原因是因为较新的模型（Chat 3.5-Turbo 和 GPT-4）比早期模型更高效、更便宜、更强大。我们看到的完成用例，如分析和生成文本以及从知识库中回答问题，都可以使用聊天完成
    API。
- en: Implementing the chat completion API nonchat scenarios usually involves structuring
    the conversation with a series of messages and a system message to set the assistant’s
    behavior. For example, as shown in the following listing, the system message sets
    the role of the assistant, and the user message provides the task.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 实现聊天完成 API 的非聊天场景通常涉及使用一系列消息和一个系统消息来结构化对话，以设置助手的行为了。例如，如以下列表所示，系统消息设置助手的角色，用户消息提供任务。
- en: Listing 3.23 Chat completion as a completion API example
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.23 作为完成 API 示例的聊天完成
- en: '[PRE34]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can also use a series of user messages to provide more context or accomplish
    more complex tasks, as shown in the next listing. In this example, the first user
    message sets up the task, and the second user message provides more specific details.
    The assistant generates a response that attempts to complete the task in the user
    messages.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用一系列用户消息来提供更多上下文或完成更复杂的任务，如下一列表所示。在这个例子中，第一条用户消息设置任务，第二条用户消息提供更具体的细节。助手生成一个尝试在用户消息中完成任务的响应。
- en: Listing 3.24 Chat completion as a completion API example
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.24 作为完成 API 示例的聊天完成
- en: '[PRE35]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 3.4.4 Managing conversation
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 管理对话
- en: Our examples keep running, but the conversation will hit the model’s token limit
    as it continues. With each turn of the conversation (i.e., the question asked
    and the answer received), the list of messages grows. As a reminder, the token
    limit for GPT-35 Turbo is 4K tokens, and for GPT-4 and GPT-4 32K, it is 8K and
    32K, respectively; these include the total count from the message list sent and
    the model response. We get an exception if the total count exceeds the relevant
    model limit.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例一直在运行，但随着对话的继续，对话将触及模型的标记限制。随着每次对话的回合（即提出的问题和收到的答案），消息列表会增长。提醒一下，GPT-35
    Turbo 的标记限制为 4K 标记，而 GPT-4 和 GPT-4 32K 分别为 8K 和 32K；这些包括发送的消息列表的总数和模型响应。如果总数超过相关模型限制，我们会得到一个异常。
- en: No out-of-the-box option can track this token count for us and ensure it falls
    within the token limit. As part of the enterprise app design, we need to track
    the token count and only send a prompt that falls within the limit.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何现成的选项可以为我们跟踪这个令牌计数并确保它不超过令牌限制。作为企业应用程序设计的一部分，我们需要跟踪令牌计数，并且只发送不超过限制的提示。
- en: 'Many enterprises are in the process of implementing an enterprise version of
    ChatGPT using the chat API. Here are some of the best practices that can help
    enterprises manage these conversations. Remember, the best way to get your desired
    output involves iterative testing and refining your instructions:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 许多企业正在使用聊天 API 实施ChatGPT的企业版本。以下是一些可以帮助企业管理这些对话的最佳实践。记住，获取所需输出的最佳方式涉及迭代测试和改进你的指令：
- en: '*Setting the behavior with system message*—You should use the system message
    at the start of the conversation to guide the model’s behavior and for enterprises
    to tune to reflect their brand or IP.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用系统消息设置行为*—你应该在对话开始时使用系统消息来引导模型的行为，以及企业调整以反映其品牌或知识产权。'
- en: '*Providing explicit instructions*—If the model is not generating your desired
    output, make your instructions more explicit. Think about it at the same level
    as if you were telling a toddler what not to do.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提供明确的指令*—如果模型没有生成你期望的输出，请使你的指令更加明确。想想看，就像你告诉一个幼儿不要做什么一样。'
- en: '*Breaking down complex tasks*—If you have a complex task, break it down into
    several simpler tasks, and send them as separate user messages. You often need
    to show, not explain it. This is called Chain of Thought (CoT), and it will be
    covered in more detail in chapter 6\.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分解复杂任务*—如果你有一个复杂任务，将其分解为几个更简单的任务，并将它们作为单独的用户消息发送。你通常需要展示而不是解释。这被称为思维链（CoT），将在第6章中详细介绍。'
- en: '*Experimentation*—Feel free to experiment with the parameters to get the desired
    output. A higher temperature value (e.g., 0.8) makes the generation more random,
    while a lower value (e.g., 0.2) makes it more deterministic. You can also use
    the maximum token value to limit response length.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实验*—自由地尝试参数以获取所需的输出。更高的温度值（例如，0.8）会使生成更加随机，而较低的值（例如，0.2）会使生成更加确定。你还可以使用最大令牌值来限制响应长度。'
- en: '*Managing tokens*—Be aware of the total number of tokens in a conversation,
    as input and output tokens count toward the total. You must truncate, omit, or
    shorten your text if a conversation has too many tokens to fit within the model''s
    maximum limit.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*管理令牌*—注意对话中的总令牌数，因为输入和输出令牌都计入总数。如果对话中的令牌太多，无法适应模型的最高限制，你必须截断、省略或缩短你的文本。'
- en: '*Handling sensitive content*—If you’re dealing with potentially unsafe content,
    you should look at Azure OpenAI’s Responsible AI guidelines ([https://mng.bz/pxVK](https://mng.bz/pxVK)).
    However, if you are using OpenAI’s API, then OpenAI’s moderation guide is helpful
    ([https://mng.bz/OmEw](https://mng.bz/OmEw)) for adding a moderation layer to
    the outputs of the chat API.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*处理敏感内容*—如果你正在处理可能不安全的内容，你应该查看 Azure OpenAI 的负责任 AI 指南 ([https://mng.bz/pxVK](https://mng.bz/pxVK))。然而，如果你正在使用
    OpenAI 的 API，那么 OpenAI 的审核指南对于为聊天 API 的输出添加审核层是有帮助的 ([https://mng.bz/OmEw](https://mng.bz/OmEw))。'
- en: Tracking tokens
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跟踪令牌
- en: 'As outlined earlier, keeping track of tokens when using the conversational
    API is key. Not only will the experience suffer if we go over the total token
    size, but the total number of tokens in an API also has a direct effect on latency
    and on how long the call takes. Finally, the more tokens we use, the more we pay.
    Here are some ways you can manage tokens:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在使用对话 API 时跟踪令牌至关重要。如果我们超过总令牌大小，体验将受到影响，API 中的总令牌数也会直接影响延迟和调用所需的时间。最后，我们使用的令牌越多，我们支付的越多。以下是一些你可以管理令牌的方法：
- en: '*Count tokens**.* Use the `tiktoken` library, which allows us to count how
    many tokens are in a string without making an API call.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计数令牌*。使用 `tiktoken` 库，它允许我们在不进行 API 调用的前提下计算字符串中的令牌数量。'
- en: '*Limit response length**.* When making an API call, use the `max_tokens` property
    to limit the length of the model’s responses.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*限制响应长度*。在发起 API 调用时，使用 `max_tokens` 属性来限制模型的响应长度。'
- en: '*Truncate long conversations**.* If a conversation has too many tokens to fit
    within the model’s maximum limit, we must truncate, omit, or shorten our text.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*截断长对话*。如果对话中的令牌太多，无法适应模型的最高限制，我们必须截断、省略或缩短我们的文本。'
- en: '*Limit the number of turns**.* Limiting the number of turns in the conversation
    is a good way to truncate or shorten the text. This also helps steer the model
    better when the conversation gets longer and tends to start hallucinating.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*限制轮数**。* 限制对话中的轮数是截断或缩短文本的好方法。这也帮助在对话变长并倾向于开始产生幻觉时更好地引导模型。'
- en: '*Check the* `usage`*field in the API response**.* After making an API call,
    we can check the usage field in the API response to see the total number of tokens
    used. This is ongoing and includes both input and output tokens. It is a good
    way to keep track of tokens and show them to the user via some UX.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检查API响应中的* `usage`*字段**。* 在进行API调用后，我们可以检查API响应中的使用字段，以查看使用的总标记数。这是一个持续的过程，包括输入和输出标记。这是一种跟踪标记并通过某些用户体验向用户展示的好方法。'
- en: '*Reduce temperature**.* Reducing the temperature parameter can make the model''s
    outputs more focused and concise, which can help reduce the number of tokens used
    in the response.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降低温度**。* 降低温度参数可以使模型的输出更加专注和简洁，这有助于减少响应中使用的标记数。'
- en: Say we want to build a chat application for our pet salon and allow customers
    to ask us questions about pets, grooming, and their needs. We can build a console
    chat application, as shown in listing 3.25\. It also shows us a possible way to
    track and manage tokens. In this example, we have a function `num_tokens_from_messages`
    which, as the name suggests, is used to calculate the number of tokens in a conversation.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要为我们的宠物沙龙构建一个聊天应用程序，并允许客户询问有关宠物、美容和需求的问题。我们可以构建一个控制台聊天应用程序，如列表3.25所示。它还展示了跟踪和管理标记的可能方法。在这个例子中，我们有一个名为`num_tokens_from_messages`的函数，正如其名称所暗示的，用于计算对话中的标记数。
- en: As the conversation grows turn by turn, we calculate the number of tokens used,
    and once it reaches the model limit, the old messages are removed from the conversation.
    Note that we start at index 1\. This ensures we always preserve the system message
    at index 0 and only remove user/assistant messages.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对话逐轮进行，我们计算使用的标记数，一旦达到模型限制，旧消息将从对话中移除。请注意，我们从索引1开始。这确保我们始终保留索引0的系统消息，并且只移除用户/助手消息。
- en: Listing 3.25 `ConsoleChatApp:` Token management
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.25 `ConsoleChatApp:` 标记管理
- en: '[PRE36]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '#1 Sets up the OpenAI environment and configuration details'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置OpenAI环境和配置细节'
- en: '#2 Sets up the system message for the chat'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 设置聊天系统消息'
- en: '#3 Function to count the total tokens from all the messages in the conversation'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 函数用于计算对话中所有消息的总标记数'
- en: '#4 Uses the tiktoken library to count tokens'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用tiktoken库来计算标记'
- en: '#5 Loops through the messages'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 遍历消息'
- en: '#6 Captures the user input'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 捕获用户输入'
- en: '#7 When the total tokens exceed the token limit, we remove the second token.
    The first token is the system token, which we always want.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 当总标记数超过标记限制时，我们移除第二个标记。第一个标记是系统标记，我们总是想要的。'
- en: '#8 Chat completion API call'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 聊天完成API调用'
- en: Chat completion vs. completion API
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 聊天完成与完成API
- en: Both chat completion and completion APIs are designed to generate human-like
    text and are used in different contexts. The completion API is designed for single-turn
    tasks, providing completion to a prompt provided by the user. It is most suited
    for tasks where only a single response is required.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天完成和完成API都旨在生成类似人类的文本，并用于不同的上下文。完成API旨在处理单轮任务，为用户提供提示的完成。它最适合只需要单个响应的任务。
- en: In contrast, the chat completion API is designed for multiturn conversations,
    maintaining the context of the conversation over multiple exchanges. This makes
    it more suitable for interactive applications such as chatbots. The chat completion
    API is a new dedicated API for interacting with the GPT-35-Turbo and GPT-4 models
    and is the preferred method. The chat completion API is geared more toward chatbots,
    and using the different roles (`system`, `user`, and `assistant`), we can get
    the memory of previous messages and organize few-shot examples.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，聊天完成API旨在处理多轮对话，在多次交流中保持对话的上下文。这使得它更适合交互式应用程序，如聊天机器人。聊天完成API是用于与GPT-35-Turbo和GPT-4模型交互的新专用API，是首选方法。聊天完成API更侧重于聊天机器人，通过使用不同的角色（`system`、`user`和`assistant`），我们可以获取先前消息的内存并组织少量示例。
- en: 3.4.5 Best practices for managing tokens
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.5 管理标记的最佳实践
- en: 'For LLMs, tokens are the new currency. As most enterprises go beyond kicking
    tires to business-critical use cases, managing tokens would become a priority
    for computations, cost, and overall experience. From an enterprise application
    perspective, here are some of the considerations for managing tokens:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLM，令牌是新的货币。随着大多数企业从试水转向业务关键用例，管理令牌将成为计算、成本和整体体验的一个重点。从企业应用的角度来看，以下是管理令牌的一些考虑因素：
- en: '*Concise prompts*—Where possible, using concise prompts and limiting the maximum
    number of tokens will reduce the token’s usage, making it more cost-effective.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*简洁提示*—尽可能使用简洁的提示并限制最大令牌数量，这将减少令牌的使用，使其更具成本效益。'
- en: '*Stop sequences*—Use stop sequences to stop the generations to avoid generating
    unnecessary tokens.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*停止序列*—使用停止序列来停止生成，以避免生成不必要的令牌。'
- en: '*Counting tokens*—We can count tokens using the `tiktoken` library as outlined
    earlier and avoid making the API calls do the same.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计数令牌*—我们可以使用前面概述的`tiktoken`库来计数令牌，以避免API调用重复进行相同的操作。'
- en: '*Smaller models*—Generally speaking, in computing, bigger and newer hardware
    and software are considered faster, cheaper, and better; however, this isn’t necessarily
    the case for LLMs. Where possible, consider using smaller models such as GPT-3.5
    Turbo first, and when they might not be a good fit, consider going to the next
    one. Smaller models are less compute intensive and, hence, are more economical.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*小型模型*—一般来说，在计算中，更大、更新的硬件和软件被认为是更快、更便宜、更好的；然而，对于LLM来说，情况并不一定如此。尽可能首先考虑使用较小的模型，如GPT-3.5
    Turbo，如果它们不适合，再考虑下一个。小型模型计算量较小，因此更具经济性。'
- en: '*Use caching*—For prompts that are either quite static or frequently repeated,
    implementing a caching strategy would help save tokens and avoid making API calls
    repeatedly. For more complex scenarios, look to cache the embeddings using a vector
    search and store, such as Azure Cognitive Search, Pinecone, etc. The last chapter
    covered an introduction to embeddings, and we will get more details on embeddings
    and searching later in chapters 7 and 8 when we cover RAG and chatting with your
    data.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用缓存*—对于既静态又频繁重复的提示，实施缓存策略将有助于节省令牌并避免重复调用API。在更复杂的场景中，可以考虑使用向量搜索和存储来缓存嵌入，例如Azure认知搜索、Pinecone等。上一章介绍了嵌入的概念，我们将在第7章和第8章中详细介绍嵌入和搜索，届时我们将讨论RAG和与数据聊天。'
- en: 3.4.6 Additional LLM providers
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.6 其他LLM提供商
- en: Additional vendors also now have LLMs to use for enterprises. These are either
    available via APIs or, in some cases, as model weights that enterprises can self-host.
    Table 3.7 outlines some of the more famous ones available at the time of publication.
    Please note that some restrictions are in place from a commercial-licensing perspective.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 其他供应商现在也提供了企业级LLM。这些LLM可以通过API获取，或者在某些情况下，作为企业可以自行托管模型权重。表3.7概述了在出版时的一些著名LLM。请注意，从商业许可的角度来看，存在一些限制。
- en: Table 3.7 Other LLM providers
  id: totrans-364
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.7 其他LLM提供商
- en: '| Models | Descriptions |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 描述 |'
- en: '| --- | --- |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Llama 2  | Meta released Llama 2, an open source LLM, which comes in three
    sizes (7 billion, 13 billion, and 70 billion parameters) and is free for research
    and commercial purposes. Companies can access this through cloud options such
    as Azure AI’s model catalog, Hugging Face, or AWS. Enterprises that want to host
    it using their own compute and GPUs can request access from Meta via [https://ai.meta.com/llama/](https://ai.meta.com/llama/).  |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2  | Meta发布了Llama 2，这是一个开源的LLM，有三种大小（70亿、130亿和700亿参数），用于研究和商业目的免费。公司可以通过云选项访问它，例如Azure
    AI的模型目录、Hugging Face或AWS。希望使用自己的计算和GPU托管它的企业可以通过[https://ai.meta.com/llama/](https://ai.meta.com/llama/)从Meta请求访问。  |'
- en: '| PaLM  | PaLM is a 13 billion-parameter model from Google that is part of
    their generative AI for developer products. The model can perform text summarization,
    dialogue generation, and natural language inference tasks. At the time of publication,
    there was a waitlist for an API key; details are available at [https://developers.generativeai.google/](https://developers.generativeai.google/).  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| PaLM  | PaLM是谷歌的一个130亿参数模型，是它们为开发者产品提供的生成式AI的一部分。该模型可以执行文本摘要、对话生成和自然语言推理任务。在出版时，有一个API密钥的等待列表；详细信息可在[https://developers.generativeai.google/](https://developers.generativeai.google/)找到。  |'
- en: '| BLOOM  | Bloom is a 223-billion parameter, open source multilingual model
    that can understand and generate text in over 100 languages by collaborating with
    over 1,000 researchers across more than 250 institutions. It is available via
    Hugging Face for deployment. More details are available at [https://huggingface.co/bigscience/bloom](https://huggingface.co/bigscience/bloom).  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | Bloom是一个223亿参数的开源多语言模型，通过与超过250个机构中的1000多名研究人员合作，可以在100多种语言中理解和生成文本。它可以通过Hugging
    Face进行部署。更多详情请见[https://huggingface.co/bigscience/bloom](https://huggingface.co/bigscience/bloom)。'
- en: '| Claude  | Claude is a 12-billion parameter developed by Anthropic. It is
    accessible through a playground interface and API in its developer console for
    development and evaluation purposes only. At publication, for production use,
    enterprises must contact Claude for commercial discussions. More details can be
    found at [https://mng.bz/YVqz](https://mng.bz/YVqz).  |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| Claude | Claude是由Anthropic开发的12亿参数模型。它可以通过开发者的控制台中的游乐场界面和API进行开发和评估。在发布时，为了生产使用，企业必须联系Claude进行商业讨论。更多详情请见[https://mng.bz/YVqz](https://mng.bz/YVqz)。'
- en: '| Gemini  | Google recently released a new LLM called Gemini, a successor to
    PaLM 2 and optimized for different sizes: ultra, pro, and nano. It is designed
    to be more powerful than its predecessor and can be used to generate new content.
    Google claims it to be their most capable AI model yet. More details can be found
    at [https://mng.bz/GNxD](https://mng.bz/GNxD).  |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| Gemini | 谷歌最近发布了一个名为Gemini的新LLM，它是PaLM 2的后继产品，针对不同大小进行了优化：超大型、专业型和纳米型。它被设计得比其前身更强大，可以用来生成新内容。谷歌声称这是他们迄今为止最强大的AI模型。更多详情请见[https://mng.bz/GNxD](https://mng.bz/GNxD)。'
- en: Interestingly, all these vendors follow a similar approach to the concepts and
    APIs established by OpenAI. For example, as outlined by their documents, the PaLM
    model from Google’s completion API equivalent is presented in the next listing.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，所有这些供应商都遵循与OpenAI建立的概念和API相似的方法。例如，根据他们的文档，谷歌完成API等价的PaLM模型在下一列表中展示。
- en: Listing 3.26 PaLM-generated text API signature
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.26 PaLM生成的文本API签名
- en: '[PRE37]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: While these options exist, and some are from reputable and leading technology
    companies, for most enterprises, Azure OpenAI and OpenAI are the most mature,
    with the most enterprise controls and support needed. The next chapter will deal
    with images, and we will learn how to move from text to images and generate in
    that modality.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些选项存在，并且其中一些来自信誉良好的领先科技公司，但对于大多数企业来说，Azure OpenAI和OpenAI是最成熟的，拥有最多的企业控制和所需的支持。下一章将涉及图像，我们将学习如何从文本过渡到图像，并在此模式中生成内容。
- en: Summary
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: GenAI models are classified into various categories, depending on the type.
    Each model has additional capabilities and characteristics. Choosing the right
    model for the use case at hand is important. And unlike computer science, in our
    case, the biggest model isn’t necessarily better.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GenAI模型根据类型被分为各种类别，每个模型都有额外的功能和特性。为当前的使用案例选择正确的模型非常重要。并且与计算机科学不同，在我们的情况下，最大的模型并不一定是更好的。
- en: The completion API is a sophisticated tool that generates text, which can be
    used to complete prompts provided by the user and forms the backbone of the text
    generation paradigm.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成API是一个复杂的工具，可以生成文本，可用于完成用户提供的提示，并构成了文本生成范式的基础。
- en: The completion API is relatively easy to use with only a few key parameters,
    such as the prompt, number of tokens to generate, temperature parameter that helps
    steer the model, and number of completions to generate.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成API相对容易使用，只需几个关键参数，例如提示、要生成的标记数、帮助引导模型的温度参数以及要生成的补全数。
- en: The API exposes many advanced options for steering models and controlling randomness
    and generated text, such as `logit_bias`, presence penalty, and frequency penalty.
    All these work in tandem and help generate better output.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API提供了许多高级选项来引导模型和控制随机性和生成的文本，例如`logit_bias`、存在惩罚和频率惩罚。所有这些协同工作，有助于生成更好的输出。
- en: When using Azure OpenAI, the content safety filter can help filter specific
    categories to identify and act on potentially harmful content as part of both
    the input prompts and generated completions.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用Azure OpenAI时，内容安全过滤器可以帮助过滤特定类别，以识别和应对潜在有害内容，这既包括输入提示也包括生成的补全。
- en: The chat completion API builds on the completion API, going from one set of
    instructions and APIs to a dialogue with the user in a turn-by-turn interaction.
    The chat completion consists of multiple systems, user, and assistance roles.
    The conversation starts with a `system` message that sets the assistant's behavior,
    followed by alternating `user` and `assistant` messages as the conversation proceeds
    turn by turn.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天完成API建立在完成API的基础上，从一组指令和API转变为与用户在逐个回合的交互中进行对话。聊天完成由多个系统、用户和辅助角色组成。对话从设置辅助者行为的`system`消息开始，随着对话的逐个回合进行，交替出现`user`和`assistant`消息。
- en: The system role is included at the beginning of the message array. It provides
    the initial instructions for the model, including personality traits, instructions
    and rules for the assistant to follow, and additional information we want to provide
    as context for the model; this additional information is called grounding the
    data.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统角色包含在消息数组的开头。它为模型提供初始指令，包括个性特征、辅助者应遵循的指令和规则，以及我们希望作为模型背景信息提供的额外信息；这些额外信息被称为数据接地。
- en: Each completion and chat completion API response has a finish reason, which
    helps us understand why the API returned the response it did. This can be useful
    for debugging and improving the application.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个完成和聊天完成API的响应都有一个完成原因，这有助于我们理解API为什么返回了这样的响应。这可以用于调试和改进应用程序。
- en: The language learning models all have a finite context window and are quite
    expensive. Managing tokens becomes important for us to be able to run things at
    a reasonable cost and within the API allowance. This also helps us manage tokens
    in conversations for improved user experience and cost-effectiveness.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言学习模型都有一个有限的范围窗口，并且相当昂贵。管理令牌对我们来说变得很重要，以便我们能够在合理的成本和API允许的范围内运行。这也帮助我们管理对话中的令牌，以改善用户体验和成本效益。
- en: In addition to Azure OpenAI and OpenAI, there are other LLM providers, such
    as Meta’s Llama 2, Google’s Gemini and PaLM, Bloom by BigScience, and Anthropic’s
    Claude. Their offerings are similar and follow the completions and chat completions
    paradigm, including similar APIs.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了Azure OpenAI和OpenAI之外，还有其他LLM提供商，例如Meta的Llama 2、Google的Gemini和PaLM、BigScience的Bloom以及Anthropic的Claude。他们的服务类似，遵循完成和聊天完成的范式，包括类似的API。
