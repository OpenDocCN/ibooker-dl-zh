- en: 11 International ACH transactions and OFAC scanning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 国际 ACH 交易和 OFAC 扫描
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: IAT batches
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IAT 批次
- en: Enhancing the project to process IAT batches
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强项目以处理 IAT 批次
- en: The OFAC list
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OFAC 列表
- en: Scanning ACH files to stay in compliance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扫描 ACH 文件以保持合规
- en: This chapter covers the final sprint of our program iteration. Of course, the
    business unit will get together for another PI planning session, and the process
    will start all over again. In this sprint, we are tasked with enhancing the project
    to expand beyond domestic ACH transactions and deal with batches containing international
    ACH transactions (IAT), which enable sending money electronically between accounts
    in different countries. Along with IAT processing, there is the need to ensure
    the financial institution is not sending transactions to individuals or countries
    that are currently under restrictions or sanctioned by the US government. Specifically,
    the Office of Foreign Asset Control (OFAC) provides a list of “Specially Designated
    Nationals,” also known as the SDN list, which is a register of individuals and
    companies whose assets are blocked, and dealing with them is prohibited.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了我们项目迭代的最后冲刺。当然，业务单元将再次聚集进行另一个 PI 规划会议，整个过程将重新开始。在这个冲刺中，我们负责增强项目，使其超越国内
    ACH 交易，并处理包含国际 ACH 交易（IAT）的批次，这允许在不同国家的账户之间进行电子转账。除了 IAT 处理外，还需要确保金融机构不会向目前受到美国政府限制或制裁的个人或国家发送交易。具体来说，外国资产控制办公室（OFAC）提供了一份“特别指定国民”名单，也称为
    SDN 列表，这是一个资产被冻结且与其交易被禁止的个人和公司注册。
- en: 11.1 Sprint planning
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 冲刺规划
- en: In this final sprint, we have a big request by the line of business, which is
    to enable IAT ACH transactions. In the original scope of our project, we were
    told that the financial institution was not going to take on the additional risk
    of processing international transactions. Therefore, the database was not designed
    to support these types of transactions. However, recently, the financial institution
    has been attempting to pursue larger business customers and has had trouble attracting
    them because such customers require the ability to receive and transfer funds
    internationally.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次最后的冲刺中，业务线提出了一个大的请求，即启用 IAT ACH 交易。在我们的项目原始范围内，我们被告知金融机构不会承担处理国际交易额外风险。因此，数据库没有设计来支持这些类型的交易。然而，最近，金融机构一直在试图吸引更大的商业客户，并且因为这类客户需要能够接收和转移国际资金的能力，所以吸引他们遇到了困难。
- en: As it often happens when meeting customer demands, we now have to update the
    dashboard to be able to support these types of transactions, which means including
    additional tables to the database, parsing of the file, and scanning of the customers
    involved to stay in compliance. Figure 11.1 provides a timeline for the proposed
    tasks associated with this sprint. Having a timeline that we can provide to other
    stakeholders can be helpful when addressing questions about the tasks we will
    be working on during the sprint and our schedule.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如通常在满足客户需求时发生的那样，我们现在必须更新仪表板以支持这些类型的交易，这意味着向数据库中添加额外的表格，解析文件，以及扫描涉及的客户以保持合规。图
    11.1 提供了与这次冲刺相关的拟议任务的进度时间线。能够向其他利益相关者提供时间线，在回答关于我们在冲刺期间将工作的任务和我们的时间表的问题时可能会有所帮助。
- en: '![A diagram of a software project  Description automatically generated](../Images/CH11_F01_Kardell.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![软件项目的图示  自动生成的描述](../Images/CH11_F01_Kardell.png)'
- en: Figure 11.1  Timeline for IAT ACH transaction processing
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.1  IAT ACH 交易处理时间线
- en: With the sprint planning in place, we can move forward with work to support
    international ACH transactions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在冲刺规划到位后，我们可以继续进行支持国际 ACH 交易的工作。
- en: 11.2 International ACH transactions
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 国际 ACH 交易
- en: Until this point, we have dealt with Prearranged Payment and Deposit Entry (PPD)
    batches. As you know, PPD batches are commonly used for direct deposits of payroll
    and pension payments. We may also remember that ACH started as a way of processing
    payments for domestic transactions in the early 1970s. With the expansion of the
    ACH system, support for IAT began in 2009, and the capabilities for ACH were expanded
    beyond domestic transactions. So, what is so different about IAT batches that
    we need to dedicate an entire sprint to add support for processing them? First
    and foremost, the IAT batches must conform to the 94-character limit imposed on
    all ACH records. As we will see in this section, this affects the way data must
    be transmitted.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经处理了预先安排的支付和存款入口（PPD）批次。正如你所知，PPD 批次通常用于工资和养老金的直接存款。我们可能也记得，ACH 在 20
    世纪 70 年代初开始作为一种处理国内交易支付的方式。随着 ACH 系统的扩展，对 IAT 的支持始于 2009 年，ACH 的能力也扩展到了国内交易之外。那么，IAT
    批次有什么不同之处，以至于我们需要专门一个冲刺来添加处理它们的支持呢？首先，IAT 批次必须符合对所有 ACH 记录施加的 94 个字符限制。正如我们将在本节中看到的，这影响了数据必须以何种方式传输。
- en: '11.2.1 IAT batches: An overview'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 IAT 批次：概述
- en: Before we dive into supporting IAT batches, we first need to understand how
    a batch may be laid out. Figure 11.2 shows a sample batch we will work with to
    better understand the data involved. This batch represents an individual named
    Elena Santiago from Bilbao, Spain, sending a gift of $1.00 to a friend named David
    Wiliams from her account at Iberia Global Bank to his checking account at Futuristic
    FinTech.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入支持 IAT 批次之前，我们首先需要了解一个批次可能如何布局。图 11.2 展示了一个我们将要处理的样本批次，以更好地理解涉及的数据。这个批次代表了一个名叫
    Elena Santiago 的西班牙毕尔巴鄂人，从她在伊比利亚全球银行的账户向她的朋友 David Wiliams 发送了 1.00 美元的礼物，他的账户在
    Futuristic FinTech 的支票账户中。
- en: '![A white background with black numbers and letters  Description automatically
    generated](../Images/CH11_F02_Kardell.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![白色背景，黑色数字和字母  自动生成的描述](../Images/CH11_F02_Kardell.png)'
- en: Figure 11.2  Sample IAT batch
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.2  样本 IAT 批次
- en: That is a fair amount of information packed into the batch, so let’s take a
    moment to unpack it (figure 11.3), and then we can jump into the code.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 批次中包含了相当多的信息，所以让我们花点时间来拆解它（图 11.3），然后我们可以进入代码部分。
- en: As shown in figure 11.4, the batch is marked as an IAT, with a Company Entry
    Description of GIFT. The FF3 is the Foreign Exchange Indicator, while the FF means
    fixed-to-fixed. The originated amount is the same as the one being received, and
    the 3 indicates that the Foreign Exchange Reference is spaces. The USDUSD represents
    the originating currency code (USD) and the destination currency code (USD), respectively.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 11.4 所示，批次被标记为 IAT，公司入口描述为 GIFT。FF3 是外汇指示符，而 FF 表示固定到固定。原始金额与接收的金额相同，而 3
    表示外汇参考是空格。USDUSD 分别代表原始货币代码（USD）和目标货币代码（USD）。
- en: The entry record contains the number of addenda records that were passed (figure
    11.5). We expect to see seven (0007) addenda records passed, which also happen
    to be a type 7\. Another important field is obviously the amount—the $1.00 that
    was sent in this case.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 入口记录包含传递的附加记录数量（图 11.5）。我们预计会看到七个（0007）附加记录被传递，这恰好也是类型 7。另一个显然重要的字段是金额——在这种情况下发送的
    1.00 美元。
- en: '![A screenshot of a computer  Description automatically generated](../Images/CH11_F03_Kardell.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](../Images/CH11_F03_Kardell.png)'
- en: Figure 11.3  Routing number and account
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.3  路由号和账户
- en: '![A computer screen with text  Description automatically generated](../Images/CH11_F04_Kardell.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![带有文本的计算机屏幕  自动生成的描述](../Images/CH11_F04_Kardell.png)'
- en: Figure 11.4  IAT batch header
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.4  IAT 批次标题
- en: '![A screen shot of a computer  Description automatically generated](../Images/CH11_F05_Kardell.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](../Images/CH11_F05_Kardell.png)'
- en: Figure 11.5  IAT entry record
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.5  IAT 入口记录
- en: There are between 7 and 12 addenda records per record when dealing with IAT
    entries. This is different from what we have dealt with when parsing PPD batches,
    as we had an indicator for an optional addenda record. While that addenda indicator
    is still present on an IAT entry, it will always be set to 1.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理 IAT 条目时，每条记录之间有 7 到 12 个附加记录。这与我们在解析 PPD 批次时遇到的不同，因为我们有一个可选附加记录的指示器。虽然这个附加记录指示器在
    IAT 条目中仍然存在，但它总是设置为 1。
- en: 'To keep things relatively simple, we will only consider the seven mandatory
    addenda records. They all start with a 7 to indicate an addenda record, followed
    by an addenda type code in the next two positions, producing the mandator records
    range from 710 to 716:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持相对简单，我们只会考虑七个强制附加记录。它们都以7开头，表示附加记录，接下来是两个位置的附加记录类型代码，强制记录的范围从710到716：
- en: '*71**0*—Foreign payment amount and receiver’s name'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*71**0*—外国支付金额和接收人姓名'
- en: '*711*—Originator’s name and street'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*711*—发起人姓名和街道'
- en: '*712*—Originator’s city, state, country, and postal code'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*712*—发起人的城市、州、国家和邮政编码'
- en: '*71**3*—ODFI name, ID, and branch'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*71**3*—ODFI名称、ID和分行'
- en: '*71**4*—RDFI name, ID, and branch'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*71**4*—RDFI名称、ID和分行'
- en: '*71**5*—Receiver’s ID number and street'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*71**5*—接收人的ID号码和街道'
- en: '*71**6*—Receiver’s city, state, country and postal code'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*71**6*—接收人的城市、州、国家和邮政编码'
- en: Equipped with new knowledge, let’s create some IAT batches.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有了新的知识，让我们创建一些IAT批次。
- en: 11.3 Creating IAT batches
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 创建IAT批次
- en: Before we can start making changes and parsing a file, we must have a file.
    We have a sample provided in figure 11.2, and that, combined with the existing
    code we have to produce ACH files, should be more than enough to build from. If
    you reference the [https://achdevguide.nacha.org/ach-file-details](https://achdevguide.nacha.org/ach-file-details)
    and [https://mng.bz/eyOv](https://mng.bz/eyOv) as guides, you can see that we
    may be dealing with a few different layouts.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始修改和解析文件之前，我们必须有一个文件。我们在图11.2中提供了一个样本，结合我们已有的用于生成ACH文件的代码，应该足够构建了。如果你以[https://achdevguide.nacha.org/ach-file-details](https://achdevguide.nacha.org/ach-file-details)和[https://mng.bz/eyOv](https://mng.bz/eyOv)作为指南，你会看到我们可能要处理几种不同的布局。
- en: When we dig into the details, we’ll find we are working with new records and
    formats. This means that when creating files specifically, we need to consider
    whether we have an IAT batch and create a file accordingly. We already have the
    code within our ach_file_creation.feature and test_create_ach_files.py to deal
    with creating batches with a specified Standard Entry Class (SEC) code such as
    the line `And` `I` `want` `to` `have` `1` `batch` `with` `ACH` `credits` `and`
    `debits` `and` `a` `standard` `entry` `class` `code` `of` `"PPD"`, which drives
    the creating of the batch. We must update our method `create_batch_header` to
    take the SEC code into consideration. So, as shown in the following listing, we
    start by defaulting (hardcoding) on some of the values. As usual, we go back when
    we need them to be dynamic and handle them at that point.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入研究细节时，我们会发现我们正在处理新的记录和格式。这意味着在创建特定文件时，我们需要考虑我们是否有IAT批次，并相应地创建文件。我们已经在ach_file_creation.feature和test_create_ach_files.py中有了处理具有指定标准录入类（SEC）代码（如`And`
    `I` `want` `to` `have` `1` `batch` `with` `ACH` `credits` `and` `debits` `and`
    `a` `standard` `entry` `class` `code` `of` `"PPD"`）的批次的代码，这驱动了批次的创建。我们必须更新我们的`create_batch_header`方法，以考虑SEC代码。所以，如以下列表所示，我们首先默认（硬编码）一些值。像往常一样，当我们需要它们是动态的并在此点处理它们时，我们会回过头来。
- en: Listing 11.1  Updated `create_batch_header`
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.1  更新的`create_batch_header`
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 IAT indicator'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 IAT指示符'
- en: '#2 Foreign exchange indicator'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 外汇指示符'
- en: '#3 Foreign exchange reference indicator'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 外汇参考指示符'
- en: '#4 Foreign exchange reference'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 外汇参考'
- en: '#5 ISO destination country code'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 ISO目的地国家代码'
- en: '#6 Originator identification'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 发起人识别'
- en: '#7 Standard entry class code'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 标准录入类代码'
- en: '#8 Company entry description'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 公司录入描述'
- en: '#9 ISO originating currency code'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 ISO发起货币代码'
- en: '#10 ISO destination currency code'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 ISO目的地货币代码'
- en: '#11 Effective entry date'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 有效录入日期'
- en: '#12 Settlement date'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '#12 结算日期'
- en: '#13 Originator status code'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#13 发起人状态代码'
- en: '#14 Originating DFI identification'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#14 发起DFI识别'
- en: '#15 Previous creation logic of the batch header'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '#15 批次头部的先前创建逻辑'
- en: Since the SEC code is stored as part of our unit test information, we can reference
    it wherever necessary, and we will need it again when writing out the entries
    for an IAT batch. As described in the previous section, every entry record for
    an IAT record contains at least seven addenda records. We follow the same pattern,
    using the SEC code to create a new method that will take care of creating the
    needed entry and addenda records keeping things hardcoded for the most part and
    allowing for dynamic values. The code to create files is now over 500 lines long,
    and we should start to consider where refactoring is possible to clean it up.
    The logic to create specific types of files is a good candidate for refactoring
    since those details do not necessarily need to be part of this process.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SEC代码存储在我们的单元测试信息中，我们可以在必要时引用它，并且在编写IAT批次的条目时我们还需要它。如前所述，每个IAT记录的条目记录至少包含七个附加记录。我们遵循相同的模式，使用SEC代码创建一个新的方法，该方法将负责创建所需的条目和附加记录，大部分保持硬编码，并允许动态值。创建文件的代码现在已超过500行，我们应该开始考虑在哪里进行重构以清理代码。创建特定类型文件的逻辑是重构的良好候选者，因为这些细节不一定需要成为此过程的一部分。
- en: For now, however, the code should suffice for our initial file creation. The
    feature in the following listing should look familiar to our other file-creation
    steps. This is a good thing as it means our grammar is generic enough to handle
    some variations. Of course, we have some work to do to really make things dynamic,
    but we have more than enough information to create the files.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前的代码应该足以满足我们初始文件创建的需求。以下列表中的功能应该看起来很熟悉，这是好事，这意味着我们的语法足够通用，可以处理一些变化。当然，我们还有一些工作要做，以真正使事情动态化，但我们有足够的信息来创建文件。
- en: Listing 11.2  Creating an ACH IAT file
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.2 创建ACH IAT文件
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With an ACH file containing an IAT batch available, we can start working on
    the necessary tables and structure to support storing the file.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在有了包含IAT批次的ACH文件后，我们可以开始工作，以必要的表格和结构来支持存储该文件。
- en: 11.3.1 Database changes
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 数据库变更
- en: We need to add a minimum of nine tables to the database to support IAT processing.
    We are only considering the new batch header format, entry format, and the required
    addenda records. The layout of our tables in the database mimics the layout of
    the ACH records. Let’s take a look at some of the tables. Remember that our database
    structure relies on having both unparsed and parsed records, the idea being that
    our system will eventually expand to process the files asynchronously once they
    have been uploaded. Since no new record numbers have been introduced, we do not
    need to expand anything regarding the unparsed records as IAT will still fit nicely
    into that structure. If we want to have our parsed records stored (and we do),
    we need to add the nine tables.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要向数据库中添加至少九个表格来支持IAT处理。我们目前只考虑新的批次头格式、条目格式以及所需的附加记录。我们数据库中表格的布局模仿了ACH记录的布局。让我们看看一些表格。记住，我们的数据库结构依赖于拥有未解析和已解析的记录，其理念是，我们的系统最终将扩展以异步处理已上传的文件。由于没有引入新的记录编号，我们不需要对未解析的记录进行任何扩展，因为IAT将很好地适应该结构。如果我们想存储已解析的记录（我们确实想这么做），我们需要添加九个表格。
- en: Let’s take a look at the `ach_iat_batch_headers` table in listing 11.3\. Notice
    how it still has a foreign key reference to the `ach_records_type_5` table. Also,
    the majority of the fields are stored as `VARCHAR`, because as an initial iteration,
    we are looking to take a simple approach to the structure. In future iterations,
    fields such as `service_class_code` and `effective_entry_date` could be updated
    to `NUMERIC` or `DATE`, respectively. Those constraints will help ensure the integrity
    of the record and are worth dealing with.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看列表11.3中的`ach_iat_batch_headers`表。注意它仍然有一个外键引用到`ach_records_type_5`表。此外，大多数字段存储为`VARCHAR`，因为作为初始迭代，我们希望对结构采取简单的方法。在未来的迭代中，字段如`service_class_code`和`effective_entry_date`可以分别更新为`NUMERIC`或`DATE`。这些约束将有助于确保记录的完整性，并且值得处理。
- en: Listing 11.3  IAT batch headers table
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.3 IAT批次头表
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 The foreign key for the batch header with CASCADEs for the deletes and updates'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 批次头的外键，对于删除和更新有CASCADEs'
- en: '#2 The rest of the fields required to support the parsing of the ACH record.
    Consider using more specific data types to ensure the record is formatted correctly
    by enforcing correct typing at the database level.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 支持解析ACH记录所需的其他字段。考虑使用更具体的数据类型，以确保通过在数据库级别强制正确的类型来正确格式化记录。'
- en: '#3 The rest of the fields required to support the parsing of the ACH record.
    Consider using more specific data types to ensure the record is formatted correctly
    by enforcing correct typing at the database level.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 支持解析ACH记录所需的其他字段。考虑使用更具体的数据类型，以确保通过在数据库级别强制正确的类型来正确格式化记录。'
- en: The rest of the tables are handled in much the same way, but we should be aware
    of a couple of design choices and standards we may need to consider when dealing
    with the addenda records. As an example, let’s use the addenda record that contains
    the originator’s city, state, country, and postal code. This addenda record has
    a record type code of 7 (because it is an addenda record) and an addenda type
    code of 12 (because that is what Nacha decided).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的表格以类似的方式处理，但我们应该意识到在处理附加记录时可能需要考虑的一些设计选择和标准。例如，让我们使用包含发起人城市、州、国家和邮政编码的附加记录。这个附加记录的记录类型代码为7（因为它是一个附加记录），附加类型代码为12（因为这是Nacha的决定）。
- en: First, we need to name our tables. So, should we use the `ach_iat_originator_address_info`
    or `ach_iat_addenda_712_records` or some variation on that? In general, it does
    not matter unless we go with a name that is extreme (i.e., too long or too short
    and cryptic). We originally went with `ach_iat_addenda_712_records` because the
    712 will be at the beginning of every line for those types of addenda records
    in the file, and we can key off that when trying to remember the name of our table.
    It also saves us from having to know what that type of addenda we are dealing
    with (i.e., whether this is the originator address or receiver address). Of course,
    there may be some complaints about the table name, such as
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要命名我们的表格。那么，我们应该使用 `ach_iat_originator_address_info`、`ach_iat_addenda_712_records`
    或其他类似名称吗？一般来说，这并不重要，除非我们选择一个极端的名称（即太长或太短且难以理解）。我们最初选择了 `ach_iat_addenda_712_records`，因为712将位于文件中此类附加记录的每一行的开头，我们可以在尝试记住表格名称时利用这一点。这也使我们免于知道我们正在处理哪种类型的附加记录（即这是发起人地址还是接收人地址）。当然，可能会有一些关于表格名称的投诉，例如
- en: It contains the word addenda and a 7 which is redundant, as a type 7 record
    is always an addenda
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含单词附加和一个7，这是多余的，因为类型7的记录总是附加记录
- en: It does not separate the 7 and 12 with an underscore even though they are two
    separate fields
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使它们是两个单独的字段，它也没有用下划线分隔7和12
- en: It uses the word records, and previously, tables using “records” contained unparsed
    records
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用单词记录，并且之前，使用“记录”的表格包含未解析的记录
- en: 'We point these out to highlight the importance of consistency and standards.
    The closer we stay to established standards, the more consistent we are likely
    to be. Given that we wanted to try to adhere to a standard, we implemented the
    following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指出这些是为了强调一致性和标准的重要性。我们越接近既定的标准，我们可能越一致。鉴于我们想要尽量遵守标准，我们实施了以下措施：
- en: Use `_details` for tables that contained the parsed records
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于包含解析记录的表格，使用 `_details`
- en: Use `_records` for tables that contained the unparsed records
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于包含未解析记录的表格，使用 `_records`
- en: Use the prefix `ach_ppd`, `ach_iat`, and so on for tables that dealt with specific
    ACH formats, as we were not always consistent with where the name `ppd` was used
    in the table
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于处理特定ACH格式的表格，使用前缀 `ach_ppd`、`ach_iat` 等，因为我们并不总是对表格中使用的名称 `ppd` 保持一致
- en: Thus, we ended up with names such as `ach_iat_entry_details` and `ach_iat_addenda_10_details`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们最终得到了像 `ach_iat_entry_details` 和 `ach_iat_addenda_10_details` 这样的名称。
- en: Next, let’s tackle the table itself. Listing 11.4 shows the `ach_iat_addenda_712_records`
    table. Some of these IAT addenda records are unique to the ACH standards because
    they contain fields that are delimited within the fixed records. Strange, huh?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们处理表格本身。列表11.4显示了 `ach_iat_addenda_712_records` 表。其中一些IAT附加记录是ACH标准的独特之处，因为它们包含在固定记录中分隔的字段。奇怪，对吧？
- en: Previously, other formats were strictly a fixed-length record. So, we could
    say those 15 characters are the name and will be stored in the name field. With
    some of these addenda records, we have a fixed length field, such as the 35 character
    “Originator City & State/Province,” which contains both the city and state. The
    data elements are delimited by an asterisk `*`, and the backslash `\` is the terminator
    for the last element. This results in a table that has seven fields (excluding
    the UUID), whereas the record has six fields. Note that one of the fields is reserved
    and not in use, and therefore, it is not represented in the table. Consequently,
    these parsed fields are represented in the table by individual fields. We left
    each individual field at the maximum field size, so although the originators city/state
    are both contained in one field of 35 characters, we kept both the city and state
    fields in the table at 35 bytes to avoid confusion.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，其他格式严格是固定长度的记录。因此，我们可以说这15个字符是名称，并将存储在名称字段中。在这些附加记录中，我们有一个固定长度字段，例如35个字符的“发起城市及州/省”，它包含城市和州。数据元素由星号`*`分隔，反斜杠`\`是最后一个元素的终止符。这导致了一个具有七个字段（不包括UUID）的表格，而记录有六个字段。请注意，其中一个字段是预留的，未使用，因此它未在表中表示。因此，这些解析字段在表中由单个字段表示。我们保留了每个单独字段的最大字段大小，因此尽管发起城市/州的名称都包含在一个35个字符的字段中，但我们仍然在表中保留了城市和州字段各35个字节，以避免混淆。
- en: Listing 11.4  Table for IAT addenda type 12
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.4  IAT附加类型12的表格
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 **Defaults the addenda_type_code to be a 12 as it must always be a 12 for
    this record**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 **将附加类型代码默认设置为12，因为此记录必须始终为12**'
- en: '#2 **Leaves these records as VARCHAR(35), although we could consider enforcing
    a stricter data type**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 **将这些记录保留为VARCHAR(35)，尽管我们可以考虑强制执行更严格的数据类型**'
- en: This code provides the pattern for how all the IAT tables were dealt with. Next,
    we look at how the records are parsed.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码提供了如何处理所有IAT表格的模式。接下来，我们看看记录是如何解析的。
- en: 11.4 IAT record parsing
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 IAT记录解析
- en: To parse the records, we use the file creation steps we built earlier to create
    a sample file and then use unit test to build the needed code to parse the various
    records and store them in the database (listing 11.5). We have our sample record
    that we are looking to parse—the expected result. We set up the necessary records
    for testing by calling `setup_iat_addenda_test`, which simply adds the needed
    headers and entry records to the database so that all the foreign keys work as
    expected. Then, we call the `_parse_iat_addenda_712` and the class `AchIat712AddendaSql`.
    It can sometimes be tempting to simply define the `expected_result` based on the
    return value when we are sure our logic is correct. We would advise against that
    and make sure the record is parsed in another way, whether that is by hand or
    with another tool so that the results are independently verified.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解析记录，我们使用我们之前构建的文件创建步骤来创建一个示例文件，然后使用单元测试来构建解析各种记录并将其存储在数据库中的所需代码（列表11.5）。我们有我们正在寻找解析的样本记录——预期结果。我们通过调用`setup_iat_addenda_test`来设置必要的测试记录，它只是将所需标题和条目记录添加到数据库中，以便所有外键都能按预期工作。然后，我们调用`_parse_iat_addenda_712`和类`AchIat712AddendaSql`。有时，当我们确信我们的逻辑是正确的时候，简单地根据返回值定义`expected_result`可能会很有诱惑力。我们建议不要这样做，并确保记录以另一种方式解析，无论是手动还是使用其他工具，以便结果得到独立验证。
- en: Listing 11.5  Unit testing the parse routine for Type 12 addenda
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.5  对Type 12附加的解析例程进行单元测试
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 The fixture to ensure the database is empty; we have autouse set to True
    so there is no need to include it in our test methods as it will automatically
    be executed.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 确保数据库为空的固定值；由于我们已将autouse设置为True，因此无需将其包含在我们的测试方法中，因为它将自动执行。'
- en: '#2 A sample addenda record taken from our test ACH file'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 从我们的测试ACH文件中取出的样本附加记录'
- en: '#3 The dictionary of the expected result to validate the retrieved record'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 验证检索记录的预期结果字典'
- en: '#4 Sets up the needed database records so the constraints are satisfied'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 设置所需的数据库记录以满足约束条件'
- en: '#5 Parses the record, which will also add it to the database'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 解析记录，这也会将其添加到数据库中'
- en: '#6 Retrieves the record, excluding the UUID field. Since the UUID is assigned
    by the database, we cannot hardcode it in our expected results.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 检索记录，排除UUID字段。由于UUID由数据库分配，我们无法在预期结果中硬编码它。'
- en: '#7 Performs asserts to ensure there is only one row and that the record matches
    our expected value that was previously defined'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 执行断言以确保只有一行，并且记录与之前定义的预期值匹配'
- en: The previous unit test will fail until we have built the required functionality.
    We will work through the required pieces of code to establish a pattern. All the
    unit tests for this will follow a similar approach. The first missing method that
    we encounter is the `_parse_iat_addenda_712` method, as in the following listing.
    Although it is straightforward, we do have to populate the `expected_record_types`,
    which helps the parser determine when records are out of sequence.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的单元测试将失败，直到我们构建了所需的功能。我们将通过必要的代码片段来建立模式。所有这些单元测试都将遵循类似的方法。我们遇到的第一个缺失方法是`_parse_iat_addenda_712`方法，如下所示。尽管它很简单，但我们确实需要填充`expected_record_types`，这有助于解析器确定记录是否顺序错误。
- en: We want to keep verification in mind when we get to updating the processing
    logic for our ACH file because we will need to determine whether we have received
    all the required records and ensure that there are no duplicate addenda records.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在更新我们的ACH文件处理逻辑时，需要牢记验证，因为我们需要确定是否已收到所有必需的记录，并确保没有重复的附加记录。
- en: Listing 11.6  The `_parse_iat_addenda_712` record
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.6 `_parse_iat_addenda_712`记录
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 The expected record types that can be passed to us'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 可以传递给我们的预期记录类型'
- en: '#2 The parsing is actually done with the AchRecordProcessor. At this point,
    we may also consider moving some of our parsing routines to new classes.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 解析实际上是由AchRecordProcessor完成的。在这个时候，我们也可以考虑将一些解析例程移动到新的类中。'
- en: '#3 With the parsed record, we need to insert it into the database.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用解析后的记录，我们需要将其插入到数据库中。'
- en: Next, we create the method `parse_iat_addenda_712`. This task could also involve
    its own separate unit tests since the purpose of that class is twofold. First,
    it consolidates the actual parsing logic into a central location, which reduces
    the code in our ACH file processor and allows clearer understanding of the ACH
    processing flow. Second, it allows us to test the parsing logic in isolation without
    the need for a bunch of setup in the database.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建`parse_iat_addenda_712`方法。这项任务也可能涉及它自己的单独单元测试，因为该类的目的是双重的。首先，它将实际的解析逻辑集中到一个中央位置，这减少了我们的ACH文件处理器中的代码，并允许更清晰地理解ACH处理流程。其次，它允许我们在不需要在数据库中进行大量设置的情况下单独测试解析逻辑。
- en: However, the parsing is not overly complicated and will be tested by this overall
    process, so for the time being, we will not worry about it having its own unit
    test. The following listing shows the code for parsing the IAT addenda.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，解析并不过于复杂，并且将通过此整体过程进行测试，所以目前我们不会担心它需要自己的单元测试。以下列表显示了解析IAT附加记录的代码。
- en: Listing 11.7  Parsing the IAT addenda record
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.7 解析IAT附加记录
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Creates a regular expression that will parse the delimited fields within
    the record'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建一个正则表达式来解析记录中的分隔字段'
- en: '#2 Ensures we have matches and extracts them for the city and state'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 确保我们有匹配项并从中提取城市和州'
- en: '#3 Ensures we have matches and extracts them for the country and postal code'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 确保我们有匹配项并从中提取国家和邮政编码'
- en: '#4 Parses the record and returns it as part of our schema'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 解析记录并将其作为我们模式的一部分返回'
- en: Moving on, we need to define the schema used for these records. As shown in
    listing 11.8, we provide a minimum layout that matches what we expect to insert
    into the database.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义用于这些记录的模式。如列表11.8所示，我们提供了一个最小布局，以匹配我们期望插入到数据库中的内容。
- en: Listing 11.8  IAT addenda 712 schema
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.8 IAT附加712模式
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The final step is to create a class to handle the SQL logic for inserting and
    retrieving the record.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是创建一个类来处理插入和检索记录的SQL逻辑。
- en: Listing 11.9  IAT addenda type 12 SQL
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.9 IAT附加类型12 SQL
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Defines a class and method to insert our record schema'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义一个类和方法来插入我们的记录模式'
- en: '#2 Obtains a database connection'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 获取数据库连接'
- en: '#3 SQL to insert the fields and values for our model'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 SQL用于插入模型字段和值'
- en: '#4 Creates a dictionary of the schema to use with the INSERT statement'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 为INSERT语句创建一个模式字典'
- en: '#5 The rest of the methods, specifically getting a record by the UUID'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 其余的方法，特别是通过UUID获取记录'
- en: With that, we should be able to go back and ensure all the import statements
    are in place and then run our unit test successfully. Assuming we are parsing
    the record correctly (for both the expected and the actual record), we should
    have a passing unit test.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们应该能够回去确保所有导入语句都已就位，然后成功运行我们的单元测试。假设我们正确解析了记录（对于预期和实际记录），我们应该有一个通过的单位测试。
- en: Unit testing IAT challenge
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 单元测试IAT挑战
- en: This pattern needs to be repeated for all the new database records, and now
    is a great time to take a break and write some code. We have sample files. It
    is now just a matter of working through parsing the remaining records. If the
    previous code examples still seem a bit daunting, we could also start at a smaller
    scale by creating unit tests for `AchRecordProcessor`, which is solely responsible
    for parsing the ACH record with no database interaction. The simpler requirements
    for testing the `Ach­RecordParser` should mean that it takes less work to set
    up our unit tests. Once the parsing is verified at that level, we can take a step
    back to see the bigger picture and begin writing unit tests that involve the database,
    as outlined in this section.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式需要为所有新的数据库记录重复，现在是休息一下并编写一些代码的好时机。我们有样本文件。现在的问题仅仅是处理剩余记录的解析。如果之前的代码示例仍然有点令人畏惧，我们也可以从小规模开始，为`AchRecordProcessor`创建单元测试，它只负责解析ACH记录，没有数据库交互。测试`Ach-RecordParser`的简单要求意味着设置我们的单元测试需要的工作量更少。一旦在该级别验证了解析，我们可以退后一步，看看更大的图景，并开始编写涉及数据库的单元测试，如本节所述。
- en: While we should have all the parsing validated at this stage, we still need
    to update our `ach_file_processor` to handle the IAT batch and make use of all
    this wonderful code we just wrote.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们应该在这个阶段验证所有的解析，但我们仍然需要更新我们的`ach_file_processor`以处理IAT批次并利用我们刚刚编写的所有这些美好代码。
- en: 11.5 IAT file processing
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 IAT文件处理
- en: 'At this point, we have built the pieces of our IAT processing. We should feel
    fairly confident that we can handle the individual records and parsing them. Now,
    we need to be able to incorporate the parsing of an actual IAT batch within a
    file. We need to keep in mind that the work we did in previous sections tested
    individual pieces. For instance, we know that when we call the `_parse_iat_batch_header`
    method and pass it an IAT batch header record, it will be parsed and stored in
    the database. However, the method is not called in the current flow of loading
    an ACH file through the `POST` call. As we work on adding the functionality to
    the parser, we should keep some goals and requirements in mind:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经构建了我们IAT处理的部分。我们应该相当自信，我们可以处理单个记录并对它们进行解析。现在，我们需要能够在文件中结合实际的IAT批次的解析。我们需要记住，我们在前面的章节中测试的是单个部分。例如，我们知道当我们调用`_parse_iat_batch_header`方法并传递一个IAT批次头记录时，它将被解析并存储在数据库中。然而，该方法在当前通过`POST`调用加载ACH文件的过程中没有被调用。当我们向解析器添加功能时，我们应该牢记一些目标和要求：
- en: Parsing a PPD batch still works as expected.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析PPD批次仍然按预期工作。
- en: Addenda records are all present.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 补充记录全部存在。
- en: Addenda records are in the correct order.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 补充记录的顺序是正确的。
- en: In the next section, we begin ensuring that we have unit tests before we start
    making changes to the actual code.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们开始确保在开始对实际代码进行更改之前，我们有单元测试。
- en: 11.5.1 Unit testing
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.1 单元测试
- en: We hope it is obvious that after adding the IAT batches, we should still be
    able to load PPD batches. So, do not forget to test and verify that we have not
    broken anything with the addition of IAT processing. This means that we want to
    ensure we implement some regression testing—the last thing we want to do is spend
    all our time working on the new processing and not validate the previous work.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望很明显，在添加IAT批次后，我们仍然能够加载PPD批次。所以，不要忘记测试和验证我们没有因为添加IAT处理而破坏任何东西。这意味着我们想要确保我们实施一些回归测试——我们最不想做的事情就是花所有时间在新的处理上，而不验证之前的工作。
- en: With that in mind, the first order of business is to create a test_loading_pdd_files
    file and ensure that we have the correct record count when loading a PPD batch.
    We start with simple tests to get the count of the unparsed records. We test for
    the individual record counts and the total number of records, ensuring that we
    did not write any exceptions, as shown in the following listing.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个前提下，首要任务是创建一个`test_loading_pdd_files`文件，并确保在加载PPD批次时我们拥有正确的记录计数。我们从简单的测试开始，以获取未解析记录的数量。我们测试单个记录的计数和记录总数，确保我们没有写入任何异常，如下所示。
- en: Listing 11.10  Testing PPD batches
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.10  测试PPD批次
- en: '[PRE9]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Defines a file name since it will be used in a few different places during
    the test'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义一个文件名，因为它将在测试的几个不同地方使用'
- en: '#2 Ensures we can reference the file'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 确保我们可以引用文件'
- en: '#3 Sets up some initial expected values'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 设置一些初始预期值'
- en: '#4 Sets up the test, parses the file, and returns any exceptions'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 设置测试，解析文件，并返回任何异常'
- en: '#5 Queries the database to get record counts for each of the tables that stores
    our unparsed records. This test contains a lot of repetitive code to obtain the
    count for each type.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 查询数据库以获取存储未解析记录的每个表的记录数。这个测试包含大量重复的代码来获取每种类型的计数。'
- en: '#6 Adds the record counts together to get the total count, but it is somewhat
    redundant since we are testing the counts individually as well. However, how many
    times we would be asked about a total count if this was not here?'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 将记录计数相加以获取总数，但这有点冗余，因为我们也在单独测试计数。然而，如果没有这个，我们会被问多少次总数呢？'
- en: '#7 Asserts the record counts are correct'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 断言记录计数是正确的'
- en: The previous test works well enough, but as it tends to happen with our development,
    we repurposed some other test to create the new one and ended up copying/pasting
    code to get the additional record counts and `assert` statement. In fact, Copilot
    is nice enough to fill in some of the code for us, so we did not even have to
    do much copying/pasting. There is one problem, though—we should be treating our
    test code as a first-class citizen, giving it the same attention we gave to our
    production code.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的测试效果足够好，但随着我们的开发工作，我们重新利用了其他测试来创建新的测试，并最终复制/粘贴代码来获取额外的记录计数和`assert`语句。事实上，Copilot足够好，为我们填写了一些代码，所以我们甚至不需要做太多的复制/粘贴。但是有一个问题——我们应该将测试代码视为一等公民，给予它与我们给予生产代码相同的关注。
- en: Let’s look at how we might rework the above code to make it more concise and
    easier to understand. The changes in the following listing shortened the code
    from 53 to 45 lines, and while that is not the only metric, we should use it for
    judging whether code is good or bad. Early on in our careers, we were told by
    one of our mentors that it felt more productive when they were removing code rather
    than writing it.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何重构上面的代码，使其更简洁、更容易理解。以下列表中的更改将代码从53行缩短到45行，虽然这不是唯一的指标，但我们应该用它来判断代码是好是坏。在我们职业生涯的早期，我们的一位导师告诉我们，当他们删除代码而不是编写代码时，他们会感到更有生产力。
- en: Listing 11.11  Refactored unit test for unparsed records
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.11  对未解析记录的单元测试重构
- en: '[PRE10]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Updates the get_db call to use a row_factory of dict_row'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 更新get_db调用以使用dict_row作为row_factory'
- en: '#2 The record counts for each of our ACH tables'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们ACH表中的每个记录计数'
- en: '#3 Computes the total records'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 计算总记录数'
- en: '#4 The queries to get the counts of each row'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 获取每行计数的查询'
- en: '#5 The queries to get the counts of each row'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 获取每行计数的查询'
- en: '#6 Gets the single result'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 获取单个结果'
- en: '#7 Adds the exception count into the record_counts dictionary'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 将异常计数添加到record_counts字典中'
- en: '#8 Compares the two dictionaries'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 比较两个字典'
- en: We have a similar query to verify that parsed records exist in the table for
    all the parsed records as well. Ideally, we would expect an exception to be in
    the table if we had any problems parsing, but in case we have not yet coded for
    that or perhaps missed a condition that would cause a parse error, it is good
    to check these tables as well.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个类似的查询来验证解析记录存在于所有解析记录的表中。理想情况下，如果我们有任何解析问题，我们期望表中会有异常，但如果我们还没有为这种情况编写代码，或者可能错过了一个会导致解析错误的条件，那么检查这些表也是好的。
- en: Listing 11.12  Unit test for parsed PPD records
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.12  解析PPD记录的单元测试
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 The selection logic can remain the same.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 选择逻辑可以保持不变。'
- en: '#2 Note that the counts are now occurring on the parsed records and that the
    tables are specific to PPD batches.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 注意，计数现在发生在解析记录上，并且表是针对PPD批次的特定表。'
- en: Our goal was to ensure that we had enough tests to validate the files processed
    successfully. We should now be confident that the file has been processed in the
    database, that their unit tests may validate those database fields, and that the
    files were parsed correctly. Remember from chapter 2 that we should test not only
    the happy path (success) but also the not-so-happy path (error handling). For
    now, we have enough to move forward with a similar test for IAT processing.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是确保我们有足够的测试来验证处理成功的文件。现在我们应该有信心，文件已经在数据库中处理过，它们的单元测试可以验证这些数据库字段，并且文件已经被正确解析。记得在第二章中，我们不仅应该测试成功的路径（成功），还应该测试不那么成功的路径（错误处理）。目前，我们有足够的测试来继续进行IAT处理的类似测试。
- en: We can create a similar test for unparsed records that loads a file containing
    an IAT batch instead. We created one and named it (rather unimaginatively) iat.ach.
    The processing for an IAT file does not change when we consider only the unparsed
    records, and that is partially why we needed to test both the unparsed and parsed
    records. Of course, we still tested the unparsed records with a unit test, but
    the real work is for the IAT parsed records, as in the following listing.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为未解析的记录创建一个类似的测试，该测试加载一个包含 IAT 批次的文件。我们创建了一个名为（相当缺乏想象力）的 iat.ach。当我们只考虑未解析的记录时，IAT
    文件的处理不会改变，这也是我们需要测试未解析和解析记录的部分原因。当然，我们仍然使用单元测试测试了未解析的记录，但真正的工作是为 IAT 解析记录，如下所示。
- en: Listing 11.13  Unit test for parsed IAT records
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.13  解析 IAT 记录的单元测试
- en: '[PRE12]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 We select the desired records and sum them up to get the total count, which
    we also verified by pulling up the file (since it is just a text file) in any
    editor and reviewing the record counts.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们选择所需的记录并将它们加起来以获取总数，我们也通过在任何编辑器中打开文件（因为它只是一个文本文件）并检查记录数来验证了这一点。'
- en: '#2 We need to include the IAT addenda tables. Keep in mind the batch header,
    entry details, and addenda records will be different from PPD batches.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们需要包含 IAT 补充表。记住，批次头、条目详情和补充记录将与 PPD 批次不同。'
- en: The previous test will fail because we have not yet updated the file parsing
    to make use of these detail tables. However, we are now confident that we have
    both PPD and IAT parsing covered by unit tests, which means we can move ahead
    to using these new IAT detail tables.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的测试将失败，因为我们还没有更新文件解析以利用这些详细表格。然而，我们现在有信心，我们已经通过单元测试覆盖了 PPD 和 IAT 解析，这意味着我们可以继续使用这些新的
    IAT 详细表格。
- en: 11.5.2 Updating file processing
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.2 更新文件处理
- en: With our passing PPD file load and the failing IAT file load, we begin working
    through the changes needed to support IAT. As we make changes, we will constantly
    be rerunning the unit tests to ensure that we are making progress with parsing
    IAT files and that we have not inadvertently broken anything. Since an ACH file
    is processed sequentially, the first record we need to address is the batch header
    record. We add a new field named `batch_type` that we will use when parsing the
    batch header. The following listing shows the simple `if`/`elif`/`else` processing
    that we have added, just calling the appropriate routine based on the SEC code
    and logging an exception when having a header we do not recognize.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 PPD 文件加载通过和 IAT 文件加载失败之后，我们开始着手支持 IAT 所需的更改。随着我们进行更改，我们将不断重新运行单元测试，以确保我们在解析
    IAT 文件方面取得进展，并且我们没有无意中破坏任何东西。由于 ACH 文件是顺序处理的，我们需要首先处理的记录是批次头记录。我们添加了一个名为 `batch_type`
    的新字段，我们将用它来解析批次头。以下列表显示了我们所添加的简单 `if`/`elif`/`else` 处理，根据 SEC 代码调用适当的例程，并在遇到我们不认识的头时记录异常。
- en: Listing 11.14  Calling the appropriate parsing routine
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.14  调用适当的解析例程
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 The SEC code is in the same position, regardless of whether the batch header
    is for IAT or PPD. This line sets the batch_type flag appropriately for the SEC
    type.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 无论批次头是 IAT 还是 PPD，SEC 代码都在相同的位置。这一行设置了适当的 batch_type 标志以适应 SEC 类型。'
- en: '#2 Previously, we just called the _parse_batch_header method. Now, we ensure
    that we are dealing with a PPD batch.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 之前，我们只是调用了 `_parse_batch_header` 方法。现在，我们确保我们正在处理一个 PPD 批次。'
- en: '#3 We would like to log an exception when processing, and we have an unrecognized
    SEC code. This could be because of an invalid formatted file or a code we do not
    support yet.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们希望在处理过程中记录异常，并且我们遇到了一个未识别的 SEC 代码。这可能是因为文件格式无效或我们尚未支持的代码。'
- en: With the proper parsing of the batch header, we should be making it a bit farther
    in our testing of IAT processing, and of course, PPD processing should not be
    broken. The next record we encounter is the type 6 record. With the introduction
    of the `batch_type` variable, it should follow the same pattern. Notice that we
    continue to write the unparsed record to the database like before. However, the
    need to parse the record and write it to the appropriate table depends on the
    `batch_type` flag. The following listing shows the required updates.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在正确解析批次头的情况下，我们应该在 IAT 处理测试中前进一点，当然，PPD 处理不应该出错。我们遇到的下一个记录是类型 6 记录。随着 `batch_type`
    变量的引入，它应该遵循相同的模式。请注意，我们继续像以前一样将未解析的记录写入数据库。然而，解析记录并将其写入适当表的需要取决于 `batch_type`
    标志。以下列表显示了所需的更新。
- en: Listing 11.15  Parsing entry details
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.15  解析条目详情
- en: '[PRE14]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 The writing of our unparsed records is independent of the batch type we
    are working with.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们未解析的记录的编写与我们正在处理的批次类型无关。'
- en: '#2 When the batch_type is set to IAT, we will call the appropriate method to
    parse the entry.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 当batch_type设置为IAT时，我们将调用适当的方法来解析条目。'
- en: '#3 The parameters for IAT and PPD parsing are exactly the same. Only the fields
    that need to be parsed are different.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 IAT和PPD解析的参数完全相同。只有需要解析的字段不同。'
- en: '#4 Adds an exception when we encounter an unexpected batch_type'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 当我们遇到意外的batch_type时，添加一个异常'
- en: Parsing the addenda records is only slightly more complicated than parsing the
    batch header and entry detail records. This is because we now also deal with the
    different addenda record types being in the expected order. Similar to the `expected_record_type`
    variable, we also introduce an `expected_addenda_type` variable. Let’s jump straight
    to the method where we parse a specific addenda type record, as shown in the following
    listing. Here, it is standard processing with the addition of the new `expected_addenda_type`.
    The next record we expect is another addenda record (type 7), and it should be
    of addenda type 11.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 解析附加记录比解析批次头和条目详细记录稍微复杂一些。这是因为我们现在还必须处理不同的附加记录类型按照期望的顺序排列。类似于expected_record_type变量，我们也引入了一个expected_addenda_type变量。让我们直接跳到解析特定附加类型记录的方法，如下面的列表所示。在这里，这是标准的处理，增加了新的expected_addenda_type。我们期望的下一条记录是另一个附加记录（类型7），它应该是附加类型11。
- en: Listing 11.16  The `_parse_iat_addenda_710` method
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.16 `_parse_iat_addenda_710`方法
- en: '[PRE15]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 The expected_record should be another addenda record.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 expected_record应该是另一个附加记录。'
- en: '#2 The expected_addenda_type should be the next sequence as the records are
    required to be in a specific order. Note that we only expect one type of record,
    so there is no need for an array as with the expected_record_type.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 expected_addenda_type应该是下一个序列，因为记录需要按照特定的顺序排列。注意，我们只期望一种类型的记录，因此不需要像expected_record_type那样使用数组。'
- en: All the addenda records will parse the same way, with the `expected_addenda_type`
    being set to the next record. But what happens when we get to the final addenda
    type (type 16)? The following listing shows how we reset our expected types once
    we reach that last record.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的附加记录将以相同的方式解析，expected_addenda_type被设置为下一条记录。但是当我们到达最后的附加类型（类型16）时会发生什么？下面的列表展示了我们如何重置我们的期望类型，一旦我们到达最后一个记录。
- en: Listing 11.17  Resetting the expected types
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.17 重置期望类型
- en: '[PRE16]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 There are some optional addenda records for IAT processing, so it is possible
    to encounter those, as well as a new entry or the end of the batch.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在IAT处理中存在一些可选的附加记录，因此可能会遇到这些记录，以及新的条目或批次的结束。'
- en: '#2 There are no more mandatory addenda records, so the expected_addenda_type
    can be set to an empty string.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 没有更多的强制附加记录，因此expected_addenda_type可以被设置为空字符串。'
- en: Now that we know how to parse the addenda records, it should be easy to put
    together how these individual methods would be called—as they follow a similar
    approach to how we parse the record types. The process is shown in the following
    listing.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何解析附加记录，应该很容易将这些单独的方法组合起来——因为它们遵循与我们解析记录类型相似的方法。这个过程在下面的列表中展示。
- en: Listing 11.18  Parsing the addenda records
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.18 解析附加记录
- en: '[PRE17]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Defines our method for parsing. This is an umbrella that will call the other
    methods to parse specific records.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义我们的解析方法。这是一个总称，将调用其他方法来解析特定记录。'
- en: '#2 Extracts the addenda type since we will be using it repeatedly'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 提取附加类型，因为我们将会反复使用它'
- en: '#3 If the addenda type is not what we expected, we need to log an exception.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 如果附加类型不是我们期望的，我们需要记录一个异常。'
- en: '#4 Depending on the addenda type we are working with, calls the appropriate
    method. It logs a different exception if we have an unexpected addenda type.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 根据我们正在处理的附加类型，调用适当的方法。如果遇到意外的附加类型，它将记录不同的异常。'
- en: With that, we should be able to parse files containing both IAT and PPD batches.
    Having these extra tables for IAT batches will have an effect on some of our APIs.
    In the next section, we will take a brief look at exactly what is affected.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们应该能够解析包含IAT和PPD批次的文件。这些额外的IAT批次表将对我们的某些API产生影响。在下一节中，我们将简要看看具体哪些方面受到了影响。
- en: 11.6 Effects on the dashboard
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.6 对仪表板的影响
- en: How does the addition of IAT processing affect the functionality of our dashboard?
    That question would likely require more discussion with the business. From an
    ACH standards perspective, it is certainly possible to have IAT batches in the
    same file as PPD batches. However, the fields and interesting information for
    an IAT batch may be different, and certainly, the introduction of new tables plays
    into how the components function. Whether it makes sense for our components to
    include all batches or just select batches may be a business decision with input
    from end-users. For now, let’s look at how we can fit the IAT batches into our
    current dashboard components.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: IAT 处理的增加如何影响我们仪表板的功能？这个问题可能需要与业务进行更多讨论。从 ACH 标准的角度来看，在同一文件中拥有 IAT 批次和 PPD 批次当然是可能的。然而，IAT
    批次的字段和有趣信息可能不同，而且新表的出现确实会影响组件的功能。我们的组件是否应该包含所有批次或仅选择批次可能是一个需要来自最终用户输入的业务决策。目前，让我们看看我们如何将
    IAT 批次纳入当前的仪表板组件中。
- en: 11.6.1 The get_batches method
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.6.1 `get_batches` 方法
- en: The `/{file_id}/batches` endpoint calls the `get_batches` method, which will
    need to be updated to incorporate the IAT batch headers. We know that the `ach_records_type_5_id`
    will exist in only one of the batch header tables. We employ that and the `COALESCE`
    command to use a field from the `ach_batch_headers` and if that is `NULL`, to
    use the `ach_iat_batch_headers` value instead. This works under the assumption
    that a field will not unexpectedly be `NULL`. The fields in question are all marked
    as `NOT NULL` in the database. Whether they stay that way indefinitely is another
    matter. For the time being, the following listing shows how we can update our
    query without being too invasive to the current API.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`/batches` 端点调用 `get_batches` 方法，该方法需要更新以包含 IAT 批次标题。我们知道 `ach_records_type_5_id`
    只会存在于批次标题表中的一个。我们利用这一点和 `COALESCE` 命令来使用 `ach_batch_headers` 字段，如果它是 `NULL`，则使用
    `ach_iat_batch_headers` 的值。这基于假设字段不会意外地成为 `NULL`。有关字段在数据库中都被标记为 `NOT NULL`。它们是否永远保持这种状态是另一个问题。目前，以下列表显示了我们可以如何更新我们的查询，而不会对当前的
    API 过于侵入性。'
- en: Listing 11.19  Updated `get_batches`
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.19  更新的 `get_batches`
- en: '[PRE18]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#1 We do not have access to a company name for IAT from the header record.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们无法从标题记录中访问 IAT 的公司名称。'
- en: '#2 If the company_identification is not available, uses the originating_dfi_identification'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 如果公司识别信息不可用，则使用 originating_dfi_identification'
- en: '#3 Picks the appropriate batch_number from the batch'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从批次中选择适当的批次号'
- en: '#4 Uses LEFT JOIN because the batch header is for one table or the other'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用 LEFT JOIN 因为批次标题是针对一个表或另一个表的'
- en: '#5 Sorts by the appropriate identification number'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 按适当的识别号排序'
- en: As shown here, we can sometimes get away with updating the existing query; however,
    that is not the case in all situations. In fact, it is probably an exception.
    The next section discusses what to do when the query is too large or complicated
    to incorporate both PPD and IAT.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如此所示，我们有时可以更新现有的查询；然而，并非所有情况下都如此。实际上，这可能是例外。下一节将讨论当查询太大或太复杂，无法同时包含 PPD 和 IAT
    时应该做什么。
- en: 11.6.2 Batch entries
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.6.2 批次条目
- en: Gathering the batch entries from the endpoint `/{file_id}/batches/{batch_id}/entries`
    requires some work. The query to build the `AchBatchEntriesResponse` was sizeable,
    and it may make more sense to leave the current query in place and create a new
    query specific to IAT transaction entries. Therefore, we will take the `get_entries`
    method, move the existing query to a method `_get_ppd_entries`, and create a new
    `_get_iat_entries`. The results of the split are shown in the following listing.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 从端点 `/{file_id}/batches/{batch_id}/entries` 收集批次条目需要一些工作。构建 `AchBatchEntriesResponse`
    的查询相当大，因此可能更有意义的是保留当前的查询，并创建一个针对 IAT 交易条目的新查询。因此，我们将采取 `get_entries` 方法，将现有的查询移动到
    `_get_ppd_entries` 方法中，并创建一个新的 `_get_iat_entries` 方法。拆分的成果如下所示。
- en: Listing 11.20  Updated `get_entries`
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.20  更新的 `get_entries`
- en: '[PRE19]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Method to determine whether the specified batch in the file is an IAT batch'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 确定文件中指定的批次是否为 IAT 批次的 方法'
- en: '#2 Method to return the IAT entries for a given file and batch'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 返回给定文件和批次的 IAT 条目的 方法'
- en: '#3 The previous SQL query has been moved to its own method.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 之前的 SQL 查询已移动到其自己的方法中。'
- en: There are some slight changes when creating the `AchBatchEntriesResponse` for
    IAT records. With the original query for PPD records to get the addenda count,
    we had to gather that information ourselves. With an IAT batch, that information
    is part of the entry record; however, we still gather the count in the same fashion,
    in part to keep the queries similar, but more importantly, because we may want
    to perform verification on the field in the future, and this approach provides
    the mechanism to do that (listing 11.21).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建用于IAT记录的`AchBatchEntriesResponse`时，有一些细微的变化。对于获取附加记录数的原始PPD记录查询，我们必须自己收集这些信息。对于IAT批量，这些信息是记录的一部分；然而，我们仍然以相同的方式收集计数，部分是为了保持查询的相似性，但更重要的是，因为我们可能希望在将来对字段进行验证，这种方法提供了实现这一目标的机制（列表11.21）。
- en: Listing 11.21  Query from `_get_iat_entries`
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.21  `_get_iat_entries`查询
- en: '[PRE20]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 We get the number of addenda records, which is also available on the record
    type 6 for IAT transactions. Eventually, we could use this approach to validate
    the addenda count that was passed.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们获取附加记录数，该数也适用于IAT交易的记录类型6。最终，我们可以使用这种方法来验证传递的附加记录数。'
- en: '#2 We pass the receiving_name back as the individual_name to conform to our
    response, which needs to be retrieved from the addenda type 10 record.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们将接收方名称作为个人名称返回，以符合我们的响应，该响应需要从附加类型10记录中检索。'
- en: '#3 Pulls the amount from the IAT entry record'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从IAT记录条目中提取金额'
- en: '#4 The entry record also has the account number.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 记录条目还包括账户号码。'
- en: '#5 Joins the necessary IAT tables'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 连接必要的IAT表'
- en: Whether we incorporate IAT into the current dashboard or create brand new components
    is going to be a matter or preference by the business and users. One aspect that
    is not up for negotiation is remaining compliant with government regulations.
    The next section discusses how to address some regulatory concerns.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是将IAT纳入当前仪表板还是创建全新的组件，这将是业务和用户偏好的问题。一个不容协商的方面是必须遵守政府法规。下一节将讨论如何解决一些监管问题。
- en: 11.7 OFAC scanning
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.7 OFAC扫描
- en: Financial institutions are subject to various regulations and compliance on
    a daily basis. Regulations such as the PATRIOT Act and Know Your Customer (KYC)
    hold the financial institution responsible for failing to report potential money
    laundering and terrorist-financing activities. The Office of Foreign Asset Control
    (OFAC) provides a “Specially Designated Nationals” list, or a SDN list, that includes
    individuals, companies, and assets such as ships/planes that are prohibited to
    do business with, and institutions may face penalties.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 金融机构在日常运营中必须遵守各种法规和合规要求。例如，爱国者法案（PATRIOT Act）和了解你的客户（KYC）规定，金融机构对未能报告潜在的洗钱和恐怖主义融资活动负有责任。外国资产控制办公室（Office
    of Foreign Asset Control, OFAC）提供了一份“特别指定国民”名单，或称SDN名单，其中包括个人、公司以及禁止与之进行商业往来的船只/飞机等资产，机构可能面临处罚。
- en: Note that there are other lists, as well as countries in general, that institutions
    are prohibited from doing business with. Doing business with any of these sanctioned
    entities can cost the bank significantly. A list of entities that violated US
    regulations and their associated fines is available at [https://mng.bz/pKN8](https://mng.bz/pKN8).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，还有其他名单，以及一般意义上的国家，机构被禁止与之进行商业往来。与任何这些受制裁实体进行商业往来可能会给银行造成重大损失。违反美国法规的实体及其相关罚款的名单可在[https://mng.bz/pKN8](https://mng.bz/pKN8)找到。
- en: While there are third-party packages that specialize in this type of scanning,
    over the next few sections, we implement basic scanning for our dashboard to get
    a feel for the process. If you are interested in ML/AI and analytics, this is
    a great opportunity to dive in and build some comprehensive detection!
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在专门从事此类扫描的第三方软件包，但在接下来的几节中，我们将实现仪表板的基本扫描，以了解该过程。如果你对机器学习/人工智能和数据分析感兴趣，这是一个深入了解并构建一些综合检测的绝佳机会！
- en: 11.7.1 Sanctioned individuals and countries
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.7.1 受制裁的个人和国家
- en: There are XML files available for download from the OFAC website at [https://sanctionslist.ofac.treas.gov/Home/SdnList](https://sanctionslist.ofac.treas.gov/Home/SdnList)
    and an online search tool as well at [https://sanctionssearch.ofac.treas.gov/](https://sanctionssearch.ofac.treas.gov/).
    Either of these is a great way to start familiarizing ourselves with the information
    available when working with sanctioned individuals.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从OFAC网站下载XML文件，网址为[https://sanctionslist.ofac.treas.gov/Home/SdnList](https://sanctionslist.ofac.treas.gov/Home/SdnList)，同时还有一个在线搜索工具，网址为[https://sanctionssearch.ofac.treas.gov/](https://sanctionssearch.ofac.treas.gov/)。这两种方式都是熟悉与受制裁个人合作时可用信息的绝佳方法。
- en: For our purposes, we create a table that contains names and aliases for made-up
    individuals to avoid any potential problems with using actual data from the list.
    We also create a list of made-up countries we should also scan for. As we saw
    from the civil penalties, many times, companies find themselves in trouble just
    by doing business with companies/individuals in other countries. Listing 11.22
    shows the `create` statements for tables to hold individual names as well as countries.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们的目的，我们创建了一个包含虚构个人姓名和化名的表格，以避免使用列表中实际数据可能带来的任何潜在问题。我们还创建了一个应扫描的虚构国家列表。正如我们从民事处罚中看到的那样，许多时候，公司仅仅与在其他国家的公司/个人做生意就会陷入麻烦。列表11.22显示了用于存储个人姓名以及国家的表的`create`语句。
- en: Listing 11.22  Creating tables for OFAC scanning
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.22  为OFAC扫描创建表
- en: '[PRE21]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Next, we populate the tables when the database is created, as shown in the following
    listing. Do not forget to add the `sdn_list` and `sanctioned_countries` tables
    to the `truncate_all` utilities method as you do not want this default data cleared
    during unit testing.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在创建数据库时填充这些表，如下所示。不要忘记将`sdn_list`和`sanctioned_countries`表添加到`truncate_all`实用方法中，因为在单元测试期间，您不希望默认数据被清除。
- en: Listing 11.23  Populating the new tables
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.23  填充新表
- en: '[PRE22]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With our data populated, we can now move on to building the API for scanning.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据被填充后，我们现在可以继续构建用于扫描的API。
- en: 11.7.2 Scanning for individuals
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.7.2 对个人进行扫描
- en: Before we build an API, let’s define the query that will return the results
    for the OFAC scan. For our needs, we will be scanning all our loaded files whenever
    someone clicks on the OFAC Scanning link in our UI. However, this feature will
    not be practical later when we have larger lists of suspects, more complex scanning
    algorithms, and more files loaded. We are going to discuss some strategies to
    handle this problem, but for now, let’s break down the query we used to gather
    the results.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建API之前，让我们定义一个查询，该查询将返回OFAC扫描的结果。根据我们的需求，每当有人点击我们UI中的OFAC扫描链接时，我们将扫描所有加载的文件。然而，当我们拥有更大的嫌疑人名单、更复杂的扫描算法和更多加载的文件时，这个功能将不再实用。我们将讨论一些处理这个问题的策略，但现在，让我们分解我们用来收集结果的查询。
- en: As you know, the goal is to search for individual names on incoming ACH trans­actions
    and match them against known individuals on the lists provided by OFAC. Up until
    this point, when searching, we have used SQL keywords such as `ILIKE` and the
    `%` wildcard. Now, we get a little more advanced by introducing fuzzy matching
    to our query. By installing the `fuzzystrmatch` into our Postgres database, as
    in the following listing, we get additional search options.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，目标是搜索传入的ACH交易中的个人姓名，并将它们与OFAC提供的名单上已知的个人进行匹配。到目前为止，在搜索时，我们使用了SQL关键字，如`ILIKE`和`%`通配符。现在，我们通过在我们的查询中引入模糊匹配来变得稍微复杂一些。通过将`fuzzystrmatch`安装到我们的Postgres数据库中，如下所示，我们获得了额外的搜索选项。
- en: Listing 11.24  Installing `fuzzystrmatch`
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.24  安装`fuzzystrmatch`
- en: '[PRE23]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This extension provides phonetic matching algorithms such as Soundex and Meta­phone,
    as well as the Levenshtein distance algorithm, for measuring the similarity between
    two strings. These algorithms can help us create a more robust searching algorithm
    because, rather than searching for an exact or partial match, we can now expand
    our search to names that may by misspelt or slightly altered by a nefarious individual
    so that they can still perform transactions. Even though we are introducing feature
    for IAT transactions, it can be incorporated into any transactions done at a financial
    institution. Often, this (and various other checks) are done when a customer opens
    an account, and the database is often periodically scanned for any individuals.
    Therefore, we start with creating a common table expression (CTE) to gather names
    for PPD transactions. The main problem is to ensure we are collecting the names
    from the appropriate record, with ACH PPD transactions that will be on the type
    6 record.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 此扩展提供了音位匹配算法，如 Soundex 和 Meta­phone，以及 Levenshtein 距离算法，用于测量两个字符串之间的相似度。这些算法可以帮助我们创建一个更健壮的搜索算法，因为我们现在可以扩展搜索范围，以包括可能被恶意个体拼写错误或轻微更改的名称，以便他们仍然可以执行交易。尽管我们正在引入
    IAT 交易的特性，但它可以融入任何在金融机构进行的交易中。通常，当客户开设账户时，会进行此类（以及各种其他检查），数据库通常定期扫描以查找任何个人。因此，我们首先创建一个公用表表达式（CTE）来收集
    PPD 交易的名称。主要问题是确保我们从适当的记录中收集名称，对于 ACH PPD 交易，这些将位于类型 6 记录上。
- en: In the following listing, we want to only select distinct names to minimize
    some of the searching across batches if we have multiple names. We may want to
    consider additional information in case we have a false positive in regard to
    one customer but not another. We also used `REPLACE` to remove any spaces from
    the name, but could also use `REGEXP_REPLACE(aepd.individual_name,` `'[^a-zA-Z0-9]',`
    `'',` `'g')` if we wanted to deal with any punctuation.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们只想选择不同的名称以最小化跨批次的搜索，如果我们有多个名称。如果我们对某个客户而不是另一个客户有假阳性，我们可能需要考虑额外的信息。我们还使用了
    `REPLACE` 来删除名称中的任何空格，但如果需要处理任何标点符号，也可以使用 `REGEXP_REPLACE(aepd.individual_name,
    '^[^a-zA-Z0-9]', '', 'g')`。
- en: Listing 11.25  Collecting individual names from PPD transactions
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.25 从 PPD 交易中收集单个名称
- en: '[PRE24]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#1 Use SELECT DISTINCT because we do not want duplicate searches; one name
    will suffice.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用 SELECT DISTINCT，因为我们不希望有重复的搜索；一个名称就足够了。'
- en: '#2 Simple replacement to remove spaces, but nothing else. Use REGEXP_REPLACE
    for more complicated removal.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 简单替换以删除空格，但仅限于此。使用 REGEXP_REPLACE 进行更复杂的删除。'
- en: '#3 The record to find the particular transaction'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 查找特定交易的记录'
- en: '#4 Necessary JOINs to get down to the individual name'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 必要的 JOIN 操作以获取单个名称'
- en: '#5 Necessary GROUP BY statements because of the SELECT DISTINCT statement'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 由于 SELECT DISTINCT 语句，必要的 GROUP BY 语句'
- en: A similar CTE is required for IAT names, as shown in the following listing.
    As for some of our other IAT processing, we need to retrieve the name from the
    addenda record instead of the entry record. We return the `receiving_name` as
    the individual name to maintain alignment between the CTEs.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 IAT 名称也需要一个类似的公用表表达式（CTE），如下所示列表所示。至于我们的一些其他 IAT 处理，我们需要从附加记录中检索名称，而不是从条目记录中检索。我们将
    `receiving_name` 作为单个名称返回，以保持 CTE 之间的对齐。
- en: Listing 11.26  Collecting individual names from IAT transactions
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.26 从 IAT 交易中收集单个名称
- en: '[PRE25]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 At this time, we do not want duplicate searches because one name will suffice.
    We need to rename the field to individual_name or find a common name for the field.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在此时刻，我们不希望有重复的搜索，因为一个名称就足够了。我们需要将字段重命名为 individual_name 或找到一个字段的通用名称。'
- en: '#2 Simple replacement to remove spaces, but nothing else. Use REGEXP_REPLACE
    for more complicated removal.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 简单替换以删除空格，但仅限于此。使用 REGEXP_REPLACE 进行更复杂的删除。'
- en: '#3 The record to find the particular transaction'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 查找特定交易的记录'
- en: '#4 Necessary JOINs to get down to the individual name'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 必要的 JOIN 操作以获取单个名称'
- en: '#5 Necessary GROUP BY statements because of the SELECT DISTINCT statement'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 由于 SELECT DISTINCT 语句，必要的 GROUP BY 语句'
- en: The next CTE is used to collect the names of our suspects (listing 11.27). For
    display purposes, we handle concatenating the name together, keeping in mind the
    possibility of the `middle_name` being `NULL` with `CONCAT_WS`. Otherwise, using
    a standard `CONCAT` function will result in two blank spaces between the first
    and last names. Of course, there are ways to handle this situation if the RDMS
    we are using does not have a function such as `CONCAT_WS`.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个CTE用于收集我们的嫌疑人的名称（列表11.27）。为了显示目的，我们处理将名称连接在一起，考虑到 `middle_name` 可能是 `NULL`
    的可能性，使用 `CONCAT_WS`。否则，使用标准的 `CONCAT` 函数将在第一个和最后一个名称之间产生两个空白空间。当然，如果我们使用的RDMS没有
    `CONCAT_WS` 这样的函数，还有处理这种情况的方法。
- en: Listing 11.27  Collecting the SDN names
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.27  收集SDN名称
- en: '[PRE26]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#1 The CONCAT_WS will skip NULL fields. It works perfectly for combining the
    first, middle, and last names into something we can use for display.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 CONCAT_WS将跳过NULL字段。它非常适合将第一个、中间和最后一个名称组合成我们可以用于显示的内容。'
- en: '#2 Concatenates the names and removes any spaces'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 连接名称并删除任何空格'
- en: '#3 A similar approach for the alias'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 对于别名采用类似的方法'
- en: With the names of our customers and suspects gathered, we can now start comparing
    them. The first method we use is the function `LEVENSHTEIN` to compute the distance,
    which considers the number of additions, deletions, and updates needed to convert
    one string into another. We convert string into a number between 0 and 100, where
    0 is no match and 100 is an exact match.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 收集了我们的客户和嫌疑人的名称后，我们现在可以开始比较它们。我们使用的第一种方法是函数 `LEVENSHTEIN` 来计算距离，它考虑了将一个字符串转换为另一个字符串所需的添加、删除和更新次数。我们将字符串转换为介于0到100之间的数字，其中0表示没有匹配，100表示完全匹配。
- en: The other method is the `DAITCH_MOKOTOFF` function, used in listing 11.28\.
    This function allows phonetic matching of names, attempting to determine if names
    sound alike. There are some additional algorithms available in the `fuzzystrmatch`
    module. In addition, there are other commercial searching algorithms. In a production
    environment, we would need additional pieces of information such as the alias,
    address, and similar to reduce the number of false positives that we may create
    by simply finding a phonetic match. However, using these methods is a good starting
    point.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是列表11.28中的 `DAITCH_MOKOTOFF` 函数，该函数允许对名称进行发音匹配，试图确定名称是否听起来相似。`fuzzystrmatch`
    模块中还有一些其他算法可用。此外，还有其他商业搜索算法。在生产环境中，我们需要额外的信息，如别名、地址等，以减少我们通过简单地找到发音匹配可能产生的误报数量。然而，使用这些方法是一个好的起点。
- en: Listing 11.28  Computing similarity scores for IAT individuals
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.28  计算IAT个人的相似度得分
- en: '[PRE27]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#1 Converts the value returned by the LEVENSHTEIN function into a percentage
    that is easier to understand for end-users'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将LEVENSHTEIN函数返回的值转换为对最终用户更容易理解的百分比'
- en: '#2 Returns whether the names matched using the DAITCH_MOKOTOFF algorithm'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用DAITCH_MOKOTOFF算法返回名称是否匹配'
- en: '#3 We use CROSS JOIN so that every name is compared.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们使用CROSS JOIN以便每个名称都进行比较。'
- en: We select all results returned from both queries, as shown in the following
    listing.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择从两个查询返回的所有结果，如下所示列表。
- en: Listing 11.29  Combining the results
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.29  合并结果
- en: '[PRE28]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#1 Pulls together all our computed scoring and matches'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 汇总我们计算的所有评分和匹配项'
- en: Finally, we filter the results as shown in the following listing, adding a unique
    row number for each row being returned to act as an identifier. There are many
    ways in which to optimize this and increase the performance of the queries. However,
    the clarity of gathering the names, comparing them, combining, and then filtering
    makes the process easier to follow in this example. We will discuss some strategies
    for actually producing this report when we discuss presenting of the OFAC results.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们按照以下列表所示过滤结果，为每个返回的行添加一个唯一的行号作为标识符。有许多方法可以优化这一点并提高查询性能。然而，收集名称、比较、组合然后过滤的清晰性使得在这个例子中过程更容易理解。当我们讨论展示OFAC结果时，我们将讨论实际生成此报告的一些策略。
- en: Listing 11.30  Filtering the results
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.30  过滤结果
- en: '[PRE29]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#1 Use the ROW_NUMBER function to generate a unique id for the row.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用ROW_NUMBER函数为行生成一个唯一的ID。'
- en: '#2 Filters the results, returning only matches of a certain threshold or the
    ones that phonetically match the name or alias'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 过滤结果，仅返回符合特定阈值或与名称或别名发音匹配的匹配项'
- en: We now have a good idea of what we will be returning based on the results we
    see when working with this query. We create a way for users to access these results
    in the next section.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经根据与这个查询一起工作的结果，对我们将要返回的内容有了很好的了解。我们将在下一节中创建一种方式，让用户可以访问这些结果。
- en: 11.8 Application programming interface
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.8 应用程序编程接口
- en: 'We now have two essential pieces: the data that exists in the database and
    a way to produce desired results. We continue to follow the well-established pattern
    for creating an API, starting with a unit test and working on the necessary pieces
    to ensure it passes.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有两个关键部分：数据库中存在的数据和产生期望结果的方法。我们继续遵循创建API的既定模式，从单元测试开始，并着手必要的部分以确保其通过。
- en: 11.8.1 Unit test
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.8.1 单元测试
- en: The unit test in the following listing loads one of our previously created files.
    Here we are concentrating on loading the data from a file called ofac_elemental_resources.ach
    and validating our API. Having previously created and loaded the database with
    the same file using the dashboard (or another unit test), we already know that
    our query should return three matches. So, the unit test simply needs to ensure
    the database is clean, load the file, and grab the results.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表中的单元测试加载了我们之前创建的一个文件。在这里，我们专注于从名为 ofac_elemental_resources.ach 的文件中加载数据并验证我们的API。因为我们已经使用仪表板（或另一个单元测试）创建并加载了包含相同文件的数据库，所以我们已经知道我们的查询应该返回三个匹配项。因此，单元测试只需要确保数据库是干净的，加载文件，并获取结果。
- en: Listing 11.31  Unit test for OFAC API
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.31  OFAC API 的单元测试
- en: '[PRE30]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '#1 Ensures a host and IP address are passed with the client as our logging
    requires that'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 确保客户端传递了主机和IP地址，因为我们的日志记录需要这些信息'
- en: '#2 Ensures that we are picking the correct file'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 确保我们选择了正确的文件'
- en: '#3 Prior to making our API call, we need to clear the database and load the
    file.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 在进行我们的API调用之前，我们需要清除数据库并加载文件。'
- en: '#4 Once we have the database set up and the client ready to make a request,
    we can pull the OFAC results from the API and ensure they are correct. We are
    only doing a quick check of the number of items returned, but want to dive deeper
    into the results to ensure the expected data is returned.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 一旦我们设置了数据库并准备好客户端进行请求，我们可以从API中提取OFAC结果并确保它们是正确的。我们只进行了一次快速检查返回的项目数量，但想更深入地检查结果以确保返回了预期的数据。'
- en: Having the unit test defined allows us to continually check that our code returns
    the desired results. Let’s make sure that happens!
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了单元测试后，我们可以持续检查我们的代码是否返回了期望的结果。让我们确保这一点发生！
- en: 11.8.2 Creating the endpoint
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.8.2 创建端点
- en: We first want to create the endpoint for the test (listing 11.32). We want to
    define the path users will be accessing. This endpoint was defined within the
    fiels.py router definition, which prefixes everything with `/api/v1/files`, so
    the path parameter only shows `/ofac`. Otherwise, we are simply returning an empty
    list, which should be enough to ensure that our unit test passes the first assert
    with a `200`-response code.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先想要为测试创建端点（列表 11.32）。我们想要定义用户将要访问的路径。这个端点在 fiels.py 路由定义中定义，其中所有内容都以前缀 `/api/v1/files`
    开头，所以路径参数只显示 `/ofac`。否则，我们只是返回一个空列表，这应该足以确保我们的单元测试通过第一个断言并返回 `200`-响应代码。
- en: Listing 11.32  Barebones endpoint
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.32  基础端点
- en: '[PRE31]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#1 Defines the path and a log message'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义了路径和日志消息'
- en: '#2 Simply returns an empty list to ensure we receive a 200-response code and
    are able to pass the first assert statement in our unit test'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 简单地返回一个空列表以确保我们收到 `200`-响应代码，并能够通过单元测试中的第一个断言语句'
- en: After we have the barebones endpoint working, we can fill in the blanks with
    a return type and call an actual method to send back the results, as shown in
    the following listing.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们有了基本的端点工作后，我们可以填充空白处，指定返回类型并调用一个实际的方法来发送结果，如下面的列表所示。
- en: Listing 11.33  Updated endpoint
  id: totrans-307
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.33  更新后的端点
- en: '[PRE32]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '#1 Our API is documented and ready for users from one of our documentation
    endpoints.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们的API已文档化，并准备好从我们的文档端点供用户使用。'
- en: '#2 We have defined the type of data we intend to return and a method to create
    it.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们已经定义了我们打算返回的数据类型以及创建它的方法。'
- en: The endpoint will no longer work because of the newly added objects of O`facScan­Results`
    and `OfacSql`, which we will define next.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 由于新添加的 `O`facScan­Results` 和 `OfacSql` 对象，端点将不再工作，这些对象我们将在下一节中定义。
- en: 11.8.3 Finishing the API
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.8.3 完成API
- en: With the API and the SQL to power it already defined, we only need to complete
    two housekeeping steps to finish the API and have a passing unit test. First,
    we want to define the `OfacScanResults`, as shown in the following listing. This
    is the natural progression from the data returned in our `SQL` query to its Pydantic
    equivalent.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在API和为其提供动力的SQL已经定义好的情况下，我们只需要完成两个维护步骤来完善API并使其通过单元测试。首先，我们希望定义`OfacScanResults`，如下所示。这是从我们的`SQL`查询返回的数据到其Pydantic等价的自然发展。
- en: Listing 11.34  The `OfacScanResults` source
  id: totrans-314
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.34  `OfacScanResults`源
- en: '[PRE33]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '#1 With these fields, users can jump directly to the file or batch that contained
    the match for further research.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用这些字段，用户可以直接跳转到包含匹配项的文件或批次，进行进一步研究。'
- en: '#2 The name/alias of the suspect and the name it matched from the ACH file'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 疑似者的名称/别名以及它从ACH文件中匹配的名称'
- en: '#3 The numeric score that was computed. For now, it is the Levenshtein score
    but could potentially reflect a score computed from a more complicated algorithm.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 计算出的数值分数。目前，它是Levenshtein分数，但可能最终会反映一个更复杂算法计算出的分数。'
- en: '#4 These Boolean values are intended to show a check mark or some other indicator
    that the record was matched by this method as well.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 这些布尔值旨在显示一个勾选标记或其他指示符，表明该记录也被此方法匹配。'
- en: With a place to store the results of our query, all that is needed is to create
    the `OfacSql` class and the associated `get_scan_results` method (listing 11.35).
    This will execute the SQL we created earlier, so this should be straightforward.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储我们查询结果的地方，所需做的只是创建`OfacSql`类及其相关的`get_scan_results`方法（列表11.35）。这将执行我们之前创建的SQL语句，因此这应该是直截了当的。
- en: Listing 11.35  The `OfacSql` class
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.35  `OfacSql`类
- en: '[PRE34]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '#1 The SQL query we designed previously'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们之前设计的SQL查询'
- en: '#2 Gets all the results from the query'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 获取查询的所有结果'
- en: '#3 Returns the results'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 返回结果'
- en: At this point, the API should is functional, and we should have a passing unit
    test. We should also circle back and update our unit test to ensure that we are
    diving into the results and validating some of the fields as we were originally
    checking the size of an array. In theory, any endpoint that returned three items
    would pass that test. Next, we look at the final piece of the puzzle—presenting
    of our results.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，API应该是功能性的，我们应该有一个通过单元测试。我们还应该回顾并更新我们的单元测试，以确保我们正在深入结果并验证一些字段，就像我们最初检查数组大小一样。理论上，任何返回三个项目的端点都会通过那个测试。接下来，我们看看拼图的最后一块——展示我们的结果。
- en: 11.9 User interface
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.9 用户界面
- en: With the data and the ability to retrieve it in place, the final step is to
    present the information to the user. Staying with our general approach, we create
    a page responsible for making an API call, the outcome of which is passed down
    to a component that will display formatted results. Figure 11.6 shows the result
    of pulling a sample report. The report provides the name of the suspect, their
    alias, and the customer’s name. We also include the score (how close of a match)
    and whether the system matched the name or alias. There are also options to find
    the file or the batch that include the suspected customer.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据和检索它的能力到位之后，最后一步是将信息展示给用户。继续我们的通用方法，我们创建一个负责发起API调用并传递其结果的页面，该结果被传递到一个将显示格式化结果的组件。图11.6显示了拉取样本报告的结果。报告提供了嫌疑人的姓名、他们的别名以及客户的姓名。我们还包括了分数（匹配的接近程度）以及系统是否匹配了名称或别名。还有选项查找包含疑似客户的文件或批次。
- en: '![A screenshot of a computer  Description automatically generated](../Images/CH11_F06_Kardell.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](../Images/CH11_F06_Kardell.png)'
- en: Figure 11.6  Sample OFAC report screen
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.6  样本OFAC报告屏幕
- en: 11.9.1 The OFAC page
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.9.1 OFAC页面
- en: As shown in the following listing, we start by creating the main page and retrieving
    the data from the API. We could start by creating a placeholder page like in previous
    sections and ensuring that we can navigate to it before we try to populate the
    data. In this instance, since we have used similar approaches for exceptions (chapter
    9) and company information (chapter 10), we jump right to building the page. The
    IDE should complain because we do not have an `OfacResponse` or `OfacRecords`
    component, but we will tackle those next.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，我们首先创建主页面并从API检索数据。我们本可以像前几节那样创建一个占位符页面，并确保在尝试填充数据之前可以导航到它。在这种情况下，因为我们已经使用了类似的方法来处理异常（第9章）和公司信息（第10章），所以我们直接进入构建页面。IDE应该会抱怨我们没有`OfacResponse`或`OfacRecords`组件，但我们将接下来解决这些问题。
- en: Listing 11.36  Sample page and API call
  id: totrans-333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.36  示例页面和API调用
- en: '[PRE35]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '#1 Uses axios to make the API request for an OFAC scan'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用axios进行OFAC扫描的API请求'
- en: '#2 Passes the results to our component to display them nicely for the user'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将结果传递给我们的组件，以便以良好的方式展示给用户'
- en: As mentioned, this has become somewhat of a boilerplate process. Depending on
    the task at hand and how many pages need to be created, we may sometimes create
    those placeholders just to get a feel for the navigation and flow. Or we may choose
    to add pages as we begin working. We should be flexible in our approach.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这已成为一种相当标准的流程。根据手头的任务和需要创建的页面数量，我们有时会创建那些占位符，只是为了感受导航和流程。或者我们可能选择在开始工作时添加页面。我们应该在方法上保持灵活。
- en: 11.9.2 OFAC components
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.9.2 OFAC组件
- en: We must create the `OfacResponse` to hold the API results. As we have worked
    with generative AI, we found that it can really help improve our productivity
    by generating some of this boilerplate code. We may use it to generate a Pydantic
    definition from a SQL `CREATE` `TABLE` statement. Then, we generate the TypeScript
    interface from the Pydantic class, which usually does a good job of capturing
    the needed fields. The following listing shows the `OfacResponse` interface.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须创建`OfacResponse`来保存API结果。正如我们与生成式AI合作时发现的那样，它确实可以通过生成一些标准代码来提高我们的生产力。我们可以用它从SQL
    `CREATE` `TABLE`语句生成Pydantic定义。然后，我们从Pydantic类生成TypeScript接口，这通常能很好地捕捉到所需字段。以下列表显示了`OfacResponse`接口。
- en: Listing 11.37  The `OfacResponse` interface
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.37  `OfacResponse` 接口
- en: '[PRE36]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We now look at creating the component itself using a MUI DataGrid like we did
    for previous components in our dashboard.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看如何创建组件本身，就像我们在仪表板中创建之前的组件一样，使用MUI DataGrid。
- en: Listing 11.38  The `OfacRecords` component
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.38  `OfacRecords` 组件
- en: '[PRE37]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '#1 The ach_files_id is used to create a link users can click to navigate to
    the file where the match was.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用ach_files_id创建用户可以点击以导航到匹配项所在文件的链接。'
- en: '#2 To navigate to the batch that has the match, we need to pull the ach_files_id
    from the row object, as well as the params.value, although we could have also
    used the params.row.ach_batch_id field.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 要导航到具有匹配项的批次，我们需要从行对象中提取ach_files_id以及params.value，尽管我们也可以使用params.row.ach_batch_id字段。'
- en: '#3 Displays the similarity score; we use Math.floor to remove any decimals
    just as a display preference.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 显示相似度得分；我们使用Math.floor来去除任何小数，仅作为显示偏好。'
- en: '#4 Based on the Boolean flag that was passed, displays a green circle with
    a check mark or nothing'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 根据传递的布尔标志，显示带有勾选标记的绿色圆圈或无任何内容'
- en: '#5 Based on the Boolean flag that was passed, displays a green circle with
    a check mark or nothing'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 根据传递的布尔标志，显示带有勾选标记的绿色圆圈或无任何内容'
- en: '#6 When the alias was matched, displays a green circle with a check mark'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 当别名匹配时，显示带有勾选标记的绿色圆圈'
- en: '#7 The columns and rows we defined are passed to the DataGrid componenet.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 我们定义的列和行被传递给DataGrid组件。'
- en: With the component created, we should be able to pull the results of an OFAC
    scan and display them in our component. That should be sufficient to meet the
    requirements for the story. In the next section, we wrap up the OFAC searching
    by outlining some potential problems.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 组件创建后，我们应该能够提取OFAC扫描的结果，并在我们的组件中显示它们。这应该足以满足故事的需求。在下一节中，我们将概述一些潜在的问题，以总结OFAC搜索。
- en: 11.9.3 OFAC results
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.9.3 OFAC结果
- en: We now have a component that can present the results of an OFAC scan and have
    met the requirements for our story. This will more than likely suffice for the
    short term, but in a production environment, we will have to address some of the
    design choices made.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个可以展示OFAC扫描结果的组件，并且已经满足了我们的故事需求。这很可能会在短期内足够使用，但在生产环境中，我们必须解决一些设计选择问题。
- en: First, the scan is done dynamically whenever the page is loaded. While this
    works great for limited data, the page will likely slow to a crawl in a production
    environment. The SDN list contains over 15,000 records, and each will be scanned
    for every ACH transaction we have. Remember that there is a large volume of payments
    flowing through the ACH system (currently over 31 billion payments per year),
    and while we will not be required to scan them all, apparently, it may result
    in a lot of scanning. The situation will also be exacerbated when we have multiple
    users trying to view the reports, since each page requests results in another
    scan.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，扫描是在页面加载时动态进行的。虽然这对于有限的数据效果很好，但在生产环境中，页面可能会慢到几乎无法移动。SDN 列表包含超过 15,000 条记录，并且每条记录都会在我们进行的每笔
    ACH 交易中被扫描。记住，有大量支付通过 ACH 系统流动（目前每年超过 3100 亿次支付），虽然我们不需要扫描所有这些支付，但显然这可能会导致大量的扫描。当我们有多个用户试图查看报告时，这种情况会进一步恶化，因为每一页的请求都会导致另一次扫描。
- en: Second, there is no way to save or export our results. Because the scanning
    is done upon request (when a user visits the page) using the data currently stored
    in the database, we do not have a way to pull previous scan results. The SDN and
    other lists provided by OFAC are revised constantly with individuals being added,
    removed, and updated. The scans we do today may yield different results tomorrow,
    which can be a compliance nightmare in the current setup. If the SDN list changes,
    and we can no longer show that a particular individual was matched on a certain
    date or transaction, we may find ourselves facing penalties, as outlined at the
    beginning of the chapter.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们无法保存或导出我们的结果。因为扫描是在请求时进行的（当用户访问页面时）使用数据库中当前存储的数据，我们没有方法来拉取之前的扫描结果。OFAC
    提供的 SDN 和其他列表会不断修订，个人会被添加、删除和更新。我们今天进行的扫描可能明天会产生不同的结果，这在当前设置中可能成为合规的噩梦。如果 SDN
    列表发生变化，我们无法再证明某个特定个人在某个日期或交易中被匹配，我们可能会发现自己面临章节开头概述的处罚。
- en: Third, in production environments, scanning is typically done at the time of
    the file load in an ACH environment. The database can also be periodically scanned
    to identify potential customers and transactions that may be in violation. All
    these scans produce a copy of the results that can be reviewed and archived in
    cold storage for later retrieval as needed. Scanning at the time of the load can
    help reduce constant requests that burden the system while providing real-time
    feedback on suspicious activity. However, for large files, this operation can
    still take some time, and with our current architecture, we would likely want
    to introduce a status for the file, so that some of these tasks could be done
    asynchronously.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，在生产环境中，扫描通常在 ACH 环境中文件加载时进行。数据库也可以定期扫描以识别可能违规的潜在客户和交易。所有这些扫描都会生成一份结果副本，可以供审查并存档在冷存储中，以便在需要时检索。在加载时进行扫描可以帮助减少对系统的持续请求，同时提供对可疑活动的实时反馈。然而，对于大文件，此操作仍然可能需要一些时间，并且根据我们当前的架构，我们可能希望为文件引入一个状态，以便这些任务中的一些可以异步完成。
- en: Those are just a few of the challenges we may face with OFAC scanning, mentioning
    nothing of creating a more complex algorithm to detect suspicious activity. Although
    it presents several challenges, we can review them as opportunities to explore
    other areas of ACH, finance, compliance, and security that we need to touch on
    with this subject.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是我们可能面临的 OFAC 扫描挑战中的一部分，更不用说创建一个更复杂的算法来检测可疑活动了。尽管它提出了几个挑战，但我们可以将它们视为探索 ACH、金融、合规和安全等领域的机会，这些领域是我们需要与这个主题相关的。
- en: In this chapter, we explored a new ACH layout for international ACH transactions.
    The introduction of this new type of transaction for our dashboard also caused
    an increased risk for our organization by potentially violating US laws and regulations
    for dealing with sanctioned individuals. To remain in compliance and reduce our
    risk, we explored scanning our ACH file for customers that may be on a list of
    sanctioned individuals provided by OFAC. To accomplish all this, we enhanced the
    parsing of our ACH files, performed fuzzy matching on customer names, provided
    an API to power it all, and presented the results in a UI component.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了国际ACH交易的新ACH布局。这种新类型交易在我们的仪表板中引入的同时，也可能使我们的组织面临风险，因为可能违反了美国关于处理受制裁个人的法律和法规。为了保持合规并降低风险，我们探讨了扫描我们的ACH文件，寻找可能出现在OFAC提供的受制裁个人名单上的客户。为了完成所有这些，我们增强了我们的ACH文件解析，对客户姓名进行了模糊匹配，提供了一个API来支持这一切，并在UI组件中展示了结果。
- en: Summary
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: International ACH transactions (IAT) introduce unique regulatory requirements
    and require precise formatting, which underscores the need for careful planning
    and compliance verification in financial systems.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国际ACH交易（IAT）引入了独特的监管要求，需要精确的格式，这强调了在金融系统中进行仔细规划和合规验证的必要性。
- en: Expanding database structures to accommodate IAT involves integrating new tables
    tailored for specific ACH components, reinforcing the importance of planning for
    system scalability.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展数据库结构以适应IAT需要整合针对特定ACH组件定制的新的表格，这强调了系统可扩展性规划的重要性。
- en: Generating IAT batches requires adapting current solutions to handle additional
    header and entry information, highlighting the complexity of international transaction
    standards.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成IAT批处理需要调整现有解决方案以处理额外的头信息和条目信息，突出了国际交易标准的复杂性。
- en: Comprehensive unit testing ensures new IAT functionalities align with existing
    PPD processes, while preventing disruptions, showcasing the importance of regression
    testing in software development.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 综合单元测试确保新的IAT功能与现有的PPD流程保持一致，同时防止中断，展示了回归测试在软件开发中的重要性。
- en: Streamlining code through refactoring enhances maintainability and reduces redundancy,
    indicating the continual need for codebase optimization as projects evolve.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过重构简化代码提高了可维护性并减少了冗余，这表明随着项目的演变，持续进行代码库优化是必要的。
- en: Integrating IAT processing in dashboards requires understanding both technical
    constraints and user needs, which emphasizes cross-functional alignment in feature
    development.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在仪表板中集成IAT处理需要理解技术限制和用户需求，这强调了在功能开发中的跨职能一致性。
- en: Designing APIs to handle IAT data ensures consistent data retrieval and processing,
    demonstrating the necessity for flexible yet robust system interfaces.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计用于处理IAT数据的API确保了数据检索和处理的连续性，展示了灵活且健壮的系统接口的必要性。
- en: OFAC compliance is crucial when processing IAT to mitigate risks associated
    with sanctioned individuals, which highlights the role of regulatory adherence
    in transaction processing.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理IAT时，OFAC合规性至关重要，以减轻与受制裁个人相关的风险，这突出了在交易处理中遵守监管规定的作用。
- en: Implementing fuzzy matching algorithms aids in detecting minor variations in
    suspect names, thus increasing accuracy in identifying sanctioned transactions.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施模糊匹配算法有助于检测嫌疑人姓名的细微变化，从而提高识别受制裁交易的准确性。
- en: Effective UI components for OFAC results delivery underscore the value of clear,
    actionable insights for users navigating complex compliance landscapes.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OFAC结果交付的有效UI组件强调了用户在复杂的合规环境中导航时清晰、可操作的见解的价值。
- en: Persisting scan results addresses compliance demands by providing traceable
    audit trails, which stresses the need for reliable data storage and retrieval
    in financial applications.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续扫描结果通过提供可追溯的审计轨迹来满足合规要求，这强调了在金融应用中可靠数据存储和检索的需求。
- en: The addition of IAT processing elevates organizational risk awareness, which
    underscores the critical need for stringent compliance frameworks to navigate
    international regulations effectively.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加IAT处理提高了组织对风险的认识，这强调了制定严格的合规框架以有效应对国际法规的迫切需要。
