- en: '7 Working with Keras: A deep dive'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 使用 Keras：深入探讨
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Creating Keras models with the `Sequential` class, the Functional API, and model
    subclassing
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `Sequential` 类、功能 API 和模型子类创建 Keras 模型
- en: Using built-in Keras training and evaluation loops
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置的 Keras 训练和评估循环
- en: Using Keras callbacks to customize training
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 回调函数自定义训练
- en: Using TensorBoard to monitor training and evaluation metrics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorBoard 监控训练和评估指标
- en: Writing training and evaluation loops from scratch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始编写训练和评估循环
- en: You’ve now got some experience with Keras—you’re familiar with the Sequential
    model, `Dense` layers, and built-in APIs for training, evaluation, and inference—`compile()`,
    `fit()`, `evaluate()`, and `predict()`. You even learned in chapter 3 how to inherit
    from the `Layer` class to create custom layers, and how to use the TensorFlow
    `GradientTape` to implement a step-by-step training loop.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在对 Keras 有了一些经验——您熟悉 Sequential 模型、Dense 层以及用于训练、评估和推断的内置 API——`compile()`、`fit()`、`evaluate()`
    和 `predict()`。您甚至在第 3 章中学习了如何从 Layer 类继承以创建自定义层，以及如何使用 TensorFlow 的 GradientTape
    实现逐步训练循环。
- en: 'In the coming chapters, we’ll dig into computer vision, timeseries forecasting,
    natural language processing, and generative deep learning. These complex applications
    will require much more than a `Sequential` architecture and the default `fit()`
    loop. So let’s first turn you into a Keras expert! In this chapter, you’ll get
    a complete overview of the key ways to work with Keras APIs: everything you’re
    going to need to handle the advanced deep learning use cases you’ll encounter
    next.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入研究计算机视觉、时间序列预测、自然语言处理和生成式深度学习。这些复杂的应用将需要比 `Sequential` 架构和默认的
    `fit()` 循环更多的内容。所以让我们首先把你变成一个 Keras 专家！在本章中，您将全面了解如何使用 Keras API：这是您将需要处理下一个遇到的高级深度学习用例的关键方法。
- en: 7.1 A spectrum of workflows
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 一系列工作流程
- en: 'The design of the Keras API is guided by the principle of *progressive disclosure
    of complexity*: make it easy to get started, yet make it possible to handle high-complexity
    use cases, only requiring incremental learning at each step. Simple use cases
    should be easy and approachable, and arbitrarily advanced workflows should be
    *possible*: no matter how niche and complex the thing you want to do, there should
    be a clear path to it. A path that builds upon the various things you’ve learned
    from simpler workflows. This means that you can grow from beginner to expert and
    still use the same tools—only in different ways.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Keras API 的设计遵循“逐步揭示复杂性”的原则：使入门变得容易，同时使处理高复杂性用例成为可能，只需要在每一步进行增量学习。简单用例应该易于接近，任意高级工作流程应该是*可能的*：无论您想做多么小众和复杂的事情，都应该有一条明确的路径。这条路径建立在您从更简单工作流程中学到的各种东西之上。这意味着您可以从初学者成长为专家，仍然可以以不同的方式使用相同的工具。
- en: As such, there’s not a single “true” way of using Keras. Rather, Keras offers
    a *spectrum of workflows*, from the very simple to the very flexible. There are
    different ways to build Keras models, and different ways to train them, answering
    different needs. Because all these workflows are based on shared APIs, such as
    `Layer` and `Model`, components from any workflow can be used in any other workflow—they
    can all talk to each other.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，并没有一种“真正”的使用 Keras 的方式。相反，Keras 提供了一系列工作流程，从非常简单到非常灵活。有不同的构建 Keras 模型的方式，以及不同的训练方式，满足不同的需求。因为所有这些工作流程都基于共享的
    API，如 `Layer` 和 `Model`，所以任何工作流程的组件都可以在任何其他工作流程中使用——它们可以相互通信。
- en: 7.2 Different ways to build Keras models
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 构建 Keras 模型的不同方式
- en: 'There are three APIs for building models in Keras (see figure 7.1):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 有三种构建模型的 API（见图 7.1）：
- en: The *Sequential model*, the most approachable API—it’s basically a Python list.
    As such, it’s limited to simple stacks of layers.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sequential 模型*，最易接近的 API——基本上就是一个 Python 列表。因此，它仅限于简单的层堆叠。'
- en: The *Functional API*, which focuses on graph-like model architectures. It represents
    a nice mid-point between usability and flexibility, and as such, it’s the most
    commonly used model-building API.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*功能 API* 专注于类似图形的模型架构。它在可用性和灵活性之间找到了一个很好的中间点，因此它是最常用的模型构建 API。'
- en: '*Model subclassing*, a low-level option where you write everything yourself
    from scratch. This is ideal if you want full control over every little thing.
    However, you won’t get access to many built-in Keras features, and you will be
    more at risk of making mistakes.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型子类化*，一种低级选项，您可以从头开始编写所有内容。如果您想要对每一点都有完全控制，这是理想的选择。但是，您将无法访问许多内置的 Keras 功能，并且更容易出错。'
- en: '![](../Images/07-01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-01.png)'
- en: Figure 7.1 Progressive disclosure of complexity for model building
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 逐步揭示模型构建的复杂性
- en: 7.2.1 The Sequential model
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 Sequential 模型
- en: The simplest way to build a Keras model is to use the Sequential model, which
    you already know about.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 Keras 模型的最简单方法是使用已知的 Sequential 模型。
- en: Listing 7.1 The `Sequential` class
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 `Sequential` 类
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that it’s possible to build the same model incrementally via the `add()`
    method, which is similar to the `append()` method of a Python list.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，可以通过 `add()` 方法逐步构建相同的模型，这类似于 Python 列表的 `append()` 方法。
- en: Listing 7.2 Incrementally building a Sequential model
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 逐步构建一个顺序模型
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You saw in chapter 4 that layers only get built (which is to say, create their
    weights) when they are called for the first time. That’s because the shape of
    the layers'' weights depends on the shape of their input: until the input shape
    is known, they can’t be created.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章中，您看到层只有在第一次调用它们时才会构建（也就是说，创建它们的权重）。这是因为层的权重形状取决于它们的输入形状：在输入形状未知之前，它们无法被创建。
- en: As such, the preceding Sequential model does not have any weights (listing 7.3)
    until you actually call it on some data, or call its `build()` method with an
    input shape (listing 7.4).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前面的 Sequential 模型没有任何权重（列表 7.3），直到您实际在一些数据上调用它，或者使用输入形状调用其 `build()` 方法（列表
    7.4）。
- en: Listing 7.3 Models that aren’t yet built have no weights
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.3 尚未构建的模型没有权重
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ At that point, the model isn’t built yet.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在那时，模型尚未构建。
- en: Listing 7.4 Calling a model for the first time to build it
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.4 第一次调用模型以构建它
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Builds the model—now the model will expect samples of shape (3,). The None
    in the input shape signals that the batch size could be anything.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建模型 - 现在模型将期望形状为(3,)的样本。输入形状中的None表示批次大小可以是任意值。
- en: ❷ Now you can retrieve the model’s weights.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 现在你可以检索模型的权重。
- en: After the model is built, you can display its contents via the `summary()` method,
    which comes in handy for debugging.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建完成后，你可以通过`summary()`方法显示其内容，这对调试很有帮助。
- en: Listing 7.5 The `summary()` method
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5 `summary()`方法
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, this model happens to be named “sequential_1.” You can give
    names to everything in Keras—every model, every layer.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个模型恰好被��名为“sequential_1”。你可以为Keras中的所有内容命名 - 每个模型，每个层。
- en: Listing 7.6 Naming models and layers with the `name` argument
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6 使用`name`参数为模型和层命名
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When building a Sequential model incrementally, it’s useful to be able to print
    a summary of what the current model looks like after you add each layer. But you
    can’t print a summary until the model is built! There’s actually a way to have
    your `Sequential` built on the fly: just declare the shape of the model’s inputs
    in advance. You can do this via the `Input` class.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步构建Sequential模型时，能够在添加每个层后打印当前模型的摘要非常有用。但在构建模型之前无法打印摘要！实际上，有一种方法可以让你的`Sequential`动态构建：只需提前声明模型输入的形状即可。你可以通过`Input`类实现这一点。
- en: Listing 7.7 Specifying the input shape of your model in advance
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.7 预先指定模型的输入形状
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Use Input to declare the shape of the inputs. Note that the shape argument
    must be the shape of each sample, not the shape of one batch.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用Input声明输入的形状。请注意，shape参数必须是每个样本的形状，而不是一个批次的形状。
- en: 'Now you can use `summary()` to follow how the output shape of your model changes
    as you add more layers:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以使用`summary()`来跟踪模型输出形状随着添加更多层而变化的情况：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a pretty common debugging workflow when dealing with layers that transform
    their inputs in complex ways, such as the convolutional layers you’ll learn about
    in chapter 8\.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 处理转换输入的层（如第8章中将学习的卷积层）时，这是一个相当常见的调试工作流程。
- en: 7.2.2 The Functional API
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 功能API
- en: 'The Sequential model is easy to use, but its applicability is extremely limited:
    it can only express models with a single input and a single output, applying one
    layer after the other in a sequential fashion. In practice, it’s pretty common
    to encounter models with multiple inputs (say, an image and its metadata), multiple
    outputs (different things you want to predict about the data), or a nonlinear
    topology.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Sequential模型易于使用，但其适用性极为有限：它只能表达具有单个输入和单个输出的模型，按顺序一个接一个地应用各个层。实际上，很常见遇到具有多个输入（例如图像及其元数据）、多个输出（关于数据的不同预测）或非线性拓扑的模型。
- en: In such cases, you’d build your model using the Functional API. This is what
    most Keras models you’ll encounter in the wild use. It’s fun and powerful—it feels
    like playing with LEGO bricks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你将使用功能API构建模型。这是你在实际应用中遇到的大多数Keras模型所使用的方法。它既有趣又强大，感觉就像玩乐高积木一样。
- en: A simple example
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的例子
- en: 'Let’s start with something simple: the stack of two layers we used in the previous
    section. Its Functional API version looks like the following listing.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些简单的东西开始：我们在上一节中使用的两个层的堆叠。其功能API版本如下列表所示。
- en: Listing 7.8 A simple Functional model with two `Dense` layers
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.8 具有两个`Dense`层的简单功能模型
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s go over this step by step.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地过一遍这个过程。
- en: 'We started by declaring an `Input` (note that you can also give names to these
    input objects, like everything else):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先声明了一个`Input`（请注意，你也可以为这些输入对象命名，就像其他所有内容一样）：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This `inputs` object holds information about the shape and dtype of the data
    that the model will process:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`inputs`对象保存了关于模型将处理的数据形状和dtype的信息：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ The model will process batches where each sample has shape (3,). The number
    of samples per batch is variable (indicated by the None batch size).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 模型将处理每个样本形状为(3,)的批次。每批次的样本数量是可变的（由None批次大小表示）。
- en: ❷ These batches will have dtype float32.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这些批次将具有dtype float32。
- en: We call such an object a *symbolic tensor*. It doesn’t contain any actual data,
    but it encodes the specifications of the actual tensors of data that the model
    will see when you use it. It *stands for* future tensors of data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这样的对象为*符号张量*。它不包含任何实际数据，但它编码了模型在使用时将看到的实际数据张量的规格。它*代表*未来的数据张量。
- en: 'Next, we created a layer and called it on the input:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建了一个层并在输入上调用它：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'All Keras layers can be called both on real tensors of data and on these symbolic
    tensors. In the latter case, they return a new symbolic tensor, with updated shape
    and dtype information:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Keras层都可以在实际数据张量和这些符号张量上调用。在后一种情况下，它们将返回一个新的符号张量，带有更新的形状和dtype信息：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After obtaining the final outputs, we instantiated the model by specifying
    its inputs and outputs in the `Model` constructor:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得最终输出后，我们通过在`Model`构造函数中指定其输入和输出来实例化模型：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here’s the summary of our model:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们模型的摘要：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Multi-input, multi-output models
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 多输入，多输出模型
- en: Unlike this toy model, most deep learning models don’t look like lists—they
    look like graphs. They may, for instance, have multiple inputs or multiple outputs.
    It’s for this kind of model that the Functional API really shines.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与这个玩具模型不同，大多数深度学习模型看起来不像列表，而更像图形。例如，它们可能具有多个输入或多个输出。正是对于这种类型的模型，功能API真正发挥作用。
- en: 'Let’s say you’re building a system to rank customer support tickets by priority
    and route them to the appropriate department. Your model has three inputs:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在构建一个系统，根据优先级对客户支持票据进行排名并将其路由到适当的部门。你的模型有三个输入：
- en: The title of the ticket (text input)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 票据的标题（文本输入）
- en: The text body of the ticket (text input)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 票据的文本主体（文本输入）
- en: Any tags added by the user (categorical input, assumed here to be one-hot encoded)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户添加的任何标签（假定为独热编码的分类输入）
- en: We can encode the text inputs as arrays of ones and zeros of size `vocabulary_size`
    (see chapter 11 for detailed information about text encoding techniques).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将文本输入编码为大小为`vocabulary_size`的一维数组（有关文本编码技术的详细信息，请参阅第 11 章）。
- en: 'Your model also has two outputs:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型还有两个输出：
- en: The priority score of the ticket, a scalar between 0 and 1 (sigmoid output)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 票证的优先级分数，介于 0 和 1 之间的标量（sigmoid 输出）
- en: The department that should handle the ticket (a softmax over the set of departments)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应处理票证的部门（对部门集合进行 softmax）
- en: You can build this model in a few lines with the Functional API.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用几行代码使用函数式 API 构建此模型。
- en: Listing 7.9 A multi-input, multi-output Functional model
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.9 多输入、多输出函数式模型
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Define model inputs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义模型输入。
- en: ❷ Combine input features into a single tensor, features, by concatenating them.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过将它们连接起来，将输入特征组合成一个张量 features。
- en: ❸ Apply an intermediate layer to recombine input features into richer representations.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 应用中间层以将输入特征重新组合为更丰富的表示。
- en: ❹ Define model outputs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义模型输出。
- en: ❺ Create the model by specifying its inputs and outputs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通过指定其输入和输出来创建模型。
- en: The Functional API is a simple, LEGO-like, yet very flexible way to define arbitrary
    graphs of layers like these.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式 API 是一种简单、类似于乐高的、但非常灵活的方式，用于定义这样的层图。
- en: Training a multi-input, multi-output model
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 训练多输入、多输出模型
- en: You can train your model in much the same way as you would train a Sequential
    model, by calling `fit()` with lists of input and output data. These lists of
    data should be in the same order as the inputs you passed to the `Model` constructor.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像训练序贯模型一样训练模型，通过使用输入和输出数据的列表调用`fit()`。这些数据列表应与传递给`Model`构造函数的输入顺序相同。
- en: Listing 7.10 Training a model by providing lists of input and target arrays
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.10 通过提供输入和目标数组列表来训练模型
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Dummy input data
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 虚拟输入数据
- en: ❷ Dummy target data
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 虚拟目标数据
- en: If you don’t want to rely on input order (for instance, because you have many
    inputs or outputs), you can also leverage the names you gave to the `Input` objects
    and the output layers, and pass data via dictionaries.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想依赖输入顺序（例如，因为您有许多输入或输出），您还可以利用给`Input`对象和输出层命名的名称，并通过字典传递数据。
- en: Listing 7.11 Training a model by providing dicts of input and target arrays
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.11 通过提供输入和目标数组的字典来训练模型
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The power of the Functional API: Access to layer connectivity'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式 API 的强大之处：访问层连接性
- en: 'A Functional model is an explicit graph data structure. This makes it possible
    to inspect how layers are connected and reuse previous graph nodes (which are
    layer outputs) as part of new models. It also nicely fits the “mental model” that
    most researchers use when thinking about a deep neural network: a graph of layers.
    This enables two important use cases: model visualization and feature extraction.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式模型是一种显式的图数据结构。这使得可以检查层如何连接并重用先前的图节点（即层输出）作为新模型的一部分。它还很好地适应了大多数研究人员在思考深度神经网络时使用的“心智模型”：层的图。这使得两个重要用例成为可能：模型可视化和特征提取。
- en: Let’s visualize the connectivity of the model we just defined (the *topology*
    of the model). You can plot a Functional model as a graph with the `plot_model()`
    utility (see figure 7.2).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化我们刚刚定义的模型的连接性（模型的*拓扑结构*）。您可以使用`plot_model()`实用程序将函数式模型绘制为图形（参见图 7.2）。
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/07-02.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-02.png)'
- en: Figure 7.2 Plot generated by `plot_model()` on our ticket classifier model
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 由`plot_model()`在我们的票证分类器模型上生成的图
- en: You can add to this plot the input and output shapes of each layer in the model,
    which can be helpful during debugging (see figure 7.3).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此图中添加模型中每个层的输入和输出形状，这在调试过程中可能会有所帮助（参见图 7.3）。
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/07-03.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-03.png)'
- en: Figure 7.3 Model plot with shape information added
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 添加形状信息的模型图
- en: 'The “None” in the tensor shapes represents the batch size: this model allows
    batches of any size.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 张量形状中的“None”表示批处理大小：此模型允许任意大小的批处理。
- en: Access to layer connectivity also means that you can inspect and reuse individual
    nodes (layer calls) in the graph. The `model.layers` model property provides the
    list of layers that make up the model, and for each layer you can query `layer.input`
    and `layer.output`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 访问层连接性还意味着您可以检查和重用图中的单个节点（层调用）。`model.layers` 模型属性提供组成模型的层列表，对于每个层，您可以查询`layer.input`
    和`layer.output`。
- en: Listing 7.12 Retrieving the inputs or outputs of a layer in a Functional model
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.12 检索函数式模型中层���输入或输出
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This enables you to do *feature extraction*, creating models that reuse intermediate
    features from another model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这使您能够进行*特征提取*，创建重用另一个模型中间特征的模型。
- en: 'Let’s say you want to add another output to the previous model—you want to
    estimate how long a given issue ticket will take to resolve, a kind of difficulty
    rating. You could do this via a classification layer over three categories: “quick,”
    “medium,” and “difficult.” You don’t need to recreate and retrain a model from
    scratch. You can start from the intermediate features of your previous model,
    since you have access to them, like this.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要向先前的模型添加另一个输出—您想要估计给定问题票证解决所需时间，一种难度评级。您可以通过三个类别的分类层来实现这一点：“快速”、“中等”和“困难”。您无需从头开始重新创建和重新训练模型。您可以从先前模型的中间特征开始，因为您可以访问它们，就像这样。
- en: Listing 7.13 Creating a new model by reusing intermediate layer outputs
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.13 通过重用中间层输出创建新模型
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ layers[4] is our intermediate Dense layer
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 层[4] 是我们的中间密集层
- en: 'Let’s plot our new model (see figure 7.4):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制我们的新模型（参见图 7.4）：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/07-04.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-04.png)'
- en: Figure 7.4 Plot of our new model
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 我们新模型的绘图
- en: 7.2.3 Subclassing the Model class
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 继承 Model 类
- en: 'The last model-building pattern you should know about is the most advanced
    one: `Model` subclassing. You learned in chapter 3 how to subclass the `Layer`
    class to create custom layers. Subclassing `Model` is pretty similar:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该了解的最后一个模型构建模式是最高级的一个：`Model`子类化。你在第3章学习了如何子类化`Layer`类来创建自定义层。子类化`Model`与此类似：
- en: In the `__init__()` method, define the layers the model will use.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ���`__init__()`方法中，定义模型将使用的层。
- en: In the `call()` method, define the forward pass of the model, reusing the layers
    previously created.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`call()`方法中，定义模型的前向传递，重用先前创建的层。
- en: Instantiate your subclass, and call it on data to create its weights.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化你的子类，并在数据上调用它以创建其权重。
- en: Rewriting our previous example as a subclassed model
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们之前的例子重写为一个子类模型
- en: 'Let’s take a look at a simple example: we will reimplement the customer support
    ticket management model using a `Model` subclass.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子：我们将使用`Model`子类重新实现客户支持票务管理模型。
- en: Listing 7.14 A simple subclassed model
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 一个简单的子类模型
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Don’t forget to call the super() constructor!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 不要忘记调用super()构造函数！
- en: ❷ Define sublayers in the constructor.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在构造函数中定义子层。
- en: ❸ Define the forward pass in the call() method.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在call()方法中定义前向传递。
- en: 'Once you’ve defined the model, you can instantiate it. Note that it will only
    create its weights the first time you call it on some data, much like `Layer`
    subclasses:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你定义了模型，你可以实例化它。请注意，它只会在第一次在一些数据上调用它时创建它的权重，就像`Layer`子类一样：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'So far, everything looks very similar to `Layer` subclassing, a workflow you
    encountered in chapter 3\. What, then, is the difference between a `Layer` subclass
    and a `Model` subclass? It’s simple: a “layer” is a building block you use to
    create models, and a “model” is the top-level object that you will actually train,
    export for inference, etc. In short, a `Model` has `fit()`, `evaluate()`, and
    `predict()` methods. Layers don’t. Other than that, the two classes are virtually
    identical. (Another difference is that you can *save* a model to a file on disk,
    which we will cover in a few sections.)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切看起来与`Layer`子类化非常相似，这是你在第3章遇到的工作流程。那么，`Layer`子类和`Model`子类之间的区别是什么呢？很简单：一个“层”是你用来创建模型的构建块，而一个“模型”是你实际上将要训练、导出用于推断等的顶层对象。简而言之，一个`Model`有`fit()`、`evaluate()`和`predict()`方法。层没有。除此之外，这两个类几乎是相同的。（另一个区别是你可以*保存*模型到磁盘上的文件中，我们将在几节中介绍。）
- en: 'You can compile and train a `Model` subclass just like a Sequential or Functional
    model:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像编译和训练Sequential或Functional模型一样编译和训练`Model`子类：
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ The structure of what you pass as the loss and metrics arguments must match
    exactly what gets returned by call()—here, a list of two elements.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 作为损失和指标参数传递的结构必须与call()返回的完全匹配——这里是两个元素的列表。
- en: ❷ The structure of the input data must match exactly what is expected by the
    call() method—here, a dict with keys title, text_body, and tags.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输入数据的结构必须与call()方法所期望的完全匹配——这里是一个具有标题、正文和标签键的字典。
- en: ❸ The structure of the target data must match exactly what is returned by the
    call() method—here, a list of two elements.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 目标数据的结构必须与call()方法返回的完全匹配——这里是两个元素的列表。
- en: The `Model` subclassing workflow is the most flexible way to build a model.
    It enables you to build models that cannot be expressed as directed acyclic graphs
    of layers—imagine, for instance, a model where the `call()` method uses layers
    inside a `for` loop, or even calls them recursively. Anything is possible—you’re
    in charge.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`Model`子类化工作流是构建模型的最灵活方式。它使你能够构建无法表示为层的有向无环图的模型——想象一下，一个模型在`call()`方法中使用层在一个`for`循环内，甚至递归调用它们。任何事情都是可能的——你有控制权。'
- en: 'Beware: What subclassed models don’t support'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：子类模型不支持的内容
- en: 'This freedom comes at a cost: with subclassed models, you are responsible for
    more of the model logic, which means your potential error surface is much larger.
    As a result, you will have more debugging work to do. You are developing a new
    Python object, not just snapping together LEGO bricks.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自由是有代价的：对于子类模型，你需要负责更多的模型逻辑，这意味着你的潜在错误面更大。因此，你将需要更多的调试工作。你正在开发一个新的Python对象，而不仅仅是将LEGO积木拼在一起。
- en: Functional and subclassed models are also substantially different in nature.
    A Functional model is an explicit data structure—a graph of layers, which you
    can view, inspect, and modify. A subclassed model is a piece of bytecode—a Python
    class with a `call()` method that contains raw code. This is the source of the
    subclassing workflow’s flexibility—you can code up whatever functionality you
    like—but it introduces new limitations.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式模型和子类模型在本质上也有很大的不同。函数式模型是一个显式的数据结构——层的图，你可以查看、检查和修改。子类模型是一段字节码——一个带有包含原始代码的`call()`方法的Python类。这是子类化工作流程灵活性的源泉——你可以编写任何你喜欢的功能，但它也引入了新的限制。
- en: For instance, because the way layers are connected to each other is hidden inside
    the body of the `call()` method, you cannot access that information. Calling `summary()`
    will not display layer connectivity, and you cannot plot the model topology via
    `plot_model()`. Likewise, if you have a subclassed model, you cannot access the
    nodes of the graph of layers to do feature extraction because there is simply
    no graph. Once the model is instantiated, its forward pass becomes a complete
    black box.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，因为层之间的连接方式隐藏在`call()`方法的内部，你无法访问该信息。调用`summary()`不会显示层连接，并且你无法通过`plot_model()`绘制模型拓扑。同样，如果你有一个子类模型，你无法访问层图的节点进行特征提取，因为根本没有图。一旦模型被实例化，其前向传递就变成了一个完全的黑匣子。
- en: 7.2.4 Mixing and matching different components
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 混合和匹配不同的组件
- en: Crucially, choosing one of these patterns—the Sequential model, the Functional
    API, or `Model` subclassing—does not lock you out of the others. All models in
    the Keras API can smoothly interoperate with each other, whether they’re Sequential
    models, Functional models, or subclassed models written from scratch. They’re
    all part of the same spectrum of workflows.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，选择这些模式之一——Sequential 模型、Functional API 或 `Model` 子类化——不会将您排除在其他模式之外。Keras
    API 中的所有模型都可以平滑地相互操作，无论它们是 Sequential 模型、Functional 模型还是从头开始编写的子类化模型。它们都是同一系列工作流的一部分。
- en: For instance, you can use a subclassed layer or model in a Functional model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以在 Functional 模型中使用子类化层或模型。
- en: Listing 7.15 Creating a Functional model that includes a subclassed model
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.15 创建包含���类化模型的 Functional 模型
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Inversely, you can use a Functional model as part of a subclassed layer or model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 相反地，您可以将 Functional 模型用作子类化层或模型的一部分。
- en: Listing 7.16 Creating a subclassed model that includes a Functional model
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.16 创建包含 Functional 模型的子类化模型
- en: '[PRE27]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '7.2.5 Remember: Use the right tool for the job'
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.5 记住：使用合适的工具来完成工作
- en: You’ve learned about the spectrum of workflows for building Keras models, from
    the simplest workflow, the Sequential model, to the most advanced one, model subclassing.
    When should you use one over the other? Each one has its pros and cons—pick the
    one most suitable for the job at hand.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经了解了构建 Keras 模型的工作流程的范围，从最简单的工作流程 Sequential 模型到最先进的工作流程模型子类化。何时应该使用其中一个而不是另一个？每种方法都有其优缺点——选择最适合手头工作的方法。
- en: In general, the Functional API provides you with a pretty good trade-off between
    ease of use and flexibility. It also gives you direct access to layer connectivity,
    which is very powerful for use cases such as model plotting or feature extraction.
    If you *can* use the Functional API—that is, if your model can be expressed as
    a directed acyclic graph of layers—I recommend using it over model subclassing.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Functional API 为您提供了易用性和灵活性之间的很好的权衡。它还为您提供了直接访问层连接性的功能，这对于模型绘图或特征提取等用例非常强大。如果您*可以*使用
    Functional API——也就是说，如果您的模型可以表示为层的有向无环图——我建议您使用它而不是模型子类化。
- en: 'Going forward, all examples in this book will use the Functional API, simply
    because all the models we will work with are expressible as graphs of layers.
    We will, however, make frequent use of subclassed layers. In general, using Functional
    models that include subclassed layers provides the best of both worlds: high development
    flexibility while retaining the advantages of the Functional API.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 今后，本书中的所有示例都将使用 Functional API，仅因为我们将使用的所有模型都可以表示为层的图。但是，我们将经常使用子类化层。一般来说，使用包含子类化层的
    Functional 模型既具有高开发灵活性，又保留了 Functional API 的优势。
- en: 7.3 Using built-in training and evaluation loops
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 使用内置的训练和评估循环
- en: The principle of progressive disclosure of complexity—access to a spectrum of
    workflows that go from dead easy to arbitrarily flexible, one step at a time—also
    applies to model training. Keras provides you with different workflows for training
    models. They can be as simple as calling `fit()` on your data, or as advanced
    as writing a new training algorithm from scratch.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步披露复杂性的原则——从非常简单到任意灵活的工作流程的访问，一步一步——也适用于模型训练。Keras 为您提供了不同的训练模型的工作流程。它们可以简单到在数据上调用
    `fit()`，也可以高级到从头开始编写新的训练算法。
- en: You are already familiar with the `compile()`, `fit()`, `evaluate()`, `predict()`
    workflow. As a reminder, take a look at the following listing.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经熟悉了 `compile()`、`fit()`、`evaluate()`、`predict()` 的工作流程。作为提醒，请查看以下列表。
- en: 'Listing 7.17 The standard workflow: `compile()`, `fit()`, `evaluate()`, `predict()`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.17 标准工作流程：`compile()`、`fit()`、`evaluate()`、`predict()`
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Create a model (we factor this into a separate function so as to reuse it
    later).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个模型（我们将其分解为一个单独的函数，以便以后重用）。
- en: ❷ Load your data, reserving some for validation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 加载数据，保留一些用于验证。
- en: ❸ Compile the model by specifying its optimizer, the loss function to minimize,
    and the metrics to monitor.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过指定其优化器、要最小化的损失函数和要监视的指标来编译模型。
- en: ❹ Use fit() to train the model, optionally providing validation data to monitor
    performance on unseen data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 fit() 训练模型，可选择提供验证数据以监视在未见数据上的性能。
- en: ❺ Use evaluate() to compute the loss and metrics on new data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 evaluate() 在新数据上计算损失和指标。
- en: ❻ Use predict() to compute classification probabilities on new data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用 predict() 在新数据上计算分类概率。
- en: 'There are a couple of ways you can customize this simple workflow:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以自定义这个简单的工作流程：
- en: Provide your own custom metrics.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供您自己的自定义指标。
- en: Pass *callbacks* to the `fit()` method to schedule actions to be taken at specific
    points during training.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *callbacks* 传递给 `fit()` 方法以安排在训练过程中的特定时间点执行的操作。
- en: Let’s take a look at these.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些。
- en: 7.3.1 Writing your own metrics
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 编写自己的指标
- en: Metrics are key to measuring the performance of your model—in particular, to
    measuring the difference between its performance on the training data and its
    performance on the test data. Commonly used metrics for classification and regression
    are already part of the built-in `keras.metrics` module, and most of the time
    that’s what you will use. But if you’re doing anything out of the ordinary, you
    will need to be able to write your own metrics. It’s simple!
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 指标对于衡量模型性能至关重要——特别是用于衡量模型在训练数据和测试数据上性能差异的指标。用于分类和回归的常用指标已经是内置的 `keras.metrics`
    模块的一部分，大多数情况下您会使用它们。但是，如果您正在做一些与众不同的事情，您将需要能够编写自己的指标。这很简单！
- en: A Keras metric is a subclass of the `keras.metrics.Metric` class. Like layers,
    a metric has an internal state stored in TensorFlow variables. Unlike layers,
    these variables aren’t updated via backpropagation, so you have to write the state-update
    logic yourself, which happens in the `update_state()` method.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 指标是 `keras.metrics.Metric` 类的子类。像层一样，指标在 TensorFlow 变量中存储内部状态。与层不同，这些变量不会通过反向传播进行更新，因此您必须自己编写状态更新逻辑，这发生在
    `update_state()` 方法中。
- en: For example, here’s a simple custom metric that measures the root mean squared
    error (RMSE).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里有一个简单的自定义指标，用于测量均方根误差（RMSE）。
- en: Listing 7.18 Implementing a custom metric by subclassing the `Metric` class
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.18 通过子类化`Metric`类实现自定义指标
- en: '[PRE29]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Subclass the Metric class.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 子类化Metric类。
- en: ❷ Define the state variables in the constructor. Like for layers, you have access
    to the add_weight() method.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在构造函数中定义状态变量。就像对于层一样，你可以访问`add_weight()`方法。
- en: ❸ Implement the state update logic in update_state(). The y_true argument is
    the targets (or labels) for one batch, while y_pred represents the corresponding
    predictions from the model. You can ignore the sample_weight argument—we won’t
    use it here.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在`update_state()`中实现状态更新逻辑。`y_true`参数是一个批次的目标（或标签），而`y_pred`表示模型的相应预测。你可以忽略`sample_weight`参数——我们这里不会用到它。
- en: ❹ To match our MNIST model, we expect categorical predictions and integer labels.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为了匹配我们的MNIST模型，我们期望分类预测和整数标签。
- en: 'You use the `result()` method to return the current value of the metric:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`result()`方法返回指标的当前值：
- en: '[PRE30]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Meanwhile, you also need to expose a way to reset the metric state without
    having to reinstantiate it—this enables the same metric objects to be used across
    different epochs of training or across both training and evaluation. You do this
    with the `reset_state()` method:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，你还需要提供一种方法来重置指标状态，而不必重新实例化它——这使得相同的指标对象可以在训练的不同时期或在训练和评估之间使用。你可以使用`reset_state()`方法来实现这一点：
- en: '[PRE31]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Custom metrics can be used just like built-in ones. Let’s test-drive our own
    metric:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义指标可以像内置指标一样使用。让我们试用我们自己的指标：
- en: '[PRE32]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You can now see the `fit()` progress bar displaying the RMSE of your model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以看到`fit()`进度条显示你的模型的RMSE。
- en: 7.3.2 Using callbacks
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 使用回调
- en: 'Launching a training run on a large dataset for tens of epochs using `model.fit()`
    can be a bit like launching a paper airplane: past the initial impulse, you don’t
    have any control over its trajectory or its landing spot. If you want to avoid
    bad outcomes (and thus wasted paper airplanes), it’s smarter to use, not a paper
    plane, but a drone that can sense its environment, send data back to its operator,
    and automatically make steering decisions based on its current state. The Keras
    *callbacks* API will help you transform your call to `model.fit()` from a paper
    airplane into a smart, autonomous drone that can self-introspect and dynamically
    take action.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型数据集上进行数十个时期的训练运行，使用`model.fit()`有点像发射纸飞机：过了初始冲动，你就无法控制它的轨迹或着陆点。如果你想避免不良结果（从而浪费纸飞机），更明智的做法是使用不是纸飞机，而是一架可以感知环境、将数据发送回操作员并根据当前状态自动做出转向决策的无人机。Keras的*回调*API将帮助你将对`model.fit()`的调用从纸飞机转变为一个智能、自主的无人机，可以自我反省并动���采取行动。
- en: 'A callback is an object (a class instance implementing specific methods) that
    is passed to the model in the call to `fit()` and that is called by the model
    at various points during training. It has access to all the available data about
    the state of the model and its performance, and it can take action: interrupt
    training, save a model, load a different weight set, or otherwise alter the state
    of the model.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 回调是一个对象（实现特定方法的类实例），它在对`fit()`的模型调用中传递给模型，并在训练过程中的各个时刻被模型调用。它可以访问有关模型状态和性能的所有可用数据，并且可以采取行动：中断训练、保存模型、加载不同的权重集，或者以其他方式改变模型的状态。
- en: 'Here are some examples of ways you can use callbacks:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些使用回调的示例：
- en: '*Model checkpointing*—Saving the current state of the model at different points
    during training.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型检查点*——在训练过程中保存模型的当前状态。'
- en: '*Early stopping*—Interrupting training when the validation loss is no longer
    improving (and of course, saving the best model obtained during training).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提前停止*——当验证损失不再改善时中断训练（当然，保存在训练过程中获得的最佳模型）。'
- en: '*Dynamically adjusting the value of certain parameters during training*—Such
    as the learning rate of the optimizer.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在训练过程中动态调整某些参数的值*——比如优化器的学习率。'
- en: '*Logging training and validation metrics during training, or visualizing the
    representations learned by the model as they’re updated*—The `fit()` progress
    bar that you’re familiar with is in fact a callback!'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在训练过程中记录训练和验证指标，或者在更新时可视化模型学习到的表示*——你熟悉的`fit()`进度条实际上就是一个回调！'
- en: 'The `keras.callbacks` module includes a number of built-in callbacks (this
    is not an exhaustive list):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`keras.callbacks`模块包括许多内置回调（这不是一个详尽的列表）：'
- en: '[PRE33]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s review two of them to give you an idea of how to use them: `EarlyStopping`
    and `ModelCheckpoint`.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾其中的两个，以便让你了解如何使用它们：`EarlyStopping`和`ModelCheckpoint`。
- en: The EarlyStopping and ModelCheckpoint callbacks
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: EarlyStopping和ModelCheckpoint回调
- en: When you’re training a model, there are many things you can’t predict from the
    start. In particular, you can’t tell how many epochs will be needed to get to
    an optimal validation loss. Our examples so far have adopted the strategy of training
    for enough epochs that you begin overfitting, using the first run to figure out
    the proper number of epochs to train for, and then finally launching a new training
    run from scratch using this optimal number. Of course, this approach is wasteful.
    A much better way to handle this is to stop training when you measure that the
    validation loss is no longer improving. This can be achieved using the `EarlyStopping`
    callback.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当你训练一个模型时，有很多事情是你无法从一开始就预测的。特别是，你无法知道需要多少个时期才能达到最佳的验证损失。到目前为止，我们的例子采用了训练足够多个时期的策略，以至于你开始过拟合，使用第一次运行来确定适当的训练时期数量，然后最终启动一个新的训练运行，使用这个最佳数量。当然，这种方法是浪费的。更好的处理方式是在测量到验证损失不再改善时停止训练。这可以通过`EarlyStopping`回调来实现。
- en: 'The `EarlyStopping` callback interrupts training once a target metric being
    monitored has stopped improving for a fixed number of epochs. For instance, this
    callback allows you to interrupt training as soon as you start overfitting, thus
    avoiding having to retrain your model for a smaller number of epochs. This callback
    is typically used in combination with `ModelCheckpoint`, which lets you continually
    save the model during training (and, optionally, save only the current best model
    so far: the version of the model that achieved the best performance at the end
    of an epoch).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`EarlyStopping`回调会在监控的目标指标停止改进一定数量的时期后中断训练。例如，此回调允许您在开始过拟合时立即中断训练，从而避免不得不为更少的时期重新训练模型。此回调通常与`ModelCheckpoint`结合使用，后者允许您在训练过程中持续保存模型（可选地，仅保存迄今为止的当前最佳模型：在时期结束时表现最佳的模型版本）。'
- en: Listing 7.19 Using the `callbacks` argument in the `fit()` method
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.19 在`fit()`方法中使用`callbacks`参数
- en: '[PRE34]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Callbacks are passed to the model via the callbacks argument in fit(), which
    takes a list of callbacks. You can pass any number of callbacks.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 回调通过`fit()`方法中的callbacks参数传递给模型，该参数接受一个回调函数列表。您可以传递任意数量的回调函数。
- en: ❷ Interrupts training when improvement stops
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当改进停止时中断训练
- en: ❸ Monitors the model’s validation accuracy
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 监控模型的验证准确率
- en: ❹ Interrupts training when accuracy has stopped improving for two epochs
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 当准确率连续两个时期没有改善时中断训练
- en: ❺ Saves the current weights after every epoch
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在每个时期结束后保存当前权重
- en: ❻ Path to the destination model file
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 目标模型文件的路径
- en: ❼ These two arguments mean you won’t overwrite the model file unless val_loss
    has improved, which allows you to keep the best model seen during training.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 这两个参数意味着除非val_loss有所改善，否则您不会覆盖模型文件，这样可以保留训练过程中看到的最佳模型。
- en: ❽ You monitor accuracy, so it should be part of the model’s metrics.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 您正在监视准确率，因此它应该是模型指标的一部分。
- en: ❾ Note that because the callback will monitor validation loss and validation
    accuracy, you need to pass validation_data to the call to fit().
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 请注意，由于回调将监视验证损失和验证准确率，您需要将validation_data传递给fit()调用。
- en: Note that you can always save models manually after training as well—just call
    `model.save('my_checkpoint_path')`. To reload the model you’ve saved, just use
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您也可以在训练后手动保存模型——只需调用`model.save('my_checkpoint_path')`。要重新加载保存的模型，只需使用
- en: '[PRE35]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 7.3.3 Writing your own callbacks
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 编写自己的回调函数
- en: 'If you need to take a specific action during training that isn’t covered by
    one of the built-in callbacks, you can write your own callback. Callbacks are
    implemented by subclassing the `keras.callbacks.Callback` class. You can then
    implement any number of the following transparently named methods, which are called
    at various points during training:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在训练过程中执行特定操作，而内置回调函数没有涵盖，您可以编写自己的回调函数。通过继承`keras.callbacks.Callback`类来实现回调函数。然后，您可以实现以下任意数量的透明命名方法，这些方法在训练过程中的各个时刻调用：
- en: '[PRE36]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Called at the start of every epoch
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在每个时期开始时调用
- en: ❷ Called at the end of every epoch
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在每个时期结束时调用
- en: ❸ Called right before processing each batch
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在处理每个批次之前调用
- en: ❹ Called right after processing each batch
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在处理每个批次后立即调用
- en: ❺ Called at the start of training
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在训练开始时调用
- en: ❻ Called at the end of training
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在训练结束时调用
- en: These methods are all called with a `logs` argument, which is a dictionary containing
    information about the previous batch, epoch, or training run—training and validation
    metrics, and so on. The `on_epoch_*` and `on_batch_*` methods also take the epoch
    or batch index as their first argument (an integer).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法都带有一个`logs`参数，其中包含有关先前批次、时期或训练运行的信息——训练和验证指标等。`on_epoch_*`和`on_batch_*`方法还将时期或批次索引作为它们的第一个参数（一个整数）。
- en: Here’s a simple example that saves a list of per-batch loss values during training
    and saves a graph of these values at the end of each epoch.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的示例，它保存了训练过程中每个批次的损失值列表，并在每个时期结束时保存了这些值的图表。
- en: Listing 7.20 Creating a custom callback by subclassing the `Callback` class
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.20 通过继承`Callback`类创建自定义回调
- en: '[PRE37]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let’s test-drive it:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试驾一下：
- en: '[PRE38]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We get plots that look like figure 7.5\.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的图表看起来像图 7.5。
- en: '![](../Images/07-05.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-05.png)'
- en: Figure 7.5 The output of our custom history plotting callback
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 我们自定义历史绘图回调的输出
- en: 7.3.4 Monitoring and visualization with TensorBoard
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.4 使用TensorBoard进行监控和可视化
- en: 'To do good research or develop good models, you need rich, frequent feedback
    about what’s going on inside your models during your experiments. That’s the point
    of running experiments: to get information about how well a model performs—as
    much information as possible. Making progress is an iterative process, a loop—you
    start with an idea and express it as an experiment, attempting to validate or
    invalidate your idea. You run this experiment and process the information it generates.
    This inspires your next idea. The more iterations of this loop you’re able to
    run, the more refined and powerful your ideas become. Keras helps you go from
    idea to experiment in the least possible time, and fast GPUs can help you get
    from experiment to result as quickly as possible. But what about processing the
    experiment’s results? That’s where TensorBoard comes in (see figure 7.6).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行良好的研究或开发良好的模型，您需要在实验过程中获得关于模型内部情况的丰富、频繁的反馈。这就是进行实验的目的：获取有关模型表现的信息——尽可能多的信息。取得进展是一个迭代过程，一个循环——您从一个想法开始，并将其表达为一个实验，试图验证或否定您的想法。您运行此实验并处理它生成的信息。这激发了您的下一个想法。您能够运行此循环的迭代次数越多，您的想法就会变得越精细、更强大。Keras帮助您在最短的时间内从想法到实验，快速的GPU可以帮助您尽快从实验到结果。但是处理实验结果呢？这就是TensorBoard的作用（见图
    7.6）。
- en: '![](../Images/07-06.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-06.png)'
- en: Figure 7.6 The loop of progress
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 进展的循环
- en: TensorBoard ([www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard))
    is a browser-based application that you can run locally. It’s the best way to
    monitor everything that goes on inside your model during training. With TensorBoard,
    you can
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard（[www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)）是一个基于浏览器的应用程序，您可以在本地运行。这是在训练过程中监视模型内部所有活动的最佳方式。使用TensorBoard，您可以
- en: Visually monitor metrics during training
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中可视化监控指标
- en: Visualize your model architecture
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化您的模型架构
- en: Visualize histograms of activations and gradients
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化激活和梯度的直方图
- en: Explore embeddings in 3D
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在3D中探索嵌入
- en: If you’re monitoring more information than just the model’s final loss, you
    can develop a clearer vision of what the model does and doesn’t do, and you can
    make progress more quickly.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您监控的信息不仅仅是模型的最终损失，您可以更清晰地了解模型的作用和不作用，并且可以更快地取得进展。
- en: The easiest way to use TensorBoard with a Keras model and the `fit()` method
    is to use the `keras.callbacks.TensorBoard` callback.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorBoard与Keras模型和`fit()`方法的最简单方法是使用`keras.callbacks.TensorBoard`回调。
- en: 'In the simplest case, just specify where you want the callback to write logs,
    and you’re good to go:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，只需指定回调写入日志的位置，然后就可以开始了：
- en: '[PRE39]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Once the model starts running, it will write logs at the target location. If
    you are running your Python script on a local machine, you can then launch the
    local TensorBoard server using the following command (note that the `tensorboard`
    executable should be already available if you have installed TensorFlow via `pip`;
    if not, you can install TensorBoard manually via `pip` `install` `tensorboard`):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型开始运行，它将在目标位置写入日志。如果您在本地计算机上运行Python脚本，则可以使用以下命令启动本地TensorBoard服务器（请注意，如果您通过`pip`安装了TensorFlow，则`tensorboard`可执行文件应该已经可用；如果没有，则可以通过`pip`
    `install` `tensorboard`手动安装TensorBoard）：
- en: '[PRE40]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You can then navigate to the URL that the command returns in order to access
    the TensorBoard interface.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以转到命令返回的URL以访问TensorBoard界面。
- en: 'If you are running your script in a Colab notebook, you can run an embedded
    TensorBoard instance as part of your notebook, using the following commands:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Colab笔记本中运行脚本，则可以作为笔记本的一部分运行嵌入式TensorBoard实例，使用以下命令：
- en: '[PRE41]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In the TensorBoard interface, you will be able to monitor live graphs of your
    training and evaluation metrics (see figure 7.7).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorBoard界面中，您将能够监视训练和评估指标的实时图表（参见图7.7）。
- en: '![](../Images/07-07.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-07.png)'
- en: Figure 7.7 TensorBoard can be used for easy monitoring of training and evaluation
    metrics.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ��7.7 TensorBoard可用于轻松监控训练和评估指标。
- en: 7.4 Writing your own training and evaluation loops
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 编写自己的训练和评估循环
- en: The `fit()` workflow strikes a nice balance between ease of use and flexibility.
    It’s what you will use most of the time. However, it isn’t meant to support everything
    a deep learning researcher may want to do, even with custom metrics, custom losses,
    and custom callbacks.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()`工作流在易用性和灵活性之间取得了很好的平衡。这是您大部分时间将使用的方法。但是，即使使用自定义指标、自定义损失和自定义回调，它也不意味着支持深度学习研究人员可能想要做的一切。'
- en: 'After all, the built-in `fit()` workflow is solely focused on *supervised learning*:
    a setup where there are known *targets* (also called *labels* or *annotations*)
    associated with your input data, and where you compute your loss as a function
    of these targets and the model’s predictions. However, not every form of machine
    learning falls into this category. There are other setups where no explicit targets
    are present, such as *generative learning* (which we will discuss in chapter 12),
    *self-supervised learning* (where targets are obtained from the inputs), and *reinforcement
    learning* (where learning is driven by occasional “rewards,” much like training
    a dog). Even if you’re doing regular supervised learning, as a researcher, you
    may want to add some novel bells and whistles that require low-level flexibility.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，内置的`fit()`工作流仅专注于*监督学习*：一种已知*目标*（也称为*标签*或*注释*）与输入数据相关联的设置，您根据这些目标和模型预测的函数计算损失。然而，并非所有形式的机器学习都属于这一类别。还有其他设置，其中没有明确的目标，例如*生成学习*（我们将在第12章中讨论）、*自监督学习*（目标来自输入）和*强化学习*（学习受偶尔“奖励”驱动，类似训练狗）。即使您正在进行常规监督学习，作为研究人员，您可能希望添加一些需要低级灵活性的新颖功能。
- en: 'Whenever you find yourself in a situation where the built-in `fit()` is not
    enough, you will need to write your own custom training logic. You already saw
    simple examples of low-level training loops in chapters 2 and 3\. As a reminder,
    the contents of a typical training loop look like this:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 每当您发现内置的`fit()`不够用时，您将需要编写自己的自定义训练逻辑。您已经在第2章和第3章看到了低级训练循环的简单示例。作为提醒，典型训练循环的内容如下：
- en: Run the forward pass (compute the model’s output) inside a gradient tape to
    obtain a loss value for the current batch of data.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行前向传播（计算模型的输出）在梯度磁带内以获得当前数据批次的损失值。
- en: Retrieve the gradients of the loss with regard to the model’s weights.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索损失相对于模型权重的梯度。
- en: Update the model’s weights so as to lower the loss value on the current batch
    of data.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新模型的权重以降低当前数据批次上的损失值。
- en: These steps are repeated for as many batches as necessary. This is essentially
    what `fit()` does under the hood. In this section, you will learn to reimplement
    `fit()` from scratch, which will give you all the knowledge you need to write
    any training algorithm you may come up with.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将根据需要重复多个批次。这基本上是`fit()`在幕后执行的操作。在本节中，您将学习如何从头开始重新实现`fit()`，这将为您提供编写任何可能想出的训练算法所需的所有知识。
- en: Let’s go over the details.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解一下。
- en: 7.4.1 Training versus inference
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 训练与推断
- en: In the low-level training loop examples you’ve seen so far, step 1 (the forward
    pass) was done via `predictions` `=` `model(inputs)`, and step 2 (retrieving the
    gradients computed by the gradient tape) was done via `gradients` `=` `tape.gradient(loss,`
    `model.weights)`. In the general case, there are actually two subtleties you need
    to take into account.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在你迄今为止看到的低级训练循环示例中，第1步（前向传播）通过`predictions` `=` `model(inputs)`完成，第2步（检索梯度）通过`gradients`
    `=` `tape.gradient(loss,` `model.weights)`完成。在一般情况下，实际上有两个你需要考虑的细微之处。
- en: Some Keras layers, such as the `Dropout` layer, have different behaviors during
    *training* and during *inference* (when you use them to generate predictions).
    Such layers expose a `training` Boolean argument in their `call()` method. Calling
    `dropout(inputs,` `training=True)` will drop some activation entries, while calling
    `dropout(inputs,` `training=False)` does nothing. By extension, Functional and
    Sequential models also expose this `training` argument in their `call()` methods.
    Remember to pass `training =True` when you call a Keras model during the forward
    pass! Our forward pass thus becomes `predictions` `=` `model(inputs,` `training=True)`.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 一些Keras层，比如`Dropout`层，在*训练*和*推理*（当你用它们生成预测时）时有不同的行为。这些层在它们的`call()`方法中暴露了一个`training`布尔参数。调用`dropout(inputs,`
    `training=True)`会丢弃一些激活条目，而调用`dropout(inputs,` `training=False)`则不会做任何操作。扩展到Functional和Sequential模型，它们的`call()`方法中也暴露了这个`training`参数。记得在前向传播时传递`training=True`给Keras模型！因此我们的前向传播变成了`predictions`
    `=` `model(inputs,` `training=True)`。
- en: 'In addition, note that when you retrieve the gradients of the weights of your
    model, you should not use `tape.gradients(loss,` `model.weights)`, but rather
    `tape .gradients(loss,` `model.trainable_weights)`. Indeed, layers and models
    own two kinds of weights:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意，当你检索模型权重的梯度时，不应该使用`tape.gradients(loss,` `model.weights)`，而应该使用`tape
    .gradients(loss,` `model.trainable_weights)`。实际上，层和模型拥有两种权重：
- en: '*Trainable weights*—These are meant to be updated via backpropagation to minimize
    the loss of the model, such as the kernel and bias of a `Dense` layer.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可训练权重*—这些权重通过反向传播来更新，以最小化模型的损失，比如`Dense`层的核和偏置。'
- en: '*Non-trainable weights*—These are meant to be updated during the forward pass
    by the layers that own them. For instance, if you wanted a custom layer to keep
    a counter of how many batches it has processed so far, that information would
    be stored in a non-trainable weight, and at each batch, your layer would increment
    the counter by one.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不可训练权重*—这些权重在前向传播过程中由拥有它们的层更新。例如，如果你想让一个自定义层记录到目前为止处理了多少批次，那么这些信息将存储在不可训练权重中，每个批次，你的层会将计数器加一。'
- en: Among Keras built-in layers, the only layer that features non-trainable weights
    is the `BatchNormalization` layer, which we will discuss in chapter 9\. The `BatchNormalization`
    layer needs non-trainable weights in order to track information about the mean
    and standard deviation of the data that passes through it, so as to perform an
    online approximation of *feature normalization* (a concept you learned about in
    chapter 6).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras内置层中，唯一具有不可训练权重的层是`BatchNormalization`层，我们将在第9章讨论。`BatchNormalization`层需要不可训练权重来跟踪通过它的数据的均值和标准差的信息，以便执行*特征归一化*的在线近似（这是你在第6章学到的概念）。
- en: 'Taking into account these two details, a supervised-learning training step
    ends up looking like this:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这两个细节，监督学习训练步骤最终看起来像这样：
- en: '[PRE42]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 7.4.2 Low-level usage of metrics
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 指标的低级使用
- en: 'In a low-level training loop, you will probably want to leverage Keras metrics
    (whether custom ones or the built-in ones). You’ve already learned about the metrics
    API: simply call `update_state(y_true,` `y_pred)` for each batch of targets and
    predictions, and then use `result()` to query the current metric value:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在低级训练循环中，你可能想要利用Keras指标（无论是自定义的还是内置的）。你已经了解了指标API：只需为每个目标和预测批次调用`update_state(y_true,`
    `y_pred)`，然后使用`result()`来查询当前指标值：
- en: '[PRE43]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You may also need to track the average of a scalar value, such as the model’s
    loss. You can do this via the `keras.metrics.Mean` metric:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还需要跟踪标量值的平均值，比如模型的损失。你可以通过`keras.metrics.Mean`指标来实现这一点：
- en: '[PRE44]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Remember to use `metric.reset_state()` when you want to reset the current results
    (at the start of a training epoch or at the start of evaluation).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要重置当前结果（在训练周期的开始或评估的开始）时，请记得使用`metric.reset_state()`。
- en: 7.4.3 A complete training and evaluation loop
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 完整的训练和评估循环
- en: Let’s combine the forward pass, backward pass, and metrics tracking into a `fit()`-like
    training step function that takes a batch of data and targets and returns the
    logs that would get displayed by the `fit()` progress bar.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将前向传播、反向传播和指标跟踪结合到一个类似于`fit()`的训练步骤函数中，该函数接受一批数据和目标，并返回`fit()`进度条显示的日志。
- en: 'Listing 7.21 Writing a step-by-step training loop: the training step function'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.21 编���逐步训练循环：训练步骤函数
- en: '[PRE45]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ Prepare the loss function.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备损失函数。
- en: ❷ Prepare the optimizer.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 准备优化器。
- en: ❸ Prepare the list of metrics to monitor.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 准备要监视的指标列表。
- en: ❹ Prepare a Mean metric tracker to keep track of the loss average.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 准备一个Mean指标追踪器来跟踪损失的平均值。
- en: ❺ Run the forward pass. Note that we pass training=True.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 进行前向传播。注意我们传递了`training=True`。
- en: ❻ Run the backward pass. Note that we use model.trainable_weights.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 进行反向传播。注意我们使用了`model.trainable_weights`。
- en: ❼ Keep track of metrics.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 跟踪指标。
- en: ❽ Keep track of the loss average.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 跟踪损失平均值。
- en: ❾ Return the current values of the metrics and the loss.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 返回当前的指标值和损失。
- en: We will need to reset the state of our metrics at the start of each epoch and
    before running evaluation. Here’s a utility function to do it.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在每个周期开始和运行评估之前重置指标的状态。这里有一个实用函数来做到这一点。
- en: 'Listing 7.22 Writing a step-by-step training loop: resetting the metrics'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.22 编写逐步训练循环：重置指标
- en: '[PRE46]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We can now lay out our complete training loop. Note that we use a `tf.data.Dataset`
    object to turn our NumPy data into an iterator that iterates over the data in
    batches of size 32.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以布置完整的训练循环。请注意，我们使用`tf.data.Dataset`对象将我们的 NumPy 数据转换为一个迭代器，该迭代器按大小为 32
    的批次迭代数据。
- en: 'Listing 7.23 Writing a step-by-step training loop: the loop itself'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.23 编写逐步训练循环：循环本身
- en: '[PRE47]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'And here’s the evaluation loop: a simple `for` loop that repeatedly calls a
    `test_step()` function, which processes a single batch of data. The `test_step()`
    function is just a subset of the logic of `train_step()`. It omits the code that
    deals with updating the weights of the model—that is to say, everything involving
    the `GradientTape` and the optimizer.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是评估循环：一个简单的`for`循环，反复调用`test_step()`函数，该函数处理一个数据批次。`test_step()`函数只是`train_step()`逻辑的一个子集。它省略了处理更新模型权重的代码——也就是说，所有涉及`GradientTape`和优化器的内容。
- en: Listing 7.24 Writing a step-by-step evaluation loop
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.24 编写逐步评估循环
- en: '[PRE48]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ❶ Note that we pass training=False.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 注意我们传递��� training=False。
- en: 'Congrats—you’ve just reimplemented `fit()` and `evaluate()`! Or almost: `fit()`
    and `evaluate()` support many more features, including large-scale distributed
    computation, which requires a bit more work. It also includes several key performance
    optimizations.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你——你刚刚重新实现了`fit()`和`evaluate()`！或几乎：`fit()`和`evaluate()`支持许多更多功能，包括大规模分布式计算，这需要更多的工作。它还包括几个关键的性能优化。
- en: 'Let’s take a look at one of these optimizations: TensorFlow function compilation.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看其中一个优化：TensorFlow 函数编译。
- en: 7.4.4 Make it fast with tf.function
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.4 使用 tf.function 使其更快
- en: You may have noticed that your custom loops are running significantly slower
    than the built-in `fit()` and `evaluate()`, despite implementing essentially the
    same logic. That’s because, by default, TensorFlow code is executed line by line,
    *eagerly*, much like NumPy code or regular Python code. Eager execution makes
    it easier to debug your code, but it is far from optimal from a performance standpoint.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，尽管实现了基本相同的逻辑，但自定义循环的运行速度明显比内置的`fit()`和`evaluate()`慢。这是因为，默认情况下，TensorFlow
    代码是逐行执行的，*急切执行*，类似于 NumPy 代码或常规 Python 代码。急切执行使得调试代码更容易，但从性能的角度来看远非最佳选择。
- en: 'It’s more performant to *compile* your TensorFlow code into a *computation
    graph* that can be globally optimized in a way that code interpreted line by line
    cannot. The syntax to do this is very simple: just add a `@tf.function` to any
    function you want to compile before executing, as shown in the following listing.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的 TensorFlow 代码*编译*成一个可以全局优化的*计算图*更有效。要做到这一点的语法非常简单：只需在要执行之前的任何函数中添加`@tf.function`，如下面的示例所示。
- en: Listing 7.25 Adding a `@tf.function` decorator to our evaluation-step function
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.25 为我们的评估步骤函数添加`@tf.function`装饰器
- en: '[PRE49]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ This is the only line that changed.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是唯一改变的一行。
- en: On the Colab CPU, we go from taking 1.80 s to run the evaluation loop to only
    0.8 s. Much faster!
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Colab CPU 上，我们从运行评估循环需要 1.80 秒，降低到只需要 0.8 秒。速度更快！
- en: Remember, while you are debugging your code, prefer running it eagerly, without
    any `@tf.function` decorator. It’s easier to track bugs this way. Once your code
    is working and you want to make it fast, add a `@tf.function` decorator to your
    training step and your evaluation step—or any other performance-critical function.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在调试代码时，最好急切地运行它，不要添加任何`@tf.function`装饰器。这样更容易跟踪错误。一旦你的代码运行正常并且想要加快速度，就在你的训练步骤和评估步骤或任何其他性能关键函数中添加`@tf.function`装饰器。
- en: 7.4.5 Leveraging fit() with a custom training loop
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.5 利用 fit() 与自定义训练循环
- en: In the previous sections, we were writing our own training loop entirely from
    scratch. Doing so provides you with the most flexibility, but you end up writing
    a lot of code while simultaneously missing out on many convenient features of
    `fit()`, such as callbacks or built-in support for distributed training.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们完全从头开始编写自己的训练循环。这样做为你提供了最大的灵活性，但同时你会写很多代码，同时错过了`fit()`的许多便利功能，比如回调或内置的分布式训练支持。
- en: 'What if you need a custom training algorithm, but you still want to leverage
    the power of the built-in Keras training logic? There’s actually a middle ground
    between `fit()` and a training loop written from scratch: you can provide a custom
    training step function and let the framework do the rest.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要一个自定义训练算法，但仍想利用内置 Keras 训练逻辑的强大功能，那么实际上在`fit()`和从头编写的训练循环之间有一个中间地带：你可以提供一个自定义训练步骤函数，让框架来处理其余部分。
- en: You can do this by overriding the `train_step()` method of the `Model` class.
    This is the function that is called by `fit()` for every batch of data. You will
    then be able to call `fit()` as usual, and it will be running your own learning
    algorithm under the hood.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过重写`Model`类的`train_step()`方法来实现这一点。这个函数是`fit()`为每个数据批次调用的函数。然后你可以像往常一样调用`fit()`，它将在幕后运行你自己的学习算法。
- en: 'Here’s a simple example:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的例子：
- en: We create a new class that subclasses `keras.Model`.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建一个继承`keras.Model`的新类。
- en: We override the method `train_step(self,` `data)`. Its contents are nearly identical
    to what we used in the previous section. It returns a dictionary mapping metric
    names (including the loss) to their current values.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们重写了方法`train_step(self,` `data)`。它的内容几乎与我们在上一节中使用的内容相同。它返回一个将度量名称（包括损失）映射到它们当前值的字典。
- en: We implement a `metrics` property that tracks the model’s `Metric` instances.
    This enables the model to automatically call `reset_state()` on the model’s metrics
    at the start of each epoch and at the start of a call to `evaluate()`, so you
    don’t have to do it by hand.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们实现了一个`metrics`属性，用于跟踪模型的`Metric`实例。这使得模型能够在每个时期开始和在调用`evaluate()`开始时自动调用`reset_state()`模型的度量，因此你不必手动执行。
- en: Listing 7.26 Implementing a custom training step to use with `fit()`
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.26 实现一个自定义训练步骤以与`fit()`一起使用
- en: '[PRE50]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: ❶ This metric object will be used to track the average of per-batch losses during
    training and evaluation.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这个度量对象将用于跟踪训练和评估过程中每个批次损失的平均值。
- en: ❷ We override the train_step method.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们重写了train_step方法。
- en: ❸ We use self(inputs, training=True) instead of model(inputs, training=True),
    since our model is the class itself.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们使用`self(inputs, training=True)`而不是`model(inputs, training=True)`，因为我们的模型就是类本身。
- en: ❹ We update the loss tracker metric that tracks the average of the loss.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们更新跟踪平均损失的损失跟踪器指标。
- en: ❺ We return the average loss so far by querying the loss tracker metric.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通过查询损失跟踪器指标返回到目前为止的平均损失。
- en: ❻ Any metric you would like to reset across epochs should be listed here.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 任何你想要在不同epoch之间重置的指标都应该在这里列出。
- en: 'We can now instantiate our custom model, compile it (we only pass the optimizer,
    since the loss is already defined outside of the model), and train it using `fit()`
    as usual:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以实例化我们的自定义模型，编译它（我们只传递了优化器，因为损失已经在模型外部定义），并像往常一样使用`fit()`进行训练：
- en: '[PRE51]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'There are a couple of points to note:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要注意：
- en: This pattern does not prevent you from building models with the Functional API.
    You can do this whether you’re building Sequential models, Functional API models,
    or subclassed models.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种模式不会阻止你使用Functional API构建模型。无论你是构建Sequential模型、Functional API模型还是子类化模型，都可以做到这一点。
- en: You don’t need to use a `@tf.function` decorator when you override `train_ step`—the
    framework does it for you.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你重写`train_step`时，不需要使用`@tf.function`装饰器—框架会为你做这件事。
- en: 'Now, what about metrics, and what about configuring the loss via `compile()`?
    After you’ve called `compile()`, you get access to the following:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于指标，以及如何通过`compile()`配置损失呢？在调用`compile()`之后，你可以访问以下内容：
- en: '`self.compiled_loss`—The loss function you passed to `compile()`.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.compiled_loss`—你传递给`compile()`的损失函数。'
- en: '`self.compiled_metrics`—A wrapper for the list of metrics you passed, which
    allows you to call `self.compiled_metrics.update_state()` to update all of your
    metrics at once.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.compiled_metrics`—对你传递的指标列表的包装器，允许你调用`self.compiled_metrics.update_state()`一次性更新所有指标。'
- en: '`self.metrics`—The actual list of metrics you passed to `compile()`. Note that
    it also includes a metric that tracks the loss, similar to what we did manually
    with our `loss_tracking_metric` earlier.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.metrics`—你传递给`compile()`的实际指标列表。请注意，它还包括一个跟踪损失的指标，类似于我们之前手动使用`loss_tracking_metric`所做的。'
- en: We can thus write
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写下
- en: '[PRE52]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: ❶ Compute the loss via self.compiled_loss.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过`self.compiled_loss`计算损失。
- en: ❷ Update the model’s metrics via self.compiled_metrics.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过`self.compiled_metrics`更新模型的指标。
- en: ❸ Return a dict mapping metric names to their current value.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回一个将指标名称映射到它们当前值的字典。
- en: 'Let’s try it:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试：
- en: '[PRE53]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: That was a lot of information, but you now know enough to use Keras to do almost
    anything.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这是很多信息，但现在你已经了解足够多的内容来使用Keras做几乎任何事情了。
- en: Summary
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Keras offers a spectrum of different workflows, based on the principle of *progressive
    disclosure of complexity*. They all smoothly inter-operate together.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras提供了一系列不同的工作流程，基于*逐步透露复杂性*的原则。它们之间可以平滑地互操作。
- en: You can build models via the `Sequential` class, via the Functional API, or
    by subclassing the `Model` class. Most of the time, you’ll be using the Functional
    API.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过`Sequential`类、Functional API或通过子类化`Model`类来构建模型。大多数情况下，你会使用Functional API。
- en: The simplest way to train and evaluate a model is via the default `fit()` and
    `evaluate()` methods.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和评估模型的最简单方法是通过默认的`fit()`和`evaluate()`方法。
- en: Keras callbacks provide a simple way to monitor models during your call to `fit()`
    and automatically take action based on the state of the model.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras回调提供了一种简单的方法，在调用`fit()`期间监视模型，并根据模型的状态自动采取行动。
- en: You can also fully take control of what `fit()` does by overriding the `train_
    step()` method.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你也可以通过重写`train_step()`方法完全控制`fit()`的行为。
- en: Beyond `fit()`, you can also write your own training loops entirely from scratch.
    This is useful for researchers implementing brand-new training algorithms.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了`fit()`，你还可以完全从头开始编写自己的训练循环。这对于实现全新训练算法的研究人员非常有用。
