- en: 6 Bayesian tools for machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 机器学习的贝叶斯工具
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Unsupervised machine learning models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督机器学习模型
- en: Bayes’ theorem, conditional probability, entropy, cross-entropy, and conditional
    entropy
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯定理、条件概率、熵、交叉熵和条件熵
- en: Maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estimation
    of model parameters
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数的最大似然估计（MLE）和最大后验估计（MAP）
- en: Evidence maximization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证据最大化
- en: KLD
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KLD
- en: Gaussian mixture models (GMM) and MLE estimation of GMM parameters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMM）和GMM参数的MLE估计
- en: The Bayesian approach to statistics tries to model the world by modeling the
    uncertainties and prevailing beliefs and knowledge about the system. This is in
    contrast to the frequentist paradigm, where probability is strictly measured by
    observing a phenomenon repeatedly and measuring the fraction of time an event
    occurs. Machine learning, in particular *unsupervised* machine learning, is a
    lot closer to the Bayesian paradigm of statistics—the subject of this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学的贝叶斯方法试图通过建模系统的不确定性和普遍信念及知识来模拟世界。这与频率主义范式形成对比，在频率主义范式中，概率是通过反复观察现象并测量事件发生的频率来严格测量的。机器学习，尤其是无监督机器学习，与统计学的贝叶斯范式非常接近——这是本章的主题。
- en: 'In chapter [1](../Text/01.xhtml#chap-overview), we primarily discussed *supervised*
    machine learning, where the training data is labeled: each input value is accompanied
    by a manually created desired output value. Labeling training inputs is a manual,
    labor-intensive process and often the worst pain point in building a machine learning–based
    system. This has led to considerable recent interest in *unsupervised* machine
    learning, where we build a model from *unlabeled* training data. How is this done?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[1](../Text/01.xhtml#chap-overview)章中，我们主要讨论了**监督**机器学习，其中训练数据是标记的：每个输入值都伴随着一个手动创建的期望输出值。标记训练输入是一个手动、劳动密集的过程，并且往往是构建基于机器学习系统的最痛苦点。这导致了近年来对**无监督**机器学习的极大兴趣，我们在未标记的训练数据上构建模型。这是如何做到的？
- en: The general approach is best visualized geometrically. Each input data instance
    is a point in a high-dimensional space. These points form an overall pattern in
    the space of all possible inputs. If the inputs all have a common property, the
    points are not distributed randomly over the input space. Rather, they occupy
    a region in the input space with a definite shape. If the inputs have multiple
    classes, each class occupies a separate cluster in the space. Sometimes we apply
    a transformation to the input first—the transform is chosen or learned so that
    the transformed points exhibit a pattern more clearly than raw input points. We
    then identify a probability distribution whose sample point cloud matches the
    shape of the (potentially transformed) training data point cloud. We can generate
    faux input by sampling from this distribution. We can also classify an arbitrary
    input by observing which cluster it falls into.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通用方法最好通过几何图形来可视化。每个输入数据实例都是高维空间中的一个点。这些点在所有可能输入的空间中形成一个整体模式。如果所有输入都具有共同属性，那么点不会在输入空间中随机分布。相反，它们占据输入空间中的一个具有确定形状的区域。如果输入具有多个类别，那么每个类别在空间中占据一个单独的簇。有时我们首先对输入进行变换——变换的选择或学习是为了使变换后的点比原始输入点更清晰地显示出模式。然后我们确定一个概率分布，其样本点云与（可能变换过的）训练数据点云的形状相匹配。我们可以从这个分布中采样生成伪输入。我们还可以通过观察它落入哪个簇来对任意输入进行分类。
- en: NOTE The complete PyTorch code for this chapter is available at [http://mng.bz](http://mng.bz/WdZa)
    [/WdZa](http://mng.bz/WdZa) in the form of fully functional and executable Jupyter
    notebooks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章的完整PyTorch代码以完全功能性和可执行的形式，作为Jupyter笔记本，可在[http://mng.bz](http://mng.bz/WdZa)
    [/WdZa](http://mng.bz/WdZa)找到。
- en: 6.1 Conditional probability and Bayes’ theorem
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 条件概率和贝叶斯定理
- en: As usual, the discussion is accompanied by examples. In this context, we first
    offer a refresher on the concepts of joint and marginal probability from section
    [5.4](../Text/05.xhtml#sec-joint-prob) (you may want to revisit the topic of joint
    probability in sections [5.4](../Text/05.xhtml#sec-joint-prob), [5.4.1](../Text/05.xhtml#sec-marginal-prob),
    and [5.4.2](../Text/05.xhtml#sec-joint-prob-depend)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，讨论伴随着示例。在此背景下，我们首先对第[5.4](../Text/05.xhtml#sec-joint-prob)节（您可能想回顾第[5.4](../Text/05.xhtml#sec-joint-prob)、[5.4.1](../Text/05.xhtml#sec-marginal-prob)和[5.4.2](../Text/05.xhtml#sec-joint-prob-depend)节中的联合概率主题）中的联合概率和边缘概率概念进行复习。
- en: 'Consider two random variables: the height and weight of adult Statsville residents.
    Weight (denoted *W*) can take three quantized values: *E*[1], *E*[2], *E*[3].
    Height (*H*) can also take three quantized values: *F*[1], *F*[2], *F*[3]. Table
    [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob) shows their joint
    probability.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个随机变量：Statsville成年居民的身高和体重。体重（表示为 *W*）可以取三个量化值：*E*[1]、*E*[2]、*E*[3]。身高 (*H*)
    也可以取三个量化值：*F*[1]、*F*[2]、*F*[3]。表 [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    展示了它们的联合概率。
- en: 6.1.1 Joint and marginal probability revisited
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 联合和边缘概率再探讨
- en: 'One glance at table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    tells us that the probabilities are concentrated along the main diagonal, which
    indicates dependent events. This can be validated by inspecting one joint probability—say,
    *p*(*E*[1], *F*[1])—and the corresponding marginal probabilities *p*(*F*[1]) and
    *p*(*E*[1]). We can see that *p*(*E*[1], *F*[1]) = 0.2 ≠ *p*(*F*[1]) × *p*(*E*[1])
    = 0.26 × 0.26, establishing that the random variables weight *W* and height *H*
    are not independent. For contrast, look at table [5.6](../Text/05.xhtml#tab-jmarginal-prob).
    In that case, for any valid *i*, *j* pair, *p*(*E[i]*, *G[j]*) = *p*(*G[i]*) ×
    *p*(*E[j]*): the two events (weight and distance of a resident’s home from the
    city center) are independent. Note the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一眼就能看出表 [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob) 中的概率主要集中在主对角线上，这表明事件是相关的。这可以通过检查一个联合概率——比如
    *p*(*E*[1], *F*[1])——以及相应的边缘概率 *p*(*F*[1]) 和 *p*(*E*[1]) 来验证。我们可以看到 *p*(*E*[1],
    *F*[1]) = 0.2 ≠ *p*(*F*[1]) × *p*(*E*[1]) = 0.26 × 0.26，这表明随机变量体重 *W* 和身高 *H*
    不是独立的。相比之下，看看表 [5.6](../Text/05.xhtml#tab-jmarginal-prob)。在那个例子中，对于任何有效的 *i*、*j*
    对，*p*(*E[i]*, *G[j]*) = *p*(*G[i]*) × *p*(*E[j]*)：两个事件（居民的体重和家到市中心的距离）是独立的。注意以下内容：
- en: Table 6.1 Example population sizes and joint probability distribution for variables
    *W* = {*E*[1], *E*[2], *E*[3]} and *H* = {*F*[1], *F*[2], *F*[3]} weights and
    heights of adult Statsville residents), showing marginal probabilities
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1 示例人口规模和变量 *W* = {*E*[1], *E*[2], *E*[3]} 和 *H* = {*F*[1], *F*[2], *F*[3]}
    的联合概率分布，显示边缘概率
- en: '|  | Less than 60 kg (*E*[1]) | Between 60 and 90 kg (*E*[2]) | More than 90
    kg (*E*[3]) | Marginals for Fs |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | 低于 60 kg (*E*[1]) | 60 至 90 kg (*E*[2]) | 超过 90 kg (*E*[3]) | Fs 的边缘概率
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Less than 160 cm** **(*F*[1])** | pop. = 20,000*p*(*E*[1], *F*[1]) = 0.2
    | pop. = 4,000*p*(*E*[2], *F*[1]) = 0.04 | pop. = 2,000*p*(*E*[3], *F*[1]) = 0.02
    | pop. = 26,000;*p*(*F*[1]) = 0.2 + 0.04 + 0.02 = 0.26 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **低于 160 cm** **(*F*[1])** | 人数 = 20,000*p*(*E*[1], *F*[1]) = 0.2 | 人数 =
    4,000*p*(*E*[2], *F*[1]) = 0.04 | 人数 = 2,000*p*(*E*[3], *F*[1]) = 0.02 | 人数 =
    26,000;*p*(*F*[1]) = 0.2 + 0.04 + 0.02 = 0.26 |'
- en: '| **Between 160 cm and 183 cm** (*F*[2]) | pop. = 4,000*p*(*E*[1], *F*[2])
    = 0.04 | pop. = 40,000*p*(*E*[2], *F*[2]) = 0.4 | pop. = 4,000*p*(*E*[3], *F*[2])
    = 0.04 | pop. = 48,000;*p*(*F*[2]) = 0.04 + 0.4 + 0.04 = 0.48 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **160 至 183 cm 之间** (*F*[2]) | 人数 = 4,000*p*(*E*[1], *F*[2]) = 0.04 | 人数
    = 40,000*p*(*E*[2], *F*[2]) = 0.4 | 人数 = 4,000*p*(*E*[3], *F*[2]) = 0.04 | 人数
    = 48,000;*p*(*F*[2]) = 0.04 + 0.4 + 0.04 = 0.48 |'
- en: '| **More than 183 cm** (*F*[3]) | pop. = 2,000*p*(*E*[1], *F*[3]) = 0.02 |
    pop. = 4,000*p*(*E*[2], *F*[3]) = 0.04 | pop. = 20,000*p*(*E*[3], *F*[3]) = 0.2
    | pop. = 26,000;*p*(*F*[3]) = 0.02 + 0.04 + 0.2 = 0.26 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **超过 183 cm** (*F*[3]) | 人数 = 2,000*p*(*E*[1], *F*[3]) = 0.02 | 人数 = 4,000*p*(*E*[2],
    *F*[3]) = 0.04 | 人数 = 20,000*p*(*E*[3], *F*[3]) = 0.2 | 人数 = 26,000;*p*(*F*[3])
    = 0.02 + 0.04 + 0.2 = 0.26 |'
- en: '| **Marginals for** ***E*s** | *p*(*E*[1])= 0.2 + 0.04 + 0.02 = 0.26 | *p*(*E*[2])=
    0.04 + 0.4 + 0.04 = 0.48 | *p*(*E*[3])= 0.02 + 0.04 + 0.2 = 0.26 | Total pop.
    = 100,000;Total prob = 1 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **E*s** 的边缘概率 | *p*(*E*[1])= 0.2 + 0.04 + 0.02 = 0.26 | *p*(*E*[2])= 0.04
    + 0.4 + 0.04 = 0.48 | *p*(*E*[3])= 0.02 + 0.04 + 0.2 = 0.26 | 总人口 = 100,000; 总概率
    = 1 |'
- en: '*Joint probability*—This is the probability of a specific combination of values
    occurring *together*. Each cell in table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    depicts one joint probability: for example, the probability that a resident’s
    weight is between 60 and 90 kg *and* that their height is greater than 183 cm
    is *p*(*E*[2], *F*[3]) = 0.04.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*联合概率*—这是特定值组合同时发生的概率。表 [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    中的每个单元格描述了一个联合概率：例如，居民体重在 60 至 90 公斤之间 *并且*身高超过 183 厘米的概率是 *p*(*E*[2], *F*[3])
    = 0.04。'
- en: '*Sum rule*—The joint probabilities of all possible variable combinations sum
    to 1 (bottom right cell in table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)):'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*求和法则*—所有可能的变量组合的联合概率之和为 1（表 [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    中的右下角单元格）：'
- en: '![](../../OEBPS/Images/eq_06-00-a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-00-a.png)'
- en: The sum of probabilities is the probability of one or another of the corresponding
    events occurring. Here we are adding all possible event combinations—one or another
    of these combinations will certainly occur. Hence the sum is 1, which matches
    our intuition.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 概率之和是其中一个或另一个相应事件发生的概率。在这里，我们添加了所有可能的事件组合——这些组合中的任何一个都肯定会发生。因此，总和为1，这与我们的直觉相符。
- en: '*Marginal probability for a variable*—This is obtained by “summing away” the
    other variables (right-most column and bottom-most row in table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)):'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变量的边缘概率*—这是通过“消除”其他变量得到的（表 [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    中的最右边列和最底行）：'
- en: '![](../../OEBPS/Images/eq_06-00-b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-00-b.png)'
- en: We have added all possible combinations of other variables, so the sum represents
    the probability of this one variable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了所有其他变量的所有可能组合，因此总和代表了这个单一变量的概率。
- en: '*Marginal probabilities*—These sum to 1:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*边缘概率*—这些概率之和为1：'
- en: '![](../../OEBPS/Images/eq_06-00-c.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-00-c.png)'
- en: The sum of the marginal probabilities is the sum of all possible joint probabilities.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘概率之和是所有可能联合概率之和。
- en: '*Dependent vs. independent variables*—If and only if the variables are independent,
    the product of the marginal probabilities is the same as the joint probability:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*相关变量与独立变量*—只有当变量是独立的，边缘概率的乘积才等于联合概率：'
- en: '*p*(*F[i] , E[j]*) ≠ *p*(*F[i]*) × *p*(*E[j]*) ⟺ for dependent variables in
    table [5.6](../Text/05.xhtml#tab-jmarginal-prob)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*F[i] , E[j]*) ≠ *p*(*F[i]*) × *p*(*E[j]*) ⟺ 对于表 [5.6](../Text/05.xhtml#tab-jmarginal-prob)
    中的相关变量'
- en: '*p*(*G[i] , E[j]*) = *p*(*G[i]*) × *p*(*E[j]*) ⟺ for independent variables
    in table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*G[i] , E[j]*) = *p*(*G[i]*) × *p*(*E[j]*) ⟺ 对于表 [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    中的独立变量'
- en: You should verify that this condition is *not satisfied* in table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    for the weight and height variables. It *is satisfied* in table [5.6](../Text/05.xhtml#tab-jmarginal-prob)
    for the weight and distance-of-home-from-city-center variables.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该验证在表 [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob) 中，对于体重和身高变量，这个条件*不成立*。在表
    [5.6](../Text/05.xhtml#tab-jmarginal-prob) 中，对于体重和从市中心到家的距离变量，这个条件*成立*。
- en: 6.1.2 Conditional probability
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 条件概率
- en: Suppose we know that the height of a subject is between 160 and 183 cm (*H*
    = *F*[2]). What is the probability of the subject’s weight being more than 90
    kg (*W* = *E*[3])? In statistical parlance, this probability is denoted *p*(*W*
    = *E*[3]|*H* = *F*[2]). It is read “probability of *W* = *E*[3] *given* *H* =
    *F*[2],” aka “probability of *W* = *E*[3] *subject to the condition* *H* = *F*[2].”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们知道某人的身高在160至183厘米之间（*H* = *F*[2]）。那么，该人体重超过90公斤（*W* = *E*[3]）的概率是多少？在统计学中，这个概率表示为
    *p*(*W* = *E*[3]|*H* = *F*[2]）。它读作“在 *H* = *F*[2] 的条件下 *W* = *E*[3] 的概率”，也称为“在
    *H* = *F*[2] 条件下 *W* = *E*[3] 的概率”。
- en: 'This is an example of *conditional probability*. Note that if we are given
    that the height is between 160 and 183 cm (*H* = *F*[2]), our universe is restricted
    to the second row of table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob).
    In particular, our population size is not 100,000 (that is, the entire population
    of Statsville). Rather, it is 48,000: the size of the population satisfying the
    given condition *H* = *F*[2]. Using the frequentist definition,'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个条件概率的例子。请注意，如果我们知道身高在160至183厘米之间（*H* = *F*[2]），我们的宇宙就限制在表 [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    的第二行。特别是，我们的种群规模不是100,000（即Statsville的全部人口）。而是48,000：满足给定条件 *H* = *F*[2] 的种群规模。使用频率主义定义，
- en: '![](../../OEBPS/Images/eq_06-00-e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-00-e.png)'
- en: Table [6.2](../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob)
    shows table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob) with conditional
    probabilities added.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6.2](../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob) 展示了在条件概率添加后的表
    [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)。
- en: Table 6.2 Example population sizes and joint, marginal, and conditional probabilities
    for variables *W* = {*E*[1], *E*[2], *E*[3]} and *H* = {*F*[1], *F*[2], *F*[3]}
    weights and heights of adult Statsville residents). (This is table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    with conditional probabilities added.)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6.2 示例人口规模以及变量 *W* = {*E*[1], *E*[2], *E*[3]} 和 *H* = {*F*[1], *F*[2], *F*[3]}
    的联合、边缘和条件概率（成年Statsville居民的体重和身高）。（这是添加了条件概率的表 [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)）。 '
- en: '|  | Less than 60 kg (*E*[1]) | Between 60 and 90 kg (*E*[2]) | More than 90
    kg (*E*[3]) | Marginals for *F*s |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | 小于60 kg (*E*[1]) | 60至90 kg (*E*[2]) | 大于90 kg (*E*[3]) | F*s 的边缘概率 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Less than 160 cm** (***F*[1]**) | pop. = 20,000*p*(*E*[1], *F*[1]) = 0.2*p*(*E*[1]&#124;
    *F*[1]) = *p*(*E*[1], *F*[1]) / *p*(*F*[1]) = 0.77*p*(*F*[1]&#124; *E*[1]) = *p*(*E*[1],
    *F*[1]) / *p*(*E*[1]) = 0.77 | pop. = 4,000*p*(*E*[2], *F*[1]) = 0.04*p*(*E*[2]&#124;
    *F*[1]) = *p*(*E*[2], *F*[1]) / *p*(*F*[1]) = 0.154*p*(*F*[1]&#124; *E*[2]) =
    *p*(*E*[2], *F*[1]) / *p*(*E*[2]) = 0.083 | pop. = 2,000*p*(*E*[3], *F*[1]) =
    0.02*p*(*E*[3]&#124; *F*[1]) = *p*(*E*[3], *F*[1]) / *p*(*F*[1]) = 0.077*p*(*F*[1]&#124;
    *E*[3]) = *p*(*E*[3], *F*[1]) / *p*(*E*[3]) = 0.077 | pop. = 26,000;*p*(*F*[1])
    = 0.2+ 0.04 + 0.02 = 0.26 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **小于160 cm** (***F*[1]**) | 人口 = 4,000*p*(*E*[1], *F*[1]) = 0.04*p*(*E*[1]&#124;
    *F*[1]) = *p*(*E*[1], *F*[1]) / *p*(*F*[1]) = 0.154*p*(*F*[1]&#124; *E*[1]) =
    *p*(*E*[1], *F*[1]) / *p*(*E*[1]) = 0.154 | 人口 = 2,000*p*(*E*[2], *F*[1]) = 0.02*p*(*E*[2]&#124;
    *F*[1]) = *p*(*E*[2], *F*[1]) / *p*(*F*[1]) = 0.077*p*(*F*[1]&#124; *E*[2]) =
    *p*(*E*[2], *F*[1]) / *p*(*E*[2]) = 0.077 | 人口 = 1,000*p*(*E*[3], *F*[1]) = 0.01*p*(*E*[3]&#124;
    *F*[1]) = *p*(*E*[3], *F*[1]) / *p*(*F*[1]) = 0.037*p*(*F*[1]&#124; *E*[3]) =
    *p*(*E*[3], *F*[1]) / *p*(*E*[3]) = 0.037 | 人口 = 26,000;*p*(*F*[1]) = 0.04 + 0.02
    + 0.01 = 0.07 |'
- en: '| **Between 160 cm and 183 cm** (*F*[2]) | pop. = 4,000*p*(*E*[1], *F*[2])
    = 0.04*p*(*E*[1]&#124; *F*[2]) = *p*(*E*[1], *F*[2]) / *p*(*F*[2]) = 0.083*p*(*F*[2])&#124;
    *E*[1]) = *p*(*E*[1], *F*[2]) / *p*(*E*[1]) = 0.154 | pop. = 40,000*p*(*E*[2],
    *F*[2]) = 0.4*p*(*E*[2]&#124; *F*[2]) = *p*(*E*[2], *F*[2]) / *p*(*F*[2]) = 0.83*p*(*F*[2])&#124;
    *E*[2]) = *p*(*E*[2], *F*[2]) / *p*(*E*[2]) = 0.83 | pop. = 4,000*p*(*E*[3], *F*[2])
    = 0.04*p*(*E*[3]&#124; *F*[2]) = *p*(*E*[3], *F*[2]) / *p*(*F*[2]) = 0.083*p*(*F*[2])&#124;
    *E*[3]) = *p*(*E*[3], *F*[2]) / *p*(*E*[3]) = 0.154 | pop. = 48,000;*p*(*F*[2])
    = 0.04 + 0.4 + 0.04 = 0.48 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **160 cm至183 cm之间** (*F*[2]) | 人口 = 4,000*p*(*E*[1], *F*[2]) = 0.04*p*(*E*[1]&#124;
    *F*[2]) = *p*(*E*[1], *F*[2]) / *p*(*F*[2]) = 0.083*p*(*F*[2])&#124; *E*[1]) =
    *p*(*E*[1], *F*[2]) / *p*(*E*[1]) = 0.154 | 人口 = 40,000*p*(*E*[2], *F*[2]) = 0.4*p*(*E*[2]&#124;
    *F*[2]) = *p*(*E*[2], *F*[2]) / *p*(*F*[2]) = 0.83*p*(*F*[2])&#124; *E*[2]) =
    *p*(*E*[2], *F*[2]) / *p*(*E*[2]) = 0.83 | 人口 = 4,000*p*(*E*[3], *F*[2]) = 0.04*p*(*E*[3]&#124;
    *F*[2]) = *p*(*E*[3], *F*[2]) / *p*(*F*[2]) = 0.083*p*(*F*[2])&#124; *E*[3]) =
    *p*(*E*[3], *F*[2]) / *p*(*E*[3]) = 0.154 | 人口 = 48,000;*p*(*F*[2]) = 0.04 + 0.4
    + 0.04 = 0.48 |'
- en: '| **More than 183 cm** (*F*[3]) | pop. = 2,000*p*(*E*[1], *F*[3]) = 0.02*p*(*E*[1]&#124;*F*[3])
    = *p*(*E*[1], *F*[3]) / *p*(*F*[3]) = 0.077*p*(*F*[3]&#124;*E*[1]) = *p*(*E*[1],
    *F*[33]) / *p*(*E*[1]) = 0.077 | pop. = 4,000*p*(*E*[2], *F*[3]) = 0.04*p*(*E*[2]&#124;*F*[3])
    = *p*(*E*[2], *F*[3]) / *p*(*F*[3]) = 0.154*p*(*F*[3]&#124;*E*[2]) = *p*(*E*[2],
    *F*[33]) / *p*(*E*[2]) = 0.083 | pop. = 20,000*p*(*E*[3], *F*[3]) = 0.2*p*(*E*[3]&#124;*F*[3])
    = *p*(*E*[3], *F*[3]) / *p*(*F*[3]) = 0.77*p*(*F*[3]&#124;*E*[3]) = *p*(*E*[3],
    *F*[33]) / *p*(*E*[3]) = 0.77 | pop. = 26,000;*p*(*F*[3]) = 0.02 + 0.04 + 0.2
    = 0.26 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **大于183 cm** (*F*[3]) | 人口 = 2,000*p*(*E*[1], *F*[3]) = 0.02*p*(*E*[1]&#124;*F*[3])
    = *p*(*E*[1], *F*[3]) / *p*(*F*[3]) = 0.077*p*(*F*[3]&#124;*E*[1]) = *p*(*E*[1],
    *F*[33]) / *p*(*E*[1]) = 0.077 | 人口 = 4,000*p*(*E*[2], *F*[3]) = 0.04*p*(*E*[2]&#124;*F*[3])
    = *p*(*E*[2], *F*[3]) / *p*(*F*[3]) = 0.154*p*(*F*[3]&#124;*E*[2]) = *p*(*E*[2],
    *F*[33]) / *p*(*E*[2]) = 0.083 | 人口 = 20,000*p*(*E*[3], *F*[3]) = 0.2*p*(*E*[3]&#124;*F*[3])
    = *p*(*E*[3], *F*[3]) / *p*(*F*[3]) = 0.77*p*(*F*[3]&#124;*E*[3]) = *p*(*E*[3],
    *F*[33]) / *p*(*E*[3]) = 0.77 | 人口 = 26,000;*p*(*F*[3]) = 0.02 + 0.04 + 0.2 =
    0.26 |'
- en: '| **Marginals for** ***E*s** | *p*(*E*[1]) = 0.2 + 0.04 + 0.02 = 0.26 | *p*(*E*[2])
    = 0.04 + 0.4 + 0.04 = 0.48 | *p*(*E*[3]) = 0.02 + 0.04 + 0.2 = 0.26 | Total pop.=
    100,000;Total prob = 1 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **E*s** 的边缘概率 | *p*(*E*[1]) = 0.2 + 0.04 + 0.02 = 0.26 | *p*(*E*[2]) = 0.04
    + 0.4 + 0.04 = 0.48 | *p*(*E*[3]) = 0.02 + 0.04 + 0.2 = 0.26 | 总人口 = 100,000;总概率
    = 1 |'
- en: 6.1.3 Bayes’ theorem
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 贝叶斯定理
- en: As demonstrated in table [6.2](../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob),
    in general,
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [6.2](../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob) 所示，一般来说，
- en: '![](../../OEBPS/Images/eq_06-00-f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-00-f.png)'
- en: 'This is the essence of Bayes’ theorem. We can generalize and say the following:
    given two random variables *X* and *Y*, the conditional probability of *X* taking
    the value *x* given the condition that *Y* has value ![](../../OEBPS/Images/AR_y.png)
    is given by the ratio of the joint probability of the two and the marginal probability
    of the condition'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是贝叶斯定理的精髓。我们可以推广并说以下内容：给定两个随机变量 *X* 和 *Y*，在 *Y* 取值 ![](../../OEBPS/Images/AR_y.png)
    的条件下，*X* 取值 *x* 的条件概率由这两个变量的联合概率与条件的边缘概率之比给出
- en: '![](../../OEBPS/Images/eq_06-01.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-01.png)'
- en: Equation 6.1
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.1
- en: Sometimes we drop the names of the random variable and just use the values.
    Using such notation, Bayes’ theorem can be stated as
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们会省略随机变量的名称，直接使用数值。使用这种符号，贝叶斯定理可以表述为
- en: '![](../../OEBPS/Images/eq_06-01-a.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-01-a.png)'
- en: Note that the denominator is the marginal probability, which can be obtained
    by summing over the joint probabilities. For instance, for continuous variables,
    Bayes’ theorem can be written as
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，分母是边缘概率，可以通过对联合概率求和得到。例如，对于连续变量，贝叶斯定理可以写成
- en: '![](../../OEBPS/Images/eq_06-01-b.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6-1-b](../../OEBPS/Images/eq_06-01-b.png)'
- en: 'Bayes’ theorem can be generalized further to more than two variables and multiple
    dimensions:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理可以进一步推广到超过两个变量和多维：
- en: '![](../../OEBPS/Images/eq_06-02.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6-2](../../OEBPS/Images/eq_06-02.png)'
- en: Equation 6.2
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.2
- en: '![](../../OEBPS/Images/eq_06-03.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6-3](../../OEBPS/Images/eq_06-03.png)'
- en: Equation 6.3
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.3
- en: It is common practice to drop the name of the random variable uppercase), retain
    only the value (lowercase), and state these equations informally as
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的做法是省略随机变量的名称（大写），只保留值（小写），并且非正式地陈述这些方程
- en: '![](../../OEBPS/Images/eq_06-03-a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6-3-a](../../OEBPS/Images/eq_06-03-a.png)'
- en: What happens if the random variables are independent? Well, let’s check out
    equation [6.1](../Text/06.xhtml#eq-bayes-theorem-2var). If *X* and *Y* are independent,
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果随机变量是独立的会发生什么？好吧，让我们检查一下方程 [6.1](../Text/06.xhtml#eq-bayes-theorem-2var)。如果
    *X* 和 *Y* 是独立的，
- en: '![](../../OEBPS/Images/eq_06-03-b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6-3-b](../../OEBPS/Images/eq_06-03-b.png)'
- en: and hence
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: '![](../../OEBPS/Images/eq_06-03-c.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6-3-c](../../OEBPS/Images/eq_06-03-c.png)'
- en: 'This makes intuitive sense: if *X* and *Y* are independent, knowing *Y* does
    not make any difference to *p*(*X* = *x*), so the probability of *X* given *Y*
    is the same as the probability of *X*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得直观上有意义：如果 *X* 和 *Y* 是独立的，知道 *Y* 对 *p*(*X* = *x*) 没有任何影响，所以给定 *Y* 的 *X* 的概率与
    *X* 的概率相同。
- en: 6.2 Entropy
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 熵
- en: 'Suppose a daily meteorological bulletin informs the folks in the United States
    whether it rained in the Sahara desert yesterday. Is there much overall information
    in that bulletin? Not really—it almost always reports the obvious. The probability
    of “no rain” is overwhelmingly high it is almost certain that there will be no
    rain), and the uncertainty associated with the outcome is very low. Even without
    the bulletin, if we guess the outcome “no rain,” we will be right almost every
    time. Similarly, a daily news bulletin telling us whether it rained yesterday
    in Cherapunji, India—a place where it pretty much rains all the time—has little
    informational content because we can guess the results with high certainty even
    without the bulletin. Stated another way, the uncertainty associated with the
    probability distributions of “rain vs. no rain in the Sahara” and or “rain vs.
    no rain in Cherapunji” is low. This is a direct consequence of the fact that the
    probability of one of the events is close to 1 and the probabilities of the other
    events are near 0: the probability density function (PDF) has a very tall peak
    at one location and very low heights elsewhere.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一份每日气象公告告知美国人民昨天撒哈拉沙漠是否下雨。这份公告中包含很多整体信息吗？实际上并没有——它几乎总是报告显而易见的事情。不下雨的概率非常高，几乎可以肯定不会下雨），与结果相关的不确定性非常低。即使没有公告，如果我们猜测结果“不下雨”，我们几乎每次都会猜对。同样，一份每日新闻公告告诉我们昨天印度切拉彭齐是否下雨——一个几乎总是下雨的地方——包含很少的信息内容，因为我们即使没有公告也可以非常确定地猜测结果。换句话说，与“撒哈拉沙漠下雨与不下雨”或“切拉彭齐下雨与不下雨”的概率分布相关的风险很低。这是这样一个事实的直接后果：其中一个事件的概率接近
    1，而其他事件的概率接近 0：概率密度函数（PDF）在一个位置上非常高大，在其他地方则非常低。
- en: On the other hand, a daily bulletin reporting whether it rained in San Francisco
    is of considerable interest because the probability of “rain” and “no rain” are
    comparable. Without the bulletin, we cannot guess the result with much certainty.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，一份每日报告是否在旧金山下雨的公告具有相当大的兴趣，因为“下雨”和“不下雨”的概率相当。没有这份公告，我们无法有很大把握地猜测结果。
- en: The concept of *entropy* attempts to quantify the uncertainty associated with
    a chancy event. If the probability for any one event is overwhelmingly high (meaning
    the probabilities of other events are very low since the sum is 1), the uncertainty
    is low—we pretty much know that the high-probability event will occur. On the
    other hand, if there are multiple events with comparable high probabilities, uncertainty
    is high—we cannot predict which event will occur. Entropy captures this notion
    of uncertainty in a system. Let’s look at another example.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 熵的概念试图量化与随机事件相关的不确定性。如果任何单个事件发生的概率极高（意味着其他事件发生的概率非常低，因为总和为1），则不确定性低——我们几乎可以肯定高概率事件会发生。另一方面，如果有多个事件具有相当高的概率，则不确定性高——我们无法预测哪个事件会发生。熵捕捉了系统中的这种不确定性概念。让我们看看另一个例子。
- en: 'Suppose we have tiny images, four pixels wide by four pixels high, and each
    pixel is one of four possible colors: G(reen), R(ed), B(lue), or Y(ellow). Two
    such images are shown in figure [6.1](../Text/06.xhtml#fig-entropy-img). We want
    to encode such images. The simplest thing to do is to use a two-bit representation
    for each color:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一些很小的图像，宽度为四个像素，高度为四个像素，每个像素可以是四种可能的颜色之一：G(绿色)、R(红色)、B(蓝色)或Y(黄色)。图6.1中显示了两个这样的图像。我们想要对这些图像进行编码。最简单的事情是为每种颜色使用两个位的表示：
- en: G(*reen*) = 00
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: G(绿色) = 00
- en: R(*ed*) = 01
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: R(红色) = 01
- en: B(*lue*) = 10
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: B(蓝色) = 10
- en: Y(*ellow*) = 11
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Y(黄色) = 11
- en: '![](../../OEBPS/Images/CH06_F01_Chaudhury.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F01_Chaudhury.png)'
- en: 'Figure 6.1 Two 4 × 4 images with different pixel color distributions. In the
    left image, the four colors R, G, B, and Y are equally probable. In the right
    image, one color (green) is much likelier than the others. The left image has
    higher entropy (uncertainty): we cannot predict any color with much certainty.
    In the right image, we can predict green with relative certainty.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1展示了两个4×4像素的图像，像素颜色分布不同。在左侧的图像中，四种颜色R、G、B和Y出现的概率相等。在右侧的图像中，一种颜色（绿色）比其他颜色更可能。左侧的图像具有更高的熵（不确定性）：我们无法非常确定地预测任何颜色。在右侧的图像中，我们可以相对确定地预测绿色。
- en: The entire 16-pixel image on the left can be represented by the string 00 00
    00 00 01 01 01 01 10 10 10 10 11 11 11 11\. Here, we have iterated over the pixels
    in *raster scan order*, left to right and top to bottom. The total number of bits
    needed to store the 16-pixel image is 16 × 2 = 32 bits. The right image can be
    represented as 00 00 00 00 00 00 00 00 00 00 00 00 01 01 10 11\. The total number
    of bits needed is 16 × 2 = 32 bits. Both images need the same amount of storage.
    But is this optimal?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的整个16像素图像可以用字符串00 00 00 00 01 01 01 01 10 10 10 10 11 11 11 11表示。在这里，我们已经按照*光栅扫描顺序*迭代了像素，从左到右，从上到下。存储16像素图像所需的位数总数是16×2=32位。右侧的图像可以表示为00
    00 00 00 00 00 00 00 00 00 00 00 01 01 10 11。所需的位数总数是16×2=32位。两种图像需要的存储量相同。但是，这是最优的吗？
- en: 'Consider the right-hand image. The color G appears much more frequently than
    the others. We can use this fact to reduce the total number of bits required to
    store the image. It is not mandatory to use the same number of bits to represent
    each color. How about using shorter representations for the more frequently occurring
    (higher-probability) colors and longer representations for the infrequent (lower-probability)
    colors? This is the core principle behind the technique of *variable bit-rate
    coding*. For instance, we can use the following representation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑右侧的图像。颜色G出现的频率远高于其他颜色。我们可以利用这个事实来减少存储图像所需的位数总数。不一定需要使用相同数量的位来表示每种颜色。对于出现频率更高（概率更高）的颜色使用更短的表示，对于出现频率较低（概率较低）的颜色使用更长的表示怎么样？这是*可变位率编码*技术背后的核心原理。例如，我们可以使用以下表示：
- en: G(*reen*) = 0
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: G(绿色) = 0
- en: R(*ed*) = 10
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: R(红色) = 10
- en: B(*lue*) = 110
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: B(蓝色) = 110
- en: Y(*ellow*) = 111
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Y(黄色) = 111
- en: The right-hand image can thus be represented as 0 0 0 0 0 0 0 0 0 0 0 0 10 10
    110 111.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，右侧的图像可以表示为0 0 0 0 0 0 0 0 0 0 0 0 10 10 110 111。
- en: 'NOTE This is an example of what is known as *prefix coding*: no two colors
    share the same prefix. It enables us to identify the color as soon as we see its
    code. For instance, if we see a 0 bit at the beginning, we immediately know the
    color is green since no other color code starts with 0. If we see 10, we immediately
    know the color is red since no other color code starts with 10, and so on.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这是一个被称为*前缀编码*的例子：没有两种颜色共享相同的前缀。这使我们能够在看到代码时立即识别颜色。例如，如果我们看到开头的0位，我们立即知道颜色是绿色，因为没有任何其他颜色代码以0开头。如果我们看到10，我们立即知道颜色是红色，因为没有任何其他颜色代码以10开头，以此类推。
- en: With this new color code, we need 12 × 1 = 12 bits to store the 12 green pixels,
    2 × 2 = 4 bits to store the 2 red pixels, 1 × 3 = 3 bits to store the single blue
    pixel, and 1 × 3 = 3 bits to store the single yellow pixel—a total of 22 pixels.
    Equivalently, we need 22/16 = 1.375 bits per pixel. This is less than the 32 pixels
    at 2 bits per pixel we needed with the simple fixed bit-rate coding.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种新的颜色代码，我们需要 12 × 1 = 12 比特来存储 12 个绿色像素，2 × 2 = 4 比特来存储 2 个红色像素，1 × 3 = 3
    比特来存储单个蓝色像素，以及 1 × 3 = 3 比特来存储单个黄色像素——总共 22 个像素。相当于我们需要 22/16 = 1.375 比特每像素。这比我们使用简单的固定比特率编码所需的
    32 个像素每比特要少。
- en: NOTE You have just learned about Huffman encoding, an important technique in
    image compression.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您刚刚学习了霍夫曼编码，这是图像压缩中的一个重要技术。
- en: 'Does the new representation result in smaller storage for the left-hand image?
    There, we need 4 × 1 = 4 bits to store the four green pixels, 4 × 2 = 8 pixels
    to store the four red pixels, 4 × 3 = 12 bits to store the four blue pixels, and
    4 × 3 = 12 bits to store the single yellow pixel: a total of 36 pixels at 36/16
    = 2.25 bits per pixel. Here, variable bit-rate coding does worse than fixed bit-rate
    coding.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 新的表示是否会导致左侧图像的存储空间更小？在那里，我们需要 4 × 1 = 4 比特来存储四个绿色像素，4 × 2 = 8 比特来存储四个红色像素，4
    × 3 = 12 比特来存储四个蓝色像素，以及 4 × 3 = 12 比特来存储单个黄色像素：总共 36 个像素，36/16 = 2.25 比特每像素。在这里，可变比特率编码比固定比特率编码效果更差。
- en: So, the probability distribution of the various pixel colors in the image affects
    how much compression can be achieved. If the distribution of pixel colors is such
    that a few colors are much more probable than others, we can assign shorter codes
    to them to reduce storage for the whole image. Viewed another way, if low uncertainty
    is associated with the system—certain colors are more or less certain to occur—we
    can achieve high compression. We assign shorter codes to nearly certain colors,
    resulting in compression. On the other hand, if high uncertainty is associated
    with the system—all colors are more or less equally probable, and no color occurs
    with high certainty—variable bit-rate coding will not be very effective. How do
    we quantify this notion? In other words, can we examine the pixel color distribution
    in an image and estimate whether variable bit-rate coding will be effective? The
    answer again is entropy. Formally,
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，图像中各种像素颜色的概率分布会影响可以实现的压缩程度。如果像素颜色的分布是这样的，即某些颜色比其他颜色更有可能，我们可以为它们分配较短的代码，以减少整个图像的存储空间。从另一个角度来看，如果系统与低不确定性相关——某些颜色或多或少是肯定发生的——我们可以实现高压缩。我们为几乎肯定的颜色分配较短的代码，从而实现压缩。另一方面，如果系统与高不确定性相关——所有颜色或多或少是同样可能的，并且没有颜色以高概率发生——可变比特率编码将不会非常有效。我们如何量化这个概念？换句话说，我们能否检查图像中的像素颜色分布并估计可变比特率编码是否有效？答案仍然是熵。正式地，
- en: Entropy measures the overall uncertainty associated with a probability distribution.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 熵衡量与概率分布相关的整体不确定性。
- en: Entropy is a measure that is *high* if everything is more or less equally probable
    and *low* if a few items have a much higher probability than the others. It measures
    the uncertainty in the system. If everything is equally probable, we cannot predict
    any one item with any extra certainty. Such a system has high entropy. On the
    other hand, if some items are much more probable than others, we can predict them
    with relative certainty. Such a system has low entropy.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是一个当所有事情或多或少同样可能时较高的度量，而当少数项目比其他项目有更高的概率时，它是一个较低的度量。它衡量系统中的不确定性。如果所有事情都是同样可能的，我们无法以任何额外的确定性预测任何一项。这样的系统具有高熵。另一方面，如果某些项目比其他项目更有可能，我们可以相对确定地预测它们。这样的系统具有低熵。
- en: In the discrete univariate case, for a random variable *X* that can take any
    one of the discrete values *x*[1], *x*[2], *x*[3], ⋯, *x[n]* with probabilities
    *p*(*x*[1]), *p*(*x*[2]), *p*(*x*[3]), ⋯, *p*(*x[n]*), entropy is defined as
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在离散单变量情况下，对于可以取离散值 *x*[1]、*x*[2]、*x*[3]、⋯、*x[n]* 的随机变量 *X*，其概率为 *p*(*x*[1])、*p*(*x*[2])、*p*(*x*[3])、⋯、*p*(*x[n]*)，熵定义为
- en: '![](../../OEBPS/Images/eq_06-04.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/eq_06-04.png)'
- en: Equation 6.4
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.4
- en: The logarithm is taken with respect to the natural base *e*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对数是以自然底数 *e* 为底。
- en: Let’s apply equation [6.4](../Text/06.xhtml#eq-entropy-discr-univar4) to the
    images in figure [6.1](../Text/06.xhtml#fig-entropy-img) to see if the results
    agree with our intuition. The computations are shown in table [6.3](../Text/06.xhtml#tab-entropies-img).
    The notion of entropy applies to continuous and multidimensional random variables
    equally well.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将方程[6.4](../Text/06.xhtml#eq-entropy-discr-univar4)应用于图[6.1](../Text/06.xhtml#fig-entropy-img)中的图像，看看结果是否与我们的直觉一致。计算结果显示在表[6.3](../Text/06.xhtml#tab-entropies-img)中。熵的概念同样适用于连续和多维随机变量。
- en: Table 6.3 Entropy computation for the pair of images in figure [6.1](../Text/06.xhtml#fig-entropy-img).
    The right-hand image has lower entropy and can be compressed more.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 图[6.1](../Text/06.xhtml#fig-entropy-img)中图像对的熵计算。右侧图像的熵较低，可以压缩得更多。
- en: '| Left image | Right image |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 左图像 | 右图像 |'
- en: '| --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| *x*[1] = *G*, *p*(*x*[1]) = 4/16 = 0.25 | *x*[1] = *G*, *p*(*x*[1]) = 12/16
    = 0.75 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] = *G*, *p*(*x*[1]) = 4/16 = 0.25 | *x*[1] = *G*, *p*(*x*[1]) = 12/16
    = 0.75 |'
- en: '| *x*[2] = *R* , *p*(*x*[2]) = 4/16 = 0.25 | *x*[2] = *R* , *p*(*x*[2]) = 2/16
    = 0.125 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| *x*[2] = *R* , *p*(*x*[2]) = 4/16 = 0.25 | *x*[2] = *R* , *p*(*x*[2]) = 2/16
    = 0.125 |'
- en: '| *x*[3] = *B*, *p*(*x*[3]) = 4/16 = 0.25 | *x*[3] = *B*, *p*(*x*[3]) = 1/16
    = 0.0625 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| *x*[3] = *B*, *p*(*x*[3]) = 4/16 = 0.25 | *x*[3] = *B*, *p*(*x*[3]) = 1/16
    = 0.0625 |'
- en: '| *x*[4] = *Y*, *p*(*x*[4]) = 4/16 = 0.25 | *x*[4] = *Y*, *p*(*x*[4]) = 1/16
    = 0.0625 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| *x*[4] = *Y*, *p*(*x*[4]) = 4/16 = 0.25 | *x*[4] = *Y*, *p*(*x*[4]) = 1/16
    = 0.0625 |'
- en: '| ℍ = −(0.25 *log*(0.25)+0.25 *log*(0.25) + + 0.25 *log*(0.25) + 0.25 *log*(0.25))
    = 1.386294 | ℍ = −(0.75 *log*(0.75)+0.125 *log*(0.125) + + 0.0625 *log*(0.0625)
    + 0.0625 *log*(0.0625)) = 0.822265 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ℍ = −(0.25 *log*(0.25)+0.25 *log*(0.25) + + 0.25 *log*(0.25) + 0.25 *log*(0.25))
    = 1.386294 | ℍ = −(0.75 *log*(0.75)+0.125 *log*(0.125) + + 0.0625 *log*(0.0625)
    + 0.0625 *log*(0.0625)) = 0.822265 |'
- en: For a univariate continuous random variable *X* that takes values *x* ∈ {−∞,∞}
    with probabilities *p*(*x*),
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个单变量连续随机变量 *X*，其取值 *x* ∈ {−∞,∞}，其概率为 *p*(*x*)，
- en: '![](../../OEBPS/Images/eq_06-05.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/eq_06-05.png)'
- en: Equation 6.5
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 方程6.5
- en: For a continuous multidimensional random variable *X* that takes values ![](../../OEBPS/Images/AR_x.png)
    in the domain *D*, (![](../../OEBPS/Images/AR_x.png) ∈ *D*) with probabilities
    *p*(![](../../OEBPS/Images/AR_x.png)),
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个在域 *D* 中取值 ![](../../OEBPS/Images/AR_x.png) 的连续多维随机变量 *X*，(![](../../OEBPS/Images/AR_x.png)
    ∈ *D*)，其概率为 *p*(![](../../OEBPS/Images/AR_x.png))，
- en: '![](../../OEBPS/Images/eq_06-06.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/eq_06-06.png)'
- en: Equation 6.6
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 方程6.6
- en: 6.2.1 Geometrical intuition for entropy
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 熵的几何直觉
- en: 'Geometrically speaking, entropy is a function of how lopsided the PDF is (see
    figure [6.2](../Text/06.xhtml#fig-entropy-density-cloud)). If all inputs are more
    or less equally probable, the density function is more or less flat and uniform
    in height everywhere (see figure [6.2a](../Text/06.xhtml#fig-entropy-high-density)).
    The corresponding sample point cloud has a diffused mass: there are no regions
    with a high concentration of points. Such a system has high uncertainty or high
    entropy (see figure [6.2b](../Text/06.xhtml#fig-entropy-high-cloud)).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何上讲，熵是PDF倾斜程度的函数（见图[6.2](../Text/06.xhtml#fig-entropy-density-cloud)）。如果所有输入的概率大致相等，密度函数在所有地方的高度大致平坦且均匀（见图[6.2a](../Text/06.xhtml#fig-entropy-high-density)）。相应的样本点云具有扩散的质量：没有点的高浓度区域。这样的系统具有高不确定性或高熵（见图[6.2b](../Text/06.xhtml#fig-entropy-high-cloud)）。
- en: '![](../../OEBPS/Images/CH06_F02a_Chaudhury.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH06_F02a_Chaudhury.png)'
- en: (a) Flatter, wider PDFs correspond to higher entropy. Entropy = 12.04.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 较平坦、较宽的PDF对应较高的熵。熵 = 12.04。
- en: '![](../../OEBPS/Images/CH06_F02b_Chaudhury.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH06_F02b_Chaudhury.png)'
- en: (b) Diffused sample point clouds correspond to higher entropy.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 扩散的样本点云对应较高的熵。
- en: '![](../../OEBPS/Images/CH06_F02c_Chaudhury.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH06_F02c_Chaudhury.png)'
- en: (c) Taller, narrower peaks in probability density functions correspond to lower
    entropy. Entropy = 7.44.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 在概率密度函数中较高的、较窄的峰值对应较低的熵。熵 = 7.44。
- en: '![](../../OEBPS/Images/CH06_F02d_Chaudhury.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH06_F02d_Chaudhury.png)'
- en: (d) Concentrated sample point clouds correspond to lower entropy.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 集中的样本点云对应较低的熵。
- en: Figure 6.2 Entropies of peaked and flat distributions
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 峰值和平坦分布的熵
- en: On the other hand, if a few of all the possible inputs have disproportionately
    high probabilities, the PDF has tall peaks in some regions and low heights elsewhere
    (see figure [6.2c](../Text/06.xhtml#fig-entropy-low-density)). The corresponding
    sample point cloud has regions of high concentration matching the peaks in the
    density function and low concentration elsewhere (see figure [6.2d](../Text/06.xhtml#fig-entropy-low-cloud)).
    Such a system has low uncertainty and low entropy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果所有可能输入中的一些具有不成比例的高概率，PDF 在某些区域有高峰，而在其他地方高度较低（见图 [6.2c](../Text/06.xhtml#fig-entropy-low-density)）。相应的样本点云在密度函数的峰值附近有高浓度区域，而在其他地方浓度较低（见图
    [6.2d](../Text/06.xhtml#fig-entropy-low-cloud)）。这样的系统具有低不确定性和低熵。
- en: NOTE Since the sum of all the probabilities is 1, if a few are high, the others
    have to be low. We cannot have all high or all low probabilities.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于所有概率之和为 1，如果其中一些概率较高，其他概率必须较低。我们不能同时所有概率都高或都低。
- en: 6.2.2 Entropy of Gaussians
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 高斯分布的熵
- en: The wider a Gaussian is, the less peaked it is, and the closer it is to being
    a uniform distribution. A univariate Gaussian’s variance, *σ*, determines its
    fatness (see figure [5.10b](../Text/05.xhtml#fig-multi-univar-gauss)). Consequently,
    we expect a Gaussian’s entropy to be an increasing function of *σ*. Indeed, that
    is the case. In this section, we derive the entropy of a Gaussian in the univariate
    case and simply state the result for the multivariate case.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 越宽的高斯分布，其峰值越低，越接近均匀分布。一元高斯分布的方差 *σ* 决定了其胖瘦（见图 [5.10b](../Text/05.xhtml#fig-multi-univar-gauss)）。因此，我们预计高斯分布的熵是
    *σ* 的增函数。确实如此。在本节中，我们推导了一元情况下高斯分布的熵，并简单地陈述了多元情况下的结果。
- en: For a random variable *x* whose PDF is given by equation [5.22](../Text/05.xhtml#eq-univar-normal)
    (repeated here for convenience),
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有方程 [5.22](../Text/05.xhtml#eq-univar-normal)（为方便起见在此重复）给出的 PDF 的随机变量 *x*，
- en: '![](../../OEBPS/Images/eq_06-06-a.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-06-a.png)'
- en: From that, we get
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 从中我们得到
- en: '![](../../OEBPS/Images/eq_06-06-b.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-06-b.png)'
- en: Using equation [6.6](../Text/06.xhtml#eq-entropy-cont-univar), the entropy is
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方程 [6.6](../Text/06.xhtml#eq-entropy-cont-univar)，熵为
- en: '![](../../OEBPS/Images/eq_06-06-c.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-06-c.png)'
- en: Remembering the probability sum rule from equation [5.6](../Text/05.xhtml#eq-continuous-prob-sum),
    ∫[*x* = −∞]^∞ *p*(*x*) *dx* = 1, we get
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 记住方程 [5.6](../Text/05.xhtml#eq-continuous-prob-sum) 中的概率和规则，∫[*x* = −∞]^∞ *p*(*x*)
    *dx* = 1，我们得到
- en: '![](../../OEBPS/Images/eq_06-06-d.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-06-d.png)'
- en: Now, by definition (see section [5.7.2](../Text/05.xhtml#sec-var-covar-std)),
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据定义（见第 [5.7.2](../Text/05.xhtml#sec-var-covar-std) 节），
- en: '![](../../OEBPS/Images/eq_06-06-e.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-06-e.png)'
- en: Hence,
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '![](../../OEBPS/Images/eq_06-07.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-07.png)'
- en: Equation 6.7
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.7
- en: 'Entropy for multivariate Gaussians is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量高斯分布的熵如下：
- en: '![](../../OEBPS/Images/eq_06-08.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-08.png)'
- en: Equation 6.8
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.8
- en: Listing 6.1 shows the Python PyTorch code to compute the entropy of a Gaussian.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 展示了计算高斯分布熵的 Python PyTorch 代码。
- en: NOTE Fully functional code to compute the entropy of a Gaussian distribution,
    executable via Jupyter Notebook, can be found at [http://mng.bz/zx7B](http://mng.bz/zx7B).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：用于计算高斯分布熵的完整代码，可通过 Jupyter Notebook 执行，可在[http://mng.bz/zx7B](http://mng.bz/zx7B)找到。
- en: Listing 6.1 Computing the entropy of a Gaussian distribution
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 计算高斯分布的熵
- en: '[PRE0]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Equation [6.7](../Text/06.xhtml#eq-entropy-gauss-univar)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ① 方程 [6.7](../Text/06.xhtml#eq-entropy-gauss-univar)
- en: ② Instantiates a Gaussian distribution
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化高斯分布
- en: ③ Computes the entropy using the direct formulaComputes the entropy using the
    direct formula
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用直接公式计算熵
- en: ④ Computes the entropy using the PyTorch interface
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用 PyTorch 接口计算熵
- en: ⑤ Asserts that the entropies computed two different ways match
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 断言两种不同方式计算的熵相等
- en: 6.3 Cross-entropy
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 交叉熵
- en: 'Consider a *supervised* classification problem where we have to analyze an
    image and identify which of the following objects is present: *cat*, *dog*, *airplane*,
    or *automobile*. We assume that one of these will always be present in our universe
    of images. Given an input image, our machine emits four probabilities: *p*(*cat*),
    *p*(*dog*), *p*(*airplane*), and *p*(*automobile*). During training, for each
    training data instance, we have a ground truth (GT): a known class to which that
    training data instance belongs. We have to estimate how different the network
    output is from the GT—this is the loss for that data instance. We adjust the machine
    parameters to minimize the loss and continue doing so until the loss stops decreasing.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个 *监督* 分类问题，其中我们需要分析一张图像并识别以下哪些对象存在：*猫*，*狗*，*飞机*，或 *汽车*。我们假设在这些图像的宇宙中，这些对象之一总是存在的。给定一个输入图像，我们的机器输出四个概率：*p*(*猫*)，*p*(*狗*)，*p*(*飞机*)，和
    *p*(*汽车*)。在训练过程中，对于每个训练数据实例，我们有一个真实值（GT）：一个已知的类别，该训练数据实例属于该类别。我们必须估计网络输出与 GT 之间的差异——这是该数据实例的损失。我们调整机器参数以最小化损失，并继续这样做，直到损失停止下降。
- en: How do we quantitatively estimate the loss—the difference between the known
    GT and the probabilities of various classes emitted by the network? One principled
    approach is to use the cross-entropy loss. Here is how it works.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何定量估计损失——已知 GT 与网络发出的各种类别的概率之间的差异？一种原则性的方法是使用交叉熵损失。以下是它是如何工作的。
- en: 'Consider a random variable *X* that can take four possible values: *X* = 1
    signifying *cat*, *X* = 2 signifying *dog*, *X* = 3 signifying *airplane*, and
    *X* = 4 signifying *automobile*. The random variable has the PDF *p*(*X* = 1)
    ≡ *p*(*cat*), *p*(*X* = 2) ≡ *p*(*dog*), *p*(*X* = 3) ≡ *p*(*airplane*), *p*(*X*
    = 4) ≡ *p*(*automobile*). The PDF for a GT, which selects one from the set of
    four possible classes, is a one-hot vector (one of the elements is 1, and the
    others are 0). Such random variables and corresponding PDFs can be associated
    with every GT and machine output. Here are some examples, which are also shown
    graphically in figure [6.3](../Text/06.xhtml#fig-cross-entropy). A PDF for GT
    *cat* (one-hot vector) is shown figure [6.3a](../Text/06.xhtml#fig-cross-entropy-gt):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个随机变量 *X*，它可以取四个可能值：*X* = 1 表示 *猫*，*X* = 2 表示 *狗*，*X* = 3 表示 *飞机*，*X* = 4
    表示 *汽车*。该随机变量的概率密度函数 *p*(*X* = 1) ≡ *p*(*猫*)，*p*(*X* = 2) ≡ *p*(*狗*)，*p*(*X* =
    3) ≡ *p*(*飞机*)，*p*(*X* = 4) ≡ *p*(*汽车*)。对于选择四个可能类别之一的目标真实值（GT），其概率密度函数是一个独热向量（其中一个元素为
    1，其余为 0）。这样的随机变量和相应的概率密度函数可以与每个 GT 和机器输出相关联。以下是一些示例，这些示例也在图 [6.3](../Text/06.xhtml#fig-cross-entropy)
    中以图形方式展示。GT *猫* 的概率密度函数（独热向量）如图 [6.3a](../Text/06.xhtml#fig-cross-entropy-gt)
    所示：
- en: How do we quantitatively estimate the loss—the difference between the known
    GT and the probabilities of various classes emitted by the network? One principled
    approach is to use the cross-entropy loss. Here is how it works.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何定量估计损失——已知 GT 与网络发出的各种类别的概率之间的差异？一种原则性的方法是使用交叉熵损失。以下是它是如何工作的。
- en: '![](../../OEBPS/Images/eq_06-08-a.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-08-a.png)'
- en: 'A PDF for a good prediction is shown figure [6.3b](../Text/06.xhtml#fig-cross-entropy-low-pred):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 良好预测的概率密度函数如图 [6.3b](../Text/06.xhtml#fig-cross-entropy-low-pred) 所示：
- en: '![](../../OEBPS/Images/eq_06-08-b.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-08-b.png)'
- en: 'A PDF for a bad prediction is shown figure [6.3c](../Text/06.xhtml#fig-cross-entropy-high-pred):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 差异预测的概率密度函数如图 [6.3c](../Text/06.xhtml#fig-cross-entropy-high-pred) 所示：
- en: '![](../../OEBPS/Images/eq_06-08-c.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-08-c.png)'
- en: '![](../../OEBPS/Images/CH06_F03a_Chaudhury.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F03a_Chaudhury.png)'
- en: (a) Ground truth probability
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 真实值概率
- en: '![](../../OEBPS/Images/CH06_F03b_Chaudhury.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F03b_Chaudhury.png)'
- en: '(b) Good prediction: probabilities similar to ground truth. Cross-entropy loss
    = 0.22.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 良好预测：概率与真实值相似。交叉熵损失 = 0.22。
- en: '![](../../OEBPS/Images/CH06_F03c_Chaudhury.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F03c_Chaudhury.png)'
- en: '(c) Bad prediction: probabilities dissimilar to ground truth. Cross-entropy
    loss = 1.38.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 差异预测：概率与真实值不相似。交叉熵损失 = 1.38。
- en: Figure 6.3 Cross-entropy loss
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 交叉熵损失
- en: 'Let *X[gt]* denote such a random variable for a specific GT and *p[gt]* denote
    the corresponding PDF. Similarly, let *X[pred]* and *p[pred]* denote the random
    variable and PDF for the machine prediction. Consider the following expression:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让 *X[gt]* 表示特定 GT 的这样一个随机变量，*p[gt]* 表示相应的概率密度函数。同样，让 *X[pred]* 和 *p[pred]* 表示机器预测的随机变量和概率密度函数。考虑以下表达式：
- en: '![](../../OEBPS/Images/eq_06-09.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-09.png)'
- en: Equation 6.9
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.9
- en: 'This is the expression for *cross-entropy*. It is a quantitative measure for
    how dissimilar the two PDFs *p[gt]* and *p[pred]* are: that is, how much error
    will be caused by approximating the PDF *p[gt]* with *p[pred]*. Equivalently,
    cross-entropy measures how well the machine is doing that output the prediction
    *p[pred]* when the correct PDF is *p[gt]*.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是交叉熵的表达式。它是两个概率密度函数 *p[gt]* 和 *p[pred]* 之间差异的定量度量：也就是说，用 *p[pred]* 近似 *p[gt]*
    会造成多少误差。等价地，交叉熵衡量了当正确概率密度函数是 *p[gt]* 时，机器输出预测 *p[pred]* 的效果如何。
- en: 'To gain insight into how ℍ*[c]*(*X[gt]*, *X[pred]*) measures dissimilarity
    between PDFs, examine the expression carefully. Remember that Σ[i]⁴[= 1] *p[gt]*
    (*i*) = Σ[i]⁴[= 1] *p[pred]* (*i*) = 1 (using the probability sum rule from equation
    [5.3](../Text/05.xhtml#eq-discrete-prob-sum)):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解 ℍ*[c]*(*X[gt]*, *X[pred]*) 如何衡量概率密度函数之间的差异，仔细检查该表达式。记住，Σ[i]⁴[= 1] *p[gt]*
    (*i*) = Σ[i]⁴[= 1] *p[pred]* (*i*) = 1（使用方程 [5.3](../Text/05.xhtml#eq-discrete-prob-sum)
    中的概率和规则）：
- en: '**Case 1**: The *i* values where *p[gt]*(*i*) is high (close to 1).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 1**：当 *p[gt]*(*i*) 较高（接近 1）的 *i* 值。'
- en: '**Case 1a**: If *p[pred]*(*i*) is also close to 1, then log (*p[pred]*(*i*))
    will be close to zero (since log 1 = 0). Hence the term *p[gt]*(*i*)log (*p[pred]*(*i*))
    will be close to zero since the product of anything with a near-zero number is
    near zero. These terms will contribute little to ℍ*[c]*(*X[gt]*, *X[pred]*).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 1a**：如果 *p[pred]*(*i*) 也接近 1，那么 log (*p[pred]*(*i*)) 将接近零（因为 log 1 = 0）。因此，*p[gt]*(*i*)log
    (*p[pred]*(*i*)) 将接近零，因为任何数与接近零的数的乘积都接近零。这些项将对 ℍ*[c]*(*X[gt]*, *X[pred]*) 的贡献很小。'
- en: '**Case 1b**: On the other hand, at the *i* values where *p[gt]*(*i*) is high,
    if *p[pred]*(*i*) is low (close to zero), then −log (*p[pred]*(*i*)) will be very
    high (since log 0 → − ∞).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 1b**：另一方面，在 *p[gt]*(*i*) 较高的 *i* 值处，如果 *p[pred]*(*i*) 较低（接近零），则 −log (*p[pred]*(*i*))
    将非常高（因为 log 0 → − ∞）。'
- en: '**Case 2**: The *i* values where *p[gt]*(*i*) is low (close to 0). These will
    have low values and will contribute little to ℍ*[c]*(*X[gt]*, *X[pred]*) since
    the product of anything with a near zero number is near zero.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 2**：当 *p[gt]*(*i*) 较低（接近 0）的 *i* 值。这些值将较低，并且对 ℍ*[c]*(*X[gt]*, *X[pred]*)
    的贡献很小，因为任何数与接近零的数的乘积都接近零。'
- en: Thus, overall, large contributions can happen only in case 1b, where *p[gt]*(*i*)
    is high and *p[pred]*(*i*) is low—that is, *p[gt]* and *p[pred]* are very dissimilar.
    What if *p[gt]*(*i*) is low and *p[pred]*(*i*) is high? They are also dissimilar,
    so those terms will not contribute much! True, but if such terms exist, there
    must be other terms where *p[gt]*(*i*) is high and *p[pred]*(*i*) is low. This
    is because the sums of all *p[gt]*(*i*) and *p[pred]*(*i*) must be both 1. Either
    way, if there is dissimilarity, the cross-entropy is high.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总体而言，只有在情况 1b 中，即 *p[gt]*(*i*) 较高而 *p[pred]*(*i*) 较低时，才会出现大的贡献——也就是说，*p[gt]*
    和 *p[pred]* 非常不同。如果 *p[gt]*(*i*) 较低而 *p[pred]*(*i*) 较高会怎样？它们也是不同的，所以这些项不会贡献太多！确实如此，但如果存在这样的项，则必须有其他项中
    *p[gt]*(*i*) 较高而 *p[pred]*(*i*) 较低。这是因为所有 *p[gt]*(*i*) 和 *p[pred]*(*i*) 的总和都必须是
    1。无论如何，如果存在差异，交叉熵就会很高。
- en: For instance, consider the case where *X[gt]* = *X[gt_cat]* and *X[pred]* =
    *X[good_pred]* or *X[pred]* = *X[bad_pred]*. We know *p[gt_cat]* is a one-hot
    selector vector, meaning it has 1 as one element and 0s elsewhere. Only a single
    term survives, corresponding to *i* = 0, and
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑 *X[gt]* = *X[gt_cat]* 和 *X[pred]* = *X[good_pred]* 或 *X[pred]* = *X[bad_pred]*
    的情况。我们知道 *p[gt_cat]* 是一个 one-hot 选择向量，这意味着它有一个元素为 1，其余为 0。只有一个项幸存，对应于 *i* = 0，并且
- en: '![](../../OEBPS/Images/eq_06-09-a.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-09-a.png)'
- en: We see that cross-entropy is higher where similarity is lower (the prediction
    is bad).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在相似性较低的地方（预测不佳），交叉熵较高。
- en: 'Finally, we are ready to formally define the cross-entropy of two arbitrary
    random variables. Let *X*[1], *X*[2] be a pair of random variables that take values
    *x* from the same input domain *D* that is, *x* ∈ *D*), with probabilities *p*[1](*x*),
    *p*[2](*x*), respectively:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备正式定义两个任意随机变量的交叉熵。设 *X*[1]，*X*[2] 是一对取值来自相同输入域 *D*（即 *x* ∈ *D*），分别具有概率
    *p*[1](*x*)，*p*[2](*x*) 的随机变量：
- en: '![](../../OEBPS/Images/eq_06-10.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-10.png)'
- en: Equation 6.10
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.10
- en: Note that cross-entropy in equation [6.10](../Text/06.xhtml#eq-cross-entropy-loss)
    reduces to entropy (equations [6.5](../Text/06.xhtml#eq-entropy-discr-univar),
    [6.6](../Text/06.xhtml#eq-entropy-cont-univar)) if *Y* = *X*. Listing 6.2 shows
    the Python PyTorch code to compute the entropy of a Gaussian.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到方程 [6.10](../Text/06.xhtml#eq-cross-entropy-loss) 中的交叉熵在 *Y* = *X* 时简化为熵（方程
    [6.5](../Text/06.xhtml#eq-entropy-discr-univar), [6.6](../Text/06.xhtml#eq-entropy-cont-univar)），列出
    6.2 展示了使用 Python PyTorch 计算高斯熵的代码。
- en: NOTE Fully functional code to compute cross-entropy, executable via Jupyter
    Notebook, can be found at [http://mng.bz/0mjN](http://mng.bz/0mjN).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：用于计算交叉熵的完整功能代码，可通过 Jupyter Notebook 执行，可以在 [http://mng.bz/0mjN](http://mng.bz/0mjN)
    找到。
- en: Listing 6.2 Computing cross-entropy
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 列出 6.2 计算交叉熵
- en: '[PRE1]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Direct computation
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ① 直接计算
- en: of cross-entropy from equation [6.9](../Text/06.xhtml#eq-cross-entropy-discrete)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程 [6.9](../Text/06.xhtml#eq-cross-entropy-discrete) 得到的交叉熵
- en: ② Probability density function for the ground truth (one-hot vector)
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ② 真实值（one-hot 向量）的概率密度函数
- en: ③ Probability density function for a good prediction
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 良好预测的概率密度函数
- en: ④ Probability density function for a bad prediction
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 坏预测的概率密度函数
- en: ⑤ Cross-entropy between *X[gt]* and *X[good_pred]* a low value)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ *X[gt]* 和 *X[good_pred]* 之间的交叉熵具有低值
- en: ⑥ Cross-entropy between *X[gt]* and *X[bad_pred]* a high value)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ *X[gt]* 和 *X[bad_pred]* 之间的交叉熵具有高值
- en: 6.4 KL divergence
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 KL 散度
- en: 'In section [6.3](../Text/06.xhtml#sec-cross-entropy), we saw that cross-entropy,
    ℍ*[c]*(*X*[1], *X*[2]), measures the dissimilarity between the distributions of
    two random variables *X*[1] and *X*[2] with probabilities *p*[1](*x*) and *p*[2](*x*).
    But cross-entropy has a curious property for a dissimilarity measure. If *X*[1]
    = *X*[2], the cross-entropy ℍ*[c]*(*X*[1], *X*[2]) reduces to the entropy ℍ(*X*[1]).
    This is somewhat counterintuitive: we expect the dissimilarity between two copies
    of the same thing to be zero.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [6.3](../Text/06.xhtml#sec-cross-entropy) 节中，我们看到了交叉熵，ℍ*[c]*(*X*[1], *X*[2])，衡量了两个随机变量
    *X*[1] 和 *X*[2] 的分布差异，其概率为 *p*[1](*x*) 和 *p*[2](*x*)。但是交叉熵作为一个差异度量具有一个奇特性质。如果
    *X*[1] = *X*[2]，交叉熵 ℍ*[c]*(*X*[1], *X*[2]) 会简化为熵 ℍ(*X*[1])。这有点反直觉：我们期望相同事物的两个副本之间的差异为零。
- en: We should look at cross-entropy as a dissimilarity with an offset. Let’s denote
    the pure dissimilarity measure as *D*(*X*[1], *X*[2]). Then
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该将交叉熵视为具有偏移量的差异度。让我们用 *D*(*X*[1], *X*[2]) 表示纯差异度量。那么
- en: '![](../../OEBPS/Images/eq_06-10-a.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6-10-a](../../OEBPS/Images/eq_06-10-a.png)'
- en: This means the pure dissimilarity measure
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着纯差异度量
- en: '![](../../OEBPS/Images/eq_06-10-b.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6-10-b](../../OEBPS/Images/eq_06-10-b.png)'
- en: This pure dissimilarity measure, *D*(*X*[1], *X*[2]), is called *Kullback–Leibler
    divergence* (KL divergence or KLD). As expected, it is 0 when the two random variables
    are identical.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这个纯差异度量，*D*(*X*[1], *X*[2])，被称为 *Kullback–Leibler 散度*（KL 散度或 KLD）。正如预期的那样，当两个随机变量相同的时候，它为
    0。
- en: 'Formally, KLD is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，KLD 如下：
- en: '![](../../OEBPS/Images/eq_06-11.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6-11](../../OEBPS/Images/eq_06-11.png)'
- en: Equation 6.11
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.11
- en: For continuous univariate randoms,
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续的单变量随机变量，
- en: '![](../../OEBPS/Images/eq_06-12.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6-12](../../OEBPS/Images/eq_06-12.png)'
- en: Equation 6.12
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.12
- en: For continuous multivariate randoms,
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续的多变量随机变量，
- en: '![](../../OEBPS/Images/eq_06-13.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6-13](../../OEBPS/Images/eq_06-13.png)'
- en: Equation 6.13
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.13
- en: 'Let’s examine some properties of KLD:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考察 KLD 的某些性质：
- en: The KLD between identical random variables is zero. If *X*[1] = *X*[2], *p*[1](*x*)
    = *p*[2](*x*)∀*x* ∈ *D*. Then the log term vanishes at every *x*, and KLD is zero.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同随机变量之间的 KLD 为零。如果 *X*[1] = *X*[2]，*p*[1](*x*) = *p*[2](*x*)∀*x* ∈ *D*，那么对数项在每一个
    *x* 处消失，KLD 为零。
- en: The KLD between non-identical probability distributions is always positive.
    We can see this by examining equation [6.11](../Text/06.xhtml#eq-kld-discr-univar).
    At all values of *x* where *p*[1](*x*) > *p*[2](*x*), the log term is positive
    (since the logarithm of a number greater than 1 is positive). On the other hand,
    at all values of *x* where *p*[1](*x*) < *p*[2](*x*), the log term is negative
    (since the logarithm of a number less than 1 is negative). But the positive terms
    get higher weights because *p*[1](*x*) are higher at these points. In this context,
    it is worth noting that given any pair of PDFs, *one cannot be uniformly higher
    than the other at all points*. This is because both of them must sum to 1. If
    one PDF is higher somewhere, it must be lower somewhere else to compensate.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非相同概率分布之间的KL散度总是正的。我们可以通过检查方程 [6.11](../Text/06.xhtml#eq-kld-discr-univar) 来看到这一点。在所有
    *x* 的值中，当 *p*[1](*x*) > *p*[2](*x*) 时，对数项是正的（因为大于1的数的对数是正的）。另一方面，在所有 *x* 的值中，当
    *p*[1](*x*) < *p*[2](*x*) 时，对数项是负的（因为小于1的数的对数是负的）。但是，正项因为 *p*[1](*x*) 在这些点上的值更高而获得更高的权重。在这种情况下，值得注意的是，对于任何一对PDF，*一个不能在所有点上均匀高于另一个*。这是因为它们都必须加起来等于1。如果一个PDF在某个地方更高，那么它必须在其他地方更低以补偿。
- en: Given a GT PDF *p[gt]* for a classification problem and a machine prediction
    *p[pred]*, minimizing the cross-entropy ℍ(*gt*, *pred*) is logically equivalent
    to minimizing the KLD *D*(*gt*, *pred*). This is because the entropy ℍ(*gt*) is
    a constant, independent of the machine parameters.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一个分类问题的GT PDF *p[gt]* 和机器预测 *p[pred]*，最小化交叉熵 ℍ(*gt*, *pred*) 在逻辑上等同于最小化KL散度
    *D*(*gt*, *pred*)。这是因为熵 ℍ(*gt*) 是一个常数，与机器参数无关。
- en: 'The KLD is *not* symmetric: *D*(*X*[1], *X*[2]) ≠ *D*(*X*[2], *X*[1]).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KL散度不是对称的：*D*(*X*[1], *X*[2]) ≠ *D*(*X*[2], *X*[1])。
- en: 6.4.1 KLD between Gaussians
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 高斯分布之间的KL散度
- en: 'Since the Gaussian probability distribution is so important, in this subsection
    we look at the KLD between two Gaussian random variables *X*[1] and *X*[2] having
    PDFs *p*[1](*x*) = 𝒩(*x*; *μ*[1], *σ*[1]) and *p*[2](*x*) = 𝒩(*x*; *μ*[2], *σ*[2]).
    We derive the expression for the univariate case and simply state the expression
    for the multivariate case:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于高斯概率分布非常重要，在本小节中，我们研究两个具有PDFs *p*[1](*x*) = 𝒩(*x*; *μ*[1], *σ*[1]) 和 *p*[2](*x*)
    = 𝒩(*x*; *μ*[2], *σ*[2]) 的高斯随机变量 *X*[1] 和 *X*[2] 之间的KL散度。我们推导出单变量情况的表达式，并简单地陈述多变量情况的表达式：
- en: '![](../../OEBPS/Images/eq_06-13-a.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-13-a.png)'
- en: Opening the parentheses, we get
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 展开括号，我们得到
- en: '![](../../OEBPS/Images/eq_06-13-b.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-13-b.png)'
- en: Expanding the square term, we get
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 展开平方项，我们得到
- en: '![](../../OEBPS/Images/eq_06-13-c.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-13-c.png)'
- en: Since
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于
- en: '![](../../OEBPS/Images/eq_06-13-d.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-13-d.png)'
- en: the final equation for the KLD between two univariate Gaussian random variables
    *X*[1], *X*[2] with PDFs 𝒩(*x*; *μ*[1], *σ*[1]) and 𝒩(*x*; *μ*[2], *σ*[2]) becomes
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 两个单变量高斯随机变量 *X*[1]，*X*[2] 之间KL散度的最终方程，其PDFs为 𝒩(*x*; *μ*[1], *σ*[1]) 和 𝒩(*x*;
    *μ*[2], *σ*[2])，变为
- en: '![](../../OEBPS/Images/eq_06-14.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-14.png)'
- en: Equation 6.14
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.14
- en: The KLD between two *d*-dimensional Gaussian random variables *X*[1], *X*[2]
    with PDFs 𝒩(![](../../OEBPS/Images/AR_x.png); *μ*[1], **Σ**[1]) and 𝒩(![](../../OEBPS/Images/AR_x.png);
    *μ*[2], **Σ**[2]) is
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 两个 *d*-维高斯随机变量 *X*[1]，*X*[2] 之间的KL散度，其PDFs为 𝒩(![](../../OEBPS/Images/AR_x.png);
    *μ*[1], **Σ**[1]) 和 𝒩(![](../../OEBPS/Images/AR_x.png); *μ*[2], **Σ**[2]) 是
- en: '![](../../OEBPS/Images/eq_06-15.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-15.png)'
- en: Equation 6.15
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.15
- en: where the operator *tr* denotes the *trace* of a matrix (sum of diagonal elements)
    and the operator *det* denotes the determinant.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，运算符 *tr* 表示矩阵的迹（对角元素之和），运算符 *det* 表示行列式。
- en: Listing 6.3 shows the Python PyTorch code to compute the KLD.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3显示了计算KL散度的Python PyTorch代码。
- en: NOTE Fully functional code to compute the KLD, executable via Jupyter Notebook,
    can be found at [http://mng.bz/KMyj](http://mng.bz/KMyj).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：用于计算KL散度的完整功能代码，可通过Jupyter Notebook执行，可在[http://mng.bz/KMyj](http://mng.bz/KMyj)找到。
- en: Listing 6.3 Computing the KLD
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 计算KL散度
- en: '[PRE2]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Instantiates three Gaussian distributions
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ① 实例化三个高斯分布
- en: with the same means but different
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 具有相同的均值但不同的
- en: standard deviations
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差
- en: ② Computes the KLD between various pairs of distributions
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ② 计算各种分布对之间的KL散度
- en: ③ The KLD between a distribution and itself is 0.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 一个分布与自身之间的KL散度为0。
- en: ④ The KLD is not symmetric.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ④ KL散度不是对称的。
- en: ⑤ See figure [6.4](../Text/06.xhtml#fig-kld-gaussian).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 见图 [6.4](../Text/06.xhtml#fig-kld-gaussian)。
- en: In figure [6.4](../Text/06.xhtml#fig-kld-gaussian), we compare three Gaussian
    distributions *p*, *q*, and *r* with the same *μ*s but different *σ*s. *KLD*(*p*,
    *q*) < *KLD*(*p*, *r*) because *σ[p]* is closer to *σ[q]* than *σ[r]*.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [6.4](../Text/06.xhtml#fig-kld-gaussian) 中，我们比较了具有相同 *μ* 但不同 *σ* 的三个高斯分布
    *p*，*q* 和 *r*。*KLD*(*p*, *q*) < *KLD*(*p*, *r*) 因为 *σ[p]* 比 *σ[r]* 更接近 *σ[q]*。
- en: In figure [6.4](../Text/06.xhtml#fig-kld-uniform), we compare a uniform distribution
    *p* with two Gaussian distributions *q* and *r* that have different *μ*s but the
    same *σ*s. *KLD*(*p*, *q*) < *KLD*(*p*, *r*) because *μ[p]* is closer to *μ[q]*
    than *μ[r]*.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [6.4](../Text/06.xhtml#fig-kld-uniform) 中，我们比较了均匀分布 *p* 与两个具有不同 *μ* 但相同 *σ*
    的高斯分布 *q* 和 *r*。*KLD*(*p*, *q*) < *KLD*(*p*, *r*) 因为 *μ[p]* 比 *μ[r]* 更接近 *μ[q]*。
- en: 6.5 Conditional entropy
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 条件熵
- en: In section [6.2](../Text/06.xhtml#sec-entropy), we learned that entropy measures
    the uncertainty in a system. Earlier, in section [6.1.2](../Text/06.xhtml#sec-cond-prob-bayes),
    we studied conditional probability, which measures the probability of occurrence
    of one set of random variables under the condition that another set has known
    fixed values. In this section, we combine the two concepts into a new concept
    called *conditional entropy*.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [6.2](../Text/06.xhtml#sec-entropy) 节中，我们学习了熵衡量系统中的不确定性。在之前的第 [6.1.2](../Text/06.xhtml#sec-cond-prob-bayes)
    节中，我们研究了条件概率，它衡量在另一组随机变量的已知固定值条件下，一组随机变量发生概率。在本节中，我们将这两个概念结合成一个新的概念，称为 *条件熵*。
- en: Consider the following question from table [6.2](../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob).
    What is the entropy of the weight variable *W* under the condition that the value
    of the height variable *H* is *F*[1]? As observed in section [6.1.1](../Text/06.xhtml#sec-joint-marginal-prob-recap),
    the condition effectively restricts our universe to a single row (in this case,
    the top row) of the table. We can compute the entropy of the elements of that
    row mathematically, using equation [6.5](../Text/06.xhtml#eq-entropy-discr-univar),
    as
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑表 [6.2](../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob)
    中的以下问题。在高度变量 *H* 的值为 *F*[1] 的条件下，权重变量 *W* 的熵是多少？如第 [6.1.1](../Text/06.xhtml#sec-joint-marginal-prob-recap)
    节所述，条件实际上将我们的宇宙限制为表的一行（在这种情况下，是顶部行）。我们可以使用方程 [6.5](../Text/06.xhtml#eq-entropy-discr-univar)
    通过数学方法计算该行元素的熵，
- en: '![](../../OEBPS/Images/eq_06-15-a.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-15-a.png)'
- en: '![](../../OEBPS/Images/CH06_F04a_Chaudhury.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F04a_Chaudhury.png)'
- en: (a) *p* ≡ 𝒩(*μ* = 0, *σ* = 5), *q* ≡ 𝒩(*μ* = 0, *σ* = 10), *r* ≡ 𝒩(*μ* = 0,
    *σ* = 20)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: (a) *p* ≡ 𝒩(*μ* = 0, *σ* = 5), *q* ≡ 𝒩(*μ* = 0, *σ* = 10), *r* ≡ 𝒩(*μ* = 0,
    *σ* = 20)
- en: '![](../../OEBPS/Images/CH06_F04b_Chaudhury.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F04b_Chaudhury.png)'
- en: (b) *p* ≡ *U*(*a* = −20, *b* = 20), *q* ≡ 𝒩(*μ* = 0, *σ* = 20), *r* ≡ 𝒩(*μ*
    = −50, *σ* = 20)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: (b) *p* ≡ *U*(*a* = −20, *b* = 20), *q* ≡ 𝒩(*μ* = 0, *σ* = 20), *r* ≡ 𝒩(*μ*
    = −50, *σ* = 20)
- en: Figure 6.4 KLD between example distributions
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 示例分布之间的 KLD
- en: Similarly,
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，
- en: '![](../../OEBPS/Images/eq_06-15-b.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-15-b.png)'
- en: 'ℍ(*W*|*H* = *F[i]*) is the entropy of *W* given *H* = *F[i]* for *i* = 1 or
    2 or 3. What is the overall conditional entropy of *W* given *H*: that is, ℍ(*W*|*H*)?
    To compute this, we take the expected value (that is, the probability-weighted
    average; see equation [5.8](../Text/05.xhtml#eq-discrete-univar-expected-val))
    of the conditional entropy ℍ(*W*|*H* = *F[i]*) over all possible values of *i*:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ℍ(*W*|*H* = *F[i]*) 是在 *H* = *F[i]* 且 *i* = 1 或 2 或 3 时 *W* 的熵。那么，给定 *H* 的 *W*
    的整体条件熵是什么？即，ℍ(*W*|*H*)？为了计算这个值，我们取条件熵 ℍ(*W*|*H* = *F[i]*) 在所有可能的 *i* 值上的期望值（即，概率加权的平均值；参见方程
    [5.8](../Text/05.xhtml#eq-discrete-univar-expected-val))：
- en: '![](../../OEBPS/Images/eq_06-15-c.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-15-c.png)'
- en: This idea can be generalized. Formally, given two random variables *X* and *Y*
    that can take values *x* ∈ *D[x]*, ![](../../OEBPS/Images/AR_y.png) ∈ *D[y]*,
    respectively,
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法可以推广。形式上，给定两个可以取值 *x* ∈ *D[x]* 和 ![](../../OEBPS/Images/AR_y.png) ∈ *D[y]*
    的随机变量 *X* 和 *Y*，
- en: '![](../../OEBPS/Images/eq_06-16.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-16.png)'
- en: Equation 6.16
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.16
- en: '![](../../OEBPS/Images/eq_06-17.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-17.png)'
- en: Equation 6.17
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.17
- en: 6.5.1 Chain rule of conditional entropy
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 条件熵的链式法则
- en: 'This rule states:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这条规则说明：
- en: ℍ(*X*|*Y*) = ℍ(*X*, *Y*) − ℍ(*Y*)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ℍ(*X*|*Y*) = ℍ(*X*, *Y*) − ℍ(*Y*)
- en: Equation 6.18
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.18
- en: This can be derived from equation [6.17](../Text/06.xhtml#eq-cond-entropy-cont).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以从方程 [6.17](../Text/06.xhtml#eq-cond-entropy-cont) 推导出来。
- en: '![](../../OEBPS/Images/eq_06-18-a.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-18-a.png)'
- en: Applying Bayes’ theorem (equation [6.1](../Text/06.xhtml#eq-bayes-theorem-2var)),
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 应用贝叶斯定理（方程 [6.1](../Text/06.xhtml#eq-bayes-theorem-2var)），
- en: '![](../../OEBPS/Images/eq_06-19.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-19.png)'
- en: Equation 6.19
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.19
- en: 6.6 Model parameter estimation
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 模型参数估计
- en: Suppose we have a set of sampled input data points *X* = {![](../../OEBPS/Images/AR_x.png)^((1)),
    ![](../../OEBPS/Images/AR_x.png)^((2)),⋯, ![](../../OEBPS/Images/AR_x.png)^((*n*))}
    from a distribution. We refer to the set collectively as *training data*. Note
    that we are *not* assuming it is *labeled* training data—we do not know the outputs
    corresponding to the inputs ![](../../OEBPS/Images/AR_x.png)^((*i*)). Also, suppose
    that based on our knowledge of the problem, we have decided which model family
    to use. Of course, simply knowing the family is not enough; we need to know (or
    estimate) the model parameters before we can use the model. For instance, our
    model family might be Gaussian, 𝒩(*x*; ![](../../OEBPS/Images/AR_micro.png), **Σ**).
    Until we know the actual value of the parameters ![](../../OEBPS/Images/AR_micro.png)
    and Σ, we do not fully know the model and cannot use it.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组从分布中采样的输入数据点 *X* = {![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),⋯,
    ![](../../OEBPS/Images/AR_x.png)^((*n*))}。我们将这个集合统称为 *训练数据*。请注意，我们 *不* 假设它是 *标记*
    训练数据——我们不知道与输入 ![](../../OEBPS/Images/AR_x.png)^((*i*)）对应的输出。此外，假设根据我们对问题的了解，我们已经决定使用哪种模型族。当然，仅仅知道家族是不够的；在我们能够使用模型之前，我们需要知道（或估计）模型参数。例如，我们的模型族可能是高斯，𝒩(*x*;
    ![](../../OEBPS/Images/AR_micro.png), **Σ**)。在我们知道参数 ![](../../OEBPS/Images/AR_micro.png)
    和 Σ 的实际值之前，我们并不完全了解模型，也无法使用它。
- en: How do we estimate the model parameters from the unlabeled training data? This
    is what we cover in this section. At the moment, we are discussing it without
    referring to any specific model architecture, so let’s denote model parameters
    with a generic symbol *θ*. For instance, when dealing with Gaussian models, *θ*
    = {![](../../OEBPS/Images/AR_micro.png), **Σ**}.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何从未标记的训练数据中估计模型参数？这正是本节要讨论的内容。目前，我们正在讨论这个问题，而不涉及任何特定的模型架构，因此让我们用通用符号 *θ*
    表示模型参数。例如，当处理高斯模型时，*θ* = {![](../../OEBPS/Images/AR_micro.png), **Σ**}。
- en: 6.6.1 Likelihood, evidence, and posterior and prior probabilities
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.1 概率、证据、后验概率和先验概率
- en: Before tackling the problem of parameter estimation, it is important to have
    a clear understanding of the terms *likelihood*, *evidence*, *posterior probability*,
    and *prior probability* in the current context. Equation [6.20](../Text/06.xhtml#eq-posterior-prior-likelihood-evidence)
    illustrates them. Using Bayes’ theorem,
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在着手解决参数估计问题之前，了解当前上下文中“似然”、“证据”、“后验概率”和“先验概率”这些术语的含义非常重要。方程 [6.20](../Text/06.xhtml#eq-posterior-prior-likelihood-evidence)
    对其进行了说明。使用贝叶斯定理，
- en: '![](../../OEBPS/Images/eq_06-20.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../../OEBPS/Images/eq_06-20.png)'
- en: Equation 6.20
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.20
- en: Let’s first examine the likelihood term. Using the fact that data instances
    are independent of each other,
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来考察一下似然项。利用数据实例之间相互独立的事实，
- en: '![](../../OEBPS/Images/eq_06-20-a.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../../OEBPS/Images/eq_06-20-a.png)'
- en: Now, *p*(![](../../OEBPS/Images/AR_x.png)^((*i*))|*θ*) is essentially the probability
    density of the distribution family we have chosen. For instance, if the model
    in question in Gaussian, then given *θ* = {![](../../OEBPS/Images/AR_micro.png),
    **Σ**}, this will be
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，*p*(![](../../OEBPS/Images/AR_x.png)^((*i*))|*θ*) 实际上是我们所选择的分布族的概率密度。例如，如果所讨论的模型是高斯模型，那么给定
    *θ* = {![](../../OEBPS/Images/AR_micro.png), **Σ**}，这将变为
- en: '![](../../OEBPS/Images/eq_06-20-b.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../../OEBPS/Images/eq_06-20-b.png)'
- en: 'which is basically an expression for the Gaussian PDF: a restatement of equation
    [5.23](../Text/05.xhtml#eq-multivar-normal) (but in equation [5.23](../Text/05.xhtml#eq-multivar-normal),
    we dropped the “given *θ*,” part in the notation and expressed *p*(![](../../OEBPS/Images/AR_x.png)|*θ*)
    simply as *p*(![](../../OEBPS/Images/AR_x.png))). Thus, we can always express
    the likelihood from the PDF of the chosen model family using the independence
    of individual training data instances.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是高斯概率密度函数的表达式：方程 [5.23](../Text/05.xhtml#eq-multivar-normal) 的重述（但在方程 [5.23](../Text/05.xhtml#eq-multivar-normal)
    中，我们在符号中省略了“给定 *θ*”的部分，并将 *p*(![](../../OEBPS/Images/AR_x.png)|*θ*) 简单地表示为 *p*(![](../../OEBPS/Images/AR_x.png)))。因此，我们可以始终使用单个训练数据实例的独立性，从所选模型族的概率密度函数中表达似然。
- en: Now let’s examine the prior probability, *p*(*θ*). It typically comes from some
    physical constraint—without referring to the input. A very popular approach is
    to say that, all other things being equal, we prefer parameters with smaller magnitudes.
    By this token, the larger the total magnitude ||*θ*||², the lower the prior probability.
    For instance, we may use
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来考察先验概率，*p*(*θ*)。它通常来自某种物理约束——不涉及输入。一个非常流行的方法是，在其他条件相同的情况下，我们更喜欢幅度较小的参数。按照这个说法，总幅度
    ||*θ*||² 越大，先验概率就越低。例如，我们可能使用
- en: '*p*(*θ*) ∝ *e*^(−||*θ*||²)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*θ*) ∝ *e*^(−||*θ*||²)'
- en: Equation 6.21
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.21
- en: An indirect justification for favoring parameter vectors with the smallest length
    (magnitude) can be found in the principle of Occam’s razor. It states, *Entia
    non sunt multiplicanda praeter necessitatem*, which roughly translates to “One
    should not multiply unnecessarily.” This is often interpreted in machine learning
    and other disciplines as “favor the briefest representation.”
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对于偏好长度（幅度）最小的参数向量，一个间接的依据可以在奥卡姆剃刀原则中找到。它声明，“*Entia non sunt multiplicanda praeter
    necessitatem*”，这大致可以翻译为“不应该无谓地增加实体。”在机器学习和其他学科中，这通常被解释为“偏好最简明的表示。”
- en: 'As shown previously, we can always express the likelihood and prior terms.
    Using them, we can formulate different paradigms, each with a different quantity,
    to optimize in order to estimate the unknown probability distribution parameters
    from training data. These techniques can be broadly classified into the following
    categories:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以总是表达似然和先验项。使用它们，我们可以制定不同的范例，每个范例都有一个不同的量要优化，以从训练数据中估计未知概率分布参数。这些技术可以广泛地分为以下几类：
- en: Maximum likelihood parameter estimation MLE)
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大似然参数估计（MLE）
- en: Maximum a posteriori (MAP) parameter estimation
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大后验（MAP）参数估计
- en: We provide an overview of them next. You will notice that, in all the methods,
    we typically preselect a distribution family as a model and then estimate the
    parameter values by maximizing one probability or another.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来概述它们。您会注意到，在所有方法中，我们通常预先选择一个分布族作为模型，然后通过最大化一个概率或另一个概率来估计参数值。
- en: 'Later in the chapter, we look at MLE in the special case of the Gaussian family
    of distributions. Further down the line, we look at MLE with respect to Gaussian
    mixture models. Another technique outlined later is evidence maximization: we
    will visit it in the context of variational autoencoders.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将探讨高斯分布族的特殊情况下的MLE。接下来，我们将探讨高斯混合模型中的MLE。稍后，我们将探讨证据最大化：我们将在变分自编码器的背景下讨论它。
- en: The log-likelihood trick
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然技巧
- en: If we choose a distribution family whose PDF is exponential (the most obvious
    example is Gaussian), instead of maximizing the likelihood, we usually maximize
    its logarithm, aka the *log-likelihood*. We can do this because whatever maximizes
    a quantity also maximizes its logarithm and vice versa. But the logarithm simplifies
    expressions in the case of exponential probability functions. This becomes obvious
    if we note that
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择一个概率密度函数为指数分布的分布族（最明显的例子是高斯分布），我们通常不是最大化似然函数，而是最大化其对数，也就是所谓的*对数似然*。我们可以这样做，因为任何最大化一个量也会最大化它的对数，反之亦然。但是，对数在指数概率函数的情况下简化了表达式。如果我们注意到这一点，这一点就会变得明显：
- en: '*log*(*e*^x) = *x*'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '*log*(*e*^x) = *x*'
- en: '*log*(Π *e*^(*x*^((*i*)))) = Σ *x*^((*i*))'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '*log*(Π *e*^(*x*^((*i*)))) = Σ *x*^((*i*))'
- en: 6.6.2 Maximum likelihood parameter estimation (MLE)
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.2 最大似然参数估计（MLE）
- en: In MLE of parameters, we ask, “What parameter values will maximize the joint
    likelihood of the training data instances?” In this context, remember that likelihood
    is the of a data instance occurring given specific parameter values (equation
    [6.20](../Text/06.xhtml#eq-posterior-prior-likelihood-evidence)). Expressed mathematically,
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数的最大似然估计（MLE）中，我们问，“什么参数值将最大化训练数据实例的联合似然？”在这个背景下，记住似然是在给定特定参数值的情况下，一个数据实例发生的概率（方程
    [6.20](../Text/06.xhtml#eq-posterior-prior-likelihood-evidence)）。用数学表达式来说，
- en: 'MLE estimates what value of *θ* maximizes *p*(*X*|*θ*). The geometric mental
    picture is as follows: we want to estimate the unknown parameters for our model
    probability distribution such that if we draw many samples from that distribution,
    the sample point cloud will largely overlap the training data.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: MLE估计什么值的*θ*将最大化*p*(*X*|*θ*)。几何上的思维图如下：我们想要估计模型概率分布中的未知参数，使得如果我们从这个分布中抽取许多样本，样本点云将主要与训练数据重叠。
- en: Often we employ the log-likelihood trick and maximize the log-likelihood instead
    of the actual likelihood.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们使用对数似然技巧，最大化对数似然而不是实际似然。
- en: For some models, such as Gaussians, this maximization problem can be solved
    analytically, and a closed-form solution can be obtained (as shown in section
    [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation)). For others, such
    as Gaussian mixture models (GMMs), the maximization problem ields no closed-form
    solution, and we go for an iterative solution (as shown in section [6.9.4](../Text/06.xhtml#sec-gmm_fit)).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些模型，例如高斯模型，这个最大化问题可以通过解析方法解决，并得到闭式解（如第[6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation)节所示）。对于其他模型，例如高斯混合模型（GMMs），最大化问题没有闭式解，我们寻求迭代解（如第[6.9.4](../Text/06.xhtml#sec-gmm_fit)节所示）。
- en: 6.6.3 Maximum a posteriori (MAP) parameter estimation and regularization
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.3 最大后验（MAP）参数估计和正则化
- en: Instead of asking what parameter value maximizes the probability of occurrence
    of the training data instances, we can ask, “What are the most probable parameter
    values, given the training data?” Expressed mathematically, in MAP, we directly
    estimate the *θ* that maximizes *p*(*θ*|*X*). Using equation [6.20](../Text/06.xhtml#eq-posterior-prior-likelihood-evidence),
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是询问什么参数值最大化了训练数据实例发生的概率，而是可以问，“给定训练数据，最可能的参数值是什么？”用数学表达式表示，在MAP中，我们直接估计最大化*p*(*θ*|*X*)的*θ*。使用公式[6.20](../Text/06.xhtml#eq-posterior-prior-likelihood-evidence)，
- en: '![](../../OEBPS/Images/eq_06-22.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![公式6-22](../../OEBPS/Images/eq_06-22.png)'
- en: Equation 6.22
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 公式6.22
- en: Since the denominator is independent of *θ*, maximizing the numerator with respect
    to *θ* maximizes the fraction. Thus
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分母与*θ*无关，相对于*θ*最大化分子最大化了分数。因此
- en: In MAP parameter estimation, we look for parameters *θ* that maximize *p*(*X*|*θ*)*p*(*θ*).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在MAP参数估计中，我们寻找最大化*p*(*X*|*θ*)*p*(*θ*)的参数*θ*。
- en: The first factor, *p*(*X*|*θ*), is what we optimized in MLE and comes from the
    model definition (such as equation [5.23](../Text/05.xhtml#eq-multivar-normal)
    for multivariate Gaussian models).
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个因素，*p*(*X*|*θ*)，是我们MLE中优化的内容，它来自模型定义（如多元高斯模型的[公式5.23](../Text/05.xhtml#eq-multivar-normal)）。
- en: The second factor, *p*(*θ*), is the prior term, which usually incentivizes the
    optimization system to choose a solution with predefined properties like smaller
    parameter magnitudes equation [6.21](../Text/06.xhtml#eq-prior-mag)).
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个因素，*p*(*θ*)，是先验项，它通常激励优化系统选择具有预定义属性（如较小的参数幅度）的解，[公式6.21](../Text/06.xhtml#eq-prior-mag)。
- en: Viewed this way, MAP estimation is equivalent to *MLE parameter estimation with
    regularization*. Regularization is a technique often used in optimization. In
    regularized optimization, we add a term to the expression being maximized or minimized.
    This term effectively incentivizes the system to choose the solution with the
    smallest magnitudes of the unknown from the set of possible solutions. It is easy
    to see that MAP estimation essentially imposes the prior probability term on top
    of MLE. This extra term acts as a regularizer, incentivizing the system to choose
    the lowest magnitude parameters while still trying to maximize the likelihood
    of the training data.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式看，MAP估计相当于**带有正则化的最大似然（MLE）参数估计**。正则化是优化中常用的一种技术。在正则化优化中，我们在要最大化或最小化的表达式中添加一个项。这个项有效地激励系统从可能的解集中选择未知参数幅度最小的解。很容易看出，MAP估计本质上是在MLE之上施加先验概率项。这个额外的项充当正则化器，激励系统选择幅度最低的参数，同时仍然试图最大化训练数据的似然。
- en: 'Equation [6.22](../Text/06.xhtml#eq-MAP) can be interpreted another way. When
    we have no training data, all we can do is estimate the parameters from our prior
    beliefs about the system: the prior term *p*(*θ*). When the training data set
    *X* arrives, it influences the system through the likelihood term *p*(*X*|*θ*).
    As more and more training data arrives, the prior term (whose magnitude does not
    change with training data) dominates less and less, and the posterior probability
    *p*(*θ*|*X*) is dominated more by the likelihood.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 公式[6.22](../Text/06.xhtml#eq-MAP)可以另一种方式来解释。当我们没有训练数据时，我们所能做的就是根据我们对系统的先验信念来估计参数：先验项*p*(*θ*)。当训练数据集*X*到达时，它通过似然项*p*(*X*|*θ*)影响系统。随着越来越多的训练数据到来，先验项（其幅度不随训练数据变化）的影响力越来越小，后验概率*p*(*θ*|*X*)越来越受似然的影响。
- en: 6.7 Latent variables and evidence maximization
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 隐变量和证据最大化
- en: 'Suppose we have the height and weight data for a population (say, for the adult
    residents of our favorite town, Statsville). A single data instance looks like
    this:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个种群（例如，我们最喜欢的城镇Statsville的成年居民）的高度和体重数据。单个数据实例看起来是这样的：
- en: '![](../../OEBPS/Images/eq_06-22-a.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-22-a.png)'
- en: Although the data is not explicitly labeled or classified, we know the data
    points can be clustered into two distinct classes, *male* and *female*. It is
    reasonable to expect that the distribution of each class is much simpler than
    the overall distribution. For instance, here, the distributions for males and
    females may be Gaussians individually (presumably, the means for females will
    occur at smaller height and weight values). The combined distribution does not
    fit any of the distributions we have discussed so far (later, we see it is a Gaussian
    mixture).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据没有明确标记或分类，但我们知道数据点可以被聚类成两个不同的类别，*男性* 和 *女性*。可以合理地预期，每个类别的分布比整体分布简单得多。例如，在这里，男性和女性的分布可能是单独的高斯分布（可能，女性的平均值将出现在更矮和更轻的身高和体重值）。组合分布不符合我们之前讨论过的任何分布（稍后我们将看到它是一个高斯混合分布）。
- en: 'We look at such situations in more detail in connection to Gaussian mixture
    modeling and variational autoencoders. Here we only note that in these cases,
    it is often beneficial to introduce a variable for the class, say *Z*. In this
    example, *Z* is discrete: it can take one of two values, *male* or *female*. Then
    we can model the overall distribution as a combination of simple distributions,
    each corresponding to a specific value of *Z*.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地探讨这些情况与高斯混合模型和变分自编码器的关系。在这里，我们只指出，在这些情况下，引入一个用于类别的变量通常是有益的，比如 *Z*。在这个例子中，*Z*
    是离散的：它可以取两个值之一，*男性* 或 *女性*。然后我们可以将整体分布建模为简单分布的组合，每个对应于 *Z* 的一个特定值。
- en: 'Such variables *Z* that are *not* part of the observed data *X* but are introduced
    to facilitate modeling are called *latent* or *hidden* variables/parameters. Latent
    variables are connected to observed variables through the usual Bayesian expression:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的变量 *Z*，它们不是观察数据 *X* 的组成部分，而是为了便于建模而引入的，被称为 *潜在* 或 *隐藏* 变量/参数。潜在变量通过通常的贝叶斯表达式与观察变量相连接：
- en: '![](../../OEBPS/Images/eq_06-22-b.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-22-b.png)'
- en: 'How do we estimate the distribution of *Z*? One way is to ask, “What distribution
    of the hidden variables would maximize the probability of exactly these training
    data points being returned if we drew random samples from the distribution?” The
    philosophy behind this is as follows: we assume that the training data points
    are fairly typical and have a high probability of occurrence in the unknown data
    distribution. Hence, we try to find a distribution under which the training data
    points will have the highest probabilities.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何估计 *Z* 的分布？一种方法是可以问，“如果我们从这个分布中抽取随机样本，那么隐藏变量的分布是什么，会最大化返回这些特定训练数据点的概率？”
    这种方法的哲学依据如下：我们假设训练数据点相当典型，在未知数据分布中出现的概率很高。因此，我们试图找到一个分布，使得训练数据点将具有最高的概率。
- en: Geometrically speaking, each data point (vector) can be viewed as a point in
    some *d*-dimensional space, where *d* is the number of elements in the vector
    ![](../../OEBPS/Images/AR_x.png)*[i]*. The training data points typically occupy
    a region within that space. We are looking for a distribution whose mass is largely
    aligned with the training data region. In other words, the probability associated
    with the training data points is as high as possible—the sample distribution cloud
    largely overlaps the training data cloud.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何学的角度来看，每个数据点（向量）可以看作是在某个 *d*-维空间中的一个点，其中 *d* 是向量 ![](../../OEBPS/Images/AR_x.png)*[i]*
    的元素数量。训练数据点通常占据该空间内的一个区域。我们正在寻找一个质量大部分与训练数据区域对齐的分布。换句话说，与训练数据点相关的概率尽可能高——样本分布云与训练数据云大部分重叠。
- en: Expressed mathematically, we want to identify *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png))
    and *p*(![](../../OEBPS/Images/AR_z.png)) that maximize the quantity
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 用数学表达，我们想要识别 *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png))
    和 *p*(![](../../OEBPS/Images/AR_z.png))，它们最大化了数量
- en: '![](../../OEBPS/Images/eq_06-23.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-23.png)'
- en: Equation 6.23
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.23
- en: As usual, we get *p*(![](../../OEBPS/Images/AR_x.png)^((*i*))|![](../../OEBPS/Images/AR_z.png))
    from the PDF of our chosen model family and *p*(![](../../OEBPS/Images/AR_z.png))
    through some physical constraint.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们从所选模型家族的PDF中得到*p*(![AR_x](../../OEBPS/Images/AR_x.png)^((*i*))|![AR_z](../../OEBPS/Images/AR_z.png))，并通过某些物理约束得到*p*(![AR_z](../../OEBPS/Images/AR_z.png))。
- en: 6.8 Maximum likelihood parameter estimation for Gaussians
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.8 高斯分布的最大似然参数估计
- en: We look at this with a one-dimensional example, but the results derived apply
    to higher dimensions. Suppose we are trying to predict whether an adult Statsville
    resident is female, given that the resident’s height lies in a specified range
    [*a*, *b*]. For this purpose, we have collected a set of height samples of adult
    *female* Statsville residents. These height samples constitute our training data.
    Let’s denote them as *x*^((1)), *x*^((2)), ⋯, *x*^((*n*)). Based on physical considerations,
    we expect the distribution of heights of adult Statsville females to be a Gaussian
    distribution with unknown mean and variance. Our goal is to determine them from
    the training data via MLE, which effectively estimates a distribution whose sample
    cloud maximally matches the distribution of the training data points.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用一个一维例子来看这个问题，但推导出的结果也适用于高维情况。假设我们正在尝试预测一个成年Statsville居民的性别，给定该居民的身高位于指定范围[*a*,
    *b*]。为此，我们收集了一组成年*女性*Statsville居民的身高样本。这些身高样本构成了我们的训练数据。让我们用*x*^((1)), *x*^((2)),
    ⋯, *x*^((*n*))来表示它们。根据物理考虑，我们预计成年Statsville女性的身高分布是一个具有未知均值和方差的高斯分布。我们的目标是通过对训练数据进行最大似然估计（MLE）来确定它们，这有效地估计了一个样本云与训练数据点分布最大匹配的分布。
- en: Let’s denote the (as yet unknown) mean and variance of the distribution as *μ*
    and *σ*. Then, from equation [5.22](../Text/05.xhtml#eq-univar-normal), we get
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用*μ*和*σ*表示分布的（尚未知的）均值和方差。然后，根据方程[5.22](../Text/05.xhtml#eq-univar-normal)，我们得到
- en: '![](../../OEBPS/Images/eq_06-23-a.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-23-a](../../OEBPS/Images/eq_06-23-a.png)'
- en: Employing the log-likelihood trick,
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对数似然技巧，
- en: '![](../../OEBPS/Images/eq_06-23-b.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-23-b](../../OEBPS/Images/eq_06-23-b.png)'
- en: To maximize with respect to *μ*, we solve
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 要使相对于*μ*最大化，我们求解
- en: '![](../../OEBPS/Images/eq_06-23-c.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-23-c](../../OEBPS/Images/eq_06-23-c.png)'
- en: or
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '![](../../OEBPS/Images/eq_06-23-d.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-23-d](../../OEBPS/Images/eq_06-23-d.png)'
- en: or
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '![](../../OEBPS/Images/eq_06-23-e.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-23-e](../../OEBPS/Images/eq_06-23-e.png)'
- en: 'Finally, we get a closed-form expression for the unknown *μ* in terms of the
    training data:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到一个关于训练数据的未知*μ*的闭式表达式：
- en: '![](../../OEBPS/Images/eq_06-23-f.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-23-f](../../OEBPS/Images/eq_06-23-f.png)'
- en: Similarly, to maximize with respect to *σ*, we solve
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，要使相对于*σ*最大化，我们求解
- en: '![](../../OEBPS/Images/eq_06-23-g.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-23-g](../../OEBPS/Images/eq_06-23-g.png)'
- en: or
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '![](../../OEBPS/Images/eq_06-23-h.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-23-h](../../OEBPS/Images/eq_06-23-h.png)'
- en: or
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '![](../../OEBPS/Images/eq_06-23-i.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-23-i](../../OEBPS/Images/eq_06-23-i.png)'
- en: 'Finally, we get a closed-form expression for the unknown *σ* in terms of the
    training data:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到一个关于训练数据的未知*σ*的闭式表达式：
- en: '![](../../OEBPS/Images/eq_06-23-j.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-23-j](../../OEBPS/Images/eq_06-23-j.png)'
- en: 'Thus we see that for a Gaussian, the maximum-likelihood solutions coincide
    with the sample mean and variance of the training data. Once we have the mean
    and standard deviation, we can calculate the probability that a female resident’s
    height belongs to a specified range [*a*, *b*] by using the following equation:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到，对于一个高斯分布，最大似然解与训练数据的样本均值和方差相一致。一旦我们得到了均值和标准差，我们可以通过以下方程计算一个女性居民的身高属于指定范围[*a*,
    *b*]的概率：
- en: '![](../../OEBPS/Images/eq_06-24.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-24](../../OEBPS/Images/eq_06-24.png)'
- en: Equation 6.24
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式6.24
- en: 'In the multidimensional case:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在多维情况下：
- en: Given a training dataset, {![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),⋯,
    ![](../../OEBPS/Images/AR_x.png)^((*n*))}, the best fit Gaussian has the mean
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个训练数据集，{![AR_x](../../OEBPS/Images/AR_x.png)^((1)), ![AR_x](../../OEBPS/Images/AR_x.png)^((2)),⋯,
    ![AR_x](../../OEBPS/Images/AR_x.png)^((*n*))}，最佳拟合高斯分布的均值
- en: '![](../../OEBPS/Images/eq_06-25.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-25](../../OEBPS/Images/eq_06-25.png)'
- en: Equation 6.25
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式6.25
- en: and the covariance matrix
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 以及协方差矩阵
- en: '![](../../OEBPS/Images/eq_06-26.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![方程式6-26](../../OEBPS/Images/eq_06-26.png)'
- en: Equation 6.26
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式6.26
- en: We began this section by stating the problem of estimating the probability of
    an adult Statsville resident being female, given that their height lies in a specified
    range [*a*, *b*], when we are provided a training dataset of *n* height values
    of adult Statsville female residents. Let’s now revisit that problem. Using (scalar
    versions of) equations [6.25](../Text/06.xhtml#eq-gauss-MLE-mean) and [6.26](../Text/06.xhtml#eq-gauss-MLE-covar),
    we can estimate *μ* and *σ* and thereby define a Gaussian probability distribution
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节开始时提出了估计成年 Statsville 居民为女性的概率问题，前提是他们的身高在指定的范围 [*a*, *b*] 内，当我们提供成年 Statsville
    女性居民的 *n* 个身高值训练数据集时。现在让我们重新审视这个问题。使用（标量形式的）方程 [6.25](../Text/06.xhtml#eq-gauss-MLE-mean)
    和 [6.26](../Text/06.xhtml#eq-gauss-MLE-covar)，我们可以估计 *μ* 和 *σ*，从而定义一个高斯概率分布
- en: '*p*(*x*) = 𝒩(*x*; *μ*, *σ*)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*x*) = 𝒩(*x*; *μ*, *σ*)'
- en: Using this, given any height *x*, we can compute the probability *p*(*x*) that
    the resident is female. Let’s see this using PyTorch.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方法，给定任何身高 *x*，我们可以计算居民为女性的概率 *p*(*x*)。让我们用 PyTorch 来看看这个例子。
- en: 6.8.1 Python PyTorch code for maximum likelihood estimation
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8.1 Python PyTorch 代码用于最大似然估计
- en: 'Suppose we assume that the height values of adult female residents of Statsville
    follow a Gaussian distribution. If we know the parameters of this Gaussian (*μ*
    and *σ*), we know the Gaussian distribution fully. That allows us to estimate
    many interesting things: for instance, the expected height of an adult female
    resident of Statsville, or the probability that the height of an adult female
    Statsville resident lies in a certain range such as between 160 and 170 cm. The
    problem is, in a typical real-life situation, we do not know the parameters *μ*
    cm and *σ*. All we have is a large dataset *X* of height values of adult Statsville
    female residents—training data. We have to use this data to estimate the unknown
    parameters *μ* cm and *σ*. Once we have these, we have an estimated distribution
    (aka model) from which we can predict the probabilities of events of interest.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们假设 Statsville 成年女性的身高值遵循高斯分布。如果我们知道这个高斯分布的参数（*μ* 和 *σ*），我们就完全知道了高斯分布。这使我们能够估计许多有趣的事情：例如，成年
    Statsville 居民的预期身高，或者成年 Statsville 居民的身高在某个范围（如 160 至 170 厘米之间）的概率。问题是，在典型的现实情况下，我们不知道参数
    *μ* 厘米和 *σ*。我们所有的是成年 Statsville 女性居民的身高值的大型数据集 *X*——训练数据。我们必须使用这些数据来估计未知的参数 *μ*
    厘米和 *σ*。一旦我们有了这些，我们就有了从其中可以预测感兴趣事件概率的估计分布（即模型）。
- en: As we saw in section [6.6.2](../Text/06.xhtml#sec-max_likelihood_estimation),
    MLE is a technique to estimate the parameters from given training data when the
    family to which the distribution belongs is known but the exact values of the
    parameters are not known. Listing 6.4 shows the PyTorch implementation of MLE
    for the Gaussian family.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [6.6.2](../Text/06.xhtml#sec-max_likelihood_estimation) 节中看到的，MLE 是一种技术，当已知分布所属的家族但不知道参数的确切值时，用于从给定的训练数据中估计参数。列表
    6.4 展示了 PyTorch 对高斯家族的 MLE 实现。
- en: NOTE Fully functional code for model parameter estimation using MLE and MAP,
    executable via Jupyter Notebook, can be found at [http://mng.bz/9Mv7](http://mng.bz/9Mv7).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：使用 MLE 和 MAP 估计模型参数的完整代码，可通过 Jupyter Notebook 执行，可在 [http://mng.bz/9Mv7](http://mng.bz/9Mv7)
    找到。
- en: Listing 6.4 Maximum likelihood estimate for a Gaussian
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.4 高斯的最大似然估计
- en: '[PRE3]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Estimates Gaussian MLE parameters ![](../../OEBPS/Images/AR_micro.png) and
    Σ. They equal the sample mean and sample covariance of the training data. See
    equations [6.25](../Text/06.xhtml#eq-gauss-MLE-mean) and [6.26](../Text/06.xhtml#eq-gauss-MLE-covar).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ① 估计高斯 MLE 参数 ![](../../OEBPS/Images/AR_micro.png) 和 Σ。它们等于训练数据的样本均值和样本协方差。参见方程
    [6.25](../Text/06.xhtml#eq-gauss-MLE-mean) 和 [6.26](../Text/06.xhtml#eq-gauss-MLE-covar)。
- en: ② Defines a Gaussian with the estimated parameters
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ② 定义具有估计参数的高斯分布
- en: ③ Once the Gaussian is estimated, we can use it to predict probabilities.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 一旦估计出高斯分布，我们就可以用它来预测概率。
- en: 6.8.2 Python PyTorch code for maximum likelihood estimation using gradient descent
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8.2 使用梯度下降的 Python PyTorch 代码进行最大似然估计
- en: 'In listing 6.4, we computed the MLE using the closed-form solution. Now, let’s
    try to compute the MLE using a different method: gradient descent. In real-life
    scenarios, we do not use gradient descent to compute the MLE because the closed-form
    solution is available. However, we discuss this method here to highlight some
    of the challenges of using gradient descent and how MAP estimation addresses these
    challenges.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 6.4 中，我们使用闭式解计算了最大似然估计。现在，让我们尝试使用不同的方法来计算最大似然估计：梯度下降。在实际场景中，我们不会使用梯度下降来计算最大似然估计，因为闭式解是可用的。然而，我们在这里讨论这种方法，以突出使用梯度下降的一些挑战以及如何通过最大后验估计来解决这些挑战。
- en: 'Our goal is to maximize the likelihood function using gradient descent. This
    can alternatively be viewed as minimizing the negative log-likelihood function.
    We choose to use the logarithm of the likelihood function since that leads to
    simpler computation without any loss of generalization. (If you want a quick refresher
    on gradient descent, see section [3.5](../Text/03.xhtml#sec-gradient-descent).)
    Following is the equation for negative log-likelihood:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是使用梯度下降来最大化似然函数。这也可以被视为最小化负对数似然函数。我们选择使用似然函数的对数，因为这会导致计算更简单，而不会损失泛化能力。（如果你想要快速复习梯度下降，请参阅第
    [3.5](../Text/03.xhtml#sec-gradient-descent) 节。）以下为负对数似然方程：
- en: '![](../../OEBPS/Images/eq_06-27.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_06-27.png)'
- en: Equation 6.27
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.27
- en: Listings 6.5 and 6.6 show the PyTorch code for the minimization process.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.5 和 6.6 展示了 PyTorch 代码的最小化过程。
- en: Listing 6.5 Gaussian negative log-likelihood for training data
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.5 训练数据的 Gaussian 负对数似然
- en: '[PRE4]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Equation [6.27](../Text/06.xhtml#eq-neg-log-likelihood)
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ① 方程式 [6.27](../Text/06.xhtml#eq-neg-log-likelihood)
- en: ② *n*/2 *log* 2 *πσ*²
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ② *n*/2 *log* 2 *πσ*²
- en: ③ (Σ[*i* = 1]*^n* (*x*[i] – *μ*)²)/(2*σ*²)
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ③ (Σ[*i* = 1]*^n* (*x*[i] – *μ*)²)/(2*σ*²)
- en: ④ Note how all the training data *X* is crunched in a single operation. Such
    vector operations are parallel and very efficient in PyTorch.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 注意所有训练数据 *X* 都在一个操作中压缩。这种向量操作在 PyTorch 中是并行且非常高效的。
- en: Listing 6.6 Minimizing MLE loss via gradient descent
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.6 通过梯度下降最小化 MLE 损失
- en: '[PRE5]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Negative log-likelihood (listing [6.5](../Text/06.xhtml#code-NLL))
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ① 负对数似然（列表 [6.5](../Text/06.xhtml#code-NLL)）
- en: ② Iterates to train
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ② 迭代训练
- en: ③ Computes the loss
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算损失
- en: ④ Computes the gradients of the loss with regard to μ and σ. PyTorch stores
    the gradients in μ.grad and σ.grad.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算损失相对于 μ 和 σ 的梯度。PyTorch 将梯度存储在 μ.grad 和 σ.grad 中。
- en: ⑤ Scales the gradients by learning the rate and update parameters
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 通过学习速率和更新参数来缩放梯度
- en: ⑥ Resets the gradients to zero post-update
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 更新后重置梯度为零
- en: '![](../../OEBPS/Images/CH06_F05a_Chaudhury.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F05a_Chaudhury.png)'
- en: '(a) MLE explodes: *μ[init]* = 1, *σ[init]* = 1.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 最大似然估计爆炸：*μ[init]* = 1，*σ[init]* = 1。
- en: '![](../../OEBPS/Images/CH06_F05b_Chaudhury.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F05b_Chaudhury.png)'
- en: '(b) MLE converges: *μ[init]* = 100, *σ[init]* = 10.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 最大似然估计收敛：*μ[init]* = 100，*σ[init]* = 10。
- en: '![](../../OEBPS/Images/CH06_F05c_Chaudhury.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F05c_Chaudhury.png)'
- en: '(c) MAP converges: *μ[init]* = 1, *σ[init]* = 1.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 最大后验估计收敛：*μ[init]* = 1，*σ[init]* = 1。
- en: Figure 6.5 Gaussian parameter estimation using maximum likelihood estimate and
    maximum a posteriori estimation. In figure [6.5a](../Text/06.xhtml#fig-gaussian-mle-explode),
    the MLE explodes because *μ* and *σ* are initialized far from *μ[expected]* and
    *σ[expected]*. However, the MLE converges in figure [6.5b](../Text/06.xhtml#fig-gaussian-mle-fit)
    because *μ* and *σ* are initialized closed to *μ[expected]* and *σ[expected]*.
    Figure [6.5c](../Text/06.xhtml#fig-gaussian-map-fit) shows how, for MAP, *μ* and
    *σ* are able to converge to *μ*.*[expected]* and *σ[expected]* even though they
    are initialized far away.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 使用最大似然估计和最大后验估计进行高斯参数估计。在图 [6.5a](../Text/06.xhtml#fig-gaussian-mle-explode)
    中，最大似然估计爆炸，因为 *μ* 和 *σ* 的初始化远离 *μ[expected]* 和 *σ[expected]*。然而，在图 [6.5b](../Text/06.xhtml#fig-gaussian-mle-fit)
    中，最大似然估计收敛，因为 *μ* 和 *σ* 的初始化接近 *μ[expected]* 和 *σ[expected]*。图 [6.5c](../Text/06.xhtml#fig-gaussian-map-fit)
    展示了对于最大后验估计，*μ* 和 *σ* 即使初始化远离 *μ[expected]* 和 *σ[expected]*，也能收敛到 *μ*.*[expected]*
    和 *σ[expected]*。
- en: Figure [6.5](../Text/06.xhtml#fig-mle-map) shows how *μ* and *σ* change with
    each iteration of gradient descent. We expect *μ* and *σ* to end up close to *μ[expected]*
    and *σ[expected]*, respectively. However, when *μ* and *σ* start off far from
    *μ[expected]* and *σ[expected]* as in figure [6.5a](../Text/06.xhtml#fig-gaussian-mle-explode)),
    they do not converge to the expected values and instead become very large numbers.
    On the other hand, when they are instantiated with values closer to *μ[expected]*
    and *σ[expected]* as in figure [6.5b](../Text/06.xhtml#fig-gaussian-mle-fit)),
    they converge to the expected values. MLE is very sensitive to the initial values
    and has no mechanism to prevent the parameters from exploding. This is why MAP
    estimation is preferred. The prior *p*(*θ*) acts as a regularizer and prevents
    the parameters from becoming too large. Figure [6.5c](../Text/06.xhtml#fig-gaussian-map-fit)
    shows how *μ* and *σ* converge to the expected values using MAP even though they
    started far away.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6.5](../Text/06.xhtml#fig-mle-map) 展示了 *μ* 和 *σ* 如何随着梯度下降的每次迭代而变化。我们期望 *μ*
    和 *σ* 分别接近 *μ[expected]* 和 *σ[expected]*。然而，当 *μ* 和 *σ* 从远离 *μ[expected]* 和 *σ[expected]*
    的状态开始，如图 [6.5a](../Text/06.xhtml#fig-gaussian-mle-explode)) 所示时，它们不会收敛到期望值，反而变成了非常大的数字。另一方面，当它们被实例化为更接近
    *μ[expected]* 和 *σ[expected]* 的值，如图 [6.5b](../Text/06.xhtml#fig-gaussian-mle-fit))
    所示时，它们会收敛到期望值。最大似然估计（MLE）对初始值非常敏感，并且没有机制来防止参数爆炸。这就是为什么MAP估计更受欢迎。先验 *p*(*θ*) 作为正则化项，防止参数变得过大。图
    [6.5c](../Text/06.xhtml#fig-gaussian-map-fit) 展示了即使 *μ* 和 *σ* 从远离的位置开始，它们如何使用MAP收敛到期望值。
- en: 'The MAP loss function is as follows. Note that it is the same equation as the
    negative log-likelihood, but with two additional terms—*μ*² and *σ*²—that act
    as regularizers:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: MAP 损失函数如下。请注意，它与负对数似然函数相同，但增加了两个额外的项—*μ*² 和 *σ*²—作为正则化项：
- en: '![](../../OEBPS/Images/eq_06-28.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-28.png)'
- en: Equation 6.28
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.28
- en: Listing 6.7 Gaussian negative log-likelihood with regularization
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.7 带正则化的高斯负对数似然
- en: '[PRE6]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Equation [6.28](../Text/06.xhtml#eq-neg-log-likelihood-reg)
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: ① 方程 [6.28](../Text/06.xhtml#eq-neg-log-likelihood-reg)
- en: ② *n*/2 *log* 2 *πσ*²
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ② *n*/2 *log* 2 *πσ*²
- en: ③ (Σ[*i* = 1]*^n* (*x*[i] – *μ*)²)/(2*σ*²)
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: ③ (Σ[*i* = 1]*^n* (*x*[i] – *μ*)²)/(2*σ*²)
- en: ④ Negative log-likelihood
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 负对数似然
- en: ⑤ Regularization
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 正则化
- en: ⑥ Note how all the training data *X* is crunched in a single operation. Such
    vector operations are parallel and very efficient in PyTorch.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 注意所有训练数据 *X* 都在一个操作中压缩。这种向量操作在 PyTorch 中是并行且非常高效的。
- en: 6.9 Gaussian mixture models
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.9 高斯混合模型
- en: 'In many real-life problems, the simple unimodal (single-peak) probability distributions
    we learned about in chapter [5](../Text/05.xhtml#chap-prob) fail to model the
    true underlying distribution of the data. For instance, consider a situation where
    we are given the heights of many adult Statsville residents. Say there are two
    classes of adults in Statsville: male and female. The height data we have is *unlabeled*,
    meaning we do not know whether a given instance of height data is associated with
    a male or a female. Thus the data is one-dimensional, and there are two classes.
    Figure [6.6](../Text/06.xhtml#fig-gmm_1d_2cls) depicts the situation. None of
    the simple probability distributions we discussed in chapter [5](../Text/05.xhtml#chap-prob)
    can be fitted to figure [6.6](../Text/06.xhtml#fig-gmm_1d_2cls). But the two partial
    bells in figure [6.6a](../Text/06.xhtml#fig-gmm_1d_2cls_PDF) suggest that we should
    be able to mix a pair of Gaussians (each of which looks like a bell) to mimic
    this distribution. This is also consistent with our knowledge that the distribution
    represents not one but two classes, each of which can be reasonably represented
    individually by Gaussians. The point cloud also indicates two separate clusters
    of points. While a single Gaussian will not work, a mixture of two separate 1D
    Gaussians can (and, as we shall shortly see, will) work.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实生活中的问题中，我们在第[5](../Text/05.xhtml#chap-prob)章中学到的简单单峰（单峰值）概率分布无法模拟数据的真实潜在分布。例如，考虑这样一种情况，我们被给出了许多成年Statsville居民的身高。假设Statsville中有两个成年类别：男性和女性。我们拥有的身高数据是*未标记的*，这意味着我们不知道给定的身高数据实例是否与男性或女性相关联。因此，数据是一维的，有两个类别。图[6.6](../Text/06.xhtml#fig-gmm_1d_2cls)描述了这种情况。我们在第[5](../Text/05.xhtml#chap-prob)章中讨论的简单概率分布都无法拟合图[6.6](../Text/06.xhtml#fig-gmm_1d_2cls)。但是图[6.6a](../Text/06.xhtml#fig-gmm_1d_2cls_PDF)中的两个部分钟形表明，我们应该能够混合一对高斯分布（每个都像钟形）来模拟这种分布。这也与我们知道分布代表的是两个类别而不是一个类别，每个类别都可以合理地单独用高斯分布来表示的知识相符。点云也表明有两个独立的点簇。虽然单个高斯分布不起作用，但两个独立的单维高斯分布的组合可以（并且，我们将很快看到，将会）起作用。
- en: '![](../../OEBPS/Images/CH06_F06a_Chaudhury.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F06a_Chaudhury.png)'
- en: (a) PDF
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PDF
- en: '![](../../OEBPS/Images/CH06_F06b_Chaudhury.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F06b_Chaudhury.png)'
- en: (b) Sample point distribution
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 样本点分布
- en: Figure 6.6 Probability density functions (PDFs) and sample point distributions
    for 1D height data of adult male and female residents of Statsville
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 Statsville成年男性和女性居民一维身高数据的概率密度函数（PDF）和样本点分布
- en: 'Let’s now discuss a slightly more complex problem in which the data is two-dimensional
    and has three classes. Here we are given the weights and heights of three classes
    of Statsville residents: adult females, adult males, and children. Again, the
    data is *unlabeled*, meaning we do not know whether a given instance of (height,
    weight) data is associated with a man, woman, or child. This is depicted in figure
    [6.7](../Text/06.xhtml#fig-gmm_2d_3cls). Once again, none of the simple probability
    distributions we studied in chapter [5](../Text/05.xhtml#chap-prob) can be fitted
    to this situation. But the PDF shows three bell-shaped peaks, the point cloud
    shows three clusters, and the physical nature of the problem indicates three separate
    classes, each of which can be reasonably represented by Gaussian. While a single
    Gaussian will not work, a mixture of three separate 2D Gaussians can (and, as
    we shall shortly see, will) work.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来讨论一个稍微复杂一些的问题，其中数据是二维的，并且有三个类别。这里我们给出了Statsville居民三个类别的体重和身高：成年女性、成年男性和儿童。再次强调，数据是*未标记的*，这意味着我们不知道给定的（身高，体重）数据实例是否与男性、女性或儿童相关联。这如图[6.7](../Text/06.xhtml#fig-gmm_2d_3cls)所示。再次强调，我们在第[5](../Text/05.xhtml#chap-prob)章中研究的简单概率分布都无法拟合这种情况。但是PDF显示了三个钟形峰值，点云显示了三个簇，而问题的物理性质表明有三个独立的类别，每个类别都可以合理地用高斯分布来表示。虽然单个高斯分布不起作用，但三个独立的二维高斯分布的组合可以（并且，我们将很快看到，将会）起作用。
- en: '*A Gaussian mixture model (GMM) is a weighted combination of a specific number
    of Gaussian components*.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '*高斯混合模型（GMM）是特定数量高斯成分的加权组合*。'
- en: '![](../../OEBPS/Images/CH06_F07a_Chaudhury.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F07a_Chaudhury.png)'
- en: (a) PDF
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PDF
- en: '![](../../OEBPS/Images/CH06_F07b_Chaudhury.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F07b_Chaudhury.png)'
- en: (b) Sample point distributions
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 样本点分布
- en: Figure 6.7 Probability density functions (PDFs) and sample point distributions
    for 2D (height, weight) data of children, adult males, and adult females of Statsville
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 Statsville儿童、成年男性和成年女性二维（身高，体重）数据的概率密度函数（PDF）和样本点分布
- en: For instance, in our first problem with one dimension and two classes, we choose
    a mixture of two 1D Gaussians. For the second problem, we take a mixture of three
    2D Gaussians. Each individual Gaussian component corresponds to a specific class.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的第一个问题中，只有一个维度和两个类别，我们选择两个一维高斯分布的混合。对于第二个问题，我们采用三个二维高斯分布的混合。每个单独的高斯组件对应一个特定的类别。
- en: 6.9.1 Probability density function of the GMM
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9.1 GMM 的概率密度函数
- en: Formally,
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，
- en: The PDF for a GMM is
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 的 PDF 为
- en: '![](../../OEBPS/Images/eq_06-29.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-29.png)'
- en: Equation 6.29
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.29
- en: where *π[k]* is the weight of the *k*th Gaussian component, satisfying
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *π[k]* 是第 *k* 个高斯组件的权重，满足
- en: '![](../../OEBPS/Images/eq_06-29-a.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-29-a.png)'
- en: '*K* is the number of classes or Gaussian components, and 𝒩(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[k]*, **Σ***[k]*) defined in equation [5.23](../Text/05.xhtml#eq-multivar-normal))
    is the PDF for the *k*th Gaussian component. Such a GMM models a *K*-peaked PDF
    or, equivalently, a *K*-clustered sample point cloud.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '*K* 是类别数或高斯组件数，方程 [5.23](../Text/05.xhtml#eq-multivar-normal) 中定义的 𝒩(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[k]*, **Σ***[k]*) 是第 *k* 个高斯组件的概率密度函数（PDF）。这样的高斯混合模型模拟了一个
    *K* 峰 PDF 或等价地，一个 *K* 聚类样本点云。'
- en: 'For instance, the PDF and sample point clouds shown in figure [6.6](../Text/06.xhtml#fig-gmm_1d_2cls)
    correspond to the following Gaussian mixture:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图 [6.6](../Text/06.xhtml#fig-gmm_1d_2cls) 中所示的 PDF 和样本点云对应以下高斯混合模型：
- en: '![](../../OEBPS/Images/eq_06-29-b.png)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-29-b.png)'
- en: 'The 2D three-class problem, PDF, and sample point clouds shown in figure [6.7](../Text/06.xhtml#fig-gmm_2d_3cls)
    correspond to the following Gaussian mixture:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6.7](../Text/06.xhtml#fig-gmm_2d_3cls) 中所示的二维三类别问题、PDF 和样本点云对应以下高斯混合模型：
- en: '![](../../OEBPS/Images/eq_06-29-c.png)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-29-c.png)'
- en: The PDF and sample point distribution of the GMM depend on the values of *π[k]*s,
    *μ[k]*s, **Σ***[k]*s, and *K*. In particular, *K* influences the number of peaks
    in the PDF (although if two peaks are very close, sometimes they merge). It also
    influences the number of clusters in the sample point cloud (again, if two clusters
    are too close, they may not be visually distinct). The *π[k]*s regulate the relative
    heights of the hills. The *μ[k]*s and **Σ***[k]*s influence the individual hills
    in the PDF as well as the individual clusters in the sample point cloud. Specifically,
    *μ[k]* regulates the locations of the *k*th peak in the PDF and the centroid of
    the *k*th cluster in the sample point cloud. The **Σ***[k]*s regulate the shape
    of the *k*th individual hill and the *k*th cluster in the sample point cloud.
    Figures [6.8](../Text/06.xhtml#fig-gmm-1d), [6.9](../Text/06.xhtml#fig-gmm-2d-pis),
    [6.10](../Text/06.xhtml#fig-gmm-2d-mussigmas), and [6.11](../Text/06.xhtml#fig-gmm-2d-pt-distr)
    show some example GMMs with various values of these parameters.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 的 PDF 和样本点分布取决于 *π[k]*、*μ[k]*、**Σ***[k]* 和 *K* 的值。特别是，*K* 影响PDF中的峰值数量（尽管如果两个峰值非常接近，有时它们会合并）。它还影响样本点云中的聚类数量（再次，如果两个聚类太接近，它们可能不会在视觉上明显区分）。*π[k]*
    调节山丘的相对高度。*μ[k]* 和 **Σ***[k]* 影响PDF中的单个山丘以及样本点云中的单个聚类。具体来说，*μ[k]* 调节 PDF 中第 *k*
    个峰的位置和样本点云中第 *k* 个聚类的质心。**Σ***[k]* 调节第 *k* 个单个山丘和样本点云中第 *k* 个聚类的形状。图 [6.8](../Text/06.xhtml#fig-gmm-1d)、[6.9](../Text/06.xhtml#fig-gmm-2d-pis)、[6.10](../Text/06.xhtml#fig-gmm-2d-mussigmas)
    和 [6.11](../Text/06.xhtml#fig-gmm-2d-pt-distr) 展示了一些具有这些参数不同值的示例 GMM。
- en: Figure [6.8](../Text/06.xhtml#fig-gmm-1d) shows a pair of Gaussian distributions
    and various GMMs with those as components, with different values for the parameters.
    Figure [6.9](../Text/06.xhtml#fig-gmm-2d-pis) depicts 2D GMMs with various *π[k]*s.
    Figure [6.10](../Text/06.xhtml#fig-gmm-2d-mussigmas) shows GMMs with non-circular
    bases (non-symmetric Σs) and various *μ*s).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6.8](../Text/06.xhtml#fig-gmm-1d) 展示了一对高斯分布以及作为组件的各种高斯混合模型（GMM），参数值各不相同。图
    [6.9](../Text/06.xhtml#fig-gmm-2d-pis) 描述了具有各种 *π[k]* 的高斯混合模型。图 [6.10](../Text/06.xhtml#fig-gmm-2d-mussigmas)
    展示了具有非圆形基（非对称 Σ）和不同 *μ* 的高斯混合模型。
- en: '![](../../OEBPS/Images/CH06_F08a_Chaudhury.png)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F08a_Chaudhury.png)'
- en: (a) Gaussian components *μ*[1] = 152, *μ*[2] = 175, *σ*[1] = *σ*[2] = 9
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 高斯组件 *μ*[1] = 152, *μ*[2] = 175, *σ*[1] = *σ*[2] = 9
- en: '![](../../OEBPS/Images/CH06_F08b_Chaudhury.png)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F08b_Chaudhury.png)'
- en: (b) GMM with *π*[1] = 0.5, *π*[2] = 0.5
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: (b) *π*[1] = 0.5, *π*[2] = 0.5 的高斯混合模型
- en: '![](../../OEBPS/Images/CH06_F08c_Chaudhury.png)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F08c_Chaudhury.png)'
- en: (c) GMM with *π*[1] = 0.7, *π*[2] = 0.3
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: (c) *π*[1] = 0.7, *π*[2] = 0.3 的高斯混合模型
- en: '![](../../OEBPS/Images/CH06_F08d_Chaudhury.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F08d_Chaudhury.png)'
- en: (d) GMM with *π*[1] = 0.3, *π*[2] = 0.7
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: (d) GMM中*π*[1] = 0.3, *π*[2] = 0.7
- en: Figure 6.8 Various GMMs (solid curves) with the same Gaussian components (dotted
    and dashed curves, respectively) but different *π*[1] and *π*[2] values
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 各种GMMs（实线）具有相同的高斯组件（分别用虚线和虚线表示），但*π*[1]和*π*[2]的值不同
- en: Another way to visualize GMMs is via sample point distributions. Figure [6.11](../Text/06.xhtml#fig-gmm-2d-pt-distr)
    shows the sample points from a pair of 2D Gaussians and the points sampled from
    a GMM having those Gaussians as components and various mixture-selections probabilities.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可视化高斯混合模型（GMMs）的方法是通过样本点分布。图[6.11](../Text/06.xhtml#fig-gmm-2d-pt-distr)展示了从一对二维高斯分布中抽取的样本点，以及从具有这些高斯分布作为组件且具有不同混合选择概率的高斯混合模型中抽取的点。
- en: '![](../../OEBPS/Images/CH06_F09a_Chaudhury.jpg)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F09a_Chaudhury.jpg)'
- en: (a) *π*[1] = 0.5, *π*[2] = 0.5
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: (a) *π*[1] = 0.5, *π*[2] = 0.5
- en: '![](../../OEBPS/Images/CH06_F09b_Chaudhury.jpg)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F09b_Chaudhury.jpg)'
- en: (b) *π*[1] = 0.4, *π*[2] = 0.6
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: (b) *π*[1] = 0.4, *π*[2] = 0.6
- en: '![](../../OEBPS/Images/CH06_F09c_Chaudhury.jpg)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F09c_Chaudhury.jpg)'
- en: (c) *π*[1] = 0.7, *π*[2] = 0.3
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: (c) *π*[1] = 0.7, *π*[2] = 0.3
- en: '![](../../OEBPS/Images/CH06_F09d_Chaudhury.jpg)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F09d_Chaudhury.jpg)'
- en: (d) *π*[1] = 0.3, *π*[2] = 0.7
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: (d) *π*[1] = 0.3, *π*[2] = 0.7
- en: Figure 6.9 Two-dimensional GMMs with circular bases,
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 具有圆形基的两维GMMs，
- en: '![](../../OEBPS/Images/eq_06-29-d.png).'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/CH06_F09d_Chaudhury.jpg)'
- en: Note how the relative heights of the hills depend on *π*s.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 注意山丘的相对高度取决于*π*。
- en: '![](../../OEBPS/Images/CH06_F10a_Chaudhury.jpg)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F10a_Chaudhury.jpg)'
- en: (a) ![](../../OEBPS/Images/eq_06-29-e.png)
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ![](../../OEBPS/Images/eq_06-29-e.png)
- en: '![](../../OEBPS/Images/CH06_F10b_Chaudhury.jpg)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F10b_Chaudhury.jpg)'
- en: (b) ![](../../OEBPS/Images/eq_06-29-f.png)
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: (b) ![](../../OEBPS/Images/eq_06-29-f.png)
- en: Figure 6.10 Two-dimensional GMMs with elliptical bases, *π*[1] = 0.3, *π*[2]
    = 0.7. Note how the shape of the hill base depends on Σ and how the hill positions
    depend on the ![](../../OEBPS/Images/AR_micro.png)s.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 具有椭圆形基的两维GMMs，*π*[1] = 0.3, *π*[2] = 0.7。注意山丘基部的形状取决于Σ，以及山丘位置取决于![](../../OEBPS/Images/AR_micro.png)s。
- en: '![](../../OEBPS/Images/CH06_F11a_Chaudhury.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F11a_Chaudhury.png)'
- en: (a)
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![](../../OEBPS/Images/CH06_F11b_Chaudhury.png)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F11b_Chaudhury.png)'
- en: (b)
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: Figure 6.11 (a) ![](../../OEBPS/Images/eq_06-29-g.png). (b) 1, 000 random samples
    from a GMM with the same three component Gaussians as in (a) and *π*[1] = *π*[2]
    = 0.4, *π*[3] = 0.2. Note how the GMM sample distribution shape mimics the combined
    sample distribution shape of the component Gaussians.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 (a) ![](../../OEBPS/Images/eq_06-29-g.png). (b) 从具有与(a)中相同的三个高斯组件和*π*[1]
    = *π*[2] = 0.4, *π*[3] = 0.2的GMM中抽取的1,000个随机样本。注意GMM样本分布形状如何模仿组件高斯分布的合并样本分布形状。
- en: 'It can be proved that equation [6.29](../Text/06.xhtml#eq-gmm) is a proper
    probability: that is, it sums to 1 over the space of all possible inputs (all
    possible values of ![](../../OEBPS/Images/AR_x.png) in the *d*-dimensional space).
    Here is the proof outline:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明方程[6.29](../Text/06.xhtml#eq-gmm)是一个合适的概率：即在所有可能的输入空间（*d*-维空间中所有可能的![](../../OEBPS/Images/AR_x.png)的值）上求和为1。以下是证明概要：
- en: '![](../../OEBPS/Images/eq_06-29-h.png)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-29-h.png)'
- en: 6.9.2 Latent variables for class selection
  id: totrans-475
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9.2 用于类别选择的潜在变量
- en: Let’s discuss GMMs in more detail. In particular, we look at the physical meaning
    of the various terms in equation [6.29](../Text/06.xhtml#eq-gmm).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论GMMs。特别是，我们查看方程[6.29](../Text/06.xhtml#eq-gmm)中各种术语的物理意义。
- en: Before diving in, let’s introduce an auxiliary random variable *Z*, which effectively
    is a *class selector*. In the context of equation [6.29](../Text/06.xhtml#eq-gmm),
    *Z* can take discrete values in the range [1⋯*K*]. It thus follows a categorical
    distribution (see section [5.9.6](../Text/05.xhtml#sec-categorical-distr)). Physically,
    *Z* = *k* means the *k*th class—that is, the *k*th component of the Gaussian mixture—has
    been selected.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨之前，让我们引入一个辅助随机变量*Z*，它实际上是一个*类别选择器*。在方程[6.29](../Text/06.xhtml#eq-gmm)的上下文中，*Z*可以在[1⋯*K*]范围内取离散值。因此，它遵循分类分布（参见第[5.9.6](../Text/05.xhtml#sec-categorical-distr)节）。从物理上讲，*Z*
    = *k*意味着已经选择了第*k*个类别——即高斯混合模型的第*k*个组件。
- en: NOTE As usual, we are denoting the random variable with uppercase and the specific
    value it takes in a given instance with lowercase.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如往常一样，我们用大写字母表示随机变量，用小写字母表示它在特定实例中取的具体值。
- en: 'For instance, in the two-class problem shown in figure [6.6](../Text/06.xhtml#fig-gmm_1d_2cls),
    *Z* can take one of two values: 1 (implying adult female) or 2 (implying adult
    male). For the three-class problem shown in figure [6.7](../Text/06.xhtml#fig-gmm_2d_3cls),
    *Z* can take one of three values: 1 (adult female), 2 (adult male), or 3 (child).
    *Z* is called a *latent (hidden) random variable* because its values are not directly
    observed. Contrast this with the input random variable ![](../../OEBPS/Images/AR_x.png)
    whose values are explicitly observed. You may recognize *Z* as a latent variable
    in the GMM (latent variables were introduced in section [6.7](../Text/06.xhtml#sec-evidence_maximization)).'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在图 [6.6](../Text/06.xhtml#fig-gmm_1d_2cls) 中显示的两个类别问题中，*Z* 可以取两个值之一：1（表示成年女性）或2（表示成年男性）。在图
    [6.7](../Text/06.xhtml#fig-gmm_2d_3cls) 中显示的三个类别问题中，*Z* 可以取三个值之一：1（成年女性）、2（成年男性）或3（儿童）。*Z*
    被称为 *潜在（隐藏）随机变量*，因为其值不能直接观察到。这与输入随机变量 ![](../../OEBPS/Images/AR_x.png) 的值是明确观察到的形成对比。你可能会将
    *Z* 识别为 GMM 中的潜在变量（潜在变量在 [6.7](../Text/06.xhtml#sec-evidence_maximization) 节中介绍）。
- en: Consider the joint probability *p*(*X* = ![](../../OEBPS/Images/AR_x.png), *Z*
    = *k*), which we sometimes informally denote as *p*(![](../../OEBPS/Images/AR_x.png),
    *k*). This is the probability of the input variable ![](../../OEBPS/Images/AR_x.png)
    occurring together with the class *k*. Using Bayes’ theorem,
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑联合概率 *p*(*X* = ![](../../OEBPS/Images/AR_x.png), *Z* = *k*), 我们有时非正式地表示为 *p*(![](../../OEBPS/Images/AR_x.png),
    *k*). 这是输入变量 ![](../../OEBPS/Images/AR_x.png) 与类别 *k* 同时发生的概率。使用贝叶斯定理，
- en: '*p*(![](../../OEBPS/Images/AR_x.png), *k*) = *p*(![](../../OEBPS/Images/AR_x.png)|*k*)*p*(*k*)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(![](../../OEBPS/Images/AR_x.png), *k*) = *p*(![](../../OEBPS/Images/AR_x.png)|*k*)*p*(*k*)'
- en: The conditional probability term *p*(![](../../OEBPS/Images/AR_x.png)|*k*) is
    the probability of ![](../../OEBPS/Images/AR_x.png) when the *k*th class has been
    selected. This means it is the PDF for the *k*th Gaussian component, which is
    a Gaussian distribution by assumption. As such, using equation [5.23](../Text/05.xhtml#eq-multivar-normal),
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 条件概率项 *p*(![](../../OEBPS/Images/AR_x.png)|*k*) 是在选择了第 *k* 类时的 ![](../../OEBPS/Images/AR_x.png)
    的概率。这意味着它是第 *k* 个高斯成分的概率密度函数，根据假设这是一个高斯分布。因此，使用方程 [5.23](../Text/05.xhtml#eq-multivar-normal),
- en: '*p*(![](../../OEBPS/Images/AR_x.png)|*k*) = 𝒩(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[k]*, **Σ***[k]*) *k* ∈ [1, *K*]'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(![](../../OEBPS/Images/AR_x.png)|*k*) = 𝒩(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[k]*, **Σ***[k]*) *k* ∈ [1, *K*]'
- en: 'On the other hand, *p*(*Z* = *k*), which we sometimes informally refer to as
    *p*(*k*), is the *prior probability (that is, without reference to the input)
    of the input belonging to one of the classes*. Let’s denote it as follows:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*p*(*Z* = *k*), 我们有时非正式地称之为 *p*(*k*)，是输入属于某一类别的先验概率（即，不考虑输入）。让我们如下表示它：
- en: '*p*(*k*) = *π^k*, ∀*k* ∈ {1, *K*}'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*k*) = *π^k*, ∀*k* ∈ {1, *K*}'
- en: 'This is often modeled as the *fraction of training data points belonging to
    class k*:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被建模为属于类别 *k* 的训练数据点的 *分数*：
- en: '![](../../OEBPS/Images/eq_06-29-i.png)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-29-i.png)'
- en: where *N[k]* is the number of training data instances belonging to class *k*,
    and *N* is the total number of training data instances.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *N[k]* 是属于类别 *k* 的训练数据实例的数量，而 *N* 是训练数据实例的总数。
- en: From this, we get
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个方程中，我们得到
- en: '*p*(![](../../OEBPS/Images/AR_x.png), *k*) = *p*(*k*)*p*(![](../../OEBPS/Images/AR_x.png)|*k*)
    = *π^k* 𝒩(![](../../OEBPS/Images/AR_x.png); *![](../../OEBPS/Images/AR_micro.png)[k]*,
    **Σ***[k]*) *k* ∈ [1, *K*]'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(![](../../OEBPS/Images/AR_x.png), *k*) = *p*(*k*)*p*(![](../../OEBPS/Images/AR_x.png)|*k*)
    = *π^k* 𝒩(![](../../OEBPS/Images/AR_x.png); *![](../../OEBPS/Images/AR_micro.png)[k]*,
    **Σ***[k]*) *k* ∈ [1, *K*]'
- en: From equation [5.5](../Text/05.xhtml#eq-marginal-prob), we get the marginal
    probability *p*(*x*)
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程 [5.5](../Text/05.xhtml#eq-marginal-prob) 中，我们得到边缘概率 *p*(*x*)
- en: '![](../../OEBPS/Images/eq_06-29-j.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-29-j.png)'
- en: which is the same as equation [6.29](../Text/06.xhtml#eq-gmm).
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 这与方程 [6.29](../Text/06.xhtml#eq-gmm) 相同。
- en: 'This leads to the following physical interpretations:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下物理解释：
- en: A GMM can be viewed as a weighted sum of *K* Gaussian components. Equation [6.29](../Text/06.xhtml#eq-gmm)
    depicts the PDF of the overall GMM.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GMM 可以被视为 *K* 个高斯成分的加权求和。方程 [6.29](../Text/06.xhtml#eq-gmm) 描述了整体 GMM 的概率密度函数。
- en: 'The *weights* *π[k]* *are component selection probabilities*. Specifically,
    *π[k]* can be interpreted as the prior probability *p*(*Z* = *k*), aka *p*(*k*),
    of selecting the *k*th subclass—modeled as the fraction of the population belonging
    to the *k*th subclass. The *π[k]* are probabilities in a categorical distribution
    with *K* classes. The *π[k]*s sum up to 1. Sampling from the GMM can be viewed
    as a two-step process:'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重**π[k]**是组件选择概率。具体来说，**π[k]**可以解释为选择第k个子类的先验概率*p*(*Z* = *k*)，也称为*p*(*k*)，即第k个子类的比例——模型化为属于第k个子类的人口比例。**π[k]**是在具有*K*个类别的分类分布中的概率。**π[k]**的总和为1。从高斯混合模型（GMM）中采样可以看作是两步过程：
- en: Randomly select a component. The probability of the *k*th component being selected
    is *π[k]*. The sum of all *π[k]*s is 1, which signifies that one or another component
    must be selected.
  id: totrans-497
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个组件。第k个组件被选择的概率是**π[k]**。所有**π[k]**的总和为1，这表示必须选择一个或另一个组件。
- en: Random sample from the selected Gaussian component. The probability of generating
    vector ![](../../OEBPS/Images/AR_x.png) is 𝒩(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[k]*, **Σ***[k]*).
  id: totrans-498
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从所选的高斯组件中随机采样。生成向量![](../../OEBPS/Images/AR_x.png)的概率是𝒩(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[k]*, **Σ***[k]*)。
- en: Each of the *K* Gaussian components models an individual class. Geometrically
    speaking, the components correspond to the clusters in the sample point cloud
    or the peaks in the PDF of the GMM.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个*K*个高斯组件模型一个单独的类别。从几何上讲，这些组件对应于样本点云中的聚类或GMM概率密度函数（PDF）中的峰值。
- en: The *k*th Gaussian component, 𝒩(![](../../OEBPS/Images/AR_x.png); *![](../../OEBPS/Images/AR_micro.png)[k]*,
    **Σ***[k]*), can be interpreted as the conditional probability, *p*(![](../../OEBPS/Images/AR_x.png)|*k*).
    This is the likelihood—the probability of data value ![](../../OEBPS/Images/AR_x.png)
    occurring, *given* that the *k*th subclass has been selected.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第k个高斯组件，𝒩(![](../../OEBPS/Images/AR_x.png); *![](../../OEBPS/Images/AR_micro.png)[k]*,
    **Σ***[k]*)，可以解释为条件概率，*p*(![](../../OEBPS/Images/AR_x.png)|*k*)。这是似然——在选择了第k个子类的情况下，数据值![](../../OEBPS/Images/AR_x.png)发生的概率。
- en: The product *π[k]* 𝒩(![](../../OEBPS/Images/AR_x.png); *![](../../OEBPS/Images/AR_micro.png)[k]*,
    **Σ***[k]*) then represents the joint probability *p*(![](../../OEBPS/Images/AR_x.png),
    *k*) = *p*(![](../../OEBPS/Images/AR_x.png)|*k*) *p*(*k*).
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乘积**π[k]** 𝒩(![](../../OEBPS/Images/AR_x.png); *![](../../OEBPS/Images/AR_micro.png)[k]*,
    **Σ***[k]*)然后代表联合概率*p*(![](../../OEBPS/Images/AR_x.png), *k*) = *p*(![](../../OEBPS/Images/AR_x.png)|*k*)
    *p*(*k*)。
- en: The sum of all the joint subclass probabilities is the marginal probability
    *p*(![](../../OEBPS/Images/AR_x.png)) of the data value ![](../../OEBPS/Images/AR_x.png).
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有联合子类概率之和是数据值![](../../OEBPS/Images/AR_x.png)的边缘概率*p*(![](../../OEBPS/Images/AR_x.png))。
- en: Listing 6.8 Gaussian mixture model distribution
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.8高斯混合模型分布
- en: '[PRE7]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Pytorch supports distributions that are mixtures of the same family (here,
    Gaussian)
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: ① Pytorch支持同一家族（此处为高斯）的分布混合
- en: '② Prior probabilities over the three classes (male, female, child): categorical
    distribution'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: ② 三种类别（男性、女性、儿童）的先验概率：分类分布
- en: ③ Mean height, weight for the three classes (male, female, child)
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 三种类别（男性、女性、儿童）的平均身高和体重
- en: ④ Covariance matrices for the three classes male, female, child)
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 男性、女性、儿童三种类别的协方差矩阵
- en: ⑤ Creates the component Gaussians
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建组件高斯分布
- en: ⑥ Creates the GMM
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 创建高斯混合模型（GMM）
- en: 6.9.3 Classification via GMM
  id: totrans-511
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9.3 通过GMM进行分类
- en: 'A typical practical problem involving GMMs goes as follows. A set of unlabeled
    input data *X* training data) is provided. It is important to note that this is
    unsupervised machine learning—the training data does not come with known output
    classes. The physical nature of the problem indicates the subclasses in the data
    (denoted by indices [1⋯*K*]). The goal is to classify any arbitrary input ![](../../OEBPS/Images/AR_x.png):
    that is, map it to one of the *K* classes. To do this, we have to fit a GMM (that
    is, derive the values of *π[k]*, *![](../../OEBPS/Images/AR_micro.png)[k]*, **Σ***[k]*
    for all *k* ∈ [1 ⋯ *K*]). Given an arbitrary ![](../../OEBPS/Images/AR_x.png),
    we compute *p*(*k*|![](../../OEBPS/Images/AR_x.png)) for all the classes (all
    values of *k*). The value of *k* yielding the max value for *p*(*k*|![](../../OEBPS/Images/AR_x.png))
    is the class corresponding to ![](../../OEBPS/Images/AR_x.png). How do we compute
    *p*(*k*|![](../../OEBPS/Images/AR_x.png))?'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及 GMM 的典型实际问题如下。提供一组未标记的输入数据 *X*（训练数据）。重要的是要注意，这是一个无监督的机器学习——训练数据没有附带已知的输出类别。问题的物理性质表明数据中的子类（用索引
    [1⋯*K*] 表示）。目标是分类任何任意的输入 ![](../../OEBPS/Images/AR_x.png)：也就是说，将其映射到 *K* 个类别中的一个。为此，我们必须拟合一个
    GMM（即推导出 *π[k]*, *![](../../OEBPS/Images/AR_micro.png)[k]*, **Σ***[k]* 对于所有 *k*
    ∈ [1 ⋯ *K*]）的值）。给定一个任意的 ![](../../OEBPS/Images/AR_x.png)，我们计算所有类别的 *p*(*k*|![](../../OEBPS/Images/AR_x.png))。产生
    *p*(*k*|![](../../OEBPS/Images/AR_x.png)) 最大值的 *k* 是与 ![](../../OEBPS/Images/AR_x.png)
    对应的类别。我们是怎样计算 *p*(*k*|![](../../OEBPS/Images/AR_x.png)) 的呢？
- en: Using Bayes’ theorem,
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理，
- en: '![](../../OEBPS/Images/eq_06-30.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-30.png)'
- en: Equation 6.30
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.30
- en: 'If we know all the GMM parameters, evaluating equation [6.30](../Text/06.xhtml#eq-gmm-posterior-for-classification)
    is straightforward. We classify the input ![](../../OEBPS/Images/AR_x.png) by
    assigning it to the cluster *k* that yields the highest value of *p*(*Z* = *k*|*X*
    = *x*). Geometrically, this assigns the input to the cluster with the “closest”
    mean—with distance normalized by the variance of the respective distribution.
    Basically, we are measuring the distance from the mean, but in clusters of high
    variance, we are more tolerant of distance from the mean. This makes intuitive
    sense: if the cluster is widely spread has high variance), a point relatively
    far from the cluster mean can be said to belong to the cluster. On the other hand,
    a point the same distance from the mean of a tightly packed cluster may be deemed
    to be outside the cluster.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道所有的 GMM 参数，评估方程 [6.30](../Text/06.xhtml#eq-gmm-posterior-for-classification)
    是直接的。我们通过将输入 ![](../../OEBPS/Images/AR_x.png) 分配给产生最高 *p*(*Z* = *k*|*X* = *x*)
    值的簇 *k* 来对输入进行分类。从几何上看，这是将输入分配给具有“最接近”均值的簇——距离由相应分布的方差进行归一化。基本上，我们是在测量与均值的距离，但在高方差簇中，我们对与均值的距离更加宽容。这从直觉上是有意义的：如果一个簇分布广泛（即高方差），那么相对于簇均值较远的点可以被认为是属于该簇的。另一方面，与紧密排列的簇的均值相同距离的点可能被认为是簇外部的。
- en: 6.9.4 Maximum likelihood estimation of GMM parameters (GMM fit)
  id: totrans-517
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9.4 GMM 参数的最大似然估计（GMM 适配）
- en: A GMM is fully described in terms of its parameter set *θ* = {*π^k*, *![](../../OEBPS/Images/AR_micro.png)[k]*, **Σ***[k]*
    ∀*k *∈ [1 ⋯ *K*]}. But how do we estimate these parameter values? In typical real-life
    situations, they are not given to us. We only have a set of observed unlabeled
    training data points *X* = {![](../../OEBPS/Images/AR_x.png)^((*i*))}, such as
    (weight, height) values for Statsville residents.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: GMM（高斯混合模型）完全由其参数集 *θ* = {*π^k*, *![](../../OEBPS/Images/AR_micro.png)[k]*,
    **Σ***[k]* ∀*k *∈ [1 ⋯ *K*]} 描述。但我们是怎样估计这些参数值的呢？在典型的现实情况下，这些参数并没有给我们。我们只有一组观察到的未标记的训练数据点
    *X* = {![](../../OEBPS/Images/AR_x.png)^((*i*))}，例如 Statsville 居民的体重和身高值。
- en: Geometrically speaking, each data instance in the training dataset corresponds
    to a single point in the multidimensional feature space. The training dataset
    is a point cloud that naturally clusters into Gaussian subclouds (otherwise, we
    should not be trying GMMs). Our GMM mimicking this dataset should have as many
    components as there are natural clusters in the data. The parameter values *π[k]*,
    *![](../../OEBPS/Images/AR_micro.png)[k]*, **Σ***[k]* for *k * ∈  [1 ⋯ *K*] should
    be estimated such that the GMM’s sample point cloud overlaps the training data
    point cloud as much as possible. That is the basic problem we try to solve in
    this section.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何学的角度来看，训练数据集中的每个数据实例对应于多维特征空间中的一个单独点。训练数据集是一个自然聚集成高斯子云的点云（否则，我们就不应该尝试GMMs）。我们的GMM模仿这个数据集应该有与数据中自然聚类的数量一样多的组件。参数值*π[k]*，*![](../../OEBPS/Images/AR_micro.png)[k]*，**Σ***[k]*对于*k *
    ∈  [1 ⋯ *K*]应该被估计，使得GMM的样本点云尽可能多地与训练数据点云重叠。这正是本节我们试图解决的问题的基本问题。
- en: NOTE We do not estimate *K*, the number of classes; rather, we use a fixed value
    of *K*, usually estimated from the physical conditions of the problem. For example,
    in the problem with men, women, and children, it is pretty obvious that *K* =
    3.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们不估计*K*，即类别数量；而是使用一个固定的*K*值，通常从问题的物理条件中估计得出。例如，在涉及男人、女人和孩子的问题上，很明显*K* =
    3。
- en: In section [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation), we did
    MLE for a simple Gaussian. We computed an expression for the joint log-likelihood
    of all the training data given a Gaussian probability distribution. Then we took
    the gradient of that expression with respect to the parameters and equated it
    to zero. We were able to solve that equation to derive a *closed-form* solution
    for the parameters, ![](../../OEBPS/Images/AR_micro.png) and Σ (equations [6.25](../Text/06.xhtml#eq-gauss-MLE-mean)
    and [6.26](../Text/06.xhtml#eq-gauss-MLE-covar) ). This means we simplified the
    equation into a form where the unknown (to be solved) appeared alone on the left-hand
    side and there were only known entities on the right-hand side.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation)节中，我们对一个简单的高斯分布进行了最大似然估计（MLE）。我们计算了给定高斯概率分布的所有训练数据的联合对数似然表达式。然后，我们计算该表达式相对于参数的梯度，并将其等于零。我们能够解这个方程，从而推导出参数的*封闭形式*解，![](../../OEBPS/Images/AR_micro.png)和Σ（方程[6.25](../Text/06.xhtml#eq-gauss-MLE-mean)和[6.26](../Text/06.xhtml#eq-gauss-MLE-covar)）。这意味着我们将方程简化为一种形式，其中未知（需要求解）的量单独出现在左边，而右边只有已知实体。
- en: Unfortunately, with GMMs, equating the gradient of the log-likelihood to zero
    leads to an equation that has no closed-form solution. So, we cannot reduce the
    equation to a form where the unknowns *π[k]*s, *μ[k]*s, and **Σ***[k]* appear
    alone on the left-hand sides and only known entities (![](../../OEBPS/Images/AR_x.png)*[i]*s)
    appear on the right-hand side. Consequently, we have to go for an iterative approximation.
    We rewrite the equation we get by equating the gradient of the log-likelihood
    to zero such that the unknowns *μ*s and *σ*s appear alone on the right-hand side.
    It looks something like
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在使用GMMs时，将对数似然梯度的值设为零会导致一个没有封闭形式解的方程。因此，我们不能将方程简化为未知量*π[k]*s，*μ[k]*s，和**Σ***[k]*单独出现在左边，而只有已知实体(![](../../OEBPS/Images/AR_x.png)*[i]*s)出现在右边的形式。因此，我们必须求助于迭代近似。我们将通过将对数似然梯度的值设为零得到的方程重写，使得未知量*μ*s和*σ*s单独出现在右边。它看起来像这样
- en: '*π^k* = *f*[1](*X, Θ*)'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '*π^k* = *f*[1](*X, Θ*)'
- en: '*![](../../OEBPS/Images/AR_micro.png)[k]* = *f*[2](*X, Θ*)'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](../../OEBPS/Images/AR_micro.png)[k]* = *f*[2](*X, Θ*)'
- en: '**Σ***[k]* = *f*[3](*X, Θ*)'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '**Σ***[k]* = *f*[3](*X, Θ*)'
- en: 'where *f*[1], *f*[2], *f*[3] are some functions whose exact nature is unimportant
    at the moment. Note that the right-hand side also contains the unknowns: *θ* contains
    *π[k]*s, *μ[k]*s, and **Σ***[k]*. We cannot directly solve such equations, but
    we can use *iterative relaxation*, which works roughly as follows:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*f*[1]，*f*[2]，*f*[3]是一些函数，其确切性质在目前并不重要。请注意，右边也包含未知量：*θ*包含*π[k]*s，*μ[k]*s，和**Σ***[k]*。我们无法直接求解这样的方程，但我们可以使用*迭代松弛*，其工作原理大致如下：
- en: Start with random values of *π[k]*s, *![](../../OEBPS/Images/AR_micro.png)[k]*s,
    and **Σ***[k]*s.
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从随机的*π[k]*s，*![](../../OEBPS/Images/AR_micro.png)[k]*s，和**Σ***[k]*s值开始。
- en: Evaluate the right-hand side by plugging current values of *π[k]*s, *![](../../OEBPS/Images/AR_micro.png)[k]*s,
    and **Σ***[k]*s into functions *f*[1], *f*[2], and *f*[3].
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将*π[k]*s，*![](../../OEBPS/Images/AR_micro.png)[k]*s，和**Σ***[k]*s的当前值代入函数*f*[1]，*f*[2]，和*f*[3]来评估右边。
- en: Use the values estimated in step 2 to set new values of *π[k]*s, *![](../../OEBPS/Images/AR_micro.png)[k]*s,
    and **Σ***[k]*s.
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤2中估计的值来设置新的*π[k]*s、*![](../../OEBPS/Images/AR_micro.png)[k]*s和**Σ***[k]*s的值。
- en: Repeat steps 1–3 until the parameter values stop changing appreciably.
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1-3，直到参数值不再明显变化。
- en: The actual functions *f*[1], *f*[2], *f*[3] are worked out in equations [6.36](../Text/06.xhtml#eq-gmmfit-pi),
    [6.37](../Text/06.xhtml#eq-gmmfit-mu), and [6.38](../Text/06.xhtml#eq-gmmfit-sigma)).
    As iteration progresses, the values of *π[k]*s, *![](../../OEBPS/Images/AR_micro.png)[k]*s,
    and **Σ***[k]*s start to converge to their true values. This is not a lucky coincidence.
    If we follow algorithm [6.3](../Text/06.xhtml#alg-gmm_fit), it can be proved that
    every iteration improves the approximation, even if by a minuscule amount. Eventually,
    we reach a point when the approximation is no longer improving appreciably. This
    is called the *fixed point*, and we should stop iterating and declare the current
    values final.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 实际函数*f*[1]、*f*[2]、*f*[3]在方程[6.36](../Text/06.xhtml#eq-gmmfit-pi)、[6.37](../Text/06.xhtml#eq-gmmfit-mu)、[6.38](../Text/06.xhtml#eq-gmmfit-sigma)中给出。随着迭代的进行，*π[k]*s、*![](../../OEBPS/Images/AR_micro.png)[k]*s和**Σ***[k]*s的值开始收敛到它们的真实值。这不是幸运的巧合。如果我们遵循算法[6.3](../Text/06.xhtml#alg-gmm_fit)，可以证明每一次迭代都会改善近似，即使改善的量非常小。最终，我们达到一个点，此时近似不再明显改善。这被称为*固定点*，我们应该停止迭代并宣布当前值为最终值。
- en: 'Figure [6.12](../Text/06.xhtml#fig-GMM-fit-iters) shows the progression of
    an iterative GMM fit algorithm. Figure [6.12a](../Text/06.xhtml#fig-GMM-fit-gaussians)
    shows the sampled training data distribution. Figure [6.12b](../Text/06.xhtml#fig-GMM-fit-step0)
    shows the fitted GMM at the beginning: the parameters are essentially random,
    and the GMM looks nothing like the target training data distribution. It improves
    slowly until at iteration 15, it matches the target distribution snugly figure
    [6.12d](../Text/06.xhtml#fig-GMM-fit-step15)).'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6.12](../Text/06.xhtml#fig-GMM-fit-iters)展示了迭代GMM拟合算法的进展。图[6.12a](../Text/06.xhtml#fig-GMM-fit-gaussians)展示了采样训练数据分布。图[6.12b](../Text/06.xhtml#fig-GMM-fit-step0)展示了初始拟合的GMM：参数基本上是随机的，GMM看起来与目标训练数据分布毫不相似。它缓慢地改进，直到迭代15次，它紧密地匹配了目标分布[6.12d](../Text/06.xhtml#fig-GMM-fit-step15))。
- en: '![](../../OEBPS/Images/CH06_F12a_Chaudhury.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F12a_Chaudhury.png)'
- en: (a) Training data point cloud (target for fitting)
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 训练数据点云（拟合的目标）
- en: '![](../../OEBPS/Images/CH06_F12b_Chaudhury.png)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F12b_Chaudhury.png)'
- en: (b) Fitted GMM’s sample point cloud at step 0
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 第0步拟合的GMM样本点云
- en: '![](../../OEBPS/Images/CH06_F12c_Chaudhury.png)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F12c_Chaudhury.png)'
- en: (c) Fitted GMM’s sample point cloud at step 5
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 第5步拟合的GMM样本点云
- en: '![](../../OEBPS/Images/CH06_F12d_Chaudhury.png)'
  id: totrans-539
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F12d_Chaudhury.png)'
- en: (d) Fitted GMM’s sample point cloud at step 15\. It almost matches the target.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 第15步拟合的GMM样本点云。它几乎与目标匹配。
- en: Figure 6.12 Progression of maximum likelihood estimation for GMM parameters
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 GMM参数最大似然估计的进展
- en: Now let’s discuss the details. We *already know* the dataset *X* that has been
    observed. What parameter set *θ* will maximize the conditional probability, *p*(*X*|*θ*),
    of exactly these data points, given the parameter set? In other words, what model
    parameters will maximize the overall likelihood of the training data? Those will
    be our best guesses for the unknown model parameters. This is MLE, which we encountered
    in section [6.6.2](../Text/06.xhtml#sec-max_likelihood_estimation).
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来讨论细节。我们*已经知道*观察到的数据集*X*。哪个参数集*θ*将最大化给定参数集的这些数据点的条件概率*p*(*X*|*θ*)？换句话说，哪些模型参数将最大化训练数据的整体似然？那些将是我们对未知模型参数的最佳猜测。这是我们在第[6.6.2](../Text/06.xhtml#sec-max_likelihood_estimation)节中遇到的MLE。
- en: Let {![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((*n*)),⋯![](../../OEBPS/Images/AR_x.png)^((*n*))}
    be the set of observed data points, aka training data. From equation [6.29](../Text/06.xhtml#eq-gmm),
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 设{![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((*n*)),⋯![](../../OEBPS/Images/AR_x.png)^((*n*))}为观察数据点的集合，即训练数据。从方程[6.29](../Text/06.xhtml#eq-gmm)，
- en: '![](../../OEBPS/Images/eq_06-30-a.png)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-30-a.png)'
- en: Henceforth, for simplicity, we drop the “given *θ*” part and refer to *p*(![](../../OEBPS/Images/AR_x.png)^((*i*))|*θ*)
    simply as *p*(![](../../OEBPS/Images/AR_x.png)^((*i*))). As usual, instead of
    maximizing the likelihood directly, we maximize its logarithm, the *log-likelihood*.
    This will yield the same parameters as maximizing the likelihood directly.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 从此以后，为了简便起见，我们省略了“给定 *θ*”的部分，并将 *p*(![](../../OEBPS/Images/AR_x.png)^((*i*))|*θ*)
    简单地称为 *p*(![](../../OEBPS/Images/AR_x.png)^((*i*))). 如同往常，我们不是直接最大化似然，而是最大化其对数，即
    *log-likelihood*。这将产生与直接最大化似然相同的参数。
- en: Since the *x*^((*i*))s are independent, their joint probability, as per equation
    [5.4](../Text/05.xhtml#eq-joint-prob-indep), is
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *x*^((*i*))s 是独立的，根据方程 [5.4](../Text/05.xhtml#eq-joint-prob-indep)，它们的联合概率是
- en: '*p*(![](../../OEBPS/Images/AR_x.png)^((1)))*p*(![](../../OEBPS/Images/AR_x.png)^((2)))⋯*p*(![](../../OEBPS/Images/AR_x.png)^((*n*)))'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(![](../../OEBPS/Images/AR_x.png)^((1)))*p*(![](../../OEBPS/Images/AR_x.png)^((2)))⋯*p*(![](../../OEBPS/Images/AR_x.png)^((*n*)))'
- en: The corresponding log joint probability is
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的对数联合概率是
- en: '![](../../OEBPS/Images/eq_06-31.png)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-31.png)'
- en: Equation 6.31
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.31
- en: At this point, we begin to see a difficulty peculiar to GMMs. We have a logarithm
    of a sum, which is not a very friendly expression to handle; the logarithm of
    products is much nicer to deal with. But let’s soldier on.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们开始看到 GMMs 的一个特殊困难。我们有一个和的对数，这不是一个很容易处理的表达式；乘积的对数则更容易处理。但让我们继续前进。
- en: 'To identify the parameters ![](../../OEBPS/Images/AR_micro.png)¹, Σ[1], ![](../../OEBPS/Images/AR_micro.png)²,
    Σ[2], ⋯ that will maximize the log joint probability, we take the gradient of
    the log joint probability with respect to these parameters, equate them to zero,
    and solve for the parameter value (as discussed in section [3.3.1](../Text/03.xhtml#sec-gradient)).
    Here we demonstrate the process with respect to ![](../../OEBPS/Images/AR_micro.png)[1]:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别将最大化对数联合概率的参数 ![](../../OEBPS/Images/AR_micro.png)¹, Σ[1], ![](../../OEBPS/Images/AR_micro.png)²,
    Σ[2], ⋯，我们取对数联合概率对这些参数的梯度，将它们设为零，并求解参数值（如第 [3.3.1](../Text/03.xhtml#sec-gradient)
    节所述）。在这里，我们以 ![](../../OEBPS/Images/AR_micro.png)[1] 为例展示这个过程：
- en: ∇[![](../../OEBPS/Images/AR_micro.png)[1]]*log*(*p*(![](../../OEBPS/Images/AR_x.png)^((1)))*p*(![](../../OEBPS/Images/AR_x.png)^((2)))⋯*p*(![](../../OEBPS/Images/AR_x.png)^((*n*))))
    = 0
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: ∇[![](../../OEBPS/Images/AR_micro.png)[1]]*log*(*p*(![](../../OEBPS/Images/AR_x.png)^((1)))*p*(![](../../OEBPS/Images/AR_x.png)^((2)))⋯*p*(![](../../OEBPS/Images/AR_x.png)^((*n*))))
    = 0
- en: Since the log of products is the sum of logs, we get
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 由于乘积的对数是各个对数的和，我们得到
- en: '![](../../OEBPS/Images/eq_06-31-a.png)'
  id: totrans-555
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-31-a.png)'
- en: Applying equation [6.29](../Text/06.xhtml#eq-gmm), we get
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程 [6.29](../Text/06.xhtml#eq-gmm) 应用于我们的最大化问题，我们得到
- en: '![](../../OEBPS/Images/eq_06-31-b.png)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-31-b.png)'
- en: 'Since the gradient is a linear operator, we can move it inside the summation:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 由于梯度是一个线性算子，我们可以将其移到求和符号内部：
- en: '![](../../OEBPS/Images/eq_06-31-c.png)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-31-c.png)'
- en: Since *d*/*dx* log (*f*(*x*)) = 1/*f*(*x*) *df*/*dx*, we get
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *d*/*dx* log (*f*(*x*)) = 1/*f*(*x*) *df*/*dx*，我们得到
- en: '![](../../OEBPS/Images/eq_06-31-d.png)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-31-d.png)'
- en: Now, if *x*[1] and *x*[2] are independent variables, *dx*[2]/*dx*[1] = 0. Consequently,
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果 *x*[1] 和 *x*[2] 是独立变量，则 *dx*[2]/*dx*[1] = 0。因此，
- en: '![](../../OEBPS/Images/eq_06-31-e.png)'
  id: totrans-563
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-31-e.png)'
- en: Only a single term corresponding to *k* = 1 survives the differentiation (gradient)
    in the numerator. So,
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 只有与 *k* = 1 对应的单个项在分子中的微分（梯度）过程中幸存。所以，
- en: '![](../../OEBPS/Images/eq_06-31-f.png)'
  id: totrans-565
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-31-f.png)'
- en: Now *d*/*dx* *e*^(–(*x – μ*)²) = –2(*x – μ*) *e*^(–(*x – μ*)²), and in multiple
    dimensions,
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 *d*/*dx* *e*^(–(*x – μ*)²) = –2(*x – μ*) *e*^(–(*x – μ*)²)，在多维情况下，
- en: '![](../../OEBPS/Images/eq_06-31-g.png)'
  id: totrans-567
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-31-g.png)'
- en: Plugging equation [5.23](../Text/05.xhtml#eq-multivar-normal) into our maximization
    problem, we get
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程 [5.23](../Text/05.xhtml#eq-multivar-normal) 带入我们的最大化问题，我们得到
- en: '![](../../OEBPS/Images/eq_06-31-h.png)'
  id: totrans-569
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-31-h.png)'
- en: 'Furthermore, with a little effort, you can prove the following about the gradient
    of a quadratic form:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过一点努力，你可以证明关于二次型梯度的以下内容：
- en: ∇[![](../../OEBPS/Images/AR_x.png)](![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png))
    = *A*![](../../OEBPS/Images/AR_x.png)
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: ∇[![](../../OEBPS/Images/AR_x.png)](![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png))
    = *A*![](../../OEBPS/Images/AR_x.png)
- en: Equation 6.32
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.32
- en: Applying equation [6.32](../Text/06.xhtml#eq-grad-quad_form) to our problem,
    we get
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程 [6.32](../Text/06.xhtml#eq-grad-quad_form) 应用于我们的问题，我们得到
- en: '![](../../OEBPS/Images/eq_06-32-a.png)'
  id: totrans-574
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-32-a.png)'
- en: Multiplying both sides by the constant **Σ**[1], we get
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 将等式两边乘以常数 **Σ**[1]，我们得到
- en: '![](../../OEBPS/Images/eq_06-32-b.png)'
  id: totrans-576
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-32-b.png)'
- en: Substituting
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 代入
- en: '![](../../OEBPS/Images/eq_06-33.png)'
  id: totrans-578
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-33.png)'
- en: Equation 6.33
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.33
- en: we get
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到
- en: '![](../../OEBPS/Images/eq_06-33-a.png)'
  id: totrans-581
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-33-a.png)'
- en: This expression has *μ*[1] inside *γ*[*i*1] as well. It is impossible to extract
    *μ*[1] alone on the left side of the equation. In other words, we cannot create
    a *closed-form* solution for *μ*[1]. Hence, we have to solve it iteratively.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式在 *γ*[*i*1] 中也有 *μ*[1]。在方程的左侧不可能单独提取 *μ*[1]。换句话说，我们无法为 *μ*[1] 创建一个 *封闭形式*
    的解。因此，我们必须迭代求解。
- en: We can rewrite the previous equation as
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面的方程重写为
- en: '![](../../OEBPS/Images/eq_06-33-b.png)'
  id: totrans-584
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-33-b.png)'
- en: where
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../../OEBPS/Images/eq_06-34.png)'
  id: totrans-586
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-34.png)'
- en: Equation 6.34
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.34
- en: 'Proceeding similarly, we can derive the corresponding expressions for *π*[1]
    and **Σ**[1]. Let’s collect all the equations for updating the GMM parameters:'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，我们可以推导出 *π*[1] 和 **Σ**[1] 的相应表达式。让我们收集更新 GMM 参数的所有方程：
- en: '![](../../OEBPS/Images/eq_06-35.png)'
  id: totrans-589
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-35.png)'
- en: Equation 6.35
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.35
- en: '![](../../OEBPS/Images/eq_06-36.png)'
  id: totrans-591
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-36.png)'
- en: Equation 6.36
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.36
- en: '![](../../OEBPS/Images/eq_06-37.png)'
  id: totrans-593
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-37.png)'
- en: Equation 6.37
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.37
- en: '![](../../OEBPS/Images/eq_06-38.png)'
  id: totrans-595
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-38.png)'
- en: Equation 6.38
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 6.38
- en: Equations [6.36](../Text/06.xhtml#eq-gmmfit-pi), [6.37](../Text/06.xhtml#eq-gmmfit-mu),
    and [6.38](../Text/06.xhtml#eq-gmmfit-sigma) provide the definitions for functions
    *f*[1], *f*[2], and *f*[3] that we saw at the beginning of this section in the
    context of iterative relaxation. We can deal similarly with *k* = 2⋯*K*.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 [6.36](../Text/06.xhtml#eq-gmmfit-pi)，[6.37](../Text/06.xhtml#eq-gmmfit-mu)，和
    [6.38](../Text/06.xhtml#eq-gmmfit-sigma) 提供了我们在本节开头关于迭代松弛的上下文中看到的函数 *f*[1]，*f*[2]，和
    *f*[3] 的定义。我们可以以类似的方式处理 *k* = 2⋯*K*。
- en: Physical significance of *γ[ik]*
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '*γ[ik]* 的物理意义'
- en: We encountered the entity *γ[ik]* while computing the gradient of the log-likelihood.
    It appeared as a multiplicative weight in the final iterative expression for computing
    *μ[k]* and *Σ[k]* in equations [6.37](../Text/06.xhtml#eq-gmmfit-mu) and [6.38](../Text/06.xhtml#eq-gmmfit-sigma).
    It is not an arbitrary entity. By comparing equations [6.33](../Text/06.xhtml#eq-gmmfit-gamma)
    and [6.30](../Text/06.xhtml#eq-gmm-posterior-for-classification), we can see that
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算对数似然梯度时，我们遇到了实体 *γ[ik]*。它作为乘性权重出现在计算方程 [6.37](../Text/06.xhtml#eq-gmmfit-mu)
    和 [6.38](../Text/06.xhtml#eq-gmmfit-sigma) 中 *μ[k]* 和 *Σ[k]* 的最终迭代表达式中。它不是一个任意实体。通过比较方程
    [6.33](../Text/06.xhtml#eq-gmmfit-gamma) 和 [6.30](../Text/06.xhtml#eq-gmm-posterior-for-classification)，我们可以看到
- en: '*γ[ik]* = *p*(*k*|![](../../OEBPS/Images/AR_x.png)^((*i*)))'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '*γ[ik]* = *p*(*k*|![](../../OEBPS/Images/AR_x.png)^((*i*)))'
- en: 'In other words, the quantity *γ[ik]* is really the posterior probability: the
    conditional probability of the class *k* given the *i*th data point.'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，量 *γ[ik]* 实际上是后验概率：给定第 *i* 个数据点的类别 *k* 的条件概率。
- en: 'This gives us a new way to look at equations [6.35](../Text/06.xhtml#eq-gmmfit-Nk),
    [6.36](../Text/06.xhtml#eq-gmmfit-pi), [6.37](../Text/06.xhtml#eq-gmmfit-mu),
    and [6.38](../Text/06.xhtml#eq-gmmfit-sigma):'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了观察方程 [6.35](../Text/06.xhtml#eq-gmmfit-Nk)，[6.36](../Text/06.xhtml#eq-gmmfit-pi)，[6.37](../Text/06.xhtml#eq-gmmfit-mu)，和
    [6.38](../Text/06.xhtml#eq-gmmfit-sigma) 的新方法：
- en: Equation [6.35](../Text/06.xhtml#eq-gmmfit-Nk) essentially assigns to *N*[1]
    the probability mass concentrated in class 1 as per the current parameter values.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程式 [6.35](../Text/06.xhtml#eq-gmmfit-Nk) 实质上根据当前参数值将 *N*[1] 赋值为集中在类别 1 中的概率质量。
- en: Equation [6.36](../Text/06.xhtml#eq-gmmfit-pi) assigns to *π*[1] the fractional
    mass in class 1 as per the current parameter values.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程式 [6.36](../Text/06.xhtml#eq-gmmfit-pi) 将 *π*[1] 赋值为根据当前参数值在类别 1 中的分数质量。
- en: Equation [6.37](../Text/06.xhtml#eq-gmmfit-mu) assigns to *μ*[1] the centroid
    of all the training data points. Each data point’s contribution is weighted by
    the posterior probability, as per the current parameter values, of that data point
    belonging to class 1.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程式 [6.37](../Text/06.xhtml#eq-gmmfit-mu) 将 *μ*[1] 赋值为所有训练数据点的质心。每个数据点的贡献都通过后验概率加权，该概率是根据当前参数值计算出的该数据点属于类别
    1 的概率。
- en: Equation [6.38](../Text/06.xhtml#eq-gmmfit-sigma) assigns to **Σ**[1] the covariance
    of the training data points. Each data point’s contribution is weighted by the
    posterior probability, as per the current parameter values, of that data point
    belonging to class 1.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程式 [6.38](../Text/06.xhtml#eq-gmmfit-sigma) 将 **Σ**[1] 赋值为训练数据点的协方差。每个数据点的贡献都通过后验概率加权，该概率是根据当前参数值计算出的该数据点属于类别
    1 的概率。
- en: Algorithm [6.3](../Text/06.xhtml#alg-gmm_fit) ties together equations [6.33](../Text/06.xhtml#eq-gmmfit-gamma),
    [6.36](../Text/06.xhtml#eq-gmmfit-pi), [6.37](../Text/06.xhtml#eq-gmmfit-mu),
    and [6.38](../Text/06.xhtml#eq-gmmfit-sigma) into a complete approach for iterative
    MLE of GMM parameters. It is an example of a general class of algorithms called
    *expectation maximization*.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 算法[6.3](../Text/06.xhtml#alg-gmm_fit)将方程[6.33](../Text/06.xhtml#eq-gmmfit-gamma)，[6.36](../Text/06.xhtml#eq-gmmfit-pi)，[6.37](../Text/06.xhtml#eq-gmmfit-mu)，和[6.38](../Text/06.xhtml#eq-gmmfit-sigma)结合成一个完整的迭代MLE
    GMM参数的方法。它是一类称为*期望最大化*的通用算法的例子。
- en: Algorithm 6.3 GMM fit (MLE of GMM parameters from unlabeled training data)
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 算法6.3 GMM拟合（从未标记的训练数据中估计GMM参数的MLE）
- en: 'Input: *X* = ![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),
    … , ![](../../OEBPS/Images/AR_x.png)^((*n*))'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：*X* = ![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),
    … , ![](../../OEBPS/Images/AR_x.png)^((*n*))
- en: Initialize parameters *Θ* = {*π^k* , *μ[k]* , **Σ***[k]* *k* ∈ [1, *K*]} with
    random values
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机值初始化参数*Θ* = {*π^k* , *μ[k]* , **Σ***[k]* *k* ∈ [1, *K*]}
- en: ⊳ repeat E-step and M-step until likelihood stops increasing
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: ⊳ 重复E步和M步直到似然停止增加
- en: '**while** (likelihood is increasing ) **do**'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '**while** (似然增加) **do**'
- en: ⊳E-step
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: ⊳E步
- en: '![](../../OEBPS/Images/eq_06-38-a.png)'
  id: totrans-614
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-38-a.png)'
- en: ⊳M-step
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: ⊳M步
- en: '![](../../OEBPS/Images/eq_06-38-b.png)'
  id: totrans-616
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_06-38-b.png)'
- en: '**end** **while**'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '**end** **while**'
- en: return {*x*[1] , *μ*[1], **Σ[1]**, *x*[2], *μ*[2], **Σ[2]** , … , *x[K]*, *μ[K]*,
    **Σ*[K]***}
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 返回{*x*[1] , *μ*[1], **Σ[1]**, *x*[2], *μ*[2], **Σ[2]** , … , *x[K]*, *μ[K]*,
    **Σ*[K]***}
- en: NOTE Fully functional code for Gaussian mixture modeling, executable via Jupyter
    Notebook, can be found at [http://mng.bz/j4er](http://mng.bz/j4er).
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：完全功能的高斯混合模型代码，可通过Jupyter Notebook执行，可在[http://mng.bz/j4er](http://mng.bz/j4er)找到。
- en: Listing 6.9 GMM fit
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.9 GMM拟合
- en: '[PRE8]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Repeats until the likelihood increase is negligible
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: ① 重复直到似然增加可以忽略不计
- en: ② Computes the posterior probabilities *γ[i,k]* = *p*(*Z* = *k*|*X* = *x[i]*)
    using current *![](../../OEBPS/Images/AR_micro.png)[k]*s and Σ*[k]*s, equation
    [6.33](../Text/06.xhtml#eq-gmmfit-gamma)
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用当前的*![](../../OEBPS/Images/AR_micro.png)[k]*s和Σ*[k]*s计算后验概率*γ[i,k]* = *p*(*Z*
    = *k*|*X* = *x[i]*)，方程[6.33](../Text/06.xhtml#eq-gmmfit-gamma)
- en: ③ Tensor of shape [K] holding *π[k]*s for all [k]
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 维度为[K]的张量，包含所有[k]的*π[k]*值
- en: ④ Gaussian objects 𝒩(![](../../OEBPS/Images/AR_x.png), *![](../../OEBPS/Images/AR_micro.png)[k]*,
    Σ*[k]*) for all *k*
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '④ 对于所有*k*，高斯对象𝒩(![](../../OEBPS/Images/AR_x.png), *![](../../OEBPS/Images/AR_micro.png)[k]*,
    Σ*[k]*) '
- en: ⑤ Vector computation of log of *γ[i, k]* numerators for all i, k, equation [6.33](../Text/06.xhtml#eq-gmmfit-gamma)
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 对于所有i, k，计算*γ[i, k]*分子的对数，方程[6.33](../Text/06.xhtml#eq-gmmfit-gamma)
- en: ⑥ In practice, the probability involving an exponential goes to 0\. So we use
    the log probability.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 在实践中，涉及指数的概率趋近于0。因此，我们使用对数概率。
- en: ⑦ Vector computation of the log of *γ*[*i,* *k*] denominators for all i, k,
    equation [6.33](../Text/06.xhtml#eq-gmmfit-gamma)
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 对于所有i, k，计算*γ*[*i,* *k*]分母的对数，方程[6.33](../Text/06.xhtml#eq-gmmfit-gamma)
- en: ⑧ Vector computation of the [*n* *×* *K*] tensor *γ*[*i,* *k*], equation [6.33](../Text/06.xhtml#eq-gmmfit-gamma)
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 向量计算[*n* *×* *K*]张量*γ*[*i,* *k*]，方程[6.33](../Text/06.xhtml#eq-gmmfit-gamma)
- en: ⑨ Updates *![](../../OEBPS/Images/AR_micro.png)[k]* and Σ[k] for all *k* using
    *γ*[*i,* *k*] = *p*(*Z* = *k*|*X*) from the E-step via equations [6.36](../Text/06.xhtml#eq-gmmfit-pi),
    [6.37](../Text/06.xhtml#eq-gmmfit-mu), and [6.38](../Text/06.xhtml#eq-gmmfit-sigma)
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 使用E步得到的*γ*[*i,* *k*] = *p*(*Z* = *k*|*X*)更新所有*k*的*![](../../OEBPS/Images/AR_micro.png)[k]*和Σ[k]，通过方程[6.36](../Text/06.xhtml#eq-gmmfit-pi)，[6.37](../Text/06.xhtml#eq-gmmfit-mu)，和[6.38](../Text/06.xhtml#eq-gmmfit-sigma)
- en: ⑩ Number of data points
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 数据点的数量
- en: ⑪ Vector update of *π[k]* for all k, equation [6.36](../Text/06.xhtml#eq-gmmfit-pi)
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 对于所有k，更新*π[k]*的向量，方程[6.36](../Text/06.xhtml#eq-gmmfit-pi)
- en: ⑫ Vector update of [*K* × *d*] tensor, *![](../../OEBPS/Images/AR_micro.png)[k]*
    for all k, equation [6.37](../Text/06.xhtml#eq-gmmfit-mu)
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ 对于所有k，[*K* × *d*]张量*![](../../OEBPS/Images/AR_micro.png)[k]*的向量更新，方程[6.37](../Text/06.xhtml#eq-gmmfit-mu)
- en: ⑬ Vector computation of (*![](../../OEBPS/Images/AR_x.png)[i]* – *![](../../OEBPS/Images/AR_micro.png)[k]*)
    (*![](../../OEBPS/Images/AR_x.png)[i]* – *![](../../OEBPS/Images/AR_micro.png)[k]*)^T
    for all *l, k*
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ 对于所有*l, k*，向量计算(*![](../../OEBPS/Images/AR_x.png)[i]* – *![](../../OEBPS/Images/AR_micro.png)[k]*)
    (*![](../../OEBPS/Images/AR_x.png)[i]* – *![](../../OEBPS/Images/AR_micro.png)[k]*)^T
- en: ⑭ Vector update of *K* × *d* × *d*] tensor Σ[k] for all k, equation [6.38](../Text/06.xhtml#eq-gmmfit-sigma)
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: ⑭ 对于所有k，[*K* × *d* × *d*]张量Σ[k]的向量更新，方程[6.38](../Text/06.xhtml#eq-gmmfit-sigma)
- en: ⑮ log likelihood, equation [6.31](../Text/06.xhtml#eq-gmm-loglikelihood)
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: ⑮ 对数似然，方程[6.31](../Text/06.xhtml#eq-gmm-loglikelihood)
- en: Summary
  id: totrans-637
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at the Bayesian tools for decision-making in uncertain
    systems. We discussed conditional probability and Bayes’ theorem, which connects
    conditional probabilities to joint and marginal probabilities.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了在不确定系统中进行决策的贝叶斯工具。我们讨论了条件概率和贝叶斯定理，它们将条件概率与联合概率和边缘概率联系起来。
- en: Conditional probability is the probability of an event occurring subject to
    the condition that another event has already occurred. In machine learning, we
    are often interested in the conditional probability *p*(![](../../OEBPS/Images/AR_x.png)|*θ*)
    of an input ![](../../OEBPS/Images/AR_x.png) given that the parameters of the
    model predicting the input are *θ*. This conditional probability is known as the
    likelihood of the input. We are also interested in the conditional probability
    *p*(*θ*|![](../../OEBPS/Images/AR_x.png)), known as the posterior probability.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件概率是在另一个事件已经发生的情况下，一个事件发生的概率。在机器学习中，我们通常对给定模型预测输入的参数为*θ*时，输入![](../../OEBPS/Images/AR_x.png)的条件概率*p*(![](../../OEBPS/Images/AR_x.png)|*θ*)感兴趣。这个条件概率被称为输入的似然。我们还对条件概率*p*(*θ*|![](../../OEBPS/Images/AR_x.png))感兴趣，这被称为后验概率。
- en: 'Joint probability is the probability of a set of events occurring together.
    If the events are independent, the joint probability is the product of their individual
    probabilities. Whether events are independent or not, Bayes’ theorem connects
    joint and conditional probabilities. Of particular interest in machine learning
    is the Bayes’ theorem expression connecting the likelihood and joint and posterior
    probabilities of inputs and parameters: *p*(![](../../OEBPS/Images/AR_x.png),
    *θ*) = *p*(![](../../OEBPS/Images/AR_x.png)|*θ*)*p*(*θ*) and *p*(![](../../OEBPS/Images/AR_x.png)|*θ*)*p*(*θ*)
    / *p*(![](../../OEBPS/Images/AR_x.png)). *p*(![](../../OEBPS/Images/AR_x.png)|*θ*)
    is the probability distribution function of the chosen distribution family. *p*(*θ*)
    is the prior probability that codifies our belief, sans data, about the system.
    A popular choice is *p*(*θ*) ∝ *e*^(−||*θ*||²), implying smaller probabilities
    for higher-magnitude parameters and vice versa.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联合概率是一组事件同时发生的概率。如果事件是独立的，联合概率是它们各自概率的乘积。无论事件是否独立，贝叶斯定理都将联合概率和条件概率联系起来。在机器学习中，特别感兴趣的是贝叶斯定理表达式，它将输入和参数的似然、联合和后验概率联系起来：*p*(![](../../OEBPS/Images/AR_x.png),
    *θ*) = *p*(![](../../OEBPS/Images/AR_x.png)|*θ*)*p*(*θ*) 和 *p*(![](../../OEBPS/Images/AR_x.png)|*θ*)*p*(*θ*)
    / *p*(![](../../OEBPS/Images/AR_x.png))。*p*(![](../../OEBPS/Images/AR_x.png)|*θ*)是所选分布族的概率分布函数。*p*(*θ*)是我们对系统信念的先验概率，没有数据。一个流行的选择是*p*(*θ*)
    ∝ *e*^(−||*θ*||²)，这意味着参数的绝对值越大，概率越小，反之亦然。
- en: Entropy models the uncertainty in a system. Systems where all events have more
    or less similar probabilities tend to be high-entropy. Systems where a particular
    subset of possible events have significantly high probabilities and others have
    significantly low probabilities tend to be low-entropy. Equivalently, the probability
    density functions of low-entropy systems tend to have tall peaks, and their sample
    point clouds have a high concentration of points in some regions. High-entropy
    systems tend to have flat probability density functions and diffused sample point
    clouds.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵模拟了一个系统中的不确定性。所有事件具有或多或少相似概率的系统往往具有高熵。具有特定可能事件子集概率显著高而其他事件概率显著低的系统往往具有低熵。等价地，低熵系统的概率密度函数往往具有高尖峰，它们的样本点云在某些区域有高浓度的点。高熵系统往往具有平坦的概率密度函数和扩散的样本点云。
- en: Cross-entropy allows us to quantify how good our modeling is against a known
    ground truth.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉熵使我们能够量化我们的模型与已知真实值之间的好坏。
- en: Kullback–Leibler divergence gives us a measure of the dissimilarity between
    two probability distributions.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库尔巴克-莱布勒散度为我们提供了两个概率分布之间差异的度量。
- en: Maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estimation
    are two paradigms for estimating model parameters. MLE maximizes *p*(*X*|*θ*),
    and MAP maximizes *p*(*X*|*θ*)*p*(*θ*). MLE essentially tries to estimate probability
    distribution parameters that maximize the overlap between the sample point cloud
    of the probability distribution and the training data point cloud. MAP is MLE
    with a regularization condition. The regularization condition is injected via
    the prior probability term *p*(*θ*), which favors solutions with a certain property
    (such as small parameter magnitudes) that we believe to be true from empirical
    knowledge without data. MLE for Gaussian distributions has a closed-form solution.
    The mean and variance (covariance in the multidimensional case) of the optimal
    probability distribution that best fits the training data are the sample mean
    and sample variance or covariance on the training dataset.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大似然估计（MLE）和最大后验估计（MAP）是估计模型参数的两种范式。MLE最大化*p*(*X*|*θ*)，而MAP最大化*p*(*X*|*θ*)*p*(*θ*)。MLE本质上试图估计概率分布参数，这些参数最大化概率分布样本点云与训练数据点云之间的重叠。MAP是带有正则化条件的MLE。正则化条件通过先验概率项*p*(*θ*)注入，它倾向于具有某些属性（如小的参数幅度）的解，这些属性是我们根据经验知识而没有数据相信是真实的。高斯分布的MLE有一个封闭形式的解。最佳拟合训练数据的概率分布的均值和方差（在多维情况下为协方差）是训练数据集上的样本均值和样本方差或协方差。
- en: Latent variables in a machine learning system are auxiliary variables that are
    not directly observed but can be derived from the input. They facilitate the expression
    of the goal of optimization or the loss to be minimized.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习系统中，潜在变量是辅助变量，它们不能直接观察到，但可以从输入中推导出来。它们有助于表达优化目标或要最小化的损失。
- en: Gaussian mixture models (GMM) are unsupervised probability models that fit multiclass
    data distributions having multiple clusters in the training dataset, each corresponding
    to a different class. Here, MLE does not yield a closed-form solution but instead
    yields an iterative solution to estimate the mixture weights, means, and variances
    of the individual Gaussians in the mixture.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMM）是一种无监督概率模型，它适合于训练数据集中具有多个簇的多类数据分布，每个簇对应一个不同的类别。在这里，最大似然估计（MLE）不产生封闭形式的解，而是产生一个迭代解来估计混合权重、均值和方差。
