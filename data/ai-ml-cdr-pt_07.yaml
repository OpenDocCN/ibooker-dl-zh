- en: Chapter 6\. Making Sentiment Programmable by Using Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章\. 通过使用嵌入使情感可编程
- en: In [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759),
    you saw how to take words and encode them into tokens. Then, you saw how to encode
    sentences full of words into sequences full of tokens, padding or truncating them
    as appropriate to end up with a well-shaped set of data that you can use to train
    a neural network. However, in none of that was there any type of modeling of the
    *meaning* of a word. And while it’s true that there’s no absolute numeric encoding
    that could encapsulate meaning, there are relative ones.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)中，你看到了如何将单词编码成标记。然后，你看到了如何将充满单词的句子编码成充满标记的序列，根据需要填充或截断它们，以得到一个形状良好的数据集，你可以用它来训练神经网络。然而，在那之中没有任何对单词*意义*的建模。虽然确实没有能够封装意义的绝对数值编码，但存在相对的编码。
- en: In this chapter, you’ll learn about techniques to encapsulate meaning, and in
    particular, the concept of *embeddings*, in which vectors in high-dimensional
    space are created to represent words. The directions of these vectors can be learned
    over time, based on the use of the words in the corpus. Then, when you’re given
    a sentence, you can investigate the directions of the word vectors, sum them up,
    and from the overall direction of the summation, establish the sentiment of the
    sentence as a product of its words. Also, related to this, as the model scans
    the sentences, the positioning of the words in the sentence can also help train
    an appropriate embedding.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解封装意义的技术，特别是*嵌入*的概念，其中在多维空间中创建向量来表示单词。这些向量的方向可以根据语料库中单词的使用情况随时间学习。然后，当你得到一个句子时，你可以调查单词向量的方向，将它们相加，并从总和的整体方向中，确定句子的情感作为其单词的产物。此外，与此相关的是，当模型扫描句子时，句子中单词的位置也可以帮助训练适当的嵌入。
- en: In this chapter, we’ll also explore how that works. Using the News Headlines
    Dataset for Sarcasm Detection dataset from [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759),
    you’ll build embeddings to help a model detect sarcasm in a sentence. You’ll also
    work with some cool visualization tools that help you understand how words in
    a corpus get mapped to vectors so you can see which words determine the overall
    classification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们还将探讨它是如何工作的。使用来自[第5章](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)的“用于讽刺检测的新闻标题数据集”，你将构建嵌入来帮助模型检测句子中的讽刺。你还将使用一些酷炫的可视化工具，这些工具可以帮助你理解语料库中的单词是如何映射到向量上的，这样你就可以看到哪些单词决定了整体的分类。
- en: Establishing Meaning from Words
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从单词中建立意义
- en: 'Before we get into the higher-dimensional vectors for embeddings, let’s use
    some simple examples to try to visualize how meaning can be derived from numerics.
    Consider this: using the sarcasm dataset from [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759),
    what would happen if you encoded all of the words that make up sarcastic headlines
    with positive numbers and those that make up realistic headlines with negative
    numbers?'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入到用于嵌入的高维向量之前，让我们用一些简单的例子来尝试可视化从数值中推导出意义。考虑这个：使用来自[第5章](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)的讽刺数据集，如果你用正数编码构成讽刺标题的所有单词，而用负数编码构成现实标题的所有单词，会发生什么？
- en: 'A Simple Example: Positives and Negatives'
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单示例：正面和负面
- en: 'Take, for example, this sarcastic headline from the dataset:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以数据集中这个讽刺性的标题为例：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Assuming that all words in our vocabulary start with a value of 0, we could
    add 1 to the value for each of the words in this sentence, and we would end up
    with this:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们词汇表中的所有单词的初始值都是0，我们可以给这个句子中的每个单词的值加1，最终我们会得到这个：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This isn’t the same as the *tokenization* of words that you did in the last
    chapter. You could consider replacing each word (e.g., *christian*) with the token
    representing it that is encoded from the corpus, but I’ll leave the words in for
    now to make the code easier to read.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这与你在上一章中进行的*分词*不同。你可以考虑将每个单词（例如，*christian*）替换为其由语料库编码的标记表示，但为了使代码更容易阅读，我现在将保留这些单词。
- en: 'Then, in the next step, consider an ordinary headline (not a sarcastic one),
    like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在下一步中，考虑一个普通的标题（不是一个讽刺性的标题），例如这个：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Because this is a different sentiment, we could instead subtract 1 from the
    current value of each word, so our value set would look like this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个不同的情感，我们可以从每个词的当前值中减去1，所以我们的值集看起来会是这样：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the sarcastic `bale` (from `christian bale`) has been offset by the
    nonsarcastic `bale` (from `gareth bale`), so its score ends up as 0\. Repeat this
    process thousands of times and you’ll end up with a huge list of words from your
    corpus that are scored based on their usage.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，讽刺的“bale”（来自“christian bale”）被非讽刺的“bale”（来自“gareth bale”）所抵消，所以它的分数最终为0。重复这个过程数千次，你将得到一个巨大的单词列表，这些单词根据它们在语料库中的使用情况进行了评分。
- en: 'Now, imagine we want to establish the sentiment of this sentence:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象我们想要确定这个句子的情感：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using our existing value set, we could look at the scores of each word and add
    them up. We would get a score of 2, indicating (because it’s a positive number)
    that this is a sarcastic sentence.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们现有的值集，我们可以查看每个词的分数并将它们加起来。我们会得到2分的分数，这表明（因为它是一个正数）这是一个讽刺的句子。
- en: Note
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For what it’s worth, the word *bale* is used five times in the Sarcasm dataset,
    twice in a normal headline and three times in a sarcastic one. So, in a model
    like this, the word *bale* would be scored –1 across the whole dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在讽刺数据集中，“bale”这个词被使用了五次，在正常标题中出现了两次，在讽刺标题中出现了三次。所以，在这种模型中，“bale”这个词在整个数据集中会被评分为一1。
- en: 'Going a Little Deeper: Vectors'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入一点：向量
- en: Hopefully, the previous example has helped you understand the mental model of
    establishing some form of *relative* meaning for a word, through its association
    with other words in the same “direction.” In our case, while the computer doesn’t
    understand the meanings of individual words, it can move labeled words from a
    known sarcastic headline in one direction (by adding 1) and move labeled words
    from a known normal headline in another direction (by subtracting 1). This gives
    us a basic understanding of the meaning of the words, but it does lose some nuance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 希望前面的例子已经帮助你理解了通过与其他相同“方向”中的单词关联来为单词建立某种形式*相对*意义的心理模型。在我们的情况下，虽然计算机不理解单个单词的含义，但它可以将标记的单词从已知的讽刺标题的一个方向（通过加1）移动，并将标记的单词从已知的正常标题的另一个方向（通过减1）移动。这让我们对单词的含义有了基本的理解，但它确实失去了一些细微差别。
- en: But what if we increased the dimensionality of the direction to try to capture
    some more information? For example, suppose we were to look at characters from
    the Jane Austen novel *Pride and Prejudice*, considering the dimensions of gender
    and nobility. We could plot the former on the *x*-axis and the latter on the *y*-axis,
    with the length of the vector denoting each character’s wealth (see [Figure 6-1](#ch06_clean_figure_1_1748752380714921)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们增加方向的维度以尝试捕捉更多信息呢？例如，假设我们要查看简·奥斯汀的小说《傲慢与偏见》中的字符，考虑性别和贵族的维度。我们可以在*x*轴上绘制前者，在*y*轴上绘制后者，向量的长度表示每个角色的财富（见[图6-1](#ch06_clean_figure_1_1748752380714921)）。
- en: '![](assets/aiml_0601.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0601.png)'
- en: Figure 6-1\. Characters in Pride and Prejudice as vectors
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 《傲慢与偏见》中的角色作为向量
- en: From an inspection of the graph, you can derive a fair amount of information
    about each character. Three of them are male. Mr. Darcy is extremely wealthy,
    but his nobility isn’t clear (he’s called “Mister,” unlike the less wealthy but
    apparently more noble Sir William Lucas). The other “Mister,” Mr. Bennet, is clearly
    not nobility and is struggling financially. Elizabeth Bennet, his daughter, is
    similar to him but female. Lady Catherine, the other female character in our example,
    is noble and incredibly wealthy. The romance between Mr. Darcy and Elizabeth causes
    tension—with *prejudice* coming from the noble side of the vectors toward the
    less-noble.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表的检查中，你可以得出关于每个角色的相当多的信息。其中三个是男性。达西先生非常富有，但他的贵族身份并不明确（他被称为“Mister”，与不那么富有但显然更贵族的威廉·卢卡斯爵士不同）。另一位“Mister”，本内特先生，显然不是贵族，并且经济上很困难。伊丽莎白·本内特，他的女儿，与他相似，但她是女性。凯瑟琳夫人，我们例子中的另一位女性角色，是贵族并且极其富有。达西先生和伊丽莎白之间的浪漫关系引起了紧张——*偏见*来自向量中贵族一方对不那么贵族一方的偏见。
- en: As this example shows, by considering multiple dimensions, we can begin to see
    real meaning in the words (which are character names here). Again, we’re not talking
    about concrete definitions but more about a *relative* meaning based on the axes
    and the relationship between the vector for one word and the other vectors.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如此示例所示，通过考虑多个维度，我们可以开始看到词语（在这里是角色名称）的真实含义。再次强调，我们这里讨论的不是具体的定义，而是基于轴和向量之间关系的一种*相对*含义。
- en: This leads us to the concept of an *embedding*, which is simply a vector representation
    of a word that is learned while training a neural network. We’ll explore that
    next.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了“嵌入”的概念，它简单地说是在训练神经网络时学习到的词语的向量表示。我们将在下一节中探讨这一点。
- en: Embeddings in PyTorch
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch中的嵌入
- en: Much like you’ve seen with `Linear` and `Conv2D`, PyTorch implements embeddings
    by using a layer. This creates a lookup table that maps from an integer to an
    embedding table, the contents of which are the coefficients of the vector representing
    the word identified by that integer. So, in the *Pride and Prejudice* example
    from the previous section, the *x* and *y* coordinates would give us the embeddings
    for a particular character from the book. Of course, in a real NLP problem, we’ll
    use far more than two dimensions. Thus, the direction of a vector in the vector
    space could be seen as encoding the “meaning” of a word, and words with similar
    vectors (i.e., pointing in roughly the same direction) could be considered related
    to that word.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与您所看到的`Linear`和`Conv2D`类似，PyTorch通过使用一个层来实现嵌入。这创建了一个查找表，它将整数映射到嵌入表中，该表的内容是代表该整数的词语的向量系数。因此，在上一节中提到的《傲慢与偏见》示例中，*x*和*y*坐标将给我们提供书中特定角色的嵌入。当然，在真实的NLP问题中，我们将使用远超过两个维度的数据。因此，向量空间中向量的方向可以看作是编码词语的“含义”，具有相似向量的词语（即大致指向同一方向）可以被认为是与该词语相关的。
- en: The embedding layer will be initialized randomly—that is, the coordinates of
    the vectors will be completely random to start with and will be learned during
    training by using backpropagation. When training is complete, the embeddings will
    roughly encode similarities between words, allowing us to identify words that
    are somewhat similar based on the direction of the vectors for those words.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层将随机初始化——也就是说，向量的坐标最初是完全随机的，并在训练过程中通过反向传播学习。当训练完成后，嵌入将大致编码词语之间的相似性，使我们能够根据这些词语的向量方向识别出某些相似的词语。
- en: This is all quite abstract, so I think the best way to understand how to use
    embeddings is to roll up your sleeves and give them a try. Let’s start with a
    sarcasm detector using the Sarcasm dataset from [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都是相当抽象的，所以我认为理解如何使用嵌入的最佳方式是亲自动手尝试。让我们从一个使用第5章中提到的讽刺数据集的讽刺检测器开始。
- en: Building a Sarcasm Detector by Using Embeddings
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用嵌入构建讽刺检测器
- en: 'In [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759),
    you loaded and did some preprocessing on a JSON dataset called the News Headlines
    Dataset for Sarcasm Detection (the sarcasm dataset, for short). By the time you
    were done, you had lists of training and testing data and labels:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)中，您加载并处理了一个名为“新闻标题数据集用于讽刺检测”（简称讽刺数据集）的JSON数据集。完成这些操作后，您拥有了训练数据和测试数据的列表以及标签：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For the training data, you created a `build_vocab` helper function to create
    a dictionary of the frequency of each word, sorted in order of the most frequent.
    The size of this dictionary is the `vocab_size`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据，您创建了一个`build_vocab`辅助函数来创建一个字典，该字典按词语频率排序。这个字典的大小是`vocab_size`。
- en: 'To get an embedding layer in PyTorch, you can use the `nn.Embedding` layer
    type, like this, by specifying the desired vocab size and the number of embedding
    dimensions:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要在PyTorch中获得嵌入层，您可以使用`nn.Embedding`层类型，如下所示，通过指定所需的词汇大小和嵌入维度数：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will initialize a vector with `embedding_dim` axes for each word. So, for
    example, if `embedding_dim` is `16`, then every word in the vocabulary will be
    assigned a 16-dimensional vector.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为每个词语初始化一个具有`embedding_dim`轴的向量。例如，如果`embedding_dim`是`16`，那么词汇表中的每个词语都将被分配一个16维的向量。
- en: Over time, the attributes for each token (encoded as values for the vector in
    each of its dimensions) will be learned through backpropagation as the network
    learns by matching the training data to its labels.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，每个标记的属性（编码为每个维度的向量值）将通过反向传播作为网络通过将训练数据与其标签匹配来学习而学习。
- en: An important next step is feeding the output of the embedding layer into a dense
    layer. The easiest way to do this, similar to how you would when using a convolutional
    neural network, is to use pooling. In this instance, the dimensions of the embeddings
    are averaged out to produce a fixed-length output vector, and `Adaptive​A⁠ve​Pool1d(1)`
    reduces the input along the length of the sequence to a fixed vector size of 1.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个重要步骤是将嵌入层的输出输入到一个密集层。最简单的方法，类似于使用卷积神经网络时，是使用池化。在这种情况下，嵌入的维度被平均化，以产生一个固定长度的输出向量，`AdaptiveAvePool1d(1)`将输入沿序列长度减少到固定向量大小1。
- en: 'As an example, consider this model architecture:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下模型架构：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here, an embedding layer is defined, and it’s given the vocab size and an embedding
    dimension. Let’s take a look at the number of trainable parameters in the network,
    using `torchinfo.summary`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，定义了一个嵌入层，并给出了词汇大小和嵌入维度。让我们看看使用`torchinfo.summary`的网络中可训练参数的数量：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The vocabulary size is 24,292 words, and as the embedding has 100 dimensions,
    the total number of trainable parameters in the embedding layer will be 2,429,200\.
    The first linear layer has 100 values in with 24 values out, so that’s a total
    of 2,400 weights, but each of the neurons also has a bias, so add 24 to get to
    24, 24\.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇量大小为24,292个单词，由于嵌入有100个维度，嵌入层中的可训练参数总数将是2,429,200。第一层线性有100个输入值和24个输出值，所以总共有2,400个权重，但每个神经元也有一个偏差，所以加上24得到24,
    24\。
- en: Similarly, the last linear has 24 values in, with just a single neuron out.
    For a total of 24 parameters, plus one for the bias, this equals 25\. The entire
    network has 2,431,649 parameters to learn. Note that the average pooling layer
    has 0 trainable parameters, as it’s just averaging the parameters in the embedding
    layer before it to get a single 16-value vector.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，最后一层线性有24个输入值，只有一个输出神经元。总共有24个参数，加上一个偏差，这等于25。整个网络有2,431,649个参数需要学习。请注意，平均池化层没有可训练参数，因为它只是对其前面的嵌入层中的参数进行平均，以得到一个单一的16值向量。
- en: If we train this model, we’ll get a pretty decent training accuracy of 99%+
    after 30 epochs—but our validation accuracy will be below 80% (see [Figure 6-2](#ch06_clean_figure_2_1748752380714952)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练这个模型，经过30个epoch后，我们将获得一个相当不错的训练准确率99%+，但我们的验证准确率将低于80%（见图[图6-2](#ch06_clean_figure_2_1748752380714952)）。
- en: '![](assets/aiml_0602.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0602.png)'
- en: Figure 6-2\. Training accuracy versus validation accuracy
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 训练准确率与验证准确率
- en: That might seem to be a reasonable curve, given that the validation data likely
    contains many words that aren’t present in the training data. However, if you
    examine the loss curves for training versus validation over one hundred epochs,
    you’ll see a problem. Although you would expect to see that the training accuracy
    is higher than the validation accuracy, a clear indicator of overfitting is that
    while the validation accuracy is dropping a little over time (as shown in [Figure 6-2](#ch06_clean_figure_2_1748752380714952)),
    its loss is increasing sharply, as shown in [Figure 6-3](#ch06_clean_figure_3_1748752380714977).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到验证数据可能包含许多训练数据中不存在的单词，这可能看起来是一条合理的曲线。然而，如果你检查训练与验证在一百个epoch中的损失曲线，你会看到一个问题。虽然你可能会期望看到训练准确率高于验证准确率，但过度拟合的一个明显迹象是，尽管验证准确率随着时间的推移略有下降（如图[图6-2](#ch06_clean_figure_2_1748752380714952)所示），但其损失却在急剧增加，如图[图6-3](#ch06_clean_figure_3_1748752380714977)所示。
- en: '![](assets/aiml_0603.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0603.png)'
- en: Figure 6-3\. Training loss versus validation loss
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 训练损失与验证损失
- en: Overfitting like this is common with NLP models, due to the somewhat unpredictable
    nature of language. In the next sections, we’ll look at how to reduce this effect
    by using a number of techniques.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语言的某种不可预测性，这种过度拟合在NLP模型中很常见。在接下来的几节中，我们将探讨如何通过使用多种技术来减少这种影响。
- en: Reducing Overfitting in Language Models
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在语言模型中减少过度拟合
- en: Overfitting happens when the network becomes overspecialized to the training
    data, and one part of this involves the network becoming very good at matching
    patterns in “noisy” data in the training set that doesn’t exist anywhere else.
    Because this particular noise isn’t present in the validation set, the better
    the network gets at matching it, the worse the loss of the validation set will
    be. This can result in the escalating loss that you saw in [Figure 6-3](#ch06_clean_figure_3_1748752380714977).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当网络对训练数据过度专业化时，会发生过拟合，这其中包括网络在训练集中匹配“噪声”数据中的模式变得非常出色，而这些模式在其他任何地方都不存在。由于这种特定的噪声在验证集中不存在，网络在匹配它方面变得越好，验证集的损失就会越差。这可能导致你在[图6-3](#ch06_clean_figure_3_1748752380714977)中看到的损失不断上升。
- en: In this section, we’ll explore several ways to generalize the model and reduce
    overfitting.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨几种泛化模型和减少过拟合的方法。
- en: Adjusting the learning rate
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整学习率
- en: A hyperparameter of the optimizer is the learning rate (LR). The details of
    this parameter are beyond the scope of this chapter, but consider it to be a value
    that if too high will cause the network to potentially learn too quickly and miss
    nuance. The flipside is also true—if you set it too low, your network may not
    learn effectively.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器的一个超参数是学习率（LR）。这个参数的细节超出了本章的范围，但可以将其视为一个值，如果太高，可能会导致网络学习得太快而错过细微之处。另一方面也是真的——如果你设置得太低，你的网络可能无法有效学习。
- en: 'Perhaps the biggest factor that can lead to overfitting is whether the LR of
    your optimizer is too high. If it is, then your network learns *too quickly*.
    For this example, the code to define the optimizer was as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可能导致过拟合的最大因素是优化器的LR是否过高。如果是这样，那么你的网络学习得太快了。对于这个例子，定义优化器的代码如下：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'These are the defaults for the `Adam` optimizer. One thing to experiment with
    is the `learning rate` parameter (`lr`), and in the following code, you’ll see
    the results of an instance when I reduced by an order of 10 to 0.0001, like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是`Adam`优化器的默认值。你可以尝试实验的一个参数是`学习率`（`lr`），在下面的代码中，你会看到我将学习率降低了一个数量级到0.0001的结果，如下所示：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `betas` values stay at their defaults, as does `amsgrad`. Also note that
    both `beta` values must be between 0 and 1, and typically, both are close to 1\.
    Amsgrad is an alternative implementation of the Adam optimizer that was introduced
    in the paper [“On the Convergence of Adam and Beyond” by Sashank Reddi, Satyen
    Kale, and Sanjiv Kumar](https://oreil.ly/FhTDi).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`betas`值保持默认，`amsgrad`也是如此。请注意，两个`beta`值必须在0到1之间，通常两者都接近1。Amsgrad是Adam优化器的一种替代实现，该实现由Sashank
    Reddi、Satyen Kale和Sanjiv Kumar在论文“On the Convergence of Adam and Beyond”中提出（[链接](https://oreil.ly/FhTDi)）。'
- en: This much lower LR has a profound impact on the network. [Figure 6-4](#ch06_clean_figure_4_1748752380714997)
    shows the accuracy of the network over one hundred epochs. The lower LR can be
    seen in the first 10 epochs or so, where it appears that the network isn’t learning,
    before it “breaks out” and starts to learn quickly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更低的学习率对网络有深远的影响。[图6-4](#ch06_clean_figure_4_1748752380714997)显示了网络在一百个epoch中的准确率。在最初的10个epoch左右，可以看到网络似乎没有在学习，然后“突破”并开始快速学习。
- en: Exploring the loss (as illustrated in [Figure 6-5](#ch06_clean_figure_5_1748752380715014)),
    we can see that even while the accuracy wasn’t going up for the first few epochs,
    the loss was going down. You could therefore be confident that the network would
    eventually start to learn, if you were watching it epoch by epoch.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 探索损失（如图6-5所示），我们可以看到，尽管在前几个epoch中准确率没有上升，但损失却在下降。因此，如果你逐个epoch地观察，你可以有信心网络最终会开始学习。
- en: '![](assets/aiml_0604.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0604.png)'
- en: Figure 6-4\. Accuracy with a lower LR
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4\. 使用较低学习率（LR）的准确率
- en: '![](assets/aiml_0605.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0605.png)'
- en: Figure 6-5\. Loss with a lower LR
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5\. 使用较低学习率（LR）的损失
- en: And while the loss does start to show the same curve of overfitting that you
    saw in [Figure 6-3](#ch06_clean_figure_3_1748752380714977), note that it happens
    much later and at a much lower rate. By epoch 30, the loss is at about 0.49, whereas
    with the higher LR in [Figure 6-3](#ch06_clean_figure_3_1748752380714977), it
    was more than double that amount. And while it takes the network longer to get
    to a good accuracy rate, it does so with less loss, so you can be more confident
    in the results. With these hyperparameters, the loss on the validation set started
    to increase at about epoch 60, at which point, the training set had 90% accuracy
    and the validation set had about 81% accuracy, showing that we have quite an effective
    network.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 并且虽然损失确实开始显示出与您在 [图 6-3](#ch06_clean_figure_3_1748752380714977) 中看到的相同的过拟合曲线，但请注意，它发生得晚得多，并且速率低得多。到第
    30 个周期时，损失约为 0.49，而 [图 6-3](#ch06_clean_figure_3_1748752380714977) 中的更高学习率（LR）时，损失量超过两倍。虽然网络需要更长的时间才能达到良好的准确率，但它以更低的损失率做到这一点，因此您可以更有信心地相信结果。使用这些超参数，验证集上的损失在大约第
    60 个周期时开始增加，此时训练集的准确率为 90%，验证集的准确率约为 81%，这表明我们有一个相当有效的网络。
- en: Of course, it’s easy to just tweak the optimizer and then declare victory, but
    there are a number of other methods you can use to improve your model. You’ll
    learn about those in the next few sections, and for them, I’ve reverted back to
    using the default Adam optimizer so the effects of tweaking the LR won’t hide
    the benefits offered by these other techniques.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，只是调整优化器然后宣布胜利很容易，但您还可以使用其他一些方法来改进您的模型。您将在接下来的几节中了解到这些方法，并且为了这些方法，我已重新使用默认的
    Adam 优化器，这样调整学习率（LR）的效果就不会掩盖这些其他技术提供的优势。
- en: Exploring vocabulary size
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索词汇量大小
- en: The sarcasm dataset deals with words, so if you explore the words in the dataset
    and in particular their frequency, you might get a clue that helps fix the overfitting
    issue.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 意图讽刺数据集处理的是单词，因此如果您探索数据集中的单词及其频率，您可能会得到一些有助于解决过拟合问题的线索。
- en: 'I’ve provided a `word_frequency` helper function that lets you explore the
    frequency of words in the vocabulary. It looks like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我提供了一个 `word_frequency` 辅助函数，让您可以探索词汇表中的单词频率。它看起来是这样的：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can run it with code like this:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用如下代码运行它：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You’ll then see results like this: a dictionary containing the frequency of
    each word, starting with the most frequently used one, and moving on from there.
    Here are the first few words:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到如下结果：一个包含每个单词频率的字典，从最常用的单词开始，然后继续下去。以下是前几个单词：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you want to plot this, you can iterate through each item in the list and
    make the *x* value the ordinal of where you are (1 for the first item, 2 for the
    second item, etc.). The *y* value will then be a `newlist[item]`, which you can
    plot with `matplotlib`. Here’s the code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想绘制这个，您可以遍历列表中的每个项目，将 *x* 值设置为您的位置序号（第一个项目为 1，第二个项目为 2 等）。*y* 值将是 `newlist[item]`，您可以使用
    `matplotlib` 绘制它。以下是代码：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The result is shown in [Figure 6-6](#ch06_clean_figure_6_1748752380715030).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在 [图 6-6](#ch06_clean_figure_6_1748752380715030) 中。
- en: '![](assets/aiml_0606.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_0606.png](assets/aiml_0606.png)'
- en: Figure 6-6\. Exploring the frequency of words
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. 探索单词的频率
- en: This “hockey stick” curve shows us that very few words are used many times,
    whereas most words are used very few times. But every word is effectively weighted
    equally because every word has an “entry” in the embedding. Given that we have
    a relatively large training set in comparison with the validation set, we’re ending
    up in a situation where there are many words present in the training set that
    aren’t present in the validation set.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这条“曲棍球棒”曲线告诉我们，很少的单词被多次使用，而大多数单词被使用的次数很少。但每个单词都被有效地同等加权，因为每个单词在嵌入中都有一个“条目”。鉴于我们的训练集相对于验证集来说相对较大，我们最终处于一个训练集中存在许多在验证集中不存在的单词的情况。
- en: 'You can zoom in on the data by changing the axis of the plot just before calling
    `plt.show`. For example, to look at the volume of words from 300 to 10,000 on
    the *x*-axis with the scale from 0 to 100 on the *y*-axis, you can use this code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在调用 `plt.show` 之前更改图表的轴来放大数据。例如，为了在 *x* 轴上查看从 300 到 10,000 的单词量，在 *y* 轴上从
    0 到 100 的刻度，您可以使用以下代码：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The result is in [Figure 6-7](#ch06_clean_figure_7_1748752380715045).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在 [图 6-7](#ch06_clean_figure_7_1748752380715045) 中。
- en: '![Frequency of words 300–10,000](assets/aiml_0607.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![单词频率 300–10,000](assets/aiml_0607.png)'
- en: Figure 6-7\. Frequency of words from 300 to 10,000
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. 300 到 10,000 个单词的频率
- en: There are almost 25,000 words in the corpus, and the code is set up to only
    train for all of them! But if we look at the words in positions 2,000 onward,
    which is over 90% of our vocabulary, we’ll see that they’re each used fewer than
    20 times in the entire corpus!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库中几乎有25,000个单词，代码被设置为只训练所有这些单词！但如果我们查看位置2,000及以后的单词，这超过了我们词汇量的90%，我们会发现它们在整个语料库中每个单词的使用次数都不到20次！
- en: 'This could explain the overfitting, so the logical next step is to see if we
    can reduce the vocabulary we are training for. Within the `build_vocab` helper
    function, we can add a parameter for the maximum vocab size we’re interested in,
    like this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能解释了过度拟合的原因，所以合理的下一步是看看我们是否可以减少我们正在训练的词汇量。在`build_vocab`辅助函数中，我们可以添加一个参数来指定我们感兴趣的最大词汇量，如下所示：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, when building our `word_index`, we can specify a maximum vocab size that
    we’re interested in exploring:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在构建我们的`word_index`时，我们可以指定我们感兴趣探索的最大词汇量：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The embedding layer was already initialized with the vocab size, so the model
    architecture doesn’t need to change. Indeed, with the reduced vocab size, the
    number of learned parameters drops sharply, giving us a simpler network that learns
    faster:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层已经根据词汇量初始化，所以模型架构不需要改变。实际上，随着词汇量的减少，学习到的参数数量急剧下降，给我们一个更简单的网络，它学习得更快：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The model has shrunk from 2.4 million parameters to only 202,549\.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 模型从240万个参数缩小到只有202,549个。
- en: After retraining and exploring the smaller model, we can see that the results
    have changed.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新训练和探索较小的模型后，我们可以看到结果已经改变。
- en: '[Figure 6-8](#ch06_clean_figure_8_1748752380715056) shows the accuracy metrics.
    Now, the training set accuracy is about 82% and the validation accuracy is about
    76%. They’re closer to each other and not diverging, which is a good sign that
    we’ve gotten rid of most of the overfitting.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-8](#ch06_clean_figure_8_1748752380715056)显示了准确度指标。现在，训练集准确度大约为82%，验证集准确度大约为76%。它们彼此更接近，没有发散，这是一个好迹象，表明我们已经消除了大部分过度拟合。'
- en: '![](assets/aiml_0608.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0608.png)'
- en: Figure 6-8\. Accuracy with a two thousand–word vocabulary
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8\. 具有两千个单词词汇量的准确度
- en: This is somewhat reinforced by the loss plot in [Figure 6-9](#ch06_clean_figure_9_1748752380715067).
    The loss on the validation set is rising but much slower than before, so reducing
    the size of the vocabulary to prevent the training set from overfitting on low-frequency
    words that were possibly only present in the training set appears to have worked.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这在一定程度上得到了[图6-9](#ch06_clean_figure_9_1748752380715067)中的损失图的加强。验证集上的损失在上升，但比之前慢得多，所以减少词汇量以防止训练集过度拟合可能只在训练集中出现过的低频单词似乎已经奏效。
- en: It’s worth experimenting with different vocab sizes, but remember that you can
    also have too small of a vocab size and overfit to that. You’ll need to find a
    balance. In this case, my choice of taking words that appear 20 times or more
    was purely arbitrary.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 值得尝试不同的词汇量大小，但请记住，词汇量也可能太小，导致过度拟合。你需要找到一个平衡点。在这种情况下，我选择出现20次或以上的单词纯粹是随机的。
- en: '![](assets/aiml_0609.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0609.png)'
- en: Figure 6-9\. Loss with a two thousand–word vocabulary
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-9\. 具有两千个单词词汇量的损失
- en: Exploring embedding dimensions
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索嵌入维度
- en: For this example, I arbitrarily chose an embedding dimension of 16\. In this
    instance, words are encoded as vectors in 16-dimensional space, with their directions
    indicating their overall meaning. But is 16 a good number? With only two thousand
    words in our vocabulary, it might be on the high side, leading to a high degree
    of sparseness of direction.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我随意选择了16维的嵌入维度。在这种情况下，单词被编码为16维空间中的向量，其方向表示它们的整体意义。但16是一个好数字吗？在我们的词汇量中只有两千个单词的情况下，它可能有点高，导致方向稀疏度较高。
- en: Note
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: I believe that the best way to think about sparseness is to project into three
    dimensions. Think of it like the earth, with one thousand vectors pointing from
    the core to a place on the surface. The vectors are in three dimensions, *x*,
    *y*, and *z*. There’s a lot of surface area for them to cover, but if many of
    them are missing *x* and *y*, meaning they’re just zero, a lot of them will be
    pointing to (0, 0, *z*) and a whole lot of the earth’s surface will be untouched!
    Thus, there will be a total lack of distinctiveness.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为思考稀疏性的最佳方式是将它们投影到三个维度。想象一下地球，有一千个向量从核心指向地表的一个地方。这些向量在三个维度上，*x*，*y*和*z*。它们有很多表面要覆盖，但如果其中许多缺少*x*和*y*，意味着它们只是零，那么很多向量将会指向(0,
    0, *z*)，而地球表面的大部分将保持未触及！因此，将缺乏独特性。
- en: Research has shown that a best practice for embedding size is to have it be
    the fourth root of the vocabulary size. The fourth root of 2,000 is 6.687, so
    let’s explore what happens if we round this up and change the embedding dimension
    to 7.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，对于嵌入大小的一个最佳实践是使其为词汇大小的四次方根。2000的四次方根是6.687，所以让我们看看如果我们将其四舍五入并将嵌入维度改为7会发生什么。
- en: You can see the result of training for one hundred epochs in [Figure 6-10](#ch06_clean_figure_10_1748752380715076).
    The training set’s accuracy stabilized at about 83%, and the validation set’s
    accuracy stabilized at about 77%. Despite some jitters, the lines are pretty flat,
    showing that the model has converged. This isn’t much different from the results
    in [Figure 6-8](#ch06_clean_figure_8_1748752380715056), but reducing the embedding
    dimensionality allows the model to train significantly faster.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图6-10](#ch06_clean_figure_10_1748752380715076)中看到训练100个epoch的结果。训练集的准确度稳定在大约83%，验证集的准确度稳定在大约77%。尽管有一些波动，但线条相当平稳，表明模型已经收敛。这与[图6-8](#ch06_clean_figure_8_1748752380715056)中的结果没有太大不同，但减少嵌入维度性允许模型训练得更快。
- en: '![](assets/aiml_0610.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0610.png)'
- en: Figure 6-10\. Training versus validation accuracy for seven dimensions
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-10\. 七维度的训练与验证准确度
- en: '[Figure 6-11](#ch06_clean_figure_11_1748752380715085) shows the loss in training
    and validation. While it initially appeared that the loss was climbing at about
    epoch 20, it soon flattened out. Again, a good sign!'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-11](#ch06_clean_figure_11_1748752380715085) 展示了训练和验证过程中的损失。虽然最初看起来损失在约20个epoch时上升，但它很快趋于平稳。再次，这是一个好兆头！'
- en: '![](assets/aiml_0611.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0611.png)'
- en: Figure 6-11\. Training versus validation loss for seven dimensions
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11\. 七维度的训练与验证损失
- en: Now that the dimensionality has been reduced, we can do a bit more tweaking
    of the model architecture.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在维度已经减少，我们可以对模型架构进行一些调整。
- en: Exploring the model architecture
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索模型架构
- en: 'After the optimizations in the previous sections, the model architecture looks
    like this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节的优化之后，模型架构看起来是这样的：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: One thing that comes to mind is the dimensionality—the `GlobalAveragePooling1D`
    layer now emits just 7 dimensions, but they’re being fed into a hidden layer of
    24 neurons, which is overkill. Let’s explore what happens when this is reduced
    to 8 neurons and trained for 100 epochs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 想到的一件事是维度——`GlobalAveragePooling1D`层现在只发出7个维度，但它们被输入到24个神经元的隐藏层中，这有点过度。让我们看看当这个数量减少到8个神经元并且训练100个epoch时会发生什么。
- en: You can see the training versus validation accuracy in [Figure 6-12](#ch06_clean_figure_12_1748752380715093).
    When compared to [Figure 6-7](#ch06_clean_figure_7_1748752380715045), where 24
    neurons were used, the overall result is quite similar, but the model was somewhat
    faster to train.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图6-12](#ch06_clean_figure_12_1748752380715093)中看到训练与验证的准确度。与使用24个神经元的[图6-7](#ch06_clean_figure_7_1748752380715045)相比，整体结果相当相似，但模型训练得更快。
- en: '![](assets/aiml_0612.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0612.png)'
- en: Figure 6-12\. Reduced dense-architecture accuracy results
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12\. 减少密集架构准确度结果
- en: The loss curves in [Figure 6-13](#ch06_clean_figure_13_1748752380715102) show
    similar results.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-13](#ch06_clean_figure_13_1748752380715102)中的损失曲线显示了类似的结果。'
- en: By following these exercises, we were able to reduce the model architecture
    significantly, reducing the number of parameters while improving the quality and
    mitigating overfitting. But there are a few more things we can do—starting with
    dropout.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些练习，我们能够显著减少模型架构，减少参数数量，同时提高质量并减轻过拟合。但我们还可以做更多的事情——从dropout开始。
- en: '![](assets/aiml_0613.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0613.png)'
- en: Figure 6-13\. Reduced dense architecture loss results
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-13\. 减少密集架构损失结果
- en: Using dropout
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用dropout
- en: A common technique for reducing overfitting is to add dropout to a dense neural
    network. We explored this for convolutional neural networks back in [Chapter 3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912),
    so it’s tempting to go straight to it here to see its effects on overfitting.
    But in this case, I want to wait until the vocabulary size, embedding size, and
    architecture complexity have been addressed. Those changes can often have a much
    larger impact than using dropout, and we’ve already seen some nice results.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 减少过拟合的常见技术是在密集神经网络中添加dropout。我们早在[第3章](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)中探讨了卷积神经网络，所以在这里直接探讨它对过拟合的影响是很诱人的。但在这个案例中，我想等到词汇量、嵌入大小和架构复杂性被解决后再进行。这些变化往往比使用dropout有更大的影响，而且我们已经看到了一些很好的结果。
- en: 'Now that our architecture has been simplified to have only eight neurons in
    the middle dense layer, the effect of dropout may be minimized—but let’s explore
    it anyway. Here’s the updated code for the model architecture to add a dropout
    of 0.25 (which equates to two of our eight neurons):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将架构简化到中间密集层只有八个神经元，dropout的影响可能被最小化——但让我们仍然探索它。以下是添加0.25dropout（相当于八个神经元中的两个）的模型架构更新的代码：
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Figure 6-14](#ch06_clean_figure_14_1748752380715110) shows the accuracy results
    when trained for one hundred epochs. This time, we see that the training accuracy
    and validation accuracy are converging, with the training accuracy now lower than
    before. Similarly, the loss curves in [Figure 6-15](#fig-6-15) show convergence,
    so while dropout is making our network a little *less* accurate, it appears to
    generalize it better.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-14](#ch06_clean_figure_14_1748752380715110)显示了训练100个epoch时的准确率结果。这次，我们看到训练准确率和验证准确率正在收敛，训练准确率现在低于之前。同样，[图6-15](#fig-6-15)中的损失曲线也显示了收敛，所以尽管dropout使我们的网络略微**降低**了准确率，但它似乎能更好地泛化。'
- en: But do exercise caution before declaring victory! A close examination of the
    curves shows that the losses have nicely converged but that they *are* higher
    than previously. The training loss is above 0.5 with dropout but was around 0.3
    without. It is also trending downward, so it’s worth experimenting to see whether
    longer training will produce a better result.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 但在宣布胜利之前，请务必谨慎！仔细观察曲线显示，损失已经很好地收敛，但它们**确实**比之前更高。有dropout时训练损失超过0.5，而没有dropout时大约是0.3。它也在下降趋势中，所以值得实验看看更长的训练是否能产生更好的结果。
- en: '![](assets/aiml_0614.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_0614.png](assets/aiml_0614.png)'
- en: Figure 6-14\. Accuracy with added dropout
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-14\. 添加dropout后的准确率
- en: '![](assets/aiml_0615.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_0615.png](assets/aiml_0615.png)'
- en: Figure 6-15\. Loss with added dropout
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-15\. 添加dropout后的损失
- en: You can also see that the model is heading back to its previous pattern of increasing
    validation loss over time. It’s not nearly as bad as before, but it’s heading
    in the wrong direction.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以看到，模型正在回归到之前随时间增加验证损失的旧模式。这并不像之前那么糟糕，但它正朝着错误的方向发展。
- en: In this case, when there were very few neurons, introducing dropout probably
    wasn’t the right idea. It’s still good to have this tool in your arsenal, though,
    so be sure to keep it in mind for more sophisticated architectures than this one.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种情况下，当神经元非常少时，引入dropout可能不是正确的想法。尽管如此，这个工具仍然值得保留在你的工具箱中，所以请确保记住它，用于比这个更复杂的架构。 '
- en: Using regularization
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用正则化
- en: '*Regularization* is a technique that helps prevent overfitting by reducing
    the polarization of weights. If the weights on some of the neurons are too heavy,
    regularization effectively punishes them. Broadly speaking, there are two types
    of regularization:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化**是一种帮助通过减少权重极化来防止过拟合的技术。如果某些神经元的权重过重，正则化会有效地惩罚它们。广义而言，正则化有两种类型：'
- en: L1 regularization
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: L1正则化
- en: This is often called *least absolute shrinkage* and *selection operator* (*lasso*)
    regularization. It effectively helps us ignore the zero or close-to-zero weights
    when calculating a result in a layer.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为**最小绝对收缩**和**选择算子**（lasso）正则化。它有效地帮助我们计算层中的结果时忽略零或接近零的权重。
- en: L2 regularization
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: L2正则化
- en: This is often called *ridge* regression because it pushes values apart by taking
    their squares. This tends to amplify the differences between nonzero values and
    zero or close-to-zero ones, creating a ridge effect.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为**岭回归**，因为它通过取平方值来推远数值。这往往放大了非零值与零或接近零的值之间的差异，从而产生岭效应。
- en: The two approaches can also be combined into what is sometimes called *elastic*
    regularization.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法也可以结合成有时被称为 *弹性* 正则化的方法。
- en: 'For NLP problems like the one we’re considering, L2 is most commonly used.
    It can be added as the `weight_decay` attribute to the `optimizer.` Here’s an
    example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像我们所考虑的 NLP 问题，L2 最常用。它可以作为 `weight_decay` 属性添加到 `optimizer` 中。以下是一个示例：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will apply the `weight_decay` of `0.01.` (Usually, you’ll have a value
    between 0.01 and 0.001 here). Alternatively, a neat trick you can do with PyTorch
    is to define different weight decays for different layers by specifying them within
    the `Adam` declaration call, like this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这将应用 `weight_decay` 的 `0.01`。通常，你在这里会有一个介于 0.01 和 0.001 之间的值。或者，你可以使用 PyTorch
    做的一个巧妙技巧是，通过在 `Adam` 声明调用中指定它们来为不同的层定义不同的权重衰减，如下所示：
- en: '[PRE22]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The impact of adding regularization in a simple model like this isn’t particularly
    large, but it does smooth out our training loss and validation loss somewhat.
    It might be overkill for this scenario, but as with dropout, it’s a good idea
    to understand how to use regularization to prevent your model from getting overspecialized.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样一个简单模型中添加正则化的影响并不特别大，但它确实在一定程度上平滑了我们的训练损失和验证损失。对于这个场景来说可能有些过度，但就像 dropout
    一样，了解如何使用正则化来防止模型过度专业化是一个好主意。
- en: Other optimization considerations
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他优化考虑因素
- en: 'While the modifications we’ve made have given us a much-improved model with
    less overfitting, there are other hyperparameters that you can experiment with.
    For example, we chose to make the maximum sentence length one hundred words, but
    that was purely arbitrary and probably not optimal. It’s a good idea to explore
    the corpus and see what a better sentence length might be. Here’s a snippet of
    code that looks at the sentences and plots the lengths of each one, sorted from
    low to high:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们所做的修改使我们得到了一个拟合度更低的改进模型，但还有其他超参数你可以尝试。例如，我们选择将最大句子长度设为 100 个单词，但这完全是随意的，可能不是最优的。探索语料库并查看可能更好的句子长度是一个好主意。以下是一段代码，它查看句子并按从低到高的顺序绘制每个句子的长度：
- en: '[PRE23]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: See [Figure 6-16](#ch06_clean_figure_15_1748752380715124) for the results of
    this.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 见 [图 6-16](#ch06_clean_figure_15_1748752380715124) 以查看此结果。
- en: '![](assets/aiml_0616.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0616.png)'
- en: Figure 6-16\. Exploring sentence length
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-16\. 探索句子长度
- en: Less than 200 sentences in the total corpus of 26,000+ have a length of 100
    words or greater, so by choosing this as the maximum length, we’re introducing
    a lot of padding that isn’t necessary and thus affecting the model’s performance.
    Reducing the maximum to 85 words would still keep 26,000 of the sentences (99%+)
    with greatly reduced padding.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在 26,000+ 的总语料库中，少于 200 个句子的长度超过 100 个单词，因此选择这个作为最大长度，我们引入了大量的不必要的填充，从而影响了模型的性能。将最大长度减少到
    85 个单词仍然可以保留 26,000 个句子（99%+），同时大幅减少填充。
- en: Putting It All Together
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整合所有内容
- en: Taking all of the preceding optimizations into effect and retraining the model
    for three hundred epochs gives you the results in [Figure 6-17](#ch06_clean_figure_16_1748752380715140)
    for training and validation accuracies. Given that their curves are roughly matched,
    it shows that we’ve taken huge steps toward avoiding overfitting and that we have
    a network that’s learning effectively.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有前面的优化措施生效并重新训练模型 300 个周期，可以得到 [图 6-17](#ch06_clean_figure_16_1748752380715140)
    中训练和验证准确率的结果。鉴于它们的曲线大致匹配，这表明我们已迈出了巨大的步伐，避免了过拟合，并且我们有一个正在有效学习的网络。
- en: Similarly, the training and validation loss curves over three hundred epochs
    are showing remarkable similarity, as depicted in [Figure 6-18](#ch06_clean_figure_17_1748752380715155),
    which indicates that the optimizations are a step in the right direction to prevent
    overfitting for this model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，经过 300 个周期的训练和验证损失曲线显示出显著相似性，如 [图 6-18](#ch06_clean_figure_17_1748752380715155)
    所示，这表明优化是防止此模型过拟合的正确方向的一步。
- en: '![](assets/aiml_0617.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0617.png)'
- en: Figure 6-17\. Optimized training and validation accuracy
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-17\. 优化后的训练和验证准确率
- en: '![](assets/aiml_0618.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0618.png)'
- en: Figure 6-18\. Optimized training and validation loss
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-18\. 优化后的训练和验证损失
- en: Using the Model to Classify a Sentence
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型对句子进行分类
- en: 'Now that you’ve created the model, trained it, and optimized it to remove a
    lot of the problems that caused the overfitting, the next step is to run the model
    and inspect its results. To do this, you’ll create an array of new sentences.
    Consider, for example:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经创建了模型，训练了它，并优化了它以消除导致过拟合的许多问题，下一步是运行模型并检查其结果。为此，你需要创建一个新句子的数组。例如，考虑以下内容：
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can then encode these by using the same tokenizer that you used when creating
    the vocabulary for training:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用与创建训练词汇表时相同的分词器来对这些句子进行编码：
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It’s important to use this tokenizer because it has the tokens for the words
    that the network was trained on!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个分词器很重要，因为它包含了网络训练时使用的标记！
- en: 'The output of the print statement will be the sequences for the preceding sentences:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 打印语句的输出将是前面句子的序列：
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: There are a lot of `1` tokens here (“<OOV>”), because words like *granny* and
    *spiders* don’t appear in the dictionary. The sequences are also shorter because
    the stopwords have been removed.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多`1`标记（“<OOV>”），因为像*granny*和*spiders*这样的词在字典中没有出现。由于已经移除了停用词，序列也变短了。
- en: 'Next, before you can pass the sequences to the model, you’ll need to put them
    in the shape that the model expects—that is, the desired length. You can do this
    with `pad_sequences` in the same way you did when training the model:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在将序列传递给模型之前，你需要将它们转换为模型期望的形状——即所需的长度。你可以像训练模型时那样使用`pad_sequences`来完成此操作：
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will output the sentences as sequences of length `85`, so the output for
    the first sequence will be as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出长度为`85`的句子序列，因此第一个序列的输出如下：
- en: '[PRE28]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It was a very short sentence, so it’s padded up to 85 characters with a lot
    of zeros!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常短的句子，所以它被填充到85个字符，其中包含很多零！
- en: Now that you’ve padded and tokenized the sentences to fit the model’s expectations
    for the input dimensions, it’s time to pass them to the model and get predictions
    back.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经将句子填充和分词以适应模型对输入维度的期望，是时候将它们传递给模型并获取预测结果了。
- en: 'This involves multiple steps. First, convert the padded sequence into an input
    tensor:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到多个步骤。首先，将填充的序列转换为输入张量：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, put the model into evaluation mode to get the predictions, and then simply
    pass the `input_ids` to it to get the outputs:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将模型置于评估模式以获取预测结果，然后简单地将`input_ids`传递给它以获取输出：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The results will be passed back as a list and printed, with high values indicating
    likely sarcasm. Here are the results for our sample sentences:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将以列表的形式返回并打印，高值表示可能存在讽刺。以下是针对我们的样本句子的结果：
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The high score for the first sentence (“granny starting to fear spiders in the
    garden might be real”), despite it having a lot of stopwords and being padded
    with a lot of zeros, indicates that there is a level of sarcasm there. The other
    two sentences scored much lower, indicating a lower likelihood of sarcasm in them.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管第一个句子（“granny starting to fear spiders in the garden might be real”）有很多停用词，并且被填充了很多零，但其高分数表明其中存在一定程度的讽刺。其他两个句子的分数要低得多，表明它们中讽刺的可能性较低。
- en: 'To get the probabilities, you can call the `squeeze()` method to retrieve the
    tensor values. And if you want to make a comparison to a threshold to get your
    prediction—for example, above 0.5 indicates sarcasm and below 0.5 indicates no
    sarcasm—then you can use code like this:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取概率，你可以调用`squeeze()`方法来检索张量值。如果你想将预测结果与阈值进行比较以获取预测结果——例如，大于0.5表示讽刺，小于0.5表示没有讽刺——那么你可以使用如下代码：
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Based on your network tuning, you could also establish what you think the appropriate
    threshold should be. Running this with a 0.5 threshold gives us the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的网络调整，你也可以确定你认为适当的阈值应该是多少。以0.5阈值为例运行，结果如下：
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: So, with these test sentences, we’re beginning to get a good indication that
    our network is performing as desired. You should test it with other data to see
    if you can break it, and if you break it consistently, then it’ll be time to try
    a different model architecture, use transfer learning from an existing working
    network, or explore using pretrained embeddings.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用这些测试句子，我们开始得到一个很好的迹象，表明我们的网络正在按预期运行。你应该用其他数据测试它，看看你是否能破坏它，如果你能持续地破坏它，那么就是时候尝试不同的模型架构，从现有的工作网络中利用迁移学习，或者探索使用预训练的嵌入进行了。
- en: We’ll learn about this in the next section, but before that, I’d like to show
    you how you can visualize the custom embeddings that this network learned.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中学习这个内容，但在那之前，我想向你展示你可以如何可视化这个网络学习到的自定义嵌入。
- en: Visualizing the Embeddings
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化嵌入
- en: To visualize embeddings, you can use an online tool called the [Embedding Projector](http://projector.tensorflow.org).
    It comes preloaded with many existing datasets, but in this section, you’ll see
    how to take the data from the model you’ve just trained and visualize it by using
    this tool.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化嵌入，你可以使用一个名为[嵌入投影仪](http://projector.tensorflow.org)的在线工具。它预先加载了许多现有数据集，但在本节中，你将看到如何使用这个工具通过模型训练的数据来可视化。
- en: 'But first, you’ll need a function to reverse the word index. It currently has
    the word as the token and the key as the value, but you need to invert it so you’ll
    have word values to plot on the projector. Here’s the code to do this:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，你需要一个函数来反转单词索引。它目前将单词作为标记，将键作为值，但你需要将其反转，以便你可以在投影仪上绘制单词值。以下是执行此操作的代码：
- en: '[PRE34]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You’ll also need to extract the weights of the vectors in the embeddings:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要提取嵌入中的向量权重：
- en: '[PRE35]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'If you’ve followed the optimizations in this chapter, the output of this will
    be `(2000,7)` because we used a 2,000 word vocabulary and 7 dimensions for the
    embedding. If you want to explore a word and its vector details, you can do so
    with code like this:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经跟随了本章中的优化，这个输出的结果将是 `(2000,7)`，因为我们使用了2,000个单词的词汇量和7个维度的嵌入。如果你想探索一个单词及其向量细节，你可以使用如下代码：
- en: '[PRE36]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This will produce the following output:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE37]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: So, the word *new* is represented by a vector with those seven coefficients
    on its axes.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单词 *new* 由一个在轴上有那些七个系数的向量表示。
- en: 'The Embedding Projector uses two tab-separated values (TSV) files, one for
    the vector dimensions and one for metadata. This code will generate them for you:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Embedding Projector 使用两个以制表符分隔的值（TSV）文件，一个用于向量维度，一个用于元数据。以下代码将为你生成它们：
- en: '[PRE38]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Alternatively, if you are using Google Colab, you can download the TSV files
    with the following code or from the Files pane:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你使用Google Colab，你可以使用以下代码或从文件面板下载TSV文件：
- en: '[PRE39]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Once you have the files, you can press the Load button on the projector to visualize
    the embeddings (see [Figure 6-19](#ch06_clean_figure_18_1748752380715170)).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了这些文件，你就可以在投影仪上按下“加载”按钮来可视化嵌入（见[图6-19](#ch06_clean_figure_18_1748752380715170)）。
- en: '![](assets/aiml_0619.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![嵌入投影仪](assets/aiml_0619.png)'
- en: Figure 6-19\. Using the Embeddings Projector
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-19. 使用嵌入投影仪
- en: You can also use the vectors and meta TSV files where recommended in the resulting
    dialog and then click Sphereize Data on the projector. This will cause the words
    to be clustered in a sphere and will give you a clear visualization of the binary
    nature of this classifier. It’s only been trained on sarcastic and nonsarcastic
    sentences, so words tend to cluster toward one label or another (see [Figure 6-20](#ch06_clean_figure_19_1748752380715184)).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在结果对话框中推荐的位置使用向量元数据TSV文件，然后在投影仪上点击“球化数据”。这将导致单词在球体中聚集，并为你提供这个分类器二进制性质的清晰可视化。它只训练在讽刺和非讽刺句子上，所以单词倾向于聚集在某个标签附近（见[图6-20](#ch06_clean_figure_19_1748752380715184)）。
- en: '![](assets/aiml_0620.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![嵌入投影仪](assets/aiml_0620.png)'
- en: Figure 6-20\. Visualizing the sarcasm embeddings
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-20. 可视化讽刺嵌入
- en: Screenshots don’t do all of this justice—you should try it for yourself! You
    can rotate the center sphere and explore the words on each “pole” to see the impact
    they have on the overall classification, and you can also select words and show
    related ones in the righthand pane. Have a play and experiment!
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕截图并不能完全体现这些功能——你应该亲自尝试！你可以旋转中心球体，探索每个“极点”上的单词，以查看它们对整体分类的影响，你还可以在右侧面板中选择单词并显示相关的单词。玩一玩，实验一下！
- en: Using Pretrained Embeddings
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练嵌入
- en: An alternative to training your own embeddings is to use ones that have been
    pretrained by others on your behalf. There are many sources where you can find
    these, including Kaggle and Hugging Face. You can even find pretrained embeddings
    posted alongside research results. One such set of pretrained embeddings is the
    [Stanford GloVe embeddings](https://oreil.ly/s1YWw), and we’ll explore those here.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 训练自己的嵌入的另一种选择是使用其他人预先训练的嵌入。你可以在许多来源找到这些嵌入，包括Kaggle和Hugging Face。你甚至可以在研究结果旁边找到预先训练的嵌入。这样一组预先训练的嵌入是[斯坦福GloVe嵌入](https://oreil.ly/s1YWw)，我们将在下面探讨。
- en: Note, however, that when using embeddings that have been pretrained, you should
    also consider updating and changing your tokenizer to match any rules used with
    the pretrained embeddings.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，当使用预训练的嵌入时，你也应该考虑更新和更改你的分词器，以匹配预训练嵌入使用的任何规则。
- en: For example, with the GloVE pretrained embeddings—which simply comprise a large
    text file of words with their pretrained embedding in a number of dimensions from
    50 to 300—the rules used to tokenize words are a little different from those for
    the handmade tokenizer we’ve been using for raw data. So, for GloVe, you should
    consider rules such as all of the words being lowercase or numbers being normalized
    to 0.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用GloVE预训练嵌入——它仅仅是一个包含单词及其在50到300个维度上的预训练嵌入的大文本文件——用于分词单词的规则与我们一直用于原始数据的手动分词器略有不同。因此，对于GloVe，你应该考虑所有单词都小写或数字归一化到0的规则。
- en: 'Once you’ve done this (I’ve provided code for GloVe in the downloads, and I
    discuss it in a little more detail in the next chapter), then it’s simply a matter
    of loading the weights of the pretrained embeddings to your model definition like
    this:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成这个操作（我在下载中提供了GloVe的代码，并在下一章中进行了更详细的讨论），那么只需将预训练嵌入的权重加载到你的模型定义中，如下所示：
- en: '[PRE40]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: If you don’t want to learn from these embeddings and you want to just use them,
    then you should set `freeze_embeddings` to `True`. Otherwise, the network will
    fine-tune by using the pre-loaded embedding weights as a starting point.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想从这些嵌入中学习，只想使用它们，那么你应该将`freeze_embeddings`设置为`True`。否则，网络将使用预加载的嵌入权重作为起点进行微调。
- en: This model will rapidly reach peak accuracy in training, and it will not overfit
    as much as we saw previously. The accuracy over three hundred epochs shows that
    training and validation are very much in step with each other (see [Figure 6-22](#ch06_clean_figure_21_1748752380715231)).
    The loss values are also in step, which shows that we are fitting very nicely
    over the first couple of hundred epochs. However, they also begin to diverge (see
    [Figure 6-22](#ch06_clean_figure_21_1748752380715231)).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型将迅速达到训练中的峰值准确度，并且它不会像我们之前看到的那样过度拟合。经过三百个epoch的准确度显示，训练和验证非常一致（参见[图6-22](#ch06_clean_figure_21_1748752380715231)）。损失值也是一致的，这表明我们在前几百个epoch中拟合得非常好。然而，它们也开始发散（参见[图6-22](#ch06_clean_figure_21_1748752380715231)）。
- en: On the other hand, it is worth noting that the overall accuracy (at about 70%)
    is quite low, considering that a coin flip would have a 50% chance of getting
    it right! So, while using pretrained embeddings can make for much faster training
    with less overfitting, you should also understand what it is that they’re useful
    for and that they may not always be best for your scenario. You may therefore
    need to explore optimization methods or alternatives where appropriate.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，值得注意的是，整体准确度（大约70%）相当低，考虑到抛硬币有50%的机会猜对！因此，虽然使用预训练嵌入可以加快训练速度并减少过度拟合，但你应该了解它们有什么用，以及它们可能并不总是最适合你的场景。因此，你可能需要探索适当的优化方法或替代方案。
- en: '![](assets/aiml_0621.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0621.png)'
- en: Figure 6-21\. Accuracy metrics using GloVe embeddings
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-21\. 使用GloVe嵌入的准确度指标
- en: '![](assets/aiml_0622.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0622.png)'
- en: Figure 6-22\. Loss metrics using GloVe embeddings
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-22\. 使用GloVe嵌入的损失指标
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you built your first model that can understand sentiment in
    text. It did this by taking the tokenized text from [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)
    and mapping it to vectors. Then, using backpropagation, it learned the appropriate
    “direction” for each vector based on the label for the sentence containing it.
    Finally, it was able to use all of the vectors for a collection of words to build
    up an idea of the sentiment within the sentence.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你构建了第一个能够理解文本情感的模型。它是通过从[第5章](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)获取分词文本并将其映射到向量来实现的。然后，使用反向传播，它根据包含该句子的标签学习每个向量的适当“方向”。最后，它能够使用一组单词的所有向量来构建句子中情感的概念。
- en: You also explored ways to optimize your model to avoid overfitting, and you
    saw a neat visualization of the final vectors representing your words. But while
    this was a nice way to classify sentences, it simply treated each sentence as
    a bunch of words. There was no inherent sequence involved, and the order of appearance
    of words is very important in determining the real meaning of a sentence.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你还探索了优化模型以避免过拟合的方法，并看到了表示你单词的最终向量的整洁可视化。但尽管这是一种很好的句子分类方法，它只是简单地将每个句子视为一堆单词。其中没有固有的序列，而在确定句子的真正含义时，单词出现的顺序非常重要。
- en: 'Therefore, it’s a good idea to see if we can improve our models by taking sequence
    into account. We’ll explore that in the next chapter with the introduction of
    a new layer type: a *recurrent* layer, which is the foundation of recurrent neural
    networks.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑如果我们能通过考虑序列来改进我们的模型，那将是个不错的想法。我们将在下一章中通过介绍一种新的层类型：一个*循环*层，这是循环神经网络的基础，来探讨这一点。
