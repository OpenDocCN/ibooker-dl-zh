- en: 3 It starts with a tensor
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 它始于一个张量
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding tensors, the basic data structure in PyTorch
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解张量，PyTorch 中的基本数据结构
- en: Indexing and operating on tensors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量的索引和操作
- en: Interoperating with NumPy multidimensional arrays
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 NumPy 多维数组的互操作
- en: Moving computations to the GPU for speed
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将计算迁移到 GPU 以提高速度
- en: In the previous chapter, we took a tour of some of the many applications that
    deep learning enables. They invariably consisted of taking data in some form,
    like images or text, and producing data in another form, like labels, numbers,
    or more images or text. Viewed from this angle, deep learning really consists
    of building a system that can transform data from one representation to another.
    This transformation is driven by extracting commonalities from a series of examples
    that demonstrate the desired mapping. For example, the system might note the general
    shape of a dog and the typical colors of a golden retriever. By combining the
    two image properties, the system can correctly map images with a given shape and
    color to the golden retriever label, instead of a black lab (or a tawny tomcat,
    for that matter). The resulting system can consume broad swaths of similar inputs
    and produce meaningful output for those inputs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们参观了深度学习所能实现的许多应用。它们无一例外地包括将某种形式的数据（如图像或文本）转换为另一种形式的数据（如标签、数字或更多图像或文本）。从这个角度来看，深度学习实际上是构建一个能够将数据从一种表示转换为另一种表示的系统。这种转换是通过从一系列示例中提取所需映射的共同点来驱动的。例如，系统可能注意到狗的一般形状和金毛寻回犬的典型颜色。通过结合这两个图像属性，系统可以正确地将具有特定形状和颜色的图像映射到金毛寻回犬标签，而不是黑色实验室（或者一只黄褐色的公猫）。最终的系统可以处理��量相似的输入并为这些输入产生有意义的输出。
- en: The process begins by converting our input into floating-point numbers. We will
    cover converting image pixels to numbers, as we see in the first step of figure
    3.1, in chapter 4 (along with many other types of data). But before we can get
    to that, in this chapter, we learn how to deal with all the floating-point numbers
    in PyTorch by using tensors.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程始于将我们的输入转换为浮点数。我们将在第 4 章中涵盖将图像像素转换为数字的过程，正如我们在图 3.1 的第一步中所看到的那样（以及许多其他类型的数据）。但在我们开始之前，在本章中，我们将学习如何通过张量在
    PyTorch 中处理所有浮点数。
- en: 3.1 The world as floating-point numbers
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 世界是由浮点数构成的
- en: Since floating-point numbers are the way a network deals with information, we
    need a way to encode real-world data of the kind we want to process into something
    digestible by a network and then decode the output back to something we can understand
    and use for our purpose.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于浮点数是网络处理信息的方式，我们需要一种方法将我们想要处理的现���世界数据编码为网络可以理解的内容，然后将输出解码回我们可以理解并用于我们目的的内容。
- en: '![](../Images/CH03_F01_Stevens2_GS.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F01_Stevens2_GS.png)'
- en: 'Figure 3.1 A deep neural network learns how to transform an input representation
    to an output representation. (Note: The numbers of neurons and outputs are not
    to scale.)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 一个深度神经网络学习如何将输入表示转换为输出表示。（注意：神经元和输出的数量不是按比例缩放的。）
- en: A deep neural network typically learns the transformation from one form of data
    to another in stages, which means the partially transformed data between each
    stage can be thought of as a sequence of intermediate representations. For image
    recognition, early representations can be things such as edge detection or certain
    textures like fur. Deeper representations can capture more complex structures
    like ears, noses, or eyes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络通常通过阶段性地学习从一种数据形式到另一种数据形式的转换来进行学习，这意味着每个阶段之间部分转换的数据可以被视为一系列中间表示。对于图像识别，早期的表示可以是边缘检测或某些纹理，如毛皮。更深层次的表示可以捕捉更复杂的结构，如耳朵、鼻子或眼睛。
- en: In general, such intermediate representations are collections of floating-point
    numbers that characterize the input and capture the data’s structure in a way
    that is instrumental for describing how inputs are mapped to the outputs of the
    neural network. Such characterization is specific to the task at hand and is learned
    from relevant examples. These collections of floating-point numbers and their
    manipulation are at the heart of modern AI--we will see several examples of this
    throughout the book.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，这种中间表示是描述输入并以对描述输入如何映射到神经网络输出至关重要的方式捕捉数据结构的一组浮点数。这种描述是针对手头的任务具体的，并且是从相关示例中学习的。这些浮点数集合及其操作是现代人工智能的核心--我们将在本书中看到几个这样的例子。
- en: It’s important to keep in mind that these intermediate representations (like
    those shown in the second step of figure 3.1) are the results of combining the
    input with the weights of the previous layer of neurons. Each intermediate representation
    is unique to the inputs that preceeded it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住这些中间表示（如图 3.1 的第二步所示）是将输入与前一层神经元的权重相结合的结果。每个中间表示对应于其前面的输入是独一无二的。
- en: Before we can begin the process of converting our data to floating-point input,
    we must first have a solid understanding of how PyTorch handles and stores data--as
    input, as intermediate representations, and as output. This chapter will be devoted
    to precisely that.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始将数据转换为浮点输入的过程之前，我们必须首先对 PyTorch 如何处理和存储数据--作为输入、中间表示和输出有一个扎实的理解。本章将专门讨论这一点。
- en: 'To this end, PyTorch introduces a fundamental data structure: the *tensor*.
    We already bumped into tensors in chapter 2, when we ran inference on pretrained
    networks. For those who come from mathematics, physics, or engineering, the term
    *tensor* comes bundled with the notion of spaces, reference systems, and transformations
    between them. For better or worse, those notions do not apply here. In the context
    of deep learning, tensors refer to the generalization of vectors and matrices
    to an arbitrary number of dimensions, as we can see in figure 3.2\. Another name
    for the same concept is *multidimensional array*. The dimensionality of a tensor
    coincides with the number of indexes used to refer to scalar values within the
    tensor.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，PyTorch引入了一种基本数据结构：*张量*。我们在第2章中已经遇到了张量，当我们对预训练网络进行推断时。对于���些来自数学、物理或工程领域的人来说，*张量*这个术语通常与空间、参考系统和它们之间的变换捆绑在一起。在深度学习的背景下，张量是将向量和矩阵推广到任意维数的概念，正如我们在图3.2中所看到的。同一概念的另一个名称是*多维数组*。张量的维数与用于引用张量内标量值的索引数量相一致。
- en: '![](../Images/CH03_F02_Stevens2_GS.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F02_Stevens2_GS.png)'
- en: Figure 3.2 Tensors are the building blocks for representing data in PyTorch.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 张量是PyTorch中表示数据的基本构件。
- en: PyTorch is not the only library that deals with multidimensional arrays. NumPy
    is by far the most popular multidimensional array library, to the point that it
    has now arguably become the *lingua franca* of data science. PyTorch features
    seamless interoperability with NumPy, which brings with it first-class integration
    with the rest of the scientific libraries in Python, such as SciPy ([www.scipy.org](https://www.scipy.org/)),
    Scikit-learn ([https://scikit-learn .org](https://scikit-learn.org)), and Pandas
    ([https://pandas.pydata.org](https://pandas.pydata.org)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch并不是唯一处理多维数组的库。NumPy是迄今为止最流行的多维数组库，以至于现在可以说它已经成为数据科学的*通用语言*。PyTorch与NumPy具有无缝互操作性，这带来了与Python中其他科学库的一流集成，如SciPy
    ([www.scipy.org](https://www.scipy.org/))、Scikit-learn ([https://scikit-learn.org](https://scikit-learn.org))和Pandas
    ([https://pandas.pydata.org](https://pandas.pydata.org))。
- en: Compared to NumPy arrays, PyTorch tensors have a few superpowers, such as the
    ability to perform very fast operations on graphical processing units (GPUs),
    distribute operations on multiple devices or machines, and keep track of the graph
    of computations that created them. These are all important features when implementing
    a modern deep learning library.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与NumPy数组相比，PyTorch张量具有一些超能力，比如能够在图形处理单元（GPU）上执行非常快速的操作，将操作分布在多个设备或机器上，并跟踪创建它们的计算图。这些都是在实现现代深度学习库时的重要特性。
- en: We’ll start this chapter by introducing PyTorch tensors, covering the basics
    in order to set things in motion for our work in the rest of the book. First and
    foremost, we’ll learn how to manipulate tensors using the PyTorch tensor library.
    This includes things like how the data is stored in memory, how certain operations
    can be performed on arbitrarily large tensors in constant time, and the aforementioned
    NumPy interoperability and GPU acceleration. Understanding the capabilities and
    API of tensors is important if they’re to become go-to tools in our programming
    toolbox. In the next chapter, we’ll put this knowledge to good use and learn how
    to represent several different kinds of data in a way that enables learning with
    neural networks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过介绍PyTorch张量来开始本章，涵盖基础知识，以便为本书其余部分的工作做好准备。首先，我们将学习如何使用PyTorch张量库来操作张量。这包括数据在内存中的存储方式，如何在常数时间内对任意大的张量执行某些操作，以及前面提到的NumPy互操作性和GPU加速。如果我们希望张量成为编程工具箱中的首选工具，那么理解张量的能力和API是很重要的。在下一章中，我们将把这些知识应用到实践中，并学习如何以一种能够利用神经网络进行学习的方式表示多种不同类型的数据。
- en: '3.2 Tensors: Multidimensional arrays'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 张量：多维数组
- en: 'We have already learned that tensors are the fundamental data structure in
    PyTorch. A tensor is an array: that is, a data structure that stores a collection
    of numbers that are accessible individually using an index, and that can be indexed
    with multiple indices.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到了张量是PyTorch中的基本数据结构。张量是一个数组：即，一种数据结构，用于存储一组可以通过索引单独访问的数字，并且可以用多个索引进行索引。
- en: 3.2.1 From Python lists to PyTorch tensors
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 从Python列表到PyTorch张量
- en: 'Let’s see `list` indexing in action so we can compare it to tensor indexing.
    Take a list of three numbers in Python (.code/p1ch3/1_tensors.ipynb):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`list`索引是如何工作的，这样我们就可以将其与张量索引进行比较。在Python中，取一个包含三个数字的列表（.code/p1ch3/1_tensors.ipynb）：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can access the first element of the list using the corresponding zero-based
    index:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相应的从零开始的索引来访问列表的第一个元素：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It is not unusual for simple Python programs dealing with vectors of numbers,
    such as the coordinates of a 2D line, to use Python lists to store the vectors.
    As we will see in the following chapter, using the more efficient tensor data
    structure, many types of data--from images to time series, and even sentences--can
    be represented. By defining operations over tensors, some of which we’ll explore
    in this chapter, we can slice and manipulate data expressively and efficiently
    at the same time, even from a high-level (and not particularly fast) language
    such as Python.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于处理数字向量的简单Python程序，比如2D线的坐标，使用Python列表来存储向量并不罕见。正如我们将在接下来的章节中看到的，使用更高效的张量数据结构，可以表示许多类型的数据--从图像到时间序列，甚至句子。通过定义张量上的操作，其中一些我们将在本章中探讨，我们可以高效地切片和操作数据，即使是从一个高级（并不特别快速）语言如Python。
- en: 3.2.2 Constructing our first tensors
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 构建我们的第一个张量
- en: 'Let’s construct our first PyTorch tensor and see what it looks like. It won’t
    be a particularly meaningful tensor for now, just three ones in a column:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建我们的第一个PyTorch张量并看看它是什么样子。暂时它不会是一个特别有意义的张量，只是一个列中的三个1：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Imports the torch module
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入torch模块
- en: ❷ Creates a one-dimensional tensor of size 3 filled with 1s
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个大小为3、填充为1的一维张量
- en: After importing the `torch` module, we call a function that creates a (one-dimensional)
    tensor of size 3 filled with the value `1.0`. We can access an element using its
    zero-based index or assign a new value to it. Although on the surface this example
    doesn’t differ much from a list of number objects, under the hood things are completely
    different.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `torch` 模块后，我们调用一个函数，创建一个大小为 3、填充值为 `1.0` 的（一维）张量。我们可以使用基于零的索引访问元素或为其分配新值。尽管表面上这个例子与数字对象列表没有太大区别，但在底层情况完全不同。
- en: 3.2.3 The essence of tensors
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 张量的本质
- en: Python lists or tuples of numbers are collections of Python objects that are
    individually allocated in memory, as shown on the left in figure 3.3\. PyTorch
    tensors or NumPy arrays, on the other hand, are views over (typically) contiguous
    memory blocks containing *unboxed* C numeric types rather than Python objects.
    Each element is a 32-bit (4-byte) `float` in this case, as we can see on the right
    side of figure 3.3\. This means storing a 1D tensor of 1,000,000 float numbers
    will require exactly 4,000,000 contiguous bytes, plus a small overhead for the
    metadata (such as dimensions and numeric type).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Python 列表或数字元组是单独分配在内存中的 Python 对象的集合，如图 3.3 左侧所示。另一方面，PyTorch 张量或 NumPy 数组是对（通常）包含*未装箱*的
    C 数值类型而不是 Python 对象的连续内存块的视图。在这种情况下，每个元素是一个 32 位（4 字节）的 `float`，正如我们在图 3.3 右侧所看到的。这意味着存储
    1,000,000 个浮点数的 1D 张量将需要确切的 4,000,000 个连续字节，再加上一些小的开销用于元数据（如维度和数值类型）。
- en: '![](../Images/CH03_F03_Stevens2_GS.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F03_Stevens2_GS.png)'
- en: Figure 3.3 Python object (boxed) numeric values versus tensor (unboxed array)
    numeric values
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 Python 对象（带框）数值值与张量（未带框数组）数值值
- en: 'Say we have a list of coordinates we’d like to use to represent a geometrical
    object: perhaps a 2D triangle with vertices at coordinates (4, 1), (5, 3), and
    (2, 1). The example is not particularly pertinent to deep learning, but it’s easy
    to follow. Instead of having coordinates as numbers in a Python list, as we did
    earlier, we can use a one-dimensional tensor by storing *X*s in the even indices
    and *Y*s in the odd indices, like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个坐标列表，我们想用它来表示一个几何对象：也许是一个顶点坐标为 (4, 1), (5, 3) 和 (2, 1) 的 2D 三角形。这个例子与深度学习无关，但很容易理解。与之前将坐标作为
    Python 列表中的数字不同，我们可以使用一维张量，将*X*存储在偶数索引中，*Y*存储在奇数索引中，如下所示：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Using .zeros is just a way to get an appropriately sized array.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 .zeros 只是获取一个适当大小的数组的一种方式。
- en: ❷ We overwrite those zeros with the values we actually want.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们用我们实际想要的值覆盖了那些零值。
- en: 'We can also pass a Python list to the constructor, to the same effect:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将 Python 列表传递给构造函数，效果相同：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To get the coordinates of the first point, we do the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取第一个点的坐标，我们执行以下操作：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is OK, although it would be practical to have the first index refer to
    individual 2D points rather than point coordinates. For this, we can use a 2D
    tensor:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是可以的，尽管将第一个索引指向单独的 2D 点而不是点坐标会更实用。为此，我们可以使用一个 2D 张量：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we pass a list of lists to the constructor. We can ask the tensor about
    its shape:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将一个列表的列表传递给构造函数。我们可以询问张量的形状：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This informs us about the size of the tensor along each dimension. We could
    also use `zeros` or `ones` to initialize the tensor, providing the size as a tuple:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们张量沿每个维度的大小。我们也可以使用 `zeros` 或 `ones` 来初始化张量，提供大小作为一个元组：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we can access an individual element in the tensor using two indices:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用两个索引访问张量中的单个元素：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This returns the *Y*-coordinate of the zeroth point in our dataset. We can
    also access the first element in the tensor as we did before to get the 2D coordinates
    of the first point:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回我们数据集中第零个点的*Y*坐标。我们也可以像之前那样访问张量中的第一个元素，以获取第一个点的 2D 坐标：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The output is another tensor that presents a different *view* of the same underlying
    data. The new tensor is a 1D tensor of size 2, referencing the values of the first
    row in the `points` tensor. Does this mean a new chunk of memory was allocated,
    values were copied into it, and the new memory was returned wrapped in a new tensor
    object? No, because that would be very inefficient, especially if we had millions
    of points. We’ll revisit how tensors are stored later in this chapter when we
    cover views of tensors in section 3.7\.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是另一个张量，它呈现了相同基础数据的不同*视图*。新张量是一个大小为 2 的 1D 张量，引用了 `points` 张量中第一行的值。这是否意味着分配了一个新的内存块，将值复制到其中，并返回了包装在新张量对象中的新内存？不，因为那样会非常低效，特别是如果我们有数百万个点。当我们在本章后面讨论张量视图时，我们将重新讨论张量是如何存储的。
- en: 3.3 Indexing tensors
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 张量索引
- en: 'What if we need to obtain a tensor containing all points but the first? That’s
    easy using range indexing notation, which also applies to standard Python lists.
    Here’s a reminder:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要获取一个不包含第一个点的张量，那很容易使用范围索引表示法，这也适用于标准 Python 列表。这里是一个提醒：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ All elements in the list
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 列表中的所有元素
- en: ❷ From element 1 inclusive to element 4 exclusive
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从第 1 个元素（包括）到第 4 个元素（不包括）
- en: ❸ From element 1 inclusive to the end of the list
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从第 1 个元素（包括）到列表末尾
- en: ❹ From the start of the list to element 4 exclusive
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从列表开头到第 4 个元素（不包括）
- en: ❺ From the start of the list to one before the last element
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从列表开头到倒数第二个元素之前
- en: ❻ From element 1 inclusive to element 4 exclusive, in steps of 2
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 从第 1 个元素（包括）到第 4 个元素（不包括），步长为 2
- en: 'To achieve our goal, we can use the same notation for PyTorch tensors, with
    the added benefit that, just as in NumPy and other Python scientific libraries,
    we can use range indexing for each of the tensor’s dimensions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现我们的目标，我们可以使用与 PyTorch 张量相同的符号表示法，其中的额外好处是，就像在 NumPy 和其他 Python 科学库中一样，我们可以为张量的每个维度使用范围索引：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ All rows after the first; implicitly all columns
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个之后的所有行；隐式地所有列
- en: ❷ All rows after the first; all columns
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第一个之后的所有行；所有列
- en: ❸ All rows after the first; first column
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第一个之后的所有行；第一列
- en: ❹ Adds a dimension of size 1, just like unsqueeze
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加一个大小为 1 的维度，就像 unsqueeze 一样
- en: In addition to using ranges, PyTorch features a powerful form of indexing, called
    *advanced indexing*, which we will look at in the next chapter.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用范围，PyTorch 还具有一种强大的索引形式，称为*高级索引*，我们将在下一章中看到。
- en: 3.4 Named tensors
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 命名张量
- en: The dimensions (or axes) of our tensors usually index something like pixel locations
    or color channels. This means when we want to index into a tensor, we need to
    remember the ordering of the dimensions and write our indexing accordingly. As
    data is transformed through multiple tensors, keeping track of which dimension
    contains what data can be error-prone.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的张量的维度（或轴）通常索引像素位置或颜色通道之类的内容。这意味着当我们想要索引张量��，我们需要记住维度的顺序，并相应地编写我们的索引。随着数据通过多个张量进行转换，跟踪哪个维度包含什么数据可能会出错。
- en: To make things concrete, imagine that we have a 3D tensor like `img_t` from
    section 2.1.4 (we will use dummy data for simplicity here), and we want to convert
    it to grayscale. We looked up typical weights for the colors to derive a single
    brightness value:[¹](#pgfId-1013535)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使事情具体化，想象我们有一个三维张量 `img_t`，来自第2.1.4节（这里为简单起见使用虚拟数据），我们想将其转换为灰度。我们查找了颜色的典型权重，以得出单个亮度值：[¹](#pgfId-1013535)
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We also often want our code to generalize--for example, from grayscale images
    represented as 2D tensors with height and width dimensions to color images adding
    a third channel dimension (as in RGB), or from a single image to a batch of images.
    In section 2.1.4, we introduced an additional batch dimension in `batch_t`; here
    we pretend to have a batch of 2:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常希望我们的代码能够泛化--例如，从表示为具有高度和宽度维度的2D张量的灰度图像到添加第三个通道维度的彩色图像（如 RGB），或者从单个图像到一批图像。在第2.1.4节中，我们引入了一个额外的批处理维度
    `batch_t`；这里我们假装有一个批处理为2的批次：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'So sometimes the RGB channels are in dimension 0, and sometimes they are in
    dimension 1\. But we can generalize by counting from the end: they are always
    in dimension -3, the third from the end. The lazy, unweighted mean can thus be
    written as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有时 RGB 通道在维度0中，有时它们在维度1中。但我们可以通过从末尾计数来概括：它们总是在维度-3中，距离末尾的第三个。因此，懒惰的、无权重的平均值可以写成如下形式：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'But now we have the weight, too. PyTorch will allow us to multiply things that
    are the same shape, as well as shapes where one operand is of size 1 in a given
    dimension. It also appends leading dimensions of size 1 automatically. This is
    a feature called *broadcasting*. `batch_t` of shape (2, 3, 5, 5) is multiplied
    by `unsqueezed_weights` of shape (3, 1, 1), resulting in a tensor of shape (2,
    3, 5, 5), from which we can then sum the third dimension from the end (the three
    channels):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我们也有了权重。PyTorch 将允许我们将形状相同的东西相乘，以及其中一个操作数在给定维度上的大小为1。它还会自动附加大小为1的前导维度。这是一个称为*广播*的特性。形状为
    (2, 3, 5, 5) 的 `batch_t` 乘以形状为 (3, 1, 1) 的 `unsqueezed_weights`，得到形状为 (2, 3, 5,
    5) 的张量，然后我们可以对末尾的第三个维度求和（三个通道）：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Because this gets messy quickly--and for the sake of efficiency--the PyTorch
    function `einsum` (adapted from NumPy) specifies an indexing mini-language[²](#pgfId-1014597)
    giving index names to dimensions for sums of such products. As often in Python,
    broadcasting--a form of summarizing unnamed things--is done using three dots `''...''`;
    but don’t worry too much about `einsum`, because we will not use it in the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这很快变得混乱--出于效率考虑--PyTorch 函数 `einsum`（改编自 NumPy）指定了一个索引迷你语言[²](#pgfId-1014597)，为这些乘积的和给出维度的索引名称。就像在
    Python 中经常一样，广播--一种总结未命名事物的形式--使用三个点 `'...'` 完成；但不要太担心 `einsum`，因为我们接下来不会使用它：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As we can see, there is quite a lot of bookkeeping involved. This is error-prone,
    especially when the locations where tensors are created and used are far apart
    in our code. This has caught the eye of practitioners, and so it has been suggested[³](#pgfId-1015567)
    that the dimension be given a name instead.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，涉及到相当多的簿记工作。这是容易出错的，特别是当张量的创建和使用位置在我们的代码中相距很远时。这引起了从业者的注意，因此有人建议[³](#pgfId-1015567)给维度赋予一个名称。
- en: 'PyTorch 1.3 added *named tensors* as an experimental feature (see [https://pytorch
    .org/tutorials/intermediate/named_tensor_tutorial.html](https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html)
    and [https://pytorch.org/ docs/stable/named_tensor.html](https://pytorch.org/docs/stable/named_tensor.html)).
    Tensor factory functions such as `tensor` and `rand` take a `names` argument.
    The names should be a sequence of strings:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 1.3 添加了*命名张量*作为一个实验性功能（参见[https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html](https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html)
    和 [https://pytorch.org/docs/stable/named_tensor.html](https://pytorch.org/docs/stable/named_tensor.html)）。张量工厂函数如
    `tensor` 和 `rand` 接受一个 `names` 参数。这些名称应该是一个字符串序列：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When we already have a tensor and want to add names (but not change existing
    ones), we can call the method `refine_names` on it. Similar to indexing, the ellipsis
    (`...)` allows you to leave out any number of dimensions. With the `rename` sibling
    method, you can also overwrite or drop (by passing in `None`) existing names:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们已经有一个张量并想要添加名称（但不更改现有名称）时，我们可以在其上调用方法 `refine_names`。类似于索引，省略号 (`...`) 允许您省略任意数量的维度。使用
    `rename` 兄弟方法，您还可以覆盖或删除（通过传入 `None`）现有名称：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For operations with two inputs, in addition to the usual dimension checks--whether
    sizes are the same, or if one is 1 and can be broadcast to the other--PyTorch
    will now check the names for us. So far, it does not automatically align dimensions,
    so we need to do this explicitly. The method `align_as` returns a tensor with
    missing dimensions added and existing ones permuted to the right order:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有两个输入的操作，除了通常的维度检查--大小是否相同，或者一个是否为1且可以广播到另一个--PyTorch 现在将为我们检查名称。到目前为止，它不会自动对齐维度，因此我们需要明确地执行此操作。方法
    `align_as` 返回一个具有缺失维度的张量，并将现有维度排列到正确的顺序：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Functions accepting dimension arguments, like `sum`, also take named dimensions:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接受维度参数的函数，如 `sum`，也接受命名维度：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If we try to combine dimensions with different names, we get an error:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试结合具有不同名称的维度，我们会收到一个错误：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If we want to use tensors outside functions that operate on named tensors,
    we need to drop the names by renaming them to `None`. The following gets us back
    into the world of unnamed dimensions:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在不操作命名张量的函数之外使用张量，我们需要通过将它们重命名为 `None` 来删除名称。以下操作使我们回到无名称维度的世界：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Given the experimental nature of this feature at the time of writing, and to
    avoid mucking around with indexing and alignment, we will stick to unnamed in
    the remainder of the book. Named tensors have the potential to eliminate many
    sources of alignment errors, which--if the PyTorch forum is any indication--can
    be a source of headaches. It will be interesting to see how widely they will be
    adopted.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于在撰写时此功能的实验性质，并为避免处理索引和对齐，我们将在本书的其余部分坚持使用无名称。命名张量有潜力消除许多对齐错误的来源，这些错误——如果以 PyTorch
    论坛为例——可能是头痛的根源。看到它们将被广泛采用将是很有趣的。
- en: 3.5 Tensor element types
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 张量元素类型
- en: 'So far, we have covered the basics of how tensors work, but we have not yet
    touched on what kinds of numeric types we can store in a `Tensor`. As we hinted
    at in section 3.2, using the standard Python numeric types can be suboptimal for
    several reasons:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了张量如何工作的基础知识，但我们还没有涉及可以存储在 `Tensor` 中的数值类型。正如我们在第 3.2 节中暗示的，使用标准的
    Python 数值类型可能不是最佳选择，原因有几个：
- en: '*Numbers in Python are objects.* Whereas a floating-point number might require
    only, for instance, 32 bits to be represented on a computer, Python will convert
    it into a full-fledged Python object with reference counting, and so on. This
    operation, called *boxing*, is not a problem if we need to store a small number
    of numbers, but allocating millions gets very inefficient.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python 中的数字是对象。* 虽然浮点数可能只需要，例如，32 位来在计算机上表示，但 Python 会将其转换为一个完整的 Python 对象，带有引用计数等等。这个操作，称为*装箱*，如果我们需要存储少量数字，那么这并不是问题，但分配数百万个数字会变得非常低效。'
- en: '*Lists in Python are meant for sequential collections of objects.* There are
    no operations defined for, say, efficiently taking the dot product of two vectors,
    or summing vectors together. Also, Python lists have no way of optimizing the
    layout of their contents in memory, as they are indexable collections of pointers
    to Python objects (of any kind, not just numbers). Finally, Python lists are one-dimensional,
    and although we can create lists of lists, this is again very inefficient.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python 中的列表用于对象的顺序集合。* 没有为例如高效地计算两个向量的点积或将向量相加等操作定义。此外，Python 列表无法优化其内容在内存中的布局，因为它们是指向
    Python 对象（任何类型，不仅仅是数字）的可索引指针集合。最后，Python 列表是一维的，虽然我们可以创建列表的列表，但这同样非常低效。'
- en: '*The Python interpreter is slow compared to optimized, compiled code.* Performing
    mathematical operations on large collections of numerical data can be much faster
    using optimized code written in a compiled, low-level language like C.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与优化的编译代码相比，Python 解释器速度较慢。* 在大量数值数据上执行数学运算时，使用在编译、低级语言如 C 中编写的优化代码可以更快地完成。'
- en: For these reasons, data science libraries rely on NumPy or introduce dedicated
    data structures like PyTorch tensors, which provide efficient low-level implementations
    of numerical data structures and related operations on them, wrapped in a convenient
    high-level API. To enable this, the objects within a tensor must all be numbers
    of the same type, and PyTorch must keep track of this numeric type.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，数据科学库依赖于 NumPy 或引入专用数据结构如 PyTorch 张量，它们提供了高效的低级数值数据结构实现以及相关操作，并包装在方便的高级
    API 中。为了实现这一点，张量中的对象必须都是相同类型的数字，并且 PyTorch 必须跟踪这种数值类型。
- en: 3.5.1 Specifying the numeric type with dtype
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 使用 dtype 指定数值类型
- en: 'The `dtype` argument to tensor constructors (that is, functions like `tensor`,
    `zeros`, and `ones`) specifies the numerical data (d) type that will be contained
    in the tensor. The data type specifies the possible values the tensor can hold
    (integers versus floating-point numbers) and the number of bytes per value.[⁴](#pgfId-1018955)
    The `dtype` argument is deliberately similar to the standard NumPy argument of
    the same name. Here’s a list of the possible values for the `dtype` argument:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 张量构造函数（如 `tensor`、`zeros` 和 `ones`）的 `dtype` 参数指定了张量中将包含的数值数据类型。数据类型指定了张量可以保存的可能值（整数与浮点数）以及每个值的字节数。`dtype`
    参数故意与同名的标准 NumPy 参数相似。以下是 `dtype` 参数可能的值列表：
- en: '`torch.float32` or `torch.float`: 32-bit floating-point'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.float32` 或 `torch.float`：32 位浮点数'
- en: '`torch.float64` or `torch.double`: 64-bit, double-precision floating-point'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.float64` 或 `torch.double`：64 位，双精度浮点数'
- en: '`torch.float16` or `torch.half`: 16-bit, half-precision floating-point'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.float16` 或 `torch.half`：16 位，半精度浮点数'
- en: '`torch.int8`: signed 8-bit integers'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.int8`：有符号 8 位整数'
- en: '`torch.uint8`: unsigned 8-bit integers'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.uint8`：无符号 8 位整数'
- en: '`torch.int16` or `torch.short`: signed 16-bit integers'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.int16` 或 `torch.short`：有符号 16 位整数'
- en: '`torch.int32` or `torch.int`: signed 32-bit integers'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.int32` 或 `torch.int`：有符号 32 位整数'
- en: '`torch.int64` or `torch.long`: signed 64-bit integers'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.int64` 或 `torch.long`：有符号 64 位整数'
- en: '`torch.bool`: Boolean'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.bool`：布尔值'
- en: The default data type for tensors is 32-bit floating-point.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的默认数据类型是 32 位浮点数。
- en: 3.5.2 A dtype for every occasion
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 每个场合的 dtype
- en: As we will see in future chapters, computations happening in neural networks
    are typically executed with 32-bit floating-point precision. Higher precision,
    like 64-bit, will not buy improvements in the accuracy of a model and will require
    more memory and computing time. The 16-bit floating-point, half-precision data
    type is not present natively in standard CPUs, but it is offered on modern GPUs.
    It is possible to switch to half-precision to decrease the footprint of a neural
    network model if needed, with a minor impact on accuracy.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在未来的章节中看到的，神经网络中发生的计算通常以32位浮点精度执行。更高的精度，如64位，不会提高模型的准确性，并且会消耗更多的内存和计算时间。16位浮点、半精度数据类型在标准
    CPU 上并不存在，但在现代 GPU 上提供。如果需要，可以切换到半精度以减少神经网络模型的占用空间，对准确性的影响很小。
- en: Tensors can be used as indexes in other tensors. In this case, PyTorch expects
    indexing tensors to have a 64-bit integer data type. Creating a tensor with integers
    as arguments, such as using `torch.tensor([2, 2])`, will create a 64-bit integer
    tensor by default. As such, we’ll spend most of our time dealing with `float32`
    and `int64`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 张量可以用作其他张量的索引。在这种情况下，PyTorch 期望索引张量具有64位整数数据类型。使用整数作为参数创建张量，例如使用 `torch.tensor([2,
    2])`，将默认创建一个64位整数张量。因此，我们将大部分时间处理 `float32` 和 `int64`。
- en: Finally, predicates on tensors, such as `points > 1.0`, produce `bool` tensors
    indicating whether each individual element satisfies the condition. These are
    the numeric types in a nutshell.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，关于张量的谓词，如 `points > 1.0`，会产生 `bool` 张量，指示每个单独元素是否满足条件。这就是数值类型的要点。
- en: 3.5.3 Managing a tensor’s dtype attribute
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 管理张量的 dtype 属性
- en: 'In order to allocate a tensor of the right numeric type, we can specify the
    proper `dtype` as an argument to the constructor. For example:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分配正确数值类型的张量，我们可以将适当的 `dtype` 作为构造函数的参数指定。例如：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can find out about the `dtype` for a tensor by accessing the corresponding
    attribute:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问相应的属性，我们可以了解张量的 `dtype`：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can also cast the output of a tensor creation function to the right type
    using the corresponding casting method, such as
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用相应的转换方法将张量创建函数的输出转换为正确的类型，例如
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'or the more convenient `to` method:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 或更方便的 `to` 方法：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Under the hood, `to` checks whether the conversion is necessary and, if so,
    does it. The `dtype`-named casting methods like `float` are shorthands for `to`,
    but the `to` method can take additional arguments that we’ll discuss in section
    3.9.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，`to` 检查转换是否必要，并在必要时执行。像 `float` 这样以 `dtype` 命名的转换方法是 `to` 的简写，但 `to` 方法可以接受我们将在第3.9节讨论的其他参数。
- en: 'When mixing input types in operations, the inputs are converted to the larger
    type automatically. Thus, if we want 32-bit computation, we need to make sure
    all our inputs are (at most) 32-bit:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作中混合输入类型时，输入会自动转换为较大的类型。因此，如果我们想要32位计算，我们需要确保所有输入都是（最多）32位：
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ rand initializes the tensor elements to random numbers between 0 and 1.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ rand 将张量元素初始化为介于0和1之间的随机数。
- en: 3.6 The tensor API
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 张量 API
- en: At this point, we know what PyTorch tensors are and how they work under the
    hood. Before we wrap up, it is worth taking a look at the tensor operations that
    PyTorch offers. It would be of little use to list them all here. Instead, we’re
    going to get a general feel for the API and establish a few directions on where
    to find things in the online documentation at [http://pytorch.org/docs](http://pytorch.org/docs).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们知道 PyTorch 张量是什么，以及它们在幕后是如何工作的。在我们结束之前，值得看一看 PyTorch 提供的张量操作。在这里列出它们都没有太大用处。相反，我们将对
    API 有一个大致了解，并在在线文档 [http://pytorch.org/docs](http://pytorch.org/docs) 中确定一些查找内容的方向。
- en: First, the vast majority of operations on and between tensors are available
    in the `torch` module and can also be called as methods of a tensor object. For
    instance, the `transpose` function we encountered earlier can be used from the
    `torch` module
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，大多数张量上的操作都可以在 `torch` 模块中找到，并且也可以作为张量对象的方法调用。例如，我们之前遇到的 `transpose` 函数可以从
    `torch` 模块中使用
- en: '[PRE29]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'or as a method of the `a` tensor:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 或作为 `a` 张量的方法：
- en: '[PRE30]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: There is no difference between the two forms; they can be used interchangeably.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种形式之间没有区别；它们可以互换使用。
- en: '*We* mentioned the online docs earlier ([http://pytorch.org/docs](http://pytorch.org/docs)).
    They are exhaustive and well organized, with the tensor operations divided into
    groups:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们* 之前提到过在线文档 ([http://pytorch.org/docs](http://pytorch.org/docs))。它们非常详尽且组织良好，将张量操作分成了不同的组：'
- en: '*Creation ops* --Functions for constructing a tensor, like `ones` and `from_numpy`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*创建操作* --用于构建张量的函数，如 `ones` 和 `from_numpy`'
- en: '*Indexing, slicing, joining, mutating ops* --Functions for changing the shape,
    stride, or content of a tensor, like `transpose`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*索引、切片、连接、变异操作* --用于改变张量形状���步幅或内容的函数，如 `transpose`'
- en: '*Math ops* --Functions for manipulating the content of the tensor through computations'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*数学操作* --通过计算来操作张量内容的函数'
- en: '*Pointwise ops* --Functions for obtaining a new tensor by applying a function
    to each element independently, like `abs` and `cos`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*逐点操作* --通过独立地对每个元素应用函数来获取新张量的函数，如 `abs` 和 `cos`'
- en: '*Reduction ops* --Functions for computing aggregate values by iterating through
    tensors, like `mean`, `std`, and `norm`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缩减操作* --通过迭代张量计算聚合值的函数，如 `mean`、`std` 和 `norm`'
- en: '*Comparison ops* --Functions for evaluating numerical predicates over tensors,
    like `equal` and `max`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*比较操作* --用于在张量上评估数值谓词的函数，如 `equal` 和 `max`'
- en: '*Spectral ops* --Functions for transforming in and operating in the frequency
    domain, like `stft` and `hamming_window`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*频谱操作* --用于在频域中进行转换和操作的函数，如 `stft` 和 `hamming_window`'
- en: '*Other operations* --Special functions operating on vectors, like `cross`,
    or matrices, like `trace`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*其他操作* --在向量上操作的特殊函数，如 `cross`，或在矩阵上操作的函数，如 `trace`'
- en: '*BLAS and LAPACK operations* --Functions following the Basic Linear Algebra
    Subprograms (BLAS) specification for scalar, vector-vector, matrix-vector, and
    matrix-matrix operations'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BLAS和LAPACK操作* --遵循基本线性代数子程序（BLAS）规范的函数，用于标量、向量-向量、矩阵-向量和矩阵-矩阵操作'
- en: '*Random sampling* --Functions for generating values by drawing randomly from
    probability distributions, like `randn` and `normal`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机抽样* --通过从概率分布���随机抽取值生成值的函数，如`randn`和`normal`'
- en: '*Serialization* --Functions for saving and loading tensors, like `load` and
    `save`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*序列化* --用于保存和加载张量的函数，如`load`和`save`'
- en: '*Parallelism* --Functions for controlling the number of threads for parallel
    CPU execution, like `set_num_threads`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*并行性* --用于控制并行CPU执行线程数的函数，如`set_num_threads`'
- en: Take some time to play with the general tensor API. This chapter has provided
    all the prerequisites to enable this kind of interactive exploration. We will
    also encounter several of the tensor operations as we proceed with the book, starting
    in the next chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 花些时间玩玩通用张量API。本章提供了进行这种交互式探索所需的所有先决条件。随着我们继续阅读本书，我们还将遇到几个张量操作，从下一章开始。
- en: '3.7 Tensors: Scenic views of storage'
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 张量：存储的景观
- en: 'It is time for us to look a bit closer at the implementation under the hood.
    Values in tensors are allocated in contiguous chunks of memory managed by `torch.Storage`
    instances. A storage is a one-dimensional array of numerical data: that is, a
    contiguous block of memory containing numbers of a given type, such as `float`
    (32 bits representing a floating-point number) or `int64` (64 bits representing
    an integer). A PyTorch `Tensor` instance is a view of such a `Storage` instance
    that is capable of indexing into that storage using an offset and per-dimension
    strides.[⁵](#pgfId-1020404)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候更仔细地查看底层实现了。张量中的值是由`torch.Storage`实例管理的连续内存块分配的。存储是一个一维数值数据数组：即，包含给定类型数字的连续内存块，例如`float`（表示浮点数的32位）或`int64`（表示整数的64位）。PyTorch的`Tensor`实例是这样一个`Storage`实例的视图，能够使用偏移量和每维步长索引到该存储中。[⁵](#pgfId-1020404)
- en: '![](../Images/CH03_F04_Stevens2_GS.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F04_Stevens2_GS.png)'
- en: Figure 3.4 Tensors are views of a Storage instance.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 张量是`Storage`实例的视图。
- en: Multiple tensors can index the same storage even if they index into the data
    differently. We can see an example of this in figure 3.4\. In fact, when we requested
    `points[0]` in section 3.2, what we got back is another tensor that indexes the
    same storage as the `points` tensor--just not all of it, and with different dimensionality
    (1D versus 2D). The underlying memory is allocated only once, however, so creating
    alternate tensor-views of the data can be done quickly regardless of the size
    of the data managed by the `Storage` instance.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 即使多个张量以不同方式索引数据，它们可以索引相同的存储。我们可以在图3.4中看到这种情况。实际上，在我们在第3.2节请求`points[0]`时，我们得到的是另一个索引与`points`张量相同存储的张量--只是不是全部，并且具有不同的维度（1D与2D）。然而，底层内存只分配一次，因此可以快速创建数据的备用张量视图，而不管`Storage`实例管理的数据大小如何。
- en: 3.7.1 Indexing into storage
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.1 存储索引
- en: 'Let’s see how indexing into the storage works in practice with our 2D points.
    The storage for a given tensor is accessible using the `.storage` property:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在实践中使用我们的二维点进行存储索引。给定张量的存储可以通过`.storage`属性访问：
- en: '[PRE31]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Even though the tensor reports itself as having three rows and two columns,
    the storage under the hood is a contiguous array of size 6\. In this sense, the
    tensor just knows how to translate a pair of indices into a location in the storage.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管张量报告自身具有三行和两列，但底层存储是一个大小为6的连续数组。在这种意义上，张量只知道如何将一对索引转换为存储中的位置。
- en: 'We can also index into a storage manually. For instance:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以手动索引到存储中。例如：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can’t index a storage of a 2D tensor using two indices. The layout of a storage
    is always one-dimensional, regardless of the dimensionality of any and all tensors
    that might refer to it.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能使用两个索引索引二维张量的存储。存储的布局始终是一维的，而不管可能引用它的任何和所有张量的维度如何。
- en: 'At this point, it shouldn’t come as a surprise that changing the value of a
    storage leads to changing the content of its referring tensor:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，改变存储的值导致改变其引用张量的内容应该不会让人感到意外：
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '3.7.2 Modifying stored values: In-place operations'
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.2 修改存储的值：原地操作
- en: 'In addition to the operations on tensors introduced in the previous section,
    a small number of operations exist only as methods of the `Tensor` object. They
    are recognizable from a trailing underscore in their name, like `zero_`, which
    indicates that the method operates *in place* by modifying the input instead of
    creating a new output tensor and returning it. For instance, the `zero_` method
    zeros out all the elements of the input. Any method *without* the trailing underscore
    leaves the source tensor unchanged and instead returns a new tensor:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前一节介绍的张量操作外，还存在一小部分操作仅作为`Tensor`对象的方法存在。它们可以通过名称末尾的下划线识别，比如`zero_`，表示该方法通过修改输入来*原地*操作，而不是创建新的输出张量并返回它。例如，`zero_`方法将所有输入元素都置零。任何*没有*末尾下划线的方法都不会改变源张量，并且会返回一个新的张量：
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '3.8 Tensor metadata: Size, offset, and stride'
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.8 张量元数据：大小、偏移和步长
- en: 'In order to index into a storage, tensors rely on a few pieces of information
    that, together with their storage, unequivocally define them: size, offset, and
    stride. How these interact is shown in figure 3.5\. The size (or shape, in NumPy
    parlance) is a tuple indicating how many elements across each dimension the tensor
    represents. The storage offset is the index in the storage corresponding to the
    first element in the tensor. The stride is the number of elements in the storage
    that need to be skipped over to obtain the next element along each dimension.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了索引到存储中，张量依赖于一些信息，这些信息与它们的存储一起，明确定义它们：尺寸、偏移和步幅。它们的相互作用如图3.5所示。尺寸（或形状，在NumPy术语中）是一个元组，指示张量在每个维度上代表多少个元素。存储偏移是存储中对应于张量第一个元素的索引。步幅是在存储中需要跳过的元素数量，以获取沿每个维度的下一个元素。
- en: '![](../Images/CH03_F05_Stevens2_GS.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F05_Stevens2_GS.png)'
- en: Figure 3.5 Relationship between a tensor's offset, size, and stride. Here the
    tensor is a view of a larger storage, like one that might have been allocated
    when creating a larger tensor.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 张量的偏移、尺寸和步幅之间的关系。这里的张量是一个更大存储的视图，就像在创建更大的张量时可能分配的存储一样。
- en: 3.8.1 Views of another tensor’s storage
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.1 另一个张量存储的视图
- en: 'We can get the second point in the tensor by providing the corresponding index:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供相应的索引，我们可以获取张量中的第二个点：
- en: '[PRE35]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The resulting tensor has offset 2 in the storage (since we need to skip the
    first point, which has two items), and the size is an instance of the `Size` class
    containing one element, since the tensor is one-dimensional. It’s important to
    note that this is the same information contained in the `shape` property of tensor
    objects:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 结果张量在存储中的偏移为2（因为我们需要跳过第一个点，它有两个项目），尺寸是`Size`类的一个实例，包含一个元素，因为张量是一维的。重要的是要注意，这与张量对象的`shape`属性中包含的信息相同：
- en: '[PRE36]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The stride is a tuple indicating the number of elements in the storage that
    have to be skipped when the index is increased by 1 in each dimension. For instance,
    our `points` tensor has a stride of `(2, 1)`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 步幅是一个元组，指示当索引在每个维度上增加1时，必须跳过存储中的元素数量。例如，我们的`points`张量的步幅是`(2, 1)`：
- en: '[PRE37]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Accessing an element `i, j` in a 2D tensor results in accessing the `storage_offset
    + stride[0] * i + stride[1] * j` element in the storage. The offset will usually
    be zero; if this tensor is a view of a storage created to hold a larger tensor,
    the offset might be a positive value.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在2D张量中访问元素`i, j`会导致访问存储中的`storage_offset + stride[0] * i + stride[1] * j`元素。偏移通常为零；如果这个张量是一个查看存储的视图，该存储是为容纳更大的张量而创建的，则偏移可能是一个正值。
- en: This indirection between `Tensor` and `Storage` makes some operations inexpensive,
    like transposing a tensor or extracting a subtensor, because they do not lead
    to memory reallocations. Instead, they consist of allocating a new `Tensor` object
    with a different value for size, storage offset, or stride.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tensor`和`Storage`之间的这种间接关系使得一些操作变得廉价，比如转置张量或提取子张量，因为它们不会导致内存重新分配。相反，它们包括为尺寸、存储偏移或步幅分配一个具有不同值的新`Tensor`对象。'
- en: 'We already extracted a subtensor when we indexed a specific point and saw the
    storage offset increasing. Let’s see what happens to the size and stride as well:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们索引特定点并看到存储偏移增加时，我们已经提取了一个子张量。让我们看看尺寸和步幅会发生什么变化：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The bottom line is that the subtensor has one less dimension, as we would expect,
    while still indexing the same storage as the original `points` tensor. This also
    means changing the subtensor will have a side effect on the original tensor:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 底线是，子张量的维度少了一个，正如我们所期望的那样，同时仍然索引与原始`points`张量相同的存储。这也意味着改变子张量将对原始张量产生副作用：
- en: '[PRE39]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This might not always be desirable, so we can eventually clone the subtensor
    into a new tensor:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能并不总是理想的，所以我们最终可以将子张量克隆到一个新的张量中：
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 3.8.2 Transposing without copying
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.2 在不复制的情况下转置
- en: 'Let’s try transposing now. Let’s take our `points` tensor, which has individual
    points in the rows and *X* and *Y* coordinates in the columns, and turn it around
    so that individual points are in the columns. We take this opportunity to introduce
    the `t` function, a shorthand alternative to `transpose` for two-dimensional tensors:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝���转置。让我们拿出我们的`points`张量，其中行中有单独的点，列中有*X*和*Y*坐标，并将其转向，使单独的点在列中。我们借此机会介绍`t`函数，这是二维张量的`transpose`的简写替代品：
- en: '[PRE41]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '*Tip* To help build a solid understanding of the mechanics of tensors, it may
    be a good idea to grab a pencil and a piece of paper and scribble diagrams like
    the one in figure 3.5 as we step through the code in this section.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示* 为了帮助建立对张量机制的扎实理解，可能是一个好主意拿起一支铅笔和一张纸，像图3.5中的图一样在我们逐步执行本节代码时涂鸦图表。'
- en: We can easily verify that the two tensors share the same storage
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松验证这两个张量共享相同的存储
- en: '[PRE42]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'and that they differ only in shape and stride:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 它们只在形状和步幅上有所不同：
- en: '[PRE43]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This tells us that increasing the first index by one in `points`--for example,
    going from `points[0,0]` to `points[1,0]`--will skip along the storage by two
    elements, while increasing the second index--from `points[0,0]` to `points[0,1]`--will
    skip along the storage by one. In other words, the storage holds the elements
    in the tensor sequentially row by row.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，在`points`中将第一个索引增加1（例如，从`points[0,0]`到`points[1,0]`）将会跳过存储中的两个元素，而增加第二个索引（从`points[0,0]`到`points[0,1]`）将会跳过存储中的一个元素。换句话说，存储按行顺序顺序保存张量中的元素。
- en: 'We can transpose `points` into `points_t`, as shown in figure 3.6\. We change
    the order of the elements in the stride. After that, increasing the row (the first
    index of the tensor) will skip along the storage by one, just like when we were
    moving along columns in `points`. This is the very definition of transposing.
    No new memory is allocated: transposing is obtained only by creating a new `Tensor`
    instance with different stride ordering than the original.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`points`转置为`points_t`，如图 3.6 所示。我们改变了步幅中元素的顺序。之后，增加行（张量的第一个索引）将沿着存储跳过一个元素，就像我们在`points`中沿着列移动一样。这就是转置的定义。不会分配新的内存：转置只是通过创建一个具有不同步幅顺序的新`Tensor`实例来实现的。
- en: '![](../Images/CH03_F06_Stevens2_GS.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F06_Stevens2_GS.png)'
- en: Figure 3.6 Transpose operation applied to a tensor
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 张量的转置操作
- en: 3.8.3 Transposing in higher dimensions
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.3 高维度中的转置
- en: 'Transposing in PyTorch is not limited to matrices. We can transpose a multidimensional
    array by specifying the two dimensions along which transposing (flipping shape
    and stride) should occur:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，转置不仅限于矩阵。我们可以通过指定应该发生转置（翻转形状和步幅）的两个维度来转置多维数组：
- en: '[PRE44]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: A tensor whose values are laid out in the storage starting from the rightmost
    dimension onward (that is, moving along rows for a 2D tensor) is defined as `contiguous`.
    Contiguous tensors are convenient because we can visit them efficiently in order
    without jumping around in the storage (improving data locality improves performance
    because of the way memory access works on modern CPUs). This advantage of course
    depends on the way algorithms visit.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 从存储中右起维度开始排列数值（即，对于二维张量，沿着行移动）的张量被定义为`contiguous`。连续张量很方便，因为我们可以有效地按顺序访问它们，而不需要在存储中跳跃（改善数据局部性会提高性能，因为现代
    CPU 的内存访问方式）。当然，这种优势取决于算法的访问方式。
- en: 3.8.4 Contiguous tensors
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.4 连续张量
- en: Some tensor operations in PyTorch only work on contiguous tensors, such as `view`,
    which we’ll encounter in the next chapter. In that case, PyTorch will throw an
    informative exception and require us to call `contiguous` explicitly. It’s worth
    noting that calling `contiguous` will do nothing (and will not hurt performance)
    if the tensor is already contiguous.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 中的一些张量操作仅适用于连续张量，例如我们将在下一章中遇到的`view`。在这种情况下，PyTorch 将抛出一个信息性异常，并要求我们显式调用`contiguous`。值得注意的是，如果张量已经是连续的，则调用`contiguous`不会做任何事情（也不会影响性能）。
- en: 'In our case, `points` is contiguous, while its transpose is not:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，`points`是连续的，而其转置则不是：
- en: '[PRE45]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can obtain a new contiguous tensor from a non-contiguous one using the `contiguous`
    method. The content of the tensor will be the same, but the stride will change,
    as will the storage:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`contiguous`方法从非连续张量中获得一个新的连续张量。张量的内容将保持不变，但步幅和存储将发生变化：
- en: '[PRE46]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Notice that the storage has been reshuffled in order for elements to be laid
    out row-by-row in the new storage. The stride has been changed to reflect the
    new layout.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，存储已经重新排列，以便元素按行排列在新存储中。步幅已更改以反映新布局。
- en: As a refresher, figure 3.7 shows our diagram again. Hopefully it will all make
    sense now that we’ve taken a good look at how tensors are built.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 作为复习，图 3.7 再次显示了我们的图表。希望现在我们已经仔细研究了张量是如何构建的，一切都会变得清晰。
- en: '![](../Images/CH03_F07_Stevens2_GS.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F07_Stevens2_GS.png)'
- en: Figure 3.7 Relationship between a tensor's offset, size, and stride. Here the
    tensor is a view of a larger storage, like one that might have been allocated
    when creating a larger tensor.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 张量的偏移、大小和步幅之间的关系。这里的张量是一个更大存储的视图，就像在创建更大的张量时可能分配的存储一样。
- en: 3.9 Moving tensors to the GPU
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.9 将张量移动到 GPU
- en: 'So far in this chapter, when we’ve talked about storage, we’ve meant memory
    on the CPU. PyTorch tensors also can be stored on a different kind of processor:
    a graphics processing unit (GPU). Every PyTorch tensor can be transferred to (one
    of) the GPU(s) in order to perform massively parallel, fast computations. All
    operations that will be performed on the tensor will be carried out using GPU-specific
    routines that come with PyTorch.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，当我们谈论存储时，我们指的是 CPU 上的内存。PyTorch 张量也可以存储在不同类型的处理器上：图形处理单元（GPU）。每个
    PyTorch 张量都可以传输到 GPU 中的一个（或多个）以执行高度并行、快速的计算。将在张量上执行的所有操作都将使用 PyTorch 提供的 GPU 特定例程执行。
- en: PyTorch support for various GPUs
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 对各种 GPU 的支持
- en: 'As of mid-2019, the main PyTorch releases only have acceleration on GPUs that
    have support for CUDA. PyTorch can run on AMD’s ROCm ([https://rocm.github.io](https://rocm.github.io)),
    and the master repository provides support, but so far, you need to compile it
    yourself. (Before the regular build process, you need to run `tools/amd_build/build_amd.py`
    to translate the GPU code.) Support for Google’s tensor processing units (TPUs)
    is a work in progress ([https://github.com/pytorch/xla](https://github.com/pytorch/xla)[),
    with the current proof of concept available to the public in Google Colab: https://colab.research.google.com.
    Implementation of data structures and kernels on other GPU technologies, such
    as OpenCL, are not planned at the time of this writing.](https://colab.research.google.com)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 2019 年中期，主要的 PyTorch 发行版只在支持 CUDA 的 GPU 上有加速。PyTorch 可以在 AMD 的 ROCm 上运行（[https://rocm.github.io](https://rocm.github.io)），主存储库提供支持，但到目前为止，您需要自行编译它。（在常规构建过程之前，您需要运行`tools/amd_build/build_amd.py`来转换
    GPU 代码。）对 Google 的张量处理单元（TPU）的支持正在进行中（[https://github.com/pytorch/xla](https://github.com/pytorch/xla)），当前的概念验证可在
    Google Colab 上公开访问：https://colab.research.google.com。在撰写本文时，不计划在其他 GPU 技术（如 OpenCL）上实现数据结构和内核。](https://colab.research.google.com)
- en: 3.9.1 Managing a tensor’s device attribute
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.9.1 管理张量的设备属性
- en: 'In addition to `dtype`, a PyTorch `Tensor` also has the notion of `device`,
    which is where on the computer the tensor data is placed. Here is how we can create
    a tensor on the GPU by specifying the corresponding argument to the constructor:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`dtype`，PyTorch 的`Tensor`还有`device`的概念，即张量数据所放置的计算机位置。以下是我们如何通过为构造函数指定相应参数来在
    GPU 上创建张量的方法：
- en: '[PRE47]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We could instead copy a tensor created on the CPU onto the GPU using the `to`
    method:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`to`方法将在CPU上创建的张量复制到GPU上：
- en: '[PRE48]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Doing so returns a new tensor that has the same numerical data, but stored in
    the RAM of the GPU, rather than in regular system RAM. Now that the data is stored
    locally on the GPU, we’ll start to see the speedups mentioned earlier when performing
    mathematical operations on the tensor. In almost all cases, CPU- and GPU-based
    tensors expose the same user-facing API, making it much easier to write code that
    is agnostic to where, exactly, the heavy number crunching is running.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做会返回一个新的张量，其中包含相同的数值数据，但存储在GPU的RAM中，而不是常规系统RAM中。现在数据存储在GPU上，当对张量执行数学运算时，我们将开始看到之前提到的加速效果。在几乎所有情况下，基于CPU和GPU的张量都暴露相同的用户接口，这样编写代码就更容易，不用关心重要的数值计算到底在哪里运行。
- en: If our machine has more than one GPU, we can also decide on which GPU we allocate
    the tensor by passing a zero-based integer identifying the GPU on the machine,
    such as
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的机器有多个GPU，我们还可以通过传递一个从零开始的整数来决定将张量分配到哪个GPU上，例如
- en: '[PRE49]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'At this point, any operation performed on the tensor, such as multiplying all
    elements by a constant, is carried out on the GPU:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，对张量执行的任何操作，例如将所有元素乘以一个常数，都是在GPU上执行的：
- en: '[PRE50]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: ❶ Multiplication performed on the CPU
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在CPU上执行的乘法
- en: ❷ Multiplication performed on the GPU
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在GPU上执行的乘法
- en: 'Note that the `points_gpu` tensor is not brought back to the CPU once the result
    has been computed. Here’s what happened in this line:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，`points_gpu`张量在计算结果后并没有返回到CPU。这是这一行中发生的事情： '
- en: The `points` tensor is copied to the GPU.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`points`张量被复制到GPU上。'
- en: A new tensor is allocated on the GPU and used to store the result of the multiplication.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GPU上分配一个新的张量，并用于存储乘法的结果。
- en: A handle to that GPU tensor is returned.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回一个指向该GPU张量的句柄。
- en: Therefore, if we also add a constant to the result
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们还向结果添加一个常数
- en: '[PRE51]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: the addition is still performed on the GPU, and no information flows to the
    CPU (unless we print or access the resulting tensor). In order to move the tensor
    back to the CPU, we need to provide a `cpu` argument to the `to` method, such
    as
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 加法仍然在GPU上执行，没有信息流向CPU（除非我们打印或访问生成的张量）。为了将张量移回CPU，我们需要在`to`方法中提供一个`cpu`参数，例如
- en: '[PRE52]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We can also use the shorthand methods `cpu` and `cuda` instead of the `to`
    method to achieve the same goal:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`cpu`和`cuda`的简写方法，而不是`to`方法来实现相同的目标：
- en: '[PRE53]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: ❶ Defaults to GPU index 0
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 默认为GPU索引0
- en: It’s also worth mentioning that by using the `to` method, we can change the
    placement and the data type simultaneously by providing both `device` and `dtype`
    as arguments.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得一提的是，通过使用`to`方法，我们可以通过同时提供`device`和`dtype`作为参数来同时更改位置和数据类型。
- en: 3.10 NumPy interoperability
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.10 NumPy 互操作性
- en: We’ve mentioned NumPy here and there. While we do not consider NumPy a prerequisite
    for reading this book, we strongly encourage you to become familiar with NumPy
    due to its ubiquity in the Python data science ecosystem. PyTorch tensors can
    be converted to NumPy arrays and vice versa very efficiently. By doing so, we
    can take advantage of the huge swath of functionality in the wider Python ecosystem
    that has built up around the NumPy array type. This zero-copy interoperability
    with NumPy arrays is due to the storage system working with the Python buffer
    protocol ([https://docs.python.org/3/c-api/buffer.html](https://docs.python.org/3/c-api/buffer.html)).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里和那里提到了NumPy。虽然我们不认为NumPy是阅读本书的先决条件，但我们强烈建议您熟悉NumPy，因为它在Python数据科学生态系统中无处不在。PyTorch张量可以与NumPy数组之间进行非常高效的转换。通过这样做，我们可以利用围绕NumPy数组类型构建起来的Python生态系统中的大量功能。这种与NumPy数组的零拷贝互操作性归功于存储系统与Python缓冲区协议的工作（[https://docs.python.org/3/c-api/buffer.html](https://docs.python.org/3/c-api/buffer.html)）。
- en: To get a NumPy array out of our `points` tensor, we just call
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要从我们的`points`张量中获取一个NumPy数组，我们只需调用
- en: '[PRE54]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: which will return a NumPy multidimensional array of the right size, shape, and
    numerical type. Interestingly, the returned array shares the same underlying buffer
    with the tensor storage. This means the `numpy` method can be effectively executed
    at basically no cost, as long as the data sits in CPU RAM. It also means modifying
    the NumPy array will lead to a change in the originating tensor. If the tensor
    is allocated on the GPU, PyTorch will make a copy of the content of the tensor
    into a NumPy array allocated on the CPU.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个正确大小、形状和数值类型的NumPy多维数组。有趣的是，返回的数组与张量存储共享相同的底层缓冲区。这意味着`numpy`方法可以在基本上不花费任何成本地执行，只要数据位于CPU
    RAM中。这也意味着修改NumPy数组将导致源张量的更改。如果张量分配在GPU上，PyTorch将把张量内容复制到在CPU上分配的NumPy数组中。
- en: Conversely, we can obtain a PyTorch tensor from a NumPy array this way
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以通过以下方式从NumPy数组获得一个PyTorch张量
- en: '[PRE55]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: which will use the same buffer-sharing strategy we just described.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用我们刚刚描述的相同的缓冲区共享策略。
- en: '*Note* While the default numeric type in PyTorch is 32-bit floating-point,
    for NumPy it is 64-bit. As discussed in section 3.5.2, we usually want to use
    32-bit floating-points, so we need to make sure we have tensors of `dtype torch
    .float` after converting.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* PyTorch中的默认数值类型是32位浮点数，而NumPy中是64位。正如在第3.5.2节中讨论的那样，我们通常希望使用32位浮点数，因此在转换后，我们需要确保具有`dtype
    torch .float`的张量。'
- en: 3.11 Generalized tensors are tensors, too
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.11 广义张量也是张量
- en: 'For the purposes of this book, and for the vast majority of applications in
    general, tensors are multidimensional arrays, just as we’ve seen in this chapter.
    If we risk a peek under the hood of PyTorch, there is a twist: how the data is
    stored under the hood is separate from the tensor API we discussed in section
    3.6\. Any implementation that meets the contract of that API can be considered
    a tensor!'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书的目的，以及一般大多数应用程序，张量都是多维数组，就像我们在本章中看到的那样。如果我们冒险窥探PyTorch的内部，会有一个转折：底层数据存储方式与我们在第3.6节讨论的张量API是分开的。只要满足该API的约定，任何实现都可以被视为张量！
- en: 'PyTorch will cause the right computation functions to be called regardless
    of whether our tensor is on the CPU or the GPU. This is accomplished through a
    *dispatching* mechanism, and that mechanism can cater to other tensor types by
    hooking up the user-facing API to the right backend functions. Sure enough, there
    are other kinds of tensors: some are specific to certain classes of hardware devices
    (like Google TPUs), and others have data-representation strategies that differ
    from the dense array style we’ve seen so far. For example, sparse tensors store
    only nonzero entries, along with index information. The PyTorch dispatcher on
    the left in figure 3.8 is designed to be extensible; the subsequent switching
    done to accommodate the various numeric types of figure 3.8 shown on the right
    is a fixed aspect of the implementation coded into each backend.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch将调用正确的计算函数，无论我们的张量是在CPU还是GPU上。这是通过*调度*机制实现的，该机制可以通过将用户界面API连接到正确的后端函数来满足其他张量类型的需求。确实，还有其他种类的张量：有些特定于某些类别的硬件设备（如Google
    TPU���，而其他的数据表示策略与我们迄今所见的稠密数组风格不同。例如，稀疏张量仅存储非零条目，以及索引信息。图3.8左侧的PyTorch调度程序被设计为可扩展的；图3.8右侧所示的用于适应各种数字类型的后续切换是实现的固定方面，编码到每个后端中。
- en: '![](../Images/CH03_F08_Stevens2_GS.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F08_Stevens2_GS.png)'
- en: Figure 3.8 The dispatcher in PyTorch is one of its key infrastructure bits.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 PyTorch中的调度程序是其关键基础设施之一。
- en: We will meet *quantized* tensors in chapter 15, which are implemented as another
    type of tensor with a specialized computational backend. Sometimes the usual tensors
    we use are called *dense* or *strided* to differentiate them from tensors using
    other memory layouts.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第15章遇到*量化*张量，它们作为另一种具有专门计算后端的张量类型实现。有时，我们使用的通常张量被称为*稠密*或*分步*，以区别于使用其他内存布局的张量。
- en: As with many things, the number of kinds of tensors has grown as PyTorch supports
    a broader range of hardware and applications. We can expect new kinds to continue
    to arise as people explore new ways to express and perform computations with PyTorch.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多事物一样，随着PyTorch支持更广泛的硬件和应用程序范围，张量种类的数量也在增加。我们可以期待随着人们探索用PyTorch表达和执行计算的新方法，新的种类将继续出现。
- en: 3.12 Serializing tensors
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.12 序列化张量
- en: 'Creating a tensor on the fly is all well and good, but if the data inside is
    valuable, we will want to save it to a file and load it back at some point. After
    all, we don’t want to have to retrain a model from scratch every time we start
    running our program! PyTorch uses `pickle` under the hood to serialize the tensor
    object, plus dedicated serialization code for the storage. Here’s how we can save
    our `points` tensor to an ourpoints.t file:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要的时候，即使现场创建张量也很好，但如果其中的数据很有价值，我们会希望将其保存到文件中，并在某个时候加载回来。毕竟，我们不想每次运行程序时都从头开始重新训练模型！PyTorch在底层使用`pickle`来序列化张量对象，还有专门的存储序列化代码。这里是如何将我们的`points`张量保存到一个ourpoints.t文件中的方法：
- en: '[PRE56]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'As an alternative, we can pass a file descriptor in lieu of the filename:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 作为替代方案，我们可以传递文件描述符而不是文件名：
- en: '[PRE57]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Loading our points back is similarly a one-liner
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，加载我们的点也是一行代码
- en: '[PRE58]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: or, equivalently,
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，等效地，
- en: '[PRE59]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'While we can quickly save tensors this way if we only want to load them with
    PyTorch, the file format itself is not interoperable: we can’t read the tensor
    with software other than PyTorch. Depending on the use case, this may or may not
    be a limitation, but we should learn how to save tensors interoperably for those
    times when it is. We’ll look next at how to do so.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以快速以这种方式保存张量，如果我们只想用PyTorch加载它们，但文件格式本身不具有互操作性：我们无法使用除PyTorch之外的软件读取张量。根据使用情况，这可能是一个限制，也可能不是，但我们应该学会如何在需要时以互操作的方式保存张量。接下来我们将看看如何做到这一点。
- en: 3.12.1 Serializing to HDF5 with h5py
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.12.1 使用h5py序列化到HDF5
- en: Every use case is unique, but we suspect needing to save tensors interoperably
    will be more common when introducing PyTorch into existing systems that already
    rely on different libraries. New projects probably won’t need to do this as often.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 每种用例都是独特的，但我们怀疑在将PyTorch引入已经依赖不同库的现有系统时，需要以互操作方式保存张量将更常见。新项目可能不需要这样做那么频繁。
- en: For those cases when you need to, however, you can use the HDF5 format and library
    ([www.hdfgroup.org/solutions/hdf5](https://www.hdfgroup.org/solutions/hdf5)).
    HDF5 is a portable, widely supported format for representing serialized multidimensional
    arrays, organized in a nested key-value dictionary. Python supports HDF5 through
    the `h5py` library ([www.h5py.org](http://www.h5py.org/)), which accepts and returns
    data in the form of NumPy arrays.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在需要时，您可以使用HDF5格式和库（[www.hdfgroup.org/solutions/hdf5](https://www.hdfgroup.org/solutions/hdf5)）。HDF5是一种便携式、广泛支持的格式，用于表示序列化的多维数组，以嵌套的键值字典组织。Python通过`h5py`库（[www.h5py.org](http://www.h5py.org/)）支持HDF5，该库接受并返回NumPy数组形式的数据。
- en: We can install `h5py` using
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令安装`h5py`
- en: '[PRE60]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'At this point, we can save our `points` tensor by converting it to a NumPy
    array (at no cost, as we noted earlier) and passing it to the `create_dataset`
    function:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以通过将其转换为NumPy数组（如前所述，没有成本）并将其传递给`create_dataset`函数来保存我们的`points`张量：
- en: '[PRE61]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Here `''coords''` is a key into the HDF5 file. We can have other keys--even
    nested ones. One of the interesting things in HDF5 is that we can index the dataset
    while on disk and access only the elements we’re interested in. Let’s suppose
    we want to load just the last two points in our dataset:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`'coords'`是HDF5文件中的一个键。我们可以有其他键--甚至是嵌套的键。在HDF5中的一个有趣之处是，我们可以在磁盘上索引数据集，并且只访问我们感兴趣的元素。假设我们只想加载数据集中的最后两个点：
- en: '[PRE62]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The data is not loaded when the file is opened or the dataset is required. Rather,
    the data stays on disk until we request the second and last rows in the dataset.
    At that point, `h5py` accesses those two columns and returns a NumPy array-like
    object encapsulating that region in that dataset that behaves like a NumPy array
    and has the same API.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 当打开文件或需要数据集时，数据不会被加载。相反，数据会保留在磁盘上，直到我们请求数据集中的第二行和最后一行。在那时，`h5py`访问这两列并返回一个类似NumPy数组的对象，封装了数据集中的那个区域，行为类似NumPy数组，并具有相同的API。
- en: 'Owing to this fact, we can pass the returned object to the `torch.from_numpy`
    function to obtain a tensor directly. Note that in this case, the data is copied
    over to the tensor’s storage:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个事实，我们可以将返回的对象传递给`torch.from_numpy`函数，直接获得一个张量。请注意，在这种情况下，数据被复制到张量的存储中：
- en: '[PRE63]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Once we’re finished loading data, we close the file. Closing the HDFS file invalidates
    the datasets, and trying to access `dset` afterward will give an exception. As
    long as we stick to the order shown here, we are fine and can now work with the
    `last_points` tensor.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据完成后，我们关闭文件。关闭HDFS文件会使数据集无效，尝试在此之后访问`dset`将导致异常。只要我们按照这里显示的顺序进行操作，我们就可以正常工作并现在可以使用`last_points`张量。
- en: 3.13 Conclusion
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.13 结论
- en: Now we have covered everything we need to get started with representing everything
    in floats. We’ll cover other aspects of tensors--such as creating views of tensors;
    indexing tensors with other tensors; and broadcasting, which simplifies performing
    element-wise operations between tensors of different sizes or shapes--as needed
    along the way.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了我们需要开始用浮点数表示一切的一切。我们将根据需要涵盖张量的其他方面--例如创建张量的视图；使用其他张量对张量进行索引；以及广播，简化了在不同大小或形状的张量之间执行逐元素操作的操作--。
- en: In chapter 4, we will learn how to represent real-world data in PyTorch. We
    will start with simple tabular data and move on to something more elaborate. In
    the process, we will get to know more about tensors.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们将学习如何在PyTorch中表示现实世界的数据。我们将从简单的表格数据开始，然后转向更复杂的内容。在这个过程中，我们将更多地了解张量。
- en: 3.14 Exercises
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.14 练习
- en: Create a tensor `a` from `list(range(9))`. Predict and then check the size,
    offset, and stride.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`list(range(9))`创建一个张量`a`。预测并检查大小、偏移和步长。
- en: Create a new tensor using `b = a.view(3, 3)`. What does `view` do? Check that
    `a` and `b` share the same storage.
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`b = a.view(3, 3)`创建一个新的张量。`view`函数的作用是什么？检查`a`和`b`是否共享相同的存储。
- en: Create a tensor `c = b[1:,1:]`. Predict and then check the size, offset, and
    stride.
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个张量`c = b[1:,1:]`。预测并检查大小、偏移和步长。
- en: Pick a mathematical operation like cosine or square root. Can you find a corresponding
    function in the `torch` library?
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个数学运算，如余弦或平方根。你能在`torch`库中找到相应的函数吗？
- en: Apply the function element-wise to `a`. Why does it return an error?
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`a`逐元素应用函数。为什么会返回错误？
- en: What operation is required to make the function work?
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使函数工作需要什么操作？
- en: Is there a version of your function that operates in place?
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否有一个在原地操作的函数版本？
- en: 3.15 Summary
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.15 总结
- en: Neural networks transform floating-point representations into other floating-point
    representations. The starting and ending representations are typically human interpretable,
    but the intermediate representations are less so.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络将浮点表示转换为其他浮点表示。起始和结束表示通常是人类可解释的，但中间表示则不太容易理解。
- en: These floating-point representations are stored in tensors.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些浮点表示存储在张量中。
- en: Tensors are multidimensional arrays; they are the basic data structure in PyTorch.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量是多维数组；它们是PyTorch中的基本数据结构。
- en: PyTorch has a comprehensive standard library for tensor creation, manipulation,
    and mathematical operations.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch拥有一个全面的标准库，用于张量的创建、操作和数学运算。
- en: Tensors can be serialized to disk and loaded back.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量可以序列化到磁盘并重新加载。
- en: All tensor operations in PyTorch can execute on the CPU as well as on the GPU,
    with no change in the code.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch中的所有张量操作都可以在CPU和GPU上执行，而不需要更改代码。
- en: PyTorch uses a trailing underscore to indicate that a function operates in place
    on a tensor (for example, `Tensor.sqrt_`).
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch使用尾随下划线来表示一个函数在张量上的原地操作（例如，`Tensor.sqrt_`）。
- en: '* * *'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.)As perception is not trivial to norm, people have come up with many weights.
    For example, see [https://en.wikipedia.org/wiki/Luma_(video)](https://en.wikipedia.org/wiki/Luma_(video)).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)由于感知不是一个简单的规范，人们提出了许多权重。例如，参见[https://en.wikipedia.org/wiki/Luma_(video)](https://en.wikipedia.org/wiki/Luma_(video))。
- en: ^(2.)Tim Rocktäschel’s blog post “Einsum is All You Need--Einstein Summation
    in Deep Learning” ([https:// rockt.github.io/2018/04/30/einsum](https://rockt.github.io/2018/04/30/einsum))
    gives a good overview.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.)Tim Rocktäschel的博文“Einsum is All You Need--Einstein Summation in Deep Learning”（[https://
    rockt.github.io/2018/04/30/einsum](https://rockt.github.io/2018/04/30/einsum)）提供了很好的概述。
- en: ^(3.)See Sasha Rush, “Tensor Considered Harmful,” Harvardnlp, [http://nlp.seas.harvard.edu/NamedTensor](http://nlp.seas.harvard.edu/NamedTensor).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ^(3.)参见Sasha Rush的博文“Tensor Considered Harmful”，Harvardnlp，[http://nlp.seas.harvard.edu/NamedTensor](http://nlp.seas.harvard.edu/NamedTensor)。
- en: ^(4.)And signed-ness, in the case of `uint8`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ^(4.)以及在`uint8`的情况下的符号。
- en: ^(5.)`Storage` may not be directly accessible in future PyTorch releases, but
    what we show here still provides a good mental picture of how tensors work under
    the hood.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ^(5.)在未来的PyTorch版本中，`Storage`可能无法直接访问，但我们在这里展示的内容仍然提供了张量在内部工作方式的良好思维图。
