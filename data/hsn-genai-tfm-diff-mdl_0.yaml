- en: Chapter 1\. Diffusion Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。扩散模型
- en: In late 2020 a little-known class of models called diffusion models began causing
    a stir in the machine-learning world. Researchers figured out how to use these
    models to generate synthetic images at higher quality than any produced by previous
    techniques. A flurry of papers followed, proposing improvements and modifications
    that pushed the quality up even further. By late 2021 there were models like GLIDE
    that showcased incredible results on text-to-image tasks, and a few months later,
    these models had entered the mainstream with tools like DALL-E 2 and Stable Diffusion.
    These models made it easy for anyone to generate images just by typing in a text
    description of what they wanted to see.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年末，一个名为扩散模型的鲜为人知的模型类别开始在机器学习领域引起轰动。研究人员找出了如何使用这些模型生成比以前技术产生的合成图像质量更高的图像。随后出现了一系列论文，提出了改进和修改，进一步提高了质量。到2021年底，出现了像GLIDE这样的模型，展示了在文本到图像任务上令人难以置信的结果，几个月后，这些模型已经进入了主流，如DALL-E
    2和Stable Diffusion等工具，使任何人都可以通过输入所需看到的文本描述来生成图像。
- en: In this chapter, we’re going to dig into the details of how these models work.
    We’ll outline the key insights that make them so powerful, generate images with
    existing models to get a feel for how they work, and then train our own models
    to deepen this understanding further. The field is still rapidly evolving, but
    the topics covered here should give you a solid foundation to build on. Chapter
    5 will explore more advanced techniques through the lens of a model called Stable
    Diffusion, and chapter 6 will explore applications of these techniques beyond
    simple image generation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入了解这些模型的工作原理。我们将概述使它们如此强大的关键见解，使用现有模型生成图像，以了解它们的工作方式，然后训练我们自己的模型，以进一步加深对此的理解。该领域仍在快速发展，但本章涵盖的主题应该为您打下坚实的基础。第5章将通过一个名为Stable
    Diffusion的模型，探索更高级的技术，第6章将探讨这些技术在简单图像生成之外的应用。
- en: 'The Key Insight: Iterative Refinement'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键见解：迭代细化
- en: So what is it that makes diffusion models so powerful? Previous techniques,
    such as VAEs or GANs, generate their final output via a single forward pass of
    the model. This means the model must get everything right on the first try. If
    it makes a mistake, it can’t go back and fix it. Diffusion models, on the other
    hand, generate their output by iterating over many steps. This ‘iterative refinement’
    allows the model to correct mistakes made in previous steps and gradually improve
    the output. To illustrate this, let’s look at an example of a diffusion model
    in action.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是什么使扩散模型如此强大呢？先前的技术，如VAEs或GANs，通过模型的单次前向传递生成最终输出。这意味着模型必须在第一次尝试时就做到一切正确。如果它犯了一个错误，就无法返回并修复它。另一方面，扩散模型通过迭代多个步骤生成其输出。这种“迭代细化”允许模型纠正之前步骤中的错误，并逐渐改进输出。为了说明这一点，让我们看一个扩散模型的示例。
- en: 'We can load a pre-trained model using the Hugging Face diffusers library. The
    pipeline can be used to create images directly, but this doesn’t show us what
    is going on under the hood:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Hugging Face扩散器库加载预训练模型。该管道可用于直接创建图像，但这并不能显示出底层发生了什么：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![image](assets/cell-4-output-3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-4-output-3.png)'
- en: We can re-create the sampling process step by step to get a better look at what
    is happening as the model generates images. We initialize our sample x with random
    noise and then run it through the model for 30 steps. On the right, you can see
    the model’s prediction for what the final image will look like at specific steps
    - note that the initial predictions are not particularly good! Instead of jumping
    right to that final predicted image, we only modify x by a small amount in the
    direction of the prediction (shown on the left). We then feed this new, slightly
    better x through the model again for the next step, hopefully resulting in a slightly
    improved prediction, which can be used to update x a little more, and so on. With
    enough steps, the model can produce some impressively realistic images.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以逐步重新创建采样过程，以更好地了解模型生成图像时发生了什么。我们使用随机噪声初始化我们的样本x，然后通过模型运行30步。在右侧，您可以看到模型对特定步骤的最终图像预测-请注意，最初的预测并不特别好！我们不是直接跳到最终预测的图像，而是只在预测的方向上稍微修改x（显示在左侧）。然后，我们再次将这个新的、稍微更好的x通过模型进行下一步的处理，希望能产生稍微改进的预测，这可以用来进一步更新x，依此类推。经过足够的步骤，模型可以生成一些令人印象深刻的逼真图像。
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![image](assets/cell-5-output-1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-5-output-1.png)'
- en: '![image](assets/cell-5-output-2.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-5-output-2.png)'
- en: '![image](assets/cell-5-output-3.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-5-output-3.png)'
- en: '![image](assets/cell-5-output-4.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-5-output-4.png)'
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Don’t worry if that code looks a bit intimidating - we’ll explain how this all
    works over the course of this chapter. For now, just focus on the results.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果那段代码看起来有点吓人-我们将在本章中解释这一切是如何工作的。现在，只需专注于结果。
- en: This core idea of learning how to refine a ‘corrupted’ input gradually can be
    applied to a wide range of tasks. In this chapter, we’ll focus on unconditional
    image generation - that is, generating images that resemble the training data,
    with no additional controls over what these generated samples look like. Diffusion
    models have also been applied to audio, video, text and more. And while most implementations
    use some variant of the ‘denoising’ approach that we’ll cover here, new approaches
    utilizing different types of ‘corruption’ together with iterative refinement are
    emerging that may move the field beyond the current focus on denoising diffusion
    specifically. Exciting times!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何逐渐改进“损坏”的输入的核心思想可以应用于广泛的任务范围。在本章中，我们将专注于无条件图像生成-也就是说，生成类似训练数据的图像，而不对这些生成的样本的外观进行额外的控制。扩散模型也已应用于音频、视频、文本等。虽然大多数实现使用我们将在这里介绍的某种“去噪”方法的变体，但正在出现利用不同类型的“损坏”以及迭代细化的新方法，这可能会使该领域超越目前专注于去噪扩散的焦点。令人兴奋的时刻！
- en: Training a Diffusion Model
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练扩散模型
- en: 'In this section, we’re going to train a diffusion model from scratch to gain
    a better understanding of how they work. We’ll start by using components from
    the Hugging Face diffusers library. As the chapter progresses, we’ll gradually
    demystify how each component works. Training a diffusion model is relatively straightforward
    compared to other types of generative models. We repeatedly:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从头开始训练一个扩散模型，以更好地了解它们的工作原理。我们将首先使用Hugging Face diffusers库中的组件。随着本章的进行，我们将逐渐揭开每个组件的工作原理。与其他类型的生成模型相比，训练扩散模型相对简单。我们反复进行：
- en: Load in some images from the training data.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从训练数据中加载一些图像。
- en: Add noise in different amounts. Remember, we want the model to do a good job
    estimating how to ‘fix’ (denoise) both extremely noisy images and images that
    are close to perfect.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以不同的量添加噪音。记住，我们希望模型能够很好地估计如何“修复”（去噪）极其嘈杂的图像和接近完美的图像。
- en: Feed the noisy versions of the inputs into the model.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入的嘈杂版本馈送到模型中。
- en: Evaluate how well the model does at denoising these inputs.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型在去噪这些输入时的表现。
- en: Use this information to update the model weights.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这些信息来更新模型权重。
- en: To generate new images with a trained model, we begin with a completely random
    input and repeatedly feed it through the model, updating the input on each iteration
    by a small amount based on the model prediction. As we’ll see, there are a number
    of sampling methods that try to streamline this process so that we can generate
    good images with as few steps as possible.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用经过训练的模型生成新图像，我们从完全随机的输入开始，并通过模型重复地将其馈送到模型中，在每次迭代中根据模型预测更新输入的小量。正如我们将看到的，有许多采样方法试图简化这个过程，以便我们可以尽可能少的步骤生成好的图像。
- en: The Data
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: For this example, we’ll use a dataset of images from the Hugging Face Hub- specifically,
    [this collection of 1000 butterfly pictures](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset).
    Later on, in the projects section, you will see how to use your own data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用来自Hugging Face Hub的图像数据集-具体来说，[这个包含1000张蝴蝶图片的集合](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset)。在项目部分，您将看到如何使用自己的数据。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We need to do some preparation before this data can be used to train a model.
    Images are typically represented as a grid of ‘pixels’, with color values between
    0 and 255 for each of the three color channels (Red, Green and Blue). To process
    these and make them ready for training, we: - Resize them to a fixed size - (Optional)
    Add some augmentation by randomly flipping them horizontally, effectively doubling
    the size of our dataset - Convert them to a PyTorch tensor (which represents the
    color values as floats between 0 and 1) - Normalize them to have a mean of 0,
    with values between -1 and 1'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这些数据训练模型之前，我们需要做一些准备。图像通常表示为一个“像素”网格，每个像素有三个颜色通道（红色、绿色和蓝色）的颜色值在0到255之间。为了处理这些图像并使它们准备好进行训练，我们需要：
    - 将它们调整为固定大小 - （可选）通过随机水平翻转来添加一些增强，有效地使我们的数据集大小加倍 - 将它们转换为PyTorch张量（表示颜色值为0到1之间的浮点数）
    - 将它们标准化为具有均值为0的值，值在-1到1之间
- en: 'We can do all of this with `torchvision.transforms`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`torchvision.transforms`来完成所有这些操作：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we need to create a dataloader to load the data in batches with these
    transforms applied:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个数据加载器，以便加载应用了这些转换的数据批次：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can check that this worked by loading a single batch and inspecting the images.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过加载单个批次并检查图像来检查这是否有效。
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![image](assets/cell-9-output-2.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-9-output-2.png)'
- en: Adding Noise
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加噪音
- en: 'How do we gradually corrupt our data? The most common approach is to add noise
    to the images. The amount of noise we add is controlled by a noise schedule. Different
    papers and approaches tackle this in different ways, which we’ll explore later
    in the chapter. For now, let’s see one common approach in action based on the
    paper [“Denoising diffusion probabilistic models”](https://arxiv.org/abs/2006.11239)
    by Ho et al. In the diffusers library, adding noise is handled by something called
    a scheduler, which takes in a batch of images and a list of ‘timesteps’ and determines
    how to create the noisy versions of those images:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何逐渐破坏我们的数据？最常见的方法是向图像添加噪音。我们添加的噪音量由噪音时间表控制。不同的论文和方法以不同的方式处理这个问题，我们将在本章后面进行探讨。现在，让我们看看一种常见的方法，基于Ho等人的论文“去噪扩散概率模型”。在扩散器库中，添加噪音是由称为调度器的东西处理的，它接收一批图像和一个“时间步长”列表，并确定如何创建这些图像的嘈杂版本：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![image](assets/cell-10-output-1.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-10-output-1.png)'
- en: During training, we’ll pick the timesteps at random. The scheduler takes some
    parameters (beta_start and beta_end) which it uses to determine how much noise
    should be present for a given timestep. We will cover schedulers in more detail
    in section X.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们将随机选择时间步长。调度器接收一些参数（beta_start和beta_end），它用这些参数来确定给定时间步长应该存在多少噪音。我们将在第X节中更详细地介绍调度器。
- en: The UNet
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UNet
- en: UNet is a convolutional neural network invented for tasks such as image segmentation,
    where the desired output has the same spatial extent as the input. It consists
    of a series of ‘downsampling’ layers that reduce the spatial size of the input,
    followed by a series of ‘upsampling’ layers that increase the spatial extent of
    the input again. The downsampling layers are also typically followed by a ‘skip
    connection’ that connects the downsampling layer’s output to the upsampling layer’s
    input. This allows the upsampling layers to ‘see’ the higher-resolution representations
    from earlier in the network, which is useful for tasks with image-like outputs
    where this high-resolution information is especially useful.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: UNet是一种卷积神经网络，用于诸如图像分割之类的任务，其中期望的输出与输入具有相同的空间范围。它由一系列“下采样”层组成，用于减小输入的空间大小，然后是一系列“上采样”层，用于再次增加输入的空间范围。下采样层通常也后面跟着一个“跳跃连接”，将下采样层的输出连接到上采样层的输入。这允许上采样层“看到”网络早期的高分辨率表示，这对于具有图像样式输出的任务特别有用，其中这些高分辨率信息尤为重要。
- en: The UNet architecture used in the diffusers library is more advanced than the
    [original UNet proposed in 2015](https://arxiv.org/abs/1505.04597) by Ronneberger
    et al, with additions like attention and residual blocks. We’ll take a closer
    look later, but the key feature here is that it can take in an input (the noisy
    image) and produce a prediction that is the same shape (the predicted noise).
    For diffusion models, the UNet typically also takes in the timestep as additional
    conditioning, which again we will explore in the UNet deep dive section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散库中使用的UNet架构比2015年Ronneberger等人提出的[原始UNet](https://arxiv.org/abs/1505.04597)更先进，增加了注意力和残差块等功能。我们稍后会更仔细地看一下，但这里的关键特点是它可以接受一个输入（嘈杂的图像）并产生一个形状相同的预测（预测的噪音）。对于扩散模型，UNet通常还接受时间步作为额外的条件，我们将在UNet深入探讨部分再次探讨这一点。
- en: 'Here’s how we might create a UNet and feed our batch of noisy images through
    it:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何创建一个UNet并将我们的一批嘈杂图像输入其中的方法：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the output is the same shape as the input, which is exactly what we
    want.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意输出与输入的形状相同，这正是我们想要的。
- en: Training
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'Now that we have our model and our data ready, we can train it. We’ll use the
    AdamW optimizer with a learning rate of 3e-4\. For each training step, we:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型和数据准备好了，我们可以开始训练。我们将使用学习率为3e-4的AdamW优化器。对于每个训练步骤，我们：
- en: Load a batch of images.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载一批图像。
- en: Add noise to the images, choosing random timesteps to determine how much noise
    is added.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向图像添加噪音，选择随机时间步来确定添加多少噪音。
- en: Feed the noisy images into the model.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将嘈杂图像输入模型。
- en: Calculate the loss, which is the mean squared error between the model’s predictions
    and the target - which in this case is the *noise* that we added to the images.
    This is called the noise or ‘epsilon’ objective. You can find more information
    on the different training objectives in section X.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失，即模型预测与目标之间的均方误差 - 在这种情况下是我们添加到图像中的*噪音*。这被称为噪音或“epsilon”目标。您可以在第X节中找到有关不同训练目标的更多信息。
- en: Backpropagate the loss and update the model weights with the optimizer.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播损失并使用优化器更新模型权重。
- en: 'Here’s what all of that looks like in code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，所有这些看起来是这样的：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It takes an hour or so to run the above code on a GPU, so get some tea while
    you wait or lower the number of epochs. Here’s what the loss curve looks like
    after training:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上运行上述代码大约需要一个小时，所以在等待时喝杯茶或者减少时代的数量。训练后损失曲线如下所示：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![image](assets/cell-12-output-1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-12-output-1.png)'
- en: The loss curve trends downwards as the model learns to denoise the images. The
    curve is fairly noisy, thanks to different amounts of noise being added to the
    images based on the random sampling of timesteps for each iteration. It is hard
    to tell just by looking at the mean squared error of the noise predictions whether
    this model will be any good at generating samples, so let’s move on to the next
    section and see how well it does.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型学习去噪图像，损失曲线呈下降趋势。由于根据每次迭代中随机时间步的随机抽样向图像添加不同数量的噪音，曲线相当嘈杂。仅通过观察噪音预测的均方误差很难判断这个模型是否能够很好地生成样本，所以让我们继续下一节，看看它的表现如何。
- en: Sampling
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采样
- en: 'The diffusers library uses the idea of ‘pipelines’ which bundle together all
    of the components needed to generate samples with a diffusion model:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散库使用了“管道”的概念，将生成扩散模型样本所需的所有组件捆绑在一起：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![image](assets/cell-13-output-2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-13-output-2.png)'
- en: 'Of course, offloading the job of creating samples to the pipeline doesn’t really
    show us what is going on. So, here is a simple sampling loop that shows how the
    model is gradually refining the input image:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，将创建样本的工作交给管道并不能真正展示出我们正在进行的工作。因此，这里有一个简单的采样循环，展示了模型如何逐渐改进输入图像：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![image](assets/cell-14-output-1.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-14-output-1.png)'
- en: This is the same code we used at the beginning of the chapter to illustrate
    the idea of iterative refinement, but hopefully, now you have a better understanding
    of what is going on here. We start with a completely random input, which is then
    refined by the model in a series of steps. Each step is a small update to the
    input, based on the model’s prediction for the noise at that timestep. We’re still
    abstracting away some complexity behind the call to `pipeline.scheduler.step()`
    - in a later chapter we will dive deeper into different sampling methods and how
    they work.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在本章开头使用的代码相同，用来说明迭代改进的概念，但希望现在你对这里发生的事情有了更好的理解。我们从一个完全随机的输入开始，然后在一系列步骤中由模型进行改进。每一步都是对输入的小更新，基于模型对该时间步的噪音的预测。我们仍然在`pipeline.scheduler.step()`的调用背后抽象了一些复杂性
    - 在后面的章节中，我们将更深入地探讨不同的采样方法以及它们的工作原理。
- en: Evaluation
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: Generative model performance can be evaluated using FID scores (Fréchet Inception
    Distance). FID scores measure how closely generated samples match real-world samples
    by comparing statistics between feature maps extracted from both sets of data
    using a pre-trained neural network. The lower the score, the better the quality
    and realism of generated images produced by a given model. FID scores are popular
    due to their ability to provide an ‘objective’ comparison metric for different
    types of generative networks without relying on human judgment.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用FID分数（Fréchet Inception Distance）来评估生成模型的性能。FID分数通过比较从预训练的神经网络中提取的特征图之间的统计数据，衡量生成样本与真实样本的相似程度。分数越低，给定模型生成的图像的质量和逼真度就越好。FID分数因其能够提供对不同类型生成网络的“客观”比较指标而受到欢迎，而无需依赖人类判断。
- en: 'As convenient as FID scores are, there are some important caveats to be aware
    of:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管FID分数很方便，但也有一些重要的注意事项需要注意：
- en: The FID score for a given model depends on the number of samples used to calculate
    it, so when comparing between model,s we need to make sure both reported scores
    are calculated using the same number of samples. Common practice is to use 50,000
    samples for this purpose, although to save time, you may evaluate on a smaller
    number of samples during development and only do the full evaluation once you’re
    ready to publish the results.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定模型的FID分数取决于用于计算它的样本数量，因此在模型之间进行比较时，我们需要确保报告的分数都是使用相同数量的样本计算的。通常做法是在这个目的上使用50,000个样本，尽管为了节省时间，您可能在开发过程中评估较少数量的样本，只有在准备发布结果时才进行完整评估。
- en: When calculating FID, images are resized to 299px square images. This makes
    it less useful as a metric for extremely low-res or high-res images. There are
    also minor differences between how resizing is handled by different deep learning
    frameworks, which can result in small differences in the FID score! We recommend
    using a library such as `clean-fid` to standardize the FID calculation.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算FID时，图像被调整为299像素的正方形图像。这使得它对于极低分辨率或高分辨率图像的度量变得不太有用。不同的深度学习框架处理调整大小的方式也有细微差异，这可能导致FID分数有小的差异！我们建议使用`clean-fid`这样的库来标准化FID计算。
- en: The network used as a feature extractor for FID is typically a model trained
    on the Imagenet classification task. When generating images in a different domain,
    the features learned by this model may be less useful. A more accurate approach
    would be to somehow train a classification network on domain-specific data first,
    but this would make it harder to compare scores between different papers and approaches,
    so for now the imagenet model is the standard choice.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用作FID特征提取器的网络通常是在Imagenet分类任务上训练的模型。在生成不同领域的图像时，这个模型学到的特征可能不太有用。更准确的方法是在特定领域的数据上训练一个分类网络，但这会使得在不同论文和方法之间比较分数变得更加困难，所以目前Imagenet模型是标准选择。
- en: If you save generated samples for later evaluation, the format and compression
    can again affect the FID score. Avoid low-quality JPEG images where possible.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您保存生成的样本以供以后评估，格式和压缩也会影响FID分数。尽量避免低质量的JPEG图像。
- en: Even if you account for all these caveats, FID scores are just a rough measure
    of quality and do not perfectly capture the nuances of what makes images look
    more ‘real’. So, use them to get an idea of how one model performs relative to
    another but also look at the actual images generated by each model to get a better
    sense of how they compare. Human preference is still the gold standard for quality
    in what is ultimately a fairly subjective field!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 即使考虑了所有这些警告，FID分数只是质量的粗略度量，不能完美地捕捉使图像看起来更“真实”的微妙之处。因此，用它们来了解一个模型相对于另一个模型的表现，但也要看看每个模型生成的实际图像，以更好地了解它们的比较。人类偏好仍然是质量的黄金标准，最终这是一个相当主观的领域！
- en: 'In Depth: Noise Schedules'
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入：噪音时间表
- en: In the training example above, one of the steps was ‘add noise, in different
    amounts’. We achieved this by picking a random timestep between 0 and 1000 and
    then relying on the scheduler to add the appropriate amount of noise. Likewise,
    during sampling, we again relied on the scheduler to tell us which timesteps to
    use and how to move from one to the next given the model predictions. It turns
    out that choosing how much noise to add is an important design decision that can
    drastically affect the performance of a given model. In this section, we’ll see
    why this is the case and explore some of the different approaches that are used
    in practice.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的训练示例中，其中一步是“添加不同数量的噪音”。我们通过在0到1000之间选择一个随机时间步长来实现这一点，然后依靠调度程序添加适当数量的噪音。同样，在采样过程中，我们再次依靠调度程序告诉我们使用哪些时间步长以及如何根据模型预测从一个时间步长移动到下一个时间步长。事实证明，选择添加多少噪音是一个重要的设计决策，可以极大地影响给定模型的性能。在本节中，我们将看到为什么会这样，并探讨实践中使用的一些不同方法。
- en: Why Add Noise?
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么要添加噪音？
- en: At the start of this chapter, we said that the key idea behind diffusion models
    is that of iterative refinement. During training, we ‘corrupt’ an input by different
    amounts. During inference, we begin with a ‘maximally corrupted’ input and iteratively
    ‘de-corrupt’ it, in the hopes that we will eventually end up with a nice final
    result.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们说扩散模型背后的关键思想是迭代改进。在训练期间，我们通过不同的方式“腐败”输入。在推断期间，我们从“最大程度上腐败”的输入开始，迭代地“去腐败”它，希望最终得到一个不错的最终结果。
- en: 'So far, we’ve focused on one specific kind of ‘corruption’: adding Gaussian
    noise. One reason for this is the theoretical underpinnings of diffusion models
    - if we use a different corruption method we are no longer technically doing ‘diffusion’!
    However, a paper titled [*Cold Diffusion*](https://arxiv.org/abs/2208.09392) by
    Bansal et al dramatically demonstrated that we do not necessarily need to constrain
    ourselves to this method just for theoretical convenience. They showed that a
    diffusion-model-like approach works for many different ‘corruption’ methods (see
    [Figure 1-1](#c4_f1)). More recently, models like [MUSE](https://arxiv.org/abs/2301.00704),
    [MaskGIT](https://arxiv.org/abs/2202.04200) and [PAELLA](https://arxiv.org/abs/2211.07292)
    have used random token masking or replacement as an equivalent ‘corruption’ method
    for quantized data - that is, data that is represented by discrete tokens rather
    than continuous values.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经专注于一种特定的“腐败”：添加高斯噪声。这样做的一个原因是扩散模型的理论基础 - 如果我们使用不同的腐败方法，我们在技术上就不再做“扩散”了！然而，Bansal等人的一篇题为[*Cold
    Diffusion*](https://arxiv.org/abs/2208.09392)的论文戏剧性地表明，我们不一定需要仅仅出于理论上的便利而限制自己使用这种方法。他们表明，类似扩散模型的方法适用于许多不同的“腐败”方法（见[图1-1](#c4_f1)）。最近，像[MUSE](https://arxiv.org/abs/2301.00704)、[MaskGIT](https://arxiv.org/abs/2202.04200)和[PAELLA](https://arxiv.org/abs/2211.07292)这样的模型已经使用了随机标记屏蔽或替换作为等效的“腐败”方法，用于量化数据
    - 也就是说，用离散标记而不是连续值表示的数据。
- en: '![image.png](assets/84ca400c-1-image.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![image.png](assets/84ca400c-1-image.png)'
- en: Figure 1-1\. Illustration of the different degradations used in the Cold Diffusion
    Paper
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1。Cold Diffusion Paper中使用的不同退化的示意图
- en: 'Nonetheless, adding noise remains the most popular approach for several reasons:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，出于几个原因，添加噪音仍然是最受欢迎的方法：
- en: We can easily control the amount of noise added, giving a smooth transition
    from ‘perfect’ to ‘completely corrupted’. This is not the case for something like
    reducing the resolution of an image, which may result in ‘discrete’ transitions.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以轻松控制添加的噪音量，从“完美”到“完全损坏”平稳过渡。这对于像减少图像分辨率这样的事情并不适用，这可能会导致“离散”的过渡。
- en: We can have many valid random starting points for inference, unlike some methods
    which may only have a limited number of possible initial (fully corrupted) states,
    such as a completely black image or a single-pixel image.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以有许多有效的随机起始点进行推断，不像一些方法可能只有有限数量的可能的初始（完全损坏）状态，比如完全黑色的图像或单像素图像。
- en: So, for the moment at least, we’ll stick with adding noise as our corruption
    method. Next, let’s take a closer look at how we add noise to our images.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，至少目前，我们将坚持添加噪音作为我们的损坏方法。接下来，让我们更仔细地看看如何向图像添加噪音。
- en: Starting Simple
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始简单
- en: We have some images (x) and we’d like to combine them somehow with some random
    noise.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一些图像（x），我们想以某种方式将它们与一些随机噪音结合起来。
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'One way we could do this is to linearly interpolate (lerp) between them by
    some amount. This gives us a function that smoothly transitions from the original
    image x to pure noise as the ‘amount’ varies from 0 to 1:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以线性插值（lerp）它们之间的一些量。这给我们一个函数，它在“量”从0到1变化时，从原始图像x平稳过渡到纯噪音：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s see this in action on a batch of data, with the amount of noise varying
    from 0 to 1:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一批数据上看看这个过程，噪音的量从0到1变化：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![image](assets/cell-17-output-1.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-17-output-1.png)'
- en: 'This seems to be doing exactly what we want, smoothly transitioning from the
    original image to pure noise. Now, we’ve created a noise schedule here that takes
    in a value for ‘amount’ from 0 to 1\. This is called the ‘continuous time’ approach,
    where we represent the full path on a time scale from 0 to 1\. Other approaches
    use a discrete time approach, with some large integer number of ‘timesteps’ used
    to define the noise scheduler. We can wrap our function into a class that converts
    from continuous time to discrete timesteps and adds noise appropriately:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎正是我们想要的，从原始图像平稳过渡到纯噪音。现在，我们在这里创建了一个噪音时间表，它接受从0到1的“量”值。这被称为“连续时间”方法，我们在时间尺度上表示从0到1的完整路径。其他方法使用离散时间方法，使用一些大整数的“时间步长”来定义噪音调度器。我们可以将我们的函数封装成一个类，将连续时间转换为离散时间步长，并适当添加噪音：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![image](assets/cell-18-output-1.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-18-output-1.png)'
- en: 'Now we have something that we can directly compare to the schedulers used in
    the diffusers library, such as the DDPMScheduler we used during training. Let’s
    see how it compares:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一些可以直接与扩散库中使用的调度器进行比较的东西，比如我们在训练中使用的DDPMScheduler。让我们看看它是如何比较的：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![image](assets/cell-19-output-1.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-19-output-1.png)'
- en: The Maths
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学
- en: There are many competing notations and approaches in the literature. For example,
    some papers parametrize the noise schedule in *continuous-time* where t runs from
    0 (no noise) to 1 (fully corrupted) - just like our `corrupt` function in the
    previous section. Others use a *discrete-time* approach with integer timesteps
    running from 0 to some large number T, typically 1000\. It is possible to convert
    between these two approaches the way we did with our `SimpleScheduler` class -
    just make sure you’re consistent when comparing different models. We’ll stick
    with the discrete-time approach here.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中有许多竞争的符号和方法。例如，一些论文将噪音时间表参数化为“连续时间”，其中t从0（无噪音）到1（完全损坏）- 就像我们在上一节中的`corrupt`函数一样。其他人使用“离散时间”方法，其中整数时间步长从0到某个大数T，通常为1000。可以像我们的`SimpleScheduler`类一样在这两种方法之间进行转换-只需确保在比较不同模型时保持一致。我们将在这里坚持使用离散时间方法。
- en: A good place to start for pushing deeper into the maths is the DDPM paper mentioned
    earlier. You can find an [annotated implementation here](https://huggingface.co/blog/annotated-diffusion)
    which is a great additional resource for understanding this approach.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究数学的一个很好的起点是之前提到的DDPM论文。您可以在这里找到[注释实现](https://huggingface.co/blog/annotated-diffusion)，这是一个很好的额外资源，可以帮助理解这种方法。
- en: 'The paper begins by specifying a single noise step to go from timestep t-1
    to timestep t. Here’s how they write it:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 论文开始时指定了从时间步t-1到时间步t的单个噪音步骤。这是他们的写法：
- en: <math display="block"><mrow><mi>q</mi> <mrow><mo>(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>𝒩</mi> <mrow><mo>(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub>
    <mo>;</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msub><mi>β</mi> <mi>t</mi></msub> <mi>𝐈</mi> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>q</mi> <mrow><mo>(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>𝒩</mi> <mrow><mo>(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub>
    <mo>;</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msub><mi>β</mi> <mi>t</mi></msub> <mi>𝐈</mi> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
- en: 'Here <math alttext="beta Subscript t"><msub><mi>β</mi> <mi>t</mi></msub></math>
    is defined for all timesteps t and is used to specify how much noise is added
    at each step. This notation can be a little intimidating, but what this equation
    tells us is that the noisier <math alttext="bold x Subscript t"><msub><mi>𝐱</mi>
    <mi>t</mi></msub></math> is a *distribution* with a mean of <math alttext="StartRoot
    1 minus beta Subscript t Baseline EndRoot bold x Subscript t minus 1"><mrow><msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt> <msub><mi>𝐱</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math> and a variance
    of <math alttext="beta Subscript t"><msub><mi>β</mi> <mi>t</mi></msub></math>
    . In other words, <math alttext="bold x Subscript t"><msub><mi>𝐱</mi> <mi>t</mi></msub></math>
    is a mix of <math alttext="bold x Subscript t minus 1"><msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    (scaled by <math alttext="StartRoot 1 minus beta Subscript t Baseline EndRoot"><msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt></math> ) and some
    random noise, which we can think of as unit-variance noise scaled by <math alttext="StartRoot
    beta Subscript t Baseline EndRoot"><msqrt><msub><mi>β</mi> <mi>t</mi></msub></msqrt></math>
    . Given <math alttext="x Subscript t minus 1"><msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    and some noise <math alttext="epsilon"><mi>ϵ</mi></math> , we can sample from
    this distribution to get <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    with:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里<math alttext="beta Subscript t"><msub><mi>β</mi> <mi>t</mi></msub></math>被定义为所有时间步长t，并用于指定每个步骤添加多少噪声。这种符号可能有点令人生畏，但这个方程告诉我们的是，更嘈杂的<math
    alttext="bold x Subscript t"><msub><mi>𝐱</mi> <mi>t</mi></msub></math>是一个*分布*，其均值为<math
    alttext="StartRoot 1 minus beta Subscript t Baseline EndRoot bold x Subscript
    t minus 1"><mrow><msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>，方差为<math
    alttext="beta Subscript t"><msub><mi>β</mi> <mi>t</mi></msub></math>。换句话说，<math
    alttext="bold x Subscript t"><msub><mi>𝐱</mi> <mi>t</mi></msub></math>是<math alttext="bold
    x Subscript t minus 1"><msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>（按<math
    alttext="StartRoot 1 minus beta Subscript t Baseline EndRoot"><msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt></math>缩放）和一些随机噪声的混合，我们可以将其视为按<math
    alttext="StartRoot beta Subscript t Baseline EndRoot"><msqrt><msub><mi>β</mi>
    <mi>t</mi></msub></msqrt></math>缩放的单位方差噪声。给定<math alttext="x Subscript t minus
    1"><msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>和一些噪声<math
    alttext="epsilon"><mi>ϵ</mi></math>，我们可以从这个分布中采样得到<math alttext="x Subscript t"><msub><mi>x</mi>
    <mi>t</mi></msub></math>：
- en: <math><mrow><msub><mi>𝐱</mi> <mi>t</mi></msub> <mo>=</mo> <msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt> <msub><mi>𝐱</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msqrt><msub><mi>β</mi>
    <mi>t</mi></msub></msqrt> <mi>ϵ</mi></mrow></math>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi>𝐱</mi> <mi>t</mi></msub> <mo>=</mo> <msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt> <msub><mi>𝐱</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msqrt><msub><mi>β</mi>
    <mi>t</mi></msub></msqrt> <mi>ϵ</mi></mrow></math>
- en: 'To get the noisy input at timestep t, we could begin at t=0 and repeatedly
    apply this single step, but this would be very inefficient. Instead, we can find
    a formula to move to any timestep t in one go. We define <math alttext="alpha
    Subscript t Baseline equals 1 minus beta Subscript t"><mrow><msub><mi>α</mi> <mi>t</mi></msub>
    <mo>=</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></math>
    and then use the following formula:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要在时间步t获得嘈杂的输入，我们可以从t=0开始，并反复应用这一步，但这将非常低效。相反，我们可以找到一个公式一次性移动到任何时间步t。我们定义<math
    alttext="alpha Subscript t Baseline equals 1 minus beta Subscript t"><mrow><msub><mi>α</mi>
    <mi>t</mi></msub> <mo>=</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></math>，然后使用以下公式：
- en: <math><mrow><msub><mi>x</mi> <mi>t</mi></msub> <mo>=</mo> <msqrt><msub><mover
    accent="true"><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi></msub></msqrt> <msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover> <mi>t</mi></msub></mrow></msqrt> <mi>ϵ</mi></mrow></math>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi>x</mi> <mi>t</mi></msub> <mo>=</mo> <msqrt><msub><mover
    accent="true"><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi></msub></msqrt> <msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover> <mi>t</mi></msub></mrow></msqrt> <mi>ϵ</mi></mrow></math>
- en: where - <math alttext="epsilon"><mi>ϵ</mi></math> is some gaussian noise with
    unit variance - <math alttext="alpha overbar"><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover></math> (‘alpha_bar’) is the cumulative product of all the <math
    alttext="alpha"><mi>α</mi></math> values up to the time <math alttext="t"><mi>t</mi></math>
    .
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里 - <math alttext="epsilon"><mi>ϵ</mi></math> 是一些方差为单位的高斯噪声 - <math alttext="alpha
    overbar"><mover accent="true"><mi>α</mi> <mo>¯</mo></mover></math>（'alpha_bar'）是直到时间<math
    alttext="t"><mi>t</mi></math>的所有<math alttext="alpha"><mi>α</mi></math>值的累积乘积。
- en: 'So <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    is a mixture of <math alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math>
    (scaled by <math alttext="StartRoot alpha overbar Subscript t Baseline EndRoot"><msqrt><msub><mover
    accent="true"><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi></msub></msqrt></math> )
    and <math alttext="epsilon"><mi>ϵ</mi></math> (scaled by <math alttext="StartRoot
    1 minus alpha overbar Subscript t Baseline EndRoot"><msqrt><mrow><mn>1</mn> <mo>-</mo>
    <msub><mover accent="true"><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi></msub></mrow></msqrt></math>
    ). In the diffusers library the <math alttext="alpha overbar"><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover></math> values are stored in `scheduler.alphas_cumprod`. Knowing
    this, we can plot the scaling factors for the original image <math alttext="x
    0"><msub><mi>x</mi> <mn>0</mn></msub></math> and the noise <math alttext="epsilon"><mi>ϵ</mi></math>
    across the different timesteps for a given scheduler:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，<math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>是<math
    alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math>（由<math alttext="StartRoot
    alpha overbar Subscript t Baseline EndRoot"><msqrt><msub><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover> <mi>t</mi></msub></msqrt></math>缩放）和<math alttext="epsilon"><mi>ϵ</mi></math>（由<math
    alttext="StartRoot 1 minus alpha overbar Subscript t Baseline EndRoot"><msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mover accent="true"><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi></msub></mrow></msqrt></math>缩放）的混合物。在diffusers库中，<math
    alttext="alpha overbar"><mover accent="true"><mi>α</mi> <mo>¯</mo></mover></math>值存储在`scheduler.alphas_cumprod`中。知道这一点，我们可以绘制给定调度程序的不同时间步骤中原始图像<math
    alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math>和噪音<math alttext="epsilon"><mi>ϵ</mi></math>的缩放因子：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![image](assets/cell-21-output-1.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-21-output-1.png)'
- en: 'Our SimpleScheduler above just linearly mixes between the original image and
    noise, as we can see if we plot the scaling factors (equivalent to <math alttext="StartRoot
    alpha overbar Subscript t Baseline EndRoot"><msqrt><msub><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover> <mi>t</mi></msub></msqrt></math> and <math alttext="StartRoot
    left-parenthesis 1 minus alpha overbar Subscript t Baseline right-parenthesis
    EndRoot"><msqrt><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover> <mi>t</mi></msub> <mo>)</mo></mrow></msqrt></math> in the DDPM
    case):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面的SimpleScheduler只是在原始图像和噪音之间线性混合，我们可以看到如果我们绘制缩放因子（相当于<math alttext="StartRoot
    alpha overbar Subscript t Baseline EndRoot"><msqrt><msub><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover> <mi>t</mi></msub></msqrt></math>和<math alttext="StartRoot left-parenthesis
    1 minus alpha overbar Subscript t Baseline right-parenthesis EndRoot"><msqrt><mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mover accent="true"><mi>α</mi> <mo>¯</mo></mover>
    <mi>t</mi></msub> <mo>)</mo></mrow></msqrt></math>在DDPM情况下）：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![image](assets/cell-22-output-1.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-22-output-1.png)'
- en: 'A good noise schedule will ensure that the model sees a mix of images at different
    noise levels. The best choice will differ based on the training data. Visualizing
    a few more options, note that:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好的噪音调度将确保模型看到不同噪音水平的图像混合。最佳选择将根据训练数据而异。可视化一些更多选项，注意：
- en: Setting beta_end too low means we never completely erase the image, so the model
    will never see anything like the random noise used as a starting point for inference.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将beta_end设置得太低意味着我们永远不会完全擦除图像，因此模型永远不会看到任何类似于用作推理起点的随机噪音的东西。
- en: Setting beta_end extremely high means that most of the timesteps are spent on
    almost complete noise, which will result in poor training performance.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将beta_end设置得非常高意味着大多数时间步骤都花在几乎完全的噪音上，这将导致训练性能不佳。
- en: Different beta schedules give different curves.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的beta调度给出不同的曲线。
- en: The ‘cosine’ schedule is a popular choice, as it gives a smooth transition from
    the original image to the noise.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: “余弦”调度是一个受欢迎的选择，因为它可以使原始图像平稳过渡到噪音。
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![image](assets/cell-23-output-1.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-23-output-1.png)'
- en: Note
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: All of the schedules shown here are called ‘Variance Preserving’ (VP), meaning
    that the variance of the model input is kept close to 1 across the entire schedule.
    You may also encounter ‘Variance Exploding’ (VE) formulations where noise is simply
    added to the original image in different amounts (resulting in high-variance inputs).
    We’ll go into this more in the chapter on sampling. Our SimpleScheduler is almost
    a VP schedule, but the variance is not quite preserved due to the linear interpolation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示的所有调度都被称为“方差保持”（VP），这意味着模型输入的方差在整个调度过程中保持接近1。您可能还会遇到“方差爆炸”（VE）公式，其中噪音只是以不同的量添加到原始图像（导致高方差输入）。我们将在采样章节中更详细地讨论这一点。我们的SimpleScheduler几乎是一个VP调度，但由于线性插值，方差并没有完全保持。
- en: As with many diffusion-related topics, there is a constant stream of new papers
    exploring the topic of noise schedules, so by the time you read this there will
    likely be a large collection of options to try out!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多与扩散相关的主题一样，不断有新的论文探讨噪音调度的主题，因此当您阅读本文时，可能会有大量的选项可供尝试！
- en: Effect of Input Resolution and Scaling
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入分辨率和缩放的影响
- en: One aspect of noise schedules that was mostly overlooked until recently is the
    effect of input size and scaling. Many papers test potential schedulers on small-scale
    datasets and at low resolution, and then use the best-performing scheduler to
    train their final models on larger images. The problem with this is can be seen
    if we add the same amount of noise to two images of different sizes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，噪音调度的一个方面大多被忽视，即输入大小和缩放的影响。许多论文在小规模数据集和低分辨率上测试潜在的调度程序，然后使用表现最佳的调度程序来训练其最终模型的较大图像。这样做的问题在于，如果我们向两个不同大小的图像添加相同数量的噪音，就会看到问题。
- en: '![image](assets/cell-24-output-1.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-24-output-1.png)'
- en: Figure 1-2\. Comparing the effect of adding noise to images of different sizes
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2。比较向不同大小的图像添加噪音的效果
- en: Images at high resolution tend to contain a lot of redundant information. This
    means that even if a single pixel is obscured by noise, the surrounding pixels
    contain enough information to reconstruct the original image. This is not the
    case for low-resolution images, where a single pixel can contain a lot of information.
    This means that adding the same amount of noise to a low-resolution image will
    result in a much more corrupted image than adding the equivalent amount of noise
    to a high-resolution image.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 高分辨率的图像往往包含大量冗余信息。这意味着即使单个像素被噪音遮挡，周围的像素也包含足够的信息来重建原始图像。但对于低分辨率图像来说并非如此，其中单个像素可能包含大量信息。这意味着向低分辨率图像添加相同量的噪音将导致比向高分辨率图像添加等量噪音更加损坏的图像。
- en: 'This effect was thoroughly investigated in two independent papers, both of
    which came out in January 2023\. Each used the new insights to train models capable
    of generating high-resolution outputs without requiring any of the tricks that
    have previously been necessary. [*Simple diffusion*](https://arxiv.org/abs/2301.11093)
    by Hoogeboom et al introduced a method for adjusting the noise schedule based
    on the input size, allowing a schedule optimized on low-resolution images to be
    appropriately modified for a new target resolution. A paper called [“On the Importance
    of Noise Scheduling for Diffusion Models”](https://arxiv.org/abs/2301.10972) by
    Ting Chen performed similar experiments, and noted another key variable: input
    scaling. That is, how do we represent our images? If the images are represented
    as floats between 0 and 1 then they will have a lower variance than the noise
    (which is typically unit variance) and thus the signal-to-noise ratio will be
    lower for a given noise level than if the images were represented as floats between
    -1 and 1 (which we used in the training example above) or something else. Scaling
    the input images shifts the signal-to-noise ratio, and so modifying this scaling
    is another way we can adjust when training on larger images.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这种效应在两篇独立的论文中得到了彻底的调查，这两篇论文分别于2023年1月发表。每篇论文都利用新的见解来训练能够生成高分辨率输出的模型，而无需任何以前必需的技巧。Hoogeboom等人的《简单扩散》介绍了一种根据输入大小调整噪音计划的方法，允许在低分辨率图像上优化的计划适当地修改为新的目标分辨率。陈婷的一篇名为“关于扩散模型噪音调度的重要性”的论文进行了类似的实验，并注意到另一个关键变量：输入缩放。也就是说，我们如何表示我们的图像？如果图像表示为0到1之间的浮点数，那么它们的方差将低于噪音（通常是单位方差），因此对于给定的噪音水平，信噪比将低于如果图像表示为-1到1之间的浮点数（我们在上面的训练示例中使用的方式）或其他方式。缩放输入图像会改变信噪比，因此修改这种缩放是我们在训练更大图像时可以调整的另一种方式。
- en: 'In Depth: UNets and Alternatives'
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解：UNets和替代方案
- en: Now let’s address the actual model that makes the all-important predictions!
    To recap, this model must be capable of taking in a noisy image and estimating
    how to denoise it. This requires a model that can take in an image of arbitrary
    size and output an image of the same size. Furthermore, the model should be able
    to make precise predictions at the pixel level, while also capturing higher-level
    information about the image as a whole. A popular approach is to use an architecture
    called a UNet. UNets were invented in 2015 for medical image segmentation, and
    have since become a popular choice for various image-related tasks. Like the AutoEncoders
    and VAEs we looked at in the previous chapter, UNets are made up of a series of
    ‘downsampling’ and ‘upsampling’ blocks. The downsampling blocks are responsible
    for reducing the size of the image, while the upsampling blocks are responsible
    for increasing the size of the image. The downsampling blocks are typically made
    up of a series of convolutional layers, followed by a pooling or downsampling
    layer. The upsampling blocks are typically made up of a series of convolutional
    layers, followed by an upsampling or ‘transposed convolution’ layer. The transposed
    convolution layer is a special type of convolutional layer that increases the
    size of the image, rather than reducing it.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来讨论真正进行重要预测的模型！回顾一下，这个模型必须能够接收嘈杂的图像并估计如何去除噪音。这需要一个能够接收任意大小的图像并输出相同大小的图像的模型。此外，该模型应能够在像素级别进行精确预测，同时也捕捉关于整个图像的更高级别信息。一个流行的方法是使用一种称为UNet的架构。UNet是在2015年为医学图像分割而发明的，并且后来成为各种与图像相关的任务的流行选择。就像我们在上一章中看到的AutoEncoders和VAEs一样，UNets由一系列“下采样”和“上采样”块组成。下采样块负责减小图像的大小，而上采样块负责增加图像的大小。下采样块通常由一系列卷积层组成，然后是池化或下采样层。上采样块通常由一系列卷积层组成，然后是上采样或“转置卷积”层。转置卷积层是一种特殊类型的卷积层，它增加图像的大小，而不是减小图像的大小。
- en: The reason a regular AutoEncoder or VAE is not a good choice for this task is
    that they are less capable of making precise predictions at the pixel level since
    the output must be entirely re-constructed from the low-dimensional latent space.
    In a UNet, the downsampling and upsampling blocks are connected by ‘skip connections’,
    which allow information to flow directly from the downsampling blocks to the upsampling
    blocks. This allows the model to make precise predictions at the pixel level,
    while also capturing higher-level information about the image as a whole.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 常规的AutoEncoder或VAE之所以不适合这个任务，是因为它们在像素级别进行精确预测的能力较弱，因为输出必须完全从低维潜在空间重新构建。在UNet中，通过“跳跃连接”连接下采样和上采样块，允许信息直接从下采样块流向上采样块。这使得模型能够在像素级别进行精确预测，同时也捕捉关于整个图像的更高级别信息。
- en: A Simple UNet
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个简单的UNet
- en: To better understand the structure of a UNet, let’s build a simple UNet from
    scratch.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解UNet的结构，让我们从头开始构建一个简单的UNet。
- en: '![image.png](assets/595852ce-1-image.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![image.png](assets/595852ce-1-image.png)'
- en: Figure 1-3\. Our simple UNet architecture
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3。我们简单的UNet架构
- en: 'This UNet takes single-channel inputs at 32px resolution and outputs single-channel
    outputs at 32px resolution, which we could use to build a diffusion model for
    the MNIST dataset. There are three layers in the encoding path, and three layers
    in the decoding path. Each layer consists of a convolution followed by an activation
    function and an upsampling or downsampling step (depending on whether we are in
    the encoding or decoding path). The skip connections allow information to flow
    directly from the downsampling blocks to the upsampling blocks, and are implemented
    by adding the output of the downsampling block to the input of the corresponding
    upsampling block. Some UNets instead concatenate the output of the downsampling
    block to the input of the corresponding upsampling block, and may also include
    additional layers in the skip connections. Here’s what this network looks like
    in code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个UNet以32px分辨率接收单通道输入，并以32px分辨率输出单通道输出，我们可以用它来构建MNIST数据集的扩散模型。编码路径中有三层，解码路径中也有三层。每一层由卷积后跟激活函数和上采样或下采样步骤（取决于我们是否在编码或解码路径中）组成。跳过连接允许信息直接从下采样块流向上采样块，并通过将下采样块的输出添加到相应上采样块的输入来实现。一些UNet将下采样块的输出连接到相应上采样块的输入，并可能还在跳过连接中包含额外的层。以下是这个网络的代码：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A diffusion model trained with this architecture on MNIST produces the following
    samples (code included in the supplementary material but omitted here for brevity):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST上使用这种架构训练的扩散模型产生以下样本（代码包含在补充材料中，这里为了简洁起见而省略）：
- en: Improving the UNet
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进UNet
- en: This simple UNet works for this relatively easy task, but it is far from ideal.
    So, what can we do to improve it?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的UNet适用于这个相对简单的任务，但远非理想。那么，我们可以做些什么来改进它呢？
- en: Add more parameters. This can be accomplished by using multiple convolutional
    layers in each block, by using a larger number of filters in each convolutional
    layer, or by making the network deeper.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多参数。可以通过在每个块中使用多个卷积层，通过在每个卷积层中使用更多的滤波器，或者通过使网络更深来实现。
- en: Add residual connections. Using ResBlocks instead of regular convolutional layers
    can help the model learn more complex functions while keeping training stable.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加残差连接。使用ResBlocks而不是常规卷积层可以帮助模型学习更复杂的功能，同时保持训练稳定。
- en: Add normalization, such as batch normalization. Batch normalization can help
    the model learn more quickly and reliably, by ensuring that the outputs of each
    layer are centered around 0 and have a standard deviation of 1.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加归一化，如批归一化。批归一化可以帮助模型更快、更可靠地学习，确保每一层的输出都围绕0中心，并具有标准差为1。
- en: Add regularization, such as dropout. Dropout helps by preventing the model from
    overfitting to the training data, which is important when working with smaller
    datasets.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加正则化，如dropout。Dropout有助于防止模型过度拟合训练数据，这在处理较小的数据集时非常重要。
- en: Add attention. By introducing self-attention layers we allow the model to focus
    on different parts of the image at different times, which can help it learn more
    complex functions. The addition of transformer-like attention layers also lets
    us increase the number of learnable parameters, which can help the model learn
    more complex functions. The downside is that attention layers are much more expensive
    to compute than regular convolutional layers at higher resolutions, so we typically
    only use them at lower resolutions (i.e. the lower resolution blocks in the UNet).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加注意力。通过引入自注意力层，我们允许模型在不同时间集中关注图像的不同部分，这有助于学习更复杂的功能。类似变压器的注意力层的添加也可以增加可学习参数的数量，这有助于模型学习更复杂的功能。缺点是注意力层在更高分辨率时计算成本要比常规卷积层高得多，因此我们通常只在较低分辨率（即UNet中的较低分辨率块）使用它们。
- en: Add an additional input for the timestep, so that the model can tailor its predicitons
    according to the noise level. This is called timestep conditioning, and is used
    in almost all recent diffusion models. We’ll see more on conditional models in
    the next chapter.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个额外的输入用于时间步长，这样模型可以根据噪音水平调整其预测。这称为时间步长调节，几乎所有最近的扩散模型都在使用。我们将在下一章中更多地了解有条件的模型。
- en: 'For comparison, here are the results on MNIST when using the UNet implementation
    in the diffusers library, which features all of the above improvements:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对比，在diffusers库中使用UNet实现时，在MNIST上的结果如下，该库包含了上述所有改进：
- en: Warning
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This section will likely be expanded with results and more details in the future.
    We just haven’t gotten around to training variants with the different improvements
    yet!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分可能会在未来通过结果和更多细节进行扩展。我们还没有开始训练具有不同改进的变体！
- en: Alternative Architectures
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替代架构
- en: 'More recently, a number of alternative architectures have been proposed for
    diffusion models. These include:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些替代架构已被提出用于扩散模型。这些包括：
- en: Transformers. The DiT paper ([“Scalable Diffusion Models with Transformers”](https://arxiv.org/abs/2212.09748))
    by Peebles and Xie showed that a transformer-based architecture can be used to
    train a diffusion model, with great results. However, the compute and memory requirements
    of the transformer architecture remain a challenge for very high resolutions.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器。Peebles和Xie的DiT论文（“具有变压器的可扩展扩散模型”）显示了基于变压器的架构可以用于训练扩散模型，并取得了很好的结果。然而，变压器架构的计算和内存需求仍然是非常高分辨率的挑战。
- en: The *UViT* architecture from the [Simple Diffusion paper](https://arxiv.org/abs/2301.11093)
    link aims to get the best of both worlds by replacing the middle layers of the
    UNet with a large stack of transformer blocks. A key insight of this paper was
    that focusing the majority of the compute at the lower resolution blocks of the
    UNet allows for more efficient training of high-resolution diffusion models. For
    very high resolutions, they do some additional pre-processing using something
    called a wavelet transform to reduce the spatial resolution of the input image
    while keeping as much information as possible through the use of additional channels,
    again reducing the amount of compute spent on the higher spatial resolutions.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Simple Diffusion paper](https://arxiv.org/abs/2301.11093)链接中的*UViT*架构旨在兼顾两者的优点，通过用一大堆变压器块替换UNet的中间层。该论文的一个关键见解是，将大部分计算集中在UNet的较低分辨率块上，可以更有效地训练高分辨率扩散模型。对于非常高的分辨率，他们使用称为小波变换的东西进行一些额外的预处理，以减少输入图像的空间分辨率，同时通过使用额外的通道尽可能保留更多信息，再次减少在更高空间分辨率上花费的计算量。'
- en: Recurrent Interface Networks. The [RIN paper](https://arxiv.org/abs/2212.11972)
    (Jabri et al) takes a similar approach, first mapping the high-resolution inputs
    to a more manageable and lower-dimensional ‘latent’ representation which is then
    processed by a stack of transformer blocks before being decoded back out to an
    image. Additionally, the RIN paper introduces an idea of ‘recurrence’ where information
    is passed to the model from the previous processing step, which can be beneficial
    for the kind of iterative improvement that diffusion models are designed to perform.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环接口网络。[RIN paper](https://arxiv.org/abs/2212.11972)（Jabri等）采用类似的方法，首先将高分辨率输入映射到更易处理和低维的“潜在”表示，然后通过一堆变压器块进行处理，然后解码回到图像。此外，RIN论文引入了“循环”概念，其中信息从上一个处理步骤传递给模型，这对于扩散模型旨在执行的迭代改进可能是有益的。
- en: It remains to be seen whether transformer-based approaches completely supplant
    UNets as the go-to architecture for diffusion models, or whether hybrid approaches
    like the UViT and RIN architectures will prove to be the most effective.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尚不清楚基于变压器的方法是否会完全取代UNet作为扩散模型的首选架构，还是像UViT和RIN架构这样的混合方法将被证明是最有效的。
- en: 'In Depth: Objectives and Pre-Conditioning'
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入：目标和预处理
- en: 'We’ve spoken about diffusion models taking a noisy input and “learning to denoise”
    it. At first glance, you might assume that the natural prediction target for the
    network is the denoised version of the image, which we’ll call `x0`. However,
    in the code, we compared the model prediction with the unit-variance noise that
    was used to create the noisy version (often called the epsilon objective, `eps`).
    The two appear mathematically identical since if we know the noise and the timestep
    we can derive `x0` and vice versa. While this is true, the choice of objective
    has some subtle effects on how large the loss is at different timesteps, and thus
    which noise levels the model learns to denoise best. To gain some intuition, let’s
    visualize some different objectives across different timesteps:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经谈到扩散模型接受嘈杂的输入并“学会去噪”它。乍一看，你可能会认为网络的自然预测目标是图像的去噪版本，我们将其称为`x0`。然而，在代码中，我们将模型预测与用于创建嘈杂版本的单位方差噪声进行了比较（通常称为epsilon目标，`eps`）。这两者在数学上看起来是相同的，因为如果我们知道噪声和时间步长，我们可以推导出`x0`，反之亦然。虽然这是真的，但目标的选择对不同时间步的损失有一些微妙的影响，因此模型学习最佳去噪哪个噪声水平。为了获得一些直觉，让我们在不同的时间步上可视化一些不同的目标：
- en: '![image](assets/cell-26-output-1.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-26-output-1.png)'
- en: At extremely low noise levels, the `x0` objective is trivially easy while predicting
    the noise accurately is almost impossible. Likewise, at extremely high noise levels,
    the `eps` objective is easy while predicting the denoised image accurately is
    almost impossible. Neither case is ideal, and so additional objectives have been
    introduced that have the model predict a mix of `x0` and `eps` at different timesteps.
    The `v` objective (introduced in [“Progressive distillation for fast sampling
    of diffusion models.”](https://arxiv.org/abs/2202.00512) by Salimans and Ho) is
    one such objective, which is defined as <math alttext="v equals StartRoot alpha
    overbar EndRoot dot epsilon plus StartRoot 1 minus alpha overbar EndRoot dot x
    0"><mrow><mi>v</mi> <mo>=</mo> <msqrt><mover accent="true"><mi>α</mi> <mo>¯</mo></mover></msqrt>
    <mo>·</mo> <mi>ϵ</mi> <mo>+</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover></mrow></msqrt> <mo>·</mo> <msub><mi>x</mi> <mn>0</mn></msub></mrow></math>
    . The [EDM paper](https://arxiv.org/abs/2206.00364) by Karras et al introduce
    a similar idea via a parameter called `c_skip`, and unify the different diffusion
    model formulations into a consistent framework. If you’re interested in learning
    more about the different objectives, scalings and other nuances of the different
    diffusion model formulations, we recommend reading their paper for a more in-depth
    discussion.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在极低的噪声水平下，`x0`目标是非常容易的，而准确预测噪声几乎是不可能的。同样，在极高的噪声水平下，`eps`目标是容易的，而准确预测去噪图像几乎是不可能的。两种情况都不理想，因此引入了其他目标，使模型在不同的时间步预测`x0`和`eps`的混合。[“用于快速采样扩散模型的渐进蒸馏。”](https://arxiv.org/abs/2202.00512)中引入的`v`目标是其中之一，它被定义为<math
    alttext="v equals StartRoot alpha overbar EndRoot dot epsilon plus StartRoot 1
    minus alpha overbar EndRoot dot x 0"><mrow><mi>v</mi> <mo>=</mo> <msqrt><mover
    accent="true"><mi>α</mi> <mo>¯</mo></mover></msqrt> <mo>·</mo> <mi>ϵ</mi> <mo>+</mo>
    <msqrt><mrow><mn>1</mn> <mo>-</mo> <mover accent="true"><mi>α</mi> <mo>¯</mo></mover></mrow></msqrt>
    <mo>·</mo> <msub><mi>x</mi> <mn>0</mn></msub></mrow></math>。Karras等人在[EDM paper](https://arxiv.org/abs/2206.00364)中通过一个称为`c_skip`的参数引入了类似的想法，并将不同的扩散模型公式统一到一个一致的框架中。如果您对了解不同目标、缩放和其他不同扩散模型公式的微妙之处感兴趣，我们建议阅读他们的论文以获得更深入的讨论。
- en: 'Project Time: Train Your Own Diffusion Model'
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目时间：训练您自己的扩散模型
- en: Now that you have an understanding of the basics of diffusion models, it’s time
    to train some for yourself! The supplementary material for this chapter includes
    a notebook that walks you through the process of training a diffusion model on
    your own dataset. As you work through it, check back with this chapter and see
    how the different pieces fit together. The notebook also includes lots of suggested
    changes you can make to better explore how different model architectures and training
    strategies affect the results.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了扩散模型的基础知识，现在是时候自己训练一些了！本章的补充材料包括一个笔记本，指导您如何在自己的数据集上训练扩散模型的过程。在您进行操作时，请回顾本章，看看不同部分是如何相互配合的。笔记本还包括许多建议的更改，以更好地探索不同的模型架构和训练策略如何影响结果。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we’ve seen how the idea of iterative refinement can be applied
    to train a diffusion model capable of turning noise into beautiful images. You’ve
    seen some of the design choices that go into creating a successful diffusion model,
    and hopefully put them into practice by training your own model. In the next chapter,
    we’ll take a look at some of the more advanced techniques that have been developed
    to improve the performance of diffusion models and to give them extraordinary
    new capabilities!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了迭代改进的想法如何应用于训练扩散模型，使其能够将噪音转化为美丽的图像。您已经看到了一些设计选择，这些选择是创建成功的扩散模型所必需的，并希望通过训练自己的模型来实践这些选择。在下一章中，我们将看看一些更先进的技术，这些技术已经被开发出来，以改进扩散模型的性能，并赋予它们非凡的新能力！
- en: References
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ho, Jonathan, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic
    models.” Advances in Neural Information Processing Systems 33 (2020): 6840-6851.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ho, Jonathan, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic
    models.” Advances in Neural Information Processing Systems 33 (2020): 6840-6851.'
- en: 'Ronneberger, O., Fischer, P. and Brox, T., 2015\. U-net: Convolutional networks
    for biomedical image segmentation. In Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18 (pp. 234-241). Springer International Publishing.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ronneberger, O., Fischer, P. and Brox, T., 2015. U-net: Convolutional networks
    for biomedical image segmentation. In Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18 (pp. 234-241). Springer International Publishing.'
- en: 'Bansal, Arpit, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong
    Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. “Cold diffusion: Inverting
    arbitrary image transforms without noise.” arXiv preprint arXiv:2208.09392 (2022).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 'Bansal, Arpit, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong
    Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. “Cold diffusion: Inverting
    arbitrary image transforms without noise.” arXiv preprint arXiv:2208.09392 (2022).'
- en: 'Hoogeboom, Emiel, Jonathan Heek, and Tim Salimans. “simple diffusion: End-to-end
    diffusion for high resolution images.” arXiv preprint arXiv:2301.11093 (2023).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hoogeboom, Emiel, Jonathan Heek, and Tim Salimans. “simple diffusion: End-to-end
    diffusion for high resolution images.” arXiv preprint arXiv:2301.11093 (2023).'
- en: 'Chang, Huiwen, Han Zhang, Jarred Barber, A. J. Maschinot, Jose Lezama, Lu Jiang,
    Ming-Hsuan Yang et al. “Muse: Text-To-Image Generation via Masked Generative Transformers.”
    arXiv preprint arXiv:2301.00704 (2023).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chang, Huiwen, Han Zhang, Jarred Barber, A. J. Maschinot, Jose Lezama, Lu Jiang,
    Ming-Hsuan Yang et al. “Muse: Text-To-Image Generation via Masked Generative Transformers.”
    arXiv preprint arXiv:2301.00704 (2023).'
- en: 'Chang, Huiwen, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. “Maskgit:
    Masked generative image transformer.” In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 11315-11325\. 2022.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chang, Huiwen, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. “Maskgit:
    Masked generative image transformer.” In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 11315-11325. 2022.'
- en: Rampas, Dominic, Pablo Pernias, Elea Zhong, and Marc Aubreville. “Fast Text-Conditional
    Discrete Denoising on Vector-Quantized Latent Spaces.” arXiv preprint arXiv:2211.07292
    (2022).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Rampas, Dominic, Pablo Pernias, Elea Zhong, and Marc Aubreville. “Fast Text-Conditional
    Discrete Denoising on Vector-Quantized Latent Spaces.” arXiv preprint arXiv:2211.07292
    (2022).
- en: Chen, Ting “On the Importance of Noise Scheduling for Diffusion Models.” arXiv
    preprint arXiv:2301.10972 (2023).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Chen, Ting “On the Importance of Noise Scheduling for Diffusion Models.” arXiv
    preprint arXiv:2301.10972 (2023).
- en: Peebles, William, and Saining Xie. “Scalable Diffusion Models with Transformers.”
    arXiv preprint arXiv:2212.09748 (2022).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Peebles, William, and Saining Xie. “Scalable Diffusion Models with Transformers.”
    arXiv preprint arXiv:2212.09748 (2022).
- en: Jabri, Allan, David Fleet, and Ting Chen. “Scalable Adaptive Computation for
    Iterative Generation.” arXiv preprint arXiv:2212.11972 (2022).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Jabri, Allan, David Fleet, and Ting Chen. “Scalable Adaptive Computation for
    Iterative Generation.” arXiv preprint arXiv:2212.11972 (2022).
- en: Salimans, Tim, and Jonathan Ho. “Progressive distillation for fast sampling
    of diffusion models.” arXiv preprint arXiv:2202.00512 (2022).)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Salimans, Tim, and Jonathan Ho. “Progressive distillation for fast sampling
    of diffusion models.” arXiv preprint arXiv:2202.00512 (2022).)
- en: Karras, Tero, Miika Aittala, Timo Aila, and Samuli Laine. “Elucidating the design
    space of diffusion-based generative models.” arXiv preprint arXiv:2206.00364 (2022).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Karras, Tero, Miika Aittala, Timo Aila, and Samuli Laine. “Elucidating the design
    space of diffusion-based generative models.” arXiv preprint arXiv:2206.00364 (2022).
