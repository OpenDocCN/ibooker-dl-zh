- en: 2 Clustering techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 聚类技术
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Clustering techniques and salient use cases in the industry
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类技术及其在行业中的显著应用案例
- en: Simple k-means, hierarchical, and density-based spatial clustering algorithms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的k-means、层次和基于密度的空间聚类算法
- en: Implementation of algorithms in Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python中算法的实现
- en: A case study on cluster analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于聚类分析的案例研究
- en: Simplicity is the ultimate sophistication.—Leonardo da Vinci
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 简单即复杂。——达芬奇
- en: Nature loves simplicity and teaches us to follow the same path. Most of the
    time, our decisions are simple choices. Simple solutions are easier to comprehend,
    less time-consuming, and painless to maintain and ponder over. The machine learning
    world is no different. An elegant machine learning solution is not the one that
    is the most complicated algorithm available but the one that solves the business
    problem. A robust machine learning solution is easy enough to readily decipher
    and pragmatic enough to implement. Clustering solutions are generally easier to
    understand.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自然喜欢简单，并教导我们遵循相同的道路。大多数时候，我们的决策都是简单的选择。简单的解决方案更容易理解，耗时更少，维护和思考起来也更轻松。机器学习世界也不例外。优雅的机器学习解决方案不是最复杂的算法，而是解决业务问题的解决方案。稳健的机器学习解决方案足够简单，可以轻松解码，并且足够实用，可以实施。聚类解决方案通常更容易理解。
- en: 'In the previous chapter, we defined unsupervised learning and discussed the
    various unsupervised algorithms available. We will cover each of those algorithms
    as we work through this book; in this second chapter, we focus on the first of
    these: clustering algorithms.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们定义了无监督学习并讨论了可用的各种无监督算法。随着我们阅读本书，我们将涵盖这些算法中的每一个；在这一章中，我们专注于这些中的第一个：聚类算法。
- en: We will define clustering first and then study the different types of clustering
    techniques. We will examine the mathematical foundation, accuracy measurements,
    and pros and cons of each algorithm. We will implement three of these algorithms
    using Python code on a dataset to complement theoretical knowledge. The chapter
    ends with the various use cases of clustering techniques in the pragmatic business
    scenario to prepare for the actual business world. This technique is followed
    throughout the book—we study the concepts first, implement the actual code to
    enhance the Python skills, and then dive into real-world business problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义聚类，然后研究不同的聚类技术类型。我们将检查每个算法的数学基础、准确度测量以及优缺点。我们将使用Python代码在数据集上实现这三种算法，以补充理论知识。本章以聚类技术在实际业务场景中的各种应用案例结束，为实际商业世界做准备。本书中始终遵循这一技术——我们首先研究概念，然后实现实际代码以增强Python技能，最后深入现实世界的商业问题。
- en: We study basic clustering algorithms in this chapter, which are k-means clustering,
    hierarchical clustering, and density-based spatial clustering of applications
    with noise (DBSCAN) clustering. These clustering algorithms are generally the
    starting points whenever we want to study clustering. In the later chapters of
    the book, we will explore more complex algorithms like spectrum clustering, Gaussian
    mixture models, time series clustering, fuzzy clustering, and others. If you have
    a good understanding of k-means clustering, hierarchical clustering, and DBSCAN,
    you can skip to the next chapter. Still, it is advisable to read this chapter
    once—you might find something useful to refresh your concepts!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们研究基本的聚类算法，包括k-means聚类、层次聚类和基于密度的空间聚类应用噪声（DBSCAN）聚类。这些聚类算法通常是我们想要研究聚类时的起点。在本书的后续章节中，我们将探索更复杂的算法，如谱聚类、高斯混合模型、时间序列聚类、模糊聚类等。如果您对k-means聚类、层次聚类和DBSCAN有很好的理解，您可以跳到下一章。然而，建议您至少阅读这一章——您可能会发现一些有用的内容来刷新您的概念！
- en: Let’s first understand what we mean by clustering. Good luck on your journey
    to master unsupervised learning–based clustering techniques!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解我们所说的聚类是什么。祝您在掌握基于无监督学习的聚类技术之路上好运！
- en: 2.1 Technical toolkit
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 技术工具箱
- en: We use the latest version of Python in this chapter. A basic understanding of
    Python and code execution is expected. You are advised to refresh your knowledge
    of object-oriented programming and Python.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用Python的最新版本。预期您对Python和代码执行有基本了解。建议您复习面向对象编程和Python的知识。
- en: Throughout the book, we use Jupyter Notebook to execute the code. Jupyter offers
    flexibility in execution and debugging. It is quite user-friendly and is platform
    or operating-system agnostic. So, if you are using Windows, macOS, or Linux, Jupyter
    should work just fine.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们使用Jupyter Notebook来执行代码。Jupyter在执行和调试方面提供了灵活性。它非常用户友好，并且与平台或操作系统无关。因此，如果你使用Windows、macOS或Linux，Jupyter应该可以正常工作。
- en: 'All the datasets and code files are checked into the GitHub repository at [https://mng.bz/lYq2](https://mng.bz/lYq2).
    You need to install the following Python libraries to execute the code: `numpy`,
    `pandas`, `matplotlib`, `scipy`, and `sklearn`. CPU is good enough for execution,
    but if you face some computing lags and would like to speed up the execution,
    switch to GPU or Google Collaboratory (Colab). Google Colab offers free computation
    for machine learning solutions. I recommend studying more about Google Colab and
    how to use it for training machine learning algorithms.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据集和代码文件都已提交到GitHub仓库[https://mng.bz/lYq2](https://mng.bz/lYq2)。你需要安装以下Python库来执行代码：`numpy`、`pandas`、`matplotlib`、`scipy`和`sklearn`。CPU足以执行，但如果你遇到一些计算延迟并且想要加快执行速度，切换到GPU或Google
    Collaboratory（Colab）。Google Colab为机器学习解决方案提供免费计算。我建议更多地了解Google Colab以及如何用它来训练机器学习算法。
- en: 2.2 Clustering
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 聚类
- en: 'Consider this scenario: a group of children is asked to group the items in
    a room into different segments. Each child can use their own logic. Some might
    group the objects based on weight; other children might use material or color;
    while yet others might use all three: weight, material, and color. There are many
    permutations, and they depend on the parameters used for grouping. Here, a child
    is segmenting or clustering objects based on the chosen logic.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下场景：一群孩子被要求将房间里的物品分组到不同的段。每个孩子可以使用自己的逻辑。有些人可能会根据重量分组物体；其他孩子可能会使用材料或颜色；而还有一些孩子可能会使用所有三个：重量、材料和颜色。有无数种排列组合，并且它们取决于分组所使用的参数。在这里，一个孩子是根据选择的逻辑对物体进行分段或聚类的。
- en: Formally put, *clustering* is used to group objects with similar attributes
    in the same segments and objects with different attributes in different segments.
    The resultant clusters share similarities within themselves while they are more
    heterogeneous between each other. We can understand this better by looking at
    figure 2.1\.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，*聚类*是用来将具有相似属性的物体分组到同一段，而将具有不同属性的物体分组到不同段。结果簇在自身内部具有相似性，而在彼此之间则更加异质。我们可以通过查看图2.1来更好地理解这一点。
- en: '![figure](../Images/CH02_F01_Verdhan.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F01_Verdhan.png)'
- en: Figure 2.1 Clustering is grouping objects with similar attributes into logical
    segments. The grouping is based on a similar trait shared by different observations,
    and hence they are gathered into a group. We are using shape as a variable for
    clustering here.
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 聚类是将具有相似属性的物体分组到逻辑段。分组基于不同观察结果共享的相似特征，因此它们被聚集到一组。在这里，我们使用形状作为聚类的变量。
- en: Cluster analysis is not one individual algorithm or solution; rather it is used
    as a problem-solving mechanism in practical business scenarios. It is a class
    of algorithms under unsupervised learning and an iterative process following a
    logical approach and qualitative business inputs. It results in the generation
    of a thorough understanding of the data and the logical patterns in it, pattern
    discovery, and information retrieval. As an unsupervised approach, clustering
    does not need a target variable. It performs segmenting by analyzing underlying
    patterns in the dataset, which are generally multidimensional and, hence, difficult
    to analyze with traditional methods.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析不是一个单独的算法或解决方案；相反，它在实际商业场景中用作解决问题的机制。它是无监督学习下的一类算法，是一个遵循逻辑方法和定性商业输入的迭代过程。它导致对数据及其逻辑模式的深入理解，模式发现和信息检索。作为一种无监督方法，聚类不需要目标变量。它通过分析数据集中的潜在模式进行分段，这些模式通常是多维的，因此难以用传统方法分析。
- en: 'Ideally, we want the clustering algorithms to have the following attributes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望聚类算法具有以下属性：
- en: The output clusters should be easy to explain and comprehend, usable, and make
    business sense. The number of clusters should not be too few or too many. For
    example, it is not ideal to have only two clusters, and the division is not clear
    and decisive. On the other hand, if we have 20 clusters, handling them will become
    a challenge.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出的聚类应该易于解释和理解，易于使用，并且具有商业意义。聚类数量不应过多或过少。例如，只有两个聚类并不理想，且划分不清晰和决断。另一方面，如果我们有20个聚类，处理起来将变得具有挑战性。
- en: The algorithm should not be too sensitive to outliers or missing values or the
    noise in the dataset. Generally put, a good solution will be able to handle multiple
    data types.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法不应过于敏感于异常值、缺失值或数据集中的噪声。一般来说，一个好的解决方案将能够处理多种数据类型。
- en: It is advisable for a data analyst/scientist to have a good grip on the business
    domain, althougha good clustering solution may allow analysts with less domain
    understanding to train the clustering algorithm.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于数据分析师/科学家来说，掌握业务领域是明智的，尽管一个好的聚类解决方案可能允许对领域理解较少的分析员训练聚类算法。
- en: The algorithm should be independent of the order of the input parameters. If
    the order matters, the clustering is biased on the order and hence will add more
    confusion to the process.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法应独立于输入参数的顺序。如果顺序很重要，聚类将受到顺序的偏差，从而给过程增加更多混乱。
- en: As we generate new datasets continuously, the clusters should be scalable to
    newer training examples and should not be a time-consuming process.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着我们持续生成新的数据集，聚类应该能够扩展到新的训练示例，并且不应是耗时的过程。
- en: As one could imagine, the clustering output will depend on the attributes used
    for grouping. In figure 2.2, there can be two logical groupings for the same dataset,
    and both are equally valid. Hence, it is prudent that the attributes or *variables*
    for clustering are chosen wisely, and often that decision depends on the business
    problem at hand.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如同想象中的那样，聚类输出将取决于用于分组的属性。在图2.2中，对于同一数据集，可能存在两种逻辑分组，且两者都是有效的。因此，明智的做法是谨慎选择用于聚类的属性或*变量*，通常这个决定取决于手头的业务问题。
- en: '![figure](../Images/CH02_F02_Verdhan.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F02_Verdhan.png)'
- en: Figure 2.2 Using different attributes for clustering results in different clusters
    for the same dataset. Hence, choosing the correct set of attributes defines the
    final set of results we will achieve.
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 使用不同的属性进行聚类会导致同一数据集产生不同的聚类结果。因此，选择正确的属性集定义了我们最终将获得的结果集。
- en: Along with the attributes used in clustering, the actual technique used also
    makes a big difference. There are quite a few (in fact, more than 80) clustering
    techniques. For the interested audience, we provide a list of all the clustering
    algorithms in the appendix.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于聚类的属性外，实际使用的技术也会产生很大的影响。实际上，有相当多的（实际上超过80种）聚类技术。对于感兴趣的读者，我们在附录中提供了所有聚类算法的列表。
- en: Clustering can be achieved using a variety of algorithms. These algorithms use
    different methodologies to define similarity between objects—for example, density-based
    clustering, centroid-based clustering, distribution-based methods, and others.
    Multiple techniques, such as Euclidean distance, Manhattan distance, etc., are
    available to measure the distance between objects. The choice of distance measurement
    leads to different similarity scores. We will study these similarity measurement
    parameters in a later section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以通过多种算法实现。这些算法使用不同的方法来定义对象之间的相似性——例如，基于密度的聚类、基于质心的聚类、基于分布的方法等。有多种技术，如欧几里得距离、曼哈顿距离等，可用于测量对象之间的距离。距离测量的选择会导致不同的相似度得分。我们将在后面的章节中研究这些相似度测量参数。
- en: 'At a high level, we can identify two broad clustering methods: *hard clustering*
    and *soft clustering* (see figure 2.3). When the decision is quite clear that
    an object belongs to a certain class or cluster, it is referred to as hard clustering.
    In hard clustering, an algorithm is quite sure of an object’s class. On the other
    hand, soft clustering assigns a likelihood score for an object belonging to a
    particular cluster. So, a soft clustering method will not put an object into a
    cluster; rather, an object can belong to multiple clusters. Soft clustering sometimes
    is also called *fuzzy* clustering.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，我们可以识别两种广泛的聚类方法：*硬聚类*和*软聚类*（见图2.3）。当决策非常明确，一个对象属于某个类别或聚类时，它被称为硬聚类。在硬聚类中，算法对对象的类别非常确定。另一方面，软聚类为对象属于特定聚类分配一个可能性分数。因此，软聚类方法不会将对象放入一个聚类中；相反，一个对象可以属于多个聚类。软聚类有时也被称为*模糊*聚类。
- en: '![figure](../Images/CH02_F03_Verdhan.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F03_Verdhan.png)'
- en: Figure 2.3 Hard clustering has distinct clusters, whereas in the case of soft
    clustering, a data point can belong to multiple clusters, and we get a likelihood
    score for a data point to belong to a cluster. The figure on the left is hard
    clustering, and the one on the right is soft clustering.
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3硬聚类有明显的聚类，而在软聚类的案例中，一个数据点可以属于多个聚类，我们得到一个数据点属于聚类的可能性分数。左边的图是硬聚类，右边的图是软聚类。
- en: We can broadly classify the clustering techniques as shown in table 2.1\. The
    methods described are not the only ones available. We can have graph-based models,
    overlapping clustering, subspace models, etc.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将聚类技术大致分类，如表2.1所示。描述的方法并非唯一。我们可以有基于图模型、重叠聚类、子空间模型等。
- en: Table 2.1 Classification of clustering methodologies, brief descriptions, and
    examples
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.1聚类方法分类、简要描述和示例
- en: '| Serial no. | Clustering methodology | Brief description of the method | Example
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 序号 | 聚类方法 | 方法的简要描述 | 示例 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1  | Centroid-based clustering  | Distance from a defined centroid  | k-means  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 1  | 基于质心的聚类  | 与定义的质心的距离  | k-means  |'
- en: '| 2  | Density-based models  | Data points are connected in dense regions in
    a vector space  | DBSCAN, OPTICS  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 2  | 基于密度的模型  | 数据点在向量空间中的密集区域连接  | DBSCAN, OPTICS  |'
- en: '| 3  | Connectivity-based clustering  | Distance connectivity is the modus
    operandi  | Hierarchical clustering, balanced iterative reducing and clustering
    using hierarchies  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 3  | 基于连接性的聚类  | 连接距离是操作方式  | 层次聚类、平衡迭代减少和基于层次结构的聚类  |'
- en: '| 4  | Distribution models  | Modeling is based on statistical distributions  |
    Gaussian mixture models  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 4  | 分布模型  | 建模基于统计分布  | 高斯混合模型  |'
- en: '| 5  | Deep learning models  | Unsupervised neural network based  | Self-organizing
    maps  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 5  | 深度学习模型  | 基于无监督神经网络的  | 自组织映射  |'
- en: 'Generally, the six most popular algorithms used in clustering in the industry
    are as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在工业界中用于聚类的最流行的六个算法如下：
- en: k-means clustering (with variants like k-medians, k-medoids)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means聚类（包括k-medians、k-medoids等变体）
- en: Agglomerative clustering or hierarchical clustering
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类层次聚类或层次聚类
- en: DBSCAN
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN
- en: Spectral clustering
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谱聚类
- en: Gaussian mixture models
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: Balanced iterative reducing and clustering using hierarchies
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡迭代减少和基于层次结构的聚类
- en: Multiple other algorithms are available, like Chinese whisper, canopy clustering,
    SUBCLU, FLAME, and others. We will study the first three algorithms in this chapter
    and some of the advanced ones in subsequent chapters in the book.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他算法可用，如中国 whispers、 canopy聚类、SUBCLU、FLAME等。我们将在本章中研究前三种算法，并在本书的后续章节中研究一些高级算法。
- en: Exercise 2.1
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习2.1
- en: 'Use these questions to check your understanding:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下问题来检查你的理解：
- en: DBSCAN clustering is a centroid-based clustering technique. True or False?
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DBSCAN聚类是一种基于质心的聚类技术。对还是错？
- en: Clustering is a supervised learning technique with a fixed target variable.
    True or False?
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类是一种具有固定目标变量的监督学习技术。对还是错？
- en: What is the difference between hard clustering and soft clustering?
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬聚类和软聚类之间有什么区别？
- en: 2.3 Centroid-based clustering
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 基于质心的聚类
- en: Centroid-based algorithms measure the similarity of the objects based on their
    distance to the centroid of the clusters (for more information on centroids, see
    the appendix). The distance is measured between a specific data point to the centroid
    for the cluster. The smaller the distance, the higher the similarity. We can understand
    the concept by looking at figure 2.4\. The figure on the right side represents
    the respective centroids for each of the group of clusters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基于质心的算法根据对象到簇质心的距离来衡量它们的相似度（有关质心的更多信息，请参阅附录）。距离是测量特定数据点到簇质心的距离。距离越小，相似度越高。我们可以通过查看图2.4来理解这个概念。右侧的图表示每个簇组的相应质心。
- en: TIP  To get more clarity on the concept of centroid and other mathematical concepts,
    refer to the appendix.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: TIP　为了更清楚地了解质心和其他数学概念，请参阅附录。
- en: '![figure](../Images/CH02_F04_Verdhan.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F04_Verdhan.png)'
- en: Figure 2.4 Centroid-based clustering methods create a centroid for the respective
    clusters, and the similarity is measured based on the distance from the centroid.
    In this case, we have five centroids; hence, we have five distinct clusters.
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4 基于质心的聚类方法为各自的簇创建一个质心，相似度是根据质心的距离来衡量的。在这种情况下，我们有五个质心，因此我们有五个不同的簇。
- en: 'In clustering, distance plays a central role as many algorithms use it as a
    metric to measure the similarity. In centroid-based clustering, distance is measured
    between points and between centroids. There are multiple ways to measure the distance.
    The most widely used are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，距离起着核心作用，因为许多算法将其用作度量来衡量相似度。在基于质心的聚类中，距离是在点与点之间以及点与质心之间测量的。有多种方法可以测量距离。最常用的如下：
- en: '*Euclidean distance*—This is the most common distance metric used. It represents
    the straight-line distance between the two points in space and is the shortest
    path between the two points. For example, if we want to calculate the distance
    between points *P*[1] and *P*[2] where coordinates are (*x*[1], *y*[1]) for *P*[1]
    and (*x*[2], *y*[2]) for *P*[2], Euclidean distance is given by equation 2.1\.
    The geometric representation is shown in figure 2.5:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*欧几里得距离*——这是最常用的距离度量。它表示空间中两点之间的直线距离，是两点之间的最短路径。例如，如果我们想计算点 *P*[1] 和 *P*[2]
    之间的距离，其中 *P*[1] 的坐标是 (*x*[1], *y*[1])，*P*[2] 的坐标是 (*x*[2], *y*[2])，欧几里得距离由方程2.1给出。几何表示如图2.5所示：'
- en: (2.1)
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (2.1)
- en: Distance = √(*y*[2] – *y*[1])[²] + (*x*[2] – *x*[1])[²]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 距离 = √(*y*[2] – *y*[1])[²] + (*x*[2] – *x*[1])[²]
- en: '*Chebyshev distance*—Named after Russian mathematician Pafnuty Chebyshev, this
    is defined as the distance between two points such that their differences are
    maximum value along any coordinate dimension. Mathematically, we can represent
    Chebyshev distance in equation 2.2 and as shown in figure 2.5:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*切比雪夫距离*——以俄罗斯数学家帕夫努提·切比雪夫的名字命名，定义为两点之间的距离，其差异是任何坐标维度的最大值。数学上，我们可以用方程2.2表示切比雪夫距离，如图2.5所示：'
- en: (2.2)
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (2.2)
- en: Chebyshev distance = max (|*y*[2] – *y*[1]|, |*x*[2] – *x*[1]|)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 切比雪夫距离 = max(|*y*[2] – *y*[1]|, |*x*[2] – *x*[1]|)
- en: '![figure](../Images/CH02_F05_Verdhan.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F05_Verdhan.png)'
- en: Figure 2.5 Euclidean distance, Chebyshev distance, Manhattan distance, and cosine
    distance are the primary distance metrics used. Note how the distance is different
    for two points using these metrics. In Euclidean distance, the direct distance
    is measured between two points, as shown by the first figure on the left.
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 欧几里得距离、切比雪夫距离、曼哈顿距离和余弦距离是主要的距离度量。注意使用这些度量时两点之间的距离如何不同。在欧几里得距离中，两点之间的直接距离是测量的，如左图的第一幅图所示。
- en: '*Manhattan distance*—This is a very easy concept. It simply calculates the
    distance between two points in a grid-like path, and the distance is hence measured
    along the axes at right angles. Hence, sometimes it is also referred to as city
    block distance or the taxicab metric. Mathematically, we can represent the Manhattan
    distance in equation 2.3 and as shown in figure 2.5:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*曼哈顿距离*——这是一个非常容易理解的概念。它简单地计算两个点在网格状路径上的距离，因此距离是沿着垂直于轴线的方向测量的。因此，有时它也被称为街区距离或出租车度量。数学上，我们可以用方程2.3表示曼哈顿距离，如图2.5所示：'
- en: (2.3)
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (2.3)
- en: Manhattan distance = (|*y*[2] – *y*[1]| + |*x*[2] – *x*[1]|)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离 = (|*y*[2] – *y*[1]| + |*x*[2] – *x*[1]|)
- en: Manhattan distance is in L1 norm form while Euclidean distance is in L2 norm
    form. Refer to the appendix to study the L1 norm and L2 norm in detail. If we
    have a high number of dimensions or variables in the dataset, Manhattan distance
    is a better choice than Euclidean distance. This is due to the *curse of dimensionality,*
    which we will study in chapter 3.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离是L1范数形式，而欧几里得距离是L2范数形式。请参阅附录以详细了解L1范数和L2范数。如果我们有大量维度或变量在数据集中，曼哈顿距离比欧几里得距离更好。这是由于“维度诅咒”，我们将在第3章中研究。
- en: '*Cosine distance*—Cosine distance is used to measure the similarity between
    two points in a vector-space diagram. In trigonometry, the cosine of 0 is 1 and
    the cosine of 90⁰ is 0\. Hence, if two points are similar to each other, the angle
    between them will be zero; hence, cosine will be 1, which means the two points
    are very similar to each other and vice versa. Mathematically, cosine similarity
    is shown in equation 2.4\. If we want to measure the cosine between two vectors
    *A* and *B*, then cosine is'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*余弦距离*——余弦距离用于测量向量空间图中两点之间的相似度。在三角学中，0度的余弦值为1，90度的余弦值为0。因此，如果两点彼此相似，它们之间的角度将是零；因此，余弦值为1，这意味着两点非常相似，反之亦然。数学上，余弦相似度在方程2.4中显示。如果我们想测量向量*A*和*B*之间的余弦值，那么余弦值是'
- en: (2.4)
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (2.4)
- en: Cosine distance = (*A* . *B*) / (||*A*|| ||*B*||)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦距离 = (*A* . *B*) / (||*A*|| ||*B*||)
- en: TIP  If you want to refresh your knowledge on the concepts of vector factorization,
    refer to the appendix.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：如果您想刷新关于向量分解概念的知识，请参阅附录。
- en: Other distance-measuring metrics, such as Hamming distance, Jaccard distance,
    and others, are available. Mostly, we use Euclidean distance in our pragmatic
    business problems, but other distance metrics are also used sometimes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其他距离度量标准，如汉明距离、杰卡德距离等，也是可用的。在大多数情况下，我们在实际业务问题中使用欧几里得距离，但有时也会使用其他距离度量标准。
- en: Note  These distance metrics are true for other clustering algorithms too. I
    recommend testing the Python codes in the book with different distance metrics
    and comparing the performance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这些距离度量标准也适用于其他聚类算法。我建议您使用书中的Python代码测试不同的距离度量标准，并比较性能。
- en: Now that we understand the various distance metrics, we proceed to study k-means
    clustering, which is the most widely used algorithm.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了各种距离度量标准，我们继续研究k-means聚类，这是最广泛使用的算法。
- en: 2.3.1 K-means clustering
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 K-means聚类
- en: K-means clustering is an easy and straightforward approach. It is arguably the
    most widely used clustering method to segment data points and create nonoverlapping
    clusters. We have to specify the number of clusters *k* we wish to create as an
    input, and the algorithm will associate each observation to exactly one of the
    k clusters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类是一种简单直接的方法。它可以说是最广泛使用的聚类方法，用于分割数据点并创建非重叠的聚类。我们必须指定我们希望创建的聚类数量*k*作为输入，算法将把每个观测值与k个聚类中的确切一个关联起来。
- en: Note  K-means clustering is sometimes confused with the k-nearest neighbor (KNN)
    classifier. Although there is some relationship between the two, KNN is used for
    classification and regression problems.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：K-means聚类有时会与k近邻（KNN）分类器混淆。尽管两者之间有一些关系，但KNN用于分类和回归问题。
- en: K-means clustering is quite an elegant approach and starts with some initial
    cluster centers and then iterates to assign each observation to the closest center.
    In the process, the centers are recalculated as the mean of points in the cluster.
    Let’s study the approach used in a step-by-step fashion by using the diagram in
    figure 2.6\. For the sake of simplicity, we are assuming that there are three
    clusters in the dataset.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类是一种相当优雅的方法，它从一些初始聚类中心开始，然后迭代地将每个观测值分配到最近的中心。在这个过程中，中心点被重新计算为聚类中点的平均值。让我们通过使用图2.6中的图表以逐步的方式研究这种方法。为了简化，我们假设数据集中有三个聚类。
- en: 'The steps are as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: Let’s assume that we have all the data points, as shown in step 1\.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们已经拥有了所有数据点，如图1所示。
- en: 'The three centers are initialized randomly, as shown by three squares: blue,
    red, and green. This input of three is the final number of clusters we wish to
    have.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 三个中心点被随机初始化，如图中三个正方形所示：蓝色、红色和绿色。这三个输入值是我们希望拥有的最终聚类数量。
- en: '![figure](../Images/CH02_F06_Verdhan.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F06_Verdhan.png)'
- en: Figure 2.6 Step 1 represents the raw dataset. In step 2, the algorithm initiates
    three random centroids as we have given the input of the number of clusters as
    three. In step 3, all the neighboring points of the centroids are assigned to
    the same cluster.
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 步骤1表示原始数据集。在步骤2中，算法根据我们输入的聚类数量为三个，初始化了三个随机质心。在步骤3中，所有质心的邻近点被分配到相同的簇。
- en: 3\. The distance of all the data points is calculated to the centers, and the
    points are assigned to the nearest center. Note that the points have attained
    blue, red, and green colors as they are nearest to those respective centers. (The
    colors are not distinguishable in the print version; hence we have grouped them
    together.)
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 所有数据点到中心的距离被计算，并将点分配到最近的中心。请注意，这些点已经根据它们各自最近的中心获得了蓝色、红色和绿色，因为它们离这些中心最近。（在打印版本中颜色不可区分；因此我们将它们分组在一起。）
- en: 4\. The three centers are readjusted in this step. The centers are recalculated
    as the mean of the points in that cluster, as shown in figure 2.7\. We can see
    that in step 4, the three squares have changed their respective positions as compared
    to step 3.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 在这个步骤中，三个中心被重新调整。中心被重新计算为该簇中点的平均值，如图2.7所示。我们可以看到，在步骤4中，与步骤3相比，三个正方形已经改变了它们的位置。
- en: '![figure](../Images/CH02_F07_Verdhan.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F07_Verdhan.png)'
- en: Figure 2.7 The centroids are recalculated in step 4\. In step 5, the data points
    are again reassigned new centers. In step 6, the centroids are again readjusted
    as per the new calculations.
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7 在步骤4中重新计算了质心。在步骤5中，数据点再次被分配到新的中心。在步骤6中，根据新的计算再次调整了质心。
- en: 5\. The distance of all the data points is recalculated to the new centers and
    the points are reassigned to the nearest centers again. Note that two blue data
    points have become red while a red point has become green in this step.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 所有数据点到新中心的距离被重新计算，并且点再次被分配到最近的中心。请注意，在这个步骤中，两个蓝色数据点变成了红色，而一个红色点变成了绿色。
- en: 6\. The centers are again readjusted as they were in step 4\.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 中心再次调整，就像步骤4中一样。
- en: 7\. The data points are again assigned a new cluster, as shown in figure 2.8\.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 7. 数据点再次被分配到一个新的簇，如图2.8所示。
- en: 8\. The process will continue until convergence is achieved. In other words,
    the process continues until there is no more reassignment of the data points;
    hence, we cannot improve the clustering further, and the final clustering is achieved.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 8. 过程将继续，直到收敛。换句话说，过程将继续，直到没有更多的数据点重新分配；因此，我们不能再进一步改善聚类，最终聚类完成。
- en: '![figure](../Images/CH02_F08_Verdhan.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F08_Verdhan.png)'
- en: Figure 2.8 The centroids are recalculated, and this process continues until
    we can no longer improve the clustering. Then the process stops, as shown in step
    8.
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8 质心被重新计算，这个过程会一直持续到我们不能再改善聚类效果。然后，过程停止，如图8所示。
- en: The objective of k-means clustering is to ensure that the within-cluster variation
    is as small as possible while the difference between clusters is as big as possible.
    In other words, the members of the same cluster are most similar to each other,
    while members in different clusters are dissimilar. Once the results no longer
    change, we can conclude that a local optimum has been reached, and clustering
    can stop. Hence, the final clusters are homogeneous within themselves while heterogeneous
    with each other.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类的目标是确保簇内变异尽可能小，而簇间的差异尽可能大。换句话说，同一簇的成员之间最为相似，而不同簇的成员之间则不相似。一旦结果不再变化，我们可以得出结论，已经达到了局部最优，聚类可以停止。因此，最终的簇在自身内部是同质的，而与其他簇是异质的。
- en: 'It is imperative to note two points here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个要点至关重要：
- en: K-means clustering initializes the centers randomly; hence it finds a local
    optimum solution rather than a global optimum solution. Thus it is advisable to
    iterate the solution multiple times and choose the best output from all the results.
    By iteration, we mean to repeat the process multiple times, as in each of the
    iterations, the centroid chosen randomly will be different.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means聚类随机初始化中心；因此，它找到一个局部最优解而不是全局最优解。因此，建议多次迭代解决方案，并从所有结果中选择最佳输出。通过迭代，我们指的是重复这个过程多次，因为在每次迭代中，随机选择的质心都会不同。
- en: We have to input the number of final clusters *k* we wish to have, and it changes
    the output drastically. A very small value of *k* relative to the data size will
    result in redundant clusters as they will not be of any use. In other words, if
    we have a very small value of *k* relative to big-sized data, data points with
    different characteristics will be cobbled together in a few groups. Having a very
    high value of *k* will create clusters that are different from each other minutely.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须输入我们希望拥有的最终聚类数*k*，这将极大地改变输出结果。与数据量相比，*k*的值非常小会导致冗余聚类，因为它们将没有任何用处。换句话说，如果我们相对于大量数据有非常小的*k*值，具有不同特征的数据点将被组合在几个组中。*k*的值非常高将创建彼此之间细微差异的聚类。
- en: Moreover, having a very high number of clusters will be difficult to manage
    and refresh in the long run. Let’s study an example. If a telecom operator has
    1 million subscribers, and if we take the number of clusters as two or three,
    the resultant cluster size will be very large. This can also lead to different
    customers classified in the same segment. On the other hand, if we take the number
    of clusters as 50 or 60, due to the sheer number of clusters, the output becomes
    unmanageable to use, analyze, and maintain.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，拥有一个非常高的聚类数在长期内将难以管理和更新。让我们研究一个例子。如果一个电信运营商有100万用户，如果我们把聚类数定为两个或三个，结果聚类的大小将会非常大。这也可能导致同一细分市场中的不同客户被分类。另一方面，如果我们把聚类数定为50或60，由于聚类数量庞大，输出结果将难以使用、分析和维护。
- en: With different values of *k*, we get different results; hence, it is necessary
    that we understand how we can choose the optimum number of clusters for a dataset.
    Now let’s examine the process to measure the accuracy of clustering solutions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 随着不同的*k*值，我们得到不同的结果；因此，了解我们如何为数据集选择最佳聚类数是必要的。现在让我们来检查衡量聚类解决方案准确性的过程。
- en: 2.3.2 Measuring the accuracy of clustering
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 聚类准确性的度量
- en: 'One objective of clustering is to find the cleanest clusters. Theoretically
    (though not ideally), if we have the same number of clusters as the number of
    observations, the results will be completely accurate. In other words, if we have
    1 million customers, the purest clustering will have 1 million clusters, wherein
    each customer is in a separate cluster. But it is not the best approach and is
    not a pragmatic solution either. Clustering intends to create a group of similar
    observations in one cluster, and we use the same principle to measure the accuracy
    of our solution. Other options include the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的一个目标是要找到最干净的聚类。理论上（尽管不是理想情况），如果我们有与观察数相同的聚类数，结果将完全准确。换句话说，如果我们有100万客户，最纯的聚类将会有100万个聚类，其中每个客户都在一个单独的聚类中。但这不是最佳方法，也不是一个实际的解决方案。聚类旨在在一个聚类中创建一组相似的观察值，我们使用相同的原理来衡量我们解决方案的准确性。其他选项包括以下内容：
- en: '*Within the cluster sum of squares (WCSS) or cohesion*—This index measures
    the variability of the data points with respect to the distance they are from
    the centroid of the cluster. This metric is the average distance of each data
    point from the cluster’s centroid, which is repeated for each data point. If the
    value is too large, it shows there is a large data spread, whereas the smaller
    value indicates that the data points are quite similar and homogeneous and hence
    the cluster is compact.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类内平方和（WCSS）或凝聚力*——这个指标衡量数据点相对于它们与聚类质心的距离的变异性。这个度量是每个数据点到聚类质心的平均距离，对每个数据点重复进行。如果值太大，表明数据分布范围很大，而较小的值则表明数据点相当相似和同质，因此聚类是紧凑的。'
- en: Sometimes, this intracluster distance is also referred to as *inertia* for that
    cluster. It is simply the summation of all the distances. The lower the value
    of inertia, the better the cluster is.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，这种聚类内的距离也被称为该聚类的*惯性*。它只是所有距离的总和。惯性值越低，聚类越好。
- en: '*Intercluster sum of squares*—This metric is used to measure the distance between
    centroids of all the clusters. To get it, we measure the distance between centroids
    of all the clusters and divide it by the number of clusters to get the average
    value. The bigger it is, the better the clustering is, indicating that clusters
    are heterogeneous and distinguishable from each other, as represented in figure
    2.9.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*簇间平方和*——这个指标用于衡量所有簇的质心之间的距离。为了得到它，我们测量所有簇的质心之间的距离，并将其除以簇的数量以得到平均值。它的值越大，表示聚类越好，表明簇是异质且彼此可区分的，如图2.9所示。'
- en: '![figure](../Images/CH02_F09_Verdhan.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F09_Verdhan.png)'
- en: Figure 2.9 Intracluster vs. intercluster distance. Both are used to measure
    the purity of the final clusters and the performance of the clustering solution.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9 簇内距离与簇间距离。两者都用于衡量最终簇的纯度和聚类解决方案的性能。
- en: '*Silhouette value*—This is one of the metrics used to measure the success of
    clustering. It ranges from –1 to +1, and a higher value is better. It measures
    how a data point is similar to other data points in its own cluster as compared
    to other clusters. As a first step, for each observation we calculate the average
    distance from all the data points in the same cluster; let’s call it *x*[*i*].
    Then we calculate the average distance from all the data points in the nearest
    cluster; let’s call it *y*[*i*].We will then calculate the coefficient by equation
    2.5:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*轮廓值*——这是衡量聚类成功的一个指标。它介于-1到+1之间，值越高越好。它衡量一个数据点与其簇内其他数据点的相似性，与簇外其他簇相比。作为第一步，对于每个观测值，我们计算其与同一簇中所有数据点的平均距离；让我们称它为*x*[*i*]。然后我们计算其与最近簇中所有数据点的平均距离；让我们称它为*y*[*i*]。然后我们将通过方程2.5计算系数：'
- en: (2.5)
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (2.5)
- en: Silhouette coefficient = (*y*[*i*] – *x*[*i*])/ max (*y*[*i*], *x*[*i*])
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数 = (*y*[*i*] – *x*[*i*])/ max (*y*[*i*], *x*[*i*])
- en: If the value of coefficient is –1, it means that the observation is in the wrong
    cluster. If it is 0, the observation is very close to the neighboring clusters.
    If the value of coefficient is +1, it means that the observation is at a distance
    from the neighboring clusters. Hence, we would expect to get the highest value
    for the coefficient to have a good clustering solution.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系数的值为-1，则表示观测值位于错误的簇中。如果值为0，则表示观测值非常接近邻近簇。如果系数的值为+1，则表示观测值与邻近簇的距离较远。因此，我们期望系数的值越高，表示聚类解决方案越好。
- en: '*Dunn index*—Thiscan also be used to measure the efficacy of the clustering.
    It uses the inter- and intradistance measurements defined in the previous intercluster
    sum of squares silhouette value sections and is given by equation 2.6:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dunn指数*——这也可以用来衡量聚类的有效性。它使用之前定义的簇间和簇内距离测量值，并由方程2.6给出：'
- en: (2.6)
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (2.6)
- en: Dunn index = min (intercluster distance)/max (intracluster distance)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Dunn指数 = 最小（簇间距离）/最大（簇内距离）
- en: Clearly, we would strive to maximize the value of Dunn index. To achieve it,
    the numerator should be as big as possible, implying that clusters are at a distance
    from each other, while the denominator should be as low as possible, signifying
    that the clusters are quite robust and close-packed.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们会努力最大化Dunn指数的值。为了实现这一点，分子应该尽可能大，这意味着簇彼此之间距离较远，而分母应该尽可能低，这表示簇非常稳健且紧密排列。
- en: 2.3.3 Finding the optimum value of k
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 寻找k的最佳值
- en: Choosing the optimum number of clusters is not easy. As I said earlier, the
    finest clustering is when the number of clusters equals the number of observations,
    but as we studied in the last section, it is not practically possible. Hence,
    we should provide the number of clusters *k* as an input to the algorithm.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳簇数并不容易。正如我之前所说的，最佳的聚类是簇数等于观测数的情况，但正如我们在上一节所研究的，这在实际中是不可能的。因此，我们应该将簇数*k*作为算法的输入。
- en: Perhaps the most widely used method for finding the optimum value of *k* is
    the *elbow method.* In this method, we calculate within the cluster sum of squares
    or WCSS for different values of *k*. The process is the same as discussed in the
    last section. Then, WCSS is plotted on a graph against different values of *k*.
    Wherever we observe a kink or elbow, as shown in figure 2.10, we find the optimum
    number of clusters for the dataset. Notice the sharp edge.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最优值 *k* 最广泛使用的方法可能是 *肘部方法*。在此方法中，我们计算不同 *k* 值的簇内平方和或WCSS。过程与上一节讨论的相同。然后，将WCSS绘制在图表中，与不同的
    *k* 值对应。无论何时观察到如图2.10所示的拐点或肘部，我们就能找到数据集的最佳簇数。注意尖锐的边缘。
- en: '![figure](../Images/CH02_F10_Verdhan.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F10_Verdhan.png)'
- en: Figure 2.10 The elbow method to find the optimal number of clusters. The circle
    shows the kink. However, the final number of clusters is dependent on business
    logic, and often we merge/split clusters based on this. The ease of maintaining
    the clusters also plays a crucial role.
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10：寻找最佳簇数的肘部方法。圆圈显示了拐点。然而，最终的簇数取决于业务逻辑，我们通常根据这个逻辑合并/拆分簇。簇的维护难度也起着至关重要的作用。
- en: Exercise 2.2
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习2.2
- en: 'Answer these questions to check your understanding:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题以检查你的理解：
- en: K-means clustering does not require the number of clusters as an input. True
    or False?
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: k-means聚类不需要输入簇数。对或错？
- en: KNN and k-means clustering are the same thing. True or False?
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KNN和k-means聚类是同一件事。对或错？
- en: Describe one possible process to find the optimal value of *k*.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述一种寻找最优值 *k* 的可能过程。
- en: But this does not mean that it is the final number of clusters we suggest for
    the business problem. Based on the number of observations falling in each of the
    clusters, a few clusters might be combined or broken into subclusters. We also
    consider the computation cost required to create the clusters. The higher the
    number of clusters, the greater the computation cost and the time required. We
    can also find the optimum number of clusters using the silhouette coefficient
    we discussed earlier.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不意味着这是我们为业务问题建议的最终簇数。基于每个簇中观察到的数量，一些簇可能需要合并或拆分成子簇。我们还考虑了创建簇所需的计算成本。簇的数量越多，计算成本和时间需求就越大。我们还可以使用之前讨论过的轮廓系数来找到最佳的簇数。
- en: Note  It is imperative that the business logic of merging a few clusters or
    breaking a few clusters is explored. Ultimately, the solution has to be implemented
    in real-world business scenarios.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：探索合并几个簇或拆分几个簇的业务逻辑是至关重要的。最终，解决方案必须在实际业务场景中实施。
- en: With this, we have examined the nuts and bolts of k-means clustering—the mathematical
    concepts and the process, the various distance metrics, and determining the best
    value of *k*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们已经探讨了k-means聚类的核心——数学概念和过程、各种距离度量以及确定最佳值 *k*。
- en: 2.3.4 Pros and cons of k-means clustering
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.4 k-means聚类的优缺点
- en: 'The k-means algorithm is quite a popular and widely implemented clustering
    solution. The solution offers the following advantages:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法是一个非常流行且广泛实施的聚类解决方案。该解决方案提供了以下优点：
- en: It is simple to comprehend and relatively easier to implement as compared to
    other algorithms. The distance measurement calculation makes it quite intuitive
    to understand even by users from nonstatistics backgrounds.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他算法相比，它易于理解且相对容易实现。距离测量计算使得即使是非统计学背景的用户也能很容易地理解。
- en: If the number of dimensions is large, the k-means algorithm is faster than other
    clustering algorithms and creates tighter clusters. Hence, it is preferred if
    the number of dimensions is quite high.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果维度数量很大，k-means算法比其他聚类算法更快，并且创建的簇更紧密。因此，当维度数量相当高时，它更受欢迎。
- en: It quickly adapts to new observations and can generalize very well to clusters
    of various shapes and sizes.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它能迅速适应新的观察结果，并且能够很好地推广到各种形状和大小的簇。
- en: The solution produces results through a series of iterations of recalculations.
    Most of the time, the Euclidean distance metric is used, which makes it less computationally
    expensive. It also ensures that the algorithm surely converges and produces results.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该解决方案通过一系列迭代重新计算来产生结果。大多数情况下，使用欧几里得距离度量，这使得计算成本较低。它还确保算法一定收敛并产生结果。
- en: 'K-means is widely used for real-life business problems. Though there are clear
    advantages of k-means clustering, we do face certain challenges with the algorithm:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: K-means在现实生活中的商业问题中得到了广泛应用。尽管k-means聚类有明显的优势，但我们确实面临着算法的一些挑战：
- en: Choosing the optimum number of clusters is not easy. We should provide it as
    an input. With different values of *k*, the results will be completely different.
    The process of choosing the best value of *k* is explored in the previous section.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最佳聚类数量并不容易。我们应该将其作为输入提供。随着*k*的不同值，结果将完全不同。选择最佳*k*值的过程在前一节中进行了探讨。
- en: The solution is dependent on the initial values of centroids. Since the centroids
    are initialized randomly, the output will be different with each iteration. Hence,
    it is advisable to run multiple versions of the solution and choose the best one.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该解决方案依赖于质心的初始值。由于质心是随机初始化的，每次迭代的输出将不同。因此，建议运行多个版本的解决方案，并选择最佳方案。
- en: The algorithm is quite sensitive to outliers. Outliers can mess up the final
    results, and hence it is imperative that we treat outliers before starting with
    clustering. We can also implement other variants of the k-means algorithm, like
    k-modes clustering, to deal with the problem of outliers. We discuss dealing with
    outliers in section 11.4.4 of chapter 11\. You can refer to it if you want to
    know how to deal with outliers.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法对异常值非常敏感。异常值可能会破坏最终结果，因此在开始聚类之前处理异常值至关重要。我们还可以实现k-means算法的其他变体，如k-modes聚类，来处理异常值问题。我们将在第11章的第11.4.4节中讨论处理异常值。如果您想了解如何处理异常值，可以参考它。
- en: Since the basic principle of k-means clustering is to calculate the distance,
    the solution is not directly applicable to categorical variables. In other words,
    we cannot use categorical variables directly since we can calculate the distance
    between numeric values but cannot perform mathematical calculations on categorical
    variables. To resolve this, we can convert categorical variables to numeric ones
    using one-hot encoding. We discuss dealing with categorical variables in section
    11.4.2 of chapter 11\. You can refer to it if you want to know how to deal with
    categorical variables.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于k-means聚类的根本原理是计算距离，因此该解决方案不能直接应用于分类变量。换句话说，我们不能直接使用分类变量，因为我们可以计算数值之间的距离，但不能对分类变量进行数学计算。为了解决这个问题，我们可以使用one-hot编码将分类变量转换为数值。我们将在第11章的第11.4.2节中讨论处理分类变量。如果您想了解如何处理分类变量，可以参考它。
- en: 'Despite these problems, k-means clustering is one of the most used clustering
    solutions due to its simplicity and ease of implementation. There are different
    implementations of k-means algorithms like k-median, k-medoids, etc., which are
    sometimes used to resolve the problems faced:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些问题，k-means聚类由于其简单性和易于实现，仍然是使用最广泛的聚类解决方案之一。k-means算法有不同的实现，如k-中位数、k-medoids等，有时用于解决面临的难题：
- en: As the name suggests, *k-median clustering* is based on the medians of the dataset
    as compared to the centroid in k-means. This increases the amount of computation
    time as the median can be found only after the data has been sorted. But at the
    same time, k-means is sensitive to outliers whereas k-median is less affected
    by them.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如其名所示，*k-中位数聚类*是基于数据集的中位数，而不是k-means中的质心。这增加了计算时间，因为中位数只能在数据排序后找到。但与此同时，k-means对异常值敏感，而k-中位数则受其影响较小。
- en: '*K-medoids clustering* is one of the variants of the k-means algorithm. Medoids
    are similar to means, except they are always from the same dataset and are implemented
    when it is difficult to get means, like with images. A medoid can be thought of
    as the most central point in a cluster that is least dissimilar to all the other
    members in the cluster. K-medoids choose the actual observations as the centers
    as compared to k-means, where the centroids may not even be part of the data.
    It is less sensitive to outliers as compared to the k-means clustering algorithm.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*K-medoids聚类*是k-means算法的一种变体。Medoids类似于均值，但它们总是来自同一数据集，并在难以获取均值时实现，例如图像。Medoid可以被视为簇中最中心且与其他簇成员最不相似的点。与k-means不同，k-medoids选择实际观测值作为中心，而k-means的质心可能甚至不是数据的一部分。与k-means聚类算法相比，它对异常值的影响较小。'
- en: There are other versions too, including k-means++, mini-batch k-means, and others.
    Generally, in the industry, k-means is used for most of the clustering solutions.
    You can explore other options like k-means++, mini-batch k-means, etc., if the
    results are not desirable or if the computation is taking a lot of time. Moreover,
    having different distance measurement metrics may produce different results for
    the k-means algorithm.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他版本，包括k-means++、mini-batch k-means等。通常，在工业界，k-means用于大多数聚类解决方案。如果结果不理想或计算花费时间过多，您可以探索其他选项，如k-means++、mini-batch
    k-means等。此外，不同的距离度量指标可能会为k-means算法产生不同的结果。
- en: This section concludes our discussion on the k-means clustering algorithm. It
    is time to hit the lab and develop actual Python code!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本节总结了我们对k-means聚类算法的讨论。现在是时候进入实验室并开发实际的Python代码了！
- en: 2.3.5 K-means clustering implementation using Python
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.5 使用Python实现K-means聚类
- en: 'We will now create a Python solution for k-means clustering. In this case,
    we are using the dataset from the link at GitHub at [https://mng.bz/lYq2](https://mng.bz/lYq2).
    This dataset has information about the features of four models of cars. Based
    on the features of the car, we are going to group them into different clusters:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将为k-means聚类创建一个Python解决方案。在这种情况下，我们使用GitHub上链接的数据集[https://mng.bz/lYq2](https://mng.bz/lYq2)。这个数据集包含关于四种车型特征的信息。基于车型的特征，我们将它们分组到不同的簇中：
- en: 'Import the libraries and the dataset into a dataframe. Here, `vehicles.csv`
    is the input data file. If the data file is not in the same folder as the Jupyter
    notebook, you would have to provide the complete path to the file. `dropna` is
    used to remove the missing values, if any:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将库和数据集导入到dataframe中。在这里，`vehicles.csv`是输入数据文件。如果数据文件不在Jupyter笔记本的同一文件夹中，您必须提供文件的完整路径。`dropna`用于删除任何缺失值：
- en: '[PRE0]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '2\. Perform some initial checks on the data, like shape, info, top five rows,
    distribution of classes, etc. This is to ensure that we have loaded the complete
    dataset and there is no corruption while loading the dataset. The `Shape` command
    will give the number of rows and columns in the data, `info` will describe all
    the variables and their respective types, and `head` will display the first five
    rows. The `value_counts` displays the distribution for the `class` variable. Or,
    in other words, `value_counts` returns the count of the unique values:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 对数据进行一些初步检查，如形状、信息、前五行、类别的分布等。这是为了确保我们已经加载了完整的数据集，并且在加载数据集时没有损坏。`Shape`命令将给出数据中的行数和列数，`info`将描述所有变量及其相应的类型，而`head`将显示前五行。`value_counts`显示了`class`变量的分布。或者换句话说，`value_counts`返回唯一值的计数：
- en: '[PRE1]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '3\. Generate two plots for the variable `class`. The dataset has more examples
    from `car` while for `bus` and `van` it is balanced data. I used the `matplotlib`
    library to plot these graphs. The outputs of the plots are as follows (see figure
    2.11):'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 为变量`class`生成两个图表。数据集包含来自`car`的更多示例，而`bus`和`van`的数据是平衡的。我使用了`matplotlib`库来绘制这些图表。图表的输出如下（见图2.11）：
- en: '[PRE2]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![figure](../Images/CH02_F11_Verdhan.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图表](../Images/CH02_F11_Verdhan.png)'
- en: Figure 2.11 Two plots for the variable `class`
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11 变量`class`的两个图表
- en: '4\. Check for any missing data points in the dataset. There are no missing
    data points in our dataset, as we have already dealt with them:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 检查数据集中是否有缺失的数据点。在我们的数据集中没有缺失的数据点，因为我们已经处理了它们：
- en: '[PRE3]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note  We cover the methods to deal with missing values in section 11.4.3 of
    chapter 11 as dropping the missing values is generally not the best approach.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们在第11章的第11.4.3节中介绍了处理缺失值的方法，因为删除缺失值通常不是最佳方法。
- en: '5\. Standardize the dataset. It is a good practice to standardize the dataset
    for clustering. It is important, as the different dimensions might be on a different
    scale, and one dimension may dominate the computation of distance if its values
    are naturally much larger than other dimensions. This is done using the `StandardScaler()`
    function. Refer to the appendix to examine different scaling techniques:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 标准化数据集。对于聚类来说，标准化数据集是一个好的实践。这是很重要的，因为不同的维度可能处于不同的尺度上，如果一个维度的值自然比其他维度大得多，那么它可能会主导距离的计算。这是通过使用`StandardScaler()`函数来完成的。请参考附录以检查不同的缩放技术：
- en: '[PRE4]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '6\. Have a quick look at the dataset by generating a scatter plot. The plot
    displays the distribution of all the data points we have created as `X_standard`
    in the last step (see figure 2.12):'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 通过生成散点图快速查看数据集。该图表显示了我们在上一步创建的`X_standard`中的所有数据点的分布（见图2.12）：
- en: '[PRE5]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![figure](../Images/CH02_F12_Verdhan.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F12_Verdhan.png)'
- en: Figure 2.12 A scatter plot of the dataset
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.12 数据集的散点图
- en: 7\. Perform k-means clustering. First, we have to select the optimum number
    of clusters using the elbow method. From the `sklearn` library, we import `KMeans`.
    In a `for` loop, we iterate for the values of clusters from 1 to 10\. In other
    words, the algorithm will create between 1 and 10 clusters and will then generate
    the results for us to choose the optimal value of *k*.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 7. 执行k-means聚类。首先，我们必须使用肘部方法选择最佳聚类数量。从`sklearn`库中导入`KMeans`。在一个`for`循环中，我们遍历从1到10的聚类值。换句话说，算法将创建1到10个聚类，然后为我们生成结果以选择最优的*k*值。
- en: 'In the following code snippet, the model object contains the output of the
    k-means algorithm, which is then fit on the `X_standard` generated in the last
    step. Here, Euclidean distance has been used as a distance metric (see figure
    2.13):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，模型对象包含了k-means算法的输出，然后将其拟合到上一步生成的`X_standard`。在这里，已经使用了欧几里得距离作为距离度量（见图2.13）：
- en: '[PRE6]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![figure](../Images/CH02_F13_Verdhan.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F13_Verdhan.png)'
- en: Figure 2.13 K-means clustering
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.13 k-means聚类
- en: '8\. As we can observe, the optimal number of clusters is three. It is the point
    where we can observe a sharp kink in the graph. We will continue with k-means
    clustering with the number of clusters as three. While there is nothing special
    about the number 3 here, it is best suited for this dataset. `random_state` is
    a parameter that is used to determine random numbers for centroid initialization.
    We set it to a value to make randomness deterministic. If you want to repeat the
    same results again, use the same random state number. It acts like a seed number:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 8. 如我们所观察到的，最佳聚类数量是三个。这是我们可以观察到图形中尖锐拐点的点。我们将继续使用三个聚类数量的k-means聚类。在这里，数字3并没有什么特殊之处，但它最适合这个数据集。`random_state`是一个用于确定质心初始化随机数的参数。我们将其设置为一个值以使随机性确定。如果您想再次重复相同的结果，请使用相同的随机状态数字。它就像一个种子数字：
- en: '[PRE7]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '9\. Get the `centroids` for the clusters:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 9. 获取聚类的`centroids`：
- en: '[PRE8]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '10\. Now we use the `centroids` so that they can be profiled by the `columns`:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 10. 现在我们使用`centroids`，这样它们就可以通过`columns`进行配置：
- en: '[PRE9]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '11\. We will now create a `dataframe` only for the purpose of creating the
    `labels`, and then we convert it into categorical variables:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 11. 现在我们只为创建`labels`的目的创建一个`dataframe`，然后将其转换为分类变量：
- en: '[PRE10]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '12\. In this step, we join the two `dataframes`:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 12. 在这一步，我们将两个`dataframes`连接起来：
- en: '[PRE11]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '13\. A `groupby` is done to create a data frame required for the analysis:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 13. 进行`groupby`操作以创建分析所需的数据框：
- en: '[PRE12]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '14\. Now we create a visualization for the clusters we have defined. This is
    done using the `mpl_toolkits` library. The logic is simple to understand. The
    data points are colored as per the respective labels. The rest of the steps are
    related to the display of the plot by adjusting the label, title, ticks, etc.
    Since it is not possible to plot all 18 variables in the plot, we have chosen
    3 variables to show in the plot (see figure 2.14):'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 14. 现在我们为定义的聚类创建一个可视化。这是使用`mpl_toolkits`库完成的。逻辑简单易懂。数据点根据相应的标签着色。其余步骤与调整标签、标题、刻度等以显示图表有关。由于无法在图表中绘制所有18个变量，我们选择了3个变量在图表中显示（见图2.14）：
- en: '[PRE13]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can also test the preceding code with multiple other values of *k*. We have
    created the code with different values of *k*. In the interest of space, we have
    put the code for testing with different values of *k* at the GitHub location.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用多个其他的*k*值测试前面的代码。我们已经为不同的*k*值创建了代码。为了节省空间，我们将测试不同*k*值的代码放在GitHub位置。
- en: '![figure](../Images/CH02_F14_Verdhan.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F14_Verdhan.png)'
- en: Figure 2.14 K-means clustering for the vehicles dataset
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.14 车辆数据集的k-means聚类
- en: Note  Exploratory data analysis holds the key to a robust machine learning solution
    and a successful project. In the subsequent chapters, we will create detailed
    exploratory data analyses for datasets.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：探索性数据分析是稳健的机器学习解决方案和成功项目的关键。在随后的章节中，我们将为数据集创建详细的数据探索分析。
- en: In the preceding example, we first did a small exploratory analysis of the dataset.
    This was followed by identifying the optimum number of clusters, which in this
    case comes out to be three. Then we implemented k-means clustering. You are expected
    to iterate the k-means solution with different initializations and compare the
    results, iterate with different values of *k*, and visualize to analyze the movements
    of data points.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的例子中，我们首先对数据集进行了小规模的探索性分析。随后，我们确定了最佳簇的数量，在这个例子中是三个。然后我们实现了k-means聚类。你被期望迭代k-means解决方案，使用不同的初始化并比较结果，使用不同的*k*值进行迭代，并可视化以分析数据点的移动。
- en: Centroid-based clustering is one of the most recommended solutions due to its
    less complicated logic, ease of implementation, flexibility, and trouble-free
    maintenance. Whenever we require clustering as a solution, mostly we start with
    creating a k-means clustering solution that acts as a benchmark. The algorithm
    is highly popular and generally one of the first solutions utilized for clustering.
    Then we test and iterate with other algorithms.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其逻辑简单、易于实现、灵活且易于维护，基于质心的聚类是推荐的最解决方案之一。每当我们需要聚类作为解决方案时，我们通常首先创建一个k-means聚类解决方案作为基准。该算法非常受欢迎，通常也是用于聚类的第一个解决方案之一。然后我们测试并迭代其他算法。
- en: 2.4 Connectivity-based clustering
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 基于连接性的聚类
- en: “Birds of a feather flock together” is the principle followed in connectivity-based
    clusters. The core concept is that objects that are connected with each other
    are similar to each other. Hence, based on the connectivity between these objects,
    they are grouped into clusters. An example of such a representation is shown in
    figure 2.15, where we can iteratively group observations. As an example, we are
    initiating with all things, dividing into living and nonliving, and so on. Such
    representation is known as a *dendrogram.* Since there is a tree-like structure,
    connectivity-based clustering is sometimes referred to as hierarchical clustering.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: “物以类聚”是连接性聚类所遵循的原则。其核心概念是相互连接的对象彼此相似。因此，基于这些对象之间的连接性，它们被分组到不同的簇中。图2.15展示了这种表示的一个例子，其中我们可以迭代地分组观察。例如，我们首先从所有事物开始，将其分为生物和非生物等。这种表示被称为*树状图*。由于存在树状结构，基于连接性的聚类有时也被称为层次聚类。
- en: '![figure](../Images/CH02_F15_Verdhan.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F15_Verdhan.png)'
- en: Figure 2.15 Hierarchical clustering utilizes grouping similar objects iteratively.
    Such representation is known as a dendrogram.
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.15 展示了层次聚类利用迭代分组相似对象的表示。这种表示被称为树状图。
- en: Hierarchical clustering fits nicely into human intuition and, hence, is easy
    to comprehend. Unlike k-means clustering, in hierarchical clustering we do not
    have to input the number of final clusters, but the method does require a termination
    condition (i.e., when the clustering should stop). At the same time, hierarchical
    clustering does not suggest the optimum number of clusters. From the hierarchy/dendrogram
    generated, we have to choose the best number of clusters ourselves. We will explore
    this more when we create the Python code for it in subsequent sections.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类与人类的直觉相吻合，因此易于理解。与k-means聚类不同，在层次聚类中，我们不需要输入最终簇的数量，但该方法确实需要一个终止条件（即聚类何时停止）。同时，层次聚类也不建议簇的最佳数量。从生成的层次/树状图中，我们必须自己选择最佳的簇数量。我们将在后续章节中创建Python代码时进一步探讨这一点。
- en: Figure 2.16 shows an example of hierarchical clustering. Here, the first node
    is the root, which is then iteratively split into nodes and subnodes. Whenever
    a node cannot be split further, it is called a terminal node or *leaf*.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 展示了层次聚类的一个例子。在这里，第一个节点是根节点，然后它被迭代地分割成节点和子节点。每当一个节点不能再分割时，它被称为终端节点或*叶节点*。
- en: '![figure](../Images/CH02_F16_Verdhan.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F16_Verdhan.png)'
- en: Figure 2.16 Hierarchical clustering has a root that splits into nodes and subnodes.
    A node that cannot be split further is called the leaf. In the bottom-up approach,
    a merging of the leaves will take place.
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.16 展示了层次聚类的结构，其根部分裂成节点和子节点。不能再进一步分割的节点被称为叶节点。在自底向上的方法中，叶节点将进行合并。
- en: 'Since there is more than one process or logic to merge the observations into
    clusters, we can generate a large number of dendrograms, which is given by equation
    2.7:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在多个将观察结果合并到簇中的过程或逻辑，我们可以生成大量的树状图，这由方程2.7给出：
- en: (2.7)
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (2.7)
- en: Number of dendrograms = (2*n* – 3)!/[2^(^(*n*)^(–2)) (*n* – 2)!]
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图的数目 = (2*n* – 3)!/[2^(^(*n*)^(–2)) (*n* – 2)!]
- en: where *n* is the number of observations or the leaves. So, if we have only two
    observations, we can have only one dendrogram. If we have 5 observations, we can
    have 105 dendrograms. Hence, based on the number of observations, we can generate
    a lot of dendrograms.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n* 是观测的数量或叶子节点。因此，如果我们只有两个观测，我们只能有一个树状图。如果我们有5个观测，我们可以有105个树状图。因此，根据观测的数量，我们可以生成大量的树状图。
- en: Hierarchical clustering can be further classified based on the process used
    to create the grouping of observations, which we explore next.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类可以根据创建观测分组所使用的流程进一步分类，我们将在下一节探讨。
- en: 2.4.1 Types of hierarchical clustering
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 层次聚类的类型
- en: 'Based on the grouping strategy, hierarchical clustering can be subdivided into
    two types: *agglomerative* clustering and *divisive* clustering (see table 2.2).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分组策略，层次聚类可以分为两种类型：*聚类*聚类和*分裂*聚类（见表2.2）。
- en: Table 2.2 Different types of hierarchical clustering
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.2 不同类型的层次聚类
- en: '| Serial no. | Agglomerative clustering | Divisive clustering |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 序号 | 聚类方法 | 分裂方法 |'
- en: '| --- | --- | --- |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1  | Bottom-up approach.  | Top-down approach.  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 1  | 自底向上的方法  | 自顶向下的方法  |'
- en: '| 2  | Each observation creates its own cluster and then merging takes place
    as the algorithm goes up.  | We start with one cluster and then observations are
    iteratively split to create a tree-like structure.  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 2  | 每个观测创建自己的簇，然后随着算法向上进行，合并发生。  | 我们从一个簇开始，然后迭代地分裂观测以创建树状结构。  |'
- en: '| 3  | Greedy approach is followed to merge (the greedy approach is described
    below).  | Greedy approach is followed to split.  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 3  | 采用贪婪方法进行合并（贪婪方法将在下面描述）。  | 采用贪婪方法进行分裂。  |'
- en: '| 4  | An observation will find the best pair to merge and the process completes
    when all the observations have merged with each other.  | All the observations
    are taken at the start and then, based on division conditions, splitting takes
    place until all the observations are exhausted or the termination condition is
    met.  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 4  | 观测将找到最佳的合并配对，并且当所有观测都相互合并时，过程完成。  | 所有观测在开始时都被考虑，然后根据分裂条件进行分裂，直到所有观测被耗尽或满足终止条件。  |'
- en: Let’s explore the meaning of the greedy approach first. The greedy approach
    or greedy algorithm is any algorithm that makes the best choice at each step without
    considering the effect on future states. In other words, we live in the moment
    and choose the best option from the available choices at that moment. The current
    choice is independent of the future choices, and the algorithm will solve the
    subproblems later. The greedy approach may not provide the most optimal solution
    but generally provides a locally optimal solution that is close to the best solution
    in a reasonable amount of time. Hierarchical clustering follows this greedy approach
    while merging or splitting at a node.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们探讨贪婪方法的意义。贪婪方法或贪婪算法是任何在每一步都做出最佳选择而不考虑对未来状态影响的算法。换句话说，我们活在当下，并从当时可用的选择中做出最佳选择。当前的选择独立于未来的选择，算法将在以后解决子问题。贪婪方法可能不会提供最优化解，但通常在合理的时间内提供接近最佳解的局部最优解。层次聚类在合并或分裂节点时遵循这种贪婪方法。
- en: 'We next examine the steps followed in the hierarchical clustering approach:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查层次聚类方法中遵循的步骤：
- en: 'As shown in figure 2.17, let us say we have five observations in our dataset:
    1, 2, 3, 4, and 5\.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如图2.17所示，假设我们的数据集中有五个观测：1、2、3、4和5。
- en: In this step, observations 1 and 2 are grouped into one and 4 and 5 are grouped
    into one; 3 is not grouped in any one.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，观测1和2被归为一组，4和5也被归为一组；3没有被归入任何一组。
- en: In this step, we group the output of 4,5 in the last step and observation 3
    into one cluster.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将上一步的4、5步的输出和观测3合并到一个簇中。
- en: The output from step 3 is grouped with the output of 1,2 as a single cluster.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第3步的输出与第1、2步的输出合并为一个单独的簇。
- en: '![figure](../Images/CH02_F17_Verdhan.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F17_Verdhan.png)'
- en: Figure 2.17 Steps followed in hierarchical clustering. From left to right, we
    have agglomerative clustering (merging of the nodes), while from right to left,
    we have divisive clustering (splitting of the nodes).
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.17 层次聚类所遵循的步骤。从左到右，我们看到了聚类（节点合并），而从右到左，我们看到了分裂聚类（节点分裂）。
- en: In this approach, from left to right, we have an agglomerative approach, and
    from right to left, a divisive approach is represented. In an agglomerative approach,
    we merge the observations, while in a divisive approach, we split the observations.
    We can use both agglomerative and divisive approaches for hierarchical clustering.
    Divisive clustering is an exhaustive approach and sometimes might take more time
    than the other.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，从左到右，我们有一个聚合方法，而从右到左，则表示一个分裂方法。在聚合方法中，我们合并观测值，而在分裂方法中，我们分割观测值。我们可以使用聚合和分裂方法进行层次聚类。分裂聚类是一种穷举方法，有时可能比其他方法花费更多时间。
- en: Similar to k-means clustering, the distance metric used to measure plays a significant
    role here. We are aware of and understand how to measure the distance between
    data points, but there are multiple methods to define that distance, which we
    study next.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 与k均值聚类类似，用于测量的距离度量在这里起着重要作用。我们了解并理解如何测量数据点之间的距离，但定义该距离的方法有很多，我们将在下一节中研究。
- en: 2.4.2 Linkage criterion for distance measurement
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 距离测量的连接标准
- en: 'We can use Euclidean distance, Manhattan distance, Chebyshev distance, and
    others to measure the distance between two observations. At the same time, we
    can employ various methods to define that distance. Based on this input criterion,
    the resultant clusters will be different. The various methods to define the distance
    metric are as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用欧几里得距离、曼哈顿距离、切比雪夫距离等来测量两个观测值之间的距离。同时，我们可以采用各种方法来定义该距离。基于这个输入标准，生成的簇将不同。定义距离度量的各种方法如下：
- en: '*Nearest neighbors or single linkages* use the distance between the two nearest
    points in different clusters. The distance between the closest neighbors in distinct
    clusters is calculated, and this is used to determine the next split/merging.
    It is done by an exhaustive search among all the pairs.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最近邻或单连接*使用不同簇中两个最近点之间的距离。计算不同簇中最近邻之间的距离，并以此确定下一个分割/合并。这是通过在所有成对中进行的穷举搜索来完成的。'
- en: '*Farthest neighbor or complete linkage* is the opposite of the nearest neighbor
    approach. Here, instead of taking the nearest neighbors, we concentrate on the
    most distant neighbors in different clusters. In other words, the distance between
    the clusters is calculated by the greatest distance between two objects.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最远邻或完全连接*是与最近邻方法相反的。在这里，我们不是选择最近的邻居，而是关注不同簇中最远的邻居。换句话说，簇之间的距离是通过两个对象之间最大的距离来计算的。'
- en: '*Group average linkage* calculates the average of the distances between all
    the possible pairs of objects in two different clusters.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*组平均连接*计算两个不同簇中所有可能成对对象之间的距离的平均值。'
- en: The *Ward linkage* method aims to minimize the variability of the clusters that
    are getting merged into one.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ward连接*方法旨在最小化合并到一个簇中的簇的变异性。'
- en: We can use these options of distance metrics while we are developing the actual
    code for hierarchical clustering and compare the accuracies to determine the best
    distance metrics for the dataset. During algorithm training, the algorithm merges
    the observations, which will minimize the linkage criteria chosen. We can visualize
    the various linkages in figure 2.18.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开发层次聚类的实际代码时，我们可以使用这些距离度量的选项，并比较准确性以确定数据集的最佳距离度量。在算法训练期间，算法合并观测值，这将最小化选择的连接标准。我们可以在图2.18中可视化各种连接方式。
- en: Note  Such inputs to the algorithm are referred to as hyperparameters. These
    are the values we feed to the algorithm to generate the results as per our requirement,
    and they act as our control on the algorithm. An example of a hyperparameter is
    *k* in k-means clustering.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：算法中的这些输入被称为超参数。这些是我们输入到算法中的值，以生成符合我们要求的结果，并且它们作为我们对算法的控制。k均值聚类中的一个超参数示例是*k*。
- en: '![figure](../Images/CH02_F18_Verdhan.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F18_Verdhan.png)'
- en: Figure 2.18 Single linkage is for closest neighbors (left); complete linkage
    is for farthest neighbors (center); and group average is for the average of the
    distance between clusters (right).
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.18：单连接用于最近邻（左）；完全连接用于最远邻（中）；组平均用于簇间距离的平均（右）。
- en: With this, we have understood the working mechanisms in hierarchical clustering.
    But we have still not addressed the mechanism to determine the optimum number
    of clusters using hierarchical clustering, which we examine next.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们已经理解了层次聚类的工作机制。但我们还没有解决使用层次聚类确定最佳簇数量的机制，这将在下一部分进行探讨。
- en: 2.4.3 Optimal number of clusters
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 最佳簇数量
- en: Recall that in k-means clustering, we have to give the number of clusters as
    an input to the algorithm. We use the elbow method to determine the optimum number
    of clusters. In the case of hierarchical clustering, we do not have to specify
    the number of clusters to the algorithm, but we still have to identify the number
    of final clusters we wish to have. We use a dendrogram to answer that question.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在k-means聚类中，我们必须将簇的数量作为输入提供给算法。我们使用肘部方法来确定最佳簇数量。在层次聚类的案例中，我们不需要将簇的数量指定给算法，但我们仍然需要确定我们希望拥有的最终簇数量。我们使用树状图来回答这个问题。
- en: Let us assume that we have 10 data points in total at the bottom of the chart,
    as shown in figure 2.19\. The clusters are merged iteratively until we get the
    one final cluster at the top. The height of the dendrogram at which two clusters
    get merged with each other represents the respective distance between the said
    clusters in the vector-space diagram.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设图表底部总共有10个数据点，如图2.19所示。簇会迭代合并，直到我们得到顶部的最终簇。树状图中两个簇合并在一起的高度代表了在向量空间图中这两个簇之间的相应距离。
- en: '![figure](../Images/CH02_F19_Verdhan.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F19_Verdhan.png)'
- en: Figure 2.19 Dendrogram to identify the optimum number of clusters. The distance
    between X and Y is more than between A and B and between P and Q; hence, we choose
    that as the cut to create clusters and the number of clusters chosen is five.
    The x-axis represents the clusters, while the y-axis represents the distance (dissimilarity)
    between two clusters.
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.19 树状图以确定最佳簇数量。X和Y之间的距离比A和B以及P和Q之间的距离要远；因此，我们选择它作为切割以创建簇，选择的簇数量是五个。x轴代表簇，而y轴代表两个簇之间的距离（相似度）。
- en: From a dendrogram, the number of clusters is given by the number of vertical
    lines being cut by a horizontal line. The *optimum* number of clusters is given
    by the number of vertical lines in the dendrogram cut by a horizontal line such
    that it intersects the tallest of the vertical lines. Or if the cut is shifted
    from one end of the vertical line to another, the length covered is the maximum.
    A dendrogram utilizes branches of clusters to show how closely various data points
    are related to each other. In a dendrogram, clusters that are located at the same
    height level are more closely related than clusters that are located at different
    height levels.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 从树状图中，簇的数量由被水平线切割的垂直线的数量给出。最佳簇数量由树状图中被水平线切割的垂直线的数量给出，这样它就与最高的垂直线相交。或者，如果切割从垂直线的这一端移动到另一端，覆盖的长度是最大的。树状图利用簇的分支来显示各种数据点之间是如何相互关联的。在树状图中，位于相同高度水平的簇比位于不同高度水平的簇更紧密相关。
- en: 'In the example shown in figure 2.19, we have shown three potential cuts: AB,
    PQ, and XY. If we take a cut above AB, it will result in two very broad clusters,
    while below PQ will result in nine clusters that will become difficult to analyze
    further.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.19中显示的例子中，我们展示了三个潜在的切割：AB、PQ和XY。如果我们对AB进行切割，它将导致两个非常宽泛的簇，而低于PQ将导致九个簇，这将变得难以进一步分析。
- en: Here, the distance between X and Y is more than between A and B and between
    P and Q. So we can conclude that the distance between X and Y is the maximum,
    and hence, we can finalize that as the best cut. This cut intersects at five distinct
    points; hence, we should have five clusters. The height of the cut in the dendrogram
    is similar to the value of *k* in k-means clustering. In k-means clustering, *k*
    determines the number of clusters. In hierarchical clustering, the best cut determines
    the number of clusters we wish to have.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，X和Y之间的距离比A和B以及P和Q之间的距离要远。因此，我们可以得出结论，X和Y之间的距离是最大的，因此，我们可以将其最终确定为最佳切割。这个切割与五个不同的点相交；因此，我们应该有五个簇。树状图上切割的高度与k-means聚类中的*k*值相似。在k-means聚类中，*k*决定了簇的数量。在层次聚类中，最佳切割决定了我们希望拥有的簇的数量。
- en: Similar to k-means clustering, the final number of clusters is not dependent
    on the choice from the algorithm only. Business acumen and pragmatic logic play
    a vital role in determining the final number of clusters. Recall that one of the
    important attributes of clusters is their usability, which we discussed in section
    2.2\.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 与k-means聚类类似，最终的聚类数量不仅取决于算法的选择。商业敏锐度和实用逻辑在确定最终聚类数量中起着至关重要的作用。回想一下，簇的一个重要属性是它们的可用性，我们在第2.2节中讨论了这一点。
- en: Sometimes we also use cophenetic correlation coefficient to measure how well
    the dendrogram represents the actual pairwise distance between the points. It
    compares the cophenetic distance, which is the height at which two points merged
    first in the dendrogram, with the original dissimilarity between the points.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们也会使用柯普仁尼克相关系数来衡量树状图如何很好地表示点之间的实际成对距离。它比较了柯普仁尼克距离，即两个点在树状图中首次合并的高度，与点之间的原始差异。
- en: There is one more index known as the Calinski-Haranasz index. It measures the
    ratio of between-cluster dispersion to within-cluster dispersion. A higher value
    means better clustering, and hence we choose the optimal number of clusters to
    maximize this index.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个称为Calinski-Haranasz指数的指标。它衡量了簇间分散与簇内分散的比率。值越高表示聚类越好，因此我们选择最大化这个指数的最佳聚类数量。
- en: Exercise 2.3
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习2.3
- en: 'Answer these questions to check your understanding:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: What is the greedy approach used in hierarchical clustering?
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层次聚类中使用的贪婪方法是什么？
- en: Complete linkage is used for finding distances for closest neighbors. True or
    False?
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完全连接用于寻找最近邻的距离。对或错？
- en: What is the difference between group linkage and ward linkage?
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 群连接和ward连接之间的区别是什么？
- en: Describe the process to find the most optimal value of *k*.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述寻找最优化*k*值的过程。
- en: 2.4.4 Pros and cons of hierarchical clustering
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 层次聚类的优缺点
- en: 'Hierarchical clustering is a strong clustering technique and is quite popular,
    too. Similar to k-means, it also uses distance as a metric to measure similarity.
    At the same time, there are a few challenges with the algorithm. The advantages
    of hierarchical clustering are as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是一种强大的聚类技术，也非常受欢迎。与k-means类似，它也使用距离作为度量标准来衡量相似性。同时，算法存在一些挑战。层次聚类的优点如下：
- en: Perhaps the biggest advantage of hierarchical clustering is the reproducibility
    of results. Recall in k-means clustering, the process starts with random initialization
    of centroids giving different results. In hierarchical clustering, we can reproduce
    the results.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类的最大优势可能是结果的再现性。回想一下k-means聚类，过程从质心的随机初始化开始，给出不同的结果。在层次聚类中，我们可以再现结果。
- en: In hierarchical clustering, we do not have to input the number of clusters to
    segment the data.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在层次聚类中，我们不需要输入聚类的数量来分割数据。
- en: The implementation is easy to implement and comprehend. Since it follows a tree-like
    structure, it is explainable to users from nontechnical backgrounds.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现起来既容易又易懂。因为它遵循树状结构，所以可以解释给非技术背景的用户。
- en: The dendrogram generated can be interpreted to give a very good understanding
    of the data with a visualization.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的树状图可以通过可视化来解释，从而对数据有一个很好的理解。
- en: 'At the same time, we do face some challenges with hierarchical clustering algorithms,
    which are as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们确实面临一些层次聚类算法的挑战，如下所示：
- en: The biggest challenge we face with hierarchical clustering is the time taken
    to converge. The time complexity for k-means is linear, while for hierarchical
    clustering it is quadratic. For example, if we have “*n*” data points, then for
    k-means clustering the time complexity will be *O*(*n*), while for hierarchical
    clustering it is *O*(*n*³).
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在层次聚类中面临的最大挑战是收敛所需的时间。k-means的时间复杂度是线性的，而层次聚类是二次的。例如，如果我们有“*n*”个数据点，那么对于k-means聚类，时间复杂度将是*O*(*n*)，而对于层次聚类，它是*O*(*n*³)。
- en: TIP  Refer to the appendix if you want to study *O*(*n*).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: TIP   如果你想研究*O*(*n*)，请参考附录。
- en: Since the time complexity is *O*(*n*³), it is a time-consuming task. Moreover,
    the memory required to compute is at least *O*(*n*²), making hierarchical clustering
    quite a time-consuming and memory-intensive process. And this is the problem even
    if the dataset is medium. The computation required might not be a challenge if
    we are using high-end processors, but it surely can be a concern for regular computers.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于时间复杂度为 *O*(*n*³)，这是一个耗时的任务。此外，计算所需的内存至少为 *O*(*n*²)，这使得层次聚类成为一个耗时且内存密集的过程。即使数据集是中等规模，所需的计算也可能不是挑战，但对于普通计算机来说，这确实可能是一个问题。
- en: The interpretation of dendrograms at times can be subjective; hence due diligence
    is required while interpreting dendrograms. The key to interpreting a dendrogram
    is to focus on the height at which any two data points are connected. It can be
    subjective, as different analysts can decipher different cuts and try to prove
    their methodology. Hence, it is advisable to interpret the results in the light
    of mathematics and marry the results with real-world business problems.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时对树状图的解释可能是主观的；因此，在解释树状图时需要尽职尽责。解释树状图的关键是关注任何两个数据点连接的高度。这可能具有主观性，因为不同的分析师可以解读不同的切割并试图证明他们的方法。因此，建议在数学和现实世界业务问题的背景下解释结果。
- en: Hierarchical clustering cannot undo the previous steps it has done. Even if
    we feel that a connection made is not proper and should be rolled back, there
    is no mechanism to remove the connection.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类无法撤销它已经完成的先前步骤。即使我们觉得建立的联系不合适，应该回滚，也没有机制来删除连接。
- en: The algorithm is very sensitive to outliers and messy datasets. The presence
    of outliers, NULL, missing values, duplicates, etc., makes a dataset messy. Hence
    the resultant output might not be proper or what we expected.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法对异常值和杂乱的数据集非常敏感。异常值、NULL、缺失值、重复值等的存在使得数据集变得杂乱。因此，结果可能不正确或不是我们所期望的。
- en: But despite all the challenges, hierarchical clustering is one of the most widely
    used clustering algorithms. Generally, we create both k-means clustering and hierarchical
    clustering for the same dataset to compare the results of the two. If the number
    of clusters suggested and the distribution of respective clusters look similar,
    we get more confident about the clustering methodology used.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在所有挑战，层次聚类仍然是使用最广泛的聚类算法之一。通常，我们为同一数据集创建k-means聚类和层次聚类来比较两种算法的结果。如果建议的聚类数量和相应聚类的分布看起来相似，我们就更有信心关于所使用的聚类方法。
- en: We have covered the theoretical background of hierarchical clustering. It is
    time to take action and jump into Python for coding.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了层次聚类的理论背景。现在是时候采取行动，跳入Python进行编码了。
- en: 2.4.5 Hierarchical clustering case study using Python
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.5 使用Python进行层次聚类案例研究
- en: 'We will now create a Python solution for hierarchical clustering using the
    same dataset we used for k-means clustering:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用与k-means聚类相同的相同数据集为层次聚类创建一个Python解决方案：
- en: Load the required libraries and dataset. For this, follow steps 1 to 6 we followed
    for the k-means algorithm.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的库和数据集。为此，遵循我们为k-means算法遵循的步骤1到6。
- en: '![figure](../Images/CH02_F20_Verdhan.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F20_Verdhan.png)'
- en: Figure 2.20 Hierarchical clustering using average, ward, and complete linking
    methods (top to bottom, respectively)
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.20 使用平均、ward和完全链接方法进行层次聚类（从上到下分别为）
- en: '2\. Next, we create hierarchical clustering using three linkage methods: average,
    ward, and complete. Then the clusters will be plotted. The input to the method
    is the `X_Standard` variable, the linkage method used, and the distance metric.
    Then, using the `matplotlib` library, we plot the dendrogram. In the following
    code snippet, simply change the method from “average” to “ward” and “complete”
    and get the respective results (see figure 2.20):'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 接下来，我们使用三种链接方法：平均、ward和完全来创建层次聚类。然后绘制聚类。该方法输入是`X_Standard`变量，使用的链接方法和距离度量。然后，使用`matplotlib`库绘制树状图。在下面的代码片段中，只需将方法从“平均”更改为“ward”和“完全”，即可获得相应的结果（见图2.20）：
- en: '[PRE14]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '3\. We now want to choose the number of clusters we wish to have. For this
    purpose, let’s re-create the dendrogram by subsetting the last 10 merged clusters.
    We have chosen 10 as it is generally an optimal choice; I advise you to test with
    other values too (see figure 2.21):'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 我们现在想要选择我们希望拥有的聚类数量。为此，让我们通过子集最后合并的10个聚类来重新创建树状图。我们选择10是因为它通常是一个最佳选择；我建议你也用其他值进行测试（见图2.21）：
- en: '![figure](../Images/CH02_F21_Verdhan.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F21_Verdhan.png)'
- en: Figure 2.21 A dendrogram subsetting the last 10 merged clusters
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.21 显示最后10个合并聚类的树状图
- en: '[PRE15]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 4\. We observe that the most optimal distance is 10\.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 我们观察到最佳距离是10。
- en: '5\. Cluster the data into different groups. By using the logic described in
    the last section, the number of optimal clusters is going to be four:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 将数据聚类到不同的组中。通过使用上一节中描述的逻辑，最佳聚类的数量将是四个：
- en: '[PRE16]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '6\. Plot the distinct clusters using the `matplotlib` library. In the print
    version of the book, you will not see different colors. The output of the Python
    code will have the colors; I advise that you run the code to appreciate the output.
    The same output is available in the GitHub repository (see figure 2.22):'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 使用`matplotlib`库绘制不同的聚类。在书的打印版本中，您将看不到不同的颜色。Python代码的输出将包含颜色；我建议您运行代码以欣赏输出效果。相同的输出可以在GitHub仓库中找到（见图2.22）：
- en: '[PRE17]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![figure](../Images/CH02_F22_Verdhan.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F22_Verdhan.png)'
- en: Figure 2.22 A plot of the distinct clusters using the `matplotlib` library
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.22 使用`matplotlib`库绘制的不同聚类图
- en: 7\. For different values of distance, the number of clusters will change, and
    the plot will look different. We are showing different results for distances of
    5, 15, and 20 and different numbers of clusters generated for each iteration.
    Figure 2.23 shows that we get completely different results for different values
    of distances as we move from left to right. We should be cautious when we choose
    the value of the distance, and sometimes we might have to iterate a few times
    to get the best value.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 7. 对于不同的距离值，聚类的数量将改变，图表看起来也会不同。我们展示了距离为5、15和20时以及每次迭代生成的不同数量的聚类结果。图2.23显示，当我们从左到右移动时，对于不同的距离值，我们得到完全不同的结果。在选择距离值时，我们应该谨慎，有时我们可能需要迭代几次才能得到最佳值。
- en: '![figure](../Images/CH02_F23_Verdhan.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F23_Verdhan.png)'
- en: Figure 2.23 The number of clusters using different values of distance
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.23 使用不同距离值的聚类数量
- en: Using hierarchical clustering, we segment the data on the left side to the one
    on the right side of figure 2.24\. The left side represents the raw data, while
    on the right, we have a representation of the clustered dataset. In the print
    version of the book, you won’t see the different colors. The output of the Python
    code will have the colors. The same output is available at the GitHub repository.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 使用层次聚类，我们将图2.24左侧的数据分割到右侧。左侧表示原始数据，而右侧我们有聚类数据集的表示。在书的打印版本中，您将看不到不同的颜色。Python代码的输出将包含颜色。相同的输出可以在GitHub仓库中找到。
- en: '![figure](../Images/CH02_F24_Verdhan.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F24_Verdhan.png)'
- en: Figure 2.24 Segmenting the data using hierarchical clustering
  id: totrans-294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.24 使用层次聚类分割数据
- en: Hierarchical clustering is a robust method and is highly recommended. Along
    with k-means, it creates a great foundation for clustering-based solutions. Most
    of the time, at least these two techniques are used when we create clustering
    solutions, and then we move on to iterate with other methodologies.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是一种稳健的方法，并且非常推荐。与k-means一起，它为基于聚类的解决方案提供了一个很好的基础。大多数时候，至少我们在创建聚类解决方案时使用这两种技术，然后我们继续使用其他方法进行迭代。
- en: 2.5 Density-based clustering
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 基于密度的聚类
- en: We have studied k-means in the earlier sections. Recall how it uses a centroid-based
    method to assign a cluster to each of the data points. If an observation is an
    outlier, the outlier point pulls the centroid toward itself and is also assigned
    a cluster like a normal observation. These outliers do not necessarily bring information
    to the cluster and can affect other data points disproportionally but are still
    made a part of the cluster. Moreover, getting clusters of arbitrary shapes, as
    shown in figure 2.25, is a challenge with the k-means algorithm. Density-based
    clustering methods solve the problem.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面章节中学习了k-means。回想一下它如何使用基于质心的方法将每个数据点分配到一个聚类中。如果一个观测值是异常值，异常点会将质心拉向自己，并且像正常观测值一样被分配到一个聚类中。这些异常值不一定向聚类提供信息，并且可能不成比例地影响其他数据点，但它们仍然被作为聚类的一部分。此外，如图2.25所示，使用k-means算法获取任意形状的聚类是一个挑战。基于密度的聚类方法解决了这个问题。
- en: '![figure](../Images/CH02_F25_Verdhan.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F25_Verdhan.png)'
- en: Figure 2.25 DBSCAN is highly recommended for irregular-shaped clusters. With
    k-means, we generally get spherical clusters; DBSCAN can resolve it.
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.25 DBSCAN非常适合不规则形状的聚类。与k-means相比，我们通常得到球形聚类；DBSCAN可以解决这个问题。
- en: In the density-based method, the clusters are identified as the areas that have
    a higher density as compared to the rest of the dataset. In other words, given
    a vector-space diagram where the data points are represented, a cluster is defined
    by adjacent regions or neighboring regions of high-density points. This cluster
    will be separated from other clusters by regions of low-density points. The observations
    in the sparse areas or separating regions are considered noise or outliers in
    the dataset. A few examples of density-based clustering are shown in figure 2.25.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于密度的方法中，簇被识别为密度高于数据集其余部分的区域。换句话说，在一个表示数据点的向量空间图中，簇被定义为高密度点的相邻区域或邻近区域。这个簇将由低密度点的区域与其他簇分开。稀疏区域或分隔区域中的观测被认为是数据集中的噪声或异常值。图2.25展示了基于密度的聚类的一些示例。
- en: 'We mentioned two terms: neighborhood and density. To understand density-based
    clustering, we will study these terms in the next section.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到了两个术语：邻域和密度。为了理解基于密度的聚类，我们将在下一节中研究这些术语。
- en: 2.5.1 Neighborhood and density
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 邻域和密度
- en: Imagine we represent data observations in a vector-space, and we have a point
    P. We now define the neighborhood for this point P. The representation is shown
    in figure 2.26\. For a point P we have defined an *ε*—neighborhoods for it that
    are the points equidistant from P. In a 2D space, it is represented by a circle;
    in a 3D space it is a sphere; and for a *n*-dimensional space, it is *n*-sphere
    with center P and radius *ε*. This defines the concept of *neighborhood*.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们用向量空间表示数据观测，并且有一个点P。我们现在定义这个点P的邻域。表示如图2.26所示。对于点P，我们定义了一个*ε*邻域，即与P等距离的点。在二维空间中，它表示为一个圆；在三维空间中，它是一个球体；而对于*n*维空间，它是以P为中心、半径为*ε*的*n*球体。这定义了*邻域*的概念。
- en: '![figure](../Images/CH02_F26_Verdhan.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F26_Verdhan.png)'
- en: Figure 2.26 Representation of data points in a vector-space diagram. On the
    right-side we have a point P, and the circle drawn is of radius *ε*. So, for *ε*
    > 0, the neighborhood of P is defined by the set of points that are at less than
    or equal to *ε* distance from the point P.
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.26向量空间图中数据点的表示。在右侧有一个点P，画出的圆的半径为*ε*。因此，对于*ε* > 0，点P的邻域由距离点P小于或等于*ε*的点集定义。
- en: Now let’s explore the term *density*. Recall density is mass divided by volume
    (mass/volume). The higher the mass, the higher the density, and the lower the
    mass, the lower the density. Conversely, the lower the volume, the higher the
    density, and vice versa.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探讨一下*密度*这个术语。回想一下，密度是质量除以体积（质量/体积）。质量越高，密度越高；质量越低，密度越低。相反，体积越低，密度越高，反之亦然。
- en: In the previous context, mass is the number of points in the neighborhood. In
    figure 2.26 we can observe the effect of *ε* on the number of data points or the
    mass. When it comes to volume, in the case of 2D space, volume is π*r*², while
    for a sphere that is 3D, it is 4/3 π*r*³. For spheres of *n*-dimensions, we can
    calculate the respective volume as per the number of dimensions, which will be
    π times a numerical constant raised to the number of dimensions.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的上下文中，质量是邻域中点的数量。在图2.26中，我们可以观察到*ε*对数据点数量或质量的影响。当涉及到体积时，在二维空间中，体积是π*r*²，而对于三维的球体，它是4/3
    π*r*³。对于*n*维的球体，我们可以根据维数计算相应的体积，这将是一个数值常数乘以维数的幂。
- en: So, in the two cases shown in figure 2.27, for a point P, we can get the number
    of points (mass) and volumes, and then we can calculate the respective densities.
    But the absolute values of these densities mean nothing to us; rather how they
    are similar (or different) from nearby areas is what’s important. The points that
    are in the same neighborhood and have similar densities can be grouped into one
    cluster.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在图2.27所示的两种情况下，对于点P，我们可以得到点的数量（质量）和体积，然后我们可以计算出各自的密度。但是，这些密度的绝对值对我们来说没有任何意义；相反，它们与附近区域的相似性（或不同性）才是重要的。位于同一区域且具有相似密度的点可以被归入一个簇。
- en: '![figure](../Images/CH02_F27_Verdhan.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F27_Verdhan.png)'
- en: Figure 2.27 The effect of radius *ε*. On the left side, the number of points
    is more than on the right side. So the mass of the right side is less, since it
    contains a smaller number of data points.
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.27 半径*ε*的影响。在左侧，点的数量多于右侧。因此，右侧的质量较小，因为它包含的数据点较少。
- en: In an ideal case scenario, we wish to have highly dense clusters with a maximum
    number of points. In the two cases shown in figure 2.28, we have a less dense
    cluster depicted on the left and a high-dense one on the right.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，我们希望拥有高度密集的聚类，具有最大数量的点。在图2.28所示的两种情况下，左侧显示了一个较稀疏的聚类，而右侧显示了一个高密度的聚类。
- en: '![figure](../Images/CH02_F28_Verdhan.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F28_Verdhan.png)'
- en: Figure 2.28 Denser clusters are preferred over less dense ones. Ideally, a dense
    cluster, with a maximum number of data points, is what we aim to achieve from
    clustering.
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.28 较密集的聚类比较稀疏的聚类更受欢迎。理想情况下，一个具有最大数据点的密集聚类是我们从聚类中希望实现的目标。
- en: From the preceding discussion, we can conclude that
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的讨论中，我们可以得出结论：
- en: If we *increase* the value of *ε*, we will get a *higher* volume but not necessarily
    a higher number of points (mass). It depends on the distribution of the data points.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们*增加*ε的值，我们将获得更大的体积，但不一定是更多的点数（质量）。这取决于数据点的分布。
- en: If we *decrease* the value of *ε*, we will get a *lower* volume but not necessarily
    a lower number of points (mass).
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们*减少*ε的值，我们将获得较小的体积，但不一定是较少的点数（质量）。
- en: These are the fundamental points we adhere to. Hence, it is imperative that
    we choose clusters that have high density and cover the maximum number of neighboring
    points.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们坚持的基本原则。因此，我们必须选择具有高密度并覆盖最多邻近点的聚类。
- en: 2.5.2 DBSCAN clustering
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 DBSCAN聚类
- en: DBSCAN clustering is one of the highly recommended density-based algorithms.
    It clusters the data observations that are closely packed in a densely populated
    area but does not consider the outliers in low-density regions. Unlike k-means,
    we do not specify the number of clusters, and the algorithm is able to identify
    irregular-shaped clusters, whereas k-means generally proposes spherical-shaped
    clusters. Similar to hierarchical clustering, it works by connecting the data
    points but with the observations that satisfy the density criteria or the threshold
    value.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN聚类是一种高度推荐的基于密度的算法。它将密集区域中紧密排列的数据观察值聚类，但不考虑低密度区域的异常值。与k-means不同，我们不需要指定聚类数量，该算法能够识别不规则形状的聚类，而k-means通常提出球形聚类。类似于层次聚类，它通过连接数据点来工作，但与满足密度标准或阈值值的观察值相关。
- en: Note  DBSCAN was proposed in 1996 by Martin Ester, Hans-Peter Kriegal, Jörg
    Sander, and Xiaowei Xu. The algorithm was given the Test of Time award in 2014
    at ACM SIGKDD. The paper can be accessed at [https://mng.bz/BXv1](https://mng.bz/BXv1).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：DBSCAN由Martin Ester、Hans-Peter Kriegal、Jörg Sander和Xiaowei Xu于1996年提出。该算法在2014年ACM
    SIGKDD的测试时间奖中获奖。论文可在[https://mng.bz/BXv1](https://mng.bz/BXv1)访问。
- en: DBSCAN works on the concepts of neighborhood we discussed in the last section.
    We will now dive deeper into the working methodology and building blocks of DBSCAN.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN基于我们在上一节讨论的邻域概念。现在我们将深入探讨DBSCAN的工作方法和构建模块。
- en: nuts and bolts of DBSCAN clustering
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DBSCAN聚类的要点
- en: Let’s now examine the core building blocks of DBSCAN clustering. We know it
    is a density-based clustering algorithm, and hence the neighborhood concept is
    applicable here.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来检查DBSCAN聚类的核心构建模块。我们知道它是一种基于密度的聚类算法，因此邻域概念在这里适用。
- en: 'Say we have a few data observations that we need to cluster. We also locate
    a data point P. Then we can easily define two hyperparameter terms:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一些需要聚类的数据观察值。我们也定位了一个数据点P。然后我们可以轻松地定义两个超参数术语：
- en: The radius of the neighborhood around P, known as *ε*, which we discussed in
    the last section.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P点周围的邻域半径，称为*ε*，这是我们上一节讨论的内容。
- en: The minimum number of points we wish to have in the neighborhood of P or, in
    other words, the minimum number of points that are required to create a dense
    region. This is referred to as minimum points (minPts). It is one of the parameters
    we can input by applying a threshold on minPts.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望在P点邻域中拥有的最小点数，换句话说，创建密集区域所需的最小点数。这被称为最小点数（minPts）。这是我们可以在minPts上应用阈值输入的参数之一。
- en: 'Based on these concepts, we can classify the observations into three broad
    categories: core points, border or reachable points, and outliers:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些概念，我们可以将观察值分为三个广泛的类别：核心点、边界或可达点，和异常值：
- en: '*Core points*—Any data point x can be termed as a core point if at least minPts
    are within *ε* distance of it (including x itself), shown as squares in figure
    2.29\. They are the building blocks of our clusters and are called core points.
    We use the same value of radius (*ε*) for each point and hence the *volume* of
    each neighborhood remains constant. But the number of points will vary and hence
    the *mass* varies. Consequently, the density varies as well. Since we put a threshold
    using minPts, we are putting a limit on density. So we can conclude that core
    points fulfill the minimum density threshold requirement. It is imperative to
    note that we can choose different values of *ε* and minPts to iterate and fine-tune
    the clusters.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*核心点*—任何数据点x，如果至少有minPts个点在它（包括x本身）的ε距离范围内，则可以称为核心点，在图2.29中用方块表示。它们是我们聚类的构建块，被称为核心点。我们对每个点使用相同的半径值（ε），因此每个邻域的体积保持不变。但点的数量会变化，因此质量也会变化。因此，密度也会变化。由于我们使用minPts设置了一个阈值，我们实际上是在对密度设置一个限制。因此，我们可以得出结论，核心点满足最小密度阈值要求。必须注意的是，我们可以选择不同的ε和minPts值来迭代和微调聚类。'
- en: '*Border points or reachable points*—A point that is not a core point in the
    clusters is called a border point, shown as filled circles in figure 2.29\.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*边界点或可达点*—在聚类中不是核心点的点称为边界点，在图2.29中用实心圆表示。'
- en: '![figure](../Images/CH02_F29_Verdhan.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F29_Verdhan.png)'
- en: Figure 2.29 Core points are shown as squares; border points are shown as filled
    circles, while noise is unfilled circles. Together, these three are the building
    blocks for DBSCAN clustering.
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.29核心点用方块表示；边界点用实心圆表示，而噪声用空心圆表示。这三个共同构成了DBSCAN聚类的构建块。
- en: A point y is directly reachable from x if y is within *ε* distance of core point
    x. A point can only be approached from a core point, and it is the primary condition
    or rule to be followed. Only a core point can reach a noncore point, and the opposite
    is not true. In other words, a noncore point can only be reached by other core
    points; it cannot reach anyone else. In figure 2.29, border points are represented
    as dark circles.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果点y在核心点x的ε距离内，则y可以从x直接到达。一个点只能从一个核心点接近，这是需要遵循的主要条件或规则。只有核心点可以到达非核心点，反之则不成立。换句话说，一个非核心点只能被其他核心点到达；它不能到达任何人。在图2.29中，边界点用深色圆圈表示。
- en: 'To understand the process better, we have to understand the term *density-reachable*
    or *connectedness*. In figure 2.30, we have two core points: X and Y. We can directly
    go from X to Y. Point Z is not in the neighborhood of X but is in the neighborhood
    of Y. So we cannot directly reach Z from X, but we can surely reach Z from X through
    Y or, in other words, using the neighborhood of Y, we can travel to Z from X.
    Conversely, we cannot go from Z to X since Z is the border point and, as described
    earlier, we cannot travel from a border point.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个过程，我们必须理解术语*密度可达*或*连通性*。在图2.30中，我们有两个核心点：X和Y。我们可以直接从X到Y。点Z不在X的邻域内，但在Y的邻域内。因此，我们不能直接从X到达Z，但我们可以肯定地通过Y或换句话说，通过Y的邻域从X到达Z。相反，我们不能从Z到X，因为Z是边界点，如前所述，我们不能从边界点进行旅行。
- en: '![figure](../Images/CH02_F30_Verdhan.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F30_Verdhan.png)'
- en: Figure 2.30 X and Y are the core points, and we can travel from X to Y. Though
    Z is not in the immediate neighborhood of X, we can still reach Z from X through
    Y. This is the core concept of density-connected points used in DBSCAN clustering.
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.30中，X和Y是核心点，我们可以从X到Y旅行。尽管Z不在X的最近邻域内，但我们仍然可以通过Y从X到达Z。这是DBSCAN聚类中使用的密度连通点的核心概念。
- en: '*Outliers*—All the other points are outliers. In other words, if it is not
    a core point or is not a reachable point, it is an outlier, shown as unfilled
    circles in figure 2.29\. They are not assigned any cluster.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异常值*—所有其他点都是异常值。换句话说，如果不是核心点或不是可达点，它就是异常值，在图2.29中用空心圆表示。它们没有被分配到任何聚类中。'
- en: steps in DBSCAN clustering
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DBSCAN聚类的步骤
- en: 'The steps in DBSCAN clustering are as follows:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN聚类的步骤如下：
- en: We start with assigning values for *ε* and minPts required to create a cluster.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从为创建聚类所需的ε和minPts赋值开始。
- en: We start with picking a random point, let’s say P, which is not yet given any
    label (i.e., it has not been analyzed and assigned any cluster).
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从选择一个随机点开始，比如说P，它还没有被赋予任何标签（即，它还没有被分析并分配到任何聚类中）。
- en: We then analyze the neighborhood for P. If it contains a sufficient number of
    points (i.e., higher than minPts), then the condition is met to start a cluster.
    If so, we tag the point P as the *core point*. If a point cannot be recognized
    as a core point, we will assign it the tag of *outlier* or *noise*. We should
    note this point can be made a part of a different cluster later. Then we go back
    to step 2\.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们分析P的邻域。如果它包含足够多的点（即，高于minPts），那么满足开始簇的条件。如果是这样，我们将点P标记为*核心点*。如果一个点不能被识别为核心点，我们将将其标记为*异常点*或*噪声*。我们应该注意这个点以后可以成为不同簇的一部分。然后我们回到步骤2。
- en: Once this core point P is found, we start creating the cluster by adding all
    directly reachable points from P and then increase this cluster size by adding
    more points directly reachable from P. Then we add all the points to the cluster,
    which can be included using the neighborhood by iterating through all these points.
    If we add an outlier point to the cluster, the tag of the outlier point is changed
    to a border point.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦找到这个核心点P，我们就开始通过添加从P直接可达的所有点来创建簇，然后通过添加更多从P直接可达的点来增加这个簇的大小。然后我们通过迭代所有这些点，将所有点添加到簇中，这些点可以通过邻域包含。如果我们向簇中添加一个异常点，则将异常点的标签更改为边界点。
- en: This process continues until the density cluster is complete. We then find a
    new unassigned point and repeat the process.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程会一直持续到密度簇完成。然后我们找到一个新未分配的点，并重复这个过程。
- en: Once all the points have been assigned to a cluster or called an outlier, we
    stop our clustering process.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有点都被分配到一个簇或称为异常点，我们就停止我们的聚类过程。
- en: There are iterations in the process. Then, once the clustering concludes, we
    utilize business logic to either merge or split a few clusters.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中有迭代。然后，一旦聚类完成，我们利用业务逻辑来合并或拆分几个簇。
- en: Exercise 2.4
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习2.4
- en: 'Answer these questions to check your understanding:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: Compare and contrast the importance of DBSCAN clustering with respect to k-means
    clustering.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较和对比DBSCAN聚类与k-means聚类的重要性。
- en: A noncore point can reach a core point and vice versa is also true. True or
    False?
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非核心点可以到达核心点，反之亦然。对还是错？
- en: Explain the significance of neighborhood and minPts.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释邻域和minPts的重要性。
- en: Describe the process to find the most optimal value of *k*.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述寻找*k*的最优值的过程。
- en: Now we are clear with the process of DBSCAN clustering. Before creating the
    Python solution, we will examine the advantages and disadvantages of the DBSCAN
    algorithm.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清楚DBSCAN聚类的过程。在创建Python解决方案之前，我们将检查DBSCAN算法的优点和缺点。
- en: pros and cons of DBSCAN clustering
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DBSCAN聚类的优缺点
- en: 'DBSCAN has the following advantages:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN有以下优点：
- en: Unlike k-means, we need not specify the number of clusters to DBSCAN.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与k-means不同，我们不需要指定DBSCAN的簇数。
- en: The algorithm is quite a robust solution for unclean datasets. Unlike other
    algorithms, it can deal with outliers effectively.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法是对不干净数据集相当鲁棒的解决方案。与其他算法不同，它可以有效地处理异常值。
- en: We can determine irregular-shaped clusters too. Arguably, this is the biggest
    advantage of DBSCAN clustering.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们也可以确定不规则形状的簇。可以说，这是DBSCAN聚类最大的优点。
- en: Only the input of radius and minPts is required by the algorithm.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法只需要输入半径和minPts。
- en: 'DBSCAN has the following challenges:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN有以下挑战：
- en: The differentiation in clusters is sometimes not clear using DBSCAN. Depending
    on the order of processing the observations, a point can change its cluster. In
    other words, if a border point P is accessible by more than one cluster, P can
    belong to either cluster, which is dependent on the order of processing the data.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DBSCAN有时无法清楚地区分簇。根据处理观察值的顺序，一个点可以改变其簇。换句话说，如果一个边界点P可以被多个簇访问，P可以属于任何一个簇，这取决于处理数据的顺序。
- en: If the difference in densities among different areas of the datasets is very
    big, then the optimum combination of *ε* and minPts will be difficult to determine,
    and hence DBSCAN will not generate effective results.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据集不同区域之间的密度差异非常大，那么确定*ε*和minPts的最佳组合将非常困难，因此DBSCAN将无法生成有效结果。
- en: The distance metric used plays a highly significant role in clustering algorithms,
    including DBSCAN. Arguably, the most common metric used is Euclidean distance,
    but if the number of dimensions is quite large, then it becomes a challenge to
    compute.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用的距离度量在聚类算法中起着高度重要的作用，包括DBSCAN。可以说，最常用的度量是欧几里得距离，但如果维度数量相当大，那么计算就成为一个挑战。
- en: The algorithm is very sensitive to different values of *ε* and minPts. Sometimes
    finding the most optimum value becomes a challenge.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法对*ε*和minPts的不同值非常敏感。有时找到最优化值成为一个挑战。
- en: python solution for DBSCAN clustering
  id: totrans-364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DBSCAN聚类的Python解决方案
- en: 'We will use the same dataset we have used for k-means and hierarchical clustering:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与k-means和层次聚类相同的相同数据集：
- en: Load the libraries and dataset up to step 6 in the k-means algorithm.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在k-means算法的第6步之前，加载库和数据集。
- en: 'Import additional libraries:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入额外的库：
- en: '[PRE18]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here we fit the model with a value for minimum distance and radius:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用最小距离和半径的值来拟合模型：
- en: '[PRE19]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The number of distinct clusters is 1:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 独特聚类的数量是1：
- en: '[PRE20]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 3\. We are not getting any results for clustering here. In other words, there
    will not be any logical results of clustering since we have not provided the optimal
    values for minPts and *ε*.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 在这里我们没有得到任何关于聚类的结果。换句话说，由于我们没有提供minPts和*ε*的最佳值，因此不会有任何逻辑上的聚类结果。
- en: '4\. Now we will find the optimum values for *ε* (see figure 2.31). For this,
    we will calculate the distance to the nearest points for each point and then sort
    and plot the results. Wherever the curvature is maximum, it is the best value
    for *ε*. For minPts, generally minPts ≥ *d* + 1 where *d* is the number of dimensions
    in the dataset:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 现在，我们将找到*ε*的最佳值（见图2.31）。为此，我们将计算每个点到最近点的距离，然后对结果进行排序和绘图。曲线最大的地方就是*ε*的最佳值。对于minPts，通常minPts
    ≥ *d* + 1，其中*d*是数据集中的维度数：
- en: '[PRE21]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![figure](../Images/CH02_F31_Verdhan.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH02_F31_Verdhan.png)'
- en: Figure 2.31 Finding the optimum value of *ε*
  id: totrans-377
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.31 寻找*ε*的最佳值
- en: Note  See the paper at [https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf](https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf)
    for further study on how to choose the values of radius for DBSCAN.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有关如何选择DBSCAN半径值的进一步研究，请参阅[https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf](https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf)。
- en: '5\. The best value is coming up as 1.5, as observed in the point of defection.
    We will use it and set the minPts as 5, which is generally taken as a standard:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 观察到缺陷点时，最佳值是1.5，我们将使用它并将minPts设置为5，这通常被视为标准：
- en: '[PRE22]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '6\. Now we can observe that we are getting more than one cluster:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 现在，我们可以观察到我们得到了多个聚类：
- en: '[PRE23]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '7\. Let’s plot the clusters (see figure 2.32). In the print version of the
    book, you will not see different colors. The output of the Python code will have
    the colors. The same output is available at the GitHub repository:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 7. 让我们绘制聚类（见图2.32）。在书的打印版本中，您将看不到不同的颜色。Python代码的输出将包含颜色。相同的输出可在GitHub仓库中找到：
- en: '[PRE24]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![figure](../Images/CH02_F32_Verdhan.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH02_F32_Verdhan.png)'
- en: Figure 2.32 Plotting the clusters
  id: totrans-386
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.32 绘制聚类
- en: We have thus created a solution using DBSCAN. I advise you to compare the results
    from all three algorithms. In real-world scenarios, we test the solution with
    multiple algorithms, iterate with hyperparameters, and then choose the best solution.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用DBSCAN创建了一个解决方案。我建议您比较所有三个算法的结果。在现实世界的场景中，我们使用多个算法测试解决方案，迭代超参数，然后选择最佳解决方案。
- en: Density-based clustering is quite an efficient solution and, to a certain extent,
    is a very effective one too. It is heavily recommended if the shape of the clusters
    is suspected to be irregular.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的聚类是一个相当有效的解决方案，并且在某种程度上也是一个非常有效的解决方案。如果怀疑聚类的形状是不规则的，则强烈推荐使用。
- en: With this, we conclude our discussion on DBSCAN clustering. In the next section,
    we solve a business use case on clustering. In the case study, the focus is less
    on technical concepts and more on business understanding and solution generation.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们结束了DBSCAN聚类的讨论。在下一节中，我们将解决一个关于聚类的商业案例。在案例研究中，重点较少放在技术概念上，而更多放在商业理解和解决方案生成上。
- en: 2.6 Case study using clustering
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 使用聚类进行案例研究
- en: We will now define a case study that employs clustering as one of the solutions.
    The objective of the case study is to give you a flavor of the practical and real-life
    business world. This case study–based approach is also followed in job-related
    interviews, wherein a case is discussed during the interview stage. I highly recommend
    you understand how we implement machine learning solutions in pragmatic business
    scenarios.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将定义一个案例研究，该研究将聚类作为解决方案之一。案例研究的目的是让您了解实际和现实生活中的商业世界。这种基于案例研究的方法也适用于与工作相关的面试，其中在面试阶段讨论案例。我强烈建议您了解我们如何在务实商业场景中实施机器学习解决方案。
- en: A case study typically has a business problem, the dataset available, the various
    solutions that can be used, the challenges faced, and the final chosen solution.
    We also discuss the problems faced while implementing the solution in real-world
    business.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 案例研究通常包括业务问题、可用数据集、可用的各种解决方案、面临的挑战以及最终选择的解决方案。我们还讨论了在现实世界业务中实施解决方案时遇到的问题。
- en: So let’s start our case study on clustering using unsupervised learning. In
    the case study, we focus on the steps we take to solve the case study and not
    on the technical algorithms, as there can be multiple technical solutions to a
    particular problem.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们从使用无监督学习的聚类案例研究开始。在案例研究中，我们关注解决案例研究的步骤，而不是技术算法，因为对于特定问题可能有多种技术解决方案。
- en: 2.6.1 Business context
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.1 业务背景
- en: The industry we are considering can be retail; telecom; banking, financial services,
    and insurance; aviation; healthcare; or any other industry that has a customer
    base. For any business, the objective is to generate more revenue for the business
    and ultimately increase the overall profit of the business. To increase revenue,
    the business would want to have increasingly more new customers. The business
    would also want the existing consumers to buy more and buy more often. So the
    business always strives to keep the consumers engaged and happy and to increase
    their transactional value with the business.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑的行业可以是零售业；电信业；银行、金融服务和保险业；航空业；医疗保健业；或任何拥有客户基础的行业。对于任何业务，目标是为业务创造更多收入，并最终增加业务的总体利润。为了增加收入，业务希望拥有越来越多的新客户。业务也希望现有消费者购买更多，并且更频繁地购买。因此，业务总是努力保持消费者的参与和满意度，并增加他们与业务的交易价值。
- en: For this to happen, the business must have a thorough understanding of its consumer
    base; it must know their tastes, price points, category preferences, affinity,
    preferred marketing/communication channels, etc. Once the business has examined
    and understood the consumer base minutely, then
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，业务必须对其消费者基础有深入的了解；它必须了解他们的口味、价格点、类别偏好、亲和力、首选的营销/沟通渠道等。一旦业务仔细检查并理解了消费者基础，那么
- en: The product team can improve the product features as per the consumer’s need.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品团队可以根据消费者的需求改进产品功能。
- en: The pricing team can improve the price of the products by aligning them to customers’
    preferred prices. The prices can be customized for a customer, or loyalty discounts
    can be offered.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定价团队可以通过将产品与客户偏好的价格对齐来提高产品的价格。可以为客户定制价格，或者提供忠诚度折扣。
- en: The marketing team and customer relationship team can target the consumers with
    a customized offer.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场营销团队和客户关系团队可以针对消费者提供定制化优惠。
- en: The teams can win back the consumers who are going to churn or stop buying from
    the business, can enhance their spending, increase the stickiness, and increase
    the customer lifetime value.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队可以挽回即将流失或停止购买业务的消费者，提高他们的消费，增加粘性，并提高客户终身价值。
- en: Overall, different teams can align their offerings as per the understanding
    of the consumers generated. And the end consumer will be happier, more engaged,
    and more loyal to the business, leading to more fruitful consumer engagement.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体而言，不同的团队可以根据对消费者生成的理解来调整他们的产品。最终消费者将更加满意，更加投入，并对业务更加忠诚，从而实现更有成效的消费者互动。
- en: The business hence should dive deep into the consumers’ data and generate an
    understanding of the base. The customer data can look like that shown in the next
    section.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，业务应深入挖掘消费者数据，并生成对基础的理解。客户数据可能如下节所示。
- en: 2.6.2 Dataset for the analysis
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.2 分析用数据集
- en: We take as an example an apparel retailer that has a loyalty program and that
    saves the customer’s transaction details. The various (not exhaustive) data sources
    are shown in figure 2.33.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以一个拥有忠诚度计划并保存客户交易详情的服装零售商为例。各种（非详尽）数据源如图2.33所示。
- en: '![figure](../Images/CH02_F33_Verdhan.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F33_Verdhan.png)'
- en: Figure 2.33 Data sources for an apparel retail store
  id: totrans-406
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.33 服装零售店的数据源
- en: We can have store details, such as store ID, store name, city, area, number
    of employees, etc. We can have an item hierarchies table, which has all the details
    of the items like price, category, etc. Then we can have customer demographic
    details like age, gender, city, and customer transactional history. Clearly, by
    joining such tables, we will be able to create a master table that will have all
    the details in one place.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以拥有商店详情，如商店 ID、商店名称、城市、区域、员工数量等。我们可以有一个项目层级表，其中包含所有项目的详细信息，如价格、类别等。然后我们可以有客户人口统计详情，如年龄、性别、城市和客户交易历史。显然，通过连接这些表，我们将能够创建一个包含所有详细信息的主表。
- en: Note  I advise you to develop a good skill set for SQL. It is required in almost
    all of the domains related to data—be it data science, data engineering, or data
    visualization, SQL is ubiquitous.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我建议你培养良好的 SQL 技能。在几乎所有的与数据相关的领域都需要 SQL——无论是数据科学、数据工程还是数据可视化，SQL 都无处不在。
- en: Figure 2.34 is an example of a master table. This is not an exhaustive list
    of variables, and the number of variables can be much larger than the ones shown.
    The master table has some raw variables like Revenue, Invoices, etc., and some
    derived variables like Average Transaction Value, Average Basket Size, etc.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.34 是主表的一个示例。这不是变量的完整列表，变量的数量可能比显示的要多得多。主表包含一些原始变量，如收入、发票等，以及一些派生变量，如平均交易价值、平均购物篮大小等。
- en: '![figure](../Images/CH02_F34_Verdhan.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH02_F34_Verdhan.png)'
- en: Figure 2.34 A master table
  id: totrans-411
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.34 主表
- en: We could also take an example of a telecom operator. In that subscriber usage,
    call rate, revenue, days spent on the network, data usage, etc., will be the attributes
    we analyze. Hence, based on the business domain at hand, the datasets will change.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以举一个电信运营商的例子。在那个订阅者使用情况中，通话费率、收入、网络使用天数、数据使用等将是我们要分析的属性。因此，根据手头的业务领域，数据集将有所不同。
- en: Once we have the dataset, we generally create derived attributes from it. For
    example, the average transaction value attribute is total revenue divided by the
    number of invoices. We create such attributes in addition to the raw variables
    we already have.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了数据集，我们通常从中创建派生属性。例如，平均交易价值属性是总收入除以发票数量。我们在已有的原始变量之外创建这样的属性。
- en: 2.6.3 Suggested solutions
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.3 建议的解决方案
- en: 'There can be multiple solutions to the problem, some of which we include in
    the following:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题可能有多个解决方案，其中一些我们将在以下内容中包括：
- en: We can create a dashboard to depict the major key performance indicators. This
    will allow us to analyze the history and take necessary actions based on it. But
    the dashboard will only show the information that we already know (to some extent).
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以创建一个仪表板来展示主要的关键绩效指标。这将使我们能够分析历史数据并据此采取必要的行动。但仪表板只会显示我们已知（在一定程度上）的信息。
- en: We can perform data analysis using some of techniques we used in the solutions
    in the earlier sections. This will solve a part of the problem and, moreover,
    it is difficult to consider multiple dimensions simultaneously.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用在前面章节中使用的某些技术进行数据分析。这将解决部分问题，而且同时考虑多个维度是困难的。
- en: We can create predictive models to predict if the customers are going to shop
    in the coming months or are going to churn in the next *X* days, but this will
    not solve the problem completely. To be clear, “churn” here means that the customer
    no longer shops with the retailer in the next *X* days. Here, duration *X* is
    defined as per the business domain. For example, for the telecom domain, *X* will
    be less than in the insurance domain. This is due to the fact that people use
    mobile phones every day, whereas in the insurance domain, most customers pay the
    premium yearly. So customer interaction is less for insurance.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以创建预测模型来预测客户在接下来的几个月内是否会购物，或者在未来 *X* 天内是否会流失，但这并不能完全解决问题。为了明确，“流失”在这里的意思是客户在接下来的
    *X* 天内不再与零售商购物。在这里，持续时间 *X* 是根据业务领域定义的。例如，在电信领域，*X* 将小于在保险领域。这是因为人们每天都在使用手机，而在保险领域，大多数客户每年支付一次保费。因此，保险领域的客户互动较少。
- en: We can create customer segmentation solutions wherein we group customers based
    on their historical trends and attributes. This is the solution we will use to
    solve this business problem.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以创建客户细分解决方案，根据客户的历史趋势和属性将客户分组。这是我们用来解决这个业务问题的解决方案。
- en: 2.6.4 Solution for the problem
  id: totrans-420
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.4 解决方案
- en: 'Recall figure 1.9 in chapter 1, where we discussed the steps we follow in the
    machine learning algorithm. Everything starts with defining the business problem
    and then we move on to data discovery, preprocessing, etc. For our case study
    here, we will utilize a similar strategy. We have already defined the business
    problem; data discovery is done and we have completed the exploratory data analysis
    and the preprocessing of the data. To create a segmentation solution using clustering,
    follow these steps:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第1章中的图1.9，我们讨论了在机器学习算法中遵循的步骤。一切从定义业务问题开始，然后我们继续进行数据发现、预处理等。在我们的案例研究中，我们将采用类似的策略。我们已经定义了业务问题；数据发现已完成，我们已经完成了探索性数据分析和数据的预处理。要使用聚类创建分段解决方案，请遵循以下步骤：
- en: We start with finalizing the dataset we wish to feed to the clustering algorithms.
    We might have created some derived variables, treated some missing values or outliers,
    etc. In the case study, we would want to know the minimum/maximum/average values
    of transactions, invoices, items bought, etc. We would be interested to know the
    gender and age distribution. We also would like to know the mutual relationships
    between these variables, such as if women customers use the online mode more than
    male customers. All of these questions are answered as part of this step.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从确定要输入聚类算法的数据集开始。我们可能已经创建了某些派生变量，处理了一些缺失值或异常值等。在案例研究中，我们想知道交易、发票、购买物品等的最大/最小/平均值。我们会对性别和年龄分布感兴趣。我们还想知道这些变量之间的相互关系，例如，女性客户是否比男性客户更倾向于使用在线模式。所有这些问题都是这一步骤的一部分。
- en: TIP  A Python Jupyter notebook is checked in at the GitHub repository, which
    provides detailed steps and code for the exploratory data analysis and data preprocessing.
    Check it out!
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: TIP   在GitHub仓库中检查了Python Jupyter笔记本，其中提供了探索性数据分析和数据预处理的详细步骤和代码。查看它！
- en: 2\. We create the first solution using k-means clustering followed by hierarchical
    clustering. For each of the algorithms, iterations are done by changing hyperparameters.
    In the case study, we will choose parameters like the number of visits, total
    revenue, distinct categories purchased, online/offline transactions ratio, gender,
    age, etc., as parameters for clustering.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 我们使用k-means聚类和层次聚类创建第一个解决方案。对于每个算法，通过改变超参数进行迭代。在案例研究中，我们将选择如访问次数、总收入、独特购买类别、在线/离线交易比率、性别、年龄等作为聚类的参数。
- en: 3\. A final version of the algorithm and respective hyperparameters are chosen.
    The clusters are analyzed further in the light of business understanding.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 选择算法的最终版本及其相应的超参数。在业务理解的基础上进一步分析聚类。
- en: 4\. More often, the clusters are merged or broken, depending on the size of
    the observations and the nature of the attributes present in them. For example,
    if the total customer base is 1 million, it will be really hard to take action
    on a cluster of size 100\. At the same time, it will be equally difficult to manage
    a cluster of size 700,000\.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 更常见的是，根据观察值的大小和其中属性的性质，聚类会被合并或拆分。例如，如果总客户群为100万，对100人的聚类采取行动将非常困难。同时，管理一个700,000人的聚类也将同样困难。
- en: 5\. We then analyze the clusters we finally have. The clusters distribution
    is checked for the variables, their distinguishing factors are understood, and
    we give logical names to the clusters. We can expect to see such a clustering
    output as shown in figure 2.35\.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 然后我们分析我们最终拥有的聚类。检查变量的聚类分布，了解它们的区分因素，并为聚类给出逻辑名称。我们可以预期看到如图2.35所示的聚类输出。
- en: '![figure](../Images/CH02_F35_Verdhan.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F35_Verdhan.png)'
- en: Figure 2.35 Segmentation based on a few dimensions like response, life stage,
    engagement, and spending patterns. The dimensions are not exhaustive, and in a
    real-world business problem, the number of dimensions can be higher.
  id: totrans-429
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.35 基于响应、生命周期、参与度和消费模式等少数维度的分段。这些维度并不全面，在现实世界的业务问题中，维度的数量可能更多。
- en: In the example clusters shown, we have depicted spending patterns, responsiveness
    to previous campaigns, life stage, and overall engagement as a few dimensions.
    Respective subdivisions of each of these dimensions are also shown. The clusters
    will be a logical combination of these dimensions. The actual dimensions can be
    much higher.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例聚类中，我们展示了消费模式、对先前活动的响应、生命周期和整体参与度作为几个维度。这些维度的相应子分类也显示出来。聚类将是这些维度的逻辑组合。实际的维度可能更多。
- en: The segmentation shown in figure 2.35 can be used for multiple domains and businesses.
    The parameters and attributes might change, the business context may be different,
    the extent of data available might vary—but the overall approach remains similar.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.35中展示的分割可以用于多个领域和企业。参数和属性可能会变化，业务背景可能不同，可用的数据量可能有所不同——但整体方法保持相似。
- en: 'In addition to the applications we saw in the last section, let’s examine a
    few use cases here:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上一节中我们看到的应用之外，让我们在这里考察一些其他用例：
- en: Market research utilizes clustering to segment the groups of consumers into
    market segments; then the groups can be analyzed better in terms of their preferences.
    Product placement can be improved, pricing can be made tighter, and geography
    selection will be more scientific.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场研究利用聚类将消费者群体分割成市场细分；然后可以更好地分析这些群体的偏好。产品定位可以改进，定价可以更紧密，地理选择将更加科学。
- en: In the bioinformatics and medical industry, clustering can be used to group
    genes into distinct categories. Groups of genes can be segmented and comparisons
    can be assessed by analyzing the attributes of the groups.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生物信息学和医疗行业，聚类算法可以用来将基因分组到不同的类别中。通过分析这些基因组的属性，可以对基因组进行分割，并评估比较。
- en: Clustering is used as an effective data preprocessing step before we create
    algorithms using supervised learning solutions. It can also be used to reduce
    the data size by focusing on the data points belonging to a cluster.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们使用监督学习解决方案创建算法之前，聚类被用作一个有效的数据预处理步骤。它还可以通过关注属于某个聚类的数据点来减少数据量。
- en: Clustering is utilized for pattern detection across both structured and unstructured
    datasets. We have already studied the case for a structured dataset. For text
    data, it can be used to group similar types of documents, journals, news, etc.
    We can also employ clustering to work and develop solutions for images. We will
    study unsupervised learning solutions for text and images in later chapters.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类被用于在结构化和非结构化数据集中进行模式检测。我们已经研究了结构化数据集的案例。对于文本数据，它可以用来将类似类型的文档、期刊、新闻等进行分组。我们还可以使用聚类来处理和开发图像解决方案。我们将在后面的章节中研究文本和图像的无监督学习解决方案。
- en: As the algorithms work on similarity measurements, clustering can be used to
    segment the incoming dataset as fraud or genuine, which can be used to reduce
    the number of criminal activities.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于算法基于相似度测量，聚类可以用来将传入的数据集分割为欺诈或真实，这可以用来减少犯罪活动的数量。
- en: The use cases of clustering are many. We have discussed only the prominent ones.
    Clustering is one of the algorithms that changes the working methodologies and
    generates a lot of insights around the data. It is widely used across telecom;
    retail; banking, financial services, and insurance; aviation; and others.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法的应用场景非常多样。我们在这里只讨论了其中一些突出的例子。聚类算法是那些能够改变工作方法并围绕数据产生大量洞察力的算法之一。它在电信、零售、银行、金融服务和保险、航空以及其他领域得到广泛应用。
- en: At the same time, there are a few problems with the algorithm. We next examine
    the common problems we face with clustering.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，算法也存在一些问题。接下来，我们将考察我们在聚类中面临的常见问题。
- en: 2.7 Common challenges faced in clustering
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 聚类中常见的挑战
- en: 'Clustering is not a completely straightforward solution without any challenges.
    Like any other solution in the world, clustering too has its share of problems.
    The most common challenges we face in clustering are as follows:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类并不是一个完全没有挑战的简单解决方案。像世界上任何其他解决方案一样，聚类也有其问题。我们在聚类中面临的最常见挑战如下：
- en: '*Too much data*—Sometimes the magnitude of the data is quite big, and there
    are a lot of dimensions available. In such a case, it becomes difficult to manage
    the dataset. The computation power might be limited, and like any project, there
    is finite time available. To overcome the problem, we can'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据量过大*——有时数据的规模相当大，有很多维度可用。在这种情况下，管理数据集变得困难。计算能力可能有限，而且像任何项目一样，时间有限。为了克服这个问题，我们可以'
- en: Try to reduce the number of dimensions by finding the most significant variables
    by using a supervised learning-based regression approach or decision tree algorithm,
    etc.
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试通过使用基于监督学习的回归方法或决策树算法等，找到最显著的变量来减少维度数量。
- en: Reduce the number of dimensions by employing principal component analysis or
    singular value decomposition, etc.
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用主成分分析或奇异值分解等方法来减少维度数量。
- en: '*A noisy dataset*—“Garbage in, garbage out” is a cliché that is true for clustering
    too. If the dataset is messy, it creates a lot of problems. The problems can include'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*噪声数据集*——“垃圾输入，垃圾输出”这个陈词滥调对于聚类也是适用的。如果数据集很乱，就会产生很多问题。这些问题可能包括'
- en: Missing values (i.e., NULL, NA, ?, blanks, etc.).
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值（即NULL、NA、？、空白等）。
- en: Outliers present in the dataset.
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集中的异常值。
- en: 'Junk values like #€¶§^ etc., present in the dataset.'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集中存在的垃圾值，如#€¶§^等。
- en: Wrong entries made in the data. For example, if a name is entered in the Revenue
    field, it is an incorrect entry.
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中的错误条目。例如，如果在收入字段中输入了一个名称，那么它就是一个错误的条目。
- en: We discuss the steps and the process to resolve these problems in later chapters.
    In this chapter, we are examining how to work with categorical variables.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节中讨论解决这些问题的步骤和过程。在本章中，我们正在探讨如何处理分类变量。
- en: '*Categorical variables*—While discussing, recall the problem where k-means
    was not able to use categorical variables. We solve that problem next.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类变量*——在讨论时，回想一下k-means无法使用分类变量的问题。我们将在下一节解决这个问题。'
- en: To convert categorical variables into numeric ones, we can use *one-hot encoding*.
    This technique adds additional columns equal to the number of distinct classes
    as shown in the following figure. The variable city has unique values as London
    and New Delhi. We can observe that two additional columns have been created with
    0 or 1 filled in for the values (see figure 2.36).
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将分类变量转换为数值变量，我们可以使用*独热编码*。这种技术会添加额外的列，数量等于不同类别的数量，如下面的图所示。变量city有独特的值，如伦敦和新德里。我们可以观察到，已经创建了两个额外的列，用0或1填充了值（见图2.36）。
- en: '![figure](../Images/CH02_F36_Verdhan.png)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F36_Verdhan.png)'
- en: Figure 2.36 Using one-hot encoding to convert categorical variables into numeric
    ones
  id: totrans-454
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.36 使用独热编码将分类变量转换为数值变量
- en: Using one-hot encoding does not always ensure an effective and efficient solution.
    Imagine if the number of cities in this example is 100; then we will have 100
    additional columns in the dataset, and most of the values will be filled in with
    0\. Hence, in such a situation, it is advisable to group a few values.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 使用独热编码并不总是能确保有效的解决方案。想象一下，如果这个例子中的城市数量是100，那么数据集中将会有100个额外的列，其中大部分值将被填充为0。因此，在这种情况下，建议将一些值分组。
- en: '*Distance metrics*—With different distance metrics, we might get different
    results. Though there is no “one size fits all,” Euclidean distance is most often
    used for measuring distance.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*距离度量*——使用不同的距离度量，我们可能会得到不同的结果。尽管没有“一刀切”的方法，但欧几里得距离通常用于测量距离。'
- en: '*Subjective interpretations*—Interpretations for the clusters are quite subjective.
    By using different attributes, completely different clustering can be done for
    the same datasets. As discussed earlier, the focus should be on solving the business
    problem at hand. This holds the key to choosing the hyperparameters and the final
    algorithm.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主观解释*——对于聚类的解释相当主观。通过使用不同的属性，可以对相同的数据集进行完全不同的聚类。如前所述，重点应放在解决当前的业务问题上。这是选择超参数和最终算法的关键。'
- en: '*Time-consuming*—Since a lot of dimensions are dealt with simultaneously, sometimes
    converging the algorithm takes a lot of time.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*耗时*——由于同时处理了很多维度，有时算法收敛需要花费很多时间。'
- en: Despite all these challenges, clustering is a widely recognized and utilized
    technique.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在所有这些挑战，聚类仍然是一种被广泛认可和使用的技巧。
- en: 2.8 Concluding thoughts
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 总结思考
- en: Unsupervised learning is not an easy task. But it is certainly a very engaging
    one. It does not require any target variable, and the solution identifies the
    patterns itself, which is one of the biggest advantages of unsupervised learning
    algorithms. And the implementations are already having a great effect on the business
    world. We studied one of these solution classes called clustering in this chapter.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习不是一个容易的任务。但它确实是一个非常吸引人的任务。它不需要任何目标变量，解决方案会自己识别模式，这是无监督学习算法最大的优点之一。并且这些解决方案已经在商业世界中产生了巨大的影响。在本章中，我们研究了这些解决方案类别中的一个，称为聚类。
- en: Clustering is an unsupervised learning solution that is useful for pattern identifications,
    exploratory analysis, and, of course, segmenting the data points. Organizations
    heavily use clustering algorithms and proceed to the next level of understanding
    consumer data. Better prices can be offered, more relevant offers can be suggested,
    consumer engagement can be improved, and overall customer experience becomes better.
    After all, a happy consumer is the goal of any business. Clustering can be used
    not only for structured data but for text data, images, videos, and audio too.
    Due to its capability to find patterns across multiple datasets using a large
    number of dimensions, clustering is the go-to solution whenever we want to analyze
    multiple dimensions together.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督学习解决方案，它对于模式识别、探索性分析和当然，数据点的分割都是非常有用的。组织机构大量使用聚类算法，并继续深入理解消费者数据。可以提供更好的价格，提出更相关的优惠，提高消费者参与度，并使整体客户体验变得更好。毕竟，让消费者满意是任何企业的目标。聚类不仅可以用于结构化数据，还可以用于文本数据、图像、视频和音频。由于其能够在多个数据集和大量维度中找到模式的能力，聚类是我们想要一起分析多个维度时的首选解决方案。
- en: In this second chapter of this book, we introduced concepts of unsupervised-based
    clustering methods. We examined different types of clustering algorithms—k-means
    clustering, hierarchical clustering, and DBSCAN clustering—along with their mathematical
    concepts, respective use cases, and pros and cons with an emphasis on creating
    actual Python code for these datasets.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第二章中，我们介绍了基于无监督的聚类方法的概念。我们检查了不同类型的聚类算法——k-means聚类、层次聚类和DBSCAN聚类——以及它们的数学概念、相应的用例和优缺点，并着重于为这些数据集创建实际的Python代码。
- en: In the following chapter, we will study dimensionality reduction techniques
    like principal component analysis and singular value decomposition. We will discuss
    the building blocks for techniques, their mathematical foundation, advantages
    and disadvantages, and use cases and perform actual Python implementation.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究降维技术，如主成分分析和奇异值分解。我们将讨论技术的基础构件、它们的数学基础、优缺点以及用例，并执行实际的Python实现。
- en: 2.9 Practical next steps and suggested readings
  id: totrans-465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.9 实践下一步行动和建议阅读
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 以下提供了一些下一步行动的建议和一些有用的阅读材料：
- en: Get the online retail data from [https://mng.bz/dXqo](https://mng.bz/dXqo).
    This dataset contains all the online transactions occurring between January 12,
    2010, and September 12, 2011, for a UK-based retailer. Apply the three algorithms
    described in the chapter to identify which customers the company should target
    and why.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从[https://mng.bz/dXqo](https://mng.bz/dXqo)获取在线零售数据。这个数据集包含了2010年1月12日至2011年9月12日之间一个英国零售商的所有在线交易。应用章节中描述的三个算法来识别公司应该针对哪些客户以及为什么。
- en: Get the IRIS dataset from [https://www.kaggle.com/uciml/iris](https://www.kaggle.com/uciml/iris).
    It includes three iris species with 50 samples, each having some properties of
    the flowers. Use k-means and DBSCAN and compare the results.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从[https://www.kaggle.com/uciml/iris](https://www.kaggle.com/uciml/iris)获取IRIS数据集。它包括三种鸢尾花物种，每个物种有50个样本，每个样本都有一些花的属性。使用k-means和DBSCAN算法，并比较结果。
- en: Explore the dataset at UCI for clustering at [http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php).
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在UCI探索聚类数据集[http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)。
- en: 'Study the following papers on k-means clustering, hierarchical clustering,
    and DBSCAN clustering:'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究以下关于k-means聚类、层次聚类和DBSCAN聚类的论文：
- en: K-means clustering
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means聚类
- en: '[https://mng.bz/rKqJ](https://mng.bz/rKqJ)'
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://mng.bz/rKqJ](https://mng.bz/rKqJ)'
- en: '[https://mng.bz/VVEy](https://mng.bz/VVEy)'
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://mng.bz/VVEy](https://mng.bz/VVEy)'
- en: '[https://ieeexplore.ieee.org/document/1017616](https://ieeexplore.ieee.org/document/1017616)'
  id: totrans-474
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://ieeexplore.ieee.org/document/1017616](https://ieeexplore.ieee.org/document/1017616)'
- en: Hierarchical clustering
  id: totrans-475
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类
- en: '[https://ieeexplore.ieee.org/document/7100308](https://ieeexplore.ieee.org/document/7100308)'
  id: totrans-476
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://ieeexplore.ieee.org/document/7100308](https://ieeexplore.ieee.org/document/7100308)'
- en: '[https://mng.bz/xKqd](https://mng.bz/xKqd)'
  id: totrans-477
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://mng.bz/xKqd](https://mng.bz/xKqd)'
- en: '[https://mng.bz/AQno](https://mng.bz/AQno)'
  id: totrans-478
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://mng.bz/AQno](https://mng.bz/AQno)'
- en: DBSCAN clustering
  id: totrans-479
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN聚类
- en: '[https://arxiv.org/pdf/1810.13105.pdf](https://arxiv.org/pdf/1810.13105.pdf)'
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1810.13105.pdf](https://arxiv.org/pdf/1810.13105.pdf)'
- en: '[https://ieeexplore.ieee.org/document/9356727](https://ieeexplore.ieee.org/document/9356727)'
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://ieeexplore.ieee.org/document/9356727](https://ieeexplore.ieee.org/document/9356727)'
- en: Summary
  id: totrans-482
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Clustering is used for a variety of purposes across all industries, such as
    retail, telecom, finance, and pharma. Clustering solutions are implemented for
    customer and marketing segmentation to better understand the customer base, which
    further improves targeting.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类在所有行业中都有多种用途，例如零售、电信、金融和制药。聚类解决方案被用于客户和市场营销细分，以更好地理解客户基础，这进一步提高了定位。
- en: Clustering groups objects with similar attributes into segments, aiding in data
    understanding and pattern discovery without needing a target variable.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类将具有相似属性的物体分组到段中，有助于数据理解和模式发现，而无需目标变量。
- en: Using clustering, we find the underlying patterns in a dataset and identify
    the natural groupings in the data.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过聚类，我们可以在数据集中找到潜在的规律，并识别数据的自然分组。
- en: There can be multiple clustering techniques based on the methodology. A few
    examples are k-means clustering, hierarchical clustering, DBSCAN, and fuzzy clustering.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据方法论，可以有多种聚类技术。一些例子包括k-means聚类、层次聚类、DBSCAN和模糊聚类。
- en: Different clustering algorithms (k-means, hierarchical, DBSCAN) offer distinct
    pros and cons, and each is suitable for different data characteristics and purposes.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的聚类算法（k-means、层次、DBSCAN）提供不同的优缺点，每种算法都适用于不同的数据特征和目的。
- en: Clustering is categorized into hard clustering, where objects belong to a single
    cluster, and soft clustering, where objects can belong to multiple clusters.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类分为硬聚类，其中对象属于单个聚类，和软聚类，其中对象可以属于多个聚类。
- en: Different clustering attributes and techniques, such as centroid-based, density-based,
    and distribution models, lead to varied clustering results.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的聚类属性和技术，如基于质心的、基于密度的和分布模型，会导致不同的聚类结果。
- en: Effective clustering algorithms produce comprehensible, scalable, and independent
    clusters, handling outliers and multiple data types with minimal domain input.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效的聚类算法能够生成可理解、可扩展和独立的聚类，通过最小领域输入处理异常值和多种数据类型。
- en: Distance metrics for clustering include Euclidean, Chebyshev, Manhattan, and
    cosine distances.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类的距离度量包括欧几里得、切比雪夫、曼哈顿和余弦距离。
- en: Centroid-based clustering measures similarity based on the distance to the centroid
    of clusters.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于质心的聚类通过到聚类质心的距离来衡量相似性。
- en: K-means clustering creates nonoverlapping clusters by specifying the number
    of clusters, *k*, and assigning data points to the nearest center iteratively.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means聚类通过指定聚类数量，*k*，以及迭代地将数据点分配到最近的中心来创建非重叠的聚类。
- en: The elbow method is a common technique to determine the optimal number of clusters
    in k-means clustering.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肘部方法是一种常用的技术，用于确定k-means聚类中的最佳聚类数量。
- en: K-means is based on the centroid of the cluster.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means聚类基于聚类的质心。
- en: Hierarchical clustering creates clusters based on connectivity and does not
    require a predefined number of clusters.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类基于连通性创建聚类，不需要预定义的聚类数量。
- en: Hierarchical clustering can be agglomerative (bottom-up) or divisive (top-down)
    and uses linkage criteria to measure distances.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类可以是聚类的（自下而上）或分级的（自上而下），并使用链接标准来衡量距离。
- en: DBSCAN identifies clusters based on point density and effectively distinguishes
    outliers.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN根据点密度识别聚类，并有效地区分异常值。
- en: DBSCAN does not require specifying the number of clusters and is suited for
    irregular-shaped clusters.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN不需要指定聚类数量，适用于不规则形状的聚类。
- en: Measuring clustering accuracy involves metrics like WCSS, intercluster sum of
    squares, silhouette value, and the Dunn index.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量聚类精度涉及WCSS、簇间平方和、轮廓值和Dunn指数等指标。
