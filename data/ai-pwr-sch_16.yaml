- en: 13 Semantic search with dense vectors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 使用密集向量进行语义搜索
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Semantic search using embeddings from LLMs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLMs的嵌入进行语义搜索
- en: An introduction to Transformers, and their effect on text representation and
    retrieval
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers简介及其对文本表示和检索的影响
- en: Building autocomplete using Transformer models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Transformer模型构建自动补全
- en: Using ANN search and vector quantization to speed up dense vector retrieval
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ANN搜索和向量量化来加速密集向量检索
- en: Semantic search with bi-encoders and cross-encoders
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用双编码器和交叉编码器进行语义搜索
- en: In this chapter, we’ll start our journey into dense vector search, where the
    hyper-contextual vectors generated by large language models (LLMs) drive significant
    improvements to the interpretation of queries, documents, and search results.
    Generative LLMs (like ChatGPT by OpenAI and many other commercial and open source
    alternatives) are also able to use these vectors to generate new content, including
    query expansion, search training data, and summarization of search results, as
    we’ll explore further in the coming chapters.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始我们的密集向量搜索之旅，其中大型语言模型（LLMs）生成的超上下文向量将显著提高对查询、文档和搜索结果的解释。生成式LLMs（如OpenAI的ChatGPT和许多其他商业和开源替代品）也能够使用这些向量生成新内容，包括查询扩展、搜索训练数据和搜索结果摘要，我们将在接下来的章节中进一步探讨。
- en: The state of the art in LLMs is changing on a monthly (and sometimes almost
    daily) basis, but even the best general-purpose models can currently be outperformed
    on specific tasks by fine-tuning other smaller models for those tasks. Over the
    next few chapters, we’ll discuss the concepts behind LLMs and how to best use
    them in your search application. We’ll introduce Transformers in this chapter
    and discuss how to use them for semantic search with dense vectors. We’ll cover
    fine-tuning an LLM for question answering in chapter 14 and leveraging LLMs and
    other foundation models for generative search in chapter 15.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）的尖端技术每月（有时几乎是每日）都在变化，但即使是最好的通用模型，在特定任务上也可能被针对这些任务微调的其他较小模型所超越。在接下来的几章中，我们将讨论LLMs背后的概念以及如何在您的搜索应用中最佳地使用它们。在本章中，我们将介绍Transformers，并讨论如何使用它们进行语义搜索和密集向量。在第14章中，我们将讨论如何微调LLM以进行问答，在第15章中，我们将讨论如何利用LLMs和其他基础模型进行生成式搜索。
- en: 'Our story begins with what you learned in section 2.5: that we can represent
    context as numerical vectors, and we can compare these vectors to see which ones
    are closer using a similarity metric. In chapter 2, we demonstrated the concept
    of searching on dense vectors, a technique known as *dense vector search*, but
    our examples were simple and contrived (searching on made-up food attributes).
    In this chapter, we’ll pose the questions “How can we convert real-world unstructured
    text into a high dimensional dense vector space that attempts to model the actual
    meaning of the text representation?” and “How can we use this representation of
    knowledge for advanced search applications?”.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的故事从你在2.5节中学到的内容开始：我们可以将上下文表示为数值向量，我们可以使用相似度度量来比较这些向量，看哪些更接近。在第2章中，我们展示了在密集向量上搜索的概念，这是一种称为*密集向量搜索*的技术，但我们的例子简单且人为（基于虚构的食物属性进行搜索）。在本章中，我们将提出问题：“我们如何将现实世界的非结构化文本转换为高维密集向量空间，该空间试图模拟文本表示的实际意义？”以及“我们如何使用这种知识表示来为高级搜索应用服务？”
- en: 13.1 Representation of meaning through embeddings
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 通过嵌入表示意义
- en: 'We’re going to use language translation as an example to understand what we
    mean by “dense vector” embeddings. Take the following two sentences: “Hello to
    you!” (English) and “Barev Dzes” (Armenian). These two expressions hold approximately
    the same meaning: each is a greeting, with some implied sense of formality.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用语言翻译作为例子来理解我们所说的“密集向量”嵌入的含义。以下两句：“Hello to you!”（英语）和“Barev Dzes”（亚美尼亚语）。这两个表达大约具有相同的意义：每个都是问候，带有一些隐含的正式感。
- en: Computationally, to successfully respond to the greeting of “Hello to you!”
    the machine must both comprehend the meaning of the prompt and also comprehend
    all the possible ideal responses, in the same vector space. When the answer is
    decided upon, the machine must then express it to a person by generating the label
    from the answer’s vector representation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算上，为了成功回应“Hello to you!”这样的问候，机器必须同时理解提示的意义以及所有可能的理想回应，在相同的向量空间中。当答案确定后，机器必须通过从答案的向量表示中生成标签来表达它给一个人的信息。
- en: This vector representation of meaning is called an *embedding*. Embeddings are
    used interchangeably between natural language processing (NLP) tasks and can be
    further molded to meet specific use cases. We generated embeddings from an LLM
    (`all -mpnet-base-v2`) in chapter 9, but we glossed over most of the details on
    how embeddings work. In this chapter, we’ll introduce techniques and tools for
    getting embeddings from text, and we’ll use them to significantly enhance query
    and document interpretation within our search engine.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种意义的向量表示称为*嵌入*。嵌入在自然语言处理（NLP）任务中可以互换使用，并且可以进一步塑形以满足特定用例。我们在第9章中生成了来自LLM（`all
    -mpnet-base-v2`）的嵌入，但我们忽略了关于嵌入如何工作的大多数细节。在本章中，我们将介绍从文本中获取嵌入的技术和工具，并将它们用于显著增强我们搜索引擎中的查询和文档解释。
- en: Natural language processing
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: NLP is the set of techniques and tools that converts unstructured text into
    machine-actionable data. The field of NLP is quite large and includes many research
    areas and types of problems (NLP tasks) to be solved. A comprehensive list of
    the problem areas is maintained on the NLP-Progress site ([https://nlpprogress.com](https://nlpprogress.com)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是一套技术和工具，它将非结构化文本转换为机器可操作的数据。NLP领域相当广泛，包括许多研究领域和待解决的问题（NLP任务）。NLP-Progress网站上维护了一个问题领域
    comprehensive 列表（[https://nlpprogress.com](https://nlpprogress.com)）。
- en: We will be focusing specifically on applying NLP to information retrieval, an
    important requirement of AI-powered search.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将特别关注将自然语言处理（NLP）应用于信息检索，这是人工智能搜索的一个重要要求。
- en: 'One important point is worth noting up front: behind the two short English
    and Armenian greetings we mentioned, there are deep cultural nuances. Each of
    them carries a rich history, and learning them thus carries the context of those
    histories. This was the case with the semantic knowledge graphs we explored in
    chapter 5, but those only used the context of documents within the search engine
    as their model. Transformers are usually trained on much larger corpuses of text,
    bringing in significantly more of this nuanced context from external sources.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的要点需要提前指出：在我们提到的两个简短的英语和亚美尼亚问候语背后，有着深刻的文化细微差别。每一个都承载着丰富的历史，学习它们因此承载了这些历史的背景。这与我们在第5章中探讨的语义知识图谱的情况相同，但那些只使用了搜索引擎内文档的上下文作为它们的模型。Transformer通常在更大的文本语料库上训练，从而从外部来源引入了更多的这种细微的上下文。
- en: We can use the human brain as an analogy for how Transformer models learn to
    represent meaning. How did you, as a baby, child, teen, and beyond, learn the
    meaning of words? You were told, and you consumed knowledge and its representation.
    People who taught you already had this knowledge and the power to express it.
    Aside from someone pointing out a cat and saying “kitty” to you, you also watched
    movies and videos and then moved on to literature and instructional material.
    You read books, blogs, periodicals, and letters. Through all of your experiences,
    you incorporated this knowledge into your brain, creating a dense representation
    of concepts and how they relate to one another, allowing you to reason about them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将人脑作为Transformer模型如何学习表示意义的类比。你们作为婴儿、儿童、青少年以及更久之前，是如何学习词语的意义的呢？你们被告知，并吸收了知识和其表现形式。教你们的人已经拥有了这种知识和表达能力。除了有人指出一只猫并对你们说“kitty”之外，你们还观看了电影和视频，然后转向文学和教学材料。你们阅读书籍、博客、期刊和信件。通过所有这些经历，你们将这些知识融入大脑，创建了一个概念及其相互关系密集的表示，使你们能够对它们进行推理。
- en: Can we impart to machines the same content from which we obtained this power
    of language, and then expect them to understand and respond sensibly when queried?
    Hold on to your hats!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否将我们从语言能力中获得的内容传授给机器，并期待它们在查询时能够理解和合理地做出回应？请系好你们的帽子！
- en: 13.2 Search using dense vectors
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 使用密集向量进行搜索
- en: Understanding when to use dense vectors for search instead of sparse vectors
    requires understanding how to process and relate text. This section briefly reviews
    how sparse vector search works in comparison with dense vector search. We will
    also introduce *nearest-neighbor search* as one type of similarity used for dense
    vector search, as compared to BM25 (the most common similarity function used for
    sparse vector search).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 理解何时使用密集向量进行搜索而不是稀疏向量，需要了解如何处理和关联文本。本节简要回顾了与密集向量搜索相比，稀疏向量搜索是如何工作的。我们还将介绍*最近邻搜索*作为密集向量搜索中用于相似度的一种类型，与BM25（用于稀疏向量搜索的最常见相似度函数）相比。
- en: Vector-based nearest-neighbor search
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于向量的最近邻搜索
- en: 'Also known as KNN (*k*-nearest neighbor), vector-based nearest-neighbor search
    is the problem space of indexing numerical vectors of a uniform dimensionality
    into a data structure and searching that data structure with a query vector for
    the closest `k`- related vectors. We mentioned in chapter 3 that there are many
    similarity measures for comparing numerical vectors: cosine similarity, dot product,
    Euclidean distance, and so on. We will use cosine similarity (implemented as dot
    product over unit-normalized vectors) throughout this chapter for vector similarity
    comparisons.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为KNN（*k*-最近邻），基于向量的最近邻搜索是将具有统一维度的数值向量索引到数据结构中，并使用查询向量搜索该数据结构以找到最近的`k`个相关向量的问题空间。我们在第3章中提到，有许多相似度度量用于比较数值向量：余弦相似度、点积、欧几里得距离等。在本章中，我们将使用余弦相似度（作为单位归一化向量上的点积实现）进行向量相似度比较。
- en: 13.2.1 A brief refresher on sparse vectors
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 稀疏向量的简要复习
- en: Sparse vector search is usually implemented using an inverted index. An inverted
    index is like what you find in the back of any textbook—a list of terms that reference
    their location in the source content. To efficiently find text, we structure information
    in an index by processing and normalizing tokens into a dictionary with references
    to the postings (the document identifiers and positions in which they occur).
    The resulting data structure is a sparse vector representation that allows for
    fast lookup of those tokens.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏向量搜索通常使用倒排索引实现。倒排索引就像你在任何教科书后面找到的那样——一个列出术语及其在源内容中位置的列表。为了有效地查找文本，我们通过处理和规范化标记到一个字典中，该字典包含对帖子（文档标识符和它们出现的位置）的引用来构建索引。结果数据结构是一个稀疏向量表示，允许快速查找这些标记。
- en: At search time, we tokenize and normalize the query terms and, using the inverted
    index, match the document hits for retrieval. Then we apply the BM25 formula to
    score the documents and rank them by similarity, as we covered in section 3.2.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索时，我们对查询词进行分词和规范化，并使用倒排索引匹配文档命中以进行检索。然后我们应用BM25公式对文档进行评分并按相似度进行排序，正如我们在第3.2节中所述。
- en: Applying scores for each query term and document feature gives us fast and relevant
    search, but this model suffers from the “query term dependence” model of relevance,
    in which the terms (and normalized forms) are retrieved and ranked. The problem
    is that it uses the presence (and count) of query term strings to search and rank
    instead of the *meaning* represented by those strings. Therefore, the relevance
    scores are only useful in a relative sense, to tell you which documents best matched
    the query, but not to measure if any of the documents were objectively good matches.
    Dense vector approaches, as we’ll see, can supply a more global sense of relevance
    that is also comparable across queries.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个查询词和文档特征应用分数可以为我们提供快速且相关的搜索，但这个模型受限于“查询词依赖”的相关性模型，其中检索和排序的是术语（及其规范化形式）。问题是它使用查询词字符串的存在（和计数）来搜索和排序，而不是这些字符串所代表的*意义*。因此，相关性分数仅在相对意义上有用，只能告诉你哪些文档与查询最匹配，但不能衡量任何文档是否是客观上的良好匹配。正如我们将看到的，密集向量方法可以提供更全局的相关性感知，并且可以在查询之间进行比较。
- en: 13.2.2 A conceptual dense vector search engine
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 概念密集向量搜索引擎
- en: We want to capture the meaning of content when we process documents, and we
    want to retrieve and rank based on the meaning and intent of the query when searching.
    With this goal in mind, we process documents to generate embeddings and then store
    those embeddings in the search index. At search time, we process queries to get
    embeddings and use those query embeddings to search the indexed document embeddings.
    Figure 13.1 shows a simplified diagram of this process, which we’ll expand upon
    in section 13.4\.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理文档时，我们希望捕捉内容的意义，当搜索时，我们希望根据查询的意义和意图进行检索和排序。带着这个目标，我们处理文档以生成嵌入，然后将这些嵌入存储在搜索索引中。在搜索时，我们处理查询以获取嵌入，并使用这些查询嵌入来搜索索引中的文档嵌入。图13.1显示了此过程的简化图，我们将在第13.4节中对其进行扩展。
- en: '![figure](../Images/CH13_F01_Grainger.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F01_Grainger.png)'
- en: Figure 13.1 Building and searching the embeddings index. The content is processed
    and added to the index from the left, and a user queries the index to retrieve
    results.
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.1 构建和搜索嵌入索引。内容从左侧处理并添加到索引中，用户查询索引以检索结果。
- en: The embeddings for both documents and queries exist in the same vector space.
    This is very important. If you map documents to one vector space and queries to
    another vector space, you’ll be matching apples to oranges. The embeddings must
    belong to the same space for this to work effectively.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 文档和查询的嵌入存在于同一个向量空间中。这一点非常重要。如果您将文档映射到一个向量空间，而将查询映射到另一个向量空间，您将会匹配苹果和橙子。为了有效地工作，嵌入必须属于同一个空间。
- en: But what is an “embedding” exactly, and how do we search one? Well, an embedding
    is a vector of some set number of dimensions representing information. That information
    could be a query, a document, a word, a sentence, an image or video, or any other
    type of information.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但“嵌入”究竟是什么，我们如何搜索它呢？嗯，嵌入是表示某些集合数量的维度的向量，代表信息。这些信息可以是查询、文档、单词、句子、图像或视频，或任何其他类型的信息。
- en: Embedding scope and chunking
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 嵌入范围和分块
- en: One important engineering task when working with embeddings is to figure out
    the right level of granularity for an embedding. An embedding can be made to represent
    a single word, sentence, paragraph, or much larger document.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理嵌入时，一个重要的工程任务是确定嵌入的正确粒度级别。嵌入可以用来表示单个单词、句子、段落或更大的文档。
- en: When generating embeddings, it is often useful to break apart larger documents
    into sections and generate a separate embedding for each section, a process known
    as *chunking*. You can chunk your content by sentence, paragraph, or other conceptual
    boundaries, and you can even create overlapping chunks to ensure that the process
    of splitting the document doesn’t destroy relevant context across splits.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成嵌入时，通常很有用将较大的文档分解成部分，并为每个部分生成一个单独的嵌入，这个过程称为*分块*。您可以通过句子、段落或其他概念边界来分块内容，甚至可以创建重叠的分块以确保分割文档的过程不会破坏分割之间的相关上下文。
- en: If your search engine supports multivalued vector fields, you can index many
    embeddings into a single document and match based on any of its embeddings. Alternatively,
    you can index a separate document for each chunk, each with a single embedding,
    and then store the original document ID as a field to be returned when the indexed
    chunk document is matched.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的搜索引擎支持多值向量场，您可以将多个嵌入索引到单个文档中，并根据其任何嵌入进行匹配。或者，您可以为每个块索引一个单独的文档，每个文档包含一个嵌入，并将原始文档ID作为字段存储，以便在索引的块文档匹配时返回。
- en: It is difficult for very large chunks to be fully represented by an embedding,
    just as it is difficult for very small chunks to contain the full context needed
    for an embedding, so figuring out the right chunking granularity for your app
    can be an important consideration for improving recall.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常大的块来说，很难完全由嵌入表示，就像对于非常小的块来说，很难包含嵌入所需的全部上下文一样，因此确定您应用程序的正确分块粒度可能是提高召回率的一个重要考虑因素。
- en: Since embeddings are represented as vectors, we can use cosine similarity (which
    was covered in depth in chapters 2–3) or another similar distance measurement
    to compare two embedding vectors to each other and get a similarity score. This
    allows us to compare the vector of a query to the vectors of all the documents
    in the content we want to search. The document vectors that are the most similar
    to the query vector are referred to as *nearest neighbors*. Figure 13.2 illustrates
    this with three 2D vectors.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于嵌入表示为向量，我们可以使用余弦相似度（在第2-3章中进行了深入探讨）或另一种类似的距离测量来比较两个嵌入向量，并得到一个相似度分数。这允许我们将查询的向量与我们要搜索的内容中所有文档的向量进行比较。与查询向量最相似的文档向量被称为*最近邻*。图13.2用三个二维向量说明了这一点。
- en: 'The cosine similarities between the vectors shown in figure 13.2, ordered by
    highest similarity first, are as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 按照相似度从高到低的顺序，图13.2中显示的向量之间的余弦相似度如下：
- en: '`cos(b, c) = 0.9762`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cos(b, c) = 0.9762`'
- en: '`cos(a, b) = 0.7962`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cos(a, b) = 0.7962`'
- en: '`cos(a, c) = 0.6459`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cos(a, c) = 0.6459`'
- en: It is clear both visually and mathematically that `b` and `c` are closest to
    each other, so we say that `b` and `c` are the most similar of the three vectors.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉和数学上都很明显，`b`和`c`彼此最接近，因此我们说`b`和`c`是这三个向量中最相似的。
- en: '![figure](../Images/CH13_F02_Grainger.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F02_Grainger.png)'
- en: Figure 13.2 Three vectors (a, b, and c) plotted on a Cartesian plane. The similarities
    between a and b, and between b and c, are illustrated using the cos*q* function.
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.2 在笛卡尔平面上绘制了三个向量（a、b和c）。使用cos*q*函数说明了a和b之间，以及b和c之间的相似性。
- en: We can easily apply cosine similarity to vectors of any length. In 3D space,
    we compare vectors with three features `[x, y, z]`. In a dense vector embedding
    space, we may use vectors with hundreds or thousands of dimensions. But no matter
    the number of dimensions, the formula is the same, as shown in figure 13.3.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地将余弦相似度应用于任何长度的向量。在三维空间中，我们比较具有三个特征 `[x, y, z]` 的向量。在密集向量嵌入空间中，我们可能使用具有数百或数千维的向量。但无论维数的数量如何，公式都是相同的，如图13.3所示。
- en: '![figure](../Images/grainger-ch13-eqs-0x.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/grainger-ch13-eqs-0x.png)'
- en: Figure 13.3 Formula for the cosine similarity of two vectors
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.3 两个向量余弦相似度的公式
- en: 'See section 3.1 for a recap of using this cosine similarity calculation to
    score the similarity between vectors. There, we walked through examples of calculating
    both the cosine similarity between vectors and the dot product between vectors.
    From the formula for cosine similarity (figure 13.3), however, you can see that
    the cosine is equal to the dot product (`a` `.` `b`) divided by the product of
    the lengths of the vectors (`|a| x |b|`). This means that if we can normalize
    the features in vectors `a` and `b` so that each of their lengths is `1` (a process
    called *unit-normalizing*), the cosine similarity and dot product are equal to
    each other:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 详见第3.1节，回顾使用这种余弦相似度计算来评分向量之间的相似度。在那里，我们探讨了计算向量之间的余弦相似度和点积的示例。然而，从余弦相似度的公式（图13.3）中，你可以看到余弦值等于点积（`a`
    `.` `b`）除以向量长度的乘积（`|a| x |b|`）。这意味着如果我们能将向量`a`和`b`的特征归一化，使得每个向量的长度都是`1`（这个过程称为*单位归一化*），那么余弦相似度和点积是相等的：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When a vector is normalized such that its length equals `1`, it is known as
    a *unit-normalized* vector. But why would we care about normalizing vectors like
    this? Well, it turns out that calculating a dot product is much more efficient
    than calculating a cosine similarity, because there is no need to divide the dot
    product by the magnitudes of each vector (which requires using the Pythagorean
    theorem to calculate the square root of the sum of squares of each vector’s features).
    Since the cosine calculation is often the most expensive part of the search when
    scoring a large number of documents, unit-normalizing vectors at index time and
    performing a dot product of the indexed vectors with a unit-normalized query vector
    at search time can substantially speed up searches while providing the same result:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个向量的长度等于`1`时，它被称为*单位归一化向量*。但为什么我们会关心这样的向量归一化呢？好吧，实际上计算点积比计算余弦相似度要高效得多，因为不需要除以每个向量的模长（这需要使用勾股定理来计算每个向量特征的平方和的平方根）。由于余弦计算通常是评分大量文档时搜索中最昂贵的部分，因此在索引时间对向量进行单位归一化，并在搜索时间使用单位归一化查询向量与索引向量进行点积，可以显著加快搜索速度，同时提供相同的结果：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 The normalization of vectors a and b to a unit vector. All indexed vectors
    have this normalization done once, before being indexed.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将向量a和b归一化为单位向量。所有索引向量在索引之前都会进行这种归一化处理。'
- en: '#2 Full cosine similarity calculation. Notice the denominator performs a square
    root of the sum of the squares for each vector.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 完整的余弦相似度计算。注意分母对每个向量的平方和进行开方运算。'
- en: '#3 cos(vector_a, vector_b) = dot_product(unit_vector_a, unit_vector_b) = 0.9762'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 cos(vector_a, vector_b) = dot_product(unit_vector_a, unit_vector_b) = 0.9762'
- en: '#4 The dot product calculation on unit-normalized vectors. Notice the absence
    of the denominator and the much simpler sum of multiplied feature weights.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 单位归一化向量的点积计算。注意没有分母，并且乘积特征权重的和要简单得多。'
- en: While we are conceptually still performing a cosine similarity (due to the unit-normalized
    vectors), using the dot product allows us to perform substantially faster calculations
    at query time. Because this optimization is possible, it is not a good idea performance-wise
    to perform a full cosine similarity calculation in production. While there are
    good use-case-specific reasons you would want to perform a cosine versus dot product,
    such as to ignore or consider the magnitude of vectors (see section 3.1.4 for
    a refresher), you will virtually always at least *implement* cosine similarity
    using unit-normalized vectors and a dot product calculation for performance reasons.
    To reinforce best practices, we’ll consistently utilize this pattern in all the
    remaining code listings when cosine similarity is being implemented.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从概念上我们仍在执行余弦相似度（由于单位归一化向量），但使用点积允许我们在查询时进行实质上更快的计算。由于这种优化是可能的，因此在生产中执行完整的余弦相似度计算在性能上并不是一个好主意。虽然有一些针对特定用例的好理由，你可能会选择执行余弦而不是点积，例如忽略或考虑向量的幅度（参见第3.1.4节以刷新记忆），但你几乎总是至少*实现*使用单位归一化向量和点积计算来执行余弦相似度，出于性能原因。为了强化最佳实践，当实现余弦相似度时，我们将一致地在所有剩余的代码列表中利用这种模式。
- en: Optimizing for vector search compute performance and cost
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优化向量搜索的计算性能和成本
- en: Performing vector similarity calculations can be slow and computationally expensive
    at scale, so it’s important to understand how to make the right trade-offs to
    optimize for performance and cost.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 执行向量相似度计算可能在大规模下变得缓慢且计算成本高昂，因此了解如何进行正确的权衡以优化性能和成本是很重要的。
- en: 'Because dot products are significantly faster to calculate than cosine similarities,
    we recommend you always implement cosine similarity by indexing unit-normalized
    vectors and then doing dot product calculations between document vectors and a
    unit-normalized query vector at search time. In addition, it is often possible
    to save significant memory and substantially improve search times using other
    optimization techniques:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于点积的计算速度比余弦相似度快得多，我们建议你始终通过索引单位归一化向量，然后在搜索时在文档向量和单位归一化查询向量之间执行点积计算来实现余弦相似度。此外，使用其他优化技术通常可以显著节省内存并大幅提高搜索时间：
- en: Using approximate nearest neighbors (ANN) approaches to quickly filter to the
    top-*N* results to rank instead of all documents (covered in section 13.5.3)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用近似最近邻（ANN）方法快速筛选出前*N*个结果进行排名，而不是所有文档（在第13.5.3节中介绍）
- en: Quantizing (compressing) vectors to reduce the number of bits used to represent
    each feature in a vector (covered in section 13.7)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将向量量化（压缩）以减少表示向量中每个特征所需的位数（在第13.7节中介绍）
- en: Using Matryoshka Representation Learning (MRL) to only index and/or search on
    key portions of your embeddings while still maintaining most of your recall (covered
    in section 13.7)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用玛特罗什卡表示学习（MRL）仅索引和/或搜索嵌入的关键部分，同时仍然保持大部分召回率（在第13.7节中介绍）
- en: Over-requesting a limited number of optimized search results using a cheaper
    search algorithm or similarity metric, and then reranking the top-*N* results
    using a more expensive similarity metric (covered in sections 13.5.3 and 13.7)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更便宜的搜索算法或相似度度量标准过度请求有限数量的优化搜索结果，然后使用更昂贵的相似度度量标准重新排名前*N*个结果（在第13.5.3节和13.7节中介绍）
- en: With the ability to perform dense vector search and nearest-neighbor similarity
    in place, the next critical step is to figure out a way to generate these mysterious
    embeddings.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在能够执行密集向量搜索和最近邻相似度的情况下，下一个关键步骤是找出生成这些神秘嵌入的方法。
- en: 13.3 Getting text embeddings by using a Transformer encoder
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 通过使用Transformer编码器获取文本嵌入
- en: In this section, we cover Transformers and how they work to represent meaning.
    We also discuss how they are used to encode that meaning into embeddings.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍Transformer及其如何表示意义。我们还讨论了它们如何被用来将这种意义编码到嵌入中。
- en: 13.3.1 What is a Transformer?
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.1 什么是Transformer？
- en: Transformers are a class of deep neural network architectures that are optimized
    to encode meaning as embeddings and also decode the meaning back from embeddings.
    Text-based Transformers do this by first representing term labels as dense vectors
    by using their surrounding context in a sentence (the encoding part) and then
    leveraging an output model to translate the vectors into different text representations
    (the decoding part).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers是一类深度神经网络架构，它们被优化为将意义编码为嵌入，并从嵌入中解码意义。基于文本的Transformers通过首先使用句子中的周围上下文（编码部分）将术语标签表示为密集向量，然后利用输出模型将向量转换为不同的文本表示（解码部分）来实现这一点。
- en: One beautiful feature of this approach is the separation of concerns between
    encoding and decoding. We will take advantage of this feature and use the encoding
    mechanism just to obtain the embeddings, which we can then use as a semantic representation
    of meaning independent of any decoding steps.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个美妙特性是编码和解码之间的关注点分离。我们将利用这个特性，仅使用编码机制来获取嵌入，然后我们可以将其用作与任何解码步骤无关的意义的语义表示。
- en: Representing meaning
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表示意义
- en: Recall the example English and Armenian greetings from the introduction to section
    13.1\. Using a specialized Transformer and dataset for English to Armenian language
    translation, it would be possible to train a model to encode the two phrases “Hello
    to you!” and “Barev Dzes” into nearly identical dense vectors. These vectors could
    then be decoded back into text to translate or reconstitute something closer to
    the original text.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下第13.1节介绍中的英语和亚美尼亚问候语示例。使用专门为英语到亚美尼亚语言翻译设计的Transformer和数据集，可以训练一个模型将两个短语“Hello
    to you!”和“Barev Dzes”编码成几乎相同的密集向量。然后，这些向量可以被解码回文本，以翻译或重构更接近原始文本的内容。
- en: Let’s start our journey with Transformers by understanding how Transformer encoder
    models are trained and what they ultimately learn. To understand the motivations
    and mechanisms behind Transformers, it is important to know some history of the
    underlying concepts.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从理解Transformer编码模型的训练方式和它们最终学习到什么开始我们的Transformer之旅。为了理解Transformer背后的动机和机制，了解一些基础概念的历史是很重要的。
- en: The year is 1953\. You find yourself in a classroom with 20 other students,
    each of you sitting at your own desk. On your desk is a pencil, and a sheet of
    paper with the sentence `Q:` `I` `went` `to` `the ________ and` `bought` `some`
    `vegetables.` You already know what to do, and you write down “store” in the blank.
    You peek over at a classmate at the desk next to you, and they have written “market”.
    A chime rings, and the answers are tallied. The most common answer is “store”,
    and there are several with “market” and several with “grocer”. This is the Cloze
    test. It is meant to test reading comprehension.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 那一年是1953年。你发现自己在一个教室里，和其他20个学生一起，每个人坐在自己的桌子前。你的桌子上有一支铅笔和一张纸，上面写着句子 `Q:` `I`
    `went` `to` `the ________ and` `bought` `some` `vegetables.` 你已经知道该怎么做，你在空白处写下“store”。你瞥了一眼坐在你旁边桌子上的同学，他们写的是“market”。一阵铃声响起，答案被统计。最常见的答案是“store”，还有几个答案是“market”和“grocer”。这是完形填空测试。它的目的是测试阅读理解。
- en: 'You are transported now to the year 1995\. You are sitting in another classroom
    with students taking another test. This time, your sheet of paper has a very long
    paragraph. It looks to be about 60 words long and is somewhat complicated:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你被带到了1995年。你坐在另一个教室里，学生们正在参加另一场考试。这次，你的试卷上有一段非常长的段落。看起来大约有60个单词长，有些复杂：
- en: Ours was the marsh country, down by the river, within, as the river wound, twenty
    miles of the sea. My first most vivid and broad impression of the identity of
    things seems to me to have been gained on a memorable raw afternoon towards evening.
    At such a time I found out for certain that this bleak place overgrown with nettles
    was the churchyard.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们所在的是沼泽地区，靠近河流，随着河流的蜿蜒，离海有二十英里。我在一个难忘的阴雨下午接近傍晚时，似乎获得了对事物本质的第一印象。在这样的时刻，我确信这个长满荨麻的荒凉地方是教堂墓地。
- en: 'After the paragraph, there is a question listed with a prompt for an answer:
    `Q:` `How` `far away` `from` `the` `sea` `is` `the churchyard? A:________`. You
    write in the blank, “twenty miles”. You have just completed one of a dozen questions
    in the Regents English reading comprehension test. Specifically, this tested your
    attention.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在段落之后，列出了一个带有答案提示的问题：`Q:` `How` `far away` `from` `the` `sea` `is` `the churchyard?
    A:________`。你在空白处写下，“二十英里”。你刚刚完成了一打问题中的一个问题，这是纽约州教育委员会英语阅读理解测试的一部分。具体来说，这是测试你的注意力。
- en: These two tests are foundational to how we measure written language comprehension.
    To be able to pass these tests, you must read, read, read, and read some more.
    In fact, by the time most people take these tests in school, they have already
    practiced reading for about 14 years and have amassed a huge amount of contextual
    knowledge.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这两项测试是我们衡量书面语言理解能力的基础。要能够通过这些测试，你必须阅读，阅读，再阅读，并且还要多读一些。实际上，当大多数人在学校参加这些测试时，他们已经练习阅读大约14年了，并且积累了大量的背景知识。
- en: These theories form the basis for LLMs—NLP models trained on lots and lots of
    text (for example, the entirety of the Common Crawl dataset of the web).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些理论构成了LLMs（在大量文本上训练的自然语言处理模型）的基础——例如，整个网络Common Crawl数据集。
- en: 'Major breakthroughs in the NLP field culminated in a 2018 paper by researchers
    at Google (Jacob Devlin et al.) titled *BERT: Pre-training of Deep Bidirectional
    Transformers for Language Understanding*, which made use of the Cloze test and
    attention mechanisms within Transformers to reach state-of-the-art performance
    on many language comprehension benchmarks ([https://arxiv.org/pdf/1810.04805](https://arxiv.org/pdf/1810.04805)).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理领域的重大突破在2018年由谷歌的研究人员（雅各布·德夫林等人）发表的一篇论文中达到顶峰，这篇论文的标题是 *BERT：用于语言理解的深度双向变换器预训练*，它利用了Cloze测试和Transformer中的注意力机制，在许多语言理解基准测试中达到了最先进的性能（[https://arxiv.org/pdf/1810.04805](https://arxiv.org/pdf/1810.04805)）。
- en: BERT, specifically, performs self-learning by presenting Cloze tests to itself.
    The training style is “self-supervised”, which means it is supervised learning,
    framed as an unsupervised task. This is ideal, because it does not require data
    to be manually labeled beforehand for the initial model pretraining. You can give
    it any text, and it will make the tests itself. In the training context, the Cloze
    test is known as *masked language modeling*. The model starts with a more basic
    embedding (for example, using the well-known word2vec or GloVe libraries for each
    word in the vocabulary), and it will randomly remove 15% of the tokens in a sentence
    for the test. The model then optimizes a loss function that will result in a higher
    Cloze test success rate. Also, during the training process, it uses the surrounding
    tokens and contexts (the attention). Given a vector in a single training example,
    the resulting trained output vector is an embedding that contains a deeply learned
    representation of the word and the surrounding contexts.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: BERT特别通过向自己展示Cloze测试来进行自我学习。训练方式是“自监督”，这意味着它是监督学习，但被构造成一个无监督任务。这是理想的，因为它不需要在初始模型预训练之前手动标记数据。你可以给它任何文本，它将自行进行测试。在训练环境中，Cloze测试被称为
    *掩码语言模型*。模型从一个更基本的嵌入（例如，使用众所周知的word2vec或GloVe库对词汇表中的每个单词进行嵌入）开始，并将随机删除句子中的15%的标记进行测试。然后模型优化一个损失函数，这将导致Cloze测试的成功率更高。此外，在训练过程中，它使用周围的标记和上下文（注意力）。给定单个训练示例中的向量，训练后的输出向量是一个包含单词及其周围上下文深度学习表示的嵌入。
- en: We encourage you to learn more about Transformers and BERT, if you are interested,
    by reading the paper. All you need to understand for now, however, is how to get
    embeddings from a BERT encoder. The basic concept is shown in figure 13.4.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对Transformers和BERT感兴趣，我们鼓励你通过阅读这篇论文来了解更多信息。然而，现在你需要了解的只是如何从BERT编码器中获取嵌入。基本概念如图13.4所示。
- en: '![figure](../Images/CH13_F04_Grainger.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F04_Grainger.png)'
- en: Figure 13.4 The Transformer encoder
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.4 Transformer编码器
- en: 'In figure 13.4, we process text by first passing it through a tokenizer. The
    tokenizer splits text into *word pieces*, which are predefined parts of words
    that are represented in a vocabulary. This vocabulary is established for the model
    before it is trained. For example, the term “It’s” will be split into three word
    pieces during tokenization: `it`, `''`, and `s`. The vocabulary used in the BERT
    paper included 30,000 word pieces. BERT also uses special word pieces to denote
    the beginning and end of sentences: `[CLS]` and `[SEP]` respectively. Once tokenized,
    the token stream is passed into the BERT model for encoding. The encoding process
    then outputs a *tensor*, which is an array of vectors (one vector for each token).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在图13.4中，我们首先通过分词器处理文本。分词器将文本分割成*词元*，这些是词汇表中的预定义单词部分。这个词汇表在模型训练之前就已经建立。例如，术语“It’s”在分词过程中将被分割成三个词元：`it`，`'`和`s`。BERT论文中使用的词汇表包含30,000个词元。BERT还使用特殊的词元来表示句子的开始和结束：分别是`[CLS]`和`[SEP]`。一旦分词，标记流将被传递到BERT模型进行编码。编码过程随后输出一个*tensor*，这是一个向量数组（每个标记一个向量）。
- en: 13.3.2 Openly available pretrained Transformer models
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.2 公开可用的预训练Transformer模型
- en: While Transformers enable state-of-the-art language models to be built, having
    the knowledge and resources to build them from scratch can present a large hurdle
    for many. One very important aspect of working with Transformers is the large
    community and open toolsets that make it possible for any engineer to quickly
    get up and running with the technology. All it takes is some knowledge of Python
    and an internet connection.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Transformer使得构建最先进的语言模型成为可能，但拥有从头开始构建它们的知识和资源对于许多人来说可能是一个巨大的障碍。与Transformer一起工作的一个非常重要的方面是庞大的社区和开源工具集，这使得任何工程师都能快速开始使用这项技术。只需要一些Python知识和互联网连接即可。
- en: The models that are trained by this process from scratch are large and range
    from hundreds of MBs to hundreds of GBs in size, often needing similar amounts
    of GPU memory (VRAM) to run them quickly. The training itself also takes a large
    amount of expensive computing power and time, so being able to use preexisting
    models as a starting point provides a significant advantage. We’ll use this advantage
    in the next section as we begin applying one of these models to search.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个过程从头开始训练的模型很大，大小从几百MB到几百GB不等，通常需要相似数量的GPU内存（VRAM）来快速运行它们。训练本身也需要大量的昂贵计算能力和时间，因此能够使用现有的模型作为起点提供了显著的优势。我们将在下一节中利用这个优势，开始应用这些模型之一来进行搜索。
- en: 13.4 Applying Transformers to search
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4 将Transformer应用于搜索
- en: In this section, we’ll build a highly accurate natural language autocomplete
    for search, which will recommend more precise and otherwise related keywords based
    on a prefix of terms. We’ll do this by first passing our corpus text through a
    Transformer to get an index of embeddings. Then we’ll use this Transformer at
    query time to get the query embedding and search the embedding index for the *k*-nearest
    documents with the most similar embeddings. Figure 13.5 is an architecture diagram
    demonstrating the steps in this process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个高度准确的自然语言自动补全搜索，它将根据术语的前缀推荐更精确的相关关键词。我们将通过首先将语料库文本通过Transformer来获取嵌入索引来实现这一点。然后，我们将在查询时使用这个Transformer来获取查询嵌入，并在嵌入索引中搜索与最相似嵌入的*k*个最近文档。图13.5是一个展示此过程步骤的架构图。
- en: '![figure](../Images/CH13_F05_Grainger.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F05_Grainger.png)'
- en: Figure 13.5 A conceptual architecture for end-to-end search using Transformer-encoded
    vectors
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.5 使用Transformer编码向量进行端到端搜索的概念架构
- en: We have a content source, a nearest-neighbor index, a way to retrieve vectors
    from a Transformer, and a similarity formula. We can now build pipelines for all
    these pieces to process and index content, and then to retrieve and rank documents
    with a query.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个内容来源，一个最近邻索引，一种从Transformer中检索向量的方法，以及一个相似度公式。我们现在可以构建所有这些组件的管道来处理和索引内容，然后根据查询检索和排名文档。
- en: 13.4.1 Using the Stack Exchange outdoors dataset
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4.1 使用Stack Exchange户外数据集
- en: 'In chapter 5, we introduced several datasets from Stack Exchange. We’re choosing
    to use another one, the Stack Exchange outdoors dataset, here for a very important
    reason: the vocabulary and contexts in the outdoor question-and-answer domain
    already have good coverage in the Transformer models we’ll be using. Specifically,
    Wikipedia is used when training many Transformer models, and Wikipedia has a section
    specifically on outdoors content ([https://en.wikipedia.org/wiki/Outdoor](https://en.wikipedia.org/wiki/Outdoor)).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们介绍了Stack Exchange的几个数据集。我们选择在这里使用另一个数据集，Stack Exchange的户外数据集，有一个非常重要的原因：户外问题与答案领域的词汇和上下文在我们的Transformer模型中已经得到了很好的覆盖。具体来说，Wikipedia在训练许多Transformer模型时被使用，Wikipedia有一个专门关于户外内容的章节（[https://en.wikipedia.org/wiki/Outdoor](https://en.wikipedia.org/wiki/Outdoor)）。
- en: 'NOTE The outdoors examples used in chapters 13–15, along with references to
    Stack Exchange datasets in other chapters, are licensed by Stack Exchange under
    the CC-by-SA 4.0 License: [https://creativecommons.org/licenses/by-sa/4.0](https://creativecommons.org/licenses/by-sa/4.0).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：第13-15章中使用的户外示例，以及其他章节中提到的Stack Exchange数据集，均由Stack Exchange根据CC-by-SA 4.0许可证授权：[https://creativecommons.org/licenses/by-sa/4.0](https://creativecommons.org/licenses/by-sa/4.0)。
- en: The following listing walks through creating an outdoors collection and then
    indexing the outdoors question and answer data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了创建户外集合并随后索引户外问题和答案数据的过程。
- en: Listing 13.1 Indexing the outdoors dataset
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.1 索引户外数据集
- en: '[PRE2]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is the schema for the `outdoors` collection created in listing 13.1:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第13.1节中创建的`outdoors`集合的架构：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The indexed dataset contains documents representing both questions and answers,
    with answers linked to their original questions through the `parent_id` field.
    Each document contains a `post_type` field to differentiate whether it contains
    a “question” or an “answer”.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 索引的数据集包含代表问题和答案的文档，答案通过`parent_id`字段链接到其原始问题。每个文档都包含一个`post_type`字段，用于区分它是否包含“问题”或“答案”。
- en: The following listing shows a question post and its related answers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了一个问题帖子和其相关的答案。
- en: Listing 13.2 Exploring post data for a question on `climbing knots`
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.2 探索关于`攀岩结`的问题的帖子数据
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Document 18826 is marked as the accepted answer to the question.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 文档18826被标记为问题的接受答案。'
- en: '#2 This is our question document.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 这是我们的问题文档。'
- en: '#3 These documents are answers to the question in the first document (id=18825).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 这些文档是第一个文档（id=18825）中问题的答案。'
- en: '#4 These are answer documents that relate to the question.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 这些是与问题相关的答案文档。'
- en: '#5 This is the document marked as the accepted answer to the question (accepted_answer_id=18826).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 这是被标记为问题接受答案的文档（接受答案_id=18826）。'
- en: In the preceding listing, the first document is a question most related to the
    query `climbing knots`. The question has two answers that are linked back to the
    parent question through the `parent_id` field on each answer. One of these answers
    has been chosen as the accepted answer, which is identified by setting the `accepted_answer_id`
    field (to 18826 in this case) on the question document.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的列表中，第一个文档是与查询`攀岩结`最相关的问题。该问题有两个答案，通过每个答案上的`parent_id`字段链接回父问题。其中之一已被选为接受答案，通过在问题文档上设置`accepted_answer_id`字段（在本例中为18826）来识别。
- en: The `body` field of question documents contains elaborations on the question,
    while the `body` field of an answer contains the full answer. Only question posts
    have a `title`, which is a summary of the question. Several other fields (such
    as `view_count`, `answer_count`, and `owner_user_id`) are omitted here but are
    available in the full dataset as metadata fields, which can help with search relevance
    using BM25 mixed with other signals.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 问题文档的`body`字段包含对问题的详细说明，而答案的`body`字段包含完整的答案。只有问题帖子有一个`title`，它是问题的摘要。其他几个字段（如`view_count`、`answer_count`和`owner_user_id`）在此省略，但在完整数据集中作为元数据字段提供，这有助于使用BM25与其他信号混合进行搜索相关性。
- en: Now that you’re familiarized with the data model, let’s take a moment to try
    out some queries and see what types of questions come back. The following listing
    searches for questions matching a common query.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了数据模型，让我们花点时间尝试一些查询，看看会返回什么类型的问题。以下列表搜索与常见查询匹配的问题。
- en: Listing 13.3 Running a basic lexical search for `climbing` `knots`
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.3 对`攀岩` `结`运行基本的词汇搜索
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Match the query against the title and body fields.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将查询与标题和正文字段匹配。'
- en: 'Response:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can see that these are somewhat relevant titles for this lexical query. But
    this is just a basic keyword search. Other queries do not perform nearly as well;
    for example, the query `What is DEET` in the next listing shows very irrelevant
    results.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这些对于这个词汇查询来说是有些相关的标题。但这仅仅是一个基本的关键词搜索。其他查询的表现远不如这个；例如，在下一条列表中的查询`What
    is DEET`显示了非常不相关的结果。
- en: Listing 13.4 Basic lexical matching can yield irrelevant results
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.4 基本词汇匹配可能导致不相关结果
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Response:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This shows how traditional lexical search can fail for common natural language
    use cases. Specifically, the inverted index suffers from the query-term dependency
    problem. This means that the terms in the query are being matched as strings to
    terms in the index. This is why you see strong matches for `what is` in the results
    in listing 13.4\. The *meaning* of the query is not comprehended, so the retrieval
    can only be based on string matching.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了传统词汇搜索如何失败于常见的自然语言用例。具体来说，倒排索引受到查询-术语依赖问题的困扰。这意味着查询中的术语被匹配为字符串与索引中的术语。这就是为什么你在列表13.4的结果中看到对`what
    is`的强匹配。查询的意义没有被理解，因此检索只能基于字符串匹配。
- en: The rest of this chapter will provide the fundamentals needed to use Transformers
    for natural language search, and in chapter 14 we’ll solve the question-answering
    problem evident in listing 13.4\.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分将提供使用Transformers进行自然语言搜索所需的基本知识，而在第14章中，我们将解决列表13.4中明显的问题-回答问题。
- en: 13.4.2 Fine-tuning and the Semantic Text Similarity Benchmark
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4.2 微调和语义文本相似度基准
- en: Using a pretrained Transformer model out of the box usually won’t yield optimal
    results for task-specific prompts. This is because the initial training was done
    in a general language context without any specific use case or domain in mind.
    In essence, it is “untuned”, and using models this way is akin to indexing content
    in a search engine without tuning for relevance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的Transformer模型通常不会为特定任务的提示产生最佳结果。这是因为初始训练是在没有特定用例或领域的通用语言环境中进行的。本质上，它是“未调优”的，以这种方式使用模型类似于在搜索引擎中索引内容而不进行相关性调优。
- en: To realize the full potential of Transformers, they need to be refined to accomplish
    a specific task. This is known as *fine-tuning*—the process of taking a pretrained
    model and training it on more fit-for-purpose data to achieve a specific use case
    goal. For both autocomplete and semantic search, we are interested in fine-tuning
    to accomplish text similarity discovery tasks.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现Transformers的全部潜力，它们需要被微调以完成特定任务。这被称为**微调**——即通过在更适合目的的数据上训练预训练模型，以实现特定用例目标的过程。对于自动补全和语义搜索，我们感兴趣的是微调以完成文本相似度发现任务。
- en: That brings us to the Semantic Text Similarity Benchmark (STS-B) training and
    testing set ([https://ixa2.si.ehu.eus/stswiki/](https://ixa2.si.ehu.eus/stswiki/)).
    This benchmark includes passages that are semantically similar and dissimilar,
    and they’re labeled accordingly. Using this dataset, a model can be fine-tuned
    to improve the accuracy of nearest-neighbor search between a set of terms and
    many passages in a corpus, which will be our use case in this chapter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这将带我们来到语义文本相似度基准（STS-B）的训练和测试集([https://ixa2.si.ehu.eus/stswiki/](https://ixa2.si.ehu.eus/stswiki/))。这个基准包括语义相似和不相似的段落，并且它们被相应地标记。使用这个数据集，模型可以被微调以提高一组术语与语料库中许多段落之间的最近邻搜索的准确性，这将是本章中的用例。
- en: 'In chapter 14, we will fine-tune our own question-answering model so you can
    see how it is done. For our purposes in this chapter, however, we’ll use a project
    that already includes a pretuned model for this task: SBERT.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在第14章中，我们将微调我们自己的问题回答模型，以便你可以看到它是如何完成的。然而，在本章中，为了我们的目的，我们将使用一个已经包含为此任务预调优模型的工程：SBERT。
- en: 13.4.3 Introducing the SBERT Transformer library
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4.3 介绍SBERT Transformer库
- en: SBERT, or *Sentence-BERT*, is a technique and Python library based on Transformers
    that is built on the idea that a BERT model can be fine-tuned in such a way that
    two semantically similar sentences, and not just tokens, should be represented
    closer in vector space. Specifically, SBERT *pools* all the BERT embeddings in
    one sentence to a single vector. (Pooling is a fancy way of saying it combines
    the values.) Once SBERT pools the values, it trains for similarity between sentences
    by using a special-purpose neural network that learns to optimize for the STS-B
    task. For further implementation details, check out the “Sentence-BERT” paper
    by Nils Reimers and Iryna Gurevych ([https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: SBERT，或称*Sentence-BERT*，是一种基于Transformers的技术和Python库，其基于这样一个理念：BERT模型可以通过某种方式微调，使得两个语义相似的句子（而不仅仅是标记）在向量空间中应该表示得更近。具体来说，SBERT会将一个句子中的所有BERT嵌入汇总成一个单一的向量。（汇总可以理解为它结合了这些值。）一旦SBERT汇总了这些值，它将通过使用一个专门用于学习优化STS-B任务的神经网络来训练句子之间的相似性。有关进一步的实施细节，请参阅Nils
    Reimers和Iryna Gurevych撰写的“Sentence-BERT”论文（[https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084)）。
- en: The upcoming listings will give you an overview of how to use SBERT via the
    `sentence_transformers` Python library. We’ll start by importing the library using
    a pretrained model named `roberta-base-nli-stsb-mean-tokens`, which is based on
    an architecture called RoBERTa. It’s helpful to think of RoBERTa as an evolved
    and improved version of BERT, with optimized hyperparameters (configuration settings)
    and slight modifications to the original techniques.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的列表将为您概述如何通过`sentence_transformers` Python库使用SBERT。我们将从导入名为`roberta-base-nli-stsb-mean-tokens`的预训练模型开始，该模型基于RoBERTa架构。将RoBERTa视为BERT的进化版和改进版，具有优化的超参数（配置设置）和轻微的技术修改，这有助于理解。
- en: Hyperparameters
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超参数
- en: In machine learning, hyperparameters are any parameter values that can be changed
    before training and that will alter the learning process and affect the resulting
    model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，超参数是指在训练之前可以更改的任何参数值，它们将改变学习过程并影响最终模型。
- en: Unfortunately, you often don’t know what the hyperparameter values should be
    set to when you start, so you may have to learn optimized values over time using
    iteration and measurement.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当你开始时，你通常不知道应该将超参数值设置为什么，因此你可能需要通过迭代和测量来学习优化的值。
- en: In the model name, `roberta-base-nli-stsb-mean-tokens`, we can also see some
    terms that you may not recognize, including “nli” and “mean-tokens”. NLI stands
    for *natural language inference* (a subdomain of NLP used for language prediction),
    and mean-tokens refers to the whole sentence’s tokenization being pooled together
    as a mean of the numeric values of the token embeddings. Using mean-tokens returns
    a single 768-dimension embedding for the entire sentence.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型名称`roberta-base-nli-stsb-mean-tokens`中，我们还可以看到一些可能不认识的术语，包括“nli”和“mean-tokens”。NLI代表*自然语言推理*（NLP的一个子领域，用于语言预测），mean-tokens指的是整个句子的标记化被汇总为一个标记嵌入数值的平均值。使用mean-tokens会返回一个整个句子的单一768维嵌入。
- en: The following listing imports the `sentence_transformers` library, loads the
    model, and displays the full network architecture.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了如何导入`sentence_transformers`库，加载模型，并显示完整的网络架构。
- en: Listing 13.5 Loading the RoBERTa `SentenceTransformer` model
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.5：加载RoBERTa `SentenceTransformer`模型
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now the PyTorch `transformer` object holds the neural network architecture for
    the Transformer as well as all the model weights.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，PyTorch的`transformer`对象包含了Transformer的神经网络架构以及所有模型权重。
- en: With our model loaded, we can retrieve embeddings from text. This is where the
    fun really begins. We can take sentences and pass them through the neural network
    architecture using the pretrained model and get the embeddings as a result. We
    have four sentences that we’ll encode and assess in the following listings.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 加载我们的模型后，我们可以从文本中检索嵌入。这正是真正有趣的地方。我们可以将句子通过预训练模型传递到神经网络架构中，从而得到嵌入结果。在接下来的列表中，我们将对四个句子进行编码和评估。
- en: Listing 13.6 demonstrates how to encode multiple phrases into dense vector embeddings.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.6演示了如何将多个短语编码为密集向量嵌入。
- en: Listing 13.6 Encoding phrases as dense vector embeddings
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.6：将短语编码为密集向量嵌入
- en: '[PRE10]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 The four sentences we want to encode. We will pass all of these to be encoded
    as a single batch.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们想要编码的四个句子。我们将将这些句子全部传递给单个批次进行编码。'
- en: '#2 Just call transformer.encode, and the abstraction of sentence_transformers
    does all the heavy lifting for you.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 只需调用transformer.encode，sentence_transformers的抽象就会为你完成所有繁重的工作。'
- en: 'Response:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding listing, we take each sentence and pass it to the encoder.
    This results in a tensor for each sentence. A *tensor* is a generalizable data
    structure for holding potentially multidimensional values. A scalar (single value),
    vector (array of scalars), matrix (array of vectors), or even multidimensional
    matrix (array of matrices, matrix of matrices, etc.) are all examples of tensors
    of various dimensions. Tensors are produced by Transformer encoders, such as SBERT,
    when encoding text. For our use case, the tensor in listing 13.6 is an embedding
    containing 768 dimensions represented as floating point numbers.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的列表中，我们取每个句子并将其传递给编码器。这为每个句子生成一个张量。*张量* 是一个可以存储多维值的通用数据结构。标量（单个值）、向量（标量数组）、矩阵（向量数组）或甚至是多维矩阵（矩阵数组、矩阵的矩阵等）都是不同维度的张量示例。张量是由Transformer编码器（如SBERT）在编码文本时产生的。对于我们的用例，列表13.6中的张量是一个包含768个维度的嵌入，表示为浮点数。
- en: With our embeddings, we can now perform cosine similarities (dot product on
    unit-normalized vectors) to see which phrases are closest neighbors to each other.
    We’ll compare each phrase to every other phrase, and sort them by similarity to
    see which are most similar. This process is covered step by step in listings 13.7
    and 13.8.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的嵌入，我们现在可以执行余弦相似度（单位归一化向量的点积）来查看哪些短语彼此最接近。我们将比较每个短语与每个其他短语，并按相似度排序，以查看哪些最相似。这个过程在列表13.7和13.8中逐步介绍。
- en: We’ll use a PyTorch built-in library for calculating the dot product to do these
    comparisons, which allows us to pass in the embeddings with a single function
    call. We can then sort the resulting similarities and see which two phrases are
    most similar to one another and which two are most dissimilar. The following listing
    calculates the similarities between each of the phrase embeddings.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用PyTorch内置库来计算点积来完成这些比较，这允许我们通过单个函数调用传递嵌入。然后我们可以对结果相似度进行排序，并查看哪些两个短语彼此最相似，哪些两个短语最不相似。以下列表计算了短语嵌入之间的相似度。
- en: Listing 13.7 Comparing all the phrases to each other
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.7 比较所有短语
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Unit-normalizes embeddings for speed so dot products = cosine similarities'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 单位归一化嵌入以提高速度，因此点积等于余弦相似度'
- en: 'Output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We print the shape of the similarities object in listing 13.7 to see how many
    comparisons we have. The shape is 4 x 4 (`[4, 4]`) because we have 4 phrases,
    and each phrase has a similarity score to every other phrase and itself. All the
    similarity scores are between 0.0 (least similar) and 1.0 (most similar). The
    shape is included here to help show the complexity of comparing many phrases.
    If there were 100 phrases, the similarities shape would be 100 x 100\. If there
    were 10,000 phrases, the similarities shape would be 10,000 x 10,000\. So, as
    you add phrases to compare, the computational and space costs will increase with
    complexity *n**²*, where *n* is the number of phrases.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在列表13.7中打印相似度对象的形状，以查看我们进行了多少比较。形状是4 x 4 (`[4, 4]`)，因为我们有4个短语，每个短语与其他每个短语以及自身的相似度分数。所有相似度分数都在0.0（最不相似）和1.0（最相似）之间。这里包含形状是为了帮助展示比较多个短语复杂性。如果有100个短语，相似度形状将是100
    x 100。如果有10,000个短语，相似度形状将是10,000 x 10,000。所以，随着你添加要比较的短语，计算和空间成本将随着复杂性的 *n**²*
    增加，其中 *n* 是短语的数量。
- en: With the similarities for our four phrases computed, we sort and print them
    in the following listing.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了我们的四个短语的相似度后，我们按顺序打印它们在以下列表中。
- en: Listing 13.8 Sorting by similarities and printing the results
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.8 按相似度排序并打印结果
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Append all the phrase pairs to a dataframe.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将所有短语对附加到数据框中。'
- en: '#2 We don’t duplicate phrases or append a phrase’s similarity to itself, as
    it will always be 1.0.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们不会重复短语或附加短语相似度，因为它始终是1.0。'
- en: '#3 Get the score for each pair.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 获取每对分数。'
- en: '#4 Add the index column'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 添加索引列'
- en: '#5 Sort the scores (ascending=False for highest scores first).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 对分数进行排序（ascending=False以显示最高分数）。'
- en: 'Response:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can now see that the two phrases that are most similar to one another are
    “it’s raining hard” and “it is wet outside”. We also see a strong similarity between
    cars and motorcycles.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到，彼此最相似的短语是“it’s raining hard”和“it is wet outside”。我们还看到汽车和摩托车之间存在强烈的相似性。
- en: 'The two most dissimilar phrases are “it is wet outside” and “cars drive fast”.
    It’s very clear from these examples that this semantic encoding process is working—we
    can associate rain with it being wet outside. The dense vector representations
    captured the context, and even though the words are different, the meaning is
    still there. Note the scores: the top two similar comparisons have a score of
    greater than 0.59, and the next closest comparison has a score of less than 0.29,
    This is because only the top two comparisons seem to be similar to one another,
    as we would perceive them in a *natural language understanding* (NLU) task. As
    intelligent people, we can group rain and wet (“weather”), and we can also group
    cars and motorcycles (“vehicle”). Also interestingly, cars likely drive slower
    when it is wet on the ground, so that likely explains the low similarity of the
    last pair.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 两个最不相似的表达是“外面很湿”和“汽车开得快”。从这些例子中可以非常清楚地看出，这个语义编码过程正在起作用——我们可以将雨与外面很湿联系起来。密集的向量表示捕捉了上下文，尽管单词不同，但意义仍然存在。注意分数：前两个相似比较的分数大于0.59，下一个最接近的比较分数小于0.29。这是因为只有前两个比较看起来彼此相似，正如我们在自然语言理解（NLU）任务中感知的那样。作为聪明的人，我们可以将雨和湿（“天气”）分组，我们也可以将汽车和摩托车（“车辆”）分组。还有一点很有趣，当地面湿的时候，汽车可能开得更慢，这可能是最后一对相似度低的原因。
- en: 13.5 Natural language autocomplete
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.5 自然语言自动补全
- en: Now that we know our vector encoding and similarity process is working well,
    it’s time to put this embedding technique to work in a real search use case—natural
    language autocomplete!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道我们的向量编码和相似性处理工作良好，现在是时候将这种嵌入技术应用于实际的搜索用例——自然语言自动补全！
- en: In this section, we’ll show a practical use for sentence Transformers at search
    time, with a basic and fast semantic autocomplete implementation. We will apply
    what we’ve learned thus far to extract concepts from the outdoors dataset. Using
    spaCy (the Python NLP library we used in chapter 5), we will chunk nouns and verbs
    to get outdoors concepts. We will put these concepts in a dictionary and process
    them to get their embeddings. Then we’ll use the dictionary in an approximate
    nearest-neighbor (ANN) index to query in real time. This will give us the ability
    to enter a prefix or a term and get the most similar concepts that exist in the
    dictionary. Finally, we will take those concepts and present them to the user
    in order of similarity, demonstrating a smart, natural language autocomplete.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示在搜索时使用句子转换器的实际应用，以及一个基本的快速语义自动补全实现。我们将把迄今为止所学的内容应用到从户外数据集中提取概念。使用spaCy（我们在第5章中使用的Python
    NLP库），我们将分块名词和动词以获取户外概念。我们将这些概念放入字典中并处理它们以获取它们的嵌入。然后，我们将使用字典在近似最近邻（ANN）索引中进行实时查询。这将使我们能够输入一个前缀或一个术语，并获取字典中存在的最相似的概念。最后，我们将按相似度顺序将这些概念展示给用户，演示智能的自然语言自动补全。
- en: Experience and testing show that this works much better than even a well-tuned
    suggester in most lexical search engines. We will see that it’s much less noisy
    and also that similar terms that are not spelled the same will be automatically
    included in the suggestions. This is because we are not comparing keyword strings
    to one another; rather, we are comparing the embeddings, which represent meaning
    and context. This is the embodiment of searching for “things, not strings”, as
    introduced in section 1.2.4.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 经验和测试表明，即使在大多数经过良好调整的搜索引擎建议器中，这种方法也表现得更好。我们将看到它产生的噪声更少，而且相似但拼写不同的术语也会自动包含在建议中。这是因为我们不是在比较关键字字符串；而是在比较嵌入，它们代表意义和上下文。这是在1.2.4节中介绍的“寻找事物，而不是字符串”的体现。
- en: 13.5.1 Getting noun and verb phrases for our nearest-neighbor vocabulary
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.5.1 为我们的最近邻词汇获取名词和动词短语
- en: Using what we learned in chapter 5, we’ll write a simple function to extract
    *concepts* from the corpus. We won’t include any taxonomical hierarchy, and we
    won’t be building a full knowledge graph here. We just want a reliable list of
    frequently used nouns and verbs.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们在第5章中学到的知识，我们将编写一个简单的函数从语料库中提取*概念*。我们不会包括任何分类层次结构，也不会在这里构建一个完整的知识图谱。我们只想得到一个可靠的使用频率较高的名词和动词列表。
- en: The concepts in our example are the important “things” and “actions” that people
    usually search for. We also need to understand the dataset, which is best accomplished
    by spending time reviewing the concepts and how they relate to one another. Understanding
    the corpus is critical when building any search application, and there’s no exception
    when using advanced NLP techniques.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们示例中的概念是人们通常搜索的重要的“事物”和“动作”。我们还需要理解数据集，这最好通过花时间审查概念以及它们之间的关系来实现。在构建任何搜索应用时，理解语料库至关重要，使用高级NLP技术也不例外。
- en: The following listing shows a strategy that will provide a decent quality baseline
    of candidate concepts for our vocabulary while removing significant noise from
    the autocomplete results.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了一种策略，它将为我们的词汇表提供合理的候选概念基线，同时从自动完成结果中去除显著的噪声。
- en: Listing 13.9 Using the spaCy Matcher to get desired parts of text
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.9 使用spaCy Matcher获取所需文本部分
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 Loads the English spaCy NLP model'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载英语spaCy NLP模型'
- en: '#2 All the normalized noun/verb phrases (“concepts”) in the corpus'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 语料库中所有标准化名词/动词短语（“概念”）'
- en: '#3 The original text labels that were normalized to the concept.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将原始文本标签标准化到概念中'
- en: '#4 Uses the spaCy Matcher to chunk patterns into concept labels'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用spaCy Matcher将模式分块为概念标签'
- en: '#5 Part-of-speech tags that match nouns'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 匹配名词的词性标签'
- en: '#6 Part-of-speech tags that match verbs'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 匹配动词的词性标签'
- en: '#7 Adds a noun phrase-matching pattern to the spaCy analysis pipeline'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 将名词短语匹配模式添加到spaCy分析管道中'
- en: '#8 Adds the verb phrase matching pattern. You can add more NOT_IN patterns
    to exclude other “stop word” verbs.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 添加动词短语匹配模式。您可以添加更多NOT_IN模式来排除其他“停用词”动词。'
- en: '#9 Processes the body field for each outdoors question, in batches of 40 documents
    using 4 threads'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 以40个文档的批次，使用4个线程处理每个户外问题的正文字段'
- en: '#10 Gets all the noun and verb phrase matches, and keeps them in the sources
    and phrases lists'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 获取所有名词和动词短语匹配，并将它们保留在源和短语列表中'
- en: '#11 Aggregates the normalized concepts by term frequency'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 通过词频聚合标准化概念'
- en: In the preceding listing, we use the spaCy Matcher to detect patterns as part-of-speech
    tags. We also explicitly remove forms of the verb “to be” from the verb concepts.
    The verb “to be” is used frequently in many non-useful situations and often clutters
    the concept suggestions. We could further improve quality by removing other noisy
    verbs like “have” and “can”, but this is just an example for now. SpaCy’s language
    pipeline (`nlp.pipe`) is also introduced in this listing. The `pipe` function
    accepts a batch size and the number of threads to use as parameters, and it then
    performs streaming processing of the text in parallel batches (and thus more quickly
    than making individual calls for each document).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的列表中，我们使用spaCy Matcher检测作为词性标签的模式。我们还明确地从动词概念中移除了“to be”动词的形式。在许多非有用情况下，“to
    be”动词被频繁使用，并经常使概念建议变得杂乱。我们可以通过移除其他噪声动词（如“have”和“can”）来进一步提高质量，但这里只是一个示例。此列表中还介绍了spaCy的语言管道（`nlp.pipe`）。`pipe`函数接受一个批次大小和要使用的线程数作为参数，然后以并行批次的流式处理文本（因此比为每个文档单独调用更快）。
- en: With the function in listing 13.9, we can now get the list of concepts. When
    running this on your machine it may take some time, so be patient. The following
    listing returns the most prominent concepts and labels from the outdoors collection.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列表13.9中的函数，我们现在可以获取概念列表。当在您的机器上运行此操作时可能需要一些时间，所以请耐心等待。以下列表返回了户外集合中最显著的概念和标签。
- en: Listing 13.10 Generating the most frequent concepts in the corpus
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.10 生成语料库中最频繁的概念
- en: '[PRE17]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Response:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE18]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Aside from getting the concepts for the outdoors dataset, listing 13.10 filtered
    the total dataset to `topcons`, which only includes concepts with a frequency
    greater than `5`. Filtering will limit noise from terms that don’t appear as often
    in the corpus, such as misspellings and rare terms we don’t want to suggest in
    an autocomplete scenario.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 除了获取户外数据集的概念外，列表13.10还对整个数据集进行了过滤，只包括频率大于`5`的概念。过滤将限制来自语料库中不常出现的术语的噪声，例如拼写错误和我们不希望在自动完成场景中建议的罕见术语。
- en: 13.5.2 Getting embeddings
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.5.2 获取嵌入
- en: We’re going to perform a complex normalization that will normalize similarly
    related concepts. But instead of algorithmic normalization (like stemming), we
    are normalizing to a dense vector space of 768 feature dimensions. Similar to
    stemming, the purpose of this is to increase *recall* (the percentage of relevant
    documents successfully returned). But instead of using a stemmer, we’re finding
    and mapping together closely related concepts. As a reminder, we’re only normalizing
    noun and verb phrases. Ignoring the other words is similar to stop-word removal,
    but that’s OK, because we want to suggest similar concepts as concisely as possible.
    We’ll also have a much better representation of the context and meaning of the
    remaining phrases. Therefore, the surrounding non-noun and non-verb terms are
    implied.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行一个复杂的归一化操作，该操作将归一化类似相关的概念。但与算法归一化（如词干提取）不同，我们正在将数据归一化到一个包含768个特征维度的密集向量空间。与词干提取类似，这样做的目的是为了提高*召回率*（成功返回的相关文档的百分比）。但与使用词干提取器不同，我们正在寻找并将紧密相关的概念映射在一起。提醒一下，我们只对名词和动词短语进行归一化。忽略其他单词类似于停用词去除，但这没关系，因为我们希望尽可能简洁地提出类似的概念。我们还将更好地表示剩余短语的上下文和意义。因此，周围的非名词和非动词术语是隐含的。
- en: Now that we have a list of concepts (from the last section), we’re going to
    process them using our loaded `model` (the RoBERTa Sentence Transformer model
    we loaded in listing 13.5) to retrieve the embeddings. This may take a while if
    you don’t have a GPU, so after we calculate the embeddings the first time, we’ll
    persist them to a “pickle file” (a serialized Python object that can be easily
    stored and loaded to and from disk). If you ever want to rerun the notebook, you
    can just load the previously created pickle file and not waste another half hour
    reprocessing the raw text.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个概念列表（来自上一节），我们将使用我们加载的`model`（在列表13.5中加载的RoBERTa句子转换器模型）来处理它们以检索嵌入。如果你没有GPU，这可能需要一段时间，所以我们在第一次计算嵌入后，将它们持久化到“pickle文件”（一个可以轻松存储和从磁盘加载的序列化Python对象）。如果你想要重新运行笔记本，你只需加载之前创建的pickle文件，而无需再花半小时重新处理原始文本。
- en: Hyperparameter alert! The `minimum_frequency` term is a hyperparameter, and
    it’s set to be greater than five (`>=6`) in the following listing to minimize
    noise from rare terms. We’ll encounter more hyperparameters in other listings
    in this chapter and the next, especially when we get into fine-tuning. After you’ve
    been through the rest of the listings in this chapter, we encourage you to come
    back and change the value of `minimum_frequency` and see how it alters the results
    that are retrieved. You may find a value that’s more suitable and more accurate
    than what we’ve arrived at here.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数警告！`minimum_frequency`术语是一个超参数，在下面的列表中设置为大于五（`>=6`），以最小化罕见术语的噪声。在本章和下一章的其他列表中，我们将遇到更多的超参数，尤其是在我们进行微调时。在你浏览了本章中其余的列表之后，我们鼓励你回来更改`minimum_frequency`的值，看看它如何改变检索到的结果。你可能找到一个更适合且更准确的价值，比我们在这里得到的价值更好。
- en: Listing 13.11 Retrieving the embeddings of our concept vocabulary
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.11 检索我们的概念词汇的嵌入
- en: '[PRE19]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Caching code removed for brevity'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为了简洁起见，移除了缓存代码'
- en: '#2 This is a hyperparameter! We are ignoring terms that occur less than this
    number of times in the entire corpus. Lowering this threshold may lower precision,
    and raising it may lower recall.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 这是一个超参数！我们正在忽略在整个语料库中出现的次数少于这个数字的术语。降低这个阈值可能会降低精确度，而提高它可能会降低召回率。'
- en: 'Response:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: From listing 13.11, you can see that one embedding was generated from each of
    our 12,375 concepts. All embeddings have the same dimensionality from the same
    dense vector space and can therefore be directly compared with one another.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 从列表13.11中，你可以看到每个嵌入都是由我们的12,375个概念生成的。所有嵌入都具有来自相同密集向量空间相同的维度性，因此可以直接相互比较。
- en: Figure 13.6 demonstrates what these embeddings look like and how they relate
    to one another when plotted in 3D.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6展示了这些嵌入在3D图中的样子以及它们之间的关系。
- en: The similarities of some concepts in the figure have been labeled to show neighborhoods
    of meaning. Concepts related to “wind” and “block” illustrate where they are located
    relative to each other in the vector space. We used dimensionality reduction to
    reduce the 768 dimensions for each embedding into 3 dimensions (*x*, *y*, *z*)
    so they could be easily plotted. *Dimensionality reduction* is a technique for
    condensing one vector with many features into another vector with fewer features.
    During this reduction, the relationships in the vector space are maintained as
    much as possible.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图中一些概念的相似度已被标注出来，以显示意义邻域。与“风”和“块”相关的概念说明了它们在向量空间中相对于彼此的位置。我们使用维度降低将每个嵌入的768维降低到3维（*x*，*y*，*z*），以便它们可以轻松绘制。*维度降低*是一种将具有许多特征的向量压缩为具有较少特征的向量的技术。在这次降低过程中，尽可能保持向量空间中的关系。
- en: Dimensionality reduction loses context
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 维度降低会丢失上下文
- en: A lot of context is lost when performing dimensionality reduction, so the visualization
    in figure 13.6 is only presented to give you an intuition of the vector space
    and concept similarity, not to suggest that reducing to three dimensions is an
    ideal way to represent the concepts.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行维度降低时，会丢失很多上下文，因此图13.6中的可视化只是为了给你一个向量空间和概念相似性的直观感受，并不建议将概念降低到三维是一个理想的表示方法。
- en: '![figure](../Images/CH13_F06_Grainger.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH13_F06_Grainger.png)'
- en: Figure 13.6 The vector space for the concept embeddings mapped to a 3D visualization
  id: totrans-216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.6 将概念嵌入映射到3D可视化的向量空间
- en: With the embeddings calculated from listing 13.11, we can now perform a massive
    comparison to see which terms are more closely related to one another. We will
    do this by calculating the cosine similarity—the dot product for each unit-normalized
    embedding related to every other unit-normalized embedding. Note that we’re limiting
    the number of embeddings that we’re comparing in this example, because the number
    of calculations that are required to be performed grows exponentially as the number
    of embeddings increases. If you’re not sure what we mean, let’s do some quick
    math. Each embedding has a dimensionality of 768 floating point values. Comparing
    the top `250` embeddings all against each other results in `250` × `250` × `768
    = 48,000,000` floating point calculations. Were we to compare the full list of
    12,375 embeddings, that would be `12,375` × `12,375` × `768` `=` `117,612,000,000`
    floating point calculations. Not only would this be slow to process, but it would
    also take a very large amount of memory.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用从列表13.11计算出的嵌入，我们现在可以执行大规模比较，以查看哪些术语彼此之间更为紧密相关。我们将通过计算余弦相似度——每个单位归一化嵌入与每个其他单位归一化嵌入的点积来完成此操作。请注意，在这个例子中，我们限制了要比较的嵌入数量，因为随着嵌入数量的增加，所需的计算数量呈指数增长。如果您不确定我们的意思，让我们做一些快速的计算。每个嵌入具有768个浮点值。比较前`250`个嵌入的结果是`250`
    × `250` × `768` = `48,000,000`个浮点计算。如果我们比较完整的12,375个嵌入列表，那将是`12,375` × `12,375`
    × `768` = `117,612,000,000`个浮点计算。这不仅处理速度慢，而且还需要非常大的内存。
- en: The following listing performs a brute-force comparison of the top 250 concepts,
    to assess how similarity scores are distributed.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表执行了前250个概念的暴力比较，以评估相似度得分是如何分布的。
- en: Listing 13.12 Exploring similarity scores from the head of the vocabulary
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.12 从词汇表头部探索相似度得分
- en: '[PRE21]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#1 Finds the pairs with the highest dot product scores'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 找到具有最高点积得分的成对概念'
- en: '#2 Ranks similarities as defined in listing 13.8'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 按照第13.8节列表中定义的相似度进行排名'
- en: 'Response:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE22]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see in listing 13.12, the `scores` dataframe now holds a sorted list
    of all phrases compared to one another, with the most similar being “protect”
    and “protection” with a dot product similarity of `0.928`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表13.12所示，`scores`数据框现在包含所有相互比较的短语的排序列表，其中最相似的是“保护”和“保护”，点积相似度为`0.928`。
- en: 'Note that the index of `250` is arbitrary and can be changed to larger values
    for more data to visualize. Remember what we learned in listing 13.7: using `n`
    concepts yields a tensor of `[n,` `n]` in shape. This yields a total of `250`
    × `250` = `62500` similarities for the example in listing 13.12.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`250`的索引是任意的，可以根据需要将更大的值用于可视化更多数据。记住我们在列表13.7中学到的：使用`n`个概念会产生形状为`[n, n]`的张量。这会产生列表13.12中示例的`250`
    × `250` = `62500`个相似度。
- en: The following listing plots the distribution of the top 250 concept comparison
    similarity scores.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表绘制了前250个概念比较相似度得分的分布。
- en: Listing 13.13 The distribution of word similarities
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.13 词相似度分布
- en: '[PRE23]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The output of listing 13.13 is shown in figure 13.7, and the distribution clarifies
    the percentage of concepts that are most related to one another. You see that
    very few comparisons have a similarity score greater than `0.6`, and the vast
    majority have similarity scores less than that.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.13的输出显示在图13.7中，分布清楚地说明了哪些概念之间最相关。您可以看到，非常少的比较具有大于`0.6`的相似度得分，而绝大多数得分低于那个值。
- en: '![figure](../Images/CH13_F07_Grainger.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F07_Grainger.png)'
- en: Figure 13.7 Distribution of how the top 250 concepts score with a dot product
    similarity when compared with each other. Note that very few comparisons result
    in a score higher than `0.6`, and most scores are less than `0.4` (a very low
    confidence).
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.7展示了前250个概念之间通过点积相似度比较的得分分布。请注意，非常少的比较得分高于`0.6`，而大多数得分低于`0.4`（非常低的置信度）。
- en: We plotted the distribution of scores so we can assess them and use our intuition
    for choosing a baseline similarity threshold at query time (used later in listing
    13.15). The visualization in figure 13.7 is very promising. Since most concepts
    are noted as dissimilar, we can reliably choose a high enough number as a threshold
    of quality suggestions (such as `0.6` in this example). When we’re performing
    an autocomplete during search, we’re only interested in seeing the top five to
    ten suggested terms, so this distribution shows we can do that reliably.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制了得分的分布，以便我们评估它们，并在查询时使用我们的直觉来选择一个基线相似度阈值（在列表13.15中稍后使用）。图13.7中的可视化非常有前景。由于大多数概念都被标记为不相似，我们可以可靠地选择一个足够高的数值作为质量建议的阈值（例如，本例中的`0.6`）。当我们进行搜索时的自动完成功能时，我们只对看到前五到十个建议的术语感兴趣，因此这个分布表明我们可以可靠地做到这一点。
- en: 13.5.3 ANN search
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.5.3 ANN搜索
- en: Before implementing the working autocomplete, we have one more important problem
    to solve. The problem is that, at query time, we ideally don’t want to compare
    each search term to all 12,375 other terms. That would be inefficient and slow
    due to the dimensionality and computational overhead of using the `sentence_transformers
    .util.dot_score` function. Even if we were willing to calculate dot product similarities
    for all our documents, this would just get slower and slower as we scaled to millions
    of documents, so we ideally would only score documents that have a high chance
    of being similar.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现工作自动完成之前，我们还有一个更重要的问题要解决。问题是，在查询时，我们理想情况下不希望将每个搜索词与12,375个其他词进行比较。由于使用`sentence_transformers
    .util.dot_score`函数的维度和计算开销，这将是不高效的且速度慢。即使我们愿意计算所有文档的点积相似度，随着我们扩展到数百万个文档，这只会变得越来越慢，因此我们理想情况下只对有很高相似可能性的文档进行评分。
- en: We can accomplish this goal by performing what is known as an *approximate-nearest-neighbor*
    (ANN) search. ANN search will efficiently return the most closely related documents
    when given a vector, without the overhead of calculating embedding similarities
    across the entire corpus. ANN search is meant to trade some accuracy in exchange
    for improved logarithmic computational complexity, as well as memory and space
    efficiencies.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过执行所谓的*近似最近邻*（ANN）搜索来实现这一目标。当给定一个向量时，ANN搜索将有效地返回最相关的文档，而无需在整个语料库中计算嵌入相似度。ANN搜索旨在以牺牲一些精度为代价，以换取改进的对数计算复杂度，以及内存和空间效率。
- en: To implement our ANN search, we will be using an index-time strategy to store
    searchable content vectors ahead of time in a specialized data structure. Think
    of ANN search like an “inverted index” for dense vector search.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现我们的ANN搜索，我们将使用索引时间策略，在专门的数据结构中预先存储可搜索的内容向量。将ANN搜索想象成密集向量搜索的“倒排索引”。
- en: 'For our purposes, we will use *Hierarchical Navigable Small World* (HNSW) graphs
    to index and query our dense vectors. We’ll also cover other approaches based
    on clustering and hashing, like product quantization and inverted file indexes
    (IVF), in section 13.7\. HNSW is described in the abstract of its research paper,
    “Efficient and robust approximate nearest neighbor search using Hierarchical Navigable
    Small World graphs,” by Yu. A. Malkov and D.A. Yashunin ([https://arxiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320)):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们的目的，我们将使用*分层可导航小世界*（HNSW）图来索引和查询我们的密集向量。我们还将介绍第13.7节中基于聚类和哈希的其他方法，如产品量化（product
    quantization）和倒排文件索引（IVF）。HNSW在其研究论文的摘要中进行了描述，论文标题为“使用分层可导航小世界图进行高效且鲁棒的近似最近邻搜索”，作者为Yu.
    A. Malkov和D.A. Yashunin ([https://arxiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320))：
- en: We present a new approach for the approximate K-nearest neighbor search based
    on navigable small world graphs with controllable hierarchy (Hierarchical NSW,
    HNSW)…. Hierarchical NSW incrementally builds a multilayer structure consisting
    of hierarchical sets of proximity graphs (layers) for nested subsets of the stored
    elements.
  id: totrans-239
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们提出了一种基于可导航小世界图和可控层次结构（分层NSW，HNSW）的近似K最近邻搜索的新方法……分层NSW逐步构建一个多层结构，该结构由存储元素嵌套子集的层次邻近图集（层）组成。
- en: What this means is that HNSW will cluster similar vectors together as it builds
    the index. Navigable Small World graphs work by organizing data into neighborhoods
    and connecting the neighborhoods with probable relationship edges. When a dense
    vector representation is being indexed, the most appropriate neighborhood and
    its potential connections are identified and stored in the graph data structure.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在构建索引时，HNSW会将相似的向量聚类在一起。可导航小世界图通过将数据组织成邻域并使用可能的关系边连接这些邻域来工作。当对密集向量表示进行索引时，最合适的邻域及其潜在连接将被识别并存储在图数据结构中。
- en: Different ANN approaches
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不同的ANN方法
- en: In this chapter, we use the HNSW algorithm for ANN search. HNSW provides a great
    balance between recall and query throughput and is currently (as of this writing)
    among the most popular ANN approaches. However, many other ANN approaches exist,
    including much simpler techniques like locality-sensitive hashing (LSH). LSH breaks
    the vector space into hash buckets (representing neighborhoods in the vector space)
    and encodes (hashes) each dense vector into one of those buckets. While recall
    is typically much higher for HNSW versus LSH, HNSW depends upon your data to generate
    the neighborhoods, and the neighborhoods can shift over time to better fit your
    data. LSH neighborhoods (hashes) are generated in a data-independent way, which
    can better meet some use cases requiring a priori sharding in distributed systems.
    We’ll also cover other approaches based on clustering and hashing, like product
    quantization and inverted file indexes (IVF), in section 13.7\. It may be worth
    your while to research different ANN algorithms to find the one that best suits
    your application.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用HNSW算法进行ANN搜索。HNSW在召回率和查询吞吐量之间提供了很好的平衡，并且目前（截至本文撰写时）是最受欢迎的ANN方法之一。然而，存在许多其他ANN方法，包括像局部敏感哈希（LSH）这样的简单技术。LSH将向量空间划分为哈希桶（代表向量空间中的邻域）并将每个密集向量编码（哈希）到这些桶中的一个。虽然与LSH相比，HNSW的召回率通常要高得多，但HNSW依赖于您的数据来生成邻域，并且邻域可能会随时间推移而移动以更好地适应您的数据。LSH的邻域（哈希）以数据无关的方式生成，这可以更好地满足一些需要分布式系统预先分片的使用案例。我们还将介绍基于聚类和哈希的其他方法，如产品量化（Product
    Quantization）和倒排文件索引（IVF），在第13.7节中。研究不同的ANN算法以找到最适合您应用程序的一个可能值得您去做。
- en: When an HNSW search is initiated using a dense vector query, it finds the best
    cluster entry point for the query and searches for the closest neighbors. There
    are many other optimization techniques that HNSW implements, and we encourage
    you to read the paper if you want to learn more.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用密集向量查询启动HNSW搜索时，它会找到查询的最佳聚类入口点并搜索最近的邻居。HNSW实现了许多其他优化技术，我们鼓励您阅读论文以了解更多信息。
- en: 13.5.4 ANN index implementation
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.5.4 ANN索引实现
- en: For our ANN search implementation, we’ll start by using a library called the
    Non-Metric Space Library (NMSLIB). This library includes a canonical implementation
    of the *HNSW* algorithm.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的ANN搜索实现，我们将首先使用一个名为非度量空间库（NMSLIB）的库。这个库包括*HNSW*算法的规范实现。
- en: We’ve chosen this library because not only is it fast, but it’s also easy to
    use and requires very little code. Apache Lucene also includes a dense vector
    field type with native HNSW support, making the algorithm available in Solr, OpenSearch,
    Elasticsearch, and other Lucene-based engines like MongoDB Atlas Search. An implementation
    of HNSW is additionally available in other search engines, such as Vespa.ai, Weaviate,
    Milvus, and more.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择这个库的原因不仅是因为它速度快，而且使用起来也很简单，并且需要非常少的代码。Apache Lucene还包括一个具有原生HNSW支持的密集向量字段类型，这使得算法在Solr、OpenSearch、Elasticsearch和其他基于Lucene的引擎（如MongoDB
    Atlas Search）中可用。HNSW的实现还可在其他搜索引擎中找到，例如Vespa.ai、Weaviate、Milvus等。
- en: 'NMSLIB is robust, well-tested, and widely used for ANN applications. NMSLIB
    is also appropriate for showing the simplicity of ANN search without getting into
    the details of the implementation. There are many other ANN libraries available,
    and we encourage you to investigate some of them listed on the excellent ANN Benchmarks
    site: [https://ann-benchmarks.com](https://ann-benchmarks.com).'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: NMSLIB是健壮的、经过充分测试的，并且广泛用于ANN应用程序。NMSLIB也适合展示ANN搜索的简单性，而不涉及实现的细节。还有许多其他ANN库可用，我们鼓励您调查在优秀的ANN
    Benchmarks网站上列出的其中一些：[https://ann-benchmarks.com](https://ann-benchmarks.com)。
- en: To begin using NMSLIB, we simply need to import the library, initialize an index,
    add all of our embeddings to the index as a batch, and then commit. Autocomplete
    is an ideal use case when building an index in this way, because the vocabulary
    is rarely updated. Even though NMSLIB and other libraries may suffer from write-time
    performance in certain situations, this won’t affect our read-heavy autocomplete
    application. From a practical standpoint, we can update our index offline as an
    evening or weekend job and deploy to production when appropriate.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用NMSLIB，我们只需导入库，初始化一个索引，将所有嵌入作为批次添加到索引中，然后提交。当以这种方式构建索引时，自动补全是一个理想的使用案例，因为词汇表很少更新。尽管NMSLIB和其他库可能在某些情况下遭受写入性能的损失，但这不会影响我们的读密集型自动补全应用程序。从实际的角度来看，我们可以将索引更新作为晚上或周末的工作离线进行，并在适当的时候部署到生产环境中。
- en: The following listing creates our HNSW index from all 12,375 embeddings and
    then performs an example search for concepts similar to the term “bag”.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表创建了一个包含所有12,375个嵌入的HNSW索引，然后执行了一个搜索类似“bag”这个术语的概念的示例搜索。
- en: Listing 13.14 ANN search using NMSLIB
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.14 使用NMSLIB进行ANN搜索
- en: '[PRE24]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#1 Initializes a new index, using an HNSW graph in the negdotprod metric space
    (distance function is –1 * dot_product)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用negdotprod度量空间中的HNSW图（距离函数是-1 * 点积）初始化一个新的索引。'
- en: '#2 All the embeddings can be added in a single batch.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 所有嵌入都可以在一个批次中添加。'
- en: '#3 Commits the index to memory. This must be done before you can query for
    nearest neighbors.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将索引提交到内存。在查询最近邻之前，必须完成此操作。'
- en: '#4 Gets the top k nearest neighbors for the term query “bag” (embedding 25)
    in our embeddings'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 获取术语查询“bag”（嵌入25）在我们的嵌入中的前k个最近邻。'
- en: '#5 Looks up the label for each term'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 查找每个术语的标签。'
- en: 'Output:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE25]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: With the index created and committed, we ran a small example comparing the term
    “bag” and seeing what comes back. Interestingly, all these terms are hyponyms,
    which reveals another ideal result. We are interested in suggesting more precise
    terms to the user at autocomplete time. This has a higher likelihood of giving
    the user the chance to select the term most closely associated with their particular
    information need.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建并提交索引后，我们运行了一个小示例，比较了术语“bag”并查看返回的内容。有趣的是，所有这些术语都是下位词，这揭示了另一个理想的结果。我们希望在自动补全时向用户提供更精确的术语。这更有可能让用户有机会选择与他们特定的信息需求最相关的术语。
- en: With our index confirmed working, we can now construct a straightforward query
    function that accepts any term and returns the top suggestions. SBERT has been
    trained using a technique that encodes similar terms to similar vector embeddings.
    Importantly, and in contrast to most lexical autocomplete implementations, this
    function accepts any query regardless of whether it’s already in our dictionary.
    We first take the query and retrieve embeddings by passing the query through the
    same SBERT encoder we used to index our documents. With these embeddings, we access
    the nearest neighbors from the index. If the similarity score is greater than
    `0.75`, we count it as a match and include that as a suggestion. With this function,
    we can get suggestions for complete terms, such as “mountain hike”, as well as
    prefixes, such as “dehyd”.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在确认索引正常工作后，我们现在可以构建一个简单的查询函数，该函数接受任何术语并返回顶级建议。SBERT使用一种技术对相似术语进行编码，以生成相似的向量嵌入。重要的是，与大多数词汇自动补全实现不同，此函数接受任何查询，无论它是否已经在我们的字典中。我们首先获取查询并通过相同的SBERT编码器检索嵌入，该编码器用于索引我们的文档。有了这些嵌入，我们就可以从索引中访问最近邻。如果相似度得分大于`0.75`，我们将其视为匹配并包括作为建议。使用此函数，我们可以获取完整术语的建议，例如“mountain
    hike”，以及前缀，例如“dehyd”。
- en: Listing 13.15 shows our autocomplete `semantic_suggest` function implementation,
    which performs an ANN search for concepts. Our query might not be in the dictionary,
    but we can get the embeddings on demand. We will use the threshold `dist>=0.75`
    to only return similar terms for which we see a high confidence in similarity.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.15显示了我们的自动补全`semantic_suggest`函数实现，该函数执行概念的高斯搜索。我们的查询可能不在字典中，但我们可以在需要时获取嵌入。我们将使用阈值`dist>=0.75`，只返回我们对其相似性有高度信心的相似术语。
- en: Choose a good similarity threshold
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择一个好的相似度阈值
- en: We arrived at the `0.75` threshold for listing 13.15 by looking at the distribution
    from figure 13.7\. This should be further tuned by looking at the quality of results
    for your actual user queries.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过查看图13.7的分布得出了列表13.15的`0.75`阈值。这应该通过查看实际用户查询的结果质量来进一步调整。
- en: NOTE  This function may cause a CPU bottleneck in production, so we recommend
    measuring throughput at scale and adding hardware accordingly.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此函数可能在生产中引起CPU瓶颈，因此我们建议在规模上测量吞吐量并根据需要添加硬件。
- en: Listing 13.15 Encoding a query and returning the *k*-nearest-neighbor concepts
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.15 编码查询并返回*k*个最近邻概念
- en: '[PRE26]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#1 We set k=20 for illustration purposes. In a production application, this
    would likely be set somewhere from 5 to 10.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为了说明目的，我们将k设置为20。在实际应用中，这可能会设置在5到10之间。'
- en: '#2 Gets the embeddings for the query'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 获取查询的嵌入'
- en: '#3 Converts negative dot product distance into a positive dot product'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将负点积距离转换为正点积'
- en: '#4 We’re only returning the terms with 0.75 or higher similarity.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 我们只返回相似度在0.75或更高的术语。'
- en: '#5 No neighbors found! Returns just the original term'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 没有找到邻居！只返回原始术语'
- en: 'Response:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE27]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We’ve done it! We can now efficiently serve up a semantic autocomplete based
    on Transformer embeddings and approximate nearest-neighbor search.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做到了！现在我们可以高效地提供基于Transformer嵌入和近似最近邻搜索的语义自动补全。
- en: Overall, the quality of results for many queries with this model is quite impressive.
    But beware, it’s extremely important to use a labeled dataset to measure success
    before deploying a solution like this to real customers. We’ll demonstrate this
    process of using labeled data to measure and improve relevance when implementing
    question-answering in chapter 14\.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，使用此模型查询许多查询的结果质量相当令人印象深刻。但请注意，在将此类解决方案部署给真实客户之前，使用标记数据集来衡量成功至关重要。我们将在第14章中演示使用标记数据来衡量和改进相关性的过程。
- en: 13.6 Semantic search with LLM embeddings
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.6 使用LLM嵌入进行语义搜索
- en: 'Using what we’ve learned so far, we’ll now take dense vector search to the
    next level: we are going to query the document embeddings with the query embeddings
    as a recall step at search time.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们迄今为止所学到的知识，我们现在将密集向量搜索提升到下一个层次：我们将在搜索时将查询嵌入作为召回步骤查询文档嵌入。
- en: We specifically started with autocomplete as our first implementation, because
    it was helpful to understand the basics of language similarity. It is essential
    to develop a strong intuition about why things are similar or dissimilar in a
    vector space. Otherwise, you will endlessly chase recall problems when using embeddings.
    To build that intuition we started with matching and scoring basic concepts only
    a few words in length each.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特别从自动补全开始作为我们的第一个实现，因为这对理解语言相似性的基础知识很有帮助。在向量空间中，对事物相似或不相似有强烈的直觉是至关重要的。否则，在使用嵌入时，你将无休止地追逐召回问题。为了培养这种直觉，我们只从匹配和评分长度只有几个单词的基本概念开始。
- en: With that understanding, we will now move on to comparing entire sentences.
    We’re going to perform a semantic search for titles. Remember that we’re searching
    on the Stack Exchange outdoors dataset, so the document titles are really the
    summaries of the questions being asked by the contributors. As a bonus, we can
    use the same implementation from the last section to search for question titles
    that are similar to one another.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个理解，我们现在将转向比较整个句子。我们将对标题执行语义搜索。记住，我们正在Stack Exchange户外数据集上搜索，因此文档标题实际上是贡献者提出的问题的摘要。作为额外的好处，我们可以使用上一节中的相同实现来搜索彼此相似的标题。
- en: This function will mostly be a repeat of the encoding and similarity functions
    from the previous section. The code in this section is even shorter, since we
    don’t need to extract concepts.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将主要重复上一节中的编码和相似度函数。本节中的代码甚至更短，因为我们不需要提取概念。
- en: 'Here are the steps we’ll follow:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们将遵循的步骤：
- en: Get the embeddings for all the titles in the outdoors dataset.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取户外数据集中所有标题的嵌入。
- en: Create an NMSLIB index with the embeddings.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用嵌入创建一个NMSLIB索引。
- en: Get the embeddings for a query.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取查询的嵌入。
- en: Search the NMSLIB index.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索NMSLIB索引。
- en: Show the nearest-neighbor titles.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示最近邻标题。
- en: 13.6.1 Getting titles and their embeddings
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.6.1 获取标题及其嵌入
- en: Our NMSLIB index will be made up of title embeddings. We’re using the exact
    same function from the previous autocomplete example, but instead of transforming
    concepts, we’re now transforming the titles of all the questions the outdoors
    community asked. The following listing shows the process of encoding the titles
    into embeddings.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们NMSLIB索引将由标题嵌入组成。我们使用与之前自动完成示例中完全相同的函数，但与转换概念不同，我们现在正在转换户外社区提出的所有问题的标题。以下列表显示了将标题编码为嵌入的过程。
- en: Listing 13.16 Encoding the titles into embeddings
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.16 将标题编码为嵌入
- en: '[PRE28]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#1 Gets the titles for every question in the outdoors corpus'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 获取户外语料库中每个问题的标题'
- en: '#2 Gets the embeddings for the titles (this takes a little while on the first
    run, until it is cached)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 获取标题的嵌入（第一次运行时需要一点时间，直到它被缓存）'
- en: 'Response:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE29]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We have encoded 5,331 titles into embeddings, and figure 13.8 plots the title
    embedding similarity distribution.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将5,331个标题编码为嵌入，图13.8绘制了标题嵌入相似度分布。
- en: Compare figure 13.8 to the concept similarity distributions from figure 13.7\.
    Note the slightly different shape and score distributions, due to the difference
    between titles and concepts. Figure 13.7 has a longer “needle” on top. This is
    because titles are more specific, and therefore will relate differently than broader
    noun and verb phrases.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 将图13.8与图13.7中的概念相似度分布进行比较。注意形状和分数分布略有不同，这是由于标题和概念之间的差异。图13.7顶部有一个更长的“针”。这是因为标题更具体，因此与更广泛的名词和动词短语的关系会有所不同。
- en: '![figure](../Images/CH13_F08_Grainger.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH13_F08_Grainger.png)'
- en: Figure 13.8 The distribution of similarity scores when comparing all title embeddings
    to each another
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.8 比较所有标题嵌入之间的相似度分数分布
- en: 13.6.2 Creating and searching the nearest-neighbor index
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.6.2 创建和搜索最近邻索引
- en: Now that we have generated the embeddings for all the question titles in the
    corpus, we can easily create the nearest-neighbor index.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为语料库中所有问题标题生成了嵌入，我们可以轻松地创建最近邻索引。
- en: Listing 13.17 Creating the ANN title embeddings index
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.17 创建ANN标题嵌入索引
- en: '[PRE30]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: With our newly created index, searching is easy! Listing 13.18 shows the new
    `semantic_ search` function, which implements ANN search for question titles given
    a query. This is very similar to the `semantic_suggest` from listing 13.15 that
    we implemented for autocomplete—the main difference is that here the underlying
    embedding index is comprised of `title` content instead of concepts extracted
    from the `body` content.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们新创建的索引，搜索变得简单！列表13.18显示了新的`semantic_search`函数，它实现了给定查询的标题的ANN搜索。这与我们为自动完成实现的列表13.15中的`semantic_suggest`非常相似——主要区别在于，这里的底层嵌入索引由`title`内容组成，而不是从`body`内容中提取的概念。
- en: Listing 13.18 Performing a semantic search for titles
  id: totrans-304
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.18 对标题执行语义搜索
- en: '[PRE31]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#1 embedding_search from listing 13.15'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 来自列表13.15的embedding_search'
- en: 'Response:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE32]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now let’s take a moment and reflect on these results. Are they all relevant?
    Yes—they are all absolutely questions related to the query `mountain hike`. But,
    and this is very important, are they the *most* relevant documents? We don’t know!
    The reason we don’t know is that `mountain hike` does not provide much context
    at all. So, while the titles are all semantically similar to the query, we don’t
    have enough information to know if they are the documents we should surface for
    the user.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们花点时间来反思这些结果。它们都是相关的吗？是的——它们都是与查询`mountain hike`绝对相关的所有问题。但是，这非常重要，它们是最相关的文档吗？我们不知道！我们不知道的原因是`mountain
    hike`几乎不提供任何上下文。因此，虽然标题在语义上都与查询相似，但我们没有足够的信息来确定它们是否是我们应该向用户展示的文档。
- en: That said, it is clear that this embedding-based approach to search brings interesting
    new capabilities to our matching and ranking toolbox, providing the ability to
    conceptually relate results. Whether those results are better or not depends on
    the context.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，很明显，这种基于嵌入的搜索方法为我们的匹配和排名工具箱带来了有趣的新功能，提供了概念上关联结果的能力。这些结果是否更好取决于上下文。
- en: Thus far, we’ve implemented dense vector search in this chapter by hand, relying
    on the NMSLIB library to do the heavy lifting, but otherwise showing you how to
    build a dense vector index with ANN (HNSW) support and query it. We’ve done this
    intentionally to help you understand the inner workings of dense vector search.
    In your production system, however, you’re likely going to use your search engine’s
    built-in support for dense vector search. In the next listing, we switch over
    to using our `collection` interface to implement the same semantic search functionality
    using your configured search engine or vector database.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们通过手动实现密集向量搜索，依靠NMSLIB库来完成繁重的工作，但同时也向您展示了如何使用ANN（HNSW）支持构建密集向量索引并对其进行查询。我们故意这样做是为了帮助您理解密集向量搜索的内部工作原理。然而，在您的生产系统中，您可能更倾向于使用您搜索引擎内置的密集向量搜索支持。在下一个列表中，我们将切换到使用我们的`collection`接口，通过配置的搜索引擎或向量数据库实现相同的语义搜索功能。
- en: Listing 13.19 Performing vector search with the configured search engine
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.19 使用配置的搜索引擎执行向量搜索
- en: '[PRE33]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '#1 Builds a new collection with documents containing their title and embeddings'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 构建包含文档标题和嵌入的新集合'
- en: '#2 Calculates normalized embeddings for all documents'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为所有文档计算归一化嵌入'
- en: '#3 Returns documents by searching with a query against title_embedding'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 通过对标题嵌入进行查询来返回文档'
- en: '#4 The string query is encoded and normalized then built into a vector search
    request.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将字符串查询编码和归一化，然后构建成向量搜索请求。'
- en: '#5 The quantization size of the embeddings, which in this case is 32 bits'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 嵌入的量化大小，在本例中为32位'
- en: 'Response:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE34]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Every search engine or vector database has its own unique APIs for implementing
    keyword search, vector search, hybrid keyword and vector search, and other capabilities.
    The `semantic_search_with_engine` function in listing 13.19 demonstrates using
    an engine-agnostic interface to query your configured search engine, though you
    may find it more powerful to perform certain operations using your engine’s APIs
    directly for more advanced use cases.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 每个搜索引擎或向量数据库都有自己的独特API来实现关键词搜索、向量搜索、混合关键词和向量搜索以及其他功能。列表13.19中的`semantic_search_with_engine`函数展示了使用一个与引擎无关的接口来查询您配置的搜索引擎，尽管您可能会发现直接使用您引擎的API进行某些操作在更高级用例中更加强大。
- en: We used NMSLIB earlier in this chapter to help you better understand the inner
    workings of dense vector search. Unless you’re doing something very custom, though,
    you likely will want to use your search engine’s built-in, scalable support for
    dense vector search as opposed to implementing it by hand locally with a library
    like NMSLIB, FAISS (which we’ll introduce later in the chapter), or NumPy. You’ll
    note that listing 13.19 returns the exact same results from your engine as the
    NMSLIB implementation returned in listing 13.18.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章早期使用了NMSLIB来帮助您更好地理解密集向量搜索的内部工作原理。然而，除非您正在做非常定制化的工作，否则您可能更希望使用您搜索引擎内置的、可扩展的密集向量搜索支持，而不是像使用NMSLIB、FAISS（我们将在本章后面介绍）或NumPy这样的库手动本地实现。您会注意到列表13.19返回的结果与列表13.18中NMSLIB实现返回的结果完全相同。
- en: Reranking results found with dense vector similarity
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用密集向量相似度重新排序结果
- en: In listing 13.18, we chose the default `min_similarity` threshold to require
    a similarity score of `0.6` or greater. Examine the title similarity distributions
    in figure 13.8—would you change this number to be different than `0.6`?
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表13.18中，我们选择了默认的`min_similarity`阈值，要求相似度分数为`0.6`或更高。检查图13.8中的标题相似度分布——您会改变这个数字使其不同于`0.6`吗？
- en: You could set `min_similarity` to a value lower than `0.6` to potentially increase
    recall and change `k` to be a value higher than `5` as a rerank window size (for
    example, `250`). Then, using this larger result set, you could perform a rerank
    using dot product similarity. Using what you learned in chapters 10–12, you could
    also incorporate the dense vector similarity as a feature (potentially among many)
    in a more sophisticated learning-to-rank model.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将`min_similarity`设置为低于`0.6`的值，以潜在地增加召回率，并将`k`设置为高于`5`的值作为重新排序窗口大小（例如，`250`）。然后，使用这个更大的结果集，您可以使用点积相似度进行重新排序。利用您在10-12章中学到的知识，您还可以将密集向量相似度作为特征（可能还有许多其他特征）纳入更复杂的排序学习模型中。
- en: Similarity scoring of embeddings is one of many features in a mature AI-powered
    search stack. This similarity will be used alongside personalization, learning
    to rank, and knowledge graphs for a robust search experience. Nearest-neighbor-based
    dense vector search is rapidly growing in popularity and is likely to supplant
    Boolean matching with BM25 ranking at some point as the most common retrieval
    and ranking technique for searching unstructured text. The two approaches—dense
    vector search and lexical search—are complementary, however, and hybrid approaches
    combining the two usually work even better.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的相似度评分是成熟人工智能搜索堆栈中的许多功能之一。这种相似度将与个性化、学习排序和知识图谱结合使用，以提供强大的搜索体验。基于最近邻的密集向量搜索正迅速增长，并可能在某个时刻取代布尔匹配与BM25排名，成为搜索非结构化文本最常见的信息检索和排序技术。这两种方法——密集向量搜索和词汇搜索——是互补的，而结合两种方法的混合方法通常效果更好。
- en: 13.7 Quantization and representation learning for more efficient vector search
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.7 量化与表示学习以实现更高效的向量搜索
- en: 'In the last section, we introduced two concepts commonly used to speed up dense
    vector search: ANN search and reranking. Conceptually, ANN is a way to reduce
    the number of vector similarity calculations necessary at query time by efficiently
    locating and filtering to the top vectors that are the most likely to be similar
    to the query vector. Because ANN search is an approximation of the best results,
    it’s common to rerank those top potential results using more precise (and computationally
    expensive) vector similarity calculations to get recall and relevance back on
    par with a non-ANN-optimized search.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了两个常用于加速密集向量搜索的概念：ANN搜索和重排序。从概念上讲，ANN是一种通过高效定位和过滤出最有可能与查询向量相似的前置向量，从而在查询时减少必要的向量相似度计算数量的方法。由于ANN搜索是对最佳结果的近似，因此通常使用更精确（且计算成本更高）的向量相似度计算对这些顶级潜在结果进行重排序，以恢复召回率和相关性，使其与非ANN优化的搜索相当。
- en: 'The computation time and amount of memory required to represent and perform
    similarity calculations on vectors is directly related to the size of the vectors
    being searched. In this section, we’ll introduce a few additional techniques for
    improving the efficiency of vector search:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 表示和执行向量相似度计算所需的计算时间和内存量与被搜索向量的规模直接相关。在本节中，我们将介绍一些额外的技术来提高向量搜索的效率：
- en: Scalar quantization
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量量化
- en: Binary quantization
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制量化
- en: Product quantization
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品量化
- en: Matryoshka Representation Learning
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马特罗什卡表示学习
- en: '*Quantization* is a technique used to reduce the memory footprint and computational
    complexity of numeric data representations, such as embedding vectors. In the
    context of embeddings, quantization involves compressing them by reducing the
    number of bits used to represent the features of the vector. Embeddings are typically
    represented as floating point numbers (floats), which are 32 bits (or 4 bytes)
    in size by default. If a typical vector embedding has 1,024 dimensions, this translates
    into 1,024 x 4 bytes, or 4,096 bytes (4 KB) per vector. If you have a large number
    of vectors to store and search, this can quickly add up to a significant amount
    of memory and computational overhead.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '*量化*是一种用于减少数值数据表示（如嵌入向量）的内存占用和计算复杂度的技术。在嵌入的上下文中，量化涉及通过减少表示向量特征的位数来压缩它们。嵌入通常表示为浮点数（floats），默认情况下大小为32位（或4字节）。如果一个典型的向量嵌入有1,024个维度，这相当于1,024
    x 4字节，即每个向量4,096字节（4 KB）。如果你有大量向量需要存储和搜索，这会迅速增加大量的内存和计算开销。'
- en: Quantizing embeddings allows you to trade off some ideally small amount of recall
    for significant improvements in storage efficiency and query speed, which is crucial
    for large-scale search systems. For example, you can reduce the memory usage of
    a vector of 32-bit floats (Float32) by 75% by converting each feature to an 8-bit
    integer (Int8). Compressing the individual numerical values (scalars) of each
    dimension like this is known as *scalar quantization*. This can often be done
    without significantly affecting recall, and it can be especially useful when you
    have a large number of vectors to store and search. You can even quantize each
    feature down to a single bit—a technique known as *binary quantization*—and still
    maintain a relatively high level of recall if it’s combined with a reranking step
    using a higher-precision vector on the top-*N* results. Figure 13.9 visually demonstrates
    the concepts of scalar quantization and binary quantization using a vector containing
    an image of the cover of this book (assume the dimensions of the vector represent
    pixels in the image).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 量化嵌入允许你以牺牲一些理想上很小量的召回率为代价，显著提高存储效率和查询速度，这对于大规模搜索系统至关重要。例如，你可以通过将每个特征转换为 8 位整数（Int8）来减少
    32 位浮点数（Float32）向量的内存使用量 75%。以这种方式压缩每个维度的单个数值（标量）称为 *标量量化*。这通常可以在不显著影响召回率的情况下完成，并且当你有大量向量需要存储和搜索时特别有用。你甚至可以将每个特征量化到单个比特——一种称为
    *二进制量化* 的技术——如果与使用更高精度向量在顶部-*N* 结果上重新排序的步骤结合使用，仍然可以保持相对较高的召回率。图 13.9 使用包含本书封面图像的向量（假设向量的维度代表图像中的像素）直观地展示了标量量化和二进制量化的概念。
- en: '![figure](../Images/CH13_F09_Grainger.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH13_F09_Grainger.png)'
- en: 'Figure 13.9 Quantizing data: full precision, reduced scalar precision, and
    binary precision'
  id: totrans-337
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 13.9 数据量化：全精度、降低标量精度和二进制精度
- en: In the figure, you can see the original image (no quantization), a scalar quantized
    image (using a reduced color palette that maps to similar ranges but with less
    colors/precision), and a binary quantized image (where each pixel is either black
    or white). You’ll notice that the scalar quantized image still retains most of
    the important detail from the original image, and the binary quantized version
    is still clearly recognizable, albeit losing some important data (some of the
    characters in the title and the colors).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，你可以看到原始图像（未量化）、标量量化图像（使用减少的颜色调色板，映射到相似的范围但颜色/精度较少）和二进制量化图像（每个像素要么是黑色要么是白色）。你会注意到标量量化图像仍然保留了原始图像的大部分重要细节，而二进制量化版本仍然可以清楚地识别，尽管丢失了一些重要数据（标题中的某些字符和颜色）。
- en: In this section, we’ll cover scalar quantization, binary quantization, and a
    third type of quantization called product quantization. We’ll also introduce a
    multi-tiered embedding approach known as Matryoshka Representation Learning, which
    can be used to dynamically switch the precision levels of already-generated embeddings
    without the need for additional quantization or retraining.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍标量量化、二进制量化和另一种称为乘积量化的第三种量化类型。我们还将介绍一种称为马特罗什卡表示学习的多层嵌入方法，它可以用来动态切换已生成的嵌入的精度级别，而无需额外的量化或重新训练。
- en: 13.7.1 Scalar quantization
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.7.1 标量量化
- en: 'Scalar quantization is the simplest form of quantization, where each value
    in the embedding vector is independently mapped to a lower-precision representation.
    Consider the following two vectors:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 标量量化是量化最简单的形式，其中嵌入向量中的每个值独立映射到较低精度的表示。考虑以下两个向量：
- en: '[PRE35]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The first vector is a 32-bit float representation, and the second is a 16-bit
    float representation, each rounded to the maximum reliable precision. The second
    vector requires 50% less memory (2 bytes versus 4 bytes), all while still representing
    approximately the same values, just with less precision.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个向量是一个 32 位浮点表示，第二个是一个 16 位浮点表示，每个都四舍五入到最大可靠的精度。第二个向量需要的内存减少了 50%（2 字节对 4
    字节），同时仍然表示大约相同的价值，只是精度较低。
- en: This reduced precision is a simple example of scalar quantization, taking higher-precision
    values and mapping them into lower-precision representations requiring less memory
    to store and less computation to process.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这种降低精度是标量量化的一个简单例子，将更高精度的值映射到需要更少内存来存储和更少计算来处理的较低精度表示。
- en: But what if we wanted to compress our vectors even further to a single byte
    (or even a few bits)—can we still pull this off and preserve most of our recall?
    The answer is yes, and we commonly accomplish this by mapping the range of float
    values into an Int8, as shown in figure 13.10.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们想进一步压缩向量到单个字节（甚至几个比特）——我们还能做到这一点并保留大部分召回率吗？答案是肯定的，我们通常通过将浮点值范围映射到Int8来实现这一点，如图13.10所示。
- en: '![figure](../Images/CH13_F10_Grainger.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH13_F10_Grainger.png)'
- en: Figure 13.10 Scalar quantization from Float32 to Int8
  id: totrans-347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.10 从Float32到Int8的标量量化
- en: In the figure, the curve at the top represents the distribution of values in
    the floating point range. Since we are quantizing from 32 bits to 8 bits, we map
    the range of the Float32 values into the smaller range of –128 to 127 (or 0 to
    255 if using an unsigned integer). Depending on the quantization algorithm being
    used, attempts are often made to utilize the new ranges as fully as possible by
    *clamping* the values (limiting the range to the minimum and maximum), as well
    as by utilizing the density of values in the original vector to map more evenly
    into the new, quantized range.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，顶部的曲线表示浮点数范围内的值分布。由于我们从32位量化到8位，我们将Float32值的范围映射到更小的范围-128到127（如果使用无符号整数，则为0到255）。根据所使用的量化算法，通常会尝试尽可能充分地利用新的范围，通过*钳位*（限制范围到最小和最大值）以及利用原始向量中值的密度，将它们更均匀地映射到新的、量化的范围内。
- en: Let’s implement a scalar quantization example to see what effect this optimization
    yields on index size, recall, and search speed. Multiple libraries exist for performing
    scalar quantization. You can use the `sentence_transformers.quantization` module,
    or your search engine or language model may have its own quantization implementations
    built in. We are going to utilize the FAISS library for our quantized indexes,
    and a combination for the `sentence_transformers` library and FAISS for each of
    our quantization examples. FAISS (Facebook AI Similarity Search) is an open source
    library designed for efficient similarity search and clustering of dense vectors.
    It’s similar to the NMSLIB library we used earlier in the chapter for semantic
    search, but it has some additional features, including built-in support for quantization.
    FAISS is widely used in production systems for dense vector search, and it’s a
    great choice for implementing quantized indexes.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个标量量化示例，看看这种优化对索引大小、召回率和搜索速度有什么影响。存在多个用于执行标量量化的库。您可以使用`sentence_transformers.quantization`模块，或者您的搜索引擎或语言模型可能内置了自己的量化实现。我们将利用FAISS库来构建我们的量化索引，并使用`sentence_transformers`库和FAISS的组合来构建我们的每个量化示例。FAISS（Facebook
    AI Similarity Search）是一个开源库，旨在高效地进行密集向量的相似性搜索和聚类。它与我们在本章早期用于语义搜索的NMSLIB库类似，但它有一些额外的功能，包括内置的量化支持。FAISS在密集向量搜索的生产系统中得到广泛应用，并且是实现量化索引的一个很好的选择。
- en: We’ll look at an example later of how to run a search using the `collection`
    abstraction for your chosen search engine, but since not every engine has support
    for all quantization modes, and since each engine has different overhead and performance
    characteristics, we’ll use FAISS for our benchmarking.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会通过一个示例来展示如何使用所选搜索引擎的`collection`抽象进行搜索，但由于并非每个搜索引擎都支持所有量化模式，并且每个搜索引擎都有不同的开销和性能特征，我们将使用FAISS进行基准测试。
- en: Let’s now build a FAISS index with full-precision Float32 embeddings. We’ll
    then use this as our baseline versus various quantized embedding indexes to compare
    index sizes, search speeds, and recall rates. Listing 13.20 shows the code to
    create the full-precision Float32 embeddings and index them into a FAISS index.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将构建一个具有全精度Float32嵌入的FAISS索引。然后我们将将其作为基准与各种量化嵌入索引进行比较，以比较索引大小、搜索速度和召回率。列表13.20显示了创建全精度Float32嵌入并将它们索引到FAISS索引中的代码。
- en: Listing 13.20 Indexing full-precision embeddings using FAISS
  id: totrans-352
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.20 使用FAISS索引全精度嵌入
- en: '[PRE36]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '#1 This model creates embeddings supporting all upcoming optimization techniques.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 该模型创建支持所有即将到来的优化技术的嵌入。'
- en: '#2 The original embeddings will have 1,024 dimensions.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 原始嵌入将具有1,024个维度。'
- en: '#3 IndexFlatIP is a simple, unoptimized index supporting different embedding
    formats.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 IndexFlatIP是一个简单、未优化的索引，支持不同的嵌入格式。'
- en: '#4 Adds documents to the index'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将文档添加到索引中'
- en: '#5 Writes the index to disk'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 将索引写入磁盘'
- en: '#6 Generates embeddings for the outdoors dataset'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 为户外数据集生成嵌入'
- en: '#7 Creates a full-precision (Float32) FAISS index'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 创建一个全精度（Float32）的FAISS索引'
- en: In this listing, we calculate embeddings for the outdoors dataset using the
    `mixedbread -ai/mxbai-embed-large-v1` model, which produces high-quality embeddings
    that work well with all our quantization techniques and also supports Matryoshka
    Representation Learning, which we’ll explore later in the chapter. We then index
    those embeddings to a full-precision (Float32) FAISS index, which we’ll use soon
    as our baseline for benchmarking the performance of various quantization techniques.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个列表中，我们使用 `mixedbread -ai/mxbai-embed-large-v1` 模型为户外数据集计算嵌入表示，该模型产生高质量的嵌入表示，与我们的所有量化技术都很好地配合工作，并且支持玛特罗什卡表示学习，我们将在本章后面探讨。然后我们将这些嵌入索引到一个完整精度（Float32）的FAISS索引中，我们将很快将其用作基准来评估各种量化技术的性能。
- en: For our benchmarks, we’ll also need to encode some test queries into embeddings,
    which is shown in listing 13.21.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的基准测试，我们还需要将一些测试查询编码成嵌入表示，这将在列表 13.21 中展示。
- en: Listing 13.21 Generating query embeddings and full-index benchmarks
  id: totrans-363
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.21 生成查询嵌入和完整索引基准
- en: '[PRE37]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '#1 Gets test queries for benchmarking'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 获取基准测试的测试查询'
- en: '#2 Generates embeddings for each query'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为每个查询生成嵌入表示'
- en: '#3 Generates search time, index size, and recall statistics for the full-precision
    (Float32) index'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 为完整精度（Float32）索引生成搜索时间、索引大小和召回率统计'
- en: '#4 Displays the benchmarking stats'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 显示基准测试统计信息'
- en: 'Output:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE38]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the preceding listing, we define a list of queries we’ll use to benchmark
    our full-precision index against various vector search optimization strategies.
    We then call a `time_and_execute_search` function (omitted for brevity) on the
    full-precision index from listing 13.20 and then pass the results to a `display_statistics`
    function (also omitted for brevity), which displays the search time, index size,
    and recall statistics.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的列表中，我们定义了一个查询列表，我们将使用它来基准测试我们的完整精度索引与各种向量搜索优化策略。然后我们对列表 13.20 中的完整精度索引调用
    `time_and_execute_search` 函数（为了简洁省略），然后将结果传递给 `display_statistics` 函数（同样为了简洁省略），该函数显示搜索时间、索引大小和召回率统计。
- en: 'This provides a baseline for comparison with our upcoming quantized (or otherwise
    optimized) indexes. Listing 13.22 shows the implementation of two additional functions
    we’ll use to compare results from other indexing strategies: an `evaluate_search`
    function and an `evaluate_rerank_search` function.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们即将推出的量化（或优化）索引提供了一个比较基准。列表 13.22 展示了我们将使用来比较其他索引策略结果的两个附加函数的实现：`evaluate_search`
    函数和 `evaluate_rerank_search` 函数。
- en: Listing 13.22 Functions to benchmark optimized search approaches
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.22 用于基准测试优化搜索方法的函数
- en: '[PRE39]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '#1 Brings back the top 25 results from each index and compares them'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从每个索引中检索前25个结果并进行比较'
- en: '#2 Calculates query speed, index size, and recall for optimized versus full-precision
    index'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 计算优化索引与完整精度索引的查询速度、索引大小和召回率'
- en: '#3 Same as evaluate_search, but over-requests to k=50 results (by default)
    and reranks those using full-precision embeddings'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 与 evaluate_search 相同，但请求 k=50 个结果（默认值）并使用完整精度嵌入重新排序这些结果'
- en: '#4 Generates embeddings for each doc in the outdoors dataset'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 为户外数据集中的每个文档生成嵌入表示'
- en: '#5 Performs a dot product between the query embedding and the top-k embeddings'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 在查询嵌入和前k个嵌入之间执行点积操作'
- en: '#6 Sorts the results by dot product score to rerank them'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 按点积分数对结果进行排序以重新排序'
- en: '#7 Helper function (omitted) that combines IDs and scores with other document
    fields to return'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 辅助函数（省略）用于将ID和分数与其他文档字段结合，以返回'
- en: '#8 Calculates the recall of results after reranking versus the full-precision
    index'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 计算重新排序后的结果与完整精度索引的召回率'
- en: The `evaluate_search` function internally calls the `time_and_execute_search`
    function on both the full-precision index and the quantized index, and it passes
    the results to a `display_statistics` function to compare and display the search
    time, index size, and recall statistics.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_search` 函数在内部对完整精度索引和量化索引都调用 `time_and_execute_search` 函数，并将结果传递给
    `display_statistics` 函数以比较和显示搜索时间、索引大小和召回率统计。'
- en: The `evaluate_rerank_search` function then calculates recall again after reranking
    the top-*N* results from the quantized index using full-precision embeddings.
    While quantization can drastically reduce memory and search times, we’ll also
    see that it reduces recall, meaning that some of the results that *should* be
    returned are not. But by over-requesting and reranking just the top-*N* results
    using full-precision embeddings (typically loaded from disk after the quantized
    search, not kept in memory with the index), we can recapture most of that lost
    recall.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`evaluate_rerank_search` 函数在重新排序使用全精度嵌入从量化索引中获取的顶级-*N* 结果后，再次计算召回率。虽然量化可以大幅减少内存和搜索时间，但我们会看到它也降低了召回率，这意味着一些*应该*返回的结果没有返回。但是，通过过度请求并使用全精度嵌入（通常在量化搜索后从磁盘加载，而不是与索引一起保留在内存中）重新排序仅顶级-*N*
    结果，我们可以恢复大部分丢失的召回率。
- en: We’ll show both the quantized recall and the reranked quantized recall in each
    subsequent listing to demonstrate the trade-offs between full-precision searches,
    quantized searches, and reranked quantized searches. For our first quantization
    example, let’s implement Int8 scalar quantization.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在每个后续列表中展示量化召回率和重新排序的量化召回率，以展示全精度搜索、量化搜索和重新排序的量化搜索之间的权衡。对于我们的第一个量化示例，让我们实现
    Int8 标量量化。
- en: Listing 13.23 Creating an Int8 quantized embeddings index using FAISS
  id: totrans-386
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.23 使用 FAISS 创建 Int8 量化嵌入索引
- en: '[PRE40]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '#1 Quantizes the doc embeddings to Int8 precision'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将文档嵌入量化为 Int8 精度'
- en: '#2 Creates an index configured to expect the shape of the embeddings'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建一个配置为期望嵌入形状的索引'
- en: '#3 Adds the quantized embeddings to the index'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将量化嵌入添加到索引中'
- en: '#4 Saves the index to disk so we can measure its size'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将索引保存到磁盘，以便我们可以测量其大小'
- en: '#5 Quantizes the query embeddings to Int8 precision'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 将查询嵌入量化为 Int8 精度'
- en: '#6 Performs benchmarks for search time, index size, and recall'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 对搜索时间、索引大小和召回率进行基准测试'
- en: '#7 Performs benchmarks again allowing reranking of top results with full-precision
    embeddings'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 再次执行基准测试，允许使用全精度嵌入重新排序顶级结果'
- en: 'Output:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE41]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In listing 13.23’s output, we see that the Int8 quantized index is 75% smaller
    than the full-precision index, which makes sense given that we’ve reduced from
    Float32 to Int8 precision, which is an ~75% reduction from 32 bits to 8 bits.
    Likewise, we see that the total search time improved due to lower precision numbers
    being more efficient to process (at least on some systems). Note that search speed
    will vary *considerably* on all these benchmarks from system to system and run
    to run, but the index size benchmark should always be the same. Also note that
    we have a fairly small number of documents in our index, and query speed improvements
    from quantization are likely to become more pronounced as the number of documents
    increases.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 13.23 的输出中，我们看到 Int8 量化索引比全精度索引小 75%，这是有道理的，因为我们已经从 Float32 精度降低到 Int8 精度，这是从
    32 位到 8 位的 ~75% 减少。同样，我们看到由于精度较低的数字处理效率更高（至少在某些系统上），总搜索时间得到了改善。请注意，所有这些基准测试的搜索速度都会因系统而异，每次运行而异，但索引大小基准测试应该始终相同。此外，请注意，我们索引中的文档数量相当少，量化带来的查询速度提升随着文档数量的增加可能会变得更加明显。
- en: The most important numbers, however, are the recall numbers. The Int8 quantized
    search maintained a 92.89% recall rate. This means that we achieved an ~75% reduction
    in index size and a significant improvement in search speed with only 7.11% of
    the top *N*=25 results missing in the quantized search. Just as the middle image
    of this book’s cover in figure 13.9 maintained the vast majority of the important
    details of the original image, we were likewise able to retain high fidelity to
    our quantized embeddings in listing 13.23 when using Int8 quantization.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最重要的数字是召回率。Int8 量化搜索保持了 92.89% 的召回率。这意味着我们实现了索引大小约 75% 的减少和搜索速度的显著提高，而只有
    7.11% 的顶级 *N*=25 结果在量化搜索中丢失。正如本书封面图 13.9 的中间图像保持了原始图像的大部分重要细节一样，我们同样能够在使用 Int8
    量化时在列表 13.23 中保留量化嵌入的高保真度。
- en: 'The `Reranked recall` value of 1.0 further indicates that by just requesting
    the top *N*=50 results and reranking them using the original full-precision embeddings,
    we get back to 100% recall. This is a common pattern when using quantization in
    dense vector search: perform an initial search that over-requests results using
    a quantized index (for a significant memory and speed improvement) and then rerank
    the top *N* results using higher-precision embeddings (usually pulled from disk
    so they don’t affect your index and memory requirements) to recapture the lost
    recall and ranking precision. While these improvements are impressive, we could
    continue compressing even further, using 4 bits (Int4) or less. In the next section,
    we’ll see what happens when we compress each dimension all the way down to a single
    bit!'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '`重排序召回率` 值为 1.0 进一步表明，仅通过请求前 *N*=50 个结果并使用原始全精度嵌入进行重排序，我们就能达到 100% 的召回率。当在密集向量搜索中使用量化时，这是一个常见的模式：首先使用量化索引进行初始搜索，以请求过多的结果（为了显著提高内存和速度），然后使用更高精度的嵌入（通常从磁盘上拉取，这样就不会影响您的索引和内存需求）对前
    *N* 个结果进行重排序，以重新捕获丢失的召回率和排名精度。虽然这些改进令人印象深刻，但我们还可以进一步压缩，使用 4 比特（Int4）或更少。在下一节中，我们将看到当我们将每个维度压缩到单个比特位时会发生什么！'
- en: 13.7.2 Binary quantization
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.7.2 二进制量化
- en: Binary quantization is an extreme form of quantization where each value in the
    embedding vector is represented by a single bit, reducing the values to `0` or
    `1`. This method is akin to converting an image to black and white only (just
    one shade of black, not grayscale), like in the example on the right in figure
    13.9\.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制量化是一种极端的量化形式，其中嵌入向量中的每个值都由单个比特位表示，将值简化为 `0` 或 `1`。这种方法类似于将图像转换为仅黑白（只有一种黑色，不是灰度），就像图
    13.9 右侧的示例中那样。
- en: Quantizing every feature into a single bit can be done using a simple threshold
    to assign a bit of `0` for any feature less than or equal to `0` and a value of
    `1` for any feature greater than `1.0`. This works well if feature values across
    embeddings have a uniform distribution between positive and negative values. If
    feature values are not uniformly distributed across documents, however, it can
    be helpful to use the median value for each feature, or some similar threshold
    for assigning `0` versus `1` as the binary quantized value, in order to more evenly
    distribute the values.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的阈值将每个特征量化为单个比特位，可以为任何小于或等于 `0` 的特征分配比特 `0`，对于任何大于 `1.0` 的特征分配值为 `1`。如果特征值在嵌入向量中在正负值之间具有均匀分布，这种方法效果很好。然而，如果特征值在文档中分布不均匀，则使用每个特征的中值或类似的阈值来分配
    `0` 或 `1` 作为二进制量化值，可以帮助更均匀地分布这些值。
- en: Figure 13.11 illustrates the result of binary quantization, where only the most
    essential information is preserved, akin to the black-and-white version of the
    cover of this book. The far left image demonstrates using a simple threshold,
    the middle image represents using a non-uniform threshold that assigns values
    based on the relative distribution of values for each feature in the original
    embeddings, and the far right image shows an optimal learned binary representation
    directly from the Transformer encoder model. This last “model-based” binary quantization
    looks the best because the choice of values takes into account the context of
    the entire image when generating the binary representation, not just the individual
    features. This allows for a much more meaningful and accurate representation of
    the original embedding to be encoded into the available binary quantized feature
    values based on how the model has been trained to understand the image as a whole.
    We won’t demonstrate an example of doing model-based binary quantization, since
    we’re trying to benchmark recall for the same embeddings at different levels of
    quantization, but it’s worth keeping in mind that if your model supports model-based
    binary quantization, it will generally provide better results than performing
    binary quantization on features after the model has already encoded and returned
    them at a higher level of precision.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11展示了二进制量化的结果，其中仅保留了最基本的信息，类似于这本书封面的黑白版本。最左边的图像展示了使用简单的阈值，中间的图像表示使用非均匀阈值，该阈值根据原始嵌入中每个特征的相对值分布来分配值，而最右边的图像显示了直接从Transformer编码器模型中学习到的最优二进制表示。这种最后的“基于模型”的二进制量化看起来最好，因为值的选取在生成二进制表示时考虑了整个图像的上下文，而不仅仅是单个特征。这允许根据模型如何被训练来理解整个图像，对原始嵌入进行更有意义和更准确的编码，基于可用的二进制量化特征值。我们不会展示基于模型进行二进制量化的示例，因为我们正在尝试在不同量化级别上对相同的嵌入进行召回率基准测试，但值得记住的是，如果你的模型支持基于模型的二进制量化，它通常会比在模型已经以更高精度编码并返回它们之后对特征进行二进制量化提供更好的结果。
- en: '![figure](../Images/CH13_F11_Grainger.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F11_Grainger.png)'
- en: Figure 13.11 Different binary quantization techniques
  id: totrans-405
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.11不同的二进制量化技术
- en: Let’s now implement binary quantization using FAISS and benchmark the results.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用FAISS实现二进制量化并基准测试结果。
- en: Listing 13.24 Indexing and benchmarking binary quantized embeddings
  id: totrans-407
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.24索引和基准测试二进制量化嵌入
- en: '[PRE42]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '#1 Quantizes the doc embeddings to binary (1 bit per dimension)'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将文档嵌入量化为二进制（每个维度1位）'
- en: '#2 Creates the binary embeddings index'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建二进制嵌入索引'
- en: '#3 Adds all the doc embeddings to the index'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将所有文档嵌入添加到索引中'
- en: '#4 Writes the index to disk'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将索引写入磁盘'
- en: '#5 Quantizes the query embeddings to binary'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 将查询嵌入量化为二进制'
- en: '#6 The doc embeddings are provided to the quantizer to calibrate the best thresholds
    for assigning 0 or 1.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 将文档嵌入提供给量化器以校准分配0或1的最佳阈值。'
- en: '#7 Saves every 8 dimensions as 1 byte, encoded as unsigned Int8'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 将每8个维度保存为1字节，编码为无符号Int8'
- en: '#8 Performs benchmarks with and without reranking versus the full-precision
    index'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 对全精度索引进行带和不带重新排序的基准测试'
- en: 'Output:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE43]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: While it should come as no surprise that binary quantization reduces the index
    size and memory requirements by 96.87% (from 32 bits to 1 bit), it is mind-boggling
    that we were able to retain 60.44% of the recall with just a single bit per embedding
    feature. While 60% recall isn’t necessarily sufficient for many search use cases,
    note that when we over-requested by a factor of 2 (`N=50` to find the top 25)
    and reranked, we were able to get recall back to 100% for our test queries. Across
    a larger dataset and number of queries, you won’t likely be able to maintain 100%
    recall, but even approaching full recall with just a single bit per feature for
    your initial search is an impressive feat.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然二进制量化将索引大小和内存需求减少了96.87%（从32位到1位）这一点并不令人意外，但令人惊讶的是，我们仅通过每个嵌入特征使用一个比特就能保留60.44%的召回率。虽然60%的召回率对于许多搜索用例来说可能并不充分，但请注意，当我们通过2倍增加请求量（`N=50`以找到前25个）并重新排序时，我们能够将测试查询的召回率恢复到100%。在更大的数据集和查询数量中，你不太可能保持100%的召回率，但即使仅使用每个特征的一个比特进行初始搜索就能接近完全召回，这也是一项令人印象深刻的成就。
- en: With the ability to achieve such extreme compression while still getting back
    to nearly 100% recall by over-requesting and reranking, binary quantization pushes
    the limits of what’s possible with quantization. You might think that a single
    bit per dimension would be the limit for how far we can quantize our embeddings,
    but in the next section we’ll introduce a technique that allows us to compress
    even further—product quantization.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在能够通过过度请求和重新排序而几乎达到100%的召回率的同时实现如此极端的压缩，二进制量化推动了量化可能性的极限。你可能认为每个维度一个比特将是我们可以量化嵌入的极限，但在下一节中，我们将介绍一种允许我们进一步压缩的技术——产品量化。
- en: 13.7.3 Product quantization
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.7.3 产品量化
- en: While scalar quantization and binary quantization focus on reducing the precision
    of the individual features of an embedding, product quantization (PQ) instead
    focuses on quantizing the entire embedding down to a more memory-efficient representation.
    It allows for even deeper compression than binary quantization, making PQ particularly
    beneficial for large-scale search applications with a high number of embedding
    dimensions.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然标量量化二进制量化关注于降低嵌入中单个特征的精度，但产品量化（PQ）则专注于将整个嵌入量化为更内存高效的表示。它允许比二进制量化更深层次的压缩，使得PQ对于具有大量嵌入维度的大规模搜索应用尤其有益。
- en: Imagine you have a long sentence and you split it into shorter phrases; it’s
    easier to handle and process each phrase independently. Similarly, PQ divides
    the vector space into smaller regions (subvectors) and quantizes each subvector
    individually. PQ then clusters each of the subvector regions to find a set of
    cluster centroids and finally assigns one centroid ID to each subvector for each
    document. This list of centroid IDs (one per subvector) for each document is called
    a PQ code, and it is the quantized representation of the document’s embedding.
    Figure 13.12 demonstrates the PQ process.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有一个很长的句子，你可以将其拆分成更短的短语；这样处理和加工每个短语就更容易了。同样，PQ将向量空间划分为更小的区域（子向量），并对每个子向量单独进行量化。然后，PQ将每个子向量区域进行聚类，以找到一组聚类中心，并最终为每个文档的每个子向量分配一个中心点ID。每个文档的每个子向量对应的中心点ID列表（每个子向量一个）被称为PQ码，它是文档嵌入的量化表示。图13.12展示了PQ过程。
- en: '![figure](../Images/CH13_F12_Grainger.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F12_Grainger.png)'
- en: Figure 13.12 The product quantization process
  id: totrans-425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.12 产品量化过程
- en: The quantization process starts with the top layer of the figure and progresses
    sequentially down. First, the original vector space of `d` dimensions is divided
    into `M` subvector spaces. For each of the `M` subvector spaces, the corresponding
    subvector is partitioned from each original embedding. Clustering (usually using
    a *k*-means algorithm) is then performed on all the subvectors within each subspace
    to create a list of `num_clusters` clusters identified by their centroid vector
    within the subspace. Each centroid is then assigned an ID, which is recorded in
    a *codebook* containing a mapping of each centroid ID to its full subvectors.
    Finally, the centroid IDs for every subspace are concatenated together per document
    to create the *PQ code*, which is the official quantized representation of each
    document.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 量化过程从图形的顶层开始，并按顺序向下进行。首先，将`d`维度的原始向量空间划分为`M`个子向量空间。对于每个`M`个子向量空间，从每个原始嵌入中划分出相应的子向量。然后，在子空间内的所有子向量上执行聚类（通常使用*k*-means算法），以创建一个由子空间内其质心向量标识的`num_clusters`个聚类列表。然后，每个质心被分配一个ID，该ID记录在包含每个质心ID与其完整子向量映射的*代码簿*中。最后，每个文档的每个子空间的质心ID按顺序连接在一起，创建出*PQ码*，这是每个文档的官方量化表示。
- en: The documents’ PQ codes are then indexed, and at query time the query embedding
    is divided into the same `M` subspaces, and the distance between the query subvector
    in that subspace and each centroid in the subspace is calculated and cached in
    a lookup table. Since each document’s PQ code maps to a specific centroid in each
    subspace, we can get the approximate distance between the query subvector and
    document subvector by looking it up in the cached lookup table. The vector similarity
    scores between the query and each document can then be calculated and ranked by
    the smallest distance across the combined subspaces, using a metric like inner
    product or Euclidean distance. Calculating the Euclidean distance, for example,
    just requires taking the square root of the sum of the squared distances from
    each subvector.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的PQ代码随后被索引，在查询时，查询嵌入被划分为相同的`M`个子空间，并计算该子空间中查询子向量与每个聚类中心之间的距离，并将其缓存在查找表中。由于每个文档的PQ代码映射到每个子空间中的特定聚类中心，我们可以通过查找缓存的查找表来获取查询子向量与文档子向量之间的近似距离。然后，可以使用内积或欧几里得距离等度量标准计算查询与每个文档之间的向量相似度得分，并按组合子空间中最小距离进行排序。例如，计算欧几里得距离只需要对每个子向量距离平方和的平方根。
- en: Libraries like FAISS provide efficient implementations of PQ out of the box.
    Listing 13.25 demonstrates the process of building an index using PQ.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 像FAISS这样的库提供了PQ的现成高效实现。列表13.25展示了使用PQ构建索引的过程。
- en: Listing 13.25 Building and benchmarking a product quantization index
  id: totrans-429
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.25构建和基准测试产品量化索引
- en: '[PRE44]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '#1 The original embedding is 1024 dimensions.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 原始嵌入是1024维。'
- en: '#2 Divides the embedding into M=16 subvectors (of 64 dimensions each)'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将嵌入划分为M=16个子向量（每个子向量为64维）'
- en: '#3 8 bits = 256 maximum cluster centroids per subvector.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 8位=每个子向量最多256个聚类中心。'
- en: '#4 Creates the PQ index'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 创建PQ索引'
- en: '#5 Generates the cluster centroids using k-means clustering'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 使用k-means聚类生成聚类中心'
- en: '#6 Adds all the doc_embeddings to the index'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 将所有doc_embeddings添加到索引中'
- en: '#7 Saves the index to disk so we can measure the size'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 将索引保存到磁盘，以便我们可以测量其大小'
- en: '#8 Runs the benchmarks versus the full-precision index'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 运行与全精度索引的基准测试'
- en: 'Output:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE45]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You’ll notice immediately that the recall for the PQ index is significantly
    lower than the scalar and binary quantization types we benchmarked at 33.33% recall
    without reranking and 68% recall after reranking. You’ll also notice, however,
    that the index size is 98.22% smaller than the original full-precision index.
    Obviously, we’ve made an intentional trade-off here to hyper-optimize the index
    size in exchange for recall. Unlike the scalar and binary quantization techniques,
    however, PQ provides levers for adjusting that trade-off by increasing either
    the number of subvectors (`M_subvectors`) or the `num_bits` to store more precision
    in the index and improve recall. For example, if you were to set `M_subvectors`
    to 64, you would get the following:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 你会立即注意到，PQ索引的召回率显著低于我们在33.33%召回率（未重排序）和68%召回率（重排序后）时基准测试的标量和二进制量化类型。然而，你也会注意到，索引大小比原始全精度索引小98.22%。显然，我们在这里有意做出了权衡，以超优化索引大小而牺牲召回率。然而，与标量和二进制量化技术不同，PQ提供了调整这种权衡的杠杆，通过增加子向量的数量（`M_subvectors`）或存储更多精度的`num_bits`来提高召回率。例如，如果你将`M_subvectors`设置为64，你会得到以下结果：
- en: '[PRE46]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: These results are now more on par with the binary quantization results. The
    key takeaway, then, is that the primary benefit of PQ is in the flexibility to
    control the trade-off between index size and recall, particularly when you need
    to significantly compress your embeddings.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果现在与二进制量化结果更加接近。因此，主要收获是PQ的主要优势在于控制索引大小和召回率之间的权衡的灵活性，尤其是在你需要显著压缩嵌入时。
- en: We’ve now explored several quantization approaches to reduce the size of the
    indexed embeddings and speed up search times while maintaining a very high level
    of recall relative to the compression level. In the next subsection, we’ll explore
    a different approach to tackling the compression versus recall trade-off by introducing
    embeddings that actually encode multiple levels of precision inside the original
    embedding representation.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了多种量化方法来减小索引嵌入的大小并加快搜索时间，同时保持相对于压缩水平的高召回率。在下一小节中，我们将通过引入实际上在原始嵌入表示中编码多个精度级别的嵌入来探索解决压缩与召回率权衡的不同方法。
- en: 13.7.4 Matryoshka Representation Learning
  id: totrans-445
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.7.4 马特罗什卡表示学习
- en: Matryoshka Representation Learning (MRL) is a novel approach to vector performance
    optimization that learns a hierarchical representation of a vector space, where
    multiple levels of precision are encoded by different ranges of dimensions within
    the vector space. This allows for flexible-length representations where shorter
    segments of an embedding can approximate the meaning of the full embedding, just
    with reduced precision. MRL is named after the Russian nesting dolls (Matryoshka
    dolls), signaling the “layers” of precision to be discovered inside of other layers.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 马特罗什卡表示学习（MRL）是一种新颖的向量性能优化方法，它学习向量空间的层次表示，其中多个精度级别由向量空间内不同范围的维度编码。这允许灵活长度的表示，其中嵌入的较短部分可以近似完整嵌入的意义，只是精度较低。MRL以俄罗斯套娃（马特罗什卡娃娃）命名，标志着要发现的其他层中的“层”精度。
- en: 'As a conceptual explanation of how MRL works, imagine that someone who’s never
    seen the animated Disney movie *The Lion King* asked you to describe it. If you’re
    familiar with the movie, you might start with a very high-level summary and then
    expand on your description if the person wants more details. For example, you
    could imagine the following possible responses:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 作为MRL工作原理的概念性解释，想象一下，如果有人从未看过迪士尼动画电影《狮子王》，让你描述它。如果你熟悉这部电影，你可能会从一个非常高级的总结开始，然后如果那个人想要更多细节，再扩展你的描述。例如，你可以想象以下可能的回答：
- en: It’s a Disney animated movie about a lion.
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一部关于狮子的迪士尼动画电影。
- en: It’s a Disney animated movie about a lion cub who grows up to become king.
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一部关于一只小狮子成长为国王的迪士尼动画电影。
- en: It’s a Disney animated movie about a lion cub named Simba who grows up to become
    king after his father is killed by his uncle.
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一部关于名叫辛巴的小狮子成长为国王的迪士尼动画电影，他的父亲被叔叔杀害。
- en: It’s a Disney animated movie about a lion cub named Simba who must discover
    his place in the circle of life after running away from his kingdom as a boy and
    returning as an adult to reclaim his throne from his uncle who killed his father.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一部关于名叫辛巴的小狮子成长为国王的迪士尼动画电影，他在作为男孩逃离王国后，成年归来，从杀害他父亲的叔叔那里夺回王位。
- en: Notice that these all give the main idea of the story at various levels of granularity.
    If it was really important that you provide the most accurate description, you’d
    go with the one with the most detail, but in reality, it can be more effective
    in some cases to start with the higher-level description and only provide additional
    levels of detail as needed.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些都在不同粒度级别上给出了故事的主要思想。如果你真的需要提供最准确描述，你会选择最详细的那个，但在现实中，在某些情况下，从更高层次的描述开始，并在需要时提供额外的详细级别可能更有效。
- en: This concept of hierarchical representations that reveal more about the subject
    at different hierarchical levels of granularity is the key idea behind MRL. This
    technique allows vectors to have progressive accuracy, meaning that the more of
    the vector you use, the more precise the representation becomes. To illustrate
    this, let’s consider figure 13.13, which shows the cover of this book at various
    pixelation levels.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在不同层次粒度级别上揭示更多关于主题的概念是MRL背后的关键思想。这种技术允许向量具有渐进的准确性，这意味着你使用的向量越多，表示就越精确。为了说明这一点，让我们考虑图13.13，它显示了这本书在不同像素化级别上的封面。
- en: '![figure](../Images/CH13_F13_Grainger.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH13_F13_Grainger.png)'
- en: Figure 13.13 Hierarchical representation levels balancing accuracy and compression
  id: totrans-455
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.13 层次表示级别平衡准确性和压缩
- en: As you can see, the image at the top left is highly pixelated, representing
    a very coarse approximation of the original cover. Moving to the right, and then
    to the next row, each subsequent image includes twice as many dimensions, eventually
    leading to a clear and detailed representation. These dimensions are *refinements*
    on the previous dimensions, however; they are not entirely new information. In
    other words, it’s possible to use a full vector, only the first half of the vector,
    only the first quarter of the vector, and so on, and still get an approximation
    of the original vector, just with less precision. This is similar to how MRL embeddings
    work.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，左上角的图像非常像素化，代表了原始封面的非常粗糙的近似。向右移动，然后到下一行，每个后续图像包含两倍多的维度，最终达到清晰和详细的表示。然而，这些维度是对先前维度的细化；它们并不是完全新的信息。换句话说，可以使用完整的向量，只使用向量的前半部分，只使用向量的前四分之一，等等，仍然可以得到原始向量的近似，只是精度较低。这与MRL嵌入的工作方式类似。
- en: Listing 13.26 demonstrates building and benchmarking a FAISS index using MRL
    embeddings. Note that because the lower-precision representations are achieved
    by simply cutting off later dimensions of the original embedding, no special indexing
    strategy is necessary for MRL embeddings. You simply need to reduce the number
    of dimensions of the dense vector field you are indexing and searching to the
    number of dimensions for the MRL embeddings you choose to use (cutting them in
    half each time and discarding the latter half).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.26 展示了使用 MRL 嵌入构建和基准测试 FAISS 索引。请注意，由于通过简单地截断原始嵌入的后续维度来实现低精度表示，因此对于 MRL
    嵌入不需要特殊的索引策略。你只需将你要索引和搜索的密集向量场的维度数减少到你选择的 MRL 嵌入的维度数（每次减半并丢弃后半部分）。
- en: Listing 13.26 Benchmarking MRL embeddings at different thresholds
  id: totrans-458
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 13.26 展示了在不同阈值下基准测试 MRL 嵌入
- en: '[PRE47]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '#1 Uses only the top num_dimensions dimensions'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 仅使用前 num_dimensions 维度'
- en: '#2 An MRL index is a standard index, just with a reduced number of dimensions.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 MRL 索引是一个标准索引，只是维度数减少了。'
- en: '#3 1024 dimensions'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 1024 维度'
- en: '#4 512 dimensions'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 512 维度'
- en: '#5 256 dimensions'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 256 维度'
- en: '#6 128 dimensions'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 128 维度'
- en: '#7 Benchmark MRL search'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 基准 MRL 搜索'
- en: '#8 Benchmark MRL search + reranking'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 基准 MRL 搜索 + 重新排序'
- en: 'Output:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE48]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the output, we see that 70.22% of the recall was encoded into the first 50%
    of the features of the embedding, 47.56% was encoded into the first 25% of the
    features, and 24.89% was encoded into the first 12.5% of the features. While this
    level of recall relative to the compression may not be as impressive as some of
    the earlier quantization approaches, the fact that it is built into the embedding
    representation to be used (or not used) whenever desired with no special indexing
    requirements provides some useful flexibility. Additionally, as we’ll see in the
    next subsection, MRL can also be combined with most of the other approaches.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，我们看到 70.22% 的召回率被编码到嵌入的前 50% 特征中，47.56% 被编码到前 25% 特征中，24.89% 被编码到前 12.5%
    特征中。虽然相对于压缩的这种召回率可能不如一些早期的量化方法那么令人印象深刻，但它在嵌入表示中内置，可以随时（或不用）使用，无需特殊索引要求，提供了一些有用的灵活性。此外，正如我们将在下一小节中看到的，MRL
    还可以与其他大多数方法结合。
- en: 13.7.5 Combining multiple vector search optimization approaches
  id: totrans-471
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.7.5 结合多个向量搜索优化方法
- en: 'In this chapter, we’ve discussed multiple methods to improve the efficiency
    of vector search:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了多种提高向量搜索效率的方法：
- en: ANN search to filter the number of documents that must be scored to those likely
    to score well
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ANN 搜索用于过滤必须评分的文档数量，使其更可能获得高分
- en: Quantization techniques to reduce the memory requirements and processing time
    for embeddings
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化技术以减少嵌入的内存需求和处理时间
- en: MRL to reduce the number of dimensions required to find an initial set of search
    results
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MRL 以减少找到初始搜索结果集所需的维度数
- en: Reranking to improve the recall and ranking of the top-*N* results after applying
    the other techniques more aggressively
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排序以在应用其他技术后更积极地提高召回率和排名
- en: 'In practice, you can often combine several of these techniques to achieve your
    desired level of performance. Listing 13.27 shows a final example combining each
    of these approaches in a single implementation: ANN using an inverted file index
    (IVF) for performance, binary quantization for performance and compression, MRL
    at 1/2 the original dimensionality for performance and compression, and reranking
    2 times the number of results to improve recall and relevance ranking.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你通常可以结合这些技术中的几个来达到你期望的性能水平。表 13.27 展示了一个最终示例，它在一个单一实现中结合了这些方法：使用倒排文件索引
    (IVF) 的 ANN 以提高性能，二进制量化以提高性能和压缩，MRL 在原始维度的一半进行性能和压缩，以及重新排序两倍的结果数量以提高召回率和相关性排名。
- en: Listing 13.27 Combining ANN, quantization, MRL, and reranking
  id: totrans-478
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 13.27 结合 ANN、量化、MRL 和重新排序
- en: '[PRE49]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '#1 Binary quantization: applies quantization to the doc embeddings'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 二进制量化：对文档嵌入应用量化'
- en: '#2 Configuration so the index knows how the doc embeddings have been quantized'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 配置，以便索引知道文档嵌入是如何量化的'
- en: '#3 ANN: uses a binary-quantized IVF index for ANN search'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 ANN：使用二进制量化的 IVF 索引进行 ANN 搜索'
- en: '#4 Trains, adds documents, and saves the combined index to disk'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 训练、添加文档并将组合索引保存到磁盘'
- en: '#5 MRL: gets reduced-dimension doc embeddings'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 MRL：获取降维文档嵌入'
- en: '#6 MRL: gets reduced-dimension query embeddings'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 MRL：获取降维查询嵌入'
- en: '#7 Binary quantization: applies quantization to the query embeddings'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 二进制量化：对查询嵌入应用量化'
- en: '#8 Benchmarks the binary ANN, binary quantization, and MRL embeddings'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 对二进制ANN、二进制量化和MRL嵌入进行基准测试'
- en: '#9 Benchmarks again with reranking using full-precision embeddings'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 再次使用全精度嵌入进行基准测试，使用重新排序'
- en: 'Output:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE50]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As you can see from the listing, the different embedding optimization approaches
    can be combined in complementary ways to achieve your desired balance between
    query speed, index compression and memory usage, and final relevance ranking.
    In this case, by combining ANN, MRL, binary quantization, and reranking, we were
    able to achieve by far the fastest search time (~99% faster than full-precision
    search) and by far the smallest index size (~98% reduction), while still maintaining
    over 72% recall after reranking.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表所示，不同的嵌入优化方法可以以互补的方式结合，以实现您在查询速度、索引压缩和内存使用以及最终相关性排名之间期望的平衡。在这种情况下，通过结合ANN、MRL、二进制量化和重新排序，我们实现了迄今为止最快的搜索时间（比全精度搜索快99%）和最小的索引大小（比全精度搜索小98%），同时在重新排序后仍保持超过72%的召回率。
- en: Using quantization in a supported engine
  id: totrans-492
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在支持的引擎中使用量化
- en: 'We’ve created quantized indexes using FAISS in order to ensure all the demonstrated
    ANN and quantization approaches can be easily reproduced for index size, search
    speed, and recall, independent of your configured default `engine`. Different
    search engines have varying levels of support for quantization approaches, with
    some performing quantization inside the engine and some requiring you to quantize
    outside of the engine and configure an appropriate quantization format (`FLOAT32`,
    `INT8`, `BINARY`, and so on). The `search` method on our `collection` interface
    implements support for scalar quantization and binary quantization using the latter
    approach by accepting a `quantization_size` parameter (previously demonstrated
    in listing 11.19), and MRL should be supported by any engine with vector search
    capabilities by truncating MRL embeddings prior to indexing and searching. Reranking
    is also supported by adding a `rerank_query` section to your search request. For
    example:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用FAISS创建了量化索引，以确保所有展示的ANN和量化方法都可以轻松地根据索引大小、搜索速度和召回率进行复制，而不依赖于您配置的默认`engine`。不同的搜索引擎对量化方法的支持程度不同，有的在引擎内部执行量化，有的则需要您在引擎外部进行量化并配置适当的量化格式（`FLOAT32`、`INT8`、`BINARY`等）。我们`collection`接口上的`search`方法通过接受一个`quantization_size`参数（在列表11.19中已展示）实现了对标量量化和二进制量化的支持，使用后者通过截断MRL嵌入在索引和搜索之前来实现。通过在您的搜索请求中添加`rerank_query`部分，也支持重新排序。例如：
- en: '[PRE51]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This example implements an initial binary quantized query (to whatever degree
    your engine supports it) and returns the top 25 results after reranking the top
    50 results using full-precision (`Float32`) embeddings.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例实现了一个初始的二进制量化查询（根据您的引擎支持的程度），并在使用全精度（`Float32`）嵌入重新排序前50个结果后返回前25个结果。
- en: 'Combining various ANN, scalar quantization, binary quantization, product quantization,
    and MRL techniques can significantly improve the efficiency of your vector search
    system. Many other techniques for quantization exist, and more are being developed
    all the time, but this overview should give you a great starting point if you’re
    looking to optimize your vector search usage. By experimenting with these techniques
    and combining them in different ways, you can optimize your search system to achieve
    the desired balance between speed, memory usage, and recall, especially when applying
    reranking as a final step. But reranking based on embeddings isn’t always the
    best way to get the most relevant final search results. In the next section, we’ll
    explore an often better way to perform the final reranking of top results: using
    a cross-encoder.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合各种ANN、标量量化、二进制量化、乘积量化和MRL技术，可以显著提高您的向量搜索系统的效率。存在许多其他量化技术，并且还在不断开发中，但这个概述应该为您提供一个很好的起点，如果您正在寻找优化向量搜索使用的方法。通过实验这些技术并以不同的方式组合它们，您可以优化您的搜索系统，以实现速度、内存使用和召回率之间的期望平衡，尤其是在将重新排序作为最终步骤应用时。但是，基于嵌入的重新排序并不总是获取最相关最终搜索结果的最佳方式。在下一节中，我们将探讨一种通常更好的方法来执行顶级结果的最终重新排序：使用交叉编码器。
- en: 13.8 Cross-encoders vs. bi-encoders
  id: totrans-497
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.8 交叉编码器与双编码器比较
- en: In this chapter, we’ve built semantic search using a Transformer-based encoder
    to generate embeddings. Our strategy involved finding the similarity between a
    query and document by separately encoding both of them into embeddings, and then
    using a cosine similarity to generate a relevance score based on the similarity
    of the two embeddings. An encoder like this that separately encodes each input
    into an embedding, so that those embeddings can be compared, is known as a *bi-encoder*.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用基于Transformer的编码器构建了语义搜索来生成嵌入。我们的策略涉及通过将查询和文档分别编码成嵌入来找到它们之间的相似度，然后使用余弦相似度根据两个嵌入的相似度生成一个相关性分数。这种将每个输入单独编码成嵌入以便进行比较的编码器被称为*双编码器*。
- en: In contrast with a bi-encoder, a *cross-encoder* encodes pairs of inputs together
    and returns a similarity score. Assuming the inputs are a query and a document,
    the cross-encoder will concatenate and then encode the query and document together,
    returning a similarity score measuring how well the document answers the query.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 与双编码器相比，*交叉编码器*将输入对一起编码并返回一个相似度分数。假设输入是查询和文档，交叉编码器将查询和文档连接起来并一起编码，返回一个衡量文档如何回答查询的相似度分数。
- en: Functionally, the cross-encoder is still generating a similarity score between
    the two inputs, but it can capture the shared context between the query and document
    in a way that a bi-encoder cannot. For example, a bi-encoder might place the query
    `mountain hike` near the document containing the text “first time snow hiking”
    because they are both related to a similar concept—hiking. But the cross-encoder
    would pass both the query and document to the encoder (a Transformer), which could
    then identify through its attention mechanism that while both inputs are about
    hiking, the document is about beginner snow hiking (which likely wouldn’t involve
    mountains the first time) instead of specifically being about the query of `mountain
    hike`. By using the context of the query to interpret the document, the cross-encoder
    can therefore reach a more nuanced interpretation of how well a document matches
    a query than a bi-encoder, which only interprets the query and document independently.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 从功能上讲，交叉编码器仍在生成两个输入之间的相似度分数，但它可以以双编码器无法实现的方式捕获查询和文档之间的共享上下文。例如，一个双编码器可能会将查询“登山远足”放置在包含文本“第一次雪地远足”的文档附近，因为它们都与一个类似的概念——远足相关。但交叉编码器会将查询和文档都传递给编码器（一个Transformer），然后通过其注意力机制识别出，虽然两个输入都是关于远足的，但文档是关于初学者雪地远足（可能第一次不会涉及山脉）而不是具体关于查询“登山远足”。通过使用查询的上下文来解释文档，交叉编码器因此可以对文档与查询匹配得有多好进行更细微的解释，而双编码器只能独立地解释查询和文档。
- en: Figure 13.14 visualizes the bi-encoder and cross-encoder architectures.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.14可视化了双编码器和交叉编码器的架构。
- en: '![figure](../Images/CH13_F14_Grainger.png)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH13_F14_Grainger.png)'
- en: Figure 13.14 Bi-encoders vs. cross-encoders. Bi-encoders process query and document
    inputs separately, whereas cross-encoders process them together into a similarity
    score.
  id: totrans-503
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.14 双编码器与交叉编码器。双编码器分别处理查询和文档输入，而交叉编码器将它们一起处理以生成相似度分数。
- en: 'In the figure, note the following key characteristics:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，请注意以下关键特征：
- en: Bi-encoders encode queries and documents separately into embeddings that can
    then be compared using a similarity function (like cosine).
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双编码器将查询和文档分别编码成可以用于相似度函数（如余弦）比较的嵌入。
- en: Cross-encoders encode queries together to assign a similarity score.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉编码器将查询一起编码以分配相似度分数。
- en: Solid-colored arrows indicate data processing that must occur at query time,
    whereas striped arrows indicate work that can be performed at index time and cached.
    Note that bi-encoders only need to encode the query once at query time, whereas
    cross-encoders must encode the query along with every document that needs a similarity
    score at query time.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实心箭头表示必须在查询时发生的数据处理，而条纹箭头表示可以在索引时执行并缓存的工作。请注意，双编码器只需要在查询时对查询进行一次编码，而交叉编码器必须在查询时对每个需要相似度分数的文档进行编码。
- en: Cross-encoders are more computationally expensive than bi-encoders at query
    time, but they are also usually more accurate than bi-encoders because they can
    capture the relevant interactions between the query and the document contexts.
    For this reason, cross-encoders are often used to rerank a small subset of the
    top results from a much faster bi-encoder-based vector search or lexical search
    to provide a more accurate similarity score for the top query and document pairs.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉编码器在查询时的计算成本比双编码器更高，但它们通常也比双编码器更准确，因为它们可以捕捉查询和文档上下文之间的相关交互。因此，交叉编码器通常用于从基于双编码器的向量搜索或词汇搜索的快速搜索中重新排序一小部分顶级结果，以提供更准确的顶级查询和文档对的相似度评分。
- en: Just like the learning-to-rank models we built in chapters 10–12, cross-encoders
    are another form of *ranking classifier*—a model that classifies inputs into probable
    similarity scores. Listing 13.28 demonstrates how to invoke a cross-encoder to
    rerank search results. We’ll use the same initial query from back in listing 13.19,
    but we’ll over-request the number of search results (`limit=50`), so we’ll provide
    more options for the cross-encoder to rerank.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在第10章到第12章中构建的学习到排名模型一样，交叉编码器是另一种形式的*排名分类器*——一种将输入分类为可能的相似度评分的模型。列表13.28展示了如何调用交叉编码器来重新排序搜索结果。我们将使用列表13.19中的相同初始查询，但我们将请求更多的搜索结果数量（`limit=50`），这样我们就可以为交叉编码器提供更多的重新排序选项。
- en: Listing 13.28 Reranking search results with a cross-encoder
  id: totrans-510
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.28 使用交叉编码器重新排序搜索结果
- en: '[PRE52]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '#1 Cross-encoder model trained on a similar dataset of questions and answers.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在类似的问题和答案数据集上训练的交叉编码器模型。'
- en: '#2 Generates a pair of query + document title to score for each document'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为每个文档生成一个查询+文档标题的配对进行评分'
- en: '#3 Over-requests 50 results to supply sufficient candidates to rerank'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 请求50个结果以提供足够的候选结果进行重新排序'
- en: '#4 Invokes the cross-encoder to score each pair'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 调用交叉编码器对每对进行评分'
- en: '#5 Optional activation function to normalize between 0 and 1'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 可选的激活函数，用于将评分归一化到0到1之间'
- en: '#6 Updates the relevance ranking based on the cross-encoder scores'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 根据交叉编码器的评分更新相关性排名'
- en: 'Response:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE53]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Though the scores are not cosine similarities (and thus not directly comparable
    with the bi-encoder scores), the quality of the results seems generally improved
    after applying the bi-encoder. Notice that more documents are now related to mountain
    hiking as opposed to hiking in general.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然评分不是余弦相似度（因此不能直接与双编码器评分进行比较），但应用双编码器后，结果的质量似乎普遍有所提高。注意，现在与一般徒步旅行相比，更多文档与登山有关。
- en: 'Taking this chapter as a whole, one common pattern for integrating cross-encoders
    with bi-encoders is as follows:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 以本章整体来看，将交叉编码器与双编码器集成的常见模式如下：
- en: Perform an initial search using a combination of ANN, quantization, and representation
    learning techniques (super fast).
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ANN、量化和表示学习技术的组合进行初始搜索（超级快）。
- en: Rerank a medium-sized number of results (hundreds or thousands) using *N* higher-precision
    vectors loaded from disk for only the top *N* results (fast, since the number
    results is reduced).
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用从磁盘加载的*N*个更高精度的向量重新排序中等数量的结果（数百或数千个），仅针对前*N*个结果（由于结果数量减少，因此速度快）。
- en: Take the top page or two of results, and rerank them with a cross-encoder to
    get the optimal ranking for the top results (slow, but only for a small number
    of results).
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取结果的前一页或两页，并使用交叉编码器重新排序以获得顶级结果的最佳排名（慢，但仅针对少量结果）。
- en: You can, of course, use any learning-to-rank model for your final ranking step.
    Cross-encoders are one of the most-deployed kinds of learning-to-rank model (focused
    solely on document content), as they don’t require explicitly modeled features
    and instead have learned to model language and content features automatically
    from a deep learning training process. You can (and should) certainly fine-tune
    your cross-encoder model, leveraging the techniques from chapters 10 and 11, but
    many people just use off-the-shelf pretrained cross-encoder models without fine-tuning
    them, because they tend to generalize well when dealing with general text content.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 你当然可以使用任何学习到排名模型进行最终的排名步骤。交叉编码器是最常部署的学习到排名模型之一（专注于文档内容），因为它们不需要显式建模的特征，而是从深度学习训练过程中自动学习语言和内容特征。你可以（并且应该）当然微调你的交叉编码器模型，利用第10章和第11章中的技术，但许多人只是使用现成的预训练交叉编码器模型而不进行微调，因为它们在处理通用文本内容时往往具有很好的泛化能力。
- en: 'With what you’ve learned in this chapter, you should now be able to do the
    following with your own content:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章所学的内容，你现在应该能够用你自己的内容做以下事情：
- en: Assess and choose an existing fine-tuned Transformer encoder model that matches
    your use case.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估和选择一个与您的用例相匹配的现有微调Transformer编码器模型。
- en: Encode important text from your documents, and add them to an embeddings index
    for ANN search.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对您文档中的重要文本进行编码，并将它们添加到嵌入索引中，以便进行ANN搜索。
- en: Build an autocomplete pipeline to accept plain text queries, and quickly return
    the most closely related concepts.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个自动补全管道，以接受纯文本查询，并快速返回最相关的概念。
- en: Add a powerful high-recall semantic search to your product.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的产品添加一个强大的高召回率语义搜索。
- en: Optimize the performance of your dense vector search system by combining ANN,
    quantization, MRL, and reranking techniques.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过结合ANN、量化、MRL和重排序技术，优化您密集向量搜索系统的性能。
- en: Rank results with a bi-encoder, and rerank those search results with a cross-encoder
    to improve search relevance ranking.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用双编码器对结果进行排序，并使用交叉编码器对这些搜索结果进行重排序，以提高搜索相关性排序。
- en: The technology underlying dense vector search still has room for improvement,
    because ranking on embeddings can be calculation-intensive, and ANN approaches
    have nontrivial scaling trade-offs. Sparse term vectors leveraging an inverted
    index are still much more efficient and simpler to scale. But tremendous forward
    progress continues to be made toward productionizing these dense vector search
    techniques, and for good reason. Not only does searching on vectors enable better
    semantic search on text, but it also enables cutting-edge approaches to question
    answering, results summarization, image search, and other advanced search use
    cases like retrieval augmented generation (RAG), all of which we’ll cover in the
    following chapters.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 密集向量搜索背后的技术仍有改进的空间，因为基于嵌入的排序可能计算密集，而ANN方法有非平凡的扩展权衡。利用倒排索引的稀疏项向量仍然更加高效且易于扩展。但向生产化这些密集向量搜索技术取得的巨大进步仍在继续，这是有充分理由的。不仅基于向量的搜索能够使文本上的语义搜索更好，而且它还使问答、结果摘要、图像搜索以及其他高级搜索用例（如检索增强生成（RAG））等前沿方法成为可能，所有这些内容我们将在接下来的章节中介绍。
- en: Summary
  id: totrans-534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Dense vector search ranks relevant documents by comparing the distance between
    embedding vectors, such as from large language models (LLMs).
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集向量搜索通过比较嵌入向量之间的距离（例如来自大型语言模型（LLMs））来对相关文档进行排序。
- en: Transformers enable LLMs to encode the meaning of content (queries, documents,
    sentences, etc.) into vectors and also to decode the meaning out of encoded vectors.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器使LLM能够将内容（查询、文档、句子等）的意义编码成向量，并且也能从编码的向量中解码意义。
- en: Semantic search and other use search cases like semantic autocomplete can be
    implemented using embeddings.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义搜索和其他使用案例，如语义自动补全，可以使用嵌入来实现。
- en: Approximate nearest-neighbor (ANN) search is a technique to speed up dense vector
    retrieval by filtering to documents containing similar vectors prior to performing
    expensive similarity calculations between the query and each document’s vectors.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近似最近邻（ANN）搜索是一种通过在执行查询与每个文档向量之间的昂贵相似度计算之前，过滤包含相似向量的文档来加速密集向量检索的技术。
- en: Dense vector search can be heavily optimized for search speed, memory usage,
    and recall of the best results by combining techniques like ANN search, quantization,
    Matryoshka Representation Learning (MRL) embeddings, and over-requesting and reranking
    of results.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稠密向量搜索可以通过结合诸如ANN搜索、量化、玛特罗什卡表示学习（MRL）嵌入以及结果的重请求和重排序等技术，在搜索速度、内存使用和最佳结果召回方面进行高度优化。
- en: Bi-encoders generate separate embeddings for queries and documents and support
    high-volume matching and ranking, whereas cross-encoders require much more computation
    at query time and are thus best used to rerank a smaller number of top results
    from a bi-encoder or lexical search.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双编码器为查询和文档生成单独的嵌入，并支持高容量匹配和排序，而交叉编码器在查询时需要更多的计算，因此最好用于从双编码器或词汇搜索中重排序少量顶级结果。
