- en: Chapter 14\. Using Third-Party Models and Hubs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章\. 使用第三方模型和中心
- en: The success of the open source PyTorch framework has led to the growth of supplementary
    ecosystems. In this chapter, we’ll look at the various options of pretrained models
    and the associated tools and resources used to download, instantiate, and use
    them for inference.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 开源PyTorch框架的成功导致了补充生态系统的增长。在本章中，我们将探讨预训练模型的多种选项以及用于下载、实例化和使用它们进行推理的相关工具和资源。
- en: While the PyTorch framework provides the foundation for deep learning, the community
    has created numerous repositories and hubs that store models that are ready to
    use and extend, making it easier for you to use and extend existing work rather
    than starting from scratch. I like to call this “standing on the shoulders of
    giants.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PyTorch框架为深度学习提供了基础，但社区已经创建了大量的存储库和中心，其中存储了可供使用和扩展的模型，这使得你更容易使用和扩展现有工作而不是从头开始。我将其称为“站在巨人的肩膀上”。
- en: Since the advent of generative AI, these hubs have exploded in popularity, and
    many scenarios of generative ML models within workflows have grown out of this.
    As a result, when it comes to using pretrained models, there are many options.
    You might use them directly for inference, taking advantage of those trained on
    massive datasets that would be impractical to replicate. Or you might use these
    models as starting points for fine-tuning, adapting them to specific domains or
    tasks while retaining their learned features. This can take the form of low-rank
    adaption (LoRA), as we’ll discuss in [Chapter 20](ch20.html#ch20_tuning_generative_image_models_with_lora_and_diffu_1748550104901965),
    or *transfer learning*, in which knowledge from one task is applied to another.
    Transfer learning or other fine-tuning has become a standard practice, especially
    when working with limited data or computational resources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自从生成式AI出现以来，这些中心在受欢迎程度上急剧增长，许多工作流程中生成式ML模型的场景都由此产生。因此，当涉及到使用预训练模型时，有许多选择。你可以直接使用它们进行推理，利用那些在大量数据集上训练的模型，这些数据集的复制是不切实际的。或者，你可以将这些模型作为微调的起点，将它们适应到特定的领域或任务中，同时保留它们学习到的特征。这可以采取低秩适应（LoRA）的形式，正如我们将在[第20章](ch20.html#ch20_tuning_generative_image_models_with_lora_and_diffu_1748550104901965)中讨论的那样，或者采取*迁移学习*的形式，其中将一个任务的知识应用于另一个任务。迁移学习或其他微调已成为一种标准做法，尤其是在处理有限的数据或计算资源时。
- en: The advantages of using pretrained models extend beyond saving computational
    resources and time. These models often represent state-of-the-art architectures,
    and they’ve been trained on diverse, high-quality datasets that you may not have
    direct access to.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型的优点不仅限于节省计算资源和时间。这些模型通常代表了最先进的架构，并且它们在多样化的、高质量的数据库上进行了训练，这些数据库你可能无法直接访问。
- en: Additionally, the providers generally release the model with extensive documentation,
    performance benchmarks, and community support, giving you a long head start. Given
    the importance of responsible AI, these models often come with model cards that
    help you understand any research and work done so you can navigate any potential
    responsibility issues.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，提供商通常还会发布包含详细文档、性能基准和社区支持的模型，这为你提供了长远的优势。鉴于负责任AI的重要性，这些模型通常附带模型卡，帮助你了解任何研究和工作，以便你能够导航任何潜在的责任问题。
- en: There is no “One Hub to Rule Them All,” so it’s useful to understand each of
    the major ones and how you can make the most of them. To that end, we’ll look
    at some of the more popular ones in this chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 没有所谓的“一统天下的中心”，因此了解每个主要中心以及如何最大限度地利用它们是有用的。为此，我们将在本章中探讨一些更受欢迎的中心。
- en: Hugging Face has become the de facto standard for transformer models, while
    PyTorch Hub offers officially supported implementations. Platforms like Kaggle
    provide competition-winning models, and GitHub-based TorchHub enables direct access
    to research implementations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face已成为Transformer模型的默认标准，而PyTorch Hub提供了官方支持的实施。像Kaggle这样的平台提供了获奖模型，基于GitHub的TorchHub允许直接访问研究实现。
- en: I think it’s important for you to understand these resources and how to use
    them effectively. As the field of deep learning continues to advance, these hubs
    play an increasingly crucial role in widening access to state-of-the-art models
    and enabling rapid development of AI applications. And as the role of AI developer
    matures and grows, I’m personally seeing huge growth in the careers of software
    developers who don’t train models from scratch and instead use or fine-tune existing
    ones. To that end, I hope this chapter helps you grow!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为了解这些资源以及如何有效地使用它们非常重要。随着深度学习领域的不断进步，这些中心在扩大对最先进模型的访问和促进AI应用的快速开发中发挥着越来越重要的作用。随着AI开发者角色的成熟和增长，我个人的观察是，那些不从头开始训练模型而是使用或微调现有模型的软件开发者的职业生涯正在迅速增长。为此，我希望这一章能帮助你成长！
- en: The Hugging Face Hub
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hugging Face Hub
- en: 'In recent years, particularly with the rise of generative AI, the Hugging Face
    Hub has emerged as a leading platform for discovering and using pretrained ML
    models, particularly for NLP. Much of its usefulness (and a significant driver
    of its success) is the open source availability of two things: a transformers
    library (which makes using pretrained language models very easy to use) and a
    diffusers library (which does the same for text-to-image generative models like
    stable diffusion).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，尤其是随着生成式AI的兴起，Hugging Face Hub已经成为发现和使用预训练ML模型的主要平台，尤其是在NLP领域。其大部分实用性（以及其成功的一个重要推动力）是两样开源事物的可用性：一个transformers库（它使得使用预训练语言模型变得非常容易）和一个diffusers库（它对文本到图像生成模型如稳定扩散做了同样的事情）。
- en: As a result, what started as a repository for transformer-based models has evolved
    into a comprehensive ecosystem supporting computer vision, audio processing, and
    reinforcement learning models. It has grown into a one-stop shop combining version
    control for models, documentation, and model cards—and because of the PyTorch-friendly
    libraries like transformers and diffusers, using these models with your Python
    and PyTorch skills is relatively easy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最初作为一个基于transformer模型的存储库，已经发展成为一个支持计算机视觉、音频处理和强化学习模型的综合生态系统。它已经成长为一个一站式商店，结合了模型的版本控制、文档和模型卡片——而且由于像transformers和diffusers这样的PyTorch友好型库，使用这些模型与你的Python和PyTorch技能相结合相对容易。
- en: Collaboration has also been one of the keys to the Hub’s success. You can download,
    use, and fine-tune models with just a few lines of code, and many developers and
    organizations have shared their models or fine-tunes with the community. There
    were over 900,000 publicly available models at the time of writing, so there’s
    plenty to choose from!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 协作也是Hub成功的关键之一。你只需用几行代码就可以下载、使用和微调模型，许多开发者和组织已经与社区分享了他们的模型或微调。在撰写本文时，有超过90万个公开可用的模型，所以选择余地很大！
- en: Using Hugging Face Hub
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Hugging Face Hub
- en: Before rolling up your sleeves to code with Hugging Face Hub, you should get
    an account and use it to get an API token.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在动手使用Hugging Face Hub编码之前，你应该先注册一个账户，并使用它来获取一个API令牌。
- en: Getting a Hugging Face token
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取Hugging Face令牌
- en: This section will walk you through the [*HuggingFace.co*](http://huggingface.co)
    user interface as it existed at the time of writing. It may have changed by the
    time you’re reading this, but the principles are still the same. Hopefully, they’ll
    still apply!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将指导你了解在撰写本文时存在的[*HuggingFace.co*](http://huggingface.co)用户界面。它可能在你阅读本文时已经改变，但原则仍然是相同的。希望它们仍然适用！
- en: Start by visiting [*Huggingface.co*](http://huggingface.co), and if you don’t
    already have an account, you can use the Sign Up button at the top right to create
    one (see [Figure 14-1](#ch14_figure_1_1748549787233053)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，访问[*Huggingface.co*](http://huggingface.co)，如果你还没有账户，可以在右上角的“注册”按钮创建一个（见[图14-1](#ch14_figure_1_1748549787233053)）。
- en: '![](assets/aiml_1401.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_1401.png](assets/aiml_1401.png)'
- en: Figure 14-1\. Signing up for Hugging Face
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-1\. 在Hugging Face注册
- en: Once you’ve signed up and gotten an account, you can sign in, and in the top-right-hand
    corner of the page, you’ll see your avatar icon. Select this and a drop-down menu
    will appear. On this menu, you’ll see an option to Access Tokens, and you can
    select it to view your access tokens (see [Figure 14-2](#ch14_figure_2_1748549787233102)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注册并获取账户后，你可以登录，在页面右上角你会看到你的头像图标。选择这个图标，将出现一个下拉菜单。在这个菜单中，你会看到一个访问令牌的选项，你可以选择它来查看你的访问令牌（见[图14-2](#ch14_figure_2_1748549787233102)）。
- en: '![](assets/aiml_1402.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_1402.png](assets/aiml_1402.png)'
- en: Figure 14-2\. Access tokens
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-2\. 访问令牌
- en: On this page, you’ll see a Create New Token button, which will take you to a
    screen where you can specify your token details. Select the Read tab and give
    the token a name. For example, in [Figure 14-3](#ch14_figure_3_1748549787233128),
    you can see where I created a new Read token called PyTorch Book.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在此页面上，您将看到一个创建新令牌按钮，它将带您到一个可以指定令牌详细信息的屏幕。选择读取选项卡，并为令牌命名。例如，在[图 14-3](#ch14_figure_3_1748549787233128)中，您可以看到我创建了一个名为
    PyTorch Book 的新读取令牌。
- en: You’ll also see a pop-up asking you to save your access token (see [Figure 14-4](#ch14_figure_4_1748549787233152)).
    Note that it tells you that you will not be able to see the token again after
    you close this dialog modal, so be sure to hit the Copy button to have the token
    ready for the next steps.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会看到一个弹出窗口要求您保存访问令牌（见[图 14-4](#ch14_figure_4_1748549787233152)）。请注意，它告诉您在关闭此对话框模态后您将无法再次看到令牌，因此请确保点击复制按钮，以便令牌为下一步做好准备。
- en: '![](assets/aiml_1403.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1403.png)'
- en: Figure 14-3\. Creating an access token
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-3\. 创建访问令牌
- en: '![](assets/aiml_1404.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1404.png)'
- en: Figure 14-4\. Saving your access token
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-4\. 保存您的访问令牌
- en: If you *do* forget the token, you’ll have to Invalidate and Refresh it on the
    token list screen. To do this, you select the three dots to the right of the token
    and then select Invalidate and Refresh from the drop-down menu (see [Figure 14-5](#ch14_figure_5_1748549787233175)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您*忘记*了令牌，您需要在令牌列表屏幕上将其作废并刷新。为此，您选择令牌右侧的三个点，然后从下拉菜单中选择作废和刷新（见[图 14-5](#ch14_figure_5_1748549787233175)）。
- en: '![](assets/aiml_1405.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1405.png)'
- en: Figure 14-5\. Invalidating and refreshing a token
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-5\. 作废和刷新令牌
- en: Then, go back to the dialog from [Figure 14-4](#ch14_figure_4_1748549787233152)
    with a new token value. Copy it if you want to use it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用新的令牌值回到[图 14-4](#ch14_figure_4_1748549787233152)中的对话框。如果您想使用它，请复制它。
- en: Now that you have a token, let’s explore how to configure Colab to use it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了令牌，让我们探索如何配置 Colab 以使用它。
- en: Getting permission to use models
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取使用模型的权限
- en: 'Many models on Hugging Face will require additional permission to use them.
    In those cases, you should always check the model page and apply for permission
    on the link provided. Your permission to use the model will be tracked using the
    Hugging Face token. If you do *not* have permission, you’ll see an error like
    this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 上的许多模型在使用时需要额外的权限。在这种情况下，您应始终检查模型页面，并通过提供的链接申请权限。您使用该模型的权限将通过 Hugging
    Face 令牌进行跟踪。如果您没有权限，您将看到如下错误：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When this happens, the easiest thing to do is use the model name to find its
    landing page on the Hugging Face Hub and follow the steps to get permission to
    use it from there.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当这种情况发生时，最简单的方法是使用模型名称在 Hugging Face Hub 上找到其着陆页，并遵循步骤从那里获取使用权限。
- en: Configuring Colab for a Hugging Face token
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为 Hugging Face 令牌配置 Colab
- en: If you want to use models from Hugging Face in Google Colab, then you need to
    configure a Colab secret in which code executing in Colab will read the token
    value, send it to Hugging Face on your behalf, and grant you access to the object.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在 Google Colab 中使用 Hugging Face 的模型，那么您需要配置一个 Colab 秘密，其中 Colab 中执行的代码将读取令牌值，代表您将其发送到
    Hugging Face，并授予您访问该对象的权限。
- en: It’s pretty easy to do. First, in Colab, you select the key icon on the left
    of the screen (see [Figure 14-6](#ch14_figure_6_1748549787233195)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常简单。首先，在 Colab 中，您选择屏幕左侧的键图标（见[图 14-6](#ch14_figure_6_1748549787233195)）。
- en: You should see a list of secrets that looks like the one in [Figure 14-7](#ch14_figure_7_1748549787233216).
    Don’t worry if you don’t have any API keys there yet. At the bottom of the list
    is a button that says Add new secret, and you’ll select that.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一个类似于[图 14-7](#ch14_figure_7_1748549787233216)中的秘密列表。如果您那里还没有任何 API 密钥，请不要担心。列表底部有一个按钮，上面写着添加新秘密，您将选择该按钮。
- en: '![](assets/aiml_1406.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1406.png)'
- en: Figure 14-6\. Selecting the Colab secrets
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-6\. 选择 Colab 秘密
- en: '![](assets/aiml_1407.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1407.png)'
- en: Figure 14-7\. List of Colab secrets
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-7\. Colab 秘密列表
- en: Use the name “HF_TOKEN” in the Name field, and paste the value of the key into
    the Value field. Then, flip the switch to give Notebook Access to the secret (see
    [Figure 14-8](#ch14_figure_8_1748549787233237)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在名称字段中使用“HF_TOKEN”，并将密钥的值粘贴到值字段中。然后，切换开关以授予秘密笔记本访问权限（见[图 14-8](#ch14_figure_8_1748549787233237)）。
- en: '![](assets/aiml_1408.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1408.png)'
- en: Figure 14-8\. Configuring the HF_TOKEN in Colab
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-8\. 在 Colab 中配置 HF_TOKEN
- en: Your code in Colab will now use this token to access Hugging Face.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您在 Colab 中的代码现在将使用此令牌访问 Hugging Face。
- en: Using the Hugging Face token in code
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在代码中使用 Hugging Face 标记
- en: If you just want to use the token directly in your code, whether in Colab or
    not, you’ll have to log in to the Hugging Face Hub in your code and pass the key
    to it. It’s pretty straightforward, with the Hugging Face Hub libraries providing
    the required support.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想直接在代码中使用标记，无论是在 Colab 中还是不在，你将需要在你的代码中登录到 Hugging Face Hub，并将密钥传递给它。这相当直接，Hugging
    Face Hub 库提供了所需的支持。
- en: 'To start, just import the `login` class like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，只需像这样导入 `login` 类：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can then pass the token to the `login` class and initialize by using it
    in your Python session like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以将标记传递给 `login` 类，并在你的 Python 会话中使用它来初始化，如下所示：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The Hugging Face classes will then use the token for the remainder of your session.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 的类将使用在整个会话中剩余的标记。
- en: Using a Model From Hugging Face Hub
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用来自 Hugging Face Hub 的模型
- en: 'Once you have your token set up, getting and using a model is very simple.
    For this walk-through, we’ll explore using a language model for text classification
    and sentiment analysis. This will require you to use the transformers library,
    so be sure to have it installed with this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你设置了标记，获取和使用模型就非常简单。在这个教程中，我们将探索使用语言模型进行文本分类和情感分析。这需要你使用 `transformers` 库，所以请确保你已经安装了它：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `transformers` API offers a pipeline class that lets you download and use
    a model based on its name in the Hugging Face repository. This one was fine-tuned
    using the SST sentiment analysis dataset from Stanford:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers` API 提供了一个 `pipeline` 类，允许你根据 Hugging Face 仓库中的名称下载和使用模型。这个模型是使用斯坦福大学的
    SST 情感分析数据集进行微调的：'
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pipeline offers much more than just downloading. It encapsulates what’s needed
    to perform common tasks with models. The first parameter, which in this case is
    `sentiment analysis`, describes the overall pipeline task that you’ll do. Transformers
    offer a variety of task types, including this, text classification, text generation,
    and a whole lot more.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 管道提供了不仅仅是下载的功能。它封装了执行模型常见任务所需的所有内容。第一个参数，在这个例子中是“情感分析”，描述了你将执行的整体管道任务。Transformers
    提供了各种任务类型，包括这个，文本分类，文本生成，等等。
- en: 'When using the `pipeline` class, a number of key steps take place under the
    hood. These include the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `pipeline` 类时，幕后发生了一系列关键步骤。以下包括：
- en: Tokenization
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化
- en: In this step, the text is converted into tokens (as we discussed in [Chapter 4](ch04.html#ch04_using_data_with_pytorch_1748548966496246)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，文本被转换为标记（正如我们在[第 4 章](ch04.html#ch04_using_data_with_pytorch_1748548966496246)中讨论的）。
- en: Input processing
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输入处理
- en: In this step, special tokens are added and the text is converted into tensors.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，添加了特殊标记，并将文本转换为张量。
- en: The model forward pass
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模型前向传递
- en: In this step, the tokenized input is passed through the model’s layers to get
    a result.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，标记化后的输入被传递到模型的层中，以获得结果。
- en: Output processing
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出处理
- en: In this step, the output is decoded from tensors back to the desired labels.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，输出将从张量解码回所需的标签。
- en: You can see this workflow in [Figure 14-9](#ch14_figure_9_1748549787233257).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 14-9](#ch14_figure_9_1748549787233257)中看到这个工作流程。
- en: '![](assets/aiml_1409.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_1409.png](assets/aiml_1409.png)'
- en: Figure 14-9\. NLP pipeline flow
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-9\. NLP 管道流程
- en: 'Then, when you want to use it, the burden of coding is removed from you as
    the developer and you just use the model like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当你想要使用它时，作为开发者的你将不再承担编码的负担，你只需像这样使用模型：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: All of the steps required for text classification and sentiment analysis are
    encapsulated and abstracted away from you. It makes your code much simpler!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所有用于文本分类和情感分析所需的步骤都被封装并从你这里抽象出来。这使得你的代码变得更加简单！
- en: Similarly, if you want to use the diffusers library, it comes with a number
    of pipelines that are often associated with a model type. So, for example, if
    you want to use the popular Stable Diffusion model for text to image—in which
    you give a prompt and the model will draw an image based on that prompt—you can
    do so very easily.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果你想使用 diffusers 库，它包含了许多与模型类型相关的管道。例如，如果你想使用流行的 Stable Diffusion 模型进行文本到图像——其中你给出一个提示，模型将根据该提示绘制图像——你可以非常容易地做到这一点。
- en: Let’s explore this with an example.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来探索这个问题。
- en: 'First, from the diffusers library, you can import the pipeline that supports
    Stable Diffusion like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可以从 diffusers 库中导入支持 Stable Diffusion 的管道，如下所示：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With this, you can specify the name of the model in the Hugging Face repository
    and use it to initialize the pipeline:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，你可以指定 Hugging Face 仓库中模型的名称，并使用它来初始化管道：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Similar to the preceding text example, the pipeline encapsulates and abstracts
    a number of steps away from you. This means you can write relatively simple code
    like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的文本示例类似，该流程封装并抽象出许多步骤，这意味着你可以编写相对简单的代码，如下所示：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'But a number of steps have been handled for you. These include the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但有许多步骤已经为你处理好了。以下是一些包括的内容：
- en: In the text encoding step, Stable Diffusion uses a technology called CLIP to
    take the text prompts and turn them into embeddings that the model can understand.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文本编码步骤中，Stable Diffusion 使用一种称为 CLIP 的技术将文本提示转换为模型可以理解的嵌入。
- en: An initial image is then constructed from random noise.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后从随机噪声中构建一个初始图像。
- en: The embeddings are then fed into the model, which uses a process of denoising
    to create pixels and features that match the embeddings.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将嵌入输入到模型中，该模型使用去噪过程创建与嵌入匹配的像素和特征。
- en: The final output of the model is then converted from tensors into an RGB image.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的最终输出随后被从张量转换为 RGB 图像。
- en: The overall process of creating an image from text is beyond the scope of this
    chapter, but it’s well explained in [this video from Google Research](https://oreil.ly/zNjUT).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本创建图像的整体过程超出了本章的范围，但在 [Google Research 的这个视频中](https://oreil.ly/zNjUT)有很好的解释。
- en: The important thing to note here is that because the image-generation process
    begins by creating random noise, any images you create with the preceding code
    will be different. So, don’t be alarmed if you’re not getting the same picture
    consistently! There are ways of guiding this noise by using a seed, which we’ll
    discuss in later chapters.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的重要一点是，由于图像生成过程是从创建随机噪声开始的，因此使用前面的代码创建的任何图像都将不同。所以，如果你没有得到一致的图片，请不要惊慌！我们可以通过使用种子来引导这种噪声，我们将在后面的章节中讨论这一点。
- en: PyTorch Hub
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch Hub
- en: One of the primary reasons for PyTorch’s success—particularly in the research
    community—is the foresight of the developers in creating a hub where people could
    share their models. While this functionality has been massively superseded by
    Hugging Face Hub, as described earlier in this chapter, it’s still worth looking
    at because many new and innovative models (or updates to existing ones) like YOLO
    are often shared on the Hub.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 成功的主要原因之一——尤其是在研究社区中——是开发者有远见地创建了一个共享模型的地方。虽然如本章前面所述，这个功能已被 Hugging
    Face Hub 大量取代，但它仍然值得一看，因为许多新的和创新的模型（或现有模型的更新）如 YOLO 经常在 Hub 上共享。
- en: Note
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: YOLO is “You Only Look Once,"” a popular and efficient object detection model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 是“你只看一次”，一个流行且高效的物体检测模型。
- en: As with Hugging Face Hub, the primary benefit of PyTorch Hub is that it gives
    you access either to models that you may not have the compute resources to train
    yourself or to the required data used to train them. At its core, PyTorch Hub
    functions as a centralized repository where researchers and developers can publish,
    share, and access models that have been trained on diverse datasets across various
    domains.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Hugging Face Hub 类似，PyTorch Hub 的主要好处是它为你提供了访问可能没有计算资源自己训练的模型或用于训练它们的所需数据的途径。在核心上，PyTorch
    Hub 是一个集中式仓库，研究人员和开发者可以在其中发布、共享和访问在各个领域不同数据集上训练的模型。
- en: In this section, we’ll explore PyTorch Hub and the APIs that you’ll use to access
    models within it. Unfortunately, the APIs aren’t as consistent as they could be,
    and it can sometimes be a little bit of a struggle to understand everything. But
    hopefully, this chapter will help!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨 PyTorch Hub 以及你将使用的 API 来访问其中的模型。不幸的是，这些 API 并不是完全一致的，有时理解所有内容可能有些挑战。但希望这一章能有所帮助！
- en: We’ll start with the PyTorch Vision libraries, which are composed of image classifiers,
    object detectors, and other computer vision models.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 PyTorch 视觉库开始，它由图像分类器、物体检测器和其他计算机视觉模型组成。
- en: Using PyTorch Vision Models
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PyTorch 视觉模型
- en: 'Before you begin, you’ll need to ensure that you have torchvision installed.
    Use this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，你需要确保你已经安装了 torchvision。使用以下命令：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: When you have installed it, you’ll see the install version. This is really important
    when using Hub, in particular when you want to list the models to see what’s available.
    You can also see the versions of these [on GitHub](https://oreil.ly/KIiFD).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，你会看到安装版本。在使用 Hub 时，这尤其重要，特别是当你想列出模型以查看可用性时。你还可以在 [GitHub](https://oreil.ly/KIiFD)
    上查看这些版本的详细信息。
- en: 'So, to list the models that are available, you’ll use code like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要列出可用的模型，你可以使用如下代码：
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note the version number (which you can get from the GitHub page we just mentioned).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意版本号（你可以从我们刚才提到的GitHub页面获取）。
- en: 'At the time of writing, there were close to a hundred models on this list.
    Do note that your version of the tag should match your version of torchvision,
    so if you are having problems, you can use this code to see your current version:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，这个列表上接近有一百个模型。请注意，你的标签版本应该与你的torchvision版本相匹配，所以如果你遇到问题，可以使用以下代码查看你的当前版本：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can also choose a model from those available and load it into memory like
    this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以从那些可用的模型中选择一个，并像这样将其加载到内存中：
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The model will be downloaded, cached, and then placed into evaluation mode.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将被下载、缓存，然后放入评估模式。
- en: Next up, you’ll need to get your data ready for inference, and that requires
    you to have some domain knowledge of the model. So, for example, in the code we
    just cited, we used `resnet50` as the model. This model (ResNet) is a very popular
    one for image classification that uses CNNs. A great place to go to learn more
    about this is the [PyTorch Hub site](https://pytorch.org/hub).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要准备好你的数据以便进行推理，这需要你对模型有一定的领域知识。例如，在我们刚才引用的代码中，我们使用了`resnet50`作为模型。这个模型（ResNet）是图像分类中一个非常流行的使用CNN的模型。了解更多关于这个模型的好地方是[PyTorch
    Hub网站](https://pytorch.org/hub)。
- en: 'From here, you can dig into model details—such as the size of the desired input,
    the labels that it can classify, etc. Then, with this information in hand, you
    can write inference code for the model. Here’s an example:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，你可以深入了解模型细节——例如所需的输入大小、它可以分类的标签等。然后，有了这些信息，你可以为模型编写推理代码。以下是一个例子：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You saw similar code in [Chapter 3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)
    and [Chapter 4](ch04.html#ch04_using_data_with_pytorch_1748548966496246), where
    you explored building your own image classifier. This code resizes the images
    to 256 × 256 and then crops a 224 × 224 image from the center of that. Images
    are typically 32 bit RGB, in which each pixel is represented by 8 bits of alpha,
    8 bits of red, 8 bits of green, and 8 bits of blue. However, for image classification,
    the neural network usually expects normalized values (i.e., between 0 and 1),
    so the transform to normalize the image performs this.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你在[第3章](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)和[第4章](ch04.html#ch04_using_data_with_pytorch_1748548966496246)中看到了类似的代码，在那里你探索了构建自己的图像分类器。这段代码将图像调整大小到256
    × 256，然后从中心裁剪一个224 × 224的图像。图像通常是32位RGB，其中每个像素由8位的alpha、8位的红色、8位的绿色和8位的蓝色表示。然而，对于图像分类，神经网络通常期望归一化的值（即介于0和1之间），因此归一化图像的转换执行此操作。
- en: When you’re using models in PyTorch, even though you know the desired dimensions
    (in this case, 224 × 224), you’ll also need to batch the images for inference,
    even if you’re just doing a single image. The `input_tensor.unsqueeze(0)` adds
    this extra dimension to the input tensor to handle this.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在PyTorch中使用模型时，即使你知道所需的维度（在这种情况下，224 × 224），你也需要对图像进行批处理以进行推理，即使你只处理一张图像。`input_tensor.unsqueeze(0)`向输入张量添加这个额外的维度来处理这个问题。
- en: 'Next up, you’ll do the actual inference, which just means you’ll load the model
    onto the appropriate device—which is cuda if you have a GPU and the CPU otherwise.
    You’ll then pass the input batch to the model to get an output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将进行实际的推理，这仅仅意味着你将加载模型到适当的设备上——如果你有GPU，则是cuda，否则是CPU。然后，你将输入批次传递给模型以获取输出：
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `predicted_idx` is the output for the class that has the highest probability
    of matching the input image. You’ll see something numeric, like `tensor([153])`,
    in the output here. Recall that models’ output layers will be neurons that correspond
    to the index of the required label. In the case of ResNet, the number 153 is a
    Maltese dog.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`predicted_idx`是具有最高匹配输入图像概率的类的输出。你在这里会看到类似`tensor([153])`的数字。回想一下，模型的输出层将是与所需标签索引相对应的神经元。在ResNet的情况下，数字153代表一只马耳他狗。'
- en: 'To decode this, you can use code like the following. You can find the URL of
    the labels file by digging into the model page on PyTorch Hub:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要解码这个，你可以使用以下代码。你可以通过挖掘PyTorch Hub上的模型页面来找到标签文件的URL：
- en: '[PRE15] `class_labels` `=` `json``.``load``(``urllib``.``request``.``urlopen``(``url``))`
    `predicted_label` `=` `class_labels``[``predicted_idx``]` `print``(``"Predicted
    Label:"``,` `predicted_label``)` [PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE15] `class_labels` `=` `json``.``load``(``urllib``.``request``.``urlopen``(``url``))`
    `predicted_label` `=` `class_labels``[``predicted_idx``]` `print``(``"Predicted
    Label:"``,` `predicted_label``)` [PRE16]'
- en: '[PRE17]  [PRE18]`## Natural Language Processing    The PyTorch Hub for NLP
    ultimately directs to two different repositories: those implemented using [Hugging
    Face Transformers](https://oreil.ly/j1w3-) as outlined earlier in this chapter
    and those from [Facebook’s fairseq research team](https://oreil.ly/mmQeY).    If
    you use the fairseq models, you may encounter a lot of sharp edges. Therefore,
    I thoroughly recommend setting up an environment with Python 3.11 (no later than
    that).    Within that, you can set up fairseq2 like this:    [PRE19]    You’ll
    likely also need other dependencies like hydra-core, OmegaConf, and requests.    Once
    you have the full system set up, you can use fairseq models like this:    [PRE20]    Do
    note that the environment will be very picky about which versions of PyTorch,
    pip, and many other libraries you can use. It can make for a very brittle experience,
    and unless you really want to use the models from the fairseq repository, I’d
    recommend just going with the Hugging Face transformer versions.    ## Other Models    PyTorch
    Hub also has repos for a variety of other model types, including audio, reinforcement
    learning, generative AI, and more. I’ve found the best way to explore them is
    to browse at the [PyTorch Hub](https://pytorch.org/hub) and use the links on the
    landing pages to navigate to the requisite GitHub.[PRE21]``  `` `# Summary    This
    chapter explores the ecosystem of pretrained models and model repositories for
    PyTorch, focusing on two major platforms: Hugging Face Hub and PyTorch Hub. While
    PyTorch Hub was the granddaddy that started the ball rolling, Hugging Face Hub
    has rapidly taken over as the go-to resource for pretrained models.    We took
    a look at how to use the transformers and diffusers libraries from Hugging Face,
    which encapsulate model loading and instantiation. With these, you have the keys
    to over 900,000 publicly available models. As a bonus, many of these have comprehensive
    documentation and model cards to get you up and running quickly and responsibly.
    You also got hands-on with using them, including setting up your account and getting
    authentication from Hugging Face using tokens.    Hugging Face APIs offer pipelines
    that encapsulate many of the common tasks of using models, such as tokenization
    and sequencing for NLP under the hood, making your coding surface much easier.
    We explored these with a text sentiment analysis scenario as well as another for
    image classification.    While PyTorch Hub has a lot less in it and accessing
    the models can be brittle in comparison, it’s worth looking at because it’s still
    well used in the research community. We looked at how to access PyTorch Vision
    models, prepare data for inference, and handle model outputs. The Hub also includes
    practical examples of using pretrained models like ResNet50 for image classification.    Ultimately,
    you should consider the advantages of using pretrained models, which have been
    built by expert researchers who have used expensive hardware and high-quality
    datasets that you may not otherwise have access to. To that end, you may find
    that using and fine-tuning existing models rather than training from scratch might
    be better for your scenario. We’re going to explore that over the next few chapters,
    starting with [Chapter 15](ch15.html#ch15_transformers_and_transformers_1748549808974580),
    where we will go deeper into using LLMs with Hugging Face Transformers.` ``'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE17]  [PRE18]`## 自然语言处理    PyTorch Hub for NLP 最终指向两个不同的仓库：那些使用 [Hugging
    Face Transformers](https://oreil.ly/j1w3-) 实现的，如本章前面所述，以及来自 [Facebook 的 fairseq
    研究团队](https://oreil.ly/mmQeY) 的。    如果你使用 fairseq 模型，可能会遇到很多尖锐的边缘。因此，我强烈建议设置一个
    Python 3.11（不晚于该版本）的环境。    在其中，你可以这样设置 fairseq2：    [PRE19]    你可能还需要其他依赖项，如 hydra-core、OmegaConf
    和 requests。    一旦你设置了完整的系统，你就可以这样使用 fairseq 模型：    [PRE20]    请注意，环境对你可以使用的 PyTorch、pip
    以及许多其他库的版本非常挑剔。这可能会造成非常脆弱的体验，除非你真的想使用 fairseq 仓库中的模型，否则我建议只使用 Hugging Face transformer
    版本。    ## 其他模型    PyTorch Hub 还为各种其他模型类型提供了仓库，包括音频、强化学习、生成式 AI 等。我发现探索它们最好的方法是浏览
    [PyTorch Hub](https://pytorch.org/hub)，并使用首页上的链接导航到必要的 GitHub。[PRE21]``  `` `#
    摘要    本章探讨了 PyTorch 的预训练模型和模型仓库生态系统，重点关注两个主要平台：Hugging Face Hub 和 PyTorch Hub。虽然
    PyTorch Hub 是开创性的，但 Hugging Face Hub 已迅速成为预训练模型的首选资源。    我们探讨了如何使用 Hugging Face
    的 transformers 和 diffusers 库，这些库封装了模型加载和实例化。有了这些，你就有超过 900,000 个公开可用的模型的钥匙。作为额外的好处，其中许多都有全面的文档和模型卡片，可以帮助你快速、负责任地开始使用。你还可以亲身体验使用它们，包括设置你的账户并使用令牌从
    Hugging Face 获取认证。    Hugging Face API 提供了封装了许多模型使用常见任务的管道，例如 NLP 内部的标记化和序列化，这使得你的编码表面变得更加容易。我们通过文本情感分析场景以及图像分类场景探索了这些管道。    虽然
    PyTorch Hub 中包含的内容较少，并且与访问模型相比可能比较脆弱，但它仍然在研究社区中得到广泛使用。我们探讨了如何访问 PyTorch Vision
    模型，为推理准备数据，以及处理模型输出。该 Hub 还包括使用预训练模型（如 ResNet50）进行图像分类的实用示例。    最终，你应该考虑使用预训练模型的优点，这些模型是由专家研究人员构建的，他们使用了你可能无法获得的昂贵硬件和高质量数据集。为此，你可能发现使用和微调现有模型而不是从头开始训练可能更适合你的场景。我们将在接下来的几章中探讨这一点，从
    [第 15 章](ch15.html#ch15_transformers_and_transformers_1748549808974580) 开始，我们将更深入地探讨使用
    Hugging Face Transformers 的 LLM。` ``'
