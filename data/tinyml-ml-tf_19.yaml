- en: Chapter 19\. Porting Models from TensorFlow to TensorFlow Lite
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第19章。将模型从TensorFlow迁移到TensorFlow Lite
- en: If you’ve made it this far, you’ll understand that we’re in favor of reusing
    existing models for new tasks whenever you can. Training an entirely new model
    from scratch can take a lot of time and experimentation, and even experts often
    can’t predict the best approach ahead of time without trying a lot of different
    prototypes. This means that a full guide to creating new architectures is beyond
    the scope of this book, and we recommend looking in [Chapter 21](ch21.xhtml#ch21)
    for further reading on the topic. There are some aspects (like working with a
    restricted set of operations or preprocessing demands) that are unique to resource-constrained,
    on-device machine learning, though, so this chapter offers advice on those.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经走到这一步，你会明白我们倡导在新任务中尽可能重用现有模型。从头开始训练一个全新的模型可能需要大量时间和实验，即使是专家也经常无法在尝试许多不同的原型之前预测最佳方法。这意味着创建新架构的完整指南超出了本书的范围，我们建议查看[第21章](ch21.xhtml#ch21)以获取更多相关信息。然而，有一些方面（如使用受限操作集或预处理需求）是独特于资源受限、设备端机器学习的，因此本章提供了关于这些方面的建议。
- en: Understand What Ops Are Needed
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解需要哪些操作
- en: This book is focused on models created in TensorFlow because the authors work
    on the team at Google, but even within a single framework there are a lot of different
    ways of creating models. If you look at [the speech commands training script](https://oreil.ly/ZTYu7),
    you’ll see that it’s building a model using core TensorFlow ops directly as building
    blocks, and manually running a training loop. This is quite an old-fashioned way
    of working these days (the script was originally written in 2017), and modern
    examples with TensorFlow 2.0 are likely to use Keras as a high-level API that
    takes care of a lot of the details.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书侧重于在TensorFlow中创建的模型，因为作者在Google团队工作，但即使在一个框架内，创建模型的方式有很多不同。如果你查看[语音命令训练脚本](https://oreil.ly/ZTYu7)，你会看到它直接使用核心TensorFlow操作构建模型，并手动运行训练循环。这在当今是一种相当老式的工作方式（该脚本最初是在2017年编写的），而使用TensorFlow
    2.0的现代示例可能会使用Keras作为一个高级API，它会处理很多细节。
- en: 'The downside to this is that the underlying operations that a model uses are
    no longer obvious from inspecting the code. Instead, they will be created as part
    of layers which represent larger chunks of the graph in a single call. This is
    a problem because knowing what TensorFlow operations are being used by a model
    is very important for understanding whether the model will run in TensorFlow Lite,
    and what the resource requirements will be. Luckily you can access the underlying
    low-level operations even from Keras, as long as you can retrieve the underlying
    `Session` object using [`tf.keras.backend.get_session()`](https://oreil.ly/4zurk).
    If you’re coding directly in TensorFlow, it’s likely that you already have the
    session in a variable, so the following code should still work:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的缺点是，从检查代码中不再明显地了解模型使用的底层操作。相反，它们将作为层的一部分被创建，这些层代表图中的较大块在一个调用中。这是一个问题，因为了解模型使用了哪些TensorFlow操作对于理解模型是否能在TensorFlow
    Lite中运行以及资源需求是非常重要的。幸运的是，即使从Keras中，只要可以使用[`tf.keras.backend.get_session()`](https://oreil.ly/4zurk)检索底层的`Session`对象，你仍然可以访问底层的低级操作。如果你直接在TensorFlow中编码，很可能已经将会话存储在一个变量中，所以下面的代码仍然有效：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you’ve assigned your session to the `sess` variable, this will print out
    the types of all the ops in your model. You can also access other properties,
    like `name`, to get more information. Understanding what TensorFlow operations
    are present will help a lot in the conversion process to TensorFlow Lite; otherwise,
    any errors you see will be much more difficult to understand.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将会话分配给了`sess`变量，这将打印出模型中所有操作的类型。你也可以访问其他属性，比如`name`，以获取更多信息。了解TensorFlow操作的存在将有助于在转换过程中到TensorFlow
    Lite时；否则，你看到的任何错误将更难理解。
- en: Look at Existing Op Coverage in Tensorflow Lite
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看Tensorflow Lite中现有操作的覆盖范围
- en: TensorFlow Lite supports only a subset of TensorFlow’s operations, and with
    some restrictions. You can see the latest list in [the ops compatibility guide](https://oreil.ly/Pix9U).
    This means that if you’re planning a new model, you should ensure at the outset
    that you aren’t relying on features or ops that aren’t supported. In particular,
    LSTMs, GRUs, and other recurrent neural networks are not yet usable. There’s also
    currently a gap between what’s available in the full mobile version of TensorFlow
    Lite and the microcontroller branch. The simplest way to understand what operations
    are supported by TensorFlow Lite for Microcontrollers at the moment is to look
    at [*all_ops_resolver.cc*](https://oreil.ly/HNpmM), because ops are constantly
    being added.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite仅支持TensorFlow的一部分操作，并且有一些限制。你可以在[操作兼容性指南](https://oreil.ly/Pix9U)中查看最新列表。这意味着如果你计划创建一个新模型，你应该确保一开始就不依赖于不受支持的功能或操作。特别是，LSTMs、GRUs和其他递归神经网络目前还不能使用。目前在完整的移动版本TensorFlow
    Lite和微控制器分支之间存在差距。了解当前TensorFlow Lite for Microcontrollers支持哪些操作的最简单方法是查看[*all_ops_resolver.cc*](https://oreil.ly/HNpmM)，因为操作不断被添加。
- en: It can become a bit confusing comparing the ops that show up in your TensorFlow
    training session and those supported by TensorFlow Lite, because there are several
    transformation steps that take place during the export process. These turn weights
    that were stored as variables into constants, for example, and might quantize
    float operations into their integer equivalents as an optimization. There are
    also ops that exist only as part of the training loop, like those involved in
    backpropagation, and these are stripped out entirely. The best way to figure out
    what issues you might encounter is to try exporting a prospective model as soon
    as you’ve created it, before it’s trained, so that you can adjust its structure
    before you’ve spent a lot of time on the training process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 训练会话中显示的操作与 TensorFlow Lite 支持的操作进行比较可能会有点混淆，因为在导出过程中会发生几个转换步骤。例如，这些步骤将存储为变量的权重转换为常量，并可能将浮点操作量化为其整数等效项以进行优化。还有一些仅作为训练循环的一部分存在的操作，比如参与反向传播的操作，这些操作将被完全剥离。找出可能遇到的问题的最佳方法是在创建模型后立即尝试导出潜在模型，而不是在训练之前，这样您就可以在花费大量时间进行训练之前调整其结构。
- en: Move Preprocessing and Postprocessing into Application Code
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将预处理和后处理移入应用代码
- en: It’s common for deep learning models to have three stages. There’s often a preprocessing
    step, which might be as simple as loading images and labels from disk and decoding
    the JPEGs, or as complex as the speech example which transforms audio data into
    spectrograms. There’s then a core neural network that takes in arrays of values
    and outputs results in a similar form. Finally, you need to make sense of these
    values in a postprocessing step. For many classification problems this is as simple
    as matching scores in a vector to the corresponding labels, but if you look at
    a model like [MobileSSD](https://oreil.ly/QT_dS), the network output is a soup
    of overlapping bounding boxes that need to go through a complex process called
    “non-max suppression” to be useful as results.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通常有三个阶段。通常有一个预处理步骤，可能只是从磁盘加载图像和标签并解码 JPEG，或者像将音频数据转换为频谱图这样复杂的语音示例。然后是一个核心神经网络，它接收值数组并以类似形式输出结果。最后，您需要在后处理步骤中理解这些值。对于许多分类问题，这只是将向量中的分数与相应的标签进行匹配，但是如果看一下像
    [MobileSSD](https://oreil.ly/QT_dS) 这样的模型，网络输出是一堆重叠的边界框，需要经过一个称为“非最大抑制”的复杂过程才能作为结果有用。
- en: The core neural network model is usually the most computationally intensive,
    and is often composed of a comparatively small number of operations like convolutions
    and activations. The pre- and postprocessing stages frequently require a lot more
    operations, including control flow, even though their computational load is a
    lot lower. This means that it often makes more sense to implement the non-core
    steps as regular code in the application, rather than baking them into the TensorFlow
    Lite model. For example, the neural network portion of a machine vision model
    will take in an image of a particular size, like 224 pixels high by 224 pixels
    wide. In the training environment, we’ll use a `DecodeJpeg` op followed by a `ResizeImages`
    operation to convert the result into the correct size. When we’re running on a
    device, however, we’re almost certainly grabbing input images from a fixed-size
    source with no decompression required, so writing custom code to create the neural
    network input makes a lot more sense than relying on a general-purpose operation
    from our library. We’ll probably also be dealing with asynchronous capture and
    might be able to get some benefits from threading the work involved. In the case
    of speech commands, we do a lot of work to cache intermediate results from the
    FFT so that we can reuse as many calculations as possible as we’re running on
    streaming input.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 核心神经网络模型通常是计算量最大的部分，通常由相对较少的操作组成，如卷积和激活。预处理和后处理阶段通常需要更多的操作，包括控制流，尽管它们的计算负载要低得多。这意味着通常更合理的做法是将非核心步骤作为应用中的常规代码实现，而不是将它们嵌入到
    TensorFlow Lite 模型中。例如，机器视觉模型的神经网络部分将接收特定尺寸的图像，如高224像素，宽224像素。在训练环境中，我们将使用 `DecodeJpeg`
    操作，然后是 `ResizeImages` 操作将结果转换为正确的尺寸。然而，在设备上运行时，我们几乎肯定是从固定大小的源中获取输入图像，无需解压缩，因此编写自定义代码来创建神经网络输入比依赖库中的通用操作更有意义。我们可能还需要处理异步捕获，并可能从线程化所涉及的工作中获得一些好处。在语音命令的情况下，我们会做很多工作来缓存
    FFT 的中间结果，以便在流式输入运行时尽可能重用尽可能多的计算。
- en: Not every model has a significant postprocessing stage in the training environment,
    but when we’re running on a device, it’s very common to want to take advantage
    of coherency over time to improve the results shown to the user. Even though the
    model is just a classifier, the wake-word detection code runs multiple times a
    second and [uses averaging](https://oreil.ly/E68Q4) to increase the accuracy of
    the results. This sort of code is also best implemented at the application level,
    given that expressing it as TensorFlow Lite operations is difficult and doesn’t
    offer many benefits. It is possible, as you can see in [*detection_postprocess.cc*](https://oreil.ly/IMlsT),
    but it involves a lot of work wiring through from the underlying TensorFlow graph
    during the export process because the way it’s typically expressed as small ops
    in the TensorFlow is not an efficient way to implement it on-device.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个模型在训练环境中都有显著的后处理阶段，但是在设备上运行时，通常希望利用随时间的连贯性来改善向用户显示的结果。即使模型只是一个分类器，唤醒词检测代码每秒运行多次并且
    [使用平均值](https://oreil.ly/E68Q4) 来提高结果的准确性是非常常见的。这种代码最好在应用级别实现，因为将其表达为 TensorFlow
    Lite 操作很困难，并且并不提供太多好处。虽然可能会看到在 [*detection_postprocess.cc*](https://oreil.ly/IMlsT)
    中，但是这需要在导出过程中从底层 TensorFlow 图中进行大量工作的连接，因为通常表达为 TensorFlow 中的小操作并不是在设备上实现它的有效方式。
- en: This all means that you should try to exclude non-core parts of the graph, which
    will require some work determining what parts are which. We find [Netron](https://oreil.ly/qoQNY)
    to be a good tool for exploring TensorFlow Lite graphs to understand what ops
    are present, and get a sense for whether they’re part of the core of the neural
    network or just processing steps. Once you understand what is happening internally,
    you should be able to isolate the core network, export just those ops, and implement
    the rest as application code.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着您应该尝试排除图中的非核心部分，这将需要一些工作来确定哪些部分是哪些。我们发现[Netron](https://oreil.ly/qoQNY)是一个很好的工具，可以用来探索
    TensorFlow Lite 图，了解存在哪些操作，并了解它们是神经网络的核心部分还是仅仅是处理步骤。一旦了解内部发生的情况，您应该能够隔离核心网络，仅导出这些操作，并将其余部分实现为应用程序代码。
- en: Implement Required Ops if Necessary
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 必要时实现所需操作
- en: 'If you do find that there are TensorFlow operations that you absolutely need
    that are not supported by TensorFlow Lite, it is possible to save them as *custom*
    operations inside the TensorFlow Lite file format, and then implement them yourself
    within the framework. The full process is beyond the scope of this book, but here
    are the key steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现有一些您绝对需要的 TensorFlow 操作在 TensorFlow Lite 中不受支持，那么可以将它们保存为 TensorFlow Lite
    文件格式中的 *自定义* 操作，然后在框架内自行实现。完整的过程超出了本书的范围，但以下是关键步骤：
- en: Run `toco` with `allow_custom_ops` enabled, so that unsupported operations are
    stored as custom ops in the serialized model file.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用启用 `allow_custom_ops` 的 `toco` 运行，以便将不受支持的操作存储为序列化模型文件中的自定义操作。
- en: Write a kernel implementing the operation and register it using `AddCustom()`
    in the op resolver you’re using in your application.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写实现操作的内核，并在您的应用程序中使用的 op 解析器中使用 `AddCustom()` 进行注册。
- en: Unpack the parameters that are stored in a FlexBuffer format when your `Init()`
    method is called.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在调用 `Init()` 方法时，解压存储在 FlexBuffer 格式中的参数。
- en: Optimize Ops
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化操作
- en: Even if you’re using supported operations in your new model, you might be using
    them in a way that hasn’t yet been optimized. The TensorFlow Lite team’s priorities
    are driven by particular use cases, so if you are running a new model, you might
    run into code paths that haven’t been optimized yet. We covered this in [Chapter 15](ch15.xhtml#optimizing_latency),
    but just as we recommend you check export compatibility as soon as possible—even
    before you’ve trained a model—it’s worth ensuring that you get the performance
    you need before you plan your development schedule, because you might need to
    budget some time to work on operation latency.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您在新模型中使用了受支持的操作，您可能以尚未优化的方式使用它们。TensorFlow Lite 团队的优先事项受特定用例驱动，因此如果您正在运行一个新模型，可能会遇到尚未优化的代码路径。我们在[第15章](ch15.xhtml#optimizing_latency)中讨论了这一点，但正如我们建议您尽快检查导出兼容性一样——甚至在训练模型之前——确保在计划开发时间表之前获得所需的性能是值得的，因为您可能需要预留一些时间来处理操作延迟。
- en: Wrapping Up
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Training a novel neural network to complete a task successfully is already challenging,
    but figuring out how to build a network that will produce good results and run
    efficiently on embedded hardware is even tougher! This chapter discussed some
    of the challenges you’ll face, and provided suggestions on approaches to overcome
    them, but it’s a large and growing area of study, so we recommend taking a look
    at some of the resources in [Chapter 21](ch21.xhtml#ch21) to see whether there
    are new sources of inspiration for your model architecture. In particular, this
    is an area where following the latest research papers on arXiv can be very useful.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个新颖的神经网络以成功完成任务本身就具有挑战性，但要想构建一个能够产生良好结果并在嵌入式硬件上高效运行的网络更加困难！本章讨论了您将面临的一些挑战，并提供了克服这些挑战的方法建议，但这是一个庞大且不断增长的研究领域，因此我们建议查看[第21章](ch21.xhtml#ch21)中的一些资源，看看是否有新的灵感来源可以用于您的模型架构。特别是，在这个领域，跟踪
    arXiv 上最新的研究论文可能非常有用。
- en: After overcoming all these challenges, you should have a small, fast, power-efficient
    product that’s ready to be deployed in the real world. It’s worth thinking about
    what potentially harmful impacts it could have on your users before you release
    it, though, so [Chapter 20](ch20.xhtml#ch20) covers questions around privacy and
    security.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 克服所有这些挑战后，您应该拥有一个小巧、快速、节能的产品，可以随时部署到现实世界中。在发布之前，值得考虑一下它可能对用户造成的潜在有害影响，因此[第20章](ch20.xhtml#ch20)涵盖了围绕隐私和安全的问题。
