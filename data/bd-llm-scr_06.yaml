- en: 7 Fine-tuning to follow instructions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 微调以遵循指令
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The instruction fine-tuning process of LLMs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的指令微调过程
- en: Preparing a dataset for supervised instruction fine-tuning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备用于监督指令微调的数据集
- en: Organizing instruction data in training batches
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织训练批次中的指令数据
- en: Loading a pretrained LLM and fine-tuning it to follow human instructions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载预训练的LLM并将其微调以遵循人类指令
- en: Extracting LLM-generated instruction responses for evaluation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取用于评估的LLM生成的指令响应
- en: Evaluating an instruction-fine-tuned LLM
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估指令微调的LLM
- en: 'Previously, we implemented the LLM architecture, carried out pretraining, and
    imported pretrained weights from external sources into our model. Then, we focused
    on fine-tuning our LLM for a specific classification task: distinguishing between
    spam and non-spam text messages. Now we’ll implement the process for fine-tuning
    an LLM to follow human instructions, as illustrated in figure 7.1\. Instruction
    fine-tuning is one of the main techniques behind developing LLMs for chatbot applications,
    personal assistants, and other conversational tasks.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们实现了LLM架构，进行了预训练，并将外部来源的预训练权重导入到我们的模型中。然后，我们专注于对LLM进行微调，以完成特定的分类任务：区分垃圾邮件和非垃圾邮件短信。现在，我们将实现微调LLM以遵循人类指令的过程，如图7.1所示。指令微调是开发用于聊天机器人应用、个人助理和其他对话任务LLM的主要技术之一。
- en: '![figure](../Images/7-1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-1.png)'
- en: 'Figure 7.1 The three main stages of coding an LLM. This chapter focuses on
    step 9 of stage 3: fine-tuning a pretrained LLM to follow human instructions.'
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1 编码LLM的三个主要阶段。本章重点介绍第三阶段的第9步：微调预训练的LLM以遵循人类指令。
- en: 'Figure 7.1 shows two main ways of fine-tuning an LLM: fine-tuning for classification
    (step 8) and fine-tuning an LLM to follow instructions (step 9). We implemented
    step 8 in chapter 6\. Now we will fine-tune an LLM using an *instruction dataset*.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1显示了微调LLM的两种主要方式：用于分类的微调（步骤8）和微调LLM以遵循指令（步骤9）。我们在第6章中实现了步骤8。现在，我们将使用*指令数据集*来微调LLM。
- en: 7.1 Introduction to instruction fine-tuning
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 指令微调简介
- en: We now know that pretraining an LLM involves a training procedure where it learns
    to generate one word at a time. The resulting pretrained LLM is capable of *text
    completion*, meaning it can finish sentences or write text paragraphs given a
    fragment as input. However, pretrained LLMs often struggle with specific instructions,
    such as “Fix the grammar in this text” or “Convert this text into passive voice.”
    Later, we will examine a concrete example where we load the pretrained LLM as
    the basis for *instruction fine-tuning*, also known as *supervised instruction
    fine-tuning*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，预训练LLM涉及一个训练过程，其中它学会一次生成一个单词。结果得到的预训练LLM能够进行*文本补全*，这意味着它可以根据输入的片段完成句子或撰写文本段落。然而，预训练的LLM在特定指令上往往很吃力，例如“修复这段文字的语法”或“将这段文字转换为被动语态”。稍后，我们将考察一个具体示例，其中我们将预训练的LLM作为*指令微调*（也称为*监督指令微调*）的基础。
- en: Here, we focus on improving the LLM’s ability to follow such instructions and
    generate a desired response, as illustrated in figure 7.2\. Preparing the dataset
    is a key aspect of instruction fine-tuning. Then we’ll complete all the steps
    in the three stages of the instruction fine-tuning process, beginning with the
    dataset preparation, as shown in figure 7.3.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们专注于提高LLM遵循此类指令并生成期望响应的能力，如图7.2所示。准备数据集是指令微调的关键方面。然后，我们将完成指令微调过程的三个阶段的所有步骤，从数据集准备开始，如图7.3所示。
- en: '![figure](../Images/7-2.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-2.png)'
- en: Figure 7.2 Examples of instructions that are processed by an LLM to generate
    desired responses
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2 LLM处理以生成期望响应的指令示例
- en: '![figure](../Images/7-3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-3.png)'
- en: 'Figure 7.3 The three-stage process for instruction fine-tuning an LLM. Stage
    1 involves dataset preparation, stage 2 focuses on model setup and fine-tuning,
    and stage 3 covers the evaluation of the model. We will begin with step 1 of stage
    1: downloading and formatting the dataset.'
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3 指令微调LLM的三阶段过程。第一阶段涉及数据集准备，第二阶段侧重于模型设置和微调，第三阶段涵盖模型的评估。我们将从第一阶段的第一步开始：下载和格式化数据集。
- en: 7.2 Preparing a dataset for supervised instruction fine-tuning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 准备用于监督指令微调的数据集
- en: Let’s download and format the instruction dataset for instruction fine-tuning
    a pretrained LLM. The dataset consists of 1,100 *instruction–response pairs* similar
    to those in figure 7.2\. This dataset was created specifically for this book,
    but interested readers can find alternative, publicly available instruction datasets
    in appendix B.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载并格式化指令数据集，以便对预训练的LLM进行指令微调。该数据集包含1,100个*指令-响应对*，类似于图7.2中的那些。这个数据集是专门为这本书创建的，但感兴趣的读者可以在附录B中找到其他可公开获取的指令数据集。
- en: The following code implements and executes a function to download this dataset,
    which is a relatively small file (only 204 KB) in JSON format. JSON, or JavaScript
    Object Notation, mirrors the structure of Python dictionaries, providing a simple
    structure for data interchange that is both human readable and machine friendly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实现并执行了一个函数来下载这个数据集，它是一个相对较小的文件（只有204 KB），格式为JSON。JSON，或JavaScript对象表示法，与Python字典的结构相似，提供了一个简单的数据交换结构，既适合人类阅读，又适合机器处理。
- en: Listing 7.1 Downloading the dataset
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1 下载数据集
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The output of executing the preceding code is
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前述代码的输出是
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `data` list that we loaded from the JSON file contains the 1,100 entries
    of the instruction dataset. Let’s print one of the entries to see how each entry
    is structured:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从JSON文件中加载的`data`列表包含指令数据集的1,100个条目。让我们打印其中一个条目，看看每个条目的结构：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The content of the example entry is
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 示例条目的内容是
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we can see, the example entries are Python dictionary objects containing
    an `''instruction''`, `''input''`, and `''output''`. Let’s take a look at another
    example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，示例条目是包含`'instruction'`、`'input'`和`'output'`的Python字典对象。让我们看看另一个示例：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Based on the contents of this entry, the `''input''` field may occasionally
    be empty:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 根据此条目的内容，`'input'`字段有时可能为空：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Instruction fine-tuning involves training a model on a dataset where the input-output
    pairs, like those we extracted from the JSON file, are explicitly provided. There
    are various methods to format these entries for LLMs. Figure 7.4 illustrates two
    different example formats, often referred to as *prompt styles*, used in the training
    of notable LLMs such as Alpaca and Phi-3\.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调涉及在数据集上训练模型，其中输入-输出对，如我们从JSON文件中提取的，是明确提供的。有各种方法来格式化这些条目以供LLM使用。图7.4说明了两种不同的示例格式，通常被称为*提示风格*，用于训练像阿尔帕卡和Phi-3这样的知名LLM。
- en: '![figure](../Images/7-4.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-4.png)'
- en: Figure 7.4 Comparison of prompt styles for instruction fine-tuning in LLMs.
    The Alpaca style (left) uses a structured format with defined sections for instruction,
    input, and response, while the Phi-3 style (right) employs a simpler format with
    designated `<|user|>` and `<|assistant|>` tokens.
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4 比较LLM中指令微调的提示风格。阿尔帕卡风格（左）使用具有定义明确的指令、输入和响应部分的格式，而Phi-3风格（右）使用具有指定`<|user|>`和`<|assistant|>`标记的更简单的格式。
- en: Alpaca was one of the early LLMs to publicly detail its instruction fine-tuning
    process. Phi-3, developed by Microsoft, is included to demonstrate the diversity
    in prompt styles. The rest of this chapter uses the Alpaca prompt style since
    it is one of the most popular ones, largely because it helped define the original
    approach to fine-tuning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 阿尔帕卡（Alpaca）是早期公开详细说明其指令微调过程的LLM之一。微软开发的Phi-3被包括在内，以展示提示风格的多样性。本章的其余部分使用阿尔帕卡提示风格，因为它是最受欢迎的之一，很大程度上是因为它帮助定义了微调的原始方法。
- en: Exercise 7.1 Changing prompt styles
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习7.1 更改提示风格
- en: After fine-tuning the model with the Alpaca prompt style, try the Phi-3 prompt
    style shown in figure 7.4 and observe whether it affects the response quality
    of the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用阿尔帕卡提示风格微调模型后，尝试图7.4中显示的Phi-3提示风格，并观察它是否会影响模型的响应质量。
- en: Let’s define a `format_input` function that we can use to convert the entries
    in the `data` list into the Alpaca-style input format.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个`format_input`函数，我们可以用它将`data`列表中的条目转换为阿尔帕卡风格的输入格式。
- en: Listing 7.2 Implementing the prompt formatting function
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.2 实现提示格式化函数
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This `format_input` function takes a dictionary `entry` as input and constructs
    a formatted string. Let’s test it to dataset entry `data[50]`, which we looked
    at earlier:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`format_input`函数接受一个字典`entry`作为输入，并构建一个格式化的字符串。让我们测试它对数据集条目`data[50]`，这是我们之前看过的：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The formatted input looks like as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 格式化后的输入看起来如下所示：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that the `format_input` skips the optional `###` `Input:` section if the
    `''input''` field is empty, which we can test out by applying the `format_input`
    function to entry `data[999]` that we inspected earlier:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`format_input`会跳过可选的`###` `Input:`部分，如果`'input'`字段为空，我们可以通过将`format_input`函数应用于我们之前检查的条目`data[999]`来测试这一点：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output shows that entries with an empty `''input''` field don’t contain
    an `###` `Input:` section in the formatted input:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示，具有空`'input'`字段的条目在格式化输入中不包含`###` `Input:`部分：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Before we move on to setting up the PyTorch data loaders in the next section,
    let’s divide the dataset into training, validation, and test sets analogous to
    what we have done with the spam classification dataset in the previous chapter.
    The following listing shows how we calculate the portions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一节设置PyTorch数据加载器之前，让我们将数据集划分为训练集、验证集和测试集，类似于我们在上一章中对垃圾邮件分类数据集所做的那样。以下列表显示了如何计算这些部分。
- en: Listing 7.3 Partitioning the dataset
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.3 分区数据集
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Use 85% of the data for training'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用85%的数据进行训练'
- en: '#2 Use 10% for testing'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用10%进行测试'
- en: '#3 Use remaining 5% for validation'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用剩余的5%进行验证'
- en: 'This partitioning results in the following dataset sizes:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分区结果导致以下数据集大小：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Having successfully downloaded and partitioned the dataset and gained a clear
    understanding of the dataset prompt formatting, we are now ready for the core
    implementation of the instruction fine-tuning process. Next, we focus on developing
    the method for constructing the training batches for fine-tuning the LLM.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功下载和分区数据集，并对数据集提示格式有清晰理解之后，我们现在可以准备指令微调过程的核心实现。接下来，我们专注于开发用于微调LLM的训练批次构建方法。
- en: 7.3 Organizing data into training batches
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 将数据组织到训练批次中
- en: As we progress into the implementation phase of our instruction fine-tuning
    process, the next step, illustrated in figure 7.5, focuses on constructing the
    training batches effectively. This involves defining a method that will ensure
    our model receives the formatted training data during the fine-tuning process.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入指令微调过程的实现阶段时，下一步，如图7.5所示，专注于有效地构建训练批次。这涉及到定义一个方法，确保我们的模型在微调过程中接收格式化的训练数据。
- en: '![figure](../Images/7-5.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-5.png)'
- en: 'Figure 7.5 The three-stage process for instruction fine-tuning an LLM. Next,
    we look at step 2 of stage 1: assembling the training batches.'
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5 指令微调LLM的三阶段过程。接下来，我们看看第一阶段步骤2：组装训练批次。
- en: In the previous chapter, the training batches were created automatically by
    the PyTorch `DataLoader` class, which employs a default *collate* function to
    combine lists of samples into batches. A collate function is responsible for taking
    a list of individual data samples and merging them into a single batch that can
    be processed efficiently by the model during training.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，训练批次是由PyTorch的`DataLoader`类自动创建的，该类使用默认的*合并*函数将样本列表组合成批次。合并函数负责将单个数据样本的列表合并成一个可以由模型在训练过程中高效处理的单一批次。
- en: However, the batching process for instruction fine-tuning is a bit more involved
    and requires us to create our own custom collate function that we will later plug
    into the `DataLoader`. We implement this custom collate function to handle the
    specific requirements and formatting of our instruction fine-tuning dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，指令微调的批处理过程稍微复杂一些，需要我们创建自己的自定义合并函数，稍后将其插入到`DataLoader`中。我们实现这个自定义合并函数来处理我们指令微调数据集的特定要求和格式。
- en: Let’s tackle the *batching process* in several steps, including coding the custom
    collate function, as illustrated in figure 7.6\. First, to implement steps 2.1
    and 2.2, we code an `InstructionDataset` class that applies `format_input` and
    *pretokenizes* all inputs in the dataset, similar to the `SpamDataset` in chapter
    6\. This two-step process, detailed in figure 7.7, is implemented in the `__init__`
    constructor method of the `InstructionDataset`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分几个步骤来处理批处理过程，包括编写自定义合并函数，如图7.6所示。首先，为了实现步骤2.1和2.2，我们编写了一个`InstructionDataset`类，该类应用`format_input`并*预处理*数据集中的所有输入，类似于第6章中的`SpamDataset`。这个两步过程，如图7.7所示，是在`InstructionDataset`的`__init__`构造方法中实现的。
- en: '![figure](../Images/7-6.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-6.png)'
- en: 'Figure 7.6 The five substeps involved in implementing the batching process:
    (2.1) applying the prompt template, (2.2) using tokenization from previous chapters,
    (2.3) adding padding tokens, (2.4) creating target token IDs, and (2.5) replacing
    `-100` placeholder tokens to mask padding tokens in the loss function.'
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6 实现批处理过程的五个子步骤：（2.1）应用提示模板，（2.2）使用前几章中的标记化，（2.3）添加填充标记，（2.4）创建目标标记ID，以及（2.5）在损失函数中将
    `-100` 占位符标记替换为掩码填充标记。
- en: '![figure](../Images/7-7.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-7.png)'
- en: Figure 7.7 The first two steps involved in implementing the batching process.
    Entries are first formatted using a specific prompt template (2.1) and then tokenized
    (2.2), resulting in a sequence of token IDs that the model can process.
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.7 实现批处理过程涉及的前两个步骤。首先使用特定的提示模板（2.1）格式化条目，然后进行标记化（2.2），从而生成模型可以处理的标记ID序列。
- en: Listing 7.4 Implementing an instruction dataset class
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.4 实现指令数据集类
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Pretokenizes texts'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 预标记文本'
- en: Similar to the approach used for classification fine-tuning, we want to accelerate
    training by collecting multiple training examples in a batch, which necessitates
    padding all inputs to a similar length. As with classification fine-tuning, we
    use the `<|endoftext|>` token as a padding token.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与用于分类微调的方法类似，我们希望通过在批次中收集多个训练示例来加速训练，这需要将所有输入填充到相似长度。与分类微调一样，我们使用 `<|endoftext|>`
    标记作为填充标记。
- en: 'Instead of appending the `<|endoftext|>` tokens to the text inputs, we can
    append the token ID corresponding to `<|endoftext|>` to the pretokenized inputs
    directly. We can use the tokenizer’s `.encode` method on an `<|endoftext|>` token
    to remind us which token ID we should use:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在将 `<|endoftext|>` 标记附加到文本输入之前，直接将对应于 `<|endoftext|>` 的标记ID附加到预标记的输入中。我们可以使用标记器的
    `.encode` 方法在 `<|endoftext|>` 标记上，以提醒我们应使用哪个标记ID：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The resulting token ID is `50256`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的标记ID是 `50256`。
- en: Moving on to step 2.3 of the process (see figure 7.6), we adopt a more sophisticated
    approach by developing a custom collate function that we can pass to the data
    loader. This custom collate function pads the training examples in each batch
    to the same length while allowing different batches to have different lengths,
    as demonstrated in figure 7.8\. This approach minimizes unnecessary padding by
    only extending sequences to match the longest one in each batch, not the whole
    dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来进行步骤2.3（见图7.6），我们采用了一种更复杂的方法，通过开发一个自定义的 collate 函数，我们可以将其传递给数据加载器。这个自定义的
    collate 函数将每个批次中的训练示例填充到相同的长度，同时允许不同的批次有不同的长度，如图7.8所示。这种方法通过仅将序列扩展到匹配每个批次中最长的序列，而不是整个数据集，从而最小化了不必要的填充。
- en: '![figure](../Images/7-8.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-8.png)'
- en: Figure 7.8 The padding of training examples in batches using token ID `50256`
    to ensure uniform length within each batch. Each batch may have different lengths,
    as shown by the first and second.
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.8 使用标记ID `50256` 在批次中对训练示例进行填充，以确保每个批次内的长度一致。每个批次可能有不同的长度，如第一和第二个所示。
- en: 'We can implement the padding process with a custom collate function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用自定义的 collate 函数来实现填充过程：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Finds the longest sequence in the batch'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在批次中找到最长的序列'
- en: '#2 Pads and prepares inputs'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 填充并准备输入'
- en: '#3 Removes extra padded token added earlier'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 移除之前添加的额外填充标记'
- en: '#4 Converts the list of inputs to a tensor and transfers it to the target device'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将输入列表转换为张量并将其传输到目标设备'
- en: 'The `custom_collate_draft_1` we implemented is designed to be integrated into
    a PyTorch `DataLoader`, but it can also function as a standalone tool. Here, we
    use it independently to test and verify that it operates as intended. Let’s try
    it on three different inputs that we want to assemble into a batch, where each
    example gets padded to the same length:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现的 `custom_collate_draft_1` 被设计成可以集成到 PyTorch `DataLoader` 中，但它也可以作为一个独立的工具使用。在这里，我们独立使用它来测试和验证它是否按预期运行。让我们尝试将以下三个不同的输入组装成一个批次，其中每个示例都被填充到相同的长度：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The resulting batch looks like the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的批次看起来如下所示：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This output shows all inputs have been padded to the length of the longest input
    list, `inputs_1`, containing five token IDs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出显示所有输入都已填充到最长输入列表 `inputs_1` 的长度，包含五个标记ID。
- en: We have just implemented our first custom collate function to create batches
    from lists of inputs. However, as we previously learned, we also need to create
    batches with the target token IDs corresponding to the batch of input IDs. These
    target IDs, as shown in figure 7.9, are crucial because they represent what we
    want the model to generate and what we need during training to calculate the loss
    for the weight updates. That is, we modify our custom collate function to return
    the target token IDs in addition to the input token IDs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚实现了我们的第一个自定义collate函数，用于从输入列表创建批处理。然而，正如我们之前学到的，我们还需要创建与输入ID批处理相对应的目标标记ID批处理。这些目标ID，如图7.9所示，至关重要，因为它们代表我们希望模型生成的以及我们在训练中需要用于计算权重更新的损失。也就是说，我们修改了我们的自定义collate函数，使其除了返回输入标记ID外，还返回目标标记ID。
- en: '![figure](../Images/7-9.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-9.png)'
- en: Figure 7.9 The five substeps involved in implementing the batching process.
    We are now focusing on step 2.4, the creation of target token IDs. This step is
    essential as it enables the model to learn and predict the tokens it needs to
    generate.
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.9 实施批处理过程涉及的五个子步骤。我们现在专注于第2.4步，即创建目标标记ID。这一步至关重要，因为它使模型能够学习和预测它需要生成的标记。
- en: Similar to the process we used to pretrain an LLM, the target token IDs match
    the input token IDs but are shifted one position to the right. This setup, as
    shown in figure 7.10, allows the LLM to learn how to predict the next token in
    a sequence.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们用于预训练LLM的过程类似，目标标记ID与输入标记ID匹配，但向右移动一个位置。这种设置，如图7.10所示，允许LLM学习如何预测序列中的下一个标记。
- en: '![figure](../Images/7-10.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-10.png)'
- en: Figure 7.10 The input and target token alignment used in the instruction fine-tuning
    process of an LLM. For each input sequence, the corresponding target sequence
    is created by shifting the token IDs one position to the right, omitting the first
    token of the input, and appending an end-of-text token.
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.10 在LLM的指令微调过程中使用的输入和目标标记对齐。对于每个输入序列，相应的目标序列是通过将标记ID向右移动一个位置，省略输入的第一个标记，并附加一个文本结束标记来创建的。
- en: 'The following updated collate function generates the target token IDs from
    the input token IDs:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的更新版collate函数从输入标记ID生成目标标记ID：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#1 Truncates the last token for inputs'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 对输入截断最后一个标记'
- en: '#2 Shifts +1 to the right for targets'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 对目标向右移动+1'
- en: 'Applied to the example `batch` consisting of three input lists we defined earlier,
    the new `custom_collate_draft_2` function now returns the input and the target
    batch:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 将应用于我们之前定义的包含三个输入列表的示例`batch`，新的`custom_collate_draft_2`函数现在返回输入和目标批处理：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 The first tensor represents inputs.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第一个张量代表输入。'
- en: '#2 The second tensor represents the targets.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 第二个张量代表目标。'
- en: In the next step, we assign a `-100` placeholder value to all padding tokens,
    as highlighted in figure 7.11\. This special value allows us to exclude these
    padding tokens from contributing to the training loss calculation, ensuring that
    only meaningful data influences model learning. We will discuss this process in
    more detail after we implement this modification. (When fine-tuning for classification,
    we did not have to worry about this since we only trained the model based on the
    last output token.)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将`-100`占位符值分配给所有填充标记，如图7.11所示。这个特殊值允许我们排除这些填充标记对训练损失计算的贡献，确保只有有意义的数据影响模型学习。我们将在实施此修改后更详细地讨论此过程。（当进行分类微调时，我们不必担心这一点，因为我们只基于最后一个输出标记训练模型。）
- en: '![figure](../Images/7-11.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-11.png)'
- en: Figure 7.11 The five substeps involved in implementing the batching process.
    After creating the target sequence by shifting token IDs one position to the right
    and appending an end-of-text token, in step 2.5, we replace the end-of-text padding
    tokens with a placeholder value (`-100`).
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.11 实施批处理过程涉及的五个子步骤。在通过将标记ID向右移动一个位置并附加一个文本结束标记创建目标序列后，在第2.5步中，我们将文本结束填充标记替换为占位符值（`-100`）。
- en: However, note that we retain one end-of-text token, ID `50256`, in the target
    list, as depicted in figure 7.12\. Retaining it allows the LLM to learn when to
    generate an end-of-text token in response to instructions, which we use as an
    indicator that the generated response is complete.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，我们在目标列表中保留了最后一个文本结束标记，ID为`50256`，如图7.12所示。保留它允许LLM学习在响应指令时何时生成文本结束标记，这我们用作生成响应完整的指示器。
- en: '![figure](../Images/7-12.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-12.png)'
- en: Figure 7.12 Step 2.4 in the token replacement process in the target batch for
    the training data preparation. We replace all but the first instance of the end-of-text
    token, which we use as padding, with the placeholder value `-100`, while keeping
    the initial end-of-text token in each target sequence.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.12 步骤2.4 在训练数据准备中目标批次的标记替换过程。我们将除用作填充的文本结束标记的第一个实例之外的所有文本结束标记替换为占位符值 `-100`，同时保持每个目标序列中的初始文本结束标记。
- en: In the following listing, we modify our custom collate function to replace tokens
    with ID `50256` with `-100` in the target lists. Additionally, we introduce an
    `allowed_ max_length` parameter to optionally limit the length of the samples.
    This adjustment will be useful if you plan to work with your own datasets that
    exceed the 1,024-token context size supported by the GPT-2 model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们修改了我们的自定义 collate 函数，将目标列表中的标记ID `50256` 替换为 `-100`。此外，我们引入了一个 `allowed_max_length`
    参数，以可选方式限制样本的长度。如果您计划使用自己的数据集，这些数据集的标记上下文大小超过了GPT-2模型支持的1,024标记，这种调整将非常有用。
- en: Listing 7.5 Implementing a custom batch collate function
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.5 实现自定义批次 collate 函数
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 Pads sequences to max_length'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将序列填充到 max_length'
- en: '#2 Truncates the last token for inputs'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 截断输入的最后标记'
- en: '#3 Shifts +1 to the right for targets'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将目标向右移动 +1'
- en: '#4 Replaces all but the first padding tokens in targets by ignore_index'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将目标中的除第一个填充标记外的所有标记替换为 ignore_index'
- en: '#5 Optionally truncates to the maximum sequence length'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 可选地截断到最大序列长度'
- en: 'Again, let’s try the collate function on the sample batch that we created earlier
    to check that it works as intended:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们尝试使用我们之前创建的样本批次来检查 collate 函数是否按预期工作：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The results are as follows, where the first tensor represents the inputs and
    the second tensor represents the targets:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下，其中第一个张量表示输入，第二个张量表示目标：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The modified collate function works as expected, altering the target list by
    inserting the token ID `-100`. What is the logic behind this adjustment? Let’s
    explore the underlying purpose of this modification.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的 collate 函数按预期工作，通过插入标记ID `-100` 来修改目标列表。这种调整背后的逻辑是什么？让我们探索这种修改的潜在目的。
- en: 'For demonstration purposes, consider the following simple and self-contained
    example where each output logit corresponds to a potential token from the model’s
    vocabulary. Here’s how we might calculate the cross entropy loss(introduced in
    chapter 5) during training when the model predicts a sequence of tokens, which
    is similar to what we did when we pretrained the model and fine-tuned it for classification:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，考虑以下简单且自包含的示例，其中每个输出对数概率对应于模型词汇表中的一个潜在标记。以下是我们在训练过程中如何计算交叉熵损失（第5章中介绍）的示例，当模型预测一系列标记时，这与我们在预训练模型并对其进行分类微调时所做的是类似的：
- en: '[PRE23]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#1 predictions for 1st token'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 对第一个标记进行预测'
- en: '#2 predictions for 2nd token'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 对第二个标记进行预测'
- en: 'The loss value calculated by the previous code is `1.1269`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个代码计算出的损失值为 `1.1269`：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As we would expect, adding an additional token ID affects the loss calculation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所预期，添加一个额外的标记ID会影响损失计算：
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 New third token ID prediction'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 新的第三标记ID预测'
- en: After adding the third token, the loss value is `0.7936`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 添加第三个标记后，损失值为 `0.7936`。
- en: 'So far, we have carried out some more or less obvious example calculations
    using the cross entropy loss function in PyTorch, the same loss function we used
    in the training functions for pretraining and fine-tuning for classification.
    Now let’s get to the interesting part and see what happens if we replace the third
    target token ID with `-100`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用PyTorch中的交叉熵损失函数进行了一些或多或少明显的示例计算，这与我们在预训练和分类微调训练函数中使用的损失函数相同。现在让我们进入有趣的部分，看看如果我们用
    `-100` 替换第三个目标标记ID会发生什么：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The resulting output is
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出为
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The resulting loss on these three training examples is identical to the loss
    we calculated from the two training examples earlier. In other words, the cross
    entropy loss function ignored the third entry in the `targets_3` vector, the token
    ID corresponding to `-100`. (Interested readers can try to replace the `-100`
    value with another token ID that is not `0` or `1`; it will result in an error.)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个训练示例的结果损失与我们之前从两个训练示例中计算出的损失相同。换句话说，交叉熵损失函数忽略了 `targets_3` 向量中的第三个条目，即对应于
    `-100` 的标记ID。（感兴趣的读者可以尝试用另一个非 `0` 或 `1` 的标记ID替换 `-100` 值；这将导致错误。）
- en: So what’s so special about `-100` that it’s ignored by the cross entropy loss?
    The default setting of the cross entropy function in PyTorch is `cross_entropy(...,`
    `ignore_index=-100)`. This means that it ignores targets labeled with `-100`.
    We take advantage of this `ignore_index` to ignore the additional end-of-text
    (padding) tokens that we used to pad the training examples to have the same length
    in each batch. However, we want to keep one `50256` (end-of-text) token ID in
    the targets because it helps the LLM to learn to generate end-of-text tokens,
    which we can use as an indicator that a response is complete.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 `-100` 有什么特别之处，以至于它被交叉熵损失所忽略？PyTorch 中交叉熵函数的默认设置是 `cross_entropy(..., ignore_index=-100)`。这意味着它忽略了标记为
    `-100` 的目标。我们利用这个 `ignore_index` 来忽略我们用来填充训练示例以使每个批次长度相同的额外文本结束（填充）标记。然而，我们想在目标中保留一个
    `50256`（文本结束）标记 ID，因为它有助于 LLM 学习生成文本结束标记，我们可以将其用作响应完整的指示器。
- en: In addition to masking out padding tokens, it is also common to mask out the
    target token IDs that correspond to the instruction, as illustrated in figure
    7.13\. By masking out the LLM’s target token IDs corresponding to the instruction,
    the cross entropy loss is only computed for the generated response target IDs.
    Thus, the model is trained to focus on generating accurate responses rather than
    memorizing instructions, which can help reduce overfitting.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 除了遮罩掉填充标记外，遮罩掉与指令对应的目标标记 ID 也是常见的，如图 7.13 所示。通过遮罩掉 LLM 的与指令对应的目标标记 ID，交叉熵损失仅计算生成的响应目标
    ID。因此，模型被训练来专注于生成准确的响应，而不是记住指令，这有助于减少过拟合。
- en: '![figure](../Images/7-13.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-13.png)'
- en: 'Figure 7.13 Left: The formatted input text we tokenize and then feed to the
    LLM during training. Right: The target text we prepare for the LLM where we can
    optionally mask out the instruction section, which means replacing the corresponding
    token IDs with the `-100` `ignore_index` value.'
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.13 左：我们在训练期间分词并输入到 LLM 中的格式化输入文本。右：我们为 LLM 准备的目标文本，我们可以选择遮罩掉指令部分，这意味着用 `-100`
    的 `ignore_index` 值替换相应的标记 ID。
- en: As of this writing, researchers are divided on whether masking the instructions
    is universally beneficial during instruction fine-tuning. For instance, the 2024
    paper by Shi et al., “Instruction Tuning With Loss Over Instructions” ([https://arxiv.org/abs/2405.14394](https://arxiv.org/abs/2405.14394)),
    demonstrated that not masking the instructions benefits the LLM performance (see
    appendix B for more details). Here, we will not apply masking and leave it as
    an optional exercise for interested readers.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到本文写作时，研究人员在是否在指令微调期间遮罩指令普遍有益的问题上存在分歧。例如，Shi 等人于 2024 年发表的论文“带有指令损失的指令微调”（[https://arxiv.org/abs/2405.14394](https://arxiv.org/abs/2405.14394)）表明，不遮罩指令有利于
    LLM 性能（更多细节请见附录 B）。在这里，我们将不应用遮罩，将其作为对感兴趣读者的一项可选练习。
- en: Exercise 7.2 Instruction and input masking
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 7.2 指令和输入遮罩
- en: After completing the chapter and fine-tuning the model with `InstructionDataset`,
    replace the instruction and input tokens with the `-100` mask to use the instruction
    masking method illustrated in figure 7.13\. Then evaluate whether this has a positive
    effect on model performance.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 完成章节并使用 `InstructionDataset` 微调模型后，将指令和输入标记替换为 `-100` 遮罩，以使用图 7.13 中展示的指令遮罩方法。然后评估这是否对模型性能有积极影响。
- en: 7.4 Creating data loaders for an instruction dataset
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 为指令数据集创建数据加载器
- en: We have completed several stages to implement an `InstructionDataset` class
    and a `custom_collate_fn` function for the instruction dataset. As shown in figure
    7.14, we are ready to reap the fruits of our labor by simply plugging both `InstructionDataset`
    objects and the `custom_collate_fn` function into PyTorch data loaders. These
    loaders will automatically shuffle and organize the batches for the LLM instruction
    fine-tuning process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了几个阶段，以实现 `InstructionDataset` 类和用于指令数据集的 `custom_collate_fn` 函数。如图 7.14
    所示，我们只需将 `InstructionDataset` 对象和 `custom_collate_fn` 函数简单地插入 PyTorch 数据加载器中，就可以收获我们的劳动成果。这些加载器将自动打乱和组织批次，以进行
    LLM 指令微调过程。
- en: '![figure](../Images/7-14.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-14.png)'
- en: Figure 7.14 The three-stage process for instruction fine-tuning an LLM. Thus
    far, we have prepared the dataset and implemented a custom collate function to
    batch the instruction dataset. Now, we can create and apply the data loaders to
    the training, validation, and test sets needed for the LLM instruction fine-tuning
    and evaluation.
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.14 指示LLM微调的三阶段过程。到目前为止，我们已经准备好了数据集并实现了一个自定义的collate函数来批量处理指令数据集。现在，我们可以创建并应用数据加载器到LLM指令微调和评估所需的训练、验证和测试集。
- en: Before we implement the data loader creation step, we have to briefly talk about
    the `device` setting of the `custom_collate_fn`. The `custom_collate_fn` includes
    code to move the input and target tensors (for example, `torch.stack(inputs_lst).to
    (device)`) to a specified device, which can be either `"cpu"` or `"cuda"` (for
    NVIDIA GPUs) or, optionally, `"mps"` for Macs with Apple Silicon chips.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现数据加载器创建步骤之前，我们必须简要谈谈`custom_collate_fn`的`device`设置。`custom_collate_fn`包括将输入和目标张量（例如，`torch.stack(inputs_lst).to(device)`）移动到指定设备的代码，这可以是`"cpu"`或`"cuda"`（用于NVIDIA
    GPU）或可选的`"mps"`（用于配备Apple Silicon芯片的Mac）。
- en: Note  Using an `"mps"` device may result in numerical differences compared to
    the contents of this chapter, as Apple Silicon support in PyTorch is still experimental.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：使用`"mps"`设备可能与本章内容中的内容产生数值差异，因为PyTorch对Apple Silicon的支持仍然是实验性的。
- en: Previously, we moved the data onto the target device (for example, the GPU memory
    when `device="cuda"`) in the main training loop. Having this as part of the collate
    function offers the advantage of performing this device transfer process as a
    background process outside the training loop, preventing it from blocking the
    GPU during model training.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的训练循环中，我们将数据移动到了目标设备（例如，当`device="cuda"`时的GPU内存）。将此作为collate函数的一部分提供了优势，即在训练循环之外作为后台进程执行此设备传输过程，防止它阻塞模型训练时的GPU。
- en: 'The following code initializes the `device` variable:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码初始化了`device`变量：
- en: '[PRE28]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#1 Uncomments these two lines to use the GPU on an Apple Silicon chip'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 取消注释这两行以在Apple Silicon芯片上使用GPU'
- en: This will either print `"Device:` `cpu"` or `"Device:` `cuda"`, depending on
    your machine.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将根据您的机器打印出`"Device:` cpu`或`"Device:` cuda"。
- en: 'Next, to reuse the chosen device setting in `custom_collate_fn` when we plug
    it into the PyTorch `DataLoader` class, we use the `partial` function from Python’s
    `functools` standard library to create a new version of the function with the
    device argument prefilled. Additionally, we set the `allowed_max_length` to `1024`,
    which truncates the data to the maximum context length supported by the GPT-2
    model, which we will fine-tune later:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了在将`custom_collate_fn`插入PyTorch的`DataLoader`类时重用选择的设备设置，我们使用Python的`functools`标准库中的`partial`函数创建一个带有预填充设备参数的新函数版本。此外，我们将`allowed_max_length`设置为`1024`，这将截断数据到GPT-2模型支持的上下文最大长度，该模型我们稍后将对其进行微调：
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Next, we can set up the data loaders as we did previously, but this time, we
    will use our custom collate function for the batching process.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以像之前一样设置数据加载器，但这次，我们将使用我们的自定义collate函数进行批处理过程。
- en: Listing 7.6 Initializing the data loaders
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.6 初始化数据加载器
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '#1 You can try to increase this number if parallel Python processes are supported
    by your operating system.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 如果您的操作系统支持并行Python进程，您可以尝试增加这个数字。'
- en: 'Let’s examine the dimensions of the input and target batches generated by the
    training loader:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查由训练加载器生成的输入和目标批次的维度：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows (truncated to conserve space):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了节省空间已截断）：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This output shows that the first input and target batch have dimensions 8 ×
    61, where 8 represents the batch size and 61 is the number of tokens in each training
    example in this batch. The second input and target batch have a different number
    of tokens—for instance, 76\. Thanks to our custom collate function, the data loader
    is able to create batches of different lengths. In the next section, we load a
    pretrained LLM that we can then fine-tune with this data loader.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出显示，第一个输入和目标批次具有8 × 61的维度，其中8代表批次大小，61是此批次中每个训练示例中的标记数。第二个输入和目标批次具有不同的标记数——例如，76。多亏了我们的自定义collate函数，数据加载器能够创建不同长度的批次。在下文中，我们将加载一个预训练的LLM，然后我们可以使用这个数据加载器对其进行微调。
- en: 7.5 Loading a pretrained LLM
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 加载预训练的LLM
- en: We have spent a lot of time preparing the dataset for instruction fine-tuning,
    which is a key aspect of the supervised fine-tuning process. Many other aspects
    are the same as in pretraining, allowing us to reuse much of the code from earlier
    chapters.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在准备指令微调数据集上花费了大量时间，这是监督微调过程中的一个关键方面。许多其他方面与预训练相同，使我们能够重用早期章节中大部分的代码。
- en: Before beginning instruction fine-tuning, we must first load a pretrained GPT
    model that we want to fine-tune (see figure 7.15), a process we have undertaken
    previously. However, instead of using the smallest 124-million-parameter model
    as before, we load the medium-sized model with 355 million parameters. The reason
    for this choice is that the 124-million-parameter model is too limited in capacity
    to achieve satisfactory results via instruction fine-tuning. Specifically, smaller
    models lack the necessary capacity to learn and retain the intricate patterns
    and nuanced behaviors required for high-quality instruction-following tasks.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始指令微调之前，我们必须首先加载一个我们想要微调的预训练 GPT 模型（见图 7.15），这是一个我们之前已经执行过的过程。然而，与之前使用最小的
    1240 万参数模型不同，我们这次加载了具有 3.55 亿参数的中等大小模型。选择这个模型的原因是，1240 万参数模型在容量上过于有限，无法通过指令微调获得令人满意的结果。具体来说，较小的模型缺乏学习并保留高质量指令遵循任务所需的复杂模式和细微行为所必需的容量。
- en: '![figure](../Images/7-15.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-15.png)'
- en: Figure 7.15 The three-stage process for instruction fine-tuning an LLM. After
    the dataset preparation, the process of fine-tuning an LLM for instruction-following
    begins with loading a pretrained LLM, which serves as the foundation for subsequent
    training.
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.15 指令微调 LLM 的三个阶段过程。在数据集准备之后，指令遵循的 LLM 微调过程开始于加载一个预训练 LLM，这作为后续训练的基础。
- en: Loading our pretrained models requires the same code as when we pretrained the
    data (section 5.5) and fine-tuned it for classification (section 6.4), except
    that we now specify `"gpt2-medium` `(355M)"` instead of `"gpt2-small` `(124M)"`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 加载我们的预训练模型需要与我们在第 5.5 节中预训练数据时相同的代码，以及在第 6.4 节中对其进行分类微调时的代码，只是我们现在指定 `"gpt2-medium`
    `(355M)"` 而不是 `"gpt2-small` `(124M)"`。
- en: Note  Executing this code will initiate the download of the medium-sized GPT
    model, which has a storage requirement of approximately 1.42 gigabytes. This is
    roughly three times larger than the storage space needed for the small model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：执行此代码将启动下载中等大小的 GPT 模型，该模型的存储需求约为 1.42 吉字节。这大约是小型模型所需存储空间的 三倍。
- en: Listing 7.7 Loading the pretrained model
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.7 加载预训练模型
- en: '[PRE33]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After executing the code, several files will be downloaded:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，将下载几个文件：
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, let’s take a moment to assess the pretrained LLM’s performance on one
    of the validation tasks by comparing its output to the expected response. This
    will give us a baseline understanding of how well the model performs on an instruction-following
    task right out of the box, prior to fine-tuning, and will help us appreciate the
    effect of fine-tuning later on. We will use the first example from the validation
    set for this assessment:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们花一点时间评估预训练 LLM 在验证任务中的一个表现，通过将其输出与预期响应进行比较。这将使我们获得一个基准理解，了解模型在未经微调的情况下直接执行指令遵循任务的表现如何，并有助于我们后来欣赏微调的效果。我们将使用验证集的第一个示例进行此评估：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The content of the instruction is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 指令内容如下：
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next we generate the model’s response using the same `generate` function we
    used to pretrain the model in chapter 5:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用与第 5 章中预训练模型相同的 `generate` 函数来生成模型的响应：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `generate` function returns the combined input and output text. This behavior
    was previously convenient since pretrained LLMs are primarily designed as text-completion
    models, where the input and output are concatenated to create coherent and legible
    text. However, when evaluating the model’s performance on a specific task, we
    often want to focus solely on the model’s generated response.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate` 函数返回组合的输入和输出文本。这种行为之前很方便，因为预训练 LLM 主要被设计为文本补全模型，其中输入和输出被连接起来以创建连贯且易于阅读的文本。然而，当评估模型在特定任务上的性能时，我们通常只想关注模型生成的响应。'
- en: 'To isolate the model’s response text, we need to subtract the length of the
    input instruction from the start of the `generated_text`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了隔离模型的响应文本，我们需要从 `generated_text` 的开始减去输入指令的长度：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This code removes the input text from the beginning of the `generated_text`,
    leaving us with only the model’s generated response. The `strip()` function is
    then applied to remove any leading or trailing whitespace characters. The output
    is
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码从`generated_text`的开头移除输入文本，只留下模型的生成响应。然后应用`strip()`函数来移除任何前导或尾随空白字符。输出如下
- en: '[PRE39]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This output shows that the pretrained model is not yet capable of correctly
    following the given instruction. While it does create a Response section, it simply
    repeats the original input sentence and part of the instruction, failing to convert
    the active sentence to passive voice as requested. So, let’s now implement the
    fine-tuning process to improve the model’s ability to comprehend and appropriately
    respond to such requests.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出显示，预训练模型尚不能正确地遵循给定的指令。虽然它确实创建了一个响应部分，但它只是重复了原始输入句子和部分指令，未能将主动句转换为请求的被动语态。因此，我们现在实施微调过程，以提高模型理解和适当响应此类请求的能力。
- en: 7.6 Fine-tuning the LLM on instruction data
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 在指令数据上微调LLM
- en: 'It’s time to fine-tune the LLM for instructions (figure 7.16). We will take
    the loaded pretrained model in the previous section and further train it using
    the previously prepared instruction dataset prepared earlier in this chapter.
    We already did all the hard work when we implemented the instruction dataset processing
    at the beginning of this chapter. For the fine-tuning process itself, we can reuse
    the loss calculation and training functions implemented in chapter 5:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候微调LLM以处理指令了（图7.16）。我们将使用上一节中加载的预训练模型，并使用本章前面准备好的指令数据集进一步训练它。当我们在本章开头实现指令数据集处理时，我们已经完成了所有艰苦的工作。对于微调过程本身，我们可以重用第5章中实现的损失计算和训练函数：
- en: '![figure](../Images/7-16.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-16.png)'
- en: Figure 7.16 The three-stage process for instruction fine-tuning an LLM. In step
    5, we train the pretrained model we previously loaded on the instruction dataset
    we prepared earlier.
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.16 对LLM指令微调的三阶段过程。在第5步中，我们在之前准备好的指令数据集上训练之前加载的预训练模型。
- en: '[PRE40]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Before we begin training, let’s calculate the initial loss for the training
    and validation sets:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始训练之前，让我们计算训练集和验证集的初始损失：
- en: '[PRE41]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The initial loss values are as follows; as previously, our goal is to minimize
    the loss:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 初始损失值如下；如前所述，我们的目标是使损失最小化：
- en: '[PRE42]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Dealing with hardware limitations
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 处理硬件限制
- en: 'Using and training a larger model like GPT-2 medium (355 million parameters)
    is more computationally intensive than the smaller GPT-2 model (124 million parameters).
    If you encounter problems due to hardware limitations, you can switch to the smaller
    model by changing `CHOOSE_MODEL` `=` `"gpt2-medium` `(355M)"` to `CHOOSE_MODEL`
    `=` `"gpt2-small` `(124M)"` (see section 7.5). Alternatively, to speed up the
    model training, consider using a GPU. The following supplementary section in this
    book’s code repository lists several options for using cloud GPUs: [https://mng.bz/EOEq](https://mng.bz/EOEq).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用和训练一个更大的模型，如GPT-2中等（355百万参数），比较小的GPT-2模型（124百万参数）计算量更大。如果您因硬件限制而遇到问题，可以通过将`CHOOSE_MODEL`
    `=` `"gpt2-medium` `(355M)"`更改为`CHOOSE_MODEL` `=` `"gpt2-small` `(124M)"`（见第7.5节）来切换到较小的模型。或者，为了加快模型训练，考虑使用GPU。本书代码库中的以下补充部分列出了使用云GPU的几种选项：[https://mng.bz/EOEq](https://mng.bz/EOEq)。
- en: The following table provides reference run times for training each model on
    various devices, including CPUs and GPUs, for GPT-2\. Running this code on a compatible
    GPU requires no code changes and can significantly speed up training. For the
    results shown in this chapter, I used the GPT-2 medium model and trained it on
    an A100 GPU.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 下表提供了在包括CPU和GPU在内的各种设备上训练每个模型的参考运行时间，对于GPT-2。在兼容的GPU上运行此代码无需代码更改，并且可以显著加快训练速度。对于本章中显示的结果，我使用了GPT-2中等模型，并在A100
    GPU上对其进行训练。
- en: '| Model name | Device | Run time for two epochs |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 设备 | 两个epoch的运行时间 |'
- en: '| --- | --- | --- |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| gpt2-medium (355M)  | CPU (M3 MacBook Air)  | 15.78 minutes  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| gpt2-medium (355M)  | CPU (M3 MacBook Air)  | 15.78分钟  |'
- en: '| gpt2-medium (355M)  | GPU (NVIDIA L4)  | 1.83 minutes  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| gpt2-medium (355M)  | GPU (NVIDIA L4)  | 1.83分钟  |'
- en: '| gpt2-medium (355M)  | GPU (NVIDIA A100)  | 0.86 minutes  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| gpt2-medium (355M)  | GPU (NVIDIA A100)  | 0.86分钟  |'
- en: '| gpt2-small (124M)  | CPU (M3 MacBook Air)  | 5.74 minutes  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| gpt2-small (124M)  | CPU (M3 MacBook Air)  | 5.74分钟  |'
- en: '| gpt2-small (124M)  | GPU (NVIDIA L4)  | 0.69 minutes  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| gpt2-small (124M)  | GPU (NVIDIA L4)  | 0.69分钟  |'
- en: '| gpt2-small (124M)  | GPU (NVIDIA A100)  | 0.39 minutes  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| gpt2-small (124M)  | GPU (NVIDIA A100)  | 0.39分钟  |'
- en: With the model and data loaders prepared, we can now proceed to train the model.
    The code in listing 7.8 sets up the training process, including initializing the
    optimizer, setting the number of epochs, and defining the evaluation frequency
    and starting context to evaluate generated LLM responses during training based
    on the first validation set instruction (`val_data[0]`) we looked at in section
    7.5.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据加载器准备就绪后，我们现在可以开始训练模型。列表7.8中的代码设置了训练过程，包括初始化优化器、设置epoch数量以及定义评估频率和起始上下文，以便在训练期间根据我们在7.5节中查看的第一个验证集指令（`val_data[0]`）评估生成的LLM响应。
- en: Listing 7.8 Instruction fine-tuning the pretrained LLM
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.8 指令微调预训练的LLM
- en: '[PRE43]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The following output displays the training progress over two epochs, where
    a steady decrease in losses indicates improving ability to follow instructions
    and generate appropriate responses:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出显示了两个epoch的训练进度，其中损失值的持续下降表明了模型遵循指令和生成适当响应的能力在提高：
- en: '[PRE44]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The training output shows that the model is learning effectively, as we can
    tell based on the consistently decreasing training and validation loss values
    over the two epochs. This result suggests that the model is gradually improving
    its ability to understand and follow the provided instructions. (Since the model
    demonstrated effective learning within these two epochs, extending the training
    to a third epoch or more is not essential and may even be counterproductive as
    it could lead to increased overfitting.)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 训练输出显示模型正在有效地学习，我们可以根据两个epoch中训练和验证损失值的持续下降来判断。这一结果表明，模型正在逐渐提高其理解和遵循提供指令的能力。（由于模型在这两个epoch内展示了有效的学习，因此将训练扩展到第三个epoch或更多可能是不必要的，甚至可能适得其反，因为它可能导致过拟合的增加。）
- en: 'Moreover, the generated responses at the end of each epoch let us inspect the
    model’s progress in correctly executing the given task in the validation set example.
    In this case, the model successfully converts the active sentence `"The` `chef`
    `cooks` `the` `meal` `every` `day."` into its passive voice counterpart: `"The`
    `meal` `is` `cooked` `every` `day` `by` `the` `chef."`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个epoch结束时生成的响应使我们能够检查模型在验证集示例中正确执行给定任务的过程。在这种情况下，模型成功地将主动句“`The` `chef`
    `cooks` `the` `meal` `every` `day.`”转换为它的被动语态对应句：“`The` `meal` `is` `cooked` `every`
    `day` `by` `the` `chef.`”
- en: 'We will revisit and evaluate the response quality of the model in more detail
    later. For now, let’s examine the training and validation loss curves to gain
    additional insights into the model’s learning process. For this, we use the same
    `plot_losses` function we used for pretraining:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后更详细地回顾和评估模型的响应质量。现在，让我们检查训练和验证损失曲线，以获得对模型学习过程的更多见解。为此，我们使用与预训练相同的`plot_losses`函数：
- en: '[PRE45]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: From the loss plot shown in figure 7.17, we can see that the model’s performance
    on both the training and validation sets improves substantially over the course
    of training. The rapid decrease in losses during the initial phase indicates that
    the model quickly learns meaningful patterns and representations from the data.
    Then, as training progresses to the second epoch, the losses continue to decrease
    but at a slower rate, suggesting that the model is fine-tuning its learned representations
    and converging to a stable solution.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 从图7.17所示的损失图中，我们可以看到模型在训练过程中在训练集和验证集上的性能显著提高。在初始阶段损失值的快速下降表明模型迅速从数据中学习到有意义的模式和表示。然后，随着训练进展到第二个epoch，损失值继续下降但速度较慢，这表明模型正在微调其学习到的表示并收敛到一个稳定的解决方案。
- en: '![figure](../Images/7-17.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-17.png)'
- en: Figure 7.17 The training and validation loss trends over two epochs. The solid
    line represents the training loss, showing a sharp decrease before stabilizing,
    while the dotted line represents the validation loss, which follows a similar
    pattern.
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.17展示了两个epoch的训练和验证损失趋势。实线代表训练损失，显示在稳定之前急剧下降，而虚线代表验证损失，其模式类似。
- en: While the loss plot in figure 7.17 indicates that the model is training effectively,
    the most crucial aspect is its performance in terms of response quality and correctness.
    So, next, let’s extract the responses and store them in a format that allows us
    to evaluate and quantify the response quality.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图7.17中的损失图表明模型正在有效地训练，但最关键的是其在响应质量和正确性方面的表现。因此，接下来，我们将提取响应并将它们存储在一个允许我们评估和量化响应质量的格式中。
- en: Exercise 7.3 Fine-tuning on the original Alpaca dataset
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习7.3 在原始Alpaca数据集上进行微调
- en: The Alpaca dataset, by researchers at Stanford, is one of the earliest and most
    popular openly shared instruction datasets, consisting of 52,002 entries. As an
    alternative to the `instruction-data.json` file we use here, consider fine-tuning
    an LLM on this dataset. The dataset is available at [https://mng.bz/NBnE](https://mng.bz/NBnE).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福大学的研究人员制作的阿尔帕卡数据集是早期且最受欢迎的公开共享指令数据集之一，包含52,002条条目。作为我们这里使用的`instruction-data.json`文件的替代方案，可以考虑在这个数据集上微调一个大型语言模型。该数据集可在[https://mng.bz/NBnE](https://mng.bz/NBnE)找到。
- en: This dataset contains 52,002 entries, which is approximately 50 times more than
    those we used here, and most entries are longer. Thus, I highly recommend using
    a GPU to conduct the training, which will accelerate the fine-tuning process.
    If you encounter out-of-memory errors, consider reducing the `batch_size` from
    8 to 4, 2, or even 1\. Lowering the `allowed_max_length` from 1,024 to 512 or
    256 can also help manage memory problems.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含52,002条条目，大约是我们这里使用的条目数量的50倍，而且大多数条目更长。因此，我强烈建议使用GPU进行训练，这将加速微调过程。如果你遇到内存不足的错误，可以考虑将`batch_size`从8减少到4、2，甚至1。将`allowed_max_length`从1,024降低到512或256也可以帮助管理内存问题。
- en: 7.7 Extracting and saving responses
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.7 提取和保存响应
- en: Having fine-tuned the LLM on the training portion of the instruction dataset,
    we are now ready to evaluate its performance on the held-out test set. First,
    we extract the model-generated responses for each input in the test dataset and
    collect them for manual analysis, and then we evaluate the LLM to quantify the
    quality of the responses, as highlighted in figure 7.18.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在对指令数据集的训练部分微调LLM后，我们现在准备评估它在保留的测试集上的性能。首先，我们提取测试数据集中每个输入的模型生成的响应，并将它们收集起来进行人工分析，然后评估LLM以量化响应的质量，如图7.18所示。
- en: '![figure](../Images/7-18.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-18.png)'
- en: Figure 7.18 The three-stage process for instruction fine-tuning the LLM. In
    the first two steps of stage 3, we extract and collect the model responses on
    the held-out test dataset for further analysis and then evaluate the model to
    quantify the performance of the instruction-fine-tuned LLM.
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.18 指令微调LLM的三阶段过程。在第三阶段的前两步中，我们提取并收集了在保留的测试数据集上的模型响应，以进行进一步分析，然后评估模型以量化指令微调LLM的性能。
- en: 'To complete the response instruction step, we use the `generate` function.
    We then print the model responses alongside the expected test set answers for
    the first three test set entries, presenting them side by side for comparison:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成响应指令步骤，我们使用`generate`函数。然后，我们将模型响应与预期的前三个测试集条目的测试集答案并排打印出来，以便进行比较：
- en: '[PRE46]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '#1 Iterates over the first three test set samples'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 遍历前三个测试集样本'
- en: '#2 Uses the generate function imported in section 7.5'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用第7.5节中导入的`generate`函数'
- en: As mentioned earlier, the `generate` function returns the combined input and
    output text, so we use slicing and the `.replace()` method on the `generated_text`
    contents to extract the model’s response. The instructions, followed by the given
    test set response and model response, are shown next.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`generate`函数返回组合的输入和输出文本，因此我们使用切片和`.replace()`方法对`generated_text`内容进行操作，以提取模型的响应。接下来显示了指令、给定的测试集响应和模型响应。
- en: '![figure](../Images/7_prompt.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7_prompt.png)'
- en: As we can see based on the test set instructions, given responses, and the model’s
    responses, the model performs relatively well. The answers to the first and last
    instructions are clearly correct, while the second answer is close but not entirely
    accurate. The model answers with “cumulus cloud” instead of “cumulonimbus,” although
    it’s worth noting that cumulus clouds can develop into cumulonimbus clouds, which
    are capable of producing thunderstorms.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 根据测试集指令、给定响应和模型的响应，我们可以看到，模型的表现相对较好。第一和最后一条指令的答案是明显正确的，而第二条答案虽然接近但不完全准确。模型用“积云”而不是“积雨云”来回答，尽管值得注意的是，积云可以发展成为积雨云，而积雨云能够产生雷暴。
- en: 'Most importantly, model evaluation is not as straightforward as it is for classification
    fine-tuning, where we simply calculate the percentage of correct spam/non-spam
    class labels to obtain the classification’s accuracy. In practice, instruction-fine-tuned
    LLMs such as chatbots are evaluated via multiple approaches:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的，模型评估并不像分类微调那样直接，在分类微调中，我们只需计算正确垃圾邮件/非垃圾邮件类别标签的百分比来获得分类的准确率。在实践中，如聊天机器人这样的指令微调LLM通过多种方法进行评估：
- en: Short-answer and multiple-choice benchmarks, such as Measuring Massive Multitask
    Language Understanding (MMLU; [https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)),
    which test the general knowledge of a model.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简答题和多选题基准，如衡量大规模多任务语言理解（MMLU；[https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)），这些基准测试模型的一般知识。
- en: Human preference comparison to other LLMs, such as LMSYS chatbot arena ([https://arena.lmsys.org](https://arena.lmsys.org)).
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他LLM（如LMSYS聊天机器人竞技场 [https://arena.lmsys.org](https://arena.lmsys.org)）的人类偏好比较。
- en: Automated conversational benchmarks, where another LLM like GPT-4 is used to
    evaluate the responses, such as AlpacaEval ([https://tatsu-lab.github.io/alpaca_eval/](https://tatsu-lab.github.io/alpaca_eval/)).
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化对话基准测试，其中使用另一个LLM（如GPT-4）来评估响应，例如AlpacaEval ([https://tatsu-lab.github.io/alpaca_eval/](https://tatsu-lab.github.io/alpaca_eval/))。
- en: 'In practice, it can be useful to consider all three types of evaluation methods:
    multiple-choice question answering, human evaluation, and automated metrics that
    measure conversational performance. However, since we are primarily interested
    in assessing conversational performance rather than just the ability to answer
    multiple-choice questions, human evaluation and automated metrics may be more
    relevant.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，考虑所有三种类型的评估方法可能是有用的：多项选择题回答、人工评估和自动指标，这些指标衡量对话性能。然而，由于我们主要对评估对话性能感兴趣，而不仅仅是评估回答多项选择题的能力，因此人工评估和自动指标可能更为相关。
- en: Conversational performance
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话性能
- en: Conversational performance of LLMs refers to their ability to engage in human-like
    communication by understanding context, nuance, and intent. It encompasses skills
    such as providing relevant and coherent responses, maintaining consistency, and
    adapting to different topics and styles of interaction.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的对话性能指的是它们通过理解上下文、细微差别和意图来参与类似人类的交流的能力。它包括提供相关且连贯的响应、保持一致性以及适应不同主题和互动风格等技能。
- en: Human evaluation, while providing valuable insights, can be relatively laborious
    and time-consuming, especially when dealing with a large number of responses.
    For instance, reading and assigning ratings to all 1,100 responses would require
    a significant amount of effort.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 人工评估虽然能提供有价值的见解，但可能相对费时费力，尤其是在处理大量响应时。例如，阅读并给所有1,100个响应评分就需要相当大的努力。
- en: So, considering the scale of the task at hand, we will implement an approach
    similar to automated conversational benchmarks, which involves evaluating the
    responses automatically using another LLM. This method will allow us to efficiently
    assess the quality of the generated responses without the need for extensive human
    involvement, thereby saving time and resources while still obtaining meaningful
    performance indicators.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到手头任务的规模，我们将实施一种类似于自动化对话基准测试的方法，该方法涉及使用另一个LLM自动评估响应。这种方法将使我们能够高效地评估生成响应的质量，而无需大量的人类参与，从而节省时间和资源，同时仍然获得有意义的性能指标。
- en: Let’s employ an approach inspired by AlpacaEval, using another LLM to evaluate
    our fine-tuned model’s responses. However, instead of relying on a publicly available
    benchmark dataset, we use our own custom test set. This customization allows for
    a more targeted and relevant assessment of the model’s performance within the
    context of our intended use cases, represented in our instruction dataset.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们采用受AlpacaEval启发的方案，使用另一个LLM来评估我们微调模型的响应。然而，我们不是依赖于公开可用的基准数据集，而是使用我们自己的定制测试集。这种定制化允许我们在我们预期的用例背景下，对模型性能进行更有针对性和相关的评估，这些用例在我们的指令数据集中表示。
- en: To prepare the responses for this evaluation process, we append the generated
    model responses to the `test_set` dictionary and save the updated data as an `"instruction-data-with-response.json"`
    file for record keeping. Additionally, by saving this file, we can easily load
    and analyze the responses in separate Python sessions later on if needed.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备此评估过程的响应，我们将生成的模型响应追加到`test_set`字典中，并将更新后的数据保存为`"instruction-data-with-response.json"`文件以供记录。此外，通过保存此文件，我们可以在需要时轻松地加载和分析响应，以便在单独的Python会话中进行。
- en: The following code listing uses the `generate` method in the same manner as
    before; however, we now iterate over the entire `test_set`. Also, instead of printing
    the model responses, we add them to the `test_set` dictionary.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码列表使用与之前相同的方式调用`generate`方法；然而，我们现在遍历整个`test_set`。此外，我们不再打印模型响应，而是将它们添加到`test_set`字典中。
- en: Listing 7.9 Generating test set responses
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.9 生成测试集响应
- en: '[PRE47]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '#1 indent for pretty-printing'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用于美化打印的缩进'
- en: 'Processing the dataset takes about 1 minute on an A100 GPU and 6 minutes on
    an M3 MacBook Air:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在A100 GPU上处理数据集大约需要1分钟，在M3 MacBook Air上需要6分钟：
- en: '[PRE48]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let’s verify that the responses have been correctly added to the `test_set`
    dictionary by examining one of the entries:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过检查`test_set`字典中的一个条目来验证响应是否已正确添加：
- en: '[PRE49]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output shows that the `model_response` has been added correctly:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示`model_response`已正确添加：
- en: '[PRE50]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, we save the model as `gpt2-medium355M-sft.pth` file to be able to
    reuse it in future projects:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将模型保存为`gpt2-medium355M-sft.pth`文件，以便在未来的项目中重用：
- en: '[PRE51]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '#1 Removes white spaces and parentheses from file name'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从文件名中删除空白和括号'
- en: The saved model can then be loaded via `model.load_state_dict(torch.load("gpt2
    -medium355M-sft.pth")`).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 保存的模型可以通过`model.load_state_dict(torch.load("gpt2 -medium355M-sft.pth"))`加载。
- en: 7.8 Evaluating the fine-tuned LLM
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.8 评估微调LLM
- en: Previously, we judged the performance of an instruction-fine-tuned model by
    looking at its responses on three examples of the test set. While this gives us
    a rough idea of how well the model performs, this method does not scale well to
    larger amounts of responses. So, we implement a method to automate the response
    evaluation of the fine-tuned LLM using another, larger LLM, as highlighted in
    figure 7.19.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们通过查看测试集三个示例的响应来判断指令微调模型的性能。虽然这让我们对模型的表现有一个大致的了解，但这种方法并不适用于大量响应。因此，我们实现了一种方法，使用另一个更大的LLM来自动评估微调LLM的响应，如图7.19所示。
- en: '![figure](../Images/7-19.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-19.png)'
- en: Figure 7.19 The three-stage process for instruction fine-tuning the LLM. In
    this last step of the instruction-fine-tuning pipeline, we implement a method
    to quantify the performance of the fine-tuned model by scoring the responses it
    generated for the test.
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.19 指令微调LLM的三阶段过程。在这个指令微调管道的最后一步，我们实现了一种方法，通过评分测试中生成的响应来量化微调模型的表现。
- en: To evaluate test set responses in an automated fashion, we utilize an existing
    instruction-fine-tuned 8-billion-parameter Llama 3 model developed by Meta AI.
    This model can be run locally using the open source Ollama application ([https://ollama.com](https://ollama.com)).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以自动化的方式评估测试集响应，我们利用由Meta AI开发的现有的指令微调的80亿参数Llama 3模型。此模型可以使用开源的Ollama应用程序([https://ollama.com](https://ollama.com))本地运行。
- en: NOTE  Ollama is an efficient application for running LLMs on a laptop. It serves
    as a wrapper around the open source llama.cpp library ([https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)),
    which implements LLMs in pure C/C++ to maximize efficiency. However, Ollama is
    only a tool for generating text using LLMs (inference) and does not support training
    or fine-tuning LLMs.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：Ollama是一个在笔记本电脑上运行LLM的高效应用程序。它作为开源的llama.cpp库的包装器([https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp))，该库使用纯C/C++实现LLM以最大化效率。然而，Ollama仅是一个用于使用LLM生成文本的工具（推理），不支持训练或微调LLM。
- en: Using larger LLMs via web APIs
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过Web API使用更大的LLM
- en: The 8-billion-parameter Llama 3 model is a very capable LLM that runs locally.
    However, it’s not as capable as large proprietary LLMs such as GPT-4 offered by
    OpenAI. For readers interested in exploring how to utilize GPT-4 through the OpenAI
    API to assess generated model responses, an optional code notebook is available
    within the supplementary materials accompanying this book at [https://mng.bz/BgEv](https://mng.bz/BgEv).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 80亿参数的Llama 3模型是一个非常强大的本地运行的LLM。然而，它并不像OpenAI提供的GPT-4这样的大型专有LLM那样强大。对于有兴趣探索如何通过OpenAI
    API利用GPT-4来评估生成模型响应的读者，本书的补充材料中提供了一个可选的代码笔记本，可在[https://mng.bz/BgEv](https://mng.bz/BgEv)找到。
- en: 'To execute the following code, install Ollama by visiting [https://ollama.com](https://ollama.com)and
    follow the provided instructions for your operating system:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行以下代码，请访问[https://ollama.com](https://ollama.com)安装Ollama，并按照为您的操作系统提供的说明进行操作：
- en: '*For macOS and Windows users*—Open the downloaded Ollama application. If prompted
    to install command-line usage, select Yes.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对于macOS和Windows用户*—打开下载的Ollama应用程序。如果提示安装命令行使用，请选择是。'
- en: '*For Linux users*—Use the installation command available on the Ollama website.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对于Linux用户*—使用Ollama网站上的安装命令。'
- en: Before implementing the model evaluation code, let’s first download the Llama
    3 model and verify that Ollama is functioning correctly by using it from the command-line
    terminal. To use Ollama from the command line, you must either start the Ollama
    application or run `ollama` `serve` in a separate terminal, as shown in figure
    7.20.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现模型评估代码之前，让我们首先下载 Llama 3 模型，并通过使用命令行终端来验证 Ollama 是否正常工作。要从命令行使用 Ollama，您必须启动
    Ollama 应用程序或在单独的终端中运行 `ollama` `serve`，如图 7.20 所示。
- en: '![figure](../Images/7-20.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-20.png)'
- en: Figure 7.20 Two options for running Ollama. The left panel illustrates starting
    Ollama using `ollama` `serve`. The right panel shows a second option in macOS,
    running the Ollama application in the background instead of using the `ollama`
    `serve` command to start the application.
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.20 运行 Ollama 的两种选项。左侧面板说明了使用 `ollama` `serve` 启动 Ollama。右侧面板显示了 macOS 中的第二种选项，在后台运行
    Ollama 应用程序而不是使用 `ollama` `serve` 命令来启动应用程序。
- en: 'With the Ollama application or `ollama` `serve` running in a different terminal,
    execute the following command on the command line (not in a Python session) to
    try out the 8-billion-parameter Llama 3 model:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的终端中运行 Ollama 应用程序或 `ollama` `serve` 时，请在命令行（而不是 Python 会话）中执行以下命令，以尝试 80
    亿参数的 Llama 3 模型：
- en: '[PRE52]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The first time you execute this command, this model, which takes up 4.7 GB
    of storage space, will be automatically downloaded. The output looks like the
    following:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次执行此命令时，这个占用 4.7 GB 存储空间的模型将被自动下载。输出如下所示：
- en: '[PRE53]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Alternative Ollama models
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 替代 Ollama 模型
- en: The `llama3` in the `ollama` `run` `llama3` command refers to the instruction-fine-tuned
    8-billion-parameter Llama 3 model. Using Ollama with the `llama3` model requires
    approximately 16 GB of RAM. If your machine does not have sufficient RAM, you
    can try using a smaller model, such as the 3.8-billion-parameter `phi3` model
    via `ollama` `run` `llama3`, which only requires around 8 GB of RAM.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '`ollama` `run` `llama3` 命令中的 `llama3` 指的是指令微调的 80 亿参数 Llama 3 模型。使用 Ollama
    和 `llama3` 模型大约需要 16 GB 的 RAM。如果您的机器没有足够的 RAM，您可以通过 `ollama` `run` `llama3` 使用较小的模型，例如
    38 亿参数的 `phi3` 模型，这只需要大约 8 GB 的 RAM。'
- en: For more powerful computers, you can also use the larger 70-billion-parameter
    Llama 3 model by replacing `llama3` with `llama3:70b`. However, this model requires
    significantly more computational resources.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更强大的计算机，您也可以使用更大的 70 亿参数的 Llama 3 模型，通过将 `llama3` 替换为 `llama3:70b` 来实现。然而，这个模型需要显著更多的计算资源。
- en: Once the model download is complete, we are presented with a command-line interface
    that allows us to interact with the model. For example, try asking the model,
    “What do llamas eat?”
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 模型下载完成后，我们将看到一个命令行界面，允许我们与模型交互。例如，尝试询问模型：“骆驼吃什么？”
- en: '[PRE54]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Note that the response you see might differ since Ollama is not deterministic
    as of this writing.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您看到的响应可能会有所不同，因为截至本文撰写时，Ollama 不是一个确定性的系统。
- en: You can end this `ollama` `run` `llama3` session using the input `/bye`. However,
    make sure to keep the `ollama` `serve` command or the Ollama application running
    for the remainder of this chapter.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用输入 `/bye` 结束此 `ollama` `run` `llama3` 会话。但是，请确保在本章剩余部分保持 `ollama` `serve`
    命令或 Ollama 应用程序运行。
- en: 'The following code verifies that the Ollama session is running properly before
    we use Ollama to evaluate the test set responses:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在我们在 Ollama 中评估测试集响应之前，验证 Ollama 会话是否正常运行：
- en: '[PRE55]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Ensure that the output from executing the previous code displays `Ollama` `running:`
    `True`. If it shows `False`, verify that the `ollama` `serve` command or the Ollama
    application is actively running.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 确保执行前面的代码后显示 `Ollama` `running:` `True`。如果显示 `False`，请验证 `ollama` `serve` 命令或
    Ollama 应用程序是否正在积极运行。
- en: Running the code in a new Python session
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在新的 Python 会话中运行代码
- en: 'If you already closed your Python session or if you prefer to execute the remaining
    code in a different Python session, use the following code, which loads the instruction
    and response data file we previously created and redefines the `format_input`
    function we used earlier (the `tqdm` progress bar utility is used later):'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经关闭了 Python 会话，或者您更喜欢在不同的 Python 会话中执行剩余的代码，请使用以下代码，该代码加载我们之前创建的指令和响应数据文件，并重新定义我们之前使用的
    `format_input` 函数（稍后使用 `tqdm` 进度条实用程序）：
- en: '[PRE56]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: An alternative to the `ollama` `run` command for interacting with the model
    is through its REST API using Python. The `query_model` function shown in the
    following listing demonstrates how to use the API.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `ollama` `run` 命令交互模型的另一种方法是使用 Python 通过其 REST API。以下列表中显示的 `query_model`
    函数演示了如何使用该 API。
- en: Listing 7.10 Querying a local Ollama model
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.10 查询本地 Ollama 模型
- en: '[PRE57]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '#1 Creates the data payload as a dictionary'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建数据有效负载作为字典'
- en: '#2 Settings for deterministic responses'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 确定响应的设置'
- en: '#3 Converts the dictionary to a JSON-formatted string and encodes it to bytes'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将字典转换为 JSON 格式的字符串，并编码为字节'
- en: '#4 Creates a request object, setting the method to POST and adding necessary
    headers'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 创建请求对象，设置方法为 POST 并添加必要的头信息'
- en: '#5 Sends the request and captures the response'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 发送请求并捕获响应'
- en: Before running the subsequent code cells in this notebook, ensure that Ollama
    is still running. The previous code cells should print `"Ollama` `running:` `True"`
    to confirm that the model is active and ready to receive requests.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此笔记本中的后续代码单元之前，请确保 Ollama 仍在运行。之前的代码单元应该打印 `"Ollama` `running:` `True"` 以确认模型处于活动状态并准备好接收请求。
- en: 'The following is an example of how to use the `query_model` function we just
    implemented:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用我们刚刚实现的 `query_model` 函数的示例：
- en: '[PRE58]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The resulting response is as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的响应如下：
- en: '[PRE59]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Using the `query_model` function defined earlier, we can evaluate the responses
    generated by our fine-tuned model that prompts the Llama 3 model to rate our fine-tuned
    model’s responses on a scale from 0 to 100 based on the given test set response
    as reference.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前定义的 `query_model` 函数，我们可以评估由我们的微调模型生成的响应，该模型提示 Llama 3 模型根据给定的测试集响应作为参考，对微调模型的响应进行从
    0 到 100 的评分。
- en: 'First, we apply this approach to the first three examples from the test set
    that we previously examined:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将这种方法应用于我们之前检查的前三个测试集示例：
- en: '[PRE60]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This code prints outputs similar to the following (as of this writing, Ollama
    is not fully deterministic, so the generated texts may vary):'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码打印的输出类似于以下内容（截至本文撰写时，Ollama 不是一个完全确定性的模型，因此生成的文本可能会有所不同）：
- en: '![figure](../Images/7_prompt2.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7_prompt2.png)'
- en: The generated responses show that the Llama 3 model provides reasonable evaluations
    and is capable of assigning partial points when a model’s answer is not entirely
    correct. For instance, if we consider the evaluation of the “cumulus cloud” answer,
    the model acknowledges the partial correctness of the response.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的响应显示，Llama 3 模型提供了合理的评估，并且能够在模型答案不完全正确时分配部分分数。例如，如果我们考虑“积云”答案的评估，模型承认响应的部分正确性。
- en: The previous prompt returns highly detailed evaluations in addition to the score.
    We can modify the prompt to just generate integer scores ranging from 0 to 100,
    where 100 represents the best possible score. This modification allows us to calculate
    an average score for our model, which serves as a more concise and quantitative
    assessment of its performance. The `generate_model_scores` function shown in the
    following listing uses a modified prompt telling the model to `"Respond` `with`
    `the` `integer` `number` `only."`
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的提示除了得分外还返回了高度详细的分析。我们可以修改提示，只生成从 0 到 100 的整数得分，其中 100 代表最佳可能得分。这种修改使我们能够计算我们模型的平均得分，这作为对其性能的更简洁和定量的评估。以下列表中显示的
    `generate_model_scores` 函数使用修改后的提示告诉模型“仅`Respond` `with` `the` `integer` `number`。”
- en: Listing 7.11 Evaluating the instruction fine-tuning LLM
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.11 评估指令微调 LLM
- en: '[PRE61]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '#1 Modified instruction line to only return the score'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 修改指令行以仅返回得分'
- en: 'Let’s now apply the `generate_model_scores` function to the entire `test_data`
    set, which takes about 1 minute on a M3 Macbook Air:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将 `generate_model_scores` 函数应用于整个 `test_data` 集合，这在 M3 Macbook Air 上大约需要
    1 分钟：
- en: '[PRE62]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The results are as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE63]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: The evaluation output shows that our fine-tuned model achieves an average score
    above 50, which provides a useful benchmark for comparison against other models
    or for experimenting with different training configurations to improve the model’s
    performance.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 评估输出显示，我们的微调模型平均得分超过 50，这为与其他模型进行比较或尝试不同的训练配置以提高模型性能提供了一个有用的基准。
- en: It’s worth noting that Ollama is not entirely deterministic across operating
    systems at the time of this writing, which means that the scores you obtain might
    vary slightly from the previous scores. To obtain more robust results, you can
    repeat the evaluation multiple times and average the resulting scores.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在撰写本文时，Ollama在操作系统上并不完全确定，这意味着你获得的分数可能与之前的分数略有不同。为了获得更稳健的结果，你可以多次重复评估并平均结果分数。
- en: To further improve our model’s performance, we can explore various strategies,
    such as
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高我们模型的表现，我们可以探索各种策略，例如
- en: Adjusting the hyperparameters during fine-tuning, such as the learning rate,
    batch size, or number of epochs
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在微调期间调整超参数，如学习率、批量大小或epoch数量
- en: Increasing the size of the training dataset or diversifying the examples to
    cover a broader range of topics and styles
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加训练数据集的大小或多样化示例以涵盖更广泛的主题和风格
- en: Experimenting with different prompts or instruction formats to guide the model’s
    responses more effectively
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的提示或指令格式来更有效地引导模型的响应
- en: Using a larger pretrained model, which may have greater capacity to capture
    complex patterns and generate more accurate responses
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更大的预训练模型，这可能具有更大的能力来捕捉复杂模式并生成更准确的响应
- en: NOTE  For reference, when using the methodology described herein, the Llama
    3 8B base model, without any fine-tuning, achieves an average score of 58.51 on
    the test set. The Llama 3 8B instruct model, which has been fine-tuned on a general
    instruction-following dataset, achieves an impressive average score of 82.6.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：为了参考，当使用本文中描述的方法时，未经任何微调的Llama 3 8B基础模型在测试集上实现了平均分数58.51。经过在通用指令遵循数据集上微调的Llama
    3 8B指令模型，实现了令人印象深刻的平均分数82.6。
- en: Exercise 7.4 Parameter-efficient fine-tuning with LoRA
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习7.4 使用LoRA进行参数高效的微调
- en: To instruction fine-tune an LLM more efficiently, modify the code in this chapter
    to use the low-rank adaptation method (LoRA) from appendix E. Compare the training
    run time and model performance before and after the modification.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更有效地微调一个LLM，修改本章中的代码以使用附录E中的低秩自适应方法（LoRA）。比较修改前后的训练运行时间和模型性能。
- en: 7.9 Conclusions
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.9 结论
- en: This chapter marks the conclusion of our journey through the LLM development
    cycle. We have covered all the essential steps, including implementing an LLM
    architecture, pretraining an LLM, and fine-tuning it for specific tasks, as summarized
    in figure 7.21\. Let’s discuss some ideas for what to look into next.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 本章标志着我们通过LLM开发周期的旅程的结束。我们已经涵盖了所有基本步骤，包括实现LLM架构、预训练LLM以及针对特定任务进行微调，如7.21图所示。让我们讨论一下接下来要关注的一些想法。
- en: '![figure](../Images/7-21.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-21.png)'
- en: Figure 7.21 The three main stages of coding an LLM.
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.21 编写LLM的三个主要阶段。
- en: 7.9.1 What’s next?
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.9.1 接下来是什么？
- en: 'While we covered the most essential steps, there is an optional step that can
    be performed after instruction fine-tuning: preference fine-tuning. Preference
    fine-tuning is particularly useful for customizing a model to better align with
    specific user preferences. If you are interested in exploring this further, see
    the `04_preference-tuning-with-dpo` folder in this book’s supplementary GitHub
    repository at [https://mng.bz/dZwD](https://mng.bz/dZwD).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经涵盖了最重要的步骤，但在指令微调之后还有一个可选步骤：偏好微调。偏好微调特别有用，可以帮助定制模型以更好地符合特定用户偏好。如果你对此感兴趣，请参阅本书补充GitHub仓库中的`04_preference-tuning-with-dpo`文件夹，网址为[https://mng.bz/dZwD](https://mng.bz/dZwD)。
- en: 'In addition to the main content covered in this book, the GitHub repository
    also contains a large selection of bonus material that you may find valuable.
    To learn more about these additional resources, visit the Bonus Material section
    on the repository’s README page: [https://mng.bz/r12g](https://mng.bz/r12g).'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 除了本书涵盖的主要内容外，GitHub仓库还包含大量你可能觉得有价值的额外材料。要了解更多关于这些额外资源的信息，请访问仓库的README页面上的“额外材料”部分：[https://mng.bz/r12g](https://mng.bz/r12g)。
- en: 7.9.2 Staying up to date in a fast-moving field
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.9.2 在快速发展的领域中保持最新
- en: The fields of AI and LLM research are evolving at a rapid (and, depending on
    who you ask, exciting) pace. One way to keep up with the latest advancements is
    to explore recent research papers on arXiv at [https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent).
    Additionally, many researchers and practitioners are very active in sharing and
    discussing the latest developments on social media platforms like X (formerly
    Twitter) and Reddit. The subreddit r/LocalLLaMA, in particular, is a good resource
    for connecting with the community and staying informed about the latest tools
    and trends. I also regularly share insights and write about the latest in LLM
    research on my blog, available at [https://magazine.sebastianraschka.com](https://magazine.sebastianraschka.com)
    and [https://sebastianraschka.com/blog/](https://sebastianraschka.com/blog/).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能和LLM研究领域正在迅速（并且，根据您询问的人，令人兴奋）地发展。跟上最新进展的一种方式是探索arXiv上的最新研究论文，网址为[https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent)。此外，许多研究人员和实践者都在社交媒体平台如X（前身为Twitter）和Reddit上非常活跃地分享和讨论最新的发展。特别是，r/LocalLLaMA
    subreddir是一个很好的资源，用于与社区建立联系并了解最新的工具和趋势。我还在我的博客上定期分享见解并撰写关于LLM研究的最新内容，博客地址为[https://magazine.sebastianraschka.com](https://magazine.sebastianraschka.com)和[https://sebastianraschka.com/blog/](https://sebastianraschka.com/blog/)。
- en: 7.9.3 Final words
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.9.3 最后的话
- en: I hope you have enjoyed this journey of implementing an LLM from the ground
    up and coding the pretraining and fine-tuning functions from scratch. In my opinion,
    building an LLM from scratch is the most effective way to gain a deep understanding
    of how LLMs work. I hope that this hands-on approach has provided you with valuable
    insights and a solid foundation in LLM development.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望您已经享受了从零开始实现LLM并从头编写预训练和微调函数的旅程。在我看来，从头开始构建LLM是最有效地深入了解LLM工作原理的方法。我希望这种动手方法为您提供了宝贵的见解和LLM开发的坚实基础。
- en: While the primary purpose of this book is educational, you may be interested
    in utilizing different and more powerful LLMs for real-world applications. For
    this, I recommend exploring popular tools such as Axolotl ([https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl))
    or LitGPT ([https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt)),
    which I am actively involved in developing.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本书的主要目的是教育性的，您可能对利用不同且更强大的LLMs进行实际应用感兴趣。为此，我建议探索流行的工具，如Axolotl ([https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl))
    或 LitGPT ([https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt))，我积极参与了这些工具的开发。
- en: Thank you for joining me on this learning journey, and I wish you all the best
    in your future endeavors in the exciting field of LLMs and AI!
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您与我一同踏上这段学习之旅，并祝您在LLMs和AI这个激动人心的领域中未来的努力一切顺利！
- en: Summary
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The instruction-fine-tuning process adapts a pretrained LLM to follow human
    instructions and generate desired responses.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令微调过程将预训练的LLM调整为遵循人类指令并生成期望的响应。
- en: Preparing the dataset involves downloading an instruction-response dataset,
    formatting the entries, and splitting it into train, validation, and test sets.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据集涉及下载指令-响应数据集，格式化条目，并将其分为训练集、验证集和测试集。
- en: Training batches are constructed using a custom collate function that pads sequences,
    creates target token IDs, and masks padding tokens.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练批次是通过一个自定义的collate函数构建的，该函数填充序列，创建目标标记ID，并屏蔽填充标记。
- en: We load a pretrained GPT-2 medium model with 355 million parameters to serve
    as the starting point for instruction fine-tuning.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们加载了一个预训练的GPT-2中等模型，包含3.55亿个参数，作为指令微调的起点。
- en: The pretrained model is fine-tuned on the instruction dataset using a training
    loop similar to pretraining.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用与预训练类似的训练循环，在指令数据集上对预训练模型进行微调。
- en: Evaluation involves extracting model responses on a test set and scoring them
    (for example, using another LLM).
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估涉及从测试集中提取模型响应并对其进行评分（例如，使用另一个LLM）。
- en: The Ollama application with an 8-billion-parameter Llama model can be used to
    automatically score the fine-tuned model’s responses on the test set, providing
    an average score to quantify performance.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配备了80亿参数的Llama模型的Ollama应用程序可以用于自动评分测试集中微调模型的响应，提供一个平均分数来量化性能。
