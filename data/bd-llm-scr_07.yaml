- en: appendix A Introduction to PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录A PyTorch简介
- en: This appendix is designed to equip you with the necessary skills and knowledge
    to put deep learning into practice and implement large language models (LLMs)
    from scratch. PyTorch, a popular Python-based deep learning library, will be our
    primary tool for this book. I will guide you through setting up a deep learning
    workspace armed with PyTorch and GPU support.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录旨在为您提供将深度学习应用于实践并从头开始实现大型语言模型（LLMs）所需的基本技能和知识。PyTorch，一个流行的基于Python的深度学习库，将是本书的主要工具。我将指导您使用PyTorch和GPU支持设置深度学习工作空间。
- en: Then you’ll learn about the essential concept of tensors and their usage in
    PyTorch. We will also delve into PyTorch’s automatic differentiation engine, a
    feature that enables us to conveniently and efficiently use backpropagation, which
    is a crucial aspect of neural network training.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将了解张量这一基本概念及其在PyTorch中的使用。我们还将深入了解PyTorch的自动微分引擎，这是一个使我们能够方便且高效地使用反向传播的功能，而反向传播是神经网络训练的关键方面。
- en: This appendix is meant as a primer for those new to deep learning in PyTorch.
    While it explains PyTorch from the ground up, it’s not meant to be an exhaustive
    coverage of the PyTorch library. Instead, we’ll focus on the PyTorch fundamentals
    we will use to implement LLMs. If you are already familiar with deep learning,
    you may skip this appendix and directly move on to chapter 2.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录旨在为那些刚开始使用PyTorch进行深度学习的人提供入门指南。虽然它从底层解释了PyTorch，但它并不旨在全面覆盖PyTorch库。相反，我们将专注于我们将用于实现LLMs的PyTorch基础知识。如果您已经熟悉深度学习，您可以跳过本附录，直接进入第2章。
- en: A.1 What is PyTorch?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.1 什么是PyTorch？
- en: PyTorch ([https://pytorch.org/](https://pytorch.org/)) is an open source Python-based
    deep learning library. According to *Papers With Code* ([https://paperswithcode.com/trends](https://paperswithcode.com/trends)),
    a platform that tracks and analyzes research papers, PyTorch has been the most
    widely used deep learning library for research since 2019 by a wide margin. And,
    according to the *Kaggle Data Science and Machine Learning Survey 2022* ([https://www.kaggle.com/c/kaggle-survey-2022](https://www.kaggle.com/c/kaggle-survey-2022)),
    the number of respondents using PyTorch is approximately 40%, which grows every
    year.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch ([https://pytorch.org/](https://pytorch.org/)) 是一个开源的基于Python的深度学习库。根据
    *Papers With Code* ([https://paperswithcode.com/trends](https://paperswithcode.com/trends))
    平台，该平台跟踪和分析研究论文，PyTorch自2019年以来一直是研究中最广泛使用的深度学习库，并且差距很大。根据 *Kaggle数据科学和机器学习调查2022*
    ([https://www.kaggle.com/c/kaggle-survey-2022](https://www.kaggle.com/c/kaggle-survey-2022))，使用PyTorch的受访者数量大约为40%，并且每年都在增长。
- en: One of the reasons PyTorch is so popular is its user-friendly interface and
    efficiency. Despite its accessibility, it doesn’t compromise on flexibility, allowing
    advanced users to tweak lower-level aspects of their models for customization
    and optimization. In short, for many practitioners and researchers, PyTorch offers
    just the right balance between usability and features.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch之所以如此受欢迎，其中一个原因就是其用户友好的界面和效率。尽管它易于访问，但它并没有在灵活性上妥协，允许高级用户调整模型的高级方面以进行定制和优化。简而言之，对于许多实践者和研究人员来说，PyTorch在可用性和功能之间提供了恰到好处的平衡。
- en: A.1.1 The three core components of PyTorch
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1.1 PyTorch的三个核心组件
- en: PyTorch is a relatively comprehensive library, and one way to approach it is
    to focus on its three broad components, summarized in figure A.1\.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是一个相对全面的库，一种接近它的方法是关注其三个主要组件，如图A.1所示。
- en: '![figure](../Images/A-1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/A-1.png)'
- en: Figure A.1 PyTorch’s three main components include a tensor library as a fundamental
    building block for computing, automatic differentiation for model optimization,
    and deep learning utility functions, making it easier to implement and train deep
    neural network models.
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.1 PyTorch的三个主要组件包括作为计算基本构建块的张量库、用于模型优化的自动微分以及深度学习实用函数，这使得实现和训练深度神经网络模型变得更加容易。
- en: First, PyTorch is a *tensor library* that extends the concept of the array-oriented
    programming library NumPy with the additional feature that accelerates computation
    on GPUs, thus providing a seamless switch between CPUs and GPUs. Second, PyTorch
    is an *automatic differentiation engine*, also known as autograd, that enables
    the automatic computation of gradients for tensor operations, simplifying backpropagation
    and model optimization. Finally, PyTorch is a *deep learning library*. It offers
    modular, flexible, and efficient building blocks, including pretrained models,
    loss functions, and optimizers, for designing and training a wide range of deep
    learning models, catering to both researchers and developers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，PyTorch是一个*张量库*，它扩展了数组导向编程库NumPy的概念，并增加了加速GPU上计算的功能，从而在CPU和GPU之间提供无缝切换。其次，PyTorch是一个*自动微分引擎*，也称为autograd，它能够自动计算张量操作的梯度，简化了反向传播和模型优化。最后，PyTorch是一个*深度学习库*。它提供了模块化、灵活且高效的构建块，包括预训练模型、损失函数和优化器，用于设计和训练各种深度学习模型，满足研究人员和开发者的需求。
- en: A.1.2 Defining deep learning
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1.2 定义深度学习
- en: In the news, LLMs are often referred to as AI models. However, LLMs are also
    a type of deep neural network, and PyTorch is a deep learning library. Sound confusing?
    Let’s take a brief moment and summarize the relationship between these terms before
    we proceed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在新闻中，大型语言模型（LLMs）通常被称为AI模型。然而，LLMs也是一种深度神经网络，PyTorch是一个深度学习库。听起来很复杂？在我们继续之前，让我们简要总结一下这些术语之间的关系。
- en: '*AI* is fundamentally about creating computer systems capable of performing
    tasks that usually require human intelligence. These tasks include understanding
    natural language, recognizing patterns, and making decisions. (Despite significant
    progress, AI is still far from achieving this level of general intelligence.)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能*的基本目标是创建能够执行通常需要人类智能的任务的计算机系统。这些任务包括理解自然语言、识别模式和做出决策。（尽管取得了重大进展，但AI距离达到这种通用智能水平还有很长的路要走。）'
- en: '*Machine learning* represents a subfield of AI, as illustrated in figure A.2,
    that focuses on developing and improving learning algorithms. The key idea behind
    machine learning is to enable computers to learn from data and make predictions
    or decisions without being explicitly programmed to perform the task. This involves
    developing algorithms that can identify patterns, learn from historical data,
    and improve their performance over time with more data and feedback.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习*是AI的一个子领域，如图A.2所示，它专注于开发和改进学习算法。机器学习背后的关键思想是使计算机能够从数据中学习并做出预测或决策，而无需明确编程来执行该任务。这涉及到开发能够识别模式、从历史数据中学习，并在更多数据和反馈的帮助下随着时间的推移提高其性能的算法。'
- en: '![figure](../Images/A-2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/A-2.png)'
- en: Figure A.2 Deep learning is a subcategory of machine learning focused on implementing
    deep neural networks. Machine learning is a subcategory of AI that is concerned
    with algorithms that learn from data. AI is the broader concept of machines being
    able to perform tasks that typically require human intelligence.
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.2 深度学习是机器学习的一个子类别，专注于实现深度神经网络。机器学习是AI的一个子类别，它关注的是从数据中学习的算法。AI是更广泛的概念，即机器能够执行通常需要人类智能的任务。
- en: Machine learning has been integral in the evolution of AI, powering many of
    the advancements we see today, including LLMs. Machine learning is also behind
    technologies like recommendation systems used by online retailers and streaming
    services, email spam filtering, voice recognition in virtual assistants, and even
    self-driving cars. The introduction and advancement of machine learning have significantly
    enhanced AI’s capabilities, enabling it to move beyond strict rule-based systems
    and adapt to new inputs or changing environments.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习一直是AI演变的关键，推动了今天我们所看到的许多进步，包括LLMs。机器学习还支持在线零售商和流媒体服务使用的推荐系统、电子邮件垃圾邮件过滤、虚拟助手中的语音识别，甚至自动驾驶汽车等技术。机器学习的引入和进步显著增强了AI的能力，使其能够超越严格的基于规则的系统，并适应新的输入或不断变化的环境。
- en: '*Deep learning* is a subcategory of machine learning that focuses on the training
    and application of deep neural networks. These deep neural networks were originally
    inspired by how the human brain works, particularly the interconnection between
    many neurons. The “deep” in deep learning refers to the multiple hidden layers
    of artificial neurons or nodes that allow them to model complex, nonlinear relationships
    in the data. Unlike traditional machine learning techniques that excel at simple
    pattern recognition, deep learning is particularly good at handling unstructured
    data like images, audio, or text, so it is particularly well suited for LLMs.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习*是机器学习的一个子类别，它专注于深度神经网络的训练和应用。这些深度神经网络最初是受人类大脑工作方式的启发，特别是许多神经元之间的相互连接。深度学习中的“深度”指的是人工神经元或节点的多层隐藏层，这使得它们能够模拟数据中的复杂、非线性关系。与擅长简单模式识别的传统机器学习技术不同，深度学习特别擅长处理非结构化数据，如图像、音频或文本，因此它特别适合LLMs。'
- en: The typical predictive modeling workflow (also referred to as *supervised learning*)
    in machine learning and deep learning is summarized in figure A.3\.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和深度学习中的典型预测建模工作流程（也称为*监督学习*）在图A.3中进行了总结。
- en: '![figure](../Images/A-3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/A-3.png)'
- en: Figure A.3 The supervised learning workflow for predictive modeling consists
    of a training stage where a model is trained on labeled examples in a training
    dataset. The trained model can then be used to predict the labels of new observations.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.3 预测建模的监督学习工作流程包括一个训练阶段，在这个阶段，模型在训练数据集上的标记示例上进行训练。训练好的模型随后可以用来预测新观察结果的标签。
- en: Using a learning algorithm, a model is trained on a training dataset consisting
    of examples and corresponding labels. In the case of an email spam classifier,
    for example, the training dataset consists of emails and their “spam” and “not
    spam” labels that a human identified. Then the trained model can be used on new
    observations (i.e., new emails) to predict their unknown label (“spam” or “not
    spam”). Of course, we also want to add a model evaluation between the training
    and inference stages to ensure that the model satisfies our performance criteria
    before using it in a real-world application.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习算法，模型在由示例及其对应标签组成的训练数据集上进行训练。例如，在电子邮件垃圾邮件分类器的情况下，训练数据集包括电子邮件及其人类识别的“垃圾邮件”和“非垃圾邮件”标签。然后，训练好的模型可以用于新的观察结果（即新的电子邮件）来预测它们的未知标签（“垃圾邮件”或“非垃圾邮件”）。当然，我们还想在训练和推理阶段之间添加模型评估，以确保在将其用于实际应用之前，模型满足我们的性能标准。
- en: If we train LLMs to classify texts, the workflow for training and using LLMs
    is similar to that depicted in figure A.3\. If we are interested in training LLMs
    to generate texts, which is our main focus, figure A.3 still applies. In this
    case, the labels during pretraining can be derived from the text itself (the next-word
    prediction task introduced in chapter 1). The LLM will generate entirely new text
    (instead of predicting labels), given an input prompt during inference.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练LLMs来对文本进行分类，训练和使用LLMs的工作流程与图A.3中描述的类似。如果我们对训练LLMs生成文本感兴趣，这是我们主要关注的焦点，图A.3仍然适用。在这种情况下，预训练期间的标签可以从文本本身（第1章中引入的下一词预测任务）中推导出来。在推理期间，LLM将根据输入提示生成全新的文本（而不是预测标签）。
- en: A.1.3 Installing PyTorch
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1.3 安装PyTorch
- en: PyTorch can be installed just like any other Python library or package. However,
    since PyTorch is a comprehensive library featuring CPU- and GPU-compatible codes,
    the installation may require additional explanation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的安装方法与其他Python库或包类似。然而，由于PyTorch是一个包含CPU和GPU兼容代码的综合库，安装可能需要额外的解释。
- en: Python version
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Python版本
- en: Many scientific computing libraries do not immediately support the newest version
    of Python. Therefore, when installing PyTorch, it’s advisable to use a version
    of Python that is one or two releases older. For instance, if the latest version
    of Python is 3.13, using Python 3.11 or 3.12 is recommended.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 许多科学计算库并不立即支持Python的最新版本。因此，在安装PyTorch时，建议使用一个比最新版本早一两个发布版本的Python。例如，如果Python的最新版本是3.13，那么使用Python
    3.11或3.12是推荐的。
- en: 'For instance, there are two versions of PyTorch: a leaner version that only
    supports CPU computing and a full version that supports both CPU and GPU computing.
    If your machine has a CUDA-compatible GPU that can be used for deep learning (ideally,
    an NVIDIA T4, RTX 2080 Ti, or newer), I recommend installing the GPU version.
    Regardless, the default command for installing PyTorch in a code terminal is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，PyTorch 有两个版本：一个仅支持 CPU 计算的精简版和一个支持 CPU 和 GPU 计算的完整版。如果您的机器有一个可用于深度学习的 CUDA
    兼容 GPU（理想情况下是 NVIDIA T4、RTX 2080 Ti 或更新的型号），我建议安装 GPU 版本。无论如何，在代码终端中安装 PyTorch
    的默认命令是：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Suppose your computer supports a CUDA-compatible GPU. In that case, it will
    automatically install the PyTorch version that supports GPU acceleration via CUDA,
    assuming the Python environment you’re working on has the necessary dependencies
    (like `pip`) installed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的计算机支持 CUDA 兼容的 GPU，那么它将自动安装支持通过 CUDA 进行 GPU 加速的 PyTorch 版本，前提是您正在工作的 Python
    环境已安装必要的依赖项（如 `pip`）。
- en: NOTE  As of this writing, PyTorch has also added experimental support for AMD
    GPUs via ROCm. See [https://pytorch.org](https://pytorch.org) for additional instructions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：截至本文撰写时，PyTorch 还通过 ROCm 添加了对 AMD GPU 的实验性支持。有关更多信息，请参阅 [https://pytorch.org](https://pytorch.org)。
- en: To explicitly install the CUDA-compatible version of PyTorch, it’s often better
    to specify the CUDA you want PyTorch to be compatible with. PyTorch’s official
    website ([https://pytorch.org](https://pytorch.org)) provides the commands to
    install PyTorch with CUDA support for different operating systems. Figure A.4
    shows a command that will also install PyTorch, as well as the `torchvision` and
    `torchaudio` libraries, which are optional for this book.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要明确安装与 CUDA 兼容的 PyTorch 版本，通常最好指定 PyTorch 要兼容的 CUDA 版本。PyTorch 的官方网站 ([https://pytorch.org](https://pytorch.org))
    为不同操作系统提供了安装具有 CUDA 支持的 PyTorch 的命令。图 A.4 显示了一个将安装 PyTorch 以及可选的 `torchvision`
    和 `torchaudio` 库的命令。
- en: '![figure](../Images/A-4.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/A-4.png)'
- en: Figure A.4 Access the PyTorch installation recommendation on [https://pytorch.org](https://pytorch.org)
    to customize and select the installation command for your system.
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.4 通过 [https://pytorch.org](https://pytorch.org) 访问 PyTorch 安装推荐，以自定义并选择适合您系统的安装命令。
- en: 'I use PyTorch 2.4.0 for the examples, so I recommend that you use the following
    command to install the exact version to guarantee compatibility with this book:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我在示例中使用 PyTorch 2.4.0，因此我建议您使用以下命令安装确切版本，以确保与本书兼容：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: However, as mentioned earlier, given your operating system, the installation
    command might differ slightly from the one shown here. Thus, I recommend that
    you visit [https://pytorch.org](https://pytorch.org) and use the installation
    menu (see figure A.4) to select the installation command for your operating system.
    Remember to replace `torch` with `torch==2.4.0` in the command.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如前所述，根据您的操作系统，安装命令可能与这里显示的略有不同。因此，我建议您访问 [https://pytorch.org](https://pytorch.org)
    并使用安装菜单（见图 A.4）选择适合您操作系统的安装命令。请记住，在命令中将 `torch` 替换为 `torch==2.4.0`。
- en: 'To check the version of PyTorch, execute the following code in PyTorch:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查 PyTorch 的版本，请在 PyTorch 中执行以下代码：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This prints
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PyTorch and Torch
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PyTorch 和 Torch
- en: The Python library is named PyTorch primarily because it’s a continuation of
    the Torch library but adapted for Python (hence, “PyTorch”). “Torch” acknowledges
    the library’s roots in Torch, a scientific computing framework with wide support
    for machine learning algorithms, which was initially created using the Lua programming
    language.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Python 库被命名为 PyTorch，主要是因为它是 Torch 库的延续，但已针对 Python 进行了适配（因此称为“PyTorch”）。“Torch”承认该库起源于
    Torch，这是一个广泛支持机器学习算法的科学计算框架，最初是用 Lua 编程语言创建的。
- en: If you are looking for additional recommendations and instructions for setting
    up your Python environment or installing the other libraries used in this book,
    visit the supplementary GitHub repository of this book at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要额外的建议和说明，用于设置 Python 环境或安装本书中使用的其他库，请访问本书的补充 GitHub 仓库 [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)。
- en: 'After installing PyTorch, you can check whether your installation recognizes
    your built-in NVIDIA GPU by running the following code in Python:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 PyTorch 后，您可以通过在 Python 中运行以下代码来检查您的安装是否识别了内置的 NVIDIA GPU：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This returns
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If the command returns `True`, you are all set. If the command returns `False`,
    your computer may not have a compatible GPU, or PyTorch does not recognize it.
    While GPUs are not required for the initial chapters in this book, which are focused
    on implementing LLMs for educational purposes, they can significantly speed up
    deep learning–related computations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令返回 `True`，则一切准备就绪。如果命令返回 `False`，则您的计算机可能没有兼容的 GPU，或者 PyTorch 无法识别它。虽然本书的前几章不需要
    GPU，这些章节主要关注于教育目的实现 LLM，但 GPU 可以显著加快深度学习相关计算。
- en: If you don’t have access to a GPU, there are several cloud computing providers
    where users can run GPU computations against an hourly cost. A popular Jupyter
    notebook–like environment is Google Colab ([https://colab.research.google.com](https://colab.research.google.com)),
    which provides time-limited access to GPUs as of this writing. Using the Runtime
    menu, it is possible to select a GPU, as shown in the screenshot in figure A.5.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您无法访问 GPU，有几个云计算提供商允许用户按小时计费运行 GPU 计算。一个流行的类似 Jupyter 笔记本的环境是 Google Colab
    ([https://colab.research.google.com](https://colab.research.google.com))，截至本文撰写时，它提供了时间有限的
    GPU 访问。使用运行时菜单，可以选择一个 GPU，如图 A.5 中的截图所示。
- en: '![figure](../Images/A-5.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/A-5.png)'
- en: Figure A.5 Select a GPU device for Google Colab under the Runtime/Change Runtime
    Type menu.
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.5 在运行时/更改运行时类型菜单下选择 Google Colab 的 GPU 设备。
- en: PyTorch on Apple Silicon
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 苹果硅上的 PyTorch
- en: 'If you have an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or
    newer models), you can use its capabilities to accelerate PyTorch code execution.
    To use your Apple Silicon chip for PyTorch, you first need to install PyTorch
    as you normally would. Then, to check whether your Mac supports PyTorch acceleration
    with its Apple Silicon chip, you can run a simple code snippet in Python:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一台配备苹果硅芯片的苹果 Mac（如 M1、M2、M3 或更新的型号），您可以使用其功能来加速 PyTorch 代码的执行。要使用您的苹果硅芯片为
    PyTorch，您首先需要像平常一样安装 PyTorch。然后，为了检查您的 Mac 是否支持使用其苹果硅芯片进行 PyTorch 加速，您可以在 Python
    中运行一个简单的代码片段：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If it returns `True`, it means that your Mac has an Apple Silicon chip that
    can be used to accelerate PyTorch code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它返回 `True`，则意味着您的 Mac 拥有可以用于加速 PyTorch 代码的苹果硅芯片。
- en: Exercise A.1
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 A.1
- en: Install and set up PyTorch on your computer
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的计算机上安装和设置 PyTorch
- en: Exercise A.2
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 A.2
- en: Run the supplementary code at [https://mng.bz/o05v](https://mng.bz/o05v) that
    checks whether your environment is set up correctly.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 运行补充代码 [https://mng.bz/o05v](https://mng.bz/o05v)，检查您的环境是否设置正确。
- en: A.2 Understanding tensors
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.2 理解张量
- en: Tensors represent a mathematical concept that generalizes vectors and matrices
    to potentially higher dimensions. In other words, tensors are mathematical objects
    that can be characterized by their order (or rank), which provides the number
    of dimensions. For example, a scalar (just a number) is a tensor of rank 0, a
    vector is a tensor of rank 1, and a matrix is a tensor of rank 2, as illustrated
    in figure A.6.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 张量代表了一种将向量矩阵推广到可能更高维度的数学概念。换句话说，张量是可以通过其阶数（或阶数）来表征的数学对象，它提供了维数的数量。例如，标量（只是一个数字）是阶数为0的张量，向量是阶数为1的张量，矩阵是阶数为2的张量，如图
    A.6 所示。
- en: '![figure](../Images/A-6.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/A-6.png)'
- en: Figure A.6 Tensors with different ranks. Here 0D corresponds to rank 0, 1D to
    rank 1, and 2D to rank 2\. A three-dimensional vector, which consists of three
    elements, is still a rank 1 tensor.
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.6 不同阶的张量。其中0D对应阶数为0，1D对应阶数为1，2D对应阶数为2。一个由三个元素组成的三维向量仍然是一个阶数为1的张量。
- en: From a computational perspective, tensors serve as data containers. For instance,
    they hold multidimensional data, where each dimension represents a different feature.
    Tensor libraries like PyTorch can create, manipulate, and compute with these arrays
    efficiently. In this context, a tensor library functions as an array library.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算的角度来看，张量作为数据容器。例如，它们持有多维数据，其中每个维度代表一个不同的特征。像 PyTorch 这样的张量库可以高效地创建、操作和计算这些数组。在这种情况下，张量库充当数组库。
- en: PyTorch tensors are similar to NumPy arrays but have several additional features
    that are important for deep learning. For example, PyTorch adds an automatic differentiation
    engine, simplifying *computing gradients* (see section A.4). PyTorch tensors also
    support GPU computations to speed up deep neural network training (see section
    A.9).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 张量类似于 NumPy 数组，但具有几个对深度学习非常重要的附加功能。例如，PyTorch 添加了一个自动微分引擎，简化了*计算梯度*（见第
    A.4 节）。PyTorch 张量还支持 GPU 计算，以加快深度神经网络训练（见第 A.9 节）。
- en: PyTorch with a NumPy-like API
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 带有NumPy-like API的PyTorch
- en: 'PyTorch adopts most of the NumPy array API and syntax for its tensor operations.
    If you are new to NumPy, you can get a brief overview of the most relevant concepts
    via my article “Scientific Computing in Python: Introduction to NumPy and Matplotlib”
    at [https://sebastianraschka.com/blog/2020/numpy-intro.html](https://sebastianraschka.com/blog/2020/numpy-intro.html).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch采用了NumPy数组API和语法的大多数部分来执行其张量操作。如果您是NumPy的新手，您可以通过我的文章“Python中的科学计算：NumPy和Matplotlib简介”快速了解最相关的概念，该文章可在[https://sebastianraschka.com/blog/2020/numpy-intro.html](https://sebastianraschka.com/blog/2020/numpy-intro.html)找到。
- en: A.2.1 Scalars, vectors, matrices, and tensors
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.1 标量、向量、矩阵和张量
- en: As mentioned earlier, PyTorch tensors are data containers for array-like structures.
    A scalar is a zero-dimensional tensor (for instance, just a number), a vector
    is a one-dimensional tensor, and a matrix is a two-dimensional tensor. There is
    no specific term for higher-dimensional tensors, so we typically refer to a three-dimensional
    tensor as just a 3D tensor, and so forth. We can create objects of PyTorch’s `Tensor`
    class using the `torch.tensor` function as shown in the following listing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，PyTorch张量是类似数组的结构的数据容器。标量是零维张量（例如，只是一个数字），向量是一维张量，矩阵是二维张量。对于更高维度的张量没有特定的术语，所以我们通常将三维张量称为3D张量，依此类推。我们可以使用
    `torch.tensor` 函数创建PyTorch的 `Tensor` 类对象，如下所示。
- en: Listing A.1 Creating PyTorch tensors
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.1 创建PyTorch张量
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Creates a zero-dimensional tensor (scalar) from a Python integer'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从Python整数创建零维张量（标量）'
- en: '#2 Creates a one-dimensional tensor (vector) from a Python list'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 从Python列表创建一维张量（向量）'
- en: '#3 Creates a two-dimensional tensor from a nested Python list'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从嵌套Python列表创建二维张量'
- en: '#4 Creates a three-dimensional tensor from a nested Python list'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 从嵌套Python列表创建三维张量'
- en: A.2.2 Tensor data types
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.2 张量数据类型
- en: 'PyTorch adopts the default 64-bit integer data type from Python. We can access
    the data type of a tensor via the `.dtype` attribute of a tensor:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch采用Python的默认64位整数数据类型。我们可以通过张量的 `.dtype` 属性访问张量的数据类型：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This prints
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we create tensors from Python floats, PyTorch creates tensors with a 32-bit
    precision by default:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从Python浮点数创建张量，PyTorch默认创建32位精度的张量：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The output is
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This choice is primarily due to the balance between precision and computational
    efficiency. A 32-bit floating-point number offers sufficient precision for most
    deep learning tasks while consuming less memory and computational resources than
    a 64-bit floating-point number. Moreover, GPU architectures are optimized for
    32-bit computations, and using this data type can significantly speed up model
    training and inference.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种选择主要是由于精度和计算效率之间的平衡。32位浮点数提供了足够的精度，适用于大多数深度学习任务，同时比64位浮点数消耗更少的内存和计算资源。此外，GPU架构针对32位计算进行了优化，使用这种数据类型可以显著加快模型训练和推理。
- en: 'Moreover, it is possible to change the precision using a tensor’s `.to` method.
    The following code demonstrates this by changing a 64-bit integer tensor into
    a 32-bit float tensor:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以使用张量的 `.to` 方法更改精度。以下代码通过将64位整数张量转换为32位浮点张量来演示这一点：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This returns
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For more information about different tensor data types available in PyTorch,
    check the official documentation at [https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有关PyTorch中可用的不同张量数据类型的更多信息，请查看官方文档[https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)。
- en: A.2.3 Common PyTorch tensor operations
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.3 常见的PyTorch张量操作
- en: Comprehensive coverage of all the different PyTorch tensor operations and commands
    is outside the scope of this book. However, I will briefly describe relevant operations
    as we introduce them throughout the book.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不涵盖所有不同的PyTorch张量操作和命令的全面介绍。然而，随着我们在书中介绍这些操作，我会简要描述相关的操作。
- en: 'We have already introduced the `torch.tensor()` function to create new tensors:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了 `torch.tensor()` 函数来创建新的张量：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This prints
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In addition, the `.shape` attribute allows us to access the shape of a tensor:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`.shape` 属性允许我们访问张量的形状：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The output is
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As you can see, `.shape` returns `[2,` `3]`, meaning the tensor has two rows
    and three columns. To reshape the tensor into a 3 × 2 tensor, we can use the `.reshape`
    method:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`.shape` 返回 `[2,` `3]`，这意味着张量有两行三列。要将张量重塑为3 × 2张量，我们可以使用 `.reshape` 方法：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This prints
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'However, note that the more common command for reshaping tensors in PyTorch
    is `.view()`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，在 PyTorch 中重塑张量的更常见命令是 `.view()`：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The output is
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Similar to `.reshape` and `.view`, in several cases, PyTorch offers multiple
    syntax options for executing the same computation. PyTorch initially followed
    the original Lua Torch syntax convention but then, by popular request, added syntax
    to make it similar to NumPy. (The subtle difference between `.view()` and `.reshape()`
    in PyTorch lies in their handling of memory layout: `.view()` requires the original
    data to be contiguous and will fail if it isn’t, whereas `.reshape()` will work
    regardless, copying the data if necessary to ensure the desired shape.)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `.reshape` 和 `.view` 类似，在几个情况下，PyTorch 为执行相同的计算提供了多个语法选项。PyTorch 最初遵循原始 Lua
    Torch 语法约定，但后来，根据普遍的要求，添加了与 NumPy 类似的语法。（PyTorch 中 `.view()` 和 `.reshape()` 之间的细微差别在于它们对内存布局的处理：`.view()`
    要求原始数据是连续的，如果不是，将会失败，而 `.reshape()` 将会工作，如果需要，会复制数据以确保所需的形状。）
- en: 'Next, we can use `.T` to transpose a tensor, which means flipping it across
    its diagonal. Note that this is not the same as reshaping a tensor, as you can
    see based on the following result:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用 `.T` 来转置一个张量，这意味着沿着其对角线翻转它。请注意，这与重塑张量不同，正如以下结果所示：
- en: '[PRE22]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The output is
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Lastly, the common way to multiply two matrices in PyTorch is the `.matmul`
    method:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，PyTorch 中乘以两个矩阵的常用方法是 `.matmul` 方法：
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The output is
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'However, we can also adopt the `@` operator, which accomplishes the same thing
    more compactly:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也可以采用 `@` 操作符，它可以更紧凑地完成相同的事情：
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This prints
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印
- en: '[PRE27]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As mentioned earlier, I introduce additional operations when needed. For readers
    who’d like to browse through all the different tensor operations available in
    PyTorch (we won’t need most of these), I recommend checking out the official documentation
    at [https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当需要时，我会引入额外的操作。对于想要浏览 PyTorch 中所有不同张量操作的读者（我们不需要这些中的大多数），我建议查看官方文档[https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)。
- en: A.3 Seeing models as computation graphs
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.3 将模型视为计算图
- en: Now let’s look at PyTorch’s automatic differentiation engine, also known as
    autograd. PyTorch’s autograd system provides functions to compute gradients in
    dynamic computational graphs automatically.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看 PyTorch 的自动微分引擎，也称为 autograd。PyTorch 的 autograd 系统提供函数来自动计算动态计算图中的梯度。
- en: A computational graph is a directed graph that allows us to express and visualize
    mathematical expressions. In the context of deep learning, a computation graph
    lays out the sequence of calculations needed to compute the output of a neural
    network—we will need this to compute the required gradients for backpropagation,
    the main training algorithm for neural networks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图是一个有向图，它允许我们表达和可视化数学表达式。在深度学习的上下文中，计算图展示了计算神经网络输出所需的一系列计算——我们将需要它来计算反向传播所需的梯度，这是神经网络的主要训练算法。
- en: Let’s look at a concrete example to illustrate the concept of a computation
    graph. The code in the following listing implements the forward pass (prediction
    step) of a simple logistic regression classifier, which can be seen as a single-layer
    neural network. It returns a score between 0 and 1, which is compared to the true
    class label (0 or 1) when computing the loss.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个具体的例子来说明计算图的概念。以下列表中的代码实现了简单逻辑回归分类器的正向传递（预测步骤），这可以被视为单层神经网络。它返回一个介于 0
    和 1 之间的分数，当计算损失时，这个分数会与真实的类别标签（0 或 1）进行比较。
- en: Listing A.2 A logistic regression forward pass
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.2 逻辑回归正向传递
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#1 This import statement is a common convention in PyTorch to prevent long
    lines of code.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 这个导入语句是 PyTorch 中的常见约定，用于防止代码行过长。'
- en: '#2 True label'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 真实标签'
- en: '#3 Input feature'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 输入特征'
- en: '#4 Weight parameter'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 权重参数'
- en: '#5 Bias unit'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 偏置单元'
- en: '#6 Net input'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 网络输入'
- en: '#7 Activation and output'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 激活和输出'
- en: If not all components in the preceding code make sense to you, don’t worry.
    The point of this example is not to implement a logistic regression classifier
    but rather to illustrate how we can think of a sequence of computations as a computation
    graph, as shown in figure A.7\.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的代码中有些部分对你来说没有意义，不要担心。这个示例的目的不是实现一个逻辑回归分类器，而是说明我们可以如何将一系列计算视为一个计算图，如图 A.7
    所示。
- en: '![figure](../Images/A-7.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/A-7.png)'
- en: Figure A.7 A logistic regression forward pass as a computation graph. The input
    feature x[1] is multiplied by a model weight w[1] and passed through an activation
    function s after adding the bias. The loss is computed by comparing the model
    output a with a given label y.
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.7 展示了逻辑回归的前向传递作为计算图。输入特征x[1]乘以模型权重w[1]，在添加偏差后通过激活函数s，然后通过比较模型输出a与给定的标签y来计算损失。
- en: In fact, PyTorch builds such a computation graph in the background, and we can
    use this to calculate gradients of a loss function with respect to the model parameters
    (here *w*[1] and *b*) to train the model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，PyTorch在后台构建这样的计算图，我们可以使用它来计算损失函数相对于模型参数（此处*w*[1]和*b*）的梯度，以训练模型。
- en: A.4 Automatic differentiation made easy
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.4 自动微分变得简单
- en: If we carry out computations in PyTorch, it will build a computational graph
    internally by default if one of its terminal nodes has the `requires_grad` attribute
    set to `True`. This is useful if we want to compute gradients. Gradients are required
    when training neural networks via the popular backpropagation algorithm, which
    can be considered an implementation of the *chain rule* from calculus for neural
    networks, illustrated in figure A.8.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在PyTorch中进行计算，它将默认内部构建一个计算图，如果其终端节点之一设置了`requires_grad`属性为`True`。如果我们想计算梯度，这很有用。在通过流行的反向传播算法训练神经网络时需要梯度，这可以被认为是微积分中的链式法则在神经网络中的实现，如图A.8所示。
- en: '![figure](../Images/A-8.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/A-8.png)'
- en: Figure A.8 The most common way of computing the loss gradients in a computation
    graph involves applying the chain rule from right to left, also called reverse-model
    automatic differentiation or backpropagation. We start from the output layer (or
    the loss itself) and work backward through the network to the input layer. We
    do this to compute the gradient of the loss with respect to each parameter (weights
    and biases) in the network, which informs how we update these parameters during
    training.
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.8 计算图中损失梯度的最常见方法是从右到左应用链式法则，也称为反向模型自动微分或反向传播。我们从输出层（或损失本身）开始，通过网络反向到输入层。我们这样做是为了计算损失相对于网络中每个参数（权重和偏差）的梯度，这告诉我们如何在训练期间更新这些参数。
- en: Partial derivatives and gradients
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偏导数和梯度
- en: Figure A.8 shows partial derivatives, which measure the rate at which a function
    changes with respect to one of its variables. A *gradient* is a vector containing
    all of the partial derivatives of a multivariate function, a function with more
    than one variable as input.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.8显示了偏导数，它衡量了函数相对于其变量的变化率。*梯度*是一个包含多变量函数所有偏导数的向量，该函数的输入变量不止一个。
- en: If you are not familiar with or don’t remember the partial derivatives, gradients,
    or chain rule from calculus, don’t worry. On a high level, all you need to know
    for this book is that the chain rule is a way to compute gradients of a loss function
    given the model’s parameters in a computation graph. This provides the information
    needed to update each parameter to minimize the loss function, which serves as
    a proxy for measuring the model’s performance using a method such as gradient
    descent. We will revisit the computational implementation of this training loop
    in PyTorch in section A.7.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对偏导数、梯度或微积分中的链式法则不熟悉或记不清楚，不要担心。从高层次来看，这本书你需要知道的是，链式法则是通过计算图中的模型参数来计算损失函数梯度的方法。这提供了更新每个参数以最小化损失函数所需的信息，损失函数作为衡量模型性能的代理，可以使用梯度下降等方法。我们将在A.7节中回顾PyTorch中这个训练循环的计算实现。
- en: How is this all related to the automatic differentiation (autograd) engine,
    the second component of the PyTorch library mentioned earlier? PyTorch’s autograd
    engine constructs a computational graph in the background by tracking every operation
    performed on tensors. Then, calling the `grad` function, we can compute the gradient
    of the loss concerning the model parameter `w1`, as shown in the following listing.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都是如何与前面提到的PyTorch库的第二组件自动微分（autograd）引擎相关联的呢？PyTorch的autograd引擎通过跟踪对张量执行的每个操作，在后台构建一个计算图。然后，调用`grad`函数，我们可以计算关于模型参数`w1`的损失梯度，如下面的列表所示。
- en: Listing A.3 Computing gradients via autograd
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.3 通过autograd计算梯度
- en: '[PRE29]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#1 By default, PyTorch destroys the computation graph after calculating the
    gradients to free memory. However, since we will reuse this computation graph
    shortly, we set retain_graph=True so that it stays in memory.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 默认情况下，PyTorch 在计算梯度后会销毁计算图以释放内存。然而，由于我们很快将重用这个计算图，我们设置 retain_graph=True
    以使其留在内存中。'
- en: The resulting values of the loss gradients given the model’s parameters are
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 给定模型参数的损失梯度的结果值是
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This prints
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: This prints
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here, we have been using the grad function manually, which can be useful for
    experimentation, debugging, and demonstrating concepts. But, in practice, PyTorch
    provides even more high-level tools to automate this process. For instance, we
    can call `.backward` on the loss, and PyTorch will compute the gradients of all
    the leaf nodes in the graph, which will be stored via the tensors’ `.grad` attributes:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们一直在手动使用 grad 函数，这对于实验、调试和展示概念可能很有用。但是，在实践中，PyTorch 提供了更多高级工具来自动化这个过程。例如，我们可以在损失上调用
    `.backward`，PyTorch 将计算图中所有叶节点的梯度，这些梯度将通过张量的 `.grad` 属性存储：
- en: '[PRE32]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The outputs are
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: I’ve provided you with a lot of information, and you may be overwhelmed by the
    calculus concepts, but don’t worry. While this calculus jargon is a means to explain
    PyTorch’s autograd component, all you need to take away is that PyTorch takes
    care of the calculus for us via the `.backward` method—we won’t need to compute
    any derivatives or gradients by hand.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经提供了很多信息，你可能被微积分概念所淹没，但不用担心。虽然这种微积分术语是解释 PyTorch 的 autograd 组件的手段，但你只需要记住
    PyTorch 通过 `.backward` 方法为我们处理微积分——我们不需要手动计算任何导数或梯度。
- en: A.5 Implementing multilayer neural networks
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.5 实现多层神经网络
- en: Next, we focus on PyTorch as a library for implementing deep neural networks.
    To provide a concrete example, let’s look at a multilayer perceptron, a fully
    connected neural network, as illustrated in figure A.9.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将重点关注 PyTorch 作为实现深度神经网络的库。为了提供一个具体的例子，让我们看看一个多层感知器，一个全连接神经网络，如图 A.9 所示。
- en: '![figure](../Images/A-9.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/A-9.png)'
- en: Figure A.9 A multilayer perceptron with two hidden layers. Each node represents
    a unit in the respective layer. For illustration purposes, each layer has a very
    small number of nodes.
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.9 具有两个隐藏层的多层感知器。每个节点代表相应层中的一个单元。为了说明目的，每个层都有非常少的节点。
- en: When implementing a neural network in PyTorch, we can subclass the `torch.nn.Module`
    class to define our own custom network architecture. This `Module` base class
    provides a lot of functionality, making it easier to build and train models. For
    instance, it allows us to encapsulate layers and operations and keep track of
    the model’s parameters.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 PyTorch 中实现神经网络时，我们可以通过继承 `torch.nn.Module` 类来定义我们自己的自定义网络架构。这个 `Module`
    基类提供了很多功能，使得构建和训练模型变得更加容易。例如，它允许我们封装层和操作，并跟踪模型的参数。
- en: Within this subclass, we define the network layers in the `__init__` constructor
    and specify how the layers interact in the forward method. The forward method
    describes how the input data passes through the network and comes together as
    a computation graph. In contrast, the backward method, which we typically do not
    need to implement ourselves, is used during training to compute gradients of the
    loss function given the model parameters (see section A.7). The code in the following
    listing implements a classic multilayer perceptron with two hidden layers to illustrate
    a typical usage of the `Module` class.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个子类中，我们在 `__init__` 构造函数中定义网络层，并在 `forward` 方法中指定层之间的交互方式。`forward` 方法描述了输入数据如何通过网络，并作为一个计算图汇集在一起。相比之下，我们通常不需要自己实现的
    `backward` 方法，在训练期间用于根据模型参数计算损失函数的梯度（参见 A.7 节）。以下列表中的代码实现了一个具有两个隐藏层的经典多层感知器，以展示
    `Module` 类的典型用法。
- en: Listing A.4 A multilayer perceptron with two hidden layers
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.4 具有两个隐藏层的多层感知器
- en: '[PRE34]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '#1 Coding the number of inputs and outputs as variables allows us to reuse
    the same code for datasets with different numbers of features and classes'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将输入和输出编码为变量允许我们为具有不同特征和类数的不同数据集重用相同的代码'
- en: '#2 The Linear layer takes the number of input and output nodes as arguments.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 线性层将输入和输出节点数作为参数。'
- en: '#3 Nonlinear activation functions are placed between the hidden layers.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 非线性激活函数放置在隐藏层之间。'
- en: '#4 The number of output nodes of one hidden layer has to match the number of
    inputs of the next layer.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 一个隐藏层的输出节点数必须与下一层的输入数相匹配。'
- en: '#5 The outputs of the last layer are called logits.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 最后层的输出被称为logits。'
- en: 'We can then instantiate a new neural network object as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以如下实例化一个新的神经网络对象：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Before using this new `model` object, we can call `print` on the model to see
    a summary of its structure:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这个新的`model`对象之前，我们可以对模型调用`print`来查看其结构的摘要：
- en: '[PRE36]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This prints
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印
- en: '[PRE37]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note that we use the `Sequential` class when we implement the `NeuralNetwork`
    class. `Sequential` is not required, but it can make our life easier if we have
    a series of layers we want to execute in a specific order, as is the case here.
    This way, after instantiating `self.layers` `=` `Sequential(...)` in the `__init__`
    constructor, we just have to call the `self.layers` instead of calling each layer
    individually in the `NeuralNetwork`’s `forward` method.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当我们实现`NeuralNetwork`类时，我们使用`Sequential`类。`Sequential`不是必需的，但如果我们要按特定顺序执行一系列层，这可以使我们的工作更简单，就像在这里的情况一样。这样，在`__init__`构造函数中将`self.layers`设置为`Sequential(...)`之后，我们只需要调用`self.layers`，而不是在`NeuralNetwork`的`forward`方法中逐个调用每个层。
- en: 'Next, let’s check the total number of trainable parameters of this model:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检查这个模型的可训练参数总数：
- en: '[PRE38]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This prints
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印
- en: '[PRE39]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Each parameter for which `requires_grad=True` counts as a trainable parameter
    and will be updated during training (see section A.7).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个`requires_grad=True`的参数，都算作一个可训练参数，将在训练过程中更新（参见第A.7节）。
- en: In the case of our neural network model with the preceding two hidden layers,
    these trainable parameters are contained in the `torch.nn.Linear` layers. A `Linear`
    layer multiplies the inputs with a weight matrix and adds a bias vector. This
    is sometimes referred to as a *feedforward* or *fully connected* layer.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们前面两个隐藏层的神经网络模型中，这些可训练参数包含在`torch.nn.Linear`层中。一个`Linear`层将输入与权重矩阵相乘并添加一个偏差向量。这有时被称为*前馈*或*全连接*层。
- en: 'Based on the `print(model)` call we executed here, we can see that the first
    `Linear` layer is at index position 0 in the layers attribute. We can access the
    corresponding weight parameter matrix as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在这里执行的`print(model)`调用，我们可以看到第一个`Linear`层在`layers`属性中的索引位置是0。我们可以如下访问相应的权重参数矩阵：
- en: '[PRE40]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This prints
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印
- en: '[PRE41]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Since this large matrix is not shown in its entirety, let’s use the `.shape`
    attribute to show its dimensions:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个大矩阵没有全部显示，让我们使用`.shape`属性来显示其维度：
- en: '[PRE42]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The result is
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE43]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: (Similarly, you could access the bias vector via `model.layers[0].bias`.)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: （同样，你也可以通过`model.layers[0].bias`访问偏差向量。）
- en: The weight matrix here is a 30 × 50 matrix, and we can see that `requires_grad`
    is set to `True`, which means its entries are trainable—this is the default setting
    for weights and biases in `torch.nn.Linear`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的权重矩阵是一个30 × 50的矩阵，我们可以看到`requires_grad`被设置为`True`，这意味着它的条目是可训练的——这是`torch.nn.Linear`中权重和偏差的默认设置。
- en: If you execute the preceding code on your computer, the numbers in the weight
    matrix will likely differ from those shown. The model weights are initialized
    with small random numbers, which differ each time we instantiate the network.
    In deep learning, initializing model weights with small random numbers is desired
    to break symmetry during training. Otherwise, the nodes would be performing the
    same operations and updates during backpropagation, which would not allow the
    network to learn complex mappings from inputs to outputs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在你自己的计算机上执行前面的代码，权重矩阵中的数字可能会与显示的不同。模型权重使用小的随机数初始化，每次实例化网络时都不同。在深度学习中，使用小的随机数初始化模型权重是为了在训练期间打破对称性。否则，节点在反向传播期间会执行相同的操作和更新，这不会允许网络从输入到输出学习复杂的映射。
- en: 'However, while we want to keep using small random numbers as initial values
    for our layer weights, we can make the random number initialization reproducible
    by seeding PyTorch’s random number generator via `manual_seed`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虽然我们希望继续使用小的随机数作为层权重的初始值，但我们可以通过`manual_seed`对PyTorch的随机数生成器进行播种来使随机数初始化可重现：
- en: '[PRE44]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The result is
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE45]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now that we have spent some time inspecting the `NeuralNetwork` instance, let’s
    briefly see how it’s used via the forward pass:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经花了一些时间检查了`NeuralNetwork`实例，让我们简要看看它是如何通过前向传递来使用的：
- en: '[PRE46]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The result is
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE47]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In the preceding code, we generated a single random training example `X` as
    a toy input (note that our network expects 50-dimensional feature vectors) and
    fed it to the model, returning three scores. When we call `model(x)`, it will
    automatically execute the forward pass of the model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们生成了一个单一的随机训练示例`X`作为玩具输入（注意我们的网络期望50维的特征向量），并将其输入到模型中，返回三个分数。当我们调用`model(x)`时，它将自动执行模型的正向传递。
- en: The forward pass refers to calculating output tensors from input tensors. This
    involves passing the input data through all the neural network layers, starting
    from the input layer, through hidden layers, and finally to the output layer.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正向传递是指从输入张量计算输出张量的过程。这涉及到将输入数据通过所有神经网络层，从输入层开始，通过隐藏层，最后到输出层。
- en: These three numbers returned here correspond to a score assigned to each of
    the three output nodes. Notice that the output tensor also includes a `grad_fn`
    value.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个返回的数字对应于分配给三个输出节点的分数。请注意，输出张量还包括一个`grad_fn`值。
- en: Here, `grad_fn=<AddmmBackward0>` represents the last-used function to compute
    a variable in the computational graph. In particular, `grad_fn=<AddmmBackward0>`
    means that the tensor we are inspecting was created via a matrix multiplication
    and addition operation. PyTorch will use this information when it computes gradients
    during backpropagation. The `<AddmmBackward0>` part of `grad_fn=<AddmmBackward0>`
    specifies the operation performed. In this case, it is an `Addmm` operation. `Addmm`
    stands for matrix multiplication (`mm`) followed by an addition (`Add`).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`grad_fn=<AddmmBackward0>`表示在计算图中计算变量的最后一个使用的函数。特别是，`grad_fn=<AddmmBackward0>`意味着我们正在检查的张量是通过矩阵乘法和加法操作创建的。PyTorch将在反向传播期间计算梯度时使用这些信息。`grad_fn=<AddmmBackward0>`中的`<AddmmBackward0>`部分指定了执行的操作。在这种情况下，它是一个`Addmm`操作。`Addmm`代表矩阵乘法（`mm`）后跟加法（`Add`）。
- en: 'If we just want to use a network without training or backpropagation—for example,
    if we use it for prediction after training—constructing this computational graph
    for backpropagation can be wasteful as it performs unnecessary computations and
    consumes additional memory. So, when we use a model for inference (for instance,
    making predictions) rather than training, the best practice is to use the `torch.no_grad()`
    context manager. This tells PyTorch that it doesn’t need to keep track of the
    gradients, which can result in significant savings in memory and computation:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只想使用一个网络而不进行训练或反向传播——例如，如果我们使用它进行训练后的预测——构建这个反向传播的计算图可能是浪费的，因为它执行了不必要的计算并消耗了额外的内存。因此，当我们使用模型进行推理（例如，进行预测）而不是训练时，最佳实践是使用`torch.no_grad()`上下文管理器。这告诉PyTorch它不需要跟踪梯度，这可以显著节省内存和计算：
- en: '[PRE48]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The result is
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE49]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In PyTorch, it’s common practice to code models such that they return the outputs
    of the last layer (logits) without passing them to a nonlinear activation function.
    That’s because PyTorch’s commonly used loss functions combine the `softmax` (or
    `sigmoid` for binary classification) operation with the negative log-likelihood
    loss in a single class. The reason for this is numerical efficiency and stability.
    So, if we want to compute class-membership probabilities for our predictions,
    we have to call the `softmax` function explicitly:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，常见的做法是编写模型，使其返回最后一层的输出（logits），而不将它们传递给非线性激活函数。这是因为PyTorch常用的损失函数将`softmax`（或二分类中的`sigmoid`）操作与单个类中的负对数似然损失结合在一起。这样做的原因是数值效率和稳定性。因此，如果我们想计算预测的类成员概率，我们必须显式调用`softmax`函数：
- en: '[PRE50]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This prints
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印
- en: '[PRE51]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The values can now be interpreted as class-membership probabilities that sum
    up to 1\. The values are roughly equal for this random input, which is expected
    for a randomly initialized model without training.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以将这些值解释为类成员概率，它们的总和为1。对于这个随机输入，这些值大致相等，这是对随机初始化且未经训练的模型所预期的。
- en: A.6 Setting up efficient data loaders
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.6 设置高效的数据加载器
- en: Before we can train our model, we have to briefly discuss creating efficient
    data loaders in PyTorch, which we will iterate over during training. The overall
    idea behind data loading in PyTorch is illustrated in figure A.10.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够训练我们的模型之前，我们必须简要讨论在PyTorch中创建高效的数据加载器，我们将在训练过程中遍历这些数据加载器。PyTorch中数据加载的整体思想如图A.10所示。
- en: '![figure](../Images/A-10.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/A-10.png)'
- en: Figure A.10 PyTorch implements a `Dataset` and a `DataLoader` class. The `Dataset`
    class is used to instantiate objects that define how each data record is loaded.
    The `DataLoader` handles how the data is shuffled and assembled into batches.
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.10 PyTorch 实现了 `Dataset` 和 `DataLoader` 类。`Dataset` 类用于实例化对象，这些对象定义了如何加载每个数据记录。`DataLoader`
    处理数据如何打乱和组装成批次。
- en: 'Following figure A.10, we will implement a custom `Dataset` class, which we
    will use to create a training and a test dataset that we’ll then use to create
    the data loaders. Let’s start by creating a simple toy dataset of five training
    examples with two features each. Accompanying the training examples, we also create
    a tensor containing the corresponding class labels: three examples belong to class
    0, and two examples belong to class 1\. In addition, we make a test set consisting
    of two entries. The code to create this dataset is shown in the following listing.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 A.10 之后，我们将实现一个自定义 `Dataset` 类，我们将使用它来创建一个训练集和一个测试集，然后我们将使用这些数据集来创建数据加载器。让我们先创建一个包含五个训练示例的简单玩具数据集，每个示例有两个特征。与训练示例一起，我们还创建了一个包含相应类标签的张量：三个示例属于类别
    0，两个示例属于类别 1。此外，我们还创建了一个包含两个条目的测试集。创建此数据集的代码如下所示。
- en: Listing A.5 Creating a small toy dataset
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.5 创建一个小型玩具数据集
- en: '[PRE52]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: NOTE  PyTorch requires that class labels start with label 0, and the largest
    class label value should not exceed the number of output nodes minus 1 (since
    Python index counting starts at zero). So, if we have class labels 0, 1, 2, 3,
    and 4, the neural network output layer should consist of five nodes.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：PyTorch 要求类标签从 0 开始，最大的类标签值不应超过输出节点数减 1（因为 Python 索引计数从零开始）。所以，如果我们有类标签 0、1、2、3
    和 4，神经网络输出层应该由五个节点组成。
- en: Next, we create a custom dataset class, `ToyDataset`, by subclassing from PyTorch’s
    `Dataset` parent class, as shown in the following listing.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过从 PyTorch 的 `Dataset` 父类派生，创建一个自定义数据集类 `ToyDataset`，如下所示。
- en: Listing A.6 Defining a custom `Dataset` class
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.6 定义自定义 `Dataset` 类
- en: '[PRE53]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '#1 Instructions for retrieving exactly one data record and the corresponding
    label'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 检索一个数据记录及其对应标签的说明'
- en: '#2 Instructions for returning the total length of the dataset'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 返回数据集总长度的说明'
- en: The purpose of this custom `ToyDataset` class is to instantiate a PyTorch `DataLoader`.
    But before we get to this step, let’s briefly go over the general structure of
    the `ToyDataset` code.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这个自定义 `ToyDataset` 类的目的是实例化一个 PyTorch `DataLoader`。但在我们到达这一步之前，让我们简要地回顾一下 `ToyDataset`
    代码的一般结构。
- en: In PyTorch, the three main components of a custom `Dataset` class are the `__init__`
    constructor, the `__getitem__` method, and the `__len__` method (see listing A.6).
    In the `__init__` method, we set up attributes that we can access later in the
    `__getitem__` and `__len__` methods. These could be file paths, file objects,
    database connectors, and so on. Since we created a tensor dataset that sits in
    memory, we simply assign `X` and `y` to these attributes, which are placeholders
    for our tensor objects.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，自定义 `Dataset` 类的三个主要组件是 `__init__` 构造函数、`__getitem__` 方法以及 `__len__`
    方法（见列表 A.6）。在 `__init__` 方法中，我们设置可以在后续的 `__getitem__` 和 `__len__` 方法中访问的属性。这些可能是文件路径、文件对象、数据库连接器等等。由于我们创建了一个驻留在内存中的张量数据集，我们只需将这些属性分配给
    `X` 和 `y`，它们是我们张量对象的占位符。
- en: In the `__getitem__` method, we define instructions for returning exactly one
    item from the dataset via an `index`. This refers to the features and the class
    label corresponding to a single training example or test instance. (The data loader
    will provide this `index`, which we will cover shortly.)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `__getitem__` 方法中，我们定义了通过 `index` 返回数据集中一个项目的说明。这指的是与单个训练示例或测试实例对应的特征和类标签。（数据加载器将提供这个
    `index`，我们将在稍后介绍。）
- en: 'Finally, the `__len__` method contains instructions for retrieving the length
    of the dataset. Here, we use the `.shape` attribute of a tensor to return the
    number of rows in the feature array. In the case of the training dataset, we have
    five rows, which we can double-check:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`__len__` 方法包含了检索数据集长度的说明。在这里，我们使用张量的 `.shape` 属性来返回特征数组中的行数。在训练数据集的情况下，我们有五行，我们可以进行双重检查：
- en: '[PRE54]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The result is
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE55]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Now that we’ve defined a PyTorch `Dataset` class we can use for our toy dataset,
    we can use PyTorch’s `DataLoader` class to sample from it, as shown in the following
    listing.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了一个可以用于我们的玩具数据集的 PyTorch `Dataset` 类，我们可以使用 PyTorch 的 `DataLoader` 类从中采样，如下所示。
- en: Listing A.7 Instantiating data loaders
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.7 实例化数据加载器
- en: '[PRE56]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '#1 The ToyDataset instance created earlier serves as input to the data loader.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 之前创建的ToyDataset实例作为数据加载器的输入。'
- en: '#2 Whether or not to shuffle the data'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 是否洗牌数据'
- en: '#3 The number of background processes'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 背景进程的数量'
- en: '#4 It is not necessary to shuffle a test dataset.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 测试数据集不需要洗牌。'
- en: 'After instantiating the training data loader, we can iterate over it. The iteration
    over the `test_loader` works similarly but is omitted for brevity:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化训练数据加载器之后，我们可以遍历它。对`test_loader`的遍历方式类似，但为了简洁起见省略了：
- en: '[PRE57]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The result is
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE58]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: As we can see based on the preceding output, the `train_loader` iterates over
    the training dataset, visiting each training example exactly once. This is known
    as a training epoch. Since we seeded the random number generator using `torch.manual_seed(123)`
    here, you should get the exact same shuffling order of training examples. However,
    if you iterate over the dataset a second time, you will see that the shuffling
    order will change. This is desired to prevent deep neural networks from getting
    caught in repetitive update cycles during training.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的输出，我们可以看到`train_loader`遍历训练数据集，每个训练示例恰好访问一次。这被称为一个训练周期。由于我们在这里使用`torch.manual_seed(123)`初始化了随机数生成器，你应该得到相同的训练示例洗牌顺序。然而，如果你第二次遍历数据集，你会看到洗牌顺序会改变。这是期望的，以防止深度神经网络在训练过程中陷入重复的更新循环。
- en: We specified a batch size of 2 here, but the third batch only contains a single
    example. That’s because we have five training examples, and 5 is not evenly divisible
    by 2\. In practice, having a substantially smaller batch as the last batch in
    a training epoch can disturb the convergence during training. To prevent this,
    set `drop_last=True`, which will drop the last batch in each epoch, as shown in
    the following listing.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定了批大小为2，但第三个批次只包含一个示例。这是因为我们有五个训练示例，而5不能被2整除。在实际应用中，如果训练周期中的最后一个批次显著较小，可能会在训练过程中干扰收敛。为了防止这种情况，设置`drop_last=True`，这将丢弃每个周期中的最后一个批次，如下面的列表所示。
- en: Listing A.8 A training loader that drops the last batch
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.8 一个丢弃最后一个批次的训练加载器
- en: '[PRE59]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now, iterating over the training loader, we can see that the last batch is
    omitted:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，遍历训练加载器，我们可以看到最后一个批次被省略了：
- en: '[PRE60]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The result is
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE61]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Lastly, let’s discuss the setting `num_workers=0` in the `DataLoader`. This
    parameter in PyTorch’s `DataLoader` function is crucial for parallelizing data
    loading and preprocessing. When `num_workers` is set to 0, the data loading will
    be done in the main process and not in separate worker processes. This might seem
    unproblematic, but it can lead to significant slowdowns during model training
    when we train larger networks on a GPU. Instead of focusing solely on the processing
    of the deep learning model, the CPU must also take time to load and preprocess
    the data. As a result, the GPU can sit idle while waiting for the CPU to finish
    these tasks. In contrast, when `num_workers` is set to a number greater than 0,
    multiple worker processes are launched to load data in parallel, freeing the main
    process to focus on training your model and better utilizing your system’s resources
    (figure A.11).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们讨论`DataLoader`中的设置`num_workers=0`。在PyTorch的`DataLoader`函数中，此参数对于并行化数据加载和预处理至关重要。当`num_workers`设置为0时，数据加载将在主进程中完成，而不是在单独的工作进程中。这看起来可能没有问题，但当我们在大GPU上训练更大的网络时，它可能导致模型训练期间出现显著的减速。在这种情况下，CPU必须花费时间来加载和预处理数据，而不是仅仅关注深度学习模型的处理。因此，GPU可能会空闲等待CPU完成这些任务。相比之下，当`num_workers`设置为大于0的数字时，会启动多个工作进程以并行加载数据，从而让主进程专注于训练你的模型，并更好地利用系统资源（图A.11）。
- en: '![figure](../Images/A-11.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/A-11.png)'
- en: Figure A.11 Loading data without multiple workers (setting `num_workers=0`)
    will create a data loading bottleneck where the model sits idle until the next
    batch is loaded (left). If multiple workers are enabled, the data loader can queue
    up the next batch in the background (right).
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.11 在没有多个工作进程（设置`num_workers=0`）的情况下加载数据将创建一个数据加载瓶颈，其中模型处于空闲状态，直到加载下一个批次（左侧）。如果启用了多个工作进程，数据加载器可以在后台排队下一个批次（右侧）。
- en: However, if we are working with very small datasets, setting `num_workers` to
    1 or larger may not be necessary since the total training time takes only fractions
    of a second anyway. So, if you are working with tiny datasets or interactive environments
    such as Jupyter notebooks, increasing `num_workers` may not provide any noticeable
    speedup. It may, in fact, lead to some problems. One potential problem is the
    overhead of spinning up multiple worker processes, which could take longer than
    the actual data loading when your dataset is small.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们处理的是非常小的数据集，将`num_workers`设置为1或更大可能不是必要的，因为总训练时间只需要几毫秒。所以，如果你处理的是小型数据集或Jupyter笔记本等交互式环境，增加`num_workers`可能不会带来任何明显的加速。实际上，它可能引起一些问题。一个潜在的问题是启动多个工作进程的开销，当数据集较小时，这可能会比实际的数据加载时间更长。
- en: Furthermore, for Jupyter notebooks, setting `num_workers` to greater than 0
    can sometimes lead to problems related to the sharing of resources between different
    processes, resulting in errors or notebook crashes. Therefore, it’s essential
    to understand the tradeoff and make a calculated decision on setting the `num_workers`
    parameter. When used correctly, it can be a beneficial tool but should be adapted
    to your specific dataset size and computational environment for optimal results.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于Jupyter笔记本，将`num_workers`设置为大于0有时会导致与不同进程之间资源共享相关的问题，从而引起错误或笔记本崩溃。因此，理解权衡并就设置`num_workers`参数做出计算决策至关重要。当正确使用时，它可以是一个有益的工具，但应该根据您特定的数据集大小和计算环境进行调整以获得最佳结果。
- en: In my experience, setting `num_workers=4` usually leads to optimal performance
    on many real-world datasets, but optimal settings depend on your hardware and
    the code used for loading a training example defined in the `Dataset` class.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的经验，将`num_workers`设置为4通常在许多实际数据集上能带来最佳性能，但最佳设置取决于您的硬件以及用于加载`Dataset`类中定义的训练示例的代码。
- en: A.7 A typical training loop
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.7 典型的训练循环
- en: Let’s now train a neural network on the toy dataset. The following listing shows
    the training code.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将在玩具数据集上训练一个神经网络。以下列表展示了训练代码。
- en: Listing A.9 Neural network training in PyTorch
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.9 PyTorch中的神经网络训练
- en: '[PRE62]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '#1 The dataset has two features and two classes.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 数据集有两个特征和两个类别。'
- en: '#2 The optimizer needs to know which parameters to optimize.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 优化器需要知道要优化的参数。'
- en: '#3 Sets the gradients from the previous round to 0 to prevent unintended gradient
    accumulation'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将上一轮的梯度设置为0，以防止意外的梯度累积'
- en: '#4 Computes the gradients of the loss given the model parameters'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 根据模型参数计算损失梯度'
- en: '#5 The optimizer uses the gradients to update the model parameters.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 优化器使用梯度来更新模型参数。'
- en: 'Running this code yields the following outputs:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码会产生以下输出：
- en: '[PRE63]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: As we can see, the loss reaches 0 after three epochs, a sign that the model
    converged on the training set. Here, we initialize a model with two inputs and
    two outputs because our toy dataset has two input features and two class labels
    to predict. We used a stochastic gradient descent (`SGD`) optimizer with a learning
    rate (`lr`) of 0.5\. The learning rate is a hyperparameter, meaning it’s a tunable
    setting that we must experiment with based on observing the loss. Ideally, we
    want to choose a learning rate such that the loss converges after a certain number
    of epochs—the number of epochs is another hyperparameter to choose.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，损失在三个epoch后达到0，这是模型在训练集上收敛的标志。在这里，我们初始化了一个有两个输入和两个输出的模型，因为我们的玩具数据集有两个输入特征和两个类别标签需要预测。我们使用了一个学习率（`lr`）为0.5的随机梯度下降（`SGD`）优化器。学习率是一个超参数，意味着它是一个可调整的设置，我们必须根据观察损失进行实验。理想情况下，我们希望选择一个学习率，使得损失在经过一定数量的epoch后收敛——epoch的数量是另一个需要选择的超参数。
- en: Exercise A.3
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习A.3
- en: How many parameters does the neural network introduced in listing A.9 have?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 列表A.9中引入的神经网络有多少个参数？
- en: In practice, we often use a third dataset, a so-called validation dataset, to
    find the optimal hyperparameter settings. A validation dataset is similar to a
    test set. However, while we only want to use a test set precisely once to avoid
    biasing the evaluation, we usually use the validation set multiple times to tweak
    the model settings.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们经常使用第三个数据集，即所谓的验证数据集，以找到最佳的超参数设置。验证数据集类似于测试集。然而，我们只想精确使用一次测试集以避免评估偏差，我们通常多次使用验证集来调整模型设置。
- en: We also introduced new settings called `model.train()` and `model.eval()`. As
    these names imply, these settings are used to put the model into a training and
    an evaluation mode. This is necessary for components that behave differently during
    training and inference, such as *dropout* or *batch normalization* layers. Since
    we don’t have dropout or other components in our `NeuralNetwork` class that are
    affected by these settings, using `model.train()` and `model.eval()` is redundant
    in our preceding code. However, it’s best practice to include them anyway to avoid
    unexpected behaviors when we change the model architecture or reuse the code to
    train a different model.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还引入了新的设置，称为`model.train()`和`model.eval()`。正如这些名称所暗示的，这些设置用于将模型置于训练和评估模式。这对于在训练和推理期间表现不同的组件是必要的，例如*dropout*或*批归一化*层。由于我们的`NeuralNetwork`类中没有受这些设置影响的dropout或其他组件，因此在前面的代码中使用`model.train()`和`model.eval()`是多余的。然而，最好的做法是仍然包含它们，以避免在更改模型架构或重用代码来训练不同模型时出现意外行为。
- en: As discussed earlier, we pass the logits directly into the `cross_entropy` loss
    function, which will apply the `softmax` function internally for efficiency and
    numerical stability reasons. Then, calling `loss.backward()` will calculate the
    gradients in the computation graph that PyTorch constructed in the background.
    The `optimizer.step()` method will use the gradients to update the model parameters
    to minimize the loss. In the case of the SGD optimizer, this means multiplying
    the gradients with the learning rate and adding the scaled negative gradient to
    the parameters.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们直接将logits传递到`cross_entropy`损失函数中，该函数将内部应用`softmax`函数以提高效率和数值稳定性。然后，调用`loss.backward()`将在PyTorch在后台构建的计算图中计算梯度。`optimizer.step()`方法将使用梯度来更新模型参数以最小化损失。对于SGD优化器来说，这意味着将梯度乘以学习率并将缩放后的负梯度加到参数上。
- en: NOTE  To prevent undesired gradient accumulation, it is important to include
    an `optimizer.zero_grad()` call in each update round to reset the gradients to
    0\. Otherwise, the gradients will accumulate, which may be undesired.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了防止不希望的梯度累积，在每个更新轮次中包含一个`optimizer.zero_grad()`调用以将梯度重置为0是非常重要的。否则，梯度将累积，这可能是我们不希望的。
- en: 'After we have trained the model, we can use it to make predictions:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练了模型之后，我们可以使用它来进行预测：
- en: '[PRE64]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The results are
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE65]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'To obtain the class membership probabilities, we can then use PyTorch’s `softmax`
    function:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得类别成员概率，我们然后可以使用PyTorch的`softmax`函数：
- en: '[PRE66]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: This outputs
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出
- en: '[PRE67]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Let’s consider the first row in the preceding code output. Here, the first value
    (column) means that the training example has a 99.91% probability of belonging
    to class 0 and a 0.09% probability of belonging to class 1\. (The `set_printoptions`
    call is used here to make the outputs more legible.)
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑前面代码输出的第一行。在这里，第一个值（列）表示训练示例有99.91%的概率属于类别0，有0.09%的概率属于类别1。（在这里使用`set_printoptions`调用是为了使输出更易读。）
- en: 'We can convert these values into class label predictions using PyTorch’s `argmax`
    function, which returns the index position of the highest value in each row if
    we set `dim=1` (setting `dim=0` would return the highest value in each column
    instead):'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用PyTorch的`argmax`函数将这些值转换为类别标签预测，如果我们设置`dim=1`（设置`dim=0`将返回每列的最高值）：
- en: '[PRE68]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This prints
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印
- en: '[PRE69]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Note that it is unnecessary to compute `softmax` probabilities to obtain the
    class labels. We could also apply the `argmax` function to the logits (outputs)
    directly:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了获得类别标签，不需要计算`softmax`概率。我们也可以直接将`argmax`函数应用于logits（输出）：
- en: '[PRE70]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The output is
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE71]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Here, we computed the predicted labels for the training dataset. Since the
    training dataset is relatively small, we could compare it to the true training
    labels by eye and see that the model is 100% correct. We can double-check this
    using the `==` comparison operator:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们计算了训练数据集的预测标签。由于训练数据集相对较小，我们可以通过肉眼将其与真实的训练标签进行比较，并看到模型是100%正确的。我们可以使用`==`比较运算符来双重检查：
- en: '[PRE72]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The results are
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE73]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Using `torch.sum`, we can count the number of correct predictions:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`torch.sum`，我们可以计算正确预测的数量：
- en: '[PRE74]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The output is
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE75]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Since the dataset consists of five training examples, we have five out of five
    predictions that are correct, which has 5/5 × 100% = 100% prediction accuracy.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集由五个训练示例组成，我们有五个预测全部正确，这表示5/5 × 100% = 100%的预测准确率。
- en: To generalize the computation of the prediction accuracy, let’s implement a
    `compute_accuracy` function, as shown in the following listing.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 为了泛化预测准确度的计算，让我们实现一个 `compute_accuracy` 函数，如下所示。
- en: Listing A.10 A function to compute the prediction accuracy
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.10 计算预测准确度的函数
- en: '[PRE76]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '#1 Returns a tensor of True/False values depending on whether the labels match'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 根据标签是否匹配返回 True/False 值的张量'
- en: '#2 The sum operation counts the number of True values.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 求和操作计算 True 值的数量。'
- en: '#3 The fraction of correct prediction, a value between 0 and 1\. .item() returns
    the value of the tensor as a Python float.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 正确预测的比例，一个介于 0 和 1 之间的值\. .item() 返回张量的值作为 Python 浮点数。'
- en: The code iterates over a data loader to compute the number and fraction of the
    correct predictions. When we work with large datasets, we typically can only call
    the model on a small part of the dataset due to memory limitations. The `compute_accuracy`
    function here is a general method that scales to datasets of arbitrary size since,
    in each iteration, the dataset chunk that the model receives is the same size
    as the batch size seen during training. The internals of the `compute_accuracy`
    function are similar to what we used before when we converted the logits to the
    class labels.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 代码遍历数据加载器以计算正确预测的数量和比例。当我们处理大型数据集时，通常由于内存限制，我们只能对数据集的一小部分调用模型。这里的 `compute_accuracy`
    函数是一个通用方法，可以扩展到任意大小的数据集，因为在每次迭代中，模型接收到的数据集块的大小与训练期间看到的批量大小相同。`compute_accuracy`
    函数的内部结构与我们在将 logits 转换为类别标签时使用的方法类似。
- en: 'We can then apply the function to the training:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将函数应用于训练：
- en: '[PRE77]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The result is
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE78]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Similarly, we can apply the function to the test set:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以将函数应用于测试集：
- en: '[PRE79]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: This prints
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印
- en: '[PRE80]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: A.8 Saving and loading models
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.8 保存和加载模型
- en: 'Now that we’ve trained our model, let’s see how to save it so we can reuse
    it later. Here’s the recommended way of saving and loading models in PyTorch:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经训练了我们的模型，让我们看看如何保存它，以便以后可以重用。以下是使用 PyTorch 保存和加载模型的推荐方法：
- en: '[PRE81]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The model’s `state_dict` is a Python dictionary object that maps each layer
    in the model to its trainable parameters (weights and biases). `"model.pth"` is
    an arbitrary filename for the model file saved to disk. We can give it any name
    and file ending we like; however, `.pth` and `.pt` are the most common conventions.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的 `state_dict` 是一个映射模型中的每个层到其可训练参数（权重和偏差）的 Python 字典对象。`"model.pth"` 是保存到磁盘的模型文件的任意名称。我们可以给它任何我们喜欢的名称和文件扩展名；然而，`.pth`
    和 `.pt` 是最常见的约定。
- en: 'Once we saved the model, we can restore it from disk:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 保存模型后，我们可以从磁盘恢复它：
- en: '[PRE82]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The `torch.load("model.pth")` function reads the file `"model.pth"` and reconstructs
    the Python dictionary object containing the model’s parameters while `model.load_state_dict()`
    applies these parameters to the model, effectively restoring its learned state
    from when we saved it.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.load("model.pth")` 函数读取文件 `"model.pth"` 并重建包含模型参数的 Python 字典对象，而 `model.load_state_dict()`
    将这些参数应用于模型，有效地从我们保存它的状态中恢复其学习状态。'
- en: The line `model` `=` `NeuralNetwork(2,` `2)` is not strictly necessary if you
    execute this code in the same session where you saved a model. However, I included
    it here to illustrate that we need an instance of the model in memory to apply
    the saved parameters. Here, the `NeuralNetwork(2,` `2)` architecture needs to
    match the original saved model exactly.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这行 `model = NeuralNetwork(2, 2)` 如果你在保存模型的同一会话中执行此代码，则不是严格必要的。然而，我将其包括在这里，以说明我们需要在内存中有一个模型的实例来应用保存的参数。在这里，`NeuralNetwork(2,
    2)` 架构需要与原始保存的模型完全匹配。
- en: A.9 Optimizing training performance with GPUs
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.9 使用 GPU 优化训练性能
- en: Next, let’s examine how to utilize GPUs, which accelerate deep neural network
    training compared to regular CPUs. First, we’ll look at the main concepts behind
    GPU computing in PyTorch. Then we will train a model on a single GPU. Finally,
    we’ll look at distributed training using multiple GPUs.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何利用 GPU，与常规 CPU 相比，GPU 可以加速深度神经网络训练。首先，我们将探讨 PyTorch 中 GPU 计算背后的主要概念。然后，我们将在单个
    GPU 上训练一个模型。最后，我们将探讨使用多个 GPU 的分布式训练。
- en: A.9.1 PyTorch computations on GPU devices
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.9.1 在 GPU 设备上执行 PyTorch 计算
- en: Modifying the training loop to run optionally on a GPU is relatively simple
    and only requires changing three lines of code (see section A.7). Before we make
    the modifications, it’s crucial to understand the main concept behind GPU computations
    within PyTorch. In PyTorch, a device is where computations occur and data resides.
    The CPU and the GPU are examples of devices. A PyTorch tensor resides in a device,
    and its operations are executed on the same device.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练循环修改为可选地在GPU上运行相对简单，只需更改三行代码（参见A.7节）。在我们进行修改之前，理解PyTorch中GPU计算背后的主要概念至关重要。在PyTorch中，设备是计算发生和数据驻留的地方。CPU和GPU是设备的例子。PyTorch张量位于设备中，其操作在相同的设备上执行。
- en: 'Let’s see how this works in action. Assuming that you installed a GPU-compatible
    version of PyTorch (see section A.1.3), we can double-check that our runtime indeed
    supports GPU computing via the following code:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个在实际操作中是如何工作的。假设你已经安装了与GPU兼容的PyTorch版本（参见A.1.3节），我们可以通过以下代码来双重检查我们的运行时是否确实支持GPU计算：
- en: '[PRE83]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The result is
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE84]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Now, suppose we have two tensors that we can add; this computation will be
    carried out on the CPU by default:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有两个可以相加的张量；这个计算默认将在CPU上执行：
- en: '[PRE85]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: This outputs
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出
- en: '[PRE86]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We can now use the `.to()` method. This method is the same as the one we use
    to change a tensor’s datatype (see 2.2.2) to transfer these tensors onto a GPU
    and perform the addition there:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`.to()`方法。这个方法与我们用来更改张量数据类型（参见2.2.2节）的方法相同，用于将这些张量传输到GPU并执行加法操作：
- en: '[PRE87]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: The output is
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE88]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: The resulting tensor now includes the device information, `device='cuda:0'`,
    which means that the tensors reside on the first GPU. If your machine hosts multiple
    GPUs, you can specify which GPU you’d like to transfer the tensors to. You do
    so by indicating the device ID in the transfer command. For instance, you can
    use `.to("cuda:0")`, `.to("cuda:1")`, and so on.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 结果张量现在包含了设备信息，`device='cuda:0'`，这意味着张量位于第一块GPU上。如果你的机器有多个GPU，你可以指定你希望将张量传输到哪个GPU。你可以通过在传输命令中指定设备ID来实现这一点。例如，你可以使用`.to("cuda:0")`、`.to("cuda:1")`等等。
- en: 'However, all tensors must be on the same device. Otherwise, the computation
    will fail, where one tensor resides on the CPU and the other on the GPU:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，所有张量必须在同一设备上。否则，计算将失败，其中一个张量位于CPU上，另一个位于GPU上：
- en: '[PRE89]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: The results are
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE90]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: In sum, we only need to transfer the tensors onto the same GPU device, and PyTorch
    will handle the rest.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们只需要将张量传输到相同的GPU设备，PyTorch将处理其余部分。
- en: A.9.2 Single-GPU training
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.9.2 单GPU训练
- en: Now that we are familiar with transferring tensors to the GPU, we can modify
    the training loop to run on a GPU. This step requires only changing three lines
    of code, as shown in the following listing.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们熟悉了将张量传输到GPU的过程，我们可以修改训练循环以在GPU上运行。这一步只需要更改三行代码，如下所示。
- en: Listing A.11 A training loop on a GPU
  id: totrans-356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.11 GPU上的训练循环
- en: '[PRE91]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '#1 Defines a device variable that defaults to a GPU'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义一个默认为GPU的设备变量'
- en: '#2 Transfers the model onto the GPU'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将模型传输到GPU'
- en: '#3 Transfers the data onto the GPU'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将数据传输到GPU'
- en: 'Running the preceding code will output the following, similar to the results
    obtained on the CPU (section A.7):'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码将输出以下内容，类似于在CPU上获得的结果（参见A.7节）：
- en: '[PRE92]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We can use `.to("cuda")` instead of `device` `=` `torch.device("cuda")`. Transferring
    a tensor to `"cuda"` instead of `torch.device("cuda")` works as well and is shorter
    (see section A.9.1). We can also modify the statement, which will make the same
    code executable on a CPU if a GPU is not available. This is considered best practice
    when sharing PyTorch code:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`.to("cuda")`而不是`device` `=` `torch.device("cuda")`。将张量传输到`"cuda"`而不是`torch.device("cuda")`同样有效，并且更简洁（参见A.9.1节）。我们还可以修改语句，这样相同的代码在没有GPU的情况下也能在CPU上执行。这是共享PyTorch代码时的最佳实践：
- en: '[PRE93]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: In the case of the modified training loop here, we probably won’t see a speedup
    due to the memory transfer cost from CPU to GPU. However, we can expect a significant
    speedup when training deep neural networks, especially LLMs.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里修改后的训练循环的情况下，我们可能不会看到由于CPU到GPU的内存传输成本而带来的加速。然而，当训练深度神经网络，尤其是LLMs时，我们可以期待一个显著的加速。
- en: PyTorch on macOS
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PyTorch on macOS
- en: On an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or newer models)
    instead of a computer with an Nvidia GPU, you can change
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在配备苹果硅芯片（如M1、M2、M3或更新的型号）的苹果Mac上，而不是配备Nvidia GPU的计算机上，你可以更改
- en: '[PRE94]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: to
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: to
- en: '[PRE95]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: to take advantage of this chip.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 以利用这个芯片。
- en: Exercise A.4
  id: totrans-372
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习A.4
- en: 'Compare the run time of matrix multiplication on a CPU to a GPU. At what matrix
    size do you begin to see the matrix multiplication on the GPU being faster than
    on the CPU? Hint: use the `%timeit` command in Jupyter to compare the run time.
    For example, given matrices `a` and `b`, run the command `%timeit` `a` `@` `b`
    in a new notebook cell.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 将CPU上矩阵乘法的运行时间与GPU上的运行时间进行比较。在什么矩阵大小下，你开始看到GPU上的矩阵乘法比CPU上的更快？提示：使用Jupyter中的`%timeit`命令来比较运行时间。例如，给定矩阵`a`和`b`，在新的笔记本单元中运行命令`%timeit
    a @ b`。
- en: A.9.3 Training with multiple GPUs
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.9.3 使用多个GPU进行训练
- en: Distributed training is the concept of dividing the model training across multiple
    GPUs and machines. Why do we need this? Even when it is possible to train a model
    on a single GPU or machine, the process could be exceedingly time-consuming. The
    training time can be significantly reduced by distributing the training process
    across multiple machines, each with potentially multiple GPUs. This is particularly
    crucial in the experimental stages of model development, where numerous training
    iterations might be necessary to fine-tune the model parameters and architecture.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练是将模型训练分布在多个GPU和机器上的概念。为什么我们需要这样做？即使可以在单个GPU或机器上训练模型，这个过程也可能非常耗时。通过将训练过程分布在多个机器上，每个机器可能配备多个GPU，可以显著减少训练时间。这在模型开发的实验阶段尤为重要，在该阶段可能需要进行多次训练迭代来微调模型参数和架构。
- en: NOTE  For this book, access to or use of multiple GPUs is not required. This
    section is included for those interested in how multi-GPU computing works in PyTorch.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：对于本书，不需要访问或使用多个GPU。本节包含在此处是为了让那些对PyTorch中多GPU计算如何工作感兴趣的人。
- en: 'Let’s begin with the most basic case of distributed training: PyTorch’s `DistributedDataParallel`
    (DDP) strategy. DDP enables parallelism by splitting the input data across the
    available devices and processing these data subsets simultaneously.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从分布式训练的最基本案例开始：PyTorch的`DistributedDataParallel`（DDP）策略。DDP通过将输入数据分割到可用的设备上并同时处理这些数据子集来实现并行化。
- en: How does this work? PyTorch launches a separate process on each GPU, and each
    process receives and keeps a copy of the model; these copies will be synchronized
    during training. To illustrate this, suppose we have two GPUs that we want to
    use to train a neural network, as shown in figure A.12\.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何工作的？PyTorch在每个GPU上启动一个单独的进程，并且每个进程接收并保留模型的一个副本；这些副本将在训练过程中同步。为了说明这一点，假设我们有两个GPU，我们想用它们来训练一个神经网络，如图A.12所示。
- en: '![figure](../Images/A-12.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/A-12.png)'
- en: Figure A.12 The model and data transfer in DDP involves two key steps. First,
    we create a copy of the model on each of the GPUs. Then we divide the input data
    into unique minibatches that we pass on to each model copy.
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.12 DDP中的模型和数据传输涉及两个关键步骤。首先，我们在每个GPU上创建模型的副本。然后我们将输入数据划分为唯一的minibatch，并将它们传递给每个模型副本。
- en: Each of the two GPUs will receive a copy of the model. Then, in every training
    iteration, each model will receive a minibatch (or just “batch”) from the data
    loader. We can use a `DistributedSampler` to ensure that each GPU will receive
    a different, non-overlapping batch when using DDP.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 两个GPU中的每一个都将接收到模型的一个副本。然后，在每次训练迭代中，每个模型将从数据加载器接收一个minibatch（或简称“batch”）。我们可以使用`DistributedSampler`来确保在DDP中使用时，每个GPU将接收到不同且不重叠的批次。
- en: Since each model copy will see a different sample of the training data, the
    model copies will return different logits as outputs and compute different gradients
    during the backward pass. These gradients are then averaged and synchronized during
    training to update the models. This way, we ensure that the models don’t diverge,
    as illustrated in figure A.13.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个模型副本将看到不同的训练数据样本，因此模型副本将返回不同的logits作为输出，并在反向传播期间计算不同的梯度。然后，这些梯度在训练过程中被平均并同步，以更新模型。这样，我们确保模型不会发散，如图A.13所示。
- en: '![figure](../Images/A-13.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/A-13.png)'
- en: Figure A.13 The forward and backward passes in DDP are executed independently
    on each GPU with its corresponding data subset. Once the forward and backward
    passes are completed, gradients from each model replica (on each GPU) are synchronized
    across all GPUs. This ensures that every model replica has the same updated weights.
  id: totrans-384
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.13 在DDP中，正向和反向传播在每个GPU上独立执行，并处理其对应的数据子集。一旦正向和反向传播完成，每个模型副本（在每个GPU上）的梯度将在所有GPU之间同步。这确保了每个模型副本都有相同的更新权重。
- en: The benefit of using DDP is the enhanced speed it offers for processing the
    dataset compared to a single GPU. Barring a minor communication overhead between
    devices that comes with DDP use, it can theoretically process a training epoch
    in half the time with two GPUs compared to just one. The time efficiency scales
    up with the number of GPUs, allowing us to process an epoch eight times faster
    if we have eight GPUs, and so on.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DDP的好处是它提供的处理数据集的速度比单个GPU更快。除了DDP使用时设备之间附带的一点点通信开销外，理论上，使用两个GPU可以在一半的时间内处理一个训练周期，而只用一个GPU则不行。时间效率随着GPU数量的增加而提高，如果我们有八个GPU，我们可以将一个周期处理得快八倍，依此类推。
- en: NOTE  DDP does not function properly within interactive Python environments
    like Jupyter notebooks, which don’t handle multiprocessing in the same way a standalone
    Python script does. Therefore, the following code should be executed as a script,
    not within a notebook interface like Jupyter. DDP needs to spawn multiple processes,
    and each process should have its own Python interpreter instance.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：DDP在像Jupyter笔记本这样的交互式Python环境中无法正常工作，因为这些环境没有像独立Python脚本那样处理多进程。因此，以下代码应作为脚本执行，而不是在Jupyter这样的笔记本界面中执行。DDP需要生成多个进程，并且每个进程都应该有自己的Python解释器实例。
- en: Let’s now see how this works in practice. For brevity, I focus on the core parts
    of the code that need to be adjusted for DDP training. However, readers who want
    to run the code on their own multi-GPU machine or a cloud instance of their choice
    should use the standalone script provided in this book’s GitHub repository at
    [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看这在实践中是如何工作的。为了简洁起见，我专注于需要调整以进行DDP训练的代码的核心部分。然而，那些想在他们的多GPU机器或他们选择的云实例上运行代码的读者应使用本书GitHub仓库中提供的独立脚本，网址为[https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)。
- en: First, we import a few additional submodules, classes, and functions for distributed
    training PyTorch, as shown in the following listing.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入了一些用于分布式训练的PyTorch的附加子模块、类和函数，如下所示。
- en: Listing A.12 PyTorch utilities for distributed training
  id: totrans-389
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.12 PyTorch分布式训练实用工具
- en: '[PRE96]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Before we dive deeper into the changes to make the training compatible with
    DDP, let’s briefly go over the rationale and usage for these newly imported utilities
    that we need alongside the `DistributedDataParallel` class.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨使训练与DDP兼容的更改之前，让我们简要地回顾一下这些新导入的实用工具的原理和用法，这些实用工具是我们需要与`DistributedDataParallel`类一起使用的。
- en: PyTorch’s `multiprocessing` submodule contains functions such as `multiprocessing
    .spawn`, which we will use to spawn multiple processes and apply a function to
    multiple inputs in parallel. We will use it to spawn one training process per
    GPU. If we spawn multiple processes for training, we will need a way to divide
    the dataset among these different processes. For this, we will use the `DistributedSampler`.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的`multiprocessing`子模块包含`multiprocessing.spawn`等函数，我们将使用这些函数来生成多个进程，并将函数并行应用于多个输入。我们将用它来为每个GPU生成一个训练进程。如果我们为训练生成多个进程，我们需要一种方法将这些不同的进程中的数据集划分。为此，我们将使用`DistributedSampler`。
- en: '`init_process_group` and `destroy_process_group` are used to initialize and
    quit the distributed training mods. The `init_process_group` function should be
    called at the beginning of the training script to initialize a process group for
    each process in the distributed setup, and `destroy_process_group` should be called
    at the end of the training script to destroy a given process group and release
    its resources. The code in the following listing illustrates how these new components
    are used to implement DDP training for the `NeuralNetwork` model we implemented
    earlier.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '`init_process_group`和`destroy_process_group`用于初始化和退出分布式训练模块。应在训练脚本的开始处调用`init_process_group`函数来初始化分布式设置中每个进程的过程组，并在训练脚本的末尾调用`destroy_process_group`来销毁给定过程组并释放其资源。以下列表中的代码说明了如何使用这些新组件来实现我们之前实现的`NeuralNetwork`模型的DDP训练。'
- en: Listing A.13 Model training with the `DistributedDataParallel` strategy
  id: totrans-394
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.13 使用`DistributedDataParallel`策略进行模型训练
- en: '[PRE97]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '#1 Address of the main node'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 主节点的地址'
- en: '#2 Any free port on the machine'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 机器上的任何空闲端口'
- en: '#3 nccl stands for NVIDIA Collective Communication Library.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 nccl代表NVIDIA集体通信库。'
- en: '#4 rank refers to the index of the GPU we want to use.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 rank指的是我们想要使用的GPU的索引。'
- en: '#5 world_size is the number of GPUs to use.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 world_size表示要使用的GPU数量。'
- en: '#6 Sets the current GPU device on which tensors will be allocated and operations
    will be performed'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 设置当前GPU设备，在该设备上分配张量并执行操作'
- en: '#7 Distributed-Sampler takes care of the shuffling now.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 分布式-Sampler现在负责打乱。'
- en: '#8 Enables faster memory transfer when training on GPU'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 在GPU上训练时，启用更快的内存传输'
- en: '#9 Splits the dataset into distinct, non-overlapping subsets for each process
    (GPU)'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 将数据集分割成每个进程（GPU）的独立、不重叠的子集'
- en: '#10 The main function running the model training'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 运行模型训练的主函数'
- en: '#11 rank is the GPU ID'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 `rank` 是GPU ID'
- en: '#12 Cleans up resource allocation'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '#12 清理资源分配'
- en: '#13 Launches the main function using multiple processes, where nprocs=world_size
    means one process per GPU.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '#13 使用多个进程启动主函数，其中 nprocs=world_size 表示每个GPU一个进程。'
- en: Before we run this code, let’s summarize how it works in addition to the preceding
    annotations. We have a `__name__` `==` `"__main__"` clause at the bottom containing
    code executed when we run the code as a Python script instead of importing it
    as a module. This code first prints the number of available GPUs using `torch.cuda.device_count()`,
    sets a random seed for reproducibility, and then spawns new processes using PyTorch’s
    `multiprocessesing.spawn` function. Here, the `spawn` function launches one process
    per GPU setting `nproces=world_size`, where the world size is the number of available
    GPUs. This `spawn` function launches the code in the `main` function we define
    in the same script with some additional arguments provided via `args`. Note that
    the `main` function has a `rank` argument that we don’t include in the `mp.spawn()`
    call. That’s because the `rank`, which refers to the process ID we use as the
    GPU ID, is already passed automatically.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此代码之前，让我们总结一下它的工作原理，以及前面的注释。我们在底部有一个 `__name__ == "__main__"` 子句，包含当我们以Python脚本的形式运行代码而不是将其作为模块导入时执行的代码。此代码首先使用
    `torch.cuda.device_count()` 打印可用的GPU数量，设置随机种子以确保可重复性，然后使用PyTorch的 `multiprocessing.spawn`
    函数启动新进程。在这里，`spawn` 函数为每个GPU启动一个进程，设置 `nprocs=world_size`，其中世界大小是可用的GPU数量。此 `spawn`
    函数使用通过 `args` 提供的一些额外参数在同一个脚本中定义的 `main` 函数启动代码。请注意，`main` 函数有一个 `rank` 参数，我们不包括在
    `mp.spawn()` 调用中。这是因为 `rank`，它指的是我们用作GPU ID的进程ID，已经自动传递。
- en: The `main` function sets up the distributed environment via `ddp_setup`—another
    function we defined—loads the training and test sets, sets up the model, and carries
    out the training. Compared to the single-GPU training (section A.9.2), we now
    transfer the model and data to the target device via .`to(rank)`, which we use
    to refer to the GPU device ID. Also, we wrap the model via `DDP`, which enables
    the synchronization of the gradients between the different GPUs during training.
    After the training finishes and we evaluate the models, we use `destroy_process_group()`
    to cleanly exit the distributed training and free up the allocated resources.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '`main` 函数通过 `ddp_setup`（我们定义的另一个函数）设置分布式环境，加载训练和测试集，设置模型，并执行训练。与单GPU训练（第A.9.2节）相比，我们现在通过
    `.to(rank)` 将模型和数据传输到目标设备，我们使用 `rank` 来引用GPU设备ID。此外，我们通过 `DDP` 包装模型，这使不同GPU在训练期间能够同步梯度。训练完成后，我们评估模型后，使用
    `destroy_process_group()` 清理分布式训练并释放分配的资源。'
- en: Earlier I mentioned that each GPU will receive a different subsample of the
    training data. To ensure this, we set `sampler=DistributedSampler(train_ds)` in
    the training loader.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到每个GPU将接收到训练数据的不同子样本。为了确保这一点，我们在训练加载器中设置 `sampler=DistributedSampler(train_ds)`。
- en: The last function to discuss is `ddp_setup`. It sets the main node’s address
    and port to allow for communication between the different processes, initializes
    the process group with the NCCL backend (designed for GPU-to-GPU communication),
    and sets the `rank` (process identifier) and world size (total number of processes).
    Finally, it specifies the GPU device corresponding to the current model training
    process rank.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要讨论的函数是 `ddp_setup`。它设置主节点的地址和端口，以便不同进程之间的通信，使用NCCL后端（专为GPU到GPU通信设计）初始化进程组，并设置
    `rank`（进程标识符）和世界大小（进程总数）。最后，它指定与当前模型训练进程 `rank` 对应的GPU设备。
- en: Selecting available GPUs on a multi-GPU machine
  id: totrans-413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在多GPU机器上选择可用的GPU
- en: 'If you wish to restrict the number of GPUs used for training on a multi-GPU
    machine, the simplest way is to use the `CUDA_VISIBLE_DEVICES` environment variable.
    To illustrate this, suppose your machine has multiple GPUs, and you only want
    to use one GPU—for example, the GPU with index 0\. Instead of `python` `some_script.py`,
    you can run the following code from the terminal:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望限制在多 GPU 机器上用于训练的 GPU 数量，最简单的方法是使用 `CUDA_VISIBLE_DEVICES` 环境变量。为了说明这一点，假设您的机器有多个
    GPU，您只想使用一个 GPU——例如，索引为 0 的 GPU。您可以从终端运行以下代码而不是 `python some_script.py`：
- en: '[PRE98]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Or, if your machine has four GPUs and you only want to use the first and third
    GPU, you can use
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您的机器有四个 GPU，您只想使用第一个和第三个 GPU，您可以使用
- en: '[PRE99]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Setting `CUDA_VISIBLE_DEVICES` in this way is a simple and effective way to
    manage GPU allocation without modifying your PyTorch scripts.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式设置 `CUDA_VISIBLE_DEVICES` 是一种简单有效的方法来管理 GPU 分配，而无需修改您的 PyTorch 脚本。
- en: 'Let’s now run this code and see how it works in practice by launching the code
    as a script from the terminal:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行此代码，通过从终端作为脚本启动代码来实际查看其工作情况：
- en: '[PRE100]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Note that it should work on both single and multi-GPU machines. If we run this
    code on a single GPU, we should see the following output:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，它应该在单 GPU 和多 GPU 机器上都能工作。如果我们在这个单 GPU 上运行此代码，我们应该看到以下输出：
- en: '[PRE101]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: The code output looks similar to that using a single GPU (section A.9.2), which
    is a good sanity check.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出与使用单个 GPU 的输出类似（第 A.9.2 节），这是一个很好的合理性检查。
- en: 'Now, if we run the same command and code on a machine with two GPUs, we should
    see the following:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们在一个具有两个 GPU 的机器上运行相同的命令和代码，我们应该看到以下输出：
- en: '[PRE102]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'As expected, we can see that some batches are processed on the first GPU (`GPU0`)
    and others on the second (`GPU1`). However, we see duplicated output lines when
    printing the training and test accuracies. Each process (in other words, each
    GPU) prints the test accuracy independently. Since DDP replicates the model onto
    each GPU and each process runs independently, if you have a print statement inside
    your testing loop, each process will execute it, leading to repeated output lines.
    If this bothers you, you can fix it using the rank of each process to control
    your print statements:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，我们可以看到一些批次在第一个 GPU (`GPU0`) 上处理，而其他批次在第二个 (`GPU1`) 上处理。然而，当打印训练和测试准确率时，我们看到了重复的输出行。每个进程（换句话说，每个
    GPU）独立地打印测试准确率。由于 DDP 将模型复制到每个 GPU，并且每个进程独立运行，如果您在测试循环中有一个打印语句，每个进程都会执行它，导致重复的输出行。如果您觉得这很麻烦，您可以使用每个进程的秩来控制您的打印语句：
- en: '[PRE103]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '#1 Only print in the first process'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 只在第一个进程中打印'
- en: This is, in a nutshell, how distributed training via DDP works. If you are interested
    in additional details, I recommend checking the official API documentation at
    [https://mng.bz/9dPr](https://mng.bz/9dPr).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这就是通过 DDP 进行分布式训练的工作方式。如果您对更多细节感兴趣，我建议您查看官方 API 文档，网址为 [https://mng.bz/9dPr](https://mng.bz/9dPr)。
- en: Alternative PyTorch APIs for multi-GPU training
  id: totrans-430
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多 GPU 训练的替代 PyTorch API
- en: 'If you prefer a more straightforward way to use multiple GPUs in PyTorch, you
    can consider add-on APIs like the open-source Fabric library. I wrote about it
    in “Accelerating PyTorch Model Training: Using Mixed-Precision and Fully Sharded
    Data Parallelism” ([https://mng.bz/jXle](https://mng.bz/jXle)).'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您更喜欢在 PyTorch 中使用多个 GPU 的更直接的方法，您可以考虑附加 API，如开源的 Fabric 库。我在“加速 PyTorch 模型训练：使用混合精度和完全分片数据并行”一文中提到了它（[https://mng.bz/jXle](https://mng.bz/jXle)）。
- en: Summary
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'PyTorch is an open source library with three core components: a tensor library,
    automatic differentiation functions, and deep learning utilities.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 是一个开源库，包含三个核心组件：张量库、自动微分函数和深度学习工具。
- en: PyTorch’s tensor library is similar to array libraries like NumPy.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 的张量库类似于 NumPy 等数组库。
- en: In the context of PyTorch, tensors are array-like data structures representing
    scalars, vectors, matrices, and higher-dimensional arrays.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 的上下文中，张量是表示标量、向量、矩阵和更高维数组的类似数组的结构。
- en: PyTorch tensors can be executed on the CPU, but one major advantage of PyTorch’s
    tensor format is its GPU support to accelerate computations.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 张量可以在 CPU 上执行，但 PyTorch 张量格式的一个主要优势是其对 GPU 的支持，可以加速计算。
- en: The automatic differentiation (autograd) capabilities in PyTorch allow us to
    conveniently train neural networks using backpropagation without manually deriving
    gradients.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的自动微分（autograd）功能使我们能够方便地使用反向传播训练神经网络，而无需手动推导梯度。
- en: The deep learning utilities in PyTorch provide building blocks for creating
    custom deep neural networks.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的深度学习工具提供了创建自定义深度神经网络的构建块。
- en: PyTorch includes `Dataset` and `DataLoader` classes to set up efficient data-loading
    pipelines.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 包含 `Dataset` 和 `DataLoader` 类来设置高效的数据加载管道。
- en: It’s easiest to train models on a CPU or single GPU.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 CPU 或单个 GPU 上训练模型最为简单。
- en: Using `DistributedDataParallel` is the simplest way in PyTorch to accelerate
    the training if multiple GPUs are available.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有多个 GPU 可用，使用 `DistributedDataParallel` 是 PyTorch 中加速训练的最简单方法。
