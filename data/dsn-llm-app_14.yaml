- en: Chapter 11\. Representation Learning and Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章\. 表示学习和嵌入
- en: In the previous chapter, we learned how we can interface language models with
    external tools, including data stores. External data can be present in the form
    of text files, database tables, and knowledge graphs. Data can span a wide variety
    of content types, from proprietary domain-specific knowledge bases to intermediate
    results and outputs generated by LLMs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何将语言模型与外部工具接口，包括数据存储。外部数据可以以文本文件、数据库表和知识图谱的形式存在。数据可以跨越广泛的内容类型，从专有领域的知识库到由LLM生成的中间结果和输出。
- en: If the data are structured, for example residing in a relational database, the
    language model can issue a SQL query to retrieve the data it needs. But what if
    the data are present in unstructured form?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是有结构的，例如存储在关系型数据库中，语言模型可以发出SQL查询来检索所需的数据。但如果是非结构化形式的数据呢？
- en: 'One way to retrieve data from unstructured text datasets is to search by keywords
    or use regular expressions. For the Apple CFO example in the previous chapter,
    we can retrieve text containing CFO mentions from a corpus containing financial
    disclosures, hoping that it will contain the join date or tenure information.
    For instance, you can use the regex:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从非结构化文本数据集中检索数据的一种方法是通过关键词搜索或使用正则表达式。对于上一章中提到的苹果公司CFO的例子，我们可以从包含财务披露的语料库中检索包含CFO提及的文本，希望其中包含加入日期或任期信息。例如，你可以使用以下正则表达式：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Keyword search is limited in its effectiveness. There are a very large number
    of ways to express CFO join date or tenure in a corpus, if it is present at all.
    Trying to use a catch-all regex like the above could result in a large proportion
    of false positives.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词搜索在有效性上存在局限性。如果语料库中存在CFO加入日期或任期，那么表达这些信息的方式有非常多种。尝试使用上述通用的正则表达式可能会导致大量误报。
- en: Therefore, we need to move beyond keyword search. Over the last few decades,
    the field of information retrieval has developed several methods like BM25 that
    have shaped search systems. We will learn more about these methods in [Chapter 12](ch12.html#ch12).
    In the LLM era, embedding-based search systems are fast becoming the standard
    way of implementing search.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要超越关键词搜索。在过去几十年中，信息检索领域已经发展出多种方法，如BM25，这些方法塑造了搜索系统。我们将在[第12章](ch12.html#ch12)中了解更多关于这些方法的内容。在LLM时代，基于嵌入的搜索系统正迅速成为实现搜索的标准方式。
- en: In this chapter, we will learn how embeddings work. We will explore the concept
    of semantic similarity and examine various similarity measures. We will learn
    how to use popular embedding models and evaluate their performance. We will also
    show how to fine-tune embedding models to suit specific use cases and domains.
    We will show how to interpret these embeddings using sparse autoencoders (SAEs).
    Finally, we will discuss techniques for optimizing embeddings to reduce storage
    requirements and computational overhead.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习嵌入是如何工作的。我们将探讨语义相似性的概念，并检查各种相似度度量。我们将学习如何使用流行的嵌入模型并评估其性能。我们还将展示如何微调嵌入模型以适应特定的用例和领域。我们将展示如何使用稀疏自编码器（SAEs）来解释这些嵌入。最后，我们将讨论优化嵌入以减少存储需求和计算开销的技术。
- en: Introduction to Embeddings
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入简介
- en: Representation learning is a subfield of machine learning that deals with learning
    to represent data in a way that captures its meaningful features, often in a low
    dimensional space. In the context of NLP, this involves transforming textual units
    like words, sentences, or paragraphs into vector form, called embeddings. Embeddings
    capture semantic (meaning-related) and pragmatic (social context-related) features
    of the input.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 表示学习是机器学习的一个子领域，它涉及学习以捕捉数据有意义特征的方式表示数据，通常是在低维空间中。在NLP的背景下，这涉及到将文本单元如单词、句子或段落转换为向量形式，称为嵌入。嵌入捕捉输入的语义（与意义相关）和语用（与社会语境相关）特征。
- en: Embeddings can be generated using both open source libraries and paywalled APIs.
    [Sentence Transformers](https://oreil.ly/4OSVd) is a very well-known open source
    library for generating embeddings, and it provides access to embedding models
    that performs competitively with respect to proprietary ones.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入可以使用开源库和付费API生成。[Sentence Transformers](https://oreil.ly/4OSVd)是一个非常著名的开源库，用于生成嵌入，它提供了与专有模型竞争的嵌入模型。
- en: 'Let’s generate embeddings using the `Sentence Transformers` library:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`Sentence Transformers`库来生成嵌入：
- en: '[PRE1] `convert_to_tensor``=``True``)` `print``(``"Embedding size:"``,` `embedding``.``shape``[``0``])`
    `print``(``embedding``)` [PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]` Output:    [PRE4]    For this model, the embedding size is 768, which
    means each vector has 768 dimensions. The sequence length of this particular model
    is 512, which means the input text is restricted to 512 tokens, beyond which it
    will be truncated. The embedding vector is made up of floating-point numbers,
    which by themselves are not interpretable. We will discuss techniques for interpreting
    embeddings later in this chapter.    Most embedding models used today are based
    on encoder-only language models, which we introduced in [Chapter 4](ch04.html#chapter_transformer-architecture).
    The underlying models are BERT, RoBERTa, MPNet, etc., and are typically fine-tuned
    on paraphrasing/question-answering/natural language inference datasets. Let’s
    see how to derive embeddings from these types of models (which is what the `sentence_transformers.encode()`
    function does under the hood):    [PRE5]   `In this example, the embedding is
    drawn from the [CLS] token of the last layer of the DistilBERT model. Other ways
    of extracting embeddings from models include:    *   Mean pooling, where the average
    is taken across all token outputs in the sequence           *   Max pooling, where
    the maximum value in each dimension across all tokens is taken           *   Weighted
    mean, where more weight is given to the last few tokens           *   Last token,
    where the embedding is just the encoder output of the last token              ######
    Tip    Whether the last token (or the first token) contains good representations
    of the entire sequence depends a lot on the pre-training and the fine-tuning objective.
    BERT’s pre-training objective (next-sentence prediction) ensures that the [CLS]
    token is much richer in representation than, say, RoBERTa, which doesn’t use the
    next-sentence prediction objective and thus its <s> start sequence token isn’t
    as informative.    Recently, decoder-based embedding models have started gaining
    prominence, like the [SGPT family of models](https://oreil.ly/AztT9). OpenAI exposes
    a single embedding endpoint for both search and similarity. OpenAI embeddings
    have a much larger maximum sequence length (8,192 tokens), and a much larger dimension
    size (1,536–3,072). Cohere and Jina are examples of other embedding providers.    Choosing
    the right model for your task depends on cost, latency, storage limitations, performance,
    and the data domain of your use case. I suggest starting off with the small but
    effective all-mpnet-base-v2 model available through the Sentence Transformers
    library, which I consider the workhorse of the field of NLP. As always, experimenting
    with different models never hurts. More tips on selecting the right models will
    be provided throughout the rest of the chapter. Later in the chapter, we will
    also show how to evaluate embedding models and introduce popular benchmarks.    ######
    Warning    There is no such thing as infinite compression! Embedding sizes are
    fixed, so the longer your input, the less information can be encoded in its embedding.
    Managing this tradeoff differs by use case.` [PRE6]``  [PRE7][PRE8]``py[PRE9][PRE10]
    chunks = [''The President of the U.S is Joe Biden'', ''Ramen consumption has increased
    in the last 5 months''] [PRE11] from sentence_transformers import SentenceTransformer,
    util sbert_model = SentenceTransformer(''msmarco-distilbert-base-tas-b'') chunk_embeddings
    = sbert_model.encode(chunks, show_progress_bar=True, device=''cuda'', normalize_embeddings=True,
    convert_to_tensor=True) query_embedding = sbert_model.encode(query, device=''cuda'',
    normalize_embeddings=True, convert_to_tensor=True) matches = util.semantic_search(query_embedding,
    chunk_embeddings, score_function=util.dot_score) [PRE12] [[{''corpus_id'': 0,
    ''score'': 0.8643729090690613},   {''corpus_id'': 1, ''score'': 0.6223753690719604}]]
    [PRE13] !pip install sentence-transformers  from sentence_transformers import
    SentenceTransformer, util model = SentenceTransformer(''all-mpnet-base-v2'')  sentences
    = [''After his 25th anniversary at the company, Mr. Pomorenko `confirmed` `that`
    `he` `is` `not` `retiring``'',  ''``Mr``.` `Pomorenko` `announced` `his` `retirement`
    `yesterday``'']` [PRE14] [PRE15]`` [PRE16] Cosine Similarity: 0.7870 [PRE17] Cosine
    Similarity: 0.7677! [PRE18]` [PRE19][PRE20][PRE21] from datasets import load_dataset
    from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer
    from sentence_transformers.losses import TripletLoss  model = SentenceTransformer(
    "''all-mpnet-base-v2''")  dataset = load_dataset("csv", data_files="negatives_dataset.csv")  loss
    = TripletLoss(model)  trainer = SentenceTransformerTrainer(     model=model,     train_dataset=dataset     loss=loss    )
    trainer.train() model.save_pretrained("mpnet_finetuned_negatives") [PRE22] !pip
    install InstructorEmbedding  from InstructorEmbedding import INSTRUCTOR model
    = INSTRUCTOR(''hkunlp/instructor-large'')  customized_embeddings = model.encode(
    [[''Represent the question for retrieving supporting documents:'',   ''Who is
    the CEO of Apple''],  [''Represent the sentence for retrieval:'',   ''Tim Cook
    is the CEO of Apple''],  [''Represent the sentence for retrieval:'',   ''He is
    a musically gifted CEO''], ) [PRE23] ‘Represent the {domain} {text_type} for {task_objective}:’
    [PRE24] from sentence_transformers import SentenceTransformer from sentence_transformers
    import SentenceTransformerTrainer, losses from datasets import load_dataset  model
    = SentenceTransformer("all-mpnet-base-v2") train_dataset = load_dataset("csv",
    data_files="finetune_dataset.csv") loss = losses.MultipleNegativesRankingLoss(model)
    loss = losses.MatryoshkaLoss(model, loss, [768, 512, 256, 128]])  trainer = SentenceTransformerTrainer(     model=model,     train_dataset=train_dataset,     loss=loss,
    ) trainer.train() [PRE25] from sentence_transformers.quantization import quantize_embeddings  model
    = SentenceTransformer("all-mpnet-base-v2") embeddings = model.encode(["I heard
    the horses are excited for Halloween.", "Dalmatians are the most patriotic of
    dogs.", "This restaurant is making me `nostalgic``.``"])` [PRE26] [PRE27] `` `quantize_embeddings`
    also supports int8 quantization. In this scheme, the four bytes representing each
    dimension are converted into an integer value, represented in one byte. The integer
    can be either signed or unsigned, thus representing values between –127 and 127
    or between 0 and 255, respectively. The conversion process is guided using a calibration
    dataset of embeddings, from which we calculate the minimum and maximum value of
    each dimension. These values are then used in the normalization formula to convert
    the numbers from one range to another.    ###### Tip    It has been shown that
    for some [embedding models](https://oreil.ly/Mp3pu), binary embeddings perform
    better than int8 embeddings despite the reduced precision! This is largely because
    of the calibration dataset used and the challenge involved in mapping float values
    to buckets of int8 values. `` [PRE28]`` [PRE29] !pip install chromadb  import
    chromadb chroma_client = chromadb.Client()  collection = chroma_client.create_collection(name="mango_science")
    chunks = [''353 varieties of mangoes are now extinct'', ''Mangoes are grown in
    the tropics''] metadata = [{"topic": "extinction", "chapter": "2"}, {"topic":
    "regions",   "chapter": "5"}] unique_ids = [str(i) for i in range(len(chunks))]  collection.add(    documents=chunks,    metadatas=metadata,    ids=unique_ids   )
    results = collection.query(    query_texts=["Where are mangoes grown?"],    n_results=2,    where={"chapter":
    { "$ne": "2"}},    where_document={"$contains":"grown"} ) [PRE30]` [PRE31][PRE32][PRE33][PRE34][PRE35]
    [PRE36]'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]` 输出：    [PRE4]    对于这个模型，嵌入大小为768，这意味着每个向量有768个维度。这个特定模型的序列长度为512，这意味着输入文本被限制在512个标记内，超出部分将被截断。嵌入向量由浮点数组成，这些浮点数本身是不可解释的。我们将在本章后面讨论解释嵌入的技术。    目前使用的多数嵌入模型都是基于仅编码器语言模型，我们在[第4章](ch04.html#chapter_transformer-architecture)中介绍了这种模型。底层模型包括BERT、RoBERTa、MPNet等，通常在释义/问答/自然语言推理数据集上进行微调。让我们看看如何从这些类型的模型中提取嵌入（这是`sentence_transformers.encode()`函数在底层所做的事情）：    [PRE5]   `在这个例子中，嵌入是从DistilBERT模型的最后一层的[CLS]标记中提取的。从模型中提取嵌入的其他方法包括：    *   均值池化，即在序列中所有标记输出上取平均值           *   最大池化，即在所有标记的每个维度上取最大值           *   加权均值，即给予最后几个标记更多的权重           *   最后一个标记，其中嵌入只是最后一个标记的编码器输出              ######
    小贴士    最后一个标记（或第一个标记）是否包含整个序列的良好表示，很大程度上取决于预训练和微调目标。BERT的预训练目标（下一句预测）确保了[CLS]标记比，比如说，不使用下一句预测目标的RoBERTa更丰富，因此其<s>起始序列标记的信息量不是很大。    最近，基于解码器的嵌入模型开始变得突出，如[SGPT模型系列](https://oreil.ly/AztT9)。OpenAI为搜索和相似度暴露了一个单一的嵌入端点。OpenAI嵌入具有更大的最大序列长度（8,192个标记）和更大的维度大小（1,536–3,072）。Cohere和Jina是其他嵌入提供商的例子。    选择适合您任务的正确模型取决于成本、延迟、存储限制、性能以及您用例的数据域。我建议从Sentence
    Transformers库中提供的有效但小巧的all-mpnet-base-v2模型开始，我认为它是NLP领域的“工作马”。像往常一样，尝试不同的模型永远不会有害。本章的其余部分将提供有关选择正确模型的更多提示。稍后在本章中，我们还将展示如何评估嵌入模型并介绍流行的基准。    ######
    警告    没有无限压缩这回事！嵌入大小是固定的，所以您的输入越长，其嵌入中可以编码的信息就越少。管理这种权衡因用例而异。` [PRE6]``  [PRE7][PRE8]``py[PRE9][PRE10]
    chunks = [''The President of the U.S is Joe Biden'', ''Ramen consumption has increased
    in the last 5 months''] [PRE11] 从 sentence_transformers 导入 SentenceTransformer,
    util sbert_model = SentenceTransformer(''msmarco-distilbert-base-tas-b'') chunk_embeddings
    = sbert_model.encode(chunks, show_progress_bar=True, device=''cuda'', normalize_embeddings=True,
    convert_to_tensor=True) query_embedding = sbert_model.encode(query, device=''cuda'',
    normalize_embeddings=True, convert_to_tensor=True) matches = util.semantic_search(query_embedding,
    chunk_embeddings, score_function=util.dot_score) [PRE12] [[{''corpus_id'': 0,
    ''score'': 0.8643729090690613},   {''corpus_id'': 1, ''score'': 0.6223753690719604}]]
    [PRE13] !pip install sentence-transformers  from sentence_transformers import
    SentenceTransformer, util model = SentenceTransformer(''all-mpnet-base-v2'')  sentences
    = [''After his 25th anniversary at the company, Mr. Pomorenko `confirmed` `that`
    `he` `is` `not` `retiring``'',  ''``Mr``.` `Pomorenko` `announced` `his` `retirement`
    `yesterday``'']` [PRE14] [PRE15]`` [PRE16] 余弦相似度：0.7870 [PRE17] 余弦相似度：0.7677!
    [PRE18]` [PRE19][PRE20][PRE21] 从 datasets 导入 load_dataset 从 sentence_transformers
    导入 SentenceTransformer, SentenceTransformerTrainer 从 sentence_transformers.losses
    导入 TripletLoss model = SentenceTransformer("''all-mpnet-base-v2''") dataset =
    load_dataset("csv", data_files="negatives_dataset.csv") loss = TripletLoss(model)
    trainer = SentenceTransformerTrainer(     model=model,     train_dataset=dataset,     loss=loss    )
    trainer.train() model.save_pretrained("mpnet_finetuned_negatives") [PRE22] !pip
    install InstructorEmbedding  from InstructorEmbedding 导入 INSTRUCTOR model = INSTRUCTOR(''hkunlp/instructor-large'')  customized_embeddings
    = model.encode( [[''Represent the question for retrieving supporting documents:'',   ''Who
    is the CEO of Apple''],  [''Represent the sentence for retrieval:'',   ''Tim Cook
    is the CEO of Apple''],  [''Represent the sentence for retrieval:'',   ''He is
    a musically gifted CEO''], ) [PRE23] ‘Represent the {domain} {text_type} for {task_objective}:’
    [PRE24] 从 sentence_transformers 导入 SentenceTransformer 从 sentence_transformers
    导入 SentenceTransformerTrainer, losses 从 datasets 导入 load_dataset model = SentenceTransformer("all-mpnet-base-v2")
    train_dataset = load_dataset("csv", data_files="finetune_dataset.csv") loss =
    losses.MultipleNegativesRankingLoss(model) loss = losses.MatryoshkaLoss(model,
    loss, [768, 512, 256, 128]])  trainer = SentenceTransformerTrainer(     model=model,     train_dataset=train_dataset,     loss=loss,
    ) trainer.train() [PRE25] 从 sentence_transformers.quantization 导入 quantize_embeddings
    model = SentenceTransformer("all-mpnet-base-v2") embeddings = model.encode(["I
    heard the horses are excited for Halloween.", "Dalmatians are the most patriotic
    of dogs.", "This restaurant is making me `nostalgic``.``"])` [PRE26] [PRE27] ``
    `quantize_embeddings` 也支持int8量化。在这种方案中，代表每个维度的四个字节被转换成一个整数值，用一个字节表示。这个整数可以是带符号的或无符号的，因此可以表示介于-127和127或介于0和255之间的值。转换过程使用嵌入的校准数据集指导，从中我们计算每个维度的最小值和最大值。然后，这些值用于归一化公式，将数字从一种范围转换为另一种范围。    ######
    小贴士    已有研究表明，对于某些[嵌入模型](https://oreil.ly/Mp3pu)，二进制嵌入比int8嵌入表现更好，尽管精度有所降低！这很大程度上是因为使用的校准数据集和将浮点值映射到int8值桶的挑战。
    `` [PRE28]`` [PRE29] !pip install chromadb  导入 chromadb chroma_client = chromadb.Client()  collection
    = chroma_client.create_collection(name="mango_science") chunks = [''353 varieties
    of mangoes are now extinct'', ''Mangoes are grown in the tropics''] metadata =
    [{"topic": "extinction", "chapter": "2"}, {"topic": "regions",   "chapter": "5"}]
    unique_ids = [str(i) for i in range(len(chunks))]  collection.add(    documents=chunks,    metadatas=metadata,    ids=unique_ids   )
    results = collection.query(    query_texts=["Where are mangoes grown?"],    n_results=2,    where={"chapter":
    { "$ne": "2"}},    where_document={"$contains":"grown"} ) [PRE30]` [PRE31][PRE32][PRE33][PRE34][PRE35]
    [PRE36]'
