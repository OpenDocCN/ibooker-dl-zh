- en: Chapter 1\. LLM Fundamentals with LangChain
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章：使用LangChain的LLM基础
- en: The [Preface](preface01.html#pr01_preface_1736545679069216) gave you a taste
    of the power of LLM prompting, where we saw firsthand the impact that different
    prompting techniques can have on what you get out of LLMs, especially when judiciously
    combined. The challenge in building good LLM applications is, in fact, in how
    to effectively construct the prompt sent to the model and process the model’s
    prediction to return an accurate output (see [Figure 1-1](#ch01_figure_1_1736545659763063)).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[前言](preface01.html#pr01_preface_1736545679069216)让你领略了LLM提示的强大功能，我们亲眼见证了不同的提示技术如何影响从LLM中获得的结果，尤其是在谨慎结合时。实际上，构建好的LLM应用的挑战在于如何有效地构建发送给模型的提示，并处理模型的预测以返回准确的结果（参见[图1-1](#ch01_figure_1_1736545659763063)）。'
- en: '![](assets/lelc_0101.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/lelc_0101.png)'
- en: Figure 1-1\. The challenge in making LLMs a useful part of your application
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1. 将LLM作为应用程序有用部分的挑战
- en: If you can solve this problem, you are well on your way to building LLM applications,
    simple and complex alike. In this chapter, you’ll learn more about how LangChain’s
    building blocks map to LLM concepts and how, when combined effectively, they enable
    you to build LLM applications. But first, the sidebar [“Why LangChain?”](#ch01_why_langchain_1736545659776355)
    is a brief primer on why we think it useful to use LangChain to build LLM applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够解决这个问题，那么你已经在构建LLM应用的道路上走得很远了，无论是简单还是复杂的LLM应用。在本章中，你将了解LangChain的构建块如何映射到LLM概念，以及当它们有效结合时，如何帮助你构建LLM应用。但首先，侧边栏[“为什么使用LangChain？”](#ch01_why_langchain_1736545659776355)是一个关于为什么我们认为使用LangChain构建LLM应用有用的简要介绍。
- en: Getting Set Up with LangChain
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LangChain进行设置
- en: To follow along with the rest of the chapter, and the chapters to come, we recommend
    setting up LangChain on your computer first.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本章的其余部分以及后续章节，我们建议首先在你的计算机上设置LangChain。
- en: See the instructions in the [Preface](preface01.html#pr01_preface_1736545679069216)
    regarding setting up an OpenAI account and complete these if you haven’t yet.
    If you prefer using a different LLM provider, see [“Why LangChain?”](#ch01_why_langchain_1736545659776355)
    for alternatives.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[前言](preface01.html#pr01_preface_1736545679069216)中有关设置OpenAI账户的说明，并在尚未完成的情况下完成这些操作。如果你更喜欢使用不同的LLM提供商，请参阅[“为什么使用LangChain？”](#ch01_why_langchain_1736545659776355)以获取替代方案。
- en: Then head over to the [API Keys page](https://oreil.ly/BKrtV) on the OpenAI
    website (after logging in to your OpenAI account), create an API key, and save
    it—you’ll need it soon.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，前往OpenAI网站上的[API密钥页面](https://oreil.ly/BKrtV)（在登录OpenAI账户后），创建一个API密钥，并保存它——你很快就会需要它。
- en: Note
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this book, we’ll show code examples in both Python and JavaScript (JS). LangChain
    offers the same functionality in both languages, so just pick the one you’re most
    comfortable with and follow the respective code snippets throughout the book (the
    code examples for each language are equivalent).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将展示Python和JavaScript（JS）的代码示例。LangChain在这两种语言中都提供相同的功能，所以只需选择你最熟悉的一种，并在整本书中遵循相应的代码片段（每种语言的代码示例是等效的）。
- en: 'First, some setup instructions for readers using Python:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为使用Python的读者提供一些设置说明：
- en: Ensure that you have Python installed. See the [instructions for your operating
    system](https://oreil.ly/20K9l).
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你已经安装了Python。请参阅[针对你操作系统的说明](https://oreil.ly/20K9l)。
- en: Install Jupyter if you want to run the examples in a notebook environment. You
    can do this by running `pip install notebook` in your terminal.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想在笔记本环境中运行示例，请安装Jupyter。你可以在终端中运行`pip install notebook`来完成此操作。
- en: 'Install the LangChain library by running the following commands in your terminal:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在终端运行以下命令安装LangChain库：
- en: '[PRE0]'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Take the OpenAI API key you generated at the beginning of this section and
    make it available in your terminal environment. You can do this by running the
    following:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本节开头生成的OpenAI API密钥在终端环境中可用。你可以通过运行以下命令来完成此操作：
- en: '[PRE1]'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Don’t forget to replace `your-key` with the API key you generated previously.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要忘记将`your-key`替换为你之前生成的API密钥。
- en: 'Open a Jupyter notebook by running this command:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令打开Jupyter笔记本：
- en: '[PRE2]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You’re now ready to follow along with the Python code examples.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你准备好跟随Python代码示例了。
- en: 'Here are the instructions for readers using JavaScript:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是为使用JavaScript的读者提供的说明：
- en: 'Take the OpenAI API key you generated at the beginning of this section and
    make it available in your terminal environment. You can do this by running the
    following:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本节开头生成的OpenAI API密钥在终端环境中可用。你可以通过运行以下命令来完成此操作：
- en: '[PRE3]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Don’t forget to replace `your-key` with the API key you generated previously.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要忘记将`your-key`替换为您之前生成的API密钥。
- en: If you want to run the examples as Node.js scripts, install Node by following
    the [instructions](https://oreil.ly/5gjiO).
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想将示例作为Node.js脚本运行，请按照[说明](https://oreil.ly/5gjiO)安装Node。
- en: 'Install the LangChain libraries by running the following commands in your terminal:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在您的终端中运行以下命令安装LangChain库：
- en: '[PRE4]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Take each example, save it as a *.js* file and run it with `node ./file.js`.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个示例保存为*.js*文件，并使用`node ./file.js`运行它。
- en: Using LLMs in LangChain
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在LangChain中使用LLM
- en: 'To recap, LLMs are the driving engine behind most generative AI applications.
    LangChain provides two simple interfaces to interact with any LLM API provider:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，LLM是大多数生成式AI应用的驱动引擎。LangChain提供了两个简单的接口来与任何LLM API提供商交互：
- en: Chat models
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天模型
- en: LLMs
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs
- en: The LLM interface simply takes a string prompt as input, sends the input to
    the model provider, and then returns the model prediction as output.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLM接口简单地接受一个字符串提示作为输入，将输入发送给模型提供商，然后返回模型预测作为输出。
- en: 'Let’s import LangChain’s OpenAI LLM wrapper to `invoke` a model prediction
    using a simple prompt:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入LangChain的OpenAI LLM包装器，使用简单的提示来`invoke`模型预测：
- en: '*Python*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*JavaScript*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*The output:*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tip
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Notice the parameter `model` passed to `OpenAI`. This is the most common parameter
    to configure when using an LLM or chat model, the underlying model to use, as
    most providers offer several models with different trade-offs in capability and
    cost (usually larger models are more capable, but also more expensive and slower).
    See [OpenAI’s overview](https://oreil.ly/dM886) of the models they offer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意传递给`OpenAI`的参数`model`。这是使用LLM或聊天模型时配置的最常见参数，即要使用的底层模型，因为大多数提供商提供具有不同能力和成本折衷的多个模型（通常较大的模型功能更强，但成本更高，速度更慢）。请参阅[OpenAI提供的模型概述](https://oreil.ly/dM886)。
- en: Other useful parameters to configure include the following, offered by most
    providers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其他有用的配置参数包括以下内容，大多数提供商都提供。
- en: '`temperature`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`temperature`'
- en: 'This controls the sampling algorithm used to generate output. Lower values
    produce more predictable outputs (for example, 0.1), while higher values generate
    more creative, or unexpected, results (such as 0.9). Different tasks will need
    different values for this parameter. For instance, producing structured output
    usually benefits from a lower temperature, whereas creative writing tasks do better
    with a higher value:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这控制了用于生成输出的采样算法。较低的值产生更可预测的输出（例如，0.1），而较高的值产生更具创造性或意外的结果（例如，0.9）。不同的任务需要不同的参数值。例如，生成结构化输出通常受益于较低的温度，而创意写作任务则更适合较高的值：
- en: '`max_tokens`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_tokens`'
- en: This limits the size (and cost) of the output. A lower value may cause the LLM
    to stop generating the output before getting to a natural end, so it may appear
    to have been truncated.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这限制了输出的尺寸（和成本）。较低的值可能会导致LLM在达到自然结束之前停止生成输出，因此可能看起来被截断了。
- en: Beyond these, each provider exposes a different set of parameters. We recommend
    looking at the documentation for the one you choose. For an example, refer to
    [OpenAI’s platform](https://oreil.ly/5O1RW).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，每个提供商都公开了一组不同的参数。我们建议查看您选择的文档。例如，请参阅[OpenAI的平台](https://oreil.ly/5O1RW)。
- en: 'Alternatively, the chat model interface enables back and forth conversations
    between the user and model. The reason why it’s a separate interface is because
    popular LLM providers like OpenAI differentiate messages sent to and from the
    model into *user*, *assistant*, and *system* roles (here *role* denotes the type
    of content the message contains):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，聊天模型界面允许用户和模型之间进行双向对话。之所以是独立的界面，是因为流行的LLM提供商，如OpenAI，将发送给模型和从模型发送的消息区分成*用户*、*助手*和*系统*角色（在这里*角色*表示消息包含的内容类型）：
- en: System role
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 系统角色
- en: Used for instructions the model should use to answer a user question
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 用于模型应使用的指令来回答用户问题
- en: User role
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 用户角色
- en: Used for the user’s query and any other content produced by the user
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 用于用户的查询和用户产生的任何其他内容
- en: Assistant role
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 助手角色
- en: Used for content generated by the model
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 用于模型生成的内容
- en: 'The chat model’s interface makes it easier to configure and manage conversions
    in your AI chatbot application. Here’s an example utilizing LangChain’s ChatOpenAI
    model:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天模型的界面使得在您的AI聊天机器人应用程序中配置和管理转换变得更加容易。以下是一个使用LangChain的ChatOpenAI模型的示例：
- en: '*Python*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*JavaScript*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*The output:*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Instead of a single prompt string, chat models make use of different types
    of chat message interfaces associated with each role mentioned previously. These
    include the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与单个提示字符串不同，聊天模型利用与之前提到的每个角色关联的不同类型的聊天消息接口。以下是一些例子：
- en: '`HumanMessage`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`HumanMessage`'
- en: A message sent from the perspective of the human, with the user role
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类用户角色的角度发送的消息
- en: '`AIMessage`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`AIMessage`'
- en: A message sent from the perspective of the AI that the human is interacting
    with, with the assistant role
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类与交互的AI的角度发送的消息，具有助手角色
- en: '`SystemMessage`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`SystemMessage`'
- en: A message setting the instructions the AI should follow, with the system role
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 设置AI应遵循的指令的消息，具有系统角色
- en: '`ChatMessage`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChatMessage`'
- en: A message allowing for arbitrary setting of role
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 允许任意设置角色的消息
- en: 'Let’s incorporate a `SystemMessage` instruction in our example:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的例子中包含一个`SystemMessage`指令：
- en: '*Python*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*JavaScript*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*The output:*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, the model obeyed the instruction provided in the `SystemMessage`
    even though it wasn’t present in the user’s question. This enables you to preconfigure
    your AI application to respond in a relatively predictable manner based on the
    user’s input.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，模型遵循了`SystemMessage`中提供的指令，即使它没有出现在用户的问题中。这使你可以根据用户的输入预先配置你的AI应用程序以相对可预测的方式响应。
- en: Making LLM Prompts Reusable
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使LLM提示可重用
- en: The previous section showed how the `prompt` instruction significantly influences
    the model’s output. Prompts help the model understand context and generate relevant
    answers to queries.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节展示了`prompt`指令如何显著影响模型的输出。提示有助于模型理解上下文并生成针对查询的相关答案。
- en: 'Here is an example of a detailed prompt:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个详细提示的例子：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Although the prompt looks like a simple string, the challenge is figuring out
    what the text should contain and how it should vary based on the user’s input.
    In this example, the Context and Question values are hardcoded, but what if we
    wanted to pass these in dynamically?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提示看起来像是一个简单的字符串，但挑战在于确定文本应该包含什么，以及它应该如何根据用户的输入而变化。在这个例子中，上下文和问题值是硬编码的，但如果我们想动态地传递这些值怎么办？
- en: 'Fortunately, LangChain provides prompt template interfaces that make it easy
    to construct prompts with dynamic inputs:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，LangChain提供了提示模板接口，这使得构建具有动态输入的提示变得容易：
- en: '*Python*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*JavaS**cript*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaS**cript*'
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*The output:*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This example takes the static prompt from the previous block and makes it dynamic.
    The `template` contains the structure of the final prompt alongside the definition
    of where the dynamic inputs will be inserted.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子将之前块中的静态提示变为动态。`template`包含最终提示的结构以及动态输入插入的定义。
- en: As such, the template can be used as a recipe to build multiple static, specific
    prompts. When you format the prompt with some specific values—in this case, `context`
    and `question`—you get a static prompt ready to be passed in to an LLM.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该模板可以用作构建多个静态、特定提示的配方。当你使用一些特定值格式化提示时——在这个例子中，`context`和`question`——你将得到一个准备好的静态提示，可以传递给一个LLM。
- en: As you can see, the `question` argument is passed dynamically via the `invoke`
    function. By default, LangChain prompts follow Python’s `f-string` syntax for
    defining dynamic parameters—any word surrounded by curly braces, such as `{question}`,
    are placeholders for values passed in at runtime. In the previous example, `{question}`
    was replaced by `“Which model providers offer LLMs?”`
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，`question`参数通过`invoke`函数动态传递。默认情况下，LangChain提示遵循Python的`f-string`语法来定义动态参数——任何被大括号包围的单词，如`{question}`，都是运行时传入值的占位符。在前一个例子中，`{question}`被替换为`“Which
    model providers offer LLMs?”`
- en: 'Let’s see how we’d feed this into an LLM OpenAI model using LangChain:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用LangChain将这个例子输入到LLM OpenAI模型中：
- en: '*Python*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*JavaScript*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*The output:*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If you’re looking to build an AI chat application, the `ChatPromptTemplate`
    can be used instead to provide dynamic inputs based on the role of the chat message:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要构建一个AI聊天应用程序，可以使用`ChatPromptTemplate`来提供基于聊天消息角色的动态输入：
- en: '*Python*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*JavaScript*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*The output:*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Notice how the prompt contains instructions in a `SystemMessage` and two instances
    of `HumanMessage` that contain dynamic `context` and `question` variables. You
    can still format the template in the same way and get back a static prompt that
    you can pass to a large language model for a prediction output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意提示中包含的指令在 `SystemMessage` 中，以及包含动态 `context` 和 `question` 变量的两个 `HumanMessage`
    实例。您仍然可以以相同的方式格式化模板，并获取一个静态提示，可以将其传递给大型语言模型以获得预测输出：
- en: '*Python*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '*JavaScript*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*The output:*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Getting Specific Formats out of LLMs
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 LLM 获取特定格式
- en: Plain text outputs are useful, but there may be use cases where you need the
    LLM to generate a *structured* output—that is, output in a machine-readable format,
    such as JSON, XML, CSV, or even in a programming language such as Python or JavaScript.
    This is very useful when you intend to hand that output off to some other piece
    of code, making an LLM play a part in your larger application.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 纯文本输出很有用，但可能存在需要 LLM 生成结构化输出的用例——即以机器可读的格式输出，例如 JSON、XML、CSV，甚至以 Python 或 JavaScript
    等编程语言输出。当您打算将输出传递给其他代码，使 LLM 在您的更大应用中发挥作用时，这非常有用。
- en: JSON Output
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JSON 输出
- en: The most common format to generate with LLMs is JSON. JSON outputs can (for
    example) be sent over the wire to your frontend code or be saved to a database.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLM 生成最常见格式的是 JSON。JSON 输出可以（例如）通过网络发送到您的前端代码或保存到数据库中。
- en: 'When generating JSON, the first task is to define the schema you want the LLM
    to respect when producing the output. Then, you should include that schema in
    the prompt, along with the text you want to use as the source. Let’s see an example:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当生成 JSON 时，首先要定义 LLM 在生成输出时需要遵守的模式。然后，您应将该模式包含在提示中，以及您想要用作源文本的文本。让我们看一个例子：
- en: '*Python*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE27]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '*JavaScript*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE28]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*The output:*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE29]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'So, first define a schema. In Python, this is easiest to do with Pydantic (a
    library used for validating data against schemas). In JS, this is easiest to do
    with Zod (an equivalent library). The method `with_structured_output` will use
    that schema for two things:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先定义一个模式。在 Python 中，这可以通过 Pydantic（一个用于验证数据与模式匹配的库）最简单地完成。在 JS 中，这可以通过 Zod（一个等效的库）最简单地完成。`with_structured_output`
    方法将使用该模式进行两件事：
- en: The schema will be converted to a `JSONSchema` object (a JSON format used to
    describe the shape [types, names, descriptions] of JSON data), which will be sent
    to the LLM. For each LLM, LangChain picks the best method to do this, usually
    function calling or prompting.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模式将被转换为 `JSONSchema` 对象（一种用于描述 JSON 数据形状 [类型、名称、描述] 的 JSON 格式），并将其发送到 LLM。对于每个
    LLM，LangChain 会选择最佳方法来完成此操作，通常是函数调用或提示。
- en: The schema will also be used to validate the output returned by the LLM before
    returning it; this ensures the output produced respects the schema you passed
    in exactly.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模式还将用于在返回之前验证 LLM 返回的输出；这确保了生成的输出完全符合您传入的模式。
- en: Other Machine-Readable Formats with Output Parsers
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他机器可读格式与输出解析器
- en: 'You can also use an LLM or chat model to produce output in other formats, such
    as CSV or XML. This is where output parsers come in handy. *Output parsers* are
    classes that help you structure large language model responses. They serve two
    functions:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 LLM 或聊天模型以其他格式生成输出，例如 CSV 或 XML。这就是输出解析器派上用场的地方。*输出解析器* 是帮助您结构化大型语言模型响应的类。它们有两个功能：
- en: Providing format instructions
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 提供格式指令
- en: Output parsers can be used to inject some additional instructions in the prompt
    that will help guide the LLM to output text in the format it knows how to parse.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用输出解析器在提示中注入一些额外的指令，这将有助于引导 LLM 以其能解析的格式输出文本。
- en: Validating and parsing output
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 验证和解析输出
- en: The main function is to take the textual output of the LLM or chat model and
    render it to a more structured format, such as a list, XML, or other format. This
    can include removing extraneous information, correcting incomplete output, and
    validating the parsed values.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 主要功能是将 LLM 或聊天模型的文本输出渲染为更结构化的格式，例如列表、XML 或其他格式。这可能包括删除无关信息、纠正不完整的输出以及验证解析的值。
- en: 'Here’s an example of how an output parser works:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个输出解析器的工作示例：
- en: '*Python*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE30]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '*JavaScript*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE31]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '*The output:*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE32]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: LangChain provides a variety of output parsers for various use cases, including
    CSV, XML, and more. We’ll see how to combine output parsers with models and prompts
    in the next section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 为各种用例提供了各种输出解析器，包括 CSV、XML 等。我们将在下一节中看到如何将输出解析器与模型和提示结合使用。
- en: Assembling the Many Pieces of an LLM Application
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组装 LLM 应用程序的多个部分
- en: 'The key components you’ve learned about so far are essential building blocks
    of the LangChain framework. Which brings us to the critical question: How do you
    combine them effectively to build your LLM application?'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你迄今为止学到的关键组件是 LangChain 框架的基本构建块。这引出了关键问题：你如何有效地将它们组合起来以构建你的 LLM 应用程序？
- en: Using the Runnable Interface
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用可运行接口
- en: 'As you may have noticed, all the code examples used so far utilize a similar
    interface and the `invoke()` method to generate outputs from the model (or prompt
    template, or output parser). All components have the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所注意到的，迄今为止使用的所有代码示例都使用了类似的接口和 `invoke()` 方法来从模型（或提示模板，或输出解析器）生成输出。所有组件都具有以下功能：
- en: 'There is a common interface with these methods:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法有一个共同的接口：
- en: '`invoke`: transforms a single input into an output'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`invoke`：将单个输入转换为输出'
- en: '`batch`: efficiently transforms multiple inputs into multiple outputs'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch`：高效地将多个输入转换为多个输出'
- en: '`stream`: streams output from a single input as it’s produced'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stream`：以流的形式从单个输入中输出，当输出产生时'
- en: There are built-in utilities for retries, fallbacks, schemas, and runtime configurability.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有内置的重试、回退、模式以及运行时配置的实用工具。
- en: In Python, each of the three methods have `asyncio` equivalents.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中，这三个方法都有 `asyncio` 的等效方法。
- en: 'As such, all components behave the same way, and the interface learned for
    one of them applies to all:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所有组件的行为方式相同，为其中一个组件学习到的接口适用于所有：
- en: '*Python*'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE33]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '*JavaScript*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE34]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In this example, you see how the three main methods work:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你可以看到三种主要方法是如何工作的：
- en: '`invoke()` takes a single input and returns a single output.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`invoke()` 接受单个输入并返回单个输出。'
- en: '`batch()` takes a list of outputs and returns a list of outputs.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch()` 接受一个输出列表并返回一个输出列表。'
- en: '`stream()` takes a single input and returns an iterator of parts of the output
    as they become available.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stream()` 接受单个输入并返回输出部分的迭代器，当输出可用时。'
- en: In some cases, where the underlying component doesn’t support iterative output,
    there will be a single part containing all output.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，如果底层组件不支持迭代输出，将会有一个包含所有输出的单个部分。
- en: 'You can combine these components in two ways:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过两种方式组合这些组件：
- en: Imperative
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 命令式
- en: Call your components directly, for example, with `model.invoke(...)`
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 直接调用你的组件，例如，使用 `model.invoke(...)`。
- en: Declarative
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 声明式
- en: Use LangChain Expression Language (LCEL), as covered in an upcoming section
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LangChain 表达式语言（LCEL），如即将介绍的章节所述
- en: '[Table 1-1](#ch01_table_1_1736545659767905) summarizes their differences, and
    we’ll see each in action next.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 1-1](#ch01_table_1_1736545659767905) 总结了它们之间的区别，我们将在接下来的操作中看到每个示例。'
- en: Table 1-1\. The main differences between imperative and declarative composition.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1-1\. 命令式和声明式组合的主要区别。
- en: '|   | Imperative | Declarative |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|   | 命令式 | 声明式 |'
- en: '| --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Syntax | All of Python or JavaScript | LCEL |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 语法 | Python 或 JavaScript 的全部 | LCEL |'
- en: '| Parallel execution | Python: with threads or coroutinesJavaScript: with `Promise.all`
    | Automatic |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 并行执行 | Python：使用线程或协程JavaScript：使用 `Promise.all` | 自动 |'
- en: '| Streaming | With yield keyword | Automatic |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 流式传输 | 使用 `yield` 关键字 | 自动 |'
- en: '| Async execution | With async functions | Automatic |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 异步执行 | 使用异步函数 | 自动 |'
- en: Imperative Composition
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命令式组合
- en: '*Imperative composition* is just a fancy name for writing the code you’re used
    to writing, composing these components into functions and classes. Here’s an example
    combining prompts, models, and output parsers:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*命令式组合* 只是一个花哨的名称，指的是编写你习惯的代码，将这些组件组合成函数和类。以下是一个结合提示、模型和输出解析器的示例：'
- en: '*Python*'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '*JavaScript*'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '*The output:*'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The preceding is a complete example of a chatbot, using a prompt and chat model.
    As you can see, it uses familiar Python syntax and supports any custom logic you
    might want to add in that function.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子是一个完整的聊天机器人示例，使用提示和聊天模型。正如你所看到的，它使用了熟悉的 Python 语法并支持你可能在那个函数中添加的任何自定义逻辑。
- en: 'On the other hand, if you want to enable streaming or async support, you’d
    have to modify your function to support it. For example, streaming support can
    be added as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你想启用流式传输或异步支持，你必须修改你的函数以支持它。例如，可以通过以下方式添加流式传输支持：
- en: '*Python*'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE38]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '*JavaScript*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '*The output:*'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE40]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: So, either in JS or Python, you can enable streaming for your custom function
    by yielding the values you want to stream and then calling it with `stream`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论是使用 JS 还是 Python，你都可以通过返回你想要流式的值并使用 `stream` 来调用它来为你的自定义函数启用流式传输。
- en: 'For asynchronous execution, you’d rewrite your function like this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于异步执行，你需要这样重写你的函数：
- en: '*Python*'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE41]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This one applies to Python only, as asynchronous execution is the only option
    in JavaScript.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这个只适用于Python，因为在JavaScript中异步执行是唯一的选择。
- en: Declarative Composition
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 声明式组合
- en: LCEL is a *declarative language* for composing LangChain components. LangChain
    compiles LCEL compositions to an *optimized execution plan*, with automatic parallelization,
    streaming, tracing, and async support.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: LCEL是一种用于组合LangChain组件的*声明式语言*。LangChain将LCEL组合编译成*优化后的执行计划*，具有自动并行化、流式处理、跟踪和异步支持。
- en: 'Let’s see the same example using LCEL:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用LCEL来查看相同的例子：
- en: '*Python*'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '*JavaScript*'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '*The output:*'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE44]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Crucially, the last line is the same between the two examples—that is, you
    use the function and the LCEL sequence in the same way, with `invoke/stream/batch`.
    And in this version, you don’t need to do anything else to use streaming:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，两个例子中的最后一行是相同的——也就是说，你使用函数和LCEL序列的方式相同，使用`invoke/stream/batch`。在这个版本中，你不需要做任何事情就可以使用流式处理：
- en: '*Python*'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE45]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '*JavaScript*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE46]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'And, for Python only, it’s the same for using asynchronous methods:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，对于Python来说，使用异步方法也是一样的：
- en: '*Python*'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE47]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Summary
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you’ve learned about the building blocks and key components
    necessary to build LLM applications using LangChain. LLM applications are essentially
    a chain consisting of the large language model to make predictions, the prompt
    instruction(s) to guide the model toward a desired output, and an optional output
    parser to transform the format of the model’s output.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了构建LLM应用程序所需的构建块和关键组件，使用LangChain构建LLM应用程序。LLM应用程序本质上是一个由大型语言模型（用于做出预测）、提示指令（用于引导模型达到期望的输出）以及可选的输出解析器（用于转换模型输出的格式）组成的链。
- en: All LangChain components share the same interface with `invoke`, `stream`, and
    `batch` methods to handle various inputs and outputs. They can either be combined
    and executed imperatively by calling them directly or declaratively using LCEL.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 所有LangChain组件都共享相同的接口，使用`invoke`、`stream`和`batch`方法来处理各种输入和输出。它们可以通过直接调用或使用LCEL声明式地组合和执行。
- en: The imperative approach is useful if you intend to write a lot of custom logic,
    whereas the declarative approach is useful for simply assembling existing components
    with limited customization.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想编写大量的自定义逻辑，命令式方法是有用的，而声明式方法则适用于简单地组装现有组件，且定制化有限。
- en: In [Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927),
    you’ll learn how to provide external data to your AI chatbot as *context* so that
    you can build an LLM application that enables you to “chat” with your data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)中，你将学习如何提供外部数据作为*上下文*给你的AI聊天机器人，这样你就可以构建一个LLM应用程序，让你能够“聊天”你的数据。
