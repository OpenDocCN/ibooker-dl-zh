- en: Chapter 3\. Evaluation Methodology
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章\. 评估方法学
- en: The more AI is used, the more opportunity there is for catastrophic failure.
    We’ve already seen many failures in the short time that foundation models have
    been around. A man committed suicide after being [encouraged by a chatbot](https://oreil.ly/tMH21).
    Lawyers submitted [false evidence hallucinated by AI](https://oreil.ly/-0Iq1).
    Air Canada was ordered to pay damages when its AI chatbot [gave a passenger false
    information](https://oreil.ly/kKWnZ). Without a way to quality control AI outputs,
    the risk of AI might outweigh its benefits for many applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的使用越多，发生灾难性失败的机会就越大。我们已经看到，在基础模型存在的不长的时间里，已经出现了许多失败。一个人在聊天机器人的[鼓励](https://oreil.ly/tMH21)下自杀。律师提交了由人工智能[生成的虚假证据](https://oreil.ly/-0Iq1)。加拿大航空被命令支付赔偿，因为其人工智能聊天机器人[向乘客提供了错误信息](https://oreil.ly/kKWnZ)。如果没有一种方法来质量控制人工智能输出，人工智能的风险可能超过其在许多应用中的益处。
- en: As teams rush to adopt AI, many quickly realize that the biggest hurdle to bringing
    AI applications to reality is evaluation. For some applications, figuring out
    evaluation can take up the majority of the development effort.^([1](ch03.html#id871))
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着团队纷纷采纳人工智能，许多人很快意识到将人工智能应用变为现实的最大障碍是评估。对于某些应用，确定评估方法可能需要占据大部分开发工作。[1](ch03.html#id871)
- en: Due to the importance and complexity of evaluation, this book has two chapters
    on it. This chapter covers different evaluation methods used to evaluate open-ended
    models, how these methods work, and their limitations. The next chapter focuses
    on how to use these methods to select models for your application and build an
    evaluation pipeline to evaluate your application.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于评估的重要性与复杂性，本书对此进行了两章的讨论。本章涵盖了用于评估开放性模型的多种评估方法，这些方法是如何工作的，以及它们的局限性。下一章将重点介绍如何使用这些方法为您的应用选择模型，并构建评估管道以评估您的应用。
- en: While I discuss evaluation in its own chapters, evaluation has to be considered
    in the context of a whole system, not in isolation. Evaluation aims to mitigate
    risks and uncover opportunities. To mitigate risks, you first need to identify
    the places where your system is likely to fail and design your evaluation around
    them. Often, this may require redesigning your system to enhance visibility into
    its failures. Without a clear understanding of where your system fails, no amount
    of evaluation metrics or tools can make the system robust.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我在单独的章节中讨论了评估，但评估必须在整个系统的背景下考虑，而不是孤立地考虑。评估的目的是减轻风险并揭示机会。为了减轻风险，您首先需要确定系统可能失败的地方，并围绕这些地方设计评估。通常，这可能需要重新设计系统以增强对其失败的可见性。如果没有对系统失败地点的明确理解，那么无论多少评估指标或工具都无法使系统变得稳健。
- en: Before diving into evaluation methods, it’s important to acknowledge the challenges
    of evaluating foundation models. Because evaluation is difficult, many people
    settle for *word of mouth*^(^([2](ch03.html#id872))) (e.g., someone says that
    the model X is good) or eyeballing the results.^([3](ch03.html#id873)) This creates
    even more risk and slows application iteration. Instead, we need to invest in
    systematic evaluation to make the results more reliable.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究评估方法之前，重要的是要承认评估基础模型的挑战。由于评估困难，许多人满足于“口碑”^(^([2](ch03.html#id872)))（例如，有人说模型X很好）或仅凭肉眼查看结果.^([3](ch03.html#id873))
    这会带来更多的风险并减缓应用迭代。相反，我们需要投资于系统性的评估，以使结果更加可靠。
- en: Since many foundation models have a language model component, this chapter will
    provide a quick overview of the metrics used to evaluate language models, including
    cross entropy and perplexity. These metrics are essential for guiding the training
    and finetuning of language models and are frequently used in many evaluation methods.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多基础模型具有语言模型组件，本章将简要概述用于评估语言模型的指标，包括交叉熵和困惑度。这些指标对于指导语言模型的训练和微调至关重要，并且在许多评估方法中经常被使用。
- en: Evaluating foundation models is especially challenging because they are open-ended,
    and I’ll cover best practices for how to tackle these. Using human evaluators
    remains a necessary option for many applications. However, given how slow and
    expensive human annotations can be, the goal is to automate the process. This
    book focuses on automatic evaluation, which includes both exact and subjective
    evaluation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 评估基础模型尤其具有挑战性，因为它们是开放性的，我将介绍如何应对这些挑战的最佳实践。对于许多应用来说，使用人工评估员仍然是一个必要的选项。然而，鉴于人工标注既缓慢又昂贵，目标是自动化这一过程。本书专注于自动评估，这包括精确评估和主观评估。
- en: The rising star of subjective evaluation is AI as a judge—the approach of using
    AI to evaluate AI responses. It’s subjective because the score depends on what
    model and prompt the AI judge uses. While this approach is gaining rapid traction
    in the industry, it also invites intense opposition from those who believe that
    AI isn’t trustworthy enough for this important task. I’m especially excited to
    go deeper into this discussion, and I hope you will be, too.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 主观评估领域的新星是作为裁判的AI——即使用AI来评估AI响应的方法。这是主观的，因为评分取决于AI裁判使用的模型和提示。虽然这种方法在业界迅速获得认可，但也引起了那些认为AI不足以胜任这项重要任务的人的强烈反对。我特别期待深入探讨这个话题，也希望你们会一样。
- en: Challenges of Evaluating Foundation Models
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础模型评估的挑战
- en: Evaluating ML models has always been difficult. With the introduction of foundation
    models, evaluation has become even more so. There are multiple reasons why evaluating
    foundation models is more challenging than evaluating traditional ML models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 评估机器学习模型一直都很困难。随着基础模型的引入，评估变得更加困难。评估基础模型比评估传统机器学习模型更具挑战性的原因有很多。
- en: 'First, the more intelligent AI models become, the harder it is to evaluate
    them. Most people can tell if a first grader’s math solution is wrong. Few can
    do the same for a PhD-level math solution.^([4](ch03.html#id874)) It’s easy to
    tell if a book summary is bad if it’s gibberish, but a lot harder if the summary
    is coherent. To validate the quality of a summary, you might need to read the
    book first. This brings us to a corollary: evaluation can be so much more time-consuming
    for sophisticated tasks. You can no longer evaluate a response based on how it
    sounds. You’ll also need to fact-check, reason, and even incorporate domain expertise.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，随着AI模型变得越来越智能，评估它们变得越来越困难。大多数人可以判断一年级学生的数学解答是否正确。很少有人能对博士水平的数学解答做出同样的判断。[4](ch03.html#id874)
    如果一本书的摘要满是胡言乱语，那么判断其摘要是否糟糕很容易，但如果摘要条理清晰，那就困难得多。为了验证摘要的质量，你可能需要先阅读这本书。这引出一个推论：对于复杂任务，评估可能需要更多的时间。你不能再仅仅根据响应听起来如何来评估它。你还需要进行事实核查、推理，甚至结合领域专业知识。
- en: Second, the open-ended nature of foundation models undermines the traditional
    approach of evaluating a model against ground truths. With traditional ML, most
    tasks are close-ended. For example, a classification model can only output among
    the expected categories. To evaluate a classification model, you can evaluate
    its outputs against the expected outputs. If the expected output is category X
    but the model’s output is category Y, the model is wrong. However, for an open-ended
    task, for a given input, there are so many possible correct responses. It’s impossible
    to curate a comprehensive list of correct outputs to compare against.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，基础模型的开放性本质削弱了传统的方法，即通过对比真实情况来评估模型。在传统的机器学习中，大多数任务都是封闭的。例如，一个分类模型只能输出预期的类别之一。要评估一个分类模型，你可以将其输出与预期的输出进行比较。如果预期的输出是类别X，但模型的输出是类别Y，那么模型就是错误的。然而，对于开放性任务，对于给定的输入，有如此多的可能正确响应。不可能编制一个全面的正确输出列表来与之比较。
- en: Third, most foundation models are treated as black boxes, either because model
    providers choose not to expose models’ details, or because application developers
    lack the expertise to understand them. Details such as the model architecture,
    training data, and the training process can reveal a lot about a model’s strengths
    and weaknesses. Without those details, you can evaluate only a model by observing
    its outputs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，大多数基础模型被视为黑盒，要么是因为模型提供者选择不公开模型细节，要么是因为应用开发者缺乏理解它们的专长。模型架构、训练数据和训练过程等细节可以揭示模型的优势和劣势。没有这些细节，你只能通过观察其输出来评估模型。
- en: At the same time, publicly available evaluation benchmarks have proven to be
    inadequate for evaluating foundation models. Ideally, evaluation benchmarks should
    capture the full range of model capabilities. As AI progresses, benchmarks need
    to evolve to catch up. A benchmark becomes saturated for a model once the model
    achieves the perfect score. With foundation models, benchmarks are becoming saturated
    fast. The benchmark [GLUE](https://arxiv.org/abs/1804.07461) (General Language
    Understanding Evaluation) came out in 2018 and became saturated in just a year,
    necessitating the introduction of [SuperGLUE](https://arxiv.org/abs/1905.00537)
    in 2019\. Similarly, [NaturalInstructions](https://arxiv.org/abs/2104.08773) (2021)
    was replaced by [Super-NaturalInstructions](https://arxiv.org/abs/2204.07705)
    (2022). [MMLU](https://arxiv.org/abs/2009.03300) (2020), a strong benchmark that
    many early foundation models relied on, was largely replaced by [MMLU-Pro](https://arxiv.org/abs/2406.01574)
    (2024).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，公开可用的评估基准已被证明不足以评估基础模型。理想情况下，评估基准应捕捉到模型能力的全部范围。随着AI的发展，基准需要进化以跟上。一旦模型达到完美分数，基准就会对模型饱和。对于基础模型，基准正迅速饱和。基准[GLUE](https://arxiv.org/abs/1804.07461)（通用语言理解评估）于2018年发布，并在一年内饱和，这促使2019年引入了[SuperGLUE](https://arxiv.org/abs/1905.00537)。同样，[NaturalInstructions](https://arxiv.org/abs/2104.08773)（2021）被[Super-NaturalInstructions](https://arxiv.org/abs/2204.07705)（2022）所取代。[MMLU](https://arxiv.org/abs/2009.03300)（2020），一个许多早期基础模型依赖的强大基准，在很大程度上被[MMLU-Pro](https://arxiv.org/abs/2406.01574)（2024）所取代。
- en: Last but not least, the scope of evaluation has expanded for general-purpose
    models. With task-specific models, evaluation involves measuring a model’s performance
    on its trained task. However, with general-purpose models, evaluation is not only
    about assessing a model’s performance on known tasks but also about discovering
    new tasks that the model can do, and these might include tasks that extend beyond
    human capabilities. Evaluation takes on the added responsibility of exploring
    the potential and limitations of AI.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，评估的范围已经扩展到了通用模型。对于特定任务的模型，评估涉及测量模型在其训练任务上的性能。然而，对于通用模型，评估不仅关于评估模型在已知任务上的性能，还关于发现模型可以执行的新任务，这些任务可能包括超出人类能力范围的任务。评估承担了探索AI潜力和局限性的额外责任。
- en: The good news is that the new challenges of evaluation have prompted many new
    methods and benchmarks. [Figure 3-1](#ch03a_figure_1_1730150757025034) shows that
    the number of published papers on LLM evaluation grew exponentially every month
    in the first half of 2023, from 2 papers a month to almost 35 papers a month.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，评估的新挑战促使许多新的方法和基准出现。[图3-1](#ch03a_figure_1_1730150757025034)显示，2023年上半年，关于LLM评估的已发表论文数量每月呈指数增长，从每月2篇增加到几乎每月35篇。
- en: '![A graph with a line going up'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![一条向上的线'
- en: Description automatically generated](assets/aien_0301.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0301.png)
- en: Figure 3-1\. The trend of LLMs evaluation papers over time. Image from [Chang
    et al. (2023)](https://arxiv.org/abs/2307.03109).
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\. 随时间推移的LLM评估论文趋势。图片来自[Chang等人（2023）](https://arxiv.org/abs/2307.03109)。
- en: In my own analysis of the [top 1,000 AI-related repositories on GitHub](https://huyenchip.com/llama-police),
    as ranked by the number of stars, I found over 50 repositories dedicated to evaluation
    (as of May 2024).^([5](ch03.html#id875)) When plotting the number of evaluation
    repositories by their creation date, the growth curve looks exponential, as shown
    in [Figure 3-2](#ch03a_figure_2_1730150757025049).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我对GitHub上按星级排名的[前1000个AI相关仓库](https://huyenchip.com/llama-police)的分析中，我发现超过50个仓库专注于评估（截至2024年5月）。当按创建日期绘制评估仓库数量时，增长曲线看起来是指数级的，如图[图3-2](#ch03a_figure_2_1730150757025049)所示.^([5](ch03.html#id875))
- en: The bad news is that despite the increased interest in evaluation, it lags behind
    in terms of interest in the rest of the AI engineering pipeline. [Balduzzi et
    al. from DeepMind](https://arxiv.org/abs/1806.02643) noted in their paper that
    “developing evaluations has received little systematic attention compared to developing
    algorithms.” According to the paper, experiment results are almost exclusively
    used to improve algorithms and are rarely used to improve evaluation. Recognizing
    the lack of investments in evaluation, [Anthropic](https://oreil.ly/gPbjS) called
    on policymakers to increase government funding and grants both for developing
    new evaluation methodologies and analyzing the robustness of existing evaluations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 坏消息是，尽管对评估的兴趣增加，但在整个人工智能工程流程中，对评估的兴趣仍然落后。[DeepMind的Balduzzi等人](https://arxiv.org/abs/1806.02643)在他们的论文中指出，“与开发算法相比，开发评估得到的系统关注很少。”根据论文，实验结果几乎完全用于改进算法，而很少用于改进评估。认识到对评估的投资不足，[Anthropic](https://oreil.ly/gPbjS)呼吁政策制定者增加政府资金和拨款，用于开发新的评估方法和分析现有评估的鲁棒性。
- en: '![A graph of a graph showing the growth of a number of people'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个显示人数增长的图表](assets/aien_0303.png)'
- en: Description automatically generated with medium confidence](assets/aien_0302.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成，置信度中等](assets/aien_0302.png)
- en: Figure 3-2\. Number of open source evaluation repositories among the 1,000 most
    popular AI repositories on GitHub.
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. 在GitHub上1,000个最受欢迎的AI代码库中开源评估代码库的数量。
- en: To further demonstrate how the investment in evaluation lags behind other areas
    in the AI space, the number of tools for evaluation is small compared to the number
    of tools for modeling and training and AI orchestration, as shown in [Figure 3-3](#ch03a_figure_3_1730150757025061).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明在人工智能领域，对评估的投资落后于其他领域，评估工具的数量与建模、训练和人工智能编排工具的数量相比很少，如图3-3所示。
- en: Inadequate investment leads to inadequate infrastructure, making it hard for
    people to carry out systematic evaluations. When asked how they are evaluating
    their AI applications, many people told me that they just eyeballed the results.
    Many have a small set of go-to prompts that they use to evaluate models. The process
    of curating these prompts is ad hoc, usually based on the curator’s personal experience
    instead of based on the application’s needs. You might be able to get away with
    this ad hoc approach when getting a project off the ground, but it won’t be sufficient
    for application iteration. This book focuses on a systematic approach to evaluation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 投资不足导致基础设施不足，使得人们难以进行系统性的评估。当被问及他们如何评估他们的AI应用时，许多人告诉我他们只是凭直觉看了结果。许多人有一小套固定的提示词，他们用这些提示词来评估模型。这些提示词的编纂过程是临时的，通常基于编纂者的个人经验，而不是基于应用的需求。在启动项目时，你可能可以通过这种临时方法侥幸成功，但这种方法不足以支持应用的迭代。本书侧重于评估的系统方法。
- en: '![A bar graph with text'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个带有文本的条形图](assets/aien_0303.png)'
- en: Description automatically generated](assets/aien_0303.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](assets/aien_0303.png)
- en: Figure 3-3\. According to data sourced from my list of the 1,000 most popular
    AI repositories on GitHub, evaluation lags behind other aspects of AI engineering
    in terms of open source tools.
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3\. 根据来源于我列出的GitHub上1,000个最受欢迎的AI代码库的数据，在开源工具方面，评估在人工智能工程的其他方面落后。
- en: Understanding Language Modeling Metrics
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解语言建模指标
- en: Foundation models evolved out of language models. Many foundation models still
    have language models as their main components. For these models, the performance
    of the language model component tends to be well correlated to the foundation
    model’s performance on downstream applications ([Liu et al., 2023](https://oreil.ly/vX-My)).
    Therefore, a rough understanding of language modeling metrics can be quite helpful
    in understanding downstream performance.^([6](ch03.html#id878))
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型是从语言模型演变而来的。许多基础模型仍然将语言模型作为其主要组成部分。对于这些模型，语言模型组件的性能往往与基础模型在下游应用中的性能高度相关([Liu等人，2023](https://oreil.ly/vX-My))。因此，对语言建模指标的大致了解对于理解下游性能非常有帮助.^([6](ch03.html#id878))
- en: As discussed in [Chapter 1](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319),
    language modeling has been around for decades, popularized by Claude Shannon in
    his 1951 paper “Prediction and Entropy of Printed English”. The metrics used to
    guide the development of language models haven’t changed much since then. Most
    autoregressive language models are trained using cross entropy or its relative,
    perplexity. When reading papers and model reports, you might also come across
    bits-per-character (BPC) and bits-per-byte (BPB); both are variations of cross
    entropy.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319)所述，语言模型已经存在了几十年，由克劳德·香农在1951年的论文“预测和印刷英语的熵”中普及。自那时起，用于指导语言模型发展的指标并没有太大变化。大多数自回归语言模型都是使用交叉熵或其相关指标困惑度进行训练的。当阅读论文和模型报告时，你也可能会遇到每字符比特数（BPC）和每字节比特数（BPB）；两者都是交叉熵的变体。
- en: All four metrics—cross entropy, perplexity, BPC, and BPB—are closely related.
    If you know the value of one, you can compute the other three, given the necessary
    information. While I refer to them as language modeling metrics, they can be used
    for any model that generates sequences of tokens, including non-text tokens.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所有四个指标——交叉熵、困惑度、BPC和BPB——都密切相关。如果你知道其中一个的值，在提供必要信息的情况下，你可以计算出其他三个。虽然我将它们称为语言模型指标，但它们可以用于任何生成标记序列的模型，包括非文本标记。
- en: Recall that a language model encodes statistical information (how likely a token
    is to appear in a given context) about languages. Statistically, given the context
    “I like drinking __”, the next word is more likely to be “tea” than “charcoal”.
    The more statistical information that a model can capture, the better it is at
    predicting the next token.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，语言模型编码了关于语言统计信息（一个标记在给定上下文中出现的可能性）。从统计上讲，给定上下文“我喜欢喝__”，下一个词更有可能是“茶”而不是“木炭”。一个模型能够捕捉到的统计信息越多，它在预测下一个标记方面的能力就越强。
- en: In ML lingo, a language model learns the distribution of its training data.
    The better this model learns, the better it is at predicting what comes next in
    the training data, and the lower its training cross entropy. As with any ML model,
    you care about its performance not just on the training data but also on your
    production data. In general, the closer your data is to a model’s training data,
    the better the model can perform on your data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习术语中，语言模型学习其训练数据的分布。这个模型学得越好，它在预测训练数据中的下一个内容方面的能力就越强，其训练交叉熵就越低。与任何机器学习模型一样，你不仅关心它在训练数据上的表现，还关心它在生产数据上的表现。一般来说，你的数据越接近模型的训练数据，模型在你的数据上的表现就越好。
- en: Compared to the rest of the book, this section is math-heavy. If you find it
    confusing, feel free to skip the math part and focus on the discussion of how
    to interpret these metrics. Even if you’re not training or finetuning language
    models, understanding these metrics can help with evaluating which models to use
    for your application. These metrics can occasionally be used for certain evaluation
    and data deduplication techniques, as discussed throughout this book.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书的其余部分相比，本节数学内容较多。如果你觉得这部分内容难以理解，可以自由地跳过数学部分，专注于对这些指标的解释讨论。即使你不在训练或微调语言模型，理解这些指标也有助于评估适用于你应用的最佳模型。这些指标有时可用于某些评估和数据去重技术，如本书中所述。
- en: Entropy
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熵
- en: '*Entropy* measures how much information, on average, a token carries. The higher
    the entropy, the more information each token carries, and the more bits are needed
    to represent a token.^([7](ch03.html#id881))'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*熵*衡量一个标记平均携带多少信息。熵越高，每个标记携带的信息越多，表示一个标记所需的位数也越多。[7](ch03.html#id881))'
- en: Let’s use a simple example to illustrate this. Imagine you want to create a
    language to describe positions within a square, as shown in [Figure 3-4](#ch03a_figure_4_1730150757025074).
    If your language has only two tokens, shown as (a) in [Figure 3-4](#ch03a_figure_4_1730150757025074),
    each token can tell you whether the position is upper or lower. Since there are
    only two tokens, one bit is sufficient to represent them. The entropy of this
    language is, therefore, 1.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个简单的例子来说明这一点。想象一下，你想要创建一种语言来描述正方形内的位置，如图[图3-4](#ch03a_figure_4_1730150757025074)所示。如果你的语言只有两个标记，如图[图3-4](#ch03a_figure_4_1730150757025074)中的(a)所示，每个标记都可以告诉你位置是上方还是下方。由于只有两个标记，一个比特就足以表示它们。因此，这种语言的熵为1。
- en: '![A couple of squares with numbers'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![几个带有数字的正方形'
- en: Description automatically generated](assets/aien_0304.png)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0304.png)
- en: Figure 3-4\. Two languages describe positions within a square. Compared to the
    language on the left (a), the tokens on the right (b) carry more information,
    but they need more bits to represent them.
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4\. 两种语言描述方形内的位置。与左边的语言（a）相比，右边的标记（b）携带更多信息，但需要更多的比特来表示它们。
- en: 'If your language has four tokens, shown as (b) in [Figure 3-4](#ch03a_figure_4_1730150757025074),
    each token can give you a more specific position: upper-left, upper-right, lower-left,
    or lower-right. However, since there are now four tokens, you need two bits to
    represent them. The entropy of this language is 2\. This language has higher entropy,
    since each token carries more information, but each token requires more bits to
    represent.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的语言有四个标记，如[图3-4](#ch03a_figure_4_1730150757025074)中的(b)所示，每个标记可以给你一个更具体的位置：左上角、右上角、左下角或右下角。然而，由于现在有四个标记，你需要两个比特来表示它们。这种语言的熵是2。这种语言的熵更高，因为每个标记携带更多信息，但每个标记需要更多的比特来表示。
- en: Intuitively, entropy measures how difficult it is to predict what comes next
    in a language. The lower a language’s entropy (the less information a token of
    a language carries), the more predictable that language. In our previous example,
    the language with only two tokens is easier to predict than the language with
    four (you have to predict among only two possible tokens compared to four). This
    is similar to how, if you can perfectly predict what I will say next, what I say
    carries no new information.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，熵衡量的是在一种语言中预测接下来会发生什么有多困难。一种语言的熵越低（一个语言标记携带的信息越少），这种语言的可预测性就越高。在我们的上一个例子中，只有两个标记的语言比有四个标记的语言更容易预测（你只需要在两个可能的标记中进行预测，而四个标记则需要更多）。这类似于如果你可以完美地预测我接下来会说什么，我说的话就不再包含任何新信息。
- en: Cross Entropy
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉熵
- en: When you train a language model on a dataset, your goal is to get the model
    to learn the distribution of this training data. In other words, your goal is
    to get the model to predict what comes next in the training data. A language model’s
    cross entropy on a dataset measures how difficult it is for the language model
    to predict what comes next in this dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在数据集上训练语言模型时，你的目标是让模型学习训练数据的分布。换句话说，你的目标是让模型预测训练数据中的下一个内容。语言模型在数据集上的交叉熵衡量语言模型预测数据集中下一个内容有多困难。
- en: 'A model’s cross entropy on the training data depends on two qualities:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在训练数据上的交叉熵取决于两个品质：
- en: The training data’s predictability, measured by the training data’s entropy
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练数据的可预测性，通过训练数据的熵来衡量
- en: How the distribution captured by the language model diverges from the true distribution
    of the training data
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语言模型捕捉到的分布与训练数据的真实分布之间的差异
- en: 'Entropy and cross entropy share the same mathematical notation, *H*. Let *P*
    be the true distribution of the training data, and *Q* be the distribution learned
    by the language model. Accordingly, the following is true:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 熵和交叉熵具有相同的数学符号，*H*。设 *P* 为训练数据的真实分布，*Q* 为语言模型学习的分布。因此，以下成立：
- en: The training data’s entropy is, therefore, *H*(*P*).
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，训练数据的熵是 *H*(*P*)。
- en: The divergence of *Q* with respect to *P* can be measured using the Kullback–Leibler
    (KL) divergence, which is mathematically represented as $upper D Subscript upper
    K upper L Baseline left-parenthesis upper P StartAbsoluteValue EndAbsoluteValue
    upper Q right-parenthesis$ .
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q* 相对于 *P* 的差异可以使用Kullback–Leibler（KL）散度来衡量，它在数学上表示为 $upper D Subscript upper
    K upper L Baseline left-parenthesis upper P StartAbsoluteValue EndAbsoluteValue
    upper Q right-parenthesis$ .'
- en: 'The model’s cross entropy with respect to the training data is therefore: $upper
    H left-parenthesis upper P comma upper Q right-parenthesis equals upper H left-parenthesis
    upper P right-parenthesis plus upper D Subscript upper K upper L Baseline left-parenthesis
    upper P StartAbsoluteValue EndAbsoluteValue upper Q right-parenthesis$ .'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型相对于训练数据的交叉熵是：$upper H left-parenthesis upper P comma upper Q right-parenthesis
    equals upper H left-parenthesis upper P right-parenthesis plus upper D Subscript
    upper K upper L Baseline left-parenthesis upper P StartAbsoluteValue EndAbsoluteValue
    upper Q right-parenthesis$ .
- en: Cross entropy isn’t symmetric. The cross entropy of *Q* with respect to *P*—*H*(*P*,
    *Q*)—is different from the cross entropy of *P* with respect to *Q*—*H*(*Q*, *P*).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵不是对称的。*Q* 相对于 *P* 的交叉熵—*H*(*P*, *Q*)—与 *P* 相对于 *Q* 的交叉熵—*H*(*Q*, *P*)—是不同的。
- en: A language model is trained to minimize its cross entropy with respect to the
    training data. If the language model learns perfectly from its training data,
    the model’s cross entropy will be exactly the same as the entropy of the training
    data. The KL divergence of Q with respect to P will then be 0\. You can think
    of a model’s cross entropy as its approximation of the entropy of its training
    data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型被训练以最小化其与训练数据的交叉熵。如果语言模型能够完美地从其训练数据中学习，那么模型的交叉熵将正好等于训练数据的熵。因此，Q相对于P的KL散度将为0。你可以将模型的交叉熵视为其对训练数据熵的近似。
- en: Bits-per-Character and Bits-per-Byte
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每字符比特数和每字节比特数
- en: One unit of entropy and cross entropy is bits. If the cross entropy of a language
    model is 6 bits, this language model needs 6 bits to represent each token.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 信息熵和交叉熵的单位是比特。如果一个语言模型的交叉熵是6比特，那么这个语言模型需要6比特来表示每个标记。
- en: Since different models have different tokenization methods—for example, one
    model uses words as tokens and another uses characters as tokens—the number of
    bits per token isn’t comparable across models. Some use the number of *bits-per-character*
    (BPC) instead. If the number of bits per token is 6 and on average, each token
    consists of 2 characters, the BPC is 6/2 = 3.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不同的模型有不同的分词方法——例如，一个模型使用单词作为标记，而另一个模型使用字符作为标记——因此不同模型之间的每标记比特数是不可比较的。有些人使用每字符比特数（BPC）来代替。如果每标记的比特数是6，并且平均每个标记由2个字符组成，那么BPC是6/2
    = 3。
- en: One complication with BPC arises from different character encoding schemes.
    For example, with ASCII, each character is encoded using 7 bits, but with UTF-8,
    a character can be encoded using anywhere between 8 and 32 bits. A more standardized
    metric would be *bits-per-byte* (BPB), the number of bits a language model needs
    to represent one byte of the original training data. If the BPC is 3 and each
    character is 7 bits, or ⅞ of a byte, then the BPB is 3 / (⅞) = 3.43.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 每字符比特数（BPC）的一个复杂之处在于不同的字符编码方案。例如，在ASCII中，每个字符使用7比特进行编码，但在UTF-8中，一个字符可以使用8到32比特之间的任意比特数进行编码。一个更标准化的指标是每字节比特数（BPB），即语言模型需要表示原始训练数据中的一个字节的比特数。如果BPC是3，每个字符是7比特，或者说是字节的7/8，那么BPB是3
    / (7/8) = 3.43。
- en: Cross entropy tells us how efficient a language model will be at compressing
    text. If the BPB of a language model is 3.43, meaning it can represent each original
    byte (8 bits) using 3.43 bits, this language model can compress the original training
    text to less than half the text’s original size.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵告诉我们语言模型在压缩文本方面的效率。如果一个语言模型的BPB是3.43，这意味着它可以使用3.43比特来表示每个原始字节（8比特），那么这个语言模型可以将原始训练文本压缩到原始文本大小的一半以下。
- en: Perplexity
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 困惑度
- en: '*Perplexity* is the exponential of entropy and cross entropy. Perplexity is
    often shortened to PPL. Given a dataset with the true distribution *P*, its perplexity
    is defined as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*困惑度*是熵和交叉熵的指数。困惑度通常简称为PPL。给定一个具有真实分布*P*的数据集，其困惑度定义为：'
- en: $upper P upper P upper L left-parenthesis upper P right-parenthesis equals 2
    Superscript upper H left-parenthesis upper P right-parenthesis$
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: $PPL(P) = 2^{H(P)}$
- en: 'The perplexity of a language model (with the learned distribution *Q*) on this
    dataset is defined as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集上，语言模型（具有学习到的分布*Q*）的困惑度定义为：
- en: $upper P upper P upper L left-parenthesis upper P comma upper Q right-parenthesis
    equals 2 Superscript upper H left-parenthesis upper P comma upper Q right-parenthesis$
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: $PPL(P, Q) = 2^{H(P, Q)}$
- en: If cross entropy measures how difficult it is for a model to predict the next
    token, perplexity measures the amount of uncertainty it has when predicting the
    next token. Higher uncertainty means there are more possible options for the next
    token.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果交叉熵衡量模型预测下一个标记的难度，那么困惑度衡量的是模型在预测下一个标记时的不确定性。更高的不确定性意味着下一个标记有更多的可能选项。
- en: Consider a language model trained to encode the 4 position tokens, as in [Figure 3-4](#ch03a_figure_4_1730150757025074)
    (b), perfectly. The cross entropy of this language model is 2 bits. If this language
    model tries to predict a position in the square, it has to choose among 2 ^(=
    4 possible options. Thus, this language model has a perplexity of 4.)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个训练有素的语言模型，它可以完美地编码4位置标记，如[图3-4](#ch03a_figure_4_1730150757025074)（b）所示。这个语言模型的交叉熵是2比特。如果这个语言模型试图预测方格中的位置，它必须在4个可能选项中选择。因此，这个语言模型的困惑度是4。
- en: So far, I’ve been using *bit* as the unit for entropy and cross entropy. Each
    bit can represent 2 unique values, hence the base of 2 in the preceding perplexity
    equation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我一直使用*bit*作为熵和交叉熵的单位。每个比特可以表示2个独特的值，因此前一个困惑度方程中的基数是2。
- en: 'Popular ML frameworks, including TensorFlow and PyTorch, use *nat* (natural
    log) as the unit for entropy and cross entropy. Nat uses the [base of *e*](https://en.wikipedia.org/wiki/E_(mathematical_constant)),
    the base of natural logarithm.^([8](ch03.html#id892)) If you use *nat* as the
    unit, perplexity is the exponential of *e*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的机器学习框架，包括TensorFlow和PyTorch，使用*nat*（自然对数）作为熵和交叉熵的单位。Nat使用自然对数的底数[*e*](https://en.wikipedia.org/wiki/E_(mathematical_constant))。如果你使用*nat*作为单位，困惑度是*e*的指数：
- en: $upper P upper P upper L left-parenthesis upper P comma upper Q right-parenthesis
    equals e Superscript upper H left-parenthesis upper P comma upper Q right-parenthesis$
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: $upper P upper P upper L left-parenthesis upper P comma upper Q right-parenthesis
    equals e Superscript upper H left-parenthesis upper P comma upper Q right-parenthesis$
- en: Due to the confusion around *bit* and *nat*, many people report perplexity,
    instead of cross entropy, when reporting their language models’ performance.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对*bit*和*nat*的混淆，许多人报告的是困惑度，而不是交叉熵，当报告他们的语言模型性能时。
- en: Perplexity Interpretation and Use Cases
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 困惑度的解释和应用场景
- en: As discussed, cross entropy, perplexity, BPC, and BPB are variations of language
    models’ predictive accuracy measurements. The more accurately a model can predict
    a text, the lower these metrics are. In this book, I’ll use perplexity as the
    default language modeling metric. Remember that the more uncertainty the model
    has in predicting what comes next in a given dataset, the higher the perplexity.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，交叉熵、困惑度、BPC和BPB是语言模型预测准确度测量的变体。模型能够更准确地预测文本，这些指标就越低。在这本书中，我将使用困惑度作为默认的语言模型度量。记住，模型在预测给定数据集中接下来发生的事情的不确定性越大，困惑度就越高。
- en: 'What’s considered a good value for perplexity depends on the data itself and
    how exactly perplexity is computed, such as how many previous tokens a model has
    access to. Here are some general rules:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 被认为好的困惑度值取决于数据本身以及困惑度是如何计算的，例如模型可以访问多少个之前的标记。以下是一些一般规则：
- en: More structured data gives lower expected perplexity
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 更结构化的数据给出更低的预期困惑度
- en: More structured data is more predictable. For example, HTML code is more predictable
    than everyday text. If you see an opening HTML tag like `<head>,` you can predict
    that there should be a closing tag, `</head>,` nearby. Therefore, the expected
    perplexity of a model on HTML code should be lower than the expected perplexity
    of a model on everyday text.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 更结构化的数据更具可预测性。例如，HTML代码比日常文本更具可预测性。如果你看到一个开头的HTML标签如`<head>`，你可以预测附近应该有一个闭合标签`</head>`。因此，模型在HTML代码上的预期困惑度应该低于在日常文本上的预期困惑度。
- en: The bigger the vocabulary, the higher the perplexity
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表越大，困惑度越高
- en: Intuitively, the more possible tokens there are, the harder it is for the model
    to predict the next token. For example, a model’s perplexity on a children’s book
    will likely be lower than the same model’s perplexity on *War and Peace*. For
    the same dataset, say in English, character-based perplexity (predicting the next
    character) will be lower than word-based perplexity (predicting the next word),
    because the number of possible characters is smaller than the number of possible
    words.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 直观来说，可能的标记越多，模型预测下一个标记就越困难。例如，一个模型在儿童读物上的困惑度可能低于在《战争与和平》上的困惑度。对于相同的数据集，比如英语，基于字符的困惑度（预测下一个字符）将低于基于单词的困惑度（预测下一个单词），因为可能的字符数量少于可能的单词数量。
- en: The longer the context length, the lower the perplexity
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文长度越长，困惑度越低
- en: The more context a model has, the less uncertainty it will have in predicting
    the next token. In 1951, Claude Shannon evaluated his model’s cross entropy by
    using it to predict the next token conditioned on up to 10 previous tokens. As
    of this writing, a model’s perplexity can typically be computed and conditioned
    on between 500 and 10,000 previous tokens, and possibly more, upperbounded by
    the model’s maximum context length.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 模型拥有的上下文越多，它在预测下一个标记时的不确定性就越小。在1951年，克劳德·香农通过使用它来预测最多10个之前的标记来评估他的模型的交叉熵。截至本文写作时，一个模型的困惑度通常可以在500到10,000个之前的标记之间计算和条件化，可能更多，上限是模型的最大上下文长度。
- en: For reference, it’s not uncommon to see perplexity values as low as 3 or even
    lower. If all tokens in a hypothetical language have an equal chance of happening,
    a perplexity of 3 means that this model has a 1 in 3 chance of predicting the
    next token correctly. Given that a model’s vocabulary is in the order of 10,000s
    and 100,000s, these odds are incredible.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了参考，看到困惑度值低至3甚至更低并不罕见。如果假设中的语言中所有标记发生的概率相等，那么困惑度为3意味着该模型有1/3的机会正确预测下一个标记。考虑到一个模型的词汇量在数千到数万之间，这些概率是惊人的。
- en: Other than guiding the training of language models, perplexity is useful in
    many parts of an AI engineering workflow. First, perplexity is a good proxy for
    a model’s capabilities. If a model’s bad at predicting the next token, its performance
    on downstream tasks will also likely be bad. OpenAI’s GPT-2 report shows that
    larger models, which are also more powerful models, consistently give lower perplexity
    on a range of datasets, as shown in [Table 3-1](#ch03a_table_1_1730150757038068).
    Sadly, following the trend of companies being increasingly more secretive about
    their models, many have stopped reporting their models’ perplexity.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除了指导语言模型的训练外，困惑度在人工智能工程工作流程的许多部分都很有用。首先，困惑度是衡量模型能力的一个很好的代理指标。如果一个模型在预测下一个标记方面表现不佳，那么它在下游任务上的表现也可能不佳。OpenAI的GPT-2报告显示，更大的模型，也就是更强大的模型，在一系列数据集上始终给出较低的困惑度，如[表3-1](#ch03a_table_1_1730150757038068)所示。遗憾的是，随着公司越来越不愿意透露他们的模型信息，许多公司已经停止报告他们模型的困惑度。
- en: 'Table 3-1\. Larger GPT-2 models consistently give lower perplexity on different
    datasets. Source: [OpenAI, 2018](https://oreil.ly/Loidb).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-1\. 较大的GPT-2模型在不同数据集上始终给出较低的困惑度。来源：[OpenAI, 2018](https://oreil.ly/Loidb)。
- en: '|  | LAMBADA (PPL) | LAMBADA (ACC) | CBT-CN (ACC) | CBT-NE (ACC) | WikiText2
    (PPL) | PTB (PPL) | enwiki8 (BPB) | text8 (BPC) | WikiText103 (PBL) | IBW (PPL)
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | LAMBADA (PPL) | LAMBADA (ACC) | CBT-CN (ACC) | CBT-NE (ACC) | WikiText2
    (PPL) | PTB (PPL) | enwiki8 (BPB) | text8 (BPC) | WikiText103 (PBL) | IBW (PPL)
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| SOTA | 99.8 | 59.23 | 85.7 | 82.3 | 39.14 | 46.54 | 0.99 | 1.08 | 18.3 |
    21.8 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| SOTA | 99.8 | 59.23 | 85.7 | 82.3 | 39.14 | 46.54 | 0.99 | 1.08 | 18.3 |
    21.8 |'
- en: '| 117M | 35.13 | 45.99 | 87.65 | 83.4 | 29.41 | 65.85 | 1.16 | 1.17 | 37.50
    | 75.20 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 117M | 35.13 | 45.99 | 87.65 | 83.4 | 29.41 | 65.85 | 1.16 | 1.17 | 37.50
    | 75.20 |'
- en: '| 345M | 15.60 | 55.48 | 92.35 | 87.1 | 22.76 | 47.33 | 1.01 | 1.06 | 26.37
    | 55.72 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 345M | 15.60 | 55.48 | 92.35 | 87.1 | 22.76 | 47.33 | 1.01 | 1.06 | 26.37
    | 55.72 |'
- en: '| 762M | 10.87 | 60.12 | 93.45 | 88.0 | 19.93 | 40.31 | 0.97 | 1.02 | 22.05
    | 44.575 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 762M | 10.87 | 60.12 | 93.45 | 88.0 | 19.93 | 40.31 | 0.97 | 1.02 | 22.05
    | 44.575 |'
- en: '| 1542M | 8.63 | 63.24 | 93.30 | 89.05 | 18.34 | 35.76 | 0.93 | 0.98 | 17.48
    | 42.16 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 1542M | 8.63 | 63.24 | 93.30 | 89.05 | 18.34 | 35.76 | 0.93 | 0.98 | 17.48
    | 42.16 |'
- en: Warning
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Perplexity might not be a great proxy to evaluate models that have been post-trained
    using techniques like SFT and RLHF.^([9](ch03.html#id895)) Post-training is about
    teaching models how to complete tasks. As a model gets better at completing tasks,
    it might get worse at predicting the next tokens. A language model’s perplexity
    typically increases after post-training. Some people say that post-training *collapses*
    entropy. Similarly, quantization—a technique that reduces a model’s numerical
    precision and, with it, its memory footprint—can also change a model’s perplexity
    in unexpected ways.^([10](ch03.html#id896))
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度可能不是评估使用SFT和RLHF等技术进行后训练的模型的一个很好的代理指标。后训练是关于教会模型如何完成任务。随着模型在完成任务方面变得更好，它可能在预测下一个标记方面变得更差。语言模型的困惑度通常在训练后会增加。有些人说后训练*崩溃*了熵。同样，量化——一种降低模型数值精度及其内存占用的一种技术——也可能以意想不到的方式改变模型的困惑度。[9](ch03.html#id895)
- en: 'Recall that the perplexity of a model with respect to a text measures how difficult
    it is for this model to predict this text. For a given model, perplexity is the
    lowest for texts that the model has seen and memorized during training. Therefore,
    perplexity can be used to detect whether a text was in a model’s training data.
    This is useful for detecting data contamination—if a model’s perplexity on a benchmark’s
    data is low, this benchmark was likely included in the model’s training data,
    making the model’s performance on this benchmark less trustworthy. This can also
    be used for deduplication of training data: e.g., add new data to the existing
    training dataset only if the perplexity of the new data is high.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，一个模型相对于文本的困惑度衡量了该模型预测该文本的难度。对于给定的模型，困惑度在模型在训练期间看到并记忆的文本中最低。因此，困惑度可以用来检测文本是否在模型的训练数据中。这对于检测数据污染很有用——如果模型在基准数据上的困惑度低，那么这个基准很可能包含在模型的训练数据中，使得模型在这个基准上的表现不太可信。这也可以用于训练数据的去重：例如，只有当新数据的困惑度高时，才将新数据添加到现有的训练数据集中。
- en: Perplexity is the highest for unpredictable texts, such as texts expressing
    unusual ideas (like “my dog teaches quantum physics in his free time”) or gibberish
    (like “home cat go eye”). Therefore, perplexity can be used to detect abnormal
    texts.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不可预测的文本，如表达不寻常想法的文本（例如，“我的狗在空闲时间教授量子物理学”）或胡言乱语（例如，“家猫去眼睛”），困惑度是最高的。因此，困惑度可以用来检测异常文本。
- en: Perplexity and its related metrics help us understand the performance of the
    underlying language model, which is a proxy for understanding the model’s performance
    on downstream tasks. The rest of the chapter discusses how to measure a model’s
    performance on downstream tasks directly.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度及其相关指标帮助我们了解底层语言模型的表现，这可以作为理解模型在下游任务上表现的一个代理。本章的其余部分将讨论如何直接测量模型在下游任务上的表现。
- en: Exact Evaluation
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确评估
- en: When evaluating models’ performance, it’s important to differentiate between
    exact and subjective evaluation. Exact evaluation produces judgment without ambiguity.
    For example, if the answer to a multiple-choice question is A and you pick B,
    your answer is wrong. There’s no ambiguity around that. On the other hand, essay
    grading is subjective. An essay’s score depends on who grades the essay. The same
    person, if asked twice some time apart, can give the same essay different scores.
    Essay grading can become more exact with clear grading guidelines. As you’ll see
    in the next section, AI as a judge is subjective. The evaluation result can change
    based on the judge model and the prompt.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型的表现时，区分精确评估和主观评估非常重要。精确评估产生的是没有歧义的判断。例如，如果多项选择题的答案是A，而你选择了B，你的答案就是错误的。在这方面没有歧义。另一方面，论文评分是主观的。论文的分数取决于评分者。同一个人，如果隔了一段时间再次评分，可能会给出不同的分数。有了明确的评分指南，论文评分可以变得更加精确。正如你将在下一节中看到的，作为评判者的AI是主观的。评估结果可能会根据评判模型和提示而改变。
- en: 'I’ll cover two evaluation approaches that produce exact scores: functional
    correctness and similarity measurements against reference data. Note that this
    section focuses on evaluating open-ended responses (arbitrary text generation)
    as opposed to close-ended responses (such as classification). This is not because
    foundation models aren’t being used for close-ended tasks. In fact, many foundation
    model systems have at least a classification component, typically for intent classification
    or scoring. This section focuses on open-ended evaluation because close-ended
    evaluation is already well understood.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我将介绍两种产生精确分数的评估方法：功能正确性和与参考数据的相似度测量。请注意，本节重点在于评估开放式回答（任意文本生成），而不是封闭式回答（如分类）。这并不是因为基础模型没有用于封闭式任务。事实上，许多基础模型系统至少包含一个分类组件，通常用于意图分类或评分。本节重点在于开放式评估，因为封闭式评估已经得到了很好的理解。
- en: Functional Correctness
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 功能正确性
- en: Functional correctness evaluation means evaluating a system based on whether
    it performs the intended functionality. For example, if you ask a model to create
    a website, does the generated website meet your requirements? If you ask a model
    to make a reservation at a certain restaurant, does the model succeed?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 功能正确性评估意味着根据系统是否执行了预期的功能来评估系统。例如，如果你要求一个模型创建一个网站，生成的网站是否符合你的要求？如果你要求一个模型在某家餐厅预订，模型是否成功？
- en: Functional correctness is the ultimate metric for evaluating the performance
    of any application, as it measures whether your application does what it’s intended
    to do. However, functional correctness isn’t always straightforward to measure,
    and its measurement can’t be easily automated.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 功能正确性是评估任何应用程序性能的终极指标，因为它衡量了你的应用程序是否做了它打算做的事情。然而，功能正确性并不总是容易衡量，其测量也不能轻易自动化。
- en: Code generation is an example of a task where functional correctness measurement
    can be automated. Functional correctness in coding is sometimes *execution accuracy*.
    Say you ask the model to write a Python function, `gcd(num1, num2)`, to find the
    greatest common denominator (gcd) of two numbers, num1 and num2\. The generated
    code can then be input into a Python interpreter to check whether the code is
    valid and if it is, whether it outputs the correct result of a given pair `(num1,
    num2)`. For example, given the pair `(num1=15, num2=20)`, if the function `gcd(15,
    20)` doesn’t return 5, the correct answer, you know that the function is wrong.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成是功能正确性测量可以自动化的任务示例。在编码中，功能正确性有时被称为*执行精度*。比如说，你要求模型编写一个Python函数`gcd(num1,
    num2)`，以找到两个数num1和num2的最大公约数（gcd）。然后可以将生成的代码输入到Python解释器中，以检查代码是否有效，如果是的话，是否输出给定一对`(num1,
    num2)`的正确结果。例如，给定一对`(num1=15, num2=20)`，如果函数`gcd(15, 20)`没有返回正确答案5，你知道该函数是错误的。
- en: Long before AI was used for writing code, automatically verifying code’s functional
    correctness was standard practice in software engineering. Code is typically validated
    with [unit tests](https://en.wikipedia.org/wiki/Unit_testing) where code is executed
    in different scenarios to ensure that it generates the expected outputs. Functional
    correctness evaluation is how coding platforms like LeetCode and HackerRank validate
    the submitted solutions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能被用于编写代码之前，自动验证代码的功能正确性是软件工程中的标准做法。代码通常通过[单元测试](https://en.wikipedia.org/wiki/Unit_testing)进行验证，其中代码在不同的场景下执行，以确保它生成预期的输出。功能正确性评估是LeetCode和HackerRank等编码平台验证提交的解决方案的方式。
- en: Popular benchmarks for evaluating AI’s code generation capabilities, such as
    [OpenAI’s HumanEval](https://oreil.ly/CjYs9) and [Google’s MBPP](https://github.com/google-research/google-research/tree/master/mbpp)
    (Mostly Basic Python Problems Dataset) use functional correctness as their metrics.
    Benchmarks for text-to-SQL (generating SQL queries from natural languages) like
    Spider ([Yu et al., 2018](https://oreil.ly/ijU20)), BIRD-SQL (Big Bench for Large-scale
    Database Grounded Text-to-SQL Evaluation) ([Li et al., 2023](https://oreil.ly/rrSS9)),
    and WikiSQL ([Zhong, et al., 2017](https://arxiv.org/abs/1709.00103)) also rely
    on functional correctness.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 评估人工智能代码生成能力的流行基准，例如[OpenAI的HumanEval](https://oreil.ly/CjYs9)和[Google的MBPP](https://github.com/google-research/google-research/tree/master/mbpp)（主要是基本Python问题数据集），将功能正确性作为其指标。像Spider
    ([Yu et al., 2018](https://oreil.ly/ijU20))、BIRD-SQL（大规模数据库基于文本的SQL基准）([Li et
    al., 2023](https://oreil.ly/rrSS9))和WikiSQL ([Zhong, et al., 2017](https://arxiv.org/abs/1709.00103))这样的文本到SQL（从自然语言生成SQL查询）基准也依赖于功能正确性。
- en: 'A benchmark problem comes with a set of test cases. Each test case consists
    of a scenario the code should run and the expected output for that scenario. Here’s
    an example of a problem and its test cases in HumanEval:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基准问题附带一组测试用例。每个测试用例包括代码应运行的场景和该场景的预期输出。以下是在HumanEval中一个问题和其测试用例的示例：
- en: '[PRE0]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When evaluating a model, for each problem a number of code samples, denoted
    as *k*, are generated. A model solves a problem if any of the *k* code samples
    it generated pass all of that problem’s test cases. The final score, called *pass@k*,
    is the fraction of the solved problems out of all problems. If there are 10 problems
    and a model solves 5 with *k* = 3, then that model’s pass@3 score is 50%. The
    more code samples a model generates, the more chance the model has at solving
    each problem, hence the greater the final score. This means that in expectation,
    pass@1 score should be lower than pass@3, which, in turn, should be lower than
    pass@10.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估一个模型时，对于每个问题，会生成一定数量的代码样本，记为*k*。如果一个模型生成的*k*个代码样本中任何一个都能通过该问题的所有测试用例，则该模型解决了该问题。最终得分，称为*pass@k*，是解决的所有问题中解决问题的比例。如果有10个问题，并且一个模型用*k*
    = 3解决了5个，那么该模型的pass@3得分是50%。一个模型生成的代码样本越多，该模型解决每个问题的机会就越大，因此最终得分就越高。这意味着在期望中，pass@1得分应该低于pass@3，而pass@3得分应该低于pass@10。
- en: Another category of tasks whose functional correctness can be automatically
    evaluated is game bots. If you create a bot to play *Tetris*, you can tell how
    good the bot is by the score it gets. Tasks with measurable objectives can typically
    be evaluated using functional correctness. For example, if you ask AI to schedule
    your workloads to optimize energy consumption, the AI’s performance can be measured
    by how much energy it saves.^([11](ch03.html#id906))
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类可以自动评估其功能正确性的任务是游戏机器人。如果你创建一个玩*俄罗斯方块*的机器人，你可以通过机器人获得的分数来判断它的好坏。具有可衡量目标的任务通常可以使用功能正确性来评估。例如，如果你要求AI安排你的工作负载以优化能耗，AI的性能可以通过它节省的能源量来衡量.^([11](ch03.html#id906))
- en: Similarity Measurements Against Reference Data
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与参考数据进行的相似度测量
- en: If the task you care about can’t be automatically evaluated using functional
    correctness, one common approach is to evaluate AI’s outputs against reference
    data. For example, if you ask a model to translate a sentence from French to English,
    you can evaluate the generated English translation against the correct English
    translation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你关心的任务不能使用功能正确性自动评估，一种常见的方法是将AI的输出与参考数据进行比较。例如，如果你要求一个模型将句子从法语翻译成英语，你可以将生成的英语翻译与正确的英语翻译进行比较。
- en: Each example in the reference data follows the format (input, reference responses).
    An input can have multiple reference responses, such as multiple possible English
    translations of a French sentence. Reference responses are also called *ground
    truths* or *canonical responses*. Metrics that require references are *reference-based*,
    and metrics that don’t are *reference-free*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 参考数据中的每个示例都遵循格式（输入，参考响应）。一个输入可以有多个参考响应，例如一个法语句子的多个可能的英语翻译。参考响应也被称为*基准真实值*或*规范响应*。需要参考的指标是*基于参考的*，而不需要参考的指标是*无参考的*。
- en: Since this evaluation approach requires reference data, it’s bottlenecked by
    how much and how fast reference data can be generated. Reference data is generated
    typically by humans and increasingly by AIs. Using human-generated data as the
    reference means that we treat human performance as the gold standard, and AI’s
    performance is measured against human performance. Human-generated data can be
    expensive and time-consuming to generate, leading many to use AI to generate reference
    data instead. AI-generated data might still need human reviews, but the labor
    needed to review it is much less than the labor needed to generate reference data
    from scratch.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种评估方法需要参考数据，因此它受到参考数据生成多少和速度的限制。参考数据通常由人类生成，越来越多地由AI生成。使用人类生成数据作为参考意味着我们将人类性能视为黄金标准，AI的性能与人类性能相比较。人类生成数据可能很昂贵且耗时，导致许多人使用AI生成参考数据。AI生成的数据可能仍然需要人类审查，但审查所需的劳动量远小于从头开始生成参考数据的劳动量。
- en: 'Generated responses that are more similar to the reference responses are considered
    better. There are four ways to measure the similarity between two open-ended texts:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与参考响应更相似的生成响应被认为是更好的。有两种方式来衡量两个开放式文本之间的相似度：
- en: Asking an evaluator to make the judgment whether two texts are the same
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要求评估员判断两个文本是否相同
- en: 'Exact match: whether the generated response matches one of the reference responses
    exactly'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 精确匹配：生成的响应是否与参考响应之一完全匹配
- en: 'Lexical similarity: how similar the generated response looks to the reference
    responses'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词汇相似度：生成的响应在视觉上与参考响应的相似程度
- en: 'Semantic similarity: how close the generated response is to the reference responses
    in meaning (semantics)'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语义相似度：生成的响应在意义上与参考响应的接近程度（语义）
- en: Two responses can be compared by human evaluators or AI evaluators. AI evaluators
    are increasingly common and will be the focus of the next section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过人工评估员或AI评估员来比较两种响应。AI评估员越来越普遍，并将是下一节的重点。
- en: 'This section focuses on hand-designed metrics: exact match, lexical similarity,
    and semantic similarity. Scores by exact matching are binary (match or not), whereas
    the other two scores are on a sliding scale (such as between 0 and 1 or between
    –1 and 1). Despite the ease of use and flexibility of the AI as a judge approach,
    hand-designed similarity measurements are still widely used in the industry for
    their exact nature.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍手工设计的指标：精确匹配、词汇相似度和语义相似度。精确匹配的分数是二进制的（匹配或不匹配），而其他两个分数是滑动尺度（例如在0到1或-1到1之间）。尽管AI作为评委的方法易于使用且灵活，但手工设计的相似度测量在工业界仍然被广泛使用，因为它们的精确性。
- en: Note
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'This section discusses how you can use similarity measurements to evaluate
    the quality of a generated output. However, you can also use similarity measurements
    for many other use cases, including but not limited to the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了如何使用相似度测量来评估生成输出的质量。然而，你也可以使用相似度测量来处理许多其他用例，包括但不限于以下内容：
- en: Retrieval and search
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 检索和搜索
- en: find items similar to a query
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 查找与查询相似的项
- en: Ranking
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 排序
- en: rank items based on how similar they are to a query
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 根据与查询的相似度对项进行排序
- en: Clustering
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类
- en: cluster items based on how similar they are to each other
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 根据彼此的相似度对项进行聚类
- en: Anomaly detection
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测
- en: detect items that are the least similar to the rest
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 检测与其它项最不相似的项
- en: Data deduplication
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 数据去重
- en: remove items that are too similar to other items
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 移除与其他项过于相似的项
- en: Techniques discussed in this section will come up again throughout the book.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的技术将在整本书中再次出现。
- en: Exact match
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全匹配
- en: 'It’s considered an exact match if the generated response matches one of the
    reference responses exactly. Exact matching works for tasks that expect short,
    exact responses such as simple math problems, common knowledge queries, and trivia-style
    questions. Here are examples of inputs that have short, exact responses:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生成的响应与参考响应之一完全匹配，则被认为是完全匹配。完全匹配适用于期望简短、精确响应的任务，例如简单的数学问题、常识查询和Trivia风格的提问。以下是一些具有简短、精确响应的输入示例：
- en: “What’s 2 + 3?”
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “2 + 3等于多少？”
- en: “Who was the first woman to win a Nobel Prize?”
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “谁是第一个获得诺贝尔奖的女性？”
- en: “What’s my current account balance?”
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我的当前账户余额是多少？”
- en: '“Fill in the blank: Paris to France is like ___ to England.”'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “填空题：巴黎到法国就像___到英国。”
- en: There are variations to matching that take into account formatting issues. One
    variation is to accept any output that contains the reference response as a match.
    Consider the question “What’s 2 + 3?” The reference response is “5”. This variation
    accepts all outputs that contain “5”, including “The answer is 5” and “2 + 3 is
    5”.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些匹配的变体会考虑格式问题。一种变体是接受任何包含参考响应的输出作为匹配。考虑一下这个问题：“2 + 3等于多少？”参考响应是“5”。这种变体接受所有包含“5”的输出，包括“答案是5”和“2
    + 3等于5”。
- en: However, this variation can sometimes lead to the wrong solution being accepted.
    Consider the question “What year was Anne Frank born?” Anne Frank was born on
    June 12, 1929, so the correct response is 1929\. If the model outputs “September
    12, 1929”, the correct year is included in the output, but the output is factually
    wrong.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种变体有时可能导致接受错误的解决方案。考虑一下这个问题：“安妮·弗兰克出生在哪一年？”安妮·弗兰克出生于1929年6月12日，所以正确的响应是1929年。如果模型输出“1929年9月12日”，则正确的年份包含在输出中，但输出在事实上是错误的。
- en: Beyond simple tasks, exact match rarely works. Given the original French sentence
    “Comment ça va?”, there are multiple possible English translations, such as “How
    are you?”, “How is everything?”, and “How are you doing?” If the reference data
    contains only these three translations and a model generates “How is it going?”,
    the model’s response will be marked as wrong. The longer and more complex the
    original text, the more possible translations there are. It’s impossible to create
    an exhaustive set of possible responses for an input. For complex tasks, lexical
    similarity and semantic similarity work better.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单任务之外，完全匹配很少有效。给定原始法语句子“Comment ça va?”，有多个可能的英语翻译，例如“你好吗？”，“一切都好吗？”，以及“你怎么样？”。如果参考数据只包含这三种翻译，并且模型生成“How
    is it going?”，则模型的响应将被标记为错误。原始文本越长、越复杂，可能的翻译就越多。为输入创建一个可能的响应的详尽集合是不可能的。对于复杂任务，词汇相似度和语义相似度效果更好。
- en: Lexical similarity
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词汇相似度
- en: Lexical similarity measures how much two texts overlap. You can do this by first
    breaking each text into smaller tokens.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇相似度衡量两个文本重叠的程度。你可以通过首先将每个文本分解成更小的标记来实现这一点。
- en: 'In its simplest form, lexical similarity can be measured by counting how many
    tokens two texts have in common. As an example, consider the reference response
    *“My cats scare the mice”* and two generated responses:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单形式中，词汇相似度可以通过计算两个文本有多少共同标记来衡量。例如，考虑参考响应*“我的猫吓跑了老鼠”*和两个生成的响应：
- en: “My cats eat the mice”
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我的猫吃老鼠”
- en: “Cats and mice fight all the time”
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “猫和老鼠总是打架”
- en: Assume that each token is a word. If you count overlapping of individual words
    only, response A contains 4 out of 5 words in the reference response (the similarity
    score is 80%), whereas response B contains only 3 out of 5 (the similarity score
    is 60%). Response A is, therefore, considered more similar to the reference response.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 假设每个标记是一个单词。如果你只计算单个单词的重叠，响应A包含参考响应中的5个单词中的4个（相似度分数为80%），而响应B只包含5个中的3个（相似度分数为60%）。因此，响应A被认为与参考响应更相似。
- en: 'One way to measure lexical similarity is *approximate string matching*, known
    colloquially as *fuzzy matching*. It measures the similarity between two texts
    by counting how many edits it’d need to convert from one text to another, a number
    called *edit distance*. The usual three edit operations are:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 测量词汇相似度的一种方法是*近似字符串匹配*，俗称*模糊匹配*。它通过计算将一个文本转换为另一个文本所需的编辑次数来衡量两个文本之间的相似度，这个数字被称为*编辑距离*。通常的三个编辑操作是：
- en: 'Deletion: “b*r*ad” -> “bad”'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除：“b*r*ad”删除为“bad”
- en: 'Insertion: “bad” -> “ba*r*d”'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入：将“bad”替换为“ba*r*d”
- en: 'Substitution: “b*a*d” -> “b*e*d”'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 替换：“b*a*d”替换为“b*e*d”
- en: 'Some fuzzy matchers also treat transposition, swapping two letters (e.g., “ma*ts*”
    -> “ma*st*”), to be an edit. However, some fuzzy matchers treat each transposition
    as two edit operations: one deletion and one insertion.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模糊匹配器也将置换（交换两个字母，例如，“ma*ts*” -> “ma*st*”）视为一个编辑。然而，一些模糊匹配器将每个置换视为两个编辑操作：一个删除和一个插入。
- en: For example, “bad” is one edit to “bard” and three edits to “cash”, so “bad”
    is considered more similar to “bard” than to “cash”.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，“bad”只需一次编辑即可变为“bard”，而需要三次编辑才能变为“cash”，因此“bad”被认为比“cash”更接近“bard”。
- en: 'Another way to measure lexical similarity is *n-gram similarity*, measured
    based on the overlapping of sequences of tokens, *n-grams*, instead of single
    tokens. A 1-gram (unigram) is a token. A 2-gram (bigram) is a set of two tokens.
    “My cats scare the mice” consists of four bigrams: “my cats”, “cats scare”, “scare
    the”, and “the mice”. You measure what percentage of n-grams in reference responses
    is also in the generated response.^([12](ch03.html#id922))'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 测量词汇相似度的另一种方法是*n-gram相似度*，它基于标记序列（*n-grams*）的重叠来衡量，而不是单个标记。1-gram（单语）是一个标记。2-gram（双语）是一组两个标记。“My
    cats scare the mice”包含四个双语：“my cats”、“cats scare”、“scare the”和“the mice”。你测量参考响应中n-grams的百分比，这些n-grams也出现在生成的响应中.^([12](ch03.html#id922))
- en: Common metrics for lexical similarity are BLEU, ROUGE, METEOR++, TER, and CIDEr.
    They differ in exactly how the overlapping is calculated. Before foundation models,
    BLEU, ROUGE, and their relatives were common, especially for translation tasks.
    Since the rise of foundation models, fewer benchmarks use lexical similarity.
    Examples of benchmarks that use these metrics are [WMT](https://oreil.ly/92yRh),
    [COCO Captions](https://oreil.ly/BO3-0), and [GEMv2](https://arxiv.org/abs/2206.11249).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的词汇相似度度量标准有BLEU、ROUGE、METEOR++、TER和CIDEr。它们在计算重叠部分的方式上有所不同。在基础模型出现之前，BLEU、ROUGE及其相关度量标准很常见，尤其是在翻译任务中。随着基础模型的出现，使用词汇相似度的基准测试越来越少。使用这些度量标准的基准测试示例包括[WMT](https://oreil.ly/92yRh)、[COCO
    Captions](https://oreil.ly/BO3-0)和[GEMv2](https://arxiv.org/abs/2206.11249)。
- en: A drawback of this method is that it requires curating a comprehensive set of
    reference responses. A good response can get a low similarity score if the reference
    set doesn’t contain any response that looks like it. On some benchmark examples,
    [Adept](https://oreil.ly/OWD2v) found that its model Fuyu performed poorly not
    because the model’s outputs were wrong, but because some correct answers were
    missing in the reference data. [Figure 3-5](#ch03a_figure_5_1730150757025084)
    shows an example of an image-captioning task in which Fuyu generated a correct
    caption but was given a low score.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是它需要整理一套全面的参考响应。如果参考集中不包含任何看起来类似的响应，一个好的响应可能会得到一个低的相似度分数。在某些基准测试示例中，[Adept](https://oreil.ly/OWD2v)发现其模型Fuyu表现不佳，并不是因为模型的输出是错误的，而是因为参考数据中缺少一些正确的答案。[图3-5](#ch03a_figure_5_1730150757025084)展示了Fuyu生成正确标题但得分较低的一个图像标题任务的示例。
- en: Not only that, but references can be wrong. For example, the organizers of the
    WMT 2023 Metrics shared task, which focuses on examining evaluation metrics for
    machine translation, reported that they found many bad reference translations
    in their data. Low-quality reference data is one of the reasons that reference-free
    metrics were strong contenders for reference-based metrics in terms of correlation
    to human judgment ([Freitag et al., 2023](https://oreil.ly/tmWqk)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅如此，参考文献也可能错误。例如，专注于检查机器翻译评估指标的WMT 2023 Metrics共享任务的组织者报告说，他们在数据中发现了许多质量低下的参考翻译。低质量的参考数据是参考无关指标在相关性方面成为基于参考指标强劲竞争者之一的原因（[Freitag等人，2023](https://oreil.ly/tmWqk)）。
- en: Another drawback of this measurement is that higher lexical similarity scores
    don’t always mean better responses. For example, on HumanEval, a code generation
    benchmark, OpenAI found that BLEU scores for incorrect and correct solutions were
    similar. This indicates that optimizing for BLEU scores isn’t the same as optimizing
    for functional correctness ([Chen et al., 2021](https://arxiv.org/abs/2107.03374)).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这种测量的另一个缺点是，更高的词汇相似度分数并不总是意味着更好的响应。例如，在HumanEval这个代码生成基准测试中，OpenAI发现错误和正确解决方案的BLEU分数相似。这表明优化BLEU分数并不等同于优化功能性正确性（[Chen等人，2021](https://arxiv.org/abs/2107.03374)）。
- en: '![A screenshot of a computer'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机屏幕截图'
- en: Description automatically generated](assets/aien_0305.png)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0305.png)
- en: Figure 3-5\. An example where Fuyu generated a correct option but was given
    a low score because of the limitation of reference captions.
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5. Fuyu生成正确选项但因其参考标题的限制而得到低分的例子。
- en: Semantic similarity
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义相似度
- en: Lexical similarity measures whether two texts look similar, not whether they
    have the same meaning. Consider the two sentences “What’s up?” and “How are you?”
    Lexically, they are different—there’s little overlapping in the words and letters
    they use. However, semantically, they are close. Conversely, similar-looking texts
    can mean very different things. “Let’s eat, grandma” and “Let’s eat grandma” mean
    two completely different things.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇相似度衡量两个文本看起来是否相似，而不是它们是否有相同的意义。考虑以下两个句子：“What’s up?” 和 “How are you?” 从词汇上看，它们是不同的——它们使用的单词和字母重叠很少。然而，从语义上看，它们是接近的。相反，看起来相似的文字可能意味着非常不同的东西。“Let’s
    eat, grandma” 和 “Let’s eat grandma” 意味着完全不同的事情。
- en: '*Semantic similarity* aims to compute the similarity in semantics. This first
    requires transforming a text into a numerical representation, which is called
    an *embedding*. For example, the sentence “the cat sits on a mat” might be represented
    using an embedding that looks like this: `[0.11, 0.02, 0.54]`. Semantic similarity
    is, therefore, also called *embedding similarity*.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*语义相似度*旨在计算语义上的相似度。这首先需要将文本转换为一个数值表示，这被称为*嵌入*。例如，句子“the cat sits on a mat”可能使用如下嵌入来表示：`[0.11,
    0.02, 0.54]`。因此，语义相似度也被称为*嵌入相似度*。'
- en: '[“Introduction to Embedding”](#ch03a_introduction_to_embedding_1730150757064669)
    discusses how embeddings work. For now, let’s assume that you have a way to transform
    texts into embeddings. The similarity between two embeddings can be computed using
    metrics such as cosine similarity. Two embeddings that are exactly the same have
    a similarity score of 1\. Two opposite embeddings have a similarity score of –1.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[“嵌入介绍”](#ch03a_introduction_to_embedding_1730150757064669) 讨论了嵌入是如何工作的。目前，让我们假设你有一种将文本转换为嵌入的方法。两个嵌入之间的相似度可以使用诸如余弦相似度等指标来计算。两个完全相同的嵌入具有相似度分数1。两个相反的嵌入具有相似度分数-1。'
- en: '*I’m using text examples, but semantic similarity can be computed for embeddings
    of any data modality, including images and audio.* Semantic similarity for text
    is sometimes called semantic textual similarity.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在使用文本示例，但语义相似度可以计算任何数据模态的嵌入，包括图像和音频。* 文本的语义相似度有时被称为语义文本相似度。'
- en: Warning
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: While I put semantic similarity in the exact evaluation category, it can be
    considered subjective, as different embedding algorithms can produce different
    embeddings. However, given two embeddings, the similarity score between them is
    computed exactly.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我将语义相似度放在了精确评估类别中，但它可以被认为是主观的，因为不同的嵌入算法可以产生不同的嵌入。然而，给定两个嵌入，它们之间的相似度分数是精确计算的。
- en: 'Mathematically, let A be an embedding of the generated response, and B be an
    embedding of a reference response. The cosine similarity between A and B is computed
    as $f r a c upper A dot upper B StartAbsoluteValue EndAbsoluteValue upper A StartAbsoluteValue
    EndAbsoluteValue StartAbsoluteValue EndAbsoluteValue upper B StartAbsoluteValue
    EndAbsoluteValue$ , with:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，设 A 为生成响应的嵌入，B 为参考响应的嵌入。A 和 B 之间的余弦相似度计算为 $f r a c upper A dot upper B
    StartAbsoluteValue EndAbsoluteValue upper A StartAbsoluteValue EndAbsoluteValue
    StartAbsoluteValue EndAbsoluteValue upper B StartAbsoluteValue EndAbsoluteValue$
    ，其中：
- en: $upper A dot upper B$ being the dot product of A and B
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $upper A dot upper B$ 是 A 和 B 的点积
- en: $StartAbsoluteValue EndAbsoluteValue upper A StartAbsoluteValue EndAbsoluteValue$
    being the Euclidean norm (also known as $upper L squared$ norm) of A. If A is
    [0.11, 0.02, 0.54], $StartAbsoluteValue EndAbsoluteValue upper A StartAbsoluteValue
    EndAbsoluteValue equals StartRoot 0.11 squared plus 0.02 squared plus 0.54 squared
    EndRoot$
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $StartAbsoluteValue EndAbsoluteValue upper A StartAbsoluteValue EndAbsoluteValue$
    是 A 的欧几里得范数（也称为 $upper L squared$ 范数）。如果 A 是 [0.11, 0.02, 0.54]，则 $StartAbsoluteValue
    EndAbsoluteValue upper A StartAbsoluteValue EndAbsoluteValue$ 等于 $StartRoot 0.11
    squared plus 0.02 squared plus 0.54 squared EndRoot$
- en: Metrics for semantic textual similarity include [BERTScore](https://arxiv.org/abs/1904.09675)
    (embeddings are generated by BERT) and [MoverScore](https://oreil.ly/v2ENK) (embeddings
    are generated by a mixture of algorithms).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 语义文本相似度的度量包括由 BERT 生成的 [BERTScore](https://arxiv.org/abs/1904.09675) 和由多种算法混合生成的
    [MoverScore](https://oreil.ly/v2ENK)。
- en: Semantic textual similarity doesn’t require a set of reference responses as
    comprehensive as lexical similarity does. However, the reliability of semantic
    similarity depends on the quality of the underlying embedding algorithm. Two texts
    with the same meaning can still have a low semantic similarity score if their
    embeddings are bad. Another drawback of this measurement is that the underlying
    embedding algorithm might require nontrivial compute and time to run.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 语义文本相似度不需要像词汇相似度那样全面的一组参考响应。然而，语义相似度的可靠性取决于底层嵌入算法的质量。即使两个文本具有相同的意义，如果它们的嵌入质量差，它们的语义相似度得分仍然可能很低。这种测量的另一个缺点是，底层嵌入算法可能需要非平凡的计算和时间来运行。
- en: Before we move on to discuss AI as a judge, let’s go over a quick introduction
    to embedding. The concept of embedding lies at the heart semantic similarity,
    and is the backbone of many topics we explore throughout the book, including vector
    search in [Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386) and data
    deduplication in [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续讨论 AI 作为法官之前，让我们快速了解一下嵌入的简介。嵌入的概念位于语义相似性的核心，也是本书中探讨的许多主题的骨干，包括第 6 章（ch06.html#ch06_rag_and_agents_1730157386571386）中的向量搜索和第
    8 章（ch08.html#ch08_dataset_engineering_1730130932019888）中的数据去重。
- en: Introduction to Embedding
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入简介
- en: Since computers work with numbers, a model needs to convert its input into numerical
    representations that computers can process. *An embedding is a numerical representation
    that aims to capture the meaning of the original data.*
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算机使用数字进行工作，因此模型需要将其输入转换为计算机可以处理的数值表示。*嵌入是一种数值表示，旨在捕捉原始数据的含义。*
- en: 'An embedding is a vector. For example, the sentence *“the cat sits on a mat”*
    might be represented using an embedding vector that looks like this: `[0.11, 0.02,
    0.54]`. Here, I use a small vector as an example. In reality, the size of an embedding
    vector (the number of elements in the embedding vector) is typically between 100
    and 10,000.^([13](ch03.html#id927))'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是一个向量。例如，句子 *“the cat sits on a mat”* 可以使用如下嵌入向量表示：`[0.11, 0.02, 0.54]`。在这里，我使用一个小向量作为例子。实际上，嵌入向量的大小（嵌入向量中的元素数量）通常在
    100 到 10,000 之间。[13](ch03.html#id927)
- en: Models trained especially to produce embeddings include the open source models
    BERT, CLIP (Contrastive Language–Image Pre-training), and [Sentence Transformers](https://github.com/UKPLab/sentence-transformers).
    There are also proprietary embedding models provided as APIs.^([14](ch03.html#id929))
    [Table 3-2](#ch03a_table_2_1730150757038080) shows the embedding sizes of some
    popular models.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 专门用于生成嵌入的模型包括开源模型 BERT、CLIP（对比语言-图像预训练）和 [Sentence Transformers](https://github.com/UKPLab/sentence-transformers)。还有一些作为
    API 提供的专有嵌入模型。[14](ch03.html#id929) [表 3-2](#ch03a_table_2_1730150757038080) 展示了一些流行模型的嵌入大小。
- en: Table 3-2\. Embedding sizes used by common models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-2\. 常见模型使用的嵌入大小。
- en: '| Model | Embedding size |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 嵌入大小 |'
- en: '| --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| [Google’s BERT](https://arxiv.org/abs/1810.04805) | BERT base: 768 BERT large:
    1024 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| [Google 的 BERT](https://arxiv.org/abs/1810.04805) | BERT base: 768 BERT large:
    1024 |'
- en: '| [OpenAI’s CLIP](https://oreil.ly/0Cfcw) | Image: 512 Text: 512 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| [OpenAI 的 CLIP](https://oreil.ly/0Cfcw) | Image: 512 Text: 512 |'
- en: '| [OpenAI Embeddings API](https://oreil.ly/SBUiU) | text-embedding-3-small:
    1536 text-embedding-3-large: 3072 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| [OpenAI Embeddings API](https://oreil.ly/SBUiU) | text-embedding-3-small:
    1536 text-embedding-3-large: 3072 |'
- en: '| [Cohere’s Embed v3](https://oreil.ly/BNNNm) | embed-english-v3.0: 1024 embed-english-light-3.0:
    384 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| [Cohere 的 Embed v3](https://oreil.ly/BNNNm) | embed-english-v3.0: 1024 embed-english-light-3.0:
    384 |'
- en: Because models typically require their inputs to first be transformed into vector
    representations, many ML models, including GPTs and Llamas, also involve a step
    to generate embeddings. [“Transformer architecture”](ch02.html#ch02_transformer_architecture_1730147895571820)
    visualizes the embedding layer in a transformer model. If you have access to the
    intermediate layers of these models, you can use them to extract embeddings. However,
    the quality of these embeddings might not be as good as the embeddings generated
    by specialized embedding models.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型通常需要将输入首先转换为向量表示，许多机器学习模型，包括 GPT 和 Llamas，也涉及一个生成嵌入的步骤。[“Transformer 架构”](ch02.html#ch02_transformer_architecture_1730147895571820)
    在 Transformer 模型中可视化嵌入层。如果你可以访问这些模型的中间层，你可以使用它们来提取嵌入。然而，这些嵌入的质量可能不如专门嵌入模型生成的嵌入质量好。
- en: The goal of the embedding algorithm is to produce embeddings that capture the
    essence of the original data. How do we verify that? The embedding vector `[0.11,
    0.02, 0.54]` looks nothing like the original text “the cat sits on a mat”.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入算法的目标是生成能够捕捉原始数据本质的嵌入。我们如何验证这一点？嵌入向量 `[0.11, 0.02, 0.54]` 与原始文本“the cat sits
    on a mat”看起来毫无相似之处。
- en: At a high level, an embedding algorithm is considered good if more-similar texts
    have closer embeddings, measured by cosine similarity or related metrics. The
    embedding of the sentence “the cat sits on a mat” should be closer to the embedding
    of “the dog plays on the grass” than the embedding of “AI research is super fun”.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，一个嵌入算法被认为是好的，如果更相似的文字有更接近的嵌入，通过余弦相似度或相关指标来衡量。句子“the cat sits on a mat”的嵌入应该比“AI
    research is super fun”的嵌入更接近句子“the dog plays on the grass”的嵌入。
- en: You can also evaluate the quality of embeddings based on their utility for your
    task. Embeddings are used in many tasks, including classification, topic modeling,
    recommender systems, and RAG. An example of benchmarks that measure embedding
    quality on multiple tasks is MTEB, Massive Text Embedding Benchmark ([Muennighoff
    et al., 2023](https://arxiv.org/abs/2210.07316)).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以根据嵌入在任务中的实用性来评估嵌入的质量。嵌入被用于许多任务中，包括分类、主题建模、推荐系统和 RAG。一个衡量多个任务嵌入质量的基准是 MTEB，大规模文本嵌入基准
    ([Muennighoff et al., 2023](https://arxiv.org/abs/2210.07316))。
- en: I use texts as examples, but any data can have embedding representations. For
    example, ecommerce solutions like [Criteo](https://arxiv.org/abs/1607.07326) and
    [Coveo](https://oreil.ly/a6jbV) have embeddings for products. [Pinterest](https://oreil.ly/uJNFH)
    has embeddings for images, graphs, queries, and even users.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用文本作为例子，但任何数据都可以有嵌入表示。例如，像 [Criteo](https://arxiv.org/abs/1607.07326) 和 [Coveo](https://oreil.ly/a6jbV)
    这样的电子商务解决方案为产品提供嵌入。[Pinterest](https://oreil.ly/uJNFH) 为图像、图形、查询甚至用户提供嵌入。
- en: A new frontier is to create joint embeddings for data of different modalities.
    CLIP ([Radford et al., 2021](https://arxiv.org/abs/2103.00020)) was one of the
    first major models that could map data of different modalities, text and images,
    into a joint embedding space. ULIP (unified representation of language, images,
    and point clouds), ([Xue et al., 2022](https://arxiv.org/abs/2212.05171)) aims
    to create unified representations of text, images, and 3D point clouds. ImageBind
    ([Girdhar et al., 2023](https://arxiv.org/abs/2305.05665)) learns a joint embedding
    across six different modalities, including text, images, and audio.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新的前沿是为不同模态的数据创建联合嵌入。CLIP ([Radford et al., 2021](https://arxiv.org/abs/2103.00020))
    是第一个能够将不同模态的数据，如文本和图像，映射到联合嵌入空间的主要模型之一。ULIP (统一的语言、图像和点云表示)，([Xue et al., 2022](https://arxiv.org/abs/2212.05171))
    旨在创建文本、图像和 3D 点云的统一表示。ImageBind ([Girdhar et al., 2023](https://arxiv.org/abs/2305.05665))
    在包括文本、图像和音频在内的六个不同模态之间学习联合嵌入。
- en: '[Figure 3-6](#ch03a_figure_6_1730150757025090) visualizes CLIP’s architecture.
    CLIP is trained using (image, text) pairs. The text corresponding to an image
    can be the caption or a comment associated with this image. For each (image, text)
    pair, CLIP uses a text encoder to convert the text to a text embedding, and an
    image encoder to convert the image to an image embedding. It then projects both
    these embeddings into a joint embedding space. The training goal is to get the
    embedding of an image close to the embedding of the corresponding text in this
    joint space.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-6](#ch03a_figure_6_1730150757025090) 展示了 CLIP 的架构。CLIP 使用 (图像，文本) 对进行训练。与图像对应的文本可以是图像的标题或与该图像相关的评论。对于每个
    (图像，文本) 对，CLIP 使用文本编码器将文本转换为文本嵌入，并使用图像编码器将图像转换为图像嵌入。然后，它将这两个嵌入投影到联合嵌入空间中。训练目标是获得图像嵌入，使其接近该联合空间中相应文本的嵌入。'
- en: '![A diagram of a computer'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机图示'
- en: Description automatically generated](assets/aien_0306.png)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0306.png)
- en: Figure 3-6\. CLIP’s architecture (Radford et al., 2021).
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-6\. CLIP 的架构（Radford 等人，2021 年）。
- en: A joint embedding space that can represent data of different modalities is a
    *multimodal embedding space*. In a text–image joint embedding space, the embedding
    of an image of a man fishing should be closer to the embedding of the text “a
    fisherman” than the embedding of the text “fashion show”. This joint embedding
    space allows embeddings of different modalities to be compared and combined. For
    example, this enables text-based image search. Given a text, it helps you find
    images closest to this text.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 能够表示不同模态数据的联合嵌入空间被称为 *多模态嵌入空间*。在文本-图像联合嵌入空间中，钓鱼图像的嵌入应比“时装秀”文本的嵌入更接近“渔夫”文本的嵌入。这个联合嵌入空间允许不同模态的嵌入进行比较和组合。例如，这可以实现基于文本的图像搜索。给定一个文本，它可以帮助你找到与该文本最接近的图像。
- en: AI as a Judge
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 作为裁判
- en: The challenges of evaluating open-ended responses have led many teams to fall
    back on human evaluation. As AI has successfully been used to automate many challenging
    tasks, can AI automate evaluation as well? The approach of using AI to evaluate
    AI is called AI as a judge or LLM as a judge. An AI model that is used to evaluate
    other AI models is called an *AI judge*.^([15](ch03.html#id937))
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 评估开放式响应的挑战导致许多团队回归到人工评估。随着 AI 成功地被用于自动化许多具有挑战性的任务，AI 是否也能自动化评估？使用 AI 评估 AI 的方法被称为
    AI 作为裁判或 LLM 作为裁判。用于评估其他 AI 模型的 AI 模型被称为 *AI 裁判*。[15](ch03.html#id937))
- en: While the idea of using AI to automate evaluation has been around for a long
    time,^([16](ch03.html#id938)) it only became practical when AI models became capable
    of doing so, which was around 2020 with the release of GPT-3\. As of this writing,
    AI as a judge has become one of the most, if not the most, common methods for
    evaluating AI models in production. Most demos of AI evaluation startups I saw
    in 2023 and 2024 leveraged AI as a judge in one way or another. [LangChain’s *State
    of AI*](https://oreil.ly/7Fkh-) report in 2023 noted that 58% of evaluations on
    their platform were done by AI judges. AI as a judge is also an active area of
    research.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用 AI 自动化评估的想法已经存在很长时间了，[16](ch03.html#id938)]，但直到 AI 模型能够做到这一点才变得可行，这大约是在
    2020 年 GPT-3 发布时。截至本文写作时，AI 作为裁判已成为评估生产中 AI 模型最常见的方法之一，如果不是最常见的方法。我在 2023 年和 2024
    年看到的许多 AI 评估初创公司的演示都以某种方式利用了 AI 作为裁判。2023 年 LangChain 的 *AI 状态* 报告指出，他们平台上 58%
    的评估是由 AI 裁判完成的。AI 作为裁判也是一个活跃的研究领域。
- en: Why AI as a Judge?
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么是 AI 作为裁判？
- en: AI judges are fast, easy to use, and relatively cheap compared to human evaluators.
    They can also work without reference data, which means they can be used in production
    environments where there is no reference data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 与人工评估员相比，AI 评估员速度快、易于使用，且相对便宜。它们还可以在没有参考数据的情况下工作，这意味着它们可以在没有参考数据的生产环境中使用。
- en: 'You can ask AI models to judge an output based on any criteria: correctness,
    repetitiveness, toxicity, wholesomeness, hallucinations, and more. This is similar
    to how you can ask a person to give their opinion about anything. You might think,
    “But you can’t always trust people’s opinions.” That’s true, and you can’t always
    trust AI’s judgments, either. However, as each AI model is an aggregation of the
    masses, it’s possible for AI models to make judgments representative of the masses.
    With the right prompt for the right model, you can get reasonably good judgments
    on a wide range of topics.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以要求AI模型根据任何标准来判断输出：正确性、重复性、毒性、完整性、幻觉等。这类似于你可以要求一个人对任何事情发表意见。你可能认为，“但你不能总是相信人们的意见。”这是真的，你也不能总是相信AI的判断。然而，由于每个AI模型都是大众的集合，AI模型可能做出代表大众的判断。通过为正确的模型提供正确的提示，你可以在广泛的主题上获得相当好的判断。
- en: Studies have shown that certain AI judges are strongly correlated to human evaluators.
    In 2023, [Zheng et al.](https://arxiv.org/abs/2306.05685) found that on their
    evaluation benchmark, MT-Bench, the agreement between GPT-4 and humans reached
    85%, which is even higher than the agreement among humans (81%). AlpacaEval authors
    ([Dubois et al., 2023](https://arxiv.org/abs/2404.04475)) also found that their
    AI judges have a near perfect (0.98) correlation with LMSYS’s Chat Arena leaderboard,
    which is evaluated by humans.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，某些AI评委与人类评估者有很强的相关性。2023年，[郑等](https://arxiv.org/abs/2306.05685)在他们评估基准MT-Bench上发现，GPT-4与人类之间的协议达到了85%，甚至高于人类之间的协议（81%）。AlpacaEval作者([Dubois等，2023](https://arxiv.org/abs/2404.04475))也发现，他们的AI评委与LMSYS的Chat
    Arena排行榜（由人类评估）有近完美的（0.98）相关性。
- en: Not only can AI evaluate a response, but it can also explain its decision, which
    can be especially useful when you want to audit your evaluation results. [Figure 3-7](#ch03a_figure_7_1730150757025099)
    shows an example of GPT-4 explaining its judgment.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅AI可以评估一个回答，它还可以解释其决定，这在你想审计你的评估结果时特别有用。[图3-7](#ch03a_figure_7_1730150757025099)展示了GPT-4解释其判断的示例。
- en: Its flexibility makes AI as a judge useful for a wide range of applications,
    and for some applications, it’s the only automatic evaluation option. Even when
    AI judgments aren’t as good as human judgments, they might still be good enough
    to guide an application’s development and provide sufficient confidence to get
    a project off the ground.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 它的灵活性使得AI作为评委在广泛的领域中非常有用，对于某些应用来说，它是唯一的自动评估选项。即使AI的判断不如人类判断好，它们可能仍然足够好，以指导应用的发展，并足以使项目得以启动。
- en: '![A screenshot of a paper'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![一张论文的截图'
- en: Description automatically generated](assets/aien_0307.png)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0307.png)
- en: Figure 3-7\. Not only can AI judges score, they also can explain their decisions.
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7\. 不仅AI评委可以评分，他们还可以解释他们的决定。
- en: How to Use AI as a Judge
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用AI作为评委
- en: 'There are many ways you can use AI to make judgments. For example, you can
    use AI to evaluate the quality of a response by itself, compare that response
    to reference data, or compare that response to another response. Here are naive
    example prompts for these three approaches:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用许多方法来使用AI进行判断。例如，你可以使用AI来评估回答的质量，将该回答与参考数据进行比较，或将该回答与另一个回答进行比较。以下是这三个方法的简单示例提示：
- en: 'Evaluate the quality of a response by itself, given the original question:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据原始问题本身评估回答的质量：
- en: '[PRE1]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Compare a generated response to a reference response to evaluate whether the
    generated response is the same as the reference response. This can be an alternative
    approach to human-designed similarity measurements:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的回答与参考回答进行比较，以评估生成的回答是否与参考回答相同。这可以是一种替代人工设计相似度测量的方法：
- en: '[PRE2]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Compare two generated responses and determine which one is better or predict
    which one users will likely prefer. This is helpful for generating preference
    data for post-training alignment (discussed in [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359)),
    test-time compute (discussed in [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359)),
    and ranking models using comparative evaluation (discussed in the next section):'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较两个生成的回答，并确定哪一个更好，或者预测用户可能会更喜欢哪一个。这对于生成训练后对齐（在第2章中讨论）的数据、测试时计算（在第2章中讨论）以及使用比较评估对排名模型进行排序（在下一节中讨论）很有帮助：
- en: '[PRE3]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A general-purpose AI judge can be asked to evaluate a response based on any
    criteria. If you’re building a roleplaying chatbot, you might want to evaluate
    if a chatbot’s response is consistent with the role users want it to play, such
    as “Does this response sound like something Gandalf would say?” If you’re building
    an application to generate promotional product photos, you might want to ask “From
    1 to 5, how would you rate the trustworthiness of the product in this image?”
    [Table 3-3](#ch03a_table_3_1730150757038092) shows common built-in AI as a judge
    criteria offered by some AI tools.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 通用人工智能评判者可以根据任何标准来评估一个回应。如果你正在构建一个角色扮演聊天机器人，你可能想要评估聊天机器人的回应是否与用户期望的角色一致，例如“这个回应听起来像甘道夫会说的话吗？”如果你正在构建一个生成促销产品图片的应用程序，你可能想问“从1到5，你会如何评价这张图片中产品的可信度？”[表3-3](#ch03a_table_3_1730150757038092)展示了某些AI工具提供的常见内置AI评判标准。
- en: Table 3-3\. Examples of built-in AI as a judge criteria offered by some AI tools,
    as of September 2024\. Note that as these tools evolve, these built-in criteria
    will change.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-3\. 截至2024年9月，某些AI工具提供的内置AI评判标准示例。请注意，随着这些工具的发展，这些内置标准将会变化。
- en: '| AI Tools | Built-in criteria |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| AI工具 | 内置标准 |'
- en: '| --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| [Azure AI Studio](https://oreil.ly/57jOL) | Groundedness, relevance, coherence,
    fluency, similarity |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| [Azure AI Studio](https://oreil.ly/57jOL) | 基础性，相关性，连贯性，流畅性，相似性 |'
- en: '| [MLflow.metrics](https://oreil.ly/2oEO1) | Faithfulness, relevance |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| [MLflow.metrics](https://oreil.ly/2oEO1) | 忠实度，相关性 |'
- en: '| [LangChain Criteria Evaluation](https://oreil.ly/R1sCz) | Conciseness, relevance,
    correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality,
    misogyny, insensitivity, criminality |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| [LangChain Criteria Evaluation](https://oreil.ly/R1sCz) | 简洁性，相关性，正确性，连贯性，有害性，恶意性，帮助性，争议性，性别歧视，不敏感，犯罪性
    |'
- en: '| [Ragas](https://oreil.ly/5T3ey) | Faithfulness, answer relevance |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| [Ragas](https://oreil.ly/5T3ey) | 忠实度，答案相关性 |'
- en: It’s essential to remember that AI as a judge criteria aren’t standardized.
    Azure AI Studio’s relevance scores might be very different from MLflow’s relevance
    scores. These scores depend on the judge’s underlying model and prompt.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 必须记住，作为评判标准的AI并没有标准化。Azure AI Studio的相关性评分可能与MLflow的相关性评分大相径庭。这些评分取决于评判者底层的模型和提示。
- en: 'How to prompt an AI judge is similar to how to prompt any AI application. In
    general, a judge’s prompt should clearly explain the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如何提示AI评判者与如何提示任何AI应用类似。一般来说，评判者的提示应该清楚地解释以下内容：
- en: The task the model is to perform, such as to evaluate the relevance between
    a generated answer and the question.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型需要执行的任务，例如评估生成的答案与问题之间的相关性。
- en: The criteria the model should follow to evaluate, such as “Your primary focus
    should be on determining whether the generated answer contains sufficient information
    to address the given question according to the ground truth answer”. The more
    detailed the instruction, the better.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型在评估时应遵循的标准，例如“你的主要关注点应该是确定生成的答案是否包含足够的信息，根据真实答案来回答给定的问题”。指令越详细，越好。
- en: 'The scoring system, which can be one of these:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评分系统可以是以下之一：
- en: Classification, such as good/bad or relevant/irrelevant/neutral.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类，例如好/坏或相关/不相关/中性。
- en: Discrete numerical values, such as 1 to 5\. Discrete numerical values can be
    considered a special case of classification, where each class has a numerical
    interpretation instead of a semantic interpretation.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散数值，例如1到5。离散数值可以被视为分类的特殊情况，其中每个类别都有一个数值解释而不是语义解释。
- en: Continuous numerical values, such as between 0 and 1, e.g., when you want to
    evaluate the degree of similarity.
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续数值，例如介于0到1之间，例如当你想评估相似度程度时。
- en: Tip
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Language models are generally better with text than with numbers. It’s been
    reported that AI judges work better with classification than with numerical scoring
    systems.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型在处理文本方面通常比处理数字更好。据报道，AI评判者在分类任务上比在数值评分系统上表现更好。
- en: For numerical scoring systems, discrete scoring seems to work better than continuous
    scoring. Empirically, the wider the range for discrete scoring, the worse the
    model seems to get. Typical discrete scoring systems are between 1 and 5.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值评分系统，离散评分似乎比连续评分更有效。经验上，离散评分的范围越广，模型似乎表现得越差。典型的离散评分系统是在1到5之间。
- en: Prompts with examples have been shown to perform better. If you use a scoring
    system between 1 and 5, include examples of what a response with a score of 1,
    2, 3, 4, or 5 looks like, and if possible, why a response receives a certain score.
    Best practices for prompting are discussed in [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 包含示例的提示已被证明表现更好。如果你使用1到5的评分系统，包括1、2、3、4或5分的响应示例，如果可能的话，解释为什么一个响应会得到某个分数。关于提示的最佳实践在第5章[第5章](ch05.html#ch05a_prompt_engineering_1730156991195551)中讨论。
- en: Here’s part of the prompt used for the criteria [*relevance*](https://oreil.ly/Hlkax)
    by Azure AI Studio. It explains the task, the criteria, the scoring system, an
    example of an input with a low score, and a justification for why this input has
    a low score. Part of the prompt was removed for brevity.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是Azure AI Studio用于标准[*相关性*](https://oreil.ly/Hlkax)的部分提示。它解释了任务、标准、评分系统、一个得分低的输入示例以及为什么这个输入得分低的原因。为了简洁，部分提示已被删除。
- en: '[PRE4]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Figure 3-8](#ch03a_figure_8_1730150757025107) shows an example of an AI judge
    that evaluates the quality of an answer when given the question.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-8](#ch03a_figure_8_1730150757025107) 展示了一个示例，展示了当给出问题时，一个AI评委如何评估答案的质量。'
- en: '![A diagram of a question'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个问题的图示'
- en: Description automatically generated](assets/aien_0308.png)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0308.png)
- en: Figure 3-8\. An example of an AI judge that evaluates the quality of an answer
    given a question.
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8\. 给定问题时评估答案质量的AI评委示例。
- en: An AI judge is not just a model—it’s a system that includes both a model and
    a prompt. Altering the model, the prompt, or the model’s sampling parameters results
    in a different judge.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: AI评委不仅是一个模型——它是一个包括模型和提示的系统。改变模型、提示或模型的采样参数会导致不同的评委。
- en: Limitations of AI as a Judge
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI作为评委的局限性
- en: Despite the many advantages of AI as a judge, many teams are hesitant to adopt
    this approach. Using AI to evaluate AI seems tautological. The probabilistic nature
    of AI makes it seem too unreliable to act as an evaluator. AI judges can potentially
    introduce nontrivial costs and latency to an application. Given these limitations,
    some teams see AI as a judge as a fallback option when they don’t have any other
    way of evaluating their systems, especially in production.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AI作为评委有许多优点，但许多团队都犹豫不决地采用这种方法。使用AI来评估AI似乎是一种同义反复。AI的概率性质使其看起来作为评估者过于不可靠。AI评委可能会给应用程序带来非平凡的代价和延迟。鉴于这些局限性，一些团队将AI作为评委视为当没有其他评估系统的方法时的备选方案，尤其是在生产环境中。
- en: Inconsistency
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不一致性
- en: For an evaluation method to be trustworthy, its results should be consistent.
    Yet AI judges, like all AI applications, are probabilistic. The same judge, on
    the same input, can output different scores if prompted differently. Even the
    same judge, prompted with the same instruction, can output different scores if
    run twice. This inconsistency makes it hard to reproduce or trust evaluation results.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 要使评估方法值得信赖，其结果应该是一致的。然而，AI评委，就像所有AI应用一样，是概率性的。同一个评委，在相同的输入下，如果被不同地提示，可能会输出不同的分数。即使是同一个评委，如果被相同的指令提示，如果运行两次，也可能输出不同的分数。这种不一致性使得重现或信任评估结果变得困难。
- en: It’s possible to get an AI judge to be more consistent. [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359)
    discusses how to do so with sampling variables. [Zheng et al. (2023)](https://arxiv.org/abs/2306.05685)
    showed that including evaluation examples in the prompt can increase the consistency
    of GPT-4 from 65% to 77.5%. However, they acknowledged that high consistency may
    not imply high accuracy—the judge might consistently make the same mistakes. On
    top of that, including more examples makes prompts longer, and longer prompts
    mean higher inference costs. In Zheng et al.’s experiment, including more examples
    in their prompts caused their GPT-4 spending to quadruple.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能使AI评委更加一致。[第2章](ch02.html#ch02_understanding_foundation_models_1730147895571359)讨论了如何通过采样变量来实现这一点。[Zheng等人（2023）](https://arxiv.org/abs/2306.05685)表明，在提示中包含评估示例可以将GPT-4的一致性从65%提高到77.5%。然而，他们承认，高一致性并不一定意味着高精度——评委可能会持续犯同样的错误。此外，包含更多示例会使提示更长，而更长的提示意味着更高的推理成本。在Zheng等人的实验中，在他们的提示中包含更多示例导致他们的GPT-4花费翻了两番。
- en: Criteria ambiguity
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准模糊性
- en: Unlike many human-designed metrics, AI as a judge metrics aren’t standardized,
    making it easy to misinterpret and misuse them. As of this writing, the open source
    tools MLflow, Ragas, and LlamaIndex all have the built-in criterion *faithfulness*
    to measure how faithful a generated output is to the given context, but their
    instructions and scoring systems are all different. As shown in [Table 3-4](#ch03a_table_4_1730150757038100),
    MLflow uses a scoring system from 1 to 5, Ragas uses 0 and 1, whereas LlamaIndex’s
    prompt asks the judge to output YES and NO.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多人为设计的指标不同，作为评判指标的AI并没有标准化，这使得它们容易被误解和误用。截至本文写作时，开源工具MLflow、Ragas和LlamaIndex都内置了用于衡量生成的输出与给定上下文一致性的标准*忠诚度*，但它们的说明和评分系统各不相同。如[表3-4](#ch03a_table_4_1730150757038100)所示，MLflow使用1到5的评分系统，Ragas使用0和1，而LlamaIndex的提示要求评判者输出YES和NO。
- en: Table 3-4\. Different tools can have very difficult default prompts for the
    same criteria.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-4。不同的工具对于同一标准可能有非常困难的默认提示。
- en: '| Tool | Prompt [partially omitted for brevity] | Scoring system |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 工具 | 提示[部分省略以节省篇幅] | 评分系统 |'
- en: '| --- | --- | --- |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [MLflow](https://github.com/mlflow/mlflow/blob/5cdae7c4321015620032d02a3b84fb6127247392/mlflow/metrics/genai/prompts/v1.py)
    | `Faithfulness is only evaluated with the provided output and provided context,
    please ignore the provided input entirely when scoring faithfulness.` `Faithfulness`
    `assesses how much of the` `provided` `output is factually consistent with the
    provided context.…`'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '| [MLflow](https://github.com/mlflow/mlflow/blob/5cdae7c4321015620032d02a3b84fb6127247392/mlflow/metrics/genai/prompts/v1.py)
    | `忠诚度仅通过提供的输出和提供的上下文进行评估，请在评分忠诚度时完全忽略提供的输入。` `忠诚度` `评估提供的输出中有多少与提供的上下文在事实上是一致的。……`'
- en: '`Faithfulness: Below are the details for different scores:``- Score 1: None
    of the claims in the output can be inferred from the provided` `context.``- Score
    2: …` | 1–5 |'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`忠诚度：以下为不同分数的详细信息：``- 分数1：输出中的所有主张都无法从提供的` `上下文中推断出来。``- 分数2：……` | 1–5 |'
- en: '| [Ragas](https://github.com/explodinggradients/ragas/blob/b276f59c0d4eb4795dc28966bfbce14d5aacd140/src/ragas/metrics/_faithfulness.py#L93C1-L94C1)
    | `Your task is to judge the faithfulness of a series of statements based on a
    given context. For each statement you must return verdict as 1 if the` `statement`
    `can be verified based on the context or 0 if the statement can not be verified
    based on the context.` | 0 and 1 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| [Ragas](https://github.com/explodinggradients/ragas/blob/b276f59c0d4eb4795dc28966bfbce14d5aacd140/src/ragas/metrics/_faithfulness.py#L93C1-L94C1)
    | `你的任务是判断一系列陈述的忠诚度，基于给定的上下文。对于每个陈述，你必须返回1，如果陈述可以根据上下文验证，或者返回0，如果陈述不能根据上下文验证。`
    | 0和1 |'
- en: '| [LlamaIndex](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/evaluation/faithfulness.py)
    | `Please tell if a given piece of information is supported by the context.``You
    need to answer with either YES or NO.``Answer YES if any of the context` `supports`
    `the information, even if most of the context is unrelated. Some examples are`
    `provided` `below.` `Information: Apple pie is generally double-crusted.``Context:
    An apple pie is a fruit pie… It is generally double-crusted, with` `pastry` `both
    above and below the filling ...``Answer: YES` | YES and NO |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| [LlamaIndex](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/evaluation/faithfulness.py)
    | `请告知给定信息是否由上下文支持。``你需要回答YES或NO。``如果上下文中的任何部分支持信息，即使大部分上下文与信息无关，也要回答YES。以下提供了一些示例。`
    `信息：苹果派通常是双层皮的。``上下文：苹果派是一种水果派……它通常是双层皮的，` `面皮` `在填充物上方和下方。``答案：YES` | YES和NO
    |'
- en: The faithfulness scores outputted by these three tools won’t be comparable.
    If, given a (context, answer) pair, MLflow gives a faithfulness score of 3, Ragas
    outputs 1, and LlamaIndex outputs NO, which score would you use?
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个工具输出的忠诚度分数不可比较。如果给定一个（上下文，答案）对，MLflow给出忠诚度分数为3，Ragas输出1，LlamaIndex输出NO，你会使用哪个分数？
- en: An application evolves over time, but the way it’s evaluated ideally should
    be fixed. This way, evaluation metrics can be used to monitor the application’s
    changes. However, AI judges are also AI applications, which means that they also
    can change over time.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一个应用程序会随着时间的推移而发展，但理想的评估方式应该是固定的。这样，评估指标就可以用来监控应用程序的变化。然而，AI评判者也是AI应用程序，这意味着它们也可能随着时间的推移而变化。
- en: Imagine that last month, your application’s coherence score was 90%, and this
    month, this score is 92%. Does this mean that your application’s coherence has
    improved? It’s hard to answer this question unless you know for sure that the
    AI judges used in both cases are exactly the same. What if the judge’s prompt
    this month is different from the one last month? Maybe you switched to a slightly
    better-performing prompt or a coworker fixed a typo in last month’s prompt, and
    the judge this month is more lenient.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，上个月你的应用程序的连贯性得分为90%，而这个月这个得分是92%。这难道意味着你的应用程序的连贯性有所提高吗？除非你确定在这两种情况下使用的AI评委完全相同，否则很难回答这个问题。如果这个月的评委提示与上个月的不同怎么办？也许你切换到了一个表现略好的提示，或者同事在上个月的提示中纠正了一个错误，而这个月的评委更加宽容。
- en: This can become especially confusing if the application and the AI judge are
    managed by different teams. The AI judge team might change the judges without
    informing the application team. As a result, the application team might mistakenly
    attribute the changes in the evaluation results to changes in the application,
    rather than the changes in the judges.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序和AI评委由不同的团队管理，这可能会变得特别令人困惑。AI评委团队可能会在不通知应用程序团队的情况下更改评委。结果，应用程序团队可能会错误地将评估结果的变化归因于应用程序的变化，而不是评委的变化。
- en: Tip
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Do not trust any AI judge if you can’t see the model and the prompt used for
    the judge.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看不到用于评委的模型和提示，不要信任任何AI评委。
- en: Evaluation methods take time to standardize. As the field evolves and more guardrails
    are introduced, I hope that future AI judges will become a lot more standardized
    and reliable.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 评估方法需要时间标准化。随着该领域的演变和更多安全措施的实施，我希望未来的AI评委将变得更加标准化和可靠。
- en: Increased costs and latency
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成本和延迟增加
- en: You can use AI judges to evaluate applications both during experimentation and
    in production. Many teams use AI judges as guardrails in production to reduce
    risks, showing users only generated responses deemed good by the AI judge.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在实验和生产过程中使用AI评委来评估应用程序。许多团队在生产中使用AI评委作为安全措施来降低风险，只向用户展示AI评委认为好的生成响应。
- en: Using powerful models to evaluate responses can be expensive. If you use GPT-4
    to both generate and evaluate responses, you’ll do twice as many GPT-4 calls,
    approximately doubling your API costs. If you have three evaluation prompts because
    you want to evaluate three criteria—say, overall response quality, factual consistency,
    and toxicity—you’ll increase your number of API calls four times.^([17](ch03.html#id948))
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 使用强大的模型来评估响应可能会很昂贵。如果你使用GPT-4来生成和评估响应，你将进行两倍的GPT-4调用，大约加倍你的API成本。如果你有三个评估提示，因为你想要评估三个标准——比如，整体响应质量、事实一致性和毒性——你的API调用次数将增加四倍。[17](ch03.html#id948)
- en: 'You can reduce costs by using weaker models as the judges (see [“What Models
    Can Act as Judges?”](#ch03a_what_models_can_act_as_judges_1730150757064924).)
    You can also reduce costs with *spot-checking*: evaluating only a subset of responses.^([18](ch03.html#id949))
    Spot-checking means you might fail to catch some failures. The larger the percentage
    of samples you evaluate, the more confidence you will have in your evaluation
    results, but also the higher the costs. Finding the right balance between cost
    and confidence might take trial and error. This process is discussed further in
    [Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863). All things considered,
    AI judges are much cheaper than human evaluators.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用作为评委的较弱模型来降低成本（参见[“哪些模型可以作为评委？”](#ch03a_what_models_can_act_as_judges_1730150757064924)）。你也可以通过*抽查*来降低成本：仅评估响应的一个子集。[18](ch03.html#id949)
    抽查意味着你可能无法捕捉到一些失败。你评估的样本百分比越大，你对评估结果就越有信心，但成本也越高。在成本和信心之间找到正确的平衡可能需要尝试和错误。这个过程在[第4章](ch04.html#ch04_evaluate_ai_systems_1730130866187863)中进一步讨论。综合考虑，AI评委比人工评估员便宜得多。
- en: 'Implementing AI judges in your production pipeline can add latency. If you
    evaluate responses before returning them to users, you face a trade-off: reduced
    risk but increased latency. The added latency might make this option a nonstarter
    for applications with strict latency requirements.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产流程中实施AI评委可能会增加延迟。如果你在将响应返回给用户之前评估它们，你会面临一个权衡：降低风险但增加延迟。增加的延迟可能会使这个选项对于对延迟要求严格的应用程序来说不可行。
- en: Biases of AI as a judge
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AI评委的偏见
- en: Human evaluators have biases, and so do AI judges. Different AI judges have
    different biases. This section will discuss some of the common ones. Being aware
    of your AI judges’ biases helps you interpret their scores correctly and even
    mitigate these biases.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 人工评估者有偏见，AI裁判也是如此。不同的AI裁判有不同的偏见。本节将讨论一些常见的偏见。了解你的AI裁判的偏见有助于你正确解读其评分，甚至可以减轻这些偏见。
- en: AI judges tend to have *self-bias*, where a model favors its own responses over
    the responses generated by other models. The same mechanism that helps a model
    compute the most likely response to generate will also give this response a high
    score. In [Zheng et al.’s 2023 experiment](https://arxiv.org/abs/2306.05685),
    GPT-4 favors itself with a 10% higher win rate, while Claude-v1 favors itself
    with a 25% higher win rate.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: AI裁判倾向于存在*自我偏见*，即模型更偏好自己的回答而不是其他模型生成的回答。帮助模型计算最可能生成回答的相同机制也会给这个回答一个高分。在[Zheng等人2023年的实验](https://arxiv.org/abs/2306.05685)中，GPT-4以10%更高的胜率偏向自己，而Claude-v1以25%更高的胜率偏向自己。
- en: Many AI models have first-position bias. An AI judge may favor the first answer
    in a pairwise comparison or the first in a list of options. This can be mitigated
    by repeating the same test multiple times with different orderings or with carefully
    crafted prompts. The position bias of AI is the opposite of that of humans. Humans
    tend to favor [the answer they see last](https://oreil.ly/2XDI0)*,* which is called
    *recency bias*.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 许多AI模型存在首位置偏见。AI裁判可能更偏好成对比较中的第一个答案或列表中的第一个选项。这可以通过多次重复相同的测试并使用不同的顺序或精心设计的提示来减轻。AI的位置偏见与人类相反。人类倾向于偏好[他们最后看到的答案](https://oreil.ly/2XDI0)*,*
    这被称为*近期偏见*。
- en: Some AI judges have *verbosity bias*, favoring lengthier answers, regardless
    of their quality. [Wu and Aji (2023)](https://arxiv.org/abs/2307.03025) found
    that both GPT-4 and Claude-1 prefer longer responses (~100 words) with factual
    errors over shorter, correct responses (~50 words). [Saito et al. (2023)](https://oreil.ly/IOp9H)
    studied this bias for creative tasks and found that when the length difference
    is large enough (e.g., one response is twice as long as the other), the judge
    almost always prefers the longer one.^([19](ch03.html#id952)) Both Zheng et al.
    (2023) and Saito et al. (2023), however, discovered that GPT-4 is less prone to
    this bias than GPT-3.5, suggesting that this bias might go away as models become
    stronger.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一些AI裁判存在*冗长偏见*，倾向于偏好较长的答案，而不论其质量如何。[吴和Aji (2023)](https://arxiv.org/abs/2307.03025)发现，GPT-4和Claude-1都更偏好较长的回答（约100个单词）并包含事实错误，而不是较短的、正确的回答（约50个单词）。[Saito等人
    (2023)](https://oreil.ly/IOp9H)研究了这种偏见在创造性任务中的应用，并发现当长度差异足够大时（例如，一个回答是另一个的两倍长），裁判几乎总是偏好较长的那个.^([19](ch03.html#id952))然而，Zheng等人（2023）和Saito等人（2023）都发现，GPT-4比GPT-3.5更不容易出现这种偏见，这表明随着模型的增强，这种偏见可能会消失。
- en: On top of all these biases, AI judges have the same limitations as all AI applications,
    including privacy and IP. If you use a proprietary model as your judge, you’d
    need to send your data to this model. If the model provider doesn’t disclose their
    training data, you won’t know for sure if the judge is commercially safe to use.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些偏见之上，AI裁判与所有AI应用一样，存在相同的局限性，包括隐私和知识产权。如果你使用专有模型作为你的裁判，你需要将你的数据发送到这个模型。如果模型提供者不公开其训练数据，你将无法确定裁判是否商业上安全使用。
- en: Despite the limitations of the AI as a judge approach, its many advantages make
    me believe that its adoption will continue to grow. However, AI judges should
    be supplemented with exact evaluation methods and/or human evaluation.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管作为裁判的AI方法存在局限性，但其众多优势使我相信其应用将继续增长。然而，AI裁判应辅以精确的评价方法以及/或人工评价。
- en: What Models Can Act as Judges?
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哪些模型可以作为裁判？
- en: The judge can either be stronger, weaker, or the same as the model being judged.
    Each scenario has its pros and cons.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 裁判可以比被评估的模型更强、更弱或相同。每种情况都有其利弊。
- en: At first glance, a stronger judge makes sense. Shouldn’t the exam grader be
    more knowledgeable than the exam taker? Not only can stronger models make better
    judgments, but they can also help improve weaker models by guiding them to generate
    better responses.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，一个更强的裁判似乎是合理的。难道考试评分者不应该比考试者更有知识吗？更强的模型不仅可以做出更好的判断，还可以通过指导它们生成更好的回答来帮助改进较弱的模型。
- en: 'You might wonder: if you already have access to the stronger model, why bother
    using a weaker model to generate responses? The answer is cost and latency. You
    might not have the budget to use the stronger model to generate all responses,
    so you use it to evaluate a subset of responses. For example, you may use a cheap
    in-house model to generate responses and GPT-4 to evaluate 1% of the responses.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：如果你已经可以访问更强的模型，为什么还要费心使用较弱的模型来生成响应呢？答案是成本和延迟。你可能没有足够的预算使用更强的模型来生成所有响应，所以你用它来评估响应的一个子集。例如，你可能使用一个廉价的内部模型来生成响应，并用GPT-4来评估1%的响应。
- en: The stronger model also might be too slow for your application. You can use
    a fast model to generate responses while the stronger, but slower, model does
    evaluation in the background. If the strong model thinks that the weak model’s
    response is bad, remedy actions might be taken, such as updating the response
    with that of the strong model. Note that the opposite pattern is also common.
    You use a strong model to generate responses, with a weak model running in the
    background to do evaluation.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 更强的模型也可能对你的应用来说太慢了。你可以在更强的、但较慢的模型在后台进行评估的同时，使用快速模型生成响应。如果强模型认为弱模型的响应不好，可能会采取补救措施，例如用强模型的响应更新响应。请注意，相反的模式也很常见。你使用强模型生成响应，而弱模型在后台进行评估。
- en: Using the stronger model as a judge leaves us with two challenges. First, the
    strongest model will be left with no eligible judge. Second, we need an alternative
    evaluation method to determine which model is the strongest.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更强的模型作为评委给我们留下了两个挑战。首先，最强的模型将没有合格的评委。其次，我们需要一个替代的评估方法来确定哪个模型是最强的。
- en: 'Using a model to judge itself, *self-evaluation* or *self-critique*, sounds
    like cheating, especially because of self-bias. However, self-evaluation can be
    great for sanity checks. If a model thinks its own response is incorrect, the
    model might not be that reliable. Beyond sanity checks, asking a model to evaluate
    itself can nudge a model to revise and improve its responses ([Press et al., 2022](https://arxiv.org/abs/2210.03350);
    [Gou et al., 2023](https://arxiv.org/abs/2305.11738); [Valmeekamet et al., 2023](https://arxiv.org/abs/2310.08118)).^([20](ch03.html#id955))
    This example shows what self-evaluation might look like:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型对自己进行评判，*自我评估*或*自我批评*，听起来像是作弊，尤其是由于自我偏差。然而，自我评估对于理智检查非常有用。如果一个模型认为自己的响应是错误的，那么这个模型可能并不那么可靠。除了理智检查之外，要求模型进行自我评估可以促使模型修改和改进其响应（[Press等，2022](https://arxiv.org/abs/2210.03350)；[Gou等，2023](https://arxiv.org/abs/2305.11738)；[Valmeekamet等，2023](https://arxiv.org/abs/2310.08118)）。^([20](ch03.html#id955))
    这个例子展示了自我评估可能的样子：
- en: '[PRE5]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: One open question is whether the judge can be weaker than the model being judged.
    Some argue that judging is an easier task than generating. Anyone can have an
    opinion about whether a song is good, but not everyone can write a song. Weaker
    models should be able to judge the outputs of stronger models.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 一个开放的问题是评委是否可以比被评判的模型弱。有些人认为评判是一个比生成更容易的任务。任何人都可以对一首歌是否好发表意见，但并不是每个人都能写歌。较弱的模型应该能够评判较强模型的输出。
- en: '[Zheng et al. (2023)](https://arxiv.org/abs/2306.05685) found that stronger
    models are better correlated to human preference, which makes people opt for the
    strongest models they can afford. However, this experiment was limited to general-purpose
    judges. One research direction that I’m excited about is small, specialized judges.
    Specialized judges are trained to make specific judgments, using specific criteria
    and following specific scoring systems. A small, specialized judge can be more
    reliable than larger, general-purpose judges for specific judgments.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[郑等（2023）](https://arxiv.org/abs/2306.05685)发现更强的模型与人类偏好相关性更好，这使得人们选择他们负担得起的最强模型。然而，这个实验仅限于通用评委。我非常感兴趣的一个研究方向是小型、专业的评委。专业的评委被训练来做出特定的判断，使用特定的标准和遵循特定的评分系统。对于特定的判断，小型、专业的评委可能比大型、通用的评委更可靠。'
- en: 'Because there are many possible ways to use AI judges, there are many possible
    specialized AI judges. Here, I’ll go over examples of three specialized judges:
    reward models, reference-based judges, and preference models:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用AI评委有众多可能的方式，因此存在许多可能的专用AI评委。在这里，我将介绍三种专用评委的例子：奖励模型、基于参考的评委和偏好模型：
- en: Reward model
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励模型
- en: A reward model takes in a (prompt, response) pair and scores how good the response
    is given the prompt. Reward models have been successfully used in RLHF for many
    years. [Cappy](https://arxiv.org/abs/2311.06720) is an example of a reward model
    developed by Google (2023). Given a pair of (prompt, response), Cappy produces
    a score between 0 and 1, indicating how correct the response is. Cappy is a lightweight
    scorer with 360 million parameters, much smaller than general-purpose foundation
    models.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励模型接收一个（提示，响应）对，并评分响应根据提示有多好。奖励模型在 RLHF 中已经成功使用了多年。[Cappy](https://arxiv.org/abs/2311.06720)
    是 Google 开发的一个奖励模型的例子（2023）。给定一对（提示，响应），Cappy 生成一个介于 0 到 1 之间的分数，表示响应有多正确。Cappy
    是一个轻量级的评分器，有 3600 万个参数，比通用基础模型小得多。
- en: Reference-based judge
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 基于参考的评判
- en: A reference-based judge evaluates the generated response with respect to one
    or more reference responses. This judge can output a similarity score or a quality
    score (how good the generated response is compared to the reference responses).
    For example, BLEURT ([Sellam et al., 2020](https://arxiv.org/abs/2004.04696))
    takes in a (candidate response, reference response) pair and outputs a similarity
    score between the candidate and reference response.^([21](ch03.html#id959)) Prometheus
    ([Kim et al., 2023](https://arxiv.org/abs/2310.08491)) takes in (prompt, generated
    response, reference response, scoring rubric) and outputs a quality score between
    1 and 5, assuming that the reference response gets a 5.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 基于参考的评判者根据一个或多个参考响应来评估生成的响应。这个评判者可以输出相似度分数或质量分数（与参考响应相比生成的响应有多好）。例如，BLEURT ([Sellam
    et al., 2020](https://arxiv.org/abs/2004.04696)) 接收一个（候选响应，参考响应）对，并输出候选响应与参考响应之间的相似度分数。[21](ch03.html#id959)
    Prometheus ([Kim et al., 2023](https://arxiv.org/abs/2310.08491)) 接收（提示，生成响应，参考响应，评分标准）并输出一个介于
    1 到 5 之间的质量分数，假设参考响应得到 5 分。
- en: Preference model
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好模型
- en: A preference model takes in (prompt, response 1, response 2) as input and outputs
    which of the two responses is better (preferred by users) for the given prompt.
    This is perhaps one of the more exciting directions for specialized judges. Being
    able to predict human preference opens up many possibilities. As discussed in
    [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359),
    preference data is essential for aligning AI models to human preference, and it’s
    challenging and expensive to obtain. Having a good human preference predictor
    can generally make evaluation easier and models safer to use. There have been
    many initiatives in building preference models, including PandaLM ([Wang et al.,
    2023](https://arxiv.org/abs/2306.05087)) and JudgeLM ([Zhu et al., 2023](https://arxiv.org/abs/2310.17631)).
    [Figure 3-9](#ch03a_figure_9_1730150757025114) shows an example of how PandaLM
    works. It not only outputs which response is better but also explains its rationale.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一个偏好模型接收（提示，响应 1，响应 2）作为输入，并输出对于给定的提示，两个响应中哪一个更好（用户更偏好）。这可能是专门评判中更令人兴奋的方向之一。能够预测人类偏好开辟了许多可能性。如[第
    2 章](ch02.html#ch02_understanding_foundation_models_1730147895571359)中所述，偏好数据对于将
    AI 模型与人类偏好对齐至关重要，获取这些数据具有挑战性且成本高昂。拥有一个好的偏好预测器通常可以使评估更容易，并且模型更安全使用。已经有许多建立偏好模型的倡议，包括
    PandaLM ([Wang et al., 2023](https://arxiv.org/abs/2306.05087)) 和 JudgeLM ([Zhu
    et al., 2023](https://arxiv.org/abs/2310.17631))。[图 3-9](#ch03a_figure_9_1730150757025114)
    展示了 PandaLM 的工作示例。它不仅输出哪个响应更好，还解释了其推理依据。
- en: '![A diagram of a diagram'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图表的图表'
- en: Description automatically generated](assets/aien_0309.png)
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0309.png)
- en: Figure 3-9\. An example output of PandaLM, given a human prompt and two generated
    responses. Picture from Wang et al. (2023), modified slightly for readability.
    The original image is available under the Apache License 2.0.
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-9\. 给定人类提示和两个生成响应的 PandaLM 的一个示例输出。图片来自 Wang et al. (2023)，略有修改以增强可读性。原始图像可在
    Apache License 2.0 许可下获得。
- en: Despite its limitations, the AI as a judge approach is versatile and powerful.
    Using cheaper models as judges makes it even more useful. Many of my colleagues,
    who were initially skeptical, have started to rely on it more in production.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有其局限性，但作为评判者的 AI 方法具有多样性和强大的功能。使用更便宜的模型作为评判者使其更加有用。许多最初持怀疑态度的同事已经开始在生产中更多地依赖它。
- en: AI as a judge is exciting, and the next approach we’ll discuss is just as intriguing.
    It’s inspired by game design, a fascinating field..
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 作为评判者的 AI 非常吸引人，我们将讨论的下一种方法同样引人入胜。它受到游戏设计的启发，这是一个迷人的领域。
- en: Ranking Models with Comparative Evaluation
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于比较评估的排名模型
- en: Often, you evaluate models not because you care about their scores, but because
    you want to know which model is the best for you. What you want is a ranking of
    these models. You can rank models using either pointwise evaluation or comparative
    evaluation.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您评估模型并不是因为您关心它们的分数，而是因为您想知道哪个模型最适合您。您想要的是这些模型的排名。您可以使用点评分评估或比较评估来对模型进行排名。
- en: With pointwise evaluation, you evaluate each model independently,^([22](ch03.html#id965))
    then rank them by their scores. For example, if you want to find out which dancer
    is the best, you evaluate each dancer individually, give them a score, then pick
    the dancer with the highest score.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在点评分评估中，您独立评估每个模型，^([22](ch03.html#id965))然后根据它们的分数进行排名。例如，如果您想找出哪个舞者是最佳的，您可以单独评估每个舞者，给他们打分，然后选择得分最高的舞者。
- en: With comparative evaluation, you evaluate models against each other and compute
    a ranking from comparison results. For the same dancing contest, you can ask all
    candidates to dance side-by-side and ask the judges which candidate’s dancing
    they like the most, and pick the dancer preferred by most judges.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较评估，您可以将模型相互比较，并从比较结果中计算出一个排名。对于同一个舞蹈比赛，您可以要求所有候选人并肩跳舞，并询问评委他们最喜欢哪个候选人的舞蹈，然后选择大多数评委偏好的舞者。
- en: For responses whose quality is subjective, comparative evaluation is typically
    easier to do than pointwise evaluation. For example, it’s easier to tell which
    song of the two songs is better than to give each song a concrete score.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 对于主观质量的主观回应，比较评估通常比点评分评估更容易进行。例如，判断哪首歌比另一首歌更好，比给每首歌一个具体的分数要容易。
- en: In AI, comparative evaluation was first used in 2021 by [Anthropic](https://arxiv.org/abs/2112.00861)
    to rank different models. It also powers the popular LMSYS’s [Chatbot Arena](https://oreil.ly/MHt5H)
    leaderboard that ranks models using scores computed from pairwise model comparisons
    from the community.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI中，比较评估首次于2021年由[Anthropic](https://arxiv.org/abs/2112.00861)用于对不同模型进行排名。它还支持流行的LMSYS的[聊天机器人竞技场](https://oreil.ly/MHt5H)排行榜，该排行榜使用从社区中计算出的成对模型比较的分数来对模型进行排名。
- en: Many model providers use comparative evaluation to evaluate their models in
    production. [Figure 3-10](#ch03a_figure_10_1730150757025123) shows an example
    of ChatGPT asking its users to compare two outputs side by side. These outputs
    could be generated by different models, or by the same model with different sampling
    variables.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型提供商在生产中使用比较评估来评估他们的模型。[图3-10](#ch03a_figure_10_1730150757025123)展示了ChatGPT要求用户对比两个输出并排的例子。这些输出可能由不同的模型生成，或者由同一个模型使用不同的采样变量生成。
- en: '![A screenshot of a dictionary'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '![字典的截图'
- en: Description automatically generated](assets/aien_0310.png)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0310.png)
- en: Figure 3-10\. ChatGPT occasionally asks users to compare two outputs side by
    side.
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10。ChatGPT偶尔会要求用户对比两个输出并排。
- en: For each request, two or more models are selected to respond. An evaluator,
    which can be human or AI, picks the winner. Many developers allow for ties to
    avoid a winner being picked at random when drafts are equally good or bad.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个请求，选择两个或更多模型进行回应。评估者，可以是人类或AI，选择获胜者。许多开发者允许平局，以避免在草稿同样好或坏时随机选择获胜者。
- en: A very important thing to keep in mind is that *not all questions should be
    answered by preference*. Many questions should be answered by correctness instead.
    Imagine asking the model “Is there a link between cell phone radiation and brain
    tumors?” and the model presents two options, “Yes” and “No”, for you to choose
    from. Preference-based voting can lead to wrong signals that, if used to train
    your model, can result in misaligned behaviors.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常重要的事情是要记住，*并非所有问题都应该通过偏好来回答*。许多问题应该通过正确性来回答。想象一下，你问模型“手机辐射和脑瘤之间有联系吗？”而模型为你提供了两个选项，“是”和“否”，让你选择。基于偏好的投票可能会产生错误的信号，如果用于训练你的模型，可能会导致行为错位。
- en: Asking users to pick can also cause user frustration. Imagine asking the model
    a math question because you don’t know the answer, and the model gives you two
    different answers and asks you to pick the one you prefer. If you had known the
    right answer, you wouldn’t have asked the model in the first place.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 要求用户进行选择也可能导致用户感到沮丧。想象一下，因为你不知道答案，所以向模型提出一个数学问题，而模型给出了两个不同的答案，并要求你选择你更喜欢的一个。如果你知道正确答案，你最初就不会向模型提问。
- en: When collecting comparative feedback from users, one challenge is to determine
    what questions can be determined by preference voting and what shouldn’t be. Preference-based
    voting only works if the voters are knowledgeable in the subject. This approach
    generally works in applications where AI serves as an intern or assistant, helping
    users speed up tasks they know how to do—and not where users ask AI to perform
    tasks they themselves don’t know how to do.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集用户比较反馈时，一个挑战是确定哪些问题可以通过偏好投票来确定，哪些不应该。基于偏好的投票只有在投票者对主题有知识时才有效。这种方法通常适用于AI作为实习生或助手的应用，帮助用户加快他们知道如何完成的任务——而不是用户要求AI执行他们自己不知道如何完成的任务。
- en: Comparative evaluation shouldn’t be confused with A/B testing. In A/B testing,
    a user sees the output from one candidate model at a time. In comparative evaluation,
    a user sees outputs from multiple models at the same time.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 比较评估不应与A/B测试混淆。在A/B测试中，用户一次只能看到候选模型的一个输出。在比较评估中，用户同时看到多个模型的输出。
- en: Each comparison is called a *match*. This process results in a series of comparisons,
    as shown in [Table 3-5](#ch03a_table_5_1730150757038107).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 每次比较称为**匹配**。这个过程产生了一系列比较，如[表3-5](#ch03a_table_5_1730150757038107)所示。
- en: Table 3-5\. Examples of a history of pairwise model comparisons.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-5\. 双边模型比较历史的示例。
- en: '| Match # | Model A | Model B | Winner |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 匹配编号 | 模型A | 模型B | 胜者 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | Model 1 | Model 2 | Model 1 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 模型1 | 模型2 | 模型1 |'
- en: '| 2 | Model 3 | Model 10 | Model 10 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 模型3 | 模型10 | 模型10 |'
- en: '| 3 | Model 7 | Model 4 | Model 4 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 模型7 | 模型4 | 模型4 |'
- en: '| … |  |  |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| … |  |  |  |'
- en: The probability that model A is preferred over model B is the *win rate* of
    A over B. We can compute this win rate by looking at all matches between A and
    B and calculating the percentage in which A wins.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 模型A相对于模型B被优先选择的概率是A相对于B的**胜率**。我们可以通过查看A和B之间的所有比赛，并计算A获胜的百分比来计算这个胜率。
- en: If there are only two models, ranking them is straightforward. The model that
    wins more often ranks higher. The more models there are, the more challenging
    ranking becomes. Let’s say that we have five models with the empirical win rates
    between model pairs, as shown in [Table 3-6](#ch03a_table_6_1730150757038117).
    It’s not obvious, from looking at the data, how these five models should be ranked.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只有两个模型，排名它们是直接的。获胜次数更多的模型排名更高。模型越多，排名就越具挑战性。假设我们有五个模型，它们之间的经验胜率如[表3-6](#ch03a_table_6_1730150757038117)所示。从数据中看，这些五个模型应该如何排名并不明显。
- en: Table 3-6\. Example win rates of five models. The A >> B column denotes the
    event that A is preferred to B.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-6\. 五个模型的示例胜率。A >> B列表示A相对于B被优先选择的事件。
- en: '| Model pair # | Model A | Model B | # matches | A >> B |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 模型对编号 | 模型A | 模型B | 匹配次数 | A >> B |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | Model 1 | Model 2 | 1000 | 90% |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 模型1 | 模型2 | 1000 | 90% |'
- en: '| 2 | Model 1 | Model 3 | 1000 | 40% |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 模型1 | 模型3 | 1000 | 40% |'
- en: '| 3 | Model 1 | Model 4 | 1000 | 15% |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 模型1 | 模型4 | 1000 | 15% |'
- en: '| 4 | Model 1 | Model 5 | 1000 | 10% |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 模型1 | 模型5 | 1000 | 10% |'
- en: '| 5 | Model 2 | Model 3 | 1000 | 60% |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 模型2 | 模型3 | 1000 | 60% |'
- en: '| 6 | Model 2 | Model 4 | 1000 | 80% |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 模型2 | 模型4 | 1000 | 80% |'
- en: '| 7 | Model 2 | Model 5 | 1000 | 80% |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 模型2 | 模型5 | 1000 | 80% |'
- en: '| 8 | Model 3 | Model 4 | 1000 | 70% |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 模型3 | 模型4 | 1000 | 70% |'
- en: '| 9 | Model 3 | Model 5 | 1000 | 10% |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 模型3 | 模型5 | 1000 | 10% |'
- en: '| 10 | Model 4 | Model 5 | 1000 | 20% |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 模型4 | 模型5 | 1000 | 20% |'
- en: Given comparative signals, a *rating algorithm* is then used to compute a ranking
    of models. Typically, this algorithm first computes a score for each model from
    the comparative signals and then ranks models by their scores.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 给定比较信号后，随后使用**评分算法**来计算模型的排名。通常，该算法首先从比较信号中为每个模型计算一个分数，然后根据分数对模型进行排名。
- en: Comparative evaluation is new in AI but has been around for almost a century
    in other industries. It’s especially popular in sports and video games. Many rating
    algorithms developed for these other domains can be adapted to evaluating AI models,
    such as Elo, Bradley–Terry, and TrueSkill. LMSYS’s Chatbot Arena originally used
    Elo to compute models’ ranking but later switched to the Bradley–Terry algorithm
    because they found Elo sensitive to the order of evaluators and prompts.^([23](ch03.html#id970))
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 比较评估在人工智能领域是新的，但在其他行业中已经存在了近一个世纪。它在体育和电子游戏中尤其受欢迎。为这些其他领域开发的许多评分算法可以适应评估人工智能模型，例如Elo、Bradley–Terry和TrueSkill。LMSYS的聊天机器人竞技场最初使用Elo来计算模型的排名，但后来切换到Bradley–Terry算法，因为他们发现Elo对评估者和提示的顺序敏感.^([23](ch03.html#id970))
- en: '*A ranking is correct if, for any model pair, the higher-ranked model is more
    likely to win in a match against the lower-ranked model*. If model A ranks higher
    than model B, users should prefer model A to model B more than half the time.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果一个排名是正确的，那么对于任何模型对，排名更高的模型在比赛中赢得排名较低的模型的可能性更大**。如果模型A的排名高于模型B，那么用户应该有超过一半的时间更喜欢模型A而不是模型B。'
- en: Through this lens, model ranking is a predictive problem. We compute a ranking
    from historical match outcomes and use it to predict future match outcomes. Different
    ranking algorithms can produce different rankings, and there’s no ground truth
    for what the correct ranking is. The quality of a ranking is determined by how
    good it is in predicting future match outcomes. My analysis of Chatbot Arena’s
    ranking shows that the produced ranking is good, at least for model pairs with
    sufficient matches. See the book’s [GitHub repo](https://github.com/chiphuyen/aie-book)
    for the analysis.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个视角，模型排名是一个预测问题。我们通过计算历史比赛结果来得出排名，并使用它来预测未来的比赛结果。不同的排名算法可以产生不同的排名，而且没有关于正确排名的绝对真理。排名的质量取决于它在预测未来比赛结果方面的好坏。我对聊天机器人竞技场的排名分析表明，产生的排名是好的，至少对于有足够匹配的模型对来说是这样。请参阅本书的[GitHub仓库](https://github.com/chiphuyen/aie-book)以获取分析。
- en: Challenges of Comparative Evaluation
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较评估的挑战
- en: With pointwise evaluation, the heavy-lifting part of the process is in designing
    the benchmark and metrics to gather the right signals. Computing scores to rank
    models is easy. With comparative evaluation, both signal gathering and model ranking
    are challenging. This section goes over the three common challenges of comparative
    evaluation.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在点值评估中，过程中的繁重工作在于设计基准和指标以收集正确的信号。计算分数以排名模型是容易的。在比较评估中，信号收集和模型排名都具有挑战性。本节将介绍比较评估的三个常见挑战。
- en: Scalability bottlenecks
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可扩展性瓶颈
- en: Comparative evaluation is data-intensive. The number of model pairs to compare
    grows quadratically with the number of models. In January 2024, LMSYS evaluated
    57 models using 244,000 comparisons. Even though this sounds like a lot of comparisons,
    this averages only 153 comparisons per model pair (57 models correspond to 1,596
    model pairs). This is a small number, considering the wide range of tasks we want
    a foundation model to do.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 比较评估数据密集。要比较的模型对数量与模型数量的平方成正比。2024年1月，LMSYS使用244,000次比较评估了57个模型。尽管这听起来像很多比较，但这平均每个模型对只有153次比较（57个模型对应1,596个模型对）。考虑到我们希望基础模型完成的广泛任务，这仍然是一个小数字。
- en: Fortunately, we don’t always need direct comparisons between two models to determine
    which one is better. Ranking algorithms typically assume *transitivity*. If model
    A ranks higher than B, and B ranks higher than C, then with transitivity, you
    can infer that A ranks higher than C. This means that if the algorithm is certain
    that A is better than B and B is better than C, it doesn’t need to compare A against
    C to know that A is better.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们并不总是需要直接比较两个模型来确定哪个更好。排名算法通常假设**传递性**。如果模型A的排名高于B，而B的排名高于C，那么根据传递性，你可以推断出A的排名高于C。这意味着如果算法确定A比B好，而B比C好，那么它不需要比较A和C就能知道A更好。
- en: However, it’s unclear if this transitivity assumption holds for AI models. Many
    papers that analyze Elo for AI evaluation cite transitivity assumption as a limitation
    ([Boubdir et al.](https://arxiv.org/abs/2311.17295); [Balduzzi et al.](https://arxiv.org/abs/1806.02643);
    and [Munos et al.](https://arxiv.org/abs/2312.00886)). They argued that human
    preference is not necessarily transitive. In addition, non-transitivity can happen
    because different model pairs are evaluated by different evaluators and on different
    prompts.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尚不清楚这个传递性假设是否适用于AI模型。许多分析Elo用于AI评估的论文将传递性假设作为局限性引用([Boubdir等人](https://arxiv.org/abs/2311.17295);
    [Balduzzi等人](https://arxiv.org/abs/1806.02643); 以及 [Munos等人](https://arxiv.org/abs/2312.00886))。他们认为人类偏好不一定具有传递性。此外，由于不同的模型对由不同的评估者在不同的提示下进行评估，非传递性也可能发生。
- en: There’s also the challenge of evaluating new models. With independent evaluation,
    only the new model needs to be evaluated. With comparative evaluation, the new
    model has to be evaluated against existing models, which can change the ranking
    of existing models.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 评估新模型也存在挑战。在独立评估中，只需要评估新模型。在对比评估中，新模型必须与现有模型进行比较，这可能会改变现有模型的排名。
- en: This also makes it hard to evaluate private models. Imagine you’ve built a model
    for your company, using internal data. You want to compare this model with public
    models to decide whether it would be more beneficial to use a public one. If you
    want to use comparative evaluation for your model, you’ll likely have to collect
    your own comparative signals and create your own leaderboard or pay one of those
    public leaderboards to run private evaluation for you.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这也使得评估私有模型变得困难。想象一下，你为你的公司构建了一个模型，使用了内部数据。你想要将这个模型与公开模型进行比较，以决定是否使用公开模型更有利。如果你想对你的模型进行对比评估，你很可能会不得不收集自己的对比信号并创建自己的排行榜，或者支付给那些公开排行榜之一以为你进行私有评估。
- en: The scaling bottleneck can be mitigated with better matching algorithms. So
    far, we’ve assumed that models are selected randomly for each match, so all model
    pairs appear in approximately the same number of matches. However, not all model
    pairs need to be equally compared. Once we’re confident about the outcome of a
    model pair, we can stop matching them against each other. An efficient matching
    algorithm should sample matches that reduce the most uncertainty in the overall
    ranking.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 通过更好的匹配算法可以缓解扩展瓶颈。到目前为止，我们假设每个匹配中模型都是随机选择的，因此所有模型对在匹配中出现的大致数量相同。然而，并非所有模型对都需要进行同等比较。一旦我们对模型对的结果有信心，我们就可以停止将它们相互匹配。一个高效的匹配算法应该采样那些能最大程度减少整体排名不确定性的匹配。
- en: Lack of standardization and quality control
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺乏标准化和质量控制
- en: One way to collect comparative signals is to crowdsource comparisons to the
    community the way LMSYS Chatbot Arena does. Anyone can go to [the website](https://oreil.ly/td_MY),
    enter a prompt, get back two responses from two anonymous models, and vote for
    the better one. Only after voting is done are the model names revealed.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 收集对比信号的一种方式是通过像LMSYS Chatbot Arena那样从社区中众包比较。任何人都可以访问[网站](https://oreil.ly/td_MY)，输入一个提示，从两个匿名模型那里获得两个回应，并为更好的一个投票。只有在投票完成后，才会揭示模型名称。
- en: The benefit of this approach is that it captures a wide range of signals and
    is relatively difficult to game.^([24](ch03.html#id976)) However, the downside
    is that it’s hard to enforce standardization and quality control.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是它捕捉了广泛范围的信号，并且相对难以操纵。[24](ch03.html#id976) 然而，缺点是难以强制执行标准化和质量控制。
- en: First, anyone with internet access can use any prompt to evaluate these models,
    and there’s no standard on what should constitute a better response. It might
    be a lot to expect volunteers to fact-check the responses, so they might unknowingly
    prefer responses that sound better but are factually incorrect.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，任何有互联网接入的人都可以使用任何提示来评估这些模型，并且没有标准来规定什么应该构成更好的回应。可能期望志愿者进行事实核查回应是相当多的，因此他们可能无意中更喜欢听起来更好但实际上是错误的回应。
- en: Some people might prefer polite and moderate responses, while others might prefer
    responses without a filter. This is both good and bad. It’s good because it helps
    capture human preference in the wild. It’s bad because human preference in the
    wild might not be appropriate for all use cases. For example, if a user asks a
    model to tell an inappropriate joke and a model refuses, the user might downvote
    it. However, as an application developer, you might prefer that the model refuses.
    Some users might even maliciously pick the toxic responses as the preferred ones,
    polluting the ranking.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能更喜欢礼貌和适度的回应，而有些人可能更喜欢未经过滤的回应。这既是好事也是坏事。好事在于它有助于捕捉现实世界中的人类偏好。坏事在于现实世界中的人类偏好可能不适合所有用例。例如，如果用户要求模型讲一个不恰当的笑话而模型拒绝，用户可能会给它差评。然而，作为一个应用程序开发者，你可能会更喜欢模型拒绝。一些用户甚至可能恶意选择有毒的回应作为首选，从而污染排名。
- en: Second, crowdsourcing comparisons require users to evaluate models outside of
    their working environments. Without real-world grounding, test prompts might not
    reflect how these models are being used in the real world. People might just use
    the first prompts that come to mind and are unlikely to use sophisticated prompting
    techniques.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，众包比较需要用户在他们的工作环境之外评估模型。如果没有现实世界的依据，测试提示可能无法反映这些模型在现实世界中的使用情况。人们可能会只使用首先想到的提示，而不太可能使用复杂的提示技术。
- en: Among [33,000 prompts](https://oreil.ly/eI9Vq) published by LMSYS Chatbot Arena
    in 2023, 180 of them are “hello” and “hi”, which account for 0.55% of the data,
    and this doesn’t yet count variations like “hello!”, “hello.”, “hola”, “hey”,
    and so on. There are many brainteasers. The question “X has 3 sisters, each has
    a brother. How many brothers does X have?” was asked 44 times.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年LMSYS Chatbot Arena发布的[33,000个提示](https://oreil.ly/eI9Vq)中，其中180个是“hello”和“hi”，占数据的0.55%，而且这还不包括“hello!”、“hello.”、“hola”、“hey”等变体。有很多脑筋急转弯。问句“X有3个姐妹，每个姐妹都有一个兄弟。X有多少个兄弟？”被问过44次。
- en: Simple prompts are easy to respond to, making it hard to differentiate models’
    performance. Evaluating models using too many simple prompts can pollute the ranking.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的提示很容易回应，这使得很难区分模型的性能。使用过多的简单提示来评估模型可能会污染排名。
- en: If a public leaderboard doesn’t support sophisticated context construction,
    such as augmenting the context with relevant documents retrieved from your internal
    databases, its ranking won’t reflect how well a model might work for your RAG
    system. The ability to generate good responses is different from the ability to
    retrieve the most relevant documents.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如果公共排行榜不支持复杂的上下文构建，例如使用从您内部数据库检索的相关文档来增强上下文，那么其排名将无法反映模型对您的RAG系统可能的工作效果。生成良好回应的能力与检索最相关文档的能力不同。
- en: One potential way to enforce standardization is to limit users to a set of predetermined
    prompts. However, this might impact the leaderboard’s ability to capture diverse
    use cases. LMSYS instead lets users use any prompts but then filter out [hard
    prompts](https://x.com/lmarena_ai/status/1792625968865026427) using their internal
    model and rank models using only these hard prompts.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 一种强制标准化的潜在方法是限制用户只能使用一组预定的提示。然而，这可能会影响排行榜捕捉多样化用例的能力。LMSYS相反，允许用户使用任何提示，但随后使用他们的内部模型过滤掉[困难提示](https://x.com/lmarena_ai/status/1792625968865026427)，并仅使用这些困难提示对模型进行排名。
- en: Another way is to use only evaluators that we can trust. We can train evaluators
    on the criteria to compare two responses or train them to use practical prompts
    and sophisticated prompting techniques. This is the approach that Scale uses with
    [their private comparative leaderboard](https://oreil.ly/kIJ9F). The downside
    of this approach is that it’s expensive and it can severely reduce the number
    of comparisons we can get.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是只使用我们可以信任的评估者。我们可以训练评估者比较两个回应的标准，或者训练他们使用实用的提示和复杂的提示技术。这是Scale使用[他们的私人比较排行榜](https://oreil.ly/kIJ9F)的方法。这种方法的不利之处在于它成本高昂，并且可能会严重减少我们可以获得的比较数量。
- en: Another option is to incorporate comparative evaluation into your products and
    let users evaluate models during their workflows. For example, for the code generation
    task, you can suggest users two code snippets inside the user’s code editor and
    let them pick the better one. Many chat applications are already doing this. However,
    as mentioned previously, the user might not know which code snippet is better,
    since they’re not the expert.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是将比较评估纳入您的产品中，并让用户在他们的工作流程中评估模型。例如，对于代码生成任务，您可以在用户的代码编辑器中建议两个代码片段，并让他们选择更好的一个。许多聊天应用已经这样做了。然而，如前所述，用户可能不知道哪个代码片段更好，因为他们不是专家。
- en: On top of that, users might not read both options and just randomly click on
    one. This can introduce a lot of noise to the results. However, the signals from
    the small percentage of users who vote correctly can sometimes be sufficient to
    help determine which model is better.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，用户可能不会阅读两个选项，而是随机点击一个。这可能会给结果引入很多噪音。然而，正确投票的小部分用户的信号有时足以帮助确定哪个模型更好。
- en: '*Some teams prefer AI to human evaluators. AI might not be as good as trained
    human experts but it might be more reliable than random internet users*.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '*一些团队更倾向于使用人工智能评估者而不是人类评估者。人工智能可能不如训练有素的专家，但它可能比随机互联网用户更可靠*。'
- en: From comparative performance to absolute performance
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从比较性能到绝对性能
- en: 'For many applications, we don’t necessarily need the best possible models.
    We need a model that is good enough. Comparative evaluation tells us which model
    is better. It doesn’t tell us how good a model is or whether this model is good
    enough for our use case. Let’s say we obtained the ranking that model B is better
    than model A. Any of the following scenarios could be valid:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用来说，我们并不一定需要最好的模型。我们需要的是一个足够好的模型。比较评估告诉我们哪个模型更好。它并没有告诉我们模型有多好，或者这个模型是否足够适合我们的用例。假设我们得到了模型B比模型A更好的排名。以下任何一种情况都可能有效：
- en: Model B is good, but model A is bad.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型B很好，但模型A不好。
- en: Both model A and model B are bad.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型A和模型B都不好。
- en: Both model A and model B are good.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型A和模型B都很好。
- en: You need other forms of evaluation to determine which scenario is true.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要其他形式的评估来确定哪种情况是真实的。
- en: Imagine that we’re using model A for customer support, and model A can resolve
    70% of all the tickets. Consider model B, which wins against A 51% of the time.
    It’s unclear how this 51% win rate will be converted to the number of requests
    model B can resolve. Several people have told me that in their experience, a 1%
    change in the win rate can induce a huge performance boost in some applications
    but just a minimal boost in other applications.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在使用模型A进行客户支持，模型A可以解决70%的所有工单。考虑模型B，它在51%的时间里战胜了A。不清楚这个51%的胜率将如何转化为模型B可以解决的请求数量。有几个人告诉我，根据他们的经验，胜率1%的变化在某些应用中可以引起巨大的性能提升，而在其他应用中只是微小的提升。
- en: When deciding to swap out A for B, human preference isn’t everything. We also
    care about other factors like cost. Not knowing what performance boost to expect
    makes it hard to do the cost–benefit analysis. If model B costs twice as much
    as A, comparative evaluation isn’t sufficient to help us determine if the performance
    boost from B will be worth the added cost.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 当决定用B替换A时，人的偏好并不是一切。我们还关心其他因素，如成本。不知道可以期待的性能提升使得进行成本效益分析变得困难。如果模型B的成本是A的两倍，比较评估不足以帮助我们确定B的性能提升是否值得额外的成本。
- en: The Future of Comparative Evaluation
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较评估的未来
- en: Given so many limitations of comparative evaluation, you might wonder if there’s
    a future to it. There are many benefits to comparative evaluation. First, as discussed
    in [“Post-Training”](ch02.html#ch02_post_training_1730147895572108), people have
    found that it’s easier to compare two outputs than to give each output a concrete
    score. As models become stronger, surpassing human performance, it might become
    impossible for human evaluators to give model responses concrete scores. However,
    human evaluators might still be able to detect the difference, and comparative
    evaluation might remain the only option. For example, the Llama 2 paper shared
    that when the model ventures into the kind of writing beyond the ability of the
    best human annotators, humans can still provide valuable feedback when comparing
    two answers ([Touvron et al., 2023](https://arxiv.org/abs/2307.09288)).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到比较评估存在许多局限性，你可能会想知道它是否有未来。比较评估有许多好处。首先，如“训练后”章节（ch02.html#ch02_post_training_1730147895572108）中讨论的那样，人们发现比较两个输出比给每个输出一个具体的分数要容易。随着模型变得越来越强大，超越人类性能，人类评估者可能无法给模型响应给出具体的分数。然而，人类评估者仍然能够检测到差异，比较评估可能仍然是唯一的选择。例如，Llama
    2论文分享说，当模型进入最佳人类标注者能力之外的写作领域时，人类仍然可以在比较两个答案时提供有价值的反馈（[Touvron et al., 2023](https://arxiv.org/abs/2307.09288))。
- en: 'Second, comparative evaluation aims to capture the quality we care about: human
    preference. It reduces the pressure to have to constantly create more benchmarks
    to catch up with AI’s ever-expanding capabilities. Unlike benchmarks that become
    useless when model performance achieves perfect scores, comparative evaluations
    will never get saturated as long as newer, stronger models are introduced.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，比较评估旨在捕捉我们关心的质量：人类偏好。它减轻了必须不断创建更多基准以跟上人工智能不断扩展的能力的压力。与模型性能达到完美分数时变得无用的基准不同，只要引入了更新、更强大的模型，比较评估就不会饱和。
- en: Comparative evaluation is relatively hard to game, as there’s no easy way to
    cheat, like training your model on reference data. For this reason, many trust
    the results of public comparative leaderboards more than any other public leaderboards.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 比较评估相对难以操纵，因为没有简单的方法来作弊，比如在参考数据上训练你的模型。因此，许多人比其他任何公开排行榜更信任公共比较排行榜的结果。
- en: Comparative evaluation can give us discriminating signals about models that
    can’t be obtained otherwise. For offline evaluation, it can be a great addition
    to evaluation benchmarks. For online evaluation, it can be complementary to A/B
    testing.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 比较评估可以给我们提供其他方式无法获得的关于模型的区分性信号。对于离线评估，它可以成为评估基准的绝佳补充。对于在线评估，它可以与A/B测试相辅相成。
- en: Summary
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The stronger AI models become, the higher the potential for catastrophic failures,
    which makes evaluation even more important. At the same time, evaluating open-ended,
    powerful models is challenging. These challenges make many teams turn toward human
    evaluation. Having humans in the loop for sanity checks is always helpful, and
    in many cases, human evaluation is essential. However, this chapter focused on
    different approaches to automatic evaluation.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能模型变得越来越强大，灾难性故障的潜在风险也越高，这使得评估变得更加重要。同时，评估开放性、强大的模型具有挑战性。这些挑战使得许多团队转向人工评估。在评估过程中引入人类进行合理性检查总是有帮助的，而且在许多情况下，人工评估是必不可少的。然而，本章重点介绍了自动评估的不同方法。
- en: This chapter starts with a discussion on why foundation models are harder to
    evaluate than traditional ML models. While many new evaluation techniques are
    being developed, investments in evaluation still lag behind investments in model
    and application development.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从讨论为什么基础模型比传统机器学习模型更难评估开始。虽然许多新的评估技术正在被开发，但评估的投资仍然落后于模型和应用开发的投资。
- en: Since many foundation models have a language model component, we zoomed into
    language modeling metrics, including perplexity and cross entropy. Many people
    I’ve talked to find these metrics confusing, so I included a section on how to
    interpret these metrics and leverage them in evaluation and data processing.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多基础模型包含语言模型组件，我们聚焦于语言建模指标，包括困惑度和交叉熵。我交谈过的许多人发现这些指标令人困惑，因此我包括了一个关于如何解释这些指标以及在评估和数据处理中利用它们的章节。
- en: This chapter then shifted the focus to the different approaches to evaluate
    open-ended responses, including functional correctness, similarity scores, and
    AI as a judge. The first two evaluation approaches are exact, while AI as a judge
    evaluation is subjective.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 本章随后将重点转向评估开放式回答的不同方法，包括功能正确性、相似度评分以及将AI作为评判者。前两种评估方法是精确的，而将AI作为评判者的评估则是主观的。
- en: Unlike exact evaluation, subjective metrics are highly dependent on the judge.
    Their scores need to be interpreted in the context of what judges are being used.
    Scores aimed to measure the same quality by different AI judges might not be comparable.
    AI judges, like all AI applications, should be iterated upon, meaning their judgments
    change. This makes them unreliable as benchmarks to track an application’s changes
    over time. While promising, AI judges should be supplemented with exact evaluation,
    human evaluation, or both.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 与精确评估不同，主观指标高度依赖于评判者。他们的评分需要在所使用的评判者背景下进行解读。旨在通过不同的AI评判者测量相同质量的评分可能不可比较。AI评判者，如同所有AI应用一样，应该进行迭代，这意味着他们的判断会改变。这使得它们作为跟踪应用随时间变化的基准不可靠。虽然前景看好，但AI评判者应该辅以精确评估、人工评估或两者兼具。
- en: 'When evaluating models, you can evaluate each model independently, and then
    rank them by their scores. Alternatively, you can rank them using comparative
    signals: which of the two models is better? Comparative evaluation is common in
    sports, especially chess, and is gaining traction in AI evaluation. Both comparative
    evaluation and the post-training alignment process need preference signals, which
    are expensive to collect. This motivated the development of preference models:
    specialized AI judges that predict which response users prefer.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型时，你可以独立评估每个模型，然后根据它们的分数进行排名。或者，你可以使用比较信号进行排名：两个模型中哪一个更好？比较评估在体育中很常见，尤其是在国际象棋中，并且在AI评估中越来越受欢迎。比较评估和训练后对齐过程都需要偏好信号，这些信号收集起来很昂贵。这促使了偏好模型的发展：专门预测用户偏好的AI评判者。
- en: While language modeling metrics and hand-designed similarity measurements have
    existed for some time, AI as a judge and comparative evaluation have only gained
    adoption with the emergence of foundation models. Many teams are figuring out
    how to incorporate them into their evaluation pipelines. Figuring out how to build
    a reliable evaluation pipeline to evaluate open-ended applications is the topic
    of the next chapter.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管语言模型指标和手工设计的相似度测量已经存在了一段时间，但AI作为评判者和比较评估只有在基础模型出现后才开始被采用。许多团队正在研究如何将它们纳入他们的评估流程中。下一章的主题是如何构建一个可靠的评估流程来评估开放式应用。
- en: ^([1](ch03.html#id871-marker)) In December 2023, Greg Brockman, an OpenAI cofounder,
    [tweeted](https://x.com/gdb/status/1733553161884127435) that “evals are surprisingly
    often all you need.”
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#id871-marker)) 2023年12月，OpenAI的联合创始人格雷格·布鲁克曼 [推文](https://x.com/gdb/status/1733553161884127435)
    表示，“评估往往就是你所需要的全部。”
- en: ^([2](ch03.html#id872-marker)) A 2023 study by [a16z](https://oreil.ly/fti6d)
    showed that 6 out of 70 decision makers evaluated models by word of mouth.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.html#id872-marker)) 2023年，由 [a16z](https://oreil.ly/fti6d) 进行的一项研究表明，70位决策者中有6位通过口碑评估模型。
- en: ^([3](ch03.html#id873-marker)) Also known as *vibe check*.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.html#id873-marker)) 也称为“感觉检查”。
- en: ^([4](ch03.html#id874-marker)) When OpenAI’s GPT-o1 came out in September 2024,
    the [Fields medalist Terrence Tao](https://oreil.ly/4KJQM) compared the experience
    of working with this model to working with “a mediocre, but not completely incompetent,
    graduate student.” He speculated that it may only take one or two further iterations
    until AI reaches the level of a “competent graduate student.” In response to his
    assessment, many people joked that if we’re already at the point where we need
    the brightest human minds to evaluate AI models, we’ll have no one qualified to
    evaluate future models.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch03.html#id874-marker)) 当OpenAI的GPT-o1在2024年9月发布时，[菲尔兹奖获得者特伦斯·陶](https://oreil.ly/4KJQM)
    将与该模型合作的经验与与“一个平庸但并非完全无能的研究生”合作的经验进行了比较。他推测，可能只需再经过一到两次迭代，AI就能达到“合格研究生”的水平。对他的评估做出回应时，许多人开玩笑说，如果我们已经到了需要最聪明的人类头脑来评估AI模型的地步，那么我们将没有人有资格评估未来的模型。
- en: ^([5](ch03.html#id875-marker)) I searched for all repositories with at least
    500 stars using the keywords “LLM”, “GPT”, “generative”, and “transformer”. I
    also crowdsourced for missing repositories through my website [*https://huyenchip.com*](https://huyenchip.com/llama-police).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch03.html#id875-marker)) 我使用关键词“LLM”、“GPT”、“生成”和“transformer”搜索了至少有500个星标的所有仓库。我还通过我的网站[*https://huyenchip.com*](https://huyenchip.com/llama-police)收集了缺失的仓库。
- en: ^([6](ch03.html#id878-marker)) While there’s a strong correlation, language
    modeling performance doesn’t fully explain downstream performance. This is an
    active area of research.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch03.html#id878-marker)) 虽然存在强烈的关联，但语言模型的表现并不能完全解释下游的表现。这是一个活跃的研究领域。
- en: '^([7](ch03.html#id881-marker)) As discussed in [Chapter 1](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319),
    a token can be a character, a word, or part of a word. When Claude Shannon introduced
    entropy in 1951, the tokens he worked with were characters. Here’s entropy in
    [his own words](https://oreil.ly/HjUlH): *“*The entropy is a statistical parameter
    which measures, in a certain sense, how much information is produced on the average
    for each letter of a text in the language. If the language is translated into
    binary digits (0 or 1) in the most efficient way, the entropy is the average number
    of binary digits required per letter of the original language.”'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch03.html#id881-marker)) 如同在[第一章](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319)中讨论的，一个标记可以是一个字符、一个单词，或者单词的一部分。当克劳德·香农在1951年引入熵的概念时，他所处理的标记是字符。以下是他的原话[原文](https://oreil.ly/HjUlH)：*“熵是一个统计参数，它在某种意义上衡量了在语言中，平均每个文本字符产生多少信息。如果语言以最有效的方式翻译成二进制数字（0或1），那么熵就是每个原始语言字符所需的平均二进制数字数量。”*
- en: ^([8](ch03.html#id892-marker)) One reason many people might prefer natural log
    over log base 2 is because natural log has certain properties that makes its math
    easier. For example, the derivative of natural log ln(*x*) is 1/*x*.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch03.html#id892-marker)) 许多人可能更喜欢自然对数而不是以2为底的对数的一个原因是，自然对数具有某些性质，使得数学计算更简单。例如，自然对数ln(*x*)的导数是1/*x*。
- en: ^([9](ch03.html#id895-marker)) If you’re unsure what SFT (supervised finetuning)
    and RLHF (reinforcement learning from human feedback) mean, revisit [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch03.html#id895-marker)) 如果你不确定SFT（监督微调）和RLHF（基于人类反馈的强化学习）的含义，请回顾[第二章](ch02.html#ch02_understanding_foundation_models_1730147895571359)。
- en: ^([10](ch03.html#id896-marker)) Quantization is discussed in [Chapter 7](ch07.html#ch07).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch03.html#id896-marker)) 量化在[第七章](ch07.html#ch07)中有讨论。
- en: ^([11](ch03.html#id906-marker)) The challenge is that while many complex tasks
    have measurable objectives, AI isn’t quite good enough to perform complex tasks
    end-to-end, so AI might be used to do part of the solution. Sometimes, evaluating
    a part of a solution is harder than evaluating the end outcome. Imagine you want
    to evaluate someone’s ability to play chess. It’s easier to evaluate the end game
    outcome (win/lose/draw) than to evaluate just one move.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch03.html#id906-marker)) 挑战在于，尽管许多复杂任务都有可衡量的目标，但人工智能在端到端执行复杂任务方面还不够完善，因此人工智能可能被用来完成解决方案的一部分。有时，评估解决方案的一部分比评估最终结果更难。想象一下，你想评估某人下棋的能力。评估最终游戏结果（赢/输/平局）比评估单一步骤更容易。
- en: ^([12](ch03.html#id922-marker)) You might also want to do some processing depending
    on whether you want “cats” and “cat” or “will not” and “won’t” to be considered
    two separate tokens.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch03.html#id922-marker)) 你可能还想要根据是否需要将“cats”和“cat”或“will not”和“won’t”视为两个不同的标记来进行一些处理。
- en: ^([13](ch03.html#id927-marker)) While a 10,000-element vector space seems high-dimensional,
    it’s much lower than the dimensionality of the raw data. An embedding is, therefore,
    considered a representation of complex data in a lower-dimensional space.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch03.html#id927-marker)) 虽然一个包含10,000个元素的向量空间看起来是高维的，但它远低于原始数据的维度。因此，嵌入被认为是在低维空间中复杂数据的表示。
- en: '^([14](ch03.html#id929-marker)) There are also models that generate word embeddings,
    as opposed to documentation embeddings, such as word2vec (Mikolov et al., [“Efficient
    Estimation of Word Representations in Vector Space”](https://arxiv.org/abs/1301.3781),
    *arXiv*, v3, September 7, 2013) and GloVe (Pennington et al., [“GloVe: Global
    Vectors for Word Representation”](https://oreil.ly/O5QTX), the Stanford University
    Natural Language Processing Group (blog), 2014.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '^([14](ch03.html#id929-marker)) 也有一些模型生成词嵌入，而不是文档嵌入，例如word2vec（Mikolov等人，[“Efficient
    Estimation of Word Representations in Vector Space”](https://arxiv.org/abs/1301.3781)，*arXiv*，v3，2013年9月7日）和GloVe（Pennington等人，[“GloVe:
    Global Vectors for Word Representation”](https://oreil.ly/O5QTX)，斯坦福大学自然语言处理组（博客），2014年）。'
- en: ^([15](ch03.html#id937-marker)) The term *AI judge* is not to be confused with
    the use case where AI is used as a judge in court.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch03.html#id937-marker)) “AI裁判”这个术语不要与AI在法庭上作为裁判的使用场景混淆。
- en: ^([16](ch03.html#id938-marker)) In 2017, I presented at a NeurIPS workshop [MEWR](https://x.com/chipro/status/937384141791698944)
    (Machine translation Evaluation metric Without Reference text), an evaluation
    method that leverages stronger language models to automatically evaluate machine
    translations. Sadly, I never pursued this line of research because life got in
    the way.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch03.html#id938-marker)) 2017年，我在NeurIPS研讨会[MEWR](https://x.com/chipro/status/937384141791698944)（无参考文本的机器翻译评估指标）上发表了演讲，这是一种利用更强的语言模型来自动评估机器翻译的方法。遗憾的是，我从未继续这条研究路线，因为生活让我分心。
- en: ^([17](ch03.html#id948-marker)) In some cases, evaluation can take up the majority
    of the budget, even more than response generation.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch03.html#id948-marker)) 在某些情况下，评估可能占据预算的大部分，甚至超过响应生成。
- en: ^([18](ch03.html#id949-marker)) Spot-checking is the same as sampling.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch03.html#id949-marker)) 检查点与抽样相同。
- en: ^([19](ch03.html#id952-marker)) Saito et al. (2023) found that humans tend to
    favor longer responses too, but to a much lesser extent.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch03.html#id952-marker)) Saito等人（2023）发现，人类也倾向于更喜欢较长的响应，但程度要小得多。
- en: ^([20](ch03.html#id955-marker)) This technique is sometimes referred to as *self-critique*
    or *self-ask*.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch03.html#id955-marker)) 这种技术有时被称为*自我批评*或*自我提问*。
- en: '^([21](ch03.html#id959-marker)) The BLEURT score range is confusing. It’s approximately
    [between -2.5 and 1.0](https://github.com/google-research/bleurt/issues/1). This
    highlights the challenge of criteria ambiguity with AI judges: the score range
    can be arbitrary.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch03.html#id959-marker)) BLEURT分数的范围令人困惑。它大约在[-2.5和1.0](https://github.com/google-research/bleurt/issues/1)之间。这突显了AI裁判中标准模糊性的挑战：分数范围可以是任意的。
- en: ^([22](ch03.html#id965-marker)) Such as using a [Likert scale](https://en.wikipedia.org/wiki/Likert_scale).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch03.html#id965-marker)) 例如使用[李克特量表](https://en.wikipedia.org/wiki/Likert_scale)。
- en: ^([23](ch03.html#id970-marker)) Even though Chatbot Arena stopped using the
    Elo rating algorithm, its developers, for a while, continued referring to their
    model ratings “Elo scores”. They scaled the resulting Bradley-Terry scores to
    make them look like Elo scores. The scaling is fairly complicated. Each score
    is multiplied by 400 (the scale used in Elo) and added to 1,000 (the initial Elo
    score). Then this score is rescaled so that the model Llama-13b has a score of
    800.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch03.html#id970-marker)) 即使Chatbot Arena停止使用Elo评级算法，其开发者在一段时间内仍然继续将他们的模型评级称为“Elo分数”。他们通过缩放得到的Bradley-Terry分数，使其看起来像Elo分数。这种缩放相当复杂。每个分数乘以400（Elo使用的比例）并加到1000（初始Elo分数）。然后对这个分数进行重新缩放，使得Llama-13b模型的分数为800。
- en: ^([24](ch03.html#id976-marker)) As Chatbot Arena becomes more popular, attempts
    to game it have become more common. While no one has admitted to me that they
    tried to game the ranking, several model developers have told me that they’re
    convinced their competitors try to game it.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch03.html#id976-marker)) 随着Chatbot Arena越来越受欢迎，试图操纵它的情况也变得越来越常见。虽然没有人向我承认他们试图操纵排名，但几位模型开发者告诉我，他们相信他们的竞争对手试图操纵它。
