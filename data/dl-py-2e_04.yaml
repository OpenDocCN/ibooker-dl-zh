- en: '4 Getting started with neural networks: Classification and regression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 入门神经网络：分类和回归
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Your first examples of real-world machine learning workflows
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的第一个真实世界机器学习工作流示例
- en: Handling classification problems over vector data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理矢量数据上的分类问题
- en: Handling continuous regression problems over vector data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理矢量数据上的连续回归问题
- en: 'This chapter is designed to get you started using neural networks to solve
    real problems. You’ll consolidate the knowledge you gained from chapters 2 and
    3, and you’ll apply what you’ve learned to three new tasks covering the three
    most common use cases of neural networks—binary classification, multiclass classification,
    and scalar regression:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在帮助您开始使用神经网络解决实际问题。您将巩固从第 2 章和第 3 章中获得的知识，并将所学应用于三个新任务，涵盖神经网络的三种最常见用例 — 二元分类、多类分类和标量回归：
- en: Classifying movie reviews as positive or negative (binary classification)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将电影评论分类为正面或负面（二元分类）
- en: Classifying news wires by topic (multiclass classification)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据主题对新闻线进行分类（多类分类）
- en: Estimating the price of a house, given real-estate data (scalar regression)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定房地产数据估计房屋价格（标量回归）
- en: 'These examples will be your first contact with end-to-end machine learning
    workflows: you’ll get introduced to data preprocessing, basic model architecture
    principles, and model evaluation.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例将是您与端到端机器学习工作流的第一次接触：您将介绍数据预处理、基本模型架构原则和模型评估。
- en: Classification and regression glossary
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 分类和回归术语表
- en: 'Classification and regression involve many specialized terms. You’ve come across
    some of them in earlier examples, and you’ll see more of them in future chapters.
    They have precise, machine learning–specific definitions, and you should be familiar
    with them:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 分类和回归涉及许多专门术语。您在早期示例中已经遇到了一些，您将在未来章节中看到更多。它们具有精确的、机器学习特定的定义，您应该熟悉它们：
- en: '*Sample* or *input*—One data point that goes into your model.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sample* 或 *input* — 进入您的模型的一个数据点。'
- en: '*Prediction* or *output*—What comes out of your model.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Prediction* 或 *output* — 您的模型输出的内容。'
- en: '*Target*—The truth. What your model should ideally have predicted, according
    to an external source of data.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Target* — 真相。根据外部数据源，您的模型理想情况下应该预测的内容。'
- en: '*Prediction error* or *loss value*—A measure of the distance between your model’s
    prediction and the target.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Prediction error* 或 *loss value* — 您的模型预测与目标之间距离的度量。'
- en: '*Classes*—A set of possible labels to choose from in a classification problem.
    For example, when classifying cat and dog pictures, “dog” and “cat” are the two
    classes.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Classes* — 在分类问题中可供选择的可能标签集。例如，当对猫和狗图片进行分类时，“狗”和“猫”是两个类别。'
- en: '*Label* —A specific instance of a class annotation in a classification problem.
    For instance, if picture #1234 is annotated as containing the class “dog,” then
    “dog” is a label of picture #1234.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Label* — 分类问题中类别注释的特定实例。例如，如果图片 #1234 被注释为包含“狗”类，则“狗”是图片 #1234 的一个标签。'
- en: '*Ground-truth* or *annotations*—All targets for a dataset, typically collected
    by humans.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ground-truth* 或 *annotations* — 数据集中的所有目标，通常由人类收集。'
- en: '*Binary classification*—A classification task where each input sample should
    be categorized into two exclusive categories.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Binary classification* — 一个分类任务，其中每个输入样本应该被分类到两个互斥的类别中。'
- en: '*Multiclass classification*—A classification task where each input sample should
    be categorized into more than two categories: for instance, classifying handwritten
    digits.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Multiclass classification* — 一个分类任务，其中每个输入样本应该被分类到两个以上的类别中：例如，分类手写数字。'
- en: '*Multilabel classification*—A classification task where each input sample can
    be assigned multiple labels. For instance, a given image may contain both a cat
    and a dog and should be annotated both with the “cat” label and the “dog” label.
    The number of labels per image is usually variable.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Multilabel classification* — 一个分类任务，其中每个输入样本可以被分配多个标签。例如，给定图像可能同时包含猫和狗，并且应该同时用“猫”标签和“狗”标签进行注释。每个图像的标签数量通常是可变的。'
- en: '*Scalar regression*—A task where the target is a continuous scalar value. Predicting
    house prices is a good example: the different target prices form a continuous
    space.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Scalar regression* — 目标是一个连续标量值的任务。预测房价是一个很好的例子：不同的目标价格形成一个连续空间。'
- en: '*Vector regression*—A task where the target is a set of continuous values:
    for example, a continuous vector. If you’re doing regression against multiple
    values (such as the coordinates of a bounding box in an image), then you’re doing
    vector regression.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Vector regression* — 目标是一组连续值的任务：例如，一个连续的矢量。如果您正在针对多个值进行回归（例如图像中边界框的坐标），那么您正在进行矢量回归。'
- en: '*Mini-batch* or *batch*—A small set of samples (typically between 8 and 128)
    that are processed simultaneously by the model. The number of samples is often
    a power of 2, to facilitate memory allocation on GPU. When training, a mini-batch
    is used to compute a single gradient-descent update applied to the weights of
    the model.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Mini-batch* 或 *batch* — 模型同时处理的一小组样本（通常在 8 到 128 之间）。样本数量通常是 2 的幂，以便在 GPU
    上进行内存分配。在训练时，一个小批量用于计算应用于模型权重的单个梯度下降更新。'
- en: By the end of this chapter, you’ll be able to use neural networks to handle
    simple classification and regression tasks over vector data. You’ll then be ready
    to start building a more principled, theory-driven understanding of machine learning
    in chapter 5.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章结束时，您将能够使用神经网络处理矢量数据上的简单分类和回归任务。然后，您将准备好在第 5 章开始构建更有原则、理论驱动的机器学习理解。
- en: '4.1 Classifying movie reviews: A binary classification example'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 电影评论分类：一个二元分类示例
- en: Two-class classification, or binary classification, is one of the most common
    kinds of machine learning problems. In this example, you’ll learn to classify
    movie reviews as positive or negative, based on the text content of the reviews.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类，或二元分类，是最常见的机器学习问题之一。在这个示例中，您将学习根据评论的文本内容将电影评论分类为正面或负面。
- en: 4.1.1 The IMDB dataset
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 IMDB 数据集
- en: 'You’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews
    from the Internet Movie Database. They’re split into 25,000 reviews for training
    and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive
    reviews.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用 IMDB 数据集：来自互联网电影数据库的 50,000 条高度极化评论。它们被分为 25,000 条用于训练和 25,000 条用于测试的评论，每组评论包含
    50% 的负面评论和 50% 的正面评论。
- en: 'Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It
    has already been preprocessed: the reviews (sequences of words) have been turned
    into sequences of integers, where each integer stands for a specific word in a
    dictionary. This enables us to focus on model building, training, and evaluation.
    In chapter 11, you’ll learn how to process raw text input from scratch.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 MNIST 数据集一样，IMDB 数据集已经打包到 Keras 中。它已经经过预处理：评论（单词序列）已经转换为整数序列，其中每个整数代表字典中的特定单词。这使我们能够专注于模型构建、训练和评估。在第
    11 章中，您将学习如何从头开始处理原始文本输入。
- en: The following code will load the dataset (when you run it the first time, about
    80 MB of data will be downloaded to your machine).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将加载数据集（第一次运行时，将下载约 80 MB 的数据到您的计算机）。
- en: Listing 4.1 Loading the IMDB dataset
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.1 加载 IMDB 数据集
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The argument `num_words=10000` means you’ll only keep the top 10,000 most frequently
    occurring words in the training data. Rare words will be discarded. This allows
    us to work with vector data of manageable size. If we didn’t set this limit, we’d
    be working with 88,585 unique words in the training data, which is unnecessarily
    large. Many of these words only occur in a single sample, and thus can’t be meaningfully
    used for classification.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`num_words=10000`表示您只会保留训练数据中出现频率最高的前 10,000 个单词。罕见单词将被丢弃。这使我们可以处理可管理大小的向量数据。如果我们不设置这个限制，我们将使用训练数据中的
    88,585 个独特单词，这是不必要的庞大数量。其中许多单词只在一个样本中出现，因此无法有意义地用于分类。
- en: 'The variables `train_data` and `test_data` are lists of reviews; each review
    is a list of word indices (encoding a sequence of words). `train_labels` and `test_labels`
    are lists of 0s and 1s, where 0 stands for *negative* and 1 stands for *positive*:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`train_data`和`test_data`是评论列表；每个评论是一个单词索引列表（编码为单词序列）。`train_labels`和`test_labels`是
    0 和 1 的列表，其中 0 代表*负面*，1 代表*正面*：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Because we’re restricting ourselves to the top 10,000 most frequent words,
    no word index will exceed 10,000:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们限制自己只使用前 10,000 个最常见的单词，所以没有单词索引会超过 10,000：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For kicks, here’s how you can quickly decode one of these reviews back to English
    words.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了好玩，这里是如何快速将其中一个评论��码回英文单词。
- en: Listing 4.2 Decoding reviews back to text
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.2 将评论解码回文本
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ word_index is a dictionary mapping words to an integer index.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ word_index 是一个将单词映射到整数索引的字典。
- en: ❷ Reverses it, mapping integer indices to words
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将其反转，将整数索引映射到单词
- en: ❸ Decodes the review. Note that the indices are offset by 3 because 0, 1, and
    2 are reserved indices for “padding,” “start of sequence,” and “unknown.”
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 解码评论。请注意，索引偏移了 3，因为 0、1 和 2 是“填充”、“序列开始”和“未知”保留索引。
- en: 4.1.2 Preparing the data
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 准备数据
- en: 'You can’t directly feed lists of integers into a neural network. They all have
    different lengths, but a neural network expects to process contiguous batches
    of data. You have to turn your lists into tensors. There are two ways to do that:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您不能直接将整数列表输入神经网络。它们的长度各不相同，但神经网络期望处理连续的数据批次。您必须将列表转换为张量。有两种方法可以做到这一点：
- en: Pad your lists so that they all have the same length, turn them into an integer
    tensor of shape `(samples,` `max_length)`, and start your model with a layer capable
    of handling such integer tensors (the `Embedding` layer, which we’ll cover in
    detail later in the book).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充列表，使它们的长度相同，将它们转换为形状为`(samples, max_length)`的整数张量，并从能够处理这种整数张量的层开始构建模型（`Embedding`层，我们稍后会详细介绍）。
- en: '*Multi-hot encode* your lists to turn them into vectors of 0s and 1s. This
    would mean, for instance, turning the sequence `[8,` `5]` into a 10,000-dimensional
    vector that would be all 0s except for indices 8 and 5, which would be 1s. Then
    you could use a `Dense` layer, capable of handling floating-point vector data,
    as the first layer in your model.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多热编码*您的列表以将它们转换为0和1的向量。这意味着，例如，将序列`[8, 5]`转换为一个 10,000 维的向量，除了索引 8 和 5 外，其他都是
    0，而索引 8 和 5 是 1。然后，您可以使用一个`Dense`层，能够处理浮点向量数据，作为模型中的第一层。'
- en: Let’s go with the latter solution to vectorize the data, which you’ll do manually
    for maximum clarity.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择后一种解决方案来对数据进行向量化，这样您可以最大程度地清晰地进行操作。
- en: Listing 4.3 Encoding the integer sequences via multi-hot encoding
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.3 通过多热编码对整数序列进行编码
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Creates an all-zero matrix of shape (len(sequences), dimension)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个形状为(len(sequences), dimension)的全零矩阵
- en: ❷ Sets specific indices of results[i] to 1s
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将结果[i]的特定索引设置为1
- en: ❸ Vectorized training data
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 向量化训练数据
- en: ❹ Vectorized test data
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 向量化测试数据
- en: 'Here’s what the samples look like now:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在样本看起来是这样的：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should also vectorize your labels, which is straightforward:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应该对标签进行向量化，这很简单：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now the data is ready to be fed into a neural network.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已准备好输入神经网络。
- en: 4.1.3 Building your model
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 构建您的模型
- en: 'The input data is vectors, and the labels are scalars (1s and 0s): this is
    one of the simplest problem setups you’ll ever encounter. A type of model that
    performs well on such a problem is a plain stack of densely connected (`Dense`)
    layers with `relu` activations.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据是向量，标签是标量（1和0）：这是您可能会遇到的最简单的问题设置之一。在这样的问题上表现良好的模型类型是具有`relu`激活的一堆密集连接（`Dense`）层。
- en: 'There are two key architecture decisions to be made about such a stack of `Dense`
    layers:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样一堆`Dense`层，有两个关键的架构决策：
- en: How many layers to use
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用多少层
- en: How many units to choose for each layer
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择每层使用多少个单元
- en: 'In chapter 5, you’ll learn formal principles to guide you in making these choices.
    For the time being, you’ll have to trust me with the following architecture choices:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5 章，你将学习指导你做出这些选择的正式原则。目前，你将不得不相信我做出以下架构选择：
- en: Two intermediate layers with 16 units each
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个中间层，每个有 16 个单元
- en: A third layer that will output the scalar prediction regarding the sentiment
    of the current review
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层将输出关于当前评论情感的标量预测
- en: '![](../Images/04-01.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-01.png)'
- en: Figure 4.1 The three-layer model
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 三层模型
- en: Figure 4.1 shows what the model looks like. And the following listing shows
    the Keras implementation, similar to the MNIST example you saw previously.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 展示了模型的外观。以下代码展示了 Keras 实现，类似于你之前看到的 MNIST 示例。
- en: Listing 4.4 Model definition
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 4.4 模型定义
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The first argument being passed to each `Dense` layer is the number of *units*
    in the layer: the dimensionality of representation space of the layer. You remember
    from chapters 2 and 3 that each such `Dense` layer with a `relu` activation implements
    the following chain of tensor operations:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给每个 `Dense` 层的第一个参数是层中的*单元数*：层的表示空间的维度。你从第 2 章和第 3 章记得，每个具有 `relu` 激活的 `Dense`
    层实现以下张量操作链：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Having 16 units means the weight matrix `W` will have shape `(input_dimension,`
    `16)`: the dot product with `W` will project the input data onto a 16-dimensional
    representation space (and then you’ll add the bias vector `b` and apply the `relu`
    operation). You can intuitively understand the dimensionality of your representation
    space as “how much freedom you’re allowing the model to have when learning internal
    representations.” Having more units (a higher-dimensional representation space)
    allows your model to learn more-complex representations, but it makes the model
    more computationally expensive and may lead to learning unwanted patterns (patterns
    that will improve performance on the training data but not on the test data).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有 16 个单元意味着权重矩阵 `W` 的形状为 `(input_dimension, 16)`：与 `W` 的点积将把输入数据投影到一个 16 维表示空间（然后你将添加偏置向量
    `b` 并应用 `relu` 操作）。你可以直观地理解表示空间的维度为“模型在学习内部表示时允许的自由度有多大”。拥有更多单元（更高维的表示空间）允许你的模型学习更复杂的表示，但会使模型在计算上更昂贵，并可能导致学习不需要的模式（这些模式会提高训练数据的性能，但不会提高测试数据的性能）。
- en: 'The intermediate layers use `relu` as their activation function, and the final
    layer uses a sigmoid activation so as to output a probability (a score between
    0 and 1 indicating how likely the sample is to have the target “1”: how likely
    the review is to be positive). A `relu` (rectified linear unit) is a function
    meant to zero out negative values (see figure 4.2), whereas a sigmoid “squashes”
    arbitrary values into the `[0,` `1]` interval (see figure 4.3), outputting something
    that can be interpreted as a probability.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层使用 `relu` 作为它们的激活函数，最后一层使用 sigmoid 激活以输出一个概率（介于 0 和 1 之间的分数，指示样本有多大可能具有目标“1”：评论有多大可能是积极的）。`relu`（线性整流单元）是一个用于将负值归零的函数（参见图
    4.2），而 sigmoid “压缩”任意值到 `[0, 1]` 区间（参见图 4.3），输出可以解释为概率。
- en: '![](../Images/04-02.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-02.png)'
- en: Figure 4.2 The rectified linear unit function
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 线性整流单元函数
- en: 'Finally, you need to choose a loss function and an optimizer. Because you’re
    facing a binary classification problem and the output of your model is a probability
    (you end your model with a single-unit layer with a sigmoid activation), it’s
    best to use the `binary_crossentropy` loss. It isn’t the only viable choice: for
    instance, you could use `mean_squared_error`. But crossentropy is usually the
    best choice when you’re dealing with models that output probabilities. *Crossentropy*
    is a quantity from the field of information theory that measures the distance
    between probability distributions or, in this case, between the ground-truth distribution
    and your predictions.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要选择一个损失函数和一个优化器。因为你面临的是一个二元分类问题，你的模型的输出是一个概率（你的模型以具有 sigmoid 激活的单单元层结束），最好使用
    `binary_crossentropy` 损失。这并不是唯一可行的选择：例如，你可以使用 `mean_squared_error`。但是当你处理输出概率的模型时，交叉熵通常是最佳选择。*交叉熵*是信息论领域的一种量，用于衡量概率分布之间的距离，或者在这种情况下，地面实况分布和你的预测之间的距离。
- en: '![](../Images/04-03.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-03.png)'
- en: Figure 4.3 The sigmoid function
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 Sigmoid 函数
- en: What are activation functions, and why are they necessary?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是什么，为什么它们是必要的？
- en: 'Without an activation function like `relu` (also called a *non-linearity*),
    the `Dense` layer would consist of two linear operations—a dot product and an
    addition:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 没有像 `relu` 这样的激活函数（也称为*非线性*），`Dense` 层将由两个线性操作组成——点积和加法：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The layer could only learn *linear transformations* (affine transformations)
    of the input data: the *hypothesis space* of the layer would be the set of all
    possible linear transformations of the input data into a 16-dimensional space.
    Such a hypothesis space is too restricted and wouldn’t benefit from multiple layers
    of representations, because a deep stack of linear layers would still implement
    a linear operation: adding more layers wouldn’t extend the hypothesis space (as
    you saw in chapter 2).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该层只能学习输入数据的*线性变换*（仿射变换）：该层的*假设空间*将是将输入数据转换为 16 维空间的所有可能线性变换的集合。这样的假设空间太受限制，不会受益于多层表示，因为深度堆叠的线性层仍然实现线性操作：增加更多层不会扩展假设空间（正如你在第
    2 章中看到的）。
- en: 'In order to get access to a much richer hypothesis space that will benefit
    from deep representations, you need a non-linearity, or activation function. `relu`
    is the most popular activation function in deep learning, but there are many other
    candidates, which all come with similarly strange names: `prelu`, `elu`, and so
    on.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得一个更丰富的假设空间，从而受益于深度表示，你需要一个非线性或激活函数。`relu` 是深度学习中最流行的激活函数，但还有许多其他候选项，它们都有类似奇怪的名称：`prelu`、`elu`
    等等。
- en: As for the choice of the optimizer, we’ll go with `rmsprop`, which is a usually
    a good default choice for virtually any problem.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 至于优化器的选择，我们将选择`rmsprop`，这通常是几乎任何问题的一个很好的默认选择。
- en: Here’s the step where we configure the model with the `rmsprop` optimizer and
    the `binary_crossentropy` loss function. Note that we’ll also monitor accuracy
    during training.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们���用`rmsprop`优化器和`binary_crossentropy`损失函数配置模型的步骤。请注意，我们还将在训练过程中监视准确性。
- en: Listing 4.5 Compiling the model
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.5 编译模型
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 4.1.4 Validating your approach
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.4 验证您的方法
- en: As you learned in chapter 3, a deep learning model should never be evaluated
    on its training data—it’s standard practice to use a validation set to monitor
    the accuracy of the model during training. Here, we’ll create a validation set
    by setting apart 10,000 samples from the original training data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在第3章中学到的，深度学习模型永远不应该在其训练数据上进行评估——在训练过程中使用验证集来监视模型的准确性是标准做法。在这里，我们将通过从原始训练数据中分离出10,000个样本来创建一个验证集。
- en: Listing 4.6 Setting aside a validation set
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.6 设置一个验证集
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We will now train the model for 20 epochs (20 iterations over all samples in
    the training data) in mini-batches of 512 samples. At the same time, we will monitor
    loss and accuracy on the 10,000 samples that we set apart. We do so by passing
    the validation data as the `validation_data` argument.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将在512个样本的小批量中对模型进行20个时代（对训练数据中的所有样本进行20次迭代）的训练。同时，我们将通过将验证数据作为`validation_data`参数传递来监视我们分离出的10,000个样本上的损失和准确性。
- en: Listing 4.7 Training your model
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.7 训练您的模型
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: On CPU, this will take less than 2 seconds per epoch—training is over in 20
    seconds. At the end of every epoch, there is a slight pause as the model computes
    its loss and accuracy on the 10,000 samples of the validation data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU上，每个时代不到2秒——训练在20秒内结束。在每个时代结束时，模型会在验证数据的10,000个样本上计算其损失和准确性，会有一个轻微的暂停。
- en: 'Note that the call to `model.fit()` returns a `History` object, as you saw
    in chapter 3\. This object has a member `history`, which is a dictionary containing
    data about everything that happened during training. Let’s look at it:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对`model.fit()`的调用会返回一个`History`对象，就像您在第3章中看到的那样。这个对象有一个成员`history`，它是一个包含训练过程中发生的一切数据的字典。让我们来看一下：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The dictionary contains four entries: one per metric that was being monitored
    during training and during validation. In the following two listings, let’s use
    Matplotlib to plot the training and validation loss side by side (see figure 4.4),
    as well as the training and validation accuracy (see figure 4.5). Note that your
    own results may vary slightly due to a different random initialization of your
    model.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 字典包含四个条目：每个在训练和验证期间监视的指标一个。在接下来的两个列表中，让我们使用Matplotlib将训练和验证损失并排绘制出来（参见图4.4），以及训练和验证准确性（参见图4.5）。请注意，由于模型的不同随机初始化，您自己的结果可能会略有不同。
- en: '![](../Images/04-04.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-04.png)'
- en: Figure 4.4 Training and validation loss
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 训练和验证损失
- en: '![](../Images/04-05.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-05.png)'
- en: Figure 4.5 Training and validation accuracy
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 训练和验证准确性
- en: Listing 4.8 Plotting the training and validation loss
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.8 绘制训练和验证损失
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ "bo" is for "blue dot."
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ "bo"代表"蓝色点"。
- en: ❷ "b" is for "solid blue line."
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ "b"代表"实线蓝色线"。
- en: Listing 4.9 Plotting the training and validation accuracy
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.9 绘制训练和验证准确性
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Clears the figure
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 清除图形
- en: 'As you can see, the training loss decreases with every epoch, and the training
    accuracy increases with every epoch. That’s what you would expect when running
    gradient-descent optimization—the quantity you’re trying to minimize should be
    less with every iteration. But that isn’t the case for the validation loss and
    accuracy: they seem to peak at the fourth epoch. This is an example of what we
    warned against earlier: a model that performs better on the training data isn’t
    necessarily a model that will do better on data it has never seen before. In precise
    terms, what you’re seeing is *overfitting*: after the fourth epoch, you’re overoptimizing
    on the training data, and you end up learning representations that are specific
    to the training data and don’t generalize to data outside of the training set.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，训练损失随着每个时代的进行而减少，而训练准确性则随着每个时代的进行而增加。这是在运行梯度下降优化时您所期望的情况——您试图最小化的量应该在每次迭代中都减少。但验证损失和准确性并非如此：它们似乎在第四个时代达到峰值。这是我们之前警告过的一个例子：在训练数据上表现更好的模型不一定会在以前从未见过的数据上表现更好。准确来说，您所看到的是*过拟合*：在第四个时代之后，您过度优化了训练数据，最终学习到的表示是特定于训练数据的，无法推广到训练集之外的数据。
- en: In this case, to prevent overfitting, you could stop training after four epochs.
    In general, you can use a range of techniques to mitigate overfitting, which we’ll
    cover in chapter 5.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，为了防止过拟合，您可以在四个时代后停止训练。一般来说，您可以使用一系列技术来减轻过拟合，我们将在第5章中介绍。
- en: Let’s train a new model from scratch for four epochs and then evaluate it on
    the test data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头开始训练一个新模型四个时代，然后在测试数据上评估它。
- en: Listing 4.10 Retraining a model from scratch
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.10 从头开始重新训练模型
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The final results are as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果如下：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The first number, 0.29, is the test loss, and the second number, 0.88, is
    the test accuracy.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个数字0.29是测试损失，第二个数字0.88是测试准确性。
- en: This fairly naive approach achieves an accuracy of 88%. With state-of-the-art
    approaches, you should be able to get close to 95%.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这种相当天真的方法实现了88%的准确性。使用最先进的方法，您应该能够接近95%。
- en: 4.1.5 Using a trained model to generate predictions on new data
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.5 使用训练好的模型在新数据上生成预测
- en: 'After having trained a model, you’ll want to use it in a practical setting.
    You can generate the likelihood of reviews being positive by using the `predict`
    method, as you’ve learned in chapter 3:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完模型后，您会想要在实际环境中使用它。您可以使用`predict`方法生成评论为正面的可能性，就像您在第3章中学到的那样：
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, the model is confident for some samples (0.99 or more, or 0.01
    or less) but less confident for others (0.6, 0.4).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，模型对某些样本非常自信（0.99或更高，或0.01或更低），但对其他样本不太自信（0.6、0.4）。
- en: 4.1.6 Further experiments
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.6 进一步的实验
- en: 'The following experiments will help convince you that the architecture choices
    you’ve made are all fairly reasonable, although there’s still room for improvement:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以下实验将帮助您确信您所做的架构选择都是相当合理的，尽管仍有改进��空间：
- en: You used two representation layers before the final classification layer. Try
    using one or three representation layers, and see how doing so affects validation
    and test accuracy.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最终分类层之前，您使用了两个表示层。尝试使用一个或三个表示层，看看这样做如何影响验证和测试准确性。
- en: 'Try using layers with more units or fewer units: 32 units, 64 units, and so
    on.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用更多单元或更少单元的层：32个单元，64个单元等等。
- en: Try using the `mse` loss function instead of `binary_crossentropy`.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用`mse`损失函数而不是`binary_crossentropy`。
- en: Try using the `tanh` activation (an activation that was popular in the early
    days of neural networks) instead of `relu`.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用`tanh`激活（这是早期神经网络中流行的激活函数）而不是`relu`。
- en: 4.1.7 Wrapping up
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.7 总结
- en: 'Here’s what you should take away from this example:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您应该从这个示例中了解到的内容：
- en: You usually need to do quite a bit of preprocessing on your raw data in order
    to be able to feed it—as tensors—into a neural network. Sequences of words can
    be encoded as binary vectors, but there are other encoding options too.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，您需要对原始数据进行大量预处理，以便能够将其（作为张量）馈送到神经网络中。单词序列可以编码为二进制向量，但也有其他编码选项。
- en: Stacks of `Dense` layers with `relu` activations can solve a wide range of problems
    (including sentiment classification), and you’ll likely use them frequently.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有`relu`激活的`Dense`层堆叠可以解决各种问题（包括情感分类），您可能经常会使用它们。
- en: 'In a binary classification problem (two output classes), your model should
    end with a `Dense` layer with one unit and a `sigmoid` activation: the output
    of your model should be a scalar between 0 and 1, encoding a probability.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二元分类问题（两个输出类别）中，您的模型应该以一个具有一个单元和`sigmoid`激活的`Dense`层结束：您的模型的输出应该是一个介于0和1之间的标量，编码为概率。
- en: With such a scalar sigmoid output on a binary classification problem, the loss
    function you should use is `binary_crossentropy`.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二元分类问题上，具有标量S形输出的损失函数应该使用`binary_crossentropy`。
- en: The `rmsprop` optimizer is generally a good enough choice, whatever your problem.
    That’s one less thing for you to worry about.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rmsprop`优化器通常是一个足够好的选择，无论您的问题是什么。这是您无需担心的一件事。'
- en: As they get better on their training data, neural networks eventually start
    overfitting and end up obtaining increasingly worse results on data they’ve never
    seen before. Be sure to always monitor performance on data that is outside of
    the training set.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着神经网络在训练数据上变得更好，最终会开始过拟合，并且在从未见过的数据上获得越来越糟糕的结果。一定要始终监视在训练集之外的数据上的性能。
- en: '4.2 Classifying newswires: A multiclass classification example'
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 新闻线分类：一个多类别分类示例
- en: In the previous section, you saw how to classify vector inputs into two mutually
    exclusive classes using a densely connected neural network. But what happens when
    you have more than two classes?
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，您看到了如何使用密集连接的神经网络将向量输入分类为两个互斥类别。但是当您有两个以上的类别时会发生什么？
- en: In this section, we’ll build a model to classify Reuters newswires into 46 mutually
    exclusive topics. Because we have many classes, this problem is an instance of
    *multiclass classification*, and because each data point should be classified
    into only one category, the problem is more specifically an instance of *single-label
    multiclass classification*. If each data point could belong to multiple categories
    (in this case, topics), we’d be facing a *multilabel multiclass classification*
    problem.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个模型，将路透社新闻线分类为46个互斥主题。因为我们有很多类别，所以这个问题是*多类别分类*的一个实例，因为每个数据点应该被分类为一个类别，所以这个问题更具体地是*单标签多类别分类*的一个实例。如果每个数据点可以属于多个类别（在这种情况下是主题），我们将面临一个*多标签多类别分类*问题。
- en: 4.2.1 The Reuters dataset
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 路透社数据集
- en: You’ll work with the *Reuters dataset*, a set of short newswires and their topics,
    published by Reuters in 1986\. It’s a simple, widely used toy dataset for text
    classification. There are 46 different topics; some topics are more represented
    than others, but each topic has at least 10 examples in the training set.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用*路透社数据集*，这是路透社在1986年发布的一组简短新闻线及其主题。这是一个简单、广泛使用的文本分类玩具数据集。有46个不同的主题；一些主题比其他主题更有代表性，但每个主题在训练集中至少有10个示例。
- en: Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let’s
    take a look.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与IMDB和MNIST一样，路透社数据集作为Keras的一部分打包提供。让我们来看看。
- en: Listing 4.11 Loading the Reuters dataset
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.11 加载路透社数据集
- en: '[PRE19]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As with the IMDB dataset, the argument `num_words=10000` restricts the data
    to the 10,000 most frequently occurring words found in the data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 与IMDB数据集一样，参数`num_words=10000`将数据限制为数据中出现频率最高的10,000个单词。
- en: 'You have 8,982 training examples and 2,246 test examples:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您有8,982个训练示例和2,246个测试示例：
- en: '[PRE20]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As with the IMDB reviews, each example is a list of integers (word indices):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与IMDB评论一样，每个示例都是一个整数列表（单词索引）：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here’s how you can decode it back to words, in case you’re curious.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您感兴趣，这是如何将其解码回单词的方法。
- en: Listing 4.12 Decoding newswires back to text
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.12 将新闻线解码回文本
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices
    for “padding,” “start of sequence,” and “unknown.”
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 请注意，索引偏移了3，因为0、1和2是“填充”、“序列开始”和“未知”保留索引。
- en: 'The label associated with an example is an integer between 0 and 45—a topic
    index:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与示例相关联的标签是介于0和45之间的整数—一个主题索引：
- en: '[PRE23]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 4.2.2 Preparing the data
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 准备数据
- en: You can vectorize the data with the exact same code as in the previous example.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用与前一个示例中完全相同的代码对数据进行向量化。
- en: Listing 4.13 Encoding the input data
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.13 对输入数据进行编码
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Vectorized training data
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 向量化训练数据
- en: ❷ Vectorized test data
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 向量化测试数据
- en: 'To vectorize the labels, there are two possibilities: you can cast the label
    list as an integer tensor, or you can use *one-hot encoding*. One-hot encoding
    is a widely used format for categorical data, also called *categorical encoding*.
    In this case, one-hot encoding of the labels consists of embedding each label
    as an all-zero vector with a 1 in the place of the label index. The following
    listing shows an example.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 要将标签向量化，有两种可能性：你可以将标签列表转换为整数张量，或者你可以使用*独热编码*。独热编码是一种广泛使用的分类数据格式，也称为*分类编码*。在这种情况下，标签的独热编码包括将每个标签嵌入为一个全零向量，其中标签索引的位置为1。下面的列表显示了一个示例。
- en: Listing 4.14 Encoding the labels
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.14 编码标签
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Vectorized training labels
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 向量化训练标签
- en: ❷ Vectorized test labels
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 向量化测试标签
- en: 'Note that there is a built-in way to do this in Keras:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Keras中有一种内置的方法可以做到这一点：
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 4.2.3 Building your model
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 构建你的模型
- en: 'This topic-classification problem looks similar to the previous movie-review
    classification problem: in both cases, we’re trying to classify short snippets
    of text. But there is a new constraint here: the number of output classes has
    gone from 2 to 46\. The dimensionality of the output space is much larger.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主题分类问题看起来与之前的电影评论分类问题相似：在这两种情况下，我们都试图对短文本进行分类。但是这里有一个新的约束：输出类别的数量从2个增加到了46个。输出空间的维度大得多。
- en: 'In a stack of `Dense` layers like those we’ve been using, each layer can only
    access information present in the output of the previous layer. If one layer drops
    some information relevant to the classification problem, this information can
    never be recovered by later layers: each layer can potentially become an information
    bottleneck. In the previous example, we used 16-dimensional intermediate layers,
    but a 16-dimensional space may be too limited to learn to separate 46 different
    classes: such small layers may act as information bottlenecks, permanently dropping
    relevant information.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在像我们一直使用的`Dense`层堆叠中，每一层只能访问前一层输出中存在的信息。如果一层丢失了与分类问题相关的一些信息，这些信息将永远无法被后续层恢复：每一层都可能成为信息瓶颈。在前面的例子中，我们使用了16维的中间层，但16维的空间可能太有限，无法学习区分46个不同的类别：这样的小层可能充当信息瓶颈，永久丢失相关信息。
- en: For this reason we’ll use larger layers. Let’s go with 64 units.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个原因，我们将使用更大的层。让我们选择64个单元。
- en: Listing 4.15 Model definition
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.15 模型定义
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: There are two other things you should note about this architecture.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两件事情你应该注意关于这个架构。
- en: First, we end the model with a `Dense` layer of size 46\. This means for each
    input sample, the network will output a 46-dimensional vector. Each entry in this
    vector (each dimension) will encode a different output class.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们用一个大小为46的`Dense`层结束模型。这意味着对于每个输入样本，网络将输出一个46维的向量。这个向量中的每个条目（每个维度）将编码一个不同的输出类别。
- en: Second, the last layer uses a `softmax` activation. You saw this pattern in
    the MNIST example. It means the model will output a *probability distribution*
    over the 46 different output classes—for every input sample, the model will produce
    a 46-dimensional output vector, where `output[i]` is the probability that the
    sample belongs to class `i`. The 46 scores will sum to 1.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，最后一层使用了`softmax`激活函数。你在MNIST示例中看到了这种模式。这意味着模型将输出46个不同输出类别的*概率分布*，对于每个输入样本，模型将产生一个46维的输出向量，其中`output[i]`是样本属于类别`i`的概率。这46个分数将总和为1。
- en: 'The best loss function to use in this case is `categorical_crossentropy`. It
    measures the distance between two probability distributions: here, between the
    probability distribution output by the model and the true distribution of the
    labels. By minimizing the distance between these two distributions, you train
    the model to output something as close as possible to the true labels.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下使用的最佳损失函数是`categorical_crossentropy`。它衡量两个概率分布之间的距离：在这里，模型输出的概率分布与标签的真实分布之间的距离。通过最小化这两个分布之间的距离，你训练模型输出尽可能接近真实标签。
- en: Listing 4.16 Compiling the model
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.16 编译模型
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 4.2.4 Validating your approach
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.4 验证你的方法
- en: Let’s set apart 1,000 samples in the training data to use as a validation set.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在训练数据中留出1,000个样本作为验证集使用。
- en: Listing 4.17 Setting aside a validation set
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.17 设置一个验证集
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, let’s train the model for 20 epochs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练模型20个周期。
- en: Listing 4.18 Training the model
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.18 训练模型
- en: '[PRE30]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: And finally, let’s display its loss and accuracy curves (see figures 4.6 and
    4.7).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们展示其损失和准确率曲线（见图4.6和4.7）。
- en: '![](../Images/04-06.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-06.png)'
- en: Figure 4.6 Training and validation loss
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 训练和验证损失
- en: '![](../Images/04-07.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-07.png)'
- en: Figure 4.7 Training and validation accuracy
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 训练和验证准确率
- en: Listing 4.19 Plotting the training and validation loss
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.19 绘制训练和验证损失
- en: '[PRE31]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Listing 4.20 Plotting the training and validation accuracy
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.20 绘制训练和验证准确率
- en: '[PRE32]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Clears the figure
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 清除图表
- en: The model begins to overfit after nine epochs. Let’s train a new model from
    scratch for nine epochs and then evaluate it on the test set.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在九个周期后开始过拟合。让我们从头开始训练一个新模型，训练九个周期，然后在测试集上评估它。
- en: Listing 4.21 Retraining a model from scratch
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.21 从头开始重新训练模型
- en: '[PRE33]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here are the final results:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是最终结果：
- en: '[PRE34]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This approach reaches an accuracy of ~80%. With a balanced binary classification
    problem, the accuracy reached by a purely random classifier would be 50%. But
    in this case, we have 46 classes, and they may not be equally represented. What
    would be the accuracy of a random baseline? We could try quickly implementing
    one to check this empirically:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法达到了约80%的准确率。对于一个平衡的二元分类问题，一个纯随机分类器达到的准确率将是50%。但在这种情况下，我们有46个类别，它们可能不会被平等地表示。一个随机基线的准确率会是多少呢？我们可以尝试快速实现一个来进行经验性检查：
- en: '[PRE35]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As you can see, a random classifier would score around 19% classification accuracy,
    so the results of our model seem pretty good in that light.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，一个随机分类器的分类准确率约为19%，所以从这个角度看，我们模型的结果似乎相当不错。
- en: 4.2.5 Generating predictions on new data
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.5 在新数据上生成预测
- en: 'Calling the model’s `predict` method on new samples returns a class probability
    distribution over all 46 topics for each sample. Let’s generate topic predictions
    for all of the test data:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在新样本上调用模型的`predict`方法会返回每个样本的46个主题的类概率分布。让我们为所有测试数据生成主题预测：
- en: '[PRE36]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Each entry in “predictions” is a vector of length 46:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: “predictions”中的每个条目都是长度为46的向量：
- en: '[PRE37]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The coefficients in this vector sum to 1, as they form a probability distribution:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量中的系数总和为1，因为它们形成一个概率分布：
- en: '[PRE38]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The largest entry is the predicted class—the class with the highest probability:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的条目是预测的类别——具有最高概率的类别：
- en: '[PRE39]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 4.2.6 A different way to handle the labels and the loss
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.6 处理标签和损失的另一种方式
- en: 'We mentioned earlier that another way to encode the labels would be to cast
    them as an integer tensor, like this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到另一种编码标签的方式是将它们转换为整数张量，就像这样：
- en: '[PRE40]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The only thing this approach would change is the choice of the loss function.
    The loss function used in listing 4.21, `categorical_crossentropy`, expects the
    labels to follow a categorical encoding. With integer labels, you should use `sparse_categorical_
    crossentropy`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法唯���改变的是损失函数的选择。列表4.21中使用的损失函数`categorical_crossentropy`期望标签遵循分类编码。对于整数标签，你应该使用`sparse_categorical_crossentropy`：
- en: '[PRE41]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This new loss function is still mathematically the same as `categorical_crossentropy`;
    it just has a different interface.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的损失函数在数学上仍然与`categorical_crossentropy`相同；它只是有一个不同的接口。
- en: 4.2.7 The importance of having sufficiently large intermediate layers
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.7 拥有足够大的中间层的重要性
- en: 'We mentioned earlier that because the final outputs are 46-dimensional, you
    should avoid intermediate layers with many fewer than 46 units. Now let’s see
    what happens when we introduce an information bottleneck by having intermediate
    layers that are significantly less than 46-dimensional: for example, 4-dimensional.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，由于最终的输出是46维的，你应该避免中间层的单元远远少于46。现在让我们看看当我们引入信息瓶颈时会发生什么，即通过具有明显低于46维的中间层，例如4维：
- en: Listing 4.22 A model with an information bottleneck
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.22 具有信息瓶颈的模型
- en: '[PRE42]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The model now peaks at ~71% validation accuracy, an 8% absolute drop. This drop
    is mostly due to the fact that we’re trying to compress a lot of information (enough
    information to recover the separation hyperplanes of 46 classes) into an intermediate
    space that is too low-dimensional. The model is able to cram *most* of the necessary
    information into these four-dimensional representations, but not all of it.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型的验证准确率达到了约71%，绝对下降了8%。这种下降主要是因为我们试图将大量信息（足以恢复46个类别的分离超平面的信息）压缩到一个过低维度的中间空间中。模型能够将*大部分*必要信息压缩到这些四维表示中，但并非全部。
- en: 4.2.8 Further experiments
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.8 进一步实验
- en: 'Like in the previous example, I encourage you to try out the following experiments
    to train your intuition about the kind of configuration decisions you have to
    make with such models:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前面的例子一样，我鼓励你尝试以下实验，以培养你对这类模型需要做出的配置决策的直觉：
- en: 'Try using larger or smaller layers: 32 units, 128 units, and so on.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用更大或更小的层：32个单元，128个单元等。
- en: You used two intermediate layers before the final softmax classification layer.
    Now try using a single intermediate layer, or three intermediate layers.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最终的softmax分类层之前使用了两个中间层。现在尝试使用一个单独的中间层，或者三个中间层。
- en: 4.2.9 Wrapping up
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.9 总结
- en: 'Here’s what you should take away from this example:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子给我们的启示是：
- en: If you’re trying to classify data points among *N* classes, your model should
    end with a `Dense` layer of size *N*.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你试图在*N*个类别中对数据点进行分类，你的模型应该以大小为*N*的`Dense`层结束。
- en: In a single-label, multiclass classification problem, your model should end
    with a `softmax` activation so that it will output a probability distribution
    over the *N* output classes.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单标签多类分类问题中，你的模型应该以`softmax`激活结束，这样它将输出关于*N*个输出类别的概率分布。
- en: Categorical crossentropy is almost always the loss function you should use for
    such problems. It minimizes the distance between the probability distributions
    output by the model and the true distribution of the targets.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于这类问题，几乎总是应该使用分类交叉熵作为损失函数。它最小化了模型输出的概率分布与目标的真实分布之间的距离。
- en: 'There are two ways to handle labels in multiclass classification:'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多类分类中有两种处理标签的方式：
- en: Encoding the labels via categorical encoding (also known as one-hot encoding)
    and using `categorical_crossentropy` as a loss function
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分类编码（也称为独热编码）对标签进行编码，并使用`categorical_crossentropy`作为损失函数
- en: Encoding the labels as integers and using the `sparse_categorical_crossentropy`
    loss function
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标签编码为整数并使用`sparse_categorical_crossentropy`损失函数
- en: If you need to classify data into a large number of categories, you should avoid
    creating information bottlenecks in your model due to intermediate layers that
    are too small.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你需要将数据分类到大量类别中，你应该避免由于中间层太小而在模型中创建信息瓶颈。
- en: '4.3 Predicting house prices: A regression example'
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 预测房价：回归示例
- en: 'The two previous examples were considered classification problems, where the
    goal was to predict a single discrete label of an input data point. Another common
    type of machine learning problem is *regression*, which consists of predicting
    a continuous value instead of a discrete label: for instance, predicting the temperature
    tomorrow, given meteorological data or predicting the time that a software project
    will take to complete, given its specifications.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的两个例子被视为分类问题，目标是预测输入数据点的单个离散标签。另一种常见的机器学习问题是*回归*，它包括预测连续值而不是离散标签：例如，根据气象数据预测明天的温度，或者根据规格说明预测软件项目完成所需的时间。
- en: Note Don’t confuse *regression* and the *logistic regression* algorithm. Confusingly,
    logistic regression isn’t a regression algorithm—it’s a classification algorithm.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意不要混淆*回归*和*逻辑回归*算法。令人困惑的是，逻辑回归并不是一个回归算法，而是一个分类算法。
- en: 4.3.1 The Boston housing price dataset
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 波士顿房价数据集
- en: 'In this section, we’ll attempt to predict the median price of homes in a given
    Boston suburb in the mid-1970s, given data points about the suburb at the time,
    such as the crime rate, the local property tax rate, and so on. The dataset we’ll
    use has an interesting difference from the two previous examples. It has relatively
    few data points: only 506, split between 404 training samples and 102 test samples.
    And each *feature* in the input data (for example, the crime rate) has a different
    scale. For instance, some values are proportions, which take values between 0
    and 1, others take values between 1 and 12, others between 0 and 100, and so on.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试预测上世纪70年代中期波士顿郊区房屋的中位价格，根据当时有关该郊区的数据点，例如犯罪率、当地财产税率等。我们将使用的数据集与前两个示例有一个有趣的区别。它的数据点相对较少：仅有506个，分为404个训练样本和102个测试样本。输入数据中的每个*特征*（例如犯罪率）具有不同的比例。例如，一些值是比例，取值介于0和1之间，其他值介于1和12之间，其他值介于0和100之间，依此类推。
- en: Listing 4.23 Loading the Boston housing dataset
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.23 加载波士顿房屋数据集
- en: '[PRE43]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let’s look at the data:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下数据：
- en: '[PRE44]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As you can see, we have 404 training samples and 102 test samples, each with
    13 numerical features, such as per capita crime rate, average number of rooms
    per dwelling, accessibility to highways, and so on.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们有404个训练样本和102个测试样本，每个样本有13个数值特征，例如人均犯罪率、每个住宅的平均房间数、高速公路的可达性等。
- en: 'The targets are the median values of owner-occupied homes, in thousands of
    dollars:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 目标值是占有住房的中位数值，以千美元为单位：
- en: '[PRE45]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The prices are typically between $10,000 and $50,000\. If that sounds cheap,
    remember that this was the mid-1970s, and these prices aren’t adjusted for inflation.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 价格通常在$10,000和$50,000之间。如果听起来很便宜，请记住这是上世纪70年代中期，这些价格没有考虑通货膨胀。
- en: 4.3.2 Preparing the data
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 准备数据
- en: 'It would be problematic to feed into a neural network values that all take
    wildly different ranges. The model might be able to automatically adapt to such
    heterogeneous data, but it would definitely make learning more difficult. A widespread
    best practice for dealing with such data is to do feature-wise normalization:
    for each feature in the input data (a column in the input data matrix), we subtract
    the mean of the feature and divide by the standard deviation, so that the feature
    is centered around 0 and has a unit standard deviation. This is easily done in
    NumPy.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 将取值范围差异很大的值输入神经网络可能会有问题。模型可能能够自动适应这种异质数据，但这肯定会使学习变得更加困难。处理这种数据的一种广泛最佳实践是进行特征归一化：对于输入数据中的每个特征（输入数据矩阵中的一列），我们减去该特征的均值并除以标准差，使得该特征以0为中心，具有单位标准差。这在NumPy中很容易实现。
- en: Listing 4.24 Normalizing the data
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.24 归一化数据
- en: '[PRE46]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note that the quantities used for normalizing the test data are computed using
    the training data. You should never use any quantity computed on the test data
    in your workflow, even for something as simple as data normalization.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，用于归一化测试数据的量是使用训练数据计算的。你绝对不应该在工作流程中使用在测试数据上计算的任何量，即使是像数据归一化这样简单的操作也不行。
- en: 4.3.3 Building your model
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 构建你的模型
- en: Because so few samples are available, we’ll use a very small model with two
    intermediate layers, each with 64 units. In general, the less training data you
    have, the worse overfitting will be, and using a small model is one way to mitigate
    overfitting.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可用样本很少，我们将使用一个非常小的模型，其中包含两个中间层，每个层有64个单元。一般来说，训练数据越少，过拟合就会越严重，使用一个小模型是缓解过拟合的一种方法。
- en: Listing 4.25 Model definition
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.25 模型定义
- en: '[PRE47]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: ❶ Because we need to instantiate the same model multiple times, we use a function
    to construct it.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 因为我们需要多次实例化相同的模型，所以我们使用一个函数来构建它。
- en: The model ends with a single unit and no activation (it will be a linear layer).
    This is a typical setup for scalar regression (a regression where you’re trying
    to predict a single continuous value). Applying an activation function would constrain
    the range the output can take; for instance, if you applied a `sigmoid` activation
    function to the last layer, the model could only learn to predict values between
    0 and 1\. Here, because the last layer is purely linear, the model is free to
    learn to predict values in any range.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 模型以一个单元结束，没有激活函数（它将是一个线性层）。这是标量回归的典型设置（一种回归，你试图预测一个单一连续值）。应用激活函数会限制输出的范围；例如，如果在最后一层应用`sigmoid`激活函数，模型只能学习预测0到1之间的值。在这里，因为最后一层是纯线性的，模型可以自由地学习预测任何范围内的值。
- en: Note that we compile the model with the `mse` loss function—*mean squared error*,
    the square of the difference between the predictions and the targets. This is
    a widely used loss function for regression problems.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用`mse`损失函数来编译模型—*均方误差*，即预测值与目标值之间的差的平方。这是回归问题中广泛使用的损失函数。
- en: 'We’re also monitoring a new metric during training: *mean absolute error* (MAE).
    It’s the absolute value of the difference between the predictions and the targets.
    For instance, an MAE of 0.5 on this problem would mean your predictions are off
    by $500 on average.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们还监控一个新的指标：*平均绝对误差*（MAE）。它是预测值与目标值之间的差的绝对值。例如，在这个问题上的MAE为0.5意味着你的预测平均偏差为$500。
- en: 4.3.4 Validating your approach using K-fold validation
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.4 使用K折验证验证你的方法
- en: 'To evaluate our model while we keep adjusting its parameters (such as the number
    of epochs used for training), we could split the data into a training set and
    a validation set, as we did in the previous examples. But because we have so few
    data points, the validation set would end up being very small (for instance, about
    100 examples). As a consequence, the validation scores might change a lot depending
    on which data points we chose for validation and which we chose for training:
    the validation scores might have a high *variance* with regard to the validation
    split. This would prevent us from reliably evaluating our model.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续调整模型参数（例如用于训练的时代数）的同时评估我们的模型，我们可以将数据分割为训练集和验证集，就像我们在之前的示例中所做的那样。但是由于数据点很少，验证集最终会变得非常小（例如，约100个示例）。因此，验证分数可能会根据我们选择用于验证和训练的数据点而变化很大：验证分数可能在验证拆分方面具有很高的*方差*。这将阻止我们可靠地评估我们的模型。
- en: The best practice in such situations is to use *K-fold* cross-validation (see
    figure 4.8).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最佳做法是使用*K折*交叉验证（参见图4.8）。
- en: '![](../Images/04-08.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-08.png)'
- en: Figure 4.8 K-fold cross-validation with K=3
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 K=3的K折交叉验证
- en: It consists of splitting the available data into *K* partitions (typically *K*
    = 4 or 5), instantiating *K* identical models, and training each one on *K* –
    1 partitions while evaluating on the remaining partition. The validation score
    for the model used is then the average of the *K* validation scores obtained.
    In terms of code, this is straightforward.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 它包括将可用数据分割为*K*个分区（通常*K*=4或5），实例化*K*个相同的模型，并在*K*-1个分区上训练每个模型，同时在剩余分区上进行评估。然后使用的模型的验证分数是获得的*K*个验证分数的平均值。在代码方面，这很简单。
- en: Listing 4.26 K-fold validation
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.26 K折交叉验证
- en: '[PRE48]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '❶ Prepares the validation data: data from partition #k'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备验证数据：来自分区#k的数据
- en: '❷ Prepares the training data: data from all other partitions'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 准备训练数据：来自所有其他分区的数据
- en: ❸ Builds the Keras model (already compiled)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 构建Keras模型（已编译）
- en: ❹ Trains the model (in silent mode, verbose = 0)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练模型（静默模式，verbose = 0）
- en: ❺ Evaluates the model on the validation data
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在验证数据上评估模型
- en: 'Running this with `num_epochs` `=` `100` yields the following results:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`num_epochs` `=` `100`运行此操作将产生以下结果：
- en: '[PRE49]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The different runs do indeed show rather different validation scores, from 2.1
    to 3.1\. The average (2.6) is a much more reliable metric than any single score—that’s
    the entire point of K-fold cross-validation. In this case, we’re off by $2,600
    on average, which is significant considering that the prices range from $10,000
    to $50,000.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的运行确实显示了相当不同的验证分数，从2.1到3.1。平均值（2.6）比任何单个分数更可靠—这就是K折交叉验证的全部意义。在这种情况下，我们平均偏差为$2,600，考虑到价格范围为$10,000到$50,000，这是一个显著的差距。
- en: 'Let’s try training the model a bit longer: 500 epochs. To keep a record of
    how well the model does at each epoch, we’ll modify the training loop to save
    the per-epoch validation score log for each fold.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试将模型训练更长一点：500个时代。为了记录模型在每个时代的表现如何，我们将修改训练循环以保存每个折叠的每个时代验证分数日志。
- en: Listing 4.27 Saving the validation logs at each fold
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.27 保存每个折叠的验证日志
- en: '[PRE50]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '❶ Prepares the validation data: data from partition #k'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备验证数据：来自分区#k的数据
- en: '❷ Prepares the training data: data from all other partitions'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 准备训练数据：来自所有其他分区的数据
- en: ❸ Builds the Keras model (already compiled)
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 构建Keras模型（已编译）
- en: ❹ Trains the model (in silent mode, verbose=0)
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练模型（静默模式，verbose=0）
- en: We can then compute the average of the per-epoch MAE scores for all folds.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以计算所有折叠的每个时代MAE分数的平均值。
- en: Listing 4.28 Building the history of successive mean K-fold validation scores
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.28 构建连续平均K折验证分数的历史
- en: '[PRE51]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Let’s plot this; see figure 4.9.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这个；参见图4.9。
- en: '![](../Images/04-09.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-09.png)'
- en: Figure 4.9 Validation MAE by epoch
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 按时代划分的验证MAE
- en: Listing 4.29 Plotting validation scores
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.29 绘制验证分数
- en: '[PRE52]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'It may be a little difficult to read the plot, due to a scaling issue: the
    validation MAE for the first few epochs is dramatically higher than the values
    that follow. Let’s omit the first 10 data points, which are on a different scale
    than the rest of the curve.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缩放问题，可能有点难以阅读图表：前几个时代的验证MAE远高于后续数值。让我们省略前10个数据点，这些数据点与曲线的其余部分处于不同的比例尺。
- en: Listing 4.30 Plotting validation scores, excluding the first 10 data points
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.30 绘制验证分数，不包括前10个数据点
- en: '[PRE53]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As you can see in figure 4.10, validation MAE stops improving significantly
    after 120–140 epochs (this number includes the 10 epochs we omitted). Past that
    point, we start overfitting.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在图4.10中所看到的，验证MAE在120-140个时代后停止显着改善（这个数字包括我们省略的10个时代）。在那之后，我们开始过拟合。
- en: '![](../Images/04-10.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-10.png)'
- en: Figure 4.10 Validation MAE by epoch, excluding the first 10 data points
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 按时代划分的验证MAE，不包括前10个数据点
- en: Once you’re finished tuning other parameters of the model (in addition to the
    number of epochs, you could also adjust the size of the intermediate layers),
    you can train a final production model on all of the training data, with the best
    parameters, and then look at its performance on the test data.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您完成调整模型的其他参数（除了时代数，您还可以调整中间层的大小），您可以使用最佳参数在所有训练数据上训练最终的生产模型，然后查看其在测试数据上的表现。
- en: Listing 4.31 Training the final model
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.31 训练最终模型
- en: '[PRE54]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: ❶ Gets a fresh, compiled model
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取一个新的、已编译的模型
- en: ❷ Trains it on the entirety of the data
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在所有数据上对其进行训练
- en: 'Here’s the final result:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最终结果：
- en: '[PRE55]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We’re still off by a bit under $2,500\. It’s an improvement! Just like with
    the two previous tasks, you can try varying the number of layers in the model,
    or the number of units per layer, to see if you can squeeze out a lower test error.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然有一点不到$2,500的差距。这是一个进步！就像前两个任务一样，您可以尝试改变模型中的层数或每层的单元数，看看是否可以减少测试误差。
- en: 4.3.5 Generating predictions on new data
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.5 在新数据上生成预测
- en: 'When calling `predict()` on our binary classification model, we retrieved a
    scalar score between 0 and 1 for each input sample. With our multiclass classification
    model, we retrieved a probability distribution over all classes for each sample.
    Now, with this scalar regression model, `predict()` returns the model’s guess
    for the sample’s price in thousands of dollars:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的二元分类模型上调用`predict()`时，我们为每个输入样本检索到介于0和1之间的标量分数。对于我们的多类分类模型，我们为每个样本检索到所有类别的概率分布。现在，对于这个标量回归模型，`predict()`返回模型对样本价格的猜测，单位为千美元：
- en: '[PRE56]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The first house in the test set is predicted to have a price of about $10,000\.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集中的第一栋房子预测价格约为$10,000。
- en: 4.3.6 Wrapping up
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.6 总结
- en: 'Here’s what you should take away from this scalar regression example:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个标量回归示例中，您应该得出以下结论：
- en: Regression is done using different loss functions than we used for classification.
    Mean squared error (MSE) is a loss function commonly used for regression.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归使用不同的损失函数进行，与我们用于分类的不同。均方误差（MSE）是回归常用的损失函数。
- en: Similarly, evaluation metrics to be used for regression differ from those used
    for classification; naturally, the concept of accuracy doesn’t apply for regression.
    A common regression metric is mean absolute error (MAE).
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，用于回归的评估指标与用于分类的评估指标不同；自然地，准确性的概念不适用于回归。常见的回归指标是平均绝对误差（MAE）。
- en: When features in the input data have values in different ranges, each feature
    should be scaled independently as a preprocessing step.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当输入数据中的特征具有不同范围的值时，每个特征应作为预处理步骤独立缩放。
- en: When there is little data available, using K-fold validation is a great way
    to reliably evaluate a model.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据量很少时，使用K折验证是可靠评估模型的好方法。
- en: When little training data is available, it’s preferable to use a small model
    with few intermediate layers (typically only one or two), in order to avoid severe
    overfitting.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当可用的训练数据很少时，最好使用只有少数中间层（通常只有一个或两个）的小型模型，以避免严重过拟合。
- en: Summary
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The three most common kinds of machine learning tasks on vector data are binary
    classification, multiclass classification, and scalar regression.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在向量数据上，机器学习任务的三种最常见类型是二元分类、多类分类和标量回归。
- en: The “Wrapping up” sections earlier in the chapter summarize the important points
    you’ve learned regarding each task.
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章前面的“总结”部分总结了您对每个任务学到的重要知识点。
- en: Regression uses different loss functions and different evaluation metrics than
    classification.
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归使用不同的损失函数和不同的评估指标，与分类不同。
- en: You’ll usually need to preprocess raw data before feeding it into a neural network.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将原始数据输入神经网络之前，通常需要对其进行预处理。
- en: When your data has features with different ranges, scale each feature independently
    as part of preprocessing.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您的数据具有不同范围的特征时，作为预处理的一部分，应独立缩放每个特征。
- en: As training progresses, neural networks eventually begin to overfit and obtain
    worse results on never-before-seen data.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着训练的进行，神经网络最终开始过拟合，并在以前未见过的数据上获得更糟糕的结果。
- en: If you don’t have much training data, use a small model with only one or two
    intermediate layers, to avoid severe overfitting.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您没有太多的训练数据，可以使用只有一个或两个中间层的小型模型，以避免严重过拟合。
- en: If your data is divided into many categories, you may cause information bottlenecks
    if you make the intermediate layers too small.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的数据被分成许多类别，如果将中间层设置得太小，可能会导致信息瓶颈。
- en: When you’re working with little data, K-fold validation can help reliably evaluate
    your model.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您处理少量数据时，K折验证可以帮助可靠评估您的模型。
