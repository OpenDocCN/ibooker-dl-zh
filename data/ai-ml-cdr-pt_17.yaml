- en: Chapter 16\. Using LLMs with Custom Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章\. 使用自定义数据与LLM交互
- en: In [Chapter 15](ch15.html#ch15_transformers_and_transformers_1748549808974580),
    we looked at Transformers and how their encoder, decoder, and encoder-decoder
    architectures work. The results of their revolutionizing NLP can’t be disputed!
    Then, we looked at transformers, which form the Python library from Hugging Face
    that’s designed to make it easier to use Transformers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第15章](ch15.html#ch15_transformers_and_transformers_1748549808974580)中，我们探讨了Transformers以及它们的编码器、解码器和编码器-解码器架构是如何工作的。它们在革命性NLP方面的成果不容置疑！然后，我们探讨了Hugging
    Face的Python库中的transformers，该库旨在使使用Transformers更加容易。
- en: Large Transformer-based models, which are trained on vast amounts of text, are
    very powerful, but they aren’t always ideal for specific tasks or domains. In
    this chapter, we’ll look at how you can use transformers and other APIs to adapt
    these models to your specific needs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大型Transformer的模型，在大量文本上训练，非常强大，但它们并不总是适合特定的任务或领域。在本章中，我们将探讨如何使用transformers和其他API来适应这些模型以满足您的特定需求。
- en: Fine-tuning allows you to customize pretrained models with your specific data.
    You could use this approach to create a chatbot, improve classification accuracy,
    or develop text generation for a more specific domain.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 微调允许您使用您特定的数据来定制预训练模型。您可以使用这种方法来创建聊天机器人，提高分类准确率，或者为更具体的领域开发文本生成。
- en: There are several techniques for doing this, including traditional fine-tuning
    and parameter-efficient tuning with methods like LoRA and parameter-efficient
    fine-tuning (PEFT). You can also get more out of your LLMs with retrieval-augmented
    generation (RAG), which we’ll explore in [Chapter 18](ch18.html#ch18_introduction_to_rag_1748550073472936).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以做到这一点，包括传统的微调以及使用LoRA和参数高效微调（PEFT）等方法的参数高效微调。您还可以通过检索增强生成（RAG）从您的LLM中获得更多收益，我们将在[第18章](ch18.html#ch18_introduction_to_rag_1748550073472936)中探讨。
- en: In this chapter, we’ll explore some hands-on examples, starting with traditional
    fine-tuning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索一些实际示例，从传统的微调开始。
- en: Fine-Tuning an LLM
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调LLM
- en: Let’s take a look, step by step, at how to fine-tune an LLM like BERT. We’ll
    take the IMDb database and fine-tune the model on it to be better at detecting
    sentiment in movie reviews. There are a number of steps involved in doing this,
    so we’ll look at each one in detail.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地看看如何微调一个像BERT这样的LLM。我们将使用IMDb数据库，并在其上微调模型以更好地检测电影评论中的情感。这个过程涉及多个步骤，所以我们将详细查看每一个。
- en: Setup and Dependencies
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置和依赖项
- en: 'We’ll start by setting up everything that we need to do fine-tuning with PyTorch.
    In addition to the basics, there are three new things that you’ll need to include:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先设置所有我们需要用PyTorch进行微调的东西。除了基础知识外，您还需要包括以下三个新内容：
- en: Datasets
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集
- en: We covered datasets in [Chapter 4](ch04.html#ch04_using_data_with_pytorch_1748548966496246).
    We’re going to use these to load the IMDb dataset and the built-in splits for
    training and testing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4章](ch04.html#ch04_using_data_with_pytorch_1748548966496246)中介绍了数据集。我们将使用这些数据集来加载IMDb数据集和内置的训练和测试分割。
- en: Evaluate
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 评估
- en: This library provides metrics for measuring load performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库提供了用于衡量加载性能的指标。
- en: transformers
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: transformers
- en: As we covered in Chapters [14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)
    and [15](ch15.html#ch15_transformers_and_transformers_1748549808974580), the transformers
    Hugging Face library is designed to make using LLMs much easier.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第14章](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)和[第15章](ch15.html#ch15_transformers_and_transformers_1748549808974580)中所述，Hugging
    Face的transformers库旨在使使用LLM变得更加容易。
- en: 'We’ll use some classes from the Hugging Face transformers library for this
    chapter’s fine-tuning exercise. These include the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这个章节的微调练习中使用Hugging Face transformers库的一些类。这些包括以下内容：
- en: AutoModelForSequenceClassification
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AutoModelForSequenceClassification
- en: This class loads pretrained models for classification tasks and adds a classification
    head to the top of the base model. This classification head is then optimized
    for the specific classification scenario you are fine-tuning for, instead of being
    a generic model. If we specify the checkpoint name, it will automatically handle
    the model architecture for us. So, to use the BERT model with a linear classifier
    layer, we’ll use `bert-base-uncased`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类加载用于分类任务的预训练模型，并在基础模型顶部添加一个分类头。这个分类头随后将针对您正在微调的特定分类场景进行优化，而不是成为一个通用模型。如果我们指定了检查点名称，它将自动为我们处理模型架构。因此，要使用BERT模型和线性分类器层，我们将使用`bert-base-uncased`。
- en: AutoTokenizer
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: AutoTokenizer
- en: This class automatically initializes the appropriate tokenizer. This converts
    text to the appropriate tokens and adds the appropriate special tokens, padding,
    truncation, etc.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类会自动初始化适当的分词器。这会将文本转换为适当的标记，并添加适当的特殊标记、填充、截断等。
- en: TrainingArguments
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: TrainingArguments
- en: This class lets us configure the training settings and all the hyperparameters,
    as well as setting up things like the device to use.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类让我们可以配置训练设置和所有超参数，以及设置诸如要使用的设备等事项。
- en: Trainer
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Trainer
- en: This class manages the training loop on your behalf, handling batching, optimization,
    loss, backpropagation, and everything you need to retrain the model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类代表你管理训练循环，处理批处理、优化、损失、反向传播以及你需要重新训练模型的所有内容。
- en: DataCollatorWithPadding
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DataCollatorWithPadding
- en: The number of records in the dataset doesn’t always line up with the batch size.
    This class therefore efficiently batches examples to the appropriate batch sizes
    while also handling details like attention masks and other model-specific inputs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的记录数量并不总是与批大小相匹配。因此，这个类有效地将示例分批到适当的批大小，同时处理诸如注意力掩码和其他模型特定输入等细节。
- en: 'We can see this in code here:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在代码中看到这一点：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that the dependencies are in place, we’ll load the data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有依赖项都已就绪，我们将加载数据。
- en: Loading and Examining the Data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据并检查
- en: 'Next up, let’s load our data by using the datasets API. We’ll also explore
    the test and training dataset sizes. You can use the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用datasets API加载数据。我们还将探索测试和训练数据集的大小。你可以使用以下代码：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'That will output the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The next step is to initialize the model and the tokenizer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是初始化模型和分词器。
- en: Initializing the Model and Tokenizer
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化模型和分词器
- en: 'We’ll use the `bert-base-uncased` model in this example, so we need to initialize
    it by using `AutoModelForSequenceClassification` and getting its associated tokenizer:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用`bert-base-uncased`模型，因此我们需要使用`AutoModelForSequenceClassification`初始化它并获取其关联的分词器：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note the `AutoModelForSequenceClassification` needs to be initialized with the
    number of labels that we want to classify for. This defines the new classification
    head with two labels. The IMDb database that we’ll be using has two labels for
    positive and negative sentiment, so we’ll retrain for that.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`AutoModelForSequenceClassification`需要用我们想要分类的标签数量进行初始化。这定义了一个具有两个标签的新分类头。我们将使用的IMDb数据库有两个标签，分别表示正面和负面情感，因此我们将针对这一点进行重新训练。
- en: At this point, it’s also a good idea to specify the device that the model will
    run on. Training with this model is computationally intensive, and if you’re using
    Colab, you’ll likely need a high-RAM GPU like an A100\. Training with that will
    take a couple of minutes, but it can take many hours on a CPU!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，指定模型将运行的设备也是一个好主意。使用这个模型进行训练是计算密集型的，如果你使用Colab，你可能需要一个高RAM的GPU，比如A100。使用它将需要几分钟，但在CPU上可能需要几个小时！
- en: Preprocessing the Data
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Once we have the data, we want to preprocess it just to get what we need to
    train. The first step in this, of course, will be to tokenize the text, and the
    `preprocess` function here handles that, giving a sequence length of 512 characters
    with padding:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了数据，我们想要对其进行预处理，只是为了得到我们训练所需的内容。当然，这一步的第一步将是分词文本，这里的`preprocess`函数处理这一点，给出一个带有填充的512个字符的序列长度：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: One important note here is that the original data came with columns for `text`
    denoting the review and `label` being 0 or 1 for negative or positive sentiment.
    However, we don’t *need* the `text` column to train the data, and the Hugging
    Face Trainer (which we will see in a moment) expects the column containing the
    label to be called `labels` (plural). Therefore, you’ll see that we remove all
    of the columns in the original dataset, and the tokenized dataset will have the
    tokenized data and a column called `labels` instead of `label`, with the original
    values copied over.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个重要注意事项是，原始数据带有表示评论的`text`列和表示负面或正面情感的`label`列（0或1）。然而，我们**不需要**`text`列来训练数据，并且Hugging
    Face Trainer（我们将在稍后看到）期望包含标签的列被命名为`labels`（复数）。因此，你会看到我们移除了原始数据集中的所有列，并且标记化数据集将包含标记化数据和名为`labels`的列，而不是`label`，原始值被复制过来。
- en: Collating the Data
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集
- en: When we’re dealing with passing sequenced, tokenized data into a model in batches,
    there can be differences in batch or sequence size that need processing. In our
    case, we shouldn’t have to worry about the sequence size because we used a tokenizer
    that forces the length to be 512 (in the previous set). However, as part of the
    transformers library, the collator classes are still equipped to deal with it,
    and we’ll be using them to ensure consistent batch sizing.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们批量处理序列化、分词数据输入模型时，可能会存在批或序列大小差异需要处理。在我们的案例中，我们不需要担心序列大小，因为我们使用了一个将长度强制设置为512（在之前的设置中）的分词器。然而，作为transformers库的一部分，collator类仍然能够处理它，我们将使用它们来确保批大小的一致性。
- en: So ultimately, the role of the `DataCollatorWithPadding` class is to take multiple
    examples of different lengths, provide padding if and when necessary, convert
    the inputs into tensors, and create attention masks if necessary.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`DataCollatorWithPadding`类的最终作用是接受不同长度的多个示例，在必要时提供填充，将输入转换为张量，并在必要时创建注意力掩码。
- en: In our case, we’re really only getting the conversion to tensors for input to
    the model, but it’s still good practice to use `DataCollatorWithPadding` if we
    want to change anything in the tokenization process later.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们实际上只是在将数据转换为张量以供模型输入，但如果以后想要在分词过程中进行任何更改，使用`DataCollatorWithPadding`仍然是一个好的实践。
- en: 'Here’s the code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Defining Metrics
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义指标
- en: 'Now, let’s define some metrics that we want to capture as we’re training the
    model. We’ll just do accuracy, where we compare the predicted value to the actual
    value. Here’s some simple code to achieve that:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一些我们希望在训练模型时捕获的指标。我们将只做准确度，即比较预测值和实际值。以下是一些简单的代码来实现这一点：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It’s using `evaluate.load` from Hugging Face’s evaluate library, which provides
    a simple standardized interface that’s specifically designed for tasks like this
    one. It can handle the heavy lifting for us, instead of requiring us to roll our
    own metrics, and for an evaluate task, we simply pass it the set of predictions
    and the set of labels and have it do the computation. The evaluate library is
    prebuilt to handle a number of metrics, including f1, BLEU, and many others.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用了Hugging Face的evaluate库中的`evaluate.load`，该库提供了一个简单标准化的接口，专门为这类任务设计。它可以为我们处理繁重的工作，而不是要求我们自行创建指标，对于评估任务，我们只需传递预测集和标签集，然后由它进行计算。evaluate库预先构建，可以处理包括f1、BLEU在内的多种指标。
- en: Configuring Training
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置训练
- en: Next up, we can configure *how* the model will retrain by using the `TrainingArguments`
    object. This offers a large variety of hyperparameters you can set—including those
    for the learning rate, weight decay, etc., as used by the `optimizer` and `loss`
    function. It’s designed to give you granular control over the learning process
    while abstracting away the complexity.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过使用`TrainingArguments`对象来配置模型如何重新训练。这提供了一系列你可以设置的超参数，包括用于学习率、权重衰减等，这些参数由`optimizer`和`loss`函数使用。它旨在让你对学习过程有更细致的控制，同时简化复杂性。
- en: 'Here’s the set that I used for fine-tuning with IMDb:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我用于与IMDb进行微调的设置：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It’s important to note and tweak hyperparameters for different results. In addition
    to the aforementioned ones for the optimizer, you’ll want to consider the batch
    sizes. You can set different parameters for training or evaluation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意并调整超参数以获得不同的结果非常重要。除了上述用于优化器的参数外，你还需要考虑批大小。你可以为训练或评估设置不同的参数。
- en: One very useful parameter—in particular for training sessions that are longer
    than the three epochs here—is `load_best_model_at_end`. Instead of always using
    the final checkpoint, it will keep track of the best checkpoint according to the
    specified metric (in this case, accuracy) and will load that one when it’s done.
    And because I set the `evaluation` and `save` strategies to `epoch`, it will only
    do this at the end of an epoch.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有用的参数——特别是对于超过这里三个epoch的训练会话——是`load_best_model_at_end`。它不会总是使用最后的检查点，而是会根据指定的指标（在这种情况下是准确度）跟踪最佳的检查点，并在完成后加载它。由于我将`evaluation`和`save`策略设置为`epoch`，它只会在epoch结束时这样做。
- en: 'Note also the `report_to` parameter: the training uses `weights and biases`
    as the backend for reporting by default. I set `report_to` to `none` to turn off
    this reporting. If you want to keep it, you’ll need a Weights and Biases API key.
    You can get this very easily from the status window or by going to the [Weights
    and Biases website](https://oreil.ly/yMX1A). As you train, you’ll be asked to
    paste in this API key. Be sure to do so before you walk away, particularly if
    you are paying for compute units on Colab.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `report_to` 参数：默认情况下，训练使用 `weights and biases` 作为报告的后端。我将 `report_to` 设置为
    `none` 以关闭此报告。如果你想保留它，你需要一个 Weights and Biases API 密钥。你可以很容易地从状态窗口或通过访问 [Weights
    and Biases 网站](https://oreil.ly/yMX1A) 获取这个密钥。在训练过程中，你会被要求粘贴这个 API 密钥。确保在离开之前完成此操作，尤其是如果你在
    Colab 上支付计算单元费用的话。
- en: There’s a wealth of parameters to experiment with, and being able to parameterize
    easily like this also allows you easily to do a neural architecture search with
    tools like [Ray Tune](https://oreil.ly/fDAhG).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多参数可以进行实验，并且能够这样轻松地进行参数化，也允许你使用像 [Ray Tune](https://oreil.ly/fDAhG) 这样的工具轻松地进行神经架构搜索。
- en: Initializing the Trainer
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化 Trainer
- en: As with the training parameters, transformers give you a trainer class that
    you can use alongside them to encapsulate a full training cycle.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练参数一样，transformers 提供了一个 trainer 类，你可以与它们一起使用来封装一个完整的训练周期。
- en: 'You initialize it with the model, the training arguments, the data, the collator,
    and the metrics strategy that you’ve previously initialized. All the previous
    steps build up to this. Here’s the code you’ll need:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过模型、训练参数、数据、collator 以及你之前初始化的度量策略来初始化它。所有之前的步骤都是为了这一步做准备。以下是你需要用到的代码：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training and Evaluation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和评估
- en: With everything now set up, it becomes as simple as calling the `train()` method
    on the trainer to do the training and the `evaluate()` method to do the evaluation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切都已经设置好了，调用 trainer 上的 `train()` 方法进行训练，以及 `evaluate()` 方法进行评估，变得非常简单。
- en: 'Here’s the code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As an example, while you could train this model with the free tiers in Google
    Colab, your experience in timing might vary. With the CPU alone, it can take many
    hours. I trained this model with the T4 High Ram GPU, which costs 1.6 compute
    units per hour. The entire training process was about 50 minutes, but I’ll round
    that up to an hour to include all the downloading and setup. At the time of writing,
    a pro Colab subscription gets one hundred compute Units with the US$9.99 per month
    subscription. You could also choose the A100 GPU, which is much faster (training
    took me about 12 minutes with it) but also more expensive, at about 6.8 compute
    units per hour.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，虽然你可以在 Google Colab 的免费层中训练这个模型，但你的计时体验可能会有所不同。仅使用 CPU，可能需要好几个小时。我用 T4 高内存
    GPU 训练了这个模型，每小时需要 1.6 个计算单元。整个训练过程大约需要 50 分钟，但我将其四舍五入到一小时，包括所有下载和设置。在撰写本文时，专业
    Colab 订阅每月 9.99 美元可以获得一百个计算单元。你也可以选择 A100 GPU，它要快得多（使用它训练我大约只需要 12 分钟），但价格也更贵，每小时大约
    6.8 个计算单元。
- en: 'After training, the results looked like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，结果看起来是这样的：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see quite high accuracy on the evaluation dataset (about 94%) after only
    three epochs, which is a good sign—but of course, there may be overfitting going
    on that would require a separate evaluation. But after about 12 minutes of work
    fine-tuning an LLM, we’re clearly moving in the right direction!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估数据集上，我们只经过三个周期就看到了相当高的准确率（大约 94%），这是一个好兆头——但当然，可能存在过拟合，这需要单独的评估。但在大约 12 分钟的
    LLM 微调工作后，我们显然正在朝着正确的方向前进！
- en: Saving and Testing the Model
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和测试模型
- en: 'Once we’ve trained the model, it’s a good idea to save it out for future use,
    and the `trainer` object makes this easy:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了模型，保存它以供将来使用是个好主意，而 `trainer` 对象使得这一点变得简单：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once we’ve saved the model, we can start using it. To that end, let’s create
    a helper function that takes in the input text, tokenizes it, and then turns those
    tokens into a set of input vectors of keys and values (k, v):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 保存模型后，我们可以开始使用它。为此，让我们创建一个辅助函数，该函数接受输入文本，对其进行标记化，然后将这些标记转换为键值（k, v）的一组输入向量：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can then use PyTorch in inference mode to get the outputs from those inputs
    and turn them into a set of predictions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 PyTorch 的推理模式从这些输入中获取输出，并将它们转换为一组预测：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The returned predictions will be a tensor with two dimensions. Neuron 0 is
    the probability that the prediction is negative, and neuron 1 is the probability
    that the prediction is positive. Therefore, we can look at the positive probability
    and return a sentiment and confidence with its values. We could also have done
    the same with the negative one; it’s purely arbitrary:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的预测将是一个二维张量。神经元0是预测为负的概率，神经元1是预测为正的概率。因此，我们可以查看正概率，并返回一个带有其值的情感和置信度。我们也可以对负概率做同样的处理；这完全是任意的：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now test the prediction with code like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用如下代码进行预测测试：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And the output would look something like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can see that this statement is positive, with high confidence!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这个声明是积极的，并且信心很高！
- en: In this process, you can see how, step by step, you can fine-tune an existing
    LLM on new data to turn it into a classification engine! In many circumstances,
    this may be overkill (and training your own model instead of fine-tuning an LLM
    may be quicker and cheaper), but it’s certainly worth evaluating this process.
    Sometimes, even untuned LLMs will work well for classification! In my experience,
    using the general artificial-understanding nature of LLMs will lead to the creation
    of far more effective classifiers with stronger results.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，您可以逐步看到如何对现有LLM进行微调，以在新数据上将其转变为分类引擎！在许多情况下，这可能有些过度（而且训练自己的模型而不是微调LLM可能更快、更便宜），但评估这个过程肯定是有价值的。有时，甚至未经调整的LLM在分类方面也能很好地工作！根据我的经验，利用LLM的一般人工理解性质将导致创建出更有效的分类器，并产生更强的结果。
- en: Prompt-Tuning an LLM
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示调整LLM
- en: A lightweight alternative to fine-tuning is *prompt tuning,* in which you can
    adapt a model to specific tasks. With prompt tuning, you do this by prepending
    trainable *soft prompts* to each input instead of modifying the model weights.
    These soft prompts will then be optimized during training.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与微调相比，*提示调整*是一种轻量级的替代方案，您可以通过向每个输入前缀可训练的*软提示*来调整模型以适应特定任务。在提示调整中，您通过修改模型权重而不是修改模型权重来完成此操作。这些软提示将在训练过程中进行优化。
- en: These soft prompts are like learned instructions that guide the model’s behavior.
    Unlike discrete text prompts (such as `Classify the sentiment`), the idea of soft
    prompts is that they exist in the model’s embedding space as continuous vectors.
    So, for example, when processing “This movie was great,” the model would see “[V1][V2]…[V20]This
    movie was great.” In this case, [V1][V2]...[V20] are vectors that will help steer
    the model toward the desired classification.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些软提示就像学习到的指令，可以引导模型的行为。与离散文本提示（如`Classify the sentiment`）不同，软提示的想法是它们存在于模型的嵌入空间中作为连续向量。例如，当处理“这部电影太棒了”时，模型会看到“[V1][V2]…[V20]这部电影太棒了”。在这种情况下，[V1][V2]...[V20]是帮助将模型引导到所需分类的向量。
- en: Ultimately, the advantage here is efficiency. So instead of fine-tuning a model,
    amending its weights for each task, and saving the entire model for reuse, you
    only need to save the soft prompt vectors. These are much smaller, and they can
    help you have a suite of fine-tunes that you can easily use to guide the model
    to a specific task without needing to manage multiple models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这里的优势在于效率。所以，您不需要微调模型，修改每个任务的权重，并保存整个模型以供重用，您只需要保存软提示向量。这些向量要小得多，并且可以帮助您拥有一系列微调，您可以使用它们轻松地引导模型执行特定任务，而无需管理多个模型。
- en: Prompt tuning like this can actually match or exceed the performance of full
    fine-tuning, particularly with larger models, and it’s significantly more efficient.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的提示调整实际上可以匹配或超过完整微调的性能，尤其是在较大的模型中，并且效率显著更高。
- en: Now, let’s explore how to prompt-tune the BART LLM with the IMDb dataset in
    direct comparison to the fine-tuning earlier in this chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探讨如何使用IMDb数据集直接比较本章早些时候的微调来提示调整BART LLM。
- en: Preparing the Data
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'Let’s start by preparing our data, loading it from the IMDb dataset, and setting
    up the virtual tokens. Here’s the code you’ll need:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从准备我们的数据开始，从IMDb数据集中加载数据，并设置虚拟标记。以下是您需要的代码：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This will also tokenize our incoming examples so you should note that the maximum
    length of any example will now be reduced by the number of virtual tokens. So,
    for example, with BERT, we have a maximum length of 512, but if we’re going to
    have 20 virtual tokens, then the sequence maximum length will now be 492.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这也将对输入的示例进行分词，所以你应该注意，任何示例的最大长度现在将减少虚拟标记的数量。例如，对于BERT，我们有一个最大长度为512，但如果我们有20个虚拟标记，那么序列最大长度现在将是492。
- en: We’ll now load a subset of the data and try with 5,000 examples, instead of
    25,000\. You can experiment with this number and trade off smaller amounts for
    faster training against larger amounts for better accuracy.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将加载数据的一个子集，并尝试使用5,000个示例，而不是25,000个。你可以对这个数字进行实验，以较小的样本量换取更快的训练速度，或者以较大的样本量换取更高的准确性。
- en: 'First, we’ll create the indices that we want to take from the dataset for training,
    and we’ll test them. Think of these as pointers to the records we’re interested
    in. We’re randomly sampling here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建从数据集中提取用于训练的索引，并对它们进行测试。将这些视为指向我们感兴趣记录的指针。在这里我们进行随机采样：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Then, the last two lines define the mapping function, which simply takes the
    values from the dataset we’re interested in and tokenizes them. We’ll see that
    in the next step.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，最后两行定义了映射函数，它简单地从我们感兴趣的数据库中提取值并对它们进行分词。我们将在下一步中看到这一点。
- en: Creating the Data Loaders
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据加载器
- en: 'Now that we have sets of tokenized training and test data, we want to turn
    them into data loaders. We’ll do this by first selecting the raw examples from
    the underlying data that match the content in our indices:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了分词的训练和测试数据集，我们希望将它们转换为数据加载器。我们将通过首先从底层数据中选择与我们的索引内容匹配的原始示例来完成此操作：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we’ll set the format of the data that we’re interested in. There may
    be many columns in a dataset, but you won’t use them all for training. In this
    case, we’ll want the `input_ids`, which are the tokenized versions of our input
    content; the `attention_mask`, which is a set of vectors that tells us which tokens
    in the `input_ids` we should be interested in (this has the effect of filtering
    out padding or other nonsemantic tokens); and the label:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将设置我们感兴趣的数据格式。数据集中可能有多个列，但你在训练中不会使用它们全部。在这种情况下，我们想要`input_ids`，这是我们输入内容的分词版本；`attention_mask`，它是一组向量，告诉我们应该关注`input_ids`中的哪些标记（这具有过滤掉填充或其他非语义标记的效果）；以及标签：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we can specify the DataLoader that takes these training and test sets.
    I have a large batch size here because I was testing on a 40Gb GRAM GPU in Colab.
    In your environment, you may need to adjust these:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以指定用于这些训练和测试集的数据加载器。在这里我有一个较大的批量大小，因为我是在Colab上的40Gb GRAM GPU上进行测试。在你的环境中，你可能需要调整这些：
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now that the data is processed and loaded into DataLoaders, we can go to the
    next step: defining the model.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经处理并加载到数据加载器中，我们可以进行下一步：定义模型。
- en: Defining the Model
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义模型
- en: 'First, let’s see how to instantiate the model, and then we can go back to the
    raw definition. Typically, in our code, once we’ve set up our DataLoaders, we’ll
    want to create an instance of the model. We’ll use code like this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何实例化模型，然后我们可以回到原始定义。通常，在我们的代码中，一旦我们设置了数据加载器，我们就会想要创建一个模型的实例。我们将使用如下代码：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This keeps it nice and simple, and we’ll encapsulate the underlying BERT in
    an override for a prompt-tuning version. Now, as nice as it would be for transformers
    to have one, they don’t, so we need to create this class for ourselves.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得代码既简洁又清晰，我们将把底层的BERT封装在一个提示调整版本的覆盖中。现在，虽然对于转换器来说有一个是很好的，但它们并没有，所以我们需要为自己创建这个类。
- en: 'As we would with any PyTorch class that defines a model, we’ll create it with
    an `__init__` method to set it up and a forward method that PyTorch’s training
    loop will call during the forward pass. So, let’s start with the `__init__` method
    and the class definition:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们会用任何定义模型的PyTorch类一样，我们将通过一个`__init__`方法来创建它以进行设置，以及一个PyTorch训练循环在正向传递期间会调用的正向方法。所以，让我们从`__init__`方法和类定义开始：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: There’s a lot going on here, so let’s break it down little by little. First
    of all, I set the defaults for the `num_virtual_tokens` to 50 and the `max_length`
    default to 512\. If you don’t specify your own defaults when you instantiate the
    class, you’ll get these values. In this case, the calling code sets them to 20
    and 512, respectively, but you’re free to experiment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多事情在进行中，所以让我们一点一点地分解它。首先，我将`num_virtual_tokens`的默认值设置为50，将`max_length`的默认值设置为512。如果你在实例化类时没有指定自己的默认值，你会得到这些值。在这种情况下，调用代码将它们分别设置为20和512，但你完全可以根据自己的需要进行实验。
- en: 'Next, the code sets up the transformers `AutoModelForSequenceClassification`
    class to get BERT:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，代码设置了transformers的`AutoModelForSequenceClassification`类以获取BERT：
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As with fine-tuning for IMDb, we’re interested in training the model to recognize
    two labels, so they’re set up here. However, one difference from fine-tuning is
    that we’re not going to change any of the weights within the BERT model itself,
    so we set that we don’t want gradients and freeze it like this:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与为IMDb进行的微调一样，我们感兴趣的是训练模型以识别两个标签，所以它们在这里被设置。然而，与微调的一个不同之处在于，我们不会改变BERT模型内部的任何权重，所以我们设置我们不希望梯度并像这样冻结它：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The secret sauce in generating the soft prompts that we’re going to use comes
    at the end of the init. We’ll create a vector to contain our number of virtual
    tokens, and I just initialized it with random tokens from the vocabulary. There
    are smarter things that we might do here to make training more efficient over
    time, but for the sake of simplicity, let’s go with this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的软提示的秘密配方在初始化的末尾。我们将创建一个向量来包含我们的虚拟标记数量，我刚刚用词汇表中的随机标记初始化了它。我们可能在这里做更聪明的事情来使训练在长时间内更有效率，但为了简单起见，让我们就这样做：
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The pretrained BERT model in transformers comes with embeddings, so we can
    use them to turn our list of random tokens into embeddings:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在transformers库中的预训练BERT模型自带嵌入层，因此我们可以使用它们将我们的随机标记列表转换为嵌入层：
- en: '[PRE27]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Importantly, we should now specify that the `prompt_embeddings` are parameters
    of the neural network. This will be important later, when we define the optimizer.
    We recently specified that all of the BERT parameters were frozen, but *these*
    parameters are not part of that and thus are not frozen, so they will be tweaked
    by the optimizer during training:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们现在应该指定`prompt_embeddings`是神经网络的参数。这将在我们定义优化器时变得很重要。我们最近指定了所有的BERT参数都被冻结，但*这些*参数不包含在其中，因此它们不会被冻结，所以它们将在训练过程中被优化器调整：
- en: '[PRE28]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We have now initialized a subclassed version of the tunable BERT, specified
    that we don’t want to amend its gradients, and created a set of soft prompts that
    we will append to the examples as we’re training—and we’ll tweak only those soft
    prompts to soft-tune the two output neurons.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在初始化了一个可调整的BERT的子类版本，指定我们不希望修改其梯度，并创建了一组软提示，我们将在训练过程中将其附加到示例上——并且我们只调整这些软提示以软调整两个输出神经元。
- en: 'Now, let’s look at the `forward` function that will be called during the forward
    pass at training time. Given that we’ve set up everything, this is pretty straightforward:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在训练时前向传递过程中将被调用的`forward`函数。鉴于我们已经设置了一切，这个过程相当直接：
- en: '[PRE29]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s look at it step-by-step. During the forward pass in training, this function
    will be passed batches of data. Therefore, we need to understand what the size
    of this batch is and then extract the `input_ids` (the tokens for the values read
    from the dataset) and the attention mask for that particular ID:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地看看。在训练过程中的前向传递中，这个函数将传递数据批次。因此，我们需要了解这个批次的大小，然后提取`input_ids`（从数据集中读取的值的标记）和特定ID的注意力掩码：
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We’ll also need to convert the `input_ids` into embeddings:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将`input_ids`转换为嵌入层：
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Our soft prompts are also tokenized sentences. Originally, they were initialized
    to random words, and we’ll see over time that they’ll adjust appropriately. But
    for this step, these tokens need to be converted to embeddings:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的软提示也是标记化的句子。最初，它们被初始化为随机单词，随着时间的推移，我们会看到它们会相应地调整。但在这个步骤中，这些标记需要被转换为嵌入层：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `expand` method just adds the batch size to the prompt embeddings. When
    we defined the class, we didn’t know how large each batch coming in would be (and
    the code is written to let you tweak that based on the size of your available
    memory), so using `expand(batch_size, –1, –1)` turns the vector of prompt embeddings,
    which was of shape `[1, num_prompt_tokens, embedding_dimensions]`, into `[batch_size,
    num_prompt_tokens, embedding_dimensions]`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`expand` 方法只是将批大小添加到提示嵌入中。当我们定义类时，我们不知道每个进入的批大小会有多大（代码编写是为了让你根据可用内存的大小进行调整），所以使用
    `expand(batch_size, –1, –1)` 将提示嵌入的向量，其形状为 `[1, num_prompt_tokens, embedding_dimensions]`，转换为
    `[batch_size, num_prompt_tokens, embedding_dimensions]`。'
- en: 'Our soft prompt tuning involved prepending the soft embeddings to the embeddings
    for the actual input data, so we do that with this:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的软提示调整涉及将软嵌入添加到实际输入数据的嵌入之前，所以我们这样做：
- en: '[PRE33]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'BERT uses an `attention_mask` to filter out the tokens we don’t want to worry
    about at training or inference time, which are usually the padding tokens. But
    we want BERT to pay attention to all of the soft prompt tokens, so we’ll set the
    attention mask for them to be all 1s and then append that to the incoming attention
    mask(s) for the training data. Here’s the code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: BERT使用 `attention_mask` 来过滤掉在训练或推理时间我们不关心的标记，通常是填充标记。但我们要让BERT关注所有的软提示标记，所以我们将它们的注意力掩码设置为全1，然后将它们附加到训练数据的传入注意力掩码（s）中。以下是代码：
- en: '[PRE34]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now that we’ve done all our tuning, we need to pass the data to the model to
    have it optimize and calculate the loss:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了所有的调整，我们需要将数据传递给模型以进行优化和计算损失：
- en: '[PRE35]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We will see how this data is used in the training loop, next.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到这些数据如何在训练循环中使用。
- en: Training the Model
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'The key to this training is that we’re going to do a full, normal training
    loop but in a special circumstance. In this case, we previously froze *everything*
    in the BERT model, *except* for the soft prompts that we defined as model parameters.
    Therefore, say we define the optimizer like this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这次训练的关键是我们将进行完整的、正常的训练循环，但在特殊情况下。在这种情况下，我们之前冻结了BERT模型中的所有内容，*除了*我们定义的作为模型参数的软提示。因此，例如，我们定义优化器如下：
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In this case, we’re using standard code, telling it to tweak the model’s parameters.
    But the only ones that are available to tune are the soft prompts, so this should
    be quick!
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用标准代码，告诉它调整模型的参数。但唯一可调整的是软提示，所以这应该很快！
- en: Note that the value for the learning rate is quite large. This helps the system
    learn quickly, but in a real system, you’d likely want the value to be smaller—or
    at least adjustable, starting large and then shrinking in later epochs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，学习率的值相当大。这有助于系统快速学习，但在实际系统中，你可能希望该值更小——或者至少是可调整的，从大的值开始，然后在后续的周期中减小。
- en: 'So now, let’s get into training. First, we’ll set up the training loop:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入训练阶段。首先，我们将设置训练循环：
- en: '[PRE37]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Managing data batches
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理数据批次
- en: 'For each batch, we’ll get the columns (`input_ids` and `attention_masks`) as
    well as the labels and pass them to the model:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个批次，我们将获取列（`input_ids` 和 `attention_masks`）以及标签，并将它们传递给模型：
- en: '[PRE38]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This looks a little different from ones earlier in this book, but it’s pretty
    much doing the same thing. The `tqdm` code just gives us a status bar because
    we’re training. We read the data batch by batch, but we want the data to be on
    the same device as the model. So, for example, if the model is running on a GPU,
    we want it to access data in the GPU’s memory. Therefore, this line will iterate
    through each column, reading the key and passing the value to the device:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来与本书中较早的部分略有不同，但它基本上在做同样的事情。`tqdm` 代码只是提供了一个状态栏，因为我们正在训练。我们按批次读取数据，但我们希望数据与模型位于同一设备上。例如，如果模型在GPU上运行，我们希望它能够访问GPU的内存。因此，此行将遍历每一列，读取键并将值传递到设备：
- en: '[PRE39]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'It redefines the batch that was read in to ensure that the data is on the same
    device as the model. But we don’t want the labels to be in the batch because the
    model expects them to be fed in separately, so we remove them from the batch with
    the `pop()` method:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 它重新定义了读入的批次，以确保数据与模型位于同一设备上。但我们不希望标签在批次中，因为模型期望它们单独输入，所以我们使用 `pop()` 方法将它们从批次中移除：
- en: '[PRE40]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, we can use the shorthand of `**batch` to pass the set of input values
    (in this case, the `input_ids` and the `attention_mask`) to the forward method
    of the model and unpack the dictionary along with the labels, like this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`**batch`的缩写来将输入值的集合（在这种情况下，是`input_ids`和`attention_mask`）传递给模型的forward方法，并像这样与标签一起解包字典：
- en: '[PRE41]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Handling the loss
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理损失
- en: 'The forward pass sends the data to the model and gets the loss back. We use
    this to update our overall loss, and we can then backward pass:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传递将数据发送到模型并返回损失。我们使用这个来更新我们的总损失，然后我们可以进行反向传递：
- en: '[PRE42]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: With the gradients flowing back, the optimizer can now do its job.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 随着梯度的反向流动，优化器现在可以开始工作。
- en: Optimizing for loss
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化损失
- en: 'Remembering that the `model.parameters()` will only manage the *trainable unfrozen*
    parameters, we can now call the optimizer. I added something called *gradient
    clipping* here to make the training a little more efficient, but the rest is just
    calling the optimizer’s next step and then zeroing out the gradients so we can
    use them next time:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 记住`model.parameters()`只会管理*可训练未冻结*的参数，我们现在可以调用优化器。我在这里添加了*梯度裁剪*来使训练更有效率，但其余的只是调用优化器的下一步，然后清零梯度以便我们下次可以使用：
- en: '[PRE43]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The idea behind *gradient clipping* is that sometimes, during backpropagation,
    the gradients can be too large and the optimizer might take very large steps.
    This can lead to a problem called *exploding gradients*, in which the value changes
    hide the nuances of what might be learned. But clipping scales the gradients down
    if their values grow too large, and in a situation like this one, they may not
    even be necessary.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度裁剪*背后的想法是，在反向传播过程中，梯度有时可能太大，优化器可能会采取非常大的步骤。这可能导致一个称为*梯度爆炸*的问题，其中值的改变隐藏了可能学习到的细微差别。但是，如果梯度值变得太大，裁剪会将其缩小，在这种情况下，它们甚至可能不是必需的。'
- en: Evaluation During Training
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练期间的评估
- en: 'We also have a set of test data, so we can evaluate how the model performs
    during the training cycle. In each epoch, once the forward and backward passes
    are done and the model parameters are reset, we can switch the model into evaluation
    mode and then start passing all of the test data through it to get inference.
    We’ll also compare the results of the inference against the actual labels to calculate
    accuracy:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一组测试数据，因此我们可以评估模型在整个训练周期中的表现。在每个epoch中，一旦完成前向和反向传递并且模型参数被重置，我们可以将模型切换到评估模式，然后开始将所有测试数据通过它以获取推理。我们还将比较推理结果与实际标签以计算准确率：
- en: '[PRE44]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we’ll have similar code—but this time, it will be to read the eval batches,
    turn them into outputs with labels, and get predictions and loss values from the
    model:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将有类似的代码——但这次，它将是读取评估批次，将它们转换为带有标签的输出，并从模型中获取预测和损失值：
- en: '[PRE45]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Once we’ve calculated these values, then at the end of each epoch, we can report
    on them and on training loss.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算了这些值，那么在每个epoch结束时，我们可以报告它们和训练损失。
- en: Reporting Training Metrics
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 报告训练指标
- en: 'During training in each epoch, we calculated the training loss, so we can now
    get the average across all records. We can do the same thing with the validation
    loss and (of course) with the validation accuracy and then report on them:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个epoch的训练中，我们计算了训练损失，因此我们现在可以获取所有记录的平均值。我们可以用验证损失做同样的事情，当然还有验证准确率，然后报告它们：
- en: '[PRE46]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Running this training for three epochs gives us this:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 进行三个epoch的训练给出以下结果：
- en: '[PRE47]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This was done on an A100 in Colab with 40 Gb of GRAM, and as you can see, each
    epoch only took about 1 minute to train and 30 seconds to evaluate.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在Colab上的A100上完成的，有40 Gb的GRAM，正如你所看到的，每个epoch的训练只需大约1分钟，评估只需30秒。
- en: By the end, the average training loss had dropped from about 0.65 to 0.58\.
    The accuracy was 0.8736\. So, it’s likely overfitting because we only trained
    for three epochs.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 到最后，平均训练损失从大约0.65下降到0.58。准确率为0.8736。因此，很可能是过拟合，因为我们只训练了三个epoch。
- en: Saving the Prompt Embeddings
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存提示嵌入
- en: 'What’s really nice about this approach is that you can simply save out the
    prompt embeddings when you’re done. You can also load them back in for inference
    later, as you’ll see in the next section:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的真正好处是，当你完成时，你可以简单地保存提示嵌入。你还可以在稍后加载它们以进行推理，正如你将在下一节中看到的：
- en: '[PRE48]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: What I find really cool about this is that this file is relatively small (61
    K), and it doesn’t require you to amend the underlying model in any way. Thus,
    in an application, you could potentially have a number of these prompt-tuning
    files and hot-swap and replace them as needed so that you can have multiple models
    that you can orchestrate, which is the basis for an agentic solution.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得这真的很酷的是，这个文件相对较小（61 K），并且它不需要你以任何方式修改底层模型。因此，在应用中，你可能会有一系列这样的提示调整文件，并且可以根据需要热插拔和替换它们，这样你就可以拥有多个可以协调的模型，这是代理解决方案的基础。
- en: Performing Inference with the Model
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型进行推理
- en: To perform inference with a prompt-tuned model, you’ll simply define the model
    with the soft prompts and then, instead of training them, load the pretrained
    soft prompts back from disk. We’ll explore that in this section. If you don’t
    want to train your own model, then [in the download](https://github.com/lmoroney/PyTorch-Book-FIles),
    I’ve provided soft prompts from a version of the model that was trained for 30
    epochs instead of 3.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用提示调整模型进行推理，你只需用软提示定义模型，然后，而不是训练它们，从磁盘加载预训练的软提示。我们将在本节中探讨这一点。如果你不想训练自己的模型，那么[在下载](https://github.com/lmoroney/PyTorch-Book-FIles)中，我提供了一种经过30个epoch训练的模型版本的软提示，而不是3个epoch。
- en: 'For tidier encapsulation, I created a class that is similar to the one we used
    for training but that is just for inference. I call it `PromptTunedBERTInference`,
    and here’s its initializer:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更整洁的封装，我创建了一个类，它类似于我们用于训练的类，但仅用于推理。我称它为`PromptTunedBERTInference`，以下是它的初始化器：
- en: '[PRE49]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'It’s very similar to the initializer for the trainable one, except for a couple
    of important points. The first is that because we’re *only* using it for inference,
    I’ve set it into eval mode:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 它与可训练的初始化器非常相似，除了几个重要的点。首先是因为我们*仅*使用它进行推理，我已经将其设置为评估模式：
- en: '[PRE50]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The second is that we don’t need to train the embeddings and do all the associated
    plumbing—instead, we just load them from the specified path:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 第二点是我们不需要训练嵌入并执行所有相关操作——相反，我们只需从指定的路径加载它们：
- en: '[PRE51]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: And that’s it! As you can see, it’s quite lightweight and pretty straightforward.
    It won’t have a `forward` function because we’re not training it, but let’s add
    a `predict` function that encapsulates doing inference with it.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！如你所见，它相当轻量级且非常直接。它不会有`forward`函数，因为我们没有在训练它，但让我们添加一个`predict`函数来封装使用它的推理。
- en: The predict function
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`predict`函数'
- en: The job of the `predict` function is to take in the string(s) that we want to
    perform inference with, tokenize it (them), and then pass it to the model with
    the soft tokens prepended. Let’s take a look at the code, piece by piece.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict`函数的职责是接收我们想要进行推理的字符串（字符串），对其进行标记化（它们），然后将它们与前置的软标记一起传递给模型。让我们逐块查看代码。'
- en: 'First, let’s define it and have it accept text that it will then tokenize:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义它，并让它接受它将随后进行标记化的文本：
- en: '[PRE52]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The text will be tokenized up to the maximum length, less the size of the soft
    prompt, and then each of the items in the input will be loaded into a dictionary.
    Note that the tokenizer will return multiple columns for the text—usually, the
    tokens and the attention mask—so we’ll follow this approach to turn them into
    a set of key-value pairs that are easy for us to work with later.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 文本将被标记化到最大长度，减去软提示的大小，然后输入中的每个项目将被加载到一个字典中。请注意，标记器将为文本返回多个列——通常是标记和注意力掩码——因此我们将遵循这种方法将它们转换成一组易于我们以后处理的键值对。
- en: 'Now that we have our inputs, it’s time to pass them to the model. We’ll start
    by putting `torch` into `no_grad()` mode because we’re not interested in training
    gradients. We’ll then get the embeddings for each of our tokens:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了输入，是时候将它们传递给模型以获取我们的推理结果了。我们首先将`torch`置于`no_grad()`模式，因为我们不感兴趣于训练梯度。然后我们将为我们的每个标记获取嵌入：
- en: '[PRE53]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We have an attention mask for the input that’s generated by the tokenizer,
    but we don’t have one for the soft prompt. So, let’s create one and then append
    it to the input attention mask:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个由标记器生成的输入注意力掩码，但没有软提示的注意力掩码。所以，让我们创建一个，并将其附加到输入注意力掩码上：
- en: '[PRE54]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now that we have everything in place, we can pass our data to the model to
    get our inferences back:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了所有东西，我们可以将我们的数据传递给模型以获取我们的推理结果：
- en: '[PRE55]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The outputs will be the logits from the two neurons, one representing positive
    sentiment and the other negative. We can then Softmax these to get the prediction:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是两个神经元的logits，一个表示积极情绪，另一个表示消极。然后我们可以对它们进行Softmax以获得预测：
- en: '[PRE56]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Usage example
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用示例
- en: 'Using this class for a prediction is then pretty straightforward. We create
    an instance of the class and pass a string to it to get results. The results will
    contain a prediction and a confidence value, which we can then output:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个类进行预测是非常直接的。我们创建类的实例，并传递一个字符串给它以获取结果。结果将包含一个预测和一个置信度值，然后我们可以将其输出：
- en: '[PRE57]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: One note you might see with prompt tuning is low confidence values that can
    lead to mis-predictions, especially with binary classifiers like this one. It’s
    good to explore your inference to make sure that it’s working well, and there
    are also techniques that you could explore to ensure that the logits are giving
    the values you want. These include setting the temperature of the Softmax, using
    more prompt tokens to give the model more capacity, and initializing the prompt
    tokens with sentiment-related words (instead of random tokens, like we did here).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会在提示微调中看到一些低置信度值，这可能导致错误预测，尤其是在像这样的二元分类器中。探索你的推理以确保其正常工作是个好主意，同时也有一些技术你可以探索来确保logits给出你想要的价值。这包括设置Softmax的温度、使用更多的提示标记来给模型更多的容量，以及用与情感相关的词初始化提示标记（而不是像我们这里这样使用随机标记）。
- en: Summary
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we explored different methods for customizing LLMs with our
    own data. We looked at two main approaches: traditional fine-tuning and prompt
    tuning.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用我们自己的数据定制LLMs的不同方法。我们研究了两种主要方法：传统的微调和提示微调。
- en: Using the IMDb dataset, you saw how to fine-tune BERT for sentiment analysis
    and walked through all the steps—from data preparation, to model configuration,
    to training and evaluation. The model achieved an impressive 95% accuracy in sentiment
    classification in just a few epochs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 使用IMDb数据集，你看到了如何微调BERT进行情感分析，并走过了所有步骤——从数据准备到模型配置，再到训练和评估。该模型在仅几个epoch内就实现了令人印象深刻的95%的情感分类准确率。
- en: However, fine-tuning may not be appropriate in all cases, and to that end, you
    explored a lightweight alternative called prompt tuning. Instead of modifying
    model weights, the idea here was to prepend trainable soft prompts to inputs,
    which are optimized during training. This approach provides significant advantages
    in that it can be much faster and it doesn’t change the underlying model. In this
    case, the tuned prompts could be saved (and they were only a few Kb) and then
    reloaded to program the model to perform the desired task. You then went through
    a full implementation, showing you how to create, train, and save these soft prompts,
    plus load them back to perform inference.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，微调可能并不适用于所有情况，因此你探索了一个轻量级的替代方案，称为提示微调。在这里，不是修改模型权重，而是将可训练的软提示预置于输入之前，这些提示在训练过程中被优化。这种方法提供了显著的优势，它可以更快，并且不会改变底层模型。在这种情况下，调整后的提示可以被保存（并且它们只有几Kb），然后重新加载以编程模型执行所需的任务。然后你进行了一次完整的实现，展示了如何创建、训练和保存这些软提示，以及如何将它们加载回来以执行推理。
- en: In the next chapter, we’ll explore how you can serve LLMs, including customized
    ones. I’ll explain how to do this in your own data center by using Ollama, which
    is a powerful tool for handling the serving and management of LLMs. You’ll learn
    how to take models and turn them into services, and we’ll also explore how to
    set up Ollama and use it over HTTP to talk with models in your data center.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何提供LLMs，包括定制的LLMs。我会解释如何通过使用Ollama来实现这一点，Ollama是一个用于处理LLMs的托管和管理功能强大的工具。你将学习如何将模型转换为服务，我们还将探讨如何设置Ollama并使用它通过HTTP与数据中心中的模型进行通信。
