- en: Chapter 3\. Getting Up to Speed on Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。快速了解机器学习
- en: There are few areas in technology with the mystique that surrounds machine learning
    and artificial intelligence (AI). Even if you’re an experienced engineer in another
    domain, machine learning can seem like a dense subject with a mountain of assumed
    knowledge requirements. Many developers feel discouraged when they begin to read
    about ML and encounter explanations that invoke academic papers, obscure Python
    libraries, and advanced mathematics. It can feel daunting to even know where to
    start.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术领域中，很少有像机器学习和人工智能（AI）周围那样神秘的领域。即使您是另一个领域的经验丰富的工程师，机器学习也可能看起来是一个需要大量先验知识的复杂主题。许多开发人员在开始阅读有关机器学习的内容时会感到沮丧，因为这些解释涉及学术论文、晦涩的Python库和高级数学。甚至知道从哪里开始都可能感到令人生畏。
- en: In reality, machine learning can be simple to understand and is accessible to
    anyone with a text editor. After you learn a few key ideas, you can easily use
    it in your own projects. Beneath all the mystique is a handy set of tools for
    solving various types of problems. It might sometimes *feel* like magic, but it’s
    all just code, and you don’t need a PhD to work with it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，机器学习很容易理解，任何人都可以通过文本编辑器访问。学习了一些关键思想后，您可以轻松地在自己的项目中使用它。在所有神秘感之下，是一套解决各种问题的有用工具。有时候可能会*感觉*像魔术，但其实只是代码，您不需要博士学位来使用它。
- en: This book is about using machine learning with tiny devices. In the rest of
    this chapter, you’ll learn all the ML you need to get started. We’ll cover the
    basic concepts, explore some tools, and train a simple machine learning model.
    Our focus is tiny hardware, so we won’t spend long on the theory behind deep learning,
    or the mathematics that makes it all work. Later chapters will dig deeper into
    the tooling, and how to optimize models for embedded devices. But by the end of
    this chapter, you’ll be familiar with the key terminology, have an understanding
    of the general workflow, and know where to go to learn more.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是关于如何在微型设备上使用机器学习的。在本章的其余部分，您将学习所有开始所需的机器学习知识。我们将涵盖基本概念，探索一些工具，并训练一个简单的机器学习模型。我们的重点是微型硬件，因此我们不会花太多时间讨论深度学习背后的理论，或者使其运作的数学。后面的章节将更深入地探讨工具和如何优化嵌入式设备的模型。但是在本章结束时，您将熟悉关键术语，了解一般工作流程，并知道去哪里学习更多。
- en: 'In this chapter, we cover the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖以下内容：
- en: What machine learning actually is
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习实际上是什么
- en: The types of problems it can solve
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以解决的问题类型
- en: Key terms and ideas
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键术语和思想
- en: The workflow for solving problems with deep learning, one of the most popular
    approaches to machine learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习解决问题的工作流程，这是机器学习中最流行的方法之一
- en: Tip
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: There are many books and courses that explain the science behind deep learning,
    so we won’t be doing that here. That said, it’s a fascinating topic and we encourage
    you to explore! We list some of our favorite resources in [“Learning Machine Learning”](ch12.xhtml#learning_machine_learning).
    But remember, you don’t need all the theory to start building useful things.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多书籍和课程解释深度学习背后的科学，所以我们不会在这里做这个。尽管如此，这是一个迷人的主题，我们鼓励您去探索！我们在[“学习机器学习”](ch12.xhtml#learning_machine_learning)中列出了一些我们喜欢的资源。但请记住，您不需要所有的理论来开始构建有用的东西。
- en: What Machine Learning Actually Is
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习实际上是什么
- en: Imagine you own a machine that manufactures widgets. Sometimes it breaks down,
    and it’s expensive to repair. Perhaps if you collected data about the machine
    during operation, you might be able to predict when it is about to break down
    and halt operation before damage occurs. For instance, you could record its rate
    of production, its temperature, and how much it is vibrating. It might be that
    some combination of these factors indicates an impending problem. But how do you
    figure it out?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 想象您拥有一台制造小部件的机器。有时它会出故障，修复起来很昂贵。也许如果您在机器运行期间收集数据，您可能能够预测何时会出现故障，并在损坏发生之前停止运行。例如，您可以记录其生产速率、温度和振动情况。也许这些因素的某种组合表明即将出现问题。但是您如何找出呢？
- en: This is an example of the sort of problem machine learning is designed to solve.
    Fundamentally, machine learning is a technique for using computers to predict
    things based on past observations. We collect data about our factory machine’s
    performance and then create a computer program that analyzes that data and uses
    it to predict future states.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是机器学习旨在解决的问题类型的示例。从根本上讲，机器学习是一种利用计算机根据过去观察来预测事物的技术。我们收集有关工厂机器性能的数据，然后创建一个计算机程序来分析这些数据，并用它来预测未来状态。
- en: Creating a machine learning program is different from the usual process of writing
    code. In a traditional piece of software, a programmer designs an algorithm that
    takes an input, applies various rules, and returns an output. The algorithm’s
    internal operations are planned out by the programmer and implemented explicitly
    through lines of code. To predict breakdowns in a factory machine, the programmer
    would need to understand which measurements in the data indicate a problem and
    write code that deliberately checks for them.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 创建机器学习程序与编写代码的传统过程不同。在传统软件中，程序员设计一个算法，该算法接受输入，应用各种规则，并返回输出。程序员计划算法的内部操作，并通过代码行明确实现。要预测工厂机器的故障，程序员需要了解数据中哪些测量值表示问题，并编写代码来有意识地检查它们。
- en: This approach works fine for many problems. For example, we know that water
    boils at 100°C at sea level, so it’s easy to write a program that can predict
    whether water is boiling based on its current temperature and altitude. But in
    many cases, it can be difficult to know the exact combination of factors that
    predicts a given state. To continue with our factory machine example, there might
    be various different combinations of production rate, temperature, and vibration
    level that might indicate a problem but are not immediately obvious from looking
    at the data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法对许多问题都有效。例如，我们知道水在海平面上沸腾的温度是100°C，因此可以轻松编写一个程序，根据当前温度和海拔高度来预测水是否正在沸腾。但在许多情况下，很难知道哪些因素的确切组合预测了给定状态。继续以我们的工厂机器示例为例，可能有各种不同的生产速率、温度和振动水平的组合可能表明问题，但从数据中看不出来。
- en: To create a machine learning program, a programmer feeds data into a special
    kind of algorithm and lets the algorithm discover the rules. This means that as
    programmers, we can create programs that make predictions based on complex data
    without having to understand all of the complexity ourselves. The machine learning
    algorithm builds a *model* of the system based on the data we provide, through
    a process we call *training*. The model is a type of computer program. We run
    data through this model to make predictions, in a process called *inference*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个机器学习程序，程序员将数据输入到一种特殊类型的算法中，让算法发现规则。这意味着作为程序员，我们可以创建基于复杂数据的预测程序，而不必完全理解所有复杂性。机器学习算法基于我们提供的数据构建系统的*模型*，通过我们称之为*训练*的过程。模型是一种计算机程序。我们通过这个模型运行数据来进行预测，这个过程称为*推理*。
- en: There are many different approaches to machine learning. One of the most popular
    is *deep learning*, which is based on a simplified idea of how the human brain
    might work. In deep learning, a *network* of simulated neurons (represented by
    arrays of numbers) is trained to model the relationships between various inputs
    and outputs. Different *architectures*, or arrangements of simulated neurons,
    are useful for different tasks. For instance, some architectures excel at extracting
    meaning from image data, while other architectures work best for predicting the
    next value in a sequence.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习有许多不同的方法。其中最流行的之一是*深度学习*，它基于人类大脑可能如何工作的简化想法。在深度学习中，一组模拟神经元（由数字数组表示）被训练来模拟各种输入和输出之间的关系。不同的*架构*或模拟神经元的排列对不同的任务很有用。例如，一些架构擅长从图像数据中提取含义，而其他架构最适合预测序列中的下一个值。
- en: The examples in this book focus on deep learning, since it’s a flexible and
    powerful tool for solving the types of problems that are well suited to microcontrollers.
    It might be surprising to discover that deep learning can work even on devices
    with limited memory and processing power. In fact, over the course of this book,
    you’ll learn how to create deep learning models that do some really amazing things
    but that still fit within the constraints of tiny devices.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的示例侧重于深度学习，因为它是解决适合微控制器的问题类型的灵活且强大的工具。也许令人惊讶的是，深度学习甚至可以在内存和处理能力有限的设备上运行。事实上，在本书的过程中，您将学习如何创建一些非常惊人的深度学习模型，但这些模型仍然符合微型设备的限制。
- en: The next section explains the basic workflow for creating and using a deep learning
    model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节解释了创建和使用深度学习模型的基本工作流程。
- en: The Deep Learning Workflow
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习工作流程
- en: In the previous section, we outlined a scenario for using deep learning to predict
    when a factory machine is likely to break down. In this section, we introduce
    the work necessary to make this happen.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们概述了使用深度学习来预测工厂机器何时可能会发生故障的场景。在本节中，我们介绍了使这一情况发生所需的工作。
- en: 'This process will involve the following tasks:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程将涉及以下任务：
- en: Decide on a goal
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定目标
- en: Collect a dataset
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集数据集
- en: Design a model architecture
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设计模型架构
- en: Train the model
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Convert the model
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换模型
- en: Run inference
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行推理
- en: Evaluate and troubleshoot
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估和故障排除
- en: Let’s walk through them, one by one.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一走过它们。
- en: Decide on a Goal
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定目标
- en: When you’re designing any kind of algorithm, it’s important to start by establishing
    exactly what you want it to do. It’s no different with machine learning. You need
    to decide what you want to predict so you can decide what data to collect and
    which model architecture to use.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当您设计任何类型的算法时，重要的是首先明确您希望它做什么。机器学习也不例外。您需要决定您想要预测什么，以便确定要收集哪些数据以及使用哪种模型架构。
- en: 'In our example, we want to predict whether our factory machine is about to
    break down. We can express this as a *classification* problem. Classification
    is a machine learning task that takes a set of input data and returns the probability
    that this data fits each of a set of known *classes*. In our example, we might
    have two classes: “normal,” meaning that our machine is operating without issue,
    and “abnormal,” meaning that our machine is showing signs that it might soon break
    down.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们想要预测我们的工厂机器是否即将发生故障。我们可以将这表示为一个*分类*问题。分类是一个机器学习任务，它接受一组输入数据，并返回这些数据符合一组已知*类别*的概率。在我们的例子中，我们可能有两个类别：“正常”，表示我们的机器正常运行，没有问题，“异常”，表示我们的机器显示出可能很快会发生故障的迹象。
- en: This means that our goal is to create a model that classifies our input data
    as either “normal” or “abnormal.”
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的目标是创建一个将我们的输入数据分类为“正常”或“异常”的模型。
- en: Collect a Dataset
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集数据集
- en: Our factory is likely to have a lot of available data, ranging from the operating
    temperature of our machine through to the type of food that was served in the
    cafeteria on a given day. Given the goal we’ve just established, we can begin
    to identify what data we need.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工厂可能有大量可用数据，从机器的运行温度到某一天食堂提供的食物类型。鉴于我们刚刚建立的目标，我们可以开始确定我们需要的数据。
- en: Selecting data
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择数据
- en: Deep learning models can learn to ignore noisy or irrelevant data. That said,
    it’s best to train your model only using information that is relevant to solving
    the problem. Since it’s unlikely that today’s cafeteria food has an impact on
    the functioning of our machine, we can probably exclude it from our dataset. Otherwise,
    the model will need to learn to negate that irrelevant input, and it might be
    vulnerable to learning spurious associations—perhaps our machine has, coincidentally,
    always broken down on days that pizza is served.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型可以学会忽略嘈杂或无关的数据。也就是说，最好只使用与解决问题相关的信息来训练模型。由于今天的食堂食物不太可能影响我们机器的运行，我们可能可以将其从数据集中排除。否则，模型将需要学会否定那些无关的输入，并且可能容易学习到虚假的关联——也许我们的机器总是在提供比萨的日子出故障。
- en: You should always try to combine your domain expertise with experimentation
    when deciding whether to include data. You can also use statistical techniques
    to try to identify which data is significant. If you’re still unsure about including
    a certain data source, you can always train two models and see which one works
    best!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定是否包含数据时，您应该始终尝试将领域专业知识与实验相结合。您还可以使用统计技术来尝试识别哪些数据是重要的。如果您仍然不确定是否包含某个数据源，您可以始终训练两个模型，看哪个效果最好！
- en: Suppose that we’ve identified our most promising data as *rate of production*,
    *temperature*, and *vibration*. Our next step is to collect some data so that
    we can train a model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经确定了最有前途的数据为*生产速率*、*温度*和*振动*。我们的下一步是收集一些数据，以便我们可以训练一个模型。
- en: Tip
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: It’s really important that the data you choose will also be available when you
    want to make predictions. For example, since we have decided to train our model
    with temperature readings, we will need to provide temperature readings from the
    exact same physical locations when we run inference. This is because the model
    learns to understand how its inputs can predict its outputs. If we originally
    trained the model on temperature data from the insides of our machine, running
    the model on the current room temperature is unlikely to work.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的数据在您想要进行预测时也是可用的非常重要。例如，由于我们决定用温度读数训练我们的模型，当我们进行推理时，我们将需要提供来自完全相同物理位置的温度读数。这是因为模型学习了如何理解其输入如何预测其输出。如果我们最初在机器内部的温度数据上训练模型，那么在当前室温上运行模型可能不起作用。
- en: Collecting data
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集数据
- en: 'It’s difficult to know exactly how much data is required to train an effective
    model. It depends on many factors, such as the complexity of the relationships
    between variables, the amount of noise, and the ease with which classes can be
    distinguished. However, there’s a rule of thumb that is always true: the more
    data, the better!'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练有效的模型需要多少数据是很难确定的。这取决于许多因素，例如变量之间的关系复杂性、噪音量以及类别之间的区分程度。然而，有一个经验法则始终成立：数据越多，越好！
- en: You should aim to collect data that represents the full range of conditions
    and events that can occur in the system. If our machine can fail in several different
    ways, we should be sure to capture data around each type of failure. If a variable
    changes naturally over time, it’s important to collect data that represents the
    full range. For example, if the machine’s temperature rises on warm days, you
    should be sure to include data from both winter and summer. This diversity will
    help your model represent every possible scenario, not just a select few.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该努力收集代表系统中可能发生的所有条件和事件的数据。如果我们的机器可能以几种不同的方式出现故障，我们应该确保捕获每种类型故障周围的数据。如果一个变量随着时间自然变化，收集代表整个范围的数据是很重要的。例如，如果机器在温暖的日子里温度升高，你应该确保包括冬天和夏天的数据。这种多样性将帮助你的模型代表每种可能的情况，而不仅仅是一些特定的情况。
- en: The data we collect about our factory will likely be logged as a set of *time
    series*, meaning a sequence of readings collected on a periodic basis. For example,
    we might have a record of the temperature every minute, the rate of production
    each hour, and the level of vibration on a second-by-second basis. After we collect
    the data, we’ll need to transform these time series into a form appropriate for
    our model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集关于工厂的数据可能会被记录为一组*时间序列*，意味着定期收集的一系列读数。例如，我们可能每分钟记录一次温度，每小时记录一次生产速率，每秒记录一次振动水平。在收集数据后，我们需要将这些时间序列转换为适合我们模型的形式。
- en: Labeling data
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记数据
- en: In addition to collecting data, we need to determine which data represents “normal”
    and “abnormal” operation. We’ll provide this information during the training process
    so that our model can learn how to classify inputs. The process of associating
    data with classes is called *labeling*, and the “normal” and “abnormal” classes
    are our *labels*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了收集数据，我们还需要确定哪些数据代表“正常”和“异常”操作。我们将在训练过程中提供这些信息，以便我们的模型学习如何对输入进行分类。将数据与类别相关联的过程称为*标记*，而“正常”和“异常”类别是我们的*标签*。
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This type of training, in which you instruct the algorithm what the data means
    during training, is called *supervised learning*. The resulting classification
    model will be able to process incoming data and predict to which class it is likely
    to belong.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种训练方式，即在训练期间指导算法数据的含义，被称为*监督学习*。生成的分类模型将能够处理传入的数据并预测其可能属于哪个类别。
- en: To label the time-series data we’ve collected, we need a record of which periods
    of time the machine was working and which periods of time it was broken. We might
    assume that the period immediately prior to the machine being broken generally
    represents abnormal operation. However, since we can’t necessarily spot abnormal
    operation from a superficial look at the data, getting this correct might require
    some experimentation!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标记我们收集到的时间序列数据，我们需要记录机器工作和故障的时间段。我们可能会假设机器故障前的时间段通常代表异常操作。然而，由于我们不能从数据的表面看出异常操作，正确地获取这些信息可能需要一些实验！
- en: After we’ve decided how to label the data, we can generate a time series that
    contains the labels and add this to our dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们决定如何标记数据之后，我们可以生成一个包含标签的时间序列，并将其添加到我们的数据集中。
- en: Our final dataset
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的最终数据集
- en: '[Table 3-1](#table31) lists the data sources that we’ve assembled at this point
    in the workflow.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[表3-1](#table31) 列出了我们在工作流程中此刻已经收集的数据源。'
- en: Table 3-1\. Data sources
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-1. 数据源
- en: '| Data source | Interval | Sample reading |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 数据源 | 间隔 | 样本读数 |'
- en: '| --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Rate of production | Once every 2 minutes | 100 units |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 生产速率 | 每2分钟一次 | 100个单位 |'
- en: '| Temperature | Once every minute | 30°C |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | 每分钟一次 | 30°C |'
- en: '| Vibration (% of typical) | Once every 10 seconds | 23% |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 振动（典型值的百分比） | 每10秒一次 | 23% |'
- en: '| Label (“normal” or “abnormal”) | Once every 10 seconds | normal |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 标签（“正常”或“异常”） | 每10秒一次 | 正常 |'
- en: The table shows the interval of each data source. For example, the temperature
    is logged once per minute. We’ve also generated a time series that contains the
    labels for the data. The interval for our labels is 1 per 10 seconds, which is
    the same as the smallest interval for the other time series. This means that we
    can easily determine the label for every datapoint in our data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表格显示了每个数据源的时间间隔。例如，温度每分钟记录一次。我们还生成了一个包含数据标签的时间序列。我们的标签间隔是每10秒1次，与其他时间序列的最小间隔相同。这意味着我们可以轻松确定数据中每个数据点的标签。
- en: Now that we’ve collected our data, it’s time to use it to design and train a
    model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经收集了数据，是时候用它来设计和训练模型了。
- en: Design a Model Architecture
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计模型架构
- en: There are many types of deep learning model architectures, designed to solve
    a wide range of problems. When training a model, you can choose to design your
    own architecture or base it on an existing architecture developed by researchers.
    For many common problems, you can find pretrained models available online for
    free.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类型的深度学习模型架构，旨在解决各种问题。在训练模型时，您可以选择设计自己的架构或基于研究人员开发的现有架构。对于许多常见问题，您可以在网上找到免费的预训练模型。
- en: Over the course of this book we’ll introduce you to several different model
    architectures, but there are a huge number of possibilities beyond what is covered
    here. Designing a model is both an art and a science, and model architecture is
    a major area of research. New architectures are invented literally every day.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的过程中，我们将向您介绍几种不同的模型架构，但除了这里介绍的内容外，还有大量可能性。设计模型既是一门艺术也是一门科学，模型架构是一个重要的研究领域。每天都会有新的架构被发明。
- en: When deciding on an architecture, you need to think about the type of problem
    you are trying to solve, the type of data you have access to, and the ways you
    can transform that data before feeding it into a model (we discuss transforming
    data shortly). The fact is, because the most effective architecture varies depending
    on the type of data that you are working with, your data and the architecture
    of your model are deeply intertwined. Although we introduce them here under separate
    headings, they’ll always be considered together.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定架构时，您需要考虑您试图解决的问题类型、您可以访问的数据类型以及在将数据馈送到模型之前可以对数据进行的转换方式（我们将很快讨论数据转换）。事实上，由于最有效的架构取决于您正在处理的数据类型，您的数据和模型架构是紧密相连的。尽管我们在这里分开标题介绍它们，但它们总是会被一起考虑。
- en: You also need to think about the constraints of the device you will be running
    the model on, since microcontrollers generally have limited memory and slow processors,
    and larger models require more memory and take more time to run—the size of a
    model depends on the number of neurons it contains, and the way those neurons
    are connected. In addition, some devices are equipped with hardware acceleration
    that can speed up the execution of certain types of model architectures, so you
    might want to tailor your model to the strengths of the device you have in mind.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要考虑将在其上运行模型的设备的约束，因为微控制器通常具有有限的内存和较慢的处理器，较大的模型需要更多内存并需要更长时间运行 - 模型的大小取决于它包含的神经元数量以及这些神经元的连接方式。此外，一些设备配备了硬件加速功能，可以加快某些类型的模型架构的执行速度，因此您可能希望根据您考虑的设备的优势来定制您的模型。
- en: In our case, we might start by training a simple model with a few layers of
    neurons and then refining the architecture in an iterative process until we get
    a useful result. You’ll see how to do that later in this book.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们可能首先通过几层神经元训练一个简单模型，然后通过迭代过程优化架构，直到获得有用的结果。您将在本书的后面看到如何做到这一点。
- en: Deep learning models accept input and generate output in the form of *tensors*.
    For the purposes of this book,^([1](ch03.xhtml#idm46473587162472)) a tensor is
    essentially a list that can contain either numbers or other tensors; you can think
    of it as similar to an array. Our hypothetical simple model will take a tensor
    as its input. The following subsection describes how we transform our data into
    this form.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型接受输入并生成*张量*形式的输出。对于本书的目的，^([1](ch03.xhtml#idm46473587162472)) 张量本质上是一个可以包含数字或其他张量的列表；您可以将其视为类似于数组。我们的假设简单模型将以张量作为输入。以下小节描述了我们如何将数据转换为这种形式。
- en: Generating features from data
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从数据生成特征
- en: We’ve established that our model will accept some type of tensor as its input.
    But as we discussed earlier, our data comes in the form of time series. How do
    we transform that time-series data into a tensor that we can pass into the model?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定我们的模型将接受某种张量作为输入。但正如我们之前讨论的，我们的数据以时间序列的形式呈现。我们如何将时间序列数据转换为可以传递到模型中的张量呢？
- en: Our task now is to decide how to generate features from our data. In machine
    learning, the term *feature* refers to a particular type of information on which
    a model is trained. Different types of models are trained on different types of
    features. For example, a model might accept a single scalar value as its sole
    input feature.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的任务是决定如何从我们的数据中生成特征。在机器学习中，术语*特征*指的是模型训练的特定类型信息。不同类型的模型训练在不同类型的特征上。例如，一个模型可能接受一个单一标量值作为其唯一输入特征。
- en: 'But inputs can be much more complex than this: a model designed to process
    images might accept a multidimensional tensor of image data as its input, and
    a model designed to predict based on multiple features might accept a vector containing
    multiple scalar values, one for each feature.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但是输入可能比这更复杂：一个设计用于处理图像的模型可能接受一个多维张量的图像数据作为输入，而一个设计用于基于多个特征进行预测的模型可能接受一个包含多个标量值的向量，每个特征对应一个值。
- en: Recall that we decided that our model should use rate of production, temperature,
    and vibration to make its predictions. In their raw form, as time series with
    different intervals, these will not be suitable to pass into the model. The following
    section explains why.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们决定我们的模型应该使用生产速率、温度和振动来进行预测。以它们的原始形式，作为具有不同间隔的时间序列，这些数据将不适合传递给模型。下一节将解释原因。
- en: Windowing
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 窗口化
- en: 'In the following diagram, each piece of data in our time series is represented
    by a star. The current label is included in the data, since the label is required
    for training. Our goal is to train a model we can use to predict whether the machine
    is operating normally or abnormally at any given moment based on the current conditions:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们的时间序列中的每个数据都用一个星号表示。当前标签包含在数据中，因为标签是训练所必需的。我们的目标是训练一个模型，可以根据当前条件在任何给定时刻预测机器是正常运行还是异常运行：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'However, since our time series each have different intervals (like once per
    minute, or once per 10 seconds), if we pass in only the data available at a given
    moment, it might not include all of the types of data we have available. For example,
    in the moment highlighted in the following image, only vibration is available.
    This would mean that our model would only have information about vibration when
    attempting to make its prediction:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们的时间序列具有不同的间隔（比如每分钟一次，或者每10秒一次），如果我们只传入给定时刻可用的数据，可能不包括我们可用的所有数据类型。例如，在下图中突出显示的时刻，只有振动数据可用。这意味着我们的模型在尝试进行预测时只有振动信息：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'One solution to this problem might be to choose a window in time, and combine
    all of the data in this window into a single set of values. For example, we might
    decide on a one-minute window and look at all the values contained within it:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个方法可能是选择一个时间窗口，并将该窗口内的所有数据合并为一组数值。例如，我们可以决定使用一个一分钟的时间窗口，并查看其中包含的所有数值：
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we average all the values in the window for each time series and take the
    most recent value for any that lack a datapoint in the current window, we end
    up with a set of single values. We can decide how to label this snapshot based
    on whether there are any “abnormal” labels present in the window. If there’s any
    “abnormal” present at all, the window should be labeled “abnormal.” If not, it
    should be labeled “normal”:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对每个时间序列的窗口中的所有值求平均，并对当前窗口中缺少数据点的任何值取最近的值，我们最终得到一组单一值。我们可以根据窗口中是否存在任何“异常”标签来决定如何标记这个快照。如果窗口中有任何“异常”存在，窗口应该被标记为“异常”。如果没有，应该被标记为“正常”：
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The three non-label values are our features! We can pass them into our model
    as a vector, with one element for each time series:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个非标签数值是我们的特征！我们可以将它们作为一个向量传递给我们的模型，每个时间序列有一个元素：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: During training, we can calculate a new window for every 10 seconds of data
    and pass it into our model, using the label to inform the training algorithm of
    our desired output. During inference, whenever we want to use the model to predict
    abnormal behavior, we can just look at our data, calculate the most recent window,
    run it through the model, and receive a prediction.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们可以为每10秒的数据计算一个新的窗口，并将其传递给我们的模型，使用标签来通知训练算法我们期望的输出。在推断过程中，每当我们想要使用模型来预测异常行为时，我们只需查看我们的数据，计算最近的窗口，将其通过模型运行，并接收一个预测。
- en: This is a simplistic approach, and it might not always turn out to work in practice,
    but it’s a good enough starting point. You’ll quickly find that machine learning
    is all about trial and error!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的方法，实际上可能并不总是有效，但这是一个很好的起点。您很快会发现，机器学习就是试错的过程！
- en: Before we move on to training, let’s go over one last thing about input values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续训练之前，让我们再谈一下关于输入数值的最后一点。
- en: Normalization
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 归一化
- en: Generally, the data you feed into a neural network will be in the form of tensors
    filled with *floating-point* values, or *floats*. A float is a data type used
    to represent numbers that have decimal points. For the training algorithm to work
    effectively, these floating-point values need to be similar in size to one another.
    In fact, it’s ideal if all values are expressed as numbers in the range of 0 to
    1.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您向神经网络提供的数据将以填充有*浮点*值或*浮点数*的张量形式呈现。浮点数是一种用于表示具有小数点的数字的数据类型。为了让训练算法有效地工作，这些浮点数值需要在大小上相似。事实上，如果所有数值都表示为0到1范围内的数字，那将是理想的。
- en: 'Let’s take another look at our input tensor from the previous section:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看一下上一节中的输入张量：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These numbers are each at very different scales: the temperature is more than
    100, whereas the vibration is expressed as a fraction of 1\. To pass these values
    into our network, we need to *normalize* them so that they are all in a similar
    range.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数值在非常不同的尺度上：温度超过100，而振动表示为1的分数。为了将这些值传递给我们的网络，我们需要对它们进行*归一化*，使它们都在一个类似的范围内。
- en: 'One way of doing this is to calculate the mean of each feature across the dataset
    and subtract it from the values. This has the effect of squashing the numbers
    down so that they are closer to zero. Here’s an example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是计算数据集中每个特征的平均值，并从值中减去。这样做的效果是将数字压缩到接近零。这里有一个例子：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'One situation in which you’ll frequently encounter normalization, implemented
    in a different way, is when images are fed into a neural network. Computers often
    store images as matrices of 8-bit integers, whose values range from 0 to 255\.
    To normalize these values so that they are all between 0 and 1, each 8-bit value
    is multiplied by `1/255`. Here’s an example with a 3 × 3–pixel grayscale image,
    in which each pixel’s value represents its brightness:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 您经常会遇到归一化的情况之一是当图像被输入神经网络时，以不同的方式实现。计算机通常将图像存储为8位整数的矩阵，其值范围从0到255。为了使这些值归一化，使它们都在0到1之间，每个8位值都乘以`1/255`。这里有一个示例，其中包含一个3×3像素的灰度图像，其中每个像素的值表示其亮度：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Thinking with ML
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用机器学习思考
- en: So far, we’ve learned how to start thinking about solving problems with machine
    learning. In the context of our factory scenario, we’ve walked through deciding
    on a suitable goal, collecting and labeling the appropriate data, designing the
    features we are going to pass into our model, and choosing a model architecture.
    No matter what problem we are trying to solve, we’ll use the same approach. It’s
    important to note that this is an iterative process, and we often go back and
    forth through the stages of the ML workflow until we’ve arrived at a model that
    works—or decided that the task is too difficult.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学会了如何开始用机器学习解决问题。在我们的工厂场景中，我们已经决定了一个合适的目标，收集和标记了适当的数据，设计了要传递到模型中的特征，并选择了一个模型架构。无论我们试图解决什么问题，我们都会使用相同的方法。重要的是要注意，这是一个迭代过程，我们经常在ML工作流程的各个阶段之间来回，直到我们找到一个有效的模型，或者决定任务太困难。
- en: For example, imagine that we’re building a model to predict the weather. We’ll
    need to decide on our goal (for instance, to predict whether it’s going to rain
    tomorrow), collect and label a dataset (such as weather reports from the past
    few years), design the features that we’ll feed to our model (perhaps the average
    conditions over the past two days), and choose a model architecture suitable for
    this type of data and the device that we want to run it on. We’ll come up with
    some initial ideas, test them out, and tweak our approach until we get good results.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下我们正在构建一个预测天气的模型。我们需要决定我们的目标（例如，预测明天是否会下雨），收集和标记数据集（例如过去几年的天气报告），设计我们将传递给模型的特征（也许是过去两天的平均条件），并选择适合这种数据类型和我们要运行的设备的模型架构。我们会想出一些初始想法，测试它们，并调整我们的方法，直到获得良好的结果。
- en: The next step in our workflow is training, which we explore in the following
    section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作流程中的下一步是训练，我们将在以下部分中探讨。
- en: Train the Model
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: Training is the process by which a model learns to produce the correct output
    for a given set of inputs. It involves feeding training data through a model and
    making small adjustments to it until it makes the most accurate predictions possible.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练是模型学习为给定的输入集合产生正确输出的过程。它涉及将训练数据输入模型，并对其进行小的调整，直到它能够做出最准确的预测。
- en: As we discussed earlier, a model is a network of simulated neurons represented
    by arrays of numbers arranged in layers. These numbers are known as *weights*
    and *biases*, or collectively as the network’s *parameters*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，模型是由排列成层的数字数组表示的模拟神经元网络。这些数字被称为*权重*和*偏置*，或者统称为网络的*参数*。
- en: When data is fed into the network, it is transformed by successive mathematical
    operations that involve the weights and biases in each layer. The output of the
    model is the result of running the input through these operations. [Figure 3-1](#network_layers)
    shows a simple network with two layers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据被输入网络时，它会通过每一层中的权重和偏置进行连续的数学运算进行转换。模型的输出是通过这些操作运行输入的结果。[图3-1](#network_layers)显示了一个具有两层的简单网络。
- en: The model’s weights start out with random values, and biases typically start
    with a value of 0\. During training, *batches* of data are fed into the model,
    and the model’s output is compared with the desired output (which in our case
    is the correct label, “normal” or “abnormal”). An algorithm called *backpropagation*
    adjusts the weights and biases incrementally so that over time, the output of
    the model gets closer to matching the desired value. Training, which is measured
    in *epochs* (meaning iterations), continues until we decide to stop.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的权重从随机值开始，偏置通常从值0开始。在训练过程中，将数据的*批次*输入模型，并将模型的输出与期望输出（在我们的情况下是正确的标签“正常”或“异常”）进行比较。一种称为*反向传播*的算法逐渐调整权重和偏置，以使随着时间的推移，模型的输出越来越接近期望值。训练以*周期*（意味着迭代）来衡量，直到我们决定停止为止。
- en: '![A simple deep learning network](Images/timl_0301.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![一个简单的深度学习网络](Images/timl_0301.png)'
- en: Figure 3-1\. A simple deep learning network with two layers
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。一个具有两层的简单深度学习网络
- en: We generally stop training when a model’s performance stops improving. At the
    point that it begins to make accurate predictions, it is said to have *converged*.
    To determine whether a model has converged, we can analyze graphs of its performance
    during training. Two common performance metrics are *loss* and *accuracy*. The
    loss metric gives us a numerical estimate of how far the model is from producing
    the expected answers, and the *accuracy* metric tells us the percentage of the
    time that it chooses the correct prediction. A perfect model would have a loss
    of 0.0 and an accuracy of 100%, but real models are rarely perfect.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，当一个模型的性能停止改善时，我们会停止训练。当它开始做出准确的预测时，就说它已经*收敛*。为了确定一个模型是否已经收敛，我们可以分析其在训练过程中的性能图表。两个常见的性能指标是*损失*和*准确性*。损失指标给出了一个数值估计，表明模型离产生预期答案有多远，而*准确性*指标告诉我们它选择正确预测的百分比。一个完美的模型将具有0.0的损失和100%的准确性，但真实模型很少是完美的。
- en: '[Figure 3-2](#convergence) shows the loss and accuracy during training for
    a deep learning network. You can see how as training progresses, accuracy increases
    and loss is reduced, until we reach a point at which the model no longer improves.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-2](#convergence)显示了深度学习网络在训练过程中的损失和准确性。您可以看到随着训练的进行，准确性增加，损失减少，直到达到一个模型不再改善的点。'
- en: To attempt to improve the model’s performance, we can change our model architecture,
    and we can adjust various values used to set up the model and moderate the training
    process. These values are collectively known as *hyperparameters*, and they include
    variables such as the number of training epochs to run and the number of neurons
    in each layer. Each time we make a change, we can retrain the model, look at the
    metrics, and decide whether to optimize further. Hopefully, time and iterations
    will result in a model with acceptable accuracy!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试改善模型的性能，我们可以改变模型的架构，调整用于设置模型和调节训练过程的各种值。这些值被统称为*超参数*，它们包括诸如要运行的训练周期数和每个层中的神经元数等变量。每次我们进行更改时，我们可以重新训练模型，查看指标，并决定是否进一步优化。希望，时间和迭代将产生一个具有可接受准确性的模型！
- en: '![A graph showing model convergence during training](Images/timl_0302.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![显示训练过程中模型收敛的图表](Images/timl_0302.png)'
- en: Figure 3-2\. A graph showing model convergence during training
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2。显示训练过程中模型收敛的图表
- en: Note
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It’s important to remember there’s no guarantee that you’ll be able to achieve
    good enough accuracy for the problem you are trying to solve. There isn’t always
    enough information contained within a dataset to make accurate predictions, and
    some problems just can’t be solved, even with state-of-the-art deep learning.
    That said, your model may be useful even if it is not 100% accurate. In the case
    of our factory example, being able to predict abnormal operation even part of
    the time could be a big help.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，并没有保证你能够达到足够好的准确性来解决你正在尝试解决的问题。数据集中并不总是包含足够的信息来进行准确的预测，有些问题甚至无法解决，即使使用最先进的深度学习技术也不行。也就是说，即使模型不是100%准确，它也可能是有用的。在我们的工厂示例中，即使只能部分时间预测异常操作也可能会大有帮助。
- en: Underfitting and overfitting
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欠拟合和过拟合
- en: The two most common reasons a model fails to converge are *underfitting* and
    *overfitting*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 模型无法收敛的两个最常见原因是*欠拟合*和*过拟合*。
- en: A neural network learns to *fit* its behavior to the patterns it recognizes
    in data. If a model is correctly fit, it will produce the correct output for a
    given set of inputs. When a model is *underfit*, it has not yet been able to learn
    a strong enough representation of these patterns to be able to make good predictions.
    This can happen for a variety of reasons, most commonly that the architecture
    is too small to capture the complexity of the system it is supposed to model or
    that it has not been trained on enough data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络学习*拟合*其在数据中识别的模式。如果一个模型被正确拟合，它将为给定的输入产生正确的输出。当一个模型*欠拟合*时，它还没有能够学习到足够强的这些模式的表示形式，以便能够做出良好的预测。这可能由于各种原因导致，最常见的是架构太小，无法捕捉应该建模的系统的复杂性，或者没有足够的数据进行训练。
- en: When a model is *overfit*, it has learned its training data too well. The model
    is able to exactly predict the minutiae of its training data, but it is not able
    to generalize its learning to data it has not previously seen. Often this happens
    because the model has managed to entirely memorize the training data, or it has
    learned to rely on a shortcut present in the training data but not in the real
    world.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型*过拟合*时，它已经对其训练数据学习得太好了。模型能够准确预测其训练数据的细微之处，但它无法将其学习推广到以前没有见过的数据。通常情况下，这是因为模型已经完全记住了训练数据，或者它已经学会依赖于训练数据中存在但现实世界中不存在的一种捷径。
- en: For example, imagine you are training a model to classify photos as containing
    either dogs or cats. If all the dog photos in your training data are taken outdoors,
    and all the cat photos are taken indoors, your model may learn to cheat and use
    the presence of the sky in each photograph to predict which animal it is. This
    means that it might misclassify future dog selfies if they happen to be taken
    indoors.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下，你正在训练一个模型来将照片分类为包含狗或猫。如果你的训练数据中所有的狗照片都是在室外拍摄的，而所有的猫照片都是在室内拍摄的，你的模型可能会学会作弊，并利用每张照片中天空的存在来预测是哪种动物。这意味着如果未来的狗自拍照片恰好是在室内拍摄的话，它可能会错误分类。
- en: There are many ways to fight overfitting. One possibility is to reduce the size
    of the model so it does not have enough capacity to learn an exact representation
    of its training set. A set of techniques known as *regularization* can be applied
    during training to reduce the degree of overfitting. To make the most of limited
    data, a technique called *data augmentation* can be used to generate new, artificial
    datapoints by slicing and dicing the existing data. But the best way to beat overfitting,
    when possible, is to get your hands on a larger and more varied dataset. More
    data always helps!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法来对抗过拟合。一种可能性是减小模型的大小，使其没有足够的容量来学习其训练集的精确表示。一组称为*正则化*的技术可以在训练过程中应用，以减少过拟合的程度。为了充分利用有限的数据，可以使用一种称为*数据增强*的技术，通过切片和切块现有数据来生成新的人工数据点。但是，打败过拟合的最佳方法，如果可能的话，是获得一个更大更多样化的数据集。更多的数据总是有帮助的！
- en: Training, validation, and testing
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练、验证和测试
- en: To assess the performance of a model, we can look at how it performs on its
    training data. However, this only tells us part of the story. During training,
    a model learns to fit its training data as closely as possible. As we saw earlier,
    in some cases the model will begin to overfit the training data, meaning that
    it will work well on the training data but not in real life.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要评估模型的性能，我们可以看看它在训练数据上的表现如何。然而，这只告诉我们故事的一部分。在训练过程中，模型学会尽可能紧密地拟合其训练数据。正如我们之前看到的，在某些情况下，模型将开始过拟合训练数据，这意味着它在训练数据上表现良好，但在现实生活中却不行。
- en: To understand when this is happening, we need to *validate* the model using
    new data that wasn’t used in training. It’s common to split a dataset into three
    parts—*training*, *validation*, and *test*. A typical split is 60% training data,
    20% validation, and 20% test. This splitting must be done so that each part contains
    the same distribution of information, and in a way that preserves the structure
    of the data. For example, since our data is a time series, we could potentially
    split it into three contiguous chunks of time. If our data were not a time series,
    we could just sample the datapoints randomly.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解何时发生这种情况，我们需要使用新数据*验证*模型，这些数据在训练中没有使用。将数据集分成三部分——*训练*、*验证*和*测试*是常见的。典型的分割是60%的训练数据，20%的验证数据和20%的测试数据。这种分割必须这样做，以便每个部分包含相同的信息分布，并以保持数据结构的方式进行。例如，由于我们的数据是时间序列，我们可以将其潜在地分成三个连续的时间段。如果我们的数据不是时间序列，我们可以随机抽样数据点。
- en: During training, the *training* dataset is used to train the model. Periodically,
    data from the *validation* dataset is fed through the model, and the loss is calculated.
    Because the model has not seen this data before, its loss score is a more reliable
    measure of how the model is performing. By comparing the training and validation
    loss (and accuracy, or whichever other metrics are available) over time, you can
    see whether the model is overfitting.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，*训练*数据集用于训练模型。定期，来自*验证*数据集的数据被馈送到模型中，并计算损失。因为模型以前没有见过这些数据，所以它的损失分数是模型表现的更可靠指标。通过比较训练和验证损失（以及准确性，或其他可用的指标），您可以看到模型是否过拟合。
- en: '[Figure 3-3](#overfitting) shows a model that is overfitting. You can see how
    as the training loss has decreased, the validation loss has gone up. This means
    that the model is becoming better at predicting the training data but is losing
    its ability to generalize to new data.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-3](#overfitting)显示了一个过拟合的模型。您可以看到随着训练损失的降低，验证损失却上升了。这意味着模型在预测训练数据方面变得更好，但失去了对新数据的泛化能力。'
- en: '![A graph showing model overfitting during training](Images/timl_0303.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![显示模型在训练过程中过拟合的图表](Images/timl_0303.png)'
- en: Figure 3-3\. A graph showing model overfitting during training
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3. 显示模型在训练过程中过拟合的图表
- en: As we tweak our models and training processes to improve performance and avoid
    overfitting, we will hopefully start to see our validation metrics improve.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调整我们的模型和训练过程以提高性能并避免过拟合时，我们希望看到我们的验证指标得到改善。
- en: However, this process has an unfortunate side effect. By optimizing to improve
    the validation metrics, we might just be nudging the model toward overfitting
    both the training *and* the validation data! Each adjustment we make will fit
    the model to the validation data slightly better, and in the end, we might have
    the same overfitting problem as before.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个过程有一个不幸的副作用。通过优化以改善验证指标，我们可能只是在推动模型朝着过拟合训练数据*和*验证数据的方向！我们所做的每一个调整都会使模型稍微更好地适应验证数据，最终，我们可能会遇到与之前相同的过拟合问题。
- en: To verify that this hasn’t happened, our final step when training a model is
    to run it on our *test* data and confirm that it performs as well as during validation.
    If it doesn’t, we have optimized our model to overfit both our training and validation
    data. In this case, we might need to go back to the drawing board and come up
    with a new model architecture, since if we continue to tweak to improve performance
    on our test data, we’ll just overfit to that, too.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这种情况没有发生，我们在训练模型的最后一步是在我们的*测试*数据上运行它，并确认它的表现与验证期间一样好。如果没有，我们已经优化了我们的模型以过拟合我们的训练和验证数据。在这种情况下，我们可能需要回到起点，提出一个新的模型架构，因为如果我们继续调整以提高在测试数据上的表现，我们也会过拟合到那里。
- en: After we have a model that performs acceptably well with training, validation,
    and test data, the training part of this process is over. Next, we get our model
    ready to run on-device!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个在训练、验证和测试数据上表现良好的模型后，这个过程的训练部分就结束了。接下来，我们准备好在设备上运行我们的模型！
- en: Convert the Model
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换模型
- en: Throughout this book, we use TensorFlow to build and train models. A TensorFlow
    model is essentially a set of instructions that tell an *interpreter* how to transform
    data in order to produce an output. When we want to use our model, we just load
    it into memory and execute it using the TensorFlow interpreter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们使用TensorFlow来构建和训练模型。一个TensorFlow模型本质上是一组指令，告诉一个*解释器*如何转换数据以产生输出。当我们想要使用我们的模型时，我们只需将其加载到内存中，并使用TensorFlow解释器执行它。
- en: However, TensorFlow’s interpreter is designed to run models on powerful desktop
    computers and servers. Since we’ll be running our models on tiny microcontrollers,
    we need a different interpreter that’s designed for our use case. Fortunately,
    TensorFlow provides an interpreter and accompanying tools to run models on small,
    low-powered devices. This set of tools is called TensorFlow Lite.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，TensorFlow的解释器是设计用于在强大的台式计算机和服务器上运行模型的。由于我们将在微型微控制器上运行我们的模型，我们需要一个专为我们的用例设计的不同解释器。幸运的是，TensorFlow提供了一个解释器和相关工具，用于在小型、低功耗设备上运行模型。这套工具称为TensorFlow
    Lite。
- en: Before TensorFlow Lite can run a model, it first must be converted into the
    TensorFlow Lite format and then saved to disk as a file. We do this using a tool
    named the *TensorFlow Lite Converter*. The converter can also apply special optimizations
    aimed at reducing the size of the model and helping it run faster, often without
    sacrificing performance.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow Lite可以运行模型之前，首先必须将其转换为TensorFlow Lite格式，然后保存到磁盘上作为文件。我们使用一个名为*TensorFlow
    Lite Converter*的工具来完成这个过程。转换器还可以应用特殊优化，旨在减小模型的大小并帮助其运行更快，通常不会牺牲性能。
- en: In [Chapter 13](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers), we
    dive into the details of TensorFlow Lite and how it helps us run models on tiny
    devices. For now, all you need to know is that you’ll need to convert your models,
    and that the conversion process is quick and easy.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第13章](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers)中，我们深入探讨了TensorFlow
    Lite的细节以及它如何帮助我们在微小设备上运行模型。目前，你只需要知道你需要转换你的模型，并且转换过程快速简单。
- en: Run Inference
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行推断
- en: After the model has been converted, it’s ready to deploy! We’ll now use the
    TensorFlow Lite for Microcontrollers C++ library to load the model and make predictions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 模型转换后，就可以部署了！我们现在将使用 TensorFlow Lite for Microcontrollers C++ 库来加载模型并进行预测。
- en: Since this is the part where our model meets our application code, we need to
    write some code that takes raw input data from our sensors and transforms it into
    the same form that our model was trained on. We then pass this transformed data
    into our model and run inference.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是我们的模型与应用代码相遇的部分，我们需要编写一些代码，从传感器获取原始输入数据并将其转换为模型训练的相同形式。然后，我们将这些转换后的数据传递给我们的模型并运行推断。
- en: This will result in output data containing predictions. In the case of our classifier
    model, the output will be a score for each of our classes, “normal” and “abnormal.”
    For models that classify data, typically the scores for all of the classes will
    sum to 1, and the class with the highest score will be the prediction. The higher
    the difference between the scores, the higher the confidence in the prediction.
    [Table 3-2](#example_outputs) lists some example outputs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致包含预测的输出数据。在我们的分类器模型的情况下，输出将是每个类别“正常”和“异常”的得分。对于分类数据的模型，通常所有类别的得分将总和为1，得分最高的类别将是预测。得分之间的差异越大，对预测的置信度就越高。[表3-2](#example_outputs)列出了一些示例输出。
- en: Table 3-2\. Example outputs
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-2\. 示例输出
- en: '| Normal score | Abnormal score | Explanation |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 正常得分 | 异常得分 | 解释 |'
- en: '| --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0.1 | 0.9 | High confidence in an abnormal state |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | 0.9 | 在异常状态下有高置信度 |'
- en: '| 0.9 | 0.1 | High confidence in a normal state |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 0.9 | 0.1 | 在正常状态下有高置信度 |'
- en: '| 0.7 | 0.3 | Slight confidence in a normal state |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 0.7 | 0.3 | 对正常状态有轻微置信度 |'
- en: '| 0.49 | 0.51 | Inconclusive result, since neither state is significantly ahead
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 0.49 | 0.51 | 结果不确定，因为两种状态都没有明显领先 |'
- en: In our factory machine example, each individual inference takes into account
    only a snapshot of the data—it tells us the probability of an abnormal state within
    the last 10 seconds, based on various sensor readings. Since real-world data is
    often messy and machine learning models aren’t perfect, it’s possible that a temporary
    glitch might result in an incorrect classification. For example, we might see
    a spike in a temperature value due to a temporary sensor malfunction. This transient,
    unreliable input might result in an output classification that momentarily doesn’t
    reflect reality.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工厂机器示例中，每个单独的推断仅考虑数据的一个快照——它告诉我们在过去10秒内出现异常状态的概率，基于各种传感器读数。由于现实世界的数据通常混乱，机器学习模型并不完美，因此可能会出现临时故障导致错误分类的情况。例如，由于临时传感器故障，我们可能会看到温度值的突然上升。这种瞬态、不可靠的输入可能导致输出分类短暂地不符合现实。
- en: To prevent these momentary glitches from causing problems, we could potentially
    take the average of all of our model’s outputs across a period of time. For example,
    we could run our model on the current data window every 10 seconds, and take the
    averages of the last 6 outputs to give a smoothed score for each class. This would
    mean that transient issues are ignored, and we only act upon consistent behavior.
    We use this technique to help with wake-word detection in [Chapter 7](ch07.xhtml#chapter_speech_wake_word_example).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这些瞬时故障导致问题，我们可能会在一段时间内对模型的所有输出取平均值。例如，我们可以每10秒在当前数据窗口上运行我们的模型，并取最后6个输出的平均值，以给出每个类别的平滑得分。这意味着瞬时问题被忽略，我们只对一致的行为采取行动。我们使用这种技术来帮助唤醒词检测在[第7章](ch07.xhtml#chapter_speech_wake_word_example)。
- en: After we have a score for each class, it’s up to our application code to decide
    what to do. Perhaps if an abnormal state is detected consistently for one minute,
    our code will send a signal to shut down our machine and alert the maintenance
    team.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在为每个类别得分后，由我们的应用代码决定如何处理。也许如果连续检测到异常状态一分钟，我们的代码将发送信号关闭机器并通知维护团队。
- en: Evaluate and Troubleshoot
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估和故障排除
- en: After we’ve deployed our model and have it running on-device, we’ll start to
    see whether its real-world performance approaches what we hoped. Even though we’ve
    already proved that our model makes accurate predictions on its test data, performance
    on the actual problem might be different.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们部署了模型并在设备上运行后，我们将开始看到其真实世界的性能是否达到我们的期望。即使我们已经证明我们的模型在测试数据上做出了准确的预测，但在实际问题上的表现可能会有所不同。
- en: There are many reasons why this might happen. For example, the data used in
    training might not be exactly representative of the data available in real operation.
    Perhaps due to local climate, our machine’s temperature is generally cooler than
    the one from which our dataset was collected. This might affect the predictions
    made by our model, such that they are no longer as accurate as expected.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 出现这种情况可能有很多原因。例如，训练中使用的数据可能并不完全代表实际操作中可用的数据。也许由于当地气候，我们机器的温度通常比我们收集数据集的那台机器要凉爽。这可能会影响我们模型的预测，使其不再像预期的那样准确。
- en: Another possibility is that our model might have overfit our dataset without
    us realizing. In [“Train the Model”](#ch3_train_the_model), we learned how this
    can happen by accident when the dataset happens to contain additional signals
    that a model can learn to recognize in place of those we expect.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能性是我们的模型可能已经过度拟合我们的数据集，而我们没有意识到。在[“训练模型”](#ch3_train_the_model)中，我们学到了当数据集恰好包含模型可以学习识别的额外信号时，这种情况可能会发生。
- en: If our model isn’t working in production, we’ll need to do some troubleshooting.
    First, we rule out any hardware problems (like faulty sensors or unexpected noise)
    that might be affecting the data that gets to our model. Second, we capture some
    data from the device where the model is deployed and compare it with our original
    dataset to make sure that it is in the same ballpark. If not, perhaps there’s
    a difference in environmental conditions or sensor characteristics that we weren’t
    expecting. If the data checks out, it might be that overfitting is the problem.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型在生产中不起作用，我们需要进行一些故障排除。首先，我们排除可能影响到达我们模型的数据的任何硬件问题（如故障传感器或意外噪音）。其次，我们从部署模型的设备中捕获一些数据，并将其与我们的原始数据集进行比较，以确保它在同一范围内。如果不是，也许环境条件或传感器特性存在差异，而我们没有预料到。如果数据检查通过，可能过拟合是问题所在。
- en: After we’ve ruled out hardware issues, the best fix for overfitting is often
    to train with more data. We can capture additional data from our deployed hardware,
    combine it with our original dataset, and retrain our model. In the process, we
    can apply regularization and data augmentation techniques to help make the most
    of the data we have.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在排除硬件问题后，解决过拟合问题的最佳方法通常是使用更多数据进行训练。我们可以从部署的硬件中捕获额外的数据，将其与原始数据集结合起来，然后重新训练我们的模型。在这个过程中，我们可以应用正则化和数据增强技术，以帮助充分利用我们拥有的数据。
- en: Reaching good real-world performance can sometimes take some iteration on your
    model, your hardware, and the accompanying software. If you run into a problem,
    treat it like any other technology issue. Take a scientific approach to troubleshooting,
    eliminating possible factors, and analyze your data to figure out what is going
    wrong.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 要达到良好的实际性能，有时可能需要对模型、硬件和相关软件进行一些迭代。如果遇到问题，要像处理其他技术问题一样对待。采用科学方法进行故障排除，排除可能的因素，并分析数据以找出问题所在。
- en: Wrapping Up
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Now that you’re familiar with the basic workflow used by machine learning practitioners,
    we’re ready to take the next steps in our TinyML adventure.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经熟悉了机器学习从业者使用的基本工作流程，我们准备在TinyML冒险中迈出下一步。
- en: In [Chapter 4](ch04.xhtml#chapter_hello_world_training), we’ll build our first
    model and deploy it to some tiny hardware!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第四章](ch04.xhtml#chapter_hello_world_training)中，我们将构建我们的第一个模型并将其部署到一些微型硬件上！
- en: ^([1](ch03.xhtml#idm46473587162472-marker)) This definition of the word *tensor*
    is different from the mathematical and physics definitions of the word, but it
    has become the norm in data science.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个对于“张量”一词的定义与数学和物理学对该词的定义不同，但在数据科学中已成为常态。
