- en: 4 Sentence classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 句子分类
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Handling variable-length input with recurrent neural networks (RNN)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用循环神经网络（RNN）处理长度可变的输入
- en: Working with RNNs and their variants (LSTMs and GRUs)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理RNN及其变体（LSTM和GRU）
- en: Using common evaluation metrics for classification problems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用常见的分类问题评估指标
- en: Developing and configuring a training pipeline using AllenNLP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AllenNLP开发和配置训练管道
- en: Building a language detector as a sentence classification task
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将语言识别器构建为句子分类任务
- en: In this chapter, we are going to study the task of sentence classification,
    where an NLP model receives a sentence and assigns some label to it. A spam filter
    is an application of sentence classification. It receives an email message and
    assigns whether or not it is spam. If you want to classify news articles into
    different topics (business, politics, sports, and so on), it’s also a sentence-classification
    task. Sentence classification is one of the simplest NLP tasks that has a wide
    range of applications, including document classification, spam filtering, and
    sentiment analysis. Specifically, we are going to revisit the sentiment classifier
    we introduced in chapter 2 and discuss its components in detail. At the end of
    this section, we are going to study another application of sentence classification—language
    detection.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究句子分类任务，其中NLP模型接收一个句子并为其分配一些标签。垃圾邮件过滤器是句子分类的一个应用。它接收一封电子邮件并指定它是否为垃圾邮件。如果你想将新闻文章分类到不同的主题（商业、政治、体育等），也是一个句子分类任务。句子分类是最简单的NLP任务之一，具有广泛的应用范围，包括文档分类、垃圾邮件过滤和情感分析。具体而言，我们将重新审视第2章中介绍的情感分类器，并详细讨论其组成部分。在本节结束时，我们将研究句子分类的另一个应用——语言检测。
- en: 4.1 Recurrent neural networks (RNNs)
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 循环神经网络（RNN）
- en: The first step in sentence classification is to represent variable-length sentences
    using neural networks (RNNs). In this section, I’m going to present the concept
    of recurrent neural networks, one of the most important concepts in deep NLP.
    Many modern NLP models use RNNs in some way. I’ll explain why they are important,
    what they do, and introduce their simplest variant.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分类的第一步是使用神经网络（RNN）表示长度可变的句子。在本节中，我将介绍循环神经网络的概念，这是深度NLP中最重要的概念之一。许多现代NLP模型以某种方式使用RNN。我将解释它们为什么很重要，它们的作用是什么，并介绍它们的最简单变体。
- en: 4.1.1 Handling variable-length input
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 处理长度可变的输入
- en: The Skip-gram network structure shown in the previous chapter was simple. It
    takes a word vector of a fixed size, runs it through a linear layer, and obtains
    a distribution of scores over all the context words. The structure and the size
    of the input, output, and the network are all fixed throughout the training.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章中展示的Skip-gram网络结构很简单。它接受一个固定大小的词向量，通过线性层运行它，得到所有上下文词之间的分数分布。网络的结构和大小以及输入输出都在训练期间固定。
- en: However, many, if not most, of what we deal with in NLP are sequences of variable
    lengths. For example, words, which are sequences of characters, can be short (“a,”
    “in”) or long (“internationalization”). Sentences (sequences of words) and documents
    (sequences of sentences) can be of any lengths. Even characters, if you look at
    them as sequences of strokes, can be simple (e.g., “O” and “L” in English) or
    more complex (e.g., “鬱” is a Chinese character meaning “depression” which, depressingly,
    has 29 strokes).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自然语言处理（NLP）中面临的许多，如果不是大多数情况下，都是长度可变的序列。例如，单词是字符序列，可以短（“a”，“in”）也可以长（“internationalization”）。句子（单词序列）和文档（句子序列）可以是任何长度。即使是字符，如果将它们看作笔画序列，则可以是简单的（如英语中的“O”和“L”）或更复杂的（例如，“鬱”是一个包含29个笔画，并表示“抑郁”的中文汉字）。
- en: As we discussed in the previous chapter, neural networks can handle only numbers
    and arithmetic operations. That was why we needed to convert words and documents
    to numbers through embeddings. We used linear layers to convert a fixed-length
    vector into another. But to do something similar with variable-length inputs,
    we need to figure out how to structure the neural networks so that they can handle
    them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中讨论的那样，神经网络只能处理数字和算术运算。这就是为什么我们需要通过嵌入将单词和文档转换为数字的原因。我们使用线性层将一个固定长度的向量转换为另一个向量。但是，为了处理长度可变的输入，我们需要找到一种处理方法，使得神经网络可以对其进行处理。
- en: 'One idea is to first convert the input (e.g., a sequence of words) to embeddings,
    that is, a sequence of vectors of floating-point numbers, then average them. Let’s
    assume the input sentence is sentence = ["john", "loves", "mary", "."], and you
    already know word embeddings for each word in the sentence v("john"), v("loves"),
    and so on. The average can be obtained with the following code and illustrated
    in figure 4.1:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个想法是首先将输入（例如，一系列单词）转换为嵌入，即一系列浮点数向量，然后将它们平均。假设输入句子为sentence = ["john", "loves",
    "mary", "."]，并且你已经知道句子中每个单词的单词嵌入v("john")、v("loves")等。平均值可以用以下代码获得，并在图4.1中说明：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![CH04_F01_Hagiwara](../Images/CH04_F01_Hagiwara.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F01_Hagiwara](../Images/CH04_F01_Hagiwara.png)'
- en: Figure 4.1 Averaging embedding vectors
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 平均嵌入向量
- en: This method is quite simple and is actually used in many NLP applications. However,
    it has one critical issue, which is that it cannot take word order into account.
    Because the order of input elements doesn’t affect the result of averaging, you’d
    get the same vector for both “Mary loves John” and “John loves Mary.” Although
    it’s up to the task in hand, it’s hard to imagine many NLP applications would
    want this kind of behavior.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法相当简单，实际上在许多自然语言处理应用中都有使用。然而，它有一个关键问题，即它无法考虑词序。因为输入元素的顺序不影响平均结果，你会得到“Mary
    loves John”和“John loves Mary”两者相同的向量。尽管它能胜任手头的任务，但很难想象有多少自然语言处理应用会希望这种行为。
- en: If we step back and reflect how we humans read language, this “averaging” is
    far from actuality. When we read a sentence, we don’t usually read individual
    words in isolation and remember them first, then move on to figuring out what
    the sentence means. We usually scan the sentence from the beginning, one word
    at a time, while holding what the “partial” sentence means up until the part you
    are reading in our short-term memory. In other words, you maintain some sort of
    mental representation of the sentence while you read it. When you reach the end
    of the sentence, the mental representation is its meaning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们退后一步，思考一下我们人类如何阅读语言，这种“平均”与现实相去甚远。当我们阅读一句话时，我们通常不会孤立地阅读单个单词并首先记住它们，然后再弄清楚句子的含义。我们通常从头开始扫描句子，逐个单词地阅读，同时将“部分”句子在我们的短期记忆中的含义保持住直到你正在阅读的部分。换句话说，你在阅读时保持了一种对句子的心理表征。当你达到句子的末尾时，这种心理表征就是它的含义。
- en: Can we design a neural network structure that simulates this incremental reading
    of the input? The answer is a resounding yes. That structure is called *recurrent
    neural networks* (RNNs), which I’ll explain in detail next.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否可以设计一个神经网络结构来模拟这种对输入的逐步阅读？答案是肯定的。这种结构被称为*循环神经网络*（RNNs），我将在接下来详细解释。
- en: 4.1.2 RNN abstraction
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 RNN抽象
- en: 'If you break down the reading process mentioned earlier, its core is the repetition
    of the following series of operations:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你分解前面提到的阅读过程，其核心是以下一系列操作的重复：
- en: Read a word.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读一个词。
- en: Based on what has been read so far (your “mental state”), figure out what the
    word means.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据迄今为止所阅读的内容（你的“心理状态”），弄清楚这个词的含义。
- en: Update the mental state.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新心理状态。
- en: Move on to the next word.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续下一个词。
- en: 'Let’s see how this works using a concrete example. If the input sentence is
    sentence = ["john", "loves", "mary", "."], and each word is already represented
    as a word-embedding vector. Also, let’s denote your “mental state” as state, which
    is initialized by init_state(). Then, the reading process is represented by the
    following incremental operations:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个具体的例子来看看这是如何工作的。如果输入句子是sentence = ["john", "loves", "mary", "."]，并且每个单词已经表示为单词嵌入向量。另外，让我们将你的“心理状态”表示为state，它由init_state()初始化。然后，阅读过程由以下递增操作表示：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The final value of state becomes the representation of the entire sentence from
    this process. Notice that if you change the order in which these words are processed
    (e.g., by flipping “John” and “Mary”), the final value of state also changes,
    meaning that the state also encodes some information about the word order.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: state的最终值成为此过程中整个句子的表示。请注意，如果你改变这些单词处理的顺序（例如，交换“John”和“Mary”），state的最终值也会改变，这意味着state也编码了一些有关词序的信息。
- en: 'You can achieve something similar if you can design a network substructure
    that is applied to each element of the input while updating some internal states.
    RNNs are neural network structures that do exactly this. In a nutshell, an RNN
    is a neural network with a loop. At its core is an operation that is applied to
    every element in the input as they come in. If you wrote what RNNs do in Python
    pseudocode, it’d be like the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你可以设计一个网络子结构，可以在更新一些内部状态的同时应用于输入的每个元素，那么你可以实现类似的功能。RNNs 就是完全这样做的神经网络结构。简而言之，RNN
    是一个带有循环的神经网络。其核心是在输入中的每个元素上应用的操作。如果你用 Python 伪代码来表示 RNN 做了什么，就会像下面这样：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that there is state that gets initialized first and passed around during
    the iteration. For every input word, state is updated based on the previous state
    and the input using the function update. The network substructure corresponding
    to this step (the code block inside the loop) is called a *cell*. This stops when
    the input is exhausted, and the final value of state becomes the result of this
    RNN. See figure 4.2 for the illustration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里有一个被初始化并在迭代过程中传递的状态。对于每个输入单词，状态会根据前一个状态和输入使用`update`函数进行更新。对应于这个步骤（循环内的代码块）的网络子结构被称为*单元*。当输入用尽时，这个过程停止，状态的最终值成为该
    RNN 的结果。见图 4.2 进行说明。
- en: '![CH04_F02_Hagiwara](../Images/CH04_F02_Hagiwara.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F02_Hagiwara](../Images/CH04_F02_Hagiwara.png)'
- en: Figure 4.2 RNN abstraction
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 RNN 抽象
- en: You can see the parallelism here. When you are reading a sentence (sequence
    of words), your internal mental representation of the sentence, state, is updated
    after reading each word. You can assume that the final state encodes the representation
    of the entire sentence.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你可以看到并行性。当你阅读一个句子（一串单词）时，每读一个单词后你内部心理对句子的表示，即状态，会随之更新。可以假设最终状态编码了整个句子的表示。
- en: The only remaining work is to design two functions—init_state() and update().
    The state is usually initialized with zero (i.e., a vector filled with zeros),
    so you usually don’t have to worry about how to go about defining the former.
    It’s more important how you design update(), which determines the characteristics
    of the RNN.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的工作是设计两个函数——`init_state()` 和 `update()`。通常，状态初始化为零（即一个填满零的向量），所以你通常不用担心如何定义前者。更重要的是如何设计
    `update()`，它决定了 RNN 的特性。
- en: 4.1.3 Simple RNNs and nonlinearity
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 简单 RNNs 和非线性
- en: 'In section 3.4.3, I talked about how to implement a linear layer with an arbitrary
    number of inputs and outputs. Can we do something similar and implement update(),
    which is basically a function that takes two input variables and produces one
    output variable? After all, a cell is a neural network with its own input and
    output, right? The answer is yes, and it looks like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3.4.3 节中，我解释了如何使用任意数量的输入和输出来实现一个线性层。我们是否可以做类似的事情，并实现`update()`，它基本上是一个接受两个输入变量并产生一个输出变量的函数呢？毕竟，一个单元是一个有自己输入和输出的神经网络，对吧？答案是肯定的，它看起来像这样：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that this is strikingly similar to the linear2() function in section
    3.4.3\. In fact, if you ignore the difference in variable names, it’s exactly
    the same except for the f() function. An RNN defined by this type of the update
    function is called a *simple RNN* or *Elman RNN*, which is, as its name suggests,
    one of the simplest RNN structures.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这与第 3.4.3 节中的 `linear2()` 函数非常相似。实际上，如果忽略变量名称的差异，除了 `f()` 函数之外，它们是完全一样的。由此类型的更新函数定义的
    RNN 被称为*简单 RNN*或*Elman RNN*，正如其名称所示，它是最简单的 RNN 结构之一。
- en: You may be wondering, then, what is this function f() doing here? What does
    it look like? Do we need it here at all? The function, called an *activation function*
    or *nonlinearity*, takes a single input (or a vector) and transforms it (or every
    element of a vector) in a nonlinear fashion. Many kinds of nonlinearities exist,
    and they play an indispensable role in making neural networks truly powerful.
    What they exactly do and why they are important requires some math to understand,
    which is outside the scope of this book, but I’ll attempt an intuitive explanation
    with a simple example next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，这里的 `f()` 函数是做什么的？它是什么样的？我们是否需要它？这个函数被称为*激活函数*或*非线性函数*，它接受一个输入（或一个向量）并以非线性方式转换它（或转换向量的每个元素）。存在许多种非线性函数，它们在使神经网络真正强大方面起着不可或缺的作用。它们确切地做什么以及为什么它们重要需要一些数学知识来理解，这超出了本书的范围，但我将尝试用一个简单的例子进行直观解释。
- en: Imagine you are building an RNN that recognizes “grammatical” English sentences.
    Differentiating grammatical sentences from ungrammatical ones is itself a difficult
    NLP problem, which is, in fact, a well-established research area (see section
    1.2.1), but here, let’s simplify it and consider agreement only between the subject
    and the verb. Let’s further simplify it and assume that there are only four words
    in this “language”—“I,” “you,” “am,” and “are.” If the sentence is either “I am”
    or “you are,” it’s grammatically correct. The other two combinations, “I are”
    and “you am,” are incorrect. What you want to build is an RNN that outputs 1 for
    the correct sentences while producing 0 for the incorrect ones. How would you
    go about building such a neural network?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在构建一个识别“语法正确”的英语句子的 RNN。区分语法正确的句子和不正确的句子本身就是一个困难的自然语言处理问题，实际上是一个成熟的研究领域（参见第1.2.1节），但在这里，让我们简化它，并考虑主语和动词之间的一致性。让我们进一步简化，并假设这个“语言”中只有四个词——“I”，“you”，“am”
    和 “are”。如果句子是“I am” 或 “you are”，那么它就是语法正确的。另外两种组合，“I are” 和 “you am”，是不正确的。你想要构建的是一个
    RNN，对于正确的句子输出1，对于不正确的句子输出0。你会如何构建这样一个神经网络？
- en: The first step in almost every modern NLP model is to represent words with embeddings.
    As mentioned in the previous chapter, they are usually learned from a large dataset
    of natural language text, but here, we are simply going to give them some predefined
    values, as shown in figure 4.3.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个现代 NLP 模型的第一步都是用嵌入来表示单词。如前一章所述，它们通常是从大型自然语言文本数据集中学习到的，但在这里，我们只是给它们一些预定义的值，如图4.3所示。
- en: '![CH04_F03_Hagiwara](../Images/CH04_F03_Hagiwara.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F03_Hagiwara](../Images/CH04_F03_Hagiwara.png)'
- en: Figure 4.3 Recognizing grammatical English sentences using an RNN
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 使用 RNN 识别语法正确的英语句子
- en: 'Now, let’s imagine there was no activation function. The previous update_ simple()
    function simplifies to the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们假设没有激活函数。前面的 update_simple() 函数简化为以下形式：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will assume the initial value of state is simply [0, 0], because the specific
    initial values are not relevant to the discussion here. The RNN takes the first
    word embedding, x1, updates state, takes the second word embedding, x2, then produces
    the final state, which is a two-dimensional vector. Finally, the two elements
    in this vector are summed and converted to result. If result is close to 1, the
    sentence is grammatical. Otherwise, it is not. If you apply the update_simple_linear()
    function twice and simplify it a little bit, you get the following function, which
    is all this RNN does, after all:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设状态的初始值简单地为 [0, 0]，因为具体的初始值与此处的讨论无关。RNN 接受第一个单词嵌入 x1，更新状态，接受第二个单词嵌入 x2，然后生成最终状态，即一个二维向量。最后，将这个向量中的两个元素相加并转换为
    result。如果 result 接近于1，则句子是语法正确的。否则，不是。如果你应用 update_simple_linear() 函数两次并稍微简化一下，你会得到以下函数，这就是这个
    RNN 的全部功能：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Remember, w1, w2, and b are parameters of the model (aka “magic constants”)
    that need to be trained (adjusted). Here, instead of adjusting these parameters
    using a training dataset, let’s assign some arbitrary values and see what happens.
    For example, when w1 = [1, 0], w2 = [0, 1], and b = [0, 0], the input and the
    output of this RNN will be as shown in figure 4.4.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，w1、w2 和 b 是模型的参数（也称为“魔法常数”），需要进行训练（调整）。在这里，我们不是使用训练数据集调整这些参数，而是将一些任意值赋给它们，然后看看会发生什么。例如，当
    w1 = [1, 0]，w2 = [0, 1]，b = [0, 0] 时，这个 RNN 的输入和输出如图4.4所示。
- en: '![CH04_F04_Hagiwara](../Images/CH04_F04_Hagiwara.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F04_Hagiwara](../Images/CH04_F04_Hagiwara.png)'
- en: Figure 4.4 Input and output when w1 = [1, 0], w2 = [0, 1], and b = [0, 0] without
    an activation function
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 当 w1 = [1, 0]，w2 = [0, 1]，b = [0, 0] 且没有激活函数时的输入和输出
- en: If you look at the values of result, this RNN groups ungrammatical sentences
    (e.g., “I are”) with grammatical ones (e.g., “you are”), which is not the desired
    behavior. How about we try another set of values for the parameters? Let’s use
    w1 = [1, 0], w2 = [-1, 0], and b = [0, 0] and see what happens (see figure 4.5
    for the result).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看结果的值，这个 RNN 将不合语法的句子（例如，“I are”）与合语法的句子（例如，“you are”）分组在一起，这不是我们期望的行为。那么，我们尝试另一组参数值如何？让我们使用
    w1 = [1, 0]，w2 = [-1, 0]，b = [0, 0]，看看会发生什么（参见图4.5的结果）。
- en: '![CH04_F05_Hagiwara](../Images/CH04_F05_Hagiwara.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F05_Hagiwara](../Images/CH04_F05_Hagiwara.png)'
- en: Figure 4.5 Input and output when w1 = [1, 0], w2 = [-1, 0], and b = [0, 0] without
    an activation function
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 当 w1 = [1, 0]，w2 = [-1, 0]，b = [0, 0] 且没有激活函数时的输入和输出
- en: This is much better, because the RNN is successful in grouping ungrammatical
    sentences by assigning 0 to both “I are” and “you am.” However, it also assigns
    completely opposite values (2 and -2) to grammatical sentences (“I am” and “you
    are”).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这好多了，因为 RNN 成功地通过将 “I are” 和 “you am” 都赋值为 0 来将不符合语法的句子分组。然而，它也给语法正确的句子（“I am”
    和 “you are”）赋予了完全相反的值（2 和 -2）。
- en: I’m going to stop here, but as it turns out, you cannot use this neural network
    to differentiate grammatical sentences from ungrammatical ones, no matter how
    hard you try. Despite the values you assign to the parameters, this RNN cannot
    produce results that are close enough to the desired values and, thus, are able
    to group sentences by their grammaticality.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我要在这里停下来，但事实证明，无论你如何努力，都不能使用这个神经网络区分语法正确的句子和不正确的句子。尽管你给参数分配了值，但这个 RNN 无法产生足够接近期望值的结果，因此无法根据它们的语法性将句子分组。
- en: Let’s step back and think why this is the case. If you look at the previous
    update function, all it does is multiply the input by some value and add them
    up. In more specific terms, it only transforms the input *in a linear fashion*.
    The result of this neural network always changes by some constant amount when
    you change the value of the input by some amount. But this is obviously not desirable—you
    want the result to be 1 only when the input variables are some specific values.
    In other words, you don’t want this RNN to be linear; you want it to be nonlinear.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退一步思考为什么会出现这种情况。如果你看一下之前的更新函数，它所做的就是将输入乘以一些值然后相加。更具体地说，它只是*以线性方式*转换输入。当你改变输入的值时，这个神经网络的结果总是会以某个恒定的量变化。但显然这是不可取的——你希望结果只在输入变量是某些特定值时才为
    1。换句话说，你不希望这个 RNN 是线性的；你希望它是非线性的。
- en: To use an analogy, imagine you can use only assignment (“=”), addition (“+”),
    and multiplication (“*”) in your programming language. You can tweak the input
    values to some degree to come up with the result, but you can’t write more complex
    logic in such a restricted setting.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 用类比的方式来说，想象一下，假设你的编程语言只能使用赋值（“=”）、加法（“+”）和乘法（“*”）。你可以在这样受限制的环境中调整输入值以得到结果，但在这样的情况下，你无法编写更复杂的逻辑。
- en: 'Now let’s put the activation function f() back and see what happens. The specific
    activation function we are going to use is called *the hyperbolic tangent function*,
    or more commonly, *tanh*, which is one of the most commonly used activation functions
    in neural networks. The details of this function are not important in this discussion,
    but in a nutshell, it behaves as follows: tanh doesn’t do much to the input when
    it is close to zero, for example, 0.3 or -0.2\. In other words, the input passes
    through the function almost unchanged. When the input is far from zero, tanh tries
    to squeeze it between -1 and 1\. For example, when the input is large (say, 10.0),
    the output becomes very close to 1.0, whereas when it is small (say, -10.0), the
    output becomes almost -1.0\. This creates an effect similar to the OR logical
    gate (or an AND gate, depending on the weights) if two or more variables are fed
    into the activation function. The output of the gate becomes ON (~1) and OFF (~-1)
    depending on the input.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们把激活函数 f() 加回去，看看会发生什么。我们将使用的具体激活函数称为*双曲正切函数*，或者更常见的是*tanh*，它是神经网络中最常用的激活函数之一。在这个讨论中，这个函数的细节并不重要，但简而言之，它的行为如下：当输入接近零时，tanh
    对输入的影响不大，例如，0.3 或 -0.2。换句话说，输入几乎不经过函数而保持不变。当输入远离零时，tanh 试图将其压缩在 -1 和 1 之间。例如，当输入很大（比如，10.0）时，输出变得非常接近
    1.0，而当输入很小时（比如，-10.0）时，输出几乎为 -1.0。如果将两个或更多变量输入激活函数，这会产生类似于 OR 逻辑门（或 AND 门，取决于权重）的效果。门的输出根据输入变为开启（~1）和关闭（~-1）。
- en: When w1 = [-1, 2], w2 = [-1, 2], b = [0, 1], and the tanh activation function
    is used, the result of the RNN becomes a lot closer to what we desire (see figure
    4.6). If you round them to the closest integers, the RNN successfully groups sentences
    by their grammaticality.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当 w1 = [-1, 2]，w2 = [-1, 2]，b = [0, 1]，并且使用 tanh 激活函数时，RNN 的结果更接近我们所期望的（见图 4.6）。如果将它们四舍五入为最接近的整数，RNN
    成功地通过它们的语法性将句子分组。
- en: '![CH04_F06_Hagiwara](../Images/CH04_F06_Hagiwara.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F06_Hagiwara](../Images/CH04_F06_Hagiwara.png)'
- en: Figure 4.6 Input and output when w1 = [-1, 2], w2 = [-1, 2], and b = [0, 1]
    with an activation function
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 当 w1 = [-1, 2]，w2 = [-1, 2]，b = [0, 1] 且激活函数为时的输入和输出
- en: To use the same analogy, using activation functions in your neural networks
    is like using ANDs and ORs and IFs in your programming language, in addition to
    basic math operations like addition and multiplication. In this way, you can write
    complex logic and model complex interactions between input variables, like the
    example in this section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用同样的类比，将激活函数应用于你的神经网络就像在你的编程语言中使用AND、OR和IF以及基本的数学运算，比如加法和乘法一样。通过这种方式，你可以编写复杂的逻辑并模拟输入变量之间的复杂交互，就像本节的例子一样。
- en: NOTE This example I use in this section is a slightly modified version of the
    popular XOR (or *exclusive-or*) example commonly seen in deep learning textbooks.
    This is the most basic and simplest example that can be solved by neural networks
    but not by other linear models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节中我使用的例子是流行的XOR（或*异或*）例子的一个略微修改版本，通常在深度学习教材中见到。这是神经网络可以解决但其他线性模型无法解决的最基本和最简单的例子。
- en: Some final notes on RNNs—they are trained just like any other neural networks.
    The final outcome is compared with the desired outcome using the loss function,
    then the difference between the two—the loss—is used for updating the “magic constants.”
    The magic constants are, in this case, w1, w2, and b in the update_simple() function.
    Note that the update function and its magic constants are identical across all
    the timesteps in the loop. This means that what RNNs are learning is a general
    form of updates that can be applied to any situation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 关于RNN的一些最后说明——它们的训练方式与任何其他神经网络相同。最终的结果与期望结果使用损失函数进行比较，然后两者之间的差异——损失——用于更新“魔术常数”。在这种情况下，魔术常数是update_simple()函数中的w1、w2和b。请注意，更新函数及其魔术常数在循环中的所有时间步中都是相同的。这意味着RNN正在学习的是可以应用于任何情况的一般更新形式。
- en: 4.2 Long short-term memory units (LSTMs) and gated recurrent units (GRUs)
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 长短期记忆单元（LSTMs）和门控循环单元（GRUs）
- en: In fact, the simple RNNs that we discussed earlier are rarely used in real-world
    NLP applications due to one problem called the *vanishing gradients problem*.
    In this section, I’ll show the issue associated with simple RNNs and how more
    popular RNN architectures, namely LSTMs and GRUs, solve this particular problem.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们之前讨论过的简单RNN在真实世界的NLP应用中很少使用，因为存在一个称为*梯度消失问题*的问题。在本节中，我将展示与简单RNN相关的问题以及更流行的RNN架构，即LSTMs和GRUs，如何解决这个特定问题。
- en: 4.2.1 Vanishing gradients problem
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 梯度消失问题
- en: 'Just like any programming language, if you know the length of the input, you
    can rewrite a loop without using one. An RNN can also be rewritten without using
    a loop, which makes it look just like a regular neural network with many layers.
    For example, if you know that there are only six words in the input, the rnn()
    from earlier can be rewritten as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何编程语言一样，如果你知道输入的长度，你可以在不使用循环的情况下重写一个循环。RNN也可以在不使用循环的情况下重写，这使它看起来就像一个具有许多层的常规神经网络。例如，如果你知道输入中只有六个单词，那么之前的rnn()可以重写如下：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Representing RNNs without loops is called *unrolling*. Now we know what update()
    looks like for a simple RNN (update_simple), so we can replace the function calls
    with their bodies, as shown here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 不带循环的表示RNN被称为*展开*。现在我们知道简单RNN的update()是什么样子（update_simple），所以我们可以用其实体替换函数调用，如下所示：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is getting a bit ugly, but I just want you to notice the very deeply nested
    function calls and multiplications. Now, recall the task we wanted to accomplish
    in the previous section—classifying grammatical English sentences by recognizing
    subject-verb agreement. Let’s say the input is sentence = ["The", "books", "I",
    "read", "yesterday", "were"]. In this case, the innermost function call processes
    the first word “The,” the next one processes the second word “books,” and so on,
    all the way to the outermost function call, which processes “were.” If we rewrite
    the previous pseudocode slightly, as shown in the next code snippet, you can understand
    it more intuitively:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这变得有点丑陋，但我只是想让你注意到非常深度嵌套的函数调用和乘法。现在，回想一下我们在上一节中想要完成的任务——通过识别主谓一致来对语法正确的英语句子进行分类。假设输入是sentence
    = ["The", "books", "I", "read", "yesterday", "were"]。在这种情况下，最内层的函数调用处理第一个词“The”，下一个处理第二个词“books”，依此类推，一直到最外层的函数调用，处理“were”。如果我们稍微修改前面的伪代码，如下代码片段所示，你就可以更直观地理解它：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To recognize that the input is indeed a grammatical English sentence (or a prefix
    of a sentence), the RNN needs to retain the information about the subject (“the
    books”) in state until it sees the verb (“were”) without being distracted by anything
    in between (“I read yesterday”). In the previous pseudocode, the states are represented
    by the return values of function calls, so the information about the subject (return
    value of process_main_subject) needs to propagate all the way up in this chain
    until it reaches the outermost function (process_main_verb). This is starting
    to sound like a difficult task.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别输入确实是一句语法正确的英语句子（或一句句子的前缀），RNN需要保留有关主语（“书”）的信息在状态中，直到看到动词（“were”）而不会被中间的任何东西（“我昨天读了”）分散注意力。在先前的伪代码中，状态由函数调用的返回值表示，因此关于主题的信息（process_main_subject的返回值）需要在链中传播到达最外层函数（process_main_verb）。这开始听起来像是一项困难的任务。
- en: Things don’t look any better when it comes to training this RNN. RNNs, as well
    as any other neural networks, are trained using an algorithm called *backpropagation*.
    Backpropagation is a process where the components of a neural network communicate
    with previous components on how to adjust the parameters to minimize the loss.
    This is how it works for this particular case. First, you look at the outcome,
    that is, the return value of is_grammatical()and compare it with what you desire.
    The difference between these two is called the *loss*. The outermost function,
    is_grammatical(), basically has four ways to decrease the loss to make its output
    closer to what is desired—1) adjust w1 while fixing the return value of the nested
    function process_adverb(), 2) adjust w2, 3) adjust b, or 4) adjust the return
    value of process_adverb() while fixing the parameters. Adjusting the parameters
    (w1, w2, and b) is the easy part because the function knows the exact effect of
    adjusting each parameter to its return value. Adjusting the return value of the
    previous function, however, is not easy, because the caller has no idea about
    the inner workings of the function. Because of this, the caller tells the previous
    function (callee) to adjust its return values to minimize the loss. See figure
    4.7 for how the loss is propagated back to the parameters and previous functions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及训练该RNN时，情况并不好。 RNN和其他任何神经网络都使用称为*反向传播*的算法进行训练。反向传播是一种过程，在该过程中，神经网络的组成部分与先前的组成部分通信，以便调整参数以最小化损失。对于这个特定的示例，它是如何工作的。首先，您查看结果，即is_grammatical的返回值()并将其与您期望的内容进行比较。这两者之间的差称为*损失*。最外层函数is_grammatical()基本上有四种方式来减少损失，使其输出更接近所需内容：1)调整w1，同时固定嵌套函数process_adverb()的返回值，2)调整w2，3)调整b，或4)调整process_adverb()的返回值，同时固定参数。调整参数（w1、w2和b）很容易，因为函数知道调整每个参数对其返回值的确切影响。然而，调整上一个函数的返回值是不容易的，因为调用者不知道函数内部的工作原理。因此，调用者告诉上一个函数（被调用方）调整其返回值以最小化损失。请参见图4.7，了解损失如何向后传播到参数和先前的函数。
- en: '![CH04_F07_Hagiwara](../Images/CH04_F07_Hagiwara.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F07_Hagiwara](../Images/CH04_F07_Hagiwara.png)'
- en: Figure 4.7 Backpropagation of loss
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 损失的反向传播
- en: The nested function calls repeat this process and plays the Telephone game until
    the message reaches the innermost function. By that time, because the message
    needs to pass through many layers, it becomes so weak and obscure (or so strong
    and skewed because of some misunderstanding) that the inner functions have a difficult
    time figuring out what they did wrong.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套的函数调用重复这个过程并玩转电话游戏，直到消息传递到最内层函数。到那个时候，因为消息需要经过许多层，它变得非常微弱和模糊（或者如果有误解则非常强大和扭曲），以至于内部函数很难弄清楚自己做错了什么。
- en: Technically, the deep learning literature calls this the *vanishing gradients
    problem*. A *gradient* is a mathematical term that corresponds to the message
    signal that each function receives from the next one that states how exactly they
    should improve their process (how to change their magic constants). The reverse
    Telephone game, where messages are passed backward from the final function (=
    loss function), is called backpropagation. I’m not going into the mathematical
    details of these terms, but it is useful to understand them at least conceptually.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上讲，深度学习文献将此称为*梯度消失问题*。*梯度*是一个数学术语，对应于每个函数从下一个函数接收到的信息信号，该信号指示它们应该如何改进其过程（如何更改其魔法常数）。反向电话游戏，其中消息从最终函数（=损失函数）向后传递，称为反向传播。我不会涉及这些术语的数学细节，但至少在概念上理解它们是有用的。
- en: Because of the vanishing gradients problem, simple RNNs are difficult to train
    and rarely used in practice nowadays.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于梯度消失问题，简单的循环神经网络（Simple RNNs）难以训练，在实践中现在很少使用。
- en: 4.2.2 Long short-term memory (LSTM)
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 长短期记忆（LSTM）
- en: The way the nested functions mentioned earlier process information about grammar
    seems too inefficient. After all, why doesn’t the outermost function (is_grammatical)
    tell the particular function in charge (e.g., process_main_subject) what went
    wrong directly, instead of playing the Telephone game? It can’t, because the message
    can change its shape entirely after each function call because of w2 and f().
    The outermost function cannot tell which function was responsible for which part
    of the message from only the final output.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的嵌套函数处理语法信息的方式似乎太低效了。毕竟，为什么外部函数（is_grammatical）不直接告诉负责的特定函数（例如，process_main_subject）出了什么问题，而不是玩电话游戏呢？它不能这样做，因为每次函数调用后消息都可以完全改变其形状，这是由于
    w2 和 f()。最外层函数无法仅从最终输出中告诉哪个函数负责消息的哪个部分。
- en: How could we address this inefficiency? Instead of passing the information through
    an activation function every time and changing its shape completely, how about
    adding and subtracting information relevant to the part of the sentence being
    processed at each step? For example, if process_main_subject() can directly add
    information about the subject to some type of “memory,” and the network can make
    sure the memory passes through the intermediate functions intact, is_grammatical()will
    have a much easier time telling the previous functions what to do to adjust its
    output.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解决这个低效性呢？与其每次通过激活函数传递信息并完全改变其形状，不如在每一步中添加和减去与正在处理的句子部分相关的信息？例如，如果 process_main_subject()
    可以直接向某种“记忆”中添加有关主语的信息，并且网络可以确保记忆通过中间函数完整地传递，is_grammatical() 就会更容易告诉前面的函数如何调整其输出。
- en: '*Long short-term memory* units (LSTMs) are a type of RNN cell that is proposed
    based on this insight. Instead of passing around states, LSTM cells share a “memory”
    that each cell can remove old information from and/or add new information to,
    something like an assembly line in a manufacturing factory. Specifically, LSTM
    RNNs use the following function for updating states:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*长短期记忆*单元（LSTMs）是基于这一观点提出的一种 RNN 单元。LSTM 单元不是传递状态，而是共享“记忆”，每个单元都可以从中删除旧信息并/或添加新信息，有点像制造工厂中的装配线。具体来说，LSTM
    RNN 使用以下函数来更新状态：'
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![CH04_F08_Hagiwara](../Images/CH04_F08_Hagiwara.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F08_Hagiwara](../Images/CH04_F08_Hagiwara.png)'
- en: Figure 4.8 LSTM update function
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 LSTM 更新函数
- en: 'Although this looks relatively complicated compared to its simple version,
    if you break it down to subcomponents, it’s not that difficult to understand what
    is going on here, as described next and shown in figure 4.8:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与其简单版本相比，这看起来相对复杂，但是如果你将其分解为子组件，就不难理解这里正在发生的事情，如下所述并在图 4.8 中显示：
- en: The LSTM states comprise two halves—the cell state (the “memory” part) and the
    hidden state (the “mental representation” part).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 状态包括两个部分——细胞状态（“记忆”部分）和隐藏状态（“心理表征”部分）。
- en: The function forget() returns a value between 0 and 1, so multiplying by this
    number means erasing old memory from cell_state. How much to erase is determined
    from hidden_state and word (input). Controlling the flow of information by multiplying
    by a value between 0 and 1 is called *gating*. LSTMs are the first RNN architecture
    that uses this gating mechanism.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数 forget() 返回一个介于 0 和 1 之间的值，因此乘以这个数字意味着从 cell_state 中擦除旧的记忆。要擦除多少由 hidden_state
    和 word（输入）决定。通过乘以介于 0 和 1 之间的值来控制信息流动称为*门控*。LSTM 是第一个使用这种门控机制的 RNN 架构。
- en: The function add()returns a new value added to the memory. The value again is
    determined from hidden_state and word.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数 add() 返回添加到记忆中的新值。该值再次是由 hidden_state 和 word 决定的。
- en: Finally, hidden_state is updated using a function, whose value is computed from
    the previous hidden state, the updated memory, and the input word.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，使用一个函数更新 hidden_state，该函数的值是从前一个隐藏状态、更新后的记忆和输入单词计算得出的。
- en: I abstracted the update function by hiding some mathematical details in the
    functions forget(), add(), and update_hidden(), which are not important for the
    discussion here. If you are interested in understanding LSTMs more deeply, I refer
    you to a wonderful blog post Chris Olah wrote on this topic ([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过隐藏一些数学细节在 forget()、add() 和 update_hidden() 函数中抽象了更新函数，这些细节对于这里的讨论不重要。如果你对深入了解
    LSTM 感兴趣，我建议你阅读克里斯·奥拉在此主题上撰写的精彩博文（[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)）。
- en: Because LSTMs have this cell state that stays constant across different timesteps
    unless explicitly modified, they are easier to train and relatively well behaved.
    Because you have a shared “memory” and functions are adding and removing information
    related to different parts of the input sentence, it is easier to pinpoint which
    function did what and what went wrong. The error signal from the outermost function
    can reach responsible functions more directly.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 LSTMs 有一个在不同时间步保持不变的单元状态，除非显式修改，它们更容易训练并且相对表现良好。因为你有一个共享的“记忆”，函数正在添加和删除与输入句子的不同部分相关的信息，所以更容易确定哪个函数做了什么以及出了什么问题。来自最外层函数的错误信号可以更直接地到达负责函数。
- en: A word on the terminology—LSTM refers to one particular type of architecture
    mentioned here, but people use “LSTMs” to mean RNNs with LSTM cells. Also, “RNN”
    is often used to mean “simple RNN,” introduced in section 4.1.3\. When you see
    “RNNs” in the literature, you need to be aware of which exact architectures they
    are using.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 术语说明：LSTM 是此处提到的一种特定类型的架构，但人们使用 “LSTMs” 来表示带有 LSTM 单元的 RNN。此外，“RNN” 常常用来指代“简单
    RNN”，在第 4.1.3 节中介绍。在文献中看到“RNNs”时，你需要注意它们使用的确切架构。
- en: 4.2.3 Gated recurrent units (GRUs)
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 门控循环单元（GRUs）
- en: 'Another RNN architecture, called *Gated Recurrent Units* (GRUs), uses the gating
    mechanism. The philosophy behind GRUs is similar to that of LSTMs, but GRUs use
    only one set of states instead of two halves. The update function for GRUs is
    shown next:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种 RNN 架构称为*门控循环单元*（GRUs），它使用门控机制。GRUs 的理念与 LSTMs 相似，但 GRUs 仅使用一组状态而不是两组。GRUs
    的更新函数如下所示：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Instead of erasing or updating the memory, GRUs use a switching mechanism. The
    cell first computes the new state from the old state and the input. It then computes
    switch, a value between 0 and 1\. The state is chosen between the new state and
    the old one based on the value of switch. If it’s 0, the old state passes through
    intact. If it’s 1, it’s overwritten by the new state. If it’s somewhere in between,
    the state will be a mix of two. See figure 4.9 for an illustration of the GRU
    update function.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: GRUs 不使用擦除或更新内存，而是使用切换机制。单元首先从旧状态和输入计算出新状态。然后计算切换值，一个介于 0 和 1 之间的值。根据切换值选择新状态和旧状态之间的状态。如果它是
    0，旧状态保持不变。如果它是 1，它将被新状态覆盖。如果它在两者之间，状态将是两者的混合。请参见图 4.9，了解 GRU 更新函数的示意图。
- en: '![CH04_F09_Hagiwara](../Images/CH04_F09_Hagiwara.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F09_Hagiwara](../Images/CH04_F09_Hagiwara.png)'
- en: Figure 4.9 GRU update function
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 GRU 更新函数
- en: Notice that the update function for GRUs is much simpler than that for the LSTMs.
    Indeed, it has fewer parameters (magic constants) that need to be trained compared
    to LSTMs. Because of this, GRUs are faster to train than LSTMs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与 LSTMs 相比，GRUs 的更新函数要简单得多。实际上，它的参数（魔术常数）比 LSTMs 需要训练的参数少。因此，GRUs 比 LSTMs
    更快地训练。
- en: Finally, although we introduced two different types of RNN architecture, LSTM
    and GRU, there’s no consensus in the community on which type of architecture is
    the best for all applications. You often need to treat them as a hyperparameter
    and experiment with different configurations. Fortunately, it is easy to experiment
    with different types of RNN cells as long as you are using modern deep learning
    frameworks such as PyTorch and TensorFlow.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管我们介绍了两种不同类型的 RNN 架构，即 LSTM 和 GRU，但在社区中并没有一致的共识，哪种类型的架构对于所有应用最好。你通常需要将它们视为超参数，并尝试不同的配置。幸运的是，只要你使用现代深度学习框架如
    PyTorch 和 TensorFlow，就很容易尝试不同类型的 RNN 单元。
- en: 4.3 Accuracy, precision, recall, and F-measure
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 准确率、精确率、召回率和 F-度量
- en: In section 2.7, I briefly talked about some metrics that we use for evaluating
    the performance of a classification task. Before we move on to actually building
    a sentence classifier, I’d like to further discuss the evaluation metrics we are
    going to use—what they mean and what they actually measure.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2.7 节，我简要地讨论了一些我们用于评估分类任务性能的指标。在我们继续实际构建一个句子分类器之前，我想进一步讨论我们将要使用的评估指标——它们的含义以及它们实际上衡量的内容。
- en: 4.3.1 Accuracy
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 准确率
- en: Accuracy is probably the simplest of all the evaluation metrics that we talk
    about here. In a classification setting, accuracy is the fraction of instances
    that your model got right. For example, if there are 10 emails and your spam-filtering
    model got 8 of them correct, the accuracy of your prediction is 0.8, or 80% (see
    figure 4.10).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率可能是我们所讨论的所有评估指标中最简单的。在分类设置中，准确率是你的模型预测正确的实例的比例。例如，如果有 10 封电子邮件，而你的垃圾邮件过滤模型正确地识别了其中的
    8 封，那么你的预测准确率就是 0.8，或者 80%（见图 4.10）。
- en: '![CH04_F10_Hagiwara](../Images/CH04_F10_Hagiwara.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F10_Hagiwara](../Images/CH04_F10_Hagiwara.png)'
- en: Figure 4.10 Calculating accuracy
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 计算准确率
- en: Though simple, accuracy is not without its limitations. Specifically, accuracy
    can be misleading when the test set is imbalanced. An *imbalanced* dataset contains
    multiple class labels that greatly differ in their numbers. For example, if a
    spam-filtering dataset is imbalanced, it may contain 90% nonspam emails and 10%
    spams. In such a case, even a stupid classifier that labels everything as nonspam
    would be able to achieve an accuracy of 90%. As an example, if a “stupid” classifier
    classifies everything as “nonspam” in figure 4.10, it would still achieve an accuracy
    of 70% (7 out of 10 instances). If you look at this number in isolation, you might
    be fooled into thinking the performance of the classifier is actually great. When
    you are using accuracy as a metric, it is always a good idea to compare with the
    hypothetical, stupid classifier (*majority vote*) as a baseline.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然简单，但准确率并不是没有局限性。具体来说，在测试集不平衡时，准确率可能会误导。一个*不平衡*的数据集包含多个类别标签，它们的数量差异很大。例如，如果一个垃圾邮件过滤数据集不平衡，可能包含
    90% 的非垃圾邮件和 10% 的垃圾邮件。在这种情况下，即使一个愚蠢的分类器把一切都标记为非垃圾邮件，也能够达到 90% 的准确率。例如，如果一个“愚蠢”的分类器在图
    4.10 中将所有内容都分类为“非垃圾邮件”，它仍然会达到 70% 的准确率（10 个实例中的 7 个）。如果你孤立地看这个数字，你可能会被误导以为分类器的性能实际上很好。当你使用准确率作为指标时，将其与假想的、愚蠢的分类器（*多数投票*）作为基准进行比较总是一个好主意。
- en: 4.3.2 Precision and recall
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 精确率和召回率
- en: The rest of the metrics—precision, recall, and F-measure—are used in a binary
    classification setting. The goal of a binary classification task is to identify
    one class (called a *positive class*) from the other (called a *negative class*).
    In the spam-filtering setting, the positive class is spam, whereas the negative
    class is nonspam.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的指标——精确率、召回率和 F-度量——是在二元分类设置中使用的。二元分类任务的目标是从另一个类别（称为*负类*）中识别出一个类别（称为*正类*）。在垃圾邮件过滤设置中，正类是垃圾邮件，而负类是非垃圾邮件。
- en: 'The Venn diagram in figure 4.11 contains four subregions: true positives, false
    positives, false negatives, and true negatives. True positives (TP) are instances
    that are predicted as positive (= spam) and are indeed in the positive class.
    False positives (FP) are instances that are predicted as positive (= spam) but
    are actually not in the positive class. These are noises in the prediction, that
    is, innocent nonspam emails that are mistakenly caught by the spam filter and
    end up in the spam folder of your email client.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 中的维恩图包含四个子区域：真正例、假正例、假负例和真负例。真正例（TP）是被预测为正类（= 垃圾邮件）并且确实属于正类的实例。假正例（FP）是被预测为正类（=
    垃圾邮件）但实际上不属于正类的实例。这些是预测中的噪音，也就是被误认为垃圾邮件并最终出现在你的电子邮件客户端的垃圾邮件文件夹中的无辜非垃圾邮件。
- en: On the other hand, false negatives (FN) are instances that are predicted as
    negative but are actually in the positive class. These are spam emails that slip
    through the spam filter and end up in your inbox. Finally, true negatives (TN)
    are instances that are predicted as negative and are indeed in the negative class
    (nonspam emails in your inbox).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，假阴性（FN）是被预测为负类但实际上属于正类的实例。这些是通过垃圾邮件过滤器漏过的垃圾邮件，最终出现在你的收件箱中。最后，真阴性（TN）是被预测为负类并且确实属于负类的实例（即出现在你的收件箱中的非垃圾邮件）。
- en: Precision is the fraction of instances that the model classifies as positive
    that are indeed correct. For example, if your spam filter identifies three emails
    as spam, and two of them are indeed spam, the precision will be 2/3, or about
    66%.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率是模型将正确分类为正例的实例的比例。例如，如果你的垃圾邮件过滤器将三封邮件标记为垃圾邮件，并且其中有两封确实是垃圾邮件，则精确率将为2/3，约为66%。
- en: Recall is somewhat opposite of precision. It is the fraction of positive instances
    in your dataset that are identified as positive by your model. Again, using spam
    filtering as an example, if your dataset contains three spam emails and your model
    identifies two of them as spam successfully, the recall will be 2/3, or about
    66%.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率与精确率有些相反。它是你的模型在数据集中被正确识别为正例的正例占比。再以垃圾邮件过滤为例，如果你的数据集中有三封垃圾邮件，而你的模型成功识别了其中两封邮件为垃圾邮件，则召回率将为2/3，约为66%。
- en: Figure 4.11 shows the relationship between predicted and true labels as well
    as recall and precision.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 显示了预测标签和真实标签之间以及召回率和精确率之间的关系。
- en: '![CH04_F11_Hagiwara](../Images/CH04_F11_Hagiwara.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F11_Hagiwara](../Images/CH04_F11_Hagiwara.png)'
- en: Figure 4.11 Precision and recall
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 精确率和召回率
- en: 4.3.3 F-measure
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 F-测量
- en: You may have noticed a tradeoff between precision and recall. Imagine there’s
    a spam filter that is very, very careful in classifying emails. It outputs only
    one out of several thousand emails as spam, but when it does, it is always correct.
    This is not a difficult task, because some spam emails are pretty obvious—if they
    contain a word “v1@gra” in the text and it’s sent from someone in the spam blacklist,
    it should be pretty safe to mark it as a spam. What would the precision of this
    spam filter be? 100%. Similarly, there’s another spam filter that is very, very
    careless in classifying emails. It classifies every single email as spam, including
    the ones from your colleagues and friends. Its recall? 100%. Would any of these
    two spam filters be useful? Hardly!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到了精确率和召回率之间的权衡。想象一下有一个非常谨慎的垃圾邮件过滤器。它只有在几千封邮件中输出一封邮件为垃圾邮件，但当它输出时，它总是正确的。这不是一个困难的任务，因为一些垃圾邮件非常明显
    - 如果它们的文本中包含“v1@gra”这个词，并且是从垃圾邮件黑名单中的人发送的，将其标记为垃圾邮件应该是相当安全的。这个垃圾邮件过滤器的精确率是多少？100%。同样，还有另一个非常粗心的垃圾邮件过滤器。它将每封电子邮件都分类为垃圾邮件，包括来自同事和朋友的电子邮件。它的召回率是多少？100%。这两个垃圾邮件过滤器中的任何一个有用吗？几乎没有！
- en: As you’ve seen, improving precision or recall alone while ignoring the other
    is not a good practice, because of the tradeoff between them. It’s like you were
    looking only at your body weight when you are on a diet. You lost 10 pounds? Great!
    But what if you are seven feet tall? Not so much. You need to take into account
    both your height and weight—how much is too much depends on the other variable.
    That’s why there are measures like BMI (body mass index) that take both measures
    into account. Similarly, researchers came up with this metric called *F-measure*,
    which is an average (or, more precisely speaking, a harmonic mean) of precision
    and recall. Most often, a special case called *F1*-measure is used, which is the
    equally weighted version of F-measure. In a classification setting, it is a good
    practice to measure and try to maximize the F-measure.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，只关注精确率或召回率而忽视另一个是不好的做法，因为它们之间存在权衡。这就好比你在节食时只关注体重。你减了10磅？太棒了！但是如果你身高是7英尺呢？并不是很好。你需要同时考虑身高和体重-太多是多少取决于另一个变量。这就是为什么有像
    BMI（身体质量指数）这样的衡量标准，它同时考虑了这两个指标。同样，研究人员提出了一种叫做F-测量的度量标准，它是精确率和召回率的平均值（更准确地说是调和平均值）。通常使用的是一个叫做F1-测量的特殊案例，它是F-测量的等权版本。在分类设置中，衡量并尝试最大化F-测量是一种很好的做法。
- en: 4.4 Building AllenNLP training pipelines
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 构建 AllenNLP 训练流程
- en: In this section, we are going to revisit the sentiment analyzer we built in
    chapter 2 and discuss how to build its training pipeline in more detail. Although
    I already showed the important steps for building an NLP application using AllenNLP,
    in this section we will dive deep into some important concepts and abstractions.
    Understanding these concepts is important not just in using AllenNLP but also
    in designing NLP applications in general, because NLP applications are usually
    built using these abstractions in some way or the other.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重新审视第2章中构建的情感分析器，并详细讨论如何更详细地构建其训练流程。尽管我已经展示了使用AllenNLP
- en: 'To run the code in this section, you need to import the necessary classes and
    modules, as shown in the following code snippet (the code examples in this section
    can also be accessed via Google Colab, [http://www.realworldnlpbook.com/ch2.html#sst-nb](http://www.realworldnlpbook.com/ch2.html#sst-nb)):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本节中的代码，您需要导入必要的类和模块，如下面的代码片段所示（本节中的代码示例也可以通过 Google Colab 访问，[http://www.realworldnlpbook.com/ch2.html#sst-nb](http://www.realworldnlpbook.com/ch2.html#sst-nb)）。
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 4.4.1 Instances and fields
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 实例和字段
- en: As mentioned in section 2.2.1, an instance is the atomic unit for which a prediction
    is made by a machine learning algorithm. A dataset is a collection of instances
    of the same form. The first step in most NLP applications is to read in or receive
    some data (e.g., from a file or via network requests) and convert them to instances
    so that the NLP/ML algorithm can consume them.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 2.2.1 节所述，实例是机器学习算法进行预测的原子单位。数据集是同一形式实例的集合。大多数 NLP 应用的第一步是读取或接收一些数据（例如从文件或通过网络请求）并将其转换为实例，以便
    NLP/ML 算法可以使用它们。
- en: AllenNLP supports an abstraction called DatasetReader whose job is to read in
    some input (raw strings, CSV files, JSON data structures from network requests,
    and so on) and convert it to instances. AllenNLP already provides a wide range
    of dataset readers for major formats used in NLP, such as the CoNLL format (used
    in popular shared tasks for language analysis) and the Penn Treebank (a popular
    dataset for syntactic parsing). To read the Standard Sentiment Treebank, you can
    use the built-in StanfordSentimentTreeBankDatasetReader, which we used earlier
    in chapter 2\. You can also write your own dataset reader just by overriding some
    core methods from DatasetReader.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP 支持一个称为 DatasetReader 的抽象，它的工作是读取一些输入（原始字符串、CSV 文件、来自网络请求的 JSON 数据结构等）并将其转换为实例。AllenNLP
    已经为 NLP 中使用的主要格式提供了广泛的数据集读取器，例如 CoNLL 格式（在语言分析的流行共享任务中使用）和 Penn Treebank（一种流行的用于句法分析的数据集）。要读取
    Standard Sentiment Treebank，可以使用内置的 StanfordSentimentTreeBankDatasetReader，我们在第
    2 章中已经使用过了。您还可以通过覆盖 DatasetReader 的一些核心方法来编写自己的数据集阅读器。
- en: 'The AllenNLP class Instance represents a single instance. An instance can have
    one or more fields, which hold some type of data. For example, an instance for
    the sentiment analysis task has two fields—the text body and the label—which can
    be created by passing a dictionary of fields to its constructor as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP 类 Instance 表示一个单独的实例。一个实例可以有一个或多个字段，这些字段保存某种类型的数据。例如，情感分析任务的实例有两个字段——文本内容和标签——可以通过将字段字典传递给其构造函数来创建，如下所示：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here we assumed that you already created tokens, which is a list of tokens,
    and sentiment, a string label corresponding to the sentiment class, from reading
    the input file. AllenNLP supports other types of fields, depending on the task.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设您已经创建了 tokens（一个标记列表）和 sentiment（一个与情感类别对应的字符串标签），并从读取输入文件中获取了它们。根据任务，AllenNLP
    还支持其他类型的字段。
- en: 'The read()method of DatasetReader returns an iterator over instances, which
    enables you to enumerate the generated instances and visually check them, as shown
    in the following snippet:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: DatasetReader 的 read() 方法返回一个实例迭代器，使您能够枚举生成的实例并对其进行可视化检查，如下面的代码片段所示：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In many cases, you access your dataset readers through data loaders. A data
    loader is an AllenNLP abstraction (which is really a thin wrapper around PyTorch’s
    data loaders) that handles the data and iterates over batched instances. You can
    specify how instances are sorted, grouped into batches, and fed to the training
    algorithm by supplying a batch sampler. Here, we are using a BucketBatchSampler,
    which does this by sorting instances by their length and grouping instances with
    similar lengths into a single batch, as shown next:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，您可以通过数据加载器访问数据集阅读器。数据加载器是 AllenNLP 的一个抽象（实际上是 PyTorch 数据加载器的一个薄包装），它处理数据并迭代批量实例。您可以通过提供批量样本器来指定如何对实例进行排序、分组为批次并提供给训练算法。在这里，我们使用了一个
    BucketBatchSampler，它通过根据实例的长度对其进行排序，并将长度相似的实例分组到一个批次中，如下所示：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 4.4.2 Vocabulary and token indexers
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 词汇表和标记索引器
- en: The second step in many NLP applications is to build the vocabulary. In computer
    science, vocabulary is a theoretical concept that represents the set of *all*
    possible words in a language. In NLP, though, it often means just the set of all
    unique tokens that appeared in a dataset. It is simply impossible to know all
    the possible words in a language, nor is it necessary for an NLP application.
    What is stored in a vocabulary is called a *vocabulary item* (or just an *item*).
    A vocabulary item is usually a word, although depending on the task at hand, it
    can be any form of linguistic units, including characters, character n-grams,
    and labels for linguistic annotation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 许多NLP应用程序的第二个步骤是构建词汇表。在计算机科学中，词汇是一个表示语言中*所有*可能单词的理论概念。在NLP中，它通常只是指数据集中出现的所有唯一标记的集合。了解一种语言中所有可能的单词是不可能的，也不是NLP应用程序所必需的。词汇表中存储的内容称为*词汇项目*（或仅称为*项目*）。词汇项目通常是一个词，尽管根据手头的任务，它可以是任何形式的语言单位，包括字符、字符n-gram和用于语言注释的标签。
- en: AllenNLP provides a class called Vocabulary. It not only takes care of storing
    vocabulary items that appeared in a dataset, but it also holds mappings between
    vocabulary items and their IDs. As mentioned earlier, neural networks and machine
    learning models in general can deal only with numbers, and there needs to be a
    way to map discrete items such as words to some numerical representations such
    as word IDs. The vocabulary is also used to map the results of an NLP model back
    to the original words and labels so that humans can actually read them.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP提供了一个名为Vocabulary的类。它不仅负责存储数据集中出现的词汇项目，还保存了词汇项目和它们的ID之间的映射关系。如前所述，神经网络和一般的机器学习模型只能处理数字，而需要一种将诸如单词之类的离散项目映射到一些数字表示（如单词ID）的方式。词汇还用于将NLP模型的结果映射回原始单词和标签，以便人类实际阅读它们。
- en: 'You can create a Vocabulary object from instances as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按如下方式从实例创建一个Vocabulary对象：
- en: '[PRE15]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'A couple of things to note here: first, because we are dealing with iterators
    (returned by the data loaders’ iter_instances()method), we need to use the chain
    method from itertools to enumerate all the instances in both datasets.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意几点：首先，因为我们正在处理迭代器（由数据加载器的iter_instances()方法返回），所以我们需要使用itertools的chain方法来枚举两个数据集中的所有实例。
- en: Second, AllenNLP’s Vocabulary class supports *namespaces*, which are a system
    to separate different sets of items so that they don’t get mixed up. Here’s why
    they are useful—say you are building a machine translation system, and you just
    read a dataset that contains English and French translations. Without namespaces,
    you’d have just one set that contains all words in English and French. This is
    usually not a big issue because English words (“hi,” “thank you,” “language”)
    and French words (“bonjour,” “merci, “langue”) look quite different in most cases.
    However, a number of words look exactly the same in both languages. For example,
    “chat” means “talk” in English and “cat” in French, but it’s hard to imagine anybody
    wanting to mix those two words and assign the same ID (and embeddings). To avoid
    this conflict, Vocabulary implements namespaces and assigns separate sets of items
    of different types.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，AllenNLP的Vocabulary类支持*命名空间*，这是一种将不同的项目集分开的系统，以防它们混淆。这是为什么它们很有用——假设你正在构建一个机器翻译系统，并且刚刚读取了一个包含英语和法语翻译的数据集。如果没有命名空间，你将只有一个包含所有英语和法语单词的集合。在大多数情况下，这通常不是一个大问题，因为英语单词（“hi,”
    “thank you,” “language”）和法语单词（“bonjour,” “merci,” “langue”）在大多数情况下看起来非常不同。然而，一些单词在两种语言中看起来完全相同。例如，“chat”在英语中意思是“talk”，在法语中是“cat”，但很难想象有人想要混淆这两个词并分配相同的ID（和嵌入）。为了避免这种冲突，Vocabulary实现了命名空间并为不同类型的项目分配了单独的集合。
- en: 'You may have noticed the form_instances() function call has a min_count argument.
    For each namespace, this specifies the minimum number of occurrences in the dataset
    that is necessary for an item to be included in the vocabulary. All the items
    that appear less frequently than this threshold are treated as “unknown” items.
    Here’s why this is a good idea: in a typical language, a very small number of
    words appear a lot (in English: “the,” “a,” “of”) and a very large number of words
    appear very infrequently. This usually exhibits a long tail distribution of word
    frequencies. But it is not likely that these super infrequent words add anything
    useful to the model, and precisely because they appear infrequently, it is difficult
    to learn any useful patterns from them anyway. Also, because there are so many
    of them, they inflate the size of the vocabulary and the number of model parameters.
    In such a case, a common practice in NLP is to cut this long tail and collapse
    all the infrequent words to a single entity <UNK> (for “unknown” words).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到`form_instances()`函数调用有一个`min_count`参数。对于每个命名空间，它指定了数据集中必须出现的最小次数，以便将项目包含在词汇表中。所有出现频率低于此阈值的项目都被视为“未知”项目。这是一个好主意的原因是：在典型的语言中，很少有一些词汇会频繁出现（英语中的“the”，“a”，“of”），而有很多词汇出现的频率很低。这通常表现为词频的长尾分布。但这些频率极低的词汇不太可能对模型有任何有用的信息，并且正因为它们出现频率较低，从中学习有用的模式也很困难。此外，由于这些词汇有很多，它们会增加词汇表的大小和模型参数的数量。在这种情况下，自然语言处理中常见的做法是截去这长尾部分，并将所有出现频率较低的词汇合并为一个单一的实体<UNK>（表示“未知”词汇）。
- en: Finally, a *token indexer* is an AllenNLP abstraction that takes in a token
    and returns its index, or a list of indices that represent the token. In most
    cases, there’s a one-to-one mapping between unique tokens and their indices, but
    depending on your model, you may need more advanced ways to index the tokens (such
    as using character n-grams).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*令牌索引器*是AllenNLP的一个抽象概念，它接收一个令牌并返回其索引，或者返回表示令牌的索引列表。在大多数情况下，独特令牌和其索引之间存在一对一的映射，但根据您的模型，您可能需要更高级的方式来对令牌进行索引（例如使用字符n-gram）。
- en: 'After you create a vocabulary, you can tell the data loaders to index the tokens
    with the specified vocabulary, as shown in the next code snippet. This means that
    the tokens that the data loaders read from the datasets are converted to integer
    IDs according to the vocabulary’s mappings:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 创建词汇表后，你可以告诉数据加载器使用指定的词汇表对令牌进行索引，如下代码片段所示。这意味着数据加载器从数据集中读取的令牌会根据词汇表的映射转换为整数ID：
- en: '[PRE16]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 4.4.3 Token embedders and RNNs
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 令牌嵌入和RNN
- en: 'After you index words using a vocabulary and token indexers, you need to convert
    them to embeddings. An AllenNLP abstraction called TokenEmbedder takes word indices
    as an input and produces word embedding vectors as an output. You can embed words
    using continuous vectors in many ways, but if all you want is to map unique tokens
    to embedding vectors one-to-one, you can use the Embedding class as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用词汇表和令牌索引器索引单词后，需要将它们转换为嵌入。一个名为TokenEmbedder的AllenNLP抽象来接收单词索引作为输入并将其转换为单词嵌入向量作为输出。你可以使用多种方式嵌入连续向量，但如果你只想将唯一的令牌映射到嵌入向量时，可以使用Embedding类，如下所示：
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This will create an Embedding instance that takes word IDs and converts them
    to fixed-length vectors in a one-to-one fashion. The number of unique words this
    instance can support is given by num_embeddings, which is equal to the size of
    the tokens vocabulary namespace. The dimensionality of embeddings (i.e., the length
    of embedded vectors) is given by embedding_dim.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个Embedding实例，它接收单词ID并以一对一的方式将其转换为定长矢量。该实例支持的唯一单词数量由num_embeddings给出，它等于令牌词汇的大小。嵌入的维度（即嵌入矢量的长度）由embedding_dim给出。
- en: 'Next, let’s define our RNN and convert a variable-length input (a list of embedded
    words) to a fixed-length vector representation of the input. As we discussed in
    section 4.1, you can think of an RNN as a neural network structure that consumes
    a sequence of things (words) and returns a fixed-length vector. AllenNLP abstracts
    such models into the Seq2VecEncoder class, and you can create an LSTM RNN by using
    PytorchSeq2VecWrapper as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义我们的RNN，并将变长输入（嵌入词的列表）转换为输入的定长矢量表示。正如我们在第4.1节中讨论的那样，你可以将RNN看作是一个神经网络结构，它消耗一个序列的事物（词汇）并返回一个定长的矢量。AllenNLP将这样的模型抽象化为Seq2VecEncoder类，你可以通过使用PytorchSeq2VecWrapper创建一个LSTM
    RNN，如下所示：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: A lot is happening here, but essentially this wraps PyTorch’s LSTM implementation
    (torch.nn.LSTM) and makes it pluggable to the rest of the AllenNLP pipeline. The
    first argument to torch.nn.LSTM() is the dimensionality of the input vector, and
    the second one is that of LSTM’s internal state. The last one, batch_first, specifies
    the structure of the input/output tensors for batching, but you usually don’t
    have to worry about its details as long as you are using AllenNLP.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了很多事情，但本质上是将PyTorch的LSTM实现（torch.nn.LSTM）包装起来，使其可以插入到AllenNLP流程中。torch.nn.LSTM()的第一个参数是输入向量的维度，第二个参数是LSTM的内部状态的维度。最后一个参数batch_first指定了用于批处理的输入/输出张量的结构，但只要你使用AllenNLP，你通常不需要担心其细节。
- en: NOTE In AllenNLP, everything is batch first, meaning that the first dimension
    of any tensor is always equal to the number of instances in a batch.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在AllenNLP中，一切都是以批为单位，意味着任何张量的第一个维度始终等于批中实例的数量。
- en: 4.4.4 Building your own model
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.4 构建你自己的模型
- en: Now that we defined all the subcomponents, we are ready to build the model that
    executes the prediction. Thanks to AllenNLP’s well-designed abstractions, you
    can easily build your model by inheriting AllenNLP’s Model class and overriding
    the forward() method. You don’t usually need to be aware of details such as the
    shapes and dimensions of tensors. The following listing defines the LSTM RNN used
    for classifying sentences.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了所有的子组件，我们准备构建执行预测的模型了。由于AllenNLP的良好抽象设计，你可以通过继承AllenNLP的Model类并覆盖forward()方法来轻松构建你的模型。通常情况下，你不需要关注张量的形状和维度等细节。以下清单定义了用于分类句子的LSTM
    RNN。
- en: Listing 4.1 LSTM sentence classifier
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 清单4.1 LSTM句子分类器
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ AllenNLP models inherit Model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ AllenNLP模型继承自Model。
- en: ❷ Creates a linear layer to convert the RNN output to a vector of another length
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建线性层将RNN输出转换为另一个长度的向量
- en: ❸ F1Measure() requires the label ID for the positive class. '4' means “very
    positive.”
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ F1Measure()需要正类的标签ID。'4'表示“非常积极”。
- en: ❹ Cross-entropy loss is used for classification tasks. CrossEntropyLoss directly
    takes logits (no softmax needed).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 用于分类任务的交叉熵损失。CrossEntropyLoss直接接受logits（不需要softmax）。
- en: ❺ Instances are destructed to individual fields and passed to forward().
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 实例被解构为各个字段并传递给forward()。
- en: ❻ Output of forward() is a dict, which contains a “loss” key.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ forward()的输出是一个字典，其中包含一个“loss”键。
- en: ❼ Returns accuracy, precision, recall, and F1-measure as the metrics
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 返回准确率、精确率、召回率和F1分数作为度量标准
- en: Every AllenNLP Model inherits from PyTorch’s Module class, meaning you can use
    PyTorch’s low-level operations if necessary. This gives you a lot of flexibility
    in defining your model while leveraging AllenNLP’s high-level abstractions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每个AllenNLP模型都继承自PyTorch的Module类，这意味着如果需要，你可以使用PyTorch的低级操作。这为你在定义模型时提供了很大的灵活性，同时利用了AllenNLP的高级抽象。
- en: 4.4.5 Putting it all together
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.5 把所有东西都放在一起
- en: Finally, we finish this section by implementing the entire pipeline to train
    the sentiment analyzer, as shown next.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过实现整个流程来训练情感分析器，如下所示。
- en: Listing 4.2 Training pipeline for the sentiment analyzer
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 清单4.2 情感分析器的训练流程
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Defines how to construct the data loaders
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义如何构造数据加载器
- en: ❷ Initializes the model
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化模型
- en: ❸ Defines the optimizer
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义优化器
- en: ❹ Initializes the trainer
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 初始化训练器
- en: The training pipeline completes when the Trainer instance is created and invoked
    with train(). You pass all the ingredients that you need for training—the model,
    optimizer, data loaders, datasets, and a bunch of hyperparameters.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建Trainer实例并调用train()时，训练流程完成。你需要传递所有用于训练的要素，包括模型、优化器、数据加载器、数据集和一堆超参数。
- en: An optimizer implements an algorithm for adjusting the parameters of the model
    to minimize the loss. Here, we are using one type of optimizer called *Adam*,
    which is a good “default” optimizer to use as your first option. However, as I
    mentioned in chapter 2, you often need to experiment with many different optimizers
    that work best for your model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器实现了一个调整模型参数以最小化损失的算法。在这里，我们使用一种称为*Adam*的优化器，这是你作为首选项的一个很好的“默认”优化器。然而，正如我在第2章中提到的，你经常需要尝试许多不同的优化器，找出对你的模型效果最好的那一个。
- en: 4.5 Configuring AllenNLP training pipelines
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 配置AllenNLP训练流程
- en: You may have noticed that very little of listing 4.2 is actually specific to
    the sentence-classification problem. Indeed, loading datasets, initializing a
    model, and plugging an iterator and an optimizer into the trainer are all common
    steps across almost every NLP training pipeline. What if you want to reuse the
    same training pipeline for many related tasks without writing the training script
    from scratch? Also, what if you want to experiment with different sets of configurations
    (e.g., different hyperparameters, neural network architectures) and save the exact
    configurations you tried?
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，列表 4.2 中很少有实际针对句子分类问题的内容。事实上，加载数据集、初始化模型，并将迭代器和优化器插入训练器是几乎每个 NLP 训练管道中的常见步骤。如果您想要为许多相关任务重复使用相同的训练管道而不必从头编写训练脚本呢？另外，如果您想要尝试不同配置集（例如，不同的超参数、神经网络架构）并保存您尝试过的确切配置呢？
- en: For those problems, AllenNLP provides a convenient framework where you can write
    configuration files in the JSON format. The idea is that you write the specifics
    of your training pipeline—for example, which dataset reader to use, which models
    and their subcomponents to use, and what hyper-parameters to use for training—in
    a JSON-formatted file (more precisely, AllenNLP uses a format called *Jsonnet*,
    which is a superset of JSON). Instead of rewriting your model file or the training
    script, you feed the configuration file to the AllenNLP executable, and the framework
    takes care of running the training pipeline. If you want to try a different configuration
    for your model, you simply change the configuration file (or make a new one) and
    run the pipeline again, without changing the Python code. This is a great practice
    for making your experiment manageable and reproducible. You need to manage only
    the configuration files and their results—the same configuration always yields
    the same result.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些问题，AllenNLP 提供了一个便捷的框架，您可以在 JSON 格式的配置文件中编写配置。其思想是您在 JSON 格式文件中编写您的训练管道的具体内容——例如要使用哪个数据集读取器、要使用哪些模型及其子组件，以及用于训练的哪些超参数。然后，您将配置文件提供给
    AllenNLP 可执行文件，框架会负责运行训练管道。如果您想尝试模型的不同配置，只需更改配置文件（或创建一个新文件），然后再次运行管道，而无需更改 Python
    代码。这是一种管理实验并使其可重现的良好实践。您只需管理配置文件及其结果——相同的配置始终产生相同的结果。
- en: 'A typical AllenNLP configuration file consists of three main parts—the dataset,
    your model, and the training pipeline. The first part, shown next, specifies which
    dataset files to use and how:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的 AllenNLP 配置文件由三个主要部分组成——数据集、您的模型和训练管道。下面是第一部分，指定了要使用的数据集文件以及如何使用：
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Three keys are in this part: dataset_reader, train_data_path, and validation_data_path.
    The first key, dataset_reader, specifies which DatasetReader to use to read the
    files. Dataset readers, models, and predictors, as well as many other types of
    modules in AllenNLP, can be registered using the decorator syntax and be referred
    to from configuration files. For example, if you peek at the following code where
    StanfordSentimentTreeBankDatasetReader is defined'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分有三个键：dataset_reader、train_data_path 和 validation_data_path。第一个键 dataset_reader
    指定要使用哪个 DatasetReader 来读取文件。在 AllenNLP 中，数据集读取器、模型、预测器以及许多其他类型的模块都可以使用装饰器语法注册，并且可以在配置文件中引用。例如，如果您查看下面定义了
    StanfordSentimentTreeBankDatasetReader 的代码
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'you notice that it is decorated by @DatasetReader.register("sst_tokens"). This
    registers StanfordSentimentTreeBankDatasetReader under the name sst_tokens, which
    allows you to refer it by "type": "sst_tokens" from the configuration files.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '你注意到它被 @DatasetReader.register("sst_tokens") 装饰。这将 StanfordSentimentTreeBankDatasetReader
    注册为 sst_tokens，使您可以通过配置文件中的 "type": "sst_tokens" 来引用它。'
- en: 'In the second part of the configuration file, you specify the main model to
    be trained as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置文件的第二部分，您可以如下指定要训练的主要模型：
- en: '[PRE23]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As mentioned before, models in AllenNLP can be registered using the decorator
    syntax and be referred from the configuration files via the type key. For example,
    the LstmClassifier class referred here is defined as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，AllenNLP 中的模型可以使用装饰器语法注册，并且可以通过 type 键从配置文件中引用。例如，这里引用的 LstmClassifier
    类定义如下：
- en: '[PRE24]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Other keys in the model definition JSON dict correspond to the names of the
    parameters of the model constructor. In the previous definition, because LstmClassifier’s
    constructor takes two parameters, word_embeddings and encoder (in addition to
    vocab, which is passed by default and can be omitted, and positive_label, for
    which we are going to use the default value), the model definition has two corresponding
    keys, the values of which are also model definitions and follow the same convention.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 模型定义 JSON 字典中的其他键对应于模型构造函数的参数名称。在前面的定义中，因为 LstmClassifier 的构造函数接受了两个参数，word_embeddings
    和 encoder（除了 vocab，它是默认传递的并且可以省略，以及 positive_label，我们将使用默认值），所以模型定义有两个相应的键，它们的值也是模型定义，并且遵循相同的约定。
- en: 'In the final part of the configuration file, the data loader and the trainer
    are specified. The convention here is similar to the model definition—you specify
    the type of the class along with other parameters passed to the constructor as
    follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置文件的最后部分，指定了数据加载器和训练器。这里的约定与模型定义类似——你指定类的类型以及传递给构造函数的其他参数，如下所示：
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can see the full JSON configuration file in the code repository ([http://realworldnlpbook.com/ch4.html#sst-json](http://realworldnlpbook.com/ch4.html#sst-json)).
    Once you define the JSON configuration file, you can simply feed it to the allennlp
    command as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在代码仓库中查看完整的 JSON 配置文件（[http://realworldnlpbook.com/ch4.html#sst-json](http://realworldnlpbook.com/ch4.html#sst-json)）。一旦你定义了
    JSON 配置文件，你就可以简单地将其提供给 allennlp 命令，如下所示：
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The —serialization-dir specifies where the trained model (along with additional
    information such as serialized vocabulary data) is going to be stored. You also
    need to specify the module path to LstmClassifier using —include-package so that
    the configuration file can find the registered class.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: --serialization-dir 指定了训练模型（以及其他一些信息，如序列化的词汇数据）将要存储的位置。你还需要使用 --include-package
    指定到 LstmClassifier 的模块路径，以便配置文件能够找到注册的类。
- en: 'As we saw in chapter 2, when the training is finished, you can launch a simple
    web-based demo interface using the following command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第二章中所看到的，当训练完成时，你可以使用以下命令启动一个简单的基于 web 的演示界面：
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '4.6 Case study: Language detection'
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 案例研究：语言检测
- en: In this final section of the chapter, we are going to discuss another scenario—language
    detection—which can also be formulated as a sentence-classification task. A language-detection
    system, given a piece of text, detects the language the text is written in. It
    has a wide range of uses in other NLP applications. For example, a web search
    engine may want to detect the language a web page is written in before processing
    and indexing it. Google Translate also switches the source language automatically
    based on what is typed in the input textbox.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一节中，我们将讨论另一个场景——语言检测，它也可以被归纳为一个句子分类任务。语言检测系统，给定一段文本，检测文本所写的语言。它在其他自然语言处理应用中有着广泛的用途。例如，一个网络搜索引擎可能会在处理和索引网页之前检测网页所写的语言。Google
    翻译还会根据输入文本框中键入的内容自动切换源语言。
- en: Let’s see what this actually looks like. Can you tell the language of each of
    the following lines? These sentences are all taken from the Tatoeba project ([https://tatoeba.org/](https://tatoeba.org/)).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这实际上是什么样子。你能告诉下面每一句话是哪种语言吗？这些句子都来自 Tatoeba 项目（[https://tatoeba.org/](https://tatoeba.org/)）。
- en: Contamos con tu ayuda.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要你的帮助。
- en: Bitte überleg es dir.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑一下。
- en: Parti için planları tartıştılar.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 他们讨论了离开的计划。
- en: Je ne sais pas si je peux le faire.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道我能不能做到。
- en: Você estava em casa ontem, não estava?
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 昨天你在家，对吗？
- en: Ĝi estas rapida kaj efika komunikilo.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一种快速而有效的通讯工具。
- en: Ha parlato per un'ora.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 他讲了一个小时。
- en: Szeretnék elmenni és meginni egy italt.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我想去喝一杯。
- en: Ttwaliɣ nezmer ad nili d imeddukal.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Ttwaliɣ nezmer ad nili d imeddukal.
- en: 'The answer is: Spanish, German, Turkish, French, Portuguese, Esperanto, Italian,
    Hungarian, and Berber. I chose them from the top 10 most popular languages on
    Tatoeba that are written in the roman alphabet. You may not be familiar with some
    of the languages listed here. For those of you who are not, Esperanto is a constructed
    auxiliary language invented in the late 19th century. Berber is actually a group
    of related languages spoken in some parts of North Africa that are cousins of
    Semitic languages such as Arabic.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是：西班牙语、德语、土耳其语、法语、葡萄牙语、世界语、意大利语、匈牙利语和柏柏尔语。我从 Tatoeba 上排名前 10 的最受欢迎的使用拉丁字母表的语言中挑选了它们。你可能对这里列出的一些语言不熟悉。对于那些不熟悉的人来说，世界语是一种在19世纪末发明的构造辅助语言。柏柏尔语实际上是一组与阿拉伯语等闲语族语言表亲关系的在北非某些地区使用的语言。
- en: Maybe you were able to recognize some of these languages, even though you don’t
    actually speak them. I’d like you to step back and think *how* you did it. It’s
    quite interesting that people can do this without actually being able to speak
    the language, because these languages are all written in the roman alphabet and
    could look quite similar to each other. You may have recognized some unique diacritic
    marks (accents) for some of the languages—for example, “ü” for German and “ã”
    for Portuguese. These are a strong clue for these languages. Or you just knew
    some words—for example, “ayuda” for Spanish (meaning “help”) and “pas” in French
    (“ne . . . pas” is a French negation syntax). It appears that every language has
    its own characteristics—be it some unique characters or words—that makes it easy
    to tell it apart from others. This is starting to sound a lot like a kind of problem
    that machine learning is good at solving. Can we build an NLP system that can
    do this automatically? How should we go about building it?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你能够认出其中一些语言，尽管你实际上并不会说它们。我想让你退后一步思考*你是如何*做到的。很有趣的是，人们可以在不会说这种语言的情况下做到这一点，因为这些语言都是用拉丁字母表写成的，看起来可能非常相似。你可能认出了其中一些语言的独特变音符号（重音符号）——例如，德语的“ü”和葡萄牙语的“ã”。这些对于这些语言来说是一个强有力的线索。或者你只是认识一些单词——例如，西班牙语的“ayuda”（意思是“帮助”）和法语的“pas”（“ne...pas”是法语的否定句语法）。似乎每种语言都有其自己的特点——无论是一些独特的字符还是单词——使得它很容易与其他语言区分开来。这开始听起来很像是机器学习擅长解决的一类问题。我们能否构建一个能够自动执行此操作的
    NLP 系统？我们应该如何构建它？
- en: 4.6.1 Using characters as input
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.1 使用字符作为输入
- en: A language detector can also be built in a similar way to the sentiment analyzer.
    You can use an RNN to read the input text and convert it to some internal representation
    (hidden states). You can then use a linear layer to convert them to a set of scores
    corresponding to how likely the text is written in each language. Finally, you
    can use cross-entropy loss to train the model.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 语言检测器也可以以类似的方式构建情感分析器。你可以使用 RNN 读取输入文本并将其转换为一些内部表示（隐藏状态）。然后，你可以使用一个线性层将它们转换为一组分数，对应于文本写成每种语言的可能性。最后，你可以使用交叉熵损失来训练模型。
- en: One major difference between the sentiment analyzer and the language detector
    is how you feed the input into an RNN. When building the sentiment analyzer, we
    used the Stanford Sentiment Treebank and were able to assume that the input text
    is always English and already tokenized. But this is not the case for language
    detection. In fact, you don’t even know whether the input text is written in a
    language that can be tokenized easily—what if the sentence is written in Chinese?
    Or in Finnish, which is infamous for its complex morphology? You could use a tokenizer
    that is specific to the language if you know what language it is, but we are building
    the language detector because we don’t know what language it is in the first place.
    This sounds like a typical chicken-and-egg problem.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要区别在于情感分析器和语言检测器如何将输入馈送到 RNN 中。构建情感分析器时，我们使用了斯坦福情感树库，并且能够假设输入文本始终为英文且已经被标记化。但是对于语言检测来说情况并非如此。实际上，你甚至不知道输入文本是否是易于标记化的语言所写成——如果句子是用中文写的呢？或者是用芬兰语写的，芬兰语以其复杂的形态而臭名昭著？如果你知道是什么语言，你可以使用特定于该语言的标记器，但我们正在构建语言检测器，因为我们一开始并不知道是什么语言。这听起来像是一个典型的先有鸡还是先有蛋的问题。
- en: To address this issue, we are going to use characters instead of tokens as the
    input to an RNN. The idea is to break down the input into individual characters,
    even including whitespace and punctuation, and feed them to the RNN one at a time.
    Using characters is a common practice used when the input can be better represented
    as a sequence of characters (such as Chinese, or of an unknown origin), or when
    you’d like to make the best use of internal structures of words (such as the fastText
    model we mentioned in chapter 3). The RNN’s powerful representational power can
    still capture interactions between characters and some common words and n-grams
    mentioned earlier.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将使用字符而不是标记作为RNN的输入。这个想法是将输入分解为单个字符，甚至包括空格和标点符号，并将它们逐个馈送给RNN。当输入可以更好地表示为字符序列时（例如中文或未知来源的语言），或者当您希望充分利用单词的内部结构时（例如我们在第3章中提到的fastText模型）时，使用字符是一种常见的做法。RNN的强大表现力仍然可以捕获先前提到的字符和一些常见单词和n-gram之间的交互。
- en: 4.6.2 Creating a dataset reader
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建数据集阅读器
- en: 'For this language-detection task, I created the train and the validation datasets
    from the Tatoeba project by taking the 10 most popular languages on Tatoeba that
    use the roman alphabet and by sampling 10,000 sentences for the train set and
    1,000 for the validation set. An excerpt of this dataset follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个语言检测任务，我从Tatoeba项目中创建了train和validation数据集，方法是选择使用罗马字母的Tatoeba上最受欢迎的10种语言，并对训练集采样10,000个句子，验证集采样1,000个句子。以下是该数据集的摘录：
- en: '[PRE28]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The first field is a three-letter language code that describes which language
    the text is written in. The second field is the text itself. The fields are delimited
    by a tab character. You can obtain the datasets from the code repository ([https://github.com/mhagiwara/realworldnlp/tree/master/data/tatoeba](https://github.com/mhagiwara/realworldnlp/tree/master/data/tatoeba)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个字段是一个三个字母的语言代码，描述了文本所使用的语言。第二个字段是文本本身。字段由制表符分隔。您可以从代码存储库获取数据集（[https://github.com/mhagiwara/realworldnlp/tree/master/data/tatoeba](https://github.com/mhagiwara/realworldnlp/tree/master/data/tatoeba)）。
- en: 'The first step in building a language detector is to prepare a dataset reader
    that can read datasets in this format. In the previous example (the sentiment
    analyzer), because AllenNLP already provides StanfordSentimentTreeBankDatasetReader,
    you just needed to import and use it. In this scenario, however, you need to write
    your own. Fortunately, writing a dataset reader that can read this particular
    format is not that difficult. To write a dataset reader, you just need to do the
    following three things:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 构建语言检测器的第一步是准备一个能够读取这种格式数据集的数据集阅读器。在之前的例子（情感分析器）中，因为AllenNLP已经提供了StanfordSentimentTreeBankDatasetReader，所以您只需要导入并使用它。然而，在这种情况下，您需要编写自己的数据集阅读器。幸运的是，编写一个能够读取这种特定格式的数据集阅读器并不那么困难。要编写数据集阅读器，您只需要做以下三件事：
- en: Create your own dataset reader class by inheriting DatasetReader.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过继承DatasetReader创建自己的数据集阅读器类。
- en: Override the text_to_instance() method that takes raw text and converts it to
    an instance object.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖text_to_instance()方法，该方法接受原始文本并将其转换为实例对象。
- en: Override the_read() method that reads the content of a file and yields instances,
    by calling text_to_instance() above.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖_read()方法，该方法读取文件的内容并通过调用上面的text_to_instance()方法生成实例。
- en: 'The complete dataset reader for the language detector is shown in listing 4.3\.
    We also assume that you already imported necessary modules and classes as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 语言检测器的完整数据集阅读器如列表4.3所示。我们还假设您已经导入了必要的模块和类，如下所示：
- en: '[PRE29]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Listing 4.3 Dataset reader for the language detector
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.3 用于语言检测器的数据集阅读器
- en: '[PRE30]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Every new dataset reader inherits DatasetReader.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每个新的数据集阅读器都继承自DatasetReader。
- en: ❷ Uses CharacterTokenizer() to tokenize text into characters
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用CharacterTokenizer()将文本标记为字符
- en: ❸ Label will be None at test time.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在测试时标签将为None。
- en: ❹ If file_path is an URL, returns the actual path to a cached file on disk
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果file_path是URL，则返回磁盘上缓存文件的实际路径
- en: ❺ Yields instances using text_to_instance(), defined earlier
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用之前定义的text_to_instance()生成实例
- en: Note that the dataset reader in listing 4.3 uses CharacterTokenizer() to tokenize
    text into characters. Its tokenize() method returns a list of tokens, which are
    AllenNLP objects that represent tokens but actually contain characters in this
    scenario.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，列表4.3中的数据集阅读器使用CharacterTokenizer()将文本标记为字符。它的tokenize()方法返回一个标记列表，这些标记是AllenNLP对象，表示标记，但实际上在这种情况下包含字符。
- en: 4.6.3 Building the training pipeline
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建训练管道
- en: 'Once you build the dataset reader, the rest of the training pipeline looks
    similar to that of the sentiment analyzer. In fact, we can reuse the LstmClassifier
    class we defined previously without any modification. The entire training pipeline
    is shown in listing 4.4\. You can access the Google Colab notebook of the entire
    code from here: [http://realworldnlpbook.com/ch4.html#langdetect](http://realworldnlpbook.com/ch4.html#langdetect).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建了数据集阅读器，训练流水线的其余部分看起来与情感分析器的类似。 实际上，我们可以在不进行任何修改的情况下重用之前定义的 LstmClassifier
    类。 整个训练流水线在列表 4.4 中显示。 您可以从这里访问整个代码的 Google Colab 笔记本：[http://realworldnlpbook.com/ch4.html#langdetect](http://realworldnlpbook.com/ch4.html#langdetect)。
- en: Listing 4.4 Training pipeline for the language detector
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 语言检测器的训练流水线
- en: '[PRE31]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'When you run this training pipeline, you’ll get the metrics on the dev set
    that are in the ballpark of the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此训练流水线时，您将获得与以下大致相当的开发集上的指标：
- en: '[PRE32]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This is not bad at all! This means that the trained detector makes only one
    mistake out of about 20 sentences. Precision of 0.9481 means there’s only one
    false positive (non-English sentence) out of 20 instances that are classified
    as English. Recall of 0.9490 means there’s only one false negative (English sentence
    that was missed by the detector) out of 20 true English instances.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点一点也不糟糕！ 这意味着训练过的检测器在约 20 个句子中只犯了一个错误。 0.9481 的精确度意味着在 20 个被分类为英文的实例中只有一个假阳性（非英文句子）。
    0.9490 的召回率意味着在 20 个真正的英文实例中只有一个假阴性（被检测器漏掉的英文句子）。
- en: 4.6.4 Running the detector on unseen instances
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.4 在未见过的实例上运行检测器
- en: Finally, let’s try running the detector we just trained on a set of unseen instances
    (instances that didn’t appear either in the train or the validation sets). It
    is always a good idea to try feeding a small number of instances to your model
    and observe how it behaves.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们尝试在一组未见过的实例（既不出现在训练集也不出现在验证集中的实例）上运行我们刚刚训练过的检测器。 尝试向模型提供少量实例并观察其行为始终是一个好主意。
- en: 'The recommended way for feeding instances into a trained AllenNLP model is
    to use a predictor, as we did in chapter 2\. But here I’d like to do something
    simpler and instead write a method that, given a piece of text and a model, runs
    the prediction pipeline. To run a model on arbitrary instances, you can call the
    model’s forward_ on_instances() method, as shown in the following snippet:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 将实例提供给训练过的 AllenNLP 模型的推荐方法是使用预测器，就像我们在第二章中所做的那样。 但在这里，我想做一些更简单的事情，而是编写一个方法，给定一段文本和一个模型，运行预测流水线。
    要在任意实例上运行模型，可以调用模型的 forward_on_instances() 方法，如下面的代码片段所示：
- en: '[PRE33]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This method first takes the input (text and model) and passes it through a tokenizer
    to create an instance object. Then it calls model’s forward_on_instance() method
    to retrieve the logits, the scores for target labels (languages). It gets the
    label ID that corresponds to the maximum logit value by calling np.argmax and
    then converts it to the label text by using the vocabulary object associated with
    the model.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法首先接受输入（文本和模型）并通过分词器将其传递以创建实例对象。 然后，它调用模型的 forward_on_instance() 方法来检索 logits，即目标标签（语言）的分数。
    通过调用 np.argmax 获取对应于最大 logit 值的标签 ID，然后通过使用与模型关联的词汇表对象将其转换为标签文本。
- en: 'When I ran this method on some sentences that are not in the two datasets,
    I got the following results. Note that the result you get may be different from
    mine due to some randomness:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当我对一些不在这两个数据集中的句子运行此方法时，我得到了以下结果。 请注意，由于一些随机性，您得到的结果可能与我的不同：
- en: '[PRE34]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: These predictions are almost perfect, except the very first sentence—it is English,
    not French. It is surprising that the model makes a mistake on such a seemingly
    easy sentence while it predicts more difficult languages (such as Hungarian) perfectly.
    But remember, how difficult the language is for English speakers has nothing to
    do with how difficult it is for a computer to classify. In fact, some of the “difficult”
    languages such as Hungarian and Turkish here have very clear signals (accent marks
    and unique words) that make it easy to detect them. On the other hand, lack of
    clear signals in the first sentence may have made it more difficult to classify
    it from other languages.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测几乎完美，除了第一句话——它是英文，而不是法文。 令人惊讶的是，模型在预测更难的语言（如匈牙利语）时完美无误地犯了一个看似简单的错误。 但请记住，对于英语为母语者来说，语言有多难并不意味着计算机分类时有多难。
    实际上，一些“困难”的语言，比如匈牙利语和土耳其语，具有非常清晰的信号（重音符号和独特的单词），这使得很容易检测它们。 另一方面，第一句话中缺乏清晰的信号可能使它更难以从其他语言中分类出来。
- en: 'As a next step, you could try a couple of things: for example, you can tweak
    some of the hyperparameters to see how the evaluation metrics and the final prediction
    results change. You can also try a larger number of test instances to see how
    exactly the mistakes are distributed (e.g., between which two languages). You
    can also zero in on some of the instances and see why the model made such mistakes.
    These are all important practices when you are working on real-world NLP applications.
    I’ll discuss these topics in detail in chapter 10.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，你可以尝试一些事情：例如，你可以调整一些超参数，看看评估指标和最终预测结果如何变化。你还可以尝试增加测试实例的数量，以了解错误是如何分布的（例如，在哪两种语言之间）。你还可以把注意力集中在一些实例上，看看模型为什么会犯这样的错误。这些都是在处理真实世界的自然语言处理应用时的重要实践。我将在第
    10 章中详细讨论这些话题。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A recurrent neural network (RNN) is a neural network with a loop. It can transform
    a variable-length input to a fixed-length vector.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）是一种带有循环的神经网络。它可以将可变长度的输入转换为固定长度的向量。
- en: Nonlinearity is a crucial component that makes neural networks truly powerful.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性是使神经网络真正强大的关键组成部分。
- en: LSTMs and GRUs are two variants of RNN cells and are easier to train than vanilla
    RNNs.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 和 GRU 是 RNN 单元的两个变体，比原始的 RNN 更容易训练。
- en: You use accuracy, precision, recall, and F-measure for classification problems.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分类问题中，你可以使用准确率、精确度、召回率和 F-度量来评估。
- en: AllenNLP provides useful NLP abstractions such as dataset readers, instances,
    and vocabulary. It also provides a way to configure the training pipeline in the
    JSON format.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AllenNLP 提供了有用的自然语言处理抽象，例如数据集读取器、实例和词汇表。它还提供了一种以 JSON 格式配置训练流水线的方法。
- en: You can build a language detector as a sentence-classification application similar
    to the sentiment analyzer.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以构建一个类似于情感分析器的句子分类应用来实现语言检测器。
