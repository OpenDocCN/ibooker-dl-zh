- en: Chapter 4\. Understanding Generative AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：理解生成式人工智能
- en: In late 2015, a group of Silicon Valley entrepreneurs—including Elon Musk and
    Sam Altman—cofounded OpenAI with a mission to ensure that artificial general intelligence
    (AGI) benefits all of humanity. After initially focusing on reinforcement learning,
    the company shifted to generative AI, launching the GPT-2 model in 2019\. A year
    later, it released GPT-3, a model with 175 billion parameters trained on 570 GB
    of text, representing a massive leap from its predecessor.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2015 年晚些时候，一群硅谷企业家——包括埃隆·马斯克和山姆·奥特曼——共同创立了 OpenAI，其使命是确保通用人工智能 (AGI) 使全人类受益。最初专注于强化学习后，公司转向生成式人工智能，于
    2019 年推出了 GPT-2 模型。一年后，它发布了 GPT-3，这是一个在 570 GB 文本上训练的、具有 1750 亿个参数的模型，与前辈相比实现了巨大的飞跃。
- en: The turning point came on November 30, 2022, with the launch of ChatGPT. The
    application’s impact was immediate and transformative, attracting over one million
    users in its first week and 100 million in two months, making it the fastest-growing
    software application in history at the time. The success of ChatGPT triggered
    a surge of investment in generative AI, making the technology a priority for businesses
    worldwide. This led to the rapid development of new models from competitors, including
    Google’s Gemini and xAI’s Grok, each pushing the boundaries of the field.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 转折点出现在 2022 年 11 月 30 日，ChatGPT 的发布。该应用的影响是即时且变革性的，在第一周吸引了超过一百万用户，两个月内达到一亿，使其成为当时历史上增长最快的软件应用。ChatGPT
    的成功引发了对生成式人工智能的大量投资，使这项技术成为全球企业的优先事项。这导致竞争对手迅速开发出新的模型，包括谷歌的 Gemini 和 xAI 的 Grok，每个模型都在推动该领域的边界。
- en: This chapter explores how generative AI works, its core technologies, and its
    primary use cases.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了生成式人工智能的工作原理、其核心技术和其主要用例。
- en: Neural Networks and Deep Learning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与深度学习
- en: 'The foundational concepts behind modern generative AI began with early neural
    networks in the 1950s, which attempted to mirror the human brain. These simple
    networks had three components:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现代生成式人工智能背后的基础概念始于 20 世纪 50 年代的早期神经网络，这些网络试图模仿人脑。这些简单的网络有三个组成部分：
- en: Input layer
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层
- en: Receives the initial data
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 接收初始数据
- en: Hidden layer
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Contains nodes with random weights that process the input to find patterns
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 包含具有随机权重的节点，用于处理输入以找到模式
- en: Output layer
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层
- en: Applies an activation function to the processed data to determine the final
    output
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对处理后的数据应用激活函数以确定最终输出
- en: While these early systems were limited by the available computing power, they
    established the core principles for modern innovations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些早期系统受限于可用的计算能力，但它们为现代创新确立了核心原则。
- en: '*Deep learning* is an evolution of this structure that uses a neural network
    with many hidden layers, sometimes numbering in the hundreds. This complexity
    allows deep learning models to process vast amounts of data and detect intricate,
    nonlinear patterns that humans might not be able to identify. The basic workflow
    of a deep learning model involves data passing through these layers, with the
    model refining its predictions through a process called *backpropagation*, where
    it learns from its mistakes by adjusting the weights in the network.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习* 是这种结构的演变，它使用具有许多隐藏层的神经网络，有时数量可达数百。这种复杂性使得深度学习模型能够处理大量数据并检测复杂、非线性的模式，这些模式人类可能无法识别。深度学习模型的基本工作流程涉及数据通过这些层传递，模型通过称为
    *反向传播* 的过程不断优化其预测，在这个过程中，模型通过调整网络中的权重来从错误中学习。'
- en: Generative AI Models
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式人工智能模型
- en: Unlike traditional AI models that classify or predict based on inputs, generative
    AI produces original outputs that resemble the data it was trained on. These models
    have been used to generate everything from realistic portraits to humanlike conversations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于输入进行分类或预测的传统人工智能模型不同，生成式人工智能产生原始输出，这些输出类似于其训练数据。这些模型已被用于生成从逼真的肖像到类似人类的对话等一切内容。
- en: There are different flavors of generative AI models. Most of them rely on deep
    learning systems. But there are often many tweaks and customizations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能模型有多种类型。大多数模型依赖于深度学习系统。但通常有很多调整和定制。
- en: 'For the AIF-C01 exam, the types of generative AI models to understand include
    the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 AIF-C01 考试，需要了解的生成式人工智能模型类型包括以下几种：
- en: Generative adversarial network (GAN)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络 (GAN)
- en: Variational autoencoder (VAE)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分自编码器 (VAE)
- en: Transformer model
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器模型
- en: Diffusion model
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型
- en: Among these, the transformer model is the most important for the exam.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些模型中，转换器模型对于考试来说是最重要的。
- en: Let’s take a look at each of these in the following sections.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的章节中看看这些内容。
- en: Generative Adversarial Network
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: When Ian Goodfellow earned his Bachelor of Science and Master of Science degrees
    in computer science from Stanford University, he studied under one of the leading
    authorities on AI, Andrew Ng. This inspired him to pursue a career in this field,
    and he would go on to get a PhD in machine learning from Université de Montréal.
    It was here that he studied under Yoshua Bengio, another towering figure in AI.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当Ian Goodfellow从斯坦福大学获得计算机科学学士和硕士学位时，他师从人工智能领域的权威人物Andrew Ng。这激发了他在这个领域追求职业生涯，并且他将继续在蒙特利尔大学获得机器学习博士学位。正是在这里，他在人工智能领域的另一位杰出人物Yoshua
    Bengio的指导下学习。
- en: His first job out of school was as an intern at Google. He created a neural
    network that could translate addresses from images, which improved Google Maps.
    But it was in 2014 that Goodfellow had his major breakthrough—the generative adversarial
    network. This actually came about from a discussion he had with colleagues at
    a [microbrewery in Montreal, Canada](https://oreil.ly/9DJb4). The topic was about
    how to improve the training for a generative model. His friends talked about an
    approach that would use large amounts of resources. But Goodfellow thought a better
    method was to have two neural networks compete against each other. This would
    allow for the system to create better content.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 他大学毕业后第一份工作是谷歌的实习生。他创建了一个神经网络，可以从图像中翻译地址，这改善了谷歌地图。但2014年Goodfellow取得了重大突破——生成对抗网络。这实际上源于他在加拿大蒙特利尔的一家小啤酒厂与同事的讨论。[蒙特利尔小啤酒厂](https://oreil.ly/9DJb4)。讨论的主题是如何改进生成模型的训练。他的朋友们谈论了一种将使用大量资源的方法。但Goodfellow认为更好的方法是让两个神经网络相互竞争。这将允许系统创建更好的内容。
- en: When Goodfellow went home later in the night, he could not stop thinking about
    this concept. He spent a few hours creating the GAN model, which generated compelling
    images.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当Goodfellow在深夜回家后，他无法停止对这个概念的想法。他花了几小时创建GAN模型，该模型生成了引人入胜的图像。
- en: The two neural networks in the GAN were the generator and the discriminator,
    as shown in [Figure 4-1](#figure_four_onedot_the_gan_model).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GAN中的两个神经网络是生成器和判别器，如图[图4-1](#figure_four_onedot_the_gan_model)所示。
- en: '![](assets/awsc_0401.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/awsc_0401.png)'
- en: Figure 4-1\. The GAN model
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1. GAN模型
- en: The generator creates synthetic data from random noise inputs. The discriminator,
    on the other hand, will evaluate the synthetic data and try to assess if it is
    real or not. This “adversarial” process will iterate until the generator is creating
    data that appears real.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器从随机噪声输入中创建合成数据。另一方面，判别器将评估合成数据并尝试判断其是否真实。这个“对抗”过程将迭代，直到生成器创建出看似真实的数据。
- en: On its face, it is a simple concept. But of course, the GAN was based on complex
    math and algorithms. Not long after developing the GAN, Goodfellow published a
    paper on it, which immediately stirred significant interest from the AI community.
    Meta’s chief AI scientist, Yann LeCun, called it “the coolest idea in deep learning
    in the last 20 years.”
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在表面上，这是一个简单的概念。但当然，生成对抗网络（GAN）是基于复杂的数学和算法的。在开发生成对抗网络不久后，Goodfellow发表了一篇关于它的论文，这立即引起了人工智能社区的广泛关注。Meta的首席人工智能科学家Yann
    LeCun称其为“过去20年来深度学习中最酷的想法”。
- en: There also emerged tools to create images using GANs. Many were posted on social
    media sites like Twitter. In fact, one image was auctioned on Christie’s auction,
    fetching a hefty [$432,000](https://oreil.ly/wKwnu).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 还出现了使用GAN创建图像的工具。许多图像都发布在社交媒体网站如Twitter上。事实上，有一张图像在佳士得拍卖会上拍卖，成交价为高额的[$432,000](https://oreil.ly/wKwnu)。
- en: But GANs also proved useful for diverse areas like scientific research, such
    as to improve the accuracy of detecting behavior of subatomic particles in the
    Large Hadron Collider at CERN in Switzerland.^([1](ch04.html#ch01fn2))
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但GANs也被证明在科学研究的多个领域很有用，例如提高瑞士CERN的大型强子对撞机中检测亚原子粒子行为准确性的能力.^([1](ch04.html#ch01fn2))
- en: Variational Autoencoder
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: In a 2013 paper,^([2](ch04.html#ch01fn3)) Diederik P. Kingma and Max Welling
    introduced the variational autoencoder, as diagrammed in [Figure 4-2](#figure_four_twodot_the_process_of_a_vae).
    It was a combination of a complex neural network and advanced probability theory.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在2013年的一篇论文中^([2](ch04.html#ch01fn3))，Diederik P. Kingma和Max Welling介绍了变分自编码器，如图[图4-2](#figure_four_twodot_the_process_of_a_vae)所示。它是一种复杂神经网络和高级概率理论的结合。
- en: '![](assets/awsc_0402.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/awsc_0402.png)'
- en: Figure 4-2\. The process of a VAE
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2. VAE的过程
- en: 'A VAE has two main components that work together: an encoder and a decoder.
    The encoder acts like a smart summarizer that takes your original data and converts
    it into a compact representation, but unlike a simple summary, it creates what’s
    called a *probability distribution*. This means instead of just creating one fixed
    summary, it learns to capture the range of possible variations and uncertainties
    in the data.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 有两个主要组件协同工作：编码器和解码器。编码器就像一个智能摘要器，将你的原始数据转换为紧凑的表示，但与简单的摘要不同，它创建了一个所谓的 *概率分布*。这意味着它不仅仅创建一个固定的摘要，而是学会了捕捉数据中可能的变化和不确定性的范围。
- en: The decoder works in reverse, taking these compressed representations and reconstructing
    them back into data that resembles the original input. What makes VAEs particularly
    powerful is that they don’t just learn to copy data perfectly. Instead, they learn
    the underlying patterns and relationships. This means you can sample from the
    probability distribution in the middle—that is, the latent space—to generate entirely
    new data that shares the same characteristics as your original dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器反向工作，将这些压缩表示重新构建成与原始输入相似的数据。VAEs 特别强大的地方在于，它们不仅学会了完美复制数据。相反，它们学会了底层模式和关系。这意味着你可以从中间的概率分布——即潜在空间——中进行采样，以生成具有与原始数据集相同特征的新数据。
- en: 'A common use case for a VAE is to create images. But it can also be used for:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 的一个常见用例是创建图像。但它也可以用于：
- en: Anomaly detection
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测
- en: A VAE is effective in finding outliers, which can be critical for fraud detection
    and network security.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 在寻找异常值方面非常有效，这对于欺诈检测和网络安全至关重要。
- en: Drug discovery
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 药物发现
- en: A VAE can create molecular structures. This can help identify potential drug
    candidates quicker and more efficiently.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 可以创建分子结构。这有助于更快、更有效地识别潜在的药物候选者。
- en: Sound
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 声音
- en: You can create sound effects and even new music.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以创建音效，甚至新的音乐。
- en: Transformer Model
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 模型
- en: 'The launch of the transformer model—which is at the heart of generative AI—came
    in August 2017\. It was published in an academic paper^([3](ch04.html#ch01fn4))
    by authors who were part of the Google Research team. The inspiration for the
    model actually came about from a lunch they had. The researchers debated the question:
    How can computers generate content that is humanlike?'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型——生成式 AI 的核心——于 2017 年 8 月推出。它由谷歌研究团队的部分作者在学术论文^([3](ch04.html#ch01fn4))中发表。该模型的灵感实际上源于他们的一次午餐。研究人员讨论了这样一个问题：计算机如何生成类似人类的内容？
- en: What they came up with turned out to be one of the biggest innovations in AI—ever.
    The academic paper would ultimately be cited more than 80,000 times.^([4](ch04.html#ch01fn5))
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出的东西最终成为 AI 最大的创新之一——史无前例。这篇学术论文最终被引用超过 80,000 次.^([4](ch04.html#ch01fn5))
- en: The irony is that Google did not initially pay much attention to the transformer.
    In the meantime, various startups, including OpenAI, saw this technology as the
    best approach for AI. Some of the Google researchers would go on to start their
    own AI ventures.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 具有讽刺意味的是，谷歌最初并没有过多关注 Transformer。与此同时，包括 OpenAI 在内的各种初创公司都将这项技术视为 AI 的最佳途径。一些谷歌研究人员随后开始创办自己的
    AI 企业。
- en: 'Before the introduction of the transformer model, the main approach with NLP
    was the use of recurrent neural networks (RNNs). This processes data, like text,
    speech, and time series, sequentially. But there’s a problem with this: it can
    fail to capture the context.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformer 模型引入之前，NLP 的主要方法是使用循环神经网络（RNNs）。它按顺序处理数据，如文本、语音和时间序列。但这里有一个问题：它可能无法捕捉上下文。
- en: To deal with this, there were some innovations like long short-term memory (LSTM)
    networks, which also proved to be limited.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，出现了一些创新，如长短期记忆（LSTM）网络，但这也证明是有限的。
- en: But with the transformer model, the approach was turned on its head. Instead
    of processing data step-by-step like RNNs, transformers used attention mechanisms
    to consider all parts of the input at once—allowing for a deeper and more flexible
    understanding of context in natural language.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但随着 Transformer 模型的出现，方法发生了颠覆。与 RNNs 逐个处理数据不同，Transformer 使用注意力机制一次性考虑输入的所有部分——允许对自然语言中的上下文有更深入和更灵活的理解。
- en: 'To accomplish this, the transformer architecture relies on four main components
    (see [Figure 4-3](#figure_four_threedot_the_process_of_a_t)):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一点，Transformer 架构依赖于四个主要组件（见[图 4-3](#figure_four_threedot_the_process_of_a_t)）：
- en: Input embedding
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输入嵌入
- en: Converts words into numerical vectors that can be processed by the model
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词转换为模型可以处理的数值向量
- en: Positional encoding
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码
- en: Adds information about the position of each word in a sentence, since the model
    does not process input sequentially
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为句子中每个单词的位置添加信息，因为模型不按顺序处理输入
- en: Encoder stack
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器栈
- en: Analyzes the entire input sequence and builds a contextual representation of
    it
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 分析整个输入序列并构建其上下文表示
- en: Decoder stack
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器栈
- en: Generates the output sequence, using the encoded input and previously generated
    outputs to produce fluent, coherent text
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用编码后的输入和先前生成的输出生成输出序列，以产生流畅、连贯的文本
- en: '![](assets/awsc_0403.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/awsc_0403.png)'
- en: Figure 4-3\. The process of a transformer model
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 变换器模型的过程
- en: Let’s take a look at each of the four main components.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这四个主要组成部分中的每一个。
- en: Input embedding
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入嵌入
- en: Input embedding converts tokens into a vector representation, which is a string
    of numbers. This allows a model to analyze the data and find the patterns.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输入嵌入将标记转换为向量表示，这是一个数字的字符串。这允许模型分析数据并找到模式。
- en: 'Suppose a model processes the following: “She ate the pizza.” The input embedding
    might look like the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设模型处理以下句子：“她吃了披萨。”输入嵌入可能看起来如下：
- en: “She” → [0.25, –0.13, 0.40, ...]
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “她” → [0.25, –0.13, 0.40, ...]
- en: “ate” → [0.10, 0.22, –0.35, ...]
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “吃了” → [0.10, 0.22, –0.35, ...]
- en: “the” → [–0.05, 0.15, 0.20, ...]
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “the” → [–0.05, 0.15, 0.20, ...]
- en: “pizza” → [0.30, –0.25, 0.50, ...]
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “披萨” → [0.30, –0.25, 0.50, ...]
- en: Each number in a vector is called a *component*, and together they exist within
    a vector space—a mathematical structure made up of multiple dimensions. Each dimension
    represents a different direction or feature, allowing patterns in the data to
    be represented and analyzed. Creating these vectors involves complex calculations
    based on linear algebra.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 向量中的每个数字称为*分量*，它们共同存在于一个向量空间中——一个由多个维度组成的数学结构。每个维度代表一个不同的方向或特征，允许数据中的模式被表示和分析。创建这些向量涉及基于线性代数的复杂计算。
- en: Positional encoding
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置编码
- en: The problem with input embedding is that it will jumble the order of the words.
    No doubt, this can mean that some of the context will be lost or confused.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输入嵌入的问题在于它将打乱单词的顺序。毫无疑问，这可能导致一些上下文丢失或混淆。
- en: 'This is where positional encoding comes in. This assigns a unique numerical
    vector to each position in the sequence of the words. Here’s an example based
    on our sentence:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是位置编码发挥作用的地方。它为序列中每个位置分配一个唯一的数值向量。以下是基于我们句子的一个示例：
- en: Position 1 (for “She”) → [0.01, 0.02, 0.03]
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置 1（对应“她”）→ [0.01, 0.02, 0.03]
- en: Position 2 (for “ate”) → [0.02, 0.03, 0.04]
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置 2（对应“吃了”）→ [0.02, 0.03, 0.04]
- en: Position 3 (for “the”) → [0.03, 0.04, 0.05]
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置 3（对应“the”）→ [0.03, 0.04, 0.05]
- en: Position 4 (for “pizza”) → [0.04, 0.05, 0.06]
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置 4（对应“披萨”）→ [0.04, 0.05, 0.06]
- en: These are then added to the input embeddings.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些随后被添加到输入嵌入中。
- en: Encoder stack
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器栈
- en: The encoder stack is where the transformer model attempts to understand the
    meaning of the text. This involves different layers of processing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器栈是变换器模型试图理解文本意义的地方。这涉及到不同的处理层。
- en: The first one uses a self-attention mechanism, which shows how each word relates
    to every other word. In our example, the model will evaluate the relationship
    between “she” and “ate.” It will understand that “she” is the subject and is performing
    the action of “ate.”
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个使用自注意力机制，它展示了每个单词如何与其他每个单词相关。在我们的例子中，模型将评估“她”和“吃了”之间的关系。它将理解“她”是主语，正在执行“吃了”的动作。
- en: After this, there will be processing with more layers of self-attention. The
    goal is to get a better understanding of the meaning of the text. At the end of
    this process, the model should have a solid understanding of “She ate the pizza.”
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，将进行更多层的自注意力处理。目标是更好地理解文本的意义。在此过程结束时，模型应该对“她吃了披萨”有一个稳固的理解。
- en: Decoder stack
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器栈
- en: The decoder stack is responsible for generating an output sequence. Common tasks
    include translation, content generation, and summarization.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器栈负责生成输出序列。常见任务包括翻译、内容生成和摘要。
- en: 'Let’s say we want to translate the sentence “She ate the pizza” into Spanish.
    The decoder follows a step-by-step process for each word:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要将句子“她吃了披萨”翻译成西班牙语。解码器对每个单词遵循逐步的过程：
- en: Masked self-attention
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码自注意力
- en: This allows the decoder to focus only on the words it has already generated,
    preventing it from “seeing” future words. For the first token, it predicts the
    most likely starting word.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许解码器只关注它已经生成的单词，防止它“看到”未来的单词。对于第一个标记，它预测最可能的起始单词。
- en: Encoder-decoder attention
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器注意力
- en: This step connects the decoder to the information from the input sentence. For
    example, the model recognizes that “she” is the subject of the sentence.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤将解码器连接到输入句子的信息。例如，模型识别出“她”是句子的主语。
- en: Output generation
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 输出生成
- en: Based on the previous steps, the decoder predicts the first word in Spanish—“Ella.”
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的步骤，解码器预测西班牙语中的第一个单词——“Ella。”
- en: This process continues word by word until the entire translation is complete.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程逐词进行，直到整个翻译完成。
- en: A transformer is a prediction engine
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变压器是一种预测引擎
- en: The transformer model is certainly complex. But when you boil things down, it’s
    really about how it is a prediction engine. As we saw in the encoder and decoder
    stacks, the model is predicting the next word in a sequence.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型当然很复杂。但当你把它简化下来，它实际上就是一个预测引擎。正如我们在编码器和解码器堆栈中看到的，模型正在预测序列中的下一个单词。
- en: The predictions are based on the complex relationships among all the words in
    the dataset. This is where the power of the system shines. By leveraging attention
    mechanisms with massive datasets—which are often most of the content on the internet—the
    transformer model can understand and create content in humanlike ways. There is
    also no need for labeling data. The reason is that the transformer is analyzing
    the relationships among the tokens.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 预测基于数据集中所有单词之间的复杂关系。这正是系统力量的体现。通过利用大规模数据集（通常是互联网上大部分内容）中的注意力机制，变压器模型能够以类似人类的方式理解和创造内容。而且，无需对数据进行标记。原因是变压器正在分析标记之间的关系。
- en: However, there are issues with the transformer model. After all, predictions
    are estimates and can sometimes be wrong. We’ll discuss some of these issues with
    generative AI later in this chapter.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，变压器模型存在一些问题。毕竟，预测是估计，有时可能会出错。我们将在本章后面讨论一些与生成式AI相关的问题。
- en: 'Something else to keep in mind: the transformer model does not have inherent
    knowledge. There is no hardcoded logic, database access, and so on. Again, it’s
    all about making predictions.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的另一件事：变压器模型没有固有的知识。没有硬编码的逻辑、数据库访问等。再次强调，一切都是关于做出预测。
- en: Diffusion Model
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩散模型
- en: In 2015, researchers Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    and Surya Ganguli introduced the diffusion model. They set out the core principles
    in a paper^([5](ch04.html#ch01fn6)) that described an innovative way to create
    new data, such as for images and audio.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，研究人员Jascha Sohl-Dickstein、Eric Weiss、Niru Maheswaranathan和Surya Ganguli引入了扩散模型。他们在论文^([5](ch04.html#ch01fn6))中阐述了创建新数据（如图像和音频）的创新方法的核心原则。
- en: 'As shown in [Figure 4-4](#figure_four_fourdot_the_diffusion_model), the diffusion
    model has two primary phases:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图4-4](#figure_four_fourdot_the_diffusion_model)所示，扩散模型有两个主要阶段：
- en: Forward diffusion
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正向扩散
- en: The diffusion model gradually adds noise to the original dataset (such as images
    or audio). Through this process, it learns to understand the data distribution
    and how it transitions from structured data to pure random noise. This phase maps
    the pathway from meaningful data to complete noise.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型逐渐向原始数据集（如图像或音频）添加噪声。通过这个过程，它学会了理解数据分布以及它如何从结构化数据过渡到纯随机噪声。此阶段将有意义的数据到完全噪声的路径进行映射。
- en: Reverse diffusion
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 反向扩散
- en: The diffusion model reverses the forward process by starting with random noise
    and systematically removing it through many steps. This generates new data that
    is different from the original dataset but retains similar characteristics and
    features. The reverse phase essentially learns to reconstruct structured data
    from noise, enabling the creation of novel samples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型通过从随机噪声开始，并通过许多步骤系统地去除它来反转正向过程。这生成了与原始数据集不同但保留相似特征和特征的新数据。反向阶段本质上是从噪声中学习重建结构化数据，从而能够创建新的样本。
- en: '![](assets/awsc_0404.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/awsc_0404.png)'
- en: Figure 4-4\. The diffusion model
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4. 扩散模型
- en: Examples of popular diffusion models include OpenAI’s DALL-E, Stability AI’s
    Stable Diffusion, and Midjourney. They use the text-to-image process.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 流行扩散模型的例子包括OpenAI的DALL-E、Stability AI的Stable Diffusion和Midjourney。它们使用文本到图像的过程。
- en: 'Let’s see how this works in DALL-E (not part of AWS). In the input box for
    ChatGPT, click “…” and then select Image. Enter the following prompt:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在DALL-E（不是AWS的一部分）中是如何工作的。在ChatGPT的输入框中点击“…”然后选择图像。输入以下提示：
- en: A floating island with cascading waterfalls that fall into a swirling vortex
    of stars, under an aurora-lit sky.
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一座瀑布飞流直下，落入星云漩涡之中，天空被极光照亮。
- en: '[Figure 4-5](#figure_four_fivedot_an_image_created_by) shows the generated
    image.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-5](#figure_four_fivedot_an_image_created_by)展示了生成的图像。'
- en: '![An image created by OpenAI’s DALL-E](assets/awsc_0405.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![由OpenAI的DALL-E创建的图像](assets/awsc_0405.png)'
- en: Figure 4-5\. An image created by OpenAI’s DALL-E
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5\. 由OpenAI的DALL-E创建的图像
- en: Foundation Models
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础模型
- en: With the transformer or diffusion model, you can create foundation models (FMs).
    These are what you can use for your business or personal use, such as ChatGPT
    or Claude.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用transformer或diffusion模型，你可以创建基础模型（FM）。这些模型可以用于你的商业或个人用途，比如ChatGPT或Claude。
- en: There are two main types of FMs. One is the large language model (LLM), which
    is built on the transformer model. An LLM can handle many NLP tasks—answer questions
    about history, write a poem, write code, and so on. There seems to be no end to
    the capabilities. By comparison, traditional AI is mostly focused on a single
    task, such as making a forecast about sales or churn.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: FM主要有两种类型。一种是大型语言模型（LLM），它是基于transformer模型构建的。LLM可以处理许多NLP任务——回答关于历史的问题、写诗、编写代码等等。似乎其能力没有尽头。相比之下，传统的AI主要关注单一任务，例如预测销售或客户流失。
- en: Next, there are multimodal models. These models can understand and generate
    different types of content like text, audio, images, and video. A multimodal system
    uses both the transformer and diffusion models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是多模态模型。这些模型可以理解和生成不同类型的内容，如文本、音频、图像和视频。多模态系统同时使用transformer和diffusion模型。
- en: 'The lifecycle for training FMs and developing applications for them is different
    from traditional machine learning workflows. For the purposes of the exam, the
    steps are:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 训练FM和为其开发应用程序的生命周期与传统机器学习工作流程不同。为了考试的目的，步骤如下：
- en: Data selection
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据选择
- en: Pretraining
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练
- en: Optimization
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化
- en: Evaluation
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估
- en: Deployment
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署
- en: Let’s discuss each of these steps in the following sections.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的章节中讨论这些步骤。
- en: Data Selection
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据选择
- en: The datasets for FMs are enormous. For example, OpenAI’s GPT-4 model includes
    nearly 500 billion parameters—the internal values the model adjusts during training
    to make accurate predictions—and processes around 45 terabytes of data. Sources
    for this training data include WebTest (a filtered snapshot of web pages), English
    Wikipedia, and large collections of public domain books.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于FM来说，数据集非常庞大。例如，OpenAI的GPT-4模型包含近500亿个参数——模型在训练过程中调整的内部值，用于做出准确的预测——并处理大约45TB的数据。这些训练数据来源包括WebTest（网页的过滤快照）、英语维基百科和大量公共领域的书籍。
- en: What about the more recent models, such as GPT-4 or GPT-o1? There are no details
    on the dataset size. The main reason is to protect competitive advantages. The
    world of model development is certainly high-stakes, especially since it costs
    substantial amounts to build FMs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，关于最近的一些模型，比如GPT-4或GPT-o1呢？关于数据集大小的细节没有公布。主要原因是保护竞争优势。模型开发的世界当然风险很高，特别是构建FM需要大量的成本。
- en: For the data selection process, there is no need to wrangle or clean the datasets.
    The model will work seamlessly with unlabeled data. As we saw earlier in this
    chapter, the transformer model will detect the patterns, such as with attention
    mechanisms.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据选择过程中，不需要对数据集进行整理或清理。模型将无缝地与未标记的数据一起工作。正如我们在本章前面所看到的，transformer模型将通过注意力机制等机制检测模式。
- en: As a rule of thumb, the larger the dataset, the better. This is known as the
    *scaling laws*. Research has shown that there is a positive relationship between
    the number of parameters in the model and the performance.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，数据集越大越好。这被称为**规模定律**。研究表明，模型中参数的数量与性能之间存在正相关关系。
- en: The data must be high quality and diverse. This helps to reduce the issues with
    bias and toxic content. Because of this, there is usually extensive curation and
    filtering of the datasets, which is where data science expertise becomes essential.
    There is also the use of various data selection methods, like the Data Selection
    with Importance Resampling (DSIR) framework. This helps to focus on data that
    is most relevant for a particular application.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 数据必须是高质量的且多样化的。这有助于减少偏差和有害内容的问题。因此，通常需要对数据集进行广泛的编辑和筛选，这时数据科学专业知识变得至关重要。还有使用各种数据选择方法，如数据选择与重要性重采样（DSIR）框架。这有助于关注对特定应用最相关的数据。
- en: Pretraining
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练
- en: The pretraining stage is where the model learns to understand and generate humanlike
    text. This is done by using a technique called *semisupervised learning*. This
    takes the unlabeled dataset and creates synthetic labels, which are based on the
    data itself. For example, the model can use the transformer model to predict missing
    words or other gaps in a sentence. Given the massive sizes of the datasets, this
    automated approach is absolutely critical. It would be impossible to handle this
    in a manual way.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练阶段是模型学习理解和生成类似人类文本的阶段。这是通过使用一种称为*半监督学习*的技术来完成的。它使用未标记的数据集并创建基于数据的合成标签。例如，模型可以使用Transformer模型来预测句子中的缺失单词或其他空白。鉴于数据集的巨大规模，这种自动化的方法绝对至关重要。手动处理这是不可能的。
- en: But there is more to the process. There is also continuous pretraining, which
    is when the model is exposed to more data to refine and improve the learning.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 但过程不仅如此。还有持续预训练，这是当模型接触到更多数据以精炼和改进学习时。
- en: The pretraining and continuous pretraining require significant amounts of computing
    resources. A key part of this is the use of graphics processing units (GPUs) or
    tensor processing units (TPUs). GPUs are designed for parallel processing and
    are widely used in deep learning and generative AI models, while TPUs are specialized
    hardware developed by Google specifically to accelerate machine learning tasks.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练和持续预训练需要大量的计算资源。这部分的关键是使用图形处理单元（GPUs）或张量处理单元（TPUs）。GPU是为并行处理设计的，在深度学习和生成式AI模型中广泛使用，而TPU是谷歌专门开发用于加速机器学习任务的专用硬件。
- en: Optimization
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化
- en: 'An FM is quite powerful. But it will not be trained on proprietary data. This
    can certainly be limiting for businesses, which need more specialized FMs, say
    for handling customer support, legal, marketing, sales, and so on. What can you
    do? You can optimize an FM, which involves two main approaches:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: FM（特征模型）非常强大。但它不会在专有数据上进行训练。这对企业来说可能确实有限制，因为企业需要更多专业化的FM，比如用于处理客户支持、法律、营销、销售等。你能做什么呢？你可以优化一个FM，这涉及到两种主要方法：
- en: Fine-tuning
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调
- en: Retrieval-augmented generation
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索增强生成
- en: Fine-tuning
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调
- en: Suppose you work in your company’s legal department. While FMs are useful for
    applications like summarization, they do not perform well when it comes to complex
    legal queries.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在你公司的法律部门工作。虽然FM在摘要等应用中很有用，但在处理复杂的法律查询时表现不佳。
- en: If you want to create a system that can effectively extract contract clauses
    and entities, you can use fine-tuning of an existing FM. This is also known as
    *transfer learning*, which is where a model developed for one purpose can be used
    for another.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要创建一个能够有效提取合同条款和实体的系统，你可以使用现有FM的微调。这也被称为*迁移学习*，即一个为某个目的开发的模型可以用于另一个目的。
- en: 'Here are the main steps for fine-tuning:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是微调的主要步骤：
- en: Data collection
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集
- en: Gather relevant documents that are specific to your domain or task. In our example,
    this would include contracts, agreements, and legal correspondence.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 收集与你的领域或任务相关的相关文档。在我们的例子中，这包括合同、协议和法律信函。
- en: Privacy and security
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私和安全
- en: Fine-tuning often uses proprietary or highly sensitive data. This is why there
    needs to be strong privacy, security policies, and guardrails in place. The data
    should also be evaluated to mitigate issues with bias.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 微调经常使用专有或高度敏感的数据。这就是为什么需要实施强大的隐私、安全政策和限制措施。数据还应进行评估以减轻偏差问题。
- en: Data labeling
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标注
- en: Fine-tuning is a supervised learning process. This means you will label the
    dataset, such as marking specific clauses and entities.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是一个监督学习过程。这意味着你需要对数据集进行标注，例如标记特定的条款和实体。
- en: Training
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 训练
- en: 'You will apply an algorithm to the dataset to adjust the weights and biases
    of the model. For this, there are two main approaches—instruction fine-tuning
    and reinforcement learning from human feedback (RLHF):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你将应用算法到数据集上，以调整模型的权重和偏差。为此，有两种主要方法——指令微调和基于人类反馈的强化学习（RLHF）：
- en: Instruction fine-tuning
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调
- en: This processes examples of how a model should respond based on certain prompts
    and the output to help the system learn better. It can be quite effective for
    applications like chatbots and virtual assistants.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程基于某些提示和输出示例来展示模型应该如何响应，以帮助系统更好地学习。这对于聊天机器人和虚拟助手等应用非常有效。
- en: RLHF
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF
- en: The first step in this process is to train the model using supervised learning,
    where it learns based on predicting humanlike responses. The next step is to refine
    the responses by using reinforcement learning, which is based on human feedback.
    It will reward or punish the responses based on this. The goal is to create a
    model that aligns more with human values.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的第一个步骤是使用监督学习来训练模型，其中模型通过预测类似人类的响应来学习。下一步是使用基于人类反馈的强化学习来细化响应。它将根据这个反馈奖励或惩罚响应。目标是创建一个更符合人类价值观的模型。
- en: Iterate and evaluate
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代和评估
- en: You will iterate this on the dataset until the model learns to recognize and
    extract the clauses and entities.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在数据集上迭代，直到模型学会识别和提取子句和实体。
- en: Besides customizing the FM, fine-tuning can also improve the overall accuracy
    of the responses and help to reduce the bias. There is also the benefit of efficiency.
    You can leverage an existing model and make much smaller modifications to get
    better results.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 除了定制 FM，微调还可以提高整体响应的准确性，并有助于减少偏差。还有效率上的好处。你可以利用现有模型，并对其进行较小的修改以获得更好的结果。
- en: But there are drawbacks to fine-tuning. Like with a traditional ML model, there
    is the risk of overfitting. The reason is that the datasets can be too narrow.
    Another issue is that the fine-tuning may go too far—that is, the model may lose
    its advantages for being general-purpose. Finally, fine-tuning can still take
    considerable resources, often needing sophisticated GPUs and AI platforms.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 但微调也有缺点。就像传统的 ML 模型一样，存在过拟合的风险。原因是数据集可能过于狭窄。另一个问题是微调可能过度——也就是说，模型可能失去了其通用性的优势。最后，微调仍然需要相当多的资源，通常需要复杂的
    GPU 和 AI 平台。
- en: 'To help with the problems, there are more advanced fine-tuning methods:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助解决这些问题，有更高级的微调方法：
- en: Low-rank adaptation (LoRA)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（LoRA）
- en: Instead of adjusting all the model’s parameters, this technique takes a more
    targeted approach, using much fewer resources.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 与调整模型的所有参数不同，这项技术采取了一种更针对性的方法，使用了更少的资源。
- en: Representation fine-tuning (ReFT)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表示微调（ReFT）
- en: This is an even more efficient approach to fine-tuning, which modifies less
    than 1% of the internal weights of the model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个更高效的微调方法，它修改了模型内部不到 1% 的权重。
- en: RAG
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RAG
- en: In 2024, *Time* magazine named Patrick Lewis as one of the 100 most influential
    people in AI.^([6](ch04.html#ch01fn7)) The primary reason for this is a paper^([7](ch04.html#ch01fn8))
    he cowrote in 2020 with other researchers from Meta, which set forth a framework
    to connect data to LLMs by searching external databases.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2024 年，*时代*杂志将帕特里克·刘易斯评为 AI 领域 100 位最有影响力的人物之一.^([6](ch04.html#ch01fn7)) 主要原因是他与其他来自
    Meta 的研究人员在 2020 年共同撰写的一篇论文^([7](ch04.html#ch01fn8))，提出了一个通过搜索外部数据库将数据与 LLMs 连接起来的框架。
- en: The authors called it *retrieval-augmented generation*. It not only allowed
    for customizing LLMs but also reducing hallucinations. RAG was also generally
    easier to use than fine-tuning, as there were no changes to the weights of the
    model. The process is outlined in [Figure 4-6](#figure_four_sixdot_the_rag_process).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将其称为*检索增强生成*。它不仅允许定制 LLMs，还能减少幻觉。与微调相比，RAG 通常更容易使用，因为模型权重没有变化。该流程在[图 4-6](#figure_four_sixdot_the_rag_process)中概述。
- en: '![](assets/awsc_0406.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/awsc_0406.png)'
- en: Figure 4-6\. The RAG process
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. RAG 流程
- en: 'Here’s a look at the main steps:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是主要步骤的概述：
- en: Data collection and indexing
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集和索引
- en: The RAG process begins by collecting relevant data from various sources such
    as PDFs, reports, articles, web pages, logs, and customer feedback. This information
    is then prepared for processing and storage.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 流程首先从各种来源收集相关数据，如 PDF、报告、文章、网页、日志和客户反馈。然后，这些信息将被准备用于处理和存储。
- en: Chunking
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 分块
- en: Since LLMs have limits on how much data they can process at once, the collected
    dataset is divided into smaller, manageable chunks.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM在一次性处理数据量上有限制，收集的数据集被分成更小、更易于管理的块。
- en: Embedding creation
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入创建
- en: Each chunk of information is converted into a vector embedding using a specialized
    machine learning model. These embeddings capture the semantic meaning of the data
    and represent it in a high-dimensional mathematical format.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 每个信息块都使用专门的机器学习模型转换为向量嵌入。这些嵌入捕获数据的语义含义，并以高维数学格式表示。
- en: Vector database storage
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库存储
- en: The generated embeddings are stored in a vector database, which is a specialized
    system designed to manage and efficiently search through high-dimensional vectors.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的嵌入向量存储在向量数据库中，这是一个专门设计用于管理和高效搜索高维向量的系统。
- en: User input
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 用户输入
- en: When a user submits a query or prompt, this marks the beginning of the retrieval
    phase of the RAG process.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户提交查询或提示时，这标志着RAG过程的检索阶段的开始。
- en: Processing user input
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 处理用户输入
- en: The user’s prompt is converted into an embedding using the same ML model that
    was used for the data chunks. This ensures consistency between the user query
    representation and the stored data representations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 用户的提示被转换为嵌入向量，使用与数据块相同的机器学习模型。这确保了用户查询表示和存储数据表示之间的一致性。
- en: Retrieval
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 检索
- en: The system performs similarity search using techniques such as k-nearest neighbors
    (k-NN), which finds the most similar data points, or cosine similarity, which
    measures how closely the direction of two vectors aligns. This process locates
    the chunks in the vector database that best match the user’s prompt.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 系统使用诸如k-最近邻（k-NN）等技术进行相似性搜索，这些技术找到最相似的数据点，或者余弦相似度，它衡量两个向量的方向如何接近。这个过程定位了与用户提示最佳匹配的向量数据库中的块。
- en: Augmentation
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 扩增
- en: The retrieved relevant information is combined with the original user prompt
    to create an augmented prompt that contains both the user’s question and the contextual
    information needed to answer it.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 从向量数据库检索到的相关信息与原始用户提示结合，创建一个包含用户问题和回答所需上下文的扩展提示。
- en: Response
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 响应
- en: The augmented prompt is submitted to the LLM for processing. The LLM generates
    a response that reflects both the user’s original prompt and the relevant chunks
    of information retrieved from the vector database. This should result in a more
    informed and accurate answer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展后的提示被提交给LLM进行处理。LLM生成一个反映用户原始提示和从向量数据库中检索到的相关信息片段的响应。这应该导致一个更加信息和准确的答案。
- en: Keep in mind that AWS offers numerous options for vector database capabilities.
    Examples include Amazon OpenSearch Service, Amazon OpenSearch Serverless, and
    Amazon Kendra.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，AWS提供了多种向量数据库功能选项。例如，包括Amazon OpenSearch Service、Amazon OpenSearch Serverless和Amazon
    Kendra。
- en: There is also vector databases that use pgvector (this is for Amazon RDS and
    Amazon Aurora PostgreSQL-Compatible Edition). This is an extension for PostgreSQL,
    which is a popular open source database. Pgvector allows for storing, indexing,
    and querying of high-dimensional vector data. There are also enterprise features,
    such as atomicity, consistency, isolation, durability (ACID) compliance (which
    helps to provide for reliable transactions), point-in-time recovery, and support
    for complex queries.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 还有使用pgvector的向量数据库（这是针对Amazon RDS和Amazon Aurora PostgreSQL兼容版）。这是一个针对PostgreSQL的扩展，PostgreSQL是一个流行的开源数据库。Pgvector允许存储、索引和查询高维向量数据。还有企业级功能，如原子性、一致性、隔离性、持久性（ACID）合规性（有助于提供可靠的交易）、时间点恢复和复杂查询的支持。
- en: 'RAG has seen significant adoption. According to a survey from [451 Research](https://oreil.ly/Ld7Wg),
    about 87% of the respondents said that they consider this method to be an effective
    approach for customization. Yet RAG has some disadvantages:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: RAG得到了广泛的应用。根据[451 Research](https://oreil.ly/Ld7Wg)的调查，大约87%的受访者表示他们认为这是一种有效的定制方法。然而，RAG也有一些缺点：
- en: Data
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 数据
- en: An organization may not use enough relevant information. Or they may choose
    the wrong sources. Creating a useful RAG system usually requires data science
    expertise.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一个组织可能没有使用足够的相关信息。或者他们可能选择了错误的信息来源。创建一个有用的RAG系统通常需要数据科学专业知识。
- en: Search limitations
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索限制
- en: Semantic search may have a good match, but it can sometimes miss the overall
    context of the information.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索可能有一个好的匹配，但它有时会错过信息的整体上下文。
- en: Chunking problems
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 分块问题
- en: The chunking process can be delicate. It’s common to make inadequate divisions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 块状处理过程可能很微妙。常见的错误是划分不足。
- en: Evaluation
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: With an FM—whether it is fine-tuned or uses RAG—you will need to evaluate it.
    Is it performing properly? Are the responses accurate? Are there hallucinations?
    Is there harmful or toxic content generated?
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FM——无论是微调还是使用RAG——您需要对其进行评估。它是否表现正常？响应是否准确？是否存在幻觉？是否生成了有害或有毒内容？
- en: The evaluation process is complex and time-consuming. But it is critical, in
    terms of building trust with users and effectively solving business problems.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 评估过程复杂且耗时。但从与用户建立信任和有效解决业务问题的角度来看，它是至关重要的。
- en: 'There are three main ways to evaluate an FM:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一个FM主要有三种方式：
- en: Human evaluation
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工评估
- en: Benchmark datasets
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准数据集
- en: Standard evaluation metrics
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准评估指标
- en: Human evaluation
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工评估
- en: 'Human evaluation is essential because it helps assess aspects of a model that
    automated metrics may miss, such as user experience, creativity, and ethical behavior.
    These areas are often subjective and require nuanced human judgment. Human evaluation
    typically focuses on several key areas:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 人工评估是必不可少的，因为它有助于评估自动化指标可能遗漏的模型方面，例如用户体验、创造力和道德行为。这些领域往往是主观的，需要细微的人类判断。人工评估通常关注几个关键领域：
- en: User experience
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 用户体验
- en: 'How natural, intuitive, or satisfying is the interaction? One common metric
    is the Net Promoter Score (NPS), which asks, how likely are you to recommend this
    product or service to others?—rated on a scale from 0 to 10\. Participants may
    also be asked: Was the response easy to understand? Did it feel conversational?'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 交互有多自然、直观或令人满意？一个常见的指标是净推荐值（NPS），它询问，你有多可能向他人推荐这个产品或服务？——评分范围从0到10。参与者还可能被问到：响应是否容易理解？是否感觉像对话？
- en: Contextual appropriateness
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文适宜性
- en: Does the model stay on topic and provide responses that make sense in the given
    context? Reviewers might consider questions like, did the model understand the
    question? Did it refer to previous parts of the conversation accurately?
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是否保持主题并给出在给定上下文中有意义的回答？审阅者可能会考虑像模型是否理解了问题？它是否准确引用了对话的先前部分等问题。
- en: Creativity and flexibility
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 创造力和灵活性
- en: Are the responses varied and interesting—or repetitive and dull? Evaluators
    can assess whether the model provides diverse outputs when asked to generate content
    or brainstorm ideas.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是否多样化且有趣——或者重复且乏味？当要求生成内容或头脑风暴想法时，评估者可以评估模型是否提供多样化的输出。
- en: Ethical considerations
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理考量
- en: Are there signs of bias, harmful outputs, or inappropriate content? Human reviewers
    play a critical role in spotting these issues that may slip past automated filters.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有偏见、有害输出或不适当内容的表现？人类审阅者在发现可能被自动化过滤器遗漏的问题中扮演着关键角色。
- en: Emotional intelligence
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 情绪智力
- en: 'Can the FM detect and respond appropriately to emotional tones, such as frustration,
    excitement, or sadness? Questions may include: Did the model acknowledge the user’s
    emotions? Did it respond in a sensitive manner?'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: FM能否检测并适当地响应情绪调调，如挫败感、兴奋或悲伤？问题可能包括：模型是否承认了用户的情绪？它是否以敏感的方式做出回应？
- en: 'Human evaluations are usually conducted through panels or focus groups, which
    may range from a handful of participants to over 100\. Evaluators are often selected
    based on criteria such as:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 人工评估通常通过小组或焦点小组进行，参与者数量可能从几人到超过100人。评估者通常根据以下标准进行选择：
- en: Target audience (e.g., specific industry professionals)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标受众（例如，特定行业专业人士）
- en: Diverse backgrounds (e.g., race, gender, culture)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多样化的背景（例如，种族、性别、文化）
- en: Experience level (from novice users to experts)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经验水平（从新手用户到专家）
- en: Participants typically spend time prompting the FM, reviewing its outputs, and
    providing detailed feedback. This is often followed by surveys or structured interviews.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者通常会花费时间提示FM，审查其输出，并提供详细的反馈。这通常随后是调查或结构化访谈。
- en: There’s also a form of passive human evaluation that’s built directly into many
    FMs. For example, ChatGPT allows users to give a thumbs up or down after a response.
    Some platforms include embedded feedback forms or prompt users to rate helpfulness.
    This type of real-time, user-driven feedback is valuable for tracking long-term
    trends and improving future versions of the model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 许多FM还直接集成了被动的人工评估形式。例如，ChatGPT允许用户在响应后给出点赞或踩不点赞。一些平台包括嵌入的反馈表或提示用户对有用性进行评分。这种实时、用户驱动的反馈对于跟踪长期趋势和改进模型的未来版本非常有价值。
- en: Benchmark datasets
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基准数据集
- en: 'A benchmark dataset allows for making a quantitative evaluation of an FM. They
    help gauge:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基准数据集允许对FM进行定量评估。它们有助于衡量：
- en: Accuracy
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性
- en: Does the FM perform tasks based on certain agreed-upon standards?
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: FM是否基于某些达成一致的标准执行任务？
- en: Speed and efficiency
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 速度和效率
- en: How fast does the FM take to generate a response? How many resources does it
    use? For example, some FMs will need to work in real time, such as with self-driving
    cars.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: FM生成响应需要多快？它使用了多少资源？例如，一些FM需要实时工作，如自动驾驶汽车。
- en: Scalability
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性
- en: If the FM is serving heavy volumes, is the performance consistent?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果FM处理大量数据，其性能是否一致？
- en: Responsible AI
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的人工智能
- en: This evaluates the FM for factors like bias and fairness.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这评估了FM在偏差和公平性等方面的表现。
- en: Robustness
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 坚固性
- en: How does the FM perform when there are unusual or adversarial prompts?
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在异常或对抗性提示时，FM的表现如何？
- en: Generalization
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化
- en: This measures an FM’s ability to handle unseen data or tasks.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这衡量了FM处理未见数据或任务的能力。
- en: Creating a benchmark dataset can be challenging, as it usually takes the skills
    of an experienced data scientist. But there is also often the need to have one
    or more subject matter experts (SMEs) involved in the process. They will have
    the necessary experience for the domain that is being tested. For example, if
    a dataset benchmark is for drug discovery, then there will need to be SMEs who
    have a background in the pharmaceutical industry.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个基准数据集可能具有挑战性，因为这通常需要经验丰富的数据科学家的技能。但通常也需要有一名或多名领域专家（SMEs）参与这个过程。他们将具备正在测试的领域的必要经验。例如，如果数据集基准是针对药物发现，那么就需要有来自制药行业背景的SMEs。
- en: '[Figure 4-7](#figure_four_sevendot_process_for_develo) shows the steps to putting
    together a dataset benchmark.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-7](#figure_four_sevendot_process_for_develo)显示了创建数据集基准的步骤。'
- en: '![](assets/awsc_0407.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/awsc_0407.png)'
- en: Figure 4-7\. Process for developing dataset benchmarks
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7. 开发数据集基准的过程
- en: 'Let’s look at these steps in more detail:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些步骤：
- en: SMEs create relevant questions
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: SMEs创建相关的问题
- en: SMEs develop a comprehensive set of questions that are not only relevant to
    the domain but also challenging enough to truly evaluate the FM’s capabilities.
    These questions are designed to test the depth and breadth of the FM’s knowledge
    and reasoning abilities.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: SMEs开发了一套全面的、不仅与领域相关而且足够具有挑战性以真正评估FM能力的问答。这些问题旨在测试FM的知识深度和推理能力。
- en: SMEs create answers
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: SMEs创建答案
- en: The same SMEs conduct extensive research to provide high-quality reference answers
    to each question.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的领域专家（SMEs）进行广泛的研究，为每个问题提供高质量的参考答案。
- en: FM processing
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: FM处理
- en: The created questions are submitted to the FM being evaluated. The FM generates
    its own answers to each question, which will later be compared against the SME-created
    reference answers.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的问题提交给被评估的FM。FM为每个问题生成自己的答案，这些答案将随后与SME创建的参考答案进行比较。
- en: Judge model
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 评判模型
- en: An AI model serves as the judge, comparing the FM’s generated answers against
    the SME reference answers. The judge model evaluates the FM’s responses across
    multiple criteria, including accuracy, relevance, and comprehensiveness.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一个AI模型作为评判者，将FM生成的答案与SME参考答案进行比较。评判模型从多个标准评估FM的响应，包括准确性、相关性和全面性。
- en: Performance score
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 性能评分
- en: Based on the judge model’s comparative analysis, a final benchmark score is
    generated. This score provides a quantitative measure of the FM’s performance.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 根据评判模型的比较分析，生成一个最终的基准分数。这个分数提供了FM性能的定量度量。
- en: Standard evaluation metrics
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准评估指标
- en: 'A standard evaluation metric is a widely used measurement to assess the performance
    of an FM. There are many available. But for the purposes of the AIF-C01 exam,
    these are the ones that you need to know:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 标准评估指标是一个广泛使用的测量方法，用于评估FM的性能。有许多可供选择。但为了AIF-C01考试的目的，这些是需要你了解的：
- en: Recall-Oriented Understudy for Gisting Evaluation (ROUGE)
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于摘要评估的召回辅助（ROUGE）
- en: Bilingual evaluation understudy (BLEU)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双语评估助手（BLEU）
- en: Bidirectional encoder representations from transformers score (BERTScore)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自转换器的双向编码器表示分数（BERTScore）
- en: ROUGE
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ROUGE
- en: ROUGE is a collection of metrics to evaluate automatic summarization of text
    and machine translation, such as for foreign languages. It compares the overlaps
    of the content generated with an LLM to reference summaries, which are usually
    created by humans.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE是一组用于评估文本自动摘要和机器翻译（如外语）的度量标准。它比较使用LLM生成的内容与由人类创建的参考摘要的重叠部分。
- en: There are different types of ROUGE metrics, and each one measures similarity
    between machine-generated text and reference text in a specific way. One key category
    is ROUGE-N, which focuses on n-gram overlap. An *n*-gram is a sequence of *n*
    consecutive words.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同类型的ROUGE度量标准，每一种都以特定方式衡量机器生成文本与参考文本之间的相似性。一个关键类别是ROUGE-N，它侧重于n-gram重叠。一个*n*-gram是由*n*个连续单词组成的序列。
- en: 'For example, ROUGE-1 evaluates unigram (single word) overlap:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，ROUGE-1评估单语（单词）重叠：
- en: 'Sentence: “The car stopped suddenly”'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子：“汽车突然停下”
- en: 'Unigrams: “The,” “car,” “stopped,” “suddenly”'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单语：“The”，“car”，“stopped”，“suddenly”
- en: 'Then ROUGE-2 evaluates bigram (two-word) overlap:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，ROUGE-2评估双语（两个单词）重叠：
- en: 'Bigrams: “The car,” “car stopped,” “stopped suddenly”'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双语：“The car”，“car stopped”，“stopped suddenly”
- en: You can continue with ROUGE-3, ROUGE-4, etc., to evaluate longer phrase matches.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续使用ROUGE-3、ROUGE-4等，以评估更长的短语匹配。
- en: 'Why use different *n* values? Because they give you different perspectives:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用不同的*n*值？因为它们提供了不同的视角：
- en: ROUGE-1 captures basic word usage.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROUGE-1捕捉基本的单词使用。
- en: ROUGE-2 and higher capture more structure, phrasing, and fluency.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROUGE-2及以上版本捕捉到更多的结构、措辞和流畅性。
- en: These distinctions help break down what kind of similarity the model is capturing—are
    the right words there? Are they in the right order? This makes evaluation more
    granular and insightful.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这些区别有助于分析模型捕捉到的相似性类型——是否有正确的单词？它们是否按正确的顺序排列？这使得评估更加细致和有洞察力。
- en: 'For interpreting ROUGE-N scores, the range is 0 to 1:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解释ROUGE-N分数，范围是0到1：
- en: Good score
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 好评
- en: Typically, a score above 0.5 (50%) is considered strong, especially for ROUGE-1\.
    However, what’s “good” depends on the task.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，分数高于0.5（50%）被认为是强的，特别是对于ROUGE-1。然而，“好”取决于任务。
- en: Bad score
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 差评
- en: A score near 0 means very little overlap, suggesting the model didn’t capture
    important content.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 分数接近0意味着重叠非常少，这表明模型没有捕捉到重要内容。
- en: In short, ROUGE-N helps you assess how well a model captures the key words and
    phrases a human would expect—step-by-step, from simple terms to full phrasing.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，ROUGE-N帮助你评估模型捕捉人类预期关键词和短语的能力——从简单术语到完整措辞的逐步过程。
- en: Next, there is ROUGE-L (longest common subsequence), which is a metric used
    to evaluate the similarity between a machine-generated text and a human-written
    reference by measuring the longest sequence of words that appears in both texts
    in the same order, though not necessarily consecutively. It is especially useful
    for evaluating long-form content like summaries or narratives, where exact word
    matches might be less important than preserving the structure and meaning of the
    original.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是ROUGE-L（最长公共子序列），这是一个用于评估机器生成文本与人工撰写参考文本之间相似性的度量标准，通过测量两个文本中出现的最长单词序列，这些单词序列在两个文本中以相同的顺序出现，尽管不一定连续。它特别适用于评估长篇内容，如摘要或叙述，在这些内容中，精确的单词匹配可能不如保留原文的结构和意义重要。
- en: 'Let’s take an example. Suppose we have this reference that is human written:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个例子。假设我们有一个由人类撰写的参考文本：
- en: The quick brown fox jumps over the lazy dog.
  id: totrans-280
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 快速的棕色狐狸跳过懒狗。
- en: 'This is what the model generated:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型生成的结果：
- en: The brown fox quickly jumps over a lazy dog.
  id: totrans-282
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 棕色狐狸迅速跳过懒狗。
- en: 'Let’s find the longest common subsequence (LCS)—words that appear in both texts,
    in the same order:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找到最长公共子序列（LCS）——在两个文本中出现的单词，以相同的顺序：
- en: 'Common words: “The,” “brown,” “fox,” “jumps,” “over,” “lazy,” “dog”'
  id: totrans-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 常见单词：“The”，“brown”，“fox”，“jumps”，“over”，“lazy”，“dog”
- en: 'Then this is the LCS:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这是LCS：
- en: The brown fox jumps over lazy dog.
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 红色狐狸跳过懒狗。
- en: This is a strong structural match even though some words differ slightly (*quickly*
    versus *quick*, or missing *the*).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有些单词略有不同（*quickly*与*quick*，或缺少*the*），这也是一个强大的结构匹配。
- en: 'In terms of the score:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在分数方面：
- en: Scores above 0.6 are generally good, especially for complex narratives.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分数高于0.6通常被认为是好的，特别是对于复杂的叙述。
- en: Scores in the 0.3–0.5 range may indicate decent content, but poorer structure
    or coherence.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0.3–0.5范围内的分数可能表明内容尚可，但结构或连贯性较差。
- en: A key benefit of ROUGE is that it is fairly simple, straightforward, and based
    on human judgment. But it is also an effective metric in measuring similarity.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE的一个关键优势是它相当简单、直接，并且基于人类判断。但它在衡量相似度方面也是一个有效的指标。
- en: BLEU
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BLEU
- en: BLEU is a metric used to evaluate the quality of machine-generated text by comparing
    it to human-written reference text. The closer the match, the better the quality—especially
    in translation tasks. BLEU scores range from 0 to 1, with 1 indicating a perfect
    match.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU是一种通过将其与人类编写的参考文本进行比较来评估机器生成文本质量的指标。匹配越接近，质量越好——特别是在翻译任务中。BLEU分数范围从0到1，1表示完美匹配。
- en: While BLEU is similar to ROUGE in that both rely on *n*-gram analysis, there
    are key differences. BLEU focuses on precision, measuring how many of the generated
    *n*-grams appear in the reference and averaging them. It also penalizes shorter
    outputs through a “brevity penalty,” ensuring translations aren’t overly concise
    just to match key terms.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然BLEU与ROUGE相似，因为两者都依赖于*n*-gram分析，但它们之间存在关键差异。BLEU侧重于精确度，衡量生成的*n*-gram中有多少出现在参考文本中，并取平均值。它还通过“简洁度惩罚”惩罚较短的输出，确保翻译不会过于简洁以匹配关键词。
- en: Introduced in the early 2000s, BLEU was one of the first automatic evaluation
    metrics for machine translation and remains widely used for its effectiveness
    and simplicity.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 2000年代初引入的BLEU是机器翻译的第一个自动评估指标之一，由于其有效性和简单性，至今仍被广泛使用。
- en: BERTScore
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BERTScore
- en: In 2018, researchers published a paper^([8](ch04.html#ch01fn9)) in which they
    created a new model called BERT, representing a major breakthrough for NLP. Google
    would eventually open source it. The result was that BERT became quite popular
    in the AI community, spawning variations on the model like RoBERTa and DistilBERT.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，研究人员发表了一篇论文^([8](ch04.html#ch01fn9))，在其中他们创建了一个名为BERT的新模型，这代表了自然语言处理领域的一个重大突破。谷歌最终将其开源。结果是BERT在人工智能社区中变得非常流行，产生了像RoBERTa和DistilBERT这样的模型变体。
- en: 'BERT also laid the foundation for a new evaluation metric: BERTScore. Unlike
    BLEU and ROUGE, which rely on exact or partial *n*-gram matches, BERTScore evaluates
    the semantic similarity between generated text and reference text. That is, it
    doesn’t just look at whether the same words appear—it checks whether the meaning
    is preserved. This is made possible through semantic search, a technique that
    uses vector embeddings to compare the meanings of words or sentences rather than
    their surface forms. For example, if a reference sentence says, “The cat sat on
    the mat,” and the generated version says, “A feline rested on a rug,” traditional
    metrics might score this poorly due to word mismatch. But BERTScore could recognize
    that the meaning is nearly identical.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: BERT也为一个新的评估指标奠定了基础：BERTScore。与依赖于精确或部分*n*-gram匹配的BLEU和ROUGE不同，BERTScore评估生成文本与参考文本之间的语义相似度。也就是说，它不仅看是否出现了相同的词语，还检查意义是否得到保留。这是通过语义搜索实现的，这是一种使用向量嵌入来比较词语或句子意义而不是其表面形式的技巧。例如，如果参考句子说，“The
    cat sat on the mat”，而生成版本说，“A feline rested on a rug”，传统指标可能会因为词语不匹配而给出较差的评分。但BERTScore可以识别出意义几乎相同。
- en: BERTScore offers a more nuanced view of text quality, which is especially useful
    when evaluating LLM outputs that may use different wording but convey the same
    idea. That said, it’s not meant to replace BLEU or ROUGE. In practice, data scientists
    often use multiple metrics together to get a fuller picture of model performance.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: BERTScore提供了对文本质量的更细致的视角，这在评估可能使用不同措辞但传达相同思想的LLM输出时特别有用。尽管如此，它并不是要取代BLEU或ROUGE。在实践中，数据科学家经常使用多个指标一起使用，以获得模型性能的更全面图景。
- en: 'Resource for benchmark metrics: Hugging Face'
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准指标资源：Hugging Face
- en: In 2016, Clément Delangue, Julien Chaumond, and Thomas Wolf launched Hugging
    Face (*huggingface.co*). They first developed a chatbot that allowed teens to
    interact with an AI pal. The startup failed to get traction. But the founders
    did not give up. When building their chatbot, they saw that there was a need to
    have a hub for open source AI models and applications.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，Clément Delangue、Julien Chaumond和Thomas Wolf创建了Hugging Face (*huggingface.co*)。他们最初开发了一个聊天机器人，允许青少年与人工智能伙伴互动。该初创公司未能获得成功。但创始人并没有放弃。在构建他们的聊天机器人时，他们发现需要一个开源人工智能模型和应用的枢纽。
- en: Today, Hugging Face is the place that many AI people go. There are over 1.4
    million AI models that you can download, and there are over 318,000 datasets.
    For all the AI models, there are detailed profiles, which include documentation,
    code samples, use cases, license information, and limitations. There are also
    benchmark metrics, which will often have comparisons to other models.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Hugging Face是许多AI人士会去的地方。这里有超过140万个可下载的AI模型，以及超过31.8万个数据集。对于所有AI模型，都有详细的简介，包括文档、代码示例、用例、许可信息和限制。还有基准指标，通常会有与其他模型的比较。
- en: Issues with benchmark metrics
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准指标的问题
- en: 'The use of metrics can be a controversial topic. There has been growing concern
    that they are not particularly effective or may be misleading. Part of the reason
    is that LLMs are highly complex and their inner workings may not be disclosed.
    Next, the LLM developers themselves are often the ones who compute the metrics,
    such as by publishing a blog or a white paper. But there are other issues:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 指标的使用可能是一个有争议的话题。人们越来越担心它们并不特别有效，或者可能是误导性的。部分原因是LLM非常复杂，其内部工作原理可能不会公开。其次，LLM的开发者本身往往是计算指标的人，例如通过发布博客或白皮书。但还有其他问题：
- en: Prompts
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Even a small change in the wording can have a major impact on the results for
    a metric.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是措辞上的微小变化也可能对指标的成果产生重大影响。
- en: Copying
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 复制
- en: The dataset for the metrics may actually be part of the training for an LLM.
    In a way, this is like when a student cheats on an exam.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 用于指标的数据库实际上可能是LLM训练的一部分。从某种意义上说，这就像学生考试作弊一样。
- en: Real-world application
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界应用
- en: Tests generally are focused on more theoretical aspects of an LLM. It may not
    pick up on real-world situations.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 测试通常更关注LLM的理论方面。它可能不会注意到现实世界的情况。
- en: Narrowness
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 狭隘性
- en: An evaluation metric will usually focus on one task or category.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标通常会关注一个任务或类别。
- en: Edge cases
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘情况
- en: It’s common for evaluation metrics to focus mostly on common use cases.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标通常主要关注常见用例。
- en: To help address these problems, there have emerged platforms that rank LLMs,
    such as Chatbot Arena. UC Berkeley roommates—Anastasios Angelopoulos and Wei-Lin
    Chiang—launched it in 2023\. What started as a school project has turned into
    one of the most popular destinations for data scientists and AI companies.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助解决这些问题，已经出现了对LLM进行排名的平台，例如Chatbot Arena。加州大学伯克利分校的室友——Anastasios Angelopoulos和Wei-Lin
    Chiang——在2023年启动了它。最初只是一个学校项目，现在已经变成了数据科学家和AI公司最受欢迎的目的地之一。
- en: The system uses a simple form where users ask a question and there are responses
    from two anonymous LLMs. They will rate which is better. Currently, there are
    over [170 models](https://oreil.ly/GojrE) on the platform, and they have logged
    more than two million votes.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统使用一个简单的形式，用户提出问题，有两个匿名的LLM提供回答。他们将评估哪个更好。目前，该平台上已有超过[170个模型](https://oreil.ly/GojrE)，并且已经记录了超过两百万票。
- en: Deployment
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署
- en: After an LLM meets the necessary performance criteria, it’s time to put it into
    production. This can mean that the model will be placed into an application or
    made available as an API. If the model is open source, it can be posted on a platform
    like Hugging Face or Grok.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 当LLM达到必要的性能标准后，就是将其投入生产的时候了。这可能意味着模型将被放入应用程序或作为API提供。如果模型是开源的，它可以在Hugging Face或Grok等平台上发布。
- en: As with any AI model, there should be ongoing monitoring and tracking, such
    as for accuracy, latency, and performance. There should also be evaluation of
    bias, energy usage, security, and potential toxic content. All this data is collected
    for the next model. For example, it’s typical for there to be minor updates every
    couple months. As for major upgrades, these may happen every six months to a year.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何AI模型一样，应该持续监控和跟踪，例如准确性、延迟和性能。还应该评估偏差、能源消耗、安全性和潜在的有害内容。所有这些数据都是为了下一个模型收集的。例如，通常每两个月会有小更新。至于重大升级，这些可能每六个月到一年发生一次。
- en: Capabilities of Generative AI
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式AI的能力
- en: Generative AI is a powerful technology. But there are certain areas where it
    performs exceptionally well. It’s important to know about them when implementing
    this technology, which will allow for better results.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI是一项强大的技术。但它在某些领域表现出色。在实施这项技术时了解这些领域非常重要，这将有助于获得更好的结果。
- en: Perhaps one of the biggest capabilities of generative AI is that it can automate
    tedious activities. True, a person can summarize a long document. But an FM can
    do this in a few seconds—and often with higher accuracy. What this means is that
    the AI can free up time for people to work on more important tasks.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能可能的最大能力之一是它可以自动化繁琐的活动。诚然，一个人可以总结一份长文档。但一个FM可以在几秒钟内完成——并且通常具有更高的准确性。这意味着AI可以为人们腾出时间来处理更重要的事情。
- en: 'For the exam, here are some of the other capabilities to understand about generative
    AI:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 对于考试，以下是一些您需要了解的生成式人工智能的其他能力：
- en: Adaptability
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 适应性
- en: An FM can span many domains. This is one of the most important advantages of
    generative AI. For businesses, this means that—instead of relying on multiple
    applications—there can be just one.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 一个FM可以跨越许多领域。这是生成式人工智能最重要的优势之一。对于企业来说，这意味着——而不是依赖多个应用程序——只需一个即可。
- en: Responsiveness
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 响应性
- en: Generative AI usually can generate responses in near real time, which is critical
    for applications like chatbots.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能通常可以几乎实时地生成响应，这对于聊天机器人等应用至关重要。
- en: Simplicity
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 简单性
- en: With a prompt or two, you can generate humanlike content, say a blog, memo,
    or email.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一两个提示，您可以生成类似人类的内容，比如博客、备忘录或电子邮件。
- en: Data efficiency
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 数据效率
- en: You can help an FM learn using a few pieces of data.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用少量数据帮助FM学习。
- en: Personalization
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化
- en: The responses can be tailored to your preferences. This can be automated based
    on prior interactions with the AI application.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 响应可以根据您的偏好进行定制。这可以根据与人工智能应用的先前互动进行自动化。
- en: Scalability
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Generative AI can process large amounts of data. Depending on the model, it
    can be as large as books.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能可以处理大量数据。根据模型的不同，它可以大如书籍。
- en: In fact, an FM can even exhibit creativity, allowing for sparking ideas. A key
    reason for this is that the model leverages huge amounts of data and is based
    on probabilistic relationships.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，一个FM甚至可以表现出创造力，激发新想法。这其中的一个关键原因是模型利用了大量的数据，并基于概率关系。
- en: 'Consider a case study from a Wharton School MBA innovation course. Professors
    asked students to come up with a dozen ideas for new products or services. ChatGPT
    generated its own ideas, which included a dorm room chef kit and a collapsible
    laundry hamper. The professors then had an independent online panel evaluate the
    ideas and the results were startling. For those that were judged to be good ideas,
    40% were from the students and 47% came from ChatGPT (the rest were neutral).^([9](ch04.html#ch01fn10))
    Moreover, of the 40 best ideas, only five came from the students. According to
    the professors:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下沃顿商学院MBA创新课程的一个案例研究。教授们要求学生提出一打新产品或服务的想法。ChatGPT生成了自己的想法，包括宿舍厨师套件和可折叠洗衣篮。然后教授们让一个独立的在线小组评估这些想法，结果令人震惊。对于被认为是有好想法的，40%来自学生，47%来自ChatGPT（其余的是中立的）.^([9](ch04.html#ch01fn10))
    此外，在40个最佳想法中，只有五个来自学生。根据教授的说法：
- en: We predict such a human-machine collaboration will deliver better products and
    services to the market, and improved solutions for whatever society needs in the
    future.
  id: totrans-338
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们预测这种人机协作将向市场提供更好的产品和服务，并为未来社会所需解决的问题提供改进的解决方案。
- en: Drawbacks of Generative AI
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式人工智能的缺点
- en: After the launch of ChatGPT, it would not take long for issues to arise. By
    March 2023, technology leaders and researchers wrote an open letter expressing
    fears that generative AI could lead to major disruptions, like job displacement,
    disinformation, and loss of control of critical systems.^([10](ch04.html#ch01fn11))
    The letter called for the pause—for six months—of the development of FMs that
    were more powerful than OpenAI’s GPT-4 (at the time, this was the company’s top
    model). Some of the notable signatories were Steve Wozniak, Yuval Noah Harari,
    and Elon Musk.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在ChatGPT发布后，问题不会很快出现。到2023年3月，技术领袖和研究人员撰写了一封公开信，表达了对生成式人工智能可能导致重大破坏的担忧，如就业流失、虚假信息和关键系统失控.^([10](ch04.html#ch01fn11))
    信中呼吁暂停——六个月——开发比OpenAI的GPT-4（当时这是公司的顶级模型）更强大的FM。一些值得注意的签署者包括史蒂夫·沃兹尼亚克、尤瓦尔·赫拉利和埃隆·马斯克。
- en: 'Of course, the AI industry did not pause development. Rather, the pace increased
    significantly. There were no major incidents or mishaps, but this does not imply
    that generative AI is not without considerable issues. Some of the main ones include:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，人工智能行业并没有停止发展。相反，发展速度显著加快。没有发生重大事件或事故，但这并不意味着生成式人工智能没有相当多的问题。其中一些主要问题包括：
- en: Hallucinations
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉
- en: Nondeterminism
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非确定性
- en: Interpretability
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性
- en: Data security and privacy
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据安全和隐私
- en: Social and branding risks
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社会和品牌风险
- en: Limited context windows
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有限的上下文窗口
- en: Recency
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近期性
- en: Costs
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本
- en: Data challenge
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据挑战
- en: Hallucinations
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幻觉
- en: A hallucination is where an FM creates a false or misleading response. Some
    of the reasons for this are datasets that do not have enough relevant information
    and the probabilistic nature of the generative AI. According to research from
    Vectara—which publishes a hallucination leaderboard on GitHub—the largest FMs
    from OpenAI, Google, and Anthropic show hallucination rates of anywhere from 2.5%
    to 8.5%.^([11](ch04.html#ch01fn12)) In some cases, it can be more than 15%.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉是指FM创建了一个虚假或误导性的响应。造成这种情况的一些原因包括数据集缺乏足够的相关信息和生成式AI的概率性质。根据来自Vectara的研究——该研究在GitHub上发布幻觉排行榜——来自OpenAI、Google和Anthropic的最大FM的幻觉率从2.5%到8.5%。^([11](ch04.html#ch01fn12))在某些情况下，它可能超过15%。
- en: A way to deal with this is to customize FMs, such as with fine-tuning and RAG.
    Another approach is to be more thoughtful with creating prompts, which we’ll learn
    about in the next chapter.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题的一种方法是定制FM，例如通过微调和RAG。另一种方法是更仔细地创建提示，我们将在下一章中学习。
- en: Some of the LLM providers have been integrating systems to help check the accuracy
    of the systems. This can involve doing a search of the internet to verify facts.
    Take ChatGPT. When you write a prompt, you can specify “Search.” This will access
    the internet for the response. It will also provide links to the references. ChatGPT
    also has a feature called *deep research*. This conducts a multistep research
    of a topic, such as by using more than 10 resources when generating a response.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 一些LLM（大型语言模型）提供商已经开始整合系统来帮助检查系统的准确性。这可能包括在互联网上搜索以验证事实。以ChatGPT为例。当你编写提示时，你可以指定“搜索”。这将访问互联网以获取响应。它还会提供参考链接。ChatGPT还有一个名为*深度研究*的功能。这会进行多步骤的主题研究，例如在生成响应时使用超过10个资源。
- en: Nondeterminism
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非确定性
- en: Nondeterminism is when an FM generates a different response even though the
    prompt is the same. This may not necessarily mean there are hallucinations. But
    the response may have a different sentence structure, emphasis on certain points,
    and not even mention certain topics.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 非确定性是指即使提示相同，FM（生成式模型）也会产生不同的响应。这并不一定意味着存在幻觉。但响应可能会有不同的句子结构，对某些点的强调，甚至不提及某些主题。
- en: A key reason for nondeterminism is the temperature of an FM, which is a setting
    for the randomness or “creativity” for the responses. For FMs like ChatGPT and
    Claude, you cannot set the temperature. Rather, it is an unknown value. But there
    are other FMs that allow you to do so. This is usually available to developers
    who use the APIs for these systems.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 非确定性的一个关键原因是FM（生成式模型）的温度，这是响应的随机性或“创造力”的设置。对于ChatGPT和Claude这样的FM，你不能设置温度。相反，它是一个未知值。但其他一些FM允许你这样做。这通常适用于使用这些系统API的开发者。
- en: Nondeterminism has its advantages, though, as we saw with our example with the
    Wharton School MBA innovation case study. But then again, it could add too much
    uncertainty for certain applications, especially where there are higher stakes.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管非确定性有其优点，正如我们在沃顿商学院MBA创新案例研究中看到的例子，但它也可能为某些应用增加过多的不确定性，尤其是在有更高风险的情况下。
- en: Interpretability
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性
- en: Interpretability describes the level of understanding, explainability, and trust
    with an FM. This can be fairly low because of the sheer complexity of these systems.
    And as we’ve mentioned earlier, most of the details of the model may not be disclosed.
    This is known as a *black box*.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性描述了与FM（生成式模型）的理解、可解释性和信任程度。这可以相当低，因为这些系统的复杂性本身就很复杂。而且，正如我们之前提到的，模型的许多细节可能不会公开。这被称为*黑盒*。
- en: This is even the case with open source models. It’s not uncommon for the FM
    developer to not disclose the weights and biases. This may also include the underlying
    datasets. The lack of interpretability can be difficult for industries that are
    highly regulated. In fact, they may even be prohibited from using a model that
    is not explainable.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这甚至适用于开源模型。FM开发者不公开权重和偏差的情况并不少见。这还可能包括底层数据集。缺乏可解释性对于高度监管的行业来说可能很困难。事实上，他们甚至可能被禁止使用不可解释的模型。
- en: Data Security and Privacy
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据安全和隐私
- en: Interacting with an FM can pose data security and privacy threats. If you enter
    sensitive information into a system—such as PII—this can be exposed. For some
    FMs, such as ChatGPT or Claude, the data is sent to the cloud. You will need to
    rely on their own security systems.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 与FMs交互可能会带来数据安全和隐私威胁。如果你将敏感信息输入到系统中——例如PII（个人身份信息）——这可能会被暴露。对于一些FMs，例如ChatGPT或Claude，数据会被发送到云端。你需要依赖它们自己的安全系统。
- en: 'It’s true that such large organizations have extensive guardrails. In the case
    of OpenAI, they include:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 诚然，这样的大型组织有广泛的防护措施。以OpenAI为例，它们包括：
- en: Data encryption
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加密
- en: This uses AES256 for data at rest—or in storage—and TLS 1.2 or higher for data
    in transit.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用AES256对静态或存储中的数据进行加密，并使用TLS 1.2或更高版本对传输中的数据进行加密。
- en: Internal security
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 内部安全
- en: There are rigorous access controls for authorized personnel. There are also
    advanced cybersecurity systems like firewalls and intrusion protection.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 对授权人员有严格的访问控制。还有像防火墙和入侵保护这样的高级网络安全系统。
- en: Audits
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 审计
- en: Open AI has been vetted by third-party evaluations, such as SOC 2 Type 2 audits.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI已经通过了第三方评估，如SOC 2 Type 2审计。
- en: Bounty program
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励计划
- en: OpenAI pays rewards for those who find vulnerabilities.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI为发现漏洞的人支付奖励。
- en: Safety and Security Committee
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 安全与安全委员会
- en: This is an independent group that evaluates the AI models and infrastructure.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个独立的小组，评估AI模型和基础设施。
- en: Despite all these measures, security is never foolproof. This is why you should
    be careful with what information you enter into an FM. Of course, the same goes
    for when you use datasets when applying customization techniques like RAG and
    fine-tuning. This may include approaches like anonymization of the data. There
    should also be strong cybersecurity protection systems and policies. This can
    help mitigate the risk of data poisoning, which is when a hacker breaches a dataset
    and injects malicious information into it. It can mean that a FM generates toxic
    content or could allow for a backdoor into the model.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管采取了所有这些措施，安全永远不能万无一失。这就是为什么你应该小心你输入到FMs中的信息。当然，当你使用数据集应用定制技术如RAG和微调时，也是如此。这可能包括数据匿名化等方法。还应该有强大的网络安全保护和政策。这有助于减轻数据中毒的风险，即黑客入侵数据集并向其中注入恶意信息。这可能意味着FMs生成有毒内容或可能允许对模型进行后门攻击。
- en: Social and Branding Risks
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社会和品牌风险
- en: In late 2023, a Chevrolet dealership rolled out a chatbot for its website. Unfortunately,
    some of the responses proved embarrassing.^([12](ch04.html#ch01fn13)) In some
    cases, the chatbot recommended cars for rivals. It even offered a Chevrolet Tahoe
    for just $1.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年底，一家雪佛兰经销商为其网站推出了一个聊天机器人。不幸的是，一些回应证明是尴尬的。[^([12](ch04.html#ch01fn13))]在某些情况下，聊天机器人推荐了竞争对手的汽车。它甚至以仅1美元的价格提供雪佛兰塔霍。
- en: This example was not a one-off. Generative AI does have social risks, which
    could damage a company’s brand and reputation. Customization techniques of an
    FM can certainly help. But in some cases, it may be best to not have a chatbot
    handle certain topics, which can be done by including filters in the chatbot.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子并非孤例。生成式AI确实存在社会风险，这可能会损害公司的品牌和声誉。FMs的定制技术当然可以帮助。但在某些情况下，最好不让聊天机器人处理某些话题，这可以通过在聊天机器人中包含过滤器来实现。
- en: Limited Context Windows
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有限上下文窗口
- en: A context window is the amount of text—expressed in tokens—a model can process
    at one time. For example, let’s say you are using a chatbot with a context window
    of 5,000 tokens. In your chat, the total number of tokens for the prompts and
    responses is 10,000\. This means that when generating a response, the chatbot
    will use the last 5,000 tokens. The rest will be ignored.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文窗口是模型一次可以处理的文本量——以token表示。例如，假设你正在使用一个上下文窗口为5,000 token的聊天机器人。在你的聊天中，提示和响应的总token数为10,000。这意味着当生成响应时，聊天机器人将使用最后5,000个token。其余的将被忽略。
- en: Over the years, the context windows for FMs have expanded greatly. You can find
    some examples in [Table 4-1](#table_four_onedot_context_windows).
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，FMs（功能模块）的上下文窗口得到了极大的扩展。你可以在[表4-1](#table_four_onedot_context_windows)中找到一些例子。
- en: Table 4-1\. Context windows
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1\. 上下文窗口
- en: '| Model | Size |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 |'
- en: '| --- | --- |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-4 Turbo | 128,000 tokens |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 Turbo | 128,000 tokens |'
- en: '| Meta Llama 3 | 128,000 tokens |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| Meta Llama 3 | 128,000 tokens |'
- en: '| Claude 3 Opus | 200,000 tokens |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Opus | 200,000 tokens |'
- en: '| Gemini 1.5 | 1,000,000 tokens |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| Gemini 1.5 | 1,000,000 tokens |'
- en: To put this into perspective, Gemini 1.5’s context window would handle about
    750,000 words or 2,500 pages. This would certainly be sufficient for many tasks.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地了解这一点，Gemini 1.5的上下文窗口可以处理大约750,000个单词或2,500页。这当然对于许多任务来说是足够的。
- en: But the context window may still not adequately capture the meaning of the text.
    The reason is the “lost-in-the-middle effect.” This describes how an FM can actually
    get lost when processing the information in the midsection of the tokens.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 但上下文窗口可能仍然不足以充分捕捉文本的意义。原因是“中间丢失效应”。这描述了FM在处理标记中间部分的信息时可能会迷失方向。
- en: Recency
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时效性
- en: 'FMs are pretrained, which means they are trained on datasets as of a certain
    cutoff date. But this presents a problem: recency. It means that more current
    information is not reflected in the FM, which can result in responses that are
    outdated. For example, if there has been a recent election, it may not know who
    the current president is.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: FM是预训练的，这意味着它们是在截至某个日期的数据集上训练的。但这带来一个问题：时效性。这意味着FM中不会反映更多当前的信息，这可能导致过时的响应。例如，如果最近有选举，它可能不知道现任总统是谁。
- en: But some of the leading FMs have a workaround that allows for internet queries,
    which we learned about earlier in this chapter.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 但一些领先的FM有一种解决方案，允许进行互联网查询，我们在这章的早期就了解到了这一点。
- en: Costs
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: 'The costs for building FMs can be huge. These are the main categories:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 构建FM的成本可能非常高。以下是主要类别：
- en: Training
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 训练
- en: You will need to hire a team of highly qualified data scientists, data engineers,
    and software engineers. Such skilled persons can fetch hefty salaries. In some
    cases, companies like OpenAI are offering seven-figure compensation packages for
    data scientists.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 您将需要雇佣一支由高素质的数据科学家、数据工程师和软件工程师组成的团队。这样的专业人士可以赚取高额的薪水。在某些情况下，像OpenAI这样的公司为数据科学家提供七位数的薪酬方案。
- en: Data
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 数据
- en: There are the costs for collecting and licensing of the data. For example, in
    2024 OpenAI announced a [five-year partnership for $250+ million](https://oreil.ly/wU-us)
    with News Corp for licensing of content from publications like the Wall Street
    Journal, Barron’s, the New York Post, and MarketWatch.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括收集和许可数据的成本。例如，2024年OpenAI宣布与新闻集团签订了价值2.5亿多美元的[五年合作协议](https://oreil.ly/wU-us)，用于许可《华尔街日报》、《巴伦周刊》、《纽约邮报》和《市场观察》等出版物的内容。
- en: Infrastructure
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施
- en: It can take thousands of GPUs to train an FM. To run an AI model at scale, some
    systems require over 100,000 GPUs. These chips are not cheap either, fetching
    $25,000 to $30,000 each. There may even be shortages of GPUs, requiring buying
    systems from a third party—at higher prices—or being put on a waitlist.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个FM可能需要成千上万个GPU。要大规模运行AI模型，一些系统需要超过10万个GPU。这些芯片也不便宜，每个售价在2.5万到3万美元之间。甚至可能存在GPU短缺，需要以更高的价格从第三方购买系统，或者被列入等待名单。
- en: Getting an estimate for the costs of an FM can be difficult, as the developers
    mostly keep this information private. But an interview with the CEO of Anthropic,
    Dario Amodei, does shed some light on this.^([13](ch04.html#ch01fn14)) He said
    that the training costs could be anywhere from $5 billion to $10 billion for the
    years 2025 to 2026\. Over the long term—which he did not specify—a model could
    cost a staggering $100 billion.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 获取FM成本估算可能很困难，因为开发者大多将此信息保密。但Anthropic公司首席执行官达里奥·阿莫迪的访谈对此有所透露.^([13](ch04.html#ch01fn14))他表示，2025年至2026年的训练成本可能在50亿到100亿美元之间。从长远来看——他没有具体说明——一个模型可能需要惊人的1000亿美元。
- en: Then there is the ambitious Stargate project,^([14](ch04.html#ch01fn15)) which
    is a joint venture between OpenAI, Oracle, and SoftBank. The goal is to raise
    $500 billion for building the infrastructure for next-generation AI models. The
    Stargate supercomputer system is expected to have 2 million GPUs and use one gigawatt
    of power each year.^([15](ch04.html#ch01fn16))
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是雄心勃勃的Stargate项目^([14](ch04.html#ch01fn15))，这是OpenAI、Oracle和SoftBank之间的合资企业。目标是筹集5000亿美元用于建设下一代AI模型的基础设施。预计Stargate超级计算机系统将拥有200万个GPU，每年消耗10亿瓦特的电力.^([15](ch04.html#ch01fn16))
- en: 'To get a sense of the sheer scale of state-of-the-art AI data centers, look
    at Meta. The company’s CEO and cofounder, Mark Zuckerberg, had this to say about
    its [latest project](https://oreil.ly/HW7OE):'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解最先进的人工智能数据中心规模之大，看看Meta公司就知道了。该公司的首席执行官和联合创始人马克·扎克伯格对其[最新项目](https://oreil.ly/HW7OE)有这样的说法：
- en: I announced last week that we expect to bring online almost a gigawatt of capacity
    this year. And we’re building a two-gigawatt and potentially bigger AI data center
    that is so big that it will cover a significant part of Manhattan if we were placed
    there.
  id: totrans-405
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我上周宣布，我们预计今年将上线近10吉瓦的产能。我们正在建设一个20吉瓦甚至更大的AI数据中心，如果将其放置在那里，它将覆盖曼哈顿的一个显著部分。
- en: 'With the escalating costs, there is a nagging question: Can AI be profitable?
    Many of the world’s top technology companies and venture capitalists think the
    answer is a clear yes.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 随着成本的上升，有一个令人烦恼的问题：AI能否盈利？世界上许多顶级科技公司和风险投资家认为答案是明确的肯定。
- en: 'In 2025, Google, Meta, Microsoft, and Amazon plan to spend at least $215 billion
    on the infrastructure for AI.^([16](ch04.html#ch01fn17)) This is what Amazon CEO,
    Andy Jassy, said about it: “We think virtually every application that we know
    of today is going to be reinvented with AI inside of it.”'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 到2025年，谷歌、Meta、微软和亚马逊计划至少在AI基础设施上投入至少2150亿美元。[16](ch04.html#ch01fn17)这是亚马逊CEO安迪·贾西对此的看法：“我们认为我们今天所知道的大多数应用程序都将通过AI进行重新发明。”
- en: But it’s still early days with AI and the pace of innovation has been dramatic.
    There will also likely be innovations and breakthroughs for more efficient and
    optimal approaches for creating and operating this technology.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 但AI仍处于早期阶段，创新的速度非常快。也可能会出现更多创新和突破，以更高效和更优化的方法来创建和运营这项技术。
- en: A notable example of this was from China. A 40-year-old Chinese billionaire
    investor launched his own AI startup, called DeepSeek. He believed there was an
    opportunity to disrupt the FM market by leveraging much better approaches to model
    development. With a fraction of the resources of companies like OpenAI—at least
    in terms of GPUs—his team was able to create a highly sophisticated model, called
    R1\. It proved quite capable based on a variety of benchmarks. It also apparently
    cost less than $6 million to develop.^([17](ch04.html#ch01fn18))
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显著的例子来自中国。一位40岁的中国亿万富翁投资者推出了自己的AI初创公司，名为DeepSeek。他相信通过利用更好的模型开发方法，有机会颠覆FM市场。与OpenAI等公司相比，他的团队在GPU资源方面仅占一小部分，却能创建一个高度复杂的模型，称为R1。基于各种基准测试，它证明相当有能力。它显然开发成本不到600万美元。[17](ch04.html#ch01fn18)
- en: Data Challenge
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据挑战
- en: Could we be running out of data for FMs? Well, some academic research predicts
    that this could be the case—at least in terms of quality data. If this turns out
    to be true, this would certainly represent a major issue for AI progress. If anything,
    there are already signs of problems. When new models are announced, they usually
    have minor improvements.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否正在耗尽FM的数据？好吧，一些学术研究预测，这可能是情况——至少在高质量数据方面。如果这被证明是正确的，这无疑将代表AI进步的一个重大问题。如果有什么的话，已经有问题的迹象。当宣布新的模型时，它们通常只有轻微的改进。
- en: But FM developers are looking at ways to deal with this problem, such as with
    synthetic data. This is data that is generally created by using AI models, which
    has proven helpful with use cases like self-driving cars. But the field of synthetic
    data is still in the early phases and there is much that needs to be done.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 但是FM开发者正在寻找解决这个问题的方法，例如使用合成数据。这种数据通常是通过使用AI模型创建的，这在自动驾驶汽车等用例中已被证明是有帮助的。但合成数据领域仍处于早期阶段，还有很多工作要做。
- en: 'Besides the potential of data scarcity, there is another looming issue: the
    proliferation of AI-generated content. A study from AWS estimates that it’s about
    57% of internet text.^([18](ch04.html#ch01fn19)) Some researchers think this could
    mean an adverse feedback loop for FMs, leading to model degradation or even collapse.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据稀缺的潜力外，还有一个迫在眉睫的问题：AI生成内容的激增。一项来自AWS的研究估计，这大约是互联网文本的57%。一些研究人员认为，这可能意味着FM的负面反馈循环，导致模型退化甚至崩溃。
- en: This is not to say that AI is doomed. Let’s face it, there have been many such
    predictions over the years—and yet the technology has continued to remain robust.
    But it does mean that the industry needs to be vigilant and continue to invest
    in ways for improvement.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是说AI注定要失败。让我们面对现实，多年来已经有过许多这样的预测——然而这项技术却继续保持着稳健。但这确实意味着该行业需要保持警惕，并继续投资于改进的方法。
- en: Evolution of FMs
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FM的演变
- en: Daniel Kahneman, famous psychologist, won the Nobel Prize in Economics in 2002\.
    This was for his pioneering research about human judgment under uncertainty. In
    fact, his ideas provide an interesting perspective on generative AI models. In
    his book *Thinking, Fast and Slow*, he set forth his ideas about the human thinking
    process. One is System 1 thinking, which is where a person thinks quickly and
    automatically. In a way, this is how GPT models operate. For example, with ChatGPT,
    it’s a quick interactive experience—with a prompt and response.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 丹尼尔·卡尼曼，著名心理学家，于2002年获得了诺贝尔经济学奖。这是为了他关于人类在不确定性下判断的开拓性研究。事实上，他的想法为生成式AI模型提供了一个有趣的视角。在他的著作《思考，快与慢》中，他阐述了自己关于人类思维过程的想法。其中一个是系统1思维，这是一个人快速且自动思考的地方。从某种意义上说，这就是GPT模型运作的方式。例如，使用ChatGPT，它提供了一种快速互动的体验——通过提示和响应。
- en: Next, there is System 2 thinking, which is when a person takes more time thinking
    about a problem, such as with sophisticated reasoning and planning. System 2 thinking
    applies to next-generation generative AI models. These are often referred to as
    reasoning models or agentic AI.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是系统2思维，这是一个人花更多时间去思考问题的时候，比如通过复杂的推理和计划。系统2思维适用于下一代生成式AI模型。这些模型通常被称为推理模型或代理AI。
- en: Besides taking a multistep approach to solving problems, these models also can
    act autonomously—or near autonomously—and also use tools to carry out tasks. There
    will often be multiple generative AI agents that will work collaboratively.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 除了采取多步骤解决问题的方法外，这些模型还可以自主或近乎自主地行动，并使用工具执行任务。通常会有多个生成式AI代理协同工作。
- en: 'Take an example for customer support. For this, we have the following agents:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 以客户支持为例。为此，我们有以下代理：
- en: Customer interaction agent
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 客户互动代理
- en: A chatbot that communicates with the customer can handle common questions, such
    as by invoking a knowledge base agent. But for more difficult matters, the agent
    will delegate these to other agents.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 与客户沟通的聊天机器人可以处理常见问题，例如通过调用知识库代理。但对于更复杂的问题，代理将把这些任务委托给其他代理。
- en: Issue categorization agent
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 问题分类代理
- en: This will evaluate the issue and determine what it is about, such as billing,
    technical issues, and so on.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 这将评估问题并确定其内容，例如账单、技术问题等。
- en: Sentiment analysis agent
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析代理
- en: If there are indications of frustration, then this can escalate the situation.
    This may mean handing off the customer to a human agent. This can be done with
    a task routing agent, which makes a determination based on factors like skill
    sets, experience, and workload.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有挫败感的迹象，这可能会升级情况。这可能意味着将客户转交给人类代理。这可以通过任务路由代理来完成，该代理根据技能、经验和工作量等因素做出决定。
- en: Resolution monitoring agent
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 决策监控代理
- en: This will track the progress of all the support tickets. If there are gaps or
    problems, then the agent will escalate the tasks.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 这将跟踪所有支持票证的进度。如果有差距或问题，代理将升级任务。
- en: This agentic technology is still in the early phases—but it is showing much
    progress. It’s a major priority for many of the largest technology firms like
    Amazon, Microsoft, and Salesforce.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这种代理技术仍处于早期阶段，但它取得了很大的进展。对于亚马逊、微软和Salesforce等许多最大的技术公司来说，这是一项主要优先事项。
- en: Consider that Gartner said that agentic AI is poised to be the biggest technology
    trend for 2025.^([19](ch04.html#ch01fn20)) This is backed up with other research,
    such as from Deloitte. The firm predicts that 25% of companies using generative
    AI will [launch agentic AI projects or proof of concepts](https://oreil.ly/LOVKT)
    in 2025\. The adoption rate is expected to reach 50% by 2027.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到Gartner表示，代理AI有望成为2025年最大的技术趋势。[19](ch04.html#ch01fn20) 这得到了其他研究机构的支持，例如德勤。该机构预测，到2025年，使用生成式AI的25%的公司将[启动代理AI项目或概念验证](https://oreil.ly/LOVKT)。预计到2027年，采用率将达到50%。
- en: AGI
  id: totrans-430
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AGI
- en: Earlier in this chapter, we mentioned the vision of OpenAI, which is about AGI.
    The company defined it as a system that is “smarter than humans.”
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早期，我们提到了OpenAI关于AGI的愿景。该公司将其定义为“比人类更聪明”的系统。
- en: But the topic of AGI is complicated. After all, the concept of “intelligence”
    is elusive, as there is no generally accepted standard. For example, a person
    does not have to have a genius-level IQ to be a genius.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 但AGI的话题很复杂。毕竟，“智能”这个概念很模糊，因为没有普遍接受的标准。例如，一个人不必拥有天才级的智商才能成为天才。
- en: Consider Richard Feynman. In World War II, he worked on the Manhattan Project,
    helping to develop the atomic bomb by solving complex neutron equations. After
    this, he would become a professor at the California Institute of Technology (Caltech).
    There, he would make pioneering contributions to quantum electrodynamics and win
    the Nobel Prize in Physics in 1965.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 以理查德·费曼为例。在二战期间，他参与了曼哈顿计划，通过解决复杂的中子方程式帮助开发原子弹。此后，他成为加州理工学院（Caltech）的教授。在那里，他对量子电动力学做出了开创性的贡献，并在1965年获得了诺贝尔物理学奖。
- en: So what was his IQ? It was [125](https://oreil.ly/o9HHc), which is average intelligence.
    By comparison, a genius level is 180+.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，他的智商是多少？是[125](https://oreil.ly/o9HHc)，这是平均智力。相比之下，天才的智商是180+。
- en: 'Then there are cases where people are geniuses and have fairly low IQs, such
    as Leslie Lemke. Even with an IQ of 58 and blind,^([20](ch04.html#ch01fn21)) he
    was able to play complex piano pieces—after hearing them only once. So what does
    intelligence mean for AGI? There are other characteristics that are important
    for superhuman capabilities:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 然后有一些情况，人们是天才，但智商却相对较低，例如莱斯利·莱姆克。即使智商只有58，并且失明，^([20](ch04.html#ch01fn21))他也能演奏复杂的钢琴曲——只需听一次。那么对于通用人工智能（AGI）来说，智力意味着什么？还有其他一些对于超人类能力很重要的特征：
- en: Efficiency
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 效率
- en: As we have seen in this chapter, it takes huge amounts of resources and energy
    for AI. But this is not practical for the proliferation of AGI. After all, the
    human brain is only about three pounds and consumes about 20 watts of power, or
    the amount for a dim light bulb.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中看到的，AI需要巨大的资源和能量。但这对于AGI的普及来说并不实用。毕竟，人脑只有大约三磅重，消耗大约20瓦特的电力，或者说相当于一盏昏暗灯泡的电力。
- en: Interact with the environment
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 与环境互动
- en: An AGI system should be able to have physical capabilities like sight, smell,
    and feel. This will make it much more useful. It will also allow for more learning.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 一个AGI系统应该能够拥有物理能力，如视觉、嗅觉和触觉。这将使其更加有用。它还将允许更多的学习。
- en: Autonomous
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 自主
- en: An AGI system must be able to make effective decisions on its own.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 一个AGI系统必须能够独立做出有效的决策。
- en: Creativity
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 创造力
- en: While generative AI has shown some capacity for this, it is far from the levels
    that we have seen from humans, say with Steve Jobs, Einstein, or Shakespeare.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然生成式人工智能在这方面已经显示出一些能力，但远远达不到我们人类所达到的水平，比如说史蒂夫·乔布斯、爱因斯坦或莎士比亚。
- en: There are various estimates on when AGI will be reached. Ray Kurzweil, who is
    a futurist, says this will happen by 2029.^([21](ch04.html#ch01fn22)) Sir Demis
    Hassabis, who is the CEO of Google DeepMind, is not as optimistic. He predicts
    AGI will be reached by 2034 or so.^([22](ch04.html#ch01fn23))
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 关于何时达到AGI，有各种各样的估计。未来学家雷·库兹韦尔说，这将在2029年实现.^([21](ch04.html#ch01fn22))谷歌DeepMind的首席执行官德米斯·哈萨比斯并不那么乐观。他预测AGI将在2034年左右实现.^([22](ch04.html#ch01fn23))
- en: It’s likely that there will need to be more major innovations to achieve AGI.
    It seems that the transformer model will not be enough. If anything, next-generation
    models will need to go beyond being prediction machines. There will also need
    to be new forms of chips and computer systems, like quantum computing.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能需要更多的重大创新才能实现AGI。看起来，变换器模型将不足以实现这一点。如果有什么的话，下一代模型将需要超越预测机器。还需要新的芯片和计算机系统形式，比如量子计算。
- en: Conclusion
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Generative AI is a key part of the AIF-C01 exam. This is why we did a deep
    dive—in this chapter—of this important technology. We looked at the core building
    blocks, like neural networks and deep learning. We also saw how generative AI
    has different forms: GANs, VAEs, diffusion models, and the transformer.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能是AIF-C01考试的关键部分。这就是为什么我们在本章中深入探讨了这项重要技术。我们研究了核心构建块，如神经网络和深度学习。我们还看到了生成式人工智能的不同形式：GANs、VAEs、扩散模型和变换器。
- en: Among these, the transformer is the one that is most prominent. So, we looked
    at the inner workings, as well as the lifecycle, how it is a part of FMs, and
    the pros and cons.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些特征中，变换器是最突出的。因此，我们研究了其内部工作原理以及生命周期，以及它是如何成为FM的一部分，以及其优缺点。
- en: In the next chapter, we will look at how to evaluate generative AI solutions
    as well as the various use cases.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何评估生成式人工智能解决方案以及各种用例。
- en: Quiz
  id: totrans-450
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测验
- en: To check your answers, please refer to the [“Chapter 4 Answer Key”](app02.html#answers_ch_4).
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查你的答案，请参阅[“第4章答案键”](app02.html#answers_ch_4)。
- en: Which generative AI model involves two neural networks that compete against
    each other?
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种生成式人工智能模型涉及两个相互竞争的神经网络？
- en: Variational autoencoder (VAE)
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）
- en: Transformer model
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformer 模型
- en: Generative adversarial network (GAN)
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成对抗网络 (GAN)
- en: Diffusion model
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩散模型
- en: What is the purpose of positional encoding for a transformer model?
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 位置编码对于 Transformer 模型的目的是什么？
- en: To enhance the performance of backpropagation
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了增强反向传播的性能
- en: To process words in the original order of the text
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了按文本原始顺序处理单词
- en: To improve the reliability of GPUs
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了提高 GPU 的可靠性
- en: To make AI models more cost-effective
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使 AI 模型更具成本效益
- en: What is a key advantage of retrieval-augmented generation (RAG) compared to
    the process of fine-tuning?
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与微调过程相比，检索增强生成（RAG）的关键优势是什么？
- en: RAG requires less energy.
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: RAG 需要更少的能量。
- en: It significantly increases the model’s speed.
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它显著提高了模型的速度。
- en: RAG mitigates bias in AI responses.
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: RAG 减少了 AI 响应中的偏差。
- en: It does not require the modification of a model’s internal weights.
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它不需要修改模型的内部权重。
- en: What is a key advantage of a transformer model over a recurrent neural network
    (RNN)?
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与循环神经网络（RNN）相比，Transformer 模型的关键优势是什么？
- en: Transformers require no labeled data.
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformers 不需要标记数据。
- en: Transformers can process entire datasets in parallel.
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformers 可以并行处理整个数据集。
- en: RNNs cannot process text.
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN 无法处理文本。
- en: Transformers do not require training.
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformers 不需要训练。
- en: Why does an AI model create hallucinations?
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么 AI 模型会创造幻觉？
- en: They rely on probabilistic predictions.
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们依赖于概率预测。
- en: They use GPUs, which can be unpredictable.
  id: totrans-474
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们使用 GPU，这可能是不可预测的。
- en: They are based on supervised learning.
  id: totrans-475
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们基于监督学习。
- en: They are only a problem with small models.
  id: totrans-476
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们仅在小模型中存在问题。
- en: Which part of a neural network allows for detecting patterns in a dataset?
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络中哪一部分允许检测数据集中的模式？
- en: Input layer
  id: totrans-478
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入层
- en: Output layer
  id: totrans-479
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出层
- en: Hidden layer
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Activation layer
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活层
- en: ^([1](ch04.html#ch01fn2-marker)) CERN, [“Large Hadron Collider Begins Third
    Run”](https://oreil.ly/eo_Ke), HPCwire (website), July 6, 2022.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#ch01fn2-marker)) CERN，[“大型强子对撞机开始第三次运行”](https://oreil.ly/eo_Ke)，HPCwire（网站），2022
    年 7 月 6 日。
- en: ^([2](ch04.html#ch01fn3-marker)) Diederik P. Kingma and Max Welling, [“Auto-Encoding
    Variational Bayes”](https://oreil.ly/Glgrm), arXiv, revised December 10, 2022.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#ch01fn3-marker)) Diederik P. Kingma 和 Max Welling，[“自动编码变分贝叶斯”](https://oreil.ly/Glgrm)，arXiv，2022
    年 12 月 10 日修订。
- en: ^([3](ch04.html#ch01fn4-marker)) Ashish Vaswani et al., [“Attention Is All You
    Need”](https://oreil.ly/AHoJR), arXiv, revised August 2, 2023.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.html#ch01fn4-marker)) Ashish Vaswani 等人，[“注意力即一切”](https://oreil.ly/AHoJR)，arXiv，2023
    年 8 月 2 日修订。
- en: ^([4](ch04.html#ch01fn5-marker)) Parmy Olson, [“Meet the $4 Billion AI Superstars
    That Google Lost”](https://oreil.ly/cWBBa), Bloomberg (website), July 13, 2023.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.html#ch01fn5-marker)) Parmy Olson，[“谷歌失去的 40 亿美元 AI 明星”](https://oreil.ly/cWBBa)，彭博社（网站），2023
    年 7 月 13 日。
- en: '^([5](ch04.html#ch01fn6-marker)) Jascha Sohl-Dickstein et al., [“Deep Unsupervised
    Learning Using Nonequilibrium Thermodynamics”](https://oreil.ly/IWAVQ), Proceedings
    of the 32nd International Conference on Machine Learning, JMLR: W&CP 37 (2015).'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '^([5](ch04.html#ch01fn6-marker)) Jascha Sohl-Dickstein 等人，[“使用非平衡热力学进行深度无监督学习”](https://oreil.ly/IWAVQ)，第
    32 届国际机器学习会议论文集，JMLR: W&CP 37 (2015).'
- en: ^([6](ch04.html#ch01fn7-marker)) Harry Booth, [“Patrick Lewis”](https://oreil.ly/8E_nA),
    *Time*, September 5, 2024.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.html#ch01fn7-marker)) Harry Booth，[“帕特里克·刘易斯”](https://oreil.ly/8E_nA)，《时代》杂志，2024
    年 9 月 5 日。
- en: ^([7](ch04.html#ch01fn8-marker)) Aleksandra Piktus et al., [“Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks”](https://oreil.ly/MoQye), Meta,
    Conference on Neural Information Processing Systems (NeurIPS), December 6, 2020.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.html#ch01fn8-marker)) Aleksandra Piktus 等人，[“用于知识密集型 NLP 任务的检索增强生成”](https://oreil.ly/MoQye)，Meta，神经信息处理系统会议（NeurIPS），2020
    年 12 月 6 日。
- en: '^([8](ch04.html#ch01fn9-marker)) Jacob Devlin et al., [“BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding”](https://oreil.ly/nfnkQ),
    preprint, arXiv, October 11, 2018.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch04.html#ch01fn9-marker)) Jacob Devlin 等人，[“BERT：用于语言理解的深度双向变换器预训练”](https://oreil.ly/nfnkQ)，预印本，arXiv，2018
    年 10 月 11 日。
- en: '^([9](ch04.html#ch01fn10-marker)) Christian Terwiesch and Karl Ulrich, [“M.B.A.
    Students vs. AI: Who Comes Up with More Innovative Ideas?”](https://oreil.ly/WlZg8),
    *Wall Street Journal*, September 9, 2023.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch04.html#ch01fn10-marker)) Christian Terwiesch 和 Karl Ulrich，[“MBA 学生与
    AI：谁提出更具创新性的想法？”](https://oreil.ly/WlZg8)，《华尔街日报》，2023 年 9 月 9 日。
- en: '^([10](ch04.html#ch01fn11-marker)) Future of Life Institute, [“Pause Giant
    AI Experiments: An Open Letter”](https://oreil.ly/FXRBX), March 22, 2023.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch04.html#ch01fn11-marker)) 生命未来研究所，[“暂停巨型 AI 实验：一封公开信”](https://oreil.ly/FXRBX)，2023
    年 3 月 22 日。
- en: '^([11](ch04.html#ch01fn12-marker)) Karen Emslie, [“LLM Hallucinations: A Bug
    or a Feature?”](https://oreil.ly/fA_7_), *Communications of the ACM*, May 23,
    2024.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch04.html#ch01fn12-marker)) Karen Emslie, [“LLM幻觉：是缺陷还是特性？”](https://oreil.ly/fA_7_),
    *ACM通讯*, 2024年5月23日。
- en: ^([12](ch04.html#ch01fn13-marker)) Ben Sherry, [“A Chevrolet Dealership Used
    ChatGPT for Customer Service and Learned That AI Isn’t Always on Your Side”](https://oreil.ly/ZvcJo),
    *Inc.*, December 18, 2023.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch04.html#ch01fn13-marker)) Ben Sherry, [“一家雪佛兰经销商使用ChatGPT进行客户服务，并了解到人工智能并不总是站在你这边”](https://oreil.ly/ZvcJo),
    *Inc.*，2023年12月18日。
- en: ^([13](ch04.html#ch01fn14-marker)) Erin Snodgrass, [“CEO of Anthropic…Says It
    Could Cost $10 billion to Train AI in 2 Years”](https://oreil.ly/Dc6cC) *Business
    Insider*, April 30, 2024.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch04.html#ch01fn14-marker)) Erin Snodgrass, [“Anthropic首席执行官表示，两年内训练人工智能可能需要100亿美元”](https://oreil.ly/Dc6cC)
    *商业内幕*, 2024年4月30日。
- en: '^([14](ch04.html#ch01fn15-marker)) Joanna Stern, [“OpenAI Hails $500 Billion
    Stargate Plan: ‘More Computer Leads to Better Models’”](https://oreil.ly/NxHco),
    *Wall Street Journal*, updated January 22, 2025.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch04.html#ch01fn15-marker)) Joanna Stern, [“OpenAI称赞5000亿美元的星门计划：‘更多的计算机导致更好的模型’”](https://oreil.ly/NxHco),
    *华尔街日报*, 更新于2025年1月22日。
- en: ^([15](ch04.html#ch01fn16-marker)) Jeremy Kahn, [“The $19.6 Billion Pivot”](https://oreil.ly/1vWk2)
    *Fortune*, February 25, 2025.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch04.html#ch01fn16-marker)) Jeremy Kahn, [“19.6亿美元的转型”](https://oreil.ly/1vWk2)
    *财富杂志*, 2025年2月25日。
- en: ^([16](ch04.html#ch01fn17-marker)) Nate Rattner, [“Tech Giants Double Down on
    Their Massive AI Spending”](https://oreil.ly/HCq32) *Wall Street Journal*, February
    6, 2025.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch04.html#ch01fn17-marker)) Nate Rattner, [“科技巨头加大在人工智能上的巨额投资”](https://oreil.ly/HCq32)
    *华尔街日报*, 2025年2月6日。
- en: ^([17](ch04.html#ch01fn18-marker)) Kif Leswing, [“Nvidia Calls China’s DeepSeek
    R1 Model ‘an Excellent AI Advancement’”](https://oreil.ly/t76q_), CNBC, January
    27, 2025.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch04.html#ch01fn18-marker)) Kif Leswing, [“英伟达称中国的DeepSeek R1模型是‘卓越的人工智能进步’”](https://oreil.ly/t76q_),
    美国消费者新闻与商业频道，2025年1月27日。
- en: ^([18](ch04.html#ch01fn19-marker)) Tor Constantino, [“Is AI Quietly Killing
    Itself—and the Internet?”](https://oreil.ly/p1p-x), *Forbes* Australia, September
    2, 2024.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch04.html#ch01fn19-marker)) Tor Constantino, [“人工智能是否正在悄无声息地毁灭自己——以及互联网？”](https://oreil.ly/p1p-x),
    *福布斯澳大利亚*, 2024年9月2日。
- en: ^([19](ch04.html#ch01fn20-marker)) David Ramel, [“Agentic AI Named Top Tech
    Trend for 2025”](https://oreil.ly/uFxHh), *Campus Technology*, October 23, 2024.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch04.html#ch01fn20-marker)) David Ramel, [“代理人工智能被评为2025年顶级技术趋势”](https://oreil.ly/uFxHh),
    *校园技术*, 2024年10月23日。
- en: ^([20](ch04.html#ch01fn21-marker)) Darold Treffert, [“Whatever Happened to Leslie
    Lemke”](https://oreil.ly/5Mr6I), *Scientific American* (blog), June 17, 2014.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch04.html#ch01fn21-marker)) Darold Treffert, [“莱斯利·莱姆克发生了什么事？”](https://oreil.ly/5Mr6I),
    *科学美国人*（博客），2014年6月17日。
- en: ^([21](ch04.html#ch01fn22-marker)) Pranav Dixit, [“At TIME100 Impact Dinner,
    AI Leaders Discuss the Technology’s Transformative Potential”](https://oreil.ly/by-I6)
    *Time*, September 17, 2024.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch04.html#ch01fn22-marker)) Pranav Dixit, [“在TIME100影响力晚宴上，人工智能领导者讨论了技术的变革潜力”](https://oreil.ly/by-I6)
    *时代杂志*, 2024年9月17日。
- en: ^([22](ch04.html#ch01fn23-marker)) James Hurley, [“AI Has the Potential to ‘Cure
    All Diseases,’ Says DeepMind Chief”](https://oreil.ly/USHZW), *Sunday Times*,
    October 2, 2024.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch04.html#ch01fn23-marker)) James Hurley, [“DeepMind首席执行官表示，人工智能有‘治愈所有疾病’的潜力”](https://oreil.ly/USHZW),
    *星期日泰晤士报*, 2024年10月2日。
