- en: 7 Scaling and performance optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 规模和性能优化
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Optimizing data analysis code
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化数据分析代码
- en: Scaling code across multiple CPUs and GPUs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个 CPU 和 GPU 上扩展代码
- en: Code conversion between programming languages
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程语言之间的代码转换
- en: Cloud-based deployment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于云的部署
- en: Using the Dask framework for all of this
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Dask 框架来完成所有这些
- en: In today’s data-driven world, it’s more likely than not that the insights required
    by your business will be gained from vast amounts of data. As data volumes continue
    to grow and analytical complexity increases, and as algorithms and data processing
    pipelines reach the point where the sheer volume of data or the complexity of
    operations limits their ability to deliver timely and accurate results, you’ll
    encounter the need for code performance optimization. It will likely become essential
    for ensuring your analytics remain effective and efficient, even when the underlying
    data and computational demands grow.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天以数据驱动为主的世界里，你的业务所需洞察很可能来自大量数据。随着数据量的持续增长和分析复杂性的增加，以及算法和数据处理管道达到数据量本身或操作复杂度限制了它们提供及时和准确结果的能力，你将遇到代码性能优化的需求。这很可能成为确保你的分析保持有效和高效的关键，即使底层数据和计算需求增长。
- en: Throughout this chapter, we’ll assume that the analysis of the Brazilian e-commerce
    data we performed in previous chapters was so successful that, instead of encompassing
    a set of a hundred thousand orders, it now needs to be performed periodically
    on datasets of millions of orders. To make things even more interesting, let’s
    assume that the business grew and became multinational. Such growth could easily
    increase the amount of data by several orders of magnitude. At first glance, you
    might think throwing more CPUs at the problem or migrating to the latest GPU for
    its sheer processing power would solve the problem, but this would invariably
    involve either capital expenditure or an extension of the budget for cloud solutions.
    Just as an old IT adage states that any data will quickly fill all space allocated
    to it, you will quickly find that, when attempting any non-trivial data analysis,
    the calculations invariably consume all computing power available.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将假设我们在前几章中进行的巴西电子商务数据分析非常成功，以至于现在它不再需要涵盖一百万个订单的集合，而是需要定期在数百万个订单的数据集上执行。为了使事情更有趣，让我们假设业务增长并成为跨国公司。这种增长可能会轻易地将数据量增加几个数量级。乍一看，你可能认为将更多的
    CPU 投入问题或迁移到最新的 GPU 以其强大的处理能力来解决问题，但这不可避免地会涉及资本支出或扩展云解决方案的预算。正如一句古老的 IT 格言所说，任何数据都会迅速填满分配给它的所有空间，你很快就会发现自己尝试任何非平凡的数据分析时，计算总是消耗所有可用的计算能力。
- en: Fortunately, you have numerous strategies for scaling and performance optimization
    at your disposal, including parallel and distributed computing, GPU acceleration,
    and specialized hardware or software solutions. By exploring various methods and
    tools, you should be able to find the most appropriate techniques for your specific
    data analytics needs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，你有许多策略可用于扩展和性能优化，包括并行和分布式计算、GPU 加速以及专门的硬件或软件解决方案。通过探索各种方法和工具，你应该能够找到最适合你特定数据分析需求的技术。
- en: There is a catch, though. There is no one-size-fits-all solution. Your choices
    will almost always be limited by the technology stack available to you and either
    by the budget or by what your business or institution has authorized for use.
    That is why we’ll provide an overview of different methods here rather than taking
    a deep dive into a single option.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然如此，有一个问题。没有一种适合所有情况的解决方案。你的选择几乎总是受限于你可用的技术栈，以及预算或你的企业或机构授权使用的范围。这就是为什么我们将在这里提供一个不同方法的概述，而不是深入研究单一选项。
- en: As you’ll see, generative AI, and particularly GPT models, can greatly support
    your optimization and scaling efforts. These advanced natural language processing
    models can help you identify relevant strategies, suggest improvements, review
    code, and even generate optimized code snippets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，生成式 AI，尤其是 GPT 模型，可以极大地支持你的优化和扩展工作。这些高级自然语言处理模型可以帮助你识别相关策略，提出改进建议，审查代码，甚至生成优化的代码片段。
- en: In this chapter, we will delve deeper into the importance of scaling and optimization
    in the context of data analytics. We will cover various techniques and approaches
    to achieve optimal performance and discuss the role of GPT models in supporting
    this process. Additionally, we will provide practical examples and case studies
    to demonstrate the real-world impact of effective scaling and optimization in
    data analytics.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨扩展和优化在数据分析背景下的重要性。我们将涵盖各种技术和方法来实现最佳性能，并讨论GPT模型在支持这一过程中的作用。此外，我们将提供实际案例研究和案例研究，以展示有效扩展和优化在数据分析中的实际影响。
- en: Although there are many tools that purport to perform optimization and scaling
    automatically, we believe that knowing these topics is vital in understanding
    proper code performance. Sadly, a lot of newbie data scientists, analysts, and
    engineers take code performance for granted or delegate it to said automated tools,
    potentially seriously underachieving in their efforts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有许多声称可以自动执行优化和扩展的工具，但我们认为了解这些主题对于理解适当的代码性能至关重要。遗憾的是，许多新手数据科学家、分析师和工程师将代码性能视为理所当然，或者将其委托给这些自动化工具，这可能导致他们的努力严重不足。
- en: By the end of this chapter, you will have a solid understanding of the critical
    role that scaling and performance optimization play in data analytics, and you’ll
    have the knowledge necessary to identify, evaluate, and implement effective strategies
    to ensure your analytics pipeline remains efficient and effective in the face
    of ever-growing data volumes and computational demands.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将深刻理解扩展和性能优化在数据分析中的关键作用，并且你将拥有识别、评估和实施有效策略的知识，以确保你的分析管道在面对不断增长的数据量和计算需求时保持高效和有效。
- en: 7.1 Performance measurement
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 性能测量
- en: 'When setting out to optimize your analytics, the first crucial step is defining
    what good performance means for your goals. You need a clear yardstick to measure
    the success of your efforts. This means defining two basic concepts:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当开始优化你的分析时，第一步是定义对你目标来说良好的性能意味着什么。你需要一个清晰的标尺来衡量你努力的成败。这意味着定义两个基本概念：
- en: What characteristics of the system’s performance are most important? Should
    it prioritize quick response or accuracy? Should it allow multiple user access,
    or is it OK for users to have to queue? Can it block access while it is processing,
    or does it still need to allow users to view the data while they are waiting?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统性能的哪些特征最重要？它应该优先考虑快速响应还是准确性？它应该允许多个用户访问，还是用户需要排队？它在处理时可以阻止访问，还是它仍然需要在用户等待时允许用户查看数据？
- en: What are the baselines for the preceding characteristics? Should we aim for
    the current expected load, or already plan for future expansion? Can we reliably
    define what a normal load will look like?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前述特性的基线是什么？我们应该针对当前的预期负载，还是已经计划未来的扩展？我们能否可靠地定义正常负载将是什么样子？
- en: 'Here, we could consider such metrics as the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以考虑以下指标：
- en: '*Latency**—*The time it takes for a data analysis system to begin displaying
    initial results after the data input is complete.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*延迟**—*数据输入完成后，数据分析系统开始显示初始结果所需的时间。'
- en: '*Throughput**—*The amount of data the system can process within a given time
    frame. It is typically measured in records per second or gigabytes per hour, reflecting
    the system’s efficiency in handling large datasets.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*吞吐量**—*系统在给定时间框架内可以处理的数据量。通常以每秒记录数或每小时千兆字节来衡量，反映了系统处理大数据集的效率。'
- en: '*Parallelism or concurrency**—*The number of data analysis queries or processes
    the system can handle simultaneously without performance degradation. It demonstrates
    the system’s ability to manage multiple user requests or tasks at the same time.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*并行或并发**—*系统可以同时处理而不降低性能的数据分析查询或进程的数量。它展示了系统同时管理多个用户请求或任务的能力。'
- en: '*Bandwidth*—In the context of data analysis, this refers to the volume of data
    that can be imported into or exported out of the system per unit time. It is crucial
    for systems that need to handle large data transfers efficiently.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*带宽*—在数据分析的背景下，这指的是单位时间内可以导入或导出系统的数据量。对于需要高效处理大量数据传输的系统至关重要。'
- en: '*Response time*—The total time it takes for a system to complete a data analysis
    query or task from the moment it is initiated. This metric includes both the computation
    time and the time taken to retrieve and display the results.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*响应时间*—系统从启动数据分析查询或任务的那一刻起，完成该查询或任务所需的总时间。这个指标包括计算时间和检索并显示结果所需的时间。'
- en: '*Availability*—The likelihood that the data analysis system is operational
    and available for use when needed. It’s particularly important in systems requiring
    high uptime for continuous data analysis and decision-making processes.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可用性*—数据分析系统在需要时能够正常运行并可供使用的可能性。这对于需要高可用性以进行连续数据分析和决策过程的系统尤为重要。'
- en: '*Scalability*—The ability of the data analysis system to maintain or improve
    performance as the size of the data grows. This includes adding more data sources,
    handling more complex queries, or serving more users without a drop in performance.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展性*—数据分析系统在数据量增长时保持或提高性能的能力。这包括添加更多数据源、处理更复杂的查询或为更多用户提供服务而不会降低性能。'
- en: '*Reliability*—The consistency of the data analysis system in providing accurate
    and timely results under different conditions. This metric is often assessed by
    the frequency and severity of system failures or inaccuracies in the output data.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可靠性*—数据分析系统在不同条件下提供准确和及时结果的一致性。这个指标通常通过系统故障的频率和严重性或输出数据中的不准确度来评估。'
- en: 'Let’s look at a real-world scenario: a company was moving its infrastructure
    to the cloud and, suddenly, solutions that worked well on virtual machines became
    unusable due to the sheer amount of data to be analyzed. Processing times shot
    up dramatically, sometimes exceeding a 5,400% increase. Management had to reassess
    their approach, targets, and priorities swiftly, and, due to limited capacities,
    it became critical to have a good understanding of what “good” means in this specific
    context.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个现实世界的场景：一家公司正在将其基础设施迁移到云端，突然，由于需要分析的数据量巨大，在虚拟机上运行良好的解决方案变得无法使用。处理时间急剧上升，有时甚至超过5400%的增长。管理层不得不迅速重新评估他们的方法、目标和优先级，由于容量有限，对“好”的具体含义有良好的理解变得至关重要。
- en: Comparing vastly different domains can illustrate the different priorities.
    If you consider a data analysis system for an astronomical observatory, it is
    likely that latency or response time will not be as critical as in the case of,
    for example, a patient-monitoring system in a hospital. Furthermore, it would
    probably make more sense to build a system for monitoring a single patient, and
    create separate instances for each patient, than invest in a single multi-patient
    system, given that each patient is likely to require separate care. In contrast,
    an automation system for a chemical plant would likely require many concurrent
    and linked processes to run and link together to provide a full picture to the
    operators.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 比较截然不同的领域可以说明不同的优先级。如果您考虑一个用于天文观测的数据分析系统，那么延迟或响应时间可能不会像在例如医院中的患者监控系统那样关键。此外，鉴于每个患者可能需要单独的护理，可能更有意义的是为单个患者构建一个监控系统，并为每个患者创建单独的实例，而不是投资于一个单一的多患者系统。相比之下，一个化工厂的自动化系统可能需要许多并发和链接的过程来运行并连接起来，以便为操作员提供完整的画面。
- en: Once you understand your requirements, you’ll be able to define whether your
    current system, whether automated or manual, is sufficient, and where it requires
    improvements. Then, once changes are made, you should be able to understand how
    your new system performs compared to the previous version, and whether the changes
    provided the desired impact.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您了解了您的需求，您就能确定您当前的系统，无论是自动化的还是手动的，是否足够，以及它需要改进的地方。然后，一旦进行了更改，您应该能够理解与之前版本相比，您的新系统表现如何，以及这些更改是否产生了预期的效果。
- en: In this section, we’ll explore straightforward approaches and metrics that can
    be used to analyze the efficiency of your data analytics systems.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨可以用来分析您数据分析系统效率的简单方法和指标。
- en: How to measure?
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 如何衡量？
- en: 'There are various approaches to measuring the performance of data analytics
    systems, including the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 测量数据分析系统性能的方法有很多，包括以下几种：
- en: '*Profiling*—Collecting detailed information about the execution of a program
    or algorithm, such as the time spent in each function or the amount of memory
    allocated. This data can help identify hot spots or areas of the code that consume
    the most resources.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性能分析*—收集有关程序或算法执行详细信息的活动，例如每个函数花费的时间或分配的内存量。这些数据可以帮助识别热点或消耗最多资源的代码区域。'
- en: '*Stress testing*—Related to profiling, but under loads approaching or exceeding
    maximum expected amounts. This allows you to check if the system can still perform
    adequately if pushed to the limit.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*压力测试*—与性能分析相关，但处于接近或超过预期最大量的负载下。这允许您检查在极限压力下系统是否仍能良好地执行。'
- en: '*Benchmarking*—Comparing the performance of a system against a reference implementation
    or industry-standard metrics.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基准测试*—将系统的性能与参考实现或行业标准指标进行比较。'
- en: '*Monitoring*—Collecting and analyzing performance metrics over time, allowing
    businesses to track the impact of optimization efforts and identify any emerging
    issues or trends in response to changing data volumes and computational demands.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监控*—随着时间的推移收集和分析性能指标，使企业能够跟踪优化工作的影响，并识别对数据量和计算需求变化的响应中出现的任何新兴问题或趋势。'
- en: Let’s take an analysis example performed in chapter 4, where we used bootstrap
    methods to calculate confidence intervals for the average sales price and average
    review score per state. Bootstrapping is a computationally intensive technique,
    as it involves repeatedly drawing random samples with replacements from the original
    data, and recalculating the statistic of interest.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以第4章中进行的分析为例，其中我们使用了自举方法来计算每个州的平均销售价格和平均评论分数的置信区间。自举是一种计算密集型技术，因为它涉及到从原始数据中反复抽取带有替换的随机样本，并重新计算感兴趣的统计量。
- en: 'Several performance metrics can be used to evaluate the efficiency of a data
    analytics system. We’ll look at three of the most important areas in the rest
    of this section: execution time, throughput, and resource utilization. We will,
    however, focus mostly on the execution time. Depending on the properties and limitations
    of your computing environment, some of these areas might be of more importance
    than others.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用几个性能指标来评估数据分析系统的效率。在本节的剩余部分，我们将探讨三个最重要的领域：执行时间、吞吐量和资源利用率。然而，我们将主要关注执行时间。根据您计算环境的特性和限制，这些领域中的某些可能比其他领域更重要。
- en: 7.1.1 Execution time
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 执行时间
- en: Execution time is a direct indicator of your system’s overall speed and responsiveness.
    It’s defined as the time it takes to complete a specific task or process a given
    volume of data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时间是您系统整体速度和响应性的直接指标。它定义为完成特定任务或处理给定数据量所需的时间。
- en: In CPU time, I’m young; in real time, my coffee’s cold again
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在CPU时间上，我年轻；在真实时间上，我的咖啡又凉了
- en: In today’s multitasking and multithreading systems, measuring execution time
    cannot be reliably performed with just a stopwatch. If your CPU is busy crunching
    numbers, and you decide to play the latest AAA game on the same machine to kill
    time, you could be disappointed with the performance of your program, even though
    it’s trying its absolute best.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今的多任务和多线程系统中，仅用秒表来测量执行时间是不可靠的。如果您的CPU正忙于处理数字，而您决定在同一台机器上玩最新的AAA游戏来消磨时间，您可能会对程序的性能感到失望，尽管它正在尽力而为。
- en: In this context, *CPU time* refers to the actual time your processor spent on
    the calculation, as opposed to handling all the other tasks, like updating your
    screen, running your antivirus in the background, and refreshing your browser
    while you’re reading the news. Invariably, this will be significantly less than
    *real time*, or elapsed time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，*CPU时间*指的是处理器在计算上实际花费的时间，而不是处理所有其他任务的时间，比如更新屏幕、在后台运行杀毒软件，以及您阅读新闻时刷新浏览器。不可避免的是，这将会显著少于*实际时间*，或经过的时间。
- en: 'Some of the methods used in this chapter will rely on measuring elapsed time
    as a proxy, since it’s much easier to do and usually suffices unless you are running
    advanced analysis of execution times on very short pieces of code. However, bear
    these points in mind:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的某些方法将依赖于测量经过的时间作为代理，因为它更容易实现，并且通常足够，除非您正在对非常短的代码片段进行执行时间的先进分析。然而，请记住这些要点：
- en: It’s best to not run other intensive processes while measuring the performance
    of your calculations.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测量计算性能时，最好不要运行其他密集型进程。
- en: When measuring performance, use relatively large dataset sizes, as this will
    make comparisons between different pieces of code more reliable by reducing the
    impact of overhead processing.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测量性能时，使用相对较大的数据集大小，因为这将通过减少开销处理的影响，使不同代码片段之间的比较更加可靠。
- en: Don’t be surprised if running the same code multiple times produces different
    execution times. It’s best to average the runtime of several runs to get a better
    approximation. Alternatively, take the minimum runtime from several attempts,
    as this will reflect the minimum impact of overhead on the processor.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果多次运行相同的代码产生不同的执行时间，请不要感到惊讶。最好是将多次运行的运行时间取平均值，以获得更好的近似值。或者，从多次尝试中取最小运行时间，因为这将反映处理器上开销的最小影响。
- en: Let’s see if we can get our GPT advisor to provide us with the code required
    to measure the execution time of our bootstrap function.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们能否让我们的GPT顾问提供我们测量bootstrap函数执行时间的所需代码。
- en: '**![image](../Images/Init-MA.png)**I have a function `bootstrap_CI()`. `bootstrap_CI()`
    is being run from within another function, `calculate_ci(group)`, which, in turn,
    is run with the command: `ci_df = grouped_data.apply(calculate_ci)`. The `grouped_data`
    object is created as `grouped_data = filtered_df.groupby(''seller_state'')`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**我有一个名为`bootstrap_CI()`的函数。`bootstrap_CI()`函数在另一个函数`calculate_ci(group)`内部运行，而`calculate_ci(group)`函数又通过命令`ci_df
    = grouped_data.apply(calculate_ci)`运行。`grouped_data`对象是通过`grouped_data = filtered_df.groupby(''seller_state'')`创建的。'
- en: Please provide code to measure the performance of the `bootstrap_CI()` function
    depending on the size of `filtered_df` DataFrame (randomly selecting appropriate
    subset), from 100 to 9944 elements, and graph it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请提供代码以测量`bootstrap_CI()`函数的性能，该函数根据`filtered_df` DataFrame的大小（随机选择合适的子集）进行测量，从100到9944个元素，并将其绘制成图表。
- en: '**![image](../Images/Response.png)**To measure the performance of the `bootstrap_CI()`
    function depending on the number of randomly selected entries from `filtered_df`
    DataFrame, you can create the function which uses the `sample` function from pandas,
    applies the `groupby` operation, and measures the time it takes to run the `apply`
    function with `calculate_ci` on the grouped data. Here’s the sample code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**要测量`bootstrap_CI()`函数的性能，该函数根据从`filtered_df`
    DataFrame中随机选择的条目数量进行测量，你可以创建一个函数，该函数使用pandas的`sample`函数，应用`groupby`操作，并测量对分组数据运行`apply`函数`calculate_ci`所需的时间。以下是示例代码：'
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code measures the performance of the `calculate_ci()` function for different
    sizes of randomly-selected subsets of `filtered_df`. The execution times are plotted
    against the sizes of the subsets.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码测量`calculate_ci()`函数对不同大小的随机选择的`filtered_df`子集的性能。执行时间与子集的大小成比例绘制。
- en: The result is presented in figure 7.1.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 结果展示在图7.1中。
- en: '![figure](../Images/CH07_F01_Siwiak3.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F01_Siwiak3.png)'
- en: Figure 7.1 Chart of time of executing bootstrapping function depending on the
    size of the processed DataFrame
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1 执行bootstrapping函数的时间与处理后的DataFrame大小的图表
- en: This metric is pretty straightforward; however, you need to remember that this
    is the narrowest metric, as its measurement is only valid for the specific input,
    system load, configurations, and so on.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标非常直接；然而，你需要记住，这是最窄的指标，因为它的测量只对特定的输入、系统负载、配置等有效。
- en: 'Furthermore, this simple example only takes into account a single measurement.
    In practice, this will not be very accurate, as modern computers are designed
    to perform many tasks concurrently, and that will have an impact on the one business-critical
    process you are trying to measure. Therefore, to isolate these variable effects,
    but still get a reliable reading, you should do the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这个简单的例子只考虑了单次测量。在实践中，这不会非常准确，因为现代计算机被设计成可以同时执行许多任务，这将对你要测量的一个关键业务流程产生影响。因此，为了隔离这些变量效应，但仍然获得可靠的读数，你应该做以下事情：
- en: Perform the profiling under expected loads. There is no point profiling a system
    on an isolated test server, and then expecting it to perform identically on a
    production server with 50 other processes running, competing for resources.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预期的负载下进行性能分析。在孤立的测试服务器上分析系统是没有意义的，然后期望它在有50个其他进程运行的生产服务器上以相同的方式表现，这些进程正在争夺资源。
- en: Profile many runs and perform appropriate statistical analysis, calculating
    at the very least the minimum, maximum, and average times. This will give you
    a much better picture of expected performance than a single run.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对多次运行进行配置文件分析并执行适当的统计分析，至少计算最小值、最大值和平均值。这将比单次运行给出一个更好的预期性能图景。
- en: 7.1.2 Throughput
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 吞吐量
- en: Throughput is a measure of the rate at which a system can process data, typically
    measured in transactions per second or records processed per second. High throughput
    is essential for handling large data volumes and meeting real-time processing
    requirements.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量是衡量系统处理数据速率的指标，通常以每秒事务数或每秒处理的记录数来衡量。高吞吐量对于处理大量数据并满足实时处理需求至关重要。
- en: As throughput is a simple division of the size of data by elapsed time, we slightly
    modified the code we used for the execution time measurement. You can find the
    full code in the Jupyter notebook in the book’s GitHub repository. The result,
    presented in figure 7.2, may be somewhat surprising.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于吞吐量是数据大小与经过时间的简单除法，我们对用于执行时间测量的代码进行了轻微修改。您可以在书中GitHub仓库的Jupyter笔记本中找到完整的代码。如图7.2所示的结果可能有些令人惊讶。
- en: '![figure](../Images/CH07_F02_Siwiak3.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F02_Siwiak3.png)'
- en: Figure 7.2 Dependence of the execution time (solid line) and throughput (dashed
    line) of the bootstrapping function on the size of the processed DataFrame
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2 启动函数的执行时间（实线）和吞吐量（虚线）与处理DataFrame大小的关系
- en: Intuitively, if throughput is the number of processed records divided by the
    time of processing all records, each should be processed in the same way. It’s
    true that, if each record were processed independently and in a constant time,
    the throughput would remain constant. However, in practice, this is not always
    the case. Various factors can affect the processing time of each record and the
    overall throughput, among them record size and complexity (not our case), data
    dependencies (not our case), resource contention (maybe our case), algorithmic
    behavior (likely our case), or external factors like the network latency, disk
    speed, or other system processes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地讲，如果吞吐量是处理记录数除以处理所有记录的时间，每个记录都应以相同的方式进行处理。确实，如果每个记录都是独立且以恒定时间处理的，吞吐量将保持不变。然而，在实践中，情况并不总是如此。各种因素可以影响每个记录的处理时间和整体吞吐量，其中记录大小和复杂性（不是我们的情况）、数据依赖性（不是我们的情况）、资源竞争（可能是我们的情况）、算法行为（很可能是我们的情况），或外部因素如网络延迟、磁盘速度或其他系统进程。
- en: Throughput can be a helpful tool when estimating the projected increase in resource
    requirements in relation to an increase in the amount of data to be processed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过吞吐量可以作为一个有用的工具，在估计与处理数据量增加相关的资源需求预期增长时。
- en: 7.1.3 Resource utilization
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 资源利用率
- en: To get a fuller view of how your code is performing, you should not just take
    into account the execution time of your code, but also observe system resources,
    like memory and storage, used during code execution. High resource utilization
    can tell you if the system is efficiently using the available resources, and it
    may also signal potential bottlenecks or resource contention.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要更全面地了解代码的性能，您不仅应考虑代码的执行时间，还应观察代码执行期间使用的系统资源，如内存和存储。高资源利用率可以告诉您系统是否有效地使用可用资源，并且它还可能表明潜在的瓶颈或资源竞争。
- en: Monitoring system resources is material for a separate book (and likely more
    than one), but some simple measurements and observations can tell you if you are
    hitting the limits, and measurements should be available for you irrespective
    of whether you use your Windows laptop (the Resource Monitor app), Linux (the
    top utility), or cloud solutions (provider-dependent).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 监控系统资源是另一本书（可能不止一本）的内容，但一些简单的测量和观察可以告诉您是否达到了极限，并且无论您使用Windows笔记本电脑（资源监视器应用）、Linux（top实用程序）还是云解决方案（供应商相关），都应该有可用的测量数据。
- en: 'Measurement of resource utilization is critical in distinguishing two primary
    limiting factors, or bottlenecks, of your program or task:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 资源利用率的测量对于区分程序或任务的两个主要限制因素，或瓶颈，至关重要：
- en: '*CPU-bound tasks***—**These tasks are mainly limited by the processing power
    of the CPU on which your program runs. The performance of CPU-bound tasks can
    be improved by using a faster CPU or by parallelizing the task across multiple
    CPU cores. Examples of CPU-bound tasks include numerical simulations, complex
    mathematical calculations, and data processing tasks like sorting or searching.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CPU-bound tasks***—**这些任务主要受限于您程序运行的CPU的处理能力。通过使用更快的CPU或将任务并行化到多个CPU核心，可以提高CPU-bound
    tasks的性能。CPU-bound tasks的例子包括数值模拟、复杂的数学计算以及排序或搜索等数据处理任务。'
- en: '*I/O-bound tasks*—These tasks are primarily limited by input/output (I/O) operations,
    such as reading from or writing to a file (swapping data to a hard drive if you
    run out of memory), network communication, or user input. I/O-bound tasks spend
    most of their time waiting for I/O operations to complete rather than executing
    instructions on the CPU. The performance of I/O-bound tasks can be improved by
    using faster storage devices, optimizing I/O operations, or using asynchronous
    I/O to allow the program to perform other tasks while waiting for I/O operations
    to complete. Examples of I/O-bound tasks include reading large files, downloading
    data from the internet, or waiting for user input.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*I/O-bound tasks*—这些任务主要受限于输入/输出（I/O）操作，例如从文件读取或写入（如果你内存不足，则将数据交换到硬盘），网络通信或用户输入。I/O-bound
    tasks的大部分时间都花在等待I/O操作完成上，而不是在CPU上执行指令。通过使用更快的存储设备、优化I/O操作或使用异步I/O以允许程序在等待I/O操作完成时执行其他任务，可以提高I/O-bound
    tasks的性能。I/O-bound tasks的例子包括读取大文件、从互联网下载数据或等待用户输入。'
- en: Let’s see if we can get some quick solutions for resource measurement from our
    GPT advisor.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们能否从我们的GPT顾问那里获得一些关于资源测量的快速解决方案。
- en: '**![image](../Images/Init-MA.png)**How can I quickly check if my function is
    CPU-bound or IO-bound, and what is its memory usage?'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**如何快速检查我的函数是CPU-bound还是I/O-bound，以及它的内存使用情况？'
- en: '**![image](../Images/Response.png)**To quickly determine if your function is
    CPU-bound or I/O-bound, and what is its memory usage, you can monitor the CPU
    usage, I/O activity and used memory while the function is running. Here’s a high-level
    approach to achieve this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**为了快速确定您的函数是CPU-bound还是I/O-bound，以及它的内存使用情况，您可以在函数运行时监控CPU使用率、I/O活动和已用内存。以下是一个实现此目的的高级方法：'
- en: 'Monitor CPU usage: Track the CPU usage during the execution of your function.
    If the CPU usage remains consistently high, it is a strong indication that your
    function is CPU-bound.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 监控CPU使用：在您的函数执行期间跟踪CPU使用情况。如果CPU使用率保持持续较高，这是一个强烈的迹象表明您的函数是CPU-bound。
- en: 'Monitor I/O activity: Track I/O operations, such as disk reads/writes and network
    activity, during the execution of your function. If you observe high I/O activity
    but relatively low CPU usage, it is a strong indication that your function is
    I/O-bound.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 监控I/O活动：在您的函数执行期间跟踪I/O操作，例如磁盘读写和网络活动。如果您观察到高I/O活动但相对较低的CPU使用率，这是一个强烈的迹象表明您的函数是I/O-bound。
- en: 'Monitor memory use: Track memory use during the execution of your function.
    To account only for the memory usage associated with the execution of your function,
    you can capture the memory usage before and after running the function and subtract
    the background memory usage accordingly.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 监控内存使用：在您的函数执行期间跟踪内存使用情况。为了仅计算与您的函数执行相关的内存使用情况，您可以在运行函数前后捕获内存使用情况，并相应地减去背景内存使用情况。
- en: 'You can use built-in operating system tools or Python libraries to monitor
    CPU, I/O activity, and memory usage. Here’s an example using the psutil library
    to monitor them:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用内置的操作系统工具或Python库来监控CPU、I/O活动和内存使用。以下是一个使用psutil库来监控它们的示例：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Replace `your_function()` with the function you want to analyze. This script
    will monitor the CPU usage, I/O activity and memory usage for 10 seconds with
    1-second intervals. If the average CPU usage is high, your function is likely
    CPU-bound. If the I/O activity (read and write bytes) is high but the CPU usage
    is relatively low, your function is likely I/O-bound.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将`your_function()`替换为您想要分析的功能。此脚本将以1秒间隔监控10秒内的CPU使用率、I/O活动和内存使用情况。如果平均CPU使用率较高，则您的功能可能是CPU-bound。如果I/O活动（读取和写入的字节）较高，但CPU使用率相对较低，则您的功能可能是I/O-bound。
- en: Keep in mind that this is a simple example, and you might need to adjust the
    monitoring duration and interval to match the runtime of your function for more
    accurate results. Additionally, this approach does not account for other factors
    that could be affecting your function’s performance, such as memory consumption
    or network latency.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这是一个简单的例子，你可能需要调整监控时长和间隔以匹配你函数的运行时间，以获得更准确的结果。此外，这种方法没有考虑到可能影响你函数性能的其他因素，例如内存消耗或网络延迟。
- en: 'In this chapter’s Jupyter notebook in the book’s GitHub repository, we provide
    the code modified for our example. These are the results we got:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书GitHub仓库中的这一章节的Jupyter笔记本中，我们提供了为我们的示例修改过的代码。以下是我们的结果：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With this scale of data and simplicity of performed operations, the limiting
    factor is related to disk operations. In this case, we shouldn’t expect much improvement
    if we parallelize a calculation across multiple processors, although we will do
    so in the following section for illustration’s sake. Increasing CPU performance
    will likely have minimum impact if all the parallel tasks have to fight to read
    data from a single file.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的数据规模和操作简单性的情况下，限制因素与磁盘操作相关。在这种情况下，如果我们跨多个处理器并行化计算，我们不应该期望有太大的改进，尽管我们将在下一节中为了说明的目的这样做。如果所有并行任务都必须争夺从单个文件中读取数据，那么提高CPU性能可能对整体性能的提升影响最小。
- en: Measuring where your data analysis spends most of its time and energy is critical
    to understanding how to make it go faster. There is no point optimizing a calculation
    if the limiting factor is the data transfer bandwidth—swapping a hard drive for
    a faster one might give you a much more significant boost, costing only a few
    bucks.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 测量你的数据分析在哪些地方花费了大部分的时间和精力对于理解如何让它更快至关重要。如果限制因素是数据传输带宽，那么优化计算是没有意义的——用更快的硬盘替换可能只会给你带来更大的提升，而且只需花费几美元。
- en: 7.2 Improving code performance
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 提高代码性能
- en: 'In the context of data analytics, you may encounter two terms used interchangeably:
    “code optimization” and “code scaling.” These refer to two distinct concepts.
    They are both crucial to improving the performance and efficiency of data processing
    pipelines, but it’s important to distinguish between them, as they have different
    roles in overall performance improvement:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析的情况下，你可能会遇到两个可以互换使用的术语：“代码优化”和“代码扩展”。这两个术语指的是两个不同的概念。它们对于提高数据处理管道的性能和效率都至关重要，但区分它们很重要，因为它们在整体性能提升中扮演着不同的角色：
- en: '*Code optimization* is the process of fine-tuning the algorithm, code, or data
    processing pipeline to improve its performance, speed, or resource utilization.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代码优化*是指调整算法、代码或数据处理管道的过程，以提高其性能、速度或资源利用率。'
- en: '*Code scaling* refers to adapting the code to use larger or better performing
    hardware more efficiently, or to adapting it to different architectures, such
    as GPU or multithreading.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代码扩展*指的是将代码调整以更有效地使用更大或性能更好的硬件，或者将其调整到不同的架构，例如GPU或多线程。'
- en: Let’s consider some examples. In the case of the patient-monitoring system we
    mentioned in section 7.1, it is likely that each patient who requires monitoring
    will be provided with their own unit. (Many units may connect to a single monitoring
    dashboard at the nurses’ station, but the data analysis will be done independently
    for each patient, which is what counts here.) Each unit should perform the analysis
    quickly and accurately, with very high availability, or even redundancy, as a
    human life may be at stake. Code optimization may be the best approach here, as
    each unit will be self-contained, will need to be mobile so it can move with the
    patient if they are taken for tests, and will likely already be very expensive.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些例子。在7.1节中提到的患者监控系统的情况下，很可能每个需要监控的患者都会得到自己的单元。（许多单元可能连接到护士站的单个监控仪表板，但数据分析将独立为每个患者进行，这才是这里的关键。）每个单元应快速准确地执行分析，具有非常高的可用性，甚至冗余，因为人的生命可能处于危险之中。代码优化可能是这里最好的方法，因为每个单元将是自包含的，需要是移动的，以便在患者进行测试时可以与患者一起移动，并且可能已经非常昂贵。
- en: By contrast, a monitoring system for financial markets will likely not require
    such a focus on optimizing every line of code. To preserve the maintainability
    and usability of such a complex system, it will be much better to ensure that
    the code is clearly written and documented; adequate performance will be achieved
    by running the system on sufficiently powerful servers capable of handling the
    load. Additional capacity may then be obtained by scaling—adding more CPUs or
    network bandwidth, or introducing processing segmentation and parallelism.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，金融市场监控系统可能不需要如此关注每行代码的优化。为了保持这样一个复杂系统的可维护性和可用性，确保代码清晰编写和文档化会更好；通过在足够强大的服务器上运行系统以处理负载，可以实现足够的性能。然后可以通过扩展来获得额外的容量——添加更多的CPU或网络带宽，或者引入处理分段和并行化。
- en: In this section, we will first try to optimize and then scale our `bootstrap_CI`
    function.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先尝试优化，然后扩展我们的 `bootstrap_CI` 函数。
- en: 7.2.1 Optimizing code
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 优化代码
- en: You can optimize code with various techniques, such as code refactoring, algorithmic
    improvements, or by using more efficient data structures and libraries. If it
    seems daunting, there’s a good reason. As much as code and algorithm optimization
    is a valuable activity, it’s often difficult and time-consuming if performed manually.
    Reviewing each line of code by hand and trying to optimize it while avoiding introducing
    bugs can conceivably be done for concise and simple snippets of code. For longer
    ones, more automated approaches are necessary. Let’s see if GPT models can be
    of help here.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用各种技术来优化代码，例如代码重构、算法改进，或者使用更高效的数据结构和库。如果这看起来很令人畏惧，那是有原因的。尽管代码和算法优化是一项有价值的活动，但如果手动执行，它通常既困难又耗时。手动审查每一行代码并尝试优化它，同时避免引入错误，对于简短且简单的代码片段可能是可行的。对于更长的代码，需要更自动化的方法。让我们看看GPT模型在这里是否能提供帮助。
- en: '**![image](../Images/Init-MA.png)**Optimize this code, preferably without parallelization.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)** 最好在不并行化的情况下优化这段代码。'
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Name the resulting function `bootstrap_CI_optimized`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成的函数命名为 `bootstrap_CI_optimized`。
- en: '**![image](../Images/Response.png)**You can optimize the code without parallelization
    by using NumPy’s vectorized operations to generate all the bootstrap samples at
    once. Here’s an optimized version:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)** 你可以通过使用NumPy的向量化操作一次性生成所有bootstrap样本来优化代码，而不需要并行化。下面是一个优化的版本：'
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this version, we create an index array `idx` with the shape `(n_bootstraps,
    len(data))` using `np.random.randint`. Then, we use advanced indexing to create
    the `bootstrap_samples` array, which contains all the bootstrap samples at once.
    Finally, we apply the `func` directly to the `bootstrap_samples` array along the
    specified axis. This version takes advantage of NumPy’s vectorized operations
    to efficiently compute the bootstrapped statistics without parallelization.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个版本中，我们使用 `np.random.randint` 创建了一个形状为 `(n_bootstraps, len(data))` 的索引数组 `idx`。然后，我们使用高级索引创建
    `bootstrap_samples` 数组，该数组包含所有一次性的bootstrap样本。最后，我们直接在指定的轴上对 `bootstrap_samples`
    数组应用 `func`。这个版本利用了NumPy的向量化操作，在不并行化的情况下高效地计算bootstrap统计量。
- en: It seems that despite the simplicity of the provided code (and the fact that
    it was generated by ChatGPT 4 in the first place), it looks like there was still
    room for improvement. Or was there?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，尽管提供的代码（以及它最初是由ChatGPT 4生成的）很简单，但似乎仍有改进的空间。或者，是吗？
- en: Before running this code on the biggest cluster we can find, we need to check
    if the proposed change indeed brought improvements. A comparison of the execution
    time of the basic and optimized functions is presented in figure 7.3.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能在找到的最大集群上运行此代码之前，我们需要检查所提出的更改是否确实带来了改进。基本函数和优化函数的执行时间比较在图7.3中展示。
- en: '![figure](../Images/CH07_F03_Siwiak3.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F03_Siwiak3.png)'
- en: Figure 7.3 Dependence of the execution time of base (solid line) and optimized
    (dashed line) bootstrapping functions on the size of the processed DataFrame
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3 基础（实线）和优化（虚线）bootstrapping函数的执行时间与处理DataFrame大小的依赖关系
- en: Seems like code optimization did well. If you compare the scales of the two
    graphs, the time required for function execution dropped by a factor of almost
    seven. Looks like there is now much less time to drink coffee between calculation
    runs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 代码优化似乎做得很好。如果你比较两个图表的规模，函数执行所需的时间几乎减少了七倍。看起来现在在计算运行之间喝咖啡的时间要少得多。
- en: Don’t trust it further than you can throw it
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不要信任它超过你能扔到的距离
- en: While the optimized code may look great, and we are assured by the model that
    it performs the same function, it is imperative to subject the code to the same
    thorough testing as you would for human-developed code. See chapter 8 for a more
    complete discussion of the risks associated with using GPT models and the phenomenon
    of AI delusion.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管优化后的代码看起来很棒，并且模型保证它执行相同的函数，但将代码提交与人类开发的代码相同的彻底测试是至关重要的。请参阅第8章，以更全面地讨论使用GPT模型的风险以及AI幻觉现象。
- en: Note that even though LLMs with very large or even theoretically infinite context
    sizes have been developed, it is likely that you may have to split long code into
    several prompts to fit into your particular model’s maximum context size. While
    a human expert would certainly ask for background information and a wider context,
    especially if questioned repeatedly about related concepts, ChatGPT will not.
    It’s therefore very important that you properly test the code to verify any assumptions
    you make about the output of the conversation with ChatGPT or the function of
    the final code. Also, using the iterative prompting techniques described in section
    7.3.2 may be helpful in getting ChatGPT to perform correct optimizations and prevent
    it from hallucinating too much.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管已经开发出具有非常大型或甚至理论上无限上下文大小的LLM（大型语言模型），但您可能仍然需要将长代码分成几个提示，以适应您特定模型的上下文大小限制。虽然人类专家肯定会要求背景信息和更广泛的上下文，尤其是在被反复询问相关概念时，但ChatGPT不会这样做。因此，正确测试代码以验证您对与ChatGPT对话的输出或最终代码功能的任何假设非常重要。此外，使用第7.3.2节中描述的迭代提示技术可能有助于ChatGPT执行正确的优化，并防止它过度幻想。
- en: 7.2.2 Scaling code
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 代码缩放
- en: Let’s assume that we are content with how our function runs on a single processor
    or thread. It may now be worthwhile to look at how it can be implemented on better-performing
    architectures, allowing you to achieve better performance still. An obvious example
    would be to invest in a faster processor or more RAM. Yet, potentially surprisingly,
    most implementations don’t take full advantage of the architectures of commonly
    available hardware. Almost all processors currently available on the market have
    multiple cores, and GPUs are installed in virtually every PC. Both of these allow
    several calculations to run in parallel, albeit with some technical differences.
    We will discuss these a bit later, but first let’s focus on how ChatGPT can help
    us utilize both.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们对函数在单个处理器或线程上的运行情况感到满意。现在，研究如何在性能更好的架构上实现它可能是有价值的，这样您仍然可以取得更好的性能。一个明显的例子就是投资更快的处理器或更多的RAM。然而，可能令人惊讶的是，大多数实现并没有充分利用常见硬件的架构。目前市场上几乎所有的处理器都有多个核心，几乎每台PC都安装了GPU。这两者都允许并行运行多个计算，尽管有一些技术差异。我们稍后会讨论这些，但首先让我们关注ChatGPT如何帮助我们利用这两者。
- en: Scaling across multiple CPU cores
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在多个CPU核心上缩放
- en: You can easily scale your code to utilize the multicore architectures available
    in virtually all modern processors with available libraries and the power of ChatGPT.
    Let’s try to parallelize our optimized bootstrap function.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用可用的库和ChatGPT的力量轻松地将代码缩放以利用几乎所有现代处理器中可用的多核架构。让我们尝试并行化我们的优化引导函数。
- en: '**![image](../Images/Init-MA.png)**Can you parallelize the following code?'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**您能并行化以下代码吗？'
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Name the resulting function `bootstrap_CI_parallelized`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成的函数命名为`bootstrap_CI_parallelized`。
- en: '**![image](../Images/Response.png)**You can parallelize `bootstrap_CI_optimized`
    function using the `concurrent .futures` module. I’ll demonstrate using a `ThreadPoolExecutor`.
    Here’s the modified version of the `bootstrap_CI_optimized` function:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**您可以使用`concurrent.futures`模块并行化`bootstrap_CI_optimized`函数。我将使用`ThreadPoolExecutor`进行演示。以下是`bootstrap_CI_optimized`函数的修改版本：'
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will parallelize the `bootstrap_CI_optimized` function across multiple
    threads. However, please note that if your data is stored in a Pandas DataFrame
    and you are using a custom function for `func`, you may need to handle the GIL
    (Global Interpreter Lock) in Python, which could limit the parallelization benefits
    in some cases.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在多个线程上并行化`bootstrap_CI_optimized`函数。但是，请注意，如果您的数据存储在Pandas DataFrame中，并且您为`func`使用自定义函数，您可能需要处理Python中的GIL（全局解释器锁），这可能在某些情况下限制并行化的好处。
- en: Now that we have parallelized our function, it will run like a bat out of hell!
    Yeah!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经并行化了我们的函数，它将像离弦之箭一样运行！是的！
- en: The results of the run on 16 parallel threads are presented in figure 7.4\.
    The code is now considerably . . . slower! Looks like ChatGPT’s final remark was
    true.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在16个并行线程上运行的运行结果如图7.4所示。现在代码的运行速度明显慢了下来！看来ChatGPT的最后评论是真的。
- en: '![figure](../Images/CH07_F04_Siwiak3.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F04_Siwiak3.png)'
- en: Figure 7.4 Dependence of the execution time of optimized (solid line) and parallelized
    (dashed line) bootstrapping functions on the size of the processed DataFrame
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4 优化（实线）和并行化（虚线）的bootstrapping函数的执行时间与处理DataFrame大小的依赖关系
- en: Fortunately, there is a lesson to be learned here. In section 7.1, we distinguished
    between CPU-bound and I/O-bound tasks. Our `bootstrap_CI` function utilized 1.5%
    of the available processing power, and we mentioned that this function doesn’t
    seem like the best candidate for parallelization. We did it anyway, and now we’re
    suffering the consequences.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这里有一个教训可以吸取。在第7.1节中，我们区分了CPU密集型和I/O密集型任务。我们的`bootstrap_CI`函数使用了1.5%的可用处理能力，我们提到这个函数似乎不是并行化的最佳候选。我们还是做了，现在我们正在承受后果。
- en: There ain’t no such thing as a free lunch
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 没有免费的午餐
- en: Parallelization is not free. There is an associated overhead cost related to
    dividing data into smaller chunks, starting and queuing the processes, and gluing
    the results together afterward. In our case, the main problem is the relatively
    minuscule size of the data, but this issue might also be important for large-scale
    tasks, depending on the structure of your data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化并非没有代价。与将数据分成更小的块、启动和排队进程以及随后将结果粘合在一起相关的开销成本。在我们的情况下，主要问题是数据相对较小，但这个问题对于大规模任务也可能很重要，具体取决于你的数据结构。
- en: Our ChatGPT advisor warned us that something called the Global Interpreter Lock
    (GIL) might limit our parallelization benefits. GIL is a mechanism that simplifies
    Python’s memory management but at the cost of serious limitations on the performance
    of multithreaded applications and their ability to utilize multiple CPU cores
    efficiently.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的ChatGPT顾问警告我们，一个叫做全局解释器锁（GIL）的东西可能会限制我们的并行化收益。GIL是一种简化Python内存管理的机制，但代价是严重限制了多线程应用程序的性能以及它们有效利用多个CPU核心的能力。
- en: Let’s see if moving from CPU to GPU will improve things considerably.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看从CPU迁移到GPU是否会显著提高性能。
- en: Scaling by using GPU
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用GPU进行扩展
- en: With certain exceptions, scaling across CPUs may provide significant benefits,
    but the GPU (graphics processing unit) is another piece of hardware available
    on most PCs nowadays. This piece of silicone is specifically optimized for performing
    a vast number of identical operations on vectors or matrices of data on hundreds
    of cores in parallel, a characteristic required for most computer-generated graphics
    calculations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些例外情况下，跨CPU的扩展可能带来显著的好处，但GPU（图形处理单元）是现在大多数PC上可用的另一块硬件。这块硅片专门优化了在数百个核心上并行执行大量相同操作的能力，这对于大多数计算机生成的图形计算是必需的。
- en: There is nothing preventing GPUs being used for calculations that have similar
    requirements but are not related to graphics. It is important, however, to understand
    that a GPU and CPU are very different, as we’ll discuss in the next section.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何东西阻止GPU用于具有类似要求但与图形无关的计算。然而，重要的是要理解GPU和CPU非常不同，我们将在下一节中讨论这一点。
- en: For now, let’s see how a GPU might be used in a simple example. The following
    code and techniques below use an Nvidia graphics card with a CUDA interface, allowing
    us to direct the code to utilize an Nvidia GPU for scaling.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看GPU在简单示例中的应用。以下代码和下面的技术使用带有CUDA界面的Nvidia显卡，使我们能够将代码直接指向Nvidia GPU进行扩展。
- en: '**![image](../Images/Init-MA.png)**Can you adjust the `bootstrap_CI_optimized`
    function to run with CUDA? Name the resulting function `bootstrap_CI_cuda`.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**你能调整`bootstrap_CI_optimized`函数以使用CUDA运行吗？将生成的函数命名为`bootstrap_CI_cuda`。'
- en: '**![image](../Images/Response.png)**To run the `bootstrap_CI_optimized` function
    on CUDA, you can use the cupy library, which provides GPU-accelerated versions
    of many NumPy functions. First, install the cupy library if you haven’t already:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**要在CUDA上运行`bootstrap_CI_optimized`函数，你可以使用提供许多NumPy函数GPU加速版本的cupy库。首先，如果你还没有安装，请安装cupy库：'
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, modify the `bootstrap_CI_optimized` function to use `cupy`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，修改`bootstrap_CI_optimized`函数以使用`cupy`：
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Please note that the `func` argument must also be compatible with cupy arrays.
    If the `func` relies on NumPy functions, you may need to replace them with their
    equivalent cupy functions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`func`参数也必须与cupy数组兼容。如果`func`依赖于NumPy函数，您可能需要用它们的等效cupy函数替换它们。
- en: Keep in mind that, depending on your problem size and GPU, you might not see
    significant performance improvements. GPU acceleration tends to be more beneficial
    for larger data and more computationally intensive operations.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，根据您的问题规模和GPU，您可能看不到显著的性能提升。GPU加速通常对大数据和计算密集型操作更有益。
- en: ChatGPT’s final remark certainly doesn’t bring a lot of optimism, especially
    after our parallelization effort. Nevertheless, we need to stop delaying the inevitable
    and move to the results presented in figure 7.5\. Fortunately, our effort this
    time was not in vain. We can see a significant improvement in the code execution
    time.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的最后一句话确实没有带来很多乐观，尤其是在我们的并行化努力之后。尽管如此，我们需要停止推迟不可避免的事情，转向图7.5中展示的结果。幸运的是，我们这次的努力并非徒劳。我们可以看到代码执行时间有显著改善。
- en: '![figure](../Images/CH07_F05_Siwiak3.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH07_F05_Siwiak3.png)'
- en: Figure 7.5 Dependence of the execution time of optimized (solid line) and GPU-utilizing
    (dashed line) bootstrapping functions on the size of the processed DataFrame
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5 优化（实线）和GPU利用（虚线）引导函数的执行时间与处理DataFrame大小的依赖关系
- en: To infinity and beyond
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 无限与超越
- en: GPUs are not the end of the line, at least not for all applications. Google
    designed hardware specialized for high-performance machine learning tasks, particularly
    deep learning, called Tensor Processing Units (TPUs).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: GPU并非所有应用的终点，至少对于某些应用来说不是。谷歌设计了一种专门用于高性能机器学习任务，尤其是深度学习的硬件，称为张量处理单元（TPUs）。
- en: TPUs are particularly well-suited for large-scale training and inference of
    neural networks, offering faster processing times and better energy efficiency
    compared to CPUs and GPUs. They can be especially beneficial for natural language
    processing, computer vision, and reinforcement learning applications. Incorporating
    TPUs into your data analytics pipeline can further optimize performance and reduce
    the time required to train and deploy complex machine learning models.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: TPUs特别适合大规模训练和推理神经网络，与CPU和GPU相比，提供更快的处理时间和更好的能源效率。它们对于自然语言处理、计算机视觉和强化学习应用尤其有益。将TPUs纳入您的数据分析流程可以进一步优化性能并减少训练和部署复杂机器学习模型所需的时间。
- en: Following the piece of advice from our last conversation with ChatGPT, you should
    be aware that many popular Python libraries already have counterparts optimized
    for GPUs and TPUs (Tensor Processing Units) or provide built-in support to accelerate
    computations. Keep that in the back of your mind if you’re planning to utilize
    GPUs/TPUs in your work.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们与ChatGPT的最后一次对话中的建议，您应该意识到许多流行的Python库已经针对GPU和TPUs（张量处理单元）优化，或者提供内置支持以加速计算。如果您计划在工作中使用GPU/TPUs，请记住这一点。
- en: There is no need to memorize all these optimized libraries, as you can ask ChatGPT
    to provide you with a list, as in the following transcript. Similarly, you can
    ask ChatGPT to reference other implementations with specific niche uses or optimizations.
    It could save you hours of looking through the web.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 没有必要记住所有这些优化库，因为您可以让ChatGPT提供列表，如下面的记录所示。同样，您可以要求ChatGPT引用具有特定用途或优化的其他实现。这可以节省您数小时在网上搜索的时间。
- en: '**![image](../Images/Init-MA.png)**Which Python libraries have GPU-optimized
    counterparts?'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**![图片](../Images/Init-MA.png)**哪些Python库有GPU优化的对应库？'
- en: '**![image](../Images/Response.png)**Many popular Python libraries have GPU-optimized
    counterparts or provide built-in GPU support to accelerate computations. Some
    of these libraries include:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**![图片](../Images/Response.png)**许多流行的Python库都有针对GPU优化的对应库或提供内置的GPU支持以加速计算。这些库包括：'
- en: 'CuPy: A GPU-accelerated version of NumPy that runs on NVIDIA CUDA. It has a
    very similar interface to NumPy, making it easy to port existing code to CuPy.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CuPy：NumPy的GPU加速版本，在NVIDIA CUDA上运行。它具有与NumPy非常相似的接口，使得将现有代码移植到CuPy变得容易。
- en: 'Numba: A Just-In-Time (JIT) compiler for Python functions that can generate
    GPU-accelerated code for NVIDIA CUDA-enabled GPUs.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Numba：Python函数的即时（JIT）编译器，可以为NVIDIA CUDA启用GPU生成GPU加速代码。
- en: 'SciPy:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: SciPy：
- en: 'CuPy: CuPy also provides GPU-accelerated versions of some SciPy functions,
    such as sparse matrix operations and linear algebra routines.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CuPy: CuPy 也提供了某些 SciPy 函数的 GPU 加速版本，例如稀疏矩阵运算和线性代数例程。'
- en: 'ArrayFire: A high-performance library that provides GPU-optimized versions
    of some SciPy functions for NVIDIA, AMD, and Intel GPUs.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ArrayFire: 一个高性能库，为 NVIDIA、AMD 和 Intel GPU 提供了某些 SciPy 函数的 GPU 优化版本。'
- en: 'Pandas:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pandas:'
- en: 'cuDF: A part of RAPIDS, cuDF is a GPU-accelerated library for data manipulation
    and analysis, providing a pandas-like API for handling data on NVIDIA CUDA-enabled
    GPUs.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'cuDF: 作为 RAPIDS 的一部分，cuDF 是一个用于数据操作和分析的 GPU 加速库，为处理 NVIDIA CUDA 兼容 GPU 上的数据提供了类似
    pandas 的 API。'
- en: 'Dask: While not strictly a GPU-optimized counterpart, Dask can work with cuDF
    to parallelize and distribute data processing tasks on GPUs.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dask: 虽然不是严格意义上的 GPU 优化对应版本，但 Dask 可以与 cuDF 一起工作，在 GPU 上并行化和分配数据处理任务。'
- en: 'Scikit-learn:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 'Scikit-learn:'
- en: 'cuML: Also part of RAPIDS, cuML is a GPU-accelerated library for machine learning
    that provides an API compatible with scikit-learn. It supports various algorithms
    like linear regression, k-means clustering, and principal component analysis,
    optimized for NVIDIA CUDA-enabled GPUs.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'cuML: 也是 RAPIDS 的一部分，cuML 是一个用于机器学习的 GPU 加速库，它提供了一个与 scikit-learn 兼容的 API。它支持各种算法，如线性回归、k-means
    聚类和主成分分析，针对 NVIDIA CUDA 兼容 GPU 进行优化。'
- en: 'Dask-ML: Dask-ML extends Dask to parallelize machine learning algorithms, including
    support for GPU-accelerated libraries like CuPy and cuDF.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dask-ML: Dask-ML 将 Dask 扩展到并行化机器学习算法，包括对 CuPy 和 cuDF 等GPU 加速库的支持。'
- en: 'TensorFlow and PyTorch:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 'TensorFlow 和 PyTorch:'
- en: Both TensorFlow and PyTorch are popular deep learning libraries that provide
    built-in support for GPU acceleration using NVIDIA CUDA and cuDNN libraries.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 和 PyTorch 都是流行的深度学习库，它们内置了对 NVIDIA CUDA 和 cuDNN 库的 GPU 加速支持。
- en: 'JAX:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'JAX:'
- en: JAX is a library for composable transformations of Python+NumPy programs, including
    automatic differentiation, vectorization, and parallelization. It has built-in
    support for GPU acceleration using NVIDIA CUDA and also supports TPUs.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX 是一个用于 Python+NumPy 程序可组合转换的库，包括自动微分、向量化和平行化。它内置了对 NVIDIA CUDA 的 GPU 加速支持，也支持
    TPUs。
- en: These are just a few examples of Python libraries with optimized counterparts
    or built-in GPU/TPU support. Depending on your specific use case and hardware,
    you might find other libraries that can leverage specialized chips for accelerated
    computation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是 Python 库中具有优化对应版本或内置 GPU/TPU 支持的几个例子。根据您的具体用例和硬件，您可能会发现其他可以利用专用芯片进行加速计算的库。
- en: ChatGPT learning cutoff
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ChatGPT 学习截止点
- en: As stated several times before, ChatGPT has limited knowledge beyond a certain
    point, depending on the version, so its knowledge of the latest developments and
    most recent versions of libraries may be limited. Nevertheless, it should provide
    a good starting point for further exploration.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述多次，ChatGPT 在某个点之后的知识有限，具体取决于版本，因此其对最新发展和库的最新版本的认知可能有限。尽管如此，它应该为进一步的探索提供一个良好的起点。
- en: Keep in mind that GPU/TPU optimizations can have varying levels of performance
    improvements, so it’s essential to profile and test your code on the target hardware
    to ensure that you’re getting the desired performance gains.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，GPU/TPU 优化可能具有不同级别的性能提升，因此对目标硬件进行性能分析和测试至关重要，以确保您获得期望的性能提升。
- en: Scaling across CPUs vs. GPUs
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 CPU 和 GPU 之间的扩展
- en: The decision to scale operations across CPUs or GPUs depends on the task’s nature
    and the available hardware resources. Here’s a general guideline for when to use
    CPUs and GPUs for different types of operations.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 决定在 CPU 或 GPU 之间扩展操作取决于任务的性质和可用的硬件资源。以下是一般指南，说明何时使用 CPU 和 GPU 进行不同类型的操作。
- en: Use CPUs for
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CPU 进行
- en: Sequential tasks or tasks that involve branching or complex control structures,
    or that require a limited amount of parallelism
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序任务或涉及分支或复杂控制结构或需要有限并行性的任务
- en: Tasks that have high I/O requirements or involve extensive communication between
    processes or threads
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要高 I/O 要求或涉及进程或线程之间大量通信的任务
- en: Tasks with a small or moderate dataset size, as, even though the available memory
    on CPUs is typically larger than that on GPUs, processing of large datasets on
    CPUs may be less effective than on GPUs.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于小或中等数据集大小的任务，尽管 CPU 上的可用内存通常大于 GPU，但在 CPU 上处理大型数据集可能不如在 GPU 上有效。
- en: General-purpose tasks, as CPUs are designed to handle a wide variety of workloads
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用任务，因为 CPU 设计用于处理各种工作负载
- en: Use GPUs for
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU进行
- en: Highly parallel tasks that can be divided into thousands of smaller tasks to
    be executed simultaneously
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以分成成千上万个较小的任务同时执行的高度并行任务
- en: Tasks involving many arithmetic operations, such as matrix multiplications,
    convolutions, or element-wise operations on large datasets
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涉及许多算术运算的任务，例如矩阵乘法、卷积或在大数据集上的逐元素操作
- en: Tasks with large dataset sizes, as GPUs are designed to handle massive amounts
    of data quickly due to their high memory bandwidth
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据集大小的任务，因为GPU设计用于快速处理大量数据，这得益于其高内存带宽
- en: Deep learning, computer vision, or scientific simulations, which can benefit
    from the specialized architecture of GPUs
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习、计算机视觉或科学模拟，这些都可以从GPU的专用架构中受益
- en: In general, if the task can be broken down into many simpler, preferably similar,
    subtasks, and requires a large amount of computation, it’s likely better suited
    for a GPU. However, if the task involves complex control structures, extensive
    communication, or is I/O-bound, it’s likely better suited for a CPU. Additionally,
    you need to consider the available hardware resources, the efficiency of the libraries
    being used, and your task’s specific requirements before deciding whether to use
    CPUs or GPUs for your operations.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果任务可以分解成许多更简单、最好是相似的子任务，并且需要大量的计算，那么它可能更适合使用GPU。然而，如果任务涉及复杂的控制结构、广泛的通信或I/O受限，那么它可能更适合使用CPU。此外，在决定是否使用CPU或GPU进行操作之前，你需要考虑可用的硬件资源、所使用库的效率以及你任务的具体要求。
- en: There is a very useful heuristic when deciding which approach to use. Unless
    you are prepared to rewrite large portions of the code to include parallelism,
    consider which libraries you use in your original code and whether they already
    have parallelized versions. If you are a heavy user of libraries with GPU-optimized
    counterparts and have the relevant GPU hardware available, it’s usually best to
    go with those. Otherwise, consider sticking with CPU parallelization.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定使用哪种方法时，有一个非常有用的启发式方法。除非你准备重写大量代码以包括并行性，否则考虑你在原始代码中使用的库，以及它们是否已经有了并行化版本。如果你是使用GPU优化库的重度用户，并且有相关的GPU硬件可用，通常最好选择这些库。否则，考虑坚持使用CPU并行化。
- en: You can also combine both approaches, switching to using GPU-optimized libraries
    in the sections of the code most involved with calling these and then parallelizing
    the rest of the code using multiple CPUs. Be careful, though. As we showed in
    section 7.2.2, any parallelization comes with overhead, especially when communication
    or synchronization between different CPU cores is involved. You may end up with
    overheads far exceeding any benefits.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以结合两种方法，在代码中与这些调用最相关的部分切换到使用GPU优化的库，然后使用多个CPU并行化其余的代码。但要注意。正如我们在7.2.2节中所示，任何并行化都伴随着开销，尤其是在涉及不同CPU核心之间的通信或同步时。你可能会遇到远超过任何好处的高开销。
- en: When you run out of PC cores, you may need to turn to cloud solutions. Those
    benefits come at the price of some extra work on your part to deploy your solution
    to your friendly cloud provider.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的PC核心用尽时，你可能需要转向云解决方案。这些好处需要你在部署解决方案到你的友好云提供商时做一些额外的工作。
- en: 7.3 Cloud-based deployment
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 基于云的部署
- en: As data continues to grow in volume, variety, and complexity, it may exceed
    the computing power of your PC machine. Instead of waiting unreasonably long to
    see the output (or a memory overflow error) or splashing out on the latest overpriced
    chips when they come into stock after being sold out to scalpers, you can reach
    into the vast power of cloud computing.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量、种类和复杂性的持续增长，它可能超过你的PC机器的计算能力。与其不合理地等待看到输出（或内存溢出错误），或者当 scalpers 售罄后，在最新的高价芯片上市时大举投资，你还可以利用云计算的巨大力量。
- en: 7.3.1 What is cloud computing?
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 什么是云计算？
- en: Cloud computing is a technology model that enables users to access and utilize
    computing resources, such as storage, processing power, and software applications,
    over the internet. Instead of investing in and maintaining physical hardware and
    software on-premises, users can leverage cloud service providers. The providers
    manage and maintain the infrastructure in large, remote data centers, providing
    on-demand access to a wide range of computing resources, storage, and managed
    services. You can scale your workload up or down as needed and analyze massive
    amounts of data without investing in expensive hardware.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算是一种技术模型，它允许用户通过互联网访问和使用计算资源，例如存储、处理能力和软件应用。用户无需在本地投资和维护物理硬件和软件，而是可以利用云服务提供商。提供商在大型、远程数据中心管理和维护基础设施，提供对广泛计算资源、存储和管理服务的按需访问。您可以根据需要扩展或缩减工作量，并分析大量数据，而无需投资昂贵的硬件。
- en: The main advantages of cloud computing include scalability, flexibility, cost-efficiency,
    and reliability. You can also choose to use exotic architectures like TPUs (or
    GPUs) from the safe confines of your laptop.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算的主要优势包括可扩展性、灵活性、成本效益和可靠性。您还可以选择在笔记本电脑的安全环境中使用异构架构，如TPUs（或GPU）。
- en: 1+1 ≠ 2
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 1+1 ≠ 2
- en: Multiple interconnected computers, known as nodes, working together as a single
    unified system to perform complex computational tasks are referred to as *clusters*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 多个相互连接的计算机，称为节点，作为一个单一统一的系统协同工作以执行复杂的计算任务，这些系统被称为*集群*。
- en: In cluster computing, tasks are often divided into smaller subtasks and distributed
    across the nodes in the cluster. The nodes then work in parallel, processing their
    assigned subtasks simultaneously, with specific processes providing synchronization
    and fault-tolerance to the whole system. Since this is a complex topic, we won’t
    be delving deeper into these issues, assuming instead that users will be more
    likely to utilize one of the established frameworks that provide solutions to
    these issues than write their own.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群计算中，任务通常被划分为更小的子任务，并在集群的节点之间分配。然后节点并行工作，同时处理分配给它们的子任务，特定的进程为整个系统提供同步和容错。由于这是一个复杂的话题，我们不会深入探讨这些问题，而是假设用户更有可能利用提供解决方案的既定框架，而不是自己编写。
- en: Common cluster computing software frameworks include Apache Hadoop, Apache Spark,
    and the Message Passing Interface (MPI), which provide tools and libraries to
    simplify the development, deployment, and management of distributed applications
    across cluster nodes.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的集群计算软件框架包括Apache Hadoop、Apache Spark和消息传递接口（MPI），它们提供工具和库以简化跨集群节点的分布式应用程序的开发、部署和管理。
- en: The code we used as an example to compare performance didn’t turn out to be
    the best candidate for parallelization. In this section, we will work with a function
    calculating straight-line (or Euclidean) distances between sets of points. It
    may not be very useful for most business cases, but it parallelizes beautifully—each
    pair’s distance can be calculated independently, so no communication between processors
    or nodes will be required, apart from gathering the final results.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用作性能比较示例的代码并没有成为并行化的最佳候选者。在本节中，我们将使用一个计算点集之间直线（或欧几里得）距离的函数。这可能对大多数商业案例不太有用，但它并行化得很好——每一对的距离可以独立计算，因此除了收集最终结果之外，不需要处理器或节点之间的通信。
- en: 7.3.2 Moving your code to the cloud
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 将代码迁移到云端
- en: 'The simplest cloud computing use case is to deploy your code to a virtual machine
    (VM) in the cloud. The VM is like a computer within a computer, with its own operating
    system and all. It acts as an isolated space, enabling you to test software, run
    different applications, or work in separate environments without affecting the
    main system. You run your analytics on a VM just as you would on your local PC.
    ChatGPT can provide you with complete instructions:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的云计算用例是将您的代码部署到云中的虚拟机（VM）上。虚拟机就像一台计算机中的计算机，拥有自己的操作系统等一切。它作为一个隔离的空间，使您能够在不影响主系统的情况下测试软件、运行不同的应用程序或在不同环境中工作。您在虚拟机上运行分析的方式与在本地PC上运行的方式相同。ChatGPT可以为您提供完整的说明：
- en: '**![image](../Images/Init-MA.png)**How can I deploy my calculation to the cloud?
    Provide me with a walkthrough.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**![图片](../Images/Init-MA.png)**如何将我的计算部署到云端？请为我提供操作指南。'
- en: Since the exact steps change frequently, we’ve decided to omit the lengthy transcript
    here. It should suffice to say that ChatGPT can provide detailed walkthroughs,
    but only up to the point where the provider changes the interface and the steps
    differ. Caveat lector (reader beware).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于具体步骤频繁变化，我们决定在此省略冗长的记录。只需说明，ChatGPT可以提供详细的操作指南，但仅限于服务提供商更改界面和步骤不同的点。读者注意（读者小心）。
- en: '**![image](../Images/Response.png)**Depending on your specific use case and
    requirements, you might need to customize the environment, configure additional
    resources, or use other cloud services, such as managed data processing services
    or serverless computing options like Google Cloud Functions or AWS Lambda.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**根据您的具体用例和需求，您可能需要自定义环境、配置额外资源或使用其他云服务，例如托管数据处理服务或无服务器计算选项，如Google
    Cloud Functions或AWS Lambda。'
- en: You can ask ChatGPT about the details of each of these steps.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以询问ChatGPT关于这些步骤的详细信息。
- en: ChatGPT’s narrow context
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ChatGPT的狭窄上下文
- en: You’ll notice that, in the discussion with ChatGPT on the deployment of code
    to the cloud, the response only provided a way to run the code on a single virtual
    machine, since the prompt didn’t specify anything otherwise. This kind of deployment
    is usually the first step in moving your application to the cloud, so it’s still
    worth looking at, although this will usually provide only a moderate increase
    in computational power, limited by the power of a single VM, while having the
    significant benefit of requiring no changes to the code.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，在与ChatGPT讨论将代码部署到云端的过程中，回答只提供了一个在单个虚拟机上运行代码的方法，因为提示没有指定其他内容。这种部署通常是将您的应用程序迁移到云端的第一个步骤，所以它仍然值得一看，尽管这通常只会提供有限的计算能力提升，受限于单个虚拟机的性能，但具有不需要修改代码的显著优势。
- en: 'Finally, let’s look at how ChatGPT can help us take our code into the cloud,
    this time properly distributing it across multiple cloud resources so that we
    can harness the vast power of cluster computing. Let’s look at the solution proposed
    by ChatGPT:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看ChatGPT如何帮助我们把我们代码迁移到云端，这次是正确地跨多个云资源进行分配，以便我们能够利用集群计算强大的能力。让我们看看ChatGPT提出的解决方案：
- en: ChatGPT is not loyal to one platform
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ChatGPT对任何平台都不忠诚
- en: In the responses in this section, you’ll see ChatGPT starting to answer using
    Amazon AWS. The prompt did not specify the choice of the cloud provider, although
    we have discussed using Google Cloud before. Don’t be afraid to stop ChatGPT if
    you don’t like where it is going and redirect it on a path you prefer by refining
    the prompt.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中的回复中，您将看到ChatGPT开始使用Amazon AWS来回答问题。提示并未指定选择云服务提供商，尽管我们之前讨论过使用Google Cloud。如果您不喜欢ChatGPT的走向，不要害怕停止它，并通过细化提示将其引导到您更喜欢的路径上。
- en: '**![image](../Images/Init-MA.png)**I now want to distribute the calculation
    across the cloud. How can I do that? (Forget saving to disk.)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**我现在想将计算分布到云端。我该如何操作？（忘记保存到磁盘。）'
- en: '**![image](../Images/Response.png)**To distribute the calculation across the
    cloud, you can use Dask along with a cloud provider like Amazon Web Services (AWS),
    Google Cloud Platform (GCP), or Microsoft Azure. In this example, we’ll use Dask
    with AWS, but the process is similar for other cloud providers.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**要跨云端分布计算，您可以使用Dask以及像Amazon Web Services
    (AWS)、Google Cloud Platform (GCP)或Microsoft Azure这样的云服务提供商。在这个例子中，我们将使用Dask与AWS，但对于其他云服务提供商，过程是相似的。'
- en: First, you’ll need to set up a Dask cluster on AWS. You can follow the official
    Dask guide for setting up a Dask cluster on AWS using the dask-cloudprovider package.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要在AWS上设置一个Dask集群。您可以根据官方Dask指南使用dask-cloudprovider包在AWS上设置Dask集群。
- en: Once you’ve set up your Dask cluster
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您设置了您的Dask集群
- en: We stopped the response generation at this point.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此处停止了回答的生成。
- en: '**![image](../Images/Init-MA.png)**Please change the last answer to use GCP'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**请将最后一个答案改为使用GCP'
- en: '**![image](../Images/Response.png)**To distribute the calculation across the
    cloud using Google Cloud Platform (GCP), you can use Dask along with the dask-cloudprovider
    package to create and manage Dask clusters on GCP.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**要使用Google Cloud Platform (GCP)在云端分布计算，您可以使用Dask以及dask-cloudprovider包来创建和管理GCP上的Dask集群。'
- en: <Here, we again received an answer applicable at the time of ChatGPTs knowledge
    cutoff.>
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: <在这里，我们再次收到了适用于ChatGPT知识截止日期的回答。>
- en: The rest of the code is similar to the previous examples but now uses GCP cluster
    as the underlying infrastructure. The key point here is that there are ready-made
    solutions, as mentioned in the previous sections, which can abstract away the
    technicalities of cloud or cluster computing and allow you to focus on coding
    the core problem rather than worrying whether the network communication is going
    to work correctly. Frameworks like Dask can provide these solutions and even allow
    you to switch almost seamlessly between different low-level providers like AWS
    or GCP.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码与之前的示例类似，但现在使用GCP集群作为底层基础设施。这里的关键点是，正如前几节中提到的，有现成的解决方案，这些解决方案可以抽象出云或集群计算的技术细节，并允许你专注于编码核心问题，而不是担心网络通信是否能够正确工作。像Dask这样的框架可以提供这些解决方案，甚至允许你几乎无缝地在不同的底层提供者（如AWS或GCP）之间切换。
- en: It’s important to remember that by submitting code and prompts requesting different
    iterations of code, we may end up with an implementation containing a mixture
    of the different solutions, or we may end up with totally different functionality.
    Whatever modifications we request ChatGPT to make to the code, it’s vital that
    it is retested with the same, or even higher, scrutiny as if it were written from
    scratch. Furthermore, since parallelized or distributed code is usually much harder
    to test fully than a localized single-threaded version, care should be taken to
    apply best practices, which are beyond the scope of this book.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，通过提交代码和请求不同迭代版本的提示，我们可能会得到一个包含不同解决方案混合的实现，或者我们可能会得到完全不同的功能。无论我们请求ChatGPT对代码进行何种修改，都必须确保它以与从头开始编写时相同的，甚至更高的标准进行重新测试。此外，由于并行化或分布式代码通常比本地化单线程版本更难完全测试，因此应采取最佳实践，这些实践超出了本书的范围。
- en: 7.4 Code conversion
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 代码转换
- en: 'Finally, let’s look at using ChatGPT to convert code between programming languages.
    This can serve multiple purposes, such as generating optimized code or adapting
    code to work within specific frameworks like Hadoop. When translating code from
    one language to another, developers have the opportunity to refactor, streamline,
    and improve the code in the target language. It can lead to optimization in several
    ways. Let’s look at some of these:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看如何使用ChatGPT在编程语言之间转换代码。这可以服务于多个目的，例如生成优化代码或使代码适应在特定框架（如Hadoop）中工作。当将代码从一种语言翻译到另一种语言时，开发者有机会在目标语言中对代码进行重构、精简和改进。这可以通过几种方式实现优化。让我们看看其中的一些：
- en: '*Improved performance**—*Converting code to a more efficient language or one
    that offers better support for specific hardware or platforms can lead to performance
    gains. For example, translating code from a high-level interpreted language like
    Python to a lower-level compiled language like C++ can result in faster execution
    times.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提高性能**—*将代码转换为更高效的语言或提供更好特定硬件或平台支持的代码可以带来性能提升。例如，将代码从高级解释语言（如Python）翻译到低级编译语言（如C++）可能会导致更快的执行时间。'
- en: '*Reduced memory usage**—*Code conversion can lead to more memory-efficient
    implementations. For instance, certain data structures and algorithms in the target
    language might consume less memory than their counterparts in the source language.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*减少内存使用**—*代码转换可能导致更高效的实现。例如，目标语言中的某些数据结构和算法可能比源语言中的对应项消耗更少的内存。'
- en: '*Better maintainability**—*Converting code to a more modern or widely used
    language can improve maintainability by leveraging newer language features, libraries,
    and community support. This can make the code easier to read, understand, and
    modify in the long run.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更好的可维护性**—*将代码转换为更现代或更广泛使用的语言可以通过利用更新的语言特性、库和社区支持来提高可维护性。这可以使代码在长期内更容易阅读、理解和修改。'
- en: '*Framework compatibility**—*Ensuring code compatibility with different frameworks
    often requires converting it to a supported language. For instance, in the next
    section, we’ll be discussing distributing the processing to the cloud, and one
    of the options would be to use Hadoop. However, when working with Hadoop, you
    might need to convert a Python script to Java or Scala to utilize the framework’s
    ecosystem efficiently. This ensures seamless integration with Hadoop’s Java-based
    libraries and tools.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*框架兼容性**—*确保代码与不同框架的兼容性通常需要将其转换为支持的语言。例如，在下一节中，我们将讨论将处理分配到云中，其中一个选项将是使用Hadoop。然而，当与Hadoop一起工作时，您可能需要将Python脚本转换为Java或Scala，以便有效地利用框架的生态系统。这确保了与基于Java的Hadoop库和工具的无缝集成。'
- en: As the frameworks and compatibility restrictions change fairly often, it is
    best to check the current documentation online to make informed decisions regarding
    which framework is the best option for your specific implementation.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 由于框架和兼容性限制经常发生变化，最好是在线查看当前文档，以便就哪个框架最适合您的特定实现做出明智的决定。
- en: It’s essential to carefully consider the cost and complexity of code conversion,
    as it can be time-consuming and may introduce new bugs or issues, and manual code
    conversion is rarely worth the effort. Although automated tools for code conversion
    have existed before, the use of models like ChatGPT has made code conversion much
    easier and may actually bring sizable benefits even with significant chunks of
    code.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细考虑代码转换的成本和复杂性是至关重要的，因为这可能会耗费大量时间，并可能引入新的错误或问题，而且手动代码转换很少值得付出这样的努力。尽管在ChatGPT出现之前就已经存在代码转换的自动化工具，但使用ChatGPT这样的模型使得代码转换变得更加容易，甚至对于大量代码也可能带来显著的益处。
- en: Limited trust still applies
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 仍然需要有限的信任
- en: Please remember that even though it may look like the code conversion is easy,
    thorough testing and performance monitoring are still necessary to ensure the
    translated code works as expected and meets the desired optimization goals. Techniques
    linked to unit testing, system testing, and integration testing, which are beyond
    the scope of this book, should be applied both before and after changing the code
    to reduce the risk of bugs creeping in.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，尽管代码转换看起来可能很简单，但彻底的测试和性能监控仍然是必要的，以确保转换后的代码按预期工作并满足所需的优化目标。本书范围之外的单元测试、系统测试和集成测试等技术应在更改代码前后应用，以降低错误渗入的风险。
- en: Let’s look at some examples of code conversion using ChatGPT. Imagine you wrote
    the code in Python for one of your analyses but are now looking to reuse the same
    concept in a different part of the organization that has everything else written
    in Java. Although there are solutions for interoperability between the two languages,
    converting the code directly to Java might be the simplest solution.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使用ChatGPT进行代码转换的一些示例。假设您为您的分析之一编写了Python代码，但现在您希望在不同的组织部分重用相同的概念，而该组织其他所有内容都是用Java编写的。尽管存在两种语言之间互操作性的解决方案，但直接将代码转换为Java可能是最简单的解决方案。
- en: '**![image](../Images/Init-MA.png)**Take this code and convert it into Java:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**将此代码转换为Java：'
- en: '[PRE9]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**![image](../Images/Response.png)**Here’s the equivalent Java code for the
    provided Python function:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**以下是提供的Python函数的等效Java代码：'
- en: '[PRE10]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This code uses the Apache Commons Math library for the mean calculation, replacing
    the custom mean calculation from the previous example. Adjust the `calculateStatistics`
    method if you have a different custom statistics calculation logic.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用Apache Commons Math库进行均值计算，取代了之前示例中的自定义均值计算。如果您有不同的自定义统计计算逻辑，请调整`calculateStatistics`方法。
- en: It may be interesting at this point to investigate how much ChatGPT “understands”
    the code it is converting. It’s interesting to see how ChatGPT might be able to
    conceptualize the function of the code, which may lead to more efficient conversion
    with the right prompting.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，研究ChatGPT“理解”它正在转换的代码的程度可能很有趣。看看ChatGPT如何概念化代码的功能，这可能会导致在适当的提示下进行更有效的转换。
- en: '**![image](../Images/Init-MA.png)**What does this code do?'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**这段代码做了什么？'
- en: '**![image](../Images/Response.png)**This code defines a Java utility class,
    `BootstrapUtils`, that provides a method `bootstrapCI` for calculating bootstrap
    confidence intervals (CI) for a given dataset. Let’s break down the code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**这段代码定义了一个Java实用类`BootstrapUtils`，它提供了一个名为`bootstrapCI`的方法，用于计算给定数据集的引导置信区间（CI）。让我们来分析一下这段代码：'
- en: 'Imports:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入：
- en: The code imports classes from the Apache Commons Math library for statistical
    calculations, including random data generation and percentile calculation.
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码从Apache Commons Math库导入类，用于统计计算，包括随机数据生成和百分位数计算。
- en: '`BootstrapUtils` Class:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`BootstrapUtils` 类：'
- en: 'Contains two methods: `bootstrapCI` and `calculateStatistics`.'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包含两个方法：`bootstrapCI` 和 `calculateStatistics`。
- en: '`bootstrapCI` Method:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`bootstrapCI` 方法：'
- en: 'Takes three parameters: an array of data, the number of bootstrap samples (`nBootstraps`),
    and the confidence interval percentage (`ci`).'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受三个参数：数据数组、重采样样本数（`nBootstraps`）和置信区间百分比（`ci`）。
- en: Creates a random data generator and a `Percentile` object with a specific estimation
    type.
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个随机数据生成器和具有特定估计类型的 `Percentile` 对象。
- en: Initializes an array to store bootstrap statistics and gets the size of the
    input data.
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个数组来存储重采样统计信息，并获取输入数据的大小。
- en: 'Bootstrap Sampling:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重采样：
- en: Iterates `nBootstraps` times.
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代 `nBootstraps` 次。
- en: For each iteration, it creates a bootstrap sample by randomly selecting data
    points from the original dataset.
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每次迭代，它通过从原始数据集中随机选择数据点来创建一个重采样样本。
- en: 'Statistics Calculation:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 统计计算：
- en: Calls the `calculateStatistics` method to calculate the statistics of each bootstrap
    sample. The default implementation uses the mean calculation from the Apache Commons
    Math library.
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `calculateStatistics` 方法来计算每个重采样样本的统计信息。默认实现使用Apache Commons Math库中的均值计算。
- en: 'Confidence Interval Calculation:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 置信区间计算：
- en: Uses the `Percentile` object to calculate the lower and upper bounds of the
    confidence interval based on the percentiles of the bootstrapped statistics.
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Percentile` 对象根据重采样统计的百分位数来计算置信区间的下限和上限。
- en: 'Return:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回：
- en: Returns an array containing the lower and upper bounds of the bootstrap confidence
    interval.
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回一个包含重采样置信区间下限和上限的数组。
- en: '`calculateStatistics` Method:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`calculateStatistics` 方法：'
- en: A private method for calculating statistics on a given array. The default implementation
    uses the mean calculation from the Apache Commons Math library.
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个用于在给定数组上计算统计信息的私有方法。默认实现使用Apache Commons Math库中的均值计算。
- en: '`main` Method (Example Usage):'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`main` 方法（示例用法）：'
- en: Demonstrates how to use the `bootstrapCI` method with an example dataset (`data`).
    It calculates the bootstrap confidence interval with 1000 bootstraps and a 95%
    confidence interval.
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过示例数据集（`data`）演示如何使用 `bootstrapCI` 方法。它使用1000次重采样和95%的置信区间来计算重采样置信区间。
- en: In summary, this Java code provides a flexible and reusable utility for calculating
    bootstrap confidence intervals. You can replace the `calculateStatistics` method
    with your custom statistics calculation logic if needed. The code is well-organized
    and modular, making it easy to understand and extend for different statistical
    calculations.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这段Java代码提供了一个灵活且可重用的实用工具，用于计算重采样置信区间。如果需要，你可以用你自定义的统计计算逻辑替换 `calculateStatistics`
    方法。代码组织良好且模块化，便于理解和扩展以进行不同的统计计算。
- en: Now that we’ve extracted the function of the code from ChatGPT, we can ask it
    for a more optimal solution in our target language. Please note that the prompt
    asking ChatGPT for the description of the function of the code is important in
    the flow of the conversation, as it steers ChatGPT toward the correct understanding
    of our intention to convert the function of the code rather than translate it
    instruction-by-instruction. Furthermore, it allows us to check if ChatGPT’s understanding
    of the function is correct. If it were not correct, we might adapt subsequent
    prompts to steer it toward the right output. In our case, it deduced the function
    correctly, so we can just ask it for an optimized solution in the next prompt.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从ChatGPT中提取了代码的功能，我们可以请求它为我们目标语言提供一个更优化的解决方案。请注意，向ChatGPT请求代码功能描述的提示在对话流程中很重要，因为它引导ChatGPT正确理解我们的意图，即转换代码的功能而不是逐条指令翻译。此外，它还允许我们检查ChatGPT对功能的理解是否正确。如果不正确，我们可能需要调整后续提示，以引导它指向正确的输出。在我们的案例中，它正确地推断出了功能，因此我们可以在下一个提示中请求它提供一个优化的解决方案。
- en: '**![image](../Images/Init-MA.png)**Can this be done using some optimized implementation?'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**可以使用某种优化实现来完成吗？'
- en: '**![image](../Images/Response.png)**Certainly! If you’re looking for optimized
    implementations, Java has libraries that provide efficient statistical calculations.
    One such library is the Apache Commons Math library. Here’s the modified code
    using Apache Commons Math for both bootstrap sampling and statistics calculation:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**![图片](../Images/Response.png)**'
- en: '[PRE11]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This version uses `org.apache.commons.math3.stat.StatUtils.mean` for the mean
    calculation, which is an optimized implementation from Apache Commons Math. Make
    sure to include the Apache Commons Math library in your project dependencies.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本使用`org.apache.commons.math3.stat.StatUtils.mean`进行平均值计算，这是Apache Commons
    Math的一个优化实现。确保将Apache Commons Math库包含在你的项目依赖中。
- en: In the preceding exchange, ChatGPT has translated the code directly. Further
    interrogation provided even better alternatives, such as by asking ChatGPT to
    replace parts of the code with optimized implementations from other libraries
    available for Java, although it’s up to you to explore this. Don’t leave it to
    the machine to do *all* the work.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的交流中，ChatGPT直接翻译了代码。进一步的询问提供了更好的替代方案，例如，通过要求ChatGPT用Java可用的其他库中的优化实现替换代码的一部分，尽管这取决于你自己去探索。不要让机器做所有的工作。
- en: By this point, we trust you’ve gained an understanding of how you can use generative
    AI to choose, implement, and assess various approaches to enhance the efficiency
    of the code employed in your analysis. However, it’s crucial to exercise discretion
    in choosing your optimization battles. If your analysis is a single, ad hoc occurrence,
    the time required to achieve optimal performance might surpass the potential time
    savings.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们相信你已经了解了如何使用生成式AI来选择、实施和评估各种方法来提高你在分析中使用的代码的效率。然而，在选择优化策略时，谨慎行事至关重要。如果你的分析是一次性的、临时的，那么达到最佳性能所需的时间可能会超过潜在的时间节省。
- en: Things to ask ChatGPT
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 询问ChatGPT的事项
- en: Modify my code to run on multiple GPUs/CPUs.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改我的代码使其能在多个GPU/CPUs上运行。
- en: Convert this snippet I downloaded from StackOverflow to my favorite programming
    language.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我从StackOverflow下载的代码片段转换为我最喜欢的编程语言。
- en: Explain what this code snippet does.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释这个代码片段的功能。
- en: Modify my code to run on the Amazon/Google/Microsoft Azure cloud.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改我的代码使其能在亚马逊/谷歌/微软Azure云上运行。
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Don’t try to re-implement common algorithms, as there are usually optimized
    implementations available.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要尝试重新实现常见的算法，因为通常有可用的优化实现。
- en: If you want to use a different programming language, ChatGPT can significantly
    simplify migration. Just remember to retest the code thoroughly.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想要使用不同的编程语言，ChatGPT可以显著简化迁移。只需记住要彻底重新测试代码。
- en: Code should be optimized to achieve results as quickly as possible on a given
    platform. If this is not fast enough, you can scale up by using multiple CPUs
    or GPUs or deploying to the cloud.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码应该优化以在给定平台上尽可能快地实现结果。如果这还不够快，你可以通过使用多个CPU或GPU或将代码部署到云上来进行扩展。
- en: Frameworks like Dask can help you achieve scaling with minimum modifications
    to your code.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于Dask这样的框架可以帮助你在最小修改代码的情况下实现扩展。
