- en: Chapter 7\. Advanced Fine-Tuning Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章：高级微调技术
- en: In the previous chapter, we presented the canonical way to fine-tune a typical
    LLM. In the real world, there are a wide variety of motivations for updating an
    LLM, and similarly there are multiple ways to update it. In this chapter, we will
    describe several advanced fine-tuning techniques and highlight the scenarios in
    which each technique would be suitable.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了微调典型LLM的规范方法。在现实世界中，更新LLM有多种动机，同样也有多种更新方法。在本章中，我们将描述几种高级微调技术，并强调每种技术适用的场景。
- en: 'Why would you want to update the parameters of an LLM? We touched upon this
    in previous chapters but let’s go through it in more detail now:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你为什么想更新LLM的参数？我们已经在之前的章节中提到了这一点，但现在让我们更详细地探讨一下：
- en: Domain adaptation
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 领域自适应
- en: The data that we work with belongs to a specialized domain that the LLM might
    not have been familiarized with during pre-training. In this case, we would like
    to update the model by training it on domain-specific data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们处理的数据属于一个LLM在预训练期间可能没有熟悉的特定领域。在这种情况下，我们希望通过在特定领域的数据上训练来更新模型。
- en: Task adaptation
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 任务自适应
- en: We care about LLM performance on specific downstream tasks. To improve the LLM’s
    performance on these tasks, we can train it on task-specific data. This can be
    supervised or unsupervised.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注LLM在特定下游任务上的性能。为了提高LLM在这些任务上的性能，我们可以在特定任务的数据上对其进行训练。这可以是监督学习或无监督学习。
- en: Knowledge updating
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 知识更新
- en: We would like to keep the LLM’s knowledge up-to-date by continually training
    it on new data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过不断用新数据对其进行训练，以保持LLM（大型语言模型）的知识是最新的。
- en: Controllability/steerability
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 可控性/可引导性
- en: We would like to control the behavior of the LLM, including making it more likely
    to follow user requests written in natural language, reject certain types of requests,
    and so on. Techniques to achieve this are collectively called alignment training.
    We will defer discussion of alignment training to [Chapter 8](ch08.html#ch8).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望控制LLM的行为，包括使其更有可能遵循用自然语言编写的用户请求，拒绝某些类型的请求等。实现这一目标的技术统称为对齐训练。我们将把对齐训练的讨论推迟到第8章。
- en: 'In this chapter, we will learn techniques that can be used to update the LLM
    for the aforementioned reasons. To this end, the chapter is divided into three
    sections:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习可用于更新LLM以实现上述目的的技术。为此，本章分为三个部分：
- en: Continual pre-training
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 持续预训练
- en: Primarily used for domain adaptation and keeping the knowledge of the LLM up-to-date
    (the latter is also called lifelong-learning).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 主要用于领域自适应和保持LLM的知识更新（后者也称为终身学习）。
- en: Parameter-Efficient Fine-Tuning (PEFT)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）
- en: A set of fine-tuning techniques that make the fine-tuning process more efficient
    by updating only a small number of model parameters, thus needing less memory
    and compute.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一组微调技术，通过仅更新少量模型参数来使微调过程更高效，从而需要更少的内存和计算资源。
- en: Model merging/model fusion
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型合并/模型融合
- en: An exciting new subfield of LLMs that explores combining the parameters of two
    or more models. I call this the “dark arts” of NLP, as it is poorly understood
    but uncannily effective if done the right way.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个令人兴奋的新子领域，探索结合两个或更多模型的参数。我称这为NLP的“黑暗艺术”，因为它理解得不好，但如果做得正确，效果惊人。
- en: 'Let’s begin with my personal favorite: continual pre-training!'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我最喜欢的开始：持续预训练！
- en: Continual Pre-Training
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续预训练
- en: The premise of continual pre-training is simple. Take a pre-trained model checkpoint
    and continue pre-training it with your own data. But why would you want to do
    that? Here are some scenarios where continual pre-training can help.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 持续预训练的前提很简单。取一个预训练模型检查点，并继续用你自己的数据对其进行预训练。但为什么你想这么做呢？以下是一些持续预训练可以帮助的场景。
- en: You work in a specialized domain like law, finance, or biomedical. In each of
    these cases, text belonging to these domains differs linguistically and structurally
    from naturally occurring English text. For example, legal text is characterized
    by long sentences written in a formal tone, containing jargon specific to the
    legal domain. Financial text is interspersed with a lot of numbers. Both legal
    and financial text contain a significant proportion of boilerplate text. Biomedical
    text contains a lot of scientific terms that are not part of the standard English
    vocabulary. In all these cases, you would like to pre-train your LLM on domain-specific
    data so that the LLM is exposed to the nuances and characteristics of domain-specific
    text. This is called *domain-adaptive pre-training (DAPT)*.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在法律、金融或生物医学等特定领域工作。在这些情况下，这些领域的文本在语言和结构上都与自然发生的英语文本不同。例如，法律文本以正式的语调写成的长句为特征，包含法律领域的特定术语。金融文本中穿插着许多数字。法律和金融文本都包含相当比例的模板文本。生物医学文本包含许多不属于标准英语词汇的科学术语。在这些所有情况下，你希望在你选择的领域特定数据上预训练你的LLM，以便LLM接触到领域特定文本的细微差别和特征。这被称为*领域自适应预训练（DAPT）*。
- en: Taking DAPT one step further, you can also continue pre-training your model
    not just on general text from your domain of interest but also on domain text
    specifically related to your downstream tasks. This is called *task-adaptive pre-training
    (TAPT)*.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将DAPT进一步发展，你还可以继续在不仅是从你感兴趣的领域的一般文本，而且是从与你的下游任务具体相关的领域文本上预训练你的模型。这被称为*任务自适应预训练（TAPT）*。
- en: Your LLM is a reservoir of knowledge. But this knowledge can become obsolete
    over time. To keep its knowledge up-to-date, you continue pre-training the model
    at regular time periods or when new data is available. This is called *life-long
    learning*.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的LLM是一个知识的宝库。但这个知识可能会随着时间的推移而变得过时。为了保持其知识的时效性，你会在定期的时间间隔或新数据可用时继续预训练模型。这被称为*终身学习*。
- en: Note
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You might be thinking, “If I want a domain-specific LLM, why don’t I just take
    my domain-specific data and train an LLM from scratch?” Well, you can, but your
    LLM just won’t be as performant, and the exercise will cost a whole lot more than
    continual pre-training. LLMs learn a wide variety of linguistic capabilities that
    might not be able to be learned from domain-specific text alone. Therefore, it
    is better to take an already pre-trained LLM that was trained on general text
    and then continue pre-training it with domain-specific text.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，“如果我想得到一个特定领域的LLM，为什么我不直接用我的领域特定数据从头开始训练一个LLM呢？”好吧，你可以，但你的LLM的表现可能不会那么好，而且这项练习的成本将远远高于持续预训练。LLM学习了许多广泛的言语能力，这些能力可能无法仅从领域特定文本中学习到。因此，最好是采用已经预训练的LLM，该LLM是在通用文本上训练的，然后继续用领域特定文本对其进行预训练。
- en: In practice, continual pre-training is a challenging exercise. This is due to
    the phenomenon of catastrophic forgetting, where the LLM *forgets* its previously
    learned capabilities and knowledge when it continues to be trained on new and
    different data. We will soon explore various techniques to combat the catastrophic
    forgetting problem.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，持续预训练是一项具有挑战性的练习。这是由于灾难性遗忘现象，当LLM继续在新和不同的数据上训练时，它会*忘记*之前学到的能力和知识。我们将很快探讨各种对抗灾难性遗忘问题的技术。
- en: How does continual pre-training differ from fine-tuning? The differences are
    mostly cosmetic and terminology-related. Just like pre-training, continual pre-training
    is self-supervised, while we typically use the term fine-tuning when we use supervised
    datasets. Continual pre-training uses the same (but not necessarily) learning
    objective as the one used in the original pre-training setup. Finally, continual
    pre-training datasets are usually orders of magnitude larger than typical fine-tuning
    datasets.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 持续预训练与微调有何不同？差异主要在于外观和术语相关。就像预训练一样，持续预训练是自监督的，而我们通常在用监督数据集时使用微调这个术语。持续预训练使用与原始预训练设置中使用的相同（但不一定）的学习目标。最后，持续预训练数据集通常比典型的微调数据集大得多。
- en: '[Figure 7-1](#continual-pt) depicts the general continual pre-training process.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-1](#continual-pt)展示了一般的持续预训练过程。'
- en: '![continual-pt](assets/dllm_0701.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![continual-pt](assets/dllm_0701.png)'
- en: Figure 7-1\. Illustration of the continual pre-training process
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 持续预训练过程的示意图
- en: This book’s [GitHub repo](https://oreil.ly/llm-playbooks) contains a tutorial
    for continual pre-training. This setup is no different than fine-tuning, except
    that the dataset is not labeled (self-supervised training), and the dataset is
    orders of magnitude larger than typical fine-tuning datasets.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本书[GitHub仓库](https://oreil.ly/llm-playbooks)包含持续预训练的教程。这种设置与微调没有不同，只是数据集未标记（自监督训练），并且数据集的大小比典型的微调数据集大得多。
- en: 'As mentioned earlier, naive continual pre-training leads to catastrophic forgetting
    of capabilities and knowledge learned previously. Several techniques exist to
    alleviate this issue:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，简单的持续预训练会导致先前学习的能力和知识的灾难性遗忘。存在几种技术可以减轻这个问题：
- en: Replay (memory)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重放（记忆）
- en: Uses training examples from the original pre-training and mixes them with the
    new training data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原始预训练的训练示例，并将它们与新训练数据混合。
- en: Distillation
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏
- en: Takes an older checkpoint of the model and during training compares the KL-divergence
    between the older and the current representations and penalizes it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，取模型的较旧检查点，并比较较旧表示和当前表示之间的KL散度，并对其惩罚。
- en: Regularization
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化
- en: Penalizes large changes to the parameters during continual training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在持续训练期间惩罚参数的大幅变化。
- en: Parameter expansion
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 参数扩展
- en: Adds more parameters to the model as continual pre-training is performed. This
    can be done by increasing either the width or the depth of the model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在持续预训练过程中向模型添加更多参数。这可以通过增加模型的宽度或深度来完成。
- en: For a more comprehensive set of continual learning techniques, check out [Jin
    et al.’s paper](https://oreil.ly/yNa-H). In this chapter, we will dive deeper
    into replay and parameter expansion methods.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更全面的持续学习技术集，请参阅Jin等人发表的论文（https://oreil.ly/yNa-H）。在本章中，我们将更深入地探讨重放和参数扩展方法。
- en: Replay (Memory)
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重放（记忆）
- en: Replay-based techniques are one of the simplest techniques to alleviate catastrophic
    forgetting. In this approach, we store pre-training examples from the original
    dataset and interleave them with the continual training dataset. Thus, the data
    drift is not so pronounced.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于重放的技术是减轻灾难性遗忘的最简单技术之一。在这种方法中，我们存储来自原始数据集的预训练示例，并将它们与持续训练数据集交织在一起。因此，数据漂移并不那么明显。
- en: 'The following formula has worked very well for me: sample from different subsets
    of the original pre-training datasets and mix them with the continual training
    dataset. At the start of training, let the proportion of new data be around 25%.
    Over training steps, this can be slowly increased up to a maximum proportion,
    like 80%.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式对我非常有效：从原始预训练数据集的不同子集中采样，并将它们与持续训练数据集混合。在训练开始时，让新数据的比例约为25%。在训练步骤中，可以逐渐增加到最大比例，如80%。
- en: If the original pre-training dataset is a monolith and not made up of several
    smaller datasets, you might need to identify domains yourself so that all domains
    in the original pre-training set are included.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果原始预训练数据集是一个整体，而不是由几个较小的数据集组成，你可能需要自己识别领域，以确保原始预训练集中的所有领域都包括在内。
- en: Parameter Expansion
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数扩展
- en: An alternative to the replay approach is to use parameter expansion techniques.
    The naive way would be to just add a new layer or two on top of the model and
    train only those parameters during continual pre-training. You can also insert
    and train domain-specific parameter modules (called adapters) within existing
    layers. We will discuss adapter-based approaches in [“Parameter-Efficient Fine-Tuning”](#parameter-efficient-fine-tuning).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 重放方法的替代方案是使用参数扩展技术。一种简单的方法是在模型顶部添加一个或两个新层，并在持续预训练期间仅训练这些参数。你还可以在现有层中插入并训练特定领域的参数模块（称为适配器）。我们将在“参数高效微调”（#parameter-efficient-fine-tuning）中讨论基于适配器的方法。
- en: As mentioned earlier, continual pre-training can also be used to facilitate
    life-long learning, with the model continually being updated with new facts and
    knowledge. However, currently this may not be the most effective paradigm for
    new knowledge learning. You are probably better off using RAG for that. We will
    explore RAG in more detail in [Chapter 12](ch12.html#ch12).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，持续预训练也可以用来促进终身学习，模型持续更新以包含新的事实和知识。然而，目前这可能不是新知识学习最有效的范式。你可能更适合使用RAG。我们将在第12章中更详细地探讨RAG。
- en: Tip
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: '[Task-adaptive pre-training (TAPT)](https://oreil.ly/H38wF) is a useful supplement
    to domain-adaptive pre-training. TAPT involves continual pre-training of the LLM
    on a much smaller but more task-specific unsupervised dataset. To prevent catastrophic
    forgetting, you should perform DAPT first before TAPT, and then subsequently perform
    any supervised fine-tuning on your downstream tasks. Unsupervised data for TAPT
    can be selected using similar methods as that used for DAPT: by constructing embeddings
    of data and selecting data that is clustered with gold-truth sentences.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[任务自适应预训练（TAPT）](https://oreil.ly/H38wF) 是领域自适应预训练的有用补充。TAPT 涉及在更小但更特定于任务的未标记数据集上对
    LLM 进行持续预训练。为了防止灾难性遗忘，你应该在 TAPT 之前先进行 DAPT，然后随后在下游任务上执行任何监督微调。TAPT 的未标记数据可以选择使用与
    DAPT 类似的方法：通过构建数据嵌入并选择与黄金真实句子聚类的数据。'
- en: In summary, continual pre-training can be very effective in cases where you
    have a large body of domain-specific text and the domain is very distinctly characterized
    by a specialized linguistic structure or vocabulary. Continual pre-training can
    also be used to help adapt the LLM to a new language.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，持续预训练在拥有大量特定领域文本且领域特征明显由专门的语料库或词汇所表征的情况下非常有效。持续预训练还可以用来帮助 LLM 适应新的语言。
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Domain-specific text can contain jargon specific to that domain. One strategy
    that has worked well for me is to add extra tokens to represent domain-specific
    jargon.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 特定领域的文本可能包含该领域特有的术语。对我有效的一个策略是添加额外的标记来表示特定领域的术语。
- en: Continual pre-training can take a lot of computational resources. Fine-tuning
    on smaller datasets takes substantially less resources. However, in the era of
    large language models, it is imperative to do all we can to reduce compute and
    memory requirements. Therefore, let’s next discuss some parameter-efficient fine-tuning
    techniques that make the fine-tuning process more accessible in resource-constrained
    environments.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 持续预训练需要大量的计算资源。在较小的数据集上进行微调所需的资源要少得多。然而，在大语言模型的时代，我们必须尽一切努力来减少计算和内存需求。因此，接下来让我们讨论一些参数高效的微调技术，这些技术使得在资源受限的环境中微调过程更加可行。
- en: Parameter-Efficient Fine-Tuning
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数高效微调
- en: In PEFT, instead of updating all the parameters of the model, we update only
    a small number of parameters. This can vastly bring down compute and storage requirements.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PEFT 中，我们不是更新模型的全部参数，而是仅更新一小部分参数。这可以大幅降低计算和存储需求。
- en: 'We can categorize current PEFT techniques into three types:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将当前的 PEFT 技术分为三种类型：
- en: Adding new parameters
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 添加新参数
- en: This involves adding some extra parameters to the LLM and training only them.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及向 LLM 添加一些额外的参数，并仅训练这些参数。
- en: Selecting a subset of parameters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 选择参数子集
- en: This involves choosing to update only a small subset of parameters of the LLM,
    either by selecting the subset apriori or by learning the appropriate subset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及选择仅更新 LLM 的一小部分参数，无论是通过事先选择子集还是通过学习适当的子集。
- en: Low-rank methods
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩方法
- en: This involves using methods that reduce the number of parameters to train by
    finding a smaller matrix containing almost the same information as a larger matrix.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及使用通过找到包含几乎与较大矩阵相同信息的小矩阵来减少训练参数数量的方法。
- en: Let’s now go through each of these in detail.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将逐一详细说明这些内容。
- en: Adding New Parameters
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加新参数
- en: Perhaps your work needs you to fine-tune models for a large number of tasks.
    Or maybe you need to drive personalization by fine-tuning a model for each user.
    It will be cumbersome to maintain and deploy so many full copies of fine-tuned
    models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可能你的工作需要你对大量任务进行模型微调。或者，你可能需要通过为每个用户微调模型来驱动个性化。维护和部署如此多的微调模型副本将会很麻烦。
- en: One way to avoid updating all the parameters of the model is to add a few extra
    parameters to the model and train only them. Instead of storing and deploying
    full copies of each fine-tuned model, you store only the newly added parameters.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 避免更新模型所有参数的一种方法是在模型中添加一些额外的参数，并仅训练这些参数。而不是存储和部署每个微调模型的完整副本，你只需存储新添加的参数。
- en: 'Common ways of adding new parameters for fine-tuning include:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 添加新参数进行微调的常见方法包括：
- en: Bottleneck adapters
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 瓶颈适配器
- en: These are lightweight modules added to the Transformer layers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是添加到 Transformer 层的轻量级模块。
- en: Prefix tuning
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀微调
- en: These are task-specific vectors that are trained and prefixed to the input.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是针对特定任务的向量，经过训练并附加到输入前。
- en: Prompt tuning (soft prompts)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 提示调优（软提示）
- en: This is similar to prefix tuning but with a simplified training approach.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这与前缀调优类似，但采用了简化的训练方法。
- en: Let’s discuss each of these techniques in detail.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论这些技术中的每一个。
- en: Bottleneck adapters
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 瓶颈适配器
- en: Adapters are parameter modules attached to the LLM architecture. Adapters can
    be integrated into the LLM architecture in a variety of ways, but in Transformers,
    the common way is to insert them at each layer of the Transformer. To reduce the
    number of parameters, the width of the adapter module should be much less than
    the width of the underlying Transformer model. This constitutes a *down-projection*,
    also called a bottleneck.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器是附加到 LLM 架构的参数模块。适配器可以以多种方式集成到 LLM 架构中，但在 Transformers 中，常见的方式是在 Transformer
    的每一层插入它们。为了减少参数数量，适配器模块的宽度应该远小于底层 Transformer 模型的宽度。这构成了一个 *下投影*，也称为瓶颈。
- en: Therefore, a bottleneck adapter sublayer consists of a down-projection matrix,
    an up-projection matrix at the end to project back to the original dimensions,
    and parameters that can be configured in a variety of ways in the middle. During
    fine-tuning, only the adapter modules are updated. The original pre-trained model
    is not updated. Adapters are initialized with a near-identity initialization to
    ensure smooth training.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，瓶颈适配器子层由一个下投影矩阵、一个在末尾的上投影矩阵（用于将维度投影回原始大小）以及中间可以以多种方式配置的参数组成。在微调过程中，仅更新适配器模块。原始预训练模型不会被更新。适配器使用接近单位矩阵的初始化来确保训练平稳。
- en: '[Figure 7-2](#adapters) shows where in the Transformer architecture the bottleneck
    adapters typically are inserted. Note that this is just one possible configuration.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-2](#adapters) 展示了在 Transformer 架构中瓶颈适配器通常插入的位置。请注意，这只是一个可能的配置。'
- en: '![adapters](assets/dllm_0702.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![适配器](assets/dllm_0702.png)'
- en: Figure 7-2\. Adapter modules in the Transformer
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. Transformer 中的适配器模块
- en: How does this all work in practice? The [*adapters* library](https://oreil.ly/z05rI)
    comes in handy to facilitate fine-tuning LLMs using these advanced techniques.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在实际操作中是如何工作的呢？[*适配器*库](https://oreil.ly/z05rI)非常有用，可以方便地使用这些高级技术微调 LLM。
- en: 'Here is how you can start using bottleneck adapters using the adapters library:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您可以使用适配器库开始使用瓶颈适配器的方法：
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`DoubleSeqBnConfig` refers to a config natively supported by the library, corresponding
    to the adapter architecture shown in [Figure 7-2](#adapters). But as I mentioned
    before, you can change the size and shape of the adapters as you wish. To do that,
    we need to use `BnConfig`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`DoubleSeqBnConfig` 指的是库原生支持的配置，对应于 [图 7-2](#adapters) 中所示的适配器架构。但如我之前提到的，您可以按需更改适配器的大小和形状。为此，我们需要使用
    `BnConfig`：'
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is what these arguments stand for:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是这些参数的含义：
- en: '`mh_adapter`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`mh_adapter`'
- en: Refers to the adapter modules added right after the multi-head attention sublayer
    of the Transformer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 指的是在 Transformer 的多头注意力子层之后添加的适配器模块。
- en: '`output_adapter`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`output_adapter`'
- en: Refers to the adapter modules added right after the feedforward network sublayer
    of the Transformer.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 指的是在 Transformer 的前馈网络子层之后添加的适配器模块。
- en: '`reduction_factor`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduction_factor`'
- en: 'Refers to the down-projection factor: by how much should the adapter width
    be scaled down compared to the Transformer layer width?'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 指的是下投影因子：适配器宽度相对于 Transformer 层宽应该缩小多少？
- en: '`non_linearity`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`non_linearity`'
- en: Refers to the activation function being used, like RELU or GELU.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 指的是所使用的激活函数，例如 RELU 或 GELU。
- en: Refer to the adapters library [documentation](https://oreil.ly/n1Pga) for more
    configuration options. There are so many configuration options available!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考适配器库的[文档](https://oreil.ly/n1Pga)以获取更多配置选项。可用的配置选项非常多！
- en: While using bottleneck adapters leads to a vast decrease in fine-tuning time
    and complexity, adding parameters across all layers of the Transformer increases
    inference latency by a small amount. Typically, the inference time using commonly
    used adapter configurations is expected to increase by 6%–8%.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用瓶颈适配器可以大幅减少微调时间和复杂性，但将参数添加到 Transformer 的所有层会增加推理延迟。通常，使用常用适配器配置的推理时间预计会增加
    6%–8%。
- en: Tip
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: It is possible to reduce the inference latency by dropping some adapter layers
    during inference. [Rücklé et al. propose AdapterDrop](https://oreil.ly/GM_1X),
    a set of methods for dropping adapter modules during training and inference. They
    propose dropping adapters from the first few layers of the Transformer during
    inference or pruning the adapters from each layer that is the least activated.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，通过删除一些适配器层，可以降低推理延迟。[Rücklé等人提出了AdapterDrop](https://oreil.ly/GM_1X)，这是一组在训练和推理期间删除适配器模块的方法。他们建议在推理期间从Transformer的前几层删除适配器，或者从每个最少激活的层修剪适配器。
- en: Prefix-tuning
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前缀调整
- en: One drawback of using adapter-based fine-tuning techniques is that during inference,
    each batch can support only a single adapter instance, i.e., an adapter fine-tuned
    for a particular task. Prefix-tuning, in contrast, enables multiple tasks to be
    run in the same batch.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于适配器的微调技术的缺点是，在推理过程中，每个批次只能支持单个适配器实例，即针对特定任务微调的适配器。相比之下，前缀调整允许在同一批次中运行多个任务。
- en: In prefix-tuning, we add and train task-specific vectors to the prefix of the
    input. This vastly reduces the number of parameters we need to fine-tune. Recall
    that the prompt contains the instruction, the input, and optionally some few-shot
    examples. The text generated by the LLM is conditioned on the output generated
    so far, and the prompt. To this, we add additional context that the LLM can attend
    to, in the form of these prefix vectors. The new tokens prefixed to the input
    are called **virtual tokens** or **soft prompts**.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在前缀调整中，我们将特定于任务的向量添加到输入的前缀中。这大大减少了我们需要微调的参数数量。回想一下，提示包含指令、输入以及可选的一些少样本示例。LLM生成的文本是基于到目前为止生成的输出和提示的。为此，我们添加了额外的上下文，LLM可以通过这些前缀向量来关注。添加到输入前缀的新标记被称为**虚拟标记**或**软提示**。
- en: '[Figure 7-3](#prefix-tuning) shows how prefix-tuning occurs in the Transformer.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-3](#prefix-tuning)展示了前缀调整在Transformer中的发生过程。'
- en: '![prefix-tuning](assets/dllm_0703.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![prefix-tuning](assets/dllm_0703.png)'
- en: Figure 7-3\. Prefix-tuning
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3\. 前缀调整
- en: As the figure shows, prefix parameters are added at each layer.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，前缀参数被添加到每一层。
- en: Prefix-tuning is much more parameter-efficient than bottleneck adapters, taking
    up only 0.1% or less of a model’s parameters, as compared to adapters where it
    is usually 2% or more. However, prefix-tuning is harder to train effectively than
    adapters. Prefix-tuning also reduces the sequence length of the model in order
    to accommodate the virtual tokens.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀调整比瓶颈适配器参数效率更高，仅占用模型参数的0.1%或更少，而适配器通常占2%或更多。然而，前缀调整比适配器更难有效训练。前缀调整还减少了模型的序列长度，以适应虚拟标记。
- en: Similar to adapters, initialization is very important for prefix-tuning. The
    virtual tokens can be initialized by choosing words that are related to the task
    the model is being fine-tuned for.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 与适配器类似，初始化对于前缀调整非常重要。虚拟标记可以通过选择与模型正在微调的任务相关的单词来初始化。
- en: 'Using the adapters library, we can implement prefix-tuning:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用适配器库，我们可以实现前缀调整：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Prompt tuning
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示调整
- en: Prompt tuning is a simplified version of prefix-tuning. Unlike prefix tuning,
    there are no prefix parameters at each layer.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 提示调整是前缀调整的简化版本。与前缀调整不同，在每一层都没有前缀参数。
- en: '[Figure 7-4](#prompt-tuning) shows how prompt-tuning occurs in the Transformer.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-4](#prompt-tuning)展示了提示调整在Transformer中的发生过程。'
- en: '![prompt-tuning](assets/dllm_0704.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![prompt-tuning](assets/dllm_0704.png)'
- en: Figure 7-4\. Prompt-tuning
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4\. 提示调整
- en: 'The adapters library provides a built-in configuration for prompt tuning:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器库为提示调整提供了内置的配置：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Some relevant configuration parameters for prompt tuning include:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一些与提示调整相关的配置参数包括：
- en: '`prompt_length`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`prompt_length`'
- en: The length of the prompt tokens; 10–30 is a good start.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 提示标记的长度；10-30是一个好的开始。
- en: '`prompt_init`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`prompt_init`'
- en: The method for initializing these tokens. They can be initialized either through
    the embedding of a string or by a random uniform initialization.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化这些标记的方法。它们可以通过字符串的嵌入或随机均匀初始化来初始化。
- en: '`prompt_init_text`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`prompt_init_text`'
- en: If the soft prompt is initialized by string, the text that is used to initialize
    it. This can be a descriptor of the task at hand.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果软提示通过字符串初始化，则用于初始化它的文本。这可以是当前任务的描述符。
- en: '[Lester et al.](https://oreil.ly/BPpRu), who introduced prompt-tuning, also
    leverage it to perform soft prompt ensembling. For soft prompt ensembling, you
    train several soft prompts for each task. Then, for a given input, you use each
    of them as a prefix separately and generate the output. You can then use majority
    voting to select the correct output among the generated ones.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Lester等人（[Lester et al.](https://oreil.ly/BPpRu)），他们引入了提示微调，还利用它来执行软提示集成。对于软提示集成，你为每个任务训练几个软提示。然后，对于给定的输入，你分别使用它们作为前缀并生成输出。然后，你可以使用多数投票来从生成的输出中选择正确的输出。
- en: So far, we have seen techniques where new parameters are added to the model
    for fine-tuning. However, we can implement PEFT by fine-tuning only a small subset
    of parameters of the model without having to add new parameters. Let’s explore
    these methods next.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了添加新参数到模型中进行微调的技术。然而，我们可以通过仅微调模型的一小部分参数来实现PEFT（参数有效的微调），而不需要添加新参数。让我们接下来探索这些方法。
- en: Subset Methods
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子集方法
- en: A naive way of choosing a subset of parameters to fine-tune on would be to fine-tune
    only the upper layers of the Transformer and keep everything else frozen. The
    lower layers of the Transformer are known to be specialized in more fundamental
    aspects of language like syntax, which we want the LLM to preserve.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择子集参数进行微调的简单方法是对Transformer的上层进行微调，而将其他所有内容冻结。众所周知，Transformer的下层在语言的基础方面具有专业性，比如句法，这是我们希望LLM保留的。
- en: Another way is to fine-tune only the bias terms (discussed in [Chapter 2](ch02.html#ch02))
    of the Transformer. This was proposed by [Zaken et al.](https://oreil.ly/SaWoe),
    who show that you can gain almost the same level of performance as that of fully
    fine-tuning a model by just fine-tuning on the bias terms. The authors observed
    that this technique is mostly effective when your training data is limited.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是仅微调Transformer的偏置项（在第2章中讨论）。这是由Zaken等人提出的，他们表明，通过仅对偏置项进行微调，你可以获得几乎与完全微调模型相同水平的性能。作者观察到，当你的训练数据有限时，这种技术主要有效。
- en: Ultimately, as we have seen here, there are tradeoffs involved in selecting
    each of these fine-tuning approaches. The ML community is working on developing
    best practices around this area. In the meanwhile, experimentation is key!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，正如我们在这里所看到的，选择这些微调方法中每一种都涉及权衡。机器学习社区正在努力开发这一领域的最佳实践。与此同时，实验是关键！
- en: 'Next, let’s look at another way to update the parameters of an LLM: by merging
    it with the parameters of another LLM.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看另一种更新LLM参数的方法：通过将其与另一个LLM的参数合并。
- en: Combining Multiple Models
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合多个模型
- en: If you have access to multiple LLMs, each of them overlapping in terms of capabilities
    yet possessing certain unique characteristics, you want to leverage the capabilities
    of all the models in your downstream tasks in some way. This can be done by a
    variety of means, including model ensembling and model fusion or merging. This
    area of LLMs is in its infancy, and more work remains to be done to reap its full
    benefits. I call it the dark arts of NLP because the theoretical underpinnings
    of these techniques remain poorly understood. However, I do believe that even
    with these caveats it merits inclusion in this book, because the practical benefits
    are already visible. Let’s explore a few of these methods.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够访问多个LLM（大型语言模型），它们在能力上相互重叠，但各自具有某些独特的特点，你希望在下游任务中以某种方式利用所有模型的能力。这可以通过多种方式实现，包括模型集成、模型融合或合并。LLM的这个领域还处于起步阶段，还有更多的工作要做以充分利用其全部优势。我将其称为NLP的黑暗艺术，因为这些技术的理论基础仍然理解不深。然而，我确实相信，即使有这些警告，它也值得被纳入这本书中，因为其实际效益已经显现。让我们探索其中的一些方法。
- en: Model Ensembling
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型集成
- en: Different LLMs may possess different but complementary capabilities, a byproduct
    of the difference in their training regimens, training hyperparameters, etc. This
    is especially true when it comes to open source LLMs, where we have a plethora
    of models, most of them being trained on largely overlapping datasets, performing
    very closely to each other in benchmark evaluation metrics. Thus, an ensembling
    approach might bring forth benefits by allowing complementary capabilities from
    multiple models to be leveraged to generate better outputs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的 LLM 可能具有不同但互补的能力，这是由于它们训练规程、训练超参数等的差异所导致的。这在开源 LLM 中尤其如此，我们拥有大量的模型，其中大多数在大量重叠的数据集上训练，在基准评估指标上非常接近。因此，通过允许利用多个模型的互补能力来生成更好的输出，集成方法可能会带来好处。
- en: In [Chapter 5](ch05.html#chapter_utilizing_llms), we discussed how, for generative
    tasks, it is useful to generate multiple outputs for the same input and select
    the best one using heuristics. We can extend this principle to multiple models.
    Each input is passed through *n* models. Optionally, an initial step can choose
    the top k models with the most high-quality or relevant outputs. The outputs from
    these models can be combined and fed through a model (which can be an LLM) to
    generate the final output.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 5 章](ch05.html#chapter_utilizing_llms) 中，我们讨论了对于生成任务，生成相同输入的多个输出并使用启发式方法选择最佳输出是有用的。我们可以将这个原则扩展到多个模型。每个输入都通过
    *n* 个模型。可选地，一个初始步骤可以选择具有最高质量或相关输出的前 k 个模型。这些模型的输出可以组合并通过一个模型（可以是 LLM）输入以生成最终输出。
- en: '[Jiang et al.](https://oreil.ly/Sipzu) present a framework called LLM-Blender
    for enabling LLM ensembling. The framework consists of two components:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[江等人](https://oreil.ly/Sipzu) 提出了一个名为 LLM-Blender 的框架，用于实现 LLM 集成。该框架由两个组件组成：'
- en: PairRanker scores the output from two models, thus choosing a winner.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PairRanker 对两个模型的输出进行评分，从而选择一个赢家。
- en: GenFuser takes as input the output from k different models to generate the final
    output.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GenFuser 接收来自 k 个不同模型的输出以生成最终输出。
- en: '[Figure 7-5](#LLMBlender) shows the workings of the LLM-Blender framework.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-5](#LLMBlender) 展示了 LLM-Blender 框架的工作原理。'
- en: '![LLMBlender](assets/dllm_0705.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![LLMBlender](assets/dllm_0705.png)'
- en: Figure 7-5\. LLM-Blender
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. LLM-Blender
- en: Let’s dig deeper into each of these modules.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解这些模块中的每一个。
- en: PairRanker
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PairRanker
- en: Consider you have access to *n* different models. For a given input, you feed
    the input to each of these models to generate the outputs. Now, for each pair
    of outputs, you can combine them with the input and feed them to the PairRanker
    module. The PairRanker module is trained to provide scores for each of the outputs.
    If you end up feeding all the pairs of outputs to the PairRanker module, you will
    then find the output (model) with the highest score. This output can then be taken
    as the final output.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑你拥有 *n* 个不同的模型。对于给定的输入，你将输入提供给这些模型中的每一个以生成输出。现在，对于每一对输出，你可以将它们与输入结合并输入到 PairRanker
    模块中。PairRanker 模块被训练来为每个输出提供分数。如果你最终将所有输出对都输入到 PairRanker 模块中，你将找到得分最高的输出（模型）。这个输出可以被视为最终输出。
- en: However, this just selects the best output and doesn’t necessarily combine the
    capabilities of the different models. For that, the LLM-Blender framework consists
    of a module called GenFuser.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这仅仅选择了最好的输出，并不一定结合了不同模型的能力。为此，LLM-Blender 框架包含一个名为 GenFuser 的模块。
- en: GenFuser
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GenFuser
- en: For GenFuser, we take the top k results from the PairRanker scores. We then
    feed them together to the GenFuser, which generates the final output. The GenFuser
    in practice is just a fine-tuned LLM that is tuned to accept several candidate
    inputs and generate an output that combines the characteristics of the different
    candidates.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GenFuser，我们从 PairRanker 分数中选取前 k 个结果。然后我们将它们一起输入到 GenFuser 中，生成最终输出。在实践中，GenFuser
    只是一个经过微调的 LLM，它被调整以接受多个候选输入并生成一个结合了不同候选者特征的输出。
- en: 'Let’s see how this works in practice. We can use the [LLM-Blender library](https://oreil.ly/F2IcX):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它在实际中是如何工作的。我们可以使用 [LLM-Blender 库](https://oreil.ly/F2IcX)：
- en: '[PRE4]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Given an input and a list of `candidate_outputs` from *n* different language
    models, we rank the outputs using the PairRanker and then select the top-k ranked
    outputs and fuse them to generate the final output.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入和来自 *n* 个不同语言模型的 `candidate_outputs` 列表，我们使用 PairRanker 对输出进行排名，然后选择排名前
    k 的输出并将它们融合以生成最终输出。
- en: While ensembling methods can be effective, there is a lot of recent interest
    in model fusion techniques.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然集成方法可能有效，但最近对模型融合技术产生了很大兴趣。
- en: Model Fusion
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型融合
- en: In this approach, we combine the parameters of multiple models in some way.
    The idea is that by combining the parameters of multiple models, we might be able
    to benefit from all the complementary capabilities possessed by each of the individual
    models, within a single model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们以某种方式结合多个模型的参数。想法是，通过结合多个模型的参数，我们可能能够从每个单独模型所拥有的所有互补能力中受益，在一个单一模型中。
- en: 'Some of the common methods used in model fusion are:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型融合中常用的方法包括：
- en: Averaging
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 平均
- en: The simplest way to combine multiple models is to average their parameters.
    Simple averaging has been shown to be quite effective.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 结合多个模型的最简单方法是对它们的参数进行平均。简单的平均已经被证明是非常有效的。
- en: Weighted averaging
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 加权平均
- en: During averaging, certain models or even certain layers in models can be weighted
    more.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在平均过程中，某些模型或甚至模型中的某些层可以被赋予更高的权重。
- en: Interpolation
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 插值
- en: 'Each model can be weighted by a factor w1, w2,…wn, with:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型可以通过一个因子w1, w2,…wn进行加权，其中：
- en: '[PRE5]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: where p1, p2, p3…​pn are the parameters of models m1, m2, m3…mn.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中p1, p2, p3…pn是模型m1, m2, m3…mn的参数。
- en: One of the benefits in merging multiple models is model reuse. Say you have
    a base LLM at your organization. It is used by people all across the organization,
    who take the model and fine-tune it on their own tasks. They then upload the fine-tuned
    models back. You can then merge the weights of all the models, resulting in a
    stronger pre-trained model. This model can then be used as a new version of the
    base model. This process has been coined Collaborative Descent (ColD) Fusion by
    [Don-Yehiya et al.](https://oreil.ly/LTcdf)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 多模型合并的一个好处是模型重用。假设你组织里有一个基础的大型语言模型（LLM）。这个模型被组织中的所有人使用，他们将自己的任务对模型进行微调，然后将微调后的模型上传回来。然后你可以合并所有模型的权重，从而得到一个更强的预训练模型。这个模型可以作为一个新版本的基础模型。这个过程被Don-Yehiya等人称为协作下降（ColD）融合（[Don-Yehiya
    et al.](https://oreil.ly/LTcdf)）。
- en: Why would we want to do this? The idea is that if we want to fine-tune an LLM
    on a dataset, it would be nice to have a good starting point such that the training
    is optimal. The hypothesis is that if we already fine-tuned the LLM on another
    task, the fine-tuned LLM is a better starting point than the base LLM. This is
    called intertraining. This too is a fairly new concept, so proceed with caution.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要这样做呢？这个想法是，如果我们想在数据集上微调一个LLM，那么有一个好的起点将使训练更优化。假设我们已经在一个其他任务上微调了LLM，那么微调后的LLM将比基础LLM是一个更好的起点。这被称为交叉训练。这也是一个相对较新的概念，所以请谨慎行事。
- en: Instead of merging all the parameters of the model, you can merge only a small
    portion of them. In fact, we could just merge the adapter modules.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以不合并模型的全部参数，而只合并其中的一小部分。实际上，我们甚至可以只合并适配器模块。
- en: Adapter Merging
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适配器合并
- en: Earlier in the chapter, we learned about adapters, which can be used for a variety
    of purposes including domain-adaptive pre-training. While you can train different
    adapters for different domains, the question remains on how you would treat new
    domains seen at inference time. One solution would be to average the adapters
    related to the closest domains and use that for novel domains. This has been shown
    to work well, by [Chronopoulou et al.’s AdapterSoup framework](https://oreil.ly/mKoZ1).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，我们学习了适配器，它们可以用作多种目的，包括领域自适应预训练。虽然你可以为不同的领域训练不同的适配器，但问题仍然在于如何在推理时处理遇到的新领域。一个解决方案是对与最接近的领域相关的适配器进行平均，并使用这个结果来处理新领域。这已经被Chronopoulou等人通过AdapterSoup框架（[Chronopoulou
    et al.’s AdapterSoup framework](https://oreil.ly/mKoZ1)）证明是有效的。
- en: Another way to combine adapter parameters is in the context of an MoE framework,
    introduced in [Chapter 4](ch04.html#chapter_transformer-architecture). Recall
    that in a mixture-of-experts model, the routing function determines which expert(s)
    will handle the input. [Wang et al.’s AdaMix framework](https://oreil.ly/pc7Js)
    extends this to adapter modules. Instead of learning only one adapter module per
    layer, we learn multiple expert modules. During inference, all the adaptation
    layers are merged.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种结合适配器参数的方法是在第4章中引入的MoE框架的上下文中。回想一下，在混合专家模型中，路由函数决定了哪个专家（们）将处理输入。[Wang等人提出的AdaMix框架](https://oreil.ly/pc7Js)将此扩展到适配器模块。我们不是为每一层学习一个适配器模块，而是学习多个专家模块。在推理过程中，所有适配层都被合并。
- en: Model merging is a fascinating subarea of LLMs. Even if you are not using it
    in your applications, I highly recommend experimenting with it because it doubles
    as a really neat tool to understand the working of LLMs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 模型合并是LLMs的一个迷人的子领域。即使你不在你的应用中使用它，我也强烈推荐你尝试一下，因为它同时也是一个真正 neat 的工具，可以帮助你理解LLMs的工作原理。
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned a plethora of advanced fine-tuning techniques, including
    continual pre-training strategies like experience replay and parameter expansion;
    parameter-efficient fine-tuning techniques like bottleneck adapters, prefix tuning,
    prompt tuning, and subset selection; and various types of model merging and ensembling.
    We also learned the various motivations for updating model weights and the suitability
    of different methods for each of those situations.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了许多高级微调技术，包括经验回放和参数扩展等持续预训练策略；瓶颈适配器、前缀调整、提示调整和子集选择等参数高效微调技术；以及各种类型的模型合并和集成。我们还学习了更新模型权重的各种动机以及不同方法在每种情况下的适用性。
- en: As discussed in the previous and current chapter, fine-tuning is not a panacea
    and cannot learn new capabilities or necessarily digest new knowledge. In the
    next chapter, we will discuss limitations of LLMs like poor steerability, hallucinations,
    and reasoning issues, along with techniques for mitigating them.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章和本章所讨论的，微调并非万能，不能学习新的能力或必然消化新的知识。在下一章中，我们将讨论LLMs的局限性，如较差的可控性、幻觉和推理问题，以及缓解这些问题的技术。
