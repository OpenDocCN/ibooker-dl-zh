- en: Appendix A. Tips on Model Construction and Using TensorFlow Serving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录A. 模型构建和使用TensorFlow Serving的提示
- en: Model Structuring and Customization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型结构化和定制化
- en: In this short section we will focus on two topics that continue from and extend
    the previous chapters—how to construct a proper model, and how to customize the
    model’s entities. We start by describing how we can effectively reframe our code
    by using encapsulations and allow its variables to be shared and reused. In the
    second part of this section we will talk about how to customize our own loss functions
    and operations and use them for optimization.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简短的部分中，我们将专注于两个主题，这些主题延续并扩展了前几章——如何构建一个合适的模型，以及如何定制模型的实体。我们首先描述如何通过使用封装来有效地重构我们的代码，并允许其变量被共享和重复使用。在本节的第二部分，我们将讨论如何定制我们自己的损失函数和操作，并将它们用于优化。
- en: Model Structuring
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型结构化
- en: Ultimately, we would like to design our TensorFlow code efficiently, so that
    it can be reused for multiple tasks and is easy to follow and pass around. One
    way to make things cleaner is to use one of the available TensorFlow extension
    libraries, which were discussed in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications).
    However, while they are great to use for typical networks, models with new components
    that we wish to implement may sometimes require the full flexibility of lower-level
    TensorFlow.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们希望设计我们的TensorFlow代码高效，以便可以重用于多个任务，并且易于跟踪和传递。使事情更清晰的一种方法是使用可用的TensorFlow扩展库之一，这些库在[第7章](ch07.html#tensorflow_abstractions_and_simplifications)中已经讨论过。然而，虽然它们非常适合用于典型的网络，但有时我们希望实现的具有新组件的模型可能需要较低级别TensorFlow的完全灵活性。
- en: 'Let’s take another look at the optimization code from the previous chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看一下前一章的优化代码：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We get:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The entire code here is simply stacked line by line. This is OK for simple and
    focused examples. However, this way of coding has its limits—it’s neither reusable
    nor very readable when the code gets more complex.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的整个代码只是简单地一行一行堆叠。对于简单和专注的示例来说，这是可以的。然而，这种编码方式有其局限性——当代码变得更加复杂时，它既不可重用也不太可读。
- en: Let’s zoom out and think about what characteristics our infrastructure should
    have. First, we would like to encapsulate the model so it can be used for various
    tasks like training, evaluation, and forming predictions. Furthermore, it can
    also be more efficient to construct the model in a modular fashion, giving us
    specific control over its subcomponents and increasing readability. This will
    be the focus of the next few sections.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们放大视野，思考一下我们的基础设施应该具有哪些特征。首先，我们希望封装模型，以便可以用于各种任务，如训练、评估和形成预测。此外，以模块化的方式构建模型可能更有效，使我们能够对其子组件具有特定控制，并增加可读性。这将是接下来几节的重点。
- en: Modular design
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模块化设计
- en: 'A good start is to split the code into functions that capture different elements
    in the learning model. We can do this as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的开始是将代码分成捕捉学习模型中不同元素的函数。我们可以这样做：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And here is the result:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now we can reuse the code with different inputs, and this division makes it
    easier to read, especially when it gets more complex.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以重复使用具有不同输入的代码，这种划分使其更易于阅读，特别是在变得更加复杂时。
- en: In this example we called the main function twice with the same inputs and printed
    the variables that were created. Note that each call created a different set of
    variables, resulting in the creation of four variables. Let’s assume, for example,
    a scenario where we wish to build a model with multiple inputs, such as two different
    images. Say we wish to apply the same convolutional filters to both input images. New
    variables will be created. To avoid this, we “share” the filter variables, using
    the same variables on both images.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们两次调用了主函数并打印了创建的变量。请注意，每次调用都会创建不同的变量集，从而创建了四个变量。例如，假设我们希望构建一个具有多个输入的模型，比如两个不同的图像。假设我们希望将相同的卷积滤波器应用于两个输入图像。将创建新的变量。为了避免这种情况，我们可以“共享”滤波器变量，在两个图像上使用相同的变量。
- en: Variable sharing
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变量共享
- en: 'It’s possible to reuse the same variables by creating them with `tf.get_variable()`
    instead of `tf.Variable()`. We use this very similarly to `tf.Variable()`, except
    that we need to pass an initializer as an argument:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`tf.get_variable()`而不是`tf.Variable()`，可以重复使用相同的变量。我们使用方式与`tf.Variable()`非常相似，只是需要将初始化器作为参数传递：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here we used `tf.zeros_initializer()`. This initializer is very similar to `tf.zeros()`,
    except that it doesn’t get the shape as an argument, but rather arranges the values
    according to the shape specified by `tf.get_variable()`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`tf.zeros_initializer()`。这个初始化器与`tf.zeros()`非常相似，只是它不会将形状作为参数，而是根据`tf.get_variable()`指定的形状排列值。
- en: In this example the variable `w` will be initialized as `[0,0,0]`, as specified
    by the given shape, `[1,3]`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，变量`w`将被初始化为`[0,0,0]`，如给定的形状`[1,3]`所指定。
- en: With `get_variable()` we can reuse variables that have the same name (including
    the scope prefix, which can be set by `tf.variable_scope()`). But first we need
    to indicate this intention by either using `tf.variable_scope.reuse_variable()`
    or setting the `reuse` flag (`tf.variable.scope(reuse=True)`). An example of how
    to share variables is shown in the code that follows.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`get_variable()`，我们可以重复使用具有相同名称的变量（包括作用域前缀，可以通过`tf.variable_scope()`设置）。但首先，我们需要通过使用`tf.variable_scope.reuse_variable()`或设置`reuse`标志（`tf.variable.scope(reuse=True)`）来表明这种意图。下面的代码示例展示了如何共享变量。
- en: Heads-up for flag misuse
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标志误用的注意事项
- en: Whenever a variable has the exact same name as another, an exception will be
    thrown when the `reuse` flag is not set. The same goes for the opposite scenario—variables
    with mismatching names that are expected to be reused (when `reuse = True`) will
    cause an exception as well.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 每当一个变量与另一个变量具有完全相同的名称时，在未设置`reuse`标志时会抛出异常。相反的情况也是如此——期望重用的名称不匹配的变量（当`reuse=True`时）也会导致异常。
- en: 'Using these methods, and setting the scope prefix to `Regression`, by printing
    their names we can see that the same variables are reused:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些方法，并将作用域前缀设置为`Regression`，通过打印它们的名称，我们可以看到相同的变量被重复使用：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is shown here:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`tf.get_variables()` is a neat, lightweight way to share variables. Another
    approach is to encapsulate our model as a class and manage the variables there.
    This approach has many other benefits, as described in the following section'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.get_variables()`是一个简洁、轻量级的共享变量的方法。另一种方法是将我们的模型封装为一个类，并在那里管理变量。这种方法有许多其他好处，如下一节所述'
- en: Class encapsulation
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类封装
- en: As with any other program, when things get more complex and the number of code
    lines grows, it becomes very convenient to have our TensorFlow code reside within
    a class, giving us quick access to methods and attributes that belong to the same
    model. Class encapsulation allows us to maintain the state of our variables and
    then perform various post-training tasks like forming predictions, model evaluation,
    further training, saving and restoring our weights, and whatever else is related
    to the specific problem our model solves.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他程序一样，当事情变得更加复杂，代码行数增加时，将我们的TensorFlow代码放在一个类中变得非常方便，这样我们就可以快速访问属于同一模型的方法和属性。类封装允许我们维护变量的状态，然后执行各种训练后任务，如形成预测、模型评估、进一步训练、保存和恢复权重，以及与我们的模型解决的特定问题相关的任何其他任务。
- en: In the next batch of code we see an example of a simple class wrapper. The model
    is created when the instance is instantiated, and the training process is performed
    by calling the `fit()` method.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一批代码中，我们看到一个简单的类包装器示例。当实例化时创建模型，并通过调用`fit()`方法执行训练过程。
- en: '@property and Python decorators'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '@property和Python装饰器'
- en: This code uses a `@property` decorator. A *decorator* is simply a function that
    takes another function as input, does something with it (like adding some functionality),
    and returns it. In Python, a decorator is defined with the `@` symbol.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用了`@property`装饰器。*装饰器*只是一个以另一个函数作为输入的函数，对其进行一些操作（比如添加一些功能），然后返回它。在Python中，装饰器用`@`符号定义。
- en: '`@property` is a decorator used to handle access to class attributes.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`@property`是一个用于处理类属性访问的装饰器。'
- en: 'Our class wrapper is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的类包装器如下：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And we get this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到这个：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Splitting the code into functions is somewhat redundant in the sense that the
    same lines of code are recomputed with every call. One simple solution is to add
    a condition at the beginning of each function. In the next code iteration we will
    see an even nicer workaround.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码拆分为函数在某种程度上是多余的，因为相同的代码行在每次调用时都会重新计算。一个简单的解决方案是在每个函数的开头添加一个条件。在下一个代码迭代中，我们将看到一个更好的解决方法。
- en: In this setting there is no need to use variable sharing since the variables
    are kept as attributes of the model object. Also, after calling the training method
    `model.fit()` twice, we see that the variables have maintained their current state.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，不需要使用变量共享，因为变量被保留为模型对象的属性。此外，在调用两次训练方法`model.fit()`后，我们看到变量保持了它们的当前状态。
- en: In our last batch of code for this section we add another enhancement, creating
    a custom decorator that automatically checks whether the function was already
    called.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的最后一批代码中，我们添加了另一个增强功能，创建一个自定义装饰器，自动检查函数是否已被调用。
- en: Another improvement we can make is having all of our variables kept in a dictionary. This
    will allow us to keep track of our variables after each operation, as we saw in
    [Chapter 10](ch10.html#exporting_and_serving_models_with_tensorflow) when we looked
    at saving weights and models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的另一个改进是将所有变量保存在字典中。这将使我们能够在每次操作后跟踪我们的变量，就像我们在[第10章](ch10.html#exporting_and_serving_models_with_tensorflow)中看到的那样，当我们查看保存权重和模型时。
- en: 'Finally, additional functions for getting the values of the loss function and
    our weights are added:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，添加了用于获取损失函数值和权重的额外函数：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The custom decorator checks whether an attribute exists, and if not, it sets
    it according to the input function. Otherwise, it returns the attribute. `functools.wrap()`
    is used so we can reference the name of the function:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义装饰器检查属性是否存在，如果不存在，则根据输入函数设置它。否则，返回属性。使用`functools.wrap()`，这样我们就可以引用函数的名称：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This was a fairly basic example of how we can improve the overall code for our
    model. This kind of optimization might be overkill for our simple linear regression
    example, but it will definitely be worth the effort for complicated models with
    plenty of layers, variables, and features.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当基本的示例，展示了我们如何改进模型的整体代码。这种优化可能对我们简单的线性回归示例来说有些过度，但对于具有大量层、变量和特征的复杂模型来说，这绝对是值得的努力。
- en: Customization
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定制
- en: 'So far we’ve used two loss functions. In the classification example in [Chapter 2](ch02.html#go_with_the_flow)
    we used the cross-entropy loss, defined as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用了两个损失函数。在[第2章](ch02.html#go_with_the_flow)中的分类示例中，我们使用了交叉熵损失，定义如下：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In contrast, in the regression example in the previous section we used the
    square error loss, defined as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在前一节的回归示例中，我们使用了平方误差损失，定义如下：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: These are the most commonly used loss functions in machine learning and deep
    learning right now. The purpose of this section is twofold. First, we want to
    point out the more general capabilities of TensorFlow in utilizing custom loss
    functions. Second, we will discuss regularization as a form of extension of any
    loss function in order to achieve a specific goal, irrespective of the basic loss
    function used.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是目前在机器学习和深度学习中最常用的损失函数。本节的目的是双重的。首先，我们想指出TensorFlow在利用自定义损失函数方面的更一般能力。其次，我们将讨论正则化作为任何损失函数的扩展形式，以实现特定目标，而不考虑使用的基本损失函数。
- en: Homemade loss functions
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自制损失函数
- en: This book (and presumably our readers) takes a specific view of TensorFlow with
    the aspect of deep learning in mind. However, TensorFlow is more general in scope,
    and most machine learning problems can be formulated in a way that TensorFlow
    can be used to solve. Furthermore, any computation that can be formulated in the
    computation graph framework is a good candidate to benefit from TensorFlow.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本书（以及我们的读者）以深度学习为重点来看待 TensorFlow。然而，TensorFlow的范围更广泛，大多数机器学习问题都可以以一种 TensorFlow
    可以解决的方式来表述。此外，任何可以在计算图框架中表述的计算都是从 TensorFlow 中受益的好候选。
- en: The predominant special case is the class of unconstrained optimization problems.
    These are extremely common throughout scientific (and algorithmic) computing,
    and for these, TensorFlow is especially helpful. The reason these problems stand
    out is that TensorFlow provides an automatic mechanism for computing gradients,
    which affords a tremendous speedup in development time for such problems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 主要特例是无约束优化问题类。这些问题在科学（和算法）计算中非常常见，对于这些问题，TensorFlow尤为有用。这些问题突出的原因是，TensorFlow提供了计算梯度的自动机制，这为解决这类问题的开发时间提供了巨大的加速。
- en: In general, optimization with respect to an arbitrary loss function will be
    in the form
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，对于任意损失函数的优化将采用以下形式
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: where any optimizer could be used in place of the `GradientDescentOptimizer`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 任何优化器都可以用于替代 `GradientDescentOptimizer`。
- en: Regularization
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: Regularizationis the restriction of an optimization problem by imposing a penalty
    on the complexity of the solution (see the note in [Chapter 4](ch04.html#convolutional_neural_networks)
    for more details). In this section we take a look at specific instances where
    the penalty is directly added to the basic loss function in an additive form.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是通过对解决方案的复杂性施加惩罚来限制优化问题（有关更多详细信息，请参见[第4章](ch04.html#convolutional_neural_networks)中的注释）。在本节中，我们将看一下特定情况下，惩罚直接添加到基本损失函数中的附加形式。
- en: 'For example, building on the softmax example from [Chapter 2](ch02.html#go_with_the_flow),
    we have this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，基于[第2章](ch02.html#go_with_the_flow)中 softmax 示例，我们有以下内容：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The difference between this and the original in [Chapter 2](ch02.html#go_with_the_flow)
    is that we added `LAMBDA * tf.nn.l2_loss(W)` to the loss we are optimizing with
    respect to. In this case, using a small value of the trade-off parameter `LAMBDA`
    will have very little effect on the resulting accuracy (a large value will be
    detrimental). In large networks, where overfitting is a serious issue, this sort
    of regularization can often be a lifesaver.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与[第2章](ch02.html#go_with_the_flow)中的原始版本的区别在于，我们将 `LAMBDA * tf.nn.l2_loss(W)`
    添加到我们正在优化的损失中。在这种情况下，使用较小的权衡参数 `LAMBDA` 值对最终准确性的影响很小（较大的值会有害）。在大型网络中，过拟合是一个严重问题，这种正则化通常可以拯救一命。
- en: Regularization of this sort can be done with respect to the weights of the model,
    as shown in the previous example (also called *weight decay*, since it will cause
    the weights to have smaller values), as well as to the activations of a specific
    layer, or indeed all layers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的正则化可以针对模型的权重进行，如前面的示例所示（也称为*权重衰减*，因为它会使权重值变小），也可以针对特定层或所有层的激活进行。
- en: Another factor is what function we use—we could have used `l1` instead of the
    `l2` regularization, or a combination of the two. All combinations of these regularizers
    are valid and used in various contexts.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个因素是我们使用的函数——我们可以使用 `l1` 而不是 `l2` 正则化，或者两者的组合。所有这些正则化器的组合都是有效的，并在各种情境中使用。
- en: Many of the abstraction layers make the application of regularization as easy
    as specifying the number of filters, or the activation function. In Keras (a very
    popular extension reviewed in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications)),
    for instance, we are provided with the regularizers listed in [Table A-1](#tableA1),
    applicable to all the standard layers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 许多抽象层使正则化的应用变得简单，只需指定滤波器数量或激活函数即可。例如，在 Keras（在[第7章](ch07.html#tensorflow_abstractions_and_simplifications)中审查的非常流行的扩展）中，我们提供了适用于所有标准层的正则化器，列在[表A-1](#tableA1)中。
- en: Table A-1\. Regularization with Keras
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表A-1. 使用 Keras 进行正则化
- en: '| Regularizer | What it does | Example |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 正则化器 | 作用 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `l1` | `l1` regularization of weights |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `l1` | `l1` 正则化权重 |'
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `l2` | `l2` regularization of weights  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `l2` | `l2` 正则化权重 |'
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `l1l2` | Combined `l1 + l2` regularization of weights |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `l1l2` | 组合 `l1 + l2` 正则化权重 |'
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `activity_l1` | `l1` regularization of activations |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `activity_l1` | `l1` 正则化激活 |'
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `activity_l2` | `l2` regularization of activations |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `activity_l2` | `l2` 正则化激活 |'
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `activity_l1l2` | Combined `l1 + l2` regularization of activations |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| `activity_l1l2` | 组合 `l1 + l2` 正则化激活 |'
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Using these shortcuts makes it easy to test different regularization schemes
    when a model is overfitting.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型过拟合时，使用这些快捷方式可以轻松测试不同的正则化方案。
- en: Writing your very own op
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写自己的操作
- en: 'TensorFlow comes ready packed with a large number of native ops, ranging from
    standard arithmetic and logical operations to matrix operations, deep learning–specific
    functions, and more. When these are not enough, it is possible to extend the system
    by creating a new op. This is done in one of two ways:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow预装了大量本地操作，从标准算术和逻辑操作到矩阵操作、深度学习特定函数等等。当这些操作不够时，可以通过创建新操作来扩展系统。有两种方法可以实现这一点：
- en: Writing a “from scratch” C++ version of the operation
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写一个“从头开始”的 C++ 版本的操作
- en: Writing Python code that combines existing ops and Python code to create the
    new one
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写结合现有操作和 Python 代码创建新操作的 Python 代码
- en: We will spend the remainder of this section discussing the second option.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节的其余部分讨论第二个选项。
- en: 'The main reason to construct a Python op is to utilize NumPy functionality
    in the context of a TensorFlow computational graph. For the sake of illustration,
    we will construct the regularization example from the previous section by using
    the NumPy multiplication function rather than the TensorFlow op:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 构建Python op的主要原因是在TensorFlow计算图的上下文中利用NumPy功能。为了说明，我们将使用NumPy乘法函数构建前一节中的正则化示例，而不是TensorFlow
    op：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that this is done for the sake of illustration, and there is no special
    reason why anybody would want to use this instead of the native TensorFlow op.
    We use this oversimplified example in order to shift the focus to the details
    of the mechanism rather than the computation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是为了说明的目的，没有特别的原因让任何人想要使用这个而不是原生的TensorFlow op。我们使用这个过度简化的例子是为了将焦点转移到机制的细节而不是计算上。
- en: 'In order to use our new creation from within TensorFlow, we use the `py_func()`
    functionality:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在TensorFlow内部使用我们的新创建，我们使用`py_func()`功能：
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In our case, this means we compute the total loss as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，这意味着我们计算总损失如下：
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Doing this, however, will not be enough. Recall that TensorFlow keeps track
    of the gradients of each of the ops in order to perform gradient-based training
    of our overall model. In order for this to work with the new Python-based op,
    we have to specify the gradient manually. This is done in two steps.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这样做还不够。回想一下，TensorFlow会跟踪每个op的梯度，以便对我们整体模型进行基于梯度的训练。为了使这个与新的基于Python的op一起工作，我们必须手动指定梯度。这分为两个步骤。
- en: 'First, we create and register the gradient:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建并注册梯度：
- en: '[PRE25]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, when using the function, we point to this function as the gradient of
    the op. This is done using the string registered in the previous step:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在使用函数时，我们将这个函数指定为op的梯度。这是使用在上一步中注册的字符串完成的：
- en: '[PRE26]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Putting it all together, the code for the softmax model with regularization
    through our new Python-based op is now:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容放在一起，通过我们基于新Python op的正则化softmax模型的代码现在是：
- en: '[PRE27]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This can now be trained using the same code as in [Chapter 2](ch02.html#go_with_the_flow),
    when this model was first introduced.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用与[第2章](ch02.html#go_with_the_flow)中首次介绍该模型时相同的代码进行训练。
- en: Using the inputs in the computation of gradients
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在计算梯度时使用输入
- en: 'In the simple example we just showed, the gradient depends only on the gradient
    with respect to the input, and not on the input itself. In the general case, we
    will need access to the input as well. This is done easily, using the `op.input`s
    field:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚展示的简单示例中，梯度仅取决于相对于输入的梯度，而不是输入本身。在一般情况下，我们还需要访问输入。这很容易做到，使用`op.input`s字段：
- en: '[PRE28]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Other inputs (if they exist) are accessed in the same way.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其他输入（如果存在）以相同的方式访问。
- en: Required and Recommended Components for TensorFlow Serving
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Serving所需和推荐的组件
- en: In this section, we add details on some of the material covered in [Chapter 10](ch10.html#exporting_and_serving_models_with_tensorflow)
    and review in more depth some of the technical components used behind the scenes
    in TensorFlow Serving.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们添加了一些在[第10章](ch10.html#exporting_and_serving_models_with_tensorflow)中涵盖的材料的细节，并更深入地审查了TensorFlow
    Serving背后使用的一些技术组件。
- en: 'In [Chapter 10](ch10.html#exporting_and_serving_models_with_tensorflow), we
    used Docker to run TensorFlow Serving. Those who prefer to avoid using a Docker
    container need to have the following installed:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#exporting_and_serving_models_with_tensorflow)中，我们使用Docker来运行TensorFlow
    Serving。那些喜欢避免使用Docker容器的人需要安装以下内容：
- en: Bazel
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Bazel
- en: 'Bazel is Google’s own build tool, which recently became publicly available.
    When we use the term *build*, we are referring to using a bunch of rules to create
    output software from source code in a very efficient and reliable manner. The
    build process can also be used to reference external dependencies that are required
    to build the outputs. Among other languages, Bazel can be used to build C++ applications,
    and we exploit this to build the C++-written TensorFlow Serving’s programs. The
    source code Bazel builds upon is organized in a workspace directory inside nested
    hierarchies of packages, where each package groups related source files together.
    Every package consists of three types of files: human-written source files called
    *targets*, *generated files* created from the source files, and *rules* specifying
    the steps for deriving the outputs from the inputs.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Bazel是谷歌自己的构建工具，最近才公开。当我们使用术语*构建*时，我们指的是使用一堆规则从源代码中创建输出软件，以非常高效和可靠的方式。构建过程还可以用来引用构建输出所需的外部依赖项。除了其他语言，Bazel还可以用于构建C++应用程序，我们利用这一点来构建用C++编写的TensorFlow
    Serving程序。Bazel构建的源代码基于一个工作区目录，其中包含一系列包含相关源文件的嵌套层次结构的包。每个软件包包含三种类型的文件：人工编写的源文件称为*targets*，从源文件创建的*生成文件*，以及指定从输入派生输出的步骤的*规则*。
- en: Each package has a *BUILD* file, specifying the output to be built from the
    files inside that package. We use basic Bazel commands like `bazel build` to build
    generated files from targets, and `bazel run` to execute a build rule. We use
    the `-bin` flag when we want to specify the directories to contain the build outputs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 每个软件包都有一个*BUILD*文件，指定从该软件包内的文件构建的输出。我们使用基本的Bazel命令，比如`bazel build`来从目标构建生成的文件，以及`bazel
    run`来执行构建规则。当我们想要指定包含构建输出的目录时，我们使用`-bin`标志。
- en: Downloads and installation instructions can be found on [the Bazel website](https://bazel.build/versions/master/docs/install.html).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下载和安装说明可以在[Bazel网站](https://bazel.build/versions/master/docs/install.html)上找到。
- en: gRPC
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC
- en: Remote procedure call (RPC) is a form of client (caller)–server (executer) interaction;
    a program can request a procedure (for example, a method) that is executed on
    another computer (commonly in a shared network). gRPC is an open source framework
    developed by Google. Like any other RPC framework, gRPC lets you directly call
    methods on other machines, making it easier to distribute the computations of
    an application. The greatness of gRPC lies in how it handles the serialization, using
    the fast and efficient protocol buffers instead of XML or other methods.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 远程过程调用（RPC）是一种客户端（调用者）-服务器（执行者）交互的形式；程序可以请求在另一台计算机上执行的过程（例如，一个方法）（通常在共享网络中）。gRPC是由Google开发的开源框架。与任何其他RPC框架一样，gRPC允许您直接调用其他机器上的方法，从而更容易地分发应用程序的计算。gRPC的伟大之处在于它如何处理序列化，使用快速高效的协议缓冲区而不是XML或其他方法。
- en: Downloads and installation instructions can be found [on GitHub](https://github.com/grpc/grpc/tree/master/src/python/grpcio).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下载和安装说明可以在[GitHub](https://github.com/grpc/grpc/tree/master/src/python/grpcio)上找到。
- en: 'Next, you need to make sure that the necessary dependencies for Serving are
    installed with the following command:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要确保使用以下命令安装了Serving所需的依赖项：
- en: '[PRE29]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'And lastly, clone Serving:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，克隆Serving：
- en: '[PRE30]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As illustrated in [Chapter 10](ch10.html#exporting_and_serving_models_with_tensorflow), another
    option is to use a Docker container, allowing a simple and clean installation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第10章](ch10.html#exporting_and_serving_models_with_tensorflow)所示，另一个选择是使用Docker容器，实现简单干净的安装。
- en: What Is a Docker Container and Why Do We Use It?
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是Docker容器，为什么我们要使用它？
- en: Docker is essentially solving the same problem as Vagrant with VirtualBox, and
    that is making sure our code will run smoothly on other machines. Different machines
    might have different operating systems as well as different tool sets (installed
    software, configurations, permissions, etc.). By replicating the same environment—maybe
    for production purposes, maybe just to share with others—we guarantee that our
    code will run exactly the same way elsewhere as on our original development machine.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Docker本质上解决了与VirtualBox的Vagrant相同的问题，即确保我们的代码在其他机器上能够顺利运行。不同的机器可能具有不同的操作系统以及不同的工具集（安装的软件、配置、权限等）。通过复制相同的环境——也许是为了生产目的，也许只是与他人分享——我们保证我们的代码在其他地方与在原始开发机器上运行的方式完全相同。
- en: What’s unique about Docker is that, unlike other similarly purposed tools, it
    doesn’t create a fully operational virtual machine on which the environment will
    be built, but rather creates a *container* on top of an existing system (Ubuntu,
    for example), acting as a virtual machine in a sense and using our existing OS
    resources. These containers are created from a local Docker *image*, which is
    built from a *dockerfile* and encapsulates everything we need (dependency installations,
    project code, etc.). From that image we can create as many containers as we want
    (until we run out of memory, of course). This makes Docker a very cool tool with
    which we can easily create complete multiple environment replicas that contain
    our code and run them anywhere (very useful for cluster computing).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Docker的独特之处在于，与其他类似用途的工具不同，它不会创建一个完全操作的虚拟机来构建环境，而是在现有系统（例如Ubuntu）之上创建一个*容器*，在某种意义上充当虚拟机，并利用我们现有的操作系统资源。这些容器是从本地Docker
    *镜像*创建的，该镜像是从*dockerfile*构建的，并封装了我们需要的一切（依赖安装、项目代码等）。从该镜像中，我们可以创建任意数量的容器（当然，直到内存用尽为止）。这使得Docker成为一个非常酷的工具，我们可以轻松地创建包含我们的代码的完整多个环境副本，并在任何地方运行它们（对于集群计算非常有用）。
- en: Some Basic Docker Commands
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些基本的Docker命令
- en: To get you a bit more comfortable with using Docker, here’s a quick look at
    some useful commands, written in their most simplified form. Given that we have
    a dockerfile ready, we can build an image by using `docker build <*dockerfile*>`.
    From that image we can then create a new container by using the `docker run <*image*>`
    command. This command will also automatically run the container and open a terminal
    (type `exit` to close the terminal). To run, stop, and delete existing containers,
    we use the `docker start <*container id*>`, `docker stop <*container id*>`, and
    `docker rm <*container id*>` commands, respectively.  To see the list of all of
    our instances, both running and idle, we write `docker ps -a`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您更加熟悉使用Docker，这里简要介绍一些有用的命令，以最简化的形式编写。假设我们已经准备好一个dockerfile，我们可以使用`docker
    build <*dockerfile*>`来构建一个镜像。然后，我们可以使用`docker run <*image*>`命令创建一个新的容器。该命令还将自动运行容器并打开一个终端（输入`exit`关闭终端）。要运行、停止和删除现有容器，我们分别使用`docker
    start <*container id*>`、`docker stop <*container id*>`和`docker rm <*container
    id*>`命令。要查看所有实例的列表，包括正在运行和空闲的实例，我们输入`docker ps -a`。
- en: When we run an instance, we can add the `-p` flag followed by a port for Docker
    to expose, and the `-v` flag followed by a home directory to be mounted, which
    will enable us to work locally (the home directory is addressed via the `/mnt/home`
    path in the container).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行一个实例时，我们可以添加`-p`标志，后面跟一个端口供Docker暴露，以及`-v`标志，后面跟一个要挂载的主目录，这将使我们能够在本地工作（主目录通过容器中的`/mnt/home`路径进行访问）。
