- en: 1 Introduction to natural language processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 自然语言处理入门
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: What natural language processing (NLP) is, what it is not, and why it’s such
    an interesting, yet challenging, field
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是什么，它不是什么，为什么它是一个有趣而具有挑战性的领域
- en: How NLP relates to other fields, including artificial intelligence (AI) and
    machine learning (ML)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理与其他领域的关系，包括人工智能（AI）和机器学习（ML）
- en: What typical NLP applications and tasks are
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型的自然语言处理应用程序和任务是什么
- en: How a typical NLP application is developed and structured
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型自然语言处理应用程序的开发和结构
- en: This is not an introductory book to machine learning or deep learning. You won’t
    learn how to write neural networks in mathematical terms or how to compute gradients,
    for example. But don’t worry, even if you don’t have any idea what they are. I’ll
    explain those concepts as needed, not mathematically but conceptually. In fact,
    this book contains no mathematical formulae—not a single one. Also, thanks to
    modern deep learning libraries, you don’t really need to understand the math to
    build practical NLP applications. If you are interested in learning the theories
    and the math behind machine learning and deep learning, you can find a number
    of great resources out there.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一本机器学习或深度学习的入门书籍。你不会学到如何以数学术语编写神经网络，或者如何计算梯度，等等。但是不用担心，即使你对这些概念一无所知，我会在需要的时候进行解释，不会使用数学术语，而是从概念上解释。事实上，这本书不包含任何数学公式，一个都没有。此外，多亏了现代深度学习库，你真的不需要理解数学就能构建实用的自然语言处理应用程序。如果你有兴趣学习机器学习和深度学习背后的理论和数学，可以找到很多优秀的资源。
- en: But you do need to be at least comfortable enough to write in Python and know
    its ecosystems. However, you don’t need to be an expert in software engineering
    topics. In fact, this book’s purpose is to introduce software engineering best
    practices for developing NLP applications. You also don’t need to know NLP in
    advance. Again, this book is designed to be a gentle introduction to the field.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但你至少需要对Python编程感到自在，并了解它的生态系统。然而，你不需要成为软件工程领域的专家。事实上，本书的目的是介绍开发自然语言处理应用程序的软件工程最佳实践。你也不需要事先了解自然语言处理。再次强调，本书旨在对这个领域进行初步
- en: You need Python version 3.6.1 or higher and AllenNLP 2.5.0 or higher to run
    the code examples in this book. Note that we do not support Python 2, mainly because
    AllenNLP ([https://allennlp.org/](https://allennlp.org/)), the deep natural language
    processing framework I’m going to heavily use in this book, supports only Python
    3\. If you haven’t done so, I strongly recommend upgrading to Python 3 and familiarizing
    yourself with the latest language features such as type hints and new string-formatting
    syntax. This will be helpful, even if you are developing non-NLP applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 运行本书中代码示例需要Python版本3.6.1或更高版本以及AllenNLP版本2.5.0或更高版本。请注意，我们不支持Python 2，主要是因为这本书中我要大量使用的深度自然语言处理框架AllenNLP（[https://allennlp.org/](https://allennlp.org/)）只支持Python
    3。如果还没有升级到Python 3，我强烈建议你进行升级，并熟悉最新的语言特性，如类型提示和新的字符串格式化语法。即使你正在开发非自然语言处理的应用程序，这也会很有帮助。
- en: Don’t worry if you don’t have a Python development environment ready. Most of
    the examples in this book can be run via the Google Colab platform ([https://colab.research.google.com](https://colab.research.google.com)).
    You need only a web browser to build and experiment with NLP models!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有准备好Python开发环境，不要担心。本书中的大多数示例可以通过Google Colab平台（[https://colab.research.google.com](https://colab.research.google.com)）运行。你只需要一个网页浏览器就可以构建和实验自然语言处理模型！
- en: This book will use PyTorch ([https://pytorch.org/](https://pytorch.org/)) as
    its main choice of deep learning framework. This was a difficult decision for
    me, because several deep learning frameworks are equally great choices for building
    NLP applications, namely, TensorFlow, Keras, and Chainer. A few factors make PyTorch
    stand out among those frameworks—it’s a flexible and dynamic framework that makes
    it easier to prototype and debug NLP models; it’s becoming increasingly popular
    within the research community, so it’s easy to find open source implementations
    of major models; and the deep NLP framework AllenNLP mentioned earlier is built
    on top of PyTorch.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将使用PyTorch（[https://pytorch.org/](https://pytorch.org/)）作为主要的深度学习框架。这对我来说是一个难以选择的决定，因为有几个深度学习框架同样适合构建自然语言处理应用程序，包括TensorFlow、Keras和Chainer。有几个因素使得PyTorch在这些框架中脱颖而出——它是一个灵活且动态的框架，使得原型设计和调试自然语言处理模型更容易；它在研究界越来越受欢迎，所以很容易找到一些主要模型的开源实现；而之前提到的深度自然语言处理框架AllenNLP是建立在PyTorch之上的。
- en: 1.1 What is natural language processing (NLP)?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 什么是自然语言处理（NLP）？
- en: NLP is a principled approach to processing human language. Formally, it is a
    subfield of artificial intelligence (AI) that refers to computational approaches
    to process, understand, and generate human language. The reason it is part of
    AI is because language processing is considered a huge part of human intelligence.
    The use of language is arguably the most salient skill that separates humans from
    other animals.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 是一种处理人类语言的原则性方法。从形式上来说，它是人工智能（AI）的一个子领域，指的是处理、理解和生成人类语言的计算方法。之所以将其归为人工智能，是因为语言处理被认为是人类智能的一个重要组成部分。使用语言可以说是区分人类与其他动物的最显著的技能之一。
- en: 1.1.1 What is NLP?
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 什么是自然语言处理（NLP）？
- en: NLP includes a range of algorithms, tasks, and problems that take human-produced
    text as an input and produce some useful information, such as labels, semantic
    representations, and so on, as an output. Other tasks, such as translation, summarization,
    and text generation, directly produce text as output. In any case, the focus is
    on producing some output that is useful per se (e.g., a translation) or as input
    to other downstream tasks (e.g., parsing). I’ll touch upon some popular NLP applications
    and tasks in section 1.3.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 包括一系列算法、任务和问题，它们以人类生成的文本作为输入，并生成一些有用的信息，如标签、语义表示等，作为输出。其他任务，如翻译、摘要和文本生成，直接产生文本作为输出。无论哪种情况，重点是产生一些有用的输出本身（例如翻译），或作为其他下游任务的输入（例如解析）。我将在第1.3节介绍一些流行的
    NLP 应用和任务。
- en: 'You might wonder why NLP explicitly has “natural” in its name. What does it
    mean for a language to be natural? Are there any *un*natural languages? Is English
    natural? Which is more natural: Spanish or French?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么自然语言处理中明确地带有“自然”一词。一个语言被称为自然语言意味着什么？是否存在*非*自然语言？英语是自然语言吗？哪种更自然：西班牙语还是法语？
- en: The word “natural” here is used to contrast natural languages with formal languages.
    In this sense, all the languages humans speak are natural. Many experts believe
    that language emerged naturally tens of thousands of years ago and has evolved
    organically ever since. Formal languages, on the other hand, are types of languages
    that are invented by humans and have strictly and explicitly defined syntax (i.e.,
    what is grammatical) and semantics (i.e., what it means).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的“自然”一词用于将自然语言与形式语言进行对比。从这个意义上说，人类所说的所有语言都是自然语言。许多专家认为语言在数万年前自然形成，并自那时起有机地发展壮大。另一方面，形式语言是人类发明的一种语言类型，其语法（即什么是语法正确的）和语义（即它的含义）严格而明确地定义。
- en: Programming languages such as C and Python are good examples of formal languages.
    These languages are defined in such a strict way that it is always clear what
    is grammatical and ungrammatical. When you run a compiler or an interpreter on
    the code you write in those languages, you either get a syntax error or not. The
    compiler won’t say something like, “Hmm, this code is maybe 50% grammatical.”
    Also, the behavior of your program is always the same if it’s run on the same
    code, assuming external factors such as the random seed and the system states
    remain constant. Your interpreter won’t show one result 50% of the time and another
    the other 50% of the time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: C 和 Python 等编程语言是形式语言的好例子。这些语言被定义得非常严格，以至于始终清楚什么是语法正确的，什么是语法错误的。当你在这些语言中编写代码并运行编译器或解释器时，要么会得到语法错误，要么不会。编译器不会说：“嗯，这段代码可能有50%是语法正确的。”此外，如果在相同的代码上运行程序，假设外部因素如随机种子和系统状态保持不变，你的程序的行为总是相同的。你的解释器不会有50%的时间显示一种结果，另外50%的时间显示另一种结果。
- en: 'This is not the case for human languages. You can write a sentence that is
    *maybe* grammatical. For example, do you consider the phrase “The person I spoke
    to” ungrammatical? There are some grammar topics where even experts disagree with
    each other. This is what makes human languages interesting but challenging, and
    why the entire field of NLP even exists. Human languages are ambiguous, meaning
    that their interpretation is often not unique. Both structures (how sentences
    are formed) and semantics (what sentences mean) can have ambiguities in human
    language. As an example, let’s take a close look at the next sentence:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这在人类语言中并非如此。你可以写出一句*可能*是语法正确的句子。例如，你认为短语“我跟谁说话了”是语法错误的吗？在某些语法主题上，即使是专家之间也会存在意见分歧。这就是人类语言有趣但具有挑战性的地方，也是整个自然语言处理领域存在的原因。人类语言是有歧义的，这意味着它们的解释通常不是唯一的。在人类语言中，结构（句子如何构成）和语义（句子的含义）都可能存在歧义。举个例子，让我们仔细看一下下一句：
- en: '*He saw a girl with a telescope.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*他用望远镜看到一个女孩。*'
- en: When you read this sentence, who do you think has a telescope? Is it the boy,
    who’s using a telescope to see a girl (from somewhere far), or the girl, who has
    a telescope and is seen by the boy? There seem to be at least two interpretations
    of this sentence as shown in figure 1.1.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当你读到这句话时，你认为谁有望远镜呢？是男孩，他用望远镜看着一个女孩（从远处），还是女孩，她有望远镜被男孩看见了？这句话似乎至少有两种解释，如图1.1所示。
- en: '![CH01_F01_Hagiwara](../Images/CH01_F01_Hagiwara.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F01_Hagiwara](../Images/CH01_F01_Hagiwara.png)'
- en: Figure 1.1 Two interpretations of “He saw a girl with a telescope.”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 “他用望远镜看到一个女孩”两种解释。
- en: The reason you are confused upon reading this sentence is because you don’t
    know what the phrase “with a telescope” is about. More technically, you don’t
    know what this prepositional phrase (PP) modifies. This is called a *PP-attachment*
    problem and is a classic example of *syntactic ambiguity*. A syntactically ambiguous
    sentence has more than one interpretation of how the sentence is structured. You
    can interpret the sentence in multiple ways, depending on which structure of the
    sentence you believe.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当你读到这句话时感到困惑的原因是因为你不知道短语“用望远镜”是关于什么的。更具体地说，你不知道这个介词短语（PP）修改的是什么。这被称为*PP-attachment*问题，是*语法歧义*的经典例子。一个具有语法歧义的句子有多种解释句子结构的方式。你可以根据你相信的句子结构来对句子进行多种解释。
- en: 'Another type of ambiguity that may arise in natural language is *semantic ambiguity*.
    This is when the meaning of a word or a sentence, not its structure, is ambiguous.
    For example, let’s look at the following sentence:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言中可能出现的另一种歧义类型是*语义歧义*。当一个词或句子的含义，而不是它的结构，是模糊的时候，就是语义歧义。例如，让我们看看以下句子：
- en: '*I saw a bat.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*我看到了一只蝙蝠。*'
- en: 'There is no question how this sentence is structured. The subject of the sentence
    is “I” and the object is “a bat,” connected by the verb “saw.” In other words,
    there is no syntactical ambiguity in it. But how about its meaning? “Saw” has
    at least two meanings. One is the past tense of the verb “to see.” The other is
    to cut some object with a saw. Similarly, “a bat” can mean two very different
    things: is it a nocturnal flying mammal or a piece of wood used to hit a ball?
    All in all, does this sentence mean that I observed a nocturnal flying mammal
    or that I cut a baseball or cricket bat? Or even (cruelly) that I cut a nocturnal
    animal with a saw? You never know, at least from this sentence alone.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这句话的结构毫无疑问。句子的主语是“我”，宾语是“一个球棒”，由动词“看到”连接。换句话说，它没有语法上的歧义。但它的含义呢？“看到”至少有两种意思。一种是动词“看”的过去式。另一种是用锯子切割某个物体。同样，“一个球棒”可能意味着两种非常不同的东西：是夜间飞行的哺乳动物还是用来打击球的木头？总的来说，这句话是说我观察到了一只夜间飞行的哺乳动物，还是我切割了一个棒球或板球棒？或者（残酷地）我用锯子切割了一只夜间动物？你永远不知道，至少单从这句话来看。
- en: Ambiguity is what makes natural languages rich but also challenging to process.
    We can’t simply run a compiler or an interpreter on a piece of text and just “get
    it.” We need to face the complexities and subtleties of human languages. We need
    a scientific, principled approach to deal with them. That’s what NLP is all about.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 歧义是使自然语言丰富但也难以处理的原因。我们不能简单地在一段文字上运行编译器或解释器，然后“搞定”。我们需要面对人类语言的复杂性和微妙性。我们需要一种科学的、原则性的方法来处理它们。这就是自然语言处理的全部意义所在。
- en: Welcome to the beautiful world of natural languages.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到自然语言的美丽世界。
- en: 1.1.2 What is not NLP?
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.2 什么不是自然语言处理？
- en: 'Now let’s consider the following scenario and think how you’d approach this
    problem: you are working as a junior developer at a midsized company that has
    a consumer-facing product line. It’s 3 p.m. on a Friday. The rest of the team
    is becoming more and more restless as the weekend approaches. That’s when your
    boss drops by at your cubicle.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑以下情景，并思考你将如何解决这个问题：你正在一家中型公司担任初级开发人员，该公司拥有面向消费者的产品线。现在是星期五下午3点。随着周末的临近，团队的其他成员变得越来越不安。就在这时，你的老板来到了你的小隔间。
- en: “Hey, got a minute? I’ve got something interesting to show you. I just sent
    it to you.”
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: “嘿，有一分钟吗？我有一些有趣的东西要给你看。我刚刚发给你。”
- en: Your boss just sent you an email with a huge zip file attached to it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你的老板刚刚给你发了一封带有一个巨大压缩文件的电子邮件。
- en: “OK, so this is a giant TSV file. It contains all the responses to the survey
    questions about our product. I just got this data from the marketing team.”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: “好的，所以这是一个巨大的TSV文件。它包含了关于我们产品的调查问题的所有回答。我刚刚从市场团队那里得到了这些数据。”
- en: Obviously, the marketing team has been collecting user opinions about one of
    the products through a series of survey questions online.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，市场团队一直通过一系列在线调查问题收集用户对其中一个产品的意见。
- en: “The survey questions include standard ones like ‘How did you know about our
    product?’ and ‘How do you like our product?’ There is also a free-response question,
    where our customers can write whatever they feel about our product. The thing
    is, the marketing team realized there was a bug in the online system and the answers
    to the second question were not recorded in the database at all.”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: “调查问题包括标准问题，比如‘你是怎么知道我们的产品的？’和‘你喜欢我们的产品吗？’还有一个自由回答的问题，我们的客户可以写下他们对我们产品的感受。问题是，市场团队意识到在线系统中存在一个错误，第二个问题的答案根本没有被记录在数据库中。”
- en: “Wait, so there’s no way to tell how the customers are feeling about our product?”
    This sounds weirdly familiar. This must be a copy-and-paste error. When you first
    created an online data-collection interface, you copied and pasted the backend
    code without modifying the ID parameters, resulting in a loss of some data fields.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: “等等，那么我们没办法知道客户对我们的产品有什么感觉了？”这听起来怪怪的。这一定是一个复制粘贴错误。当你第一次创建在线数据收集界面时，你复制粘贴了后端代码，而没有修改
    ID 参数，导致一些数据字段丢失。
- en: “So,” your boss continues. “I was wondering if we can recover the lost data
    somehow. The marketing team is a little desperate now because they need to report
    the results to the VP early next week.”
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: “所以，”你的老板继续说道。“我在想我们是否能够以某种方式恢复丢失的数据。市场团队现在有点绝望，因为他们需要在下周初向副总裁报告结果。”
- en: At this point, your bad feeling has been confirmed. Unless you come up with
    a way to get this done as quickly as possible, your weekend plans will be ruined.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你的不好的感觉已经得到了确认。除非你想出一个尽快完成的方法，否则你的周末计划将被毁掉。
- en: “Didn’t you say you were interested in some machine learning? I think this is
    a perfect project for you. Anyway, it’d be great if you can give it a stab and
    let me know what you find. Do you think you can have some results by Monday?”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: “你不是说你对一些机器学习感兴趣吗？我觉得这对你来说是一个完美的项目。无论如何，如果你能试试并告诉我你的发现，那就太好了。你觉得周一之前能有一些结果吗？”
- en: “Well, I’ll give it a try.”
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: “好吧，我试试看。”
- en: You know “no” is not an acceptable answer here. Satisfied with your answer,
    your boss smiles and walks off.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道在这里“不行”是不可接受的回答。满意了你的回答，你的老板微笑着离开了。
- en: You start by skimming the TSV file. To your relief, its structure is fairly
    standard—it has several fields such as timestamps and submission IDs. At the end
    of each line is a lengthy field for the free-response question. Here they are,
    you think. At least you know where to look for some clues.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你开始浏览 TSV 文件。让你松了一口气的是，它的结构相当标准——它有几个字段，比如时间戳和提交 ID。每行的末尾是一个用于自由回答问题的冗长字段。这就是它们，你想。至少你知道可以在哪里找到一些线索。
- en: 'After a quick glance over the field, you find responses such as “A very good
    product!” and “Very bad. It crashes all the time!” Not too bad, you think. At
    least you can capture these simple cases. You start by writing the following method
    that captures those two cases:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 快速浏览字段后，你发现了诸如“一个非常好的产品！”和“很糟糕。它总是崩溃！”等响应。不算太糟糕，你想。至少你能捕捉到这些简单的情况。你开始编写以下方法来捕捉这两种情况：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then you run this method on the responses in the file and log the results, along
    with the original input. As intended, this method seems to be able to capture
    a dozen or so of the responses that contains “good” or “bad.”
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你对文件中的响应运行这个方法，并记录结果，以及原始输入。按照预期，这个方法似乎能够捕捉到包含“好”或“坏”几个响应。
- en: 'But then you start to see something alarming, as shown next:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但是接下来你开始看到一些令人担忧的东西，如下所示：
- en: '“I can’t think of a single good reason to use this product”: positive'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: “我想不出一个使用这个产品的好理由。”：正面
- en: '“It’s not bad.”: negative'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: “还行。”：负面
- en: 'Oops, you think. *Negation*. Yeah, of course. But this is pretty easy to deal
    with. You modify the method as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 糟糕，你想。*否定*。是的，当然。但这个很容易处理。你修改了方法如下：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You run the script again. This time, it seems to be behaving as intended, until
    you see an even more complicated example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你再次运行脚本。这一次，它似乎按预期运行，直到你看到了一个更复杂的例子：
- en: '“The product is not only cheap but also very good!”: negative'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: “这个产品不仅便宜，而且质量也非常好！”：负面
- en: 'Hmm, you think. This is probably not as straightforward as I initially thought
    after all. Maybe the negation has to be somewhere near “good” or “bad” for it
    to be effective. Wondering what steps you could take next, you scroll down to
    see more examples, which is when you see responses like these:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: “嗯，你想。这可能并不像我最初想的那么简单。也许否定词必须在‘好’或‘坏’附近才能有效果。想知道接下来可以采取什么步骤，你向下滚动以查看更多示例，这时你看到了这样的回答：
- en: '“I always wanted this feature badly!”: negative'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '“我一直很想要这个功能！”: negative'
- en: '“It’s very badly made.”: negative'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '“它做得很糟糕。”: negative'
- en: You silently curse to yourself. How could a single word in a language have two
    completely opposite meanings? At this point, your little hope for enjoying the
    weekend has already disappeared. You are already wondering what excuses you use
    with your boss next Monday.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你默默地咒骂自己。一个语言中的一个单词怎么会有完全相反的两个意思？此时，你对周末愉快的小希望已经消失了。你已经在想下周一对老板使用什么借口了。
- en: As a reader of this book, you’ll know better. You’ll know that NLP is not about
    throwing a bunch of ifs and thens at natural language text. It is a more principled
    approach to processing natural language. In the following chapters, you’ll learn
    how you should approach this problem before writing a single line of code and
    how to build a custom-made NLP application just for your task at hand.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本书的读者，你会更清楚。你会知道 NLP 不是简单地在自然语言文本中加入一堆 if 和 then。这是一个更有原则性的处理自然语言的方法。在接下来的章节中，你将学习在编写一行代码之前应该如何处理这个问题，以及如何为手头的任务构建一个定制的
    NLP 应用程序。
- en: 1.1.3 AI, ML, DL, and NLP
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.3 AI、ML、DL 和 NLP
- en: Before delving into the details of NLP, it’d be useful to clarify how it relates
    to other, similar fields. Most of you have at least heard about artificial intelligence
    (AI) and machine learning (ML). You may also have heard of deep learning (DL),
    because it’s generating a lot of buzz in popular media these days. Figure 1.2
    illustrates how those different fields overlap with each other.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究 NLP 的细节之前，澄清它与其他类似领域的关系是有用的。你们大多数人至少听说过人工智能（AI）和机器学习（ML）。你们可能也听说过深度学习（DL），因为它在当今流行媒体中引起了很多关注。图
    1.2 显示了这些不同领域之间的重叠关系。
- en: '![CH01_F02_Hagiwara](../Images/CH01_F02_Hagiwara.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F02_Hagiwara](../Images/CH01_F02_Hagiwara.png)'
- en: 'Figure 1.2 The relationship among different fields: AI, ML, DL, and NLP'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 不同领域之间的关系：AI、ML、DL 和 NLP
- en: Artificial intelligence (AI) is a broad umbrella field that is concerned with
    achieving human-like intelligence using machines. It encompasses a wide range
    of subfields, including machine learning, natural language processing, computer
    vision, and speech recognition. The field also includes subfields such as reasoning,
    planning, and search, which do not fall under either machine learning or natural
    language processing and are not in the scope of this book.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）是一个广泛的领域，致力于利用机器实现类似人类的智能。它涵盖了一系列子领域，包括机器学习、自然语言处理、计算机视觉和语音识别。该领域还包括推理、规划和搜索等子领域，这些子领域既不属于机器学习也不属于自然语言处理，也不在本书的范围内。
- en: Machine learning (ML) is usually considered a subfield of artificial intelligence
    that is about improving computer algorithms through experience and data. This
    includes learning a general function that maps inputs to outputs based on past
    experience (supervised learning), drawing hidden patterns and structures from
    data (unsupervised learning), and learning how to act in a dynamic environment
    based on indirect rewards (reinforcement learning). Throughout this book, we’ll
    make a heavy use of supervised machine learning, which is the main paradigm for
    training NLP models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）通常被认为是人工智能的一个子领域，它通过经验和数据改进计算机算法。这包括学习一个基于过去经验将输入映射到输出的一般函数（监督学习）、从数据中提取隐藏的模式和结构（无监督学习），以及根据间接奖励学习如何在动态环境中行动（强化学习）。在本书中，我们将大量使用监督机器学习，这是训练
    NLP 模型的主要范式。
- en: Deep learning (DL) is a subfield of machine learning that usually uses deep
    neural networks. These neural network models are called “deep” because they consist
    of a number of layers. A *layer* is just a fancy word for a substructure of neural
    networks. By having many stacked layers, deep neural networks can learn complex
    representations of data and can capture highly complicated relationships between
    the input and the output.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）是机器学习的一个子领域，通常使用深度神经网络。这些神经网络模型之所以称为“深度”，是因为它们由许多层组成。*层*只是神经网络的一个子结构的花哨说法。通过具有许多堆叠层，深度神经网络可以学习数据的复杂表示，并可以捕捉输入和输出之间的高度复杂的关系。
- en: As the amount of available data and computational resources increases, modern
    NLP makes a heavier and heavier use of machine learning and deep learning. Modern
    NLP applications and tasks are usually built on top of machine learning pipelines
    and trained from data. But also notice in figure 1.2 that a part of NLP does not
    overlap with machine learning. Traditional methods such as counting words and
    measuring similarities between text are usually not considered to be machine learning
    techniques per se, although they can be important building blocks for ML-based
    models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 随着可用数据量和计算资源的增加，现代NLP越来越多地使用机器学习和深度学习。现代NLP的应用和任务通常建立在机器学习管道之上，并从数据中进行训练。但请注意，在图1.2中，NLP的一部分与机器学习不重叠。诸如计数单词和衡量文本相似性之类的传统方法通常不被视为机器学习技术本身，尽管它们可以是ML模型的重要构建块。
- en: I’d also like to mention some other fields that are related to NLP. One such
    field is *computational linguistics* (CL). As its name suggests, computational
    linguistics is a subfield of linguistics that uses computational approaches to
    study human language. The main distinction between CL and NLP is that the former
    encompasses scientific approaches to study language, whereas the latter is concerned
    with engineering approaches for making computers perform something useful related
    to language. People often use those terms interchangeably, partly due to some
    historical reasons. For example, the most prestigious conference in the field
    of NLP is called ACL, which actually stands for “the Association for Computational
    Linguistics!”
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我还想提一下与自然语言处理（NLP）相关的其他领域。其中一个领域是*计算语言学*（CL）。顾名思义，计算语言学是语言学的一个子领域，它使用计算方法来研究人类语言。CL和NLP的主要区别在于前者涵盖了研究语言的科学方法，而后者关注的是使计算机执行与语言相关的有用任务的工程方法。人们经常将这些术语互换使用，部分原因是由于历史原因。例如，该领域中最负盛名的会议被称为ACL，实际上代表着“计算语言学协会！”
- en: Another related field is *text mining*. Text mining is a type of data mining
    targeted at textual data. Its focus is on drawing useful insights from unstructured
    textual data, which is a type of text data that is not formatted in a form that
    is easily interpretable by computers. Such data is usually collected from various
    sources, such as crawling the web and social media. Although its purpose is slightly
    different from that of NLP, these two fields are similar, and we can use the same
    tools and algorithms for both.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相关领域是*文本挖掘*。文本挖掘是一种针对文本数据的数据挖掘类型。它的重点是从非结构化的文本数据中获取有用的见解，这种文本数据不易被计算机解释。这些数据通常来自各种来源，如网络爬虫和社交媒体。虽然其目的与NLP略有不同，但这两个领域相似，我们可以为两者使用相同的工具和算法。
- en: 1.1.4 Why NLP?
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么选择NLP？
- en: If you are reading this, you have at least some interest in NLP. Why is NLP
    exciting? Why is it worth learning more about NLP and, specifically, real-world
    NLP?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读这篇文章，你至少对NLP有一些兴趣。为什么NLP令人兴奋？为什么值得更深入了解NLP，尤其是现实中的NLP？
- en: The first reason is that NLP is booming. Even without the recent AI and ML boom,
    NLP is more important than ever. We are witnessing the advent of practical NLP
    applications in our daily lives, such as conversational agents (think Apple Siri,
    Amazon Alexa, and Google Assistant) and near human-level machine translation (think
    Google Translate). A number of NLP applications are already an integral part of
    our day-to-day activities, such as spam filtering, search engines, and spelling
    correction, as we’ll discuss later. The number of Stanford students enrolled in
    NLP classes grew fivefold from 2008 to 2018 ([http://realworldnlpbook.com/ch1.html#tweet1](http://realworldnlpbook.com/ch1.html#tweet1)).
    Similarly, the number of attendees for EMNLP (Empirical Methods in Natural Language
    Processing), one of the top NLP conferences, doubled within just one year ([http://realworldnlpbook
    .com/ch1.html#tweet2](http://realworldnlpbook.com/ch1.html#tweet2)). Other major
    NLP conferences are also experiencing similar increases in participants and paper
    submissions ([http://realworldnlpbook.com/ch1 .html#nivre17](http://realworldnlpbook.com/ch1.html#nivre17)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原因是NLP正在蓬勃发展。即便没有最近的人工智能和机器学习热潮，NLP比以往任何时候都更为重要。我们正在见证实用NLP应用在我们日常生活中的出现，比如对话代理（想想苹果的Siri、亚马逊的Alexa和谷歌的Assistant）以及接近人类水平的机器翻译（想想谷歌翻译）。许多NLP应用已经成为我们日常活动的一部分，比如垃圾邮件过滤、搜索引擎和拼写纠正，我们稍后还会讨论。斯坦福大学选修NLP课程的学生人数从2008年到2018年增加了五倍（[http://realworldnlpbook.com/ch1.html#tweet1](http://realworldnlpbook.com/ch1.html#tweet1)）。同样，参加EMNLP（实证自然语言处理方法）这一顶级NLP会议的人数在短短一年内翻了一番（[http://realworldnlpbook.com/ch1.html#tweet2](http://realworldnlpbook.com/ch1.html#tweet2)）。其他主要的NLP会议也经历了类似的参与者和论文提交量的增加（[http://realworldnlpbook.com/ch1.html#nivre17](http://realworldnlpbook.com/ch1.html#nivre17)）。
- en: The second reason is that NLP is an evolving field. The field of NLP itself
    has a long history. The first experiment to build a machine translation system,
    called *The Georgetown-IBM Experiment*, was attempted back in 1954\. For more
    than 30 years since this experiment, most NLP systems relied on handwritten rules.
    Yes, it was not much different from what you just saw in section 1.1.1\. The first
    milestone, which came in the late 1980s, was the use of statistical methods and
    machine learning for NLP. Many NLP systems started leveraging statistical models
    trained from data. This led to some recent successes in NLP, including, most notably,
    IBM Watson. The second milestone was more drastic. Starting around the late 2000s,
    the use of so-called deep learning, that is, deep neural network models, took
    the field by storm. By the mid-2010s, deep neural network models became the new
    standard in the field.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是自然语言处理（NLP）是一个不断发展的领域。自然语言处理本身有着悠久的历史。最初的尝试建立机器翻译系统的实验，名为*乔治城-IBM实验*，始于1954年。自那次实验起30多年来，大多数NLP系统都依赖于手写规则。是的，这与你在1.1.1节中看到的没有太大不同。第一个里程碑出现在1980年代末，是使用统计方法和机器学习进行NLP。许多NLP系统开始利用从数据训练的统计模型。这导致了NLP近期的一些成功，其中最著名的包括IBM
    Watson。第二个里程碑变化更为剧烈。从2000年代末开始，所谓的深度学习，即深度神经网络模型，迅速席卷了这个领域。到2010年代中期，深度神经网络模型成为了该领域的新标准。
- en: This second milestone was so drastic and fast that it’s worth noting here. New
    neural network-based NLP models are not only more effective but also a lot simpler.
    For example, it used to take a lot of expertise and effort to replicate even a
    simple, baseline machine translation model. One of the most popular open source
    software packages for statistical machine translation, called Moses ([http://www.statmt.org/moses/](http://www.statmt.org/moses/)),
    is a behemoth, consisting of 100,000s of lines of code and dozens of supporting
    modules and tools. Experts spent hours just installing the software and making
    it work. On the other hand, as of 2018, anyone with some prior programming experience
    could run a neural machine translation system more powerful than traditional statistical
    models with a fraction of the code size—less than a few thousand lines of code
    (e.g., see TensorFlow’s neural machine translation tutorial at [https://github.com/tensorflow/nmt](https://github.com/tensorflow/nmt)).
    Also, the new neural network models are trained “end-to-end,” which means that
    those big, monolithic networks take the input and directly produce the output.
    Entire models are trained to match the desired output. On the other hand, traditional
    machine learning models consist of (at least) several submodules. These submodules
    are trained separately using different machine learning algorithms. In this book,
    I’ll mainly discuss modern neural network-based approaches to NLP but also touch
    upon some traditional concepts as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这第二个里程碑变化如此巨大和迅速，以至于值得在这里注意。基于新的神经网络的自然语言处理模型不仅更有效，而且更简单。例如，以前复制甚至是一个简单的基准机器翻译模型都需要很多专业知识和努力。一个最流行的用于统计机器翻译的开源软件包，称为Moses（[http://www.statmt.org/moses/](http://www.statmt.org/moses/)），是一个庞然大物，包含数十万行代码和数十个支持模块和工具。专家们花了几个小时的时间来安装软件并使其正常工作。另一方面，截至2018年，只要有一些先前的编程经验，任何人都可以运行一个比传统的统计模型更强大的神经机器翻译系统，代码量只有几千行以下（例如，请参阅TensorFlow的神经机器翻译教程[https://github.com/tensorflow/nmt](https://github.com/tensorflow/nmt)）。此外，新的神经网络模型是“端到端”训练的，这意味着那些庞大的、整体的网络接收输入并直接产生输出。整个模型都是为了匹配所需的输出而进行训练的。另一方面，传统的机器学习模型由（至少）几个子模块组成。这些子模块是使用不同的机器学习算法分别训练的。在本书中，我将主要讨论基于现代神经网络的自然语言处理方法，但也会涉及一些传统概念。
- en: The third and final reason is that NLP is challenging. Understanding and producing
    language is the central problem of artificial intelligence, as we saw in the previous
    section. The accuracy and performance in major NLP tasks such as speech recognition
    and machine translation got drastically better in the past decade or so. But human-level
    understanding of language is far from being solved.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个也是最后一个原因是自然语言处理是具有挑战性的。理解和产生语言是人工智能的核心问题，正如我们在前一节中看到的。在过去的十年左右，主要的自然语言处理任务，如语音识别和机器翻译的准确性和性能都得到了显著提高。但是，人类水平的语言理解距离解决尚远。
- en: 'To verify this quickly, open up your favorite machine translation service (or
    simply Google Translate), and type this sentence: “I saw her duck.” Try to translate
    it to Spanish or some other language you understand. You should see words like
    “*pato*,” which means “a duck” in Spanish. But did you notice another interpretation
    of this sentence? See figure 1.3 for the two interpretations. The word “duck”
    here could be a verb meaning “to crouch down.” Try adding another sentence after
    this, such as “She tried to avoid a flying ball.” Did the machine translation
    change the first translation in any way? The answer is probably no. You should
    still see the same “*pato*” in the translation. As you can see, most (if not all)
    commercial machine translation systems that are available as of today do not understand
    the context outside of the sentence that is being translated. A lot of research
    effort is spent on this problem in academia, but this is still one of many problems
    in NLP that is considered unsolved.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速验证这一点，打开你最喜欢的机器翻译服务（或简单地使用谷歌翻译），然后输入这句话：“I saw her duck.” 尝试将其翻译成西班牙语或其他你理解的语言。你应该看到像“*pato*”这样的词，在西班牙语中意思是“一只鸭子”。但是你是否注意到了这句话的另一种解释？请参见图1.3，其中包含了两种解释。这里的“duck”可能是一个动词，意思是“蹲下”。尝试在此之后添加另一句话，例如“她试图避开一只飞来的球。”
    机器翻译是否以任何方式改变了第一种翻译？答案很可能是否定的。你仍然应该在翻译中看到同样的“*pato*”。正如你所看到的，截至目前为止，大多数（如果不是全部）商业机器翻译系统都无法理解除正在翻译的句子之外的上下文。学术界在解决这个问题上花费了大量研究力量，但这仍然是自然语言处理中被认为尚未解决的问题之一。
- en: '![CH01_F03_Hagiwara](../Images/CH01_F03_Hagiwara.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F03_Hagiwara](../Images/CH01_F03_Hagiwara.png)'
- en: Figure 1.3 Two interpretations of “I saw her duck.”
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 “我看见她的鸭子”的两种解释。
- en: Compared to other AI fields such as robotics and computer vision, language has
    its own quirks. Unlike images, utterances and sentences have variable length.
    You can say a very short sentence (“Hello.”) or a very long one (“A quick brown
    fox . . .”). Most machine learning algorithms are not good at dealing with something
    of variable length, and you need to come up with ways to represent languages with
    something more fixed. If you look back at the history of the field, NLP is largely
    concerned with the problem of how to *represent* language mathematically. Vector
    space models and word embeddings (discussed in chapter 3) are some examples of
    this.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器人技术和计算机视觉等其他人工智能领域相比，语言有其自己的特点。与图像不同，话语和句子的长度是可变的。你可以说一个非常短的句子（“你好。”）或一个非常长的句子（“一个快速的棕色狐狸……”）。大多数机器学习算法不擅长处理可变长度的东西，你需要想办法用更固定的东西来表示语言。如果你回顾一下这个领域的历史，你会发现自然语言处理主要关注的问题是如何数学上*表示*语言。向量空间模型和词嵌入（在第3章中讨论）就是一些例子。
- en: Another characteristic of language is that it is *discrete*. What this means
    is that things in languages are separate as concepts. For example, if you take
    a word “rat” and change its first letter to the next one, you’ll have “sat.” In
    computer memory, the difference is just a single bit. However, there is no relationship
    between those two words except they both end with “at,” and maybe a rat can sit.
    There is no such thing as something that is in between “rat” and “sat.” These
    two are totally discrete, separate concepts that happen to have similar spelling.
    On the other hand, if you take an image of a car and change the value of a pixel
    by a single bit, you still have a car that is almost identical to the one before
    this change. Maybe it has a slightly different color. In other words, images and
    sounds are continuous, meaning that you can make small modifications without greatly
    affecting what they are. Many mathematical toolkits, such as vectors, matrices,
    and functions, are good at dealing with something continuous. The history of NLP
    is actually a history of challenging this discreteness of language, and only recently
    have we begun to see some successes on this front, for example, with word embeddings.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 语言的另一个特点是它是*离散*的。这意味着语言中的事物作为概念是分离的。例如，如果你拿一个词“rat”并将它的第一个字母改为下一个，你会得到“sat”。在计算机内存中，它们之间的差异仅仅是一个比特。然而，除了它们都以“at”结尾以外，这两个单词之间没有关系，也许老鼠可以坐着。不存在介于“rat”和“sat”之间的东西。这两者是完全离散的，独立的概念，只是拼写相似。另一方面，如果你拿一张汽车的图像并将一个像素的值改变一个比特，你仍然得到一辆几乎与改变前相同的汽车。也许颜色略有不同。换句话说，图像和声音是连续的，这意味着你可以做出小的修改而不会对它们的本质产生太大影响。许多数学工具包，如向量、矩阵和函数，都擅长处理连续性的事物。自然语言处理的历史实际上是挑战语言的这种离散性的历史，而且直到最近我们才开始在这个方面取得一些成功，例如，使用词嵌入。
- en: 1.2 How NLP is used
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 自然语言处理的应用
- en: As I mentioned previously, NLP is already an integral part of our daily life.
    In modern life, a larger and larger portion of our daily communication is done
    online, and our online communication is still largely conducted in natural language
    text. Think of your favorite social networking services, such as Facebook and
    Twitter. Although you can post photos and videos, a large portion of communication
    is still in text. As long as you are dealing with text, there is a need for NLP.
    For example, how do you know if a particular post is spam? How do you know which
    posts are the ones you are most likely to “like?” How do you know which ads you
    are most likely to click?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，自然语言处理已经成为我们日常生活的一个组成部分。在现代生活中，我们日常通信的越来越大的部分是在线完成的，而我们的在线通信仍然主要是自然语言文本。想想你最喜欢的社交网络服务，如Facebook和Twitter。虽然你可以发布照片和视频，但是很大一部分的通信仍然是文本。只要你在处理文本，就需要自然语言处理。例如，你怎么知道某个帖子是垃圾邮件？你怎么知道哪些帖子是你最可能“喜欢”的？你怎么知道哪些广告是你最可能点击的？
- en: Because many large internet companies need to deal with text in one way or another,
    chances are many of them are already using NLP. You can also confirm this from
    their “careers” page—you’ll see that they are always hiring NLP engineers and
    data scientists. NLP is also used to a varying extent in many other industries
    and products including, but not limited to, customer service, e-commerce, education,
    entertainment, finance, and health care, which all involve text in some ways.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因为许多大型互联网公司需要以某种方式处理文本，所以很有可能他们中的许多人已经在使用NLP。您还可以从他们的“招聘”页面确认这一点-您会看到他们一直在招聘NLP工程师和数据科学家。NLP在许多其他行业和产品中也以不同程度使用，包括但不限于客户服务，电子商务，教育，娱乐，金融和医疗保健，这些都以某种方式涉及文本。
- en: Many NLP systems and services can be classified into or built by combining some
    major types of NLP applications and tasks. In this section, I’m going to introduce
    some of the most popular applications of NLP as well as common NLP tasks.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 许多自然语言处理（NLP）系统和服务可以分类或通过结合一些主要类型的NLP应用和任务来构建。在本节中，我将介绍一些最受欢迎的NLP应用以及常见的NLP任务。
- en: 1.2.1 NLP applications
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 NLP应用
- en: An NLP application is a software application whose main purpose is to process
    natural language text and draw some useful information from it. Similar to general
    software applications, it can be implemented in various ways, such as an offline
    data-processing script, an offline standalone application, a backend service,
    or a full-stack service with a frontend, depending on its scope and use cases.
    It can be built for end users to use directly, for other backend services to consume
    its output, or for other businesses to use as a SaaS (software as a service).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: NLP应用是一种主要目的是处理自然语言文本并从中提取一些有用信息的软件应用。与一般软件应用类似，它可以以各种方式实现，例如离线数据处理脚本，离线独立应用程序，后端服务或具有前端的全栈服务，具体取决于其范围和用例。它可以为最终用户直接使用，供其他后端服务使用其输出，或供其他企业用作SaaS（软件即服务）使用。
- en: You can use many NLP applications out of the box, such as machine translation
    software and major SaaS products (e.g., Google Cloud API), if your requirement
    is generic and doesn’t require a high level of customization. You can also build
    your own NLP applications if you need customizations and/or you need to deal with
    a specific target domain. This is exactly what you’ll learn throughout this book!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的需求是通用的，并且不需要高度定制，则可以直接使用许多NLP应用，例如机器翻译软件和主要SaaS产品（例如，Google Cloud API）。如果您需要定制化和/或需要处理特定目标领域，则还可以构建自己的NLP应用。这正是您将在本书中学到的内容！
- en: Machine translation
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Machine translation is probably one of the most popular and easy-to-understand
    NLP applications. Machine translation (MT) systems translate a given text from
    one language to another language. An MT system can be implemented as a full-stack
    service (e.g., Google Translate), as well as a pure backend service (e.g., NLP
    SaaS products). The language the input text is written in is called *the source
    language*, whereas the one for the output is called *the target language*. MT
    encompasses a wide range of NLP problems, including language understanding and
    generation, because MT systems need to understand the input and then generate
    the output. MT is one of the most well-studied areas in NLP, and it was one of
    the earliest applications of NLP as well.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译可能是最受欢迎且易于理解的NLP应用之一。机器翻译（MT）系统将给定的文本从一种语言翻译为另一种语言。MT系统可以作为全栈服务（例如，Google翻译）实现，也可以作为纯后端服务（例如，NLP
    SaaS产品）实现。输入文本所使用的语言称为*源语言*，而输出文本所使用的语言称为*目标语言*。MT涵盖了一系列NLP问题，包括语言理解和生成，因为MT系统需要理解输入然后生成输出。MT是NLP中研究最深入的领域之一，也是最早的NLP应用之一。
- en: One challenge in MT is the tradeoff between *fluency* and *adequacy*. Translation
    needs to be fluent, meaning that the output has to sound natural in the target
    language. Translation also needs to be adequate, meaning that the output has to
    reflect the meaning expressed by the input as closely as possible. These two are
    often in conflict, especially when the source and the target languages are not
    very similar (e.g., English and Mandarin Chinese). You can write a sentence that
    is a precise, verbatim translation of the input, but doing so often leads to something
    that doesn’t sound natural in the target language. On the other hand, you can
    make up something that sounds natural but might not reflect the precise meaning.
    Good human translators address this tradeoff in a creative way. It’s their job
    to come up with translations that are natural in the target language while reflecting
    the meaning of the original.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译中的一个挑战是 *流畅度* 和 *充分性* 之间的权衡。翻译必须流畅，意思是输出必须在目标语言中听起来自然。翻译还必须充分，意思是输出必须尽可能地反映输入表达的意思。这两者经常发生冲突，特别是当源语言和目标语言不是很相似时（例如，英语和汉语）。你可以写出一句精确、逐字的翻译，但这样做通常会导致输出在目标语言中听起来不自然。另一方面，你可以编造一些听起来自然但可能不反映准确含义的东西。优秀的人类翻译者以一种创造性的方式解决了这种权衡。他们的工作是提出在目标语言中自然的翻译，同时反映原文的含义。
- en: Grammatical and spelling error correction
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 语法和拼写错误校正
- en: Most major web browsers nowadays support spelling correction. Even if you forget
    how to spell “Mississippi,” you can do your best and type what you remember, and
    the browser highlights it with a correction. Some word-processing software applications,
    including recent versions of Microsoft Word, do more than just correct spelling.
    They point out grammatical errors such as uses of “it’s” instead of “its.” This
    is not an easy feat, because both words are, in a sense, “correct” (no mistakes
    in spelling), and the system needs to infer whether they are used correctly from
    the context. Some commercial products (most notably, Grammarly, [https://www.grammarly.com/](https://www.grammarly.com/))
    specialize in grammatical error correction. Some products go a long way and point
    out incorrect usage of punctuation and even writing styles. These products are
    popular among both native and non-native speakers of the language.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当今大多数主要的网络浏览器都支持拼写纠正。即使你忘记了如何拼写“密西西比”，你也可以尽力输入你记得的内容，浏览器会用修正来突出显示它。一些文字处理软件应用程序，包括最近版本的微软
    Word，不仅仅纠正拼写。它们还指出语法错误，比如使用“it's”而不是“its”。这并不是一件容易的事情，因为从某种意义上说，这两个单词都是“正确的”（拼写上没有错误），系统需要从上下文中推断它们是否被正确使用。一些商业产品（尤其是
    Grammarly，[https://www.grammarly.com/](https://www.grammarly.com/)）专门用于语法错误校正。一些产品走得更远，指出了错误的标点使用甚至写作风格。这些产品在母语和非母语使用者中都很受欢迎。
- en: Research into grammatical error correction has been active due to the increasing
    number of non-native English speakers. Traditionally, grammatical error correction
    systems for non-native English speakers dealt with individual types of mistakes
    one by one. For example, you could think of a subcomponent of the system that
    detects and corrects only incorrect uses of articles (*a*, *an*, *the*, etc.),
    which is very common among non-native English speakers. More recent approaches
    to grammatical error correction are similar to the ones for machine translation.
    You can think of the (potentially incorrect) input as one language and the corrected
    output as another. Then your job is to “translate” between these two languages!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于非母语使用者的数量增加，语法错误校正的研究变得活跃起来。传统上，针对非母语使用者的语法错误校正系统是一次处理一个错误类型的。例如，你可以想象一个子系统，它只检测和纠正非母语使用者中非常常见的冠词使用错误（*a*,
    *an*, *the* 等）。最近的语法错误校正方法与机器翻译的方法类似。你可以将（可能是错误的）输入看作是一种语言，将校正后的输出看作是另一种语言。然后你的任务就是在这两种语言之间“翻译”！
- en: Search engine
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎
- en: Another application of NLP that is already an integral part of our daily lives
    is search engines. Few people would think of search engines as an NLP application,
    yet NLP plays such an important role in making search engines useful that they
    are worth mentioning here.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 的另一个已经成为我们日常生活中不可或缺部分的应用是搜索引擎。很少有人会将搜索引擎视为 NLP 应用，但 NLP 在使搜索引擎变得有用方面起着如此重要的作用，以至于在这里提到它们是值得的。
- en: Page analysis is one area where NLP is heavily used for search engines. Ever
    wonder why you don’t see any “hot dog” pages when you search for “dogs?” If you
    have any experience building your own full-text search engines using open source
    software such as Solr and Elasticsearch, and if you simply used a word-based index,
    your search result pages would be littered with “hot dogs,” even when you want
    just “dogs.” Major commercial search engines solve this problem by running the
    page content being indexed through NLP pipelines that recognize that “hot dogs”
    are not a type of “dogs.” But the extent and types of NLP pipelines that go into
    page analysis is confidential information for search engines and is difficult
    to know.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 页面分析是自然语言处理在搜索引擎中广泛应用的领域之一。您是否想知道为什么在搜索“dogs”时，您不会看到任何“hot dog”页面？如果您有使用开源软件如Solr和Elasticsearch构建自己的全文搜索引擎的经验，并且仅使用了基于单词的索引，那么您的搜索结果页面将会充满“hot
    dogs”，即使您只想搜索“dogs”。主要商业搜索引擎通过运行正在被索引的页面内容经过NLP流水线处理来解决这个问题，该流水线能够识别“hot dogs”不是一种“dogs”。但是，关于页面分析所涉及的NLP流水线的程度和类型是搜索引擎的机密信息，很难知道。
- en: Query analysis is another NLP application in search engines. If you have noticed
    Google showing a box with pictures and bios when you search for a celebrity or
    a box with the latest news stories when you search for certain current events,
    that’s query analysis in action. Query analysis identifies the intent (what the
    user wants) of the query and shows relevant information accordingly. A common
    way to implement query analysis is to make it a classification problem, where
    an NLP pipeline classifies queries into classes of intent (e.g., celebrity, news,
    weather, videos), although again, the details of how commercial search engines
    run query analysis are usually highly confidential.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 查询分析是搜索引擎中另一个NLP应用。如果您注意到，当您搜索某位名人时，Google会显示一个包含照片和个人简介的框，或者当您搜索某些时事时，会显示一个包含最新新闻故事的框，那就是查询分析在起作用。查询分析能够识别查询的意图（用户想要什么），并相应地显示相关信息。一种常用的实现查询分析的方法是将其视为分类问题，其中一个NLP流水线将查询分类为意图类别（如名人、新闻、天气、视频），尽管商业搜索引擎运行查询分析的细节通常是高度机密的。
- en: Finally, search engines are not only about analyzing pages and classifying queries.
    They have many other functionalities that make your searches easier, one of which
    is query correction. This comes into play when you make a spelling or a grammatical
    mistake when formulating the query, and Google and other major search engines
    show corrections with labels such as “showing results for:” and “Did you mean.”
    How this works is somewhat similar to grammatical error correction that I mentioned
    earlier, except it is optimized for the types of mistakes and queries that search
    engine users use.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，搜索引擎并不仅仅是关于分析页面和分类查询。它们还有很多其他功能，为您的搜索提供更便利的功能之一就是查询纠正。当您在查询时拼写错误或语法错误时，Google和其他主要的搜索引擎会显示带有标签“显示结果：“和“您是指：”的纠正。这是与我之前提到的语法错误纠正有些类似，只是它针对搜索引擎用户使用的错误和查询进行了优化。
- en: Dialog systems
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统
- en: Dialog systems are machines that humans can have conversations with. The field
    of dialog systems has a long history. One of the earliest dialog systems, ELIZA,
    was developed in 1966.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统是人类可以与之对话的机器。对话系统的领域有着悠久的历史。最早的对话系统之一是在1966年开发的ELIZA。
- en: But it’s only recently that dialog systems have found their ways into our daily
    lives. We have seen an almost exponential increase in their popularity in recent
    years, mainly driven by the availability of consumer-facing “conversational AI”
    products such as Amazon Alexa and Google Assistant. In fact, according to a survey
    in 2018, 20% of US homes already own a smart speaker. You may also remember being
    mind-blown watching the keynote at Google IO in 2018, where Google’s conversational
    AI called Google Duplex was shown making a phone call to a hair salon and a restaurant,
    having natural conversations with the staff at the business, and making an appointment
    on behalf of its user.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但是直到最近，对话系统才逐渐进入我们的日常生活。近年来，由消费者面向的“会话式AI”产品（如亚马逊Alexa和谷歌助手）的普及推动了对话系统的 popularity
    的近似指数增长。事实上，根据2018年的一项调查，美国家庭中已经有20%拥有智能音箱。您可能还记得在2018年的Google IO主题演讲中，谷歌的会话式AI——谷歌Duplex展示了向发型沙龙和餐厅打电话，并与业务人员进行自然对话，并代表用户预约的情景，令人惊叹不已。
- en: The two main types of dialog systems are task-oriented and chatbots. Task-oriented
    dialog systems are used to achieve specific goals (for example, reserving a plane
    ticket), obtaining some information, and, as we saw, making a reservation at a
    restaurant. Task-oriented dialog systems are usually built as an NLP pipeline
    consisting of several components, including speech recognition, language understanding,
    dialog management, response generation, and speech synthesis, which are usually
    trained separately. Similar to machine translation, though, there are new deep
    learning approaches where dialog systems (or their subsystems) are trained end-to-end.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 两种主要类型的对话系统是面向任务和聊天机器人。面向任务的对话系统用于实现特定目标（例如，预订机票）、获取一些信息，并且，正如我们所见，预订餐馆。面向任务的对话系统通常被构建为一个包含几个组件的自然语言处理管道，包括语音识别、语言理解、对话管理、响应生成和语音合成，这些组件通常是分开训练的。类似于机器翻译，然而，也有新的深度学习方法，其中对话系统（或其子系统）是端到端训练的。
- en: The other type of dialog system is chatbots, whose main purpose is to have conversations
    with humans. Traditional chatbots are usually managed by a set of handwritten
    rules (e.g., when the human says this, say that). Recently, the use of deep neural
    networks, particularly sequence-to-sequence models and reinforcement learning,
    has become increasingly popular. However, because the chatbots do not serve particular
    purposes, the evaluation of chatbots, that is, assessing how good a particular
    chatbot is, remains an open question.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种对话系统是聊天机器人，其主要目的是与人类进行交谈。传统的聊天机器人通常由一组手写规则管理（例如，当人类说这个时，说那个）。最近，深度神经网络的使用变得越来越流行，特别是序列到序列模型和强化学习。然而，由于聊天机器人不提供特定目的，评估聊天机器人，即评估特定聊天机器人的好坏，仍然是一个未决问题。
- en: 1.2.2 NLP tasks
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 NLP 任务
- en: Behind the scenes, many NLP applications are built by combining multiple NLP
    components that solve different NLP problems. In this section, I introduce some
    notable NLP tasks that are commonly used in NLP applications.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 幕后，许多自然语言处理应用是通过组合多个解决不同自然语言处理问题的自然语言处理组件构建的。在本节中，我介绍了一些在自然语言处理应用中常用的显著的自然语言处理任务。
- en: Text classification
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类
- en: Text classification is the process of classifying pieces of text into different
    categories. This NLP task is one of the simplest yet most widely used. You might
    not have heard of the term “text classification” before, but I bet most of you
    benefit from this NLP task every day. For example, spam filtering is one type
    of text classification. It classifies emails (or other types of text, such as
    web pages) into two categories—spam or not spam. This is why you get very few
    spam emails when you use Gmail and you see so few spammy (low-quality) web pages
    when you use Google.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是将文本片段分类到不同类别的过程。这个自然语言处理任务是最简单但也是最广泛使用的之一。你可能之前没听说过“文本分类”这个术语，但我打赌你们大多数人每天都从这个自然语言处理任务中受益。例如，垃圾邮件过滤就是一种文本分类。它将电子邮件（或其他类型的文本，如网页）分类为两类——垃圾邮件或非垃圾邮件。这就是为什么当你使用
    Gmail 时你几乎不会收到垃圾邮件，当你使用 Google 时你几乎看不到垃圾（低质量）网页。
- en: Another type of text classification is called *sentiment analysis*, which is
    what we saw in section 1.1\. Sentiment analysis is used to automatically identify
    subjective information, such as opinions, emotions, and feelings, within text.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种文本分类称为*情感分析*，这是我们在第1.1节中看到的。情感分析用于自动识别文本中的主观信息，如意见、情绪和感情。
- en: Part-of-speech tagging
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注
- en: A *part of speech* (POS) is a category of words that share the similar grammatical
    properties. In English, for example, nouns describe the names of things like objects,
    animals, people, and concepts, among many other things. A noun can be used as
    a subject of a verb, an object of a verb, and an object of a preposition. Verbs,
    in contrast, describe actions, states, and occurrences. Other English parts of
    speech include adjectives (*green*, *furious*), adverbs (*cheerfully*, *almost*),
    determiners (*a*, *the*, *this*, *that*), prepositions (*in*, *from*, *with*),
    conjunctions (*and*, *yet*, *because*), and many others. Almost all languages
    have nouns and verbs, but other parts of speech differ from language to language.
    For example, many languages, such as Hungarian, Turkish, and Japanese, have *postpositions*
    instead of prepositions, which are placed *after* words to add some extra meaning
    to them. A group of NLP researchers came up with a set of tags that cover frequent
    parts of speech that exist in most languages, called a *universal part-of-speech
    tagset* ([http://realworldnlpbook.com/ch1.html#universal-pos](http://realworldnlpbook.com/ch1.html#universal-pos)).
    This tagset is widely used for language-independent tasks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*词性*（POS）是共享相似语法属性的单词类别。 例如，在英语中，名词描述了物体、动物、人和概念等许多事物的名称。 名词可以用作动词的主语、动词的宾语和介词的宾语。
    相比之下，动词描述了动作、状态和事件。 其他英语词性包括形容词（*green*, *furious*）、副词（*cheerfully*, *almost*）、限定词（*a*,
    *the*, *this*, *that*）、介词（*in*, *from*, *with*）、连词（*and*, *yet*, *because*）等。
    几乎所有的语言都有名词和动词，但其他词性在语言之间有所不同。 例如，许多语言，如匈牙利语、土耳其语和日语，使用 *后置词* 而不是介词，后置词放在单词后面，为其添加一些额外的含义。
    一组自然语言处理研究人员提出了一组覆盖大多数语言中常见词性的标签，称为 *通用词性标签集* ([http://realworldnlpbook.com/ch1.html#universal-pos](http://realworldnlpbook.com/ch1.html#universal-pos))。
    这个标签集被广泛用于语言无关的任务。'
- en: Part-of-speech tagging is the process of tagging each word in a sentence with
    a corresponding part-of-speech tag. Some of you may have done this at school.
    As an example, let’s take the sentence “I saw a girl with a telescope.” The POS
    tags for this sentence are shown in figure 1.4.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注是给句子中的每个单词打上相应词性标签的过程。 你们中的一些人可能在学校已经做过这个了。 举个例子，让我们来看句子“I saw a girl with
    a telescope.” 这个句子的词性标签如图1.4所示。
- en: '![CH01_F04_Hagiwara](../Images/CH01_F04_Hagiwara.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F04_Hagiwara](../Images/CH01_F04_Hagiwara.png)'
- en: Figure 1.4 Part-of-speech (POS) tagging
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '图1.4 词性标注（POS） '
- en: These tags come from the Penn Treebank POS tagset, which is the most popular
    standard corpus for training and evaluating various NLP tasks such as POS tagging
    and parsing. Traditionally, POS tagging was solved by sequential labeling algorithms
    such as hidden Markov models (HMMs) and conditional random fields (CRFs). Recently,
    recurrent neural networks (RNNs) have become a popular and practical choice for
    training a POS tagger with high accuracy. The results of POS tagging are often
    used as the input to other downstream NLP tasks, such as machine translation and
    parsing. I’ll cover part-of-speech tagging in more detail in chapter 5.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标签来自宾树库词性标签集，它是训练和评估各种自然语言处理任务（如词性标注和解析）的最受欢迎的标准语料库。 传统上，词性标注是通过诸如隐马尔可夫模型（HMMs）和条件随机场（CRFs）之类的序列标记算法来解决的。
    最近，循环神经网络（RNNs）已成为训练高准确性词性标注器的流行和实用选择。 词性标注的结果通常被用作其他下游自然语言处理任务（如机器翻译和解析）的输入。
    我将在第5章中更详细地介绍词性标注。
- en: Parsing
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 解析
- en: Parsing is the task of analyzing the structure of a sentence. Broadly speaking,
    there are two main types of parsing, *constituency parsing* and *dependency parsing*,
    which we’ll discuss in detail next.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 解析是分析句子结构的任务。 广义上说，解析主要有两种类型，*成分解析* 和 *依存解析*，我们将在接下来详细讨论。
- en: 'Constituency parsing uses *context-free grammars* to represent natural language
    sentences. (See [http://mng.bz/GO5q](http://mng.bz/GO5q) for a brief introduction
    to context-free grammars). A context-free grammar is a way to specify how smaller
    building blocks of a language (e.g., words) are combined to form larger building
    blocks (e.g., phrases and clauses) and eventually sentences. To put it another
    way, it specifies how the largest unit (a sentence) is broken down to phrases
    and clauses and all the way down to words. The ways the linguistic units interact
    with each other are specified by a set of *production rules as follows*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 成分句法分析使用*上下文无关文法*来表示自然语言句子。（详见[http://mng.bz/GO5q](http://mng.bz/GO5q) 有关上下文无关文法的简要介绍）。上下文无关文法是一种指定语言的较小构建块（例如，单词）如何组合成较大构建块（例如，短语和从句），最终形成句子的方法。换言之，它指定了最大单位（句子）如何被分解为短语和从句，一直到单词。语言单元之间的交互方式由一组*产生式规则*来指定：
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A production rule describes a transformation from the symbol on the left-hand
    side (e.g., “S”) to the symbols on the right-hand side (e.g., “NP VP”). The first
    rule means that a sentence is a noun phrase (NP) followed by a verb phrase (VP).
    Some of the symbols (e.g., DT, NN, VBD) may look familiar to you—yes, they are
    the POS tags we just saw in the POS tagging section. In fact, you can consider
    POS tags as the smallest grammatical categories that behave in similar ways (because
    they are!).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 产生式规则描述了从左侧符号（例如，“S”）到右侧符号（例如，“NP VP”）的转换。第一条规则意味着句子是名词短语（NP）后跟动词短语（VP）。其中的一些符号（例如，DT，NN，VBD）可能看起来很熟悉——是的，它们是我们刚刚在词性标注部分看到的词性标记。事实上，你可以把词性标记看作行为类似的最小语法类别（因为它们就是！）。
- en: Now the parser’s job is to figure out how to reach the final symbol (in this
    case, “S”) starting from the raw words in the sentence. You can think of those
    rules as transformation rules from the symbols on the right to the ones on the
    left by traversing the arrow backward. For example, using the rule “DT  a” and
    “NN  girl,” you can convert “a girl” to “DT NN.” Then, if you use “NP  DT NN,”
    you can reduce the entire phrase to “NP.” If you illustrate this process in a
    tree-like diagram, you get something like the one shown in figure 1.5.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在解析器的工作就是找出如何从句子中的原始单词到达最终符号（在本例中是“S”）。你可以将这些规则看作是从右侧符号向左侧符号的转换规则，通过向后遍历箭头来做。例如，使用规则“DT
     a”和“NN  girl”，你可以将“a girl”转换为“DT NN”。然后，如果你使用“NP  DT NN”，你可以将整个短语缩减为“NP”。如果你将这个过程以树状图的方式呈现出来，你会得到类似图1.5所示的结果。
- en: '![CH01_F05_Hagiwara](../Images/CH01_F05_Hagiwara.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F05_Hagiwara](../Images/CH01_F05_Hagiwara.png)'
- en: Figure 1.5 Subtree for “a girl”
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5“a girl”子树
- en: Tree structures that are created in the process of parsing are called *parse
    trees**,* or simply *parses*. The figure is a subtree because it doesn’t cover
    the entirety of the tree (i.e., it doesn’t show all the way from “S” to words).
    Using the sentence “I saw a girl with a telescope” that we discussed earlier and
    see if you can parse it by hand. If you keep breaking down the sentence using
    the production rules until you get the final “S” symbol, you get the tree-like
    structure shown in figure 1.6.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析过程中创建的树形结构称为*解析树*，或简称*解析*。图中的子树因为并不涵盖整个树（即，不显示从“S”到单词的全部内容）而被称为子树。使用我们之前讨论过的句子“I
    saw a girl with a telescope”，试着手动解析一下看看。如果你一直使用产生式规则分解句子，直到得到最终的“S”符号，你就可以得到图1.6所示的树形结构。
- en: '![CH01_F06_Hagiwara](../Images/CH01_F06_Hagiwara.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F06_Hagiwara](../Images/CH01_F06_Hagiwara.png)'
- en: Figure 1.6 Parse tree for “I saw a girl with a telescope.”
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6“I saw a girl with a telescope.”的解析树
- en: Don’t worry if the tree in figure 1.6 is different from what you got. Actually,
    there’s another parse tree that is a valid parse of this sentence, shown in figure
    1.7.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图1.6中的树形结构和你得到的不一样，不用担心。实际上，有另一个解析树是这个句子的有效解析，如图1.7所示。
- en: '![CH01_F07_Hagiwara](../Images/CH01_F07_Hagiwara.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F07_Hagiwara](../Images/CH01_F07_Hagiwara.png)'
- en: Figure 1.7 Another parse tree for “I saw a girl with a telescope.”
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7“I saw a girl with a telescope.”的另一个解析树
- en: If you look at those two trees carefully, you’ll notice a difference where the
    “PP” (prepositional phrase) is located, or attached. In fact, these two parse
    trees correspond to the two different interpretations of this sentence we discussed
    in section 1.1\. The first tree (figure 1.6), where the PP attaches the verb “saw,”
    corresponds to the interpretation where the boy is using a telescope to see the
    girl. In the second tree (figure 1.7), where the PP attaches to the noun “a girl,”
    the boy saw the girl who has a telescope. Parsing is a great step forward to reveal
    the structure and the semantics of a sentence, but in cases like this one, parsing
    alone cannot uniquely decide what is the single most likely interpretation of
    a sentence.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细看这两棵树，你会注意到一个区别，即“PP”（介词短语）的位置或连接位置。 实际上，这两个分析树对应于我们在第1.1节中讨论的这个句子的两种不同解释。
    第一棵树（图1.6），其中PP连接动词“saw”，对应于男孩使用望远镜看女孩的解释。 在第二棵树（图1.7）中，其中PP连接到名词“a girl”，男孩看到了拿着望远镜的女孩。
    解析是揭示句子结构和语义的一个重要步骤，但在像这样的情况下，仅靠解析无法唯一确定句子的最可能解释。
- en: The other type of parsing is called *dependency parsing*. Dependency parsing
    uses dependency grammars to describe the structure of sentences, not in terms
    of phrases but in terms of words and the binary relations between them. For example,
    the result of dependency parsing of the earlier sentence is shown in figure 1.8.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解析的类型被称为*依存句法分析*。 依存句法分析使用依存语法来描述句子的结构，不是以短语为单位，而是以词和它们之间的二元关系为单位。 例如，先前句子的依存句法分析结果如图1.8所示。
- en: '![CH01_F08_Hagiwara](../Images/CH01_F08_Hagiwara.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F08_Hagiwara](../Images/CH01_F08_Hagiwara.png)'
- en: Figure 1.8 Dependency parse for “I saw a girl with a telescope.”
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 “我用望远镜看见了一个女孩”的依存解析。
- en: Notice that each relation is directional and labeled. A relation specifies which
    word depends on which word and the type of relationship between the two. For example,
    the relation connecting “a” to “girl” is labeled “det,” meaning the first word
    is the determiner of the second. If you take the most central word, “saw,” and
    pull it upward, you’ll notice that these words and relations form a tree. Such
    trees are called *dependency trees*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个关系都是有方向性的，并带有标签。 一个关系指明了一个词依赖于另一个词以及两者之间的关系类型。 例如，连接“a”到“girl”的关系标记为“det”，表示第一个词是第二个词的冠词。
    如果你把最中心的词“saw”拉向上方，你会注意到这些词和关系形成了一棵树。 这样的树被称为*依存树*。
- en: One advantage of dependency grammars is that they are agnostic regarding some
    word-order changes, meaning that the order of certain words in the sentence will
    not change the dependency tree. For example, in English, there is some freedom
    as to where to put an adverb in a sentence, especially when the adverb describes
    the manner in which the action referred to by the verb is done. For example, “I
    carefully painted the house” and “I painted the house carefully” are both acceptable
    and mean the same thing. If you represent these sentences by a dependency grammar,
    the word “carefully” always modifies the verb “painted,” and the two sentences
    have completely identical dependency trees. Dependency grammars capture more than
    just phrasal structures of sentences—they capture something more fundamental about
    the relationship of the words. Therefore, dependency parsing is considered an
    important step toward semantic analysis of natural language. A group of researchers
    is working on a formal language-independent dependency grammar, called Universal
    Dependencies, that is linguistically motivated and applicable to many languages,
    similar to the universal POS tagset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 依存语法的一个优点是它们对于某些词序变化是不可知的，这意味着句子中某些词的顺序不会改变依存树。 例如，在英语中，有些自由地将副词放在句子中的位置，特别是当副词描述由动词引起的动作的方式时。
    例如，“我小心地涂了房子”和“我小心地涂了房子”都是可以接受的，并且意思相同。 如果用依存语法表示这些句子，那么词“carefully”总是修改动词“painted”，而且两个句子具有完全相同的依存树。
    依存语法捕捉了句子的短语结构以上的东西-它们捕捉了关于词之间关系的更根本的东西。 因此，依存句法分析被认为是向自然语言语义分析迈出的重要一步。 一组研究人员正在开发一个称为Universal
    Dependencies的正式的语言无关依存语法，这个依存语法受语言启发，并且适用于许多语言，类似于通用POS标记集。
- en: Text generation
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成
- en: Text generation, also called *natural language generation* (NLG), is the process
    of generating natural language text from something else. In a broader sense, machine
    translation, which we discussed previously, involves a text-generation problem,
    because MT systems need to generate text in the target language. Similarly, summarization,
    text simplification, and grammatical error correction all produce natural language
    text as output and are instances of text-generation tasks. Because all of these
    tasks take natural language text as their input, they are called *text-to-text*
    generation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成，也称为*自然语言生成*（NLG），是从其他内容生成自然语言文本的过程。从更广泛的意义上讲，我们之前讨论过的机器翻译涉及到一个文本生成的问题，因为机器翻译系统需要在目标语言中生成文本。同样，摘要、文本简化和语法错误修正都会产生自然语言文本作为输出，并且都是文本生成任务的实例。因为所有这些任务都以自然语言文本作为输入，所以它们被称为*文本到文本*生成。
- en: Another class of text-generation task is called *data-to-text* generation. For
    those tasks, the input is data that is not text. For example, a dialog system
    needs to generate natural utterances based on the current state of the conversation.
    A publisher may wish to generate news text based on events such as sports game
    outcomes and weather. There is also a growing interest in generating natural language
    text that best describes a given image, called *image captioning*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类文本生成任务称为*数据到文本*生成。对于这些任务，输入是非文本数据。例如，对话系统需要根据对话当前状态生成自然的表达。出版商可能希望根据事件（例如体育比赛结果和天气）生成新闻文本。还存在着对生成最能描述给定图像的自然语言文本的兴趣，称为*图像字幕*。
- en: 'Finally, a third class of text classification is unconditional text generation,
    where natural language text is generated randomly from a model. You can train
    models so that they can generate random academic papers, Linux source code, or
    even poems and play scripts. For example, Andrej Karpathy trained an RNN model
    from all of Shakespeare’s works and succeeded in generating pieces of text that
    look exactly like his work ([http://realworldnlpbook.com/ch1.html#karpathy15](http://realworldnlpbook.com/ch1.html#karpathy15)),
    as shown next:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第三类文本分类是无条件文本生成，其中自然语言文本是从模型中随机生成的。您可以训练模型，使其能够生成随机的学术论文，Linux源代码，甚至是诗歌和剧本。例如，Andrej
    Karpathy训练了一个RNN模型，使用了莎士比亚的全部作品，并成功地生成了看起来完全像他的作品的文本片段（[http://realworldnlpbook.com/ch1.html#karpathy15](http://realworldnlpbook.com/ch1.html#karpathy15)），如下所示：
- en: '[PRE3]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Traditionally, text generation has been solved by handcrafted templates and
    rules for generating text from some information. You can think of this as the
    reverse of parsing, where rules are used to infer information about natural language
    text, as we discussed earlier. In recent years, neural network models are an increasingly
    popular choice for natural language generation, be it text-to-text generation
    (sequence-to-sequence models), data-to-text generation (encoder-decoder models),
    and unconditional text generation (neural language models and generative adversarial
    networks, or GANs). We’ll discuss text generation more in chapter 5.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，文本生成是通过手工制作的模板和规则来解决的，这些模板和规则用于从某些信息生成文本。您可以将其视为解析的反向过程，解析过程中使用规则来推断有关自然语言文本的信息，正如我们之前讨论的那样。近年来，神经网络模型越来越成为自然语言生成的流行选择，无论是文本到文本生成（序列到序列模型）、数据到文本生成（编码器-解码器模型）还是无条件文本生成（神经语言模型和生成对抗网络，或GANs）。我们将在第5章更深入地讨论文本生成。
- en: 1.3 Building NLP applications
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 构建NLP应用程序
- en: In this section, I’m going to show you how NLP applications are typically developed
    and structured. Although details may vary on a case-by-case basis, understanding
    the typical process helps you plan and budget before you start developing an application.
    It also goes a long way if you know best practices in developing NLP applications
    beforehand.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示NLP应用程序通常是如何开发和构建的。尽管具体细节可能会因案例而异，但了解典型的过程有助于您在开始开发应用程序之前进行规划和预算。如果您事先了解开发NLP应用程序的最佳实践，这也会有所帮助。
- en: 1.3.1 Development of NLP applications
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 NLP应用程序的开发
- en: The development of NLP applications is a highly iterative process, consisting
    of many phases of research, development, and operations (figure 1.9). Most learning
    materials such as books and online tutorials focus mainly on the training phase,
    although all the other phases of application development are equally important
    for real-world NLP applications. In this section, I briefly introduce what each
    stage involves. Note that no clear boundary exists between these phases. It is
    not uncommon that application developers (researchers, engineers, managers, and
    other stakeholders) go back and forth between some of these phases through trial
    and error.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: NLP应用的开发是一个高度迭代的过程，包括许多研究、开发和运营阶段（见图1.9）。大多数学习材料，如书籍和在线教程，主要关注训练阶段，尽管应用开发的所有其他阶段对于实际NLP应用同样重要。在本节中，我简要介绍了每个阶段涉及的内容。请注意，这些阶段之间没有明确的界限。应用开发者（研究人员、工程师、经理和其他利益相关者）经常会在一些阶段之间反复试验。
- en: '![CH01_F09_Hagiwara](../Images/CH01_F09_Hagiwara.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F09_Hagiwara](../Images/CH01_F09_Hagiwara.png)'
- en: Figure 1.9 The development cycle of NLP applications
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9 NLP应用的开发循环
- en: Data Collection
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集
- en: Most modern NLP applications are based on machine learning. Machine learning,
    by definition, requires data on which NLP models are trained (remember the definition
    of ML we talked about previously—it’s about improving algorithms through data).
    In this phase, NLP application developers discuss how to formulate the application
    as an NLP/ML problem and what kind of data should be collected. Data can be collected
    from humans (e.g., by hiring in-house annotators and having them go through a
    bunch of text instances), crowdsourcing (e.g., using platforms such as Amazon
    Mechanical Turk), or automated mechanisms (e.g., from application logs or clickstreams).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代自然语言处理（NLP）应用都是基于机器学习的。根据定义，机器学习需要训练NLP模型的数据（记住我们之前讨论过的ML的定义——它是通过数据来改进算法的）。在这个阶段，NLP应用开发者讨论如何将应用构建为一个NLP/ML问题，以及应收集哪种类型的数据。数据可以从人类那里收集（例如，通过雇佣内部注释者并让他们浏览一堆文本实例），众包（例如，使用亚马逊机械土耳其等平台），或自动机制（例如，从应用程序日志或点击流中收集）。
- en: You may choose not to use machine learning approaches for your NLP application
    at first, which could totally be the right choice depending on various factors,
    such as time, budgets, the complexity of the task, and the expected amount of
    data you might be able to collect. Even in that case, it may be a good idea to
    collect a small amount of data for validation purposes. I’ll talk more about training,
    validation, and testing of NLP applications in chapter 11.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能首先选择不使用机器学习方法进行你的NLP应用，这完全可能是正确的选择，这取决于各种因素，比如时间、预算、任务的复杂性以及你可能能够收集的数据量。即使在这种情况下，收集少量数据进行验证也可能是一个好主意。我将在第11章更详细地讨论NLP应用的训练、验证和测试。
- en: Analysis and experimenting
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 分析和实验
- en: 'After collecting the data, you move on to the next phase where you analyze
    and run some experiments. For analyses, you usually look for signals such as:
    What are the characteristics of the text instances? How are the training labels
    distributed? Can you come up with signals that are correlated with the training
    labels? Can you come up with some simple rules that can predict the training labels
    with reasonable accuracy? Should we even use ML? This list goes on and on. This
    analysis phase includes aspects of data science, where various statistical techniques
    may come in handy.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 收集数据后，您将进入下一个阶段，进行分析和运行一些实验。对于分析，您通常寻找诸如：文本实例的特征是什么？训练标签的分布情况如何？您能否提出与训练标签相关的信号？您能否提出一些简单的规则，以合理的准确性预测训练标签？我们甚至应该使用ML吗？这个清单不胜枚举。这个分析阶段包括数据科学的方面，各种统计技术可能会派上用场。
- en: 'You run experiments to try a number of prototypes quickly. The goal in this
    phase is to narrow down the possible set of approaches to a couple of promising
    ones, before you go all-in and start training a gigantic model. By running experiments,
    you wish to answer questions including: What types of NLP tasks and approaches
    are appropriate for this NLP application? Is this a classification, parsing, sequence
    labeling, regression, text generation, or some other problem? What is the performance
    of the baseline approach? What is the performance of the rule-based approach?
    Should we even use ML? What is the estimate of training and serving time for the
    promising approaches?'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你运行实验来快速尝试一些原型。这个阶段的目标是在你全力投入并开始训练庞大模型之前将可能的方法集缩小到几个有前途的方法。通过运行实验，你希望回答的问题包括：哪些类型的自然语言处理任务和方法适用于这个自然语言处理应用？这是一个分类、解析、序列标记、回归、文本生成还是其他一些问题？基线方法的性能如何？基于规则的方法的性能如何？我们是否应该使用机器学习？有关有前途方法的训练和服务时间的估计是多少？
- en: I call these first two phases the “research” phase. The existence of this phase
    is arguably the biggest difference between NLP applications and other generic
    software systems. Due to its nature, it is difficult to predict the performance
    and the behavior of a machine learning system, or an NLP system, for that matter.
    At this point, you might not have written a single line of production code, and
    that’s totally fine. The point of this research phase is to prevent you from wasting
    your effort writing production code that turns out to be useless at a later stage.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我把这两个阶段称为“研究”阶段。这个阶段的存在可以说是自然语言处理应用与其他通用软件系统之间最大的区别。由于其特性，很难预测机器学习系统或自然语言处理系统的性能和行为。在这一点上，你可能还没有写一行生产代码，但完全没问题。这个研究阶段的目的是防止你在以后的阶段浪费精力编写后来证明是无用的生产代码。
- en: Training
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 训练
- en: At this point you have pretty clear ideas what the approaches will be for your
    NLP application. This is when you start adding more data and computational resources
    (e.g., GPUs) for training your model. It is not uncommon for modern NLP models
    to take days if not weeks to train, especially if they are based on neural network
    models. It is always a good practice to gradually ramp up the amount of the data
    and the size of the model you train. You don’t want to spend weeks training a
    gigantic neural network model only to find that a smaller and simpler model performs
    just as well, or even worse, that you introduced a bug in the model and that the
    model you spent weeks training is simply useless!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你已经对你的自然语言处理应用的方法有了相当清晰的想法。这时你开始增加更多的数据和计算资源（例如，GPU）来训练你的模型。现代自然语言处理模型通常需要花费几天甚至几周的时间进行训练，尤其是基于神经网络模型的模型。逐渐增加你训练的数据量和模型的大小是一种很好的实践。你不想花几周的时间训练一个庞大的神经网络模型，只是发现一个更小、更简单的模型效果一样好，甚至更糟糕的是，你在模型中引入了一个
    bug，而你花了几周时间训练的模型根本没用！
- en: It is critical at this phase that you keep your training pipeline reproducible.
    Chances are, you will need to run this several times with different sets of hyperparameters,
    which are tuning values set before starting the model’s learning process. It is
    also likely that you will need to run this pipeline several months later, if not
    years. I’ll touch upon some best practices when training NLP/ML models in chapter
    10.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，保持你的训练流水线可复制是至关重要的。很可能你需要用不同的超参数集合运行这个流水线多次，超参数是在启动模型学习过程之前设置的调整值。很可能几个月甚至几年后你还需要再次运行这个流水线。我会在第10章讨论一些训练自然语言处理/机器学习模型的最佳实践。
- en: Implementation
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 实施
- en: 'When you have a model that is working with acceptable performance, you move
    on to the implementation phase. This is when you start making your application
    “production ready.” This process basically follows software engineering best practices,
    including: writing unit and integration tests for your NLP modules, refactoring
    your code, having your code reviewed by other developers, improving the performance
    of your NLP modules, and dockerizing your application. I’ll talk more about this
    process in chapter 11.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有一个表现良好的模型时，你就会进入实施阶段。这是你开始使你的应用“投入生产”的时候。这个过程基本上遵循软件工程的最佳实践，包括：为你的自然语言处理模块编写单元和集成测试，重构你的代码，让其他开发人员审查你的代码，提高你的自然语言处理模块的性能，并将你的应用程序打包成
    Docker 镜像。我将在第11章更详细地讨论这个过程。
- en: Deploying
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 部署
- en: Your NLP application is finally ready to deploy. You can deploy your NLP application
    in many ways—it can be an online service, a recurring batch job, an offline application,
    or an offline one-off task. If this is an online service that needs to serve its
    predictions in real time, it is a good idea to make this a *microservice* to make
    it loosely coupled with other services. In any case, it is a good practice to
    use continuous integration (CI) for your application, where you run tests and
    verify that your code and model are working as intended every time you make changes
    to your application.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 NLP 应用程序终于准备好部署了。你可以以多种方式部署你的 NLP 应用程序 —— 它可以是一个在线服务、一个定期批处理作业、一个离线应用程序，或者是一个离线一次性任务。如果这是一个需要实时提供预测的在线服务，将其打造成一个*微服务*以使其与其他服务松耦合是个好主意。无论如何，对于你的应用程序来说，使用持续集成（CI）是一个很好的实践，在这种情况下，你在每次对应用程序进行更改时都会运行测试，并验证你的代码和模型是否按预期工作。
- en: Monitoring
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 监控
- en: 'An important final step for developing NLP applications is monitoring. This
    not only includes monitoring the infrastructure such as server CPU, memory, and
    request latency, but also higher-level ML statistics such as the distributions
    of the input and the predicted labels. Some of the important questions to ask
    at this stage are: What do the input instances look like? Are they what you expected
    when you built your model? What do the predicted labels look like? Does the predicted
    label distribution match the one in the training data? The purpose of the monitoring
    is to check that the model you built is behaving as intended. If the incoming
    text or data instances or the predicted labels do not match your expectation,
    you may have an out-of-domain problem, meaning that the domain of the natural
    language data you are receiving is different from the one in which your model
    is trained. Machine learning models are usually not good at dealing with out-of-domain
    data, and the prediction accuracy may suffer. If this issue becomes obvious, it
    may be a good idea to repeat the whole process again, starting from collecting
    more in-domain data.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 开发 NLP 应用程序的一个重要的最终步骤是监控。这不仅包括监控基础架构，比如服务器 CPU、内存和请求延迟，还包括更高级别的 ML 统计信息，比如输入和预测标签的分布。在这个阶段要问一些重要的问题是：输入实例是什么样子的？它们是否符合你构建模型时的预期？预测的标签是什么样子的？预测的标签分布是否与训练数据中的分布相匹配？监控的目的是检查你构建的模型是否按预期运行。如果传入的文本或数据实例或预测的标签与你的期望不符，那么你可能遇到了一个领域外的问题，这意味着你收到的自然语言数据的领域与你的模型训练的领域不同。机器学习模型通常不擅长处理领域外数据，预测精度可能会受到影响。如果这个问题变得明显，那么重新开始整个过程可能是一个好主意，从收集更多的领域内数据开始。
- en: 1.3.2 Structure of NLP applications
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 NLP 应用程序的结构
- en: The structures of modern, machine learning-based NLP applications are becoming
    surprisingly similar for two main reasons—one is that most modern NLP applications
    rely on machine learning to some degree, and they should follow best practices
    for machine learning applications. The other is that, due to the advent of neural
    network models, a number of NLP tasks, including text classification, machine
    translation, dialog systems, and speech recognition, can now be trained end-to-end,
    as I mentioned before. Some of these tasks used to be hairy, enormous monsters
    with dozens of components with complex plumbing. Now, however, some of these tasks
    can be solved by less than 1,000 lines of Python code, provided that there’s enough
    data to train the model end-to-end.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现代基于机器学习的 NLP 应用程序的结构出人意料地相似，主要有两个原因——一个是大多数现代 NLP 应用程序在某种程度上依赖于机器学习，并且它们应该遵循机器学习应用程序的最佳实践。另一个原因是，由于神经网络模型的出现，一些
    NLP 任务，包括文本分类、机器翻译、对话系统和语音识别，现在可以端到端地进行训练，正如我之前提到的。其中一些任务过去是复杂的、包含数十个组件且具有复杂管道的庞然大物。然而，现在，一些这样的任务可以通过不到
    1000 行的 Python 代码来解决，只要有足够的数据来端到端地训练模型。
- en: 'Figure 1.10 illustrates the typical structure of a modern NLP application.
    There are two main infrastructures: the training and the serving infrastructure.
    The training infrastructure is usually offline and serves the purpose of training
    the machine learning model necessary for the application. It takes the training
    data, converts it to some data structure that can be handled by the pipeline,
    and further processes it by transforming the data and extracting the features.
    This part varies greatly from task to task. Finally, if the model is a neural
    network, data instances are batched and fed to the model, which is optimized to
    minimize the loss. Don’t worry if you don’t understand what I’m talking about
    in that last sentence—we’ll talk about those technical terms used with neural
    networks in chapter 2\. The trained model is usually serialized and stored to
    be passed to the serving infrastructure.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10展示了现代NLP应用程序的典型结构。有两个主要基础设施:训练基础设施和服务基础设施。训练基础设施通常是离线的，用于训练应用程序所需的机器学习模型。它接收训练数据，将其转换为可以由管道处理的某种数据结构，并通过转换数据和提取特征进一步处理数据。这一部分因任务而异。最后，如果模型是神经网络，将数据实例分批处理并馈送到模型中，该模型经过优化以最小化损失。如果你不理解我在最后一句说的是什么，不要担心，我们将在第二章讨论与神经网络一起使用的技术术语。训练好的模型通常是序列化并存储以传递给服务基础设施。
- en: '![CH01_F10_Hagiwara](../Images/CH01_F10_Hagiwara.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F10_Hagiwara](../Images/CH01_F10_Hagiwara.png)'
- en: Figure 1.10 Structure of typical NLP applications
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10典型NLP应用程序的结构
- en: The serving infrastructure’s job is to, given a new instance, produce the prediction,
    such as classes, tags, or translations. The first part of this infrastructure,
    which reads the instance and transforms it into some numbers, is similar to the
    one for training. In fact, you must keep the dataset reader and the transformer
    identical. Otherwise, discrepancies will arise in the way those two process the
    data, also known as *training-serving skew*. After the instance is processed,
    it’s fed to the pretrained model to produce the prediction. I’ll talk more about
    designing your NLP applications in chapter 11.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 服务基础设施的任务是在给定新实例的情况下生成预测，例如类别、标签或翻译。这个基础设施的第一部分，读取实例并将其转换为一些数字，与训练的部分类似。事实上，你必须保持数据集读取器和转换器相同。否则，这两个过程数据的方式将产生差异，也被称为*训练
    - 服务差异*。在处理实例后，它被馈送到预训练模型以生成预测。我将在第11章更多地讨论设计NLP应用程序的方法。
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Natural language processing (NLP) is a subfield of artificial intelligence (AI)
    that refers to computational approaches to process, understand, and generate human
    language.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理(NLP)是人工智能(AI)的一个子领域，指的是处理、理解和生成人类语言的计算方法。
- en: One of the challenges for NLP is ambiguity in natural languages. There is syntactic
    and semantic ambiguity.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP面临的挑战之一是自然语言中的歧义性。有句法和语义歧义。
- en: Where there is text, there is NLP. Many tech companies use NLP to draw information
    from a large amount of text. Typical NLP applications include machine translation,
    grammatical error correction, search engines, and dialog systems.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有文本的地方就有NLP。许多技术公司使用NLP从大量文本中提取信息。典型的NLP应用包括机器翻译、语法错误纠正、搜索引擎和对话系统。
- en: NLP applications are developed in an iterative way, with more emphasis on the
    research phase.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP应用程序以迭代方式开发，更多注重研究阶段。
- en: Many modern NLP applications rely heavily on machine learning (ML) and are structurally
    similar to ML systems.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多现代自然语言处理(NLP)应用程序严重依赖于机器学习(ML)，并且在结构上与ML系统相似。
